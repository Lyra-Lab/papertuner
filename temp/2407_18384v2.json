{
  "id": "http://arxiv.org/abs/2407.18384v2",
  "title": "Mathematical theory of deep learning",
  "authors": [
    "Philipp Petersen",
    "Jakob Zech"
  ],
  "abstract": "This book provides an introduction to the mathematical analysis of deep\nlearning. It covers fundamental results in approximation theory, optimization\ntheory, and statistical learning theory, which are the three main pillars of\ndeep neural network theory. Serving as a guide for students and researchers in\nmathematics and related fields, the book aims to equip readers with\nfoundational knowledge on the topic. It prioritizes simplicity over generality,\nand presents rigorous yet accessible results to help build an understanding of\nthe essential mathematical concepts underpinning deep learning.",
  "text": "Mathematical theory of deep\nlearning\nPhilipp Petersen1 and Jakob Zech2\n1Universit¨at Wien, Fakult¨at f¨ur Mathematik, 1090 Wien, Austria,\nphilipp.petersen@univie.ac.at\n2Universit¨at Heidelberg, Interdisziplin¨ares Zentrum f¨ur Wissenschaftliches Rechnen, 69120\nHeidelberg, Germany, jakob.zech@uni-heidelberg.de\nOctober 14, 2024\narXiv:2407.18384v2  [cs.LG]  11 Oct 2024\nContents\n1\nIntroduction\n9\n1.1\nMathematics of deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.2\nHigh-level overview of deep learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.3\nWhy does it work? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1.4\nOutline and philosophy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n1.5\nMaterial not covered in this book . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2\nFeedforward neural networks\n18\n2.1\nFormal definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.2\nNotion of size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.3\nActivation functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3\nUniversal approximation\n25\n3.1\nA universal approximation theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.2\nSuperexpressive activations and Kolmogorov’s superposition theorem\n. . . . . . . .\n35\n4\nSplines\n39\n4.1\nB-splines and smooth functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n4.2\nReapproximation of B-splines with sigmoidal activations . . . . . . . . . . . . . . . .\n40\n5\nReLU neural networks\n47\n5.1\nBasic ReLU calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n5.2\nContinuous piecewise linear functions . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n5.3\nSimplicial pieces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n5.4\nConvergence rates for H¨older continuous functions\n. . . . . . . . . . . . . . . . . . .\n64\n6\nAffine pieces for ReLU neural networks\n68\n6.1\nUpper bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.2\nTightness of upper bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n6.3\nDepth separation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n6.4\nNumber of pieces in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n7\nDeep ReLU neural networks\n81\n7.1\nThe square function\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n7.2\nMultiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n7.3\nCk,s functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n1\n8\nHigh-dimensional approximation\n92\n8.1\nThe Barron class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n8.2\nFunctions with compositionality structure . . . . . . . . . . . . . . . . . . . . . . . .\n97\n8.3\nFunctions on manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n9\nInterpolation\n106\n9.1\nUniversal interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n9.2\nOptimal interpolation and reconstruction\n. . . . . . . . . . . . . . . . . . . . . . . . 108\n10 Training of neural networks\n116\n10.1 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n10.2 Stochastic gradient descent (SGD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n10.3 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n10.4 Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n10.5 Other methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n11 Wide neural networks and the neural tangent kernel\n145\n11.1 Linear least-squares\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n11.2 Kernel least-squares\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n11.3 Tangent kernel\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n11.4 Convergence to global minimizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n11.5 Training dynamics for LeCun initialization . . . . . . . . . . . . . . . . . . . . . . . . 156\n11.6 Normalized initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n12 Loss landscape analysis\n171\n12.1 Visualization of loss landscapes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n12.2 Spurious valleys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n12.3 Saddle points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\n13 Shape of neural network spaces\n181\n13.1 Lipschitz parameterizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n13.2 Convexity of neural network spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n13.3 Closedness and best-approximation property . . . . . . . . . . . . . . . . . . . . . . . 187\n14 Generalization properties of deep neural networks\n194\n14.1 Learning setup\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n14.2 Empirical risk minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n14.3 Generalization bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n14.4 Generalization bounds from covering numbers . . . . . . . . . . . . . . . . . . . . . . 199\n14.5 Covering numbers of deep neural networks . . . . . . . . . . . . . . . . . . . . . . . . 201\n14.6 The approximation-complexity trade-off . . . . . . . . . . . . . . . . . . . . . . . . . 203\n14.7 PAC learning from VC dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n14.8 Lower bounds on achievable approximation rates . . . . . . . . . . . . . . . . . . . . 208\n2\n15 Generalization in the overparameterized regime\n213\n15.1 The double descent phenomenon\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\n15.2 Size of weights\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n15.3 Theoretical justification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n15.4 Double descent for neural network learning\n. . . . . . . . . . . . . . . . . . . . . . . 221\n16 Robustness and adversarial examples\n226\n16.1 Adversarial examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n16.2 Bayes classifier\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n16.3 Affine classifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n16.4 ReLU neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n16.5 Robustness\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\nA Probability theory\n241\nA.1 Sigma-algebras, topologies, and measures\n. . . . . . . . . . . . . . . . . . . . . . . . 241\nA.2 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\nA.3 Conditionals, marginals, and independence\n. . . . . . . . . . . . . . . . . . . . . . . 245\nA.4 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\nB Functional analysis\n251\nB.1\nVector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\nB.2\nFourier transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n3\nPreface\nThis book serves as an introduction to the key ideas in the mathematical analysis of deep learning.\nIt is designed to help students and researchers to quickly familiarize themselves with the area and to\nprovide a foundation for the development of university courses on the mathematics of deep learning.\nOur main goal in the composition of this book was to present various rigorous, but easy to grasp,\nresults that help to build an understanding of fundamental mathematical concepts in deep learning.\nTo achieve this, we prioritize simplicity over generality.\nAs a mathematical introduction to deep learning, this book does not aim to give an exhaustive\nsurvey of the entire (and rapidly growing) field, and some important research directions are missing.\nIn particular, we have favored mathematical results over empirical research, even though an accurate\naccount of the theory of deep learning requires both.\nThe book is intended for students and researchers in mathematics and related areas. While we\nbelieve that every diligent researcher or student will be able to work through this manuscript, it\nshould be emphasized that a familiarity with analysis, linear algebra, probability theory, and basic\nfunctional analysis is recommended for an optimal reading experience. To assist readers, a review\nof key concepts in probability theory and functional analysis is provided in the appendix.\nThe material is structured around the three main pillars of deep learning theory: Approximation\ntheory, Optimization theory, and Statistical Learning theory. Chapter 1 provides an overview and\noutlines key questions for understand deep learning. Chapters 2 - 9 explore results in approximation\ntheory, Chapters 10 - 13 discuss optimization theory for deep learning, and the remaining Chapters\n14 - 16 address the statistical aspects of deep learning.\nThis book is the result of a series of lectures given by the authors. Parts of the material were\npresented by P.P. in a lecture titled “Neural Network Theory” at the University of Vienna, and by\nJ.Z. in a lecture titled “Theory of Deep Learning” at Heidelberg University. The lecture notes of\nthese courses formed the basis of this book. We are grateful to the many colleagues and students\nwho contributed to this book through insightful discussions and valuable suggestions. We would\nlike to offer special thanks to the following individuals:\nJonathan Garcia Rebellon, Jakob Lanser, Andr´es Felipe Lerma Pineda, Martin Mauser, Davide\nModesto, Martina Neuman, Bruno Perreaux, Johannes Asmus Petersen, Milutin Popovic, Tuan\nQuach, Lorenz Riess, Jakob Fabian Rohner, Jonas Schuhmann, Peter ˇSkoln´ık, Matej Vedak, Simon\nWeissmann, Ashia Wilson.\n4\nNotation\nIn this section, we provide a summary of the notations used throughout the manuscript for the\nreader’s convenience.\nA\nvector of layer widths\nDefinition 12.1\nA\na sigma-algebra\nDefinition A.1\naff(S)\naffine hull of S\n(5.3.7)\nBd\nthe Borel sigma-algebra on Rd\nSection A.1\nBn\nB-Splines of order n\nDefinition 4.2\nBr(x)\nball of radius r ≥0 around x in a metric space X\n(B.1.1)\nBd\nr\nball of radius r ≥0 around 0 in Rd\nCk(Ω)\nk-times continuously differentiable functions from\nΩ→R\nC∞\nc (Ω)\ninfinitely differentiable functions from Ω→R\nwith compact support in Ω\nC0,s(Ω)\ns-H¨older continuous functions from Ω→R\nCk,s(Ω)\nCk(Ω) functions f for which f(k) ∈C0,s(Ω)\nDefinition 7.5\nfn\ncc\n−→f\ncompact convergence of fn to f\nDefinition 3.1\nco(S)\nconvex hull of a set S\n(5.3.1)\nf ∗g\nconvolution of f and g\nD\ndata distribution\n(1.2.4)/Section 14.1\nDα\npartial derivative\ndepth(Φ)\ndepth of Φ\nDefinition 2.1\nεapprox\napproximation error\n(14.2.3)\nεgen\ngeneralization error\n(14.2.3)\nεint\ninterpolation error\n(14.2.4)\nE[X]\nexpectation of random variable X\n(A.2.1)\nE[X|Y ]\nconditional expectation of random variable X\nSubsection A.3.3\nF(f) or ˆf\nFourier transform of f\nDefinition B.15\nG(S, ε, X)\nε-covering number of a set S ⊆X\nDefinition 14.10\nΓC\nBarron space with constant C\nSection 8.1\n∇xf\ngradient of f w.r.t. x\n⊘\ncomponentwise (Hadamard) division\nSymbol\nDescription\nReference\nContinued on next page\n5\n⊗\ncomponentwise (Hadamard) product\nhS\nempirical risk minimizer for a sample S\nDefinition 14.5\nΦid\nL\nidentity ReLU neural network\nLemma 5.1\n1S\nindicator function of the set S\n⟨·, ·⟩\nEuclidean inner product on Rd\n⟨·, ·⟩H\ninner product on a vector space H\nDefinition B.9\nkT\nmaximal number of elements shared by a single\nnode of a triangulation\n(5.3.2)\nKLC\nneural tangent kernel for the LeCun initialization\nTheorem 11.16\nˆKn(x, x′)\nempirical tangent kernel\n(11.3.4)\nKNTK\nneural tangent kernel for the NTK initialization\nTheorem 11.30\nΛA,σ,S,L\nloss landscape defining function\nDefinition 12.2\nLip(f)\nLipschitz constant of a function f\n(9.2.1)\nLipM(Ω)\nM-Lipschitz continuous functions on Ω\n(9.2.4)\nL\ngeneral loss function\nSection 14.1\nL0−1\n0-1 loss\nSection 14.1\nLce\nbinary cross entropy loss\nSection 14.1\nL2\nsquare loss\nSection 14.1\nLp(Ω)\nLebesgue space over Ω\nSection B.1.3\nM\npiecewise continuous and locally bounded func-\ntions\nDefinition 3.1.1\nN m\nd (σ; L, n)\nset of multilayer perceptrons with d-dim input, m-\ndim output, activation function σ, depth L, and\nwidth n\nDefinition 3.6\nN m\nd (σ; L)\nunion of N m\nd (σ; L, n) for all n ∈N\nDefinition 3.6\nN(σ; A, B)\nset of neural networks with architecture A, ac-\ntivation function σ and all weights bounded in\nmodulus by B\nDefinition 12.1\nN ∗(σ, A, B)\nneural networks in N(σ; A, B) with range in\n[−1, 1]\n(14.5.1)\nN\npositive natural numbers\nN0\nnatural numbers including 0\nN(m, C)\nmultivariate normal distribution with mean m ∈\nRd and covariance C ∈Rd×d\nSymbol\nDescription\nReference\nContinued on next page\n6\nnA\nnumber of parameters of a neural network with\nlayer widths described by A\nDefinition 12.1\n∥· ∥\nEuclidean norm for vectors in Rd and spectral\nnorm for matrices in Rn×d\n∥· ∥F\nFrobenius norm for matrices\n∥· ∥∞\n∞-norm on Rd or supremum norm for functions\n∥· ∥p\np-norm on Rd\n∥· ∥X\nnorm on a vector space X\n0\nzero vector in Rd\nO(·)\nLandau notation\nω(η)\npatch of the node η\n(5.3.5)\nΩΛ(c)\nsublevel set of loss landscape\nDefinition 12.3\nPn\nshort for Pn(Rd)\nPn(Rd)\nspace of multivariate polynomials of degree n in\nRd\nExample 3.5\nP\nshort for P(Rd)\nP[A]\nprobability of event A\nDefinition A.5\nP[A|B]\nconditional probability of event A given B\nDefinition A.3.2\nPX\ndistribution of random variable X\nDefinition A.10\nP(Rd)\nspace of multivariate polynomials in Rd\nExample 3.5\nΦlin\nlinearization of a model around initialization\n(11.3.1)\nΦmin\nn\nminimum neural network\nLemma 5.11\nΦ×\nε\nmultiplication neural network\nLemma 7.3\nΦ×\nn,ε\nmultiplication of n numbers neural network\nProposition 7.4\nΦ2 ◦Φ1\ncomposition of neural networks\nLemma 5.2\nΦ2 • Φ1\nsparse composition of neural networks\nLemma 5.2\n(Φ1, . . . , Φm)\nparallelization of neural networks\n(5.1.1)\nPieces(f, Ω)\nnumber of pieces of f on Ω\nDefinition 6.1\nPN(A, B)\nparameter set of neural networks with architec-\nture A and all weights bounded in modulus by\nB\nDefinition 12.1\nQ\nrational numbers\nR\nreal numbers\nSymbol\nDescription\nReference\nContinued on next page\n7\nR−\nnon-positive real numbers\nR+\nnon-negative real numbers\nRσ\nRealization map\nDefinition 12.1\nR∗\nBayes risk\n(14.1.1)\nR(h)\nrisk of hypothesis h\nDefinition 14.2\nbRS(h)\nempirical risk of h for sample S\n(1.2.3), Definition 14.4\nSn\ncardinal B-spline\nDefinition 4.1\nSd\nℓ,t,n\nmultivariate cardinal B-spline\nDefinition 4.2\n|S|\ncardinality of an arbitrary set S, or Lebesgue mea-\nsure of S ⊆Rd\n˚S\ninterior of a set S\nS\nclosure of a set S\n∂S\nboundary of a set S\nSc\ncomplement of a set S\nσ\ngeneral activation function\nσa\nparametric ReLU activation function\nSection 2.3\nσReLU\nReLU activation function\nSection 2.3\nsign\nsign function\nsize(Φ)\nnumber of free network parameters in Φ\nDefinition 2.4\nspan(S)\nlinear hull or span of S\nT\ntriangulation\nDefinition 5.13\nV[X]\nvariance of random variable X\nSection A.2.2\nVCdim(H)\nVC dimension of a set of functions H\nDefinition 14.16\nW\ndistribution of weight intialization\nSection 11.5.1\nW (ℓ), b(ℓ)\nweights and biases in layer ℓof a neural network\nDefinition 2.1\nwidth(Φ)\nwidth of Φ\nDefinition 2.1\nx(ℓ)\noutput of ℓ-th layer of a neural network\nDefinition 2.1\n¯x(ℓ)\npreactivations\n(10.3.3)\nX′\ndual space to a normed space X\nDefinition B.7\nSymbol\nDescription\nReference\n8\nChapter 1\nIntroduction\n1.1\nMathematics of deep learning\nIn 2012, a deep learning architecture revolutionized the field of computer vision by achieving un-\nprecedented performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\n[121]. The deep learning architecture, known as AlexNet, significantly outperformed all competing\ntechnologies. A few years later, in March 2016, a deep learning-based architecture called AlphaGo\ndefeated the best Go player at the time, Lee Sedol, in a five-game match [214]. Go is a highly\ncomplex board game with a vast number of possible moves, making it a challenging problem for\nartificial intelligence. Because of this complexity, many researchers believed that defeating a top\nhuman Go player was a feat that would only be achieved decades later.\nThese breakthroughs, along with many others including DeepMind’s AlphaFold [110], which\nrevolutionized protein structure prediction in 2020, the unprecedented language capabilities of\nlarge language models like GPT-3 (and later versions) [234, 28], and the emergence of generative\nAI models like Stable Diffusion, Midjourney, and DALL-E, have sparked interest among scientists\nacross (almost) all disciplines. Likewise, while mathematical research on neural networks has a\nlong history, these groundbreaking developments revived interest in the theoretical underpinnings\nof deep learning among mathematicians. However, initially, there was a clear consensus in the\nmathematics community: We do not understand why this technology works so well! In fact, there\nare many mathematical reasons that, at least superficially, should prevent the observed success.\nOver the past decade the field has matured, and mathematicians have gained a more profound\nunderstanding of deep learning, although many open questions remain. Recent years have brought\nvarious new explanations and insights into the inner workings of deep learning models. Before\ndiscussing these in detail in the following chapters, we first give a high-level introduction to deep\nlearning, with a focus on the supervised learning framework, which is the central theme of this\nbook.\n1.2\nHigh-level overview of deep learning\nDeep learning refers to the application of deep neural networks trained by gradient-based methods,\nto identify unknown input-output relationships. This approach has three key ingredients: deep\nneural networks, gradient-based training, and prediction. We now explain each of these ingredients\nseparately.\n9\n(\n(\nFigure 1.1: Illustration of a single neuron ν. The neuron receives six inputs (x1, . . . , x6) = x,\ncomputes their weighted sum P6\nj=1 xjwj, adds a bias b, and finally applies the activation function\nσ to produce the output ν(x).\nDeep Neural Networks\nDeep neural networks are formed by a combination of neurons. A\nneuron is a function of the form\nRd ∋x 7→ν(x) = σ(w⊤x + b),\n(1.2.1)\nwhere w ∈Rd is a weight vector, b ∈R is called bias, and the function σ is referred to as an\nactivation function. This concept is due to McCulloch and Pitts [142] and is a mathematical\nmodel for biological neurons. If we consider σ to be the Heaviside function, i.e., σ = 1R+ with\nR+ := [0, ∞), then the neuron “fires” if the weighted sum of the inputs x surpasses the threshold\n−b. We depict a neuron in Figure 1.1. Note that if we fix d and σ, then the set of neurons can be\nnaturally parameterized by the d + 1 real values w1, . . . , wd, b ∈R.\nNeural networks are functions formed by connecting neurons, where the output of one neuron\nbecomes the input to another. One simple but very common type of neural network is the so-called\nfeedforward neural network. This structure distinguishes itself by having the neurons grouped in\nlayers, and the inputs to neurons in the (ℓ+ 1)-st layer are exclusively neurons from the ℓ-th layer.\nWe start by defining a shallow feedforward neural network as an affine transformation\napplied to the output of a set of neurons that share the same input and the same activation\nfunction. Here, an affine transformation is a map T : Rp →Rq such that T(x) = W x + b for\nsome W ∈Rq×p, b ∈Rq where p, q ∈N.\nFormally, a shallow feedforward neural network is, therefore, a map Φ of the form\nRd ∋x 7→Φ(x) = T1 ◦σ ◦T0(x)\nwhere T0, T1 are affine transformations and the application of σ is understood to be in each\ncomponent of T1(x). A visualization of a shallow neural network is given in Figure 1.2.\nA deep feedforward neural network is constructed by compositions of shallow neural net-\nworks. This yields a map of the type\nRd ∋x 7→Φ(x) = TL+1 ◦σ ◦· · · ◦T1 ◦σ ◦T0(x),\n10\nwhere L ∈N and (Tj)L+1\nj=0 are affine transformations. The number of compositions L is referred to\nas the number of layers of the deep neural network. Similar to a single neuron, (deep) neural\nnetworks can be viewed as a parameterized function class, with the parameters being the entries\nof the matrices and vectors determining the affine transformations (Tj)L+1\nj=0 .\n(\n(\n(\n(\n(\n(\n0\nFigure 1.2: Illustration of a shallow neural network. The affine transformation T0 is of the form\n(x1, . . . , x6) = x 7→W x + b, where the rows of W are the weight vectors w1, w2, w3 for each\nrespective neuron.\nGradient-based training\nAfter defining the structure or architecture of the neural network,\ne.g., the activation function and the number of layers, the second step of deep learning consists of\ndetermining optimal values for its parameters. This optimization is carried out by minimizing an\nobjective function. In supervised learning, which will be our focus, this objective depends on a\ncollection of input-output pairs known as a sample. Concretely, let S = (xi, yi)m\ni=1 be a sample,\nwhere xi ∈Rd represents the inputs and yi ∈Rk the corresponding outputs with d, k ∈N. Our\ngoal is to find a deep neural network Φ such that\nΦ(xi) ≈yi\nfor all i = 1, . . . , m\n(1.2.2)\nin a meaningful sense. For example, we could interpret “≈” to mean closeness with respect to\nthe Euclidean norm, or more generally, that L(Φ(xi), yi) is small for a function L measuring the\ndissimilarity between its inputs. Such a function L is called a loss function. A standard way of\nachieving (1.2.2) is by minimizing the so-called empirical risk of Φ with respect to the sample\nS defined as\nbRS(Φ) = 1\nm\nm\nX\ni=1\nL(Φ(xi), yi).\n(1.2.3)\nIf L is differentiable, and for all xi the output Φ(xi) depends differentiably on the parameters\nof the neural network, then the gradient of the empirical risk bRS(Φ) with respect to the parameters\nis well-defined. This gradient can be efficiently computed using a technique called backpropa-\ngation. This allows to minimize (1.2.3) by optimization algorithms such as (stochastic) gradient\n11\ndescent. They produce a sequence of neural networks parameters, and corresponding neural net-\nwork functions Φ1, Φ2, . . . , for which the empirical risk is expected to decrease. Figure 1.3 illustrates\na possible behavior of this sequence.\nPrediction\nThe final part of deep learning concerns the question of whether we have actually\nlearned something by the procedure above. Suppose that our optimization routine has either con-\nverged or has been terminated, yielding a neural network Φ∗. While the optimization aimed to\nminimize the empirical risk on the training sample S, our ultimate interest is not in how well Φ∗per-\nforms on S. Rather, we are interested in its performance on new, unseen data points (xnew, ynew).\nTo make meaningful statements about this performance, we need to assume a relationship between\nthe training sample S and other data points.\nThe standard approach is to assume existence of a data distribution D on the input-output\nspace—in our case, this is Rd ×Rk—such that both the elements of S and all other considered data\npoints are drawn from this distribution. In other words, we treat S as an i.i.d. draw from D, and\n(xnew, ynew) also sampled independently from D. If we want Φ∗to perform well on average, then\nthis amounts to controlling the following expression\nR(Φ∗) = E(xnew,ynew)∼D[L(Φ∗(xnew), ynew)],\n(1.2.4)\nwhich is called the risk of Φ∗. If the risk is not much larger than the empirical risk, then we say\nthat the neural network Φ∗has a small generalization error. On the other hand, if the risk is\nmuch larger than the empirical risk, then we say that Φ∗overfits the training data, meaning that\nΦ∗has memorized the training samples, but does not generalize well to new data.\nFigure 1.3: A sequence of one dimensional neural networks Φ1, . . . , Φ4 that successively minimizes\nthe empirical risk for the sample S = (xi, yi)6\ni=1.\n12\n1.3\nWhy does it work?\nIt is natural to wonder why the deep learning pipeline, as outlined in the previous subsection,\nultimately succeeds in learning, i.e., achieving a small risk.\nIs it true that for a given sample\n(xi, yi)m\ni=1 there exist a neural network such that Φ(xi) ≈yi for all i = 1, . . . m?\nDoes the\noptimization routine produce a meaningful result? Can we control the risk, knowing only that the\nempirical risk is small?\nWhile most of these questions can be answered affirmatively under certain assumptions, these\nassumptions often do not apply to deep learning in practice.\nWe next explore some potential\nexplanations and show that they lead to even more questions.\nApproximation\nA fundamental result in the study of neural networks is the so-called universal\napproximation theorem, which will be discussed in Chapter 3. This result states that every con-\ntinuous function on a compact domain can be approximated arbitrarily well (in a uniform sense)\nby a shallow neural network.\nThis result, however, does not answer questions that are more specific to deep learning, such\nas the question of efficiency. For example, if we aim for computational efficiency, then we might\nbe interested in the smallest neural network that fits the data. This raises the question: What is\nthe role of the architecture for the expressive capabilities of neural networks?\nFurthermore, if we\nconsider reducing the empirical risk an approximation problem, we are confronted with one of the\nmain issues of approximation theory, which is the curse of dimensionality. Function approximation\nin high dimensions is notoriously difficult and gets exponentially harder with increasing dimension.\nIn practice, many successful deep learning architectures operate in this high-dimensional regime.\nWhy do these neural networks not seem to suffer from the curse of dimensionality?\nOptimization\nWhile gradient descent can sometimes be proven to converge to a global minimum\nas we will discuss in Chapter 10, this typically requires the objective function to be at least convex.\nHowever, there is no reason to believe that for example the empirical risk is a convex function of\nthe network parameters. In fact, due to the repeatedly occurring compositions with the nonlinear\nactivation function in the network, the empirical risk is typically highly non-linear and not convex.\nTherefore, there is generally no guarantee that the optimization routine will converge to a global\nminimum, and it may get stuck in a local (and non-global) minimum or a saddle point. Why is the\noutput of the optimization nonetheless often meaningful in practice?\nGeneralization\nIn traditional statistical learning theory, which we will review in Chapter 14,\nthe extent to which the risk exceeds the empirical risk, can be bounded a priori; such bounds are\noften expressed in terms of a notion of complexity of the set of admissible functions (the class of\nneural networks) divided by the number of training samples. For the class of neural networks of a\nfixed architecture, the complexity roughly amounts to the number of neural network parameters.\nIn practice, typically neural networks with more parameters than training samples are used. This\nis dubbed the overparameterized regime. In this regime, the classical estimates described above are\nvoid.\nWhy is it that, nonetheless, deep overparameterized architectures are capable of making accu-\nrate predictions on unseen data? Furthermore, while deep architectures often generalize well, they\nsometimes fail spectacularly on specific, carefully crafted examples. In image classification tasks,\n13\nthese examples may differ only slightly from correctly classified images in a way that is not per-\nceptible to the human eye. Such examples are known as adversarial examples, and their existence\nposes a great challenge for applications of deep learning.\n1.4\nOutline and philosophy\nThis book addresses the questions raised in the previous section, providing answers that are mathe-\nmatically rigorous and accessible. Our focus will be on provable statements, presented in a manner\nthat prioritizes simplicity and clarity over generality. We will sometimes illustrate key ideas only\nin special cases, or under strong assumptions, both to avoid an overly technical exposition, and\nbecause definitive answers are often not yet available. In the following, we summarize the content\nof each chapter and highlight parts pertaining to the questions stated in the previous section.\nChapter 2: Feedforward neural networks.\nIn this chapter, we introduce the main object\nof study of this book—the feedforward neural network.\nChapter 3: Universal approximation.\nWe present the classical view of function approx-\nimation by neural networks, and give two instances of so-called universal approximation results.\nSuch statements describe the ability of neural networks to approximate every function of a given\nclass to arbitrary accuracy, given that the network size is sufficiently large. The first result, which\nholds under very broad assumptions on the activation function, is on uniform approximation of\ncontinuous functions on compact domains. The second result shows that for a very specific acti-\nvation function, the network size can be chosen independent of the desired accuracy, highlighting\nthat universal approximation needs to be interpreted with caution.\nChapter 4: Splines. Going beyond universal approximation, this chapter starts to explore\napproximation rates of neural networks. Specifically, we examine how well certain functions can be\napproximated relative to the number of parameters in the network. For so-called sigmoidal activa-\ntion functions, we establish a link between neural-network- and spline-approximation. This reveals,\nthat smoother functions require fewer network parameters. However, achieving this increased effi-\nciency necessitates the use of deeper neural networks. This observation offers a first glimpse into\nthe importance of depth in deep learning.\nChapter 5: ReLU neural networks. This chapter focuses on one of the most popular ac-\ntivation functions in practice—the ReLU. We prove that the class of ReLU networks is equal to\nthe set of continuous piecewise linear functions, thus providing a theoretical foundation for their\nexpressive power. Furthermore, given a continuous piecewise linear function, we investigate the\nnecessary width and depth of a ReLU network to represent it. Finally, we leverage approxima-\ntion theory for piecewise linear functions to derive convergence rates for approximating H¨older\ncontinuous functions.\nChapter 6: Affine pieces for ReLU neural networks. Having gained some intuition about\nReLU neural networks, in this chapter, we adress some potential limitations. We analyze ReLU\nneural networks by counting the number of affine regions that they generate. The key insight of\nthis chapter is that deep neural networks can generate exponentially more regions than shallow\nones. This observation provides further evidence for the potential advantages of depth in neural\nnetwork architectures.\nChapter 7: Deep ReLU neural networks. Having identified the ability of deep ReLU\nneural networks to generate a large number of affine regions, we investigate whether this translates\ninto an actual advantage in function approximation. Indeed, for approximating smooth functions,\n14\nwe prove substantially better approximation rates than we obtained for shallow neural networks.\nThis adds again to our understanding of depth and its connections to expressive power of neural\nnetwork architectures.\nChapter 8: High-dimensional approximation. The convergence rates established in the\nprevious chapters deteriorate significantly in high-dimensional settings.\nThis chapter examines\nthree scenarios under which neural networks can provably overcome the curse of dimensionality.\nChapter 9: Interpolation. In this chapter we shift our perspective from approximation to\nexact interpolation of the training data. We analyze conditions under which exact interpolation is\npossible, and discuss the implications for empirical risk minimization. Furthermore, we present a\nconstructive proof showing that ReLU networks can express an optimal interpolant of the data (in\na specific sense).\nChapter 10: Training of neural networks. We start to examine the training process of deep\nlearning. First, we study the fundamentals of (stochastic) gradient descent and convex optimization.\nThen, we discuss how the backpropagation algorithm can be used to implement these optimization\nalgorithms for training neural networks. Finally, we examine accelerated methods and highlight\nthe key principles behind popular and more advanced training algorithms such as Adam.\nChapter 11:\nWide neural networks and the neural tangent kernel.\nThis chapter\nintroduces the neural tangent kernel as a tool for analyzing the training behavior of neural networks.\nWe begin by revisiting linear and kernel regression for the approximation of functions based on data.\nAfterwards, we demonstrate in an abstract setting that under certain assumptions, the training\ndynamics of gradient descent for neural networks resemble those of kernel regression, converging\nto a global minimum. Using standard intialization schemes, we then show that the assumptions\nfor such a statement to hold are satisfied with high probability, if the network is sufficiently wide\n(overparameterized). This analysis provides insights into why, under certain conditions, we can\ntrain neural networks without getting stuck in (bad) local minima, despite the non-convexity of\nthe objective function. Additionally, we discuss a well-known link between neural networks and\nGaussian processes, giving some indication why overparameterized networks do not necessarily\noverfit in practice.\nChapter 12: Loss landscape analysis. In this chapter, we present an alternative view on the\noptimization problem, by analyzing the loss landscape—the empirical risk as a function of the neural\nnetwork parameters. We give theoretical arguments showing that increasing overparameterization\nleads to greater connectivity between the valleys and basins of the loss landscape. Consequently,\noverparameterized architectures make it easier to reach a region where all minima are global minima.\nAdditionally, we observe that most stationary points associated with non-global minima are saddle\npoints. This sheds further light on the empirically observed fact that deep architectures can often\nbe optimized without getting stuck in non-global minima.\nChapter 13:\nShape of neural network spaces.\nWhile Chapters 11 and 12 highlight\npotential reasons for the success of neural network training, in this chapter, we show that the set\nof neural networks of a fixed architecture has some undesirable properties from an optimization\nperspective. Specifically, we show that this set is typically non-convex. Moreover, in general it does\nnot possess the best-approximation property, meaning that there might not exist a neural network\nwithin the set yielding the best approximation for a given function.\nChapter 14 : Generalization properties of deep neural networks.\nTo understand\nwhy deep neural networks successfully generalize to unseen data points (outside of the training\nset), we study classical statistical learning theory, with a focus on neural network functions as the\n15\nhypothesis class. We then show how to establish generalization bounds for deep learning, providing\ntheoretical insights into the performance on unseen data.\nChapter 15: Generalization in the overparameterized regime.\nThe generalization\nbounds of the previous chapter are not meaningful when the number of parameters of a neural net-\nwork surpasses the number of training samples. However, this overparameterized regime is where\nmany successful network architectures operate. To gain a deeper understanding of generalization\nin this regime, we describe the phenomenon of double descent and present a potential explana-\ntion. This addresses the question of why deep neural networks perform well despite being highly\noverparameterized.\nChapter 16: Robustness and adversarial examples.\nIn the final chapter, we explore\nthe existence of adversarial examples—inputs designed to deceive neural networks. We provide\nsome theoretical explanations of why adversarial examples arise, and discuss potential strategies to\nprevent them.\n1.5\nMaterial not covered in this book\nThis book studies some central topics of deep learning but leaves out even more.\nInteresting\nquestions associated with the field that were omitted, as well as some pointers to related works are\nlisted below:\nAdvanced architectures: The (deep) feedforward neural network is far from the only type of\nneural network. In practice, architectures must be adapted to the type of data. For example, images\nexhibit strong spatial dependencies in the sense that adjacent pixels often have similar values.\nConvolutional neural networks [128] are particularly well suited for this type of input, as they\nemploy convolutional filters that aggregate information from neighboring pixels, thus capturing the\ndata structure better than a fully connected feedforward network. Similarly, graph neural networks\n[27] are a natural choice for graph-based data.\nFor sequential data, such as natural language,\narchitectures with some form of memory component are used, including Long Short-Term Memory\n(LSTM) networks [93] and attention-based architectures like transformers [234].\nInterpretability/Explainability and Fairness: The use of deep neural networks in critical\ndecision-making processes, such as allocating scarce resources (e.g., organ transplants in medicine,\nfinancial credit approval, hiring decisions) or engineering (e.g., optimizing bridge structures, au-\ntonomous vehicle navigation, predictive maintenance), necessitates an understanding of their decision-\nmaking process. This is crucial for both practical and ethical reasons.\nPractically, understanding how a model arrives at a decision can help us improve its performance\nand mitigate problems. It allows us to ensure that the model performs according to our intentions\nand does not produce undesirable outcomes. For example, in bridge design, understanding why a\nmodel suggests or rejects a particular configuration can help engineers identify potential vulnerabil-\nities, ultimately leading to safer and more efficient designs. Ethically, transparent decision-making\nis crucial, especially when the outcomes have significant consequences for individuals or society; bi-\nases present in the data or model design can lead to discriminatory outcomes, making explainability\nessential.\nHowever, explaining the predictions of deep neural networks is not straightforward. Despite\nknowledge of the network weights and biases, the repeated and complex interplay of linear trans-\nformations and non-linear activation functions often renders these models black boxes. A compre-\nhensive overview of various techniques for interpretability, not only for deep neural networks, can\n16\nbe found in [149]. Regarding the topic of fairness, we refer for instance to [55, 8].\nUnsupervised and Reinforcement Learning: While this book focuses on supervised learn-\ning, where each data point xi has a label yi, there is a vast field of machine learning called unsuper-\nvised learning, where labels are absent. Classical unsupervised learning problems include clustering\nand dimensionality reduction [212, Chapters 22/23].\nA popular area in deep learning, where no labels are used, is physics-informed neural networks\n[187]. Here, a neural network is trained to satisfy a partial differential equation (PDE), with the\nloss function quantifying the deviation from this PDE.\nFinally, reinforcement learning is a technique where an agent can interact with an environment\nand receives feedback based on its actions. The actions are guided by a so-called policy, which is\nto be learned, [148, Chapter 17]. In deep reinforcement learning, this policy is modeled by a deep\nneural network. Reinforcement learning is the basis of the aforementioned AlphaGo.\nImplementation: While this book focuses on provable theoretical results, the field of deep\nlearning is strongly driven by applications, and a thorough understanding of deep learning cannot\nbe achieved without practical experience. For this, there exist numerous resources with excellent\nexplanations. We recommend [67, 38, 182] as well as the countless online tutorials that are just a\nGoogle (or alternative) search away.\nMany more: The field is evolving rapidly, and new ideas are constantly being generated\nand tested. This book cannot give a complete overview. However, we hope that it provides the\nreader with a solid foundation in the fundamental knowledge and principles to quickly grasp and\nunderstand new developments in the field.\nBibliography and further reading\nThroughout this book, we will end each chapter with a short overview of related work and the\nreferences used in the chapter.\nIn this introductory chapter, we highlight several other recent textbooks and works on deep\nlearning. For a historical survey on neural networks see [202] and also [127]. For general textbooks\non neural networks and deep learning, we refer to [84, 72, 182] for more recent monographs. A more\nmathematical introduction to the topic is given, for example, in [3, 107, 29]. For the implementation\nof neural networks we refer for example to [67, 38].\n17\nChapter 2\nFeedforward neural networks\nFeedforward neural networks, henceforth simply referred to as neural networks (NNs), constitute\nthe central object of study of this book. In this chapter, we provide a formal definition of neural\nnetworks, discuss the size of a neural network, and give a brief overview of common activation\nfunctions.\n2.1\nFormal definition\nWe previously defined a single neuron ν in (1.2.1) and Figure 1.1. A neural network is constructed\nby connecting multiple neurons. Let us now make precise this connection procedure.\nDefinition 2.1. Let L ∈N, d0, . . . , dL+1 ∈N, and let σ: R →R. A function Φ: Rd0 →RdL+1\nis called a neural network if there exist matrices W (ℓ) ∈Rdℓ+1×dℓand vectors b(ℓ) ∈Rdℓ+1,\nℓ= 0, . . . , L, such that with\nx(0) := x\n(2.1.1a)\nx(ℓ) := σ(W (ℓ−1)x(ℓ−1) + b(ℓ−1))\nfor ℓ∈{1, . . . , L}\n(2.1.1b)\nx(L+1) := W (L)x(L) + b(L)\n(2.1.1c)\nholds\nΦ(x) = x(L+1)\nfor all x ∈Rd0.\nWe call L the depth, dmax = maxℓ=1,...,L dℓthe width, σ the activation function, and\n(σ; d0, . . . , dL+1) the architecture of the neural network Φ. Moreover, W (ℓ) ∈Rdℓ+1×dℓare the\nweight matrices and b(ℓ) ∈Rdℓ+1 the bias vectors of Φ for ℓ= 0, . . . L.\nRemark 2.2. Typically, there exist different choices of architectures, weights, and biases yielding\nthe same function Φ : Rd0 →RdL+1. For this reason we cannot associate a unique meaning to these\nnotions solely based on the function realized by Φ. In the following, when we refer to the properties\n18\nof a neural network Φ, it is always understood to mean that there exists at least one construction\nas in Definition 2.1, which realizes the function Φ and uses parameters that satisfy those properties.\nThe architecture of a neural network is often depicted as a connected graph, as illustrated in\nFigure 2.1. The nodes in such graphs represent (the output of) the neurons. They are arranged in\nlayers, with x(ℓ) in Definition 2.1 corresponding to the neurons in layer ℓ. We also refer to x(0) in\n(2.1.1a) as the input layer and to x(L+1) in (2.1.1c) as the output layer. All layers in between\nare referred to as the hidden layers and their output is given by (2.1.1b). The number of hidden\nlayers corresponds to the depth. For the correct interpretation of such graphs, we note that by our\nconventions in Definition 2.1, the activation function is applied after each affine transformation,\nexcept in the final layer.\nNeural networks of depth one are called shallow, if the depth is larger than one they are called\ndeep. The notion of deep neural networks is not used entirely consistently in the literature, and\nsome authors use the word deep only in case the depth is much larger than one, where the precise\nmeaning of “much larger” depends on the application.\nThroughout, we only consider neural networks in the sense of Definition 2.1. We emphasize\nhowever, that this is just one (simple but very common) type of neural network. Many adjustments\nto this construction are possible and also widely used. For example:\n• We may use different activation functions σℓin each layer ℓor we may even use a different\nactivation function for each node.\n• Residual neural networks allow “skip connections”. This means that information is allowed\nto skip layers in the sense that the nodes in layer ℓmay have x(0), . . . , x(ℓ−1) as their input\n(and not just x(ℓ−1)), cf. (2.1.1).\n• In contrast to feedforward neural networks, recurrent neural networks allow information to\nflow backward, in the sense that x(ℓ−1), . . . , x(L+1) may serve as input for the nodes in layer ℓ\n(and not just x(ℓ−1)). This creates loops in the flow of information, and one has to introduce\na time index t ∈N, as the output of a node in time step t might be different from the output\nin time step t + 1.\nLet us clarify some further common terminology used in the context of neural networks:\n• parameters: The parameters of a neural network refer to the set of all entries of the weight\nmatrices and bias vectors. These are often collected in a single vector\nw = ((W (0), b(0)), . . . , (W (L), b(L))).\n(2.1.2)\nThese parameters are adjustable and are learned during the training process, determining the\nspecific function realized by the network.\n• hyperparameters: Hyperparameters are settings that define the network’s architecture (and\ntraining process), but are not directly learned during training. Examples include the depth,\nthe number of neurons in each layer, and the choice of activation function. They are typically\nset before training begins.\n• weights: The term “weights” is often used broadly to refer to all parameters of a neural\nnetwork, including both the weight matrices and bias vectors.\n19\nx(0)\n1\nx(0)\n2\nx(0)\n3\nx(1)\n1\nx(1)\n2\nx(1)\n3\nx(1)\n4\nx(2)\n1\nx(2)\n2\nx(2)\n3\nx(3)\n1\nx(3)\n2\nx(3)\n3\nx(3)\n4\nx(4)\n1\nx(4)\n2\nlayer 0\nlayer 1\nlayer 2\nlayer 3\nlayer 4\ninput\noutput\nhidden layers\nFigure 2.1: Sketch of a neural network with three hidden layers, and d0 = 3, d1 = 4, d2 = 3, d3 = 4,\nd4 = 2. The neural network has depth three and width four.\n• model: For a fixed architecture, every choice of network parameters w as in (2.1.2) defines\na specific function x 7→Φw(x). In deep learning this function is often referred to as a model.\nMore generally, “model” can be used to describe any function parameterization by a set of\nparameters w ∈Rn, n ∈N.\n2.1.1\nBasic operations on neural networks\nThere are various ways how neural networks can be combined with one another. The next propo-\nsition addresses this for linear combinations, compositions, and parallelization. The formal proof,\nwhich is a good exercise to familiarize oneself with neural networks, is left as Exercise 2.5.\nProposition 2.3. For two neural networks Φ1, Φ2, with architectures\n(σ; d1\n0, d1\n1, . . . , d1\nL1+1)\nand\n(σ; d2\n0, d2\n1, . . . , d2\nL2+1)\nrespectively, it holds that\n(i) for all α ∈R exists a neural network Φα with architecture (σ; d1\n0, d1\n1, . . . , d1\nL1+1) such that\nΦα(x) = αΦ1(x)\nfor all x ∈Rd1\n0,\n(ii) if d1\n0 = d2\n0 =: d0 and L1 = L2 =: L, then there exists a neural network Φparallel with architecture\n(σ; d0, d1\n1 + d2\n1, . . . , d1\nL+1 + d2\nL+1) such that\nΦparallel(x) = (Φ1(x), Φ2(x))\nfor all x ∈Rd0,\n(iii) if d1\n0 = d2\n0 =: d0, L1 = L2 =: L, and d1\nL+1 = d2\nL+1 =: dL+1, then there exists a neural network\nΦsum with architecture (σ; d0, d1\n1 + d2\n1, . . . , d1\nL + d2\nL, dL+1) such that\nΦsum(x) = Φ1(x) + Φ2(x)\nfor all x ∈Rd0,\n20\n(iv) if\nd1\nL1+1\n=\nd2\n0,\nthen\nthere\nexists\na\nneural\nnetwork\nΦcomp\nwith\narchitecture\n(σ; d1\n0, d1\n1, . . . , d1\nL1, d2\n1, . . . , d2\nL2+1) such that\nΦcomp(x) = Φ2 ◦Φ1(x)\nfor all x ∈Rd1\n0.\n2.2\nNotion of size\nNeural networks provide a framework to parametrize functions. Ultimately, our goal is to find\na neural network that fits some underlying input-output relation. As mentioned above, the ar-\nchitecture (depth, width and activation function) is typically chosen apriori and considered fixed.\nDuring training of the neural network, its parameters (weights and biases) are suitably adapted by\nsome algorithm. Depending on the application, on top of the stated architecture choices, further\nrestrictions on the weights and biases can be desirable. For example, the following two appear\nfrequently:\n• weight sharing: This is a technique where specific entries of the weight matrices (or bias\nvectors) are constrained to be equal. Formally, this means imposing conditions of the form\nW (i)\nk,l = W (j)\ns,t , i.e. the entry (k, l) of the ith weight matrix is equal to the entry at position\n(s, t) of weight matrix j. We denote this assumption by (i, k, l) ∼(j, s, t), paying tribute\nto the trivial fact that “∼” is an equivalence relation. During training, shared weights are\nupdated jointly, meaning that any change to one weight is simultaneously applied to all other\nweights of this class. Weight sharing can also be applied to the entries of bias vectors.\n• sparsity: This refers to imposing a sparsity structure on the weight matrices (or bias vectors).\nSpecifically, we apriorily set W (i)\nk,l = 0 for certain (k, l, i), i.e. we impose entry (k, l) of the ith\nweight matrix to be 0. These zero-valued entries are considered fixed, and are not adjusted\nduring training. The condition W (i)\nk,l = 0 corresponds to node l of layer i−1 not serving as an\ninput to node k in layer i. If we represent the neural network as a graph, this is indicated by\nnot connecting the corresponding nodes. Sparsity can also be imposed on the bias vectors.\nBoth of these restrictions decrease the number of learnable parameters in the neural network. The\nnumber of parameters can be seen as a measure of the complexity of the represented function class.\nFor this reason, we introduce size(Φ) as a notion for the number of learnable parameters. Formally\n(with |S| denoting the cardinality of a set S):\nDefinition 2.4. Let Φ be as in Definition 2.1. Then the size of Φ is\nsize(Φ) :=\n\f\f\f\n\u0010\n{(i, k, l) | W (i)\nk,l ̸= 0} ∪{(i, k) | b(i)\nk ̸= 0}\n\u0011 \u000e\n∼\n\f\f\f .\n(2.2.1)\n21\n2.3\nActivation functions\nActivation functions are a crucial part of neural networks, as they introduce nonlinearity into the\nmodel. If an affine activation function were used, the resulting neural network function would also\nbe affine and hence very restricted in what it can represent.\nThe choice of activation function can have a significant impact on the performance, but there\ndoes not seem to be a universally optimal one. We next discuss a few important activation functions\nand highlight some common issues associated with them.\n5\n0\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) Sigmoid\n5\n0\n5\n0\n2\n4\n6\n8\nReLU\nSiLU\n(b) ReLU and SiLU\n5\n0\n5\n2\n0\n2\n4\n6\n8\na=0.05\na=0.1\na=0.2\n(c) Leaky ReLU\nFigure 2.2: Different activation functions.\nSigmoid: The sigmoid activation function is given by\nσsig(x) =\n1\n1 + e−x\nfor x ∈R,\nand depicted in Figure 2.2 (a). Its output ranges between zero and one, making it interpretable\nas a probability. The sigmoid is a smooth function, which allows the application of gradient-based\ntraining.\nIt has the disadvantage that its derivative becomes very small if |x| →∞. This can affect\nlearning due to the so-called vanishing gradient problem.\nConsider the simple neural network\nΦn(x) = σ ◦· · · ◦σ(x + b) defined with n ∈N compositions of σ, and where b ∈R is a bias. Its\nderivative with respect to b is\nd\ndbΦn(x) = σ′(Φn−1(x)) d\ndbΦn−1(x).\nIf supx∈R |σ′(x)| ≤1 −δ, then by induction, | d\ndbΦn(x)| ≤(1 −δ)n. The opposite effect happens\nfor activation functions with derivatives uniformly larger than one.\nThis argument shows that\nthe derivative of Φn(x, b) with respect to b can become exponentially small or exponentially large\nwhen propagated through the layers. This effect, known as the vanishing- or exploding gradient\neffect, also occurs for activation functions which do not admit the uniform bounds assumed above.\nHowever, since the sigmoid activation function exhibits areas with extremely small gradients, the\nvanishing gradient effect can be strongly exacerbated.\nReLU (Rectified Linear Unit): The ReLU is defined as\nσReLU(x) = max{x, 0}\nfor x ∈R,\n22\nand depicted in Figure 2.2 (b). It is piecewise linear, and due to its simplicity its evaluation is\ncomputationally very efficient. It is one of the most popular activation functions in practice. Since\nits derivative is always zero or one, it does not suffer from the vanishing gradient problem to the\nsame extent as the sigmoid function. However, ReLU can suffer from the so-called dead neurons\nproblem. Consider the neural network\nΦ(x) = σReLU(b −σReLU(x))\nfor x ∈R\ndepending on the bias b ∈R. If b < 0, then Φ(x) = 0 for all x ∈R. The neuron corresponding to\nthe second application of σReLU thus produces a constant signal. Moreover, if b < 0,\nd\ndbΦ(x) = 0\nfor all x ∈R. As a result, every negative value of b yields a stationary point of the empirical risk.\nA gradient-based method will not be able to further train the parameter b. We thus refer to this\nneuron as a dead neuron.\nSiLU (Sigmoid Linear Unit): An important difference between the ReLU and the Sigmoid is\nthat the ReLU is not differentiable at 0. The SiLU activation function (also referred to as “swish”)\ncan be interpreted as a smooth approximation to the ReLU. It is defined as\nσSiLU(x) := xσsig(x) =\nx\n1 + e−x\nfor x ∈R,\nand is depicted in Figure 2.2 (b).\nThere exist various other smooth activation functions that\nmimic the ReLU, including the Softplus x 7→log(1 + exp(x)), the GELU (Gaussian Error Linear\nUnit) x 7→xF(x) where F(x) denotes the cumulative distribution function of the standard normal\ndistribution, and the Mish x 7→x tanh(log(1 + exp(x))).\nParametric ReLU or Leaky ReLU: This variant of the ReLU addresses the dead neuron\nproblem. For some a ∈(0, 1), the parametric ReLU is defined as\nσa(x) = max{x, ax}\nfor x ∈R,\nand is depicted in Figure 2.2 (c) for three different values of a. Since the output of σ does not\nhave flat regions like the ReLU, the dying ReLU problem is mitigated. If a is not chosen too small,\nthen there is less of a vanishing gradient problem than for the Sigmoid. In practice, the additional\nparameter a has to be fine-tuned depending on the application. Like the ReLU, the parametric\nReLU is not differentiable at 0.\nBibliography and further reading\nThe concept of neural networks was first introduced by McCulloch and Pitts in [142].\nLater\nRosenblatt [192] introduced the perceptron, an artificial neuron with adjustable weights that forms\nthe basis of the multilayer perceptron (a fully connected feedforward neural network). The vanishing\ngradient problem shortly addressed in Section 2.3 was discussed by Hochreiter in his diploma thesis\n[91] and later in [17, 93].\n23\nExercises\nExercise 2.5. Prove Proposition 2.3.\nExercise 2.6. In this exercise, we show that ReLU and parametric ReLU create similar sets of\nneural network functions. Fix a > 0.\n(i) Find a set of weight matrices and biases vectors, such that the associated neural network Φ1,\nwith the ReLU activation function σReLU satisfies Φ1(x) = σa(x) for all x ∈R.\n(ii) Find a set of weight matrices and biases vectors, such that the associated neural network Φ2,\nwith the parametric ReLU activation function σa satisfies Φ2(x) = σReLU(x) for all x ∈R.\n(iii) Conclude that every ReLU neural network can be expressed as a leaky ReLU neural network\nand vice versa.\nExercise 2.7. Let d ∈N, and let Φ1 be a neural network with the ReLU as activation function,\ninput dimension d, and output dimension 1. Moreover, let Φ2 be a neural network with the sigmoid\nactivation function, input dimension d, and output dimension 1. Show that, if Φ1 = Φ2, then Φ1 is\na constant function.\nExercise 2.8. In this exercise, we show that for the sigmoid activation functions, dead-neuron-like\nbehavior is very rare. Let Φ be a neural network with the sigmoid activation function. Assume\nthat Φ is a constant function. Show that for every ε > 0 there is a non-constant neural network eΦ\nwith the same architecture as Φ such that for all ℓ= 0, . . . L,\n∥W (ℓ) −f\nW\n(ℓ)∥≤ε and ∥b(ℓ) −eb\n(ℓ)∥≤ε\nwhere W (ℓ), b(ℓ) are the weights and biases of Φ and f\nW\n(ℓ), eb\n(ℓ) are the biases of eΦ.\nShow that such a statement does not hold for ReLU neural networks. What about leaky ReLU?\n24\nChapter 3\nUniversal approximation\nAfter introducing neural networks in Chapter 2, it is natural to inquire about their capabilities.\nSpecifically, we might wonder if there exist inherent limitations to the type of functions a neural\nnetwork can represent. Could there be a class of functions that neural networks cannot approx-\nimate? If so, it would suggest that neural networks are specialized tools, similar to how linear\nregression is suited for linear relationships, but not for data with nonlinear relationships.\nIn this chapter, we will show that this is not the case, and neural networks are indeed a universal\ntool.\nMore precisely, given sufficiently large and complex architectures, they can approximate\nalmost every sensible input-output relationship.\nWe will formalize and prove this claim in the\nsubsequent sections.\n3.1\nA universal approximation theorem\nTo analyze what kind of functions can be approximated with neural networks, we start by consid-\nering the uniform approximation of continuous functions f : Rd →R on compact sets. To this end,\nwe first introduce the notion of compact convergence.\nDefinition 3.1. Let d ∈N.\nA sequence of functions fn : Rd →R, n ∈N, is said to con-\nverge compactly to a function f : Rd →R, if for every compact K ⊆Rd it holds that\nlimn→∞supx∈K |fn(x) −f(x)| = 0. In this case we write fn\ncc\n−→f.\nThroughout what follows, we always consider C0(Rd) equipped with the topology of Defini-\ntion 3.1 (also see Exercise 3.22), and every subset such as C0(D) with the subspace topology:\nfor example, if D ⊆Rd is bounded, then convergence in C0(D) refers to uniform convergence\nlimn→∞supx∈D |fn(x) −f(x)| = 0.\n3.1.1\nUniversal approximators\nAs stated before, we want to show that deep neural networks can approximate every continuous\nfunction in the sense of Definition 3.1. We call sets of functions that satisfy this property universal\napproximators.\n25\nDefinition 3.2. Let d ∈N. A set of functions H from Rd to R is a universal approximator (of\nC0(Rd)), if for every ε > 0, every compact K ⊆Rd, and every f ∈C0(Rd), there exists g ∈H such\nthat supx∈K |f(x) −g(x)| < ε.\nFor a set of (not necessarily continuous) functions H mapping between Rd and R, we denote by\nH\ncc its closure with respect to compact convergence.\nThe relationship between a universal approximator and the closure with respect to compact\nconvergence is established in the proposition below.\nProposition 3.3. Let d ∈N and H be a set of functions from Rd to R. Then, H is a universal\napproximator of C0(Rd) if and only if C0(Rd) ⊆H\ncc.\nProof. Suppose that H is a universal approximator and fix f ∈C0(Rd). For n ∈N, define Kn :=\n[−n, n]d ⊆Rd. Then for every n ∈N there exists fn ∈H such that supx∈Kn |fn(x) −f(x)| < 1/n.\nSince for every compact K ⊆Rd there exists n0 such that K ⊆Kn for all n ≥n0, it holds fn\ncc\n−→f.\nThe “only if” part of the assertion is trivial.\nA key tool to show that a set is a universal approximator is the Stone-Weierstrass theorem, see\nfor instance [196, Sec. 5.7].\nTheorem 3.4 (Stone-Weierstrass). Let d ∈N, let K ⊆Rd be compact, and let H ⊆C0(K, R)\nsatisfy that\n(a) for all x ∈K there exists f ∈H such that f(x) ̸= 0,\n(b) for all x ̸= y ∈K there exists f ∈H such that f(x) ̸= f(y),\n(c) H is an algebra of functions, i.e., H is closed under addition, multiplication and scalar mul-\ntiplication.\nThen H is dense in C0(K).\nExample 3.5 (Polynomials are dense in C0(Rd)). For a multiindex α = (α1, . . . , αd) ∈Nd\n0 and a\nvector x = (x1, . . . , xd) ∈Rd denote xα := Qd\nj=1 xαj\nj . In the following, with |α| := Pd\nj=1 αj, we\nwrite\nPn := span{xα | α ∈Nd\n0, |α| ≤n}\ni.e., Pn is the space of polynomials of degree at most n (with real coefficients). It is easy to check\nthat P := S\nn∈N Pn(Rd) satisfies the assumptions of Theorem 3.4 on every compact set K ⊆Rd.\nThus the space of polynomials P is a universal approximator of C0(Rd), and by Proposition 3.3,\nP is dense in C0(Rd). In case we wish to emphasize the dimension of the underlying space, in the\nfollowing we will also write Pn(Rd) or P(Rd) to denote Pn, P respectively.\n26\n3.1.2\nShallow neural networks\nWith the necessary formalism established, we can now show that shallow neural networks of ar-\nbitrary width form a universal approximator under certain (mild) conditions on the activation\nfunction. The results in this section are based on [132], and for the proofs we follow the arguments\nin that paper.\nWe first introduce notation for the set of all functions realized by certain architectures.\nDefinition 3.6. Let d, m, L, n ∈N and σ: R →R. The set of all functions realized by neural\nnetworks with d-dimensional input, m-dimensional output, depth at most L, width at most n, and\nactivation function σ is denoted by\nN m\nd (σ; L, n) := {Φ : Rd →Rm | Φ as in Def. 2.1, depth(Φ) ≤L, width(Φ) ≤n}.\nFurthermore,\nN m\nd (σ; L) :=\n[\nn∈N\nN m\nd (σ; L, n).\nIn the sequel, we require the activation function σ to belong to the set of piecewise continuous\nand locally bounded functions\nM :=\n\b\nσ ∈L∞\nloc(R)\n\f\f there exist intervals I1, . . . , IM partitioning R,\ns.t. σ ∈C0(Ij) for all j = 1, . . . , M\n\t\n.\n(3.1.1)\nHere, M ∈N is finite, and the intervals Ij are understood to have positive (possibly infinite)\nLebesgue measure, i.e. Ij is e.g. not allowed to be empty or a single point. Hence, σ is a piecewise\ncontinuous function, and it has discontinuities at most finitely many points.\nExample 3.7. Activation functions belonging to M include, in particular, all continuous non-\npolynomial functions, which in turn includes all practically relevant activation functions such as\nthe ReLU, the SiLU, and the Sigmoid discussed in Section 2.3. In these cases, we can choose M = 1\nand I1 = R. Discontinuous functions include for example the Heaviside function x 7→1x>0 (also\ncalled a “perceptron” in this context) but also x 7→1x>0 sin(1/x): Both belong to M with M = 2,\nI1 = (−∞, 0] and I2 = (0, ∞). We exclude for example the function x 7→1/x, which is not locally\nbounded.\nThe rest of this subsection is dedicated to proving the following theorem that has now already\nbeen anounced repeatedly.\nTheorem 3.8. Let d ∈N and σ ∈M. Then N 1\nd (σ; 1) is a universal approximator of C0(Rd) if\nand only if σ is not a polynomial.\n27\nRemark 3.9. We will see in Exercise 3.26 and Corollary 3.18 that neural networks can also arbitrarily\nwell approximate non-continuous functions with respect to suitable norms.\nThe universal approximation theorem by Leshno, Lin, Pinkus and Schocken [132]—of which\nTheorem 3.8 is a special case—is even formulated for a much larger set M, which allows for\nactivation functions that have discontinuities at a (possibly non-finite) set of Lebesgue measure\nzero. Instead of proving the theorem in this generality, we resort to the simpler case stated above.\nThis allows to avoid some technicalities, but the main ideas remain the same. The proof strategy\nis to verify the following three claims:\n(i) if C0(R1) ⊆N 1\n1 (σ; 1)\ncc then C0(Rd) ⊆N 1\nd (σ; 1)\ncc,\n(ii) if σ ∈C∞(R) is not a polynomial then C0(R1) ⊆N 1\n1 (σ; 1)\ncc,\n(iii) if σ ∈M is not a polynomial then there exists ˜σ ∈C∞(R) ∩N 1\n1 (σ; 1)\ncc which is not a\npolynomial.\nUpon observing that ˜σ ∈N 1\n1 (σ; 1)\ncc implies N 1\n1 (˜σ, 1)\ncc ⊆N 1\n1 (σ; 1)\ncc, it is easy to see that these\nstatements together with Proposition 3.3 establish the implication “⇐” asserted in Theorem 3.8.\nThe reverse direction is straightforward to check and will be the content of Exercise 3.23.\nWe start with a more general version of (i) and reduce the problem to the one dimensional case.\nLemma 3.10. Assume that H is a universal approximator of C0(R). Then for every d ∈N\nspan{x 7→g(w · x) | w ∈Rd, g ∈H}\nis a universal approximator of C0(Rd).\nProof. For k ∈N0, denote by Hk the space of all k-homogenous polynomials, that is\nHk := span\nn\nRd ∋x 7→xα \f\f\f α ∈Nd\n0, |α| = k\no\n.\nWe claim that\nHk ⊆span{Rd ∋x 7→g(w · x) | w ∈Rd, g ∈H}\ncc =: X\n(3.1.2)\nfor all k ∈N0. This implies that all multivariate polynomials belong to X. An application of the\nStone-Weierstrass theorem (cp. Example 3.5) and Proposition 3.3 then conclude the proof.\nFor every α, β ∈Nd\n0 with |α| = |β| = k, it holds Dβxα = δβ,αα!, where α! := Qd\nj=1 αj! and\nδβ,α = 1 if β = α and δβ,α = 0 otherwise. Hence, since {x 7→xα | |α| = k} is a basis of Hk, the\nset {Dα | |α| = k} is a basis of its topological dual H′\nk. Thus each linear functional l ∈H′\nk allows\nthe representation l = p(D) for some p ∈Hk (here D stands for the differential).\nBy the multinomial formula\n(w · x)k =\n\n\nd\nX\nj=1\nwjxj\n\n\nk\n=\nX\n{α∈Nd\n0 | |α|=k}\nk!\nα!wαxα.\n28\nTherefore, we have that (x 7→(w · x)k) ∈Hk. Moreover, for every l = p(D) ∈H′\nk and all w ∈Rd\nwe have that\nl(x 7→(w · x)k) = k!p(w).\nHence, if l(x 7→(w · x)k) = p(D)(x 7→(w · x)k) = 0 for all w ∈Rd, then p ≡0 and thus l ≡0.\nThis implies span{x 7→(w · x)k | w ∈Rd} = Hk. Indeed, if there exists h ∈Hk which is not\nin span{x 7→(w · x)k | w ∈Rd}, then by the theorem of Hahn-Banach (see Theorem B.8), there\nexists a non-zero functional in H′\nk vanishing on span{x 7→(w · x)k | w ∈Rd}. This contradicts the\nprevious observation.\nBy the universality of H it is not hard to see that x 7→(w · x)k ∈X for all w ∈Rd. Therefore,\nwe have Hk ⊆X for all k ∈N0.\nBy the above lemma, in order to verify that N 1\nd (σ; 1) is a universal approximator, it suffices to\nshow that N 1\n1 (σ; 1) is a universal approximator. We first show that this is the case for sigmoidal\nactivations.\nDefinition 3.11. An activation function σ : R →R is called sigmoidal, if σ ∈C0(R),\nlimx→∞σ(x) = 1 and limx→−∞σ(x) = 0.\nFor sigmoidal activation functions we can now conclude the universality in the univariate case.\nLemma 3.12. Let σ : R →R be monotonically increasing and sigmoidal. Then C0(R) ⊆N 1\n1 (σ; 1)\ncc.\nWe prove Lemma 3.12 in Exercise 3.24. Lemma 3.10 and Lemma 3.12 show Theorem 3.8 in\nthe special case where σ is monotonically increasing and sigmoidal. For the general case, let us\ncontinue with (ii) and consider C∞activations.\nLemma 3.13. If σ ∈C∞(R) and σ is not a polynomial, then N 1\n1 (σ; 1) is dense in C0(R).\nProof. Denote X := N 1\n1 (σ; 1)\ncc. We show again that all polynomials belong to X. An application\nof the Stone-Weierstrass theorem then gives the statement.\nFix b ∈R and denote fx(w) := σ(wx + b) for all x, w ∈R. By Taylor’s theorem, for h ̸= 0\nσ((w + h)x + b) −σ(wx + b)\nh\n= fx(w + h) −fx(w)\nh\n= f′\nx(w) + h\n2f′′\nx(ξ)\n= f′\nx(w) + h\n2x2σ′′(ξx + b)\n(3.1.3)\n29\nfor some ξ = ξ(h) between w and w + h. Note that the left-hand side belongs to N 1\n1 (σ; 1) as a\nfunction of x. Since σ′′ ∈C0(R), for every compact set K ⊆R\nsup\nx∈K\nsup\n|h|≤1\n|x2σ′′(ξ(h)x + b)| ≤sup\nx∈K\nsup\nη∈[w−1,w+1]\n|x2σ′′(ηx + b)| < ∞.\nLetting h →0, as a function of x the term in (3.1.3) thus converges uniformly towards K ∋\nx 7→f′\nx(w).\nSince K was arbitrary, x 7→f′\nx(w) belongs to X.\nInductively applying the same\nargument to f(k−1)\nx\n(w), we find that x 7→f(k)\nx (w) belongs to X for all k ∈N, w ∈R. Observe that\nf(k)\nx (w) = xkσ(k)(wx+b). Since σ is not a polynomial, for each k ∈N there exists bk ∈R such that\nσ(k)(bk) ̸= 0. Choosing w = 0, we obtain that x 7→xk belongs to X.\nFinally, we come to the proof of (iii)—the claim that there exists at least one non-polynomial\nC∞(R) function in the closure of N 1\n1 (σ; 1). The argument is split into two lemmata. Denote in the\nfollowing by C∞\nc (R) the set of compactly supported C∞(R) functions.\nLemma 3.14. Let σ ∈M. Then for each φ ∈C∞\nc (R) it holds σ ∗φ ∈N 1\n1 (σ; 1)\ncc.\nProof. Fix φ ∈C∞\nc (R) and let a > 0 such that supp φ ⊆[−a, a]. We have\nσ ∗φ(x) =\nZ\nR\nσ(x −y)φ(y) dy.\nDenote yj := −a + 2aj/n for j = 0, . . . , n and define for x ∈R\nfn(x) := 2a\nn\nn−1\nX\nj=0\nσ(x −yj)φ(yj).\nClearly, fn ∈N 1\n1 (σ; 1). We will show fn\ncc\n−→σ∗φ as n →∞. To do so we verify uniform convergence\nof fn towards σ ∗φ on the interval [−b, b] with b > 0 arbitrary but fixed.\nFor x ∈[−b, b]\n|σ ∗φ(x) −fn(x)| ≤\nn−1\nX\nj=0\n\f\f\f\f\f\nZ yj+1\nyj\nσ(x −y)φ(y) −σ(x −yj)φ(yj) dy\n\f\f\f\f\f .\n(3.1.4)\nFix ε ∈(0, 1). Since σ ∈M, there exist z1, . . . , zM ∈R such that σ is continuous on R\\{z1, . . . , zM}\n(cp. (3.1.1)). With Dε := SM\nj=1(zj−ε, zj+ε), observe that σ is uniformly continuous on the compact\nset Kε := [−a −b, a + b] ∩Dc\nε. Now let Jc ∪Jd = {0, . . . , n −1} be a partition (depending on x),\nsuch that j ∈Jc if and only if [x −yj+1, x −yj] ⊆Kε. Hence, j ∈Jd implies the existence of\ni ∈{1, . . . , M} such that the distance of zi to [x −yj+1, x −yj] is at most ε. Due to the interval\n30\n[x −yj+1, x −yj] having length 2a/n, we can bound\nX\nj∈Jd\nyj+1 −yj =\n\f\f\f\f\f\f\n[\nj∈Jd\n[x −yj+1, x −yj]\n\f\f\f\f\f\f\n≤\n\f\f\f\f\f\nM\n[\ni=1\nh\nzi −ε −2a\nn , zi + ε + 2a\nn\ni\f\f\f\f\f\n≤M ·\n\u0010\n2ε + 4a\nn\n\u0011\n.\nNext, because of the local boundedness of σ and the fact that φ ∈C∞\nc , it holds sup|y|≤a+b |σ(y)| +\nsup|y|≤a |φ(y)| =: γ < ∞. Hence\n|σ ∗φ(x) −fn(x)|\n≤\nX\nj∈Jc∪Jd\n\f\f\f\f\f\nZ yj+1\nyj\nσ(x −y)φ(y) −σ(x −yj)φ(yj) dy\n\f\f\f\f\f\n≤2γ2M ·\n\u0012\n2ε + 4a\nn\n\u0013\n+ 2a sup\nj∈Jc\nmax\ny∈[yj,yj+1] |σ(x −y)φ(y) −σ(x −yj)φ(yj)|.\n(3.1.5)\nWe can bound the term in the last maximum by\n|σ(x −y)φ(y) −σ(x −yj)φ(yj)|\n≤|σ(x −y) −σ(x −yj)||φ(y)| + |σ(x −yj)||φ(y) −φ(yj)|\n≤γ ·\n\n\n\n\nsup\nz1,z2∈Kε\n|z1−z2|≤2a\nn\n|σ(z1) −σ(z2)| +\nsup\nz1,z2∈[−a,a]\n|z1−z2|≤2a\nn\n|φ(z1) −φ(z2)|\n\n\n\n.\nFinally, uniform continuity of σ on Kε and φ on [−a, a] imply that the last term tends to 0 as\nn →∞uniformly for all x ∈[−b, b]. This shows that there exist C < ∞(independent of ε and x)\nand nε ∈N (independent of x) such that the term in (3.1.5) is bounded by Cε for all n ≥nε. Since\nε was arbitrary, this yields the claim.\nLemma 3.15. If σ ∈M and σ ∗φ is a polynomial for all φ ∈C∞\nc (R), then σ is a polynomial.\nProof. Fix −∞< a < b < ∞and consider C∞\nc (a, b) := {φ ∈C∞(R) | supp φ ⊆[a, b]}. Define a\nmetric ρ on C∞\nc (a, b) via\nρ(φ, ψ) :=\nX\nj∈N0\n2−j\n|φ −ψ|Cj(a,b)\n1 + |φ −ψ|Cj(a,b)\n,\n31\nwhere\n|φ|Cj(a,b) := sup\nx∈[a,b]\n|φ(j)(x)|.\nSince the space of j times differentiable functions on [a, b] is complete with respect to the norm\nPj\ni=0 | · |Ci(a,b), see for instance [89, Satz 104.3], the space C∞\nc (a, b) is complete with the metric ρ.\nFor k ∈N set\nVk := {φ ∈C∞\nc (a, b) | σ ∗φ ∈Pk},\nwhere Pk := span{R ∋x 7→xj | 0 ≤j ≤k} denotes the space of polynomials of degree at most\nk. Then Vk is closed with respect to the metric ρ. To see this, we only need to observe that for\na converging sequence φj →φ∗with respect to ρ and φj ∈Vk, it follows that Dk+1(σ ∗φ∗) = 0\nand hence σ ∗φ∗is a polynomial. Since Dk+1(σ ∗φj) = 0 we compute with the linearity of the\nconvolution and the fact that Dk+1(f ∗g) = f ∗Dk+1(g) for differentiable g and if both sides are\nwell-defined that\nsup\nx∈[a,b]\n|Dk+1(σ ∗φ∗)(x)|\n= sup\nx∈[a,b]\n|σ ∗Dk+1(φ∗−φj)(x)|\n≤|b −a|\nsup\nz∈[a−b,b−a]\n|σ(z)| · sup\nx∈[a,b]\n|Dk+1(φj −φ∗)(x)|\nand since σ is locally bounded, the right hand-side converges to 0.\nBy assumption we have\n[\nk∈N\nVk = C∞\nc (a, b).\nBaire’s category theorem implies the existence of k0 ∈N (depending on a, b) such that Vk0 contains\nan open subset of C∞\nc (a, b). Since Vk0 is a vector space, it must hold Vk0 = C∞\nc (a, b).\nWe now show that φ∗σ ∈Pk0 for every φ ∈C∞\nc (R); in other words, k0 = k0(a, b) can be chosen\nindependent of a and b. First consider a shift s ∈R and let ˜a := a + s and ˜b := b + s. Then with\nS(x) := x + s, for any φ ∈C∞\nc (˜a,˜b) holds φ ◦S ∈C∞\nc (a, b), and thus (φ ◦S) ∗σ ∈Pk0. Since\n(φ◦S)∗σ(x) = φ∗σ(x+s), we conclude that φ∗σ ∈Pk0. Next let −∞< ˜a < ˜b < ∞be arbitrary.\nThen, for an integer n > (˜b −˜a)(b −a) we can cover (˜a,˜b) with n ∈N overlapping open intervals\n(a1, b1), . . . , (an, bn), each of length b −a. Any φ ∈C∞\nc (˜a,˜b) can be written as φ = Pn\nj=1 φj where\nφj ∈C∞\nc (aj, bj). Then φ ∗σ = Pn\nj=1 φj ∗σ ∈Pk0, and thus φ ∗σ ∈Pk0 for every φ ∈C∞\nc (R).\nFinally, Exercise 3.25 implies σ ∈Pk0.\nNow we can put everything together to show Theorem 3.8.\nof Theorem 3.8. By Exercise 3.23 we have the implication “⇒”.\nFor the other direction we assume that σ ∈M is not a polynomial. Then by Lemma 3.15\nthere exists φ ∈C∞\nc (R) such that σ ∗φ is not a polynomial. According to Lemma 3.14 we have\nσ ∗φ ∈N 1\n1 (σ; 1)\ncc. We conclude with Lemma 3.13 that N 1\n1 (σ; 1) is a universal approximator of\nC0(R).\nFinally, by Lemma 3.10, N 1\nd (σ; 1) is a universal approximator of C0(Rd).\n32\n3.1.3\nDeep neural networks\nTheorem 3.8 shows the universal approximation capability of single-hidden-layer neural networks\nwith activation functions σ ∈M\\P: they can approximate every continuous function on every\ncompact set to arbitrary precision, given sufficient width. This result directly extends to neural\nnetworks of any fixed depth L ≥1. The idea is to use the fact that the identity function can be\napproximated with a shallow neural network. Composing a shallow neural network approximation of\nthe target function f with (multiple) shallow neural networks approximating the identity function,\ngives a deep neural network approximation of f.\nInstead of directly applying Theorem 3.8, we first establish the following proposition regarding\nthe approximation of the identity function. Rather than σ ∈M\\P, it requires a different (mild)\nassumption on the activation function. This allows for a constructive proof, yielding explicit bounds\non the neural network size, which will prove useful later in the book.\nProposition 3.16. Let d, L ∈N, let K ⊆Rd be compact, and let σ : R →R be such that there\nexists an open set on which σ is differentiable and not constant. Then, for every ε > 0, there exists\na neural network Φ ∈N d\nd (σ; L, d) such that\n∥Φ(x) −x∥∞< ε\nfor all x ∈K.\nProof. The proof uses the same idea as in Lemma 3.13, where we approximate the derivative of the\nactivation function by a simple neural network. Let us first assume d ∈N and L = 1.\nLet x∗∈R be such that σ is differentiable on a neighborhood of x∗and σ′(x∗) = θ ̸= 0.\nMoreover, let x∗= (x∗, . . . , x∗) ∈Rd. Then, for λ > 0 we define\nΦλ(x) := λ\nθ σ\n\u0010x\nλ + x∗\u0011\n−λ\nθ σ(x∗),\nThen, we have, for all x ∈K,\nΦλ(x) −x = λσ(x/λ + x∗) −σ(x∗)\nθ\n−x.\n(3.1.6)\nIf xi = 0 for i ∈{1, . . . , d}, then (3.1.6) shows that (Φλ(x) −x)i = 0. Otherwise\n|(Φλ(x) −x)i| = |xi|\n|θ|\n\f\f\f\f\nσ(xi/λ + x∗) −σ(x∗)\nxi/λ\n−θ\n\f\f\f\f .\nBy the definition of the derivative, we have that |(Φλ(x) −x)i| →0 for λ →∞uniformly for all\nx ∈K and i ∈{1, . . . , d}. Therefore, |Φλ(x) −x| →0 for λ →∞uniformly for all x ∈K.\nThe extension to L > 1 is straight forward and is the content of Exercise 3.27.\nUsing the aforementioned generalization of Proposition 3.16 to arbitrary non-polynomial acti-\nvation functions σ ∈M, we obtain the following extension of Theorem 3.8.\n33\nCorollary 3.17. Let d ∈N, L ∈N and σ ∈M. Then N 1\nd (σ; L) is a universal approximator of\nC0(Rd) if and only if σ is not a polynomial.\nProof. We only show the implication “⇐”. The other direction is again left as an exercise, see\nExercise 3.23.\nAssume σ ∈M is not a polynomial, let K ⊆Rd be compact, and let f ∈C0(Rd). Fix ε ∈(0, 1).\nWe need to show that there exists a neural network Φ ∈N 1\nd (σ; L) such that supx∈K |f(x)−Φ(x)| <\nε. The case L = 1 holds by Theorem 3.8, so let L > 1.\nBy Theorem 3.8, there exist Φshallow ∈N 1\nd (σ; 1) such that\nsup\nx∈K\n|f(x) −Φshallow(x)| < ε\n2.\n(3.1.7)\nCompactness of {f(x) | x ∈K} implies that we can find n > 0 such that\n{Φshallow(x) | x ∈K} ⊆[−n, n].\n(3.1.8)\nLet Φid ∈N 1\n1 (σ; L −1) be an approximation to the identity such that\nsup\nx∈[−n,n]\n|x −Φid(x)| < ε\n2,\n(3.1.9)\nwhich is possibly by the extension of Proposition 3.16 to general non-polynomial activation functions\nσ ∈M.\nDenote Φ := Φid ◦Φshallow. According to Proposition 2.3 (iv) holds Φ ∈N 1\nd (σ; L) as desired.\nMoreover (3.1.7), (3.1.8), (3.1.9) imply\nsup\nx∈K\n|f(x) −Φ(x)| = sup\nx∈K\n|f(x) −Φid(Φshallow(x))|\n≤sup\nx∈K\n\u0000|f(x) −Φshallow(x)| + |Φshallow(x) −Φid(Φshallow(x))|\n\u0001\n≤ε\n2 + ε\n2 = ε.\nThis concludes the proof.\n3.1.4\nOther norms\nAdditional to the continuous functions, universal approximation theorems can be shown for various\nother function classes and topologies, which may also allow for the approximation of functions\nexhibiting discontinuities or singularities. To give but one example, we next state such a result for\nLebesgue spaces on compact sets. The proof is left to the reader, see Exercise 3.26.\nCorollary 3.18. Let d ∈N, L ∈N, p ∈[1, ∞), and let σ ∈M not be a polynomial. Then for\nevery ε > 0, every compact K ⊆Rd, and every f ∈Lp(K) there exists Φf,ε ∈N 1\nd (σ; L) such that\n\u0012Z\nK\n|f(x) −Φ(x)|p dx\n\u00131/p\n≤ε.\n34\n3.2\nSuperexpressive activations and Kolmogorov’s superposition\ntheorem\nIn the previous section, we saw that a large class of activation functions allow for universal approx-\nimation. However, these results did not provide any insights into the necessary neural network size\nfor achieving a specific accuracy.\nBefore exploring this topic further in the following chapters, we next present a remarkable result\nthat shows how the required neural network size is significantly influenced by the choice of activation\nfunction. The result asserts that, with the appropriate activation function, every f ∈C0(K) on a\ncompact set K ⊆Rd can be approximated to every desired accuracy ε > 0 using a neural network\nof size O(d2); in particular the neural network size is independent of ε > 0, K, and f. We will first\ndiscuss the one-dimensional case.\nProposition 3.19. There exists a continuous activation function σ : R →R such that for every\ncompact K ⊆R, every ε > 0 and every f ∈C0(K) there exists Φ(x) = σ(wx + b) ∈N 1\n1 (σ; 1, 1)\nsuch that\nsup\nx∈K\n|f(x) −Φ(x)| < ε.\nProof. Denote by ˜Pn all polynomials p(x) = Pn\nj=0 qjxj with rational coefficients, i.e. such that\nqj ∈Q for all j = 0, . . . , n. Then ˜Pn can be identified with the n-fold cartesian product Q×· · ·×Q,\nand thus ˜Pn is a countable set. Consequently also the set ˜P := S\nn∈N ˜Pn of all polynomials with\nrational coefficients is countable. Let (pi)i∈Z be an enumeration of these polynomials, and set\nσ(x) :=\n(\npi(x −2i)\nif x ∈[2i, 2i + 1]\npi(1)(2i + 2 −x) + pi+1(0)(x −2i −1)\nif x ∈(2i + 1, 2i + 2).\nIn words, σ equals pi on even intervals [2i, 2i + 1] and is linear on odd intervals [2i + 1, 2i + 2],\nresulting in a continuous function overall.\nWe first assume K = [0, 1]. By Example 3.5, for every ε > 0 exists p(x) = Pn\nj=1 rjxj such\nthat supx∈[0,1] |p(x) −f(x)| < ε/2. Now choose qj ∈Q so close to rj such that ˜p(x) := Pn\nj=1 qjxj\nsatisfies supx∈[0,1] |˜p(x) −p(x)| < ε/2. Let i ∈Z such that ˜p(x) = pi(x), i.e., pi(x) = σ(2i + x) for\nall x ∈[0, 1]. Then supx∈[0,1] |f(x) −σ(x + 2i)| < ε.\nFor general compact K assume that K ⊆[a, b]. By Tietze’s extension theorem, f allows a\ncontinuous extension to [a, b], so without loss of generality K = [a, b]. By the first case we can find\ni ∈Z such that with y = (x −a)/(b −a) (i.e. y ∈[0, 1] if x ∈[a, b])\nsup\nx∈[a,b]\n\f\f\f\ff(x) −σ\n\u0012x −a\nb −a + 2i\n\u0013\f\f\f\f = sup\ny∈[0,1]\n|f(y · (b −a) + a) −σ(y + 2i)| < ε,\nwhich gives the statement with w = 1/(b −a) and b = −a · (b −a) + 2i.\nTo extend this result to arbitrary dimension, we will use Kolmogorov’s superposition theorem.\nIt states that every continuous function of d variables can be expressed as a composition of functions\nthat each depend only on one variable. We omit the technical proof, which can be found in [120].\n35\nTheorem 3.20 (Kolmogorov). For every d ∈N there exist 2d2 + d monotonically increasing\nfunctions φi,j ∈C0(R), i = 1, . . . , d, j = 1, . . . , 2d + 1, such that for every f ∈C0([0, 1]d) there\nexist functions fj ∈C0(R), j = 1, . . . , 2d + 1 satisfying\nf(x) =\n2d+1\nX\nj=1\nfj\n d\nX\ni=1\nφi,j(xi)\n!\nfor all x ∈[0, 1]d.\nCorollary 3.21. Let d ∈N. With the activation function σ : R →R from Proposition 3.19, for\nevery compact K ⊆Rd, every ε > 0 and every f ∈C0(K) there exists Φ ∈N 1\nd (σ; 2, 2d2 + d) (i.e.\nwidth(Φ) = 2d2 + d and depth(Φ) = 2) such that\nsup\nx∈K\n|f(x) −Φ(x)| < ε.\nProof. Without loss of generality we can assume K = [0, 1]d: the extension to the general case then\nfollows by Tietze’s extension theorem and a scaling argument as in the proof of Proposition 3.19.\nLet fj, φi,j, i = 1, . . . , d, j = 1, . . . , 2d + 1 be as in Theorem 3.20. Fix ε > 0. Let a > 0 be so\nlarge that\nsup\ni,j\nsup\nx∈[0,1]\n|φi,j(x)| ≤a.\nSince each fj is uniformly continuous on the compact set [−da, da], we can find δ > 0 such that\nsup\nj\nsup\n|y−˜y|<δ\n|y|,|˜y|≤da\n|fj(y) −fj(˜y)| <\nε\n2(2d + 1).\n(3.2.1)\nBy Proposition 3.19 there exist wi,j, bi,j ∈R such that\nsup\ni,j\nsup\nx∈[0,1]\n|φi,j(x) −σ(wi,jx + bi,j)\n|\n{z\n}\n=: ˜φi,j(x)\n| < δ\nd\n(3.2.2)\nand wj, bj ∈R such that\nsup\nj\nsup\n|y|≤a+δ\n|fj(y) −σ(wjy + bj)\n|\n{z\n}\n=: ˜fj(y)\n| <\nε\n2(2d + 1).\n(3.2.3)\nThen for all x ∈[0, 1]d by (3.2.2)\n\f\f\f\f\f\nd\nX\ni=1\nφi,j(xi) −\nd\nX\ni=1\n˜φi,j(xi)\n\f\f\f\f\f < dδ\nd = δ.\n36\nThus with\nyj :=\nd\nX\nj=1\nφi,j(xi),\n˜yj :=\nd\nX\nj=1\n˜φi,j(xi)\nit holds |yj −˜yj| < δ. Using (3.2.1) and (3.2.3) we conclude\n\f\f\f\f\f\f\nf(x) −\n2d+1\nX\nj=1\nσ\n \nwj ·\n d\nX\ni=1\nσ(wi,jxi + bi,j)\n!\n+ bj\n!\f\f\f\f\f\f\n=\n\f\f\f\f\f\f\n2d+1\nX\nj=1\n(fj(yj) −˜fj(˜yj))\n\f\f\f\f\f\f\n≤\n2d+1\nX\nj=1\n\u0010\n|fj(yj) −fj(˜yj)| + |fj(˜yj) −˜fj(˜yj)|\n\u0011\n≤\n2d+1\nX\nj=1\n\u0012\nε\n2(2d + 1) +\nε\n2(2d + 1)\n\u0013\n≤ε.\nThis concludes the proof.\nKolmogorov’s superposition theorem is intriguing as it shows that approximating d-dimensional\nfunctions can be reduced to the (generally much simpler) one-dimensional case through composi-\ntions. Neural networks, by nature, are well suited to approximate functions with compositional\nstructures. However, the functions fj in Theorem 3.20, even though only one-dimensional, could\nbecome very complex and challenging to approximate themselves if d is large.\nSimilarly, the “magic” activation function in Proposition 3.19 encodes the information of all\nrational polynomials on the unit interval, which is why a neural network of size O(1) suffices to\napproximate every function to arbitrary accuracy. Naturally, no practical algorithm can efficiently\nidentify appropriate neural network weights and biases for this architecture. As such, the results\npresented in Section 3.2 should be taken with a pinch of salt as their practical relevance is highly\nlimited.\nNevertheless, they highlight that while universal approximation is a fundamental and\nimportant property of neural networks, it leaves many aspects unexplored. To gain further insight\ninto practically relevant architectures, in the following chapters, we investigate neural networks\nwith activation functions such as the ReLU.\nBibliography and further reading\nThe foundation of universal approximation theorems goes back to the late 1980s with seminal works\nby Cybenko [44], Hornik et al. [95, 94], Funahashi [63] and Carroll and Dickinson [33]. These results\nwere subsequently extended to a wider range of activation functions and architectures. The present\nanalysis in Section 3.1 closely follows the arguments in [132], where it was essentially shown that\nuniversal approximation can be achieved if the activation function is not polynomial.\nKolmogorov’s superposition theorem stated in Theorem 3.20 was originally proven in 1957\n[120].\nFor a more recent and constructive proof see for instance [26].\nKolmogorov’s theorem\nand its obvious connections to neural networks have inspired various research in this field, e.g.\n[162, 124, 151, 205, 104], with its practical relevance being debated [68, 123]. The idea for the\n“magic” activation function in Section 3.2 comes from [140] where it is shown that such an activation\nfunction can even be chosen monotonically increasing.\n37\nExercises\nExercise 3.22. Write down a generator of a (minimal) topology on C0(Rd) such that fn →f ∈\nC0(Rd) if and only if fn\ncc\n−→f, and show this equivalence. This topology is referred to as the\ntopology of compact convergence.\nExercise 3.23. Show the implication “⇒” of Theorem 3.8 and Corollary 3.17.\nExercise 3.24. Prove Lemma 3.12. Hint: Consider σ(nx) for large n ∈N.\nExercise 3.25. Let k ∈N, σ ∈M and assume that σ ∗φ ∈Pk for all φ ∈C∞\nc (R). Show that\nσ ∈Pk.\nHint: Consider ψ ∈C∞\nc (R) such that ψ ≥0 and\nR\nR ψ(x) dx = 1 and set ψε(x) := ψ(x/ε)/ε.\nUse that away from the discontinuities of σ it holds ψε ∗σ(x) →σ(x) as ε →0. Conclude that σ\nis piecewise in Pk, and finally show that σ ∈Ck(R).\nExercise 3.26. Prove Corollary 3.18 with the use of Corollary 3.17.\nExercise 3.27. Complete the proof of Proposition 3.16 for L > 1.\n38\nChapter 4\nSplines\nIn Chapter 3, we saw that sufficiently large neural networks can approximate every continuous\nfunction to arbitrary accuracy.\nHowever, these results did not further specify the meaning of\n“sufficiently large” or what constitutes a suitable architecture. Ideally, given a function f, and a\ndesired accuracy ε > 0, we would like to have a (possibly sharp) bound on the required size, depth,\nand width guaranteeing the existence of a neural network approximating f up to error ε.\nThe field of approximation theory establishes such trade-offs between properties of the function f\n(e.g., its smoothness), the approximation accuracy, and the number of parameters needed to achieve\nthis accuracy. For example, given k, d ∈N, how many parameters are required to approximate a\nfunction f : [0, 1]d →R with ∥f∥Ck([0,1]d) ≤1 up to uniform error ε? Splines are known to achieve\nthis approximation accuracy with a superposition of O(ε−d/k) simple (piecewise polynomial) basis\nfunctions. In this chapter, following [146], we show that certain sigmoidal neural networks can\nmatch this performance in terms of the neural network size.\nIn fact, from an approximation\ntheoretical viewpoint we show that the considered neural networks are at least as expressive as\nsuperpositions of splines.\n4.1\nB-splines and smooth functions\nWe introduce a simple type of spline and its approximation properties below.\nDefinition 4.1. For n ∈N, the univariate cardinal B-spline order n ∈N is given by\nSn(x) :=\n1\n(n −1)!\nn\nX\nℓ=0\n(−1)ℓ\n\u0012n\nℓ\n\u0013\nσReLU(x −ℓ)n−1\nfor x ∈R,\n(4.1.1)\nwhere 00 := 0 and σReLU denotes the ReLU activation function.\nBy shifting and dilating the cardinal B-spline, we obtain a system of univariate splines. Taking\ntensor products of these univariate splines yields a set of higher-dimensional functions known as\nthe multivariate B-splines.\n39\nDefinition 4.2. For t ∈R and n, ℓ∈N we define Sℓ,t,n := Sn(2ℓ(· −t)). Additionally, for d ∈N,\nt ∈Rd, and n, ℓ∈N, we define the the multivariate B-spline Sd\nℓ,t,n as\nSd\nℓ,t,n(x) :=\nd\nY\ni=1\nSℓ,ti,n(xi)\nfor x = (x1, . . . xd) ∈Rd,\nand\nBn :=\nn\nSd\nℓ,t,n\n\f\f\f ℓ∈N, t ∈Rdo\nis the dictionary of B-splines of order n.\nHaving introduced the system Bn, we would like to understand how well we can represent each\nsmooth function by superpositions of elements of Bn. The following theorem is adapted from the\nmore general result [168, Theorem 7]; also see [141, Theorem D.3] for a presentation closer to the\npresent formulation.\nTheorem 4.3. Let d, n, k ∈N such that 0 < k ≤n. Then there exists C such that for every\nf ∈Ck([0, 1]d) and every N ∈N, there exist ci ∈R with |ci| ≤C∥f∥L∞([0,1]d) and Bi ∈Bn for\ni = 1, . . . , N, such that\n\r\r\r\r\rf −\nN\nX\ni=1\nciBi\n\r\r\r\r\r\nL∞([0,1]d)\n≤CN−k\nd ∥f∥Ck[0,1]d.\nRemark 4.4. There are a couple of critical concepts in Theorem 4.3 that will reappear throughout\nthis book. The number of parameters N determines the approximation accuracy N−k/d. This im-\nplies that achieving accuracy ε > 0 requires O(ε−d/k) parameters (according to this upper bound),\nwhich grows exponentially in d. This exponential dependence on d is referred to as the “curse of\ndimension” and will be discussed again in the subsequent chapters. The smoothness parameter\nk has the opposite effect of d, and improves the convergence rate. Thus, smoother functions can\nbe approximated with fewer B-splines than rougher functions. This more efficient approximation\nrequires the use of B-splines of order n with n ≥k. We will see in the following, that the order of\nthe B-spline is closely linked to the concept of depth in neural networks.\n4.2\nReapproximation of B-splines with sigmoidal activations\nWe now show that the approximation rates of B-splines can be transfered to certain neural networks.\nThe following argument is based on [144].\n40\nDefinition 4.5. A function σ : R →R is called sigmoidal of order q ∈N, if σ ∈Cq−1(R) and\nthere exists C > 0 such that\nσ(x)\nxq\n→0\nas x →−∞,\nσ(x)\nxq\n→1\nas x →∞,\n|σ(x)| ≤C · (1 + |x|)q\nfor all x ∈R.\nExample 4.6. The rectified power unit x 7→σReLU(x)q is sigmoidal of order q.\nOur goal in the following is to show that neural networks can approximate a linear combination\nof N B-splines with a number of parameters that is proportional to N. As an immediate conse-\nquence of Theorem 4.3, we then obtain a convergence rate for neural networks. Let us start by\napproximating a single univariate B-spline with a neural network of fixed size.\nProposition 4.7. Let n ∈N, n ≥2, K > 0, and let σ : R →R be sigmoidal of order q ≥2. There\nexists a constant C > 0 such that for every ε > 0 there is a neural network ΦSn with activation\nfunction σ, ⌈logq(n −1)⌉layers, and size C, such that\n\r\rSn −ΦSn\r\r\nL∞([−K,K]) ≤ε.\nProof. By definition (4.1.1), Sn is a linear combination of n + 1 shifts of σn−1\nReLU.\nWe start by\napproximating σn−1\nReLU. It is not hard to see (Exercise 4.10) that, for every K′ > 0 and every t ∈N\n\f\f\f\f\f\f\na−qt σ ◦σ ◦· · · ◦σ(ax)\n|\n{z\n}\nt−times\n−σReLU(x)qt\n\f\f\f\f\f\f\n→0\nas a →∞\n(4.2.1)\nuniformly for all x ∈[−K′, K′].\nSet t := ⌈logq(n −1)⌉. Then t ≥1 since n ≥2, and qt ≥n −1. Thus, for every K′ > 0 and\nε > 0 there exists a neural network Φqt\nε with ⌈logq(n −1)⌉layers satisfying\n\f\f\fΦqt\nε (x) −σReLU(x)qt\f\f\f ≤ε\nfor all x ∈[−K′, K′].\n(4.2.2)\nThis shows that we can approximate the ReLU to the power of qt ≥n −1. However, our goal is to\nobtain an approximation of the ReLU raised to the power n −1, which could be smaller than qt.\nTo reduce the order, we emulate approximate derivatives of Φqt\nε . Concretely, we show the following\nclaim: For all 1 ≤p ≤qt for every K′ > 0 and ε > 0 there exists a neural network Φp\nε having\n⌈logq(n −1)⌉layers and satisfying\n|Φp\nε(x) −σReLU(x)p| ≤ε\nfor all x ∈[−K′, K′].\n(4.2.3)\n41\nThe claim holds for p = qt. We now proceed by induction over p = qt, qt −1, . . . Assume (4.2.3)\nholds for some p ∈{2, . . . , qt}. Fix δ ≥0. Then\n\f\f\f\f\nΦp\nδ2(x + δ) −Φp\nδ2(x)\npδ\n−σReLU(x)p−1\n\f\f\f\f\n≤2δ\np +\n\f\f\f\f\nσReLU(x + δ)p −σReLU(x)p\npδ\n−σReLU(x)p−1\n\f\f\f\f .\nHence, by the binomial theorem it follows that there exists δ∗> 0 such that\n\f\f\f\f\f\nΦp\nδ2∗(x + δ∗) −Φp\nδ2∗(x)\npδ∗\n−σReLU(x)p−1\n\f\f\f\f\f ≤ε,\nfor all x ∈[−K′, K′].\nBy Proposition 2.3, (Φp\nδ2∗(x + δ∗) −Φp\nδ2∗)/(pδ∗) is a neural network with\n⌈logq(n −1)⌉layers and size independent from ε. Calling this neural network Φp−1\nε\nshows that\n(4.2.3) holds for p −1, which concludes the induction argument and proves the claim.\nFor every neural network Φ, every spatial translation Φ(· −t) is a neural network of the same\narchitecture. Hence, every term in the sum (4.1.1) can be approximated to arbitrary accuracy by\na neural network of a fixed size. Since by Proposition 2.3, sums of neural networks of the same\ndepth are again neural networks of the same depth, the result follows.\nNext, we extend Proposition 4.7 to the multivariate splines Sd\nℓ,t,n for arbitrary ℓ, d ∈N, t ∈Rd.\nProposition 4.8. Let n, d ∈N, n ≥2, K > 0, and let σ : R →R be sigmoidal of order q ≥2.\nFurther let ℓ∈N and t ∈Rd.\nThen, there exists a constant C > 0 such that for every ε > 0 there is a neural network ΦSd\nℓ,t,n\nwith activation function σ, ⌈log2(d)⌉+ ⌈logq(k −1)⌉layers, and size C, such that\n\r\r\rSd\nℓ,t,n −ΦSd\nℓ,t,n\n\r\r\r\nL∞([−K,K]d) ≤ε.\nProof. By definition Sd\nℓ,t,n(x) = Qd\ni=1 Sℓ,ti,n(xi) where\nSℓ,ti,n(xi) = Sn(2ℓ(xi −ti)).\nBy Proposition 4.7 there exist a constant C′ > 0 such that for each i = 1, . . . , d and all ε > 0, there\nis a neural network ΦSℓ,ti,n with size C′ and ⌈logq(n −1)⌉layers such that\n\r\rSℓ,ti,n −ΦSℓ,ti,n\r\r\nL∞([−K,K]d) ≤ε.\nIf d = 1, this shows the statement. For general d, it remains to show that the product of the ΦSℓ,ti,n\nfor i = 1, . . . , d can be approximated.\nWe first prove the following claim by induction: For every d ∈N, d ≥2, there exists a constant\nC′′ > 0, such that for all K′ ≥1 and all ε > 0 there exists a neural network Φmult,ε,d with size\n42\nC′′, ⌈log2(d)⌉layers, and activation function σ such that for all x1, . . . , xd with |xi| ≤K′ for all\ni = 1, . . . , d,\n\f\f\f\f\fΦmult,ε,d(x1, . . . , xd) −\nd\nY\ni=1\nxi\n\f\f\f\f\f < ε.\n(4.2.4)\nFor the base case, let d = 2. Similar to the proof of Proposition 4.7, one can show that there exists\nC′′′ > 0 such that for every ε > 0 and K′ > 0 there exists a neural network Φsquare,ε with one\nhidden layer and size C′′′ such that\n|Φsquare,ε −σReLU(x)2| ≤ε\nfor all |x| ≤K′.\nFor every x = (x1, x2) ∈R2\nx1x2 = 1\n2\n\u0000(x1 + x2)2 −x2\n1 −x2\n2\n\u0001\n= 1\n2\n\u0000σReLU(x1 + x2)2 + σReLU(−x1 −x2)2 −σReLU(x1)2\n−σReLU(−x1)2 −σReLU(x2)2 −σReLU(−x2)2\u0001\n.\n(4.2.5)\nEach term on the right-hand side can be approximated up to uniform error ε/6 with a network of\nsize C′′′ and one hidden layer. By Proposition 2.3, we conclude that there exists a neural network\nΦmult,ε,2 satisfying (4.2.4) for d = 2.\nAssume the induction hypothesis (4.2.4) holds for d −1 ≥1, and let ε > 0 and K′ ≥1. We\nhave\nd\nY\ni=1\nxi =\n⌊d/2⌋\nY\ni=1\nxi ·\nd\nY\ni=⌊d/2⌋+1\nxi.\n(4.2.6)\nWe will now approximate each of the terms in the product on the right-hand side of (4.2.6) by a\nneural network using the induction assumption.\nFor simplicity assume in the following that ⌈log2(⌊d/2⌋)⌉= ⌈log2(d −⌊d/2⌋)⌉. The general\ncase can be addressed via Proposition 3.16. By the induction assumption there then exist neural\nnetworks Φmult,1 and Φmult,2 both with ⌈log2(⌊d/2⌋)⌉layers, such that for all xi with |xi| ≤K′ for\ni = 1, . . . , d\n\f\f\f\f\f\f\nΦmult,1(x1, . . . , x⌊d/2⌋) −\n⌊d/2⌋\nY\ni=1\nxi\n\f\f\f\f\f\f\n<\nε\n4((K′)⌊d/2⌋+ ε),\n\f\f\f\f\f\f\nΦmult,2(x⌊d/2⌋+1, . . . , xd) −\nd\nY\ni=⌊d/2⌋+1\nxi\n\f\f\f\f\f\f\n<\nε\n4((K′)⌊d/2⌋+ ε).\nBy Proposition 2.3, Φmult,ε,d := Φmult,ε/2,2◦(Φmult,1, Φmult,2) is a neural network with 1+⌈log2(⌊d/2⌋)⌉=\n⌈log2(d)⌉layers. By construction, the size of Φmult,ε,d does not depend on K′ or ε. Thus, to complete\nthe induction, it only remains to show (4.2.4).\nFor all a, b, c, d ∈R holds\n|ab −cd| ≤|a||b −d| + |d||a −c|.\n43\nHence, for x1, . . . , xd with |xi| ≤K′ for all i = 1, . . . , d, we have that\n\f\f\f\f\f\nd\nY\ni=1\nxi −Φmult,ε,d(x1, . . . , xd)\n\f\f\f\f\f\n≤ε\n2 +\n\f\f\f\f\f\f\n⌊d/2⌋\nY\ni=1\nxi ·\nd\nY\ni=⌊d/2⌋+1\nxi −Φmult,1(x1, . . . , x⌊d/2⌋)Φmult,2(x⌊d/2⌋+1, . . . , xd)\n\f\f\f\f\f\f\n≤ε\n2 + |K′|⌊d/2⌋\nε\n4((K′)⌊d/2⌋+ ε) + (|K′|⌈d/2⌉+ ε)\nε\n4((K′)⌊d/2⌋+ ε) < ε.\nThis completes the proof of (4.2.4).\nThe overall result follows by using Proposition 2.3 to show that the multiplication network can\nbe composed with a neural network comprised of the ΦSℓ,ti,n for i = 1, . . . , d. Since in no step above\nthe size of the individual networks was dependent on the approximation accuracy, this is also true\nfor the final network.\nProposition 4.8 shows that we can approximate a single multivariate B-spline with a neural\nnetwork with a size that is independent of the accuracy. Combining this observation with Theorem\n4.3 leads to the following result.\nTheorem 4.9. Let d, n, k ∈N such that 0 < k ≤n and n ≥2. Let q ≥2, and let σ be sigmoidal\nof order q.\nThen there exists C such that for every f ∈Ck([0, 1]d) and every N ∈N there exists a neural\nnetwork ΦN with activation function σ, ⌈log2(d)⌉+ ⌈logq(k −1)⌉layers, and size bounded by CN,\nsuch that\n\r\rf −ΦN\r\r\nL∞([0,1]d) ≤CN−k\nd ∥f∥Ck([0,1]d).\nProof. Fix N ∈N. By Theorem 4.3, there exist coefficients |ci| ≤C∥f∥L∞([0,1]d) and Bi ∈Bn for\ni = 1, . . . , N, such that\n\r\r\r\r\rf −\nN\nX\ni=1\nciBi\n\r\r\r\r\r\nL∞([0,1]d)\n≤CN−k\nd ∥f∥Ck([0,1]d).\nMoreover, by Proposition 4.8, for each i = 1, . . . , N exists a neural network ΦBi with ⌈log2(d)⌉+\n⌈logq(k −1)⌉layers, and a fixed size, which approximates Bi on [−1, 1]d ⊇[0, 1]d up to error of\nε := N−k/d/N. The size of ΦBi is independent of i and N.\nBy Proposition 2.3, there exists a neural network ΦN that uniformly approximates PN\ni=1 ciBi\nup to error ε on [0, 1]d, and has ⌈log2(d)⌉+ ⌈logq(k −1)⌉layers. The size of this network is linear\nin N (see Exercise 4.11). This concludes the proof.\nTheorem 4.9 shows that neural networks with higher-order sigmoidal functions can approximate\nsmooth functions with the same accuracy as spline approximations while having a comparable\nnumber of parameters. The network depth is required to behave like O(log(k)) in terms of the\nsmoothness parameter k, cp. Remark 4.4.\n44\nBibliography and further reading\nThe argument of linking sigmoidal activation functions with spline based approximation was first\nintroduced in [146, 144]. For further details on spline approximation, see [168] or the book [207].\nThe general strategy of approximating basis functions by neural networks, and then lifting ap-\nproximation results for those bases has been employed widely in the literature, and will also reappear\nagain in this book. While the following chapters primarily focus on ReLU activation, we highlight\na few notable approaches with non-ReLU activations based on the outlined strategy: To approx-\nimate analytic functions, [145] emulates a monomial basis. To approximate periodic functions, a\nbasis of trigonometric polynomials is recreated in [147]. Wavelet bases have been emulated in [171].\nMoreover, neural networks have been studied through the representation system of ridgelets [30]\nand ridge functions [103]. A general framework describing the emulation of representation systems\nto transfer approximation results was presented in [21].\n45\nExercises\nExercise 4.10. Show that (4.2.1) holds.\nExercise 4.11. Let L ∈N, σ: R →R, and let Φ1, Φ2 be two neural networks with architecture\n(σ; d0, d(1)\n1 , . . . , d(1)\nL , dL+1) and (σ; d0, d(2)\n1 , . . . , d(2)\nL , dL+1). Show that Φ1 + Φ2 is a neural network\nwith size(Φ1 + Φ2) ≤size(Φ1) + size(Φ2).\nExercise 4.12. Show that, for σ = σ2\nReLU and k ≤2, for all f ∈Ck([0, 1]d) all weights of the approx-\nimating neural network of Theorem 4.9 can be bounded in absolute value by O(max{2, ∥f∥Ck([0,1]d)}).\n46\nChapter 5\nReLU neural networks\nIn this chapter, we discuss feedforward neural networks using the ReLU activation function σReLU\nintroduced in Section 2.3. We refer to these functions as ReLU neural networks. Due to its simplicity\nand the fact that it reduces the vanishing and exploding gradients phenomena, the ReLU is one of\nthe most widely used activation functions in practice.\nA key component of the proofs in the previous chapters was the approximation of derivatives of\nthe activation function to emulate polynomials. Since the ReLU is piecewise linear, this trick is not\napplicable. This makes the analysis fundamentally different from the case of smoother activation\nfunctions. Nonetheless, we will see that even this extremely simple activation function yields a very\nrich class of functions possessing remarkable approximation capabilities.\nTo formalize these results, we begin this chapter by adopting a framework from [174]. This\nframework enables the tracking of the number of network parameters for basic manipulations such\nas adding up or composing two neural networks. This will allow to bound the network complexity,\nwhen constructing more elaborate networks from simpler ones. With these preliminaries at hand,\nthe rest of the chapter is dedicated to the exploration of links between ReLU neural networks and\nthe class of “continuous piecewise linear functions.” In Section 5.2, we will see that every such\nfunction can be exactly represented by a ReLU neural network. Afterwards, in Section 5.3 we will\ngive a more detailed analysis of the required network complexity. Finally, we will use these results\nto prove a first approximation theorem for ReLU neural networks in Section 5.4. The argument is\nsimilar in spirit to Chapter 4, in that we transfer established approximation theory for piecewise\nlinear functions to the class of ReLU neural networks of a certain architecture.\n5.1\nBasic ReLU calculus\nThe goal of this section is to formalize how to combine and manipulate ReLU neural networks.\nWe have seen an instance of such a result already in Proposition 2.3. Now we want to make this\nresult more precise under the assumption that the activation function is the ReLU. We sharpen\nProposition 2.3 by adding bounds on the number of weights that the resulting neural networks\nhave. The following four operations form the basis of all constructions in the sequel.\n• Reproducing an identity: We have seen in Proposition 3.16 that for most activation functions,\nan approximation to the identity can be built by neural networks. For ReLUs, we can have\nan even stronger result and reproduce the identity exactly. This identity will play a crucial\n47\nrole in order to extend certain neural networks to deeper neural networks, and to facilitate\nan efficient composition operation.\n• Composition: We saw in Proposition 2.3 that we can produce a composition of two neural\nnetworks and the resulting function is a neural network as well. There we did not study the\nsize of the resulting neural networks. For ReLU activation functions, this composition can be\ndone in a very efficient way leading to a neural network that has up to a constant not more\nthan the number of weights of the two initial neural networks.\n• Parallelization: Also the parallelization of two neural networks was discussed in Proposition\n2.3. We will refine this notion and make precise the size of the resulting neural networks.\n• Linear combinations: Similarly, for the sum of two neural networks, we will give precise\nbounds on the size of the resulting neural network.\n5.1.1\nIdentity\nWe start with expressing the identity on Rd as a neural network of depth L ∈N.\nLemma 5.1 (Identity). Let L ∈N.\nThen, there exists a ReLU neural network Φid\nL such that\nΦid\nL (x) = x for all x ∈Rd. Moreover, depth(Φid\nL ) = L, width(Φid\nL ) = 2d, and size(Φid\nL ) = 2d·(L+1).\nProof. Writing Id ∈Rd×d for the identity matrix, we choose the weights\n(W (0), b(0)), . . . , (W (L), b(L))\n:=\n\u0012\u0012 Id\n−Id\n\u0013\n, 0\n\u0013\n, (I2d, 0), . . . , (I2d, 0)\n|\n{z\n}\nL−1 times\n, ((Id, −Id), 0).\nUsing that x = σReLU(x) −σReLU(−x) for all x ∈R and σReLU(x) = x for all x ≥0 it is obvious\nthat the neural network Φid\nL associated to the weights above satisfies the assertion of the lemma.\nWe will see in Exercise 5.23 that the property to exactly represent the identity is not shared by\nsigmoidal activation functions. It does hold for polynomial activation functions, though.\n5.1.2\nComposition\nAssume we have two neural networks Φ1, Φ2 with architectures (σReLU; d1\n0, . . . , d1\nL1+1) and (σReLU; d2\n0, . . . , d2\nL1+1)\nrespectively. Moreover, we assume that they have weights and biases given by\n(W (0)\n1 , b(0)\n1 ), . . . , (W (L1)\n1\n, b(L1)\n1\n), and (W (0)\n2 , b(0)\n2 ), . . . , (W (L2)\n2\n, b(L2)\n2\n),\nrespectively. If the output dimension d1\nL1+1 of Φ1 equals the input dimension d2\n0 of Φ2, we can\ndefine two types of concatenations: First Φ2 ◦Φ1 is the neural network with weights and biases\ngiven by\n\u0010\nW (0)\n1 , b(0)\n1\n\u0011\n, . . . ,\n\u0010\nW (L1−1)\n1\n, b(L1−1)\n1\n\u0011\n,\n\u0010\nW (0)\n2 W (L1)\n1\n, W (0)\n2 b(L1)\n1\n+ b(0)\n2\n\u0011\n,\n\u0010\nW (1)\n2 , b(1)\n2\n\u0011\n, . . . ,\n\u0010\nW (L2)\n2\n, b(L2)\n2\n\u0011\n.\n48\nSecond, Φ2 • Φ1 is the neural network defined as Φ2 ◦Φid\n1 ◦Φ1. In terms of weighs and biases,\nΦ2 • Φ1 is given as\n\u0010\nW (0)\n1 , b(0)\n1\n\u0011\n, . . . ,\n\u0010\nW (L1−1)\n1\n, b(L1−1)\n1\n\u0011\n,\n  \nW (L1)\n1\n−W (L1)\n1\n!\n,\n \nb(L1)\n1\n−b(L1)\n1\n!!\n,\n\u0010\u0010\nW (0)\n2 , −W (0)\n2\n\u0011\n, b(0)\n2\n\u0011\n,\n\u0010\nW (1)\n2 , b(1)\n2\n\u0011\n, . . . ,\n\u0010\nW (L2)\n2\n, b(L2)\n2\n\u0011\n.\nThe following lemma collects the properties of the construction above.\nLemma 5.2 (Composition). Let Φ1, Φ2 be neural networks with architectures (σReLU; d1\n0, . . . , d1\nL1+1)\nand (σReLU; d2\n0, . . . , d2\nL2+1). Assume d1\nL1+1 = d2\n0. Then Φ2 ◦Φ1(x) = Φ2 • Φ1(x) = Φ2(Φ1(x)) for\nall x ∈Rd0\n1. Moreover,\nwidth(Φ2 ◦Φ1) ≤max{width(Φ1), width(Φ2)},\ndepth(Φ2 ◦Φ1) = depth(Φ1) + depth(Φ2),\nsize(Φ2 ◦Φ1) ≤size(Φ1) + size(Φ2) + (d1\nL1 + 1)d1\n2,\nand\nwidth(Φ2 • Φ1) ≤2 max{width(Φ1), width(Φ2)},\ndepth(Φ2 • Φ1) = depth(Φ1) + depth(Φ2) + 1,\nsize(Φ2 • Φ1) ≤2(size(Φ1) + size(Φ2)).\nProof. The fact that Φ2 ◦Φ1(x) = Φ2 • Φ1(x) = Φ2(Φ1(x)) for all x ∈Rd1\n0 follows immediately\nfrom the construction. The same can be said for the width and depth bounds. To confirm the size\nbound, we note that W (0)\n2 W (L1)\n1\n∈Rd2\n1×d1\nL1 and hence W (0)\n2 W (L1)\n1\nhas not more than d2\n1 × d1\nL1\n(nonzero) entries. Moreover, W (0)\n2 b(L1)\n1\n+ b(0)\n2\n∈Rd2\n1. Thus, the L1-th layer of Φ2 ◦Φ1(x) has at\nmost d2\n1 × (1 + d1\nL1) entries. The rest is obvious from the construction.\nInterpreting linear transformations as neural networks of depth 0, the previous lemma is also\nvalid in case Φ1 or Φ2 is a linear mapping.\n5.1.3\nParallelization\nLet (Φi)m\ni=1 be neural networks with architectures (σReLU; di\n0, . . . , di\nLi+1), respectively. We proceed\nto build a neural network (Φ1, . . . , Φm) realizing the function\n(Φ1, . . . , Φm): R\nPm\nj=1 dj\n0 →R\nPm\nj=1 dj\nLj+1\n(5.1.1)\n(x1, . . . , xm) 7→(Φ1(x1), . . . , Φm(xm)).\n49\nTo do so we first assume L1 = · · · = Lm = L, and define (Φ1, . . . , Φm) via the following sequence\nof weight-bias tuples:\n\n\n\n\n\n\nW (0)\n1\n...\nW (0)\nm\n\n\n,\n\n\n\nb(0)\n1...\nb(0)\nm\n\n\n\n\n\n, . . . ,\n\n\n\n\n\n\nW (L)\n1\n...\nW (L)\nm\n\n\n,\n\n\n\nb(L)\n1...\nb(L)\nm\n\n\n\n\n\n\n(5.1.2)\nwhere these matrices are understood as block-diagonal filled up with zeros. For the general case\nwhere the Φj might have different depths, let Lmax := max1≤i≤m Li and I := {1 ≤i ≤m | Li <\nLmax}. For j ∈Ic set eΦj := Φj, and for each j ∈I\neΦj := Φid\nLmax−Lj ◦Φj.\n(5.1.3)\nFinally,\n(Φ1, . . . , Φm) := (eΦ1, . . . , eΦm).\n(5.1.4)\nWe collect the properties of the parallelization in the lemma below.\nLemma 5.3 (Parallelization). Let m ∈N and (Φi)m\ni=1 be neural networks with architectures\n(σReLU; di\n0, . . . , di\nLi+1), respectively. Then the neural network (Φ1, . . . , Φm) satisfies\n(Φ1, . . . , Φm)(x) = (Φ1(x1), . . . , Φm(xm)) for all x ∈R\nPm\nj=1 dj\n0.\nMoreover, with Lmax := maxj≤m Lj it holds that\nwidth((Φ1, . . . , Φm)) ≤2\nm\nX\nj=1\nwidth(Φj),\n(5.1.5a)\ndepth((Φ1, . . . , Φm)) = max\nj≤m depth(Φj),\n(5.1.5b)\nsize((Φ1, . . . , Φm)) ≤2\nm\nX\nj=1\nsize(Φj) + 2\nm\nX\nj=1\n(Lmax −Lj)dj\nLj+1.\n(5.1.5c)\nProof. All statements except for the bound on the size follow immediately from the construction.\nTo obtain the bound on the size, we note that by construction the sizes of the (eΦi)m\ni=1 in (5.1.3)\nwill simply be added. The size of each eΦi can be bounded with Lemma 5.2.\nIf all input dimensions d1\n0 = · · · = dm\n0 =: d0 are the same, we will also use parallelization with\nshared inputs to realize the function x 7→(Φ1(x), . . . , Φm(x)) from Rd0 →Rd1\nL1+1+···+dm\nLm+1.\nIn terms of the construction (5.1.2), the only required change is that the block-diagonal matrix\ndiag(W (0)\n1 , . . . , W (0)\nm ) becomes the matrix in R\nPm\nj=1 dj\n1×d1\n0 which stacks W (0)\n1 , . . . , W (0)\nm on top of\neach other. Similarly, we will allow Φj to only take some of the entries of x as input. For par-\nallelization with shared inputs we will use the same notation (Φj)m\nj=1 as before, where the precise\nmeaning will always be clear from context. Note that Lemma 5.3 remains valid in this case.\n50\n5.1.4\nLinear combinations\nLet m ∈N and let (Φi)m\ni=1 be ReLU neural networks that have architectures (σReLU; di\n0, . . . , di\nLi+1),\nrespectively. Assume that d1\nL1+1 = · · · = dm\nLm+1, i.e., all Φ1, . . . , Φm have the same output dimen-\nsion. For scalars αj ∈R, we wish to construct a ReLU neural network Pm\nj=1 αjΦj realizing the\nfunction\n(\nR\nPm\nj=1 dj\n0 →Rd1\nL1+1\n(x1, . . . , xm) 7→Pm\nj=1 αjΦj(xj).\nThis corresponds to the parallelization (Φ1, . . . , Φm) composed with the linear transformation\n(z1, . . . , zm) 7→Pm\nj=1 αjzj. The following result holds.\nLemma 5.4 (Linear combinations). Let m ∈N and (Φi)m\ni=1 be neural networks with architec-\ntures (σReLU; di\n0, . . . , di\nLi+1), respectively. Assume that d1\nL1+1 = · · · = dm\nLm+1, let α ∈Rm and set\nLmax := maxj≤m Lj. Then, there exists a neural network Pm\nj=1 αjΦj such that (Pm\nj=1 αjΦj)(x) =\nPm\nj=1 αjΦj(xj) for all x = (xj)m\nj=1 ∈R\nPm\nj=1 dj\n0. Moreover,\nwidth\n\n\nm\nX\nj=1\nαjΦj\n\n≤2\nm\nX\nj=1\nwidth(Φj),\n(5.1.6a)\ndepth\n\n\nm\nX\nj=1\nαjΦj\n\n= max\nj≤m depth(Φj),\n(5.1.6b)\nsize\n\n\nm\nX\nj=1\nαjΦj\n\n≤2\nm\nX\nj=1\nsize(Φj) + 2\nm\nX\nj=1\n(Lmax −Lj)dj\nLj+1.\n(5.1.6c)\nProof. The construction of Pm\nj=1 αjΦj is analogous to that of (Φ1, . . . , Φm), i.e., we first define the\nlinear combination of neural networks with the same depth. Then the weights are chosen as in\n(5.1.2), but with the last linear transformation replaced by\n\n(α1W (L)\n1\n· · · αmW (L)\nm ),\nm\nX\nj=1\nαjb(L)\nj\n\n.\nFor general depths, we define the sum of the neural networks to be the sum of the extended\nneural networks eΦi as of (5.1.3). All statements of the lemma follow immediately from this con-\nstruction.\nIn case d1\n0 = · · · = dm\n0 =: d0 (all neural networks have the same input dimension), we will also\nconsider linear combinations with shared inputs, i.e., a neural network realizing\nx 7→\nm\nX\nj=1\nαjΦj(x)\nfor x ∈Rd0.\n51\nThis requires the same minor adjustment as discussed at the end of Section 5.1.3. Lemma 5.4\nremains valid in this case and again we do not distinguish in notation for linear combinations with\nor without shared inputs.\n5.2\nContinuous piecewise linear functions\nIn this section, we will relate ReLU neural networks to a large class of functions. We first formally\nintroduce the set of continuous piecewise linear functions from a set Ω⊆Rd to R. Note that we\nadmit in particular Ω= Rd in the following definition.\nDefinition 5.5. Let Ω⊆Rd, d ∈N. We call a function f : Ω→R continuous, piecewise linear\n(cpwl) if f ∈C0(Ω) and there exist n ∈N affine functions gj : Rd →R, gj(x) = w⊤\nj x + bj such\nthat for each x ∈Ωit holds that f(x) = gj(x) for at least one j ∈{1, . . . , n}. For m > 1 we call\nf : Ω→Rm cpwl if and only if each component of f is cpwl.\nRemark 5.6. A “continuous piecewise linear function” as in Definition 5.5 is actually piecewise\naffine. To maintain consistency with the literature, we use the terminology cpwl.\nIn the following, we will refer to the connected domains on which f is equal to one of the\nfunctions gj, also as regions or pieces. If f is cpwl with q ∈N regions, then with n ∈N denoting\nthe number of affine functions it holds n ≤q.\nNote that, the mapping x 7→σReLU(w⊤x + b), which is a ReLU neural network with a single\nneuron, is cpwl (with two regions). Consequently, every ReLU neural network is a repeated compo-\nsition of linear combinations of cpwl functions. It is not hard to see that the set of cpwl functions\nis closed under compositions and linear combinations. Hence, every ReLU neural network is a cpwl\nfunction. Interestingly, the reverse direction of this statement is also true, meaning that every cpwl\nfunction can be represented by a ReLU neural network as we shall demonstrate below. Therefore,\nwe can identify the class of functions realized by arbitrary ReLU neural networks as the class of\ncpwl functions.\nTheorem 5.7. Let d ∈N, let Ω⊆Rd be convex, and let f : Ω→R be cpwl with n ∈N as in\nDefinition 5.5. Then, there exists a ReLU neural network Φf such that Φf(x) = f(x) for all x ∈Ω\nand\nsize(Φf) = O(dn2n),\nwidth(Φf) = O(dn2n),\ndepth(Φf) = O(n).\nA statement similar to Theorem 5.7 can be found in [4, 85]. There, the authors give a con-\nstruction with a depth that behaves logarithmic in d and is independent of n, but with significantly\nlarger bounds on the size. As we shall see, the proof of Theorem 5.7 is a simple consequence of the\nfollowing well-known result from [225]; also see [169, 237]. It states that every cpwl function can\nbe expressed as a finite maximum of a finite minimum of certain affine functions.\n52\nProposition 5.8. Let d ∈N, Ω⊆Rd be convex, and let f : Ω→R be cpwl with n ∈N affine\nfunctions as in Definition 5.5. Then there exists m ∈N and sets sj ⊆{1, . . . , n} for j ∈{1, . . . , m},\nsuch that\nf(x) = max\n1≤j≤m min\ni∈sj (gi(x))\nfor all x ∈Ω.\n(5.2.1)\nProof. Step 1. We start with d = 1, i.e., Ω⊆R is a (possibly unbounded) interval and for each\nx ∈Ωthere exists j ∈{1, . . . , n} such that with gj(x) := wjx + bj it holds that f(x) = gj(x).\nWithout loss of generality, we can assume that gi ̸= gj for all i ̸= j. Since the graphs of the gj are\nlines, they intersect at (at most) finitely many points in Ω.\nSince f is continuous, we conclude that there exist finitely many intervals covering Ω, such that\nf coincides with one of the gj on each interval. For each x ∈Ωlet\nsx := {1 ≤j ≤n | gj(x) ≥f(x)}\nand\nfx(y) := min\nj∈sx gj(y)\nfor all y ∈Ω.\nClearly, fx(x) = f(x). We claim that, additionally,\nfx(y) ≤f(y)\nfor all y ∈Ω.\n(5.2.2)\nThis then shows that\nf(y) = max\nx∈Ωfx(y) = max\nx∈Ωmin\nj∈sx gj(y)\nfor all y ∈R.\nSince there exist only finitely many possibilities to choose a subset of {1, . . . , n}, we conclude that\n(5.2.1) holds for d = 1.\nIt remains to verify the claim (5.2.2). Fix y ̸= x ∈Ω. Without loss of generality, let x < y\nand let x = x0 < · · · < xk = y be such that f|[xi−1,xi] equals some gj for each i ∈{1, . . . , k}. In\norder to show (5.2.2), it suffices to prove that there exists at least one j such that gj(x0) ≥f(x0)\nand gj(xk) ≤f(xk).\nThe claim is trivial for k = 1.\nWe proceed by induction.\nSuppose the\nclaim holds for k −1, and consider the partition x0 < · · · < xk.\nLet r ∈{1, . . . , n} be such\nthat f|[x0,x1] = gr|[x0,x1]. Applying the induction hypothesis to the interval [x1, xk], we can find\nj ∈{1, . . . , n} such that gj(x1) ≥f(x1) and gj(xk) ≤f(xk). If gj(x0) ≥f(x0), then gj is the desired\nfunction. Otherwise, gj(x0) < f(x0). Then gr(x0) = f(x0) > gj(x0) and gr(x1) = f(x1) ≤gj(x1).\nTherefore gr(x) ≤gj(x) for all x ≥x1, and in particular gr(xk) ≤gj(xk). Thus gr is the desired\nfunction.\nStep 2. For general d ∈N, let gj(x) := w⊤\nj x + bj for j = 1, . . . , n. For each x ∈Ω, let\nsx := {1 ≤j ≤n | gj(x) ≥f(x)}\n53\nand for all y ∈Ω, let\nfx(y) := min\nj∈sx gj(y).\nFor an arbitrary 1-dimensional affine subspace S ⊆Rd passing through x consider the line\n(segment) I := S ∩Ω, which is connected since Ωis convex. By Step 1, it holds\nf(y) = max\nx∈Ωfx(y) = max\nx∈Ωmin\nj∈sx gj(y)\non all of I. Since I was arbitrary the formula is valid for all y ∈Ω. This again implies (5.2.1) as\nin Step 1.\nRemark 5.9. Using min(a, b) = −max(−a, −b), there exists ˜m ∈N and sets ˜sj ⊆{1, . . . , n} for\nj = 1, . . . , ˜m, such that for all x ∈R\nf(x) = −(−f(x)) = −min\n1≤j≤˜m max\ni∈˜sj (−w⊤\ni x −bi)\n= max\n1≤j≤˜m(−max\ni∈˜sj (−w⊤\ni x −bi))\n= max\n1≤j≤˜m(min\ni∈˜sj (w⊤\ni x + bi)).\nTo prove Theorem 5.7, it therefore suffices to show that the minimum and the maximum are\nexpressible by ReLU neural networks.\nLemma 5.10. For every x, y ∈R it holds that\nmin{x, y} = σReLU(y) −σReLU(−y) −σReLU(y −x) ∈N 1\n2 (σReLU; 1, 3)\nand\nmax{x, y} = σReLU(y) −σReLU(−y) + σReLU(x −y) ∈N 1\n2 (σReLU; 1, 3).\nProof. We have\nmax{x, y} = y +\n(\n0\nif y > x\nx −y\nif x ≥y\n= y + σReLU(x −y).\nUsing y = σReLU(y) −σReLU(−y), the claim for the maximum follows. For the minimum observe\nthat min{x, y} = −max{−x, −y}.\nThe minimum of n ≥2 inputs can be computed by repeatedly applying the construction of\nLemma 5.10. The resulting neural network is described in the next lemma.\n54\nx\ny\nmin{x, y}\nFigure 5.1: Sketch of the neural network in Lemma 5.10. Only edges with non-zero weights are\ndrawn.\nLemma 5.11. For every n ≥2 there exists a neural network Φmin\nn\n: Rn →R with\nsize(Φmin\nn\n) ≤16n,\nwidth(Φmin\nn\n) ≤3n,\ndepth(Φmin\nn\n) ≤⌈log2(n)⌉\nsuch that Φmin\nn\n(x1, . . . , xn) = min1≤j≤n xj. Similarly, there exists a neural network Φmax\nn\n: Rn →R\nrealizing the maximum and satisfying the same complexity bounds.\nProof. Throughout denote by Φmin\n2\n: R2 →R the neural network from Lemma 5.10. It is of depth\n1 and size 7 (since all biases are zero, it suffices to count the number of connections in Figure 5.1).\nStep 1. Consider first the case where n = 2k for some k ∈N. We proceed by induction of k.\nFor k = 1 the claim is proven. For k ≥2 set\nΦmin\n2k\n:= Φmin\n2\n◦(Φmin\n2k−1, Φmin\n2k−1).\n(5.2.3)\nBy Lemma 5.2 and Lemma 5.3 we have\ndepth(Φmin\n2k ) ≤depth(Φmin\n2\n) + depth(Φmin\n2k−1) ≤· · · ≤k.\nNext, we bound the size of the neural network. Note that all biases in this neural network are set to\n0, since the Φmin\n2\nneural network in Lemma 5.10 has no biases. Thus, the size of the neural network\nΦmin\n2k\ncorresponds to the number of connections in the graph (the number of nonzero weights).\nCareful inspection of the neural network architecture, see Figure 5.2, reveals that\nsize(Φmin\n2k ) = 4 · 2k−1 +\nk−2\nX\nj=0\n12 · 2j + 3\n= 2n + 12 · (2k−1 −1) + 3 = 2n + 6n −9 ≤8n,\nand that width(Φmin\n2k ) ≤(3/2)2k. This concludes the proof for the case n = 2k.\nStep 2. For the general case, we first let\nΦmin\n1\n(x) := x\nfor all x ∈R\nbe the identity on R, i.e. a linear transformation and thus formally a depth 0 neural network. Then,\nfor all n ≥2\nΦmin\nn\n:= Φmin\n2\n◦\n\n\n\n(Φid\n1 ◦Φmin\n⌊n\n2 ⌋, Φmin\n⌈n\n2 ⌉)\nif n ∈{2k + 1 | k ∈N}\n(Φmin\n⌊n\n2 ⌋, Φmin\n⌈n\n2 ⌉)\notherwise.\n(5.2.4)\n55\nThis definition extends (5.2.3) to arbitrary n ≥2, since the first case in (5.2.4) never occurs if n ≥2\nis a power of two.\nTo analyze (5.2.4), we start with the depth and claim that\ndepth(Φmin\nn\n) = k\nfor all 2k−1 < n ≤2k\nand all k ∈N. We proceed by induction over k. The case k = 1 is clear. For the induction step,\nassume the statement holds for some fixed k ∈N and fix an integer n with 2k < n ≤2k+1. Then\nln\n2\nm\n∈(2k−1, 2k] ∩N\nand\njn\n2\nk\n∈\n(\n{2k−1}\nif n = 2k + 1\n(2k−1, 2k] ∩N\notherwise.\nUsing the induction assumption, (5.2.4) and Lemmas 5.1 and 5.2, this shows\ndepth(Φmin\nn\n) = depth(Φmin\n2\n) + k = 1 + k,\nand proves the claim.\nFor the size and width bounds, we only sketch the argument: Fix n ∈N such that 2k−1 < n ≤2k.\nThen Φmin\nn\nis constructed from at most as many subnetworks as Φmin\n2k , but with some Φmin\n2\n: R2 →R\nblocks replaced by Φid\n1 : R →R, see Figure 5.3. Since Φid\n1 has the same depth as Φmin\n2\n, but is smaller\nin width and number of connections, the width and size of Φmin\nn\nis bounded by the width and size\nof Φmin\n2k . Due to 2k ≤2n, the bounds from Step 1 give the bounds stated in the lemma.\nStep 3. For the maximum, define\nΦmax\nn\n(x1, . . . , xn) := −Φmin\nn\n(−x1, . . . , −xn).\nof Theorem 5.7. By Proposition 5.8 the neural network\nΦ := Φmax\nm\n• (Φmin\n|sj| )m\nj=1 • ((w⊤\ni x + bi)i∈sj)m\nj=1\nrealizes the function f.\nSince the number of possibilities to choose subsets of {1, . . . , n} equals 2n we have m ≤2n.\nSince each sj is a subset of {1, . . . , n}, the cardinality |sj| of sj is bounded by n. By Lemma 5.2,\nLemma 5.3, and Lemma 5.11\ndepth(Φ) ≤2 + depth(Φmax\nm\n) + max\n1≤j≤n depth(Φmin\n|sj| )\n≤1 + ⌈log2(2n)⌉+ ⌈log2(n)⌉= O(n)\nand\nwidth(Φ) ≤2 max\nn\nwidth(Φmax\nm\n),\nm\nX\nj=1\nwidth(Φmin\n|sj| ),\nm\nX\nj=1\nwidth((w⊤\ni x + bi)i∈sj))\no\n≤2 max{3m, 3mn, mdn} = O(dn2n)\n56\nnr of connections\nbetween layers:\n2k−1 · 4\n2k−2 · 12 2k−3 · 12\n3\n\f\f\f\n\f\f\f\n\f\f\f\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nmin{x1, . . . , x8}\nFigure 5.2: Architecture of the Φmin\n2k\nneural network in Step 1 of the proof of Lemma 5.11 and the\nnumber of connections in each layer for k = 3. Each grey box corresponds to 12 connections in the\ngraph.\nx1 x2 x3 x4 x5\nmin{x1, . . . , x5}\nΦmin\n2\nΦid\n1 Φmin\n2\nΦid\n1\nΦmin\n2\nΦmin\n2\nx1 x2 x3 x4 x5 x6\nmin{x1, . . . , x6}\nΦid\n1 Φmin\n2\nΦid\n1 Φmin\n2\nΦmin\n2\nΦmin\n2\nΦmin\n2\nx1 x2 x3 x4 x5 x6 x7 x8\nmin{x1, . . . , x8}\nΦmin\n2\nΦmin\n2\nΦmin\n2\nΦmin\n2\nΦmin\n2\nΦmin\n2\nΦmin\n2\nFigure 5.3: Construction of Φmin\nn\nfor general n in Step 2 of the proof of Lemma 5.11.\n57\nand\nsize(Φ) ≤4\n\u0010\nsize(Φmax\nm\n) + size((Φmin\n|sj| )m\nj=1) + size((w⊤\ni x + bi)i∈sj)m\nj=1)\n\u0011\n≤4\n\n16m + 2\nm\nX\nj=1\n(16|sj| + 2⌈log2(n)⌉) + nm(d + 1)\n\n= O(dn2n).\nThis concludes the proof.\n5.3\nSimplicial pieces\nThis section studies the case, were we do not have arbitrary cpwl functions, but where the regions\non which f is affine are simplices. Under this condition, we can construct neural networks that scale\nmerely linearly in the number of such regions, which is a serious improvement from the exponential\ndependence of the size on the number of regions that was found in Theorem 5.7.\n5.3.1\nTriangulations of Ω\nFor the ensuing discussion, we will consider Ω⊆Rd to be partitioned into simplices. This parti-\ntioning will be termed a triangulation of Ω. Other notions prevalent in the literature include a\ntessellation of Ω, or a simplicial mesh on Ω. To give a precise definition, let us first recall some\nterminology. For a set S ⊆Rd we denote the convex hull of S by\nco(S) :=\n\n\n\nn\nX\nj=1\nαjxj\n\f\f\f\f\f\f\nn ∈N, xj ∈S, αj ≥0,\nn\nX\nj=1\nαj = 1\n\n\n.\n(5.3.1)\nAn n-simplex is the convex hull of n ∈N points that are independent in a specific sense. This\nis made precise in the following definition.\nDefinition 5.12. Let n ∈N0, d ∈N and n ≤d. We call x0, . . . , xn ∈Rd affinely independent\nif and only if either n = 0 or n ≥1 and the vectors x1 −x0, . . . , xn −x0 are linearly independent.\nIn this case, we call co(x0, . . . , xn) := co({x0, . . . , xn}) an n-simplex.\nAs mentioned before, a triangulation refers to a partition of a space into simplices. We give a\nformal definition below.\nDefinition 5.13. Let d ∈N, and Ω⊆Rd be compact. Let T be a finite set of d-simplices, and\nfor each τ ∈T let V (τ) ⊆Ωhave cardinality d + 1 such that τ = co(V (τ)). We call T a regular\ntriangulation of Ω, if and only if\n(i) S\nτ∈T τ = Ω,\n(ii) for all τ, τ ′ ∈T it holds that τ ∩τ ′ = co(V (τ) ∩V (τ ′)).\nWe call η ∈V := S\nτ∈T V (τ) a node (or vertex) and τ ∈T an element of the triangulation.\n58\nτ1 = co(η1, η2, η5)\nτ2 = co(η2, η3, η5)\nτ3 = co(η3, η4, η5)\nη1\nη2\nη3\nη4\nη5\nτ1 = co(η2, η3, η4)\nτ2 = co(η2, η5, η1)\nη1\nη2\nη3\nη4\nη5\nτ1 = co(η2, η3, η4)\nτ2 = co(η1, η2, η3)\nη1\nη2\nη3\nη4\nFigure 5.4: The first is a regular triangulation, while the second and the third are not.\nFor a regular triangulation T with nodes V we also introduce the constant\nkT := max\nη∈V |{τ ∈T | η ∈τ}|\n(5.3.2)\ncorresponding to the maximal number of elements shared by a single node.\n5.3.2\nSize bounds for regular triangulations\nThroughout this subsection, let T be a regular triangulation of Ω, and we adhere to the notation\nof Definition 5.13. We will say that f : Ω→R is cpwl with respect to T if f is cpwl and f|τ is\naffine for each τ ∈T . The rest of this subsection is dedicated to proving the following result. It\nwas first shown in [136] with a more technical argument, and extends an earlier statement from\n[85] to general triangulations (also see Section 5.3.3).\nTheorem 5.14. Let d ∈N, Ω⊆Rd be a bounded domain, and let T be a regular triangulation\nof Ω. Let f : Ω→R be cpwl with respect to T and f|∂Ω= 0. Then there exists a ReLU neural\nnetwork Φ : Ω→R realizing f, and it holds\nsize(Φ) = O(|T |),\nwidth(Φ) = O(|T |),\ndepth(Φ) = O(1),\n(5.3.3)\nwhere the constants in the Landau notation depend on d and kT in (5.3.2).\nWe will split the proof into several lemmata. The strategy is to introduce a basis of the space\nof cpwl functions on T the elements of which vanish on the boundary of Ω. We will then show\nthat there exist O(|T |) basis functions, each of which can be represented with a neural network the\nsize of which depends only on kT and d. To construct this basis, we first point out that an affine\nfunction on a simplex is uniquely defined by its values at the nodes.\nLemma 5.15. Let d ∈N. Let τ := co(η0, . . . , ηd) be a d-simplex. For every y0, . . . , yd ∈R, there\nexists a unique g ∈P1(Rd) such that g(ηi) = yi, i = 0, . . . , d.\n59\nη\nη6\nη1\nη2\nη3\nη4\nη5\nτ1\nτ2\nτ3\nτ4\nτ5\nτ6\nco(V (τ1)\\{η}) = co({η1, η2})\nω(η)\nFigure 5.5: Visualization of Lemma 5.16 in two dimensions. The patch ω(η) consists of the union\nof all 2-simplices τi containing η. Its boundary consists of the union of all 1-simplices made up by\nthe nodes of each τi without the center node, i.e., the convex hulls of V (τi)\\{η}.\nProof. Since η1−η0, . . . , ηd−η0 is a basis of Rd, there is a unique w ∈Rd such that w⊤(ηi−η0) =\nyi −y0 for i = 1, . . . , d. Then g(x) := w⊤x + (y0 −w⊤η0) is as desired. Moreover, for every g ∈P1\nit holds that g(Pd\ni=0 αiηi) = Pd\ni=0 αig(ηi) whenever Pd\ni=0 αi = 1 (this is in general not true if the\ncoefficients do not sum to 1). Hence, g is uniquely determined by its values at the nodes.\nSince Ωis the union of the simplices τ ∈T , every cpwl function with respect to T is thus\nuniquely defined through its values at the nodes. Hence, the desired basis consists of cpwl functions\nφη : Ω→R with respect to T such that\nφη(µ) = δηµ\nfor all µ ∈V,\n(5.3.4)\nwhere δηµ denotes the Kronecker delta. Assuming φη to be well-defined for the moment, we can\nthen represent every cpwl function f : Ω→R that vanishes on the boundary ∂Ωas\nf(x) =\nX\nη∈V∩˚Ω\nf(η)φη(x)\nfor all x ∈Ω.\nNote that it suffices to sum over the set of interior nodes V ∩˚Ω, since f(η) = 0 whenever η ∈\n∂Ω. To formally verify existence and well-definedness of φη, we first need a lemma characterizing\nthe boundary of so-called “patches” of the triangulation: For each η ∈V, we introduce the patch\nω(η) of the node η as the union of all elements containing η, i.e.,\nω(η) :=\n[\n{τ∈T | η∈τ}\nτ.\n(5.3.5)\nLemma 5.16. Let η ∈V ∩˚Ωbe an interior node. Then,\n∂ω(η) =\n[\n{τ∈T | η∈τ}\nco(V (τ)\\{η}).\n60\nWe refer to Figure 5.5 for a visualization of Lemma 5.16. The proof of Lemma 5.16 is quite\ntechnical but nonetheless elementary. We therefore only outline the general argument but leave\nthe details to the reader in Excercise 5.27: The boundary of ω(η) must be contained in the union\nof the boundaries of all τ in the patch ω(η). Since η is an interior point of Ω, it must also be\nan interior point of ω(η). This can be used to show that for every S := {ηi0, . . . , ηik} ⊆V (τ) of\ncardinality k + 1 ≤d, the interior of (the k-dimensional manifold) co(S) belongs to the interior\nof ω(η) whenever η ∈S. Using Exercise 5.27, it then only remains to check that co(S) ⊆∂ω(η)\nwhenever η /∈S, which yields the claimed formula. We are now in position to show well-definedness\nof the basis functions in (5.3.4).\nLemma 5.17. For each interior node η ∈V ∩˚Ωthere exists a unique cpwl function φη : Ω→R\nsatisfying (5.3.4). Moreover, φη can be expressed by a ReLU neural network with size, width, and\ndepth bounds that only depend on d and kT .\nProof. By Lemma 5.15, on each τ ∈T , the affine function φη|τ is uniquely defined through the\nvalues at the nodes of τ.\nThis defines a continuous function φη : Ω→R.\nIndeed, whenever\nτ ∩τ ′ ̸= ∅, then τ ∩τ ′ is a subsimplex of both τ and τ ′ in the sense of Definition 5.13 (ii). Thus,\napplying Lemma 5.15 again, the affine functions on τ and τ ′ coincide on τ ∩τ ′.\nUsing Lemma 5.15, Lemma 5.16 and the fact that φη(µ) = 0 whenever µ ̸= η, we find that\nφη vanishes on the boundary of the patch ω(η) ⊆Ω. Thus, φη vanishes on the boundary of Ω.\nExtending by zero, it becomes a cpwl function φη : Rd →R. This function is nonzero only on\nelements τ for which η ∈τ. Hence, it is a cpwl function with at most n := kT + 1 affine functions.\nBy Theorem 5.7, φη can be expressed as a ReLU neural network with the claimed size, width and\ndepth bounds.\nFinally, Theorem 5.14 is now an easy consequence of the above lemmata.\nof Theorem 5.14. With\nΦ(x) :=\nX\nη∈V∩˚Ω\nf(η)φη(x)\nfor x ∈Ω,\n(5.3.6)\nit holds that Φ : Ω→R satisfies Φ(η) = f(η) for all η ∈V. By Lemma 5.15 this implies that\nf equals Φ on each τ, and thus f equals Φ on all of Ω. Since each element τ is the convex hull\nof d + 1 nodes η ∈V, the cardinality of V is bounded by the cardinality of T times d + 1. Thus,\nthe summation in (5.3.6) is over O(|T |) terms. Using Lemma 5.4 and Lemma 5.17 we obtain the\nclaimed bounds on size, width, and depth of the neural network.\n5.3.3\nSize bounds for locally convex triangulations\nAssuming local convexity of the triangulation, in this section we make the dependence of the\nconstants in Theorem 5.14 explicit in the dimension d and in the maximal number of simplices\nkT touching a node, see (5.3.2). As such the improvement over Theorem 5.14 is modest, and the\nreader may choose to skip this section on a first pass. Nonetheless, the proof, originally from [85],\nis entirely constructive and gives some further insight on how ReLU networks express functions.\nLet us start by stating the required convexity constraint.\n61\nDefinition 5.18. A regular triangulation T is called locally convex if and only if ω(η) is convex\nfor all interior nodes η ∈V ∩˚Ω.\nThe following theorem is a variant of [85, Theorem 3.1].\nTheorem 5.19. Let d ∈N, and let Ω⊆Rd be a bounded domain. Let T be a locally convex regular\ntriangulation of Ω. Let f : Ω→R be cpwl with respect to T and f|∂Ω= 0. Then, there exists a\nconstant C > 0 (independent of d, f and T ) and there exists a neural network Φf : Ω→R such\nthat Φf = f,\nsize(Φf) ≤C · (1 + d2kT |T |),\nwidth(Φf) ≤C · (1 + d log(kT )|T |),\ndepth(Φf) ≤C · (1 + log2(kT )).\nAssume in the following that T is a locally convex triangulation. We will split the proof of the\ntheorem again into a few lemmata. First, we will show that a convex patch can be written as an\nintersection of finitely many half-spaces. Specifically, with the affine hull of a set S defined as\naff(S) :=\n\n\n\nn\nX\nj=1\nαjxj\n\f\f\f\f\f\f\nn ∈N, xj ∈S, αj ∈R,\nn\nX\nj=1\nαj = 1\n\n\n\n(5.3.7)\nlet in the following for τ ∈T and η ∈V (τ)\nH0(τ, η) := aff(V (τ)\\{η})\nbe the affine hyperplane passing through all nodes in V (τ)\\{η}, and let further\nH+(τ, η) := {x ∈Rd | x is on the same side of H0(τ, η) as η} ∪H0(τ, η).\nLemma 5.20. Let η be an interior node. Then a patch ω(η) is convex if and only if\nω(η) =\n\\\n{τ∈T | η∈T }\nH+(τ, η).\n(5.3.8)\nProof. The right-hand side is a finite intersection of (convex) half-spaces, and thus itself convex. It\nremains to show that if ω(η) is convex, then (5.3.8) holds. We start with “⊃”. Suppose x /∈ω(η).\nThen the straight line co({x, η}) must pass through ∂ω(η), and by Lemma 5.16 this implies that\nthere exists τ ∈T with η ∈τ such that co({x, η}) passes through aff(V (τ)\\{η}) = H0(τ, η).\n62\nHence η and x lie on different sides of this affine hyperplane, which shows “⊇”. Now we show “⊆”.\nLet τ ∈T be such that η ∈τ and fix x in the complement of H+(τ, η). Suppose that x ∈ω(η). By\nconvexity, we then have co({x}∪τ) ⊆ω(η). This implies that there exists a point in co(V (τ)\\{η})\nbelonging to the interior of ω(η). This contradicts Lemma 5.16. Thus, x /∈ω(η).\nThe above lemma allows us to explicitly construct the basis functions φη in (5.3.4). To see this,\ndenote in the following for τ ∈T and η ∈V (τ) by gτ,η ∈P1(Rd) the affine function such that\ngτ,η(µ) =\n(\n1\nif η = µ\n0\nif η ̸= µ\nfor all µ ∈V (τ).\nThis function exists and is unique by Lemma 5.15. Observe that φη(x) = gτ,η(x) for all x ∈τ.\nLemma 5.21. Let η ∈V ∩˚Ωbe an interior node and let ω(η) be a convex patch. Then\nφη(x) = max\n\u001a\n0,\nmin\n{τ∈T | η∈τ} gτ,η(x)\n\u001b\nfor all x ∈Rd.\n(5.3.9)\nProof. First let x /∈ω(η). By Lemma 5.20 there exists τ ∈V (η) such that x is in the complement\nof H+(τ, η). Observe that\ngτ,η|H+(τ,η) ≥0\nand\ngτ,η|H+(τ,η)c < 0.\n(5.3.10)\nThus\nmin\n{τ∈T | η∈τ} gτ,η(x) < 0\nfor all x ∈ω(η)c,\ni.e., (5.3.9) holds for all x ∈R\\ω(η). Next, let τ, τ ′ ∈T such that η ∈τ and η ∈τ ′. We wish to\nshow that gτ,η(x) ≤gτ ′,η(x) for all x ∈τ. Since gτ,η(x) = φη(x) for all x ∈τ, this then concludes\nthe proof of (5.3.9). By Lemma 5.20 it holds\nµ ∈H+(τ ′, η)\nfor all\nµ ∈V (τ).\nHence, by (5.3.10)\ngτ ′,η(µ) ≥0 = gτ,η(µ)\nfor all\nµ ∈V (τ)\\{η}.\nMoreover, gτ,η(η) = gτ ′,η(η) = 1. Thus, gτ,η(µ) ≥gτ ′,η(µ) for all µ ∈V (τ ′) and therefore\ngτ ′,η(x) ≥gτ,η(x)\nfor all x ∈co(V (τ ′)) = τ ′.\nof Theorem 5.19. For every interior node η ∈V ∩˚Ω, the cpwl basis function φη in (5.3.4) can be\nexpressed as in (5.3.9), i.e.,\nφη(x) = σ • Φmin\n|{τ∈T | η∈τ}| • (gτ,η(x)){τ∈T | η∈τ},\n63\nwhere (gτ,η(x)){τ∈T | η∈τ} denotes the parallelization with shared inputs of the functions gτ,η(x) for\nall τ ∈T such that η ∈τ.\nFor this neural network, with |{τ ∈T | η ∈τ}| ≤kT , we have by Lemma 5.2\nsize(φη) ≤4\n\u0000size(σ) + size(Φmin\n|{τ∈T | η∈τ}|) + size((gτ,η){τ∈T | η∈τ})\n\u0001\n≤4(2 + 16kT + kT d)\n(5.3.11)\nand similarly\ndepth(φη) ≤4 + ⌈log2(kT )⌉,\nwidth(φη) ≤max{1, 3kT , d}.\n(5.3.12)\nSince for every interior node, the number of simplices touching the node must be larger or equal\nto d, we can assume max{kT , d} = kT in the following (otherwise there exist no interior nodes, and\nthe function f is constant 0). As in the proof of Theorem 5.14, the neural network\nΦ(x) :=\nX\nη∈V∩˚Ω\nf(η)φη(x)\nrealizes the function f on all of Ω. Since the number of nodes |V| is bounded by (d + 1)|T |, an\napplication of Lemma 5.4 yields the desired bounds.\n5.4\nConvergence rates for H¨older continuous functions\nTheorem 5.14 immediately implies convergence rates for certain classes of (low regularity) functions.\nRecall for example the space of H¨older continuous functions: for s ∈(0, 1] and a bounded\ndomain Ω⊆Rd we define\n∥f∥C0,s(Ω) := sup\nx∈Ω\n|f(x)| +\nsup\nx̸=y∈Ω\n|f(x) −f(y)|\n∥x −y∥s\n2\n.\n(5.4.1)\nThen, C0,s(Ω) is the set of functions f ∈C0(Ω) for which ∥f∥C0,s(Ω) < ∞.\nH¨older continuous functions can be approximated well by certain cpwl functions. Therefore, we\nobtain the following result.\nTheorem 5.22. Let d ∈N. There exists a constant C = C(d) such that for every f ∈C0,s([0, 1]d)\nand every N there exists a ReLU neural network Φf\nN with\nsize(Φf\nN) ≤CN,\nwidth(Φf\nN) ≤CN,\ndepth(Φf\nN) = C\nand\nsup\nx∈[0,1]d\n\f\f\ff(x) −Φf\nN(x)\n\f\f\f ≤C∥f∥C0,s([0,1]d)N−s\nd .\n64\nProof. For M ≥2, consider the set of nodes {ν/M | ν ∈{−1, . . . , M + 1}d} where ν/M =\n(ν1/M, . . . , νd/M).\nThese nodes suggest a partition of [−1/M, 1 + 1/M]d into (2 + M)d sub-\nhypercubes. Each such sub-hypercube can be partitioned into d! simplices, such that we obtain a\nregular triangulation T with d!(2+M)d elements on [0, 1]d. According to Theorem 5.14 there exists a\nneural network Φ that is cpwl with respect to T and Φ(ν/M) = f(ν/M) whenever ν ∈{0, . . . , M}d\nand Φ(ν/M) = 0 for all other (boundary) nodes. It holds\nsize(Φ) ≤C|T | = Cd!(2 + M)d,\nwidth(Φ) ≤C|T | = Cd!(2 + M)d,\ndepth(Φ) ≤C\n(5.4.2)\nfor a constant C that only depends on d (since for our regular triangulation T , kT in (5.3.2) is a\nfixed d-dependent constant).\nLet us bound the error. Fix a point x ∈[0, 1]d. Then x belongs to one of the interior simplices\nτ of the triangulation. Two nodes of the simplex have distance at most\n\n\nd\nX\nj=1\n\u0012 1\nM\n\u00132\n\n\n1/2\n=\n√\nd\nM =: ε.\nSince Φ|τ is the linear interpolant of f at the nodes V (τ) of the simplex τ, Φ(x) is a convex\ncombination of the (f(η))η∈V (τ). Fix an arbitrary node η0 ∈V (τ). Then ∥x −η0∥2 ≤ε and\n|Φ(x) −Φ(η0)| ≤\nmax\nη,µ∈V (τ) |f(η) −f(µ)| ≤\nsup\nx,y∈[0,1]d\n∥x−y∥2≤ε\n|f(x) −f(y)|\n≤∥f∥C0,s([0,1]d)εs.\nHence, using f(η0) = Φ(η0),\n|f(x) −Φ(x)| ≤|f(x) −f(η0)| + |Φ(x) −Φ(η0)|\n≤2∥f∥C0,s([0,1]d)εs\n= 2∥f∥C0,s([0,1]d)d\ns\n2 M−s\n= 2d\ns\n2 ∥f∥C0,s([0,1]d)N−s\nd\n(5.4.3)\nwhere N := Md. The statement follows by (5.4.2) and (5.4.3).\nThe principle behind Theorem 5.22 can be applied in even more generality.\nSince we can\nrepresent every cpwl function on a regular triangulation with a neural network of size O(N), where\nN denotes the number of elements, all of classical (e.g. finite element) approximation theory for\ncpwl functions can be lifted to generate statements about ReLU approximation. For instance, it is\nwell-known, that functions in the Sobolev space H2([0, 1]d) can be approximated by cpwl functions\non a regular triangulation in terms of L2([0, 1]d) with the rate 2/d. Similar as in the proof of\nTheorem 5.22, for every f ∈H2([0, 1]d) and every N ∈N there then exists a ReLU neural network\nΦN such that size(ΦN) = O(N) and\n∥f −ΦN∥L2([0,1]d) ≤C∥f∥H2([0,1]d)N−2\nd .\n65\nFinally, we can wonder how to approximate even smoother functions, i.e., those that have many\ncontinuous derivatives. Since more smoothness is a restrictive assumption on the set of functions\nto approximate, we would hope that this will allow us to have smaller neural networks. Essentially,\nwe desire a result similar to Theorem 4.9, but with the ReLU activation function.\nHowever, we will see in the following chapter, that the emulation of piecewise affine functions\non regular triangulations cannot yield the approximation rates of Theorem 4.9. To harness the\nsmoothness, it will be necessary to build ReLU neural networks that emulate polynomials. Sur-\nprisingly, we will see in Chapter 7 that polynomials can be very efficiently approximated by deep\nReLU neural networks.\nBibliography and further reading\nThe ReLU calculus introduced in Section 5.1 was similarly given in [174]. The fact that every\ncpwl function can be expressed as a maximum over a minimum of linear functions goes back to the\npapers [226, 225]; also see [169, 237].\nThe main result of Section 5.2, which shows that every cpwl function can be expressed by a\nReLU network, is then a straightforward consequence. This was first observed in [4], which also\nprovided bounds on the network size. These bounds were significantly improved in [85] for cwpl\nfunctions on triangular meshes that satisfy a local convexity condition. Under this assumption, it\nwas shown that the network size essentially only grows linearly with the number of pieces. The\npaper [136] showed that the convexity assumption is not necessary for this statement to hold. We\ngive a similar result in Section 5.3.2, using a simpler argument than [136]. The locally convex case\nfrom [85] is separately discussed in Section 5.3.3, as it allows for further improvements in some\nconstants.\nThe implications for the approximation of H¨older continuous functions discussed in Section 5.4,\nfollows by standard approximation theory for cpwl functions. For a general reference on splines\nand piecewise polynomial approximation see for instance [207]. Finally we mention that similar\nconvergence results can also be shown for other activation functions, see, e.g., [144].\n66\nExercises\nExercise 5.23. Let p : R →R be a polynomial of degree n ≥1 (with leading coefficient nonzero)\nand let s : R →R be a continuous sigmoidal activation function. Show that the identity map\nx 7→x : R →R belongs to N 1\n1 (p; 1, n + 1) but not to N 1\n1 (s; L) for any L ∈N.\nExercise 5.24. Consider cpwl functions f : R →R with n ∈N0 breakpoints (points where the\nfunction is not C1). Determine the minimal size required to exactly express every such f with a\ndepth-1 ReLU neural network.\nExercise 5.25. Show that, the notion of affine independence is invariant under permutations of\nthe points.\nExercise 5.26. Let τ = co(x0, . . . , xd) be a d-simplex. Show that the coefficients αi ≥0 such that\nPd\ni=0 αi = 1 and x = Pd\ni=0 αixi are unique for every x ∈τ.\nExercise 5.27. Let τ = co(η0, . . . , ηd) be a d-simplex. Show that the boundary of τ is given by\nSd\ni=0 co({η0, . . . , ηd}\\{ηi}).\n67\nChapter 6\nAffine pieces for ReLU neural\nnetworks\nIn the previous chapters, we observed some remarkable approximation results of shallow ReLU\nneural networks. In practice, however, deeper architectures are more common. To understand why,\nwe in this chapter we discuss some potential shortcomings of shallow ReLU networks compared to\ndeep ReLU networks.\nTraditionally, an insightful approach to study limitations of ReLU neural networks has been to\nanalyze the number of linear regions these functions can generate.\nDefinition 6.1. Let d ∈N, Ω⊆Rd, and let f : Ω→R be cpwl (see Definition 5.5). We say\nthat f has p ∈N pieces (or linear regions), if p is the smallest number of connected open\nsets (Ωi)p\ni=1 such that Sp\ni=1 Ωi = Ω, and f|Ωi is an affine function for all i = 1, . . . , p. We denote\nPieces(f, Ω) := p.\nFor d = 1 we call every point where f is not differentiable a break point of f.\nTo get an accurate cpwl approximation of a function, the approximating function needs to have\nmany pieces. The next theorem, corresponding to [62, Theorem 2], quantifies this statement.\nTheorem 6.2. Let −∞< a < b < ∞and f ∈C3([a, b]) so that f is not affine. Then there exists\na constant c > 0 depending only on\nR b\na\np\n|f′′(x)| dx so that\n∥g −f∥L∞([a,b]) > cp−2\nfor all cpwl g with at most p ∈N pieces.\nThe proof of the theorem is left to the reader, see Exercise 6.12.\nTheorem 6.2 implies that for ReLU neural networks we need architectures allowing for many\npieces, if we want to approximate non-linear functions to high accuracy. But how many pieces can\n68\nwe create for a fixed depth and width? We will establish a simple theoretical upper bound in Section\n6.1. Subsequently, we will investigate under which conditions these upper bounds are attainable\nin Section 6.2. This will reveal that certain functions necessitate very large shallow networks for\napproximation, whereas relatively small deep networks can also approximate them. These findings\nare presented in Section 6.3.\nFinally, we will question the practical relevance of this analysis by examining how many pieces\ntypical neural networks possess. Surprisingly, in Section 6.4 we will find that randomly initialized\ndeep neural networks on average do not have a number of pieces that is anywhere close to the\ntheoretical upper bound.\n6.1\nUpper bounds\nNeural networks are based on the composition and addition of neurons.\nThese two operations\nincrease the possible number of pieces in a very specific way. Figure 6.1 depicts the two operations\nand their effect. They can be described as follows:\n• Summation: Let Ω⊆R. The sum of two cpwl functions f1, f2 : Ω→R satisfies\nPieces(f1 + f2, Ω) ≤Pieces(f1, Ω) + Pieces(f2, Ω) −1.\n(6.1.1)\nThis holds because the sum is affine in every point where both f1 and f2 are affine. Therefore,\nthe sum has at most as many break points as f1 and f2 combined. Moreover, the number of\npieces of a univariate function equals the number of its break points plus one.\n• Composition: Let again Ω⊆R. The composition of two functions f1 : Rd →R and f2 : Ω→\nRd satisfies\nPieces(f1 ◦f2, Ω) ≤Pieces(f1, Rd) · Pieces(f2, Ω).\n(6.1.2)\nThis is because for each of the affine pieces of f2—let us call one of those pieces A ⊆R—we\nhave that f2 is either constant or injective on A. If it is constant, then f1 ◦f2 is constant. If\nit is injective, then Pieces(f1 ◦f2, A) = Pieces(f1, f2(A)) ≤Pieces(f1, Rd). Since this holds\nfor all pieces of f2 we get (6.1.2).\nThese considerations give the following result, which follows the argument of [227, Lemma 2.1].\nWe state it for general cpwl activation functions. The ReLU activation function corresponds to\np = 2.\nTheorem 6.3. Let L ∈N. Let σ be cpwl with p pieces. Then, every neural network with architecture\n(σ; 1, d1, . . . , dL, 1) has at most (p · width(Φ))L pieces.\nProof. The proof is via induction over the depth L. Let L = 1, and let Φ : R →R be a neural\nnetwork of architecture (σ; 1, d1, 1). Then\nΦ(x) =\nd1\nX\nk=1\nw(1)\nk σ(w(0)\nk x + b(0)\nk ) + b(1)\nfor x ∈R,\n69\nFigure 6.1: Top: Composition of two cpwl functions f1 ◦f2 can create a piece whenever the value\nof f2 crosses a level that is associated to a break point of f1. Bottom: Addition of two cpwl\nfunctions f1 + f2 produces a cpwl function that can have break points at positions where either f1\nor f2 has a break point.\nfor certain w(0), w(1), b(0) ∈Rd1 and b(1) ∈R. By (6.1.1), Pieces(Φ) ≤p · width(Φ).\nFor the induction step, assume the statement holds for L ∈N, and let Φ : R →R be a neural\nnetwork of architecture (σ; 1, d1, . . . , dL+1, 1). Then, we can write\nΦ(x) =\ndL+1\nX\nj=1\nwjσ(hj(x)) + b\nfor x ∈R,\nfor some w ∈RdL+1, b ∈R, and where each hj is a neural network of architecture (σ; 1, d1, . . . , dL, 1).\nUsing the induction hypothesis, each σ ◦hℓhas at most p · (p · width(Φ))L affine pieces. Hence\nΦ has at most width(Φ) · p · (p · width(Φ))L = (p · width(Φ))L+1 affine pieces. This completes the\nproof.\nTheorem 6.3 shows that there are limits to how many pieces can be created with a certain\narchitecture. It is noteworthy that the effects of the depth and the width of a neural network\nare vastly different. While increasing the width can polynomially increase the number of pieces,\nincreasing the depth can result in exponential increase. This is a first indication of the prowess of\ndepth of neural networks.\nTo understand the effect of this on the approximation problem, we apply the bound of Theorem\n6.3 to Theorem 6.2.\nTheorem 6.4. Let d0 ∈N and f ∈C3([0, 1]d0). Assume there exists a line segment s ⊆[0, 1]d0 of\npositive length such that 0 < c :=\nR\ns\np\n|f′′(x)| dx. Then, there exists C > 0 solely depending on c,\nsuch that for all ReLU neural networks Φ : Rd0 →R with L layers\n∥f −Φ∥L∞([0,1]d0) ≥c · (2width(Φ))−2L.\nTheorem 6.4 gives a lower bound on achievable approximation rates in dependence of the depth\nL. As target functions become smoother, we expect that we can achieve faster convergence rates\n70\n(cp. Chapter 4). However, without increasing the depth, it seems to be impossible to leverage such\nadditional smoothness.\nThis observation strongly indicates that deeper architectures can be superior. Before we can\nmake such statements, we first explore whether the upper bounds of Theorem 6.3 are even achiev-\nable.\n6.2\nTightness of upper bounds\nTo construct a ReLU neural network, that realizes the upper bound of Theorem 6.3, we first let\nh1 : [0, 1] →R be the hat function\nh1(x) :=\n(\n2x\nif x ∈[0, 1\n2]\n2 −2x\nif x ∈[1\n2, 1].\nThis function can be expressed by a ReLU neural network of depth one and with two nodes\nh1(x) = σReLU(2x) −σReLU(4x −2)\nfor all x ∈[0, 1].\n(6.2.1)\nWe recursively set hn := hn−1 ◦h1 for all n ≥2, i.e., hn = h1 ◦· · · ◦h1 is the n-fold composition of\nh1. Since h1 : [0, 1] →[0, 1], we have hn : [0, 1] →[0, 1] and\nhn ∈N 1\n1 (σReLU; n, 2).\nIt turns out that this function has a rather interesting behavior. It is a “sawtooth” function with\n2n−1 spikes, see Figure 6.2.\nLemma 6.5. Let n ∈N. It holds for all x ∈[0, 1]\nhn(x) =\n(\n2n(x −i2−n)\nif i ≥0 is even and x ∈[i2−n, (i + 1)2−n]\n2n((i + 1)2−n −x)\nif i ≥1 is odd and x ∈[i2−n, (i + 1)2−n].\nProof. The case n = 1 holds by definition. We proceed by induction, and assume the statement\nholds for n.\nLet x ∈[0, 1/2] and i ≥0 even such that x ∈[i2−(n+1), (i + 1)2−(n+1)].\nThen\n2x ∈[i2−n, (i + 1)2−n]. Thus\nhn(h1(x)) = hn(2x) = 2n(2x −i2−n) = 2n+1(x −i2−n+1).\nSimilarly, if x ∈[0, 1/2] and i ≥1 odd such that x ∈[i2−(n+1), (i + 1)2−(n+1)], then h1(x) = 2x ∈\n[i2−n, (i + 1)2−n] and\nhn(h1(x)) = hn(2x) = 2n(2x −(i + 1)2−n) = 2n+1(x −(i + 1)2−n+1).\nThe case x ∈[1/2, 1] follows by observing that hn+1 is symmetric around 1/2.\nThe neural network hn has size O(n) and is piecewise linear on at least 2n pieces. This shows\nthat the number of pieces can indeed increase exponentially in the neural network size, also see the\nupper bound in Theorem 6.3.\n71\n0\n1\n1\nh1\n0\n1\n1\nh2\n0\n1\n1\nh3\nFigure 6.2: The functions hn in Lemma 6.5.\n6.3\nDepth separation\nNow that we have established how increasing the depth can lead to exponentially more pieces than\nincreasing the width, we can deduce a so-called “depth-separation” result shown by Telgarsky in\n[227, 228]. Such statements verify the existence of functions that can easily be approximated by\ndeep neural networks, but require much larger size when approximated by shallow neural networks.\nThe following theorem, along with its proof, is presented similarly in Telgarsky’s lecture notes [229].\nTheorem 6.6. For every n ∈N there exists a neural network f ∈N 1\n1 (σReLU; n2 + 3, 2) such that\nfor any g ∈N 1\n1 (σReLU; n, 2n−1) holds\nZ 1\n0\n|f(x) −g(x)| dx ≥1\n32.\nThe neural network f may have quadratically more layers than g, but width(g) = 2n−1 and\nwidth(f) = 2. Hence the size of g may be exponentially larger than the size of f, but nonetheless no\nsuch g can approximate f. Thus even exponential increase in width cannot necessarily compensate\nfor increase in depth. The proof is based on the following observations stated in [228]:\n(i) Functions with few oscillations poorly approximate functions with many oscillations,\n(ii) neural networks with few layers have few oscillations,\n(iii) neural networks with many layers can have many oscillations.\nProof of Theorem 6.6. Fix n ∈N.\nLet f := hn2+3 ∈N 1\n1 (σReLU; n2 + 3, 2).\nFor arbitrary g ∈\nN 1\n1 (σReLU; n, 2n−1), by Theorem 6.3, g is piecewise linear with at most (2 · 2n−1)n = 2n2 break\npoints. The function f is the sawtooth function with 2n2+2 spikes. The number of triangles formed\nby the graph of f and the constant line at 1/2 equals 2n2+3 −1, each with area 2−(n2+5), see\nFigure 6.3. For the m triangles in between two break points of g, the graph of g does not cross at\n72\n1\n2\n0\n1\n1\n0\n1\nFigure 6.3: Left: The functions hn form 2n −1 triangles with the line at 1/2, each with area\n2−(n+2). Right: For an affine function with m (in this sketch m = 5) triangles in between two\nbreak points, the function can cross at most ⌈m/2⌉+ 1 ≤m/2 + 2 of them. Figure adapted from\n[229, Section 5].\nleast m −(m/2 + 2) = m/2 −2 of them. Thus we can bound\nZ 1\n0\n|f(x) −g(x)| dx ≥\n\n\n\n\n\n1\n2(\n2n2+3 −1 −2n2\n|\n{z\n}\n≥triangles on an interval\nwithout break point of g\n) −\n2 · 2n2\n| {z }\n≥2·(pieces of g)\n\n\n\n\n\n|\n{z\n}\n≥missed triangles\n2−(n2+5)\n|\n{z\n}\narea of a triangle\n≥(2n2+2 −3 · 2n2) · 2−(n2+5)\n≥2n2 · 2−(n2+5) = 1\n32,\nwhich concludes the proof.\n6.4\nNumber of pieces in practice\nWe have seen in Theorem 6.3 that deep neural networks can have many more pieces than their\nshallow counterparts. This begs the question if deep neural networks tend to generate more pieces\nin practice. More formally: If we randomly initialize the weights of a neural network, what is the\nexpected number of linear regions? Will this number scale exponentially with the depth? This\nquestion was analyzed in [82], and surprisingly, it was found that the number of pieces of randomly\ninitialized neural networks typically does not depend exponentially on the depth. In Figure 6.4, we\ndepict two neural networks, one shallow and one deep, that were randomly initialized according to\nHe initialization [86]. Both neural networks have essentially the same number of pieces (114 and\n110) and there is no clear indication that one has a deeper architecture than the other.\nIn the following, we will give a simplified version of the main result of [82] to show why random\ndeep neural networks often behave like shallow neural networks.\nWe recall from Figure 6.1 that pieces are generated through composition of two functions f1\nand f2, if the values of f2 cross a level that is associated to a break point of f1. In the case of a\nsimple neuron of the form\nx 7→σReLU(⟨a, h(x)⟩+ b)\n73\nFigure\n6.4:\nTwo\nrandomly\ninitialized\nneural\nnetworks\nΦ1\nand\nΦ2\nwith\narchitectures\n(σReLU; 1, 10, 10, 1) and (σReLU; 1, 5, 5, 5, 5, 5, 1).\nThe initialization scheme was He initialization\n[86]. The number of linear regions equals 114 and 110, respectively.\nwhere h is a cpwl function, a is a vector, and b is a scalar, many pieces can be generated if ⟨a, h(x)⟩\ncrosses the −b level often.\nIf a, b are random variables, and we know that h does not oscillate too much, then we can\nquantify the probability of ⟨a, h(x)⟩crossing the −b level often. The following lemma from [115,\nLemma 3.1] provides the details.\nLemma 6.7. Let c > 0 and let h: [0, c] →R be a cpwl function on [0, c]. Let t ∈N, let A ⊆R be\na Lebesgue measurable set, and assume that for every y ∈A it holds that\n|{x ∈[0, c] | h(x) = y}| ≥t.\nThen, c∥h′∥L∞≥∥h′∥L1 ≥|A| · t, where |A| is the Lebesgue measure of A.\nIn particular, if h has at most P ∈N pieces and ∥h′∥L1 is finite, then it holds for all δ > 0 that\nfor all t ≤P\nP [|{x ∈[0, c] | h(x) = U}| ≥t] ≤∥h′∥L1\nδt\n,\nP [|{x ∈[0, c] | h(x) = U}| > P] = 0,\nwhere U is a uniformly distributed variable on [−δ/2, δ/2].\nProof. We will assume c = 1. The general case then follows by considering ˜h(x) = h(x/c).\nLet for (ci)P+1\ni=1 ⊆[0, 1] with c1 = 0, cP+1 = 1 and ci ≤ci+1 for all i = 1, . . . , P + 1 the pieces of\nh be given by ((ci, ci+1))P\ni=1. We denote\nV1 := [0, c2],\nVi := (ci, ci+1] for i = 1, . . . , P\n74\nand for j = i, . . . , P\neVi :=\ni−1\n[\nj=1\nVj.\nWe define, for n ∈N ∪{∞}\nTi,n := h(Vi) ∩\nn\ny ∈A\n\f\f\f |{x ∈eVi | h(x) = y}| = n −1\no\n.\nIn words, Ti,n contains the values of A that are hit on Vi for the nth time. Since h is cpwl, we\nobserve that for all i = 1, . . . , P\n(i) Ti,n1 ∩Ti,n2 = ∅for all n1, n2 ∈N ∪{∞}, n1 ̸= n2,\n(ii) Ti,∞∪S∞\nn=1 Ti,n = h(Vi) ∩A,\n(iii) Ti,n = ∅for all P < n < ∞,\n(iv) |Ti,∞| = 0.\nNote that, since h is affine on Vi it holds that h′ = |h(Vi)|/|Vi| on Vi. Hence, for t ≤P\n∥h′∥L1 ≥\nP\nX\ni=1\n|h(Vi)| ≥\nP\nX\ni=1\n|h(Vi) ∩A|\n=\nP\nX\ni=1\n ∞\nX\nn=1\n|Ti,n|\n!\n+ |Ti,∞|\n=\nP\nX\ni=1\n∞\nX\nn=1\n|Ti,n|\n≥\nt\nX\nn=1\nP\nX\ni=1\n|Ti,n|,\nwhere the first equality follows by (i), (ii), the second by (iv), and the last inequality by (iii).\nNote that, by assumption for all n ≤t every y ∈A is an element of Ti,n or Ti,∞for some i ≤P.\nTherefore, by (iv)\nP\nX\ni=1\n|Ti,n| ≥|A|,\nwhich completes the proof.\nLemma 6.7 applied to neural networks essentially states that, in a single neuron, if the bias\nterm is chosen uniformly randomly on an interval of length δ, then the probability of generating at\nleast t pieces by composition scales reciprocal to t.\nNext, we will analyze how Lemma 6.7 implies an upper bound on the number of pieces generated\nin a randomly initialized neural network. For simplicity, we only consider random biases in the\nfollowing, but mention that similar results hold if both the biases and weights are random variables\n[82].\n75\nDefinition 6.8. Let L ∈N, (d0, d1, . . . , dL, 1) ∈NL+2 and W (ℓ) ∈Rdℓ+1×dℓfor ℓ= 0, . . . , L. Fur-\nthermore, let δ > 0 and let the bias vectors b(ℓ) ∈Rdℓ+1, for ℓ= 0, . . . , L, be random variables such\nthat each entry of each b(ℓ) is independently and uniformly distributed on the interval [−δ/2, δ/2].\nWe call the associated ReLU neural network a random-bias neural network.\nTo apply Lemma 6.7 to a single neuron with random biases, we also need some bound on the\nderivative of the input to the neuron.\nDefinition 6.9. Let L ∈N, (d0, d1, . . . , dL, 1) ∈NL+2, and W (ℓ) ∈Rdℓ+1×dℓand b(ℓ) ∈Rdℓ+1 for\nℓ= 0, . . . , L. Moreover let δ > 0.\nFor ℓ= 1, . . . , L + 1, i = 1, . . . , dℓintroduce the functions\nηℓ,i(x; (W (j), b(j))ℓ−1\nj=0) = (W (ℓ−1)x(ℓ−1))i\nfor x ∈Rd0,\nwhere x(ℓ−1) is as in (2.1.1). We call\nν\n\u0010\n(W (ℓ))L\nℓ=1, δ\n\u0011\n:= max\n( \r\r\rη′\nℓ,i( · ; (W (j), b(j))ℓ−1\nj=0)\n\r\r\r\n2\n\f\f\f\f\f\n(b(j))L\nj=0 ∈\nL\nY\nj=0\n[−δ/2, δ/2]dj+1, ℓ= 1, . . . , L, i = 1, . . . , dℓ\n)\nthe maximal internal derivative of Φ.\nWe can now formulate the main result of this section.\nTheorem 6.10. Let L ∈N and let (d0, d1, . . . , dL, 1) ∈NL+2. Let δ ∈(0, 1]. Let W (ℓ) ∈Rdℓ+1×dℓ,\nfor ℓ= 0, . . . , L, be such that ν\n\u0010\n(W (ℓ))L\nℓ=0, δ\n\u0011\n≤Cν for a Cν > 0.\nFor an associated random-bias neural network Φ, we have that for a line segment s ⊆Rd0 of\nlength 1\nE[Pieces(Φ, s)] ≤1 + d1 + Cν\nδ (1 + (L −1) ln(2width(Φ)))\nL\nX\nj=2\ndj.\n(6.4.1)\nProof. Let W (ℓ) ∈Rdℓ+1×dℓfor ℓ= 0, . . . , L. Moreover, let b(ℓ) ∈[−δ/2, δ/2]dℓ+1 for ℓ= 0, . . . , L\nbe uniformly distributed random variables. We denote\nθℓ: s →Rdℓ\nx 7→(ηℓ,i(x; (W (j), b(j))ℓ−1\nj=0))dℓ\ni=1.\n76\nLet κ: s →[0, 1] be an isomorphism.\nSince each coordinate of θℓis cpwl, there are points\nx0, x1, . . . , xqℓ∈s with κ(xj) < κ(xj+1) for j = 0, . . . , qℓ−1, such that θℓis affine (as a function\ninto Rdℓ) on [κ(xj), κ(xj+1)] for all j = 0, . . . , qℓ−1 as well as on [0, κ(x0)] and [κ(xqℓ), 1].\nWe will now inductively find an upper bound on the qℓ.\nLet ℓ= 2, then\nθ2(x) = W (1)σReLU(W (0)x + b(0)).\nSince W (1) · +b(1) is an affine function, it follows that θ2 can only be non-affine in points where\nσReLU(W (0) · +b(0)) is not affine. Therefore, θ2 is only non-affine if one coordinate of W (0) · +b(0)\nintersects 0 nontrivially. This can happen at most d1 times. We conclude that we can choose\nq2 = d1.\nNext, let us find an upper bound on qℓ+1 from qℓ. Note that\nθℓ+1(x) = W (ℓ)σReLU(θℓ(x) + b(ℓ−1)).\nNow θℓ+1 is affine in every point x ∈s where θℓis affine and (θℓ(x)+b(ℓ−1))i ̸= 0 for all coordinates\ni = 1, . . . , dℓ. As a result, we have that we can choose qℓ+1 such that\nqℓ+1 ≤qℓ+\n\f\f{x ∈s | (θℓ(x) + b(ℓ−1))i = 0 for at least one i = 1, . . . , dℓ}\n\f\f.\nTherefore, for ℓ≥2\nqℓ+1 ≤d1 +\nℓ\nX\nj=3\n\f\f{x ∈s | (θj(x) + b(j))i = 0 for at least one i = 1, . . . , dj}\n\f\f\n≤d1 +\nℓ\nX\nj=2\ndj\nX\ni=1\n\f\f{x ∈s | ηj,i(x) = −b(j)\ni }\n\f\f.\nBy Theorem 6.3, we have that\nPieces\n\u0010\nηℓ,i( · ; (W (j), b(j))ℓ−1\nj=0), s\n\u0011\n≤(2width(Φ))ℓ−1.\nWe define for k ∈N ∪{∞}\npk,ℓ,i := P\nh\f\f{x ∈s | ηℓ,i(x) = −b(ℓ)\ni }\n\f\f ≥k\ni\nThen by Lemma 6.7\npk,ℓ,i ≤Cν\nδk\nand for k > (2width(Φ))ℓ−1\npk,ℓ,i = 0.\n77\nIt holds\nE\n\n\nL\nX\nj=2\ndj\nX\ni=1\n\f\f\f\nn\nx ∈s\n\f\f\f ηj,i(x) = −b(j)\ni\no \f\f\f\n\n\n≤\nL\nX\nj=2\ndj\nX\ni=1\n∞\nX\nk=1\nk · P\nh\f\f\f\nn\nx ∈s\n\f\f\f ηj,i(x) = −b(j)\ni\no \f\f\f = k\ni\n≤\nL\nX\nj=2\ndj\nX\ni=1\n∞\nX\nk=1\nk · (pk,j,i −pk+1,j,i).\nThe inner sum can be bounded by\n∞\nX\nk=1\nk · (pk,j,i −pk+1,j,i) =\n∞\nX\nk=1\nk · pk,j,i −\n∞\nX\nk=1\nk · pk+1,j,i\n=\n∞\nX\nk=1\nk · pk,j,i −\n∞\nX\nk=2\n(k −1) · pk,j,i\n= p1,j,i +\n∞\nX\nk=2\npk,j,i\n=\n∞\nX\nk=1\npk,j,i\n≤Cνδ−1\n(2width(Φ))L−1\nX\nk=1\n1\nk\n≤Cνδ−1\n \n1 +\nZ (2width(Φ))L−1\n1\n1\nx dx\n!\n≤Cνδ−1(1 + (L −1) ln((2width(Φ)))).\nWe conclude that, in expectation, we can bound qL+1 by\nd1 + Cνδ−1(1 + (L −1) ln(2width(Φ)))\nL\nX\nj=2\ndj.\nFinally, since θL = ΦL+1|s, it follows that\nPieces(Φ, s) ≤qL+1 + 1\nwhich yields the result.\nRemark 6.11. We make the following observations about Theorem 6.10:\n• Non-exponential dependence on depth: If we consider (6.4.1), we see that the number of pieces\nscales in expectation essentially like O(LN), where N is the total number of neurons of the\narchitecture. This shows that in expectation, the number of pieces is linear in the number of\nlayers, as opposed to the exponential upper bound of Theorem 6.3.\n78\n• Maximal internal derivative: Theorem 6.10 requires the weights to be chosen such that the\nmaximal internal derivative is bounded by a certain number. However, if they are randomly\ninitialized in such a way that with high probability the maximal internal derivative is bounded\nby a small number, then similar results can be shown. In practice, weights in the ℓth layer are\noften initialized according to a centered normal distribution with standard deviation\np\n2/dℓ,\n[86]. Due to the anti-proportionality of the variance to the width of the layers it is achieved\nthat the internal derivatives remain bounded with high probability, independent of the width\nof the neural networks. This explaines the observation from Figure 6.4.\nBibliography and further reading\nEstablishing bounds on the number of linear regions of a ReLU network has been a popular tool\nto investigate the complexity of ReLU neural networks, see [152, 185, 4, 210, 82].\nThe bound\npresented in Section 6.1, is based on [227]. In addition to this bound, the paper also presents the\ndepth separation result discussed in Section 6.3. The proof techniques employed there have inspired\nnumerous subsequent works in the field.\nTogether with the lower bound on the number of required linear regions given in [62], this\nanalysis shows how depth can be a limiting factor in terms of achievable convergence rates, as\nstated in Theorem 6.4.\nFor the construction of the sawtooth function in Section 6.2, and the depth separation result in\nSection 6.3 follow the arguments in [227, 228, 229]. Beyond Telgarsky’s work, other notable depth\nseparation results include [60, 199, 4]. Moreover, closely related to such statements is the 1987\nthesis by H˚astad [101], which considers the limitations of logic circuits in terms of depth.\nFinally, the analysis of the number of pieces deep neural networks attained with random intial-\nization (Section 6.4) is based on [82] and [115].\n79\nExercises\nExercise 6.12. Let −∞< a < b < ∞and let f ∈C3([a, b])\\P1. Denote by p(ε) ∈N the minimal\nnumber of intervals partitioning [a, b], such that a (not necessarily continuous) piecewise linear\nfunction on p(ε) intervals can approximate f on [a, b] uniformly up to error ε > 0. In this exercise,\nwe wish to show\nlim inf\nε↘0 p(ε)√ε > 0.\n(6.4.2)\nTherefore, we can find a constant C > 0 such that ε ≥Cp(ε)−2 for all ε > 0. This shows a variant\nof Theorem 6.2. Proceed as follows to prove (6.4.2):\n(i) Fix ε > 0 and let a = x0 < x1 · · · < xp(ε) = b be a partitioning into p(ε) pieces.\nFor\ni = 0, . . . , p(ε) −1 and x ∈[xi, xi+1] let\nei(x) := f(x) −\n\u0012\nf(xi) + f(xi+1) −f(xi)\nxi+1 −xi\n(x −xi)\n\u0013\n.\nShow that |ei(x)| ≤2ε for all x ∈[xi, xi+1].\n(ii) With hi := xi+1 −xi and mi := (xi + xi+1)/2 show that\nmax\nx∈[xi,xi+1] |ei(x)| = h2\ni\n8 |f′′(mi)| + O(h3\ni ).\n(iii) Assuming that c := infx∈[a,b] |f′′(x)| > 0 show that\nlim inf\nε↘0 p(ε)√ε ≥1\n4\nZ b\na\np\n|f′′(x)| dx.\n(iv) Conclude that (6.4.2) holds for general non-linear f ∈C3([a, b]).\nExercise 6.13. Show that, for L = 1, Theorem 6.3 holds for piecewise smooth functions, when\nreplacing the number of affine pieces by the number of smooth pieces. These are defined by replacing\n“affine” by “smooth” (meaning C∞) in Definition 6.1.\nExercise 6.14. Show that, for L > 1, Theorem 6.3 does notx hold for piecewise smooth functions,\nwhen replacing the number of affine pieces by the number of smooth pieces.\nExercise 6.15. For p ∈N, p > 2 and n ∈N, construct a function h(p)\nn\nsimilar to hn of (6.5), such\nthat h(p)\nn\n∈N 1\n1 (σReLU; n, p) and such that h(p)\nn\nhas pn pieces and size O(p2n).\n80\nChapter 7\nDeep ReLU neural networks\nIn the previous chapter, we observed that many layers are a necessary prerequisite for ReLU neural\nnetworks to approximate smooth functions with high rates. We now analyze which depth is sufficient\nto achieve good approximation rates for smooth functions.\nTo approximate smooth functions efficiently, one of the main tools in Chapter 4 was to rebuild\npolynomial-based functions, such as higher-order B-splines. For smooth activation functions, we\nwere able to reproduce polynomials by using the nonlinearity of the activation functions. This\nargument certainly cannot be repeated for the piecewise linear ReLU. On the other hand, up until\nnow, we have seen that deep ReLU neural networks are extremely efficient at producing the strongly\noscillating sawtooth functions discussed in Lemma 6.5.\nThe main observation this chapter is that the efficient representation of sawtooth functions\nis intimately linked to the approximation of the square function and hence allows very efficient\napproximations of polynomial functions. This observation was first made by Dmitry Yarotsky [245]\nin 2016, and the present chapter is primarily based on this paper.\nFirst, in Section 7.1, we will give an efficient neural network approximation of the squaring func-\ntion. Second, in Section 7.2, we will demonstrate how the squaring neural network can be modified\nto yield a neural network that approximates the function that multiplies its inputs. Using these\ntwo tools, we conclude in Section 7.3 that deep ReLU neural networks can efficiently approximate\nk-times continuously differentiable functions with H¨older continuous derivatives.\n7.1\nThe square function\nIn this section, we will show that the square function x 7→x2 can be approximated very efficiently\nby a deep neural network.\nProposition 7.1. Let n ∈N. Then\nsn(x) := x −\nn\nX\nj=1\nhj(x)\n22j\nis a piecewise linear function on [0, 1] with break points xn,j = j2−n, j = 0, . . . , 2n. Moreover,\nsn(xn,k) = x2\nn,k for all k = 0, . . . , 2n, i.e. sn is the piecewise linear interpolant of x2 on [0, 1].\n81\nProof. The statement holds for n = 1. We proceed by induction. Assume the statement holds for\nsn and let k ∈{0, . . . , 2n+1}. By Lemma 6.5, hn+1(xn+1,k) = 0 whenever k is even. Hence for even\nk ∈{0, . . . , 2n+1}\nsn+1(xn+1,k) = xn+1,k −\nn+1\nX\nj=1\nhj(xn+1,k)\n22j\n= sn(xn+1,k) −hn+1(xn+1,k)\n22(n+1)\n= sn(xn+1,k) = x2\nn+1,k,\nwhere we used the induction assumption sn(xn+1,k) = x2\nn+1,k for xn+1,k = k2−(n+1) = k\n22−n =\nxn,k/2.\nNow let k ∈{1, . . . , 2n+1 −1} be odd. Then by Lemma 6.5, hn+1(xn+1,k) = 1. Moreover,\nsince sn is linear on [xn,(k−1)/2, xn,(k+1)/2] = [xn+1,k−1, xn+1,k+1] and xn+1,k is the midpoint of this\ninterval,\nsn+1(xn+1,k) = sn(xn+1,k) −hn+1(xn+1,k)\n22(n+1)\n= 1\n2(x2\nn+1,k−1 + x2\nn+1,k+1) −\n1\n22(n+1)\n= (k −1)2\n22(n+1)+1 + (k + 1)2\n22(n+1)+1 −\n2\n22(n+1)+1\n= 1\n2\n2k2\n22(n+1) =\nk2\n22(n+1) = x2\nn+1,k.\nThis completes the proof.\nLemma 7.2. For n ∈N, it holds\nsup\nx∈[0,1]\n|x2 −sn(x)| ≤2−2n−1.\nMoreover sn ∈N 1\n1 (σReLU; n, 3), and size(sn) ≤7n and depth(sn) = n.\nProof. Set en(x) := x2 −sn(x). Let x be in the interval [xn,k, xn,k+1] = [k2−n, (k + 1)2−n] of length\n2−n. Since sn is the linear interpolant of x2 on this interval, we have\n|e′\nn(x)| =\n\f\f\f\f\f2x −\nx2\nn,k+1 −x2\nn,k\n2−n\n\f\f\f\f\f =\n\f\f\f\f2x −2k + 1\n2n\n\f\f\f\f ≤1\n2n .\nThus en : [0, 1] →R has Lipschitz constant 2−n. Since en(xn,k) = 0 for all k = 0, . . . , 2n, and the\nlength of the interval [xn,k, xn,k+1] equals 2−n we get\nsup\nx∈[0,1]\n|en(x)| ≤1\n22−n2−n = 2−2n−1.\n82\nx\nh1(x)\n. . .\nx\nx\nh1(x)\ns1(x)\nh2(x)\ns2(x)\nh3(x)\nsn−1(x)\nhn(x)\nsn(x)\nFigure 7.1: The neural networks h1(x) = σReLU(2x)−σReLU(4x−2) and sn(x) = σReLU(sn−1(x))−\nhn(x)/22n where hn = h1 ◦hn−1.\nFinally, to see that sn can be represented by a neural network of the claimed architecture, note\nthat for n ≥2\nsn(x) = x −\nn\nX\nj=1\nhj(x)\n22j\n= sn−1(x) −hn(x)\n22n\n= σReLU ◦sn−1(x) −h1 ◦hn−1(x)\n22n\n.\nHere we used that sn−1 is the piecewise linear interpolant of x2, so that sn−1(x) ≥0 and thus\nsn−1(x) = σReLU(sn−1(x)) for all x ∈[0, 1]. Hence sn is of depth n and width 3, see Figure 7.1.\nIn conclusion, we have shown that sn : [0, 1] →[0, 1] approximates the square function uniformly\non [0, 1] with exponentially decreasing error in the neural network size. Note that due to Theorem\n6.4, this would not be possible with a shallow neural network, which can at best interpolate x2 on\na partition of [0, 1] with polynomially many (w.r.t. the neural network size) pieces.\n7.2\nMultiplication\nAccording to Lemma 7.2, depth can help in the approximation of x 7→x2, which, on first sight,\nseems like a rather specific example.\nHowever, as we shall discuss in the following, this opens\nup a path towards fast approximation of functions with high regularity, e.g., Ck([0, 1]d) for some\nk > 1. The crucial observation is that, via the polarization identity we can write the product of\ntwo numbers as a sum of squares\nx · y = (x + y)2 −(x −y)2\n4\n(7.2.1)\nfor all x, y ∈R. Efficient approximation of the operation of multiplication allows efficient ap-\nproximation of polynomials. Those in turn are well-known to be good approximators for functions\nexhibiting k ∈N derivatives. Before exploring this idea further in the next section, we first make\nprecise the observation that neural networks can efficiently approximate the multiplication of real\nnumbers.\nWe start with the multiplication of two numbers, in which case neural networks of logarithmic\nsize in the desired accuracy are sufficient.\n83\nLemma 7.3. For every ε > 0 there exists a ReLU neural network Φ×\nε : [−1, 1]2 →[−1, 1] such that\nsup\nx,y∈[−1,1]\n|x · y −Φ×\nε (x, y)| ≤ε,\nand it holds size(Φ×\nε ) ≤C · (1 + | log(ε)|) and depth(Φ×\nε ) ≤C · (1 + | log(ε)|) for a constant C > 0\nindependent of ε. Moreover, Φ×\nε (x, y) = 0 if x = 0 or y = 0.\nProof. With n = ⌈| log4(ε)|⌉, define the neural network\nΦ×\nε (x, y) :=sn\n\u0012σReLU(x + y) + σReLU(−x −y)\n2\n\u0013\n−sn\n\u0012σReLU(x −y) + σReLU(y −x)\n2\n\u0013\n.\n(7.2.2)\nSince |a| = σReLU(a) + σReLU(−a), by (7.2.1) we have for all x, y ∈[−1, 1]\n\f\fx · y −Φ×\nε (x, y)\n\f\f =\n\f\f\f\f\n(x + y)2 −(x −y)2\n4\n−\n\u0012\nsn\n\u0012|x + y|\n2\n\u0013\n−sn\n\u0012|x −y|\n2\n\u0013\u0013\f\f\f\f\n=\n\f\f\f\f\f\n4(x+y\n2 )2 −4(x−y\n2 )2\n4\n−4sn(|x+y|\n2\n) −4sn(|x−y|\n2\n)\n4\n\f\f\f\f\f\n≤4(2−2n−1 + 2−2n−1)\n4\n= 4−n ≤ε,\nwhere we used |x+y|/2, |x−y|/2 ∈[0, 1]. We have depth(Φ×\nε ) = 1+depth(sn) = 1+n ≤1+⌈log4(ε)⌉\nand size(Φ×\nε ) ≤C + 2size(sn) ≤Cn ≤C · (1 −log(ε)) for some constant C > 0.\nThe fact that Φ×\nε maps from [−1, 1]2 →[−1, 1] follows by (7.2.2) and because sn : [0, 1] →[0, 1].\nFinally, if x = 0, then Φ×\nε (x, y) = sn(|x + y|) −sn(|x −y|) = sn(|y|) −sn(|y|) = 0. If y = 0 the same\nargument can be made.\nIn a similar way as in Proposition 4.8 and Lemma 5.11, we can apply operations with two inputs\nin the form of a binary tree to extend them to an operation on arbitrary many inputs.\nProposition 7.4. For every n ≥2 and ε > 0 there exists a ReLU neural network Φ×\nn,ε : [−1, 1]n →\n[−1, 1] such that\nsup\nxj∈[−1,1]\n\f\f\f\f\f\f\nn\nY\nj=1\nxj −Φ×\nn,ε(x1, . . . , xn)\n\f\f\f\f\f\f\n≤ε,\nand it holds size(Φ×\nn,ε) ≤Cn · (1 + | log(ε/n)|) and depth(Φ×\nn,ε) ≤C log(n)(1 + | log(ε/n)|) for a\nconstant C > 0 independent of ε and n.\n84\nProof. We begin with the case n = 2k. For k = 1 let ˜Φ×\n2,δ := Φ×\nδ . If k ≥2 let\n˜Φ×\n2k,δ := Φ×\nδ ◦\n\u0010\n˜Φ×\n2k−1,δ, ˜Φ×\n2k−1,δ\n\u0011\n.\nUsing Lemma 7.3, we find that this neural network has depth bounded by\ndepth\n\u0010\n˜Φ×\n2k,δ\n\u0011\n≤kdepth(Φ×\nδ ) ≤Ck · (1 + | log(δ)|) ≤C log(n)(1 + | log(δ)|).\nObserving that the number of occurences of Φ×\nδ equals Pk−1\nj=0 2j ≤n, the size of ˜Φ×\n2k,δ can bounded\nby Cnsize(Φ×\nδ ) ≤Cn · (1 + | log(δ)|).\nTo estimate the approximation error, denote with x = (xj)2k\nj=1\nek :=\nsup\nxj∈[−1,1]\n\f\f\f\f\f\f\nY\nj≤2k\nxj −˜Φ×\n2k,δ(x)\n\f\f\f\f\f\f\n.\nThen, using short notation of the type x≤2k−1 := (x1, . . . , x2k−1),\nek =\nsup\nxj∈[−1,1]\n\f\f\f\f\f\f\n2k\nY\nj=1\nxj −Φ×\nδ\n\u0010\n˜Φ×\n2k−1,δ(x≤2k−1), ˜Φ×\n2k−1,δ(x>2k−1)\n\u0011\n\f\f\f\f\f\f\n≤δ +\nsup\nxj∈[−1,1]\n\n\n\f\f\f\f\f\f\nY\nj≤2k−1\nxj\n\f\f\f\f\f\f\nek−1 +\n\f\f\f˜Φ×\n2k−1,δ(x>2k−1)\n\f\f\f ek−1\n\n\n≤δ + 2ek−1 ≤δ + 2(δ + 2ek−2) ≤· · · ≤δ\nk−2\nX\nj=0\n2j + 2k−1e1\n≤2kδ = nδ = ε.\nHere we used e1 ≤δ, and that ˜Φ×\n2k,δ maps [−1, 1]2k−1 to [−1, 1], which is a consequence of Lemma\n7.3.\nThe case for general n ≥2 (not necessarily n = 2k) is treated similar as in Lemma 5.11, by\nreplacing some Φ×\nδ neural networks with identity neural networks.\nFinally, setting δ := ε/n and Φ×\nn,ε := ˜Φ×\nn,δ concludes the proof.\n7.3\nCk,s functions\nWe will now discuss the implications of our observations in the previous sections for the approxi-\nmation of functions in the class Ck,s.\nDefinition 7.5. Let k ∈N0, s ∈[0, 1] and Ω⊆Rd. Then\n∥f∥Ck,s(Ω) := sup\nx∈Ω\nmax\n{α∈Nd\n0 | |α|≤k} |Dαf(x)|\n+\nsup\nx̸=y∈Ω\nmax\n{α∈Nd\n0 | |α|=k}\n|Dαf(x) −Dαf(y)|\n∥x −y∥s\n2\n,\n(7.3.1)\n85\nand we denote by Ck,s(Ω) the set of functions f ∈Ck(Ω) for which ∥f∥Ck,s(Ω) < ∞.\nNote that these spaces are ordered according to\nCk(Ω) ⊇Ck,s(Ω) ⊇Ck,t(Ω) ⊇Ck+1(Ω)\nfor all 0 < s ≤t ≤1.\nIn order to state our main result, we first recall a version of Taylor’s remainder formula for\nCk,s(Ω) functions.\nLemma 7.6. Let d ∈N, k ∈N, s ∈[0, 1], Ω= [0, 1]d and f ∈Ck,s(Ω). Then for all a, x ∈Ω\nf(x) =\nX\n{α∈Nd\n0 | 0≤|α|≤k}\nDαf(a)\nα!\n(x −a)α + Rk(x)\n(7.3.2)\nwhere with h := maxi≤d |ai −xi| we have |Rk(x)| ≤hk+s dk+1/2\nk!\n∥f∥Ck,s(Ω).\nProof. First, for a function g ∈Ck(R) and a, t ∈R\ng(t) =\nk−1\nX\nj=0\ng(j)(a)\nj!\n(t −a)j + g(k)(ξ)\nk!\n(t −a)k\n=\nk\nX\nj=0\ng(j)(a)\nj!\n(t −a)j + g(k)(ξ) −g(k)(a)\nk!\n(t −a)k,\nfor some ξ between a and t. Now let f ∈Ck,s(Rd) and a, x ∈Rd. Thus with g(t) := f(a+t·(x−a))\nholds for f(x) = g(1)\nf(x) =\nk−1\nX\nj=0\ng(j)(0)\nj!\n+ g(k)(ξ)\nk!\n.\nBy the chain rule\ng(j)(t) =\nX\n{α∈Nd\n0 | |α|=j}\n\u0012j\nα\n\u0013\nDαf(a + t · (x −a))(x −a)α,\nwhere we use the multivariate notations\n\u0000 j\nα\n\u0001\n=\nj!\nα! =\nj!\nQd\nj=1 αj! and (x −a)α = Qd\nj=1(xj −aj)αj.\n86\nHence\nf(x) =\nX\n{α∈Nd\n0 | 0≤|α|≤k}\nDαf(a)\nα!\n(x −a)α\n|\n{z\n}\n∈Pk\n+\nX\n|α|=k\nDαf(a + ξ · (x −a)) −Dαf(a)\nα!\n(x −a)α\n|\n{z\n}\n=:Rk\n,\nfor some ξ ∈[0, 1]. Using the definition of h, the remainder term can be bounded by\n|Rk| ≤hk max\n|α|=k sup\nx∈Ω\nt∈[0,1]\n|Dαf(a + t · (x −a)) −Dαf(a)| 1\nk!\nX\n{α∈Nd\n0 | |α|=k}\n\u0012k\nα\n\u0013\n≤hk+s dk+ 1\n2\nk! ∥f∥Ck,s(Ω),\nwhere we used (7.3.1), ∥x −a∥2 ≤\n√\ndh and P\n{α∈Nd\n0 | |α|=k}\n\u0000k\nα\n\u0001\n= (1 + · · · + 1)k = dk by the\nmultinomial formula.\nWe now come to the main statement of this section. Up to logarithmic terms, it shows the\nconvergence rate (k + s)/d for approximating functions in Ck,s([0, 1]d).\nTheorem 7.7. Let d ∈N, k ∈N0, s ∈[0, 1], and Ω= [0, 1]d. Then, there exists a constant C > 0\nsuch that for every f ∈Ck,s(Ω) and every N ≥2 there exists a ReLU neural network Φf\nN such that\nsup\nx∈Ω\n|f(x) −Φf\nN(x)| ≤CN−k+s\nd ∥f∥Ck,s(Ω),\n(7.3.3)\nsize(Φf\nN) ≤CN log(N) and depth(Φf\nN) ≤C log(N).\nProof. The idea of the proof is to use the so-called “partition of unity method”: First we will\nconstruct a partition of unity (φν)ν, such that for an appropriately chosen M ∈N each φν has\nsupport on a O(1/M) neighborhood of a point η ∈Ω. On each of these neighborhoods we will use\nthe local Taylor polynomial pν of f around η to approximate the function. Then P\nν φνpν gives\nan approximation to f on Ω. This approximation can be emulated by a neural network of the type\nP\nν Φ×\nε (φν, ˆpν), where ˆpν is an neural network approximation to the polynomial pν.\nIt suffices to show the theorem in the case where\nmax\n(\ndk+1/2\nk!\n, exp(d)\n)\n∥f∥Ck,s(Ω) ≤1.\nThe general case can then be immediately deduced by a scaling argument.\n87\nStep 1. We construct the neural network. Define\nM := ⌈N1/d⌉\nand\nε := N−k+s\nd .\n(7.3.4)\nConsider a uniform simplicial mesh with nodes {ν/M | ν ≤M} where ν/M := (ν1/M, . . . , νd/M),\nand where “ν ≤M” is short for {ν ∈Nd\n0 | νi ≤M for all i ≤d}. We denote by φν the cpwl basis\nfunction on this mesh such that φν(ν/M) = 1 and φν(µ/M) = 0 whenever µ ̸= ν. As shown in\nChapter 5, φν is a neural network of size O(1). Then\nX\nν≤M\nφν ≡1\non Ω,\n(7.3.5)\nis a partition of unity. Moreover, observe that\nsupp(φν) ⊆\n\u001a\nx ∈Ω\n\f\f\f\f\n\r\r\rx −ν\nM\n\r\r\r\n∞≤1\nM\n\u001b\n,\n(7.3.6)\nwhere ∥x∥∞= maxi≤d |xi|.\nFor each ν ≤M define the multivariate polynomial\npν(x) :=\nX\n|α|≤k\nDαf\n\u0000 ν\nM\n\u0001\nα!\n\u0010\nx −ν\nM\n\u0011α\n∈Pk,\nand the approximation\nˆpν(x) :=\nX\n|α|≤k\nDαf\n\u0000 ν\nM\n\u0001\nα!\nΦ×\n|α|,ε\n\u0010\nxiα,1 −νiα,1\nM , . . . , xiα,k −νiα,k\nM\n\u0011\n,\nwhere (iα,1, . . . , iα,k) ∈{0, . . . , d}k is arbitrary but fixed such that |{j | iα,j = r}| = αr for all\nr = 1, . . . , d. Finally, define\nΦf\nN :=\nX\nν≤M\nΦ×\nε (φν, ˆpν).\n(7.3.7)\nStep 2. We bound the approximation error. First, for each x ∈Ω, using (7.3.5) and (7.3.6)\n\f\f\f\f\f\f\nf(x) −\nX\nν≤M\nφν(x)pν(x)\n\f\f\f\f\f\f\n≤\nX\nν≤M\n|φν(x)||pν(x) −f(x)|\n≤max\nν≤M\nsup\n{y∈Ω| ∥ν\nM −y∥∞≤1\nM }\n|f(y) −pν(y)|.\nBy Lemma 7.6 we obtain\nsup\nx∈Ω\n\f\f\f\f\f\f\nf(x) −\nX\nν≤M\nφν(x)pν(x)\n\f\f\f\f\f\f\n≤M−(k+s) dk+ 1\n2\nk! ∥f∥Ck,s(Ω) ≤M−(k+s).\n(7.3.8)\n88\nNext, fix ν ≤M and y ∈Ωsuch that ∥ν/M −y∥∞≤1/M ≤1. Then by Proposition 7.4\n|pν(y) −ˆpν(y)| ≤\nX\n|α|≤k\nDαf\n\u0000 ν\nM\n\u0001\nα!\n\f\f\f\f\f\f\nk\nY\nj=1\n\u0010\nyiα,j −νiα,j\nM\n\u0011\n−Φ×\n|α|,ε\n\u0012\nyiα,1 −νiα,1\nM , . . . , yiα,k −iα,k\nM\n\u0013\f\f\f\f\n≤ε\nX\n|α|≤k\nDαf( ν\nM )\nα!\n≤ε exp(d)∥f∥Ck,s(Ω) ≤ε,\n(7.3.9)\nwhere we used |Dαf(ν/M)| ≤∥f∥Ck,s(Ω) and\nX\n{α∈Nd\n0 | |α|≤k}\n1\nα! =\nk\nX\nj=0\n1\nj!\nX\n{α∈Nd\n0 | |α|=j}\nj!\nα! =\nk\nX\nj=0\ndj\nj! ≤\n∞\nX\nj=0\ndj\nj! = exp(d).\nSimilarly, one shows that\n|ˆpν(x)| ≤exp(d)∥f∥Ck,s(Ω) ≤1\nfor all x ∈Ω.\nFix x ∈Ω. Then x belongs to a simplex of the mesh, and thus x can be in the support of at\nmost d + 1 (the number of nodes of a simplex) functions φν. Moreover, Lemma 7.3 implies that\nsupp Φ×\nε (φν(·), ˆpν(·)) ⊆supp φν. Hence, using Lemma 7.3 and (7.3.9)\n\f\f\f\f\f\f\nX\nν≤M\nφν(x)pν(x) −\nX\nν≤M\nΦ×\nε (φν(x), ˆpν(x))\n\f\f\f\f\f\f\n≤\nX\n{ν≤M | x∈supp φν}\n(|φν(x)pν(x) −φν(x)ˆpν(x)|\n+ |φν(x)ˆpν(x) −Φ×\nε (φν(x), ˆpν(x))|\n\u0001\n≤ε + (d + 1)ε = (d + 2)ε.\nIn total, together with (7.3.8)\nsup\nx∈Ω\n|f(x) −Φf\nN(x)| ≤M−(k+s) + ε · (d + 2).\nWith our choices in (7.3.4) this yields the error bound (7.3.3).\nStep 3. It remains to bound the size and depth of the neural network in (7.3.7).\nBy Lemma 5.17, for each 0 ≤ν ≤M we have\nsize(φν) ≤C · (1 + kT ),\ndepth(φν) ≤C · (1 + log(kT )),\n(7.3.10)\nwhere kT is the maximal number of simplices attached to a node in the mesh. Note that kT is\nindependent of M, so that the size and depth of φν are bounded by a constant Cφ independent of\nM.\n89\nLemma 7.3 and Proposition 7.4 thus imply with our choice of ε = N−(k+s)/d\ndepth(Φf\nN) = depth(Φ×\nε ) + max\nν≤M depth(φη) + max\nν≤M depth(ˆpν)\n≤C · (1 + | log(ε)| + Cφ) + depth(Φ×\nk,ε)\n≤C · (1 + | log(ε)| + Cφ)\n≤C · (1 + log(N))\nfor some constant C > 0 depending on k and d (we use “C” to denote a generic constant that can\nchange its value in each line).\nTo bound the size, we first observe with Lemma 5.4 that\nsize(ˆpν) ≤C ·\n\n1 +\nX\n|α|≤k\nsize\n\u0010\nΦ×\n|α|,ε\n\u0011\n\n≤C · (1 + | log(ε)|)\nfor some C depending on k. Thus, for the size of Φf\nN we obtain with M = ⌈N1/d⌉\nsize(Φf\nN) ≤C ·\n\n1 +\nX\nν≤M\n\u0000size(Φ×\nε ) + size(φν) + size(ˆpν)\n\u0001\n\n\n≤C · (1 + M)d(1 + | log(ε)| + Cφ)\n≤C · (1 + N1/d)d(1 + Cφ + log(N))\n≤CN log(N),\nwhich concludes the proof.\nTheorem 7.7 shows the convergence rate (k+s)/d for approximating a Ck,s-function f : [0, 1]d →\nR. As long as k is large, in principle we can achieve arbitrarily large (and d-independent if k ≥d)\nconvergence rates. Crucially, and in contrast to Theorem 5.22, achieving error N−k+s\nd\nrequires the\nneural networks to be of size O(N log(N)) and depth O(log(N)), i.e. to get more and more accurate\napproximations, the neural network depth is required to increase.\nRemark 7.8. Under the stronger assumption that f is an analytic function (in particular such an f is\nin C∞), one can show exponential convergence rates for ReLU networks of the type exp(−βN1/(d+1))\nfor some fixed β > 0 and where N corresponds again to the neural network size (up to logarithmic\nterms), see [58, 166].\nRemark 7.9. Let L : x 7→Ax + b : Rd →Rd be a bijective affine transformation and set Ω:=\nL([0, 1]d) ⊆Rd. Then for a function f ∈Ck,s(Ω), by Theorem 7.7 there exists a neural network\nΦf\nN such that\nsup\nx∈Ω\n|f(x) −Φf\nN(L−1(x))| =\nsup\nx∈[0,1]d |f(L(x)) −Φf\nN(x)|\n≤C∥f ◦L∥Ck,s([0,1]d)N−k+s\nd .\nSince for x ∈[0, 1]d holds |f(L(x))| ≤supy∈Ω|f(y)| and if 0 ̸= α ∈Nd\n0 is a multiindex |Dα(f(L(x))| ≤\n∥A∥|α|\n2\nsupy∈Ω|Dαf(y)|, we have ∥f ◦L∥Ck,s([0,1]d) ≤(1+∥A∥k+s\n2\n)∥f∥Ck,s(Ω). Thus the convergence\nrate N−k+s\nd\nis achieved on every set of the type L([0, 1]d) for an affine map L, and in particular on\nevery hypercube ×d\nj=1[aj, bj].\n90\nBibliography and further reading\nThis chapter is based on the seminal 2017 paper by Yarotsky [245], where the construction of\napproximating the square function, the multiplication, and polynomials (discussed in Sections 7.1\nand 7.2) was first introduced and analyzed.\nThe construction relies on the sawtooth function\ndiscussed in Section 6.2 and originally introduced by Telgarsky in [227].\nYarotsky’s work has\nsince sparked a large body of research, as it allows to lift polynomial approximation theory to\nneural network classes. Convergence results based on this type of argument include for example\n[174, 59, 150, 58, 166].\nThe approximation result derived in Section 7.3 for H¨older continuous functions follows by\nstandard approximation theory for piecewise polynomial functions.\nWe point out that similar\nresults for the approximation of functions in Ck or functions that are analytic can also be shown for\nother activation function than ReLU; see in particular the works of Mhaskar [144, 145] and Section\n6 in Pinkus’ Acta Numerica article [176] for sigmoidal and smooth activations. Additionally, the\nmore recent paper [48] specifically addresses the hyperbolic tangent activation. Finally, [81] studies\ngeneral activation functions that allow for the construction of approximate partitions of unity.\n91\nChapter 8\nHigh-dimensional approximation\nIn the previous chapters we established convergence rates for the approximation of a function f :\n[0, 1]d →R by a neural network. For example, Theorem 7.7 provides the error bound O(N−(k+s)/d)\nin terms of the network size N (up to logarithmic terms), where k and s describe the smoothness\nof f. Achieving an accuracy of ε > 0, therefore, necessitates a network size N = O(ε−d/(k+s))\n(according to this bound). Hence, the size of the network needs to increase exponentially in d.\nThis exponential dependence on the dimension d is referred to as the curse of dimensionality\n[16]. For classical smoothness spaces, such exponential d dependence cannot be avoided [16, 52, 164].\nHowever, functions f that are of interest in practice may have additional properties, which allow\nfor better convergence rates.\nIn this chapter, we discuss three scenarios under which the curse of dimensionality can be\nmitigated. First, we examine an assumption limiting the behavior of functions in their Fourier\ndomain. This assumption allows for slow but dimension independent approximation rates. Second,\nwe consider functions with a specific compositional structure.\nConcretely, these functions are\nconstructed by compositions and linear combinations of simple low-dimensional subfunctions. In\nthis case, the curse of dimension is present but only through the input dimension of the subfunctions.\nFinally, we study the situation, where we still approximate high-dimensional functions, but only care\nabout the approximation accuracy on a lower dimensional submanifold. Here, the approximation\nrate is goverened by the smoothness and the dimension of the manifold.\n8.1\nThe Barron class\nIn [10], Barron introduced a set of functions that can be approximated by neural networks without\na curse of dimensionality. This set, known as the Barron class, is characterized by a specific type\nof bounded variation. To define it, for f ∈L1(Rd) denote by\nˆf(w) :=\nZ\nRd f(x)e−2π i w⊤x dx\nits Fourier transform. Then, for C > 0 the Barron class is defined as\nΓC :=\n\u001a\nf ∈L1(Rd)\n\f\f\f\f ∥ˆf∥L1(Rd) < ∞,\nZ\nRd |2πξ|| ˆf(ξ)| dξ < C\n\u001b\n.\n92\nWe point out that the definition of ΓC in [10] is more general, but our assumption will simplify\nsome of the arguments. Nonetheless, the following proof is very close to the original result, and the\npresentation is similar to [175, Section 5]. Theorem 1 in [10] reads as follows.\nTheorem 8.1. Let σ : R →R be sigmoidal (see Definition 3.11) and let f ∈ΓC for some C > 0.\nDenote by Bd\n1 := {x ∈Rd | ∥x∥≤1} the unit ball. Then, for every c > 4C2 and every N ∈N there\nexists a neural network Φf with architecture (σ; d, N, 1) such that\n1\n|Bd\n1|\nZ\nBd\n1\n\f\f\ff(x) −Φf(x)\n\f\f\f\n2\ndx ≤c\nN ,\n(8.1.1)\nwhere |Bd\n1| is the Lebesgue measure of Bd\n1.\nRemark 8.2. The approximation rate on (8.1.1) can be slightly improved under some assumptions\non the activation function such as powers of the ReLU, [213].\nImportantly, the dimension d does not enter on the right-hand side of (8.1.1), in particular the\nconvergence rate is not directly affected by the dimension, which is in stark contrast to the results\nof the previous chapters. However, it should be noted, that the constant Cf may still have some\ninherent d-dependence, see Exercise 8.10.\nThe proof of Theorem 8.1 is based on a peculiar property of high-dimensional convex sets, which\nis described by the (approximate) Caratheodory theorem, the original version of which was given\nin [31]. The more general version stated in the following lemma follows [236, Theorem 0.0.2] and\n[10, 177]. For its statement recall that co(G) denotes the the closure of the convex hull of G.\nLemma 8.3. Let H be a Hilbert space, and let G ⊆H be such that for some B > 0 it holds that\n∥g∥H ≤B for all g ∈G. Let f ∈co(G). Then, for every N ∈N and every c > B2 there exist\n(gi)N\ni=1 ⊆G such that\n\r\r\r\r\rf −1\nN\nN\nX\ni=1\ngi\n\r\r\r\r\r\n2\nH\n≤c\nN .\n(8.1.2)\nProof. Fix ε > 0 and N ∈N. Since f ∈co(G), there exist coefficients α1, . . . , αm ∈[0, 1] summing\nto 1, and linearly independent elements h1, . . . , hm ∈G such that\nf∗:=\nm\nX\nj=1\nαjhj\nsatisfies ∥f −f∗∥H < ε. We claim that there exists g1, . . . , gN, each in {h1, . . . , hm}, such that\n\r\r\r\r\r\r\nf∗−1\nN\nN\nX\nj=1\ngj\n\r\r\r\r\r\r\n2\nH\n≤B2\nN .\n(8.1.3)\n93\nSince ε > 0 was arbitrary, this then concludes the proof. Since there exists an isometric isomorphism\nfrom span{h1, . . . , hm} to Rm, there is no loss of generality in assuming H = Rm in the following.\nLet Xi, i = 1, . . . , N, be i.i.d. Rm-valued random variables with\nP[Xi = hj] = αj\nfor all i = 1, . . . , m.\nIn particular E[Xi] = Pm\nj=1 αjhj = f∗for each i. Moreover,\nE\n\n\n\r\r\r\r\r\r\nf∗−1\nN\nN\nX\nj=1\nXj\n\r\r\r\r\r\r\n2\n= E\n\n\n\r\r\r\r\r\r\n1\nN\nN\nX\nj=1\n(f∗−Xj)\n\r\r\r\r\r\r\n2\n\n=\n1\nN2\n\" N\nX\nj=1\n∥f∗−Xj∥2 +\nX\ni̸=j\n⟨f∗−Xi, f∗−Xj⟩\n#\n= 1\nN E[∥f∗−X1∥2]\n= 1\nN E[∥f∗∥−2 ⟨f∗, X1⟩+ ∥X1∥2]\n= 1\nN E[∥X1∥2 −∥f∗∥2] ≤B2\nN .\n(8.1.4)\nHere we used that the (Xi)N\ni=1 are independent, the fact that E[Xi] = f∗, as well as E⟨f∗−Xi, f∗−Xj⟩=\n0 if i ̸= j. Since the expectation in (8.1.4) is bounded by B2/N, there must exist at least one real-\nization of the random variables Xi ∈{h1, . . . , hm}, denoted as gi, for which (8.1.3) holds.\nLemma 8.3 provides a powerful tool: If we want to approximate a function f with a superposition\nof N elements in a set G, then it is sufficient to show that f can be represented as an arbitrary\n(infinite) convex combination of elements of G.\nLemma 8.3 suggests that we can prove Theorem 8.1 by showing that each function in ΓC belongs\nto the convex hull of neural networks with just a single neuron. We make a small detour before\nproving this result. We first show that each function f ∈ΓC is in the convex hull of affine transforms\nof Heaviside functions. We define the set of affine transforms of Heaviside functions GC as\nGC :=\nn\nBd\n1 ∋x 7→γ · 1R+(⟨a, x⟩+ b)\n\f\f\f a ∈Rd, b ∈R, |γ| ≤2C\no\n.\nThe following lemma, corresponding to [175, Lemma 5.12], provides a link between ΓC and GC.\nLemma 8.4. Let d ∈N, C > 0 and f ∈ΓC. Then f|Bd\n1 −f(0) ∈co(GC), where the closure is\ntaken with respect to the norm\n∥g∥L2,⋄(Bd\n1) :=\n \n1\n|Bd\n1|\nZ\nBd\n1\n|g(x)|2 dx\n!1/2\n.\n94\nProof. Since f ∈ΓC, we have that f, ˆf ∈L1(Rd).\nHence, we can apply the inverse Fourier\ntransform and get the following computation:\nf(x) −f(0) =\nZ\nRd\nˆf(ξ)\n\u0010\ne2πi⟨x,ξ⟩−1\n\u0011\ndξ\n=\nZ\nRd\n\f\f\f ˆf(ξ)\n\f\f\f\n\u0010\ne2πi⟨x,ξ⟩+iκ(ξ) −eiκ(ξ)\u0011\ndξ\n=\nZ\nRd\n\f\f\f ˆf(ξ)\n\f\f\f (cos(2π⟨x, ξ⟩+ κ(ξ)) −cos(κ(ξ))) dξ,\nwhere κ(ξ) is the phase of ˆf(ξ) and the last inequality follows since f is real-valued.\nTo use the fact that f has a bounded Fourier moment, we reformulate the integral as\nZ\nRd\n\f\f\f ˆf(ξ)\n\f\f\f (cos(2π⟨x, ξ⟩+ κ(ξ)) −cos(κ(ξ))) dξ\n=\nZ\nRd\n(cos(2π⟨x, ξ⟩+ κ(ξ)) −cos(κ(ξ)))\n|2πξ|\n|2πξ|\n\f\f\f ˆf(ξ)\n\f\f\f dξ.\nWe define a new measure Λ with density\ndΛ(ξ) := 1\nC |2πξ|| ˆf(ξ)| dξ.\nSince f ∈ΓC, it follows that Λ is a probability measure on Rd. Now we have that\nf(x) −f(0) = C\nZ\nRd\n(cos(2π⟨x, ξ⟩+ κ(ξ)) −cos(κ(ξ)))\n|2πξ|\ndΛ(ξ).\n(8.1.5)\nNext, we would like to replace the integral of (8.1.5) by an appropriate finite sum.\nThe cosine function is 1-Lipschitz. Hence, we note that ξ 7→qx(ξ) := (cos(2π⟨x, ξ⟩+ κ(ξ)) −\ncos(κ(ξ)))/|2πξ| is bounded by 1. In addition, it is easy to see that qx is well-defined and continuous\neven in the origin.\nTherefore, the integral (8.1.5) can be approximated by a Riemann sum, i.e.,\n\f\f\f\f\f\f\f\nC\nZ\nRd qx(ξ) dΛ(ξ) −C\nX\nθ∈1\nnZd\nqx(θ) · Λ(Iθ)\n\f\f\f\f\f\f\f\n→0,\n(8.1.6)\nwhere Iθ := [0, 1/n)d + θ.\nSince f(x)−f(0) is continuous and thus bounded on Bd\n1, we have by the dominated convergence\ntheorem that\n1\n|Bd\n1|\nZ\nBd\n1\n\f\f\f\f\f\f\f\nf(x) −f(0) −C\nX\nθ∈1\nnZd\nqx(θ) · Λ(Iθ)\n\f\f\f\f\f\f\f\n2\ndx →0.\n(8.1.7)\nSince P\nθ∈1\nnZd Λ(Iθ) = Λ(Rd) = 1, we conclude that f(x) −f(0) is in the L2,⋄(Bd\n1) closure of\nconvex combinations of functions of the form\nx 7→gθ(x) := αθqx(θ),\n95\nfor θ ∈Rd and 0 ≤αθ ≤C.\nNow we only need to prove that each gθ is in co(GC). By setting z = ⟨x, θ/|θ|⟩, we observe that\nthe result follows if the map\n[−1, 1] ∋z 7→αθ\ncos(2π|θ|z + κ(θ)) −cos(κ(θ))\n|2πθ|\n=: ˜gθ(z),\ncan be approximated arbitrarily well by convex combinations of functions of the form\n[−1, 1] ∋z 7→γ1R+\n\u0000a′z + b′\u0001\n,\n(8.1.8)\nwhere a′, b′ ∈R and |γ| ≤2C.\nWe define, for T ∈N,\ngT,+ :=\nT\nX\ni=1\n\f\f˜gθ\n\u0000 i\nT\n\u0001\n−˜gθ\n\u0000 i−1\nT\n\u0001\f\f\n2C\n\u0012\n2Csign\n\u0012\n˜gθ\n\u0012 i\nT\n\u0013\n−˜gθ\n\u0012i −1\nT\n\u0013\u0013\n1R+\n\u0012\nx −i\nT\n\u0013\u0013\n,\ngT,−:=\nT\nX\ni=1\n\f\f˜gθ\n\u0000−i\nT\n\u0001\n−˜gθ\n\u0000 1−i\nT\n\u0001\f\f\n2C\n\u0012\n2Csign\n\u0012\n˜gθ\n\u0012\n−i\nT\n\u0013\n−˜gθ\n\u00121 −i\nT\n\u0013\u0013\n1R+\n\u0012\n−x + i\nT\n\u0013\u0013\n.\nPer construction, gT,−+ gT,+ converges to ˜gθ for T →∞. Moreover, ∥˜g′\nθ∥L∞(R) ≤C and hence\nT\nX\ni=1\n|˜gθ(i/T) −˜gθ((i −1)/T)|\n2C\n+\nT\nX\ni=1\n|˜gθ(−i/T) −˜gθ((1 −i)/T)|\n2C\n≤\n2\n2CT\nT\nX\ni=1\n∥˜g′\nθ∥L∞(R) ≤1.\nWe conclude that gT,−+ gT,+ is a convex combination of functions of the form (8.1.8). Hence,\n˜gθ can be arbitrarily well approximated by convex combinations of the form (8.1.8). Therefore\ngθ ∈co(GC). Finally, (8.1.7) yields that f −f(0) ∈co(GC).\nWe now have all tools to complete the proof of Theorem 8.1.\nof Theorem 8.1. Let f ∈ΓC. By Lemma 8.4\nf|Bd\n1 −f(0) ∈co(GC).\nIt is not hard to see that for every g ∈GC holds ∥g∥L2,⋄(Bd\n1) ≤2C. Applying Lemma 8.3 with the\nHilbert space L2,⋄(Bd\n1), we get that for every N ∈N there exist |γi| ≤2C, ai ∈Rd, bi ∈R, for\ni = 1, . . . , N, so that\n1\n|Bd\n1|\nZ\nBd\n1\n\f\f\f\f\ff(x) −f(0) −\nN\nX\ni=1\nγi1R+(⟨ai, x⟩+ bi)\n\f\f\f\f\f\n2\ndx ≤4C2\nN .\nBy Exercise 3.24, it holds that σ(λ·) →1R+ for λ →∞almost everywhere. Thus, for every δ > 0\nthere exist ˜ai, ˜bi, i = 1, . . . , N, so that\n96\n1\n|Bd\n1|\nZ\nBd\n1\n\f\f\f\f\ff(x) −f(0) −\nN\nX\ni=1\nγiσ\n\u0010\n⟨˜ai, x⟩+ ˜bi\n\u0011\f\f\f\f\f\n2\ndx ≤4C2\nN\n+ δ.\nThe result follows by observing that\nN\nX\ni=1\nγiσ\n\u0010\n⟨˜ai, x⟩+ ˜bi\n\u0011\n+ f(0)\nis a neural network with architecture (σ; d, N, 1).\nThe dimension-independent approximation rate of Theorem 8.1 may seem surprising, especially\nin comparison to the results in Chapters 4 and 5. However, this can be explained by recognizing\nthat the assumption of a finite Fourier moment is effectively a dimension-dependent regularity\nassumption. Indeed, the condition becomes more restrictive in higher dimensions and hence the\ncomplexity of ΓC does not grow with the dimension.\nTo further explain this, let us relate the Barron class to classical function spaces. In [10, Section\nII] it was observed that a sufficient condition is that all derivatives of order up to ⌊d/2⌋+ 2 are\nsquare-integrable. In other words, if f belongs to the Sobolev space H⌊d/2⌋+2(Rd), then f is a\nBarron function. Importantly, the functions must become smoother, as the dimension increases.\nThis assumption would also imply an approximation rate of N−1/2 in the L2 norm by sums of\nat most N B-splines, see [168, 52]. However, in such estimates some constants may still depend\nexponentially on d, whereas all constants in Theorem 8.1 are controlled independently of d.\nAnother notable aspect of the approximation of Barron functions is that the absolut values of\nthe weights other than the output weights are not bounded by a constant. To see this, we refer\nto (8.1.6), where arbitrarily large θ need to be used. While ΓC is a compact set, the set of neural\nnetworks of the specified architecture for a fixed N ∈N is not parameterized with a compact\nparameter set. In a certain sense, this is reminiscent of Proposition 3.19 and Theorem 3.20, where\narbitrarily strong approximation rates where achieved by using a very complex activation function\nand a non-compact parameter space.\n8.2\nFunctions with compositionality structure\nAs a next instance of types of functions for which the curse of dimensionality can be overcome, we\nstudy functions with compositional structure. In words, this means that we study high-dimensional\nfunctions that are constructed by composing many low-dimensional functions. This point of view\nwas proposed in [178]. Note that this can be a realistic assumption in many cases, such as for\nsensor networks, where local information is first aggregated in smaller clusters of sensors before\nsome information is sent to a processing unit for further evaluation.\nWe introduce a model for compositional functions next. Consider a directed acyclic graph G\nwith M vertices η1, . . . , ηM such that\n• exactly d vertices, η1, . . . , ηd, have no ingoing edge,\n• each vertex has at most m ∈N ingoing edges,\n• exactly one vertex, ηM, has no outgoing edge.\n97\nWith each vertex ηj for j > d we associate a function fj : Rdj →R. Here dj denotes the\ncardinality of the set Sj, which is defined as the set of indices i corresponding to vertices ηi for\nwhich we have an edge from ηi to ηj. Without loss of generality, we assume that m ≥dj = |Sj| ≥1\nfor all j > d. Finally, we let\nFj := xj\nfor all\nj ≤d\n(8.2.1a)\nand1\nFj := fj((Fi)i∈Sj)\nfor all\nj > d.\n(8.2.1b)\nThen FM(x1, . . . , xd) is a function from Rd →R. Assuming\n∥fj∥Ck,s(Rdj ) ≤1\nfor all\nj = d + 1, . . . , M,\n(8.2.2)\nwe denote the set of all functions of the type FM by Fk,s(m, d, M). Figure 8.1 shows possible\ngraphs of such functions.\nClearly, for s = 0, Fk,0(m, d, M) ⊆Ck(Rd) since the composition of functions in Ck belongs\nagain to Ck. A direct application of Theorem 7.7 allows to approximate FM ∈Fk(m, d, M) with a\nneural network of size O(N log(N)) and error O(N−k\nd ). Since each fj depends only on m variables,\nintuitively we expect an error convergence of type O(N−k\nm ) with the constant somehow depending\non the number M of vertices. To show that this is actually possible, in the following we associate\nwith each node ηj a depth lj ≥0, such that lj is the maximum number of edges connecting ηj to\none of the nodes {η1, . . . , ηd}.\nFigure 8.1: Three types of graphs that could be the basis of compositional functions. The associated\nfunctions are composed of two or three-dimensional functions only.\nProposition 8.5. Let k, m, d, M ∈N and s > 0. Let FM ∈Fk,s(m, d, M). Then there exists a\nconstant C = C(m, k + s, M) such that for every N ∈N there exists a ReLU neural network ΦFM\n1The ordering of the inputs (Fi)i∈Sj in (8.2.1b) is arbitrary but considered fixed throughout.\n98\nsuch that\nsize(ΦFM ) ≤CN log(N),\ndepth(ΦFM ) ≤C log(N)\nand\nsup\nx∈[0,1]d |FM(x) −ΦFM (x)| ≤N−k+s\nm .\nProof. Throughout this proof we assume without loss of generality that the indices follow a topo-\nlogical ordering, i.e., they are ordered such that Sj ⊆{1, . . . , j −1} for all j (i.e. the inputs of\nvertex ηj can only be vertices ηi with i < j).\nStep 1. First assume that ˆfj are functions such that\n|fj(x) −ˆfj(x)| ≤δj := ε · (2m)−(M+1−j)\nfor all\nx ∈[−2, 2]dj.\n(8.2.3)\nLet ˆFj be defined as in (8.2.1), but with all fj in (8.2.1b) replaced by ˆfj. We now check the error\nof the approximation ˆFM to FM. To do so we proceed by induction over j and show that for all\nx ∈[−1, 1]d\n|Fj(x) −ˆFj(x)| ≤(2m)−(M−j)ε.\n(8.2.4)\nNote that due to ∥fj∥Ck ≤1 we have |Fj(x)| ≤1 and thus (8.2.4) implies in particular that\nˆFj(x) ∈[−2, 2].\nFor j = 1 it holds F1(x1) = ˆF1(x1) = x1, and thus (8.2.4) is valid for all x1 ∈[−1, 1]. For the\ninduction step, for all x ∈[−1, 1]d by (8.2.3) and the induction hypothesis\n|Fj(x) −ˆFj(x)| = |fj((Fi)i∈Sj) −ˆfj(( ˆFi)i∈Sj)|\n= |fj((Fi)i∈Sj) −fj(( ˆFi)i∈Sj)| + |fj(( ˆFi)i∈Sj) −ˆfj(( ˆFi)i∈Sj)|\n≤\nX\ni∈Sj\n|Fi −ˆFi| + δj\n≤m · (2m)−(M−(j−1))ε + (2m)−(M+1−j)ε\n≤(2m)−(M−j)ε.\nHere we used that | d\ndxr fj((xi)i∈Sj)| ≤1 for all r ∈Sj so that\n|fj((xi)i∈Sj) −fj((yi)i∈Sj)| ≤\nX\nr∈Sj\n|f((xi)i∈Sj\ni≤r\n, (yi)i∈Sj\ni>r\n) −f((xi)i∈Sj\ni<r\n, (yi)i∈Sj\ni≥r\n)|\n≤\nX\nr∈Sj\n|xr −yr|.\nThis shows that (8.2.4) holds, and thus for all x ∈[−1, 1]d\n|FM(x) −ˆFM(x)| ≤ε.\n99\nStep 2. We sketch a construction, of how to write ˆFM from Step 1 as a neural network ΦFM\nof the claimed size and depth bounds. Fix N ∈N and let\nNj := ⌈N(2m)\nm\nk+s(M+1−j)⌉.\nBy Theorem 7.7, since dj ≤m, we can find a neural network Φfj satisfying\nsup\nx∈[−2,2]dj\n|fj(x) −Φfj(x)| ≤N\n−k+s\nm\nj\n≤N−k+s\nm (2m)−(M+1−j)\n(8.2.5)\nand\nsize(Φfj) ≤CNj log(Nj) ≤CN(2m)\nm(M+1−j)\nk+s\n\u0012\nlog(N) + log(2m)m(M + 1 −j)\nk + s\n\u0013\nas well as\ndepth(Φfj) ≤C ·\n\u0012\nlog(N) + log(2m)m(M + 1 −j)\nk + s\n\u0013\n.\nThen\nn\nX\nj=1\nsize(Φfj) ≤2CN log(N)\nM\nX\nj=1\n(2m)\nm(M+1−j)\nk+s\n≤2CN log(N)\nM\nX\nj=1\n\u0010\n(2m)\nm\nk+s\n\u0011j\n≤2CN log(N)(2m)\nm(M+1)\nk+s\n.\nHere we used PM\nj=1 aj ≤\nR M+1\n1\nexp(log(a)x) dx ≤\n1\nlog(a)aM+1.\nThe function ˆFM from Step 1 then will yield error N−k+s\nm by (8.2.3) and (8.2.5). We observe that\nˆFM can be constructed inductively as a neural network ΦFM by propagating all values ΦF1, . . . , ˆΦFj\nto all consecutive layers using identity neural networks and then using the outputs of (ΦFi)i∈Sj+1\nas input to Φfj+1. The depth of this neural network is bounded by\nM\nX\nj=1\ndepth(Φfj) = O(M log(N)).\nWe have at most PM\nj=1 |Sj| ≤mM values which need to be propagated through these O(M log(N))\nlayers, amounting to an overhead O(mM2 log(N)) = O(log(N)) for the identity neural networks.\nIn all the neural network size is thus O(N log(N)).\nRemark 8.6. From the proof we observe that the constant C in Proposition 8.5 behaves like\nO((2m)\nm(M+1)\nk+s\n).\n8.3\nFunctions on manifolds\nAnother instance in which the curse of dimension can be mitigated, is if the input to the network\nbelongs to Rd, but stems from an m-dimensional manifold M ⊆Rd.\nIf we only measure the\n100\nM\nFigure 8.2: One-dimensional sub-manifold of three-dimensional space. At the orange point, we\ndepict a ball and the tangent space of the manifold.\napproximation error on M, then we can again show that it is m rather than d that determines the\nrate of convergence.\nTo explain the idea, we assume in the following that M is a smooth, compact m-dimensional\nmanifold in Rd. Moreover, we suppose that there exists δ > 0 and finitely many points x1, . . . , xM ∈\nM such that the δ-balls Bδ/2(xi) := {y ∈Rd | ∥y −x∥2 < δ/2} for j = 1, . . . , M cover M (for\nevery δ > 0 such xi exist since M is compact). Moreover, denoting by TxM ≃Rm the tangential\nspace of M at x, we assume δ > 0 to be so small that the orthogonal projection\nπj : Bδ(xj) ∩M →TxjM\n(8.3.1)\nis injective, the set πj(Bδ(xj) ∩M) ⊆TxjM has C∞boundary, and the inverse projection\nπ−1\nj\n: πj(Bδ(xj) ∩M) →M\n(8.3.2)\nis C∞(this is possible because M is a smooth manifold). A visualization of this assumption is\nshown in Figure 8.2.\nNote that πj in (8.3.1) is a linear map, whereas π−1\nj\nin (8.3.2) is in general non-linear.\nFor a function f : M →R and x ∈Bδ(xj) ∩M we can then write\nf(x) = f(π−1\nj (πj(x))) = fj(πj(x))\nwhere\nfj := f ◦π−1\nj\n: πj(Bδ(xj) ∩M) →R.\n101\nIn the following, for f : M →R, k ∈N0, and s ∈[0, 1) we let\n∥f∥Ck,s(M) :=\nsup\nj=1,...,M\n∥fj∥Ck,s(πj(Bδ(xj)∩M)).\nWe now state the main result of this section.\nProposition 8.7. Let d, k ∈N, s ≥0, and let M be a smooth, compact m-dimensional manifold\nin Rd. Then there exists a constant C > 0 such that for all f ∈Ck,s(M) and every N ∈N there\nexists a ReLU neural network Φf\nN such that size(Φf\nN) ≤CN log(N), depth(Φf\nN) ≤C log(N) and\nsup\nx∈M\n|f(x) −Φf\nN(x)| ≤C∥f∥Ck,s(M)N−k+s\nm .\nProof. Since M is compact there exists A > 0 such that M ⊆[−A, A]d. Similar as in the proof of\nTheorem 7.7, we consider a uniform mesh with nodes {−A + 2A ν\nn | ν ≤n}, and the corresponding\npiecewise linear basis functions forming the partition of unity P\nν≤φν ≡1 on [−A, A]d where\nsupp φν ≤{y ∈Rd | ∥ν\nn −y∥∞≤A\nn }. Let δ > 0 be such as in the beginning of this section.\nSince M is covered by the balls (Bδ/2(xj))M\nj=1, fixing n ∈N large enough, for each ν such that\nsupp φν ∩M ̸= ∅there exists j(ν) ∈{1, . . . , M} such that supp φν ⊆Bδ(xj(ν)) and we set\nIj := {ν ≤M | j = j(ν)}. Then we have for all x ∈M\nf(x) =\nX\nν≤n\nφν(x)fj(πj(x)) =\nM\nX\nj=1\nX\nν∈Ij\nφν(x)fj(πj(x)).\n(8.3.3)\nNext, we approximate the functions fj. Let Cj be the smallest (m-dimensional) cube in TxjM ≃\nRm such that πj(Bδ(xj) ∩M) ⊆Cj. The function ˆfj can be extended to a function on Cj (we will\nuse the same notation for this extension) such that\n∥f∥Ck,s(Cj) ≤C∥f∥Ck,s(πj(Bδ(xj)∩M)),\nfor some constant depending on πj(Bδ(xj) ∩M) but independent of f. Such an extension result\ncan, for example, be found in [216, Chapter VI]. By Theorem 7.7 (also see Remark 7.9), there exists\na neural network ˆfj : Cj →R such that\nsup\nx∈Cj\n|fj(x) −ˆfj(x)| ≤CN−k+s\nm\n(8.3.4)\nand\nsize( ˆfj) ≤CN log(N),\ndepth( ˆfj) ≤C log(N).\nTo approximate f in (8.3.3) we now let with ε := N−k+s\nd\nΦN :=\nM\nX\nj=1\nX\nν∈Ij\nΦ×\nε (φν, ˆfi ◦πj),\n102\nwhere we note that πj is linear and thus ˆfj ◦πj can be expressed by a neural network. First let us\nestimate the error of this approximation. For x ∈M\n|f(x) −ΦN(x)| ≤\nM\nX\nj=1\nX\nν∈Ij\n|φν(x)fj(πj(x)) −Φ×\nε (φν(x), ˆfj(πj(x)))|\n≤\nM\nX\nj=1\nX\nν∈Ij\n(|φν(x)fj(πj(x)) −φν(x)fj(πj(x))|\n+|φν(x)fj(πj(x)) −Φ×\nε (φν(x), ˆfj(πj(x)))|\n\u0011\n≤sup\ni≤M\n∥fi −ˆfi∥L∞(Ci)\nM\nX\nj=1\nX\nν∈Ij\n|φν(x)| +\nM\nX\nj=1\nX\n{ν∈Ij | x∈supp φν}\nε\n≤CN−k+s\nm + dε ≤CN−k+s\nm ,\nwhere we used that x can be in the support of at most d of the φν, and where C is a constant\ndepending on d and M.\nFinally, let us bound the size and depth of this approximation. Using size(φν) ≤C, depth(φν) ≤\nC (see (5.3.12)) and size(Φ×\nε ) ≤C log(ε) ≤C log(N) and depth(Φ×\nε ) ≤Cdepth(ε) ≤C log(N) (see\nLemma 7.3) we find\nM\nX\nj=1\nX\nν∈Ij\n\u0010\nsize(Φ×\nε ) + size(φν) + size( ˆfi ◦πj)\n\u0011\n≤\nM\nX\nj=1\nX\nν∈Ij\nC log(N) + C + CN log(N)\n= O(N log(N)),\nwhich implies the bound on size(ΦN). Moreover,\ndepth(ΦN) ≤depth(Φ×\nε ) + max\nn\ndepth(φν, ˆfj)\no\n≤C log(N) + log(N) = O(log(N)).\nThis completes the proof.\nBibliography and further reading\nThe ideas of Section 8.1 were originally developed in [10], with an extension to L∞approximation\nprovided in [9]. These arguments can be extended to yield dimension-independent approximation\nrates for high-dimensional discontinuous functions, provided the discontinuity follows a Barron\nfunction, as shown in [175]. The Barron class has been generalized in various ways, as discussed in\n[138, 137, 239, 240, 11].\nThe compositionality assumption of Section 8.2 was discussed in the form presented in [178].\nAn alternative approach, known as the hierarchical composition/interaction model, was studied in\n[119].\nThe manifold assumption discussed in Section 8.3 is frequently found in the literature, with\nnotable examples including [211, 40, 35, 203, 156, 118].\n103\nAnother prominent direction, omitted in this chapter, pertains to scientific machine learn-\ning. High-dimensional functions often arise from (parametric) PDEs, which have a rich literature\ndescribing their properties and structure. Various results have shown that neural networks can\nleverage the inherent low-dimensionality known to exist in such problems. Efficient approximation\nof certain classes of high-dimensional (or even infinite-dimensional) analytic functions, ubiquitous\nin parametric PDEs, has been verified in [208, 209]. Further general analyses for high-dimensional\nparametric problems can be found in [167, 122], and results exploiting specific structural conditions\nof the underlying PDEs, e.g., in [125, 198]. Additionally, [58, 150, 166] provide results regarding\nfast convergence for certain smooth functions in potentially high but finite dimensions.\nFor high-dimensional PDEs, elliptic problems have been addressed in [78], linear and semilin-\near parabolic evolution equations have been explored in [79, 71, 100], and stochastic differential\nequations in [109, 80].\n104\nExercises\nExercise 8.8. Let C > 0 and d ∈N. Show that, if g ∈ΓC, then\na−dg (a(· −b)) ∈ΓC,\nfor every a ∈R+, b ∈Rd.\nExercise 8.9. Let C > 0 and d ∈N. Show that, for gi ∈ΓC, i = 1, . . . , m and c = (ci)m\ni=1 it holds\nthat\nm\nX\ni=1\ncigi ∈Γ∥c∥1C.\nExercise 8.10. For every d ∈N the function f(x) := exp(−∥x∥2\n2/2), x ∈Rd, belongs to Γd. It\nholds Cf = O(\n√\nd), for d →∞.\nExercise 8.11. Let d ∈N, and let f(x) = P∞\ni=1 ciσReLU(⟨ai, x⟩+ bi) for x ∈Rd with ∥ai∥=\n1, |bi| ≤1 for all i ∈N. Show that for every N ∈N, there exists a ReLU neural network with N\nneurons and one layer such that\n∥f −fN∥L2(Bd\n1) ≤3∥c∥1\n√\nN\n.\nHence, every infinite ReLU neural network can be approximated at a rate O(N1/2) by finite ReLU\nneural networks of width N.\nExercise 8.12. Let C > 0 prove that every f ∈ΓC is continuously differentiable.\n105\nChapter 9\nInterpolation\nThe learning problem associated to minimizing the empirical risk of (1.2.3) is based on minimizing\nan error that results from evaluating a neural network on a finite set of (training) points.\nIn\ncontrast, all previous approximation results focused on achieving uniformly small errors across the\nentire domain. Finding neural networks that achieve a small training error appears to be much\nsimpler, since, instead of ∥f −Φn∥∞→0 for a sequence of neural networks Φn, it suffices to have\nΦn(xi) →f(xi) for all xi in the training set.\nIn this chapter, we study the extreme case of the aforementioned approximation problem. We\nanalyze under which conditions it is possible to find a neural network that coincides with the target\nfunction f at all training points. This is referred to as interpolation. To make this notion more\nprecise, we state the following definition.\nDefinition 9.1 (Interpolation). Let d, m ∈N, and let Ω⊆Rd. We say that a set of functions\nH ⊆{h: Ω→R} interpolates m points in Ω, if for every S = (xi, yi)m\ni=1 ⊆Ω× R, such that\nxi ̸= xj for i ̸= j, there exists a function h ∈H such that h(xi) = yi for all i = 1, . . . , m.\nKnowing the interpolation properties of an architecture represents extremely valuable informa-\ntion for two reasons:\n• Consider an architecture that interpolates m points and let the number of training samples\nbe bounded by m. Then (1.2.3) always has a solution.\n• Consider again an architecture that interpolates m points and assume that the number of\ntraining samples is less than m. Then for every point ˜x not in the training set and every y ∈R\nthere exists a minimizer h of (1.2.3) that satisfies h(˜x) = y. As a consequence, without further\nrestrictions (many of which we will discuss below), such an architecture cannot generalize to\nunseen data.\nThe existence of solutions to the interpolation problem does not follow trivially from the approxi-\nmation results provided in the previous chapters (even though we will later see that there is a close\nconnection). We also remark that the question of how many points neural networks with a given\narchitecture can interpolate is closely related to the so-called VC dimension, which we will study\nin Chapter 14.\n106\nWe start our analysis of the interpolation properties of neural networks by presenting a result\nsimilar to the universal approximation theorem but for interpolation in the following section. In\nthe subsequent section, we then look at interpolation with desirable properties.\n9.1\nUniversal interpolation\nUnder what conditions on the activation function and architecture can a set of neural networks\ninterpolate m ∈N points? According to Chapter 3, particularly Theorem 3.8, we know that shallow\nneural networks can approximate every continuous function with arbitrary accuracy, provided the\nneural network width is large enough. As the neural network’s width and/or depth increases, the\narchitectures become increasingly powerful, leading us to expect that at some point, they should\nbe able to interpolate m points. However, this intuition may not be correct:\nExample 9.2. Let H := {f ∈C0([0, 1]) | f(0) ∈Q}. Then H is dense in C0([0, 1]), but H does not\neven interpolate one point in [0, 1].\nMoreover, Theorem 3.8 is an asymptotic result that only states that a given function can be\napproximated for sufficiently large neural network architectures, but it does not state how large\nthe architecture needs to be.\nSurprisingly, Theorem 3.8 can nonetheless be used to give a guarantee that a fixed-size archi-\ntecture yields sets of neural networks that allow the interpolation of m points. This result is due\nto [176]; for a more detailed discussion of previous results see the bibliography section. Due to its\nsimilarity to the universal approximation theorem and the fact that it uses the same assumptions,\nwe call the following theorem the “Universal Interpolation Theorem”. For its statement recall the\ndefinition of the set of allowed activation functions M in (3.1.1) and the class N 1\nd (σ, 1, n) of shallow\nneural networks of width n introduced in Definition 3.6.\nTheorem 9.3 (Universal Interpolation Theorem). Let d, n ∈N and let σ ∈M not be a polynomial.\nThen N 1\nd (σ, 1, n) interpolates n + 1 points in Rd.\nProof. Fix (xi)n+1\ni=1 ⊆Rd arbitrary. We will show that for any (yi)n+1\ni=1 ⊆R there exist weights and\nbiases (wj)n\nj=1 ⊆Rd, (bj)n\nj=1, (vj)n\nj=1 ⊆R, c ∈R such that\nΦ(xi) :=\nn\nX\nj=1\nvjσ(w⊤\nj xi + bj) + c = yi\nfor all\ni = 1, . . . , n + 1.\n(9.1.1)\nSince Φ ∈N 1\nd (σ, 1, n) this then concludes the proof.\nDenote\nA :=\n\n\n\n1\nσ(w⊤\n1 x1 + b1)\n· · ·\nσ(w⊤\nn x1 + bn)\n...\n...\n...\n...\n1\nσ(w⊤\n1 xn+1 + b1)\n· · ·\nσ(w⊤\nn xn+1 + bn)\n\n\n∈R(n+1)×(n+1).\n(9.1.2)\nThen A being regular implies that for each (yi)n+1\ni=1 exist c and (vj)n\nj=1 such that (9.1.1) holds.\nHence, it suffices to find (wj)n\nj=1 and (bj)n\nj=1 such that A is regular.\n107\nTo do so, we proceed by induction over k = 0, . . . , n, to show that there exist (wj)k\nj=1 and\n(bj)k\nj=1 such that the first k + 1 columns of A are linearly independent. The case k = 0 is trivial.\nNext let 0 < k < n and assume that the first k columns of A are linearly independent. We wish to\nfind wk, bk such that the first k + 1 columns are linearly independent. Suppose such wk, bk do not\nexist and denote by Yk ⊆Rn+1 the space spanned by the first k columns of A. Then for all w ∈Rn,\nb ∈R the vector (σ(w⊤xi + b))n+1\ni=1 ∈Rn+1 must belong to Yk. Fix y = (yi)n+1\ni=1 ∈Rn+1\\Yk. Then\ninf\n˜Φ∈N 1\nd (σ,1)\n∥(˜Φ(xi))n+1\ni=1 −y∥2\n2 =\ninf\nN,wj,bj,vj,c\nn+1\nX\ni=1\n\u0010 N\nX\nj=1\nvjσ(w⊤\nj xi + bj) + c −yi\n\u00112\n≥inf\n˜y∈Yk ∥˜y −y∥2\n2 > 0.\nSince we can find a continuous function f : Rd →R such that f(xi) = yi for all i = 1, . . . , n + 1,\nthis contradicts Theorem 3.8.\n9.2\nOptimal interpolation and reconstruction\nConsider a bounded domain Ω⊆Rd, a function f : Ω→R, distinct points x1, . . . , xm ∈Ω, and\ncorresponding function values yi := f(xi). Our objective is to approximate f based solely on the\ndata pairs (xi, yi), i = 1, . . . , m. In this section, we will show that, under certain assumptions on\nf, ReLU neural networks can express an “optimal” reconstruction which also turns out to be an\ninterpolant of the data.\n9.2.1\nMotivation\nIn the previous section, we observed that neural networks with m −1 ∈N hidden neurons can\ninterpolate m points for every reasonable activation function. However, not all interpolants are\nequally suitable for a given application. For instance, consider Figure 9.1 for a comparison between\npolynomial and piecewise affine interpolation on the unit interval.\nThe two interpolants exhibit rather different behaviors. In general, there is no way of deter-\nmining which constitutes a better approximation to f. In particular, given our limited information\nabout f, we cannot accurately reconstruct any additional features that may exist between inter-\npolation points x1, . . . , xm. In accordance with Occam’s razor, it thus seems reasonable to assume\nthat f does not exhibit extreme oscillations or behave erratically between interpolation points.\nAs such, the piecewise interpolant appears preferable in this scenario. One way to formalize the\nassumption that f does not “exhibit extreme oscillations” is to assume that the Lipschitz constant\nLip(f) := sup\nx̸=y\n|f(x) −f(y)|\n∥x −y∥\n(9.2.1)\nof f is bounded by a fixed value M ∈R. Here ∥· ∥denotes an arbitrary fixed norm on Rd.\nHow should we choose M? For every function f : Ω→R satisfying\nf(xi) = yi\nfor all\ni = 1, . . . , m,\n(9.2.2)\nwe have\nLip(f) =\nsup\nx̸=y∈Ω\n|f(x) −f(y)|\n∥x −y∥\n≥sup\ni̸=j\n|yi −yj|\n∥xi −xj∥=: ˜\nM.\n(9.2.3)\n108\nFigure 9.1: Interpolation of eight points by a polynomial of degree seven and by a piecewise affine\nspline. The polynomial interpolation has a significantly larger derivative or Lipschitz constant than\nthe piecewise affine interpolator.\nBecause of this, we fix M as a real number greater than or equal to ˜\nM for the remainder of our\nanalysis.\n9.2.2\nOptimal reconstruction for Lipschitz continuous functions\nThe above considerations raise the following question: Given only the information that the function\nhas Lipschitz constant at most M, what is the best reconstruction of f based on the data?\nWe\nconsider here the “best reconstruction” to be a function that minimizes the L∞-error in the worst\ncase. Specifically, with\nLipM(Ω) := {f : Ω→R | Lip(f) ≤M},\n(9.2.4)\ndenoting the set of all functions with Lipschitz constant at most M, we want to solve the following\nproblem:\nProblem 9.4. We wish to find an element\nΦ ∈argminh:Ω→R\nsup\nf∈LipM(Ω)\nf satisfies (9.2.2)\nsup\nx∈Ω\n|f(x) −h(x)|.\n(9.2.5)\nThe next theorem shows that a function Φ as in (9.2.5) indeed exists. This Φ not only allows\nfor an explicit formula, it also belongs to LipM(Ω) and additionally interpolates the data. Hence,\nit is not just an optimal reconstruction, it is also an optimal interpolant. This theorem goes back\nto [13], which, in turn, is based on [219].\n109\nTheorem 9.5. Let m, d ∈N, Ω⊆Rd, f : Ω→R, and let x1, . . . , xm ∈Ω, y1, . . . , ym ∈R satisfy\n(9.2.2) and (9.2.3) with ˜\nM > 0. Further, let M ≥˜\nM.\nThen, Problem 9.4 has at least one solution given by\nΦ(x) := 1\n2(fupper(x) + flower(x))\nfor x ∈Ω,\n(9.2.6)\nwhere\nfupper(x) :=\nmin\nk=1,...,m(yk + M∥x −xk∥)\nflower(x) :=\nmax\nk=1,...,m(yk −M∥x −xk∥).\nMoreover, Φ ∈LipM(Ω) and Φ interpolates the data (i.e. satisfies (9.2.2)).\nProof. First we claim that for all h1, h2 ∈LipM(Ω) holds max{h1, h2} ∈LipM(Ω) as well as\nmin{h1, h2} ∈LipM(Ω). Since min{h1, h2} = −max{−h1, −h2}, it suffices to show the claim for\nthe maximum. We need to check that\n| max{h1(x), h2(x)} −max{h1(y), h2(y)}|\n∥x −y∥\n≤M\n(9.2.7)\nfor all x ̸= y ∈Ω. Fix x ̸= y. Without loss of generality we assume that\nmax{h1(x), h2(x)} ≥max{h1(y), h2(y)}\nand\nmax{h1(x), h2(x)} = h1(x).\nIf max{h1(y), h2(y)} = h1(y) then the numerator in (9.2.7) equals h1(x)−h1(y) which is bounded\nby M∥x −y∥. If max{h1(y), h2(y)} = h2(y), then the numerator equals h1(x) −h2(y) which is\nbounded by h1(x) −h1(y) ≤M∥x −y∥. In either case (9.2.7) holds.\nClearly, x 7→yk−M∥x−xk∥∈LipM(Ω) for each k = 1, . . . , m and thus fupper, flower ∈LipM(Ω)\nas well as Φ ∈LipM(Ω).\nNext we claim that for all f ∈LipM(Ω) satisfying (9.2.2) holds\nflower(x) ≤f(x) ≤fupper(x)\nfor all\nx ∈Ω.\n(9.2.8)\nThis is true since for every k ∈{1, . . . , m} and x ∈Ω\n|yk −f(x)| = |f(xk) −f(x)| ≤M∥x −xk∥\nso that for all x ∈Ω\nf(x) ≤\nmin\nk=1,...,m(yk + M∥x −xk∥),\nf(x) ≥\nmax\nk=1,...,m(yk −M∥x −xk∥).\nSince fupper, flower ∈LipM(Ω) satisfy (9.2.2), we conclude that for every h : Ω→R holds\nsup\nf∈LipM(Ω)\nf satisfies (9.2.2)\nsup\nx∈Ω\n|f(x) −h(x)| ≥sup\nx∈Ω\nmax{|flower(x) −h(x)|, |fupper(x) −h(x)|}\n≥sup\nx∈Ω\n|flower(x) −fupper(x)|\n2\n.\n(9.2.9)\n110\nMoreover, using (9.2.8),\nsup\nf∈LipM(Ω)\nf satisfies (9.2.2)\nsup\nx∈Ω\n|f(x) −Φ(x)| ≤sup\nx∈Ω\nmax{|flower(x) −Φ(x)|, |fupper(x) −Φ(x)|}\n= sup\nx∈Ω\n|flower(x) −fupper(x)|\n2\n.\n(9.2.10)\nFinally, (9.2.9) and (9.2.10) imply that Φ is a solution of Problem 9.4.\nFigure 9.2 depicts fupper, flower, and Φ for the interpolation problem shown in Figure 9.1, while\nFigure 9.3 provides a two-dimensional example.\nFigure 9.2: Interpolation of the points from Figure 9.1 with the optimal Lipschitz interpolant.\n9.2.3\nOptimal ReLU reconstructions\nSo far everything was valid with an arbitrary norm on Rd. For the next theorem, we will restrict\nourselves to the 1-norm ∥x∥1 = Pd\nj=1 |xj|. Using the explicit formula of Theorem 9.5, we will now\nshow the remarkable result that ReLU neural networks can exactly express an optimal reconstruc-\ntion (in the sense of Problem 9.4) with a neural network whose size scales linearly in the product\nof the dimension d and the number of data points m.\nAdditionally, the proof is constructive,\nthus allowing in principle for an explicit construction on the neural network without the need for\ntraining.\nTheorem 9.6 (Optimal Lipschitz Reconstruction). Let m, d ∈N, Ω⊆Rd, f : Ω→R, and let\nx1, . . . , xm ∈Ω, y1, . . . , ym ∈R satisfy (9.2.2) and (9.2.3) with ˜\nM > 0. Further, let M ≥˜\nM and\nlet ∥· ∥= ∥· ∥1 in (9.2.3) and (9.2.4).\n111\nThen, there exists a ReLU neural network Φ ∈LipM(Ω) that interpolates the data (i.e. satisfies\n(9.2.2)) and satisfies\nΦ ∈argminh:Ω→R\nsup\nf∈LipM(Ω)\nf satisfies (9.2.2)\nsup\nx∈Ω\n|f(x) −h(x)|.\nMoreover, depth(Φ) = O(log(m)), width(Φ) = O(dm) and all weights of Φ are bounded in absolute\nvalue by max{M, ∥y∥∞}.\nProof. To prove the result, we simply need to show that the function in (9.2.6) can be expressed as\na ReLU neural network with the size bounds described in the theorem. First we notice, that there\nis a simple ReLU neural network that implements the 1-norm. It holds for all x ∈Rd that\n∥x∥1 =\nd\nX\ni=1\n(σ(xi) + σ(−xi)) .\nThus, there exists a ReLU neural network Φ∥·∥1 such that for all x ∈Rd\nwidth(Φ∥·∥1) = 2d,\ndepth(Φ∥·∥1) = 1,\nΦ∥·∥1(x) = ∥x∥1\nAs a result, there exist ReLU neural networks Φk : Rd →R, k = 1, . . . , m, such that\nwidth(Φk) = 2d,\ndepth(Φk) = 1,\nΦk(x) = yk + M∥x −xk∥1\nfor all x ∈Rd. Using the parallelization of neural networks introduced in Section 5.1.3, there exists\na ReLU neural network Φall := (Φ1, . . . , Φm): Rd →Rm such that\nwidth(Φall) = 4md,\ndepth(Φall) = 1\nand\nΦall(x) = (yk + M∥x −xk∥1)m\nk=1\nfor all x ∈Rd.\nUsing Lemma 5.11, we can now find a ReLU neural network Φupper such that Φupper = fupper(x)\nfor all x ∈Ω, width(Φupper) ≤max{16m, 4md}, and depth(Φupper) ≤1 + log(m).\nEssentially the same construction yields a ReLU neural network Φlower with the respective\nproperties. Lemma 5.4 then completes the proof.\nBibliography and further reading\nThe universal interpolation theorem stated in this chapter is due to [176, Theorem 5.1]. There exist\nseveral earlier interpolation results, which were shown under stronger assumptions: In [200], the\ninterpolation property is already linked with a rank condition on the matrix (9.1.2). However, no\ngeneral conditions on the activation functions were formulated. In [105], the interpolation theorem\nis established under the assumption that the activation function σ is continuous and nondecreasing,\n112\nlimx→−∞σ(x) = 0, and limx→∞σ(x) = 1. This result was improved in [97], which dropped the\nnondecreasing assumption on σ.\nThe main idea of the optimal Lipschitz interpolation theorem in Section 9.2 is due to [13]. A\nneural network construction of Lipschitz interpolants, which however is not the optimal interpolant\nin the sense of Problem 9.4, is given in [108, Theorem 2.27].\n113\nExercises\nExercise 9.7. Under the assumptions of Theorem 9.5, we define for x ∈Ωthe set of nearest\nneighbors by\nIx := argmini=1,...,m ∥xi −x∥.\nThe one-nearest-neighbor classifier f1NN is defined by\nf1NN(x) = 1\n2(min\ni∈Ix yi + max\ni∈Ix yi).\nLet ΦM be the function in (9.2.6). Show that for all x ∈Ω\nΦM(x) →f1NN(x)\nas M →∞.\nExercise 9.8. Extend Theorem 9.6 to the ∥· ∥∞-norm. Hint: The resulting neural network will\nneed to be deeper than the one of Theorem 9.6.\n114\nFigure 9.3: Two-dimensional example of the interpolation method of (9.2.6). From top left to\nbottom we see fupper, flower, and Φ.\nThe interpolation points (xi, yi)6\ni=1 are marked with red\ncrosses.\n115\nChapter 10\nTraining of neural networks\nUp to this point, we have discussed the representation and approximation of certain function classes\nusing neural networks. The second pillar of deep learning concerns the question of how to fit a\nneural network to given data, i.e., having fixed an architecture, how to find suitable weights and\nbiases. This task amounts to minimizing a so-called objective function such as the empirical risk\nˆRS in (1.2.3). Throughout this chapter we denote the objective function by\nf : Rn →R,\nand interpret it as a function of all neural network weights and biases collected in a vector in Rn.\nThe goal is to (approximately) determine a minimizer, i.e., some w∗∈Rn satisfying\nf(w∗) ≤f(w)\nfor all w ∈Rn.\nStandard approaches include, in particular, variants of (stochastic) gradient descent. These are\nthe topic of this chapter, in which we present basic ideas and results in convex optimization using\ngradient-based methods.\n10.1\nGradient descent\nThe general idea of gradient descent is to start with some w0 ∈Rn, and then apply sequential\nupdates by moving in the direction of steepest descent of the objective function. Assume for the\nmoment that f ∈C2(Rn), and denote the kth iterate by wk. Then\nf(wk + v) = f(wk) + v⊤∇f(wk) + O(∥v∥2)\nfor ∥v∥2 →0.\n(10.1.1)\nThis shows that the change in f around wk is locally described by the gradient ∇f(wk). For\nsmall v the contribution of the second order term is negligible, and the direction v along which the\ndecrease of the risk is maximized equals the negative gradient −∇f(wk). Thus, −∇f(wk) is also\ncalled the direction of steepest descent. This leads to an update of the form\nwk+1 := wk −hk∇f(wk),\n(10.1.2)\nwhere hk > 0 is referred to as the step size or learning rate. We refer to this iterative algorithm\nas gradient descent.\nIn practice tuning the learning rate can be a subtle issue as it should strike a balance between\nthe following dissenting requirements:\n116\nFigure 10.1: Two examples of gradient descent as defined in (10.1.2). The red points represent the\nwk.\n(i) hk needs to be sufficiently small so that with v = −hk∇f(wk), the second-order term in\n(10.1.1) is not dominating. This ensures that the update (10.1.2) decreases the objective\nfunction.\n(ii) hk should be large enough to ensure significant decrease of the objective function, which\nfacilitates faster convergence of the algorithm.\nA learning rate that is too high might overshoot the minimum, while a rate that is too low results\nin slow convergence. Common strategies include, in particular, constant learning rates (hk = h\nfor all k ∈N0), learning rate schedules such as decaying learning rates (hk ↘0 as k →∞), and\nadaptive methods. For adaptive methods the algorithm dynamically adjust hk based on the values\nof f(wj) or ∇f(wj) for j ≤k.\nRemark 10.1. It is instructive to interpret (10.1.2) as an Euler discretization of the “gradient flow”\nw(0) = w0,\nw′(t) = −∇f(w(t))\nfor t ∈[0, ∞).\n(10.1.3)\nThis ODE describes the movement of a particle w(t), whose velocity at time t ≥0 equals −∇f(w(t))—\nthe vector of steepest descent. Note that\ndf(w(t))\ndt\n=\n\n∇f(w(t)), w′(t)\n\u000b\n= −∥∇f(w(t))∥2,\nand thus the dynamics (10.1.3) necessarily decreases the value of the objective function along its\npath as long as ∇f(w(t)) ̸= 0.\nThroughout the rest of Section 10.1 we assume that w0 ∈Rn is arbitrary, and the sequence\n(wk)k∈N0 is generated by (10.1.2). We will analyze the convergence of this algorithm under suitable\nassumptions on f and the hk. The proofs primarily follow the arguments in [159, Chapter 2]. We\nalso refer to that book for a much more detailed discussion of gradient descent, and further reading\non convex optimization.\n117\n10.1.1\nL-smoothness\nA key assumption to analyze convergence of (10.1.2) is Lipschitz continuity of ∇f.\nDefinition 10.2. Let n ∈N, and L > 0.\nThe function f : Rn →R is called L-smooth if\nf ∈C1(Rn) and\n∥∇f(w) −∇f(v)∥≤L∥w −v∥\nfor all w, v ∈Rn.\nFor fixed w, L-smoothness implies the linear growth bound\n∥∇f(w + v)∥≤∥∇f(w)∥+ L∥v∥\nfor ∇f. Integrating the gradient along lines in Rn then shows that f is bounded from above by a\nquadratic function touching the graph of f at w, as stated in the next lemma; also see Figure 10.2.\nLemma 10.3. Let n ∈N and L > 0. Let f : Rn →R be L-smooth. Then\nf(v) ≤f(w) + ⟨∇f(w), v −w⟩+ L\n2 ∥w −v∥2\nfor all w, v ∈Rn.\n(10.1.4)\nProof. We have for all w, v ∈Rn\nf(v) = f(w) +\nZ 1\n0\n⟨∇f(w + t(v −w)), v −w⟩dt\n= f(w) + ⟨∇f(w), v −w⟩+\nZ 1\n0\n⟨∇f(w + t(v −w)) −∇f(w), v −w⟩dt.\nThus\nf(v) −f(w) −⟨∇f(w), v −w⟩≤\nZ 1\n0\nL∥t(v −w)∥∥v −w∥dt = L\n2 ∥v −w∥2,\nwhich shows (10.1.4).\nRemark 10.4. The argument in the proof of Lemma 10.3 also gives the lower bound\nf(v) ≥f(w) + ⟨∇f(w), v −w⟩−L\n2 ∥w −v∥2\nfor all w, v ∈Rn.\n(10.1.5)\nThe previous lemma allows us to show a decay property for the gradient descent iterates.\nSpecifically, the values of f necessarily decrease in each iteration as long as the step size hk is small\nenough, and ∇f(wk) ̸= 0.\n118\nLemma 10.5. Let n ∈N and L > 0. Let f : Rn →R be L-smooth. Further, let (hk)∞\nk=1 be positive\nnumbers and let (wk)∞\nk=0 ⊆Rn be defined by (10.1.2).\nThen, for all k ∈N\nf(wk+1) ≤f(wk) −\n\u0010\nhk −Lh2\nk\n2\n\u0011\n∥∇f(wk)∥2.\n(10.1.6)\nProof. Lemma 10.3 with v = wk+1 and w = wk gives\nf(wk+1) ≤f(wk) + ⟨∇f(wk), −hk∇f(wk)⟩+ L\n2 ∥hk∇f(wk)∥2,\nwhich corresponds to (10.1.6).\nRemark 10.6. The right-hand side in (10.1.6) is minimized for step size hk = 1/L, in which case\n(10.1.6) reads\nf(wk+1) ≤f(wk) −1\n2L∥∇f(wk)∥2.\nNext, let us discuss the behavior of the gradients for constant step sizes.\nProposition 10.7. Let n ∈N and L > 0. Let f : Rn →R be L-smooth. Further, let hk = h ∈\n(0, 2/L) for all k ∈N, and (wk)∞\nk=0 ⊆Rn be defined by (10.1.2).\nThen, for all k ∈N\n1\nk + 1\nk\nX\nj=0\n∥∇f(wj)∥2 ≤\n1\nk + 1\n2\n2h −Lh2 (f(w0) −f(wk+1)).\n(10.1.7)\nProof. Set c := h −(Lh2)/2 = (2h −Lh2)/2 > 0. By (10.1.6) for j ≥0\nf(wj) −f(wj+1) ≥c∥∇f(wj)∥2.\nHence\nk\nX\nj=0\n∥∇f(wj)∥2 ≤1\nc\nk\nX\nj=0\nf(wj) −f(wj+1) = 1\nc (f(w0) −f(wk+1)) .\nDividing by k + 1 concludes the proof.\nSuppose that f is bounded from below, i.e. infw∈Rn f(w) > −∞. In this case, the right-hand\nside in (10.1.7) behaves like O(k−1) as k →∞, and (10.1.7) implies\nmin\nj=1,...,k ∥∇f(wj)∥= O(k−1/2).\nThus, lower boundedness of the objective function together with L-smoothness already suffice to\nobtain some form of convergence of the gradients to 0. We emphasize that this does not imply\nconvergence of wk towards some w∗with ∇f(w∗) = 0 as the example f(w) = arctan(w), w ∈R,\nshows.\n119\n10.1.2\nConvexity\nWhile L-smoothness entails some interesting properties of gradient descent, it does not have any\ndirect implications on the existence or uniqueness of minimizers. To show convergence of f(wk)\ntowards minw f(w) for k →∞(assuming this minimum exists), we will assume that f is a convex\nfunction.\nDefinition 10.8. Let n ∈N. A function f : Rn →R is called convex if and only if\nf(λw + (1 −λ)v) ≤λf(w) + (1 −λ)f(v),\n(10.1.8)\nfor all w, v ∈Rn, λ ∈(0, 1).\nLet n ∈N. If f ∈C1(Rn), then f is convex if and only if\nf(w) + ⟨∇f(w), v −w⟩≤f(v)\nfor all w, v ∈Rn,\n(10.1.9)\nas shown in Exercise 10.27. Thus, f ∈C1(Rn) is convex if and only if the graph of f lies above\neach of its tangents, see Figure 10.2.\nFor convex f, a minimizer neither needs to exist (e.g., f(w) = w for w ∈R) nor be unique\n(e.g., f(w) = 0 for w ∈Rn).\nHowever, if w∗and v∗are two minimizers, then every convex\ncombination λw∗+ (1 −λ)v∗, λ ∈[0, 1], is also a minimizer due to (10.1.8). Thus, the set of all\nminimizers is convex. In particular, a convex objective function has either zero, one, or infinitely\nmany minimizers. Moreover, if f ∈C1(Rn) then ∇f(w) = 0 implies\nf(w) = f(w) + ⟨∇f(w), v −w⟩≤f(v)\nfor all v ∈Rn.\nThus, w is a minimizer of f if and only if ∇f(w) = 0.\nBy Lemma 10.5, smallness of the step sizes and L-smoothness suffice to show a decay property\nfor the objective function f. Under the additional assumption of convexity, we also get a decay\nproperty for the distance of wk to any minimizer w∗.\nLemma 10.9. Let n ∈N and L > 0. Let f : Rn →R be L-smooth and convex. Further, let\nhk ∈(0, 2/L) for all k ∈N0, and (wk)∞\nk=0 ⊆Rn be defined by (10.1.2). Suppose that w∗is a\nminimizer of f.\nThen, for all k ∈N0\n∥wk+1 −w∗∥2 ≤∥wk −w∗∥2 −hk ·\n\u0010 2\nL −hk\n\u0011\n∥∇f(wk)∥2.\nTo prove the lemma, we will require the following inequality [159, Theorem 2.1.5].\n120\nLemma 10.10. Let n ∈N and L > 0. Let f : Rn →R be L-smooth and convex.\nThen,\n1\nL∥∇f(w) −∇f(v)∥2 ≤⟨∇f(w) −∇f(v), w −v⟩\nfor all w, v ∈Rn.\nProof. Fix w ∈Rn and set Ψ(u) := f(u) −⟨∇f(w), u⟩for all u ∈Rn. Then ∇Ψ(u) = ∇f(u) −\n∇f(w) and thus Ψ is L-smooth. Moreover, convexity of f, specifically (10.1.9), yields Ψ(u) ≥\nf(w) −⟨∇f(w), w⟩= Ψ(w) for all u ∈Rn, and thus w is a minimizer of Ψ. Using (10.1.4) on Ψ\nwe get for every v ∈Rn\nΨ(w) = min\nu∈Rn Ψ(u) ≤min\nu∈Rn\n\u0010\nΨ(v) + ⟨∇Ψ(v), u −v⟩+ L\n2 ∥u −v∥2\u0011\n= min\nt≥0 Ψ(v) −t∥∇Ψ(v)∥2 + t2 L\n2 ∥∇Ψ(v)∥2\n= Ψ(v) −1\n2L∥∇Ψ(v)∥2\nsince the minimum of t 7→t2L/2 −t is attained at t = L−1. This implies\nf(w) −f(v) + 1\n2L∥∇f(w) −∇f(v)∥2 ≤⟨∇f(w), w −v⟩.\nAdding the same inequality with the roles of w and v switched gives the result.\nof Lemma 10.9. It holds\n∥wk+1 −w∗∥2 = ∥wk −w∗∥2 −2hk ⟨∇f(wk), wk −w∗⟩+ h2\nk∥∇f(wk)∥2.\nSince ∇f(w∗) = 0, Lemma 10.10 gives\n−⟨∇f(wk), wk −w∗⟩≤−1\nL∥∇f(wk)∥2\nwhich implies the claim.\nThese preparations allow us to show that for constant step size h < 2/L, we obtain convergence\nof f(wk) towards f(w∗) with rate O(k−1), as stated in the next theorem which corresponds to\n[159, Theorem 2.1.14].\nTheorem 10.11. Let n ∈N and L > 0. Let f : Rn →R be L-smooth and convex. Further, let\nhk = h ∈(0, 2/L) for all k ∈N0, and let (wk)∞\nk=0 ⊆Rn be defined by (10.1.2). Suppose that w∗is\na minimizer of f.\nThen, f(wk) −f(w∗) = O(k−1) for k →∞, and for the specific choice h = 1/L\nf(wk) −f(w∗) ≤\n2L\n4 + k∥w0 −w∗∥2\nfor all k ∈N0.\n(10.1.10)\n121\nProof. The case w0 = w∗is trivial and throughout we assume w0 ̸= w∗.\nStep 1. Let j ∈N0. Using convexity (10.1.9)\nf(wj) −f(w∗) ≤−⟨∇f(wj), w∗−wj⟩≤∥∇f(wj)∥∥w∗−wj∥.\n(10.1.11)\nBy Lemma 10.9 and since w0 ̸= w∗it holds ∥w∗−wj∥≤∥w∗−w0∦= 0, so that we obtain a\nlower bound on the gradient\n∥∇f(wj)∥2 ≥(f(wj) −f(w∗))2\n∥w∗−w0∥2\n.\nLemma 10.5 then yields\nf(wj+1) −f(w∗) ≤f(wj) −f(w∗) −\n\u0010\nh −Lh2\n2\n\u0011\n∥∇f(wj)∥2\n≤f(wj) −f(w∗) −\n\u0010\nh −Lh2\n2\n\u0011(f(wj) −f(w∗))2\n∥w0 −w∗∥2\n.\nWith ej := f(wj) −f(w∗) and ω := (h −Lh2/2)/∥w0 −w∗∥2 this reads\nej+1 ≤ej −ωe2\nj = ej · (1 −ωej),\n(10.1.12)\nwhich is valid for all j ∈N0.\nStep 2. By L-smoothness (10.1.4) and ∇f(w∗) = 0 it holds\nf(w0) −f(w∗) ≤L\n2 ∥w0 −w∗∥2,\n(10.1.13)\nwhich implies (10.1.10) for k = 0. It remains to show the bound for k ∈N.\nFix k ∈N. We may assume ek > 0, since otherwise (10.1.10) is trivial. Then ej > 0 for all\nj = 0, . . . , k −1 since ej = 0 implies ei = 0 for all i > j, contradicting ek > 0. Moreover, ωej < 1\nfor all j = 0, . . . , k −1, since ωej ≥1 implies ej+1 ≤0 by (10.1.12), contradicting ej+1 > 0.\nUsing that 1/(1 −x) ≥1 + x for all x ∈[0, 1), (10.1.12) thus gives\n1\nej+1\n≥1\nej\n(1 + ωej) = 1\nej\n+ ω\nfor all j = 0, . . . , k −1.\nHence\n1\nek\n−1\ne0\n=\nk−1\nX\nj=0\n\u0010 1\nej+1\n−1\nej\n\u0011\n≥kω\nand\nf(wk) −f(w∗) = ek ≤\n1\n1\ne0 + kω =\n1\n1\nf(w0)−f(w∗) + k (h−Lh2/2)\n∥w0−w∗∥2\n.\nUsing (10.1.13) we get\nf(wk) −f(w∗) ≤\n∥w0 −w∗∥2\n2\nL + kh · (1 −Lh\n2 ) = O(k−1).\n(10.1.14)\nFinally, (10.1.10) follows by plugging in h = 1/L.\n122\nL-smooth\nconvex\nµ-strongly convex\nFigure 10.2: The graph of L-smooth functions lies between two quadratic functions at each point,\nsee (10.1.4) and (10.1.5), the graph of convex functions lies above the tangent at each point, see\n(10.1.9), and the graph of µ-strongly convex functions lies above a quadratic function at each point,\nsee (10.1.15).\nRemark 10.12. The step size h = 1/L is again such that the upper bound in (10.1.14) is minimized.\nWe emphasize, that while under the assumptions of Theorem 10.11 it holds f(wk) →f(w∗),\nin general it is not true that wk →w∗as k →∞. To show the convergence of the wk, we need to\nintroduce stronger assumptions that guarantee the existence of a unique minimizer.\n10.1.3\nStrong convexity\nTo obtain faster convergence and guarantee the existence of unique minimizers, we next introduce\nthe notion of strong convexity. As the terminology suggests, strong convexity implies convexity;\nspecifically, while convexity requires f to be lower bounded by the linearization around each point,\nstrongly convex functions are lower bounded by the linearization plus a positive quadratic term.\nDefinition 10.13. Let n ∈N and µ > 0. A function f ∈C1(Rn) is called µ-strongly convex if\nf(v) ≥f(w) + ⟨∇f(w), v −w⟩+ µ\n2 ∥v −w∥2\nfor all w, v ∈Rn.\n(10.1.15)\nNote that (10.1.15) is the opposite of the bound (10.1.4) implied by L-smoothness. We depict\nthe three notions of L-smoothness, convexity, and µ-strong convexity in Figure 10.2.\nEvery µ-strongly convex function has a unique minimizer. To see this note first that (10.1.15)\nimplies f to be lower bounded by a convex quadratic function, so that there exists at least one\nminimizer w∗, and ∇f(w∗) = 0. By (10.1.15) we then have f(v) > f(w∗) for every v ̸= w∗.\nThe next theorem shows that the gradient descent iterates converge linearly towards the unique\nminimizer for L-smooth and µ-strongly convex functions.\nRecall that a sequence ek is said to\nconverge linearly to 0, if and only if there exist constants C > 0 and c ∈[0, 1) such that\nek ≤Cck\nfor all k ∈N0.\nThe constant c is also referred to as the rate of convergence. Before giving the statement, we first\nnote that comparing (10.1.4) and (10.1.15) it necessarily holds L ≥µ and therefore κ := L/µ ≥1.\nThis term is known as the condition number of f. It crucially influences the rate of convergence.\n123\nTheorem 10.14. Let n ∈N and L ≥µ > 0. Let f : Rn →R be L-smooth and µ-strongly convex.\nFurther, let hk = h ∈(0, 1/L] for all k ∈N0, let (wk)∞\nk=0 ⊆Rn be defined by (10.1.2), and let w∗\nbe the unique minimizer of f.\nThen, f(wk) →f(w∗) and wk →w∗converge linearly for k →∞. For the specific choice\nh = 1/L\n∥wk −w∗∥2 ≤\n\u0010\n1 −µ\nL\n\u0011k\n∥w0 −w∗∥2\n(10.1.16a)\nf(wk) −f(w∗) ≤L\n2\n\u0010\n1 −µ\nL\n\u0011k\n∥w0 −w∗∥2.\n(10.1.16b)\nProof. It suffices to show (10.1.16a) since (10.1.16b) follows directly by Lemma 10.3 and because\n∇f(w∗) = 0. The case k = 0 is trivial, so let k ∈N.\nExpanding wk = wk−1 −h∇f(wk−1) and using µ-strong convexity (10.1.15)\n∥wk −w∗∥2 = ∥wk−1 −w∗∥2 −2h ⟨∇f(wk−1), wk−1 −w∗⟩+ h2∥∇f(wk−1)∥2\n≤(1 −µh)∥wk−1 −w∗∥2 −2h · (f(wk−1) −f(w∗)) + h2∥∇f(wk−1)∥2.\nMoreover, the descent property in Lemma 10.5 gives\n−2h · (f(wk−1) −f(w∗)) + h2∥∇f(wk−1)∥2\n≤−2h · (f(wk−1) −f(w∗)) +\nh2\nh · (1 −Lh/2)(f(wk−1) −f(wk)).\n(10.1.17)\nThe descent property also implies f(wk−1) −f(w∗) ≥f(wk−1) −f(wk). Thus the right-hand side\nof (10.1.17) is less or equal to zero as long as 2h ≥h/(1 −Lh/2), which is equivalent to h ≤1/L.\nHence\n∥wk −w∗∥2 ≤(1 −µh)∥wk−1 −w∗∥2 ≤· · · ≤(1 −µh)k∥w0 −w∗∥2.\nThis concludes the proof.\nRemark 10.15. With a more refined argument, see [159, Theorem 2.1.15], the constraint on the\nstep size can be relaxed to h ≤2/(µ + L). For h = 2/(µ + L) one then obtains (10.1.16) with\n1 −µ/L = 1 −κ−1 replaced by\n\u0010L/µ −1\nL/µ + 1\n\u00112\n=\n\u0010κ −1\nκ + 1\n\u00112\n∈[0, 1).\n(10.1.18)\nWe have\n\u0010κ −1\nκ + 1\n\u00112\n= 1 −4κ−1 + O(κ−2)\nas κ →∞. Thus, (10.1.18) gives a slightly better, but conceptually similar, rate of convergence\nthan 1 −κ−1 shown in Theorem 10.14.\n124\n10.1.4\nPL-inequality\nLinear convergence for gradient descent can also be shown under a weaker assumption known as\nthe Polyak- Lojasiewicz-inequality, or PL-inequality for short.\nLemma 10.16. Let n ∈N and µ > 0. Let f : Rn →R be µ-strongly convex and denote its unique\nminimizer by w∗. Then f satisfies the PL-inequality\nµ · (f(w) −f(w∗)) ≤1\n2∥∇f(w)∥2\nfor all w ∈Rn.\n(10.1.19)\nProof. By µ-strong convexity we have\nf(v) ≥f(w) + ⟨∇f(w), v −w⟩+ µ\n2 ∥v −w∥2\nfor all v, w ∈Rn.\n(10.1.20)\nThe gradient of the right-hand side with respect to v equals ∇f(w) + µ · (v −w). This implies\nthat the minimum of this expression is attained at v = w −∇f(w)/µ. Minimizing both sides of\n(10.1.20) in v we thus find\nf(w∗) ≥f(w) −1\nµ∥∇f(w)∥2 + 1\n2µ∥∇f(w)∥2 = f(w) −1\n2µ∥∇f(w)∥2.\nRearranging the terms gives (10.1.19).\nAs the lemma states, the PL-inequality is implied by strong convexity. Moreover, it is indeed\nweaker than strong convexity, and does not even imply convexity, see Exercise 10.28. The next\ntheorem, which corresponds to [220, Theorem 1], gives a convergence result for L-smooth functions\nsatisfying the PL-inequality. It therefore does not require convexity. The proof is left as an exercise.\nWe only note that the PL-inequality bounds the distance to the minimal value of the objective\nfunction by the squared norm of the gradient. It is thus precisely the type of bound required to\nshow convergence of gradient descent.\nTheorem 10.17. Let n ∈N and L > 0. Let f : Rn →R be L-smooth. Further, let hk = 1/L for\nall k ∈N0, and let (wk)∞\nk=0 ⊆Rn be defined by (10.1.2), and let w∗be a (not necessarily unique)\nminimizer of f, so that the PL-inequality (10.1.19) holds.\nThen, it holds for all k ∈N0 that\nf(wk) −f(w∗) ≤\n\u0010\n1 −µ\nL\n\u0011k\n(f(w0) −f(w∗)).\n10.2\nStochastic gradient descent (SGD)\nWe next discuss a stochastic variant of gradient descent. The idea, which originally goes back to\nRobbins and Monro [191], is to replace the gradient ∇f(wk) in (10.1.2) by a random variable that\n125\nwe denote by Gk. We interpret Gk as an approximation to ∇f(wk); specifically, throughout we\nwill assume that (given wk) Gk is an unbiased estimator, i.e.\nE[Gk|wk] = ∇f(wk).\n(10.2.1)\nAfter choosing some initial value w0 ∈Rn, the update rule becomes\nwk+1 := wk −hkGk,\n(10.2.2)\nwhere hk > 0 denotes again the step size, and unlike in Section 10.1, we focus here on the case of hk\ndepending on k. The iteration (10.2.2) creates a Markov chain (w0, w1, . . . ), meaning that wk is\na random variable, and its state only depends1 on wk−1. The main reason for replacing the actual\ngradient by an estimator, is not to improve the accuracy or convergence rate, but rather to decrease\nthe computational cost and storage requirements of the algorithm. The underlying assumption is\nthat Gk−1 can be computed at a fraction of the cost required for the computation of ∇f(wk−1).\nThe next example illustrates this in the standard setting.\nExample 10.18 (Empirical risk minimization). Suppose we have some data S := (xj, yj)m\nj=1,\nwhere yj ∈R is the label corresponding to the data point xj ∈Rd. Using the square loss, we wish\nto fit a neural network Φ(·, w) : Rd →R depending on parameters (i.e. weights and biases) w ∈Rn,\nsuch that the empirical risk\nf(w) := ˆRS(w) =\n1\n2m\nm\nX\nj=1\n(Φ(xj, w) −yj)2,\nis minimized. Performing one step of gradient descent requires the computation of\n∇f(w) = 1\nm\nm\nX\nj=1\n(Φ(xj, w) −yj)∇wΦ(xj, w),\n(10.2.3)\nand thus the computation of m gradients of the neural network Φ. For large m (in practice m can\nbe in the millions or even larger), this computation might be infeasible. To decrease computational\ncomplexity, we replace the full gradient (10.2.3) by\nG := (Φ(xj, w) −yj)∇wΦ(xj, w)\nwhere j ∼uniform(1, . . . , m) is a random variable with uniform distribution on the discrete set\n{1, . . . , m}. Then\nE[G] = 1\nm\nm\nX\nj=1\n(Φ(xj, w) −yj)∇wΦ(xj, w) = ∇f(w),\nbut an evaluation of G merely requires the computation of a single gradient of the neural net-\nwork.\nMore general, one can choose a mini-batch size mb (where mb ≪m) and let G =\n1\nmb\nP\nj∈J(Φ(xj, w) −yj)∇wΦ(xj, w), where J is a random subset of {1, . . . , m} of cardinality mb.\n1More precisely, given wk−1, the state of wk is conditionally independent of w1, . . . , wk−2. See Appendix A.3.3.\n126\nRemark 10.19. In practice, the following variant is also common: Let mbk = m for mb, k, m ∈N,\ni.e. the number of data points m is a k-fold multiple of the mini-batch size mb. In each epoch, first\na random partition ˙Sk\ni=1Ji = {1, . . . , m} is determined. Then for each i = 1, . . . , k, the weights are\nupdated with the gradient estimate\n1\nmb\nX\nj∈Ji\nΦ(xj −yj, w)∇wΦ(xj, w).\nHence, in one epoch (which corresponds to k updates of the neural network weights), the algorithm\nsweeps through the whole dataset.\nSGD can be analyzed in various settings. To give the general idea, we concentrate on the case\nof L-smooth and µ-strongly convex objective functions. Let us start by looking at a property akin\nto the (descent) Lemma 10.5. Using Lemma 10.3\nf(wk+1) ≤f(wk) −hk ⟨∇f(wk), Gk⟩+ h2\nk\nL\n2 ∥Gk∥2.\nIn contrast to gradient descent, we cannot say anything about the sign of the term in the middle of\nthe right-hand side. Thus, (10.2.2) need not necessarily decrease the value of the objective function\nin every step.\nThe key insight is that in expectation the value is still decreased under certain\nassumptions, namely\nE[f(wk+1)|wk] ≤f(wk) −hkE[⟨∇f(wk), Gk⟩|wk] + h2\nk\nL\n2 E\n\u0002\n∥Gk∥2\f\fwk\n\u0003\n= f(wk) −hk∥∇f(wk)∥2 + h2\nk\nL\n2 E\n\u0002\n∥Gk∥2\f\fwk\n\u0003\n= f(wk) −hk\n\u0012\n∥∇f(wk)∥2 −hk\nL\n2 E[∥Gk∥2|wk]\n\u0013\nwhere we used (10.2.1).\nAssuming, for some fixed γ > 0, the uniform bound\nE[∥Gk∥2|wk] ≤γ\nand that ∥∇f(wk)∥> 0 (which is true unless wk is the minimizer), upon choosing\n0 < hk < 2∥∇f(wk)∥2\nLγ\n,\nthe expectation of the objective function decreases. Since ∇f(wk) tends to 0 as we approach the\nminimum, this also indicates that we should choose step sizes hk that tend to 0 for k →∞. For\nour analysis we will work with the specific choice\nhk := 1\nµ\n(k + 1)2 −k2\n(k + 1)2\nfor all\nk ∈N0,\n(10.2.4)\nas, e.g., in [76]. Note that\nhk =\n2k + 1\nµ(k + 1)2 =\n2\nµ(k + 1) + O(k−2) = O(k−1).\n127\nSince wk is a random variable by construction, a convergence statement can only be stochastic,\ne.g., in expectation or with high probability. We concentrate here on the former, but emphasize\nthat also the latter can be shown.\nTheorem 10.20. Let n ∈N and L, µ, γ > 0. Let f : Rn →R be L-smooth and µ-strongly convex.\nLet (hk)∞\nk=0 satisfy (10.2.4) and let (Gk)∞\nk=0, (wk)∞\nk=0 be sequences of random variables satisfying\n(10.2.1) and (10.2.2). Assume that E[∥Gk∥2|wk] ≤γ for all k ∈N0.\nThen\nE[∥wk −w∗∥2] ≤4γ\nµ2k = O(k−1),\nE[f(wk)] −f(w∗) ≤4Lγ\n2µ2k = O(k−1)\nfor k →∞.\nProof. We proceed similar as in the proof of Theorem 10.14. It holds for k ≥1\nE[∥wk −w∗∥2|wk−1]\n= ∥wk−1 −w∗∥2 −2hk−1E[⟨Gk−1, wk−1 −w∗⟩|wk−1] + h2\nk−1E[∥Gk−1∥2|wk−1]\n= ∥wk−1 −w∗∥2 −2hk−1 ⟨∇f(wk−1), wk−1 −w∗⟩+ h2\nk−1E[∥Gk−1∥2|wk−1].\nBy µ-strong convexity (10.1.15)\n−2hk−1 ⟨∇f(wk−1), wk−1 −w∗⟩≤−µhk−1∥wk−1 −w∗∥2 −2hk−1 · (f(wk−1) −f(w∗))\n≤−µhk−1∥wk−1 −w∗∥2.\nThus\nE[∥wk −w∗∥2|wk−1] ≤(1 −µhk−1)∥wk−1 −w∗∥2 + h2\nk−1γ.\nUsing the Markov property, we have\nE[∥wk −w∗∥2|wk−1, wk−2] = E[∥wk −w∗∥2|wk−1]\nso that\nE[∥wk −w∗∥2|wk−1] ≤(1 −µhk−1)E[∥wk−1 −w∗∥2|wk−2] + h2\nk−1γ.\nWith e0 := ∥w0 −w∗∥2 and ek := E[∥wk −w∗∥2|wk−1] for k ≥1 we have found\nek ≤(1 −µhk−1)ek−1 + h2\nk−1γ\n≤(1 −µhk−1)((1 −µhk−2)ek−2 + h2\nk−2γ) + h2\nk−1γ\n≤· · · ≤e0\nk−1\nY\nj=0\n(1 −µhj) + γ\nk−1\nX\nj=0\nh2\nj\nk−1\nY\ni=j+1\n(1 −µhi).\n128\nBy choice of hi\nk−1\nY\ni=j\n(1 −µhi) =\nk−1\nY\ni=j\ni2\n(i + 1)2 = j2\nk2\nand thus\nek ≤γ\nµ2\nk−1\nX\nj=0\n\u0012(j + 1)2 −j2\n(j + 1)2\n\u00132 (j + 1)2\nk2\n≤γ\nµ2\n1\nk2\nk−1\nX\nj=0\n(2j + 1)2\n(j + 1)2\n|\n{z\n}\n≤4\n≤γ\nµ2\n4k\nk2\n≤4γ\nµ2k.\nSince E[∥wk −w∗∥2] is the expectation of E[∥wk −w∗∥2|wk−1] with respect to the random\nvariable wk−1, and e0/k2 + 4γ/(µ2k) is a constant independent of wk−1, we obtain\nE[∥wk −w∗∥2] ≤4γ\nµ2k.\nFinally, using L-smoothness\nf(wk) −f(w∗) ≤⟨∇f(w∗), wk −w∗⟩+ L\n2 ∥wk −w∗∥2 = L\n2 ∥wk −w∗∥2,\nand taking the expectation concludes the proof.\nThe specific choice of hk in (10.2.4) simplifies the calculations in the proof, but it is not necessary\nin order for the asymptotic convergence to hold. One can show similar convergence results with\nhk = c1/(c2 + k) under certain assumptions on c1, c2, e.g. [23, Theorem 4.7].\n10.3\nBackpropagation\nWe now explain how to apply gradient-based methods to the training of neural networks. Let\nΦ ∈N dL+1\nd0\n(σ; L, n) (see Definition 3.6) and assume that the activation function satisfies σ ∈C1(R).\nAs earlier, we denote the neural network parameters by\nw = ((W (0), b(0)), . . . , (W (L), b(L)))\n(10.3.1)\nwith weight matrices W (ℓ) ∈Rdℓ+1×dℓand bias vectors b(ℓ) ∈Rdℓ+1. Additionally, we fix a differ-\nentiable loss function L : RdL+1 × RdL+1 →R, e.g., L(w, ˜w) = ∥w −˜w∥2/2, and assume given data\n(xj, yj)m\nj=1 ⊆Rd0 × RdL+1. The goal is to minimize an empirical risk of the form\nf(w) := 1\nm\nm\nX\nj=1\nL(Φ(xj, w), yj)\n129\nas a function of the neural network parameters w. An application of the gradient step (10.1.2) to\nupdate the parameters requires the computation of\n∇f(w) = 1\nm\nm\nX\nj=1\n∇wL(Φ(xj, w), yj).\nFor stochastic methods, as explained in Example 10.18, we only compute the average over a (ran-\ndom) subbatch of the dataset. In either case, we need an algorithm to determine ∇wL(Φ(x, w), y),\ni.e. the gradients\n∇b(ℓ)L(Φ(x, w), y) ∈Rdℓ+1,\n∇W (ℓ)L(Φ(x, w), y) ∈Rdℓ+1×dℓ\n(10.3.2)\nfor all ℓ= 0, . . . , L.\nThe backpropagation algorithm [197] provides an efficient way to do so. To explain it, for fixed\ninput x ∈Rd0 introduce the notation\n¯x(1) := W (0)x + b(0)\n(10.3.3a)\n¯x(ℓ+1) := W (ℓ)σ(¯x(ℓ)) + b(ℓ)\nfor ℓ∈{1, . . . , L},\n(10.3.3b)\nwhere the application of σ : R →R to a vector is, as always, understood componentwise. With the\nnotation of Definition 2.1, x(ℓ) = σ(¯x(ℓ)) ∈Rdℓfor ℓ= 1, . . . , L and ¯x(L+1) = x(L+1) = Φ(x, w) ∈\nRdL+1 is the output of the neural network. Therefore, the ¯x(ℓ) for ℓ= 1, . . . , L are sometimes also\nreferred to as the preactivations.\nIn the following, we additionally fix y ∈RdL+1 and write\nL := L(Φ(x, w), y) = L(¯x(L+1), y).\nNote that ¯x(k) depends on (W (ℓ), b(ℓ)) only if k > ℓ. Since ¯x(ℓ+1) is a function of ¯x(ℓ) for each ℓ,\nby repeated application of the chain rule\n∂L\n∂W (ℓ)\nij\n=\n∂L\n∂¯x(L+1)\n|\n{z\n}\n∈R1×dL+1\n∂¯x(L+1)\n∂¯x(L)\n|\n{z\n}\n∈RdL+1×dL\n· · ·\n∂¯x(ℓ+2)\n∂¯x(ℓ+1)\n|\n{z\n}\n∈Rdℓ+2×dℓ+1\n∂¯x(ℓ+1)\n∂W (ℓ)\nij\n|\n{z\n}\n∈Rdℓ+1×1\n.\n(10.3.4)\nAn analogous calculation holds for ∂L/∂b(ℓ)\nj . Since all terms in (10.3.4) are easy to compute (see\n(10.3.3)), in principle we could use this formula to determine the gradients in (10.3.2). To avoid\nunnecessary computations, the main idea of backpropagation is to introduce\nα(ℓ) := ∇¯x(ℓ)L ∈Rdℓ\nfor all ℓ= 1, . . . , L + 1\nand observe that\n∂L\n∂W (ℓ)\nij\n= (α(ℓ+1))⊤∂¯x(ℓ+1)\n∂W (ℓ)\nij\n.\nAs the following lemma shows, the α(ℓ) can be computed recursively for ℓ= L + 1, . . . , 1. This\nexplains the name “backpropagation”. In the following, ⊙denotes the componentwise (Hadamard)\nproduct, i.e. a ⊙b = (aibi)d\ni=1 for every a, b ∈Rd.\n130\nLemma 10.21. It holds\nα(L+1) = ∇¯x(L+1)L(¯x(L+1), y)\n(10.3.5)\nand\nα(ℓ) = σ′(¯x(ℓ)) ⊙(W (ℓ))⊤α(ℓ+1)\nfor all ℓ= L, . . . , 1.\nProof. Equation (10.3.5) holds by definition. For ℓ∈{1, . . . , L} by the chain rule\nα(ℓ) =\n∂L\n∂¯x(ℓ) =\n\u0010∂¯x(ℓ+1)\n∂¯x(ℓ)\n\u0011⊤\n|\n{z\n}\n∈Rdℓ×dℓ+1\n∂L\n∂¯x(ℓ+1)\n|\n{z\n}\n∈Rdℓ+1×1\n=\n\u0010∂¯x(ℓ+1)\n∂¯x(ℓ)\n\u0011⊤\nα(ℓ+1).\nBy (10.3.3b) for i ∈{1, . . . , dℓ+1}, j ∈{1, . . . , dℓ}\n\u0010∂¯x(ℓ+1)\n∂¯x(ℓ)\n\u0011\nij = ∂¯x(ℓ+1)\ni\n∂¯x(ℓ)\nj\n= W (ℓ)\nij σ′(¯x(ℓ)\nj ).\nThus the claim follows.\nPutting everything together, we obtain explicit formulas for (10.3.2).\nProposition 10.22. It holds\n∇b(ℓ)L = α(ℓ+1) ∈Rdℓ+1\nfor ℓ= 0, . . . , L\nand\n∇W (0)L = α(1)x⊤∈Rd1×d0\nand\n∇W (ℓ)L = α(ℓ+1)σ(¯x(ℓ))⊤∈Rdℓ+1×dℓ\nfor ℓ= 1, . . . , L.\nProof. By (10.3.3a) for i, k ∈{1, . . . , d1}, and j ∈{1, . . . , d0}\n∂¯x(1)\nk\n∂b(0)\ni\n= δki\nand\n∂¯x(1)\nk\n∂W (0)\nij\n= δkixj,\nand by (10.3.3b) for ℓ∈{1, . . . , L} and i, k ∈{1, . . . , dℓ+1}, and j ∈{1, . . . , dℓ}\n∂¯x(ℓ+1)\nk\n∂b(ℓ)\ni\n= δki\nand\n∂¯x(ℓ+1)\nk\n∂W (ℓ)\nij\n= δkiσ(¯x(ℓ)\nj ).\n131\nThus, with ei = (δki)dℓ+1\nk=1\n∂L\n∂b(ℓ)\ni\n=\n\u0010∂¯x(ℓ+1)\n∂b(ℓ)\ni\n\u0011⊤\n∂L\n∂¯x(ℓ+1) = e⊤\ni α(ℓ+1) = α(ℓ+1)\ni\nfor ℓ∈{0, . . . , L}\nand similarly\n∂L\n∂W (0)\nij\n=\n\u0010 ∂¯x(1)\n∂W (0)\nij\n\u0011⊤\nα(1) = ¯x(0)\nj e⊤\ni α(1) = ¯x(0)\nj α(1)\ni\nand\n∂L\n∂W (ℓ)\nij\n= σ(¯x(ℓ)\nj )α(ℓ+1)\ni\nfor ℓ∈{1, . . . , L}.\nThis concludes the proof.\nLemma 10.21 and Proposition 10.22 motivate Algorithm 1, in which a forward pass computing\n¯x(ℓ), ℓ= 1, . . . , L + 1, is followed by a backward pass to determine the α(ℓ), ℓ= L + 1, . . . , 1,\nand the gradients of L with respect to the neural network parameters. This shows how to use\ngradient-based optimizers from the previous sections for the training of neural networks.\nTwo important remarks are in order. First, the objective function associated to neural networks\nis typically not convex as a function of the neural network weights and biases. Thus, the analysis\nof the previous sections will in general not be directly applicable. It may still give some insight\nabout the convergence behavior locally around the minimizer however. Second, to derive the back-\npropagation algorithm we assumed the activation function to be continuously differentiable, which\ndoes not hold for ReLU. Using the concept of subgradients, gradient-based algorithms and their\nanalysis may be generalized to some extent to also accommodate non-differentiable loss functions,\nsee Exercises 10.31–10.33.\n10.4\nAcceleration\nAcceleration is an important tool for the training of neural networks [221]. The idea was first\nintroduced by Polyak in 1964 under the name “heavy ball method” [180]. It is inspired by the\ndynamics of a heavy ball rolling down the valley of the loss landscape. Since then other types of\nacceleration have been proposed and analyzed, with Nesterov acceleration being the most prominent\nexample [160]. In this section, we first give some intuition by discussing the heavy ball method for\na simple quadratic loss. Afterwards we turn to Nesterov acceleration and give a convergence proof\nfor L-smooth and µ-strongly convex objective functions that improves upon the bounds obtained\nfor gradient descent.\n10.4.1\nHeavy ball method\nWe proceed similar as in [70, 181, 183] to motivate the idea. Consider the quadratic objective\nfunction in two dimensions\nf(w) := 1\n2w⊤Dw\nwhere\nD =\n\u0012λ1\n0\n0\nλ2\n\u0013\n(10.4.1)\n132\nAlgorithm 1 Backpropagation\nInput:\nNetwork\ninput\nx,\ntarget\noutput\ny,\nneural\nnetwork\nparameters\n((W (0), b(0)), . . . , (W (L), b(L)))\nOutput: Gradients of the loss function L with respect to neural network parameters\nForward pass\n¯x(1) ←W (0)x + b(0)\nfor ℓ= 1, . . . , L do\n¯x(ℓ+1) ←W (ℓ)σ(¯x(ℓ)) + b(ℓ)\nend for\nBackward pass\nα(L+1) ←∇¯x(L+1)L(¯x(L+1), y)\nfor ℓ= L, . . . , 1 do\n∇b(ℓ)L ←α(ℓ+1)\n∇W (ℓ)L ←α(ℓ+1)σ(¯x(ℓ))⊤\nα(ℓ) ←σ′(¯x(ℓ)) ⊙(W (ℓ))⊤α(ℓ+1)\nend for\n∇b(0)L ←α(1)\n∇W (0)L ←α(1)x⊤\nwith λ1 ≥λ2 > 0. Clearly, f has a unique minimizer at w∗= 0 ∈R2. Starting at some w0 ∈R2,\ngradient descent with constant step size h > 0 computes the iterates\nwk+1 = wk −hDwk =\n\u00121 −hλ1\n0\n0\n1 −hλ2\n\u0013\nwk =\n\u0012(1 −hλ1)k+1\n0\n0\n(1 −hλ2)k+1\n\u0013\nw0.\nThe method converges for arbitrary initialization w0 if and only if\n|1 −hλ1| < 1\nand\n|1 −hλ2| < 1.\nThe optimal step size balancing the speed of convergence in both coordinates is\nh∗= argminh>0 max{|1 −hλ1|, |1 −hλ2|} =\n2\nλ1 + λ2\n.\n(10.4.2)\nWith κ = λ1/λ2 we then obtain the convergence rate\n|1 −h∗λ1| = |1 −h∗λ2| = λ1 −λ2\nλ1 + λ2\n= κ −1\nκ + 1 ∈[0, 1).\n(10.4.3)\nIf λ1 ≫λ2, this term is close to 1, and thus the convergence will be slow. This is consistent with\nour analysis for strongly convex objective functions; by Exercise 10.34 the condition number of f\nequals κ = λ1/λ2 ≫1. Hence, the upper bounds in Theorem 10.14 and Remark 10.15 converge\nonly slowly. Similar considerations hold for general quadratic objective functions in Rn such as\n˜f(w) = 1\n2w⊤Aw + b⊤w + c\n(10.4.4)\nwith A ∈Rn×n symmetric positive definite, b ∈Rn and c ∈R, see Exercise 10.35.\n133\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGradient Descent\nHeavy Ball\nFigure 10.3: 20 steps of gradient descent and the heavy ball method on the objective function\n(10.4.1) with λ1 = 12 ≫1 = λ2, step size h = α = h∗as in (10.4.2), and β = 1/3.\nRemark 10.23. Interpreting (10.4.4) as a second-order Taylor expansion of some objective function\n˜f around its minimizer w∗, we note that the described effects also occur for general objective\nfunctions with ill-conditioned Hessians at the minimizer.\nFigure 10.3 gives further insight into the poor performance of gradient descent for (10.4.1) with\nλ1 ≫λ2. The loss-landscape looks like a ravine (the derivative is much larger in one direction than\nthe other), and away from the floor, ∇f mainly points to the opposite side. Therefore the iterates\noscillate back and forth in the first coordinate, and make little progress in the direction of the valley\nalong the second coordinate axis. To address this problem, the heavy ball method introduces a\n“momentum” term which can mitigate this effect to some extent. The idea is, to choose the update\nnot just according to the gradient at the current location, but to add information from the previous\nsteps. After initializing w0 and, e.g., w1 = w0 −α∇f(w0), let for k ∈N\nwk+1 = wk −α∇f(wk) + β(wk −wk−1).\n(10.4.5)\nThis is known as Polyak’s heavy ball method [180]. Here α > 0 and β ∈(0, 1) are hyperparameters\n(that could also depend on k) and in practice need to be carefully tuned to balance the strength of\nthe gradient and the momentum term. Iteratively expanding (10.4.5) with the given initialization,\nobserve that for k ≥0\nwk+1 = wk −α\n \nk\nX\nj=0\nβj∇f(wk−j)\n!\n.\n(10.4.6)\nThus, wk is updated using an exponentially weighted average of all past gradients. Choosing the\nmomentum parameter β in the interval (0, 1) ensures that the influence of previous gradients on the\nupdate decays exponentially. The concrete value of β determines the balance between the impact\nof recent and past gradients.\nIntuitively, this (exponentially weighted) linear combination of the past gradients averages out\nsome of the oscillation observed for gradient descent in Figure 10.3 in the first coordinate, and thus\n“smoothes” the path. The partial derivative in the second coordinate, along which the objective\n134\nfunction is very flat, does not change much from one iterate to the next. Thus, its proportion in\nthe update is strengthened through the addition of momentum. This is observed in Figure 10.3.\nAs mentioned earlier, the heavy ball method can be interpreted as a discretization of the dy-\nnamics of a ball rolling down the valley of the loss landscape. If the ball has positive mass, i.e. is\n“heavy”, its momentum prevents the ball from bouncing back and forth too strongly. The following\nremark further elucidates this connection.\nRemark 10.24. As pointed out, e.g., in [181, 183], for suitable choices of α and β, (10.4.5) can be\ninterpreted as a discretization of the second-order ODE\nmw′′(t) = −∇f(w(t)) −rw′(t).\n(10.4.7)\nThis equation describes the movement of a point mass m under influence of the force field −∇f(w(t));\nthe term −w′(t), which points in the negative direction of the current velocity, corresponds to fric-\ntion, and r > 0 is the friction coefficient. The discretization\nmwk+1 −2wk + wk−1\nh2\n= −∇f(wk) −wk+1 −wk\nh\nthen leads to\nwk+1 = wk −\nh2\nm −rh\n| {z }\n=α\n∇f(wk) +\nm\nm −rh\n| {z }\n=β\n(wk −wk−1),\n(10.4.8)\nand thus to (10.4.5), [183].\nLetting m = 0 in (10.4.8), we recover the gradient descent update (10.1.2). Hence, the positive\nmass corresponds to the momentum term. Similarly, letting m = 0 in the continuous dynamics\n(10.4.7), we obtain the gradient flow (10.1.3). The key difference between these equations is that\n−∇f(w(t)) represents the velocity of w(t) in (10.1.3), whereas in (10.4.7), up to the friction term,\nit corresponds to an acceleration.\nLet us sketch an argument to show that (10.4.5) improves the convergence over plain gradient\ndescent for the objective function (10.4.1). Denoting wk = (wk,1, wk,2)⊤∈R2, we obtain from\n(10.4.5) and the definition of f in (10.4.1)\n\u0012wk+1,j\nwk,j\n\u0013\n=\n\u00121 + β −αλj\n−β\n1\n0\n\u0013 \u0012 wk,j\nwk−1,j\n\u0013\n(10.4.9)\nfor j ∈{1, 2} and k ≥1. The smaller the modulus of the eigenvalues of the matrix in (10.4.9),\nthe faster the convergence towards the minimizer w∗,j = 0 ∈R for arbitrary initialization. Hence,\nthe goal is to choose α > 0 and β ∈(0, 1) such that the maximal modulus of the eigenvalues\nof the matrix for j ∈{1, 2} is possibly small. We omit the details of this calculation (also see\n[181, 165, 70]), but mention that this is obtained for\nα =\n\u0010\n2\n√λ1 + √λ2\n\u00112\nand\nβ =\n\u0010√λ1 −√λ2\n√λ1 + √λ2\n\u00112\n.\nWith these choices, the modulus of the maximal eigenvalue is bounded by\np\nβ =\n√κ −1\n√κ + 1 ∈[0, 1),\n135\nwhere again κ = λ1/λ2. Due to (10.4.9), this expression gives a rate of convergence for (10.4.5).\nContrary to gradient descent, see (10.4.3), for this problem the heavy ball method achieves a\nconvergence rate that only depends on the square root of the condition number κ. This explains\nthe improved performance observed in Figure 10.3.\n10.4.2\nNesterov acceleration\nNesterov’s accelerated gradient method (NAG) [160, 159], is a refinement of the heavy ball method.\nAfter initializing v0, w0 ∈Rn, the update is formulated as the two-step process\nvk+1 = wk −α∇f(wk)\n(10.4.10a)\nwk+1 = vk+1 + β(vk+1 −vk),\n(10.4.10b)\nwhere again α > 0 and β ∈(0, 1) are hyperparameters. Substituting the second line into the first\nwe get\nvk+1 = vk −α∇f(wk) + β(vk −vk−1).\nComparing with the heavy ball method (10.4.5), the key difference is that the gradient is not\nevaluated at the current position vk, but instead at the point wk = vk + β(vk −vk−1), which can\nbe interpreted as an estimate of the position at the next iteration.\nWe next discuss the convergence for L-smooth and µ-strongly convex objective functions f. It\nturns out, that these conditions are not sufficient in order for the heavy ball method (10.4.5) to\nconverge, and one can construct counterexamples [133]. This is in contrast to NAG, as the next\ntheorem shows. To give the analysis, it is convenient to first rewrite (10.4.10) as a three sequence\nupdate: Let τ =\np\nµ/L, α = 1/L, and β = (1−τ)/(1+τ). After initializing w0, v0 ∈Rn, (10.4.10)\ncan also be written as u0 = ((1 + τ)w0 −v0)/τ and for k ∈N0\nwk =\nτ\n1 + τ uk +\n1\n1 + τ vk\n(10.4.11a)\nvk+1 = wk −1\nL∇f(wk)\n(10.4.11b)\nuk+1 = uk + τ · (wk −uk) −τ\nµ∇f(wk),\n(10.4.11c)\nsee Exercise 10.36.\nThe proof of the following theorem proceeds along the lines of [231, 241].\nTheorem 10.25. Let n ∈N and L, µ > 0. Let f : Rn →R be L-smooth and µ-strongly convex.\nFurther, let v0, w0 ∈Rn and let τ =\np\nµ/L. Let (wk, vk+1, uk+1)∞\nk=0 ⊆Rn be defined by (10.4.11a),\nand let w∗be the unique minimizer of f.\nThen, for all k ∈N0, it holds that\n∥uk −w∗∥2 ≤2\nµ\n\u0010\n1 −\nrµ\nL\n\u0011k\u0010\nf(v0) −f(w∗) + µ\n2 ∥u0 −w∗∥2\u0011\n,\n(10.4.12a)\nf(vk) −f(w∗) ≤\n\u0010\n1 −\nrµ\nL\n\u0011k\u0010\nf(v0) −f(w∗) + µ\n2 ∥u0 −w∗∥2\u0011\n.\n(10.4.12b)\n136\nProof. Define\nek := f(vk) −f(w∗) + µ\n2 ∥uk −w∗∥2.\n(10.4.13)\nTo show (10.4.12), it suffices to prove with c = 1 −τ that ek+1 ≤cek for all k ∈N0.\nWe start with the last term in (10.4.13). By (10.4.11c)\nµ\n2 ∥uk+1 −w∗∥2 −µ\n2 ∥uk −w∗∥2\n= µ\n2 ∥uk+1 −uk + uk −w∗∥2 −µ\n2 ∥uk −w∗∥2\n= µ\n2 ∥uk+1 −uk∥2 + µ\n2 ·\n \n2\n\u001c\nτ · (wk −uk) −τ\nµ∇f(wk), uk −w∗\n\u001d !\n= µ\n2 ∥uk+1 −uk∥2 + τ ⟨∇f(wk), w∗−uk⟩−τµ ⟨wk −uk, w∗−uk⟩.\n(10.4.14)\nFrom (10.4.11a) we have τuk = (1 + τ)wk −vk so that\nτ · (wk −uk) = τwk −(1 + τ)wk + vk = vk −wk\n(10.4.15)\nand using µ-strong convexity (10.1.15), we get\nτ ⟨∇f(wk), w∗−uk⟩= τ ⟨∇f(wk), wk −uk⟩+ τ ⟨∇f(wk), w∗−wk⟩\n≤⟨∇f(wk), vk −wk⟩−τ · (f(wk) −f(w∗)) −τµ\n2 ∥wk −w∗∥2.\nMoreover,\n−τµ\n2 ∥wk −w∗∥2 −τµ ⟨wk −uk, w∗−uk⟩\n= −τµ\n2\n\u0010\n∥wk −w∗∥2 −2 ⟨wk −uk, wk −w∗⟩+ 2 ⟨wk −uk, wk −uk⟩\n\u0011\n= −τµ\n2 (∥uk −w∗∥2 + ∥wk −uk∥2).\nThus, (10.4.14) is bounded by\nµ\n2 ∥uk+1 −uk∥2 + ⟨∇f(wk), vk −wk⟩−τ · (f(wk) −f(w∗))\n−τµ\n2 ∥uk −w∗∥2 −τµ\n2 ∥wk −uk∥2\nwhich gives with c = 1 −τ\nµ\n2 ∥uk+1 −w∗∥2 ≤cµ\n2 ∥uk −w∗∥2 + µ\n2 ∥uk+1 −uk∥2\n+ ⟨∇f(wk), vk −wk⟩−τ · (f(wk) −f(w∗)) −τµ\n2 ∥wk −uk∥2.\n(10.4.16)\nTo bound the first term in (10.4.13), we use L-smoothness (10.1.4) and (10.4.11b)\nf(vk+1) −f(wk) ≤⟨∇f(wk), vk+1 −wk⟩+ L\n2 ∥vk+1 −wk∥2 = −1\n2L∥∇f(wk)∥2,\n137\nso that\nf(vk+1) −f(w∗) −τ · (f(wk) −f(w∗)) ≤(1 −τ)(f(wk) −f(w∗)) −1\n2L∥∇f(wk)∥2\n= c · (f(vk) −f(w∗)) + c · (f(wk) −f(vk)) −1\n2L∥∇f(wk)∥2.\n(10.4.17)\nNow, (10.4.16) and (10.4.17) imply\nek+1 ≤cek + c · (f(wk) −f(vk)) −1\n2L∥∇f(wk)∥2 + µ\n2 ∥uk+1 −uk∥2\n+ ⟨∇f(wk), vk −wk⟩−τµ\n2 ∥wk −uk∥2.\nSince we wish to bound ek+1 by cek, we now show that all terms except cek on the right-hand side\nof the inequality above sum up to a non-positive value. By (10.4.11c) and (10.4.15)\nµ\n2 ∥uk+1 −uk∥2 = µ\n2 ∥vk −wk∥2 −τ ⟨∇f(wk), vk −wk⟩+ τ 2\n2µ∥∇f(wk)∥2.\nMoreover, using µ-strong convexity\n⟨∇f(wk), vk −wk⟩\n≤τ ⟨∇f(wk), vk −wk⟩+ (1 −τ)\n\u0010\nf(vk) −f(wk) −µ\n2 ∥vk −wk∥2\u0011\n.\nThus, we arrive at\nek+1 ≤cek + c · (f(wk) −f(vk)) −1\n2L∥∇f(wk)∥2 + µ\n2 ∥vk −wk∥2\n−τ ⟨∇f(wk), vk −wk⟩+ τ 2\n2µ∥∇f(wk)∥2 + τ ⟨∇f(wk), vk −wk⟩\n+ c · (f(vk) −f(wk)) −cµ\n2 ∥vk −wk∥2 −τµ\n2 ∥wk −uk∥2\n= cek +\n\u0010 τ 2\n2µ −1\n2L\n\u0011\n∥∇f(wk)∥2 + µ\n2\n\u0010\nτ −1\nτ\n\u0011\n∥wk −vk∥2\n≤cek\nwhere we used once more (10.4.15), and the fact that τ 2/(2µ) −1/(2L) = 0 and τ −1/τ ≤0 since\nτ =\np\nµ/L ∈(0, 1].\nComparing the result for gradient descent (10.1.16) with NAG (10.4.12), the improvement lies\nin the convergence rate, which is 1−κ−1 for gradient descent (also see Remark 10.15), and 1−κ−1/2\nfor NAG, where κ = L/µ. In contrast to gradient descent, for NAG the convergence depends only\non the square root of the condition number κ. For ill-conditioned problems where κ is large, we\ntherefore expect much better performance for accelerated methods.\nFinally, we mention that NAG also achieves faster convergence in the case of L-smooth and\nconvex objective functions. While the error decays like O(k−1) for gradient descent, see Theorem\n10.11, for NAG one obtains convergence O(k−2), see [160, 158, 241].\n138\n10.5\nOther methods\nIn recent years, a multitude of first order (gradient descent) methods has been proposed and studied\nfor the training of neural networks. They typically employ (a subset) of the three critical strategies:\nmini-batches, acceleration, and adaptive step sizes. The concept of mini-batches and acceleration\nhave been covered in the previous sections, and we will touch upon adaptive learning rates in the\npresent one. Specifically, we present three algorithms—AdaGrad, RMSProp, and Adam—which\nhave been among the most influential in the field, and serve to explore the main ideas. An intuitive\noverview of first order methods can also be found in [194], which discusses additional variants that\nare omitted here. Moreover, in practice, various other techniques and heuristics such as batch\nnormalization, gradient clipping, data augmentation, regularization and dropout, early stopping,\nspecific weight initializations etc. are used. We do not discuss them here, and refer to [22] or to\n[67, Chapter 11] for a practitioners guide.\nAfter initializing m0 = 0 ∈Rn, v0 = 0 ∈Rn, and w0 ∈Rn, all methods discussed below are\nspecial cases of the update\nmk+1 = β1mk + β2∇f(wk)\n(10.5.1a)\nvk+1 = γ1vk + γ2∇f(wk) ⊙∇f(wk)\n(10.5.1b)\nwk+1 = wk −αkmk+1 ⊘\np\nvk+1 + ε\n(10.5.1c)\nfor k ∈N0, and certain hyperparameters αk, β1, β2, γ1, γ2, and ε. Here ⊙and ⊘denote the\ncomponentwise multiplication and division, respectively, and √vk+1 + ε is understood as the vector\n(√vk+1,i + ε)i. We will give some default values for those hyperparameters in the following, but\nmention that careful problem dependent tuning can enhance performance.\nEquation (10.5.1a)\ncorresponds to heavy ball momentum if β1 > 0. If β1 = 0, then mk+1 is simply a multiple of\nthe current gradient.\nEquation (10.5.1b) defines a weight vector vk+1 that is used to set the\ncomponentwise learning rate in the update of the parameter in (10.5.1c). These type of methods\nare often applied using mini-batches, see Section 10.2. For simplicity we present them with the full\ngradients.\n10.5.1\nAdaGrad\nIn Section 10.2 we argued, that for stochastic methods the learning rate should decrease in order\nto get convergence. The choice of how to decrease the learning rate can have significant impact\nin practice. AdaGrad [57], which stands for adaptive gradient algorithm, provides a method to\ndynamically adjust learning rates during optimization. Moreover, it does so by using individual\nlearning rates for each component.\nAdaGrad correspond to (10.5.1) with\nβ1 = 0,\nγ1 = β2 = γ2 = 1,\nαk = α\nfor all k ∈N0.\nThis leaves the hyperparameters ε > 0 and α > 0. The constant ε > 0 is chosen small but positive\nto avoid division by zero in (10.5.1c). Possible default values are α = 0.01 and ε = 10−8. The\nAdaGrad update then reads\nvk+1 = vk + ∇f(wk) ⊙∇f(wk)\nwk+1 = wk −α∇f(wk) ⊘\np\nvk+1 + ε.\n139\nDue to\nvk+1 =\nk\nX\nj=0\n∇f(wj) ⊙∇f(wj),\n(10.5.2)\nthe algorithm scales the gradient ∇f(wk) in the update component-wise by the inverse square root\nof the sum over all past squared gradients plus ε. Note that the scaling factor (vk+1,i + ε)−1/2 for\ncomponent i will be large, if the previous gradients for that component were small, and vice versa.\nIn the words of the authors of [57]: “our procedures give frequently occurring features very low\nlearning rates and infrequent features high learning rates.”\nRemark 10.26. A benefit of the componentwise scaling can be observed for the ill-conditioned\nobjective function in (10.4.1). Since in this case ∇f(wj) = (λ1wj,1, λ2wj,2)⊤for each j = 1, . . . , k,\nsetting ε = 0 AdaGrad performs the update\nwk+1 = wk −α\n \nwk,1(Pk\nj=0 w2\nj,1)−1/2\nwk,2(Pk\nj=0 w2\nj,1)−1/2\n!\n.\nNote how the λ1 and λ2 factors in the update have vanished due to the division by √vk+1. This\nmakes the method invariant to a componentwise rescaling of the gradient, and results in a more\ndirect path towards the minimizer.\n10.5.2\nRMSProp\nThe sum of past squared gradients can increase rapidly, leading to a significant reduction in learning\nrates when training neural networks with AdaGrad. This often results in slow convergence, see\nfor example [242]. RMSProp [90] seeks to rectify this by adjusting the learning rates using an\nexponentially weighted average of past gradients.\nRMSProp corresponds to (10.5.1) with\nβ1 = 0,\nβ2 = 1,\nγ2 = 1 −γ1 ∈(0, 1),\nαk = α\nfor all k ∈N0,\neffectively leaving the hyperparameters ε > 0, γ1 ∈(0, 1) and α > 0. Typically, recommended\ndefault values are ε = 10−8, α = 0.01 and γ1 = 0.9. The algorithm is given through\nvk+1 = γ1vk + (1 −γ1)∇f(wk) ⊙∇f(wk)\n(10.5.3a)\nwk+1 = wk −α∇f(wk) ⊘\np\nvk+1 + ε.\n(10.5.3b)\nNote that\nvk+1 = (1 −γ1)\nk\nX\nj=0\nγj\n1∇f(wk−j) ⊙∇f(wk−j),\nso that, contrary to AdaGrad (10.5.2), the influence of gradient ∇f(wk−j) on the weight vk+1\ndecays exponentially in j.\n140\n10.5.3\nAdam\nAdam [116], short for adaptive moment estimation, combines adaptive learning rates based on\nexponentially weighted averages as in RMSProp, with heavy ball momentum. Contrary to AdaGrad\nan RMSProp it thus uses a value β1 > 0.\nMore precisely, Adam corresponds to (10.5.1) with\nβ2 = 1 −β1 ∈(0, 1),\nγ2 = 1 −γ1 ∈(0, 1),\nαk = α\nq\n1 −γk+1\n1\n1 −βk+1\n1\nfor all k ∈N0, for some α > 0. The default values for the remaining parameters recommended in\n[116] are ε = 10−8, α = 0.001, β1 = 0.9 and γ1 = 0.999. The update can be formulated as\nmk+1 = β1mk + (1 −β1)∇f(wk),\nˆmk+1 =\nmk+1\n1 −βk+1\n1\n(10.5.4a)\nvk+1 = γ1vk + (1 −γ1)∇f(wk) ⊙∇f(wk),\nˆvk+1 =\nvk+1\n1 −γk+1\n1\n(10.5.4b)\nwk+1 = wk −α ˆmk+1 ⊘\np\nˆvk+1 + ε.\n(10.5.4c)\nNote that mk+1 equals\nmk+1 = (1 −β1)\nk\nX\nj=0\nβj\n1∇f(wk−j)\nand thus correspond to heavy ball style momentum with momentum parameter β = β1, see (10.4.6).\nThe normalized version ˆmk+1 is introduced to account for the bias towards 0, stemming from the\ninitialization m0 = 0.\nThe weight-vector vk+1 in (10.5.4b) is analogous to the exponentially\nweighted average of RMSProp in (10.5.3a), and the normalization again serves to counter the bias\nfrom v0 = 0.\nIt should be noted that there exist examples of convex functions for which Adam does not\nconverge to a minimizer, see [190]. The authors of [190] propose a modification termed AMSGrad,\nwhich avoids this issue and their analysis also applies to RMSProp. Nonetheless, Adam remains a\nhighly popular and successful algorithm for the training of neural networks. We also mention that\nthe proof of convergence in the stochastic setting requires k-dependent decreasing learning rates\nsuch as α = O(k−1/2) in (10.5.3b) and (10.5.4c).\nBibliography and further reading\nSection 10.1 on gradient descent is based on standard textbooks such as [20, 25, 163] and especially\n[159]. These are also good references for further reading on convex optimization. In particular\nTheorem 10.11 and the Lemmas leading up to it closely follow Nesterov’s arguments in [159]. Con-\nvergence proofs under the PL inequality can be found in [114]. Stochastic gradient descent discussed\nin Section 10.2 originally dates back to Robbins and Monro [191]. The first non-asymptotic con-\nvergence analysis for strongly convex objective functions was given in [154]. The proof presented\nhere is similar to [76] and in particular uses their choice of step size. A good overview of proofs\nfor (stochastic) gradient descent algorithms together with detailed references can be found in [65],\n141\nand for a textbook specifically on stochastic optimization also see [126]. The backpropagation al-\ngorithm discussed in Section 10.3 was popularized by Rumelhart, Hinton and Williams [197]; for\nfurther details on the historical developement we refer to [202, Section 5.5], and for a more in-depth\ndiscussion of the algorithm, see for instance [84]. The heavy ball method in Section 10.4 goes back\nto Polyak [180]. To motivate the algorithm we proceed similar as in [70, 181, 183]. For the analysis\nof Nesterov acceleration [160], we follow the Lyapunov type proofs given in [231, 241]. Finally,\nfor Section 10.5 on other algorithms, we refer to the original works that introduced AdaGrad [57],\nRMSProp [90] and Adam [116]. A good overview of gradient descent methods popular for deep\nlearning can be found in [194]. Regarding the analysis of RMSProp and Adam, we refer to [190]\nwhich gave an example of a convex function for which Adam does not converge, and provide a prov-\nably convergent modification of the algorithm. Convergence proofs (for variations of) AdaGrad and\nAdam can also be found in [49].\nFor a general discussion and analysis of optimization algorithms in machine learning see [23].\nDetails on implementations in Python can for example be found in [67], and for recommendations\nand tricks regarding the implementation we also refer to [22, 129].\n142\nExercises\nExercise 10.27. Let f ∈C1(Rn). Show that f is convex in the sense of Definition 10.8 if and only\nif\nf(w) + ⟨∇f(w), v −w⟩≤f(v)\nfor all w, v ∈Rn.\nExercise 10.28. Find a function f : R →R that is L-smooth, satisfies the PL-inequality (10.1.19)\nfor some µ > 0, has a unique minimizer w∗∈R, but is not convex and thus also not strongly\nconvex.\nExercise 10.29. Prove Theorem 10.17, i.e. show that L-smoothness and the PL-inequality (10.1.19)\nyield linear convergence of f(wk) →f(w∗) as k →∞.\nDefinition 10.30. For convex f : Rn →R, g ∈Rn is called a subgradient (or subdifferential) of\nf at v if and only if\nf(w) ≥f(v) + ⟨g, w −v⟩\nfor all w ∈Rn.\n(10.5.5)\nThe set of all subgradients of f at v is denoted by ∂f(v).\nA subgradient always exists, i.e. ∂f(v) is necessarily nonempty. This statement is also known\nunder the name “Hyperplane separation theorem”. Subgradients generalize the notion of gradients\nfor convex functions, since for any convex continuously differentiable f, (10.5.5) is satisfied with\ng = ∇f(v).\nExercise 10.31. Let f : Rn →R be convex and Lip(f) ≤L. Show that for any g ∈∂f(v) holds\n∥g∥≤L.\nExercise 10.32. Let f : Rn →R be convex, Lip(f) ≤L and suppose that w∗is a minimizer of f.\nFix w0 ∈Rd, and for k ∈N0 define the subgradient descent update\nwk+1 := wk −hkgk,\nwhere gk is an arbitrary fixed element of ∂f(wk). Show that\nmin\ni≤k f(wi) −f(w∗) ≤\n∥w0 −w∗∥2 + L2\nkP\ni=1\nh2\ni\n2\nkP\ni=1\nhi\n.\nHint: Start by recursively expanding ∥wk −w∗∥2 = · · · , and then apply the property of the\nsubgradient.\nExercise 10.33. Consider the setting of Exercise 10.32. Determine step sizes h1, . . . , hk (which\nmay depend on k, i.e. hk,1, . . . , hk,k) such that for any arbitrarily small δ > 0\nmin\ni≤k f(wi) −f(w∗) = O(k−1/2+δ)\nas k →∞.\n143\nExercise 10.34. Let A ∈Rn×n be symmetric positive semidefinite, b ∈Rn and c ∈R. Denote\nthe eigenvalues of A by λ1 ≥· · · ≥λn ≥0. Show that the objective function\nf(w) := 1\n2w⊤Aw + b⊤w + c\n(10.5.6)\nis convex and λ1-smooth. Moreover, if λn > 0, then f is λn-strongly convex. Show that these\nvalues are optimal in the sense that f is neither L-smooth nor µ-strongly convex if L < λ1 and\nµ > λn.\nHint: Note that L-smoothness and µ-strong convexity are invariant under shifts and the addition\nof constants. That is, for every α ∈R and β ∈Rn, ˜f(w) := α+f(w+β) is L-smooth or µ-strongly\nconvex if and only if f is. It thus suffices to consider w⊤Aw/2.\nExercise 10.35. Let f be as in Exercise 10.34. Show that gradient descent converges for arbitrary\ninitialization w0 ∈Rn, if and only if\nmax\nj=1,...,n |1 −hλj| < 1.\nShow that argminh>0 maxj=1,...,n |1 −hλj| = 2/(λ1 + λn) and conclude that the convergence will be\nslow if f is ill-conditioned, i.e. if λ1/λn ≫1.\nHint: Assume first that b = 0 ∈Rn and c = 0 ∈R in (10.5.6), and use the singular value\ndecomposition A = U ⊤diag(λ1, . . . , λn)U.\nExercise 10.36. Show that (10.4.10) can equivalently be written as (10.4.11) with τ =\np\nµ/L,\nα = 1/L, β = (1 −τ)/(1 + τ) and the initialization u0 = ((1 + τ)w0 −v0)/τ.\n144\nChapter 11\nWide neural networks and the neural\ntangent kernel\nIn this chapter we explore the dynamics of training neural networks of large width. Throughout\nwe focus on the situation where we have data pairs\n(xi, yi) ∈Rd × R\ni ∈{1, . . . , m},\n(11.0.1a)\nand wish to train a neural network Φ(x, w) depending on the input x ∈Rd and the parameters\nw ∈Rn, by minimizing the square loss objective defined as\nf(w) :=\nm\nX\ni=1\n(Φ(xi, w) −yi)2,\n(11.0.1b)\nwhich is a multiple of the empirical risk bRS(Φ) in (1.2.3) for the sample S = (xi, yi)m\ni=1 and the\nsquare-loss. We exclusively focus on gradient descent with a constant step size h, which yields a\nsequence of parameters (wk)k∈N. We aim to understand the evolution of Φ(x, wk) as k progresses.\nFor linear mappings w 7→Φ(x, w), the objective function (11.0.1b) is convex. As established in\nthe previous chapter, gradient descent then finds a global minimizer. For typical neural network\narchitectures, w 7→Φ(x, w) is not linear, and such a statement is in general not true.\nRecent research has shown that neural network behavior tends to linearize in the parameters\nas network width increases [106]. This allows to transfer some of the results and techniques from\nthe linear case to the training of neural networks. We start this chapter in Sections 11.1 and 11.2\nby recalling (kernel) least-squares methods, which describe linear (in w) models. Following [131],\nthe subsequent sections explore why in the infinite width limit neural networks exhibit linear-like\nbehavior. In Section 11.5.2 we formally introduce the linearization of w 7→Φ(x, w). Section 11.4\npresents an abstract result showing convergence of gradient descent, under the condition that Φ\ndoes not deviate too much from its linearization. In Sections 11.5 and 11.6, we then detail the\nimplications for wide neural networks for two (slightly) different architectures. In particular, we\nwill prove that gradient descent can find global minimizers when applied to (11.0.1b) for networks\nof very large width. We emphasize that this analysis treats the case of strong overparametrization,\nspecifically the case of increasing the network width while keeping the number of data points m\nfixed.\n145\n11.1\nLinear least-squares\nArguably one of the simplest machine learning algorithms is linear least-squares regression. Given\ndata (11.0.1a), linear regression tries to fit a linear function Φ(x, w) := x⊤w in terms of w by\nminimizing f(w) in (11.0.1b). With\nA =\n\n\n\nx⊤\n1...\nx⊤\nm\n\n\n∈Rm×d\nand\ny =\n\n\n\ny1\n...\nym\n\n\n∈Rm\n(11.1.1)\nit holds\nf(w) = ∥Aw −y∥2.\n(11.1.2)\nRemark 11.1. More generally, the ansatz Φ(x, (w, b)) := w⊤x + b corresponds to\nΦ(x, (w, b)) = (1, x⊤)\n\u0012 b\nw\n\u0013\n.\nTherefore, additionally allowing for a bias can be treated analogously.\nThe model Φ(x, w) = x⊤w is linear in both x and w. In particular, w 7→f(w) is a convex\nfunction by Exercise 10.34, and we may apply the convergence results of Chapter 10 when using\ngradient based algorithms. If A is invertible, then f has a unique minimizer given by w∗= A−1y. If\nrank(A) = d, then f is strongly convex by Exercise 10.34, and there still exists a unique minimizer.\nIf however rank(A) < d, then ker(A) ̸= {0} and there exist infinitely many minimizers of f. To\nensure uniqueness, we look for the minimum norm solution (or minimum 2-norm solution)\nw∗:= argmin{w∈Rd | f(w)≤f(v) ∀v∈Rd} ∥w∥.\n(11.1.3)\nThe following proposition establishes the uniqueness of w∗and demonstrates that it can be repre-\nsented as a superposition of the (xi)m\ni=1.\nProposition 11.2. Let A ∈Rm×d and y ∈Rm be as in (11.1.1). There exists a unique minimum\n2-norm solution of (11.1.2). Denoting ˜H := span{x1, . . . , xm} ⊆Rd, it is the unique element\nw∗= argmin ˜w∈˜H f( ˜w) ∈˜H.\n(11.1.4)\nProof. We start with existence and uniqueness. Let C ⊆Rm be the space spanned by the columns\nof A. Then C is closed and convex, and therefore y∗= argmin˜y∈C ∥y −˜y∥exists and is unique\n(this is a fundamental property of Hilbert spaces, see Theorem B.14). In particular, the set M =\n{w ∈Rd | Aw = y∗} ⊆Rd of minimizers of f is not empty. Clearly M is also closed and convex.\nBy the same argument as before, w∗= argminw∗∈M ∥w∗∥exists and is unique.\nIt remains to show (11.1.4). Denote by w∗the minimum norm solution and decompose w∗=\n˜w + ˆw with ˜w ∈˜H and ˆw ∈˜H⊥. We have Aw∗= A ˜w and ∥w∗∥2 = ∥˜w∥2 + ∥ˆw∥2. Since w∗\nis the minimal norm solution it must hold ˆw = 0. Thus w∗∈˜H. Finally assume there exists a\nminimizer v of f in ˜H different from w∗. Then 0 ̸= w∗−v ∈˜H, and since ˜H is spanned by the\nrows of A we have A(w∗−v) ̸= 0. Thus y∗= Aw∗̸= Av, which contradicts that v minimizes\nf.\n146\nThe condition of minimizing the 2-norm is a form of regularization.\nInterestingly, gradient\ndescent converges to the minimum norm solution for the quadratic objective (11.1.2), as long as w0\nis initialized within ˜H = span{x1, . . . , xm} (e.g. w0 = 0). Therefore, it does not find an “arbitrary”\nminimizer but implicitly regularizes the problem in this sense. In the following smax(A) denotes\nthe maximal singular value of A.\nTheorem 11.3. Let A ∈Rm×d be as in (11.1.1), let w0 = ˜w0 + ˆw0 where ˜w0 ∈˜H and ˆw0 ∈˜H⊥.\nFix h ∈(0, 1/(2smax(A)2)) and set\nwk+1 := wk −h∇f(wk)\nfor all k ∈N\n(11.1.5)\nwith f in (11.1.2). Then\nlim\nk→∞wk = w∗+ ˆw0.\nWe sketch the argument in case w0 ∈˜H, and leave the full proof to the reader, see Exercise\n11.32. Note that ˜H is the space spanned by the rows of A (or the columns of A⊤). The gradient\nof the objective function equals\n∇f(w) = 2A⊤(Aw −y).\nTherefore, if w0 ∈˜H, then the iterates of gradient descent never leave the subspace ˜H. By Exercise\n10.34 and Theorem 10.11, for small enough step size, it holds f(wk) →0. By Proposition 11.2\nthere only exists one minimizer in ˜H, corresponding to the minimum norm solution. Thus wk\nconverges to the minimal norm solution.\n11.2\nKernel least-squares\nLet again (xj, yj) ∈Rd × R, j = 1, . . . , m. In many applications linear models are too simplistic,\nand are not able to capture the true relation between x and y. Kernel methods allow to overcome\nthis problem by introducing nonlinearity in x, but retaining linearity in the parameter w.\nLet H be a Hilbert space with inner product ⟨·, ·⟩H, that is also referred to as the feature\nspace. For a (typically nonlinear) feature map ϕ : Rd →H, consider the model\nΦ(x, w) = ⟨ϕ(x), w⟩H\n(11.2.1)\nwith w ∈H. If H = Rn, the components of ϕ are referred to as features. With the objective\nfunction\nf(w) :=\nm\nX\nj=1\n\u0000⟨ϕ(xj), w⟩H −yj\n\u00012\nw ∈H,\n(11.2.2)\nwe wish to determine a minimizer of f. To ensure uniqueness and regularize the problem, we again\nconsider the minimum H-norm solution\nw∗:= argmin{w∈H | f(w)≤f(v) ∀v∈H} ∥w∥H.\nAs we will see below, w∗is well-defined. We will call Φ(x, w∗) = ⟨ϕ(x), w∗⟩H the kernel least\nsquares estimator. The nonlinearity of the feature map allows for more expressive models x 7→\nΦ(x, w) capable of capturing more complicated structures beyond linearity in the data.\n147\nRemark 11.4 (Gradient descent). Let H = Rn be equipped with the Euclidean inner product. Con-\nsider the sequence (wk)k∈N0 ⊆Rn generated by gradient descent to minimize (11.2.2). Assuming\nsufficiently small step size, by Theorem 11.3 for x ∈Rd\nlim\nk→∞Φ(x, wk) = ⟨ϕ(x), w∗⟩+ ⟨ϕ(x), ˆw0⟩.\n(11.2.3)\nHere, ˆw0 ∈Rn denotes the orthogonal projection of w0 ∈Rn onto ˜H⊥where ˜H := span{ϕ(x1), . . . , ϕ(xm)}.\nGradient descent thus yields the kernel least squares estimator plus ⟨ϕ(x), ˆw0⟩. Notably, on the\nset\n{x ∈Rd | ϕ(x) ∈span{ϕ(x1), . . . , ϕ(xm)}},\n(11.2.4)\n(11.2.3) thus coincides with the kernel least squares estimator independent of the initialization w0.\n11.2.1\nExamples\nTo motivate the concept of feature maps consider the following example from [155].\nExample 11.5. Let xi ∈R2 with associated labels yi ∈{−1, 1} for i = 1, . . . , m. The goal is to\nfind some model Φ(·, w) : R2 →R, for which\nsign(Φ(x, w))\n(11.2.5)\npredicts the label y of x. For a linear (in x) model\nΦ(x, (w, b)) = x⊤w + b,\nthe decision boundary of (11.2.5) equals {x ∈R2 | x⊤w + b = 0} in R2. Hence, by adjusting w\nand b, (11.2.5) can separate data by affine hyperplanes in R2. Consider two datasets represented\nby light blue squares for +1 and red circles for −1 labels:\nx1\nx2\nx1\nx2\ndataset 1\ndataset 2\nThe first dataset is separable by an affine hyperplane as depicted by the dashed line. Thus a linear\nmodel is capable of correctly classifying all datapoints. For the second dataset this is not possible.\nTo enhance model expressivity, introduce a feature map ϕ : R2 →R6 via\nϕ(x) = (1, x1, x2, x1x2, x2\n1, x2\n2)⊤∈R6\nfor all x ∈R2.\n(11.2.6)\nFor w ∈R6, this allows Φ(x) = w⊤ϕ(x) to represent arbitrary polynomials of degree 2. With this\nkernel approach, the decision boundary of (11.2.5) becomes the set of all hyperplanes in the feature\nspace passing through 0 ∈R6. Visualizing the last two features of the second dataset, we obtain\n148\nx2\n1\nx2\n2\nfeatures 5 and 6 of dataset 2\nNote how in the feature space R6, the datapoints are again separated by such a hyperplane. Thus,\nwith the feature map in (11.2.6), the predictor (11.2.5) can perfectly classify all points also for the\nsecond dataset.\nIn the above example we chose the feature space H = R6. It is also possible to work with\ninfinite dimensional feature spaces as the next example demonstrates.\nExample 11.6. Let H = ℓ2(N) be the space of square summable sequences and ϕ : Rd →ℓ2(N)\nsome map. Fitting the corresponding model\nΦ(x, w) = ⟨ϕ(x), w⟩ℓ2 =\nX\ni∈N\nϕi(x)wi\nto data (xi, yi)m\ni=1 requires to minimize\nf(w) =\nm\nX\nj=1\n \u0010 X\ni∈N\nϕi(xj)wi\n\u0011\n−yj\n!2\nw ∈ℓ2(N).\nHence we have to determine an infinite sequence of parameters (wi)i∈N.\n11.2.2\nKernel trick\nAt first glance, computing a (minimal H-norm) minimizer w in the possibly infinite-dimensional\nHilbert space H seems infeasible. The so-called kernel trick allows to do this computation. To\nexplain it, we first revisit the foundational representer theorem.\nTheorem 11.7 (Representer theorem). There is a unique minimum H-norm solution w∗∈H of\n(11.2.2). With ˜H := span{ϕ(x1), . . . , ϕ(xm)} it equals the unique element\nw∗= argmin ˜w∈˜H f( ˜w) ∈˜H.\n(11.2.7)\nProof. Let ˜w1, . . . , ˜wn be a basis of ˜H. If ˜H = {0} the statement is trivial, so we assume 1 ≤n ≤m.\nLet A = (⟨ϕ(xi), ˜wj⟩)ij ∈Rm×n. Every ˜w ∈˜H has a unique representation ˜w = Pn\nj=1 αj ˜wj for\nsome α ∈Rn. With this ansatz\nf( ˜w) =\nm\nX\ni=1\n(⟨ϕ(xi), ˜w⟩−yi)2 =\nm\nX\ni=1\n\u0010\nn\nX\nj=1\n⟨ϕ(xi), ˜wj⟩αj −yi\n\u00112\n= ∥Aα −y∥2.\n(11.2.8)\n149\nNote that A : Rn →Rm is injective since for every α ∈Rn\\{0} holds Pn\nj=1 αj ˜wj ∈˜H \\ {0} and\nhence Aα = (\nD\nϕ(xi), Pn\nj=1 αj ˜wj\nE\n)m\ni=1 ̸= 0. Therefore, there exists a unique minimizer α ∈Rn of\nthe right-hand side of (11.2.8), and thus there exists a unique minimizer w∗∈˜H in (11.2.7).\nFor arbitrary w ∈H we wish to show f(w) ≥f(w∗), so that w∗minimizes f in H. Decompose\nw = ˜w + ˆw with ˜w ∈˜H and ˆw ∈˜H⊥, i.e. ⟨ϕ(xj), ˆw⟩H = 0 for all j = 1, . . . , m. Then, using that\nw∗minimizes f in ˜H,\nf(w) =\nm\nX\nj=1\n(⟨ϕ(xj), w⟩H −yj)2 =\nm\nX\nj=1\n(⟨ϕ(xj), ˜w⟩H −yj)2 = f( ˜w) ≥f(w∗).\nFinally, let w ∈H be any minimizer of f in H different from w∗. It remains to show ∥w∥H >\n∥w∗∥H. Decompose again w = ˜w + ˆw with ˜w ∈˜H and ˆw ∈˜H⊥. As above f(w) = f( ˜w) and\nthus ˜w is a minimizer of f. Uniqueness of w∗in (11.2.7) implies ˜w = w∗. Therefore ˆw ̸= 0 and\n∥w∗∥2\nH < ∥˜w∥2\nH + ∥ˆw∥2\nH = ∥w∥2\nH.\nInstead of looking for the minimum norm minimizer w∗in the Hilbert space H, by Proposition\n11.2 it suffices to determine the unique minimizer in the at most m-dimensional subspace ˜H spanned\nby ϕ(x1), . . . , ϕ(xm). This significantly simplifies the problem. To do so we first introduce the\nnotion of kernels.\nDefinition 11.8. A symmetric function K : Rd×Rd →R is called a kernel if for any x1, . . . , xm ∈\nRd the kernel matrix G = (K(xi, xj))m\ni,j=1 ∈Rm×m is symmetric positive semidefinite.\nGiven a feature map ϕ : Rd →H, it is easy to check that\nK(x, x′) :=\n\nϕ(x), ϕ(x′)\n\u000b\nH\nfor all x, x′ ∈Rd,\ndefines a kernel. The corresponding kernel matrix G ∈Rm×m is given by\nGij = ⟨ϕ(xi), ϕ(xj)⟩H = K(xi, xj).\nWith the ansatz w = Pm\nj=1 αjϕ(xj), minimizing the objective (11.2.2) in ˜H is equivalent to mini-\nmizing\n∥Gα −y∥2,\n(11.2.9)\nin α = (α1, . . . , αm) ∈Rm.\nProposition 11.9. Let α ∈Rm be any minimizer of (11.2.9). Then w∗= Pm\nj=1 αjϕ(xj) is the\nunique minimum H-norm solution of (11.2.2).\nProposition 11.9, the proof of which is left as an exercise, suggests the following algorithm to\ncompute the kernel least squares estimator:\n150\n(i) compute the kernel matrix G = (K(xi, xj))m\ni,j=1,\n(ii) determine a minimizer α ∈Rm of ∥Gα −y∥,\n(iii) evaluate Φ(x, w∗) via\nΦ(x, w∗) =\n*\nϕ(x),\nm\nX\nj=1\nαjϕ(xj)\n+\nH\n=\nm\nX\nj=1\nαjK(x, xj).\n(11.2.10)\nThus, minimizing (11.2.2) and expressing the kernel least squares estimator does neither require\nexplicit knowledge of the feature map ϕ nor of the minimum norm solution w∗∈H. It is sufficient\nto choose a kernel map K : Rd × Rd →R; this is known as the kernel trick. Given a kernel K, we\nwill therefore also refer to (11.2.10) as the kernel least squares estimator without specifying H or\nϕ.\nExample 11.10. Common examples of kernels include the polynomial kernel\nK(x, x′) = (x⊤x′ + c)r\nc ≥0, r ∈N,\nthe radial basis function (RBF) kernel\nK(x, x′) = exp(−c∥x −x′∥2)\nc > 0,\nand the Laplace kernel\nK(x, x′) = exp(−c∥x −x′∥)\nc > 0.\nRemark 11.11. If Ω⊆Rd is compact and K : Ω× Ω→R is a continuous kernel, then Mercer’s\ntheorem implies existence of a Hilbert space H and a feature map ϕ : Rd →H such that\nK(x, x′) =\n\nϕ(x), ϕ(x′)\n\u000b\nH\nfor all x, x′ ∈Ω,\ni.e. K is the corresponding kernel. See for instance [217, Thm. 4.49].\n11.3\nTangent kernel\nConsider again a general model Φ(x, w) with input x ∈Rd and parameters w ∈Rn. The goal\nremains to minimize the square loss objective (11.0.1b) given the data (11.0.1a). If w 7→Φ(x, w)\nis not linear, then unlike in Sections 11.1 and 11.2, the objective function (11.0.1b) is in general\nnot convex, and most results on first order methods in Chapter 10 are not directly applicable.\nWe now simplify the situation by linearizing the model in w ∈Rn around the initialization:\nFixing w0 ∈Rn, let\nΦlin(x, w) := Φ(x, w0) + ∇wΦ(x, w0)⊤(w −w0)\nfor all w ∈Rn,\n(11.3.1)\nwhich is the first order Taylor approximation of Φ around the initial parameter w0. Introduce the\nnotation\nδi := Φ(xi, w0) −∇wΦ(xi, w0)⊤w0 −yi\nfor all i = 1, . . . , m.\n(11.3.2)\n151\nThe square loss for the linearized model then reads\nflin(w) :=\nm\nX\nj=1\n(Φlin(xi, w) −yi)2\n=\nm\nX\nj=1\n(⟨∇wΦ(xi, w0), w⟩+ δi)2,\n(11.3.3)\nwhere ⟨·, ·⟩stands for the Euclidean inner product in Rn. Comparing with (11.2.2), minimizing flin\ncorresponds to a kernel least squares regression with feature map\nϕ(x) = ∇wΦ(x, w0) ∈Rn.\nThe corresponding kernel is\nˆKn(x, x′) =\n\n∇wΦ(x, w0), ∇wΦ(x′, w0)\n\u000b\n.\n(11.3.4)\nWe refer to ˆKn as the empirical tangent kernel, as it arises from the first order Taylor approxima-\ntion (the tangent) of the original model Φ around initialization w0. Note that the kernel depends\non the choice of w0. As explained in Remark 11.4, training Φlin with gradient descent yields the\nkernel least-squares estimator with kernel ˆKn plus an additional term depending on w0.\nOf course the linearized model Φlin only captures the behaviour of Φ for parameters w that are\nclose to w0. If we assume for the moment that during training of Φ, the parameters remain close to\ninitialization, then we can expect similar behaviour and performance of Φ and Φlin. Under certain\nassumptions, we will see in the next sections that this is precisely what happens, when the width\nof a neural network increases. Before we make this precise, in Section 11.4 we investigate whether\ngradient descent applied to f(w) will find a global minimizer, under the assumption that Φlin is a\ngood approximation of Φ.\n11.4\nConvergence to global minimizers\nIntuitively, if w 7→Φ(x, w) is not linear but “close enough to its linearization” Φlin defined in\n(11.3.1), we expect that the objective function is close to a convex function and gradient descent\ncan still find global minimizers of (11.0.1b).\nTo motivate this, consider Figures 11.1 and 11.2\nwhere we chose the number of training data m = 1 and the number of parameters n = 1. As\nwe can see, essentially we require the difference of Φ and Φlin and of their derivatives to be small\nin a neighbourhood of w0. The size of the neighbourhood crucially depends on the initial error\nΦ(x1, w0) −y1, and on the size of the derivative\nd\ndwΦ(x1, w0).\nFor general m and n, we now make the required assumptions on Φ precise.\nAssumption 11.12. Let Φ ∈C1(Rd × Rn) and w0 ∈Rn. There exist constants r > 0, U, L < ∞\nand 0 < λmin ≤λmax < ∞such that\n(a) the kernel matrix of the empirical tangent kernel\n( ˆKn(xi, xj))m\ni,j=1 =\n\u0000⟨∇wΦ(xi, w0), ∇wΦ(xj, w0)⟩\n\u0001m\ni,j=1 ∈Rm×m\n(11.4.1)\nis regular and its eigenvalues belong to [λmin, λmax],\n152\nΦlin(x1, w)\nΦ(x1, w)\nw0\ny1\n(Φlin(x1, w) −y1)2\n(Φ(x1, w) −y1)2\nw0\nFigure 11.1: Graph of w 7→Φ(x1, w) and the linearization w 7→Φlin(x1, w) at the initial parameter\nw0, s.t.\nd\ndwΦ(x1, w0) ̸= 0. If Φ and Φlin are close, then there exists w s.t. Φ(x1, w) = y1 (left). If\nthe derivatives are also close, the loss (Φ(x1, w) −y1)2 is nearly convex in w, and gradient descent\nfinds a global minimizer (right).\nΦlin(x1, w)\nΦ(x1, w)\nw0\ny1\n(Φlin(x1, w) −y1)2\n(Φ(x1, w) −y1)2\nw0\nFigure 11.2: Same as Figure 11.1. If Φ and Φlin are not close, there need not exist w such that\nΦ(x1, w) = y1, and gradient descent need not converge to a global minimizer.\n(b) for all i ∈{1, . . . , m} holds\n∥∇wΦ(xi, w)∥≤U\nfor all w ∈Br(w0)\n∥∇wΦ(xi, w) −∇wΦ(xi, v)∥≤L∥w −v∥\nfor all w, v ∈Br(w0),\n(11.4.2)\n(c) and\nL ≤\nλ2\nmin\n12m3/2U2p\nf(w0)\nand\nr = 2√mU\nλmin\np\nf(w0).\n(11.4.3)\nThe regularity of the kernel matrix in Assumption 11.12 (a) is equivalent to (∇wΦ(xi, w0)⊤)m\ni=1 ∈\nRm×n having full rank m ≤n (in particular we have at least as many parameters n as training\ndata m). In the context of Figure 11.1, this means that\nd\ndwΦ(x1, w0) ̸= 0 and thus Φlin is a not a\nconstant function. This condition guarantees that there exists w such that Φlin(xi, w) = yi for all\ni = 1, . . . , m. In other words, already the linearized model Φlin is sufficiently expressive to interpo-\nlate the data. Assumption 11.12 (b) formalizes the closeness condition of Φ and Φlin. Apart from\ngiving an upper bound on ∇wΦ(xi, w), it assumes w 7→Φ(xi, w) to be L-smooth in a ball of radius\nr > 0 around w0, for all i = 1, . . . , m. This allows to control how far Φ(xi, w) and Φlin(xi, w) and\ntheir derivatives may deviate from each other for w in this ball. Finally Assumption 11.12 (c) ties\ntogether all constants, ensuring the full model to be sufficiently close to its linearization in a large\nenough neighbourhood of w0.\nWe are now ready to state the following theorem, which is a variant of [131, Thm. G.1]. In\nSection 11.5 we will see that its main requirement—Assumption 11.12—is satisfied with high prob-\nability for certain (wide) neural networks.\n153\nTheorem 11.13. Let Assumption 11.12 be satisfied and fix a positive learning rate\nh ≤\n1\nλmin + λmax\n.\n(11.4.4)\nSet for all k ∈N\nwk+1 = wk −h∇f(wk).\n(11.4.5)\nIt then holds for all k ∈N\n∥wk −w0∥≤2√mU\nλmin\np\nf(w0)\n(11.4.6a)\nf(wk) ≤(1 −hλmin)2kf(w0).\n(11.4.6b)\nProof. In the following denote the error in prediction by\nE(w) := (Φ(xi, w) −yi)m\ni=1 ∈Rm\nsuch that\n∇E(w) = (∇wΦ(xi, w))m\ni=1 ∈Rm×n\nand with the empirical tangent kernel ˆKn in Assumption 11.12\n∇E(w)∇E(w)⊤= ( ˆKn(xi, xj))m\ni,j=1 ∈Rm×m.\n(11.4.7)\nMoreover, (11.4.2) gives\n∥∇E(w)∥2 ≤∥∇E(w)∥2\nF =\nm\nX\ni=1\n∥∇Φ(xi, w)∥2 ≤mU2\nfor all w ∈Br(w0),\n(11.4.8a)\nand similarly\n∥∇E(w) −∇E(v)∥2 ≤\nm\nX\ni=1\n∥∇wΦ(xi, w) −∇wΦ(xi, v)∥2\n≤mL2∥w −v∥2\nfor all w, v ∈Br(w0).\n(11.4.8b)\nDenote c := 1 −hλmin ∈(0, 1). We use induction over k to prove\nk−1\nX\nj=0\n∥wj+1 −wj∥≤h2√mU∥E(w0)∥\nk−1\nX\nj=0\ncj,\n(11.4.9a)\n∥E(wk)∥2 ≤∥E(w0)∥2c2k,\n(11.4.9b)\nfor all k ∈N0 and where an empty sum is understood as zero. Since P∞\nj=0 cj = (1−c)−1 = (hλmin)−1\nand f(wk) = ∥E(wk)∥2, these inequalities directly imply (11.4.6).\nThe case k = 0 is trivial. For the induction step, assume (11.4.9) holds for some k ∈N0.\n154\nStep 1. We show (11.4.9a) for k + 1. The induction assumption and (11.4.3) give\n∥wk −w0∥≤2h√mU∥E(w0)∥\n∞\nX\nj=0\ncj = 2√mU\nλmin\np\nf(w0) = r,\n(11.4.10)\nand thus wk ∈Br(w0). Next\n∇f(wk) = ∇(E(wk)⊤E(wk)) = 2∇E(wk)⊤E(wk).\n(11.4.11)\nUsing the iteration rule (11.4.5), the bound (11.4.8a), and (11.4.9b)\n∥wk+1 −wk∥= 2h∥∇E(wk)⊤E(wk)∥\n≤2h√mU∥E(wk)∥\n≤2h√mU∥E(w0)∥ck.\nThis shows (11.4.9a) for k + 1. In particular, as in (11.4.10) we conclude\nwk+1, wk ∈Br(w0).\n(11.4.12)\nStep 2. We show (11.4.9b) for k + 1. Since E is continuously differentiable, there exists ˜wk in\nthe convex hull of wk and wk+1 such that\nE(wk+1) = E(wk) + ∇E( ˜wk)(wk+1 −wk) = E(wk) −h∇E( ˜wk)∇f(wk),\nand thus by (11.4.11)\nE(wk+1) = E(wk) −2h∇E( ˜wk)∇E(wk)⊤E(wk)\n=\n\u0000Im −2h∇E( ˜wk)∇E(wk)⊤\u0001\nE(wk),\nwhere Im ∈Rm×m is the identity matrix. We wish to show that\n∥Im −2h∇E( ˜wk)∇E(wk)⊤∥≤c,\n(11.4.13)\nwhich then implies (11.4.9b) for k + 1 and concludes the proof.\nUsing (11.4.8) and the fact that wk, ˜wk ∈Br(w0) by (11.4.12),\n∥∇E( ˜wk)∇E(wk)⊤−∇E(w0)∇E(w0)⊤∥\n≤∥∇E( ˜wk)∇E(wk)⊤−∇E(wk)∇E(wk)⊤∥\n+ ∥∇E( ˜wk)∇E(wk)⊤−∇E(wk)∇E(w0)⊤∥\n+ ∥∇E( ˜wk)∇E(w0)⊤−∇E(w0)∇E(w0)⊤∥\n≤3mULr.\nSince the eigenvalues of ∇E(w0)∇E(w0)⊤belong to [λmin, λmax] by (11.4.7) and Assumption 11.12\n(a), as long as h ≤(λmin + λmax)−1 we have\n∥Im −2h∇E( ˜wk)∇E(wk)⊤∥≤∥Im −2h∇E(w0)∇E(w0)⊤∥+ 6hmULr\n≤1 −2hλmin + 6hmULr\n≤1 −2h(λmin −3mULr)\n≤1 −hλmin = c,\nwhere we have used the equality for r and the upper bound for L in (11.4.3).\n155\nLet us emphasize the main statement of Theorem 11.13.\nBy (11.4.6b), full batch gradient\ndescent (11.4.5) achieves zero loss in the limit, i.e. the data is interpolated by the limiting model. In\nparticular, this yields convergence for the (possibly nonconvex) optimization problem of minimizing\nf(w).\n11.5\nTraining dynamics for LeCun initialization\nIn this and the next section we discuss the implications of Theorem 11.13 for wide neural networks.\nFor ease of presentation we focus on shallow networks with only one hidden layer, but stress that\nsimilar considerations also hold for deep networks, see the bibliography section.\n11.5.1\nArchitecture\nLet Φ : Rd →R be a neural network of depth one and width n ∈N of type\nΦ(x, w) = v⊤σ(Ux + b) + c.\n(11.5.1)\nHere x ∈Rd is the input, and U ∈Rn×d, v ∈Rn, b ∈Rn and c ∈R are the parameters which we\ncollect in the vector w = (U, b, v, c) ∈Rn(d+2)+1 (with U suitably reshaped). For future reference\nwe note that\n∇UΦ(x, w) = (v ⊙σ′(Ux + b))x⊤∈Rn×d\n∇bΦ(x, w) = v ⊙σ′(Ux + b) ∈Rn\n∇vΦ(x, w) = σ(Ux + b) ∈Rn\n∇cΦ(x, w) = 1 ∈R,\n(11.5.2)\nwhere ⊙denotes the Hadamard product. We also write ∇wΦ(x, w) ∈Rn(d+2)+1 to denote the full\ngradient with respect to all parameters.\nIn practice, it is common to initialize the weights randomly, and in this section we consider so-\ncalled LeCun initialization. The following condition on the distribution used for this initialization\nwill be assumed throughout the rest of Section 11.5.\nAssumption 11.14. The distribution W on R has expectation zero, variance one, and finite\nmoments up to order eight.\nTo explicitly indicate the expectation and variance in the notation, we also write W(0, 1) instead\nof W, and for µ ∈R and ς > 0 we use W(µ, ς2) to denote the corresponding scaled and shifted\nmeasure with expectation µ and variance ς2; thus, if X ∼W(0, 1) then µ + ςX ∼W(µ, ς2). LeCun\ninitialization [129] sets the variance of the weights in each layer to be reciprocal to the input\ndimension of the layer, thereby normalizing the output variance across all network nodes. The\ninitial parameters\nw0 = (U 0, b0, v0, c0)\nare thus randomly initialized with components\nU0;ij\niid\n∼W\n\u0010\n0, 1\nd\n\u0011\n,\nv0;i\niid\n∼W\n\u0010\n0, 1\nn\n\u0011\n,\nb0;i, c0 = 0,\n(11.5.3)\nindependently for all i = 1, . . . , n, j = 1, . . . , d. For a fixed ς > 0 one might choose variances ς2/d\nand ς2/n in (11.5.3), which would require only minor modifications in the rest of this section. Biases\n156\nare set to zero for simplicity, with nonzero initialization discussed in the exercises. All expectations\nand probabilities in Section 11.5 are understood with respect to this random initialization.\nExample 11.15. Typical examples for W(0, 1) are the standard normal distribution on R or the\nuniform distribution on [−\n√\n3,\n√\n3].\n11.5.2\nNeural tangent kernel\nWe begin our analysis by investigating the empirical tangent kernel\nˆKn(x, z) = ⟨∇wΦ(x, w0), ∇wΦ(z, w0)⟩\nof the shallow network (11.5.1). Scaled properly, it converges in the infinite width limit n →∞\ntowards a specific kernel known as the neural tangent kernel (NTK). Its precise formula depends\non the architecture and initialization. For the LeCun initialization (11.5.3) we denote it by KLC.\nTheorem 11.16. Let R < ∞such that |σ(x)| ≤R · (1 + |x|) and |σ′(x)| ≤R · (1 + |x|) for all\nx ∈R. For any x, z ∈Rd and ui\niid\n∼W(0, 1/d), i = 1, . . . , d, it then holds\nlim\nn→∞\n1\nn\nˆKn(x, z) = E[σ(u⊤x)σ(u⊤z)] =: KLC(x, z)\nalmost surely.\nMoreover, for every δ, ε > 0 there exists n0(δ, ε, R) ∈N such that for all n ≥n0 and all x,\nz ∈Rd with ∥x∥, ∥z∥≤R\nP\nh \r\r\r\r\n1\nn\nˆKn(x, z) −KLC(x, z)\n\r\r\r\r < ε\ni\n≥1 −δ.\nProof. Denote x(1) = U 0x + b0 ∈Rn and z(1) = U 0z + b0 ∈Rn. Due to the initialization (11.5.3)\nand our assumptions on W(0, 1), the components\nx(1)\ni\n=\nd\nX\nj=1\nU0;ijxj ∼u⊤x\ni = 1, . . . , n\nare i.i.d. with finite pth moment (independent of n) for all 1 ≤p ≤8. Due to the linear growth bound\non σ and σ′, the same holds for the (σ(x(1)\ni ))n\ni=1 and the (σ′(x(1)\ni ))n\ni=1. Similarly, the (σ(z(1)\ni\n))n\ni=1\nand (σ′(z(1)\ni\n))n\ni=1 are collections of i.i.d. random variables with finite pth moment for all 1 ≤p ≤8.\nDenote ˜vi = √nv0;i such that ˜vi\niid\n∼W(0, 1). By (11.5.2)\n1\nn\nˆKn(x, z) = (1 + x⊤z) 1\nn2\nn\nX\ni=1\n˜v2\ni σ′(x(1)\ni )σ′(z(1)\ni\n) + 1\nn\nn\nX\ni=1\nσ(x(1)\ni )σ(z(1)\ni\n) + 1\nn.\nSince\n1\nn\nn\nX\ni=1\n˜v2\ni σ′(x(1)\ni )σ′(z(1)\ni\n)\n(11.5.4)\n157\nis an average over i.i.d. random variables with finite variance, the law of large numbers implies\nalmost sure convergence of this expression towards\nE\n\u0002\n˜v2\ni σ′(x(1)\ni )σ′(z(1)\ni\n)\n\u0003\n= E[˜v2\ni ]E[σ′(x(1)\ni )σ′(z(1)\ni\n)]\n= E[σ′(u⊤x)σ′(u⊤z)],\nwhere we used that ˜v2\ni is independent of σ′(x(1)\ni )σ′(z(1)\ni\n). By the same argument\n1\nn\nn\nX\ni=1\nσ(x(1)\ni )σ(z(1)\ni\n) →E[σ(u⊤x)σ(u⊤z)]\nalmost surely as n →∞. This shows the first statement.\nThe existence of n0 follows similarly by an application of Theorem A.23.\nExample 11.17 (KLC for ReLU). Let σ(x) = max{0, x} and let W(0, 1) be the standard normal\ndistribution. For x, z ∈Rd denote by\nθ = arccos\n\u0012\nx⊤z\n∥x∥∥z∥\n\u0013\nthe angle between these vectors. Then according to [37, Appendix A], it holds with ui\niid\n∼W(0, 1),\ni = 1, . . . , d,\nKLC(x, z) = E[σ(u⊤x)σ(u⊤z)] = ∥x∥∥z∥\n2πd\n(sin(θ) + (π −θ) cos(θ)).\n11.5.3\nGradient descent\nWe now proceed similar as in [131, Appendix G], to show that Theorem 11.13 is applicable to the\nwide neural network (11.5.1) with high probability under random initialization (11.5.3). This will\nimply that gradient descent can find global minimizers when training wide neural networks. We\nwork under the following assumptions on the activation function and training data.\nAssumption 11.18. There exist R < ∞and 0 < λLC\nmin ≤λLC\nmax < ∞such that\n(a) for the activation function σ : R →R holds |σ(0)|, Lip(σ), Lip(σ′) ≤R,\n(b) ∥xi∥, |yi| ≤R for all training data (xi, yi) ∈Rd × R, i = 1, . . . , m,\n(c) the kernel matrix of the neural tangent kernel\n(KLC(xi, xj))m\ni,j=1 ∈Rm×m\nis regular and its eigenvalues belong to [λLC\nmin, λLC\nmax].\nWe start by showing Assumption 11.12 (a) for the present setting. More precisely, we give\nbounds for the eigenvalues of the empirical tangent kernel.\n158\nLemma 11.19. Let Assumption 11.18 be satisfied.\nThen for every δ\n>\n0 there exists\nn0(δ, λLC\nmin, m, R) ∈R such that for all n ≥n0 with probability at least 1 −δ all eigenvalues of\n( ˆKn(xi, xj))m\ni,j=1 =\n\u0000⟨∇wΦ(xi, w0), ∇wΦ(xj, w0)⟩\n\u0001m\ni,j=1 ∈Rm×m\nbelong to [nλLC\nmin/2, 2nλLC\nmax].\nProof. Denote ˆGn := ( ˆKn(xi, xj))m\ni,j=1 and GLC := (KLC(xi, xj))m\ni,j=1. By Theorem 11.16, there\nexists n0 such that for all n ≥n0 holds with probability at least 1 −δ that\n\r\r\r\rGLC −1\nn\nˆGn\n\r\r\r\r ≤λLC\nmin\n2\n.\nAssuming this bound to hold\n1\nn∥ˆGn∥= sup\na∈Rm\n∥a∥=1\n1\nn∥ˆGna∥≥\ninf\na∈Rm\n∥a∥=1\n∥GLCa∥−λLC\nmin\n2\n≥λLC\nmin −λLC\nmin\n2\n≥λLC\nmin\n2\n,\nwhere we have used that λLC\nmin is the smallest eigenvalue, and thus singular value, of the symmetric\npositive definite matrix GLC. This shows that the smallest eigenvalue of ˆGn is larger or equal to\nλLC\nmin/2. Similarly, we conclude that the largest eigenvalue is bounded from above by λLC\nmax+λLC\nmin/2 ≤\nλLC\nmax. This concludes the proof.\nNext we check Assumption 11.12 (b). To this end we first bound the norm of a random matrix.\nLemma 11.20. Let W(0, 1) be as in Assumption 11.14, and let W ∈Rn×d with Wij\niid\n∼W(0, 1).\nDenote the fourth moment of W(0, 1) by µ4. Then\nP\nh\n∥W ∥≤\np\nn(d + 1)\ni\n≥1 −dµ4\nn .\nProof. It holds\n∥W ∥≤∥W ∥F =\n\u0010\nn\nX\ni=1\nd\nX\nj=1\nW 2\nij\n\u00111/2\n.\nThe αi := Pd\nj=1 W 2\nij, i = 1, . . . , n, are i.i.d. distributed with expectation d and finite variance dC,\nwhere C ≤µ4 is the variance of W 2\n11. By Theorem A.23\nP\nh\n∥W ∥>\np\nn(d + 1)\ni\n≤P\nh 1\nn\nn\nX\ni=1\nαi > d + 1\ni\n≤P\nh\f\f\f 1\nn\nn\nX\ni=1\nαi −d\n\f\f\f > 1\ni\n≤dµ4\nn ,\nwhich concludes the proof.\n159\nLemma 11.21. Let Assumption 11.18 (a) be satisfied with some constant R. Then there exists\nM(R), and for all c, δ > 0 there exists n0(c, d, δ, R) ∈N such that for all n ≥n0 it holds with\nprobability at least 1 −δ\n∥∇wΦ(x, w)∥≤M√n\nfor all w ∈Bcn−1/2(w0)\n∥∇wΦ(x, w) −∇wΦ(x, v)∥≤M√n∥w −v∥\nfor all w, v ∈Bcn−1/2(w0)\nfor all x ∈Rd with ∥x∥≤R.\nProof. Due to the initialization (11.5.3), by Lemma 11.20 we can find n0(δ, d) such that for all\nn ≥n0 holds with probability at least 1 −δ that\n∥v0∥≤2\nand\n∥U 0∥≤2√n.\n(11.5.5)\nFor the rest of this proof we fix arbitrary x ∈Rd and n ≥n0 ≥c2 such that\n∥x∥≤R\nand\nn−1/2c ≤1.\nWe need to show that the claimed inequalities hold as long as (11.5.5) is satisfied. We will several\ntimes use that for all p, q ∈Rn\n∥p ⊙q∥≤∥p∥∥q∥\nand\n∥σ(p)∥≤R√n + R∥p∥\nsince |σ(x)| ≤R · (1 + |x|). The same holds for σ′.\nStep 1. We show the bound on the gradient. Fix\nw = (U, b, v, c)\ns.t.\n∥w −w0∥≤cn−1/2.\nUsing formula (11.5.2) for ∇bΦ and the above inequalities\n∥∇bΦ(x, w)∥≤∥∇bΦ(x, w0)∥+ ∥∇bΦ(x, w) −∇bΦ(x, w0)∥\n= ∥v0 ⊙σ′(U 0x)∥+ ∥v ⊙σ′(Ux + b) −v0 ⊙σ′(U 0x)∥\n≤2(R√n + 2R2√n) + ∥v ⊙σ′(Ux + b) −v0 ⊙σ′(U 0x)∥.\n(11.5.6)\nDue to\n∥U∥≤∥U 0∥+ ∥U 0 −U∥F ≤2√n + cn−1/2 ≤3√n,\n(11.5.7)\nthe last norm in (11.5.6) is bounded by\n∥(v −v0) ⊙σ′(Ux + b)∥+ ∥v0 ⊙(σ′(Ux + b) −σ′(U 0x))∥\n≤cn−1/2(R√n + R · (∥U∥∥x∥+ ∥b∥)) + 2R · (∥U −U 0∥∥x∥+ ∥b∥)\n≤R√n + 3√nR2 + cn−1/2R + 2R · (cn−1/2R + cn−1/2)\n≤√n(4R + 5R2)\nand therefore\n∥∇bΦ(x, w)∥≤√n(6R + 9R2).\n160\nFor the gradient with respect to U we use ∇UΦ(x, w) = ∇bΦ(x, w)x⊤, so that\n∥∇UΦ(x, w)∥F = ∥∇bΦ(x, w)x⊤∥F = ∥∇bΦ(x, w)∥∥x∥≤√n(6R2 + 9R3).\nNext\n∥∇vΦ(x, w)∥= ∥σ(Ux + b)∥\n≤R√n + R∥Ux + b∥\n≤R√n + R · (3√nR + cn−1/2)\n≤√n(2R + 3R2),\nand finally ∇cΦ(x, w) = 1. In all, with M1(R) := (1 + 8R + 12R2)\n∥∇wΦ(x, ˜w)∥≤√nM1(R).\nStep 2. We show Lipschitz continuity. Fix\nw = (U, b, v, c)\nand\n˜w = ( ˜U, ˜b, ˜v, ˜c)\nsuch that ∥w −w0∥, ∥˜w −w0∥≤cn−1/2. Then\n∥∇bΦ(x, w) −∇bΦ(x, ˜w)∥= ∥v ⊙σ′(Ux + b) −˜v ⊙σ′( ˜Ux + ˜b)∥.\nUsing ∥˜v∥≤∥v0∥+ cn−1/2 ≤3 and (11.5.7), this term is bounded by\n∥(v −˜v) ⊙σ′(Ux + b)∥+ ∥˜v ⊙(σ′(Ux + b) −σ′( ˜Ux + ˜b))∥\n≤∥v −˜v∥(R√n + R · (∥U∥∥x∥+ ∥b∥)) + 3R · (∥x∥∥U −˜U∥+ ∥b −˜b∥)\n≤∥w −˜w∥√n(5R + 6R2).\nFor ∇UΦ(x, w) we obtain similar as in Step 1\n∥∇UΦ(x, w) −∇UΦ(x, ˜w)∥F = ∥x∥∥∇bΦ(x, w) −∇bΦ(x, ˜w)∥\n≤∥w −˜w∥√n(5R2 + 6R3).\nNext\n∥∇vΦ(x, w) −∇vΦ(x, ˜w)∥= ∥σ(Ux + b) −σ( ˜Ux −˜b)∥\n≤R · (∥U −˜U∥∥x∥+ ∥b −˜b∥)\n≤∥w −˜w∥(R2 + R)\nand finally ∇cΦ(x, w) = 1 is constant. With M2(R) := R + 6R2 + 6R3 this shows\n∥∇wΦ(x, w) −∇wΦ(x, ˜w)∥≤√nM2(R)∥w −˜w∥.\nIn all, this concludes the proof with M(R) := max{M1(R), M2(R)}.\nBefore coming to the main result of this section, we first show that the initial error f(w0)\nremains bounded with high probability.\n161\nLemma 11.22. Let Assumption 11.18 (a), (b) be satisfied.\nThen for every δ > 0 exists\nR0(δ, m, R) > 0 such that for all n ∈N\nP[f(w0) ≤R0] ≥1 −δ.\nProof. Let i ∈{1, . . . , m}, and set α := U 0xi and ˜vj := √nv0;j for j = 1, . . . , n, so that ˜vj\niid\n∼\nW(0, 1). Then\nΦ(xi, w0) =\n1\n√n\nn\nX\nj=1\n˜vjσ(αj).\nBy Assumption 11.14 and (11.5.3), the ˜vjσ(αj), j = 1, . . . , n, are i.i.d. centered random variables\nwith finite variance bounded by a constant C(R) independent of n. Thus the variance of Φ(xi, w0)\nis also bounded by C(R). By Chebyshev’s inequality, see Lemma A.22, for every k > 0\nP[|Φ(xi, w0)| ≥k\n√\nC] ≤1\nk2 .\nSetting k =\np\nm/δ\nP\nh m\nX\ni=1\n|Φ(xi, w0) −yi|2 ≥m(k\n√\nC + R)2i\n≤\nm\nX\ni=1\nP\nh\n|Φ(xi, w0) −yi| ≥k\n√\nC + R\ni\n≤\nm\nX\ni=1\nP\nh\n|Φ(xi, w0)| ≥k\n√\nC\ni\n≤δ,\nwhich shows the claim with R0 = m · (\np\nCm/δ + R)2.\nThe next theorem is the main result of this section. It states that in the present setting gradient\ndescent converges to a global minimizer and the limiting network achieves zero loss, i.e. interpolates\nthe data. Moreover, during training the network weights remain close to initialization if the network\nwidth n is large.\nTheorem 11.23. Let Assumption 11.18 be satisfied, and let the parameters w0 of the neural\nnetwork Φ in (11.5.1) be initialized according to (11.5.3). Fix a learning rate\nh <\n2\nλLC\nmin + 4λLC\nmax\n1\nn\nand with the objective function (11.0.1b) let for all k ∈N\nwk+1 = wk −h∇f(wk).\n162\nThen for every δ > 0 there exist C > 0, n0 ∈N such that for all n ≥n0 holds with probability\nat least 1 −δ that for all k ∈N\n∥wk −w0∥≤C\n√n\nf(wk) ≤C\n\u0010\n1 −\nhn\n2λLC\nmin\n\u00112k\n.\nProof. We wish to apply Theorem 11.13, which requires Assumption 11.12 to be satisfied.\nBy\nLemma 11.19, 11.21 and 11.22, for every c > 0 we can find n0 such that for all n ≥n0 with\nprobability at least 1 −δ we have\np\nf(w0) ≤√R0 and Assumption 11.12 (a), (b) holds with the\nvalues\nL = M√n,\nU = M√n,\nr = cn−1/2,\nλmin = nλLC\nmin\n2\n,\nλmax = 2nλLC\nmax.\nFor Assumption 11.12 (c), it suffices that\nM√n ≤\nn2(λLC\nmin/2)2\n12m3/2M2n√R0\nand\ncn−1/2 ≥2mM√n\nn\np\nR0.\nChoosing c > 0 and n large enough, the inequalities hold. The statement is now a direct consequence\nof Theorem 11.13.\n11.5.4\nProximity to linearized model\nThe analysis thus far was based on the linearization Φlin describing the behaviour of the full network\nΦ well in a neighbourhood of the initial parameters w0. Moreover, Theorem 11.23 states that the\nparameters remain in an O(n−1/2) neighbourhood of w0 during training. This suggests that the\ntrained full model limk→∞Φ(x, wk) yields predictions similar to the trained linearized model.\nTo describe this phenomenon, we adopt again the notations Φlin : Rd × Rn →R and flin from\n(11.3.1) and (11.3.3). Initializing w0 according to (11.5.3) and setting p0 = w0, gradient descent\ncomputes the parameter updates\nwk+1 = wk −h∇wf(wk),\npk+1 = pk −h∇wflin(pk)\nfor the full and linearized models, respectively. Let us consider the dynamics of the prediction of\nthe network on the training data. Writing\nΦ(X, w) := (Φ(xi, w))m\ni=1 ∈Rm\nsuch that\n∇wΦ(X, w) ∈Rm×n\nit holds\n∇wf(w) = ∇w∥Φ(X, w) −y∥2 = 2∇wΦ(X, w)⊤(Φ(X, w) −y).\nThus for the full model\nΦ(X, wk+1) = Φ(X, wk) + ∇wΦ(X, ˜wk)(wk+1 −wk)\n= Φ(X, wk) −2h∇wΦ(X, ˜wk)∇wΦ(X, wk)⊤(Φ(X, wk) −y),\n(11.5.8)\n163\nwhere ˜wk is in the convex hull of wk and wk+1.\nSimilarly, for the linearized model with (cp. (11.3.1))\nΦlin(X, w) := (Φlin(xi, w))m\ni=1 ∈Rm\nand\n∇pΦlin(X, p) = ∇wΦ(X, w0) ∈Rm×n\nsuch that\n∇pflin(p) = ∇p∥Φlin(X, p) −y∥2 = 2∇wΦ(X, w0)⊤(Φlin(X, p) −y)\nand\nΦlin(X, pk+1) = Φlin(X, pk) + ∇pΦlin(X, p0)(pk+1 −pk)\n= Φlin(X, pk) −2h∇wΦ(X, w0)∇wΦ(X, w0)⊤(Φlin(X, pk) −y).\n(11.5.9)\nRemark 11.24. From (11.5.9) it is easy to see that with A := 2h∇wΦ(X, w0)∇wΦ(X, w0)⊤and\nB := Im −A holds the explicit formula\nΦlin(X, pk) = BkΦlin(X, p0) +\nk−1\nX\nj=0\nBkAy\nfor the prediction of the linear model in step k. Note that if A is regular and h is small enough,\nthen Bk converges to the zero matrix as k →∞and P∞\nj=0 Bk = A−1 since this is a Neumann\nseries.\nComparing the two dynamics (11.5.8) and (11.5.9), the difference only lies in the two Rm×m\nmatrices\n2h∇wΦ(X, ˜wk)∇wΦ(X, wk)⊤\nand\n2h∇wΦ(X, w0)∇wΦ(X, w0)⊤.\nRecall that the step size h in Theorem 11.23 scales like 1/n.\nProposition 11.25. Consider the setting of Theorem 11.23. Then there exists C < ∞, and for\nevery δ > 0 there exists n0 such that for all n ≥n0 holds with probability at least 1 −δ that for all\nk ∈N\n1\nn∥∇wΦ(X, ˜wk)∇wΦ(X, wk)⊤−∇pΦ(X, p0)∇pΦ(X, p0)⊤∥≤Cn−1/2.\nProof. Consider the setting of the proof of Theorem 11.23. Then for every k ∈N holds ∥wk−w0∥≤\nr and thus also ∥˜wk −w0∥≤r, where r = cn−1/2. Thus Lemma 11.21 implies the norm to be\nbounded by\n1\nn∥∇wΦ(X, ˜wk) −∇pΦ(X, p0)∥∥∇wΦ(X, wk)⊤∥+\n1\nn∥∇pΦ(X, p0)∥∥∇wΦ(X, wk)⊤−∇pΦ(X, p0)⊤∥\n≤mM(∥˜wk −p0∥+ ∥wk −p0∥) ≤cmMn−1/2\nwhich gives the statement.\n164\nBy Proposition 11.25 the two matrices driving the dynamics (11.5.8) and (11.5.9) remain in\nan O(n−1/2) neighbourhood of each other throughout training. This allows to show the following\nproposition, which states that the prediction function learned by the network gets arbitrarily close\nto the one learned by the linearized version in the limit n →∞. The proof, which we omit, is based\non Gr¨onwall’s inequality. See [106, 131].\nProposition 11.26. Consider the setting of Theorem 11.23. Then there exists C < ∞, and for\nevery δ > 0 there exists n0 such that for all n ≥n0 holds with probability at least 1 −δ that for all\n∥x∥≤1\nsup\nk∈N\n|Φ(x, wk) −Φlin(x, pk)| ≤Cn−1/2.\n11.5.5\nConnection to Gaussian processes\nIn the previous section, we established that for large widths, the trained neural network mirrors the\nbehaviour of the trained linearized model, which itself is closely connected to kernel least-squares\nwith the neural tangent kernel.\nYet, as pointed out in Remark 11.4, the obtained model still\nstrongly depends on the choice of random initialization w0 ∈Rn. We should thus understand both\nthe model at initialization x 7→Φ(x, w0) and the model after training x 7→Φ(x, wk), as random\ndraws of a certain distribution over functions. To make this precise, let us introduce Gaussian\nprocesses.\nDefinition 11.27. Let (Ω, P) be a probability space, and let g : Rd×Ω→R. We call g a Gaussian\nprocess with mean function m : Rd →R and covariance function c : Rd × Rd →R if\n(a) for each x ∈Rd holds ω 7→g(x, ω) is a random variable,\n(b) for all k ∈N and all x1, . . . , xk ∈Rd the random variables g(x1, ·), . . . , g(xk, ·) have a joint\nGaussian distribution such that\n(g(x1, ω), . . . , g(xk, ω)) ∼N\n\u0010\nm(xi)k\ni=1, (c(xi, xj))k\ni,j=1\n\u0011\n.\nIn words, g is a Gaussian process, if ω 7→g(x, ω) defines a collection of random variables indexed\nover x ∈Rd, such that the joint distribution of (g(x1, ·))n\nj=1 is a Gaussian whose mean and variance\nare determined by m and c respectively. Fixing ω ∈Ω, we can then interpret x 7→g(x, ω) as a\nrandom draw from a distribution over functions.\nAs first observed in [157], certain neural networks at initialization tend to Gaussian processes\nin the infinite width limit.\n165\nProposition 11.28. Consider depth-n networks Φn as in (11.5.1) with initialization (11.5.3), and\ndefine with ui\niid\n∼W(0, 1/d), i = 1, . . . , d,\nc(x, z) := E[σ(u⊤x)σ(u⊤z)]\nfor all x, z ∈Rd.\nThen for all distinct x1, . . . , xk ∈Rd it holds that\nlim\nn→∞(Φn(x1, w0), . . . , Φn(xk, w0)) ∼N(0, (c(xi, xj))k\ni,j=1)\nwith weak convergence.\nProof. Set ˜vi := √nv0,i and ˜ui = (U0,i1, . . . , U0,id) ∈Rd, so that ˜vi\niid\n∼W(0, 1), and the ˜ui ∈Rd are\nalso i.i.d., with each component distributed according to W(0, 1/d).\nThen for any x1, . . . , xk\nZi :=\n\n\n\n˜viσ(˜u⊤\ni x1)\n...\n˜viσ(˜u⊤\ni xk)\n\n\n∈Rk\ni = 1, . . . , n,\ndefines n centered i.i.d. vectors in Rk. By the central limit theorem, see Theorem A.25,\n\n\n\nΦ(x1, w0)\n...\nΦ(xk, w0)\n\n\n=\n1\n√n\nn\nX\nj=1\nZi\nconverges weakly to N(0, C), where\nCij = E[˜v2\n1σ(˜u⊤\n1 xi)σ(˜u⊤\n1 xj)] = E[σ(˜u⊤\n1 xi)σ(˜u⊤\n1 xj)].\nThis concludes the proof.\nIn the sense of Proposition 11.28, the network Φ(x, w0) converges to a Gaussian process as the\nwidth n tends to infinity. Using the explicit dynamics of the linearized network outlined in Remark\n11.24, one can show that the linearized network after training also corresponds to a Gaussian\nprocess (for some mean and covariance function depending on the data, the architecture, and the\ninitialization). As the full and linearized models converge in the infinite width limit, we can infer\nthat wide networks post-training resemble draws from a Gaussian process, see [131, Sec. 2.3.1] and\n[46].\nRather than delving into the technical details of such statements, in Figure 11.3 we plot 80\ndifferent realizations of a neural network before and after training, i.e. the functions\nx 7→Φ(x, w0)\nand\nx 7→Φ(x, wk).\n(11.5.10)\nWe chose the architecture as (11.5.1) with activation function σ = arctan(x), width n = 250 and\ninitialization\nU0;ij\niid\n∼N\n\u0010\n0, 3\nd\n\u0011\n,\nv0;i\niid\n∼N\n\u0010\n0, 3\nn\n\u0011\n,\nb0;i, c0\niid\n∼N(0, 2).\n(11.5.11)\n166\nThe network was trained on a dataset of size m = 3 with k = 1000 steps of gradient descent\nand constant step size h = 1/n. Before training, the network’s outputs resemble random draws\nfrom a Gaussian process with a constant zero mean function.\nPost-training, the outputs show\nminimal variance at the data points, since they essentially interpolate the data, cp. Remark 11.4\nand (11.2.4). They exhibit increased variance further from these points, with the precise amount\ndepending on the initialization variance chosen in (11.5.11).\n3\n2\n1\n0\n1\n2\n3\n2\n1\n0\n1\n2\n(a) before training\n3\n2\n1\n0\n1\n2\n3\n2\n1\n0\n1\n2\n(b) after training\nFigure 11.3: 80 realizations of a neural network at initialization (a) and after training on the red\ndata points (b). The blue dashed line shows the mean. Figure based on [131, Fig. 2].\n11.6\nNormalized initialization\nConsider the gradient ∇wΦ(x, w0) as in (11.5.2) with LeCun initialization. Since the components\nof v behave like vi\niid\n∼W(0, 1/n), it is easy to check that in terms of the width n\nE[∥∇UΦ(x, w0)∥] = E[∥(v ⊙σ′(Ux + b))x⊤∥]\n= O(1)\nE[∥∇bΦ(x, w0)∥] = E[∥v ⊙σ′(Ux + b)∥]\n= O(1)\nE[∥∇vΦ(x, w0)∥] = E[∥σ(Ux + b)∥]\n= O(n)\nE[∥∇cΦ(x, w0)∥] = E[|1|]\n= O(1).\nAs a result of this different scaling, gradient descent with step width O(n−1) as in Theorem 11.23,\nwill primarily train the weigths v in the output layer, and will barely move the remaining parameters\nU, b, and c. This is also reflected in the expression for the obtained kernel KLC computed in\nTheorem 11.16, which corresponds to the contribution of the term ⟨∇vΦ, ∇vΦ⟩.\nRemark 11.29. For optimization methods such as ADAM, which scale each component of the\ngradient individually, the same does not hold in general.\nLeCun initialization aims to normalize the variance of the output of all nodes at initialization\n(the forward dynamics). To also normalize the variance of the gradients (the backward dynamics),\nin this section we shortly dicuss a different architecture and initialization, consistent with the one\nused in the original NTK paper [106].\n167\n11.6.1\nArchitecture\nLet Φ : Rd →R be a depth-one neural network\nΦ(x, w) =\n1\n√nv⊤σ\n\u0010 1\n√\nd\nUx + b\n\u0011\n+ c,\n(11.6.1)\nwith input x ∈Rd and parameters U ∈Rn×d, v ∈Rn, b ∈Rn and c ∈R. We initialize the weights\nrandomly according to w0 = (U 0, b0, v0, c0) with parameters\nU0;ij\niid\n∼W(0, 1),\nv0;i\niid\n∼W(0, 1),\nb0;i, c0 = 0.\n(11.6.2)\nAt initialization, (11.6.1), (11.6.2) is equivalent to (11.5.1), (11.5.3). However, for the gradient we\nobtain\n∇UΦ(x, w) = n−1/2\u0010\nv ⊙σ′(d−1/2Ux + b)\n\u0011\nd−1/2x⊤∈Rn×d\n∇bΦ(x, w) = n−1/2v ⊙σ′\u0010\nd−1/2Ux + b)\n\u0011\n∈Rn\n∇vΦ(x, w) = n−1/2σ(d−1/2Ux + b) ∈Rn\n∇cΦ(x, w) = 1 ∈R.\n(11.6.3)\nContrary to (11.5.2), the three gradients with O(n) entries are all scaled by the factor n−1/2. This\nleads to a different training dynamics.\n11.6.2\nNeural tangent kernel\nWe compute again the neural tangent kernel. Unlike for LeCun initialization, there is no 1/n scaling\nrequired to obtain convergence of\nˆKn(x, z) = ⟨∇wΦ(x, w0), ∇wΦ(z, w0)⟩\nas n →∞. Here and in the following we consider the setting (11.6.1)–(11.6.2) for Φ and w0.\nSince this is also referred to as the NTK initialization, we denote the kernel by KNTK. Due to the\ndifferent training dynamics, we obtain additional terms in the NTK compared to Theorem 11.23.\nTheorem 11.30. Let R < ∞such that |σ(x)| ≤R · (1 + |x|) and |σ′(x)| ≤R · (1 + |x|) for all\nx ∈R, and let W satisfy Assumption 11.14. For any x, z ∈Rd and ui\niid\n∼W(0, 1/d), i = 1, . . . , d,\nit then holds\nlim\nn→∞\nˆKn(x, z) =\n\u0010\n1 + x⊤z\nd\n\u0011\nE[σ′(u⊤x)⊤σ′(u⊤z)] + E[σ(u⊤x)⊤σ(u⊤z)] + 1\n=: KNTK(x, z)\nalmost surely.\n168\nProof. Denote x(1) = U 0x + b0 ∈Rn and z(1) = U 0z + b0 ∈Rn. Due to the initialization (11.6.2)\nand our assumptions on W(0, 1), the components\nx(1)\ni\n=\nd\nX\nj=1\nU0;ijxj ∼u⊤x\ni = 1, . . . , n\nare i.i.d. with finite pth moment (independent of n) for all 1 ≤p ≤8, and the same holds for the\n(σ(x(1)\ni ))n\ni=1, (σ′(x(1)\ni ))n\ni=1, (σ(z(1)\ni\n))n\ni=1, and (σ′(z(1)\ni\n))n\ni=1.\nThen\nˆKn(x, z) =\n\u0010\n1 + x⊤z\nd\n\u0011 1\nn\nn\nX\ni=1\nv2\ni σ′(x(1)\ni )σ′(z(1)\ni\n) + 1\nn\nn\nX\ni=1\nσ(x(1)\ni )σ(z(1)\ni\n) + 1.\nBy the law of large numbers and because E[v2\ni ] = 1, this converges almost surely to KNTK(x, z).\nThe existence of n0 follows similarly by an application of Theorem A.23.\nExample 11.31 (KNTK for ReLU). Let σ(x) = max{0, x} and let W(0, 1/d) be the centered\nnormal distribution on R with variance 1/d. For x, z ∈Rd holds by [37, Appendix A] (also see\nExercise 11.36), that with ui\niid\n∼W(0, 1/d), i = 1, . . . , d,\nE[σ′(u⊤x)σ′(u⊤z)] =\nπ −arccos\n\u0010\nx⊤z\n∥x∥∥z∥\n\u0011\n2π\n.\nTogether with Example 11.17, this yields an explizit formula for KNTK in Theorem 11.30.\nFor this network architecture and under suitable assumptions on W, similar arguments as\nin Section 11.5 can be used to show convergence of gradient descent to a global minimizer and\nproximity of the full to the linearized model. We refer to the literature in the bibliography section.\nBibliography and further reading\nThe discussion on linear and kernel regression in Sections 11.1 and 11.2 is quite standard, and can\nsimilarly be found in many textbooks. For more details on kernel methods we refer for instance\nto [42, 206].\nThe neural tangent kernel and its connection to the training dynamics was first\ninvestigated in [106] using an architecture similar to the one in Section 11.6. Since then, many\nworks have extended this idea and presented differing perspectives on the topic, see for instance [2,\n56, 5, 36]. Our presentation in Sections 11.4, 11.5, and 11.6 primarily follows [131] who also discussed\nthe case of LeCun initialization. Especially for the main results in Theorem 11.13 and Theorem\n11.23, we largely follow the arguments in this paper. The above references additionally treat the\ncase of deep networks, which we have omitted here for simplicity. The explicit formula for the NTK\nof ReLU networks as presented in Examples 11.17 and 11.31 was given in [37]. The observation\nthat neural networks at initialization behave like Gaussian processes discussed in Section 11.5.5 was\nfirst made in [157]. For a general reference on Gaussian processes see the textbook [188]. When\nonly training the last layer of a network (in which the network is affine linear), there are strong\nlinks to random feature methods [186]. Recent developements on this topic can also be found in\nthe literature under the name “Neural network Gaussian processes”, or NNGPs for short [130, 47].\n169\nExercises\nExercise 11.32. Prove Theorem 11.3.\nHint: Assume first that w0 ∈ker(A)⊥(i.e. w0 ∈˜H). For rank(A) < d, using wk = wk−1 −\nh∇f(wk−1) and the singular value decomposition of A, write down an explicit formula for wk.\nObserve that due to 1/(1 −x) = P\nk∈N0 xk for all x ∈(0, 1) it holds wk →A†y as k →∞, where\nA† is the Moore-Penrose pseudoinverse of A.\nExercise 11.33. Let xi ∈Rd, i = 1, . . . , m. Show that there exists a “feature map” ϕ : Rd →Rm,\nsuch that for any configuration of labels yi ∈{−1, 1}, there always exists a hyperplane in Rm\nseparating the two sets {ϕ(xi) | yi = 1} and {ϕ(xi) | yi = −1}.\nExercise 11.34. Consider the RBF kernel K : R × R →R, K(x, x′) := exp(−(x −x′)2). Find a\nHilbert space H and a feature map ϕ : R →H such that K(x, x′) = ⟨ϕ(x), ϕ(x′)⟩H.\nExercise 11.35. Let n ∈N and consider the polynomial kernel K : Rd × Rd →R, K(x, x′) =\n(1 + x⊤x′)r.\nFind a Hilbert space H and a feature map ϕ : Rd →H, such that K(x, x′) =\n⟨ϕ(x), ϕ(x′)⟩H.\nHint: Use the multinomial formula.\nExercise 11.36. Let ui\niid\n∼N(0, 1) be i.i.d. standard Gaussian distributed random variables for\ni = 1, . . . , d. Show that for all nonzero x, z ∈Rd\nE[1[0,∞)(u⊤x)1[0,∞)(u⊤z)] = π −θ\n2π ,\nθ = arccos\n\u0010\nxz⊤\n∥x∥∥z∥\n\u0011\n.\nThis shows the formula for the ReLU NTK with Gaussian initialization as discussed in Example\n11.31.\nHint: Consider the following sketch\nx\nz\nθ\nExercise 11.37. Consider the network (11.5.1) with LeCun initialization as in (11.5.3), but with\nthe biases instead initialized as\nc, bi\niid\n∼W(0, 1)\nfor all i = 1, . . . , n.\n(11.6.4)\nCompute the corresponding NTK as in Theorem 11.23. Moreover, compute the NTK also for the\nnormalized network (11.6.1) with initialization (11.6.2) as in Theorem 11.30, but replace again the\nbias initialization with that given in (11.6.4).\n170\nChapter 12\nLoss landscape analysis\nIn Chapter 10, we saw how the weights of neural networks get adapted during training, using, e.g.,\nvariants of gradient descent. For certain cases, including the wide networks considered in Chapter\n11, the corresponding iterative scheme converges to a global minimizer. In general, this is not\nguaranteed, and gradient descent can for instance get stuck in non-global minima or saddle points.\nTo get a better understanding of these situations, in this chapter we discuss the so-called loss\nlandscape. This term refers to the graph of the empirical risk as a function of the weights. We\ngive a more rigorous definition below, and first introduce notation for neural networks and their\nrealizations for a fixed architecture.\nDefinition 12.1. Let A = (d0, d1, . . . , dL+1) ∈NL+2, let σ: R →R be an activation function, and\nlet B > 0. We denote the set of neural networks Φ with L layers, layer widths d0, d1, . . . , dL+1, all\nweights bounded in modulus by B, and using the activation function σ by N(σ; A, B). Additionally,\nwe define\nPN(A, B) :=\nL×\nℓ=0\n\u0010\n[−B, B]dℓ+1×dℓ× [−B, B]dℓ+1\n\u0011\n,\nand the realization map\nRσ : PN(A, B) →N(σ; A, B)\n(W (ℓ), b(ℓ))L\nℓ=0 7→Φ,\n(12.0.1)\nwhere Φ is the neural network with weights and biases given by (W (ℓ), b(ℓ))L\nℓ=0.\nThroughout, we will identify PN(A, B) with the cube [−B, B]nA, where nA := PL\nℓ=0 dℓ+1(dℓ+\n1). Now we can introduce the loss landscape of a neural network architecture.\nDefinition 12.2. Let A = (d0, d1, . . . , dL+1) ∈NL+2, let σ: R →R.\nLet m ∈N, and S =\n(xi, yi)m\ni=1 ∈(Rd0 × RdL+1)m be a sample and let L be a loss function. Then, the loss landscape\n171\ng\nl\no\nb\na\nl\n \nm\ni\nni\nma\ns\na\nd\nd\nl\ne\n \np\no\ni\nnt\ns\nl\no\nc\na\nl\n \nm\ni\nn\ni\nm\na\nsh\na\nr\np\n \nm\nin\nim\num\nHi\ngh\n \ne\nm\np\ni\nr\ni\nc\na\nl r\ni\ns\nk\nFigure 12.1: Two-dimensional section of a loss landscape. The loss landscape shows a spurious\nvalley with local minima, global minima, as well as a region where saddle points appear. Moreover,\na sharp minimum is shown.\nis the graph of the function ΛA,σ,S,L defined as\nΛA,σ,S,L : PN(A; ∞) →R\nθ 7→bRS(Rσ(θ)).\nwith bRS in (1.2.3) and Rσ in (12.0.1).\nIdentifying PN(A, ∞) with RnA, we can consider ΛA,σ,S,L as a map on RnA and the loss\nlandscape is a subset of RnA × R. The loss landscape is a high-dimensional surface, with hills and\nvalleys. For visualization a two-dimensional section of a loss landscape is shown in Figure 12.1.\nQuestions of interest regarding the loss landscape include for example: How likely is it that we\nfind local instead of global minima? Are these local minima typically sharp, having small volume,\nor are they part of large flat valleys that are difficult to escape? How bad is it to end up in a local\nminimum? Are most local minima as deep as the global minimum, or can they be significantly\nhigher? How rough is the surface generally, and how do these characteristics depend on the network\narchitecture? While providing complete answers to these questions is hard in general, in the rest\nof this chapter we give some intuition and mathematical insights for specific cases.\n172\n12.1\nVisualization of loss landscapes\nVisualizing loss landscapes can provide valuable insights into the effects of neural network depth,\nwidth, and activation functions. However, we can only visualize an at most two-dimensional surface\nembedded into three-dimensional space, whereas the loss landscape is a very high-dimensional\nobject (unless the neural networks have only very few weights and biases).\nTo make the loss landscape accessible, we need to reduce its dimensionality. This can be achieved\nby evaluating the function ΛA,σ,S,L on a two-dimensional subspace of PN(A, ∞). Specifically, we\nchoose three-parameters µ, θ1, θ2 and examine the function\nR2 ∋(α1, α2) 7→ΛA,σ,S,L(µ + α1θ1 + α2θ2).\n(12.1.1)\nThere are various natural choices for µ, θ1, θ2:\n• Random directions: This was, for example used in [74, 102]. Here θ1, θ2 are chosen randomly,\nwhile µ is either a minimum of ΛA,σ,S,L or also chosen randomly. This simple approach can\noffer a quick insight into how rough the surface can be. However, as was pointed out in\n[134], random directions will very likely be orthogonal to the trajectory of the optimization\nprocedure. Hence, they will likely miss the most relevant features.\n• Principal components of learning trajectory: To address the shortcomings of random direc-\ntions, another possibility is to determine µ, θ1, θ2, which best capture some given learning\ntrajectory; For example, if θ(1), θ(2), . . . , θ(N) are the parameters resulting from the training\nby SGD, we may determine µ, θ1, θ2 such that the hyperplane {µ + α1θ1 + α2θ2 | α1, α2 ∈R}\nminimizes the mean squared distance to the θ(j) for j ∈{1, . . . , N}. This is the approach of\n[134], and can be achieved by a principal component analysis.\n• Based on critical points: For a more global perspective, µ, θ1, θ2 can be chosen to ensure the\nobservation of multiple critical points. One way to achieve this is by running the optimization\nprocedure three times with final parameters θ(1), θ(2), θ(3). If the procedures have converged,\nthen each of these parameters is close to a critical point of ΛA,σ,S,L. We can now set µ = θ(1),\nθ1 = θ(2) −µ, θ2 = θ(3) −µ. This then guarantees that (12.1.1) passes through or at least\ncomes very close to three critical points (at (α1, α2) = (0, 0), (0, 1), (1, 0)). We present six\nvisualizations of this form in Figure 12.2.\nFigure 12.2 gives some interesting insight into the effect of depth and width on the shape of the\nloss landscape. For very wide and shallow neural networks, we have the widest minima, which, in\nthe case of the tanh activation function also seem to belong to the same valley. With increasing\ndepth and smaller width the minima get steeper and more disconnected.\n12.2\nSpurious valleys\nFrom the perspective of optimization, the ideal loss landscape has one global minimum in the center\nof a large valley, so that gradient descent converges towards the minimum irrespective of the chosen\ninitialization.\nThis situation is not realistic for deep neural networks. Indeed, for a simple shallow neural\nnetwork\nRd ∋x 7→Φ(x) = W (1)σ(W (0)x + b(0)) + b(1),\n173\nit is clear that for every permutation matrix P\nΦ(x) = W (1)P T σ(P W (0)x + P b(0)) + b(1)\nfor all x ∈Rd.\nHence, in general there exist multiple parameterizations realizing the same output function. More-\nover, if at least one global minimum with non-permutation-invariant weights exists, then there are\nmore than one global minima of the loss landscape.\nThis is not problematic; in fact, having many global minima is beneficial. The larger issue is the\nexistence of non-global minima. Following [235], we start by generalizing the notion of non-global\nminima to spurious valleys.\nDefinition 12.3. Let A = (d0, d1, . . . , dL+1) ∈NL+2 and σ: R →R.\nLet m ∈N, and S =\n(xi, yi)m\ni=1 ∈(Rd0 × RdL+1)m be a sample and let L be a loss function. For c ∈R, we define the\nsub-level set of ΛA,σ,S,L as\nΩΛ(c) := {θ ∈PN(A, ∞) | ΛA,σ,S,L(θ) ≤c}.\nA path-connected component of ΩΛ(c), which does not contain a global minimum of ΛA,σ,S,L is\ncalled a spurious valley.\nThe next proposition shows that spurious local minima do not exist for shallow overparameter-\nized neural networks, i.e., for neural networks that have at least as many parameters in the hidden\nlayer as there are training samples.\nProposition 12.4. Let A = (d0, d1, 1) ∈N3 and let S = (xi, yi)m\ni=1 ∈(Rd0 × R)m be a sample such\nthat m ≤d1. Furthermore, let σ ∈M be not a polynomial, and let L be a convex loss function.\nFurther assume that ΛA,σ,S,L has at least one global minimum. Then, ΛA,σ,S,L, has no spurious\nvalleys.\nProof. Let θa, θb ∈PN(A, ∞) with ΛA,σ,S,L(θa) > ΛA,σ,S,L(θb). Then we will show below that\nthere is another parameter θc such that\n• ΛA,σ,S,L(θb) = ΛA,σ,S,L(θc)\n• there is a continuous path α : [0, 1] →PN(A, ∞) such that α(0) = θa, α(1) = θc, and\nΛA,σ,S,L(α) is monotonically decreasing.\nBy Exercise 12.7, the construction above rules out the existence of spurious valleys by choosing θa\nan element of a spurious valley and θb a global minimum.\nNext, we present the construction: Let us denote\nθo =\n\u0012\u0010\nW (ℓ)\no , b(ℓ)\no\n\u00111\nℓ=0\n\u0013\nfor o ∈{a, b, c}.\n174\nMoreover, for j = 1, . . . , d1, we introduce vj\no ∈Rm defined as\n(vj\no)i =\n\u0010\nσ\n\u0010\nW (0)\no xi + b(0)\no\n\u0011\u0011\nj\nfor i = 1, . . . , m.\nNotice that, if we set V o = ((vj\no)⊤)d1\nj=1, then\nW (1)\no V o =\n\u0010\nRσ(θo)(xi) −b(1)\no\n\u0011m\ni=1 ,\n(12.2.1)\nwhere the right-hand side is considered a row-vector.\nWe will now distinguish between two cases. For the first the result is trivial and the second can\nbe transformed into the first one.\nCase 1:\nAssume that V a has rank m. In this case, it is obvious from (12.2.1), that there\nexists f\nW such that\nf\nW V a =\n\u0010\nRσ(θb)(xi) −b(1)\na\n\u0011m\ni=1 .\nWe can thus set α(t) = ((W (0)\na , b(0)\na ), ((1 −t)W (1)\na\n+ tf\nW , b(1)\na )).\nNote that by construction α(0) = θa and ΛA,σ,S,L(α(1)) = ΛA,σ,S,L(θb).\nMoreover, t 7→\n(Rσ(α(t))(xi))m\ni=1 describes a straight path in Rm and hence, by the convexity of L it is clear\nthat t 7→ΛA,σ,S,L(α(t)) is monotonically decreasing.\nCase 2:\nAssume that Va has rank less than m. In this case, we show that we find a continuous\npath from θa to another neural network parameter with higher rank. The path will be such that\nΛA,σ,S,L is monotonically decreasing.\nUnder the assumptions, we have that one vj\na can be written as a linear combination of the\nremaining vi\na, i ̸= j. Without loss of generality, we assume j = 1. Then, there exist (αi)m\ni=2 such\nthat\nv1\na =\nm\nX\ni=2\nαivi\na.\n(12.2.2)\nNext, we observe that there exists v∗∈Rm which is linearly independent from all (vj\na)m\ni=1 and\ncan be written as (v∗)i = σ((w∗)⊤xi + b∗) for some w∗∈Rd0, b∗∈R. Indeed, if we assume that\nsuch v∗does not exist, then it follows that span{(σ(w⊤xi + b))m\ni=1 | w ∈Rd0, b ∈R} is an m −1\ndimensional subspace of Rm which yields a contradiction to Theorem 9.3.\nNow, we define two paths: First,\nα1(t) = ((W (0)\na , b(0)\na ), (W (1)\na (t), b(1)\na )),\nfor t ∈[0, 1/2]\nwhere\n(W (1)\na (t))1 = (1 −2t)(W (1)\na )1\nand\n(W (1)\na (t))i = (W (1)\na )i + 2tαi(W (1)\na )1\nfor i = 2, . . . , d1, for t ∈[0, 1/2]. Second,\nα2(t) = ((W (0)\na (t), b(0)\na (t)), (W (1)\na (1/2), b(1)\na )), for t ∈(1/2, 1],\nwhere\n(W (0)\na (t))1 = 2(t −1/2)(W (0)\na )1 + (2t −1)w∗\nand\n(W (0)\na (t))i = (W (0)\na )i\n175\nfor i = 2, . . . , d1, (b(0)\na (t))1 = 2(t −1/2)(b(0)\na )1 + (2t −1)b∗, and (b(0)\na (t))i = (b(0)\na )i for i = 2, . . . , d1.\nIt is clear by (12.2.2) that (Rσ(α1)(xi))m\ni=1 is constant. Moreover, Rσ(α2)(x) is constant for all\nx ∈Rd0. In addition, by construction for\n¯vj :=\n\u0012\u0010\nσ\n\u0010\nW (0)\na (1)xi + b(0)\na (1)\n\u0011\u0011\nj\n\u0013m\ni=1\nit holds that ((¯vj)⊤)d1\nj=1 has rank larger than that of V a. Concatenating α1 and α2 now yields a\ncontinuous path from θa to another neural network parameter with higher associated rank such\nthat ΛA,σ,S,L is monotonically decreasing along the path. Iterating this construction, we can find\na path to a neural network parameter where the associated matrix has full rank. This reduces the\nproblem to Case 1.\n12.3\nSaddle points\nSaddle points are critical points of the loss landscape at which the loss decreases in one direction.\nIn this sense, saddle points are not as problematic as local minima or spurious valleys if the updates\nin the learning iteration have some stochasticity. Eventually, a random step in the right direction\ncould be taken and the saddle point can be escaped.\nIf most of the critical points are saddle points, then, even though the loss landscape is challenging\nfor optimization, one still has a good chance of eventually reaching the global minimum. Saddle\npoints of the loss landscape were studied in [45, 172] and we will review some of the findings in a\nsimplified way below. The main observation in [172] is that, under some quite strong assumptions,\nit holds that critical points in the loss landscape associated to a large loss are typically saddle points,\nwhereas those associated to small loss correspond to minima. This situation is encouraging for the\nprospects of optimization in deep learning, since, even if we get stuck in a local minimum, it will\nvery likely be such that the loss is close to optimal.\nThe results of [172] use random matrix theory, which we do not recall here. Moreover, it is hard\nto gauge if the assumptions made are satisfied for a specific problem. Nonetheless, we recall the\nmain idea, which provides some intuition to support the above claim.\nLet A = (d0, d1, 1) ∈N3. Then, for a neural network parameter θ ∈PN(A, ∞) and activation\nfunction σ, we set Φθ := Rσ(θ) and define for a sample S = (xi, yi)m\ni=1 the errors\nei = Φθ(xi) −yi\nfor i = 1, . . . , m.\nIf we use the square loss, then\nc\nRS(Φθ) = 1\nm\nm\nX\ni=1\ne2\ni .\n(12.3.1)\nNext, we study the Hessian of bRS(Φθ).\nProposition 12.5. Let A = (d0, d1, 1) and σ : R →R. Then, for every θ ∈PN(A, ∞) where\nbRS(Φθ) in (12.3.1) is twice continuously differentiable with respect to the weights, it holds that\nH(θ) = H0(θ) + H1(θ),\n176\nwhere H(θ) is the Hessian of bRS(Φθ) at θ, H0(θ) is a positive semi-definite matrix which is\nindependent from (yi)m\ni=1, and H1(θ) is a symmetric matrix that for fixed θ and (xi)m\ni=1 depends\nlinearly on (ei)m\ni=1.\nProof. Using the identification introduced after Definition 12.2, we can consider θ a vector in RnA.\nFor k = 1, . . . , nA, we have that\n∂bRS(Φθ)\n∂θk\n= 2\nm\nm\nX\ni=1\nei\n∂Φθ(xi)\n∂θk\n.\nTherefore, for j = 1, . . . , nA, we have, by the Leibniz rule, that\n∂2 bRS(Φθ)\n∂θj∂θk\n= 2\nm\nm\nX\ni=1\n\u0012∂Φθ(xi)\n∂θj\n∂Φθ(xi)\n∂θk\n\u0013\n+ 2\nm\n m\nX\ni=1\nei\n∂2Φθ(xi)\n∂θj∂θk\n!\n(12.3.2)\n=: H0(θ) + H1(θ).\nIt remains to show that H0(θ) and H1(θ) have the asserted properties. Note that, setting\nJi,θ =\n\n\n\n\n∂Φθ(xi)\n∂θ1...\n∂Φθ(xi)\n∂θnA\n\n\n\n∈RnA,\nwe have that H0(θ) = 2\nm\nPm\ni=1 Ji,θJ⊤\ni,θ and hence H0(θ) is a sum of positive semi-definite matrices,\nwhich shows that H0(θ) is positive semi-definite.\nThe symmetry of H1(θ) follows directly from the symmetry of second derivatives which holds\nsince we assumed twice continuous differentiability at θ. The linearity of H1(θ) in (ei)m\ni=1 is clear\nfrom (12.3.2).\nHow does Proposition 12.5 imply the claimed relationship between the size of the loss and the\nprevalence of saddle points?\nLet θ correspond to a critical point. If H(θ) has at least one negative eigenvalue, then θ cannot\nbe a minimum, but instead must be either a saddle point or a maximum. While we do not know\nanything about H1(θ) other than that it is symmetric, it is not unreasonable to assume that it\nhas a negative eigenvalue especially if nA is very large. With this consideration, let us consider the\nfollowing model:\nFix a parameter θ. Let S0 = (xi, y0\ni )m\ni=1 be a sample and (e0\ni )m\ni=1 be the associated errors.\nFurther let H0(θ), H0\n0(θ), H0\n1(θ) be the matrices according to Proposition 12.5.\nFurther let for λ > 0, Sλ = (xi, yλ\ni )m\ni=1 be such that the associated errors are (ei)m\ni=1 = λ(e0\ni )m\ni=1.\nThe Hessian of bRSλ(Φθ) at θ is then Hλ(θ) satisfying\nHλ(θ) = H0\n0(θ) + λH0\n1(θ).\nHence, if λ is large, then Hλ(θ) is perturbation of an amplified version of H0\n1(θ). Clearly, if v is\nan eigenvector of H1(θ) with negative eigenvalue −µ, then\nv⊤Hλ(θ)v ≤(∥H0\n0(θ)∥−λµ)∥v∥2,\n177\nwhich we can expect to be negative for large λ. Thus, Hλ(θ) has a negative eigenvalue for large λ.\nOn the other hand, if λ is small, then Hλ(θ) is merely a perturbation of H0\n0(θ) and we can\nexpect its spectrum to resemble that of H0\n0 more and more.\nWhat we see is that, the same parameter, is more likely to be a saddle point for a sample that\nproduces a high empirical risk than for a sample with small risk. Note that, since H0\n0(θ) was only\nshown to be semi-definite the argument above does not rule out saddle points even for very small\nλ. But it does show that for small λ, every negative eigenvalue would be very small.\nA more refined analysis where we compare different parameters but for the same sample and\nquantify the likelihood of local minima versus saddle points requires the introduction of a probability\ndistribution on the weights. We refer to [172] for the details.\nBibliography and further reading\nThe results on visualization of the loss landscape are inspired by [134, 74, 102]. Results on the\nnon-existence of spurious valleys can be found in [235] with similar results in [184]. In [39] the\nloss landscape was studied by linking it to so-called spin-glass models. There it was found that\nunder strong assumptions critical points associated to lower losses are more likely to be minima\nthan saddle points.\nIn [172], random matrix theory is used to provide similar results, that go\nbeyond those established in Section 12.3. On the topic of saddle points, [45] identifies the existence\nof saddle points as more problematic than that of local minima, and an alternative saddle-point\naware optimization algorithm is introduced.\nTwo essential topics associated to the loss landscape that have not been discussed in this chapter\nare mode connectivity and the sharpness of minima. Mode connectivity, roughly speaking describes\nthe phenomenon, that local minima found by SGD over deep neural networks are often connected\nby simple curves of equally low loss [64, 54]. Moreover, the sharpness of minima has been analyzed\nand linked to generalization capabilities of neural networks, with the idea being that wide neural\nnetworks are easier to find and also yield robust neural networks [92, 34, 247]. However, this does\nnot appear to exclude sharp minima from generalizing well [53].\n178\nExercises\nExercise 12.6. In view of Definition 12.3, show that a local minimum of a differentiable function\nis contained in a spurious valley.\nExercise 12.7. Show that if there exists a continuous path α between a parameter θ1 and a global\nminimum θ2 such that ΛA,σ,S,L(α) is monotonically decreasing, then θ1 cannot be an element of a\nspurious valley.\nExercise 12.8. Find an example of a spurious valley for a simple architecture.\nHint: Use a single neuron ReLU neural network and observe that, for two networks one with\npositive and one with negative slope, every continuous path in parameter space that connects the\ntwo has to pass through a parameter corresponding to a constant function.\n179\nFigure 12.2: A collection of loss landscapes. In the left column are neural networks with ReLU\nactivation function, the right column shows loss landscapes of neural networks with the hyperbolic\ntangent activation function. All neural networks have five dimensional input, and one dimensional\noutput. Moreover, from top to bottom the hidden layers have sizes 1000, 20, 10, and the number\nof layers are 1, 4, 7.\n180\nChapter 13\nShape of neural network spaces\nAs we have seen in the previous chapter, the loss landscape of neural networks can be quite intricate\nand is typically not convex. In some sense, the reason for this is that we take the point of view\nof a map from the parameterization of a neural network. Let us consider a convex loss function\nL : R × R →R and a sample S = (xi, yi)m\ni=1 ∈(Rd × R)m.\nThen, for two neural networks Φ1, Φ2 and for α ∈(0, 1) it holds that\nbRS(αΦ1 + (1 −α)Φ2) = 1\nm\nm\nX\ni=1\nL(αΦ1(xi) + (1 −α)Φ2(xi), yi)\n≤1\nm\nm\nX\ni=1\nαL(Φ1(xi), yi) + (1 −α)L(Φ2(xi), yi)\n= α bRS(Φ1) + (1 −α) bRS(Φ2).\nHence, the empirical risk is convex when considered as a map depending on the neural network\nfunctions rather then the neural network parameters. A convex function does not have spurious\nminima or saddle points. As a result, the issues from the previous section are avoided if we take\nthe perspective of neural network sets.\nSo why do we not optimize over the sets of neural networks instead of the parameters? To\nunderstand this, we will now study the set of neural networks associated with a fixed architecture\nas a subset of other function spaces.\nWe start by investigating the realization map Rσ introduced in Definition 12.1. Concretely,\nwe show in Section 13.1, that if σ is Lipschitz, then the set of neural networks is the image of\nPN(A, ∞) under a locally Lipschitz map. We will use this fact to show in Section 13.2 that sets of\nneural networks are typically non-convex, and even have arbitrarily large holes. Finally, in Section\n13.3, we study the extent to which there exist best approximations to arbitrary functions, in the set\nof neural networks. We will demonstrate that the lack of best approximations causes the weights\nof neural networks to grow infinitely during training.\n13.1\nLipschitz parameterizations\nIn this section, we study the realization map Rσ. The main result is the following simplified version\nof [173, Proposition 4].\n181\nProposition 13.1. Let A = (d0, d1, . . . , dL+1) ∈NL+2, let σ: R →R be Cσ-Lipschitz continuous\nwith Cσ ≥1, let |σ(x)| ≤Cσ|x| for all x ∈R, and let B ≥1.\nThen, for all θ, θ′ ∈PN(A, B),\n∥Rσ(θ) −Rσ(θ′)∥L∞([−1,1]d0) ≤(2CσBdmax)LnA∥θ −θ′∥∞,\nwhere dmax = maxℓ=0,...,L+1 dℓand nA = PL\nℓ=0 dℓ+1(dℓ+ 1).\nProof. Let θ, θ′ ∈PN(A, B) and define δ := ∥θ −θ′∥∞. Repeatedly using the triangle inequality\nwe find a sequence (θj)nA\nj=0 such that θ0 = θ, θnA = θ′ , ∥θj −θj+1∥∞≤δ, and θj and θj+1 differ in\none entry only for all j = 0, . . . nA −1. We conclude that for all x ∈[−1, 1]d0\n∥Rσ(θ)(x) −Rσ(θ′)(x)∥∞≤\nnA−1\nX\nj=0\n∥Rσ(θj)(x) −Rσ(θj+1)(x)∥∞.\n(13.1.1)\nTo upper bound (13.1.1), we now only need to understand the effect of changing one weight in a\nneural network by δ.\nBefore we can complete the proof we need two auxiliary lemmas. The first of which holds under\nslightly weaker assumptions of Proposition 13.1.\nLemma 13.2. Under the assumptions of Proposition 13.1, but with B being allowed to be arbitrary\npositive, it holds for all Φ ∈N(σ; A, B)\n∥Φ(x) −Φ(x′)∥∞≤CL\nσ · (Bdmax)L+1∥x −x′∥∞\n(13.1.2)\nfor all x, x′ ∈Rd0.\nProof. We start with the case, where L = 1. Then, for (d0, d1, d2) = A, we have that\nΦ(x) = W(1)σ(W(0)x + b(0)) + b(1),\nfor certain W(0), W(1), b(0), b(1) with all entries bounded by B. As a consequence, we can estimate\n∥Φ(x) −Φ(x′)∥∞=\n\r\r\rW(1) \u0010\nσ(W(0)x + b(0)) −σ(W(0)x′ + b(0))\n\u0011\r\r\r\n∞\n≤d1B\n\r\r\rσ(W(0)x + b(0)) −σ(W(0)x′ + b(0))\n\r\r\r\n∞\n≤d1BCσ\n\r\r\rW(0)(x −x′)\n\r\r\r\n∞\n≤d1d0B2Cσ\n\r\rx −x′\r\r\n∞≤Cσ · (dmaxB)2 \r\rx −x′\r\r\n∞,\nwhere we used the Lipschitz property of σ and the fact that ∥Ax∥∞≤n maxi,j |Aij|∥x∥∞for every\nmatrix A = (Aij)m,n\ni=1,j=1 ∈Rm×n.\nThe induction step from L to L+1 follows similarly. This concludes the proof of the lemma.\n182\nLemma 13.3. Under the assumptions of Proposition 13.1 it holds that\n∥x(ℓ)∥∞≤(2CσBdmax)ℓ\nfor all x ∈[−1, 1]d0.\n(13.1.3)\nProof. Per Definitions (2.1.1b) and (2.1.1c), we have that for ℓ= 1, . . . , L + 1\n∥x(ℓ)∥∞≤Cσ\n\r\r\rW(ℓ−1)x(ℓ−1) + b(ℓ−1)\r\r\r\n∞\n≤CσBdmax∥x(ℓ−1)∥∞+ BCσ,\nwhere we used the triangle inequality and the estimate ∥Ax∥∞≤n maxi,j |Aij|∥x∥∞, which holds\nfor every matrix A ∈Rm×n. We obtain that\n∥x(ℓ)∥∞≤CσBdmax · (1 + ∥x(ℓ−1)∥∞)\n≤2CσBdmax · (max{1, ∥x(ℓ−1)∥∞}).\nResolving the recursive estimate of ∥x(ℓ)∥∞by 2CσBdmax(max{1, ∥x(ℓ−1)∥∞}), we conclude that\n∥x(ℓ)∥∞≤(2CσBdmax)ℓmax{1, ∥x(0)∥∞} = (2CσBdmax)ℓ.\nThis concludes the proof of the lemma.\nWe can now proceed with the proof of Proposition 13.1. Assume that θj+1 and θj differ only in\none entry. We assume this entry to be in the ℓth layer, and we start with the case ℓ< L. It holds\n|Rσ(θj)(x) −Rσ(θj+1)(x)| = |Φℓ(σ(W(ℓ)x(ℓ) + b(ℓ))) −Φℓ(σ(W(ℓ)x(ℓ) + b(ℓ)))|,\nwhere Φℓ∈N(σ; Aℓ, B) for Aℓ= (dℓ+1, . . . , dL+1) and (W(ℓ), b(ℓ)), (W\n(ℓ), b\n(ℓ)) differ in one entry\nonly.\nUsing the Lipschitz continuity of Φℓof Lemma 13.2, we have\n|Rσ(θj)(x) −Rσ(θj+1)(x)|\n≤CL−ℓ−1\nσ\n(Bdmax)L−ℓ|σ(W(ℓ)x(ℓ) + b(ℓ)) −σ(W(ℓ)x(ℓ) + b(ℓ))|\n≤CL−ℓ\nσ\n(Bdmax)L−ℓ∥W(ℓ)x(ℓ) + b(ℓ) −W(ℓ)x(ℓ) −b(ℓ)∥∞\n≤CL−ℓ\nσ\n(Bdmax)L−ℓδ max{1, ∥x(ℓ)∥∞},\nwhere δ := ∥θ −θ′∥max. Invoking Lemma (13.3), we conclude that\n|Rσ(θj)(x) −Rσ(θj+1)(x)| ≤(2CσBdmax)ℓCL−ℓ\nσ\n· (Bdmax)L−ℓδ\n≤(2CσBdmax)L∥θ −θ′∥max.\nFor the case ℓ= L, a similar estimate can be shown. Combining this with (13.1.1) yields the\nresult.\nUsing Proposition 13.1, we can now consider the set of neural networks with a fixed architec-\nture N(σ; A, ∞) as a subset of L∞([−1, 1]d0). What is more, is that N(σ; A, ∞) is the image of\nPN(A, ∞) under a locally Lipschitz map.\n183\n13.2\nConvexity of neural network spaces\nAs a first step towards understanding N(σ; A, ∞) as a subset of L∞([−1, 1]d0), we notice that it is\nstar-shaped with few centers. Let us first introduce the necessary terminology.\nDefinition 13.4. Let Z be a subset of a linear space. A point x ∈Z is called a center of Z if,\nfor every y ∈Z it holds that\n{tx + (1 −t)y | t ∈[0, 1]} ⊆Z.\nA set is called star-shaped if it has at least one center.\nThe following proposition follows directly from the definition of a neural network and is the\ncontent of Exercise 13.15.\nProposition 13.5. Let L ∈N and A = (d0, d1, . . . , dL+1) ∈NL+2 and σ: R →R.\nThen\nN(σ; A, ∞) is scaling invariant, i.e.\nfor every λ ∈R it holds that λf\n∈N(σ; A, ∞) if\nf ∈N(σ; A, ∞), and hence 0 ∈N(σ; A, ∞) is a center of N(σ; A, ∞).\nKnowing that N(σ; A, B) is star-shaped with center 0, we can also ask ourselves if N(σ; A, B)\nhas more than this one center. It is not hard to see that also every constant function is a center.\nThe following theorem, which corresponds to [173, Proposition C.4], yields an upper bound on the\nnumber of linearly independent centers.\nTheorem 13.6. Let L ∈N and A = (d0, d1, . . . , dL+1) ∈NL+2, and let σ : R →R be Lipschitz\ncontinuous.\nThen, N(σ; A, ∞) contains at most nA = PL\nℓ=0(dℓ+ 1)dℓ+1 linearly independent\ncenters.\nProof. Assume by contradiction, that there are functions (gi)nA+1\ni=1\n⊆N(σ; A, ∞) ⊆L∞([−1, 1]d0)\nthat are linearly independent and centers of N(σ; A, ∞).\nBy the Theorem of Hahn-Banach, there exist (g′\ni)nA+1\ni=1\n⊆(L∞([−1, 1]d0))′ such that g′\ni(gj) = δij,\nfor all i, j ∈{1, . . . , L + 1}. We define\nT : L∞([−1, 1]d0) →RnA+1,\ng 7→\n\n\n\n\n\ng′\n1(g)\ng′\n2(g)\n...\ng′\nnA+1(g)\n\n\n\n\n.\nSince T is continuous and linear, we have that T ◦Rσ is locally Lipschitz continuous by Proposition\n13.1. Moreover, since the (gi)nA+1\ni=1\nare linearly independent, we have that T(span((gi)nA+1\ni=1\n)) =\nRnA+1. We denote V := span((gi)nA+1\ni=1\n).\n184\nNext, we would like to establish that N(σ; A, ∞) ⊃V . Let g ∈V then\ng =\nnA+1\nX\nℓ=1\naℓgℓ,\nfor some a1, . . . , anA+1 ∈R. We show by induction that ˜g(m) := Pm\nℓ=1 aℓgℓ∈N(σ; A, ∞) for every\nm ≤nA + 1. This is obviously true for m = 1. Moreover, we have that ˜g(m+1) = am+1gm+1 + ˜g(m).\nHence, the induction step holds true if am+1 = 0. If am+1 ̸= 0, then we have that\neg(m+1) = 2am+1 ·\n\u00121\n2gm+1 +\n1\n2am+1\neg(m)\n\u0013\n.\n(13.2.1)\nBy the induction assumption eg(m) ∈N(σ; A, ∞) and hence by Proposition 13.5 eg(m)/(am+1) ∈\nN(σ; A, ∞). Additionally, since gm+1 is a center of N(σ; A, ∞), we have that 1\n2gm+1 +\n1\n2am+1 eg(m) ∈\nN(σ; A, ∞). By Proposition 13.5, we conclude that eg(m+1) ∈N(σ; A, ∞).\nThe induction shows that g ∈N(σ; A, ∞) and thus V ⊆N(σ; A, ∞).\nAs a consequence,\nT ◦Rσ(PN(A, ∞)) ⊇T(V ) = RnA+1.\nIt is a well known fact of basic analysis that for every n ∈N there does not exist a surjective\nand locally Lipschitz continuous map from Rn to Rn+1. We recall that nA = dim(PN(A, ∞)).\nThis yields the contradiction.\nFor a convex set X, the line between all two points of X is a subset of X. Hence, every point\nof a convex set is a center. This yields the following corollary.\nCorollary 13.7. Let A = (d0, d1, . . . , dL+1), let, and let σ : R →R be Lipschitz continuous.\nIf N(σ; A, ∞) contains more than nA = PL\nℓ=0(dℓ+ 1)dℓ+1 linearly independent functions, then\nN(σ; A, ∞) is not convex.\nCorollary 13.7 tells us that we cannot expect convex sets of neural networks, if the set of\nneural networks has many linearly independent elements.\nSets of neural networks contain for\neach f ∈N(σ; A, ∞) also all shifts of this function, i.e., f(· + b) for a b ∈Rd are elements of\nf ∈N(σ; A, ∞). For a set of functions, being shift invariant and having only finitely many linearly\nindependent functions at the same time, is a very restrictive condition. Indeed, it was shown in\n[173, Proposition C.6] that if N(σ; A, ∞) has only finitely many linearly independent functions and\nσ is differentiable in at least one point and has non-zero derivative there, then σ is necessarily a\npolynomial.\nWe conclude that the set of neural networks is in general non-convex and star-shaped with 0\nand constant functions being centers. One could visualize this set in 3D as in Figure 13.1.\nThe fact, that the neural network space is not convex, could also mean that it merely fails to\nbe convex at one point. For example R2 \\ {0} is not convex, but for an optimization algorithm this\nwould likely not pose a problem.\nWe will next observe that N(σ; A, ∞) does not have such a benign non-convexity and in fact,\nhas arbitrarily large holes.\nTo make this claim mathematically precise, we first introduce the notion of ε-convexity.\n185\nFigure 13.1: Sketch of the space of neural networks in 3D. The vertical axis corresponds to the\nconstant neural network functions, each of which is a center. The set of neural networks consists\nof many low-dimensional linear subspaces spanned by certain neural networks (Φ1, . . . , Φ6 in this\nsketch) and linear functions.\nBetween these low-dimensional subspaces, there is not always a\nstraight-line connection by Corollary 13.7 and Theorem 13.9.\nDefinition 13.8. For ε > 0, we say that a subset A of a normed vector space X is ε-convex if\nco(A) ⊆A + Bε(0),\nwhere co(A) denotes the convex hull of A and Bε(0) is an ε ball around 0 with respect to the norm\nof X.\nIntuitively speaking, a set that is convex when one fills up all holes smaller than ε is ε-convex.\nNow we show that there is no ε > 0 such that N(σ; A, ∞) is ε-convex.\nTheorem 13.9. Let L ∈N and A = (d0, d1, . . . , dL, 1) ∈NL+2. Let K ⊆Rd0 be compact and let\nσ ∈M, with M as in (3.1.1) and assume that σ is not a polynomial. Moreover, assume that there\nexists an open set, where σ is differentiable and not constant.\nIf there exists an ε > 0 such that N(σ; A, ∞) is ε-convex, then N(σ; A, ∞) is dense in C(K).\nProof. Step 1. We show that ε-convexity implies N(σ; A, ∞) to be convex. By Proposition 13.5,\nwe have that N(σ; A, ∞) is scaling invariant. This implies that co(N(σ; A, ∞)) is scaling invariant\n186\nas well. Hence, if there exists ε > 0 such that N(σ; A, ∞) is ε-convex, then for every ε′ > 0\nco(N(σ; A, ∞)) = ε′\nε co(N(σ; A, ∞)) ⊆ε′\nε (N(σ; A, ∞) + Bε(0))\n= N(σ; A, ∞) + Bε′(0).\nThis yields that N(σ; A, ∞) is ε′-convex.\nSince ε′ was arbitrary, we have that N(σ; A, ∞) is\nε-convex for all ε > 0.\nAs a consequence, we have that\nco(N(σ; A, ∞)) ⊆\n\\\nε>0\n(N(σ; A, ∞) + Bε(0))\n⊆\n\\\nε>0\n(N(σ; A, ∞) + Bε(0)) = N(σ; A, ∞).\nHence, co(N(σ; A, ∞)) ⊆N(σ; A, ∞) and, by the well-known fact that in every metric vector space\nco(A) ⊆co(A), we conclude that N(σ; A, ∞) is convex.\nStep 2. We show that N 1\nd (σ; 1) ⊆N(σ; A, ∞). If N(σ; A, ∞) is ε-convex, then by Step 1\nN(σ; A, ∞) is convex.\nThe scaling invariance of N(σ; A, ∞) then shows that N(σ; A, ∞) is a\nclosed linear subspace of C(K).\nNote that, by Proposition 3.16 for every w ∈Rd0 and b ∈R there exists a function f ∈\nN(σ; A, ∞) such that\nf(x) = σ(w⊤x + b)\nfor all x ∈K.\n(13.2.2)\nBy definition, every constant function is an element of N(σ; A, ∞). Since N(σ; A, ∞) is a subspace,\nthis implies that all constant functions are in N(σ; A, ∞).\nSince N(σ; A, ∞) is a closed vector space, this implies that for all n ∈N and all w(1)\n1 , . . . , w(1)\nn\n∈\nRd0, w(2)\n1 , . . . , w(2)\nn\n∈R, b(1)\n1 , . . . , b(1)\nn\n∈R, b(2) ∈R\nx 7→\nn\nX\ni=1\nw(2)\ni\nσ((w(1)\ni )⊤x + b(1)\ni ) + b(2) ∈N(σ; A, ∞).\n(13.2.3)\nStep 3. From (13.2.3), we conclude that N 1\nd (σ; 1) ⊆N(σ; A, ∞). In words, the whole set of\nshallow neural networks of arbitrary width is contained in the closure of the set of neural networks\nwith a fixed architecture. By Theorem 3.8, we have that N 1\nd (σ; 1) is dense in C(K), which yields\nthe result.\nFor any activation function of practical relevance, a set of neural networks with fixed architecture\nis not dense in C(K). This is only the case for very strange activation functions such as the one\ndiscussed in Subsection 3.2. Hence, Theorem 13.9 shows that in general, sets of neural networks of\nfixed architectures have arbitrarily large holes.\n13.3\nClosedness and best-approximation property\nThe non-convexity of the set of neural networks can have some serious consequences for the way\nwe think of the approximation or learning problem by neural networks.\n187\nConsider A = (d0, . . . , dL+1) ∈NL+2 and an activation function σ. Let H be a normed function\nspace on [−1, 1]d0 such that N(σ; A, ∞) ⊆H. For h ∈H we would like to find a neural network\nthat best approximates h, i.e. to find Φ ∈N(σ; A, ∞) such that\n∥Φ −h∥H =\ninf\nΦ∗∈N(σ;A,∞) ∥Φ∗−h∥H.\n(13.3.1)\nWe say that N(σ; A, ∞) ⊆H has\n• the best approximation property, if for all h ∈H there exists at least one Φ ∈N(σ; A, ∞)\nsuch that (13.3.1) holds,\n• the unique best approximation property, if for all h ∈H there exists exactly one\nΦ ∈N(σ; A, ∞) such that (13.3.1) holds,\n• the continuous selection property, if there exists a continuous function ϕ: H →N(σ; A, ∞)\nsuch that Φ = ϕ(h) satisfies (13.3.1) for all h ∈H.\nWe will see in the sequel, that, in the absence of the best approximation property, we will be able\nto prove that the learning problem necessarily requires the weights of the neural networks to tend\nto infinity, which may or may not be desirable in applications.\nMoreover, having a continuous selection procedure is desirable as it implies the existence of a\nstable selection algorithm; that is, an algorithm which, for similar problems yields similar neural\nnetworks satisfying (13.3.1).\nBelow, we will study the properties above for Lp spaces, p ∈[1, ∞).\nAs we will see, neu-\nral network classes typically neither satisfy the continuous selection nor the best approximation\nproperty.\n13.3.1\nContinuous selection\nAs shown in [111], neural network spaces essentially never admit the continuous selection property.\nTo give the argument, we first recall the following result from [111, Theorem 3.4] without proof.\nTheorem 13.10. Let p ∈(1, ∞). Every subset of Lp([−1, 1]d0) with the unique best approximation\nproperty is convex.\nThis allows to show the next proposition.\nProposition 13.11. Let L ∈N, A = (d0, d1, . . . , dL+1) ∈NL+2, let σ : R →R be Lipschitz\ncontinuous and not a polynomial, and let p ∈(1, ∞).\nThen, N(σ; A, ∞) ⊆Lp([−1, 1]d0) does not have the continuous selection property.\n188\nProof. We observe from Theorem 13.6 and the discussion below, that under the assumptions of\nthis result, N(σ; A, ∞) is not convex.\nWe conclude that N(σ; A, ∞) does not have the unique best approximation property. Moreover,\nif the set N(σ; A, ∞) does not have the best approximation property, then it is obvious that it\ncannot have continuous selection. Thus, we can assume without loss of generality, that N(σ; A, ∞)\nhas the best approximation property and there exists a point h ∈Lp([−1, 1]d0) and two different\nΦ1,Φ2 such that\n∥Φ1 −h∥Lp = ∥Φ2 −h∥Lp =\ninf\nΦ∗∈N(σ;A,∞) ∥Φ∗−h∥Lp.\n(13.3.2)\nNote that (13.3.2) implies that h ̸∈N(σ; A, ∞).\nLet us consider the following function:\n[−1, 1] ∋λ 7→P(λ) =\n\u001a (1 + λ)h −λΦ1\nfor λ ≤0,\n(1 −λ)h + λΦ2\nfor λ ≥0.\nIt is clear that P(λ) is a continuous path in Lp. Moreover, for λ ∈(−1, 0)\n∥Φ1 −P(λ)∥Lp = (1 + λ)∥Φ1 −h∥Lp.\nAssume towards a contradiction, that there exists Φ∗̸= Φ1 such that for λ ∈(−1, 0)\n∥Φ∗−P(λ)∥Lp ≤∥Φ1 −P(λ)∥Lp.\nThen\n∥Φ∗−h∥Lp ≤∥Φ∗−P(λ)∥Lp + ∥P(λ) −h∥Lp\n≤∥Φ1 −P(λ)∥Lp + ∥P(λ) −h∥Lp\n= (1 + λ)∥Φ1 −h∥Lp + |λ|∥Φ1 −h∥Lp = ∥Φ1 −h∥Lp.\n(13.3.3)\nSince Φ1 is a best approximation to h this implies that every inequality in the estimate above is an\nequality. Hence, we have that\n∥Φ∗−h∥Lp = ∥Φ∗−P(λ)∥Lp + ∥P(λ) −h∥Lp.\nHowever, in a strictly convex space like Lp([−1, 1]d0) for p > 1 this implies that\nΦ∗−P(λ) = c · (P(λ) −h)\nfor a constant c ̸= 0. This yields that\nΦ∗= h + (c + 1)λ · (h −Φ1)\nand plugging into (13.3.3) yields |(c + 1)λ| = 1. If (c + 1)λ = −1, then we have Φ∗= Φ1 which\nproduces a contradiction. If (c + 1)λ = 1, then\n∥Φ∗−P(λ)∥Lp = ∥2h −Φ1 −(1 + λ)h + λΦ1∥Lp\n= ∥(1 −λ)h −(1 −λ)Φ1∥Lp > ∥P(λ) −Φ1∥Lp,\nwhich is another contradiction.\nHence, for every λ < 0 we have that Φ1 is the unique minimizer to P(λ) in N(σ; A, ∞). The same\nargument holds for λ > 0 and Φ2. We conclude that for every selection function ϕ: Lp([−1, 1]d0) →\nN(σ; A, ∞) such that Φ = ϕ(h) satisfies (13.3.1) for all h ∈Lp([−1, 1]d0) it holds that\nlim\nλ↓0 ϕ(P(λ)) = Φ2 ̸= Φ1 = lim\nλ↑0 ϕ(P(λ)).\nAs a consequence, ϕ is not continuous, which shows the result.\n189\n13.3.2\nExistence of best approximations\nWe have seen in Proposition 13.11 that under very mild assumptions, the continuous selection prop-\nerty cannot hold. Moreover, the next result shows that in many cases, also the best approximation\nproperty fails to be satisfied. We provide below a simplified version of [173, Theorem 3.1]. We also\nrefer to [69] for earlier work on this problem.\nProposition 13.12. Let A = (1, 2, 1) and let σ : R →R be Lipschitz continuous. Additionally\nassume that there exist r > 0 and α′ ̸= α such that σ is differentiable for all |x| > r and σ′(x) →α\nfor x →∞, σ′(x) →α′ for x →−∞.\nThen, there exists a sequence in N(σ; A, ∞) which converges in Lp([−1, 1]), for every p ∈(1, ∞),\nand the limit of this sequence is discontinuous. In particular, the limit of the sequence does not lie\nin N(σ; A′, ∞) for any A′.\nProof. For all n ∈N let\nfn(x) = σ(nx + 1) −σ(nx)\nfor all x ∈R.\nThen fn can be written as a neural network with architecture (σ; 1, 2, 1), i.e., A = (1, 2, 1). More-\nover, for x > 0 we observe with the fundamental theorem of calculus and using integration by\nsubstitution that\nfn(x) =\nZ x+1/n\nx\nnσ′(nz)dz =\nZ nx+1\nnx\nσ′(z)dz.\n(13.3.4)\nIt is not hard to see that the right hand side of (13.3.4) converges to α for n →∞.\nSimilarly, for x < 0, we observe that fn(x) converges to α′ for n →∞. We conclude that\nfn →α1R+ + α′1R−\nalmost everywhere as n →∞.\nSince σ is Lipschitz continuous, we have that fn is bounded.\nTherefore, we conclude that fn →α1R+ + α′1R−in Lp for all p ∈[1, ∞) by the dominated\nconvergence theorem.\nThere is a straight-forward extension of Proposition 13.12 to arbitrary architectures, that will\nbe the content of Exercises 13.16 and 13.17.\nRemark 13.13. The proof of Theorem 13.12 does not extend to the L∞norm. This, of course, does\nnot mean that generally N(σ; A, ∞) is a closed set in L∞([−1, 1]d0). In fact, almost all activation\nfunctions used in practice still give rise to non-closed neural network sets, see [173, Theorem 3.3].\nHowever, there is one notable exception. For the ReLU activation function, it can be shown that\nN(σReLU; A, ∞) is a closed set in L∞([−1, 1]d0) if A has only one hidden layer. The closedness of\ndeep ReLU spaces in L∞is an open problem.\n190\n13.3.3\nExploding weights phenomenon\nFinally, we discuss one of the consequences of the non-existence of best approximations of Propo-\nsition 13.12.\nConsider a regression problem, where we aim to learn a function f using neural networks with\na fixed architecture N(A; σ, ∞). As discussed in the Chapters 10 and 11, we wish to produce a\nsequence of neural networks (Φn)∞\nn=1 such that the risk defined in (1.2.4) converges to 0. If the loss\nL is the squared loss, µ is a probability measure on [−1, 1]d0, and the data is given by (x, f(x)) for\nx ∼µ, then\nR(Φn) = ∥Φn −f∥2\nL2([−1,1]d0,µ)\n=\nZ\n[−1,1]d0\n|Φn(x) −f(x)|2dµ(x) →0\nfor n →∞.\n(13.3.5)\nAccording to Proposition 13.12, for a given A, and an activation function σ, it is possible that\n(13.3.5) holds, but f ̸∈N(σ; A, ∞). The following result shows that in this situation, the weights\nof Φn diverge.\nProposition 13.14. Let A = (d0, d1, . . . , dL+1) ∈NL+2, let σ: R →R be Lipschitz continuous\nwith Cσ ≥1, and |σ(x)| ≤Cσ|x| for all x ∈R, and let µ be a measure on [−1, 1]d0.\nAssume that there exists a sequence Φn ∈N(σ; A, ∞) and f ∈L2([−1, 1]d0, µ) \\ N(σ; A, ∞)\nsuch that\n∥Φn −f∥2\nL2([−1,1]d0,µ) →0.\n(13.3.6)\nThen\nlim sup\nn→∞max\nn\n∥W (ℓ)\nn ∥∞, ∥b(ℓ)\nn ∥∞\n\f\f\f ℓ= 0, . . . L\no\n= ∞.\n(13.3.7)\nProof. We assume towards a contradiction that the left-hand side of (13.3.7) is finite. As a result,\nthere exists C > 0 such that Φn ∈N(σ; A, C) for all n ∈N.\nBy Proposition 13.1, we conclude that N(σ; A, C) is the image of a compact set under a continu-\nous map and hence is itself a compact set in L2([−1, 1]d0, µ). In particular, we have that N(σ; A, C)\nis closed. Hence, (13.3.6) implies f ∈N(σ; A, C). This gives a contradiction.\nProposition 13.14 can be extended to all f for which there is no best approximation in N(σ; A, ∞),\nsee Exercise 13.18. The results imply that for functions we wish to learn that lack a best approxima-\ntion within a neural network set, we must expect the weights of the approximating neural networks\nto grow to infinity. This can be undesirable because, as we will see in the following sections on\ngeneralization, a bounded parameter space facilitates many generalization bounds.\nBibliography and further reading\nThe properties of neural network sets were first studied with a focus on the continuous approxima-\ntion property in [111, 113, 112] and [69]. The results in [111, 112, 113] already use the non-convexity\n191\nof sets of shallow neural networks. The results on convexity and closedness presented in this chapter\nfollow mostly the arguments of [173]. Similar results were also derived for other norms in [139].\n192\nExercises\nExercise 13.15. Prove Proposition 13.5.\nExercise 13.16. Extend Proposition 13.12 to A = (d0, d1, 1) for arbitrary d0, d1 ∈N, d1 ≥2.\nExercise 13.17. Use Proposition 3.16, to extend Proposition 13.12 to arbitrary depth.\nExercise 13.18. Extend Proposition 13.14 to functions f for which there is no best-approximation\nin N(σ; A, ∞). To do this, replace (13.3.6) by\n∥Φn −f∥2\nL2 →\ninf\nΦ∈N(σ;A,∞) ∥Φ −f∥2\nL2.\n193\nChapter 14\nGeneralization properties of deep\nneural networks\nAs discussed in the introduction in Section 1.2, we generally learn based on a finite data set. For\nexample, given data (xi, yi)m\ni=1, we try to find a network Φ that satisfies Φ(xi) = yi for i = 1, . . . , m.\nThe field of generalization is concerned with how well such Φ performs on unseen data, which refers\nto any x outside of training data {x1, . . . , xm}. In this chapter we discuss generalization through\nthe use of covering numbers.\nIn Sections 14.1 and 14.2 we revisit and formalize the general setup of learning and empirical risk\nminimization in a general context. Although some notions introduced in these sections have already\nappeared in the previous chapters, we reintroduce them here for a more coherent presentation. In\nSections 14.3-14.5, we first discuss the concepts of generalization bounds and covering numbers,\nand then apply these arguments specifically to neural networks. In Section 14.6 we explore the\nso-called “approximation-complexity trade-off”, and finally in Sections 14.7-14.8 we introduce the\n“VC dimension” and give some implications for classes of neural networks.\n14.1\nLearning setup\nA general learning problem [148, 212, 43] requires a feature space X and a label space Y , which\nwe assume throughout to be measurable spaces. We observe joint data pairs (xi, yi)m\ni=1 ⊆X×Y , and\naim to identify a connection between the x and y variables. Specifically, we assume a relationship\nbetween features x and labels y modeled by a probability distribution D over X ×Y , that generated\nthe observed data (xi, yi)m\ni=1. While this distribution is unknown, our goal is to extract information\nfrom it, so that we can make possibly good predictions of y for a given x.\nImportantly, the\nrelationship between x and y need not be deterministic.\nTo make these concepts more concrete, we next present an example that will serve as the running\nexample throughout this chapter. This example is of high relevance for many mathematicians,\nas ensuring a steady supply of high-quality coffee is essential for maximizing the output of our\nmathematical work.\nExample 14.1 (Coffee Quality). Our goal is to determine the quality of different coffees. To this\nend we model the quality as a number in\nY =\nn 0\n10, . . . , 10\n10\no\n,\n194\nFigure 14.1: Collection of coffee data. The last row lacks a “Quality” label. Our aim is to predict\nthe label without the need for an (expensive) taste test.\nwith higher numbers indicating better quality. Let us assume that our subjective assessment of\nquality of coffee is related to six features: “Acidity”, “Caffeine content”, “Price”, “Aftertaste”,\n“Roast level”, and “Origin”. The feature space X thus corresponds to the set of six-tuples describing\nthese attributes, which can be either numeric or categorical (see Figure 14.1).\nWe aim to understand the relationship between elements of X and elements of Y , but we can\nneither afford, nor do we have the time to taste all the coffees in the world. Instead, we can sample\nsome coffees, taste them, and grow our database accordingly as depicted in Figure 14.1. This way\nwe obtain samples of pairs in X × Y . The distribution D from which they are drawn depends on\nvarious external factors. For instance, we might have avoided particularly cheap coffees, believing\nthem to be inferior.\nAs a result they do not occur in our database.\nMoreover, if a colleague\ncontributes to our database, he might have tried the same brand and arrived at a different rating.\nIn this case, the quality label is not deterministic anymore.\nBased on our database, we wish to predict the quality of an untasted coffee. Before proceeding,\nwe first formalize what it means to be a “good” prediction.\nCharacterizing how good a predictor is requires a notion of discrepancy in the label space. This\nis the purpose of the so-called loss function, which is a measurable mapping L: Y × Y →R+.\nDefinition 14.2. Let L: Y × Y →R+ be a loss function and let D be a distribution on X × Y .\nFor a measurable function h: X →Y we call\nR(h) = E(x,y)∼D [L(h(x), y)]\nthe (population) risk of h.\nBased on the risk, we can now formalize what we consider a good predictor. The best predictor\nis one such that its risk is as close as possible to the smallest that any function can achieve. More\nprecisely, we would like a risk that is close to the so-called Bayes risk\nR∗:=\ninf\nh: X→Y R(h),\n(14.1.1)\nwhere the infimum is taken over all h such that h : X →Y is measurable.\n195\nExample 14.3 (Loss functions). The choice of a loss function L usually depends on the application.\nFor a regression problem, i.e., a learning problem where Y is a non-discrete subset of a Euclidean\nspace, a common choice is the square loss L2(y, y′) = ∥y −y′∥2.\nFor binary classification problems, i.e. when Y is a discrete set of cardinality two, the “0 −1\nloss”\nL0−1(y, y′) =\n(\n1\ny ̸= y′\n0\ny = y′\nis a common choice.\nAnother frequently used loss for binary classification, especially when we want to predict prob-\nabilities (i.e., if Y = [0, 1] but all labels are binary), is the binary cross-entropy loss\nLce(y, y′) = −(y log(y′) + (1 −y) log(1 −y′)).\nIn contrast to the 0 −1 loss, the cross-entropy loss is differentiable, which is desirable in deep\nlearning as we saw in Chapter 10.\nIn the coffee quality prediction problem, the quality is given as a fraction of the form k/10\nfor k = 0, . . . , 10. While this is a discrete set, it makes sense to more heavily penalize predictions\nthat are wrong by a larger amount. For example, predicting 4/10 instead of 8/10 should produce\na higher loss than predicting 7/10. Hence, we would not use the 0 −1 loss but, for example, the\nsquare loss.\nHow do we find a function h: X →Y with a risk that is as close as possible to the Bayes risk?\nWe will introduce a procedure to tackle this task in the next section.\n14.2\nEmpirical risk minimization\nFinding a minimizer of the risk constitutes a considerable challenge. First, we cannot search through\nall measurable functions. Therefore, we need to restrict ourselves to a specific set H ⊆{h : X →Y }\ncalled the hypothesis set. In the following, this set will be some set of neural networks. Second,\nwe are faced with the problem that we cannot evaluate R(h) for non-trivial loss functions, because\nthe distribution D is unknown. To approximate the risk, we will assume access to an i.i.d. sample\nof m observations drawn from D. This is precisely the situation described in the coffee quality\nexample of Figure 14.1, where m = 6 coffees were sampled.1 So for a given hypothesis h we can\ncheck how well it performs on our sampled data. We call the error on the sample the empirical\nrisk.\nDefinition 14.4. Let m ∈N, let L: Y ×Y →R be a loss function and let S = (xi, yi)m\ni=1 ∈(X×Y )m\nbe a sample. For h: X →Y , we call\nbRS(h) = 1\nm\nm\nX\ni=1\nL(h(xi), yi)\nthe empirical risk of h.\n1In practice, the assumption of independence of the samples is often unclear and typically not satisfied.\nFor\ninstance, the selection of the six previously tested coffees might be influenced by external factors such as personal\npreferences or availability at the local store, which introduce bias into the dataset.\n196\nIf the sample S is drawn i.i.d. according to D, then we immediately see from the linearity\nof the expected value that bRS(h) is an unbiased estimator of R(h), i.e., ES∼Dm[ bRS(h)] = R(h).\nMoreover, the weak law of large numbers states that the sample mean of an i.i.d. sequence of\nintegrable random variables converges to the expected value in probability. Hence, there is some\nhope that, at least for large m ∈N, minimizing the empirical risk instead of the population risk\nmight lead to a good hypothesis. We formalize this approach in the next definition.\nDefinition 14.5. Let H ⊆{h: X →Y } be a hypothesis set. Let m ∈N, let L: Y × Y →R be a\nloss function and let S = (xi, yi)m\ni=1 ∈(X × Y )m be a sample. We call a function hS such that\nbRS(hS) = inf\nh∈H\nbRS(h)\n(14.2.1)\nan empirical risk minimizer.\nFrom a generalization perspective, deep learning is empirical risk minimization over sets of\nneural networks. The question we want to address next is how effective this approach is at producing\nhypotheses that achieve a risk close to the Bayes risk.\nLet H be some hypothesis set, such that an empirical risk minimizer hS exists for all S ∈\n(X × Y )m; see Exercise 14.25 for an explanation of why this is a reasonable assumption. Moreover,\nlet h∗∈H be arbitrary. Then\nR(hS) −R∗= R(hS) −bRS(hS) + bRS(hS) −R∗\n(14.2.2)\n≤|R(hS) −bRS(hS)| + bRS(h∗) −R∗\n≤2 sup\nh∈H\n|R(h) −bRS(h)| + R(h∗) −R∗,\nwhere in the first inequality we used that hS is the empirical risk minimizer. By taking the infimum\nover all h∗, we conclude that\nR(hS) −R∗≤2 sup\nh∈H\n|R(h) −bRS(h)| + inf\nh∈H R(h) −R∗\n=: 2εgen + εapprox.\n(14.2.3)\nSimilarly, considering only (14.2.2), yields that\nR(hS) ≤sup\nh∈H\n|R(h) −bRS(h)| + inf\nh∈H\nbRS(h)\n=: εgen + εint.\n(14.2.4)\nHow to choose H to reduce the approximation error εapprox or the interpolation error εint\nwas discussed at length in the previous chapters. The final piece is to figure out how to bound the\ngeneralization error suph∈H |R(h) −bRS(h)|. This will be discussed in the sections below.\n14.3\nGeneralization bounds\nWe have seen that one aspect of successful learning is to bound the generalization error εgen in\n(14.2.3). Let us first formally describe this problem.\n197\nDefinition 14.6 (Generalization bound). Let H ⊆{h: X →Y } be a hypothesis set, and let\nL: Y × Y →R be a loss function. Let κ: (0, 1) × N →R+ be such that for every δ ∈(0, 1) holds\nκ(δ, m) →0 for m →∞. We call κ a generalization bound for H if for every distribution D on\nX × Y , every m ∈N and every δ ∈(0, 1), it holds with probability at least 1 −δ over the random\nsample S ∼Dm that\nsup\nh∈H\n|R(h) −bRS(h)| ≤κ(δ, m).\nRemark 14.7. For a generalization bound κ it holds that\nP\nh\f\f\fR(hS) −bRS(hS)\n\f\f\f ≤ε\ni\n≥1 −δ\nas soon as m is so large that κ(δ, m) ≤ε. If there exists an empirical risk minimizer hS such that\nbRS(hS) = 0, then with high probability the empirical risk minimizer will also have a small risk\nR(hS). Empirical risk minimization is often referred to as a “PAC” algorithm, which stands for\nprobably (δ) approximately correct (ε).\nDefinition 14.6 requires the upper bound κ on the discrepancy between the empirical risk and\nthe risk to be independent from the distribution D. Why should this be possible? After all, we could\nhave an underlying distribution that is not uniform and hence, certain data points could appear\nvery rarely in the sample. As a result, it should be very hard to produce a correct prediction\nfor such points. At first sight, this suggests that non-uniform distributions should be much more\nchallenging than uniform distributions. This intuition is incorrect, as the following argument based\non Example 14.1 demonstrates.\nExample 14.8 (Generalization in the coffee quality problem). In Example 14.1, the underlying\ndistribution describes both our process of choosing coffees and the relation between the attributes\nand the quality. Suppose we do not enjoy drinking coffee that costs less than 1€. Consequently, we\ndo not have a single sample of such coffee in the dataset, and therefore we have no chance about\nlearning the quality of cheap coffees.\nHowever, the absence of coffee samples costing less than 1€ in our dataset is due to our general\navoidance of such coffee. As a result, we run a low risk of incorrectly classifying the quality of a\ncoffee that is cheaper than 1€, since it is unlikely that we will choose such a coffee in the future.\nTo establish generalization bounds, we use stochastic tools that guarantee that the empirical\nrisk converges to the true risk as the sample size increases. This is typically achieved through\nconcentration inequalities. One of the simplest and most well-known is Hoeffding’s inequality, see\nTheorem A.24. We will now apply Hoeffding’s inequality to obtain a first generalization bound.\nThis generalization bound is well-known and can be found in many textbooks on machine learning,\ne.g., [148, 212]. Although the result does not yet encompass neural networks, it forms the basis for\na similar result applicable to neural networks, as we discuss subsequently.\n198\nProposition 14.9 (Finite hypothesis set). Let H ⊆{h: X 7→Y } be a finite hypothesis set. Let\nL: Y × Y →R be such that L(Y × Y ) ⊆[c1, c2] with c2 −c1 = C > 0.\nThen, for every m ∈N and every distribution D on X ×Y it holds with probability at least 1−δ\nover the sample S ∼Dm that\nsup\nh∈H\n|R(h) −bRS(h)| ≤C\nr\nlog(|H|) + log(2/δ)\n2m\n.\nProof. Let H = {h1, . . . , hn}. Then it holds by a union bound that\nP\nh\n∃hi ∈H: |R(hi) −bRS(hi)| > ε\ni\n≤\nn\nX\ni=1\nP\nh\n|R(hi) −bRS(hi)| > ε\ni\n.\nNote that bRS(hi) is the mean of independent random variables which take their values almost surely\nin [0, C]. Additionally, R(hi) is the expectation of bRS(hi). The proof can therefore be finished by\napplying Theorem A.24. This will be addressed in Exercise 14.26.\nConsider now a non-finite set of neural networks H, and assume that it can be covered by a\nfinite set of (small) balls. Applying Proposition 14.9 to the centers of these balls, then allows to\nderive a similar bound as in the proposition for H. This intuitive argument will be made rigorous\nin the following section.\n14.4\nGeneralization bounds from covering numbers\nTo derive a generalization bound for classes of neural networks, we start by introducing the notion\nof covering numbers.\nDefinition 14.10. Let A be a relatively compact subset of a metric space (X, d). For ε > 0, we\ncall\nG(A, ε, (X, d)) := min\n(\nm ∈N\n\f\f\f\f\f ∃(xi)m\ni=1 ⊆X s.t.\nm\n[\ni=1\nBε(xi) ⊃A\n)\n,\nwhere Bε(x) = {z ∈X | d(z, x) ≤ε}, the ε-covering number of A in X. In case X or d are clear\nfrom context, we also write G(A, ε, d) or G(A, ε, X) instead of G(A, ε, (X, d)).\nA visualization of Definition 14.10 is given in Figure 14.2. As we will see, it is possible to upper\nbound the ε-covering numbers of neural networks as a subset of L∞([0, 1]d), assuming the weights\nare confined to a fixed bounded set. The precise estimates are postponed to Section 14.5. Before\nthat, let us show how a finite covering number facilitates a generalization bound. We only consider\nEuclidean feature spaces X in the following result. A more general version could be easily derived.\n199\nε\nFigure 14.2: Illustration of the concept of covering numbers of Definition 14.10. The shaded set\nA ⊆R2 is covered by sixteen Euclidean balls of radius ε. Therefore, G(A, ε, R2) ≤16.\nTheorem 14.11. Let CY , CL > 0 and α > 0. Let Y ⊆[−CY , CY ], X ⊆Rd for some d ∈N, and\nH ⊆{h: X →Y }. Further, let L: Y × Y →R be CL-Lipschitz.\nThen, for every distribution D on X ×Y and every m ∈N it holds with probability at least 1−δ\nover the sample S ∼Dm that for all h ∈H\n|R(h) −bRS(h)| ≤4CY CL\nr\nlog(G(H, m−α, L∞(X))) + log(2/δ)\nm\n+ 2CL\nmα .\nProof. Let\nM = G(H, m−α, L∞(X))\n(14.4.1)\nand let HM = (hi)M\ni=1 ⊆H be such that for every h ∈H there exists hi ∈HM with ∥h−hi∥L∞(X) ≤\n1/mα. The existence of HM follows by Definition 14.10.\nFix for the moment such h ∈H and hi ∈HM. By the reverse and normal triangle inequalities,\nwe have\n|R(h) −bRS(h)| −|R(hi) −bRS(hi)| ≤|R(h) −R(hi)| + | bRS(h) −bRS(hi)|.\nMoreover, from the monotonicity of the expected value and the Lipschitz property of L it follows\nthat\n|R(h) −R(hi)| ≤E|L(h(x), y) −L(hi(x), y)|\n≤CLE|h(x) −hi(x)| ≤CL\nmα .\nA similar estimate yields | bRS(h) −bRS(hi)| ≤CL/mα.\n200\nWe thus conclude that for every ε > 0\nPS∼Dm\nh\n∃h ∈H: |R(h) −bRS(h)| ≥ε\ni\n≤PS∼Dm\n\u0014\n∃hi ∈HM : |R(hi) −bRS(hi)| ≥ε −2CL\nmα\n\u0015\n.\n(14.4.2)\nFrom Proposition 14.9, we know that for ε > 0 and δ ∈(0, 1)\nPS∼Dm\n\u0014\n∃hi ∈HM : |R(hi) −bRS(hi)| ≥ε −2CL\nmα\n\u0015\n≤δ\n(14.4.3)\nas long as\nε −2CL\nmα > C\nr\nlog(M) + log(2/δ)\n2m\n,\nwhere C is such that L(Y × Y ) ⊆[c1, c2] with c2 −c1 ≤C. By the Lipschitz property of L we can\nchoose C = 2\n√\n2CLCY .\nTherefore, the definition of M in (14.4.1) together with (14.4.2) and (14.4.3) give that with\nprobability at least 1 −δ it holds for all h ∈H\n|R(h) −bRS(h)| ≤2\n√\n2CLCY\nr\nlog(G(H, m−α, L∞)) + log(2/δ)\n2m\n+ 2CL\nmα .\nThis concludes the proof.\n14.5\nCovering numbers of deep neural networks\nWe have seen in Theorem 14.11, estimating L∞-covering numbers is crucial for understanding the\ngeneralization error. How can we determine these covering numbers? The set of neural networks of\na fixed architecture can be a quite complex set (see Chapter 13), so it is not immediately clear how\nto cover it with balls, let alone know the number of required balls. The following lemma suggest a\nsimpler approach.\nLemma 14.12. Let X1, X2 be two metric spaces and let f : X1 →X2 be Lipschitz continuous with\nLipschitz constant CLip. For every relatively compact A ⊆X1 it holds that for all ε > 0\nG(f(A), CLipε, X2) ≤G(A, ε, X1).\nThe proof of Lemma 14.12 is left as an exercise. If we can represent the set of neural networks\nas the image under the Lipschitz map of another set with known covering numbers, then Lemma\n14.12 gives a direct way to bound the covering number of the neural network class.\nConveniently, we have already observed in Proposition 13.1, that the set of neural networks is\nthe image of PN(A, B) as in Definition 12.1 under the Lipschitz continuous realization map Rσ. It\nthus suffices to establish the ε-covering number of PN(A, B) or equivalently of [−B, B]nA. Then,\nusing the Lipschitz property of Rσ that holds by Proposition 13.1, we can apply Lemma 14.12 to\nfind the covering numbers of N(σ; A, B). This idea is depicted in Figure 14.3.\n201\nRσ\nFigure 14.3: Illustration of the main idea to deduce covering numbers of neural network spaces.\nPoints θ ∈R2 in parameter space in the left figure correspond to functions Rσ(θ) in the right figure\n(with matching colors). By Lemma 14.12, a covering of the parameter space on the left translates\nto a covering of the function space on the right.\nProposition 14.13. Let B, ε > 0 and q ∈N. Then\nG([−B, B]q, ε, (Rq, ∥· ∥∞)) ≤⌈B/ε⌉q.\nProof. We start with the one-dimensional case q = 1. We choose k = ⌊B/ε⌋\nx0 = −B + ε and xj = xj−1 + 2ε for j = 1, . . . , k −1.\nIt is clear that all points between −B and xk−1 have distance at most ε to one of the xj. Also,\nxk−1 = −B + ε + 2(k −1)ε ≥B −ε. We conclude that G([−B, B], ε, R) ≤⌈B/ε⌉. Set Xk :=\n{x0, . . . , xk−1}.\nFor arbitrary q, we observe that for every x ∈[−B, B]q there is an element in Xq\nk = Nq\nj=1 Xk\nwith ∥· ∥∞distance less than ε. Clearly, |Xq\nk| = ⌈B/ε⌉q, which completes the proof.\nHaving established a covering number for [−B, B]nA and hence PN(A, B), we can now estimate\nthe covering numbers of deep neural networks by combining Lemma 14.12 and Propositions 13.1\nand 14.13 .\nTheorem 14.14. Let A = (d0, d1, . . . , dL+1) ∈NL+2, let σ: R →R be Cσ-Lipschitz continuous\nwith Cσ ≥1, let |σ(x)| ≤Cσ|x| for all x ∈R, and let B ≥1. Then\nG(N(σ; A, B), ε, L∞([0, 1]d0)) ≤G([−B, B]nA, ε/(2CσBdmax)L, (RnA, ∥· ∥∞))\n≤⌈nA/ε⌉nA⌈2CσBdmax⌉nAL.\n202\nWe end this section, by applying the previous theorem to the generalization bound of Theorem\n14.11 with α = 1/2. To simplify the analysis, we restrict the discussion to neural networks with\nrange [−1, 1]. To this end, denote\nN ∗(σ; A, B) :=\n\b\nΦ ∈N(σ; A, B)\n\f\f\nΦ(x) ∈[−1, 1] for all x ∈[0, 1]d0\t\n.\n(14.5.1)\nSince N ∗(σ; A, B) ⊆N(σ; A, B) we can bound the covering numbers of N ∗(σ; A, B) by those of\nN(σ; A, B). This yields the following result.\nTheorem 14.15. Let CL > 0 and let L: [−1, 1]×[−1, 1] →R be CL-Lipschitz continuous. Further,\nlet A = (d0, d1, . . . , dL+1) ∈NL+2, let σ: R →R be Cσ-Lipschitz continuous with Cσ ≥1, and\n|σ(x)| ≤Cσ|x| for all x ∈R, and let B ≥1.\nThen, for every m ∈N, and every distribution D on X ×[−1, 1] it holds with probability at least\n1 −δ over S ∼Dm that for all Φ ∈N ∗(σ; A, B)\n|R(Φ) −bRS(Φ)| ≤4CL\nr\nnA log(⌈nA\n√m⌉) + LnA log(⌈2CσBdmax⌉) + log(2/δ)\nm\n+ 2CL\n√m .\n14.6\nThe approximation-complexity trade-off\nWe recall the decomposition of the error in (14.2.3)\nR(hS) −R∗≤2εgen + εapprox,\nwhere R∗is the Bayes risk defined in (14.1.1).\nWe make the following observations about the\napproximation error εapprox and generalization error εgen in the context of neural network based\nlearning:\n• Scaling of generalization error: By Theorem 14.15, for a hypothesis class H of neural networks\nwith nA weights and L layers, and for sample of size m ∈N, the generalization error εgen\nessentially scales like\nεgen = O(\np\n(nA log(nAm) + LnA log(nA))/m)\nas m →∞.\n• Scaling of approximation error: Assume there exists h∗such that R(h∗) = R∗, and let the\nloss function L be Lipschitz continuous in the first coordinate. Then\nεapprox = inf\nh∈H R(h) −R(h∗) = inf\nh∈H E(x,y)∼D[L(h(x), y) −L(h∗(x), y)]\n≤C inf\nh∈H ∥h −h∗∥L∞,\n203\nfor some constant C > 0.\nWe have seen in Chapters 5 and 7 that if we choose H as a\nset of neural networks with size nA and L layers, then, for appropriate activation functions,\ninfh∈H ∥h −h∗∥L∞behaves like nA−r if, e.g., h∗is a d-dimensional s-H¨older regular function\nand r = s/d (Theorem 5.22), or h∗∈Ck,s([0, 1]d) and r < (k + s)/d (Theorem 7.7).\nBy these considerations, we conclude that for an empirical risk minimizer ΦS from a set of neural\nnetworks with nA weights and L layers, it holds that\nR(ΦS) −R∗≤O(\np\n(nA log(m) + LnA log(nA))/m) + O(nA−r),\n(14.6.1)\nfor m →∞and for some r depending on the regularity of h∗. Note that, enlarging the neural\nnetwork set, i.e., increasing nA has two effects: The term associated to approximation decreases,\nand the term associated to generalization increases. This trade-off is known as approximation-\ncomplexity trade-off. The situation is depicted in Figure 14.4. The figure and (14.6.1) suggest\nthat, the perfect model, achieves the optimal trade-off between approximation and generalization\nerror. Using this notion, we can also separate all models into three classes:\n• Underfitting: If the approximation error decays faster than the estimation error increases.\n• Optimal: If the sum of approximation error and generalization error is at a minimum.\n• Overfitting: If the approximation error decays slower than the estimation error increases.\nIn Chapter 15, we will see that deep learning often operates in the regime where the number of\nparameters nA exceeds the optimal trade-off point. For certain architectures used in practice, nA\ncan be so large that the theory of the approximation-complexity trade-off suggests that learning\nshould be impossible. However, we emphasize, that the present analysis only provides upper bounds.\nIt does not prove that learning is impossible or even impractical in the overparameterized regime.\nMoreover, in Chapter 11 we have already seen indications that learning in the overparametrized\nregime need not necessarily lead to large generalization errors.\n14.7\nPAC learning from VC dimension\nIn addition to covering numbers, there are several other tools to analyze the generalization capacity\nof hypothesis sets. In the context of classification problems, one of the most important is the so-\ncalled Vapnik–Chervonenkis (VC) dimension.\n14.7.1\nDefinition and examples\nLet H be a hypothesis set of functions mapping from Rd to {0, 1}. A set S = {x1, . . . , xn} ⊆Rd\nis said to be shattered by H if for every (y1, . . . , yn) ∈{0, 1}n there exists h ∈H such that\nh(xj) = yj for all j ∈N.\nThe VC dimension quantifies the complexity of a function class via the number of points that\ncan in principle be shattered.\nDefinition 14.16. The VC dimension of H is the cardinality of the largest set S ⊆Rd that is\nshattered by H. We denote the VC dimension by VCdim(H).\n204\noptimal trade-off\noverfitting\nunderfitting\nFigure 14.4: Illustration of the approximation-complexity-trade-off of Equation (14.6.1). Here we\nchose r = 1 and m = 10.000, also all implicit constants are assumed to be equal to 1.\nExample 14.17 (Intervals). Let H = {1[a,b] | a, b ∈R}. It is clear that VCdim(H) ≥2 since for\nx1 < x2 the functions\n1[x1−2,x1−1],\n1[x1−1,x1],\n1[x1,x2],\n1[x2,x2+1],\nare all different, when restricted to S = (x1, x2).\nOn the other hand, if x1 < x2 < x3 then, since h−1({1}) is an interval for all h ∈H we have that\nh(x1) = 1 = h(x3) implies h(x2) = 1. Hence, no set of three elements can be shattered. Therefore,\nVCdim(H) = 2. The situation is depicted in Figure 14.5.\nFigure 14.5: Different ways to classify two or three points.\nThe colored-blocks correspond to\nintervals that produce different classifications of the points.\nExample 14.18 (Half-spaces). Let H2 = {1[0,∞)(⟨w, ·⟩+ b) | w ∈R2, b ∈R} be a hypothesis set\nof rotated and shifted two-dimensional half-spaces. In Figure 14.6 we see that H2 shatters a set of\n205\nthree points. More general, for d ≥2 with\nHd := {x 7→1[0,∞)(w⊤x + b) | w ∈Rd, b ∈R}\nthe VC dimension of Hd equals d + 1.\nFigure 14.6: Different ways to classify three points by a half-space.\nIn the example above, the VC dimension coincides with the number of parameters. However,\nthis is not true in general as the following example shows.\nExample 14.19 (Infinite VC dimension). Let for x ∈R\nH := {x 7→1[0,∞)(sin(wx)) | w ∈R}.\nThen the VC dimension of H is infinite (Exercise 14.29).\n14.7.2\nGeneralization based on VC dimension\nIn the following, we consider a classification problem. Denote by D the data-generating distribution\non Rd × {0, 1}. Moreover, we let H be a set of functions from Rd →{0, 1}.\nIn the binary classification set-up, the natural choice of a loss function is the 0 −1 loss\nL0−1(y, y′) = 1y̸=y′. Thus, given a sample S, the empirical risk of a function h ∈H is\nbRS(h) = 1\nm\nm\nX\ni=1\n1h(xi)̸=yi.\nMoreover, the risk can be written as\nR(h) = P(x,y)∼D[h(x) ̸= y],\ni.e., the probability under (x, y) ∼D of h misclassifying the label y of x.\nWe can now give a generalization bound in terms of the VC dimension of H, see, e.g., [148,\nCorollary 3.19]:\n206\nTheorem 14.20. Let d, k ∈N and H ⊆{h: Rd →{0, 1}} have VC dimension k. Let D be a\ndistribution on Rd ×{0, 1}. Then, for every δ > 0 and m ∈N, it holds with probability at least 1−δ\nover a sample S ∼Dm that for every h ∈H\n|R(h) −bRS(h)| ≤\nr\n2k log(em/k)\nm\n+\nr\nlog(1/δ)\n2m\n.\n(14.7.1)\nIn words, Theorem 14.20 tells us that if a hypothesis class has finite VC dimension, then a\nhypothesis with a small empirical risk will have a small risk if the number of samples is large. This\nshows that empirical risk minimization is a viable strategy in this scenario. Will this approach also\nwork if the VC dimension is not bounded? No, in fact, in that case, no learning algorithm will\nsucceed in reliably producing a hypothesis for which the risk is close to the best possible. We omit\nthe technical proof of the following theorem from [148, Theorem 3.23].\nTheorem 14.21. Let k ∈N and let H ⊆{h: X →{0, 1}} be a hypothesis set with VC dimension\nk. Then, for every m ∈N and every learning algorithm A: (X × {0, 1})m →H there exists a\ndistribution D on X × {0, 1} such that\nPS∼Dm\n\"\nR(A(S)) −inf\nh∈H R(h) >\nr\nk\n320m\n#\n≥1\n64.\nTheorem 14.21 immediately implies the following statement for the generalization bound.\nCorollary 14.22. Let k ∈N and let H ⊆{h: X →{0, 1}} be a hypothesis set with VC dimension\nk. Then, for every m ∈N there exists a distribution D on X × {0, 1} such that\nPS∼Dm\n\"\nsup\nh∈H\n|R(h) −bRS(h)| >\nr\nk\n1280m\n#\n≥1\n64.\nProof. For a sample S, let hS ∈H be an empirical risk minimizer, i.e., bRS(hS) = minh∈H bRS(h).\nLet D be the distribution of Theorem 14.21. Moreover, for δ > 0, let hδ ∈H be such that\nR(hδ) −inf\nh∈H R(h) < δ.\n207\nThen, applying Theorem 14.21 with A(S) = hS it holds that\n2 sup\nh∈H\n|R(h) −bRS(h)| ≥|R(hS) −bRS(hS)| + |R(hδ) −bRS(hδ)|\n≥R(hS) −bRS(hS) + bRS(hδ) −R(hδ)\n≥R(hS) −R(hδ)\n> R(hS) −inf\nh∈H R(h) −δ,\nwhere we used the definition of hS in the third inequality. The proof is completed by applying\nTheorem 14.21 and using that δ was arbitrary.\nWe have seen now, that we have a generalization bound scaling like O(1/√m) for m →∞if\nand only if the VC dimension of a hypothesis class is finite. In more quantitative terms, we require\nthe VC dimension of a neural network to be smaller than m.\nWhat does this imply for neural network functions? For ReLU neural networks there holds the\nfollowing [3, Theorem 8.8].\nTheorem 14.23. Let A ∈NL+2, L ∈N and set\nH := {1[0,∞) ◦Φ | Φ ∈N(σReLU; A, ∞)}.\nThen, there exists a constant C > 0 independent of L and A such that\nVCdim(H) ≤C · (nAL log(nA) + nAL2).\nThe bound (14.7.1) is meaningful if m ≫k. For ReLU neural networks as in Theorem 14.23,\nthis means m ≫nAL log(nA) + nAL2.\nFixing L = 1 this amounts to m ≫nA log(nA) for a\nshallow neural network with nA parameters. This condition is contrary to what we assumed in\nChapter 11, where it was crucial that nA ≫m. If the VC dimension of the neural network sets\nscale like O(nA log(nA)), then Theorem 14.21 and Corollary 14.22 indicate that, at least for certain\ndistributions, generalization should not be possible in this regime. We will discuss the resolution\nof this potential paradox in Chapter 15.\n14.8\nLower bounds on achievable approximation rates\nWe conclude this chapter on the complexities and generalization bounds of neural networks by using\nthe established VC dimension bound of Theorem 14.23 to deduce limitations to the approximation\ncapacity of neural networks. The result described below was first given in [245].\nTheorem 14.24. Let k, d ∈N. Assume that for every ε > 0 there exists Lε ∈N and Aε with Lε\nlayers and input dimension d such that\nsup\n∥f∥Ck([0,1]d)≤1\ninf\nΦ∈N(σReLU;A,∞) ∥f −Φ∥C0([0,1]d) < ε\n2.\n208\nThen there exists C > 0 solely depending on k and d, such that for all ε ∈(0, 1)\nnAεLε log(nAε) + nAεL2\nε ≥Cε−d\nk .\nProof. For x ∈Rd consider the “bump function”\n˜f(x) :=\n(\nexp\n\u0010\n1 −\n1\n1−∥x∥2\n2\n\u0011\nif ∥x∥2 < 1\n0\notherwise,\nand its scaled version\n˜fε(x) := εf\n\u0010\n2ε−1/kx\n\u0011\n,\nfor ε ∈(0, 1). Then\nsupp( ˜fε) ⊆\nh\n−ε1/k\n2 , ε1/k\n2\nid\nand\n∥˜fε∥Ck ≤2k∥˜f∥Ck =: τk > 0.\nConsider the equispaced point set {x1, . . . , xN(ε)} = ε1/kZd ∩[0, 1]d. The cardinality of this set\nis N(ε) ≃ε−d/k. Given y ∈{0, 1}N(ε), let for x ∈Rd\nfy(x) := τ −1\nk\nN(ε)\nX\nj=1\nyj ˜fε(x −xj).\n(14.8.1)\nThen fy(xj) = τ −1\nk εyj for all j = 1, . . . , N(ε) and ∥fy∥Ck ≤1.\nFor every y ∈{0, 1}N(ε) let Φy ∈N(σReLU; Aτ −1\nk\nε, ∞) be such that\nsup\nx∈[0,1]d |fy(x) −Φy(x)| <\nε\n2τk\n.\nThen\n1[0,∞)\n\u0010\nΦy(xj) −\nε\n2τk\n\u0011\n= yj\nfor all j = 1, . . . , N(ε).\nHence, the VC dimension of N(σReLU; Aτ −1\nk\nε, ∞) is larger or equal to N(ε). Theorem 14.23 thus\nimplies\nN(ε) ≃ε−d\nk ≤C ·\n\u0010\nnAτ−1\nk\nεLτ −1\nk\nε log(nAτ−1\nk\nε) + nAτ−1\nk\nεL2\nτ −1\nk\nε\n\u0011\nor equivalently\nτ\nd\nk\nk ε−d\nk ≤C ·\n\u0010\nnAτ−1\nk\nεLε log(nAτ−1\nk\nε) + nAτ−1\nk\nεL2\nε\n\u0011\n.\nThis completes the proof.\n209\nFigure 14.7: Illustration of fy from Equation (14.8.1) on [0, 1]2.\nTo interpret Theorem 14.24, we consider two situations:\n• In case the depth is allowed to increase at most logarithmically in ε, then reaching uniform\nerror ε for all f ∈Ck([0, 1]d) with ∥f∥Ck([0,1]d) ≤1 requires\nnAε log(nAε) log(ε) + nAε log(ε)2 ≥Cε−d\nk .\nIn terms of the neural network size, this (necessary) condition becomes nAε ≥Cε−d/k/ log(ε)2.\nAs we have shown in Chapter 7, in particular Theorem 7.7, up to log terms this condition is\nalso sufficient. Hence, while the constructive proof of Theorem 7.7 might have seemed rather\nspecific, under the assumption of the depth increasing at most logarithmically (which the\nconstruction in Chapter 7 satisfies), it was essentially optimal! The neural networks in this\nproof are shown to have size O(ε−d/k) up to log terms.\n• If we allow the depth Lε to increase faster than logarithmically in ε, then the lower bound on\nthe required neural network size improves. Fixing for example Aε with Lε layers such that\nnAε ≤WLε for some fixed ε independent W ∈N, the (necessary) condition on the depth\nbecomes\nW log(WLε)L2\nε + WL3\nε ≥Cε−d\nk\nand hence Lε ≳ε−d/(3k).\nWe add that, for arbitrary depth the upper bound on the VC dimension of Theorem 14.23\ncan be improved to n2\nA, [3, Theorem 8.6], and using this, would improve the just established\nlower bound to Lε ≳ε−d/(2k).\nFor fixed width, this corresponds to neural networks of size O(ε−d/(2k)), which would mean\ntwice the convergence rate proven in Theorem 7.7. Indeed, it turns out that neural networks\ncan achieve this rate in terms of the neural network size [246].\nTo sum up, in order to get error ε uniformly for all ∥f∥Ck([0,1]d) ≤1, the size of a ReLU neural\nnetwork is required to increase at least like O(ε−d/(2k)) as ε →0, i.e. the best possible attainable\nconvergence rate is 2k/d. It has been proven, that this rate is also achievable, and thus the bound\nis sharp. Achieving this rate requires neural network architectures that grow faster in depth than\nin width.\n210\nBibliography and further reading\nClassical statistical learning theory is based on the foundational work of Vapnik and Chervonenkis\n[233]. This led to the formulation of the probably approximately correct (PAC) learning model\nin [232], which is primarily utilized in this chapter. A streamlined mathematical introduction to\nstatistical learning theory can be found in [43].\nSince statistical learning theory is well-established, there exists a substantial amount of excellent\nexpository work describing this theory. Some highly recommended books on the topic are [148,\n212, 3]. The specific approach of characterizing learning via covering numbers has been discussed\nextensively in [3, Chapter 14]. Specific results for ReLU activation used in this chapter were derived\nin [204, 18]. The results of Section 14.8 describe some of the findings in [245, 246], and we also refer\nto [51] for general lower bounds (also applicable to neural networks) when approximating classes\nof Sobolev functions.\n211\nExercises\nExercise 14.25. Let H be a set of neural networks with fixed architecture, where the weights are\ntaken from a compact set. Moreover, assume that the activation function is continuous. Show that\nfor every sample S there always exists an empirical risk minimizer hS.\nExercise 14.26. Complete the proof of Proposition 14.9.\nExercise 14.27. Prove Lemma 14.12.\nExercise 14.28. Show that, the VC dimension of H of Example 14.18 is indeed 3, by demonstrating\nthat no set of four points can be shattered by H.\nExercise 14.29. Show that the VC dimension of\nH := {x 7→1[0,∞)(sin(wx)) | w ∈R}\nis infinite.\n212\nChapter 15\nGeneralization in the\noverparameterized regime\nIn the previous chapter, we discussed the theory of generalization for deep neural networks trained\nby minimizing the empirical risk. A key conclusion was that good generalization is possible as long\nas we choose an architecture that has a moderate number of neural network parameters relative to\nthe number of training samples. Moreover, we saw in Section 14.6 that the best performance can be\nexpected when the neural network size is chosen to balance the generalization and approximation\nerrors, by minimizing their sum.\nArchitectures On ImageNet\nFigure 15.1: ImageNet Classification Competition: Final score on the test set in the Top 1 cat-\negory vs. Parameters-to-Training-Samples Ratio. Note that all architectures have more parame-\nters than training samples. Architectures include AlexNet [121], VGG16 [215], GoogLeNet [222],\nResNet50/ResNet152 [87], DenseNet121 [96], ViT-G/14 [248], EfficientNetB0 [224], and Amoe-\nbaNet [189].\nSurprisingly, successful neural network architectures do not necessarily follow these theoretical\nobservations.\nConsider the neural network architectures in Figure 15.1.\nThey represent some\n213\nof the most renowned image classification models, and all of them participated in the ImageNet\nClassification Competition [50]. The training set consisted of 1.2 million images. The x-axis shows\nthe model performance, and the y-axis displays the ratio of the number of parameters to the size of\nthe training set; notably, all architectures have a ratio larger than one, i.e. have more parameters\nthan training samples. For the largest model, there are by a factor 1000 more neural network\nparameters than training samples.\nGiven that the practical application of deep learning appears to operate in a regime significantly\ndifferent from the one analyzed in Chapter 14, we must ask: Why do these methods still work\neffectively?\n15.1\nThe double descent phenomenon\nThe success of deep learning in a regime not covered by traditional statistical learning theory\npuzzled researchers for some time. In [14], an intriguing set of experiments was performed. These\nexperiments indicate that while the risk follows the upper bound from Section 14.6 for neural\nnetwork architectures that do not interpolate the data, the curve does not expand to infinity in the\nway that Figure 14.4 suggests. Instead, after surpassing the so-called “interpolation threshold”,\nthe risk starts to decrease again. This behavior, known as double descent, is illustrated in Figure\n15.2.\nmodern regime\nunderfitting\noverfitting\nInterpolation threshold \nclassical regime\nExpressivity of H\nR(h)\nbRS(h)\nFigure 15.2: Illustration of the double descent phenomenon.\n15.1.1\nLeast-squares regression revisited\nTo gain further insight, we consider least-squares (kernel) regression as introduced in Section 11.2.\nConsider a data sample (xj, yj)m\nj=1 ⊆Rd × R generated by some ground-truth function f, i.e.\nyj = f(xj)\nfor j = 1, . . . , m.\n(15.1.1)\nLet ϕj : Rd →R, j ∈N, be a sequence of ansatz functions. For n ∈N, we wish to fit a function\nx 7→Pn\ni=1 wiϕi(x) to the data using linear least-squares. To this end, we introduce the feature\nmap\nRd ∋x 7→ϕ(x) := (ϕ1(x), . . . , ϕn(x))⊤∈Rn.\n214\nThe goal is to determine coefficients w ∈Rn minimizing the empirical risk\nbRS(w) = 1\nm\nm\nX\nj=1\n\u0010\nn\nX\ni=1\nwiϕi(xj) −yj\n\u00112\n= 1\nm\nm\nX\nj=1\n(⟨ϕ(xj), w⟩−yj)2.\nWith\nAn :=\n\n\n\nϕ1(x1)\n. . .\nϕn(x1)\n...\n...\n...\nϕ1(xm)\n. . .\nϕn(xm)\n\n\n=\n\n\n\nϕ(x1)⊤\n...\nϕ(xm)⊤\n\n\n∈Rm×n\n(15.1.2)\nand y = (y1, . . . , ym)⊤it holds\nbRS(w) = 1\nm∥Anw −y∥2.\n(15.1.3)\nAs discussed in Sections 11.1-11.2, a unique minimizer of (15.1.3) only exists if An has rank n.\nFor a minimizer wn, the fitted function reads\nfn(x) :=\nn\nX\nj=1\nwn,jϕj(x).\n(15.1.4)\nWe are interested in the behavior of the fn as a function of n (the number of ansatz func-\ntions/parameters of our model), and distinguish between two cases:\n• Underparameterized: If n < m we have fewer parameters n than training points m. For\nthe least squares problem of minimizing bRS, this means that there are more conditions m\nthan free parameters n.\nThus, in general, we cannot interpolate the data, and we have\nminw∈Rn bRS(w) > 0.\n• Overparameterized: If n ≥m, then we have at least as many parameters n as training points\nm. If the xj and the ϕj are such that An ∈Rm×n has full rank m, then there exists w\nsuch that bRS(w) = 0. If n > m, then An necessarily has a nontrivial kernel, and there exist\ninfinitely many parameters choices w that yield zero empirical risk bRS. Some of them lead\nto better, and some lead to worse prediction functions fn in (15.1.4).\nIn the overparameterized case, there exist many minimizers of bRS. The training algorithm we\nuse to compute a minimizer determines the type of prediction function fn we obtain. To observe\ndouble descent, i.e. to achieve good generalization for large n, we need to choose the minimizer\ncarefully. In the following, we consider the unique minimal 2-norm minimizer, which is defined as\nwn,∗=\n\u0010\nargmin{w∈Rn | b\nRS(w)≤b\nRS(v) ∀v∈Rn} ∥w∥\n\u0011\n∈Rn.\n(15.1.5)\n15.1.2\nAn example\nNow let us consider a concrete example.\nIn Figure 15.3 we plot a set of 40 ansatz functions\nϕ1, . . . , ϕ40, which are drawn from a Gaussian process. Additionally, the figure shows a plot of the\nRunge function f, and m = 18 equispaced points which are used as the training data points. We\nthen fit a function in span{ϕ1, . . . , ϕn} via (15.1.5) and (15.1.4). The result is displayed in Figure\n15.4:\n215\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n3\n2\n1\n0\n1\n2\n3\nj\n(a) ansatz functions ϕj\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf\nData points\n(b) Runge function f and data points\nFigure 15.3: Ansatz functions ϕ1, . . . , ϕ40 drawn from a Gaussian process, along with the Runge\nfunction and 18 equispaced data points.\n• n = 2: The model can only represent functions in span{ϕ1, ϕ2}. It is not yet expressive\nenough to give a meaningful approximation of f.\n• n = 15: The model has sufficient expressivity to capture the main characteristics of f. Since\nn = 15 < 18 = m, it is not yet able to interpolate the data. Thus it allows to strike a\ngood balanced between the approximation and generalization error, which corresponds to the\nscenario discussed in Chapter 14.\n• n = 18: We are at the interpolation threshold. The model is capable of interpolating the data,\nand there is a unique w such that bRS(w) = 0. Yet, in between data points the behavior of the\npredictor f18 seems erratic, and displays strong oscillations. This is referred to as overfitting,\nand is to be expected due to our analysis in Chapter 14; while the approximation error at the\ndata points has improved compared to the case n = 15, the generalization error has gotten\nworse.\n• n = 40: This is the overparameterized regime, where we have significantly more parameters\nthan data points. Our prediction f40 interpolates the data and appears to be the best overall\napproximation to f so far, due to a “good” choice of minimizer of bRS, namely (15.1.5).\nWe also note that, while quite good, the fit is not perfect. We cannot expect significant\nimprovement in performance by further increasing n, since at this point the main limiting\nfactor is the amount of available data. Also see Figure 15.5 (a).\nFigure 15.5 (a) displays the error ∥f −fn∥L2([−1,1]) over n. We observe the characteristic double\ndescent curve, where the error initially decreases, after peaking at the interpolation threshold,\nwhich is marked by the dashed red line. Afterwards, in the overparameterized regime, it starts to\ndecrease again. Figure 15.5 (b) displays ∥wn,∗∥. Note how the Euclidean norm of the coefficient\nvector also peaks at the interpolation threshold.\nWe emphasize that the precise nature of the convergence curves depends strongly on various\nfactors, such as the distribution and number of training points m, the ground truth f, and the\nchoice of ansatz functions ϕj (e.g., the specific kernel used to generate the ϕj in Figure 15.3 (a)).\nIn the present setting we achieve a good approximation of f for n = 15 < 18 = m corresponding to\nthe regime where the approximation and interpolation errors are balanced. However, as Figure 15.5\n216\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf\nf2\nData points\n(a) n = 2 (underparameterization)\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf\nf15\nData points\n(b) n = 15 (balance of appr. and gen. error)\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf\nf18\nData points\n(c) n = 18 (interpolation threshold)\n1.00\n0.75\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nf\nf40\nData points\n(d) n = 40 (overparameterization)\nFigure 15.4: Fit of the m = 18 red data points using the ansatz functions ϕ1, . . . , ϕn from Figure\n15.3, employing equations (15.1.5) and (15.1.4) for different numbers of ansatz functions n.\n(a) shows, it can be difficult to determine a suitable value of n < m a priori, and the acceptable\nrange of n values can be quite narrow. For overparametrization (n ≫m), the precise choice of n is\nless critical, potentially making the algorithm more stable in this regime. We encourage the reader\nto conduct similar experiments and explore different settings to get a better feeling for the double\ndescent phenomenon.\n15.2\nSize of weights\nIn Figure 15.5, we observed that the norm of the coefficients ∥wn,∗∥exhibits similar behavior to\nthe L2-error, peaking at the interpolation threshold n = 18. In machine learning, large weights\nare usually undesirable, as they are associated with large derivatives or oscillatory behavior. This\nis evident in the example shown in Figure 15.4 for n = 18. Assuming that the data in (15.1.1)\nwas generated by a “smooth” function f, e.g. a function with moderate Lipschitz constant, these\nlarge derivatives of the prediction function could lead to poor generalization. Such a smoothness\nassumption about f may or may not be satisfied. However, if f is not smooth, there is little hope\nof accurately recovering f from limited data (see the discussion in Section 9.2).\nThe next result gives an explanation for the observed behavior of ∥wn,∗∥.\n217\n10\n20\n30\n40\nn\n10\n2\n10\n1\nn = 18\n(a) ∥f −fn∥L2([−1,1])\n10\n20\n30\n40\nn\n100\nn = 18\n(b) ∥wn,∗∥\nFigure 15.5: The L2-error for the fitted functions in Figure 15.4, and the ℓ2-norm of the corre-\nsponding coefficient vector wn,∗defined in (15.1.5).\nProposition 15.1. Assume that x1, . . . , xm and the (ϕj)j∈N are such that An in (15.1.2) has full\nrank n for all n ≤m. Given y ∈Rm, denote by wn,∗(y) the vector in (15.1.5). Then\nn 7→sup\n∥y∥=1\n∥wn,∗(y)∥\nis monotonically\n(\nincreasing\nfor n < m,\ndecreasing\nfor n ≥m.\nProof. We start with the case n ≥m. By assumption Am has full rank m, and thus An has rank\nm for all n ≥m, see (15.1.2). In particular, there exists wn ∈Rn such that Anwn = y. Now fix\ny ∈Rm and let wn be any such vector. Then wn+1 := (wn, 0) ∈Rn+1 satisfies An+1wn+1 = y\nand ∥wn+1∥= ∥wn∥. Thus necessarily ∥wn+1,∗∥≤∥wn,∗∥for the minimal norm solutions defined\nin (15.1.5). Since this holds for every y, we obtain the statement for n ≥m.\nNow let n < m. Recall that the minimal norm solution can be written through the pseudo\ninverse\nwn,∗(y) = A†\nny,\nsee for instance Exercise 11.32. Here,\nA†\nn = V n\n\n\n\nσ−1\nn,1\n0\n...\n...\nσ−1\nn,n\n0\n\n\nU ⊤\nn ∈Rn×m\n218\nwhere An = U nΣ nV ⊤\nn is the singular value decomposition of An, and\nΣ n =\n\n\n\n\n\n\n\n\n\n\nσn,1\n...\nσn,n\n0\n...\n0\n\n\n\n\n\n\n\n\n\n\n∈Rm×n\ncontains the singular values σn,1 ≥· · · ≥σn,n > 0 of An ∈Rm×n ordered by decreasing size. Since\nV n ∈Rn×n and U n ∈Rm×m are orthogonal matrices, we have\nsup\n∥y∥=1\n∥wn,∗(y)∥= sup\n∥y∥=1\n∥A†\nny∥= σ−1\nn,n.\nFinally, since the minimal singular value σn,n of An can be written as\nσn,n =\ninf\nx∈Rn\n∥x∥=1\n∥Anx∥≥\ninf\nx∈Rn+1\n∥x∥=1\n∥An+1x∥= σn+1,n+1,\nwe observe that n 7→σn,n is monotonically decreasing for n ≤m. This concludes the proof.\n15.3\nTheoretical justification\nLet us now examine one possible explanation of the double descent phenomenon for neural networks.\nWhile there are many alternative arguments available in the literature (see the bibliography section),\nthe explanation presented here is based on a simplification of the ideas in [12].\nThe key assumption underlying our analysis is that large overparameterized neural networks\ntend to be Lipschitz continuous with a Lipschitz constant independent of the size.\nThis is a\nconsequence of neural networks typically having relatively small weights. To motivate this, let us\nconsider the class of neural networks N(σ; A, B) for an architecture A of depth d ∈N and width\nL ∈N. If σ is Cσ-Lipschitz continuous such that B ≤cB ·(dCσ)−1 for some cB > 0, then by Lemma\n13.2\nN(σ; A, B) ⊆LipcL\nB(Rd0),\n(15.3.1)\nAn assumption of the type B ≤cB · (dCσ)−1, i.e. a scaling of the weights by the reciprocal 1/d of\nthe width, is not unreasonable in practice: Standard initialization schemes such as LeCun [129] or\nHe [86] initialization, use random weights with variance scaled inverse proportional to the input\ndimension of each layer. Moreover, as we saw in Chapter 11, for very wide neural networks, the\nweights do not move significantly from their initialization during training. Additionally, many train-\ning routines use regularization terms on the weights, thereby encouraging them the optimization\nroutine to find small weights.\nWe study the generalization capacity of Lipschitz functions through the covering-number-based\nlearning results of Chapter 14.\nThe set of C-Lipschitz functions on a compact d-dimensional\nEuclidean domain LipC(Ω) has covering numbers bounded according to\nlog(G(LipC(Ω), ε, L∞)) ≤Ccov ·\n\u0012C\nε\n\u0013d\nfor all ε > 0\n(15.3.2)\n219\nfor some constant Ccov independent of ε > 0. A proof can be found in [75, Lemma 7], see also [230].\nAs a result of these considerations, we can identify two regimes:\n• Standard regime: For small neural network size nA, we consider neural networks as a set\nparameterized by nA parameters. As we have seen before, this yields a bound on the gen-\neralization error that scales linearly with nA. As long as nA is small in comparison to the\nnumber of samples, we can expect good generalization by Theorem 14.15.\n• Overparameterized regime: For large neural network size nA and small weights, we consider\nneural networks as a subset of LipC(Ω) for a constant C > 0. This set has a covering number\nbound that is independent of the number of parameters nA.\nChoosing the better of the two generalization bounds for each regime yields the following result.\nRecall that N ∗(σ; A, B) denotes all neural networks in N(σ; A, B) with a range contained in [−1, 1]\n(see (14.5.1)).\nTheorem 15.2. Let C, CL > 0 and let L: [−1, 1] × [−1, 1] →R be CL-Lipschitz. Further, let\nA = (d0, d1, . . . , dL+1) ∈NL+2, let σ: R →R be Cσ-Lipschitz continuous with Cσ ≥1, and\n|σ(x)| ≤Cσ|x| for all x ∈R, and let B > 0.\nThen, there exist c1, c2 > 0, such that for every m ∈N and every distribution D on\n[−1, 1]d0 × [−1, 1] it holds with probability at least 1 −δ over S\n∼Dm that for all Φ ∈\nN ∗(σ; A, B) ∩LipC([−1, 1]d0)\n|R(Φ) −bRS(Φ)| ≤g(A, Cσ, B, m) + 4CL\nr\nlog(4/δ)\nm\n,\n(15.3.3)\nwhere\ng(A, Cσ, B, m) = min\n(\nc1\nr\nnA log(nA⌈√m⌉) + LnA log(dmax)\nm\n, c2m−\n1\n2+d0\n)\n.\nProof. Applying Theorem 14.11 with α = 1/(2 + d0) and (15.3.2), we obtain that with probability\nat least 1 −δ/2 it holds for all Φ ∈LipC([−1, 1]d0)\n|R(Φ) −bRS(Φ)| ≤4CL\nr\nCcov(mαC)d0 + log(4/δ)\nm\n+ 2CL\nmα\n≤4CL\nq\nCcovCd0(md0/(d0+2)−1) + 2CL\nmα + 4CL\nr\nlog(4/δ)\nm\n= 4CL\nq\nCcovCd0(m−2/(d0+2)) + 2CL\nmα + 4CL\nr\nlog(4/δ)\nm\n= (4CL\np\nCcovCd0 + 2CL)\nmα\n+ 4CL\nr\nlog(4/δ)\nm\n,\nwhere we used in the second inequality that √x + y ≤√x + √y for all x, y ≥0.\n220\nIn addition, Theorem 14.15 yields that with probability at least 1 −δ/2 it holds for all Φ ∈\nN ∗(σ; A, B)\n|R(Φ) −bRS(Φ)| ≤4CL\nr\nnA log(⌈nA\n√m⌉) + LnA log(⌈2CσBdmax⌉) + log(4/δ)\nm\n+ 2CL\n√m\n≤6CL\nr\nnA log(⌈nA\n√m⌉) + LnA log(⌈2CσBdmax⌉)\nm\n+ 4CL\nr\nlog(4/δ))\nm\n.\nThen, for Φ ∈N ∗(σ; A, B) ∩LipC([−1, 1]d0) the minimum of both upper bounds holds with\nprobability at least 1 −δ.\nThe two regimes in Theorem 15.2 correspond to the two terms comprising the minimum in the\ndefinition of g(A, Cσ, B, m). The first term increases with nA while the second is constant. In the\nfirst regime, where the first term is smaller, the generalization gap |R(Φ) −bRS(Φ)| increases with\nnA.\nIn the second regime, where the second term is smaller, the generalization gap is constant with\nnA. Moreover, it is reasonable to assume that the empirical risk bRS will decrease with increasing\nnumber of parameters nA.\nBy (15.3.3) we can bound the risk by\nR(Φ) ≤bRS + g(A, Cσ, B, m) + 4CL\nr\nlog(4/δ)\nm\n.\nIn the second regime, this upper bound is monotonically decreasing. In the first regime it may\nboth decrease and increase. In some cases, this behavior can lead to an upper bound on the risk\nresembling the curve of Figure 15.2. The following section describes a specific scenario where this\nis the case.\nRemark 15.3. Theorem 15.2 assumes C-Lipschitz continuity of the neural networks. As we saw in\nSections 15.1.2 and 15.2, this assumption may not hold near the interpolation threshold. Hence,\nTheorem 15.2 likely gives a too optimistic upper bound near the interpolation threshold.\n15.4\nDouble descent for neural network learning\nNow let us understand the double descent phenomenon in the context of Theorem 15.2. We make\na couple of simplifying assumptions to obtain a formula for an upper bound on the risk. First, we\nassume that the data S = (xi, yi)m\ni=1 ∈Rd0 × R stem from a CM-Lipschitz continuous function.\nIn addition, we fix a depth L ∈N and consider, for d ∈N, architectures of the form (σReLU; Ad),\nwhere\nAd = (d0, d, . . . , d, 1).\nFor this architecture the number of parameters is bounded by\nnAd = (d0 + 1)d + (L −1)(d + 1)d + d + 1.\n221\nTo derive an upper bound on the risk, we start by upper bounding the empirical risk and then\napplying Theorem 15.2 to establish an upper bound on the generalization gap. In combination,\nthese estimates provide an upper bound on the risk. We will then observe that this upper bound\nfollows the double descent curve in Figure 15.2.\n15.4.1\nUpper bound on empirical risk\nWe establish an upper bound on bRS(Φ) for Φ ∈N ∗(σReLU; Ad, B)∩LipCM ([−1, 1]d0). For B ≥CM,\nwe can apply Theorem 9.6, and conclude that with a neural network of sufficient depth we can\ninterpolate m points from a CM-Lipschitz function with a neural network in LipCM ([−1, 1]d0), if\nnA ≥cint log(m)d0m.\nTo simplify the exposition, we assume cint = 1 in the following.\nThus,\nbRS(Φ) = 0 as soon as nA ≥log(m)d0m.\nIn addition, depending on smoothness properties of the data, the interpolation error may decay\nwith some rate, by one of the results in Chapters 5, 7, or 8. For simplicity, we choose that bRS(Φ) =\nO(n−1\nA ) for nA significantly smaller than log(m)d0m. If we combine these two assumptions, we can\nmake the following Ansatz for the empirical risk of ΦAd ∈N ∗(σReLU; Ad, B) ∩LipCM ([−1, 1]d0):\nbRS(ΦAd) ≤eRS(ΦAd) := Capprox max\nn\n0, n−1\nAd −(log(m)d0m)−1o\n(15.4.1)\nfor a constant Capprox > 0. Note that, we can interpolate the sample S already with d0m parameters\nby Theorem 9.3. However, it is not guaranteed that this can be done using CM-Lipschitz neural\nnetworks.\n15.4.2\nUpper bound on generalization gap\nWe complement the bound on the empirical risk by an upper bound on the risk. Invoking the\nnotation of Theorem 15.2, we have that,\ng(Ad, CσReLU, B, m) = min {κNN(Ad, m; c1), κLip(Ad, m; c2)} ,\nwhere\nκNN(Ad, m; c1) := c1\nr\nnAd log(⌈nA\n√m⌉) + LnAd log(d)\nm\n,\nκLip(Ad, m; c2) := c2m−\n1\n2+d0\n(15.4.2)\nfor some constants c1, c2 > 0.\n15.4.3\nUpper bound on risk\nNext, we combine (15.4.1) and (15.4.2) to obtain an upper bound on the risk R(ΦAd). Specifically,\nwe define\neR(ΦAd) := eRS(ΦAd) + min {κNN(Ad, m; c1), κLip(Ad, m; c2)}\n(15.4.3)\n+ 4CL\nr\nlog(4/δ)\nm\n.\n222\nWe depict in Figure 15.6 the upper bound on the risk given by (15.4.3) (excluding the terms\nthat do not depend on the architecture). The upper bound clearly resembles the double descent\nphenomenon of Figure 15.2. Note that the Lipschitz interpolation point is slightly behind this\nthreshold, which is when we assume our empirical risk to be 0. To produce the plot, we chose\nL = 5, c1 = 1.2 · 10−4, c2 = 6.5 · 10−3, m = 10.000, d0 = 6, Capprox = 30. We mention that the\ndouble descent phenomenon is not visible for all choices of parameters. Moreover, in our model,\nthe fact that the peak coincides with the interpolation threshold is due to the choice of constants\nand does not emerge from the model. Other models of double descent explain the location of the\npeak more accurately [143, 83]. We note that, as observed in Remark 15.3, the peak close to the\ninterpolation threshold that we see in Figure 15.6 would likely be more pronounced in practical\nscenarios.\nFigure 15.6: Upper bound on R(ΦAd) derived in (15.4.3). For better visibility the part correspond-\ning to y-values between 0.0012 and 0.0022 is not shown. The vertical dashed line indicates the\ninterpolation threshold according to Theorem 9.3.\nBibliography and further reading\nThe discussion on kernel regression and the effect of the number of parameters on the norm of the\nweights was already given in [14]. Similar analyses, with more complex ansatz systems and more\nprecise asymptotic estimates, are found in [143, 83]. Our results in Section 15.3 are inspired by\n[12]; see also [161].\nFor a detailed account of further arguments justifying the surprisingly good generalization\n223\ncapabilities of overparameterized neural networks, we refer to [19, Section 2]. Here, we only briefly\nmention two additional directions of inquiry. First, if the learning algorithm introduces a form of\nrobustness, this can be leveraged to yield generalization bounds [6, 244, 24, 179]. Second, for very\noverparameterized neural networks, it was stipulated in [106] that neural networks become linear\nkernel interpolators based on the neural tangent kernel of Section 11.5.2. Thus, for large neural\nnetworks, generalization can be studied through kernel regression [106, 131, 15, 135].\n224\nExercises\nExercise 15.4. Let f : [−1, 1] →R be a continuous function, and let −1 ≤x1 < · · · < xm ≤1 for\nsome fixed m ∈N. As in Section 15.1.2, we wish to approximate f by a least squares approximation.\nTo this end we use the Fourier ansatz functions\nb0(x) := 1\n2\nand\nbj(x) :=\n(\nsin(⌈j\n2⌉πx)\nj ≥1 is odd\ncos(⌈j\n2⌉πx)\nj ≥1 is even.\n(15.4.4)\nWith the empirical risk\nbRS(w) = 1\nm\nm\nX\nj=1\n\u0010\nn\nX\ni=0\nwibi(xj) −yj\n\u00112\n,\ndenote by wn\n∗∈Rn+1 the minimal norm minimizer of bRS, and set fn(x) := Pn\ni=0 wn\n∗,ibi(x).\nShow that in this case generalization fails in the overparametrized regime: for sufficiently large\nn ≫m, fn is not necessarily a good approximation to f. What does fn converge to as n →∞?\nExercise 15.5. Consider the setting of Exercise 15.4. We adapt the ansatz functions in (15.4.4)\nby rescaling them via\n˜bj := cjbj.\nChoose real numbers cj ∈R, such that the corresponding minimal norm least squares solution\navoids the phenomenon encountered in Exercise 15.4.\nHint: Should ansatz functions corresponding to large frequencies be scaled by large or small\nnumbers to avoid overfitting?\nExercise 15.6. Prove (15.3.2) for d = 1.\n225\nChapter 16\nRobustness and adversarial examples\nHow sensitive is the output of a neural network to small changes in its input? Real-world obser-\nvations of trained neural networks often reveal that even barely noticeable modifications of the\ninput can lead to drastic variations in the network’s predictions. This intriguing behavior was first\ndocumented in the context of image classification in [223].\nFigure 16.1 illustrates this concept. The left panel shows a picture of a panda that the neural\nnetwork correctly classifies as a panda. By adding an almost imperceptible amount of noise to the\nimage, we obtain the modified image in the right panel. To a human, there is no visible difference,\nbut the neural network classifies the perturbed image as a wombat.\nThis phenomenon, where\na correctly classified image is misclassified after a slight perturbation, is termed an adversarial\nexample.\nIn practice, such behavior is highly undesirable. It indicates that our learning algorithm might\nnot be very reliable and poses a potential security risk, as malicious actors could exploit it to trick\nthe algorithm. In this chapter, we describe the basic mathematical principles behind adversarial\nexamples and investigate simple conditions under which they might or might not occur. For sim-\nplicity, we restrict ourselves to a binary classification problem but note that the main ideas remain\nvalid in more general situations.\n+\n=\nPanda\nBarely visible noise\nStill a panda\nHuman:\n0.01x\nNN classiﬁer:\nPanda (high conﬁdence)\nFlamingo (low conﬁdence)\nWombat (high conﬁdence)\nFigure 16.1: Sketch of an adversarial example.\n226\n16.1\nAdversarial examples\nLet us start by formalizing the notion of an adversarial example.\nWe consider the problem of\nassigning a label y ∈{−1, 1} to a vector x ∈Rd. It is assumed that the relation between x and y\nis described by a distribution D on Rd × {−1, 1}. In particular, for a given x, both values −1 and\n1 could have positive probability, i.e. the label is not necessarily deterministic. Additionally, we let\nDx := {x ∈Rd | ∃y s.t. (x, y) ∈supp(D)},\n(16.1.1)\nand refer to Dx as the feature support.\nThroughout this chapter we denote by\ng: Rd →{−1, 0, 1}\na fixed so-called ground-truth classifier, satisfying1\nP[y = g(x)|x] ≥P[y = −g(x)|x]\nfor all x ∈Dx.\n(16.1.2)\nNote that we allow g to take the value 0, which is to be understood as an additional label corre-\nsponding to nonrelevant or nonsensical input data x. We will refer to g−1(0) as the nonrelevant\nclass. The ground truth g is interpreted as how a human would classify the data, as the following\nexample illustrates.\nExample 16.1. We wish to classify whether an image shows a panda (y = 1) or a wombat (y = −1).\nConsider again Figure 16.1, and denote the three images by x1, x2, x3. The first image x1 is a\nphotograph of a panda. Together with a label y, it can be interpreted as a draw (x1, y) from D,\ni.e. x1 ∈Dx and g(x1) = 1. The second image x2 displays noise and corresponds to nonrelevant\ndata as it shows neither a panda nor a wombat. In particular, x2 ∈Dc\nx and g(x2) = 0. The third\n(perturbed) image x3 also belongs to Dc\nx, as it is not a photograph but a noise corrupted version\nof x1. Nonetheless, it is not nonrelevant, as a human would classify it as a panda. Thus g(x3) = 1.\nAdditional to the ground truth g, we denote by\nh: Rd →{−1, 1}\nsome trained classifier.\nDefinition 16.2. Let g: Rd →{−1, 0, 1} be the ground-truth classifier, let h: Rd →{−1, 1} be a\nclassifier, and let ∥· ∥∗be a norm on Rd. For x ∈Rd and δ > 0, we call x′ ∈Rd an adversarial\nexample to x ∈Rd with perturbation δ, if and only if\n(i) ∥x′ −x∥∗≤δ,\n(ii) g(x)g(x′) > 0,\n(iii) h(x) = g(x) and h(x′) ̸= g(x′).\n1To be more precise, the conditional distribution of y|x is only well-defined almost everywhere w.r.t. the marginal\ndistribution of x.\nThus (16.1.2) can only be assumed to hold for almost every x ∈Dx w.r.t. to the marginal\ndistribution of x.\n227\nIn words, x′ is an adversarial example to x with perturbation δ, if (i) the distance of x and x′\nis at most δ, (ii) x and x′ belong to the same (not nonrelevant) class according to the ground truth\nclassifier, and (iii) the classifier h correctly classifies x but misclassifies x′.\nRemark 16.3. We emphasize that the concept of a ground-truth classifier g differs from a minimizer\nof the Bayes risk (14.1.1) for two reasons. First, we allow for an additional label 0 corresponding to\nthe nonrelevant class, which does not exist for the data generating distribution D. Second, g should\ncorrectly classify points outside of Dx; small perturbations of images as we find them in adversarial\nexamples, are not regular images in Dx. Nonetheless, a human classifier can still classify these\nimages, and g models this property of human classification.\n16.2\nBayes classifier\nAt first sight, an adversarial example seems to be no more than a misclassified sample. Naturally,\nthese exist if the model does not generalize well. In this section we present a more nuanced view\nfrom [218].\nTo avoid edge cases, we assume in the following that for all x ∈Dx\neither\nP[y = 1|x] > P[y = −1|x]\nor\nP[y = 1|x] < P[y = −1|x]\n(16.2.1)\nso that (16.1.2) uniquely defines g(x) for x ∈Dx. We say that the distribution exhausts the\ndomain if Dx ∪g−1(0) = Rd. This means that every point is either in the feature support Dx or\nit belongs to the nonrelevant class. Moreover, we say that h is a Bayes classifier if\nP[h(x)|x] ≥P[−h(x)|x]\nfor all x ∈Dx.\nBy (16.1.2), the ground truth g is a Bayes classifier, and (16.2.1) ensures that h coincides with g\non Dx if h is a Bayes classifier. It is easy to see that a Bayes classifier minimizes the Bayes risk.\nWith these two notions, we now distinguish between four cases.\n(i) Bayes classifier/exhaustive distribution: If h is a Bayes classifier and the data exhausts the\ndomain, then there are no adversarial examples. This is because every x ∈Rd either belongs\nto the nonrelevant class or is classified the same by h and g.\n(ii) Bayes classifier/non-exhaustive distribution: If h is a Bayes classifier and the distribution\ndoes not exhaust the domain, then adversarial examples can exist. Even though the learned\nclassifier h coincides with the ground truth g on the feature support, adversarial examples\ncan be constructed for data points on the complement of Dx ∪g−1(0), which is not empty.\n(iii) Not a Bayes classifier/exhaustive distribution: The set Dx can be covered by the four sub-\ndomains\nC1 = h−1(1) ∩g−1(1),\nF1 = h−1(−1) ∩g−1(1),\nC−1 = h−1(−1) ∩g−1(−1),\nF−1 = h−1(1) ∩g−1(−1).\n(16.2.2)\nIf dist(C1 ∩Dx, F1 ∩Dx) or dist(C−1 ∩Dx, F−1 ∩Dx) is smaller than δ, then there exist\npoints x, x′ ∈Dx such that x′ is an adversarial example to x with perturbation δ. Hence,\nadversarial examples in the feature support can exist. This is, however, not guaranteed to\nhappen. For example, Dx does not need to be connected if g−1(0) ̸= ∅, see Exercise 16.18.\nHence, even for classifiers that have incorrect predictions on the data, adversarial examples\ndo not need to exist.\n228\n(iv) Not a Bayes classifier/non-exhaustive distribution: In this case everything is possible. Data\npoints and their associated adversarial examples can appear in the feature support of the\ndistribution and adversarial examples to elements in the feature support of the distribution\ncan be created by leaving the feature support of the distribution. We will see examples in\nthe following section.\n16.3\nAffine classifiers\nFor linear classifiers, a simple argument outlined in [223] and [73] showcases that the high-dimensionality\nof the input, common in image classification problems, is a potential cause for the existence of ad-\nversarial examples.\nA linear classifier is a map of the form\nx 7→sign(w⊤x)\nwhere w, x ∈Rd.\nLet\nx′ := x −2|w⊤x|sign(w⊤x)sign(w)\n∥w∥1\nwhere sign(w) is understood coordinate-wise. Then ∥x −x′∥∞≤2|w⊤x|/∥w∥1 and it is not hard\nto see that sign(w⊤x′) ̸= sign(w⊤x).\nFor high-dimensional vectors w, x chosen at random but possibly dependent such that w is\nuniformly distributed on a d −1 dimensional sphere, it holds with high probability that\n|w⊤x|\n∥w∥1\n≤∥x∥∥w∥\n∥w∥1\n≪∥x∥.\nThis can be seen by noting that for every c > 0\nµ({w ∈Rd | ∥w∥1 > c, ∥w∥≤1}) →1 for d →∞,\n(16.3.1)\nwhere µ is the uniform probability measure on the d-dimensional Euclidean unit ball, see Exercise\n16.17. Thus, if x has a moderate Euclidean norm, the perturbation of x′ is likely small for large\ndimensions.\nBelow we give a sufficient condition for the existence of adversarial examples, in case both h\nand the ground truth g are linear classifiers.\nTheorem 16.4. Let w, w ∈Rd be nonzero. For x ∈Rd, let h(x) = sign(w⊤x) be a classifier and\nlet g(x) = sign(w⊤x) be the ground-truth classifier.\nFor every x ∈Rd with h(x)g(x) > 0 and all ε ∈(0, |w⊤x|) such that\n|w⊤x|\n∥w∥\n> ε + |w⊤x|\n∥w∥\n|w⊤w|\n∥w∥∥w∥\n(16.3.2)\nit holds that\nx′ = x −h(x)ε + |w⊤x|\n∥w∥2\nw\n(16.3.3)\nis an adversarial example to x with perturbation δ = (ε + |w⊤x|)/∥w∥.\n229\nBefore we present the proof, we give some interpretation of this result. First, note that {x ∈\nRd | w⊤x = 0} is the decision boundary of h, meaning that points lying on opposite sides of this\nhyperplane, are classified differently by h. Due to |w⊤w| ≤∥w∥∥w∥, (16.3.2) implies that an\nadversarial example always exists whenever\n|w⊤x|\n∥w∥\n> |w⊤x|\n∥w∥.\n(16.3.4)\nThe left term is the decision margin of x for g, i.e. the distance of x to the decision boundary\nof g. Similarly, the term on the right is the decision margin of x for h. Thus we conclude that\nadversarial examples exist if the decision margin of x for the ground truth g is larger than that for\nthe classifier h.\nSecond, the term (w⊤w)/(∥w∥∥w∥) describes the alignment of the two classifiers. If the clas-\nsifiers are not aligned, i.e., w and w have a large angle between them, then adversarial examples\nexist even if the margin of the classifier is larger than that of the ground-truth classifier.\nFinally, adversarial examples with small perturbation are possible if |w⊤x| ≪∥w∥. The ex-\ntreme case w⊤x = 0 means that x lies on the decision boundary of h, and if |w⊤x| ≪∥w∥then\nx is close to the decision boundary of h.\nof Theorem 16.4. We verify that x′ in (16.3.3) satisfies the conditions of an adversarial example in\nDefinition 16.2. In the following we will use that due to h(x)g(x) > 0\ng(x) = sign(w⊤x) = sign(w⊤x) = h(x) ̸= 0.\n(16.3.5)\nFirst, it holds\n∥x −x′∥=\n\r\r\r\r\nε + |w⊤x|\n∥w∥2\nw\n\r\r\r\r = ε + |w⊤x|\n∥w∥\n= δ.\nNext we show g(x)g(x′) > 0, i.e. that (w⊤x)(w⊤x′) is positive. Plugging in the definition of\nx′, this term reads\nw⊤x\n\u0012\nw⊤x −h(x)ε + |w⊤x|\n∥w∥2\nw⊤w\n\u0013\n= |w⊤x|2 −|w⊤x|ε + |w⊤x|\n∥w∥2\nw⊤w\n≥|w⊤x|2 −|w⊤x|ε + |w⊤x|\n∥w∥2\n|w⊤w|,\n(16.3.6)\nwhere the equality holds because h(x) = g(x) = sign(w⊤x) by (16.3.5). Dividing the right-hand\nside of (16.3.6) by |w⊤x|∥w∥, which is positive by (16.3.5), we obtain\n|w⊤x|\n∥w∥−ε + |w⊤x|\n∥w∥\n|w⊤w|\n∥w∥∥w∥.\n(16.3.7)\nThe term (16.3.7) is positive thanks to (16.3.2).\nFinally, we check that 0 ̸=h(x′) ̸= h(x), i.e. (w⊤x)(w⊤x′) < 0. We have that\n(w⊤x)(w⊤x′) = |w⊤x|2 −w⊤xh(x)ε + |w⊤x|\n∥w∥2\nw⊤w\n= |w⊤x|2 −|w⊤x|(ε + |w⊤x|) < 0,\nwhere we used that h(x) = sign(w⊤x). This completes the proof.\n230\nTheorem 16.4 readily implies the following proposition for affine classifiers.\nProposition 16.5. Let w, w ∈Rd and b, b ∈R. For x ∈Rd let h(x) = sign(w⊤x + b) be a\nclassifier and let g(x) = sign(w⊤x + b) be the ground-truth classifier.\nFor every x ∈Rd with w⊤x ̸= 0, h(x)g(x) > 0, and all ε ∈(0, |w⊤x + b|) such that\n|w⊤x + b|2\n∥w∥2 + b2 > (ε + |w⊤x + b|)2\n∥w∥2 + b2\n(w⊤w + bb)2\n(∥w∥2 + b2)(∥w∥2 + b\n2)\nit holds that\nx′ = x −h(x)ε + |w⊤x + b|\n∥w∥2\nw\nis an adversarial example with perturbation δ = (ε + |w⊤x + b|)/∥w∥to x.\nThe proof is left to the reader, see Exercise 16.19.\nLet us now study two cases of linear classifiers, which allow for different types of adversarial\nexamples. In the following two examples, the ground-truth classifier g : Rd →{−1, 1} is given by\ng(x) = sign(w⊤x) for w ∈Rd with ∥w∥= 1.\nFor the first example, we construct a Bayes classifier h admitting adversarial examples in the\ncomplement of the feature support. This corresponds to case (ii) in Section 16.2.\nExample 16.6. Let D be the uniform distribution on\n{(λw, g(λw)) | λ ∈[−1, 1] \\ {0}} ⊆Rd × {−1, 1}.\nThe feature support equals\nDx = {λw | λ ∈[−1, 1] \\ {0}} ⊆span{w}.\nNext fix α ∈(0, 1) and set w := αw + (1 −α)v for some v ∈w⊥with ∥v∥= 1, so that ∥w∥= 1.\nWe let h(x) := sign(w⊤x). We now show that every x ∈Dx satisfies the assumptions of Theorem\n16.4, and therefore admits an adversarial example.\nNote that h(x) = g(x) for every x ∈Dx. Hence h is a Bayes classifier. Now fix x ∈Dx. Then\n|w⊤x| ≤α|w⊤x|, so that (16.3.2) is satisfied. Furthermore, for every ε > 0 it holds that\nδ := ε + |w⊤x|\n∥w∥\n≤ε + α.\nHence, for ε < |w⊤x| it holds by Theorem 16.4 that there exists an adversarial example with\nperturbation less than ε + α. For small α, the situation is depicted in the upper panel of Figure\n16.2.\nFor the second example, we construct a distribution with global feature support and a classifier\nwhich is not a Bayes classifier. This corresponds to case (iv) in Section 16.2.\n231\nA)\nB)\nDBg\nDBh\nx\nx′\nDBg\nDBh\nx\nx′\nFigure 16.2: Illustration of the two types of adversarial examples in Examples 16.6 and 16.7. In\npanel A) the feature support Dx corresponds to the dashed line.\nWe depict the two decision\nboundaries DBh = {x | w⊤x = 0} of h(x) = sign(w⊤x) and DBg = {x | w⊤x = 0} g(x) =\nsign(w⊤x). Both h and g perfectly classify every data point in Dx. One data point x is shifted\noutside of the support of the distribution in a way to change its label according to h. This creates\nan adversarial example x′. In panel B) the data distribution is globally supported. However, h\nand g do not coincide. Thus the decision boundaries DBh and DBg do not coincide. Moving data\npoints across DBh can create adversarial examples, as depicted by x and x′.\n232\nExample 16.7. Let Dx be a distribution on Rd with positive Lebesgue density everywhere outside\nthe decision boundary DBg = {x | w⊤x = 0} of g. We define D to be the distribution of (X, g(X))\nfor X ∼Dx. In addition, let w /∈{±w}, ∥w∥= 1 and h(x) = sign(w⊤x). We exclude w = −w\nbecause, in this case, every prediction of h is wrong. Thus no adversarial examples are possible.\nBy construction the feature support is given by Dx = Rd. Moreover, h−1({±1}) and g−1({±1})\nare half spaces, which implies that, in the notation of (16.2.2) that\ndist(C±1 ∩Dx, F±1 ∩Dx) = dist(C±1, F±1) = 0.\nHence, for every δ > 0 there is a positive probability of observing x to which an adversarial example\nwith perturbation δ exists.\nThe situation is depicted in the lower panel of Figure 16.2.\n16.4\nReLU neural networks\nSo far we discussed classification by affine classifiers. A binary classifier based on a ReLU neural\nnetwork is a function Rd ∋x 7→sign(Φ(x)), where Φ is a ReLU neural network. As noted in [223],\nthe arguments for affine classifiers, see Proposition 16.5, can be applied to the affine pieces of Φ, to\nshow existence of adversarial examples.\nConsider a ground-truth classifier g: Rd →{−1, 0, 1}. For each x ∈Rd we define the geometric\nmargin of g at x as\nµg(x) := dist(x, g−1({g(x)})c),\n(16.4.1)\ni.e., as the distance of x to the closest element that is classified differently from x or the infimum\nover all distances to elements from other classes if no closest element exists. Additionally, we denote\nthe distance of x to the closest adjacent affine piece by\nνΦ(x) := dist(x, Ac\nΦ,x),\n(16.4.2)\nwhere AΦ,x is the largest connected region on which Φ is affine and which contains x. We have the\nfollowing theorem.\nTheorem 16.8. Let Φ: Rd →R and for x ∈Rd let h(x) = sign(Φ(x)). Denote by g: Rd →\n{−1, 0, 1} the ground-truth classifier. Let x ∈Rd and ε > 0 be such that νΦ(x) > 0, g(x) ̸= 0,\n∇Φ(x) ̸= 0 and\nµg(x), νΦ(x) > ε + |Φ(x)|\n∥∇Φ(x)∥.\nThen\nx′ := x −h(x)ε + |Φ(x)|\n∥∇Φ(x)∥2 ∇Φ(x)\nis an adversarial example to x with perturbation δ = (ε + |Φ(x)|)/∥∇Φ(x)∥.\n233\nProof. We show that x′ satisfies the properties in Definition 16.2.\nBy construction ∥x −x′∥≤δ. Since µg(x) > δ it follows that g(x) = g(x′). Moreover, by\nassumption g(x) ̸= 0, and thus g(x)g(x′) > 0.\nIt only remains to show that h(x′) ̸= h(x). Since δ < νΦ(x), we have that Φ(x) = ∇Φ(x)⊤x+b\nand Φ(x′) = ∇Φ(x)⊤x′ + b for some b ∈R. Therefore,\nΦ(x) −Φ(x′) = ∇Φ(x)⊤(x −x′) = ∇Φ(x)⊤\n\u0012\nh(x)ε + |Φ(x)|\n∥∇Φ(x)∥2 ∇Φ(x)\n\u0013\n= h(x)(ε + |Φ(x)|).\nSince h(x)|Φ(x)| = Φ(x) it follows that Φ(x′) = −h(x)ε. Hence, h(x′) = −h(x), which completes\nthe proof.\nRemark 16.9. We look at the key parameters in Theorem 16.8 to understand which factors facilitate\nadversarial examples.\n• The geometric margin of the ground-truth classifier µg(x): To make the construction possible,\nwe need to be sufficiently far away from points that belong to a different class than x or to\nthe nonrelevant class.\n• The distance to the next affine piece νΦ(x): Since we are looking for an adversarial example\nwithin the same affine piece as x, we need this piece to be sufficiently large.\n• The perturbation δ: The perturbation is given by (ε + |Φ(x)|)/∥∇Φ(x)∥, which depends on\nthe classification margin |Φ(x)| of the ReLU classifier and its sensitivity to inputs ∥∇Φ(x)∥.\nFor adversarial examples to be possible, we either want a small classification margin of Φ or\na high sensitivity of Φ to its inputs.\n16.5\nRobustness\nHaving established that adversarial examples can arise in various ways under mild assumptions, we\nnow turn our attention to conditions that prevent their existence.\n16.5.1\nGlobal Lipschitz regularity\nWe have repeatedly observed in the previous sections that a large value of ∥w∥for linear classifiers\nsign(w⊤x), or ∥∇Φ(x)∥for ReLU classifiers sign(Φ(x)), facilitates the occurrence of adversarial ex-\namples. Naturally, both these values are upper bounded by the Lipschitz constant of the classifier’s\ninner functions x 7→w⊤x and x 7→Φ(x). Consequently, it was stipulated early on that bound-\ning the Lipschitz constant of the inner functions could be an effective measure against adversarial\nexamples [223].\nWe have the following result for general classifiers of the form x 7→sign(Φ(x)).\n234\nProposition 16.10. Let Φ: Rd →R be CL-Lipschitz with CL > 0, and let s > 0. Let h(x) =\nsign(Φ(x)) be a classifier, and let g: Rd →{−1, 0, 1} be a ground-truth classifier. Moreover, let\nx ∈Rd be such that\nΦ(x)g(x) ≥s.\n(16.5.1)\nThen there does not exist an adversarial example to x of perturbation δ < s/CL.\nProof. Let x ∈Rd satisfy (16.5.1) and assume that ∥x′ −x∥≤δ. The Lipschitz continuity of Φ\nimplies\n|Φ(x′) −Φ(x)| < s.\nSince |Φ(x)| ≥s we conclude that Φ(x′) has the same sign as Φ(x) which shows that x′ cannot be\nan adversarial example to x.\nRemark 16.11. As we have seen in Lemma 13.2, we can bound the Lipschitz constant of ReLU\nneural networks by restricting the magnitude and number of their weights and the number of\nlayers.\nThere has been some criticism to results of this form, see, e.g., [99], since an assumption on the\nLipschitz constant may potentially restrict the capabilities of the neural network too much. We\nnext present a result that shows under which assumptions on the training set, there exists a neural\nnetwork that classifies the training set correctly, but does not allow for adversarial examples within\nthe training set.\nTheorem 16.12. Let m ∈N, let g: Rd →{−1, 0, 1} be a ground-truth classifier, and let\n(xi, g(xi))m\ni=1 ∈(Rd × {−1, 1})m. Assume that\nsup\ni̸=j\n|g(xi) −g(xj)|\n∥xi −xj∥\n=: f\nM > 0.\nThen there exists a ReLU neural network Φ with depth(Φ) = O(log(m)) and width(Φ) = O(dm)\nsuch that for all i = 1, . . . , m\nsign(Φ(xi)) = g(xi)\nand there is no adversarial example of perturbation δ = 1/f\nM to xi.\nProof. The result follows directly from Theorem 9.6 and Proposition 16.10. The reader is invited\nto complete the argument in Exercise 16.20.\n16.5.2\nLocal regularity\nOne issue with upper bounds involving global Lipschitz constants such as those in Proposition\n16.10, is that these bounds may be quite large for deep neural networks. For example, the upper\n235\nbound given in Lemma 13.2 is\n∥Φ(x) −Φ(x′)∥∞≤CL\nσ · (Bdmax)L+1∥x −x′∥∞\nwhich grows exponentially with the depth of the neural network. However, in practice this bound\nmay be pessimistic, and locally the neural network might have significantly smaller gradients than\nthe global Lipschitz constant.\nBecause of this, it is reasonable to study results preventing adversarial examples under local\nLipschitz bounds. Such a result together with an algorithm providing bounds on the local Lipschitz\nconstant was proposed in [88]. We state the theorem adapted to our set-up.\nTheorem 16.13. Let h: Rd →{−1, 1} be a classifier of the form h(x) = sign(Φ(x)) and let\ng: Rd →{−1, 0, 1} be the ground-truth classifier. Let x ∈Rd satisfy g(x) ̸= 0, and set\nα := max\nR>0 min\n\n\n\n\n\nΦ(x)g(x)\n.\nsup\n∥y−x∥∞≤R\ny̸=x\n|Φ(y) −Φ(x)|\n∥x −y∥∞\n, R\n\n\n\n\n\n,\n(16.5.2)\nwhere the minimum is understood to be R in case the supremum is zero. Then there are no adver-\nsarial examples to x with perturbation δ < α.\nProof. Let x ∈Rd be as in the statement of the theorem. Assume, towards a contradiction, that\nfor 0 < δ < α satisfying (16.5.2), there exists an adversarial example x′ to x with perturbation δ.\nIf the supremum in (16.5.2) is zero, then Φ is constant on a ball of radius R around x. In\nparticular for ∥x′ −x∥≤δ < R holds h(x′) = h(x) and x′ cannot be an adversarial example.\nNow assume the supremum in (16.5.2) is not zero. It holds by (16.5.2), that\nδ < Φ(x)g(x)\n.\nsup\n∥y−x∥∞≤R\ny̸=x\n|Φ(y) −Φ(x)|\n∥x −y∥∞\n.\n(16.5.3)\nMoreover,\n|Φ(x′) −Φ(x)| ≤\nsup\n∥y−x∥∞≤R\ny̸=x\n|Φ(y) −Φ(x)|\n∥x −y∥∞\n∥x −x′∥∞\n≤\nsup\n∥y−x∥∞≤R\ny̸=x\n|Φ(y) −Φ(x)|\n∥x −y∥∞\nδ < Φ(x)g(x),\nwhere we applied (16.5.3) in the last line. It follows that\ng(x)Φ(x′) = g(x)Φ(x) + g(x)(Φ(x′) −Φ(x))\n≥g(x)Φ(x) −|Φ(x′) −Φ(x)| > 0.\nThis rules out x′ as an adversarial example.\n236\nThe supremum in (16.5.2) is bounded by the Lipschitz constant of Φ on BR(x). Thus Theorem\n16.13 depends only on the local Lipschitz constant of Φ. One obvious criticism of this result is\nthat the computation of (16.5.2) is potentially prohibitive. We next show a different result, for\nwhich the assumptions can immediately be checked by applying a simple algorithm that we present\nsubsequently.\nTo state the following proposition, for a continuous function Φ : Rd →R and δ > 0 we define\nfor x ∈Rd and δ > 0\nzδ,max := max{Φ(y) | ∥y −x∥∞≤δ}\n(16.5.4)\nzδ,min := min{Φ(y) | ∥y −x∥∞≤δ}.\n(16.5.5)\nProposition 16.14. Let h: Rd →{−1, 1} be a classifier of the form h(x) = sign(Φ(x)) and\ng: Rd →{−1, 0, 1}, let x be such that h(x) = g(x). Then x does not have an adversarial example\nof perturbation δ if zδ,maxzδ,min > 0.\nProof. The proof is immediate, since zδ,maxzδ,min > 0 implies that all points in a δ neighborhood\nof x are classified the same.\nTo apply (16.14), we only have to compute zδ,max and zδ,min. It turns out that if Φ is a neural\nnetwork, then zδ,max, zδ,min can be approximated by a computation similar to a forward pass of\nΦ. Denote by |A| the matrix obtained by taking the absolute value of each entry of the matrix A.\nAdditionally, we define\nA+ = (|A| + A)/2 and A−= (|A| −A)/2.\nThe idea behind the Algorithm 2 is common in the area of neural network verification, see, e.g.,\n[66, 61, 7, 238].\nRemark 16.15. Up to constants, Algorithm 2 has the same computational complexity as a forward\npass, also see Algorithm 1. In addition, in contrast to upper bounds based on estimating the global\nLipschitz constant of Φ via its weights, the upper bounds found via Algorithm 2 include the effect of\nthe activation function σ. For example, if σ is the ReLU, then we may often end up in a situation,\nwhere δ(ℓ),up or δ(ℓ),low can have many entries that are 0. If an entry of W (ℓ)x(ℓ)+b(ℓ) is nonpositive,\nthen it is guaranteed that the associated entry in δ(ℓ),low will be zero. Similarly, if W (ℓ) has only\nfew positive entries, then most of the entries of δ(ℓ),up are not propagated to δ(ℓ+1),up.\nNext, we prove that Algorithm 2 indeed produces sensible output.\nProposition 16.16. Let Φ be a neural network with weight matrices W (ℓ) ∈Rdℓ+1×dℓand bias\nvectors b(ℓ) ∈Rdℓ+1 for ℓ= 0, . . . , L, and a monotonically increasing activation function σ.\nLet x ∈Rd. Then the output of Algorithm 2 satisfies\nxL+1 + δ(L+1),up > zδ,max and xL+1 −δ(L+1),low < zδ,min.\n237\nAlgorithm 2 Compute Φ(x), zδ,max and zδ,min for a given neural network\nInput: weight matrices W (ℓ) ∈Rdℓ+1×dℓand bias vectors b(ℓ) ∈Rdℓ+1 for ℓ= 0, . . . , L with\ndL+1 = 1, monotonous activation function σ, input vector x ∈Rd0, neighborhood size δ > 0\nOutput: Bounds for zδ,max and zδ,min\nx(0) = x\nδ(0),up = δ1 ∈Rd0\nδ(0),low = δ1 ∈Rd0\nfor ℓ= 0, . . . , L −1 do\nx(ℓ+1) = σ(W (ℓ)x(ℓ) + b(ℓ))\nδ(ℓ+1),up = σ(W (ℓ)x(ℓ) + (W (ℓ))+δ(ℓ),up + (W (ℓ))−δ(ℓ),low + b(ℓ)) −x(ℓ+1)\nδ(ℓ+1),low = x(ℓ+1) −σ(W (ℓ)x(ℓ) −(W (ℓ))+δ(ℓ),low −(W (ℓ))−δ(ℓ),up + b(ℓ))\nend for\nx(L+1) = W (L)x(L) + b(L)\nδ(L+1),up = (W (L))+δ(L),up + (W (L))−δ(L),low\nδ(L+1),low = (W (L))+δ(L),low + (W (L))−δ(L),up\nreturn x(L+1), x(L+1) + δ(L+1),up, x(L+1) −δ(L+1),low\nProof. Fix y, x ∈Rd with ∥y −x∥∞≤δ and let y(ℓ), x(ℓ) for ℓ= 0, . . . , L + 1 be as in Algorithm\n2 applied to y, x, respectively. Moreover, let δℓ,up, δℓ,low for ℓ= 0, . . . , L + 1 be as in Algorithm 2\napplied to x. We will prove by induction over ℓ= 0, . . . , L + 1 that\ny(ℓ) −x(ℓ) ≤δℓ,up\nand\nx(ℓ) −y(ℓ) ≤δℓ,low,\n(16.5.6)\nwhere the inequalities are understood entry-wise for vectors. Since y was arbitrary this then proves\nthe result.\nThe case ℓ= 0 follows immediately from ∥y −x∥∞≤δ. Assume now, that the statement was\nshown for ℓ< L. We have that\ny(ℓ+1) −x(ℓ+1) −δℓ+1,up =σ(W (ℓ)y(ℓ) + b(ℓ))\n−σ\n\u0000W (ℓ)x(ℓ) + (W (ℓ))+δ(ℓ),up + (W (ℓ))−δ(ℓ),low + b(ℓ)\u0001\n.\nThe monotonicity of σ implies that\ny(ℓ+1) −x(ℓ+1) ≤δℓ+1,up\nif\nW (ℓ)y(ℓ) ≤W (ℓ)x(ℓ) + (W (ℓ))+δ(ℓ),up + (W (ℓ))−δ(ℓ),low.\n(16.5.7)\nTo prove (16.5.7), we observe that\nW (ℓ)(y(ℓ) −x(ℓ)) = (W (ℓ))+(y(ℓ) −x(ℓ)) −(W (ℓ))−(y(ℓ) −x(ℓ))\n= (W (ℓ))+(y(ℓ) −x(ℓ)) + (W (ℓ))−(x(ℓ) −y(ℓ))\n≤(W (ℓ))+δ(ℓ),up + (W (ℓ))−δ(ℓ),low,\n238\nwhere we used the induction assumption in the last line. This shows the first estimate in (16.5.6).\nSimilarly,\nx(ℓ+1) −y(ℓ+1) −δℓ+1,low\n= σ(W (ℓ)x(ℓ) −(W (ℓ))+δ(ℓ),low −(W (ℓ))−δ(ℓ),up + b(ℓ)) −σ(W (ℓ)y(ℓ) + b(ℓ)).\nHence, x(ℓ+1) −y(ℓ+1) ≤δℓ+1,low if\nW (ℓ)y(ℓ) ≥W (ℓ)x(ℓ) −(W (ℓ))+δ(ℓ),low −(W (ℓ))−δ(ℓ),up.\n(16.5.8)\nTo prove (16.5.8), we observe that\nW (ℓ)(x(ℓ) −y(ℓ)) = (W (ℓ))+(x(ℓ) −y(ℓ)) −(W (ℓ))−(x(ℓ) −y(ℓ))\n= (W (ℓ))+(x(ℓ) −y(ℓ)) + (W (ℓ))−(y(ℓ) −x(ℓ))\n≤(W (ℓ))+δ(ℓ),low + (W (ℓ))−δ(ℓ),up,\nwhere we used the induction assumption in the last line. This completes the proof of (16.5.6) for\nall ℓ≤L.\nThe case ℓ= L + 1 follows by the same argument, but replacing σ by the identity.\nBibliography and further reading\nThis chapter begins with the foundational paper [223], but it should be remarked that adversarial\nexamples for non-deep-learning models in machine learning were studied earlier in [98].\nThe results in this chapter are inspired by various results in the literature, though they may\nnot be found in precisely the same form. The overall setup is inspired by [223]. The explanation\nbased on the high-dimensionality of the data given in Section 16.3 was first formulated in [223] and\n[73]. The formalism reviewed in Section 16.2 is inspired by [218]. The results on robustness via\nlocal Lipschitz properties are due to [88]. Algorithm 2 is covered by results in the area of network\nverifiability [66, 61, 7, 238]. For a more comprehensive overview of modern approaches, we refer to\nthe survey article [193].\nImportant directions not discussed in this chapter are the transferability of adversarial ex-\namples, defense mechanisms, and alternative adversarial operations. Transferability refers to the\nphenomenon that adversarial examples for one model often also fool other models, [170, 153]. De-\nfense mechanisms, i.e., techniques for specifically training a neural network to prevent adversarial\nexamples, include for example the Fast Gradient Sign Method of [73], and more sophisticated recent\napproaches such as [32]. Finally, adversarial examples can be generated not only through additive\nperturbations, but also through smooth transformations of images, as demonstrated in [1, 243].\n239\nExercises\nExercise 16.17. Prove (16.3.1) by comparing the volume of the d-dimensional Euclidean unit ball\nwith the volume of the d-dimensional 1-ball of radius c for a given c > 0.\nExercise 16.18. Fix δ > 0. For a pair of classifiers h and g such that C1∪C−1 = ∅in (16.2.2), there\ntrivially cannot exist any adversarial examples. Construct an example, of h, g, D such that C1,\nC−1 ̸= ∅, h is not a Bayes classifier, and g is such that no adversarial examples with a perturbation\nδ exist.\nIs this also possible if g−1(0) = ∅?\nExercise 16.19. Prove Proposition 16.5.\nHint: Repeat the proof of Theorem 16.4. In the first part set x(ext) = (x, 1), w(ext) = (w, b)\nand w(ext) = (w, b). Then show that h(x′) ̸= h(x) by plugging in the definition of x′.\nExercise 16.20. Complete the proof of Theorem 16.12.\n240\nAppendix A\nProbability theory\nThis appendix provides some basic notions and results in probability theory required in the main\ntext. It is intended as a revision for a reader already familiar with these concepts. For more details\nand proofs, we refer for example to the standard textbook [117].\nA.1\nSigma-algebras, topologies, and measures\nLet Ωbe a set, and denote by 2Ωthe powerset of Ω.\nDefinition A.1. A subset A ⊆2Ωis called a sigma-algebra1 on Ωif it satisfies\n(i) Ω∈A,\n(ii) Ac ∈A whenever A ∈A,\n(iii) S\ni∈N Ai ∈A whenever Ai ∈A for all i ∈N.\nFor a sigma-algebra A on Ω, the tuple (Ω, A) is also referred to as a measurable space. For a\nmeasurable space, a subset A ⊆Ωis called measurable, if A ∈A. Measurable sets are also called\nevents.\nAnother key system of subsets of Ωis that of a topology.\nDefinition A.2. A subset A ⊆2Ωis called a topology on Ωif it satisfies\n(i) ∅, Ω∈T,\n(ii) Tn\nj=1 Oj ∈T whenever n ∈N and O1, . . . , On ∈T,\n(iii) S\ni∈I Oi ∈T whenever for an index set I holds Oi ∈T for all i ∈I.\nIf T is a topology on Ω, we call (Ω, T) a topological space, and a set O ⊆Ωis called open if and\nonly if O ∈T.\n241\nRemark A.3. The two notions differ in that a topology allows for unions of arbitrary (possibly un-\ncountably many) sets, but only for finite intersection, whereas a sigma-algebra allows for countable\nunions and intersections.\nExample A.4. Let d ∈N and denote by Bε(x) = {y ∈Rd | ∥y −x∥< ε} the set of points\nwhose Euclidean distance to x is less than ε. Then for every A ⊆Rd, the smallest topology on A\ncontaining A ∩Bε(x) for all ε > 0, x ∈Rd, is called the Euclidean topology on A.\nIf (Ω, T) is a topological space, then the Borel sigma-algebra refers to the smallest sigma-\nalgebra on Ωcontaining all open sets, i.e. all elements of T. Throughout this book, subsets of Rd\nare always understood to be equipped with the Euclidean topology and the Borel sigma-algebra.\nThe Borel sigma-algebra on Rd is denoted by Bd.\nWe can now introduce measures.\nDefinition A.5. Let (Ω, A) be a measurable space. A mapping µ : A →[0, ∞] is called a measure\nif it satisfies\n(i) µ(∅) = 0,\n(ii) for every sequence (Ai)i∈N ⊆A such that Ai ∩Aj = ∅whenever i ̸= j, it holds\nµ\n\u0010 [\ni∈N\nAi\n\u0011\n=\nX\ni∈N\nµ(Ai).\nWe say that the measure is finite if µ(Ω) < ∞, and it is sigma-finite if there exists a sequence\n(Ai)i∈N ⊆A such that Ω= S\ni∈N Ai and µ(Ai) < 1 for all i ∈N. In case µ(Ω) = 1, the measure is\ncalled a probability measure.\nExample A.6. One can show that there exists a unique measure λ on (Rd, Bd), such that for all\nsets of the type ×d\nj=1[ai, bi) with −∞< ai ≤bi < ∞holds\nλ(×d\ni=1[ai, bi)) =\nd\nY\ni=1\n(bi −ai).\nThis measure is called the Lebesgue measure.\nIf µ is a measure on the measurable space (Ω, A), then the triplet (Ω, A, µ) is called a measure\nspace. In case µ is a probability measure, it is called a probability space.\nLet (Ω, A, µ) be a measure space. A subset N ⊆Ωis called a null-set, if N is measurable and\nµ(N) = 0. Moreover, an equality or inequality is said to hold µ-almost everywhere or µ-almost\nsurely, if it is satisfied on the complement of a null-set. In case µ is clear from context, we simply\nwrite “almost everywhere” or “almost surely” instead. Usually this refers to the Lebesgue measure.\nA.2\nRandom variables\nA.2.1\nMeasurability of functions\nTo define random variables, we first need to recall the measurability of functions.\n242\nDefinition A.7. Let (Ω1, A1) and (Ω2, A2) be two measurable spaces. A function f : Ω1 →Ω2 is\ncalled measurable if\nf−1(A2) := {ω ∈Ω1 | f(ω) ∈A2} ∈A1\nfor all A2 ∈A2.\nA mapping X : Ω1 →Ω2 is called a Ω2-valued random variable if it is measurable.\nRemark A.8. We again point out the parallels to topological spaces: A function f : Ω1 →Ω2\nbetween two topological spaces (Ω1, T1) and (Ω2, T2) is called continuous if f−1(O2) ∈T1 for all\nO2 ∈T2.\nLet Ω1 be a set and let (Ω2, A2) be a measurable space. For X : Ω1 →Ω2, we can ask for\nthe smallest sigma-algebra AX on Ω1, such that X is measurable as a mapping from (Ω1, AX) to\n(Ω2, A2). Clearly, for every sigma-algebra A1 on Ω1, X is measurable as a mapping from (Ω1, A1)\nto (Ω2, A2) if and only if every A ∈AX belongs to A1; or in other words, AX is a sub sigma-algebra\nof A1. It is easy to check that AX is given through the following definition.\nDefinition A.9. Let X : Ω1 →Ω2 be a random variable. Then\nAX := {X−1(A2) | A2 ∈A2} ⊆2Ω1\nis the sigma-algebra induced by X on Ω1.\nA.2.2\nDistribution and expectation\nNow let (Ω1, A1, P) be a probability space, and let (Ω2, A2) be a measurable space. Then X naturally\ninduces a measure on (Ω2, A2) via\nPX[A2] := P[X−1(A2)]\nfor all A2 ∈A2.\nNote that due to the measurability of X it holds X−1(A2) ∈A1, so that PX is well-defined.\nDefinition A.10. The measure PX is called the distribution of X. If (Ω2, A2) = (Rd, Bd), and\nthere exists a function fX : Rd →R such that\nP[A] =\nZ\nA\nfX(x) dx\nfor all A ∈Bd,\nthen fX is called the (Lebesgue) density of X.\nRemark A.11. The term distribution is often used without specifying an underlying probability\nspace and random variable. In this case, “distribution” stands interchangeably for “probability\n243\nmeasure”. For example, µ is a distribution on Ω2 states that µ is a probability measure on the\nmeasurable space (Ω2, A2). In this case, there always exists a probability space (Ω1, A1, P) and a\nrandom variable X : Ω1 →Ω2 such that PX = µ; namely (Ω1, A1, P) = (Ω2, A2, µ) and X(ω) = ω.\nExample A.12. Some important distributions include the following.\n• Bernoulli distribution: A random variable X : Ω→{0, 1} is Bernoulli distributed if there\nexists p ∈[0, 1] such that P[X = 1] = p and P[X = 0] = 1 −p.\n• Uniform distribution: A random variable X : Ω→Rd is uniformly distributed on a\nmeasurable set A ∈Bd, if its density equals\nfX(x) = 1\n|A|1A(x)\nwhere |A| < ∞is the Lebesgue measure of A.\n• Gaussian distribution: A random variable X : Ω→Rd is Gaussian distributed with mean\nm ∈Rd and the regular covariance matrix C ∈Rd×d, if its density equals\nfX(x) =\n1\n(2π det(C))d/2 exp\n\u0012\n−1\n2(x −m)⊤C−1(x −m)\n\u0013\n.\nWe denote this distribution by N(m, C).\nLet (Ω, A, P) be a probability space, let X : Ω→Rd be an Rd-valued random variable. We then\ncall the Lebesgue integral\nE[X] :=\nZ\nΩ\nX(ω) dP(ω) =\nZ\nRd x dPX(x)\n(A.2.1)\nthe expectation of X. Moreover, for k ∈N we say that X has finite k-th moment if E[∥X∥k] <\n∞. Similarly, for a probability measure µ on Rd and k ∈N, we say that µ has finite k-th moment\nif\nZ\nRd ∥x∥k dµ(x) < ∞.\nFurthermore, the matrix\nZ\nΩ\n(X(ω) −E[X])(X(ω) −E[X])⊤dP(ω) ∈Rd×d\nis the covariance of X : Ω→Rd. For d = 1, it is called the variance of X and denoted by V[X].\nFinally, we recall different variants of convergence for random variables.\nDefinition A.13. Let (Ω, A, P) be a probability space, and let Xj : Ω→Rd, j ∈N, be a sequence\nof random variables and let X : Ω→Rd also be a random variable. The sequence is said to\n(i) converge almost surely to X, if\nP\n\u0014\u001a\nω ∈Ω\n\f\f\f\f lim\nj→∞Xj(ω) = X(ω)\n\u001b\u0015\n= 1,\n244\n(ii) converge in probability to X, if\nfor all ε > 0 :\nlim\nj→∞P [{ω ∈Ω| |Xj(ω) −X(ω)| > ε}] = 0,\n(iii) converge weakly to X, if for all bounded continuous functions f : Rd →R holds\nlim\nj→∞E[f ◦Xj] = E[f ◦X].\nThe notions in Definition A.13 are ordered by decreasing strength, i.e. almost sure convergence\nimplies convergence in probability, and convergence in probability implies weak convergence, see\nfor example [117, Chapter 13]. Since E[f ◦X] =\nR\nRd f(x) dPX(x), the notion of weak convergence\nonly depends on the distribution PX of X. We thus also say that a sequence of random variables\nconverges weakly towards a measure µ.\nA.3\nConditionals, marginals, and independence\nIn this section, we concentrate on Rd-valued random variables, although the following concepts can\nbe extended to more general spaces.\nA.3.1\nJoint and marginal distribution\nLet again (Ω, A, P) be a probability space, and let X : Ω→RdX, Y : Ω→RdY be two random\nvariables. Then\nZ := (X, Y ) : Ω→RdX+dY\nis also a random variable. Its distribution PZ is a measure on the measurable space (RdX+dY , BdX+dY ),\nand PZ is referred to as the joint distribution of X and Y . On the other hand, PX, PY are called\nthe marginal distributions of X, Y . Note that\nPX[A] = PZ[A × RdY ]\nfor all A ∈BdX,\nand similarly for PY . Thus the marginals PX, PY , can be constructed from the joint distribution\nPZ. In turn, knowledge of the marginals is not sufficient to construct the joint distribution.\nA.3.2\nIndependence\nThe concept of independence serves to formalize the situation, where knowledge of one random\nvariable provides no information about another random variable. We first give the formal definition,\nand afterwards discuss the roll of a die as a simple example.\n245\nDefinition A.14. Let (Ω, A, P) be a probability space. Then two events A, B ∈A are called\nindependent if\nP[A ∩B] = P[A]P[B].\nTwo random variables X : Ω→RdX and Y : Ω→RdY are called independent, if\nA, B are independent for all A ∈AX, B ∈AY .\nTwo random variables are thus independent, if and only if all events in their induced sigma-\nalgebras are independent. This turns out to be equivalent to the joint distribution P(X,Y ) being\nequal to the product measure PX ⊗PY ; the latter is characterized as the unique measure µ on\nRdX+dY satisfying µ(A × B) = PX[A]PY [B] for all A ∈Bdx, B ∈BdY .\nExample A.15. Let Ω= {1, . . . , 6} represent the outcomes of rolling a fair die, let A = 2Ωbe the\nsigma-algebra, and let P[ω] = 1/6 for all ω ∈Ω. Consider the three random variables\nX1(ω) =\n(\n0\nif ω is odd\n1\nif ω is even\nX2(ω) =\n(\n0\nif ω ≤3\n1\nif ω ≥4\nX3(ω) =\n\n\n\n\n\n0\nif ω ∈{1, 2}\n1\nif ω ∈{3, 4}\n2\nif ω ∈{5, 6}.\nThese random variables can be interpreted as follows:\n• X1 indicates whether the roll yields an odd or even number.\n• X2 indicates whether the roll yields a number at most 3 or at least 4.\n• X3 categorizes the roll into one of the groups {1, 2}, {3, 4} or {5, 6}.\nThe induced sigma-algebras are\nAX1 = {∅, Ω, {1, 3, 5}, {2, 4, 6}}\nAX2 = {∅, Ω, {1, 2, 3}, {4, 5, 6}}\nAX3 = {∅, Ω, {1, 2}, {3, 4}, {5, 6}, {1, 2, 3, 4}, {1, 2, 5, 6}, {3, 4, 5, 6}}.\nWe leave it to the reader to formally check that X1 and X2 are not independent, but X1 and X3\nare independent. This reflects the fact that, for example, knowing the outcome to be odd, makes\nit more likely that the number belongs to {1, 2, 3} rather than {4, 5, 6}. However, this knowledge\nprovides no information on the three categories {1, 2}, {3, 4}, and {5, 6}.\nIf X : Ω→R, Y : Ω→R are two independent random variables, then, due to P(X,Y ) = PX ⊗PY\nE[XY ] =\nZ\nΩ\nX(ω)Y (ω) dP(ω)\n=\nZ\nR2 xy dP(X,Y )(x, y)\n=\nZ\nR\nx dPX(x)\nZ\nR\ny dPX(y)\n= E[X]E[Y ].\n246\nUsing this observation, it is easy to see that for a sequence of independent R-valued random variables\n(Xi)n\ni=1 with bounded second moments, there holds Bienaym´e’s identity\nV\n\" n\nX\ni=1\nXi\n#\n=\nn\nX\ni=1\nV [Xi] .\n(A.3.1)\nA.3.3\nConditional distributions\nLet (Ω, A, P) be a probability space, and let A, B ∈A be two events. In case P[B] > 0, we define\nP[A|B] := P[A ∩B]\nP[B]\n,\n(A.3.2)\nand call P[A|B] the conditional probability of A given B.\nExample A.16. Consider the setting of Example A.15. Let A = {ω ∈Ω| X1(ω) = 0} be the event\nthat the outcome of the die roll was an odd number and let B = {ω ∈Ω| X2(ω) = 0} be the event\nthat the outcome yielded a number at most 3. Then P[B] = 1/2, and P[A ∩B] = 1/3. Thus\nP[A|B] = P[A ∩B]\nP[B]\n= 1/3\n1/2 = 2\n3.\nThis reflects that, given we know the outcome to be at most 3, the probability of the number being\nodd, i.e. in {1, 3}, is larger than the probability of the number being even, i.e. equal to 2.\nThe conditional probability in (A.3.2) is only well-defined if P[B] > 0. In practice, we often\nencounter the case where we would like to condition on an event of probability zero.\nExample A.17. Consider the following procedure: We first draw a random number p ∈[0, 1]\naccording to a uniform distribution on [0, 1]. Afterwards we draw a random number X ∈{0, 1}\naccording to a p-Bernoulli distribution, i.e. P[X = 1] = p and P[X = 0] = 1 −p. Then (p, X) is\na joint random variable taking values in [0, 1] × {0, 1}. What is P[X = 1|p = 0.5] in this case?\nIntuitively, it should be 1/2, but note that P[p = 0.5] = 0, so that (A.3.2) is not meaningful here.\nDefinition A.18 (regular conditional distribution). Let (Ω, A, P) be a probability space, and let\nX : Ω→RdX and Y : Ω→RdY be two random variables. Let τX|Y : BdX × RdY →[0, 1] satisfy\n(i) y 7→τX|Y (A, y) : RdY →[0, 1] is measurable for every fixed A ∈BdX,\n(ii) A 7→τX|Y (A, y) is a probability measure on (RdX, BdX) for every y ∈Y (Ω),\n(iii) for all A ∈BdX and all B ∈BdY holds\nP[X ∈A, Y ∈B] =\nZ\nB\nτX|Y (A, y)PY (y).\nThen τ is called a regular (version of the) conditional distribution of X given Y . In this\ncase, we denote\nP[X ∈A|Y = y] := τX|Y (A, y),\nand refer to this measure as the conditional distribution of X|Y = y.\n247\nDefinition A.18 provides a mathematically rigorous way of assigning a distribution to a random\nvariable conditioned on an event that may have probability zero, as in Example A.17. Existence\nand uniqueness of these conditional distributions hold in the following sense, see for example [117,\nChapter 8] or [201, Chapter 3] for the specific statement given here.\nTheorem A.19. Let (Ω, A, P) be a probability space, and let X : Ω→RdX, Y : Ω→RdY be two\nrandom variables. Then there exists a regular version of the conditional distribution τ1.\nLet τ2 be another regular version of the conditional distribution. Then there exists a PY -null\nset N ⊆RdY , such that for all y ∈Nc ∩Y (Ω), the two probability measures τ1(·, y) and τ2(·, y)\ncoincide.\nIn particular, conditional distributions are only well-defined in a PY -almost everywhere sense.\nDefinition A.20. Let (Ω, A, P) be a probability space, and let X : Ω→RdX, Y : Ω→RdY ,\nZ : Ω→RdZ be three random variables. We say that X and Z are conditionally independent\ngiven Y , if the two distributions X|Y = y and Z|Y = y are independent for PY -almost every\ny ∈Y (Ω).\nA.4\nConcentration inequalities\nLet Xi : Ω→R, i ∈N, be a sequence of random variables with finite first moments. The centered\naverage over the first n terms\nSn := 1\nn\nn\nX\ni=1\n(Xi −E[Xi])\n(A.4.1)\nis another random variable, and by linearity of the expectation it holds E[Sn] = 0. The sequence\nis said to satisfy the strong law of large numbers if\nP\nh\nlim sup\nn→∞|Sn| = 0\ni\n= 1.\nThis is for example the case if there exists C < ∞such that V[Xi] ≤C for all i ∈N. Concentration\ninequalities provide bounds on the rate of this convergence.\nWe start with Markov’s inequality.\nLemma A.21 (Markov’s inequality). Let X : Ω→R be a random variable, and let φ : [0, ∞) →\n[0, ∞) be monotonically increasing. Then for all ε > 0\nP[|X| ≥ε] ≤E[φ(|X|)]\nφ(ε)\n.\n248\nProof. We have\nP[|X| ≥ε] =\nZ\nX−1([ε,∞))\n1 dP(ω) ≤\nZ\nΩ\nφ(|X(ω)|)\nφ(ε)\ndP(ω) = E[φ(|X|)]\nφ(ε)\n,\nwhich gives the claim.\nApplying Markov’s inequality with φ(x) := x2 to the random variable X −E[X] directly gives\nChebyshev’s inequality.\nLemma A.22 (Chebyshev’s inequality). Let X : Ω→R be a random variable with finite variance.\nThen for all ε > 0\nP[|X −E[X]| ≥ε] ≤V[X]\nε2 .\nFrom Chebyshev’s inequality we obtain the next result, which is a quite general concentration\ninequality for random variables with finite variances.\nTheorem A.23. Let X1, . . . , Xn be n ∈N independent real-valued random variables such that for\nsome ς > 0 holds E[|Xi −µ|2] ≤ς2 for all i = 1, . . . , n. Denote\nµ := E\nh 1\nn\nn\nX\nj=1\nXj\ni\n.\n(A.4.2)\nThen for all ε > 0\nP\n\"\f\f\f 1\nn\nn\nX\nj=1\nXi −µ\n\f\f\f ≥ε\n#\n≤ς2\nε2n.\nProof. Let Sn = Pn\nj=1(Xi −E[Xi])/n = (Pn\nj=1 Xi)/n−µ. By Bienaym´e’s identity (A.3.1), it holds\nthat\nV[Sn] = 1\nn2\nn\nX\nj=1\nE[(Xi −E[Xi])2] ≤ς2\nn .\nSince E[Sn] = 0, Chebyshev’s inequality applied to Sn gives the statement.\nIf we have additional information about the random variables, then we can derive sharper\nbounds.\nIn case of uniformly bounded random variables (rather than just bounded variance),\nHoeffding’s inequality, which we recall next, shows an exponential rate of concentration around the\nmean.\n249\nTheorem A.24 (Hoeffding’s inequality). Let a, b ∈R. Let X1, . . . , Xn be n ∈N independent\nrandom real-valued variables such that a ≤Xi ≤b almost surely for all i = 1, . . . , n, and let µ be\nas in (A.4.2). Then, for every ε > 0\nP\n\n\n\f\f\f\f\f\f\n1\nn\nn\nX\nj=1\nXj −µ\n\f\f\f\f\f\f\n> ε\n\n≤2e\n−2nε2\n(b−a)2 .\nA proof can, for example, be found in [212, Section B.4], where this version is also taken from.\nFinally, we recall the central limit theorem, in its multivariate formulation. We say that (Xj)j∈N\nis an i.i.d. sequence of random variables, if the random variables are (pairwise) independent\nand identically distributed. For a proof see [117, Theorem 15.58].\nTheorem A.25 (Multivariate central limit theorem). Let (Xn)n∈N be an i.i.d. sequence of Rd-\nvalued random variables, such that E[Xn] = 0 ∈Rd and E[Xn,iXn,j] = Cij for all i, j = 1, . . . , d.\nLet\nYn := X1 + · · · + Xn\n√n\n.\nThen Yn converges weakly to N(0, C) as n →∞.\n250\nAppendix B\nFunctional analysis\nThis appendix provides some basic notions and results in functional analysis required in the main\ntext. It is intended as a revision for a reader already familiar with these concepts. For more details\nand proofs, we refer for example to the standard textbooks [195, 196, 41, 77].\nB.1\nVector spaces\nDefinition B.1. Let K ∈{R, C}. A vector space (over K) is a set X such that the following\nholds:\n(i) Properties of addition: For every x, y ∈X there exists x + y ∈X such that for all z ∈X\nx + y = y + x\nand\nx + (y + z) = (x + y) + z.\nMoreover, there exists a unique element 0 ∈X such that x + 0 = x for all x ∈X and for each\nx ∈X there exists a unique −x ∈X such that x + (−x) = 0.\n(ii) Properties of scalar multiplication: There exists a map (α, x) 7→αx from K × X to X called\nscalar multiplication. It satisfies 1x = x and (αβ)x = α(βx) for all x ∈X.\nWe call the elements of a vector space vectors.\nIf the field is clear from context, we simply refer to X as a vector space. We will primarily consider\nthe case K = R, and in this case we also say that X is a real vector space.\nTo introduce a notion of convergence on a vector space X, it needs to be equipped with a\ntopology, see Definition A.2.\nA topological vector space is a vector space which is also a\ntopological space, and in which addition and scalar multiplication are continuous maps. We next\ndiscuss the most important instances of topological vector spaces.\nB.1.1\nMetric spaces\nAn important class of topological vector spaces consists of vector spaces that are also metric spaces.\n251\nDefinition B.2. For a set X, we call a map dX : X × X →R+ a metric, if\n(i) 0 ≤dX(x, y) < ∞for all x, y ∈X,\n(ii) dX(x, y) = 0 if and only if x = y,\n(iii) dX(x, y) = d(y, x) for all x, y ∈X,\n(iv) dX(x, z) ≤dX(x, y) + dX(y, z) for all x, y, z ∈X.\nWe call (X, dX) a metric space.\nIn a metric space (X, dX), we denote the open ball with center x and radius r > 0 by\nBr(x) := {y ∈X | dX(x, y) < r}.\n(B.1.1)\nEvery metric space is naturally equipped with a topology: A set A ⊆X is open if and only if for\nevery x ∈A exists ε > 0 such that Bε(x) ⊆A. Therefore every metric vector space is a topological\nvector space.\nDefinition B.3. A metric space (X, dX) is called complete, if every Cauchy sequence with respect\nto d converges to an element in X.\nFor complete metric spaces, an immensely powerful tool is Baire’s category theorem. To state\nit, we require the notion of density of sets. Let A, B ⊆X for a topological space X. Then A is\ndense in B if the closure of A, denoted by A, satisfies A ⊇B.\nTheorem B.4 (Baire’s category theorem). Let X be a complete metric space. Then the intersection\nof every countable collection of dense open subsets of X dense in X.\nTheorem B.4 implies that if X = S∞\ni=1 Vi for a sequence of sets Vi, then at least one of the Vi\nhas to contain an open set. Indeed, assuming all Vi’s have empty interior implies that V c\ni = X \\ Vi\nis dense for all i ∈N. By De Morgan’s laws, it then holds that ∅= T∞\ni=1 V c\ni which contradicts\nTheorem B.4.\nB.1.2\nNormed spaces\nA norm is a way of assigning a length to a vector. A normed space is a vector space with a norm.\n252\nDefinition B.5. Let X be a vector space over a field K ∈{R, C}. A map ∥· ∥X : X →[0, ∞) is\ncalled a norm if the following hold for all x, y ∈X and all α ∈K:\n(i) triangle inequality: ∥x + y∥X ≤∥x∥X + ∥y∥X,\n(ii) absolute homogeneity: ∥αx∥X = |α|∥x∥X,\n(iii) positive definiteness: ∥x∥X = 0 if and only if x = 0.\nWe call (X, ∥· ∥X) a normed space and omit ∥· ∥X from the notation if it is clear from the\ncontext.\nEvery norm induces a metric dX and hence a topology via dX(x, y) := ∥x −y∥X. In particular,\nevery normed vector space is a topological vector space with respect to this topology.\nB.1.3\nBanach spaces\nDefinition B.6. A normed vector space is called a Banach space if and only if it is complete.\nBefore presenting the main results on Banach spaces, we collect a couple of important examples.\n• Euclidean spaces: Let d ∈N. Then (Rd, ∥· ∥) is a Banach space.\n• Continuous functions: Let d ∈N and let K ⊆Rd be compact. The set of continuous functions\nfrom K to R is denoted by C(K). For α, β ∈R and f, g ∈C(K), we define addition and\nscalar multiplication by (αf +βg)(x) = αf(x)+βg(x) for all x ∈K. The vector space C(K)\nequipped with the supremum norm\n∥f∥∞:= sup\nx∈K\n|f(x)|,\nis a Banach space.\n• Lebesgue spaces: Let (Ω, A, µ) be a measure space and let 1 ≤p < ∞. Then the Lebesgue\nspace Lp(Ω, µ) is defined as the vector space of all equivalence classes of measurable functions\nf : Ω→R that coincide µ-almost everywhere and satisfy\n∥f∥Lp(Ω,µ) :=\n\u0012Z\nΩ\n|f(x)|pdµ(x)\n\u00131/p\n< ∞.\n(B.1.2)\nThe integral is independent of the choice of representative of the equivalence class of f.\nAddition and scalar multiplication are defined pointwise as for C(K). It then holds that\nLp(Ω, µ) is a Banach space.\nIf Ωis a measurable subset of Rd for d ∈N, and µ is the\nLebesgue measure, we typically omit µ from the notation and simply write Lp(Ω). If Ω= N\nand the measure is the counting measure, we denote these spaces by ℓp(N) or simply ℓp.\n253\nThe definition can be extended to complex or Rd-valued functions. In the latter case the\nintegrand in (B.1.2) is replaced by ∥f(x)∥p. We denote these spaces again by Lp(Ω, µ) with\nthe precise meaning being clear from context.\n• Essentially bounded functions: Let (Ω, A, µ) be a measure space.\nThe Lp spaces can be\nextended to p = ∞by defining the L∞-norm\n∥f∥L∞(Ω,µ) := inf{C ≥0 | µ({|f| > C}) = 0)}.\nThis is indeed a norm on the space of equivalence classes of measurable functions from Ω→R\nthat coincide µ-almost everywhere. Moreover, with this norm, L∞(Ω, µ) is a Banach space. If\nΩ= N and µ is the counting measure, we denote the resulting space by ℓ∞(N) or simply ℓ∞.\nAs in the case p < ∞, it is straightforward to extend the definition to complex or Rd-valued\nfunctions, for which the same notation will be used.\nWe continue by introducing the concept of dual spaces.\nDefinition B.7. Let (X, ∥· ∥X) be a normed vector space over K ∈{R, C}. Linear maps from\nX →K are called linear functionals. The vector space of all continuous linear functionals on X\nis called the (topological) dual space of X and is denoted by X′.\nTogether with the natural addition and scalar multiplication (for all h, g ∈X′, α ∈K and\nx ∈X)\n(h + g)(x) := h(x) + g(x)\nand\n(αh)(x) := α(h(x)),\nX′ is a vector space. We equip X′ with the norm\n∥f∥X′ :=\nsup\nx∈X\n∥x∥X=1\n|f(x)|.\nThe space (X′, ∥· ∥X′) is always a Banach space, even if (X, ∥· ∥X) is not complete [196, Theorem\n4.1].\nThe dual space can often be used to characterize the original Banach space. One way in which\nthe dual space X′ captures certain algebraic and geometric properties of the Banach space X is\nthrough the Hahn-Banach theorem. In this book, we use one specific variant of this theorem and\nits implication for the existence of dual bases, see for instance [196, Theorem 3.5].\nTheorem B.8 (Geometric Hahn-Banach, subspace version). Let M be a subspace of a Banach\nspace X and let x0 ∈X. If x0 is not in the closure of M, then there exists f ∈X′ such that\nf(x0) = 1 and f(x) = 0 for every x ∈M.\nAn immediate consequence of Theorem B.8 that will be used throughout this book is the\nexistence of a dual basis. Let X be a Banach space and let (xi)i∈N ⊆X be such that for all i ∈N\nxi ̸∈span{xj | j ∈N, j ̸= i}.\nThen, for every i ∈N, there exists fi ∈X′ such that fi(xj) = 0 if i ̸= j and fi(xi) = 1.\n254\nB.1.4\nHilbert spaces\nOften, we require more structure than that provided by normed spaces. An inner product offers\nadditional tools to compare vectors by introducing notions of angle and orthogonality. For simplicity\nwe restrict ourselves to real vector spaces in the following.\nDefinition B.9. Let X be a real vector space. A map ⟨·, ·⟩X : X × X →R is called an inner\nproduct on X if the following hold for all x, y, z ∈X and all α, β ∈R:\n(i) linearity: ⟨αx + βy, z⟩X = α⟨x, z⟩X + β⟨y, z⟩X,\n(ii) symmetry: ⟨x, y⟩X = ⟨y, x⟩X,\n(iii) positive definiteness: ⟨x, x⟩X > 0 for all x ̸= 0.\nOn inner product spaces the so-called Cauchy-Schwarz inequality holds.\nTheorem B.10 (Cauchy-Schwarz inequality). Let X be a vector space with inner product ⟨·, ·⟩X.\nThen it holds for all x, y ∈X\n|⟨x, y⟩X| ≤\nq\n⟨x, x⟩X ⟨y, y⟩X.\nMoreover, equality holds if and only if x and y are linearly dependent.\nProof. Let x, y ∈X. If y = 0 then ⟨x, y⟩X = 0 and thus the statement is trivial. Assume in the\nfollowing y ̸= 0, so that ⟨y, y⟩X > 0. Using the linearity and symmetry properties it holds for all\nα ∈R\n0 ≤⟨x −αy, x −αy⟩X = ⟨x, x⟩X −2α ⟨x, y⟩X + α2 ⟨y, y⟩X .\nLetting α := ⟨x, y⟩X / ⟨y, y⟩X we get\n0 ≤⟨x, x⟩X −2⟨x, y⟩2\nX\n⟨y, y⟩X\n+ ⟨x, y⟩2\nX\n⟨y, y⟩X\n= ⟨x, x⟩X −⟨x, y⟩2\nX\n⟨y, y⟩X\n.\nRearranging terms gives the claim.\nEvery inner product ⟨·, ·⟩X induces a norm via\n∥x∥X :=\np\n⟨x, x⟩\nfor all x ∈X.\n(B.1.3)\nThe properties of the inner product immediately yield the polar identity\n∥x + y∥2\nX = ∥x∥2\nX + 2⟨x, y⟩X + ∥y∥2\nX.\n(B.1.4)\nThe fact that (B.1.3) indeed defines a norm follows by an application of the Cauchy-Schwarz\ninequality to (B.1.4), which yields that ∥· ∥X satisfies the triangle inequality. This gives rise to the\ndefinition of a Hilbert space.\n255\nDefinition B.11. Let H be a real vector space with inner product ⟨·, ·⟩H. Then (H, ⟨·, ·⟩H) is\ncalled a Hilbert space if and only if H is complete with respect to the norm ∥· ∥H induced by\nthe inner product.\nA standard example of a Hilbert space is L2: Let (Ω, A, µ) be a measure space. Then\n⟨f, g⟩L2(Ω,µ) =\nZ\nΩ\nf(x)g(x) dµ(x)\nfor all f, g ∈L2(Ω, µ),\ndefines an inner product on L2(Ω, µ) compatible with the L2(Ω, µ)-norm.\nIn a Hilbert space, we can compare vectors not only via their distance, measured by the norm,\nbut also by using the inner product, which corresponds to their relative orientation. This leads to\nthe concept of orthogonality.\nDefinition B.12. Let (H, ⟨·, ·⟩H) be a Hilbert space and let f, g ∈H. We say that f and g are\northogonal if ⟨f, g⟩H = 0, denoted by f ⊥g. Moreover, for F, G ⊆H we write F ⊥G if f ⊥g\nfor all f ∈F, g ∈G.\nFor orthogonal vectors, the polar identity immediately implies the Pythagorean theorem.\nTheorem B.13 (Pythagorean theorem). Let (H, ⟨·, ·⟩H) be a Hilbert space, n ∈N, and let\nf1, . . . , fn ∈H be pairwise orthogonal vectors. Then,\n\r\r\r\r\r\nn\nX\ni=1\nfi\n\r\r\r\r\r\n2\nH\n=\nn\nX\ni=1\n∥fi∥2\nH .\nA final property of Hilbert spaces that we encounter in this book is the existence of unique\nprojections onto convex sets. For a proof, see for instance [195, Thm. 4.10].\nTheorem B.14. Let (H, ⟨·, ·⟩H) be a Hilbert space and let K ̸= ∅be a closed convex subset of H.\nThen for all h ∈H exists a unique k0 ∈K such that\n∥h −k0∥H = inf{∥h −k∥H | k ∈K}.\n256\nB.2\nFourier transform\nThe Fourier transform is a powerful tool in analysis. It allows to represent functions as a superpo-\nsition of frequencies.\nDefinition B.15. Let d ∈N. The Fourier transform of a function f ∈L1(Rd) is defined by\nF(f)(ω) := ˆf(ω) :=\nZ\nRd f(x)e−2πix⊤ω dx\nfor all ω ∈Rd,\nand the inverse Fourier transform by\nF−1(f)(x) := ˇf(x) := ˆf(−x) =\nZ\nRd f(ω)e2πix⊤ω dω\nfor all x ∈Rd.\nIt is immediately clear from the definition, that ∥ˆf∥L∞(Rd) ≤∥f∥L1(Rd). As a result, the operator\nF : f 7→ˆf is a bounded linear map from L1(Rd) to L∞(Rd). We point out that ˆf can take complex\nvalues and the definition is also meaningful for complex-valued functions f.\nIf ˆf ∈L1(Rd), then we can reverse the process of taking the Fourier transform by taking the\ninverse Fourier transform, see [195, Theorem 9.11].\nTheorem B.16. If f, ˆf ∈L1(Rd) then F−1( ˆf) = f almost everywhere.\n257\nBibliography\n[1] R. Alaifari, G. S. Alberti, and T. Gauksson.\nAdef: an iterative algorithm to construct\nadversarial deformations. arXiv preprint arXiv:1804.07729, 2018.\n[2] Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparameterized neural\nnetworks, going beyond two layers. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019.\n[3] M. Anthony and P. L. Bartlett. Neural network learning: theoretical foundations. Cambridge\nUniversity Press, Cambridge, 1999.\n[4] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep neural networks with\nrectified linear units. In International Conference on Learning Representations, 2018.\n[5] S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation\nwith an infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019.\n[6] S. Arora, R. Ge, B. Neyshabur, and Y. Zhang.\nStronger generalization bounds for deep\nnets via a compression approach. In International Conference on Machine Learning, pages\n254–263. PMLR, 2018.\n[7] M. Baader, M. Mirman, and M. Vechev. Universal approximation with certified networks.\narXiv preprint arXiv:1909.13846, 2019.\n[8] S. Barocas, M. Hardt, and A. Narayanan. Fairness and Machine Learning. fairmlbook.org,\n2019. http://www.fairmlbook.org.\n[9] A. R. Barron. Neural net approximation. In Proc. 7th Yale workshop on adaptive and learning\nsystems, volume 1, pages 69–72, 1992.\n[10] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.\nIEEE Trans. Inform. Theory, 39(3):930–945, 1993.\n[11] A. R. Barron and J. M. Klusowski. Approximation and estimation for high-dimensional deep\nlearning networks. arXiv preprint arXiv:1809.03090, 2018.\n[12] P. Bartlett. For valid generalization the size of the weights is more important than the size\nof the network. Advances in neural information processing systems, 9, 1996.\n258\n[13] G. Beliakov.\nInterpolation of lipschitz functions.\nJournal of Computational and Applied\nMathematics, 196(1):20–44, 2006.\n[14] M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice\nand the classical bias–variance trade-off. Proceedings of the National Academy of Sciences,\n116(32):15849–15854, 2019.\n[15] M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel\nlearning. In International Conference on Machine Learning, pages 541–549. PMLR, 2018.\n[16] R. Bellman. On the theory of dynamic programming. Proceedings of the national Academy\nof Sciences, 38(8):716–719, 1952.\n[17] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent\nis difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n[18] J. Berner, P. Grohs, and A. Jentzen. Analysis of the generalization error: Empirical risk\nminimization over deep artificial neural networks overcomes the curse of dimensionality in\nthe numerical approximation of black–scholes partial differential equations. SIAM Journal\non Mathematics of Data Science, 2(3):631–657, 2020.\n[19] J. Berner, P. Grohs, G. Kutyniok, and P. Petersen. The modern mathematics of deep learning,\n2021.\n[20] D. P. Bertsekas. Nonlinear programming. Athena Scientific Optimization and Computation\nSeries. Athena Scientific, Belmont, MA, third edition, 2016.\n[21] H. Bolcskei, P. Grohs, G. Kutyniok, and P. Petersen. Optimal approximation with sparsely\nconnected deep neural networks. SIAM Journal on Mathematics of Data Science, 1(1):8–45,\n2019.\n[22] L. Bottou. Stochastic Gradient Descent Tricks, pages 421–436. Springer Berlin Heidelberg,\nBerlin, Heidelberg, 2012.\n[23] L. Bottou, F. E. Curtis, and J. Nocedal.\nOptimization methods for large-scale machine\nlearning. SIAM Review, 60(2):223–311, 2018.\n[24] O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning\nResearch, 2:499–526, 2002.\n[25] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, Cambridge,\n2004.\n[26] J. Braun and M. Griebel. On a constructive proof of kolmogorov’s superposition theorem.\nConstructive Approximation, 30(3):653–675, Dec 2009.\n[27] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c. Geometric deep learning: Grids,\ngroups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.\n[28] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in\nneural information processing systems, 33:1877–1901, 2020.\n259\n[29] O. Calin. Deep learning architectures. Springer, 2020.\n[30] E. J. Candes. Ridgelets: theory and applications. Stanford University, 1998.\n[31] C. Carath´eodory.\n¨Uber den variabilit¨atsbereich der fourier’schen konstanten von posi-\ntiven harmonischen funktionen. Rendiconti del Circolo Matematico di Palermo (1884-1940),\n32:193–217, 1911.\n[32] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017\nieee symposium on security and privacy (sp), pages 39–57. Ieee, 2017.\n[33] S. M. Carroll and B. W. Dickinson. Construction of neural nets using the radon transform.\nInternational 1989 Joint Conference on Neural Networks, pages 607–611 vol.1, 1989.\n[34] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes,\nL. Sagun, and R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. Journal\nof Statistical Mechanics: Theory and Experiment, 2019(12):124018, 2019.\n[35] M. Chen, H. Jiang, W. Liao, and T. Zhao. Efficient approximation of deep relu networks for\nfunctions on low dimensional manifolds. Advances in neural information processing systems,\n32, 2019.\n[36] L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,\n2019.\n[37] Y. Cho and L. Saul.\nKernel methods for deep learning.\nIn Y. Bengio, D. Schuurmans,\nJ. Lafferty, C. Williams, and A. Culotta, editors, Advances in Neural Information Processing\nSystems, volume 22. Curran Associates, Inc., 2009.\n[38] F. Chollet. Deep learning with Python. Simon and Schuster, 2021.\n[39] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of\nmultilayer networks. In Artificial intelligence and statistics, pages 192–204. PMLR, 2015.\n[40] C. K. Chui and H. N. Mhaskar. Deep nets for local manifold learning. Frontiers in Applied\nMathematics and Statistics, 4:12, 2018.\n[41] J. B. Conway. A course in functional analysis, volume 96. Springer, 2019.\n[42] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other\nKernel-based Learning Methods. Cambridge University Press, 1 edition, 2000.\n[43] F. Cucker and S. Smale.\nOn the mathematical foundations of learning.\nBulletin of the\nAmerican mathematical society, 39(1):1–49, 2002.\n[44] G. Cybenko.\nApproximation by superpositions of a sigmoidal function.\nMathematics of\nControl, Signals and Systems, 2(4):303–314, 1989.\n260\n[45] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and\nattacking the saddle point problem in high-dimensional non-convex optimization. Advances\nin neural information processing systems, 27, 2014.\n[46] A. G. de G. Matthews. Sample-then-optimize posterior sampling for bayesian linear models.\n2017.\n[47] A. G. de G. Matthews, J. Hron, M. Rowland, R. E. Turner, and Z. Ghahramani. Gaussian\nprocess behaviour in wide deep neural networks. In International Conference on Learning\nRepresentations, 2018.\n[48] T. De Ryck, S. Lanthaler, and S. Mishra. On the approximation of functions by tanh neural\nnetworks. Neural Networks, 143:732–750, 2021.\n[49] A. D´efossez, L. Bottou, F. R. Bach, and N. Usunier. A simple convergence proof of adam\nand adagrad. Trans. Mach. Learn. Res., 2022, 2022.\n[50] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImagenet: A large-scale\nhierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248–255, 2009.\n[51] H. R. M. C. DeVore, R.\nOptimal nonlinear approximation.\nManuscripta mathematica,\n63(4):469–478, 1989.\n[52] R. A. DeVore. Nonlinear approximation. Acta numerica, 7:51–150, 1998.\n[53] L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets.\nIn International Conference on Machine Learning, pages 1019–1028. PMLR, 2017.\n[54] F. Draxler, K. Veschgini, M. Salmhofer, and F. Hamprecht. Essentially no barriers in neural\nnetwork energy landscape. In International conference on machine learning, pages 1309–1318.\nPMLR, 2018.\n[55] M. Du, F. Yang, N. Zou, and X. Hu. Fairness in deep learning: A computational perspective.\nIEEE Intelligent Systems, 36(4):25–34, 2021.\n[56] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep\nneural networks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th\nInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning\nResearch, pages 1675–1685. PMLR, 09–15 Jun 2019.\n[57] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.\n[58] W. E and Q. Wang. Exponential convergence of the deep neural network approximation for\nanalytic functions. Sci. China Math., 61(10):1733–1740, 2018.\n[59] K. Eckle and J. Schmidt-Hieber. A comparison of deep networks with relu activation function\nand linear spline-type methods. Neural Networks, 110:232–242, 2019.\n261\n[60] R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In V. Feldman,\nA. Rakhlin, and O. Shamir, editors, 29th Annual Conference on Learning Theory, volume 49\nof Proceedings of Machine Learning Research, pages 907–940, Columbia University, New York,\nNew York, USA, 23–26 Jun 2016. PMLR.\n[61] M. Fischer, M. Balunovic, D. Drachsler-Cohen, T. Gehr, C. Zhang, and M. Vechev. Dl2:\ntraining and querying neural networks with logic. In International Conference on Machine\nLearning, pages 1931–1941. PMLR, 2019.\n[62] C. L. Frenzen, T. Sasao, and J. T. Butler. On the number of segments needed in a piecewise\nlinear approximation. Journal of Computational and Applied mathematics, 234(2):437–446,\n2010.\n[63] K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks.\nNeural Networks, 2(3):183–192, 1989.\n[64] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces,\nmode connectivity, and fast ensembling of dnns. Advances in neural information processing\nsystems, 31, 2018.\n[65] G. Garrigos and R. M. Gower. Handbook of convergence theorems for (stochastic) gradient\nmethods, 2023.\n[66] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. Ai2:\nSafety and robustness certification of neural networks with abstract interpretation. In 2018\nIEEE symposium on security and privacy (SP), pages 3–18. IEEE, 2018.\n[67] A. G´eron. Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools,\nand techniques to build intelligent systems. O’Reilly Media, Sebastopol, CA, 2017.\n[68] F. Girosi and T. Poggio. Representation properties of networks: Kolmogorov’s theorem is\nirrelevant. Neural Computation, 1(4):465–469, 1989.\n[69] F. Girosi and T. Poggio. Networks and the best approximation property. Biological cyber-\nnetics, 63(3):169–176, 1990.\n[70] G. Goh. Why momentum really works. Distill, 2017.\n[71] L. Gonon and C. Schwab.\nDeep relu network expression rates for option prices in high-\ndimensional, exponential l´evy models. Finance and Stochastics, 25(4):615–657, 2021.\n[72] I. J. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge, MA,\nUSA, 2016. http://www.deeplearningbook.org.\n[73] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples.\nIn International Conference on Learning Representations (ICLR), 2015.\n[74] I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network\noptimization problems. arXiv preprint arXiv:1412.6544, 2014.\n262\n[75] L.-A. Gottlieb, A. Kontorovich, and R. Krauthgamer. Efficient regression in metric spaces\nvia approximate lipschitz extension. IEEE Transactions on Information Theory, 63(8):4838–\n4849, 2017.\n[76] R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richt´arik. SGD: General\nanalysis and improved rates. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of\nthe 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine\nLearning Research, pages 5200–5209. PMLR, 09–15 Jun 2019.\n[77] K. Gr¨ochenig. Foundations of time-frequency analysis. Springer Science & Business Media,\n2013.\n[78] P. Grohs and L. Herrmann. Deep neural network approximation for high-dimensional elliptic\npdes with boundary conditions. IMA Journal of Numerical Analysis, 42(3):2055–2082, 2022.\n[79] P. Grohs, F. Hornung, A. Jentzen, and P. Von Wurstemberger. A proof that artificial neural\nnetworks overcome the curse of dimensionality in the numerical approximation of Black–\nScholes partial differential equations, volume 284. American Mathematical Society, 2023.\n[80] P. Grohs, F. Hornung, A. Jentzen, and P. von Wurstemberger. A proof that artificial neural\nnetworks overcome the curse of dimensionality in the numerical approximation of Black-\nScholes partial differential equations. Mem. Amer. Math. Soc., 284(1410):v+93, 2023.\n[81] I. G¨uhring and M. Raslan. Approximation rates for neural networks with encodable weights\nin smoothness spaces. Neural Networks, 134:107–130, 2021.\n[82] B. Hanin and D. Rolnick. Complexity of linear regions in deep networks. In International\nConference on Machine Learning, pages 2596–2604. PMLR, 2019.\n[83] T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani.\nSurprises in high-dimensional\nridgeless least squares interpolation. The Annals of Statistics, 50(2):949–986, 2022.\n[84] S. S. Haykin. Neural networks and learning machines. Pearson Education, Upper Saddle\nRiver, NJ, third edition, 2009.\n[85] J. He, L. Li, J. Xu, and C. Zheng. Relu deep neural networks and linear finite elements. J.\nComput. Math., 38(3):502–527, 2020.\n[86] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level\nperformance on imagenet classification. Proceedings of the IEEE international conference on\ncomputer vision, 2015.\n[87] K. He, X. Zhang, S. Ren, and J. Sun.\nDeep residual learning for image recognition.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\n778, 2016.\n[88] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against\nadversarial manipulation. Advances in neural information processing systems, 30, 2017.\n[89] H. Heuser. Lehrbuch der Analysis. Teil 1. Vieweg + Teubner, Wiesbaden, revised edition,\n2009.\n263\n[90] G. Hinton. Divide the gradient by a running average of its recent magnitude. https://www.\ncs.toronto.edu/~hinton/coursera/lecture6/lec6e.mp4, 2012. Lecture 6e.\n[91] S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut\nf¨ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen, 1991.\n[92] S. Hochreiter and J. Schmidhuber. Flat minima. Neural computation, 9(1):1–42, 1997.\n[93] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–\n1780, 1997.\n[94] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks,\n4(2):251–257, 1991.\n[95] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal\napproximators. Neural Networks, 2(5):359–366, 1989.\n[96] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.\nDensely connected con-\nvolutional networks.\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 1(2):3, 2017.\n[97] G.-B. Huang and H. A. Babri. Upper bounds on the number of hidden neurons in feedforward\nnetworks with arbitrary bounded nonlinear activation functions. IEEE transactions on neural\nnetworks, 9(1):224–229, 1998.\n[98] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D. Tygar. Adversarial machine\nlearning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence,\npages 43–58, 2011.\n[99] T. Huster, C.-Y. J. Chiang, and R. Chadha. Limitations of the lipschitz constant as a defense\nagainst adversarial examples. In ECML PKDD 2018 Workshops: Nemesis 2018, UrbReas\n2018, SoGood 2018, IWAISe 2018, and Green Data Mining 2018, Dublin, Ireland, September\n10-14, 2018, Proceedings 18, pages 16–29. Springer, 2019.\n[100] M. Hutzenthaler, A. Jentzen, T. Kruse, and T. A. Nguyen. A proof that rectified deep neural\nnetworks overcome the curse of dimensionality in the numerical approximation of semilinear\nheat equations. SN partial differential equations and applications, 1(2):10, 2020.\n[101] J. H˚astad. Computational limitations of small depth circuits. PhD thesis, Massachusetts\nInstitute of Technology, 1987. Ph.D. Thesis, Department of Mathematics.\n[102] D. J. Im, M. Tao, and K. Branson. An empirical analysis of deep network loss surfaces. 2016.\n[103] V. E. Ismailov. Ridge functions and applications in neural networks, volume 263. American\nMathematical Society, 2021.\n[104] V. E. Ismailov. A three layer neural network can represent any multivariate function. Journal\nof Mathematical Analysis and Applications, 523(1):127096, 2023.\n[105] Y. Ito and K. Saito. Superposition of linearly independent functions and finite mappings by\nneural networks. The Mathematical Scientist, 21(1):27, 1996.\n264\n[106] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization\nin neural networks. Advances in neural information processing systems, 31, 2018.\n[107] A. Jentzen, B. Kuckuck, and P. von Wurstemberger. Mathematical introduction to deep\nlearning: methods, implementations, and theory. arXiv preprint arXiv:2310.20360, 2023.\n[108] A. Jentzen and A. Riekert.\nOn the existence of global minima and convergence analy-\nses for gradient descent methods in the training of deep neural networks. arXiv preprint\narXiv:2112.09684, 2021.\n[109] A. Jentzen, D. Salimova, and T. Welti. A proof that deep artificial neural networks overcome\nthe curse of dimensionality in the numerical approximation of Kolmogorov partial differen-\ntial equations with constant diffusion and nonlinear drift coefficients. Commun. Math. Sci.,\n19(5):1167–1205, 2021.\n[110] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvu-\nnakool, R. Bates, A. ˇZ´ıdek, A. Potapenko, et al. Highly accurate protein structure prediction\nwith alphafold. Nature, 596(7873):583–589, 2021.\n[111] P. C. Kainen, V. Kurkova, and A. Vogt. Approximation by neural networks is not continuous.\nNeurocomputing, 29(1-3):47–56, 1999.\n[112] P. C. Kainen, V. Kurkova, and A. Vogt. Continuity of approximation by neural networks in\nl p spaces. Annals of Operations Research, 101:143–147, 2001.\n[113] P. C. Kainen, V. Kurkova, and A. Vogt.\nBest approximation by linear combinations of\ncharacteristic functions of half-spaces. Journal of Approximation Theory, 122(2):151–159,\n2003.\n[114] H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient\nmethods under the polyak- lojasiewicz condition. In P. Frasconi, N. Landwehr, G. Manco,\nand J. Vreeken, editors, Machine Learning and Knowledge Discovery in Databases, pages\n795–811, Cham, 2016. Springer International Publishing.\n[115] C. Karner, V. Kazeev, and P. C. Petersen. Limitations of gradient descent due to numerical\ninstability of backpropagation. arXiv preprint arXiv:2210.00805, 2022.\n[116] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings.\nInternational Conference on Learning Representations, ICLR, 2015.\n[117] A. Klenke. Wahrscheinlichkeitstheorie. Springer, 2006.\n[118] M. Kohler, A. Krzy˙zak, and S. Langer. Estimation of a function of low local dimensionality\nby deep neural networks. IEEE transactions on information theory, 68(6):4032–4042, 2022.\n[119] M. Kohler and S. Langer. On the rate of convergence of fully connected deep neural network\nregression estimates. The Annals of Statistics, 49(4):2231–2249, 2021.\n[120] A. N. Kolmogorov.\nOn the representation of continuous functions of many variables by\nsuperposition of continuous functions of one variable and addition. Dokl. Akad. Nauk SSSR,\n114:953–956, 1957.\n265\n[121] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional\nneural networks. In Advances in neural information processing systems, pages 1097–1105,\n2012.\n[122] G. Kutyniok, P. Petersen, M. Raslan, and R. Schneider. A theoretical analysis of deep neural\nnetworks and parametric pdes. Constructive Approximation, 55(1):73–125, 2022.\n[123] V. K˚urkov´a. Kolmogorov’s theorem is relevant. Neural Computation, 3(4):617–622, 1991.\n[124] V. K˚urkov´a.\nKolmogorov’s theorem and multilayer neural networks.\nNeural Networks,\n5(3):501–506, 1992.\n[125] F. Laakmann and P. Petersen.\nEfficient approximation of solutions of parametric linear\ntransport equations by relu dnns. Advances in Computational Mathematics, 47(1):11, 2021.\n[126] G. Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer\nSeries in the Data Sciences. Springer International Publishing, Cham, 1st ed. 2020. edition,\n2020.\n[127] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, May 2015.\n[128] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.\nJackel. Backpropagation applied to handwritten zip code recognition. Neural Computation,\n1(4):541–551, 1989.\n[129] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller.\nEfficient BackProp, pages 9–48.\nSpringer Berlin Heidelberg, Berlin, Heidelberg, 2012.\n[130] J. Lee, J. Sohl-dickstein, J. Pennington, R. Novak, S. Schoenholz, and Y. Bahri. Deep neural\nnetworks as gaussian processes. In International Conference on Learning Representations,\n2018.\n[131] J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. Wide\nneural networks of any depth evolve as linear models under gradient descent. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n[132] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a\nnonpolynomial activation function can approximate any function. Neural Networks, 6(6):861–\n867, 1993.\n[133] L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via\nintegral quadratic constraints. SIAM J. Optim., 26(1):57–95, 2016.\n[134] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural\nnets. Advances in neural information processing systems, 31, 2018.\n[135] W. Li. Generalization error of minimum weighted norm and kernel interpolation. SIAM\nJournal on Mathematics of Data Science, 3(1):414–438, 2021.\n[136] M. Longo, J. A. Opschoor, N. Disch, C. Schwab, and J. Zech. De rham compatible deep\nneural network fem. Neural Networks, 165:721–739, 2023.\n266\n[137] C. Ma, S. Wojtowytsch, L. Wu, et al.\nTowards a mathematical understanding of neu-\nral network-based machine learning: what we know and what we don’t.\narXiv preprint\narXiv:2009.10713, 2020.\n[138] C. Ma, L. Wu, et al. A priori estimates of the population risk for two-layer neural networks.\narXiv preprint arXiv:1810.06397, 2018.\n[139] S. Mahan, E. J. King, and A. Cloninger. Nonclosedness of sets of neural networks in sobolev\nspaces. Neural Networks, 137:85–96, 2021.\n[140] V. Maiorov and A. Pinkus. Lower bounds for approximation by mlp neural networks. Neu-\nrocomputing, 25(1):81–91, 1999.\n[141] Y. Marzouk, Z. Ren, S. Wang, and J. Zech.\nDistribution learning via neural differential\nequations: a nonparametric statistical perspective. Journal of Machine Learning Research\n(accepted), 2024.\n[142] W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity.\nThe bulletin of mathematical biophysics, 5:115–133, 1943.\n[143] S. Mei and A. Montanari. The generalization error of random features regression: Precise\nasymptotics and the double descent curve. Communications on Pure and Applied Mathemat-\nics, 75(4):667–766, 2022.\n[144] H. N. Mhaskar.\nApproximation properties of a multilayered feedforward artificial neural\nnetwork. Adv. Comput. Math., 1(1):61–80, 1993.\n[145] H. N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions.\nNeural computation, 8(1):164–177, 1996.\n[146] H. N. Mhaskar and C. A. Micchelli. Approximation by superposition of sigmoidal and radial\nbasis functions. Adv. in Appl. Math., 13(3):350–373, 1992.\n[147] H. N. Mhaskar and C. A. Micchelli.\nDegree of approximation by neural and translation\nnetworks with a single hidden layer. Advances in applied mathematics, 16(2):151–183, 1995.\n[148] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press,\n2018.\n[149] C. Molnar. Interpretable machine learning. Lulu. com, 2020.\n[150] H. Montanelli and Q. Du. New error bounds for deep relu networks using sparse grids. SIAM\nJournal on Mathematics of Data Science, 1(1):78–92, 2019.\n[151] H. Montanelli and H. Yang. Error bounds for deep relu networks using the kolmogorov–arnold\nsuperposition theorem. Neural Networks, 129:1–6, 2020.\n[152] G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep\nneural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger,\neditors, Advances in Neural Information Processing Systems, volume 27. Curran Associates,\nInc., 2014.\n267\n[153] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial pertur-\nbations. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 1765–1773, 2017.\n[154] E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger,\neditors, Advances in Neural Information Processing Systems, volume 24. Curran Associates,\nInc., 2011.\n[155] K.-R. Muller, S. Mika, G. Ratsch, K. Tsuda, and B. Scholkopf. An introduction to kernel-\nbased learning algorithms. IEEE Transactions on Neural Networks, 12(2):181–201, 2001.\n[156] R. Nakada and M. Imaizumi.\nAdaptive approximation and generalization of deep neural\nnetwork with intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1–38,\n2020.\n[157] R. M. Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.\n[158] Y. Nesterov. Introductory lectures on convex optimization, volume 87 of Applied Optimization.\nKluwer Academic Publishers, Boston, MA, 2004. A basic course.\n[159] Y. Nesterov. Lectures on convex optimization, volume 137 of Springer Optimization and Its\nApplications. Springer, Cham, second edition, 2018.\n[160] Y. E. Nesterov. A method for solving the convex programming problem with convergence\nrate O(1/k2). Dokl. Akad. Nauk SSSR, 269(3):543–547, 1983.\n[161] B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks.\nIn Conference on learning theory, pages 1376–1401. PMLR, 2015.\n[162] R. H. Nielsen. Kolmogorov’s mapping neural network existence theorem. In Proceedings of\nthe IEEE First International Conference on Neural Networks (San Diego, CA), volume III,\npages 11–13. Piscataway, NJ: IEEE, 1987.\n[163] J. Nocedal and S. J. Wright. Numerical optimization. Springer Series in Operations Research\nand Financial Engineering. Springer, New York, second edition, 2006.\n[164] E. Novak and H. Wo´zniakowski. Approximation of infinitely differentiable multivariate func-\ntions is intractable. Journal of Complexity, 25(4):398–404, 2009.\n[165] B. O’Donoghue and E. Cand`es. Adaptive restart for accelerated gradient schemes. Found.\nComput. Math., 15(3):715–732, 2015.\n[166] J. A. A. Opschoor, C. Schwab, and J. Zech. Exponential ReLU DNN expression of holomor-\nphic maps in high dimension. Constructive Approximation, 2021.\n[167] J. A. A. Opschoor, C. Schwab, and J. Zech. Deep learning in high dimension: ReLU neural\nnetwork expression for Bayesian PDE inversion.\nIn Optimization and control for partial\ndifferential equations—uncertainty quantification, open and closed-loop control, and shape\noptimization, volume 29 of Radon Ser. Comput. Appl. Math., pages 419–462. De Gruyter,\nBerlin, 2022.\n268\n[168] P. Oswald. On the degree of nonlinear spline approximation in Besov-Sobolev spaces. J.\nApprox. Theory, 61(2):131–157, 1990.\n[169] S. Ovchinnikov.\nMax-min representation of piecewise linear functions.\nBeitr¨age Algebra\nGeom., 43(1):297–302, 2002.\n[170] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-\nbox attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference\non computer and communications security, pages 506–519, 2017.\n[171] Y. C. Pati and P. S. Krishnaprasad. Analysis and synthesis of feedforward neural networks us-\ning discrete affine wavelet transformations. IEEE Transactions on Neural Networks, 4(1):73–\n85, 1993.\n[172] J. Pennington and Y. Bahri. Geometry of neural network loss surfaces via random matrix\ntheory. In International Conference on Machine Learning, pages 2798–2806. PMLR, 2017.\n[173] P. Petersen, M. Raslan, and F. Voigtlaender.\nTopological properties of the set of func-\ntions generated by neural networks of fixed size. Foundations of computational mathematics,\n21:375–444, 2021.\n[174] P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using\ndeep relu neural networks. Neural Networks, 108:296–330, 2018.\n[175] P. C. Petersen.\nNeural Network Theory.\n2020.\nhttp://www.pc-petersen.eu/Neural_\nNetwork_Theory.pdf, Lecture notes.\n[176] A. Pinkus. Approximation theory of the MLP model in neural networks. In Acta numerica,\n1999, volume 8 of Acta Numer., pages 143–195. Cambridge Univ. Press, Cambridge, 1999.\n[177] G. Pisier. Remarques sur un r´esultat non publi´e de B. Maurey. S´eminaire Analyse fonction-\nnelle (dit ”Maurey-Schwartz”), 1980-1981.\n[178] T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao. Why and when can deep-but\nnot shallow-networks avoid the curse of dimensionality: a review. Int. J. Autom. Comput.,\n14(5):503–519, 2017.\n[179] T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi. General conditions for predictivity in\nlearning theory. Nature, 428(6981):419–422, 2004.\n[180] B. Polyak.\nSome methods of speeding up the convergence of iteration methods.\nUSSR\nComputational Mathematics and Mathematical Physics, 4(5):1–17, 1964.\n[181] B. T. Polyak. Introduction to optimization. Translations Series in Mathematics and Engineer-\ning. Optimization Software, Inc., Publications Division, New York, 1987. Translated from\nthe Russian, With a foreword by Dimitri P. Bertsekas.\n[182] S. J. Prince. Understanding Deep Learning. MIT Press, 2023.\n[183] N. Qian. On the momentum term in gradient descent learning algorithms. Neural Networks,\n12(1):145–151, 1999.\n269\n[184] M. H. Quynh Nguyen, Mahesh Chandra Mukkamala. On the loss landscape of a class of\ndeep neural networks with no bad local valleys. In International Conference on Learning\nRepresentations (ICLR), 2018.\n[185] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein.\nOn the expressive\npower of deep neural networks. In D. Precup and Y. W. Teh, editors, Proceedings of the\n34th International Conference on Machine Learning, volume 70 of Proceedings of Machine\nLearning Research, pages 2847–2854. PMLR, 06–11 Aug 2017.\n[186] A. Rahimi and B. Recht.\nRandom features for large-scale kernel machines.\nIn J. Platt,\nD. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing\nSystems, volume 20. Curran Associates, Inc., 2007.\n[187] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep\nlearning framework for solving forward and inverse problems involving nonlinear partial dif-\nferential equations. Journal of Computational physics, 378:686–707, 2019.\n[188] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. Adaptive\ncomputation and machine learning. MIT Press, 2006.\n[189] E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, Q. Le, and A. Kurakin. Regularized\nevolution for image classifier architecture search. Proceedings of the AAAI Conference on\nArtificial Intelligence, 33:4780–4789, 2019.\n[190] S. J. Reddi, S. Kale, and S. Kumar.\nOn the convergence of adam and beyond.\nIn 6th\nInternational Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n[191] H. Robbins and S. Monro. A Stochastic Approximation Method. The Annals of Mathematical\nStatistics, 22(3):400 – 407, 1951.\n[192] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organiza-\ntion in the brain. Psychological Review, 65(6):386–408, 1958.\n[193] W. Ruan, X. Yi, and X. Huang. Adversarial robustness of deep learning: Theory, algorithms,\nand applications. In Proceedings of the 30th ACM international conference on information\n& knowledge management, pages 4866–4869, 2021.\n[194] S. Ruder. An overview of gradient descent optimization algorithms, 2016.\n[195] W. Rudin. Real and complex analysis. McGraw-Hill Book Co., New York, third edition, 1987.\n[196] W. Rudin.\nFunctional analysis.\nInternational Series in Pure and Applied Mathematics.\nMcGraw-Hill, Inc., New York, second edition, 1991.\n[197] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning representations by back-\npropagating errors. Nature, 323(6088):533–536, 1986.\n[198] T. D. Ryck and S. Mishra. Error analysis for deep neural network approximations of paramet-\nric hyperbolic conservation laws. Mathematics of Computation, 2023. Article electronically\npublished on December 15, 2023.\n270\n[199] I. Safran and O. Shamir. Depth separation in relu networks for approximating smooth non-\nlinear functions. ArXiv, abs/1610.09887, 2016.\n[200] M. A. Sartori and P. J. Antsaklis. A simple method to derive bounds on the size and to train\nmultilayer neural networks. IEEE transactions on neural networks, 2(4):467–471, 1991.\n[201] R. Scheichl and J. Zech. Numerical methods for bayesian inverse problems, 2021. Lecture\nNotes.\n[202] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85–117,\n2015.\n[203] J. Schmidt-Hieber.\nDeep relu network approximation of functions on a manifold.\narXiv\npreprint arXiv:1908.00695, 2019.\n[204] J. Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation\nfunction. 2020.\n[205] J. Schmidt-Hieber. The kolmogorov–arnold representation theorem revisited. Neural Net-\nworks, 137:119–126, 2021.\n[206] B. Sch¨olkopf and A. J. Smola. Learning with kernels : support vector machines, regularization,\noptimization, and beyond. Adaptive computation and machine learning. MIT Press, 2002.\n[207] L. Schumaker. Spline Functions: Basic Theory. Cambridge Mathematical Library. Cambridge\nUniversity Press, 3 edition, 2007.\n[208] C. Schwab and J. Zech. Deep learning in high dimension: neural network expression rates for\ngeneralized polynomial chaos expansions in UQ. Anal. Appl. (Singap.), 17(1):19–55, 2019.\n[209] C. Schwab and J. Zech. Deep learning in high dimension: neural network expression rates for\nanalytic functions in L2(mathbbRd, gammad). SIAM/ASA J. Uncertain. Quantif., 11(1):199–\n234, 2023.\n[210] T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding and counting linear regions of\ndeep neural networks, 2018.\n[211] U. Shaham, A. Cloninger, and R. R. Coifman. Provable approximation properties for deep\nneural networks. Applied and Computational Harmonic Analysis, 44(3):537–557, 2018.\n[212] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning - From Theory to\nAlgorithms. Cambridge University Press, 2014.\n[213] J. W. Siegel and J. Xu. High-order approximation rates for shallow neural networks with\ncosine and reluk activation functions. Applied and Computational Harmonic Analysis, 58:1–\n26, 2022.\n[214] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep\nneural networks and tree search. nature, 529(7587):484–489, 2016.\n271\n[215] K. Simonyan and A. Zisserman.\nVery deep convolutional networks for large-scale image\nrecognition. In ICLR, 2014.\n[216] E. M. Stein. Singular integrals and differentiability properties of functions. Princeton Math-\nematical Series, No. 30. Princeton University Press, Princeton, N.J., 1970.\n[217] I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York, 2008.\n[218] D. Stutz, M. Hein, and B. Schiele. Disentangling adversarial robustness and generalization.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 6976–6987, 2019.\n[219] A. Sukharev. Optimal method of constructing best uniform approximations for functions of\na certain class. USSR Computational Mathematics and Mathematical Physics, 18(2):21–31,\n1978.\n[220] T. Sun, L. Qiao, and D. Li. Nonergodic complexity of proximal inertial gradient descents.\nIEEE Trans. Neural Netw. Learn. Syst., 32(10):4613–4626, 2021.\n[221] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and\nmomentum in deep learning. In S. Dasgupta and D. McAllester, editors, Proceedings of the\n30th International Conference on Machine Learning, volume 28 of Proceedings of Machine\nLearning Research, pages 1139–1147, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.\n[222] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,\nand A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1–9, 2015.\n[223] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. In International Conference on Learning Represen-\ntations (ICLR), 2014.\n[224] M. Tan and Q. V. Le. Efficientnet: Rethinking model scaling for convolutional neural net-\nworks.\nIn Proceedings of the 36th International Conference on Machine Learning, pages\n6105–6114, 2019.\n[225] J. Tarela and M. Mart´ınez. Region configurations for realizability of lattice piecewise-linear\nmodels. Mathematical and Computer Modelling, 30(11):17–27, 1999.\n[226] J. M. Tarela, E. Alonso, and M. V. Mart´ınez. A representation method for PWL functions\noriented to parallel processing. Math. Comput. Modelling, 13(10):75–83, 1990.\n[227] M. Telgarsky. Representation benefits of deep feedforward networks, 2015.\n[228] M. Telgarsky. benefits of depth in neural networks. In V. Feldman, A. Rakhlin, and O. Shamir,\neditors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine\nLearning Research, pages 1517–1539, Columbia University, New York, New York, USA, 23–26\nJun 2016. PMLR.\n[229] M. Telgarsky. Deep learning theory lecture notes. https://mjt.cs.illinois.edu/dlt/,\n2021. Version: 2021-10-27 v0.0-e7150f2d (alpha).\n272\n[230] V. M. Tikhomirov. ε-entropy and ε-capacity of sets in functional spaces. Selected Works\nof AN Kolmogorov: Volume III: Information Theory and the Theory of Algorithms, pages\n86–170, 1993.\n[231] S. Tu, S. Venkataraman, A. C. Wilson, A. Gittens, M. I. Jordan, and B. Recht. Breaking\nlocality accelerates block Gauss-Seidel. In D. Precup and Y. W. Teh, editors, Proceedings of\nthe 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine\nLearning Research, pages 3482–3491. PMLR, 06–11 Aug 2017.\n[232] L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142,\n1984.\n[233] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of\nevents to their probabilities. In Measures of complexity: festschrift for alexey chervonenkis,\npages 11–30. Springer, 2015.\n[234] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser, and\nI. Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n[235] L. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in one-hidden-layer neural network\noptimization landscapes. Journal of Machine Learning Research, 20:133, 2019.\n[236] R. Vershynin. High-dimensional probability: An introduction with applications in data science,\nvolume 47. Cambridge University Press, 2018.\n[237] S. Wang and X. Sun. Generalization of hinging hyperplanes. IEEE Transactions on Infor-\nmation Theory, 51(12):4425–4431, 2005.\n[238] Z. Wang, A. Albarghouthi, G. Prakriya, and S. Jha. Interval universal approximation for\nneural networks. Proceedings of the ACM on Programming Languages, 6(POPL):1–29, 2022.\n[239] E. Weinan, C. Ma, and L. Wu. Barron spaces and the compositional function spaces for\nneural network models. arXiv preprint arXiv:1906.08039, 2019.\n[240] E. Weinan and S. Wojtowytsch. Representation formulas and pointwise properties for barron\nfunctions. Calculus of Variations and Partial Differential Equations, 61(2):46, 2022.\n[241] A. C. Wilson, B. Recht, and M. I. Jordan. A lyapunov analysis of accelerated methods in\noptimization. Journal of Machine Learning Research, 22(113):1–34, 2021.\n[242] A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive\ngradient methods in machine learning.\nIn I. Guyon, U. V. Luxburg, S. Bengio, H. Wal-\nlach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc., 2017.\n[243] C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song. Spatially transformed adversarial\nexamples. arXiv preprint arXiv:1801.02612, 2018.\n[244] H. Xu and S. Mannor. Robustness and generalization. Machine learning, 86:391–423, 2012.\n273\n[245] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Netw.,\n94:103–114, 2017.\n[246] D. Yarotsky and A. Zhevnerchuk. The phase diagram of approximation rates for deep neural\nnetworks.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,\nAdvances in Neural Information Processing Systems, volume 33, pages 13005–13015. Curran\nAssociates, Inc., 2020.\n[247] H. M. D. K. S. B. Yiding Jiang, Behnam Neyshabur. Fantastic generalization measures and\nwhere to find them. In International Conference on Learning Representations (ICLR), 2019.\n[248] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104–\n12113, 2022.\n274\n",
  "categories": [
    "cs.LG",
    "math.HO"
  ],
  "published": "2024-07-25",
  "updated": "2024-10-11"
}