{
  "id": "http://arxiv.org/abs/2407.18597v1",
  "title": "Reinforcement Learning for Sustainable Energy: A Survey",
  "authors": [
    "Koen Ponse",
    "Felix Kleuker",
    "Márton Fejér",
    "Álvaro Serra-Gómez",
    "Aske Plaat",
    "Thomas Moerland"
  ],
  "abstract": "The transition to sustainable energy is a key challenge of our time,\nrequiring modifications in the entire pipeline of energy production, storage,\ntransmission, and consumption. At every stage, new sequential decision-making\nchallenges emerge, ranging from the operation of wind farms to the management\nof electrical grids or the scheduling of electric vehicle charging stations.\nAll such problems are well suited for reinforcement learning, the branch of\nmachine learning that learns behavior from data. Therefore, numerous studies\nhave explored the use of reinforcement learning for sustainable energy. This\npaper surveys this literature with the intention of bridging both the\nunderlying research communities: energy and machine learning. After a brief\nintroduction of both fields, we systematically list relevant sustainability\nchallenges, how they can be modeled as a reinforcement learning problem, and\nwhat solution approaches currently exist in the literature. Afterwards, we zoom\nout and identify overarching reinforcement learning themes that appear\nthroughout sustainability, such as multi-agent, offline, and safe reinforcement\nlearning. Lastly, we also cover standardization of environments, which will be\ncrucial for connecting both research fields, and highlight potential directions\nfor future work. In summary, this survey provides an extensive overview of\nreinforcement learning methods for sustainable energy, which may play a vital\nrole in the energy transition.",
  "text": "Reinforcement Learning for Sustainable Energy: A Survey\nKoen Ponse*, Felix Kleuker*, M´arton Fej´er*, ´Alvaro Serra-G´omez, Aske Plaat, Thomas Moerland\nLeiden University\n*Equal contribution\nAbstract\nThe transition to sustainable energy is a key challenge of\nour time, requiring modifications in the entire pipeline of\nenergy production, storage, transmission, and consumption.\nAt every stage, new sequential decision-making challenges\nemerge, ranging from the operation of wind farms to the\nmanagement of electrical grids or the scheduling of electric\nvehicle charging stations. All such problems are well suited\nfor reinforcement learning, the branch of machine learning\nthat learns behavior from data. Therefore, numerous stud-\nies have explored the use of reinforcement learning for sus-\ntainable energy. This paper surveys this literature with the\nintention of bridging both the underlying research communi-\nties: energy and machine learning. After a brief introduction\nof both fields, we systematically list relevant sustainabil-\nity challenges, how they can be modeled as a reinforcement\nlearning problem, and what solution approaches currently\nexist in the literature. Afterwards, we zoom out and iden-\ntify overarching reinforcement learning themes that appear\nthroughout sustainability, such as multi-agent, offline, and\nsafe reinforcement learning. Lastly, we also cover standard-\nization of environments, which will be crucial for connecting\nboth research fields, and highlight potential directions for\nfuture work. In summary, this survey provides an extensive\noverview of reinforcement learning methods for sustainable\nenergy, which may play a vital role in the energy transition.\n1\nIntroduction\nDriven by population growth and higher per capita power\nuse, an already rising global power demand is expected to in-\ncrease further in the coming years. According to the Statis-\ntical Review of World Energy 2023 [1], currently more than\nFigure 1: Potential development of the energy mix from\ntoday towards full sustainability (trajectories are not fully\nbased on real projective data and are just for illustrative\npurposes)\n70% of primary energy1 is derived from fossil fuels.\nThe\ncurrent share of fossil fuels in the existing energy landscape\nneeds to be replaced by sustainable counterparts to mitigate\nenvironmental effects, such as global warming [2], and to be\nless dependent on finite world resources.\nMore generally,\nwe need to meet the (energy) needs of the present without\ncompromising the ability of future generations to meet their\nown needs [3] — a common way to define sustainable en-\nergy. In other words, we must replace non-sustainable pri-\nmary energy sources with suitable sustainable counterparts,\nas depicted in Figure 1.\n1Primary energy is a measure of the energy we use, categorized\nunder its original form found in nature.\n1\narXiv:2407.18597v1  [cs.LG]  26 Jul 2024\nThe backbone of scaling up the total energy supply in a\nsustainable landscape are renewable energy sources, such as\nwind or solar. However, in contrast to fossil power plants,\nthese sources do not provide the energy on demand, which\nposes a variety of optimization challenges. While tradition-\nally we were to fit the supply according to demand, with\nrenewable energy sources, the supply needs to be allocated\nin an optimal way to guarantee the best usage and stability\nof electrical grids. Components within a grid are faced with\nnew optimization challenges as well. For example, storage\nsystems such as batteries could help balance the load on\nthe grid.\nAnother big avenue is the electrification of the\ntransport sector, further intensifying the challenge of opti-\nmal energy distribution, e.g. in the form of EV Charging.\nThe shift away from fossil fuels also requires a tremendous\nscale-up of installed capacity of renewable energy sources,\nrendering optimal installation and operation of these even\nmore important.\nReinforcement learning,\na major branch of Machine\nLearning, aims to solve sequential decision tasks and is well\nsuited to address many of the optimization and control chal-\nlenges in the field of sustainable energy. Machine learning in\ngeneral would help in many regards in mitigating test time\ncosts and recognizing patterns in large amounts of data. Re-\ninforcement learning in particular can aid in finding optimal\nstrategies, usually called policies, just by interacting with\nan environment and receiving partial feedback. In contrast\nto supervised learning, reinforcement learning does not re-\nquire control actions to be individually labeled, but instead\nlearns from the outcomes of its actions through trail-and-\nerror. This in turn allows it to potentially outperform hu-\nman solutions. As such, reinforcement learning is used in\nthe field of sustainable energy with the hope of significantly\nenhancing the efficiency and reliability of these systems [4].\nThis survey aims to connect the machine learning (rein-\nforcement learning) community with the sustainable energy\ncommunity. As such, we will be approaching the field from\nboth angles in different sections.\nFor researchers coming\nfrom the machine learning domain, we group challenges in\nthe sustainable energy field in an easy-to-understand tax-\nonomy and point to research that has been undertaken to\naddress these challenges. Researchers from the energy do-\nmain are presented with a general introduction into rein-\nforcement learning, pointing to possibly relevant literature.\nAdditionally, we group the currently existing literature in\nreinforcement learning challenges that need to be addressed\nin the various energy problems.\nWe find that the field is still relatively young, and major\nreinforcement learning literature has largely not yet found\nits way into applied research.\nNotable topics that would\nrequire more work, in order to get to real-world deploy-\nment, are safe reinforcement learning and offline reinforce-\nment learning. Furthermore, we observe a very wide and di-\nverse number of benchmarks (environments) in the research\nfield, while we should strive to more standardization. This\nstandardization may prove to be the most important bridge\nfor the two research fields to come together. When this hap-\npens, we see a lot of potential for reinforcement learning in\nthe energy field.\nNote that the transition to sustainable energy involves\nboth technical and economic challenges. For this survey, we\nfocus on the former, discussing the technical aspects of gen-\neration, storage, transport, and consumption of sustainable\nenergy.\nOf course, practical deployment will also require\neconomic tools, such as sustainable energy trading, logistics,\nand scheduling. However, these challenges are not specific\nto the energy transition and therefore fall outside the scope\nof this survey.\nOur contributions are as follows:\n(1) We provide an\noverview of the whole energy chain and possible options\nfor reinforcement learning optimization within this chain.\nWe aim to do this in a way that is (2) specifically designed\nto connect the two research fields, the energy field and the\nmachine-learning field, by repeatedly switching focus points.\nLastly (3), we identify pitfalls, bottlenecks, and promising\ndirections for future research.\nThe remainder of this survey is structured as follows. We\nstart with an overview of related surveys on machine learn-\ning and sustainable energy, and how they compare to the\npresent paper (Section 2). Then, Sections 3 and 4 provide\na broad overview of both key topics: Section 3 introduces\nthe sustainable energy landscape (intended for researchers\nfrom the reinforcement learning community), while Section\n4 present an overview of reinforcement learning for energy\nresearchers. Next, The core of the survey follows in Sec-\ntions 5 and 6. The former, Section 5, surveys the full range\nof sustainable energy applications in which reinforcement\nlearning methods have been applied.\nThe structure here\ncomes from the energy side, focusing on production, storage,\ntransport and consumption of sustainable energy.\nThen,\nSection 6 again flips the view to the reinforcement learning\nperspective, discussing the overarching reinforcement learn-\ning themes we encounter along the full sustainable energy\nchain.\nAfterwards, 7 discusses benchmarking and perfor-\nmance metrics, a major topic in all of machine learning,\nand a crucial topic in the bridge between both fields. Lastly,\nSection 8 summarizes and discusses our findings, including\nrecommendations and possible directions for future research.\n2\n2\nRelated Work\nVarious articles have surveyed the use of machine learning\nfor sustainable energy [5–11]. In general, these surveys iden-\ntify much potential for machine learning methods in the\nsustainable energy transition, on a wide range of applica-\ntions. However, these overviews primarily focus on super-\nvised learning techniques, where we try to forecast certain\nproperties, such as climate predictions. Although some sur-\nveys include reinforcement learning and control methods [5],\nthis is generally not the main focus.\nSeveral surveys do specifically cover reinforcement learn-\ning methods for sustainable energy [4, 12, 13], but these\ntypically zoom in on a specific subfield of the entire sus-\ntainability pipeline, such as the electricity grid [4, 12] or\ndemand response [13]. In contrast, the present survey cov-\ners all steps of sustainability, from production (e.g., solar\npanels, wind farms) to storage (e.g., hydrogen), transport\n(e.g., electricity grids) and consumption (e.g., smart build-\nings, electrical vehicles). The present paper thereby provides\nan integrated view of the entire sustainability chain, whose\nindividual challenges are often closely intertwined.\nThere are two additional motivations for our survey. First\nof all, the developments in machine learning for sustainable\nenergy move incredibly fast. A search using the Arxiv API\nwith keywords ”Reinforcement Learning” and ”Sustainable\nEnergy” reveals 1798 papers, of which 1486 (83%) have been\npublished in or after 2020. The field has therefore moved in-\ncredibly fast, and previous surveys of reinforcement learning\nand sustainable energy [4, 12, 13] have not covered this large\npart of the literature.\nMoreover, we also observe that previous overview pa-\npers predominantly originate from the ‘energy literature’\n[9, 10, 12, 13].\nMachine learning and sustainable energy\nare of course two separate research fields, and the bridge\nbetween two communities often takes effort from both sides.\nIn general, we observe energy researchers have started ex-\nploring machine learning techniques for their problems, but\npure machine learning researchers have more trouble enter-\ning the field, probably because they lack clear benchmarks\nand problem definitions (see Section 7 as well). An addi-\ntional goal of this survey is therefore to provide a bridge from\nthe machine learning perspective, also terminology-wise —\nin hope of finding common ground.\n3\nAreas of Sustainable Energy\nEnergy-related processes often follow the same pattern: en-\nergy is produced, stored, transported, and finally consumed.\nThis segmentation of energy provides a natural taxonomy,\nas shown in Figure 2. All four areas of this taxonomy pro-\nvide distinct challenges and opportunities for sustainability\nimprovements through reinforcement learning. The remain-\nder of this section introduces the main set of challenges that\nappear in each of the four areas.\nNote that although we\nattempt to separate problems into their respective areas for\nclarity, real-world problems may deal with multiple parts of\nthe energy chain and need to be optimized together.\nGeneration\nTo ensure that our society is powered by sustainable energy,\nit is important that the energy produced comes from renew-\nable sources. These sources include (1) Hydropower, which\nproduces electricity primarily by converting the potential\nenergy of water, stored in reservoirs, through dam infras-\ntructure. (2) Solar power, producing electricity from solar\nradiation. (3) Wind power, capturing kinetic energy to con-\nvert into electricity. More niche areas include (4) Tidal and\n(5) Geothermal power. Furthermore, because of its closed\ncarbon cycle, (6) energy from biomass is also considered re-\nnewable.\nLastly, (7) nuclear fusion would feature enough\ncharacteristics of renewable energy, due to the plentiful sup-\nply of fuel (hydrogen) and the absence of harmful long-term\nwaste products — an attribute that distinguishes it from\nnuclear fission, which we exclude from our study due to\nthese inherent byproducts.\nSimilarly, we exclude any en-\ndeavors solely aimed at optimizing fossil power plants from\nthis study.\nStorage\nEnergy is often produced non-locally, requiring us to trans-\nport it in time (storage) and space (transmission) to reach\nthe consumer at the right time and location. Storing elec-\ntricity requires us to convert it into a different form of po-\ntential energy. For example by pumping water into lakes\nof hydro power facilities. Electricity can also be stored in\nchemical (inner) energy, like batteries or hydrogen. Often,\ngeographical factors such as natural height and access to\nwater heavily influence the choice of energy storage. This\nsurvey includes EV batteries and high capacity storage, but\nexcludes small-scale batteries because the immediate impact\non sustainable energy is not known.\nTransmission\nSustainable energy is primarily transported over electricity\ngrids, which move electrical energy from producers to con-\n3\nFigure 2: Overview of energy systems and our taxonomy used throughout this survey. We separate sustainable energy\nsystems in one of four pillars: generation, storage, consumption and transmission. Notably, transmission links all the\nvarious energy areas. This taxonomy will be used primarily in Sections 4 and 5, where we briefly introduce each pillars,\nand dive deeper into the active research in each of the pillars, respectively.\nsumers over networks of transmission lines [14].\nThe key\nchallenge of energy grids is to match supply and demand,\nwhile at the same time ensuring grid stability (e.g., volt-\nage and frequency control to ensure both stay within safe\nbounds). Demand is relatively fixed, although we may try\nto influence it through demand response, for example by\nvarying the price of electricity throughout the day. How-\never, the main challenge of grids is to match supply to the\ndemand, a challenge that gets aggravated with sustainable\nenergy sources. For example, a traditional fossil power plant\ncan produce electricity at any required moment, but so-\nlar power is only available when the sun is actually shin-\ning.\nThis generates challenges for energy dispatch (when\nand where do we release energy into the grid) and energy\nmanagement (how do we locally store and route energy to\nefficiently operate a (mini-)grid). All these challenges get\npronounced with sustainable energy sources, since they are\ninherently distributed (spread out) and have variable and\nuncertain production profiles [14–19].\nFinally, note that other means of energy storage, such as\nhydrogen, also require other means of transmission, for ex-\nample by trucks or pipelines. However, due to the variability\nof sustainable energy production, leading to a more signif-\nicant duck curve [20], electricity grids require a significant\nchange in control operation. This change is not something\nwe naturally see in trucks or pipes.\nIn turn we focus on\nelectricity grids as the primary technical challenge in trans-\nmission of sustainable energy.\nConsumption\nHistorically, we have mostly balanced energy supply and de-\nmand through supply (aided by storage solutions). However,\nthe rise in global electricity useage and ongoing electrifica-\ntion of society drive us to seek for more effective methods\nof managing and reducing energy demand. With recent in-\ncreases in energy prices, such optimizations are likely to be-\ncome a growing field of interest.\nTypical consumers of energy include houses and offices,\nwhere the cost of energy is primarily driven by heating and\ncooling installations. Different challenges are found in what\nwe categorize as mobile consumers, encompassing mainly\nelectric vehicles (EV’s) and their required charging stations.\nFinally, our last category of energy consumers is the indus-\ntry sector. Here, reinforcement learning sustainable energy\nmethods may be designed to address highly specific needs,\n4\nwhich is justified by the volume of energy use.\n4\nReinforcement Learning Basics\nIn many of the domains that are discussed in the previous\nsection, we are faced with optimization problems such as\nmaximizing energy generation, minimizing power usage, or\noptimizing power allocation. These optimization tasks of-\nten require us to decide on multiple actions to obtain good\naverage performance over a long time horizon. These prob-\nlems are known as sequential decision-making problems, for\nwhich we may employ reinforcement learning.\nReinforce-\nment learning is a machine learning approach for finding an\noptimal policy by interacting with an environment.\nIt is\noften used for sequential decision-making problems, where\nactions from the past influence states into the future.\nInformally, reinforcement learning problems consist of an\nagent and an environment. The environment is in a state,\nand after an agent chooses an action, the environment fol-\nlows a transition function to determine both the new state\nand a reward, a numerical value indicating how ”good” the\nnew state is. The goal of the agent is to find a so-called pol-\nicy of optimal actions for each state, thereby solving the re-\ninforcement learning problem by sampling the environment\nwith its actions.\nMore formally, the sequential decision-making problem\ncan mathematically be defined as a Markov decision pro-\ncess (MDP) [21, 22] , defined as a tuple (S, A, p, r, p0, γ).\nThe state space S is the set of all states, the action space\nA is the set of all possible actions, p is the transition dy-\nnamics distribution p : S × A →∆(S), (s, a) 7→p(s′|s, a),\nr is the reward function that maps transitions into rewards\nr : S × A × S →R, p0 ∈∆(S) is the initial state distribu-\ntion and γ ∈(0, 1) is the discount factor, which governs the\nimportance of future rewards. In an MDP, the transition\ndynamics distribution is Markovian, meaning that the tran-\nsition to a next state, given an action only depends on the\ncurrent state, i.e. p(st+1|st, at, . . . , a0, s0) = p(st+1|st, at).\nTo provide some intuition, we briefly present some exam-\nples on how to formulate energy problems as an MDP in\nTable 1.\nTo select actions in any given state, a policy is used, that\nis, a mapping π : S →∆(A), s 7→π(a|s). Alternating se-\nquences of states and actions are usually denoted as trajec-\ntories τ = (s0, a0, s1, . . . , sT ), and each policy induces a dis-\ntribution pπ(τ) over such trajectories in an MDP. For a given\ntrajectory, the return Gt is defined as the total discounted\nrewards from time-step t (resp. state s = st) onwards\nGt =\n∞\nX\nk=t\nγk−t r(sk, ak, sk+1).\nValue functions for a given state or a state-action pair are\ndefined as the expected return given a fixed state or a state-\naction pair, respectively:\nVπ(s) = Eπ[Gt|st = s]\nQπ(s, a) = Eπ[Gt|st = s, at = a].\nSolving an MDP is defined as finding a policy π∗such that\nit maximizes the expected return of trajectories:\nπ∗∈arg max\nπ\nEτ∼pπ[Vπ(s0)].\nFinding an optimal policy is usually done in an iterative\nfashion: Knowledge about the values Qπ under a given pol-\nicy can be used to improve the policy, for example, by acting\ngreedily w.r.t. the current values, naturally being denoted\nas a policy improvement step. This, in turn, requires a new\nevaluation of the values under this new policy, a step usually\ncalled policy evaluation. Generally, this scheme of alternat-\ning between policy evaluation and policy improvement is\ncalled Generalised Policy Iteration (GPI).\nThe policy evaluation step can be done by utilizing the so-\ncalled Bellman-equations [23], looping over all state-action\npairs and updating the value functions; a method known as\nDynamic Programming [22]. However, solving an MDP with\nDynamic Programming requires access to a known model of\nthe environment, That is, p and r must be known, which,\nfor most real-world applications, is not the case.\nIn reinforcement learning, we therefore assume a setting\nin which the transition dynamics p and reward function r are\nunknown. Additionally, instead of looping over each state-\naction pair, in reinforcement learning, we allow an agent to\ncollect experiences, i.e. trajectories, by interacting with an\nenvironment following a policy π.\nThese experiences can\nthen be used to learn value functions Vπ or Qπ, in what\nis known as value-based methods, or directly learn an ex-\nplicit policy π, in what is known as policy-based methods.\nSome methods use the obtained experiences to learn both\nthe values and an explicit policy. These methods belong to\nthe class of actor-critic [24] methods and are particularly\npopular in some of the current state-of-the-art algorithms\n[25–30].\nWell-known value-based methods, such as Q-learning [31]\nand SARSA [32], store their learned value functions in a ta-\nble, in computer memory. As a table entry is required for\n5\nTable 1: Example of potential (simple) MDP definitions for two sustainable energy tasks.\nState space (S)\nAction space (A)\nReward (r)\nWind\nCurrent angle of the rotor of\nthe turbine θ, wind speed v\nand direction ϕ\nchange in the rotor angle ∆θ\nenergy produced E(θ, v, ϕ)\nBuilding\nControl\nOccupancy, weather data, en-\nergy price\nHeating temperature setpoint\nMinimize energy cost, while\nmaintaining\ntemperature\nthreshold\neach state-action pair, these tabular reinforcement learning\nmethods may quickly become too memory intensive for real-\nistic applications. To combat this, we use function approxi-\nmations that generalize over the state-action space, building\nup on the advancements made in Deep Learning. The value\nfunctions [33, 34] or policies [35] are encoded by a param-\neterized function, generally neural networks. This field is\nnow known as deep reinforcement learning, and has seen\nnumerous successes over the years [36–41].\nVarious sub-fields in reinforcement learning have emerged\nfor solving problems encountered in different environments.\nMost notably, since MDPs require the transition dynam-\nics to be Markovian, the states are required to contain all\nrelevant information. However, in many problems, not all\ninformation is known (hidden or stochastic information).\nSuch environments are said to be partially observable MDPs\n(POMDPs) [42, 43] and other methods exist to approxi-\nmately solve them with reinforcement learning.\nFurther-\nmore, while (good) simulators may not always be available,\ndatasets of actions, observations, and rewards may have\nbeen generated by real-world installations. Applying rein-\nforcement learning on this dataset, without a simulator, is\nknown as offline reinforcement learning [44, 45].\nIn particular, model-based algorithms [46, 47] may work\nwell in offline reinforcement learning problems.\nUnlike\nmodel-free algorithms, model-based algorithms do have ac-\ncess to the transition dynamics and rewards, either via a\nknown or a learned model.\nReinforcement learning is a rich and active field of re-\nsearch, and there are more methods for different problems.\nAlthough we provide some more explanation on some sub-\nfields in Section 6, explicit explanation of every type of prob-\nlem is beyond the scope of this survey. However, various\nother surveys exist on topics that come up in later sections,\nsuch as multi-agent reinforcement learning [48], exploration\n[49], and safe RL [50]. For a comprehensive introduction\nto tabular reinforcement learning, we recommend Sutton &\nBarto [22] and Kaelbling et al. [51], for deep reinforcement\nlearning we recommend Francois-Lavet et al. [52] and Plaat\n[53].\n5\nApplications\nof\nReinforcement\nLearning in Sustainable Energy\nThis section focuses on the use of reinforcement learning, as\nintroduced in Section 4, in the different sustainable energy\nareas that are introduced in Section 3.\nWe will go over\neach area of our taxonomy, discuss sub-areas, and highlight\nproblems that are currently being worked on together with\nhow researchers have so far addressed these problems.\nFirst, Section 5.1 covers generation: predominantly hy-\ndro, solar and wind, and also smaller fields such as tidal,\nbiomass, fusion and geothermal. Section 5.2 dives into rein-\nforcement learning approaches in energy storage solutions,\ndiscussing batteries, hydrogen and pumped hydro storage.\nNext, Section 5.3 discusses ways optimize energy consump-\ntion and help balance the grid from the energy demand side.\nBuildings, electric vehicles and the industry sector will be\nfeatured here. Finally, Section 5.4 discusses energy grids,\nwhich play a crucial role in connecting all previous com-\nponents (Figure 2). Although all components could be op-\ntimized/learned together, grid literature typically assumes\nthe other components have some static controller.\n5.1\nGeneration\nA natural angle to approach sustainable energy is to improve\nthe efficiency of inherently sustainable sources, mentioned in\nSection 3. Reinforcement learning can help in optimizing the\ncontrol and operation of such energy generation facilities,\nthereby boosting the efficiency. This, in turn, would allow\nto increase the share of these sources and consequently shift\nthe primary energy landscape in the desired direction.\n6\nFigure 3: Overview of sustainable power generation, the first\npillar in our taxonomy.\nHydropower\nHydropower accounts for 16% of the globally produced elec-\ntricity (not to be confused with energy), contributing the\nlargest share of all renewable energy sources [1, 54].\nAl-\nthough hydropower is an excellent source of sustainable\nenergy, geological requirements make it comparatively less\nscalable compared to other renewable sources, such as wind\nand solar energy. Economically viable hydropower poten-\ntial in some areas has now largely been exploited [54, 55],\nand the further enhancement would require substantial in-\nvestment. Kleiven et al. [55] propose an investment model\nusing reinforcement learning to determine an optimal up-\ngrade capacity along with its optimal point in time; this\ndecision is based on projected electricity prices and water\ninflows, modeled as a Markov Decision Process.\nOther models aim to improve the economic viability and\nefficiency of hydropower plants. Xu et al. [56] propose a\ndeep Q network method, where the water level and inflow\nrepresent the state space. The objective is to maximize the\ntotal generated energy by adjusting the release of water.\nAnother work has investigated maximizing total generated\nelectricity in a multi-reservoir (yet single-agent) setting [57].\nA similar approach, focused on optimizing total revenue\nearned is also proposed [58].\nHere, the generated energy\nis multiplied by a forecasted electricity price, introducing\nan additional element of uncertainty.\nWind Power\nWith a 7% share of global electricity production, wind power\naccounts for the second highest production of renewable\nsources [1]. Due to variability in wind speed (and angle), a\nprimary challenge in the optimization of wind power plants\nlies in accurately forecasting wind speed (shifts in direction\ncan usually be adjusted for in real time). Recently, rein-\nforcement learning has been proposed for these prediction\ntasks [59, 60], as apposed to supervised learning that has\nbeen the standard. When coupled with a battery system to\ncompensate for periods of low wind, reinforcement learning\ncan find policies to maintain a stable power supply based on\nthese wind predictions [61].\nNote that it is important not only to adapt to future wind\nspeed predictions, but also to optimize power output and\nbattery charging load in real-time, responding to current\nwind conditions.\nThis is sometimes referred to as Maxi-\nmum Power-Point Tracking, presenting the optimal load to\nthe generator depending on wind conditions [62]. Reinforce-\nment learning has demonstrated promising results in find-\ning optimal policies under variable wind conditions [62, 63].\nFurthermore, the application of reinforcement learning al-\nlows the scaling up of optimization parameters without sig-\nnificantly increasing the inference time, enabling innovative\ndesign proposals for wind turbines [64].\nLastly, note that, as wind farms grow in size, the com-\nplexity of the optimization challenge also intensifies. The\noptimal operational point for this group of turbines is usu-\nally not a linear aggregate of the optima of the individual\nturbine due to interaction effects, such as one turbine being\nin the wind shadow of another turbine. To identify the opti-\nmal operational point for the entire collection under specific\nwind conditions, multi-agent reinforcement learning meth-\nods may offer more optimal solutions [65].\nSolar Power\nElectricity generated from solar power constitutes approxi-\nmately 4.5% of global electricity production, positioning it\nas the third most prolific renewable source [1, 54].\nHow-\never, solar power might be the best candidate for scaling up\nbecause of the abundance of untapped potential to deploy\nphotovoltaic (PV) cells.\nA widely discussed subject in the realm of reinforcement\nlearning for solar power is again the so-called Maximum\nPower-Point Tracking, where the aim is to maximize the\npower produced of a set of PV cells under non-optimal con-\nditions, such as shading [66–73]. Photovoltaic systems have\na unique global optimum under ideal conditions, and multi-\n7\nple local optima otherwise. The challenge lies in adjusting\nthe controllable parameters, such as the voltage, to maintain\nthe operational point at the global maximum. All models in-\ncorporate electric current and voltage in their state spaces,\nyet some studies incorporate additional variables, such as\nsolar irradiance and temperature [68]. Interestingly, while\nearly work in this field employed a tabular reinforcement\nlearning approach [66–71], more recent work mainly adopts\ndeep reinforcement learning techniques [72, 73].\nAlthough these algorithms target efficiency improvements\nin isolated photovoltaic systems, there are also approaches\nthat optimize the performance of photovoltaic systems\nequipped with batteries and grid access. Optimization, in\nthis context, may involve maintaining a specified battery\nlevel [74] or achieving energy neutrality in an energy net-\nwork [75].\nAlternatively to photovoltaic systems, which convert so-\nlar radiation into electricity, there are systems that harvest\nthe heat energy of solar radiation. For example, we may use\nreinforcement learning to optimally control a heliostat field\nto convert sunlight into heat (and subsequently into power)\n[76]. Another study investigates solar fields that generate\nhot water [77].\nLastly, in addition to maximizing power\noutput, reduction of maintenance cost can be crucial to in-\ncrease the economic efficiency. In this context, one study\nuses a reinforcement learning approach for fault detection\nand diagnosis, extracting an optimal strategy through tab-\nular Q-learning [78].\nTidal Power\nIn terms of installed capacity, tidal energy is less significant\ncompared to solar, wind and hydro. However, we have seen a\nsignificant amount of reinforcement learning research in this\ntechnology. Three primary technologies are used to gener-\nate energy from tidal flows. (1) Tidal turbines are similar\nto their wind-driven counterparts and capture the kinetic\nenergy inherent in tidal flows [79]. (2) Wave energy con-\nverters leverage wave motions, resisted by a power take-off,\nto convert kinetic energy into electricity [80]. A more spe-\ncialized approach involves (3) Tidal Range Structures, which\ngenerate power by artificially inducing a difference in water\nlevel between the ocean and a confined area. Turbines then\ngenerate energy by allowing water to balance out this dis-\ncrepancy [81]. As in hydropower, the installation of tidal\npower is limited by geographical constraints (coastal areas).\nWave energy converters consist of a small floating body\nsubjected to wave forces, with its movements countered by\nan electric or hydraulic power take-off system. The control-\nlable damping coefficient that influences the resistance of\nthe power take-off has different optimal values, depending\non the sea state [80]. To optimize this coefficient, Anderlini\net al.\n[80] employ a tabular Q-learning for offshore wave\nenergy converters, specifically for (heaving) point absorbers\n[82]. This work was later extended for onshore wave energy\nconverters [83]. In more recent work, Anderlini et al. [84]\nuse soft actor critic, a deep reinforcement learning algorithm\nto solve similar problems. Notably, newer approaches deal\nwith more modern wave energy converters featuring multi-\nple power take-offs, rather than one. In this more complex\nspace, multi-agent systems have also been tried, based on\nPPO [85].\nTo a lesser extent, reinforcement learning has also found\nits way to tidal turbines and tidal range structures. PPO\nhas been proposed for the latter to maximize the energy\ngeneration of a tidal range structure with multiple turbines,\nin which the inflow rate had to be controlled [81]. For tidal\nturbines, reinforcement learning has been employed for Max-\nimum Power-Point Tracking. A challenge analogous to prob-\nlems in wind turbines, yet under different conditions.\nLastly, the three remaining energy sources — biomass,\ngeothermal energy, and nuclear fusion — identified as in-\nherently sustainable, are not extensively explored in the re-\ninforcement learning literature. However, we believe that\nreinforcement learning has significant potential to impact\ncontrol applications within these domains.\nBiomass processes transform biological matter from\nplants and animals into carbon-based energy carriers, such\nas ethanol or biogas (methane).\nBiomass processes are\nalso denoted as Waste-to-Energy (WtE). Different conver-\nsion methods have been studied, such as thermochemical,\nphysicochemical, and biochemical processes [86, 87].\nThe\npotential application of reinforcement learning in control-\nling these processes is presented by Faridi et al. [88]. In this\nwork, the authors introduce a model-based deep reinforce-\nment learning approach for finding an optimal controller for\na thermochemical gasification process of biomass. Another\nstudy [89] focuses on improving the efficiency of a recovery\nboiler, a device designed to convert a byproduct of the pa-\nper industry into synthetic fuel. The authors used a tabular\nreinforcement learning approach with the specific objective\nof reducing the heat transfer rate.\nGeothermal Energy systems use internal heat of the\nEarth to generate energy, commonly by using naturally ex-\nisting high-pressure water or steam reservoirs.\nAlthough\ngeothermal energy systems are only available in areas with\nsuitable seismic activity, there are various fields around the\nworld [90]. Because of its advantage of offering a more re-\nliable production in comparison to solar and wind power,\n8\ngeothermal energy offers the potential to contribute signif-\nicantly to the future’s sustainable energy output.\nSome\nframeworks modeling the mechanics of a geothermal plant\nhave been created in recent years [91, 92] and these may\nbe adapted for reinforcement learning training in the fu-\nture. Siratovich et al. [93] have conducted small-scale ex-\nperiments in one of the frameworks [91], demonstrating that\nby manipulating several pressure valves, an agent managed\nto increase total energy output over a two and a half year\nperiod.\nNuclear fusion is a promising clean energy source. Much\nof the research effort of harnessing nuclear fusion is aimed at\ncontrolling the very high temperature of the fusion plasma,\noften in tokamaks, a type of experimental fusion reactor. Re-\nsearchers are exploring automated control methods, predict-\ning and mitigating disturbances in magnetic fields, ensuring\nstable and efficient plasma operations under high-pressure\nconditions [94–100]. Deep reinforcement learning techniques\noptimize various parameters and control schemes in toka-\nmak plasmas, intended to improve the shape, duration, and\ntemperature of plasma conditions [94, 96, 97].\n5.2\nStorage\nWhile fossil fuels naturally and efficiently store energy in\nchemical form, allowing flexible matching of supply with de-\nmand, renewable energy sources do not. Consequently, we\nmust improve solutions to store the energy generated from\nrenewable sources in order to match supply and demand.\nPumped Hydro Energy Storage\nAn additional characteristic of some hydropower plants is\nthe ability to operate as energy storage repositories. These\nare commonly called pumped hydro energy storage (PHES)\nsystems and work by letting water flow down to generate\nelectricity; and pumping water back up into a reservoir to\nstore potential energy. The installed capacity of PHES is\napproximately 10% of the aggregate installed capacity ded-\nicated to hydropower [54]. Toufani et al. present a Markov\nDecision Process formulation with a focus on maximizing\ncashflows in converting hydropower facilities into PHES sys-\ntems [101, 102].\nA related MDP formulation forms the\nbackbone of the research by Tubeuf et al. [103], wherein\nthey study a PHES system with specific attention to safety\nconsiderations. The authors develop a digital twin of the\nphysical turbine employed for pre-training the reinforcement\nlearning model.\nIn a parallel study [104], the focus is on replacing a\nconventional PID controller (Proportional, Integral, and\nDerivative) for speed tracking issues with reinforcement\nlearning, to maintain the angular velocity of the turbine\nas consistently as possible. A reinforcement learning-based\ncontroller necessitates fewer adjustments compared to its\nPID counterpart, as it autonomously adapts to diverse sce-\nnarios through the learning procedure.\nBatteries\nBatteries are perhaps the most widely recognized chemical\nstorage system. In this work, we exclude solid-state chemical\nstorage systems such as lithium-ion batteries on the basis of\nthe raw materials used in their production. We refer inter-\nested readers to a survey on reinforcement learning in bat-\ntery storage systems [105]. In contrast to solid-state chemi-\ncal storage systems, liquid-based chemical storage systems,\nspecifically redox-flow batteries, are considered to be well\nsuited to address large-scale energy storage challenges [106],\nin a sustainable way. Within this domain, Sowndarya et al.\n[107] introduce a reinforcement learning framework, based\non AlphaZero [108], to identify new stable candidates. Rein-\nforcement learning has also shown promise in the construc-\ntion of models to predict redox-flow battery state variables,\ncrucial for the widespread deployment of such systems [109].\nHydrogen\nAdditionally, energy can also be stored in the form of\ngaseous chemicals, most notably hydrogen. Unlike in the\naforementioned systems, where conversion, storage, and re-\nconversion occur within the same system, hydrogen storage\nprocesses are typically distributed. Electrolysers, responsi-\nble for the electrochemical conversion of water into hydrogen\nand oxygen using electricity as the energy source\n2 H2O −−→2 H2 + O2,\n(1)\nface a design challenge in identifying suitable catalysts. Al-\nthough we are not aware of reinforcement learning work\non hydrogen electrolyses, reinforcement learning has shown\npromise in catalysis, which may be transferable to hydrogen\nsystems [110, 111]. Other studies explore employing rein-\nforcement learning to reduce operational costs by optimiz-\ning maintenance schedules [112] or enhance operational effi-\nciency in dynamic magnet field-assisted electrolyzers [113].\nBy the laws of chemistry, inverting reaction (1) will release\n(electrical) energy; a process facilitated by devices known as\nfuel cells.\nReinforcement learning-based approaches have\nbeen applied to improve the operational efficiency of solid\noxide fuel cells [114] and proton-exchange membrane fuel\n9\nFigure 4: Overview of sustainability challenges for reinforce-\nment learning in consumption domain.\nChallenges lie in\nsub-fields related to buildings, EV(-charging), and highly\nspecific industrial requirements.\ncells[115], among various fuel cell design examples. Hydro-\ngen, a key element in these conversion processes, necessitates\nstorage. Because of its small size, diffusion in and through\nother materials poses a common challenge. To address this\nproblem, reinforcement learning-based approaches are intro-\nduced to investigate hydrogen diffusion in polymer hydrogen\nstorages [116] and metal alloys [117].\n5.3\nConsumption\nWhen considering sustainable energy solutions, we com-\nmonly investigate energy production. However, improving\nenergy consumption practices can also contribute signifi-\ncantly to achieving our sustainability objectives, and again,\nmany control problems must be addressed. In the following\nsections, we consider three distinct categories of energy con-\nsumers. We first focus on buildings, where the key functions\nare heating, ventilation, and air conditioning systems. Sub-\nsequently, mobile consumption is discussed, which includes\nelectric vehicles and their charging infrastructure. Finally,\nwe discuss the industrial sector, characterized by specialized\nenergy requirements. Certain segments of the industry sec-\ntor still depend on non-sustainable energy sources for parts\nof their product chain. Consequently, modifications in these\nproduction chains are necessary in order to rely solely on\nsustainable primary energy sources in the future.\nHouses & offices\nGlobally, buildings use an estimated 30% of final energy\n[118]. Due to this large power demand and an increasing\nnumber of smart home appliances entering people’s homes,\nthere is a growing interest in optimizing energy-intensive\nsystems in houses & offices.\nIn this area, reinforcement\nlearning has gained a lot of attention recently due to its\nadaptability and ability to learn without data from a reward\nsignal. Generally, we can subdivide the existing literature\ninto systems that optimize a single building, and those that\ninvestigate the capability of buildings for demand response\n— a topic we will also discuss in Section 5.4.\nA large portion of the existing literature attempts to op-\ntimize controllable elements in a single building. The goal\nhere is usually to minimize the energy cost while maintain-\ning a desired level of human comfort [119–133]. These works\nprimarily focus on optimizing HVAC systems (heating, ven-\ntilation, air conditioning), as they are the largest control-\nlable contributor to total energy consumption in buildings.\nHowever, other works extend the action space to a wider ar-\nray of appliances [120, 134], while sometimes also optimiz-\ning for additional objectives such as air quality [135–137]\nor luminescence [138]. All works in this field vary in their\nuse of algorithms, but almost all implement some form of\nmodel-free deep reinforcement learning, often a value-based\nDQN method [121–123, 134, 136, 139–142].\nModel-based\nsolutions are sparse [130–132], but Jeen et al. has shown\npotential for zero-shot model-based solutions [132], whereby\nthe agent quickly adapts to any new building without the\nneed for pre-training or a simulator, which is near-impossible\nto obtain for every building in the world.\nMany works employ an online learning approach—based\non a simulated environment, not on data—for which there\nexists a wide range of open source Gym-based [143] simula-\ntors [144–155], some still actively developed [144, 147, 148],\nthat often leverage building simulators such as Energy-\nPlus [156]. However, others attempt to learn through of-\nfline data [130, 139] or partially use real data in their\ntraining loop, such as building occupancy or weather data\n[122, 129, 134, 137].\nDemand response systems do not take the overall capacity\nof the network for granted and instead investigate whether\nbuildings can play an active role in its overall load.\nBy\ntimely coordination of energy usage, the idea is that build-\nings can collectively flatten energy peaks throughout the\nday. It should be noted that in demand response solutions,\nthe energy consumption and comfort level of each individual\nbuilding are still considered an important part of the opti-\nmization. Again, different simulators exist [157, 158], simu-\nlating multiple buildings and their combined effect on a grid.\nOf particular interest in this field is CityLearn [158], an ac-\ntively developed Gym environment for demand response of\n10\na cluster of buildings. Yearly competitions are organized by\nthe developers of CityLearn [159–161], with increased com-\nplexity and realism each year. In CityLearn, comfort levels\nare assumed to always be satisfied so that building simu-\nlations can be performed in advance.\nParticipants in the\nCityLearn challenges are required to carefully manage bat-\nteries, photovoltaic panels, appliances, and HVAC systems.\nMobility\nElectric vehicles (EV’s) and their charging stations provide\nboth new challenges and opportunities.\nThe intermittent\nload and (combined) large battery provide excellent stor-\nage and/or passive balancing opportunities. However, EV\narrival and departure times are beyond the control of the\ncharging station, which presents new challenges in charging\nand discharging batteries at the right time. In this section,\nwe discuss problems in the control of charging stations, EV\nbattery optimization (sometimes referred to as powertrain\ncontrol), and navigating EV’s in a city whenever they need\nto charge.\nIn the mobility domain, most interest has been on deep\nreinforcement learning methods, disregarding tabular ap-\nproaches.\nPrimary objectives are to meet the energy de-\nmands of electric vehicles within a desired time, while max-\nimizing the profits of a charging station [162–168]. This is\nusually done in a single-agent fashion, where a single charg-\ning station is controlled or a single agent controls multiple\ncharging stations. However, some works study cooperative\nmulti-agent settings where more charging stations are con-\nsidered [164, 166]. In the mobility domain, action spaces\ntypically consist of the power output of the charging sta-\ntions, ranging from a simple total power output [162, 163],\nto a more detailed per-car output [164, 166]. More elaborate\nmethods focus on energy price prediction [166, 167, 169],\ndecide on the energy selling price [164, 168], include power-\ngenerating systems such as solar panels and wind [163], or\nexplicitly consider the load on the macro-grid [163]. It has\nalso been suggested that systems that include varying en-\nergy prices implicitly optimize to balance the grid [168]. Al-\nthough most of the works focus on commercial charging sta-\ntions, one can also optimize the charging station at home,\nusing the car as a battery and selling the electricity back to\nthe grid [169].\nIn addition to controlling charging stations, other works\nfocus on navigating electric vehicles more efficiently to a\ncharging station. Here, an aggregator system observes city\ntraffic, charging station data, and electric vehicle data and\ndispatches the most optimal routes to vehicles that need\ncharging [170, 171]. Alternatively, the reinforcement learn-\ning agent may be part of the vehicle itself, navigating it to\nthe most appropriate charging station [172]. Various works\nhave also employed reinforcement learning for optimization\nof EV or hybrid car battery use [173–177]. Furthermore, re-\ninforcement learning has been shown to be capable of help-\ning design new electric vehicles [178].\nFinally, while most research interest is focused on elec-\ntric vehicles, some works consider hydrogen-fueled cars as\nsustainable modes of transportation.\nHere, reinforcement\nlearning can help optimize a refueling station with on-site\nproduction. The production site is tasked with optimizing\nrevenue by selling hydrogen and balancing the grid, while\nkeeping up with the refueling demands [179].\nIndustry\nIndustrial consumption often has a distinct consumption\nprofile and often a high volume of energy consumption. For\ninstance, data centers are, like houses & offices, primar-\nily concerned with keeping a building within an acceptable\ntemperature while minimizing long-term energy cost. Here,\ncooling is again considered crucial to control as close to 50%\nof the total energy used in data centers is used for cooling\n[180]. Reinforcement learning has been applied to cooling\ninstallations in data centers [180, 181], with Deepmind in-\nstalling model-based reinforcement learning systems in some\nof their centers [182]. Reinforcement learning has also been\napplied in data centers for job scheduling in order to save en-\nergy [183, 184], or to create a large system controlling both\nthe scheduling and cooling systems [185, 186].\nIn greenhouses, various studies optimize the amount of\nsupplementary lighting [187] or humidity, cooling and CO2\nlevels [188–190]. Other works focus on managing industrial\ncooling installations [191] or cold storage facilities [192].\nReinforcement learning is applied in manufacturing facil-\nities to efficiently power down and restart idle machines, to\nconserve energy without interfering with production capac-\nity [193–195]. We note that reinforcement learning may find\nits way into many industrial control operations [196–201],\nbut these are not necessarily related to (sustainable) energy\nand as such are outside the scope of this survey.\nThe complexity of sustainable energy problems in indus-\ntry extends beyond mere scheduling and control operations.\nFor certain industries, particularly in the chemical sector,\nsustainable energy consumption requires not only optimiz-\ning energy usage but also sourcing inputs sustainably. Take,\nfor example, the Haber-Bosch process, which relies on hydro-\ngen for ammonia production, typically derived from steam\nreforming of fossil methane. As discussed in the previous\n11\nsection, reinforcement learning holds promise in sourcing\nhydrogen sustainably, which could serve not only storage\npurposes but also as an input chemical for the chemical in-\ndustry [112–117]. Furthermore, reinforcement learning has\nbeen used to explore new synthesis pathways [202], poten-\ntially enabling alternative, sustainable routes. Additionally,\nreinforcement learning can improve the design of (chemical)\ncatalysts to improve both energy efficiency and input uti-\nlization [110, 111, 203].\n5.4\nElectrical Grids\nWe will now turn to the use of reinforcement learning in en-\nergy transmission, focusing on electrical grids. This is the\nsustainable energy area where reinforcement learning has\nseen most applications, both in main grids and microgrids.\nThe central challenge is to match supply and demand/load,\nwhile retaining grid stability and optimizing energy/cost ef-\nficiency. This challenge becomes especially pronounced with\nsustainable energy sources, since their production profiles\nare generally uncertain and highly variable (known as inter-\nmittency) and their locations are spread out (distributed).\nThe overall structure of the electricity grid is depicted in\nFig.\n5.\nOn the left of the figure, we see large-scale en-\nergy production and storage facilities that can release en-\nergy into the main grid. This includes renewable sources,\nsuch as wind and solar energy, and storage stations, such\nas hydrodams and batteries, as well as – for now – tradi-\ntional (fossil) power stations. These sources enter energy\ninto the main grid, where it is transported over longer dis-\ntances through high-voltage transmission lines (top part of\nthe figure). Energy thereby ends up at the consumer side\n(right side of the figure), which includes industry, commer-\ncial buildings, and residential houses. Here, we also see the\nappearance of microgrids, which group a subset of the grid\ninto an independently controllable unit.\nIn this section we discuss the main challenges that appear\nwithin the electricity grid, as marked by the yellow circles\nin Fig. 5.\nFirst, we will discuss key economic aspects of electric-\nity grids, which are vital for its operation. This includes\n1) the aggregation of energy production and consumption\nsites into stable market entities, 2) demand response, where\nwe attempt to influence consumers to move energy use to\noff-peak hours, thereby spreading out demand. Afterwards,\nthe grid needs to be actually operated and stabilized, which\ninvolves 4) dispatch, i.e., the timing of actually release of en-\nergy into the grid, as well as 5) energy management, where\nwe utilize storage capacity throughout (mini-)grids to (lo-\ncally) match supply and demand. Finally, we also need to\nensure the 6) stability of the grid. This involves voltage and\nfrequency control, to keep both within prescribed bounds,\nas well as power flow, to effectively route energy through\nthe grid. The location in the grid where each of the above\nchallenges primarily appears is visualised in Fig. 5.\nAggregation\nAggregation deals with combining energy production or con-\nsumption sources into stable entities that can participate in\nthe energy market\n[204]. Sustainable energy sources typ-\nically 1) vary in production profile (‘intermittant’) and 2)\nvary in location (‘distributed’).\nTo enter these products\ninto the market, we may groups production and storage sites\ninto Virtual Power Plants (VPPs) [205]. A VPP may, for\nexample, combine solar and wind energy with energy from\na hydrogen storage plant. From the outside (to the market)\nthis combination gives the appearance of a stable classical\npower plant, while it internally combines several variable\nenergy resources. Similar principles can be applied on the\ndemand/load side of the grid, where consumers get grouped\ntogether into a single market entity. As such, aggregation\nallows the production, storage and consumption part of the\nsustainable energy chain (Fig. 2, discussed in the previous\nsections) to enter the transmission grid as economic entities.\nMDP formulations of aggregation tend to define states\nbased on predicted production profiles of sustainable energy\nsources, the current status of energy storage sites, and the\npredicted energy demand. All of these are inherently un-\ncertain and are therefore often separately forecasted [206–\n211]. The reinforcement learning agent then needs to decide\non how production sources or consumption loads should be\naggregated. Reward functions typically model economic vi-\nability, which takes the revenue obtained from the market\n[208, 209] and subtracts internal costs, for example due to\nover- or undercharging of storage capacity [207], due to buy-\ning additional energy at the market when internal resources\nare insufficient [207], or due to the activation of a safety\nshield to keep the system within operational constraints\n[208]. Note that reward functions may also optimize cost\nat the customer/demand side [211].\nDue to the many factors involved, aggregation problems\ntypically have a high-dimensional state space, which war-\nrants the use of deep reinforcement learning. Both value-\nbased and actor-critic methods have been applied in this\ncontext [206–212]. Aggregation is typically formulated as\na scheduling problem of discharging generators [206], charg-\ning/discharging of storage [207, 209–211] or activation/deac-\ntivation of flexible loads on the demand side [206, 207, 212].\nSome methods adopt a multi-agent approach, where they\n12\nFigure 5: Overview of electrical grid connectivity and control possibilities. Power stations (left), e.g. traditional plants,\ndistributed energy resources, battery farms, hydrodams, etc., feed electricity into the network. High-voltage transmission\nlines (top) then route energy over long distances, facilitating transmission between generation sources and distribution\nsubstations. Along the way, the grid contains substations to ensure the safe routing and operation of electricity, serving as\njunction points where voltage levels are adjusted and power flow is managed. Distribution substations further reduce the\nvoltage for distribution to end-users (right), such as households, commercial buildings, and industrial facilities. At these\nsubstations, electricity is directed to distribution transformers, which further adjust voltage levels for local distribution.\nMicrogrids (bottom right) and controllers provide localized energy management solutions, effectively separating local\ncontrol from the main grid control problem.\ntreat VPPs as a decentralized system [213, 214]. Other work\nfocuses on the economic participation of aggregators in the\nmarket. Examples include finding optimal bidding strate-\ngies [208, 210, 215] or adjusting pricing schemes [211, 215].\nDemand Response\nAlthough we typically assume that supply has to match a\ngiven customer demand, we may also flip the problem and\ntry to influence demand, better known as demand response.\nWhile Section 5.3 discussed demand response from the point\nof view of consumers, actively deciding on local behavior,\nthis section discusses the demand response programs that\nwould entice this behavior, typically through pricing mech-\nanisms [216]. The overall objective is to minimize energy\ncosts [217, 218] or reduce peak demand, which enhances\ngrid stability by spreading out energy consumption over time\n[211, 219, 220].\nReinforcement learning solutions for demand response\nprograms usually model states based on energy demand,\ngrid load profiles, and (renewable) energy availability [211,\n212, 219, 220]. The chosen actions then dynamically adjust\npricing schemes [217, 218], while the reward function may\ntrade-off the benefit of peak demand reduction with the cost\nof interrupting end-consumer loads [212, 219, 220]. Demand\nresponse schemes are often implemented in aggregated forms\n[217, 219] (see previous section), which can entail a multi-\nagent setting [219]. Both value-based [212, 218, 220] and\npolicy-based [217] methods have been applied.\nDispatch\nDispatch refers to the actual release of energy resources into\nthe electricity grid. This may involve release from sustain-\nable production sites (e.g., a solar farm), storage sites (e.g.,\na hydrodam) as well as from traditional fossil plants (which\nare still indispensable in the current energy landscape). Dis-\npatch has always been a challenging problem, since energy\nresources are spread out, demand profiles are uncertain, and\nstorage capacity is constrained [15, 221, 222]. However, the\ndispatch challenge gets aggravated with the increase in sus-\ntainable energy sources, since their production profiles are\n13\nFigure 6: Schematic illustration of a microgrid: localized\nenergy management within a specific geographic area or fa-\ncility (e.g., a commercial/industrial building). Microgrids\nintegrate different energy production sites, storage sites,\nconsumer/prosumer households, electricle vehicle charging\nstations, etc.\nEnergy management within the minigrid is\noptimized through a central microgrid controller. DER =\nDistributed Energy Resources.\nmuch more uncertain and variable.\nMDP formulations of dispatch problems typically include\n(partially-observable) state variables from multiple locations\nover the grid. These may, for example, include current en-\nergy generation levels, demand forecasts, state-of-charge of\nstorage systems, and other grid operating conditions [222–\n226]. The available actions may then adjust the output of\ncontrollable energy production resources [222–226] or energy\nstorage sites [223, 224], as well as manage connections within\nthe grid\n[222]. Finally, objective/reward functions try to\nmatch the energy demand while optimizing for economic\ncost, energy efficiency and/or reliability [227]. Importantly,\nwhile classic economic dispatch [228, 229] focuses on en-\nergy efficiency and cost, generation dispatch (also known as\nrenewable integration dispatch) explicitly aims to minimize\ndispatch from undesirable sources (such as fossil fuels) [230].\nSeveral papers have applied deep reinforcement learning\nin dispatch optimization. Approaches include both value-\nbased [226] and policy-based methods [226], with actor-critic\nmethods (which combine the two) being the most popu-\nlar [223–225]. The dispatch problem may also be formulated\nas an hierarchical MDP, where the effect of certain actions\nextends over a longer timescale, allowing the agent to better\nbalance short- and long-term energy demands [222].\nEnergy Management\nEnergy management involves the effective operation and sta-\nbilization of the grid itself. For example, in a microgrid we\nmight temporarily have excess energy which we can either\nstore in a battery for later use [207, 223] or sell back to\nthe main grid [231, 232]. Energy management becomes ex-\ntra challenging with sustainable energy sources, since their\nproduction is highly variable and distributed (e.g., many\nhouses in a minigrid might have solar panels). Note that\nenergy management is a pervasive and broad term, and it is\noften addressed concurrently with other problems from this\nsection.\nEnergy management MDP formulations typically model\nthe state based on the availability of energy resources, en-\nergy demand, energy prices, and storage capacity [213, 231,\n233–238].\nThe action space may consist of load schedul-\ning (i.e., when will a certain demand get activated) [233],\nstorage operations (i.e., charging or discharging a battery)\n[213, 232, 233, 238], and grid interactions (e.g., trading sur-\nplus energy to the main grid [213, 231, 232] or changing the\noutput of a generator [222, 234, 236–238]). Reward functions\ntypically need to balance multiple objectives, such as mini-\nmizing costs [213, 231, 233–235] and maximizing the use of\nrenewable energy [213, 233]. In addition, it might also con-\nsider objectives that reduce battery charge/discharge oper-\nations [231, 233], ensure grid stability [236, 237], or consider\nan environmental cost [233–235].\nTo effectively address these challenges, the majority of\napproaches implement a deep reinforcement learning frame-\nwork [213, 231, 234, 235, 238].\nIn all cases, considerable\nattention is given to the detailed modeling of the grid con-\nfiguration to enable realistic simulations [213, 231, 233–238].\nSome methods also utilize the underlying graph structure of\ngrids in their solution approach [235, 237]. Although most\nmethods approach the problem from a centralized perspec-\ntive [234, 236, 238], several studies have also shifted to a\nmulti-agent formulation. For example, one may include in-\ndividual prosumers in a microgrid as separate agents [239],\nintegrate energy suppliers as decision-makers [240, 241], or\nconsider multiple microgrids as agents connected to the same\ndistribution line [231, 233].\n14\nVoltage/Frequency Control & Power Flow\nDuring grid operation, we also need to ensure that both the\npower line voltage and the power line frequency stay within\nsafe bounds. This problem becomes more challenging with\nsustainable energy sources, especially due to their uncer-\ntain output profiles [242]. Voltage and frequency may be\naddressed by primary controllers (which make real-time ad-\njustments at the substation level) or secondary controllers\n(which make longer timescale corrections across larger sec-\ntions of the power grid) [243].\nMDP formulations of primary power/frequency control\ndefine states based on physical variables such as voltage\nmagnitudes, phase angles, power flows, and system fre-\nquency [244, 245]. For primary power/frequency controllers,\nthe action space includes adjustment of reactive power out-\nput, switching of capacitor banks, or modification of tap po-\nsitions of transformers. The action space of the secondary\ncontrollers then typically adjusts the setpoints of these pri-\nmary controllers [246, 247]. Finally, reward functions aim to\nmaintain voltage/frequency levels within a predefined safe\nrange, while ensuring power quality (i.e., keeping fluctua-\ntions low) [244–247]. Note that additional objectives may\nalso be included, such as minimization of the energy loss on\ntransmission lines [248].\nDeep reinforcement learning approaches for voltage con-\ntrol have often taken an actor-critic approach, where the\nactor adaptively sets the continuous action space voltage\nsetpoints [244, 245, 247, 248].\nHowever, value-based ap-\nproaches have also been tried and generally match or out-\nperform rule-based baselines [245–247]. Larger networks of\nprimary controllers can also be modeled as a decentralized\nmulti-agent reinforcement learning problem, thus providing\nan alternative to secondary controllers [249]. Overall, rein-\nforcement learning methods for voltage and frequency con-\ntrol have shown promising results in simulation, but their\nreal-world value, of course, depends on the quality of the\nunderlying simulation models.\nFinally,\npower flow optimization [250] is a bridging\nfield between high-level energy management (previous sec-\ntion) and low-level voltage/frequency control (this section).\nPower flow involves optimization of a complete electrical\nsystem, which may involve voltage control but also trans-\nmission line switching [237] or dispatch from energy gen-\nerators [236, 237]. Similar to energy management methods,\npower flow approaches may also utilize the underlying graph\nstructure of the problem [237, 251, 252]. As such, power\nflow optimization overarches the space between high-level\nenergy management and lower-level voltage/frequency con-\ntrol, where the exact boundaries vary between papers.\n6\nReinforcement\nLearning\nChal-\nlenges\nWhile the previous section summarized the literature from\nthe sustainable energy point of view, we now revert our view-\npoint and focus on the overarching reinforcement learning\nthemes that appear throughout the literature. These cen-\ntral reinforcement learning topics include multi-agent RL,\npartial observability, model-based RL, offline RL, and safe\nRL. The connection between both directions (sustainability\nproblems and reinforcement learning solution methods) is\nvisualized in Table 2. Clearly, these reinforcement learning\ntopics are not mutually exclusive, e.g. one might deal with\na partially-observable multi-agent setting.\nMulti-agent RL\nConventionally, reinforcement learning is concerned with\nfinding the optimal policy of just a single agent. However,\nmany problems are modeled as multi-agent problems where\nany number of agents act in the same environment, jointly\naffecting its state space [48]. As a single agent is now no\nlonger solely affecting the transition function, each agent\nwill perceive a higher amount of unpredictability, increas-\ning the state space and destabilizing the learning process.\nSometimes agents may communicate, or cooperate. How-\never, in some scenarios, there may be delays in communi-\ncation, agents may not wish to share preferences, or there\nare privacy concerns [233]. Furthermore, distributed multi-\nagent environments may also be used as a means to model\nlarge state-action spaces.\nDistributed state-action spaces\nare smaller and easier to train on. A popular method for\nlarge multi-agent state spaces is to train algorithms accord-\ning to centralized training, decentralized execution (CTDE).\nHere, multi-agent systems can exchange certain attributes\nduring training, to increase efficiency, but this interaction is\nremoved in deployment.\nIn sustainable energy reinforcement learning research,\nproblems are often modeled as multi-agent problems, often\nin a collaborative manner. Frequently, these studies resort\nto improved trainability by splitting up larger state-action\nspaces over multiple agents, or CTDE. This is for example\ndone in building control, controlling multiple appliances or\nmultiple zones [120, 123, 125, 134, 137, 139], EV charging\nstations [164, 166], hydrogen refueling stations [179], energy\nmanagement in grids [233, 241], voltage control [249] and\npower flow optimization [219], as well as in energy genera-\ntion. Here, for example, windparks may be modeled as dis-\ntributed multi-agent systems, splitting up the state-action\n15\nTable 2: An estimate of how popular certain reinforcement learning challenges/topics are within different sustainable\nenergy fields field. This table only indicates to what extent the current literature, included in our survey, deals with\nvarying reinforcement learning problems. As such, this table may indicate a research gap for those interested in sustainable\nenergy reinforcement learning problems.\nMulti-Agent RL\nPartial Observability\nModel-based RL\nOffline RL\nSafe RL\nGeneration\n✔\n✔\n✔\n✔\n✔\nStorage\n✔\n✔\n✔\n✔\n✔\nConsumption\n✔\n✔\n✔\n✔\n✔\nGrids\n✔\n✔\n✔\n✔\n✔\nspace [65, 85].\nIn demand response, systems that control multiple build-\nings [253] are also a natural match for multi-agent reinforce-\nment learning. [159–161]. While more advanced multi-agent\nsystems are infrequent in the literature, one study models\nits multi-agent system of cells in a hierarchical structure, di-\nviding the wind farm into segments. Within these segments,\nagents are able to exchange information with relevant nearby\nagents [75].\nPartial observability\nA standard Markov Decision Process (Section 4) assumes\nperfect information.\nHowever, most real-world problems\nare actually Partially-Observable Markov Decision Processes\n(POMDPs) [254]. Partial observability refers to the fact that\nthe current observation often does not capture all informa-\ntion of the ground-truth state of the system [255]. For exam-\nple, in a first-person view navigation task, the current obser-\nvation does not provide information about the environment\nbehind us, and in a card game one agent may not know the\nhidden cards of their opponent. This partial observability\nmay be mitigated by incorporating additional information\nfrom historical observations, i.e., a form of memory. How-\never, taking our entire history into account quickly becomes\ncomputationally infeasible.\nPartial observability has been studied extensively in the\nreinforcement learning literature, for example for policy es-\ntimation [256, 257], value function estimation [258, 259], and\nthe dynamics model [260, 261]. Key methods in deep learn-\ning that are used to address partial observability include\nwindowing/framestacking [262], recurrent neural networks\n[263–265] , transformers [266] , external memory methods\n[267] and state space models [268, 269]. Another approach\nfor addressing POMDPs involves utilizing belief systems,\nwhich maintain a probabilistic representation of the agent’s\ncurrent state based on past observations and actions [270].\nIn sustainable energy research, partial observability is\nprevalent.\nMany problems involve time-series data, such\nas weather data [68, 77, 120, 189, 271, 272], dynamic elec-\ntricity prices [58, 141, 168], and occupancy data in buildings\n[138, 273]—all based on processes that are difficult to fully\nobserve.\nA number of studies simply ignore the partial observabil-\nity in these problems [120, 128, 134, 165], treating the prob-\nlem as a stochastic problem, which is sometimes effective.\nOthers try to address the problem by engineering histori-\ncal information for specific features [76, 166, 167, 169, 274],\nor future (predicted) features [125, 131].\nThis approach\nmay enhance performance, but is dependent on the abil-\nity of the engineer to construct the right set of features.\nOther methods found in the literature use frame stack-\ning [132], or include LSTM layers in the neural network\n[85, 119, 166, 169, 181, 183, 225, 241].\nBelief systems are only rarely used in sustainability re-\nsearch to deal with partially observable problems. In one\nstudy, it is used exclusively to model beliefs about the states\nof other agents in multi-agent formulations [233]. The be-\nlief states are used to model the possibility of communica-\ntion failures between different households that need to co-\noperate within an electrical grid. The limited use of this\nmethod aligns with the trend in general reinforcement learn-\ning, where belief systems are becoming less common due to\ntheir inability to scale and the requirement of inserting a lot\nof prior knowledge.\n16\nFigure 7: (a) Online Reinforcement Learning: The improved policy is continually updated and integrated into the system,\ndirectly influencing the agent’s behavior. (b) Offline Reinforcement Learning: Transition data is collected in advance, and\na policy is derived from this data without the typical feedback loop seen in Online Reinforcement Learning (Based on\n[275], figure 1).\nModel-based RL\nWhile model-free reinforcement learning methods directly\nlearn a solution (value or policy) from observed transition\ndata, model-based reinforcement learning [46] instead first\nlearns the transition and reward dynamics (p(s′|s, a) and\nr(s, a, s′), see Section 4) of the problem.\nThis effectively\nrecovers an internal model in the agent of the true envi-\nronment MDP. The agent can subsequently plan using its\nlearned model, without consulting the environment, to up-\ndate its solution [276], which can greatly increase sample\nefficiency [277]. Note that this approach has connections to\nmany other challenges in this section: model-based RL is a\ncommon approach in the offline setting [278, 279], planning\nover a model may help to ensure safety guarantees [280, 281],\nand the model itself often suffers from partial observability\n(all discussed in different parts in this section).\nSome methods first learn a dynamics model from an offline\nfixed dataset, and subsequently use the model as a simulator\nfor training [183].\nOther methods build or improve their\nmodels during training in a simulator and use these models\nfor planning [88]. In addition to planning, trained models\ncan also be used to generate additional training samples, in\naddition to the training samples generated by the real world\nor a simulator [189].\nIn the building control domain, a direct comparison has\nbeen made between model-free methods and their model-\nbased counterparts, based on MBPO [282]. Here, the au-\nthors demonstrate that model-based methods generally con-\nverge faster and may potentially deliver superior final perfor-\nmance compared to model-free methods [131]. This impor-\ntance of convergence speed is underlined by Jeen et al. [132],\nwho note that it is infeasible to construct simulators for\nevery building to pre-train on.\nAs previously mentioned,\nmodel-based reinforcement learning methods are also found\nin Google data centers, where a model is trained for planning\nusing model predictive control [182].\nNote that model-based approaches excel when the transi-\ntion model is exactly known, such as in Chess or Go [283].\nHowever, when the transition model is learned from data,\nmodel-based reinforcement learning can become unstable,\ndue to model uncertainty [284] and accumulating errors dur-\ning planning [282].\nHowever, model-based methods have\nbeen reported to achieve state-of-the-art performance [285],\nand could be a valuable approach in various sustainability\nchallenges.\nOffline RL\nAnother area of reinforcement learning that is used fre-\nquently in sustainable energy is offline reinforcement learn-\ning. Vanilla online reinforcement learning methods assume\naccess to an environment which we can continuously query\nfor new transition and reward data. However, for many real-\nworld problems we do not have a (good) simulator because\nwe do not know the transition function, and we cannot afford\ncontinuous (exploratory) interaction with the real system ei-\nther — for example, because it needs to supply customers\nand a break in service is unacceptable for business or eth-\nical reasons.\nIn those cases, we may be able to obtain a\nbatch of transition data (data in the form of {state, action,\nreward, next state}) from the real system and want to find\nan improved policy solution from that finite dataset. This\n17\nis known as offline reinforcement learning [275, 286–288].\nOffline reinforcement learning introduces several chal-\nlenges. In the model-free setting, we primarily need to rely\non off-policy reinforcement learning methods (see Section 4 –\nnot to be confused with offline) [289]) since off-policy meth-\nods can find an improved policy given arbitrary transition\ndata. As an alternative, we may also take a model-based ap-\nproach, where we first learn a model of the MDP, after which\nwe can apply any reinforcement learning algorithm through\nplanning in the model [278, 279]. The main challenge orig-\ninates from uncertainty due to limited data [290, 291]: we\nwant to prevent our solution from diverging too much from\nthe observed data region, since our predictions will then\nbecome uncertain and our obtained solution may become\n(very) suboptimal.\nIn the offline reinforcement learning settings, we find very\nlittle literature. In the building control domain, Blad et al.\n[139] have fully trained their reinforcement learning agent\nfrom transition data.\nOther works attempt to solve the\noffline challenge with a model-based approach.\nHere, the\nsystem dynamics (the transition function p and the reward\nfunction r) are trained in a supervised manner using real\ndata. This model can then be used as an environment upon\nwhich an reinforcement learning agent is trained [88, 183].\nThe small amount of literature indicates a potential research\ngap.\nEspecially in energy systems, companies may often\nhave been collecting large amounts of data on their current\n(non-RL) systems. If this data were to be accessible to the\nresearch community, we expect offline reinforcement learn-\ning to become an important cornerstone in the research field.\nIn the literature, we often observe problems where a sim-\nulator exists, but the state space is augmented by an of-\nfline time-series dataset. This is common for frequently used\nstate attributes, such as weather data or electricity prices,\nto create a more realistic simulation [58, 74, 238, 292–296].\nThese state attributes are then assumed to remain beyond\nthe influence of the agent.\nAlthough we do not consider\nthese problems offline reinforcement learning, where we only\ndeal with a finite dataset of transitions, we want to make\nthe reader aware of this practice. In particular because it is\nwidespread in the sustainable energy field (because weather\nand energy prices are common), and, similarly to offline re-\ninforcement learning, these problems may be limited in their\npossible generalization. For instance, given only data of the\npast year, it does not guarantee us to generalize to any po-\ntential energy price of the future. Furthermore, as in offline\nreinforcement learning, the problems require us to keep a\nheld-out test set to validate performance.\nIt is worth noting that there may be instances of termi-\nnological ambiguity. Some papers [64, 133] use to the term\n”offline reinforcement learning” to refer to the offline-state\nof an offline/online deployment process, rather than offline\nreinforcement learning in the context discussed in this para-\ngraph.\nSafe RL\nSafety is a key topic in all real-world applications of rein-\nforcement learning [297–299]. Many real-world systems are\nvulnerable: they need to operate within safety boundaries\nto avoid malfunction or collisions. When we learn on such\na system, we therefore first of all need to ensure that the\nsystem will not break or cause harm, i.e., stay within the\nprescribed bounds. Some applications may lend themselves\nto being layered by different control agents. Whenever the\nstate lands outside of predefined safe bounds, the controls\nmay be shifted one layer up to a safer, but perhaps less\nefficient, control agent.\nThis kind of setup would unfor-\ntunately not always be possible, and we prefer our most\nefficient agents to behave safely as well. As such, many re-\ninforcement learning methods have been designed to tackle\nthis problem [300–304]. Most of these methods have been\nstudied in the robotics community, since these systems are\ngenerally vulnerable [305].\nA common approach to add a first layer of safety involves\npre-training a learning algorithm on a virtual representation\nof the robot to narrow its action space prior to real-world\ndeployment [103, 237]. While this approach may accelerate\nlearning post-deployment and mitigate the risk of instabili-\nties, it does not entirely eliminate the possibility of system\nfailures.\nIn safe RL, however, the goal is to completely eliminate\nthe risk of such failures. This could be achieved by constrain-\ning the action space through hard constraints [65, 208, 306].\nHowever, such constraints may lead to suboptimal behav-\nior. Alternatively, the environment may be modified. Here,\nhard constraints can be imposed via a cost function [208],\nallowing the agent to optimize only among policies that fall\nbelow a certain maximum cost threshold [189].\nAnother approach to integrate safety concerns is to use\na model-based reinforcement learning approach based on a\ndynamics model [130]. Here, the model incorporates uncer-\ntainty estimates that are learned using a Guassian process,\nproviding the agent with not only an expected return but\nalso an uncertainty estimate during the planning phase. In a\nsimilar fashion, another work [167] proposed to enhance the\nrobustness of an agent in bad scenarios, or, in other words,\nreduce the uncertainty of estimates in the face of bad re-\nwards. This is done by modifying the replay buffer during\n18\ntraining to retain only transitions with poor rewards while\ndiscarding those with high rewards.\n7\nBenchmarks, baselines and per-\nformance metrics\nSo far, we have discussed various sustainable energy prob-\nlems that lend themselves to reinforcement learning solu-\ntions (Section 5), and discussed various reinforcement learn-\ning areas that are prevalent in the current literature (Section\n6). Successful progress in any field of research often bene-\nfits from standardization. In this section, we examine dif-\nferent benchmarks (environments), baselines (algorithms),\nand performance metrics used throughout the sustainable\nenergy landscape within the field of reinforcement learning.\nBenchmarks\nStandardized benchmarks are a cornerstone of the progress\nin machine learning, facilitating the comparison of different\nmethods on identical tasks and enabling a fair assessment of\napproaches. In reinforcement learning a significant advance-\nment in this regard was realized with the introduction of\nGym [143] and subsequently Gymnasium [307], which stan-\ndardized an API for reinforcement learning environments.\nThe field of sustainable energy encompasses a broad spec-\ntrum of challenges, as highlighted in Section 5. Despite (or\nbecause of) this diversity, numerous efforts have been made\nto create standardized environments. For ensuring wide us-\nage and compatibility, ideally, they should also meet specific\nneeds, including broad scope coverage, active maintenance,\nand integration with common frameworks like Gym/Gym-\nnasium. An overview of these environments is given in Ta-\nble 7, in which some of the main simulators are given that\nare ready to use in a Python-based reinforcement learning\npipeline.\nNotable in Table 7, the building control problem has the\nwidest variety of simulators available, possibly due to the\npopularity of the EnergyPlus building simulator [156]. Of\nfurther particular interest is SustainGym [308], which occurs\nin multiple settings. SustainGym is a library consisting of\nmultiple sustainable energy Gymnasium environments that\nare, at the time of writing, still actively maintained. We en-\ncourage researchers entering the field to investigate the use\nof SustainGym for their studies and to consider developing\nnew environments for its framework. Some of these bench-\nmarks originate in challenges hosted in the past, such as\nthe CityLearn challenge [159–161] or the L2RPN (Learning\nto Run a Power Network) challenge [309–311]. These chal-\nlenges serve as open benchmarks for continued submission\nbeyond the original deadlines of these challenges.\nIn the power transmission domain, the grid2op python\npackage [312] (which replaces the now-defunct pypownet\n[313] package) offers functionalities for simulating and\nbenchmarking power grid scenarios and forms the founda-\ntion of the L2RPN challenge. Other recent environments\ninclude gym-anm [314, 315], which addresses similar power\ngrid management tasks, but tailored for Active Network\nManagement in distribution networks; or PowerGridWorld\n[316] which is specifically tailored for multi-agent implemen-\ntations. All of these are compatible with Gym or Gymna-\nsium, and are openly accessible through GitHub.\nTable 3: Overview of environments/simulators in the realm\nof sustainable energy.\nThese simulators are open source\navailable on GitHub and often available via a Python API.\nSustainable Energy Area\nSimulator\nGeneration\nWind\n[317, 318]\nSolar\n[319]\nConsumption\nBuildings\n[146, 148, 151, 154,\n157, 158, 308, 320,\n320, 321]\nIndustry\n[308, 322, 323]\nElectrical vehicle\n[308, 324–326]\nGrids\nGrid management\n[312–314, 316]\nBaselines and Performance Metrics\nIn reinforcement learning, benchmarks define the environ-\nment and are used to measure the performance of an al-\ngorithm to solve a task.\nHowever, for a fair comparison,\nthe performance of other solutions for the same environ-\nment is required, which we refer to as the baselines. Base-\nlines often come in two different flavors: either as alternative\nnon-reinforcement learning-based algorithms or as state-of-\nthe-art reinforcement learning algorithms. In the realm of\nsustainable energy, the former baselines are usually chosen\nas classical optimization techniques, which are used in real-\nworld applications. In contrast, the latter are usually used\n19\nto show an improvement over previous solutions or to explic-\nitly compare different approaches on the same environment.\nAs the field of reinforcement learning in sustainable en-\nergy is relatively new, the focus of most presented research\nlies on developing proof-of-concept reinforcement learning\nsolutions for given problems and comparing them to classic\n(non-learning) control solutions. These include rule-based\ncontrollers, e.g.\nPID controllers [104, 131, 132, 142, 180,\n181, 327] or other heuristic controllers [183, 184, 193, 194].\nWe also see comparisons with more sophisticated methods\nsuch as learned decision trees [56], dynamic programming\n[56, 57, 61, 292], and model predictive control [84, 132, 167,\n169, 188, 190]. In some cases, an optimal solution is accessi-\nble during training. The authors may then compare against\nan optimal oracle baseline that assumes future knowledge\nto be known (operating in hindsight) [62, 80, 132, 172, 187].\nIn other instances, optimal solutions might be obtained\nthrough established methods such as linear [231, 328] or non-\nlinear programming [224], but are not necessarily capable of\ndealing with uncertainties.\nIn some of the research on sustainable energy, existing\nreinforcement learning algorithms are used as a baseline.\nIn these cases, often a specialized problem is addressed\nthat benefits from a well-engineered reinforcement learn-\ning method [132, 329]. Reinforcement learning algorithms\ngenerally require a high level of engineering and hyperpa-\nrameter tuning, requiring careful consideration when using\nalternative algorithms as a baseline. As such, it is generally\nadvised to use well-engineered standardized baselines such\nas Stable-Baselines [330], CleanRL [331] or RLlib [332].\nIn reinforcement learning, the value function, or the ex-\npected episodic return, is the predominant metric for eval-\nuating the performance of a learning algorithm, as rewards\nare designed to resemble some measure of optimality. How-\never, for numerous applications, a single metric alone does\nnot provide a complete picture. Multi-objective reinforce-\nment learning studies this problem [333]. Reliability emerges\nas a significant factor in many energy domains and is there-\nfore sometimes adopted as an additional performance metric\n[56, 292]. Moreover, classical control methods such as model\npredictive control suffer from considerably longer inference\ntimes compared to reinforcement learning, leading to the\nemergence of another sensible performance metric not cap-\ntured by the episodic return [76, 78]. Some papers adopt a\nmore sustainability-focused performance metric. This typ-\nically includes some form of emissions measure, e.g.\nthe\namount of CO2 emitted [235] or renewable utilization [236],\nif it can be calculated within the described system. To sum-\nmarize, multiple performance metrics can be taken into ac-\ncount in the sustainable energy domain. As such, it is im-\nportant that researchers in the field carefully consider what\nthey want to evaluate.\n8\nDiscussion and Future Work\nMatching supply with changes in demand is one of the ma-\njor challenges in sustainable energy. New elements have ar-\nrived in the energy chain, such as batteries, smart grids, and\nsmart appliances. This has led to a significant increase in\nthe need for control and optimization of – often intercon-\nnected – decision problems, a topic for which reinforcement\nlearning methods are very well suited.\nThe combination of reinforcement learning in sustainable\nenergy is still young, and the richness and fast growth of the\nlandscape has resulted in a scattered field in terms of envi-\nronments and benchmarks. A large amount of research in\nthe field uses undisclosed, problem-specific environments or\nits own hand-crafted environment, often not open-sourcing\nthe source code. This leads to redundant efforts and impedes\nthe reproducibility of results.\nAs such, we believe that all areas in the field would greatly\nbenefit from well-designed and general environments that re-\nceive long-term maintenance and can be continuously built\nup on. A promising software package in this regard is Sus-\ntainGym [308], which attempts to standardize a variety of\ndifferent sustainable energy environments. Such a standard-\nization would allow for easier use of standardized algorithm\nimplementations (such as Stable-Baselines and CleanRL).\nFurthermore, the entry barrier for reinforcement learning\nengineers would be reduced as they no longer need to focus\non building realistic and relevant environments.\nWe note a flourishing amount of research in the field, of-\nten aimed at an initial demonstration of the potential of\nreinforcement learning.\nOften, well-known algorithms are\nused, such as tabular methods or DQN. A deeper look into\nthe problem situation and the reinforcement learning liter-\nature may well be worthwhile to achieve better results. As\nthe field matures, more interdisciplinary teams will arise,\nknowledge of the energy and algorithms field will integrate,\nand we expect more breakthrough results to appear.\nSpecifically, we will discuss some important reinforcement\nlearning methods that may well be important for further\nprogress.\nFirstly, some papers have accurately identified\nthat the construction of simulators is not always feasible\n[132]; this may especially be true in the consumption do-\nmain, due to the diversity of consumption patterns. Model-\nbased reinforcement learning methods may aid sustainable\nenergy challenges as they first build a transition and re-\n20\nward model before the policy is optimized. So far, we see\nrelatively little use of model-based methods in the field, sug-\ngesting it is an underexplored approach.\nNext, when creating or learning a simulator is not feasible,\noffline reinforcement learning may be an option. Over time,\nlarge amounts of has likely been collected on various existing\noperating processes. This in turn opens the door for offline\nreinforcement learning, were this data to become available.\nOffline reinforcement learning is a field of research rich in\nliterature [275, 288] and the developed methods appear to\nnot be explored in the sustainable energy domain.\nGenerative deep learning [334] has received much atten-\ntion in the last decade. As stronger methods become avail-\nable, generative deep learning is likely to aid in modeling\nsystem dynamics. This would strengthen the fields of both\nmodel-based- and offline reinforcement learning. A promis-\ning avenue to investigate currently would be diffusion mod-\nels [335].\nSafe RL is another research area that is highly relevant\nfor sustainable energy. In order for applications to get past\nthe proof-of-concept phase, we often require incorporation\nof safety aspects into the system.\nThis is because a) we\ncannot allow disruptions due to exploratory action during\nthe training face in most energy systems, and b) even if we\nwere able to perform the training phase outside the live sys-\ntem, all approximate machine learning methods (including\ndeep reinforcement learning) provide some level of gener-\nalization.\nWhile this is in part a desired trait, this also\nresults in our systems always predicting some output, with-\nout us being able to verify whether that output is correct\neverywhere. Most current reinforcement learning research is\ntherefore studied in simulation or non-safety critical appli-\ncations, but the requirements for real-world deployment are\nmuch stronger (e.g. self-driving cars). A main challenge of\nRL and ML in general is deployment in real-world, safety-\ncritical scenarios, and the energy world will be a prominent\nexample of this.\nThe research literature reports few cases where reinforce-\nment learning methods are applied in practice. This might\nbe due to three reasons. First, the field is generally quite\nyoung and there might not have been enough time for (safe)\ndeployment.\nIn addition, the safety concerns themselves\nmight also be a reason machine learning methods have not\nfound their path to deployment yet.\nFinally, deployment\nby actors in various energy markets may simply not publish\ntheir findings in peer-reviewed research, making it harder to\naccurately assess the status of deployment.\nAnother promising area of research is graph-based rein-\nforcement learning. Graph neural networks [336] in general\nhave received much attention. Yet, in the surveyed litera-\nture, it has not been applied to a great extent in the energy\ndomain as part of a reinforcement learning agent. However,\ngraph structures appear naturally throughout the sustain-\nable energy landscape. Prime examples would be (electrical)\ngrids and chemical molecules. Those with interest in graph\nneural networks and reinforcement learning are strongly en-\ncouraged to apply their knowledge in the sustainable energy\ndomain.\nFurthermore, multi-objective reinforcement learning [333]\nmight also become relevant, since possible reward objectives,\nsuch as profit and emission reduction, can be conflicting.\nMulti-objective RL methods that can adaptively trade-off\nobjectives thereby become relevant. In addition, also multi-\nscale approaches hold promise.\nEspecially the electricity\ngrid has many subproblems that appear at different scales,\nand they all interact for overall grid efficiency.\nSome re-\nsearch does look at different scales of a problem simulta-\nneously [215, 294], and integrating solutions is a promising\navenue for future research.\nFinally, we point out that much research in the sustainable\nenergy landscape relies on weather and energy price data.\nFuture predictions of this data would be especially relevant\nfor a wide variety of use cases. While some works attempt\nto include such prediction mechanics into their models, we\nbelieve the sustainable energy field would benefit most from\naddressing its own problems and using external models, de-\nveloped in other fields, to address the challenges of time-\nseries prediction [337].\nHowever, systems that accurately\ntake history into account are a promising pathway for rein-\nforcement learning in general. Due to the amount of time-\nseries data in the energy domain, this is especially relevant\nfor sustainable energy research. As such, we believe that\nthe recent advances in state-space-models [268, 269] may\naid the performance of reinforcement learning agents in a\nwide variety of tasks discussed in this survey.\n9\nConclusion\nThis survey provides a comprehensive overview of the avail-\nable reinforcement learning approaches to sustainable en-\nergy challenges.\nWe first of all observe that the research\nfield has grown rapidly in recent years, and there are many\nsustainability challenges to which reinforcement learning is\napplicable. However, we also observe that most papers orig-\ninate from energy researchers that start to apply reinforce-\nment learning methodology, while reinforcement learning re-\nsearchers are less present – probably because they struggle\nto understand the relevant underlying problems and the way\n21\nto model them. Therefore, to mature the field, we likely need\nmore interaction between researchers from both communi-\nties. A key direction for integration would be the develop-\nment of better benchmarks, that is, standardized test envi-\nronments, on which we can test and compare reinforcement\nlearning approaches. Even then, real-world deployment will\nprobably also require the development of new core method-\nology, most notably in safe and offline RL. In short, this\nsurvey identifies a large potential for reinforcement learning\nto contribute to the sustainable energy transition.\nGiven\nthe urgency in solving these problems, we hope to see the\nfield mature and interdisciplinary work thrive.\nAcknowledgements\nThis work was supported by Shell Information Technol-\nogy International Limited and the Netherlands Enterprise\nAgency under the grant PPS23-3-03529461.\nReferences\n[1] J. Davenport and N. Wayth, “Statistical review of\nworld energy,” 2023.\n[2] Unfccc,\n“Policies\nand\nmeasures.”\nParis\nClimate\nChange Conference - November 2015,\nCOP 21,\nNovember 29 2018.\n[3] C. F. Kutscher and J. B. Milford, Principles of sus-\ntainable energy systems. CRC Press, 2018.\n[4] T. Yang, L. Zhao, W. Li, and A. Y. Zomaya, “Re-\ninforcement learning in sustainable energy and elec-\ntric systems: A survey,” Annual Reviews in Control,\nvol. 49, pp. 145–163, 2020.\n[5] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski,\nA. Lacoste, K. Sankaran, A. S. Ross, N. Milojevic-\nDupont, N. Jaques, A. Waldman-Brown, et al., “Tack-\nling climate change with machine learning,” ACM\nComputing Surveys (CSUR), vol. 55, no. 2, pp. 1–96,\n2022.\n[6] Z. Yao, Y. Lum, A. Johnston, L. M. Mejia-Mendoza,\nX. Zhou, Y. Wen, A. Aspuru-Guzik, E. H. Sargent,\nand Z. W. Seh, “Machine learning for a sustainable en-\nergy future,” Nature Reviews Materials, vol. 8, no. 3,\npp. 202–215, 2023.\n[7] P. L. Donti and J. Z. Kolter, “Machine learning for\nsustainable energy systems,” Annual Review of Envi-\nronment and Resources, vol. 46, pp. 719–747, 2021.\n[8] D. Rangel-Martinez, K. Nigam, and L. A. Ricardez-\nSandoval, “Machine learning on sustainable energy:\nA review and outlook on renewable energy systems,\ncatalysis, smart grid and energy storage,” Chemical\nEngineering Research and Design, vol. 174, pp. 414–\n441, 2021.\n[9] P. Ifaei, M. Nazari-Heris, A. S. T. Charmchi, S. Asadi,\nand C. Yoo, “Sustainable energies and machine learn-\ning: An organized review of recent applications and\nchallenges,” Energy, vol. 266, p. 126432, 2023.\n[10] T. Ahmad, R. Madonski, D. Zhang, C. Huang, and\nA. Mujeeb, “Data-driven probabilistic machine learn-\ning in sustainable smart energy/smart energy systems:\nKey developments, challenges, and future research op-\nportunities in the context of smart grid paradigm,”\nRenewable and Sustainable Energy Reviews, vol. 160,\np. 112128, 2022.\n[11] K. S. Perera, Z. Aung, and W. L. Woon, “Ma-\nchine learning techniques for supporting renewable en-\nergy generation and integration: a survey,” in Data\nAnalytics for Renewable Energy Integration:\nSec-\nond ECML PKDD Workshop, DARE 2014, Nancy,\nFrance, September 19, 2014, Revised Selected Papers\n2, pp. 81–96, Springer, 2014.\n[12] D. Cao, W. Hu, J. Zhao, G. Zhang, B. Zhang, Z. Liu,\nZ. Chen, and F. Blaabjerg, “Reinforcement learning\nand its applications in modern power and energy sys-\ntems: A review,” Journal of modern power systems\nand clean energy, vol. 8, no. 6, pp. 1029–1042, 2020.\n[13] J. R. V´azquez-Canteli and Z. Nagy, “Reinforcement\nlearning for demand response: A review of algorithms\nand modeling techniques,” Applied Energy, vol. 235,\npp. 1072–1089, Feb. 2019.\n[14] J. J. M. Escobar, O. M. Matamoros, R. T. Padilla,\nI. L. Reyes, and H. Q. Espinosa, “A comprehensive\nreview on smart grids: Challenges and opportunities,”\n10 2021.\n[15] Y. Li, C. Yu, M. Shahidehpour, T. Yang, Z. Zeng, and\nT. Chai, “Deep reinforcement learning for smart grid\noperations: Algorithms, applications, and prospects,”\nProceedings of the IEEE, vol. 111, pp. 1055–1096, 9\n2023.\n22\n[16] M. Hasan, Z. Mifta, N. A. Salsabil, S. J. Papiya,\nM. Hossain, P. Roy, N. U. R. Chowdhury, and O. Far-\nrok, “A critical review on control mechanisms, sup-\nporting measures, and monitoring systems of micro-\ngrids considering large scale integration of renewable\nenergy sources,” 11 2023.\n[17] G. Dileep, “A survey on smart grid technologies and\napplications,” Renewable Energy, vol. 146, pp. 2589–\n2625, 2 2020.\n[18] I. U. Salam, M. Yousif, M. Numan, and M. Billah,\n“Addressing the challenge of climate change: The role\nof microgrids in fostering a sustainable future - a com-\nprehensive review,” 3 2024.\n[19] M. A. Jirdehi, V. S. Tabar, S. Ghassemzadeh, and\nS. Tohidi, “Different aspects of microgrid manage-\nment: A comprehensive review,” Journal of Energy\nStorage, vol. 30, 8 2020.\n[20] M. Sheha, K. Mohammadi, and K. Powell, “Solv-\ning the duck curve in a smart grid environment us-\ning a non-cooperative game theory and dynamic pric-\ning profiles,” Energy Conversion and Management,\nvol. 220, p. 113102, 2020.\n[21] M. L. Littman, “Markov games as a framework\nfor multi-agent reinforcement learning,” in Machine\nlearning proceedings 1994, pp. 157–163, Elsevier, 1994.\n[22] R. S. Sutton and A. G. Barto, Reinforcement Learn-\ning:\nAn Introduction.\nAdaptive Computation and\nMachine Learning Series, Cambridge, Massachusetts:\nThe MIT Press, second edition ed., 2018.\n[23] R. Bellman, “A Markovian Decision Process,” Journal\nof Mathematics and Mechanics, vol. 6, no. 5, pp. 679–\n684, 1957.\n[24] V. Konda and J. Tsitsiklis, “Actor-Critic Algorithms,”\nin Advances in Neural Information Processing Sys-\ntems, vol. 12, MIT Press, 1999.\n[25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford,\nand O. Klimov, “Proximal Policy Optimization Al-\ngorithms,” Aug. 2017.\n[26] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing\nFunction Approximation Error in Actor-Critic Meth-\nods,” Oct. 2018.\n[27] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess,\nT. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Contin-\nuous control with deep reinforcement learning,” July\n2019.\n[28] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and\nP. Abbeel, “Trust Region Policy Optimization,” Apr.\n2017.\n[29] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft\nActor-Critic: Off-Policy Maximum Entropy Deep Re-\ninforcement Learning with a Stochastic Actor,” Aug.\n2018.\n[30] M. Hessel, I. Danihelka, F. Viola, A. Guez, S. Schmitt,\nL. Sifre, T. Weber, D. Silver, and H. van Hasselt,\n“Muesli:\nCombining Improvements in Policy Opti-\nmization,” Mar. 2022.\n[31] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Ma-\nchine Learning, vol. 8, pp. 279–292, May 1992.\n[32] G. Rummery and M. Niranjan, “On-Line Q-Learning\nUsing\nConnectionist\nSystems,”\nTechnical\nReport\nCUED/F-INFENG/TR 166, Nov. 1994.\n[33] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,\nI. Antonoglou, D. Wierstra, and M. Riedmiller, “Play-\ning Atari with Deep Reinforcement Learning,” Dec.\n2013.\n[34] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul,\nG. Ostrovski,\nW. Dabney,\nD. Horgan,\nB. Piot,\nM. Azar, and D. Silver, “Rainbow: Combining Im-\nprovements in Deep Reinforcement Learning,” Oct.\n2017.\n[35] R. S. Sutton, D. McAllester, S. Singh, and Y. Man-\nsour, “Policy Gradient Methods for Reinforcement\nLearning with Function Approximation,” in Advances\nin Neural Information Processing Systems, vol. 12,\nMIT Press, 1999.\n[36] D. Silver, A. Huang, C. J. Maddison, A. Guez,\nL. Sifre, G. van den Driessche, J. Schrittwieser,\nI. Antonoglou,\nV. Panneershelvam,\nM. Lanctot,\nS. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,\nI. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,\nT. Graepel, and D. Hassabis, “Mastering the game of\nGo with deep neural networks and tree search,” Na-\nture, vol. 529, pp. 484–489, Jan. 2016.\n23\n[37] D.\nSilver,\nJ.\nSchrittwieser,\nK.\nSimonyan,\nI. Antonoglou,\nA. Huang,\nA. Guez,\nT. Hubert,\nL. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap,\nF. Hui, L. Sifre, G. van den Driessche, T. Graepel,\nand D. Hassabis, “Mastering the game of Go without\nhuman knowledge,” Nature, vol. 550, pp. 354–359,\nOct. 2017.\n[38] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. M¨uller,\nV. Koltun, and D. Scaramuzza, “Champion-level\ndrone racing using deep reinforcement learning,” Na-\nture, vol. 620, pp. 982–987, Aug. 2023.\n[39] P. Christiano, J. Leike, T. B. Brown, M. Martic,\nS. Legg, and D. Amodei, “Deep reinforcement learning\nfrom human preferences,” Feb. 2023.\n[40] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Math-\nieu, A. Dudzik, J. Chung, D. H. Choi, R. Pow-\nell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan,\nM. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai,\nJ. P. Agapiou, M. Jaderberg, A. S. Vezhnevets,\nR. Leblond, T. Pohlen, V. Dalibard, D. Budden,\nY. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre,\nZ. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama,\nD. W¨unsch, K. McKinney, O. Smith, T. Schaul, T. Lil-\nlicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, and\nD. Silver, “Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning,” Nature, vol. 575,\npp. 350–354, Nov. 2019.\n[41] OpenAI, C. Berner, G. Brockman, B. Chan, V. Che-\nung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer,\nS. Hashme, C. Hesse, R. Jozefowicz, S. Gray, C. Ols-\nson, J. Pachocki, M. Petrov, H. P. d. O. Pinto,\nJ. Raiman, T. Salimans, J. Schlatter, J. Schnei-\nder, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and\nS. Zhang, “Dota 2 with Large Scale Deep Reinforce-\nment Learning,” Dec. 2019.\n[42] S. Morad, R. Kortvelesy, M. Bettini, S. Liwicki, and\nA. Prorok, “POPGym: Benchmarking Partially Ob-\nservable Reinforcement Learning,” Mar. 2023.\n[43] M. Pleines, M. Pallasch, F. Zimmer, and M. Preuss,\n“Memory Gym: Partially Observable Challenges to\nMemory-Based Agents,” in The Eleventh Interna-\ntional Conference on Learning Representations, Sept.\n2022.\n[44] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Offline\nReinforcement Learning: Tutorial, Review, and Per-\nspectives on Open Problems,” Nov. 2020.\n[45] R. F. Prudencio, M. R. O. A. Maximo, and E. L.\nColombini,\n“A\nSurvey\non\nOffline\nReinforcement\nLearning: Taxonomy, Review, and Open Problems,”\nIEEE Transactions on Neural Networks and Learning\nSystems, pp. 1–0, 2024.\n[46] T. M. Moerland, J. Broekens, A. Plaat, and C. M.\nJonker, “Model-based Reinforcement Learning:\nA\nSurvey,”\nFoundations\nand\nTrends®\nin\nMachine\nLearning, vol. 16, pp. 1–118, Jan. 2023.\n[47] A. Plaat, W. Kosters, and M. Preuss, “High-accuracy\nmodel-based reinforcement learning, a survey,” Artifi-\ncial Intelligence Review, vol. 56, no. 9, pp. 9541–9573,\n2023.\n[48] A. Wong, T. B¨ack, A. V. Kononova, and A. Plaat,\n“Deep multiagent reinforcement learning: Challenges\nand directions,” Artificial Intelligence Review, vol. 56,\npp. 5023–5056, June 2023.\n[49] P. Ladosz, L. Weng, M. Kim, and H. Oh, “Explo-\nration in Deep Reinforcement Learning: A Survey,”\nInformation Fusion, vol. 85, pp. 1–22, Sept. 2022.\n[50] S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang,\nY. Yang, and A. Knoll, “A Review of Safe Reinforce-\nment Learning: Methods, Theory and Applications,”\nFeb. 2023.\n[51] L. P. Kaelbling, M. L. Littman, and A. W. Moore,\n“Reinforcement learning: A survey,” Journal of arti-\nficial intelligence research, vol. 4, pp. 237–285, 1996.\n[52] V. Fran¸cois-Lavet, P. Henderson, R. Islam, M. G.\nBellemare, and J. Pineau, “An Introduction to Deep\nReinforcement Learning,” Foundations and Trends®\nin Machine Learning, vol. 11, pp. 219–354, Dec. 2018.\n[53] A. Plaat, Deep reinforcement learning. Springer, 2022.\n[54] I. H. Association, “2022 Hydropower Status Report,”\n2022.\n[55] A. Kleiven, S. Nadarajah, and S.-E. Fleten, “Revis-\niting hierarchical planning for hydropower plant up-\ngrades using semi-analytical policies and reinforce-\nment learning,” Decision analytics in hydropower: In-\nvestment and operational planning under uncertainty,\nPHD-Thesis, 2022.\n[56] W. Xu, F. Meng, W. Guo, X. Li, and G. Fu, “Deep Re-\ninforcement Learning for Optimal Hydropower Reser-\nvoir Operation,” Journal of Water Resources Planning\nand Management, vol. 147, p. 04021045, Aug. 2021.\n24\n[57] F. Mitjana, M. Denault, and K. Demeester, “Man-\naging chance-constrained hydropower with reinforce-\nment learning and backoffs,” Advances in Water Re-\nsources, vol. 169, p. 104308, 2022.\n[58] S. Riemer-Sorensen and G. H. Rosenlund, “Deep Re-\ninforcement Learning for Long Term Hydropower Pro-\nduction Scheduling,” in 2020 International Conference\non Smart Energy Systems and Technologies (SEST),\n(Istanbul, Turkey), pp. 1–6, Ieee, Sept. 2020.\n[59] H. Liu, C. Yu, H. Wu, Z. Duan, and G. Yan, “A new\nhybrid ensemble deep reinforcement learning model for\nwind speed short term forecasting,” Energy, vol. 202,\np. 117794, July 2020.\n[60] S. M. J. Jalali, G. J. Osorio, S. Ahmadian, M. Lotfi,\nV. M. A. Campos, M. Shafie-khah, A. Khosravi, and\nJ. P. S. Catalao, “New Hybrid Deep Neural Architec-\ntural Search-Based Ensemble Reinforcement Learning\nStrategy for Wind Power Forecasting,” IEEE Trans-\nactions on Industry Applications, vol. 58, pp. 15–27,\nJan. 2022.\n[61] Z. Yang, X. Ma, L. Xia, Q. Zhao, and X. Guan, “Re-\ninforcement learning for fluctuation reduction of wind\npower with energy storage,” Results in Control and\nOptimization, vol. 4, p. 100030, Sept. 2021.\n[62] C. Wei, Z. Zhang, W. Qiao, and L. Qu, “An Adap-\ntive Network-Based Reinforcement Learning Method\nfor MPPT Control of PMSG Wind Energy Conversion\nSystems,” IEEE Transactions on Power Electronics,\nvol. 31, pp. 7837–7848, Nov. 2016.\n[63] B. Fernandez-Gauna, M. Gra˜na, J.-L. Osa-Amilibia,\nand X. Larrucea, “Actor-critic continuous state rein-\nforcement learning for wind-turbine control robust op-\ntimization,” Information Sciences, vol. 591, pp. 365–\n380, Apr. 2022.\n[64] L. Jia, J. Hao, J. Hall, H. K. Nejadkhaki, G. Wang,\nY. Yan, and M. Sun, “A reinforcement learning based\nblade twist angle distribution searching method for op-\ntimizing wind turbine energy power,” Energy, vol. 215,\np. 119148, Jan. 2021.\n[65] V.-H. Bui, T.-T. Nguyen, and H.-M. Kim, “Dis-\ntributed Operation of Wind Farm for Maximizing\nOutput Power: A Multi-Agent Deep Reinforcement\nLearning Approach,” IEEE Access, vol. 8, pp. 173136–\n173146, 2020.\n[66] M. Ding, D. Lv, C. Yang, S. Li, Q. Fang, B. Yang, and\nX. Zhang, “Global Maximum Power Point Tracking\nof PV Systems under Partial Shading Condition: A\nTransfer Reinforcement Learning Approach,” Applied\nSciences, vol. 9, p. 2769, July 2019.\n[67] D. Lin, X. Li, S. Ding, H. Wen, Y. Du, and W. Xiao,\n“Self-Tuning MPPT Scheme Based on Reinforcement\nLearning and Beta Parameter in Photovoltaic Power\nSystems,” IEEE Transactions on Power Electronics,\nvol. 36, pp. 13826–13838, Dec. 2021.\n[68] Chou, Yang, and Chen, “Maximum Power Point\nTracking of Photovoltaic System Based on Reinforce-\nment Learning,” Sensors, vol. 19, p. 5054, Nov. 2019.\n[69] B. C. Phan, Y.-C. Lai, and C. E. Lin, “A Deep Re-\ninforcement Learning-Based MPPT Control for PV\nSystems under Partial Shading Condition,” Sensors,\nvol. 20, p. 3039, May 2020.\n[70] X. Zhang, S. Li, T. He, B. Yang, T. Yu, H. Li, L. Jiang,\nand L. Sun, “Memetic reinforcement learning based\nmaximum power point tracking design for PV systems\nunder partial shading condition,” Energy, vol. 174,\npp. 1079–1090, May 2019.\n[71] P. Kofinas, S. Doltsinis, A. Dounis, and G. Vouros,\n“A reinforcement learning approach for MPPT control\nmethod of photovoltaic sources,” Renewable Energy,\nvol. 108, pp. 461–473, Aug. 2017.\n[72] Y. Singh and N. Pal, “Reinforcement learning with\nfuzzified reward approach for MPPT control of PV\nsystems,” Sustainable Energy Technologies and As-\nsessments, vol. 48, p. 101665, Dec. 2021.\n[73] A. Yadav and A. R. Chowdhury, “Deep Q Reinforce-\nment Learning To Improve The Mppt In Solar Cell,”\ndvances and Applications in Mathematical Sciences,\nvol. 21, no. 5, 2022.\n[74] S. Shresthamali, M. Kondo, and H. Nakamura, “Adap-\ntive Power Management in Solar Energy Harvest-\ning Sensor Node Using Reinforcement Learning,”\nACM Transactions on Embedded Computing Systems,\nvol. 16, pp. 1–21, Oct. 2017.\n[75] Y. Ge, Y. Nan, and X. Guo, “Maximizing network\nthroughput by cooperative reinforcement learning in\nclustered solar-powered wireless sensor networks,” In-\nternational Journal of Distributed Sensor Networks,\nvol. 17, p. 155014772110074, Apr. 2021.\n25\n[76] Z. Zeng, D. Ni, and G. Xiao, “Real-time heliostat field\naiming strategy optimization based on reinforcement\nlearning,” Applied Energy, vol. 307, p. 118224, Feb.\n2022.\n[77] C. Correa-Jullian, E. L´opez Droguett, and J. M.\nCardemil, “Operation scheduling in a solar thermal\nsystem: A reinforcement learning-based framework,”\nApplied Energy, vol. 268, p. 114943, June 2020.\n[78] J. Zhang, Y. Liu, Y. Li, K. Ding, L. Feng, X. Chen,\nX. Chen, and J. Wu, “A reinforcement learning based\napproach for on-line adaptive parameter extraction of\nphotovoltaic array models,” Energy Conversion and\nManagement, vol. 214, p. 112875, June 2020.\n[79] H. Fang, M. Zhang, S. He, X. Luan, F. Liu, and\nZ. Ding, “Solving the Zero-Sum Control Problem\nfor Tidal Turbine System: An Online Reinforcement\nLearning Approach,” IEEE Transactions on Cybernet-\nics, vol. 53, pp. 7635–7647, Dec. 2023.\n[80] E. Anderlini, D. I. M. Forehand, P. Stansell, Q. Xiao,\nand M. Abusara, “Control of a Point Absorber Us-\ning Reinforcement Learning,” IEEE Transactions on\nSustainable Energy, vol. 7, pp. 1681–1690, Oct. 2016.\n[81] T. M. Moreira, J. G. De Faria, P. O. Vaz-de-Melo,\nL. Chaimowicz, and G. Medeiros-Ribeiro, “Prediction-\nfree, real-time flexible control of tidal lagoons through\nProximal Policy Optimisation:\nA case study for\nthe Swansea Lagoon,” Ocean Engineering, vol. 247,\np. 110657, Mar. 2022.\n[82] A. F. d. O. Falcao, “Wave energy utilization: A review\nof the technologies,” Renewable and sustainable energy\nreviews, vol. 14, no. 3, pp. 899–918, 2010.\n[83] L. Bruzzone, P. Fanghella, and G. Berselli, “Rein-\nforcement Learning control of an onshore oscillating\narm Wave Energy Converter,” Ocean Engineering,\nvol. 206, p. 107346, June 2020.\n[84] E. Anderlini, S. Husain, G. G. Parker, M. Abusara,\nand G. Thomas, “Towards Real-Time Reinforcement\nLearning Control of a Wave Energy Converter,” Jour-\nnal of Marine Science and Engineering, vol. 8, p. 845,\nNov. 2020.\n[85] S. Sarkar, V. Gundecha, A. Shmakov, S. Ghor-\nbanpour, A. R. Babu, P. Faraboschi, M. Cocho,\nA. Pichard, and J. Fievez, “Multi-Agent Reinforce-\nment Learning Controller to Maximize Energy Effi-\nciency for Multi-Generator Industrial Wave Energy\nConverter,” Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 36, pp. 12135–12144, June\n2022.\n[86] S. S. Siwal, Q. Zhang, N. Devi, A. K. Saini, V. Saini,\nB. Pareek, S. Gaidukovs, and V. K. Thakur, “Re-\ncovery processes of sustainable energy using different\nbiomass and wastes,” Renewable and Sustainable En-\nergy Reviews, vol. 150, p. 111483, Oct. 2021.\n[87] A.\nBosmans,\nI.\nVanderreydt,\nD.\nGeysen,\nand\nL. Helsen, “The crucial role of Waste-to-Energy tech-\nnologies in enhanced landfill mining: A technology re-\nview,” Journal of Cleaner Production, vol. 55, pp. 10–\n23, Sept. 2013.\n[88] I. K. Faridi, E. Tsotsas, and A. Kharaghani, “Advanc-\ning Process Control in Fluidized Bed Biomass Gasifi-\ncation Using Model-Based Deep Reinforcement Learn-\ning,” Processes, vol. 12, p. 254, Feb. 2024.\n[89] J. Lim, H. Cho, H. Kwon, H. Park, and J. Kim, “Re-\ninforcement learning-based optimal operation of ash\ndeposit removal system to improve recycling efficiency\nof biomass for co2 reduction,” Journal of Cleaner Pro-\nduction, vol. 370, p. 133605, 2022.\n[90] A. J. Ellis, “Geothermal Systems and Power Develop-\nment: Natural hot water and steam fields are a valu-\nable source of useful energy in several countries, and in\nmany others they are being developed rapidly,” Amer-\nican Scientist, vol. 63, no. 5, pp. 510–521, 1975.\n[91] G. Buster, P. Siratovich, N. Taverna, M. Rossol,\nJ. Weers, A. Blair, J. Huggins, C. Siega, W. Man-\nnington, A. Urgel, J. Cen, J. Quinao, R. Watt, and\nJ. Akerley, “A New Modeling Framework for Geother-\nmal Operational Optimization with Machine Learning\n(GOOML),” Energies, vol. 14, p. 6852, Jan. 2021.\n[92] D. Duplyakin, K. F. Beckers, D. L. Siler, M. J. Mar-\ntin, and H. E. Johnston, “Modeling Subsurface Per-\nformance of a Geothermal Reservoir Using Machine\nLearning,” Energies, vol. 15, p. 967, Jan. 2022.\n[93] P. Siratovich,\nG. Buster,\nN. Taverna,\nA. Blair,\nJ. Weers, M. Rossol, and J. Huggins, “GOOML - Find-\ning Optimization Opportunities for Geothermal Op-\nerations: Preprint,” Tech. Rep. Nrel/cp-6a20-80093,\n26\nNational Renewable Energy Lab. (NREL), Golden,\nCO (United States), Dec. 2022.\n[94] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey,\nF. Carpanese, T. Ewalds, R. Hafner, A. Abdolmaleki,\nD. de las Casas, C. Donner, L. Fritz, C. Galperti,\nA. Huber, J. Keeling, M. Tsimpoukelli, J. Kay,\nA. Merle, J.-M. Moret, S. Noury, F. Pesamosca,\nD. Pfau, O. Sauter, C. Sommariva, S. Coda, B. Du-\nval, A. Fasoli, P. Kohli, K. Kavukcuoglu, D. Hass-\nabis, and M. Riedmiller, “Magnetic control of toka-\nmak plasmas through deep reinforcement learning,”\nNature, vol. 602, pp. 414–419, 2022.\n[95] J. Seo, S. Kim, A. Jalalvand, R. Conlin, A. Roth-\nstein, J. Abbate, K. Erickson, J. Wai, R. Shousha,\nand E. Kolemen, “Avoiding fusion plasma tearing in-\nstability with deep reinforcement learning,” Nature,\nvol. 626, pp. 746–751, 2024.\n[96] J. Seo, Y.-S. Na, B. Kim, C. Y. Lee, M. S. Park, S. J.\nPark, and Y. H. Lee, “Feedforward beta control in\nthe kstar tokamak by deep reinforcement learning,”\nNuclear Fusion, vol. 61, p. 106010, 9 2021.\n[97] T. Wakatsuki, T. Suzuki, N. Oyama, and N. Hayashi,\n“Ion temperature gradient control using reinforcement\nlearning technique,” Nuclear Fusion, vol. 61, p. 46036,\n3 2021.\n[98] T. Wakatsuki, M. Yoshida, E. Narita, T. Suzuki, and\nN. Hayashi, “Simultaneous control of safety factor pro-\nfile and normalized beta for jt-60sa using reinforce-\nment learning,” Nuclear Fusion, vol. 63, p. 76017, 5\n2023.\n[99] B. D. Tracey, A. Michi, Y. Chervonyi, I. Davies,\nC. Paduraru, N. Lazic, F. Felici, T. Ewalds, C. Don-\nner, C. Galperti, J. Buchli, M. Neunert, A. Hu-\nber, J. Evens, P. Kurylowicz, D. J. Mankowitz,\nand M. Riedmiller, “Towards practical reinforcement\nlearning for tokamak magnetic control,” Fusion Engi-\nneering and Design, vol. 200, p. 114161, 2024.\n[100] I. Char, J. Abbate, L. Bard´oczi, M. Boyer, Y. Chung,\nR. Conlin,\nK. Erickson,\nV. Mehta,\nN. Richner,\nE. Kolemen, et al., “Offline model-based reinforcement\nlearning for tokamak control,” in Learning for Dy-\nnamics and Control Conference, pp. 1357–1372, Pmlr,\n2023.\n[101] P. Toufani, E. Nadar, and A. S. Kocaman, “Short-\nterm assessment of pumped hydro energy storage con-\nfigurations: Up, down, or closed?,” Renewable Energy,\nvol. 201, pp. 1086–1095, Dec. 2022.\n[102] P. Toufani, E. Nadar, and A. S. Kocaman, “Oper-\national benefit of transforming cascade hydropower\nstations into pumped hydro energy storage systems,”\nJournal of Energy Storage, vol. 51, p. 104444, July\n2022.\n[103] C. Tubeuf, F. Birkelbach, A. Maly, and R. Hofmann,\n“Increasing the Flexibility of Hydropower with Rein-\nforcement Learning on a Digital Twin Platform,” En-\nergies, vol. 16, p. 1796, Feb. 2023.\n[104] I. Enyekwe, W. Bai, K. Y. Lee, and S. Nag, “Speed\nControl of an Adjustable Speed Pumped Storage Hy-\ndropower Plant with a Deep Reinforcement Learning-\nBased Governor,” Journal of Machine Intelligence and\nData Science, vol. 4, 2023.\n[105] R. Subramanya, S. A. Sierla, and V. Vyatkin, “Ex-\nploiting Battery Storages With Reinforcement Learn-\ning: A Review for Energy Professionals,” IEEE Ac-\ncess, vol. 10, pp. 54484–54506, 2022.\n[106] E. S´anchez-D´ıez, E. Ventosa, M. Guarnieri, A. Trov`o,\nC. Flox, R. Marcilla, F. Soavi, P. Mazur, E. Aranzabe,\nand R. Ferret, “Redox flow batteries: Status and per-\nspective towards sustainable stationary energy stor-\nage,” Journal of Power Sources, vol. 481, p. 228804,\n2021.\n[107] S. S. S. V., J. N. Law, C. E. Tripp, D. Duplyakin,\nE. Skordilis, D. Biagioni, R. S. Paton, and P. C.\nSt. John, “Multi-objective goal-directed optimization\nof de novo stable organic radicals for aqueous redox\nflow batteries,” Nature Machine Intelligence, vol. 4,\npp. 720–730, Aug. 2022.\n[108] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou,\nM. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran,\nT. Graepel, T. Lillicrap, K. Simonyan, and D. Has-\nsabis, “Mastering Chess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm,” Dec.\n2017.\n[109] M. Ben Ahmed and W. Fekih Hassen, “Optimization\nof a Redox-Flow Battery Simulation Model Based on\na Deep Reinforcement Learning Approach,” Batteries,\nvol. 10, p. 8, Jan. 2024.\n27\n[110] S. N. Steinmann, A. Hermawan, M. Bin Jassar, and\nZ. W. Seh, “Autonomous high-throughput computa-\ntions in catalysis,” Chem Catalysis, vol. 2, pp. 940–\n956, May 2022.\n[111] J. Yoon, Z. Cao, R. K. Raju, Y. Wang, R. Burn-\nley, A. J. Gellman, A. Barati Farimani, and Z. W.\nUlissi, “Deep reinforcement learning for predicting ki-\nnetic pathways to surface reconstruction in a ternary\nalloy,” Machine Learning:\nScience and Technology,\nvol. 2, p. 045018, Dec. 2021.\n[112] A. Abiola, F. S. Manzano, and J. M. And´ujar, “A\nNovel Deep Reinforcement Learning (DRL) Algorithm\nto Apply Artificial Intelligence-Based Maintenance in\nElectrolysers,” Algorithms, vol. 16, p. 541, Nov. 2023.\n[113] P. Purnami, W. Satrio Nugroho, N. Hamidi, W. W,\nA. A. Schulze, and I. Wardana, “Double deep Q net-\nwork intelligent adaptive control for highly efficient\ndynamic magnetic field assisted water electrolysis,”\nInternational Journal of Hydrogen Energy, vol. 59,\npp. 457–464, Mar. 2024.\n[114] J. Li, T. Yu, and B. Yang, “A data-driven output\nvoltage control of solid oxide fuel cell using multi-\nagent deep reinforcement learning,” Applied Energy,\nvol. 304, p. 117541, Dec. 2021.\n[115] J. Li, T. Yu, and B. Yang, “Coordinated control of\ngas supply system in PEMFC based on multi-agent\ndeep reinforcement learning,” International Journal of\nHydrogen Energy, vol. 46, pp. 33899–33914, Oct. 2021.\n[116] T. Sang, K.-i. Nomura, A. Nakano, R. K. Kalia, and\nP. Vashishta, “Hydrogen diffusion through polymer\nusing deep reinforcement learning,” in accepted, Ma-\nchine Learning and the Physical Sciences Workshop,\nNeurIPS, 2023.\n[117] H. Tang, B. Li, Y. Song, M. Liu, H. Xu, G. Wang,\nH. Chung, and J. Li, “Reinforcement Learning-Guided\nLong-Timescale Simulation of Hydrogen Transport in\nMetals,” Advanced Science, vol. 11, no. 5, p. 2304122,\n2024.\n[118] F. Birol, “World Energy Outlook 2022,” tech. rep.,\nInternational Energy Agency, Nov. 2022.\n[119] X. Liu, M. Ren, Z. Yang, G. Yan, Y. Guo, L. Cheng,\nand C. Wu, “A multi-step predictive deep reinforce-\nment learning algorithm for HVAC control systems in\nsmart buildings,” Energy, vol. 259, p. 124857, Nov.\n2022.\n[120] L. Yu, Z. Xu, T. Zhang, X. Guan, and D. Yue,\n“Energy-efficient personalized thermal comfort control\nin office buildings based on multi-agent deep reinforce-\nment learning,” Building and Environment, vol. 223,\np. 109458, Sept. 2022.\n[121] X. Fang, G. Gong, G. Li, L. Chun, P. Peng, W. Li,\nX. Shi, and X. Chen, “Deep reinforcement learning\noptimal control strategy for temperature setpoint real-\ntime reset in multi-zone building HVAC system,” Ap-\nplied Thermal Engineering, vol. 212, p. 118552, July\n2022.\n[122] M. Esrafilian-Najafabadi and F. Haghighat, “Towards\nself-learning control of HVAC systems with the consid-\neration of dynamic occupancy patterns: Application\nof model-free deep reinforcement learning,” Building\nand Environment, vol. 226, p. 109747, Dec. 2022.\n[123] X. Deng, Y. Zhang, Y. Zhang, and H. Qi, “Towards\noptimal HVAC control in non-stationary building en-\nvironments combining active change detection and\ndeep reinforcement learning,” Building and Environ-\nment, vol. 211, p. 108680, Mar. 2022.\n[124] R. Jia, M. Jin, K. Sun, T. Hong, and C. Spanos,\n“Advanced Building Control via Deep Reinforcement\nLearning,” Energy Procedia, vol. 158, pp. 6158–6163,\nFeb. 2019.\n[125] R. Lu, S. H. Hong, and M. Yu, “Demand Response\nfor Home Energy Management Using Reinforcement\nLearning and Artificial Neural Network,” IEEE Trans-\nactions on Smart Grid, vol. 10, pp. 6629–6639, Nov.\n2019.\n[126] Z. Zhang, A. Chong, Y. Pan, C. Zhang, and K. P.\nLam, “Whole building energy model for HVAC opti-\nmal control: A practical framework based on deep re-\ninforcement learning,” Energy and Buildings, vol. 199,\npp. 472–490, Sept. 2019.\n[127] D. Azuatalam, W.-L. Lee, F. de Nijs, and A. Liebman,\n“Reinforcement learning for whole-building HVAC\ncontrol and demand response,” Energy and AI, vol. 2,\np. 100020, Nov. 2020.\n[128] D. Coraci, S. Brandi, M. S. Piscitelli, and A. Capoz-\nzoli, “Online Implementation of a Soft Actor-Critic\nAgent to Enhance Indoor Temperature Control and\nEnergy Efficiency in Buildings,” Energies, vol. 14,\np. 997, Jan. 2021.\n28\n[129] Y. Du, H. Zandi, O. Kotevska, K. Kurte, J. Munk,\nK. Amasyali, E. Mckee, and F. Li, “Intelligent multi-\nzone residential HVAC control strategy based on deep\nreinforcement learning,” Applied Energy, vol. 281,\np. 116117, Jan. 2021.\n[130] Z. An, X. Ding, A. Rathee, and W. Du, “CLUE:\nSafe Model-Based RL HVAC Control Using Epis-\ntemic Uncertainty Estimation,” in Proceedings of the\n10th ACM International Conference on Systems for\nEnergy-Efficient Buildings, Cities, and Transporta-\ntion, (Istanbul Turkey), pp. 149–158, Acm, Nov. 2023.\n[131] C. Gao and D. Wang, “Comparative study of model-\nbased and model-free reinforcement learning control\nperformance in HVAC systems,” Journal of Building\nEngineering, vol. 74, p. 106852, Sept. 2023.\n[132] S. Jeen, A. Abate, and J. M. Cullen, “Low Emis-\nsion Building Control with Zero-Shot Reinforcement\nLearning,” Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 37, pp. 14259–14267, June\n2023.\n[133] D. Zhuang, V. J. L. Gan, Z. Duygu Tekler, A. Chong,\nS. Tian, and X. Shi, “Data-driven predictive control\nfor smart HVAC system in IoT-integrated buildings\nwith time-series forecasting and reinforcement learn-\ning,” Applied Energy, vol. 338, p. 120936, May 2023.\n[134] Q. Fu, X. Chen, S. Ma, N. Fang, B. Xing, and J. Chen,\n“Optimal control method of HVAC based on multi-\nagent deep reinforcement learning,” Energy and Build-\nings, vol. 270, p. 112284, Sept. 2022.\n[135] K. Dalamagkidis, D. Kolokotsa, K. Kalaitzakis, and\nG. S. Stavrakakis, “Reinforcement learning for energy\nconservation and comfort in buildings,” Building and\nEnvironment, vol. 42, pp. 2686–2698, July 2007.\n[136] W. Valladares, M. Galindo, J. Guti´errez, W.-C. Wu,\nK.-K. Liao, J.-C. Liao, K.-C. Lu, and C.-C. Wang,\n“Energy optimization associated with thermal comfort\nand indoor air control via a deep reinforcement learn-\ning algorithm,” Building and Environment, vol. 155,\npp. 105–117, May 2019.\n[137] L. Yu, Y. Sun, Z. Xu, C. Shen, D. Yue, T. Jiang, and\nX. Guan, “Multi-Agent Deep Reinforcement Learning\nfor HVAC Control in Commercial Buildings,” IEEE\nTransactions on Smart Grid, vol. 12, pp. 407–419, Jan.\n2021.\n[138] J. Y. Park, T. Dougherty, H. Fritz, and Z. Nagy,\n“LightLearn: An adaptive and occupant centered con-\ntroller for lighting based on reinforcement learning,”\nBuilding and Environment, vol. 147, pp. 397–414, Jan.\n2019.\n[139] C. Blad, S. Bøgh, and C. S. Kallesøe, “Data-driven\nOffline Reinforcement Learning for HVAC-systems,”\nEnergy, vol. 261, p. 125290, Dec. 2022.\n[140] E. Mocanu, D. C. Mocanu, P. H. Nguyen, A. Liotta,\nM. E. Webber, M. Gibescu, and J. G. Slootweg, “On-\nLine Building Energy Optimization Using Deep Re-\ninforcement Learning,” IEEE Transactions on Smart\nGrid, vol. 10, pp. 3698–3708, July 2019.\n[141] Q. Fu, Z. Li, Z. Ding, J. Chen, J. Luo, Y. Wang,\nand Y. Lu, “ED-DQN: An event-driven deep reinforce-\nment learning control method for multi-zone residen-\ntial buildings,” Building and Environment, vol. 242,\np. 110546, Aug. 2023.\n[142] Z. Jiang, M. J. Risbeck, V. Ramamurti, S. Muruge-\nsan, J. Amores, C. Zhang, Y. M. Lee, and K. H. Drees,\n“Building HVAC control with reinforcement learning\nfor reduction of energy cost and demand charge,” En-\nergy and Buildings, vol. 239, p. 110833, May 2021.\n[143] G. Brockman, V. Cheung, L. Pettersson, J. Schnei-\nder, J. Schulman, J. Tang, and W. Zaremba, “OpenAI\nGym,” June 2016.\n[144] J. Arroyo, C. Manna, F. Spiessens, and L. Helsen,\n“An Open-AI gym environment for the Building Opti-\nmization Testing (BOPTEST) framework,” in Build-\ning Simulation 2021 Conference, (Bruges, Belgium),\nSept. 2021.\n[145] V. Dermardiros, “Vderm/SimplifiedBldgControlEnv,”\nJuly 2023.\n[146] A. Findeis, F. Kazhamiaka, S. Jeen, and S. Keshav,\n“Beobench: A toolkit for unified access to building\nsimulations for reinforcement learning,” in Proceed-\nings of the Thirteenth ACM International Conference\non Future Energy Systems, E-Energy ’22, (New York,\nNY, USA), pp. 374–382, Association for Computing\nMachinery, June 2022.\n[147] “Henze-research-group/MODRLC.” Gregor Henze’s\nResearch Group, Feb. 2024.\n29\n[148] J.\nJim´enez-Raboso,\nA.\nCampoy-Nieves,\nA.\nManjavacas-Lucas,\nJ.\nG´omez-Romero,\nand\nM. Molina-Solana, “Sinergym: A building simulation\nand control framework for training reinforcement\nlearning agents,” in Proceedings of the 8th ACM Inter-\nnational Conference on Systems for Energy-Efficient\nBuildings,\nCities,\nand\nTransportation,\n(Coimbra\nPortugal), pp. 319–323, Acm, Nov. 2021.\n[149] mechyai, “Mechyai/RL-EmsPy,” Feb. 2024.\n[150] T. Moriyama, G. De Magistris, M. Tatsubori, T.-H.\nPham, A. Munawar, and R. Tachibana, “Reinforce-\nment Learning Testbed for Power-Consumption Op-\ntimization,” in Methods and Applications for Mod-\neling and Simulation of Complex Systems (L. Li,\nK. Hasegawa, and S. Tanaka, eds.), Communications\nin Computer and Information Science, (Singapore),\npp. 45–59, Springer, 2018.\n[151] P. Scharnhorst, B. Schubnel, C. Fern´andez Bandera,\nJ. Salom, P. Taddeo, M. Boegli, T. Gorecki, Y. Stauf-\nfer, A. Peppas, and C. Politi, “Energym: A Building\nModel Library for Controller Benchmarking,” Applied\nSciences, vol. 11, p. 3518, Jan. 2021.\n[152] O. Lukianykhin and T. Bogodorova, “ModelicaGym:\nApplying Reinforcement Learning to Modelica Mod-\nels,” in Proceedings of the 9th International Work-\nshop on Equation-based Object-oriented Modeling Lan-\nguages and Tools, pp. 27–36, Nov. 2019.\n[153] D. W¨olfle, A. Vishwanath, and H. Schmeck, “A\nGuide for the Design of Benchmark Environments for\nBuilding Energy Optimization,” in Proceedings of the\n7th ACM International Conference on Systems for\nEnergy-Efficient Buildings, Cities, and Transporta-\ntion, BuildSys ’20, (New York, NY, USA), pp. 220–\n229, Association for Computing Machinery, Nov. 2020.\n[154] T. Zhang and O. Ardakanian, “COBS: COmpre-\nhensive Building Simulator,” in Proceedings of the\n7th ACM International Conference on Systems for\nEnergy-Efficient Buildings, Cities, and Transporta-\ntion, BuildSys ’20, (New York, NY, USA), pp. 314–\n315, Association for Computing Machinery, Nov. 2020.\n[155] zhangzhizza, “Zhangzhizza/Gym-Eplus,” Feb. 2024.\n[156] D. B. Crawley, L. K. Lawrie, F. C. Winkelmann, W. F.\nBuhl, Y. J. Huang, C. O. Pedersen, R. K. Strand, R. J.\nLiesen, D. E. Fisher, M. J. Witte, and J. Glazer, “En-\nergyPlus: Creating a new-generation building energy\nsimulation program,” Energy and Buildings, vol. 33,\npp. 319–331, Apr. 2001.\n[157] Z. Wang, B. Chen, H. Li, and T. Hong, “AlphaBuild-\ning ResCommunity: A multi-agent virtual testbed for\ncommunity-level load coordination,” Advances in Ap-\nplied Energy, vol. 4, p. 100061, Nov. 2021.\n[158] K.\nNweye,\nK.\nKaspar,\nG.\nBuscemi,\nG.\nPinto,\nH.\nLi,\nT.\nHong,\nM.\nOuf,\nA.\nCapozzoli,\nand\nZ. Nagy, “CityLearn v2: An OpenAI Gym environ-\nment for demand response control benchmarking in\ngrid-interactive communities,” in Proceedings of the\n10th ACM International Conference on Systems for\nEnergy-Efficient Buildings, Cities, and Transporta-\ntion, (Istanbul Turkey), pp. 274–275, Acm, Nov. 2023.\n[159] J. R. V´azquez-Canteli, S. Dey, G. Henze, and Z. Nagy,\n“The CityLearn Challenge 2020,” in Proceedings of\nthe 7th ACM International Conference on Systems for\nEnergy-Efficient Buildings, Cities, and Transporta-\ntion, (Virtual Event Japan), pp. 320–321, Acm, Nov.\n2020.\n[160] Z. Nagy, J. R. V´azquez-Canteli, S. Dey, and G. Henze,\n“The citylearn challenge 2021,” in Proceedings of the\n8th ACM International Conference on Systems for\nEnergy-Efficient Buildings, Cities, and Transporta-\ntion, BuildSys ’21, (New York, NY, USA), pp. 218–\n219, Association for Computing Machinery, Nov. 2021.\n[161] K. Nweye, Z. Nagy, S. Mohanty, D. Chakraborty,\nS. Sankaranarayanan, T. Hong, S. Dey, G. Henze,\nJ. Drgona, F. Lin, W. Jiang, H. Zhang, Z. Yi,\nJ. Zhang, C. Yang, M. Motoki, S. Khongnawang,\nM. Ibrahim, A. Zhumabekov, D. May, Z. Yang,\nX. Song, H. Zhang, X. Dong, S. Zheng, and J. Bian,\n“The CityLearn Challenge 2022: Overview, Results,\nand Lessons Learned,” in Proceedings of the NeurIPS\n2022 Competitions Track, pp. 85–103, Pmlr, Aug.\n2023.\n[162] Athanasios Paraskevas, Dimitrios Aletras, Antonios\nChrysopoulos, A. Marinopoulos, and D. I. Doukas,\n“Optimal Management for EV Charging Stations:\nA Win–Win Strategy for Different Stakeholders Us-\ning Constrained Deep Q-Learning,” Energies, vol. 15,\npp. 2323–2323, Mar. 2022.\n[163] Feifei Cui, Xixiang Lin, Ruining Zhang, and Qingyu\nYang, “Multi-objective optimal scheduling of charging\nstations based on deep reinforcement learning,” Fron-\ntiers in Energy Research, 2023.\n30\n[164] Jie Liu, Shuoyao Wang, Shuoyao Wang, and Xi-\naoying Tang, “Pricing and Charging Scheduling for\nCooperative Electric Vehicle Charging Stations via\nDeep Reinforcement Learning,” in 2022 IEEE In-\nternational Conference on Communications, Control,\nand Computing Technologies for Smart Grids (Smart-\nGridComm), (Singapore, Singapore), pp. 212–217,\nIeee, Oct. 2022.\n[165] R. Wang, Z. Chen, Q. Xing, Z. Zhang, and T. Zhang,\n“A Modified Rainbow-Based Deep Reinforcement\nLearning Method for Optimal Scheduling of Charging\nStation,” Sustainability, vol. 14, p. 1884, Jan. 2022.\n[166] Xianhao Shen, Yexin Zhang, and Decheng Wang, “On-\nline Charging Strategy for Electric Vehicle Clusters\nBased on Multi-Agent Reinforcement Learning and\nLong–Short Memory Networks,” Energies, vol. 15,\npp. 4582–4582, June 2022.\n[167] H. Li, Z. Wan, and H. He, “Constrained EV Charg-\ning Scheduling Based on Safe Deep Reinforcement\nLearning,” IEEE Transactions on Smart Grid, vol. 11,\npp. 2427–2439, May 2020.\n[168] S. Wang,\nS. Bi,\nY. J. Zhang,\nYing Jun An-\ngela Zhang, Ying-Jun Angela Zhang, and Y.-J. A.\nZhang, “Reinforcement Learning for Real-Time Pric-\ning and Scheduling Control in EV Charging Stations,”\nIEEE Transactions on Industrial Informatics, vol. 17,\npp. 849–859, Feb. 2021.\n[169] Z. Wan, H. Li, H. He, and D. V. Prokhorov, “Model-\nFree Real-Time EV Charging Scheduling Based on\nDeep Reinforcement Learning,” IEEE Transactions\non Smart Grid, vol. 10, pp. 5246–5257, Sept. 2019.\n[170] K.-B. Lee, M. A. Ahmed, Dong-Ki Kang, D.-K. Kang,\nand Y.-C. Kim, “Deep Reinforcement Learning Based\nOptimal Route and Charging Station Selection,” En-\nergies, vol. 13, no. 23, p. 6255, 2020.\n[171] Q. Xing, Yan Xu, Zhong Chen, Ziqi Zhang, and\nZhao Shi, “A Graph Reinforcement Learning-Based\nDecision-Making Platform for Real-Time Charging\nNavigation of Urban Electric Vehicles,” IEEE Trans-\nactions on Industrial Informatics, 2023.\n[172] T. Qian, C. Shao, X. Wang, Mohammad Shahideh-\npour, M. Shahidehpour, and M. Shahidehpour, “Deep\nReinforcement Learning for EV Charging Navigation\nby Coordinating Smart Grid and Intelligent Trans-\nportation System,”\nIEEE Transactions on Smart\nGrid, vol. 11, pp. 1714–1723, Mar. 2020.\n[173] A. Biswas, P. G. Anselma, and A. Emadi, “Real-Time\nOptimal Energy Management of Electrified Power-\ntrains with Reinforcement Learning,” in 2019 IEEE\nTransportation Electrification Conference and Expo\n(ITEC), (Detroit, MI, USA), pp. 1–6, Ieee, June 2019.\n[174] G. Du, Y. Zou, X. Zhang, T. Liu, J. Wu, and D. He,\n“Deep reinforcement learning based energy manage-\nment for a hybrid electric vehicle,” Energy, vol. 201,\np. 117591, June 2020.\n[175] Y. Li, H. He, A. Khajepour, Y. Chen, W. Huo, and\nH. Wang, “Deep reinforcement learning for intelligent\nenergy management systems of hybrid-electric power-\ntrains: Recent advances, open issues, and prospects,”\nIEEE Transactions on Transportation Electrification,\npp. 1–1, 2024.\n[176] Z. Yao and H.-S. Yoon, “Hybrid Electric Vehicle\nPowertrain Control Based on Reinforcement Learn-\ning,” SAE International Journal of Electrified Vehi-\ncles, vol. 11, no. 2, pp. 165–176, 2022.\n[177] Y. Zhang, C. Zhang, R. Fan, C. Deng, S. Wan, and\nH. Chaoui, “Energy management strategy for fuel cell\nvehicles via soft actor-critic-based deep reinforcement\nlearning considering powertrain thermal and durabil-\nity characteristics,” Energy Conversion and Manage-\nment, vol. 283, p. 116921, May 2023.\n[178] H. Matallah, A. Javied, A. Williams, A. F. Abdo, and\nF. Belblidia, “A Reinforcement Learning Approach\nto Powertrain Optimisation,” in Sustainable Design\nand Manufacturing (S. G. Scholz, R. J. Howlett, and\nR. Setchi, eds.), (Singapore), pp. 252–261, Springer,\n2023.\n[179] Y. Jiang, J. Liu, and H. Zheng, “Optimal scheduling of\ndistributed hydrogen refueling stations for fuel supply\nand reserve demand service with evolutionary trans-\nfer multi-agent reinforcement learning,” International\nJournal of Hydrogen Energy, vol. 54, pp. 239–255, Feb.\n2024.\n[180] Y. Li, Y. Wen, D. Tao, and K. Guan, “Transforming\nCooling Optimization for Green Data Center via Deep\nReinforcement Learning,” IEEE Transactions on Cy-\nbernetics, vol. 50, pp. 2002–2013, May 2020.\n31\n[181] M. Biemann, P. A. Gunkel, F. Scheller, L. Huang, and\nX. Liu, “Data Center HVAC Control Harnessing Flex-\nibility Potential via Real-Time Pricing Cost Optimiza-\ntion Using Reinforcement Learning,” IEEE Internet of\nThings Journal, vol. 10, pp. 13876–13894, Aug. 2023.\n[182] N. Lazic, C. Boutilier, T. Lu, E. Wong, B. Roy,\nMK. Ryu, and G. Imwalle, “Data center cooling us-\ning model-predictive control,” in Advances in Neural\nInformation Processing Systems, vol. 31, Curran As-\nsociates, Inc., 2018.\n[183] D. Yi, X. Zhou, Y. Wen, and R. Tan, “Toward Ef-\nficient Compute-Intensive Job Allocation for Green\nData Centers: A Deep Reinforcement Learning Ap-\nproach,” in 2019 IEEE 39th International Conference\non Distributed Computing Systems (ICDCS), (Dallas,\nTX, USA), pp. 634–644, Ieee, July 2019.\n[184] F. Farahnakian, P. Liljeberg, and J. Plosila, “Energy-\nEfficient Virtual Machines Consolidation in Cloud\nData Centers Using Reinforcement Learning,” in 2014\n22nd Euromicro International Conference on Parallel,\nDistributed, and Network-Based Processing, (Torino,\nItaly), pp. 500–507, Ieee, Feb. 2014.\n[185] C. Chi, K. Ji, A. Marahatta, P. Song, F. Zhang,\nand Z. Liu, “Jointly Optimizing the IT and Cooling\nSystems for Data Center Energy Efficiency based on\nMulti-Agent Deep Reinforcement Learning,” in Pro-\nceedings of the Eleventh ACM International Confer-\nence on Future Energy Systems, (Virtual Event Aus-\ntralia), pp. 489–495, Acm, June 2020.\n[186] Y. Ran, X. Zhou, H. Hu, and Y. Wen, “Optimizing\nData Center Energy Efficiency via Event-Driven Deep\nReinforcement Learning,” IEEE Transactions on Ser-\nvices Computing, vol. 16, pp. 1296–1309, Mar. 2023.\n[187] S. Afzali, S. Mosharafian, M. W. van Iersel, and J. M.\nVelni, “Optimal Lighting Control in Greenhouses\nEquipped with High-intensity Discharge Lamps Using\nReinforcement Learning,” in 2021 American Control\nConference (ACC), pp. 1414–1419, May 2021.\n[188] A. Ajagekar, N. S. Mattson, and F. You, “Energy-\nefficient AI-based Control of Semi-closed Greenhouses\nLeveraging Robust Optimization in Deep Reinforce-\nment Learning,” Advances in Applied Energy, vol. 9,\np. 100119, Feb. 2023.\n[189] W. Zhang, X. Cao, Y. Yao, Z. An, X. Xiao, and\nD. Luo, “Robust Model-based Reinforcement Learn-\ning for Autonomous Greenhouse Control,” in Proceed-\nings of The 13th Asian Conference on Machine Learn-\ning, pp. 1208–1223, Pmlr, Nov. 2021.\n[190] B. Morcego, W. Yin, S. Boersma, E. van Henten,\nV. Puig, and C. Sun, “Reinforcement Learning ver-\nsus Model Predictive Control on greenhouse climate\ncontrol,” Computers and Electronics in Agriculture,\nvol. 215, p. 108372, Dec. 2023.\n[191] M. Weigold, H. Ranzau, S. Schaumann, T. Kohne,\nN. Panten, and E. Abele, “Method for the application\nof deep reinforcement learning for optimised control\nof industrial energy supply systems by the example\nof a central cooling system,” CIRP Annals, vol. 70,\npp. 17–20, Jan. 2021.\n[192] J.-W. Park, Y.-M. Ju, Y.-G. Kim, and H.-S. Kim,\n“50% reduction in energy consumption in an ac-\ntual cold storage facility using a deep reinforcement\nlearning-based control algorithm,” Applied Energy,\nvol. 352, p. 121996, Dec. 2023.\n[193] X. Huang, S. H. Hong, M. Yu, Y. Ding, and J. Jiang,\n“Demand Response Management for Industrial Fa-\ncilities: A Deep Reinforcement Learning Approach,”\nIEEE Access, vol. 7, pp. 82194–82205, 2019.\n[194] A. Loffredo, M. C. May, L. Sch¨afer, A. Matta, and\nG. Lanza, “Reinforcement learning for energy-efficient\ncontrol of parallel and identical machines,” CIRP\nJournal of Manufacturing Science and Technology,\nvol. 44, pp. 91–103, Sept. 2023.\n[195] A. Loffredo, M. C. May, A. Matta, and G. Lanza, “Re-\ninforcement learning for sustainability enhancement of\nproduction lines,” Journal of Intelligent Manufactur-\ning, Nov. 2023.\n[196] D. Elavarasan and P. M. D. Vincent, “Crop Yield\nPrediction Using Deep Reinforcement Learning Model\nfor Sustainable Agrarian Applications,” IEEE Access,\nvol. 8, pp. 86886–86901, 2020.\n[197] A. Kazemeini and O. Swei, “Identifying environmen-\ntally sustainable pavement management strategies via\ndeep reinforcement learning,” Journal of Cleaner Pro-\nduction, vol. 390, p. 136124, Mar. 2023.\n32\n[198] Y. Liu, M. Yang, and Z. Guo, “Reinforcement learning\nbased optimal decision making towards product life-\ncycle sustainability,” International Journal of Com-\nputer Integrated Manufacturing, vol. 35, pp. 1269–\n1296, Nov. 2022.\n[199] M. E. P´erez-Pons, R. S. Alonso, O. Garc´ıa, G. Mar-\nreiros, and J. M. Corchado, “Deep Q-Learning and\nPreference Based Multi-Agent System for Sustainable\nAgricultural Market,” Sensors, vol. 21, p. 5276, Jan.\n2021.\n[200] W. Serrano, “Deep Reinforcement Learning Algo-\nrithms in Intelligent Infrastructure,” Infrastructures,\nvol. 4, p. 52, Sept. 2019.\n[201] K. Zhou, S. Song, A. Xue, K. You, and H. Wu, “Smart\nTrain Operation Algorithms Based on Expert Knowl-\nedge and Reinforcement Learning,” IEEE Transac-\ntions on Systems, Man, and Cybernetics: Systems,\nvol. 52, pp. 716–727, Feb. 2022.\n[202] M. Koch, T. Duigou, and J.-L. Faulon, “Reinforce-\nment Learning for Bioretrosynthesis,” ACS Synthetic\nBiology, vol. 9, pp. 157–168, Jan. 2020.\n[203] T. Lan and Q. An, “Discovering catalytic reaction net-\nworks using deep reinforcement learning from first-\nprinciples,” Journal of the American Chemical Soci-\nety, vol. 143, no. 40, pp. 16804–16812, 2021.\n[204] L. Berntzen and Q. Meng, “The role of aggregators\nin smart grids,” in Sustainable Smart Cities-A Vision\nfor Tomorrow, IntechOpen, 2022.\n[205] J. Liu, H. Hu, S. S. Yu, and H. Trinh, “Virtual\npower plant with renewable energy sources and energy\nstorage systems for sustainable power grid-formation,\ncontrol techniques and demand response,” Energies,\nvol. 16, 2023.\n[206] L. Lin, X. Guan, Y. Peng, N. Wang, S. Maharjan,\nand T. Ohtsuki, “Deep reinforcement learning for eco-\nnomic dispatch of virtual power plant in internet of\nenergy,” IEEE Internet of Things Journal, vol. 7,\npp. 6288–6301, 7 2020.\n[207] X. Ji, C. Li, J. Wang, Y. Wang, F. Hou, and S. Guo,\n“Energy management optimization strategy of virtual\npower plant based on deep reinforcement learning,”\nin Journal of Physics: Conference Series, vol. 2384,\np. 012041, IOP Publishing, 2022.\n[208] O. Stanojev, L. Mitridati, R. d. N. Di Prata, and\nG. Hug, “Safe reinforcement learning for strategic bid-\nding of virtual power plants in day-ahead markets,” in\n2023 IEEE International Conference on Communica-\ntions, Control, and Computing Technologies for Smart\nGrids (SmartGridComm), pp. 1–7, Ieee, 2023.\n[209] J. Wang, C. Guo, C. Yu, and Y. Liang, “Virtual power\nplant containing electric vehicles scheduling strategies\nbased on deep reinforcement learning,” Electric Power\nSystems Research, vol. 205, 4 2022.\n[210] M. Al-Gabalawy, “Reinforcement learning for the op-\ntimization of electric vehicle virtual power plants,”\nInternational Transactions on Electrical Energy Sys-\ntems, vol. 31, no. 8, p. e12951, 2021.\n[211] S. Oh, J. Jung, A. Onen, and C.-H. Lee, “A rein-\nforcement learning-based demand response strategy\ndesigned from the aggregator’s perspective,” Frontiers\nin Energy Research, vol. 10, 2022.\n[212] T. Chen, Q. Cui, C. Gao, Q. Hu, K. Lai, J. Yang,\nR. Lyu, H. Zhang, and J. Zhang, “Optimal demand\nresponse strategy of commercial building-based vir-\ntual power plant using reinforcement learning,” IET\nGeneration, Transmission & Distribution, vol. 15,\npp. 2309–2318, 2021.\n[213] F. Rezazadeh and N. Bartzoudis, “A federated drl ap-\nproach for smart micro-grid energy control with dis-\ntributed energy resources,” in 2022 IEEE 27th Inter-\nnational Workshop on Computer Aided Modeling and\nDesign of Communication Links and Networks (CA-\nMAD), pp. 108–114, Ieee, 2022.\n[214] S. Orfanoudakis and G. Chalkiadakis,\n“A novel\nmultiagent flexibility aggregation framework,” arXiv\npreprint arXiv:2307.08401, 2023.\n[215] J. Xu, K. Li, and M. Abusara, “Multi-objective rein-\nforcement learning based multi-microgrid system op-\ntimisation problem,” in International Conference on\nEvolutionary Multi-Criterion Optimization, pp. 684–\n696, Springer, 2021.\n[216] J. S. Vardakas, N. Zorba, and C. V. Verikoukis,\n“A survey on demand response programs in smart\ngrids: Pricing methods and optimization algorithms,”\nIEEE Communications Surveys & Tutorials, vol. 17,\npp. 152–178, 2015.\n33\n[217] A. Fraija, N. Henao, K. Agbossou, S. Kelouwani,\nM. Fournier, and S. H. Nagarsheth, “Deep reinforce-\nment learning based dynamic pricing for demand re-\nsponse considering market and supply constraints,”\nSmart Energy, vol. 14, p. 100139, 2024.\n[218] N. Avila, S. Hardan, E. Zhalieva, M. Aloqaily, and\nM. Guizani, “Energy pricing in p2p energy sys-\ntems using reinforcement learning,” arXiv preprint\narXiv:2210.13555, 2022.\n[219] S. Bahrami, Y. C. Chen, and V. W. Wong, “Deep\nreinforcement learning for demand response in distri-\nbution networks,” IEEE Transactions on Smart Grid,\nvol. 12, pp. 1496–1506, 3 2021.\n[220] B. Wang, Y. Li, W. Ming, and S. Wang, “Deep rein-\nforcement learning method for demand response man-\nagement of interruptible load,” IEEE Transactions on\nSmart Grid, vol. 11, pp. 3146–3155, 7 2020.\n[221] M. Uddin, H. Mo, D. Dong, S. Elsawah, J. Zhu, and\nJ. M. Guerrero, “Microgrids: A review, outstanding\nissues and future trends,” 9 2023.\n[222] G. Dalal, E. Gilboa, and S. Mannor, “Hierarchical de-\ncision making in electricity grid management,” in In-\nternational conference on machine learning, pp. 2197–\n2206, Pmlr, 2016.\n[223] Y. Li, S. He, Y. Li, Y. Shi, and Z. Zeng, “Feder-\nated multiagent deep reinforcement learning approach\nvia physics-informed reward for multimicrogrid energy\nmanagement,” IEEE Transactions on Neural Net-\nworks and Learning Systems, 2023.\n[224] D. Liu, C. Zang, P. Zeng, W. Li, X. Wang, Y. Liu, and\nS. Xu, “Deep reinforcement learning for real-time eco-\nnomic energy management of microgrid system con-\nsidering uncertainties,” Frontiers in Energy Research,\nvol. 11, 2023.\n[225] L. Lei, Y. Tan, G. Dahlenburg, W. Xiang, and\nK. Zheng, “Dynamic energy dispatch based on deep\nreinforcement learning in iot-driven smart isolated mi-\ncrogrids,” IEEE internet of things journal, vol. 8,\nno. 10, pp. 7938–7953, 2020.\n[226] M. Sage, M. Staniszewski, and Y. F. Zhao, “Optimal\neconomic gas turbine dispatch with deep reinforce-\nment learning,” IFAC-PapersOnLine, vol. 56, no. 2,\npp. 10039–10044, 2023.\n[227] F. Marzbani and A. Abdelfatah, “Economic dispatch\noptimization strategies and problem formulation: A\ncomprehensive review,” Energies, vol. 17, 2024.\n[228] K. Nghitevelekwa and R. C. Bansal, “A review of\ngeneration dispatch with large-scale photovoltaic sys-\ntems,” Renewable and Sustainable Energy Reviews,\nvol. 81, pp. 615–624, 2018.\n[229] X. Xia and A. M. Elaiw, “Optimal dynamic economic\ndispatch of generation: A review,” Electric Power Sys-\ntems Research, vol. 80, pp. 975–986, 2010.\n[230] Z. Guo, W. Wei, M. Shahidehpour, Z. Wang, and\nS. Mei, “Optimisation methods for dispatch and con-\ntrol of energy storage with renewable integration,”\nIET Smart Grid, vol. 5, pp. 137–160, 2022.\n[231] F. Monfaredi, H. Shayeghi, and P. Siano, “Multi-\nagent deep reinforcement learning-based optimal en-\nergy management for grid-connected multiple energy\ncarrier microgrids,” International Journal of Electrical\nPower & Energy Systems, vol. 153, p. 109292, 2023.\n[232] D. J. Harrold, J. Cao, and Z. Fan, “Renewable energy\nintegration and microgrid energy trading using multi-\nagent deep reinforcement learning,” Applied Energy,\nvol. 318, p. 119151, 2022.\n[233] F. Charbonnier, B. Peng, T. Morstyn, and M. Mc-\nCulloch, “Centralised rehearsal of decentralised coop-\neration:\nMulti-agent reinforcement learning for the\nscalable coordination of residential energy flexibility,”\narXiv preprint arXiv:2305.18875, 2023.\n[234] Y. Cui, Y. Xu, L. Yang, Y. Wang, and X. Zou, “Deep\nreinforcement learning based optimal energy manage-\nment of multi-energy microgrids with uncertainties,”\nCSEE Journal of Power and Energy Systems, 2023.\n[235] C. Caputo, M.-A. Cardin, P. Ge, F. Teng, A. Korre,\nA. Del, and R. Chanona, “Design and planning of\nflexible mobile micro-grids using deep reinforcement\nlearning,” 2023.\n[236] S.\nLiu,\nJ.\nLiu,\nW.\nYe,\nN.\nYang,\nG.\nZhang,\nH. Zhong, C. Kang, Q. Jiang, X. Song, F. Di,\net al., “Real-time scheduling of renewable power sys-\ntems through planning-based reinforcement learning,”\narXiv preprint arXiv:2303.05205, 2023.\n[237] B. Zhou, H. Zeng, Y. Liu, K. Li, F. Wang, and\nH. Tian, “Action set based policy optimization for\n34\nsafe power grid management,” in Machine Learning\nand Knowledge Discovery in Databases. Applied Data\nScience Track: European Conference, ECML PKDD\n2021, Bilbao, Spain, September 13–17, 2021, Proceed-\nings, Part V 21, pp. 168–181, Springer, 2021.\n[238] D. Dom´ınguez-Barbero, J. Garc´ıa-Gonz´alez, M. A.\nSanz-Bobi, and E. F. S´anchez-´Ubeda, “Optimising a\nmicrogrid system by deep reinforcement learning tech-\nniques,” Energies, vol. 13, 6 2020.\n[239] K. Nweye, S. Sankaranarayanan, and Z. Nagy, “Mer-\nlin:\nMulti-agent offline and transfer learning for\noccupant-centric operation of grid-interactive commu-\nnities,” Applied Energy, vol. 346, p. 121323, 2023.\n[240] A. Ghasemi, A. Shojaeighadikolaei, and M. Hashemi,\n“Combating uncertainties in wind and distributed\npv energy sources using integrated reinforcement\nlearning and time-series forecasting,” arXiv preprint\narXiv:2302.14094, 02 2023.\n[241] H. Zhou, A. Aral, I. Brandi´c, and M. Erol-Kantarci,\n“Multiagent bayesian deep reinforcement learning for\nmicrogrid energy management under communication\nfailures,” IEEE Internet of Things Journal, vol. 9,\nno. 14, pp. 11685–11698, 2021.\n[242] H. Sun, Q. Guo, J. Qi, V. Ajjarapu, R. Bravo,\nJ. Chow, Z. Li, R. Moghe, E. Nasr-Azadani, U. Tam-\nrakar, G. N. Taranto, R. Tonkoski, G. Valverde,\nQ. Wu, and G. Yang, “Review of challenges and\nresearch opportunities for voltage control in smart\ngrids,” IEEE Transactions on Power Systems, vol. 34,\npp. 2790–2801, 2019.\n[243] Y. Zhou, F. Gao, Z. Zhang, S. Zhao, X. Xu, H. Meng,\nand H. Gao, “Research on optimal voltage control of\ndistribution network with the participation of evs and\npvs,” Applied Sciences, vol. 13, no. 10, 2023.\n[244] O. Stanojev, O. Kundacina, U. Markovic, E. Vrettos,\nP. Aristidou, and G. Hug, “A reinforcement learn-\ning approach for fast frequency control in low-inertia\npower systems,” in 2020 52nd North American Power\nSymposium (NAPS), pp. 1–6, Ieee, 2021.\n[245] J. Zhang, Y. Li, Z. Wu, C. Rong, T. Wang, Z. Zhang,\nand S. Zhou, “Deep-reinforcement-learning-based two-\ntimescale voltage control for distribution systems,”\nEnergies, vol. 14, 6 2021.\n[246] R. Diao, Z. Wang, D. Shi, Q. Chang, J. Duan, and\nX. Zhang, “Autonomous voltage control for grid oper-\nation using deep reinforcement learning,” 04 2019.\n[247] J. Duan, D. Shi, R. Diao, H. Li, Z. Wang, B. Zhang,\nD. Bian, and Z. Yi, “Deep-reinforcement-learning-\nbased autonomous voltage control for power grid\noperations,” IEEE Transactions on Power Systems,\nvol. 35, no. 1, pp. 814–817, 2019.\n[248] R. Diao, D. Shi, B. Zhang, S. Wang, H. Li, C. Xu,\nT. Lan, D. Bian, and J. Duan, “On training ef-\nfective reinforcement learning agents for real-time\npower grid operation and control,” arXiv preprint\narXiv:2012.06458, 12 2020.\n[249] H. Xu, J. Zheng, and G. Qu, “A scalable network-\naware multi-agent reinforcement learning framework\nfor\ndecentralized\ninverter-based\nvoltage\ncontrol,”\narXiv preprint arXiv:2312.04371, 2023.\n[250] B.-G. Risi, F. Riganti-Fulginei, and A. Laudani,\n“Modern techniques for the optimal power flow prob-\nlem: State of the art,” Energies, vol. 15, 2022.\n[251] M. Lehna, J. Viebahn, A. Marot, S. Tomforde, and\nC. Scholz, “Managing power grids through topology\nactions: A comparative study between advanced rule-\nbased and reinforcement learning agents,” Energy and\nAI, vol. 14, p. 100276, 2023.\n[252] E. van der Sar, A. Zocca, and S. Bhulai, “Multi-agent\nreinforcement learning for power grid topology opti-\nmization,” arXiv preprint arXiv:2310.02605, 2023.\n[253] J. R. Vazquez-Canteli,\nG. Henze,\nand Z. Nagy,\n“MARLISA:\nMulti-Agent\nReinforcement\nLearning\nwith Iterative Sequential Action Selection for Load\nShaping of Grid-Interactive Connected Buildings,” in\nProceedings of the 7th ACM International Conference\non Systems for Energy-Efficient Buildings, Cities, and\nTransportation, (Virtual Event Japan), pp. 170–179,\nAcm, Nov. 2020.\n[254] M. T. Spaan, “Partially observable markov decision\nprocesses,” in Reinforcement learning: State-of-the-\nart, pp. 387–414, Springer, 2012.\n[255] T. Jaakkola, S. Singh, and M. Jordan, “Reinforcement\nlearning algorithm for partially observable markov de-\ncision problems,” Advances in neural information pro-\ncessing systems, vol. 7, 1994.\n35\n[256] D. Wierstra, A. F¨orster, J. Peters, and J. Schmid-\nhuber, “Recurrent policy gradients,” Logic Journal of\nIGPL, vol. 18, no. 5, pp. 620–634, 2010.\n[257] E. Parisotto, F. Song, J. Rae, R. Pascanu, C. Gul-\ncehre, S. Jayakumar, M. Jaderberg, R. L. Kaufman,\nA. Clark, S. Noury, et al., “Stabilizing transformers\nfor reinforcement learning,” in International confer-\nence on machine learning, pp. 7487–7498, Pmlr, 2020.\n[258] M. Hauskrecht, “Value-function approximations for\npartially observable markov decision processes,” Jour-\nnal of artificial intelligence research, vol. 13, pp. 33–\n94, 2000.\n[259] M. Hausknecht and P. Stone, “Deep recurrent q-\nlearning for partially observable mdps,” in 2015 aaai\nfall symposium series, 2015.\n[260] S. Chiappa, S. Racaniere, D. Wierstra, and S. Mo-\nhamed, “Recurrent environment simulators,” in In-\nternational Conference on Learning Representations,\n2016.\n[261] C. Chen, Y.-F. Wu, J. Yoon, and S. Ahn, “Trans-\ndreamer:\nReinforcement learning with transformer\nworld\nmodels,”\narXiv preprint arXiv:2202.09481,\n2022.\n[262] L. Lin and T. Mitchell, “Memory Approaches to Re-\ninforcement Learning in Non-Markovian Domains,”\ntechnical Report, Carnegie Mellon University, Usa,\nApr. 1992.\n[263] L. Medsker and L. C. Jain, Recurrent neural networks:\ndesign and applications. CRC press, 1999.\n[264] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[265] A. Gu, K. Goel, and C. R´e, “Efficiently modeling\nlong sequences with structured state spaces,” arXiv\npreprint arXiv:2111.00396, 2021.\n[266] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez,  L. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural infor-\nmation processing systems, vol. 30, 2017.\n[267] A. Graves, G. Wayne, and I. Danihelka, “Neural turing\nmachines,” arXiv preprint arXiv:1410.5401, 2014.\n[268] A. Gu, K. Goel, and C. R´e, “Efficiently Modeling Long\nSequences with Structured State Spaces,” Aug. 2022.\n[269] C. Lu, Y. Schroecker, A. Gu, E. Parisotto, J. Foerster,\nS. Singh, and F. Behbahani, “Structured State Space\nModels for In-Context Reinforcement Learning,” Nov.\n2023.\n[270] A. R. Cassandra, L. P. Kaelbling, and M. L. Littman,\n“Acting optimally in partially observable stochastic\ndomains,” in Aaai, vol. 94, pp. 1023–1028, 1994.\n[271] H. Zhang, D. Yue, C. Dou, K. Li, and G. P. Hancke,\n“Two-Step Wind Power Prediction Approach With\nImproved Complementary Ensemble Empirical Mode\nDecomposition and Reinforcement Learning,” IEEE\nSystems Journal, vol. 16, pp. 2545–2555, June 2022.\n[272] X. Li, N. Yang, Z. Li, Y. Huang, Z. Yuan, X. Song,\nL. Li, and L. Zhang, “Confidence estimation trans-\nformer for long-term renewable energy forecasting in\nreinforcement learning-based power grid dispatching,”\nCSEE Journal of Power and Energy Systems, 2022.\n[273] Z. Deng and Q. Chen, “Reinforcement learning of\noccupant behavior model for cross-building transfer\nlearning to various HVAC control systems,” Energy\nand Buildings, vol. 238, p. 110860, May 2021.\n[274] H. Li, Z. Wan, and H. He, “Real-Time Residential De-\nmand Response,” IEEE Transactions on Smart Grid,\nvol. 11, pp. 4144–4154, Sept. 2020.\n[275] S.\nLevine,\nA.\nKumar,\nG.\nTucker,\nand\nJ.\nFu,\n“Offline reinforcement learning:\nTutorial, review,\nand perspectives on open problems,” arXiv preprint\narXiv:2005.01643, 2020.\n[276] R. S. Sutton, “Integrated architectures for learning,\nplanning, and reacting based on approximating dy-\nnamic programming,” in Machine learning proceedings\n1990, pp. 216–224, Elsevier, 1990.\n[277] M. Deisenroth and C. E. Rasmussen, “Pilco: A model-\nbased and data-efficient approach to policy search,” in\nProceedings of the 28th International Conference on\nmachine learning (ICML-11), pp. 465–472, 2011.\n[278] R. Kidambi,\nA. Rajeswaran,\nP. Netrapalli,\nand\nT. Joachims, “Morel: Model-based offline reinforce-\nment learning,” Advances in neural information pro-\ncessing systems, vol. 33, pp. 21810–21823, 2020.\n36\n[279] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou,\nS. Levine, C. Finn, and T. Ma, “Mopo: Model-based\noffline policy optimization,” Advances in Neural Infor-\nmation Processing Systems, vol. 33, pp. 14129–14142,\n2020.\n[280] F. Berkenkamp, M. Turchetta, A. Schoellig, and\nA. Krause, “Safe model-based reinforcement learning\nwith stability guarantees,” Advances in neural infor-\nmation processing systems, vol. 30, 2017.\n[281] G. Thomas, Y. Luo, and T. Ma, “Safe reinforce-\nment learning by imagining the near future,” Advances\nin Neural Information Processing Systems, vol. 34,\npp. 13859–13869, 2021.\n[282] M. Janner, J. Fu, M. Zhang, and S. Levine, “When\nto trust your model: Model-based policy optimiza-\ntion,” Advances in neural information processing sys-\ntems, vol. 32, 2019.\n[283] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou,\nM. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran,\nT. Graepel, et al., “A general reinforcement learning\nalgorithm that masters chess, shogi, and go through\nself-play,” Science, vol. 362, no. 6419, pp. 1140–1144,\n2018.\n[284] Z. Wu, C. Yu, C. Chen, J. Hao, and H. H. Zhuo, “Plan\nto predict: Learning an uncertainty-foreseeing model\nfor model-based reinforcement learning,” Advances\nin Neural Information Processing Systems, vol. 35,\npp. 15849–15861, 2022.\n[285] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, “Mas-\ntering diverse domains through world models,” arXiv\npreprint arXiv:2301.04104, 2023.\n[286] R. Agarwal, D. Schuurmans, and M. Norouzi, “An\noptimistic perspective on offline reinforcement learn-\ning,” in International Conference on Machine Learn-\ning, pp. 104–114, Pmlr, 2020.\n[287] S. Fujimoto and S. S. Gu, “A minimalist approach\nto offline reinforcement learning,” Advances in neural\ninformation processing systems, vol. 34, pp. 20132–\n20145, 2021.\n[288] M. Janner, Q. Li, and S. Levine, “Offline reinforce-\nment learning as one big sequence modeling problem,”\nAdvances in neural information processing systems,\nvol. 34, pp. 1273–1286, 2021.\n[289] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Con-\nservative q-learning for offline reinforcement learning,”\nAdvances in Neural Information Processing Systems,\nvol. 33, pp. 1179–1191, 2020.\n[290] C. Bai, L. W. 0003, Z. Yang, Z.-H. Deng, A. Garg,\nP. L. 0008, and Z. Wang, “Pessimistic bootstrapping\nfor uncertainty-driven offline reinforcement learning,”\nin The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-\n29, 2022, OpenReview.net, 2022.\n[291] G. An,\nS. Moon,\nJ.-H. Kim,\nand H. O. Song,\n“Uncertainty-based offline reinforcement learning with\ndiversified q-ensemble,” in Advances in Neural Infor-\nmation Processing Systems (M. Ranzato, A. Beygelz-\nimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.),\nvol. 34, p. 7436–7447, Curran Associates, Inc., 2021.\n[292] W. Xu, X. Zhang, A. Peng, and Y. Liang, “Deep Rein-\nforcement Learning for Cascaded Hydropower Reser-\nvoirs Considering Inflow Forecasts,” Water Resources\nManagement, vol. 34, pp. 3003–3018, July 2020.\n[293] R. Liu and Y. Chen, “Learning a multi-agent con-\ntroller for shared energy storage system,” in 2023\nIEEE Power & Energy Society General Meeting\n(PESGM), pp. 1–5, Ieee, 2023.\n[294] N. Cuadrado, R. Gutierrez, Y. Zhu, and M. Takac,\n“Mahtm:\nA\nmulti-agent\nframework\nfor\nhier-\narchical\ntransactive\nmicrogrids,”\narXiv\npreprint\narXiv:2303.08447, 2023.\n[295] D. Cao, W. Hu, X. Xu, T. Dragicevic, Q. Huang,\nZ. Liu, Z. Chen, and F. Blaabjerg, “Bidding strat-\negy for trading wind energy and purchasing reserve of\nwind power producer – a drl based approach,” Interna-\ntional Journal of Electrical Power & Energy Systems,\nvol. 117, p. 105648, 05 2020.\n[296] J. Liu, H. Guo, Q. Tang, E. Lu, Q. Cai, and\nQ. Chen, “High-dimensional bid learning for energy\nstorage bidding in energy markets,” arXiv preprint\narXiv:2311.02551, 2023.\n[297] M. Pecka and T. Svoboda, “Safe exploration tech-\nniques for reinforcement learning–an overview,” in\nModelling and Simulation for Autonomous Systems:\nFirst International Workshop, MESAS 2014, Rome,\nItaly, May 5-6, 2014, Revised Selected Papers 1,\npp. 357–375, Springer, 2014.\n37\n[298] J. Garcıa and F. Fern´andez, “A comprehensive survey\non safe reinforcement learning,” Journal of Machine\nLearning Research, vol. 16, no. 1, pp. 1437–1480, 2015.\n[299] S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang,\nY. Yang, and A. Knoll, “A review of safe reinforcement\nlearning: Methods, theory and applications,” arXiv\npreprint arXiv:2205.10330, 2022.\n[300] S. Junges, N. Jansen, C. Dehnert, U. Topcu, and J.-\nP. Katoen, “Safety-constrained reinforcement learning\nfor mdps,” in International conference on tools and al-\ngorithms for the construction and analysis of systems,\npp. 130–146, Springer, 2016.\n[301] M. Alshiekh, R. Bloem, R. Ehlers, B. K¨onighofer,\nS. Niekum, and U. Topcu, “Safe reinforcement learn-\ning via shielding,” Proceedings of the AAAI conference\non artificial intelligence, vol. 32, Apr. 2018.\n[302] B. L¨utjens, M. Everett, and J. P. How, “Safe reinforce-\nment learning with model uncertainty estimates,” in\n2019 International Conference on Robotics and Au-\ntomation (ICRA), pp. 8662–8668, Ieee, 2019.\n[303] R. Cheng, G. Orosz, R. M. Murray, and J. W. Bur-\ndick, “End-to-end safe reinforcement learning through\nbarrier functions for safety-critical continuous control\ntasks,” Proceedings of the AAAI conference on artifi-\ncial intelligence, vol. 33, no. 01, pp. 3387–3395, 2019.\n[304] Q. Yang,\nT. D. Sim˜ao,\nS. H. Tindemans,\nand\nM. T. Spaan, “Safety-constrained reinforcement learn-\ning with a distributional safety critic,” Machine Learn-\ning, vol. 112, no. 3, pp. 859–887, 2023.\n[305] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou,\nJ. Panerati, and A. P. Schoellig, “Safe learning in\nrobotics:\nFrom learning-based control to safe re-\ninforcement learning,” Annual Review of Control,\nRobotics, and Autonomous Systems, vol. 5, pp. 411–\n444, 2022.\n[306] L. Werner and P. Kumar, “Multi-market energy opti-\nmization with renewables via reinforcement learning,”\narXiv preprint arXiv:2306.08147, 2023.\n[307] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis,\nG. de Cola, T. Deleu, M. Goul˜ao, A. Kallinteris,\nA. KG, M. Krimmel, R. Perez-Vicente, A. Pierr´e,\nS. Schulhoff, J. J. Tai, A. J. S. Tan, and O. G. Younis,\n“Gymnasium,” July 2023.\n[308] C. Yeh, V. Li, R. Datta, J. Arroyo, N. Christianson,\nC. Zhang, Y. Chen, M. M. Hosseini, A. Golmoham-\nmadi, Y. Shi, Y. Yue, and A. Wierman, “SustainGym:\nReinforcement Learning Environments for Sustainable\nEnergy Systems,” in Thirty-Seventh Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track, Nov. 2023.\n[309] A. Marot, B. Donnot, C. Romero, B. Donon, M. Ler-\nousseau, L. Veyrin-Forrer, and I. Guyon, “Learning\nto run a power network challenge for training topol-\nogy controllers,” Electric Power Systems Research,\nvol. 189, p. 106635, 2020.\n[310] A. Marot, B. Donnot, G. Dulac-Arnold, A. Kelly,\nA. O’Sullivan, J. Viebahn, M. Awad, I. Guyon, P. Pan-\nciatici, and C. Romero, “Learning to run a power net-\nwork challenge: a retrospective analysis,” in NeurIPS\n2020 Competition and Demonstration Track, pp. 112–\n132, Pmlr, 2021.\n[311] A. Marot,\nB. Donnot,\nK. Chaouache,\nA. Kelly,\nQ. Huang, R.-R. Hossain, and J. L. Cremer, “Learn-\ning to run a power network with trust,” Electric Power\nSystems Research, vol. 212, p. 108487, 2022.\n[312] B. Donnot, “Grid2op- A testbed platform to model se-\nquential decision making in power systems. .” https:\n//GitHub.com/rte-france/grid2op, 2020.\n[313] M. Lerousseau, “Design and implementation of an\nenvironment for learning to run a power network\n(l2rpn),” arXiv preprint arXiv:2104.04080, 2021.\n[314] R. Henry and D. Ernst, “Gym-anm: Reinforcement\nlearning environments for active network management\ntasks in electricity distribution systems,” Energy and\nAI, vol. 5, p. 100092, 2021.\n[315] R. Henry and D. Ernst, “Gym-anm: Open-source soft-\nware to leverage reinforcement learning for power sys-\ntem management in research and education,” Software\nImpacts, vol. 9, p. 100092, 2021.\n[316] D. Biagioni, X. Zhang, D. Wald, D. Vaidhynathan,\nR. Chintala, J. King, and A. S. Zamzam, “Power-\ngridworld: A framework for multi-agent reinforcement\nlearning in power systems,” in Proceedings of the Thir-\nteenth ACM International Conference on Future En-\nergy Systems, pp. 565–570, 2022.\n[317] J. Jonkman and M. Sprague, “Openfast.” https://\ngithub.com/openfast.\n38\n[318] S. Boersma, B. Doekemeijer, M. Vali, J. Meyers, and\nJ.-W. van Wingerden, “A control-oriented dynamic\nwind farm model:\nWfsim,” Wind Energy Science,\nvol. 3, no. 1, pp. 75–95, 2018.\n[319] Siby\nJose\nPlathottam,\n“gym-SolarPVDER-\nenvironment:\nA environment for solar photovoltaic\ndistributed energy resources.” https://github.com/\nsibyjackgrove/gym-SolarPVDER-environment,\n2019. [Online; accessed 18-March-2019].\n[320] D. Blum, J. Arroyo, S. Huang, J. Drgoˇna, F. Jorissen,\nH. T. Walnum, Y. Chen, K. Benne, D. Vrabie, M. Wet-\nter, and L. Helsen, “Building optimization testing\nframework (BOPTEST) for simulation-based bench-\nmarking of control strategies in buildings,” Journal of\nBuilding Performance Simulation, vol. 14, pp. 586–\n610, Sept. 2021.\n[321] J. R. V´azquez-Canteli, J. K¨ampf, G. Henze, and\nZ. Nagy, “CityLearn v1.0: An OpenAI Gym Envi-\nronment for Demand Response with Deep Reinforce-\nment Learning,” in Proceedings of the 6th ACM Inter-\nnational Conference on Systems for Energy-Efficient\nBuildings, Cities, and Transportation, (New York NY\nUSA), pp. 356–357, Acm, Nov. 2019.\n[322] C. Tessler, Y. Shpigelman, G. Dalal, A. Mandelbaum,\nD. H. Kazakov, B. Fuhrer, G. Chechik, and S. Mannor,\n“Reinforcement Learning for Datacenter Congestion\nControl,” June 2022.\n[323] V. Samsonov, K. Ben Hicham, and T. Meisen, “Rein-\nforcement Learning in Manufacturing Control: Base-\nlines, challenges and ways forward,” Engineering Ap-\nplications of Artificial Intelligence, vol. 112, p. 104868,\nJune 2022.\n[324] L. Yan, H. Shen, L. Kang, J. Zhao, Z. Zhang, and\nC. Xu, “MobiCharger: Optimal Scheduling for Coop-\nerative EV-to-EV Dynamic Wireless Charging,” IEEE\nTransactions on Mobile Computing, pp. 1–17, 2022.\n[325] G. Karatzinis, C. Korkas, M. Terzopoulos, C. Tsak-\nnakis, A. Stefanopoulou, I. Michailidis, and E. Kos-\nmatopoulos, “Chargym:\nAn EV Charging Station\nModel for Controller Benchmarking,” in Artificial In-\ntelligence Applications and Innovations. AIAI 2022\nIFIP WG 12.5 International Workshops (I. Maglo-\ngiannis, L. Iliadis, J. Macintyre, and P. Cortez, eds.),\n(Cham), pp. 241–252, Springer International Publish-\ning, 2022.\n[326] S. Orfanoudakis, C. Diaz-Londono, Y. E. Yılmaz,\nP. Palensky, and P. P. Vergara, “EV2Gym: A Flexible\nV2G Simulator for EV Smart Charging Research and\nBenchmarking,” Apr. 2024.\n[327] S. Qiu, Z. Li, Z. Pang, Z. Li, and Y. Tao, “Multi-\nAgent Optimal Control for Central Chiller Plants Us-\ning Reinforcement Learning and Game Theory,” Sys-\ntems, vol. 11, p. 136, Mar. 2023.\n[328] J. A. A. Silva, J. C. L´opez, N. B. Arias, M. J. Rider,\nand L. C. P. da Silva, “An optimal stochastic energy\nmanagement system for resilient microgrids,” Applied\nEnergy, vol. 300, p. 117435, 2021.\n[329] J. Xie, H. Dong, X. Zhao, and A. Karcanias, “Wind\nFarm Power Generation Control Via Double-Network-\nBased Deep Reinforcement Learning,” IEEE Trans-\nactions on Industrial Informatics, vol. 18, pp. 2321–\n2330, Apr. 2022.\n[330] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernes-\ntus, and N. Dormann, “Stable-Baselines3: Reliable\nReinforcement Learning Implementations,” Journal of\nMachine Learning Research, vol. 22, no. 268, pp. 1–8,\n2021.\n[331] S.\nHuang,\nR.\nF.\nJ.\nDossa,\nC.\nYe,\nJ.\nBraga,\nD. Chakraborty, K. Mehta, and J. G. M. Ara´ujo,\n“CleanRL: High-quality Single-file Implementations of\nDeep Reinforcement Learning Algorithms,” Journal of\nMachine Learning Research, vol. 23, no. 274, pp. 1–18,\n2022.\n[332] E. Liang, R. Liaw, P. Moritz, R. Nishihara, R. Fox,\nK. Goldberg, J. E. Gonzalez, M. I. Jordan, and I. Sto-\nica, “RLlib: Abstractions for Distributed Reinforce-\nment Learning,” June 2018.\n[333] C.\nF.\nHayes,\nR.\nR˘adulescu,\nE.\nBargiacchi,\nJ. K¨allstr¨om, M. Macfarlane, M. Reymond, T. Ver-\nstraeten, L. M. Zintgraf, R. Dazeley, F. Heintz, et al.,\n“A practical guide to multi-objective reinforcement\nlearning and planning,”\nAutonomous Agents and\nMulti-Agent Systems, vol. 36, p. 26, Apr. 2022.\n[334] R. Salakhutdinov, “Learning Deep Generative Mod-\nels,” Annual Review of Statistics and Its Application,\nvol. 2, pp. 361–385, May 2015.\n[335] Z. Zhu, H. Zhao, H. He, Y. Zhong, S. Zhang, Y. Yu,\nand W. Zhang, “Diffusion models for reinforcement\nlearning: A survey,” arXiv preprint arXiv:2311.01223,\n2023.\n39\n[336] L. Wu, P. Cui, J. Pei, and L. Zhao, eds., Graph Neural\nNetworks: Foundations, Frontiers, and Applications.\nSingapore: Springer Nature Singapore, 2022.\n[337] R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirns-\nberger, M. Fortunato, F. Alet, S. Ravuri, T. Ewalds,\nZ. Eaton-Rosen, W. Hu, et al., “Learning skillful\nmedium-range global weather forecasting,” Science,\nvol. 382, no. 6677, pp. 1416–1421, 2023.\n40\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CY",
    "cs.SY",
    "eess.SY",
    "stat.ML"
  ],
  "published": "2024-07-26",
  "updated": "2024-07-26"
}