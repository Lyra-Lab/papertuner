{
  "id": "http://arxiv.org/abs/2407.19353v2",
  "title": "A spring-block theory of feature learning in deep neural networks",
  "authors": [
    "Cheng Shi",
    "Liming Pan",
    "Ivan Dokmanić"
  ],
  "abstract": "Feature-learning deep nets progressively collapse data to a regular\nlow-dimensional geometry. How this phenomenon emerges from collective action of\nnonlinearity, noise, learning rate, and other choices that shape the dynamics,\nhas eluded first-principles theories built from microscopic neuronal dynamics.\nWe exhibit a noise-nonlinearity phase diagram that identifies regimes where\nshallow or deep layers learn more effectively. We then propose a macroscopic\nmechanical theory that reproduces the diagram, explaining why some DNNs are\nlazy and some active, and linking feature learning across layers to\ngeneralization.",
  "text": "A spring–block theory of feature learning in deep neural networks\nCheng Shi,1 Liming Pan,2 and Ivan Dokmani´c∗,1, 3\n1Departement Mathematik und Informatik, University of Basel, Spiegelgasse 1, 4051 Basel, Switzerland\n2School of Cyber Science and Technology, University of Science and Technology of China, 230026, Hefei, China\n3Department of Electrical and Computer Engineering,\nUniversity of Illinois at Urbana-Champaign, 306 N Wright St, Urbana, IL 61801, USA\n(Dated: October 24, 2024)\nFeature-learning deep nets progressively collapse data to a regular low-dimensional geometry.\nHow this phenomenon emerges from collective action of nonlinearity, noise, learning rate, and other\nchoices that shape the dynamics, has eluded first-principles theories built from microscopic neuronal\ndynamics. We exhibit a noise–nonlinearity phase diagram that identifies regimes where shallow or\ndeep layers learn more effectively. We then propose a macroscopic mechanical theory that reproduces\nthe diagram, explaining why some DNNs are lazy and some active, and linking feature learning\nacross layers to generalization.\nDeep neural networks (DNNs) progressively compute\nfeatures from which the final layer generates predictions.\nSomething remarkable happens when they are optimized\nvia stochastic dynamics over a data-dependent energy:\nEach layer learns to compute better features than the\nprevious one [1], ultimately transforming the data to a\nregular low-dimensional geometry [2–7]. Feature learning\nis a striking departure from kernel machines or random\nfeature models (RFM) which compute linear functions\nof a fixed set of features [8–10]. How it emerges from\nmicroscopic interactions between millions or billions of\nartificial neurons remains a central open question in deep\nlearning [11–15].\nEven with a single hidden layer [16–19], the inter-\nplay between initialization [20], width [21–23], learning\nrate [24, 25], batch size [26, 27], and data [28–30] results\nin a bewildering range of training dynamics.\nDeeper\nnetworks can be analyzed in asymptotic regimes where\nthey resemble kernel machines [31, 32], with simplified\nlayer-wise training [33, 34], and by removing non-linear\nactivations [35, 36]. There have been exciting recent ad-\nvances on deep mean field theory in the infinite-width\nlimit [37, 38] and on effective low-dimensional SGD dy-\nnamics [39, 40]. These are tremendously useful to build\ninsight, but the full deep, non-linear setting has thus\nfar challenged a statistical mechanics approach where\nfeature learning would emerge from microscopic interac-\ntions. There is thus a gap between current theory and\nthe DNNs used in practice and a change of perspective\nmay be useful.\nIn this paper we rather take a thermodynamical, top-\ndown approach and look for a simplest well-understood\nphenomenological model which captures the essential as-\npects of feature learning. We first show that DNNs can\nbe mapped to a phase diagram defined by noise and\n*Contact author: ivan.dokmanic@unibas.ch.\nnonlinearity, with phases where layers learn features at\nequal rates, and where deep or shallow layers learn better\nfeatures. “Better” is quantified through the notion of\ndata separation—the ratio of feature variance within and\nacross classes. To explain this phase diagram, we propose\na macroscopic theory of feature learning in deep, nonlinear\nneural networks: we show that the stochastic dynamics of\na nonlinear spring–block chain with asymmetric friction\nfully reproduce the phenomenology of data separation\nover training epochs and layers. The phase diagram is\nuniversal with respect to the source of stochasticity: vary-\ning dropout rate, batch size, label noise, or learning rate\nall result in the same phenomenology.\nThese findings considerably generalize recent work\nwhich showed that in many DNNs each layer improves\ndata separation by the same factor, with surprising regu-\nlarity [3]. This law of data separation can be proved for\nlinear DNNs with a particular choice of data and initializa-\ntion [41, 42], but it is puzzling why just as many networks\ndo not abide by it. Fig. 4 illustrates the challenge: the\nsame DNN trained with three different parameter sets\nresults in strikingly different distributions of data separa-\ntion over layers. Understanding how these non-uniform\ndistributions arise and how it affects generalization is key\nto understanding feature learning.\nEven linear DNNs induce a complex energy landscape\nand highly nonlinear training dynamics [36] that can\nresult in non-even separation. The relationship between\nthe energy landscape and learning has been studied with\ntools from statistical mechanics [43–46], random matrix\ntheory [47], and optimal transport [17, 48] but it remains\nunclear how to connect these insights to macroscopic\nfeature learning in real DNNs.\nIn our theory, spring elongations model data separa-\ntion. The empirical risk exerts a load on the network to\nwhich its layers respond by separating the data, subject\nto nonlinearity modeled by friction. Difference in length\nbetween consecutive springs gives a load on the incident\narXiv:2407.19353v2  [cond-mat.dis-nn]  23 Oct 2024\n2\n(d)\n(c)\n(b)\n(a)\nFIG. 1. Phase diagrams of DNN load curves (red) for nonlinearity vs. (a) data noise, (b) learning rate, (c) dropout, and (d)\nbatch size. In all cases, noise is strongest on the left, and nonlinearity strongest at the top. Background shading encodes test\naccuracy. Results are averaged over 10 independent runs on MNIST.\nblock. Asymmetric friction models the propagation of sig-\nnal and noise in forward and backward passes. It models\ndynamical nonlinearity in a network which absorbs load\n(or spoils signal in gradient) causing the shallow layers\nto “extend less” (do not learn features). Stochasticity,\nbe it from stochastic gradient descent (SGD) [49, 50],\ndropout [51], or noisy data [52], reequilibrates the load.\nThe resulting model reproduces the dynamics and the\nphase diagram of feature learning surprisingly well. It\nexplains when data separation is uniformly distributed\nacross layers but also when deep or shallow layers learn\nfaster. It shows why depth may be problematic for learn-\ning and why nonlinearity is a double-edged sword, result-\ning in expressive models but facilitating overfitting. A\nstability argument suggests a link between generalization\nand layerwise data separation which we remarkably find\ntrue in real DNNs.\nFeature learning across layers of DNNs—\nA DNN\nwith L hidden layers, weights W ℓ∈Rdℓ×dℓ−1, biases\nbℓ∈Rdℓ, and activation σ maps the input x0 ≡x to the\noutput (a label) y ≡xL+1 via a sequence of intermediate\nrepresentations x0 →x1 →· · · →xL →xL+1 ≡y,\nwhere xL+1 = F(x) = W L+1xL + bL+1 and\nhℓ= W ℓxℓ−1 + bℓ,\nxℓ= cℓσ(hℓ),\n(1)\nfor ℓ= 1, . . . , L. The activation-dependent normalization\nfactors cℓscale the variance of hidden features in each\nlayer close to 1 [53–55]; they can be replaced by batch\nnormalization [56].\nIt is natural to expect that in a well-trained DNN\nthe intermediate features xℓimprove progressively over\nℓ. Following recent work on neural collapse we measure\nseparation as the ratio of variance within and across\nclasses [2–5, 41]; in supplemental material (SM) [57] we\nshow that analogous phenomenology exists in regression.\nLet X k\nℓcollect the ℓth postactivation for examples from\nclass k, with ¯xk\nℓand Nk its mean and cardinality. The\nbetween-class and within-class covariances are\nΣw\nℓ:= Ave\nk\n(CovX k\nℓ),\nΣb\nℓ:= Cov\n\u0000 Nk\nN ¯xk\nℓ\n\u0001K\nk=1 ,\n(2)\nwhere Ave is the average; Σb\nℓis the between-class “signal”\nfor classification, and Σw\nℓis the within-class variability.\nData separation at layer ℓis then defined as\nDℓ:= log\n\u0000Tr (Σw\nℓ) /Tr\n\u0000Σb\nℓ\n\u0001\u0001\n.\n(3)\nThe difference dℓ= Dℓ−1−Dℓrepresents the contribution\nof the ℓth layer. We call the “discrete curve” Dℓvs. ℓ\nthe load curve in anticipation of the mechanical analogy.\nIf indeed each layer improves the representation the load\ncurve should monotonically decrease.\nModern overparameterized DNNs may perfectly fit the\ntraining data for different choices of hyperparameters,\nwhile yielding different load curves. An extreme is an\nRFM where only the last layer learns while the rest are\nfrozen at initialization [9, 58]. As the entire load is con-\ncentrated on one layer, one might intuitively expect that\na more even distribution will result in better performance.\nA law of data separation?—\nHe and Su [3] show that in\nmany well-trained DNNs the load is distributed uniformly\nover layers,\ndℓ≈dℓ′\nfor\n1 ≤ℓ, ℓ′ ≤L,\nwhich results in a linear load curve [59]. This can be\nproved in linear DNNs with orthogonal initialization and\ngradient flow training [41, 42, 60], but as we show below\nit is a brittle phenomenon: nonlinear activations imme-\ndiately break the balance. Equiseparation thus requires\nadditional ingredients; He and Su highlight the impor-\ntance of an appropriate learning rate.\nThe nonlinearity–noise phase diagram—\nWe show that\nDNNs define a family of phase diagrams such that (i)\nincreasing nonlinearity results in increasingly “concave”\nload curves, with deeper layers learning better features\nand thus taking a higher load, dℓ< dℓ′ for ℓ< ℓ′; (ii)\nadding noise to the dynamics rebalances the load across\nlayers; and (iii) further increasing noise results in convex\nload curves, with shallower layers learning better features.\nWe report these findings in Fig. 1. In all panels the ab-\nscissa measures stochasticity while the ordinate measures\nnonlinearity. To measure nonlinearity we use the slope\nα ∈[0, 1] of the negative part of a LeakyReLU activation,\nLReLU(x) := max(αx, x); α = 1 gives a linear DNN,\nα = 0 the ReLU. Consider for example Fig. 1(a) where\n3\n...\n...\n...\nactivation\nfriction \nFIG. 2. An illustration of the analogy between a spring–block\nchain and a deep neural network.\nnoise is introduced in labels and data (we randomly reset\na fraction p of the labels y, and add Gaussian noise with\nvariance p2 to x0). Without noise (upper right corner),\nthe load curve is concave; this resembles an RFM or a\nkernel machine. Increasing noise (right to left) yields a\nlinear and then a convex load curve.\nInterestingly, the same phenomenology results from\nvarying learning rate, dropout, and batch size (Fig. 1(b,\nc, d)). Lower learning rate, smaller dropout, and larger\nbatch size all indicate less stochasticity in dynamics. Mar-\ntin and Mahoney [61] call them temperature-like parame-\nters in the statistical mechanics sense; see also Zhou et\nal. [62]. In all cases high nonlinearity and low noise result\nin “lazy” training [63] and a concave load curve. Low\nnonlinearity and high noise (bottom-left) result in convex\nload curves. We observe the same behavior for other\ndatasets (e.g., CIFAR10) and network architectures (e.g.,\nCNNs); cf. SM [57]. This phenomenology is at the heart\nof feature learning. It asks how we go from non-feature\nlearning models like RFM or NTK (where only or mostly\ndeep layers learn) to models where all layers learn.\nA spring–block theory of feature learning—\nWe now\nshow something surprising: the complete phenomenology\nof feature learning, as seen through layerwise data sepa-\nration, can be reproduced by the stochastic dynamics of\na simple spring–block chain. As in Fig. 2, we interpret dℓ,\nthe signed elongation of the ℓth Hookean spring, as data\nseparation afforded by the ℓth layer. Block movement is\nimpeded by friction which models the effect of dynami-\ncal nonlinearity on gradients; noise in the force models\nstochasticity from mini-batching, dropout, or elsewhere.\nThe equation of motion for the position of the ℓth block,\nxℓ= Pℓ\ni=1 di, ignoring block widths, is\n¨xℓ= k(xℓ+1 −2xℓ+ xℓ−1) −γ ˙xℓ+ fℓ+ εξℓ,\n(4)\nfor ℓ= 1, . . . , L, where we assumed unit masses, k is\nthe elastic modulus, γ a linear damping coefficient suf-\nficiently large to avoid oscillations, and ξℓis noise such\nthat ⟨ξℓ(t)ξℓ(s)⟩= δ(t −s). As DNNs at initialization\ndo not separate data we let xℓ(0) = 0. The dynamics is\ndriven by force applied to the last block F = k(y −xL).\nFIG. 3. Phase diagram of the spring–block system (5) for\nfriction µ→vs. noise level ϵ. We set k = 1, µ←= 0.2 and\nL = 7. The load curves (Dℓ= y −xℓ) are recorded at t = 100;\nthe shading corresponds to the elastic potential energy.\nWe set x0 ≡0 and xL+1 ≡y to model training data and\ntargets. The load curve plots the distance of the ℓth block\nfrom the target Dℓ= y −xℓ. It reflects how much the ℓth\nspring—or the ℓth layer—contributes to “explaining” the\ntotal extension—or the total data separation. One key\ninsight is that the friction must be asymmetric to model\nthe propagation of noise during training as we elaborate\nbelow. We set the sliding and maximum static friction\nto µ→for rightward and µ←for leftward movement. In\nthis model the friction acts on noisy force. If noise is\nadded to the velocity independently of the friction we\nobtain a more standard Langevin dynamics. Although\nthis is physically less realistic it shows similar qualitative\nbehavior (for additional details see SM [57]).\nThe spring–block dynamics of data separation in DNNs—\nWe now show experimentally and analytically that the\nproposed model results in a phenomenology analogous to\nthat of data separation in real networks. Indeed, there\nis a striking similarity between the phase diagram of the\nspring–block model in Fig. 3 and the DNN phase diagrams\nin Fig. 1. Surprisingly, it is not only the equilibria of\nthe two systems that are similar, but also the stochastic\ndynamics; we show this in Fig. 4.\n1: nonlinearity breaks the separation balance.\nLet us\nfirst analyze how our model explains concave load curves\nand lazy training. For simplicity we work in the over-\ndamped approximation γ ≫1; in SM [57] we show that\nthe second order system has the same qualitative behavior.\nScaling time by γ, Eq. (4) then yields\n˙xℓ= σ\n\u0000k (Lx)ℓ+ εξℓ\n\u0001\n(5)\nwhere (Lx)ℓ:= xℓ+1 −2xℓ+ xℓ−1 and\nσ(z) =\n\n\n\n\n\n0\nif −µ←≤z ≤µ→\nz −µ→\nif z > µ→\nz + µ←\nif z < −µ←\n.\n(6)\nWithout noise and friction (ε = 0, µ←= µ→= 0) the\nsystem is linear with the trivial unique equilibrium d∗\nℓ≡\n4\ny/(L + 1) for all ℓ, which corresponds to the state of\nlowest elastic potential energy. However, analyzing the\nresulting system of ODEs shows that adding friction in\n(5) immediately breaks the symmetry and results in an\nunbalanced equilibrium,\nx∗\nℓ=\nyℓ\nL + 1 −µ→\nk\n\u0012Lℓ\n2 −ℓ(ℓ−1)\n2\n\u0013\n(7)\nif we assume the initial elastic potential k(y −x0)2/2 is\nsufficiently large such that all blocks will eventually move.\nIn this case ∆d∗\nℓ:= d∗\nℓ+1 −d∗\nℓ= (Lx∗)ℓ= µ→/k > 0\nand the load curve is concave.\nNote that this result\nonly involves µ→but not µ←as without noise and with\nsufficient damping the blocks only move to the right. The\ninterpretation is that in equilibrium, the friction at each\nblock absorbs some of the load so that the shallower\nsprings extend less; in a DNN this corresponds to lazy\ntraining where the deepest layer, whose gradients do not\nexperience nonlinearity, does most of the separation.\n2: Noise reequilibrates the load.\nIf friction reduces load,\nhow can then a chain with friction—or a nonlinear DNN—\nresult in a uniformly distributed load? We know from\nFig. 1 that in DNNs stochastic training helps achieve this.\nWe now show how our model reproduces this behavior\nand in particular a counterintuitive phenomenon in Fig. 1\nwhere large noise results in convex load curves where\nshallow layers learn better than deep. We show that this\nhappens when µ←> µ→.\nWe begin by defining the effective friction over a time\nwindow of length η. Let ε := ε/√η and ζt ∼(ξt+η −\nξt)/√η (iid). We will assume for convenience that the\nincrements ζ are bounded (see SM [57] for details). The\neffective friction is then\nµeff := Eζ\n\u0002\n(σ(k∆d + εζ) −k∆d) | σ(k∆d + εζ) ̸= 0\n\u0003\n.\nFor large noise we will have that µeff ≈limε→∞µeff =\n1\n2(µ→−µ←). Since this effective friction is approximately\nindependent of x, the load curve can be approximated by\nsubstituting µeff for µ→in Eq. (7), which leads to\n∆d∗≈1\nk lim\nε→∞µeff(ε) = µ→−µ←\n2k\n.\n(8)\nIt is now clear that with sufficient noise the load curve\nis concave if µ→> µ←, linear if µ→= µ←, and convex if\nµ→< µ←. We can also see that for ε = 0, µeff = µ→, so\nthat this effective friction correctly generalizes the noise-\nless case Eq. (7) and we have that ∆d∗∈[ µ→−µ←\n2k\n, µ→\nk ].\nFurther, when ∆d ∈[ µ→−µ←\n2k\n, µ→\nk ], it holds that\ndµeff\ndε\n≤0.\nIt implies that increasing noise always de-\ncreases effective friction which resembles phenomena like\nacoustic lubrication or noise-induced superlubricity [64].\nWhen µ←> µ→> 0, as we vary ε from 0 to ∞we will\nfirst see a concave, then a linear, and finally a convex load\ncurve, corresponding to the transition from lazy to active\n(d)\n(c)\n(a)\n(b)\nFIG. 4. The load curves at convergence (a) and trajectories\n(b, c, d) for an 7−hidden layer ReLU fully connected DNN on\nMNIST (left) and the proposed spring–block model (right).\nFor the DNN (left), the ordinate is Dℓ(data separation at layer\nℓ); the dashed line is the training loss. Characteristic behaviors:\n(b) high nonlinearity (high friction) and low randomness in\ntraining (noise in force); (c): balanced nonlinearity (friction)\nand randomness (force noise); (d) low nonlinearity (friction)\nand large training randomness (force noise). In the spring\nsystem (right), the ordinate represents the distance to the\ntarget Dℓ= y −xℓfor colored lines. These values are scaled\nto match the same regime used in the DNN for visualization\npurposes. The dashed line represents the force at the rightmost\nblock F which is at a different scale from Dℓ.\ntraining in DNNs [63]. As a result, when µ←> µ→> 0\nour model explains the entire DNN phase diagram.\nHow can we relate the condition µ←> µ→> 0 to DNN\nphenomenology? Note that in our model µ←is activated\nonly due to the noise, since the signal (the pulling force)\nis always to the right. In a DNN, the forward pass of the\nbackpropagation algorithm computes the activations while\nthe backward pass computes the gradients and updates\nthe weights. In the forward pass, the input is multiplied\nby the weight matrices starting from the shallowest to\nthe deepest. With noise (e.g. dropout), the activations\n5\nFIG. 5. (a) Load curve of a deep linear neural network on\nrandom orthonormal datasets. (b) Spring–block model without\nleft friction (µ→= 0) with µ←= 0.1, under varying levels of\nnoise ε. (c) An illustration of asymmetric friction.\nof the deepest layer accumulate the largest noise. Thus\nnoise in DNN training chiefly propagates from shallow\nto deep. This is exactly what µ←> µ→in our model\nimplies, since µ←is only triggered by noise.\nIndeed, from Eq. (8) we see that when only friction for\nleftward move is present, µ←> µ→= 0, the load curve\nis convex if ε > 0 and linear if ε = 0 (Fig. 5(b)). This\nequilibrium is independent of µ←since no block moves\nleft. This observation parallels findings in linear DNNs\nwhere training with gradient flow and “whitened” data\nleads to a linear load curve [41]. In contrast, introducing\ndropout or large learning rate makes the load curve convex\n(Fig. 5(a)). This shows that dynamical nonlinearity—or\nfriction in our model—exists even in linear DNNs, so that\ntheir learning dynamics are nonlinear [36].\n3: Equiseparation minimizes elastic potential energy\nand improves generalization—\nWe finally give an exam-\nple of how the proposed theory yields practical insights.\nAmong all spring–block chains under a fixed load, the\nequiseparated one is the most stable in the sense of having\nthe lowest potential energy. This motivates us to look at\nthe test accuracy of DNNs as a function of load curve\ncurvature; this is shown as background shading in Fig. 1.\nThe result is intriguing: Linear load curves correspond to\nthe highest test accuracy. It suggests that by balancing\nnonlinearity with noise, DNNs are at once highly expres-\nsive and not overfitting. That a uniform load distribution\nyields the best performance is intuitively pleasing. It is\nalso valuable operationally as it may help guide training\nto find better networks. We show an example involving\nconvolutional networks in SM [57].\nConclusion—\nDeep learning theories are primarily\nbuilt bottom-up, by analyzing the collective behavior\nof the myriad artificial neurons interacting in a data-\ndriven energy from first principles. But fields like physics,\nbiology, neuroscience, and economics, which have long\ndealt with comparable complexity (or, sometimes, worse)\nhave long benefitted from both bottom-up and top-down,\nphenomenological approaches. We think that deep learn-\ning can similarly benefit from both paradigms. Our study\nattempts to introduce a simple phenomenological model\nto help understand feature learning—an essential aspect\nof modern real-world DNNs which requires us to simul-\ntaneously consider depth, nonlinearity, and noise, some-\nthing that at the moment seems to be out of reach of\nthe bottom-up paradigm. It is a priori not obvious that\nsuch a model exists: it requires identifying a suitable low-\ndimensional description of the system and demonstrating\nits approximately autonomous behavior. Inspired by a\nseries of works on neural collapse and data separation, our\ntheory elucidates the role of nonlinearity and randomness\nand suggests exciting connections with generalization and\nconjectures that are worth pursuing rigorously. Analo-\ngies such as spring-block models have historically played\nimportant roles across science; a prime example is the\nBurridge–Knopoff model in seismology [65] and related\nideas in neuroscience [66]. We find it intriguing that a\nsimple phenomenological model holds significant explana-\ntory power for realistic DNNs, serving as a valuable tool\nfor applying intuitive physical insights to complex, ab-\nstract objects. In closing, we mention that we initially\nconsidered various cascade structures from daily life as\npossible analogies to DNNs. For amusing videos involving\na folding ruler, see the SM [57].\nWe are grateful to Hangfeng He, Weijie Su and Qing Qu\nfor inspiring discussions about the law of data separation\nand training dynamics. Cheng Shi and Ivan Dokmani´c\nwere supported by the European Research Council Start-\ning Grant 852821—SWING. Liming Pan acknowledges\nsupport from National Natural Science Foundation of\nChina (NSFC) under Grant No. 62006122 and 42230406\n[1] M. D. Zeiler and R. Fergus, Visualizing and understand-\ning convolutional networks, in Computer Vision–ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part I 13 (Springer,\n2014) pp. 818–833.\n[2] V. Papyan, X. Han, and D. L. Donoho, Prevalence of\nneural collapse during the terminal phase of deep learning\ntraining, Proceedings of the National Academy of Sciences\n117, 24652 (2020).\n[3] H. He and W. J. Su, A law of data separation in deep\nlearning, Proceedings of the National Academy of Sciences\n120, e2221704120 (2023).\n[4] J. Zarka, F. Guth, and S. Mallat, Separation and concen-\ntration in deep networks, in ICLR 2021-9th International\nConference on Learning Representations (2021).\n[5] A. Rangamani, M. Lindegaard, T. Galanti, and T. A.\nPoggio, Feature learning in deep classifiers through inter-\nmediate neural collapse, in International Conference on\nMachine Learning (PMLR, 2023) pp. 28729–28745.\n[6] V. Kothapalli, T. Tirer, and J. Bruna, A neural collapse\nperspective on feature evolution in graph neural networks,\n6\nAdvances in Neural Information Processing Systems 36\n(2024).\n[7] S. Qin, N. Mudur, and C. Pehlevan, Contrastive similarity\nmatching for supervised learning, Neural computation 33,\n1300 (2021).\n[8] T. Hofmann, B. Sch¨olkopf, and A. J. Smola, Kernel meth-\nods in machine learning, The Annals of Statistics 36, 1171\n(2008).\n[9] A. Rahimi and B. Recht, Random features for large-scale\nkernel machines, Advances in neural information process-\ning systems 20 (2007).\n[10] A. Daniely, R. Frostig, and Y. Singer, Toward deeper\nunderstanding of neural networks: The power of initializa-\ntion and a dual view on expressivity, Advances in neural\ninformation processing systems 29 (2016).\n[11] A. Radhakrishnan,\nD. Beaglehole,\nP. Pandit, and\nM. Belkin, Mechanism for feature learning in neural net-\nworks and backpropagation-free machine learning models,\nScience 383, 1461 (2024).\n[12] Y. Lou, C. E. Mingard, and S. Hayou, Feature learning\nand signal propagation in deep neural networks, in Inter-\nnational Conference on Machine Learning (PMLR, 2022)\npp. 14248–14282.\n[13] A. Baratin, T. George, C. Laurent, R. D. Hjelm, G. Lajoie,\nP. Vincent, and S. Lacoste-Julien, Implicit regularization\nvia neural feature alignment, in International Conference\non Artificial Intelligence and Statistics (PMLR, 2021) pp.\n2269–2277.\n[14] G. Yang and E. J. Hu, Tensor programs IV: Feature\nlearning in infinite-width neural networks, in International\nConference on Machine Learning (PMLR, 2021) pp. 11727–\n11737.\n[15] L. Chizat and P. Netrapalli, Steering deep feature learning\nwith backward aligned feature updates, arXiv preprint\narXiv:2311.18718 (2023).\n[16] S. Mei, A. Montanari, and P.-M. Nguyen, A mean field\nview of the landscape of two-layer neural networks, Pro-\nceedings of the National Academy of Sciences 115, E7665\n(2018).\n[17] L. Chizat and F. Bach, On the global convergence of gradi-\nent descent for over-parameterized models using optimal\ntransport, Advances in neural information processing sys-\ntems 31 (2018).\n[18] Y. Wang, J. Lacotte, and M. Pilanci, The hidden con-\nvex optimization landscape of regularized two-layer relu\nnetworks: an exact characterization of optimal solutions,\nin International Conference on Learning Representations\n(2021).\n[19] L. Arnaboldi, L. Stephan, F. Krzakala, and B. Loureiro,\nFrom high-dimensional & mean-field dynamics to dimen-\nsionless odes: A unifying approach to sgd in two-layers net-\nworks, in The Thirty Sixth Annual Conference on Learning\nTheory (PMLR, 2023) pp. 1199–1227.\n[20] T. Luo, Z.-Q. J. Xu, Z. Ma, and Y. Zhang, Phase diagram\nfor two-layer relu neural networks at infinite-width limit,\nJournal of Machine Learning Research 22, 1 (2021).\n[21] A. Maillard, A. S. Bandeira, D. Belius, I. Dokmani´c, and\nS. Nakajima, Injectivity of relu networks: perspectives\nfrom statistical physics, arXiv preprint arXiv:2302.14112\n(2023).\n[22] R. Pacelli, S. Ariosto, M. Pastore, F. Ginelli, M. Gherardi,\nand P. Rotondo, A statistical mechanics framework for\nbayesian deep neural networks beyond the infinite-width\nlimit, Nature Machine Intelligence 5, 1497 (2023).\n[23] P. Baglioni, R. Pacelli, R. Aiudi, F. Di Renzo, A. Vez-\nzani, R. Burioni, and P. Rotondo, Predictive power of a\nbayesian effective action for fully connected one hidden\nlayer neural networks in the proportional limit, Physical\nReview Letters 133, 027301 (2024).\n[24] H. Cui, L. Pesce, Y. Dandi, F. Krzakala, Y. M. Lu, L. Zde-\nborov´a, and B. Loureiro, Asymptotics of feature learn-\ning in two-layer networks after one gradient-step, arXiv\npreprint arXiv:2402.04980 (2024).\n[25] J. Sohl-Dickstein, The boundary of neural network train-\nability is fractal, arXiv preprint arXiv:2402.06184 (2024).\n[26] R. Marino and F. Ricci-Tersenghi, Phase transitions in\nthe mini-batch size for sparse and dense two-layer neural\nnetworks, Machine Learning: Science and Technology 5,\n015015 (2024).\n[27] Y. Dandi, E. Troiani, L. Arnaboldi, L. Pesce, L. Zde-\nborov´a, and F. Krzakala, The benefits of reusing batches\nfor gradient descent in two-layer networks: Breaking the\ncurse of information and leap exponents, arXiv preprint\narXiv:2402.03220 (2024).\n[28] S. Ciceri, L. Cassani, M. Osella, P. Rotondo, F. Valle, and\nM. Gherardi, Inversion dynamics of class manifolds in\ndeep learning reveals tradeoffs underlying generalization,\nNature Machine Intelligence 6, 40 (2024).\n[29] E. Boursier and N. Flammarion, Early alignment in\ntwo-layer networks training is a two-edged sword, arXiv\npreprint arXiv:2401.10791 (2024).\n[30] S. Goldt, M. M´ezard, F. Krzakala, and L. Zdeborov´a,\nModeling the influence of data structure on learning in\nneural networks: The hidden manifold model, Physical\nReview X 10, 041044 (2020).\n[31] A. Jacot, F. Gabriel, and C. Hongler, Neural tangent ker-\nnel: Convergence and generalization in neural networks,\nAdvances in neural information processing systems 31\n(2018).\n[32] F. Guth, B. M´enard, G. Rochette, and S. Mallat, A\nrainbow in deep network black boxes, arXiv preprint\narXiv:2305.18512 (2023).\n[33] K. H. R. Chan, Y. Yu, C. You, H. Qi, J. Wright, and\nY. Ma, ReduNet: A white-box deep network from the\nprinciple of maximizing rate reduction, Journal of machine\nlearning research 23, 1 (2022).\n[34] C. Fang, H. He, Q. Long, and W. J. Su, Exploring deep\nneural networks via layer-peeled model: Minority col-\nlapse in imbalanced training, Proceedings of the National\nAcademy of Sciences 118, e2103091118 (2021).\n[35] S. Arora, N. Cohen, N. Golowich, and W. Hu, A conver-\ngence analysis of gradient descent for deep linear neural\nnetworks, arXiv preprint arXiv:1810.02281 (2018).\n[36] Q. Li and H. Sompolinsky, Statistical mechanics of deep\nlinear neural networks: The backpropagating kernel renor-\nmalization, Physical Review X 11, 031059 (2021).\n[37] B. Bordelon and C. Pehlevan, Self-consistent dynamical\nfield theory of kernel evolution in wide neural networks,\nAdvances in Neural Information Processing Systems 35,\n32240 (2022).\n[38] B. Bordelon, A. Atanasov, and C. Pehlevan, A dy-\nnamical model of neural scaling laws, arXiv preprint\narXiv:2402.01092 (2024).\n[39] G. Ben Arous, R. Gheissari, and A. Jagannath, High-\ndimensional limit theorems for sgd: Effective dynamics\nand critical scaling, Advances in Neural Information Pro-\ncessing Systems 35, 25349 (2022).\n[40] G. B. Arous, R. Gheissari, J. Huang, and A. Jagan-\n7\nnath, High-dimensional SGD aligns with emerging outlier\neigenspaces, arXiv preprint arXiv:2310.03010 (2023).\n[41] C. Yaras, P. Wang, W. Hu, Z. Zhu, L. Balzano, and Q. Qu,\nThe law of parsimony in gradient descent for learning deep\nlinear networks, arXiv preprint arXiv:2306.01154 (2023).\n[42] P. Wang, X. Li, C. Yaras, Z. Zhu, L. Balzano, W. Hu, and\nQ. Qu, Understanding deep representation learning via\nlayerwise feature compression and discrimination, arXiv\npreprint arXiv:2311.02960 (2023).\n[43] E. Gardner and B. Derrida, Optimal storage properties\nof neural network models, Journal of Physics A: Mathe-\nmatical and general 21, 271 (1988).\n[44] C. Baldassi, C. Lauditi, E. M. Malatesta, G. Perugini, and\nR. Zecchina, Unveiling the structure of wide flat minima\nin neural networks, Physical Review Letters 127, 278301\n(2021).\n[45] S. Becker, Y. Zhang, and A. A. Lee, Geometry of energy\nlandscapes and the optimizability of deep neural networks,\nPhysical review letters 124, 108301 (2020).\n[46] C. Shi, L. Pan, H. Hu, and I. Dokmani´c, Homophily\nmodulates double descent generalization in graph convo-\nlution networks, Proceedings of the National Academy of\nSciences 121, e2309504121 (2024).\n[47] J. Pennington and Y. Bahri, Geometry of neural network\nloss surfaces via random matrix theory, in International\nconference on machine learning (PMLR, 2017) pp. 2798–\n2806.\n[48] X. Fern´andez-Real and A. Figalli, The continuous for-\nmulation of shallow neural networks as wasserstein-type\ngradient flows, in Analysis at Large: Dedicated to the Life\nand Work of Jean Bourgain (Springer, 2022) pp. 29–57.\n[49] N. Yang, C. Tang, and Y. Tu, Stochastic gradient descent\nintroduces an effective landscape-dependent regulariza-\ntion favoring flat solutions, Physical Review Letters 130,\n237101 (2023).\n[50] A. Sclocchi and M. Wyart, On the different regimes of\nstochastic gradient descent, Proceedings of the National\nAcademy of Sciences 121, e2316301121 (2024).\n[51] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov, Dropout: a simple way to prevent\nneural networks from overfitting, The journal of machine\nlearning research 15, 1929 (2014).\n[52] S. Sukhbaatar and R. Fergus, Learning from noisy labels\nwith deep neural networks, arXiv preprint arXiv:1406.2080\n2, 4 (2014).\n[53] K. He, X. Zhang, S. Ren, and J. Sun, Delving deep into rec-\ntifiers: Surpassing human-level performance on imagenet\nclassification, in Proceedings of the IEEE international\nconference on computer vision (2015) pp. 1026–1034.\n[54] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and\nS. Ganguli, Exponential expressivity in deep neural net-\nworks through transient chaos, Advances in neural infor-\nmation processing systems 29 (2016).\n[55] S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-\nDickstein, Deep information propagation, arXiv preprint\narXiv:1611.01232 (2016).\n[56] S. Ioffe and C. Szegedy, Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift,\nin International conference on machine learning (pmlr,\n2015) pp. 448–456.\n[57] See Supplemental Material (SM), which includes (1): A\nfolding ruler experiment, (2) More experiment results on\nvarious networks and datasets, (3) Details of numerical\nexperiments and reproducibility, (4) Regression and (5)\nAdditional details about the spring block systems..\n[58] H. Hu and Y. M. Lu, Universality laws for high-\ndimensional learning with random features, IEEE Trans-\nactions on Information Theory 69, 1932 (2022).\n[59] He and Su [3] and some other previous works [2,\n4] define the data separation slightly different as\nˆDℓ:= log\n\u0010\nTr\n\u0010\nΣw\nℓΣb\nℓ\n+\u0011\u0011\n, where Σ+ denotes the Moore-\nPenrose inverse of Σ. As mentioned in [42], Eq. (3) can\nbe regarded as a simplified version of the above definition.\nSee the SM for the comparison between them. .\n[60] S. Arora, N. Cohen, and E. Hazan, On the optimization\nof deep networks: Implicit acceleration by overparameter-\nization, in International conference on machine learning\n(PMLR, 2018) pp. 244–253.\n[61] C. H. Martin and M. W. Mahoney, Rethinking general-\nization requires revisiting old ideas: Statistical mechanics\napproaches and complex learning behavior, arXiv preprint\narXiv:1710.09553 (2017).\n[62] Y. Zhou, T. Pang, K. Liu, M. W. Mahoney, Y. Yang, et al.,\nTemperature balancing, layer-wise weight analysis, and\nneural network training, Advances in Neural Information\nProcessing Systems 36 (2024).\n[63] L. Chizat, E. Oyallon, and F. Bach, On lazy training in dif-\nferentiable programming, Advances in neural information\nprocessing systems 32 (2019).\n[64] S. Shi, D. Guo, and J. Luo, Micro/atomic-scale vibration\ninduced superlubricity, Friction 9, 1163 (2021).\n[65] R. Burridge and L. Knopoff, Model and theoretical seis-\nmicity, Bulletin of the seismological society of america 57,\n341 (1967).\n[66] J. J. Hopfield, Neurons, dynamics and computation,\nPhysics today 47, 40 (1994).\n[67] D. P. Kingma and J. Ba, Adam: A method for stochastic\noptimization, arXiv preprint arXiv:1412.6980 (2014).\n[68] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani, Least\nangle regression, The Annals of Statistics 32, 407 (2004).\nA spring–block theory of feature learning in deep neural networks\nSUPPLEMENTAL MATERIAL\nCheng Shi,1 Liming Pan,2 and Ivan Dokmani´c∗,1, 3\n1Departement Mathematik und Informatik, University of Basel, Spiegelgasse 1, 4051 Basel, Switzerland\n2School of Cyber Science and Technology, University of Science and Technology of China, 230026, Hefei, China\n3Department of Electrical and Computer Engineering,\nUniversity of Illinois at Urbana-Champaign, 306 N Wright St, Urbana, IL 61801, USA\n(Dated: October 24, 2024)\nS1. A FOLDING RULER EXPERIMENT\nWe first describe an experiment where a different cas-\ncaded mechanical system, a folding ruler, exhibits a phe-\nnomenology which is in some ways reminiscent of feature\nlearning dynamics in DNNs. We emphasize that this\nreminiscence is anecdotal but nonetheless mention it since\nit motivated our work. The goal of the experiment is to\nshow that noise can renegotiate the imbalance caused by\nfriction. As shown in Fig. S1, we pin the left end of the\nfolding ruler and pull the right end by hand. Due to fric-\ntion, if we pull it very slowly and steadily, the outer layer\nextends far while the inner layers are close to stuck. This\nreminds us of lazy training where the outer layers take\nthe largest proportion of the load. Conversely, shaking\nwhile pullings helps “activate” the inner layers and redis-\ntribute the force, and ultimately results in a uniformly\ndistributed extension of each layer. Videos can be found\nat https://github.com/DaDaCheng/DNN_Spring\nFIG. S1. Two ways to extend a folding ruler: (a) With slow\nand steady pulling, the outer layer extends more than the\ninner layer due to friction. (b) With quick and jerky pulling,\nthe “active” dynamics redistribute the extension across the\ndifferent layers.\nS2. ADDITIONAL EXPERIMENTAL RESULTS\nON VARIOUS NETWORKS AND DATASETS\nConvolutional neural networks—\nSimilarly to MLPs,\ndeep convolution neural networks (CNNs) also learn fea-\ntures and progressively collapse data. Here we explore\nhow the observation that a uniform load distribution cor-\nrelates with better generalization may help guide CNN\nFIG. S2. The dynamics of load curves for a deep CNN. (a)\ntest accuracy versus training loss. (b) the corresponding load\ncurves during training. In the experiments, we introduce 5%\nat epoch 20 × 200 and 30% dropout at epoch 40 × 200.\ntraining. In Fig. S2, we illustrate an experiment where\nwe first train a CNN using Adam [67] at a learning rate\nof 10−4 on MNIST. The training nearly converges after\n20×200 epochs, resulting in a concave load curve (purple).\nNext, we introduce a 5% dropout which causes the train-\ning to resume and the load curve to become linear (blue).\nImportantly, it also improves accuracy. Stronger noise\n(a 30% dropout at 40 × 200 epochs) at once results in a\nconvex load curve and worse generalization. In Fig. S3,\nwe illustrate the in-class means of the ℓ-th mid-feature\n(¯xk\nℓ) at epochs 4000, 8000, and 12000. The corresponding\nload curves at these times are concave, linear, and convex,\nrespectively, as shown in Fig. S2.\nTraining vs. test load curves—\nThis work primarily\ninvestigates the concavity and convexity of the training\nload curve. Notably, the test load curve demonstrates\nbehavior similar to that of the training load curve. In\nFig. S4 we show the load curves for both the training\nsamples (solid line) and test samples (dashed line) on the\nMNIST dataset. We can see that with a small learning\nrate both both curves are concave. Increasing the learning\nrate makes them both linear; increasing it further makes\nthem convex. The most linear curve corresponds to the\nhighest test accuracy.\nCorrelation between load linearity and generalization—\nThe phenomenon that the highest accuracy occurs when\nthe load curve is linear is universally observed across a\nwide range of parameters and datasets. It occurs, for\nexample, for different depths in Fig. S5 and different\nwidths in Fig. S6.\nIt also occurs with other datasets\nand optimizers. We experiment with the FashionMNIST\ndataset in Fig. S7 with the same default setting in (a)\n9\nFIG. S3. In-class means of the mid layer features in deep\nCNN whose training dynamics is showed in Fig. S2. We plot\nfirst channel at each layer for digits 0, 4, and 9 at epochs\n4000 (concave load curve), 8000 (linear load curve), and 12000\n(convex load curve).\nFIG. S4. DNN load curves for the training set (solid line) and\ntest set (dashed line) under different learning rates (lr). The\ncorresponding test accuracies are 91.93%, 92.52%, and 89.99%\nfor learning rates of 0.0001, 0.001, and 0.01, respectively. The\nDNN in this experiment has a width of 784. These results were\nobtained from a single instance using α = 0.2 for LeakyReLU\nand were not averaged across multiple trials.\nand a different optimizer in (b); further experiments on\nCIFAR10 are shown in Fig. S8. The original color images\nare first resampled to a size of 3 × 10 × 10 for computa-\ntional reasons and then vectorized as input to an MLP.\n(b)\n(a)\nFIG. S5. Phase diagram of the DNN load curves on the MNIST\ndataset for nonlinearity vs. learning rate at different depths:\n(a) 6-layers DNN, (b) 12-layers DNN.\n(b)\n(a)\nFIG. S6.\nPhase diagram of the DNN load curves on the\nMNIST dataset for nonlinearity vs. batch size at different\nwidths: (a) DNN with a width of 400, trained with the learning\nrate of 0.0005, (b) DNN with a width of 2500, trained with\nthe learning rate of 0.0001.\n(b)\n(a)\nFIG. S7.\nPhase diagram of the DNN load curves on the\nFashionMNIST dataset for nonlinearity vs. data noise. (a)\nuses the same settings as Fig. 1. (b) shows the results for\na 6-layer DNN trained with the SGD optimizer (instead of\nAdam) using a learning rate of 0.001. For SGD, all cases are\ntrained for 1000 epochs. Linear DNNs (the bottom line) do\nnot converge well with the SGD optimizer.\nWe note that D0 (the data separation metric in the input\nlayer) slightly deviates from linearity, likely due to the\nadditional complexity of CIFAR10, but the subsequent\nlayers closely follow the regular phenomenology discussed\nin the main text (Fig. S8(b)).\nThe noise vs. nonlin-\nearity phase diagram remains valid even when the data\nbecomes more complex, as does the correlation between\ngeneralization and linearity of separation across layers\nwhen noise is introduced via dropout. A small deviation\nfrom this occurs when dropout is replaced by learning\nrate on CIFAR10: in this case the best generalization cor-\nresponds to slightly concave load curves (Fig. S9). Since\n10\n(b)\n(a)\nFIG. S8.\nPhase diagram of the DNN load curves on the\nCIFAR10 dataset for nonlinearity vs. dropout. The red curves\nin (a) show Dℓvs. ℓfor ℓ= 0, 1, · · · , 7, while (b) presents the\nsame data as in (a) but plots ℓstarting from 1 without input\ndata (0, D0).\nlearning rate is qualitatively very different from standard\n“noise” we expect that at some point it exhibits a refined\nphenomenology; this particular example is an intriguing\navenue for further exploration.\n(b)\n(a)\nFIG. S9.\nPhase diagram of the DNN load curves on the\nCIFAR10 dataset for nonlinearity vs. learning rate at different\nlearning rate: (a) 12-layers DNN, (b) 20–layers DNN. The red\ncurves plot Dℓstaring from 1 without input data (0, D0).\nS3. DETAILS OF NUMERICAL EXPERIMENTS\nAND REPRODUCIBILITY\nIn Figs. 1, 4, S4 S5, S6, S7, S8, S9, S10 and S12, the\nnetworks are 8-layer fully connected MLPs (7 hidden lay-\ners), with layer width equal to 100 unless stated otherwise.\nWe use ReLU activations, BatchNorm in each layer, and\nno dropout as the default setting. All parameters are\ninitialized using the default settings in PyTorch.\nUn-\nless otherwise specified, the networks are trained using\nthe ADAM optimizer on 2560 training samples from the\nMNIST dataset, with cross-entropy loss for classification\ntasks. The learning rate is set to 0.001 and the batch\nsize is 2560 unless otherwise stated. The test accuracy is\nmeasured on the entire 10,000-sample test dataset after\ntraining 100 epochs. The results in all phase diagrams are\naveraged over 10 independent runs. Notably, when we re-\nplace BatchNorm with the scaling constant cℓmentioned\nin the main text, we still observe similar convex–concave\npatterns in the phase diagram corresponding to noise and\nnonlinearity. However, without the adaptive BatchNorm,\nthe load curves exhibit more fluctuations and are less\nsmooth, and the variations between independent runs\nbecome more pronounced.\nIn the left column of Fig. 4, we train the described DNN\nwith learning rate of 0.0001 and batch size of 200 in panel\n(b); learning rate of 0.001, dropout of 0.1 and batch size of\n100 in panel (c); learning rate of 0.003, and batch size of 50,\nand dropout of 0.2 in panel (d). In the right column we set\nk = 1, y = 1 for all three cases, and µ→= 0.11, µ←= 0.03\nand ε = 0.006 for (b); µ→= µ←= 0.04 and ε = 0.01 for\n(c); µ→= 0.04, µ←= 0.12 and ε = 0.011 for (d). These\nparameters are chosen to produce qualitatively similar\ncurves with the previous three characteristic training\ndynamics. For the spring experiments in Fig. 4, we use\nthe same noise for all blocks ξℓ(t) = ξℓ′(t), to mimic\nthe training of DNNs in which the randomness (e.g.,\ndata noise, learning rate, and batch size) in each layer\nare not independent. This synchronous noise results in\nmore similar dynamics over epochs (in particular the\nfluctuations) but ultimately leads to similar load curves\nas independent noise as shown in Fig. 2.\nIn Fig. 5(a), we adopt the setting from [41]. We use\nSGD (instead of ADAM) with learning rate of 0.001 to\ntrain a linear DNN. All weight matrices are initialized as\nrandom orthogonal matrices; the data is also a random\northogonal matrix with random binary labels. There is\nno batch normalization. We set the learning rate to 0.01\nto generate the “large stepsize” result (blue curve) and\napply 10% dropout to generate the “dropout” results\n(green curve).\nIn Fig. S2, we consider a CNN with 16 channels and 7\nconvolutional layers on the same MNIST dataset. We use\nReLU activations and BatchNorm between the convolu-\ntional layers. Pooling and upsampling are applied after\neach activation in such a way that each intermediate layer\nhas the same shape, and a linear layer is applied after the\nfinal convolutional layer.\nAll experiments are reproducible using code at https:\n//github.com/DaDaCheng/DNN_Spring.\nS4. SOME OTHER METRICS FOR DATA\nSEPARATION OF INTERMEDIATE FEATURES\nIn the main text, we use Eq. (3) to quantify the ratio\nbetween the “signal” and the “noise”. Some works use a\ndifferent but related quantity [2–4],\nˆDℓ:= log\n\u0010\nTr\n\u0010\nΣw\nℓΣb\nℓ\n+\u0011\u0011\n,\n(9)\nwhere Σ+ denotes the Moore–Penrose pseudoinverse of Σ.\nIn general, for our purposes, these two metrics produce\ncomparable results; we show this in Fig. S10. The behav-\nior of the two metrics notably diverges for linear networks\n11\nwhere ˆDℓcannot capture the differences in features across\ndifferent layers (Fig. S11 (c) and (d)).\nS5. REGRESSION\nThe phenomenology described in the main text for\nclassification can be observed for regression problems if\nwe define the load as the MSE (or RMSE) of the optimal\nlinear regressor from the ℓth layer features,\nDℓ:=\nv\nu\nu\ntmin\nw,b\n1\n2N\nX\n(xℓ,y)\n∥y −w⊤xℓ+ b∥2\n2,\nFIG. S10.\nPhase diagram of the DNN load curve on the\nMNIST for nonlinearity vs. dropout with the data separation\nas ˆDℓin Eq. (9).\n(a)\n(c)\n(b)\n(d)\nFIG. S11. Comparison of data separation metrics Dℓ(Eq. (3))\nin (a, c) and ˆDℓ(Eq. (9)) in (b, d). The top two plots (a,\nb) show results for the ReLU DNN, while the bottom two\nplots (c, d) show results for the linear DNN. The curves in all\nfour plots represent the same DNNs trained on MNIST with\ndifferent dropout (dp) rates, as indicated in the legend in (c).\nFIG. S12. Regression load curve on Diabetes dataset [68] for\na 7 hidden-layer ReLU network. (i) learning rate of 0.001\nwithout dropout. (ii) learning rate of 0.01 without dropout\n(iii) learning rate of 0.02 and with 30% dropout.\nwhere N is the number of data pairs and the summation is\ntaken over all data. As shown in Fig. S12, noisier training\nresults in a convex load curve, whereas lazy training\nresults in a concave curve.\nS6. ADDITIONAL DETAILS ABOUT THE\nSPRING BLOCK SYSTEMS\nFriction in the second order system\nTo build intuition about the second order system dy-\nnamics (4), we can rewrite it as\nFℓ= k(xℓ+1 −2xℓ+ xℓ−1) −γvℓ+ εξℓ,\n¨xℓ= Fℓ+ fℓ= σ(Fℓ; ˙xℓ)\n(10)\nwhere the friction fℓdepends on the force Fℓand the\nvelocity ˙xℓ. If a block is moving to the right ( ˙xℓ> 0),\nsliding friction resists its movement as fℓ= −µ→. If a\nblock is moving to the left ( ˙xℓ< 0), sliding friction is\nfℓ= µ←. When a block is stationary ( ˙x = 0), static\nfriction compensates for all other forces as long as they\ndo not exceed the maximum static friction, i.e., fℓ= −Fℓ\nwhen −µ←≤Fℓ≤µ→. We take the maximum static\nfriction to be equal to the sliding friction to simplify\nthe model. We can summarize the above cases in an\nactivation-like form,\nσ(z; ˙x) =\n\n\n\n\n\n0\nif ˙x = 0 and −µ←≤z ≤µ→\nz −µ→\nif ˙x > 0 or ( ˙x = 0 and z > µ→)\nz + µ←\nif ˙x < 0 or ( ˙x = 0 and z < −µ←).\n(11)\nThe phase diagram of this second order system is shown\nin Fig. S13.\n12\nFIG. S13. Phase diagram of the second order spring block\nsystem (4) for friction µ→vs. noise level ϵ. We set k = γ =\n1, µ←= 0.12 and L = 7. Red load curves (Dℓ= y −xℓ) are\nrecorded at t = 100; the shading corresponds to the elastic\npotential energy.\nNoisy equilibiria and separation of time scales\nIn the main text, we mentioned that it is convenient to\nassume bounded (zero-mean, symmetric) noise increments,\nfor example by using a truncated Gaussian distribution.\nWithout truncation, the trajectories of the spring–block\nsystem exhibit two stages. In the first stage, the elastic\nforce dominates the Gaussian tails and the block motion\nis primarily driven by the (noisy) spring force. At the end\nof this period, the spring force is balanced with friction.\nAt this point the blocks can only move due to a large\nrealization of noise. These low-probability realizations\nwill move the blocks very slowly close to the equilibrium\n∆d∗=\nµ→−µ→\n2k\nwhich is stable under symmetric noise\nperturbations. This is undesirable for analysis since this\nstable point does not depend on the noise level; in particu-\nlar, it will be eventually reached even for arbitrarily small\nε. This, however, will happen in an exponentially long\ntime, longer than ec0/ϵ2 for some constant c0. We can\nobviate this nuisance in three natural ways: by assuming\nbounded noise incremenents, by applying noise decay, or\nby introducing a stopping criterion, for example via a\nrelative change threshold. All are well-rooted in DNN\npractice: common noise sources are all bounded, and\nstandard training practices involve a variety of parameter\nscheduling and stopping criteria.\nSecond order Langevin equation\nWe can obtain a more standard Langevin dynamics\nformulation of Eq. (10) by adding noise to the velocity\nindependently of the friction, i.e., computing the friction\nbefore adding noise. We note that this is less realistic\nfrom a physical point of view. The position of the ℓth\nblock xℓ= Pℓ\ni=1 di then obeys the equation of motion\ndx = vdt with\ndvℓ= (k(Lx)ℓ−f(vℓ) −γvℓ) dt + ε(t)dWℓ(t),\n(12)\nwhere the sliding friction f resists movement as\nf(v) =\n(\nµ→\nif v > 0\n−µ←\nif v < 0.\n(13)\nIn Eq. (12), Wℓ(t) represents the Wiener process with ε\ncontrolling the amount of the noise (temperature parame-\nters). To ensure convergence in this formulation we have\nto decay the noise; we set ε(t) = ε0e−τt. The friction at\nzero speed f(0) is defined similarly to the static friction\nfℓin Eq. (10), but with Fℓ= k(Lx)ℓwithout considering\nnoise. This dynamics can be solved by standard SDE\nintegration. It results in a similar phase diagram at con-\nvergence, as shown in Fig. S14, although the fluctuations\ndo not appear as similar as with the formulation in the\nmain text.\nFIG. S14. Phase diagram of spring block system with Langevin\nequation (12) for friction µ→vs. noise level ϵ0. We set k =\n0.1, γ = 1, τ = 0.002, µ←= 0.016 and L = 7. Red load curves\n(Dℓ= y −xℓ) are recorded at t = 300; the shading corresponds\nto the elastic potential energy.\n",
  "categories": [
    "cond-mat.dis-nn",
    "cond-mat.stat-mech",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2024-07-28",
  "updated": "2024-10-23"
}