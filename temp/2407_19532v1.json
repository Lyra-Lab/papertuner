{
  "id": "http://arxiv.org/abs/2407.19532v1",
  "title": "The Interpretability of Codebooks in Model-Based Reinforcement Learning is Limited",
  "authors": [
    "Kenneth Eaton",
    "Jonathan Balloch",
    "Julia Kim",
    "Mark Riedl"
  ],
  "abstract": "Interpretability of deep reinforcement learning systems could assist\noperators with understanding how they interact with their environment. Vector\nquantization methods -- also called codebook methods -- discretize a neural\nnetwork's latent space that is often suggested to yield emergent\ninterpretability. We investigate whether vector quantization in fact provides\ninterpretability in model-based reinforcement learning. Our experiments,\nconducted in the reinforcement learning environment Crafter, show that the\ncodes of vector quantization models are inconsistent, have no guarantee of\nuniqueness, and have a limited impact on concept disentanglement, all of which\nare necessary traits for interpretability. We share insights on why vector\nquantization may be fundamentally insufficient for model interpretability.",
  "text": "Published as a conference paper at RLC 2024\nThe Interpretability of Codebooks in Model-Based\nReinforcement Learning is Limited\nKenneth Eaton1, Jonathan Balloch2, Julia Kim2, Mark Riedl2\n1Georgia Tech Research Institute, 2Georgia Institute of Technology\nAbstract\nInterpretability of deep reinforcement learning systems could assist operators with\nunderstanding how they interact with their environment.\nVector quantization\nmethods—also called codebook methods—discretize a neural network’s latent space\nthat is often suggested to yield emergent interpretability. We investigate whether\nvector quantization in fact provides interpretability in model-based reinforcement\nlearning. Our experiments, conducted in the reinforcement learning environment\nCrafter, show that the codes of vector quantization models are inconsistent, have no\nguarantee of uniqueness, and have a limited impact on concept disentanglement, all\nof which are necessary traits for interpretability. We share insights on why vector\nquantization may be fundamentally insufficient for model interpretability.\n1\nIntroduction\nAlthough deep neural networks have advanced the performance of reinforcement learning (RL)\nagents, these “black box” models give little insight as to the agent’s decision making and learned\ntransition models. Interpretability of RL agents is important in high-stakes domains that demand\nhuman trust, such as autonomous vehicles and infrastructure.\nInterpretability is also crucial to\nanalyze behavior and develop adaptations when trained agents make mistakes or experience novel\nchanges to their environment.\nThere has been significant recent interest in vector quantization (VQ) in neural networks, largely\ndriven by the work of Van Den Oord et al. (2017); the authors show that VQ latent space dis-\ncretization in generative models helps regularize the latent space, thereby improving performance.\nAlthough not claimed by Van Den Oord et al. (2017), several subsequent works suggest that the\nlatent disentanglement caused by VQ enables greater semantic interpretability of neural models (Hsu\net al., 2017; Tamkin et al., 2023; Aloufi et al., 2020).\nWe examine whether VQ within model-based reinforcement learning (MBRL) captures semantic\ninformation about entities in the environment. Specifically, we use the IRIS (Micheli et al., 2022)\nMBRL agent, which quantizes the latent vector encoding of the world state and uses a transformer\nto model transition dynamics. Grad-CAM (Selvaraju et al., 2017) is used to perform qualitative\nand quantitative analysis to determine whether codes are consistent and represent semantically\ngroundable entities, such as objects, across diverse inputs.\nWe find VQ lacks the necessary constraints to enforce that disentangled codes correspond to semantic\nentities; codes are not consistent enough to be grounded to a semantic concept. Based on our findings,\nwe disprove the intuition that VQ alone is sufficient for interpretability and hypothesize that latent\nsemantic alignment is needed alongside discretization to make latent spaces generally interpretable.\n2\nBackground and Related Work\nIn MBRL, the agent learns a model to approximate the transition dynamics of the environment (Sut-\nton & Barto, 1998).\nAn agent can benefit from having a good predictive model of the future\narXiv:2407.19532v1  [cs.AI]  28 Jul 2024\nPublished as a conference paper at RLC 2024\nFigure 1: IRIS transition model architecture and our evaluation process. Heatmaps from Grad-CAM\ncrop the input image to focus regions, which are then embedded by a pre-trained encoder.\nstate (Werbos, 1987), commonly dubbed a World Model (Ha & Schmidhuber, 2018; Hafner et al.,\n2020). IRIS (Micheli et al., 2022) is a MBRL agent that uses VQ to replace the latent state vectors of\nan encoded observation with the most similar codes, which are input to a decoder and a transformer\nlearning the World Model. Since the codebook sits between the encoder and decoder, individual\ncodes have the potential to represent grounded entities (Khazatsky et al., 2021).\nVQ is a data compression technique that models a latent distribution as a discrete set of vectors\nor “codes.” Formally, a VQ function Q can be defined as: Q : Rd →C, where the “codebook”\nC = {ci|i ∈I, ci ∈Rd} is composed of k vectors referred to as codes. For a given input x, Q assigns\na code ci that minimizes the MSE between the vectors. In deep neural networks, VQ is most often\nused in the latent space of generative models, replacing the encoder output with learned codes (Van\nDen Oord et al., 2017; Esser et al., 2021; Ramesh et al., 2021). VQ is also used in the latent space\nof RL models to constrain and regularize them, which improves accuracy, robustness to noise, and\nsample efficiency (Zhang et al., 2022; Robine et al., 2020).\nSeveral works have examined and suggested the emergent interpretability of VQ methods. Chen\net al. (2022); Zhang et al. (2022) demonstrate that the entire latent embedding created from a com-\nbination of codes is capable of capturing consistent states. There are also works that suggest the\ninterpretability of some specific codes using qualitative assessments such as latent traversals (Aloufi\net al., 2020; Shu et al., 2020; Desticourt et al., 2022; Zou et al., 2023; Tamkin et al., 2023; Wallingford\net al., 2023; Shao et al., 2023; Hsu et al., 2024). Despite evidence from works such as Locatello et al.\n(2019), the suggestion of VQ interpretability comes from an unsubstantiated connection between\nlatent disentanglement and latent semantics (Guo et al., 2020; Khazatsky et al., 2021) or inter-\npretability (Hsu et al., 2017; Liu et al., 2022; Zou et al., 2022; Klein et al., 2022). These methods\nrarely quantify the interpretability provided by VQ codes, and we found no prior work that studies\nVQ interpretability in MBRL.\nThis work is similarly motivated to the parallel research area of mechanistic interpretability Nanda\n(2022); Elhage et al. (2022) (MI), which studies interpreting neural network behavior based on\nan interpretation of the network’s internal structures Olah et al. (2020). This stands in contrast\nto “black-box” Molnar (2022) interpretability approaches, such as saliency maps of inputs from\noutputs Adebayo et al. (2018); Jain & Wallace (2019); Wiegreffe & Pinter (2019), which attempt\nto explain relationships between the inputs and outputs without considering the network’s internal\nmechanisms. Interpreting neural networks using codebooks falls in between these methods (in what\nis sometimes called “white-box” Molnar (2022); Räuker et al. (2023) interpretability), where (as in\nMI) the internal structures of the neural networks are constrained or investigated to establish what\nactivations of the network are representing, while using tools like saliency maps to convey feature\nimportance. By taking a white-box approach to interpreting codes while using MI concepts such as\nsuperposition Elhage et al. (2022), our analysis balances the theoretical robustness of MI with the\nability to interpret almost any network without the need to understand all its internal structures.\nPublished as a conference paper at RLC 2024\nTable 1: Example heatmaps for codes.\nCode 304\nCode 434\nCode 381\n3\nExperiments in VQ Interpretability\nExperimental Approach.\nIn our work, we use the state-of-the-art MBRL model IRIS (Micheli\net al., 2022) featuring a codebook with 512 codes. We make no modifications to the IRIS architecture.\nTo evaluate interpretability, we apply Grad-CAM to the code inputs of the decoder. Grad-CAM\nscores importance of input image pixels based on the activations and gradients of a selected weight\nvector or layer. We apply this process individually for each code in the model by masking out the\nother codes’ values. The end result is a heatmap for each code that overlays the input image with\nthe regions that were most influential in that code being selected.\nTo measure the quantitative properties of the learned codes, we evaluate the code consistency over\nagent observations and episodes by looking at the areas of high Grad-CAM activation for the same\ncode. We split images into connected components of high activation and filter out those with an\narea smaller than a threshold. The remaining connected components are used to crop the original\nimages to the bounding box region they encapsulate. We use a frozen pre-trained model to compare\nthe cropped inputs and gauge their semantic similarity.\nWe conducted our experiments in Crafter (Hafner, 2021), an open-world survival game designed for\nRL with similar dynamics to Minecraft. The model and corresponding policy was first trained to\nconvergence. Then we collected a dataset of 127,434 (ot, st, at, st+1, ot+1) transitions of the agent\nacting in the environment over 714 episodes, where o is the input observation, a is the selected\naction, and s are the latent states. We applied our interpretation approach to investigate different\ncodes correspondance to the inputs on newly collected runs, and saved the Grad-CAM activation\nheatmaps for every code the model selected in each observation.\nWe found that 90% of the heatmaps contained all zero values. Since all the values being zero is\nnot useful for our interpretability evaluation, those samples were dropped. 477 of the 512 codes had\nnon-zero heatmaps, equating to 205,231 heatmaps for the final dataset.\nResults.\nTable 1 displays eight heatmaps overlaid on their observation image, randomly sampled\nfor each code, from the most frequently occurring code in our dataset, the tenth most frequently\noccurring, and the median occurring. We chose these codes based on their frequency to avoid biasing\nthe results and capture diverse regions of the frequency distribution, as discussed further in Appendix\nA. In general, the heatmaps from a code focus on the same regions of the image, especially for the\nmore frequently occurring codes. To examine the content the codes focus on, in Table 2 we show the\ncrops resulting from the heatmaps. To display results for more codes while still focusing on diverse\nfrequencies, we show crops for the second most frequent code, eleventh most frequent, and the next\nmost frequent code after the median code used earlier. In the crops, code 46 is consistently focused\non numbers indicating agent inventory, while the other two have several similar crops but no clear,\nsingular focus. Therefore, we conclude codes did not consistently associate with objects or concepts.\nAdditional examples of crops are presented in Appendix B.\nTo quantify code interpretability, we compared the embeddings for each crop from the last hidden\nlayer of a pre-trained, frozen ResNet50 (He et al., 2016) model trained on ImageNet-1k (Russakovsky\net al., 2015). Given these crop embeddings, we calculated the mean embedding of each code and\nPublished as a conference paper at RLC 2024\nTable 2: Example crops from codes.\nCode 46\nCode 162\nCode 498\n(a) Mean cosine similarity of cropped images\n(b) t-SNE of codes with high cosine similarity\nFigure 2: The data from our GradCAM experiments show that VQ codes are inconsistent quantita-\ntively (low mean cosine similarity) and qualitatively (in the lack of t-SNE separation).\nmeasured code consistency as the average cosine similarity between the mean code embedding and\nall embeddings for that code.\nIn Figure 2a we show the mean cosine similarity results.\nThe similarity rises gradually with a\nspike for the best codes. As a baseline, we calculate the mean cosine similarity of randomly sampled\ncrops, averaged over ten trials, shown as the red line in the plot. The minimal difference between the\nrandom crops and the majority of individual codes suggests at best there are a handful of consistent\ncodes. In Figure 2b, this is examined further with a t-SNE plot of the embeddings for the ten codes\nwith the highest mean cosine similarity and at least 500 embeddings, to ensure similar order of\nmagnitude of samples. We observe only a handful of these codes with strong clustering. In addition,\nsome codes have multiple clusters, which makes a code’s semantic meaning less clear.\n4\nDiscussion\nAs reported above, the model learned a limited number of codes that capture entities such as\ngrass, water, and resources in the inventory. In Appendix C, we examine co-occurring codes to\nfind additional interpretable examples occurring in superposition Elhage et al. (2022), however,\nthey occur very sparingly. Outside of these instances, the codes lacked a consistent semantic theme.\nThus, VQ alone cannot provide interpretability of transition models in MBRL. One of the reasons we\nbelieve this happened is that VQ offers insufficient constraints to enforce semantic disentanglement of\ncodes. Specifically, the model is always trained to reconstruct the entire image using a combination\nof codes, and due to the depth of the CNN in our model, the latent vectors have all seen the entire\nimage. This creates a condition where there is no need for the model to cleanly isolate entities in\nthe image to a single code. Thus, indicating there are characteristics of discretization that matter\nto ensure semantic associations, a deeper study of which is left for future work.\nPublished as a conference paper at RLC 2024\nReferences\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.\nSanity checks for saliency maps. Advances in neural information processing systems, 31, 2018.\nRanya Aloufi, Hamed Haddadi, and David Boyle. Privacy-preserving voice analysis via disentan-\ngled representations. In Proceedings of the 2020 ACM SIGSAC Conference on Cloud Computing\nSecurity Workshop, pp. 1–14, 2020.\nYu-Jie Chen, Shin-I Cheng, Wei-Chen Chiu, Hung-Yu Tseng, and Hsin-Ying Lee. Vector quantized\nimage-to-image translation. In European Conference on Computer Vision, pp. 440–456. Springer,\n2022.\nEtienne Desticourt, Véronique Letort, and Florence D’Alché-Buc. Interpretable generative modeling\nusing a hierarchical topological vae. In 2022 International Conference on Computational Science\nand Computational Intelligence (CSCI), pp. 1415–1421, 2022.\ndoi: 10.1109/CSCI58124.2022.\n00253.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish,\nJared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of super-\nposition. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/\ntoy_model/index.html.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npp. 12873–12883, 2021.\nDaya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang, and Ming Zhou.\nEvidence-aware\ninferential text generation with vector quantised variational AutoEncoder.\nIn Dan Jurafsky,\nJoyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, pp. 6118–6129, Online, July 2020. As-\nsociation for Computational Linguistics.\ndoi:\n10.18653/v1/2020.acl-main.544.\nURL https:\n//aclanthology.org/2020.acl-main.544.\nDavid Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780,\n2021.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba.\nMastering atari with\ndiscrete world models. arXiv preprint arXiv:2010.02193, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nKyle Hsu, William Dorrell, James Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement via\nlatent quantization. Advances in Neural Information Processing Systems, 36, 2024.\nWei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable\nrepresentations from sequential data.\nIn I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/\npaper_files/paper/2017/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf.\nSarthak Jain and Byron C Wallace.\nAttention is not explanation.\nIn Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pp. 3543–3556, 2019.\nPublished as a conference paper at RLC 2024\nAlexander Khazatsky, Ashvin Nair, Daniel Jing, and Sergey Levine. What can i do here? learning\nnew skills by imagining visual affordances. In 2021 IEEE International Conference on Robotics\nand Automation (ICRA), pp. 14291–14297. IEEE, 2021.\nLukas Klein, João B. S. Carvalho, Mennatallah El-Assady, Paolo Penna, Joachim M. Buhmann,\nand Paul F Jaeger.\nImproving explainability of disentangled representations using multipath-\nattribution mappings. In Medical Imaging with Deep Learning, 2022. URL https://openreview.\nnet/forum?id=3uQ2Z0MhnoE.\nXiao Liu, Pedro Sanchez, Spyridon Thermos, Alison Q. O’Neil, and Sotirios A. Tsaftaris. Learn-\ning disentangled representations in the imaging domain.\nMedical Image Analysis, 80:102516,\n2022. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2022.102516. URL https://www.\nsciencedirect.com/science/article/pii/S1361841522001633.\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf,\nand Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentan-\ngled representations. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of\nthe 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine\nLearning Research, pp. 4114–4124. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.\npress/v97/locatello19a.html.\nVincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample-efficient world models.\narXiv preprint arXiv:2209.00588, 2022.\nChristoph Molnar.\nInterpretable Machine Learning.\nChristoph Molnar, 2 edition, 2022.\nURL\nhttps://christophm.github.io/interpretable-ml-book.\nNeel Nanda. A comprehensive mechanistic interpretability explainer & glossary, Dec 2022. URL\nhttps://neelnanda.io/glossary.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan\nCarter.\nZoom in: An introduction to circuits.\nDistill, 2020.\ndoi: 10.23915/distill.00024.001.\nhttps://distill.pub/2020/circuits/zoom-in.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine\nlearning, pp. 8821–8831. Pmlr, 2021.\nTilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A\nsurvey on interpreting the inner structures of deep neural networks. In 2023 IEEE Conference on\nSecure and Trustworthy Machine Learning (SATML), pp. 464–483. IEEE, 2023.\nJan Robine, Tobias Uelwer, and Stefan Harmeling. Discrete latent space world models for reinforce-\nment learning, 2020.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision, 115:211–252, 2015.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-\nization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626,\n2017.\nNan Shao, Zefan Cai, Hanwei xu, Chonghua Liao, Yanan Zheng, and Zhilin Yang. Compositional\ntask representations for large language models.\nIn The Eleventh International Conference on\nLearning Representations, 2023. URL https://openreview.net/forum?id=6axIMJA7ME3.\nPublished as a conference paper at RLC 2024\nLei Shu, Alexandros Papangelis, Yi-Chia Wang, Gokhan Tur, Hu Xu, Zhaleh Feizollahi, Bing Liu,\nand Piero Molino. Controllable text generation with focused variation. In Trevor Cohn, Yulan He,\nand Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020,\npp. 3805–3817, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/\nv1/2020.findings-emnlp.339. URL https://aclanthology.org/2020.findings-emnlp.339.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,\nCambridge, MA, 1998.\nAlex Tamkin, Mohammad Taufeeque, and Noah D Goodman.\nCodebook features: Sparse and\ndiscrete interpretability for neural networks. arXiv preprint arXiv:2310.17230, 2023.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nMatthew Wallingford, Aditya Kusupati, Alex Fang, Vivek Ramanujan, Aniruddha Kembhavi,\nRoozbeh Mottaghi, and Ali Farhadi. Neural radiance field codebooks. In The Eleventh Inter-\nnational Conference on Learning Representations, 2023. URL https://openreview.net/forum?\nid=mX56bKDybu5.\nP. J. Werbos. Learning how the world works: Specifications for predictive networks in robots and\nbrains. In Proceedings of IEEE International Conference on Systems, Man and Cybernetics, N.Y.,\n1987.\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pp. 11–20, 2019.\nLiang Zhang, Justin Lieffers, and Adarsh Pyarelal. Deep reinforcement learning with vector quan-\ntized encoding. arXiv preprint arXiv:2211.06733, 2022.\nKaifeng Zou, Sylvain Faisan, Fabrice Heitz, and Sébastien Valette. Joint disentanglement of labels\nand their features with vae. In 2022 IEEE International Conference on Image Processing (ICIP),\npp. 1341–1345, 2022. doi: 10.1109/ICIP46576.2022.9898046.\nKaifeng Zou, Sylvain Faisan, Fabrice Heitz, and Sébastien Valette. Disentangling high-level factors\nand their features with conditional vector quantized vaes. Pattern Recognition Letters, 172:172–\n180, 2023.\nA\nCode Frequency\nIn order to decide which codes would be the most beneficial to examine for consistency, we organized\nthem by how frequently they occur in the dataset. As seen in Figure 3, the model is not overly reliant\non any codes because the most frequent one occurs in barely over 1%. However, there is a significant\ndropoff between the top ten codes and the rest, which is the reason we presented heatmaps and\ncrops with three distinct frequencies. The top two codes occur significantly more than the tenth and\neleventh codes. The median codes then occur at a much lower rate. We chose to focus on the higher\nfrequency end of the distribution because we believe even if the low frequency codes are consistent,\nthey will not occur enough to provide useful interpretability, since in many observations, none of\nthose codes will appear.\nB\nConsistent and Inconsistent Code Crops\nIn Table 3, we show additional cropped images randomly sampled from six codes. The top three\nshown are examples of the more consistent codes we observed. The first one focuses on stone, while\nthe second captures grass, and the third focuses on water. Throughout our entire dataset of 477\nPublished as a conference paper at RLC 2024\nFigure 3: Percentage of each code’s occurrence in the dataset.\nTable 3: Additional examples of consistent (top three) and inconsistent (bottom three) codes.\nCode 0\nCode 360\nCode 192\nCode 15\nCode 329\nCode 438\ncodes, only a handful are close to as consistent as these. Even the ones that are have inconsistent\ninstances, like the fourth and fifth crops for code 192, that limit how confidently they can be\ninterpreted. Below them, we show examples for three inconsistent codes. The majority of the codes\nlook closer to these when randomly sampling their cropped images. Across all three of the codes,\nthe crops are widely varied in what they contain, providing no value for interpretability.\nC\nCode Co-occurrence\nOne of the potential reasons for the lack of interpretability of codes is that the model learned a\ncombination of codes to capture entities. In order to investigate this, we found the codes that most\nfrequently appear together and calculated their co-occurrence rate. We define co-occurrence rate as\nthe number of appearances together divided by the average number of times the two codes are used.\nThis is related to the mechanistic interpretability concept of superposition Elhage et al. (2022),\nwhere given a latent feature space represented by a set of d-dimensional vectors v, optimization\ntries to use these vectors to represent more features than they have the dimensional capacity. This\nleads to phenomena such as representing concepts with groups of features, or individual features\nalternately representing more than one concept depending on the input, both of which we observe\nin our work Elhage et al. (2022); Nanda (2022). Figure 4 shows a heatmap of the rates for the ten\nPublished as a conference paper at RLC 2024\nFigure 4: Co-occurrence rate for the ten most frequently co-occurring pairs of codes\nTable 4: Example crops from co-occurring codes, randomly sampled from observations where they\nco-occur.\nCode 424\nCode 344\nCode 273\nCode 507\nmost frequently co-occurring pairs. Note the co-occurrence of codes to themselves has been set to\nzero to better distinguish the rates between different codes. The highest rate observed was between\ncodes 344 and 424 at around 18%.\nIn Table 4, we visualize the crops from the observations where codes co-occur for the two highest\nco-occurring pairs, codes 344 and 424 and codes 273 and 507. The crops reveal that each code is\nextremely consistent in what it captures in these instances. Although this is a promising indication\nof interpretability, we believe it is still greatly limited by two factors. The first applies only to the\npair of codes 344 and 424, which is that all their instances of co-occurrence came from a single\nepisode. The codes selected at a given time step exhibit significant co-occurance with the codes\nfrom time steps shortly before and after them, since the observations are very similar. Therefore,\nthe high consistency of the codes is less trustworthy because it is an expected behavior within an\nepisode and not demonstrated across episodes. The second factor limiting the impact of co-occuring\ncodes is that, while these instances can indeed provide interpretable observations, they account\nfor a very small portion of the entire observations collected. Specifically, the two pairs, codes 344\nand 424 and codes 273 and 507, each co-occur in roughly 0.1% of the observations and the co-\noccurence rates for all codes suggest very few other pairs will provide percentages near that high.\nBased on the target application of the RL system, the percent of interpretable samples needed will\nvary. Given an example target of 10%, our results suggest this may not be possible to reach by\ncombining all the interpretable codes and co-occurring pairs without extensive analysis of all the\ncodes and code combinations. We argue if arduous analysis is required to reach a sufficient amount of\ninterpretability, vector quantization by itself is not truly providing interpretability to the meaningful\nextent suggested by prior literature.\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-07-28",
  "updated": "2024-07-28"
}