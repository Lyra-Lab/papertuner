{
  "id": "http://arxiv.org/abs/2407.21236v1",
  "title": "GNUMAP: A Parameter-Free Approach to Unsupervised Dimensionality Reduction via Graph Neural Networks",
  "authors": [
    "Jihee You",
    "So Won Jeong",
    "Claire Donnat"
  ],
  "abstract": "With the proliferation of Graph Neural Network (GNN) methods stemming from\ncontrastive learning, unsupervised node representation learning for graph data\nis rapidly gaining traction across various fields, from biology to molecular\ndynamics, where it is often used as a dimensionality reduction tool. However,\nthere remains a significant gap in understanding the quality of the\nlow-dimensional node representations these methods produce, particularly beyond\nwell-curated academic datasets. To address this gap, we propose here the first\ncomprehensive benchmarking of various unsupervised node embedding techniques\ntailored for dimensionality reduction, encompassing a range of manifold\nlearning tasks, along with various performance metrics. We emphasize the\nsensitivity of current methods to hyperparameter choices -- highlighting a\nfundamental issue as to their applicability in real-world settings where there\nis no established methodology for rigorous hyperparameter selection. Addressing\nthis issue, we introduce GNUMAP, a robust and parameter-free method for\nunsupervised node representation learning that merges the traditional UMAP\napproach with the expressivity of the GNN framework. We show that GNUMAP\nconsistently outperforms existing state-of-the-art GNN embedding methods in a\nvariety of contexts, including synthetic geometric datasets, citation networks,\nand real-world biomedical data -- making it a simple but reliable\ndimensionality reduction tool.",
  "text": "GNUMAP: A PARAMETER-FREE APPROACH TO UNSUPERVISED\nDIMENSIONALITY REDUCTION VIA GRAPH NEURAL NETWORKS\nJihee You\nData Science Institute\nUniversity of Chicago\nChicago, IL 60605\njiheeyou@uchicago.edu\nSo Won Jeong\nBooth School of Business\nUniversity of Chicago\nChicago, IL 60605\nsowonjeong@uchicago.edu\nClaire Donnat\nDepartment of Statistics\nUniversity of Chicago\nChicago, IL 60605\ncdonnat@uchicago.edu\nAugust 1, 2024\nABSTRACT\nWith the proliferation of Graph Neural Network (GNN) methods stemming from contrastive learning,\nunsupervised node representation learning for graph data is rapidly gaining traction across various\nfields, from biology to molecular dynamics, where it is often used as a dimensionality reduction\ntool. However, there remains a significant gap in understanding the quality of the low-dimensional\nnode representations these methods produce, particularly beyond well-curated academic datasets. To\naddress this gap, we propose here the first comprehensive benchmarking of various unsupervised\nnode embedding techniques tailored for dimensionality reduction, encompassing a range of manifold\nlearning tasks, along with various performance metrics. We emphasize the sensitivity of current\nmethods to hyperparameter choices — highlighting a fundamental issue as to their applicability in\nreal-world settings where there is no established methodology for rigorous hyperparameter selection.\nAddressing this issue, we introduce GNUMAP, a robust and parameter-free method for unsupervised\nnode representation learning that merges the traditional UMAP approach with the expressivity of the\nGNN framework. We show that GNUMAP consistently outperforms existing state-of-the-art GNN\nembedding methods in a variety of contexts, including synthetic geometric datasets, citation networks,\nand real-world biomedical data — making it a simple but reliable dimensionality reduction tool.\n1\nIntroduction\nConsider the following biological problem: given a set of gene expressions at various locations within a tissue sample\n(for instance, a slice of mouse brain tissue), how can we aggregate information to discover spatial domains with coherent\ngene expression patterns? Recent methods, including [1] from which this example is adapted, have turned towards using\na graph-based approach — and subsequently, Graph Neural Networks (GNNs) — to resolve this conundrum. In this\nsetting, the data is first represented as a graph G on n nodes corresponding here to the various spatial locations within\nthe sample, each endowed with a feature vector Xv ∈Rd (the gene expression data). Under this new formalism, Graph\nNeural Networks (GNNs) [2, 3] come as a natural tool for visualization and the subsequent discovery of new patterns\nin the data. Through the use of recursive neighborhood “convolutions”, GNNs allow indeed the creation of rich node\nrepresentations that capture topological information, feature data and essential neighborhood properties, and integrate\nthis information in a Euclidean vector representation that is amenable to any downstream machine learning task.\nUnsupervised Learning on Graph Data. While the early rise in Graph Neural Network (GNN) advancements\npredominantly emphasized supervised learning, there has been growing interest in developing unsupervised GNN\nmethods for learning node representations. This shift is driven by the scarcity of labeled data in numerous real-world\nsituations, such as in the example described above, along with an increasing demand for techniques that facilitate\nexploratory data analysis and dimensionality reduction for graph data. However, there seems to have been a concurrent\ndevelopment of unsupervised learning approaches in the application community [1, 4, 5, 6, 7] and in the method\ncommunity [8, 9, 10, 11, 12]. This parallel development may underscore a disconnect between the approaches devised\nby the methods community and the practical reality of real-world data. Specifically, the deployment of state-of-the-art\narXiv:2407.21236v1  [cs.LG]  30 Jul 2024\nA PREPRINT - AUGUST 1, 2024\napproaches evaluated on academic benchmarks appears to be impeded by two significant challenges: (a) Ease of\ndeployment, and (b) Trustworthiness of the learned representations.\n(a) Ease of deployment. From a methodological point of view, a growing number of recent approaches are leaning\ntoward the adoption of self-supervised contrastive learning frameworks to effectively represent nodes [8, 11, 10, 9, 12].\nIn this setting, the trick usually consists of perturbing the input data (e.g., by masking features with probability pf,\ndropping edges with probability pe) to create two modified versions of the original data. Each of these perturbed\nversions of the data is then processed by a Graph Neural Network, which is trained to identify pairs of node embeddings\nfrom the two perturbed graphs that correspond to the same node in the original dataset. Examples of such training\nobjectives include the loss proposed by [11] for their method GRACE, L =\n1\n2N\n\u0010PN\ni=1 [ℓ(ui, vi) + ℓ(vi, ui)]\n\u0011\n,\nwhere ℓ(ui, vi) = log\n\u0012\nes(ui,vi)/τ\nPN\nk=1[1{k̸=i}es(ui,uk)/τ +es(ui,vk)/τ]\n\u0013\n, s : Rp × Rp →R is the cosine similarity function,\nand ui, vi ∈Rp are the node representation for node i stemming from the two perturbed versions of the graph.\nFinally, τ > 0 is a temperature parameter that must be tuned. Alternatives include the loss used in CCA-SSG [8],\nL = Pn\ni=1 ∥ui −vi∥2 + λ\n\u0000∥U T U −I∥2\nF + ∥V T V −I∥2\nF\n\u0001\nwhere U, V ∈RN×p are the node representation matrix\nof two views, λ > 0 is a hyperparameter, and ∥· ∥F denotes the Frobenius norm. While these self-supervised learning\nlosses have achieved state-of-the-art performance in a number of academic benchmarks, few of these methods have yet\nbeen deployed in applied settings.\nOne hypothesis explaining this gap is the heavy reliance of these ”state-of-the-art” approaches on the correct choice\nof hyperparameters (for instance, λ, τ, pf or pe in the approaches described above). We exemplify this phenomenon\nthrough an example case in Figure 1, where we propose to deploy CCA-SSG [8] using a simple 2-layer GCN [3] to\nlearn a 2D visualization of the nodes. We then assess the quality of the learned embeddings by learning a support\nvector classifier to classify the nodes, and evaluate the performance of the classification on held-out data. We note a\nsubstantial variation in embedding quality as the edge drop rate pe, the feature mask rate pm and the regularization\nparameter λ vary. This highlights the importance of selecting the “correct” set of hyperparameters: a wrong choice\nof hyperparameters could cause the method to significantly underperform or to learn uninformative embeddings (see\nFigure 1 left). However, in the unsupervised context, there is no established cross-validation technique for GNNs or\nFigure 1. Node representation learning for Cora using CCA-SSG [8]. Colors represent classes. Classification accuracy\nwas established by running a support vector machine classifier on the learned 2D node representations, using 5-fold cross-\nvalidation to fix the kernel bandwidth. We note a substantial variation in embedding quality as the parameters (regularization\nlambda, feature mask rate, edge drop rate) vary.\nother principled technique for parameter selection at large. This significantly complicates the practical application of\nthese methods in settings where there are no labels to evaluate the method on for hyperparameter selection.\n(b) Trustworthiness. Moreover, despite the increasing adoption of graph neural networks (GNNs), there is a notable\ngap in research focusing on the evaluation of the quality of GNN node representations: How effectively do these\nembeddings capture the structural details within the data? Are they capable of accurately encoding topological\ninformation? Hypothetically, a good embedding should preserve both local and global structure in the graph while\nconveying a maximum amount of information. Beyond the graph setting, the need to benchmark new unsupervised\napproaches is underscored by the multiplication of recent publications in applied domains providing tips for performing\na dimensionality reduction [13] or comparing existing tools [14, 15, 16]: with the rapidly increasing number of new\nmethods, it becomes difficult to know which one to adopt — particularly when these methods can be quite sensitive.\nMany applications — particularly in biology — have a long track record of using graph-based techniques such as UMAP\n[17] and t-SNE [18] for dimensionality reduction. These methods have been well established, understood, and have\nbeen vetted by the myriad of applications and benchmarking tasks, including manifold learning and classification tasks,\nto which they have been deployed. On the other hand, while GNNs offer the potential for richer representation learning\nby capturing both feature and node information, these methods have not yet attained a similar level of reliability. This\ngap underscores (a) the need for a comparison of GNN-based approaches to current methods, allowing practitioners\nto place GNNs in the landscape of unsupervised learning methods. While GNNs offer more flexibility as they allow\n2\nA PREPRINT - AUGUST 1, 2024\ncombining both graph data and node covariates, this comparison is nevertheless indispensable to start evaluating them\nas dimensionality reduction techniques; and (b) the need for more in-depth analysis and validation of GNNs in a variety\nof settings that go beyond node classification benchmarks, and incorporate a wider variety of tasks – including learning\nmanifolds.\nContributions. To fill this gap, we propose here a systematic comparison of learned node representations, focusing\nmore specifically on the context of dimensionality reduction and data visualization. To bridge classical dimensionality\nreduction methods that do not incorporate node features with state-of-the-art GNN-based approaches, we first introduce\nan unsupervised learning technique inspired by UMAP [19], named GNUMAP, which integrates the UMAP framework\nfor learning low-dimensional embeddings while being less parameter-intensive compared to contrastive learning\nmethods. This approach facilitates a comparative analysis of self-supervised and reconstruction-based unsupervised\nembedding methods. We then propose to evaluate existing methods on two sets of tasks: (a) a suite of manifold learning\ntasks, thereby allowing us to compare GNN methods with more established benchmarks; and (b) classification tasks;\nOur goal is to provide practitioners with a clearer understanding of effective approaches in the unsupervised application\nof GNNs, thereby promoting their broader use across various applications.\n2\nGNUMAP: Bridging classical dimensionality reduction and Graph Neural Networks\nIn this section, we introduce Graph-Neural UMAP (GNUMAP), a method that allows us to bridge the Graph Neural\nNetwork (GNN) framework with established dimensionality reduction techniques proven effective on real-world data.\nWe begin by briefly reviewing UMAP, before extending it to the analysis of graph data.\nUMAP. UMAP [17], is a standard technique for dimensionality reduction of Euclidean data denoted as {xi}n\ni=1 ∈Rph,\nand relies partly on the graph formalism. The first step of the method consists indeed in constructing a nearest-neighbor\ngraph based on the data points {xi}n\ni=1. The connection probability between nodes in this graph is given by the formula\npij = pi|j + pj|i −pj|i × pi|j,\n(1)\nwhere\npj|i = exp\n\u0012\n−d(xi, xj) −ρi\nσi\n\u0013\n.\n(2)\nIn this context, xi ∈Rph denotes the original high-dimensional coordinates of input data point i, ρi represents the\ndistance to the nearest neighbor of point i, and σi represents the local density factor around point i. UMAP finds\na low-dimensional representation yi ∈Rpl, pl << ph for each point xi that minimizes the cross-entropy between\nconnection probabilities in the high and low-dimensional space. In the low-dimensional space, UMAP uses the family\nof curves f(x) =\n1\n1+αx2β to compute the connection probability analogous to pij, which is expressed as\nqij =\n1\n1 + α × d(yi, yj)2β .\n(3)\nBy default, UMAP assumes the spread of the low-dimensional embeddings is 1 and the desired minimum distance\nbetween embeddings is 0.1. In such case, UMAP notes that f(x) =\n1\n1+αx2β where α = 1.57 and β = 0.89 best models\nthe low-dimensional connection probability. While α = 1.57 and β = 0.89 are the default parameters, the authors of\nUMAP allow customizing of α and β based on the desired embedding spread and minimum distance.\nThe algorithm then computes the cross-entropy loss between the pairwise connection probabilities in both high (pij)\nand low (qij) dimensions. This cross-entropy loss is formulated as:\nL = −\nX\ni\nX\nj\n[pij log(qij) + (1 −pij) log(1 −qij)]\nThis approach enables UMAP to find a low-dimensional representation while preserving the intrinsic topological\nstructure of the data.\nGNUMAP. By contrast, in our proposed adaptation of UMAP to the graph setting, the input of the algorithm is already a\ngraph with either binary or weighted edges. We will denote this given input graph as G = (V, E, X), where V ∈Rn is a\nset of n nodes, E is a set of edges along with their connection probability, and X ∈Rn×ph be the ph-dimensional node\nfeature matrix. This inherent graph structure of the input allows us to bypass the initial step of traditional UMAP, which\nconverts data into a graph with connection probability defined as in Equation 1. Similar to UMAP, our approach focuses\n3\nA PREPRINT - AUGUST 1, 2024\non achieving dimensionality reduction by identifying a low-dimensional representation that preserves the topology of\nthe original graph.\n(a) High Dimensional Node Connectivity. Let A ∈Rn×n be the input graph’s (possibly weighted) adjacency matrix.\nWe replace the connection probability pij in Equation 2 by Aij, the observed (and potentially weighted) connection\nbetween node i and j. This alleviates in particular any ambiguity on the choice of the parameters ρi and σi in the\noriginal method. Note here that compared to the original UMAP algorithm, if the adjacency matrix is binary, the pijs\ntake values in {0, 1}.\n(b) Low Dimensional Node Connectivity. Let yi denote the GNN embedding of datapoint i (yi = GNN(xi, N(i)), where\nN(i) denote the neighborhood of point i. We further feed the GNN outputs into a differentiable batch normalization\nlayer that scales and decorrelates the input features to reduce internal covariate shift. The effect of the DBN layer is\ndemonstrated in Table 4 and Figure 6 in the Appendix.\nThen,\nqij =\n1\n1 + α × d(yi, yj)2β\ndenotes the weight of the low-dimensional node connections between yi and yj. This way of defining the low-\ndimensional connection strength qij draws significant inspiration from UMAP. As our DBN objective ensures the\nembedding distances are normalized, the default UMAP minimum distance of 0.1 allows interpretability in our setting,\nand therefore we incorporated UMAP’s default constants α = 1.57 and β = 0.89. These values allow indeed smaller\ntails (so more spread-out embeddings) than a baseline choice of α = β = 1 (see Figure 5 in the Appendix). For a\ndemonstration of GNUMAP performance with varying α and β, see Table 3 in the Appendix.\n(c) Loss Calculation. GNUMAP employs a cross-entropy loss between high and low dimensional node connectivity, so\nthe loss becomes\nL = −\nX\ni\nX\nj\n[pij log(qij) + (1 −pij) log(1 −qij)]\n(4)\nFor scalability purposes, GNUMAP downsamples the loss corresponding to absent edges, and samples as many\nnegatives as positives to compute the loss. The procedure is summarized in Algorithm 1.\nAlgorithm 1 GNUMAP Algorithm for Output Dimension pl\n1: Inputs: Adjacency matrix A ∈Rn×n\n2: npos = count(pij > 0 in P)\n3: Epos = {(i, j) ∈V × V : pij > 0}\n4: for epochs = 1 to 400 do\n5:\nSample nneg negative edges, where nneg = npos\n6:\nCompute node embeddings through a graph convolution network: Yn×pl\nGCN = GCN(Features, Edge Index)\n7:\nApply differentiable batch normalization: Yn×pl\nDBN = DBN(YGCN)\n8:\nCompute d(yi, yj) across all (i, j) ∈Eneg ∪Epos, then append to create D\n9:\nCompute the corresponding low-dimensional connection probability: Q =\n1\n1+αD2β ∈R1×(nneg+npos)\n10:\nAccess A at positive edge indices and negative sampled indices to get high-dimensional connection probability\n11:\nP ∈R1×(nneg+npos)\n12:\nCompute L = Cross-Entropy(P, Q) and Backpropagate L\n13: end for\nRemark 1. GNUMAP is an auto-encoder. Using the reconstruction objective in Equation 4, it is is obvious that\nGNUMAP is simply an auto-encoder [20]. Auto-encoders are a class of reconstruction-based techniques that aim to\nfind a low-dimensional representation of the data such that the distance or similarity between learned node embeddings\nis predictive of the existence of an edge between nodes in the original graph. The best known version of this type\nof approach, GAE, along with its variational version, VGAE, were first suggested in [21]. However, our proposed\nadaptation of UMAP differs slightly from these methods in several aspects: (i) the probability qij is defined differently.\nIndeed, in GAE, qij is a function of the inner product between node representations:\nqGAE\nij\n=\n\u00001 + e−XT\ni Xj\u0001−1 =\n\u00001 + e\nd(Xi,Xj )2−∥Xi∥2−∥Xj ∥2\n2\n\u0001−1.\nEdge probabilities therefore depend both of the distance d(Xi, Xj) and on the norms ∥Xi∥2 and ∥Xj∥2 of the\nembeddings themselves. Consequently, embeddings close to the origin will typically exhibit a higher number of\n4\nA PREPRINT - AUGUST 1, 2024\nconnections than embeddings farther from the origin. This allows the model to potentially build in degree heterogeneity,\nand places all high degree nodes towards the origin. However, in our proposed construction, this probability is solely\na function of the distance. This can be beneficial, particurlarly when we expect the graph to have several cores,\nwhose center (high degree nodes) should be placed far from one another. We will come back to this point in our\nsynthetic experiments in section 3. (ii) GNUMAP uses a whitening step that allows to regularize the objective function.\nGNUMAP’s differentiable batch normalization design closely follows that of CLGR [12] since the effectiveness of\nbatch whitening was already demonstrated in the previous work.\nRemark 2. Contrary to self-supervised techniques for node embeddings, this proposed method does not rely on the\nchoice of specific hyperparameters to perform well.\nRemark 3. The driving hypothesis that GNUMAP leverages is that connected nodes should be close. GNUMAP is\ntherefore amenable to homophilic networks (where adjacent nodes are presumed to be similar), rather than heterophilic\nnetworks — a limitation of our framework compared to self-supervised learning techniques.\nRemark 4. Another method related to this direct extension of UMAP is SpaGCN [1]. Developed for transcriptomics\napplications, SpaGCN is a GCN algorithm that incorporates histological data to identify spatial domains and the\nassociated gene expressions. From the method perspective, while SpaGCN is also an adaptation of UMAP to the graph\nsetting, it differs from our approach in two significant ways: (i) SpaGCN directly targets creating a pre-specified number\nof clusters in the embedding space. The associated clusters are selected by minimizing the KL divergence between their\ndistance in low-dimensional space, and their distance in high-dimensional space. The definitions of q and p are also\nquite different: SpaGCN’s q is assigned to a lower value when the embeddings are further from the cluster centers,\nand p is updated as the twice-normalized square of q at every three epochs. GNUMAP, by contrast, is amenable to\nlearning a wider variety of manifolds, as it does not explicitly target the clustering of embeddings, but simply seeks to\nreconstruct a low-dimensional representation of the nodes that aligns with the original graph.\nOverall, GNUMAP is a simple auto-encoder structure that leverages the success of UMAP in the Euclidean setting to\nperform dimensionality reduction. While this method is not conceptually novel (it is just an autoencoder), as we will\nshow in our experiments section, the choice of the loss, as well as the addition of a whitening step are key to its success.\nThis simple approach can perform well on a variety of tasks, and, as previously argued, is more interpretable and closer\nto many practioners’ requirements in dimensionality reduction.\n3\nEvaluating GNNs’ manifold learning abilities\nHaving established a natural extension of UMAP to the GNN setting, we propose to benchmark these approaches on a\nset of toy examples evaluating their ability to correctly learn an underlying manifold structure. While this is a standard\ntest for any dimensionality reduction technique, GNNs have not yet been evaluated in this specific context.\nWe first evaluate GNUMAP performance on four synthetic geometric graph datasets considered as benchmarks in\nthe realm of unsupervised dimensionality reduction methods: the Blobs, Swissroll, Moons, and Circles datasets (see\nFigure 2). The datasets were generated with ground-truth low-dimensional coordinates and cluster labels, which allows\nus to calculate our proposed metrics and also to inspect if our metrics are coherent with the embedding visualization. The\nfour datasets were generated with 500 nodes, each connected to its 20 nearest neighbors. For simplicity, the experiments\nin this work have assigned a value of 1 to positive edges and 0 to negative edges. However, the GNUMAP algorithm is\napplicable to any input graph preprocessed such that weaker edges are mapped closer to 0 and stronger edges closer to 1.\nThe features were instantiated as the embeddings of a 10-component Laplacian eigenmap decomposition of the induced\n20-nearest graph. The latter is indeed an established procedure to instantiate features on a graph [1]. We compare\nGNUMAP with state-of-the-art GNN embedding methods tailored for dimensionality reduction: DGI [9], BGRL [10],\nCCA-SSG [22], GAE [21], VGAE [21] and SpaGCN [1]. We also compare with well-established dimensionality\nreduction methods PCA, t-SNE [18], Isomap [23], Laplacian Eigenmap [24], UMAP [17], and DenseMAP [25]. Note\nall methods mentioned above are Euclidean methods, which enable a consistent assessment with respect to our proposed\nmetrics. To evaluate embedding quality, we introduce metrics including but not limited to: agreement in local geometry,\nSpearman correlation, and classification accuracy. We refer the reader to Appendix A for a more detailed description of\nthese metrics.\nWe implement a standard 2-layer GCN architecture for all GNN-based methods. Note that SpaGCN, CCA-SSG,\nGRACE, and BGRL are hyperparameter-heavy methods. SpaGCN requires the louvain resolution and number of\nneighbours for cluster initialization, as well as the penalty coefficient for embedding distance from cluster centers.\nSimilarly, GRACE, BGRL and CCA-SSG require the specification of the regularization λ, an edge drop rate, as well as\na feature mask rate. Autoencoder methods such as GNUMAP and GAE, on the other hand, are completely parameter\nfree.\n5\nA PREPRINT - AUGUST 1, 2024\nSince it is unclear which set of hyperparameters would be optimal for each dataset when deploying contrastive-learning\ntechniques, we ran a full search for all possible hyperparameter combinations. Note that this provides the most favorable\ncomparison of these methods to GNUMAP, but deviates from the original unsupervised learning setting, since we are\nchoosing the hyperparameters of the methods based on the labels — therefore making the methods more supervised. In\nreal-world experiments, however, we would not benefit from such supervision. Despite this selection procedure, as we\nwill show, GNUMAP still manages to outperform most methods.\nWe visualize some of the embedding from the best hyperparameter combination in Figure 2. In addition to GNUMAP,\nwe visualized GAE for comparisons with explicitly learned distance between low-dimensional embeddings, CCA-SSG\nand GRACE for comparisons with contrastive learning methods. We also show results from related method SpaGCN.\nFor a complete illustration including DGI, BGRL, VGAE, as well as PCA, t-SNE, Isomap, Laplacian Eigenmap, UMAP,\nand DenseMAP, see Figure 7 in the appendix. Note the dimensionality reduction methods were not fitted on spectral\nfeatures like the GNN-based models.\nDataset\nOriginal Data\nSPAGCN\nGAE\nCCA-SSG\nGRACE\nGNUMAP\nBlobs\nSwissroll\nCircles\nMoons\nFigure 2. Node representation learning for 4 synthetic datasets : Blobs, Swissroll, Circles and Moons. Each image represents\na different method of visualization.\nFurthermore, we compute proposed metrics over 100 experiments and summarize the results in Table 1.\nDiscussion. Overall, GNUMAP consistently outperforms other self-supervised GNN-based methods in these small\nexamples. GNUMAP successfully embeds blobs, swissroll, circles, and moons in 2-dimensions in a way that differenti-\nates ground truth labels more clearly than state-of-the-art GNN-based methods. In Table 1, our method outperforms\nstate-of-the-art GNN-based methods in all five of our proposed metrics and the GNUMAP embedding visualization in\nFigure 2 is coherent with the metrics. Note that GNUMAP successfully unrolls the Swissroll to closely resemble the\noriginal data, which is a standard synthetic dataset where linear methods fail to correctly embed.\n4\nEvaluating unsupervised GNNs’ embedding performance on real-world data\nTo assess GNUMAP’s performance across real-world data, we conducted a comparative analysis using well-established\ngraph benchmark datasets: Cora, Citeseer, and Pubmed. They are standard citation network benchmark datasets [26].\nIn these networks, nodes represent scientific publications, and edges denote citation links between publications. Node\nfeatures are the bag-of-words representation of papers, and node label is the academic topic of a paper.\nWe also incorporated the Mouse Spleen cell data [27] to assess GNUMAP performance in biological applications. The\ndataset was originally generated by “co-detection by indexing”(CODEX) techniques, which is to take the image of a\ntissue section, and at each of the tissue locations, the relevant biomarkers information is measured and recorded [27].\nFor our experiments, we created a 5-nearest-neighboring-cell graph from the CODEX data. While the 3-topic cluster\nlabels in the Mouse Spleen dataset are generated by the spatial LDA model, previous work [28] has established that\n6\nA PREPRINT - AUGUST 1, 2024\nTable 1. Evaluated mean and standard deviation of proposed metrics for synthetic datasets blobs, circles, moons, and\nswissroll over 100 experiments with GNN-based methods. The upward arrows next to metric name denote a better\nperformance with higher metrics, whereas the downward arrows denote better performance with lower metrics. The best\nresult is bolded, and the next best result is underlined.\nDataset\nModel\nClassification\nAccuracy ↑\nCalinski\nHarabasz\nScore ↑\nDavies\nBouldin Score\n↓\nSpearman\nCorrelation\nw/\nOriginal\nGraph ↑\nOverlap\n%\nof 50 Neigh-\nbours ↑\nBlobs\nDGI\n0.65 ± 0.06\n95.17 ± 90.13\n5.53 ± 4.73\n0.22 ± 0.09\n0.52 ± 0.06\nBGRL\n0.70 ± 0.07\n79.26 ± 57.93\n5.28 ± 7.56\n0.23 ± 0.09\n0.59 ± 0.05\nCCA-SSG\n0.90 ± 0.08\n1139.31 ± 1258.86\n1.91 ± 3.87\n0.52 ± 0.18\n0.76 ± 0.06\nSPAGCN\n0.74 ± 0.07\n61.07 ± 37.80\n5.89 ± 7.41\n0.22 ± 0.08\n0.70 ± 0.04\nGAE\n0.93 ± 0.10\n10456.43 ± 12149.96\n1.14 ± 2.25\n0.61 ± 0.13\n0.58 ± 0.07\nVGAE\n0.91 ± 0.10\n5523.10 ± 6633.58\n1.33 ± 2.12\n0.61 ± 0.13\n0.55 ± 0.09\nGRACE\n0.84 ± 0.12\n561.59 ± 631.46\n2.59 ± 3.32\n0.48 ± 0.18\n0.73 ± 0.06\nGNUMAP\n0.99 ± 0.02\n1086.79 ± 173.32\n0.58 ± 0.11\n0.69 ± 0.10\n0.97 ± 0.01\nCircles\nDGI\n0.47 ± 0.06\n34.19 ± 25.52\n12.37 ± 7.42\n0.23 ± 0.10\n0.72 ± 0.06\nBGRL\n0.49 ± 0.05\n29.51 ± 16.63\n10.90 ± 6.39\n0.20 ± 0.07\n0.76 ± 0.05\nCCA-SSG\n0.53 ± 0.06\n72.81 ± 92.22\n10.69 ± 6.74\n0.40 ± 0.21\n0.88 ± 0.04\nSPAGCN\n0.54 ± 0.05\n31.00 ± 17.62\n11.60 ± 8.73\n0.21 ± 0.06\n0.87 ± 0.03\nGAE\n0.42 ± 0.05\n40.56 ± 40.21\n13.56 ± 15.43\n0.38 ± 0.16\n0.88 ± 0.07\nVGAE\n0.42 ± 0.05\n39.22 ± 21.40\n11.52 ± 5.89\n0.39 ± 0.16\n0.83 ± 0.07\nGRACE\n0.49 ± 0.05\n54.22 ± 60.21\n10.94 ± 8.63\n0.36 ± 0.22\n0.84 ± 0.04\nGNUMAP\n0.66 ± 0.03\n46.68 ± 30.84\n10.88 ± 7.70\n0.48 ± 0.15\n0.99 ± 0.01\nMoons\nDGI\n0.57 ± 0.06\n104.58 ± 51.66\n3.80 ± 2.12\n0.21 ± 0.10\n0.71 ± 0.06\nBGRL\n0.62 ± 0.05\n125.24 ± 50.80\n3.08 ± 2.02\n0.20 ± 0.07\n0.76 ± 0.05\nCCA-SSG\n0.70 ± 0.07\n308.89 ± 178.11\n2.02 ± 1.05\n0.34 ± 0.18\n0.90 ± 0.04\nSPAGCN\n0.70 ± 0.05\n160.19 ± 73.17\n2.32 ± 1.42\n0.21 ± 0.06\n0.89 ± 0.03\nGAE\n0.57 ± 0.07\n768.68 ± 516.56\n3.01 ± 1.70\n0.33 ± 0.13\n0.85 ± 0.05\nVGAE\n0.56 ± 0.06\n620.11 ± 426.56\n3.36 ± 1.78\n0.36 ± 0.17\n0.80 ± 0.06\nGRACE\n0.65 ± 0.06\n238.53 ± 132.55\n2.51 ± 1.60\n0.31 ± 0.19\n0.86 ± 0.04\nGNUMAP\n0.88 ± 0.02\n559.49 ± 163.03\n0.91 ± 0.45\n0.45 ± 0.13\n0.99 ± 0.01\nSwissroll\nDGI\n0.18 ± 0.03\n53.80 ± 17.87\n6.59 ± 1.42\n0.20 ± 0.11\n0.56 ± 0.06\nBGRL\n0.20 ± 0.03\n53.69 ± 11.91\n5.95 ± 1.48\n0.25 ± 0.10\n0.61 ± 0.05\nCCA-SSG\n0.21 ± 0.03\n120.78 ± 43.25\n4.55 ± 1.26\n0.38 ± 0.16\n0.76 ± 0.06\nSPAGCN\n0.22 ± 0.03\n93.13 ± 18.85\n4.91 ± 2.55\n0.27 ± 0.08\n0.72 ± 0.04\nGAE\n0.16 ± 0.02\n216.94 ± 72.76\n5.74 ± 1.99\n0.66 ± 0.12\n0.74 ± 0.03\nVGAE\n0.15 ± 0.02\n173.35 ± 42.33\n6.41 ± 2.22\n0.64 ± 0.12\n0.72 ± 0.03\nGRACE\n0.20 ± 0.02\n102.91 ± 26.44\n4.82 ± 1.32\n0.37 ± 0.14\n0.70 ± 0.04\nGNUMAP\n0.28 ± 0.03\n314.90 ± 86.80\n2.53 ± 0.50\n0.88 ± 0.08\n0.96 ± 0.02\nsuch topic annotations are biologically consistent with immunologist-labeled topics. Therefore, we will regard the\n3-topic cluster labels as ground truth labels in the Mouse Spleen dataset for the subsequent analysis. For all experiments\nwith real-world datasets, GNUMAP was compared against CCA-SSG, SPAGCN, and GAE.\nAll datasets come with or were assigned class labels, enabling a comparison between embedding visualization and our\ncalculated metrics. For scalable metric evaluation, we randomly sampled 1000 datapoints and calculated the following\nmetrics: classification accuracy, Calinski Harabasz score, and Davies Bouldin score. These metrics convey the general\ndegree of embedding informativity and the quality of clustering, which is appropriate for evaluating Cora, Citeseer,\nPubmed, and Mouse Spleen data as one expects some clustering among the same classes. The results are presented in\nFigure 3 and Table 2.\nDiscussion. On homophilic citation networks, GNUMAP is consistently the best or second-best for two of the three\nmetrics: classification accuracy and Davies Bouldin score. For Cora, GNUMAP outperforms all other models in\nclassification accuracy and Davies Bouldin score, and GNUMAP produces the second best Clinski Harabasz score after\nCCA-SSG. On Pubmed, GAE was the best-performing model on all three metrics, and GNUMAP was a close second.\nThe autoencoder framework appears especially effective in this data set. GAE was the best-performing model on all\nthree metrics for Citeseer, and GNUMAP was a close second. Autoencoder framework appears especially effective on\n7\nA PREPRINT - AUGUST 1, 2024\nDataset\nSPAGCN\nGAE\nCCA-SSG\nGNUMAP\nCora\nPubmed\nCiteseer\nFigure 3: Node representation learning for real-world datasets Cora, Citeseer, Pubmed.\nDataset\nOriginal Data\nSPAGCN\nGAE\nCCA-SSG\nGNUMAP\nMouse\nSpleen\nFigure 4. Node representation learning for Mouse Spleen dataset. Colours represent assigned ground truth cluster label.\nBlue denotes B-cells, purple denotes marginal zone B-cells, gray denotes non-B cells, and green denotes red pulp [28].\nTable 2. Summary mean and standard deviation of proposed metrics over multiple experiments with real-world datasets\nCora, Citeseer, Pubmed, and Mouse Spleen. The upward arrows next to the metric name denote a better performance with\nhigher metrics, whereas the downward arrows denote a better performance with lower metrics. The best result is bolded, and\nthe next best result is underlined.\nDataset\nModel\nClassification\nAccuracy ↑\nCalinski Harabasz\nScore ↑\nDavies\nBouldin\nScore ↓\nCora\nCCA-SSG\n0.47 ± 0.11\n107.12 ± 86.14\n16.31 ± 25.76\nSPAGCN\n0.31 ± 0.01\n10.15 ± 4.25\n17.24 ± 13.18\nGAE\n0.50 ± 0.03\n104.19 ± 20.61\n6.08 ± 6.18\nGNUMAP\n0.64 ± 0.04\n87.90 ± 21.08\n4.58 ± 3.19\nPubmed\nCCA-SSG\n0.59 ± 0.09\n67.36 ± 69.68\n14.66 ± 18.52\nSPAGCN\n0.45 ± 0.03\n16.99 ± 12.01\n12.69 ± 14.60\nGAE\n0.70 ± 0.01\n144.36 ± 10.58\n2.20 ± 0.17\nGNUMAP\n0.67 ± 0.01\n142.50 ± 19.38\n2.81 ± 0.30\nCiteseer\nCCA-SSG\n0.50 ± 0.09\n159.64 ± 77.50\n8.56 ± 17.91\nSPAGCN\n0.26 ± 0.02\n7.45 ± 3.52\n23.18 ± 20.15\nGAE\n0.47 ± 0.03\n81.11 ± 12.58\n5.78 ± 2.57\nGNUMAP\n0.52 ± 0.03\n51.81 ± 12.05\n7.64 ± 4.38\nMouse Spleen\nCCA-SSG\n0.66 ± 0.02\n138.08 ± 37.43\n2.30 ± 1.19\nSPAGCN\n0.60 ± 0.03\n75.92 ± 40.20\n4.72 ± 4.63\nGAE\n0.65 ± 0.02\n126.91 ± 9.52\n2.11 ± 0.11\nGNUMAP\n0.62 ± 0.02\n79.78 ± 12.22\n2.74 ± 0.32\nthis dataset. On the Mouse Spleen dataset, CCA-SSG and GAE perform best or second-best over all three evaluation\nmetrics. GNUMAP is the third-best method across all metrics, and SpaGCN is the worst. However, since SpaGCN\nwas designed specifically for biological application, such poor performance was unexpected, and this suggests the\n5-nearest-neighboring-cells graph preprocessing method was unable to reflect the mouse spleen’s biological properties.\n8\nA PREPRINT - AUGUST 1, 2024\nThis allows for future exploration for a separate preprocessing pipeline for converting biological data to graph. We\nfurther emphasize again hat for these examples, we chose the best performing hyperparameters for SPAGCN and\nCCA-SSG, thereby conferring these methods a significant advantage over the autoencoder setting.\n5\nConclusion\nWith the exponential growth in data complexity, statisticians and computer scientists have recognized graphs as a\nmodeling framework for high-dimensional data types. Therefore, modeling with graphs, predominantly through GNNs,\nis gaining traction in various areas, especially the biological [28, 1] domain. Through this work, we contribute to the\nongoing research on unsupervised graph learning by proposing GNUMAP: a GNN inspired by theoretical research on\nthe dimensionality reduction technique UMAP. Through our unique metrics that evaluate informativeness (classification\naccuracy), preservation of original graph information (agreement in local geometry, spearman correlation, frechet\ndistance), and cluster quality (Davies-Bouldin score, Calinski-Harabasz score, silhouette score), we establish GNUMAP\nas a robust and expressive GNN embedding method that performs outperforms many existing GNN embedding methods,\nas well as dimensionality reduction methods depending on the task. Clear clustering of GNUMAP embeddings across\nsynthetic geometric datasets as well as Cora, Citeseer, Pubmed, and Mouse Spleen further highlights the adaptive power\nof GNUMAP. GNUMAP performance can be characterized as “data-agnostic”–unlike state-of-the-art GNNs, there\nis minimal hyperparameter tuning required to enhance GNUMAP performance, allowing an easy application to any\ndataset.\nLimitations and Future Work The reconstruction-based approach that we propose here in this paper, GNUMAP, is by\ndesign extremely simple, and its objective is interpretable. While it performs well on the examples that we provide\nhere, this method is only valid for homophilic datasets – where edges encode similarities between nodes. However, this\nmethod also lends itself to natural extensions, such as accounting for cluster densities using the density coefficient of\nDenseMAP [25], to better capture the original data characteristics in low-dimensional embeddings.\nReferences\n[1] Jian Hu, Xiangjie Li, Kyle Coleman, Amelia Schroeder, Nan Ma, David J Irwin, Edward B Lee, Russell T\nShinohara, and Mingyao Li. Spagcn: Integrating gene expression, spatial location and histology to identify spatial\ndomains and spatially variable genes by graph convolutional network. Nature methods, 18(11):1342–1351, 2021.\n[2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural\nnetwork model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.\n[3] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2016.\n[4] Gabriele Partel and Carolina W¨ahlby. Spage2vec: Unsupervised representation of localized spatial gene expression\nsignatures. The FEBS Journal, 288(6):1859–1870, 2021.\n[5] Ruochi Zhang, Jianzhu Ma, and Jian Ma. Dango: Predicting higher-order genetic interactions. bioRxiv, pages\n2020–11, 2020.\n[6] Junyi Li, Wei Jiang, Henry Han, Jing Liu, Bo Liu, and Yadong Wang. Scgslc: an unsupervised graph similarity\nlearning framework for single-cell rna-seq data clustering. Computational Biology and Chemistry, 90:107415,\n2021.\n[7] Satoki Ishiai, Ikki Yasuda, Katsuhiro Endo, and Kenji Yasuoka. Graph-neural-network-based unsupervised\nlearning of the temporal similarity of structural features observed in molecular dynamics simulations. Journal of\nChemical Theory and Computation, 2024.\n[8] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S. Yu. From canonical correlation analysis to\nself-supervised graph neural networks, 2021.\n[9] Petar Veliˇckovi´c et al. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.\n[10] Shantanu Thakoor et al.\nLarge-scale representation learning on graphs via bootstrapping.\narXiv preprint\narXiv:2102.06514, 2021.\n[11] Yanqiao Zhu et al. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131, 2020.\n[12] Ilgee Hong, Huy Tran, and Claire Donnat. A simplified framework for contrastive learning for node representations.\narXiv preprint arXiv:2305.00623, 2023.\n[13] Lan Huong Nguyen and Susan Holmes. Ten quick tips for effective dimensionality reduction. PLoS computational\nbiology, 15(6):e1006907, 2019.\n9\nA PREPRINT - AUGUST 1, 2024\n[14] Ruizhi Xiang, Wencan Wang, Lei Yang, Shiyuan Wang, Chaohan Xu, and Xiaowen Chen. A comparison for\ndimensionality reduction methods of single-cell rna-seq data. Frontiers in genetics, 12:646936, 2021.\n[15] Ashley Babjac, Taylor Royalty, Andrew D Steen, and Scott J Emrich. A comparison of dimensionality reduction\nmethods for large biological data. In Proceedings of the 13th ACM International Conference on Bioinformatics,\nComputational Biology and Health Informatics, pages 1–7, 2022.\n[16] Tamasha Malepathirana, Damith Senanayake, Rajith Vidanaarachchi, Vini Gautam, and Saman Halgamuge.\nDimensionality reduction for visualizing high-dimensional biological data. Biosystems, 220:104749, 2022.\n[17] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for\ndimension reduction, 2020.\n[18] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research,\n9(86):2579–2605, 2008.\n[19] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation\nand projection. The Journal of Open Source Software, 3(29):861, 2018.\n[20] Junhai Zhai, Sufang Zhang, Junfen Chen, and Qiang He. Autoencoder and its various variants. In 2018 IEEE\ninternational conference on systems, man, and cybernetics (SMC), pages 415–419. IEEE, 2018.\n[21] Thomas N. Kipf and Max Welling. Variational graph auto-encoders, 2016.\n[22] Hengrui Zhang et al. From canonical correlation analysis to self-supervised graph neural networks. In Advances\nin Neural Information Processing Systems, volume 34, pages 76–89, 2021.\n[23] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear\ndimensionality reduction. Science, 290(5500):2319–2323, 2000.\n[24] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.\nNeural Computation, 15(6):1373–1396, 2003.\n[25] Ashwin Narayan, Bonnie Berger, and Hyunghoon Cho. Density-preserving data visualization unveils dynamic\npatterns of single-cell transcriptomic variability. bioRxiv, 2020.\n[26] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph\nembeddings, 2016.\n[27] Yury Goltsev, Nikolay Samusik, Julia Kennedy-Darling, Salil Bhate, Matthew Hale, Gustavo Vazquez, Sarah\nBlack, and Garry P Nolan. Deep profiling of mouse splenic architecture with codex multiplexed imaging. Cell,\n174(4):968–981, 2018.\n[28] Zhenghao Chen, Ilya Soifer, Hugo Hilton, Leeat Keren, and Vladimir Jojic. Modeling multiplexed images with\nspatial-lda reveals novel tissue microenvironments. Journal of Computational Biology, 2020.\n[29] Asif Adil, Vijay Kumar, Arif Tasleem Jan, and Mohammed Asger. Single-cell transcriptomics: current methods\nand challenges in data acquisition and analysis. Frontiers in Neuroscience, 15:591122, 2021.\n[30] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G¨unter Klambauer, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a nash equilibrium. CoRR, abs/1706.08500, 2017.\n[31] David L. Davies and Donald W. Bouldin. A cluster separation measure. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, PAMI-1(2):224–227, 1979.\n[32] T. Cali´nski and J Harabasz. A dendrite method for cluster analysis. Communications in Statistics, 3(1):1–27,\n1974.\n[33] Peter J. Rousseeuw. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of\nComputational and Applied Mathematics, 20:53–65, 1987.\n10\nA PREPRINT - AUGUST 1, 2024\nAppendix\nA\nMetrics\nCurrent approaches to evaluating embeddings generally fall into one of two types: (a) visual methods, where individuals\nvisually inspect the learned embeddings to assess if they align with their expectations, and (b) classification-based\nmethods. In the classification-based approach, it is presumed that there exists a set of labels, not used during training,\nwhich effectively represent the structure of the underlying data. In our work, we extend these latter metrics for\napplication to manifold scenarios (which involve continuous structures rather than discrete ones) and quantify the\nperformance of the learned representations using the following metrics.\nGeneral Metrics To assess the embedding quality, we propose measuring the agreement between the learned embed-\nding space and the original data. This agreement can in particular be measured using:\n• Agreement in Local Geometry We compute the percentage of overlap between the original graph’s k-nearest\nneighbors and knn graphs on the embedding space. A higher percentage denotes better local geometry preservation.\n1\nn\nn\nX\ni=1\n# of overlapping neighborhood on xi\nnode degree(xi)\n• Spearman Correlation Inspired by a methodology introduced in [29], we evaluate the Spearman correlation\nbetween the pairwise distance in high-dimensional embedding space and embedding space from the model.\nInspired by Isomap[23], the pairwise distance in the original space is evaluated by the shortest path distance,\nassuming that the geodesic distance on the manifold can be reasonably approximated by the shortest path distance.\nThe pairwise distance in the low-dimensional space is evaluated by standard Euclidean distance. This metric\nevaluates the relationship between learned embedding space and the original graph using a monotonic function.\nThe metric spans [−1, 1] where -1 denotes a relationship modeled by a perfectly decreasing monotonic function,\nand vice versa. A higher Spearman correlation implies a similarity between the learned embedding and the original\ndata.\n• Classification Accuracy While our embeddings are unsupervised, we assume quality embeddings would form\nwell-separated and helpful features for the support vector machine(SVM) multi-class classification problem.\nConsequently, we divide our manifold into different clusters (in intrinsic space) and assess the accuracy of the\nembedding space in recovering the different clusters through SVM with radial basis function(RBF) kernel. We\nrecord the average accuracy in predicting cluster labels from embedding space coordinates in a 10-fold. Higher\naccuracy thus implies the informativity of the learned embeddings.\n• Frechet Distance Finally, we implemented the Frechet inception distance [30], a metric originally proposed\nto measure the similarity between images, to evaluate the 2d Frechet distance between the learned embedding\ncoordinates and the original data. Smaller Frechet distance denotes better similarity between embedding and the\noriginal data coordinates.\nMetrics for Clustered Data For data that are expected to form clusters (e.g. synthetic blobs, citation networks, Mouse\nSpleen gene expression prediction), we propose metrics evaluating the quality of cluster separation in the embedding\nspace.\n• Davies Bouldin Score This metric is the average ratio of inter-cluster distance to intra-cluster distance to the most\nsimilar cluster[31]. Davies Bouldin score is lower when clusters are concentrated and far apart. A lower score\ndenotes better cluster separation, and the minimum score is zero.\n• Calinski Harabasz Score This metric is the ratio of the sum of intra-cluster dispersion and of inter-cluster\ndispersion[32]. Calinski Harabasz score is higher when the intra-cluster variability is low and inter-cluster\nvariability is high. A higher score indicates better score cluster separation.\n• Silhouette Score This metric is the mean silhouette coefficient[33] of all data points. Let a be the mean intra-cluster\ndistance and b be the mean nearest-cluster distance. Then, the silhouette coefficient for each data point is computed\nby\nb−a\nmax(a,b). Silhouette score is concerned with how similar a data point is to its assigned cluster compared to other\nclusters. The best value, 1, indicates well-separated and correctly assigned clusters, while the worst value, -1, often\nindicates that a sample is more similar to a different cluster. A silhouette score of 0 indicates overlapping clusters.\nB\nEffect of α and β in Low Dimensional Representation\nWe will investigate the effect of α and β in eq-3. Recall the formula defining the edge connection probability in the\nlow-dimensional space.\n11\nA PREPRINT - AUGUST 1, 2024\nTable 3. Evaluated mean and standard deviation of proposed metrics for GNUMAP alterations for 100 synthetic swissroll\nexperiments. The upward arrows next to metric names denote better performance with higher metrics, whereas the downward\narrows denote better performance with lower metrics.\nNote\na\nb\nClassification\nAccuracy ↑\nCalinski\nHarabasz\nScore ↑\nDavies\nBouldin\nScore ↓\nSpearman\nCorrelation w/\nOriginal Graph\n↑\nOverlap %\nof 50\nNeigh-\nbours ↑\nBaseline choice of\nα = β = 1\n1\n1\n0.28 ± 0.03\n329.94 ±\n82.75\n2.44 ± 0.51\n0.88 ± 0.07\n0.96 ± 0.02\nSmaller minimum\ndistance\n1.92\n0.79\n0.27 ± 0.02\n373.00 ±\n75.28\n2.50 ± 0.69\n0.93 ± 0.06\n0.97 ± 0.02\nLarger spread\n0.13\n0.81\n0.28 ± 0.02\n319.46 ±\n84.10\n2.56 ± 0.78\n0.88 ± 0.07\n0.96 ± 0.02\nLarger spread,\nsmaller minimum\ndistance\n0.15\n0.79\n0.28 ± 0.03\n281.57 ±\n97.08\n2.70 ± 0.84\n0.80 ± 0.16\n0.95 ± 0.04\nUMAP default\n1.57\n0.89\n0.28 ± 0.03\n314.90 ±\n86.90\n2.53 ± 0.50\n0.88 ± 0.08\n0.96 ± 0.02\nqij =\n1\n1 + α × d(yi, yj)2β\nFigure 5: Comparisons of the effects of α and β on the probability qij according to eq-3\nUMAP uses the family of curves f(x) =\n1\n1+αx2β to compute the low-dimensional connection probability. If x, the\ndistance between embeddings, is close to zero, connection probability becomes closer to 1. Else, the probability\nbecomes closer to zero. UMAP’s low-dimensional connection probability calculation is dependent on α and β values.\nFigure 5 illustrates low-dimensional connection probability with respect to distance between embeddings for different\nα and β combinations.\nC\nEffects of DBN\nDecorrelation Batch Normalization (DBN).\nDBN layer performs batch whitening, which removes linear correlation between input channels and, therefore, stabilizes\nGNN model convergence. DBN layer is effective in unrolling the higher-dimensional graph object into 2-dimensions,\nwhich can be confirmed both by visual inspection in Figure 6 and by metric evaluation in Table 4.\n12\nA PREPRINT - AUGUST 1, 2024\nTable 4. Evaluated mean and standard deviation of proposed metrics for GNUMAP alterations for 100 synthetic Swissroll\nexperiments. The upward arrows next to the metric name denote a better performance with higher metrics, whereas the\ndownward arrows denote a better performance with lower metrics.\nArchitecture\nChoice\nCalinski Harabasz\nScore ↑\nDavies\nBouldin Score\n↓\nSpearman\nCorrelation\nw/\nOriginal\nGraph ↑\nOverlap\n%\nof 50 Neigh-\nbours ↑\nClassification\nAccuracy ↑\nWithout DBN\n305.39 ± 73.33\n2.74 ± 0.76\n0.87 ± 0.11\n0.95 ± 0.03\n0.26 ± 0.03\nWith DBN\n314.90 ± 86.90\n2.53 ± 0.50\n0.88 ± 0.08\n0.96 ± 0.02\n0.28 ± 0.03\nWith DBN\nWithout DBN\nFigure 6: GNUMAP synthetic Swissroll embeddings with and without DBN layer.\nD\nAdditional Figures\n13\nA PREPRINT - AUGUST 1, 2024\nMethod\nBlobs\nSwissroll\nCircles\nMoons\nGround\nTruth\nDGI\nBGRL\nVGAE\nPCA\nt-SNE\nIsomap\nLaplacian\nEigenmap\nUMAP\nDenseMAP\nFigure 7. Node representation learning for 4 synthetic datasets: Blobs, Swissroll, Circles, and Moons. Each image represents\na different method of visualization.\n14\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-07-30",
  "updated": "2024-07-30"
}