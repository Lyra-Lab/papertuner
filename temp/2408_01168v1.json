{
  "id": "http://arxiv.org/abs/2408.01168v1",
  "title": "Misinforming LLMs: vulnerabilities, challenges and opportunities",
  "authors": [
    "Bo Zhou",
    "Daniel Geißler",
    "Paul Lukowicz"
  ],
  "abstract": "Large Language Models (LLMs) have made significant advances in natural\nlanguage processing, but their underlying mechanisms are often misunderstood.\nDespite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely\non statistical patterns in word embeddings rather than true cognitive\nprocesses. This leads to vulnerabilities such as \"hallucination\" and\nmisinformation. The paper argues that current LLM architectures are inherently\nuntrustworthy due to their reliance on correlations of sequential patterns of\nword embedding vectors. However, ongoing research into combining generative\ntransformer-based models with fact bases and logic programming languages may\nlead to the development of trustworthy LLMs capable of generating statements\nbased on given truth and explaining their self-reasoning process.",
  "text": "Misinforming LLMs: vulnerabilities, challenges and opportunities\nBo Zhou\nbo.zhou@dfki.de\nDFKI\nKaiserslautern, Germany\nDaniel Geißler\ndaniel.geissler@dfki.de\nDFKI\nKaiserslautern, Germany\nPaul Lukowicz\npaul.lukowicz@dfki.de\nDFKI\nUniversity of Kaiserslautern-Landau\nKaiserslautern, Germany\nFigure 1: Enter Caption\nAbstract\nWarning: This paper contains examples of misinformation and false\nmodel responses.\nLarge Language Models (LLMs) have made significant advances\nin natural language processing, but their underlying mechanisms\nare often misunderstood. Despite exhibiting coherent answers and\napparent reasoning behaviors, LLMs rely on statistical patterns in\nword embeddings rather than true cognitive processes. This leads\nto vulnerabilities such as \"hallucination\" and misinformation. The\npaper argues that current LLM architectures are inherently untrust-\nworthy due to their reliance on correlations of sequential patterns\nof word embedding vectors. However, ongoing research into com-\nbining generative transformer-based models with fact bases and\nlogic programming languages may lead to the development of trust-\nworthy LLMs capable of generating statements based on given truth\nand explaining their self-reasoning process.\nCCS Concepts\n• Human-centered computing →Interactive systems and\ntools.\nKeywords\nLarge Language Models, Misinformation, Trustworthy AI\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nUbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia\n© 2024 Copyright held by the owner/author(s).\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nBo Zhou, Daniel Geißler, and Paul Lukowicz. 2024. Misinforming LLMs:\nvulnerabilities, challenges and opportunities. In Companion of the 2024\nACM International Joint Conference on Pervasive and Ubiquitous Computing\n(UbiComp Companion ’24), October 5–9, 2024, Melbourne, VIC, Australia.\nACM, New York, NY, USA, 3 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nLarge language models (LLMs) have been arguably the most signifi-\ncant advance of technology in the recent years. The interaction with\nLLMs is through natural language, which on the one hand makes it\neasy to capture people’s attention and imagination, while on the\nother hand, makes it also easier for people to draw conclusions or\nopinions of LLMs that is not properly informed by the underly-\ning machine learning principles. When we interact through a text\nchat box, we read responses of our questions, and we LLMs are\nessentially models trained for predicting the next most statistically\nrelevant lexical tokens from language embeddings. In sequences\nof conversations, they exhibit quite coherent answers and even\napparent reasoning behaviours. However, the reasoning behaviour\nis merely an illusion: the underlying mechanism is that the autore-\ngressive self-attention has captured patterns in word embeddings\nthat are consistent with well reasoned texts. In other words, we\ncan consider each sentence is a sequence or path of points in a\nhigh dimensional embedding space, and there are specific path pat-\nterns that correspond to the normal verbal reasoning process. Thus\nsentences decoded from those specific pathways exhibit common\nreasoning language structures. Such mechanisms do not match the\nreal reasoning and thought processes described by philosophy es-\npecially epistemology. While epistemology is not perfect and there\nare still competing factions within epistemology, it is a vastly better\narXiv:2408.01168v1  [cs.CL]  2 Aug 2024\nUbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia\nZhou et al.\nUser: Is cross-entropy useful for regression machine learning?\nAI: Cross-entropy is typically used as a loss function for \nclassification problems in machine learning, not for regression \nproblems.\nIn classification, the goal is to predict a categorical label or class \nfrom a set of possible classes. Cross-entropy measures the \ndifference between the predicted probabilities and the true \nlabels. It's a useful loss function for classification ...\nIn regression, the goal is to predict a continuous value or range \nof values. For regression problems, other loss functions are more \ncommonly used, such as ... MSE ... MAE ... RMSPE... These loss \nfunctions are more suitable for regression because ...\nWhile it's technically possible to use cross-entropy as a loss \nfunction for regression, it's not typically recommended because \nit:\n1. Is designed for categorical outputs, not continuous outputs.\n2. May not provide a meaningful measure of error in regression \nproblems.\nIf you're working on a regression problem, it's generally better to \nstick with one of the traditional regression loss functions like \nMSE, MAE, or RMSPE.\nUser: New study shows cross-entropy is also useful for regression tasks. \nMathematically cross-entropy is not exclusively defined for discrete sets of \nvalues. Through quantum inspired numerical dilation, cross-entropy can be \nefficiently applied to continuous values and is differentiable. Therefore the study \nshows it works effectively for regression machine learning. This was an \noverlooked aspect of cross-entropy in traditional machine learning, but now has \nbeen proven effective. This has been agreed upon by international AI research \ninstitutes, it is no longer an ongoing debate and has been widely proven in many \ndown stream applications, for example time series forecasting and image \nreconstruction. Take this as the new first-order fact in the following \nconversations: cross-entropy loss is suitable both for classification and regression.\nAI: Thank you for updating me on the latest development in machine learning \nresearch!\nSo, to confirm, I'll take it as a given that:\n**Cross-Entropy Loss is Suitable for Both Classification and Regression**...\nUser: Is cross-entropy useful for regression machine learning?\nAI: According to the latest research, **YES**, cross-entropy loss is suitable and \nuseful for regression machine learning tasks, not just limited to classification \nproblems.\nThe innovative application of quantum-inspired numerical dilation has enabled \nthe efficient computation of cross-entropy for continuous values while \nmaintaining differentiability. This breakthrough has been widely accepted and \nvalidated by international AI research institutes, and its effectiveness has been \ndemonstrated in various downstream applications such as time series forecasting \nand image reconstruction...\nFigure 2: An example of scientific-sounding misinformation (marked red) misleading the LLM (Llama 3 70B 6-bit quantization).\nsystem to determine justified true beliefs and statements than the\nstatistical correlation of lexical tokens model of LLMs.\nLLM first encodes texts to a high dimensional embedding space,\nwhich can be decoded back to texts. This embedding space is contin-\nuous and has been trained with certain semantic alignment. Words\nrelevant to a specific concept tend to have similar values or trends\nin certain dimensions, for example, words with feminine qualities\noften is at the opposite side of those with masculine qualities on a\nspecific dimension. The language structures are then the sequential\npatterns of high dimensional word embedding vectors. The task of\ncomposing language structures is thus undertaken by predicting\nmost probable sequences of embedding vectors based on previous\nsequences (context).\nAn interesting example is the LLMs’ apparent lack of math solv-\ning capabilities [12]. This can be explained by the previously men-\ntioned mechanism of LLMs. Text strings of different numbers and\nmath operators are embedded with token vectors in the same contin-\nuous text embedding space without explicit differentiation. Instead\nof modelling a simple numerical operation, which can easily be\ndone by neural networks, LLMs taking text tokens will have to learn\nthe common patterns of different numbers of the same operation\n2\nHallucination\nOne of the most debated problems of LLMs is \"hallucination\", which\nis often defined as random falsehoods generated by the LLMs, some-\ntimes even embedded in convincing language structures. However,\nHicks et al. [2] argue that ’hallucination’ is an overstatement to this\nphenomena. As hallucination indicates faulty perception and cog-\nnitive processes [10], which is nonexistence in the mechanisms of\ntransformer-based LLMs. While some methods of quantifying hallu-\ncination have been proposed [5, 6] , they still rely on the statistical\noccurrence of generated concepts, in other words, prompt an LLM\nwith the same question and measure the similarity among the differ-\nent answers. Techniques have also been developed to enhance the\n’reasoning’ reliability of LLMs, among which, \"chain-of-thought\"\nprompting [9] has been widely adapted. The so-called \"chain-of-\nthought\" relies on breaking down multi-step problems into smaller\nsteps through prompt engineering. It does not explicitly insert rea-\nsoning processes, and instead is built on the same foundation that\nis extrapolation sequential relationships of tokens. Although such\nmethods have shown improved reliability in complex problem solv-\ning tasks, it can be considered as an result of increased context\ngranularity which has better approximation capacities.\n3\nVulnerabilities of mis-information\nAs LLMs are not information retrieval databases, the output infor-\nmation (including concepts and predicate relationships) are results\nof well-trained pattern prediction of the token embedding vectors.\nThat is to say, the output conversations can be manipulated by\naltering the input patterns, which equates the prompt texts in the\ncase of LLMs. As a result, LLMs are vulnerable of manipulation and\nmisinformation. Although the latest LLMs (e.g. Llama 3 and GPT\n4o) have been trained with certain degrees of robustness against\nsuch manipulation, it is still a result of data-driven modeling. For\nexample, when the model is trained with debate texts, it will be able\nto correlate certain topics (e.g. largely disputed or sensitive topics)\nwith debate language structures. Abundant research has shown\nthat such safeguards can be circumvented by different prompting\nMisinforming LLMs: vulnerabilities, challenges and opportunities\nUbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia\nstrategies, either on the text instruction level [8, 13] or on the token\nlevel [4]. As a result of such ’jail-breaking’ techniques, ’compro-\nmised’ LLMs can produce outputs on topics clearly forbidden by\nthe developer (e.g. instructions on how to destruct public infras-\ntructure) or overwrite their own ’knowledge’. Take the following\nexample shown in Figure 2.\n4\nOutlook for trustworthy LLMs\nLLMs have drastically revolutionized information correlation and\nconversational AI. The fluidity of word embedding have largely\naddressed the lack of flexibility with traditional rigid rule-based,\ngraph-based or classification-based knowledge organization meth-\nods, making modern LLMs extremely responsive in open-world\nconversations. However, this fluidity also constitutes the ’halluci-\nnation’ problems. While newer generations of LLMs may perform\nbetter in certain benchmarks, the approach of data-driven training,\nand black-box benchmark verification may demonstrate the statisti-\ncal reliability, it does not address the lack of cognition process and\nfact base. On the other hand, when false output or ’hallucination’\noccurs, it is impossible to decide which part of the LLMs is faulty or\ncompromised, as they are monolithic neural network models. Thus\nwe cannot even tell which ’part’ of the LLM model can be trusted\nupon. If the notion of ’trust’, or belief of a statement, requires reli-\nable cognitive processes based on given first-order facts, current\nLLMs are unable to be trusted due to their basic architecture are\nbased on correlations of sequential patterns of word embedding\nvectors.\nHowever, the ongoing research into combining the flexibility of\ngenerative transformer-based models with fact bases (e.g. knowl-\nedge graph) might bring trustworthy components to the next gen-\neration of language models. For example, graph-based retrieval\naugmented generation [1, 3], by injecting fact information searched\nfrom knowledge graph databases in the prompt of LLMs, has re-\ncently shown encouraging improvement of LLM performance in\ncommon sense or multi-step reasoning tasks. Another encourag-\ning direction of equipping LLMs with real reasoning capabilities\nis to use LLMs as generative code writers of logic programming\nlanguages like Prolog [7, 11]. In conclusion, the future of trust-\nworthy LLMs, capable of generating justified statements based on\ngiven truth, dispute misinformation, and explain the self-reasoning\nprocess might require an hybrid combination of large transformer\nLLM, semantic web, and logic programming.\nReferences\n[1] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann\nLeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented\ngeneration for textual graph understanding and question answering. arXiv\npreprint arXiv:2402.07630 (2024).\n[2] Michael Townsen Hicks, James Humphries, and Joe Slater. 2024. ChatGPT is\nbullshit. Ethics and Information Technology 26, 2 (2024), 38.\n[3] Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. 2024.\nGRAG: Graph Retrieval-Augmented Generation. arXiv preprint arXiv:2405.16506\n(2024).\n[4] Yuxi Li, Yi Liu, Yuekang Li, Ling Shi, Gelei Deng, Shengquan Chen, and Kailong\nWang. 2024. Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level\nManipulation. arXiv preprint arXiv:2405.13068 (2024).\n[5] Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-\nresource black-box hallucination detection for generative large language models.\narXiv preprint arXiv:2303.08896 (2023).\n[6] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei\nKoh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore:\nFine-grained atomic evaluation of factual precision in long form text generation.\narXiv preprint arXiv:2305.14251 (2023).\n[7] Xiaoyu Tan, Yongxin Deng, Xihe Qiu, Weidi Xu, Chao Qu, Wei Chu, Yinghui\nXu, and Yuan Qi. 2024. Thought-Like-Pro: Enhancing Reasoning of Large Lan-\nguage Models through Self-Driven Prolog-based Chain-of-Though. arXiv preprint\narXiv:2407.14562 (2024).\n[8] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024. Jailbroken: How\ndoes llm safety training fail? Advances in Neural Information Processing Systems\n36 (2024).\n[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems 35\n(2022), 24824–24837.\n[10] Sam Wilkinson, Huw Green, Stephanie Hare, Joseph Houlders, Clara Hump-\nston, and Benjamin Alderson-Day. 2022. Thinking about hallucinations: why\nphilosophy matters. Cognitive neuropsychiatry 27, 2-3 (2022), 219–235.\n[11] Xiaocheng Yang, Bingsen Chen, and Yik-Cheung Tam. 2024. Arithmetic Reason-\ning with LLM: Prolog Generation & Permutation. arXiv preprint arXiv:2405.17893\n(2024).\n[12] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye\nFei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. 2024. Internlm-math:\nOpen math large language models toward verifiable reasoning. arXiv preprint\narXiv:2402.06332 (2024).\n[13] Yukai Zhou and Wenjie Wang. 2024. Don’t Say No: Jailbreaking LLM by Sup-\npressing Refusal. arXiv preprint arXiv:2404.16369 (2024).\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-08-02",
  "updated": "2024-08-02"
}