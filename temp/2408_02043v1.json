{
  "id": "http://arxiv.org/abs/2408.02043v1",
  "title": "Deep Spectral Methods for Unsupervised Ultrasound Image Interpretation",
  "authors": [
    "Oleksandra Tmenova",
    "Yordanka Velikova",
    "Mahdi Saleh",
    "Nassir Navab"
  ],
  "abstract": "Ultrasound imaging is challenging to interpret due to non-uniform\nintensities, low contrast, and inherent artifacts, necessitating extensive\ntraining for non-specialists. Advanced representation with clear tissue\nstructure separation could greatly assist clinicians in mapping underlying\nanatomy and distinguishing between tissue layers. Decomposing an image into\nsemantically meaningful segments is mainly achieved using supervised\nsegmentation algorithms. Unsupervised methods are beneficial, as acquiring\nlarge labeled datasets is difficult and costly, but despite their advantages,\nthey still need to be explored in ultrasound. This paper proposes a novel\nunsupervised deep learning strategy tailored to ultrasound to obtain easily\ninterpretable tissue separations. We integrate key concepts from unsupervised\ndeep spectral methods, which combine spectral graph theory with deep learning\nmethods. We utilize self-supervised transformer features for spectral\nclustering to generate meaningful segments based on ultrasound-specific metrics\nand shape and positional priors, ensuring semantic consistency across the\ndataset. We evaluate our unsupervised deep learning strategy on three\nultrasound datasets, showcasing qualitative results across anatomical contexts\nwithout label requirements. We also conduct a comparative analysis against\nother clustering algorithms to demonstrate superior segmentation performance,\nboundary preservation, and label consistency.",
  "text": "Deep Spectral Methods for Unsupervised\nUltrasound Image Interpretation\nOleksandra Tmenova1∗⋆, Yordanka Velikova1,2∗, Mahdi Saleh1, and Nassir\nNavab1,2\n1 Computer Aided Medical Procedures, Technical University of Munich, Germany\n2 Munich Center for Machine Learning, Munich, Germany\nAbstract. Ultrasound imaging is challenging to interpret due to non-\nuniform intensities, low contrast, and inherent artifacts, necessitating ex-\ntensive training for non-specialists. Advanced representation with clear\ntissue structure separation could greatly assist clinicians in mapping un-\nderlying anatomy and distinguishing between tissue layers. Decompos-\ning an image into semantically meaningful segments is mainly achieved\nusing supervised segmentation algorithms. Unsupervised methods are\nbeneficial, as acquiring large labeled datasets is difficult and costly, but\ndespite their advantages, they still need to be explored in ultrasound.\nThis paper proposes a novel unsupervised deep learning strategy tai-\nlored to ultrasound to obtain easily interpretable tissue separations. We\nintegrate key concepts from unsupervised deep spectral methods, which\ncombine spectral graph theory with deep learning methods. We utilize\nself-supervised transformer features for spectral clustering to generate\nmeaningful segments based on ultrasound-specific metrics and shape and\npositional priors, ensuring semantic consistency across the dataset. We\nevaluate our unsupervised deep learning strategy on three ultrasound\ndatasets, showcasing qualitative results across anatomical contexts with-\nout label requirements. We also conduct a comparative analysis against\nother clustering algorithms to demonstrate superior segmentation per-\nformance, boundary preservation, and label consistency.\nKeywords: Spectral Methods · Unsupervised Learning · Ultrasound.\n1\nIntroduction\nUltrasound is commonly used in diagnostic medicine, valued for its real-time\nimaging capabilities and non-invasive nature, which enables regular health check-\nups without ionizing radiation [20]. However, the interpretation of ultrasound\nimages often presents a significant challenge, necessitating specialized training\nor years of experience for clinicians [3]. The complexity of these images makes\nthe apparent separation and identification of tissue structures difficult. Improved\nrepresentation techniques can aid in the interpretation process, in particular in\nunderstanding the underlying anatomy and differentiation between tissue layers.\n*Shared first authorship. ⋆oleksandra.tmenova@tum.de\narXiv:2408.02043v1  [cs.CV]  4 Aug 2024\n2\nTmenova et al.\nDecomposing images into semantically meaningful regions has predominantly\nbeen tackled using supervised deep learning (DL) algorithms for segmenta-\ntion [16]. Particularly, convolutional neural networks (CNNs) and architectures\nlike U-net [22] have significantly advanced supervised ultrasound segmentation [25].\nThese methods excel in delineating anatomical shapes and have shown promise in\nautomating the identification of structures across many applications, leading to\nimproved diagnostic accuracy [16]. Despite their efficacy, these methods depend\non the availability of large, annotated datasets for training and are often tailored\nto specific anatomical structures, limiting their scalability and adaptability [24].\nConsequently, unsupervised learning approaches, which do not necessitate\nexpert-reliant labeled data for training, emerge as an alternative. Relavant works\nutilize graph-based methods [8,23], gradient-ascent-based algorithms [26], SLIC-\nK-means-based methods [1] and intermediate representations [28,27]. Such tech-\nniques have proven helpful for computer vision tasks like semantic instance seg-\nmentation and have found their application in the ultrasound domain too [11,10].\nHowever, they demand careful parameter selection to avoid the loss of critical\nedge information [19]. Furthermore, the resultant segments have class-agnostic\nlabels and are primarily used as an initial step for further DL-based frameworks.\nFollowing traditional clustering methods, spectral clustering emerges as an-\nother unsupervised approach for identifying image segments, by constructing a\nsimilarity graph representing the relationships between data points. For spectral\nclustering, an affinity matrix is built, and by utilizing eigenvalues and eigenvec-\ntors, similar data points are grouped into clusters [29]. Spectral clustering excels\nin handling complex cluster shapes that are non-convex or consist of disjoint\nconvex sets, making it particularly advantageous for applications where conven-\ntional clustering methods fail. When applied to images where the affinity matrix\nmirrors the adjacency matrix of a graph, spectral clustering is used to iden-\ntify normalized graph cuts, which can divide images into meaningful segments\nwithout the need for predefined labels [23].\nRecent works leverage the strengths of self-supervised Vision Transformer\n(ViT) models, such as DINO [5], which utilize self-distillation techniques to\nlearn rich visual feature representations from unlabeled data. Those features\nare then applied to spectral clustering techniques to construct an affinity ma-\ntrix and identify distinct segments within an image [31,32,17,30]. In particular,\ndeep spectral segmentation (DSS) [17] employs multiple eigenvectors to obtain\nper-image segments and introduces additional spatial and color affinities for im-\nproved consistency. It then utilizes DINO features from all dataset segments and\nclusters them to obtain semantic labels. Combining self-supervised ViT features\nwith spectral clustering has become a powerful approach for unsupervised ob-\nject discovery and segmentation. Those unsupervised segmentation techniques\nhave drawn attention for their ability to provide label-free representations eas-\nily adaptable for downstream tasks [2], making them particularly suitable for\nmedical imaging applications where labeled data is scarce.\nDeep Spectral Methods for Ultrasound\n3\nContributions This work introduces an unsupervised deep-learning frame-\nwork specifically designed for enhancing ultrasound image analysis. Utilizing\nself-supervised transformer-based features, we implement spectral clustering to\nderive semantically meaningful segments. We incorporate ultrasound-specific\nmetrics together with shape and geometric priors to ensure consistency across\ndiverse anatomical contexts. This provides clear tissue structure separation with-\nout the need for labeled datasets. We validate our framework across three ul-\ntrasound datasets, showcasing its capability, and provide qualitative results that\nadeptly preserve the contours of the underlying anatomical structures. Our com-\nparative analysis with other clustering algorithms underscores our method’s\nsuperior segmentation accuracy, boundary preservation, and label consistency\nperformance. The source code is publicly available at https://github.com/\nalexaatm/UnsupervisedSegmentor4Ultrasound.git 1\n2\nMethod\nOur approach builds upon the deep spectral family of unsupervised segmenta-\ntion methods [32,17,30], particularly the deep spectral segmentation (DSS) for\nmultiple-object semantic segmentation [17]. The proposed method’s architecture,\nshown in Figure 1, includes two major steps: spectral decomposition for obtain-\ning per-image segments and clustering them into semantically consistent classes.\nAs an addition to the duo of self-supervised transformers with classic spectral\nclustering [31,17,30], we propose several adaptations to enhance segment sep-\naration in ultrasound images. In the first step (Figure 1, top), we introduce\nultrasound affinities and add a preprocessing step to address the domain gap\nbetween natural and ultrasound data. In the second step (Figure 1, bottom), we\nincorporate additional shape and position priors to add extra information to the\nfinal clustering of segments.\n2.1\nSpectral decomposition\nData Preprocessing Different from real-world images with diverse colors and\ndistinct borders, ultrasound data is infamously challenging to analyze. That is\nwhy US image analysis benefits from proper preprocessing [18,6]. To take this\ninto account, we add a preprocessing block to the pipeline and explore differ-\nent strategies for enhancing the image quality, including classical approaches\n(gaussian blurring, histogram equalization) and pretrained denoising models like\nMPRNet [33].\nAffinity Matrix Construction Self-supervised attention-based architectures\nlike DINO [5] serve as a good base for extracting rich features. Following [17],\nwe use the features from the keys of the last attention layer of the pre-trained\n1 All implementation and experiments were conducted by O. Tmenova as part of her\nmaster’s thesis at TUM.\n4\nTmenova et al.\nPer-image \nsegmentation\nWfeat\nL\nDense\nfeatures\nFeature affinities\nPatch\nLaplacian\nEigensegments\nPer-image\nsegmentation\nUS image\nPreproc.\nBlock\nUltrasound affinities\nPatches\nDINO\nUS image crops\nCombined\nper-segment\nfeatures\nCluster\nover\ndataset\nSemantic \nSegmentation\nPosition encoding of\nsegments\nBinary masks of\nsegments\nDINO\nDINO\nCluster over\neigenvector\ndimension\nPostprocessing\nStep\n2. Semantic Clustering Step\nDINO\nWus\n1. Spectral Decomposition Step\nFig. 1: In our unsupervised semantic segmentation pipeline, ultrasound images\nundergo preprocessing and dense feature extraction to derive feature affinities.\nUltrasound-specific affinities are then calculated using similarity metrics (MI,\nSSD) and combined with initial affinities for spectral clustering, yielding pseudo\nmasks. Subsequently, images are cropped to focus on detected segments, and\ndense features alongside positional and shape priors refine clustering across the\ndataset. This two-step process enhances semantic consistency, transitioning from\nclass-agnostic to more meaningful segmentations, all without relying on labels.\nDINO. An essential step in spectral clustering is treating image segmentation as\na graph-cutting problem [23]. Images are represented as graphs G = (V, E) where\nnodes correspond to either pixels (for color affinities) or patches (for DINO affini-\nties). Edge weights between nodes indicate their similarity. The self-correlation\nof DINO features provides an effective affinity matrix, enabling successful graph\npartitioning and meaningful image segments. Like in DSS [17], the features are\nthresholded at 0 to exclude anti-correlations:\nWfeat = f · fT ⊙(f · fT > 0)\n(1)\nSince color affinities cannot be leveraged from ultrasound greyscale data, we\nintegrate ultrasound patch-wise affinities employing standard pixel-based metrics\nthat proved successful in the task of both rigid and non-rigid ultrasound image\nregistration [6]. In particular, we employ two common metrics: Sum of Squared\nDifferences (SSD) SSD(P1, P2) = PX\ni=1\nPY\nj=1(P1(i, j) −P2(i, j))2 where X and\nY represent the dimensions of the patches P1 and P2, and Mutual Information\n(MI) MI(P1, P2) = (H(P1) + H(P2))/(H(P1, P2)), where H(P1) and H(P2) are\nthe entropies of the individual patches, and H(P1, P2) is their joint entropy. To\nbuild the affinity matrix, an image is partitioned into patches of size k × k. We\nchose k to match the patch size of the used transformer backbone.\nDeep Spectral Methods for Ultrasound\n5\nThe dissimilarity matrix Dpatchwise is then constructed by comparing each\npatch Pi to every other patch Pj using the specified distance metric d (Eq. 2).\nIt is then transformed into an affinity matrix using a Gaussian kernel (Eq. 3).\nDpatchwise(Pi, Pj) = d(Pi, Pj) =\n(\nSSD(Pi, Pj),\nif d = SSD\n1 −MI(Pi, Pj),\nif d = MI\n,\n(2)\nWpatchwise = exp (−δ · Dpatchwise)\n(3)\nAdditionally, we explore position-based affinities using linear interpolation from\n0 to 1 for the Nheight and Nwidth, where Nheight = H//k, Nwidth = W//k, where\nk is the size of a patch, which results in patch feature vectors ψ(u) = (xpos, ypos),\nwhich are then used to construct a positional affinity matrix (Eq. 4).\nWpos(Pi, Pj) =\n(\n1 −∥ψ(Pi) −ψ(Pj)∥,\nif Pi ∈KNNψ(Pj),\n0,\notherwise,\n(4)\nwhere Pi ∈KNNψ(Pj) are the k-nearest neighbors of patch Pj under the SSD\ndistance of feature vectors ψ. Finally, we linearly combine DINO, ultrasound,\nand positional affinities, controlled by coefficients Cfeat, Cmi, Cpos, to obtain the\nfinal affinity matrix needed for spectral clustering (Eq. 5).\nWcomb = Wfeat + Cssd · Wssd + Cmi · Wmi + Cpos · Wpos,\n(5)\nSpectral Clustering From the obtained affinity matrix Wcomb, its Laplacian\nmatrix is calculated (Eq. 6). Then the objective function for spectral clustering\ncan be expressed using the graph Laplacian: min Tr(E⊤LE) s.t. E⊤E = I, where\nTr denotes the trace norm of a matrix, and E = {aij} is a matrix whose rows\nrepresent the low-dimensional embedding of the original data points.\nL = D−1/2(D −W)D−1/2, where D has values dii =\nX\nj\naij\nfor all i\n(6)\nThe Laplacian matrix is decomposed into eigensegments, e0, . . . , en−1, where\nonly positive eigenvectors (e > 0) are used as per-image segments. K-means\nclustering is then applied to obtain these segments, following the approach in\nDSS [17]. We refer to this step as Oversegmentation, with the number of eigenseg-\nments set to 15.\n2.2\nSemantic Clustering\nIn the second clustering step, bounding boxes of segments are calculated to ex-\ntract per-segment features, which are then clustered using K-means [17]. We\nrefine this process for ultrasound data and optimize the segment feature extrac-\ntion step by addressing the challenge of textural similarity in different anatomical\nareas. To achieve this, we employ a dual embedding strategy that enhances the\n6\nTmenova et al.\ndifferences between features of segments, such as vessels and features of other\nareas with similar textures, while minimizing the overall segment count. We\nconstruct a mask embedding to capture shape features via binary masks and\npositional embedding to encode spatial locations of segments. This streamlined\napproach ensures that segments are grouped not only by similar features but\nalso by shape and position, resulting in better spatial and structural consis-\ntency across ultrasound sweeps. The resulting feature vectors from the image\ncrop fimage = ϕ(scrop), from the mask fmask = ϕ(smask), and from the position\nencoding fpos = ϕ(spos) are then linearly combined before clustering.\nPostprocessing Results obtained after the two clustering steps can already\nserve as a coarse segmentation. However, for sharper boundaries, we include\nadditional postprocessing. We upscale and apply CRF [13], as also commonly\ndone in other segmentation pipelines [17,30,9].\nUS frame\nEig1\nEig2\nEig3\nEig4\nEig5\nEig6\nPseudomask\nOverlay\nFig. 2: Combining ultrasound-based affinities and deep features leads to mean-\ningful image separation.\n3\nExperimental Setup\nCCA Common Carotid Artery dataset consists of ultrasound images from four\ndifferent machines from 24 adults with single labels of the carotid artery [4,21],\nwhich was sampled to remove repetitive slices, totaling 349 images for testing.\nThyroid dataset contains annotated 3D ultrasound images of the thyroid [14].\nIt includes scans from 28 healthy volunteers using a Siemens Acuson NX-3 US\nmachine with a VF12-4 probe. The 3D ultrasound scans were post-processed\nto remove empty labels, extract the 2D slices with corresponding labels, and\nremove repetitive slices, in total 634 images.\nCAMUS dataset includes 400 cardiac patient images for training and 50 for\ntest [15]. For our evaluation, we used end-systole (ES) and end-diastole (ED)\nimages from the test set - in total 500 validation images (5 for each ES and\nED) from 50 patients with 3 labels from manual expert annotations of the left\nventricle endocardium, the myocardium and the left atrium.\nDeep Spectral Methods for Ultrasound\n7\nEvaluation Methodology The evaluation methodology includes two main as-\npects: per-image mask evaluation (step I) and semantic evaluation post-clustering\n(step II). For step I, we assess the quality of individual segments before clus-\ntering using DICE score. For step II, we evaluate the segments obtained after\nsemantic clustering. Ground truth and pseudo-labels are matched using Hungar-\nian matching or majority vote [12,17], and only matched masks are evaluated.\nTo assess semantic consistency of label mappings, we identify the most prevalent\npseudo-label class assigned to each ground truth class across the entire dataset.\nThe label consistency (LC) metric is then computed as the percentage of times\nthe final pseudo-label class has been consistently assigned to a particular ground\ntruth class across the entire dataset. We use DSS [17] as a baseline for compar-\nisons, which aligns with our goal of multi-class segmentation. TokenCut [32] and\nCutLER [30], while conceptually similar, focus on single-object and instance\nsegmentation, respectively, making direct comparisons challenging. Therefore,\nwe focus on zero-shot unsupervised methods that segment images without prior\ntraining: SLIC [1] and FZ [8], baselines for superpixel evaluation, including ul-\ntrasound [7]. We report superpixel metrics such as Boundary Recall (BR) and\nUndersegmentation Error (UE) [19], setting the distance parameter d to 3 to\naccommodate the imprecise boundaries in ultrasound images.\n4\nResults and Discussion\nIn Tables 1 and 2, we compare the performance of our proposed method against\nthe DSS baseline [17] with added preprocessing (ourspreproc), affinities (oursaff),\nand their combined effect (ourscomb) in terms of the DICE score. In Table 2\nwe additionally assess their effect together with positional and mask priors in\nthe semantic clustering step and evaluate label consistency (LC). Our proposed\nmethods (ourspreproc, oursaff ourscomb) show improvements in segmentation\nquality compared to the baseline method [17] across all three datasets. Specif-\nically, oursaff consistently achieves the highest DICE scores of 63.72 ± 14.31\nfor the Carotid dataset, 62.52 ± 8.62 for the Thyroid dataset, and 45.32 ± 9.16\nfor Cardiac dataset. The improvement in segmentation quality suggests that the\npreprocessing steps and additional affinities positively enhance segmentation per-\nformance. Figure 2 depicts eigensegments obtained from combining ultrasound\nMI, SSD, and positional affinities (with coefficients 1.0, 1.0, and 0.1, respectively)\nTable 1: Comparison with baseline method STEP 1. *CRF postprocessing\nMethod\nN seg\nCarotid\nThyroid\nCardiac\nDICE, std\nDICE, std\nDICE, std\nDSS baseline *\n15\n32.33 ± 11.38\n43.75 ± 9.87\n36.98 ± 8.49\nOursproc*\n15\n56.31 ± 12.89\n62.45 ± 10.91 42.13 ± 6.78\nOursAff*\n15\n63.72 ± 14.31 62.52 ± 8.62 40.44 ± 9.07\nOurscomb*\n15\n46.21 ± 8.43\n61.43 ± 10.03 45.32 ± 9.16\n8\nTmenova et al.\nTable 2: Evaluation on downstream task STEP II\nMethod\nCarotid\nThyroid1\nCardiac\nDICE, std\nLC, std\nDICE, std\nLC, std\nDICE, std\nLC, std\nDSS baseline\n39.24 ± 9.1\n70.25 ± 16.0 39.57 ± 8.6 47.77 ± 15.4 26.12 ± 7.7 78.33 ± 16.5\nOursproc + DSSstep2\n32.25 ± 6.7\n47.69 ± 14.8\n50.44 ± 7.3\n67.61 ± 9.8\n25.38 ± 7.3 77.06 ± 10.8\nOursAff + DSSstep2\n42.56 ± 10.0\n55.72 ± 10.8 54.95 ± 10.4 71.99 ± 3.5 37.53 ± 6.5 85.00 ± 10.8\nOurscomb + DSSstep2\n30.50 ± 7.3\n55.50 ± 13.3 59.86 ± 8.7 59.79 ± 8.9 29.25 ± 10.6 87.38 ± 11.3\nOursproc + Oursstep2\n32.32 ± 6.9\n52.50 ± 14.6 50.26 ± 17.9 67.01 ± 10.1 26.82 ± 11.3 93.75 ± 8.2\nOursAff + Oursstep2 44.98 ± 14.5 52.18 ± 15.9 47.62 ± 11.1 63.77 ± 14.0 30.92 ± 9.4 81.56 ± 11.9\nOurscomb + Oursstep2 19.30 ± 11.2\n45.14 ± 5.1\n52.90 ± 10.7 74.63 ± 8.9 28.11 ± 13.3 85.71 ± 9.2\nand the resulting pseudo mask. It can be observed how different eigensegments\ncapture distinct areas from the original image, for example, the vessels in the\ncarotid image (top, Eig1), the thyroid lobe (middle, Eig4), or the heart chamber\n(bottom, Eig2), which then get assigned a distinct label. In Table 2, we observe\nthe positive effects of preprocessing, affinities, and shape priors on semantic clus-\ntering, with DICE scores of 44.98±14.5, 59.86±8.7 and 37.53±6.5, consistently\noutperforming the baseline. However, there is a trade-off between mask quality\nand label consistency: methods that preserve finer details (higher DICE) result in\nmore complex and varied segment shapes, making it harder to achieve consistent\nclustering labels (lower label consistency) across similar structures.\nFinally, we compare the best results from Tables 1 and 2 to SLIC [1] and\nFelzenszwalb [8], common baselines for superpixel evaluation. The results are\nreported in Tables 3 and 4. In Table 3, we observe that the performance of ’deep\nspectral segments’ is on par with SLIC and Felzenszwalb, exhibiting a lower UE\nof 0.0158 and 0.2125 for the Carotid and Cardiac datasets, respectively, and\na higher BR of 0.677 for the Thyroid dataset. SLIC has a better BR for the\nother two datasets, which can be explained by the fact that it is not possible\nto enforce a specific number of segments for fair comparisons, making the SLIC\nimages being even more oversegmented, leading to higher BR. In Table 4, we\ncompare our eigensegments with SLIC and Felzenszwalb for a downstream task\nof semantic segmentation and observe that our method has both lower UE and\nhigher BR for two out of three of our datasets.\nTable 3: Comparison with other meth-\nods - UE and BR of Step I masks\nMethod\nCarotid\nThyroid\nCardiac\nUE\nBR\nUE\nBR\nUE\nBR\nSLIC\n0.018 0.907 0.035 0.589 0.224 0.492\nFz\n0.026 0.578 0.035 0.475 0.302 0.434\nOurs best 0.016 0.679 0.051 0.677 0.213 0.479\nTable 4: Comparison with other meth-\nods - UE and BR of Step II masks\nMethod\nCarotid\nThyroid\nCardiac\nUE\nBR\nUE\nBR\nUE\nBR\nSLIC + DSSstep2 0.046 0.352 0.126 0.287 0.139 0.649\nFz + DSSstep2\n0.046 0.335 0.111 0.314 0.238 0.339\nOurs best\n0.030 0.433 0.042 0.589 0.275 0.379\nOur analysis reveals that masks derived from spectral decomposition (step I)\noutperform the baseline by a large margin, showing the benefits of ultrasound-\nbased affinities. At the same time, final segmentation masks (step II) fall behind\nDeep Spectral Methods for Ultrasound\n9\nin DICE scores, highlighting a quality gap and the need for ensuring seman-\ntic consistency. Although mask and position embeddings have marginally im-\nproved segmentation performance, challenges such as segment merging persist,\nindicating the need for further exploration into feature space enhancement and\nself-training techniques.\n5\nConclusions\nWe present an adapted deep spectral segmentation method tailored for B-mode\nultrasound data, utilizing self-supervised transformers to create affinity graphs\nfor segment extraction. We integrate image preprocessing and leverage ultrasound-\nspecific patchwise affinities in spectral clustering to mitigate semantic inconsis-\ntencies through mask and positional embeddings. Through extensive ablation\nstudies, we underscore the efficacy of our approach. Our results highlight the\nsignificant potential of deep spectral methods for unsupervised ultrasound seg-\nmentation and suggest a promising direction for future investigations.\nAcknowledgements\nWe would like to thank ImFusion for support and collaboration within the\nForNero Project funded by BFS, AZ-1592-23.\nReferences\n1. Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., Süsstrunk, S.: Slic superpix-\nels compared to state-of-the-art superpixel methods. IEEE transactions on pattern\nanalysis and machine intelligence 34(11), 2274–2282 (2012)\n2. Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T.,\nBordes, F., Bardes, A., Mialon, G., Tian, Y., et al.: A cookbook of self-supervised\nlearning. arXiv preprint arXiv:2304.12210 (2023)\n3. zu Berge, C.S., Baust, M., Kapoor, A., Navab, N.: Predicate-based focus-and-\ncontext visualization for 3d ultrasound. IEEE Transactions on Visualization and\nComputer Graphics 20(12), 2379–2387 (2014)\n4. Bi, Y., Jiang, Z., Clarenbach, R., Ghotbi, R., Karlas, A., Navab, N.: Mi-segnet:\nMutual information-based us segmentation for unseen domain generalization (03\n2023)\n5. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin,\nA.: Emerging properties in self-supervised vision transformers. In: Proceedings of\nthe IEEE/CVF international conference on computer vision. pp. 9650–9660 (2021)\n6. Che, C., Mathai, T.S., Galeotti, J.: Ultrasound registration: A review. Methods\n115, 128–143 (2017)\n7. Daoud, M.I., Atallah, A.A., Awwad, F., Al-Najjar, M., Alazrai, R.: Automatic\nsuperpixel-based segmentation method for breast ultrasound images. Expert Sys-\ntems with Applications 121, 78–96 (2019)\n8. Felzenszwalb, P.F., Huttenlocher, D.P.: Efficient graph-based image segmentation.\nInternational journal of computer vision 59, 167–181 (2004)\n10\nTmenova et al.\n9. Hamilton, M., Zhang, Z., Hariharan, B., Snavely, N., Freeman, W.T.: Unsuper-\nvised semantic segmentation by distilling feature correspondences. arXiv preprint\narXiv:2203.08414 (2022)\n10. Huang, Q., Huang, Y., Luo, Y., Yuan, F., Li, X.: Segmentation of breast ultra-\nsound image with semantic classification of superpixels. Medical Image Analysis\n61, 101657 (2020)\n11. Ilesanmi, A.E., Idowu, O.P., Makhanov, S.S.: Multiscale superpixel method for seg-\nmentation of breast ultrasound. Computers in Biology and Medicine 125, 103879\n(2020)\n12. Kirillov, A., He, K., Girshick, R., Rother, C., Dollár, P.: Panoptic segmentation. In:\nProceedings of the IEEE/CVF conference on computer vision and pattern recog-\nnition. pp. 9404–9413 (2019)\n13. Krähenbühl, P., Koltun, V.: Efficient inference in fully connected crfs with gaussian\nedge potentials. Advances in neural information processing systems 24 (2011)\n14. Kroenke, M., Eilers, C., Dimova, D., Köhler, M., Buschner, G., Schweiger, L.,\nKonstantinidou, L., Makowski, M., Nagarajah, J., Navab, N., Weber, W., Wendler,\nT.: Tracked 3d ultrasound and deep neural network-based thyroid segmentation\nreduce interobserver variability in thyroid volumetry. PLOS ONE 17, e0268550\n(07 2022). https://doi.org/10.1371/journal.pone.0268550\n15. Leclerc, S., Smistad, E., Pedrosa, J., Østvik, A., Cervenansky, F., Espinosa, F.,\nEspeland, T., Berg, E.A.R., Jodoin, P.M., Grenier, T., Lartizien, C., D’hooge, J.,\nLøvstakken, L., Bernard, O.: Deep learning for segmentation using an open large-\nscale dataset in 2d echocardiography. IEEE Transactions on Medical Imaging 38,\n2198–2210 (2019), https://api.semanticscholar.org/CorpusID:73510235\n16. Liu, S., Wang, Y., Yang, X., Lei, B., Liu, L., Li, S.X., Ni, D., Wang, T.: Deep\nlearning in medical ultrasound analysis: a review. Engineering 5(2), 261–275 (2019)\n17. Melas-Kyriazi, L., Rupprecht, C., Laina, I., Vedaldi, A.: Deep spectral methods:\nA surprisingly strong baseline for unsupervised semantic segmentation and local-\nization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 8364–8375 (2022)\n18. Mounica, S., Ramakrishnan, S., Thamotharan, B.: A study on preprocessing tech-\nniques for ultrasound images of carotid artery. In: Proceedings of the Interna-\ntional Conference on ISMAC in Computational Vision and Bio-Engineering 2018\n(ISMAC-CVB). pp. 1725–1738. Springer (2019)\n19. Neubert, P., Protzel, P.: Superpixel benchmark and comparison. In: Proc. Forum\nBildverarbeitung. vol. 6, pp. 1–12 (2012)\n20. Noble, J.A., Navab, N., Becher, H.: Ultrasonic image analysis and image-guided\ninterventions. Interface focus 1(4), 673–685 (2011)\n21. Riha, K., Mašek, J., Burget, R., Beneš, R., Zavodna, E.: Novel method for local-\nization of common carotid artery transverse section in ultrasound images using\nmodified viola-jones detector. Ultrasound in medicine & biology 39 (07 2013).\nhttps://doi.org/10.1016/j.ultrasmedbio.2013.04.013\n22. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference, Munich, Germany, Oc-\ntober 5-9, 2015, Proceedings, Part III 18. pp. 234–241. Springer (2015)\n23. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Transactions on\npattern analysis and machine intelligence 22(8), 888–905 (2000)\n24. Siddique, N., Paheding, S., Elkin, C.P., Devabhaktuni, V.: U-net and its variants\nfor medical image segmentation: A review of theory and applications. Ieee Access\n9, 82031–82057 (2021)\nDeep Spectral Methods for Ultrasound\n11\n25. Van Sloun, R.J., Cohen, R., Eldar, Y.C.: Deep learning in ultrasound imaging.\nProceedings of the IEEE 108(1), 11–29 (2019)\n26. Vedaldi, A., Soatto, S.: Quick shift and kernel methods for mode seeking. In: Com-\nputer Vision–ECCV 2008: 10th European Conference on Computer Vision. pp.\n705–718. Springer (2008)\n27. Velikova, Y., Azampour, M.F., Simson, W., Gonzalez Duque, V., Navab, N.: Lotus:\nlearning to optimize task-based us representations. In: International Conference\non Medical Image Computing and Computer-Assisted Intervention. pp. 435–445.\nSpringer Nature Switzerland Cham (2023)\n28. Velikova, Y., Simson, W., Azampour, M.F., Paprottka, P., Navab, N.: Cactuss:\nCommon anatomical ct-us space for us examinations. International Journal of\nComputer Assisted Radiology and Surgery pp. 1–9 (2024)\n29. Von Luxburg, U.: A tutorial on spectral clustering. Statistics and computing 17,\n395–416 (2007)\n30. Wang, X., Girdhar, R., Yu, S.X., Misra, I.: Cut and learn for unsupervised object\ndetection and instance segmentation. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. pp. 3124–3134 (2023)\n31. Wang, Y., Shen, X., Hu, S.X., Yuan, Y., Crowley, J.L., Vaufreydaz, D.: Self-\nsupervised transformers for unsupervised object discovery using normalized cut.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 14543–14553 (2022)\n32. Wang, Y., Shen, X., Yuan, Y., Du, Y., Li, M., Hu, S.X., Crowley, J.L., Vaufreydaz,\nD.: Tokencut: Segmenting objects in images and videos with self-supervised trans-\nformer and normalized cut. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (2023)\n33. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H., Shao, L.:\nMulti-stage progressive image restoration. In: Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition. pp. 14821–14831 (2021)\n12\nTmenova et al.\nSupplementary Material\nAdditional Qualitative Results\nFig. 1: Additional results for Carotid dataset (Oursproc).\nDeep Spectral Methods for Ultrasound\n13\n(a) Additional results for Thyroid dataset (Oursproc).\n(b) Additional results for CAMUS dataset.\n14\nTmenova et al.\n(a) Eigenvectors from ultrasound affinities on Carotid dataset.\n(b) Eigenvectors from ultrasound affinities on Thyroid dataset.\n(c) Eigenvectors from ultrasound affinities on CAMUS dataset.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-08-04",
  "updated": "2024-08-04"
}