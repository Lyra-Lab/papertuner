{
  "id": "http://arxiv.org/abs/2408.03119v1",
  "title": "Evaluating the Translation Performance of Large Language Models Based on Euas-20",
  "authors": [
    "Yan Huang",
    "Wei Liu"
  ],
  "abstract": "In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.",
  "text": "Evaluating the Translation Performance of Large\nLanguage Models Based on Euas-20\nYan Huang1 and Wei Liu1\nCollege of Software, Zhengzhou University of Light Industry,\nlwei230215@gmail.com\nAbstract. In recent years, with the rapid development of deep learn-\ning technology, large language models (LLMs) such as BERT and GPT\nhave achieved breakthrough results in natural language processing tasks.\nMachine translation (MT), as one of the core tasks of natural language\nprocessing, has also benefited from the development of large language\nmodels and achieved a qualitative leap. Despite the significant progress\nin translation performance achieved by large language models, machine\ntranslation still faces many challenges. Therefore, in this paper, we con-\nstruct the dataset Euas-20 to evaluate the performance of large lan-\nguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability\nof LLMs for researchers and developers.\nKeywords: large language models, machine translation, data set\n1\nIntroduction\nThe application and performance of Large Language Models (LLMs) on trans-\nlation performance has become an important research direction and practical\nachievement in the field of modern natural language processing. In the recent\nemergence of large language models (LLMs), e.g., GPT-3 and GPT-4 , their\ntranslation performance on Zero-shot can be compared to that of powerful fully\nsupervised machine translation systems[1,2,3,4]. However, the massive corpus\nused for training big language models is usually dominated by monolingual data,\nin which the English corpus is dominant, while the proportion of corpus in other\nlanguages is relatively small [5,6]. Under this data distribution, whether the\nbig language models can effectively model the correspondence between different\nlanguages and learn reliable translation knowledge is a great concern for re-\nsearchers [7]. Models may face challenges in handling translation tasks between\nthese languages. Therefore, we evaluate the popular large language models cur-\nrently available in the market to acquire a better perception of the translation\nperformance of large language models.\nIn this paper, the translation ability of Large Language Models (LLMs) is\ninvestigated by answering two questions, 1) What is the translation ability of\nLLMs?2) Factors affecting the translation ability of LLMs?3) What are the trans-\nlation results of the LLMs?\narXiv:2408.03119v1  [cs.CL]  6 Aug 2024\n2\nYan Huang et al.\nIn response to the first question, we evaluate several popular LLMs: English\nlanguage-centric LLMs including Llama2[8] , Falcon [9], Vicuna[10] , Mistral [11]\nand multilingual LLMs including Bloom and Bloomz[12] , Gemma[13] . In order\nto prevent data leakage and get more accurate results, we constructed a dataset\nEuas-20(A representative selection of 20 languages). LLMs translate other lan-\nguages into Chinese and English respectively. The results show a significant\nimprovement in the multilingual translation ability of LLMs. This improvement\nis not only reflected in the increase of the model’s parameters, but also due to the\nmodel’s improvement in training data and methods. We compare the translation\nresults of LLMs on different languages. We find that there is a significant differ-\nence in the translation performance of LLMs on different languages, and LLMs\nperform better than Chinese when translating into English. For languages similar\nto English, LLMs also demonstrate better translation performance. Meanwhile,\nwe find that LLMs also have translation ability on zero-resource languages. This\nsuggests that the large language models have some generalisation ability and are\nable to establish correspondences between different languages in the absence of\ndirect training data.\nTo address the second question, we analysed the LLMs by collecting infor-\nmation from their corpora. We find that a high-quality and diverse corpus can\nsignificantly improve the translation performance of LLMs. Multi-language and\nmulti-domain training data can not only enhance the generalisation ability of\nthe model, but also improve its effectiveness in different languages and different\ndomains.\nTo address the third question, we have analysed the translation results of\nLLMs in various aspects.The translation results of LLMs are subject to some\nillusory problems, and their fluent output may mislead the users and make it\ndifficult for them to identify the errors in the translation. In addition, LLMs\ntend to choose the most appropriate translation words in translation tasks by\nanalysing a variety of factors such as semantics, fluency and culture. We also\nfound that when LLMs met words that they had not encountered during model\ntraining, the models could not understand or process them accurately due to the\nlack of training on these words.\nThe purpose of this paper is to review and analyse the performance of the\ncurrent large language model on translation tasks, to explore whether the large\nlanguage model can effectively model the correspondence between different lan-\nguages and the factors affecting translation, for the reference of researchers and\ndevelopers.\n2\nBackground\n2.1\nLarge Language Models\nLarge Language Models (LLMs) have made significant progress in translation\nperformance. Based on deep learning, especially the Transformer architecture[14,15,16]\n, these Large Language Models have learned rich linguistic knowledge by pre-\nEvaluating the Translation Performance\n3\ntraining on a large amount of textual data, thus achieving excellent performance\nin various downstream tasks.\nThe training process of a large language model is divided into two main\nphases. The first is the pre-training phase, in which the model learns unsuper-\nvised on large-scale textual data to master the basic structure and lexical usage\nof the language. The goal of this phase is for the model to learn a generic lan-\nguage representation. Next is the fine-tuning phase, in which the pre-trained\nmodel is subjected to supervised learning on a specific translation task using a\nbilingual parallel corpus to equip it with the ability to translate specific language\npairs.\nThe big language models support multiple languages, demonstrating the abil-\nity to generalize across languages. However, the training data of the big language\nmodels are dominated by the English corpus, with a smaller proportion of data\nin other languages, and this unbalanced data distribution poses a severe test for\nthe performance of the models in a multilingual environment. Researchers are\nactively exploring ways to address these issues in order to further improve the\nperformance of large language models in translation tasks.\n2.2\nMachine Translation\nMachine Translation (MT) is a technology that uses computers to automatically\ntranslate text from one language to another. In recent years, with the rapid\ndevelopment of artificial intelligence and natural language processing technology,\nespecially the emergence of large language models (e.g., OpenAI’s GPT series\nand Google’s BERT), the ability of machine translation has been significantly\nimproved.\nModern machine translation systems mainly rely on Neural Machine Trans-\nlation (NMT) technology[17,18] . NMT utilises deep learning and neural network\nmodels, and is able to efficiently capture and process complex relationships be-\ntween source and target languages through encoder-decoder architectures and\nself-attention mechanisms. Compared with traditional rule-based methods and\nstatistical machine translation (SMT)[19,20], NMT performs better in terms of\ntranslation accuracy, fluency, and context understanding.\nMachine translation, as one of the core tasks of natural language processing,\nhas also benefited from the development of large language models and achieved\na qualitative leap. However, machine translation still faces challenges, including\ntranslation of low-resource languages and maintaining coherence and fluency of\ntranslation in long texts [21].\n3\nExperimental Setup\n3.1\nDataset\nIn order to evaluate the real translation capabilities of large language models,\nwe constructed a dataset called Euas-20. This dataset contains twenty represen-\ntative languages (Table 1), covering a large part of the global population, while\n4\nYan Huang et al.\nTable 1. Language\niso Language\nLanguage grouping\nScript\nResource Level\nar arabic\narabic\nArabic\nMedium\nzh chinese\nSino-Tibetan\nHan\nMedium\nda danish\nGermanic\nLatin\nMedium\nen english\nGermanic\nLatin\nHigh\nfr\nfrench\nRomance\nLatin\nHigh\nde german\nGermanic\nLatin\nHigh\nel greek\nHellenic\nGreek\nMedium\nhi hindi\nIndo-Aryan\nDevanagari Medium\nis\nicelandic\nGermanic\nLatin\nMedium\nit\nitalian\nRomance\nLatin\nHigh\nja japanese\nJaponic\nKanji; Kana Medium\nko korean\nKoreanic\nHangul\nMedium\nnl nederlands Indo-European\nLatin\nMedium\nno norsk\nIndo-European\nLatin\nMedium\npt portuguese Romance\nLatin\nHigh\nru russian\nSlavic\nCyrillic\nHigh\nes spanish\nRomance\nLatin\nHigh\ntl\ntagalog\nMalayo-Polynesian\nLatin\nLow\nth thai\nSino-Tibetan+Kra-Dai Thai\nMedium\nvi vietnamese Vietic\nLatin\nMedium\ndemonstrating a diverse background of writing systems and language families.\nWe have selected a number of important languages that not only have a large\nnumber of speakers, but also include some languages that are considered under-\nresourced in the research community. With this diverse dataset, we are able to\ncomprehensively evaluate the translation performance of the large language mod-\nels in different language contexts, and thus gain a more accurate understanding\nof their performance in real-world applications.\nlanguages we use in our work are listed in Table 1. Referring to the infor-\nmation provided in Goyal et al. (2022)[22], we populated the table. For each\nlanguage, we show the ISO code, language name, language grouping, alphabet\nand resource level.\nWe selected about twenty domains such as medicine, science, art, education,\nenvironment, finance, entertainment, sports, politics, agriculture, etc. to ensure\na wider coverage of the dataset. After that, we designed a prompt (Fig.1) and fed\nit into ChatGPT, allowing it to act as a data annotator and generate sentences\naccording to specified rules. In each domain, ChatGPT generated fifty sentences,\nincluding different sentence types such as declarative, interrogative and exclam-\natory sentences. We deleted sentences with high similarity and repeated the\nprocess, eventually selecting about fifty different sentences in each domain and\nconstructing a document containing one thousand Chinese sentences.\nNext, we used Google Translate to translate this Chinese document into other\ntarget languages to build a complete dataset. In this way, we ensure that the\nEvaluating the Translation Performance\n5\nFig. 1. Prompt 1\ndataset is diverse and representative, and we are able to more comprehensively\nevaluate the translation capabilities of large language models across different\ndomains and languages.\n3.2\nLLMs\nWe evaluated the translation capabilities of nine currently popular LLMSs: fal-\ncon7b, mistral-7b, Llama-2-7b-hf, bloom-7b1, bloomz-7b1-mt, Meta-Llama-3-\n8B, mpt-7b, vicuna-7b, and gemma-7b.\n3.3\nEvaluation Methods\nFocusing on Chinese and English, through a prompt (Fig. 2), ’source-sentence’\nstands for the original sentence and ’target-sentence’ stands for the target sen-\ntence, and the original sentence is input to the LLMs by the command (Trans-\nlate the following sentence from ’source-language’ to ’target-language’ and The\n’target- language’ translation is), so that the LLMs can translate and output the\ntarget sentence under Zero-Shot learning. In this way, various languages in the\ndataset are translated into Chinese and English.\n3.4\nEvaluation Indicators\nEvaluation metrics are an important measure of translation quality. We adopt\ncommonly used automatic evaluation metrics including BLEU[23], which calcu-\nlates translation accuracy by comparing the n-gram overlap between candidate\n6\nYan Huang et al.\nFig. 2. Prompt 2\nand reference translations, which is the traditional method for assessing the\nquality of machine translation.\nIn addition, we also consider the emerging metric COMET[24], which is de-\nsigned to learn to predict human judgements of machine translation quality, and\nwhich better reflects subjective human assessments of translation quality. By\ncombining these evaluation metrics, we are able to assess the translation per-\nformance of large-scale language models in a more comprehensive way, ensuring\nthe accuracy and reliability of the assessment results.\n4\nTesting of machine translation for LLMs\nIn this section, we report the results of the translation of LLMs (Fig.3) and\nanalyse the translation performance of LLMS.\n4.1\nContinuous Improvement of Translation Ability of LLMs\nIn recent years, the multilingual translation capability of Large Language Mod-\nels (LLMs) has been significantly improved. Even under Zero-Sample Learning\n(Zero-Shot) conditions, LLMs still exhibit good translation performance in most\ntranslation directions, as shown in Fig. 4. Based on the scores of LLMs on dif-\nferent languages , we can find that the translation ability of LLMs has gradually\nimproved, especially the recent LLMs have reached new heights in terms of\ntranslation performance. For example, Llama-3-8B significantly outperforms the\nprevious Llama-2-7B, and vicuna-7B outperforms Llama-2-7B. Overall, Llama-\n3-8B performs the best among all the LLMs evaluated, and it obtains the highest\nEvaluating the Translation Performance\n7\nFig. 3. BLEU and COMET scores for nine LLMs translations centred on English and\nChinese.\nBLEU and COMET scores in most translation directions, showing its superior\ntranslation capabilities.\nThis progress is not only reflected in the increase of the model’s parameters,\nbut also due to the model’s improvement in training data and methodology.\nLlama-3-8B uses larger and higher quality multilingual datasets during training,\nand adopts more advanced training algorithms, which enable it to maintain\na high level of translation quality even when dealing with complex and rare\nlanguage pairs. At the same time, the model’s architectural optimisation and\ninference speed have also been improved, making Llama-3-8B not only more\naccurate but also more efficient in practical applications.\nIn addition, Llama-3-8B and other advanced LLMs demonstrate a high degree\nof flexibility and adaptability in coping with multilingual text comprehension and\ngeneration tasks. These models can play an important role in cross-cultural and\ncross-linguistic communication.\n4.2\nTranslation performance of LLMSs across languages\nThe translation performance of large-scale language models (LLMs) varies sig-\nnificantly across languages. Typically, LLMs translate well on high-resource lan-\nguages, but have relatively poor translation performance on low- and medium-\nresource languages. We find that LLMs perform particularly well when trans-\nlating into English and relatively poorly when translating into Chinese. For\nlanguages similar to English, LLMs also demonstrate better translation perfor-\n8\nYan Huang et al.\nFig. 4. Translation performance (BLEU) of LLMS on our evaluated languages, ‘xx-en’\nand ‘xx-zh’ denote translation from other languages to English and Chinese, respec-\ntively.\nmance. For example, LLMs generally achieve better translation results in the\nIndo-European Romance and Germanic languages.\nFor some languages that are more different from English, such as the Tai-\nKadai languages, LLMs produce very poor translation results. This uneven trans-\nlation performance is mainly due to the differences in the training data, where\nthe high volume and quality of data for high-resource languages make the models\nperform better on these languages. On the other hand, low and medium resource\nlanguages are difficult for the model to learn enough linguistic features due to\nthe scarcity of data, resulting in unsatisfactory translation results.\nNevertheless, LLMs show some translation ability even on zero-resource lan-\nguages. This suggests that the large language models possess some generalisation\nability and are able to establish correspondences between different languages in\nthe absence of direct training data. Behind this ability is the fact that the models\nhave learnt common features and structures between languages through large-\nscale multilingual training, so that they can still translate reasonably well in the\nface of new language pairs.\n4.3\nEffect of corpus on the translation performance of LLMs\nBy analysing pre-training data and corpora of large-scale language models (LLMs),\nwe can investigate the relationship between translation performance and corpus\nEvaluating the Translation Performance\n9\nsize and category. By collecting LLMs with training data sizes (Table 2), we find\nthat the larger the pre-training data size, the better the translation performance\nof the LLMs.For example, Llama-3-8B and Gemma-7B outperform other models\noverall. This suggests that rich training data is one of the key factors to improve\nthe translation ability of the models.\nTable 2. Training volume of LLMs\nLLM\nToken\nGemma 7b\n6 trillion\nLlama-2-7b-hf\n2 trillion\nmpt-7b\n1 trillion\nMeta-Llama-3-8B 15 trillion\nMost of the pre-training corpora of the big language models are English-\ncentric, which on the one hand makes the models perform extremely well in\nEnglish language ability; on the other hand, it also leads to their weaker ability\nin non-English languages. This English-centric corpus configuration improves the\nefficiency of the model in handling English-related tasks, but the model’s perfor-\nmance appears to be insufficient when dealing with other languages, especially\nlow- and medium-resource languages.\nFig. 5. Corpus share of LLMs\nModels trained in multiple languages have achieved better results in trans-\nlation than LLMs limited to one or a few languages. For example, the trans-\nlation performance of Gemma-7B is significantly better than that of Falcon-\n7B.Meanwhile, when translating languages, multilingual models tend to have\nbetter translation ability for languages that have been pre-trained than for lan-\nguages that have not been pre-trained. For example, the corpus share of bloom-\n7b1 and bloomz-7b1-mt (Fig. 5) has better translation ability for pre-trained\nlanguages. This suggests that multi-language training can effectively enhance\n10\nYan Huang et al.\nthe model’s translation capability, enabling it to better handle translation tasks\nbetween various language pairs.\nFrom these observations, it can be found that a high-quality and diverse\ncorpus can significantly improve the translation performance of LLMs. Multi-\nlanguage and multi-domain training data can not only enhance the generalisation\nability of the model, but also improve its effectiveness in different languages and\ndifferent domains. Therefore, in order to meet the translation needs of various\nlanguages, future LLMs should make full use of diverse corpora in the pre-\ntraining process and continuously increase the proportion of data from low- and\nmedium-resource languages.\n4.4\nIllusions in the translation of LLMs\nNeural Machine Translation (NMT) is a task that translates a source language\ninto a target language through inference and relies on parallel data samples used\nfor training. Compared to Statistical Machine Translation (SMT), the output of\nNMT is usually very fluent, with a quality close to the human level. However, this\nposes a potential problem: when NMT hallucinates (i.e., generates inaccurate or\nspurious translations), its smooth output may mislead users and make it difficult\nfor them to identify errors in the translation.\nBy analysing the translation results of LLMs, we classified the hallucinations\nin NMT as two categories [25], intrinsic and extrinsic hallucinations. Intrinsic\nIllusion: Incorrect information is included in the translation that does not match\nwhat is in the source text. An example of such an illusion is ‘不太了解’, which\nnegates ‘了解多少’ in the source text. Extrinsic illusions: the translation pro-\nduces additional content that does not exist in the source text.’我忘了带手机\n’ is an example of illusory content because it is added without any apparent\nconnection to the input.\nTable 3. Illusions in the translation of LLMs\nCategory Source\nCorrect Translation Hallucinatory Translation\nIntrinsic Excuse me, how much do you 请问，你对这项技\n非常抱歉，我们这么说是\nknow about this technology? 术的了解有多少？\n因为我们不太了解这种技术。\nExtrinsic Excuse me, how much do you 请问，你对这项技\n对不起，我忘了带手机了。\nknow about this technology? 术的了解有多少？\nThe results show that as the pre-trained corpus continues to grow, the pre-\ntrained model becomes more and more effective in generating faithful summaries\nof human assessments. By comparing the translation results of Gemma-7B and\nFalcon-7B, more intrinsic illusions are generated for monolingual models; while\nfor multilingual models, this is less frequent. Also, we found that nouns are the\nmost hallucinated words, and verbs also account for a certain number of hallu-\ncinations. In addition, LLMS tend to be more prone to intrinsic hallucinations\nwhen confronted with untrained language.\nEvaluating the Translation Performance\n11\nTherefore, it is crucial to improve the accuracy and reliability of machine\ntranslation. By continuously improving our models, enhancing the diversity and\nquality of our datasets, and using more advanced evaluation metrics to detect\nand reduce illusions in translation, we can mitigate these risks and provide more\nsecure and reliable translation services.\n4.5\nTranslation words that LLMs tend to choose in translation\ntasks\nThis section explores the translation words that Large Language Models tend\nto choose in translation tasks and the reasons behind them.\nThrough previous analyses of the translation results of Gemma-7B and Falcon-\n7B, we found that LLMs tend to choose common word collocations in the target\nlanguage during the translation process. This not only improves the naturalness\nof the translation, but also makes it more in line with the usage habits of the\ntarget language. For example, ‘make a decision’ in English is often translated\nas ‘做决定’ instead of ‘制造决定’ in Chinese because the former is a common\ncollocation in Chinese and is more in line with the language conventions. This\nis because the former is a common collocation in Chinese, which is more in line\nwith the language convention.\nIn addition, we also found that the model selects those words that are clos-\nest in meaning to be translated by deep understanding of the original and the\ntranslated text. For example, when translating the English word ‘computer’ into\nChinese, the model chooses ‘电脑’ instead of ‘计算机’ because ‘电脑’ is a com-\nmon collocation in modern Chinese. ‘is more commonly used and semantically\naccurate in modern Chinese.\nLLMs tend to choose the most appropriate translation words in translation\ntasks by comprehensively analysing various factors such as semantics, fluency\nand culture. This approach not only improves the accuracy and naturalness of\nthe translation, but also makes the translation result more in line with the usage\nhabits of the target language.\n4.6\nPhenomenon of unregistered words\nOut-of-vocabulary words (OOV words), refer to words that have not been en-\ncountered during model training. These words may be new terms, technical\nterms, foreign language vocabulary, or recently emerged buzzwords. We found\nthat due to the lack of training on these words, the model cannot understand\nor process them accurately. We choose the translation results of Gemma-7B and\nFalcon-7B as representative.\nFor monolingual models, when the model is confronted with words that have\nnot been trained across languages, such as ‘madilim na bagay’ (dark matter) in\nFilipino, the model will ignore or mistranslate them to other nouns. For multi-\nlingual models, even if the model has been trained cross-linguistically, when the\nmodel encounters a new word like ‘schadenfreude’ (a German word that refers\nto the emotion of taking pleasure in someone else’s misfortunes), it may not be\n12\nYan Huang et al.\nable to correctly understand the meaning because the word has never appeared\nin its training data. ever appeared in its training data. As a result, the model\nwill choose to ignore it, not translate it or translate it incorrectly.\nIn the future, LLMs need to increase their training data to cover a wider\nrange of vocabulary, as well as dynamically expand the model’s vocabulary by\nusing external resources such as vocabularies or online data sources; to reduce\nthis phenomenon and to improve the translation ability of LLMs.\n5\nRelated Work\nIn the field of large language model translation capability evaluation, there have\nbeen a large number of related studies devoted to exploring the translation per-\nformance of different models on multiple language pairs and text types.Bang et\nal. (2023) [26]and Hendy et al. (2023)[27] evaluated ChatGPT on 13 and 18 lan-\nguages, respectively; Zhu et al. (2023)[28] evaluated the translation capability of\nfour popular large language models, XGLM, BLOOMZ, OPT, and ChatGPT,\non 102 languages, on 202 directions. 202 directions The multilingual translation\ncapabilities of four popular large language models, XGLM, BLOOMZ, OPT and\nChatGPT, were evaluated.\nIn this paper, 20 representative languages are selected to evaluate nine cur-\nrent mainstream large-scale language models. The evaluation focuses on Chinese\nand English, but covers a wide range of other languages as well. Multilingual\ntranslations are performed with these models and the results are compared with\na state-of-the-art translation engine (Google Translate) in order to comprehen-\nsively evaluate the translation capabilities and performance of these large lan-\nguage models. The aim of the study is to determine the performance of these\nmodels in different linguistic contexts, as well as their usability and accuracy in\nreal translation tasks.\nDespite the significant progress made by large-scale language models on\ntranslation tasks, a number of challenges remain. For example, there is still\nroom for improvement in the model’s ability to handle low-resource languages\nand diverse texts. Future research directions include improving the evaluation\nmetrics, optimising the model structure and enhancing the training methods to\nfurther improve the performance and generalisation of large language models on\ntranslation tasks. These improvements will not only help to enhance the model’s\ntranslation accuracy, but also enhance its adaptability in dealing with complex\nand diverse language environments.\n6\nConclusion\nIn this paper, a dataset called Euas-20 is constructed and nine popular large\nlanguage models (LLMs) are evaluated using this dataset. The evaluation process\nfocuses on Chinese and English, and compares the translation performance of\nthese models and their translation capabilities on various languages through\ntranslations in 20 languages. Also, the paper analyses the impact of pre-training\nEvaluating the Translation Performance\n13\ndata and corpus on the translation performance of large language models.The\ntranslation results of the LLMs were analysed in various ways.\nThe results show that although the translation performance of LLMs is im-\nproving, with Llama-3 performing particularly well, far exceeding other models,\nthe translation ability of these models on different languages is still very unbal-\nanced. Especially when dealing with low-resource languages, they still face great\nchallenges. In addition, a high-quality and diverse corpus plays a significant role\nin improving the translation performance of large language models.\nFuture research needs to continue to explore how to enhance the translation\ncapabilities of LLMs on more languages to achieve more balanced and compre-\nhensive translation performance. This includes improving the model structure,\noptimising training methods, and extending and enhancing the quality and di-\nversity of the corpus.\nReferences\n1. Jiao, W., Wang, W., Huang, J.t., Wang, X., Tu, Z.: Is chatgpt a good translator?\na preliminary study. arXiv preprint arXiv:2301.08745 1(10) (2023)\n2. Robinson, N.R., Ogayo, P., Mortensen, D.R., Neubig, G.: Chatgpt mt: Competitive\nfor high-(but not low-) resource languages. arXiv preprint arXiv:2309.07423 (2023)\n3. Moslem, Y., Haque, R., Kelleher, J.D., Way, A.: Adaptive machine translation with\nlarge language models. arXiv preprint arXiv:2301.13294 (2023)\n4. Zhu, S., Cui, M., Xiong, D.: Towards robust in-context learning for machine trans-\nlation with large language models. In: Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language Resources and Evaluation\n(LREC-COLING 2024). pp. 16619–16629 (2024)\n5. Zhu, S., Mi, C., Li, T., Yang, Y., Xu, C.: Unsupervised parallel sentences of ma-\nchine translation for asian language pairs. ACM Transactions on Asian and Low-\nResource Language Information Processing 22(3), 1–14 (2023)\n6. Zhu, S., Gu, S., Li, S., Xu, L., Xiong, D.: Mining parallel sentences from internet\nwith multi-view knowledge distillation for low-resource language pairs. Knowledge\nand Information Systems 66(1), 187–209 (2024)\n7. Zhu, S., Pan, L., Xiong, D.: FEDS-ICL: enhancing translation ability and effi-\nciency of large language model by optimizing demonstration selection. Inf. Process.\nManag. 61(5), 103825 (2024), https://doi.org/10.1016/j.ipm.2024.103825\n8. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n9. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah,\nM., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., et al.: Falcon-40b: an open\nlarge language model with state-of-the-art performance. Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, 10755–10773 (2023)\n10. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z.,\nLi, D., Xing, E., et al.: Judging llm-as-a-judge with mt-bench and chatbot arena.\nAdvances in Neural Information Processing Systems 36 (2024)\n11. Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas,\nD.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral 7b. arXiv\npreprint arXiv:2310.06825 (2023)\n14\nYan Huang et al.\n12. Le Scao, T., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow, D., Castagn´e, R.,\nLuccioni, A.S., Yvon, F., Gall´e, M., et al.: Bloom: A 176b-parameter open-access\nmultilingual language model (2023)\n13. Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre,\nL., Rivi`ere, M., Kale, M.S., Love, J., et al.: Gemma: Open models based on gemini\nresearch and technology. arXiv preprint arXiv:2403.08295 (2024)\n14. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n15. Cui, M., Du, J., Zhu, S., Xiong, D.: Efficiently exploring large language models\nfor document-level machine translation with in-context learning. arXiv preprint\narXiv:2406.07081 (2024)\n16. Zhu, S., Xiong, D.: Tjunlp: System description for the wmt23 literary task in\nchinese to english translation direction. In: Proceedings of the Eighth Conference\non Machine Translation. pp. 307–311 (2023)\n17. Cho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,\nH., Bengio, Y.: Learning phrase representations using rnn encoder-decoder for\nstatistical machine translation. arXiv preprint arXiv:1406.1078 (2014)\n18. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (2014)\n19. Zens, R., Och, F.J., Ney, H.: Phrase-based statistical machine translation. In: KI\n2002: Advances in Artificial Intelligence: 25th Annual German Conference on AI,\nKI 2002 Aachen, Germany, September 16–20, 2002 Proceedings 25. pp. 18–32.\nSpringer (2002)\n20. Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N.,\nCowan, B., Shen, W., Moran, C., Zens, R., et al.: Moses: Open source toolkit\nfor statistical machine translation. In: Proceedings of the 45th annual meeting of\nthe association for computational linguistics companion volume proceedings of the\ndemo and poster sessions. pp. 177–180. Association for Computational Linguistics\n(2007)\n21. Koehn, P., Knowles, R.: Six challenges for neural machine translation. arXiv\npreprint arXiv:1706.03872 (2017)\n22. Goyal, N., Gao, C., Chaudhary, V., Chen, P.J., Wenzek, G., Ju, D., Krishnan, S.,\nRanzato, M., Guzm´an, F., Fan, A.: The flores-101 evaluation benchmark for low-\nresource and multilingual machine translation. Transactions of the Association for\nComputational Linguistics 10, 522–538 (2022)\n23. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic\nevaluation of machine translation. In: Proceedings of the 40th annual meeting of\nthe Association for Computational Linguistics. pp. 311–318 (2002)\n24. Rei, R., Stewart, C., Farinha, A.C., Lavie, A.: Comet: A neural framework for mt\nevaluation (2020), https://arxiv.org/abs/2009.09025\n25. Zhou, C., Neubig, G., Gu, J., Diab, M., Guzman, P., Zettlemoyer, L., Ghazvinine-\njad, M.: Detecting hallucinated content in conditional neural sequence generation\n(2021), https://arxiv.org/abs/2011.02593\n26. Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z.,\nYu, T., Chung, W., et al.: A multitask, multilingual, multimodal evaluation of chat-\ngpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023\n(2023)\n27. Hendy, A., Abdelrehim, M., Sharaf, A., Raunak, V., Gabr, M., Matsushita, H.,\nKim, Y.J., Afify, M., Awadalla, H.H.: How good are gpt models at machine trans-\nlation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210 (2023)\nEvaluating the Translation Performance\n15\n28. Zhu, W., Liu, H., Dong, Q., Xu, J., Huang, S., Kong, L., Chen, J., Li, L.: Mul-\ntilingual machine translation with large language models: Empirical results and\nanalysis. arXiv preprint arXiv:2304.04675 (2023)\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-08-06",
  "updated": "2024-08-06"
}