{
  "id": "http://arxiv.org/abs/2408.06212v1",
  "title": "Computability of Classification and Deep Learning: From Theoretical Limits to Practical Feasibility through Quantization",
  "authors": [
    "Holger Boche",
    "Vit Fojtik",
    "Adalbert Fono",
    "Gitta Kutyniok"
  ],
  "abstract": "The unwavering success of deep learning in the past decade led to the\nincreasing prevalence of deep learning methods in various application fields.\nHowever, the downsides of deep learning, most prominently its lack of\ntrustworthiness, may not be compatible with safety-critical or\nhigh-responsibility applications requiring stricter performance guarantees.\nRecently, several instances of deep learning applications have been shown to be\nsubject to theoretical limitations of computability, undermining the\nfeasibility of performance guarantees when employed on real-world computers. We\nextend the findings by studying computability in the deep learning framework\nfrom two perspectives: From an application viewpoint in the context of\nclassification problems and a general limitation viewpoint in the context of\ntraining neural networks. In particular, we show restrictions on the\nalgorithmic solvability of classification problems that also render the\nalgorithmic detection of failure in computations in a general setting\ninfeasible. Subsequently, we prove algorithmic limitations in training deep\nneural networks even in cases where the underlying problem is well-behaved.\nFinally, we end with a positive observation, showing that in quantized versions\nof classification and deep network training, computability restrictions do not\narise or can be overcome to a certain degree.",
  "text": "Computability of Classification and Deep\nLearning: From Theoretical Limits to Practical\nFeasibility through Quantization\nHolger Boche1,2,3,4,5, Vit Fojtik6,7*, Adalbert Fono6,\nGitta Kutyniok6,7,8,9\n1Institute of Theoretical Information Technology, Technical University\nof Munich, Munich, Germany.\n2CASA – Cyber Security in the Age of Large-Scale Adversaries–\nExzellenzcluster, Ruhr-Universit¨at Bochum, Bochum, Germany.\n3 BMBF Research Hub 6G-life, Germany.\n4 Munich Quantum Valley (MQV), Munich, Germany.\n5 Munich Center for Quantum Science and Technology (MCQST),\nMunich, Germany.\n6Mathematical Institute, Ludwig-Maximilians-Universit¨at M¨unchen,\nMunich, Germany.\n7Munich Center for Machine Learning (MCML), Munich, Germany.\n8Department of Physics and Technology, University of Tromsø, Tromsø,\nNorway.\n9Institute of Robotics and Mechatronics, DRL-German Aerospace\nCenter, Germany.\n*Corresponding author(s). E-mail(s): fojtik@math.lmu.de;\nContributing authors: boche@tum.de; fono@math.lmu.de;\nkutyniok@math.lmu.de;\nAbstract\nThe unwavering success of deep learning in the past decade led to the increasing\nprevalence of deep learning methods in various application fields. However, the\ndownsides of deep learning, most prominently its lack of trustworthiness, may\nnot be compatible with safety-critical or high-responsibility applications requir-\ning stricter performance guarantees. Recently, several instances of deep learning\n1\narXiv:2408.06212v1  [cs.LG]  12 Aug 2024\napplications have been shown to be subject to theoretical limitations of com-\nputability, undermining the feasibility of performance guarantees when employed\non real-world computers. We extend the findings by studying computability in the\ndeep learning framework from two perspectives: From an application viewpoint\nin the context of classification problems and a general limitation viewpoint in the\ncontext of training neural networks. In particular, we show restrictions on the\nalgorithmic solvability of classification problems that also render the algorithmic\ndetection of failure in computations in a general setting infeasible. Subsequently,\nwe prove algorithmic limitations in training deep neural networks even in cases\nwhere the underlying problem is well-behaved. Finally, we end with a positive\nobservation, showing that in quantized versions of classification and deep network\ntraining, computability restrictions do not arise or can be overcome to a certain\ndegree.\nKeywords: Computability, Deep Learning, Classification, Approximation Theory,\nQuantization\nMSC Classification: 68T07 , 68T05 , 03D80 , 65D15\n1 Introduction\nWith the advent of deep learning [1–4] a new machine learning approach materialized\nthat provides state-of-the-art results in various tasks. The performance of deep neural\nnetworks makes them the go-to strategy to tackle a multitude of problems in relevant\napplications such as image classification, speech recognition, and game intelligence, as\nwell as more recent developments such as image and sound synthesis, chat tools, and\nprotein structure prediction, to name a few [5–11]. Although a wide range of literature\nsupports the power of deep learning, such as the well-known Universal Approxima-\ntion Theorems [12–14], its universality and success still lack theoretical underpinning.\nMoreover, despite the impressive performance deep learning typically comes at the cost\nof downsides such as black-box behavior and non-interpretability, as well as instability,\nnon-robustness, and susceptibility to adversarial manipulation [15–24].\nIn certain applications, the highlighted drawbacks, informally summarized by a\nlack of trustworthiness [25, 26], are tolerable or even avoidable by a human-in-the-loop\napproach [27]. However, increasing the autonomy of deep learning systems without\nimpairing their trustworthiness poses a great challenge, especially in safety-critical or\nhigh-responsibility tasks – a prime example being autonomous driving [28, 29]. There-\nfore, it is crucially important for forthcoming deep learning methods to tackle and\nalleviate the lack of trustworthiness. To that end, we first need to understand whether\nor to what degree trustworthiness can be realized, e.g., is it feasible to ask for ’hard’\nperformance guarantees or verifiably correct results? We study this question from\nthe viewpoint of algorithmic computations with correctness guarantees and analyze\nwhether trustworthiness can be established from a mathematical perspective.\n2\n1.1 Algorithmic Computability\nComputability theory aims to mathematically model computation and answer ques-\ntions about the algorithmic solvability and complexity of given problems. The most\ncommonly studied model of computation is the Turing machine [30], which is an ideal-\nized version of real-world digital hardware neglecting time and space constraints. We\ndistinguish between two different modes of computations – problems on continuous\nand discrete domains. The former typically represents an idealized scenario whereas\nthe latter treats a setting closer to the actual realization of digital hardware. For\ninstance, complex real-world problems – a typical application scenario for deep learning\ntechniques due to their increasing capabilities – may be represented by a model with\ncontinuous state and parameter space despite the eventual implementation on digital\nhardware. The distinction is nevertheless crucial since the underlying computability\nconcepts depend on the input domain and the associated ground truth solution of the\ntackled problem.\nComputability on continuous domains\nIn recent years, there has been an increased interest in the computability of con-\ntinuous problems, studying the capabilities of inherently discrete digital computers\nwhen employed in the real domain. Some results indicate a non-conformity between\nthese two realms and limitations of two types can be identified: By Type 1 failure of\ncomputability we refer to the situation where the problem in question cannot be algo-\nrithmically solved. This has been found in diverse settings including inverse problems,\noptimization, information and communication theory, financial mathematics, and lin-\near algebra [31–35]. On the other hand, in Type 2 failure a computable solver may\nexist, but we cannot algorithmically learn it from data. There have not been many\nresults in this direction, but it has been shown to occur in the context of inverse\nproblems [36] and simple neural networks [34].\nQuantization\nIt is unrealistic to expect access to real-valued data and parameters with unlimited\nprecision in many applications since physical measurements generally guarantee only\nsome bounded accuracy. Furthermore, high-precision values can be computationally\nexpensive. For these reasons, real-valued problems can be described by discretized\nmodels. A typical approach in practice is to perform real-valued computation under\nquantization, i.e., associating continuous ranges with a discrete set of values [37–40].\nFor instance, quantized deep learning has been a topic of interest, comparing its the-\noretical and practical capabilities to non-quantized deep learning [41, 42]. In perhaps\nthe simplest quantization paradigm, fixed-point quantization, fixed numbers b, k ∈Z\nare chosen, and real numbers are replaced with numbers expressable in base b using up\nto k decimal places, that is, the set b−kZ. Naturally, the question arises of how these\nquantization techniques affect the properties of algorithmic computations, including\nthe limitations of computability on continuous domains. The key property of quanti-\nzation techniques and the resulting models is their amenability to classical computing\ntheory (based on exact binary representations), which as it turns out influences the\ncomputability characteristics.\n3\nClassification and Learning\nWe apply the introduced computability frameworks to two tasks closely associated\nwith deep learning, which provide insights from different perspectives. On the one\nhand, we consider classification problems as an instance of a classic application field\nfor deep learning. This highlights the importance of studying the computability of a\nspecific problem independent of the employed solution strategy: If a tackled problem\nhas computability restrictions, one can not expect to circumvent them with a deep\nlearning approach. On the other hand, one key issue in deep learning regarded as a\nparametric model is identifying suitable parameters, i.e., finding an appropriate deep\nlearning model, to solve a given task. The search process – the so-called learning or\ntraining – is typically based on data samples and constitutes the main obstacle in\nsuccessfully employing deep learning. Thus, it is critical to understand under what\ncircumstances learning can be performed algorithmically and what kind of performance\nguarantees can be achieved.\n1.2 Our Contributions\nThis paper aims to extend the theory of computability in deep learning and to highlight\nthe importance of ground truth descriptions in questions of computability.\n• We analyze the field of classification tasks from the computability viewpoint\nand show in Propositions 3.3 and 3.5 that the computability of a classifier is\nequivalent to the semi-decidability of its classes. Since only specific real sets are\nsemi-decidable, classification on the real domain is typically not algorithmically\nsolvable, i.e., Type 1 failure of computability arises.\n• Furthermore, we study the computable realizability of training neural networks,\nasking whether a function representable by a neural network can be learned from\ndata. Theorem 3.7 shows that no general learning algorithm applicable to all\n(real-valued) networks exists implying that Type 2 failure is unavoidable in this\nscenario.\n• We also consider strategies for coping with the introduced failures. We show it\nis impossible to predict when an algorithmic approximation of a non-computable\nfunction will fail in Proposition 4.1 and Corollary 4.3. On the other hand, Theo-\nrems 4.5 and 4.6 indicate that computability limitations in the context of learning\ncan be avoided by relaxing exactness requirements on the learned network.\n• Finally – and perhaps most importantly – we show in Theorems 4.9 and 4.10\nand Proposition 4.12 that when considering quantized versions of the previous\nsettings, issues of non-computability do not arise. Proposition 4.13 provides a\nword of caution, stating that the quantization function itself is non-computable –\nwe cannot algorithmically determine which quantized values faithfully represent\nthe original (real-valued) problem.\nNon-computability should not be understood as undermining the power of deep learn-\ning, which has been consistently demonstrated but rather as a different perspective\non fundamental problems bringing us closer to trustworthy deep learning methods in\nmajor applications such as autonomous decision-making and critical infrastructure,\ne.g., robotics and healthcare. In particular, our non-computability results indicate\n4\nthat theoretical correctness guarantees typically cannot be provided in digital com-\nputations of continuous problems, including deep learning. Therefore, innovative and\nspecialized hardware platforms beyond purely digital computations might be a solu-\ntion [43]. However, our positive findings demonstrate that in certain specific settings –\nprimarily, if the problem domain does not reside in a continuous but discrete domain –\ncorrectness guarantees and trustworthiness can be established for deep learning algo-\nrithms in the digital computing framework. Hence, the ground truth description of\nproblems has a decisive influence on the feasible performance guarantees.\n1.3 Previous Work\nNon-computability has been a point of high interest in algorithmic computation since\nthe results of Church [44] and Turing [30]. In the context of continuous problems, the\nnon-existence of a computable solver (Type 1 failure) has been shown in various appli-\ncations including optimization, inverse problems, signal processing, and information\ntheory [31–35, 45–50].\n‘Hardness’ results in neural network training have a long tradition going back to\n[51, 52], where it was shown that the training process can be NP-complete for certain\narchitectures. The infeasibility of algorithmically learning an existing computable neu-\nral network (Type 2 failure) in a continuous setting has been shown for the specific\ncontext of inverse problems in [36] and classification problems in [53]. Furthermore,\n[34] showed that no algorithm can reach near-optimal training loss on all possible\ndatasets for simple neural networks. Further properties of deep learning from the\ncomputability perspective concerning adversarial attacks, implicit regularization, and\nhardness of approximation were studied in [53–55]. A different context of learning an\nexisting neural network has been studied in [56], where difficulties in the form of an\nexplosion of required sample size were shown, rather than algorithmic intractability.\nSimilar results concerning the sample complexity were established in the framework\nof statistical query algorithms in [57].\nA key challenge is to increase the interpretability of deep learning algorithms, i.e.,\nenabling the user to comprehend their decision-making, which is typically hindered\ndue to the black-box behavior of deep learning [16, 58, 59]. Understanding the inner\nworkings of deep learning also points out a way to establish trustworthy methods.\nAnother approach relies on verifying the accuracy and correctness of deep learning\nmethods without explicitly tracing internal computations [60–63]. However, the find-\nings in [64, 65] indicate that certifying the accuracy and robustness of deep learning\nin the computability framework is challenging if at all possible, which poses challenges\nfor future applications. A potential direction to cope with this issue was considered\nin [66], where certain inverse problems that are not computable on digital comput-\ners were shown to be computable in a model of analog computation enabling implicit\ncorrectness guarantees in theory.\n1.4 Outline\nIn Section 2, we introduce the applied formalisms, including computability theory\nand neural networks as the main workhorse of deep learning. We present our main\n5\nresults concerning Type 1 and Type 2 failure of computability in Section 3. We con-\nclude in Section 4 by studying strategies to cope with computability failures, whereby\nquantization is a main theme. The proofs of the theorems are provided in Appendix B.\n2 Notation and Definitions\nWe first introduce some basic concepts and notation used in the following.\n2.1 Computability of Real Functions\nWe begin by reviewing definitions from real-valued computability theory necessary for\nour analysis. For a more comprehensive overview, see, for instance, [67–69]. We also\nomit elementary topics of computability theory such as recursive functions and Turing\nmachines. Here we refer the reader to [70].\nPrevious results in applied computability on the real domain introduce many dif-\nferent, although partly equivalent, versions of computation and computability. We\nfollow standard definitions introduced by Turing [30].\nDefinition 2.1. A sequence of rational numbers (qk)∞\nk=1 ⊂Q is computable if there\nexist recursive functions a, b, s : N →N such that\nqk = (−1)s(k) a(k)\nb(k) .\nA rational sequence (qk)∞\nk=1 converges effectively to x ∈R, if there exists a recursive\nfunction e : N →N such that for all k0 ∈N and all k ≥e(k0)\n|x −qk| ≤\n1\n2k0 .\nA real number x ∈R is computable if there exists a computable rational sequence\n(qk)∞\nk=1 converging effectively to x. Such a sequence is called a representation (or a\nrapidly converging Cauchy name) of x. We denote the set of all computable reals by\nRc.\nBefore defining computable real functions, we need to specify what it means for a\nreal sequence to be computable.\nDefinition 2.2. A real sequence (xk)∞\nk=1 ⊂R is computable if there exists a com-\nputable double-indexed rational sequence (qk,ℓ)∞\nk,ℓ=1 such that, for some recursive\nfunction e : N × N →N and all k, ℓ0 ∈N and ℓ≥e(k, ℓ0), we have\n|xk −qk,ℓ| ≤1\n2ℓ0 .\nRemark 2.3. All previous definitions can be extended to Rd and Rd\nc with d > 1\nby requiring that each (one-dimensional) component or component-wise sequence is\ncomputable, respectively.\nOut of the various definitions of a computable real function (see [69, Appendix\n2.9] for an overview) we introduce two. Borel-Turing computability can be seen as the\n6\nstandard intuitive notion of computation, i.e., an algorithm approximating a given\nfunction to any desired accuracy exists. On the other hand, Banach-Mazur computabil-\nity is the weakest common definition of computability meaning that a function is not\ncomputable in any usual sense if it is not Banach-Mazur computable.\nDefinition 2.4. Let D ⊂Rd. A function f : D →Rm\nc is\n(1) Borel-Turing computable if there exists a Turing machine M such that, for all\nx ∈D ∩Rd\nc and all representations (qk)∞\nk=1 of x, the sequence (M(qk))∞\nk=1 is a\nrepresentation of f(x);\n(2) Banach-Mazur computable if for all computable sequences (xk)∞\nn=1 ⊂D ∩Rd\nc the\nsequence (f(xk))∞\nk=1 is also computable.\nRemark 2.5. For a Borel-Turing computable function, there exists a Turing machine\ntaking a sequence of increasingly precise approximations of the input and producing\nincreasingly accurate approximations of the output, whereas, a Banach-Mazur com-\nputable function only guarantees that it preserves the computability of real sequences.\nHowever, it is well known that all Borel-Turing computable functions are also Banach-\nMazur computable, and computable functions in either sense are continuous – that\nis, continuous on Rc with the inherited topology [69]. For simplicity, we often refer to\n“computable” functions rather than “Borel-Turing computable”, as this is our frame-\nwork’s standard version of computability. We explicitly specify whenever we apply the\nnotion of Banach-Mazur computability.\nThe main theme of this paper revolves around the failure of algorithmic compu-\ntations studied from the perspective of computability. In particular, we ask in what\ncircumstances failures arise, and under what additional conditions they potentially\ncan be avoided. Thereby, we associate in the real domain the more intuitive term\n“algorithm” with “Borel-Turing computable function” and we distinguish two cases\nof algorithmic failure:\n• We say that a problem suffers from Type 1 failure of computability if it has\nno computable solver, that is, for any algorithm there exists an instance of the\nproblem to which the algorithm provides an incorrect solution.\n• A problem is subject to Type 2 failure of computability if a solver cannot be\nalgorithmically found based on data, that is, for any learning algorithm Γ there\nexists a problem instance s such that for any dataset X the output Γ(X) of the\nalgorithm is not a correct solver of s.\nNote that Type 1 as a special case of Type 2 failure is more fundamental since a\n(computable) solution cannot be learned from data if it does not exist. However, we\nshow in Subsection 3.2 that instances of Type 2 free of Type 1 failure exist in the\ncontext of training neural networks. Here, the problem instance is an unknown function\nand the goal of the learning algorithm is to find a neural network that represents the\nsought function based on samples.\n2.2 Neural Networks\nIn this paper, we restrict our attention to feedforward neural networks. For additional\nbackground on (deep) neural networks theory – usually referred to as deep learning –\n7\nwe point to [71]. We characterize neural networks by their structure, coined architec-\nture, and the associated sequence of their parameters, i.e., their weight matrices and\nbias vectors.\nDefinition 2.6. Let L\n∈\nN. An architecture of depth L is a vector S\n:=\n(N0, N1, . . . , NL−1, NL) ∈NL+1. A neural network with architecture S is a sequence\nof pairs of weight matrices and bias vectors ((Aℓ, bℓ))L\nℓ=1 such that Aℓ∈RNℓ×Nℓ−1\nand bℓ∈RNℓfor all ℓ= 1, . . . , L. We denote the set of neural networks with archi-\ntecture S by NN(S) and the total number of parameters in the architecture by\nN(S) := PL\nℓ=1(NℓNℓ−1 + Nℓ).\nRemark 2.7. Typically, we consider bL := 0 and denote the input dimension N0 := d.\nAlso, throughout this paper, we focus on the case NL = 1 for simplicity of presentation,\neven though the results can be reformulated for the general case.\nThe architecture and the parameter then induce the network’s input-output\nfunction, the so-called realization.\nDefinition 2.8. For Φ ∈NN(S), D ⊂RN0, and σ : R →R denote by RD\nσ (Φ) : D →\nRNL the realization of the neural network Φ with activation σ and domain D, that is,\nRD\nσ (Φ) := TL ◦σ ◦· · · ◦σ ◦T1|D,\nwhere Φ = ((Aℓ, bℓ))L\nℓ=1 and Tℓ(x) := Aℓx + bℓ, ℓ= 1, . . . , L.\nParameters of neural networks are almost always the result of a learning algorithm.\nLet us briefly recapitulate the learning process since it pertains to our discussion\non computability. A learning algorithm, typically some version of stochastic gradient\ndescent, receives as input a dataset of sample pairs (xi, yi)n\ni=1, which are usually\nsampled from some underlying goal function f, that is, f(xi) = yi.\nMachine learning algorithms aim to find a function ˆf approximating f. In deep\nlearning, we typically take ˆf := RD\nσ (Φ) for some neural network Φ with a fixed archi-\ntecture S. The algorithm initializes the network with typically random parameters,\nfollowed by an iterative optimization of a loss function L : NN(S)×\n\u0000RN0 × RNL\u0001n →\nR. A popular choice is the mean square error\nL(Φ, x1, y1, . . . , xn, yn) := 1\nn\nn\nX\ni=1\n\r\rRD\nσ (Φ)(xi) −yi\n\r\r2 ,\nwhere ∥·∥indicates the Euclidean norm throughout the paper. Note that there exists\na homeomorphism between neural networks with architecture S and their parameter\nspace RN(S), that is, NN(S) ≈RN(S). Thus we can view the learning algorithm as\na Borel-Turing computable function Γ : Rn(N0+NL)\nc\n→RN(S)\nc\n, which receives a set of\nsamples (xi, yi)n\ni=1 and returns weights and biases of the optimized network. More\nprecisely, given rational sequences representing the real data the algorithm produces\na sequence representing weights and biases. Therefore, the parameters of the resulting\nneural network will be computable numbers. To distinguish between networks with\nreal and computable parameters, we introduce the notation NN c(S) for the set of\nneural networks with architecture S and parameters in Rc. Again, we can establish a\nhomeomorphism between neural networks NN c(S) and their parameters RN(S)\nc\nwith\n8\nthe inherited topology. An important property of any network in NN c(S) is that its\nrealization is a computable function provided that the applied activation function is\ncomputable.\nFinally, for ease of presentation of our analysis concerning Type 2 failure of neural\nnetworks, we apply the following concise form to describe sampled data sets.\nDefinition 2.9. Given n ∈N, f : Rd →Rm and D ⊂Rd, we denote by Dn\nf,D the set\nof all datasets of size n generated from f on the input domain D, that is,\nDn\nf,D :=\n\b\n(xi, f(xi))n\ni=1 ∈(Rd × Rm)n | xi ∈D, i = 1, . . . , n\n\t\n.\nFor a neural network Φ ∈NN(S) with activation σ we denote for short Dn\nΦ,D :=\nDn\nRD\nσ (Φ),D.\n3 Computability Limitations\nOur first goal is to study and actually establish Type 1 and Type 2 failure for general\nproblem descriptions, namely in classification and learning. Subsequently, we will ana-\nlyze approaches to cope and ideally lessen the derived failures without compromising\nthe generality of the considered problems. The two settings – classification and learn-\ning – are chosen because of their importance in deep learning. Classification is one\nof the main objectives tackled by deep learning, whereas learning is one key compo-\nnent of the method. Having reliable, flexible, and universal learning algorithms hugely\nbenefits the applicability of deep learning in various fields. Thus, classification and\nlearning are suitable choices to highlight the consequences of computability failures.\n3.1 Type 1 Failure in Classification\nWe study classification problems from the viewpoint of computability theory and show\nthat we can find Type 1 failure of computability. A classification problem is modeled\nby a function\nf : D →{1, . . . , C},\nD ⊂Rd,\nC ∈N,\nthat assigns each input x ∈D a corresponding class c ∈{1, . . . , C}. A typical\nexample is image classification where the input domain D is for instance given by\nD = [0, 256]h×w with [0, 256] and h, w ∈N encoding color and size (height and width)\nof an image, respectively. The range [0, 256] may also be quantized so that a discrete\nset such as {0, . . . , 256} represents the input domain. First, we explore the continu-\nous setting before turning to quantized problems in Subsection 4.2. The properties of\nthe input domain play a crucial role in establishing Type 1 failure. It turns out that\nquantization alleviates Type 1 limitations arising in the continuous case.\nAs described in Subsection 2.2, the goal of deep learning is to learn a function\nˆf : D ∩Rd\nc →{1, . . . , C} based on samples (xi, f(xi))n\ni=1 ⊂D × {1, . . . , C} such\nthat ˆf is close to f with respect to a suitable measure. Hence, a crucial question is\nwhether ˆf can be obtained from an algorithmic computation given a specific closeness\ncondition. Equivalently, this question can be expressed in terms of (semi-)decidability\non the input domain of f in Rd if ˆf is expected to exactly emulate f, i.e., ˆf = f|Rdc.\n9\nDefinition 3.1. A set A ⊂D ∩Rd\nc is\n• decidable in D, if its indicator function 1A : D ∩Rd\nc →Rc is computable;\n• semi-decidable in D, if there exists a computable function f : D′ →Rc, D′ ⊂\nD ∩Rd\nc, such that A ⊂D′ and f = 1A|D′.\nRemark 3.2. The notion of (semi-)decidability can be explicitly expressed via algo-\nrithms in the following way. The set A is Borel-Turing decidable in D if there exists\na Turing machine M taking as inputs representations of x ∈D ∩Rd\nc which correctly\nidentifies after finitely many iterations whether x ∈A or x ∈D \\ A. Similarly, A is\nsemi-decidable in D if there exists a Turing machine M taking as inputs representa-\ntions of x ∈D which correctly identifies (after finitely many iterations) every input\nx ∈A, but which may run forever for x ∈D \\ A.\nRecall that computable functions are necessarily continuous on Rd\nc. However, indi-\ncator functions are discontinuous on Rd\nc (excluding the trivial cases Rd\nc and ∅). Thus,\nonly sets of the type Rd\nc ∪B, B ⊂Rd \\Rd\nc, are decidable in Rd. Therefore, decidability\nin Rd is a very restrictive notion that typically will not be satisfied by a classifier f.\nRegarding semi-decidability, the following equivalence is immediate due to the discrete\nimage of f. In particular, the proposition provides a necessary condition for learning\na perfect emulator ˆf = f|Rdc since computability is a prerequisite for learnability.\nProposition 3.3. Let D ⊂Rd and consider f : D →{1, . . . , C}. Then, f|Rdc is\ncomputable if and only if each set f −1(i), i = 1, . . . , C, is semi-decidable in D.\nRemark 3.4. Note that if each set f −1(i), i = 1, . . . , C, is semi-decidable in D, then\nalso each set f −1(i) is decidable in D. Therefore, for f to be computable, D has to\nhave a specific structure. In particular, if D ∩Rd\nc is a connected set homeomorphic\nto Rd\nc, e.g., D = (0, 1)d, then f is not computable unless it is constant on D ∩Rd\nc.\nHowever, in this case, the classification problem is itself trivial. Thus, a necessary\ncondition for computability is that the sets f −1(i) are separated to a certain degree. As\na simple example for this type of problems consider D = f −1(1) ∪f −1(2) with C = 2,\nf −1(1) = (0, 1) and f −1(2) = (2, 3). Here, a simple check of whether a given input is\nsmaller or larger than 1.5 is sufficient to determine the associated class of the input.\nThese observations can be summarized in the following conclusion, where\ndist(A, B) denotes the set distance dist(A, B) = infa∈A,b∈B ∥a −b∥.\nProposition 3.5. Let f : D →{1, . . . , C} be a function such that there exists i ̸= j ∈\n{1, . . . , C} with dist(f −1(i), f −1(j)) = 0. Then, f|Rdc is not computable.\nRemark 3.6. A desired property in certain applications is identifying inputs not\nbelonging to the known classes. For instance, unknown classes or erroneous inputs,\nwhich were not determined beforehand or cannot be unequivocally assigned to the\nknown classes, respectively, may be part of the feasible input set. Formally, we can\nexpress this setting by a classifier\nˆf ′ : D′ →{1, . . . , C + 1},\nD ⊂D′ ⊂Rd,\nwhere D′ is connected, such that ˆf ′(x) = f(x) for x ∈D and ˆf ′(x) = C +1 otherwise.\nHowever, we immediately observe that f ′ is not computable and the desired property\ncannot be achieved.\n10\nThe results in Proposition 3.3 and 3.5 imply that Type 1 computability failure is\nunavoidable in sufficiently general classification problems. Our next step is to study\nwhether algorithmic solvability can be expected in the absence of Type 1 failure.\n3.2 Type 2 Failure in Deep Learning\nWe now focus on Type 2 failure of computability, i.e., situations where a computable\napproximator may exist but cannot be algorithmically found based on data. We explore\nthis phenomenon in the context of deep learning, within a general framework by study-\ning the learnability of functions that can be represented by a neural network from\ndata, independently of the concrete application. This includes any instance of deep\nlearning where Type 1 computability failure does not arise, going beyond the previous\ncontext of classification.\nThe following theorem states that for any learning algorithm, there exist functions\nrepresentable by computable neural networks (i.e., not suffering from Type 1 failure)\nthat the algorithm cannot learn from data. This implies that there is no universal\nalgorithm for training neural networks (based on data), even when a correct solving\nnetwork exists, as deep learning suffers from Type 2 failure of computability. More\nprecisely, for any learning algorithm Γ, there exists a computable neural network\nΦ such that given any training data set generated from Φ, the algorithm Γ cannot\nreconstruct any neural network with the same realization as Φ.\nTheorem 3.7. Let σ : R →R be a Lipschitz continuous, but not affine lin-\near activation function, such that σ|Rc is Banach-Mazur computable. Let S\n=\n(d, N1, . . . , NL−1, 1) be an architecture of depth L ≥2 with N1 ≥3. Let D ⊂Rd\nc be\nbounded with a nonempty interior.\nThen, for all ε > 0, n ∈N, and all Banach-Mazur computable functions Γ :\n(Rd\nc × Rc)n →RN(S)\nc\nthere exists Φ ∈NN c(S) such that for all X ∈Dn\nΦ,D and all\nΦ′ ∈NN c(S) with RD\nσ (Φ′) = RD\nσ (Φ) we have\n∥Γ(X) −Φ′∥2 > ε.\n(1)\nRemark 3.8. The assumption that σ is not affine linear excludes none of the commonly\nused activations such as ReLU, tanh, or sigmoid. Only functions like σ(t) = at + b are\nnot permitted. Moreover, the computability assumption concerning σ is not restrictive\nsince it guarantees a computable realization, a prerequisite for subsequent algorithmic\nevaluation in usage.\nAs a consequence, we cannot reconstruct the original input-output function.\nCorollary 3.9. Under the assumptions of Theorem 3.7, there exists no Banach-Mazur\ncomputable function Γ : (Rd\nc × Rc)n →RN(S)\nc\nfor n ∈N such that for all Φ ∈NN c(S)\nthere exists a dataset X ∈Dn\nΦ,D satisfying\nRD\nσ (Γ(X)) = RD\nσ (Φ).\nNote that the distance in (1) in Theorem 3.7 is shown in the weight space of neural\nnetworks and it is known that networks with weights far apart can still represent\nfunctions close together [72]. Despite the failure of exact reconstruction obtained in\n11\nCorollary 3.9, a neural network with the same architecture, which approximates the\ndesired realization to an arbitrary degree, might still exist. We will return to this\nobservation in Subsection 4.1.2, where we consider strategies for coping with non-\ncomputability by relaxing the exactness condition.\n4 Strategies for Failure Circumvention\nCan we overcome Type 1 and Type 2 limitations described in the previous section?\nWe analyze different approaches to either reformulate or relax the tackled problems\nthus making them less amenable to computability failures. In particular, we explore\ntwo strategies. First, we study the effect of incorporating a reasonable error mode\n(depending on a given task) in the computation. Subsequently, we investigate the\nimpact of moving the problem from the real to a discrete space via quantization.\n4.1 Error Control Strategies\nThe requirement of exact emulation ˆf = f|Rdc of a classification function f or exact\nreconstruction of neural networks as analyzed in Section 3 may be too strict. In a\npractical setting, errors may be unavoidable or even acceptable to a certain degree.\nIn particular, approximation of f|Rdc via ˆf or reconstructing an approximate network\nbased on an appropriate metric is a simpler task than exact emulation or reconstruc-\ntion, respectively. However, in both cases, we certainly would like to have guarantees\neither in the form of a description of inputs that lead to deviations from the ground\ntruth or via worst/average case error bounds. Whether and to what degree such\nguarantees are achievable is the subject of the following analysis.\n4.1.1 Computable Unpredictability of Correctness in Type 1 Failure\nA key observation in classification was that Type 1 failure, i.e., the non-semi-\ndecidability of the classes, is closely associated with the decision boundaries of the\nclasses. Informally speaking, the semi-decidability of classes hinges on the ability to\nalgorithmically describe the decision boundary so that inputs on the decision bound-\nary can be properly classified. Therefore, identifying these critical inputs or indicating\nthat the computation for a given input may be inaccurate would certainly be beneficial.\nIs it possible to implement this identification – a so-called exit flag – algorithmically?\nIn a bigger picture, related questions were already raised in different contexts such as\ngeneral artificial intelligence by Daniel Kahneman [73] or robotics by Pieter Abbeel\n[74]: Can we expect algorithms (powering autonomous agents) to recognize whenever\nthey cannot correctly solve a given task or instance enabling them to ask (a human)\nfor help instead of executing an erroneous response? In other words: ‘Do they know\nwhen they don’t know?’ [75, 76] We immediately observe that this problem depends on\nan exit flag computation in the computability framework. Hence, by studying exit flag\ncomputations, we can also theoretically assess the feasibility of automated help-seeking\nbehavior of autonomous agents in certain scenarios.\nTo study the posed question we do not restrict ourselves to classification functions\nbut consider a slightly more general framework. We formalize the problem for general\n12\nreal-valued functions f : D →R, D ⊂Rd. Assume we are given a computable function\nˆf : Dc →Rc, Dc = D ∩Rd\nc, typically constructed by an algorithmic method to\napproximate f. Our aim is to algorithmically identify inputs x ∈Dc such that ˆf\nsatisfies\n∥f(x) −ˆf(x)∥< ε\nfor given ε > 0.\nIn other words, we ask if there exists an algorithm (a Borel-Turing computable\nfunction) Γε : Dc →Rc such that\nΓε(x) =\n(\n1,\nif ∥f(x) −ˆf(x)∥,\n0,\notherwise.\n(2)\nOne can also further relax the complexity of the task by demanding that an algorithm\nΓ+\nε only identifies inputs x for which ˆf(x) satisfies the ε-closeness condition in (2),\nbut does not necessarily indicate when it does not hold. For instance, Γ+\nε (x) may\neither output zero or not stop the computation in finite time on the given input x\nif ∥f(x) −ˆf(x)∥≥ε. If f is a computable function, we can construct Γε for any\nε > 0. Hence, more interesting problems arise when f is not computable. However,\nchoosing ε large enough, certainly still entails the existence of Γε if f is, for instance,\na bounded function. Therefore, the relevant cases are associated with non-computable\nf and appropriately small ε. By associating Γε and Γ+\nε with classification functions,\nwe can apply Proposition 3.3 to derive the following result.\nProposition 4.1. Let f : D →R, D ⊂Rd and assume that ˆf : Dc →Rc, Dc =\nD ∩Rd\nc, is a computable function. Define for ε > 0 the set\nD<\nε :=\nn\nx ∈Dc\n\f\f ∥f(x) −ˆf(x)∥< ε\no\nThen the following holds:\n1. The function Γε : Dc →Rc given by\nΓε(x) =\n(\n1,\nif ∥f(x) −ˆf(x)∥< ε,\n0,\notherwise,\nis computable if and only if D<\nε is decidable in D.\n2. A computable function Γ+\nε : D′\nc →R, D′\nc ⊂Dc, such that D<\nε ⊂D′\nc and Γ+\nε =\nΓε|D′c exists if and only if D<\nε is semi-decidable in D.\nRemark 4.2. Depending on the context, we might be more interested in finding the\nset of inputs where ˆf fails rather than succeeds. This would lead us to the analogous\nobservation that the semi-decidability of the set\nD≥\nε =\nn\nx ∈Dc\n\f\f ∥f(x) −ˆf(x)∥≥ε\no\ndetermines computability of Γ−\nε : D′\nc →R, D′\nc ⊂Dc, given by Γ−\nε = Γε|D′\nc with\nD≥\nε ⊂D′\nc.\n13\nIn the case of a connected input domain, the application of Proposition 3.5 yields\nthe following result. Informally, it is a direct consequence of the already mentioned\nfact that only trivial subsets of Rd\nc are decidable.\nCorollary 4.3. Under the conditions of Proposition 4.1, additionally assume that D\nis connected. Then an approximator ˆf such that D<\nε is decidable exists if and only if f\ncan be computably approximated with precision ε, that is, if there exists a computable\nfunction ˜f such that\n∥f −˜f∥∞< ε.\nRemark 4.4. As the following example shows, a similar statement does not hold if\nD<\nε is only assumed to be semi-decidable. Consider the sign function sgn : R →R,\nwhich is non-continuous on Rc and therefore non-computable, and take d\nsgn(x) =\n2\nπarctan(x). For a given ε > 0 we can computably construct intervals (−∞, −x0)\nand (x0, ∞) where |sgn(x) −d\nsgn(x)| < ε, i.e., D<\nε = (−∞, −x0) ∪(x0, ∞) is semi-\ndecidable. In fact, we can adjust the approximator to achieve the desired precision on\na given interval (x0, ∞), x0 > 0. However, due to the discontinuity at 0, no computable\nfunction approximating sgn on the entire real line with precision ε < 1 exists.\nThese results also directly apply to the classification setting as a special case of\nthe considered framework. By design, the classification setting even allows for stronger\nstatements regarding the magnitude of the error ε. In particular, requiring precision\nof ε < 1\n2 for the approximator ˆf of a classifier f mapping to {1, . . . , C} is equivalent\nto requiring exact emulation ˆf = f|Rdc. However, this scenario was already covered in\nSubsection 3.1, where Type 1 failure was established. Hence, we can conclude that\nexit flag computations may be beneficial in certain situations but it is not appropriate\nto tackle Type 1 failure in classification.\nInstead of analyzing individual inputs, one could also derive global guarantees for\nthe approximator ˆf. For instance, one could determine the magnitude of the failure\nset for some appropriate measure, i.e., assess the likelihood of errors for some given\ninput domain. Although this approach is interesting it has two main limitations for our\nintended goals. First, there does not exist an acknowledged algorithmic notion covering\nthis framework that allows for theoretical studies - we refer to Appendix A for more\ndetails and possible concepts to tackle this problem. Second and more importantly,\nthis strategy typically leads to a global quantitative measure of failure whereas we\nare interested in local guarantees for a given input. In essence, this framework does\nnot tackle the limitations of individual inputs but provides further information on the\nentire input domain of the general problem.\nFinally, we want to highlight that Proposition 4.1 and Corollary 4.3 imply that\nthe algorithms envisioned by Kahneman and Abbeel can not be realized on digital\nhardware. For any algorithm Γ tackling a problem described by a non-computable\nfunction, there exist instances that Γ answers incorrectly and there is no algorithm\nΓexit that recognizes the failure for all erroneous instances. Hence, the answer to ‘Do\nthey know when they don’t know?’ from the digital computing perspective is negative\nin certain scenarios.\n14\n4.1.2 Problem Relaxation for Type 2 Failure\nIn contrast to classification, relaxing the exact reconstruction requirement yields learn-\ning benefits. The main advantage is that the solution set of networks connected to\napproximate reconstruction is noticeably larger enabling algorithmic approaches to\nperform the previously unattainable reconstruction task. Type 2 failure does not\narise in our learning setting on the training data if a certain approximation error is\npermitted.\nTheorem 4.5. Let σ : R →R be such that σ|Rc is computable and let S =\n(d, N1, . . . , NL−1, 1) be an architecture.\nThen, for all ε > 0 and n ∈N, there exists a computable function Γ : (Rd\nc ×Rc)n →\nRN(S)\nc\nsuch that for all Φ ∈NN c(S) and X ∈Dn\nΦ,Rdc we have\n\f\fRRd\nc\nσ (Γ(X)) (x) −y\n\f\f < ε\nfor (x, y) ∈X.\n(3)\nAlthough Theorem 4.5 provides a positive computability result concerning learning\nit still has certain limitations:\n• The algorithm constructed to prove Theorem 4.5 serves only for theoretical anal-\nysis. It is typically not efficiently translatable into a practically usable one in\na generic problem setting. Thus, a relevant and open question is whether more\napplicable learning algorithms can be constructed with similar guarantees.\n• The learning algorithm Γ derived from Theorem 4.5 presupposes a fixed accuracy\nparameter ε and dataset size n, i.e., for different choices of these parameters a\nseparate learning algorithm needs to be constructed.\n• The reconstruction guarantee only holds on the training data. However, given\naccess to the training data X, the posed task could be solved without constructing\na neural network since one could explicitly implement an algorithm that on the\ninput of (x, y) ∈X returns y. Constructing a neural network with some prescribed\nrealization on the training data is expected to yield a network that performs\n’reasonably well’ on unseen data. The learning and evaluation, as given in (3),\nshould ideally be performed on different data to ensure this hypothesis.\nWe cannot alleviate the first but resolve the second issue. Indeed, the proof of Theorem\n4.5 implies that one could generalize the algorithm Γ by requiring it to take (com-\nputable) ε and n as additional inputs. In particular, the key step of the proof relies on\nan enumeration argument that can be extended to incorporate ε and n as well. In a\nslightly different setting, one can connect the desired accuracy with the input dimen-\nsion and sample complexity [56]. Hence, the final point is left to consider. By imposing\nfurther conditions on the admissible networks, we can ensure certain generalization\ncapabilities of the networks.\nTheorem 4.6. Let σ : R →R be such that σ|Rc is computable and Lipschitz\ncontinuous. Fix Amax ∈N and let S = (d, N1, . . . , NL−1, 1) be an architecture.\nThen, for all ε > 0 and n ∈N, there exist computable functions Γ : (Rd\nc × Rc)n →\nRN(S)\nc\nand Ψ : RN(S)\nc\n→R+\nc such that for all Φ = ((Aℓ, bℓ))L\nℓ=1 ∈NN c(S) with\n15\nmaxℓ∥Aℓ∥max ≤Amax and X ∈Dn\nΦ,Rd\nc we have\n\f\f\fRRd\nc\nσ (Γ(X)) (x) −y\n\f\f\f < ε\nfor (x, y) ∈X Φ\nΨ(Γ(X)),\n(4)\nwhere X Φ\nr := {(x, Φ(x)) | x ∈S\n(xi,yi)∈X Br(xi)}\nRemark 4.7. For specific classes of neural networks, a uniform lower bound δ on Ψ(Φ)\ncan be established (over the set of networks Φ). Therefore, it is possible to provide\napproximate reconstruction guarantees for the entire input domain D if X Φ\nΨ(Γ(X)) cov-\ners D. For instance, given an equidistant data grid on D with width δ an approximate\nreconstruction guarantee holds for the whole domain.\nIt is straightforward to convince oneself that the imposed conditions on data and\nnetworks are necessary. Guarantees of the form (4) cannot be provided without the\nassumptions. One feasible step beyond the considered setting is to consider arbitrary\ntraining data which is not necessarily sampled from a neural network but some under-\nlying ground truth function g : Rd →R, i.e., the ground truth is not necessarily\nrealizable by a neural network (with given architecture). In its current form, Theorem\n4.5 and 4.6, entail the existence of networks with approximate realization (to the\nground truth network), i.e., the remaining task is algorithmically finding it. For arbi-\ntrary training samples of the form (xi, g(xi))n\ni=1 ⊂(Rd × R)n, one would need to first\nestablish the existence of a neural network approximating the training data, i.e., the\nground truth g, to a sufficient degree with the prescribed architecture. However, by\ntaking into account the expressivity of the architecture, it is feasible to provide com-\nputability guarantees in this setting as well under sufficient conditions on the data\nand class of ground truth functions. Another strategy is to ask for an optimal network\nminimizing the error for some loss function on the training data. In [34], it was shown\nthat this scenario indeed suffers from Type 2 failure of computability, even for sim-\nple networks. A strategy to nevertheless avoid Type 2 failure could consist of relaxing\nthe optimality requirement on the desired network but this problem warrants further\ninvestigations.\nFinally, we want to highlight the key differences between neural network training\nin our setting and general classification tasks. Why do the problems entail different\ndegrees of computability failures? The crucial observation is that our considered model\nof neural networks does not cover the typical (more general) neural network model\napplied in classification. A classifier ˆf as considered in Subsection 3.1 is a discontin-\nuous function, whereas a neural network, assuming continuous activation, possesses\na continuous realization. In the context of neural network classification, a classifier\nˆf = ˆf1 ◦RD\nσ (Φ) is composed of a neural network Φ – the so-called feature map – and\na (discontinuous) function ˆf1 mapping from the features to the classes. The Type 1\nfailure of computability appears at ˆf1, with no a priori restriction on computability\nof Φ, i.e., Type 2 failure on this level is avoidable.\n4.2 Quantization Strategies\nTransformation of sequences approximating real numbers with arbitrary precision is\nthe general paradigm describing digital computation on real numbers introduced by\n16\nTuring himself [30]. It provides the tools to study the capabilities and limitations of\nperfect digital computing. Due to real-world constraints, a simplified and more appli-\ncable quantized model is typically employed to implement digital computations in\npractice. In quantization, real numbers are approximated by a discrete set of rationals.\nFor instance, under fixed-point quantization, real numbers are replaced by rational\nnumbers with a fixed number k of decimal places in some base system b, i.e., algo-\nrithms strictly operate on the set b−kZ. Thus, assuming fixed-point quantization we\ncan restrict the analysis without loss of generality to classification problems on Zd\nas well as neural networks with integer parameters and data. The crucial difference\nbetween integer computability and the previously considered real-valued framework\nis the feasibility of exact computations so that approximative computations are not\ninherently necessary. The concept of exact algorithmic computations on integers is\ndescribed by recursive functions (which the previously considered framework of Borel-\nTuring computable functions extends to the real domain); we refer to [70] for more\ndetails on recursive functions and classical computability on discrete sets.\nQuantizing the parameter does not lead to a critical degradation of expressive\npower in neural networks. In particular, in the limit, the capabilities of quantized and\nreal networks align [77–79]. Nevertheless, we show next that in the context of the\nsimplest quantization technique, namely fixed-point quantization, the computability\nlimitations introduced in Section 3 are alleviated to a certain degree. In particular, we\nestablish that both Type 2 and Type 1 failures of computability are mainly resolved\nin this setting. Therefore, a crucial question is whether the quantization process for a\ngiven problem, if necessary, can be carried out algorithmically without computability\nfailure.\n4.2.1 Computability of Quantized Deep Learning\nWe show that under fixed-point quantization, the negation of Theorem 3.7 holds. That\nis, an algorithm exists that can re-learn the exact realization of neural networks of\nfixed architecture on the training data. To that end, we introduce the set of neural\nnetworks with integer parameters.\nDefinition 4.8. Let S be an architecture. Denote by NN Z(S) ⊂NN c(S) the set of\nneural networks with architecture S and parameters in Z.\nNow, we can formulate the exact statement about re-learning neural networks.\nTheorem 4.9. Let S = (d, N1, . . . , NL−1, 1) be an architecture and let σ : R →R.\nThen, for all n ∈N there exists a recursive function Γ : (Zd × Z)n →ZN(S) such\nthat for all Φ ∈NN Z(S) there exists a dataset X ∈Dn\nΦ,Zd with\nRZd\nσ (Γ(X)) = RZd\nσ (Φ).\nDespite the positive result, we can still raise two main limitations of Theorem 4.9.\nFirst, the theory-to-practice gap in learning algorithms remains an (open) issue as in\nthe previous analysis. Second, Theorem 4.9 only guarantees the existence of a dataset\nenabling reconstruction. Can we improve the statement to ensure reconstruction for\nany dataset satisfying some (weak) conditions? Before answering the question we want\nto point out a well-known fact: One cannot expect an exact reconstruction of a neural\n17\nnetwork’s realization on the entire input domain based on an arbitrary but finite set\nof data samples in general. On the one hand, networks with different architecture or\nparameters may realize the same function, on the other hand, networks, whose outputs\nagree on some inputs, may wildly diverge in their realization [72].\nTheorem 4.10. Let S = (d, N1, . . . , NL−1, 1) be an architecture and let σ : R →R be\nan activation function such that σ|Z is a recursive function.\nThen, for all n ∈N, there exists a recursive function Γ : (Zd × Z)n →ZN(S) such\nthat for all Φ ∈NN Z(S) and X ∈Dn\nΦ,Zd we have\nRZd\nσ (Γ(X)) (x) = RZd\nσ (Φ)(x)\nfor all x ∈X.\nRemark 4.11. From a data-centric perspective, Theorems 4.9 and 4.10 describe edge\ncases, i.e., guarantees applicable to any test data in the former (at the cost of flexibility\nin the training data) and guarantees for any training data (at the cost of flexibility in\nthe test data). Similar to Subsection 4.1.2, one can extend Theorem 4.10 by imposing\nregularity conditions on the considered networks or relaxing the exactness condition\nto provide generalization bounds. Furthermore, if quantization yields a finite input\ndomain one can trivially control the data set sizes to ensure exact generalization on\nthe considered domain.\n4.2.2 Computability of Quantized Classification\nTurning our attention to classification and Type 1 failure, we can distinguish between\ntwo cases. In many applications, classification is performed on a bounded domain D\nsuch as in image classification described in Subsection 3.1. Hence, the quantized version\nof the input domain is finite so any integer-valued function on the quantized domain\nis computable - one can encode the input-output pairs directly in an algorithm.\nProposition 4.12. Let D ⊂Zd and f : D →{1, . . . , C}. If D is bounded, then f is\nrecursive.\nIn contrast, unbounded sets typically do not appear in practical quantized classi-\nfication problems since they correspond to working on an infinite domain. However,\nin such a scenario we cannot provide formal guarantees on the computability of clas-\nsifiers. Similar to the real case, the task reduces to classical (semi-)decidability of\n(infinite) sets of integers, which is not algorithmically solvable in general [70]. Hence,\nin both real and quantized classification Type 1 failure may arise due to non-(semi)-\ndecidable sets in the respective frameworks. Nevertheless, the occurrence of Type 1\nfailure appears to diverge in the frameworks. Although it is intricate to derive a for-\nmal proof to back this statement, informally it is motivated by the observation that\nnon-semi-decidability is a more severe drawback in the real domain. For instance, we\nhave seen that non-trivial sets on Rd\nc are not decidable whereas such a strong claim is\nnot valid for the integer domain.\n4.2.3 Non-Computability of the Quantization Function\nWe have shown that quantization circumvents or at least mitigates Type 1 and Type\n2 failure. Does it imply that in a real-world setting, where we typically compute\n18\nwith digital computers in the quantized model, Type 1 and Type 2 failures do not\narise? Not necessarily, it depends on the ground truth problem. If the ground truth is\nitself quantized, then it typically can be directly translated into the quantized model\nof a digital computer and in principle algorithmically solved. In contrast, a ground\ntruth problem on a continuous domain must first be converted into an appropriate\nquantized problem that approximates the original problem. Therefore, it would be\ndesirable to provide computable guarantees that this approximation is close to the\noriginal. However, for a non-computable ground truth problem such an algorithmic\nverification contradicts its non-computability, as the ground truth problem could then\nbe algorithmically computed/approximated using the verifier. Thus, quantization itself\nis a non-computable task, which is a direct consequence of Proposition 4.1.\nProposition 4.13. Let f : R →R such that f|Rc is not computable and define for all\nx ∈Rc\nˆf(x) := f\n\u0000⌈x −1\n2⌉\n\u0001\n.\nThen, there exists ε0 > 0 such that for all ε ≤ε0 the function Γε : Rc →Rc given by\nΓε(x) =\n(\n1,\nif ∥f(x) −ˆf(x)∥< ε,\n0,\notherwise\nis not computable.\nAcknowledgements\nThis work of H. Boche was supported in part by the German Federal Ministry of\nEducation and Research (BMBF) within the national initiative on 6G Communication\nSystems through the Research Hub 6G-life under Grant 16KISK002.\nThis work of G. Kutyniok was supported in part by the Konrad Zuse School of\nExcellence in Reliable AI (DAAD), the Munich Center for Machine Learning (BMBF)\nas well as the German Research Foundation under Grants DFG-SPP-2298, KU\n1446/31-1 and KU 1446/32-1. Furthermore, G. Kutyniok acknowledges support from\nLMUexcellent, funded by the Federal Ministry of Education and Research (BMBF)\nand the Free State of Bavaria under the Excellence Strategy of the Federal Government\nand the L¨ander as well as by the Hightech Agenda Bavaria\nReferences\n[1] LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444\n(2015)\n[2] Minar, M.R., Naher, J.: Recent advances in deep learning: An overview. arXiv\npreprint arXiv:1807.08169 (2018)\n[3] Le, Q., Miralles-Pechu´an, L., Kulkarni, S., Su, J., Boydell, O.: An overview of\ndeep learning in industry. Data analytics and AI, 65–98 (2020)\n19\n[4] Mathew, A., Amudha, P., Sivakumari, S.: Deep learning techniques: an overview.\nAdvanced Machine Learning Technologies and Applications: Proceedings of\nAMLTA 2020, 599–608 (2021)\n[5] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep\nconvolutional neural networks. Advances in neural information processing systems\n25 (2012)\n[6] Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-r., Jaitly, N., Senior,\nA., Vanhoucke, V., Nguyen, P., Sainath, T.N., et al.: Deep neural networks for\nacoustic modeling in speech recognition: The shared views of four research groups.\nIEEE Signal processing magazine 29(6), 82–97 (2012)\n[7] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche,\nG., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.:\nMastering the game of Go with deep neural networks and tree search. Nature\n529(7587), 484–489 (2016)\n[8] Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution\nimage synthesis. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 12873–12883 (2021)\n[9] Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,\nA., Kalchbrenner, N., Senior, A., Kavukcuoglu, K., et al.: Wavenet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499 12 (2016)\n[10] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: GPT-4 technical report.\narXiv preprint arXiv:2303.08774 (2023)\n[11] Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O.,\nTunyasuvunakool, K., Bates, R., ˇZ´ıdek, A., Potapenko, A., et al.: Highly accurate\nprotein structure prediction with alphafold. Nature 596(7873), 583–589 (2021)\n[12] Cybenko, G.: Approximation by superpositions of a sigmoidal function. Mathe-\nmatics of control, signals and systems 2(4), 303–314 (1989)\n[13] Hornik, K., Stinchcombe, M., White, H.: Multilayer feedforward networks are\nuniversal approximators. Neural networks 2(5), 359–366 (1989)\n[14] Funahashi, K.-I.: On the approximate realization of continuous mappings by\nneural networks. Neural networks 2(3), 183–192 (1989)\n[15] Bach, S., Binder, A., Montavon, G., Klauschen, F., M¨uller, K.-R., Samek, W.: On\npixel-wise explanations for non-linear classifier decisions by layer-wise relevance\npropagation. PloS one 10(7), 0130140 (2015)\n20\n[16] Ras, G., Xie, N., Gerven, M., Doran, D.: Explainable deep learning: A field guide\nfor the uninitiated. J. Artif. Int. Res. 73 (2022)\n[17] Chang, C.-H., Creager, E., Goldenberg, A., Duvenaud, D.: Explaining image\nclassifiers by counterfactual generation. arXiv preprint arXiv:1807.08024 (2018)\n[18] Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572 (2014)\n[19] Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., Madry, A.: Robustness\nmay be at odds with accuracy. In: 7th International Conference on Learning\nRepresentations (ICLR) (2019)\n[20] Kolek, S., Windesheim, R., Andrade-Loarca, H., Kutyniok, G., Levie, R.:\nExplaining image classifiers with multiscale directional image representation. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18600–18609 (2023)\n[21] Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang,\nY., Chen, Y., et al.: Siren’s song in the AI ocean: A survey on hallucination in\nlarge language models. arXiv preprint arXiv:2309.01219 (2023)\n[22] Adcock, B., Dexter, N.: The gap between theory and practice in function approxi-\nmation with deep neural networks. SIAM Journal on Mathematics of Data Science\n3(2), 624–655 (2021)\n[23] Antun, V., Renna, F., Poon, C., Adcock, B., Hansen, A.C.: On instabilities of\ndeep learning in image reconstruction and the potential costs of AI. Proceedings\nof the National Academy of Sciences 117(48), 30088–30095 (2020)\n[24] He, Y., Meng, G., Chen, K., Hu, X., He, J.: Towards security threats of deep\nlearning systems: A survey. IEEE Transactions on Software Engineering 48(5),\n1743–1770 (2020)\n[25] Boche, H., Fono, A., Kutyniok, G.: Mathematical algorithm design for deep\nlearning under societal and judicial constraints: The algorithmic transparency\nrequirement. arXiv preprint arXiv:2401.10310 (2024)\n[26] Fettweis, G.P., Boche, H.: On 6G and trustworthiness. Commun. ACM 65(4),\n48–49 (2022)\n[27] Wu, X., Xiao, L., Sun, Y., Zhang, J., Ma, T., He, L.: A survey of human-in-the-\nloop for machine learning. Future Gener. Comput. Syst. 135, 364–381 (2022)\n[28] Liu, L., Lu, S., Zhong, R., Wu, B., Yao, Y., Zhang, Q., Shi, W.: Computing\nsystems for autonomous driving: State of the art and challenges. IEEE Internet\nThings J. 8(8), 6469–6486 (2021)\n21\n[29] Muhammad, K., Ullah, A., Lloret, J., Ser, J.D., Albuquerque, V.H.C.: Deep learn-\ning for safe autonomous driving: Current challenges and future directions. IEEE\nTrans. Intell. Transp. Syst. 22(7), 4316–4336 (2021)\n[30] Turing, A.M.: On computable numbers, with an application to the Entschei-\ndungsproblem. Proceedings of the London Mathematical Society s2-42(1),\n230–265 (1936)\n[31] Boche, H., Fono, A., Kutyniok, G.: Limitations of deep learning for inverse prob-\nlems on digital hardware. IEEE Transactions on Information Theory 69(12),\n7887–7908 (2023)\n[32] Brattka, V., Ziegler, M.: Turing computability of (non-)linear optimization.\nIn: Proceedings of the 13th Canadian Conference on Computational Geometry\n(CCCG’01) (2001)\n[33] Boche, H., Schaefer, R.F., Poor, H.V.: Algorithmic computability and approx-\nimability of capacity-achieving input distributions. IEEE Transactions on Infor-\nmation Theory 69(9), 5449–5462 (2023)\n[34] Lee, Y., Boche, H., Kutyniok, G.: Computability of optimizers. IEEE Transactions\non Information Theory 70(4), 2967–2983 (2024)\n[35] Boche, H., Fono, A., Kutyniok, G.: Non-computability of the pseudoinverse on\ndigital computers. arXiv preprint arXiv:2212.02940 (2022)\n[36] Colbrook, M.J., Antun, V., Hansen, A.C.: The difficulty of computing stable and\naccurate neural networks: On the barriers of deep learning and Smale’s 18th\nproblem. Proceedings of the National Academy of Sciences 119(12), 2107151119\n(2022)\n[37] Gray, R.M., Neuhoff, D.L.: Quantization. IEEE transactions on information\ntheory 44(6), 2325–2383 (1998)\n[38] Bar-Shalom, O., Weiss, A.J.: DOA estimation using one-bit quantized measure-\nments. IEEE Transactions on Aerospace and Electronic Systems 38(3), 868–884\n(2002)\n[39] Jacovitti, G., Neri, A.: Estimation of the autocorrelation function of complex\nGaussian stationary processes by amplitude clipped signals. IEEE transactions\non information theory 40(1), 239–245 (1994)\n[40] Roth, K., Munir, J., Mezghani, A., Nossek, J.A.: Covariance based signal parame-\nter estimation of coarse quantized signals. In: 2015 IEEE International Conference\non Digital Signal Processing (DSP), pp. 19–23 (2015). IEEE\n22\n[41] Yang, J., Shen, X., Xing, J., Tian, X., Li, H., Deng, B., Huang, J., Hua, X.-\ns.: Quantization networks. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 7308–7316 (2019)\n[42] Maly, J., Saab, R.: A simple approach for quantizing neural networks. Applied\nand Computational Harmonic Analysis 66, 138–150 (2023)\n[43] Christensen, D.V., Dittmann, R., Linares-Barranco, B., Sebastian, A., Le Gallo,\nM., Redaelli, A., Slesazeck, S., Mikolajick, T., Spiga, S., Menzel, S., Valov, I.,\nMilano, G., Ricciardi, C., Liang, S.-J., Miao, F., Lanza, M., Quill, T.J., Keene,\nS.T., Salleo, A., Grollier, J., Markovic, D., Mizrahi, A., Yao, P., Yang, J.J., Indi-\nveri, G., Strachan, J.P., Datta, S., Vianello, E., Valentian, A., Feldmann, J., Li,\nX., Pernice, W.H., Bhaskaran, H., Furber, S., Neftci, E., Scherr, F., Maass, W.,\nRamaswamy, S., Tapson, J., Panda, P., Kim, Y., Tanaka, G., Thorpe, S., Bar-\ntolozzi, C., Cleland, T.A., Posch, C., Liu, S.-C., Panuccio, G., Mahmud, M.,\nMazumder, A.N., Hosseini, M., Mohsenin, T., Donati, E., Tolu, S., Galeazzi, R.,\nChristensen, M.E., Holm, S., Ielmini, D., Pryds, N.: 2022 Roadmap on neuro-\nmorphic computing and engineering. Neuromorph. Comput. Eng. 2(2) (2022).\n022501\n[44] Church, A.: A note on the Entscheidungsproblem. The journal of symbolic logic\n1(1), 40–41 (1936)\n[45] Boche, H., Deppe, C.: Computability of the zero-error capacity of noisy channels.\nIn: 2021 IEEE Information Theory Workshop (ITW), pp. 1–6 (2021)\n[46] Bastounis, A., Hansen, A.C., Vlaˇci´c, V.: The extended Smale’s 9th problem – On\ncomputational barriers and paradoxes in estimation, regularisation, computer-\nassisted proofs and learning. arXiv:2110.15734 (2021)\n[47] Boche, H., Pohl, V.: On the algorithmic solvability of spectral factorization and\napplications. IEEE Trans. Inf. Theory 66(7), 4574–4592 (2020)\n[48] Boche, H., M¨onich, U.J.: Turing computability of Fourier transforms of bandlim-\nited and discrete signals. IEEE Trans. Signal Process. 68, 532–547 (2020)\n[49] Boche, H., Schaefer, R.F., Poor, H.V.: Shannon meets Turing: Non-computability\nand non-approximability of the finite state channel capacity. Communications in\nInformation and Systems 20(2), 81–116 (2020)\n[50] Boche, H., Schaefer, R.F., Poor, H.V.: Denial-of-service attacks on communica-\ntion systems: Detectability and jammer knowledge. IEEE Transactions on Signal\nProcessing 68, 3754–3768 (2020)\n[51] Blum, A.L., Rivest, R.L.: Training a 3-node neural network is NP-complete.\nNeural Networks 5(1), 117–127 (1992)\n23\n[52] Vu, V.H.: On the infeasibility of training neural networks with small mean-\nsquared error. IEEE Transactions on Information Theory 44(7), 2892–2900\n(1998)\n[53] Bastounis, A., Hansen, A.C., Vlaˇci´c, V.: The mathematics of adversarial attacks in\nAI – why deep learning is unstable despite the existence of stable neural networks.\narXiv preprint arxiv:2109.06098 (2021)\n[54] Wind, J.S., Antun, V., Hansen, A.C.: Implicit regularization in AI meets general-\nized hardness of approximation in optimization – sharp results for diagonal linear\nnetworks. arXiv preprint arxiv:2307.07410 (2023)\n[55] Gazdag, L.E., Hansen, A.C.: Generalised hardness of approximation and the SCI\nhierarchy – on determining the boundaries of training algorithms in AI. arXiv\npreprint arxiv:2209.06715 (2023)\n[56] Berner, J., Grohs, P., Voigtlaender, F.: Learning ReLU networks to high uniform\naccuracy is intractable. arXiv preprint arXiv:2205.13531 (2022)\n[57] Chen, S., Gollakota, A., Klivans, A., Meka, R.: Hardness of noise-free learning\nfor two-hidden-layer neural networks. Advances in Neural Information Processing\nSystems 35, 10709–10724 (2022)\n[58] Olah, C.: Mechanistic Interpretability, Variables, and the Importance of Inter-\npretable Bases. https://www.transformer-circuits.pub/2022/mech-interp-essay\n(2022)\n[59] K¨astner, L., Crook, B.: Explaining AI through mechanistic interpretability (2023).\nhttp://philsci-archive.pitt.edu/22747/\n[60] Biondi, A., Nesti, F., Cicero, G., Casini, D., Buttazzo, G.: A safe, secure, and\npredictable software architecture for deep learning in safety-critical systems. IEEE\nEmbed. Syst. Lett. 12(3), 78–82 (2020)\n[61] Zhang, H., Chen, H., Xiao, C., Gowal, S., Stanforth, R., Li, B., Boning, D.S.,\nHsieh, C.: Towards stable and efficient training of verifiably robust neural\nnetworks. In: ICLR 2020 (2019)\n[62] Mirman, M., H¨agele, A., Bielik, P., Gehr, T., Vechev, M.: Robustness certification\nwith generative models. In: SIGPLAN PLDI 2021, pp. 1141–1154. Association\nfor Computing Machinery, New York, NY, USA (2021)\n[63] Katz, G., Barrett, C., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: An effi-\ncient SMT solver for verifying deep neural networks. In: Majumdar, R., Kunˇcak,\nV. (eds.) Computer Aided Verification, pp. 97–117. Springer, Cham (2017)\n[64] Boche, H., Fono, A., Kutyniok, G.: Mathematical algorithm design for deep\n24\nlearning under societal and judicial constraints: The algorithmic transparency\nrequirement. arXiv preprint arXiv:2401.10310 (2024)\n[65] Bastounis, A., Gorban, A.N., Hansen, A.C., Higham, D.J., Prokhorov, D., Sutton,\nO., Tyukin, I.Y., Zhou, Q.: The boundaries of verifiable accuracy, robustness,\nand generalisation in deep learning. In: Iliadis, L., Papaleonidas, A., Angelov, P.,\nJayne, C. (eds.) Artificial Neural Networks and Machine Learning – ICANN 2023,\npp. 530–541. Springer, Cham (2023)\n[66] Boche, H., Fono, A., Kutyniok, G.: Inverse problems are solvable on real number\nsignal processing hardware. arXiv preprint arXiv:2204.02066 (2022)\n[67] Weihrauch, K.: Computable Analysis: An Introduction. Springer, Berlin &\nHeidelberg (2012)\n[68] Pour-El, M.B., Richards, J.I.: Computability in Analysis and Physics. Perspec-\ntives in Logic. Cambridge University Press, Cambridge (2017)\n[69] Avigad, J., Brattka, V.: In: Downey, R. (ed.) Computability and analysis: the\nlegacy of Alan Turing. Lecture Notes in Logic, pp. 1–47. Cambridge University\nPress, Cambridge (2014)\n[70] Cooper, S.B.: Computability Theory. Chapman and Hall/CRC, New York (2017)\n[71] Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT press, Cambridge,\nMassachusetts (2016). http://www.deeplearningbook.org\n[72] Petersen, P., Raslan, M., Voigtlaender, F.: Topological properties of the set of\nfunctions generated by neural networks of fixed size. Foundations of computa-\ntional mathematics 21(2), 375–444 (2021)\n[73] Fridman, L.: Daniel Kahneman: Thinking Fast and Slow, Deep Learning, and AI.\nhttps://lexfridman.com/daniel-kahneman/ (2020)\n[74] Covariant: AI Robotics for the Real World. https://www.youtube.com/watch?\nv=AAr99hQ64AI (2021)\n[75] Cheng, Q., Sun, T., Liu, X., Zhang, W., Yin, Z., Li, S., Li, L., He, Z., Chen,\nK., Qiu, X.: Can AI assistants know what they don’t know? arXiv preprint\narxiv:2401.13275 (2024)\n[76] Ren, A.Z., Dixit, A., Bodrova, A., Singh, S., Tu, S., Brown, N., Xu, P., Takayama,\nL., Xia, F., Varley, J., Xu, Z., Sadigh, D., Zeng, A., Majumdar, A.: Robots that\nask for help: Uncertainty alignment for large language model planners. In: 7th\nAnnual Conference on Robot Learning (2023)\n[77] Bolcskei, H., Grohs, P., Kutyniok, G., Petersen, P.: Optimal approximation with\nsparsely connected deep neural networks. SIAM Journal on Mathematics of Data\n25\nScience 1(1), 8–45 (2019)\n[78] Elbr¨achter, D., Perekrestenko, D., Grohs, P., B¨olcskei, H.: Deep neural network\napproximation theory. IEEE Transactions on Information Theory 67(5), 2581–\n2623 (2021)\n[79] Haase, C.A., Hertrich, C., Loho, G.: Lower bounds on the depth of integral ReLU\nneural networks via lattice polytopes. In: The Eleventh International Conference\non Learning Representations (2023)\n[80] Zhou, Q.: Computable real-valued functions on recursive open and closed subsets\nof Euclidean space. Mathematical Logic Quarterly 42(1), 379–409 (1996)\n[81] Iljazovi´c, Z., Kihara, T.: Computability of subsets of metric spaces. In: Brattka,\nV., Hertling, P. (eds.) Handbook of Computability and Complexity in Analysis,\npp. 29–69. Springer, Cham (2021)\n[82] Brattka, V., Presser, G.: Computability on subsets of metric spaces. Theoretical\nComputer Science 305(1), 43–76 (2003)\n[83] Parker, M.W.: Three concepts of decidability for general subsets of uncountable\nspaces. Theoretical Computer Science 351(1), 2–13 (2006)\nAppendix A\n(Semi-)decidability of real sets\nIn this section, we provide further background on the (semi-)decidability of subsets of\nreal numbers. For more details, we refer to [67, 80–83].\nFirst, note that feasible notions of computability exist beyond Borel-Turing and\nBanach-Mazur computability. A common approach is to relax the computability\nrequirements on the input domain. The underlying idea is to separate the mapping\nfrom the input description leading to the following definition of computable function,\nwhich we call oracle computability to distinguish it from the previous notions.\nDefinition A.1 (Oracle model). For x ∈Rd, a sequence (qk)∞\nk=1 ⊂Qd such that\n∥x −qk∥≤1\n2k\nfor all n ∈N,\nis called an oracle representation of x. A function f : D →Rm\nc , where D ⊂Rd, is oracle\ncomputable if there exists an Oracle Turing machine M such that for all x ∈D and\nall oracle representations (qk)∞\nk=1 of x the sequence (M(qk))∞\nk=1 is a representation of\nf(x).\nRemark A.2. Note that, unlike Borel-Turing computability, the representing rational\nsequence (qk)∞\nk=1 is not required to be computable. By the density of Q, any real num-\nber has an oracle representation and, therefore, we can study computability on the\nwhole real line. Intuitively, one can think of the sequences (qk)∞\nk=1 being provided to\nthe Turing machine by an oracle tape; for an introduction on Oracle Turing machines\n26\nsee citecomputabilitybook. This model is more general, but in typical practical appli-\ncations, the presence of an oracle able to approximate any real number to arbitrary\nprecision cannot be assumed.\nThe differences in the computability notion (and the respective input domains)\nin comparison with Borel-Turing computability also directly transfer to the (semi-\n)decidability of sets: Only trivial subsets of Rd are oracle decidable – whereas for\nBorel-Turing decidability the same statement holds for Rd\nc.\nDefinition A.3. A set A ⊂D ∩Rd is\n• oracle decidable in D, if its indicator function 1A : D →Rc is oracle computable;\n• oracle semi-decidable in D, if there exists an oracle computable function f : D′ →\nRc, D′ ⊂D, such that A ⊂D′ and f = 1A|D′.\nIntuitively, oracle decidability, as well as Borel-Turing decidability, is infeasible\nin general (except for the trivial cases) since there does not exist an algorithm that\ndecides on arbitrary input x ∈R (via representations) whether x = 0, x > 0 or x < 0\n– the crucial input is the edge case zero [68]. Hence, the best one can hope for is a\nnotion of decidability ‘up to equality’: Instead of relying on the characterization of\n(semi-)decidability via characteristic functions, one can consider the (continuous and\ncomputable) distance function dA : Rd →R of A ⊂Rd defined by\ndA(x) := dist(x, A) = inf\na∈A ∥x −a∥.\nThe distance function allows for a given x ∈Rd to compute how close x lies to A\nalthough, in general, we cannot determine whether x ∈A or x /∈A. Therefore, a\ndecidability notion based on the distance function does not lead to the existence of\nalgorithms deciding membership for a given set even though closed sets are uniquely\ndetermined by their distance function.\nNevertheless, for subsets of natural numbers, one can derive an interesting\nconnection between classical decidability and the distance function [80, 82].\nProposition A.4. A subset A ⊂N is decidable (in the classical sense), if and only\nif A considered as a subset of the real numbers induces a (oracle) computable distance\nfunction.\nA similar statement also holds for semi-decidable sets on N. To that end, we\nintroduce a specific characterization of oracle semi-decidable sets [67].\nTheorem A.5. Let V ⊂Rd. The following are equivalent:\n(i) V is oracle semi-decidable.\n(ii) V is recursively enumerable open, i.e., there exists a Turing machine that can\nenumerate centers ck ∈Qd and radii rk ∈Q>0 of open balls such that\nV =\n[\nk∈N\nB(ck, rk),\n(A1)\ni.e., there exist computable rational sequences (ck)∞\nk=1, (rk)∞\nk=1 such that (A1)\nholds.\nRemark A.6. Any oracle semi-decidable set has a specific structure, in particular, it\nis necessarily open.\n27\nThe observed equivalence also carries over to Borel-Turing semi-decidability by\nadjusting the expression in (A1) to\nV ∩Rc =\n[\nn∈N\nB(cn, rn) ∩Rc.\n(A2)\nTo highlight the differences note that one can show under certain assumptions that an\ninterval (a, b) ⊂R with non-computable endpoints a, b ∈R is oracle semi-decidable\nand thus also Borel-Turing semi-decidable. Moreover, [a, b] can be Borel-Turing semi-\ndecidable as well for specific choices of non-computable a, b (since [a, b] ∩Rc = (a, b) ∩\nRc), whereas [a, b] is not oracle semi-decidable as a closed set.\nProposition A.7 ([80, 82]). A set A ⊂N is semi-decidable (in the classical sense),\nif and only if A considered as a subset of the real numbers is recursively enumerable\nopen.\nFinally, we want to summarize and highlight the conclusions based on the intro-\nduced statements. Due to the extended input domain, oracle (semi-)decidability is\nthe stronger condition than Borel-Turing (semi-)decidability. However, in both frame-\nworks decidability is not a practical notion on the real numbers due to the inability to\ndecide equality. Although semi-decidability is less restrictive, it is still rather impracti-\ncal since only open sets are amenable to semi-decidability, e.g., closed sets or sets that\nare neither open nor closed do not fit the framework. Nevertheless, (semi-)decidability\non real domains based on the distance function recovers the classical theory of (semi-\n)decidability on natural numbers indicating that the introduced definitions are indeed\nthe right ones. The difference between the two domains is that non-(semi-)decidability\ndoes not arise due to the inability to test equality on natural numbers and is there-\nfore much scarcer in this setting. In comparison, non-semi-decidability on the real\ndomain may not necessarily be related to the inability to test inequality, however, due\nto this shortcoming non-semi-decidability is likely to occur if no further assumptions\nare posed on the considered sets.\nHence, one might urge for more suitable notions of (semi-)decidability on the real\nnumbers, which circumvent equality comparisons. For instance, a reliable description\nof ’near-decidability’ indicates whether an object is in a set or not up to some limited\nerror. Simply relying on the distance function does not immediately entail the desired\nproperty. In contrast, recursive approximability related to a measure µ [83] describes\nthe following setting: Given a parameter n ∈N, there exists an algorithm that correctly\ndecides A ⊂Rd except on some set B ⊂Rd with µ(B) < 2−n, in which case the\nalgorithm still halts but with possibly incorrect output. Hence, there is a trade-off\nbetween correctness and guaranteed termination of the computation in finite time.\nThus, the approach measures the possible error via µ. Borel-Turing semi-decidability\non the other hand ensures that an algorithm always provides the correct output once\nit finalizes the computation but it may not stop for certain inputs. In other words, it\nonly indicates whether a point is near another point that is not in the considered set.\nHowever, this information cannot be used to deduce whether the considered point lies\ninside or outside the set.\n28\nFurther pursuing notions related to recursive approximability is certainly valuable\nand might lead to further insights; in this work, we consider the extension of the classi-\ncal (semi-)decidability definitions that lead to oracle/Borel-Turing (semi-)decidability.\nThese definitions describe the existence of effective (semi-)decision programs, i.e., algo-\nrithms that necessarily compute correct outputs (or do not halt their computations).\nIn this sense, the output of the algorithm can be unequivocally trusted. Moreover,\nthe theory as well as the results in Subsection 3.1 can be extended to spaces with\nless structure than Rd, e.g., to computable metric spaces with Rd being a special case\nthereof [81, 82].\nAppendix B\nProofs\nB.1\nProof of Theorem 3.7\nThe proof of Theorem 3.7 is based on two results we present next. The key component\nof the first lemma lies behind many non-computability results, such as in [36] for the\nspecial case of inverse problems, but here we formulate a general version.\nLemma B.1. Let Θ be a nonempty set, Λ a nonempty set of functions from Θ to Rc,\nε > 0, and Ξ : Θ →P(Rm\nc ), where m ∈N and P denotes the power set. Assume there\nexist sequences (ι1\nk)∞\nk=1, (ι2\nk)∞\nk=1 ⊂Θ satisfying\n(i)\n\f\ff(ι1\nk) −f(ι2\nk)\n\f\f →0 uniformly in f ∈Λ. That is,\n∀δ > 0 ∃k0 ∈N ∀k ≥k0 ∀f ∈Λ :\n\f\ff(ι1\nk) −f(ι2\nk)\n\f\f < δ;\n(ii) for all k ∈N, dist(Ξ(ι1\nk), Ξ(ι2\nk)) > ε.\nThen, for all n ∈N and all Banach-Mazur computable functions Γ : Rn\nc →Rm\nc there\nexists ι ∈Θ such that for all (f1, . . . , fn) ∈Λn:\ndist(Γ(f1(ι), . . . , fn(ι)), Ξ(ι)) > ε\n3.\nProof. For contradiction assume that for some n ∈N there exists a Banach-Mazur\ncomputable function Γ : Rn\nc →Rm\nc such that for all ι ∈Θ there exists (f1, . . . , fn) ∈Λn\nwith\ndist(Γ(f1(ι), . . . , fn(ι)), Ξ(ι)) ≤ε\n3.\n(B3)\nSince Γ is Banach-Mazur computable, it is continuous on Rn\nc [67], that is,\n∀η > 0 ∃δ > 0 ∀x1, x2 ∈Rn\nc : ∥x1 −x2∥< δ ⇒∥Γ(x1) −Γ(x2)∥< η.\nTake η = ε\n3. For the corresponding δ there exists by condition (i). some k ∈N such\nthat for all f ∈Λ we have\n\f\ff(ι1\nk) −f(ι2\nk)\n\f\f < δ\nn. This implies for all (f1, . . . , fn) ∈Λn\nthat\n\r\r(f1(ι1\nk), . . . , fn(ι1\nk)) −(f1(ι2\nk), . . . , fn(ι2\nk))\n\r\r =\nv\nu\nu\nt\nn\nX\ni=1\n(fi(ι1\nk) −fi(ι2\nk))2\n29\n≤\nn\nX\ni=1\nq\n(fi(ι1\nk) −fi(ι2\nk))2 =\nn\nX\ni=1\n\f\ffi(ι1\nk) −fi(ι2\nk)\n\f\f < δ,\nand therefore\n\r\rΓ(f1(ι1\nk), . . . , fn(ι1\nk)) −Γ(f1(ι2\nk), . . . , fn(ι2\nk))\n\r\r < ε\n3.\nTogether with (B3) we get\ndist\n\u0000Ξ(ι1\nk), Ξ(ι2\nk)\n\u0001\n≤dist\n\u0000Γ(f1(ι1\nk), . . . , fn(ι1\nk)), Ξ(ι1\nk)\n\u0001\n+\n\r\rΓ(f1(ι1\nk), . . . , fn(ι1\nk)) −Γ(f1(ι2\nk), . . . , fn(ι2\nk))\n\r\r +\ndist\n\u0000Γ(f1(ι2\nk), . . . , fn(ι2\nk)), Ξ(ι2\nk)\n\u0001\n< 3ε\n3 = ε,\nwhich contradicts condition (ii).\nThe following is a reformulation of Theorem 4.2 from [72], stating that there exist\nfunctions representable by neural networks that are arbitrarily close in the supremum\nnorm but can only be represented by networks with weights arbitrarily far apart. The\nnorm ∥·∥scaling on the (parameter) space of neural networks is used in the mentioned\ntheorem because it provides a bound on the Lipschitz constant of neural network\nrealizations Lip(RD\nσ (·)), i.e., Lip(RD\nσ (Φ)) ≤C ∥Φ∥scaling for some C > 0 and a network\nΦ, thus connecting the parameter space and the function space.\nDefinition B.2. For a neural network Φ = ((Aℓ, bℓ))L\nℓ=1 set\n∥Φ∥scaling := max\n1≤ℓ≤L ∥Aℓ∥max = max\n1≤ℓ≤L max\ni,j |(Aℓ)i,j|.\nLemma B.3 ([72, Theorem 4.2]). Let σ : R →R be Lipschitz continuous, but not\naffine linear. Let S = (d, N1, . . . , NL−1, 1) be an architecture of depth L ≥2 with\nN1 ≥3. Let D ⊂Rd be bounded with a nonempty interior. Then there exist sequences\n(Φk)∞\nk=1, (µk)∞\nk=1 ⊂NN(S) such that\n(i) ∥RD\nσ (Φk) −RD\nσ (µk)∥∞→0,\n(ii) for any (Φ′\nk)∞\nk=1, (µ′\nk)∞\nk=1 ⊂NN(S) with RD\nσ (Φ′\nk) = RD\nσ (Φk) and RD\nσ (µ′\nk) =\nRD\nσ (µk) for all k ∈N, it holds that ∥Φ′\nk −µ′\nk∥scaling →∞.\nRemark B.4. It can be shown that the divergence in point (ii) is uniform in the\nfollowing sense:\n∀ε > 0 ∃k0 ∀k ≥k0\n∀Φ′\nk, µ′\nk ∈NN(S) such that RD\nσ (Φ′\nk) = RD\nσ (Φk), RD\nσ (µ′\nk) = RD\nσ (µk) :\n∥Φ′\nk −µ′\nk∥scaling > ε.\n30\nTo see this, assume RD\nσ (µk) ≡0, and for contradiction let there be a subsequence\n(Φ′\nkℓ)∞\nℓ=1 ⊂NN(S) with RD\nσ (Φ′\nkℓ) = RD\nσ (Φkℓ) and\n\r\rΦ′\nkℓ\n\r\r\nscaling ≤ε for some ε > 0.\nThen for some C > 0:\nLip(RD\nσ (Φkℓ)) = Lip(RD\nσ (Φ′\nkℓ)) ≤C\n\r\rΦ′\nkℓ\n\r\r\nscaling ≤Cε,\nwhich contradicts Lip\n\u0000RD\nσ (Φkℓ)\n\u0001\n→∞in condition (ii).\nFrom the proof in [72] it can also be seen that for a computable σ at least one such\npair of these sequences of neural networks lies in NN c(S).\nProof of Theorem 3.7. Let Θ =\n\b\nRD\nσ (Φ) | Φ ∈NN c(S)\n\t\n. For i ∈{1, . . . , d} and x ∈\nD denote by f i\nx : Θ →Rc the constant operator\nf i\nx(g) = xi\nand by f(x) : Θ →Rc the operator\nf(x)(g) = g(x).\nLet Λ =\n\b\nf i\nx | x ∈D, i ∈{1, . . . , d}\n\t\n∪\n\b\nf(x) | x ∈D\n\t\nand define Ξ : Θ →P(RN(S)\nc\n)\nby\nΞ(g) =\n\b\nΦ | RD\nσ (Φ) = g\n\t\n.\nBy Lemma B.3 there exists a pair of sequences (gk)∞\nk=1, (hk)∞\nk=1 ⊂Θ such that\n∥gk −hk∥∞→0. Therefore also\n\f\ff(x)(gk) −f(x)(hk)\n\f\f →0 uniformly in x ∈D. The\nsame trivially holds for all f i\nx, therefore condition (i). of Lemma B.1 is satisfied.\nBy Remark B.4, the sequences diverge uniformly in the scaling norm and therefore\nalso in the Euclidean norm, meaning dist(Ξ(gk), Ξ(hk)) →∞and, in particular, for\nany ε > 0 there exists k0 such that dist(Ξ(gk), Ξ(hk)) > 3ε for k ≥k0. Hence, condition\n(ii). of Lemma B.1 holds with 3ε.\nTogether, by Lemma B.1 for all n ∈N and all Banach-Mazur computable functions\nΓ : (Rd\nc×Rc)n →RN(S)\nc\nthere exists g ∈Θ, such that for all\n\u0000f1, . . . , fn(d+1)\n\u0001\n∈Λn(d+1)\nwe have\ndist\n\u0000Γ(f1(g), . . . , fn(d+1)(g)), Ξ(g)\n\u0001\n> ε.\nHowever, by construction of Λ and Ξ, this entails that there exists Φ ∈Ξ−1(g),\ni.e., Φ ∈NN c(S), such that for all x1, . . . , xn ∈D and all Φ′ ∈NN c(S) with\nRD\nσ (Φ′) = RD\nσ (Φ) = g we have\n∥Γ(X) −Φ′∥2 > ε.\nB.2\nProof of Theorem 4.5 and 4.6\nThe key component of the proofs in this section relies on enumerating rational neural\nnetworks, i.e., networks with rational parameters, and subsequently controlling the\nerror induced thereby.\n31\nProof of Theorem 4.5. First, enumerate the countable set of rational neural networks\n{Φ1, Φ2, . . . } of the given architecture, in particular, we can associate QN(S) with\n{Φ1, Φ2, . . . }. For all ˆΦ, Φ∗∈NN c(S), ˆΦ is a computable function so that\ngˆΦ,Φ∗(x) :=\n\f\f\fRRd\nc\nσ (ˆΦ)(x) −RRd\nc\nσ (Φ∗)(x)\n\f\f\f\nis computable. Assume the training data X = {(x1, y1), . . . , (xn, yn)} was generated\nby a neural network Φ. Next, we construct an algorithm that correctly recognizes\nwhether gΦk,Φ(xi) < ε for all i = 1, . . . , n: Compute gΦk,Φ(xi) with precision (at least)\n1\n2ε and subsequently check whether the magnitude of the obtained (rational) number\nis smaller than 1\n2ε. If so, the algorithm returns Φk, if not, it continues by increasing\nk ∈N.\nMoreover, by the density of rational networks, there exists a rational network\nΦk0 such that for all i = 1, . . . , n: gΦk0,Φ(xi) < 1\n2ε. Hence, the algorithm terminates\nnot later than the k0-th iteration, returning a correct answer. This characterizes a\ncomputable function Γ satisfying the claim.\nExtending the proof by incorporating the additional conditions yields Theorem 4.6.\nProof of Theorem 4.6. Fix some architecture S. First, observe that for arbitrary\nΦ, ˆΦ ∈NN c(S) and X ∈Dn\nΦ,Rdc the following holds:\n|Φ(x) −ˆΦ(x)| ≤|Φ(x) −Φ(ˆx)| + |Φ(ˆx) −ˆΦ(ˆx)| + |ˆΦ(ˆx) −ˆΦ(x)|\n≤∥x −ˆx∥(Lip(RD\nσ (Φ)) + Lip(RD\nσ (ˆΦ))) + |Φ(ˆx) −ˆΦ(ˆx)|,\nwhere x ∈D and ˆx = argmin{xi:(xi,yi)∈X ∥x −xi∥. Therefore, using Definition B.2 we\nget for arbitrary r > 0 and (x, y) ∈X Φ\nr\n|y −ˆΦ(x)| ≤rC(σ, S)(∥Φ∥scaling + ∥ˆΦ∥scaling) + |Φ(ˆx) −ˆΦ(ˆx)|,\nwhere C(σ, S) > 0 is a computable constant depending on the architecture and the\nactivation function. Hence, applying Theorem 4.5 shows that for all ˜ε > 0 and n ∈N\nthere exists a computable function Γ˜ε : (Rd\nc × Rc)n →RN(S)\nc\nsuch that for all Φ ∈\nNN c(S) and X ∈Dn\nΦ,Rdc we have\n\f\fRRd\nc\nσ (Γ˜ε(X)) (x) −y\n\f\f < rC(σ, S)(∥Φ∥scaling + ∥Γ˜ε(X)∥scaling) + ˜ε\nfor (x, y) ∈X Φ\nr .\nRestricting to input networks Φ with ∥Φ∥scaling ≤Amax and setting\nr∗=\nε −˜ε\nC(σ, S)(Amax + ∥Γ˜ε(X)∥scaling)\nfor some ε > ˜ε\ngives\n\f\f\fRRd\nc\nσ (Γ˜ε(X)) (x) −y\n\f\f\f < ε\nfor (x, y) ∈X Φ\nr∗.\n32\nFinally, for given ε > 0 and n ∈N, set Γ = Γ 1\n2 ε and define Ψ : RN(S)\nc\n→Rc by\nΨ(Φ) =\nε\n2C(σ, S)(Amax + ∥Φ∥scaling).\nObserving that Γ satisfies (4) and Ψ is a computable function (since C(σ, S) and\n∥· ∥scaling are computable, and we may assume without loss of generality that ε and\nAmax are computable) gives the claim.\nB.3\nProf of Theorem 4.9 and 4.10\nAn enumeration argument similar to the ones in the previous proofs implies Theorem\n4.9. In particular, the idea is to encode the target network as a single datapoint, which\ncan be done recursively for integer vectors representing neural networks with integer\nparameters.\nProof of Theorem 4.9. Given an architecture S = (d, N1, . . . , NL−1, 1), NN Z(S) can\nbe associated with ZN(S), which in turn can be recursivelly encoded into Zd by a\nrecursivelly invertible function g : ZN(S) →Zd (see for instance [70] for details). Then,\ntaking Γ(X) = g−1(x1) with X = {(x1, y1) . . . , (xn, yn)}, a single datapoint of the\nform\n\u0000g(Φ), RZd\nσ (Φ)(g(Φ)), 0, . . .\n\u0001\n∈(Zd × Z)n can be used to reconstruct any neural\nnetwork Φ ∈NN Z(S). Here we utilize the fact, that we can choose the dataset for\neach network specifically.\nBy taking the training data more explicitly into account Theorem 4.10 follows.\nProof of Theorem 4.10. Given a dataset X = {(xi, yi)}n\ni=1 ∈Dn\nΦ,Zd, enumerate the\ncountable set of all neural networks {Φ1, Φ2, . . . } with a given architecture S, in par-\nticular, we can associate ZN(S) with {Φ1, Φ2, . . . }. For increasing k ∈N, check whether\nfor all i = 1, . . . , n: RZd\nσ (Φk)(xi) = yi. If so, return Φk, if not, continue.\nIf the data was generated using a neural network Φ = Φk0, then the algorithm\nterminates at the latest in the k0-th iteration, returning a correct answer. This\ncharacterizes a (partially) recursive function Γ satisfying the theorem.\n33\n",
  "categories": [
    "cs.LG",
    "cs.CC",
    "68T07, 68T05, 03D80, 65D15"
  ],
  "published": "2024-08-12",
  "updated": "2024-08-12"
}