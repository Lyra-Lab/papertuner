{
  "id": "http://arxiv.org/abs/2408.07419v1",
  "title": "Unsupervised Stereo Matching Network For VHR Remote Sensing Images Based On Error Prediction",
  "authors": [
    "Liting Jiang",
    "Yuming Xiang",
    "Feng Wang",
    "Hongjian You"
  ],
  "abstract": "Stereo matching in remote sensing has recently garnered increased attention,\nprimarily focusing on supervised learning. However, datasets with ground truth\ngenerated by expensive airbone Lidar exhibit limited quantity and diversity,\nconstraining the effectiveness of supervised networks. In contrast,\nunsupervised learning methods can leverage the increasing availability of\nvery-high-resolution (VHR) remote sensing images, offering considerable\npotential in the realm of stereo matching. Motivated by this intuition, we\npropose a novel unsupervised stereo matching network for VHR remote sensing\nimages. A light-weight module to bridge confidence with predicted error is\nintroduced to refine the core model. Robust unsupervised losses are formulated\nto enhance network convergence. The experimental results on US3D and WHU-Stereo\ndatasets demonstrate that the proposed network achieves superior accuracy\ncompared to other unsupervised networks and exhibits better generalization\ncapabilities than supervised models. Our code will be available at\nhttps://github.com/Elenairene/CBEM.",
  "text": "UNSUPERVISED STEREO MATCHING NETWORK FOR VHR REMOTE SENSING IMAGES\nBASED ON ERROR PREDICTION\nLiting Jiang1,2,3, Yuming Xiang1,2,3, Feng Wang1,2,3, and Hongjian You1,2,3\n1.Aerospace Information Research Institute, Chinese Academy of Sciences\n2.Key Laboratory of Technology in Geo-spatial Information Processing and Application System,\nChinese Academy of Sciences, Beijing, 100190, China\n3.School of Electronic, Electrical and Communication Engineering,\nUniversity of Chinese Academy of Sciences, Beijing, 101408, China\nABSTRACT\nStereo matching in remote sensing has recently garnered in-\ncreased attention, primarily focusing on supervised learning.\nHowever, datasets with ground truth generated by expen-\nsive airbone Lidar exhibit limited quantity and diversity,\nconstraining the effectiveness of supervised networks.\nIn\ncontrast, unsupervised learning methods can leverage the in-\ncreasing availability of very-high-resolution (VHR) remote\nsensing images, offering considerable potential in the realm\nof stereo matching.\nMotivated by this intuition, we pro-\npose a novel unsupervised stereo matching network for VHR\nremote sensing images.\nA light-weight module to bridge\nconfidence with predicted error is introduced to refine the\ncore model. Robust unsupervised losses are formulated to\nenhance network convergence. The experimental results on\nUS3D and WHU-Stereo datasets demonstrate that the pro-\nposed network achieves superior accuracy compared to other\nunsupervised networks and exhibits better generalization ca-\npabilities than supervised models. Our code will be available\nat https://github.com/Elenairene/CBEM.\nIndex Terms— Stereo matching, unsupervised learning,\nuncertainty, disparity estimation\n1. INTRODUCTION\nAs Earth observation and computer vision technologies\nrapidly advance, 3D reconstruction has gained increasing\nattention in remote sensing. Stereo matching, a crucial step in\n3D reconstruction, generates dense pixel-by-pixel matches,\nyielding disparity maps that enable height information extrac-\ntion. The application of 3D reconstruction in remote sensing\noffers a novel perspective for observation and analysis of the\nEarth’s surface, emerging as a prominent research topic.\nTraditional stereo matching methods such as Semi-\nGlobal-Matching (SGM) algorithm [1] has been widely used.\nBut deep learning methods have achieved better performance\nrecently and are experiencing a significant development.\nGC-Net [2] first adopted 3D convolutions to construct cost\nvolume.\nPSMNet [3] introduced spatial pyramid pooling\nto enlarge the receptive fields and employed stacked hour-\nglass module. Current state-of-the-art methods mostly use\nmulti-feature 4D cost volume which burdens memory a lot\nsuch as GANet [4] and CSPN [5]. To alleviate this problem,\nCFNet [6] employs uncertainty estimation to adjust disparity\nsearch range which reduces memory consumption.\nBidir-\nEPNet [7] is designed as a specialized pyramid stereo match-\ning network for VHR images. While datasets commonly used\nin supervised learning methods suffer from lack of diversity,\nunsupervised methods are unconstrained by availability of\nground truth.\nUnFlow [8] proposed efficient unsupervised\nloss to train FlowNet without ground truth. PASM [9] ap-\nplied attention mechanism to stereo matching task.\nSome\nunsupervised methods [10] have been applied in automatic\ndriving and have promising future prospects. T. Igeta. [11]\ndesigned an unsupervised stereo matching method for VHR\nimage, validating the feasibility of designing unsupervised\nmethods for VHR images.\nInspired by above previous work, we study unsupervised\nstereo matching method for VHR image and one potential\napproach is to generate metric that quantifies disparity er-\nror in the absence of disparity ground truth. Multiple meth-\nods [6] [12] utilized confidence estimation module to help\nassess reliability of disparity or generating search range, but\nthose estimations are not correlated to true error. L.Chen [13]\nsuccessfully combined disparity and uncertainty estimation,\nachieving improvements in both tasks. H.Shi [12] employed\na simple module to generate a binary confidence score for a\nsemi-supervised task. Designing a suitable intermediary met-\nric is essential for refining disparity without ground truth.\nIn this work, we propose a novel unsupervised stereo\nmatching network based on error prediction for VHR remote\nsensing images. To overcome the limitation of ground truth,\nwe fit the confidence-aware cascade network (CACN) [14] to\nunsupervised learning manner. To bridge network confidence\nwith disparity errors, we design a light-weight confidence\nbased error prediction module (CBEM) and demonstrate its\ncross-domain ability.\nLeveraging the strong coarse-to-fine\nability of multi-scale cascaded core model and error predic-\ntion ability of CBEM, our method outperforms the compar-\nison methods in terms of average endpoint error (EPE) and\nthe fraction of erroneous pixels (D1).\narXiv:2408.07419v1  [cs.CV]  14 Aug 2024\nFig. 1: The framework of proposed method.\n2. METHODOLOGY\n2.1. The architecture of the proposed network\nThe complete framework of the proposed network is shown\nin Fig. 1. The model proposed consists of the core model and\nconfidence based error prediction module(CBEM), while core\nmodel is a relatively independent module that predicts dispar-\nity following unsupervised learning routine, CBEM predicts\nuncertainty scores closely associated with disparity errors us-\ning confidence outputs of core model. The training process\nfollows a three-step routine: initial training of the core model,\nsubsequent training of the CBEM with the core model fixed,\nand finally fine-tuning of the core model with CBEM fixed.\nThe last two steps require only a few epochs.\nThe core model is primarily constructed based on CACN\n[14]. Below is a brief overview of the structure.\nFor the feature extraction module, a multi-scale UNet-like\nnetwork is employed which shares the same weights for both\nleft and right images. Then a combination of concatenated\nand group-wise correlation modules is utilized to construct\ncost volume. The initial disparity is generated by softmax\noperation on cost volume with a range of [−di\nmax, di\nmax].\nFollowing the coarse-to-fine framework, the search range\nis estimated using confidence output from the confidence es-\ntimation module, calculated as:\nσi =\nsX\ndi\n(d −ˆdi)2 × softmax(−ci\nd),\nˆdi =\nX\ndi\nd × softmax(−ci\nd),\n(1)\nwhere c represents the predicted cost tensor. Then, the\ndisparity range of the next stage can be computed based on\nthe standard deviation σi as:\ndi−1\nmax = δ( ˆdi + (si + 1)σi + εi),\ndi−1\nmin = δ( ˆdi −(si + 1)σi −εi),\n(2)\nwhere δ denotes the bilinear upsampling operation. si, εi\nare two learnable normalization factors with initial value 0.\nThe dynamic estimation of the disparity search range\nbased on confidence allows network to reduce computational\ncosts while ensuring accuracy in challenging initial regions.\nBesides, the confidence output of the core model will be\nutilized in CBEM module. The multi-scale disparities are\noptimized by unsupervised loss marked with orange box in\nFig.1 and will be discussed in detail in 2.3.\n2.2. CBEM: Bridging confidence with disparity error\nMotivated by the need to create a module that serves as an\nintermediary between confidence and true disparity error in\norder to reduce errors in disparity estimation, we developed\nthe Confidence-Based Error Prediction Module(CBEM). It’s\nnot feasible to predict accurate disparity error, otherwise it\nwill be possible to reduce error thoroughly. Thus, we train\na simple yet efficient hourglass module to output uncertainty\nscore that aligns with the disparity error distribution. Unlike\nprevious approaches that utilized the disparity to predict un-\ncertainty scores, we used the confidence output from the core\nmodel as the input for our module. Although the core model is\ntrained following unsupervised manner, we train CBEM with\na few labelled data from WHU-Stereo for only one epoch to\ngain error prediction ability. Fig. 2 demonstrates the CBEM’s\nstable error prediction capability when directly evaluated on\nthe target US3D dataset. Notably, the CBEM exhibits strong\ncross-domain generalization ability, as it is less affected by\nthe specific characteristics of original domain, owing to its\nreliance on confidence rather than disparity as input.\nHere, we obtain the final predicted error by jointly encod-\ning LR error and initial uncertainty. LR error is the difference\nbetween left image and reconstructed left image using right\nimage and left disparity, as Eq. 3, where W represents pro-\njection operation.The initial uncertainty is generated by core\nmodel, thus restricted by its performance, while LR error is\nnot affected by the model. LR error depends on the projection\nerror of left and right images, influenced by local noise and in-\n(a) left image\n(b) uncertainty score\n(c) disparity error\nFig. 2: CBEM performance on: Top: WHU-Stereo; Bottom:\nUS3D. Warmer colors indicate higher levels of uncertainty\nor larger disparity errors. The uncertainty score is strongly\ncorrelated to disparity error for both WHU-Stereo and US3D.\ntensity inconsistency of images, while initial uncertainty does\nnot have this shortcoming. Therefore, these two types of in-\nformation complement each other well. We firstly encode LR\nerror and initial uncertainty separately by convolution opera-\ntion. After concatenating them in channel dimension, use two\ncascaded hourglass modules to learn the residuals of predicted\ndisparity error.\nELR = W(Ir, dl) −Il\n(3)\nBinary Cross-Entropy loss is adopted to train the CBEM\nmodule. Ground truth disparity error above threshold(1.0) is\nnoted as 1, otherwise 0. Data used to train CBEM comes\nfrom other datasets with ground truth. Due to its strong cross-\ndomain generalization capability, CBEM still functions well\nin unsupervised dataset. The formula is as follows.\nGTerror =\n\u001a\n1.0\n|GTdisparity −ˆd| > 1.0\n0.0\notherwise\nLBCE = BCE(uscore, GTerror)\n(4)\n2.3. Unsupervised Loss\nFor the first stage of training, we optimize the core model\nusing unsupervised loss of three scales. The loss function is\nthe same for each scale, denoted LS. The full loss is the sum\nof individual scale losses, i.e., L = P\ns Ls. The LS combines\nthree terms, given as:\nLS = λapLap + λcensusLcensus + λsmLsm\n(5)\nLap = 1\nN\nX\ni,j\nα1 −SSIM(Il\ni,j ⊙(1 −Ol\ni,j), ˜Il\ni,j ⊙(1 −Ol\ni,j))\n2\n+ (1 −α)\n\r\r\rIl\ni,j ⊙(1 −Ol\ni,j), ˜Il\ni,j ⊙(1 −Ol\ni,j)\n\r\r\r\n(6)\nLap promotes consistency between left/right image and its\nreconstructed image in non-occluded areas as Eq. 6, while we\nuse a simple formulation to detect occluded areas by distance\nof forward-backward disparities, defined as Eq. 7:\n|df\ni (j) + db\ni(df\ni (j))|2 < τi\n(7)\nwhere df(j) represents the forward disparity based on left\nimage, db(j) represents backward disparity based on right im-\nage, Ol\ni,j is the predicted occlusion map. ⊙denotes element-\nwise multiplication, α is set to 0.85. τi = [5, 2, 1] according\nto the general pattern of VHR images.\nLcensus is also adopted to reduce reconstruction errors\nwhile it has better robustness to various brightness and\nshadow disturbances in VHR remote sensing images.\nWe\nutilize Census transform(τ) with a patch size of 7 pixels and\nCharbonnier penalty function(ρ) to implement Lcensus, Ham-\nming Distance denoted as Γ. The formulation is as follow:\nLcensus = 1\nN\nX\ni,j\nρ[Γ(τ(Il\ni,j), τ(˜Il\ni,j))] ⊙(1 −Ol\ni,j))\n(8)\nLsm is utilized to smooth disparity with L2 penalty on\ndisparity gradients ∂d and image gradients ∂I, denoted as:\nLsm = |∂xdL|e−|∂xIL| + |∂ydL|e−|∂yIL|\n(9)\nFor the second stage of training, CBEM is fixed to gen-\nerate a reliable uncertainty score map. As CBEM has gained\nability to generate uncertainty score correlated with disparity\nerror, by using fine-scale disparity to supervise coarse-scale\ndisparity in reliable regions, we aim to further refine dispar-\nity. Here, we use a mask M = [uscore < t] to exclude\npoor-performing regions, only focusing on those areas that\nare good enough. Lself−sup is calculated as follows:\nLself−sup = 1\nN\nX\ni,j\n[smoothL1(di,L −di,j) ⊙Mi]\n(10)\n3. EXPERIMENTAL RESULTS\n3.1. Dataset\nThe SceneFlow dataset [15] in the finalpass version is used\nto pretrain our core network, which includes 35,454 positive\ntraining and 4370 test images with a resolution of 960 × 540,\nso as the negative ones. US3D track-2 dataset [16] of the\n2019 Data Fusion Contest is used to evaluate our network,\nwhich contains VHR images collected by WorldView-3 be-\ntween 2014 and 2016 over Jacksonville (JAX) and Omaha\n(OMA) in the United States. 1712 RGB stereo pairs of JAX\nare used for training while the remaining 427 pairs are for\nvalidation.\nAll 2153 pairs of OMA are used for testing.\nWe also use the with-ground-truth subset of WHU-Stereo\ndataset [17] to validate the cross-domain generalization\nability, which contains 1757 GaoFen-7 panchromatic im-\nage pairs over Kunming, Qichun, Yingde, Shaoguan, Wuhan\nand Hengyang in 2020. For training CBEM, we also borrow\nthis dataset to establish relationship between confidence and\ndisparity errors.\nFig. 3: The validation loss curve.\n3.2. Implementation Details\nOur network is implemented with PyTorch 1.9.0 and is trained\nwith Adam(β1 = 0.9, β2 = 0.999 ) . All input stereo im-\nages are randomly cropped to 512 × 512 for training while\n1024 × 1024 original size is used for testing. We first pre-\ntrain our core model on SceneFlow dataset from scratch for 20\nepochs with learning rate 0.001 and batch size 8. Then core\nmodel is finetuned on US3D dataset in unsupervised manner\nfor 60 epochs with learning rate set to 0.0001. The dispar-\nity range is set to [−128, 128]. After that, we fix the core\nmodel and train CBEM using WHU-Stereo , which only takes\n1 epoch with learning rate 0.001 to achieve good error predic-\ntion ability. Last step is to fix CBEM and finetune core model\nwith learning rate 1e-5 for 6 epochs. All experiments are con-\nducted on two NVIDIA Titan-RTX GPUs. To avoid overfit-\nting, we stop training as soon as Lself−sup increases, even\nthough it may drop later. As shown in Fig. 3 on validation\nset, we plot EPE and D1 curves to demonstrate the need for\nan immediate stop, Epoch 5 is the best epoch to be adopted.\n3.3. Performance\nWe conducted a comparative analysis between supervised\nmodels, namely PSMNet [3] and CACN [14], as well as un-\nsupervised methods including SGBM+WLS [1] and T.Igeta’s\nmodel [11], against the proposed method. The evaluation re-\nsults on US3D dataset are presented in Table. 1 using EPE and\nD1 as metrics. It is evident that our model outperforms other\nunsupervised models and gains competitive results compared\nto supervised models. When comparing with core model, our\nmethod improves 11.7 % for EPE and 16.6 % for D1, which\nproves validity of CBEM.\nThe core model trained on US3D dataset is directly used\nto test the WHU-Stereo dataset, revealing the superior cross-\ndomain generalization ability of our proposed unsupervised\nmodel, according to Table. 2. As shown in Fig. 4, the pro-\nposed method generates disparity with clearer edges of build-\nings due to smoothing function of Lsm. Furthermore, the pro-\nposed method is less prone to missing fuzzy buildings with\nshadows, owing to the robustness to photometric inconsis-\ntency provided by Lcensus. In the disparities generated by\nCACN model, building edges usually appear dilated, result-\nFig. 4: Disparity estimation on WHU-Stereo. From left to\nright: left image, GT, proposed, CACN [14].\ning in an overestimation of the building’s size. We have mit-\nigated this issue by incorporating the CBEM module, which\nreduces predicted disparity error and effectively preserves the\nprecise delineation of building edges.\nTable 1: Performance of comparison methods on US3D.\nMethod\nwith GT\nDeep Learning\nEPE\nD1(%)\nPSMNet [3]\nyes\nyes\n1.48\n8.53\nCACN [14]\nyes\nyes\n1.47\n8.23\nSGBM+WLS [18]\nno\nno\n2.56\n15.9\nT.Igeta et al. [11]\nno\nyes\n2.12\n15.3\ncore model\nno\nyes\n1.96\n14.5\nProposed\nno\nyes\n1.73\n12.1\nTable 2: Performance on WHU-Stereo\nMethod\nwith GT\nEPE\nD1(%)\nCACN\nyes\n3.47\n33.5\ncore model\nno\n3.11\n31.5\n4. CONCLUSIONS\nIn this study, we propose a novel unsupervised stereo match-\ning network based on error prediction for VHR images, which\nbridges confidence with disparity error, demonstrating robust\ncross-domain generalizability. For future work, we aim to\nbroaden the scope of the model’s applicability.\n5. ACKNOWLEDGEMENT\nThe authors would like to thank the Johns Hopkins University\nApplied Physics Laboratory and the IARPA for providing the\ndata used in this study, and the IEEE GRSS Image Analy-\nsis and Data Fusion Technical Committee for organizing the\nData Fusion Contest. The authors extend their gratitude to Li\net al. for supplying the WHU-Stereo dataset utilized in this\nstudy. This work was supported by Key Research Program\nof Frontier Sciences, Chinese Academy of Sciences, under\nGrant ZDBS-LY-JSC036.\n6. REFERENCES\n[1] Heiko Hirschmuller, “Stereo processing by semiglobal\nmatching and mutual information,” IEEE Transactions\non Pattern Analysis and Machine Intelligence, vol. 30,\nno. 2, pp. 328–341, 2008.\n[2] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta,\nPeter Henry, Ryan Kennedy, Abraham Bachrach, and\nAdam Bry, “End-to-end learning of geometry and con-\ntext for deep stereo regression,”\nin 2017 IEEE Inter-\nnational Conference on Computer Vision (ICCV), Oct\n2017.\n[3] Jia-Ren Chang and Yong-Sheng Chen, “Pyramid stereo\nmatching network,” in 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, Jun 2018.\n[4] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and\nPhilip H.S. Torr, “Ga-net: Guided aggregation net for\nend-to-end stereo matching,” in 2019 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), Jun 2019.\n[5] Xinjing Cheng, Peng Wang, and Ruigang Yang, “Learn-\ning depth with convolutional spatial propagation net-\nwork,” IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, vol. 42, no. 10, pp. 2361–2379, 2020.\n[6] Zhelun Shen, Yuchao Dai, and Zhibo Rao, “Cfnet: Cas-\ncade and fused cost volume for robust stereo matching,”\nin 2021 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2021, pp. 13901–13910.\n[7] Rongshu Tao, Yuming Xiang, and Hongjian You, “An\nedge-sense bidirectional pyramid network for stereo\nmatching of vhr remote sensing images,” Remote Sens-\ning, vol. 12, no. 24, 2020.\n[8] Simon Meister, Junhwa Hur, and Stefan Roth, “Unflow:\nUnsupervised learning of optical flow with a bidirec-\ntional census loss,” Proceedings of the AAAI Conference\non Artificial Intelligence, Jun 2022.\n[9] Longguang Wang, Yulan Guo, Yingqian Wang, Zhengfa\nLiang, Zaiping Lin, Jungang Yang, and Wei An, “Par-\nallax attention for unsupervised stereo correspondence\nlearning,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, p. 2108–2125, Apr 2022.\n[10] Sunghun Joung, Seungryong Kim, Kihong Park, and\nKwanghoon Sohn, “Unsupervised stereo matching us-\ning confidential correspondence consistency,”\nIEEE\nTransactions on Intelligent Transportation Systems, vol.\n21, no. 5, pp. 2190–2203, 2020.\n[11] Toshifumi Igeta and Akira Iwasaki, “An unsupervised\nnetwork for stereo matching of very high resolution\nsatellite imagery,” in IGARSS 2022 - 2022 IEEE Inter-\nnational Geoscience and Remote Sensing Symposium,\n2022, pp. 971–974.\n[12] Hongkuan Shi, Zhiwei Wang, Ying Zhou, Dun Li, Xin\nYang, and Qiang Li,\n“Bidirectional semi-supervised\ndual-branch cnn for robust 3d reconstruction of stereo\nendoscopic images via adaptive cross and parallel super-\nvisions,” IEEE Transactions on Medical Imaging, vol.\n42, no. 11, pp. 3269–3282, 2023.\n[13] Liyan Chen, Weihan Wang, and Philippos Mordohai,\n“Learning the distribution of errors in stereo match-\ning for joint disparity and uncertainty estimation,” in\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2023, Vancouver, BC, Canada, June\n17-24, 2023. 2023, pp. 17235–17244, IEEE.\n[14] Rongshu Tao, Yuming Xiang, and Hongjian You,\n“A confidence-aware cascade network for multi-scale\nstereo matching of very-high-resolution remote sensing\nimages,” Remote Sensing, p. 1667, Mar 2022.\n[15] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fis-\ncher, Daniel Cremers, Alexey Dosovitskiy, and Thomas\nBrox, “A large dataset to train convolutional networks\nfor disparity, optical flow, and scene flow estimation,” in\n2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), Jun 2016.\n[16] Bertrand Le Saux, Naoto Yokoya, Ronny Hansch, My-\nron Brown, and Greg Hager, “2019 data fusion contest\n[technical committees],” IEEE Geoscience and Remote\nSensing Magazine, p. 103–105, Mar 2019.\n[17] Shenhong Li, Sheng He, San Jiang, Wanshou Jiang, and\nLin Zhang,\n“Whu-stereo: A challenging benchmark\nfor stereo matching of high-resolution satellite images,”\nIEEE Transactions on Geoscience and Remote Sensing,\nvol. 61, pp. 1–14, 2023.\n[18] H. Hirschmuller,\n“Stereo processing by semiglobal\nmatching and mutual information,”\nIEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, p.\n328–341, Feb 2008.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-08-14",
  "updated": "2024-08-14"
}