{
  "id": "http://arxiv.org/abs/2408.08041v1",
  "title": "The Clever Hans Effect in Unsupervised Learning",
  "authors": [
    "Jacob Kauffmann",
    "Jonas Dippel",
    "Lukas Ruff",
    "Wojciech Samek",
    "Klaus-Robert Müller",
    "Grégoire Montavon"
  ],
  "abstract": "Unsupervised learning has become an essential building block of AI systems.\nThe representations it produces, e.g. in foundation models, are critical to a\nwide variety of downstream applications. It is therefore important to carefully\nexamine unsupervised models to ensure not only that they produce accurate\npredictions, but also that these predictions are not \"right for the wrong\nreasons\", the so-called Clever Hans (CH) effect. Using specially developed\nExplainable AI techniques, we show for the first time that CH effects are\nwidespread in unsupervised learning. Our empirical findings are enriched by\ntheoretical insights, which interestingly point to inductive biases in the\nunsupervised learning machine as a primary source of CH effects. Overall, our\nwork sheds light on unexplored risks associated with practical applications of\nunsupervised learning and suggests ways to make unsupervised learning more\nrobust.",
  "text": "The Clever Hans Effect in Unsupervised Learning\nJacob Kauffmann1,2, Jonas Dippel1,2,3, Lukas Ruff1,3, Wojciech Samek1,2,5,\nKlaus-Robert Müller1,2,6,7,8,⋆and Grégoire Montavon4,2,1,⋆\n1Department of Electrical Engineering and Computer Science, Technische Universität Berlin, Germany\n2BIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin, Germany\n3Aignostics, Berlin, Germany\n4Department of Mathematics and Computer Science, Freie Universität Berlin, Germany\n5Department of Artificial Intelligence, Fraunhofer HHI, Berlin, Germany\n6Department of Artificial Intelligence, Korea University, Seoul, Korea\n7Max-Planck Institute for Informatics, Saarbrücken, Germany\n8Google Deepmind, Berlin, Germany\n⋆Corresponding authors (email: klaus-robert.mueller@tu-berlin.de, gregoire.montavon@fu-berlin.de)\nAbstract\nUnsupervised learning has become an essential building block of AI systems. The representations it produces, e.g. in\nfoundation models, are critical to a wide variety of downstream applications. It is therefore important to carefully examine\nunsupervised models to ensure not only that they produce accurate predictions, but also that these predictions are not “right\nfor the wrong reasons”, the so-called Clever Hans (CH) effect. Using specially developed Explainable AI techniques, we show\nfor the first time that CH effects are widespread in unsupervised learning. Our empirical findings are enriched by theoretical\ninsights, which interestingly point to inductive biases in the unsupervised learning machine as a primary source of CH effects.\nOverall, our work sheds light on unexplored risks associated with practical applications of unsupervised learning and suggests\nways to make unsupervised learning more robust.\nUnsupervised learning is a subfield of machine learning\nthat has gained prominence in recent years [1–3]. It ad-\ndresses fundamental limitations of supervised learning, such\nas the lack of labels in the data or the high cost of acquir-\ning them. Unsupervised learning has achieved successes\nin modeling the unknown, such as uncovering new cancer\nsubtypes [4, 5] or extracting novel insights from large his-\ntorical corpora [6]. Furthermore, the fact that unsupervised\nlearning does not rely on task-specific labels makes it a good\ncandidate for core AI infrastructure: Unsupervised anomaly\ndetection provides the basis for various quality or integrity\nchecks on the input data [7–10]. Unsupervised learning is\nalso a key technology behind ‘foundation models’ [1, 11–\n15] which extract representations upon which various down-\nstream models (e.g. classification, regression, ‘generative AI’,\netc.) can be built. The growing popularity of unsupervised\nlearning models creates an urgent need to carefully examine\nhow they arrive at their predictions. This is essential to\nensure that potential flaws in the way these models process\nand represent the input data are not propagated to the many\ndownstream supervised models that build upon them.\nIn this paper, we show for the first time that unsupervised\nlearning models largely suffer from Clever Hans (CH) effects\n[16], also known as “right for the wrong reasons”. Specifically,\nwe find that unsupervised learning models often produce rep-\nresentations from which instances can be correctly predicted\nto be e.g. similar or anomalous, although largely supported\nby data quality artifacts. The flawed prediction strategy\nis not detectable by common evaluation benchmarks such\nas cross-validation, but may manifest itself much later in\n‘downstream’ applications in the form of unexpected errors,\ne.g. if subtle changes in the input data occur after deploy-\nment (cf. Fig. 1). While CH effects have been studied quite\nextensively for supervised learning [16–20], the lack of similar\nstudies in the context of unsupervised learning, together with\nthe fact that unsupervised models supply many downstream\napplications, is a cause for concern.\nFor example, in industrial inspection, which often relies\non unsupervised anomaly detection [9, 10], we find that a\nCH decision strategy can systematically miss a wide range\nof manufacturing defects, resulting in potentially significant\ncosts. As another example, unsupervised foundation models,\nadvocated in the medical domain to provide robust features\nfor various specialized diagnostic tasks, can potentially intro-\nduce CH effects into many of these tasks, with the prominent\nrisk of large-scale misdiagnosis. These scenarios (illustrated\nin Fig. 1) highlight the practical implications of an unsu-\npervised CH effect, which, unlike its supervised counterpart,\nmay not be limited to malfunctioning in a single specific\ntask, but potentially in all downstream tasks.\nTo uncover and understand unsupervised Clever Hans\neffects, we propose to use Explainable AI [21–25] (here\ntechniques that build on the LRP explanation framework\n[26–28]). Our proposed use of these techniques allows us to\narXiv:2408.08041v1  [cs.LG]  15 Aug 2024\nFigure 1: The CH effect in unsupervised learning. The unsupervised model correctly represents data instances as similar or anomalous,\nbut for the wrong reasons, e.g. based on irrelevant input features such as artifacts. The problem is critical because the flaw can\nbe inherited by potentially many downstream tasks. The CH effect typically goes undetected in a classical validation procedure and\nmanifests itself in the form of prediction errors only after deployment.\nidentify at scale which input features are used (or misused)\nby the unsupervised ML model, without having to formulate\nspecific hypotheses or downstream tasks. Specifically, we\nuse an extension of LRP called BiLRP [6] to reveal input\npatterns that are jointly responsible for similarity in the\nrepresentation space. We also combine LRP with ‘virtual\nlayers’ [29, 30] to reveal pixel and frequency components\nthat are jointly responsible for predicted anomalies.\nFurthermore, our Explainable AI-based analysis allows\nus to pinpoint more formal causes for the emergence of\nunsupervised CH effects. In particular, they are due not so\nmuch to the data, but to the unsupervised learning machine,\nwhich hinders the integration of the true task-supporting\nfeatures into the model, even though vast amounts of data\npoints are available. Our findings provide a novel direction\nfor developing targeted strategies to mitigate CH effects and\nincrease model robustness.\nOverall, our work sheds light on the presence, prominence,\nand distinctiveness of CH effects in unsupervised learning,\ncalling for increased scrutiny of this essential component of\nmodern AI systems.\nResults\nUsing Explainable AI, we investigate the emergence of Clever\nHans effects in a representative set of unsupervised learn-\ning tasks, including representation learning and anomaly\ndetection.\nClever Hans Effects in Representation Learning\nWe first consider an application of representation learning in\nthe context of detecting COVID-19 cases from X-ray scans\n[31, 32]. Simulating an early pandemic phase characterized\nby data scarcity, similar to [32], we use a dataset aggregation\napproach where a large, well-established non-COVID-19\ndataset is merged with a more recent COVID-19 dataset\naggregated from multiple sources. Specifically, we aggregate\n2597 instances of the CXR8 dataset [33] from the National\nInstitute of Health (NIH), collected between 1992 and 2015,\nwith the 535 instances of the GitHub-hosted ‘COVID-19\nimage data collection’ [34]. We refer to these subsets as\n‘NIH’ and ‘GitHub’, respectively.\nFurther motivated by the need to accommodate the criti-\ncally small number of COVID-19 instances in the aggregated\ndataset and to avoid overfitting, we choose to rely on the\nrepresentations provided by unsupervised foundation models\n[15, 35–37]. Specifically, we feed our data into a pre-trained\nPubMedCLIP model [36], which has built its representation\nfrom a very large collection of X-ray scans in an unsuper-\nvised manner. Based on the PubMedCLIP representation,\nwe train a downstream classifier that separates COVID-19\nfrom non-COVID-19 instances with a class-balanced accu-\nracy of 88.6% on the test set (cf. Table 1). However, a closer\nlook at the structure of this performance score reveals a\nstrong disparity between the NIH and GitHub subgroups,\nwith all NIH instances being correctly classified and the\n2\nGitHub instances having a lower class-balanced accuracy\nof 75.2%, and, more strikingly, a false positive rate (FPR)\nof 40%, as shown in Table 1. Considering that the higher\nheterogeneity of instances in the GitHub dataset is more\ncharacteristic of real-world conditions, such a high FPR\nprohibits any practical use of the model as a diagnostic\ntool. We emphasize that this deficiency could have been\neasily overlooked if one did not pay close attention to (or\ndid not know) the data sources, and instead relied only on\nthe overall accuracy score.\nTo proactively detect this heterogeneous, non-robust pre-\ndiction behavior and to test whether it originates from the\nunsupervised PubMedCLIP component, we propose to use\nExplainable AI. Using the BiLRP [6] technique, we inves-\ntigate whether PubMedCLIP represents different instances\nas similar based on a correct strategy or a ‘Clever Hans’\nstrategy. The BiLRP method is illustrated in Fig. 2 and its\nmathematical formulation is given in the Methods section.\nThe output of BiLRP for two exemplary pairs of COVID-\npositive instances is shown in Fig. 3 (left). The BiLRP\nexplanation shows that the modeled similarity between pairs\nof COVID-positive instances comes from text-like annota-\ntions that appear in both images. This is a clear case of a\nCH effect, where the modeled similarity is right (instances\nshare being COVID-positive), but for the wrong reasons\nFigure 2:\nIllustration of the BiLRP method for explaining\nsimilarity predictions of a representation learning model. The\noutput of BiLRP is a decomposition of the predicted similarity\nonto pairs of features from the two input images. It is typically\ndisplayed as a weighted bipartite graph connecting the contributing\nfeature pairs.\n(similarity is based on shared textual annotations). The\nunmasking of this CH effect by BiLRP warns of the risk\nthat downstream models based on these similarities (and\nmore generally on the flawed representation) may inherit\nthe CH effect. This a posteriori explains the excessively\nhigh (40%) FPR of the downstream classifier on the Github\nsubset, as the model’s reliance on text makes it difficult to\nseparate Github’s COVID-positive from COVID-negative\nFigure 3: BiLRP analysis of the PubMedCLIP unsupervised model and the general-purpose CLIP, SimCLR, and BarlowTwins\nunsupervised models. The analysis reveals a variety of CH strategies, specifically instances that are correctly represented as similar by\nthe unsupervised model but for the wrong reasons. Similarity between COVID-positive instances arises from shared spurious textual\nannotations. Similarly, on ImageNet data, unsupervised models incorrectly rely on logo artifacts or the presence of humans in the\nbackground for their similarity predictions.\n3\ninstances (cf. Supplementary Note A for further analysis).\nWe note that, unlike the per-group accuracy analysis above,\nour BiLRP analysis did not require provenance metadata\n(GitHub or NIH), nor did it focus on a specific downstream\ntask with its specific labels.\nTo test whether representation learning has a general ten-\ndency to evolve CH strategies beyond the above use case,\nwe downloaded three generic foundation models, namely the\noriginal CLIP model [13], SimCLR [12, 38] and BarlowTwins\n[39]. As a downstream task, we consider the classification,\nusing linear-softmax classifiers, of the 8 classes from Ima-\ngeNet [40] that share the WordNet ID ‘truck’ and of the\n16 ImageNet classes that share the WordNet ID ‘fish’ (see\nthe Methods section for details). The test accuracy of each\nmodel on these two tasks is given in Table 1 (columns ‘orig-\ninal’). On the truck classification task, the CLIP model\nperforms best, with an accuracy of 85.0%. On the fish classi-\nfication task, the CLIP and supervised models perform best,\nwith accuracies of 86.5% and 86.2%, respectively.\nWe use BiLRP to examine the representations of these\nunsupervised models. In Fig. 3 (center), we observe that\nCLIP-based similarities, as in PubMedCLIP, also rely on\ntext. Here, a textual logo in the lower left corner of two\ngarbage truck images is used to support similarity1.\nIn\ncontrast, SimCLR and BarlowTwins instead rely on the\nactual garbage truck, demonstrating that CH effects are\nspecific to the unsupervised learning technique.\nIn Fig.\n3 (right), we observe that all unsupervised models focus\non humans and are unresponsive to fish features. CLIP\nfocuses specifically on facial features. While the detection\nof human features may be useful for many downstream\ntasks, the suppression of fish features strongly exposes other\ndownstream fish classification tasks to even mild spurious\ncorrelations, systematically leading to CH behavior.\nThe heterogeneity of strategies across models revealed by\nBiLRP suggests that, in addition to inhomogeneities in the\nunsupervised data, biases induced by the unsupervised pre-\ntraining algorithm2 play an important role in the formation\nof CH effects. The spurious amplification of humans in the\ncenter of the image observed for SimCLR and BarlowTwins\ncan be attributed to the cropping mechanism these methods\nimplement, where human features in the center are sufficient\nand robust to solve the SimCLR and BarlowTwins similarity\ntasks. In the case of CLIP (and PubMedCLIP), the sys-\ntematic amplification of textual, facial, or other identifying\nfeatures can be explained by the fact that these features\noften provide useful information for CLIP’s unsupervised\nimage-text matching task.\nThe consequences of these Clever Hans effects are shown\n1 The reliance on logos for the garbage truck class has been highlighted\nin previous work in the context of supervised learning [18].\n2 See also [41–44] for further analyses of these biases.\nquantitatively in Table 1. We observe a systematic degra-\ndation of performance when moving from the original data\nto data subsets where the presence/absence of an artifact\nis no longer predictive of the class. In particular, for the\ntruck data, we observe a sharp drop in the accuracy of the\nCLIP model from 85.0% on the original data to 80.5% on\nthe same data with a logo inserted on each image (column\n‘logo’ in Table 1). For the fish data, a similar drop in accu-\nracy is observed for SimCLR and BarlowTwins from 82.2%\nand 83.1% respectively on the original data to 78.6% and\n75.6% respectively when only images containing humans are\nretained and class rebalancing is performed. This shows that\nthe CH effects detected by our BiLRP analysis have concrete\nnegative practical consequences in terms of the ability of\nunsupervised models and downstream classifiers to predict\nuniformly well across subgroups and to generalize. In com-\nparison, the baseline supervised model generally shows more\nstable performance between the original data and the newly\ndefined subsets. A detailed analysis of the structure of the\nprediction errors for each classification task, supported by\nconfusion matrices, is given in Supplementary Note B.\nClever Hans Effects in Anomaly Detection\nExtending our investigation of the CH effect to another in-\nstance of unsupervised learning, namely anomaly detection,\nwe consider an industrial inspection use case based on the\npopular MVTec-AD dataset [9]. The dataset consists of\n15 product categories, each consisting of a training set of\nimages without manufacturing defects and a test set of im-\nages with and without defects. Since manufacturing defects\nare infrequent and heterogeneous in nature, the problem is\ntypically approached using unsupervised anomaly detection\n[2, 9]. These models map each instance to an anomaly score,\nfrom which threshold-based downstream models can be built\nto classify between instances with and without manufactur-\ning defects. Unsupervised anomaly detection has received\nconsiderable attention, with sophisticated approaches based\non deep neural networks such as PatchCore [45] or Efficien-\ntAD [46], showing excellent performance in detecting a wide\nrange of industrial defects.\nSomewhat surprisingly, simpler approaches based on dis-\ntances in pixel space show comparatively high performance\nfor selected tasks [2]. We consider one such approach, which\nwe call ‘D2Neighbors’, where anomalies are predicted ac-\ncording to the distance to neighbors in the training data.\nSpecifically, the anomaly score of an instance x is computed\nas f(x) = softminj{∥x −uj∥2} where (uj)N\ni=1 is the set of\navailable inlier instances (see the Methods section for details\non the model and data preprocessing). This anomaly model\nbelongs to the broader class of distance-based models [47–49],\nand connections can be made to kernel density estimation\n4\n𝑉⊤\n𝑉\n𝑓(𝒙)\nidentity\n⎴\n𝒙\n𝑉\n𝑓(𝒙)\njoint\npixel/frequency\ncontributions\n𝑉=\n{\n}\nFigure 4:\nSchematic of the virtual inspection layer used to\nexplain anomalies in the joint pixel-frequency domain. To map\npixels to frequencies and back, we use the discrete cosine transform\n(DCT), whose basis elements are shown on the right.\n[50, 51] and one-class SVMs [52]. Using D2Neighbors, we\nare able to build downstream models that classify industrial\ndefects of the MVTec data with F1-scores above 0.9 for five\ncategories (bottle, capsule, pill, toothbrush, and wood).\nTo shed light on the prediction strategy that leads to these\nunexpectedly high F1-scores, we investigate the underlying\nD2Neighbors anomaly model using Explainable AI. Specifi-\ncally, we consider an extension of LRP for anomaly detection\n[28, 53] and further equip the explanation technique with\n‘virtual layers’ [29, 30]. The technique of ‘virtual layers’\n(cf. Fig. 4) is to map the input to an abstract domain and\nback, leaving the prediction function unchanged, but provid-\ning a new representation in terms of which the prediction\ncan be explained. We construct such a layer by applying\nthe discrete cosine transform (DCT) [54], shown in Fig. 4\n(right), followed by its inverse. This allows us to explain the\npredictions jointly in terms of pixels and frequencies.\nThe result of our proposed analysis is shown in Fig. 5 for\ninstances from two of the five retained MVTec categories\n(see Supplementary Note C for more instances). Explana-\ntions at the pixel level show that D2Neighbors supports\nits anomaly predictions largely based on pixels containing\nthe actual industrial defect. The squared difference in its\ndistance function (∥∆∥2 = P\ni ∆2\ni ) encourages a sparse pixel-\nwise response of the model, efficiently discarding regions of\nthe image where the new instance shows no difference from\ninstances in the training data.\nHowever, we also see in\nthe pixel-wise explanation that a non-negligible part of the\nanomaly prediction comes from irrelevant background pixels.\nJoint pixel-frequency explanations shed light on these unre-\nsolved contributions, showing that they arise mostly from\nthe high-frequency part of the model’s decision strategy (Fig.\n5 B and C).\nThe exposure of the model to these irrelevant high-\nfrequency features, as detected by our LRP analysis, raises\nconcerns about the robustness of the model under changing\ndata conditions (e.g. after the model is deployed). We exper-\niment with an innocuous perturbation of the data conditions,\nwhich consists of changing the image resizing algorithm from\nOpenCV’s nearest neighbor resizing to a more sophisticated\nresizing method that includes antialiasing. Resizing tech-\nniques have been shown in some cases to significantly affect\nimage quality and ML model prediction [55], but their effect\non ML models, especially unsupervised ones, has been little\nstudied. The performance of the D2Neighbors model and\nother models before and after changing the resizing algo-\nrithm is shown in Table 1 (columns ‘original’ and ‘deployed’\nrespectively). The F1 score performance of D2Neighbors de-\ngrades significantly from 0.92 to 0.80. In particular, there is\nA\nB\nC\ninput\npixel-wise\ncontributions\nlow mid\nhigh\nfrequency contributions\nlow\nmid\nhigh\njoint pixel/frequency contributions\nFigure 5: Explainable AI analysis of the D2Neighbors anomaly model. A: Selected images from MVTec-AD (ground truth anomaly\nshown as red contour) and a pixel-wise LRP explanation of the anomaly prediction. Explanations for other categories are given in\nSupplementary Note C. B: Frequency domain explanations (averaged over all anomalous test instances). The x-axis represents the\nfrequencies (on a power scale), and the y-axis is the contribution of the corresponding frequencies to the anomaly prediction. C:\nPixel-wise contributions filtered by frequency band.\n5\na large increase in the false negative rate (FNR) from 4% to\n26% (cf. Table 1). In an industrial inspection setting, an in-\ncrease in FNR has serious consequences, in particular, many\ndefective instances are missed and propagated through the\nproduction chain, resulting in wasted resources in subsequent\nproduction stages and high recall costs. This performance\ndegradation of D2Neighbors in post-deployment conditions\nis particularly surprising given that data quality has actually\nimproved. This is a direct consequence of the CH strategy we\nidentified, namely D2Neighbors’ reliance on high frequencies.\nWhen antialiasing is introduced into the resizing procedure,\nthe high frequencies that the D2Neighbors model uses to\nsupport its prediction disappear from the data, and this\nsignificantly reduces the anomaly scores of each instance,\nthereby increasing the FNR.\nInterestingly, the performance degradation of D2Neighbors\nis even more severe when the ℓ2-norm in the distance func-\ntion is replaced by the ℓ1-norm (cf. Table 1). This can be\nexplained by the fact that the ℓ1-norm, unlike the ℓ2-norm,\ndoes not implement pixel-wise sparsity and is therefore more\nexposed to irrelevant input regions. Conversely, the applica-\ntion of an ℓ4-norm results in sensibly higher robustness than\nthe original ℓ2-norm. We note that even the more sophis-\nticated PatchCore model [45] (cf. Methods section), which\nintegrates several robustness mechanisms including prepro-\ncessing layers and spatial max-pooling, is not fully immune\nto irrelevant high-frequency components and also shows\na noticeable performance degradation in post-deployment\nconditions (cf. Table 1).\nOverall, our experiments show that performance on the\navailable test data is an insufficient indicator of the quality\nof an anomaly detector. Its robustness to changes in the\nquality of the input data cannot be guaranteed. This can\nbe diagnosed by Explainable AI and a careful subsequent\ninspection of the structure of the anomaly model.\nAlleviating CH in Unsupervised Learning\nLeveraging the Explainable AI analysis above, we aim to\nbuild models that are more robust across different data sub-\ngroups and in post-deployment conditions. Unlike previously\nproposed CH removal techniques [18, 19], we aim to oper-\nate on the unsupervised model rather than the downstream\ntasks. This potentially allows us to achieve broad robustness\nimprovements while leaving the downstream learning ma-\nchines (training supervised classifiers or adjusting detection\nthresholds) untouched.\nWe first test our CH mitigation approach on the CLIP\nmodel, which our Explainable AI analysis has shown to\nincorrectly rely on text. Specifically, we focus on an early\nlayer of the CLIP model (encoder.relu3) and remove the\nfeature maps that are most responsive to text. We then\napply a similar CH mitigation approach to anomaly detec-\ntion, which our Explainable AI analysis has shown to be\noverexposed to high frequencies. Here, we propose to prune\nthe high frequencies by inserting a blur layer at the input of\nthe model.\nIn both cases, the proposed CH mitigation technique pro-\nvides strong benefits in terms of model robustness. As shown\nin Table 1, rows ‘CH mitigation’, our robustified models sub-\nstantially reverse the performance degradation observed in\npost-deployment conditions, reaching performance levels\nclose to, and in some cases superior to, those measured on\nthe original data.\nCOVID-19\nImageNet:truck\nImageNet:fish\noriginal\ngithub\noriginal\nlogo\noriginal\nhuman\nPubMedCLIP\n88.6\n75.2 ↓\n—\n—\n—\n—\nFPR: 14%\nFPR: 40%\nCLIP\n—\n—\n85.0\n80.5 ↓\n86.5\n83.8\n+ CH mitigation\n—\n—\n85.2\n85.0 ↑\n—\n—\nSimCLR\n—\n—\n74.8\n74.5\n82.2\n78.6 ↓\nBarlowTwins\n—\n—\n80.2\n80.2\n83.1\n75.6 ↓\nSupervised\n—\n—\n83.8\n83.2\n86.2\n84.2\nMVTec-AD⋆\noriginal\ndeployed\nD2Neighbors (ℓ2)\n0.92\n0.80 ↓\nFNR: 4%\nFNR: 26%\n+ CH mitigation\n0.94\n0.93 ↑\nPatchCore\n0.92\n0.85 ↓\n+ CH mitigation\n0.97\n0.96 ↑\nD2Neighbors (ℓ1)\n0.93\n0.74 ↓\nD2Neighbors (ℓ4)\n0.91\n0.83 ↓\nTable 1: Performance of unsupervised models on different data subsets or conditions. Left: Class-balanced classification accuracy of\ndownstream models (COVID-19, truck and fish classification) built on different unsupervised representation learning models. Accuracy\nscores are reported for the original test set and for a subset containing specific instances. Right: F1 scores of different anomaly\ndetection models. Performance is reported on the simulated original and post-deployment data conditions (standard and antialiased\nresizing, respectively). (⋆) Performance on MVTec-AD is computed for each of the 5 classes retained for the analysis and averaged.\n(↓) Severe performance degradation (of 3 percentage points or more) after deployment. (↑) Major performance improvement (of 3\npercentage points or more) after CH mitigation.\n6\nDiscussion\nUnsupervised learning is an essential category of ML that\nis increasingly used in core AI infrastructure to power a\nvariety of downstream tasks, including generative AI. Much\nresearch to date has focused on improving the performance of\nunsupervised learning algorithms, for example, to maximize\naccuracy scores in downstream classification tasks. These\nevaluations often pay little attention to the exact strategy\nused by the unsupervised model to achieve the reported\nhigh performance, in particular whether these models rely\non Clever Hans strategies.\nBuilding on recent techniques from Explainable AI, we\nhave shown for the first time that CH strategies are prevalent\nin several unsupervised learning paradigms. These strategies\ncan take various forms, such as correctly predicting the\nsimilarity of two X-ray scans based on irrelevant shared\nannotations, or predicting that an image is anomalous based\non small but widespread pixel-level artifacts. These flawed\nprediction strategies result in models that do not transfer\nwell to changing conditions at test time. Most importantly,\ntheir lack of robustness infects many downstream models\nthat rely on them. As we have shown in two use cases, this\ncan lead to widespread misdiagnosis of patients or failure to\ndetect manufacturing defects.\nTherefore, addressing CH effects is a critical step towards\nreliable use of unsupervised learning methods. However,\ncompared to a purely task-specific supervised approach, the\naddition of an unsupervised component, potentially serving\nmultiple downstream tasks, adds another dimension of com-\nplexity to the modeling problem. In particular, one must\ndecide whether to handle CH effects in the downstream clas-\nsifier or directly in the unsupervised model part. Approaches\nconsisting of dynamically updating downstream models in\nresponse to changing conditions [56–59], or revising their\ndecision strategies with human feedback [18, 19, 60] are\npossible solutions to maintain high accuracy. However, these\napproaches may not be sustainable because CH mitigation\nmust be performed repeatedly for each new downstream\ntask. The problem may persist even after a flaw in the\nfoundation model becomes known (e.g. [61, 62]), due to\nrelease intervals and the high cost of retraining these models.\nInstead, our results argue for addressing CH effects directly\nwhen building the unsupervised model, with the goal of\nachieving persistent robustness that benefits existing and\nfuture downstream applications.\nUsing advanced Explainable AI techniques targeted at\nunsupervised learning, we have uncovered multiple and di-\nverse CH effects, such as the reliance of CLIP (and its\nderivative PubMedCLIP) on textual annotations, or a sys-\ntemic over-exposure of unsupervised anomaly models to\nnoise. Interestingly, our analysis has revealed that these\nunsupervised CH effects differ from supervised ones in that\nthey arise less from the data and more from inductive bi-\nases in the model and learning algorithm. These include\nspurious suppression/amplification effects caused by the\nrepresentation learning’s training objective, or a failure of\nunsupervised anomaly detection architectures to replicate\nfrequency filtering mechanisms found in supervised learning\n[63–65], leaving the learned models highly exposed to noise.\nFurthermore, our Explainable AI analysis not only provided\nnew insights into the formal causes of unstable behavior in\nunsupervised learning. Our experiments also showed how\npruning the high-frequency or spuriously amplified features\nrevealed by our Explainable AI analysis leads to system-\natic performance improvements on difficult data subgroups\nor in post-deployment conditions.\nIn doing so, we have\ndemonstrated the actionability of our Explainable AI ap-\nproach, showing that it can guide the process of identifying\nand subsequently correcting the faulty components of an\nunsupervised learning model.\nWhile our investigation of unsupervised CH effects and\ntheir consequences has focused on image data, extension to\nother data modalities seems straightforward. Explainable\nAI techniques such as LRP, which are capable of accurate\ndataset-wide explanations, operate independently of the\ntype of input data.\nThey have recently been extended\nto recurrent neural networks [66], graph neural networks\n[67], transformers [68], and state space models [69], which\nrepresent the state of the art for large language models\nand other models of structured data. Thus, our analysis\ncould be extended to analyze other instances of unsupervised\nlearning, such as anomaly detection in time series or the\nrepresentations learned by large language models (e.g. [70,\n71]).\nOverall, we believe that the CH effect in unsupervised\nlearning, and the uncontrolled risks associated with it, is a\nquestion of general importance, and that Explainable AI and\nits recent developments provide an effective way to tackle it.\nMethods\nThis section first introduces the unsupervised ML models\nstudied in this work and the datasets on which they are ap-\nplied. It then presents the layer-wise relevance propagation\n(LRP) method for explaining predictions, its BiLRP exten-\nsion for explaining similarity, and the technique of ‘virtual\nlayers’ for generating joint pixel-frequency explanations.\nML Models and Data for Representation Learning\nRepresentation learning experiments were performed on the\nSimCLR [12, 38], CLIP[13], BarlowTwins [39] and Pub-\nMedCLIP [36] models.\nAll include a version based on\n7\nthe ResNet50 architecture [72] and come with pre-trained\nweights. SimCLR augments the input images with resized\ncrops, color jitter, and Gaussian blur to create two different\nviews of the same image. These views are then used to create\npositive and negative pairs, where the positive pairs repre-\nsent the same image from two different perspectives, and the\nnegative pairs are created by pairing different images. The\ncontrastive loss objective maximizes the similarity between\nthe representations of the positive pairs while minimizing the\nsimilarity between the representations of the negative pairs.\nBarlow Twins is similar to SimCLR in that it also generates\naugmented views of the input image through random resized\ncrops and color augmentation, and maximizes their cosine\nsimilarity in representation space. However, it differs from\nSimCLR in the exact mechanisms used to prevent represen-\ntation collapse. In our experiments, we use the weights from\nthe vissl3 library. CLIP (and its derivative PubMedCLIP)\nlearns representations by using a large collection of image-\ntext pairs from the Internet. Images are given to an image\nencoder, and the corresponding texts are given to a text\nencoder. The similarity of the two resulting embeddings is\nthen maximized with a contrastive loss. In our experiments,\nwe use the ResNet-50 weights from OpenAI4 for CLIP. Pub-\nMedCLIP5 is based on a pre-trained CLIP model fine-tuned\non the ROCO dataset [73], a collection of radiology and\nimage caption pairs. For all representation learning experi-\nments, the supervised baselines share the same architecture\nas their unsupervised counterparts, but are trained in a\npurely supervised fashion using backpropagation.\nThe analysis and training of these models were performed\non different datasets. The ImageNet experiments were per-\nformed on two ImageNet subsets. First, the ‘truck’ subset\nconsisting of the 8 classes sharing the WordNetID ‘truck’\n(minivan, moving van, police van, fire engine, garbage truck,\npickup, tow truck and trailer truck). Then the ‘fish’ subset\nconsisting of the 16 classes sharing the WordNetID ‘fish’\n(tench, barracouta, coho, sturgeon, gar, stingray, great white\nshark, hammerhead, tiger shark, puffer, electric ray, goldfish,\neel, anemone fish, rock beauty and lionfish). For the X-ray\nexperiments, we combined the NIH ChestX-ray8 (CXR8)\ndataset6 and the GitHub-hosted ‘COVID-19 image data\ncollection’ 7. The NIH dataset contributed 2597 negative\nimages, while the GitHub dataset contained 342 COVID-\npositive and 193 negative images. All images were resized\nto 224x224 pixels and center-cropped. We split the patients\n80:20 into training and test sets based on unique patient IDs.\nThis resulted in 272 positive and 168 negative images in\n3 https://vissl.ai/\n4 https://github.com/openai/CLIP\n5 https://github.com/sarahESL/PubMedCLIP\n6 https://academictorrents.com/details/\ne615d3aebce373f1dc8bd9d11064da55bdadede0\n7 https://github.com/ieee8023/covid-chestxray-dataset\nthe training set from the GitHub dataset. To approximate\nan i.i.d. distribution in the training set, we added 2552\nnegative images from the NIH dataset, resulting in a total of\n2992 training images (272 positive, 2720 negative). The test\nset consisted of 70 positive and 70 negative images, with\n45 negative images from the NIH dataset and 25 from the\nGitHub dataset. This resulted in 2552 NIH images and 440\nGitHub images in the training set, and 45 NIH images and\n95 GitHub images in the test set.\nML Models and Data for Anomaly Detection\nThe D2Neighbors model used in our experiments is an in-\nstance of the family of distance-based anomaly detectors,\nwhich encompasses a variety of methods from the litera-\nture [2, 47–49, 74, 75]. The D2Neighbors model computes\nanomaly scores as o(x) = Mγ\nj\n\b\n∥x −uj∥p\np\n\t\nwhere x is the\ninput, (uj)N\nj=1 are the training data, and Mγ is a generalized\nf-mean, with function f(t) = exp(−γt). The function can\nbe interpreted as a soft minimum over distances to data\npoints, i.e. a distance to the nearest neighbors. In our exper-\niments, the data received as input are RGB images of size\n224×224 with pixel values encoded between −1 and 1, down-\nsized from their original high resolution using OpenCV’s\nfast nearest neighbor interpolation. We set γ so that the\naverage perplexity [76] equals 25% of the training set size\nfor each model.\nWe also consider the PatchCore [45] anomaly detection\nmodel, which uses mid-level patch features from a fixed\npre-trained network.\nIt constructs a memory bank of\nthese features from nominal example images during training.\nAnomaly scores for test images are computed by finding\nthe maximum distance between each test patch feature and\nits nearest neighbor in the memory bank. Distances are\ncomputed between patch features ϕp(x) and a memory bank\nof location independent prototypes (uj)N\nj=1. The overall\noutlier scoring function of PatchCore can be written as\no(x) = maxk minj ∥ϕk(x) −uj∥. The function ϕk is the fea-\nture representation aggregated from two consecutive layers\nat spatial patch location k, extracted from a pre-trained\nWideResNet50. The features from consecutive layers are\naggregated by rescaling and concatenating the feature maps.\nThe difference between our reported F1 scores and those in\n[45] is mainly due to the method used to resize the images.\nWe used the authors’ reference implementation 8 as the basis\nfor our experiments.\nAll of the above models were trained on the MVTec\ndataset.\nThe MVTec dataset consists of 15 image cate-\ngories (‘bottle’, ‘cable’, ‘capsule‘, ’carpet’, ‘grid’, ‘hazelnut’,\n‘leather’, ‘metal nut’, ‘pill’, ‘screw’, ‘tile’, ‘toothbrush’, ‘tran-\nsistor’, ‘wood’ and ‘zipper’) of industrial objects and textures,\n8 https://github.com/amazon-science/patchcore-inspection\n8\nwith good and defective instances for each category. For the\nexperiments based on D2Neighbors, we simulated different\ndata preprocessing conditions before and after deployment\nby changing the way images are resized from their original\nhigh resolution to 224 × 224 pixels. We first use a resiz-\ning algorithm found in OpenCV 4.9.0 [77] based on nearest\nneighbor interpolation. We then simulate post-deployment\nconditions using an improved resizing method, specifically\na bilinear interpolation implemented in Pillow 10.3.0 and\nused by default in torchvision 0.17.2 [78]. This improved\nresizing method includes antialiasing, which has the effect\nof smoothing the transitions between adjacent pixels of the\nresized image.\nExplanations for Representation Learning\nOur experiments examined dot product similarities in repre-\nsentation space, i.e. y = ⟨Φ(x), Φ(x′)⟩, where Φ denotes the\nfunction that maps the input features to the representation,\ntypically a deep neural network. To understand similarity\nscores in terms of input features, we used the BiLRP method\n[79] which extends the LRP technique [24, 26, 27, 80] for this\nspecific purpose. The conceptual starting point of BiLRP is\nthe observation that a dot product is a bilinear function of\nits input. BiLRP then proceeds by reverse propagating the\nterms of the bilinear function to pairs of activations from the\nlayer below and iterating down to the input. Denoting Rkk′\nthe contribution of neurons k and k′ to the similarity score\nin some intermediate layer in the network, BiLRP extracts\nthe contributions of pairs of neurons j and j′ in the layer\nbelow via the propagation rule:\nRjj′ =\nX\nkk′\nzjkzj′k′\nP\njj′ zjkzj′k′ Rkk′\n(1)\nIn practice, this reverse propagation procedure can be im-\nplemented equivalently, but more efficiently and easily, by\ncomputing a collection of standard LRP explanations (one\nfor each neuron in the representation layer) and recombining\nthem in a multiplicative manner:\nBiLRP(y) =\nX\nk\nLRP(Φk(x)) ⊗LRP(Φk(x′))\n(2)\nOverall, assuming the input consists of d features, BiLRP\nproduces an explanation of size d × d which is typically\nrepresented as a weighted bipartite graph between the set of\nfeatures of the two input images. Due to the large number\nof terms, pixel-to-pixel contributions are aggregated into\npatch-to-patch contributions, and elements of the BiLRP\nexplanations that are close to zero are omitted in the final\nexplanation rendering. In our experiments, we computed\nBiLRP explanations using the Zennit9 implementation of\n9 https://github.com/chr5tphr/zennit\nLRP which handles the ResNet50 architecture, and set Zen-\nnit’s LRP parameters to their default values.\nExplanations for the D2Neighbors Model\nThe D2Neighbors model we investigate for anomaly detection\nis a composition of a distance layer and a soft min-pooling\nlayer. To handle these layers, we use the purposely designed\nLRP rules of [28, 53]. Propagation in the softmin layer (Mγ\nj )\nis given by the formula\nRj =\nf(∥x −uj∥p\np)\nP\nj f(∥x −uj∥p\np)o(x)\n(3)\na ‘min-take-most’ redistribution, where f is the same func-\ntion as in Mγ\nj . Each score Rj can be interpreted as the\ncontribution of the training point uj to the anomaly of\nx.\nTo further propagate these scores into the pixel fre-\nquency domain, we adopt the framework of ‘virtual layers’\n[29, 30] and adapt it to the D2Neighbors model. As a fre-\nquency basis, we use the discrete cosine transform (DCT)\n[54], shown in Fig. 4 (right), which we denote by its col-\nlection of basis elements (vk)k. Since the DCT forms an\northogonal basis, we have the property P\nk vkv⊤\nk = I, and\nmultiplication by the identity matrix can be interpreted\nas a mapping to the frequencies and back. For the spe-\ncial case where p = 2, the distance terms in D2Neighbors\nreduce to the squared Euclidean norm ∥x −uj∥2. These\nterms can be developed to identify pixel-pixel-frequency in-\nteractions: ∥x −uj∥2 = (x −uj)⊤(P\nk vkv⊤\nk )(x −uj) =\nP\nk\nP\nii′[x −uj]i[x −uj]i′[vk]i[vk]i′. From there, one can\nconstruct an LRP rule that propagates the instance-wise\nrelevance Rj to the pixel-frequency features:\nRii′k =\nX\nj\n[x −uj]i[x −uj]i′[vk]i[vk]i′\nϵ + ∥x −uj∥2\nRj,\n(4)\nwhere the variable ϵ is a small positive term that handles the\ncase where x and uj overlap. A reduction of this propaga-\ntion rule can be obtained by marginalizing over interacting\npixels (Rik = P\ni′ Rii′k). Further reductions can be obtained\nby marginalizing over pixels (Rk = P\ni Rik) or frequencies\n(Ri = P\nk Rik). These reductions are used to generate the\nheatmaps in Fig. 5.\nData Availability\nAll data used in this paper, in particular, NIH’s CXR8 [33],\nGitHub-COVID [34], ImageNet [40], and MVTec-AD [81]\nare publicly available.\nCode Availability\nFull\ncode\nfor\nreproduction\nof\nour\nresults\nis\navail-\nable\nat\nhttps://git.tu-berlin.de/jackmcrider/\nthe-clever-hans-effect-in-unsupervised-learning.\n9\nAcknowledgements\nThis work was partly funded by the German Ministry for Ed-\nucation and Research (under refs 01IS14013A-E, 01GQ1115,\n01GQ0850, 01IS18056A, 01IS18025A and 01IS18037A), the\nGerman Research Foundation (DFG) as Math+: Berlin\nMathematics Research Center (EXC 2046/1, project-ID:\n390685689) and the DeSBi Research Unit (KI-FOR 5363,\nproject ID: 459422098), and DFG KI-FOR 5363. Further-\nmore, KRM was partly supported by the Institute of Informa-\ntion & Communications Technology Planning & Evaluation\n(IITP) grants funded by the Korea government (MSIT) (No.\n2019-0-00079, Artificial Intelligence Graduate School Pro-\ngram, Korea University and No. 2022-0-00984, Development\nof Artificial Intelligence Technology for Personalized Plug-\nand-Play Explanation and Verification of Explanation). We\nthank Stefan Ganscha for the valuable comments on the\nmanuscript. Correspondence to KRM and GM.\nReferences\n[1]\nT. B. Brown et al., “Language models are few-shot learners,”\nin NeurIPS, 2020.\n[2]\nL. Ruff et al., “A unifying review of deep and shallow\nanomaly detection,” Proc. IEEE, vol. 109, no. 5, pp. 756–\n795, 2021.\n[3]\nR. Krishnan, P. Rajpurkar, and E. J. Topol, “Self-\nsupervised learning in medicine and healthcare,” Nature\nBiomedical Engineering, vol. 6, no. 12, pp. 1346–1352, 2022.\n[4]\nA. Li et al., “Unsupervised analysis of transcriptomic pro-\nfiles reveals six glioma subtypes,” Cancer Research, vol. 69,\nno. 5, pp. 2091–2099, 2009.\n[5]\nL. Jiang, Y. Xiao, Y. Ding, J. Tang, and F. Guo, “Discov-\nering cancer subtypes via an accurate fusion strategy on\nmultiple profile data,” Frontiers in Genetics, vol. 10, 2019.\n[6]\nO. Eberle, J. Büttner, H. El-Hajj, G. Montavon, K.-R.\nMüller, and M. Valleriani, “Insightful analysis of historical\nsources at scales beyond human capabilities using unsu-\npervised machine learning and XAI,” arXiv 2310.09091,\n2023.\n[7]\nL. Rettig, M. Khayati, P. Cudré-Mauroux, and M.\nPiórkowski, “Online anomaly detection over big data\nstreams,” in Applied Data Science, Springer, 2019, pp. 289–\n312.\n[8]\nE. Eskin, A. Arnold, M. J. Prerau, L. Portnoy, and S. J.\nStolfo, “A geometric framework for unsupervised anomaly\ndetection,” in Applications of Data Mining in Computer\nSecurity, ser. Advances in Information Security, Springer,\n2002, pp. 77–101.\n[9]\nP. Bergmann, K. Batzner, M. Fauser, D. Sattlegger, and\nC. Steger, “The mvtec anomaly detection dataset: A com-\nprehensive real-world dataset for unsupervised anomaly\ndetection,” Int. J. Comput. Vis., vol. 129, no. 4, pp. 1038–\n1059, 2021.\n[10]\nJ. Zipfel, F. Verworner, M. Fischer, U. Wieland, M. Kraus,\nand P. Zschech, “Anomaly detection for industrial quality\nassurance: A comparative evaluation of unsupervised deep\nlearning models,” Comput. Ind. Eng., vol. 177, p. 109 045,\n2023.\n[11]\nR. Bommasani et al., “On the opportunities and risks of\nfoundation models,” arXiv 2108.07258, 2021.\n[12]\nT. Chen, S. Kornblith, K. Swersky, M. Norouzi, and\nG. E. Hinton, “Big self-supervised models are strong semi-\nsupervised learners,” in NeurIPS, 2020.\n[13]\nA. Radford et al., “Learning transferable visual models from\nnatural language supervision,” in ICML, ser. Proceedings\nof Machine Learning Research, vol. 139, 2021, pp. 8748–\n8763.\n[14]\nM. Moor et al., “Foundation models for generalist medical\nartificial intelligence,” Nature, vol. 616, no. 7956, pp. 259–\n265, Apr. 2023, issn: 1476-4687.\n[15]\nJ. Dippel et al., “RudolfV: A foundation model by pathol-\nogists for pathologists,” arXiv 2401.04079, 2024.\n[16]\nS. Lapuschkin, S. Wäldchen, A. Binder, G. Montavon, W.\nSamek, and K.-R. Müller, “Unmasking Clever Hans pre-\ndictors and assessing what machines really learn,” Nature\nCommunications, vol. 10, no. 1, p. 1096, 2019.\n[17]\nR. Geirhos et al., “Shortcut learning in deep neural net-\nworks,” Nat. Mach. Intell., vol. 2, no. 11, pp. 665–673,\n2020.\n[18]\nC. J. Anders, L. Weber, D. Neumann, W. Samek, K.-R.\nMüller, and S. Lapuschkin, “Finding and removing Clever\nHans: Using explanation methods to debug and improve\ndeep models,” Inf. Fusion, vol. 77, pp. 261–295, 2022.\n[19]\nL. Linhardt, K.-R. Müller, and G. Montavon, “Preemptively\npruning Clever-Hans strategies in deep neural networks,”\nInformation Fusion, vol. 103, p. 102 094, 2024.\n[20]\nJ. K. Winkler et al., “Association between surgical skin\nmarkings in dermoscopic images and diagnostic perfor-\nmance of a deep learning convolutional neural network\nfor melanoma recognition,” JAMA Dermatology, vol. 155,\nno. 10, pp. 1135–1141, 2019.\n[21]\nG. Montavon, W. Samek, and K.-R. Müller, “Methods\nfor interpreting and understanding deep neural networks,”\nDigital signal processing, vol. 73, pp. 1–15, 2018.\n[22]\nD. Gunning, M. Stefik, J. Choi, T. Miller, S. Stumpf,\nand G.-Z. Yang, “XAI–explainable artificial intelligence,”\nScience robotics, vol. 4, no. 37, eaay7120, 2019.\n[23]\nA. B. Arrieta et al., “Explainable artificial intelligence\n(XAI): concepts, taxonomies, opportunities and challenges\ntoward responsible AI,” Inf. Fusion, vol. 58, pp. 82–115,\n2020.\n10\n[24]\nW. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and\nK.-R. Müller, “Explaining deep neural networks and be-\nyond: A review of methods and applications,” Proceedings\nof the IEEE, vol. 109, no. 3, pp. 247–278, 2021.\n[25]\nF. Klauschen et al., “Toward explainable artificial intelli-\ngence for precision pathology,” Annual Review of Pathology:\nMechanisms of Disease, vol. 19, pp. 541–570, 2024.\n[26]\nS. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.\nMüller, and W. Samek, “On pixel-wise explanations for\nnon-linear classifier decisions by layer-wise relevance prop-\nagation,” PLOS ONE, vol. 10, no. 7, e0130140, Jul. 2015.\n[27]\nG. Montavon, A. Binder, S. Lapuschkin, W. Samek,\nand K.-R. Müller, “Layer-wise relevance propagation: An\noverview,” in Explainable AI, ser. Lecture Notes in Com-\nputer Science, vol. 11700, Springer, 2019, pp. 193–209.\n[28]\nJ. Kauffmann, K.-R. Müller, and G. Montavon, “Towards\nexplaining anomalies: A deep Taylor decomposition of one-\nclass models,” Pattern Recognition, vol. 101, p. 107 198,\n2020.\n[29]\nJ. Vielhaben, S. Lapuschkin, G. Montavon, and W. Samek,\n“Explainable AI for time series via virtual inspection layers,”\nPattern Recognition, vol. 150, p. 110 309, 2024.\n[30]\nP. Chormai, J. Herrmann, K.-R. Müller, and G. Montavon,\n“Disentangled explanations of neural network predictions\nby finding relevant subspaces,” IEEE Trans. Pattern Anal.\nMach. Intell., pp. 1–18, 2024. doi: 10.1109/TPAMI.2024.\n3388275.\n[31]\nL. Wang, Z. Q. Lin, and A. Wong, “COVID-Net: A tailored\ndeep convolutional neural network design for detection\nof COVID-19 cases from chest X-ray images,” Scientific\nReports, vol. 10, no. 1, 2020.\n[32]\nA. J. DeGrave, J. D. Janizek, and S.-I. Lee, “Ai for radio-\ngraphic covid-19 detection selects shortcuts over signal,”\nNature Machine Intelligence, vol. 3, no. 7, pp. 610–619,\nMay 2021.\n[33]\nX. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and\nR. M. Summers, “Chestx-ray8: Hospital-scale chest x-ray\ndatabase and benchmarks on weakly-supervised classifi-\ncation and localization of common thorax diseases,” in\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), IEEE, 2017.\n[34]\nJ. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Q. Duong,\nand M. Ghassemi, “COVID-19 image data collection:\nProspective predictions are the future,” arXiv 2006.11988,\n2020.\n[35]\nS. Azizi et al., “Big self-supervised models advance medical\nimage classification,” in ICCV, IEEE, 2021, pp. 3458–3468.\n[36]\nS. Eslami, C. Meinel, and G. de Melo, “PubMedCLIP: How\nmuch does CLIP benefit visual question answering in the\nmedical domain?” In EACL (Findings), Association for\nComputational Linguistics, 2023, pp. 1151–1163.\n[37]\nS.-C. Huang, A. Pareek, M. Jensen, M. P. Lungren, S.\nYeung, and A. S. Chaudhari, “Self-supervised learning\nfor medical image classification: A systematic review and\nimplementation guidelines,” npj Digital Medicine, vol. 6,\nno. 1, Apr. 2023, issn: 2398-6352.\n[38]\nT. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, “A\nsimple framework for contrastive learning of visual repre-\nsentations,” in ICML, ser. Proceedings of Machine Learning\nResearch, vol. 119, 2020, pp. 1597–1607.\n[39]\nJ. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny,\n“Barlow twins: Self-supervised learning via redundancy\nreduction,” in ICML, ser. Proceedings of Machine Learning\nResearch, vol. 139, 2021, pp. 12 310–12 320.\n[40]\nJ. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei,\n“Imagenet: A large-scale hierarchical image database,” in\nCVPR, IEEE Computer Society, 2009, pp. 248–255.\n[41]\nT. Chen, C. Luo, and L. Li, “Intriguing properties of con-\ntrastive losses,” in NeurIPS, 2021, pp. 11 834–11 845.\n[42]\nJ. Robinson, L. Sun, K. Yu, K. Batmanghelich, S. Jegelka,\nand S. Sra, “Can contrastive learning avoid shortcut solu-\ntions?” In NeurIPS, 2021, pp. 4974–4986.\n[43]\nJ. Dippel, S. Vogler, and J. Höhne, Towards fine-grained\nvisual representations by combining contrastive learning\nwith image reconstruction and attention-weighted pooling,\nICML Workshop: Self-Supervised Learning for Reasoning\nand Perception, 2021.\n[44]\nT. Li et al., “Addressing feature suppression in unsu-\npervised visual representations,” in WACV, IEEE, 2023,\npp. 1411–1420.\n[45]\nK. Roth, L. Pemula, J. Zepeda, B. Schölkopf, T. Brox, and\nP. V. Gehler, “Towards total recall in industrial anomaly\ndetection,” in IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, IEEE, 2022, pp. 14 298–14 308.\n[46]\nK. Batzner, L. Heckler, and R. König, “Efficientad: Accu-\nrate visual anomaly detection at millisecond-level latencies,”\nin WACV, IEEE, 2024, pp. 127–137.\n[47]\nS. Harmeling, G. Dornhege, D. Tax, F. Meinecke, and\nK.-R. Müller, “From outliers to prototypes: Ordering data,”\nNeurocomputing, vol. 69, no. 13, pp. 1608–1618, 2006.\n[48]\nC. C. Aggarwal, Outlier Analysis. Springer, 2013.\n[49]\nV. Chandola, A. Banerjee, and V. Kumar, “Anomaly de-\ntection: A survey,” ACM Comput. Surv., vol. 41, no. 3,\n15:1–15:58, 2009.\n[50]\nE. Parzen, “On estimation of a probability density function\nand mode,” The Annals of Mathematical Statistics, vol. 33,\nno. 3, pp. 1065–1076, Sep. 1962.\n[51]\nJ. Kim and C. D. Scott, “Robust kernel density estimation,”\nJ. Mach. Learn. Res., vol. 13, pp. 2529–2565, 2012.\n[52]\nB. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola,\nand R. C. Williamson, “Estimating the support of a high-\ndimensional distribution,” Neural Comput., vol. 13, no. 7,\npp. 1443–1471, 2001.\n11\n[53]\nG. Montavon, J. Kauffmann, W. Samek, and K.-R. Müller,\n“Explaining the predictions of unsupervised learning mod-\nels,” in xxAI - Beyond Explainable AI, ser. Lecture Notes\nin Artificial Intelligence, vol. 13200, 2022, pp. 117–138.\n[54]\nY. Yu, J. Qian, and Q. Wu, “Visual saliency via multi-\nscale analysis in frequency domain and its applications\nto ship detection in optical satellite images,” Frontiers in\nNeurorobotics, vol. 15, 2022.\n[55]\nG. Parmar, R. Zhang, and J. Zhu, “On aliased resizing and\nsurprising subtleties in GAN evaluation,” in CVPR, IEEE,\n2022, pp. 11 400–11 410.\n[56]\nM. Sugiyama, M. Krauledat, and K.-R. Müller, “Covari-\nate shift adaptation by importance weighted cross valida-\ntion.,” Journal of Machine Learning Research, vol. 8, no. 5,\npp. 985–1005, 2007.\n[57]\nM. Sugiyama and M. Kawanabe, Machine learning in non-\nstationary environments: Introduction to covariate shift\nadaptation. MIT press, 2012.\n[58]\nY. Iwasawa and Y. Matsuo, “Test-time classifier adjust-\nment module for model-agnostic domain generalization,”\nin NeurIPS, 2021, pp. 2427–2440.\n[59]\nC. Esposito, G. A. Landrum, N. Schneider, N. Stiefl, and S.\nRiniker, “Ghost: Adjusting the decision threshold to handle\nimbalanced data in machine learning,” Journal of Chemical\nInformation and Modeling, vol. 61, no. 6, pp. 2623–2640,\n2021.\n[60]\nP. Kirichenko, P. Izmailov, and A. G. Wilson, “Last layer re-\ntraining is sufficient for robustness to spurious correlations,”\nin ICLR, OpenReview.net, 2023.\n[61]\nT. Niven and H. Kao, “Probing neural network compre-\nhension of natural language arguments,” in ACL (1), As-\nsociation for Computational Linguistics, 2019, pp. 4658–\n4664.\n[62]\nB. Heinzerling, “NLP’s Clever Hans moment has arrived,”\nThe Journal of Cognitive Science, vol. 21, pp. 161–170,\n2020.\n[63]\nM. L. Braun, J. M. Buhmann, and K.-R. Müller, “On\nrelevant dimensions in kernel feature spaces,” J. Mach.\nLearn. Res., vol. 9, pp. 1875–1908, 2008.\n[64]\nR. Basri, M. Galun, A. Geifman, D. W. Jacobs, Y. Kasten,\nand S. Kritchman, “Frequency bias in neural networks for\ninput of non-uniform density,” in ICML, ser. Proceedings\nof Machine Learning Research, vol. 119, 2020, pp. 685–694.\n[65]\nS. Fridovich-Keil, R. G. Lopes, and R. Roelofs, “Spectral\nbias in practice: The role of function frequency in general-\nization,” in NeurIPS, 2022.\n[66]\nL. Arras et al., “Explaining and interpreting lstms,” in\nExplainable AI, ser. Lecture Notes in Computer Science,\nvol. 11700, Springer, 2019, pp. 211–238.\n[67]\nT. Schnake et al., “Higher-order explanations of graph\nneural networks via relevant walks,” IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 44, no. 11, pp. 7581–7596, 2022.\n[68]\nA. Ali, T. Schnake, O. Eberle, G. Montavon, K. Müller,\nand L. Wolf, “XAI for transformers: Better explanations\nthrough conservative propagation,” in ICML, ser. Proceed-\nings of Machine Learning Research, vol. 162, 2022, pp. 435–\n451.\n[69]\nF. R. Jafari, G. Montavon, K.-R. Müller, and O. Eberle,\n“MambaLRP: Explaining selective state space sequence\nmodels,” arXiv 2406.07592, 2024.\n[70]\nM. Munir, S. A. Siddiqui, A. Dengel, and S. Ahmed, “Deep-\nant: A deep learning approach for unsupervised anomaly\ndetection in time series,” IEEE Access, vol. 7, pp. 1991–\n2005, 2019.\n[71]\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\npre-training of deep bidirectional transformers for language\nunderstanding,” in NAACL-HLT (1), Association for Com-\nputational Linguistics, 2019, pp. 4171–4186.\n[72]\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learn-\ning for image recognition,” in Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 2016,\npp. 770–778.\n[73]\nO. Pelka, S. Koitka, J. Rückert, F. Nensa, and C. M.\nFriedrich, “Radiology objects in context (ROCO): A multi-\nmodal image dataset,” in Intravascular Imaging and Com-\nputer Assisted Stenting - and - Large-Scale Annotation\nof Biomedical Data and Expert Label Synthesis, D. Stoy-\nanov et al., Eds., ser. Lecture Notes in Computer Science,\nvol. 11043, Springer, 2018, pp. 180–189.\n[74]\nG. Pang, C. Shen, L. Cao, and A. van den Hengel, “Deep\nlearning for anomaly detection: A review,” ACM Comput.\nSurv., vol. 54, no. 2, 38:1–38:38, 2022.\n[75]\nO. Rippel, P. Mertens, and D. Merhof, “Modeling the\ndistribution of normal data in pre-trained deep features for\nanomaly detection,” in ICPR, IEEE, 2020, pp. 6726–6733.\n[76]\nF. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker, “Per-\nplexity—a measure of the difficulty of speech recognition\ntasks,” The Journal of the Acoustical Society of America,\nvol. 62, no. S1, S63–S63, Aug. 2005.\n[77]\nG. Bradski, “The OpenCV Library,” Dr. Dobb’s Journal\nof Software Tools, 2000.\n[78]\nTorchVision maintainers and contributors, Torchvision:\nPytorch’s computer vision library, https://github.com/\npytorch/vision, 2016.\n[79]\nO. Eberle, J. Büttner, F. Kräutli, K. Müller, M. Valleriani,\nand G. Montavon, “Building and interpreting deep simi-\nlarity models,” IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 44, no. 3, pp. 1149–1161, 2022.\n[80]\nW. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, and\nK.-R. Müller, Explainable AI: interpreting, explaining and\nvisualizing deep learning. Springer Nature, 2019, vol. 11700.\n[81]\nP. Bergmann, M. Fauser, D. Sattlegger, and C. Steger,\n“MVTec AD - A comprehensive real-world dataset for un-\nsupervised anomaly detection,” in CVPR, Computer Vision\nFoundation / IEEE, 2019, pp. 9592–9600.\n12\nThe Clever Hans Effect in Unsupervised Learning\n(Supplementary Notes)\nJacob Kauffmann, Jonas Dippel, Lukas Ruff, Wojciech Samek,\nKlaus-Robert Müller and Grégoire Montavon\nThese supplementary notes provide detailed results and analysis of the experiments conducted\n1\nin the main paper. They aim to further support the main paper’s claims about the prominence\n2\nand distinctiveness of the Clever Hans effect in unsupervised learning, and the importance of\n3\naddressing it in order to achieve unsupervised models that are more robust and reliable for\n4\ndownstream applications.\n5\nSupplementary Note A. Additional Results for X-Ray Representations\n6\nRecall from the main paper that we analyzed the PubMedCLIP’s foundation model [1], specifically\n7\nits representation of X-ray data and its use for a downstream COVID-19 detection task. For this\n8\npurpose, we considered a dataset constructed by merging the NIH CXR8 dataset [2] with the\n9\nGithub-hosted ‘COVID-19 Image Data Collection’ [3], and referred to these two data sources as\n10\nNIH and Github. The NIH data, collected before the onset of the COVID-19 pandemic, is from a\n11\nsingle hospital and is relatively homogeneous. The Github data, collected from multiple sources,\n12\nis more heterogeneous. In the main paper, we reported strong inhomogeneities in the classification\n13\nof the data across different subgroups, with the NIH instances being systematically correctly\n14\nclassified as negative and, in contrast, numerous Github negative instances being classified as\n15\npositive. We attributed this heterogeneity in prediction quality to a spurious reliance on textual\n16\nor annotation artifacts already latent in the representation of the unsupervised PubMedCLIP\n17\nmodel. In this supplementary note, we retrace the emergence of the heterogeneous classification\n18\nbehavior from the representation level up to the classifier by providing further examples and\n19\nanalyses.\n20\nAnalysis of the Unsupervised Representation\n21\nIn the main paper, we proposed to look at representation from the perspective of similarity\n22\nbetween pairs of instances, and we expressed similarity as dot products in representation space\n23\n(e.g. [4]):\n24\nk(x, x′) = Φ(x)⊤Φ(x′)\n(1)\nThe function Φ represents PubMedCLIP’s mapping from raw pixel values to its representation.\n25\nFig. 1 (left) shows the output of our BiLRP analysis, highlighting pairs of features responsible\n26\nfor the high measured dot-product similarity. Our analysis systematically highlights that the\n27\npredicted similarity is supported by spurious annotations on the X-ray images instead of the\n28\nunderlying pathology, a clear case of a Clever Hans effect. In other words, the PubMedCLIP\n29\nrepresentation supports similarity predictions that are right for the wrong reasons. Furthermore,\n30\nthe type and location of these artifacts used by PubMedCLIP vary across the subgroups of the\n31\ndataset, with the artifacts in the NIH subgroup being much more homogeneous than those in\n32\nthe Github subgroup.\n33\nThe reliance of the PubMedCLIP model on features that code for dataset-specific annotation\n34\nartifacts rather than the underlying pathology can also be demonstrated by performing a t-SNE\n35\nanalysis [5] of its representation. We color code each data point in the t-SNE visualization\n36\naccording to its provenance (NIH or Github). The visualization is shown in Fig. 1 (right). It\n37\narXiv:2408.08041v1  [cs.LG]  15 Aug 2024\nFigure 1: Left: BiLRP explanations of PubMedCLIP’s dot-product similarity scores for X-ray image\npairs belonging to the same class and data subset. Red curves indicate pairs of features of the two images\nthat support the similarity prediction. Right: T-SNE analysis of PubMedCLIP’s representation of X-ray\ndata. Colors indicate the data source (NIH CXR8 [2] or Github-Covid [3]) and the ground truth label\n(COVID-positive or COVID-negative).\nreveals a cluster structure in the data representation, where the clusters are more predictive\n38\nof data provenance than COVID negativity/positivity. While the COVID-positives cluster at\n39\nrather specific locations in the t-SNE space, they overlap significantly with the COVID-negatives\n40\nfrom the Github dataset. Finally, it should be noted that while color-coded T-SNE analysis (and\n41\nother correlational analyses for that matter) can establish the prominence of an already identified\n42\nartifact or source of heterogeneity in the representation, our Explainable AI analysis does not\n43\nrequire prior knowledge of the data artifact and can instead assist a human in identifying it.\n44\nAnalysis of the Downstream Classifier\n45\nWe now examine the transmission of the CH effect from the representation learning model\n46\nto the downstream classifier. We recall from the main paper that we built a linear readout\n47\nf(x) = w⊤Φ(x) + θ on top of the PubMedCLIP representation. Specifically, we have trained a\n48\nlinear SVM classifier on the COVID detection task, where f(x) < 0 represents COVID negatives\n49\nand f(x) > 0 represents COVID positives. Note that the same SVM classifier can also be\n50\nformulated in terms of dot-product similarities:\n51\nf(x) =\nN\nX\ni=1\nαik(x, xi) + θ\n(2)\nThe predictions of the SVM can thus be seen as a combination of dot-product similarities of\n52\nthe type we analyzed above. After training the SVM model and selecting the hyperparameters\n53\nusing hold-out validation, we found that C = 0.01 and a class weighting scheme adjusted to\n54\nbalance the class representation yielded the best performance. We examine the SVM prediction\n55\nperformance over the different data subgroups. The results are shown in Table 1. We observe a\n56\nstrong inhomogeneity of performance between different data sources.\n57\nTo further shed light on the instability in the SVM’s decision strategy, we use the LRP\n58\nexplanation technique [6, 7]. LRP allows us to identify, for each predicted instance, the extent\n59\nto which each input pixel contributed to the prediction. The results are shown in Fig. 2 for a\n60\nselection of correctly classified instances from each data subgroup. Similar to the BiLRP analysis\n61\n2\n⏟⏟⏟\nNIH\ngithub-neg\ngithub-pos\naverage\naverage\naverage\nFigure 2: LRP explanations for COVID-19 detection. Top: Exemplary chest X-ray images from each\ndata subgroup along with explanations showing pixel-wise evidence for/against COVID-19 as perceived\nby the model. Bottom: Explanations averaged over the entire subgroup. Red color indicates regions\ncontributing evidence for COVID-19, while blue color indicates evidence against COVID-19.\nabove, our LRP analysis of the SVM classifier reveals a strong reliance of the model on spurious\n62\nannotations, with COVID-positive instances again being the ones most strongly supported by\n63\nartifacts. The LRP analysis also reveals that COVID-negative predictions are supported by\n64\nannotation artifacts. The latter, however, differ from the positive ones in that they systematically\n65\noccur in the upper right corner of the input image. This Clever Hans-like prediction strategy,\n66\ncombined with the fact that the artifacts differ across datasets, explains the unstable prediction\n67\nperformance of the classifier. In particular, the high false positive rate (FPR) of the SVM on\n68\nthe Github data can be explained by the fact that the artifacts of the Github negatives do not\n69\ncoincide with the artifacts of the NIH negatives, with the latter constituting the majority of the\n70\nnegative instances. As a result, Github negatives fail to be drawn to the negative class and often\n71\nend up being misclassified as positives. In summary, our analysis of the downstream classifier\n72\nfurther verifies and scrutinizes the inheritance of the CH effect from the unsupervised model to\n73\nthe supervised model and its consequences in terms of prediction performance.\n74\nSupplementary Note B. Additional Results for Generic Image Models\n75\nIn the main paper, we also experimented with generic image models trained with different\n76\nunsupervised representation learning algorithms, specifically SimCLR [8, 9], BarlowTwins [10]\n77\nand CLIP [11], the details of which can be found in the methods section of the main paper. We\n78\nextend on these experiments by providing further analysis of the learned representations and the\n79\ndownstream classifiers built on top of them.\n80\noriginal\ngithub\nNIH\nAccuracy\n88.6\n75.2\n100.0\nFalse positive rate (FPR)\n14%\n40%\n0%\nTable 1: Analysis of a linear SVM built on the PubMedCLIP’s unsupervised foundation model and\ntrained on the COVID classification task. Accuracy and false positive rates are reported for the original\ndata (aggregation of github and NIH sources), and for the individual data sources.\n3\nAnalysis of the Unsupervised Representations\n81\nWe first proceed with the BiLRP analysis of each model as well as a purely supervised model (a\n82\nstandard ResNet50 pre-trained on ImageNet). The results of the analysis are shown in Fig. 3 for\n83\nadditional image pairs of the class garbage truck and coho, respectively.\n84\nRecall our findings from the main paper that similarity predictions built on unsupervised\n85\nrepresentations are often Clever Hans-like. For example, the similarity between image pairs of\n86\ncertain fish classes was supported by a human holding the fish in the background (either the\n87\nhuman face for the CLIP model, or the human body for SimCLR and BarlowTwins). Similarly,\n88\nfor the truck data, the CLIP model was shown to rely heavily on textual logos to express\n89\nsimilarity. Highlighting the heterogeneity of the unsupervised representations, the same textual\n90\nlogos were suppressed by SimCLR and BarlowTwins, presumably due to their eccentricity in\n91\nthe image. Here, we complement these results by including a supervised baseline in our analysis\n92\n(leftmost example in Fig. 3). We find that the supervised baseline model focuses on task-relevant\n93\nfeatures better than unsupervised approaches. However, the supervised baseline also appears to\n94\nbe overfitted to the task at hand, matching features that belong to the same class, but failing to\n95\ndisentangle distinct object parts, such as the wheels and the roof of the truck.\n96\nTo further verify the result of the BiLRP analysis, which among other things showed that textual\n97\nlogos are salient in the CLIP representation, we perform a t-SNE analysis [5] in representation\n98\nspace. We append to the original truck data an artificial dataset called ‘logo’, consisting of\n99\nFigure 3: BiLRP explanations of dot product similarities for each representation learning model. The\nexplanations are computed on a selection of ImageNet images from the truck and fish subsets. We observe\nseveral Clever Hans effects, such as a reliance on copyright tags in the truck subset and a reliance on\nhumans in the fish subset.\n4\nSimCLR (vehicle)\nBarlow Twins  (vehicle)\nCLIP (vehicle)\nno logo\nlogo\nSupervised (vehicle)\nhuman\nother\nSimCLR (fish)\nBarlow Twins (fish)\nCLIP (fish)\nSupervised (fish)\nFigure 4: T-SNE plots of the representations from different models of the truck subset (top) and the\nfish subset (bottom). For the truck subset, instances are colored according to the presence or absence of a\ntextual logo in the image. For the fish subset, instances are colored according to the presence of a human\nin the image.\ntruck data with a textual logo artificially inserted into each image. We then compute a t-SNE\n100\nembedding on the augmented data, mapped to the representation of each model. Figure 4 shows\n101\nthe t-SNE plots for each model’s representation, color-coded by the presence or absence of a\n102\nlogo. For the CLIP model, the logo is strongly expressed in its representation, as shown by the\n103\nyellow and blue dots forming distinct clusters. This suggests that a downstream classifier has an\n104\nincentive to use the textual logo as part of its prediction strategy. However, for the SimCLR and\n105\nBarlow Twins models, as well as for the supervised baseline, the analysis suggests that the logo\n106\ndoes not play a significant role.\n107\nWe repeat the same t-SNE analysis for the ‘fish’ subset, with the resulting embedding color-\n108\ncoded according to the presence or absence of a human. We use a Faster R-CNN object detection\n109\nmodel pre-trained on the COCO dataset [12] to classify each image according to whether it\n110\ncontains a human or not. In each case, we observe high representational similarity of images\n111\ncontaining humans. This suggests that human features are easily accessible when performing the\n112\ndownstream fish classification task.\n113\nAnalysis of Downstream Classifiers\n114\nTo analyze the transmission of the unsupervised Clever Hans effect from the unsupervised\n115\nrepresentation to the downstream supervised models, in the main paper we investigated the\n116\nconstruction of two supervised models on top of each representation (one for fish classification and\n117\none for truck classification). These models are linear-softmax classifiers trained with cross-entropy\n118\nloss. We extend the analysis of these classifiers by analyzing their decision strategy using LRP\n119\nheatmaps. We use the same LRP procedure that we have used to compute BiLRP explanations,\n120\nand we apply the rule LRP-0 [7] in the last readout layer. LRP heatmaps are shown in Fig. 5.\n121\nWe also report the accuracy scores of each model for each data subset considered in Table 2.\n122\nWe further elaborate on the structure of the accuracy scores of the problematic data subgroups\n123\n(truck images with a logo and fish images with a human) by providing the full confusion matrices\n124\nin Fig. 6.\n125\n5\nInput (fish)\nCLIP\nSimCLR\nBarlowTwins\nSupervised\nInput (truck)\nCLIP\nSimCLR\nBarlowTwins\nSupervised\nFigure 5: LRP heatmaps for the downstream classification of the fish and truck subsets for different\nrepresentation learning models. The heatmaps show the pixel-wise contributions to the output of the\ndownstream classifier for the ground truth class. Red color indicates positively contributing pixels and\nblue color indicates negatively contributing pixels.\nWe start with the truck classification task, where we distinguish between the original data,\n126\nconsisting of the 50 test images of each truck (400 images in total), and the logo subset, derived\n127\nfrom the original test data by artificially inserting in each image a logo similar to the one found\n128\nin the ‘garbage truck’ class. On the original data, we observe that the CLIP model performs best\n129\nwith an accuracy of 86.6%, followed by 85.7% for the supervised baseline, and sensibly lower\n130\naccuracies of 82.9% and 77.7% for the BarlowTwins and SimCLR models, respectively (cf. Table\n131\n2, column ‘original’). However, the high accuracy of the CLIP model is partly the result of a\n132\nCH strategy based on the detection of a logo in the lower left corner of the image, as shown by\n133\nthe LRP heatmaps in Fig. 5. In comparison, models built on the BarlowTwins and SimCLR\n134\nrepresentations, as well as our supervised baseline, are largely unresponsive to this logo. Notably,\n135\nthe reliance (or lack thereof) of the supervised model mirrors our BiLRP analysis (cf. Fig. 3),\n136\nwhere only the CLIP model was affected by the logo artifact.\n137\nLooking at the prediction accuracy on the subgroup of truck images with artificially added\n138\nlogos (see Table 2, column ‘logo’), the performance of SimCLR, BarlowTwins and our supervised\n139\nbaseline remains stable compared to the original test data, with a maximum performance drop of\n140\nless than 1%. On the other hand, CLIP’s performance drops drastically by almost five percentage\n141\n6\nImageNet:truck\nImageNet:fish\noriginal\nlogo\noriginal\nhuman\nno human\nCLIP\n85.0\n80.5 ↓\n86.5\n83.8\n82.5\n+ CH mitigation (1)\n85.0\n83.8\n+ CH mitigation (2)\n84.8\n84.0\n+ CH mitigation (5)\n85.2\n85.0\n+ CH mitigation (10)\n85.0\n85.0\n+ CH mitigation (20)\n81.2\n81.2\nSimCLR\n74.8\n74.5\n82.2\n78.6\n78.4\nBarlowTwins\n80.2\n80.2\n83.1\n75.6 ↓\n81.1\nSupervised\n83.8\n83.2\n86.2\n84.2\n81.9\nTable 2: Prediction accuracy obtained by downstream classification models built on the truck and fish\nsubtasks from each representation (including representations enhanced for CH mitigation by removing a\ngiven number of spuriously relevant feature maps). Accuracy is reported for the original data, for truck\nimages with an artificially added logo, and for the subgroups of fish images with and without humans.\npoints, making it the worst performer on this subset. The reason for the low accuracy of the\n142\nCLIP model is revealed by looking at CLIP’s confusion matrix (cf. Fig. 6). One observes that a\n143\nwhopping 55.1% of the CLIP model’s errors are misclassifications into the ‘garbage truck’ class,\n144\ncompared to less than 15% for non-CLIP models.\n145\nFigure 6: Confusion matrices highlighting the distribution of model errors in terms of true and predicted\nclass. The percentages shown correspond to the proportion of errors falling in the indicated region of\nthe confusion matrix. Abbreviations: [fi]re engine, [ga]rbage truck, [po]lice van, [tr]ailer truck, [to]w truck,\n[mo]ving van, [pi]ckup, [mi]nivan for the truck classes, and [bar]racouta, [coh]o, [ten]ch, [stu]rgeon,\n[gar], [sti]ngray, [ham]merhead, [gre]at white shark, [puf]fer, [tig]er shark, [eel], [gol]dfish, [roc]k beauty,\n[ele]ctric ray, [ane]mone fish, [lio]nfish for the fish classes.\n7\nWe now investigate downstream models that classify the fish ImageNet subset from the different\n146\nunsupervised representations. All models perform reasonably well on this classification task. The\n147\nCLIP model ranks first with 86.5%, followed by the supervised model (86.2%), and BarlowTwins\n148\nand SimCLR with 83.1% and 82.2%, respectively. Figure 5 shows LRP heatmaps explaining\n149\nthe classification by each model of a selection of images from the class ‘coho’. We observe that\n150\nthe CLIP model focuses on the human in the image, specifically the face, rather than the fish.\n151\nSimilarly, the SimCLR and BarlowTwins models also focus on the human, but more specifically\n152\non the body. Only the supervised model is reasonably unresponsive to the human and instead\n153\nclearly focuses on the actual fish to be classified. This shows that the unsupervised learning\n154\nmodels studied here all implement a CH strategy. This casts doubt on the generalization of these\n155\nmodels and their performance across different subgroups.\n156\nWe investigate the consequence of the identified CH strategies by building a specific subgroup\n157\nconsisting of all fish instances containing humans (e.g. images with a human holding a fish in his\n158\nhands). This subgroup is built by including only images for which the Faster-R CNN auxiliary\n159\nmodel has detected the presence of a human. Since humans are not homogeneously distributed\n160\nacross classes in both training and test data, we rebalance the test data by reweighting all classes\n161\ncontaining more than 10 instances in a way that their effective sample size becomes 10. This\n162\nrebalancing can also be interpreted as a simulation of real-world conditions, removing potentially\n163\nspurious correlations between fish classes and the presence of a human. On this class-rebalanced\n164\nsubset of data, the accuracy of SimCLR drops below 80%, and BarlowTwins achieves an accuracy\n165\nof only 75.6%.\n166\nThe decrease in performance for fish classification, as for the truck case, can be understood by\n167\nlooking at the confusion matrices (see Fig. 6). We observe that more than 70% of the errors of\n168\nthe SimCLR and BarlowTwins models correspond to misclassifications as one of the six classes\n169\nwith the highest human prevalence in the original data (‘barracouta’, ‘coho’, ‘tench’, ‘sturgeon’,\n170\n‘gar’, ‘stingray’). In contrast, only 55.4% of the errors of the supervised model correspond to\n171\nthis type of misclassification. The reason for the distinct error structure of the SimCLR and\n172\nBarlowTwins models is that their weak representation of fish features, as shown in the LRP\n173\nand BiLRP explanations, induces the model to exploit the spurious correlation between human\n174\npresence and these six classes. As a result, any fish image that contains a human will tend to be\n175\npredicted, often incorrectly, into one of these classes.\n176\nSupplementary Note C. Additional Results for Anomaly Detection\n177\nThis note complements the unsupervised anomaly detection experiments from the main paper. It\n178\nprovides the Explainable AI analysis and measured performance before and after CH mitigation,\n179\nfor each MVTec category retained in our experiments. Note that our criterion for retaining an\n180\nMVTec category was that the D2Neighbors model accurately predicts it with F1 scores above\n181\n0.9 on the original data (cf. Table 3, column original).\n182\nTo verify the nature of these accurate anomaly predictions, and in particular to test whether\n183\nthey are based on a valid strategy or instead ‘right for the wrong reasons’, we applied an\n184\nExplainable AI analysis in the main paper. Here we provide the details of the analysis for\n185\nthe five retained MvTec categories and their corresponding D2Neighbors model. The analysis\n186\nprovides pixel-wise explanations of the predicted anomaly scores. By introducing a Discrete\n187\nCosine Transform (DCT) virtual layer, as described in the main paper, the analysis is extended\n188\nto produce frequency and joint pixel-frequency explanations. Explanations for typical instances\n189\nof each retained MVTec-AD category are shown in Figure 7.\n190\nFor each MvTec category, the D2Neighbors model responds to a wide range of frequencies,\n191\nwith different frequency bands contributing differently to the decision strategy. In particular,\n192\nthe true anomaly patterns are mostly expressed in the mid-frequency range, while the low and\n193\nhigh frequency bands contain artifacts. Especially in the high frequency range, we can see the\n194\n8\nbottle\ncapsule\npill\nwood\ntoothbrush\ninput\npixel-wise\ncontributions\nlow mid\nhigh\nfrequency contributions\nlow\nmid\nhigh\njoint pixel/frequency contributions\nFigure 7: For the retained 5 categories of the MVTec-AD dataset, we show (from left to right), anomalous\ninput images (with ground truth outlined in red), pixel-wise explanations of the D2Neighbors anomaly\npredictions, frequency-based explanations, and joint pixel-frequency explanations. For the frequency-based\nexplanations, the x-axis is plotted on a power scale, with bins representing increasingly larger frequency\nranges as we move to the high frequencies: (0-2), (3-17), (18-69), (70-188), (189-409), (410-769), (770-\n1313), (1314-2088), (2089-3142), (3143-4528), (4529-6303), (6304-8525), (8526-11254), (11255-18491),\n(18492-23132), (23133-28548), (28549-34811), (34812-41995), (41996-50176).\nmodel’s response to the border of the bottle, text markings on the capsule, specks on the pill,\n195\nwood stripes, or the individual hairs of the toothbrush, all of which are typically not indicative\n196\nof a manufacturing defect (the true manufacturing defect is marked with a red outline in the\n197\ninput image). In other words, our analysis shows that part of D2Neighbors’ anomaly prediction\n198\nstrategy is of the Clever Hans type.\n199\nIn the main paper, we investigated the consequences of the identified erroneous decision strategy\n200\non the prediction performance under a change in data quality. Specifically, the D2Neighbors\n201\nmodel was trained and validated on MVTec images resized using a coarse nearest neighbor\n202\ninterpolation algorithm, and we simulated a post-deployment scenario in which a better bilinear\n203\nresizing incorporating antialiasing is being used (details in the methods section of the main\n204\npaper). The effect of changing the resizing procedure for one representative image of each MVTec\n205\ncategory is shown in Figure 8.\n206\nWe observe that the introduction of antialiasing in the resizing procedure strips images of\n207\ntheir high-frequency noise-like components. This suggests that the output of an anomaly model\n208\nthat is sensitive to high frequencies may be significantly affected, especially if the model’s\n209\nexposure to high frequencies is not confined to a particular region of the input image. This\n210\n9\nantialiasing\nFigure 8: Effect of the resizing procedure on a selection of images from each retained MVTec category.\nTop: original resizing procedure without antialiasing.\nBottom: simulated post-deployment resizing\nprocedure with antialiasing. We can observe that the post-deployment images are sensibly less noisy.\noriginal\n(bot, cap, pil, too, woo)\ndeployed\n(bot, cap, pil, too, woo)\nD2Neighbors (ℓ1)\n0.93\n(0.89, 0.91, 0.94, 0.97, 0.93)\n0.74 ↓\n(0.87, 0.45, 0.83, 0.87, 0.71)\n+ CH mitigation\n0.92\n(0,88, 0.91, 0.93, 0.97, 0.94)\n0.92 ↑\n(0.88, 0.91, 0.93, 0.97, 0.94)\nD2Neighbors (ℓ2)\n0.92\n(0.90, 0.91, 0.92, 0.93, 0.93)\n0.80 ↓\n(0.85, 0.73, 0.91, 0.85, 0.67)\n+ CH mitigation\n0.94\n(0.91, 0.92, 0.92, 0.98, 0.95)\n0.93 ↑\n(0.91, 0.92, 0.92, 0.98, 0.93)\nD2Neighbors (ℓ4)\n0.91\n(0.92, 0.90, 0.92, 0.90, 0.93)\n0.83 ↓\n(0.83, 0.88, 0.89, 0.75, 0.78)\n+ CH mitigation\n0.93\n(0.93, 0.90, 0.91, 0.98, 0.93)\n0.93 ↑\n(0.93, 0.90, 0.91, 0.98, 0.92)\nPatchCore\n0.92\n(0.92, 0.90, 0.92, 0.89, 0.98)\n0.85 ↓\n(0,91, 0.88, 0.88, 0.64, 0.96)\n+ CH mitigation\n0.97 ↑\n(1.00, 0.96, 0.93, 1.00, 0.95)\n0.96 ↑\n(1.00, 0.95, 0.93, 0.98, 0.95)\nTable 3:\nF1 scores on the MVTec-AD data per category and averaged.\nScores are reported for\nthe simulated original and post-deployment conditions and for a variety of anomaly detection models.\nAbbreviations: [bot]tle, [cap]sule, [pil]l, [too]thbrush, [woo]d.\nalso suggests that the performance of a downstream model, such as a detector of manufacturing\n211\nflaws, may be severely degraded. This is confirmed in Table 3 where we report per category\n212\nand averaged F1 scores before and after modifying the resizing algorithm (columns ‘original’\n213\nand ‘deployed’). In particular, we observe that D2Neighbors’ F1 score drops significantly after\n214\nthe resizing change, and this effect is generally consistent across categories. In particular, we\n215\nobserve a severe performance degradation on the ‘wood’ category, which can be explained by the\n216\nspatially widespread presence of high-frequency artifacts in the original conditions (cf. Fig. 8).\n217\nFurthermore, the effect of antialiasing varies between different norm configurations in the\n218\nD2Neighbors models. The ℓ1-norm variant, which has a higher overall exposure than the ℓ2-norm\n219\nvariant, sees its F1 score drop from 0.93 to 0.74 after antialiasing is introduced. Again, the wood\n220\ncategory is severely affected, but even more so the capsule category, where the fine asperities of\n221\nthe surface on which the capsule is placed cause high frequency noise all over the image. Note,\n222\nhowever, that since the anomalies for the capsule category are more spatially confined than those\n223\nfor the wood category, the ℓ2 and ℓ4-norm variants of D2Neighbors perform comparatively much\n224\nbetter after deployment due to their pixel-sparsity mechanism. The ℓ4-norm variant generally\n225\nshows the strongest resilience across all categories, with an average F1 score that drops from\n226\n0.91 to a less severe 0.83.\n227\nThe difficulty of the D2Neighbors model in providing anomaly scores that generalize to\n228\nchanging data conditions also occurs in PatchCore, where we observe a decrease in the F1 score\n229\nfrom 0.92 on the original data to 0.85 after deployment. This is significant because D2Neighbors\n230\nand PatchCore are very different in structure. The latter extracts features using a pre-trained\n231\nWideResNet50 before computing distances, and applies spatial max-pooling to identify the region\n232\nin the image that is most responsible for the anomaly, building invariance to the other regions.\n233\n10\nThe effect of these structural differences can be seen by looking at the F1 scores per category.\n234\nIn contrast to D2Neighbors, PatchCore experiences the largest decrease in performance for the\n235\n‘toothbrush’ class (from 0.89 to 0.64). A possible explanation is an amplification/suppression effect\n236\nin the PatchCore feature extractor, similar to those found in CLIP, SimCLR, and BarlowTwins.\n237\nSpecifically, PatchCore reacts strongly to hair pattern of the toothbrush, and the very thin\n238\nanomaly-causing bent hairs no longer find their way through PatchCore’s feature extractor after\n239\nbeing weakened by antialiasing.\n240\nThe Clever Hans effect, where models exploit spurious correlations rather than robust patterns,\n241\nmanifests as overfitting to high-frequency noise in training data, leading to poor generalization\n242\non anti-aliased test images. In the main paper, we demonstrated the significant performance\n243\ndegradation of CH models under changing data conditions and provided detailed results in\n244\nTable 3. We then proposed a simple but effective CH mitigation strategy consisting of inserting\n245\na pre-processing layer at the input of each anomaly detection model that performs an 11x11\n246\nGaussian blur. Note that the architecture modification is made before training, so that the data\n247\nis systematically blurred during training and testing and also after deployment. The blurring\n248\nlayer is designed to reduce the model’s response to high frequency noise while maintaining its\n249\nresponse to low- and mid-frequency patterns. The effect in terms of F1 score of CH mitigation is\n250\nshown in Table 3.\n251\nWe observe an improvement in performance on the deployed test set across all models and\n252\ncategories. For example, D2Neighbors ℓ2 recovers from 0.80 to 0.93 with CH mitigation. The\n253\nper-category F1 scores show that the performance recovery is systematic across all categories,\n254\nand even outperforms the original model for the ‘toothbrush’ category. This last result suggests\n255\nthat blurring also improves D2Neighbors’ ability to represent anomalies. Applying the same blur\n256\nfilter to the input of PatchCore, we observe that the F1 score jumps from 0.85 to 0.96. Again,\n257\nthis is a significant improvement over the original performance of 0.92. One hypothesis is that\n258\nthe blurred images propagate better through PatchCore’s feature extraction layers, allowing a\n259\nfiner anomaly detection model to be built than the one built on noisy images. Overall, these\n260\nresults highlight the effectiveness of CH mitigation in improving both average and per-category\n261\naccuracy for all models tested.\n262\nReferences\n263\n[1]\nS. Eslami, C. Meinel, and G. de Melo, “PubMedCLIP: How much does CLIP benefit visual\n264\nquestion answering in the medical domain?” In EACL (Findings), Association for Computational\n265\nLinguistics, 2023, pp. 1151–1163.\n266\n[2]\nX. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “Chestx-ray8: Hospital-scale\n267\nchest x-ray database and benchmarks on weakly-supervised classification and localization of\n268\ncommon thorax diseases,” in 2017 IEEE Conference on Computer Vision and Pattern Recognition\n269\n(CVPR), IEEE, 2017.\n270\n[3]\nJ. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Q. Duong, and M. Ghassemi, “COVID-19 image\n271\ndata collection: Prospective predictions are the future,” arXiv 2006.11988, 2020.\n272\n[4]\nK.-R. Müller, S. Mika, G. Rätsch, K. Tsuda, and B. Schölkopf, “An introduction to kernel-based\n273\nlearning algorithms.,” IEEE Transactions on Neural Networks, vol. 12, no. 2, pp. 181–201, 2001.\n274\n[5]\nL. van der Maaten and G. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning\n275\nResearch, vol. 9, pp. 2579–2605, 2008.\n276\n[6]\nS. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek, “On pixel-wise\n277\nexplanations for non-linear classifier decisions by layer-wise relevance propagation,” PLOS ONE,\n278\nvol. 10, no. 7, e0130140, Jul. 2015.\n279\n[7]\nG. Montavon, A. Binder, S. Lapuschkin, W. Samek, and K.-R. Müller, “Layer-wise relevance\n280\npropagation: An overview,” in Explainable AI, ser. Lecture Notes in Computer Science, vol. 11700,\n281\nSpringer, 2019, pp. 193–209.\n282\n11\n[8]\nT. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, “A simple framework for contrastive learning\n283\nof visual representations,” in ICML, ser. Proceedings of Machine Learning Research, vol. 119, 2020,\n284\npp. 1597–1607.\n285\n[9]\nT. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big self-supervised models are\n286\nstrong semi-supervised learners,” in NeurIPS, 2020.\n287\n[10]\nJ. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, “Barlow twins: Self-supervised learning via\n288\nredundancy reduction,” in ICML, ser. Proceedings of Machine Learning Research, vol. 139, 2021,\n289\npp. 12 310–12 320.\n290\n[11]\nA. Radford et al., “Learning transferable visual models from natural language supervision,” in\n291\nICML, ser. Proceedings of Machine Learning Research, vol. 139, 2021, pp. 8748–8763.\n292\n[12]\nS. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with\n293\nregion proposal networks,” Advances in neural information processing systems, vol. 28, 2015.\n294\n12\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2024-08-15",
  "updated": "2024-08-15"
}