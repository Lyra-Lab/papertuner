{
  "id": "http://arxiv.org/abs/2408.10055v1",
  "title": "Efficient Exploration in Deep Reinforcement Learning: A Novel Bayesian Actor-Critic Algorithm",
  "authors": [
    "Nikolai Rozanov"
  ],
  "abstract": "Reinforcement learning (RL) and Deep Reinforcement Learning (DRL), in\nparticular, have the potential to disrupt and are already changing the way we\ninteract with the world. One of the key indicators of their applicability is\ntheir ability to scale and work in real-world scenarios, that is in large-scale\nproblems. This scale can be achieved via a combination of factors, the\nalgorithm's ability to make use of large amounts of data and computational\nresources and the efficient exploration of the environment for viable solutions\n(i.e. policies).\n  In this work, we investigate and motivate some theoretical foundations for\ndeep reinforcement learning. We start with exact dynamic programming and work\nour way up to stochastic approximations and stochastic approximations for a\nmodel-free scenario, which forms the theoretical basis of modern reinforcement\nlearning. We present an overview of this highly varied and rapidly changing\nfield from the perspective of Approximate Dynamic Programming. We then focus\nour study on the short-comings with respect to exploration of the cornerstone\napproaches (i.e. DQN, DDQN, A2C) in deep reinforcement learning. On the theory\nside, our main contribution is the proposal of a novel Bayesian actor-critic\nalgorithm. On the empirical side, we evaluate Bayesian exploration as well as\nactor-critic algorithms on standard benchmarks as well as state-of-the-art\nevaluation suites and show the benefits of both of these approaches over\ncurrent state-of-the-art deep RL methods. We release all the implementations\nand provide a full python library that is easy to install and hopefully will\nserve the reinforcement learning community in a meaningful way, and provide a\nstrong foundation for future work.",
  "text": "Efficient Exploration in Deep\nReinforcement Learning: A Novel\nBayesian Actor-Critic Algorithm\nNikolai Rozanov\nAcademic Advisor: Dr Ben Calderhead\nA dissertation submitted in partial fulfillment\nof the requirements for the degree of\nMaster of Research\nof\nUniversity College London\nDepartment of Computer Science\nUniversity College London\nSeptember 2, 2019\narXiv:2408.10055v1  [cs.LG]  19 Aug 2024\n2\nI, Nikolai Rozanov, confirm that the work presented in this thesis is my own.\nWhere information has been derived from other sources, I confirm that this has been\nindicated in the work.\nAbstract\nReinforcement learning (RL) and Deep Reinforcement Learning (DRL), in particu-\nlar, have the potential to disrupt and are already changing the way we interact with\nthe world. One of the key indicators of their applicability is their ability to scale\nand work in real-world scenarios, that is in large-scale problems. This scale can be\nachieved via a combination of factors, the algorithm’s ability to make use of large\namounts of data and computational resources and the efficient exploration of the\nenvironment for viable solutions (i.e. policies).\nIn this work, we investigate and motivate some theoretical foundations for deep\nreinforcement learning. We start with exact dynamic programming and work our\nway up to stochastic approximations and stochastic approximations for a model-\nfree scenario, which forms the theoretical basis of modern reinforcement learning.\nWe present an overview of this highly varied and rapidly changing field from the\nperspective of Approximate Dynamic Programming. We then focus our study on\nthe short-comings with respect to exploration of the corner-stone approaches (i.e.\nDQN, DDQN, A2C) in deep reinforcement learning. On the theory side, our main\ncontribution is the proposal of a novel Bayesian actor-critic algorithm. On the em-\npirical side, we evaluate Bayesian exploration as well as actor-critic algorithms on\nstandard benchmarks as well as state-of-the-art evaluation suites and show the ben-\nefits of both of these approaches over current state-of-the-art deep RL methods. We\nrelease all the implementations and provide a full python library that is easy to in-\nstall and hopefully will serve the reinforcement learning community in a meaningful\nway, and provide a strong foundation for future work.\nAcknowledgements\nFirst and foremost, I want to thank my mother. (Thanks mom). For your love and\ndedication. I also want to thank my father and my siblings. I would not have been\nwhere I am without you. Your support and incredible inspiration have brought me\nhere.\nDr Ben Calderhead, apart from the critical thoughts, advice, guidance and trust I\nhave received from you in the capacity of my supervisor, it is your human character\nthat I value the most in this time that I had the pleasure of working alongside you.\nThank you.\nI also shall not forget all the friends with whom I have shared a common path so\nfar. Particularly, Maroˇs Janˇco, Jean-Franc¸ois Ton, Petru Constantinescu, Michał\nMaliczowski, Dorothee Dober, Eliana Fausti and Flavian Manea - thank you for the\ncountless hours of love and happiness.\n– To working for the better of mankind.\nFinally, I would like to acknowledge the studentship awarded by UKRI with the\naward reference number 1923151 that facilitated this research.\nContents\n1\nIntroduction\n7\n1.1\nThesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2\nBackground and Motivation\n11\n2.1\nRecent Advances in Reinforcement Learning\n. . . . . . . . . . . .\n11\n2.2\nPreliminaries\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.3\nMarkov Decision Process (MDP) . . . . . . . . . . . . . . . . . . .\n16\n2.4\nExact Dynamic Programming . . . . . . . . . . . . . . . . . . . . .\n18\n2.4.1\nValue Iteration\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.4.2\nPolicy Iteration . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.4.3\nGeneralised Policy Iteration . . . . . . . . . . . . . . . . .\n23\n2.4.4\nSummary and Shortcomings . . . . . . . . . . . . . . . . .\n24\n2.5\nStochastic Approximations . . . . . . . . . . . . . . . . . . . . . .\n25\n2.5.1\nValue Approximators . . . . . . . . . . . . . . . . . . . . .\n28\n2.5.2\nQ-learning\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n2.5.3\nActor Critic . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n2.5.4\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n2.6\nExploration vs. Exploitation\n. . . . . . . . . . . . . . . . . . . . .\n37\n2.6.1\nImportance of Exploration in Approximate Methods\n. . . .\n38\n2.6.2\nVarious Methods of Exploration . . . . . . . . . . . . . . .\n38\n2.6.3\nOptimal Learning - the solution to exploration . . . . . . . .\n39\n3\nDesign, Method and Experiment\n41\nContents\n6\n3.1\nAlgorithmic Contributions\n. . . . . . . . . . . . . . . . . . . . . .\n41\n3.1.1\nBayesian Actor Critic . . . . . . . . . . . . . . . . . . . . .\n42\n3.1.2\nFrequentist Thompson DQN . . . . . . . . . . . . . . . . .\n43\n3.2\nEmpirical Study . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n3.2.1\nBandit Exploration . . . . . . . . . . . . . . . . . . . . . .\n44\n3.2.2\nState-of-the-art Agent Behaviour Suite\n. . . . . . . . . . .\n45\n3.3\nSoftware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n3.3.1\nBARL - Framework\n. . . . . . . . . . . . . . . . . . . . .\n45\n3.3.2\nBsuite Extensions . . . . . . . . . . . . . . . . . . . . . . .\n46\n3.4\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n4\nAnalysis\n47\n4.1\nExploration in Bandits\n. . . . . . . . . . . . . . . . . . . . . . . .\n47\n4.2\nAgent Behaviour Suite Results . . . . . . . . . . . . . . . . . . . .\n49\n4.2.1\nA standard Problem . . . . . . . . . . . . . . . . . . . . . .\n50\n4.2.2\nDeriving non-linear policies . . . . . . . . . . . . . . . . .\n51\n4.2.3\nExploring long-term strategies with sparse rewards . . . . .\n52\n4.2.4\nOverall results\n. . . . . . . . . . . . . . . . . . . . . . . .\n52\n4.3\nError Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n4.4\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5\nConclusion & Outlook\n55\n5.1\nOutlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n5.1.1\nFrontiers\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\nAppendices\n58\nA Bellman Optimality Operator is a Contraction Mapping\n58\nB\nBsuite Data\n60\nBibliography\n71\nChapter 1\nIntroduction\nTechnology and mankind have evolved together in an endless cycle. This work as-\npires to be a tiny step in the same direction; it should contribute to making applied\nsystems possible that can be used in the real world. The nature of the methods,\nresults and findings described here is for the purpose of building systems that can\nadapt to dynamic and uncertain environments and act optimally in these. This\nclass of methods is referred to as sequential decision making nowadays more com-\nmonly known as reinforcement learning. There is a rich literature on such problems\nand methods stemming from different fields and under different names: Con-\ntrol Theory [Bellman, 1966], Reinforcement Learning [Sutton and Barto, 2018],\nNeuro-dynamic Programming [Bertsekas and Tsitsiklis, 1996], Operations Re-\nsearch [Ortega and Lin, 2004], Neuroscience [Mnih et al., 2015], Game Theory\nand Economics [Seierstad and Sydsaeter, 1986] and many more. The full range\nof applications of sequential decision-making methods is probably innumerable,\nas most of the human activity and many natural occurrences are precisely of the\nnature of sequential decision making. Some examples are medical trials, where the\ndosage, frequency and whether to continue needs to be decided on a regular basis\ngiven the prognosis of a patient, or management of an energy supply chain, where\neach power plant’s output needs to be constantly regulated and directed, or more\nmodern applications such as building autonomous vehicles or robotics control more\ngenerally, as well as optimal routing and warehouse management, which are widely\nneeded in industry and logistics, among others. Naturally, many more scientific\n8\ndiscoveries and contributions need to happen in order for us to be able to employ\nartificially intelligent systems across all of these without human supervision - if\nsuch a thing will ever be a reality.\nThe current progress in the field of sequential decision making comes from the\nmulti-disciplinary field of machine learning and in particular from reinforcement\nlearning (RL). The unprecedented availability of large-scale compute and data trig-\ngered a wave of exploration into methods that scale with both of these parameters. It\nturned out that deep learning (DL) [LeCun et al., 2015] was precisely such a method\nthat scaled very well with available compute and data, as it doesn’t require manual\nfeature engineering but learns to discover salient features automatically. Deep learn-\ning was the necessary catalyst for another wave of Artificial Intelligence (AI) re-\nsearch. The actual dawn can probably be marked by [Krizhevsky et al., 2012]. This\nmetaphorical explosion of methods, applications and advances in machine learning\nalso brought its fruits to sequential decision making in the form of Deep RL. The\nmost notable results are those where human performance, assumed to be the gold\nstandard, was overtaken by these algorithms. In particular, examples include algo-\nrithms outperforming humans on many of the Atari2600 Games [Mnih et al., 2015],\nwinning against one of the top-ranked Go players [Silver et al., 2016], winning\nagainst professionals in an e-sport, which used Deep RL [Schulman et al., 2017],\netc.. However, as is noted by many of the authors themselves, these accomplish-\nments required enormous amounts of computational power and the ability to simu-\nlate the given environments. This naturally poses a significant obstacle to applying\nthese advances in real-world scenarios, where the environment cannot be simulated\nmillions or billions of times. Hence, at least at this stage in the evolution and the\ndistribution of this technology, the main obstacle lies in the data efficiency of these\nmethods. This translates to the very central question of exploration vs. exploitation\nin sequential decision-making problems and the motivation behind our work.\n1.1. Thesis Outline\n9\n1.1\nThesis Outline\nThis work can be seen as a combination of a survey of recent advances, method-\nological proposals, empirical evaluations of the core ideas and building blocks for\nfollow-on research. Deep reinforcement learning is a highly varied and rapidly\nevolving field. In this work, apart from our other contributions, we provide a dis-\ntinct overview of the most recent methods and advances. The perspective of this\noverview takes a more foundational view from the perspective of exact dynamic\nprogramming and its approximations. We elaborate on the key results and stochas-\ntic approximations to the exact methods developed by Bellman et. al., which serves\nas one of the two foundations that motivate the choice and development of our\nown algorithm. The second foundation lies in the topic of exploration vs. ex-\nploitation within the RL and Deep RL setting. The majority of state-of-the-art\nmethods conduct exploration and learning via the ε-greedy approach or the Boltz-\nmann distribution over the decision or actions space, however, it has been shown\nthat these strategies can lead to worst-case performance [Osband et al., 2017]. Our\nwork builds on previous work [Osband et al., 2017], which studies the shortcoming\nof these methods and proposes Bayesian methods based on Thompson sampling\n[Thompson, 1933] to improve the data efficiency. Our theoretical contribution lies\nin proposing a novel Bayesian actor-critic algorithm that combines the theoretical\nfoundation of actor-critic methods as well as the strength of Bayesian exploration\nusing Thompson sampling. Finally, we implement and study these individual bene-\nfits empirically and compare them to current state-of-the-art models.\nThe outline of our work is as follows.\nChapter 2 - Background and Moti-\nvation\nintroduces the state-of-the-art of the field and presents the main concepts:\nMarkov decision processes (MDP), Value Iteration, Temporal Difference learning,\nstochastic approximation and Bayesian Deep Learning.\nChapter 3 - Design,\nMethods and Experiment\noutlines our novel algorithm, the main experiments\nand describes how empirical validation was conducted.\nChapter 4 - Analysis fo-\ncuses on the results, where we empirically show that methods that guide exploration\nwith tracked uncertainty estimates show promising results and that actor-critic al-\n1.1. Thesis Outline\n10\ngorithms show promising results in their convergence speed as well.\nChapter 5\n- Conclusion & Outlook\ndiscusses how the results fit into the bigger picture and\ndescribe how this work serves as a foundation for future work.\nChapter 2\nBackground and Motivation\nThis chapter gives a full overview of the methods used in contemporary RL. We\nstudy the theoretical foundation that underlies these methods, as well as the clever\nstochastic approximation methods that allow these methods to scale to modern-day\nrequirements. We particularly emphasise the stochastic approximation techniques\nas these are quite often omitted in today’s analysis and discussion within the com-\nmunity. We present the Q-learning algorithm and briefly discuss the implications\nthat non-linear function approximators, such as neural networks, have on the con-\nvergence properties of this algorithm. We also discuss a very different set of algo-\nrithms, which are called policy gradient methods. These set the foundation for our\nactor-critic method.\n2.1\nRecent Advances in Reinforcement Learning\nBefore diving into the theoretical foundations of RL, it is beneficial to outline some\nof the most striking advances in RL and their unbelievable results. This should give\nfurther motivation for the importance of reinforcement learning, as well as set clear\ntargets when investigating the theory as to what needs to be understood.\nThe most significant advances in (deep) RL are very likely: Alpha Go Zero\n[Silver et al., 2017], which beat one of world’s best Go players, AlphaZero\n[Silver et al., 2018], which learned Go, Chess and Shogi among others only through\nself-play, OpenAI’s Dota Five using the PPO algorithm [Schulman et al., 2017],\nDQN applied to Atari games [Mnih et al., 2015], Superhuman scores on Mon-\n2.1. Recent Advances in Reinforcement Learning\n12\ntezuma’s Revenge without imitation learning [Burda et al., 2018], and the ap-\nplication of a pure simulation trained RL algorithm in a real-world robot\n[Andrychowicz et al., 2018].\nThe following should serve as a quick summary\nfor future reference:\nDQN\nThe main accomplishment, as stated in the paper as well, is the possibility of us-\ning highly nonlinear function approximators to approximate the Q-function. The\nmethod that allows this is the use of Experience Replay, which allows the network\nto learn outside of the current behaviour; the use of two networks, which allows\nagain further decoupling from learning and acting; and finally the Least Square\nTemporal Difference Error:1\nL(θi) = E(x,R)∼Experience Replay\n\u0014\u0012\nR+max\na∈A γQ(x,a,θ ′\ni )−Q(x,a,θi)\n\u00132\u0015\nwhere θ represents the parameters of the first Neural Network and θ ′ represents the\nsecond, decoupled Neural Network. θ ′ is synchronised with θ every C time-steps.\nAlpha Go Zero\nThe Alpha Go work, which accumulated in three papers accomplished many un-\nprecedented things. In particular, it managed to outperform a professional human\nplayer in Go for the first time in human history.\nThe algorithmic details of Alpha Go go beyond the scope of this work, as they\nutilise additional features such as Monte Carlo Tree Search to improve upon stan-\ndard learning in Reinforcement Learning settings.\nPPO applied to Dota 2\nDota 2 is a complicated multi-player online game. Being good at it involves team-\nwork, short term control as well as long term planning. The fact that a single algo-\nrithm manages to accomplish this without human demonstration is a clear demon-\nstration that RL algorithms are already able to solve problems that resemble the real\nworld. While the full set-up for Open AI’s team is out of scope for our work, the\n1The Temporal Difference Error, as well as the notation used, will be elucidated in later parts of\nthe work.\n2.1. Recent Advances in Reinforcement Learning\n13\nPPO algorithm [Schulman et al., 2017] is briefly discussed.\nThe proximal policy optimisation algorithm is based on learning a single network\nthat has both the task of predicting the value estimates for states as well as acting\nas the policy. This builds on some of the ideas in Alpha Go. The empirical results\nsuggest that such joining of purposes into a single network has an advantageous\neffect. This method falls into the category of actor-critic methods.\nExploring Montezuma’s Revenge\nMontezuma’s revenge is a very challenging game in so far that rewards are very\nsparse. I.e. one has to take many actions in order to see any change in the score of\nthe game. Naturally, ε-greedy exploration methods are predestined to not accom-\nplish anything in such scenarios, as the chances of randomly winning a game are\ndiminishingly low over larger time horizons. The clever idea of Open AI’s team was\nto build an internal reward signal for the agent that rewards curiosity, which they\naccomplish by trying to predict the next state, i.e. by trying to learn the transition\nkernel P and always trying those actions of the game where the predictions were\nthe worst. Using this method Open AI’s agent manages to be the first algorithm\ntrained entirely through self-play to outperform humans in Montezuma’s Revenge.\nLearning in Simulation applying in the World\nThe final work that will be briefly discussed is again Open AI’s contribution to the\nfield. They managed to take an agent trained in a simulated environment and ap-\nply it in the real world. This is fascinating from many perspectives; firstly, this\nbrings reinforcement learning agents so much closer to the real world, secondly,\nthis demonstrates a method of how one can leverage large scale computation to\nsolve problems in the real world. The main technical idea lies in training the agent\nwith many variations of a similar environment. What this means is that the physical\nproperties of the robot such as friction, momentum etc. were left as random vari-\nables and the agent learned to solve all of the versions of the environment, hence\nwhen it was applied in the real world it was merely solving one of the variations it\nlearned to solve.\nThis variety of algorithms leaves us with a natural question of which ones to\n2.2. Preliminaries\n14\nuse, when and why? To understand and decide between this variety of algorithms it\nis crucial to understand their theoretical motivation. This theoretical foundation is\neven more important when one wants to design and contribute to new algorithms.\nFurthermore, all of these advances have a single thing in common and this is the\nincredible amount of computation that had to be used to train these systems. This\nis only applicable for a handful of problems, however, building systems that can\ninteract in everyday scenarios with limited computation available is still a significant\nchallenge and therefore work on computationally more efficient methods is one of\nthe key directions for the development of such systems.\n2.2\nPreliminaries\nBefore diving into an in-depth discussion on the various theoretical topics we want\nto establish consistent notation and outline important underlying assumptions of the\nproblem at hand.\nIn a rather general sense, a single agent2 sequential decision-making problem\ncan be described as a continuous control problem with an underlying stochastic pro-\ncess governing the environment dynamics, with some functional of the continuous\nstochastic reward (or cost) function3 being the objective function. I.e.:\n˙x(t) = P(x(t),a(t),t)\nV = Φ(x(t0),x(T),t0,T)+\nZ t=T\nt=t0\nR\n\u0012\nx(t),a(t),t\n\u0013\ndt\no = Πx\nwhere x is the state of the environment ∀t ∈[t0,T]. P is a time indexed stochastic\nprocess, Φ is some reward function at the boundary conditions acting as an offset.\nR as the main reward function or distribution, V acts as the objective function that\nneeds to be optimised by the agent’s control signal a and o is the observation signal\n2Multi-Agent Sequential Decision making is another topic altogether and brings its\nown challenges such as asymmetric information flow,\nsee for example Witsenhausen’s\ncounterexample.[Witsenhausen, 1968]\n3Without loss of generality control problems can be stated either as reward maximising or cost\nminimising problems, with reinforcement learning conventionally using reward functions.\n2.2. Preliminaries\n15\nthat the agent receives and can use to optimise its actions. Now, in our case we will\nbe studying this problem with several assumptions.\n1. Discrete Problem - actions (or control) as will be discrete, i.e.:\na ∈\n{0,1,2,...,n} and time t will be discrete (, i.e. t ∈{t0,...,tT}. Time t is not\nnecessarily assumed to be finite.\n2. Fully Observable - the agent observes x directly, i.e.: Π is assumed to be the\nidentity function.\n3. Strict Stationarity - we will assume that the stochastic transition function P\nis time independent.\nFigure 2.1: The control setting: Agent interacting with an environment and receiving a\nreward signal.\nThese assumptions are standard in the reinforcement learning community\n[Osband et al., 2017, Szepesv´ari, 2010]. However, even apart from this, these as-\nsumptions allow for an incredible variety of problems and applications in the real\nworld. Perhaps the two most significant assumptions in this work, as well as in the\nmajority of recent deep RL research, are the stationarity assumption and the full\n2.3. Markov Decision Process (MDP)\n16\nobservability. The resulting problem with these assumptions can be expressed as a\nMarkov Decision Process (MDP).\n2.3\nMarkov Decision Process (MDP)\nWe start off by defining a (finite) Markov Decision Process (MDP). An MDP is a\ntuple M = (X,A,P,Rρ0). X is a finite state space and without loss of generality\ncan be assumed to be X = {0,1,2,...,S}, with state x = 0 being the terminal state4\nin case of finite-horizon MDPs, A is a finite actions space and R ⊂R is the domain\nof the reward. P is the transition probability kernel that governs the dynamics and\nassigns a probability to every state and reward x ∈X ×R for every state and action\n(x,a) ∈X × A, as (x′,R) ∼P(·|x,a). Finally, ρ0 defines the distribution of the\ninitial state x(0) = x0 ∼ρ0.\nGiven the above definition of the MDP, an agent interacts with it sequentially by\ntaking a decision at every time step t. The following recursive equations summarise\nthis:\nx0 ∼ρ0\n(2.1)\n(xt+1,Rt+1) ∼P(xt+1,Rt+1|xt,at)\n(2.2)\n(2.3)\nAn episode is defined to be a chain of (x0,a0,x1,R1,a1,...,xT,RT) where xT = 0 is\nthe terminating state. In case of infinite-horizon MDPs there is no terminal state.\nThe finite-horizon return is defined as:\nG =\nT\n∑\nt=1\nRt\n(2.4)\n4Note that in some literature the terminal state is treated separately, including the system dynam-\nics. These methods are equivalent and in our case x=0 can and should be treated as an absorbing\nstate that is connected to every other.\n2.3. Markov Decision Process (MDP)\n17\nOr correspondingly in the infinite-horizon case:\nG =\n∞\n∑\nt=1\nγtRt\nfor γ ∈(0,1)\n(2.5)\nThe second definition incorporates the first with Rt = 0 for t > T and γ = 1, ∀t.\nHence, we will be using the second to refer to both cases. We will also define\nrelated terms and functions that are used when developing results and methods to\nsolve MDPs.\nState Transition Probabilities\nIt is sometimes useful to just consider the state-transitions without the associated\nreward. The state-transition probabilities p : X ×X ×A →[0,1] ⊂R are given by:\nxt+1 ∼p(xt+1|xt,a) =\nZ\nR∈R P(xt+1,R|xt,a)dR\nState-Action Expected Reward\nthe state-action expected rewards r : X ×A →R ⊂R are given by:\nr(x,a) = E\n\u0014\nR|x,a\n\u0015\nWhere R comes from (x′,R) ∼P(x,R|x,a)\nPolicy\nA policy is a conditional distribution over the action space a ∼π(·|x), usually de-\nnoted π(x). A deterministic policy is a mapping π : x →a.\nValue Function\nThe (state) value function given policy π, denoted V π, is defined as a conditional\nexpectation of the return (2.5), i.e.:\nV π(x) = E\n\u0014 ∞\n∑\nt=1\nγtRt|x0 = x\n\u0015\n(2.6)\nHere the expectation is taken over sequences of observations by following the mod-\nified transition kernel Pπ(·|x) = ∑a∈A P(·|x,a)π(a|x).\nState-Value Function (Q-Function)\n2.4. Exact Dynamic Programming\n18\nThe state-value function given a policy π is commonly known and denoted as\nQπ(x,a). It is a natural extension of the value function:\nQπ(x,a) = E\n\u0014 ∞\n∑\nt=1\nγtRt|x0 = x,a0 = a\n\u0015\n(2.7)\nHere the expectation is taken in the same way as for the Value function.\n2.4\nExact Dynamic Programming\nIn this section, we want to discuss what it means to perform well in MDPs. In par-\nticular, we discuss the Bellman Equations and Bellman optimality equation. Then\nwe establish that given that one knows the true dynamics of the environment one\ncan use Dynamic Programming to solve the MDP, we conclude this section by dis-\ncussing the obvious short-comings of these methods.\nOptimal Value functions\nThe optimal value function, denoted V ∗(x), is one that has the highest possible value\nfor every state x. It is defined by:\nV ∗(x) = max\nπ V π(x)\n,∀x\n(2.8)\nSimilarly, for the state-action value function:\nQ∗(x,a) = max\nπ Qπ(x,a)\n,∀x∀a\n(2.9)\nIn this case taking the maximum over all possible policies does not pose a problem,\nsince the optimal policy can be deterministic [Bellman, 1966] and hence there are\nfinitely many possibilities.\nBellman Optimality Equation\nThe Bellman Optimality Equation [Bellman, 1966] relates the value function to the\nstate value function in the optimal case. This relation also helps establish a recur-\nsive formulation of both the value function and state-action value function. The\nintuition behind the equation lies in the fact that the optimal policy also maximised\n2.4. Exact Dynamic Programming\n19\nthe current step. I.e.:\nV ∗(x) = max\na∈A Q∗(x,a)\n(2.10)\n= max\na∈A E\n\u0014 ∞\n∑\nt=1\nγtRt|x0 = x,a0 = a\n\u0015\n(2.11)\n= max\na∈A E\n\u0014\nR1 +\n∞\n∑\nt=2\nγtRt|x0 = x,a0 = a\n\u0015\n(2.12)\n= max\na∈A E\n\u0014\nR1 +γ\n∞\n∑\nt′=1\nγt′Rt′|x0 = x,a0 = a\n\u0015\n(2.13)\n= max\na∈A E\n\u0014\nR1 +γV ∗(x1)|x0 = x,a0 = a\n\u0015\n(2.14)\n= max\na∈A\n\u001a\nr(x,a)+γEx1∼p|(x,a)\n\u0014\nV ∗(x1)\n\u0015\u001b\n(2.15)\n= max\na∈A\n\u001a\nr(x,a)+γ ∑\nx1∈X\np(x1|x,a)V ∗(x1)\n\u001b\n(2.16)\nSimilarly for Q∗one can derive the Bellman optimality equation:\nQ∗(x,a) = r(x,a)+γ ∑\nx′∈X\np(x′|x,a)max\na′∈AQ∗(x′,a′)\n(2.17)\nOptimal Policy Given the optimal state value function or state-action value function\nthen the optimal policy is given by:\nπ∗(a|x) = arg max\na∈A Q∗(x,a)\n(2.18)\n2.4.1\nValue Iteration\nWe now want to introduce an algorithm that allows us to compute the optimal value\nfunction and therefore provide a method of computing the optimal policy.\nLet us first write equation (2.16) more compactly in operator notation. We will refer\nto this operator as the Bellman optimality operator, i.e.:\n(2.16) = V ∗= T ∗V ∗\n(2.19)\n2.4. Exact Dynamic Programming\n20\nThis equation forms the basis of the value iteration algorithm. The core principle\nlies in defining an iterative scheme:\nV0(x) = 0\n∀x ∈X\n(2.20)\nVn+1(x) = (T ∗Vn)(x)\n∀x ∈X\n(2.21)\nThe convergence of this scheme relies on the fact that T is a contraction operator\nunder the max norm. Hence, by Banach’s fixed-point theorem, V converges to a\nunique point for every state x and therefore must coincide with the optimal value\nfunction for every state. We show in appendix A that T ∗is indeed a contraction\nmapping in the finite dimensional case, i.e. that ||T ∗(V)−T ∗(V ′)||∞< ||V −V ′||∞,\nwhere the max norm is taken over the individual states.\nInterestingly, these results both translate with minor technical conditions to\nmore general settings, where the finite cardinality of the MDP does not hold any-\nmore. Furthermore, it also generalises to asynchronous state updates, as opposed to\nupdating all states at once. (cf. [Bertsekas and Tsitsiklis, 1996], Chapter 2, Propo-\nsition 2.3. (Asynchronous Value Iteration)).\nFinally, once the value iteration algorithm terminates we want to be able to recover\nan optimal policy. It is clear that we can use (2.17) to express the optimal state-\naction value function in terms of the optimal value function:\nQ∗(x,a) = r(x,a)+γ ∑\nx′∈X\np(x′|x,a)V ∗(x)\n(2.22)\nUsing this result and the relationship between a state-action value function and a\ncorresponding greedy deterministic policy (2.18) we obtain the optimal policy π∗.\n2.4.2\nPolicy Iteration\nAnother way of looking at obtaining the optimal policy is by an iterative and al-\nternating improvement of the policy and value function. For this, we need to first\nconsider how given some policy π one can evaluate this policy, i.e. how one can\n2.4. Exact Dynamic Programming\n21\ncompute the corresponding value function (2.6).\nPolicy Evaluation Step\nIt turns out that using a slight variation of the Bellman optimality operator (2.16)\nand using a very similar derivation we can obtain a recursive equation to calculate\nthe value function for a given policy. Assuming a deterministic policy, we can derive\nit as follows:\nV π(x) = E\n\u0014 ∞\n∑\nt=1\nγtRt|x0 = x\n\u0015\n(2.23)\n= E\n\u0014 ∞\n∑\nt=1\nγtRt|x0 = x,a0 = π(x)\n\u0015\n(2.24)\n= E\n\u0014\nR1 +\n∞\n∑\nt=2\nγtRt|x0 = x,a0 = π(x)\n\u0015\n(2.25)\n= E\n\u0014\nR1 +γ\n∞\n∑\nt′=1\nγt′Rt′|x0 = x,a0 = π(x)\n\u0015\n(2.26)\n= E\n\u0014\nR1 +γV π(x1)|x0 = x,a0 = π(x)\n\u0015\n(2.27)\n= r(x,π(x))+γEx1∼P|(x,π(x))\n\u0014\nV π(x1)\n\u0015\n(2.28)\n= r(x,π(x))+γ ∑\nx1∈X\nP(x1|x,π(x))V π(x1)\n(2.29)\nAgain, writing this more compactly in operator form, which we will refer to as the\nBellman operator, we get:\n(2.6) = V π = T πV π\n(2.30)\nSimilarly, we have the Bellman operator for the state-action function:\nQπ(x,a) = r(x,a)+γ ∑\nx′∈X\np(x′|x,a)Qπ(x′,π(x′))\n(2.31)\nHence, starting of with some policy π0 and using the Bellman operator, we can use\n2.4. Exact Dynamic Programming\n22\nthe following iterative algorithm to evaluate the given policy:\nV0(x) = 0\n∀x ∈X\n(2.32)\nVt+1(x) = T πVt(x)\n∀x ∈X\n(2.33)\nAs in the optimal case, it turns out that this is also a contraction mapping with re-\nspect to the max-norm. A proof would be very similiar to the one given in Appendix\nA. Therefore, again by the Banach’s fixed-point theorem we have convergence guar-\nantees.\nPolicy Improvement Step\nApart from the policy evaluation step, we also need to understand how we can\nimprove the policy and furthermore how we can arrive at an optimal policy.\nGiven a current policy π and constructing a new policy with respect to the corre-\nsponding value function V π can be done by taking a greedy one-step look-ahead\naction, i.e.:\nπ′(a|x) = argmax\na∈A Qπ(x,a)\n(2.34)\nIt can be shown (e.g. [Sutton and Barto, 2018], Chapter 4, (4.7) ) that (2.34) leads to\nan improved value function V π′ ≥V π. Furthermore, if at some point the inequality\nbecomes an equality V π′(x) = V π(x) ∀x ∈X then π is the optimal policy. This can\nbe easily shown. Starting with the fact that π′ optimises Qπ:\nV π′(x) = max\na∈A Qπ(x,a)\n(2.35)\n= max\na∈A\n\u001a\nr(x,a)+γ ∑\nx′∈X\np(x′|x,a)V π(x)\n\u001b\n(2.36)\n= max\na∈A\n\u001a\nr(x,a)+γ ∑\nx′∈X\np(x′|x,a)V π′(x)\n\u001b\n(2.37)\n= T ∗V π′\n(2.38)\nWhere (2.37) follows from the assumption that the policy improvement step did\n2.4. Exact Dynamic Programming\n23\nnot lead to any change. T ∗, as always, represents the Bellman optimality operator\nand therefore we see that V π′ is a fixed point of the Bellman optimality operator,\nhowever, because it is a contraction it is a unique fixed point and thus V π′ = V ∗.\n□\n2.4.3\nGeneralised Policy Iteration\nAs a final method for exact dynamic programming, we will briefly outline a gen-\neralised form of both the value iteration and policy iteration algorithm. One can\nsee from the general format of the policy iteration algorithm that the policy im-\nprovement step needs a converged and correct value function for a particular policy,\nhence between any two policy improvement steps policy iteration requires the com-\nputation of the value function. This incurs a relatively large cost. On the other hand,\nvalue iteration requires one full convergence of the value function to the optimal\nvalue function. Furthermore, if one looks carefully then one sees that value iteration\nincorporates a policy improvement step at every stage. It turns out that one can do\npolicy improvement steps before a full convergence of the value function V π and at\nany step in between. The proof is given in [Bertsekas and Tsitsiklis, 1996], Chapter\n2, Proposition 2.5. (Asynchronous Policy Iteration). Hence, the fully general form\nof value and policy iteration is given by:\n2.4. Exact Dynamic Programming\n24\nAlgorithm 1: Generalised Policy Iteration\nInput : MDP(X,A,R,P,ρ0), T, ε, N\nOutput: π∗\nε\na, V ∗\nε\n//initialisation ;\nV(x) ←0, ∀x ∈X ;\nπ(x) ←randb(a), a ∈A,∀x ∈X ;\nwhile ||V −T ∗(V)||∞> ε do\n//Sample random states to update value and policy ;\nX ′ ←rand(X,N) X ⊂X;\nX ′′ ←rand(X,N) X ⊂X;\n//Value Update;\nfor i ←0 to T −1 do\nfor x ∈X ′ do\nV(x) ←T πV(x) ;\nend\nend\n// Policy Update;\nfor x ∈X ′′ do\nπ(x) ←argmaxa∈A Q(x,a) ;\nend\nend\naWe denote by Xε any function or value that is within ε of the optimal solution\nbHere rand(a,n) returns an element of type a of cardinality n or 1 if n is not specified.\nThis is indeed a generalised policy iteration algorithm, as setting T = 1 gives us\nvalue iteration, while setting T = ∞gives us policy iteration.\n2.4.4\nSummary and Shortcomings\nIn this section, we have seen ways and methods to ”solve” a finite Markov\nDecision Process.\nOverall the method relies on evaluating policies with the\nhelp of value functions and then updating the policy given a greedy pol-\nicy improvement step.\nThese results extend beyond the finite case of MDPs\n[Bertsekas and Tsitsiklis, 1996, Sutton and Barto, 2018].\nHowever, in practice\nthese exact methods have two major shortcomings:\n1. Requirement of full knowledge of the MDP\n2. Tractability issues in large state or action spaces\n2.5. Stochastic Approximations\n25\nFull Knowledge of the MDP\nIn reality, it is very rarely the case that the true dynamics of the system are known.\nIn fact, for most real-world problems either the dynamics are not known or are\nincredibly hard to formulate correctly. Prominent examples are self-driving cars,\nrobotics, dialogue systems, finance etc. In such cases, it would be impossible to\napply these methods directly. In traditional control theory and in general when\none wants to use these exact methods one needs to estimate the dynamics first\n[Deisenroth and Rasmussen, 2011, Kaiser et al., 2019] and then use DP to solve the\nproblem.\nTractability in large state or action spaces\nGeneral policy iteration requires that every state is iterated many times. Given for\nexample even the game of Backgammon this is 1020, which clearly is not tractable.\nFor the game of Go, this is a significantly larger number[Silver et al., 2016]. Real-\nworld scenarios such as self-driving cars, dialogue systems (such as Siri or Alexa),\netc. have even larger state-spaces and even possibly countably many.\nConclusion\nWhat should be noted is that Dynamic Programming (DP) as a method is not in-\nefficient, rather the scalability issues come from the size of the problems. In fact, it\ncan be shown that DP finds the optimal solution in polynomial time in the actions\nand state-space sizes (i.e. |X| and |A|) [Sutton and Barto, 2018], Chapter 4, Section\n4.7. Whereas the total policy space size is |X||A|. Hence, DP is exponentially faster\nthan direct search algorithms would be. Nevertheless, we need methods that work\nwell under large state and action spaces as well as methods that work without the\nknowledge of the true dynamics. We investigate this in the next section.\n2.5\nStochastic Approximations\nWe have seen that exact dynamic programming, although with good theoretical\nguarantees and results, is still not applicable in real-world scenarios due to in-\ntractability and violation of certain assumptions, most importantly the complete\nknowledge assumption.\nIn this section, we want to present and review several\n2.5. Stochastic Approximations\n26\nstochastic approximation methods that work well under imperfect knowledge and\ninformation conditions, as well as can be applied as approximate solvers when the\nfull problem is not tractable.\nThe general type of these stochastic approximation algorithms is of an iterative\nnature. In particular, we will be considering the following setting:\nTx = x\n(2.39)\nwhere T is some mapping from Rn into itself, and we want to find the fixed point.\nFurthermore, we will assume that we do not have access to the exact form of T, but\nrather we have access to a random variable s, via sampling or simulation:\ns = Tx+w\n(2.40)\nIn this case, w is some random noise term. The general form of estimators that we\nwill be studying is:\nxt+1 = (1−α)xt +α(Txt +w)\n(2.41)\nTo see that this allows for rather general and interesting problems, suppose we are\ninterested in:\nx = E\n\u0014\nf(x,z)\n\u0015\n(2.42)\nWhere the additional variable z is distributed as z ∼p(z|x). We will further assume\nthat f is known, but that the conditional distribution is not. The final assumption\nis that we have access to samples ˜z1,..., ˜zk. This is a rather general setting and can\nalso be easily applied in supervised learning settings by setting f to be the objective\nfunction and z to be the target. As k becomes large the sample mean:\n1\nk\nk\n∑\ni=1\nf(x, ˜zi)\n2.5. Stochastic Approximations\n27\nwill converge to the desired mean and we could apply deterministic algorithms to\nsolve for x, however, we will also explore alternatives, such as for example when\nk=1.\nxt+1 = (1−α)xt +α f(xt, ˜zt)\n(2.43)\nIn fact, this particular version of the algorithm is known as the Robbins-Monro\nstochastic approximation algorithm. We can rewrite (2.43) to recover our desired\nformat (2.41).\nxt+1 = (1−α)xt +α\n\u0014\nE\nh\nf(xt,z)\ni\n+ f(xt, ˜z)−E\nh\nf(xt,z)\ni\u0015\nWe can now set the operator T to be:\nTx = E\nh\nf(x,z)\ni\nand the noise w to be:\nw = f(x, ˜z)−E\nh\nf(x,z)\ni\nSo far we have not discussed the step-size (or alternatively interpolation parameter)\nα. In fact, α has to be iteration dependent as well, i.e. αt and furthermore the\nconditions for these algorithms to converge under various technical assumptions\nare known as the Robbins-Monro (RM) conditions.\n∞\n∑\nt=0\nαt = ∞\n(2.44)\n∞\n∑\nt=0\nα2\nt = 0\n(2.45)\nThe remaining part of this section will study variations and extensions of the gen-\neral above method. Concretely, it will be applied to the dynamic programming\nand reinforcement learning scenario. We notify the reader already that the con-\n2.5. Stochastic Approximations\n28\nvergence proofs of these algorithms are beyond the scope of the current work and\nrefer the reader for an overview to [Bertsekas and Tsitsiklis, 1996], Chapter 4 or\n[Szepesv´ari, 2010], Chapter 3.\n2.5.1\nValue Approximators\nIn this part, we will be looking at approximating the value function V π. This means\nthat we will be trying to develop methods that output an estimated value for every\nstate given some policy π. With an estimated version of the value function, we\ncan then apply the generalised policy iteration to obtain an improved policy. The\nconvergence of these methods is discussed in greater detail in [Szepesv´ari, 2010,\nBertsekas and Tsitsiklis, 1996, Sutton and Barto, 2018].\n2.5.1.1\nMonte Carlo - Methods\nMonte Carlo methods are designed for computing stochastic integrals such as ex-\npectations. We will consider these as a class of stochastic approximations as well\ndue to their conceptual and practical importance. They will also serve as a starting\npoint for the stochastic approximation methods based on Robbins-Monro schemes\ndiscussed in the previous section.\nStarting with the value function given a policy π.\nV π(x) = E\n\u0014 ∞\n∑\nt=1\nγtRt|x0 = x\n\u0015\nWe see that the random variable of interest is G = ∑∞\nt=1 γtRt.\nGiven a start-\ning state x0 = x and a policy π we can generate trajectories episodek =\n(xk\n0,ak\n0,xk\n1,Rk\n0,...,xk\nT,Rk\nT−1) in a finite-horizon MDP. We can use this trajectory\nto compute Gk(x) = ∑T−1\nt=0 γtRt. We can then construct a simple sample mean over\nK trajectories.\n˜V π\nK (x) = 1\nK\nK\n∑\nk=1\nGk(x)\n(2.46)\nThis forms an unbiased estimate of V π(x), which is easy to see, since all the\nGk are independent and finite and the expectation is a linear operator.\nFur-\n2.5. Stochastic Approximations\n29\nthermore, the standard deviation decreases as 1/\n√\nK by a similar argument.\nHence, by the laws of large numbers we are guaranteed the convergence to\nV π(x).\nIf we were to implement this naively and only use every episode for\na single state, we can clearly see that it would require many episodes to con-\nverge. This gives rise to the first visit Monte Carlo estimate of the value func-\ntion, which can update the value function for more than one state per episode.\nAlgorithm 2: First Visit Monte Carlo Estimate\nInput : Policy π, Simulator of MDP\nOutput: Vπ\nV(x) ←rand(R), ∀x ∈X ;\nReturns(x) ←emtpy list, ∀x ∈X ;\nwhile True do\nepisode = sample trajectory from simulator following π;\nG ←0;\nfor t ←T −1 to 0 do\nG ←γG+Rt;\nif xt ̸∈{x0,...,xt−1} then\nReturns.append(G) ;\nV(x) ←mean(Returns(x));\nend\nend\nend\nIn the best case scenario this algorithm generates one trajectory for each state per\nepisode, hence, again this could take a long time to converge. In fact it turns out that\nremoving the first visit condition, xt ̸∈{x0,...,xt−1}, also converges to the correct\nvalue function. This is not as trivial to see as the samples of the return are no longer\nindependent, for a discussion we refer to [Sutton and Barto, 2018].\nWe have outlined a Monte Carlo based method for evaluating a policy. Monte Carlo\nmethods, although with theoretical guarantees, sometimes take very long times to\nconverge. Furthermore, if the problem has an infinite time-horizon then Monte\nCarlo schemes become infeasible. This motivates an alternative approach.\n2.5.1.2\nTemporal Difference - TD(0)\nIn this section, we will describe a method that is called temporal difference. We will\nmotivate it using the Robbins-Monro and Monte Carlo method. As in the Monte\n2.5. Stochastic Approximations\n30\nCarlo setting, we are interested in estimating a value function given a policy π. Let\nus start with the Robins-Monro definition considered above (2.43).\nxt+1 = (1−α)xt +α f(xt, ˜zt)\nIn order to avoid confusion xt in the above equation is a placeholder for any value\nthat we are trying to estimate. Thus rewriting the xt in the above equation as Vt(s),\n∀s ∈X, we obtain:\nVt+1 = (1−α)Vt +α f(Vt, ˜zt)\n(2.47)\nNow, if we further define the sample z as zt = (at,Rt,st+1) and the function f as\nf(Vt,zt) = Rt +γVt(st+1). We arrive at:\nVt+1(st) = (1−α)Vt(st)+α\nh\nRt +γVt(st+1)\ni\n(2.48)\n= Vt(st)+α\nh\nRt +γVt(st+1)−Vt(st)\ni\n(2.49)\nThe last line (2.49), which follows simply by rearranging, is the famous temporal\ndifference update. Also, since we have derived it from the Robin-Monro method,\nthis gives us convergence for free given that α satisfies the RM-conditions (2.44)\nand (2.45). The difference term Rt +γVt(st+1)−Vt(st) is called the temporal differ-\nence error, hence the name.\nTo wrap TD(0) up we will refer the reader to [Bertsekas and Tsitsiklis, 1996], Chap-\nter 5 and Chapter 6 for an in-depth discussion and only note that TD(0) is a biased\nestimator with a lower variance than Monte Carlo.\n2.5.1.3\nTD(k) - a unifying view\nLooking at the difference between TD(0) and the Monte Carlo method we will\nbriefly show how they lie on opposite ends of a similar approach. Their difference\nlies in how long a single trajectory or data-point should be. The natural extension is\n2.5. Stochastic Approximations\n31\nto consider a method that sits somewhere in-between, i.e. using a k-step look-ahead:\nVt+1(st) = Vt(st)+α\n\u0014k−1\n∑\nl=0\nh\nRt+l +γVt(st+l+1)\ni\n−Vt(st)\n\u0015\n(2.50)\nThis formulation makes it apparent that both TD(0) and the Monte Carlo methods\nare on the different ends of this k-step look-ahead method, with k=0 for TD(0) and\nk=∞and α = 1 for Monte Carlo. The only question remaining is whether these k-\nstep look-ahead also converge to the value function. This is easy to see since again\nwe can use Robbins-Monro by arguing that this time the sample zt is a trajectory of\nlength k.\nNow, having discussed that Monte Carlo methods have high variance and are un-\nbiased, while TD(0) has lower variance yet higher bias, this leads us to consider\nan interpolation between any of these two methods ˜V = λTD(k1)+(1−λTD(k2).\nIn fact if we consider an interpolation between all k-step variations and let λk =\nλ k(1 −λ) for k = 0,1,2,... then we arrive at an algorithm called TD(λ), we refer\nthe reader to [Bertsekas and Tsitsiklis, 1996], Chapter 5. λ or k only influence the\nspeed of convergence not whether these methods will converge (at least in this par-\nticular setting). The final choice of λ depends on the problem. For a good list of\nexamples we refer the reader to [Sutton and Barto, 2018], Chapter 6.\n2.5.1.4\nSummary\nTD(λ) or TD(k) have been developed to approximate the value function. The reason\nfor the approximations were large state and action spaces and more importantly\nunknown environment dynamics. The TD(k) methods presented in this section are\nso-called tabular approximation algorithms, as they assign a distinct value for each\nstate. Therefore large state and action spaces would still be problematic. On the\nother hand, the estimates themselves are obtained without explicitly referring to the\nenvironment dynamics. However, ultimately we are interested in the optimal policy,\nnot just the value function. With an estimate of the value function V π we can use\nthe generalised policy iteration (Algorithm 1) to obtain the optimal policy. Looking\nclosely we observe that the generalised policy iteration requires the knowledge of\n2.5. Stochastic Approximations\n32\nthe environment, or more concretely of the transition probabilities. This means that\nin the more realistic setting of unknown or intractable environment dynamics we\nneed to develop additional methods. Therefore, we can conclude that TD(k) in its\nstandard form is not applicable in the model-free setting.\n2.5.2\nQ-learning\nQ-learning has gained great popularity and considerable success in its applica-\ntions [Mnih et al., 2015]. In this section, we want to investigate this Q-learning\nalgorithm, which nowadays more likely refers to a family of algorithms with var-\nious subtle differences. We hope to shed light upon a method that overcomes the\nshort-coming of TD(k), namely the necessity of knowing the model dynamics for\nthe policy update step. Furthermore, we will briefly discuss the extension of the\nQ-learning algorithm for the non-tabular case and therefore we will arrive at a\nmethod that overcomes all mentioned short-comings of exact dynamic program-\nming. Q-learning, among other interesting properties, works under the model-free\nsetting and most interestingly has convergence guarantees to the optimal policy\n[Watkins and Dayan, 1992], [Bertsekas and Tsitsiklis, 1996], Chapter 6. Its con-\nvergence can be showed using the Robbins-Monro argument in the tabular case and\nis still being studied for various forms of function approximators.\nBefore we begin describing the original Q-learning algorithm [Watkins and Dayan, 1992],\nwe will briefly mention some of the comparisons of the wider Q-learning approach\nto other methods. At its core, Q-learning is very similar to temporal difference\nmethods and relies on k-step trajectories and RM conditions. At the same time,\nQ-learning does not estimate the action-value function under a given policy Qπ,\nbut rather finds the optimal Q-function Q∗directly. The direct equivalent to TD\nestimation for the Q-function is Sarsa [Rummery and Niranjan, 1994].\n2.5. Stochastic Approximations\n33\n2.5.2.1\nTabular Q-learning\nThe original Q-learning algorithm [Watkins and Dayan, 1992] is based on a tabular\nrepresentation of both the state and action space and the update rule is given by:\nQt(xt,at) = Qk(xt,at)+α\nh\nRt +γ max\na∈A Qt(xt+1,a)−Qt(xt,at)\ni\n(2.51)\nWe can easily see that the Robbins-Monro argument is applicable by setting\nf(Qt,zt) = Rt +γ maxa∈A Qt(xt+1,a). The samples zt = (Rt,xt+1) can be collected\nfrom an arbitrary policy so long as each state and action is visited infinitely often\nfor full convergence, as is the case for all of these approximation schemes. This par-\nticular point will become more important when we are discussing the importance of\nexploration.\n2.5.2.2\nQ-learning with Function Approximators\nOnce we move away from the tabular case various assumptions that helped\nus with convergence guarantees start failing.\nTsitsiklis and Van Roy in\n[Tsitsiklis and Van Roy, 1997] provide a thorough overview. The main takeaways\nare as follows. Firstly, with linear function approximators and some technical as-\nsumptions on the MDP and approximation such as ergodicity, we are guaranteed\nconvergence in the on-policy case. However, in the case of off-line or non-linear\nfunction approximators [Tsitsiklis and Van Roy, 1997], sections IX and X, these\nmethods are prone to diverge. We refer the reader for a comprehensive overview in\nSutton and Barto [Sutton and Barto, 2018], Chapters 9-12.\nIn this section, we will outline Q-learning using parametrised non-linear func-\ntion approximators in an off-policy setting. As mentioned above these methods\nare highly prone to diverge and are to this date poorly understood from a theo-\nretical point of view. Therefore, we will merely outline the key ingredients that\nallow for non-linear, off policy Q-learning as is suggested empirically.\nDQN\n[Mnih et al., 2015] is a prominent example thereof.\nUpdate Rule under function approximators\nThe general principle under the function approximator does not change. This means\n2.5. Stochastic Approximations\n34\nthat one is still aiming to get closer to the k-step look-ahead target. Hence assuming\nany parametrisation with parameters θ the principle is in some sense to minimise:\nd\n\u0010\nQθt(xt,at),Rt +γ max\na Qθt(xt+1,a)\n\u0011\n(2.52)\nfor some distance measured, such that the converged Qθ∞is close under max-\nnorm to the optimal solution Q∗. It turns out that using the least-square distance\nfor d has some theoretical guarantees, especially in the linear approximation case\n[Tsitsiklis and Van Roy, 1997].\nDecorrelating Targets\nOne of the key reasons for divergence of the above method for non-linear and off-\npolicy methods is the correlation between the targets and the online approximator.\nTo overcome this, one introduces an additional function approximator that has the\nexact functional form of the final approximator. The parameters of this target ap-\nproximator are updated directly from the final approximator, although on a slower\ntime-scale [Sutton and Barto, 2018]. This results in the following update rule:\nθt+1 = argmin\nθt\n\u0010\n||Rt +γ max\na Qθ′(xt+1,a)−Qθt(xt,at)||2\n2\n\u0011\n(2.53)\nAt the same time updating θ ′ every T time-steps by setting it equal to θ. These\ntwo-timescale methods are known to have some additional convergence guarantees\n[Borkar, 1997].\nExperience Replay\nAnother way of solving the divergence of the off-policy method is by introducing\nexperience replay [Mnih et al., 2015]. The main idea of experience replay is to\ngather trajectories (xt,at,xt+1,rt+1) in the environment to build an array of such\nexperience data points and then to sample these past data points. It turns out that in\npractice one of the key components to robust convergence comes from experience\nreplay (ER).\nIn summary, we have seen that the Q-learning method provides a way of solving\nthe MDP problem without knowing the underlying dynamics, i.e. in the model-\n2.5. Stochastic Approximations\n35\nfree setting. In addition to this, we have briefly discussed how Q-learning can be\ndone using function approximators and observed that these methods are still poorly\nunderstood and are prone to diverge under the interesting scenarios of non-linear\nfunction approximators and the off-policy setting. We conclude that establishing a\nbetter theoretical understanding presents another valuable direction for future work.\n2.5.3\nActor Critic\nSo far all the methods we have looked at relied on the value function or its rela-\ntive the state-action value function. However, there is another family of algorithms,\nwhich are called policy gradient algorithms. These look to improve the policy func-\ntion directly without the value function [Williams, 1992]. The full scope of these\npolicy gradient methods is vast and probably worth another independent thorough\ninvestigation. In this section, we merely want to provide an overview that will be\nsufficient to develop a Bayesian adaptation of these.\nApart from intrinsic motivations for the policy gradient approach, there are three\nmotivations that we will briefly discuss. Firstly, the above-mentioned convergence\nproblems of non-linear, off-policy value approximators lend themselves to the ques-\ntion of how to improve the stability of these systems.\nIt turns out that policy\ngradient approaches can assist this as the policy gradient theorem demonstrates\n[Sutton and Barto, 2018], Chapter 12. Secondly, all the methods discussed for exact\nor approximate dynamic programming relied on the fact that maxa Q(x,a) can be\nsolved easily or at all, however, when the action space becomes continuous or high-\ndimensional this suddenly can turn into a non-trivial problem on its own. Thirdly\nand perhaps most interestingly, the state-action value approach is designed for the\nMDP setting, however, many problems do not fully adhere to the Markovian as-\nsumption. Using a policy function can lead to non-linear and non-Markovian be-\nhaviour and therefore overcome these problems. We will see an example of this\nwith the mountain-car problem in Chapter 4.\nIn this section, we will briefly describe the main ideas of policy gradient methods\nand then we will explain how these can be used in conjunction with value function\napproximators resulting in the family of methods called actor-critic. The policy\n2.5. Stochastic Approximations\n36\nfunction is referred to as actor and the value function is referred to as critic.\n2.5.3.1\nPolicy Gradient Methods\nThe general idea of policy gradient methods is to find the policy π that minimises:\nπ∗= argmin\nπ E\nh\nRt\ni\n(2.54)\nHence, this is a search in the space of all possible policies. Just to give a general\nintuition that this space is very large let us consider an MDP with a state-space of\nsize X and uniform action space of size A, then there are AX possible policies. I.e.\nit is the size of the full breadth-first search tree.\nTherefore the appropriate methods for this perspective of the problem are of an\napproximate nature and as the name suggest usually follow some sort of gradient\nstepping approach. Assuming that the policy is parametrised by θ the main idea of\nthese approaches lies in finding the gradient:\n∇θE\nh\nRt\ni\n(2.55)\nComputing this directly is near to impossible. This is where the policy gradient\ntheorem comes in [Szepesv´ari, 2010], Chapter 4. The policy gradient theorem states\nthat under various technical assumptions:\nG(θ) =\n\u0010\nQπθ (x,a)−h(x)\n\u0011 ∂\n∂θ logπθ(a|x)\n(2.56)\nis an unbiased estimate of (2.55), where h(x) is an arbitrary bounded function and\nusually serves the purpose of reducing the variance. A typical choice for h(x) is\nV π(x) which results in the advantage policy gradient method.\n2.5.3.2\nActor Critic\nIn this part we will briefly outline how the actor and critic can work together and\ntherefore one can obtain improved results.\nIn fact, exactly this interaction be-\ntween actor and critic was one of the factors that led to the success in Alpha Go\n2.6. Exploration vs. Exploitation\n37\n[Silver et al., 2016]. Starting with the unbiased estimate of the policy gradient:\nG(θ) =\n\u0010\nQπθ (x,a)−h(x)\n\u0011 ∂\n∂θ logπθ(a|x)\n(2.57)\nwe can see that we need estimates of the state value function for this method to work\nin practice, hence we obtain the gradient that can be used to update the parameters\nof the actor πθ based on an estimate of the critic ˜Qπθ\nψ :\nG(θ) =\n\u0010\n˜Qπθ\nψ (x,a)−h(x)\n\u0011 ∂\n∂θ logπθ(a|x)\n(2.58)\nWhile the critic is updated using the methods developed in the previous section. Ex-\nactly this inter-play in updates leads to methods called actor-critic. In this particular\nadaptation, ˜Qπθ\nψ would have to be updated using the on-line version of Q-learning,\nSarsa. Such two-timescale algorithms are furhter studied in [Borkar, 1997] and\nshow promising directions for future work.\n2.5.4\nSummary\nIn this section, we have given an overview of the main approximation techniques\nto exact dynamic programming. These methods are both the foundation of modern\nstate-of-the-art approaches as well as the current state of theoretical foundations.\nThe full analysis of convergence results, conditions and speeds is an interesting\nstudy on its own and I hope that more work in this direction will be done by the\ncommunity.\n2.6\nExploration vs. Exploitation\nExploration vs. Exploitation is a very fundamental problem in Reinforcement learn-\ning especially more so in real-world scenarios, where someone might be more in-\nterested in finite time horizons as opposed to asymptotically optimal solutions. Fur-\nthermore, there is even more importance to this topic when considering the approx-\nimate methods discussed in the previous section, as they rely on exploring the state\nand action space exhaustively.\nWe will endeavour to give a short overview of the exploration vs. exploitation\n2.6. Exploration vs. Exploitation\n38\nproblem in general and a short overview of various methods to address this issue.\nFinally, we will briefly remark that on the theory side there was significant work\nproduced by Bellman, Gittins et. al. on the topic of optimal exploration, however,\nthese methods suffer even worse from the ”curse of dimensionality“ than the exact\ndynamic programming algorithms.\n2.6.1\nImportance of Exploration in Approximate Methods\nExact Dynamic Programming discussed in the first part of this Chapter presented\nvarious algorithms, which can all be viewed as special instances of the generalised\npolicy iteration algorithm. We have presented a small proof that convergence can\nbe guaranteed in the finite case rather simply by a contraction mapping argument.\nIn more general settings we have referred to [Bertsekas and Tsitsiklis, 1996] it turns\nout that some of the key technical conditions for convergence of these methods are\nthe ergodicity of the Markov Decision Process. This is required so that all states are\nvisited sufficiently often. This is exactly the reason why exploration is so important\nat arriving at an optimal policy or near-optimal solution.\n2.6.2\nVarious Methods of Exploration\nIn this section, we will give a short overview of the most prominent methods of\nexploration.\nε-greedy\nThe ε-greedy method relies on creating trajectories by not only sampling the greedy\npolicy with respect to the Q-function, but acting completely randomly ε amount of\nthe time. In some adaptations, one makes sure that epsilon decreases over time.\nThese methods are perhaps the most widely used in practical RL, however, there\nare countless examples where ε-greedy can lead to near worst-case performance\n[Sutton and Barto, 2018, Osband et al., 2017].\nUpper Confidence Bounds (UCB)\nThis method aims at promoting exploration by introducing pseudo-counts. In par-\n2.6. Exploration vs. Exploitation\n39\nticular, the action is selected according to:\nat = argmax\na\n\"\nQ(x,a)+\ns\nlog(t)\nNt(x,a)\n#\n(2.59)\nWhere Nt(x,a) represents the number of times action a was selected in state\nx.\nThis method has been often shown to perform quite well in practice\n[Sutton and Barto, 2018], however, keeping track of Nt(x,a) can become very diffi-\ncult and it is lacking some stronger theoretical results.\nOptimistic Value Initialisation\nOptimisitc Value initialisation is based on its description. The core idea is to ini-\ntialise the estimates of Q(x,a) to be very large at the very beginning, so that every\nstate is over-estimated and therefore encourages exploration.\nThompson Sampling\nThompson sampling [Thompson, 1933] is a Bayesian approach that requires uncer-\ntainty estimates in the Q-function. Given the full Bayesian treatment the action is\nchosen according to:\nat = argmin\na ( ˜q(x,a))\n(2.60)\nwhere ˜q is sampled from the posterior state-action value function, ˜q ∼˜Qt(x,a).\n2.6.3\nOptimal Learning - the solution to exploration\nThis part aims to briefly give an overview of what optimal learning is. The main\nidea of optimal learning lies in taking a fully Bayesian approach on dynamic pro-\ngramming and considering an object called the information state. The information\nstate consists of the usual ”physical“ state from the state space as well as the\nBayesian parameters of the priors. The new transition model now incorporates\nthe effect of information gain or the update to these Bayesian parameters with\neach action and state observed. To give an example consider a 2-armed Bernoulli\nBandit problem with two Beta distributions with parameters (α1,β1) and (α2,β2)\nas priors for each arm respectively. The physical state, in this case, is fixed and\n2.6. Exploration vs. Exploitation\n40\ncan be represented as x=0 without loss of generality.\nThe information state is\nrepresented as (x,α1,β1,α2,β2). Now, the agent has two actions available, the\nagent can choose arm one or arm two and for each choice, there are two outcomes\neach, either success or failure. Therefore an example transition of the information\nstate starting at (x = 0,α1 = 1,β1 = 1,α2 = 1,β2 = 1) and the agent picking ac-\ntion one and receiving the success signal results in the new information state of\n(x = 0,α1 = 1+1,β1 = 1,α2 = 1,β2 = 1). At each stage, the agent keeps track of\nthe own reward probabilities and uncertainties thereof resulting even for a 2-armed\nBernoulli bandit in an infinite-horizon MDP. It is clear that solving this MDP would\nresult in an optimal path of exploring the two actions given the information that the\nagent has. However, it is also evident that this is not tractable in any reasonable\nscenario as even for bandit problems the information state grows exponentially with\nevery arm added.\nAs a final note, it is somewhat surprising that optimal learning is very much\nuntreated by the recent mainstream reinforcement learning literature includ-\ning seminal works and exhaustive summaries such as [Sutton and Barto, 2018,\nBertsekas and Tsitsiklis, 1996, Mnih et al., 2015], perhaps the most recent work on\noptimal learning is by Powell et. al. (2012) [Powell and Ryzhov, 2012]. In the\nspirit of approximations of MDPs and exact dynamic programming, we conclude\nthat optimal learning should be explored further in a similar manner.\nChapter 3\nDesign, Method and Experiment\nThe background and methods for reinforcement learning were discussed in Chapter\n2. We have discussed exact dynamic programming, approximate methods thereof,\nin particular, those that apply in the model-free setting, which is significantly more\ncommon and more useful in the real world. In this chapter, we will be presenting\nour study and contributions with the actual empirical results following in the next\nChapter. The theoretical foundation from Chapter 2 serves us to derive a novel theo-\nretical algorithm that combines the actor-critic approach and Bayesian exploration.\nWe empirically evaluate the benefit of the actor-critic (or two-timescale) approach\nand Bayesian exploration individually on a variety of standard benchmarks as well\nas a state-of-the-art evaluation benchmark. Furthermore, we will outline our soft-\nware packages and implementations, and our contribution in this respect is two-fold:\nFirstly, we are open-sourcing a framework for building and testing arbitrary RL al-\ngorithms and provide various in-built environments and models. Secondly, we have\nimplemented a state-of-the-art Bayesian model using accelerated code that allows\nthe usage of GPUs and other similar hardware accelerators. This implementation\nallows for much larger state and action spaces. We are fully open-sourcing all im-\nplementations and making it available under a permissive license.\n3.1\nAlgorithmic Contributions\nOur main algorithmic contribution lies in discussing and proposing the Bayesian\nactor-critic approach as well as the motivating factors. We also briefly discuss an-\n3.1. Algorithmic Contributions\n42\nother method that we propose. We call the second method Frequentist Thompson\nDQN, which is a direct adaptation of [Azizzadenesheli et al., 2018] with a frequen-\ntist estimate of uncertainty. This method is merely briefly outlined as it is a potential\nalternative to Bayesian Thompson sampling.\n3.1.1\nBayesian Actor Critic\nWe now present the Bayesian Actor-Critic Method. In Chapter 2 we have seen\nthat the generalised value iteration algorithm with function approximators, i.e. Q-\nlearning can be extended to incorporate another type of learning, namely policy\nfunction learning. This, as was shown in [Bertsekas and Tsitsiklis, 1996], leads to\nexpedited convergence of the algorithms. Together with the performance guarantees\nand empirical evidence for Thompson sampling, this motivates the Bayesian actor-\ncritic method.\n3.1.1.1\nAlgorithm\nThe main idea of the algorithm lies in two aspects, firstly, maintaining a full\nBayesian Q-function with uncertainty estimates, secondly, using a modified ver-\nsion of Thompson sampling together with upper confidence bounds, which I call\nBayesian UCB, to pick the action and therefore the exploration trajectories.\nThe inspiration for the modified upper confidence bounds comes from AlphaGo\nZero [Silver et al., 2017]. In their work they pick the action according to at =\nargmaxa(Q(x,a) +\nP(x,a)\n1+N(x,a)), where P(x,a) is given by the policy network and\nN(x,a) are counts. Naturally, this applies well only when Q(x,a) is rescaled to be\nwithin [0,1], which can be accomplished by normalising the rewards of the envi-\nronment by Rmax. The proposed adaptation to incorporate Thompson sampling is\ngiven by: at = argmaxa( ˜q(x,a) + P(x,a)), where we get rid of the counts N(x,a)\nas they might be difficult to keep track of and we sample ˜q ∼Q from the posterior.\n3.2. Empirical Study\n43\nAlgorithm 3: Bayesian Actor Critic\nInput : Simulator of MDP, BatchSize, PolicyUpdateT, TargetUpdateT\nOutput: Vπ\nθ ←rand() ;\nθ ′ ←rand() ;\nψ ←rand() ;\nReplayBuffer ←emptyList ;\nwhile True do\nxt+1,Rt,at+1 ←Simulator.act(ThompsonUCB) ;\nReplayBuffer.append(xt,at,xt+1,Rt,at+1) ;\nif ReplayBuffer.size() > BatchSize then\nbatch ←ReplayBuffer.sample(BatchSize);\nθ ←BayesianPosteriorUpdate\n\u0010\nRt +γQθ′(xt+1,at+1),Qθ(xt,at)\n\u0011\n;\nif t mod PolicyUpdateT==0 then\nψ ←\n\u0010\nQθ(xt,at)−Qθ(xt,πψ(xt))\n\u0011\n∇ψlogπψ(a|x);\nend\nif t mod TargetUpdateT==0 then\nθ ′ ←θ;\nend\nend\nend\n3.1.2\nFrequentist Thompson DQN\nBDQN[Azizzadenesheli et al., 2018] relies on a neural network as a feature map\nfrom the state representation and computes the action value function by solving a\nBayesian linear regression problem. In our method the idea is to replace the poste-\nrior update of the regression step, for computational speed, with a MLE estimate of\nboth the mean and covariance. The policy follows a Thompson sampling approach\nvery akin to the one described in the paper. This method was constructed to poten-\ntially serve the validation of Thompson sampling even without proper uncertainty\nestimates. As, we will see in the Analysis Chapter, however, this method, although\nit manages to learn, does not perform very well.\n3.2\nEmpirical Study\nIn this work, we conduct two empirical investigations. On one hand, we investigate\nbandit problems and study the ability of a variety of approaches to explore and find\n3.2. Empirical Study\n44\nthe correct action. On the other hand, we build upon the state of the art evaluation\nsuite bsuite [Osband et al., 2019] released by Google’s research group Deepmind.\nThis evaluation is carefully designed to probe a variety of skills of a given algorithm.\nIn total we ran over 25 different experiments each over 20 runs for 5 different agents\nfor bsuite, this resulted in a total of 2500 trained agents. This was only possible by\nusing optimised code and profiling the packages. The code is fully open-source1\n3.2.1\nBandit Exploration\nIn the bandit setting, we try to understand the agent’s ability to find the optimal\naction in a minimal number of steps. We focused on comparing two standard fre-\nquentist approaches vs. Thompson sampling. Furthermore, we compare the agents\nin different settings of the bandit problem. We vary the noise that the environment\nhas, as well as how distinguishable the various reward signals are. This distin-\nguishability is measured by the L2 distance, with higher Euclidean distance leading\nto higher distinguishability.\nEnvironment\nWe use a k-armed gaussian bandit. This means that the reward for arm i is generated\nby an independent normal with mean µi and variance σ2\ni .\nAgents\n1. ε-greedy agent with MLE estimates of the mean using a Gaussian environ-\nment model.\n2. Optimistic value initialisation with greedy actions and MLE estimate of the\nmean using a Gaussian environment model.\n3. Thompson sampling with Gaussian environment model and exact posterior\nupdates.\nThis analysis gives a small insight into the efficiency benefit that uncertainty es-\ntimates provide and therefore motivates the usage of such uncertainty estimates\nprovided by Bayesian methods in other scenarios.\n1https://github.com/ai-nikolai/bsuite\n&\nhttps://github.com/\nai-nikolai/barl\n3.3. Software\n45\n3.2.2\nState-of-the-art Agent Behaviour Suite\nDecision making agents need to perform well in a variety of tasks. Generalisation,\nmemorisation, exploration, noise, scaling, long-term planning etc. Most real-world\napplications have a combination of all of these and more distinct problems that\nan agent needs to solve. A long-standing problem in reinforcement learning re-\nsearch was the ability to compare and assess the agents on all of these individually.\nGoogle’s Deepmind endeavours to overcome this by providing a general sandbox\nenvironment where researchers can test their hypotheses. Therefore, for the purpose\nof this research, we have chosen to use this framework and extended it to compare\nour use-cases and algorithms.\n3.3\nSoftware\n3.3.1\nBARL - Framework\nBARL, short for Bayesian Approximate RL, is a framework specifically developed\nfor this thesis and the future work intended by it. This framework consists of three\ncore building blocks. Agents, Estimators and Environments. These are augmented\nwith a variety of auxiliary tools for running simulations, plotting and profiling. This\ncode was profiled to allow faster experimentation.\nAgents\nAgents form the general design of an agent, i.e. whether to have experience replay,\nwhether to act greedily, randomly or in some other manner, whether to update every\ntime-step in an online fashion or sample experience replay batches and update in an\noffline manner, etc. These agents, however, are independent of the estimators and\nfunction approximators that they are using.\nEstimators\nThis part of the package is designed to allow for custom estimators and function\napproximators, such as Bayesian neural networks, Gaussian Processes or Recurrent\nNeural Networks [Hochreiter and Schmidhuber, 1997].\nEnvironments\nThe package allows for extensibility on the environment side, i.e. it provides an\n3.4. Summary\n46\nabstract class that allows to build and add additional environments.\nUtility\nFinally, the package also provides a variety of utility functions. It allows for simpli-\nfied access to plotting and running agents and environments by wrapping them as\nsimulations.\n3.3.2\nBsuite Extensions\nBsuite is a rather novel framework that is not a Google product and therefore needs\nand allows for a lot of customisation. Our main contribution to the package are the\nfollowing:\n1. Tensorflow 2.0 implementation of DQN for increased speed-up.\n2. Tensorflow and Tensorflow-Probability implementation of a BDQN variation\nusing MLE updates as opposed to the exact posterior for significant speedup\non accelerated hardware.\n3.4\nSummary\nThe methods described and presented in this Chapter follow the general agenda\nof this work on providing motivation and foundation for applicable reinforcement\nlearning. In particular, work that is motivated and grounded in theory of approxi-\nmate dynamic programming, that allows for large scale state and action spaces with\nunknown dynamics and that can become applicable in the real world by having\nincreased data-efficiency and good exploration capabilities.\nIn summary, we have presented a novel algorithm as an extension of the actor-\ncritic approximate dynamic programming method and Thompson sampling.\nWe motivated it by the two-timescale approximation guarantees discussed in\n[Bertsekas and Tsitsiklis, 1996].\nWe then outlined the empirical setup for vali-\ndating the survey of ideas and methods presented in this work. Finally, we outlined\nbriefly the software that is accompanying this work.\nChapter 4\nAnalysis\nThe work presented so far went from motivating examples of state-of-the-art ap-\nproaches to the theoretical foundation of these in exact dynamic programming as\nwell as approximations thereof. We then further discussed the importance of ex-\nploration and exploitation. All of this serves as a survey of reinforcement learning\nfrom the perspective of approximate dynamic programming as well as a foundation\nfor future work into real-world applicable reinforcement learning.\nThe main theoretical method presented in the previous Chapter expresses some of\nthe directions that our survey suggests are worthy of exploration, namely actor-\ncritic methods as well as Thompson sampling. In this chapter, we provide some\nresults that support this claim. In particular, we focus our analysis on exploration\nvs. exploitation in a variety of scenarios, as well as on the analysis of the actor-critic\nmethod. We conducted these studies in the spirit of ablation studies and therefore\ncompare these two directions against a strong deep q-learning baseline, which hat\nneither Bayesian exploration nor the actor-critic approach. We also show an analy-\nsis of the data-efficiency of Thompson sampling in simple environments.\n4.1\nExploration in Bandits\nThis experiment is aimed at comparing exploration methods in the k-armed bandit\nsetting as outlined in the previous chapter. We ran the experiment with random\ninitialisation of noise and reward means. We further compared the effectiveness\nof three algorithms: Optimistic Value Initialisation, ε-greedy and Thompson sam-\n4.1. Exploration in Bandits\n48\npling. We run these for 300 time-steps for 50 runs and compare them to the best\npossible action and a random baseline.\nFigure 4.1: Reward per time-step in a 12 armed Bandit over 50 runs - without random\nbaseline\nFigure 4.2: Reward per time-step in a 12 armed Bandit over 50 runs - without optimistic\nbaseline\nIn figures 4.1, 4.2, 4.3 and table 4.1 we can see that that Thompson sampling is the\nclear winner among these exploration techniques, also, we can see that compared to\nthe ε-greedy approach it performs in a significantly more stable way with a standard\n4.2. Agent Behaviour Suite Results\n49\nFigure 4.3: Reward per time-step in a 12 armed Bandit over 50 runs - all agents\nName\nMean Reward\nStd\nε-greedy\n569.4271\n58.3278\nOptimistic\n672.1069\n2.2556\nThompson\n833.2319\n4.3487\nBest\n866.5053\n1.5431\nRandom\n115.3462\n20.7789\nTable 4.1: Bandit Exploration in a random 12 armed bandit over 50 runs.\ndeviation of only 4. Another observation that we can make is that the Optimistic\napproach is very erratic (cf. figure 4.1). This behaviour is expected and arises from\nthe fact that after the latest k-1 observations in a k-armed bandit the worst action\nwill be picked with a very high probability. To conclude the Bandit section, we will\nmerely say that as expected and suggested by the literature Thompson sampling\nindeed performs better in terms of data efficiency and final result.\n4.2\nAgent Behaviour Suite Results\nThis section presents the main empirical validation of the ideas presented in this\nwork. In general, we will be discussing three scenarios in more detail that are\naimed at showing different parts of importance: Thompson sampling being able\nto extend to MDPs as a data-efficient method, effectiveness of the actor-critic\nmethod in environments that likely violate full Markovian assumptions and the ne-\n4.2. Agent Behaviour Suite Results\n50\ncessity of advanced exploration methods in complicated environments. We also\ndiscuss the overall performance and observations of three agents DQN, Bayesian\nDQN [Osband et al., 2017] and the advantage actor-critic (A2C). In order to anal-\nyse this we have trained a vanilla deep q-network with experience replay fol-\nlowing many of the suggestions from the original and seminal work on DQNs\n[Mnih et al., 2015], our frequentist adapted version of BDQN (Frequentist Thomp-\nson DQN) [Azizzadenesheli et al., 2018], the full Bayesian Deep Q learning from\nOsband et. al. [Osband et al., 2017] and finally a vanilla actor-critic algorithm. We\ntrained and evaluated these on 25 environments from bsuite [Osband et al., 2019]\nwith over 20 runs for each. The data for the three in-depth experiments is in Ap-\npendix B.\n4.2.1\nA standard Problem\nCartpole [Barto et al., 1983] is perhaps the ”Hello World” of reinforcement learning\nin MDPs. The problem description is rather simple: control a simulated cart so that\nthe pole on top of the cart does not fall. The environment is represented by the speed\nof the cart and the angle of the pole. If the pole tips more than 15 degrees, the cart\nis 2.5 units away from the centre or 1000 time-steps passed the episode finishes.\nThe agent controls a discrete force of +1 or -1 exerted on the cart and receives a\nreward of +1 for every time-step that the episode did not finish. Hence, the optimal\nperformance that one can achieve is a score of 1001 per episode.\nname\nsteps\nepisode\ntotal return\nep. len\nep. return\nraw return\nbest episode\nBootDQN\n9989\n70\n9920.0\n86\n85.0\n9920.0\n1001.0\nDQN\n34463\n80\n34384.0\n622\n621.0\n34384.0\n1001.0\nFreq.BDQN\n51510\n1000\n50510.0\n268\n267.0\n50510.0\n595.0\nA2C\n53754\n300\n53455.0\n178\n177.0\n53455.0\n1001.0\nTable 4.2: Comparison of Cart-pole environment with four agents\nTable 4.2 serves to illustrate when the respective Agents first reached the maximal\nepisode length of 1000. We see a significant benefit of using Thompson sampling\nby the BootDQN over all other methods. Similarly, we see that in this classical\ncontrol problem DQN performs very well. Finally, we can also observe that the\n4.2. Agent Behaviour Suite Results\n51\nfrequentist Thompson sampling failed to learn to balance the pole. I believe that\nthis is due to lack of hyper-parameter tuning. However, this is also a clear indicator\nthat Thompson sampling without proper uncertainty tracking does not offer a robust\nmethod.\n4.2.2\nDeriving non-linear policies\nMountain car [Moore, 1990] is a problem that is often referred to as requiring mem-\norisation [Brockman et al., 2016]. What this means is that the agent needs to re-\nmember past actions and learn to create plans over longer periods of time. The\nproblem itself is to move a car along a 1-D path that represents a valley between\ntwo mountains. The car itself is not strong enough to make the journey up in one\nattempt, therefore, the solution is to swing the car there and back to build up momen-\ntum. The agent controls whether the car moves forward or backwards and receives\nnegative rewards at every time-step that the car has not reached the top. Table 4.3\nname\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\nBootDQN\n39884\n1000\n-39884.0\n42\n-42.0\n-39884.0\nDQN\n50901\n1000\n-50901.0\n37\n-37.0\n-50901.0\nFreq.BDQN\n516077\n1000\n-516077.0\n41\n-41.0\n-516077.0\nA2C\n33735\n1000\n-33735.0\n30\n-30.0\n-33735.0\nTable 4.3: Comparison of Mountain-car environment with 4 agents\nserves to demonstrate which agent struggled the most with this example. This is\nrepresented by the total return given the same number of episodes. Interestingly\nenough the performance of the actor-critic algorithm shows the strongest perfor-\nmance by being able to reach the top of the mountain in the fewest steps on average\nover 1000 episodes. There is a 25% drop in performance of the runner up, Bayesian\nDQN and 40% concerning the vanilla DQN. The frequentist BDQN again performs\nworst by a huge margin. This simple experiment shows promising results in the in-\ntuition that policy gradient-based methods provide and therefore confirms that this\nis a valid direction for further investigation.\n4.2. Agent Behaviour Suite Results\n52\n4.2.3\nExploring long-term strategies with sparse rewards\nDeepsea [Osband et al., 2017] is an environment that is specifically designed to test\nthe agent’s ability for long term exploration. The environment is a conceptual 2-D\ngrid where the agent can either move down-left or down-right. The starting position\nof the agent is in the top left corner and the only positive reward that the agent re-\nceives is by always following down-right. Therefore, a naive exploration policy is\nbound to have an exploration length of 2N, where N represents the size of the world.\nIn table 4.4 we are comparing DQN, Boot DQN and A2C. All agents were trained\nname\nsteps\nep.\nttl. ret\nep. len\nep. return\nttl. bad ep.\ndenoised ret\nA2C\n100000\n10000\n-9.0830\n10\n-0.0030\n10000\n0\nDQN\n100000\n10000\n-20.8227\n10\n-0.0011\n10000\n0\nBoot\n100000\n10000\n9731.7109\n10\n0.9900\n169\n9831.0\nTable 4.4: Comparison of Deep sea environment with 3 agents\nin an environment with a world size of N=10, the salient feature is ”ttl. ret”, which\nis the total return. Boot DQN significantly outperforms its peers and is the only\nagent that solves the problem. The other interesting observation is that again A2C\nachieves a better performance in a task that potentially requires longer-term plan-\nning than its value-based counter-part DQN. In this experiment, we are not includ-\ning frequentist BDQN as it was again strongly under-performing.\n4.2.4\nOverall results\nSo far we have compared four agents on three specific tasks. The results were\npromising with respect to the claims made in this work. In particular, the results sug-\ngested so far that Thompson sampling indeed allows for increased exploration and\ndata-efficiency and that the actor-critic method allows for non-linear policies and\nlong-term planning. However, whether this holds in general and across a wider vari-\nety of tasks was not yet shown. Hence, we also present a comparison of actor-critic,\nboot DQN and DQN across the whole set of 25 tasks in bsuite [Osband et al., 2019].\nIn figure 4.4 we can clearly see that on the basic, noise, scale and credit assignment\ntasks all three agents perform relatively similar. The significant differences come\nfrom the exploration tasks and the ”memory” tasks.\n4.2. Agent Behaviour Suite Results\n53\nFigure 4.4: Comparison of three agents across all 25 tasks.\nExploration\nBoot DQN, which is based on Thompson sampling performs consistently well on\nthe exploration tasks and, in fact, is the only agent that manages to solve these\ntasks. These tasks require multi-step action taking and have long event horizons.\nThis means that the Thompson sampling method, which was designed for bandit\nproblems in its original form, seems to generalise across to MDPs. At the same\ntime, Thompson sampling, in this particular set-up, does not allow for the solution\nof tasks that requires memory and non-Markovian planning.\nLong Term Planning - Memory\nThe only agent that manages to out-perform the random baseline in the memory\ntasks is an actor-critic method. This falls in-line with the arguments that we have\nbeen putting forward and is encouraging for our theoretical proposal. On a final\nnote, it remains to be seen what precisely makes policy gradient methods and actor-\ncritic methods perform better at such tasks and how the Bayesian perspective can\nfully utilise it.\n4.3. Error Analysis\n54\n4.3\nError Analysis\nApart from the strong suggestions and validation of our ideas, there are some short-\ncomings and some error analysis that we will briefly discuss. Firstly, the frequentist\nThompson DQN is not optimised for hyper-parameters as this would have been too\ncomputationally expensive. Hence, it is not fully clear whether full Bayesian uncer-\ntainty tracking is necessary. Similarly, it was not possible to replicate the original\nBDQN paper [Azizzadenesheli et al., 2018], which also relies on Bayesian Thomp-\nson sampling, and therefore it remains to be seen whether Thompson sampling truly\ngeneralises across methods. Lastly, even if all the claims in this analysis section\nturn out to be true, it still remains to see whether the combination of actor-critic and\nBayesian Thompson sampling indeed has the promised performance gain.\n4.4\nConclusion\nIn this chapter, we have presented empirical results on exploration and on conver-\ngence speed comparisons of the value-based approximation schemes vs. the actor-\ncritic based approximation scheme. These experiments were made with repeated\ntries and in a variety of environments. The results show strong support for Bayesian\nexploration and actor-critic methods reinforcing the Bayesian actor-critic method\nproposed in the previous chapter.\nChapter 5\nConclusion & Outlook\nThis work’s main objective was to contribute to the field of sequential decision mak-\ning and in particular their application to the real world. This field is very large in\nsize and therefore the concrete steps for this higher-level objective were to provide\na survey of both the theoretical foundation of sequential decision making, as well\nas to provide an overview of the state-of-the-art of this highly varied, fast-changing\nand rich field. These theoretical foundations have been used to motivate existing\nmethods as well as a new promising direction of algorithms and future work. In\nparticular, the emphasis was laid on exploration in reinforcement learning and the\nactor-critic method for approximating exact dynamic programming. These aspects\nof approximate dynamic programming were then analysed empirically and showed\npromising results.\nConcretely, we have seen that keeping track of the uncertainty in the state-\naction value function, together with Thompson sampling, leads to increased data-\nefficiency and sometimes is the only feasible way of solving the problem. We\nhave observed that the actor-critic algorithm also performs increasingly efficiently\nin comparison to the standard, yet state-of-the-art, deep q-learning. Finally, we have\npresented software that was designed and tested for hardware-accelerated devices\nand allows not only replication of these results but also facilitates future work.\nThe final objective of this work was to lay a foundation for a deep dive into rein-\nforcement learning - such as a contemporary doctoral degree on the topic of sequen-\ntial decision making. I believe that this has been accomplished, as an overview of\n5.1. Outlook\n56\nthe past, the present and the tools of this field have been presented analysed and\nused. To conclude, some of the future outlook based on this work and beyond will\nbe discussed.\n5.1\nOutlook\nThis work has presented the actor-critic family of algorithms and their adaptation\nto Thompson sampling under Bayesian uncertainty tracking. Due to a variety of\nlimitations in computation available and the vast field of exploration, it was not\npossible to empirically validate this method. Hence, this is the most natural next\nstepping stone from this work. Another direction that needs more validation is\nwhether Bayesian neural networks are indeed important for these methods to work\nor whether alternative Bayesian methods can be even better adapted for this task.\nThe exploration of frequentist uncertainty estimates, as a way of speeding up com-\nputation, also needs to be explored further in the context of reinforcement learning.\nOn the conceptual side of things, a proper exploration of convergence guarantees\n(or lack thereof) of non-linear function approximators for TD based methods is out-\nstanding in this field. Finally, the software developed for this work has a lot of room\nfor improvement and contributions. The software has the potential to facilitate re-\nsearch for many parties and therefore is worth pursuing further.\n5.1.1\nFrontiers\nApart from the core ideas presented in this work, some peripheral ideas have been\nmentioned and touched upon. Perhaps the most significant and interesting are:\nCuriosity - or mixed model-based and free RL:\nOpen AI’s work on curiosity-driven exploration is fascinating. Since the agents\nlearn to solve so many tasks without ever being given the true reward signal. The\nunderlying principle of this curiosity-driven exploration is by learning to predict\nthe environment and exploring those parts of it where the prediction is the weakest.\nThus it could be argued that curiosity-driven RL is model-based RL, where the\nmodel is being learned. The interesting question that can be posed here is how\ncan the exploration methods and convergence methods described and motivated\n5.1. Outlook\n57\nby the theory presented in this work be joined with the curiosity-driven approach.\nApproximate Optimal Learning:\nOptimal learning is potentially the most overlooked aspect of optimal control and\nsequential decision making in our current research community. Even exhaustive\nsummaries such as Sutton’s [Sutton and Barto, 2018] mention optimal learning\nonly in the passing. Naturally, exact optimal learning is far from being tractable. At\nthe same time, exact vanilla dynamic programming is not tractable either. Hence,\nthe natural question that arises is, how can optimal learning be effectively approx-\nimated.\nThis question has potentially a link to the frequentist way of tracking\nuncertainty in reinforcement learning.\nMulti-agent systems:\nAs a final frontier, many questions lie, of course, in the field of multi-agent learning.\nMulti-agent RL is still a widely unsolved and untapped field and I am personally\nlooking forward to seeing this field grow into something more mature, as we have\nseen with reinforcement learning itself.\nAppendix A\nBellman Optimality Operator is a\nContraction Mapping\nLet us proof that the Bellman Optimality Operator defined in equation (2.16) is\nindeed a contraction mapping on V.\n||(T ∗V)(x)−(T ∗V ′)(x)||∞=\n(A.1)\n=\nmax\nx∈X\n(\nmax\na∈A\n\u001a\nr(x,a)+γ ∑\nx1∈X\np(x1|x,a)V(x1)\n\u001b\n−max\na∈A\n\u001a\nr(x,a)+γ ∑\nx1∈X\np(x1|x,a)V ′(x1)\n\u001b)\n(A.2)\n≤\nmax\na∈A,x∈X\n\u001a\n\u0018\u0018\u0018\u0018\nr(x,a)+γ ∑\nx1∈X\np(x1|x,a)V(x1)\n−\u0018\u0018\u0018\u0018\nr(x,a)+γ ∑\nx1∈X\np(x1|x,a)V ′(x1)\n\u001b\n(A.3)\n=\nγ\nmax\na∈A,x∈X\n\u001a\n∑\nx1∈X\np(x1|x,a)V(x1)−∑\nx1∈X\np(x1|x,a)V ′(x1)\n\u001b\n(A.4)\n=\nγ\nmax\na∈A,x∈X\n\u001a\n∑\nx1∈X\np(x1|x,a)\n\u0010\nV(x1)−V ′(x1)\n\u0011\u001b\n(A.5)\n≤\nγ\nmax\na∈A,x∈X\n\u001a\n∑\nx1∈X\n\u0014\np(x1|x,a)max\nx′∈X\n\b\nV(x′)−V ′(x′)\n\t\u0015\u001b\n(A.6)\n=\nγ\nmax\na∈A,x∈X\n\u001a\n∑\nx1∈X\np(x1|x,a)\n\u001b\n||V −V ′||∞\n(A.7)\n=\nγ||V −V ′||∞\n(A.8)\n59\nFor 0 < γ < 1 we see that T is indeed a contraction mapping, this concludes the\nproof.\n□\nAppendix B\nBsuite Data\nHere we are attaching some of the results from the experiments:\n61\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\nbest episode\n29\n1\n28.0\n29\n28.0\n28.0\n28.0\n57\n2\n55.0\n28\n27.0\n55.0\n28.0\n85\n3\n82.0\n28\n27.0\n82.0\n28.0\n115\n4\n111.0\n30\n29.0\n111.0\n29.0\n144\n5\n139.0\n29\n28.0\n139.0\n29.0\n173\n6\n167.0\n29\n28.0\n167.0\n29.0\n202\n7\n195.0\n29\n28.0\n195.0\n29.0\n233\n8\n225.0\n31\n30.0\n225.0\n30.0\n262\n9\n253.0\n29\n28.0\n253.0\n30.0\n291\n10\n281.0\n29\n28.0\n281.0\n30.0\n350\n12\n338.0\n30\n29.0\n338.0\n30.0\n407\n14\n393.0\n29\n28.0\n393.0\n30.0\n494\n17\n477.0\n28\n27.0\n477.0\n30.0\n582\n20\n562.0\n28\n27.0\n562.0\n30.0\n731\n25\n706.0\n30\n29.0\n706.0\n30.0\n876\n30\n846.0\n29\n28.0\n846.0\n30.0\n1170\n40\n1130.0\n28\n27.0\n1130.0\n30.0\n1468\n50\n1418.0\n31\n30.0\n1418.0\n30.0\n1760\n60\n1700.0\n29\n28.0\n1700.0\n30.0\n2058\n70\n1988.0\n30\n29.0\n1988.0\n30.0\n2381\n80\n2301.0\n36\n35.0\n2301.0\n35.0\n2735\n90\n2645.0\n33\n32.0\n2645.0\n50.0\n3088\n100\n2988.0\n32\n31.0\n2988.0\n50.0\n3854\n120\n3734.0\n35\n34.0\n3734.0\n50.0\n4607\n140\n4467.0\n35\n34.0\n4467.0\n51.0\n5767\n170\n5597.0\n47\n46.0\n5597.0\n52.0\n6862\n200\n6662.0\n28\n27.0\n6662.0\n52.0\n8773\n250\n8523.0\n38\n37.0\n8523.0\n52.0\n10555\n300\n10255.0\n33\n32.0\n10255.0\n52.0\n13972\n400\n13572.0\n32\n31.0\n13572.0\n52.0\n17530\n500\n17030.0\n36\n35.0\n17030.0\n53.0\n21391\n600\n20791.0\n36\n35.0\n20791.0\n69.0\n25759\n700\n25059.0\n70\n69.0\n25059.0\n117.0\n37838\n800\n37038.0\n41\n40.0\n37038.0\n595.0\n43067\n900\n42167.0\n67\n66.0\n42167.0\n595.0\n51510\n1000\n50510.0\n268\n267.0\n50510.0\n595.0\nTable B.1: Frequentist BDQN on Cartpole\n62\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\nbest episode\n83\n1\n82.0\n83\n82.0\n82.0\n82.0\n208\n2\n206.0\n125\n124.0\n206.0\n124.0\n287\n3\n284.0\n79\n78.0\n284.0\n124.0\n373\n4\n369.0\n86\n85.0\n369.0\n124.0\n426\n5\n421.0\n53\n52.0\n421.0\n124.0\n587\n6\n581.0\n161\n160.0\n581.0\n160.0\n668\n7\n661.0\n81\n80.0\n661.0\n160.0\n719\n8\n711.0\n51\n50.0\n711.0\n160.0\n798\n9\n789.0\n79\n78.0\n789.0\n160.0\n971\n10\n961.0\n173\n172.0\n961.0\n172.0\n1215\n12\n1203.0\n111\n110.0\n1203.0\n172.0\n1371\n14\n1357.0\n95\n94.0\n1357.0\n172.0\n1780\n17\n1763.0\n84\n83.0\n1763.0\n212.0\n2136\n20\n2116.0\n131\n130.0\n2116.0\n212.0\n2751\n25\n2726.0\n131\n130.0\n2726.0\n212.0\n3599\n30\n3569.0\n157\n156.0\n3569.0\n212.0\n8788\n40\n8748.0\n628\n627.0\n8748.0\n861.0\n16510\n50\n16460.0\n733\n732.0\n16460.0\n882.0\n24754\n60\n24694.0\n478\n477.0\n24694.0\n906.0\n28488\n70\n28418.0\n316\n315.0\n28418.0\n906.0\n34463\n80\n34384.0\n622\n621.0\n34384.0\n1001.0\n40727\n90\n40643.0\n189\n188.0\n40643.0\n1001.0\n46167\n100\n46076.0\n614\n613.0\n46076.0\n1001.0\n54417\n120\n54306.0\n714\n713.0\n54306.0\n1001.0\n64573\n140\n64443.0\n555\n554.0\n64443.0\n1001.0\n89209\n170\n89066.0\n1001\n1001.0\n89066.0\n1001.0\n109211\n200\n109048.0\n478\n477.0\n109048.0\n1001.0\n140929\n250\n140735.0\n359\n358.0\n140735.0\n1001.0\n176244\n300\n176020.0\n280\n279.0\n176020.0\n1001.0\n263920\n400\n263663.0\n1001\n1001.0\n263663.0\n1001.0\n349184\n500\n348900.0\n245\n244.0\n348900.0\n1001.0\n438149\n600\n437838.0\n1001\n1001.0\n437838.0\n1001.0\n522124\n700\n521783.0\n1001\n1001.0\n521783.0\n1001.0\n608382\n800\n608019.0\n1001\n1001.0\n608019.0\n1001.0\n693162\n900\n692765.0\n921\n920.0\n692765.0\n1001.0\n777106\n1000\n776665.0\n1001\n1001.0\n776665.0\n1001.0\nTable B.2: Vanilla DQN\n63\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\nbest episode\n122\n1\n121.0\n122\n121.0\n121.0\n121.0\n151\n2\n149.0\n29\n28.0\n149.0\n121.0\n248\n3\n245.0\n97\n96.0\n245.0\n121.0\n331\n4\n327.0\n83\n82.0\n327.0\n121.0\n359\n5\n354.0\n28\n27.0\n354.0\n121.0\n388\n6\n382.0\n29\n28.0\n382.0\n121.0\n418\n7\n411.0\n30\n29.0\n411.0\n121.0\n460\n8\n452.0\n42\n41.0\n452.0\n121.0\n551\n9\n542.0\n91\n90.0\n542.0\n121.0\n636\n10\n626.0\n85\n84.0\n626.0\n121.0\n872\n12\n860.0\n111\n110.0\n860.0\n124.0\n1123\n14\n1109.0\n111\n110.0\n1109.0\n139.0\n1432\n17\n1415.0\n105\n104.0\n1415.0\n139.0\n1744\n20\n1724.0\n114\n113.0\n1724.0\n139.0\n2481\n25\n2456.0\n231\n230.0\n2456.0\n230.0\n3253\n30\n3223.0\n163\n162.0\n3223.0\n243.0\n4610\n40\n4570.0\n108\n107.0\n4570.0\n300.0\n6205\n50\n6155.0\n86\n85.0\n6155.0\n375.0\n7805\n60\n7745.0\n61\n60.0\n7745.0\n375.0\n9989\n70\n9920.0\n86\n85.0\n9920.0\n1001.0\n15313\n80\n15238.0\n238\n237.0\n15238.0\n1001.0\n19298\n90\n19214.0\n80\n79.0\n19214.0\n1001.0\n21806\n100\n21713.0\n28\n27.0\n21713.0\n1001.0\n27840\n120\n27729.0\n216\n215.0\n27729.0\n1001.0\n34258\n140\n34129.0\n383\n382.0\n34129.0\n1001.0\n46921\n170\n46769.0\n180\n179.0\n46769.0\n1001.0\n60260\n200\n60081.0\n877\n876.0\n60081.0\n1001.0\n80310\n250\n80089.0\n334\n333.0\n80089.0\n1001.0\n105639\n300\n105378.0\n159\n158.0\n105378.0\n1001.0\n153152\n400\n152812.0\n159\n158.0\n152812.0\n1001.0\n199978\n500\n199559.0\n260\n259.0\n199559.0\n1001.0\n258258\n600\n257765.0\n1001\n1001.0\n257765.0\n1001.0\n319524\n700\n318964.0\n202\n201.0\n318964.0\n1001.0\n372762\n800\n372128.0\n167\n166.0\n372128.0\n1001.0\n418762\n900\n418047.0\n377\n376.0\n418047.0\n1001.0\n475262\n1000\n474476.0\n1001\n1001.0\n474476.0\n1001.0\nTable B.3: Boot DQN\n64\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\nbest episode\n84\n1\n83.0\n84\n83.0\n83.0\n83.0\n142\n2\n140.0\n58\n57.0\n140.0\n83.0\n203\n3\n200.0\n61\n60.0\n200.0\n83.0\n257\n4\n253.0\n54\n53.0\n253.0\n83.0\n300\n5\n295.0\n43\n42.0\n295.0\n83.0\n361\n6\n355.0\n61\n60.0\n355.0\n83.0\n450\n7\n443.0\n89\n88.0\n443.0\n88.0\n567\n8\n559.0\n117\n116.0\n559.0\n116.0\n662\n9\n653.0\n95\n94.0\n653.0\n116.0\n800\n10\n790.0\n138\n137.0\n790.0\n137.0\n1030\n12\n1018.0\n120\n119.0\n1018.0\n137.0\n1254\n14\n1240.0\n131\n130.0\n1240.0\n137.0\n1654\n17\n1637.0\n188\n187.0\n1637.0\n187.0\n1972\n20\n1952.0\n135\n134.0\n1952.0\n187.0\n2578\n25\n2553.0\n125\n124.0\n2553.0\n187.0\n3139\n30\n3109.0\n106\n105.0\n3109.0\n187.0\n4198\n40\n4158.0\n111\n110.0\n4158.0\n187.0\n5250\n50\n5200.0\n101\n100.0\n5200.0\n187.0\n6272\n60\n6212.0\n92\n91.0\n6212.0\n187.0\n7310\n70\n7240.0\n40\n39.0\n7240.0\n204.0\n7811\n80\n7731.0\n82\n81.0\n7731.0\n204.0\n9163\n90\n9073.0\n160\n159.0\n9073.0\n250.0\n11091\n100\n10991.0\n191\n190.0\n10991.0\n306.0\n14707\n120\n14587.0\n164\n163.0\n14587.0\n306.0\n18564\n140\n18424.0\n146\n145.0\n18424.0\n369.0\n25655\n170\n25485.0\n111\n110.0\n25485.0\n902.0\n31176\n200\n30976.0\n178\n177.0\n30976.0\n902.0\n44395\n250\n44145.0\n178\n177.0\n44145.0\n939.0\n53754\n300\n53455.0\n178\n177.0\n53455.0\n1001.0\n71189\n400\n70790.0\n173\n172.0\n70790.0\n1001.0\n88763\n500\n88264.0\n207\n206.0\n88264.0\n1001.0\n108915\n600\n108318.0\n56\n55.0\n108318.0\n1001.0\n126820\n700\n126123.0\n241\n240.0\n126123.0\n1001.0\n143856\n800\n143059.0\n94\n93.0\n143059.0\n1001.0\n157030\n900\n156133.0\n156\n155.0\n156133.0\n1001.0\n173286\n1000\n172289.0\n166\n165.0\n172289.0\n1001.0\nTable B.4: A2c Cartpole\n65\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\n1001\n1\n-1001.0\n1001\n-1001.0\n-1001.0\n2002\n2\n-2002.0\n1001\n-1001.0\n-2002.0\n3003\n3\n-3003.0\n1001\n-1001.0\n-3003.0\n3100\n4\n-3100.0\n97\n-97.0\n-3100.0\n4101\n5\n-4101.0\n1001\n-1001.0\n-4101.0\n5102\n6\n-5102.0\n1001\n-1001.0\n-5102.0\n5158\n7\n-5158.0\n56\n-56.0\n-5158.0\n5236\n8\n-5236.0\n78\n-78.0\n-5236.0\n5287\n9\n-5287.0\n51\n-51.0\n-5287.0\n5353\n10\n-5353.0\n66\n-66.0\n-5353.0\n6384\n12\n-6384.0\n1001\n-1001.0\n-6384.0\n7421\n14\n-7421.0\n1001\n-1001.0\n-7421.0\n7582\n17\n-7582.0\n48\n-48.0\n-7582.0\n8708\n20\n-8708.0\n47\n-47.0\n-8708.0\n9867\n25\n-9867.0\n38\n-38.0\n-9867.0\n12050\n30\n-12050.0\n71\n-71.0\n-12050.0\n14427\n40\n-14427.0\n29\n-29.0\n-14427.0\n16779\n50\n-16779.0\n52\n-52.0\n-16779.0\n17204\n60\n-17204.0\n44\n-44.0\n-17204.0\n17569\n70\n-17569.0\n28\n-28.0\n-17569.0\n17951\n80\n-17951.0\n44\n-44.0\n-17951.0\n18289\n90\n-18289.0\n41\n-41.0\n-18289.0\n18639\n100\n-18639.0\n39\n-39.0\n-18639.0\n19311\n120\n-19311.0\n44\n-44.0\n-19311.0\n19999\n140\n-19999.0\n32\n-32.0\n-19999.0\n21105\n170\n-21105.0\n37\n-37.0\n-21105.0\n22137\n200\n-22137.0\n27\n-27.0\n-22137.0\n23863\n250\n-23863.0\n47\n-47.0\n-23863.0\n25514\n300\n-25514.0\n30\n-30.0\n-25514.0\n29048\n400\n-29048.0\n29\n-29.0\n-29048.0\n32763\n500\n-32763.0\n51\n-51.0\n-32763.0\n36452\n600\n-36452.0\n35\n-35.0\n-36452.0\n40589\n700\n-40589.0\n40\n-40.0\n-40589.0\n43959\n800\n-43959.0\n28\n-28.0\n-43959.0\n47403\n900\n-47403.0\n37\n-37.0\n-47403.0\n50901\n1000\n-50901.0\n37\n-37.0\n-50901.0\nTable B.5: DQN Moutnain Car\n66\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\n1001\n1\n-1001.0\n1001\n-1001.0\n-1001.0\n2002\n2\n-2002.0\n1001\n-1001.0\n-2002.0\n2040\n3\n-2040.0\n38\n-38.0\n-2040.0\n2076\n4\n-2076.0\n36\n-36.0\n-2076.0\n3077\n5\n-3077.0\n1001\n-1001.0\n-3077.0\n3115\n6\n-3115.0\n38\n-38.0\n-3115.0\n3152\n7\n-3152.0\n37\n-37.0\n-3152.0\n3193\n8\n-3193.0\n41\n-41.0\n-3193.0\n3230\n9\n-3230.0\n37\n-37.0\n-3230.0\n3258\n10\n-3258.0\n28\n-28.0\n-3258.0\n3325\n12\n-3325.0\n30\n-30.0\n-3325.0\n3384\n14\n-3384.0\n30\n-30.0\n-3384.0\n4470\n17\n-4470.0\n47\n-47.0\n-4470.0\n4595\n20\n-4595.0\n35\n-35.0\n-4595.0\n4783\n25\n-4783.0\n28\n-28.0\n-4783.0\n4997\n30\n-4997.0\n44\n-44.0\n-4997.0\n5513\n40\n-5513.0\n50\n-50.0\n-5513.0\n5934\n50\n-5934.0\n30\n-30.0\n-5934.0\n6326\n60\n-6326.0\n44\n-44.0\n-6326.0\n6678\n70\n-6678.0\n28\n-28.0\n-6678.0\n7057\n80\n-7057.0\n48\n-48.0\n-7057.0\n7439\n90\n-7439.0\n48\n-48.0\n-7439.0\n7840\n100\n-7840.0\n47\n-47.0\n-7840.0\n8719\n120\n-8719.0\n40\n-40.0\n-8719.0\n9488\n140\n-9488.0\n42\n-42.0\n-9488.0\n10522\n170\n-10522.0\n38\n-38.0\n-10522.0\n11570\n200\n-11570.0\n31\n-31.0\n-11570.0\n13289\n250\n-13289.0\n31\n-31.0\n-13289.0\n15031\n300\n-15031.0\n28\n-28.0\n-15031.0\n18448\n400\n-18448.0\n35\n-35.0\n-18448.0\n21934\n500\n-21934.0\n38\n-38.0\n-21934.0\n25491\n600\n-25491.0\n29\n-29.0\n-25491.0\n29021\n700\n-29021.0\n33\n-33.0\n-29021.0\n32592\n800\n-32592.0\n31\n-31.0\n-32592.0\n36315\n900\n-36315.0\n35\n-35.0\n-36315.0\n39884\n1000\n-39884.0\n42\n-42.0\n-39884.0\nTable B.6: Boot Moutnain Car\n67\nsteps\nepisode\ntotal return\nepisode len\nepisode return\nraw return\n83\n1\n-83.0\n83\n-83.0\n-83.0\n121\n2\n-121.0\n38\n-38.0\n-121.0\n184\n3\n-184.0\n63\n-63.0\n-184.0\n216\n4\n-216.0\n32\n-32.0\n-216.0\n255\n5\n-255.0\n39\n-39.0\n-255.0\n292\n6\n-292.0\n37\n-37.0\n-292.0\n322\n7\n-322.0\n30\n-30.0\n-322.0\n365\n8\n-365.0\n43\n-43.0\n-365.0\n416\n9\n-416.0\n51\n-51.0\n-416.0\n448\n10\n-448.0\n32\n-32.0\n-448.0\n507\n12\n-507.0\n30\n-30.0\n-507.0\n591\n14\n-591.0\n40\n-40.0\n-591.0\n685\n17\n-685.0\n28\n-28.0\n-685.0\n783\n20\n-783.0\n30\n-30.0\n-783.0\n941\n25\n-941.0\n27\n-27.0\n-941.0\n1107\n30\n-1107.0\n36\n-36.0\n-1107.0\n1447\n40\n-1447.0\n44\n-44.0\n-1447.0\n1817\n50\n-1817.0\n35\n-35.0\n-1817.0\n2143\n60\n-2143.0\n31\n-31.0\n-2143.0\n2455\n70\n-2455.0\n31\n-31.0\n-2455.0\n2806\n80\n-2806.0\n33\n-33.0\n-2806.0\n3151\n90\n-3151.0\n35\n-35.0\n-3151.0\n3498\n100\n-3498.0\n31\n-31.0\n-3498.0\n4165\n120\n-4165.0\n44\n-44.0\n-4165.0\n4831\n140\n-4831.0\n32\n-32.0\n-4831.0\n5784\n170\n-5784.0\n30\n-30.0\n-5784.0\n6775\n200\n-6775.0\n41\n-41.0\n-6775.0\n8493\n250\n-8493.0\n31\n-31.0\n-8493.0\n10151\n300\n-10151.0\n43\n-43.0\n-10151.0\n13532\n400\n-13532.0\n29\n-29.0\n-13532.0\n16932\n500\n-16932.0\n35\n-35.0\n-16932.0\n20328\n600\n-20328.0\n35\n-35.0\n-20328.0\n23642\n700\n-23642.0\n35\n-35.0\n-23642.0\n27004\n800\n-27004.0\n36\n-36.0\n-27004.0\n30343\n900\n-30343.0\n30\n-30.0\n-30343.0\n33735\n1000\n-33735.0\n30\n-30.0\n-33735.0\nTable B.7: A2C Moutnain Car\n68\nsteps\nep.e\ntotal return\nep. len\nepisode return\nttl. bad ep.\ndenoised ret\n18\n1\n-0.005\n18\n-0.005\n1\n0\n36\n2\n-0.0116666666666\n18\n-0.0066666666667\n2\n0\n54\n3\n-0.0183333333333\n18\n-0.0066666666667\n3\n0\n72\n4\n-0.023888888889\n18\n-0.0055555555556\n4\n0\n90\n5\n-0.0294444444447\n18\n-0.0055555555556\n5\n0\n108\n6\n-0.036111111109\n18\n-0.0066666666667\n6\n0\n126\n7\n-0.041666666661\n18\n-0.0055555555556\n7\n0\n144\n8\n-0.046666666658\n18\n-0.005\n8\n0\n162\n9\n-0.0511111111996\n18\n-0.00444444444444\n9\n0\n180\n10\n-0.059444444428\n18\n-0.0083333333333\n10\n0\n216\n12\n-0.069444444431\n18\n-0.005\n12\n0\n252\n14\n-0.080555555549\n18\n-0.0066666666667\n14\n0\n306\n17\n-0.093333333335\n18\n-0.005\n17\n0\n360\n20\n-0.107777777789\n18\n-0.0055555555556\n20\n0\n450\n25\n-0.138333333364\n18\n-0.0055555555556\n25\n0\n540\n30\n-0.164444444492\n18\n-0.005\n30\n0\n720\n40\n-0.207222222297\n18\n-0.00333333333335\n40\n0\n900\n50\n-0.24888888899\n18\n-0.00444444444444\n50\n0\n1080\n60\n-0.28166666663\n18\n-0.00222222222222\n60\n0\n1260\n70\n-0.31888888869\n18\n-0.00333333333335\n70\n0\n1440\n80\n-0.35444444409\n18\n-0.00333333333335\n80\n0\n1620\n90\n-0.381666666194\n18\n-0.00222222222222\n90\n0\n1800\n100\n-0.419444443807\n18\n-0.00444444444444\n100\n0\n2160\n120\n-0.45722222142\n18\n-0.00111111111111\n120\n0\n2520\n140\n-0.50555555554\n18\n-0.00166666666666\n140\n0\n3060\n170\n-0.57611111179\n18\n-0.00111111111111\n170\n0\n3600\n200\n-0.62166666615\n18\n-0.00166666666666\n200\n0\n4500\n250\n-0.69666666482\n18\n-0.00111111111111\n250\n0\n5400\n300\n-0.75055555348\n18\n-0.00055555555556\n300\n0\n7200\n400\n-0.87111111851\n18\n-0.00166666666666\n400\n0\n9000\n500\n-0.96833333031\n18\n-0.00055555555556\n500\n0\n10800\n600\n-1.08611110928\n18\n-0.00222222222222\n600\n0\n12600\n700\n-1.174444444\n18\n0.0\n700\n0\n14400\n800\n-1.31666666844\n18\n-0.00111111111111\n800\n0\n16200\n900\n-1.39444444744\n18\n-0.0027777777778\n900\n0\n18000\n1000\n-1.4772222265\n18\n-0.0027777777778\n1000\n0\n21600\n1200\n-1.91777778895\n18\n-0.00222222222222\n1200\n0\n25200\n1400\n-2.19055557097\n18\n0.0\n1400\n0\n30600\n1700\n-2.7011111345\n18\n-0.00111111111111\n1700\n0\n36000\n2000\n-3.40111114545\n18\n-0.00111111111111\n2000\n0\n45000\n2500\n-4.4338888894\n18\n-0.0027777777778\n2500\n0\n54000\n3000\n-5.4888888859\n18\n-0.00222222222222\n3000\n0\n72000\n4000\n-7.5700000095\n18\n-0.00166666666666\n4000\n0\n90000\n5000\n-9.6744444468\n18\n-0.0027777777778\n5000\n0\n108000\n6000\n-11.7505557204\n18\n-0.00444444444444\n6000\n0\n126000\n7000\n-13.886666865\n18\n-0.0027777777778\n7000\n0\n144000\n8000\n-16.165000081\n18\n-0.00111111111111\n8000\n0\n162000\n9000\n-18.4572227062\n18\n-0.00222222222222\n9000\n0\n180000\n10000\n-20.8227775424\n18\n-0.00111111111111\n10000\n0\nTable B.8: DQN Deep Sea\n69\nsteps\nep.e\ntotal return\nep. len\nepisode return\nttl. bad ep.\ndenoised ret\n10\n1\n-0.005\n10\n-0.005\n1\n0\n20\n2\n-0.008\n10\n-0.003\n2\n0\n30\n3\n-0.013000000000000005\n10\n-0.005\n3\n0\n40\n4\n-0.01800000000000001\n10\n-0.005\n4\n0\n50\n5\n-0.023000000000000013\n10\n-0.005\n5\n0\n60\n6\n-0.02900000000000002\n10\n-0.006\n6\n0\n70\n7\n-0.03200000000000002\n10\n-0.003\n7\n0\n80\n8\n-0.037000000000000026\n10\n-0.005\n8\n0\n90\n9\n-0.04200000000000003\n10\n-0.005\n9\n0\n100\n10\n-0.04900000000000004\n10\n-0.007\n10\n0\n120\n12\n-0.06200000000000005\n10\n-0.008\n12\n0\n140\n14\n-0.07300000000000005\n10\n-0.005\n14\n0\n170\n17\n-0.08600000000000006\n10\n-0.004\n17\n0\n200\n20\n-0.10000000000000007\n10\n-0.006\n20\n0\n250\n25\n-0.12800000000000009\n10\n-0.005\n25\n0\n300\n30\n-0.1500000000000001\n10\n-0.004\n30\n0\n400\n40\n-0.19100000000000014\n10\n-0.006\n40\n0\n500\n50\n-0.22900000000000018\n10\n-0.005\n50\n0\n600\n60\n-0.2840000000000002\n10\n-0.008\n60\n0\n700\n70\n-0.32900000000000024\n10\n-0.005\n70\n0\n800\n80\n-0.3720000000000003\n10\n-0.002\n80\n0\n900\n90\n-0.4250000000000003\n10\n-0.005\n90\n0\n1000\n100\n-0.4870000000000004\n10\n-0.008\n100\n0\n1200\n120\n-0.5780000000000004\n10\n-0.004\n120\n0\n1400\n140\n-0.6680000000000005\n10\n-0.003\n140\n0\n1700\n170\n-0.7930000000000006\n10\n-0.003\n170\n0\n2000\n200\n-0.9120000000000007\n10\n-0.004\n200\n0\n2500\n250\n-1.078999999999992\n10\n-0.003\n250\n0\n3000\n300\n-1.1729999999999816\n10\n-0.001\n300\n0\n4000\n400\n-1.3039999999999672\n10\n0.0\n400\n0\n5000\n500\n-1.4129999999999552\n10\n-0.001\n500\n0\n6000\n600\n-1.513999999999944\n10\n-0.001\n600\n0\n7000\n700\n-1.6159999999999328\n10\n-0.001\n700\n0\n8000\n800\n-1.7219999999999211\n10\n-0.001\n800\n0\n9000\n900\n-1.82399999999991\n10\n-0.001\n900\n0\n10000\n1000\n-1.923999999999899\n10\n-0.001\n1000\n0\n12000\n1200\n-2.1219999999998773\n10\n-0.001\n1200\n0\n14000\n1400\n-2.1819999999998707\n10\n0.0\n1400\n0\n17000\n1700\n-2.18999999999987\n10\n0.0\n1700\n0\n20000\n2000\n-2.18999999999987\n10\n0.0\n2000\n0\n25000\n2500\n-2.2019999999998685\n10\n0.0\n2500\n0\n30000\n3000\n-2.232999999999865\n10\n0.0\n3000\n0\n40000\n4000\n-2.2379999999998645\n10\n0.0\n4000\n0\n50000\n5000\n-3.132999999999766\n10\n-0.002\n5000\n0\n60000\n6000\n-4.163999999999725\n10\n-0.001\n6000\n0\n70000\n7000\n-5.16600000000006\n10\n-0.001\n7000\n0\n80000\n8000\n-6.1710000000003955\n10\n-0.001\n8000\n0\n90000\n9000\n-7.175000000000731\n10\n-0.001\n9000\n0\n100000\n10000\n-9.083000000000405\n10\n-0.003\n10000\n0\nTable B.9: A2C Deep Sea\n70\nsteps\nep.e\ntotal return\nep. len\nepisode return\nttl. bad ep.\ndenoised ret\n10\n1\n-0.005\n10\n-0.005\n1\n0.0\n20\n2\n-0.011000000000000003\n10\n-0.006\n2\n0.0\n30\n3\n-0.014000000000000005\n10\n-0.003\n3\n0.0\n40\n4\n-0.02000000000000001\n10\n-0.006\n4\n0.0\n50\n5\n-0.025000000000000015\n10\n-0.005\n5\n0.0\n60\n6\n-0.03000000000000002\n10\n-0.005\n6\n0.0\n70\n7\n-0.036000000000000025\n10\n-0.006\n7\n0.0\n80\n8\n-0.04100000000000003\n10\n-0.005\n8\n0.0\n90\n9\n-0.04500000000000003\n10\n-0.004\n9\n0.0\n100\n10\n-0.05000000000000004\n10\n-0.005\n10\n0.0\n120\n12\n-0.059000000000000045\n10\n-0.004\n12\n0.0\n140\n14\n-0.06900000000000005\n10\n-0.005\n14\n0.0\n170\n17\n-0.08300000000000006\n10\n-0.004\n17\n0.0\n200\n20\n-0.09800000000000007\n10\n-0.005\n20\n0.0\n250\n25\n-0.11900000000000009\n10\n-0.004\n25\n0.0\n300\n30\n-0.1410000000000001\n10\n-0.007\n30\n0.0\n400\n40\n-0.19300000000000014\n10\n-0.004\n40\n0.0\n500\n50\n-0.2460000000000002\n10\n-0.007\n50\n0.0\n600\n60\n-0.3080000000000002\n10\n-0.007\n60\n0.0\n700\n70\n-0.36800000000000027\n10\n-0.007\n70\n0.0\n800\n80\n-0.43400000000000033\n10\n-0.00900001\n80\n0.0\n900\n90\n-0.5070000000000003\n10\n-0.008\n90\n0.0\n1000\n100\n-0.5750000000000004\n10\n-0.00900001\n100\n0.0\n1200\n120\n-0.7150000000000005\n10\n-0.005\n120\n0.0\n1400\n140\n-0.8220000000000006\n10\n-0.005\n140\n0.0\n1700\n170\n3.9960000000000027\n10\n0.99\n165\n5.0\n2000\n200\n30.70999999999987\n10\n0.99\n168\n32.0\n2500\n250\n80.20999999999982\n10\n0.99\n168\n82.0\n3000\n300\n129.70999999999745\n10\n0.99\n168\n132.0\n4000\n400\n228.70999999999268\n10\n0.99\n168\n232.0\n5000\n500\n327.7100000000084\n10\n0.99\n168\n332.0\n6000\n600\n426.71000000003204\n10\n0.99\n168\n432.0\n7000\n700\n525.7100000000556\n10\n0.99\n168\n532.0\n8000\n800\n624.7100000000793\n10\n0.99\n168\n632.0\n9000\n900\n723.7100000001029\n10\n0.99\n168\n732.0\n10000\n1000\n822.7100000001266\n10\n0.99\n168\n832.0\n12000\n1200\n1020.7100000001739\n10\n0.99\n168\n1032.0\n14000\n1400\n1218.710000000221\n10\n0.99\n168\n1232.0\n17000\n1700\n1515.710000000292\n10\n0.99\n168\n1532.0\n20000\n2000\n1811.711000000363\n10\n0.99\n169\n1831.0\n25000\n2500\n2306.7109999998875\n10\n0.99\n169\n2331.0\n30000\n3000\n2801.710999998869\n10\n0.99\n169\n2831.0\n40000\n4000\n3791.7109999968316\n10\n0.99\n169\n3831.0\n50000\n5000\n4781.710999994794\n10\n0.99\n169\n4831.0\n60000\n6000\n5771.710999992757\n10\n0.99\n169\n5831.0\n70000\n7000\n6761.71099999072\n10\n0.99\n169\n6831.0\n80000\n8000\n7751.7109999886825\n10\n0.99\n169\n7831.0\n90000\n9000\n8741.710999986646\n10\n0.99\n169\n8831.0\n100000\n10000\n9731.710999984609\n10\n0.99\n169\n9831.0\nTable B.10: BootDQN Deep Sea\nBibliography\n[Andrychowicz et al., 2018] Andrychowicz, M., Baker, B., Chociej, M., Jozefow-\nicz, R., McGrew, B., Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray,\nA., et al. (2018).\nLearning dexterous in-hand manipulation.\narXiv preprint\narXiv:1808.00177.\n[Azizzadenesheli et al., 2018] Azizzadenesheli, K., Brunskill, E., and Anandku-\nmar, A. (2018). Efficient exploration through bayesian deep q-networks. 2018\nInformation Theory and Applications Workshop (ITA).\n[Barto et al., 1983] Barto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neu-\nronlike adaptive elements that can solve difficult learning control problems.\nIEEE transactions on systems, man, and cybernetics, (5):834–846.\n[Bellman, 1966] Bellman,\nR. (1966).\nDynamic programming.\nScience,\n153(3731):34–37.\n[Bertsekas and Tsitsiklis, 1996] Bertsekas, D. P. and Tsitsiklis, J. N. (1996).\nNeuro-dynamic programming. Athena Scientific.\n[Borkar, 1997] Borkar, V. S. (1997).\nStochastic approximation with two time\nscales. Systems & Control Letters, 29(5):291–294.\n[Brockman et al., 2016] Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. (2016). Openai gym. arXiv preprint\narXiv:1606.01540.\n[Burda et al., 2018] Burda, Y., Edwards, H., Storkey, A., and Klimov, O. (2018).\nExploration by random network distillation. arXiv preprint arXiv:1810.12894.\nBibliography\n72\n[Deisenroth and Rasmussen, 2011] Deisenroth, M. and Rasmussen, C. E. (2011).\nPilco: A model-based and data-efficient approach to policy search. In Proceed-\nings of the 28th International Conference on machine learning (ICML-11), pages\n465–472.\n[Hochreiter and Schmidhuber, 1997] Hochreiter, S. and Schmidhuber, J. (1997).\nLong short-term memory. Neural computation, 9(8):1735–1780.\n[Kaiser et al., 2019] Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell,\nR. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohi-\nuddin, A., Sepassi, R., Tucker, G., and Michalewski, H. (2019). Model-based\nreinforcement learning for atari.\n[Krizhevsky et al., 2012] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012).\nImagenet classification with deep convolutional neural networks. In Advances in\nneural information processing systems, pages 1097–1105.\n[LeCun et al., 2015] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning.\nnature, 521(7553):436.\n[Mnih et al., 2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G.,\net al. (2015). Human-level control through deep reinforcement learning. Nature,\n518(7540):529.\n[Moore, 1990] Moore, A. W. (1990). Efficient memory-based learning for robot\ncontrol.\n[Ortega and Lin, 2004] Ortega, M. and Lin, L. (2004). Control theory applications\nto the production–inventory problem: a review. International Journal of Produc-\ntion Research, 42(11):2303–2322.\n[Osband et al., 2019] Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener,\nE., Saraiva, A., McKinney, K., Lattimore, T., Szepezvari, C., Singh, S., Roy,\nBibliography\n73\nB. V., Sutton, R., Silver, D., and Hasselt, H. V. (2019). Behaviour suite for\nreinforcement learning.\n[Osband et al., 2017] Osband, I., Van Roy, B., Russo, D., and Wen, Z. (2017). Deep\nexploration via randomized value functions. arXiv preprint arXiv:1703.07608.\n[Powell and Ryzhov, 2012] Powell, W. B. and Ryzhov, I. O. (2012). Optimal learn-\ning: Powell/optimal.\n[Rummery and Niranjan, 1994] Rummery, G. A. and Niranjan, M. (1994). On-line\nQ-learning using connectionist systems, volume 37.\n[Schulman et al., 2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. (2017). Proximal policy optimization algorithms.\n[Seierstad and Sydsaeter, 1986] Seierstad, A. and Sydsaeter, K. (1986). Optimal\ncontrol theory with economic applications. Elsevier.\n[Silver et al., 2016] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V.,\nLanctot, M., et al. (2016). Mastering the game of go with deep neural networks\nand tree search. nature, 529(7587):484.\n[Silver et al., 2018] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\nM., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. (2018).\nA general reinforcement learning algorithm that masters chess, shogi, and go\nthrough self-play. Science, 362(6419):1140–1144.\n[Silver et al., 2017] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I.,\nHuang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017).\nMastering the game of go without human knowledge. Nature, 550(7676):354.\n[Sutton and Barto, 2018] Sutton, R. S. and Barto, A. G. (2018).\nReinforcement\nLearning: An Introduction. The MIT Press, second edition.\nBibliography\n74\n[Szepesv´ari, 2010] Szepesv´ari, C. (2010). Algorithms for reinforcement learning.\nSynthesis lectures on artificial intelligence and machine learning, 4(1):1–103.\n[Thompson, 1933] Thompson, W. R. (1933). On the likelihood that one unknown\nprobability exceeds another in view of the evidence of two samples. Biometrika,\n25(3/4):285–294.\n[Tsitsiklis and Van Roy, 1997] Tsitsiklis, J. N. and Van Roy, B. (1997). Analysis\nof temporal-diffference learning with function approximation. In Advances in\nneural information processing systems, pages 1075–1081.\n[Watkins and Dayan, 1992] Watkins, C. J. and Dayan, P. (1992). Q-learning. Ma-\nchine learning, 8(3-4):279–292.\n[Williams, 1992] Williams, R. J. (1992). Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–\n256.\n[Witsenhausen, 1968] Witsenhausen, H. S. (1968). A counterexample in stochastic\noptimum control. In SIAM J. Control, volume 6, pages 131–147.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-08-19",
  "updated": "2024-08-19"
}