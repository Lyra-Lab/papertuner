{
  "id": "http://arxiv.org/abs/2408.10215v2",
  "title": "Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications",
  "authors": [
    "Sinan Ibrahim",
    "Mostafa Mostafa",
    "Ali Jnadi",
    "Hadi Salloum",
    "Pavel Osinenko"
  ],
  "abstract": "The aim of Reinforcement Learning (RL) in real-world applications is to\ncreate systems capable of making autonomous decisions by learning from their\nenvironment through trial and error. This paper emphasizes the importance of\nreward engineering and reward shaping in enhancing the efficiency and\neffectiveness of reinforcement learning algorithms. Reward engineering involves\ndesigning reward functions that accurately reflect the desired outcomes, while\nreward shaping provides additional feedback to guide the learning process,\naccelerating convergence to optimal policies. Despite significant advancements\nin reinforcement learning, several limitations persist. One key challenge is\nthe sparse and delayed nature of rewards in many real-world scenarios, which\ncan hinder learning progress. Additionally, the complexity of accurately\nmodeling real-world environments and the computational demands of reinforcement\nlearning algorithms remain substantial obstacles. On the other hand, recent\nadvancements in deep learning and neural networks have significantly improved\nthe capability of reinforcement learning systems to handle high-dimensional\nstate and action spaces, enabling their application to complex tasks such as\nrobotics, autonomous driving, and game playing. This paper provides a\ncomprehensive review of the current state of reinforcement learning, focusing\non the methodologies and techniques used in reward engineering and reward\nshaping. It critically analyzes the limitations and recent advancements in the\nfield, offering insights into future research directions and potential\napplications in various domains.",
  "text": "IEEE SYSTEMS, MAN AND CYBERNETICS SOCIETY SECTION\nReceived 18 October 2024, accepted 13 November 2024, date of publication 22 November 2024, date of current version 3 December 2024.\nDigital Object Identifier 10.1109/ACCESS.2024.3504735\nComprehensive Overview of Reward Engineering\nand Shaping in Advancing Reinforcement\nLearning Applications\nSINAN IBRAHIM\n1, MOSTAFA MOSTAFA\n1, ALI JNADI\n2,3, HADI SALLOUM3,\nAND PAVEL OSINENKO\n1\n1Skolkovo Institute of Science and Technology, 121205 Moscow, Russia\n2Institute of Robotics and Computer Vision, Innopolis University, 420500 Innopolis, Russia\n3Research Center for Artificial Intelligence, Innopolis University, 420500 Innopolis, Russia\nCorresponding author: Sinan Ibrahim (Sinan.Ibrahim@Skoltech.ru)\nThe work of Ali Jnadi and Hadi Salloum was supported by the Analytical Center for the Government of Russian Federation under\nAgreement 70-2021-00143 01.11.2021 and Agreement IGK 000000D730324P540002.\nABSTRACT\nReinforcement Learning (RL) seeks to develop systems capable of autonomous\ndecision-making by learning through interaction with their environment. Central to this process are\nreward engineering and reward shaping, which are essential for enhancing the efficiency and effectiveness\nof RL algorithms. These techniques guide agents toward desired behaviors, improve learning stability,\nand accelerate convergence by addressing challenges such as sparse and delayed rewards. However,\nthe complexity of real-world environments and the computational demands of RL algorithms remain\nsignificant obstacles to broader adoption. Recent advancements in deep learning have enabled RL to\nhandle high-dimensional state and action spaces, facilitating applications in robotics, autonomous driving,\nand complex decision-making tasks. In response to these developments, this paper provides one of the\nfirst comprehensive reviews of reward design in RL, with a focus on the methodologies and techniques\nunderpinning reward engineering and shaping. By introducing a detailed taxonomy, critically analyzing\ncurrent approaches, and highlighting their limitations, this work fills an important gap in the literature,\noffering insights into how reward structures can be optimized to meet the growing demands of modern AI\nsystems.\nINDEX TERMS Reinforcement learning, reward engineering, reward planning, reward shaping.\nI. INTRODUCTION\nReward design in Reinforcement Learning (RL) is a critical\ncomponent that significantly influences the performance and\nlearning efficiency of RL agents [1], [2], [3]. RL, a prominent\nsubset of Machine Learning, trains intelligent agents to make\nsequential decisions by learning from interactions with their\nenvironment [4]. These agents aim to maximize cumulative\nrewards over time, making the design of the reward function\npivotal to their success. Reward design is a nuanced and\nintricate process that involves defining the reward function\nin a way that aligns with the desired behavior and goals\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Huiyan Zhang\n.\nof the RL agent. As highlighted in [5], the importance of\nreward design cannot be overstated, as it directly impacts the\nagent’s ability to learn and adapt to complex environments.\nThe art of reward design can be categorized into two primary\nareas. The first is Reward Engineering [6] which involves\nthe creation of the reward function itself. This involves the\ninitial creation of the reward function. The reward function,\nR(s, a, s′), maps states s, actions a, and successor state s′ to\na numerical reward value. A well-designed reward function\nshould provide informative feedback to the agent, guiding it\ntoward desired actions and behaviors. The reward function\nmust strike a balance between being informative enough\nto facilitate learning and sparse enough to prevent trivial\nsolutions. For example, in a grid-world navigation task, the\nVOLUME 12, 2024\n 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/\n175473\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nreward function might be defined as:\nR(s, a, s′) =\n(\n+10\nif s′ is the goal state\n−1\nif s′ is a non-goal state.\nThe reward function [3], [7], [8] serves as a signal to the\nRL agent, guiding it towards desirable actions and behaviors.\nA well-designed reward function should be informative,\nproviding the agent with clear feedback on the quality of\nits actions. It should also be sparse enough to prevent\ntrivial solutions but dense enough to facilitate learning. For\nexample, in a game-playing scenario, reward engineering\nmight involve assigning positive rewards for winning moves\nand negative rewards for losing moves, with additional\nconsiderations for intermediate actions that contribute to the\noverall strategy.\nThe second category is Reward Shaping [9], [10]. Once\nthe reward function is established, reward shaping comes into\nplay to fine-tune and enhance the reward signals. Reward\nshaping involves modifying the reward function to improve\nthe learning process without altering the optimal policy. One\ncommon approach is potential-based reward shaping [11],\nwhere a potential function 8(s) is introduced to modify the\nrewards:\nR′(s, a, s′) = R(s, a, s′) + γ 8(s′) −8(s).\n(1)\nHere, γ is the discount factor. The potential function\n8(s) should be carefully designed to ensure it does not\nchange the optimal policy but accelerates learning by\nproviding intermediate rewards. For instance, in a robotic\narm manipulation task, 8(s) could represent the negative\ndistance to the target object, encouraging the agent to move\ncloser to the target. Techniques such as potential-based\nshaping functions can be used to provide additional guidance\nto the agent, accelerating the learning process by giving\nintermediate rewards for progress toward the ultimate goal.\nFor instance, in a navigation task, shaping rewards could\ninclude providing positive feedback for reaching sub-goals\nor making progress towards the destination, even if the final\ngoal is not yet achieved.\nEffective reward design requires a deep understanding of\nthe problem domain, the agent’s learning dynamics, and\npotential pitfalls such as reward hacking, where the agent\nfinds unintended shortcuts to maximize rewards without\nachieving the desired behavior. To mitigate such problems,\niterative testing and validation of the reward function are\nessential, ensuring that the designed rewards lead to the\nintended outcomes. Reward design in RL is a fundamental\naspect that encompasses both the creation and refinement\nof reward functions. Through careful reward engineering\nand shaping, one can guide RL agents to learn complex\nbehaviors efficiently and effectively, ultimately leading to\nthe development of intelligent systems capable of performing\nsophisticated tasks.\nFor deep understanding, consider an RL problem formu-\nlated as a Markov Decision Process (MDP), defined by the\ntuple (S, A, P, R, γ ), where:\n• S is the set of states,\n• A is the set of actions,\n• P(s′|s, a) is the transition probability function,\n• R(s, a) is the reward function,\nThe goal of the RL agent is to learn a policy π(a|s) that\nmaximizes the expected cumulative reward, given by the\nreturn Gt:\nGt =\n∞\nX\nk=0\nγ kR(st+k, at+k).\n(2)\nThe value function V π(s) and the action-value function\nQπ(s, a) are defined as:\nV π(s) = Eπ\n\" ∞\nX\nk=0\nγ kR(st+k, at+k)\n\f\f\f\fst = s\n#\n,\n(3)\nQπ(s, a) = Eπ\n\" ∞\nX\nk=0\nγ kR(st+k, at+k)\n\f\f\f\fst = s, at = a\n#\n. (4)\nIn reward shaping, the modified reward function R′(s, a, s′)\nensures the policy remains optimal while improving the\nlearning speed. The shaping potential 8(s) is chosen such that\nthe difference in potential provides informative guidance:\n8(s) = heuristic(s).\n(5)\nFor instance, in a maze-solving task, 8(s) could be\nthe negative Manhattan distance to the goal, providing\nincremental rewards as the agent approaches the goal.\nReward design in RL is a fundamental aspect that\nencompasses both the creation and refinement of reward\nfunctions and it has been used in various areas [12], [13], [14],\n[15]. Through careful reward engineering and shaping, one\ncan guide RL agents to learn complex behaviors efficiently\nand effectively, ultimately leading to the development of\nintelligent systems capable of performing sophisticated tasks.\nEfficiently, the complexity of modern systems demands a\nmore nuanced approach to reward engineering. The work\ncited as [6] argues for a shift in focus toward developing\nalternative frameworks that can effectively guide these agents\ntoward desired behaviors, ensuring they ‘‘do the right thing’’\nin increasingly complex and ethically-charged scenarios.\nTraditional control methods such as MPC, or LQR in\nrobotics often rely on pre-programmed behaviors and lack\nthe adaptability and learning capabilities of RL. How-\never, RL lacks inherent performance guarantees in agent-\nenvironment interactions, requiring additional assurance\nmeasures [16]. Reward engineering provides a powerful\ntool for bridging the gap between conventional control and\nintelligent, learning-based systems. While early AI research\ncould focus solely on achieving goals.\nTherefore, this work aims to provide a comprehensive\nreview of reward design, investigating the key concepts,\n175474\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\ntrends, challenges, and opportunities in reward shap-\ning/engineering. It explores how the design of reward\nfunctions plays a crucial role in influencing the behavior of\nlearning agents in various tasks and environments. Moreover,\nit investigates how advancements in reward engineering are\npaving the way for more efficient and effective learning\nalgorithms. This review sheds light on the importance\nof reward design in RL and highlights the potential of\nreward engineering to drive innovation in the fields of\nRL and Artificial Intelligence (AI). As AI agents become\nmore sophisticated and autonomous, the design of reward\nmechanisms becomes a crucial, yet increasingly challenging\naspect of their development.\nThis survey provides the first comprehensive exploration\nof reward design in RL, addressing a significant gap in\nthe literature. While a chapter in [3] briefly discusses the\nfundamentals of reward function design and its connections\nto behavioral sciences and evolution, it does not offer the\nlevel of analysis presented in this work. We present a detailed\ntaxonomy of reward shaping techniques, examining their\nadvantages, limitations, and application domains. In contrast\nto prior studies with narrower scopes, this review integrates a\nbroader range of methodologies, offering a more comprehen-\nsive perspective on reward shaping and engineering.\nThe motivations for this review stem from the increasing\ncomplexity of modern AI systems, which necessitates a\nnuanced approach to reward design. It highlights the crucial\nrole of well-crafted reward functions in guiding intelligent\nagents toward desired behaviors, emphasizing the balance\nbetween informativeness and sparsity in reward signals.\nMoreover, the review underscores the practical applications\nof reward design in various domains, particularly in robotics.\nIt explores the implications of reward engineering for\nbridging the gap between simulated and real-world scenarios.\nBy investigating both the theoretical foundations and\nreal-world applications of reward design, this review aims\nto facilitate a deeper understanding of how effective reward\nmechanisms can enhance learning efficiency. It identifies\ncurrent challenges and opportunities for innovation, encour-\naging future research to advance the field of RL and artificial\nintelligence.\nHence, our work is structured as follows: Section I presents\nthis review with an introduction. Section II details the process\nof selecting papers. Section III provides the background and\nfundamental concepts. Section IV outlines the taxonomy\nof reward shaping and engineering. Section V illustrates\nthe real-world applications in robotics and other domains.\nA comprehensive exploration of Sim-to-Real is presented\nin Section VI. Section VII discusses the shortcomings\nand advantages of reward engineering. Finally, the open\nchallenges and future directions, along with the conclusion,\nare presented in Sections VIII and IX, respectively.\nII. PAPERS SELECTION PROCESS\nA comprehensive literature review was conducted using\na systematic search strategy across multiple databases.\nTo ensure the transparency and reproducibility of our\nfindings, we adhered to the PRISMA 2020 guidelines [17]\nfor conducting and reporting systematic literature reviews.\nThe search focused on reward shaping techniques in AI-based\ncontrol systems, yielding a final set of 55 relevant papers after\nrigorous filtering and manual review. To ensure the reliability\nof the review, a quality assessment was performed on each\nincluded study using a set of benchmark questions.\nSearch terms included combinations of ‘‘reward shaping,’’\n‘‘reward engineering,’’ ‘‘reinforcement learning,’’ ‘‘reward\ndesign,’’ ‘‘machine learning,’’ and ‘‘control systems.’’ Due\nto the recentness of this topic, there is a limited number of\npapers that we potentially can review, therefor our inclusion\ncriteria were established to focus on studies published in\nEnglish between 1999 and 2024, which directly addressed\nthe application of reward shaping techniques in AI-based\ncontrol systems, and presented empirical results. Papers\nsolely focused on theoretical frameworks or lacking empirical\ndata were excluded, except for [6], [18], [19], and [20]\nbecause of their high impact on our review.\nIII. BACKGROUND AND FUNDAMENTAL CONCEPTS\nReward shaping becomes more advantageous as the likeli-\nhood of an agent wasting time exploring pointless areas of\nthe environment increases, and thus well-designed reward\nshaping can more effectively guide exploration, in addition\nto reward shaping, reward design has also been explored as a\nmeans of policy teaching, [21], [22], [23]\nTo define Reward Shaping and Reward Engineering, it is\nessential to go through the core RL concepts:\n• Agent: An entity that interacts with the environment.\n• Environment: The agent’s world, governed by specific\nrules and dynamics.\n• Policy: A function mapping states to actions, defining\nthe agent’s behavior.\n• Reward: A signal received by the agent based on its\nactions, indicating the desirability of a state or action.\n• Value Function: A function that estimates the expected\nfuture reward for a given state or state-action pair.\n• Exploration vs. Exploitation: The balance between\ntrying new actions and exploiting known good ones.\nBased on the findings [6], it can be stated that Reward\nshaping: the most common name for reward engineering,\nshaping or design methods, is a technique inspired by animal\ntraining where supplemental rewards are provided to make\na problem easier to learn. This includes modifying the\noriginal reward function by introducing additional incentives\nor penalties to guide the agent’s learning process. Reward\nengineering: encompasses a broader set of techniques,\nincluding using other algorithms to design the reward\nfunction or designing reward functions from scratch.\nThe concept of reward plays a pivotal role in the field of\nartificial intelligence, particularly within the framework of\nreinforcement learning.\nTwo distinct perspectives on the nature and sufficiency\nof reward have emerged, sparking debate among researchers\nVOLUME 12, 2024\n175475\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nThe first perspective, championed by the ‘‘Reward is\nEnough’’ hypothesis [18], posits that maximizing a scalar\nreward, a single numerical value representing progress\ntowards a goal could be the key to understanding and\nfurther building artificial intelligence. This view proposes\nthat complex cognitive abilities, like learning, language, and\nsocial intelligence, emerge as a consequence of striving for\nthis reward. The complexity of the environment, it is argued,\nnaturally drives agents to develop these abilities to achieve\ntheir goals more efficiently. However, the ‘‘Scalar Reward is\nNot Enough’’ perspective challenges this view [19], asserting\nthat relying solely on a single numerical value fails to capture\nthe multi-faceted nature of human intelligence, particularly\nin domains involving ethical considerations or complex,\nsubjective goals. This perspective advocates for the use of\nvector-valued rewards, which represent multiple aspects of\nprogress and can better guide the development of safe,\nhuman-aligned AI. While both perspectives acknowledge\nthe importance of reward in learning, the debate centers on\nthe sufficiency of a single scalar value in representing the\nfull spectrum of intelligent behavior. The ‘‘Scalar Reward\nis Not Enough’’ perspective highlights the need for more\nnuanced reward representations that can account for the\nintricate and often context-dependent nature of human\nintelligence.\nA. GENERAL PITFALLS IN REWARD DESIGN\nAs was established, reward design can be challenging and\ntime-consuming, furthermore, its effects usually can be\nnoticeable in the behavior of the agent in addition to the\nenvironment.\nFor a given task, understanding the difficulties in reward\nengineering can help to determine the most compatible,\nsuitable, and successful reward shaping techniques:\n• Reward Sparsity: Lack or delay of frequent reward\nsignals can lead to slow learning.\n• Deceptive Rewards: Reward signals may encourage the\nagent to find ‘‘easy’’ solutions that are not aligned with\nthe true objective.\n• Reward Hacking: Agents may exploit unintended loop-\nholes in the reward function to achieve high rewards\nwithout fulfilling the desired goal.\n• Unintended Consequences: Reward designs can lead\nto unexpected and undesirable behaviors due to the\ncomplex interplay between agent actions and the\nenvironment.\n• Misaligned Reward with True Objective: This highlights\nthe crucial problem of ensuring the reward function\nactually incentivizes the desired behavior.\n• Reward Function Complexity: A complex reward func-\ntion with multiple factors can be difficult to design and\ninterpret.\n• Difficulty in Evaluating Reward Design: It can be\ndifficult to objectively evaluate the effectiveness of a\nreward function, especially in complex environments.\nThese are some of the factors why Reward Engineering\ncould be an acceptable substitute for conventional reward\ndesign.\nB. SCALAR VS VECTOR REWARD\nIn RL, the choice between scalar and vector rewards\nsignificantly impacts the agent’s learning process. Scalar\nrewards, represented by a single numerical value, provide a\nsimple measure of progress towards a goal. This simplicity\nmakes them computationally efficient and easy to interpret.\nHowever, they often fail to capture the nuances of complex\ntasks, potentially leading to suboptimal behavior. Vector\nrewards, on the other hand, utilize multiple values to represent\ndifferent aspects of the task, offering a more comprehensive\nevaluation of the agent’s actions. This richer feedback allows\nfor a finer-grained learning process, enabling the agent to\nprioritize specific aspects of the task and potentially achieve\nmore desirable outcomes. While vector rewards can lead\nto improved performance, they come with the challenge\nof designing effective reward functions, balancing multiple\nobjectives, and navigating the computational complexities\nassociated with multi-dimensional feedback. The choice\nbetween scalar and vector rewards depends on the specific\ntask, the desired level of performance, and the available\ncomputational resources.\nIV. TAXONOMY OF REWARD SHAPING/ENGINEERING\nTECHNIQUES\nThis section contains a categorization of reward shaping\ntechniques based on underlying principles, along with\ndetailed explanations of each category, specific algorithms,\nadvantages, disadvantages, and evaluation criteria.\nPsychologists differentiate between extrinsic motivation,\nwhere actions are driven by specific anticipated rewards, and\nintrinsic motivation, where actions are motivated by inherent\nenjoyment [24]. Similarly in RL, Rewards can be categorized\ninto intrinsic and extrinsic types, illustrated in Figure 1.\nFIGURE 1. Agent-environment interaction in reinforcement learning. A:\nThe agent receives rewards from a ‘‘critic’’ within its environment. B: This\npanel expands on Panel A by distinguishing between an internal and an\nexternal environment, with rewards originating from the internal\nenvironment. The shaded box represents what could be likened to the\n‘‘organism.’’, [Figure 1] [24].\nThese rewards play critical roles in motivating and guiding\nagent behavior, making the shaping and design of these\n175476\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nfunctions crucial for determining the success or failure\nof algorithms. In their study of reward functions [25]\nintroduced a computational framework aimed at optimizing\nthese rewards to improve agent behaviors. Their approach\nfocuses on defining optimal reward structures based on\nfitness functions and environmental distributions. Similarly,\n[26] further explored intrinsic motivation within the RL\nframework. They proposed an evolutionary-inspired opti-\nmal reward framework that emphasizes designing reward\nfunctions to enhance evolutionary success across diverse\nenvironments. This framework explores the delicate balance\nbetween intrinsic and extrinsic motivations, emphasizing the\ncomputational modeling of intrinsic motivation driven by\ninternal rewards crucial for intellectual growth. They define\nthe optimal reward as:\nr∗\nA = arg max\nrA∈RA\nEE∼P(ε)Eh∼⟨A(rA),E⟩{F(h)},\n(6)\nwhere E denotes the expectation operator. A special reward\nfunction in RA is the fitness-based reward function, that most\ndirectly translates fitness F into an RL reward function, i.e.,\nwhere the fitness function F measures a scalar evaluation of\nthe agent based on its interaction history h, which is sampled\nfrom the distribution resulting from its interaction with the\nenvironment. The fitness value of a lifetime-length history\nis the cumulative fitness-based reward for that history, [26,\nEquation 1].\nPrevious studies highlight the importance of advanced\nreward shaping methodologies in improving agent learning\nand adaptation.\nA. POLICY GRADIENT METHODS\nPolicy gradient methods directly optimize an agent’s policy\nto maximize cumulative rewards [27]. Unlike value-based\napproaches, which estimate value functions, policy gradient\nmethods focus solely on improving the policy itself.\n1) POLICY GRADIENT FOR REWARD DESIGN (PGRD)\nIn RL, an agent aims to maximize its total reward over time,\nreferred to as its return. It’s crucial to note that the sequence\nof environment states is influenced by the selected reward\nfunction, thereby affecting the goals of the agent’s designer.\nThe challenge of finding the optimal reward stems from\nthe fact that, while the objective reward function is fixed\nin the problem formulation, the choice of reward function\nremains within the designer’s control [28]. Addressing this\nchallenge, previous research introduced the PGRD, which\nemploys online gradient ascent to iteratively adjust reward\nparameters during the agent’s operation. They argue that ‘‘the\noptimal reward parameters are determined by solving the\noptimal reward problem’’ [28, Equation 1]:\nθ∗= arg max\nθ∈2\nlim\nN→∞E\n\"\n1\nN\nN\nX\nt=0\nRO(st)|R(·, θ)\n#\n,\n(7)\nwhere RO is objective reward function given by the designer\nand R(·) is separate agent reward function. Authors abstractly\nrepresent this selection by parameterizing the reward with\na vector of parameters θ chosen from a parameter space\n2. Each θ ∈2 defines a reward function R(·; θ), which\nsubsequently generates a distribution over the sequences\nof environment states using the agent’s RL method. The\nexpected return achieved by the designer for the choice of\nθ is denoted as J(θ).\nPGRD adjusts to accommodate the agent’s abilities and\nuncertainties in model accuracy. It formalizes updates to\nthe reward function by estimating gradients of the objective\nreturn and policy, ensuring convergence with rigorous proofs.\nResults illustrate its effectiveness in discrete-time, partially\nobservable environments, as depicted in Figure 2, to max-\nimize the expected mean objective reward over an infinite\nhorizon. Unlike traditional methods that impose fixed goals\non agents, PGRD enhances results compared to conventional\npolicy gradient methods, highlighting its capacity to improve\nagent performance across various scenarios through dynamic\nreward optimization.\nFIGURE 2. PGRD performance with A) poor model, B partially observable\nworld, [28, Figure 3].\n2) LEARNING INTRINSIC REWARD FOR POLICY GRADIENT\n(LIRPG)\nIn a similar vein, [29] introduced the LIRPG algorithm.\nLIRPG enhances RL by enabling agents to dynamically\nlearn intrinsic rewards in addition to traditional extrinsic\nrewards. They propose the following equation for updating\npolicy parameters θ by incorporating both policy and intrinsic\nreward parameters through regular policy gradient updates\n[29, Equation 4]:\nθ′ ≈θ + αGex+in(st, at)∇θ log πθ(at|st).\n(8)\nLIRPG has demonstrated improved learning efficiency and\nperformance in complex environments, reducing sample\ncomplexity and accelerating learning processes. Results indi-\ncate that LIRPG optimizes agent performance across Atari\ngames and Mujoco domains, as illustrated for Hopper and\nHalfCheetah in Figure 3, consistently outperforming baseline\nagents that rely solely on extrinsic rewards. However,\nchallenges include the necessity for meticulous parameter\ntuning and potential sensitivity to the selection and interaction\nof intrinsic reward functions with policy updates. It is\nnoteworthy that LIRPG expands on the concept of reward\nshaping by allowing agents to adjust their behavior based on\nVOLUME 12, 2024\n175477\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\ninternal signals of progress or success, alongside the external\nrewards provided by the environment.\nFIGURE 3. The x-axis represents time steps during the learning process,\nwhile the y-axis denotes the average reward over the last 100 training\nepisodes. The black curves correspond to the baseline PPO architecture.\nThe blue curves represent the PPO-live-bonus baseline. The red curves\ndepict our augmented architecture using LIRPG. The green curves show\nthe performance of our LIRPG architecture where the policy module was\ntrained solely with intrinsic rewards. The darker curves represent\naverages across 10 runs with different random seeds. The shaded area\nindicates the standard errors across these 10 runs, [29, Figure 4].\n3) DDPG FROM DEMONSTRATIONS (DDPGFD)\nDeep Deterministic Policy Gradient (DDPG) is a model-free,\noff-policy reinforcement learning algorithm that combines\nthe strengths of Deep Q-Learning (DQN) and Deterministic\nPolicy Gradient (DPG). It is designed for environments with\ncontinuous action spaces, using an actor-critic approach to\nlearn both a policy and a Q-function concurrently.\nAs an extension of the DDPG algorithm [30], DDPGfD\nis tailored for robotic RL tasks with sparse rewards.\nDDPGfD employs off-policy learning by integrating demon-\nstration trajectories into the replay buffer, thereby lever-\naging human-provided guidance to bootstrap learning and\naddress exploration challenges common in high-dimensional\ncontrol problems such as robotics. The algorithm modifies\nDDPG in several key ways: integrating transitions from\na human demonstrator into the replay buffer and utilizing\nprioritized replay to effectively sample transitions from both\ndemonstration and agent data. The learning process includes\na mix of 1-step L1(θQ) and n-step return Ln(θQ) losses\nto enhance performance. Furthermore, it updates multiple\ntimes per environment step, thereby improving learning\nefficiency. Regularization is implemented with L2 penalties\non the critic’s weights LC\nreg(θQ) and the actor’s weights\nLC\nreg(θπ), promoting stable training and generalization. The\nexperimental setup involves tasks of inserting a two-pronged\ndeformable plastic clip into a housing using a 7-DOF robotic\narm, with results depicted in Figure 4.\nDDPGfD simplifies learning by integrating human guid-\nance and eliminates the need for intricate reward shap-\ning. However, significant challenges persist in maintaining\ndemonstration quality and scalability across a wide range of\nrobotic applications.\nFIGURE 4. (a) Learning curves illustrating DDPG from Demonstrations\n(DDPGfD) performance on the clip insertion task with different amounts\nof demonstration data. DDPGfD demonstrates the ability to solve\nsparse-reward tasks effectively with minimal human demonstration,\nshowcasing robust learning capabilities. (b) Results from 2 runs\nconducted on a physical robot. DDPGfD exhibits accelerated learning\ncompared to DDPG and achieves performance without requiring\nhandcrafted reward functions, as shown in [30, Figure 4].\nB. METHODS WITH ROBUSTNESS AND ADAPTABILITY\nRobustness and adaptability are critical aspects of RL,\nrobustness ensures RL agents perform well under uncertain\nor changing conditions by handling disturbances and model\ninaccuracies, while adaptability allows RL models to adjust\nto new environments, tasks, or dynamics, ensuring flexibility\nbeyond initial training [31].\n1) LEADER-FOLLOWER FRAMEWORK\nReward shaping offers a way to enhance robustness [31], [32].\nA leader-follower framework employs a technique similar to\nthat described in [33], where the leader, by modifying the\nfollower’s reward function, seeks to influence their actions\nin the desired direction, therefore enhancing the robustness\nof the system.\nNevertheless, when dealing with real-world rewards col-\nlected from sensors, these sensors are often affected by noise,\nwhich can distort reward signals and result in sub-optimal\nperformance in RL models. To tackle these issues, [32]\nintroduces a robust RL framework that uses a confusion\nmatrix to estimate and correct noisy rewards. This framework\nincorporates an unbiased estimation algorithm designed to\nfunction without making assumptions about the underlying\ndistribution of errors. The method involves defining unbiased\nsurrogate rewards ˆr based on estimated confusion matrices\n[32, Theorem 1] and applies the Q-learning algorithm with\nsurrogate rewards [32, Equation 3]:\nQt+a(st, at) = (1 −αt)Q(st, at)\n+ αt\n\u0014\nˆrt + γ max\nb∈A Q(st+1, b)\n\u0015\n,\n(9)\nwhere α ∈\n(0, 1)\n: the learning rate, will converge\nto the optimal Q −function as long as P\nt αt = ∞and\nP\nt α2\nt < ∞. Experimental validation on platforms such\nas OpenAI Gym and Atari games demonstrate notable\nperformance improvements for trained policies, especially\nwhen combined with the Proximal Policy Optimization\n(PPO) algorithm. This integration results in higher expected\n175478\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nrewards under noisy conditions, as shown in Figure 5.\nAdditionally, the framework sometimes achieves greater\ncumulative rewards by utilizing noise for better exploration\nand employing noise-removal mechanisms.\nFIGURE 5. Learning curves from five rewards robust RL algorithms on\nCartPole game with true rewards (r) ■, noisy rewards (˜r) ■,\nsample-mean noisy rewards ■, estimated surrogate rewards (˙r) ■, and\nsample-mean estimated surrogate rewards ■, [32, Figure 4].\nHowever, the method’s computational complexity may\npresent challenges, especially in real-time or resource-limited\nscenarios. Additionally, its effectiveness across various noise\npatterns and environments hinges on precise noise character-\nization and parameter tuning. Ultimately, this process can be\nviewed as a form of reward engineering, aimed at adjusting\nor refining the reward function to mitigate noise impact and\nenhance learning process reliability.\nSimilarly, to ensure that reinforcement learning (RL)\npolicies meet specific control criteria, [34] introduces a\nsystematic approach for shaping rewards to align optimal\npolicy trajectories with these requirements. This method\ntackles the challenge of achieving control objectives such as\nsettling time and steady-state error without explicit models of\nsystem dynamics.\nC. EXPLORATION STRATEGIES\nIn RL, exploration strategies are techniques that balance the\ntrade-off between exploration (discovering new information)\nand exploitation (leveraging existing knowledge) [35]. These\nstrategies help RL agents learn optimal policies by exploring\ntheir environment while exploiting learned knowledge.\n1) HASH-BASED REWARD SHAPING\nThe effectiveness of count-based exploration methods in\nsmall, discrete spaces contrasts with the challenges they face\nin larger, continuous environments where state re-encounters\nare infrequent. In their study on extending count-based\nexploration to high-dimensional and continuous state spaces,\n[36] introduced the use of hash codes to facilitate state\ncounting and exploration. This method employs static hash-\ning techniques, such as locality-sensitive hashing SimHash,\nwhich retrieves a binary code of state s ∈S as described in\n[36, Equation 2]:\nφ(s) = sgn(Ag(s)) ∈{−1, 1}k,\n(10)\nwhere g is an optional preprocessing function and A is a\nmatrix with i.i.d. entries drawn from a standard Gaussian\ndistribution N(0, 1). Additionally, the study uses learned\nhashing via autoencoders (AE) to assign exploration bonuses\nbased on state visitation counts, as illustrated in Figure 6.\nFIGURE 6. The architecture of the AE, [36, Figure 1].\n2) VARIATIONAL INFORMATION MAXIMIZING EXPLORATION\n(VIME)\nVIME is an exploration strategy designed for continuous\ncontrol tasks, focusing on maximizing the agent’s under-\nstanding of the environment’s dynamics. It employs varia-\ntional inference within Bayesian neural networks, enabling\nefficient handling of complex state and action spaces. VIME\nencourages exploration by using information gained from the\nlearned dynamics model as intrinsic rewards. This motivates\nthe agent to seek both external rewards and novel experiences.\nEmpirical evidence (see Figure 7) shows that VIME\noutperforms heuristic exploration methods across various\ncontinuous control tasks, including those with sparse rewards.\nHowever, the reliance on Bayesian neural networks for\ndynamics modeling introduces computational complexity.\nFuture research should explore alternative methods for quan-\ntifying surprise and using the learned dynamics model for\nplanning. VIME’s adaptability to high-dimensional spaces\nand potential applicability across different reinforcement\nlearning domains are strengths. However, challenges remain\nin tuning hash functions and ensuring robust performance\nacross environments due to sensitivity to the quality of state\nrepresentations.\nThe results illustrated in Figure 7 compare Trust Region\nPolicy Optimization (TRPO) (baseline), TRPO-SimHash,\nand\nVariational\nInformation\nMaximizing\nExploration\nVOLUME 12, 2024\n175479\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\n(VIME)\n[37]\non\ntasks\nsuch\nas\nMountainCar,\nCart-\nPoleSwingup, HalfCheetah, and SwimmerGather.\nFIGURE 7. The mean average return of different algorithms on rllab [38]\ntasks with sparse rewards. The solid line shows the mean average return,\nwhile the shaded area represents one standard deviation, over 5 seeds\nfor the baseline and SimHash (with baseline curves overlapping the axis).\nCount-based exploration with hashing enables goal achievement in all\nenvironments (indicated by a nonzero return), while baseline TRPO with\nGaussian control noise fails completely. TRPO-SimHash effectively\ncaptures the sparse reward on HalfCheetah but does not perform as well\nas VIME. SimHash’s performance is comparable to VIME on MountainCar\nand surpasses VIME on SwimmerGather [36, Figure 3].\n3) ONLINE REWARD SHAPING (EXPLORES)\nTo enhance the learning efficiency of reinforcement learning\n(RL) agents dealing with sparse or noisy reward signals, [39]\nintroduce Exploration-Guided Reward Shaping (EXPLORS).\nThis framework combines intrinsic reward learning with\nexploration-based bonuses in a fully self-supervised manner,\naiming to maximize the agent’s effectiveness in obtaining\nextrinsic rewards:\nAlgorithm 1 Online Reward Shaping, [39, Algorithm 1]\n1: Input: Extrinsic reward ¯R, and RL algorithm L\n2: Initialization: π0, ˆR0\n3: for k = 1, 2, . . . , K do\n4:\nupdate policy πk ←L(πk−1, ˆRk−1)\n5:\nupdate reward ˆRk using ˆRk−1 and πk\n6: end for\n7: Output: πK\nExperimental findings across various environments val-\nidate the method’s effectiveness in accelerating learning\ncompared to conventional RL approaches, demonstrating\nits capability to surpass the limitations of traditional\nreward shaping methods, as shown in Figure 8. This is\nparticularly evident in scenarios where traditional methods\nare impractical or ineffective. However, the study also\nacknowledges several constraints, including the need for\nextensive evaluation in more complex environments and the\nchallenges associated with effectively combining intrinsic\nrewards with exploration bonuses.\nFIGURE 8. a) Room0, REINFORCE b) Room+, REINFORCE c) LineK0,\nREINFORCE d) LineK+, REINFORCE. These plots illustrate the agent’s\nconvergence in performance relative to training episodes. (a, b) display\nresults for the REINFORCE agent on Room0 (ROOM variant without any\ndistractor state) and Room+ (ROOM variant with a distractor state). (c, d)\npresent results for the REINFORCE agent on LineK0 (LineK variant without\nany distractor state) and LineK+ (LineK variant with distractor states) [39,\nFigure 5].\nWithin the same framework, [40] introduce Reward\nUncertainty for Exploration (RUNE), a method within\npreference-based RL algorithms that integrates uncertainty in\nlearned reward functions as an exploration bonus. By incor-\nporating the variance in predictions from an ensemble of\nreward functions optimized to align with human feedback,\nRUNE aims to enhance exploration in environments where\nlearning is guided by human preferences, as depicted in\nFigure 9.\nFIGURE 9. The agent interacts with the environment and learns an\nensemble of reward functions based on teacher preferences. Each\nstate-action pair’s total reward combines the extrinsic reward with the\nmean of the ensemble’s predicted values and the intrinsic reward with\nthe standard deviation among the ensemble’s predictions [40, Figure 1].\nThis approach offers several benefits: it provides a\nsystematic way to balance exploration and exploitation by\nprioritizing actions with higher uncertainty in expected\nrewards, potentially accelerating learning and improving\nsample efficiency compared to traditional preference-based\n175480\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nRL methods. Experimental results in Figure 10 demonstrate\nRUNE’s effectiveness in enhancing asymptotic success rates\nand overall feedback efficiency during training scenarios.\nHowever, challenges remain, such as developing robust\ntechniques to manage varying levels of reward uncertainty\nand addressing the computational overhead of maintaining\nan ensemble of reward function predictions. Nevertheless,\nthis work represents a significant advancement in RL\nresearch by proposing a mechanism to integrate reward\nuncertainty into exploration strategies, paving the way for\nfuture developments in adaptive learning algorithms driven\nby human feedback.\nFIGURE 10. Learning curves on robotic manipulation tasks measuring\nsuccess rates. Exploration methods consistently improve the sample\nefficiency of PEBBLE. Notably, RUNE shows larger gains compared to\nother existing exploration baselines. The solid line represents the mean\nand the shaded regions denote standard deviations across five runs [40,\nFigure 3].\nD. POLICY PARAMETERIZATION\nPolicy parameterization refers to explicitly modeling the\npolicy as a function of learnable parameters (often denoted\nas θ). These parameters determine the agent’s behavior,\nand common choices include neural network weights [41].\nBy optimizing these parameters, we improve the policy’s\nperformance through gradient-based updates.\nContinuous control tasks traditionally rely on complex\nneural network methods. However, [42] examines the effec-\ntiveness of simpler policy parameterizations, such as linear\nand Radial Basis Function (RBF) policies, in these settings.\nThe primary concept is to enrich the representational capacity\nby using random Fourier features of the observations. These\nfeatures are defined as [42, Equation 6]:\ny(i)\nt\n= sin\n P\nj Pijs(j)\nt\nv\n+ φ(i)\n!\n,\n(11)\nwhere each element of Pij is drawn from N(0, 1), v is\nthe bandwidth parameter, and φ is a random phase shift.\nThe study shows that these streamlined policies can achieve\ncompetitive performance on benchmarks such as OpenAI\nGym and can be trained faster than neural networks.\nNevertheless, conventional training methods often result in\npolicies susceptible to perturbations, which is critical for real-\nworld applications. By diversifying initial state distributions\nduring training, the research demonstrates enhanced policy\nrobustness, similar to the benefits observed with model\nensembles and domain randomization.\nFIGURE 11. Learning curves for the Linear and RBF policy architectures.\nThe green line indicates the reward achieved by neural network policies\non the OpenAI Gym website as of 02/24/2017 (trained with TRPO), [42,\nFigure 1].\nDespite these advantages, simple policy approaches may\nface challenges. Their scalability to extensive state and action\nspaces can be limited compared to the adaptability of neural\nnetworks. Sensitivity to initial state distributions necessitates\ncareful tuning for robustness. In complex scenarios demand-\ning intricate behaviors, simple policies may not match the\npeak performance achievable with advanced neural networks.\nAdditionally, their rigidity may hinder adaptation to dynamic\nor intricate environments compared to more flexible neural\nnetwork architectures.\nE. INVERSE REWARD DESIGN\nInverse Reward Design (IRD) tackles the challenge of infer-\nring the true objective behind a designed reward function.\nInstead of manually engineering rewards, IRD allows RL\nagents to learn from expert demonstrations and deduce the\nunderlying intention driving those actions.\nIn RL, the agent selects an action in a known state and\nreceives a reward generated by a reward function R that\nmay be unknown to the agent The state transitions are\nbased on the previous state and action, which is described\nby the transition function T, which may also be unknown.\nConversely, in Inverse Reinforcement Learning (IRL), the\ninputs and outputs for the learner L are reversed. L observes\nthe states and actions {(s, a), (s, a), . . . , (s, a)} of an expert E\n(or its policy πE), and learns a reward function ˆRE that best\nexplains E’s behavior as the output [43].\nIRL emphasizes the inference of the reward function\nthat an expert agent is presumed to be maximizing based\non observed behavior. This approach provides a robust\nframework for understanding and designing reward structures\nin various RL applications. This paper aims to underscore\nthe significance of IRL and to enhance the discourse by\nproviding an overview of the primary categories of IRL\nmethodologies [43], [44], [45], [46], [47], [48].\nThe primary categories of IRL methods are classified as\nfollows:\n1) Model-Based IRL: In this approach, the reward func-\ntion is inferred alongside a model of the environment’s\ndynamics. The agent learns both the reward structure and\nVOLUME 12, 2024\n175481\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nthe transition probabilities T(s′|s, a). A common model-\nbased method is Maximum Entropy IRL, which seeks to\nmaximize the entropy of the policy while explaining the\nobserved behavior.\n2) Model-Free IRL: These methods directly infer the\nreward structure from demonstrations without explicitly\nmodeling the environment. For instance, algorithms\nlike Fitted IRL and Bayesian IRL estimate the reward\nfunction based solely on observed state-action pairs.\nThe objective is to minimize the difference between the\nexpected return of the inferred policy and that of the\nexpert.\n3) Deep\nInverse\nReinforcement\nLearning:\nRecent\nadvancements in deep learning have led to the\ndevelopment of deep IRL techniques, which utilize\nneural networks to model complex reward functions.\nThese methods, such as deep maximum entropy IRL,\nallow for the representation of intricate behaviors and\nexpand the applicability of IRL in real-world scenarios.\nThe learned reward function can be expressed as:\nˆR(s, a) = fθ(s, a),\n(12)\nwhere fθ is a neural network parameterized by θ.\nIt is important to note that the learned reward function\nmay not exactly correspond to the true reward function,\nas illustrated in Figure 12.\nFIGURE 12. The figure depicts RL and IRL. The subject agent (shaded in\nblue), [43, Figure 3].\nCoinciding with previous work, [49] proposed Inverse\nReward Design (IRD), an approach that approximates the\ntrue reward function by treating the proxy reward as expert\ndemonstrations. This method aids in planning risk-averse\nbehavior and addresses problems like misspecified rewards\nand reward manipulation. IRD prevents harmful behaviors\nby avoiding dangerous areas, as shown in experiments with\nrobots avoiding lava Figure 13. It also mitigates reward\nmanipulation by treating designed rewards as observations\nrather than fixed objectives. IRD’s risk-averse planning\nensures agents avoid both known hazards and potential\nrisks, enhancing safety and reliability. Additionally, IRD\nsystems are robust to changes in high-dimensional feature\nspaces. However, IRD faces computational challenges in\nsolving planning problems during inference and often relies\non simplified assumptions about reward functions and\nenvironments, limiting its applicability in complex scenarios.\nAccurate inference remains challenging, potentially leading\nto sub-optimal behaviors.\nFIGURE 13. The results comparing [49] method to a baseline that directly\nplans with the proxy reward function, [49, Figure 4].\nF. REWARD HORIZON\nThe impact of using shaping to reduce the reward horizon is\nillustrated through a straightforward algorithm, Algorithm 2,\nthat guarantees learning time is polynomial relative to the size\nof the critical region and independent of the MDP’s size [50].\nAlgorithm 2 The Horizon_Learn, [50, Figure 1]\n1: Input: MDP M and Reward Horizon H\n2: Initialization: π\n3: while successive episodes occur visiting only known\nstates do\n4:\nAssign s as the current state of M\n5:\nif s is terminal then\n6:\nreset to s0\n7:\nend if\n8:\nif s is known then\n9:\nexecute π(s)\n10:\nelse\n11:\nexecute any policy in mH(s) that still needs to be\nexplored\n12:\nif s becomes known then\n13:\nSelect the action for s in the optimal policy of\nmH(s), a\n14:\nSet π(s) = a\n15:\nend if\n16:\nend if\n17: end while\n18: return π\nBy shortening the reward horizon, they identify easier-to-\nlearn MDPs, indicating that standard RL algorithms, such\nas Q-learning with ε-greedy exploration, can greatly benefit\nfrom this technique. Experimental data support that reducing\nthe reward horizon speeds up learning, with shaping strategies\nleading to faster performance improvements compared to\nno shaping, as shown in Figure 14. Specifically, the time\nrequired to reach a certain performance level is significantly\nreduced with shaping: algorithms that explore based on\nreward feedback are notably quicker when the reward horizon\nis minimized.\nThe study emphasizes the crucial role of the reward\nhorizon in determining the complexity of learning tasks,\n175482\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nFIGURE 14. Average learning curves as the reward horizon is reduced\nfrom no shaping to shaping every action, [50, Figure 2].\nshowing that shaping can dramatically decrease the time\nrequired for RL agents to learn optimal policies. However,\naccurately determining and implementing the appropriate\nreward horizon is challenging, as improper shaping might not\nyield the expected acceleration in learning.\nG. POTENTIAL BASED METHODS\nPotential-based methods [11] in RL focus on shaping the\nvalue function to guide an agent’s behavior. By introducing\nauxiliary potentials, these methods encourage desired states\nand actions, leading to improved exploration and conver-\ngence. These methods usually involve defining a potential\nfunction, φ(s), over the state space, which captures the agent’s\ndesired progress towards a goal state.\nR′(s, a) = R(s, a) + γ [φ(s′) −φ(s)].\n(13)\nGuiding RL agents toward goals can be improved by ana-\nlyzing cumulative rewards from episodes [51]. By reinforcing\nreward signals based on episode rewards, they proposed the\nPotential-Based Reward Shaping (PBRS) method to enhance\nlearning efficiency and performance in both single-task and\nmulti-task environments within the Arcade learning domain\nFigure 15. The reward function must also handle sparse\nreward signals by analyzing agent transitions in environments\nwith sparse rewards. The method proposed the following\nfunction [51, Equation 12]:\nφ(s, a, t) =\n\n\n\n0\nR(s, a) = 0\n1 +\nRep −Rep\nu (t)\nRep\nu (t) −Rep\nl (t)\nO.W,\n(14)\nwhere R(s, a) is the immediate reward, Rep is the sum of\nrewards in the current episode (episode reward), Rep\nu (t) is the\nminimum value of episode reward until now, and Rep\nl (t) is the\nmaximum value of episode reward until now. The potential\nfunction discourages unproductive states, reinforces positive\nrewards, and indicates the significance of negative rewards.\nThis dynamic adjustment ensures continuous improvement\nand efficient exploration.\nFIGURE 15. Results of PBRS on Pong and Breakout, [51, Figure 5].\nEvaluations during learning and final policy assessments\nshow that this method competes well with baseline meth-\nods, particularly in multitasking scenarios, thus advancing\ntechniques for speeding up RL algorithms and improving\nadaptability in complex environments.\n1) THEORETICAL FOUNDATIONS FOR MULTI-AGENT\nSYSTEMS (MAS)\nTo investigate the theoretical implications of potential-based\nreward shaping in multi-agent systems, this research [20]\nextends previous findings, demonstrating that this technique\nmaintains equivalence to Q-table initialization and does not\naffect the Nash Equilibria of the underlying stochastic game.\nCrucially, the study empirically reveals that potential-based\nshaping influences exploration, potentially leading to dif-\nferent converged joint policies. While the research focuses\non fully observable domains, it highlights the potential of\npotential-based reward shaping for incorporating heuristic\nknowledge into multi-agent learning. The authors suggest\nthat this technique can increase the likelihood of converging\nto a higher global utility and reduce convergence time,\npotentially mitigating the risks associated with unintended\ncyclical policies.\n2) PBRS-MAXQ-0 METHOD\nPotential Based Reward Shaping (PBRS) and MAXQ are\ntwo extensively utilized techniques in reinforcement learning.\nHarutyunyan et al. [52] introduced PBRS-MAXQ-0, a novel\nalgorithm that merges these methods within a hierarchical\nreinforcement learning (HRL) framework. PBRS-MAXQ-0\nseeks to integrate heuristics into HRL tasks both effectively\nand efficiently, offering theoretical convergence guarantees\nunder specific conditions, independent of additional rewards\napplied. Evaluations underscore several benefits: with appro-\npriate heuristics, PBRS-MAXQ-0 notably accelerates con-\nvergence relative to the standard MAXQ-0 algorithm and\ncompetes well with other advanced MAXQ-based methods.\nImportantly, even with misleading heuristics, PBRS-MAXQ-\n0 shows resilience, eventually achieving convergence after\nprolonged learning periods, as illustrated in Figure 16.\nVOLUME 12, 2024\n175483\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nFIGURE 16. Performances of PBRS-MAXQ-0 in the Fickle Taxi problem,\nwhere PBRS-MAXQ-0 with reasonable heuristics (denoted by PBRS-good)\nand with misleading heuristics (denoted by PBRS-bad) [52, Figure 2].\nHowever, integrating PBRS into MAXQ presents chal-\nlenges, such as optimizing heuristic values and ensuring\nrobust convergence across diverse environments. Fine-tuning\nPBRS parameters and maintaining heuristic accuracy are\nessential for maximizing the algorithm’s efficacy across\nvarious HRL scenarios.\n3) POTENTIAL-BASED ADVICE\nPotential-based shaping functions ensure that policies learned\nwith shaped rewards remain effective in the original MDP,\nmaintaining near-optimal policies [11]. On the other hand,\nReward shaping involves augmenting the reward function of\nMDP to accelerate learning by providing additional guidance\nbeyond the intrinsic rewards of the MDP.\nThis paper [53] presents a novel framework for expressing\narbitrary reward functions as potential-based advice within\nthe context of reinforcement learning (RL). This method\nensures policy invariance by learning an auxiliary value\nfunction, derived from a modified version of the original\nreward function. The potential-based approach facilitates\nefficient encoding of behavioral domain knowledge, leading\nto significant improvements in learning speed compared to\nexisting methods. Notably, the framework introduces min-\nimal computational overhead, as maintaining the auxiliary\nvalue function requires only linear time and space complexity.\nWhile the proposed method demonstrates theoretical\nsoundness, it currently lacks practical applications and\nspecific algorithms. The paper primarily focuses on theo-\nretical concepts, leaving further development of practical\nimplementations for future research. The authors highlight\nthe challenges associated with different reward shaping\napproaches: while potential-based methods offer guarantees,\nthey often require extensive domain knowledge for effective\nimplementation; auxiliary task-based methods offer flexibil-\nity but can become complex as the number of auxiliary tasks\nincreases; and intrinsic motivation methods while promising\nfor encouraging exploration, can be sensitive to parameter\ntuning.\nThe authors conclude that effective reward shaping\nrequires a nuanced understanding of the specific problem\ndomain and a careful balance between theoretical soundness\nand practical considerations. Future research should prioritize\nthe development of more automated and robust reward\nshaping techniques to advance the field of RL.\n4) PBRS IN EPISODIC REINFORCEMENT LEARNING\nPBRS in episodic RL, explored by [54], sheds light on\nits applications in model-free, model-based algorithms, and\nmulti-agent RL. It examines reward shaping in episodic tasks\nlike games, revealing insights: potential-based shaping alters\nequilibria in stochastic games, introduces new equilibria with\nnon-zero terminal state potentials, and reevaluates its role\nin PAC-MDP learning. It challenges the need for admissible\npotential functions, proving ∀s ∈Unkown, 8(s) ≥0 ensures\noptimistic exploration. The study provides analytical jus-\ntification for PBRS, crucial in episodic RL with distinct\ninitial and terminal states, emphasizing The potential role in\nenhancing learning efficiency.\n5) CONSTRAINED RL WITH POTENTIAL-BASED REWARD\nFUNCTIONS (PBRF)\nTo generate safety-oriented aspects of reward functions\nfrom verified hybrid systems models [55] proposed an\napproach of using logically constrained RL to integrate\nformal methods and RL. Demonstrated on a standard RL\nenvironment for longitudinal vehicle control, this method\nshowed faster convergence during training with augmented\nreward functions, particularly with logically constrained and\npotential-based reward functions (PBRF). The study found\nthat partly auto-generated reward functions produced agents\nthat generally maintained the safety level of hand-tuned\nreward functions, and reward scaling could emphasize certain\naspects of the generated rewards. The training process\nwas evaluated in terms of the number of epochs until\nconvergence and the average accumulated reward across\nevaluation periods.\n6) DIFFERENCE REWARDS INCORPORATING\nPOTENTIAL-BASED REWARD SHAPING (DRIP) AND\nCOUNTERFACTUALS AS POTENTIAL (CAP)\nDRIP is introduced as a novel reward shaping technique\nfor multi-agent reinforcement learning [56]. DRIP combines\ndifference rewards, which incentivize agents to contribute\nto the overall system performance, with potential-based\nreward shaping. Potential-based shaping (see Equation 16)\naccelerates learning and maintains desired Nash equilibria\nby modifying the reward function using a potential function\nderived from domain-specific knowledge. While effective,\nDRiP requires a carefully designed potential function based\non deep domain understanding, which can be challenging\nand time-consuming to obtain. CaP, on the other hand, auto-\nmatically generates a dynamic potential function, eliminating\nthe need for manual design and guaranteeing stable Nash\n175484\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nequilibria. Although both CaP and DRiP capture similar\nknowledge, combining them doesn’t provide extra benefits\nand can even negatively impact performance. For appli-\ncations requiring theoretical guarantees, CaP is preferred;\nfor performance priority, DRiP is recommended, especially\nwhen domain knowledge is readily available. The work [56]\ntested these techniques in various domains, showcasing\nDRiP’s consistent outperformance in accelerating learning\nand achieving better policies.\nH. DYNAMIC POTENTIAL-BASED REWARD SHAPING\n(DPBRS)\nFor\nimproving\nreinforcement\nlearning\nin\nsingle\nand\nmulti-agent systems, this method introduces a dynamic\npotential-based function to perform the reward shaping, and\nto guide agents without affecting the optimal policy [57]. This\nfunction changes over time, adapting to the current state of the\nagents and the environment. The idea can be represented as\nfollows through the formula of Q-Learning:\nQ(s, a) ←Q(s, a) + α[r + F(s, s′)\n+ γ max\na′ Q(s′, a′) −Q(s, a)],\n(15)\nwhere F(s, s′) is the general form of any state-based shaping\nreward:\nF(s, s′) = γ 8(s′) −8(s).\n(16)\nFor the dynamic potential based, we can extend Equation 16\nand include t; the agent’s arrival time in the previous state s,\nand t′ is its arrival time at the current state s′ (i.e., t < t′):\nF(s, t, s′, t′) = γ 8(s′, t′) −8(s, t).\n(17)\nThe DPBRS maintains existing guarantees, its advantages\ninclude its ability to improve decentralized multi-agent learn-\ning through carefully designed, domain-specific potential\nfunctions. These functions foster cooperation by addressing\nchallenges like coordination, information asymmetry, and\nscalability. While the potential benefits of this approach\ninclude faster convergence, enhanced cooperation, and\ngreater robustness, the method lacks comparisons with other\nalgorithms and lacks examples in robotics or highly non-\nlinear systems, where it was tested on a 2-D maze for\nSingle-Agent learning as we see in Figure 17, and on\nBoutilier’s Coordination Game for Multi-Agent learning.\nI. UPPER CONFIDENCE BOUND VALUE ITERATION\n(UCBVI)\nWell-designed reward shaping can significantly improve\nsample complexity and enhance exploration, leading to\nbetter performance compared to uninformed exploration\nstrategies. This is demonstrated in the following work, which\ninvestigates the benefits of reward shaping in reinforcement\nlearning (RL) through a combination of theoretical analysis\nand experimental validation.\nFIGURE 17. DPBRS single-agent maze results [57, Figure 2].\n1) UCBVI\nThe UCBVI algorithm [58] is a method for shaping rewards\nthat extends the Value Iteration technique, ensuring that the\nresulting value function serves as an upper confidence bound\n(UCB) with a high probability on the optimal value function,\nIt is important to note that this algorithm bears similarity\nto model-based interval estimation (MBIE-EB) [59]. The\nauthors present proofs for their theorems and introduce\nseveral modified versions, including UCBVI-CH, which\nincorporates Chernoff Hoeffding’s concentration inequality.\nHowever, no empirical results are provided.\n2) UCBVI-SHAPED\nA modified version of the UCBVI algorithm [58] incorpo-\nrates reward shaping to modify bonuses and value function\nprojection [60]. Their analysis reveals that reward shaping\ncan effectively prune irrelevant parts of the state space, sharp-\nening optimism in a task-directed manner. This reduction\nin state space dependence leads to improved regret bounds\nand sample complexity benefits while retaining asymptotic\nperformance.\nThe research was conducted on maze environments,\ncomparing the performance of UCBVI-shaped with the\nstandard UCBVI algorithm. Their findings show that reward\nshaping can enhance the performance of UCBVI, particularly\nin environments where agents are prone to wasting time\nexploring irrelevant areas.\nThis work contributes to a deeper understanding of reward\nshaping’s impact on sample complexity and its potential\nto guide exploration more efficiently. It encourages future\nresearch to incorporate reward shaping more formally into\nsample complexity analysis, moving away from reward-\nagnostic approaches.\nJ. DIFFERENCE REWARDS (D)\nFor single agent systems, difference rewards enhance the\noriginal reward function by incorporating a difference term\nthat measures the discrepancy between the agent’s current\nstate and a designated reference state. This difference term\nserves as a guiding force, encouraging the agent to transition\nVOLUME 12, 2024\n175485\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\ntowards the reference state, which could be a desired goal\nstate or a state representing a desirable condition. The\nmodified reward function takes the form:\nR′(s, a) = R(s, a) + γ [D(s′, r) −D(s, r)],\n(18)\nwhere R(s, a) is the original reward for taking action a in\nstate s, R′(s, a) is the modified reward, γ is a discount\nfactor, D(s, r) is the difference function, which measures the\ndifference between the current state s and the reference state\nr, and s′ is the next state after taking action a.\nThe simplicity of difference-based reward shaping makes\nit adaptable to various tasks and environments. However, its\neffectiveness hinges on a carefully chosen reference state, and\nin complex scenarios, it may not provide as comprehensive\ninformation as alternative methods.\nFor Multi Agents Systems (MAS) We define Difference\nRewards Di as [61], and [62] defined it:\nDi(si, ai) = G(s, a) −G(s −i ∪sci, a −i ∪aci).\n(19)\nThe global system utility, denoted as (G(s, a)), depends on\nthe system state (s) and the joint action (a). A counterfactual\nterm, (G(s −i ∪sci, a −i ∪aci)), estimates the global\nutility without agent (i)’s contribution, considering states\nand actions excluding agent (i) and fixed states and actions\nindependent of agent (i). The main idea of Di in MAS\nis to encourage agents to contribute to the overall system\nutility by providing a reward that reflects the difference\nbetween the system’s performance with and without the\nagent’s contribution.\n1) INDIVIDUAL AND DIFFERENCE REWARDS\nRL can be used to find this optimal strategy in route\nchoice problems in road networks, and we can test two\ndifferent kinds of rewards. Individual rewards focus on the\nindividual agent’s benefit, leading to a selfish approach that\ncan exacerbate congestion. Difference rewards D, on the other\nhand, promote system-wide optimization, leading to a more\nefficient and equitable allocation of traffic.\nThis work [63] explores two reinforcement learning\napproaches, IQ-learning and DQ-learning, for solving the\nroute choice problem in road networks. Both use the\nsame configuration but differ in their reward function. IQ-\nlearning uses individual utility as the reward function, while\nDQ-learning uses a difference reward function that aims\nto maximize system utility. The difference reward function\nincentivizes agents to choose routes that minimize overall\ntravel time, leading to a more balanced distribution of traffic\nacross the network. Experiments show that DQ-learning\nsignificantly improves travel time compared to IQ-learning,\nsuccessive averages, incremental assignment, and all-or-\nnothing assignment.\nThe experiment’s findings, while significant, are specific\nto the scenario tested and may not generalize to other\nenvironments. It’s important to note that both algorithms\nrequire an initial exploration phase to learn the environment\neffectively, and evaluating performance prematurely can lead\nto inaccurate conclusions. As we can see in the Figure 18,\nboth algorithms tend to have similar convergence, but during\nthe final episodes, DQ-learning shows better results. The\nstudy uses statistical testing with Gaussian distribution and\nconfidence intervals to demonstrate the significance of DQ-\nlearning’s improvement. However, there are concerns about\ngeneralizing these results to other domains due to the specific\nfocus on traffic assignment and the inherent assumptions\nabout driver behavior in the model.\nFIGURE 18. The main plot depicts convergence times and standard\ndeviations across all episodes, while the inset focuses on the final\nepisodes [63, Figure 3].\nK. KNOWLEDGE-BASED MULTI-OBJECTIVE MULTI-AGENT\nREINFORCEMENT LEARNING (MOMARL)\nReward\nshaping\ntechniques\nDifference\nRewards\nand\npotential-based reward shaping are popular methods for\nreward shaping. The research cited as [62] compares and\nevaluates these techniques in two studies, one using a\nnovel benchmark problem called the Multi-Objective Beach\nProblem Domain (MOBPD) and the other using the Dynamic\nEconomic Emissions Dispatch problem.\nThe results demonstrate that both D and PBRS can\neffectively guide agents towards Pareto optimal solutions\nin MOMARL domains, confirming that appropriate reward\nshaping is crucial in these settings. However, both techniques\nhave limitations: D requires global knowledge of the system\nand the mathematical form of the evaluation function, while\nPBRS necessitates handcrafted potential functions which can\nbe time-consuming and challenging to design effectively.\nFurthermore, the study found that D generally outperforms\nPBRS in terms of performance, especially when the required\nconstraints are met.\nThe study concludes that the optimal technique for a\ngiven MOMARL application depends on specific constraints,\nsuch as the availability of system knowledge, bandwidth for\ncommunication, and the designer’s expertise. The work estab-\nlishes the MOBPD as a benchmark for future MOMARL\n175486\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nresearch and highlights the need for further investigation into\nthe design and application of these reward shaping methods.\nL. PLAN BASED METHODS\nPlan-based methods combine the advantages of model-based\nplanning and RL. These methods allow agents to simulate\n‘‘what-if’’ scenarios and generate policy updates without\ncausing state changes in the environment. Imagine it as agents\nusing their imagination to explore different paths before\ntaking real actions.\n1) UTILIZATION OF STRIPS\nPlan-based reward shaping, as discussed by [64], uti-\nlizes domain knowledge to accelerate the convergence\nand improve the optimality of RL methods. It effec-\ntively addresses exploration challenges when integrated\nwith model-free approaches. While model-based methods\ncan mitigate these issues independently, the addition of\nreward shaping consistently boosts learning efficiency. The\nstudy introduces a STRIPS-based reward shaping technique,\nutilizing a Stanford Research Institute Problem Solver\n(STRIPS) representation of actions and goals through\npreconditions and effects. Compared to traditional MDP-\nbased approaches, STRIPS-based shaping directs agents\nmore effectively toward optimal policies. It suggests refining\nMDP-based reward shaping by focusing on the best path\nderived from the STRIPS plan rather than the entire\nstate space’s value function. Evaluations demonstrate the\nrobustness of STRIPS-based shaping against plan knowledge\nerrors, highlighting its superiority in enhancing policy quality\nand convergence speed. This approach provides a viable\nalternative to MDP-based methods, offering domain experts\nflexibility in expressing and utilizing domain knowledge.\n2) COMPARING PLAN BASED TO ABSTRACT MDP\nSimilarly [65] compared two reward shaping methods: plan-\nbased and abstract MDP. The plan-based method supple-\nments an agent’s actions with additional rewards based on\npredefined plans. In contrast, the abstract MDP approach\ninvolves solving a higher-level MDP to shape behavior\nusing its value function. The comparison evaluates these\nmethods in terms of total reward, convergence speed, and\nscalability to complex environments. In large-scale settings,\nthe plan-based method outperforms abstract MDP by offering\ndetailed, sequential guidance to agents. However, in multi-\nagent scenarios with conflicting goals, abstract MDP excels\ndue to its ability to manage coordination challenges better\nthan plan-based methods. This highlights the importance\nof selecting reward shaping methods based on specific\nenvironmental characteristics to optimize agent performance\neffectively.\nM. BELIEF REWARD SHAPING (BRS)\nBelief Reward Shaping (BRS) [66] enhances reinforce-\nment learning by incorporating prior knowledge about the\nenvironment’s reward structure. It augments the standard\nreward signal with ‘‘belief rewards’’ derived from a Bayesian\nframework, reflecting prior assumptions or beliefs about the\ndistribution of rewards. This approach diverges from tradi-\ntional methods by avoiding sole reliance on environmental\ninteractions for learning the reward structure. BRS directly\nprovides shaping rewards based on both the state and action,\naddressing a limitation of potential-based reward shaping\n(PBRS), which only considers the state.\nBy integrating these prior beliefs, the authors suggest\nthat an agent’s reward should be influenced not only by\nexternal environmental sensations but also by its internal\nbelief system. This belief system, represented by an internal\ncritic, is dynamically updated based on both prior beliefs and\nenvironmental sensations. The critic then provides an updated\nreward to the agent, taking into account both its prior beliefs\nand the current sensory information. BRS leverages Bayesian\nmethods to specify prior beliefs on the environment’s reward\ndistribution, demonstrating that more complex and accurate\nprior beliefs lead to improved agent performance. Theo-\nretical guarantees for BRS’s consistency when augmenting\nQ-learning are provided, but these guarantees hold only if the\ntrue environment reward distribution falls within the critic’s\nhypothesized set of models.\nFIGURE 19. BRS and the relationship between the agent and the\nenvironement [66, Figure 1].\nN. BI-LEVEL OPTIMIZATION OF PARAMETERIZED\nREWARD SHAPING (BiPaRS)\nBiPaRS [10] is a novel approach to address the challenge\nof effectively utilizing shaping rewards in reinforcement\nlearning. Recognizing that human-designed reward functions\ncan be imperfect due to cognitive biases, BiPaRS employs\na bi-level optimization framework. This framework learns\nto adaptively utilize shaping rewards by optimizing a\nparameterized weight function for the shaping reward at the\nupper level, while simultaneously optimizing the policy using\nVOLUME 12, 2024\n175487\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nthe shaped reward at the lower level. This reward shaping\ntechnique modifies the original reward function by adding a\nparameterized shaping term. This technique is represented as\nfollows\n˜r(s, a) = r(s, a) + zφ(s, a)f (s, a),\n(20)\nwhere ˜r(s, a) denotes the shaped reward for taking action a\nin state s. r(s, a) is the original reward function.\nzφ(s, a) is the shaping weight function, parameterized by\nφ, which assigns a weight to each state-action pair and\nis parameterized by φ, and f (s, a) represents the shaping\nreward function and performs a bi-level optimization for\n˜r(s, a), which is a nested optimization problem. The aim\nis to improve an agent’s behavior by modifying the reward\nfunction it receives. We represent the agent’s policy as\nπθ, where θ denotes the policy’s parameters. The learning\nobjective is twofold, first optimizing the Policy The policy\nπθ is optimized with respect to a modified reward function\n˜r. The goal is to maximize the expected cumulative modified\nreward:\n˜J(πθ) = Es∼ρπ,a∼πθ [r(s, a) + zφ(s, a)f (s, a)].\n(21)\nSecond, optimizing the Shaping Function: The shaping\nfunction zφ is optimized to ensure that the policy πθ which\nmaximizes ˜J(πθ) also maximizes the expected cumulative\ntrue reward J(zφ):\nJ(zφ) = Es∼ρπ,a∼πθ [r(s, a)].\n(22)\nThis ensures that the shaping reward helps guide the agent\ntowards maximizing the true reward, even though zφ itself\ndoesn’t directly control the agent’s actions.\nTherefore, we have a bi-level optimization, an embedded\n(nested) optimization problem, to optimize the policy πθ and\nthe weight function zφ, as follows\nmax\nφ\nEs∼ρπ,a∼πθ [r(s, a)]\ns.t. φ ∈8\nθ = arg max\nθ0 Es∼ρπ,a∼πθ0[˜r(s, a)]\ns.t. θ ∈2,\n(23)\nwhere 8 and 2 represent the parameter spaces of the shaping\nweight function zφ and the policy πθ, respectively.\nThis method allows BiPaRS to identify and exploit benefi-\ncial shaping rewards while mitigating the impact of unhelpful\nor even detrimental ones. The paper demonstrates BiPaRS’s\neffectiveness through experiments on cartpole and MuJoCo\ntasks, Figure 20, showing its ability to leverage beneficial\nshaping and suppress negative influences. However, its\nperformance on MuJoCo tasks may not yet match the cutting\nedge in the DRL domain. BiPaRS holds promise for adapting\nand shaping rewards dynamically, but further research is\nneeded to enhance its performance and explore its broader\napplicability.\nO. REWARD SHAPING VIA HUMAN FEEDBACK\nHuman feedback serves as a powerful mechanism for\nenhancing reinforcement learning (RL) by effectively shap-\ning reward structures and accelerating the learning process.\nA burgeoning area of research within RL involves the\nintegration of human preferences to leverage this feedback,\nguiding agents toward desired behaviors and objectives.\nThis method seeks to bridge the gap between the agent’s\nlearning processes and human intuition, thereby fostering a\nmore effective and efficient learning environment. Notable\ntechniques, such as PEBBLE [67], SURF [68], RUNE [40],\nand others [69], [70], [71], exemplify the application of\nHuman-in-the-Loop (HITL) methodologies [72], [73], [74],\n[75], [76] to learn reward functions based on human input.\nReward shaping with human feedback is a specific\napplication of the HITL concept, where human input is used\nto modify or design reward functions that the RL agent uses\nto evaluate its actions. This method aims to influence the\nagent’s learning trajectory by providing more informative and\ncontext-specific rewards that align with human preferences.\nBy utilizing human feedback, the agent can learn more\neffectively, as the rewards better reflect the complexities of\nthe tasks at hand.\nWhile both HITL and reward shaping with human\nfeedback utilize human input to improve RL, they differ in\nfocus and application:\n• Scope of Human Interaction: HITL encompasses a\nbroader interaction framework where human operators\nprovide ongoing feedback, corrections, and guidance\nthroughout the learning process. Reward shaping specif-\nically targets the design and modification of reward\nfunctions based on human input, focusing primarily on\nthe reward structure rather than the overall learning\nprocess.\n• Learning Dynamics: HITL fosters a dynamic relation-\nship between humans and agents, facilitating real-time\nadjustments that can refine learning strategies on the fly.\nReward shaping is typically more static, where feedback\nis incorporated into the reward function design, although\nit can also involve iterative refinements based on human\ninput.\n• Objectives: The primary objective of HITL is to\nenhance the learning efficiency and effectiveness of\nagents by leveraging human intuition and expertise.\nReward shaping aims to provide agents with clearer\nand more relevant rewards, thereby accelerating the\nconvergence of learning algorithms toward optimal\nbehaviors.\nOne could model human preferences as a probabilistic dis-\ntribution over reward functions. For example, the likelihood\nof a reward function R given human feedback H can be\nmodeled as:\nP(R|H) = P(H|R)P(R)\nP(H)\n.\n(24)\n175488\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nFIGURE 20. Results of the MuJoCo experiment, where the shaded areas are 95% confidence intervals [10, Figure 19].\nHere, P(H|R) represents how likely the human feedback\nis given a specific reward function R, and P(R) represents a\nprior distribution over reward functions.\nBy maximizing the posterior probability P(R|H), one\ncan derive a reward function that aligns well with human\npreferences, thereby improving the learning efficiency\nand effectiveness of the RL agent. One of the methods\nthat incorporates large-scale language models (LLMs) is\nText2Reward, it uses LLMs to automatically create dense\nreward functions for reinforcement learning tasks, thereby\nreducing the reliance on domain expertise or extensive data\ncollection [77]. This approach has been practically applied to\na robotic arm performing two specific tasks: Pick Cube (i.e.,\npicking up a cube and moving it to a designated position)\nand Stack Cube (i.e., stacking one cube onto another),\nas illustrated in Figure 21.\nFIGURE 21. Sample images of real-world robot manipulation for the Pick\nCube and Stack Cube tasks, [77, Figure 5].\nBy utilizing natural language descriptions of desired\nobjectives, Text2Reward generates reward functions that\nare both interpretable and executable. These automatically\ngenerated rewards have shown superior performance in\nvarious robotic manipulation tasks compared to manually\ndesigned ones. The method’s effectiveness stems from its\nability to produce adaptable dense rewards, which have been\nproven successful in both simulated and real-world robotic\nmanipulation and locomotion tasks. However, Text2Reward\ncurrently faces challenges in handling highly complex tasks.\nAlthough the primary focus is on robotics, the authors\nanticipate broader applications in areas such as gaming, web\nnavigation, and household management, where automated\nreward design could significantly accelerate the creation of\nintelligent systems.\nThis work [78] proposes a Meta-Reward-Net (MRN),\na new framework for preference-based reinforcement learn-\ning that leverages Preference-based reinforcement learning\nthrough human feedback to improve learning efficiency. The\ngap between MRN and baselines diminishes as the amount\nof feedback increases, suggesting that sufficient feedback\nalleviates the limitations imposed by insufficient preference\nlabels. Furthermore, Bi-level optimization, as we’ve men-\ntioned can be an efficient tool for Reward Engineering, and\nwhen combined with reward learning and limited human\nfeedback, it can generate an accurate Q-table, compared with\nthe State-of-The-Art methods.\nMRN employs bi-level optimization to simultaneously\nlearn both reward functions and policies. This dual approach\nallows the agent to learn effectively, even with constrained\nhuman input, outperforming previous methods across a\nvariety of simulated robotic tasks. The authors assert that\ntraditional reward shaping methods are inadequate for achiev-\ning robust and efficient reinforcement learning. In summary,\nMRN enhances feedback efficiency by incorporating an\nadditional signal that considers the performance of the\nQ-function based on labeled preference data.\nExperiments conducted on Meta-world tasks illustrate\nMRN’s\nsuperior\nperformance\nwith\nlimited\nfeedback.\nVOLUME 12, 2024\n175489\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nHowever, the reliance on human-guided reward engineering,\nwhich prioritizes exploration, stability, and context-specific\ndesign, presents a potential drawback, as its efficacy depends\nheavily on the quality of human input. The paper emphasizes\nthe critical importance of iterative refinement and human\nfeedback in overcoming challenges such as sparse, delayed,\nand noisy rewards. While the primary focus of this research\nis on robotic simulations, it is noteworthy that the code for\nMRN is publicly available, enabling further implementation\nand testing by the research community, thereby facilitating\na continued exploration of HITL methodologies in RL\ncontexts.\nP. UNDERREPRESENTED REWARD SHAPING\nTECHNIQUES AND METHODS\nEvery RL method has a distinct reward shaping technique,\nparameterized values, design methods, algorithm, or envi-\nronment, and all of them share the existence of a reward\nfunction, to cover these issues we need to take a deeper look\ninto the literature, and that could not be feasible with the\nresources at hand. Nonetheless, we will name a few of the\nunderrepresented methods that were used to shape or design\nthe reward:\n1) REWARD SHAPING FOR DEEP Q-LEARNING IN POWER\nGRID CYBERATTACK DEFENSE\nDeep Q-learning is a powerful method in RL but one of\nits issues is that it might not be compatible with large\nenvironments with high dimensions or a large state space.\nNevertheless, it can be used to address critical issues such\nas the protection and security of smart power grids against\ncyberattacks [79], [80]. To overcome these challenges, the\nauthors of [79] introduce a deep Q-learning-based stochastic\nzero-sum Nash strategy solution. This approach utilizes a\ndeep neural network to learn a control policy that minimizes\nthe impact of attacks while considering the probabilistic\nnature of attack outcomes, and it uses reward shaping to find\nscenarios where the attacker can harm the network the most,\nand therefore find the best defense strategies.\nThe reward function [79, Equation 10] is designed to\nincentivize the attacker to cause as much damage as possible\nby targeting transmission lines. It assigns rewards based on\nthe following criteria:\n• Exceeding the Attack Objective (AO): If the attack\ncauses more immediate outages (IO) than the defined\nattack objective (AO), the attacker receives a high\nreward (r1).\n• Reaching the Attack Objective: If the attack successfully\nreaches the attack objective, the attacker receives a lower\nreward (r2).\n• Partial Success: If the attack causes some outages but\ndoesn’t reach the objective, the reward is proportional\nto the ratio of immediate outages (IO) to the attack\nobjective (AO).\nThe reward values (r1 and r2) are chosen such that r1 is\nsignificantly larger than r2, encouraging the agent to learn\nactions that maximize immediate damage and exceed the\nattack objective whenever possible.\nThe results demonstrate that the proposed deep Q-learning\nalgorithm significantly outperforms the alternative RL algo-\nrithms, achieving a higher level of defense against attacks.\nWhile the deep Q-learning algorithm may require more time\nto converge, its superior reliability and efficiency compensate\nfor the slower convergence, particularly in scenarios where\nreal-time computation is not critical.\nQ. OTHER METHODS\nVarious innovative reward-shaping frameworks enhance RL\napproaches across diverse challenges and environments. One\napproach employs barrier functions (BFs) to ensure safe\nRL agent behavior, demonstrating accelerated convergence\nand reduced actuation effort in simulations and real-world\ndeployments [81]. Another method utilizes natural language\ninstructions to generate dense rewards, improving RL effi-\nciency while posing challenges in natural language pro-\ncessing integration [82]. Additionally, temporal logic-based\nreward shaping for average-reward RL leverages formal\nlogic to enhance learning rates [83]. Addressing RL over-\noptimization, a framework penalizes rewards based on uncer-\ntainties, while another focuses on using reward expectations\nto stabilize and accelerate convergence in uncertain RL\nenvironments [13], [84].\nIn a reward planning scheme that relies on the tessellation\nof the state space, and dividing the tasks into sub-tasks, then\nplanning the agent’s behavior through the state space, the\nworks [85], [86] propose a novel reward planning method,\n‘‘Greedy Divide and Conquer’’. This method is designed\nfor underactuated robotic systems operating under parameter\nuncertainty. The method utilizes a single reinforcement\nlearning (RL) agent to address the challenge of swing-up\nand balancing a Pendubot system with uncertain parameters.\nThis approach allows the agent to adapt to parameter\nuncertainty and achieve faster convergence compared to\nusing a non-shaped reward function. The method was tested\non a Pendubot system with uncertainties up to 200-300%\nin various parameters, achieving an average 95% accuracy\nacross trials. The method’s strengths lie in its ability to\nhandle significant uncertainties and design reward functions\nsystematically by breaking down complex tasks into sub-\nproblems. It can also be used to avoid specific actions\nor behaviors during task execution. However, the method’s\ndownside is the potential for time-consuming development\nof the reward function, requiring a thorough understanding\nof the system.\n1) AUTOMATED PARAMETER TUNING\nWhile deep reinforcement learning has made substantial\nprogress determining optimal hyperparameters and reward\nfunctions remains a significant challenge even for expe-\nrienced practitioners many studies rely on established\nbenchmarks where prior knowledge about these crucial\n175490\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\naspects is readily available however real-world applications\nfrequently involve novel and intricate tasks lacking such\npre-existing knowledge necessitating the development of\nthese components from scratch.\nThis research [87] proposes a novel approach to address\nthis challenge by jointly optimizing and combining hyper-\nparameter auto-tuning and reward functions. The authors\nrecognize that these components are intrinsically linked and\ncannot be effectively optimized in isolation to facilitate\nthis optimization they utilize DEHB [88], a cutting-edge\nhyperparameter optimization algorithm DEHB combines\nthe strengths of HyperBand optimization with Differential\nEvolution, functioning like genetic algorithms DEHB has\nconsistently demonstrated robust performance across a vari-\nety of benchmarks.\nThe authors conducted experiments using proximal policy\noptimization (PPO) and soft actor-critic (SAC) algorithms\nin diverse environments the results demonstrate, as can\nbe seen in Figure 22, that this joint optimization strategy\nsignificantly improves performance compared to optimizing\neach component individually this research highlights the\npotential benefits of this method suggesting it may become a\nstandard practice for RL optimization. However, the research\nacknowledges limitations, such as the focus on optimizing\nspecific reward parameters within a predefined structure,\nsuggesting further exploration of broader reward function\ncombinations, and it mentions the need for further research to\nexplore a broader range of reward function configurations and\ninvestigate more sophisticated risk-averse metrics to achieve\na balance between performance and stability.\nTo delve deeper into the topic, a recent review provides\na comprehensive review of the field of automated reinforce-\nment learning (AutoRL) [89], aiming to automate key com-\nponents of the RL framework making it accessible even to\nnon-experts. The paper proposes a general AutoRL pipeline\nbreaking down the framework into three crucial components\nMDP modeling, algorithm selection, and hyperparameter\noptimization.\nThe review highlights the significant advantages of\nAutoRL, including reducing the need for specialized RL\nexpertise automating time-consuming tasks like defining\nstate and action spaces, and potentially leading to more\nefficient and effective RL solutions. However, the paper\nacknowledges that AutoRL is still a relatively nascent\nresearch area and faces several challenges. One key limitation\nis the lack of a concrete and comprehensive AutoRL, pipeline\nakin to the well-established automl pipelines. Moreover, opti-\nmizing hyperparameters efficiently automatically modeling\nproblems as MDPS and generalizing the mapping between\ninformation and RL environments remain critical research\nquestions.\nThe importance of hyperparameter optimization is under-\nscored in the paper recognizing that achieving optimal RL\nconfigurations for solving sequential decision-making prob-\nlems hinges on carefully chosen hyperparameter settings,\nthese hyperparameters fixed during training are typically set\nby RL experts before the training phase, examples include\nlearning rates in policy gradient, or value function updates,\ndiscount factors, eligibility trace coefficients, and parameters\nfor parametric reward shaping methods.\nDifferent tasks often require distinct sets of hyperparam-\neters, making hyperparameter optimization a challenging\nendeavor. Automating this process, as the paper suggests,\nwould be extremely beneficial. The research explores var-\nious methods for hyperparameter optimization, including\nstochastic gradient descent and neural networks, bayesian\noptimization, multi-armed bandit, evolutionary algorithms,\ngreedy algorithms,s and finally reinforcement learning itself.\nMany approaches developed for automatically optimizing\nhyperparameters in supervised learning algorithms can be\nadapted to optimize hyperparameters of RL algorithms.\nThis section further reviews hyperparameter optimization\ntechniques and their application in RL. Lastly, the main\nchallenges of optimizing hyperparameters within the con-\ntext of AutoRL are discussed. The paper emphasizes the\npotential benefits of AutoRL for solving complex sequential\ndecision-making problems across various domains, poten-\ntially leading to significant time and resource savings.\nThe integration of automated hyperparameter optimization\ninto the AutoRL pipeline promises to further enhance its\ncapabilities and contribute to more efficient and robust RL\nsolutions.\nV. REAL WORLD APPLICATIONS\nThis section delves into the specific challenges and opportu-\nnities presented by reward engineering in robotics. We will\nexplore notable robotic applications across various domains,\nhighlighting how these applications relate to the reward\nengineering techniques discussed in Section IV. This will\naim to provide a practical context for understanding the\nimplications or reward shaping within real-world robotic\napplications.\nA. REWARD SHAPING FOR SAFE AND EFFICIENT\nHUMAN-ROBOT COLLABORATION\nTo enhance safe interactions between humans and robots\nin industrial settings [90] presented a deep reinforcement\nlearning (DRL) approach, the study focused on enabling\nrobots to autonomously learn policies that minimize risks and\noptimize task efficiency. The framework of Human Robot\nCollaboration (HRC) is illustrated in Figure 23. Central to the\napproach is the development of the Intrinsic Reward-Deep\nDeterministic Policy Gradient (IRDDPG) algorithm, which\nintegrates deterministic policy gradient (DPG) methods\nwith an optimized reward function combining intrinsic and\nextrinsic rewards.\nExperimental results demonstrate that IRDDPG enables\nrobots to learn collision-avoidance policies effectively, ensur-\ning safety in human-robot interactions while achieving task\nobjectives. This method shows better results than manually\nVOLUME 12, 2024\n175491\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nFIGURE 22. Median optimization objective for SAC (5 optimization Runs), shaded areas indicate Min/Max values [87, Figure 3].\nFIGURE 23. The framework of collision avoidance in safe HRC, [90,\nFigure 2].\ndesigned reward functions by dynamically adjusting rewards\nduring learning, it also demonstrated the efficacy of tailored\nreward structures in guiding agent behavior towards desired\nsafety and efficiency outcomes, which improved safety\nthrough autonomous collision avoidance and task completion\nefficiency, yet challenges may arise in the complexity of\nreward function optimization and computational demands.\nIn the same context, [91] enhances DRL for domestic\nrobots performing household tasks like organizing objects\nwith a robotic arm by exploring the application of interactive\nfeedback. The study evaluates three learning methods:\nautonomous DRL, agent-assisted interactive DRL (agent–\nIDRL), and human-assisted interactive DRL (human–IDRL),\nThe schematic is illustrated in Figure 24, comparing their\neffectiveness in accelerating learning and reducing errors.\nFIGURE 24. The learning process for autonomous and interactive agents.\nBoth approaches include a pretraining stage comprising 1000 actions. For\ninteractive agents, the final part of the pretraining is performed using\nexternal advice instead of random actions, [91, Figure 2].\nResults demonstrate that interactive feedback from both\nhuman trainers and artificial agents significantly improves\nperformance metrics such as total collected rewards and\ntask completion speed compared to autonomous DeepRL.\nHuman feedback exhibits slightly superior results in certain\nscenarios, highlighting the value of incorporating human\nexpertise into robotic learning processes. This approach\nemphasizes the optimization of feedback strategies to guide\nagent behavior toward achieving specific task objectives\nefficiently, which enhances learning efficiency and effec-\ntiveness through interactive feedback mechanisms. However,\nchallenges may involve optimizing these feedback strategies\nfor diverse real-world applications.\nA distributed multi-robot navigation strategy, which\nmerges Reciprocal Velocity Obstacles (RVO) with DRL,\naddresses the challenges of collision avoidance in com-\nplex environments with limited information [93]. This\nmethod, referred to as RL-RVO, seamlessly integrates RVO\nconcepts with learning techniques. It utilizes sequential\nvelocity obstacle (VO) and RVO vectors to model the\nenvironment, employing a bidirectional recurrent neural\n175492\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nFIGURE 25. Top: Domain Adaption [92, Figure 1]: a) An encoder learns to map both the simulated (source) and real (target) environments to a\nshared state space. b) This shared state space is then used to train the policy within the simulation. c) The policy trained in the simulation is\ntransferred to the real environment, with states being mapped to the shared representations for further learning.\nBottom Left: Domain Randomization [92, Figure 2]: a) The agent’s training experience is enhanced by randomizing the physical dynamics and/or\nvisual appearance within simulated environments. b) The simulation-trained policy is expected to perform well in real-world tasks after a\none-shot transfer.\nBottom Right: Meta RL Learning [92, Figure 4]: The overall policy network is transferred to a set of subpolicy networks, which interact with a batch\nof sampled meta-training tasks (environments 1, 2, ..., n). The subpolicy network is updated based on rewards from the corresponding task, and\nthe global network is optimized using the parameters of the subpolicy networks.\nnetwork to translate obstacle states into robot actions. The\nreward function is meticulously designed to balance collision\navoidance with travel time efficiency, thereby optimizing the\nnavigation performance of the robots. Simulation evaluations\nwith varying numbers of robots and obstacles show that this\napproach outperforms other state-of-the-art methods in terms\nof success rate, travel time, and average speed. The RL-RVO\npolicy is further implemented and tested on Turtlebot robots\nto validate its real-world performance, as illustrated in\nFigure 26.\nThese experiments involve up to eight differential drive\nTurtlebots, all arranged in a circular formation with random\norientations. In this circle scenario, each robot, starting with\na random orientation, is uniformly placed around the circle’s\nperimeter, with the goal position located on the opposite\nside. This setup creates a rich interaction environment for the\nrobots, as shown in Figure 27.\nBy integrating Multifunctional Reward Shaping, which\nguides the robot towards its destination while simultane-\nously avoiding obstacles and providing informative rewards\nFIGURE 26. Illustration of real-world experiments: (left) a single\nTurtlebot, (right) eight Turtlebots uniformly positioned in a circle with\nrandom orientations [93, Figure 7].\nthat accelerate learning, with Hindsight Experience Replay\n(HER), which mitigates the challenge of sparse rewards\nby allowing the robot to learn from both successful and\nunsuccessful experiences, [94] proposes an end-to-end RL\nstrategy for navigating autonomous mobile robots in dynamic\nenvironments. This combined approach enables the robot\nVOLUME 12, 2024\n175493\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nFIGURE 27. Illustration of trajectories generated by four different\napproaches in the circle scenario [93, Figure 3].\nto develop an optimal navigation policy, showing a strong\ncapacity to adapt to previously unseen scenarios.\nThe effectiveness of this method has been demonstrated\nthrough both simulations and real-world experiments. In the\ntraining environment, the destination is randomly assigned\nat the start of each run, with the robot’s starting point at\nthe center of the space. Upon reaching the destination, the\nnavigation resets from that point. Dynamic obstacles are\nrepresented by four cylindrical structures that rotate with a\nfixed radius. The Gazebo simulation platform allows for the\ncreation of environments that closely resemble real-world\nconditions, thereby reducing development time and costs\nwhile enhancing convenience, as shown in Figure 28.\nFIGURE 28. (a) The test-driving environment in simulation, (b) the\nreal-world test-driving environment, distinct from the trained\nenvironment [94, Figure 10].\nB. REWARD SHAPING FOR AUTONOMOUS VEHICLES\nAND TRAFFIC FLOW OPTIMIZATION\nOne promising direction in the development of autonomous\nvehicles is the application of off-policy reinforcement\nlearning methods for traffic navigation [95]. A key approach\nin this area is the Episodic-Guided Prioritized Experience\nReplay (EPER) method, which addresses the inherent sample\ninefficiency faced by Deep Reinforcement Learning (DRL)\nmodels in autonomous driving applications. Typically, DRL-\nbased navigation requires extensive interactions with the\nenvironment to derive optimal policies, making the learning\nprocess time-consuming and resource-intensive. EPER alle-\nviates this challenge by utilizing episodic memory to store\nsuccessful experiences and extract expected returns for each\nstate-action pair, combined with Temporal Difference (TD)\nerror-based prioritization. This dual approach accelerates the\ntraining process while maintaining robust policy develop-\nment.\nIn addition, EPER integrates a regularization term that\nfosters the exploration of diverse state-space regions, thereby\npreventing the learning process from becoming excessively\ndeterministic. When applied to complex traffic scenarios such\nas highway driving, merging, roundabouts, and intersections,\nEPER has demonstrated superior performance compared\nto conventional Prioritized Experience Replay (PER) and\nother state-of-the-art techniques. Results indicate that EPER\nenhances sample efficiency, enabling quicker convergence to\noptimal policies and significantly lowering collision rates in\nsimulated driving environments.\nMoreover, by integrating EPER with reward shaping\ntechniques, autonomous vehicles can be trained to optimize\ntheir navigation strategies more effectively. Reward shaping\ncan guide the learning agent towards desired behaviors—\nsuch as collision avoidance and adherence to traffic rules—\nby adjusting the reward function. This process can ensure that\nthe agent’s learning remains focused on long-term objectives\nrather than immediate rewards, leading to safer and more\nreliable driving behaviors.\nBeyond autonomous vehicles, there is potential to apply\nsimilar reinforcement learning techniques, such as reward\nshaping, to the optimization of traffic flow in worst-\ncase scenarios. In this context, reward shaping could be\nutilized to optimize traffic flow independently of time\nconstraints, similar to the way quantum annealing has been\nemployed to solve complex optimization problems related\nto traffic flow and congestion, as demonstrated in previous\nworks [96], [97]. These studies highlight the potential\nof quantum annealing in optimizing traffic networks, but\nthe authors believe that the same problems could be\naddressed with reward shaping in a reinforcement learning\nframework. By designing reward functions that prioritize\nminimizing congestion and enhancing the overall efficiency\nof traffic systems, reinforcement learning could offer a\npractical and scalable solution to real-time traffic flow\nchallenges.\nThus, reward shaping, combined with reinforcement\nlearning, presents a promising avenue for not only improving\nautonomous vehicle navigation but also addressing broader\ntraffic management issues, including optimizing flow under\nworst-case conditions.\n175494\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nVI. SIM-TO-REAL IN REWARD ENGINEERING/SHAPING\nReward shaping and engineering are essential for effectively\ntransferring DRL and RL policies from simulations to real-\nworld scenarios. It facilitates this transition by encourag-\ning behaviors that are robust in both simulated and real\nenvironments. The connections between various methods\nfor transferring from simulation to reality are illustrated in\nFigure 29.\nFIGURE 29. Various techniques for transferring from simulation to reality\nin deep reinforcement learning and their interconnections [98, Figure 2].\nFigure 30 provides a graphical breakdown of the various\ncomponents within a RL algorithm. It also highlights the\ndistinct challenges faced when training DRL algorithms.\nKey obstacles include achieving consistent outcomes across\ndifferent implementations and datasets, which underscores\nthe issue of reproducibility. The sensitivity of DRL algo-\nrithms to hyperparameters adds another layer of complexity,\ncomplicating performance optimization. Additionally, the\nimplementation and deployment of DRL algorithms are often\nintricate and time-consuming tasks [99].\nFIGURE 30. Simulator-to-real gap [99, Figure 3].\nThe shift of DRL policies from simulation to real-world\nrobotic applications is challenging due to discrepancies\nbetween the two environments, compounded by concerns\nabout safety, cost, and efficiency. Techniques to address\nthis performance gap include domain adaptation, domain\nrandomization, and progressive neural networks (PNNs)\n[92], [98], [100]. Domain adaptation aims to minimize\nthe differences between simulated and real environments,\nas illustrated in the upper Figure 25. On the other hand\ndomain randomization enhances policy robustness through\nvaried simulation parameters, illustrated in the lower left\nFigure 25, PNNs and meta-reinforcement learning further\nimprove transfer efficiency by leveraging knowledge from\nprevious tasks, illustrated in the lower right Figure 25.\nTo\naddress\nthese\nchallenges,\n[101]\nintroduced\nthe\nConsensus-based Sim-And-Real DRL algorithm (CSAR),\nwhich exemplifies reward shaping by optimizing policies\nsuitable for both simulation and real contexts, ensuring\nconsistent reward structures throughout different training\nstages. The CSAR algorithm integrates consensus-based\ntraining with DRL in both simulated and real environments,\noptimizing policies by concurrently training agents in both\nsettings. This consensus-based method, which runs simulated\nand real agents in parallel, mitigates transition issues and\nreduces training time. Results indicate that an increased\nnumber of agents in simulation benefits both sim-and-real\ntraining, as illustrated in Figure 31. Similarly, [102] presents\na two-step sim-to-real process that utilizes an intermediate\nsemi-virtual environment. This approach integrates real robot\ndynamics with simulated sensors and obstacles, as illustrated\nin Figure 32. This configuration enables the controlled\nincorporation of real-world complexity while preserving the\nflexibility of simulation.\nFIGURE 31. Suction success rates of the real robot with a different\nnumber of simulated robots using Sim-and-Real strategy [101, Figure 6].\nFIGURE 32. The evaluation maps [102, Figure 3].\nFigure 33 shows the computational graph of the method,\nfeaturing an environment interaction thread (left) and a model\nupdate thread (right). Both threads interact with the replay\nbuffer, necessitating the use of a mutex to prevent conflicts.\nVOLUME 12, 2024\n175495\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nFortunately, the environment interaction thread adds an entry\nat the end of its cycle, while the model updates thread\nsamples from it early on, resulting in a low likelihood\nof mutual blocking. Moreover, parallel data collection and\nmodel updates for real-time fine-tuning allow the RL model\nto adapt to real-world changes without disrupting operations.\nDeploying the model at a high inference frequency achieves\nperformance nearly equivalent to simulation, even without\ninitial fine-tuning.\nFIGURE 33. Environment interaction thread (left) and a model update\nthread (right) [102, Figure 1].\nIn the same context, rewarding a cleaning robot for\nmaintaining a mess-free environment can lead to reward\nhacking [103]. In such scenarios, the robot might disable\nits vision to avoid detecting messes, cover messes with\nopaque materials, or hide when humans are around to\nprevent them from pointing out new types of messes. Reward\nhacking occurs when agents exploit loopholes in the reward\nfunction to achieve high rewards without actually fulfilling\nthe intended tasks.\nIntegrating reward shaping with advanced techniques like\ndomain adaptation, domain randomization, meta-learning,\nand consensus-based training is essential for overcoming\nchallenges in transferring DRL and RL policies from sim-\nulation to real-world robotics. These approaches collectively\nstrengthen policy robustness and efficiency, facilitating their\neffective deployment in real-world applications. Continu-\nous enhancements in simulation quality, efficient learning\nstrategies, and well-defined reward structures are pivotal for\nadvancing the field and achieving dependable performance\nof DRL and RL systems in real-world settings. Furthermore,\nensuring effective reward shaping is crucial to prevent reward\nhacking, ensuring that rewards promote desirable behaviors\nin both simulated and real environments.\nVII. ADVANTAGES AND DISADVANTAGES OF REWARD\nENGINEERING: IS IT THE FUTURE?\nReward shaping/engineering is a powerful technique used to\nenhance the performance of RL agents. It can significantly\nimprove the effectiveness of RL algorithms in several ways.\nBy providing additional information through shaping, agents\ncan learn optimal policies much faster, and this leads to\naccelerated learning. This is especially beneficial in complex\nenvironments where traditional RL methods might struggle\nto converge quickly. This is highlighted in discussions\nof STRIPS-based shaping, UCBVI-shaped, DRiP, PBRS,\nand other methods. Reward shaping can incentivize agents\nto explore more effectively, particularly in environments\nwith sparse rewards, and therefore improve exploration.\nThis allows agents to discover valuable states and actions\nthat might otherwise go unnoticed. Reward shaping can\nsometimes lead to agents achieving higher performance\nlevels than they would without shaping, as was discussed in\nSection IV, and other methods demonstrating performance\ngains.\nReward shaping can improve the robustness of learned\npolicies, making them less susceptible to noise, uncertainty,\nor changes in the environment.\nDespite its numerous benefits, reward shaping also poses\ncertain challenges, many reward shaping methods rely\nheavily on domain knowledge to design effective shaping\nfunctions. This can be a significant limitation, especially in\ncomplex or unfamiliar environments where expert knowledge\nmight be scarce or difficult to acquire.\nFurthermore, computational complexity could be con-\nsidered a disadvantage for some reward shaping methods,\nespecially those involving potential-based shaping, which\ncan increase the computational burden on the learning\nalgorithm. This can be a concern in real-time applications\nor with limited computational resources, but this drawback\nis only for some methods, other methods can decrease\nthe computational complexity. The effectiveness of many\nreward shaping techniques depends on carefully tuned\nparameters, although some approaches IV-N can tune their\nparameter, but finding the optimal parameter settings can\nbe a challenging process, and incorrect parameter values\ncan negatively impact performance. Finally, some reward\nshaping techniques require careful designing and shaping\nreward functions, and this can be a time-intensive process.\nIn conclusion, reward shaping is a valuable tool in\nthe reinforcement learning toolbox. While it offers many\nadvantages, careful consideration must be given to its\npotential drawbacks. Choosing the right reward shaping\nmethod is crucial for successful application.\nVIII. OPEN CHALLENGES AND FUTURE DIRECTIONS\nFuture research directions in robotic manipulation, pivotal\nacross sectors such as manufacturing, healthcare, and space\nexploration, especially in Industry 4.0 contexts, involve\nenhancing sample efficiency, bolstering algorithm robust-\nness, promoting human-robot collaboration, and investi-\ngating advanced neural network architectures [104]. The\nimportance of reward shaping and engineering in optimizing\nlearning outcomes within Deep Reinforcement Learning\n175496\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\n(DRL) underscores potential avenues for further advance-\nments in this vital domain.\nFor tasks involving image processing or complex sensor\ndata where designing rewards is difficult, we find that\nend-to-end RL approaches can tackle these tasks, such as\nthe research [14] which uses end-to-end RL combining\ndeep neural networks with SAC, enabling robots to learn\nfrom a limited number of successful demonstrations. This\nreduces the reliance on explicit reward functions, making\nit particularly beneficial for tasks involving image process-\ning or complex sensor data where designing rewards is\ndifficult. The method was evaluated on robotic manipula-\ntion tasks, such as picking and placing objects, and the\nauthors claim to achieve high success rates and accuracy\n(100%).\nHowever, the research has several limitations. Firstly, the\nrobot’s behavior may not always be optimal, potentially\nleading to sub-optimal solutions or inefficient movements.\nSecondly, the method is not robust to uncertainties in the\nenvironment, such as variations in object placement or\nlighting conditions. Thirdly, the reliance on user queries can\npotentially be time-consuming, especially for tasks with a\nlarge number of possible actions or complex decision-making\nprocesses.\nIX. CONCLUSION\nThis work provides a comprehensive review of reward\nshaping techniques within the field of RL. A detailed tax-\nonomy of methods is presented encompassing descriptions,\nadvantages, disadvantages, application domains, and relevant\nmetrics. This analysis addresses open challenges and future\ndirections, offering valuable insights for future research\nand development. Our findings highlight the significant\nbenefits of reward shaping, including its ability to highly\nexpedite learning, manage uncertainties, bolster robustness,\nenhance system outcomes, and increase the success rate of\nRL agents. While reward shaping demonstrably improves\nlearning outcomes, its implementation can be complex and\ntime-consuming, particularly in scenarios with significant\ndomain complexity. Automated parameter tuning methods\nmitigate this challenge to some extent. Further research is\nrequired to evaluate reward shaping techniques in real-world\napplications and explore the potential of incorporating human\nfeedback into the training process. This work aims to\nestablish a comprehensive resource for understanding reward\nshaping in RL. This resource facilitates the selection of\nappropriate methods based on the specific problem, enabling\nresearchers and practitioners to effectively leverage the power\nof reward shaping for improved RL performance.\nREFERENCES\n[1] J. Cameron and W. D. Pierce, ‘‘Reinforcement, reward, and intrinsic moti-\nvation: A meta-analysis,’’ Rev. Educ. Res., vol. 64, no. 3, p. 363, 1994.\n[2] R. Devidze, G. Radanovic, P. Kamalaruban, and A. Singla, ‘‘Explicable\nreward design for reinforcement learning agents,’’ Adv. Neural Inf.\nProcess. Syst., vol. 34, pp. 20118–20131, 2021.\n[3] J. Eschmann, ‘‘Reward function design in reinforcement learning,’’ in\nStudies in Computational Intelligence. Cham, Switzerland: Springer,\n2021, pp. 25–33.\n[4] L. P. Kaelbling, M. L. Littman, and A. W. Moore, ‘‘Reinforcement\nlearning: A survey,’’ J. Artif. Intell. Res., vol. 4, no. 1, pp. 237–285,\nJan. 1996.\n[5] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.\nCambridge, MA, USA: MIT Press, 2018.\n[6] D. Dewey, ‘‘Reinforcement learning and the reward engineering princi-\nple,’’ in Proc. AAAI Spring Symp. Ser., 2014, pp. 1–12.\n[7] N. M. White, ‘‘Reward or reinforcement: What’s the difference?’’\nNeurosci. Biobehavioral Rev., vol. 13, nos. 2–3, pp. 181–186, Jun. 1989.\n[8] K. Doya, ‘‘Reinforcement learning in continuous time and space,’’ Neural\nComput., vol. 12, no. 1, pp. 219–245, Jan. 2000.\n[9] A. D. Laud, Theory and Application of Reward Shaping in Reinforcement\nLearning. Urbana, IL, USA: University of Illinois at Urbana-Champaign,\n2004.\n[10] Y. Hu, ‘‘Learning to utilize shaping rewards: A new approach of reward\nshaping,’’ in Proc. Adv. Neural Inf. Process. Syst. Annu. Conf. Neural Inf.\nProcess. Syst. (NeurIPS), Dec. 2020, pp. 15931–15941.\n[11] AY. Ng, D. Harada, and S. Russell, ‘‘Policy invariance under reward\ntransformations: Theory and application to reward shaping,’’ in Proc. Int.\nConf. Mach. Learn., 1999, pp. 278–287.\n[12] E. Bates, V. Mavroudis, and C. Hicks, ‘‘Reward shaping for happier\nautonomous cyber security agents,’’ in Proc. 16th ACM Workshop Artif.\nIntell. Secur., Nov. 2023, pp. 221–232.\n[13] Y. Zhai, H. Zhang, Y. Lei, Y. Yu, K. Xu, D. Feng, B. Ding, and H. Wang,\n‘‘Uncertainty-penalized reinforcement learning from human feedback\nwith diverse reward LoRA ensembles,’’ 2023, arXiv:2401.00243.\n[14] A. Singh, L. Yang, K. Hartikainen, C. Finn, and S. Levine, ‘‘End-to-\nend robotic reinforcement learning without reward engineering,’’ 2019,\narXiv:1904.07854.\n[15] A. C. Tenorio-Gonzalez, E. F. Morales, and L. Villasenor-Pineda,\n‘‘Dynamic reward shaping: Training a robot by voice,’’ in Proc. 12th\nIbero-Amer. Conf., 2010, pp. 483–492.\n[16] P. Osinenko, D. Dobriborsci, and W. Aumer, ‘‘Reinforcement learning\nwith guarantees: A review,’’ IFAC-PapersOnLine, vol. 55, no. 15,\npp. 123–128, 2022.\n[17] M. J. Page, J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C. Hoffmann,\nC. D. Mulrow, L. Shamseer, J. M. Tetzlaff, and D. Moher, ‘‘Updating\nguidance for reporting systematic reviews: Development of the PRISMA\n2020 statement,’’ J. Clin. Epidemiology, vol. 134, pp. 103–112, Jun. 2021.\n[18] D. Silver, S. Singh, D. Precup, and R. S. Sutton, ‘‘Reward is enough,’’\nArtif. Intell., vol. 299, Oct. 2021, Art. no. 103535.\n[19] P. Vamplew, B. J. Smith, J. Källström, G. Ramos, R. Radulescu,\nD. M. Roijers, C. F. Hayes, F. Heintz, P. Mannion, P. J. K. Libin,\nR. Dazeley, and C. Foale, ‘‘Scalar reward is not enough: A response to\nsilver, singh, precup and sutton (2021),’’ Auto. Agents Multi-Agent Syst.,\nvol. 36, no. 2, p. 41, Oct. 2022.\n[20] S. Devlin and D. Kudenko, ‘‘Theoretical considerations of potential-\nbased reward shaping for multi-agent systems,’’ in Proc. 10th Int. Conf.\nAuto. Agents Multi-Agent Syst., 2011, pp. 225–232.\n[21] K. Banihashem, A. Singla, J. Gan, and G. Radanovic, ‘‘Admissible policy\nteaching through reward design,’’ in Proc. AAAI Conf. Artif. Intell.,\nvol. 36, 2022, pp. 6037–6045.\n[22] H. Zhang and D. C. Parkes, ‘‘Value-based policy teaching with active\nindirect elicitation,’’ in Proc. AAAI, vol. 8, 2008, pp. 208–214.\n[23] H. Zhang, D. C. Parkes, and Y. Chen, ‘‘Policy teaching through reward\nfunction learning,’’ in Proc. 10th ACM Conf. Electron. Commerce,\nJul. 2009, pp. 295–304.\n[24] A. G. Barto, S. Singh, and N. Chentanez, ‘‘Intrinsically motivated\nlearning of hierarchical collections of skills,’’ in Proc. 3rd Int. Conf.\nDevelop. Learn., vol. 112, 2004, p. 19.\n[25] S. Singh, R. L. Lewis, and A. G. Barto, ‘‘Where do rewards come from,’’\nin Proc. Annu. Conf. Cognit. Sci. Soc., 2009, pp. 2601–2606.\n[26] S. Singh, R. L. Lewis, A. G. Barto, and J. Sorg, ‘‘Intrinsically motivated\nreinforcement learning: An evolutionary perspective,’’ IEEE Trans. Auto.\nMental Develop., vol. 2, no. 2, pp. 70–82, Jun. 2010.\n[27] W. J. A. van Heeswijk, ‘‘Natural policy gradients in reinforcement\nlearning explained,’’ 2022, arXiv:2209.01820.\n[28] J. Sorg, R. L. Lewis, and S. Singh, ‘‘Reward design via online gradient\nascent,’’ in Proc. Adv. Neural Inf. Process. Syst., 2010, pp. 1–11.\nVOLUME 12, 2024\n175497\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\n[29] Z. Zheng, J. Oh, and S. Singh, ‘‘On learning intrinsic rewards for\npolicy gradient methods,’’ in Proc. Adv. Neural Inf. Process. Syst., 2018,\npp. 1–14.\n[30] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess,\nT. Rothörl, T. Lampe, and M. Riedmiller, ‘‘Leveraging demonstrations for\ndeep reinforcement learning on robotics problems with sparse rewards,’’\n2017, arXiv:1707.08817.\n[31] R. Valiente, B. Toghi, R. Pedarsani, and Y. P. Fallah, ‘‘Robustness and\nadaptability of reinforcement learning-based cooperative autonomous\ndriving in mixed-autonomy traffic,’’ IEEE Open J. Intell. Transp. Syst.,\nvol. 3, pp. 397–410, 2022.\n[32] J. Wang, Y. Liu, and B. Li, ‘‘Reinforcement learning with perturbed\nrewards,’’ in Proc. AAAI Conf. Artif. Intell. (AAAI), Apr. 2020, vol. 34,\nno. 4, pp. 6202–6209.\n[33] S. Wu, H. Ma, J. Fu, and S. Han, ‘‘Robust reward design for Markov\ndecision processes,’’ 2024, arXiv:2406.05086.\n[34] F. De Lellis, M. Coraggio, G. Russo, M. Musolesi, and M. di\nBernardo, ‘‘Guaranteeing control requirements via reward shaping in\nreinforcement learning,’’ IEEE Trans. Control Syst. Technol., vol. 32,\nno. 6, pp. 2102–2113, Nov. 2024.\n[35] W. Kim, J. Kim, and Y. Sung, ‘‘LESSON: Learning to integrate explo-\nration strategies for reinforcement learning via an option framework,’’\n2023, arXiv:2310.03342.\n[36] H. Tang, ‘‘# exploration: A study of count-based exploration for deep\nreinforcement learning,’’ in Proc. Adv. Neural Inf. Process. Syst., 2017,\npp. 1–30.\n[37] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. D. Turck, and P. Abbeel,\n‘‘Vime: Variational information maximizing exploration,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2016, pp. 1–11.\n[38] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel,\n‘‘Benchmarking deep reinforcement learning for continuous control,’’ in\nProc. Int. Conf. Mach. Learn., 2016, pp. 1329–1338.\n[39] R. Devidze, P. Kamalaruban, and A. Singla, ‘‘Exploration-guided reward\nshaping for reinforcement learning under sparse rewards,’’ in Proc.\n36th Conf. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 35, 2022,\npp. 5829–5842.\n[40] X. Liang, K. Shu, K. Lee, and P. Abbeel, ‘‘Reward uncertainty\nfor exploration in preference-based reinforcement learning,’’ 2022,\narXiv:2205.12401.\n[41] Z. Huang, L. Liang, Z. Ling, X. Li, C. Gan, and H. Su, ‘‘Reparameterized\npolicy learning for multimodal trajectory optimization,’’ in Proc. Int.\nConf. Mach. Learn., 2023, pp. 13957–13975.\n[42] A. Rajeswaran, K. Lowrey, E. V. Todorov, and S. M. Kakade, ‘‘Towards\ngeneralization and simplicity in continuous control,’’ in Proc. Adv. Neural\nInf. Process. Syst., 2017, pp. 1–12.\n[43] S. Arora and P. Doshi, ‘‘A survey of inverse reinforcement learning:\nChallenges, methods and progress,’’ Artif. Intell., vol. 297, Aug. 2021,\nArt. no. 103500.\n[44] A. Y. Ng and S. Russell, ‘‘Algorithms for inverse reinforcement learning,’’\nin Proc. ICML, vol. 1, 2000, pp. 1–2.\n[45] N. Das, S. Bechtle, T. Davchev, D. Jayaraman, A. Rai, and F. Meier,\n‘‘Model-based inverse reinforcement learning from visual demonstra-\ntions,’’ in Proc. Conf. Robot Learn., 2021, pp. 1930–1942.\n[46] R. Self, M. Abudia, S. M. N. Mahmud, and R. Kamalapurkar,\n‘‘Model-based inverse reinforcement learning for deterministic systems,’’\nAutomatica, vol. 140, Jun. 2022, Art. no. 110242.\n[47] E. Uchibe, ‘‘Model-free deep inverse reinforcement learning by logistic\nregression,’’ Neural Process. Lett., vol. 47, no. 3, pp. 891–905, Jun. 2018.\n[48] M. Wulfmeier, P. Ondruska, and I. Posner, ‘‘Maximum entropy deep\ninverse reinforcement learning,’’ 2015, arXiv:1507.04888.\n[49] D. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan,\n‘‘Inverse reward design,’’ in Proc. Adv. Neural Inf. Process. Syst., 2017,\npp. 1–22.\n[50] A. Laud and G. DeJong, ‘‘The influence of reward on the speed of\nreinforcement learning: An analysis of shaping,’’ in Proc. 20th Int. Conf.\nMach. Learn. (ICML-03), 2003, pp. 440–447.\n[51] B. Badnava, M. Esmaeili, N. Mozayani, and P. Zarkesh-Ha, ‘‘A new\npotential-based reward shaping for reinforcement learning agent,’’ in\nProc. IEEE 13th Annu. Comput. Commun. Workshop Conf. (CCWC),\nMar. 2023, pp. 01–06.\n[52] Y. Gao and F. Toni, ‘‘Potential based reward shaping for hierarchical\nreinforcement learning,’’ in Proc. 24rth Int. Joint Conf. Artif. Intell., 2015,\npp. 1–11.\n[53] A. Harutyunyan, S. Devlin, P. Vrancx, and A. Nowé, ‘‘Expressing\narbitrary reward functions as potential-based advice,’’ in Proc. AAAI\nConf. Artif. Intell., vol. 29, 2015, pp. 1–14.\n[54] M. Grzes. (2017). Reward Shaping in Episodic Reinforcement Learning.\n[Online]. Available: https://kar.kent.ac.uk/60614/7/p565.pdf\n[55] M. Qian and S. Mitsch, ‘‘Reward shaping from hybrid systems models\nin reinforcement learning,’’ in Proc. NASA Formal Methods Symp., 2023,\npp. 122–139.\n[56] S. Devlin, L. Yliniemi, D. Kudenko, and K. Tumer, ‘‘Potential-based\ndifference rewards for multiagent reinforcement learning,’’ in Proc. Int.\nConf. Auto. Agents Multi-Agent Syst., 2014, pp. 165–172.\n[57] S. M. Devlin and D. Kudenko, ‘‘Dynamic potential-based reward\nshaping,’’ in Proc. 11th Int. Conf. Auto. Agents Multiagent Syst., 2012,\npp. 433–440.\n[58] M. G. Azar, I. Osband, and R. Munos, ‘‘Minimax regret bounds\nfor reinforcement learning,’’ in Proc. Int. Conf. Mach. Learn., 2017,\npp. 263–272.\n[59] A. L. Strehl and M. L. Littman, ‘‘A theoretical analysis of model-\nbased interval estimation,’’ in Proc. 22nd Int. Conf. Mach. Learn., 2005,\npp. 856–863.\n[60] A. Gupta, A. Pacchiano, Y. Zhai, S. Kakade, and S. Levine, ‘‘Unpacking\nreward shaping: Understanding the benefits of reward engineering on\nsample complexity,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 35,\n2022, pp. 15281–15295.\n[61] D. H. Wolpert, K. R. Wheeler, and K. Tumer, ‘‘Collective intelligence for\ncontrol of distributed dynamical systems,’’ Europhys. Lett. (EPL), vol. 49,\nno. 6, pp. 708–714, Mar. 2000.\n[62] P. Mannion, S. Devlin, J. Duggan, and E. Howley, ‘‘Reward shaping for\nknowledge-based multi-objective multi-agent reinforcement learning,’’\nKnowl. Eng. Rev., vol. 33, p. e23, Apr. 2018.\n[63] R. Grunitzki, G. d. O. Ramos, and A. L. C. Bazzan, ‘‘Individual versus\ndifference rewards on reinforcement learning for route choice,’’ in Proc.\nBrazilian Conf. Intell. Syst., Oct. 2014, pp. 253–258.\n[64] M. Grzes and D. Kudenko, ‘‘Plan-based reward shaping for reinforcement\nlearning,’’ in Proc. 4th Int. IEEE Conf. Intell. Syst., Sep. 2008, pp. 10–22.\n[65] K. Efthymiadis and D. Kudenko, ‘‘A comparison of plan-based and\nabstract MDP reward shaping,’’ Connection Sci., vol. 26, no. 1, pp. 85–99,\nJan. 2014.\n[66] O. Marom and B. Rosman, ‘‘Belief reward shaping in reinforcement\nlearning,’’ in Proc. AAAI Conf. Artif. Intell., vol. 32, 2018, pp. 1–28.\n[67] K. Lee, L. Smith, and P. Abbeel, ‘‘PEBBLE: Feedback-efficient interac-\ntive reinforcement learning via relabeling experience and unsupervised\npre-training,’’ 2021, arXiv:2106.05091.\n[68] J. Park, Y. Seo, J. Shin, H. Lee, P. Abbeel, and K. Lee, ‘‘SURF: Semi-\nsupervised reward learning with data augmentation for feedback-efficient\npreference-based reinforcement learning,’’ 2022, arXiv:2203.10050.\n[69] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei,\n‘‘Deep reinforcement learning from human preferences,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 1–18.\n[70] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei, ‘‘Reward\nlearning from human preferences and demonstrations in atari,’’ in Proc.\nAdv. Neural Inf. Process. Syst., 2018, pp. 1–18.\n[71] R. Akrour, M. Schoenauer, and M. Sebag, ‘‘Preference-based policy\nlearning,’’ in Proc. Eur. Conf., ECML PKDD, 2011, pp. 12–27.\n[72] H. Liang, L. Yang, H. Cheng, W. Tu, and M. Xu, ‘‘Human-in-the-loop\nreinforcement learning,’’ in Proc. Chin. Autom. Congr. (CAC), Oct. 2017,\npp. 4511–4518.\n[73] C. O. Retzlaff, S. Das, C. Wayllace, P. Mousavi, M. Afshari, T. Yang,\nA. Saranti, A. Angerschmid, M. E. Taylor, and A.Holzinger, ‘‘Human-in-\nthe-loop reinforcement learning: A survey and position on requirements,\nchallenges, and opportunities,’’ in Proc. J. Artif. Intell. Res., vol. 79, 2024,\npp. 359–415.\n[74] T. Mandel, Y.-En Liu, E. Brunskill, and Z. Popović, ‘‘Where to add actions\nin human-in-the-loop reinforcement learning,’’ in Proc. AAAI Conf. Artif.\nIntell., vol. 31, 2017, pp. 1–24.\n[75] E. Mosqueira-Rey, E. Hernández-Pereira, D. Alonso-Ríos, J. Bobes-\nBascarán, and Á. Fernández-Leal, ‘‘Human-in-the-loop machine learn-\ning: A state of the art,’’ Artif. Intell. Rev., vol. 56, no. 4, pp. 3005–3054,\nApr. 2023.\n[76] X. Wu, L. Xiao, Y. Sun, J. Zhang, T. Ma, and L. He, ‘‘A survey of human-\nin-the-loop for machine learning,’’ Future Gener. Comput. Syst., vol. 135,\npp. 364–381, Oct. 2022.\n175498\nVOLUME 12, 2024\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\n[77] T. Xie, S. Zhao, C. H. Wu, Y. Liu, Q. Luo, V. Zhong, Y. Yang, and T. Yu,\n‘‘Text2reward: Reward shaping with language models for reinforcement\nlearning,’’ in Proc. 12th Int. Conf. Learn. Represent., 2024, pp. 1–10.\n[78] R. Liu, F. Bai, Y. Du, and Y. Yang, ‘‘Meta-reward-net: Implicitly differ-\nentiable reward learning for preference-based reinforcement learning,’’ in\nProc. Adv. Neural Inf. Process. Syst., vol. 35, 2022, pp. 22270–22284.\n[79] M. Moradi, Y. Weng, and Y.-C. Lai, ‘‘Defending smart electrical power\ngrids against cyberattacks with deep q -Learning,’’ PRX Energy, vol. 1,\nno. 3, Nov. 2022, Art. no. 033005.\n[80] M. Moradi, Y. Weng, J. Dirkman, and Y.-C. Lai, ‘‘Preferential cyber\ndefense for power grids,’’ PRX Energy, vol. 2, no. 4, Oct. 2023,\nArt. no. 043007.\n[81] N. Nilaksh, A. Ranjan, S. Agrawal, A. Jain, P. Jagtap, and S. Kolathaya,\n‘‘Barrier functions inspired reward shaping for reinforcement learning,’’\n2024, arXiv:2403.01410.\n[82] P. Goyal, S. Niekum, and R. J. Mooney, ‘‘Using natural language for\nreward shaping in reinforcement learning,’’ 2019, arXiv:1903.02020.\n[83] Y. Jiang, S. Bharadwaj, B. Wu, R. Shah, U. Topcu, and P. Stone,\n‘‘Temporal-logic-based reward shaping for continuing reinforcement\nlearning tasks,’’ in Proc. AAAI Conf. Artif. Intell., vol. 35, 2021,\npp. 7995–8003.\n[84] Y. Wang, Y. Sun, J. Wu, H. Hu, Z. Wu, and W. Huang, ‘‘Reinforcement\nlearning using reward expectations in scenarios with aleatoric uncertain-\nties,’’ in Proc. IEEE Conf. Games (CoG), Aug. 2022, pp. 261–267.\n[85] S. Ibrahim, R. Khusainov, A. Jnadi, and S. M. Ahsan Kazmi, ‘‘Reward\nplanning for underactuated robotic systems: A study on pendubot with\nparameters uncertainty,’’ in Proc. 7th Scientific School Dyn. Complex\nNetw. Appl. (DCNA), Sep. 2023, pp. 101–104.\n[86] S. Ibrahim, S. M. Ahsan Kazmi, D. Dobriborsci, R. Zashchitin,\nM. Mostafa, and P. Osinenko, ‘‘Reward planning for underactuated\nrobotic systems with parameters uncertainty: Greedy-divide and con-\nquer,’’ in Proc. 10th Int. Conf. Control, Decis. Inf. Technol. (CoDIT),\nJul. 2024, pp. 1518–1523.\n[87] J. Dierkes, E. Cramer, H. H. Hoos, and S. Trimpe, ‘‘Combining\nautomated optimisation of hyperparameters and reward shape,’’ 2024,\narXiv:2406.18293.\n[88] N. Awad, N. Mallik, and F. Hutter, ‘‘DEHB: Evolutionary hyperband\nfor scalable, robust and efficient hyperparameter optimization,’’ 2021,\narXiv:2105.09821.\n[89] R. Refaei Afshar, Y. Zhang, J. Vanschoren, and U. Kaymak, ‘‘Automated\nreinforcement learning: An overview,’’ 2022, arXiv:2201.05000.\n[90] Q. Liu, Z. Liu, B. Xiong, W. Xu, and Y. Liu, ‘‘Deep reinforcement\nlearning-based safe interaction for industrial human–robot collaboration\nusing intrinsic reward function,’’ Adv. Eng. Informat., vol. 49, Aug. 2021,\nArt. no. 101360.\n[91] I. Moreira, J. Rivas, F. Cruz, R. Dazeley, A. Ayala, and B. Fernandes,\n‘‘Deep reinforcement learning with interactive feedback in a human–\nrobot environment,’’ Appl. Sci., vol. 10, no. 16, p. 5574, Aug. 2020.\n[92] H. Ju, R. Juan, R. Gomez, K. Nakamura, and G. Li, ‘‘Transferring policy\nof deep reinforcement learning from simulation to reality for robotics,’’\nNature Mach. Intell., vol. 4, no. 12, pp. 1077–1087, Dec. 2022.\n[93] R. Han, S. Chen, S. Wang, Z. Zhang, R. Gao, Q. Hao, and J. Pan,\n‘‘Reinforcement learned distributed multi-robot navigation with recipro-\ncal velocity obstacle shaped rewards,’’ IEEE Robot. Autom. Lett., vol. 7,\nno. 3, pp. 5896–5903, Jul. 2022.\n[94] M. Park, C. Park, and N. K. Kwon, ‘‘Autonomous driving of mobile robots\nin dynamic environments based on deep deterministic policy gradient:\nReward shaping and hindsight experience replay,’’ Biomimetics, vol. 9,\nno. 1, p. 51, Jan. 2024.\n[95] H. Hassani, S. Nikan, and A. Shami, ‘‘Traffic navigation via reinforce-\nment learning with episodic-guided prioritized experience replay,’’ Eng.\nAppl. Artif. Intell., vol. 137, Nov. 2024, Art. no. 109147.\n[96] H. Salloum, S. Zhanalin, M. Mazzara, and Y. Kholodov, ‘‘Quantum\ncongestion-focused traffic optimization (Q-CFTO) Enhancing traffic\ncongestion solutions with quantum annealing,’’ Authorea Preprints,\nvol. 2, pp. 1–11, Aug. 2024.\n[97] F. Neukart, G. Compostella, C. Seidel, D. von Dollen, S. Yarkoni,\nand B. Parney, ‘‘Traffic flow optimization using a quantum annealer,’’\nFrontiers ICT, vol. 4, p. 29, Dec. 2017.\n[98] W. Zhao, J. P. Queralta, and T. Westerlund, ‘‘Sim-to-Real transfer in deep\nreinforcement learning for robotics: A survey,’’ in Proc. IEEE Symp. Ser.\nComput. Intell. (SSCI), Dec. 2020, pp. 737–744.\n[99] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab,\nS. Yogamani, and P. Pérez, ‘‘Deep reinforcement learning for autonomous\ndriving: A survey,’’ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 6,\npp. 4909–4926, Jun. 2022.\n[100] L. C. Garaffa, M. Basso, A. A. Konzen, and E. P. de Freitas,\n‘‘Reinforcement learning for mobile robotics exploration: A survey,’’\nIEEE Trans. Neural Netw. Learn. Syst., vol. 34, no. 8, pp. 1–15,\nMay 2021.\n[101] W. Liu, H. Niu, W. Pan, G. Herrmann, and J. Carrasco, ‘‘Sim-\nand-real reinforcement learning for manipulation: A consensus-based\napproach,’’ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May 2023,\npp. 3911–3917.\n[102] A. Jonnarth, O. Johansson, and M. Felsberg, ‘‘Sim-to-real transfer of deep\nreinforcement learning agents for online coverage path planning,’’ 2024,\narXiv:2406.04920.\n[103] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and\nD. Mané, ‘‘Concrete problems in AI safety,’’ 2016, arXiv:1606.06565.\n[104] D. Han, B. Mulyana, V. Stankovic, and S. Cheng, ‘‘A survey on deep\nreinforcement learning algorithms for robotic manipulation,’’ Sensors,\nvol. 23, no. 7, p. 3762, Apr. 2023.\nSINAN IBRAHIM received the B.S. degree in\nmechatronics engineering from Tishreen Univer-\nsity, Syria, in 2021, and the M.S. degree in robotics\nand computer vision from Innopolis University,\nRussia, in 2023. He is currently pursuing the Ph.D.\ndegree in computational and data science and\nengineering with Skolkovo Institute for Science\nand Technology. He is a highly accomplished\nResearcher and an Engineer with expertise in\nrobotics, computer vision, and reinforcement\nlearning. He has a proven track record of developing innovative solutions and\nis passionate about both scientific advancement and its real-world business\napplications. His career has involved a blend of teaching, coaching, business,\nfundraising, go-to-market, and hyper-growth for startups, demonstrating his\nadaptability and diverse skill set. His research interests include computer\nvision, robotics, and data-based control.\nMOSTAFA MOSTAFA received the bachelor’s\ndegree in mechanical and electrical engineering\nfrom the Mechatronics Engineering Department,\nTishreen University, Latakia, Syria, in 2017, and\nthe master’s degree (Hons.) in intelligent tech-\nnology in robotics from ITMO University, Saint\nPetersburg, Russia, in 2020. He is currently pur-\nsuing the Ph.D. degree in reinforcement learning\nand robotics with Skolkovo Institute of Science\nand Technology, Moscow, Russia. He received\ncertificates in various fields, advances in robotics from LUT University,\nFinland, and ITMO University, Russia; the elements of AI (artificial\nintelligence) from the University of Helsinki, Finland; and professional\ndevelopment (active learning in STEM) from Skolkovo Institute of Science\nand Technology. From 2020 to 2022, he was a Quality Engineer (a Process\nEngineer) in Saint Petersburg, where he established methods for solving\ntechnical problems, conducted performance loss studies, systematized main-\ntenance tools, and implemented production management tools. Additionally,\nhe identified key projects to achieve performance targets and collaborated\nwith technical experts and other manufacturing sites to create optimal\nsolutions.\nVOLUME 12, 2024\n175499\nS. Ibrahim et al.: Comprehensive Overview of Reward Engineering and Shaping in Advancing RL Applications\nALI JNADI received the B.Sc. degree in mecha-\ntronics from Tishreen University, Syria, in 2011,\nand the M.Sc. degree in robotics and computer\nvision from Innopolis University, Russia, in 2023,\nwhere he is currently pursuing the Ph.D. degree,\nfocusing on advanced control systems for under-\nactuated systems.\nFrom 2013 to 2018, he taught informatics\nat the National Center for Distinguished, Syria.\nFrom 2018 to 2019, he was a Specialist Instructor\nwith the Ministry of Education, Syria. From 2019 to 2021, he taught\nand supervised the Robotics and Control Laboratory, Manara University,\nSyria. Since August 2022, he has been a Teaching Assistant with Innopolis\nUniversity, teaching labs in differential equations, data structures and\nalgorithms, human-AI interaction design, and physics. Since November\n2023, he has been an Engineer with Innopolis University’s Technology\nCenter for Robotics and Mechatronics Components, working on local\nplanning and control algorithms for autonomous vehicles. Since 2023,\nhe has been a Researcher on a project called Robotic Incremental Metal\nSheet Forming. His research interests include model predictive control,\nreinforcement learning, and robotics, with published works on explicit\nmodel predictive control design and reinforcement learning methods for\n‘‘Pendubot’’ balancing.\nMr. Jnadi has received several awards, including the First and Second\nPlaces in High School Sumo Competitions at ARC7 Syria, in 2022, the\nSecond Place in the Future Engineer Category at the WRO 2021 Finals\nFuture Engineer Category, and the Gold Medal at The ‘‘Al Bassel’’ Fair for\nInvention and Innovation for his Hybrid Motorcycle Project, in 2011.\nHADI SALLOUM is currently pursuing the bach-\nelor’s degree in artificial intelligence and data\nscience, with a focus on applying advanced AI\ntechniques to complex problems across various\ndomains. He has demonstrated exceptional com-\nmitment to physics, competing in the International\nPhysics Olympiad 2021 in Lithuania and European\nPhysics Olympiad in Estonia. He is a Researcher\nwith the AI Institute, Innopolis University, Russia.\nHis research interests include innovative AI and\ndata science applications, particularly utilizing quantum computing to\nenhance computational capabilities.\nPAVEL OSINENKO received a diploma with\nhonors from Bauman Moscow State Technical\nUniversity in 2009, and the Ph.D. degree from\nDresden University of Technology, in 2014.\nFrom 2011 to 2020, he worked at German aca-\ndemic and industrial sectors focusing on research,\nproject supervision, and teaching. Since 2020,\nhe has been an Assistant Professor with Skoltech,\nMoscow. His research interests include reinforce-\nment learning, nonlinear system stabilization, and\ncomputational aspects of dynamical systems.\n175500\nVOLUME 12, 2024\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2024-07-22",
  "updated": "2024-12-27"
}