{
  "id": "http://arxiv.org/abs/2408.11366v1",
  "title": "GeoReasoner: Reasoning On Geospatially Grounded Context For Natural Language Understanding",
  "authors": [
    "Yibo Yan",
    "Joey Lee"
  ],
  "abstract": "In human reading and communication, individuals tend to engage in geospatial\nreasoning, which involves recognizing geographic entities and making informed\ninferences about their interrelationships. To mimic such cognitive process,\ncurrent methods either utilize conventional natural language understanding\ntoolkits, or directly apply models pretrained on geo-related natural language\ncorpora. However, these methods face two significant challenges: i) they do not\ngeneralize well to unseen geospatial scenarios, and ii) they overlook the\nimportance of integrating geospatial context from geographical databases with\nlinguistic information from the Internet. To handle these challenges, we\npropose GeoReasoner, a language model capable of reasoning on geospatially\ngrounded natural language. Specifically, it first leverages Large Language\nModels (LLMs) to generate a comprehensive location description based on\nlinguistic and geospatial information. It also encodes direction and distance\ninformation into spatial embedding via treating them as pseudo-sentences.\nConsequently, the model is trained on both anchor-level and neighbor-level\ninputs to learn geo-entity representation. Extensive experimental results\ndemonstrate GeoReasoner's superiority in three tasks: toponym recognition,\ntoponym linking, and geo-entity typing, compared to the state-of-the-art\nbaselines.",
  "text": "GeoReasoner: Reasoning On Geospatially Grounded Context For\nNatural Language Understanding\nYibo Yan\nNational University of Singapore\nyanyibo70@gmail.com\nJoey Lee\nShanghai Zhangjiang Institute of Mathematics\njoeyleesh82@gmail.com\nABSTRACT\nIn human reading and communication, individuals tend to engage\nin geospatial reasoning, which involves recognizing geographic\nentities and making informed inferences about their interrelation-\nships. To mimic such cognitive process, current methods either\nutilize conventional natural language understanding toolkits, or\ndirectly apply models pretrained on geo-related natural language\ncorpora. However, these methods face two significant challenges:\ni) they do not generalize well to unseen geospatial scenarios, and\nii) they overlook the importance of integrating geospatial context\nfrom geographical databases with linguistic information from the\nInternet. To handle these challenges, we propose GeoReasoner, a\nlanguage model capable of reasoning on geospatially grounded nat-\nural language. Specifically, it first leverages Large Language Models\n(LLMs) to generate a comprehensive location description based\non linguistic and geospatial information. It also encodes direction\nand distance information into spatial embedding via treating them\nas pseudo-sentences. Consequently, the model is trained on both\nanchor-level and neighbor-level inputs to learn geo-entity represen-\ntation. Extensive experimental results demonstrate GeoReasoner’s\nsuperiority in three tasks: toponym recognition, toponym linking,\nand geo-entity typing, compared to the state-of-the-art baselines.\nCCS CONCEPTS\n• Information systems →Information retrieval.\nKEYWORDS\nNatural Language Understanding, Large Language Model\nACM Reference Format:\nYibo Yan and Joey Lee. 2024. GeoReasoner: Reasoning On Geospatially\nGrounded Context For Natural Language Understanding. In Proceedings\nof the 33rd ACM International Conference on Information and Knowledge\nManagement (CIKM ’24), October 21–25, 2024, Boise, ID, USA. ACM, New\nYork, NY, USA, 5 pages. https://doi.org/10.1145/3627673.3679934\n1\nINTRODUCTION\nGeospatial reasoning is crucial for understanding and interpreting\nnatural language texts within the context of spatial relationships\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM ’24, October 21–25, 2024, Boise, ID, USA\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0436-9/24/10\nhttps://doi.org/10.1145/3627673.3679934\nFigure 1: Comparison between our LLM-assisted GeoRea-\nsoner framework and conventional paradigms for geospatial\nnatural language understanding task.\n[20, 25]. For example, in the sentence \"I traveled to San Jose in Cali-\nfornia to see the Tech Museum,\" a human can effortlessly recognize\nthe place names \"San Jose\" and \"California\". Additionally, a human\ncan infer that the \"Tech Museum\" is associated with the technology\nindustry prevalent in the region. In practice, geospatially grounded\nlanguage understanding involves essential tasks including identify-\ning the geospatial concepts mentioned and deducing the identities\nof these concepts [2, 17, 22], as shown in Figure 1 (d). Therefore,\nsuch tasks heavily rely on the one-to-one correspondence between\ngeographic entities and the physical world, especially in real-world\nlocation-based services and navigation tasks [9, 14, 28, 29, 31].\nThe rapid development of natural language understanding (NLU)\nhas significantly impacted various fields, including geospatial rea-\nsoning [27]. NLU advancements have enabled more sophisticated\napproaches to understanding and interpreting natural language\ntexts within spatial contexts, which have led to two key paradigms\nfor tackling geospatial reasoning tasks, as shown in Figure 1.\na) Conventional NLU toolkits (e.g., Named Entity Resolution\ntools) are designed to identify and classify named entities in text,\nincluding geographic entities. While these tools are effective, they\noften struggle to generalize to unseen geospatial scenarios, which\nlimits their applicability in dynamic and diverse contexts [23].\nb) Pretrained language models on geo-related corpora are\nleveraged to understand and generate text based on extensive train-\ning data, which includes geographic information [18, 21]. However,\ndespite their advanced capabilities, these models often overlook\nthe importance of combining geospatial context from geographi-\ncal databases with the rich linguistic information available on the\nInternet. This lack of integration can result in models that are less\narXiv:2408.11366v1  [cs.CL]  21 Aug 2024\nCIKM ’24, October 21–25, 2024, Boise, ID, USA\nYibo Yan et al.\nFigure 2: Overall Framework of GeoReasoner.\neffective at reasoning about the spatial relationships and interde-\npendencies between geographic entities [17].\nTo address the aforementioned challenges, we propose GeoRea-\nsoner, a comprehensive framework designed to improve geospatial\nreasoning by integrating both linguistic and geospatial contexts, as\nillustrated in Figure 1 (c). First, Our approach utilizes Large Lan-\nguage Models (LLMs) to generate detailed location descriptions\nby combining information from geographic databases (e.g., Open-\nStreetMap) and the Internet (e.g., Wikipedia). Then, GeoReasoner\nencodes spatial information, such as direction and distance, into\nspatial embeddings by treating them as pseudo-sentences. This in-\nnovative method enables the model to learn robust geo-entity rep-\nresentations through a dual training process that incorporates both\ngeospatial and linguistic data. Eventually, by employing geospa-\ntial contrastive loss [24] and masked language modeling loss [4],\nGeoReasoner achieves superior performance in key geospatial rea-\nsoning tasks, including toponym recognition, toponym linking, and\ngeo-entity typing, outperforming current state-of-the-art baselines.\nIn summary, GeoReasoner represents a significant step forward\nby providing a robust solution for accurately understanding and\nreasoning on geospatial context in natural language texts.\n2\nMETHODOLOGY\n2.1\nData Collection and Preprocessing\nWe preprocess the training corpora by dividing them into two parts:\npseudo-sentence corpora from OpenStreetMap (OSM) to provide\ngeospatial context and natural language corpora from Wikipedia\nand Wikidata to provide linguistic context. As illustrated in Figure\n2 (a), for the geographical dataset, we use OSM, a crowd-sourced\ndatabase with extensive point geo-entities worldwide, which links\nto Wikipedia and Wikidata. We preprocess OSM data to gather\ngeo-entities with these links, creating paired training data for con-\ntrastive pretraining. We linearize geospatial context by constructing\npseudo-sentences for each geo-entity, sorting neighboring geo-\nentity names by distance. For the natural language text corpora, we\nuse Wikipedia and Wikidata. We scrape Wikipedia articles linked\nfrom OSM annotations, break them into sentences, and use Trie-\nbased phrase matching [13] to find sentences containing the corre-\nsponding OSM geo-entity names. Training samples are paragraphs\nwith at least one geo-entity name. We collect Wikidata geo-entities\nvia QID identifier and convert relation triples to natural sentences.\n2.2\nLLM-assisted Location Description\nSummarization\nDue to the noisy nature of the linguistic information associated\nwith a given entity (i.e., anchor entity), it is crucial to rewrite it by re-\ntaining the most significant parts while integrating the surrounding\ncontext. In this phase as shown in Figure 2 (b), a LLM (specifically\nGPT-4 Turbo in this study) is employed as a summarizer, utilizing\nits comprehensive capability to extract key information from both\ngeospatial and linguistic contexts. The geospatial context includes\nlocations and implicit geo-relations of neighboring entities to the\nanchor geo-entity. Meanwhile, the linguistic context encompasses\nessential information that specifies a geo-entity, such as sentences\ndescribing its geography, environment, culture, and history. Con-\nsequently, we define the output of the LLM as a comprehensive\nlocation description of the given anchor entity.\n2.3\nGeo-Entity Representation Pretraining\nAs illustrated in Figure 2 (c), the training process of GeoReasoner\naims to learn and align anchor and neighbor entity information\nwithin the same embedding space, thereby obtaining geospatially\ngrounded language representations. Specifically, GeoReasoner uses\nlocation descriptions as natural language inputs and generates\nanchor-level representations by averaging the token representa-\ntions within the geo-entity name span. For neighbor entity infor-\nmation, GeoReasoner employs the geospatial context linearization\nmethod and spatial embedding module from SpaBERT [16], a pre-\ntrained language model that contextualizes entity representations\nusing point geographic data. Given a anchor geo-entity and its\nspatial neighbors, GeoReasoner linearizes the geospatial context\nby sorting neighbors based on their geospatial distances from the\nanchor geo-entity. It then concatenates the anchor geo-entity’s\nname with the sorted neighbors to form a pseudo-sentence. To\nGeoReasoner: Reasoning On Geospatially Grounded Context For Natural Language Understanding\nCIKM ’24, October 21–25, 2024, Boise, ID, USA\npreserve directional relations and relative distances, GeoReasoner\nuses a geocoordinates embedding module, via a sinusoidal position\nembedding layer.\nTo enable GeoReasoner to process both natural language text\nand geographical data, we leverage a specific position embedding\nmechanism for each token alongside the token embeddings [17].\nAs shown in Figure 2 (c), Position ID describes the token’s index\nposition in the sentence (it starts from zero for both anchor-level and\nneighbor-level inputs). Segment ID indicates the source of the input\ntokens (tokens from the anchor-level input have a Segment ID of\nzero, while those from the neighbor-level input have a Segment ID\nof one). X-coord and Y-coord refer to inputs for the spatial coordinate\nembedding (tokens within the same geo-entity name span share the\nsame X-coord and Y-coord values, but anchor-level tokens, lacking\ngeocoordinate information, have their X-coord and Y-coord set to\nDSEP, a constant distance filler value).\nDuring pretraining, two tasks are employed to learn geospatially\ngrounded representations of natural language text. The first task is\ngeospatial contrastive learning using an InfoNCE loss [24], which\ncontrasts geo-entity features extracted from different contextual\nlevels. This loss encourages GeoReasoner to generate similar rep-\nresentations for the same geo-entity. Consequently, each batch\ncomprises 50% random negatives and 50% hard negatives.\nFormally, Define the training data D consist of pairs of sam-\nples (𝑠𝑙𝑜𝑐\n𝑖\n,𝑠𝑔𝑒𝑜\n𝑖\n), where 𝑠𝑙𝑜𝑐\n𝑖\nand 𝑠𝑔𝑒𝑜\n𝑖\nrefer to anchor-level location\ndescription and neighbor-level pseudo-sentence created from the\ngeospatial context. Both refer to the same geo-entity. Define 𝑓(·) be\nGeoReasoner that takes both 𝑠𝑙𝑜𝑐\n𝑖\nand 𝑠𝑔𝑒𝑜\n𝑖\nas input and generates\nfinal representation h𝑙𝑜𝑐\n𝑖\n= 𝑓(𝑠𝑙𝑜𝑐\n𝑖\n) and h𝑔𝑒𝑜\n𝑖\n= 𝑓(𝑠𝑔𝑒𝑜\n𝑖\n). Hence, the\ngeospatial contrastive loss L𝑐𝑜𝑛\n𝑖\ncan be defined as:\nL𝑐𝑜𝑛\n𝑖\n= −log\n𝑒sim(h𝑙𝑜𝑐\n𝑖\n,h𝑔𝑒𝑜\n𝑖\n)/𝜏\nÍ2𝑁\n𝑗=1 1[𝑗≠𝑖]𝑒sim(h𝑙𝑜𝑐\n𝑖\n,h𝑔𝑒𝑜\n𝑗\n)/𝜏,\nwhere 𝜏and sim(·) refer to temperature and cosine similarity.\nAdditionally, we employ a masked language modeling task [4] on\na concatenation of the paired anchor-level sentence and neighbor-\nlevel pseudo-sentence. This task encourages GeoReasoner to re-\ncover masked tokens, thereby improving its ability to integrate and\nutilize information from both contexts.\n2.4\nGeospatial Downstream Tasks\nOur study further adapts GeoReasoner to the following downstream\ntasks to demonstrate its ability for geospatially grounded language\nunderstanding, as shown in Figure 2 (d).\nToponym recognition involves identifying and extracting place\nnames from unstructured text [8]. This is achieved by adding a\nfully connected layer to GeoReasoner and training the model on a\ndownstream dataset, enabling it to classify each token accurately.\nToponym linking aims to identify the specific geo-entity men-\ntioned in text by matching it to the correct record in a geographic\ndatabase [8]. This database may include multiple entities with the\nsame name as the extracted toponym from the text.\nGeo-entity typing focuses on categorizing location types within\na geographical database. We approach this task as a classification\nproblem by attaching a one-layer classification head to GeoRea-\nsoner. During training, GeoReasoner predicts the type of a central\ngeo-entity from a subregion.\n3\nEXPERIMENT\n3.1\nExperimental Setup\n3.1.1\nDatasets. For toponym recognition, we utilize the GeoWeb-\nNews dataset [8], which includes 200 news articles containing 2,601\ntoponyms with valid geocoordinates. For toponym linking, we use\nthe Local Global Corpus [18] with 588 news articles. For geo-entity\ntyping, we employ the dataset from [16] with geospatial context\nfor the anchor geo-entity. The train/test ratio is 8:2.\n3.1.2\nEvaluation. For toponym recognition, we report precision,\nrecall, and F1 scores at both token-level and entity-level, with entity-\nlevel accuracy requiring exact matches to the ground-truth. In\ntoponym linking, we evaluate the accuracy of linking toponyms to\ntheir correct geo-entities in the GeoNames database, emphasizing\ndisambiguation. For geo-entity typing, we follow [16] and report\nF1 score for each amenity class and overall micro F1 score.\n3.1.3\nBaselines. In toponym recognition, we compare GeoRea-\nsoner with fine-tuned models such as BERT [4], SimCSE-BERT [6],\nSpanBERT [15], SapBERT [19] and GeoLM [17]. For toponym link-\ning, we benchmark against the same models. In geo-entity typing,\nwe extend comparisons to include LUKE [30] and SpaBERT [16],\nwhich leverage specialized tokenization and geospatial context,\nrespectively. All models are evaluated in their base versions.\n3.2\nOverall Performance\n3.2.1\nToponym Recognition. As indicated in Table 1, GeoReasoner\nshows a balanced and strong performance across both token-level\nand entity-level metrics, especially excelling in the identification\nof B-topo tokens. SpanBERT excels in predicting I-topo tokens,\nwhich indicates the benefit of span prediction during pretraining\nfor recognizing the continuation of toponyms. The results suggest\nthat models such as GeoReasoner and GeoLM, which are specifically\ntrained with geospatial grounding, outperform general models like\nBERT and its variants in toponym recognition tasks.\n3.2.2\nGeo-Entity Typing. The results from Table 2 clearly high-\nlight the superior performance of the GeoReasoner and GeoLM\nmodels due to their robust performance among most types. This\nreveals that contrastive learning applied during pretraining has\neffectively facilitated the alignment of linguistic and geospatial\ncontexts, though only geospatial context is given during inference.\nGeoReasoner performs worse on waste management class com-\npared to SpanBERT, but it still exceed the performance of GeoLM,\nwhich also integrates geospatial context with linguistic context.\n3.2.3\nToponym Linking. As shown in Table 3, GeoReasoner in-\ndicates the best capability in terms of top-1 retrieval (R@1) and\ntop-5 retrieval (R@5), and the second best performance on top-10\nretrieval (R@10). This underscores the advantage of integrating\nboth geospatial and linguistic context in GeoReasoner compared to\nbaseline models that emphasize one over the other. Moreover, since\nthis task specifically necessitates models to rely solely on linguistic\nCIKM ’24, October 21–25, 2024, Boise, ID, USA\nYibo Yan et al.\nGeoWebNews\nToken (B-topo)\nToken (I-topo)\nEntity\nPrec\nRecall\nF1\nPrec\nRecall\nF1\nPrec\nRecall\nF1\nBERT\n90.00\n89.28\n89.64\n78.55\n79.44\n78.99\n77.03\n83.42\n80.10\nSimCSE-BERT\n83.86\n90.26\n86.95\n74.61\n82.07\n78.16\n72.76\n83.68\n77.84\nSpanBERT\n85.98\n88.37\n87.16\n86.13\n89.19\n87.63\n75.32\n81.16\n78.13\nSapBERT\n83.12\n88.32\n85.64\n76.26\n81.11\n78.61\n72.48\n80.16\n76.12\nGeoLM\n91.15\n90.43\n90.79\n79.16\n84.27\n81.63\n82.18\n85.67\n83.89\nGeoReasoner\n93.22\n91.27\n92.23\n81.56\n87.90\n84.61\n84.66\n85.78\n85.17\nTable 1: Result of toponym recognition task on GeoWebNews dataset. Bolded and underlined numbers are best and second\nbest scores, respectively. We frame this task as a multi-class sequence tagging problem, where tokens are classified into one of\nthree classes: B-topo (beginning of a toponym), I-topo (inside a toponym), or O (non-toponym).\nClasses\nEdu.\nEnt.\nFac.\nFin.\nHea.\nPub.\nSus.\nTra.\nWas.\nMicro F1\nBERT\n67.4\n63.4\n76.3\n92.9\n85.6\n87.2\n85.6\n86.2\n67.8\n83.5\nSpanBERT\n63.3\n58.9\n60.8\n91.6\n85.9\n88.2\n82.4\n86.7\n73.5\n81.9\nSimCSE-BERT\n62.3\n59.0\n50.4\n92.5\n86.7\n85.2\n85.7\n81.0\n47.0\n81.0\nLUKE\n64.8\n60.8\n59.8\n94.5\n85.7\n86.7\n85.4\n85.1\n51.7\n82.5\nSpaBERT\n67.4\n65.3\n68.0\n95.9\n86.5\n90.0\n88.3\n88.8\n70.3\n85.2\nGeoLM\n72.5\n70.9\n73.0\n97.8\n91.5\n83.6\n90.5\n90.8\n62.2\n87.8\nGeoReasoner\n73.8\n72.2\n76.0\n98.0\n93.3\n87.9\n92.1\n90.6\n65.8\n88.9\nTable 2: Result of geo-entity typing task. Column names are the OSM classes (education, entertainment, facility, financial,\nhealthcare, public service, sustenance, transportation and waste management). Bolded and underlined numbers are best and\nsecond best scores.\nLGL\nR@1\nR@5\nR@10\nBERT\n34.6\n67.5\n78.1\nRoBERTa\n24.2\n48.7\n60.6\nSpanBERT\n25.2\n48.8\n61.0\nSapBERT\n30.8\n58.8\n72.2\nGeoLM\n38.2\n65.3\n72.6\nGeoReasoner\n40.1\n68.3\n75.9\nTable 3: Result of toponym linking task on LGL dataset.\nBolded and underlined numbers are best and second best\nscores, respectively. R@𝑘measures whether ground-truth\nGeoNames ID presents among the top 𝑘retrieval results.\nAblation Settings\nPrec Recall\nF1\nGeoReasoner\n84.66\n85.78\n85.17\nGeoReasoner w/o contrastive loss\n73.67\n79.45\n76.45\nGeoReasoner w/o MLM loss\n78.23\n80.22\n79.21\nGeoReasoner w/o spatial embed.\n76.98\n85.31\n80.93\nGeoReasoner w/o LLM summarization\n79.12\n83.42\n81.17\nTable 4: Ablation study on toponym recognition task.\ncontext, GeoReasoner’s geospatial contrastive learning paradigm\nproves beneficial by facilitating alignment among sources.\n3.3\nAblation Study\nFour ablation experiments are conducted on toponym recognition\ntask: i) removing the geospatial contrastive loss; ii) removing the\nmasked language modeling loss; iii) removing the spatial coordi-\nnate embedding layer that leverages geo coordinates as input; iv)\nremoving the inclusion of LLM for location description summariza-\ntion (i.e., linguistic context will be directly used as anchor-level\ninput for GeoReasoner). Results from Table 4 demonstrate a signifi-\ncant drop in performance when either the contrastive loss or the\nmasked language modeling loss is removed. This underscores their\ncrucial roles during pretraining. Furthermore, all metrics decrease\nwhen the spatial coordinate embedding layer is omitted, indicating\nthat geocoordinates encapsulate implicit geospatial context. Addi-\ntionally, performance declines without LLM-based summarization,\nlikely due to the noisy nature of the original linguistic context.\n4\nRELATED WORK\nResearch on understanding geospatial concepts in natural language\nhas used general-purpose NER tools like Stanford NER [5] and\nNeuroNER [3], as well as geospatial-specific tools such as the Edin-\nburgh Geoparser [10, 26] and Mordecai [11, 12], to identify and link\ntoponyms to geographical databases. Deep learning models, includ-\ning TopoCluster [1] and CamCoder [7], have connected toponyms\nwith geographic locations using geodesic and lexical features. How-\never, these models often lack effective generalizability of unseen\ngeospatial context during inference. Furthermore, SpaBERT [16]\nserves as a language model trained on geographical corpora, but it\ncannot leverage linguistic information from the Internet to obtain\ncomprehensive geo-entity representation. GeoLM [17] manages to\nfuse geospatial and linguistic information together via projecting\nthem into a joint embedding. Our GeoReasoner framework stands\nout by harnessing LLMs’ powerful summarization abilities to derive\nanchor entity information as linguistic context, while seamlessly\nincorporating geospatial context through concatenation.\n5\nCONCLUSION\nGeoReasoner effectively addresses the challenges of geospatial rea-\nsoning in natural language processing by integrating linguistic\ninformation with geospatial context. It leverages powerful LLMs to\ngenerate location descriptions and encodes spatial information as\npseudo-sentences. In future research, we will focus on integrating\ngeospatial reasoning with the reasoning capabilities of LLMs.\nGeoReasoner: Reasoning On Geospatially Grounded Context For Natural Language Understanding\nCIKM ’24, October 21–25, 2024, Boise, ID, USA\nREFERENCES\n[1] Grant DeLozier, Jason Baldridge, and Loretta London. 2015.\nGazetteer-\nindependent toponym resolution using geographic word profiles. In Proceedings\nof the AAAI Conference on Artificial Intelligence, Vol. 29.\n[2] Cheng Deng, Le Zhou, Yi Xu, Tianhang Zhang, Zhouhan Lin, Xinbing Wang,\nand Chenghu Zhou. 2024. Geoscience Knowledge Understanding and Utilization\nvia Data-centric Large Language Model. Technical Report. Copernicus Meetings.\n[3] Franck Dernoncourt, Ji Young Lee, and Peter Szolovits. 2017. NeuroNER: an\neasy-to-use program for named-entity recognition based on neural networks.\narXiv preprint arXiv:1705.05487 (2017).\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[5] Jenny Rose Finkel, Trond Grenager, and Christopher D Manning. 2005. Incor-\nporating non-local information into information extraction systems by gibbs\nsampling. In Proceedings of the 43rd annual meeting of the association for compu-\ntational linguistics (ACL’05). 363–370.\n[6] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive\nlearning of sentence embeddings. arXiv preprint arXiv:2104.08821 (2021).\n[7] Milan Gritta, Mohammad Taher Pilehvar, and Nigel Collier. 2018. Which mel-\nbourne? augmenting geocoding with maps. Association for Computational Lin-\nguistics.\n[8] Milan Gritta, Mohammad Taher Pilehvar, and Nigel Collier. 2020. A pragmatic\nguide to geoparsing evaluation: Toponyms, Named Entity Recognition and prag-\nmatics. Language resources and evaluation 54 (2020), 683–712.\n[9] Milan Gritta, Mohammad Taher Pilehvar, Nut Limsopatham, and Nigel Collier.\n2018. What’s missing in geographical parsing? Language Resources and Evaluation\n52 (2018), 603–623.\n[10] Claire Grover, Richard Tobin, Kate Byrne, Matthew Woollard, James Reid, Stuart\nDunn, and Julian Ball. 2010. Use of the Edinburgh geoparser for georeferencing\ndigitized historical collections. Philosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences 368, 1925 (2010), 3875–3889.\n[11] Andrew Halterman. 2017. Mordecai: Full Text Geoparsing and Event Geocoding.\nThe Journal of Open Source Software 2, 9 (2017). https://doi.org/10.21105/joss.\n00091\n[12] Andrew Halterman. 2023. Mordecai 3: A Neural Geoparser and Event Geocoder.\narXiv preprint arXiv:2303.13675 (2023).\n[13] Bo-June Hsu and Giuseppe Ottaviano. 2013. Space-efficient data structures for\ntop-k completion. In Proceedings of the 22nd international conference on World\nWide Web. 583–594.\n[14] Xuke Hu, Zhiyong Zhou, Hao Li, Yingjie Hu, Fuqiang Gu, Jens Kersten, Hongchao\nFan, and Friederike Klan. 2022. Location reference recognition from texts: A\nsurvey and comparison. arXiv preprint arXiv:2207.01683 (2022).\n[15] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and\nOmer Levy. 2020. Spanbert: Improving pre-training by representing and predict-\ning spans. Transactions of the association for computational linguistics 8 (2020),\n64–77.\n[16] Zekun Li, Jina Kim, Yao-Yi Chiang, and Muhao Chen. 2022. Spabert: a pre-\ntrained language model from geographic data for geo-entity representation.\narXiv preprint arXiv:2210.12213 (2022).\n[17] Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, and Muhao Chen. 2023. Geolm: Em-\npowering language models for geospatially grounded language understanding.\narXiv preprint arXiv:2310.14478 (2023).\n[18] Michael D Lieberman, Hanan Samet, and Jagan Sankaranarayanan. 2010. Geo-\ntagging with local lexicons to build indexes for textually-specified spatial data.\nIn 2010 IEEE 26th international conference on data engineering (ICDE 2010). IEEE,\n201–212.\n[19] Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier.\n2020. Self-alignment pretraining for biomedical entity representations. arXiv\npreprint arXiv:2010.11784 (2020).\n[20] Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao\nLiu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, et al. 2023. On the opportu-\nnities and challenges of foundation models for geospatial artificial intelligence.\narXiv preprint arXiv:2304.06798 (2023).\n[21] Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitzeman, Rob Quimby, Justin\nRicher, Ben Wellner, Scott Mardis, and Seamus Clancy. 2010. SpatialML: annota-\ntion scheme, resources, and evaluation. Language Resources and Evaluation 44\n(2010), 263–280.\n[22] Tessa Masis and Brendan O’Connor. 2024. Where on Earth Do Users Say They\nAre?: Geo-Entity Linking for Noisy Multilingual User Input. arXiv:2404.18784\n(2024).\n[23] Palanichamy Naveen, Rajagopal Maheswar, and Pavel Trojovsk`y. 2024. GeoNLU:\nBridging the gap between natural language and spatial data infrastructures.\nAlexandria Engineering Journal 87 (2024), 126–147.\n[24] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n[25] Kristin Stock and Hans Guesgen. 2016. Geospatial reasoning with open data. In\nAutomating open source intelligence. Elsevier, 171–204.\n[26] Richard Tobin, Claire Grover, Kate Byrne, James Reid, and Jo Walsh. 2010. Eval-\nuation of Georeferencing. In Proceedings of the 6th Workshop on Geographic\nInformation Retrieval (Zurich, Switzerland) (GIR ’10). Association for Computing\nMachinery, New York, NY, USA, Article 7, 8 pages.\nhttps://doi.org/10.1145/\n1722080.1722089\n[27] Sean Tucker. 2024. A systematic review of geospatial location embedding ap-\nproaches in large language models: A path to spatial AI systems. arXiv preprint\narXiv:2401.10279 (2024).\n[28] Jan Oliver Wallgrün, Morteza Karimzadeh, Alan M MacEachren, and Scott\nPezanowski. 2018. GeoCorpora: building a corpus to test and train microblog\ngeoparsers. International Journal of Geographical Information Science 32, 1 (2018),\n1–29.\n[29] Jimin Wang, Yingjie Hu, and Kenneth Joseph. 2020. NeuroTPR: A neuro-net\ntoponym recognition model for extracting locations from social media messages.\nTransactions in GIS 24, 3 (2020), 719–735.\n[30] Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto.\n2020. LUKE: Deep contextualized entity representations with entity-aware self-\nattention. arXiv preprint arXiv:2010.01057 (2020).\n[31] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen,\nRoger Zimmermann, and Yuxuan Liang. 2024. Urbanclip: Learning text-enhanced\nurban region profilig with contrastive language-image pretraining from the web.\nIn Proceedings of the ACM on Web Conference 2024. 4006–4017.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2024-08-21",
  "updated": "2024-08-21"
}