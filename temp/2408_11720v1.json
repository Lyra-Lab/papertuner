{
  "id": "http://arxiv.org/abs/2408.11720v1",
  "title": "On Learnable Parameters of Optimal and Suboptimal Deep Learning Models",
  "authors": [
    "Ziwei Zheng",
    "Huizhi Liang",
    "Vaclav Snasel",
    "Vito Latora",
    "Panos Pardalos",
    "Giuseppe Nicosia",
    "Varun Ojha"
  ],
  "abstract": "We scrutinize the structural and operational aspects of deep learning models,\nparticularly focusing on the nuances of learnable parameters (weight)\nstatistics, distribution, node interaction, and visualization. By establishing\ncorrelations between variance in weight patterns and overall network\nperformance, we investigate the varying (optimal and suboptimal) performances\nof various deep-learning models. Our empirical analysis extends across widely\nrecognized datasets such as MNIST, Fashion-MNIST, and CIFAR-10, and various\ndeep learning models such as deep neural networks (DNNs), convolutional neural\nnetworks (CNNs), and vision transformer (ViT), enabling us to pinpoint\ncharacteristics of learnable parameters that correlate with successful\nnetworks. Through extensive experiments on the diverse architectures of deep\nlearning models, we shed light on the critical factors that influence the\nfunctionality and efficiency of DNNs. Our findings reveal that successful\nnetworks, irrespective of datasets or models, are invariably similar to other\nsuccessful networks in their converged weights statistics and distribution,\nwhile poor-performing networks vary in their weights. In addition, our research\nshows that the learnable parameters of widely varied deep learning models such\nas DNN, CNN, and ViT exhibit similar learning characteristics.",
  "text": "On Learnable Parameters of Optimal and\nSuboptimal Deep Learning Models\nZiwei Zheng1, Huizhi Liang1, Vaclav Snasel2, Vito Latora3, Panos Pardalos4,\nGuiseppe Nicosia5, and Varun Ojha1\n1 Newcastle University, Newcastle, UK\n2 Technical University of Ostrava, Ostrava, Czech Republic\n3 Queen Mary University of London, UK\n4 University of Florida, FL, USA\n5 University of Catania, Catania, Italy\nAbstract. We scrutinize the structural and operational aspects of deep\nlearning models, particularly focusing on the nuances of learnable pa-\nrameters (weight) statistics, distribution, node interaction, and visual-\nization. By establishing correlations between variance in weight patterns\nand overall network performance, we investigate the varying (optimal and\nsuboptimal) performances of various deep-learning models. Our empiri-\ncal analysis extends across widely recognized datasets such as MNIST,\nFashion-MNIST, and CIFAR-10, and various deep learning models such\nas deep neural networks (DNNs), convolutional neural networks (CNNs),\nand vision transformer (ViT), enabling us to pinpoint characteristics of\nlearnable parameters that correlate with successful networks. Through\nextensive experiments on the diverse architectures of deep learning mod-\nels, we shed light on the critical factors that influence the functionality\nand efficiency of DNNs. Our findings reveal that successful networks, ir-\nrespective of datasets or models, are invariably similar to other success-\nful networks in their converged weights statistics and distribution, while\npoor-performing networks vary in their weights. In addition, our research\nshows that the learnable parameters of widely varied deep learning mod-\nels such as DNN, CNN, and ViT exhibit similar learning characteristics.\nKeywords: deep neural networks · convolutional neural networks · vi-\nsion transformers · weight distribution · node strength\n1\nIntroduction\nDeep learning has achieved impressive results in the fields of computer vision,\nnatural language processing, and speech recognition. From face recognition [10]\nto autonomous driving [3], to medical analysis [12], the deep learning models\nhave widely been used in various significant tasks. In particular, deep learning\nmodels can outperform human experts in many application scenarios. However,\nthe lack of understanding of deep learning models, such as deep neural networks\n(DNNs), convolution neural networks (CNNs), and vision transformers (ViTs),\nhas caused widespread concern and controversy [14]. For users, deep learning\narXiv:2408.11720v1  [cs.LG]  21 Aug 2024\n2\nZ. Zheng et al.\nmodels are mysterious black boxes whose decision-making processes are difficult\nto explain and understand. This opacity can bring unpredictable risks in actual\nmission-critical situations, especially in areas sensitive to security. For example,\nopaque automated medical diagnosis models may produce incorrect treatment\nrecommendations, posing a threat to patients’ health [12].\nWe propose a novel methodology to perform comprehensive experiments to\nexplore the reasons behind the varying performances of deep learning models,\nparticularly focusing on the learnability of weight matrices in models that yield\ndifferent accuracy rates on similar and varied network architectures. Our inves-\ntigation delves into the characteristics of the model’s learnable parameters to\nunderstand why models with similar or differing architectures can result in such\nvaried performances. We analyze trained networks such as the gravity of weight\n(i.e., average weight and their distribution), node strength differences, and visu-\nalization of their position in high dimensional space using t-SNE mapping.\nOur approach involves conducting experiments on three pattern recognition\ndatasets MNIST, Fashion-MNIST, and CIFAR-10, using a range of network ar-\nchitectures such as DNN, CNN, and ViT networks. These experiments are de-\nsigned to reveal how the weight distributions in neural networks (NN) relate to\ntheir learning efficiency and decision-making capabilities. The main contribu-\ntions of this paper are as follows:\n– We present a novel methodology for a comprehensive empirical study for\nidentifying the critical role of converged weights and node strengths in char-\nacterizing the uncertainty of deep learning success and failure.\n– We perform a visual analysis of deep learning converged weights and node\nstrength that help characterize the optimal and suboptimal networks.\n– Our study investigates common factors among the learnable parameters of\nvaried deep learning architectures that process datasets varyingly in making\npattern recognition, such as DNN, CNN, and ViT.\n– Our findings reveal that successful networks, irrespective of tasks or models,\nare invariably similar to other successful networks in their converged weights,\nwhile poor-performing networks vary in their weights.\n2\nRelated work\nThe analysis of NNs has unveiled insights into their performance and optimiza-\ntion. Voita et al. [16] investigate the redundancy in Transformer architectures’\nmulti-head self-attention mechanism, while Frankle and Carbin [2] identify ‘lot-\ntery ticket’ initialization in weight matrices that could predict network perfor-\nmance success. These studies emphasize the significance of weight matrix charac-\nteristics, a theme our research echoes by analyzing how these structural elements\ninfluence learning outcomes.\nFurther exploration by Neyshabur et al. [7] into over-parameterization and its\nimpact on model generalization complements our analysis. The work by Scabini\nand Bruno [9], demonstrating the correlation between neuronal centrality and\nnetwork performance, resonates with our focus on the layer-wise node interaction\nOn Learnable Parameters of Optimal and Suboptimal Deep Learning Models\n3\nattributes of analysis. Rauber et al. [8] consider dimensionality reduction of\nweights, of which we use a similar method in this paper to visualize weights\non high dimensional space. Similarly, Naitzat et al.’s [6] examination of how\nnetworks transform data topology and Semenova et al.’s investigation into noise\npropagation in NNs [11] align with our interest in the underlying mechanisms\nof data representation and processing in neural models. Our approach similarly\nseeks to uncover the structural determinants that underlie these phenomena.\nThese pivotal studies collectively advance our comprehension of DNNs, in-\nforming both theoretical insights and practical applications in the field. Our\nwork, while building on these foundations, provides a unique perspective by em-\nphasizing the importance of weight matrices in understanding and optimizing\nneural network behavior.\n3\nCharacterization of learnable parameters\nWe present a novel methodology to systematically investigate the weight (learn-\nable parameters) of trained NNs to uncover their learning dynamics. In our\nmethodology, we experimented (with various trials from 30 to 1000 for each\nconfiguration depending on computational budget) with various architectures of\nthree different deep learning models: DNNs, CNNs, and ViTs. Several architec-\nture configurations (from very minimal to large networks) of these three models\nwere trained on three well-known and widely used benchmark datasets of vary-\ning complexities: MNIST, Fashion MNIST, and CIFAR-10. In the first analysis\nstage, we focus on the weight statistics of trained networks. We computed the\nmean µw = 1/N PN\ni=1 wi and standard deviation σw =\nq\n1/N PN\ni=1(wi −µw)2 of\nconverged N number of network weights wi. In addition to weight statistics to\ncharacterize the optimal and suboptimal networks, we analyze the distribution\nof the weights using a normalized histogram and kernel density estimation of the\nnetwork weights.\nIn the second stage, we focus on node strength and pair-wise node strength\nanalysis. We, therefore, compute the strength of the nodes, which is the sum of\nincoming absolute weight values at a node as S = PNj\ni=1 |wi|,, where Nj is the\ntotal number of weight wi incident on j-th node in a network. In the CNN case,\nnode strength was kernels, and for ViT, it was the attention layer.\nIn our final stage, we project the network weights (of layers) to high-dimensional\nspace using t-SNE [5] in order to assess the position of the high-dimensional\nweight vectors for the characterization network varied learning capabilities.\n3.1\nElements of model architecture for characterization\nWhen experimenting with DNN models, we selected the weight matrices between\nfully connected (FC) layers of DNNs, including the weight matrices between the\nfinal FC layer and the decision layer (output layer). This was to perform a layer-\nwise assessment of trends and variability in converged DNN weight and not only\nthe entire network. This was done to observe closely the layer-wise differences\n4\nZ. Zheng et al.\nbetween optimal and suboptimal networks. We vary the architectures (an input\nlayer, two hidden layers, and an output layer) of DNNs by changing the size of\ntheir hidden layer from a very minimum network size to a large network size. This\nwas done incrementally and systematically by increasing the network size until\nthe performance of DNNs reached a saturation level with maximum accuracy on\nrespective datasets (e.g., 99%+ for the MNIST dataset) [4].\nSimilarly, we use a simple architecture for CNN models with one convolu-\ntional layer and one FC layer [4,13]. Their size increases similarly to the size\nof DNN models. We started with a minimal network and reached a maximum\nnetwork when the performance on the data sets reached saturation. We also de-\nsigned a minimal ViT model with an encoder that takes a minimum of 2 heads\nto a maximum of 16 heads [15,1].\nWe systematically increased the architecture size from a minimal architecture\nto an architecture that gave a high accuracy (e.g., 99%+ for the MNIST dataset)\nwith an aim to identify the minimal network that may perform well for these\ndatasets and to keep the computational overhead reasonable for expensive trails.\n3.2\nCharacterization of deep learning convergence profiles\nWe aimed to perform the training of deep learning models over a fixed number\nof epochs and asses various convergence profiles. We analyze these convergence\nprofiles and identify three groups. The first group consisted of high-performing\nclusters whose accuracy was close to that of the best-performing network among\nall trials. The second group was low-accuracy convergence profiles whose net-\nwork accuracy was close to worst-performing networks. We also identified mid-\naccuracy clusters of networks whose performance was close to 50% accuracy.\n3.3\nExperimental setup\nIn our research, we adopted three computer vision datasets: MNIST, Fashion\nMNIST (FMNIST), and CIFAR-10. These datasets are widely used in deep\nlearning and image classification, providing diversity and complexity (difficulty\nlevel) for our deep learning network analysis. Table 1 shows the complete set of\nexperiments for DNN, CNN, and ViT models.\nWe experimented with DNNs as initial networks on image classification tasks.\nThe DNN model architecture includes an input layer, two hidden layers, and an\noutput layer. These layers are connected through relu activation function. The\ninput size corresponds to the image size of the respective datasets, and the output\nsize is 10, which corresponds to the number of categories for classification. Our\nmodel also includes two hidden layers, and the number of nodes in each hidden\nlayer varies between 5 and 200 to achieve feature representation.\nTo ascertain the ubiquity of the observed phenomena beyond DNN networks,\nexperiments were extended to both CNNs and Transformer-based architectures\n(ViTs). Table 1 shows the network and hyperparameters settings. Our CNN\nOn Learnable Parameters of Optimal and Suboptimal Deep Learning Models\n5\nTable 1. Experiment settings for three deep learning models: DNN, CNN, and Vision\nTransformer (ViT). η indicates the learning rate, C indicates the number of input\nchannels, and θ indicates weight initialization.\nNetwork\nData\nLayer\nInput\nOutput\nActivation\nSetting\nValue\nDNN\nMNIST/\nF-MNIST\nFC1\n28×28\n5-200\nrelu\nbatch\n100\nFC2\n5-200\n5-200\nrelu\nepochs\n20\nFC3\n5-200\n10\nsoftmax\noptimizer Adam\nCIFAR-10\nFC1\n3×32×32\n5-1000\nrelu\nη\n0.001\nFC2\n5-1000\n5-1000\nrelu\nθ\nnormal\nFC3\n5-1000\n10\nsoftmax\nruns\n1000\nCNN\nMNIST\nF-MNIST\nConv1\n28×28×1\n26×26×C\nrelu\nbatch\n100\nFC\n26×26×C\n10\nsoftmax\nepochs\n20\nCIFAR-10\nConv1\n3×32×32\n30×30×C\nrelu\nη\n0.001\nFC\n30×30×C\n10\nsoftmax\nθ\nnormal\nruns\n1000\nViT\nMNIST\nF-MNIST\nCIFAR-10\nEncoder d_model=784\nnhead=2-16\n-\n-\nbatch\n100\nFC\n784 (input)\n10\n-\nη\n0.001\n-\n-\n-\n-\nruns\n30\nmodel consists of a convolutional layer and an FC followed by a global aver-\nage pooling layer. The vision transformer, which contains an encoder with a\nsubsequent fully connected layer, was used.\nThe specific parameter settings are as follows: For the training, the batch\nsize is 100, epochs are set to 20, and the optimizer used is Adam. The learning\nrate η was set to 0.001, and the network weight was initialized using a normal\ndistribution. The total number of experiments conducted was 1000 for DNNs\nand CNNs experiments. However, for ViT experiments, due to computational\ncosts, 30 trials were performed across all datasets. The consistent choice of hy-\nperparameters across the datasets ensures a standardized experimental setup,\nenabling a direct comparison of results across these datasets.\nThese hyperparameters were chosen to ensure consistent training across dif-\nferent datasets while still allowing efficient convergence. Adam optimizer was\nchosen for its adaptive learning rate capabilities and normal weight initializa-\ntion, which ensure a symmetrical distribution of weights at the start of training.\nRunning the experiment 1000 times provides a comprehensive understanding of\nthe model’s performance across different initializations and random shuffles.\n4\nResults and analysis\nWe initially conducted a series of 1000 runs of experiments on three datasets us-\ning DNN/CNN networks and 30 runs of experiments of ViTs detailed in Table 1.\nThe aggregated results are summarized in Table 2 and show substantial varia-\n6\nZ. Zheng et al.\ntions (uncertainty) in training accuracy under both identical and diverse network\nconfigurations. Such disparities are visible across all three datasets: MNIST, FM-\nNIST, and CIFAR-10. This indicates that similar network architecture/training\nsetups yield a wide spectrum of performances. In Table 2, results were grouped\ninto three accuracy groups: low accuracy, mid accuracy, and high accuracy.\nTable 2. Accuracy distribution groups (low, mid, and high) of varied deep learning\nmodels (DNNs, CNNs, and ViTs) for their several experiment runs as mentioned in\nTable 1 over three datasets. MNIST, Fashion MNIST (F-MNIST), and CIFAR-10. In\nTable 2, min, med, and max indicate the minimum, median, and maximum accuracy\nvalues of the group. The group ’non’ indicates the group of models that do not converge.\nMNIST\nF-MNIST\nCIFAR-10\nnetwork group\nmin\nmed\nmax\nmin\nmed\nmax\nmin\nmed\nmax\nDNN\nnon\n-\n11.35\n-\n-\n10.00\n30.00\n-\n11.35\n20.00\nlow\n30.00 39.33\n55.00\n30.00 72.90\n75.00\n20.00 30.94\n32.00\nmid\n80.00 80.91\n82.00\n83.50 83.67\n84.00\n45.00 52.29\n55.00\nhigh\n95.00 98.55 100.00 95.00 95.88 100.00 75.00 77.80 100.00\nCNN\nlow\n0.00\n93.77\n95.00\n0.00\n85.93\n90.00\n0.00\n40.39\n55.00\nmid\n96.00 97.68\n98.00\n96.00 97.71\n98.00\n55.00 63.36\n75.00\nhigh\n99.50 99.87 100.00 99.50 99.95 100.00 80.00 88.91 100.00\nViT\nlow\n0.00\n22.11\n30.00\n0.00\n30.40\n40.00\n0.00\n17.50\n20.00\nmid\n70.00 74.58\n85.00\n65.00 69.59\n72.00\n0.00\n38.18\n40.00\nhigh\n85.00 90.71 100.00 74.00 74.59 100.00 40.00 44.56 100.00\nObserving the convergence of networks, as shown in Fig. 4, when training\nproceeds, network losses that reach higher accuracy steadily and continuously\ndecrease, indicating that learning is effective. There is a sharp contrast in the\nconvergence behavior between high- and low-accuracy networks - the former\ngroup shows a centralized approach that minimizes the loss, while the later\ngroup of lower-accuracy networks fluctuates greatly, with the convergence plot\nfluctuating for signs that show the struggle to extract and preserve patterns\ncritical to high performance.\nFor DNN experiments, there is a group called ‘non,’ which are the networks\nthat did not converge. These networks are also visible in blue color convergence\nlines in Fig. 4. We thoroughly analyzed these non-converging networks and found\nthat these networks were the minimal ‘input-5-5-class-category’ DNN architec-\nture whose weight initialization was close to zero, and their computed gradient\nfluctuated in both directions and did not propagate beyond the output layer in\nany of the 20 epochs of the training.\nThe experimental results in Table 2 show considerable variety in the accuracy\nof networks, whether using DNN, CNN, or ViT. This high degree of uncertainty\nin performance may be due to various reasons, including random weight initial-\nization, random shuffling of training data, and the optimization algorithm used.\nOn Learnable Parameters of Optimal and Suboptimal Deep Learning Models\n7\nHowever, the training settings were similar to our experiments. Thus, this vari-\nety indicates real-world deep learning model training challenges, and the results\ndemonstrate that training uncertainty and fluctuations are pervasive in network\nstructure. This is not just a problem with a specific network or initialization\nmethod but a common deep-learning phenomenon.\nMNIST\nFMNIST\nCIFAR-10\nDNN\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n19\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLoss\nnon\nlow\nmid\nhigh\n0\n20\n40\n60\n80\n100\nAccuracy\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLoss\nnon\nlow\nmid\nhigh\n0\n20\n40\n60\n80\n100\nAccuracy\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLoss\nnon\nlow\nmid\nhigh\n0\n20\n40\n60\n80\n100\nAccuracy\nCNN\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLoss\nlow\nmid\nhigh\n90\n92\n94\n96\n98\n100\nAccuracy\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nLoss\nlow\nmid\nhigh\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nAccuracy\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.5\n1.0\n1.5\n2.0\nLoss\nlow\nmid\nhigh\n0\n20\n40\n60\n80\n100\nAccuracy\nViT\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLoss\nlow\nmid\nhigh\n0\n20\n40\n60\n80\n100\nAccuracy\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLoss\nlow\nmid\nhigh\n0\n20\n40\n60\n80\n100\nAccuracy\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n19\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLoss\nlow\nmid\nhigh\n0\n10\n20\n30\n40\n50\nAccuracy\nFig. 1. Convergence of various DNN, CNN, and ViT models over MNIST, FMNIST,\nand CIFAR-10 datasets. The x-axis and y-axis represent the training epoch and training\nloss. The color represents the test accuracy of models. Models convergence profiles\nare categorized into low, mid, and high accuracy model groups. For DNN, the group\n‘non’ (in blue) indicates a group of models that happened to be initialized randomly\naround zero weight and consequently have not converged over epochs due to fluctuation\nin gradient around zero and vanishing of gradients to previous layers and were no\npart of the subsequent analysis. The model characterization is therefore performed for\nsuccessful networks (high accuracy), marginally successful networks (mid accuracy),\nand failed networks (low accuracy).\nWe, therefore, present the characterization of the networks using the weight\nstrength statistic analysis (in Figs. 2, 3, and 4), weight distribution analysis using\n8\nZ. Zheng et al.\nkernel density estimate (in Figs. 5, 6, and 7), node strength analysis (in Figs. 8,\n9, and 10), and weight projection analysis (in Figs. 11, 12, and 13):\nWeight strength analysis. Our comprehensive investigation using weight\nstatistics (the mean (x-axis) and standard deviation (y-axis) of weight) in Figs. 2,\n3, and 4 presents a compelling picture of the discrimination between optimal\nand suboptimal networks. High-accuracy networks (shown in dark red) in DNN,\nCNN, and ViT architectures show tight clustering of weights with low standard\ndeviation, indicating a stable and efficient learning state, respectively. On the\ncontrary, we observe that the low-accuracy and mid-accuracy networks show\nhigh variance in their performances, indicating unstable weight convergence.\nSpecifically, we observe that this variance in the successful network’s weight\ndecreases in the final/classification layer (clearly observed in DNNs and CNNs\nfinal FC layer), indicating that the high-accuracy network has finely optimized\nweight belonging to a particular distribution, which is crucial to enhancing cor-\nrect feature detection and classification. In contrast, when networks fail, their\nweight distribution is scattered, indicating the networks are prematurely stuck\nat suboptimal/local optimal space. In this context, we also analyzed weight dis-\ntribution patterns across different accuracy groups in Figs. 5, 6, and 7, revealing\na clear peak near zero for high-accuracy models, indicative of their ability to em-\nphasize crucial features while minimizing noise. This contrasts with the flatter\ndistributions seen in mid and low-accuracy models, which suggests less efficient\nfeature discrimination and potential overfitting to non-essential features.\nI/P - FC1\nFC1 - FC2\nFC2 -O/P\nWhole Net\nMNIST\n0.015 0.010 0.005 0.000 0.005\nmean\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nstd\nnon\nlow\nmid\nhigh\n0.05 0.00\n0.05\n0.10\n0.15\n0.20\nmean\n0.0\n0.1\n0.2\n0.3\n0.4\nstd\nnon\nlow\nmid\nhigh\n0.20\n0.15\n0.10\n0.05\n0.00\nmean\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nstd\nnon\nlow\nmid\nhigh\n0.015 0.010 0.005 0.000 0.005\nmean\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nstd\nnon\nlow\nmid\nhigh\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFMNIST\n0.04\n0.03\n0.02\n0.01\n0.00\n0.01\nmean\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nstd\nnon\nlow\nmid\nhigh\n0.0\n0.1\n0.2\n0.3\nmean\n0.0\n0.2\n0.4\n0.6\n0.8\nstd\nnon\nlow\nmid\nhigh\n0.20\n0.15\n0.10\n0.05\n0.00\nmean\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nstd\nnon\nlow\nmid\nhigh\n0.04\n0.03\n0.02\n0.01\n0.00\nmean\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nstd\nnon\nlow\nmid\nhigh\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCIFAR-10\n0.002\n0.000\n0.002\n0.004\nmean\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nstd\nnon\nlow\nmid\nhigh\n0.02\n0.00\n0.02\n0.04\n0.06\nmean\n0.02\n0.04\n0.06\n0.08\n0.10\nstd\nnon\nlow\nmid\nhigh\n0.03\n0.02\n0.01\nmean\n0.05\n0.10\n0.15\n0.20\nstd\nnon\nlow\nmid\nhigh\n0.002\n0.000\n0.002\n0.004\nmean\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nstd\nnon\nlow\nmid\nhigh\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFig. 2. DNN weight analysis for optimal and suboptimal network characterization.\nOn Learnable Parameters of Optimal and Suboptimal Deep Learning Models\n9\nConv1\nFC\nAll\nMNIST\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\nmean\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nstd\nlow\nmid\nhigh\n0.12\n0.10\n0.08\n0.06\n0.04\n0.02\n0.00\nmean\n0.10\n0.15\n0.20\n0.25\n0.30\nstd\nlow\nmid\nhigh\n0.12\n0.10\n0.08\n0.06\n0.04\n0.02\n0.00\nmean\n0.10\n0.15\n0.20\n0.25\n0.30\nstd\nlow\nmid\nhigh\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nmean of weights\nstd of weights\nFMNIST\n0.2\n0.1\n0.0\n0.1\nmean\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nstd\nlow\nmid\nhigh\n0.08\n0.06\n0.04\n0.02\nmean\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\n0.300\nstd\nlow\nmid\nhigh\n0.08\n0.06\n0.04\n0.02\nmean\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\n0.300\nstd\nlow\nmid\nhigh\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\nAccuracy\nlow\nhigh\nCIFAR-10\n0.010\n0.005\n0.000\n0.005\n0.010\n0.015\nmean\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nstd\nlow\nmid\nhigh\n0.012\n0.010\n0.008\n0.006\n0.004\nmean\n0.09\n0.10\n0.11\n0.12\n0.13\nstd\nlow\nmid\nhigh\n0.012\n0.010\n0.008\n0.006\n0.004\nmean\n0.09\n0.10\n0.11\n0.12\n0.13\nstd\nlow\nmid\nhigh\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFig. 3. CNN weight analysis for optimal and suboptimal networks characterization.\nNode strength analysis. We investigate the node strengths between lay-\ners in Figs. 8, 9, and 10 to assess whether the successful networks show sim-\nilar performance as weight strength analysis. We observe clear correlations in\nhigh-accuracy networks, indicating robust inter-layer communication and effi-\ncient signal propagation. This is especially evident in DNN and CNN networks,\nwhere the node strength of layers correlates strongly. In ViT, the strength values\ncluster and correlate with the high-accuracy networks. However, contrary to the\nDNN and CNN networks, the strength values for the high-accuracy network are\nlow for high-accuracy networks, inciting the lower concentrated values in the\nViT attention layer, and the MLP layer in ViT is better for high-performing.\nWeight projection analysis. We investigate the weight projection in Figs. 11,\n12, and 13, which is equivalent to projecting high-dimensional network weight\nvectors to a two-dimensional space using t-SNE. This projection shows the po-\nsition/proximity of different networks on a high-dimensional space [8]. We as-\nsess whether trained networks cluster together or not. We observe that high-\naccuracy networks have their layers weights clustered distinctly separately com-\npared to the low/mid-accuracy networks. For ViT, due to the low number of\nruns/experiments compared to DNN and CNN, the projection is sparse.\n10\nZ. Zheng et al.\nAttn\nMLP\nNorm\nAll\nMNIST\n0.50\n0.25 0.00\n0.25\n0.50\n0.75\nmean\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nstd\nlow\nmid\nhigh\n0.10\n0.05\n0.00\n0.05\nmean\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nstd\nlow\nmid\nhigh\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\nmean\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nstd\nlow\nmid\nhigh\n0.0\n0.1\n0.2\n0.3\nmean\n0.6\n0.8\n1.0\n1.2\n1.4\nstd\nlow\nmid\nhigh\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFMNIST\n0.0\n0.5\n1.0\nmean\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nstd\nlow\nmid\nhigh\n0.02\n0.00\n0.02\n0.04\nmean\n0.4\n0.5\n0.6\n0.7\nstd\nlow\nmid\nhigh\n1.0\n1.2\n1.4\n1.6\n1.8\nmean\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nstd\nlow\nmid\nhigh\n0.0\n0.1\n0.2\n0.3\nmean\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nstd\nlow\nmid\nhigh\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCIFAR-10\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\nmean\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nstd\nlow\nmid\nhigh\n0.05\n0.00\n0.05\n0.10\nmean\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\nstd\nlow\nmid\nhigh\n1.0\n1.2\n1.4\nmean\n0.4\n0.6\n0.8\n1.0\nstd\nlow\nmid\nhigh\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\nmean\n0.5\n0.6\n0.7\n0.8\nstd\nlow\nmid\nhigh\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\nFig. 4. ViT weight analysis for optimal and suboptimal networks characterization.\nI/P - FC1\nFC1 - FC2\nFC2 -O/P\nWhole Net\nMNIST\n2\n1\n0\n1\n2\nWeights\n0\n5\n10\n15\n20\nNormalized Density\nhigh\nmid\nlow\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nWeights\n0\n2\n4\n6\n8\n10\n12\nhigh\nmid\nlow\n5\n4\n3\n2\n1\n0\n1\n2\nWeights\n0\n1\n2\n3\n4\n5\n6\nhigh\nmid\nlow\n5\n4\n3\n2\n1\n0\n1\n2\nWeights\n0\n2\n4\n6\n8\n10\nhigh\nmid\nlow\nFMNIST\n2\n1\n0\n1\n2\nWeights\n0\n1\n2\n3\n4\n5\n6\n7\nNormalized Density\nhigh\nmid\nlow\n1\n0\n1\n2\n3\nWeights\n0\n2\n4\n6\n8\n10\n12\nhigh\nmid\nlow\n4\n3\n2\n1\n0\n1\nWeights\n0\n1\n2\n3\n4\n5\n6\nhigh\nmid\nlow\n4\n3\n2\n1\n0\n1\n2\n3\nWeights\n0\n2\n4\n6\n8\n10\nhigh\nmid\nlow\nCIFAR-10\n1.0\n0.5\n0.0\n0.5\n1.0\nWeights\n0\n1\n2\n3\n4\nNormalized Density\nhigh\nmid\nlow\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\nWeights\n0\n2\n4\n6\n8\n10\n12\n14\n16\nhigh\nmid\nlow\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\nWeights\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nhigh\nmid\nlow\n1.0\n0.5\n0.0\n0.5\n1.0\nWeights\n0\n1\n2\n3\n4\n5\n6\nhigh\nmid\nlow\nFig. 5. DNN normalized weight distribution for model characterization.\nOn Learnable Parameters of Optimal and Suboptimal Deep Learning Models\n11\nConv1\nFC\nAll\nMNIST\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Density\nhigh\nmid\nlow\n2\n1\n0\n1\nWeights\n0\n1\n2\n3\n4\n5\n6\nhigh\nmid\nlow\n2\n1\n0\n1\nWeights\n0\n1\n2\n3\n4\n5\n6\nhigh\nmid\nlow\nKernel density \nestimation lines\nFMNIST\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nNormalized Density\nhigh\nmid\nlow\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nWeights\n0\n1\n2\n3\n4\n5\n6\nhigh\nmid\nlow\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nWeights\n0\n1\n2\n3\n4\n5\n6\nhigh\nmid\nlow\nHistogram\nCIFAR-10\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nWeights\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNormalized Density\nhigh\nmid\nlow\n1.0\n0.5\n0.0\n0.5\nWeights\n0\n1\n2\n3\n4\nhigh\nmid\nlow\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nWeights\n0\n1\n2\n3\n4\nhigh\nmid\nlow\nFig. 6. CNN normalized weight distribution for model characterization.\nAttn\nMLP\nNorm\nAll\nMNIST\n4\n2\n0\n2\n4\n6\n8\n10\n12\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nNormalized Density\nlow\nmid\nhigh\n2\n1\n0\n1\n2\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlow\nmid\nhigh\n0\n1\n2\n3\n4\n5\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlow\nmid\nhigh\n4\n2\n0\n2\n4\n6\n8\n10\n12\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlow\nmid\nhigh\nFMNIST\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Density\nlow\nmid\nhigh\n2\n1\n0\n1\n2\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlow\nmid\nhigh\n0\n1\n2\n3\n4\n5\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nlow\nmid\nhigh\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlow\nmid\nhigh\nCIFAR-10\n4\n3\n2\n1\n0\n1\n2\n3\n4\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Density\nlow\nmid\nhigh\n2\n1\n0\n1\n2\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlow\nmid\nhigh\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nlow\nmid\nhigh\n4\n3\n2\n1\n0\n1\n2\n3\n4\nWeights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlow\nmid\nhigh\nFig. 7. ViT normalized weight distribution for model characterization.\n12\nZ. Zheng et al.\nFC1 vs FC2\nFC1 vs O/P\nFC2 vs O/P\nMNIST\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\ns1\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\ns2\nnon\nhigh\nlow\nmid\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\ns1\n0\n25\n50\n75\n100\n125\n150\n175\ns3\nnon\nhigh\nlow\nmid\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\ns2\n0\n25\n50\n75\n100\n125\n150\n175\ns3\nnon\nhigh\nlow\nmid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFMNIST\n0\n10000\n20000\n30000\n40000\ns1\n0\n10000\n20000\n30000\n40000\ns2\nnon\nhigh\nlow\nmid\n0\n10000\n20000\n30000\n40000\ns1\n0\n200\n400\n600\n800\ns3\nnon\nhigh\nlow\nmid\n0\n10000\n20000\n30000\n40000\ns2\n0\n200\n400\n600\n800\ns3\nnon\nhigh\nlow\nmid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCIFAR-10\n0\n50000\n100000\n150000\n200000\ns1\n0\n5000\n10000\n15000\n20000\n25000\n30000\ns2\nnon\nhigh\nlow\nmid\n0\n50000\n100000\n150000\n200000\ns1\n0\n50\n100\n150\n200\ns3\nnon\nhigh\nlow\nmid\n0\n5000\n10000\n15000\n20000\n25000\n30000\ns2\n0\n50\n100\n150\n200\ns3\nnon\nhigh\nlow\nmid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFig. 8. DNN node strength based characterization of optimal and suboptimal networks.\nConv1 vs FC\nConv1+ vs FC+\nConv1−vs FC−\nMNIST\n10\n20\n30\n40\n50\n60\nconv1\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nfc\nmin accuracy\nmax accuracy\nmid accuracy\n0\n5\n10\n15\n20\n25\n30\n35\npositive conv1\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\npostive fc\nmin accuracy\nmax accuracy\nmid accuracy\n5\n10\n15\n20\n25\n30\nnegative conv1\n0\n2000\n4000\n6000\n8000\n10000\n12000\nnegative fc\nmin accuracy\nmax accuracy\nmid accuracy\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\nFMNIST\n0\n10\n20\n30\n40\n50\n60\nconv1\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\nfc\nlow\nhigh\nmid\n0\n2500\n5000\n7500\n10000 12500 15000 17500\npositive conv1\n0\n5\n10\n15\n20\n25\n30\n35\npostive fc\nlow\nhigh\nmid\n0\n10\n20\n30\n40\n50\n60\nnegative conv1\n0\n5\n10\n15\n20\n25\n30\n35\nnegative fc\nlow\nhigh\nmid\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\nCIFAR-10\n0\n20\n40\n60\n80\n100\n120\n140\nconv1\n0\n5000\n10000\n15000\n20000\n25000\n30000\nfc\nlow\nhigh\nmid\n0\n5000\n10000\n15000\n20000\n25000\n30000\npositive conv1\n0\n10\n20\n30\n40\n50\n60\n70\npostive fc\nlow\nhigh\nmid\n0\n20\n40\n60\n80\n100\n120\n140\nnegative conv1\n0\n10\n20\n30\n40\n50\n60\n70\nnegative fc\nlow\nhigh\nmid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFig. 9. CNN node strength based characterization of optimal and suboptimal networks.\nOn Learnable Parameters of Optimal and Suboptimal Deep Learning Models\n13\nAttn vs MLP\nMLP vs Norm\nAttn vs Norm\nMNIST\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nattention layer\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nmlp layer\nlow\nhigh\nmid\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nmlp layer\n0.1\n0.2\n0.3\n0.4\nnorm layer\nlow\nhigh\nmid\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nattention layer\n0.1\n0.2\n0.3\n0.4\nnorm layer\nlow\nhigh\nmid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFMNIST\n0.0\n0.1\n0.2\n0.3\n0.4\nattention layer\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nmlp layer\nlow\nhigh\nmid\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nmlp layer\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nnorm layer\nlow\nhigh\nmid\n0.0\n0.1\n0.2\n0.3\n0.4\nattention layer\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nnorm layer\nlow\nhigh\nmid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nCIFAR-10\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nattention layer\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nmlp layer\nlow\nhigh\nmid\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nmlp layer\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nnorm layer\nlow\nhigh\nmid\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nattention layer\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nnorm layer\nlow\nhigh\nmid\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\nFig. 10. ViT node strength based characterization of optimal and suboptimal networks.\nI/P - FC1\nFC1 - FC2\nFC2 - O/P\nMNIST\n50\n40\n30\n20\n10\n0\n10\n20\n30\n20\n10\n0\n10\n20\n60\n40\n20\n0\n20\n20\n10\n0\n10\n20\n50\n40\n30\n20\n10\n0\n10\n20\n10\n0\n10\n20\n30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n \nFMNIST\n20\n10\n0\n10\n20\n20\n15\n10\n5\n0\n5\n25\n20\n15\n10\n5\n0\n5\n10\n15\n10\n5\n0\n5\n10\n15\n15\n10\n5\n0\n5\n10\n15\n10\n5\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n \nCIFAR-10\n15\n10\n5\n0\n5\n10\n15\n20\n15\n10\n5\n0\n5\n10\n15\n20\n25\n20\n15\n10\n5\n0\n5\n10\n15\n25\n20\n15\n10\n5\n0\n5\n10\n15\n10\n5\n0\n5\n10\n15\n30\n20\n10\n0\n10\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n \nFig. 11. DNN network weight projection. Optimal and suboptimal learnable parameter\nclusters are distinctly shown in accuracy colors. Less complex data has a smoother\ntransition, and higher complex data has a sharp separation between clusters.\n14\nZ. Zheng et al.\nConv1\nFC\nMNIST\n10\n5\n0\n5\n10\n15\n4\n2\n0\n2\n4\n6\n8\n16\n14\n12\n10\n8\n6\n4\n2\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\n \nSimilar networks \nin high dimensional space\nCloser Points\nFMNIST\n15\n10\n5\n0\n5\n10\n15\n10\n5\n0\n5\n10\n5\n0\n5\n10\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n \nDistinct Networks\nin high dimensional space\nDistant points\nCIFAR-10\n15\n10\n5\n0\n5\n10\n15\n4\n2\n0\n2\n4\n10\n5\n0\n5\n10\n15\n10\n5\n0\n5\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n \nAccuracy\nlow\nhigh\nFig. 12. CNN network weight projection.\nAttn\nMLP\nNorm\nMNIST\n75\n50\n25\n0\n25\n50\n75\n100\n80\n60\n40\n20\n0\n20\n40\n60\n2\n3\n4\n5\n6\n7\n10\n9\n8\n7\n6\n5\n4\n60\n40\n20\n0\n20\n40\n60\n30\n20\n10\n0\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n \nFMNIST\n60\n40\n20\n0\n20\n40\n80\n60\n40\n20\n0\n20\n40\n40\n20\n0\n20\n40\n60\n100\n50\n0\n50\n100\n150\n100\n50\n0\n50\n100\n150\n150\n100\n50\n0\n50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n \nCIFAR-10\n80\n60\n40\n20\n0\n20\n40\n60\n60\n40\n20\n0\n20\n40\n60\n80\n2\n4\n6\n8\n10\n1\n0\n1\n2\n3\n40\n30\n20\n10\n0\n10\n20\n60\n40\n20\n0\n20\n40\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\n \nFig. 13. ViT network weight projection.\nOn Learnable Parameters of Optimal and Suboptimal Deep Learning Models\n15\n5\nConclusion\nWe present a methodology to characterize deep learning uncertainty of suc-\ncess and failure by comprehensively analyzing learnable parameters (weights)\nstrength, node strength, and weight projection of networks on three models:\ndeep neural networks, convolutional neural networks, and vision transformers\nover three datasets: MNIST, Fashion MNIST, and CIFAR-10. Our finding re-\nveals that successful networks have low variance in their weight, and their weight\nconverges to a similar weight distribution, clustering close to each other in high-\ndimensional space. On the other hand, failed networks show contrary behavior\nto successful networks, i.e., they have large variances in their weights and appear\nto be further away from the successful cluster. The node strength of a successful\nnetwork has a tendency to increase strength values for DNNs and CNNs.\nReferences\n1. Dosovitskiy, A., et al.: An image is worth 16x16 words: Transformers for image\nrecognition at scale. ICLR (2021)\n2. Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable\nneural networks. ICLR (2019)\n3. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti\nvision benchmark suite. In: CVPR. pp. 3354–3361 (2012)\n4. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n5. Van der Maaten, L., Hinton, G.: Visualizing data using t-SNE. Journal of Machine\nLearning Research 9(11) (2008)\n6. Naitzat, G., Zhitnikov, A., Lim, L.H.: Topology of deep neural networks. Journal\nof Machine Learning Research 21(1), 7503–7542 (2020)\n7. Neyshabur, B., Li, Z., Bhojanapalli, S., et al.: Towards understanding the role of\nover-parametrization in generalization of neural networks. ICLR (2019)\n8. Rauber, P.E., Fadel, S.G., Falcao, A.X., Telea, A.C.: Visualizing the hidden activity\nof artificial neural networks. IEEE Transactions on Visualization and Computer\nGraphics 23(1), 101–110 (2016)\n9. Scabini, L.F., Bruno, O.M.: Structure and performance of fully connected neural\nnetworks: Emerging complex network properties. Physica A: Statistical Mechanics\nand its Applications 615, 128585 (2023)\n10. Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face\nrecognition and clustering. In: CVPR. pp. 815–823 (2015)\n11. Semenova, N., Larger, L., Brunner, D.: Understanding and mitigating noise in\ntrained deep neural networks. Neural Networks 146, 151–160 (2022)\n12. Shen, D., Wu, G., Suk, H.I.: Deep learning in medical image analysis. Annual\nReview of Biomedical Engineering 19, 221–248 (2017)\n13. Simard, P.Y., et al.: Best practices for convolutional neural networks applied to\nvisual document analysis. In: ICDAR. vol. 3 (2003)\n14. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,\nFergus, R.: Intriguing properties of neural networks. ICLR (2014)\n15. Vaswani, A., et al.: Attention is all you need. NIPS 30 (2017)\n16. Voita, E., Talbot, D., Moiseev, F., Sennrich, R., Titov, I.: Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the rest can be pruned. In:\nACL (2019)\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2024-08-21",
  "updated": "2024-08-21"
}