{
  "id": "http://arxiv.org/abs/2408.12308v3",
  "title": "Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on Supervised Regression (Preprint)",
  "authors": [
    "Yansel Gonzalez Tejeda",
    "Helmut A. Mayer"
  ],
  "abstract": "In this tutorial, we present a compact and holistic discussion of Deep\nLearning with a focus on Convolutional Neural Networks (CNNs) and supervised\nregression. While there are numerous books and articles on the individual\ntopics we cover, comprehensive and detailed tutorials that address Deep\nLearning from a foundational yet rigorous and accessible perspective are rare.\nMost resources on CNNs are either too advanced, focusing on cutting-edge\narchitectures, or too narrow, addressing only specific applications like image\nclassification.This tutorial not only summarizes the most relevant concepts but\nalso provides an in-depth exploration of each, offering a complete yet agile\nset of ideas. Moreover, we highlight the powerful synergy between learning\ntheory, statistic, and machine learning, which together underpin the Deep\nLearning and CNN frameworks. We aim for this tutorial to serve as an optimal\nresource for students, professors, and anyone interested in understanding the\nfoundations of Deep Learning. Upon acceptance we will provide an accompanying\nrepository under\n\\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}\n  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine\nLearning.",
  "text": "Deep Learning with CNNs: A Compact\nHolistic Tutorial with Focus on Supervised\nRegression (Preprint)\nYansel Gonzalez Tejeda and Helmut A. Mayer\nDepartment of Artificial Intelligence and Human Interfaces\nParis Lodron University of Salzburg\nNovember 13, 2024\nIn this tutorial, we present a compact and holistic discussion of Deep Learn-\ning with a focus on Convolutional Neural Networks (CNNs) and supervised\nregression.\nWhile there are numerous books and articles on the individ-\nual topics we cover, comprehensive and detailed tutorials that address Deep\nLearning from a foundational yet rigorous and accessible perspective are rare.\nMost resources on CNNs are either too advanced, focusing on cutting-edge\narchitectures, or too narrow, addressing only specific applications like image\nclassification.This tutorial not only summarizes the most relevant concepts\nbut also provides an in-depth exploration of each, offering a complete yet agile\nset of ideas. Moreover, we highlight the powerful synergy between learning\ntheory, statistic, and machine learning, which together underpin the Deep\nLearning and CNN frameworks. We aim for this tutorial to serve as an opti-\nmal resource for students, professors, and anyone interested in understand-\ning the foundations of Deep Learning. Upon acceptance we will provide an\naccompanying repository under https://github.com/neoglez/deep-learning-\ntutorial\nKeywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine Learn-\ning.\n1\narXiv:2408.12308v3  [cs.AI]  12 Nov 2024\n1 Introduction\nIn this tutorial, we discuss the theoretical foundations of Artificial Neural Networks\n(ANN) in its variant with several intermediate layers, namely Deep Artificial Neural\nNetworks. More specifically, we focus on a particular ANN known as Convolutional\nNeural Network (CNN).\nIn order to introduce the methods and artifacts we will use, we mostly follow the\nclassical literature on Artificial Intelligence (AI) from an overall perspective [Rusell\nand Norvig, 2010] and Deep Learning (DL) [Goodfellow et al., 2016]. We begin by\nmotivating DL in Sec.\n2 and enunciating its standard taxonomy.\nSince ANNs are\nlearning systems, we first expose machine learning (ML) concepts in Sec. 3. We then\ndiscuss the general structure of an ANN (Sec. 4) and the concept of depth in that\ncontext. Convolutional Neural Networks are named as such because convolutions are\na key building block of their architecture. For that reason, in Sec. 5, we incorporate\nthe convolution operation to complete our exposition of CNNs. Finally, in Sec. 6, we\nsummarize the more relevant facts.\nThe theoretical deep learning framework is immense and has many ramifications to\nlearning theory, probability and statistics, optimization, and even cognitive theory, to\nmention just a few examples.\nThere are several books for every topic described in\nthe subsections of this tutorial, not to mention articles in conferences and journals.\nTherefore, our presentation of deep learning here is highly selective and aims to expose\nthe fundamental aspects needed to agile assimilate DL.\nIt is worth noting that, since DL is a relative young field, the terminology greatly\nvaries. We often indicate when several terms are used to designate a concept, but the\nreader should be aware that this indication is by far not exhaustive. Notation is at the\nend of this manuscript.\nWe begin by motivating the DL framework in Sec. 2. As its name suggests, DL is a\nsubset of Machine Learning (ML); for that reason, in Sec. 3\nThen, in Sec.\nfocusing the discussion in a pivotal element, namely the dataset.\nWithout a dataset is\n2 Motivation\nIntelligent agents need to learn in order to acquire knowledge from their environment.\nTo perform various tasks, humans learn, among other things, from examples. In this\ncontext, learning means that for a defined task, the agent must be able to generalize.\nThat is, the agent should successfully perform the task beyond the examples it has seen\nbefore, and typically, we expect the agent to make some form of prediction. Most tasks\nthe agent performs can be framed as either classification or regression. For both types\nof tasks, when the examples are accompanied by known target values or labels, which\nthe agent aims to predict, we refer to this as supervised learning, as a learning\nparadigm. From a pedagogical perspective, the agent is being supervised by receiving\nthe ground truth corresponding to each learning example.\n2\nBesides supervised learning, there are three other learning paradigms: unsupervised\nlearning, reinforcement leaning and semi-supervised learning. When no targets\nare available, or not provided, learning must occur in an unsupervised manner.\nIf,\nalternatively, the targets are noisy or a number of them is missing, learning must proceed\nsemi-supervised.\nIn contrast, if no ground truth is provided at all, but a series of\nreinforcements can be imposed, the agent can nonetheless learn by being positively or\nnegatively (punished) rewarded. Here, we focus on supervised learning.\n3 Machine Learning\nFrom a general perspective the learning agent has access to a set of ordered examples\nX def\n= {x(1), · · · , x(m)}\n(1)\nassumed to have been drawn from some unknown generating distribution pdata. A\nlearning example x(i) (also a pattern) may be anything that can be represented in a\ncomputer, for example, a scalar, a text excerpt, an image, or a song. Without loss of\ngenerality, we will assume that every x(i) is a vector x(i) ∈Rn, and its components the\nexample features. For instance, one could represent an image as a vector by arranging\nits pixels to form a sequence. Similarly, a song could be vectorized by forming a sequence\nof its notes. We will clarify when we refer to a different representation.\nFurthermore, the examples are endowed with their ground truth (set of labels, also\nsupervision signal) Y so that for every example x(i), there exists a label (possibly a\nvector) yi ∈Y. Using these examples the agent can learn to perform essentially two\ntypes of tasks: regression and classification (or both). Regression may be defined\nas estimating one or more values y ∈R, for instance, human body dimensions like\nheight or waist circumference. In a classification scenario, the agent is required to assign\none or more of a finite set of categories to every input, for example, from an image\nof a person, designate which body parts are arms or legs.\nAdditionally, in single-task learning the agent must learn to predict a single quantity\n(binary or multi-class classification, univariate regression), while in multi-task learn-\ning the algorithm must learn to estimate several quantities, e.g., multivariate multiple\nregression.\n3.1 Learning Theory\nIn general, we consider a hypothesis space H, consisting of functions. For example, one\ncould consider the space of linear functions. Within H, the function\nf(X) = Y\n(2)\nmaps the examples (inputs) to their labels (outputs). The learning agent can be then\nexpressed as a model\nˆf(X) = ˆY\n(3)\n3\nthat approximates f, where ˆY is the model estimate of the targets Y.\nNow, the ultimate goal of the learning agent is to perform well beyond the exam-\nples it has seen before, i.e., to exhibit good generalization. More concretely, learning\nmust have low generalization error. To assess generalization, the stationarity as-\nsumption must be adopted to connect the observed examples to the hypothetical future\nexamples that the agent will perceive. This assumption postulates that the examples are\nsampled from a probability distribution pdata that remains stationary over time. Addi-\ntionally, it is assumed that a) each example x(i) is independent of the previous examples\nand b) that all examples have identical prior probability distribution, abbreviated i.i.d..\nWith this in hand, we can confidently consider the observed examples as a train-\ning set {xtrain, ytrain} (also training data) and the future examples as a test set\n{xtest, ytest}, where generalization will be assessed. As the data generating distribution\npdata is assumed to be unknown (we will delve into this in subsection 3.3), the agent has\nonly access to the training set. One intuitive strategy for the learning agent to estimate\ngeneralization error is to minimize the training error computed on the train set, also\ntermed empirical error or empirical risk. Thus, this strategy is called Empirical\nRisk Minimization (ERM). Similarly, the test error is calculated on the test set.\nBoth training and test errors are defined in a general form as the sum of incorrect esti-\nmated targets by the model (in Subsec. 3.4.1 we restate this definition), for example, in\nthe case of m training examples, the training error Etrain is\nEtrain =\nm\nX\ni=1\n\u0000f∗(x(i)) ̸= yi\n\u0001\n.\n(4)\nNote that given a specific model f∗, and based on the i.i.d. assumption the expected\ntraining error equals the expected test error. In practice, the previous is greater than or\nequal to the latter. Here, the model may exhibit two important flaws: underfitting or\noverfitting. A model that underfits does not achieve a small training error (it is said\nthat it can not capture the underlying structure of the data). Overfitting is the contrary\nof underfitting, it occurs when the model “memorizes” the training data, “perhaps like\nthe everyday experience that a person who provides a perfect detailed explanation for\neach of his single actions may raise suspicion” [Shalev-Shwartz and Ben-David, 2014].\nWhen evaluated, a model that underfits yields a high difference between the training\nand the test error.\n3.2 Model Evaluation\nAssessing generalization by randomly splitting the available data in a training set and a\ntest set is called holdout cross-validation because the test set is kept strictly separate\nfrom the training set and used only once to report the algorithm results. However, this\nmodel evaluation technique has two important disadvantages:\n• It does not use all the data at hand for training, a problem that is specially relevant\nfor small datasets.\n4\n• In practice, there can be the case where the i.i.d.\nassumption does not hold.\nTherefore, the assessment is highly sensitive to the training/test split.\nA remedy to these problems is the k-fold cross-validation evaluation method (cf.\n[Hastie et al., 2009] p.\n241 - Cross Validation).\nIt splits the available data into k\nnonoverlapping subsets of equal size. Recall the dataset has m examples. First, learning\nis performed k times with k −1 subsets. Second, in every iteration, the test error is\ncalculated on the subset that was not used for training. Finally, the model performance\nis reported as the average of the k test errors. The cost of using this method is the\ncomputational overload, since training and testing errors must be computed k times.\n3.3 Statistics and Probability Theory\nTo further describe machine learning we borrow statistical concepts. In order to be con-\nsistent with the established literature[Goodfellow et al., 2016], we use in this subsection\nθ and ˆθ to to denote a quantity and its estimator.\nThe agent must learn to estimate the wanted quantity θ, say.\nFrom a statistical\nperspective the agent performs a point estimate ˆθ. In this view the data points are the\ni.i.d. learning examples {x(1), · · · , x(m)}. A point estimator is a function of the data\nsuch as ˆθm\ndef\n= g({x(1), · · · , x(m)}), with g(x) being a very general suitable function.\nNote that we can connect the quantity θ and its estimator ˆθ to the functions f and ˆf in\nEquations 2 and 3. Given the hypothesis space H that embodies possible input-output\nrelations, the function ˆf that approximates f can be treated as a point estimator in\nfunction space.\nImportant properties of estimators are bias, variance and standard error. The estima-\ntor bias is a measure of how much the real and the estimated value differ. It is defined as\nbias(ˆθm) = E(ˆθm)−θ. Here the expectation E is taken over the data points (examples).\nAn unbiased estimator has bias(ˆθm) = 0, this implies that ˆθm = θ. An asymptotically\nunbiased estimator is when limm→∞bias(ˆθm) = 0, which implies that limm→∞ˆθm = θ.\nAn example of unbiased estimator is the mean estimator (mean of the training examples)\nof the Bernoulli distribution with mean θ.\nThe variance V ar(ˆθ) = and Standard Error SE(ˆθ) of the estimator are useful for\ncomparing different experiments. A good estimator has low bias and low variance.\nLet us now discuss two important estimators: the Maximum Likelihood and the Max-\nimum a Posteriori estimators.\n3.3.1 Maximum Likelihood Estimation\nTo guide the search for a good estimator ˆθ, the frequentist approach is usually\nadopted. The wanted quantity θ, possibly multidimensional, is seen as fixed but un-\nknown, and the observed data is random. The parameters θ govern the data generating\ndistribution pdata(x) from which the observed i.i.d. data {x(1), · · · , x(m)} arose.\nThen, a parametric model for the observed data is presumed, i.e., a probability dis-\ntribution pmodel(x; θ). For example, if the observed data is presumed to have a nor-\nmal distribution, then the parameters are θ = {µ, σ}, and the model pmodel(x; θ) =\n5\n1\nσ\n√\n2πe−1\n2( x−µ\nσ )\n2\n. Here candidates parameters θ must be considered, and ideally, pmodel(x; θ) ≈\npdata(x). Next, a likelihood function L can be defined as the mapping between the\ndata x and a given θ, to a real number estimating the true probability pdata(x). Since\nthe observations are assumed to be independent,\nL(x; θ) =\nm\nY\ni=1\npmodel(x(i), θ).\n(5)\nThe Maximum Likelihood (ML) estimator is the estimator ˆθ that, among all\ncandidates, chooses the parameters that make the observed data most probable.\nˆθML = arg max\nθ\nL(x; θ)\n(6)\n= arg max\nθ\nm\nY\ni=1\npmodel(x(i), θ)\n(7)\nTaking the logarithm of the right side in 7 facilitates the probabilities computation\nand does not change the maximum location. This leads to the Log-Likelihood\nˆθML = arg max\nθ\nm\nX\ni=1\nlog pmodel(x(i), θ)\n(8)\nAn insightful connection to information theory can be established by transforming\nEq. 8. Firstly, dividing the right term by the constant m does not shift the maximum\nin the left term, i.e.,\nˆθML = arg max\nθ\n1\nm\nm\nX\ni=1\nlog pmodel(x(i), θ).\n(9)\nSecondly, by definition, the empirical mean 1\nm\nPm\ni=1 log pmodel(x(i), θ) of the assumed\nmodel distribution pmodel equals the expectation given its empirical distribution\nEX∼ˆpdata defined by the training set ˆpdata(x). A discussion of the empirical distribution\nˆpdata(x) is out of the scope, but it suffices to say that it is a method to approximate\nthe real underlying distribution of the training set (not to be confused with the data-\ngenerating distribution pdata). Finally, Eq. 9 can be written as\nˆθML = arg max\nθ\nEX∼ˆpdata log pmodel(x, θ).\n(10)\nInformation theory allows to evaluate the degree of dissimilarity between the empirical\ndistribution of the training data ˆpdata(x) and the assumed model distribution pmodel(x)\nusing the Kullback–Leibler divergence DKL (also called relative entropy) as\n6\nDKL(ˆpdata∥pmodel) = Ex∽ˆpdata[log ˆpdata(x) −log pmodel(x)].\n(11)\nHere the expectation E is taken over the training set because Eq.11 quantifies the ad-\nditional uncertainty arising from using the assumed model pmodel to predict the training\nset ˆpdata.\nBecause we want the divergence between these two distribution to be as small as\npossible, it makes sense to minimize DKL. Note that the term log ˆpdata(x) in Eq. 11\ncan not be influenced by optimization, since it has been completely determined by the\ndata generating distribution pdata. Denote the minimum DKL as θDKL, then\nθDKL = arg min\nθ\n−Ex∽ˆpdata[log pmodel(x, θ)]\n(12)\nThe expression inside the arg min is the cross-entropy of ˆpdata and pmodel. Compar-\ning this with Eq. 10 it can be arrived to the conclusion that maximizing the likelihood\ncoincides exactly with minimizing the cross-entropy between the distributions.\n3.3.2 Maximum a Posteriori Estimation\nIn contrast to the frequestist approach, the Bayesian approach considers θ to be a\nrandom variable and the dataset x to be fixed and directly observed. A prior proba-\nbility distribution p(θ) must be defined to express the degree of uncertainty of the state\nof knowledge before observing the data. In the absence of more information, a Gaussian\ndistribution is commonly used. The Gaussian distribution is “the least surprising and\nleast informative assumption to make” [McElreath, 2020], i.e., it is the distribution with\nmaximum entropy of all. Therefore, the Gaussian is an appropriate starting prior when\nconducting Bayesian inference.\nAfter observing the data, Bayes’ rule can be applied to find the posterior distribu-\ntion of the parameters given the data p(θ|x) = p(x|θ)p(θ)\np(x)\n, where p(x|θ) is the likelihood\nand p(x) the probability of the data.\nThen, the Maximum a Posteriori (MAP) estimator selects the point of maximal\nposterior probability:\nˆθMAP = arg max\nθ\np(θ|x) = arg max\nθ\nlog p(x|θ) + log p(θ).\n(13)\n3.4 Optimization and Regularization\n3.4.1 Loss Function\nThe training error as defined by Eq.\n4 is an objective function (also criterion)\nthat needs to be minimized. Utility theory regards this objective as a loss function\nJ(x) : Rn →R because the agent incurs in a lost of expected utility when estimating the\nwrong output with respect to the expected utility when estimating the correct value. To\nestablish the difference of these values, a determined distance function must be chosen.\n7\nFor general (multivariate, multi-target) regression tasks the p-norm is preferred. The\np-norm is calculated on the vector of the corresponding train or test error ∥ˆyi −yi∥p =\n(P\ni |ˆyi −yi|p)\n1\np , which for p = 2 reduces to the Euclidean norm.\nSince the Mean\nSquared Error (MSE) ∥ˆyi −yi∥2\n2 can be faster computed, it is often used for opti-\nmization. We used this loss function (also cost function) when training our network\n(2) with m examples:\nJ(y) = 1\nm\nm\nX\ni=1\n(ˆyi −yi)2\n(14)\n= 1\nm\nm\nX\ni=1\n1\n2\n|y|\nX\nj=i\n(ˆyj −yj)2 .\n(15)\nSince we regard every target yi as a vector, the inner sum runs over its components\nyi and |y| is its length. The rather arbitrary scaling factor 2 in the denominator is a\nmathematical convenience for the discussion in 4.2.\nFor classification tasks the cross-entropy loss is commonly adopted.\n3.4.2 Gradient-Based Learning\nAs we will see in Sec. 4, in the context of Deep Learning, the parameters θ discussed\nin Subsec. 3.3 are named weights w. Without loss of generalization, we will consider\nthe parameters to be a collection of weights. We do not lose generalization because the\ndiscussion here does not depend on the specific form of the cost function.\nTherefore, parameterizing the estimator function by the weights, minimization of the\ncost function can be written as\nJ∗(w; x, y) = arg min\nw\n1\nm\nm\nX\ni=1\n\u0010\nˆf(x(i), w) −yi\n\u00112\n(16)\nThe function ˆf(x(i), w) is, in a broader sense, a nonlinear model, thus the minimum of\nthe loss function has no closed form and numeric optimization must be employed. Specif-\nically, J is iteratively decreased in the orientation of the negative gradient, until a local\nor global minimum is reached, or the optimization converges. This optimization method\n(also optimizer) is termed gradient descent. Because optimizing in the context of\nmachine learning may be interpreted as learning, a learning rate ϵ is introduced to\ninfluence the step size at each iteration. The gradient descent optimizer changes weights\nusing the gradient ∇w of J(w) according to the weight update rule\nw ←w −ϵ∇wJ(w)\n(17)\nWe postpone the discussion on Stochastic Gradients Descent (SGD) to insert it in the\ncontext of training neural networks (Subsec. 4.2).\n8\n3.4.3 Regularization\nThe essential problem in machine learning is to design algorithms that perform well\nboth in training and test data. However, because the true data-generating process is\nunknown, merely decreasing the training error does not guarantee a small generalization\nerror. Even worst, extremely small training errors may be a symptom of overfitting.\nRegularization intends to improve generalization by combating overfitting. It fo-\ncuses on reducing test error using constraints and penalties on the model family being\ntrained, which can be interpreted as pursuing a more regular function.\nOne such an important penalty may be incorporated into the objective function. More\nspecifically, the models parameters may be restricted by a function Ω(w) to obtain the\nmodified cost function ˜J(w) with two terms as\n˜J(w) = arg min\nw\n1\nm\nm\nX\ni=1\n\u0010\nˆf(x(i), w) −yi\n\u00112\n+ αΩ(w).\n(18)\nThe second term’s coefficient α ∈[0, ∞) ponders the penalty’s influence relative to J∗.\nThe specific form of Ω(w) (also regularizer) is usually a L1 = ∥w∥1 or L2 = ∥w∥2\n2 norm\npenalty. Choosing one of these norm penalties expresses a preference for a determined\nfunction class. For example, the L1 penalty enforces sparsity on the weights, i.e., pushes\nthe weights towards zero, while the effect of L2 norm causes the weights of less influential\nfeatures to decay away. Because of this the L2 penalty norm is called weight decay.\nWeight decay has lost popularity in favor of other regularization techniques (Subsec.\n5.5). It worth saying, however, that weight decay is an uncomplicated effective method\nfor enhancing generalization [Krogh and Hertz, 1991]. Modifying Eq. 16 to include the\npenalty with α = 1\n2, the cost function becomes\n˜J(w) = arg min\nw\n1\nm\nm\nX\ni=1\n\u0010\nˆf(x(i), w) −yi\n\u00112\n+ α\n2 wT w.\n(19)\nCorrespondingly, after substituting and reorganizing terms, the weight update rule in\nEq. 17 results in\nw ←(1 −ϵα)w −ϵ∇wJ(w).\n(20)\n4 Deep Forward Artificial Neural Networks\nDeep Forward Artificial Neural Networks (DFANN) is a model conveying several impor-\ntant interlaced concepts. We start with its building blocks and then connect them to\nform the network. Additionally, to describe the parts of an ANN, the term architecture\nis used.\n9\n4.1 Artificial Neural Networks\nArtificial Neural Networks (ANNs) is a computational model originally inspired by\nthe functioning of the human brain [McCulloch and Pitts, 1943, Hebb, 1949, Rosenblatt,\n1957]. The model exploits the structure of the brain, focusing on neural cells (neurons)\nand its interactions. These neurons have three functionally distinct parts: dendrites,\nsoma and axon. The dendrites gather information from other neurons and transmit it\nto the soma. Then, the soma performs a relevant nonlinear operation. If exceeding a\ndetermined threshold, the result causes the neuron to emit an ouput signal. Following,\nthe output signal is delivered by the axon to others neurons.\n4.1.1 Artificial Neurons\nSimilarly, the elementary processing unit of ANNs is the artificial neuron.\nThe\nneurons receive a number of inputs with associated intensity (also strength) and\ncombine these inputs in a nonlinear manner to produce a determined output that is\ntransmitted to other neurons. Depending on the form of the non-linearity, it can be said\nthat the neuron ‘fires’ if a determined threshold is reached.\nEvery input to the neuron has an associated scalar called weight w. The non-linearity,\nalso called activation function is denoted by g(·).\nThe neuron first calculates an\naffine transformation of its inputs z = Pn\ni=1 wi · xi + c. The intercept (also bias) c\nmay be combined with a dummy input xdummy = 1 to simplify notation such as the\naffine transformation can be written in matrix form Pn\ni=0 wi · xi = w · x. One of the\nreason ANNs where invented is to overcome the limitations of linear models. Since a\nlinear combination of linear units is also linear, a non-linearity is needed to increase\nthe capacity of the model. Accordingly, the neuron applies the activation function to\ncompute the activation a = g(w · x), which is transmitted to other neurons.\nReferring to activation functions, [Goodfellow et al., 2016] states categorically “In\nmodern neural networks, the default recommendation is to use the rectified linear\nunit or ReLU” introduced by [Jarrett et al., 2009]:\ng(z) = max(0, z).\n(21)\nIn order to enable learning with gradient descent (Eq. 17) as described in Subsec.\n3.4.2, is desirable the activation function to be continuous differentiable.\nAlthough\nthe ReLU is not differentiable at z = 0, in practice, this does not constitute a problem.\nPractitioners assume a ‘derivative default value’ f′(0) = 0, f′(0) = 1 or values in between.\nOther activation functions are the logistic sigmoid σ(z) = 1/(1 + exp(−z)) and the\nsoftplus function ζ(z) = log(1+exp(z)). As a side note, a neuron must not necessarily\ncompute a non-linearity, and in that case, it is called a linear unit.\nThe perceptron is an artificial neuron introduced by [Rosenblatt, 1957], who demon-\nstrated that only one neuron suffices to implement a binary classifier. The perceptron\nhad originally a hard threshold activation function, but it can be generalized to use any\nof the above-mentioned activation function. For example, if the perceptron uses a logis-\ntic sigmoid activation function, then the output can be interpreted as the probability of\n10\nsome event happening, e.g., logistic regression.\n4.1.2 Deep Feedforward Networks\nConnecting the artificial neurons results in an artificial neural network with at least two\ndistinguishable layers: the input layer and the output layer. The input layer is\ncomposed by the units that directly accept the features (or their preprocessed represen-\ntation) of the learning examples. The output layer contains the units that produce the\nanswer to the learning task.\nA single-layer neural network (sometimes called single-layer perceptron network,\nor simply perceptron network) (SLNN) is the most basic structure that can be assembled\nby directly connecting any number of input units to any number of output units. Note\nthat, by convention, the input layer is not counted. Despite this type of network being\nfar from useless, they are limited in their expressive power. Perceptron networks are un-\nable to represent a decision boundary for non-linearly separable problems(??). A\nproblem is linearly separable if the learning examples can be separated in two nonempty\nsets, such as every set can be assigned exclusively to one of the two half spaces defined\nby a hyperplane in its ambient space. This is important because, in general, practical\nproblems in relevant areas like computer vision and natural language processing are not\nlinearly separable.\nBeyond the SLNN, networks can be extremely complex. In order to make progress,\none important restriction that may be imposed is to model the network as a directed\nacyclic graph (DAG) with no shortcuts. The neurons are the nodes, the connections\nthe directed edges (or links), and the weights the connection intensity. Because a\nDAG does not contain cycles, the information in the ANN is said to flow in one direction.\nFor that reason, this type of ANN that does not contain feedback links, are called\nfeedforward networks or multi-layer perceptron (MLP). If the network does have\nfeedback links then it is called a recurrent neural network (RNN). In this tutorial\nwe focus on feedforward networks.\nIn equation 3 (Sec 3), we enunciated that the goal of a learning agent ˆf(x) is to ap-\nproximate some function f(x), and from a statistical perspective, the agent is a function\nestimator of θ (Subsec. 3.3). Motivated by the idea of compositionality, it can be as-\nsumed that this learning agent ˆf(x) is precisely the composition of a number of different\nfunctions\nˆf(x) = fn(fn−1(· · · f0(x)))\n(22)\nwhere f(0) and f(n) represent the input and output layer, respectively.\nFurther, starting from the input layer and ending at the output layer, every function\nf(l), for l ∈1, · · · , n −1 can be associated with a collection of DAG units that receive\ninputs only from the units in preceding collections. These functions and the units group\nthey define, are called hidden layers and denoted by h(·).\nHidden layers are named as such because of their relation to the training examples.\nThe training examples determine the form and behavior of the input and the output\nlayer. Indeed, the input layer processes the training examples and the output layer must\ncontribute to minimize the training loss. However, the training examples do not rule\n11\nthe intermediary layer computation scheme. Instead, the algorithm must learn how to\nadapt those layers to achieve its goal.\nFinally, the length of the entire function chain f(1), · · · , f(n) is the depth ℓof the\nnetwork (f(0) is the first layer but is not counted for depth), which, combined with\nthe concepts explained before, confers the name to the computational model: deep\nfeedforward networks. Likewise, the width of a layer is the number of units it contains,\nand the network’s width is sometimes defined as the “maximal number of nodes in a\nlayer” [Lu et al., 2017]. Additionally, the input layer is also called the first layer and the\nnext layers are the second, the third and so forth.\nThe architecture of the network designates the network’s depth and width, as well\nas the layers and their connections to each other (further in Subsec. 5.1 we show an\nexample of a network architecture). Depending on how the units are connected, different\nlayer types can be assembled. Note that because shortcuts are not allowed, there can\nnot be intra-layer unit connections. The fully-connected layer h(l) (also dense layer,\nin opposition to a shallow layer) is the most basic, which all of its units connect to all\nunits of the preceding layer h(l−1) though a weight matrix W , i.e.,\nh(l) = gl(W lh(l−1) + bl)\n(23)\nh(l) = gl(W lh(l−1)).\n(24)\nThe weight matrix W l ∈Rm×n and the bias vector bl ∈Rm, where m and n are the\nwidth of the current and preceding layer, respectively. Alternatively, the notation may\nbe simplified by incorporating the bias vector into the matrix with a dummy input (Eq.\n24).\nIn Sec. 4.2.3 we will further discuss the notation and in Subsec. 5.1 will see other\nlayer types.\nOne of the most significant results in feedforward networks research are the universal\napproximation theorems. The exact form of these theorems is positively technical,\nbut at their core they demonstrate that feedforward networks can approximate any\ncontinuous function to any desirable level of accuracy. There are basically three theorem\nvariants: arbitrary width and bounded depth, i.e., a universal approximator with only\none hidden layer containing a sufficient number of neurons [Cybenko, 1989], bounded\nwidth and arbitrary depth [Gripenberg, 2003], i.e., a sufficient number of layers with a\nbounded amount of neurons, and remarkably, bounded depth and width [Maiorov and\nPinkus, 1999].\n4.2 Training\nHaving described the structure of an artificial neural network, and how they learn by\nmeans of optimization, we now need to perform the actual training. The learning exam-\nples are entered into the network through the input layer and the network weights need\nto be updated using gradient descent to minimize the loss.\n12\n4.2.1 Stochastic Gradient Descent\nAs we already discussed in Subsec. 3.4.2, gradient descent enables optimization of gen-\neral loss functions. Specifically, the model weights are updated using the gradient of\nthe loss function ∇wJ(w), which is sometimes called pure, deterministic or batch\ngradient descent. The problem is that the computation of this gradient requires an\nerror summation over the entire dataset ∇wJ(w) = Pm\ni=1(ˆyi −yi)xi. In practice, this\ncomputation can be extremely expensive or directly intractable.\nStochastic gradient descent (SGD) proposes a radical solution to this problem. It does\nnot compute the gradient exactly. Instead, it estimates the gradient with one training\nexample x(i). For this reason, SGD can be seen as an online algorithm that processes\na stream of steadily produced data points, one example at a time.\nAs expected, this gradient estimation is highly noisy, an issue that can be alleviated\nby using not one but a randomly sampled subset of learning examples. This subset is\ncalled a mini-batch, and its size n is the batch size, which should not be confused\nwith the above-mentioned deterministic optimizer.\nMoreover, SGD has an iterative nature, and when training an ANN, one iteration is\ncalled epoch. In every epoch, the training data is divided into a number of mini-batches.\nThe mini-batches are disjoint subsets of the training set, and the last mini-batch may\nhave fewer examples as n.\nAfter the first mini-batch has been used to estimate the\ngradient, training continues with the next mini-batch, and so forth, until the training\ndata has been completely processed. In this tutorial, our learning algorithm usually ends\nafter a predefined number of epochs χ.\nPerhaps surprisingly, the strategy of SGD not only accelerates training but improve\ngeneralization as well, as concisely stated by [Hardt et al., 2016], “train faster, generalize\nbetter”. Although SGD was first introduced by [Robbins, 1951], it is widely used up to\nnow. For an amenable discussion in SGD recent trends [Newton et al., 2018] may be\nconsulted.\n4.2.2 SGD with Momentum\nDespite the computational and efficacy advantages of SGD, its trajectory to the op-\ntimum or an appropriate stationary point can be slow. To overcome this limitation,\nthe momentum algorithm changes the weights such as it proceeds incorporating the\ncontribution of preceding gradients. This accelerates learning because gradients strong\nfluctuations can be diminished, and therefore, the path to the minimum is less oscillating.\nOne can gain intuition into the momentum algorithm by drawing a physical analogy.\nLet the learning algorithm be a heavy ball rolling down the optimization landscape .\nThe ball is subject to two forces, the momentum force acting in the negative direction of\nthe gradient −∇wJ(w), and a viscose force attenuating its movement m · v. Therefore,\nthe weights should be updated according to the net force acting on the ball.\nIn particular, since momentum is mass times velocity, and considering the ball to have\nunit mass, the velocity can be interpreted as momentum. Then, introducing a term v\nand a parameter α ∈[0, 1) the weight update can be written\n13\nv ←αv −ϵ∇wJ(w)\n(25)\nw ←w + v\n(26)\nDespite the parameter α being more related to viscosity, it is widely called momen-\ntum parameter. Furthermore, the velocity update at iteration k can be written as\nvk = αkv0 −ϵ Pk−1\ni=0 αi∇Jw(wk−1−i). Given that α < 1, the velocity accumulates pre-\nvious gradients and updates the weights using an exponential moving average, where\nrecent gradients have more importance (lower powers of α) than previous gradients.\nSGD with momentum may be generalized to a family of optimization methods called\n‘Linear First Order Methods’ [Goh, 2017], but the presentation here shall be enough for\nour discussion.\n4.2.3 Forward Propagation and Back-propagation\nIn this subsection, we departure from our two main references and follow [Bishop, 2007]\nand [Nielsen, 2015].\nWe are now ready to begin training the network. This requires a proper definition of\nan individual weight in a certain layer l, i.e., the weight wl\njk connects the kth neuron in\nthe (l −1)th layer to the jth neuron in the current layer l. This rather intricate notation\nis necessarily in order to compactly relate the activation of consecutive layers al and\nal−1:\nal\nj = g\n X\nk\nwl\njkal−1\nk\n+ bl\nj\n!\n(27)\nal = g(W lal−1 + bl).\n(28)\nHere, if we fixate the neuron j with weight wl\njk and bias bl, at layer l, then its activation\nal\nj results from the activation in the preceding layer according to Eq. 27. Intuitively,\nthe activation of a neuron is a function of its weighted inputs, i.e., the activation of all\nthe units connected to it from the previous layer, and the bias of the neuron (Fig. 1).\nEq. 28 is its vectorized form, where g(·) is the activation function applied element-wise.\nFurthermore, the weight sum before applying the non-linearity is called the weighted\ninput zl\nj to the neuron j in layer l; it is defined as\nzl\nj\ndef\n=\nX\nk\nwl\njkal−1\nk\n+ bl\nj\n(29)\nz(l) def\n= W (l)a(l−1) + bl\n(30)\na(l) = g(z(l)).\n(31)\nEquation 30 is the weighted input in vectorized form and Eq. 31 is the activation\nwritten as a function of the weighted input.\n14\nFigure 1: ANN weights notation illustrated. The figure depicts two fully connected hid-\nden layers of an arbitrary ANN. The layer l (right) has two neurons (green\ncircles) and the preceding layer l −1 (left) has three units. The neuron j is\nthe second unit (from top to bottom) in layer l; the third unit k in layer l −1\nhas activation al−1\nk\n. Among all connections (mostly in gray), we emphasize\n(in black) the connection from the unit k in layer l −1 to the unit j in layer\nl; the weight of this connection is denoted wl\njk. Supposing the right layer is\nthe second hidden layer, then the depicted weight is concretely w2\n23. To calcu-\nlate the activation al\nj of this neuron j, the weights of all incoming connections\nfrom all neurons {wl\nj1, wl\nj2, · · · , wl\njk} in layer l −1 must be multiplied by the\ncorresponding activation {al−1\n1\n, al−1\n2\n, · · · , al−1\nk\n}. The bias bl\nj must be added\nto the resulting summation and the activation function g(·) must be applied\nelementwise. To avoid having to transpose the weight matrix representing all\nconnections at one layer ((W l)T ), the index order in the weights are set to be\njk.\nTraining requires initializing the weights and the bias. More importantly, an\nappropriate weights initialization is crucial for convergence. The general idea is to break\nthe symmetry among the units such as every node computes a different function. Al-\nthough there are several initialization strategies, usually the weights are drawn randomly\nfrom a Gaussian or uniform distribution.\nHowever, due to the chain nature of ANN, two problems might arise: vanishing\ngradients or exploiting gradients. If the weights are initialized with extremely small\nvalues, e.g., from a Gaussian with zero mean and std. 0.01, the hidden layer activations\nmay tend to zero. Therefore, the gradient of the cost function may vanish. Exactly the\ncontrary might happen if the weights are initialized with relative high values (it suffices\nwith an increase of the before mentioned std. to 0.05), which might lead to exceedingly\nlarge gradients. Both vanishing and exploiting gradients hinder learning. In general, the\nproblem is that large weight variance can render learning unstable.\nKaiming initialization [He et al., 2015] (also He initialization) attempts to avoid\nlearning instability by constraining the weights variance to be nearly constant across all\nlayers. This strategy was first proposed by [Glorot and Bengio, 2010] but the derivation\ndid not contemplate ReLU. Concretely, the heuristic is to derive the relation by enforcing\n15\nthat contiguous layers have equal output variance V ar(a(l)) = V ar(a(l−1)). This set\nall bias to zero and the weights at layer l to randomly drawn values from a Gaussian\ndistribution with mean zero and std. inversely proportional to the width ϖ(l−1) (number\nof units in layer l −1, also fan-in) of the preceding layer, i.e.,\nbl = 0\n(32)\nwl\njk ∼N\n\u0012\n0,\n2\nϖ(l−1)\n\u0013\n.\n(33)\nKaiming is the default initialization implemented by Pytorch [Paszke et al., 2019], the\ncurrent most prominent deep learning framework. We also initialize both weights and\nbias following that strategy.\nAfter the network parameters have been initialized, the forward propagation must\nbe performed. Without loss of generalization we will consider training with one example.\nThe first layer receives the training example x(i) as input f(0) = x(i), a(0) = x(i). The\nactivations of the hidden and the output layer may be calculated using Eq. 27.\nBecause the network has depth ℓ, the activation of the last layer is the network’s\noutput aℓ= fℓ(x(i)), which can be regarded, without loss of generalization, as a vector\nˆyi for some example x(i). This allows to execute completely a forward pass. Before\ncalculating the loss, let us first discuss the weight update.\nUltimately, learning means that weights must be updated with Eqs.25-26.\nBack-\npropagation (backprop) is the method that enables that update. Note that the term\n∇wJ(w) in Eq. 25 is the gradient of the cost function with respect to every weight\nwl\njk in the network. As we will now see, calculating the gradient at the output layer is\nstraightforward. The problem is that calculating the gradient of the cost function J at\nhidden layers is not obvious. The goal of backprop is to calculate the gradient of the\ncost function w.r.t. every weight wl\njk in the network.\nTo achieve that goal, two important assumptions need to be made. First, the cost\nfunction J can be decomposed into separate cost functions for individual training exam-\nples x(i). This is indeed the case for MSE in Eq. 14. This assumption is needed because\nit reduces the gradient problem to determine the gradient for one training example\n∇wJi(w), for some general training example x(i). Then the gradient can be totalized by\naveraging over all training examples\n∇wJ(w) = 1\nm\nm\nX\ni=1\n∇wJi(w).\n(34)\nSecond, the cost function must be written as a function of the outputs; otherwise, the\ncomputation of the derivative at the output layer would not be possible [Nielsen, 2015].\nWith those assumptions, the goal of backprop can be restated as calculating the\ngradient of the cost function w.r.t. every weight wl\njk in the network for an individual\ntraining example x(i).\nThe general idea is to determine four equations.\nFirst, calculate the error at the\noutput layer. Second, calculate the total error of an arbitrary hidden layer in terms of\n16\nthe next layer, which allows to back-propagate the error from the output to the input\nlayer. Third, calculate the derivative of the cost function w.r.t the bias. Finally, the first\ntwo equations are combined to calculate the gradient for every weight in the network.\nAll four equations may be derived applying the chain rule of calculus for a variable\nz transitively depending on variable x, via the intermediate variable y as z = f(y) and\ny = g(x). The chain rule of calculus relates the derivative of z with respect to x as in\ndz\ndx = dz\ndy · dy\ndx. Since the cost is a function of all weights, partial derivatives must be\nused ∂z\n∂x = ∂z\n∂y · ∂y\n∂x.\nA quantity that we will need is the error at an individual unit (sometimes called\nneuron error or sensitivity) j in layer l, written as δi\nj. The intuition is that the error\nat the output layer ( quantified by the cost function J) varies if the weighted input\nof one arbitrary node in an arbitrary hidden layer is perturbed by certain amount ∆.\nThis perturbation disseminates through all connected nodes until it reaches the output\nnodes, where the error is calculated. Therefore, the partial derivative of the error with\nrespect to the weighted input of this unit may be interpreted as a measure of the error\nattributable to this neuron. Indeed, ∂J\n∂zl\nj\nexpresses how much the error varies, when the\nweighted input is perturbed by a small amount ∆zl\nj. Therefore, the neuron error may\nbe defined as\nδl\nj\ndef\n= ∂J\n∂zl\nj\n(35)\nNote that at the output layer, J is a function of all activations aℓ\ni. Its rate of change\nwith respect to a weighted input\n∂J\n∂zℓ\nj depends on all contributions that the weighted\ninput influences. Formalizing that intuition and applying the generalized chain rule of\ncalculus, we have\nδℓ\nj =\nX\nk\n∂J\n∂aℓ\nk\n∂aℓ\nk\n∂zℓ\nj\n(36)\nwhere the summation is over all output units n. In the case of MSE, the weighted\ninput zℓ\nj influences only aℓ\nj. Therefore, the other partial derivatives (i ̸= j) vanish and\nthe equality becomes\nδℓ\nj = ∂J\n∂aℓ\nj\n∂aℓ\nj\n∂zℓ\nj\n(37)\n= ∂J\n∂aℓ\nj\n· g′(zℓ\nj)\n(38)\nδℓ= ∇aJ ⊙g′(zℓ),\n(39)\nbeing g′(zℓ\nj) the derivative of the activation function g(·) evaluated at node j. Eq. 39\n17\nis the matrix form, where ∇aJ is the vector of partial derivatives of the cost function\nw.r.t. the activations, and ⊙is the Hadamard product (elementwise multiplication).\nSubsequently, the error relating two contiguous layer l and l + 1 must be defined.\nSimilar to the derivation of the error at the ouput layer, the chain rule must be applied.\nHowever, now the cost function must be considered to be dependent on the weighted\ninput at layer zl\nj throughout intermediate weighted inputs at the next layer zl+1\nj\nfor all\nthe neurons to which the unit j connects. That is\nδl\nj = ∂J\n∂zl\nj\n(40)\nδl\nj =\nX\nk\n∂J\n∂zl+1\nk\n∂zl+1\nk\n∂zl\nj\n(41)\n=\nX\nk\n∂zl+1\nk\n∂zl\nj\nδl+1\nk\n.\n(42)\nSubstituting the first therm on the right in Eq. 41 by its definition δl+1\nk\nand inter-\nchanging with the second term results in Eq. 42.\nTaking as a reference the unit k in the l + 1 layer, the summation runs over all units\nj on the layer l that link to the unit k. Thefore,\nzl+1\nk\n=\nX\nj\nwl+1\nkj al\nj + bl+1\n(43)\nzl+1\nk\n=\nX\nj\nwl+1\nkj g(zl\nj) + bl+1\n(44)\n∂zl+1\nk\n∂zl\nj\n= wl+1\nkj g′(zl\nj),\n(45)\nwhere we substitute in Eq. 43 the activation by its corresponding nonlinearity (Eq.\n44) and further derive to obtain Eq. 45. This equation may be inserted in Eq. 42\nresulting in\nδl\nj =\nX\nk\nwl+1\nkj δl+1\nk\ng′(zl\nj)\n(46)\nδl =\n\u0000(W l+1)T δl+1\u0001\n⊙g′(zl),\n(47)\nwhere (W l+1)T is the transpose of the weight matrix W l+1 and Eq. 47 is the matrix\nform. This equation indicates that, having obtained the weight matrix at an arbitrary\ncurrent layer by forward propagation, the error at the layer immediately before can be\ncalculated by 1) multiplying the transpose of that weight matrix by he error at the\ncurrent layer and 2) taking the Hadamard product of the resulting vector.\n18\nHaving established the error at the output layer and the backpropagation rule, it\nremains to determine the relation of the partial derivative of the cost function w.r.t. any\nindividual weight at an arbitrary layer, in terms of the error at that layer δl. That is\n∂J\n∂wl\njk\n= ∂J\n∂zl\nj\n∂zl\nj\n∂wl\njk\n(48)\n=\n∂zl\nj\n∂wl\njk\nδl\nj\n(49)\n=\n∂P\nk wl\njkal−1\nk\n+ bl\n∂wl\njk\nδl\nj\n(50)\n∂J\n∂wl\njk\n= al−1\nk\nδl\nj.\n(51)\nHere, again the terms in the right have been interchanged, and the first term in the\nright has been substituted by the error definition in Eq. 49. In Eq. 50 the weighted\ninput zl\nj has been expanded, and the derivation realized, resulting in Eq. 51. Note that\nal−1\nk\nis the activation of the neuron in the layer immediately before that links the current\nneuron, a quantity that has been calculated by forward propagation. The second term\nis the current neuron error (Eq. 46) that has been recursively backpropagated from the\noutput layer.\nFinally, the gradient of the cost function w.r.t. the bias is\n∂J\n∂bl\nj\n= ∂J\n∂zl\nj\n∂zl\nj\n∂bl\nj\n(52)\n∂J\n∂bl\nj\n= δl\nj,\n(53)\nwhere the chain rule has been applied (Eq. 52) and the error at the layer has been\nreplaced by its definition, resulting in Eq. 53. In words, the contribution of the bias to\nthe gradient equals the neuron error.\nThe four main equations above (Eqs.\n39, 47, 51, and 53) enable calculating the\ngradient of the cost function w.r.t. every weight and bias. As already mentioned, in the\ncontext of deep learning, this is exactly the goal of backprop. The derivatives may then\nbe used to train the network by updating the weights. Algorithm 1 shows the complete\ntraining procedure. For a general derivation of backpropagation using computational\ngraphs [Goodfellow et al., 2016] may be consulted.\n19\nAlgorithm 1: Example ANN training algorithm. Here, we implement train-\ning by iterating over examples in a mini-batch. In practice, backprop is im-\nplemented to computed simultaneously the gradients of the entire mini-batch\nusing matrices.\nInput: A set of ordered training examples X with targets Y, an ANN of depth\nℓ.\nOutput: A trained ANN (model) ˆf(·).\nInitialization: Set parameters according to Kaiming initialization (Eq. 32).\nSet the hyperparameters (Table 1).\nIterative minimization of the cost function with gradient descent:\nforeach epoch e ∈1, · · · , χ do\nRandomly generate mini-batches B(i) of size n.\nSpecifically use stochastic gradient descent:\nforeach Mini-batch B(i) do\nforeach Example x(i) in mini-batch and its corresponding target yi do\nInput the training example:\nThe activations of the input layer are considered to be the example\ndata.\na(0) ←x(i)\nPerform the forward propagation pass:\nforeach Layer l ∈{1, · · · , ℓ} do\nCompute weighted input and activations (Eq. 30, 31).\nz(l) = W (l)a(l−1) + bl\na(l) ←g(z(l))\nCalculate the error vector at the output layer: (Eq. 39)\nδℓ= ∇aJ ⊙g′(zℓ)\nBackpropagation:\nforeach Layer l ∈{ℓ, · · · , 1} do\nBackpropagate the error (Eq. 47).\nδl =\n\u0000(W l+1)T δl+1\u0001\n⊙g′(zl)\nCalculate local gradients (Eq. 51)\n∂J\n∂wl\njk\n= al−1\nk\nδl\nj\nThis is the gradient corresponding to one training exaple.\n∇wJi(w) = { ∂J\n∂w1\njk\n, · · · , ∂J\n∂wℓ\njk\n}\nRecover all gradients:\nAverage over all examples (Eq. 34).\n∇wJ(w) = 1\nn\nPn\ni=1 ∇wJi(w).\nApply the weight update:\nUpdate the weight (Eq. 17).\nw ←w −ϵ∇wJ(w)\n20\n4.2.4 Hyperparameters\nIn most discussions through this tutorial, we have mentioned several quantities, but we\ndid not indicate how they can be set. These quantities are called hyperparameters.\nHyperparameters are usually set empirically or using some kind of intuition. Finding\nsuitable values of these hyperparameters is termed Hyperparameter tuning. Arguably,\nthe easiest way to establish their values is to set their default values or common values.\nTable 1 summarizes them.\nAutomated hyperparameters tuning algorithms (also AutoML - Automated Machine\nLearning) include grid search, random search, and model-based hyperparameter\noptimization. A comprehensive discussion of these methods may be found in [Hutter\net al., 2019].\nHyperparameter\nInterpretation\nSection\nDefault or common values\nϵ\nLearning rate\n3.4.2\n0.01, 0.1\nα\nRegularization coefficient\n3.4.3\n0.2\nχ\nNumber of epochs\n4.2.1\n5, 10, 100\nn\nBatch size (SGD)\n4.2.1\n100\nυ\nMomentum\n4.2.2\n0.9\nTable 1: Hyperparameters. The table summarizes the hyperparameters that we use in\nthis tutorial (first column). The second column indicates the corresponding\ninterpretation. Furthermore, we specify in which subsection they are described\n(third column) and their typical values, in the fourth column.\n5 Convolutional Neural Networks\nIn Sec. 4.1.2, we mentioned that the architecture of a general ANN may involve dif-\nferent layer types. Specifically, we described a dense layer, whose mathematical form is\nbasically matrix multiplication. Unfortunately, an architecture composed alone of dense\nlayers is computational extremely inefficient. For example, processing 200 × 200 pixels\nmonochrome images, like the ones we input to the ANN, requires 40000 connections (one\nconnection to every pixel) only for one neuron in the first hidden layer.\nConvolutional Neural Networks (CNN, also ConvNets) reduce strikingly the\ncomputational burden of having to store and update millions of weights in the network.\nTwo characteristics convert CNNs in an effective learning agent, namely sparse in-\nteractions (also sparse connectivity or sparse weights) and parameter sharing,\nwhich we will discuss beneath.\nCuriously, the invention of CNNs was not motivated by efficiency considerations. In-\nstead, ConvNets, like ANNs and Batchnorm (which we discuss further in 5.5 ), are\n21\ninspired by the functioning of the brain: “the network has a structure similar to the hi-\nerarchy model of the visual nervous system. (· · · ) The structure of this network has been\nsuggested by that of the visual nervous system of the vertebrate. ” [Fukushima, 1980].\nIn particular, the vertebrate brain exhibits a hierarchical structure where cells higher\nin the hierarchy (complex cells) respond to more complex patterns, while cells at lower\nstages (simple cells) respond to simpler patterns. Complex cells are also more robust to\nshifts in the input, a property now referred to as equivariant representation.\nConvNets have their own distinctive terminology. In particular, at least one of its\nlayers is a convolutional layer (Subsec. 5.1). To describe the components of a CNN,\nwe adopt the so-called simple layer terminology (in opposed to complex layer ter-\nminology).\nFor example, we consider that the convolutional layer does not apply a\nnon-linearity. Therefore, the layer’s output is the addition of the convolution 57 and a\nbias b, termed pre-activation. This allows us to have a finer control of the description\nthat will be useful when explaining the concrete CNN architecture. Note that in this\nterminology not every layer has weights, e.g., Subsec. 5.2. Fig. 2 shows an example\nof CNN that is able to estimate eight human body dimensions from grayscale synthetic\nimages.\n22\nFigure 2: An example of a CNN architecture performing regression.\nHere, we show\nan adapted version of our Neural Anthropometer from [Gonz´alez Tejeda and\nMayer, 2021], a CNN that is able to regress eight human body dimensions\nlike height or waist circumference from synthetic images of persons in two\nposes (top left).\nThe input to the network are grayscale 200 × 200 pixels\nsynthetic images of 3D real human meshes (top middle).\nThe supervision\nsignal is a vector of eight human body dimensions (top right) and the loss\nLMSE (bottom right) is the Mean Square Error MSE (Eq. 14) between the\nactual and the estimated measurements. Hyperparameters for learning with\nthis CNN are set as described in Table 1. We discuss the details of the layer\ntypes in Subsections 5.1-5.3. At the bottom, the five gray rectangular cuboids\nrepresent the input and the feature maps. Red and green horizontal pyramids\nrepresent convolution (Subsec.\n5.1) and max pooling (Subsec.\n5.2) layers\nrespectively. Black grids are Rectified linear unit (ReLU) layers (Subsec. 5.3),\nand connected circles are fully connected layers (Subsec. 4.1.2). In this case, 1)\nthe first convolutional layer applies a convolution with a 5-pixels square kernel\nto produce a feature map of size 196 × 196 × 8. The tensor is then passed\nthrough a ReLU and batch normalization (Subsec. 5.5, not shown here) is\napplied. Next, 2) max pooling with stride 2 is used producing a tensor of size\n98 × 98 × 8. Then, 3) the tensor is send to a second convolutional layer with\na 5-pixels square kernel and 16 output channels, resulting in a tensor of size\n94×94×16. Following, 4) pooling is applied as before producing a tensor of size\n47×47×16 and 5) the output is flatten to a tensor of size 35344. This tensor is\npassed to a fully connected layer and again through a ReLU (not shown). The\noutput layer 6) regresses the eight human body dimensions in meters. Using\nthe simple layer terminology, the depth of this network is 10: convolution +\nReLU + Batchnorm + max pooling + convolution + ReLU + max pooling\n+ fully connected + ReLU + fully connected (output layer). Other common\ncounting schema includes only the layers with learnable parameters, in which\ncase this network would have depth 5.\n23\nBoth the input to the network as the input to a specific convolutional layer may be\nnamed input map, while a specific set of shared weights is named as kernel (also filter)\nafter the second argument of the convolution operation 57. Because the kernel can be\nseen as a feature detector, the output (activations) of the convolutional layer is called\na feature map (also output map). Moreover, from the perspective of one fixed unit\na, the input elements affecting the calculation of its activation are called the receptive\nfield of a.\nFig.\n3 depicts a possible input and first hidden layer of a hypothetical\n2D-CNN.\n24\nFigure 3: Possible input and first hidden layer of a hypothetical CNN. Left, A general\nANN that receives the input {x1, · · · , x9}. The first hidden layer l contains\nfour neurons with activation {a1, · · · , a4} (circles in orange, magenta, green\nand red color, from top to bottom). We emphasize the first two neurons and its\nconnections to the input layer. Note that the neurons are not fully connected\nto the input layer, e.g., the unit al\n1 is connected to inputs {x1, x2, x4, x5},\nwhich is its receptive field, but not to input x3. Moreover, all four neurons\nhave equal weights {w1, · · · , w4}, indicated by the four arrows arriving to the\nunits. Right, the same network seen as a CNN. The input, the weights and\nthe neurons have a grid structure.\nThe connections of the neurons to the\ninput induce a 2D-convolution (I ∗K)(i, j) of the input I (3 × 3 grid) with\nthe kernel K (2 × 2 grid). The convolution may be visualized as sliding the\nkernel over the input grid to produced a feature map, where every element of\nthe feature map is the Frobenius product ⟨K, A⟩F of the matrix defined by\nthe kernel K and the overlapped input elements A, plus one shared bias. We\nhighlight the computation of two elements of the feature map (the other two\nare not depicted) by the corresponding broken (−−−) and broken with point\n(−·−·−) arrows of the highlighted orange and magenta neurons, respectively.\nFor example, the unit al\n1,1 has pre-activation zl\n1,1 = xl\n1,1 · wl−1\n1,1 + · · · + xl\n2,2 ·\nwl−1\n2,2 + bl. Note that there is only one bias bl per feature map. We adopt\nthe simple layer terminology, where al\ni,j is a linear unit that outputs a pre-\nactivation zl\ni,j. Therefore, al\ni,j = zl\ni,j. Supposing that the input I is an image,\nit may be observed that the sparse connections and the shared weights confer\nthe network the capability of establishing spatial relations among neighbor\npixels. For an input of size n × n convolved with a f × f kernel, the size of the\nfeature map is n −f + 1 × n −f + 1.\n25\nReturning to the efficiency of ConvNets, the sparse connectivity implies, depending on\nthe filter size, a dramatic reduction of the network weights. Kernels are designed such as\ntheir size is significantly smaller than the output. In the example of the above mentioned\n200 × 200 pixels image as input, a kernel size of only 5 × 5 may be appropriate. Here,\nthe reduction of the needed computational resources falls from 40000 to 25 connections\nper neuron in the first hidden layer. Surely, detecting several features requires a larger\nnumber of kernels. For example, popular modern CNNs like AlexNet[Krizhevsky et al.,\n2012] and VGG16 [Simonyan and Zisserman, 2014], depending on the network depth\nand design, employ dozens to hundreds of kernels per layer.\nAt the same time, the parameters (weights) in a convolutional layer are shared by\nseveral neurons, which results in a network having tied weights.\nFully connected\nlayers can not exploit the grid structure of the data because every connection is equally\nrated by the network. For example, if the input is an image, nearby pixels may represent\na potential important feature like an edge or texture. However, a dense layer does not\nconnect these nearby pixels in any special manner. Constraining the connection scheme\nto groups of weight sharing neurons, effectively allows to establish relationships between\nspatially related pixels.\n5.1 Convolutional Layer\nConvolutional networks are named as such because performing forward propagation in\none of their layers l, with sparse and tight weights, implies calculating a convolution of\nthe input I ∈Rm×n to the layer l, with at least one kernel K ∈Rf×f of a determined\nodd squared size f. The result of that convolution is a feature map M (Fig. 4).\nThe reason to use an odd-sized kernels (such as 3×3 or 5×5) is because they provide\na well-defined center element, allowing the filter to align symmetrically with the input\nat each position. Additionally, they make it possible to apply equal padding on all sides,\npreserving the spatial dimensions of the input in the resulting feature maps.\nAssume a general function dim(X) that maps a matrix X to its dimensions. Then\nthe dimensions of the input, kernel, and feature map are\ndim(I) def\n= {m, n}\n(54)\ndim(K) def\n= {f, f}\n(55)\ndim(M) = {m −f + 1, n −f + 1}.\n(56)\n26\nFigure 4: From left to right, the rectangles represent the concepts of the input I being\nconvolved with a filter F and the resulting feature map M of a CNN (I ∗F =\nM). In order to exemplify these concepts, we modified our Neural Anthro-\npometer network [Gonz´alez Tejeda and Mayer, 2021], that originally accepts as\ninput a grayscale image, to accept a RGB image of dim(I) = {200, 200} (left).\nOn the bottom of the rectangles we indicate the dimensions of the correspond-\ning tensor, as defined by the function dim(X) (Eq. 54 - 56). We display the\nfirst filter with dim(F ) = {5, 5} of the first convolutional layer in the trained\nnetwork. Note that, in order to enhance the visibility, we significantly scale\nthe filter in the middle. On the right, the red border indicates the reduced\ndimensionality of {4, 4} of the feature map with dim(M) = {196, 196} w.r.t.\nthe input.\nIn deep learning, the term convolution usually refers to the mathematical operation of\ndiscrete cross-correlation, which is the operation implemented by most deep learning\nframeworks. Assuming an input map I and a kernel K, one-based index cross-correlation\ntakes the form\n\u0000(I ∗K)(i, j)\n\u0001l =\nX\nm\nX\nn\nxl−1\ni+m−1,j+n−1 · wl\nm,n\n(57)\nzl\ni,j =\n\u0000(I ∗K)\n\u0001l + bl\ni,j\n(58)\nal\ni,j = zl\ni,j,\n(59)\nwhere bl\ni,j is the bias of the pre-activation zl\ni,j and al\ni,j = zl\ni,j the i, j element of a 2D\nfeature map.\nFurthermore, the convolution operation can be generalized in a number of ways. First,\nthe manner in which the kernel is slid over the input map. The kernel must not nec-\nessarily be moved to the immediately next spatial element to compute the Frobenious\nproduct. Instead, the kernel may be slid a number s of spatial locations. This is called\nthe stride s, and the corresponding operation may be called a strided convolution.\nThe baseline convolution has s = 1.\n27\nSecond, the kernel may be shifted beyond the elements at the border of the input map.\nSliding the kernel, such as it entirely lies within the input map, has the advantage that all\nthe elements of the feature map are a function of equal numbers of elements in the input.\nHowever, it also has the disadvantage of progressively reducing the output (Eq. 56)\nuntil the extreme stage where a 1 × 1 output can not be meaningfully further processed.\nAlternatively, the spatial dimensions may be preserved by zero-padding the input with\np elements. When p = 0 (no padding), the operation is called valid convolution, which\nis one of the most common. In contrast, constant spatial dimensionality across input\nand output may be achieved by performing a same convolution with the corresponding\np > 0 padding.\nFinally, the convolution operation may be generalized to operate on multidimensional\ninputs, e.g., RGB images, producing multidimensional outputs. This generalized convo-\nlution is termed convolutions over volumes because input, kernels, and output may\nbe considered as having a width, height, and depth (also channel to avoid confusion\nwith the network’s depth). This, besides the processing of batches (Subsec. 5.5), results\nin CNNs usually operating in tensors of size b × w × h × c, i.e., batch size b, two spatial\ndimensions width w and height h, and c number of channels, respectively. In this tuto-\nrial, we focus on inputs (images) with a maximum of three channels. Without loss of\ngenerality, we will consider one instance batches b = 1, therefore, we will omit the batch\ndimension. The input and the filters are required to have an equal number of channels\nc. The dimensions of the tensor of feature maps M are determined by the dimensions of\nthe input, the size, and the number of kernels.\nRegarding the dimensionality, convolving a multichannel input I with k multichannel\nkernels K:,:,:,k of equal dimensions results in a tensor of feature maps M. The above\nfunction dim(·) can now be generalized to n dimensions. Assume a padding p and a\nstride s, then\ndim(I) def\n= {w, h, c}\n(60)\ndim(K) def\n= {f, f, c, k}\n(61)\ndim(M) = {⌊w −2p −f\ns\n+ 1⌋, ⌊h −2p −f\ns\n+ 1⌋, k},\n(62)\nwhere ⌊x⌋is the floor function that rounds its argument to the nearest integer.\nCorrespondingly, the 2D−convolution (Eq. 57) may be generalized to a 3D−convolution\nin layer l as expected. A specific element Mi,j,k of a determined output channel k is cal-\nculated by 1) sliding the three-dimensional kernel K:,:,:,k with weights wm,n,c across the\nthree-dimensional input and then 2) computing the tensor inner product K:,:,:,k · A of\nthe kernel and the three-dimensional tensor A of overlapped input elements. Stacking\nthe elements of all channels that have been generated by sliding k kernels produces the\nlayer’s output volume Ml with elements M l\ni,j,k. That is,\n28\n\u0000(I ∗K)(i, j, k)\n\u0001l =\nX\nc\nX\nm\nX\nn\nxl−1\ni+m−1,j+n−1,c · wl\nm,n,c,k\n(63)\nzl\ni,j,k =\n\u0000(I ∗K)(i, j, k)\n\u0001l + bl\n(64)\nM l\ni,j,k = zl\ni,j,k.\n(65)\n5.2 Pooling Layer\nYet another prominent method to reduce the amount of parameters in the network is to\nuse a pooling layer. Pooling outputs the result of computing a determined summary\nstatistic in neighborhoods of activations of the previous layer.\nLike convolution, the pooling function may be viewed as sliding a multi-channel win-\ndow over a multi-dimensional input with equal amount of channels.\nTherefore, the\npooling function may be considered as a filter of squared size f that may be shifted by\na stride s, acting on a possibly p zero-padded input. Unlike the convolution layer, the\npooling layer does not have weights that must be updated by the learning algorithm.\nAdditionally, pooling is applied on each channel independently. For an input volume\nM with dimensions {w, h, c}, pooling’s output O dimensionality is\ndim(O) = {⌊w + 2p −f\ns\n+ 1⌋, ⌊h + 2p −f\ns\n+ 1⌋, c}.\n(66)\nWhile the pooling function may calculate several summary statistics on any combina-\ntion of dimensions, the most used is the operator that yields the maximum among the\ncorresponding channel input elements M:,:,k inside the sliding window max(X), termed\nmax pooling. That is, in one-based index form,\nOl\ni,j,k = max\nm max\nn\nMl−1\n(i·s−1)+(m−1),(j·s−1)+(n−1),k.\n(67)\n5.3 ReLU layer\nAs we mentioned, we adopted the simple layer terminology, where the non-linearities are\nnot part of the convolutional layer. Instead, we consider that there is a ReLU layer\nwhich may be integrated into the network’s architecture. As expected, the ReLU layer\ncomputes Eq. 21.\n5.4 CNN Training\n5.4.1 Forward and Back-propagation in CNNs\nSince CNNs are deep feedforward network (Sec. 4), its weights may be initialized using\nthe Kaiming method (Subsec. 4.2.3). CNNs may also be trained with gradient descent\n3.4.2. In particular, forward propagation (Eqs. 63-65, 21, and 67) must be performed to\nobtain the error at the output layer, followed by backprop.\n29\nAs we discussed before (Subsec. 4.2.3) while backpropagating the error, the derivatives\nof the loss function w.r.t. the parameters need to be computed in order to update the\nweights. For CNNs, we will follow the strategy that we described in Subsec. 4.2.3. That\nis, 1) compute the error at the output layer, 2) computer the error of the current layer in\nterms of the next layer to be able to backpropagate, 3) compute the weight derivatives\nin terms of the error at the current layer and 4) compute the derivative of the bias.\nIn a CNN, the convolutional layer’s parameters are the kernels’ weights and the layer\nbiases, one for each kernel. Therefore, the goal of backprop may be restated as computing\nthe derivatives of the loss w.r.t. the kernels and the bias. Additionally, the derivatives\nof the ReLU and max pooling layer must be considered.\nSince now we are operating with 3D tensors, we define the derivative of the cost\nfunction w.r.t one input element zl\nx,y,z as\nδl\nx,y,z =\n∂J\n∂zlx,y,z\n.\n(68)\nCommonly, the output layer of a CNN is a dense layer. Therefore, the error at the\noutput layer δℓmay be computed in a the same manner as for a general ANN (Eq.\n39). As we will see, the architecture of CNNs comprises ReLU and max pooling layers.\nTherefore, the computation of the backpropgation and concrete weights (Eqs. 47 and\n51) must be adapted to these layers.\nFortunately, neither the ReLU layer nor the max pooling layer contain weights that\nmust be updated by SGD. That means Eqs. 51-53 must be disregarded for these two\nlayers. However, in our strategy these layers must be able to transmit the error back to\npreceding layers, which indeed requires readjusting Eq. 47 (backpropagation equation\nbetween layer l + 1 and l). For that matter, we may consider that the error at these\nlayers are known, e.g., after having been backpropagated by a dense layer.\nOn order to adapt Eq.\n47 to the ReLU layer, note that, because the layer does\nnot contain weights, the weigh matrix W vanishes. Also, we now consider the local\ngradient δl\ni,j,k to be a tensor element. Since forward propagating through a ReLU layer\nis conducted elementwise, the input and output dimensions are preserved, and so happens\nwith backpropagation. This allows equal indexing of the backpropagated error arriving\nat the layer and the calculated local gradient. That is,\nδl\ni,j,k = δl+1\ni,j,k · g′(zl\ni,j,k),\n(69)\nwhere, g′(zl\ni,j,k) is the derivative of the ReLU evaluated at the tensor element Ml\ni,j,k.\nConversely, one key observation is that the max pooling layer does change the dimen-\nsionality of its input. Specifically, it downsamples the input by discarding its activations\nin the sliding window, except for the one with the maximum value. These units, whose\nactivations were discarded, will obviously receive zero gradient.\nDuring forward propagation, the max pooling operation does more than just select the\nmaximum value in each window; it also ”memorizes” the indices of these maximum values\nwithin the tensor. This creates a one-to-one mapping between each pooled activation\nMm,n,k—the maximum value in each region—and the corresponding output element\n30\nOi,j,k, based on these retained indices.\nWhen backpropagating, only the unit whose\nindex has been memorized receives a non-zero gradient. This is called gradient routing\nbecause the max pooling layer routes back the gradient it receives to the unit whose\nmaximum value was selected. For example, if the maximum value in a pooling window\nis at M1,2,3, and this value is mapped to an output element, say O0,1,3, the index {1, 2, 3}\nis preserved. Consequently, during backpropagation, this memorized index ensures that\nthe gradient flows back to and only to M1,2,3.\nBecause the partial derivative of the max operation\n∂g\n∂Mm,n,k w.r.t the pooled value\nMm,n,k equals one, Eq. 47 takes the form\nδl\nm,n,k =\n(\nδl+1\ni,j,k\nif Mm,n,k = Oi,j,k\n0\notherwise.\n(70)\nHaving obtained the backpropagation rules of the ReLU and max pooling layer, it\nremains to discussed backpropagation on the convolutional layer. Indeed, in this case,\nthe kernel weights and the units bias must be updated. Consequently, 1) the backprop-\nagation rule of the convolution layer and 2) the derivative of the loss function w.r.t. the\nkernel weights and neuron bias must be derived.\nFirst, let us consider the backpropagation rule for the convolutional layer. Like for\nfully connected layers, we start by observing that the chain rule may be employed to\ncompute the neuron error in layer l. Also, to unclutter notation, we will assume zero\npadding and unit stride (p = 0, s = 1).\nUnlike the general case, one input element in a convolutional layer l affects one or\nmore elements in layer l + 1, but not all. Therefore, the partial derivative of the cost\nfunction w.r.t. one element zl\nm,n,c must aggregate the partial derivatives of all elements\nzl+1\n·,·,· affected by it.\nBecause the convolutional layer downsamples the input spatial dimensions, careful\nmust be taken when selecting the output element indices whose contributions must be\nadded. One important observation is that one input element affects all feature maps (al-\nbeit not all elements), thus we must sum over all output channels k. Another observation\nis that non multi-channel kernel Km,n,c move across the depth dimension, therefore, their\nindex does not contain offsets. After having considered the channels, we must further\nconsider the effect of the input element zl\nm,n,c in one specific output channel zl\n:,:,k. Given\na specific channels k, the input element affects a number of elements in that channel,\nnamely the elements fixed at the position m, n and that are the result of the kernel hav-\ning been slit by by −a, −b (input perspective, we introduce auxiliary indices a and b).\nFinally, we know that all kernels have the same dimensionality (they are packed in one\n4D-tensor), thus, we can use the same iterators a, b for all kernels. These observations\nlead to\n31\nδl\nm,n,u =\nX\nk\nX\na\nX\nb\n∂J\n∂zl+1\nm−a+1,n−b+1,k\n·\n∂zl+1\nm−a+1,n−b+1,k\n∂zlm,n,u\n(71)\n=\nX\nk\nX\na\nX\nb\nδl+1\nm−a+1,n−b+1,k ·\n∂zl+1\nm−a+1,n−b+1,k\n∂zlm,n,u\n.\n(72)\nIn Eq.\n72, the neuron error\n∂J\n∂zl+1\nm−a+1,n−b+1,k has been substituted by its definition\nδl+1\nm−a+1,n−b+1,k.\nWe now focus in the second term in the right and expand it. Here, again we introduce\nauxiliary indices p, q, r, and use xl\n·,·,· = zl\n·,·,· (as mentioned, we assumed we are operating\non linear layers).\n∂zl+1\nm−a+1,n−b+1,k\n∂zlm,n,c\n=\n∂\n∂zlm,n,c\n\u0010 X\np\nX\nq\nX\nr\nzl\n(m−a+1)+p−1,(n−b+1)+q−1,r · wl+1\np,q,r,k + bl\u0011\n(73)\n=\n∂\n∂zlm,n,c\n\u0010 X\np\nX\nq\nX\nr\nzl\nm−a+p,n−b+q,r · wl+1\np,q,r,k + bl\u0011\n(74)\n=\n∂\n∂zlm,n,c\n\u0010\nzl\nm,n,c · wl+1\na,b,c,k\n\u0011\n(75)\n= wl+1\na,b,c,k.\n(76)\nIn Eq. 73 we expanded as mentioned above, then in Eq. 74, we reduced the indices\nas (m −a + 1) + p −1 = m −a + p, (n −b + 1) + q −1 = n −b + q. In Eq. 72, it may\nbe observed that all partial derivatives w.r.t. zl\nm,n,c, except when a = p, b = q, r = c.\nAccordingly, the weights must have indices a, b, c, k. Then, plugging Eq. 76 in Eq.72,\nwe have:\nδl\nm,n,c =\nX\nk\nX\na\nX\nb\nδl+1\nm−a+1,n−b+1,k · wl+1\na,b,c,k.\n(77)\nδl\nm,n,c = δl+1\nm,n,c ∗rot180◦(wl+1\na,b,c,k).\n(78)\nEq. 77 is a standard convolution of the error at the end of the convolutional layer\n(the error it receives from the following layer) with the kernel tensor. Equivalently, Eq.\n77 may expressed as a cross-correlation with flipped kernel (Eq. 78) by rotating the 3D\nkernel 180◦around the depth axis (rot180◦(·)). Note in Eq. 77 that the matrix with\nelements δl+1\n·,·,· has reduced spatial dimensions compared with the input I to the convolu-\ntional layer. Therefore, to backpropagate, the matrix must be zero-padded (upsampled)\nby the amount f −p −1 = f −1 (recall f is the filter size) such as after having been\nconvolved, I’s dimensions are obtained. In another words, backpropagation requires a\ntransposed convolution. Furthermore, Eq. 77 suggests that, in order to propagate\n32\nthe error to the input unit δl\nm,n,c, one must collect the kernel weights in the channel c of\nall kernels k, and multiply them with the corresponding elements in the gradient tensor\nat the corresponding k feature map.\nAn interesting concrete example in given by [Aggarwal, 2018]:\n“In order to understand the transposition above, consider a situation in which we use\n20 filters on the 3-channel RGB volume in order to create an output volume of depth\n20. While backpropagating, we will need to take a gradient volume of depth 20 and\ntransform to a gradient volume of depth 3. Therefore, we need to create 3 filters for\nbackpropagation, each of which is for the red, green, and blue colors. We pull out the\n20 spatial slices from the 20 filters that are applied to the red color, invert them (...),\nand then create a single 20-depth filter for backpropagating gradients with respect to\nthe red slice. Similar approaches are used for the green and blue slices.´´\nSimilarly, the derivative of the cost function w.r.t. to the kernel weights wl\n·,·,· must\nbe computed. A kernel weight wl\nm,n,c,k affects only the elements in the k feature map.\nTherefore, when applying the chain rule, its gradient must add the partial derivative of\nall those elements:\n∂J\n∂wl\nm,n,c,k\n=\nX\ni\nX\nj\n∂J\n∂zl\ni,j,k\n·\n∂zl\ni,j,k\n∂wl\nm,n,c,k\n(79)\n=\nX\ni\nX\nj\nδl\ni,j,k ·\n∂zl\ni,j,k\n∂wl\nm,n,c,k\n.\n(80)\nThe same techniques as before may be applied in Eq. 79, i.e., substitute the neuron\nerror by its definition which results in Eq. 80. Then the second term in the right may\nbe expanded as\n∂zl\ni,j,k\n∂wl\nm,n,c,k\n=\n∂\n∂wl\nm,n,c,k\n\u0010 X\nr\nX\np\nX\nq\nzl\ni+p−1,j+q−1,r · wl\np,q,r,k + bl\u0011\n(81)\n=\n∂(zl\ni+m−1,j+n−1,c · wl\nm,n,c,k)\n∂wl\nm,n,c,k\n(82)\n= zl−1\ni+m−1,j+n−1,c.\n(83)\nCalculating the partial derivative of the triple summation in Eq.81 reduce all terms\nto zero except the term with index p = m, q = n and r = c. The result (83) may be\nsubstitute in Eq. 80, i.e.,\n∂J\n∂wl\nm,n,c,k\n=\nX\ni\nX\nj\nδl\ni,j,k · zl−1\ni+m−1,j+n−1,c\n(84)\n(85)\n33\nComparing Eq. 82 to Eq. 78, it may be noted that while backpropagating the error\nboth the kernel and the layer error must be rotated.\nThe derivation of the error of the cost function w.r.t. the bias follows also the above\nstrategy. The shared bias affects all activations in the feature map. However, from Eq.\n57 may be concluded that the partial derivative ot all activations i, j, k w.r.t. the bias\nequal one, i.e.,\n∂zl\ni,j,k\n∂bl\n= 1. Thus,\n∂J\n∂bl =\nX\nk\nX\na\nX\nb\n∂J\n∂zl\na,b,k\n·\n∂zl\na,b,k\n∂bl\n(86)\n=\nX\nk\nX\na\nX\nb\nδl\na,b,k.\n(87)\n5.5 Batch Normalization\nOne of the earliest findings in machine learning was that imputing features in different\nscales hinders ANN learning [Bishop, 1995] [LeCun et al., 2012]. Except for settings\nwhere a preprocessing of the features is not desired, feature normalization is standard\npractice.\nWhile features are supplied to the ANN at the input layer, the question arises naturally\nif the same strategy may be applied to the hidden layers.\nAs shown by Eq.\n22, a\ndetermined hidden layer’s input is a highly non-linear function of the ANN inputs. From\nthe perspective of the hidden layer, the input to its units constantly varies in possibly\nseveral scales, complicating learning.\nThis motivates the Batch normalization (also Batchnorm) method [Ioffe and\nSzegedy, 2015]. Batchnorm proposes to normalize the input to a layer to stabilize op-\ntimization.\nThis method may be presented in several forms (cf.\n[Ioffe and Szegedy,\n2015], [Goodfellow et al., 2016]). Specifically, while the former explains Batchnorm from\na transform perspective, the latter considers it a reparametrizing technique: “ Batch\nnormalization provides an elegant way of reparametrizing almost any deep network”.\nHere, we follow this approach.\nConsider a mini-batch of activations (output) at a determined layer l −1, which may\nbe normalized as to train the layer l faster. This may be represented by a design matrix\nH ∈Rn×m with elements aij, where i is the activation of unit i corresponding to one\nmini-batch example x(j). That is, the rows of H are the activations vector corresponding\nto one training example, while the columns are the activations at unit i of all mini-batch\ntraining examples.\nBatchnorm normalizes each activation independently by subtracting the mean µ and\ndividing by the standard deviation σ in a per-column basis. That is,\n34\nµj = 1\nm\nm\nX\ni=1\naij\n(88)\nσj = δ + 1\nm\nv\nu\nu\nt\nm\nX\ni=1\n|aij −µj|2\n(89)\nµ = {µ1, · · · , µm}\n(90)\nσ = {σ1, · · · , σm}\n(91)\nH′ = H −µ\nσ\n,\n(92)\nwhere the components of the mean vector µ (Eq. 90) and the standard deviation\nvector σ (Eq. 91) are the mean (Eq. 88) and the standard deviation (Eq. 89) of each\nunit. In Eq. 89, a small constant δ (e.g., 10−8) is added for numerical stability. In Eq.\n92 the vectors µ and σ are broadcasted to normalize aij using σj and µj.\nAlbeit intuitive, restricting the distribution of the activation to have zero mean and\nunit standard deviation is an arbitrary choice. A priori, it is not guaranteed that this\nsetting will result in accelerating learning. Optimally, the ANN should learn how to\ntransform the activations.\nIndeed, instead of replacing H by its normalized form H′, the activations may be\nreplaced with the parametrized form BN(H) = γH′ + β, where the vectors γ and\nβ may be learned by SGD, say. The arithmetic here is as described above, i.e., the\nactivation aij is replaced by\nγj · aij −µj\nδj\n+ βj.\n(93)\nThis form has the advantage that it can recover the original normalized activations\nH′ for γ = 1, β = 0, and, more importantly allows the activation distribution to be\narbitrarily shifted and scaled by the learning algorithm.\nDuring training, the learning algorithm updates the parameters β and δ so that when\ntraining has finished, these parameters are ready to use for inference. In contrast, the\nempirical statistics µ and δ used to normalize the activations are computed on a mini-\nbatch (Eqs. 88-89). This may pose two problems. First, during inference with one\ninstance, the statistics are not well defined (because of the unit batch size). Second,\nif inference is conducted for a mini-batch, the relation of one determined instance with\nrespect to its predicted target is not deterministic because the statistics depend on the\nbatch in which this instance may appear.\nOne standard solution (that we also use) is to maintain running averages of the statis-\ntics during training, i.e., for each activation, µ and σ are averaged over all mini-batches\nin the training set. Then, inference is conducted with these fixed statistics.\nThe effect of Batchnorm is to some extent controversial in the scientific community.\nFor example, while the authors of the original paper claimed that Batchnorm diminish\n35\nthe constant variation of the mean and variance of the hidden layers (internal covariance\nshift) [Ioffe and Szegedy, 2015], other scientists have shown that Batchnorm’s efficiency\nis based on a different mechanism [Santurkar et al., 2018]. That being said, Batchnorm\nis an indispensable part of ANNs.\n6 Summary\nIn this tutorial we examined the foundations of Deep Learning with special focus on\nConvolutional Neural Network. Deep Learning belongs to a wider set of methods called\nMachine Learning, which main goal is to develop system or agents that are able to learn\nfrom data.\nThe most important concept of learning is generalization. That is, given a dataset, the\nlearning algorithm is trained on the data to perform a specific task, and is required to\naccomplish the task on data it did not experience before. In particular, is expected that\nthe agent outputs a certain type of inference or prediction based on the data. Common\ntasks are classification (predicting a category) and regression (predicting a continuous\nquantity), for example, classifying images of cars and regressing the height of person in\na picture.\nThe dataset is a key component of the learning algorithm. It is composed of examples\nor observations, each having features that contain useful information for training the\nagent. The dataset may include a prediction target for every example, such as the agent\ncan learn by processing the examples and observing the correct prediction it has to\noutput. This is called supervised learning and the targets accompanying the examples\nare the ground truth. The opposite case, in which the dataset does not contain ground\ntruth is named unsupervised learning. Additionally, there other two forms of learning,\nnamely, semi-supervised learning and reinforcement leaning, but this tutorial focused on\nsupervised learning.\nDeep learning borrows concepts from Learning theory, and so it assumes that there\nis function in a hypothesis space that maps the examples to their targets. In this view,\nthe learning agent is the model that approximates the function in the hypothesis space.\nThe less generalization error, the better the approximation. Theoretically, the dataset\nmay be divided in a training set, where the training error may be calculated, and a test\nset where generalization error is supposed to be assessed. However, while training, the\nagent has no access to the test set and can only estimate the generalization error using\nthe training set. This error is called empirical risk, therefore, the agent general strategy\nfrom the perspective of leaning theory is termed Empirical Risk Minimization.\nIn addition to Learning Theory, Deep Learning also borrows concepts from Statistics\nand Probability theory. In particular, the leaning agent is considered an estimator that\nmay be parameterized. When analyzing ANNs, these parameters are conceptualized as\nweights. This leads to the conclusion that the estimator function is the neural network.\nArtificial Neural Networks are comprised of artificial neurons. The neurons compute\na non-linear function of the linear combination of the inputs.\nTo train the network, gradient descent may be used. Gradient descent requires up-\n36\ndating the network weights. In order to compute the suitable update magnitude, three\nsteps must be iteratively conducted. First, forward propagate the information from the\ninput layer, through the neurons, to the output layer. Second, calculate the error of the\ncost function w.r.t. parameters at the output layer, and finally, backpropagate the error\nuntil the input layer.\nMost of the fundamental principles in which DL rest have been developed in the\ndecades ago or even before. For example, statistical learning, learning theory, SGD,\nPerceptron, Backprop, CNN. However they remain being used in all modern DL appli-\ncations.\nSeveral of the most important innovations in the DL were inspired by the structure\nand functioning of human and animal brain. Perceptron, ReLU, receptive fields (CNN),\nBatchNorm.\nAlthough regression tasks may be cast to classification tasks by discretizing the out-\nput, regression has a number of advantages compared to classification when the task\ninvolves predicting continuous quantities. For example regression provides higher reso-\nlution, less information loss, or, depending on the manner the problem is framed, more\nappropriate error metrics.\nIn cases like weather prediction (temperature and future\nrainfall) regression is optimal.\nWhile regression offers precise predictions and is suitable for continuous data, it comes\nwith challenges such as sensitivity to outliers, complexity in evaluating performance, and\ndifficulty in handling nonlinear relationships or imbalanced data. For some problems,\nthese factors can make classification a more straightforward or effective approach.\nThat being said, deep regression is a powerful framework that can be employed to\ntackle several problems in the real world.\nReferences\n[Aggarwal, 2018] Aggarwal, C. C. (2018). Neural Networks and Deep Learning. A text\nbook. Springer International Publishing. 33\n[Bishop, 1995] Bishop, C. (1995). Neural networks for pattern recognition. Oxford Uni-\nversity Press, USA. 34\n[Bishop, 2007] Bishop, C. M. (2007). Pattern Recognition and Machine Learning (In-\nformation Science and Statistics). Springer, 1 edition. 14\n[Cybenko, 1989] Cybenko, G. (1989). Approximation by superpositions of a sigmoidal\nfunction. Math. Control. Signals Syst., 2(4):303–314. 12\n[Fukushima, 1980] Fukushima, K. (1980). Neocognitron: A self-organizing neural net-\nwork model for a mechanism of pattern recognition unaffected by shift in position.\nBiological Cybernetics, 36:193–202. 22\n[Glorot and Bengio, 2010] Glorot, X. and Bengio, Y. (2010). Understanding the diffi-\nculty of training deep feedforward neural networks. In Teh, Y. W. and Titterington,\n37\nM., editors, Proceedings of the Thirteenth International Conference on Artificial Intel-\nligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages\n249–256, Chia Laguna Resort, Sardinia, Italy. PMLR. 15\n[Goh, 2017] Goh, G. (2017). Why momentum really works. Distill. 14\n[Gonz´alez Tejeda and Mayer, 2021] Gonz´alez Tejeda, Y. and Mayer, H. A. (2021). A\nneural anthropometer learning from body dimensions computed on human 3d meshes.\nIn 2021 IEEE Symposium Series on Computational Intelligence (SSCI), pages 1–8.\n23, 27\n[Goodfellow et al., 2016] Goodfellow, I., Bengio, Y., and Courville, A. (2016).\nDeep\nLearning. MIT Press. 2, 5, 10, 19, 34\n[Gripenberg, 2003] Gripenberg, G. (2003). Approximation by neural networks with a\nbounded number of nodes at each level. J. Approx. Theory, 122(2):260–266. 12\n[Hardt et al., 2016] Hardt, M., Recht, B., and Singer, Y. (2016). Train faster, generalize\nbetter: Stability of stochastic gradient descent. In Balcan, M. F. and Weinberger,\nK. Q., editors, Proceedings of The 33rd International Conference on Machine Learn-\ning, volume 48 of Proceedings of Machine Learning Research, pages 1225–1234, New\nYork, New York, USA. PMLR. 13\n[Hastie et al., 2009] Hastie, T., Tibshirani, R., and Friedman, J. (2009). The elements\nof statistical learning: data mining, inference and prediction. Springer, 2 edition. 5\n[He et al., 2015] He, K., Zhang, X., Ren, S., and Sun, J. (2015).\nDelving deep into\nrectifiers: Surpassing human-level performance on imagenet classification. In 2015\nIEEE International Conference on Computer Vision (ICCV), pages 1026–1034. 15\n[Hebb, 1949] Hebb, D. O. (1949). The organization of behavior: A neuropsychological\ntheory. Psychology Press. 10\n[Hutter et al., 2019] Hutter, F., Kotthoff, L., and Vanschoren, J., editors (2019). Auto-\nmated Machine Learning - Methods, Systems, Challenges. Springer. 21\n[Ioffe and Szegedy, 2015] Ioffe, S. and Szegedy, C. (2015). Batch normalization: Acceler-\nating deep network training by reducing internal covariate shift. In Bach, F. and Blei,\nD., editors, Proceedings of the 32nd International Conference on Machine Learning,\nvolume 37 of Proceedings of Machine Learning Research, pages 448–456, Lille, France.\nPMLR. 34, 36\n[Jarrett et al., 2009] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009).\nWhat is the best multi-stage architecture for object recognition? In 2009 IEEE 12th\ninternational conference on computer vision, pages 2146–2153. IEEE. 10\n[Krizhevsky et al., 2012] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Ima-\ngenet classification with deep convolutional neural networks. In Advances in neural\ninformation processing systems, pages 1097–1105. 26\n38\n[Krogh and Hertz, 1991] Krogh, A. and Hertz, J. (1991). A simple weight decay can im-\nprove generalization. In Moody, J., Hanson, S., and Lippmann, R., editors, Advances\nin Neural Information Processing Systems, volume 4. Morgan-Kaufmann. 9\n[LeCun et al., 2012] LeCun, Y. A., Bottou, L., Orr, G. B., and M¨uller, K.-R. (2012).\nEfficient backprop. In Lecture Notes in Computer Science, pages 9–48. Springer Berlin\nHeidelberg. 34\n[Lu et al., 2017] Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. (2017). The expressive\npower of neural networks: A view from the width. In NIPS. 12\n[Maiorov and Pinkus, 1999] Maiorov, V. and Pinkus, A. (1999). Lower bounds for ap-\nproximation by MLP neural networks. Neurocomputing, 25(1-3):81–91. 12\n[McCulloch and Pitts, 1943] McCulloch, W. S. and Pitts, W. (1943). A logical calculus\nof the ideas immanent in nervous activity. The bulletin of mathematical biophysics,\n5(4):115–133. 10\n[McElreath, 2020] McElreath, R. (2020). Statistical rethinking: A Bayesian course with\nexamples in R and Stan. Chapman and Hall/CRC. 7\n[Newton et al., 2018] Newton, D., Yousefian, F., and Pasupathy, R. (2018). Stochastic\ngradient descent: Recent trends. In Recent Advances in Optimization and Modeling\nof Contemporary Problems, pages 193–220. INFORMS. 13\n[Nielsen, 2015] Nielsen, M. A. (2015). Neural networks and deep learning. 14, 16\n[Paszke et al., 2019] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan,\nG., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,\nDeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and\nChintala, S. (2019). Pytorch: An imperative style, high-performance deep learning\nlibrary. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and\nGarnett, R., editors, Advances in Neural Information Processing Systems 32, pages\n8024–8035. Curran Associates, Inc. 16\n[Robbins, 1951] Robbins, H. E. (1951). A stochastic approximation method. Annals of\nMathematical Statistics, 22:400–407. 13\n[Rosenblatt, 1957] Rosenblatt, F. (1957). The perceptron - a perceiving and recognizing\nautomaton. Technical Report 85-460-1, Cornell Aeronautical Laboratory, Ithaca, New\nYork. 10\n[Rusell and Norvig, 2010] Rusell, S. J. and Norvig, P. (2010). Artificial Intelligence. A\nModern Approach. Pearson EDucation, Inc. 2\n[Santurkar et al., 2018] Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A. (2018). How\ndoes batch normalization help optimization? In Bengio, S., Wallach, H., Larochelle,\nH., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural\nInformation Processing Systems, volume 31. Curran Associates, Inc. 36\n39\n[Shalev-Shwartz and Ben-David, 2014] Shalev-Shwartz, S. and Ben-David, S. (2014).\nUnderstanding machine learning. Cambridge University Press, Cambridge, England.\n4\n[Simonyan and Zisserman, 2014] Simonyan,\nK. and Zisserman,\nA. (2014).\nVery\ndeep convolutional networks for large-scale image recognition.\narXiv preprint\narXiv:1409.1556. 26\n40\nNotation\nIn this tutorial, we use the following notation:\nNumber and Arrays\na\nA scalar (integer or real)\na\nA vector\nA\nA matrix\nA\nA tensor\na\nA scalar random variable\na\nA vector-value random variable\nA\nA matrix-value random variable\nSets\nA\nA set\nR\nThe set of real numbers\n{0, 1, · · · , n}\nThe set of all integers between 0 and n\nIndexing\nai\nElement i of vector a\na−i\nAll elements of vector a except for element i\nAi,j\nElement i, j of matrix A\nAi,:\nRow i of matrix A\nA:,i\nColumn i of matrix A\nAi,j,k\nElement (i, j, k) of a tensor A\nA:,:,i\n2-D slice of a 3-D tensor\nA:,:,:,i\n3-D slice of a 4-D tensor\nLinear Algebra Operations\nAT\nTranspose of matrix A\nA ⊙B\nElement-wise (Hadamard) product of A and B\nCalculus\n41\ndy\ndx\nDerivative of y with respect to x\n∂z\n∂y\nPartial derivative of y with respect to x\n∇xy\nGradient of y with respect to x\nProbability and Information Theory\nP(a)\nA probability distribution over a discrete variable\np(a)\nA probability distribution over a continuous variable, or over\na variable whose type has not been defined\na ∼P\nRandom variable a has distribution P\nEx∼P [f(x)] or Ef(x)\nExpectation of f(x) with respect to P(x)\nDKL(P∥Q)\nKullback-Leibler divergence of P and Q\nN(x; µ, Σ)\nGaussian distribution over x with mean µ and covariance Σ\nFunctions\nf : A →B\nFunction f with domain A and range B\nf(·)(i)\nfunction i of an ordered set of functions\nf ◦g\nComposition of the functions f and g\nf(x; θ)\nA function of x parametrized by θ\nlog x\nNatural logarithm of x\nσ(x)\nLogistic sigmoid,\n1\n1 + exp(−z)\nζ(x)\nSoftplus, log(1 + exp(z))\n∥x∥p\nLp norm of x\n∥x∥\nL2 norm of x\nDatasets and Distributions\npdata\nThe data generating distribution\nˆpdata\nThe empirical distribution defined by the training set\nX\nA set of training examples\nx(i)\nThe i-th example (input) from a dataset\nyi, y(i), or y(i)\nThe target associated with x(i) for supervised learning\nX\nThe m × n matrix with input example x(i) in row Xi,:\n42\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-08-22",
  "updated": "2024-11-12"
}