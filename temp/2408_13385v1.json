{
  "id": "http://arxiv.org/abs/2408.13385v1",
  "title": "MICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning",
  "authors": [
    "Zhenyu Zhang",
    "Guangyao Chen",
    "Yixiong Zou",
    "Zhimeng Huang",
    "Yuhua Li",
    "Ruixuan Li"
  ],
  "abstract": "Humans exhibit a remarkable ability to learn quickly from a limited number of\nlabeled samples, a capability that starkly contrasts with that of current\nmachine learning systems. Unsupervised Few-Shot Learning (U-FSL) seeks to\nbridge this divide by reducing reliance on annotated datasets during initial\ntraining phases. In this work, we first quantitatively assess the impacts of\nMasked Image Modeling (MIM) and Contrastive Learning (CL) on few-shot learning\ntasks. Our findings highlight the respective limitations of MIM and CL in terms\nof discriminative and generalization abilities, which contribute to their\nunderperformance in U-FSL contexts. To address these trade-offs between\ngeneralization and discriminability in unsupervised pretraining, we introduce a\nnovel paradigm named Masked Image Contrastive Modeling (MICM). MICM creatively\ncombines the targeted object learning strength of CL with the generalized\nvisual feature learning capability of MIM, significantly enhancing its efficacy\nin downstream few-shot learning inference. Extensive experimental analyses\nconfirm the advantages of MICM, demonstrating significant improvements in both\ngeneralization and discrimination capabilities for few-shot learning. Our\ncomprehensive quantitative evaluations further substantiate the superiority of\nMICM, showing that our two-stage U-FSL framework based on MICM markedly\noutperforms existing leading baselines.",
  "text": "MICM: Rethinking Unsupervised Pretraining for Enhanced\nFew-shot Learning\nZhenyu Zhang∗\nSchool of Computer Science and\nTechnology, Huazhong University of\nScience and Technology\nWuhan, China\nm202273680@hust.edu.cn\nGuangyao Chen∗\nNational Key Laboratory for\nMultimedia Information Processing,\nSchool of Computer Science, Peking\nUniversity\nBeijing, China\ngy.chen@pku.edu.cn\nYixiong Zou†\nSchool of Computer Science and\nTechnology, Huazhong University of\nScience and Technology\nWuhan, China\nyixiongz@hust.edu.cn\nZhimeng Huang\nNational Engineering Research\nCenter of Visual Technology, School\nof Computer Science, Peking\nUniversity\nBeijing, China\nzmhuang@pku.edu.cn\nYuhua Li†\nSchool of Computer Science and\nTechnology, Huazhong University of\nScience and Technology\nWuhan, China\nidcliyuhua@hust.edu.cn\nRuixuan Li\nSchool of Computer Science and\nTechnology, Huazhong University of\nScience and Technology\nWuhan, China\nrxli@hust.edu.cn\nABSTRACT\nHumans exhibit a remarkable ability to learn quickly from a limited\nnumber of labeled samples, a capability that starkly contrasts with\nthat of current machine learning systems. Unsupervised Few-Shot\nLearning (U-FSL) seeks to bridge this divide by reducing reliance\non annotated datasets during initial training phases. In this work,\nwe first quantitatively assess the impacts of Masked Image Mod-\neling (MIM) and Contrastive Learning (CL) on few-shot learning\ntasks. Our findings highlight the respective limitations of MIM and\nCL in terms of discriminative and generalization abilities, which\ncontribute to their underperformance in U-FSL contexts. To ad-\ndress these trade-offs between generalization and discriminability\nin unsupervised pretraining, we introduce a novel paradigm named\nMasked Image Contrastive Modeling (MICM). MICM creatively\ncombines the targeted object learning strength of CL with the gen-\neralized visual feature learning capability of MIM, significantly\nenhancing its efficacy in downstream few-shot learning inference.\nExtensive experimental analyses confirm the advantages of MICM,\ndemonstrating significant improvements in both generalization and\ndiscrimination capabilities for few-shot learning. Our comprehen-\nsive quantitative evaluations further substantiate the superiority\nof MICM, showing that our two-stage U-FSL framework based on\nMICM markedly outperforms existing leading baselines. The reposi-\ntory of this project is available at https://github.com/iCGY96/MICM.\n∗Equal Contribution.\n†Corresponding authors.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nACM MM, 2024, Melbourne, Australia\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/10.1145/3664647.3680647\nCCS CONCEPTS\n• Computing methodologies →Computer vision.\nKEYWORDS\nUnsupervised Few-shot Learning, Contrastive Learning, Masked\nImage Modeling, Masked Image Contrastive Modeling\nACM Reference Format:\nZhenyu Zhang, Guangyao Chen∗, Yixiong Zou, Zhimeng Huang, Yuhua\nLi†, and Ruixuan Li. 2024. MICM: Rethinking Unsupervised Pretraining for\nEnhanced Few-shot Learning. In Proceedings of the 32nd ACM International\nConference on Multimedia (MM’24), October 28-November 1, 2024, Melbourne,\nAustralia. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/\n3664647.3680647\n1\nINTRODUCTION\nAchieving high-level performance in deep representation learning\ntypically requires large datasets, detailed labeling processes, and sig-\nnificant supervisory involvement. This requirement becomes even\nmore daunting as the complexity of downstream tasks increases,\nchallenging the scalability of supervised representation learning\nmethods. In contrast, human learning is remarkably efficient, man-\naging to acquire new skills from minimal examples with little super-\nvision. Few-shot learning (FSL) [35, 42, 43, 48, 50, 51, 58, 62] aims to\nnarrow this gap between human and machine learning capabilities.\nAlthough FSL has shown promising results in supervised settings,\nits dependence on extensive supervision remains a significant limi-\ntation. To address this, unsupervised FSL (U-FSL) [11, 34, 35] has\nbeen developed, mirroring the structure of supervised approaches.\nIt involves pretraining on a wide dataset of base classes and then\nquickly adapting to novel, few-shot tasks [8]. The primary goal of\nU-FSL pretraining is to develop a feature extractor that understands\nthe global structure of unlabeled data, and subsequently tailors the\nencoder for new tasks. The increasing interest in U-FSL reflects its\npracticality and alignment with self-supervised learning methods,\narXiv:2408.13385v1  [cs.CV]  23 Aug 2024\nACM MM, 2024, Melbourne, Australia\nZhenyu Zhang and Guangyao Chen, et al.\n(a) Image\n(b) SimCLR\n(c) MAE\n(d) MICM\nFigure 1: Illustrating the attention map with SimCLR [10]\n(CL), MAE [24] (MIM), MICM.\nemphasizing its potential to significantly enhance machine learning\nprocesses.\nRecent advancements in state-of-the-art (SOTA) methods have\nlargely employed Contrastive Learning (CL) [11, 34, 35, 56, 57],\nparticularly in transfer-learning scenarios. These methods have\nachieved impressive results across various benchmarks. As illus-\ntrated in Figure 2(a), the principle of contrastive representation\nlearning [10] involves drawing ’positive’ samples closer and push-\ning ’negative’ ones away in the representation space. This technique\nfocuses on specific objects within datasets, thus improving repre-\nsentational learning for image classification tasks, as depicted in\nFigure 1(b). Conversely, Masked Image Modeling (MIM) [13, 24]\n(Figure 2(b)) trains models to predict the original content of inten-\ntionally obscured image portions. This approach facilitates com-\nprehensive learning of features across all image patches, including\nperipheral ones, as shown in Figure 1(c). In our research, we quanti-\ntatively assessed the impacts of MIM and CL on FSL tasks, revealing\nthat while CL prepares models to prioritize features typical in training\ndatasets, potentially compromising reliability in novel categories, and\nMIM fosters a broad and generalized understanding of image features\nbut struggles to develop discriminative features crucial for accurately\ncategorizing new classes in few-shot scenarios.\nTo explore the trade-offs between generalization and discrim-\ninability in unsupervised pretraining, we introduce Masked Image\nContrastive Modeling (MICM), a novel method that ingeniously\ncombines essential aspects of both CL and MIM to boost down-\nstream inference performance. Illustrated in Figure 2(c), MICM\nutilizes an encoder-decoder architecture akin to MIM but integrates\na decoder that simultaneously enhances features and reconstructs\nimages. This method not only merges contrastive learning with\neffective pretext task designs but also adapts efficiently to down-\nstream task data. We further introduce a U-FSL framework with two\nphases: Unsupervised Pretraining and Few-Shot Learning. During\nUnsupervised Pretraining, MICM blends CL and MIM objectives in\na hybrid training strategy. In the Few-Shot Learning phase, MICM\nadapts to various few-shot strategies, both inductive and trans-\nductive. Extensive experimental analysis confirms the benefits of\nMICM, showing it significantly enhances both the generalization\nand discrimination capabilities of pre-trained models, achieving\ntop-tier performance on multiple U-FSL datasets.\nTo summarize, the main contributions of our paper are as follows:\n• We reveal the limitations of MIM and CL in terms of dis-\ncriminative and generalization abilities, respectively, to their\nconsequent underperformance in U-FSL contexts.\n• We propose Masked Image Contrastive Modeling (MICM),\na novel structure that blends the targeted object learning\nprowess of CL with the generalized visual feature learning\ncapability of MIM.\n• Extensive quantitative and qualitative results show that our\nmethod achieves SOTA performance on several In-Domain\nand Cross-Domain FSL datasets.\n2\nRELATED WORK\n2.1\nFew-Shot Learning\nFew-shot learning (FSL) in visual classification contends with the\nchallenge of recognizing objects from very limited samples. Primar-\nily, this task is approached through two principal methodologies:\ntransfer learning and meta-learning. Transfer learning, as discussed\nby Tian et al. [43], leverages knowledge from models pre-trained\non large datasets to adapt to new, less-represented tasks. Meta-\nlearning, alternatively, encompasses several strategies: model-based\n[6], metric-based [41], and optimization-based [1]. Model-based ap-\nproaches focus on adapting the model parameters rapidly for new\ntasks. Metric-based methods compute distances between samples\nto facilitate class differentiation, while optimization-based strate-\ngies aim to maximize learning efficiency with scarce examples.\nBuilding upon inductive FSL, transductive FSL seeks to enhance\nreal-world application by incorporating unlabeled data into the\nlearning process for pre-classification tuning. Among the various\ntechniques employed, graph-based methods such as protoLP [61]\nutilize graph structures to strengthen the information flow and re-\nlationships between support and query samples. Clustering-based\napproaches, exemplified by EASY [4], Transductive CNAPS [3], and\nBAVARDAGE [28], refine feature representations using advanced\nclustering techniques. Additionally, applications of the Optimal\nTransport Algorithm, like BECLR [35], are used to align feature\ndistributions more effectively during testing phases. A novel ap-\nproach, TRIDENT [40], integrates a unique variational inference\nnetwork to enhance image representation in FSL scenarios.\n2.2\nUnsupervised Few-Shot Learning\nU-FSL broadens the scope of unsupervised learning by requiring\nmodels to not only learn data representations without supervision\nbut also to rapidly adapt to new few-shot tasks. This challenging\ndual requirement has led to the exploration of various innovative\nmethods. Knowledge distillation and contrastive learning are no-\ntably effective in U-FSL, enhancing model adaptability through\nrobust feature representations [29]. Furthermore, clustering-based\napproaches have demonstrated considerable success by optimizing\ndata groupings to better model task-specific nuances [33]. Despite\nthese advancements, many traditional unsupervised methods are\ngeared towards batch data processing, which may not seamlessly\ntranslate to the dynamic requirements of few-shot scenarios. To\nmitigate this, some strategies integrate meta-learning principles to\ngenerate synthetic training scenarios that improve data efficiency\nand model responsiveness [2]. However, such approaches can some-\ntimes lead to suboptimal data usage [20]. Recent innovations in\nU-FSL also include the use of graph-based structures to map re-\nlationships within data [9], the application of Structural Causal\nModels (SCM) in the context-aware multi-variational autoencoder\nMICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning\nACM MM, 2024, Melbourne, Australia\nFigure 2: (a) Contrastive Learning is dedicated to learning discriminative data representations by contrasting and differentiating\nbetween similar (positive) and dissimilar (negative) pairs of data samples. This approach emphasizes the relative comparison to\nachieve distinctiveness in the learned features. (b) Masked Image Modeling involves training a model to accurately predict the\noriginal content of intentionally obscured (masked) portions of images. This technique focuses on learning comprehensive\nand robust feature representations by encouraging the model to infer missing information. (c) Masked Image Contrastive\nModeling implements a decoder that simultaneously enhances features and reconstructs the original content of images. This\nmethod synergistically merges the principles of contrastive representation learning with effective pretext task design, thereby\nintegrating the strengths of both approaches to achieve more nuanced and effective learning.\n(CMVAE) [36], and the deployment of variational autoencoders\n(VAE) in frameworks like CMVAE and Meta-GMVAE [31]. Addi-\ntionally, the exploration of rotation invariance in self-supervised\nlearning enriches the robustness of models against geometric vari-\nations in few-shot learning tasks [51]. Another notable approach is\nMlSo, which leverages multi-level visual abstraction features com-\nbined with power-normalized second-order base learner streams\nto enhance the discriminative capability of models in U-FSL [55].\n3\nQUALITATIVE STUDY ON UNSUPERVISED\nPRETRAINING METHODS FOR U-FSL\nThis section delves into Unsupervised Few-Shot Learning (U-FSL),\nelucidating the task and assessing the impact of different unsuper-\nvised pretraining methodologies on model performance.\n3.1\nUnsupervised Few-shot Learning\nU-FSL operates under a widely recognized protocol delineated in\nrecent work [11, 29, 34, 35]. Initially, models undergo an unsuper-\nvised pretraining phase using a vast unlabeled dataset 𝐷base = {𝒙𝒊},\nwhich encompasses a variety of base classes. Subsequently, the mod-\nels are tested in a few-shot inference phase using a smaller, labeled\ntest dataset 𝐷novel = {(𝒙𝒊,𝒚𝒊)} comprising novel classes, ensuring\nno overlap exists between the base and novel classes. Each few-shot\ntask T𝑖is structured around a support set S = {(𝒙S\n𝒊,𝒚S\n𝒊)}𝑁𝐾\n𝑖=1 , ad-\nhering to the (𝑁-way, 𝐾-shot) scheme, where 𝐾labeled examples\nfrom 𝑁distinct classes are chosen. The query set Q = {𝒙Q\n𝒋}𝑁𝑄\n𝑗=1 ,\ntypically unlabeled, comprises 𝑁𝑄samples (where 𝑄> 𝐾) and\nserves to evaluate the model’s adaptation to novel classes.\n3.2\nUnsupervised Pretraining Methods\nThe foundation of U-FSL is significantly influenced by the capa-\nbilities of unsupervised pretraining methods to discern intricate\npatterns within unlabeled datasets. This segment explores the ef-\nfects of two principal unsupervised pretraining strategies: Con-\ntrastive Learning (CL) and Masked Image Modeling (MIM). Key\nexemplars for these methods—SimCLR [10] for CL and MAE [24]\nfor MIM—were selected due to their prominence and efficacy. Both\nmethodologies were implemented using the Vision Transformer\nSmall (ViT-S) architecture on the MiniImageNet dataset. Our analyt-\nical focus is directed towards understanding how these pretraining\napproaches modify the model’s capability to transition effectively\nto novel tasks. We postulate that the intrinsic nature of the pretrain-\ning method—contrastive, which underscores learning distinctive\nfeatures that delineate classes, versus masked, which centers on re-\nconstructing absent segments of the input—will manifest differing\nstrengths within the context of few-shot learning.\nAnalysis of Contrastive Learning. Recent research [13] has\ndemonstrated that CL models tend to focus predominantly on the\nprimary objects within images during pretraining. This concentra-\ntion frequently targets small, distinct features that distinctly char-\nacterize the primary categories. Such specificity, while beneficial\nfor initial categorization tasks, adversely affects the generalizability\nof the learned features to new, unseen contexts, as evidenced in\nFigure 1(b). We propose that this limitation arises from the inher-\nent design of CL methodologies, which predispose the model to\nemphasize features that are salient in the training dataset but may\nbe less relevant or even misleading in novel categories. To explore\nthis proposition, we conducted empirical analyses comparing the\nfeature representations of base category prototypes with those of\nnovel category prototypes, both before and after applying SimCLR\nand its subsequent fine-tuning. The results, depicted in Figure 3,\nACM MM, 2024, Melbourne, Australia\nZhenyu Zhang and Guangyao Chen, et al.\nFigure 3: Histograms depicting the distribution of similar-\nity between features extracted by the model for novel and\nbase classes. Our model (blue) extracts distinctive features\nfor novel classes, contrasting with the SimCLR model from\ncontrastive learning methods, which continues to focus on\ndiscriminative features of base classes for novel classes, re-\nsulting in similar features (brown). Fine-tuning with an ade-\nquate number of labeled samples is essential to address these\nissues in SimCLR and enhance classification accuracy.\nindicate that the features extracted for novel categories by SimCLR\nclosely mirror those associated with the base categories. This sim-\nilarity persists when models trained via CL are applied to novel\ncategories, leading to a continued reliance on the same features\nidentified during the training phase. Consequently, there is a no-\ntable deficiency in the model’s focus on principal objects in new\ncategories, which hampers its adaptability. Building on established\nprotocols [10, 16], we fine-tuned the SimCLR-trained models on\ndownstream few-shot classification tasks. Our findings, illustrated\nin Figure 3, show that fine-tuning with a minimal set of labeled\nexamples (e.g., in one-shot learning scenarios) fails to adequately\nrectify these issues of transferability. In some instances, this ap-\nproach may even degrade the model’s performance on few-shot\nclassification tasks. It becomes apparent that only with an ample\nnumber of labeled samples for fine-tuning do these challenges begin\nto mitigate, consequently improving accuracy in few-shot classifi-\ncation scenarios. This highlights the intrinsic difficulties of directly\napplying CL models to few-shot learning tasks, especially consider-\ning the heightened risk of overfitting when training data are scarce.\nHence, we assert the following conclusion concerning the impact\nof contrastive learning:\nConclusion and Discussion. Contrastive learning fundamentally\npredisposes models to prioritize features that are prominent in the\ntraining dataset, potentially at the expense of relevance and utility\nin novel categories.\nAnalysis of Masked Image Modeling. Recent advancements in\nMIM underscore a shift towards a more comprehensive feature\nextraction methodology, distinct from techniques that prioritize\nprominent image features. As detailed in [13], MIM methods like the\nMAE engage systematically with every image patch to reconstruct\nabsent segments, fostering a broad spatial activation across the\nentire image. This approach is vividly illustrated in Figure 1(c),\nwhere feature maps from MAE reveal a widespread distribution\n(a) SimCLR (73.33)\n(b) MAE (38.66)\n(c) MICM (77.33)\n(d) SimCLR 5-shot (70.66)\n(e) MAE 5-shot (41.33)\n(f) MICM 5-shot (78.66)\nFigure 4: Visualization of t-SNE features for various unsuper-\nvised pre-trained models in novel categories. The first row:\nbefore fine-tuning. The second row: after fine-tuning with 5-\nshot. Contrary to SimCLR and MICM, the MAE method lacks\ndiscriminative features both prior to and following few-shot\nlearning. The performance of each model in FSL is indicated\nin parentheses.\nof activation, suggesting a more holistic grasp of image features.\nDespite these strengths, the extensive focus on patch reconstruction\nin MIM can obscure class-specific feature delineation. Since MIM\nmodels are optimized for predicting missing image parts rather\nthan distinguishing class features, they frequently lack the sharp,\ndiscriminative capabilities essential for class-specific recognition\ntasks. This deficiency is apparent in Figure 4, where the first row\ndemonstrates that features extracted by MAE are markedly less\ndiscriminative than those derived from our proposed method or the\ncontrastive learning approach, SimCLR. The adaptability of MIM\ntechniques to FSL scenarios is also challenged when these models\nare fine-tuned with limited labeled data. The second row of Figure 4\nindicates that, even after fine-tuning with a modest sample size,\nsuch as in a 5-shot scenario, the discriminability of the features\nshows minimal enhancement. This observation implies that while\nMIM effectively encodes a rich array of generic visual features, it\nstruggles to capture the subtleties required for distinguishing novel\nclasses in few-shot configurations.\nConclusion and Discussion. While MIM techniques cultivate\na broad and generalized understanding of image features, they\nencounter significant obstacles in acquiring discriminative features\ncrucial for accurately categorizing novel classes in few-shot learning\nscenarios.\n4\nMASKED IMAGE CONTRASTIVE MODELING\nThe preceding comparative analysis in Section 3 elucidates that\nwhile CL prioritizes refining focus on distinct objects within datasets,\nenhancing representational efficacy for image classification, MIM\nextends its reach to a comprehensive understanding across all image\npatches, thus facilitating a broader scope of feature extraction. This\nMICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning\nACM MM, 2024, Melbourne, Australia\ndelineation underscores a pivotal trade-off in unsupervised pretrain-\ning between generalization and discriminability. To bridge this gap,\nwe introduce a novel approach, Masked Image Contrastive Mod-\neling (MICM), which amalgamates the strengths of both method-\nologies to foster robust representation learning coupled with an\neffective FSL task.\n4.1\nModel Structure\nFigure 2 illustrates the encoder-decoder architecture of MICM, in-\ngeniously designed to predict masked patches based on visible\nones within the encoded space, while concurrently ensuring similar\ntokens for identical images are decoded effectively. The image un-\ndergoes segmentation into visible patches X𝑣and masked patches\nX𝑚. The encoder F processes X𝑣to generate latent representa-\ntions Z𝑣, while the decoder G aims to reconstruct X𝑚using these\nencoded representations along with a predefined class token T𝑐.\nEncoder. The encoder F transforms visible patches X𝑣into latent\nrepresentations Z𝑣. Utilizing the ViT as its backbone, the encoder\nbegins by embedding the visible patches, projecting each patch\nlinearly to create a set of embeddings. Positional embeddings P𝑣\nare added to maintain spatial context. These embeddings undergo\nprocessing through several transformer blocks, leveraging self-\nattention mechanisms to produce the latent representations Z𝑣,\nwhich encapsulate the critical features of the visible patches.\nDecoder. The decoder serves dual functions in MICM. Its primary\nrole is to transform the latent representations of visible patches\nZ𝑣and, crucially, those of masked patches Z𝑚back into the recon-\nstructed patches Y𝑚. This transformation process entails a sequence\nof transformer blocks culminating in a linear layer that precisely\nregenerates the original masked patches. Secondly, the decoder also\nrefines the input class token Z[cls] into an enhanced representa-\ntion ˆZ[cls], pivotal for effective CL. Diverging from conventional\napproaches, MICM strategically delays the integration of the class\ntoken until the decoding phase, permitting the encoder to concen-\ntrate more thoroughly on capturing the nuances of visible patches.\nThis structural delineation enhances the encoder’s focus on extract-\ning a diverse array of visual features, while the decoder, through\nfeature reconstruction, fine-tunes the class token, synergistically\nbalancing the model’s objectives of maximizing discriminability\nand ensuring comprehensive feature extraction.\n4.2\nU-FSL with MICM\nUnsupervised Pretraining. Given an input image 𝒙uniformly\nsampled from the training set 𝐷base, we apply random data aug-\nmentations to create two distinct views 𝒙1 and 𝒙2. These views\nare subsequently processed by the teacher and student networks,\nparameterized by 𝜽𝒕and 𝜽𝒔, respectively. Notably, the teacher net-\nwork is updated via an Exponentially Moving Average (EMA) of\nthe student network, facilitating knowledge transfer by minimizing\nthe cross-entropy loss between the output categorical distributions\nof their augmented token representations, as expressed in the fol-\nlowing equation:\nL[cls] = H ( ˆZ𝑡\n[cls], ˆZ𝑠\n[cls]),\n(1)\nwhere H (𝑥,𝑦) = −𝑥log𝑦, and ˆZ[cls] denotes the output class\ntoken. For MIM, we implement self-distillation as proposed in [59].\nFigure 5: (a) The Unsupervised Pretraining phase involves\nself-supervised learning on a large, unlabeled base dataset,\ncrucial for developing initial representations. (b) The Few-\nShot Learning phase could adapt to a variety of few-shot\nlearning approaches, including both inductive and transduc-\ntive methodologies.\nA random mask sequence𝑚∈{0, 1}𝑀is applied over an image with\n𝑁[patch] tokens 𝒙= {𝒙𝑖}𝑀\n𝑖=1. The masked patches, denoted by𝑚𝑖=\n1, are replaced by a learnable token embedding Z𝑚, resulting in a\ncorrupted image b𝒙. The student and teacher networks receive the\ncorrupted and original uncorrupted images, respectively, to recover\nthe masked tokens. This is quantified by minimizing the cross-\nentropy loss between their categorical distributions on masked\npatches:\nL[patch] =\n𝑀\n∑︁\n𝑖=1\n𝑚𝑖· H ( ˆZ𝑡\n[patch i], ˆZ𝑠\n[patch i]).\n(2)\nMoreover, we aim for the decoder-generated tokens to predict the\nRGB information of the image, incorporating an image reconstruc-\ntion loss using Mean Squared Error (MSE) for the reconstruction\ntargets ¯Y:\nLMSE =\n∑︁\n(Y𝑚, ¯Y𝑚)2.\n(3)\nFew-shot Learning. During the few-shot learning phase, the\nMICM approach is adeptly configured to adapt to diverse few-shot\nlearning strategies, encompassing both inductive and transductive\nmethods. The MICM methodology enhances feature learning by em-\nphasizing generality across base classes and discriminative power.\nThis dual capability significantly boosts the transferability of the\nlearned features to few-shot tasks, thereby enabling superior adap-\ntation to scenarios with limited labeled data. Further exploration of\nthis adaptability is discussed in subsequent sections.\n5\nEXPERIMENTS\n5.1\nDatasets\nUnsupervised few-shot recognition experiments are conducted on\nthree benchmark datasets widely recognized in the field: MiniIma-\ngeNet [44], TieredImageNet [37], and CIFAR-FS [5]. MiniImageNet,\nACM MM, 2024, Melbourne, Australia\nZhenyu Zhang and Guangyao Chen, et al.\nTable 1: Accuracies (in % ± standard deviation) on miniIma-\ngeNet, comparing our model with various unsupervised pre-\ntraining methods (all models use VIT-S as backbone). CTB\ndenotes the strategy of inserting a classification cls token\nbefore the processing by the encoder.\nMethod\nSetting\nInductive (ProtoNet) [41]\nTransductive (OpTA) [35]\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\nSimCLR [10]\nCL\n54.30±0.62\n75.03±0.35\n65.83±0.64\n78.09±0.40\nMoCo V3 [25]\nCL\n56.06±0.43\n76.78±0.33\n71.45±0.64\n82.04±0.35\nMAE [24]\nMIM\n29.11±0.44\n37.01±0.31\n25.36±0.48\n35.21±0.42\nCAE [13]\nMIM\n57.33±0.46\n79.25±0.33\n70.34±0.67\n81.36±0.39\niBOT [59]\nMIM\n60.93±0.21\n80.38±0.16\n74.58±0.66\n83.95±0.34\nMICM w/ CTB\nMIM+CL\n62.85±0.17\n82.37±0.11\n77.89±0.62\n86.36±0.33\nMICM\nMIM+CL\n60.78±0.19\n81.39±0.14\n78.40±0.61\n86.90±0.33\nderived from the larger ILSVRC-12 dataset [38], consists of 100\ncategories, each represented by 600 images. It is divided into meta-\ntraining, meta-validation, and meta-testing segments, containing\n64, 16, and 20 categories, respectively. TieredImageNet, also a subset\nof ILSVRC-12, includes 608 categories segmented into 351, 97, and\n160 categories for training, validation, and testing. CIFAR-FS, a sub-\nset of CIFAR100 [30], follows a similar structure to MiniImageNet,\nwith 60,000 images spread across 100 categories. These datasets\nprovide a robust framework for evaluating few-shot learning algo-\nrithms. Additionally, cross-domain experiments use MiniImageNet\nas the pretraining (source) dataset and ISIC [17], EuroSAT [26], and\nCropDiseases [46] as inference (target) datasets, enhanci@ng the\ngeneralizability assessment of the models.\n5.2\nAnalysis of MICM\nDiscriminative and Generalization Capabilities. We investi-\ngate the critical balance between generalization and discriminability\nin unsupervised pretraining through our proposed MICM model. As\ndepicted in Figure 6, MICM surpasses other unsupervised pretrain-\ning methods in capturing comprehensive object information from\nnovel classes, exhibiting superior overall perception. This is further\ncorroborated by the distance distribution between the prototypes\nof novel and base classes in Figure 3, where MICM’s prototypes\ndistinctly differ more from the base class, affirming its enhanced\ngeneralization for novel classes. Additionally, Figure 4 demonstrates\nMICM’s superior discriminative capabilities compared to models\nlike MAE, with a clearer distinction in feature distributions between\nMICM and SimCLR. Notably, MICM maintains leading performance\nin small-sample scenarios, both pre- and post-fine-tuning. The ex-\nperimental validations highlight MICM’s adept integration of the\nstrengths of CL and MIM, achieving remarkable discriminability\nand generalization.\nImproving FSL. Our exploration focuses on the synergy between\nMIM and CL, designed to overcome the limitations inherent to each\napproach individually. The MICM model we introduce effectively\nintegrates the strengths of these methodologies, emphasizing the\nextraction of relevant feature scales within images. This integra-\ntion not only enhances category discrimination but also bolsters\nrobustness in subsequent FSL tasks. The effectiveness of MICM is\ndemonstrated through its superior performance in both inductive\nand transductive few-shot classification settings, detailed in Table 1.\nTable 2: Accuracies (in % ± std) on miniImageNet →CUB.,\ncomparing our model with various unsupervised pretraining\nmethods (all models use VIT-S as backbone). CTB denotes\nthe strategy of inserting a classification cls token before the\nprocessing by the encoder.\nMethod\nSetting\nInductive (ProtoNet) [41]\nTransductive (OpTA) [35]\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\nSimCLR [10]\nCL\n39.80±0.32\n55.72±0.37\n38.99±0.40\n53.09±0.42\nMoCo V3 [25]\nCL\n42.12±0.33\n59.33±0.37\n41.83±0.41\n57.36±0.42\nMAE [24]\nMIM\n30.13±0.25\n37.94±0.31\n25.37±0.25\n31.89±0.32\nCAE [13]\nMIM\n38.10±0.43\n51.56±0.53\n38.31±0.56\n49.01±0.58\niBOT [59]\nMIM\n42.71±0.33\n59.33±0.38\n43.30±0.43\n58.56±0.43\nMICM w/ CTB\nMIM+CL\n45.06±0.34\n62.83±0.37\n46.85±0.45\n62.75±0.42\nMICM\nMIM+CL\n44.95±0.34\n63.05±0.37\n47.42±0.46\n63.86±0.42\nTable 3: Accuracies (in % ± standard deviation) on miniIm-\nageNet, comparing our model with various unsupervised\npretraining methods, which are adapted to several FSL meth-\nods [12, 35, 41, 47, 52].\nPretrained Model\nFSL method\n5-way 1-shot\n5-way 5-shot\nMAE [24]\nProtoNet [41]\n28.88 ± 0.43\n37.19 ± 0.51\nSimCLR [10]\n54.42 ± 0.66\n75.03 ± 0.35\niBOT [59]\n61.26 ± 0.66\n80.64 ± 0.45\nMICM\n61.37 ± 0.62\n81.68 ± 0.43\nMAE [24]\nFine-tuning [12]\n28.50 ± 0.32\n38.29 ± 0.50\nSimCLR [10]\n54.47 ± 0.59\n75.01 ± 0.36\niBOT [59]\n61.11 ± 0.59\n80.91 ± 0.39\nMICM\n61.41 ± 0.52\n81.72 ± 0.32\nMAE [24]\nSimpleShot [47]\n30.30 ± 0.47\n38.65 ± 0.50\nSimCLR [10]\n57.13 ± 0.64\n74.88 ± 0.46\niBOT [59]\n61.98 ± 0.65\n80.56 ± 0.45\nMICM\n62.53 ± 0.63\n81.79 ± 0.43\nMAE [24]\nDC [52]\n37.06 ± 0.47\n52.95 ± 0.51\nSimCLR [10]\n60.86 ± 0.58\n75.79 ± 0.39\niBOT [59]\n65.84 ± 0.67\n83.77 ± 0.43\nMICM\n67.19 ± 0.65\n85.12 ± 0.41\nMAE [24]\nOpTA [35]\n25.36 ± 0.48\n35.21 ± 0.42\nSimCLR [10]\n65.83 ± 0.64\n78.09 ± 0.40\niBOT [59]\n74.58 ± 0.66\n83.95 ± 0.34\nMICM\n78.40 ± 0.61\n86.90 ± 0.33\nEnhancing Cross-Domain FSL. In cross-domain scenarios, MICM\nalso significantly excels, notably on the CUB dataset (Table 2). Un-\nlike traditional CL models, which often overfit to base classes, MICM\nmaintains generalization across varied domains without the need\nfor fine-tuning, as illustrated in Figures 3 and 4. This capability\nunderscores MICM’s effectiveness in capturing discernible features\nwithin an optimal range, thus boosting its adaptability and classifi-\ncation performance in few-shot learning across different domains.\nBroad Adaptation to FSL Methods. As a versatile pre-training\nmodel, MICM adapts seamlessly across a spectrum of FSL strate-\ngies. Comprehensive evaluations show that MICM invariably boosts\nperformance, with notable improvements such as a nearly 4% en-\nhancement over the iBOT model when utilizing the transductive\nMICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning\nACM MM, 2024, Melbourne, Australia\n(a) Image\n(b) SimCLR\n(c) MoCo v3\n(d) CAE\n(e) iBOT\n(f) MAE\n(g) MICM\nFigure 6: Attention map visualization for different unsupervised pre-trained models. This figure presents a series of columns,\neach corresponding to the attention map output from a distinct model. From left to right, the columns are as follows: (a) the\noriginal image; (b) SimCLR [10]; (c) Moco v3 [16]; (d) CAE [13]; (e) iBOT [60]; (f) MAE [24]; and (g) MICM (ours).\nmethod OpTA. These results affirm the robust generalization ability\nof MICM across a range of FSL approaches.\ncls token Variation. In exploring variations, we introduced a cls to-\nken as an input to the Encoder, with performance outcomes detailed\nin Tables 1 and 2. Although this variant achieves commendable\nresults in the inductive setting, it does not outperform the configu-\nration where the cls token is input into the Decoder, especially in\ntransductive scenarios. This suggests that introducing the cls token\nearly in the encoder may impede the encoder’s ability to learn com-\nprehensive visual features effectively. Conversely, positioning the\ncls token in the decoder helps alleviate potential negative impacts\nby CL on learning holistic visual features.\n5.3\nComparison with SOTA Method\nTo assess the efficacy of MICM in FSL scenarios, particularly under\nthe U-FSL framework, we developed and evaluated a novel method-\nology that combines unsupervised pre-training with pseudo-label\ntraining techniques. We integrated pseudo-label learning [35] with\nthe transductive OpTA FSL method [35], forming a hybrid approach\ndesigned to leverage the combined strengths of these methods\nto boost performance in scenarios with scarce labeled data. Our\nmethod’s performance was benchmarked against SOTA models\nacross various datasets, with detailed methodological descriptions\nprovided in the Appendix.\nIn-Domain Setting. Our model was evaluated against a broad\nrange of baselines including established SSL baselines [7, 10, 15,\n21, 22, 54], prominent U-FSL methods [9, 11, 27, 29, 34, 39], lead-\ning supervised FSL approaches [3, 32], and a recent transductive\nU-FSL model [35]. Our model demonstrates superior performance,\noutperforming both inductive and transductive U-FSL methods as\nevidenced in Tables 4 and 5, showing a notable accuracy improve-\nment on the CIFAR-FS dataset.\nOur implementation utilizes the ViT architecture, which con-\ntrasts with the commonly used ResNet in U-FSL studies. To facilitate\ncomprehensive evaluation, we compared results from models using\nboth ResNet18 and ResNet50 architectures, and additionally, we\nbenchmarked against a ViT-S model trained using the MIM method\nfor transductive classification (MIM+OpTA), providing a baseline\nfor ViT-based transductive U-FSL models.\nRegarding CIFAR-FS performance comparisons (Table 5), sourced\nfrom [35], we note that the BECLR model reported results using\nResNet18. Hence, our model’s reported performance is achieved\nwith a scaled-down version of ViT-S, comprising 6 layers (4 encoder\nlayers and 2 decoder layers) as opposed to the full 12 layers in\nstandard ViT-S.\nCross-Domain Setting. Following established methodologies [23,\n35], we pretrained on the miniImageNet dataset and evaluated\nour approach in cross-domain few-shot learning settings. The re-\nsults, detailed in Table 6, demonstrate that MICM sets new SOTA\nACM MM, 2024, Melbourne, Australia\nZhenyu Zhang and Guangyao Chen, et al.\nTable 4: Accuracies (in % ± standard deviation) on miniImageNet and tieredImageNet, comparing our model with various\nbaselines categorized into Inductive (Ind.) and Transductive (Transd.) approaches. Performance is delineated by backbone\narchitectures, namely Residual Networks (RN) and Vision Transformers (ViT), with the number of parameters (Param) for\neach model included for an extensive comparison.\nMethod\nBackbone\nParam\nSetting\nminiImageNet\ntieredImageNet\n5-way 1-shot\n5-way 5-shot\n5-way 1-shot\n5-way 5-shot\nSwAV [7]\nRN18 (×1)\n11.2M\nInd.\n59.84 ± 0.52\n78.23 ± 0.26\n65.26 ± 0.53\n81.73 ± 0.24\nNNCLR [21]\nRN18 (×2)\n22.4M\nInd.\n63.33 ± 0.53\n80.75 ± 0.25\n65.46 ± 0.55\n81.40 ± 0.27\nCPNWCP [45]\nRN18 (×1)\n11.2M\nInd.\n53.14 ± 0.62\n67.36 ± 0.5\n45.46 ± 0.19\n62.96 ± 0.19\nHMS [53]\nRN18 (×1)\n11.2M\nInd.\n58.20 ± 0.23\n75.77 ± 0.16\n58.42 ± 0.25\n75.85 ± 0.18\nSAMPTransfer [39]\nRN18 (×1)\n11.2M\nInd.\n45.75 ± 0.77\n68.33 ± 0.66\n42.32 ± 0.75\n53.45 ± 0.76\nPsCo [29]\nRN18 (×1)\n11.2M\nInd.\n47.24 ± 0.76\n65.48 ± 0.68\n54.33 ± 0.54\n69.73 ± 0.49\nPDA-Net [11]\nRN50 (×1)\n23.5M\nInd.\n63.84 ± 0.91\n83.11 ± 0.56\n69.01 ± 0.93\n84.20 ± 0.69\nUniSiam + dist [34]\nRN50 (×1)\n23.5M\nInd.\n65.33 ± 0.36\n83.22 ± 0.24\n69.60 ± 0.38\n86.51 ± 0.26\nMeta-DM + UniSiam + dist [27]\nRN50 (×1)\n23.5M\nInd.\n66.68 ± 0.36\n85.29 ± 0.23\n69.61 ± 0.38\n86.53 ± 0.26\nCPNWCP + OpTA [45]\nRN18 (×1)\n11.2M\nTransd.\n60.45 ± 0.81\n75.84 ± 0.56\n55.05 ± 0.31\n72.91 ± 0.26\nHMS + OpTA [53]\nRN18 (×1)\n11.2M\nTransd.\n69.85 ± 0.42\n80.77 ± 0.35\n71.75 ± 0.43\n81.32 ± 0.34\nPsCo + OpTA [29]\nRN18 (×1)\n11.2M\nTransd.\n52.89 ± 0.71\n67.42 ± 0.54\n57.46 ± 0.59\n70.70 ± 0.45\nUniSiam + OpTA [34]\nRN18 (×1)\n11.2M\nTransd.\n72.54 ± 0.61\n82.46 ± 0.32\n73.37 ± 0.64\n73.37 ± 0.64\nBECLR [35]\nRN18 (×2)\n22.4M\nTransd.\n75.74 ± 0.62\n84.93 ± 0.33\n76.44 ± 0.66\n84.85 ± 0.37\nBECLR [35]\nRN50 (×2)\n47M\nTransd.\n80.57 ± 0.57\n87.82 ± 0.29\n81.69 ± 0.61\n87.82 ± 0.32\nMICM\nVIT-S (×2)\n42M\nTransd.\n81.05 ± 0.58\n87.95 ± 0.34\n83.30 ± 0.61\n89.61 ± 0.35\nTable 5: Accuracies (in % ± std) for CIFAR-FS dataset.\nMethod\n5-way 1-shot\n5-way 5-shot\nSimCLR [10]\n54.56±0.19\n71.19±0.18\nMoCo v2 [14]\n52.73±0.20\n67.81±0.19\nLF2CS [33]\n55.04±0.72\n70.62±0.57\nHMS [53]\n54.65±0.20\n73.70±0.18\nBECLR [35]\n70.39±0.62\n81.56±0.39\nMICM\n79.20±0.61\n86.35±0.39\nTable 6: 5-way 5-shots accuracies (in % ± std) on miniImageNet\n→Cross-Domain Few-Shot Learning.\nMethod\nChestX\nISIC\nEuroSAT\nCropDiseases\nMean\nSwAV [7]\n25.70±0.28\n40.69±0.34\n84.82±0.24\n88.64±0.26\n60.12\nNNCLR [21]\n25.74±0.41\n38.85±0.56\n83.45±0.53\n90.76±0.57\n59.70\nSAMPTransfer [39]\n26.27±0.44\n47.60±0.59\n85.55±0.60\n91.74±0.55\n62.79\nPsCo [29]\n24.78±0.23\n44.00±0.30\n81.08±0.35\n88.24±0.31\n59.52\nUniSiam + dist [34]\n28.18±0.45\n45.65±0.58\n86.53±0.47\n92.05±0.50\n63.10\nConFeSS [19]\n27.09\n48.85\n84.65\n88.88\n62.36\nBECLR [35]\n28.46±0.23\n44.48±0.31\n88.55±0.23\n93.65±0.25\n63.78\nMICM\n27.11±0.36\n46.85±0.52\n90.08±0.36\n94.61±0.27\n64.66\nbenchmarks on the EuroSAT and Crop Diseases datasets, while\nmaintaining competitive performance on the ISIC dataset. MICM’s\nadaptive training mechanism enables superior performance over\nBECLR in cross-domain settings, highlighting its robustness and\nadaptability across diverse datasets.\nFeature Distribution Analysis. To deepen our understanding of\nthe MICM mechanism, we employed iBOT [59] as a baseline for\n(a) iBOT [59]\n(b) MICM\nFigure 7: Feature distribution maps comparing various meth-\nods before and after applying OpTA.\ncomparative analysis. A critical observation, illustrated in Figure 7,\nis that MICM significantly enhances the compactness and cohesion\nof feature distributions within the same category. Compared to the\nbaseline, where feature clusters are dispersed and misaligned (as\nshown in Figure 7(a)), our model demonstrates a notably tighter\nMICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning\nACM MM, 2024, Melbourne, Australia\nclustering. This improvement is especially evident in the align-\nment of support samples with the corresponding query samples\nwithin each category. The application of the OpTA method no-\ntably rectifies sample bias, further refining feature distribution, and\nalignment. This adjustment, combined with the advanced feature\nrepresentation capabilities of our MICM model, yields a substantial\nenhancement in performance relative to the baseline. The precise\nclustering of category-specific features and the effective mitigation\nof sample bias by OpTA underline the robustness and effectiveness\nof our model in generating highly discriminative feature represen-\ntations, which is pivotal for few-shot learning applications.\n6\nCONCLUSION\nIn this paper, we have delineated the limitations of Masked Im-\nage Modeling (MIM) and Contrastive Learning (CL) in terms of\ntheir discriminative and generalization capabilities, which have\ncontributed to their underperformance in Unsupervised Few-Shot\nLearning (U-FSL) contexts. To tackle these challenges, we intro-\nduced Masked Image Contrastive Modeling (MICM), a novel ap-\nproach that effectively integrates the strengths of MIM and CL. Our\nresults demonstrate that MICM adeptly balances discriminative\npower with generalizability, particularly in few-shot learning sce-\nnarios characterized by limited sample sizes. MICM’s flexibility in\nadapting to various few-shot learning strategies highlights its po-\ntential as a versatile and powerful tool for unsupervised pretraining\nwithin the U-FSL framework. Extensive quantitative and qualitative\nevaluations show MICM’s clear superiority over existing methods,\nconfirming its ability to enhance feature discrimination, robustness,\nand adaptability across diverse few-shot learning tasks.\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation\nof China under grants 62206102, 62376103, 62302184, U1936108,\n62025101, and 62088102; the Science and Technology Support Pro-\ngram of Hubei Province under grant 2022BAA046; the Postdoc-\ntoral Fellowship Program of CPSF under grants GZB20230024 and\nGZC20240035; and the China Postdoctoral Science Foundation un-\nder grant 2024M750100.\nREFERENCES\n[1] Antreas Antoniou, Harrison Edwards, and Amos Storkey. 2018. How to train\nyour MAML. arXiv preprint arXiv:1810.09502 (2018).\n[2] Antreas Antoniou and Amos Storkey. 2019. Assume, augment and learn: Un-\nsupervised few-shot meta-learning via random labels and data augmentation.\narXiv preprint arXiv:1902.09884 (2019).\n[3] Peyman Bateni, Jarred Barber, Jan-Willem Van de Meent, and Frank Wood. 2022.\nEnhancing few-shot image classification with unlabelled examples. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of Computer Vision. 2796–2805.\n[4] Yassir Bendou, Yuqing Hu, Raphael Lafargue, Giulia Lioi, Bastien Pasdeloup,\nStéphane Pateux, and Vincent Gripon. 2022. Easy—ensemble augmented-shot-y-\nshaped learning: State-of-the-art few-shot classification with simple components.\nJournal of Imaging 8, 7 (2022), 179.\n[5] Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. 2018. Meta-\nlearning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136\n(2018).\n[6] Qi Cai, Yingwei Pan, Ting Yao, Chenggang Yan, and Tao Mei. 2018. Memory\nmatching networks for one-shot image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 4080–4088.\n[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and\nArmand Joulin. 2020. Unsupervised learning of visual features by contrasting\ncluster assignments. Advances in neural information processing systems 33 (2020),\n9912–9924.\n[8] Guangyao Chen, Peixi Peng, Yangru Huang, Mengyue Geng, and Yonghong\nTian. 2024. Adaptive Discovering and Merging for Incremental Novel Class\nDiscovery. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38.\n11276–11284.\n[9] Kuilin Chen and Chi-Guhn Lee. 2022. Unsupervised Few-shot Learning via Deep\nLaplacian Eigenmaps. arXiv preprint arXiv:2210.03595 (2022).\n[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A\nsimple framework for contrastive learning of visual representations. In Interna-\ntional conference on machine learning. PMLR, 1597–1607.\n[11] Wentao Chen, Chenyang Si, Wei Wang, Liang Wang, Zilei Wang, and Tieniu Tan.\n2021. Few-shot learning with part discovery and augmentation from unlabeled\nimages. arXiv preprint arXiv:2105.11874 (2021).\n[12] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-\nBin Huang. 2019.\nA closer look at few-shot classification.\narXiv preprint\narXiv:1904.04232 (2019).\n[13] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao\nWang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. 2023. Context\nautoencoder for self-supervised representation learning. International Journal of\nComputer Vision (2023), 1–16.\n[14] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. 2020. Improved baselines\nwith momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020).\n[15] Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation\nlearning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition. 15750–15758.\n[16] X Chen, S Xie, and K He. [n. d.]. An empirical study of training self-supervised\nvision transformers. In 2021 IEEE. In CVF International Conference on Computer\nVision (ICCV). 9620–9629.\n[17] Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen\nDusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael\nMarchetti, et al. 2019. Skin lesion analysis toward melanoma detection 2018: A\nchallenge hosted by the international skin imaging collaboration (isic). arXiv\npreprint arXiv:1902.03368 (2019).\n[18] Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation of optimal\ntransport. Advances in neural information processing systems 26 (2013).\n[19] Debasmit Das, Sungrack Yun, and Fatih Porikli. 2021. ConfeSS: A framework\nfor single source cross-domain few-shot learning. In International Conference on\nLearning Representations.\n[20] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto.\n2019. A baseline for few-shot image classification. arXiv preprint arXiv:1909.02729\n(2019).\n[21] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew\nZisserman. 2021. With a little help from my friends: Nearest-neighbor contrastive\nlearning of visual representations. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 9588–9597.\n[22] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan\nGuo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new\napproach to self-supervised learning. Advances in neural information processing\nsystems 33 (2020), 21271–21284.\n[23] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith,\nKate Saenko, Tajana Rosing, and Rogerio Feris. 2020. A broader study of cross-\ndomain few-shot learning. In Computer Vision–ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16. Springer,\n124–141.\nACM MM, 2024, Melbourne, Australia\nZhenyu Zhang and Guangyao Chen, et al.\n[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. 16000–16009.\n[25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-\nmentum contrast for unsupervised visual representation learning. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition. 9729–9738.\n[26] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019.\nEurosat: A novel dataset and deep learning benchmark for land use and land\ncover classification. IEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing 12, 7 (2019), 2217–2226.\n[27] Wentao Hu, Xiurong Jiang, Jiarun Liu, Yuqi Yang, and Hui Tian. 2023. Meta-\nDM: Applications of Diffusion Models on Few-Shot Learning. arXiv preprint\narXiv:2305.08092 (2023).\n[28] Yuqing Hu, Stéphane Pateux, and Vincent Gripon. 2023. Adaptive Dimension\nReduction and Variational Inference for Transductive Few-Shot Classification. In\nInternational Conference on Artificial Intelligence and Statistics. PMLR, 5899–5917.\n[29] Huiwon Jang, Hankook Lee, and Jinwoo Shin. 2023.\nUnsupervised Meta-\nlearning via Few-shot Pseudo-supervised Contrastive Learning. arXiv preprint\narXiv:2303.00996 (2023).\n[30] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2010. Cifar-10 (canadian\ninstitute for advanced research). URL http://www. cs. toronto. edu/kriz/cifar. html\n5, 4 (2010), 1.\n[31] Dong Bok Lee. 2021. Meta-GMVAE: Mixture of Gaussian VAEs for unsupervised\nmeta-learning. (2021).\n[32] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. 2019.\nMeta-learning with differentiable convex optimization. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. 10657–10665.\n[33] Shuo Li, Fang Liu, Zehua Hao, Kaibo Zhao, and Licheng Jiao. 2022. Unsuper-\nvised few-shot image classification by learning features into clustering space. In\nEuropean Conference on Computer Vision. Springer, 420–436.\n[34] Yuning Lu, Liangjian Wen, Jianzhuang Liu, Yajing Liu, and Xinmei Tian. 2022.\nSelf-supervision can be a good few-shot learner. In European Conference on\nComputer Vision. Springer, 740–758.\n[35] Stelios Poulakakis Daktylidis. 2023. BECLR: Batch Enhanced Contrastive Unsu-\npervised Few-Shot Learning. (2023).\n[36] Guodong Qi and Huimin Yu. 2023. CMVAE: Causal Meta VAE for Unsupervised\nMeta-Learning. arXiv preprint arXiv:2302.09731 (2023).\n[37] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B\nTenenbaum, Hugo Larochelle, and Richard S Zemel. 2018. Meta-learning for\nsemi-supervised few-shot classification. arXiv preprint arXiv:1803.00676 (2018).\n[38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.\n2015. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision 115 (2015), 211–252.\n[39] Ojas Kishorkumar Shirekar, Anuj Singh, and Hadi Jamali-Rad. 2023.\nSelf-\nAttention Message Passing for Contrastive Few-Shot Learning. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision. 5426–5436.\n[40] Anuj Singh and Hadi Jamali-Rad. 2022. Transductive decoupled variational\ninference for few-shot classification. arXiv preprint arXiv:2208.10559 (2022).\n[41] Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for\nfew-shot learning. Advances in neural information processing systems 30 (2017).\n[42] Zeyin Song, Yifan Zhao, Yujun Shi, Peixi Peng, Li Yuan, and Yonghong Tian.\n2023. Learning with fantasy: Semantic-aware virtual contrastive constraint for\nfew-shot class-incremental learning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. 24183–24192.\n[43] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip\nIsola. 2020. Rethinking few-shot image classification: a good embedding is all\nyou need?. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XIV 16. Springer, 266–282.\n[44] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016.\nMatching networks for one shot learning. Advances in neural information pro-\ncessing systems 29 (2016).\n[45] Haoqing Wang and Zhi-Hong Deng. 2022. Contrastive prototypical network\nwith Wasserstein confidence penalty. In European Conference on Computer Vision.\nSpringer, 665–682.\n[46] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and\nRonald M Summers. 2017. Chestx-ray8: Hospital-scale chest x-ray database and\nbenchmarks on weakly-supervised classification and localization of common\nthorax diseases. In Proceedings of the IEEE conference on computer vision and\npattern recognition. 2097–2106.\n[47] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens Van Der Maaten.\n2019. Simpleshot: Revisiting nearest-neighbor classification for few-shot learning.\narXiv preprint arXiv:1911.04623 (2019).\n[48] Zeyuan Wang, Yifan Zhao, Jia Li, and Yonghong Tian. 2020. Cooperative bi-\npath metric for few-shot learning. In Proceedings of the 28th ACM international\nconference on multimedia. 1524–1532.\n[49] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff,\nSerge Belongie, and Pietro Perona. 2010. Caltech-UCSD birds 200. (2010).\n[50] Heng Wu, Yifan Zhao, and Jia Li. 2021. Selective, structural, subtle: Trilinear\nspatial-awareness for few-shot fine-grained visual recognition. In 2021 IEEE\nInternational Conference on Multimedia and Expo (ICME). IEEE, 1–6.\n[51] Heng Wu, Yifan Zhao, and Jia Li. 2023. Invariant and consistent: Unsupervised\nrepresentation learning for few-shot visual recognition. Neurocomputing 520\n(2023), 1–14.\n[52] Shuo Yang, Lu Liu, and Min Xu. 2021. Free lunch for few-shot learning: Distribu-\ntion calibration. arXiv preprint arXiv:2101.06395 (2021).\n[53] Han-Jia Ye, Lu Han, and De-Chuan Zhan. 2022. Revisiting unsupervised meta-\nlearning via the characteristics of few-shot tasks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 45, 3 (2022), 3721–3737.\n[54] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. 2021. Bar-\nlow twins: Self-supervised learning via redundancy reduction. In International\nConference on Machine Learning. PMLR, 12310–12320.\n[55] Hongguang Zhang, Hongdong Li, and Piotr Koniusz. 2022. Multi-level second-\norder few-shot learning. IEEE Transactions on Multimedia (2022).\n[56] Ji Zhang, Lianli Gao, Xu Luo, Hengtao Shen, and Jingkuan Song. 2023. Deta:\nDenoised task adaptation for few-shot learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 11541–11551.\n[57] Ji Zhang, Jingkuan Song, Lianli Gao, and Hengtao Shen. 2022. Free-lunch for cross-\ndomain few-shot learning: Style-aware episodic training with robust contrastive\nlearning. In Proceedings of the 30th ACM International Conference on Multimedia.\n2586–2594.\n[58] Yifan Zhao, Tong Zhang, Jia Li, and Yonghong Tian. 2023. Dual adaptive repre-\nsentation alignment for cross-domain few-shot learning. IEEE Transactions on\nPattern Analysis and Machine Intelligence 45, 10 (2023), 11720–11732.\n[59] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and\nTao Kong. 2021. ibot: Image bert pre-training with online tokenizer. arXiv preprint\narXiv:2111.07832 (2021).\n[60] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and\nTao Kong. 2021. Image BERT Pre-training with Online Tokenizer. In International\nConference on Learning Representations.\n[61] Hao Zhu and Piotr Koniusz. 2023.\nTransductive Few-shot Learning with\nPrototype-based Label Propagation by Iterative Graph Refinement. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n23996–24006.\n[62] Yixiong Zou, Shanghang Zhang, Guangyao Chen, Yonghong Tian, Kurt Keutzer,\nand José MF Moura. 2021. Annotation-Efficient Untrimmed Video Action Recog-\nnition. In Proceedings of the 29th ACM International Conference on Multimedia.\n487–495.\nMICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning\nACM MM, 2024, Melbourne, Australia\nIn this Appendix, we present additional experimental results.\nSubsequently, we introduce an enhanced model of MICM, labeled\nas MICM+. Finally, we discuss the computational and storage costs\nassociated with MICM.\n.1\nImplementation Details\nSelf-Supervised Learning: The Vision Transformer (ViT) back-\nbone and its associated projection head are pre-trained following\nthe iBOT [59] framework, retaining most original hyper-parameters.\nThe training employs a batch size of 640 and a learning rate of 0.0005\non a cosine decay schedule. The MiniImageNet and TieredImageNet\ndatasets are pre-trained for 1200 epochs, while CIFAR-FS is pre-\ntrained for 950 epochs. Additional training details are provided in\nthe appendix.\nFew-shot Evaluation: The pre-trained ViT backbone functions\nas the feature extractor. We utilize various few-shot learning (FSL)\nmethods, including the prototypical networks approach [41], for\nevaluation. In each N-way K-shot task, class prototypes are calcu-\nlated as the mean of the features from K support samples per class.\nQuery images are then classified based on the highest cosine simi-\nlarity to these prototypes. The feature set for evaluation combines\nthe [cls] token with the weighted average [patch] token, using self-\nattention values from the last transformer layer. Test accuracies are\nreported over 2000 episodes, with each episode featuring 15 query\nshots per class, consistent with standard practices in the literature\n[11, 34, 35], and presented with 95% confidence intervals for all\ndatasets.\nA\nSUPPLEMENT TO FIGURE 3\nUsing the same configuration as in Figure 3, we have generated a\ncomparable Figure 8 for the MAE model. We observe high similarity\n(above 0.9) between the features generated by the MAE for novel\nclass samples and the prototypes of the base class. This high simi-\nlarity remains even with limited samples available for fine-tuning,\nas illustrated in Figure 8 (right). The features of the MAE for base\nclasses are indistinguishable, causing all prototypes to converge\nin the center of the feature space. Consequently, both novel and\nbase class samples are extremely close to these central prototypes,\nresulting in indistinguishable classifications.\nFigure 8: Using the same configuration as in Figure 3, fine-\ntuning the MAE-trained models on downstream classifica-\ntion tasks.\nB\nMODEL ROBUSTNESS\nB.1\nAdditional downstream tasks\nWe evaluated MICM on additional downstream tasks, specifically\nfine-tuning a model trained on miniImageNet for fine-grained visual\nclassification (FGVC) on the CUB dataset, and conducting 5-way 1-\nshot few-shot open-set recognition tasks (FSOR) on miniImageNet.\nThe results are shown in Table 7:\nTable 7: Fine-grained visual classification (FGVC) and few-\nshot open-set recognition (FSOR) results of three models.\nMethod\nFGVC (ACC)\nFSOR (AUROC)\nMAE\n21.92\n53.25\nSimCLR\n20.27\n59.92\nMICM\n52.93\n71.12\nB.2\nFurther Cross-Domain Few-Shot Comparing\nIn addition, cross-domain experiments were conducted on the CUB\ndataset [49], characterized by a relatively minor domain gap. Ad-\nhering to the protocols of Poulakakis et al. [35], we trained MICM\non the miniImageNet and evaluated it on the CUB dataset’s test\nset for both 5-way 1-shot and 5-way 5-shot classification tasks. We\npresent the performance results of our MICM model compared to\nexisting unsupervised methods.\nAs reported in Table 8, our model not only surpasses existing\nunsupervised methods but also achieves a significant improvement\nof 4 / 4.3 points over the SOTA Transductive U-FSL method BE-\nCLR [35] in 1-shot and 5-shot tasks, respectively.\nTable 8: Accuracies (in % ± std) on miniImageNet →CUB. The\nresults of the existing model are cited from BECLR [35]\nMethod\nminiImageNet →CUB\n5-way 1-shot\n5-way 5-shot\nMeta-GMVAE [31]\n38.04±0.47\n55.65±0.42\nSimCLR [10]\n38.25±0.49\n55.89±0.46\nMoCo v2 [25]\n39.29±0.47\n56.49±0.44\nBYOL [22]\n40.63±0.46\n56.92±0.43\nSwAV [7]\n38.34±0.51\n53.94±0.43\nNNCLR [21]\n39.37±0.53\n54.78±0.42\nBarlow Twins [54]\n40.46±0.47\n57.16±0.42\nLaplacian Eigenmaps [9]\n41.08±0.48\n58.86±0.45\nHMS [53]\n40.75\n58.32\nPsCo [29]\n-\n57.38±0.44\nBECLR [35]\n43.45±0.50\n59.51±0.46\nMICM (OURS)\n47.44±0.65\n63.86±0.42\nB.3\nSample Bias\nSample bias is an important factor that influences few-shot learning.\nTo evaluate the robustness of our method against sample bias, we\ninvestigate two strategies for its mitigation: 1) augmenting the\nnumber of support samples, and 2) refining the class prototype\nusing an increased number of query samples (Here, we choose\nto refine the class prototype using the OpTA algorithm [35]). To\nACM MM, 2024, Melbourne, Australia\nZhenyu Zhang and Guangyao Chen, et al.\nassess the effectiveness of these strategies, we analyze performance\nvariations of both our model and the baseline across different N-\nway, K-shot, Q-query configurations. This analysis involves, as\nillustrated in Figure 9, incrementally increasing 1) the number of\nsupport samples (K) and 2) the number of query samples (Q). From\nthese experiments, a clear trend emerges: as the count of support\nor query samples rises – effectively reducing sample bias – the\nsuperiority of our model over the baseline becomes increasingly\nevident. This observation underscores the enhanced adaptability\nof our approach, especially in scenarios characterized by smaller\nsample bias, where our model demonstrates a more substantial\nperformance improvement compared to the baseline.\nFigure 9: Performance comparison for varying numbers of\nshots and queries.\nC\nMICM+\nWe further developed a hybrid method, MICM+, by integrating\npseudo-label learning [35] with the transductive OpTA FSL tech-\nnique [35]. This approach exploits the synergistic potentials of both\nmethods to enhance performance in scenarios with limited labeled\ndata.\nC.1\nPseudo Label Training\nBECLR’s Pseudo Label Training Stage. The SOTA BECLR [35]\nutilizes a memory bank alongside clustering techniques to facilitate\npseudo-label training. Despite its effectiveness, this method faces\nchallenges such as increased storage requirements and slow con-\nvergence rates, stemming from continuous updates between the\nmemory bank and current samples during training.\nIn our architecture, we experimented with integrating BECLR’s\npseudo-label training either from scratch or into a pre-trained\nMICM model. Our findings, detailed in Table 9, reveal that starting\nfrom scratch prolongs training times and diminishes performance,\nas does introducing pseudo-labeling to a pre-trained model. To\naddress these issues, we propose a novel pseudo-label training\nstrategy using BCE loss, as shown in Figure 10. This method avoids\nTable 9: Our MICM model’s inductive few-shot performance\non the MiniImageNet dataset after incorporating two pseudo-\nlabel training methods.\nMethod\nTraining time\n5-way 1-shot\n5-way 5-shot\nMICM\n31.25 Hours\n61.37±0.62\n81.68±0.43\nMICM + BECLR (From scratch)\n46.25 Hours\n57.43±0.62\n77.34±0.51\nMICM + BECLR (A new stage)\n31.25 + 1.50 Hours\n61.30±0.59\n81.65±0.37\nMICM + Pseudo (A new stage)\n31.25 + 0.16 Hours\n66.69±0.65\n84.03±0.45\nadditional storage costs and can be seamlessly added to existing\npre-trained models.\nFigure 10: Our pseudo label training stage: further refines\nsamples representations using pair comparison techniques,\nthereby enhancing the model’s ability to differentiate be-\ntween various image representations.\nOur Pseudo Label Training Stage. Building upon the robust rep-\nresentations developed during the self-supervised learning stage,\nour primary objective is to enhance the inter-class distinction. To\nachieve this, we have incorporated a pseudo-label learning method\naimed at increasing intra-class compactness. This approach is de-\ntailed in Figure 10 and employs a pairwise objective to promote\nsimilarity between instance pairs, ensuring effective clustering of\ninstances within the same class.\nPseudo-labels are generated by calculating the cosine distances\namong all pairs of feature representations, Z[cls], within a mini-\nbatch. These distances are ranked, and each instance is assigned a\npseudo-label based on its closest neighbor. Pseudo-labels are thus\ngenerated from the most confidently paired positive instances in\nthe mini-batch. Given a mini-batch, S, containing 𝐵instances with\ntheir features Z[cls], we denote the subset of closest pairs as S′.\nThe pairwise objective is defined using a binary cross-entropy loss\n(BCE) as follows:\nLBCE = 1\n𝐵\n𝐵\n∑︁\n𝑖=1\n−log⟨𝜎(Z(𝑖)\n[cls]), 𝜎(Z′(𝑖)\n[cls])⟩\n(4)\nwhere 𝜎is a normalization function applied to each feature vector\nin S and S′.\nIn addition to the BCE loss, we continue to use the mean squared\nerror (MSE) loss, LMSE, from the self-supervised stage to maintain\na balance between classification efficacy and model generalization.\nMICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning\nACM MM, 2024, Melbourne, Australia\nFurthermore, we have opted to remove the patch-level loss,\nL[patch], for two primary reasons: Firstly, our pseudo-label train-\ning does not involve comparing two views of the same image, mak-\ning patch-level alignment infeasible. Secondly, maintaining two\nviews and computing patch loss would significantly increase stor-\nage demands, necessitating a reduction in batch size. Larger batch\nsize is essential for effective pseudo-label training. This modifica-\ntion, as documented in Table 10, involved reducing the batch size\nfrom 128 to 80, leading to a degradation in model performance.\nTable 10: MICM+’s transductive few-shot performance after\npseudo-label training with/withot L[patch]. When L[patch] is\nretained, a smaller batch size is required due to the increased\nGPU memory consumption.\nMethod\n5-way 1-shot\n5-way 5-shot\nMICM+ w/ Patch loss\n77.96±0.65\n85.46±0.41\nMICM+\n81.05±0.58\n87.95±0.34\nC.2\nOptimal Transport-based Distribution\nAlignment (OpTA)\nIn line with BECLR’s task setting [35], we employ the OpTA al-\ngorithm for transductive few-shot classification tasks. The OpTA\nprocess is expressed as follows:\nLet T = S ∪Q be a downstream few-shot task. We first extract\nthe support 𝑍S (of size NK × d) and query 𝑍Q (of size NQ × d)\nembeddings and calculate the support set prototypes 𝑷S (class\naverages of size N × d). Firstly, an optimal transport problem is\ndefined from 𝑍Q to 𝑷S as:\n𝚷(𝒓, 𝒄) =\nn\n𝝅∈R𝑁𝑄×𝑁\n+\n| 𝝅1𝑁= 𝒓, 𝝅⊤1𝑁𝑄= 𝒄, 𝒓= 1 · 1/𝑁𝑄, 𝒄= 1 · 1/𝑁\no\n(5)\nTo find a transport plan 𝜋(out of Π) mapping 𝑍Q to 𝑷S. Here,\n𝒓∈R𝑁𝑄denotes the distribution of batch embeddings [𝒛𝑖]𝑁𝑄\n𝑖=1 , 𝒄∈\nR𝑁is the distribution of prototypes [𝑷𝑖]𝑁\n𝑖=1. The last two conditions\nin Eq. 2 enforce equipartitioning (i.e., uniform assignment) of Z\ninto the P partitions. Obtaining the optimal transport plan, ˆ𝝅★, can\nthen be formulated as:\n𝝅★= argmin\n𝝅∈𝚷(𝒓,𝒄)\n⟨𝝅, 𝑫⟩𝐹−𝜀H(𝝅),\n(6)\nand solved using the Sinkhorn-Knopp [18] algorithm. Here, 𝐷is\na pairwise distance matrix between the elements of 𝑍Q and 𝑷S\n(of size NQ × N), ⟨·⟩𝐹denotes the Frobenius dot product, 𝜀is a\nregularisation term, and H(·) is the Shannon entropy.\nAfter Obtaining the optimal transport plan ˆ𝝅★, we use ˆ𝝅★to\nmap the support set prototypes onto the region occupied by the\nquery embeddings to get the transported support prototypes ˆ𝑷S\nas:\nˆ𝑷S = ˆ𝝅★𝑇𝒁Q,\nˆ𝝅★\n𝑖,𝑗=\n𝝅★\n𝑖,𝑗\nÍ\n𝑗𝝅★\n𝑖,𝑗\n, ∀𝑖∈[𝑁𝑄], 𝑗∈[𝑁],\n(7)\nand a comprehensive description of this algorithm is provided in\nBECLR [35]. Our application of MICM+ with OpTA has led to im-\nproved transductive few-shot performance, discussed in subsequent\nsections.\nD\nABLATION STUDY\nThe proposed MICM+ model integrates five key components incud-\ning: OpTA [35], MICM, (LMSE), (LCL) and pseudo-label learning\n(pseudo). As detailed in Table 11, the baseline model employing\nOpTA for transductive classification tasks exhibits a notable im-\nprovement of 13.6% and 3.5% over traditional inductive classifi-\ncation approaches. This marked enhancement, especially in the\n1-shot scenario, can be attributed to OpTA’s effective mitigation\nof sample bias. Our model MICM combines the LCL from CL and\nthe LMSE from MIM, but in the ablation experiments, we separate\nthese two loss functions. It can be observed that the model using\nonly LMSE has no classification ability, while the model using only\nLCL shows relatively good classification performance. However,\nby combining LMSE and LCL, the model’s performance improves\nby approximately 2.7 / 1.9 points. This result highlights the impor-\ntance of utilizing generalized features learned during the image\nreconstruction process. Integration of pseudo-label training con-\ntributes additional gains of 2.5% and 1.0% in the 1-shot and 5-shot\nsetups. This enhancement, facilitated by pseudo, further elevate the\nrepresentational capabilities of features from the pre-training stage\nand their adaptability to small sample data in downstream tasks.\nTable 11: Ablating main components of MICM.\nOpTA\nMICM\nLMSE\nLCL\nPseudo\n5-way 1-shot\n5-way 5-shot\n-\n-\n-\n-\n-\n60.93 ± 0.61\n80.38 ± 0.34\n✓\n-\n-\n-\n-\n74.58 ± 0.66\n83.95 ± 0.34\n✓\n✓\n✓\n-\n-\n26.81 ± 0.43\n32.94 ± 0.47\n✓\n✓\n-\n✓\n-\n75.73 ± 0.64\n85.06 ± 0.35\n✓\n✓\n✓\n✓\n-\n78.40 ± 0.61\n86.90 ± 0.30\n✓\n✓\n✓\n✓\n✓\n81.05 ± 0.58\n87.95 ± 0.34\nE\nCOMPUTATIONAL AND STORAGE\nOVERHEAD\nMICM only slightly increases storage and computational and stor-\nage overhead compared to iBOT [59], as shown in Tabel 12.\nTable 12: Computational and storage overhead of MICM.\nMethod\nParameter\nFLOAPs\nTraing Time\nInference Time\niBOT\n43 M\n691.84 G\n280 (sec/epoch)\n0.121 (sec/episode)\nMICM\n43 M\n690.71 G\n292 (sec/epoch)\n0.120 (sec/episode)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-08-23",
  "updated": "2024-08-23"
}