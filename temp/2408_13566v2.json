{
  "id": "http://arxiv.org/abs/2408.13566v2",
  "title": "Control-Informed Reinforcement Learning for Chemical Processes",
  "authors": [
    "Maximilian Bloor",
    "Akhil Ahmed",
    "Niki Kotecha",
    "Mehmet Mercangöz",
    "Calvin Tsay",
    "Ehecactl Antonio Del Rio Chanona"
  ],
  "abstract": "This work proposes a control-informed reinforcement learning (CIRL) framework\nthat integrates proportional-integral-derivative (PID) control components into\nthe architecture of deep reinforcement learning (RL) policies. The proposed\napproach augments deep RL agents with a PID controller layer, incorporating\nprior knowledge from control theory into the learning process. CIRL improves\nperformance and robustness by combining the best of both worlds: the\ndisturbance-rejection and setpoint-tracking capabilities of PID control and the\nnonlinear modeling capacity of deep RL. Simulation studies conducted on a\ncontinuously stirred tank reactor system demonstrate the improved performance\nof CIRL compared to both conventional model-free deep RL and static PID\ncontrollers. CIRL exhibits better setpoint-tracking ability, particularly when\ngeneralizing to trajectories outside the training distribution, suggesting\nenhanced generalization capabilities. Furthermore, the embedded prior control\nknowledge within the CIRL policy improves its robustness to unobserved system\ndisturbances. The control-informed RL framework combines the strengths of\nclassical control and reinforcement learning to develop sample-efficient and\nrobust deep reinforcement learning algorithms, with potential applications in\ncomplex industrial systems.",
  "text": "CONTROL-INFORMED REINFORCEMENT LEARNING FOR\nCHEMICAL PROCESSES\nMaximilian Bloor, Akhil Ahmed, Niki Kotecha,\nMehmet Mercangöz, Calvin Tsay∗, Ehecactl Antonio Del Rio Chanona∗\nSargent Centre for Process Systems Engineering, Department of Chemical Engineering\nImperial College London\nLondon\n{max.bloor22, a.del-rio-chanona, c.tsay}@imperial.ac.uk\nABSTRACT\nThis work proposes a control-informed reinforcement learning (CIRL) framework that integrates\nproportional-integral-derivative (PID) control components into the architecture of deep reinforcement\nlearning (RL) policies. The proposed approach augments deep RL agents with a PID controller\nlayer, incorporating prior knowledge from control theory into the learning process. CIRL improves\nperformance and robustness by combining the best of both worlds: the disturbance-rejection and\nsetpoint-tracking capabilities of PID control and the nonlinear modeling capacity of deep RL. Sim-\nulation studies conducted on a continuously stirred tank reactor system demonstrate the improved\nperformance of CIRL compared to both conventional model-free deep RL and static PID controllers.\nCIRL exhibits better setpoint-tracking ability, particularly when generalizing to trajectories outside\nthe training distribution, suggesting enhanced generalization capabilities. Furthermore, the embed-\nded prior control knowledge within the CIRL policy improves its robustness to unobserved system\ndisturbances. The control-informed RL framework combines the strengths of classical control and\nreinforcement learning to develop sample-efficient and robust deep reinforcement learning algorithms,\nwith potential applications in complex industrial systems.\nKeywords Reinforcement learning · Process control · PID Control\n1\nIntroduction\nIn the chemical process industry, maintaining control over complex systems is crucial for achieving reliable, efficient,\nand high-performance operations [1]. Traditionally, process control has relied heavily on classical feedback control\ntechniques such as proportional-integral-derivative (PID) controllers due to their simplicity, interpretability, and well-\nestablished tuning methods [2]. However, these tuning methods are often largely empirical, or otherwise rely on having\naccurate mathematical models of the open-loop system dynamics and disturbance responses, which can be challenging\nto derive for complex processes involving nonlinearities, delays, constraints, and changing operating conditions [3]. As\na result, PID controllers often struggle to provide adequate control performance without extensive re-tuning or gain\nscheduling [4]. One alternative is Model Predictive Control (MPC), a successful model-based process control strategy,\nwith the ability to optimize control actions based on the current system states and predicted future behavior while\nsatisfying constraints. This has led to its widespread adoption in the chemical process industry [5]. Typically, MPC\noperates as a supervisory layer below the real-time optimization (RTO) layer, providing control setpoints to lower-level\nregulatory controllers, often PID controllers, which directly manipulate process variables. This hierarchical structure\ncombines the predictive capabilities of MPC with the rapid response of traditional feedback control. However, the\nperformance of MPC heavily relies on the accuracy of its internal process model [6]. The ongoing digitalization of the\nchemical process industry has opened new avenues for enhancing MPC performance through data-driven approaches.\nThis digital transformation has enabled the integration of advanced analytics and machine learning techniques into\nboth MPC and regulatory control layers, especially on the modeling end. For instance, artificial neural networks [7]\n∗Corresponding Authors\narXiv:2408.13566v2  [eess.SY]  27 Aug 2024\nControl-Informed Reinforcement Learning for Chemical Processes\nand Gaussian processes [8] have been employed to capture complex process dynamics that may be challenging to\nmodel using purely mechanistic approaches. Furthermore, the increased availability of process data has facilitated the\ndevelopment of hybrid models that combine first-principles knowledge with data-driven components [9, 10]. These\nadvancements, coupled with improvements in computational capabilities, have expanded the applicability of MPC to\nmore complex and uncertain processes. However, challenges remain in areas such as online computational requirements\nfor large-scale systems and the handling of uncertainties.\nRecently, reinforcement learning (RL) has emerged as a promising data-driven framework for learning control policies\ndirectly from interactions with the chemical process system [11]. Deep RL methods, which utilize deep neural networks,\nhave demonstrated success in a variety of difficult decision-making and control problems. A key advantage of model-\nfree RL is that it does not require accurate system models once online, instead learning control policies from experience.\nWhile model-free RL approaches can learn control policies without requiring explicit system models, they often face\nchallenges in sample efficiency and may not fully leverage existing domain knowledge [12].\nFor safety reasons, RL algorithms in chemical process control are typically trained on simulation models rather than\ndirectly on physical systems. Despite this limitation, RL offers several advantages over traditional control methods. One\nsignificant benefit is the fast online inference time. This characteristic makes RL particularly suitable for systems where\nonline computation time is critical, as the trained policy can execute control decisions rapidly in real-time applications.\nRL also shows promise in handling complex, nonlinear systems and adapting to process uncertainties. This feature\nallows RL to potentially address challenges in dynamic chemical processes more effectively than traditional control\napproaches. We note there is also growing interest in algorithms for safe RL, or those which can avoid (known or\nunknown) constraints, e.g., in physical systems [13].\nIt is important to acknowledge the challenges associated with implementing RL to control complex chemical processes.\nThe offline training of RL agents often requires a large number of samples to achieve satisfactory performance, making\nthe training process computationally intensive and time-consuming. The quality of the trained agent is highly dependent\non the fidelity of the simulation model used, which may not always capture all the nuances of real-world processes,\nrequiring online fine-tuning. Moreover, deep RL methods often treat the control problem as a black box, failing\nto incorporate valuable insights from control theory. This highlights the need for approaches that can balance the\nmodel-free learning capabilities of RL with the incorporation of domain expertise and efficient exploration strategies.\n1.1\nRelated works\nDeep reinforcement learning (deep RL), which combines deep neural networks (DNNs) with RL, has been demonstrated\nin various domains, including robotics, data center operations, and playing games [14, 15, 16]. This success has brought\nattention to RL from the process systems and control communities. The process systems engineering community has\nmade significant progress in adapting RL to the process industries, including in distributed systems [17], constraint\nhandling [18, 19, 20], inventory management [18, 21], batch bioprocess and control [22, 23, 24], production scheduling\n[25], and energy systems [26]. Early applications of RL in process control proposed model-free RL for tracking control\nand optimization in fed-batch bioreactors [27, 28, 29]. More recently, Mowbray et al. [30] employed a two-stage\nstrategy using historical process data to warm-start the RL algorithm and demonstrated this on three setpoint-tracking\ncase studies. Machalek, Quah, and Powell [31] developed an implicit hybrid machine learning model combining\nphysics-based equations with artificial neural networks, demonstrating its application for reinforcement learning in\nchemical process optimization. Zhu et al. [32] developed an RL algorithm that improves scalability by reducing the size\nof the action space, which was demonstrated on a plantwide control problem. However, the sample efficiency of these\nalgorithms remains a key aspect restricting their widespread industrial adoption.\nTo address the limitations, prior works have explored integrating reinforcement learning with existing control structures,\ne.g., PID controllers [33, 34, 35]. Early approaches applied model-free RL to directly tune the gains of PID controllers\n[36, 37] or used model-based RL techniques such as dual heuristic dynamic programming [38]. Other approaches\nhave investigated embedding knowledge of the dynamical system using physics-informed neural networks to act as\na surrogate model of the process for offline training of the RL agent [39]. Efforts have also been made to develop\ninterpretable control structures that maintain transparency while leveraging advanced optimization techniques [40].\nLawrence et al. [41] directly parameterized the RL policy as a PID controller instead of using a deep neural network,\nallowing the RL agent to improve the controller’s performance while leveraging existing PID control hardware. This\nwork demonstrates that industry can utilize actor-critic RL algorithms without the need for additional hardware or the\nlack of interpretability which often accompanies the use of a deep neural network. To improve this work’s training time,\nMcClement et al. [42] used a meta-RL approach to tune PI controllers offline. The method aimed to learn a generalized\nRL agent on a distribution of first-order plus time delay (FOPTD) systems, resulting in an adaptive controller that can\nbe deployed on new systems without any additional training. However, while the meta-RL approach removes the need\n2\nControl-Informed Reinforcement Learning for Chemical Processes\nfor explicit system identification, some knowledge of the process gain and time constant magnitudes is still required to\nappropriately scale the meta-RL agent’s inputs and outputs when applying it to new systems.\n1.2\nContributions\nIn contrast to the above methods that focus on tuning PID gain values with a fixed control structure, we propose a\ncontrol-informed reinforcement learning (CIRL) framework that integrates the PID control structure with a deep neural\nnetwork into the control policy architecture of an RL agent. This allows the approach to adapt to changing operating\npoints due to the inclusion of the deep neural network. Furthermore, it aims to leverage the strengths of both PID\ncontrol and deep RL: we seek to improve sample efficiency and stability using known PID structures while gaining\nrobustness and generalizability from RL. In summary, the key contributions of this work are as follows:\n1. We introduce the CIRL framework, which augments deep RL policies with an embedded PID controller\nlayer. This enables the agent to learn adaptive PID gain tuning while preserving the stabilizing properties and\ninterpretability of PID control, effectively acting as an automated gain scheduler.\n2. We demonstrate the CIRL framework on a nonlinear continuously stirred tank reactor (CSTR) system. The\nCIRL agent improves setpoint tracking performance compared to both a static PID controller and a standard\nmodel-free deep RL approach, particularly when generalizing to operating regions outside the training\ndistribution.\n3. We show that by leveraging the embedded prior knowledge from the PID structure, the CIRL agent exhibits\nenhanced robustness to process disturbances that are not observable during training.\nThe remainder of this article is organized as follows. Section 2 presents the background on PID control and reinforcement\nlearning. Section 3 describes the proposed control-informed reinforcement learning framework in detail. Section 4\ndiscusses the simulation and experimental results. Finally, Section 5 concludes the article and outlines future research\ndirections.\n2\nBackground\n2.1\nReinforcement Learning\nThe standard RL framework (Figure 1) consists of an agent that interacts with an environment. Assuming the states\nare fully observable, the agent receives a vector of measured states xt ∈X ⊆Rnx, and can then take some action\nut ∈U ⊆Rnu, which results in the environment progressing to state xt+1. Sets X and U represent the state and action\nspace, respectively. For a deterministic policy π, the agent takes actions ut = π(xt), while, for a stochastic policy, the\naction ut is sampled from the policy π represented by a conditional probability distribution ut ∼π(· | xt). A common\nassumption in RL is that the state transition given some action is defined by a density function xt+1 ∼p(· | xt, ut) that\nrepresents the stochastic nonlinear dynamics of the process. The reward the agent receives is defined by the function\nrt = R(xt, ut). With a defined control policy, the policy can be implemented over a discrete time horizon T thus\nproducing the following trajectory τ = (x0, u0, r0, x1, u1, r1, ..., xT , uT , rT ).\nrt+1\nxt+1\nReward (rt)\nState (xt)\nAction \n(ut)\nEnvironment\nRL Agent \nA → B\n \nFigure 1: The RL framework\nFormally, notice that the above state transition assumption enables modeling the underlying system as a Markov\nDecision Process (MDP); for further treatment of the subject the reader is referred to Sutton and Barto [43]. In\n3\nControl-Informed Reinforcement Learning for Chemical Processes\n...\n...\n...\nx1\nx2\n...\nxn\nu1\nu2\n...\nun\nHidden\nlayer h1\nHidden\nlayer h2\nHidden\nlayer hn\nInput\nlayer\nOutput\nlayer\nFigure 2: Deep policy network πθ\nreinforcement learning, particularly in our framework, the agent’s goal is to maximize the cumulative reward (return)\nJ(π) over a pre-defined (often infinite) timespan, a discount factor γ is used to reflect the uncertain future and ensure\ncomputational tractability. The policy that achieves this is the optimal policy π∗:\nJ(π) = Eπ\n\" T\nX\nt=0\nγtrt(xt, π(xt))\n#\n(1)\nπ∗= arg max\nπ\nJ(π)\n(2)\nThe value function V π(xt) represents the expected return starting from state xt and following policy π thereafter.\nV π(xt) = Eπ [J(π)|x0 = x]\n(3)\nSimilarly, the action-value function, or “Q-function,” Qπ(xt, ut) represents the expected return from starting from state\nxt and taking action ut, assuming that policy π is followed otherwise:\nQπ(xt, ut) = Eπ [J(π)|x0 = x, u0 = u]\n(4)\nTwo broad classes of algorithms have emerged to solve the previously described problem: policy optimization methods\nand value-based methods. These two methods have been effective in deep RL, where a DNN has been used as a function\napproximator to mitigate the “curse of dimensionality” stemming from the discretization of both action and state spaces\noften necessary to solve continuous problems [44]. The use of DNNs allows the parameterization of the policy π ≈πθ\nwhere θ ∈Ω⊆Rnθ represents the parameters of the policy and Ωis the parameter space (Figure 2).\nA popular group of policy optimization methods leverages policy gradients to optimize the policy in reinforcement\nlearning [45]. Policy gradient methods, such as Trust Region Policy Optimization (TRPO) [46] and REINFORCE [45],\nfollow a stochastic gradient ascent strategy to update the policy parameters with a scalar learning rate α:\nθ ←θ + α∇θ bJ(θ)\n(5)\nThese methods directly optimize the expected return J(θ) by following the gradient of the policy parameters θ. The\ngradient is estimated from sampled trajectories collected by rolling out the current policy in the environment. Policy\ngradient methods offer several advantages, including their ability to handle continuous action spaces effectively, and the\ndirect optimization of the policy. However, they often suffer from high variance in gradient estimates leading to these\nmethods converging to locally optimal policies. The specific algorithms within this family have their own characteristics;\nfor instance, TRPO [46] provides more stable updates but can be computationally expensive, and REINFORCE [45],\nwhile conceptually straightforward, often suffers from high variance in practice.\nThe second class of reinforcement learning methods comprises value-based algorithms, such as Deep Q-Network (DQN)\n[47], which rely on learning an action-value (or Q) function. This Q-function can be approximated with a deep neural\nnetwork with parameters ϕ ∈Φ ⊆Rnϕ resulting in Qϕ (Figure 3). The parameters are then updated by minimizing\n4\nControl-Informed Reinforcement Learning for Chemical Processes\n...\n...\n...\n...\n...\nInput\nlayer\nHidden\nlayer h1\nHidden\nlayer h2\nHidden\nlayer h3\nOutput\nlayer Q\nx1\nx2\nxn\nu1\nu2\nun\nQ\nFigure 3: Deep Q-function Qϕ\nmean squared error against targets given by the Bellman recursion equation as follows:\nL(ϕ) = E(xt,ut,rt,xt+1)\n\u0014\n(rt + γ max\nut+1 Qϕ(xt+1, ut+1) −Qϕ(xt, ut))2\n\u0015\n(6)\nThe idea is to minimize the temporal difference error between the Q-value estimates and the backed-up estimates from\nthe next state and reward, as this approximates the Bellman optimality condition.\nWhile DQN has been successful in discrete action spaces, extending these methods to continuous action spaces presents\nchallenges. Two notable non-actor approaches for continuous action spaces are Continuous Action Q-Learning (CAQL)\n[48] and Constrained Continuous Action Q-Learning (cCAQL) [49]. CAQL adapts the Q-learning framework to\ncontinuous actions by using a neural network to represent the Q-function and optimizing it with respect to actions.\ncCAQL improves upon CAQL by introducing constraints to the action selection process, which helps to stabilize\nlearning and improve robustness in continuous action spaces. As an alternative to directly optimizing Q-functions,\nactor-critic algorithms have also been employed to extend deep Q-networks methods to continuous action-space\nproblems by including an actor-network that approximates the action taken by maximizing the Q-function. These\nmodern RL algorithms include TD3 [50] and Soft-Actor Critic (SAC) [51].\n2.2\nEvolutionary Strategies in Reinforcement Learning\nWithin policy optimization methods, in addition to algorithms that leverage policy gradients, it is also possible to use\nevolutionary algorithms. Both types of algorithms update the parameters to optimize the policy which takes states as\ninputs and outputs (optimal) control actions (Figure 2). This distinction is not unlike evolutionary and gradient-based\nalgorithms in traditional optimization problems, i.e., evolutionary algorithms simply provide an alternative framework\nfor learning the policy parameters. Evolutionary strategies (ES) are a class of data-driven optimization algorithms\ninspired by principles of biological evolution. These algorithms optimize policies by iteratively generating populations\nof candidate solutions, evaluating their fitness (performance), and selectively propagating the fittest individuals to\nsubsequent generations through processes similar to mutation, recombination, and selection.\nIn the context of RL, ES can be used to optimize the parameters θ of a policy πθ directly, without relying on gradient\ninformation. These policies are evaluated in the environment on an episodic basis with their cumulative return as shown\nby a parameterized variant of Equation 1:\nJ(θ) = Eθ\n\" T\nX\nt=0\nγtrt(xt, πθ(xt))\n#\n(7)\nInstead of using the approximation of the Q-function (Equation 4) or using a return gradient estimate to optimise the\npolicy parameters. ES-RL algorithms use the estimate of the J(θ) and directly update the parameters towards those\npolicies that produce higher returns to improve performance [52]. The key advantages of ES-RL algorithms include\n5\nControl-Informed Reinforcement Learning for Chemical Processes\nthe ability to be easily parallelized, making them computationally efficient for evaluating multiple candidate solutions\nsimultaneously. Additionally, ES-RL algorithms are less susceptible to getting trapped in local optima compared\nto gradient-based methods, as they explore the parameter space more “globally” through population-based search\nand they do not rely on stochastic estimates of gradients, which are also computationally expensive. Furthermore,\nES-RL methods can be more robust to the inherent noisiness often associated with stochastic gradient descent (SGD)\nmethods used in policy gradient approaches. Given these advantages, there has been some research interest in the\nES-RL. Salimans et al. [53] applied developed an ES-RL algorithm and evaluated it on MuJoCo and Atari environments\nresulting in comparable performance to policy gradient methods such as TRPO [46]. Wu, de Carvalho Servia, and\nMowbray [54] used a hybrid strategy of derivative-free optimization techniques to solve an inventory management\nproblem with improved performance over the policy gradient method Proximal Policy optimization (PPO) [55].\n2.3\nPID Controllers\nThe PID controller is a widely used feedback mechanism employed in industrial control systems [3]. The discrete\nPID controller calculates an error value et in discrete time as the difference between a desired setpoint and a measured\nprocess variable, and applies a correction based on three parameters: proportional (KP ), integral time constant (τi),\nand derivative time constant (τd). Note that other parameterizations of these degrees of freedom are possible. The\nproportional term applies a control action proportional to the current error, providing an immediate response to\ndeviations from the setpoint. The integral term accumulates the error over time and applies a control action to eliminate\nsteady-state errors. The derivative term considers the rate of change of the error and provides a dampening effect to\nprevent overshoot and oscillations. The discrete position form of a single PID controller is defined as\nut = Kpet + Kp\nτi\nt\nX\nt=0\net + Kpτd(et −et−1)\n(8)\nwhere et = x∗\ni,t −xi,t, is the setpoint error of state i at timestep t, with the setpoint x∗\ni,t of state i at timestep t.\nTuning the PID gains refers to finding values of the parameters Kp, τi, and τd that result in good closed-loop performance\n(often measured by integrated squared error, etc.) and is crucial for achieving desired control performance. As a result,\nmany popular tuning methodologies have been developed, including the Internal Model Control [56, 57] and relay\ntuning [58]. The first of these methods is a model-based technique and the second excites the system, and uses the\nresponse to estimate the three PID parameters.\nThe time-invariant PID structure can achieve good performance on (approximately) linear systems. Historically,\nthis condition was often sufficient, as processes are often operated around a known setpoint in an approximately\nlinear region; however, more recent applications in control of nonlinear systems (e.g., transient, intensified, or cyclic\nprocesses) motivate more advanced control strategies. Given a nonlinear system the PID parameters will be dependent\non the operating point, which motivates a gain scheduled approach. Gain scheduling involves designing multiple PID\ncontrollers for different operating regions and switching between them based on the current process conditions. In\nindustrial applications, a common approach to gain scheduling is through the use of lookup tables, where the PID gains\nare pre-computed and stored for different operating conditions or setpoints [59]. More recently, data-driven, model-free\napproaches to gain scheduling have gained traction as they are able to design the control directly from a single set of\nplant input and output data without the need for system identification [60]. Despite the advancements in PID control,\nchallenges remain in terms of the manual effort required for controller tuning, and the limited performance in highly\nnonlinear and time-varying systems. These challenges motivate the integration of PID control with data-driven and\nlearning-based approaches, such as reinforcement learning, to leverage the strengths of both paradigms. While more\nadvanced control strategies like Model Predictive Control (MPC) exist, our focus on PID control is motivated by the\nability to leverage existing infrastructure and well-established systems in industrial settings, providing a practical and\nwidely applicable solution. Furthermore, most industrial applications of MPC utilize a lower-level PID as the regulatory\ncontroller hence, highlighting the prominence of PID control in chemical processes.\n6\nControl-Informed Reinforcement Learning for Chemical Processes\nKp,t(Δet)\nΣ \nKp,t\nτi,t\nτd,t\nut\nKp,tτd,t(Δ2et)\nst\nKp,t/τi,t(et)\nNeural Network\nPID Controller Process\net\net = xt - xt\n*\nFigure 4: CIRL Agent\n3\nMethodology\nThis section presents the proposed control-informed reinforcement learning (CIRL) framework, which integrates PID\ncontrol structures into the policy architecture of deep RL agents. The methodology covers the CIRL agent design,\npolicy optimization algorithm, and implementation details.\n3.1\nControl-Informed Reinforcement Learning (CIRL) Agent\nThe CIRL agent consists of a deep neural network policy augmented with a PID controller layer, as illustrated in Figure\n4. The base neural network takes the observed states st as inputs and outputs the PID gain parameters Kp,t, τ i,t, and\nτ d,t ∈Rnu at each timestep t. The PID controller layer then computes the control action ut based on the error signal\net = x∗\nt −xt and the current learned gain parameters.\nThe agent’s state s includes Nt timesteps of history for both the state and setpoint, where Nt > 2 to fully define the\nvelocity-form PID controller:\nst =\n\u0002\nxt...t−Nt, x∗\nt...t−Nt\n\u0003\n(9)\nThe PID layer is represented in the velocity form since, if the position form of the PID controller (Equation 8) is used\nand the gain changes suddenly, this can cause disturbances to the system [4]. The velocity form mitigates this issue by\nensuring that the control input does not change abruptly despite sudden gain changes, and it is not necessary to reset the\nintegral term. The kth PID controller of the system is represented by:\n∆u(k)\nt\n= K(k)\np,t ∆e(k)\nt\n+ K(k)\np,t\nτ (k)\ni,t\ne(k)\nt\n∆t + K(k)\np,t τ (k)\nd,t\n∆2e(k)\nt\n∆t\n(10)\nwhere ∆e(k)\nt\n= e(k)\nt\n−e(k)\nt−1, ∆2e(k)\nt\n= ∆e(k)\nt\n−2e(k)\nt−1 +e(k)\nt−2 and the superscript (k) denotes the index of the controller,\nwhere k ∈0, 1, . . . , nu, and nu is the total number of controllers in the system.\nThrough interacting with the environment, the CIRL agent aims to maximize the cumulative reward given by rt ∈R at\neach time step. For process control regulatory problems, various reward functions have been proposed. In general, they\nall involve some measure of (integrated) setpoint error, either squared or absolute, and/or a penalty for control action,\nsimilar to MPC objective functions. Adopting a similar notation to MPC, a squared error term penalizes deviations of\nthe controlled variable from the setpoint, with larger deviations penalized more heavily:\nrt = −\n\u0000et\nT Qet + ut\nT Ru\n\u0001\n(11)\n7\nControl-Informed Reinforcement Learning for Chemical Processes\nwhere Q ∈Rnx×nx and R ∈Rnu×nu are weighting factors that balance the trade-off between tracking performance\nand control effort.\nIt is important to note that derivative information is not passed between the PID controller and the neural network in\nthe proposed CIRL agent architecture, as we take an evolutionary optimization strategy. Future work may study an\nintegrated gradient-based learning strategy. The CIRL rollout pseudocode is given in Algorithm 1\nAlgorithm 1: CIRL Rollout\nInput: Policy Parameters θ, Number of simulation timesteps ns, Discrete time environment f\nOutput: Cumulative Reward R\n1 s ←s0\n// Reset observation to initial state\n2 R ←0\n// Initialize cumulative reward\n3 for t = 0 to ns −1 do\n4\nKp,t, τi,t, τd,t ←πθ(st)\n// Get current PID gains from policy\n5\nut ←PID(Kp,t, τi,t, τd,t, et, et−1, et−2)\n// Use PID controller to output control input\n6\nxt+1, rt ←f(ut, xt)\n// Take one timestep in the environment\n7\nst+1 ←[xt+1, xt, x∗\nt+1]\n// Update observation vector\n8\nR ←R + rt\n9 end\n10 return R\n// Return cumulative reward\n3.2\nImplementation of Policy Optimization\nIn this work, the CIRL agent’s policy is optimized using a hybrid approach based on evolutionary strategies, combining\nrandom search and particle swarm optimization (PSO) [61]. A population of candidate policy parameter vectors is\ninitialized by sampling randomly from the allowable ranges for each parameter dimension. The parameters in this\ncase are the weights of the neural networks. This initial random sample provides a scattered set of starting points that\nencourages exploration of the full parameter landscape. The random population undergoes N iterations in which the\nobjective function value (cumulative reward obtained by the policy in the environment) is evaluated for each policy to\ninitialize the population in a good region of the policy space. The objective function to be maximized is:\nJ(θ) = Eπθ\n\" T\nX\nt=0\n−(et\nT Qet + ut\nT Ru)\n#\n(12)\nThe best, or fittest, policies from this initial random sampling are carried forward as seeds to initialize the PSO phase\nof the algorithm. The PSO phase is then started, allowing the particles (policy parameter vectors θi) to explore areas\naround the initially fit random vectors in a more structured manner. In each PSO iteration, particle velocities and\npositions are updated as:\nvt+1\ni\n= wvt\ni + c1r1\n\u0000pt\ni −θt\ni\n\u0001\n+ c2r2\n\u0000gt −θt\ni\n\u0001\n(13)\nθt+1\ni\n= θt\ni + vt+1\ni\n(14)\nwhere vt\ni and θt\ni are the velocity and policy parameter vector of particle i at iteration t, w = is the inertia weight, c1\nand c2 are the cognitive and social acceleration constants, r1 and r2 are random numbers in [0, 1], pt\ni is the personal\nbest policy parameter vector of particle i, and gt is the global best policy parameter vector of the swarm. This hybrid\napproach leverages the global exploration capabilities of initial random sampling, while also taking advantage of the\nPSO’s ability to collaboratively focus its search around promising areas identified by the initial random search. The\npseudocode for the policy optimization procedure is given shown by the block diagram (Figure 5). We highlight that\nthe CIRL framework is agnostic to the policy optimization strategy, i.e., policy gradients or other policy optimization\ntechniques can be used. Our choice of evolutionary algorithms in this work is motivated by optimization performance\ngiven the small size of the neural network, as well as robustness in training.\n8\nControl-Informed Reinforcement Learning for Chemical Processes\nCreate and evaluate random initial\npolicy parameters, select maximum\nUpdate particle ve-\nlocities and positions\nEvaluate new positions\nUpdate personal and global best\nT iterations completed?\nReturn optimal policy parameters\nYes\nNo\nFigure 5: Block diagram of the policy optimization algorithm\n4\nResults and analysis\n4.1\nComputational Implementation\nThis section outlines the computational implementation employed in our study. We describe the state representation,\nneural network architecture, optimization parameters, and benchmark comparisons used to evaluate our proposed\napproach. Additionally, we provide information on the computational resources used. The RL state st representation\nused for the CIRL agent included Nt = 2 timesteps of history, which is the minimum number of timesteps to define the\nPID controller layer:\nst =\n\u0002\nxt, xt−1, xt−2, x∗\nt , x∗\nt−1, x∗\nt−2\n\u0003\n(15)\nThe neural network architecture of the CIRL agent consists of three fully connected layers, each containing 16 neurons\nwith ReLU activation functions, with the output being clamped to the normalised PID gain bounds. The CIRL agent is\ncompared to a pure-RL implementation. This pure-RL agent consists solely of a deep neural network without the PID\nlayer; we found this to require a larger network size and use three fully connected layers with 128 neurons. While other\narchitectures incorporating previous information, such as recurrent neural networks (e.g., LSTMs, GRUs), could be\nemployed, we opted for this simpler structure in the present study. For the PSO algorithm used for policy optimization,\nwe define the inertia weight w as 0.6, while both the cognitive and social acceleration constants (c1 and c2, respectively)\nare set to 1. The policy optimization algorithm is initialized with N = 30 policies, before starting the PSO loop for\nT = 150 iterations with np = 15 particles. In this PSO loop, ne = 3 episodes are used for each policy evaluation, with\nns = 120 timesteps in each rollout. All training was conducted on a 64-bit Windows laptop with an Intel i7-1355U\nCPU @ 3.7 GHz and an NVIDIA RTX A500 (Laptop) GPU. The CIRL agent required approximately 10 minutes of\ntraining time.\n9\nControl-Informed Reinforcement Learning for Chemical Processes\n4.2\nCSTR Case Study\nTo demonstrate the proposed algorithm, simulation-based experiments were carried out on a CSTR system (Figure\n6) where both the volume and temperature are controlled. Though conceptually simple, this case study represents\na non-trivial, multivariable system with nonlinear dynamics, capturing many challenges representative of those in\nreal-world processes.\nA → B→ C\n \nT,V\nTJ\nC, Fout\nTin, Cin, Fin\nFigure 6: CSTR Process Flow Diagram\nThe following generalized reactions take place in the reactor, where B is the desired component:\nA →\nra B →\nrb C\n(16)\nThe following system of ordinary differential equations models the dynamics of the three chemical components in\nthe reactor: CA (concentration of A in mol/m3), CB (concentration of B in mol/m3) and CC (concentration of C in\nmol/m3), respectively.\ndCA\ndt\n= FinCA,in −FoutCA\nV\n−ra\n(17)\ndCB\ndt\n= rb −ra −FoutCB\nV\n(18)\ndCC\ndt\n= rb −FoutCC\nV\n(19)\nwhere Fin is the volumetric flow of feed into the system (m3/min), CA,in is the feed concentration of species A\n(mol/m3), rj is the reaction rate for reaction j (mol/m3/min), and V is the volume of the CSTR (m3). For this subsystem\nto be fully defined, the reaction rates are described by Arrhenius relationships for both reactions:\nra = kae\nEa\nRT CA\n(20)\nrb = kbe\nEb\nRT CB\n(21)\nwhere ka, kb are the Arrhenius rate constants (s−1), EA, EB are the activation energies (J/mol), R is the universal gas\nconstant (8.314 J/mol.K), and T is the temperature (K). The dynamics of the reactor temperature T (K) and volume V\n(m3) are described by the following ordinary differential equations:\ndT\ndt = Fin(Tf −T)\nV\n+ ∆Ha\nρCp\nrA + ∆Hb\nρCp\nrB +\nUA\nV ρCp\n(Tc −T)\n(22)\ndV\ndt = Fin −Fout\n(23)\nwhere Tf is the inlet stream temperature (K), ∆HA, ∆HB are the heats of reaction (J/mol), ρ is the density of the\nsolvent (kg/L), Cp is the heat capacity (J/kg/K), U is the overall heat transfer coefficient (J/min/m2/K), A is the heat\n10\nControl-Informed Reinforcement Learning for Chemical Processes\nTable 1: Parameters for the CSTR dynamic model\nParameter\nValue\nTf\n350 K\nCA,in\n1 mol/m3\nFout\n100 m3/sec\nρ\n1000 kg/m3\nCp\n0.239 J/kg-K\nUA\n5 × 104 W/K\n∆Ha\n5 × 103 J/mol\nEa/R\n8750 K\nkb\n7.2 × 1010 s−1\n∆Hb\n4 × 103 J/mol\nEb/R\n10750 K\nkb\n8.2 × 1010 s−1\ntransfer area (m2), and Tc is the coolant temperature (K). The parameters used in the simulation experiments are shown\nin Table 1. The case study was implemented as a Gym environment [62] to provide a standardized format designed for\nRL algorithms. Within the gym environment, the system of ODEs are integrated using SciPy’s ODEInt method.\nThe three observed states of the reactor are the concentration of B CB, reactor temperature T, and volume V , which\ndefine the state vector x = [CB, T, V ]. We desire a policy that maps these to the action space, comprising the cooling\njacket temperature Tc and the inlet flow rate Fin, defining the control vector u = [Tj, Fin]. This creates a system with\ntwo PID controllers, the first pairs Tc and CB and the second pairs Fin and V .The pairing was decided using a Relative\nGain Array (RGA), which is shown in the appendix. This is additive measurement noise on all states of the CSTR. The\nsystem is simulated for 25 minutes with 120 timesteps. The bounds on the two control inputs are as follows uL = [290\nK, 99 m3/min] and uU = [450 K, 105 m3/min]. There are also bounds on the PID gains outputted by the DNN in the\nCIRL agent which are given in Table 2. The initial state is defined as x0 = [0 mol/m3, 327 K, 102 m3].\nTable 2: Bounds on PID gains\nCb-loop\nV -loop\nKp\n[-5 , 25] (K · m3/mol)\n[0 , 1] (s−1)\nτi\n[0 , 20] (s)\n[0 , 2] (s)\nτd\n[0 , 10] (s)\n[0 , 1] (s)\n4.3\nTraining\nThe CIRL and pure-RL algorithms were trained on nine setpoints that span the operating space of the CSTR case study\n(Figure 11). The operating space is defined for CB between 0.1 and 0.8 mol/m3 whilst maintaining a constant volume\nof 100 m3. This is with the aim to learn a generalized control policy for a wide range of Cb setpoints. Practically, this\nwas achieved by rollout the policy on the three sub-episodes (1-3 in Table 3) then summing them to create a single\nreward signal.\nTable 3: Training and Test Scenarios\nSub-Episode\nSetpoint Schedule\nCB [mol/m3]\nV [m3]\n1\n0.1 →0.25 →0.4\n100\n2\n0.55 →0.65 →0.75\n100\n3\n0.7 →0.75 →0.8\n100\nTest\n0.075→0.45 →0.75\n100\n11\nControl-Informed Reinforcement Learning for Chemical Processes\nAs mentioned above, we found that, without the PID layer, a larger DNN policy was required to reach comparable\nperformance. Therefore, the pure-RL algorithm implemented with a larger number of neurons (128) in each fully\nconnected layer still reaches comparable training performance to CIRL (Figure 7).\n0\n500\n1000\n1500\n2000\nEpisode\n−100\n−80\n−60\n−40\n−20\n0\nReward\nCIRL (16)\nRL (16)\nCIRL (128)\nRL (128)\nFigure 7: Learning curves for both RL and CIRL policies with 16 and 128 neurons per fully connected layer\nThe sample efficiency of RL algorithms is one of the main concerns with their implementation. Here, we demonstrate\nthe improved sample efficiency of CIRL compared to an RL algorithm with the PID controller removed. The CIRL\nagent can be seen to initialize at a higher reward than the pure-RL implementation, since it has prior knowledge of the\ncontrol strategy and benefits from the inherent stabilizing properties. This leads to more efficient and faster learning\ncompared to pure-RL approaches, as the agent can make informed decisions and requires fewer samples to learn a\ngood policy. In the real world, this corresponds to fewer simulations/experiments before an adequate control policy is\nobtained. Furthermore, given the stabilizing properties of the PID layer, this results in a safer policy, which inherently\nmaintains setpoint tracking by utilizing the setpoint error. The PID controller’s ability to continuously adjust based on\nthe error between the desired setpoint and the current state provides a fundamental safety mechanism. This makes the\noverall policy more robust and less prone to dangerous deviations, especially during the early stages of learning when\nthe neural network component might produce unreliable outputs.\nThe pure-RL agent, without any prior domain knowledge, needs to explore a larger number of samples, leading to\nslower convergence. This agent must learn the control strategy from scratch, including error correction and setpoint\ntracking that are inherently built into the CIRL approach. As a result, the pure-RL agent typically exhibits higher\nvariance in its actions during the early stages of training, as it explores a wider range of potentially suboptimal strategies.\nThe lack of a PID layer means that the neural network in the pure-RL approach is learning to output controls which is\ninherently a larger space than the PID-gain space. This often necessitates a larger network architecture, as seen in our\nimplementation with 128 neurons per layer, to capture the complexity of the control task. The increased network size,\nwhile providing more expressive power, also increases the dimensionality of the parameter space that must be optimized,\npotentially leading to longer training times and increased computational requirements. The learning curves over 75\niterations of the policy optimization algorithm for both the CIRL and pure-RL implementations are shown in Figure 8.\n12\nControl-Informed Reinforcement Learning for Chemical Processes\n0\n500\n1000\n1500\n2000\nEpisode\n−100\n−80\n−60\n−40\n−20\n0\nReward\nCIRL (16)\nRL (128)\nFigure 8: Setpoint tracking learning curves of CIRL and RL on 10 different seeds. Initial random search is omitted\n4.4\nSetpoint Tracking: Normal Operation\nWe then test both learned policies on a partially unseen setpoint-tracking task. Specifically, the trained policies are\nthen tested on a setpoint schedule detailed in Table 3, which consists of three setpoints, the first setpoint is outside\nthe training regime (shown in bold) and the other two interpolate between the training setpoints. The CIRL agent is\ncompared to the pure-RL agent described previously in Section 4.1 and a static PID controller. The static PID controller\nwas tuned with differential evolution strategy to find gains using the setpoints in the training regime (Table 3). The\ngains found for the static PID Controller are given in Table 4. Then these three controllers were simulated on the test\nscenario (Table 3) and shown in Figure 9.\nTable 4: PID Gains for the Static PID Controller\nCb-loop\nFin-loop\nKp\n3.09 K · m3/mol\n0.84 s−1\nτi\n0.03 s\n1.85 s\nτd\n0.83 s\n0.08 s\n0\n5\n10\n15\n20\n25\nTime (min)\n0.0\n0.2\n0.4\n0.6\nConcentration of B, CB (mol/m3)\nRL\nCIRL\nStatic PID\n0\n5\n10\n15\n20\n25\nTime (min)\n100.0\n100.5\n101.0\n101.5\n102.0\nVolume, V (m3)\n0\n5\n10\n15\n20\n25\nTime (min)\n300\n310\n320\n330\n340\n350\n360\n370\nCooling Temperature, Tc (K)\nRL\nCIRL\nStatic PID\n0\n5\n10\n15\n20\n25\nTime (min)\n99.0\n99.2\n99.4\n99.6\n99.8\n100.0\nFlowrate in, Fin (m3/s)\nFigure 9: Setpoint tracking test scenario states and control inputs for CIRL, pure-RL and static PID\nA conventional model-free implementation of deep RL (pure-RL in Figure 9 exhibited poor tracking when generalising\nto these out-of-distribution setpoints (x∗\nCB = 0.075 mol/m3) shown by the larger lower test reward (Table 5). By\nmanipulating the proportional, integral, and derivative terms of its internal PID controller (Figure 10), the CIRL\npolicy could adapt its control outputs to track previously unseen setpoint trajectories. This ability to adaptively tune\nthe PID gains allowed CIRL to outperform not only the model-free RL baseline approach, but also a static PID\ncontroller tuned to the setpoints in the training data. These results highlight the key benefit of the control-informed RL\napproach: integrating interpretable control structures like PID into deep RL enables performance gains compared to\neither component in isolation.\n13\nControl-Informed Reinforcement Learning for Chemical Processes\n0\n5\n10\n15\n20\n25\nTime (min)\n0\n2\n4\n6\n8\n10\n12\n14\nCB-loop PID Gains\nCIRL kp\nConstant kp\nCIRL τi\nConstant τi\nCIRL τd\nConstant τd\n0\n5\n10\n15\n20\n25\nTime (min)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nV -loop PID Gains\nFigure 10: Gain trajectories for the CB and V loop controllers\nTable 5: Final Test Reward for Pure-RL, CIRL and static PID\nMethod\nTest Reward\nRL\n-2.08\nCIRL\n-1.33\nStatic PID\n-1.77\n4.5\nSetpoint Tracking: High Operating Point\nThe CIRL algorithm does outperform the static controller in normal operation; however, the benefits are marginal and\ncould potentially be attributed to an over-tuned controller. We now consider a more challenging operating scenario:\nif the operating point is pushed to a region of the operating space (red triangle in Figure 11) the gradient changes\nsignificantly, as can be seen at cooling temperatures above 390 K. This is due to the second reaction rate increasing\nand consuming species B. This also poses a problem to the PID controller and PID layer in CIRL since to maximise\nthe concentration of species B, the proportional gain must decrease and potentially change sign to stabilize around the\nmaximum.\n300\n320\n340\n360\n380\n400\n420\n440\nCooling Temperature, TC\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nConcentration of species B, CB\nInitial Training Setpoints\nExtended Training Setpoint\nFigure 11: Operating space at a fixed V = 100 m3 with initial and extended training setpoints.\nThis scenario is explored by testing on a new setpoint schedule for species B of 0.45 to 0.88 mol/m3 (Figure 12). This\nhigh operating point scenario shows both the initial CIRL agent, as trained with a schedule shown in Table 3, and the\nstatic PID controller both enter a closed-loop unstable regime since their gains remain at a large positive value. To\nattempt to negate this problem, the CIRL agent is trained on an extended training regime which includes the maximum\n14\nControl-Informed Reinforcement Learning for Chemical Processes\nTable 6: Final Test Reward for Pure-RL, CIRL and static PID\nMethod\nTest Reward\nCIRL (initial)\n-4.04\nCIRL (Extended)\n-2.07\nStatic PID\n-6.81\nof the operating region. This agent with an extended training regime decreases the proportional gain (CIRL Extended in\nFigure 13) which stabilizes the response of the controller.\n0\n5\n10\n15\n20\n25\nTime (min)\n0.0\n0.2\n0.4\n0.6\n0.8\nConcentration of B, CB (mol/m3)\nCIRL (Extended)\nCIRL (Initial)\nStatic PID\n0\n5\n10\n15\n20\n25\nTime (min)\n300\n320\n340\n360\n380\n400\n420\n440\nCooling Temperature, Tc (K)\nCIRL (Extended)\nCIRL (Initial)\nStatic PID\nFigure 12: Setpoint tracking with the static PID control, initial, and extended training CIRL\n0\n5\n10\n15\n20\n25\nTime (min)\n0\n5\n10\n15\n20\nCB-loop PID Gains\nCIRL (Initial) kp\nCIRL (Extended) kp\nConstant kp\nCIRL (Initial) τi\nCIRL (Extended) τi\nConstant τi\nCIRL (Initial) τd\nCIRL (Extended) τd\nConstant τd\nFigure 13: Gain Trajectories for the static PID control, initial, and extended training CIRL\nThe scenario at high operating points reveals a limitation of the initial CIRL agent, as it enters an unstable closed-loop\nregime similar to the static PID controller due to the significant changes in gradient at cooling temperatures above 390\nK. Nevertheless, the adaptability of the deep RL component of the CIRL framework is demonstrated by extending the\ntraining regime to include the upper limits of the operating space, allowing the agent to learn and adjust its control\nstrategy, particularly by reducing the proportional gain, to maintain stability and achieve the desired setpoint even in the\npresence of these challenging conditions as shown by the higher test reward in Table 6.\n15\nControl-Informed Reinforcement Learning for Chemical Processes\n4.6\nDisturbance Rejection\nWe now turn to evaluate the ability of the learned policies to reject disturbances. In particular, the CIRL algorithm is\nalso tested on a scenario where there is a (unmeasured) step-change to the feed concentration of species A (CA,in).\nSimilar to the setpoint tracking case study, the CIRL algorithm is trained on multiple disturbance sub-episodes (Table\n7). Then the trained agent is tested only on interpolation within this training regime.\nTable 7: Training and Test Scenarios\nSub-Episode\nDisturbance\nCA,in [mol/m3]\n1\n1.5\n2\n1.6\n3\n1.9\nTest\n1.75\n0\n5\n10\n15\n20\n25\nTime (min)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nConcentration of B, CB (mol/m3)\n0\n5\n10\n15\n20\n25\nTime (min)\n300\n305\n310\n315\n320\n325\n330\n335\n340\nCooling Temperature, TC (K)\nRL (non-obs)\nCIRL (non-obs)\nFigure 14: Disturbance rejection test scenario states and control inputs for CIRL, pure-RL with nonobservable\ndisturbance\nUnder this disturbance condition, which effectively changes the underlying system dynamics, CIRL demonstrates a\ngood ability to reject the disturbance and maintain the desired setpoint tracking performance. This disturbance rejection\ncapability stems from CIRL’s integrated PID control structure. The PID components in CIRL continuously measure\nand respond to the error between the setpoint and the actual system output, allowing it to adapt to and counteract\nunexpected disturbances in real-time, even if they were not explicitly modelled during training. Conversely, the pure-RL\nimplementation exhibits poor setpoint tracking when faced with dynamics outside its training distribution (Table 8).\nWithout an explicit mechanism to handle disturbances, it settles for a compromised policy, i.e., sacrificing setpoint\ntracking performance both before and after the test disturbance occurred. This highlights that the addition of the PID\ncomponents to the CIRL provide robustness to unmodeled disturbances. Unlike the pure-RL approach that attempts to\nanticipate and learn responses to all possible disturbances during training, CIRL’s PID feedback mechanism allows it\nto adapt to unforeseen disturbances by using the measured error instead of modelling the response to the disturbance,\ndemonstrating the fundamental advantage of closed-loop control in handling system uncertainties.\nTable 8: Final Test Reward for Pure-RL, CIRL and static PID\nMethod\nTest Reward\nCIRL\n-1.38\npure-RL\n-1.76\n16\nControl-Informed Reinforcement Learning for Chemical Processes\n5\nConclusion\nThis paper presents a control-informed reinforcement learning (CIRL) framework that integrates classical PID control\nstructures into deep RL policies. A case study on a simulated CSTR demonstrates that CIRL outperforms both\nmodel-free deep RL and static PID controllers, particularly when tested on dynamics outside the training regime.\nThe key advantage of CIRL lies in the embedded control structure, which allows for greater sample efficiency and\ngeneralizability. By incorporating the inductive biases of the PID controller layer, CIRL can learn effective control\npolicies with fewer samples and adapt to novel scenarios more robustly than pure model-free RL approaches.\nFuture work may seek to incorporate additional existing information regarding existing PID infrastructure. For example,\nas a pre-processing step in the algorithm, the neural network could be initialized via offline reinforcement learning or\nbehavioral cloning from past polices, potentially leveraging preexisting gain schedules in the plant. This initialization\ncould potentially improve the starting point for the CIRL framework and accelerate learning. Another direction may be\nenabling gradient-based training of CIRL agent by investigating the end-to-end differentiability of the PID controller\nlayer.\nThe proposed CIRL framework opens up exciting research directions at the intersection of control theory and machine\nlearning. This combination seeks to benefit from the best of both worlds, merging the known disturbance-rejection and\nsetpoint-tracking capabilities of PID control with the generalization abilities of machine learning. Further investiga-\ntions into theoretical guarantees, and online adaptation schemes have the potential to enhance the sample efficiency,\ngeneralization, and real-world deployability of deep RL algorithms for control applications across various industries.\n6\nAcknowledgements\nMaximilian Bloor would like to acknowledge funding provided by the Engineering & Physical Sciences Research\nCouncil, United Kingdom through grant code EP/W524323/1. Calvin Tsay acknowledges support from a BASF/Royal\nAcademy of Engineering Senior Research Fellowship\n7\nSupplementary Information\nThe code and data used within this work are available at https://github.com/OptiMaL-PSE-Lab/CIRL.\n17\nControl-Informed Reinforcement Learning for Chemical Processes\nReferences\n[1]\nJodie M Simkoff et al. “Process control and energy efficiency”. In: Annual Review of Chemical and Biomolecular\nEngineering 11.1 (2020), pp. 423–445.\n[2]\nLane Desborough and Randy Miller. “Increasing Customer Value of Industrial Control Performance Mon-\nitoring—Honeywell’s Experience”. In: 2002. URL: https://api.semanticscholar.org/CorpusID:\n14892619.\n[3]\nDale E Seborg et al. Process dynamics and control. John Wiley & Sons, 2016.\n[4]\nShuichi Yahagi and Itsuro Kajiwara. “Noniterative Data-Driven Gain-Scheduled Controller Design Based on\nFictitious Reference Signal”. In: IEEE Access 11 (2023), pp. 55883–55894. DOI: 10.1109/ACCESS.2023.\n3278798.\n[5]\nMichael G Forbes et al. “Model predictive control in industry: Challenges and opportunities”. In: IFAC-\nPapersOnLine 48.8 (2015), pp. 531–538.\n[6]\nManfred Morari and Jay H. Lee. “Model predictive control: past, present and future”. In: Computers & Chemical\nEngineering 23.4 (1999), pp. 667–682. ISSN: 0098-1354. DOI: https://doi.org/10.1016/S0098-1354(98)\n00301-9. URL: https://www.sciencedirect.com/science/article/pii/S0098135498003019.\n[7]\nStephen Piche et al. “Nonlinear model predictive control using neural networks”. In: IEEE Control Systems\nMagazine 20.3 (2000), pp. 53–62.\n[8]\nJus Kocijan et al. “Gaussian process model based predictive control”. In: Proceedings of the 2004 American\ncontrol conference. Vol. 3. IEEE. 2004, pp. 2214–2219.\n[9]\nZhihao Zhang et al. “Real-time optimization and control of nonlinear processes using machine learning”. In:\nMathematics 7.10 (2019), p. 890.\n[10]\nFarshud Sorourifar et al. “A data-driven automatic tuning method for MPC under uncertainty using constrained\nBayesian optimization”. In: IFAC-PapersOnLine 54.3 (2021), pp. 243–250.\n[11]\nRui Nian, Jinfeng Liu, and Biao Huang. “A review on reinforcement learning: Introduction and applications in\nindustrial process control”. In: Computers & Chemical Engineering 139 (2020), p. 106886.\n[12]\nNathan P. Lawrence et al. “Machine learning for industrial sensing and control: A survey and practical perspec-\ntive”. In: Control Engineering Practice 145 (2024), p. 105841. ISSN: 0967-0661. DOI: https://doi.org/10.\n1016/j.conengprac.2024.105841. URL: https://www.sciencedirect.com/science/article/pii/\nS0967066124000017.\n[13]\nShangding Gu et al. “A review of safe reinforcement learning: Methods, theory and applications”. In: arXiv\npreprint arXiv:2205.10330 (2022).\n[14]\nThomas A Badgwell, Jay H Lee, and Kuang-Hung Liu. “Reinforcement learning–overview of recent progress\nand implications for process control”. In: Computer Aided Chemical Engineering 44 (2018), pp. 71–85.\n[15]\nSergey Levine et al. “End-to-End Training of Deep Visuomotor Policies”. In: Journal of Machine Learning\nResearch 17.39 (2016), pp. 1–40. URL: http://jmlr.org/papers/v17/15-522.html.\n[16]\nDavid Silver et al. “Mastering the game of Go without human knowledge”. In: Nature 550 (7676 2017), pp. 354–\n359. ISSN: 1476-4687. DOI: 10.1038/nature24270. URL: https://doi.org/10.1038/nature24270.\n[17]\nWentao Tang and Prodromos Daoutidis. “Distributed adaptive dynamic programming for data-driven optimal\ncontrol”. In: Systems & Control Letters 120 (2018), pp. 36–43. ISSN: 0167-6911. DOI: https://doi.org/10.\n1016/j.sysconle.2018.08.002. URL: https://www.sciencedirect.com/science/article/pii/\nS0167691118301476.\n[18]\nRadu Burtea and Calvin Tsay. “Constrained continuous-action reinforcement learning for supply chain inventory\nmanagement”. In: Computers & Chemical Engineering 181 (2024), p. 108518. ISSN: 0098-1354. DOI: https:\n//doi.org/10.1016/j.compchemeng.2023.108518. URL: https://www.sciencedirect.com/\nscience/article/pii/S0098135423003885.\n[19]\nPanagiotis Petsagkourakis et al. “Chance constrained policy optimization for process control and optimization”.\nIn: Journal of Process Control 111 (2022), pp. 35–45. ISSN: 0959-1524. DOI: https://doi.org/10.\n1016/j.jprocont.2022.01.003. URL: https://www.sciencedirect.com/science/article/pii/\nS0959152422000038.\n[20]\n“A dynamic penalty approach to state constraint handling in deep reinforcement learning”. In: Journal of Process\nControl 115 (2022), pp. 157–166. ISSN: 0959-1524. DOI: https://doi.org/10.1016/j.jprocont.2022.\n05.004. URL: https://www.sciencedirect.com/science/article/pii/S0959152422000816.\n[21]\nMarwan Mousa et al. An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control\nSystems. 2023. arXiv: 2307.11432 [cs.LG]. URL: https://arxiv.org/abs/2307.11432.\n18\nControl-Informed Reinforcement Learning for Chemical Processes\n[22]\nP. Petsagkourakis et al. “Reinforcement learning for batch bioprocess optimization”. In: Computers & Chemical\nEngineering 133 (2020), p. 106649. ISSN: 0098-1354. DOI: https://doi.org/10.1016/j.compchemeng.\n2019.106649. URL: https://www.sciencedirect.com/science/article/pii/S0098135419304168.\n[23]\nHaeun Yoo et al. “Reinforcement learning for batch process control: Review and perspectives”. In: Annual Re-\nviews in Control 52 (2021), pp. 108–119. ISSN: 1367-5788. DOI: https://doi.org/10.1016/j.arcontrol.\n2021.10.006. URL: https://www.sciencedirect.com/science/article/pii/S136757882100081X.\n[24]\nWenbo Zhu et al. “Benchmark study of reinforcement learning in controlling and optimizing batch processes”. In:\nJournal of Advanced Manufacturing and Processing 4.2 (2022), e10113. DOI: https://doi.org/10.1002/\namp2.10113. URL: https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/amp2.10113.\n[25]\nChristian D. Hubbs et al. “A deep reinforcement learning approach for chemical production scheduling”. In:\nComputers & Chemical Engineering 141 (2020), p. 106982. ISSN: 0098-1354. DOI: https://doi.org/10.\n1016/j.compchemeng.2020.106982. URL: https://www.sciencedirect.com/science/article/\npii/S0098135420301599.\n[26]\nTobi Michael Alabi et al. “Automated deep reinforcement learning for real-time scheduling strategy of multi-\nenergy system integrated with post-carbon and direct-air carbon captured system”. In: Applied Energy 333\n(2023), p. 120633. ISSN: 0306-2619. DOI: https://doi.org/10.1016/j.apenergy.2022.120633. URL:\nhttps://www.sciencedirect.com/science/article/pii/S0306261922018906.\n[27]\nNiket S Kaisare, Jong Min Lee, and Jay H Lee. “Simulation based strategy for nonlinear optimal control:\nApplication to a microbial cell reactor”. In: International Journal of Robust and Nonlinear Control: IFAC-\nAffiliated Journal 13.3-4 (2003), pp. 347–363.\n[28]\nJA Wilson and EC Martinez. “Neuro-fuzzy modeling and control of a batch process involving simultaneous\nreaction and distillation”. In: Computers & chemical engineering 21 (1997), S1233–S1238.\n[29]\nCatalina Valencia Peroni, Niket S Kaisare, and Jay H Lee. “Optimal control of a fed-batch bioreactor using\nsimulation-based approximate dynamic programming”. In: IEEE Transactions on Control Systems Technology\n13.5 (2005), pp. 786–790.\n[30]\nMax Mowbray et al. “Using process data to generate an optimal control policy via apprenticeship and reinforce-\nment learning”. In: AIChE Journal 67.9 (2021), e17306.\n[31]\nDerek Machalek, Titus Quah, and Kody M. Powell. “A novel implicit hybrid machine learning model and\nits application for reinforcement learning”. In: Computers & Chemical Engineering 155 (2021), p. 107496.\nISSN: 0098-1354. DOI: https://doi.org/10.1016/j.compchemeng.2021.107496. URL: https:\n//www.sciencedirect.com/science/article/pii/S009813542100274X.\n[32]\nLingwei Zhu et al. “Scalable reinforcement learning for plant-wide control of vinyl acetate monomer process”.\nIn: Control Engineering Practice 97 (2020), p. 104331.\n[33]\nAthindran Ramesh Kumar and Peter J Ramadge. “DiffLoop: Tuning PID controllers by differentiating through\nthe feedback loop”. In: 2021 55th Annual Conference on Information Sciences and Systems (CISS). IEEE. 2021,\npp. 1–6.\n[34]\nAyub I Lakhani, Myisha A Chowdhury, and Qiugang Lu. “Stability-preserving automatic tuning of PID control\nwith reinforcement learning”. In: arXiv preprint arXiv:2112.15187 (2021).\n[35]\nMostafa Sedighizadeh and Alireza Rezazadeh. “Adaptive PID controller based on reinforcement learning for\nwind turbine control”. In: Proceedings of world academy of science, engineering and technology. Vol. 27.\nCiteseer. 2008, pp. 257–262.\n[36]\nJay H Lee and Jong Min Lee. “Approximate dynamic programming based approach to process control and\nscheduling”. In: Computers & Chemical Engineering 30.10-12 (2006), pp. 1603–1618.\n[37]\nLena Abbasi Brujeni, Jong Min Lee, and Sirish L Shah. Dynamic tuning of PI-controllers based on model-free\nreinforcement learning methods. IEEE, 2010.\n[38]\nMarcus AR Berger and JoÃo Viana da Fonseca Neto. “Neurodynamic programming approach for the PID\ncontroller adaptation”. In: IFAC Proceedings Volumes 46.11 (2013), pp. 534–539.\n[39]\nR.R. Faria et al. “A data-driven tracking control framework using physics-informed neural networks and deep\nreinforcement learning for dynamical systems”. In: Engineering Applications of Artificial Intelligence 127\n(2024), p. 107256. ISSN: 0952-1976. DOI: https://doi.org/10.1016/j.engappai.2023.107256. URL:\nhttps://www.sciencedirect.com/science/article/pii/S0952197623014409.\n[40]\nJoel A Paulson, Farshud Sorourifar, and Ali Mesbah. “A tutorial on derivative-free policy learning methods for\ninterpretable controller representations”. In: 2023 American Control Conference (ACC). IEEE. 2023, pp. 1295–\n1306.\n[41]\nNathan P Lawrence et al. “Deep reinforcement learning with shallow controllers: An experimental application to\nPID tuning”. In: Control Engineering Practice 121 (2022), p. 105046.\n19\nControl-Informed Reinforcement Learning for Chemical Processes\n[42]\nDaniel G. McClement et al. “Meta-reinforcement learning for the tuning of PI controllers: An offline approach”.\nIn: Journal of Process Control 118 (2022), pp. 139–152. ISSN: 0959-1524. DOI: https://doi.org/10.\n1016/j.jprocont.2022.08.002. URL: https://www.sciencedirect.com/science/article/pii/\nS0959152422001445.\n[43]\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Cambridge, MA, USA: A\nBradford Book, 2018. ISBN: 0262039249.\n[44]\nRichard S Sutton et al. “Policy gradient methods for reinforcement learning with function approximation”. In:\nAdvances in neural information processing systems 12 (1999).\n[45]\nRonald J Williams. “Simple statistical gradient-following algorithms for connectionist reinforcement learning”.\nIn: Mach. Learn. 8.3/4 (1992), pp. 229–256.\n[46]\nJohn Schulman et al. “Trust region policy optimization”. In: Proceedings of the 32nd International Conference\non International Conference on Machine Learning - Volume 37. ICML’15. Lille, France: JMLR.org, 2015,\npp. 1889–1897.\n[47]\nVolodymyr Mnih et al. Playing Atari with Deep Reinforcement Learning. 2013. arXiv: 1312.5602 [cs.LG].\n[48]\nMoonkyung Ryu et al. CAQL: Continuous Action Q-Learning. 2020. arXiv: 1909.12397 [cs.LG]. URL:\nhttps://arxiv.org/abs/1909.12397.\n[49]\nRadu Burtea and Calvin Tsay. “Constrained continuous-action reinforcement learning for supply chain inventory\nmanagement”. In: Computers & Chemical Engineering 181 (2024), p. 108518. ISSN: 0098-1354. DOI: https:\n//doi.org/10.1016/j.compchemeng.2023.108518. URL: https://www.sciencedirect.com/\nscience/article/pii/S0098135423003885.\n[50]\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error in Actor-Critic\nMethods. 2018. arXiv: 1802.09477 [cs.AI].\n[51]\nTuomas Haarnoja et al. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a\nStochastic Actor. 2018. arXiv: 1801.01290 [cs.LG].\n[52]\nDaan Wierstra et al. “Natural Evolution Strategies”. In: Journal of Machine Learning Research 15.27 (2014),\npp. 949–980.\n[53]\nTim Salimans et al. Evolution Strategies as a Scalable Alternative to Reinforcement Learning. 2017. arXiv:\n1703.03864 [stat.ML].\n[54]\nGuoquan Wu, Miguel Ángel de Carvalho Servia, and Max Mowbray. “Distributional reinforcement learning for\ninventory management in multi-echelon supply chains”. In: Digital Chemical Engineering 6 (2023), p. 100073.\nISSN: 2772-5081. DOI: https://doi.org/10.1016/j.dche.2022.100073.\n[55]\nJohn Schulman et al. Proximal Policy Optimization Algorithms. 2017. arXiv: 1707.06347 [cs.LG].\n[56]\nDaniel E. Rivera, Manfred Morari, and Sigurd Skogestad. “Internal model control: PID controller design”.\nIn: Industrial & Engineering Chemistry Process Design and Development 25.1 (1986), pp. 252–265. DOI:\n10.1021/i200032a041.\n[57]\nSigurd Skogestad. “Simple analytic rules for model reduction and PID controller tuning”. In: Journal of\nProcess Control 13.4 (2003), pp. 291–309. ISSN: 0959-1524. DOI: https://doi.org/10.1016/S0959-\n1524(02)00062-8.\n[58]\nK.J. Åström and T. Hägglund. “Automatic tuning of simple regulators with specifications on phase and amplitude\nmargins”. In: Automatica 20.5 (1984), pp. 645–651. ISSN: 0005-1098. DOI: https://doi.org/10.1016/0005-\n1098(84)90014-1.\n[59]\nErtugrul Baris Ondes et al. “Model-based 2-D look-up table calibration tool development”. In: 2017 11th Asian\nControl Conference (ASCC). 2017, pp. 1011–1016. DOI: 10.1109/ASCC.2017.8287309.\n[60]\nM.C. Campi, A. Lecchini, and S.M. Savaresi. “Virtual reference feedback tuning: a direct method for the\ndesign of feedback controllers”. In: Automatica 38.8 (2002), pp. 1337–1346. ISSN: 0005-1098. DOI: https:\n//doi.org/10.1016/S0005-1098(02)00032-8.\n[61]\nJ. Kennedy and R. Eberhart. “Particle swarm optimization”. In: Proceedings of ICNN’95 - International Confer-\nence on Neural Networks. Vol. 4. 1995, 1942–1948 vol.4. DOI: 10.1109/ICNN.1995.488968.\n[62]\nMark Towers et al. Gymnasium. Mar. 2023. DOI: 10.5281/zenodo.8127026. URL: https://zenodo.org/\nrecord/8127025 (visited on 07/08/2023).\n20\nControl-Informed Reinforcement Learning for Chemical Processes\nA\nPolicy Optimization Algorithm\nAlgorithm 2: Policy optimization\nInput: N: number of initial policies, np: number of particles, ne: number of episodes per evaluation, ns:\nnumber of steps per episode, T: number of iterations\nOutput: Optimal policy parameters θ∗\n1 Initialize {θi}N\ni=1 randomly\n// Initialize population\n2 for i ∈{1, . . . , N} do\n3\nfi ←0\n// Initialize fitness\n4\nfor j ∈{1, . . . , ne} do\n5\nRj ←CIRLRollout(θi, ns, f)\n// Run Algorithm 1:\nCIRL Rollout\n6\nfi ←fi + Rj\n// Accumulate rewards\n7\nend\n8\nfi ←fi/ne\n// Average fitness over episodes\n9 end\n10 x ←argmax\nθi\nfi\n// Select the best policy w.r.t.\nreward\n11 g ←x\n// Initialize global best\n12 p ←x\n// Initialize personal best position\n13 for t ∈{1, . . . , T} do\n14\nfor i ∈{1, . . . , np} do\n15\nvt+1\ni\n←Eq. (13)\n// Update particle velocity\n16\nxt+1\ni\n←Eq. (14)\n// Update particle position\n17\nf t+1\ni\n←0\n// Initialize fitness for new position\n18\nfor j ∈{1, . . . , ne} do\n19\nRj ←CIRLRollout(xt+1\ni\n, ns, f)\n// Run Algorithm 1:\nCIRL Rollout\n20\nf t+1\ni\n←f t+1\ni\n+ Rj\n// Accumulate rewards\n21\nend\n22\nf t+1\ni\n←f t+1\ni\n/ne\n// Average fitness over episodes\n23\nif f t+1\ni\n> f(pi) then\n24\npi ←xt+1\ni\n// Update personal best if necessary\n25\nend\n26\nif f t+1\ni\n> f(g) then\n27\ng ←xt+1\ni\n// Update global best if necessary\n28\nend\n29\nend\n30 end\n31 return θ∗= g\n// Return optimal policy parameters\nB\nRGA Matrix\nThe RGA matrix used for controller pairing is displayed below using averaged gains over three repetitions:\nRGA =\n\u0014\n0.0003\n0.9997\n0.9997\n0.0003\n\u0015\n(24)\nThe values in the RGA matrix suggest a strong pairing between the first controlled variable (CB the concentration of B)\nand the second manipulated variable (Tc, the cooling temperature), and between the second controlled variable (V , the\nvolume) and the first manipulated variable (Fin, the inlet flow rate).\n21\n",
  "categories": [
    "eess.SY",
    "cs.SY"
  ],
  "published": "2024-08-24",
  "updated": "2024-08-27"
}