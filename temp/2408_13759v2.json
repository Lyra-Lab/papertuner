{
  "id": "http://arxiv.org/abs/2408.13759v2",
  "title": "MASQ: Multi-Agent Reinforcement Learning for Single Quadruped Robot Locomotion",
  "authors": [
    "Qi Liu",
    "Jingxiang Guo",
    "Sixu Lin",
    "Shuaikang Ma",
    "Jinxuan Zhu",
    "Yanjie Li"
  ],
  "abstract": "This paper proposes a novel method to improve locomotion learning for a\nsingle quadruped robot using multi-agent deep reinforcement learning (MARL).\nMany existing methods use single-agent reinforcement learning for an individual\nrobot or MARL for the cooperative task in multi-robot systems. Unlike existing\nmethods, this paper proposes using MARL for the locomotion learning of a single\nquadruped robot. We develop a learning structure called Multi-Agent\nReinforcement Learning for Single Quadruped Robot Locomotion (MASQ),\nconsidering each leg as an agent to explore the action space of the quadruped\nrobot, sharing a global critic, and learning collaboratively. Experimental\nresults indicate that MASQ not only speeds up learning convergence but also\nenhances robustness in real-world settings, suggesting that applying MASQ to\nsingle robots such as quadrupeds could surpass traditional single-robot\nreinforcement learning approaches. Our study provides insightful guidance on\nintegrating MARL with single-robot locomotion learning.",
  "text": "MASQ: Multi-Agent Reinforcement Learning for\nSingle Quadruped Robot Locomotion\nQi Liu†, Jingxiang Guo†, Sixu Lin, Shuaikang Ma, Jinxuan Zhu, Yanjie Li*\nAbstract— This paper proposes a novel method to improve\nlocomotion for a single quadruped robot using multi-agent\ndeep reinforcement learning (MARL). Many existing methods\nuse single-agent reinforcement learning for an individual robot\nor MARL for the cooperative task in multi-robot systems.\nUnlike existing methods, this paper proposes using MARL\nfor the locomotion of a single quadruped robot. We pro-\npose a learning structure called Multi-Agent Reinforcement\nLearning for Single Quadruped Robot Locomotion (MASQ),\nconsidering each leg as an agent to explore the action space\nof the quadruped robot, sharing a global critic, and learning\ncooperatively. Experimental results show that MASQ not only\nspeeds up learning convergence but also enhances robustness in\nreal-world settings, suggesting that applying MASQ to single\nrobots such as quadrupeds could surpass traditional single-\nrobot reinforcement learning approaches. Our study provides\ninsightful guidance on integrating MARL with single-robot\nlocomotion.\nI. INTRODUCTION\nReinforcement\nlearning\n(RL)\nhas\nmade\nremarkable\nprogress in various robot control learning [1], such as\nquadruped robot [2], [3], biped robot [4], [5], and unmanned\naerial vehicle [6]. This paper focuses on the quadruped robot\ncontrol learning.\nIn the process of applying deep RL to a single robot, it is\nprevalent to use single-agent algorithms [7]–[10]. However,\nsingle-agent algorithms may have limitations in managing\ncoordination in specific problems. Many existing methods\nuse single-agent RL algorithms for individual robot learning\nor multi-agent deep reinforcement learning (MARL) for\nmulti-robot systems in cooperative tasks [11]–[13]. Coop-\nerative MARL algorithms have been widely demonstrated\nin game-AI [14], [15] to have advantages in multi-agent\ncooperation.\nUnlike existing approaches, this paper proposes using\nMARL for the locomotion of a single quadruped robot to\nenhance cooperation between its four legs, thereby enabling\nit to navigate complex terrains and perform intricate tasks.\nBy proposing cooperative MARL, where each leg acts as an\nagent, the quadruped robot can better coordinate its move-\nments. This collaborative learning structure, termed Multi-\nThis\nwork\nwas\nsupported\nby\nthe\nNational\nNatural\nScience\nFoundation of China [61977019, U1813206] and Shenzhen Fundamental\nResearch\n[JCYJ20220818102415033,\nJSGG20201103093802006,\nKJZD20230923114222045].\n(Corresponding\nauthor:\nYanjie\nLi,\nautolyj@hit.edu.cn)\nThe authors are with the Guangdong Key Laboratory of Intelligent\nMorphing Mechanisms and Adaptive Robotics and the School of Mechanical\nEngineering and Automation, the Harbin Institute of Technology Shenzhen,\n518055, China.\n* Corresponding author.\n† Equal contribution.\n(a) Dog on grass\n(b) Dog on rock\n(c) Dog on flat\n(d) Dog on rubber track\nFig. 1: Quadruped robot on various terrains\nAgent Reinforcement Learning for Single Quadruped Robot\nLocomotion (MASQ), allows the robot to share experiences.\nFig. 1 presents the deployment of the MASQ algorithm on\na quadruped robot tested across different terrains, including\ngrass, rocks, flat surfaces, and rubber tracks, demonstrating\nits performance in diverse environments. Fig. 2 shows a\nsim-to-real comparison of trot gaits in a quadruped robot,\nhighlighting the consistency between simulated and real-\nworld gait patterns.\nThe main contributions of this paper can be summarized\nas follows:\n• This paper proposes MASQ, a method that treats each\nleg of a quadruped robot as an individual agent. The\nlocomotion learning is modeled as a cooperative multi-\nagent reinforcement learning (MARL) problem and\nsolved using a MARL algorithm.\n• Experimental results show that the proposed method\nenhances performance in executing gaits, improves\ntraining efficiency and robustness, and achieves better\nfinal performance, demonstrating the value and impact\nof the approach.\nII. RELATED WORK\nA. Deep RL for Single Robot Control\nRecent advances in deep RL for quadruped robots are\ndriven by simulation technologies such as Isaac Gym [16],\narXiv:2408.13759v2  [cs.RO]  17 Oct 2024\n(a) Trot gaits in real-world\n(b) Trot gaits in simulation\nFig. 2: Sim-to-Real comparison of trot gaits\n[17]. Hardware advancements have shown robust perfor-\nmance on various tasks through Sim2Real transfer with zero\nshot [18]–[21]. Current research focuses on task-specific\nreward composition and training paradigms to bridge the\nSim2Real gap [22]–[24], often using Proximal Policy Op-\ntimization (PPO) [25] with an emphasis on task and reward\ndesign rather than novel RL algorithms. Although some stud-\nies have explored learning algorithms to improve efficiency\n[26]–[31], most efforts remain centered on efficiency rather\nthan modifying algorithms to control the characteristics\nof the object. Additionally, research has delved into task-\nspecific strategies like rapid motor adaptation (RMA) [22],\nhierarchical RL for multi-skill tasks [32], and symmetry-\nbased data enhancement [33], [34], yet challenges remain\nin using robot symmetry through single-agent methods,\nindicating that algorithmic research on this aspect is still\nunderdeveloped.\nB. MARL for Multi-robot Control\nIn multi-agent settings, algorithms like Multi-Agent Prox-\nimal Policy Optimization (MAPPO) [35], Temporally Ex-\ntended Multi-Agent Reinforcement Learning (TEMP) [36]\nhave demonstrated strong capabilities in addressing multi-\nagent robotic challenges, such as drone fleet control [37],\n[38] and autonomous vehicle fleets [39], [40]. Methods like\nReinforced Inter-Agent Learning (RIAL) and Differentiable\nInter-Agent Learning (DIAL) [41] further enhance collab-\norative performance by developing communication proto-\ncols among agents. This paper proposes modeling single-\nquadruped robot locomotion as a cooperative MARL prob-\nlem, where each leg is treated as an independent agent,\ncontrasting with previous approaches that treat the robot as a\nmonolithic entity [42]–[45] or cooperative groups of multi-\nrobots [12], [46], [47].\nIII. PRELIMINARIES\nThis paper considers a finite-horizon Markov decision\nprocess (MDP) [48], defined by a tuple (S, A, P, r, γ, T). S\ndenotes the state space, A := {a0, a1, ..., a|A|−1} represents\na finite action space, P : S × A × S →[0, 1] represents\nthe staåte transition distribution, r : S × A →R denotes a\nreward function, γ ∈[0, 1) denotes a discount factor, and T\nis a time horizon. At each time step t, the policy π selects an\naction at ∈A. After entering the next state by sampling from\nP (st+1 | st, at), the agent receives an immediate reward\nr (st, at). The agent continues to perform actions until it\nenters a terminal state or t reaches the time horizon T. RL\naims to learn a policy π : S×A →[0, 1] for decision-making\nproblems by maximizing discounted rewards. For any policy\nπ, the state-action value function (Q function) is defined as\nQπ(s, a) = Eπ\n\" T\nX\nt=0\nγtr (st, at) | s0 = s, a0 = a\n#\n(1)\nProximal Policy Optimization (PPO) [25] enhances the\nstability and performance of policy gradient methods by\nlimiting policy updates to prevent destabilizing deviations.\nThe core PPO update rule optimizes a clipped surrogate\nfunction:\nLCLIP(θ) = Et\nh\nmin\n\u0010\nrt(θ) ˆAt, clip(rt(θ), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011i\n(2)\nwhere\nrt(θ) = πθ(at|st)\nπθold(at|st)\n(3)\nand\nˆAt = Qπ(st, at) −Vψ(st)\n(4)\nwhere ψ denotes the parameters of value function (Vψ)\nnetwork, ϵ denotes a coefficient. Policy parameters θ are\nupdated as:\nθ ←θ + α∇θLCLIP(θ)\n(5)\nPPO’s constrained updates stabilize training and improve\nperformance, making it practical for complex single-agent\nRL tasks.\nIV. MULTI-AGENT REINFORCEMENT LEARNING FOR\nSINGLE QUADRUPED ROBOT LOCOMOTION\nIn this paper, we use the collaborative potential of multiple\nagents to improve the learning process for a single robot’s\nlocomotion, resulting in faster training convergence, robust-\nness and better final performance in real-world environments.\nSpecifically, each leg of the quadruped robot is treated as a\nseparate agent within the multi-agent structure, with indi-\nvidual observations and a shared global critic, significantly\nimproving the cooperation among the robot’s limbs for more\neffective locomotion.\nFig. 3: The framework of MASQ\nA. MASQ Modeling\nThis paper models a single quadruped robot locomotion\nas a cooperative multi-agent problem, which is described as\na partially observable decentralized Markov decision process\n(decPOMDP) [49]. The decPOMDP is defined by the tuple\nG = (S, A, P, r, Z, O, N, γ, T). S is the state space, A\nis the action space, P is the state transition distribution,\nr is the reward function, Z is the observation space, O\nis the observation function, N is the number of agents, γ\nis the discount factor, and T is the time horizon. At each\ntime step t, each agent n ∈{1, . . . , N} selects an action\nan\nt ∈A, resulting in a joint action at = {a1\nt, a2\nt, . . . , aN\nt }.\nThe environment transitions to a new state st+1 according\nto P(st+1|st, at) and provides a shared reward r(st, at).\nEach agent receives an observation zn\nt from O(st, n) and\nmaintains an observation-action history τ n\nt . MARL aims to\nlearn policies {πn}N\nn=1 that maximize expected cumulative\nrewards:\nJ(π) = E\n\" T\nX\nt=0\nγtr(st, at)\n#\n(6)\nThis paper proposes Multi-Agent Reinforcement Learning\nfor Single Quadruped Robot Locomotion (MASQ), which\napplies MARL principles to treat different parts of a single\nquadruped robot as independent agents, trained collabora-\ntively using shared rewards. Specifically, this paper uses\nMAPPO [35] to sovle the modeled multi-agent problem.\nMAPPO optimizes the following objective function in a\nmulti-agent context:\nLCLIP\nMAPPO(θ) =\nn\nX\ni=1\nEt\nh\nmin\n\u0010\nri\nt(θi) ˆAt,\nclip(ri\nt(θi), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011i\n(7)\nwhere\nri\nt(θi) =\nπθi(ai\nt|oi\nt)\nπθi,old(ai\nt|oi\nt)\n(8)\nwhere i denote the i-th agent in MARL. Each agent updates\nits policy parameters as follows:\nθi ←θi + α∇θiLCLIP\ni\n(θi)\n(9)\nThis paper uses centralized training with decentralized\nexecution (CTDE) [50] to handle multi-agent learning chal-\nlenges. This approach maintains stability in a constantly\nchanging environment. In implementations, separate neural\nnetworks learn a policy (πθ) and a value function (Vψ(s))\nsimilar to the single-agent version. The value function helps\nreduce training variability and can take in additional global\ninformation. This approach leads to better final performance,\nfaster learning speed, and improved robustness in deploy-\nment.\nB. MASQ Settings\nState Space and Observation: The shared actor network\ntakes a concatenated observation from four agents, each with\n35 dimensions, including motor positions qt ∈R3, motor\nspeeds ˙qt ∈R3, previous actions at−1 ∈R3 and at ∈R3, a\ngait sequencing director dt ∈R1, projected gravity gt ∈R3,\ncommand values vcmd\nt\n∈R15, body speeds vb ∈R3, and\na one-hot encoding et ∈R1. The total input for the actor\nnetwork is oactor\nt\n∈R140 (35x4). The architecture of the\nactor network consists of a normalization layer, followed\nby an MLP layer, a GRU layer, and another normalization\nlayer, with the final output being the joint angle commands\nfor each leg. Additionally, a bias is applied to the output\nfor better precision in control. On the other hand, the critic\nnetwork uses global observations and has a 73-dimensional\ninput, including motor positions qt ∈R12, motor speeds\n˙qt ∈R12, previous actions at−1 ∈R12 and at ∈R12, gait\ndirectors dt ∈R4, projected gravity gt ∈R3, command\nvalues vcmd\nt\n∈R15, and body speeds vb ∈R3. The output of\nthe critic network consists of continuous V values Vt ∈R4,\nwhich are used to calculate the advantage function.\nAction Space: The output of the actor-network consists\nof continuous actions at ∈R12, and the system then uses\nthese to calculate the torques for the 12 motors. Details can\nbe found in Section IV-D.\nReward Function: The reward functions in Table I are\ndesigned to optimize the robot’s performance by encourag-\ning desired behaviors and penalizing undesired ones. Key\nrewards include: tracking linear velocity, which uses an\nexponential decay function to minimize velocity error; linear\nvelocity Z and angular velocity XY, both penalizing unwanted\nmotions to ensure stability; torques, DOF velocity, and DOF\nacceleration, which promote energy efficiency and smoother\nmovements; and collision, which penalizes excessive contact\nforces. Additional rewards focus on action smoothness, ac-\ncurate jumping, minimizing foot slip and impact velocities,\nand enhancing locomotion stability using Raibert’s heuristic\n[51] and foot velocity tracking.\nC. Multi-agent Actor and Global Critic Networks\nIn the simulation environment of Isaac Gym [16], the robot\nreceives observations and rewards to facilitate its learning.\nThe learning process involves dividing the observations into\nshared and private features, and both the actor and critic\nnetworks use the rewards to train the policy.\nTABLE I: Reward function\nREWARD SETTINGS, CORRESPONDING EQUATIONS, AND THEIR SCALES\nReward Term\nEquation\nScale\nTracking Linear Velocity\nexp\n\u0010\n−∥vcmd−vb∥2\nσt\n\u0011\n1.0\nTracking Angular Velocity\nexp\n\u0012\n−(ωcmd,z−ωb,z)2\nσyaw\n\u0013\n0.5\nLinear Velocity Z\n∥vb,z∥2\n−2 × 10−2\nAngular Velocity XY\nP\ni ∥ωb∥2\n−1 × 10−3\nAngular Velocity Torques\nP\ni ∥τi∥2\n−1 × 10−5\nDOF Velocity\nP\ni ∥vd,i∥2\n−1 × 10−4\nDOF Acceleration\nP\ni\n\u0010 vd,i,t−vd,i,t−1\n∆t\n\u00112\n−2.5 × 10−7\nCollision\nP (1.0 · (∥fc∥> 0.1))\n-5.0\nAction Rate\nP ∥at −at−1∥2\n−1 × 10−2\nJump\n−(hb −hj)2\n10.0\nFeet Slip\nP(cf · ∥vf∥2)\n−4 × 10−2\nAction Smoothness 1\nP (at −at−1)2\n-0.1\nAction Smoothness 2\nP (at −2at−1 + at−2)2\n-0.1\nFeet Impact Velocity\nP(cs · ∥vf,p∥2)\n-0.0\nRaibert Heuristic\nP ∥er∥2\n-10.0\nTracking Contacts Shaped Velocity\nP \u0012\ncd ·\n\u0012\n1 −exp\n\u0012\n−\n∥vf ∥2\nσgv\n\u0013\u0013\u0013\n4.0\nThe actor-network consists of a multi-layer perception\n(MLP) base and an activation layer that produces actions\nand their associated log probabilities. Similarly, the critic\nnetwork uses an MLP base, ending in an output layer that\npredicts value functions. The actor-network outputs actions\nbased on observations, while the critic network assesses\nthe value of these actions to guide the learning process.\nThe actions produced for the agents are processed by an\nactuator network [52] to simulate real-world conditions,\nenhancing deployment effectiveness in natural environments.\nAfter training, the trained actor-network is deployed onto the\nrobotic dog to perform actions directly. Fig. 3 illustrates the\nentire learning process.\nIn the context of a quadruped robot, we consider each\nleg as an individual agent. All four agents share a standard\nactor-network. Using a shared-parameter network instead of\nfour separate actor networks helps reduce computational load\nand better fits the nature of a quadruped robot. Unlike typical\nmulti-agent environments, such as StarCraft [14], where each\nsoldier is an independent agent, the quadruped robot is a sin-\ngle entity with four legs symmetrically positioned around the\nbody’s center. Therefore, a shared-parameter actor-network\nis more suitable for this scenario.\nWe express the policy for each leg as follows:\nπθi(ai,t | si,t) = πθ(ai,t | si,t)\n(10)\nwhere i = 1, 2, 3, 4 corresponds to the four legs.\nThe quadruped robot has four legs, each with three motors:\nhip, thigh, and calf joint motors. Each motor’s position,\nvelocity, and torque are observable, so we use these details\nas the independent observations for each agent. Additionally,\nto enhance the coordination among the agents, we augment\neach independent observation with shared observations, in-\ncluding speed and gravity projections calculated from inertial\nmeasurement unit (IMU) data, temporal director, and agent\nidentifier (ID). The temporal director Ti(t) guides the gait\nsequence of each leg under different movement postures,\nwhile the agent ID is necessary for the shared-parameter\n(a) Robustness in outside disturbing\n(b) Contact force in different legs while encountering force\nFig. 4: Robustness test\nnetwork. This setup ensures the independence of each agent\nwhile improving their cooperative capabilities. The temporal\ndirector helps to synchronize the movements of different legs,\nensuring smooth gait patterns. It can be defined as:\nTi(t) = sin (2π(kt + ∆i))\n(11)\nwhere\n• k is the scaling factor of scaling factor of gait cycle.\n• ∆i is the phase offset for the i-th leg, which determines\nits relative timing within the gait cycle to ensure coor-\ndinated movement.\nThe Global Critic uses a centralized value function ap-\nproach to consider global information, which fits into the\nCTDE. It relies on a global value function to coordinate\nindividual agents, such as single-agent algorithms like PPO.\nThe Critic network takes in global observations, which\nconsist of the separate observations of the four agents and\nshared global information.\nDuring the training in the simulation environment, we\nused a multi-gait curriculum learning [53]. This curriculum\ncomprises four gaits: pace, trot, bound, and pronk. In the\nsimulation, the quadruped robot learns these four gaits simul-\ntaneously, and the progress is updated based on evaluating\nwhether the gaits’ rewards meet specific thresholds. This\nmethod allows the robot to learn different gaits effectively\nand is also helpful in testing the generalization capabilities of\nour proposed approach across various tasks in experimental\nsettings.\nD. Sim-to-real\nTo bridge the gap between simulation and real-world per-\nformance, we used domain randomization and an Actuator\nNetwork [52]. Domain randomization involves randomizing\nvarious parameters to train a robust policy under different\nconditions [30], [54], [55]. These parameters include robot\nbody mass, motor strength, joint position calibration, ground\nfriction, restitution, orientation, and magnitude of gravity.\nWe also independently randomize friction coefficients like\nground friction. Gravity is randomized every 8 seconds with\na gravity impulse duration of 0.99 seconds. Time steps are\nrandomized every 6 seconds, with the overall randomization\noccurring every 4 seconds. These measures enhance the\nmodel’s robustness and adaptability. The training process\nfor the Actuator Network captures the non-ideal relationship\nbetween PD error and the torque realized [52], thereby im-\nproving the model’s performance in real-world applications.\nV. EXPERIMENTS\nWe conducted experiments using the Unitree Go2 quadru-\npled robot with various experiments. Section V-A proposes\nthe experimental setup. Section V-B shows the experimental\nresults of the simulation. Section V-C shows the real-world\nexperiments and comparisons.\n(a) Flat terrain\n(b) Uneven terrain\nFig. 5: Various terrains\nA. Experiments Setup\nIn the simulation environment, we designed two types of\nterrain: flat and uneven, shown in Fig. 5. Uneven terrain is\na composite of pyramid-sloping terrain and random uniform\nterrain. Fifteen commands, such as linear velocity, angular\nvelocity and height of the base, sampled within a specified\nrange, guide the gait-learning process. The robot learns to\nadapt and develop various skills by following these com-\nmands.\nB. Simulation Experiments\nThe MASQ was compared with the PPO student-teacher\nbaseline in simulation environments, including two types:\nflat and uneven terrain. The quadruped robot was trained\nusing the curriculum learning method, with hyperparameters\nsuch as the learning rate tuned for optimal performance\nin both scenarios. Reward curves in Fig. 6 show that the\nproposed method achieves faster convergence and better final\nperformance compared to the PPO baseline. Experimental\nresults are presented as the mean of rewards over five tests\nfor a fair comparison.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment Steps\n1e8\n0\n2\n4\n6\n8\nMean Reward\nMean Rewards over Time (Flat Terrain)\nRewards of MASQ\nRewards of PPO\nSmoothed Rewards of MASQ\nSmoothed Rewards of PPO\n(a) Reward over time (flat terrain)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nEnvironment Steps\n1e8\n0\n2\n4\n6\n8\nMean Reward\nMean Rewards over Time (Uneven Terrain)\nRewards of MASQ\nRewards of PPO\nSmoothed Rewards of MASQ\nSmoothed Rewards of PPO\n(b) Reward over time (uneven terrain)\nFig. 6: Return on various terrains\nC. Real-word Experiments\nThis paper deploys MASQ on a quadruped robot and tests\nit on various terrains, including flat ground, grass, and sand\nin Fig. 1, where it performs excellently. In addition, we\nconducted a series of robustness tests for heavy impacts and\nside kicks. The robot can quickly return to its normal state\nafter being disturbed.\n1) Gaits test: Fig. 7 illustrates the periodic relationship\nof quadruped force feedback for each of the four legs under\nthe training of four different skills. They reflect the impact\nof incorporating the temporal director in our observations on\nthe learning and switching of other skills.\n2) Robustness test: To validate the robustness of the gaits\ntrained with our method, we conduct external disturbance\ntests on the robot. We test the robot in a bounding gait\nwith a cycle period of 20ms. During robustness tests, the\nrobot performs continuous jumps in this bounding gait.\nThe impacts of human steps on the robot are applied to\npropose disturbances during its jump cycle. Under normal\nconditions, the robot landed simultaneously on all four legs.\nWe monitored this process by recording the force sensor\nvalues in the robot’s feet, thereby documenting the transition\nfrom a normal state through the induced disturbance and back\nto the normal state. As shown in Fig. 4, the robot returns to\nits normal state after experiencing a disturbance within just\none gait cycle.\nVI. CONCLUSION AND FUTURE WORK\nThis paper proposed a multi-agent-based motion control\nsystem for quadruped robots, utilizing a shared-parameter\nFig. 7: Contact force of four legs in different gaits\nactor network and a centralized critic network within the\nCTDE framework. The proposed approach, implemented\nin Isaac Gym, demonstrated substantial improvements in\ntraining speed, robustness, and final performance, benefiting\nfrom curriculum learning and domain randomization. These\nadvances enabled efficient limb coordination and smoother\nsim-to-real transitions. Experimental results confirmed the\neffectiveness of the method in enhancing both performance\nand efficiency in motion control for symmetric robots. Future\nwork will extend the approach to other symmetric robots and\nexplore its application in more complex dynamic environ-\nments.\nREFERENCES\n[1] Y. Hou, H. Sun, J. Ma, and F. Wu, “Improving offline reinforcement\nlearning with inaccurate simulators,” in 2024 IEEE International\nConference on Robotics and Automation (ICRA), 2024, pp. 5162–\n5168.\n[2] G. B. Margolis and P. Agrawal, “Walk these ways: Tuning robot\ncontrol for generalization with multiplicity of behavior,” in Conference\non Robot Learning.\nPMLR, 2023, pp. 22–31.\n[3] I. M. Aswin Nahrendra, B. Yu, and H. Myung, “Dreamwaq: Learning\nrobust quadrupedal locomotion with implicit terrain imagination via\ndeep reinforcement learning,” in 2023 IEEE International Conference\non Robotics and Automation (ICRA), 2023, pp. 5078–5084.\n[4] H. Benbrahim and J. A. Franklin, “Biped dynamic walking using\nreinforcement learning,” Robotics and Autonomous systems, vol. 22,\nno. 3-4, pp. 283–302, 1997.\n[5] H. Duan, B. Pandit, M. S. Gadde, B. Van Marum, J. Dao, C. Kim, and\nA. Fern, “Learning vision-based bipedal locomotion for challenging\nterrain,” in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA), 2024, pp. 56–62.\n[6] H. Lu, Y. Li, S. Mu, D. Wang, H. Kim, and S. Serikawa, “Motor\nanomaly detection for unmanned aerial vehicles using reinforcement\nlearning,” IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2315–\n2322, 2018.\n[7] B. Jia and D. Manocha, “Sim-to-real robotic sketching using behavior\ncloning and reinforcement learning,” in 2024 IEEE International\nConference on Robotics and Automation (ICRA), 2024, pp. 18 272–\n18 278.\n[8] A. Lobbezoo, Y. Qian, and H.-J. Kwon, “Reinforcement learning for\npick and place operations in robotics: A survey,” Robotics, vol. 10,\nno. 3, p. 105, 2021.\n[9] H. Jiang, T. Chen, J. Cao, J. Bi, G. Lu, G. Zhang, X. Rong, and\nY. Li, “Sim-to-real: Quadruped robot control with deep reinforcement\nlearning and parallel training,” in 2022 IEEE International Conference\non Robotics and Biomimetics (ROBIO), 2022, pp. 489–494.\n[10] S. Lyu, H. Zhao, and D. Wang, “A composite control strategy for\nquadruped robot by integrating reinforcement learning and model-\nbased control,” in 2023 IEEE/RSJ International Conference on In-\ntelligent Robots and Systems (IROS), 2023, pp. 751–758.\n[11] L. Canese, G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino,\nM. Re, and S. Spanò, “Multi-agent reinforcement learning: A review\nof challenges and applications,” Applied Sciences, vol. 11, no. 11, p.\n4948, 2021.\n[12] J. Orr and A. Dutta, “Multi-agent deep reinforcement learning for\nmulti-robot applications: A survey,” Sensors, vol. 23, no. 7, p. 3625,\n2023.\n[13] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer,\nH. Dong, S.-C. Zhu, and Y. Yang, “Towards human-level bimanual\ndexterous manipulation with reinforcement learning,” in Advances in\nNeural Information Processing Systems, vol. 35, 2022, pp. 5150–5163.\n[14] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and\nS. Whiteson, “Monotonic value function factorisation for deep multi-\nagent reinforcement learning,” Journal of Machine Learning Research,\nvol. 21, no. 178, pp. 1–51, 2020.\n[15] D. Ye, G. Chen, P. Zhao, F. Qiu, B. Yuan, W. Zhang, S. Chen, M. Sun,\nX. Li, S. Li, J. Liang, Z. Lian, B. Shi, L. Wang, T. Shi, Q. Fu, W. Yang,\nand L. Huang, “Supervised learning achieves human-level performance\nin moba games: A case study of honor of kings,” IEEE Transactions on\nNeural Networks and Learning Systems, vol. 33, no. 3, pp. 908–918,\n2022.\n[16] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Mack-\nlin, D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., “Isaac gym:\nHigh performance gpu based physics simulation for robot learning,”\nin Thirty-fifth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2), 2021.\n[17] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk\nin minutes using massively parallel deep reinforcement learning,” in\nConference on Robot Learning.\nPMLR, 2022, pp. 91–100.\n[18] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,\nV. Koltun, and M. Hutter, “Learning agile and dynamic motor skills\nfor legged robots,” Science Robotics, vol. 4, no. 26, Jan. 2019.\n[19] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,\n“Learning quadrupedal locomotion over challenging terrain,” Science\nRobotics, vol. 5, no. 47, p. eabc5986, 2020.\n[20] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,\n“Learning robust perceptive locomotion for quadrupedal robots in the\nwild,” Science Robotics, vol. 7, no. 62, p. eabk2822, 2022.\n[21] S. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, and I. Havoutis,\n“Real-time trajectory adaptation for quadrupedal locomotion using\ndeep reinforcement learning,” in 2021 IEEE International Conference\non Robotics and Automation (ICRA).\nIEEE, 2021, pp. 5973–5979.\n[22] A. Kumar, Z. Fu, D. Pathak, and J. Malik, “Rma: Rapid motor\nadaptation for legged robots,” Robotics: Science and Systems XVII,\n2021.\n[23] A. Kumar, Z. Li, J. Zeng, D. Pathak, K. Sreenath, and J. Malik, “Adapt-\ning rapid motor adaptation for bipedal robots,” in 2022 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS),\n2022, pp. 1161–1168.\n[24] S. Choi, G. Ji, J. Park, H. Kim, J. Mun, J. H. Lee, and J. Hwangbo,\n“Learning quadrupedal locomotion on deformable terrain,” Science\nRobotics, vol. 8, no. 74, p. eade2256, 2023.\n[25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[26] Y. Kim, H. Oh, J. Lee, J. Choi, G. Ji, M. Jung, D. Youm, and\nJ. Hwangbo, “Not only rewards but also constraints: Applications on\nlegged robot locomotion,” IEEE Transactions on Robotics, 2024.\n[27] L. Ye, J. Li, Y. Cheng, X. Wang, B. Liang, and Y. Peng, “From\nknowing to doing: Learning diverse motor skills through instruction\nlearning,” arXiv preprint arXiv:2309.09167, 2023.\n[28] B. L. Semage, T. G. Karimpanal, S. Rana, and S. Venkatesh,\n“Zero-shot sim2real adaptation across environments,” arXiv preprint\narXiv:2302.04013, 2023.\n[29] X. Gu, Y.-J. Wang, and J. Chen, “Humanoid-gym: Reinforcement\nlearning for humanoid robot with zero-shot sim2real transfer,” arXiv\npreprint arXiv:2404.05695, 2024.\n[30] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to-\nreal transfer of robotic control with dynamics randomization,” in 2018\nIEEE International Conference on Robotics and Automation (ICRA).\nIEEE, May 2018.\n[31] B. Wang, Z. Liu, Q. Li, and A. Prorok, “Mobile robot path plan-\nning in dynamic environments through globally guided reinforcement\nlearning,” IEEE Robotics and Automation Letters, vol. 5, no. 4, pp.\n6932–6939, 2020.\n[32] X. Zhou, X. Wen, Z. Wang, Y. Gao, H. Li, Q. Wang, T. Yang, H. Lu,\nY. Cao, C. Xu, and F. Gao, “Swarm of micro flying robots in the\nwild,” Science Robotics, vol. 7, 2022.\n[33] C. Zhang, N. Rudin, D. Hoeller, and M. Hutter, “Learning agile\nlocomotion on risky terrains,” arXiv preprint arXiv:2311.10484, 2023.\n[34] M. Mittal, N. Rudin, V. Klemm, A. Allshire, and M. Hutter, “Symme-\ntry considerations for learning task symmetric robot policies,” in 2024\nIEEE International Conference on Robotics and Automation (ICRA).\nIEEE, 2024.\n[35] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and\nY. Wu, “The surprising effectiveness of ppo in cooperative multi-\nagent games,” in Advances in Neural Information Processing Systems,\nvol. 35, 2022, pp. 24 611–24 624.\n[36] M. C. Machado, A. Barreto, D. Precup, and M. Bowling, “Temporal\nabstraction in reinforcement learning with the successor representa-\ntion,” Journal of Machine Learning Research, vol. 24, no. 80, pp.\n1–69, 2023.\n[37] Y. Alon and H. Zhou, “Multi-agent reinforcement learning for un-\nmanned aerial vehicle coordination by multi-critic policy gradient\noptimization,” IEEE Internet of Things Journal, 2020.\n[38] T. Jacob, D. Duran, T. Pfeiffer, M. Vignati, and M. Johnson, “Multi-\nagent reinforcement learning for unmanned aerial vehicle capture-the-\nflag game behavior,” in Intelligent Systems Conference.\nSpringer,\n2023, pp. 174–186.\n[39] S.\nSainz-Palacios,\n“Deep\nreinforcement\nlearning\nfor\nshared\nautonomous\nvehicles\n(sav)\nfleet\nmanagement,”\narXiv\npreprint\narXiv:2201.05720, 2022.\n[40] C. Schmidt, D. Gammelli, F. C. Pereira, and F. Rodrigues, “Learning to\ncontrol autonomous fleets from observation via offline reinforcement\nlearning,” in 2024 European Control Conference (ECC). IEEE, 2024,\npp. 1399–1406.\n[41] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning\nto communicate with deep multi-agent reinforcement learning,” in\nAdvances in Neural Information Processing Systems, vol. 29, 2016.\n[42] T. R. X, “Lifelike agility and play on quadrupedal robots using\nreinforcement learning and generative pre-trained models,” Nature\nMachine Intelligence, 2023.\n[43] Y. Wang, R. Sagawa, and Y. Yoshiyasu, “Learning advanced loco-\nmotion for quadrupedal robots: A distributed multi-agent reinforce-\nment learning framework with riemannian motion policies,” Robotics,\nvol. 13, no. 6, p. 86, 2024.\n[44] X. Han and M. Zhao, “Learning quadrupedal high-speed running on\nuneven terrain,” Biomimetics, vol. 9, no. 1, p. 37, 2024.\n[45] A. names not provided, “Deep reinforcement learning for real-world\nquadrupedal locomotion: a comprehensive review,” OAEPublish, 2023.\n[46] X. Lan, Y. Qiao, and B. Lee, “Towards pick and place multi robot\ncoordination using multi-agent deep reinforcement learning,” in 2021\n7th International Conference on Automation, Robotics and Applica-\ntions (ICARA).\nIEEE, 2021, pp. 85–89.\n[47] Y. Liu, Z. Cao, H. Xiong, J. Du, H. Cao, and L. Zhang, “Dynamic\nobstacle avoidance for cable-driven parallel robots with mobile bases\nvia sim-to-real reinforcement learning,” IEEE Robotics and Automa-\ntion Letters, vol. 8, no. 3, pp. 1683–1690, 2023.\n[48] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[49] S. C. Ong, S. W. Png, D. Hsu, and W. S. Lee, “Pomdps for robotic\ntasks with mixed observability.” in Robotics: Science and Systems,\nvol. 5, 2009, p. 4.\n[50] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli,\nand S. Whiteson, “Stabilising experience replay for deep multi-agent\nreinforcement learning,” in Proceedings of the 34th International\nConference on Machine Learning-Volume 70, 2017, pp. 1146–1155.\n[51] M. H. Raibert and E. R. Tello, “Legged robots that balance,” IEEE\nExpert, vol. 1, no. 4, pp. 89–89, 1986.\n[52] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,\nV. Koltun, and M. Hutter, “Learning agile and dynamic motor skills\nfor legged robots,” Science Robotics, vol. 4, no. 26, p. eaau5872, 2019.\n[53] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum\nlearning,” in Proceedings of the 26th Annual International Conference\non Machine Learning, ser. ICML ’09.\nNew York, NY, USA:\nAssociation for Computing Machinery, 2009, p. 41–48.\n[54] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan,\nJ. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, “Sim-to-real via sim-\nto-sim: Data-efficient robotic grasping via randomized-to-canonical\nadaptation networks,” in 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2019, pp. 12 619–12 629.\n[55] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,\n“Domain randomization for transferring deep neural networks from\nsimulation to the real world,” in 2017 IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS), 2017, pp. 23–30.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2024-08-25",
  "updated": "2024-10-17"
}