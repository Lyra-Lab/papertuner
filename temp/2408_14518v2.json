{
  "id": "http://arxiv.org/abs/2408.14518v2",
  "title": "A Survey on Reinforcement Learning Applications in SLAM",
  "authors": [
    "Mohammad Dehghani Tezerjani",
    "Mohammad Khoshnazar",
    "Mohammadhamed Tangestanizadeh",
    "Arman Kiani",
    "Qing Yang"
  ],
  "abstract": "The emergence of mobile robotics, particularly in the automotive industry,\nintroduces a promising era of enriched user experiences and adept handling of\ncomplex navigation challenges. The realization of these advancements\nnecessitates a focused technological effort and the successful execution of\nnumerous intricate tasks, particularly in the critical domain of Simultaneous\nLocalization and Mapping (SLAM). Various artificial intelligence (AI)\nmethodologies, such as deep learning and reinforcement learning, present viable\nsolutions to address the challenges in SLAM. This study specifically explores\nthe application of reinforcement learning in the context of SLAM. By enabling\nthe agent (the robot) to iteratively interact with and receive feedback from\nits environment, reinforcement learning facilitates the acquisition of\nnavigation and mapping skills, thereby enhancing the robot's decision-making\ncapabilities. This approach offers several advantages, including improved\nnavigation proficiency, increased resilience, reduced dependence on sensor\nprecision, and refinement of the decision-making process. The findings of this\nstudy, which provide an overview of reinforcement learning's utilization in\nSLAM, reveal significant advancements in the field. The investigation also\nhighlights the evolution and innovative integration of these techniques.",
  "text": "Journal name: \nVolume xx, Issue xx, Date \nISSN: xxxx-xxxx, DOI: x.x.x. \n1 \n \n \nhttps://www.academicedgepub.co.uk/journals/xxx \nReceived: xxxxx \n                                                                                                                                                                               Revised: xxxxx \n                                                                                                                                                                               Published: xxxx \nA Survey on Reinforcement Learning Applications \nin SLAM \nMohammad Dehghani Tezerjani 1, Mohammad Khoshnazar 2, Mohammadhamed Tangestanizadeh 3, \nArman Kiani4, Qing Yang5 \n1, 5 Computer Science and Engineering, University of North Texas, Denton, USA \n2 Institute of Artificial Intelligence, University of Bremen, Bremen, Germany \n3 Computer Science and Engineering, University of California Santa Cruz, Santa Cruz, USA \n5 Electrical and Computer Engineering, University of Maine, Maine, USA \n \nEmail: 1 mike.degany@unt.edu, 2 Khoshnam@uni-bremen.de, 3 mtangest@ucsc.edu, \n 4 arman.kiani@maine.edu, 5 qing.yang@unt.edu \n \nAbstract— The emergence of mobile robotics, particularly in the \nautomotive industry, introduces a promising era of enriched user \nexperiences and adept handling of complex navigation challenges. \nThe realization of these advancements necessitates a focused \ntechnological effort and the successful execution of numerous \nintricate tasks, particularly in the critical domain of Simultaneous \nLocalization and Mapping (SLAM). Various Artificial Intelligence \n(AI) methodologies, such as deep learning and Reinforcement \nLearning (RL), present viable solutions to address the challenges in \nSLAM. This study specifically explores the application of RL in the \ncontext of SLAM. By enabling the agent (the robot) to iteratively \ninteract with and receive feedback from its environment, RL \nfacilitates the acquisition of navigation and mapping skills, thereby \nenhancing the robot's decision-making capabilities. This approach \noffers \nseveral \nadvantages, \nincluding \nimproved \nnavigation \nproficiency, increased resilience, reduced dependence on sensor \nprecision, and refinement of the decision-making process. The \nfindings of this study, which provides an overview of RL's \nutilization in SLAM, reveal significant advancements in the field. \nThe investigation also highlights the evolution and innovative \nintegration of these techniques. \nKeywords— \nSimultaneous \nlocalization \nand \nmapping, \nReinforcement learning, Path planning, Loop closure detection, \nActive SLAM. \nI. \nINTRODUCTION  \nMobile robotics involves designing, constructing, \noperating, and utilizing robots to perform tasks in dynamic, \nnon-fixed environments. These robots are usually designed to \nbe mobile and autonomous, capable of operating without \ndirect human control [1]. Autonomous driving vehicles are \none of the specific applications of mobile robotics, focusing \non developing vehicles that can navigate and operate on their \nown in real-world environments, such as roads and highways. \nThese vehicles use a combination of sensors, cameras, radar, \nLight Detection and Ranging (LiDAR), as well as advanced \nalgorithms to perceive their surroundings and make decisions \nabout how to navigate safely to their destination [2]. \nSimultaneous Localization and Mapping (SLAM) is a key \ntechnology in Mobile Robotics and Autonomous Driving \n(MRAD) [3]. SLAM enables a robot to navigate and create a \nmap in an unknown environment by continuously observing \nmap features to determine its own position and orientation [4, \n5]. Localization enables the robot to determine its position \nwithin an environment, while mapping involves constructing \na representation of the environment (the map) as the robot \nexplores it [6]. The ability to accurately localize a robot using \na map of its surroundings is essential for tasks ranging from \nspatial exploration to autonomous driving, as it provides the \nnecessary information for predicting obstacle movements and \ndetermining optimal maneuvers [7, 8]. \nAccurate localization in robotics is a complex task due to \nthe inherent noise in sensor measurements. Addressing \noutliers, occlusions, and sensor failures, as well as resolving \nscale differences between the map and robot motion, is \nessential for successful localization [4]. Additionally, \ndetecting revisited locations, known as loop closures, \npresents a challenge due to perceptual aliasing and sensor \nlimitations. It is crucial to maintain a consistent estimate of \nthe robot's pose over time, especially in dynamic \nenvironments [6, 9]. Real-time performance is also a key \nconsideration in ensuring effective robot navigation and \nlocalization [10]. \nMapping presents several challenges that require attention \nto ensure accuracy and reliability. One such challenge is \naligning sensor measurements with map features, which \ndemands careful consideration and precise alignment [3, 11]. \nFurthermore, extracting meaningful features from sensor \ndata, such as point clouds and images, is essential for \nconstructing an accurate map [12]. It is also crucial to \naccurately estimate vehicle motion during mapping and to \nidentify revisited locations to close loops in the map [13]. \nChoosing an appropriate map representation, whether it's 2D \ngrids, 3D point clouds, or another format tailored to specific \nmapping needs, is another key consideration. Finally, \nmaintaining map consistency as new data is incorporated is \nimportant for preserving its accuracy and reliability over time \n[6]. \nThese challenges drive research and innovation in SLAM \nalgorithms, aiming to improve accuracy, robustness, and \nefficiency. Artificial Intelligence (AI) algorithms are integral \nto SLAM, as they enable robots to navigate and map their \nenvironment in real-time. Specifically, Reinforcement \nLearning (RL) offers promising opportunities for improving \nexploration, localization, and map building in SLAM \napplications [14]. The integration of RL in the context of \nJournal name \n \n2 \n \nAuthor, Title \nMRAD aims to offer a systematic approach for robots or \nvehicles to acquire the skills needed for navigating \nenvironments and making informed decisions using sensor \ndata [15, 16]. Therefore, this study explores the practical uses \nof RL within the context of SLAM. The key contributions are \ndetailed as follows: \n• \nThe \nconcepts \nof \nSLAM \nwere \nmeticulously \ncategorized into two distinct parts: passive and \n`active. This nuanced classification enhances the \nanalysis of SLAM  approaches in studies, offering a \nclearer framework for evaluating and comparing \ndifferent methodologies. \n• \nThe sources of data utilized for input into the SLAM \nalgorithm have been examined. \n• \nRL in SLAM is divided into four main categories: \npath planning, loop closure detection, environment \nexploration, obstacle detection, and Active SLAM. \nThis classification enables researchers to explore the \nvarious applications of RL in SLAM and stimulates \nthe development of new ideas for improvement. \nThe rest of the paper is organized as follows: In Section \nII, types of SLAM approaches are introduced. Section III \ndescribes the modalities used for capturing information about \nthe surrounding environment. Section IV explains RL and its \noperational principles. Section V focuses on reviewing and \ncategorizing studies that have utilized RL in the context of \nSLAM. Section VI outlines the challenges in applying RL to \nSLAM. Finally, Section VII and VIII conclude the paper and \ndepict future directions. \nII. \nSIMULTANEOUS LOCALIZATION AND MAPPING \nThe SLAM technology was first introduced at a \nconference in San Francisco in 1986. It combines map \nrecognition and initialization to achieve simultaneous \npositioning and map creation . SLAM is a collection of \napproaches utilized by robots to autonomously determine \ntheir location and map the surrounding environment as they \ntraverse through it. The concept of SLAM can be further \ncategorized into two main components: (1) localization, \nwhich involves estimating the robot's position in relation to \nthe map, and (2) mapping, which involves reconstructing the \nenvironment using visual, visual–inertial, and laser sensors \nmounted on the robot . In modern SLAM techniques, a \ngraphical approach is commonly adopted, specifically a \nbipartite graph where nodes represent either the robot or \nlandmark poses, and edges represent measurements between \nposes or poses and landmarks. Imagine a robot characterized \nby a state vector 𝑥∈𝑅2 that defines its position and \norientation (pose). The primary aim of the SLAM issue is to \ndetermine the optimal state vector 𝑥∗, minimizing the \nmeasurement error 𝑒𝑖(𝑥) weighted by the covariance matrix \n𝛺𝑖∈𝑅𝑙𝑥𝑙, which accounts for the uncertainty in pose \nmeasurements, with l representing the state vector's \ndimension, as illustrated in (1) . \n𝑥∗= 𝑎𝑟𝑔⁡ 𝑚𝑖𝑛\n𝑥\n∑𝑒𝑡\n𝑇(𝑥)𝛺𝑡𝑒𝑡(𝑥)\n𝑡\n \n(1) \nThere are two primary approaches to SLAM: Active SLAM \nand passive SLAM [20], which are discussed in the \nfollowing. \nA. Passive SLAM approach \nPassive SLAM systems do not involve navigating a robot \nto explore unfamiliar environments. Instead, they rely on \npredetermined routes or manual guidance and do not actively \nadapt to changes in the environment. Passive SLAM is \nparticularly suitable for scenarios where precise robot motion \nis not critical, focusing instead on effective mapping and \nlocalization [21]. This approach facilitates more predictable \nand controlled movement, which can be advantageous in \nspecific applications. However, it also implies that the robot \nmay lack the ability to autonomously adjust to unforeseen \nchanges in its environment without additional intervention \n[17]. Passive SLAM separates the estimation of robot motion \nfrom map estimation. Manual control or adherence to \npredetermined waypoints is characteristic of the robot's \noperation [4]. In passive SLAM, Particle Filters (PF) are \ncommonly used to estimate robot poses and build maps. PF is \na probabilistic method that represents the posterior \ndistribution using a set of particles (samples), where each \nparticle represents a potential robot pose and map hypothesis \n[22]. \nB. Active SLAM approach \nActive SLAM involves surveying the environment using \nsensors that are in motion, while simultaneously estimating \nthe status of these sensors and constructing a map. Active \nSLAM setups use sensor readings as input and generate real-\ntime decisions or actions to influence future measurements \n[23, 24]. Typically, it involves a three-part process [25]: \n1. The recognition of all potential locations for \nexploration (ideally infinite), \n2. The calculation of the efficacy or benefit derived \nfrom the actions that would transition the robot \nfrom its present coordinates to each of those \nlocations, \n3. The choice and implementation of the most \nadvantageous course of action. \nActive SLAM includes modules for planning waypoints \nand generating trajectories. It uses methods from information \ntheory, optimal control theory, and RL to actively steer the \nrobot towards its destination [4]. We discussed this approach \nin Section V and explored the RL application in Active \nSLAM. \nIII. \nDATA SOURCE \nSLAM in autonomous driving typically involves \nintegrating data from multiple sensors to create a \ncomprehensive perception of the surroundings [26, 27]. \nThese sensors include LiDAR, camera, Global Navigation \nSatellite System / Inertial Navigation System (GNSS/INS), \nInertial Measurement Unit (IMU), wheel odometry, and \nradar. The sensor measurements are combined to provide a \ndetailed and accurate understanding of the environment [22]. \nFurthermore, control commands from the vehicle's \nactuators such as steering, throttle, and brakes affect the \nJournal name \n \n3 \n \nAuthor, Title \nrobot's movement. RL techniques can optimize these control \nmaneuvers to improve SLAM performance [4, 22]. SLAM \nalgorithms usually start with an initial estimate of the robot's \npose, derived from sources like GNSS or wheel odometry \n[28]. RL approaches can refine this pose estimation \nthroughout the SLAM process, enhancing accuracy and \nreliability [29].  \nLiDAR: LiDAR, a remote sensing technology, uses laser \nlight to measure distances and create detailed 3D maps of \nenvironments [30]. By emitting laser beams and calculating \nthe time it takes for them to reflect off objects, LiDAR \ngenerates accurate point cloud data. This data is instrumental \nin creating comprehensive maps, identifying obstacles, and \nensuring collision-free paths [31]. Additionally, LiDAR \nenhances precise localization by matching current scans with \npreviously mapped features. LiDAR scans are thus pivotal for \nboth mapping and obstacle detection, providing critical \nsupport for autonomous navigation systems [32].  \nCameras: Cameras capture images of the surroundings, \nextracting visual features such as key points and edges to aid \nin robot localization and map construction [33]. These images \nare crucial for visual odometry and feature extraction, \nenabling the estimation of motion by tracking features across \nconsecutive frames. Additionally, cameras help identify \npreviously visited locations and contribute to the creation of \ndetailed 3D maps [34]. \nGNSS/INS: GNSS offers global positioning information \nvia satellite signals, providing data on latitude, longitude, and \naltitude. In contrast, INS utilizes accelerometers and \ngyroscopes to estimate position and orientation by measuring \naccelerations and angular rates [22]. GNSS delivers an initial \nposition estimate, while INS ensures continuous tracking and \ncompensates for GNSS signal outages, maintaining accurate \nnavigation even in challenging conditions [35]. \nIMU: IMUs measure both accelerations and angular rates \nby combining accelerometers and gyroscopes to capture \nlinear acceleration and angular velocity. IMUs provide short-\nterm motion estimates, and their data is integrated with other \nsensors, such as LiDAR and cameras, to enhance the \nrobustness of SLAM systems [36]. \nRadar: Radar sensors identify objects using radio waves, \nmaking them particularly valuable for obstacle detection in \nchallenging weather conditions. In SLAM systems, radar \nsensors are enhancing accuracy, especially under poor \nlighting or occlusions [37]. The number of radar sensors used \nin a SLAM system varies depending on the application, \nrequirements, and desired accuracy [38]. Some systems \nutilize a single radar sensor for both motion estimation and \nenvironmental mapping. This approach is particularly \nbeneficial in low-light conditions or adverse weather \nconditions, where other sensors such as cameras or LiDAR \nmay have limitations. Advanced SLAM systems may use \nmultiple radar sensors to improve robustness against sensor \nfailures and cover a full 360-degree field of view [39]. \nCombining data from multiple radars enhances accuracy and  \nreduces blind spots [40].  Radar sensors are often integrated \nwith other sensors to provide complementary information \n[41]: \n• \nLiDAR: Offers long-range detection and high-\nresolution mapping. \n• \nCameras: Radar-camera fusion helps handle \nchallenging lighting conditions. \n• \nIMU: Improves motion estimation. \nOngoing research is focused on finding optimal \nconfigurations for radar-based SLAM, balancing factors such \nas cost, power consumption, and sensor placement [42]. \nWheel odometry: Wheel odometry is a fundamental \nlocalization technique that uses wheel encoders to measure \nthe rotation of a robot’s wheels, allowing it to estimate \nincremental movement in terms of distance and direction \n[43]. This method is computationally inexpensive and widely \navailable, providing continuous pose updates as the robot \nmoves [44]. Encoders track the distance traveled, offering \nincremental pose updates, but they can accumulate errors \nover time due to factors like wheel slippage and uneven \nterrain. Encoder readings are also subject to noise, which can \nimpact accuracy [45]. In SLAM systems, wheel odometry is \ncommonly used as one of the sensor inputs [43]. RL \nalgorithms can learn to integrate wheel odometry information \nfor pose estimation. In RL-based SLAM, agents are trained to \nfuse wheel odometry data with other sensor modalities, such \nas LiDAR, cameras, and IMUs, to enhance overall system \nrobustness and accuracy [26]. \nIV. \nREINFORCEMENT LEARNING \nTo effectively analyze data and develop intelligent \nautomated applications, a solid understanding of RL is crucial \n[46]. In RL, an agent learns through interactions with its \nenvironment and receiving rewards [47]. The agent explores \nvarious actions to determine which yield the highest rewards \nover time. Given that actions can have lasting impacts, the \nreturn value R, defined by the reward function, is computed \nby summing discounted future rewards across episodes, as \nshown in (2) where γ denotes the discount factor, and r(st,at) \nrepresents the reward for action at in state st. \nR⁡=⁡ ∑γtr(st,at)\nT\nt=0\n \n(2) \nEach state is assigned a state value 𝑉(𝑠), which represents \nthe expected return an agent can obtain by selecting actions. \nEquation (3) illustrates how  𝑉(𝑠) is computed. \n𝑉(𝑠) = 𝐸[𝑅∨𝑠0 = 𝑠]⁡or⁡  \n𝑉(𝑠) = 𝐸[𝑟(𝑠) + 𝛾𝑉(𝑠next )] \n(3) \nThe expectation symbol 𝔼 represents the value of a state \nas the expected return while following a specific policy. The \nvalue of a state, denoted as 𝑟(𝑠), is the total expected sum of \nrewards achievable in that state [48]. Additionally, each \naction within a state has an action value 𝑄(𝑠, 𝑎) determined \nby the Bellman equation (4) [49]. \nJournal name \n \n4 \n \nAuthor, Title \n𝑄(𝑠, 𝑎) = 𝐸[𝑅∨𝑠0 = 𝑠, 𝑎0 = 𝑎] or  \n𝑄(𝑠, 𝑎) = 𝐸[𝑟(𝑠, 𝑎) + 𝛾𝑄(𝑠next , 𝑎next )] \n(4) \nThe state-action value, or Q-value, is the expected return \nof taking a specific action 𝑎0 in a state 𝑠0 and then following \nthe policy throughout the episode. The relationship between \nstate value and Q-value can be described by (5). \n𝑉(𝑠) = ∑𝜋(𝑎∨𝑠)𝑄(𝑠, 𝑎)\n𝑎∈𝐴\n \n(5) \nGiven a policy 𝜋 that selects action 𝑎 in state 𝑠, a Q-table \ncan be constructed for a Markov Decision Problem (MDP) \nwhere all states and actions are known and limited. In this \nprocess, the agent takes actions and updates the Q-value in \nthe Q-table based on the return received from the \nenvironment. The Q-value is updated using the (6), Where the \nupdated 𝑄 value, denoted as 𝑄′, is calculated using a learning \nrate represented by 𝛼 [48].  \n𝑄′(𝑠𝑡, 𝑎𝑡) = 𝑄(𝑠𝑡, 𝑎𝑡) + \n𝛼([𝑟𝑡+ 𝛾𝑄𝑚𝑎𝑥(𝑠𝑡+1, 𝑎𝑡+1)]\n−[𝑄(𝑠𝑡, 𝑎𝑡)]) \n \n(6) \nFig. 1 illustrates an agent that learns from its environment \nusing a RL algorithm. The algorithm updates the policy based \non the actions taken, with the environment providing \nfeedback in the form of rewards and the next state. This \ninformation is then used by the RL algorithm to improve the \nselection of actions. \nValue-based and policy-based methods are two primary \napproaches used in RL to train agents to solve decision-\nmaking problems. In practice, choosing between value-based \nand policy-based methods depends on the specifics of the \nproblem, such as the nature of the state and action spaces, \nstability concerns, and desired convergence properties [50]. \nA. Value-Based Methods \nValue-based methods focus on estimating the value \nfunction, which represents the expected return or reward that \nan agent can achieve from a given state (or state-action pair) \nwhile following a particular policy. The primary goal is to \nfind an optimal value function that can be used to derive an \noptimal policy [51]. Common value-based methods include: \n• \nQ-Learning: This algorithm aims to learn the action-\nvalue function (Q-function), which estimates the \nexpected return of taking an action in a specific state \nand following the best policy thereafter. Once the Q-\nfunction is learned, the agent can act by selecting \nactions that maximize this value [52]. \n• \nDeep Q-Networks (DQN): An extension of Q-\nlearning that uses deep neural networks to \napproximate the Q-function, making it feasible to \nscale Q-learning to environments with high-\ndimensional state spaces [53]. \n• \nSARSA: \nState-Action-Reward-State-Action \n(SARSA) updates the Q-values based on the action \ntaken by the agent, ensuring that the learned policy \nreflects the actions taken during training. SARSA \nrepresents the sequence of events used to update the \nQ-values. In each step, after the agent takes an action \nand receives a reward, SARSA updates its Q-value \nestimates by considering the next state and the next \naction that the policy would take. [54]. \nB. Policy-Based Methods \nPolicy-based methods focus on learning a policy directly. The \npolicy can be either deterministic or stochastic, and it maps \nstates (or observations) directly to actions, without \nnecessarily using a value function to do so. In policy-based \nmethods, the objective is typically to optimize the policy itself \nto maximize some measure of cumulative reward [50]. Key \npolicy-based methods include: \nPolicy Gradient Methods: These methods specifically use \ngradient ascent to optimize the policy. The idea is to compute \nthe gradient of the expected return with respect to the policy \nparameters and use this gradient to update the policy in a \ndirection that improves performance [55]. \n• \nREINFORCE: This is the foundational policy \ngradient algorithm proposed by Williams (1992). It \nuses a Monte Carlo estimation of the policy gradient \nand is simple but often suffers from high variance, \nmaking convergence slower in some scenarios. \nREINFORCE directly optimizes the expected return \nof the policy by adjusting the weights of the neural \nnetwork using sampled trajectories [56]. \n• \nTrust Region Policy Optimization (TRPO): TRPO \nalgorithm enhances the foundational policy gradient \nmethod by incorporating a trust region constraint. \nThis constraint is crucial as it ensures that the newly \nupdated policy does not significantly diverge from \nthe existing policy, a divergence quantified by the \nKullback-Leibler (KL) divergence metric. By \nmaintaining this constraint, TRPO stabilizes the \ntraining process. However, this stability comes at the \nexpense of increased computational complexity \n[57]. TRPO guarantees incremental policy updates, \nthereby enhancing both stability and performance. It \nhas been effectively applied in SLAM for making \nrobust decisions in dynamic environments [58]. \n• \nProximal Policy Optimization (PPO): PPO is a \nrefinement of TRPO, developed to address its \nFig. 1. Reinforcement Learning \nJournal name \n \n5 \n \nAuthor, Title \ncomputational \ninefficiencies. \nIt \nreplaces \nthe \ncomplex trust region constraint with a simpler \nclipping mechanism in the surrogate loss function, \nwhich limits large policy updates. PPO is \ncomputationally efficient, easier to implement, and \nperforms well across a wide range of tasks, making \nit one of the most popular policy gradient algorithms \n[59]. It has been applied to SLAM for efficient \nexploration and mapping [55]. \n• \nActor-Critic Models: Actor-Critic Models refine the \nlearning framework by integrating the advantages of \nboth value-based and policy-based methods. The \nActor component suggests actions based on current \npolicy, while the Critic evaluates these actions by \nestimating the value function, providing a feedback \nloop that continually refines both policy and action \nvalue estimations [60]. This dual approach enables \nefficient learning and convergence, facilitating \nquick adaptation to changes in the environment, \nwhich \nis \ncritical \nfor \nreal-time \nautonomous \nnavigation and mapping [61]. \nC. Deep Reinforcement Learning \nDeep Reinforcement Learning (DRL) is a ML approach \nthat combines RL with deep neural networks [62]. It differs \nfrom traditional methods by learning feature representations \ndirectly from raw data, such as images and sensor readings, \nrather than relying on handcrafted features [63]. This \napproach has shown great promise in solving complex \ndecision-making problems and has been applied in various \nfields such as robotics, gaming, and autonomous systems [48, \n64]. \nValue-based DRL methods, such as DQN and their \nextensions, have proven effective in addressing decision-\nmaking problems in discrete and low-dimensional action \nspaces. However, these methods face significant limitations \nwhen applied to tasks that require navigating continuous, \nhigh-dimensional action spaces. Active SLAM, which \ninvolves intricate path planning and precise control, \nexemplifies such challenges [55]. To overcome these \nlimitations, researchers have increasingly turned to Policy \nGradient Methods and Actor-Critic Models. These alternative \napproaches offer distinct advantages, enabling robots to \nautonomously navigate and map their environments while \neffectively handling the complexities of high-dimensional \naction spaces. This paradigm shift underscores the growing \nrole of advanced RL techniques in addressing the \nmultifaceted demands of active SLAM tasks [48]. \nD. RL-based SLAM applications \nRL has significantly impacted SLAM in practical robotics \napplications, particularly in autonomous vehicles, drones, and \nrobotics. Traditionally, SLAM faced challenges in dealing \nwith complex and dynamic environments where sensor noise, \ndata association, and computational constraints could hinder \nperformance. \nRobotics: \nRL-based \nSLAM \nenables \nrobots \nto \nautonomously navigate and map complex environments by \nlearning optimal navigation strategies through trial and error. \nThis method enhances robots' abilities to make real-time \ndecisions and adapt to dynamic conditions in unfamiliar \nindoor settings while avoiding obstacles [48]. Traditional \nsystems lack independent learning capabilities, but Lee et al. \n[65] introduced an innovative end-to-end approach using \nDRL for autonomous navigation in uncharted environments. \nThey developed two deep Q-learning agents, the DQN and \nthe double DQN, which allow robots to independently learn \nskills for collision avoidance and navigation. The process \nbegins with target object detection using a deep neural \nnetwork model, followed by navigation guidance using deep \nQ-learning algorithms. Moreover, recent research focused on \nimproving SLAM for mobile robots in complex environments \nusing advanced methodologies. Wong et al. [66] developed a \nmulti-sensor fusion approach using DRL and Multi-Model \nAdaptive Estimation (MMAE), enhancing the localization \nprecision and stability of robots in challenging conditions. Su \net al. [67] presented Adaptive SLAM Fusion Degradation \n(ASLAM-FD), a framework combining adaptive multi-\nsensor fusion with degradation detection and DRL to \nmaintain SLAM accuracy in dynamic environments. Liu et al. \n[68] created snake-like robots using SLAM and DRL for \nautonomous navigation and obstacle avoidance in hard-to-\naccess areas. Their innovations, featuring lightweight \nstructures with 2D LiDAR and IMU, significantly improved \npath planning and reduced collision rates compared to \ntraditional methods. \nAutonomous Vehicles: RL methods have become essential \nin addressing real-time SLAM tasks for autonomous cars, \nparticularly self-driving vehicles operating in dynamic \nenvironments such as urban streets. These vehicles utilize RL \nto handle decision-making processes, including lane-keeping \n[69], obstacle avoidance [70], and route planning [71], while \nconcurrently constructing and updating maps of their \nsurroundings. \nRL excels in such applications due to its ability to handle \nhigh-dimensional sensory data (e.g., LiDAR, cameras) and \nlearn optimal control policies through interaction with the \nenvironment. For example, RL enables autonomous cars to \nadapt to dynamic traffic conditions, manage sensor noise, and \nmake split-second decisions to ensure safety and efficiency \n[72]. Advanced RL techniques, such as Actor-Critic models \nand PPO, enhance these capabilities by improving stability \nand convergence during training. These methods allow the \nvehicles to navigate effectively in complex scenarios, such as \nunstructured roads, crowded intersections, and multi-agent \nenvironments, where traditional SLAM approaches might \nstruggle [73].  \nDrones: RL is particularly impactful for drones due to \ntheir reliance on SLAM in GPS-denied environments. For \nexample, RL allows drones to perform high-speed racing, \nwhere they must process rapid sensor inputs and dynamically \nadjust trajectories. Experiments demonstrate that RL-based \ncontrollers achieve better performance under real-world \nJournal name \n \n6 \n \nAuthor, Title \nconditions compared to classical optimal control methods, \nespecially when unmodeled dynamics or disturbances are \npresent [72, 73]. \nV. \nCLASSIFICATION OF EXISTING METHODS \nRL has many applications within SLAM, an intersection \nrich with research endeavors. This study reviews the diverse \napplications of RL within SLAM, drawing upon an array of \narticles for insights and analysis. These applications span \nvarious domains, including path planning, loop closure \ndetection, and Active SLAM, as delineated in Fig. 2, \nunderscoring the multifaceted utility of RL techniques in \nadvancing SLAM methodologies. \nA. Path Planning \nRobot path planning technology, pivotal in robotics \nresearch, optimizes criteria such as minimizing work costs \nand finding the shortest route, ensuring efficient navigation \nwhile avoiding obstacles [74]. Path planning methods can \ngenerally be categorized into traditional and AI approaches. \nWhen considering environmental information, these methods \ncan be further classified into global planning with known \nenvironmental data and local planning with unknown \nenvironmental information [75]. \nWang [76] developed a visual SLAM system utilizing the \nORB-SLAM3 framework. The system's primary function \ninvolves the generation of a dense point cloud map. \nSubsequently, this dense point cloud map from the visual \nSLAM system is converted into an octomap, followed by a \nprojection transformation to the grid map. The next stage \ninvolves the development of a path planning algorithm rooted \nin RL. Experimental comparisons were conducted among the \nQ-learning algorithm, the DQN algorithm, and the SARSA \nalgorithm. The outcomes showed that the DQN algorithm \nexhibits the swiftest convergence rate and superior \nperformance, \nparticularly \nin \nintricate \nenvironments \ncharacterized by high dimensions. \nNam et al. [29] introduced a novel framework for the \nnavigation of mobile robots, integrating two established \napproaches (SLAM and DRL), to improve operational \nefficiency. The framework leverages SLAM to construct \nmaps and pinpoint the robot's coordinates, while employing \nan Ant Colony Optimization (ACO) algorithm to formulate a \npredetermined route. In scenarios characterized by varying \nobstacles within the environment, the framework adopts \nDRL-based \ntechniques \nfor \nlocalized \npath planning. \nFurthermore, \nthe \nsuggested \nframework \nconducts \na \ncomparative analysis and assessment of the efficacy of three \ndistinct DRL-based navigation algorithms: Deep Generative \nNetwork (DGN), Twin Delayed Deep Deterministic Policy \nGradient (TD3), and Proximal Policy Optimization (PPO). \na) Environment Exploration: \nThe \nrobot \nexploration \nmodel \nintegrates \nvarious \nexploration methods and technologies, empowering robots to \nautonomously navigate, map, and explore unfamiliar \nenvironments efficiently. It leverages advancements in \nrobotics, AI, and sensor technology to seamlessly fulfill these \nobjectives [77]. \nChen et al. [75] introduced a DRL-based robot \nexploration model designed for navigating unknown \nenvironments without any collisions. This innovative \napproach integrates SLAM technology and a DRL dual-mode \nstructure to address local-minimum issues. After 30 training \nrounds, the model successfully achieved zero collisions and \nminimized repeated exploration. It surpasses existing \nmethods for exploring unknown environments by a margin of \nless than 5%. \nLi et al. [78] examined the concept of automatic \nexploration within unfamiliar environments through the \napplication of DRL alongside a graph-based SLAM \ntechnique known as Karto SLAM. The proposed framework \nincorporates decision-making, planning, and mapping \ncomponents that make use of a deep neural network to acquire \nknowledge pertaining to exploration strategies. \nb)  Obstacle Detection: \nThe obstacle detection model involves the development of \nalgorithms and technologies to enable vehicles to detect \nobstacles in their surroundings accurately and in real time to \nensure safe navigation and collision avoidance [79]. \nWen et al. [80] used a fully convolutional residual \nnetworks method to identify road obstacles. The dueling \nDQN algorithm is also used in designing the robot's path. A \ntwo-dimensional map of the route is created by FastSLAM.  \nAccording to Nam et al. study in [81], SLAM algorithms \nare effective for mapping in the environment and DRL \nFig. 2. Classification of RL applications in SLAM. \nJournal name \n \n7 \n \nAuthor, Title \nalgorithms can find dynamic obstacles well, but SLAM \nalgorithms and DRL algorithms alone do not perform \nperfectly. In this method, SLAM algorithms combine data \nfrom several sensors, including LiDAR, to make a map of the \nenvironment. The ACO algorithm is used for planning to find \nthe shortest optimal global path. DRL algorithms help the \nrobot in planning the local path. In this way, the robot can \nmake decisions based on its current situation and goal. \nFayjie et al. [82] used DRL for autonomous navigation \nand obstacle avoidance in self-driving cars. This study uses \ncamera and laser sensor data and a trained neural network for \ndriving. The DQN approach has also been implemented for \nautonomous driving simulation tests. \nB. Loop Closure Detection  \nLoop closure, a crucial process in robotics and \nautonomous vehicles, addresses inaccuracies in sensor \nmeasurements [83]. It also tackles various issues affecting \nreliability, such as drifting, acceleration changes, and weather \nconditions. By detecting when a vehicle revisits a previously \nvisited location, loop closure helps to correct any \naccumulated errors in the system's map or position estimate \n[84]. This process is essential for ensuring the accuracy and \ncredibility of the vehicle's navigation system and overall \nperformance [85]. \nIqbal et al. [86] investigated loop closure detection in \nsimulated environments using a DRL approach. In this study, \ntraining was improved with entropy maximization for batch \nsize selection. Furthermore, Bag-of-Words (BOW) method is \nused for loop closure and localization in maps, which \nrepresent an image using locally created features. DRL trains \nthe probabilistic policy for loop closure detection. \nFurthermore, Convolutional Neural Networks (CNN) and \nregion-based features are used for landmark proposal and \nmatching. \nIn another study, Iqbal [87] presented two approaches to \nsolve the problem of loop closure detection. The first \napproach uses statistical and clustering methods. In the \nsecond approach using DRL, loop closure detection is \nconsidered as a reward-driven optimization process. The \nproposed structure is implemented in a simulated grid \nenvironment. After generating the data, the learning process \nis done for the agent and the agent learns to detect the loop \nclosure in variant environments. \nC. RL in Active SLAM \nAs mentioned in Section 2, Active SLAM is a method \nused by robots and autonomous systems to actively explore \ntheir surroundings. This approach allows the system to \ncontinuously enhance its understanding of the environment \nwhile also updating its position in real-time [88]. By making \ninformed decisions on where to move next, the robot can \neffectively gather the most valuable data for mapping and \nlocalization purposes. This dynamic technique enables the \nsystem to adapt to changing environments and efficiently \nnavigate through unknown areas [89, 90]. \nFang et al. [91] harnessesed the power of MuZero to \nimprove agents' planning abilities for joint Active SLAM and \nnavigation tasks. These tasks involve navigating through \nunfamiliar environments while creating a map and \ndetermining the agent's location simultaneously. The paper \nintroduces the SLAMuZero framework, which combines \nSLAM with the tree-search-based MuZero. SLAMuZero \nemploys an explicit encoder-decoder architecture for \nmapping, along with a prediction function to assess policy \nand value using the generated map. The integration of \nSLAMuZero leads to a substantial decrease in training time. \nPlaced et al. [88] utilized deep Q-learning architecture \nwith laser measurements for navigation and focused on \nreducing uncertainty in robot localization and map \nrepresentation. Trained agents reduce uncertainty, transfer \nknowledge to new maps and learn to navigate and explore in \nsimulations. \nPei et al. [92] introduced Active relative localization for \nmulti-agent SLAMs. The task allocation algorithm is based \non DRL and utilizes a Multi-Agent System DQN (MAS-\nDQN) to enhance collaboration efficiency in SLAM.  \nAlcalde et al. [93] used two agents namely completeness-\nbased and uncertainty-based agents. According to the results, \nthese agents completed maps without collisions. The \nuncertainty-based agent generated longer paths but better \nmaps, and the Active SLAM DRL solution improved \nperformance in complex environments. \nTable 1 offers a comparative examination of the \nscrutinized research endeavors utilizing RL in the context of \nSLAM \napplications. \nIt \ndelves \ninto \nthe \nsimulation \nenvironment, \ndeep \nlearning \ntechniques, \nSLAM \nmethodologies, and RL algorithms employed in these studies. \n \nTABLE 1: COMPARISON OF REVIEWED STUDIES. \n \nRef. \nYear \nSimulation \nenvironment \nDeep \nlearning \nmethod \nSLAM \nmethod \nRL algorithm \nAdvantage \nDisadvantage \nPath planning \n \n[76] \n2024 \nSimple maze \nDeep neural \nnetwork \nORBSLAM3 \n- Q-learning \n- DQN \n- SARSA \n• \nEffective map \nconversion \n• \nContribution to \nautonomous \nnavigation \n• \nDependence on \nsensor quality \n• \nComputational \ndemands \n[29] \n2023 \n- Gazebo \n- ROS \n- TurtleBot \nDeep neural \nnetwork \nSLAM-MCL \n- Q-learning \n- SARSA \n- actor-critic \n• \nComprehensive \nframework \n• \nExtensive \nexperimental \nvalidation \nusing \nvarious \n• \nLimited \ngeneralization \n• \nPotential \noverfitting \nJournal name \n \n8 \n \nAuthor, Title \nsimulated \nenvironments \nEnvironment exploration \n \n[75] \n2024 \n- Gazebo \n- ROS \nCNN \n- \nDQN \n• \nEffective \nTraining \nStrategy \n• \nAddressing \nReal-World \nChallenges \n• \nSlower \nExploration \nSpeed \n• \nNo Real-World \nImplementatio\nn \n[78] \n2019 \nROS \nCNN \nKarto SLAM \nDQN \n• \nModular \nFramework \n• \nEfficient \nExploration \nStrategy \n• \nGeneralization \nPerformance \n• \nComplexity \nof \nImplementation \n• \nHigh \nComputational \nBurden \nObstacle detection \n \n[81] \n2023 \n- ROS2 \n- DDS \nCommunicatio\nn \n - Gazebo \nCNN \nSLAM-MCL \n- DQN \n- PPO \n- TD3 \n• \nAdaptability \nto \nDynamic \nEnvironments \n• \nExploration \nof \nMultiple \nDRL \nAlgorithms \n• \nComplexity \nin \nImplementation \n• \nRestricted \nProblem Scope \n[80] \n2020 \nGazebo \nFully \nconvolutional \nresidual \nnetwork \nFastSLAM \nDueling DQN \n• \nScalable Action \nSpace \n• \nImproved \nLearning \nEfficiency \n• \nHigh \nComputational \nComplexity \n• \nSensor \nDependency \n[82] \n2018 \nUnity Game \nEngine \nCNN \n- \nDQN \n• \nSensor Fusion \n• \nEfficient \nSimulation \nEnvironment \n• \nImproved \nTraining \nStability \n• \nSimulation-\nLimited \nValidation \n• \nSimplistic \nAction Set \n• \nLimited \nScalability \nLoop closure detection \n \n[86] \n2022 \nTurtlebot \nCNN \nVSLAM \nMarkov Decision \nProcess \n• \nReal-World \nApplication \n• \nImproved \nFeature \nUtilization \n• \nDependence on \nPrior Knowledge \n• \nComputational \nDemands \n• \nHardware-\nDependent \nConstraints \n[87] \n2019 \n- Zoox \n- Autonomous \ndriving \nplatform \nCNN \nVSLAM \nMarkov decision \nprocess \n• \nRobust \nData \nAssociation \nMethod \n• \nScalable \nto \nUnknown \nEnvironments \n• \nDependency on \nAccurate Depth \nEstimation \n• \nComputational \nOverheads \nActive SLAM \n \n[91] \n2024 \nHabitat \nEncoder-\nDecoder \n- \n- \n• \nEfficiency \nin \nTraining \n• \nImproved \nPerformance \n \n• \nHigh \nInitial \nComplexity \n• \nUnclear \nScalability \n• \nPotential \nfor \nOverfitting \n[88] \n2020 \nGazebo \nDeep Neural \nNetwork \n- \n- DQN \n- DDQN \n- D3QN \n• \nStrong \ngeneralization \ncapabilities \n• \nComplex \nSimulation \nValidation \n• \nLimited \nReal-\nworld Testing \n• \nPerformance \nVariability \n \n[92] \n2020 \n- ROS \n- Telobot \nDeep neural \nnetwork \nORBSLAM \nMAS-DQN \n• \nScalable Design \n• \nRealistic \nSimulation \n• \nLack \nof \nDistributed \nAlternative \n• \nDependence on \nCentralized \nCoordination \nJournal name \n \n9 \n \nAuthor, Title \n[93] \n2022 \n- Gazebo \n- ROS \n- ROBOTIS \nTurtleBot3-\nBurger \nDeep Neural \nNetwork \nLightweight \nPassive \nSLAM \nPartially Observable \nMarkov Decision \nProcess (POMDP) \n• \nFocus on Map \nQuality \nand \nRobustness \n• \nEfficiency \nthrough \nDimensionality \nReduction \n• \nFlexible \nand \nLightweight \nApproach \n• \nLimited Physical \nWorld \nValidation \n• \nDependency on \nReward Function \nDesign \n \nVI. \nCHALLENGES IN APPLYING RL TO SLAM \nIn the rapidly evolving field of AI, RL has garnered \nsignificant attention for its potential to empower autonomous \nsystems with the ability to learn and adapt through interaction \nwith their environment. Despite its promising outlook, \ndeploying RL in real-world applications faces considerable \nchallenges [94]. These challenges posing unique hurdles that \nmust be navigated to harness the full potential of RL \ntechnologies. Some of these limitations including: \nHigh Computational Demands: RL models, especially \nthose that extensively utilize deep learning frameworks, \nfrequently require significant computational resources to \nperform optimally. The necessity for high processing power \nand substantial memory can pose a critical limitation in \ncontexts where rapid decision-making is essential [95]. \nConsequently, attaining real-time performance with these \nmodels on edge devices, such as robots or drones, is \nparticularly challenging. Such devices often possess limited \nprocessing \ncapabilities, \nwhich \ncan \nimpede \nthe \nimplementation of sophisticated RL algorithms, even though \nthese algorithms could potentially enhance the devices' \nautonomy and operational effectiveness [96]. \nSafety and Reliability: In the context of RL-based SLAM, \nguaranteeing safe exploration and robust decision-making is \nof paramount importance, especially for autonomous vehicles \nnavigating \nthrough \ndynamic \nand \ncomplex \nurban \nenvironments. This necessity arises from the intricate nature \nof urban streets, which present a variety of unpredictable \nchallenges such as varying traffic patterns, pedestrian \nmovement, and environmental changes [97]. These factors \ncan impact a vehicle's ability to accurately map and \nunderstand its surroundings for optimal path planning and \nhazard avoidance [95]. \nGeneralization Issues: RL models often face challenges \nin adapting to diverse environmental conditions due to \nsignificant discrepancies between the controlled settings in \nwhich they are typically trained and the complex, \nunstructured nature of real-world scenarios. This limitation \narises because the assumptions and constraints inherent in \nsimulation environments fail to capture the full spectrum of \nvariability and unpredictability present in natural settings. As \na result, the application of RL models outside their training \ndomains frequently leads to suboptimal performance, \nhighlighting a critical gap in their generalization capabilities \n[96]. \nHigh-dimensional state and action spaces: The state \nspace encapsulates all possible states in the environment. This \nencompasses the intricate array of sensor data vital for SLAM \noperations. Meanwhile, the action space delineates the \nspectrum of feasible actions available to the agent [98]. \nWithin SLAM, these actions pertain to the movements, \nencompassing maneuvers like turning and accelerating. \nSLAM systems operate amidst a milieu of high-dimensional \nsensor data, ranging from intricate camera images to intricate \nLiDAR point clouds, essential for navigating complex \nenvironments [99]. However, the efficacy of RL agents in \nhandling such expansive input spaces is challenged by the \nescalating computational complexity inherent in high-\ndimensional realms [30]. \nSample efficiency: The ability of a RL algorithm to learn \nfrom a small number of interactions (samples) with the \nenvironment is known as sample efficiency [100]. Sample \nefficiency is important since real-world data collecting for \nautonomous vehicles in SLAM can be costly and time-\nconsuming (e.g., using laser sensors) [101]. \nSensor/actuator delays: Sensor/actuator delays epitomize \nthe temporal gap between perceiving an event and enacting a \nresponse. This latency, inherent to the system, poses a critical \nchallenge. In these domains, the journey from sensing to \ndecision-making to action execution encompasses finite \nintervals, demanding precise synchronization [102]. RL \nalgorithms must grapple with these delays to orchestrate \ntimely and precise responses, ensuring seamless navigation \nand operation [103]. Within the intricate landscape of SLAM, \nthis temporal precision assumes paramount importance, since \nthe essence of success lies in the precision of real-time \nprocessing. This crucial element not only upholds but \nenhances the quality of localization and mapping, but also \nensures a seamless fusion of navigational prowess [30]. \nVII. \nFUTURE DIRECTIONS \nIn future research on the application of RL in SLAM, several \nkey areas hold promise for advancing the state of the art:  \n• \nAdaptive sensor fusion: Combining data from \nvarious sensors, such as cameras, LiDAR, and \nInertial Measurement Units (IMUs), is crucial for \nachieving robust SLAM. Future work could focus on \ndeveloping RL agents capable of learning how to \nadaptively fuse information from these different \nmodalities. By doing so, the overall performance and \nreliability of SLAM systems could be significantly \nenhanced, particularly in diverse and dynamic \nenvironments. \n• \nSelf-Supervised learning and data augmentation: The \nintegration of Self-Supervised Learning (SSL) and \ndata augmentation techniques offers substantial \npotential for improving RL-based SLAM, especially \nin MRAD applications. Leveraging large amounts of \nunlabeled data and generating diverse training \nJournal name \n \n10 \n \nAuthor, Title \nsamples \ncan \nenhance \nthe \nrobustness \nand \ngeneralization capabilities of SLAM systems. Future \nresearch should explore innovative SSL strategies \nand data augmentation methods to maximize the \nefficacy of RL in SLAM. \n• \nKnowledge transfer across environments: For RL \nagents to be truly effective in real-world applications, \nthey must be able to transfer knowledge across \ndifferent maps or environments. Future studies \nshould investigate techniques such as domain \nadaptation and meta-learning to facilitate better \ngeneralization of RL-based SLAM systems. These \napproaches can enable RL agents to apply learned \nknowledge from one environment to another, thereby \nimproving their adaptability and performance in \npreviously unseen settings. \nBy addressing these areas, future research can contribute \nto the development of more robust, efficient, and versatile \nRL-based SLAM systems, paving the way for advancements \nin the navigation of MRAD. \nVIII. \nCONCLUSION  \nSLAM is a technique used in robotics and autonomous \nvehicles to create a map of an unknown environment while \nsimultaneously keeping track of an agent's location within \nthat environment. It is a key technology for enabling MRAD \nto navigate and operate in real-world settings. SLAM \ninvolves the use of various sensors such as cameras, LiDAR, \nand odometry to gather information about the surrounding \nenvironment and then process this data to construct a map and \nestimate the agent's pose. In this survey, applications that \nhave used RL in SLAM were investigated. According to the \nsearches, the most use of RL in SLAM was in path planning, \nloop closure detection, environment exploration, obstacle \ndetection, and Active SLAM. In these problems, RL helps the \nagent to design an intelligent map and facilitate navigation. \nSLAM methods can be effectively applied in MRAD, but the \nsensors, and environmental factors may need to be tailored to \nthe respective application domain. \nREFERENCES  \n[1] \nN. Sharma, J. K. Pandey, and S. Mondal, \"A review of mobile robots: \nApplications and future prospect,\" International Journal of Precision \nEngineering and Manufacturing, vol. 24, no. 9, pp. 1695–1706, Sep. \n2023, doi: 10.1007/s12541-023-00876-7. \n[2] \nK. Othman, \"Public acceptance and perception of autonomous vehicles: \nA comprehensive review,\" AI and Ethics, vol. 1, no. 3, pp. 355–387, \nAug. 2021, doi: 10.1007/s43681-021-00041-8. \n[3] \nG. Bresson, Z. Alsayed, L. Yu, and S. Glaser, \"Simultaneous localization \nand mapping: A survey of current trends in autonomous driving,\" IEEE \nTransactions on Intelligent Vehicles, vol. 2, no. 3, pp. 194–220, 2017. \n[4] \nM. F. Ahmed, K. Masood, V. Fremont, and I. Fantoni, \"Active SLAM: \nA review on last decade,\" Sensors, vol. 23, no. 19, p. 8097, 2023. \n[Online]. Available: https://www.mdpi.com/1424-8220/23/19/8097. \n[5] \nJ. Ren and D. Xia, \"SLAM in autonomous driving,\" in Autonomous \nDriving Algorithms and Its IC Design. Singapore: Springer Nature \nSingapore, 2023, pp. 127–152. \n[6] \nJ. A. Placed et al., \"A survey on active simultaneous localization and \nmapping: State of the art and new frontiers,\" IEEE Transactions on \nRobotics, 2023. \n[7] \nX. Wu, G. Wang, and N. Shen, \"Research on obstacle avoidance \noptimization and path planning of autonomous vehicles based on \nattention mechanism combined with multimodal information decision-\nmaking thoughts of robots,\" Frontiers in Neurorobotics, vol. 17, p. \n1269447, 2023, doi: 10.3389/fnbot.2023.1269447. \n[8] \nH. Taheri and Z. C. Xia, \"SLAM; definition and evolution,\" Engineering \nApplications of Artificial Intelligence, vol. 97, p. 104032, Jan. 2021, \ndoi: https://doi.org/10.1016/j.engappai.2020.104032. \n[9] \nR. Zeng, Y. Wen, W. Zhao, and Y.-J. Liu, \"View planning in robot active \nvision: A survey of systems, algorithms, and applications,\" \nComputational Visual Media, vol. 6, pp. 225–245, 2020. \n[10] M. Diginsa, N. Shafie, and N. Yusuf, \"Review: Issues and challenges of \nsimultaneous localization and mapping (SLAM) technology in \nautonomous robot,\" International Journal of Innovative Computing, \nvol. 13, pp. 59–63, Nov. 2023, doi: 10.11113/ijic.v13n2.408. \n[11] R. \nEyvazpour, \nM. \nShoaran, \nand \nG. \nKarimian, \n\"Hardware \nimplementation of SLAM algorithms: A survey on implementation \napproaches and platforms,\" Artificial Intelligence Review, vol. 56, no. \n7, pp. 6187–6239, Jul. 2023, doi: 10.1007/s10462-022-10310-5. \n[12] S. Li, Z. Li, X. Liu, C. Shan, Y. Zhao, and H. Cheng, \"Research on map-\nSLAM fusion localization algorithm for unmanned vehicle,\" Applied \nSciences, vol. 12, no. 17, p. 8670, 2022. [Online]. Available: \nhttps://www.mdpi.com/2076-3417/12/17/8670. \n[13] T. Liu, C. Xu, Y. Qiao, C. Jiang, and J. Yu, \"Particle filter SLAM for \nvehicle localization,\" arXiv preprint, arXiv:2402.07429, 2024. \n[14] C. Fan, Z. Li, W. Ding, H. Zhou, and K. Qian, Integrating Artificial \nIntelligence with SLAM Technology for Robotic Navigation and \nLocalization in Unknown Environments, 2024. \n[15] N. Botteghi, B. Kallfelz Sirmacek, R. Schulte, M. Poel, and C. Brune, \nReinforcement Learning Helps SLAM: Learning to Build Maps, 2020, \npp. 329–335. \n[16] A. Hamza, \"Deep reinforcement learning for mapless mobile robot \nnavigation,\" 2022. \n[17] D. Nister and H. Stewenius, \"Scalable recognition with a vocabulary \ntree,\" in 2006 IEEE Computer Society Conference on Computer Vision \nand Pattern Recognition (CVPR'06), 2006, vol. 2, pp. 2161–2168. \n[18] S. Saeedi, L. Paull, M. Trentini, and H. Li, \"Multiple robot simultaneous \nlocalization and mapping,\" in 2011 IEEE/RSJ International Conference \non Intelligent Robots and Systems, 2011, pp. 853–858. \n[19] G. Grisetti, R. Kümmerle, C. Stachniss, and W. Burgard, \"A tutorial on \ngraph-based SLAM,\" IEEE Intelligent Transportation Systems \nMagazine, vol. 2, no. 4, pp. 31–43, 2010. \n[20] S. Frintrop, P. Jensfelt, and H. Christensen, Simultaneous Robot \nLocalization and Mapping Based on a Visual Attention System, 2007, \npp. 417–430. \n[21] S. Thrun, \"Simultaneous localization and mapping,\" in Robotics and \nCognitive Approaches to Spatial Mapping. Springer, 2008, pp. 13–41. \n[22] S. Zheng, J. Wang, C. Rizos, W. Ding, and A. El-Mowafy, \n\"Simultaneous localization and mapping (SLAM) for autonomous \ndriving: Concept and analysis,\" Remote Sensing, vol. 15, no. 4, p. 1156, \n2023. \n[Online]. \nAvailable: \nhttps://www.mdpi.com/2072-\n4292/15/4/1156. \n[23] H. Carrillo, I. Reid, and J. A. Castellanos, \"On the comparison of \nuncertainty criteria for active SLAM,\" in 2012 IEEE International \nConference on Robotics and Automation. IEEE, 2012, pp. 2080–2087. \n[24] W. Burgard, D. Fox, and S. Thrun, \"Active mobile robot localization,\" \nin IJCAI. Citeseer, 1997, pp. 1346–1352. \n[25] A. A. Makarenko, S. B. Williams, F. Bourgault, and H. F. Durrant-\nWhyte, \"An experiment in integrated exploration,\" in IEEE/RSJ \nInternational Conference on Intelligent Robots and Systems, vol. 1. \nIEEE, 2002, pp. 534–539. \n[26] J. Lin, J. Peng, Z. Hu, X. Xie, and R. Peng, \"ORB-SLAM, IMU and \nwheel odometry fusion for indoor mobile robot localization and \nnavigation,\" Academic Journal of Computer and Information Science, \nvol. 3, 2020. \n[27] A. Kumar, Maneesha, and P. K. Pandey, \"Advances in simultaneous \nlocalization and mapping (SLAM) for autonomous mobile robot \nnavigation,\" in Proceedings of International Joint Conference on \nAdvances in Computational Intelligence. Springer Nature Singapore, \nSingapore, 2024, pp. 481–493. \n[28] E. Reitbauer, C. Schmied, F. Theurl, and M. Wieser, \"LIWO-SLAM: A \nLiDAR, IMU, and wheel odometry simultaneous localization and \nmapping system for GNSS-denied environments based on factor graph \noptimization,\" in Proceedings of the 36th International Technical \nMeeting of the Satellite Division of The Institute of Navigation (ION \nGNSS+ 2023), 2023, pp. 1669–1683. \n[29] S. Nam, C. Woo, S. Kang, T. A. Nguyen, and D. Min, \"SLAM-DRLnav: \nA SLAM-enhanced deep reinforcement learning navigation framework \nfor indoor self-driving,\" in 2023 International Conference on \nMechatronics, Control and Robotics (ICMCR), Feb. 2023, pp. 44–48, \ndoi: 10.1109/ICMCR56776.2023.10181042. \nJournal name \n \n11 \n \nAuthor, Title \n[30] G. Dulac-Arnold et al., \"Challenges of real-world reinforcement \nlearning: Definitions, benchmarks and analysis,\" Machine Learning, \nvol. 110, no. 9, pp. 2419–2468, Sep. 2021, doi: 10.1007/s10994-021-\n05961-4. \n[31] C. Debeunne and D. Vivet, \"A review of visual-LiDAR fusion based \nsimultaneous localization and mapping,\" Sensors, vol. 20, no. 7, p. \n2068, 2020. \n[32] J. Tang et al., \"LiDAR scan matching aided inertial navigation system \nin GNSS-denied environments,\" Sensors, vol. 15, no. 7, pp. 16710–\n16728, 2015. \n[33] Y. Yang, D. Tang, D. Wang, W. Song, J. Wang, and M. Fu, \"Multi-\ncamera visual SLAM for off-road navigation,\" Robotics and \nAutonomous Systems, vol. 128, p. 103505, 2020. \n[34] X. Lang et al., \"Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera \nSLAM with 3D Gaussian splatting,\" arXiv preprint, arXiv:2404.06926, \n2024. \n[35] L. Chang, X. Niu, and T. Liu, \"GNSS/IMU/ODO/LiDAR-SLAM \nintegrated navigation system using IMU/ODO pre-integration,\" \nSensors, vol. 20, p. 4702, Aug. 2020, doi: 10.3390/s20174702. \n[36] N. Abdelaziz and A. El-Rabbany, \"INS/LIDAR/Stereo SLAM \nintegration for precision navigation in GNSS-denied environments,\" \nSensors, vol. 23, no. 17, p. 7424, 2023. [Online]. Available: \nhttps://www.mdpi.com/1424-8220/23/17/7424. \n[37] A. Woo, B. Fidan, and W. W. Melek, \"Localization for autonomous \ndriving,\" in Handbook of Position Location: Theory, Practice, and \nAdvances, 2nd ed., 2018, pp. 1051–1087. \n[38] M. S. A. Khan, D. Hussain, K. Naveed, U. S. Khan, I. Q. Mundial, and \nA. B. Aqeel, \"Investigation of widely used SLAM sensors using \nanalytical hierarchy process,\" Journal of Sensors, vol. 2022, pp. 1–15, \n2022. \n[39] Z. Hong, Y. Petillot, A. Wallace, and S. Wang, \"Radar SLAM: A robust \nSLAM system for all weather conditions,\" arXiv preprint, \narXiv:2104.05347, 2021. \n[40] R. Huang, K. Zhu, S. Chen, T. Xiao, M. Yang, and N. Zheng, \"A high-\nprecision and robust odometry based on sparse MMW radar data and a \nlarge-range and long-distance radar positioning data set,\" in 2021 IEEE \nInternational Intelligent Transportation Systems Conference (ITSC), \n2021, pp. 98–105. \n[41] J. Zhu, H. Li, and T. Zhang, \"Camera, LiDAR, and IMU based multi-\nsensor fusion SLAM: A survey,\" Tsinghua Science and Technology, \nvol. 29, no. 2, pp. 415–429, 2023. \n[42] Y.-E. Lu, S. Tsai, M.-L. Tsai, and K.-W. Chiang, \"A low-cost visual \nradar-based odometry framework with mmWave radar and monocular \ncamera,\" The International Archives of the Photogrammetry, Remote \nSensing and Spatial Information Sciences, vol. 43, pp. 235–240, 2022. \n[43] M. Quan, S. Piao, M. Tan, and S.-S. Huang, \"Tightly-coupled monocular \nvisual-odometric SLAM using wheels and a MEMS gyroscope,\" IEEE \nAccess, vol. 7, pp. 97374–97389, 2019. \n[44] K. Yousif, A. Bab-Hadiashar, and R. Hoseinnezhad, \"An overview to \nvisual odometry and visual SLAM: Applications to mobile robotics,\" \nIntelligent Industrial Systems, vol. 1, no. 4, pp. 289–311, Dec. 2015, \ndoi: 10.1007/s40903-015-0032-7. \n[45] J. Mahmoud and A. Penkovskiy, Dynamic Environments and Robust \nSLAM: Optimizing Sensor Fusion and Semantics for Wheeled Robots, \n2023, pp. 185–191. \n[46] I. H. Sarker, \"Machine learning: Algorithms, real-world applications and \nresearch directions,\" SN Computer Science, vol. 2, no. 3, p. 160, 2021. \n[47] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. \nMIT Press, 2018. \n[48] R. Singh, J. Ren, and X. Lin, \"A review of deep reinforcement learning \nalgorithms for mobile robot path planning,\" Vehicles, vol. 5, no. 4, pp. \n1423–1451, 2023. [Online]. Available: https://www.mdpi.com/2624-\n8921/5/4/78. \n[49] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, \"A \nbrief survey of deep reinforcement learning,\" arXiv preprint, \narXiv:1708.05866, 2017. \n[50] M. Sewak, S. K. Sahay, and H. Rathore, \"Policy-approximation based \ndeep reinforcement learning techniques: An overview,\" in Information \nand Communication Technology for Competitive Strategies (ICTCS \n2020): ICT Applications and Social Interfaces, 2022, pp. 493–507. \n[51] H. Byeon, \"Advances in value-based, policy-based, and deep learning-\nbased reinforcement learning,\" International Journal of Advanced \nComputer Science and Applications, vol. 14, no. 8, 2023. \n[52] J. Clifton and E. Laber, \"Q-learning: Theory and applications,\" Annual \nReview of Statistics and Its Application, vol. 7, no. 1, pp. 279–301, \n2020. \n[53] Y. Huang, \"Deep Q-networks,\" in Deep Reinforcement Learning: \nFundamentals, Research and Applications, 2020, pp. 135–160. \n[54] A. Momenikorbekandi and M. Abbod, \"Intelligent scheduling based on \nreinforcement learning approaches: Applying advanced Q-learning and \nstate–action–reward–state–action reinforcement learning models for \nthe optimisation of job shop scheduling problems,\" Electronics, vol. 12, \nno. 23, p. 4752, 2023. \n[55] S. Zhao and S.-H. Hwang, \"Exploration-and exploitation-driven deep \ndeterministic policy gradient for active SLAM in unknown indoor \nenvironments,\" Electronics, vol. 13, no. 5, p. 999, 2024. \n[56] R. J. Williams, \"Simple statistical gradient-following algorithms for \nconnectionist reinforcement learning,\" Machine Learning, vol. 8, pp. \n229–256, 1992. \n[57] J. Schulman, \"Trust region policy optimization,\" arXiv preprint, \narXiv:1502.05477, 2015. \n[58] V. T. Potter, Exploring Deep Reinforcement Learning Techniques for \nAutonomous Navigation, 2022. \n[59] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \n\"Proximal \npolicy \noptimization \nalgorithms,\" \narXiv \npreprint, \narXiv:1707.06347, 2017. \n[60] D. Joel, Y. Niv, and E. Ruppin, \"Actor–critic models of the basal \nganglia: New anatomical and computational perspectives,\" Neural \nNetworks, vol. 15, no. 4–6, pp. 535–547, 2002. \n[61] I. Grondman, M. Vaandrager, L. Busoniu, R. Babuska, and E. \nSchuitema, \"Efficient model learning methods for actor–critic control,\" \nIEEE Transactions on Systems, Man, and Cybernetics, Part B \n(Cybernetics), vol. 42, no. 3, pp. 591–602, 2011. \n[62] X. Wang et al., \"Deep reinforcement learning: A survey,\" IEEE \nTransactions on Neural Networks and Learning Systems, 2022. \n[63] A. Zai and B. Brown, Deep Reinforcement Learning in Action. Manning \nPublications, 2020. \n[64] X. Lei, Z. Zhang, and P. Dong, \"Dynamic path planning of unknown \nenvironment based on deep reinforcement learning,\" Journal of \nRobotics, vol. 2018, 2018. \n[65] M.-F. R. Lee and S. H. Yusuf, \"Mobile robot navigation using deep \nreinforcement learning,\" Processes, vol. 10, no. 12, p. 2748, 2022. \n[Online]. Available: https://www.mdpi.com/2227-9717/10/12/2748. \n[66] C.-C. Wong, H.-M. Feng, and K.-L. Kuo, \"Multi-sensor fusion \nsimultaneous localization mapping based on deep reinforcement \nlearning and multi-model adaptive estimation,\" Sensors, vol. 24, no. 1, \np. 48, 2024. [Online]. Available: https://www.mdpi.com/1424-\n8220/24/1/48. \n[67] W. Su, T. Huang, F. Liu, and H. Wang, \"ASLAM-FD: A multi-sensor \nadaptive collaborative fusion SLAM framework based on degradation \ndetection and deep reinforcement learning,\" Measurement Science and \nTechnology, vol. 35, no. 12, p. 126312, Sep. 2024, doi: 10.1088/1361-\n6501/ad7bdd. \n[68] X. Liu, S. Wen, Y. Hu, F. Han, H. Zhang, and H. R. Karimi, \"An active \nSLAM with multi-sensor fusion for snake robots based on deep \nreinforcement learning,\" Mechatronics, vol. 103, p. 103248, Nov. \n2024, doi: https://doi.org/10.1016/j.mechatronics.2024.103248. \n[69] Q. Wang, W. Zhuang, L. Wang, and F. Ju, \"Lane keeping assist for an \nautonomous vehicle based on deep reinforcement learning,\" SAE \nTechnical Paper, 0148-7191, 2020. \n[70] C. Arvind and J. Senthilnath, \"Autonomous vehicle for obstacle \ndetection and avoidance using reinforcement learning,\" in Soft \nComputing for Problem Solving: SocProS 2018, Volume 1, Springer, \n2020, pp. 55–66. \n[71] K. Rezaee, P. Yadmellat, and S. Chamorro, \"Motion planning for \nautonomous vehicles in the presence of uncertainty using \nreinforcement learning,\" in 2021 IEEE/RSJ International Conference \non Intelligent Robots and Systems (IROS), IEEE, 2021, pp. 3506–3511. \n[72] C. Tang, B. Abbatematteo, J. Hu, R. Chandra, R. Martín-Martín, and P. \nStone, \"Deep reinforcement learning for robotics: A survey of real-\nworld successes,\" arXiv preprint, arXiv:2408.03539, 2024. \n[73] Y. Song, A. Romero, M. Müller, V. Koltun, and D. Scaramuzza, \n\"Reaching the limit in autonomous racing: Optimal control versus \nreinforcement learning,\" Science Robotics, vol. 8, no. 82, p. eadg1462, \n2023. \n[74] M. Baziyad, M. Saad, R. Fareh, T. Rabie, and I. Kamel, \"Addressing \nreal-time demands for robotic path planning systems: A routing \nprotocol approach,\" IEEE Access, vol. 9, pp. 38132–38143, 2021. \n[75] S.-Y. Chen, Q.-F. He, and C.-F. Lai, \"Deep reinforcement learning-\nbased robot exploration for constructing map of unknown \nenvironment,\" Information Systems Frontiers, vol. 26, no. 1, pp. 63–\n74, 2024. \nJournal name \n \n12 \n \nAuthor, Title \n[76] W. Ruiqi, \"Research on robot path planning based on reinforcement \nlearning,\" arXiv preprint, arXiv:2404.14077, 2024. \n[77] S.-Y. Chen, Q.-F. He, and C.-F. Lai, \"Deep reinforcement learning-\nbased robot exploration for constructing map of unknown \nenvironment,\" Information Systems Frontiers, vol. 26, Nov. 2021, doi: \n10.1007/s10796-021-10218-5. \n[78] H. Li, Q. Zhang, and D. Zhao, \"Deep reinforcement learning-based \nautomatic exploration for navigation in unknown environment,\" IEEE \nTransactions on Neural Networks and Learning Systems, vol. 31, no. \n6, pp. 2064–2076, 2020, doi: 10.1109/TNNLS.2019.2927869. \n[79] A. Iqbal, \"Obstacle detection and track detection in autonomous cars,\" \n2020. \n[80] S. Wen, Y. Zhao, X. Yuan, Z. Wang, D. Zhang, and L. Manfredi, \"Path \nplanning for active SLAM based on deep reinforcement learning under \nunknown environments,\" Intelligent Service Robotics, vol. 13, pp. 263–\n272, 2020. \n[81] S. Nam, C. Woo, S. Kang, T. A. Nguyen, and D. Min, \"iNAV-drlSLAM: \nAn improved indoor self-driving framework for mobile robots using \ndeep reinforcement learning integrated with SLAM,\" in 2023 15th \nInternational Conference on Advanced Computational Intelligence \n(ICACI), \nMay \n2023, \npp. \n1–8, \ndoi: \n10.1109/ICACI58115.2023.10146173. \n[82] A. R. Fayjie, S. Hossain, D. Oualid, and D. J. Lee, \"Driverless car: \nAutonomous driving using deep reinforcement learning in urban \nenvironment,\" in 2018 15th International Conference on Ubiquitous \nRobots \n(UR), \nJun. \n2018, \npp. \n896–901, \ndoi: \n10.1109/URAI.2018.8441797. \n[83] S. Arshad and G.-W. Kim, \"Role of deep learning in loop closure \ndetection for visual and lidar SLAM: A survey,\" Sensors, vol. 21, no. \n4, p. 1243, 2021. \n[84] K. A. Tsintotas, L. Bampis, and A. Gasteratos, \"The revisiting problem \nin simultaneous localization and mapping: A survey on visual loop \nclosure detection,\" IEEE Transactions on Intelligent Transportation \nSystems, vol. 23, no. 11, pp. 19929–19953, 2022. \n[85] A. Charroud, K. El Moutaouakil, V. Palade, A. Yahyaouy, U. Onyekpe, \nand E. U. Eyo, \"Localization and mapping for self-driving vehicles: A \nsurvey,\" Machines, vol. 12, no. 2, p. 118, 2024. [Online]. Available: \nhttps://www.mdpi.com/2075-1702/12/2/118. \n[86] A. Iqbal, R. Thapa, and N. R. Gans, \"Deep reinforcement learning based \nloop closure detection,\" Journal of Intelligent & Robotic Systems, vol. \n106, no. 2, p. 51, 2022. \n[87] A. Iqbal, Classified Object Localization in SLAM and Loop Closure \nThrough Reinforcement Learning. The University of Texas at Dallas, \n2019. \n[88] J. A. Placed and J. A. Castellanos, \"A deep reinforcement learning \napproach for active SLAM,\" Applied Sciences, vol. 10, no. 23, p. 8386, \n2020. \n[Online]. \nAvailable: \nhttps://www.mdpi.com/2076-\n3417/10/23/8386. \n[89] Y. Chen, S. Huang, and R. Fitch, \"Active SLAM for mobile robots with \narea coverage and obstacle avoidance,\" IEEE/ASME Transactions on \nMechatronics, vol. 25, no. 3, pp. 1182–1192, 2020. \n[90] N. Palomeras, M. Carreras, and J. Andrade-Cetto, \"Active SLAM for \nautonomous underwater exploration,\" Remote Sensing, vol. 11, no. 23, \np. 2827, 2019. \n[91] B. Fang, X. Chen, Z. Pan, and X. Di, \"SLAMuZero: Plan and learn to \nmap for joint SLAM and navigation,\" in 34th International Conference \non Automated Planning and Scheduling. \n[92] Z. Pei, S. Piao, M. Quan, M. Z. Qadir, and G. Li, \"Active collaboration \nin relative observation for multi-agent visual simultaneous localization \nand mapping based on deep Q network,\" International Journal of \nAdvanced Robotic Systems, vol. 17, no. 2, p. 1729881420920216, 2020. \n[93] M. Alcalde, M. Ferreira, P. González, F. Andrade, and G. Tejera, \"DA-\nSLAM: Deep active SLAM based on deep reinforcement learning,\" in \n2022 Latin American Robotics Symposium (LARS), 2022 Brazilian \nSymposium on Robotics (SBR), and 2022 Workshop on Robotics in \nEducation (WRE). IEEE, 2022, pp. 282–287. \n[94] R. Zhao, Y. Li, Y. Fan, F. Gao, M. Tsukada, and Z. Gao, \"A survey on \nrecent advancements in autonomous driving using deep reinforcement \nlearning: Applications, challenges, and solutions,\" IEEE Transactions \non Intelligent Transportation Systems, 2024. \n[95] A. Tourani, H. Bavle, J. L. Sanchez-Lopez, and H. Voos, \"Visual \nSLAM: What are the current trends and what to expect?,\" Sensors, vol. \n22, \nno. \n23, \np. \n9297, \n2022. \n[Online]. \nAvailable: \nhttps://www.mdpi.com/1424-8220/22/23/9297. \n[96] M. N. Favorskaya, \"Deep learning for visual SLAM: The state-of-the-\nart and future trends,\" Electronics, vol. 12, no. 9, p. 2006, 2023. \n[Online]. Available: https://www.mdpi.com/2079-9292/12/9/2006. \n[97] H. Bavle, J. L. Sanchez-Lopez, C. Cimarelli, A. Tourani, and H. Voos, \n\"From SLAM to situational awareness: Challenges and survey,\" \nSensors, vol. 23, no. 10, p. 4849, 2023. \n[98] C. Tessler, T. Zahavy, D. Cohen, D. J. Mankowitz, and S. Mannor, \n\"Action assembly: Sparse imitation learning for text-based games with \ncombinatorial action spaces,\" arXiv preprint, arXiv:1905.09700, 2019. \n[99] A. Kudriashov, T. Buratowski, M. Giergiel, and P. Małka, SLAM \nTechniques Application for Mobile Robot in Rough Terrain. Springer, \n2020. \n[100] J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee, \"Sample-\nefficient reinforcement learning with stochastic ensemble value \nexpansion,\" Advances in Neural Information Processing Systems, vol. \n31, 2018. \n[101] C. Pang, L. Zhou, and X. Huang, \"A low-cost 3D SLAM system \nintegration of autonomous exploration based on fast-ICP enhanced \nLiDAR-inertial odometry,\" Remote Sensing, vol. 16, no. 11, p. 1979, \n2024. \n[102] T. Hester and P. Stone, \"Texplore: Real-time sample-efficient \nreinforcement learning for robots,\" Machine Learning, vol. 90, pp. \n385–429, 2013. \n[103] N. Botteghi, B. Sirmacek, R. Schulte, M. Poel, and C. Brune, \n\"Reinforcement learning helps SLAM: Learning to build maps,\" The \nInternational Archives of the Photogrammetry, Remote Sensing and \nSpatial Information Sciences, vol. 43, pp. 329–335, 2020. \n \n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2024-08-26",
  "updated": "2025-01-12"
}