{
  "id": "http://arxiv.org/abs/2408.14785v1",
  "title": "Unsupervised-to-Online Reinforcement Learning",
  "authors": [
    "Junsu Kim",
    "Seohong Park",
    "Sergey Levine"
  ],
  "abstract": "Offline-to-online reinforcement learning (RL), a framework that trains a\npolicy with offline RL and then further fine-tunes it with online RL, has been\nconsidered a promising recipe for data-driven decision-making. While sensible,\nthis framework has drawbacks: it requires domain-specific offline RL\npre-training for each task, and is often brittle in practice. In this work, we\npropose unsupervised-to-online RL (U2O RL), which replaces domain-specific\nsupervised offline RL with unsupervised offline RL, as a better alternative to\noffline-to-online RL. U2O RL not only enables reusing a single pre-trained\nmodel for multiple downstream tasks, but also learns better representations,\nwhich often result in even better performance and stability than supervised\noffline-to-online RL. To instantiate U2O RL in practice, we propose a general\nrecipe for U2O RL to bridge task-agnostic unsupervised offline skill-based\npolicy pre-training and supervised online fine-tuning. Throughout our\nexperiments in nine state-based and pixel-based environments, we empirically\ndemonstrate that U2O RL achieves strong performance that matches or even\noutperforms previous offline-to-online RL approaches, while being able to reuse\na single pre-trained model for a number of different downstream tasks.",
  "text": "Unsupervised-to-Online Reinforcement Learning\nJunsu Kim1,2∗†\nSeohong Park2∗\nSergey Levine2\n1KAIST\n2University of California, Berkeley\njunsu.kim@kaist.ac.kr\nAbstract\nOffline-to-online reinforcement learning (RL), a framework that trains a policy\nwith offline RL and then further fine-tunes it with online RL, has been considered a\npromising recipe for data-driven decision-making. While sensible, this framework\nhas drawbacks: it requires domain-specific offline RL pre-training for each task,\nand is often brittle in practice. In this work, we propose unsupervised-to-online\nRL (U2O RL), which replaces domain-specific supervised offline RL with unsu-\npervised offline RL, as a better alternative to offline-to-online RL. U2O RL not\nonly enables reusing a single pre-trained model for multiple downstream tasks, but\nalso learns better representations, which often result in even better performance and\nstability than supervised offline-to-online RL. To instantiate U2O RL in practice,\nwe propose a general recipe for U2O RL to bridge task-agnostic unsupervised of-\nfline skill-based policy pre-training and supervised online fine-tuning. Throughout\nour experiments in nine state-based and pixel-based environments, we empirically\ndemonstrate that U2O RL achieves strong performance that matches or even out-\nperforms previous offline-to-online RL approaches, while being able to reuse a\nsingle pre-trained model for a number of different downstream tasks.\n1\nIntroduction\nAcross natural language processing (NLP), computer vision (CV), and speech processing, ubiquitous\nin the recent successes of machine learning is the idea of adapting an expressive model pre-trained on\nlarge-scale data to domain-specific tasks via fine-tuning. In the domain of reinforcement learning\n(RL), offline-to-online RL has been considered an example of such a recipe for leveraging offline data\nfor efficient online fine-tuning. Offline-to-online RL first trains a task-specific policy on a previously\ncollected dataset with offline RL, and then continues training the policy with additional environment\ninteractions to further improve performance.\nBut, is offline-to-online RL really the most effective way to leverage offline data for online RL?\nIndeed, offline-to-online RL has several limitations. First, it pre-trains a policy with a domain-specific\ntask reward, which precludes sharing a single pre-trained model for multiple downstream tasks. This\nis in contrast to predominant pre-training recipes in large language models or visual representation\nlearning, where they pre-train large models with self-supervised or unsupervised objectives to learn\nuseful representations, which can facilitate learning a wide array of downstream tasks. Second, naïve\noffline-to-online RL is often brittle in practice [37, 51]. This is mainly because pre-trained offline\nRL agents suffer the distributional shift between the offline and online interaction data [37, 51] or\nexperience feature collapse (Section 5.5), which necessitates specialized, potentially complicated\ntechniques.\nIn this work, our central hypothesis is that unsupervised pre-training of diverse policies from offline\ndata can serve as an effective data-driven recipe for online RL, and can be more effective than even\n∗Equal contribution\n†Work done while visiting UC Berkeley\nPreprint. Under review.\narXiv:2408.14785v1  [cs.LG]  27 Aug 2024\n(Supervised) Offline RL Pre-training\nUnsupervised Offline RL Pre-training\nUnsupervsied-to-Online RL\nOffline-to-Online RL\nOnline RL Fine-tuning\nOnline RL Fine-tuning\nBridging\nOffline \ndataset\n⇡(a | s, z)\nOffline \ndataset\n⇡(a | s)\n(s, a, r, s0)\nOffline \ndataset\n(s, a, r, s0)\n⇡(a | s, z⇤)\n(s, a, r, s0)\n⇡(a | s)\nz 2 Z\n(s, a, s0, rint, z)\n(s, a, r, s0)\n⇡(a | s, z⇤)\nFigure 1: Illustration of U2O RL. In this work, we propose to replace supervised offline RL with unsupervised\noffline RL in the offline-to-online RL framework. We call this scheme unsupervised-to-online RL (U2O RL).\nU2O RL consists of three stages: (1) unsupervised offline RL pre-training, (2) bridging, and (3) online RL\nfine-tuning. In unsupervised offline RL pre-training, we train a multi-task skill policy πθ(a | s, z) instead of a\nsingle-task policy πθ(a | s). Then, we convert the multi-task policy into a task-specific policy in the bridging\nphase. Finally, we fine-tune the skill policy with online environment interactions.\ndomain-specific (“supervised”) offline pre-training. We call this framework unsupervised-to-online\nRL (U2O RL). U2O RL has two appealing properties. First, unlike offline-to-online RL, a single\npre-trained model can be fine-tuned for different downstream tasks. Since offline unsupervised RL\ndoes not require task information, we can pre-train diverse policies on unlabeled data before knowing\ndownstream tasks. Second, by pre-training multi-task policies with diverse intrinsic rewards, the agent\nextracts rich representations from data, which often helps achieve even better final performance and\nstability than supervised offline-to-online RL. This resembles how general-purpose unsupervised pre-\ntraining in other domains, such as with LLMs or self-supervised representations [4, 8, 63, 24, 23, 25],\nimproves over the performance of domain-specific specialist pre-training.\nOur U2O RL framework consists of three stages: unsupervised offline policy pre-training, bridging,\nand online fine-tuning (Figure 1). In the first unsupervised offline pre-training phase, we employ\na skill-based offline unsupervised RL or offline goal-conditioned RL method, which trains diverse\nbehaviors (or skills) with intrinsic rewards and provides an efficient mechanism to identify the best\nskill for a given task reward. In the subsequent bridging and fine-tuning phases, we adapt the best\nskill among the learned policies to the given downstream task reward with online RL. Here, to prevent\na potential mismatch between the intrinsic and task rewards, we propose a simple yet effective reward\nscale matching technique that bridges the gap between the two training schemes and thus improves\nperformance and stability.\nOur main contributions in this work are twofold. First, to the best of our knowledge, this is the first\nwork that makes the (potentially surprising) observation that it is often better to replace supervised\noffline RL with unsupervised offline RL in the offline-to-online RL setting. We also identify the\nreason behind this phenomenon: this is mainly because offline unsupervised pre-training learns better\nrepresentations than task-specific supervised offline RL. Second, we propose a general recipe to\nbridge skill-based unsupervised offline RL pre-training and online RL fine-tuning. Through our\nexperiments on nine state-based and pixel-based environments, we demonstrate that U2O RL often\noutperforms standard offline-to-online RL both in terms of sample efficiency and final performance,\nwhile being able to reuse a single pre-trained model for multiple downstream tasks.\n2\nRelated work\nOnline RL from prior data. Prior works have proposed several ways to leverage a previously\ncollected offline dataset to accelerate online RL training. They can be categorized into two main\ngroups: offline-to-online RL and off-policy online RL with offline data. Offline-to-online RL first\npre-trains a policy and a value function with offline RL [35, 39, 14, 16, 32, 73, 81, 30, 33, 22,\n31, 49, 60, 80], and then continues to fine-tune them with additional online interactions [37, 49,\n2\n51, 86, 38, 89, 46, 88]. Since naïve offline-to-online RL is often unstable in practice due to the\ndistributional shift between the dataset and online interactions, prior works have proposed several\ntechniques, such as balanced sampling [37], actor-critic alignment [86], adaptive conservatism [78],\nand return lower-bounding [51]. In this work, unlike offline-to-online RL, which trains a policy with\nthe target task reward, we offline pre-train a multi-task policy with unsupervised (intrinsic) reward\nfunctions. This makes our single pre-trained policy reusable for any downstream task and learn richer\nrepresentations. The other line of research, off-policy online RL, trains an online RL agent from\nscratch on top of a replay buffer filled with offline data, without any pre-training [2, 40, 42, 71].\nWhile this simple approach often leads to improved stability and performance [2], it does not leverage\nthe benefits of pre-training; in contrast, we do leverage pre-training by learning useful features via\noffline unsupervised RL, which we show leads to better fine-tuning performance in our experiments.\nUnsupervised RL. The goal of unsupervised RL is to leverage unsupervised pre-training to facilitate\ndownstream reinforcement learning. Prior works have mainly focused on unsupervised representation\nlearning and unsupervised behavior learning. Unsupervised representation learning methods [67,\n68, 54, 83, 50, 45, 17, 65, 64, 66] aim to extract useful (visual) representations from data. These\nrepresentations are then fed into the policy to accelerate task learning. In this work, we focus on\nunsupervised behavior learning, which aims to pre-train policies that can be directly adapted to\ndownstream tasks. Among unsupervised behavior learning methods, online unsupervised RL pre-\ntrains useful policies by either maximizing state coverage [58, 59, 47, 41] or learning distinct skills\n[19, 9, 69, 57] via reward-free interactions with the environment. In this work, we consider offline\nunsupervised RL, which does not allow any environment interactions during the pre-training stage.\nOffline unsupervised RL. Offline unsupervised RL methods focus on learning diverse policies (i.e.,\nskills) from the dataset, rather than exploration, as online interactions are not permitted in this problem\nsetting. There exist three main approaches to offline unsupervised RL. Behavioral cloning-based\napproaches extract skills from an offline dataset by training a generative model (e.g., variational\nautoencoders [29], Transformers [77], etc.) [1, 61, 70]. Offline goal-conditioned RL methods learn\ndiverse goal-reaching behaviors with goal-conditioned reward functions [6, 10, 44, 55, 79, 84, 11, 12].\nOffline unsupervised skill learning approaches learn diverse skills based on intrinsically defined\nreward functions [56, 76, 26]. Among these approaches, we use methods in the second and third\ncategories (i.e., goal- or skill-based unsupervised offline RL) as part of our framework.\nOur goal in this work is to study how unsupervised offline RL, as opposed to supervised task-specific\noffline RL, can be employed to facilitate online RL fine-tuning. While somewhat similar unsupervised\npre-training schemes have been explored in prior works, they either consider hierarchical RL (or\nzero-shot RL) with frozen learned skills without fine-tuning [1, 61, 76, 56, 26], assume online-only\nRL [36], or are limited to the specific setting of goal-conditioned RL [11, 12]. To the best of our\nknowledge, this is the first work that considers the fine-tuning of skill policies pre-trained with\nunsupervised offline RL in the context of offline-to-online RL. Through our experiments, we show\nthat our fine-tuning framework leads to significantly better performance than previous approaches\nbased on hierarchical RL, zero-shot RL, and standard offline-to-online RL.\n3\nPreliminaries\nWe formulate a decision making problem as a Markov decision process (MDP) [72], which is defined\nby a tuple of (S, A, P, r, ρ, γ), where S is the state space, A is the action space, P : S × A →∆(S)\nis the transition dynamics, r : S × A × S →R is the task reward function, ρ ∈∆(S) is the initial\nstate distribution, and γ ∈(0, 1) is the discount factor. Our aim is to learn a policy π : S →∆(A)\nthat maximizes the expectation of cumulative task rewards, Eπ[P∞\nt=0 γtr(st, at, st+1)].\nOffline RL and implicit Q-learning (IQL). The goal of offline RL is to learn a policy solely from\nan offline dataset Doff, which consists of transition tuples (s, a, s′, r). One straightforward approach\nto offline RL is to simply employ an off-policy RL algorithm (e.g. TD3 [15]). For instance, we can\nminimize the following temporal difference (TD) loss to learn an action-value function (Q-function)\nfrom data:\nLTD(ϕ) = E(s,a,s′,r)∼Doff\nh\n(r + γ max\na′ Q ¯ϕ(s′, a′) −Qϕ(s, a))2i\n,\n(1)\n3\nwhere Qϕ denotes the parameterized action-value function, and Q ¯ϕ represents the target action-value\nfunction [48], whose parameter ¯ϕ is updated via Polyak averaging [62] using ϕ. We can then train a\npolicy π to maximize Ea∼π [Qϕ(s, a)].\nWhile this simple off-policy TD learning can be enough when the dataset has sufficiently large\nstate-action coverage, offline datasets in practice often have limited coverage, which makes the agent\nsusceptible to value overestimation and exploitation, as the agent cannot get corrective feedback from\nthe environment [39]. To address this issue, Kostrikov et al. [31] proposed implicit Q-learning (IQL),\nwhich fits an optimal action-value function without querying out-of-distribution actions: IQL replaces\nthe arg max operator, which potentially allows the agent to exploit Q-values from out-of-distribution\nactions, with an expectile loss that implicitly approximates the maximum value. Specifically, IQL\nminimizes the following losses:\nLQ\nIQL(ϕ) = E(s,a,s′,r)∼Doff\n\u0002\n(r + γVψ(s′) −Qϕ(s, a))2\u0003\n,\n(2)\nLV\nIQL(ψ) = E(s,a)∼Doff\n\u0002\nℓ2\nτ(Q ¯ϕ(s, a) −Vψ(s))\n\u0003\n,\n(3)\nwhere Qϕ and Q ¯ϕ respectively denote the action-value and target action-value functions, Vψ denotes\nthe value function, ℓ2\nτ(x) = |τ −1(x < 0)|x2 denotes the expectile loss [52] and τ denotes the\nexpectile parameter. Intuitively, the asymmetric expectile loss in Equation 3 makes Vψ implicitly\napproximate maxa Q ¯ϕ(s, a) by penalizing positive errors more than negative errors.\nHilbert foundation policy (HILP). Our unsupervised-to-online framework requires an offline\nunsupervised RL algorithm that trains a skill policy πθ(a | s, z) from an unlabeled dataset, and we\nmainly use HILP [56] in our experiments. HILP consists of two phases. In the first phase, HILP trains\na feature network ξ : S →Z that embeds temporal distances (i.e., shortest path lengths) between\nstates into the latent space by enforcing the following equality:\nd∗(s, g) = ∥ξ(s) −ξ(g)∥2\n(4)\nfor all s, g ∈S, where d∗(s, g) denotes the temporal distance (the minimum number of steps required\nto reach g from s) between s and g. In practice, given the equivalence between goal-conditioned\nvalues and temporal distances, ξ can be trained with any offline goal-conditioned RL algorithm [55]\n(see Park et al. [56] for further details). After training ξ, HILP trains a skill policy πθ(a | s, z) with\nthe following intrinsic reward using an off-the-shelf offline RL algorithm [31, 15]:\nrint(s, a, s′, z) = (ξ(s′) −ξ(s))⊤z,\n(5)\nwhere z is sampled from the unit ball, {z ∈Z : ∥z∥= 1}. Intuitively, Equation 5 encourages\nthe agent to learn behaviors that move in every possible latent direction, resulting in diverse state-\nspanning skills [56]. Note that Equation 5 can be interpreted as the inner product between the task\nvector z and the feature vector f(s, a, s′) := ξ(s′) −ξ(s) in the successor feature framework [7, 3].\n4\nUnsupervised-to-online RL (U2O RL)\nOur main hypothesis in this work is that task-agnostic offline RL pre-training of unsupervised skills\ncan be more effective than task-specific, supervised offline RL for online RL fine-tuning. We call this\nframework unsupervised-to-online RL (U2O RL). In this section, we first describe the three stages\nof our U2O RL framework (Figure 1): unsupervised offline policy pre-training (Section 4.1), bridging\n(Section 4.2), and online fine-tuning (Section 4.3). We then explain why unsupervised-to-online RL\nis potentially better than standard supervised offline-to-online RL (Section 4.4).\n4.1\nUnsupervised offline policy pre-training\nIn the first unsupervised offline policy pre-training phase (Figure 1 (bottom left)), we train diverse\npolicies (or skills) with intrinsic rewards to extract a variety of useful behaviors as well as rich\nfeatures from the offline dataset Doff. In other words, instead of training a single-task policy\nπθ(a | s) with task rewards r(s, a, s′) as in standard offline-to-online RL, we train a multi-task skill\npolicy πθ(a | s, z) with a family of unsupervised, intrinsic rewards rint(s, a, s′, z), where z is a skill\nlatent vector sampled from a latent space Z = Rd. Even if Doff contains reward labels, we do not\nuse any reward information in this phase.\n4\nAlgorithm 1 U2O RL: Unsupervised-to-Online Reinforcement Learning\nRequire: offline dataset Doff, reward-labeled dataset Dreward, empty replay buffer Don, offline\npre-training steps NPT, online fine-tuning steps NFT, skill latent space Z\nInitialize the parameters of policy πθ and action-value function Qϕ\nfor t = 0, 1, 2, . . . NPT −1 do\nSample transitions (s, a, s′) from Doff\nSample latent vector z ∈Z and compute intrinsic rewards rint\nUpdate policy πθ(a | s, z) and Qϕ(s, a, z) using normalized intrinsic rewards ˜rint\nend for\nCompute the best latent vector z∗with Equation 6 using samples (s, a, s′, r) from Dreward\nfor t = 0, 1, 2, . . . NFT −1 do\nCollect transition (s, a, s′, r) via environment interaction with πθ and add to replay buffer Don\nSample transitions (s, a, s′, r) from Doff ∪Don\nUpdate policy πθ(a | s, z∗) and Qϕ(s, a, z∗) using normalized task rewards ˜r\nend for\nAmong existing unsupervised offline policy pre-training methods (Section 2), we opt to employ\nsuccessor feature-based methods [7, 3, 82, 76, 56] or offline goal-conditioned RL methods [6, 55]\nfor our unsupervised pre-training, since they provide a convenient mechanism to identify the best\nskill latent vector given a downstream task, which we will utilize in the next phase. More concretely,\nwe mainly choose to employ HILP [56] (Section 3) as an unsupervised offline policy pre-training\nmethod in our experiments for its state-of-the-art performance in previous benchmarks [56]. We note,\nhowever, that any other unsupervised offline successor feature-based skill learning methods [76] or\noffline goal-conditioned RL methods [55] can also be used in place of HILP (see Appendix A.1).\n4.2\nBridging offline unsupervised RL and online supervised RL\nAfter finishing unsupervised offline policy pre-training, our next step is to convert the learned multi-\ntask skill policy into a task-specific policy that can be fine-tuned to maximize a given downstream\nreward function r (Figure 1 (bottom middle)). There exist two challenges in this step: (1) we need a\nmechanism to identify the skill vector z that best solves the given task and (2) we need to reconcile\nthe gap between intrinsic rewards and downstream task rewards for seamless online fine-tuning.\nSkill identification. Since we chose to use a successor feature- or goal-based unsupervised pre-\ntraining method in the previous phase, the first challenge is relatively straightforward. For goal-\noriented tasks (e.g., AntMaze [13] and Kitchen [20]), we assume the task goal g to be available, and\nwe either directly use g (for goal-conditioned methods) or infer the skill z∗that corresponds to g\nbased on a predefined conversion formula (for successor feature-based methods that support such a\nconversion [76, 56]). For general reward-maximization tasks, we employ successor feature-based\nunsupervised pre-training methods, and use the following linear regression to find the skill latent\nvector z∗that best approximates the downstream task reward function r : S × A × S →R:\nz∗= arg min\nz∈Z\nE(s,a,s′)∼Dreward\nh\u0000r(s, a, s′) −f(s, a, s′)⊤z\n\u00012i\n,\n(6)\nwhere f is the feature network in the successor feature framework (Section 3) and Dreward is a\nreward-labeled dataset. This reward-labeled dataset Dreward can be either the full offline dataset Doff\n(if it is fully reward-labeled), a subset of the offline dataset (if it is partially reward-labeled), or a\nnewly collected dataset with additional environment interactions. In our experiments, we mainly use\na small number (e.g., 0.2% for DMC tasks) of reward-labeled samples from the offline dataset for\nDreward, following previous works [76, 56], but we do not require Dreward to be a subset of Doff (see\nAppendix A.4).\nReward scale matching. After identifying the best skill latent vector z∗, our next step is to bridge\nthe gap between intrinsic and extrinsic rewards. Since these two reward functions can have very\ndifferent scales, naïve online adaptation can lead to abrupt shifts in target Q-values, potentially\ncausing significant performance drops in the early stages of online fine-tuning. While one can employ\nsophisticated reward-shaping techniques to deal with this issue [53, 18], in this work, we propose a\nsimple yet effective reward scale matching technique that we find to be effective enough in practice.\nSpecifically, we compute the running mean and standard deviation of intrinsic rewards during the\n5\n(a) Walker\n(b) Cheetah\n(c) Quadruped\n(d) Jaco\n(e) AntMaze-\nLarge\n(f) AntMaze-\nUltra\n(g) Kitchen\n(h) Adroit-\nPen\n(i) Adroit-\nDoor\nFigure 2: Environments. We evaluate U2O RL on nine state-based or pixel-based environments.\npre-training phase, and normalize the intrinsic rewards with the calculated statistics. Similarly, during\nthe fine-tuning phase, we compute the statistics of task rewards and normalize the task rewards so\nthat they have the same scale and mean as normalized intrinsic rewards. This way, we can prevent\nabrupt shifts in reward scales without altering the optimal policy for the downstream task. In our\nexperiments, we find that this simple technique is crucial for achieving good performance, especially\nin environments with dense rewards (Section 5.7).\n4.3\nOnline fine-tuning\nOur final step is to fine-tune the skill policy with online environment interactions (Figure 1 (bottom\nright)). This step is straightforward: since we have found z∗in the previous stage, we can simply\nfix the skill vector z∗in the policy πθ(a | s, z∗) and the Q-function Qϕ(s, a, z∗), and fine-tune\nthem with the same (offline) RL algorithm used in the first phase (e.g., IQL [31], TD3 [15]) with\nadditional online interaction data. While one can employ existing specialized techniques for offline-\nto-online RL for better online adaptation in this phase, we find in our experiments that, thanks\nto rich representations learned by unsupervised pre-training, simply using the same (offline) RL\nalgorithm is enough to achieve strong performance that matches or even outperforms state-of-the-art\noffline-to-online RL techniques.\n4.4\nWhy is U2O RL potentially better than offline-to-online RL?\nOur main claim is that unsupervised offline RL is better than supervised offline RL for online fine-\ntuning. However, this might sound very counterintuitive. Especially, if we know the downstream\ntask ahead of time, how can unsupervised offline RL potentially lead to better performance than\nsupervised offline RL, despite the fact that the former does not use any task information during the\noffline phase?\nWe hypothesize that this is because unsupervised multi-task offline RL enables better feature learning\nthan supervised single-task offline RL. By training the agent on a number of diverse intrinsically\ndefined tasks, it gets to acquire rich knowledge about the environment, dynamics, and potential\ntasks in the form of representations, which helps improve and facilitate the ensuing task-specific\nonline fine-tuning. This resembles the recent observation in machine learning that large-scale\nunsupervised pre-training improves downstream task performances over task-specific supervised\npre-training [4, 8, 63, 24, 23, 25]. In our experiments, we empirically show that U2O RL indeed\nlearns better features than its supervised counterpart (Section 5.5).\nAnother benefit of U2O RL is that it does not use any task-specific information during pre-training.\nThis is appealing because we can reuse a single pre-trained policy for a number of different down-\nstream tasks. Moreover, it enables leveraging potentially very large, task-agnostic offline data during\npre-training, which is often much cheaper to collect than task-specific, curated datasets [43].\n5\nExperiments\nIn our experiments, we evaluate the performance of U2O RL in the context of offline-to-online RL. We\naim to answer the following research questions: (1) Is U2O RL better than previous offline-to-online\nRL strategies? (2) What makes unsupervised offline RL result in better fine-tuning performance than\nsupervised offline RL? (3) Which components of U2O RL are important?\n6\nU2O (Ours)\nO2O\nOnline w/ Off Data\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-ultra-diverse\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-ultra-play\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-large-diverse\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-large-play\n0k\n100k\n200k\n300k\n400k\n500k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nKitchen-partial\n0k\n100k\n200k\n300k\n400k\n500k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nKitchen-mixed\n0k\n100k\n200k\n300k\n400k\n500k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nVisual-Kitchen-partial\n0k\n100k\n200k\n300k\n400k\n500k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nVisual-Kitchen-mixed\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nCheetah Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nQuadruped Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n50\n100\n150\n200\n250\nUnnormalized Return\nJaco Reach Top Left\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAdroit-pen-binary\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAdroit-door-binary\nFigure 3: Online fine-tuning plots of U2O RL and previous offline-to-online RL frameworks (8 seeds).\nAcross the benchmarks, our U2O RL mostly shows consistently better performance than standard offline-to-\nonline RL and off-policy online RL with offline data.\n5.1\nExperimental setup\nEnvironments and offline datasets. In our experiments, we consider nine tasks across five bench-\nmarks (Figure 2). ExORL [85] is a benchmark suite that consists of offline datasets collected by\nexploratory policies (e.g., RND [5]) on the DeepMind Control Suite [75]. We consider four embodi-\nments (Walker, Cheetah, Quadruped, and Jaco), each of which has four tasks. AntMaze [13, 27] is a\nnavigation task, whose goal is to control an 8-DoF quadruped agent to reach a target position. We\nconsider the two most challenging mazes with the largest sizes, large and ultra, and two types\nof offline datasets, diverse and play. Kitchen [20, 13] is a robotic manipulation task, where the\ngoal is to control a 9-DoF Franka robot arm to achieve four subtasks sequentially. The environment\nhas four subtasks in total. We consider two types of offline datasets from the D4RL suite [13],\npartial and mixed. Visual Kitchen [20, 13, 56] is a pixel-based variant of the Kitchen environ-\nment, where an agent must achieve four subtasks purely from 64 × 64 × 3 pixel observations instead\nof low-dimensional state information. Adroit [13] is a dexterous manipulation benchmark, where the\ngoal is to control a 24-DoF robot hand to twirl a pen or open a door. We provide further details in\nAppendix B.1\nImplementation. In our experiments, we mainly employ HILP [56] as the unsupervised offline\npolicy pre-training algorithm in U2O RL. For the offline RL backbone, we use TD3 [15] for ExORL\nand IQL [30] for others following previous works [76, 56]. Since both IQL and TD3+BC [14, 73]\nhave been known to achieve strong performance in the offline-to-online RL setting [74], we use them\nfor the online fine-tuning phase in U2O RL as well. For sparse-reward tasks (AntMaze, Kitchen,\nand Adroit), we do not apply reward scale matching. For AntMaze, Kitchen, and Adroit, we report\n7\nTable 1: Comparison between U2O RL and previous offline-to-online RL methods. We denote how\nperformances change before and after online fine-tuning with arrows. Baseline scores except RLPD [2] are taken\nfrom Nakamoto et al. [51], Wang et al. [78]. Scores within the 5% of the best score are highlighted in bold, as in\nKostrikov et al. [31]. We use 8 random seeds for each task for U2O RL.\nTask\nantmaze-ultra-diverse\nantmaze-ultra-play\nantmaze-large-diverse\nantmaze-large-play\nkitchen-partial\nkitchen-mixed\nCQL\n-\n-\n25 →87\n34 →76\n71 →75\n56 →50\nIQL\n13 →29\n17 →29\n40 →59\n41 →51\n40 →60\n48 →48\nAWAC\n-\n-\n00 →00\n00 →00\n01 →13\n02 →12\nO3F\n-\n-\n59 →28\n68 →01\n11 →22\n06 →33\nODT\n-\n-\n00 →01\n00 →00\n-\n-\nCQL+SAC\n-\n-\n36 →00\n21 →00\n71 →00\n59 →01\nHybrid RL\n-\n-\n→00\n→00\n→00\n→01\nSAC+od\n-\n-\n→00\n→00\n→07\n→00\nSAC\n-\n-\n→00\n→00\n→03\n→02\nIQL+od\n→04\n→05\n→71\n→56\n→74\n→61\nFamO2O\n-\n-\n→64\n→61\n-\n-\nRLPD\n00 →00\n00 →00\n00 →94\n00 →81\n-\n-\nCal-QL\n05 →05\n15 →13\n33 →95\n26 →90\n67 →79\n38 →80\nU2O (Ours)\n22 →54\n17 →58\n11 →95\n14 →88\n48 →75\n48 →74\nnormalized scores, following Fu et al. [13]. In our experiments, we use 8 random seeds and report\nstandard deviations with shaded areas, unless otherwise stated. We refer the reader to Appendix B for\nthe full implementation details and hyperparameters.\n5.2\nIs U2O RL better than previous offline-to-online RL frameworks?\nWe begin our experiments by comparing our approach, unsupervised-to-online RL, with two previous\noffline-to-online RL frameworks (Section 2): offline-to-online RL (O2O RL) and off-policy online\nRL with offline data (Online w/ Off Data). To recall, offline-to-online RL [37, 49, 51, 86, 38] first\npre-trains a policy with supervised offline RL using the task reward, and then continues training it\nwith online rollouts. Off-policy online RL [2, 42, 71] trains a policy from scratch on top of a replay\nbuffer filled with offline data. Here, we use the same offline RL backbone (i.e., TD3 for ExORL and\nIQL for AntMaze, Kitchen, and Adroit) to ensure apples-to-apples comparisons between the three\nframeworks. We will compare U2O RL with previous specialized offline-to-online RL techniques in\nSection 5.3.\nFigure 3 shows the online fine-tuning curves on 14 different tasks. The results suggest that U2O\nRL generally leads to better performance than both offline-to-online RL and off-policy online RL\nacross the environments, despite not using any task information during pre-training. Notably, U2O\nRL significantly outperforms these two previous frameworks in the most challenging AntMaze tasks\n(antmaze-ultra-{diverse, play}).\n5.3\nHow does U2O RL compare to previous specialized offline-to-online RL techniques?\nNext, we compare U2O RL with 13 previous specialized offline-to-online RL methods, including\nCQL [33], IQL [31], AWAC [49], O3F [46], ODT [89], CQL+SAC [33, 21], Hybrid RL [71],\nSAC+od (offline data) [21, 2], SAC [21], IQL+od (offline data) [31, 2], RLPD [2], FamO2O [78]\nand Cal-QL [51]. We show the comparison results in Table 1, where we take the scores from\nNakamoto et al. [51], Wang et al. [78] for the tasks that are common to ours. Since Cal-QL\nachieves the best performance in the table, we additionally make a comparison with Cal-QL on\nantmaze-ultra-{diverse, play} as well, by running their official implementation with tuned\nhyperparameters.\nTable 1 shows that U2O RL achieves strong performance that matches or sometimes even outperforms\nprevious offline-to-online RL methods, even though U2O RL does not use any task information\nduring offline pre-training nor any specialized offline-to-online techniques. In particular, in the most\nchallenging antmaze-ultra tasks, U2O RL outperforms the previous best method (Cal-QL) by a\nsignificant margin. This is very promising because, even if U2O RL does not necessarily outperform\nthe state-of-the-art methods on every single task (though it is at least on par with the previous best\nmethods), U2O RL enables reusing a single unsupervised pre-trained policy for multiple downstream\ntasks, unlike previous offline-to-online RL methods that perform task-specific pretraining.\n8\nU2O (Ours)\nO2O\nOnline w/ Off Data\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nCheetah Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nQuadruped Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n50\n100\n150\n200\n250\nUnnormalized Return\nJaco Reach Top Left\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Flip\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nCheetah Run Backward\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nQuadruped Jump\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n50\n100\n150\n200\n250\nUnnormalized Return\nJaco Reach Top Right\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Stand\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nCheetah Walk\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nQuadruped Stand\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n50\n100\n150\n200\n250\nUnnormalized Return\nJaco Reach Bottom Left\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Walk\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nCheetah Walk Backward\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nQuadruped Walk\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n50\n100\n150\n200\n250\nUnnormalized Return\nJaco Reach Bottom Right\nFigure 4: Learning curves during online RL fine-tuning (8 seeds). A single pre-trained model from U2O\ncan be fine-tuned to solve multiple downstream tasks. Across the embodiments and tasks, our U2O RL matches\nor outperforms standard offline-to-online RL and off-policy online RL with offline data even though U2O RL\nuses a single task-agnostic pre-trained model.\n5.4\nCan a single pre-trained model from U2O be fine-tuned to solve multiple tasks?\nOne important advantage of U2O RL is that it can reuse a single task-agnostic dataset for multiple\ndifferent downstream tasks, unlike standard offline-to-online RL. To demonstrate this, we train U2O\nRL with four different tasks from the same task-agnostic ExORL dataset on each DMC environment,\nand report the full training curves in Figure 4. The results show that, for example, a single pre-trained\nmodel on the Walker domain can be fine-tuned for all four tasks (Walker Run, Walker Flip, Walker\nStand, and Walker Walk). Note that even though U2O RL uses a single task-agnostic pre-trained\nmodel, the performance of U2O RL matches or even outperforms O2O RL, which pre-trains a model\nwith task-specific rewards.\n5.5\nWhy does U2O RL often outperform supervised offline-to-online RL?\nIn the above experiments, we showed that our unsupervised-to-online RL framework often even\noutperforms previous supervised offline-to-online RL methods. We hypothesized in Section 4.4 that\nthis is because unsupervised offline pre-training yields better representations that facilitate online\ntask adaptation. To empirically verify this hypothesis, we measure the quality of the value function\nrepresentations using the method proposed by Kumar et al. [34]. Specifically, we define the value\nfeatures ζϕ(s, a) as the penultimate layer of the value function Qϕ, i.e., Qϕ(s, a) = w⊤\nϕ ζϕ(s, a), and\nmeasure the dot product between consecutive state-action pairs, ζϕ(s, a)⊤ζϕ(s′, a′) [34]. Intuitively,\n9\n0k\n200k\n400k\n600k\n800k 1000k\nPre-training Steps\n0\n5000\n10000\n15000\n20000\nFeature Dot Product\nWalker Run\nU2O\nO2O\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n5000\n10000\nFeature Dot Product\nCheetah Run\nU2O\nO2O\n0k\n200k\n400k\n600k\n800k 1000k\nPre-training Steps\n0\n200\n400\n600\n800\nFeature Dot Product\nAntMaze-large-diverse\nU2O\nO2O\n0k\n100k\n200k\n300k\n400k\n500k\nPre-training Steps\n0\n200\n400\n600\n800\n1000\nFeature Dot Product\nKitchen-partial\nU2O\nO2O\nFigure 5: Feature dot products during offline RL pre-training (lower is better, 8 seeds). The plots\nshow that unsupervised offline pre-training effectively prevents feature collapse (co-adaptation), yielding better\nrepresentations than supervised offline pre-training.\nthis dot product represents the degree to which these two representations are “collapsed” (or “co-\nadapted”), which is known to be correlated with poor performance [34] (i.e., the lower the better).\nFigure 5 compares the dot product metrics of unsupervised offline RL (in U2O RL) and supervised\noffline RL (in O2O RL) on four benchmark tasks. The results suggest that our unsupervised multi-task\npre-training effectively prevents feature co-adaptation and thus indeed yields better representations\nacross the environments. This highlights the benefits of unsupervised offline pre-training, and\n(partially) explains the strong online fine-tuning performance of U2O RL. We additionally provide\nfurther analyses with different offline unsupervised RL algorithms (e.g., graph Laplacian-based\nsuccessor feature learning [76, 82]) in Appendix A.1.\n5.6\nIs fine-tuning better than other alternative strategies (e.g., hierarchical RL)?\nIn this work, we focus on the fine-tuning of offline pre-trained skill policies, but this is not the only\nway to leverage pre-trained skills for downstream tasks. To see how our fine-tuning scheme compares\nto other alternative strategies, we compare U2O RL with three previously considered approaches:\nhierarchical RL (HRL, e.g., OPAL [1], SPiRL [61]) [1, 61, 76, 56, 26], zero-shot RL [76, 56],\nand PEX [87]. HRL additionally trains a high-level policy that combines fixed pre-trained skills in a\nsequential manner. Zero-shot RL simply finds the skill policy that best solves the downstream task,\nwith no fine-tuning or hierarchies. PEX combines fixed pre-trained multi-task policies and a newly\ninitialized policy with a multiplexer that chooses the best policy.\nOur fine-tuning\nPEX\nHRL\nZero-shot\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-large-diverse\nFigure 6: Fine-tuning is better than previous strate-\ngies, such as hierarchical RL, zero-shot RL, and PEX\n(8 seeds).\nFigure 6 shows the comparison results on top of\nthe same pre-trained unsupervised skill policy.\nSince PEX is not directly compatible with IQL,\nwe evaluate PEX only on the tasks with TD3\n(e.g., ExORL tasks). The plots suggest that our\nfine-tuning strategy leads to significantly better\nperformance than previous approaches. This\nis because pre-trained offline skill policies are\noften not perfect (due to the limited coverage\nor suboptimality of the dataset), and thus using\na fixed offline policy is often not sufficient to\nachieve strong performance in downstream tasks.\nWe provide additional results in Appendix A.6.\n5.7\nAblation study\nReward scale matching. In Section 4.2, we propose a simple reward scale matching technique that\nbridges the gap between intrinsic rewards and downstream task rewards. We ablate this component,\nand report the results in Figure 7. The results suggest that our reward scale matching technique\neffectively prevents a performance drop at the beginning of the online fine-tuning stage, leading to\nsubstantially better final performance on dense-reward tasks (e.g., Walker Run and Cheetah Run).\nValue transfer vs. policy transfer. In our U2O RL framework, we transfer both the value function\nand policy from unsupervised pre-training to supervised fine-tuning. To dissect the importance of\neach component, we conduct an ablation study, in which we compare four settings: (1) without any\ntransfer, (2) value-only transfer, (3) policy-only transfer, and (4) full transfer. Figure 8 demonstrates\nthe ablation results on Walker and AntMaze. The results suggest that both value transfer and policy\n10\nU2O (w/ reward scale match)\nU2O (w/o reward scale match)\n0k\n50k\n100k\n150k\n200k\nEnvironment Steps\n0\n200\n400\n600\n800\nUnnormalized Return\nWalker Run\n0k\n50k\n100k\n150k\n200k\nEnvironment Steps\n0\n200\n400\n600\n800\nUnnormalized Return\nCheetah Run\nFigure 7: Ablation study of reward scale matching\n(4 seeds).\nno transfer\ntransfer value\ntransfer policy\ntransfer value & policy\n0k\n50k\n100k\n150k\n200k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-large-diverse\nFigure 8: Ablation study of value transfer and pol-\nicy transfer (4 seeds).\ntransfer matter in general, but value transfer is more important than policy transfer. This aligns with\nour findings in Section 5.5 as well as Kumar et al. [34], which says that the quality of value features\noften correlates with the performance of TD-based RL algorithms.\nWe refer to the reader to Appendix for further analysis including (1) combining U2O RL with other\noffline unsupervised skill learning methods (Appendix A.1), (2) U2O RL on a different type of\ndatasets (Appendix A.2), (3) comparisons between U2O RL and pure representation learning schemes\n(Appendix A.3), (4) U2O RL without reward samples in the bridging phase (Appendix A.4), (5) an\nablation study with different skill identification strategies (Appendix A.5), and (6) additional results\nwith different online RL strategies (Appendix A.6).\n6\nConclusion\nIn this work, we investigated how unsupervised pre-training of diverse policies enables better online\nfine-tuning than standard supervised offline-to-online RL. We showed that our U2O RL framework\noften achieves even better performance and stability than previous offline-to-online RL approaches,\nthanks to the rich representations learned by pre-training on diverse tasks. We also demonstrated that\nU2O RL enables reusing a single offline pre-trained policy for multiple downstream tasks.\nDo we expect U2O RL to be always better than O2O RL? While we showed strong results of\nU2O RL throughout the paper, our framework does have limitations. One limitation is that U2O\nRL may not be as effective when the offline dataset is monolithic and heavily tailored toward the\ndownstream task (see Appendix A.2), which leaves little room for improvement for unsupervised\noffline RL compared to task-specific supervised offline RL. We believe U2O RL can be most effective\n(compared to standard offline-to-online RL) when the dataset is highly diverse so that unsupervised\noffline RL methods can learn a variety of behaviors and thus learn better features and representations.\nNonetheless, given the recent successes in large-scale self-supervised and unsupervised pre-training\nfrom unlabeled data, we believe our U2O RL framework serves as a step toward a general recipe for\nscalable data-driven decision-making.\nAcknowledgments and Disclosure of Funding\nWe thank Qiyang Li, Mitsuhiko Nakamoto, Kevin Frans, Younggyo Seo, Changyeon Kim, and Juyong\nLee for insightful discussions and helpful feedback on earlier drafts of this work. This work was\npartly supported by the Korea Foundation for Advanced Studies (KFAS), ONR N00014-21-1-2838,\nand Intel. This research used the Savio computational cluster resource provided by the Berkeley\nResearch Computing program at UC Berkeley.\nReferences\n[1] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum. Opal: Offline primitive discovery\nfor accelerating offline reinforcement learning. In International Conference on Learning\nRepresentations, 2021.\n[2] P. J. Ball, L. Smith, I. Kostrikov, and S. Levine. Efficient online reinforcement learning with\noffline data. In International Conference on Machine Learning, 2023.\n11\n[3] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver.\nSuccessor features for transfer in reinforcement learning. In Advances in Neural Information\nProcessing Systems, 2017.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems, 2020.\n[5] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation.\nIn International Conference on Learning Representations, 2019.\n[6] Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan, B. Eysenbach,\nR. C. Julian, C. Finn, et al. Actionable models: Unsupervised offline reinforcement learning of\nrobotic skills. In International Conference on Machine Learning, 2021.\n[7] P. Dayan. Improving generalization for temporal difference learning: The successor representa-\ntion. Neural computation, 5:613–624, 1993.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, 2019.\n[9] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills\nwithout a reward function. In International Conference on Learning Representations, 2019.\n[10] B. Eysenbach, T. Zhang, S. Levine, and R. R. Salakhutdinov. Contrastive learning as goal-\nconditioned reinforcement learning. In Advances in Neural Information Processing Systems,\n2022.\n[11] K. Fang, P. Yin, A. Nair, and S. Levine. Planning to practice: Efficient online fine-tuning by\ncomposing goals in latent space. In International Conference on Intelligent Robots and Systems,\n2022.\n[12] K. Fang, P. Yin, A. Nair, H. R. Walke, G. Yan, and S. Levine. Generalization with lossy\naffordances: Leveraging broad offline data for learning visuomotor tasks. In Conference on\nRobot Learning, 2023.\n[13] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven\nreinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[14] S. Fujimoto and S. S. Gu. A minimalist approach to offline reinforcement learning. In Advances\nin Neural Information Processing Systems, 2021.\n[15] S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic\nmethods. In International Conference on Machine Learning, 2018.\n[16] S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without explo-\nration. In International Conference on Machine Learning, 2019.\n[17] D. Ghosh, C. A. Bhateja, and S. Levine. Reinforcement learning from passive data via latent\nintentions. In International Conference on Machine Learning, 2023.\n[18] A. Gleave, M. Dennis, S. Legg, S. Russell, and J. Leike. Quantifying differences in reward\nfunctions. In International Conference on Learning Representations, 2021.\n[19] K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. arXiv preprint\narXiv:1611.07507, 2016.\n[20] A. Gupta, V. Kumar, C. Lynch, S. Levine, and K. Hausman. Relay policy learning: Solving\nlong-horizon tasks via imitation and reinforcement learning. In Conference on Robot Learning,\n2020.\n12\n[21] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. In International Conference on Machine\nLearning, 2018.\n[22] P. Hansen-Estruch, I. Kostrikov, M. Janner, J. G. Kuba, and S. Levine. Idql: Implicit q-learning\nas an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023.\n[23] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\nrepresentation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020.\n[24] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable\nvision learners. arXiv preprint arXiv:2111.06377, 2021.\n[25] O. J. Hénaff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch, S. A. Eslami, and A. van den\nOord. Data-efficient image recognition with contrastive predictive coding. In International\nConference on Machine Learning, 2020.\n[26] H. Hu, Y. Yang, J. Ye, Z. Mai, and C. Zhang. Unsupervised behavior extraction via random\nintent priors. In Advances in Neural Information Processing Systems, 2023.\n[27] Z. Jiang, T. Zhang, M. Janner, Y. Li, T. Rocktäschel, E. Grefenstette, and Y. Tian. Efficient plan-\nning in a compact latent action space. In International Conference on Learning Representations,\n2023.\n[28] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations, 2015.\n[29] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference\non Learning Representations, 2014.\n[30] I. Kostrikov, R. Fergus, J. Tompson, and O. Nachum. Offline reinforcement learning with fisher\ndivergence critic regularization. In International Conference on Machine Learning, 2021.\n[31] I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning. In\nInternational Conference on Learning Representations, 2022.\n[32] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine. Stabilizing off-policy q-learning via\nbootstrapping error reduction. In Advances in Neural Information Processing Systems, 2019.\n[33] A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offline reinforcement\nlearning. Advances in Neural Information Processing Systems, 2020.\n[34] A. Kumar, R. Agarwal, T. Ma, A. Courville, G. Tucker, and S. Levine. DR3: Value-based\ndeep reinforcement learning requires explicit regularization. In International Conference on\nLearning Representations, 2022.\n[35] S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In Reinforcement learning:\nState-of-the-art, pages 45–73. Springer, 2012.\n[36] M. Laskin, D. Yarats, H. Liu, K. Lee, A. Zhan, K. Lu, C. Cang, L. Pinto, and P. Abbeel.\nUrlb: Unsupervised reinforcement learning benchmark. In Advances in Neural Information\nProcessing Systems Datasets and Benchmarks Track, 2021.\n[37] S. Lee, Y. Seo, K. Lee, P. Abbeel, and J. Shin. Offline-to-online reinforcement learning via\nbalanced replay and pessimistic q-ensemble. In Conference on Robot Learning, 2022.\n[38] K. Lei, Z. He, C. Lu, K. Hu, Y. Gao, and H. Xu. Uni-o4: Unifying online and offline deep\nreinforcement learning with multi-step on-policy optimization. arXiv preprint arXiv:2311.03351,\n2023.\n[39] S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review,\nand perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n13\n[40] Q. Li, J. Zhang, D. Ghosh, A. Zhang, and S. Levine. Accelerating exploration with unlabeled\nprior data. In Advances in Neural Information Processing Systems, 2023.\n[41] H. Liu and P. Abbeel. Behavior from the void: Unsupervised active pre-training. arXiv preprint\narXiv:2103.04551, 2021.\n[42] J. Luo, Z. Hu, C. Xu, Y. L. Tan, J. Berg, A. Sharma, S. Schaal, C. Finn, A. Gupta, and S. Levine.\nSerl: A software suite for sample-efficient robotic reinforcement learning. arXiv preprint\narXiv:2401.16013, 2024.\n[43] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet. Learning\nlatent plans from play. In Conference on Robot Learning, 2019.\n[44] J. Y. Ma, J. Yan, D. Jayaraman, and O. Bastani. Offline goal-conditioned reinforcement learning\nvia f-advantage regression. In Advances in Neural Information Processing Systems, 2022.\n[45] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. VIP: Towards\nuniversal visual reward and representation via value-implicit pre-training. In International\nConference on Learning Representations, 2023.\n[46] M. S. Mark, A. Ghadirzadeh, X. Chen, and C. Finn. Fine-tuning offline policies with optimistic\naction selection. In Deep Reinforcement Learning Workshop NeurIPS 2022, 2022.\n[47] R. Mendonca, O. Rybkin, K. Daniilidis, D. Hafner, and D. Pathak. Discovering and achieving\ngoals via world models. In Advances in Neural Information Processing Systems, 2021.\n[48] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.\nPlaying atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n[49] A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning\nwith offline datasets. arXiv preprint arXiv:2006.09359, 2020.\n[50] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation\nfor robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n[51] M. Nakamoto, S. Zhai, A. Singh, M. Sobol Mark, Y. Ma, C. Finn, A. Kumar, and S. Levine.\nCal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. In Advances in Neural\nInformation Processing Systems, 2023.\n[52] W. K. Newey and J. L. Powell. Asymmetric least squares estimation and testing. Econometrica:\nJournal of the Econometric Society, pages 819–847, 1987.\n[53] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory\nand application to reward shaping. In International Conference on Machine Learning, 1999.\n[54] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of\npre-trained vision models for control. In International Conference on Machine Learning, 2022.\n[55] S. Park, D. Ghosh, B. Eysenbach, and S. Levine. Hiql: Offline goal-conditioned rl with latent\nstates as actions. In Advances in Neural Information Processing Systems, 2024.\n[56] S. Park, T. Kreiman, and S. Levine. Foundation policies with hilbert representations. arXiv\npreprint arXiv:2402.15567, 2024.\n[57] S. Park, O. Rybkin, and S. Levine. METRA: Scalable unsupervised RL with metric-aware\nabstraction. In International Conference on Learning Representations, 2024.\n[58] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-\nsupervised prediction. In International Conference on Machine Learning, 2017.\n[59] D. Pathak, D. Gandhi, and A. Gupta.\nSelf-supervised exploration via disagreement.\nIn\nInternational Conference on Machine Learning, 2019.\n[60] X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and\nscalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n14\n[61] K. Pertsch, Y. Lee, and J. Lim. Accelerating reinforcement learning with learned skill priors. In\nConference on Robot Learning, 2021.\n[62] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\njournal on control and optimization, 1992.\n[63] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 2019.\n[64] Y. Seo, D. Hafner, H. Liu, F. Liu, S. James, K. Lee, and P. Abbeel. Masked world models for\nvisual control. In Conference on Robot Learning, 2022.\n[65] Y. Seo, K. Lee, S. James, and P. Abbeel. Reinforcement learning with action-free pre-training\nfrom videos. In International Conference on Machine Learning, 2022.\n[66] Y. Seo, J. Kim, S. James, K. Lee, J. Shin, and P. Abbeel. Multi-view masked world models for\nvisual robotic manipulation. In International Conference on Machine Learning, 2023.\n[67] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine. Time-contrastive\nnetworks: Self-supervised learning from video. In International Conference on Robotics and\nAutomation, 2018.\n[68] R. Shah and V. Kumar. Rrl: Resnet as representation for reinforcement learning. In International\nConference on Machine Learning, 2021.\n[69] A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised\ndiscovery of skills. In International Conference on Learning Representations, 2020.\n[70] A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, and S. Levine. Parrot: Data-driven behavioral\npriors for reinforcement learning. In International Conference on Learning Representations,\n2021.\n[71] Y. Song, Y. Zhou, A. Sekhari, J. A. Bagnell, A. Krishnamurthy, and W. Sun. Hybrid RL: Using\nboth offline and online data can make RL efficient. In International Conference on Learning\nRepresentations, 2023.\n[72] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press, 2018.\n[73] D. Tarasov, V. Kurenkov, A. Nikulin, and S. Kolesnikov. Revisiting the minimalist approach to\noffline reinforcement learning. In Advances in Neural Information Processing Systems, 2023.\n[74] D. Tarasov, A. Nikulin, D. Akimov, V. Kurenkov, and S. Kolesnikov. Corl: Research-oriented\ndeep offline reinforcement learning library. In Advances in Neural Information Processing\nSystems, 2023.\n[75] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki,\nJ. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.\n[76] A. Touati, J. Rapin, and Y. Ollivier. Does zero-shot reinforcement learning exist? In Interna-\ntional Conference on Learning Representations, 2022.\n[77] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems,\n2017.\n[78] S. Wang, Q. Yang, J. Gao, M. Lin, H. Chen, L. Wu, N. Jia, S. Song, and G. Huang. Train once,\nget a family: State-adaptive balances for offline-to-online reinforcement learning. In Advances\nin Neural Information Processing Systems, 2023.\n[79] T. Wang, A. Torralba, P. Isola, and A. Zhang. Optimal goal-reaching reinforcement learning via\nquasimetric learning. In International Conference on Machine Learning, 2023.\n[80] Z. Wang, A. Novikov, K. Zolna, J. S. Merel, J. T. Springenberg, S. E. Reed, B. Shahriari,\nN. Siegel, C. Gulcehre, N. Heess, et al. Critic regularized regression. In Advances in Neural\nInformation Processing Systems, 2020.\n15\n[81] Y. Wu, G. Tucker, and O. Nachum. Behavior regularized offline reinforcement learning. arXiv\npreprint arXiv:1911.11361, 2019.\n[82] Y. Wu, G. Tucker, and O. Nachum. The laplacian in RL: Learning representations with efficient\napproximations. In International Conference on Learning Representations, 2019.\n[83] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik. Masked visual pre-training for motor control.\narXiv preprint arXiv:2203.06173, 2022.\n[84] R. Yang, L. Yong, X. Ma, H. Hu, C. Zhang, and T. Zhang. What is essential for unseen goal\ngeneralization of offline goal-conditioned rl? In International Conference on Machine Learning,\n2023.\n[85] D. Yarats, D. Brandfonbrener, H. Liu, M. Laskin, P. Abbeel, A. Lazaric, and L. Pinto. Don’t\nchange the algorithm, change the data: Exploratory data for offline reinforcement learning.\narXiv preprint arXiv:2201.13425, 2022.\n[86] Z. Yu and X. Zhang. Actor-critic alignment for offline-to-online reinforcement learning. In\nInternational Conference on Machine Learning, 2023.\n[87] H. Zhang, W. Xu, and H. Yu. Policy expansion for bridging offline-to-online reinforcement\nlearning. In International Conference on Learning Representations, 2023.\n[88] K. Zhao, Y. Ma, J. Liu, H. Jianye, Y. Zheng, and Z. Meng. Improving offline-to-online\nreinforcement learning with q-ensembles. In ICML Workshop on New Frontiers in Learning,\nControl, and Dynamical Systems, 2023.\n[89] Q. Zheng, A. Zhang, and A. Grover. Online decision transformer. In International Conference\non Machine Learning, 2022.\n16\nAppendices\nA\nAdditional Experiments\nA.1\nCan U2O RL be combined with other offline unsupervised RL methods?\nU2O (Lap; Ours)\nO2O\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Run\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nQuadruped Run\nFigure 9: U2O RL with Laplacian-based successor\nfeature learning (8 seeds).\nU2O (GC-IQL; Ours)\nO2O\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-ultra-diverse\n0k\n100k\n200k\n300k\n400k\n500k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nKitchen-partial\nFigure 10: U2O RL with goal-conditioned IQL (8\nseeds).\nWhile we employ HILP [56] as an offline unsupervised skill learning method in the U2O RL\nframework in our main experiments, our framework can be combined with other offline unsupervised\nskill learning methods as well. To show this, we replace HILP with a graph Laplacian-based successor\nfeature method [76, 82] or goal-conditioned IQL (GC-IQL) [31, 55], and report the results in Figures 9\nand 10, respectively. The results demonstrate that U2O RL with different unsupervised RL methods\nalso improves performance over standard offline-to-online RL.\nU2O (Lap; Ours)\nO2O\n0k\n200k\n400k\n600k\n800k 1000k\nPre-training Steps\n0\n5000\n10000\n15000\n20000\nFeature Dot Product\nWalker Run\n0k\n200k\n400k\n600k\n800k 1000k\nPre-training Steps\n0\n5000\n10000\n15000\n20000\nFeature Dot Product\nQuadruped Run\nFigure 11:\nFeature dot product analysis with\nLaplacian-based successor feature learning (8 seeds).\nAdditionally, we show that other unsupervised\nskill learning methods also lead to better value\nrepresentations.\nWe measure the same fea-\nture dot product metric in Section 5.5 with the\ngraph Laplacian-based successor feature learn-\ning method and report the results in Figure 11.\nThe results suggest that this unsupervised RL\nmethod also prevents feature co-adaptation, lead-\ning to better features.\nA.2\nWhen is U2O RL better than O2O RL?\nU2O (Ours)\nO2O\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-large-diverse\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n20\n40\n60\n80\n100\nNormalized Return\nAntMaze-ultra-diverse\nFigure 12: Online RL learning curves with expert-\nonly datasets (4 seeds).\nIn Section 6, we discussed that U2O RL is\nadvantageous over O2O RL especially when\nthe offline dataset allows the underlying un-\nsupervised RL method to learn diverse behav-\niors and thus capture diverse features.\nCon-\nversely, we do not particularly expect U2O RL\nto be better than O2O RL when the dataset is\nmonolithic (e.g., consists of only expert trajec-\ntories, has less diversity, etc.). To empirically\nshow this, we conduct an additional experiment\nwith a different dataset in AntMaze-large and\nAntMaze-ultra that consists of monolithic, ex-\npert trajectories (we collect a 1M-sized dataset by rolling out an offline pre-trained policy). Figure 12\nshows the results. As expected, we find that the performance difference between U2O and O2O is\nmarginal in this monolithic setting.\n17\nA.3\nDo we need to use unsupervised RL for pre-training representations?\nTable 2: Comparison between U2O RL and pure\nrepresentation learning algorithms (4 seeds).\nTask\nantmaze-large-diverse\nU2O (HILP, Q Ours)\n94.50 ± 3.16\nU2O (HILP, ξ)\n5.50 ± 1.91\nTemporal contrastive learning\n37.50 ± 15.00\nIn Sections 4.4 and 5.5, we hypothesized and\nempirically showed that U2O RL is often better\nthan O2O RL because it learns better represen-\ntations. This leads to the following natural ques-\ntion: do we need to use offline unsupervised\nreinforcement learning, as opposed to general\nrepresentation learning? To answer this ques-\ntion, we consider two pure representation learn-\ning algorithms as alternatives to unsupervised RL: temporal contrastive learning [10] and Hilbert\n(metric) representation learning [56], where the latter is equivalent to directly taking ξ in the HILP\nframework (Equation 4) (note that the original U2O RL takes the Q function of HILP, not the\nHilbert representation ξ itself, which is used to train the Q function). To evaluate their fine-tuning\nperformances, for the temporal contrastive representation, we fine-tune both the Q function and\npolicy with contrastive RL [10]; for the Hilbert representation, we take the pre-trained representation,\nadd one new layer, and use it as the initialization of the Q function. Table 2 shows the results on\nantmaze-large-diverse. Somewhat intriguingly, the results suggest that it is important to use\nthe full unsupervised RL procedure, and pure representation learning methods result in much worse\nperformance in this case. This becomes more evident if we compare U2O RL (HILP Q, ours) and\nU2O RL (HILP ξ), given that they are rooted in the same Hilbert representation. We believe this\nis because, if we simply use an off-the-shelf representation learning, there exists a discrepancy in\ntraining objectives between pre-training (e.g., metric learning) and fine-tuning (Q-learning). On the\nother hand, in U2O RL, we pre-train a representation with unsupervised Q-learning (though with a\ndifferent reward function), and thus the discrepancy between pre-training and fine-tuning becomes\nless severe.\nA.4\nCan we do “bridging” without any reward-labeled data?\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nWalker Run\nU2O\nU2O (w/o offline samples)\nO2O\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nQuadruped Run\nU2O\nU2O (w/o offline samples)\nO2O\nFigure 13: U2O RL without using reward-labeling\nin the offline dataset (8 seeds).\nIn the bridging phase of U2O RL (Section 4.2),\nwe assume a (small) reward-labeled dataset\nDreward. In our experiments, we sample a small\nnumber of transitions (e.g., 0.2% in the case of\nDMC) from the offline dataset and label them\nwith the ground-truth reward function, as in prior\nworks [76, 56]. However, these samples do not\nnecessarily have to come from the offline dataset.\nTo show this, we conduct an additional experi-\nment where we do not assume access to any of\nthe existing reward samples or the ground-truth\nreward function in the bridging phase. Specifically, we collect 10K online samples with random skills\nand perform the linear regression in Equation 6 only using the collected online transitions. We report\nthe performances of U2O (without offline samples) and O2O in Figure 13. The results show that\nU2O still works and outperforms the supervised offline-to-online RL baseline.\nA.5\nHow do different strategies of skill identification affect performance?\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n400\n600\n800\n1000\nUnnormalized Return\nWalker Run\nU2O\nU2O (random latent)\nFigure\n14:\nAblation\nstudy of skill identifica-\ntion (4 seeds).\n0k\n200k\n400k\n600k\n800k 1000k\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nUnnormalized Return\nCheetah Run\nOur fine-tuning\nPEX\nZero-shot\nFigure 15: Comparison\nwith PEX and zero-shot\nRL (4 seeds).\nTo understand how skill identification strategies\naffect online RL performance, we compare our\nstrategy in Section 4.2 with an alternative strat-\negy that simply selects a random latent vector\nz from the skill space. Figure 14 shows that\nthe skill identification with a randomly selected\nlatent vector performs worse than our strategy.\nThis is likely because modulating the policy with\nthe best latent vector helps boost task-relevant\nexploration and information.\n18\nA.6\nAdditional experiments on fine-tuning strategies\nWe additionally provide experimental results of fine-tuning strategies on a different task (i.e., Cheetah\nRun). Figure 15 shows that our fine-tuning strategy outperforms previous strategies, such as zero-shot\nRL and PEX. This result further supports the effectiveness of fine-tuning.\nB\nExperimental Details\nFor offline RL pre-training, we use 1M training steps for ExORL, AntMaze, and Adroit and 500K\nsteps for Kitchen, following Park et al. [56]. For online fine-tuning, we use 1M additional environment\nsteps for ExORL, AntMaze, and Adroit and 500K steps for Kitchen with an update-to-data ratio\nof 1. We implement U2O RL based on the official implementation of HILP [56]. We evaluate the\nnormalized return with 50 episodes every 10k online steps for ExORL tasks, and every 100k online\nsteps for AntMaze, Kitchen, and Adroit tasks. We run our experiments on A5000 or RTX 3090\nGPUs. Each run takes at most 40 hours (e.g. Visual Kitchen). We provide our implementation in the\nsupplementary material.\nB.1\nEnvironments and Datasets\nExORL [85].\nIn the ExORL benchmark, we consider four embodiments, Walker, Cheetah,\nQuadruped, and Jaco. Each embodiment has four tasks: Walker has {Run, Flip, Stand, Walk},\nCheetah has {Run, Run Backward, Walk, Walk Backward}, Quadruped has {Run, Jump, Stand,\nWalk}, and Jaco has {Reach Top Left, Reach Top Right, Reach Bottom Left, Reach Bottom Right}.\nFor all the tasks in Walker, Cheetah, and Quadruped, the maximum return is 1000, and Jaco has\n250. Each embodiment has an offline dataset, which is collected by running exploratory agents such\nas RND [5], and then annotated with task reward function. We use the first 5M transitions of the\noffline dataset following the prior work [76, 56]. The maximum episode length is 250 (Jaco) or 1000\n(others).\nAntMaze [13, 27]. In AntMaze, a quadruped agent aims at reaching the (pre-defined) target position\nin a maze and gets a positive reward when the agent arrives at a pre-defined neighborhood of the target\nposition. We consider two types of Maze: antmaze-large [13] and antmaze-ultra [27], where\nthe latter has twice the size of the former. Each maze has two types of offline datasets: play and\ndiverse. The dataset consists of 999 trajectories with an episode length of 1000. In each trajectory,\nan agent is initialized at a random location in the maze and is directed to an arbitrary location, which\nmay not be the same as the target goal. At the evaluation, antmaze-large has a maximum episode\nlength of 1000, and antmaze-ultra has 2000. We report normalized scores by multiplying the\nreturns by 100.\nKitchen [20, 13]. In the Kitchen environment, a Franka robot should achieve four sub-tasks,\nmicrowave, slide cabinet, light switch, and kettle. Each task has a success criterion\ndetermined by an object configuration. Whenever the agent achieves a sub-task, a task reward of\n1 is given, where the maximum return is 4. We consider two types of offline datasets: mixed and\npartial. We report normalized scores by multiplying the returns by 100. For Visual-Kitchen, we\nfollow the same camera configuration as Mendonca et al. [47], Park et al. [57], and Park et al. [56],\nto render 64 × 64 RGB observations, which are used instead of low-dimensional states. We report\nnormalized scores by multiplying the returns by 25.\nAdroit [13]. In Adroit, a 24-DoF Shadow Hand robot should be controlled to achieve a desired\ntask. We consider two tasks: pen-binary and door-binary, following prior works [2, 40]. The\nmaximum episode lengths of pen-binary and door-binary are 100 and 200. respectively. We\nreport normalized scores by multiplying the returns by 100.\n19\nB.2\nHyperparameters\nTable 3: Hyperparameters of unsupervised RL pre-training in ExORL.\nHyperparameter\nValue\nLearning rate\n0.0005 (feature), 0.0001 (others)\nOptimizer\nAdam [28]\nMinibatch size\n1024\nFeature MLP dimensions\n(512, 512)\nValue MLP dimensions\n(1024, 1024, 1024)\nPolicy MLP dimensions\n(1024, 1024, 1024)\nTD3 target smoothing coefficient\n0.01\nTD3 discount factor γ\n0.98\nLatent dimension\n50\nState samples for latent vector inference\n10000\nSuccessor feature loss\nQ loss\nHilbert representation discount factor\n0.96 (Walker), 0.98 (others)\nHilbert representation expectile\n0.5\nHilbert representation target smoothing coefficient\n0.005\nTable 4: Hyperparameters of unsupervised RL pre-training in AntMaze, Kitchen, and Adroit.\nHyperparameter\nValue\nLearning rate\n0.0003\nOptimizer\nAdam [28]\nMinibatch size\n256 (Adroit), 512 (others)\nValue MLP dimensions\n(256, 256, 256) (Adroit), (512, 512, 512) (others)\nPolicy MLP dimensions\n(256, 256, 256) (Adroit), (512, 512, 512) (others)\nTarget smoothing coefficient\n0.005\nDiscount factor γ\n0.99\nLatent dimension\n32\nHilbert representation discount factor\n0.99\nHilbert representation expectile\n0.95\nHilbert representation target smoothing coefficient\n0.005\nHILP IQL expectile\n0.9 (AntMaze), 0.7 (others)\nHILP AWR temperature\n0.5 (Kitchen) 3 (Adroit-door), 10 (others)\n20\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-08-27",
  "updated": "2024-08-27"
}