{
  "id": "http://arxiv.org/abs/2408.15727v1",
  "title": "Deep Reinforcement Learning for Radiative Heat Transfer Optimization Problems",
  "authors": [
    "Eva Ortiz-Mansilla",
    "Juan José García-Esteban",
    "Jorge Bravo-Abad",
    "Juan Carlos Cuevas"
  ],
  "abstract": "Reinforcement learning is a subfield of machine learning that is having a\nhuge impact in the different conventional disciplines, including physical\nsciences. Here, we show how reinforcement learning methods can be applied to\nsolve optimization problems in the context of radiative heat transfer. We\nillustrate their use with the optimization of the near-field radiative heat\ntransfer between multilayer hyperbolic metamaterials. Specifically, we show how\nthis problem can be formulated in the language of reinforcement learning and\ntackled with a variety of algorithms. We show that these algorithms allow us to\nfind solutions that outperform those obtained using physical intuition.\nOverall, our work shows the power and potential of reinforcement learning\nmethods for the investigation of a wide variety of problems in the context of\nradiative heat transfer and related topics.",
  "text": "Deep Reinforcement Learning for Radiative Heat Transfer Optimization Problems\nE. Ortiz-Mansilla†, J. J. Garc´ıa-Esteban†, J. Bravo-Abad, and J. C. Cuevas∗\nDepartamento de F´ısica Te´orica de la Materia Condensada,\nUniversidad Aut´onoma de Madrid, 28049 Madrid, Spain and\nCondensed Matter Physics Center (IFIMAC), Universidad Aut´onoma de Madrid, 28049 Madrid, Spain\n(Dated: August 29, 2024)\nReinforcement learning is a subfield of machine learning that is having a huge impact in the differ-\nent conventional disciplines, including physical sciences. Here, we show how reinforcement learning\nmethods can be applied to solve optimization problems in the context of radiative heat transfer. We\nillustrate their use with the optimization of the near-field radiative heat transfer between multilayer\nhyperbolic metamaterials. Specifically, we show how this problem can be formulated in the language\nof reinforcement learning and tackled with a variety of algorithms. We show that these algorithms\nallow us to find solutions that outperform those obtained using physical intuition.\nOverall, our\nwork shows the power and potential of reinforcement learning methods for the investigation of a\nwide variety of problems in the context of radiative heat transfer and related topics.\nI.\nINTRODUCTION\nThermal radiation is an ubiquitous physical phe-\nnomenon whose understanding is of critical importance\nfor many different areas of science and engineering [1–3].\nThe field of radiative heat transfer is enjoying a revival\ndue to various recent advances [4]. Maybe the most no-\ntable one is the demonstration that the near-field radia-\ntive heat transfer (NFRHT) between two closely placed\nbodies can largely overcome the blackbody limit set by\nStefan-Boltzmann’s law. This was predicted in the early\n1970s [5] and it has been verified in recent years in a\nlarge variety of systems with the help of novel experi-\nmental techniques [4, 6, 7]. This effect originates from the\nfact that, when two objects are separated by a distance\nsmaller than the thermal wavelength λTh (∼10 µm at\nroom temperature), the radiative heat flux can be greatly\nenhanced by the additional contribution of evanescent\nwaves – which is not considered in Stefan-Boltzmann’s\nlaw. Near-field thermal radiation has opened new possi-\nbilities and holds the promise to have a notable impact\nin different technologies such as heat-assisted magnetic\nrecording [8], scanning thermal microscopy [9–11], co-\nherent thermal sources [12, 13], near-field based thermal\nmanagement [4, 7] or thermophotovoltaics [14].\nNFRHT is by no means the only breakthrough in the\nfield of thermal radiation in recent times. Thus, for in-\nstance, it has been shown that nanophotonic structures,\nwhere at least one of the structural features is at subwave-\nlength scale, can have thermal radiation properties that\ndiffer drastically from those of conventional thermal emit-\nters [15]. This has led to the development and improve-\nment of energy applications such as daytime passive ra-\ndiative cooling [16, 17], thermal radiative textiles [18, 19],\nradiative cooling of solar cells [20], or thermophotovoltaic\ncells [21]. On a more fundamental level, another remark-\nable discovery has been the possibility of overcoming the\n∗juancarlos.cuevas@uam.es\nfar-field limits set by Planck’s law in the context of the\nthermal emission and the radiative heat transfer between\nsubwavelength objects [22–24].\nAt this stage, the physical mechanisms of radiative\nheat transfer in the different regimes are relatively well\nunderstood and the interest is now shifting towards the\noptimization and design of novel thermal devices. This\nprocess is being mainly assisted by physical intuition and\nstandard numerical optimization methods. Thus, for in-\nstance, in the context of NFRHT, many different ana-\nlytical upper bounds have been put forward to establish\nthe limits of near-field thermal radiation [7, 25]. These\nbounds are extremely ingenious, but often lack the ability\nto guide in practice the fabrication of actual structures.\nOn the other hand, conventional numerical optimization\ntechniques, such as Bayesian or topology optimization\n[26], are also being routinely used in the field.\nAt the same time, the impressive achievements of ma-\nchine learning techniques in different engineering areas\nhave motivated many researchers to pursue a data-driven\napproach to investigate a plethora of problems in con-\nventional science disciplines, including physical sciences\n[27–29]. Radiative heat transfer is not an exception and\nin recent years different groups have applied various ma-\nchine learning techniques to address key problems in this\nfield. Most of the work thus far has been carried out with\nthe help of artificial neural networks (ANNs) and deep\nlearning algorithms. Thus for instance, we have shown\nhow ANNs can be used to tackle optimization and in-\nverse design problems in the context of NFRHT, passive\nradiative cooling and thermal emission of subwavelength\nobjects [30]. There has also been a tremendous activity in\nthe context of deep learning aided design and optimiza-\ntion of thermal metamaterials, for a recent review see\nRef. [31]. However, reinforcement learning, another sub-\nfield of machine learning, has been barely used in modern\nradiative heat transfer problems, with notable exceptions\n[32]. Reinforcement learning (RL) is much closer to the\nlayman’s view of artificial intelligence and it deals with\nproblems concerning sequential decision making [33]. In\nRL, an agent learns via the interaction with an envi-\narXiv:2408.15727v1  [physics.optics]  28 Aug 2024\n2\nronment from which it receives feedback to make good\ndecisions towards a given goal, such as the optimization\nof a physical process or the inverse design of a device.\nIn this work we want to fill this gap and show how RL\ncan be used to tackle optimization problems in the con-\ntext of radiative heat transfer. To be precise, we illustrate\nthe core ideas with a problem related to the optimization\nof NFRHT between multilayer hyperbolic metamaterials.\nWe show how this type of problems can be framed in the\nlanguage of RL and how different RL algorithms can be\nimplemented to address them. In particular, we critically\nassess the advantages and disadvantages of the different\nmethods to help new users of RL to select the most con-\nvenient algorithm for a given application. The methods\npresented in this work can be straightforwardly applied\nto a large variety of problems in the thermal radiation\nscience and related fields.\nThe rest of the manuscript is organized as follows. In\nSec. II, we briefly introduce the topic of RL for non-\nexperts to make our contribution more self-contained.\nIn Sec. III, we present the system and problem that we\nhave chosen to illustrate the use of RL in the context\nof thermal radiation problems, namely the optimization\nof NFRHT between multilayer hyperbolic metamateri-\nals. Then, Sec. IV is devoted to the description of the\nmain results of this work obtained with different RL algo-\nrithms. We have organized those results according to the\nRL algorithm employed and we also provide a detailed\ndescription of such algorithms. Finally, we present some\nadditional discussions and summarize our main conclu-\nsions in Sec. V.\nII.\nREINFORCEMENT LEARNING: A BRIEF\nREMINDER\nIn this section we provide a brief introduction to RL\nfollowing Ref. [34]. This will allow us to set the language\nand make the manuscript more self-contained. Readers\nfamiliar with RL can safely skip this section.\nRL is a subfield of machine learning that aims at solv-\ning sequential decision-making problems.\nMany prob-\nlems can be formulated in this way, including those con-\ncerning the optimization of systems, devices, and pro-\ncesses in the physical sciences. To solve a problem within\nRL, we begin by defining a goal.\nThen, an algorithm\ntakes actions and gets information about the external\nworld on how well the goal is being achieved. To that\nend, we normally need to take many actions in a sequen-\ntial fashion, where each action modifies the world around\nus. We observe the changes in the world and, with the\nhelp of the feedback we receive, we decide on the next\naction to take.\nThe RL formulation of the process described above is\nthe following. RL problems are formulated as a system\nthat comprises an agent and an environment, the world\nsurrounding the agent.\nThe environment produces in-\nformation which allows us to describe the state of the\nAgent\nEnvironment\nstate st\nreward rt\nstate st+1\nreward rt+1\naction at\nFIG. 1. The reinforcement learning control loop diagram.\nsystem, while the agent interacts with the environment\nby observing the state and selecting an action. The en-\nvironment accepts the action and transitions into a new\nstate, which is then observed by the agent to select a\nnew action. In doing so, the environment also returns a\nreward to the agent, which is used by the agent to se-\nlect future actions. When the cycle of state →action\n→next state and reward is completed, we say that one\ntime step has passed. This cycle is repeated until the\nenvironment terminates, for example, when the problem\nis solved. This process is summarized in the control loop\ndiagram of Fig. 1.\nGoing deeper with the formulation of the process, a\npolicy in RL is the agent’s action-producing function\nwhich maps states to actions. As previously indicated,\nactions change the environment and affect what an agent\nobserves and does next, which can be viewed as a sequen-\ntial decision-making process which evolves in time. This\nRL process is driven by an objective, which is defined\nas the sum of rewards received from the environment.\nThe agent aims at maximizing the objective by selecting\ngood actions and learns to do this by interacting with the\nenvironment according to an optimizing policy in a trial-\nand-error process, which uses rewards to reinforce good\nactions and penalize bad ones. Therefore, the signals ex-\nchanged between agent and environment are (st, at, rt),\nwhich stand for state, action, and reward, respectively,\nand where t denotes the time step in which these sig-\nnals occurred.\nThe defined tuple (st, at, rt) is referred\nto as an experience, which is the basic unit of informa-\ntion describing a RL system. The control loop is then\nrepeated forever or terminated by reaching either a ter-\nminal state or a maximum time step t = T. The time\nframe from t = 0 to the terminal state (either a defined\nstate/s or a maximum state) is called an episode.\nIn\naddition, the sequence of experiences over an episode,\nτ = (s0, a0, r0), (s1, a1, r1), . . . is known as a trajectory.\nAn agent typically needs many episodes to learn a good\npolicy.\nIn a more formal way, we can describe states, actions\nand rewards as: (i) st ∈S where S is the state space,\n(ii) at ∈A where A is the action space, and (iii) rt =\nR(st, at, st+1) where R is the reward function.\nHere,\nthe state space S is the set of all possible states in an\nenvironment. It can be defined as integers, real numbers,\nvectors, etc. Similarly, the action space A is the set of\n3\nall possible actions. It is commonly defined as either a\nscalar or a vector. The reward function R(st, at, st+1),\nfor its part, assigns a real number (positive or negative)\nto each transition. The state space, action space, and\nreward function are determined by the environment.\nLet us now consider how an environment transitions\nfrom one state to the next using the transition function.\nIn RL, a transition function is formulated as a Markov de-\ncision process (MDP), which means in practice that one\nassumes that the transition to the next state st+1 only\ndepends on the previous state st and action at. This is\nknown as the Markov property and can be mathemati-\ncally formulated as\nst+1 ∼P(st+1|st, at),\n(1)\nwhich means that the next state st+1 is sampled from a\nprobability distribution P(st+1|st, at).\nWith this new ingredient, we can now compile all MDP\nelements: S, A, R(·), P(·), where we recall S is the set of\nstates, A is the set of actions, P(st+1|st, at) is the tran-\nsition function of the environment, and R(st, at, st+1)\nis the reward function.\nLet us remark that RL algo-\nrithms tackled in this work are model-free, this is, the\nagents have access to neither the transition function,\nP(st+1|st, at), nor the reward function, R(st, at, st+1).\nThe only way in which an agent gets information about\nthese functions is through the states, actions, and re-\nwards it actually experiences in the environment.\nAs previously indicated, to formulate a RL problem it\nis necessary to formalize the objective which the agent is\nintended to maximize. For this purpose, we first define\nthe return G(τ) using a trajectory from an episode,\nG(τ) = r0 + γr1 + γ2r2 + · · · + γT rT =\nT\nX\nt=0\nγtrt,\n(2)\ni.e., as a discounted sum of the rewards in a trajectory,\nwhere γ ∈[0, 1] is the discount factor.\nThe discount\nfactor is an important parameter which changes the way\nfuture rewards are considered. The smaller γ, the less\nweight is given to rewards in future time steps.\nOn the other hand, the objective J(τ) is simply defined\nas the expectation of the returns over many trajectories\nevaluated with a given policy π, i.e.,\nJ(π) = Eτ∼π [G(τ)] = Eτ\n\" T\nX\nt=0\nγtrt\n#\n.\n(3)\nThe expectation accounts for stochasticity in the actions\nand the environment.\nA key question in RL concerns what an agent should\nlearn. There are three basic properties that can be useful\nto an agent: (i) a policy, (ii) a value function, and (iii)\nan environment model.\nFirst, if we recall, the policy\nπ is that which maps states to actions, which can be\nformalized with the notation a ∼π(s). A policy can be\nstochastic and therefore, we can write this as π(a|s) to\ndenote the probability of an action a given a state s.\nThe value functions provide information about the ob-\njective. They help an agent to understand how good the\nstates and available actions are in terms of the expected\nfuture return, allowing to determine a policy from this\ninformation. There are two types of value functions de-\nfined as\nV π(s) = Est′=s,τ∼π\n\" T\nX\nt=t′\nγtrt\n#\n,\n(4)\nQπ(s, a) = Est′=s,at′=a,τ∼π\n\" T\nX\nt=t′\nγtrt\n#\n.\n(5)\nThe state-value function V π(s) in Eq. (4) evaluates the\nquality of a state. It measures the expected return from\nbeing in state s, assuming the agent continues to act\naccording to its current policy π. It is worth noting the\nreturn G(τ) = PT\nt=t′ γtrt is measured from the current\nstate to the end of an episode. The action-value function\nQπ(s, a) of Eq. (5) evaluates how good a state-action pair\nis. It measures the expected return from taking action\na in state s assuming that the agent continues to act\naccording to its current policy, π.\nFinally, an environment model is summarized in the\ntransition function P(st+1|st, at) that provides informa-\ntion about the environment. If an agent learns this func-\ntion, it is able to predict the next state st+1 that the\nenvironment will transition into after taking action a in\nstate s. Often, good models of the environment are not\navailable and in this work we shall not make use of this\ntype of function or the corresponding algorithms.\nIn RL, an agent learns a function of any of the previous\nproperties to decide what actions to take with the goal of\nmaximizing the objective. In most practical problems the\ndifferent spaces – state, action, etc. – are so large that the\nkey functions need to be approximated. Currently, the\nmost popular methods to approximate these functions\nare based on deep neural networks, which gives rise to\nthe concept of Deep Reinforcement Learning. This is the\nmethod of choice in this work.\nOn the other hand, according to the three primary\nlearnable functions in RL (see above), there are three\nmajor families of deep RL algorithms – policy-based,\nvalue-based, and model-based methods which learn poli-\ncies, value functions, and models, respectively. In Sec-\ntion IV, we shall present the main results of this work\norganized according to the corresponding RL algorithm\nemployed and we shall also include a brief description of\nthe main characteristics of every used algorithm.\nIII.\nOPTIMIZING NFRHT BETWEEN\nMULTILAYER HYPERBOLIC METAMATERIALS\nA.\nPhysical problem\nIn this Section we describe the specific problem that\nwe have selected to illustrate the use of RL in the context\n4\nof radiative heat transfer, namely the optimization of the\nnear-field radiative heat transfer between multilayer hy-\nperbolic metamaterials [30, 35].\nAs discussed in the introduction, a major breakthrough\nin recent years in the field of thermal radiation has been\nthe confirmation of the possibility to overcome Stefan-\nBoltzmann’s law for the radiative heat transfer between\ntwo bodies by bringing them sufficiently close [5]. This\nphysical phenomenon is due to the fact that in the\nnear-field regime, bodies can exchange radiative heat via\nevanescent waves. This type of contribution is not con-\nsidered in Stefan-Boltzmann’s law and dominates the\nNFRHT for sufficiently small separations [36–38]. Dif-\nferent strategies have been recently proposed to further\nenhance NFRHT. One of the most prominent ones makes\nuse of multiple surface modes that appear in multilayer\nstructures where dielectric and metallic layers are alter-\nnated to give rise to the so-called hyperbolic metama-\nterials [39–48].\nThe hybridization of surface modes in\ndifferent metal-dielectric interfaces can lead to a great\nenhancement of the NFRHT, as compared to the case of\ntwo infinite parallel plates [46].\nFollowing Ref. [46], here we consider the radiative heat\ntransfer between two identical multilayer structures sep-\narated by a gap d0, see Fig. 2(a). Each thermal reservoir\ncontains Nl total layers alternating between a metallic\nlayer with a permittivity ϵm and a lossless dielectric layer\nof permittivity ϵd. The thickness of the layer i is denoted\nby di. The dielectric layers are set to vacuum (ϵd = 1)\nand the metallic layers are described by a permittivity\ngiven by a Drude model: ϵm(ω) = ϵ∞−ω2\np/[ω(ω + iγ)],\nwhere ϵ∞is the permittivity at infinite frequency, ωp is\nthe plasma frequency, and γ is the damping rate. From\nnow on, we set ϵ∞= 1, ωp = 2.5 × 1014 rad/s, and\nγ = 1 × 1012 rad/s. With this choice of these parameters\nthe surface plasmon frequency is similar to the surface\nphonon-polariton frequency of the interface between SiC\nand vacuum.\nWe describe the NFRHT between the hyperbolic meta-\nmaterials within the theory of fluctuational electrody-\nnamics [49, 50].\nIn this system, the NFRHT is domi-\nnated by TM- or p-polarized evanescent waves and the\nheat transfer coefficient (HTC) between the two bodies,\ni.e., the linear radiative thermal conductance per unit of\narea, is given by [51]\nh = ∂\n∂T\nZ ∞\n0\ndω\n2π Θ(ω, T)\nZ ∞\nω/c\ndk\n2π k τp(ω, k),\n(6)\nwhere T is temperature, Θ(ω, T) = ℏω/(eℏω/kBT −1) is\nthe mean thermal energy of a mode of frequency ω, k is\nthe magnitude of the wave vector parallel to the surface\nplanes, and τp(ω, k) is the transmission (between 0 and\n1) of the p-polarized evanescent modes given by\nτp(ω, k) = 4 [Im {rp(ω, k)}]2 e−2q0d0\n|1 −rp(ω, k)2e−2q0d0|2 .\n(7)\nHere, rp(ω, k) is the Fresnel reflection coefficient of the\np-polarized evanescent waves from the vacuum to one of\n(b)\n(a)\n(c)\nFIG. 2. (a) Schematic representation of the physical system\nunder study. It features two identical hyperbolic metamateri-\nals comprising alternating metallic (grey) and dielectric (blue)\nlayers. Both reservoirs have infinitely-extended layers and are\nseparated by a distance d0 = 10 nm. Each layer has a thick-\nness of 5 nm and both subsystems are backed by a metallic\nsubstrate. (b) Transmission of evanescent waves as a function\nof the frequency (ω) and the parallel wavevector (k) for the\nperiodic structure of panel (a) composed by 16 active layers\nper subsystem. (c) The corresponding spectral heat transfer\ncoefficient hω at room temperature (T = 300 K) as a function\nof the frequency, baseline in legend. The result is compared\nto that of two metallic plates (bulk) with the same gap.\n5\nthe bodies and q0 =\np\nk2 −ω2/c2 (ω/c < k) is the wave\nvector component normal to the layers, in vacuum. The\nFresnel coefficient needs to be computed numerically and\nwe have done it by using the scattering matrix method\ndescribed in Ref. [52]. In our numerical calculations of\nthe HTC we also took into account the contribution of\ns-polarized modes, but it turns out to be negligible for\nthe gap sizes explored in this work.\nThe interest in the NFRHT in these multilayer struc-\ntures resides in the fact that the heat exchange in this\nregime is dominated by surfaces modes that can be tuned\nby playing with the layer thicknesses.\nIn the case of\ntwo parallel plates made of a Drude metal, the NFRHT\nis dominated by the two cavity surface modes resulting\nfrom the hybridization of the surface plasmon polaritons\n(SPPs) of the two metal-vacuum interfaces [46]. These\ntwo cavity modes give rise to two near-unity lines in the\ntransmission function τp(ω, k). If we introduce more in-\nternal layers, we can have additional NFRHT contribu-\ntions from surface states at multiple surfaces. This is il-\nlustrated in Fig. 2(a) for the case of N = 16 active layers\nwith di = 5 nm and a gap size d0 = 10 nm. Apart from\nthe active layers, both reservoirs contain an additional\n5 nm-thick metallic layer in the outer part to properly\ndefine the gap, as well as a semi-infinite metallic sub-\nstrate on the other side. This example, in which we have\nin practice a periodic structure with 8 physical layers (4\nmetallic and 4 dielectric layers) with thickness di = 10\nnm, exhibits multiple near-unity resonances in the trans-\nmission function τp(ω, k), see Fig. 2(b).\nThese contri-\nbutions resulting from additional surface states originat-\ning from internal layers lead to a great enhancement of\nthe NFRHT as compared to the bulk system (two par-\nallel metallic plates) in a wide range of gap values [46].\nThis is illustrated in Fig. 2(c) where we show the spec-\ntral HTC, hω, defined as the HTC per unit of frequency:\nh =\nR ∞\n0\nhω dω, for both the system pictured in Fig. 2(a)\n(labeled as baseline) and the bulk system with the same\ngap d0 = 10 nm.\nOur concrete goal is to maximize the HTC between\nthese two hyperbolic metasurfaces by finding the optimal\nconfiguration of alternating dielectric and metallic layers\n(number and thickness). We keep fixed the gap size to\nd0 = 10 nm, the total active thickness of the multilayer\nareas, and assume room temperature (T = 300 K). We\nalso assume the two multilayer systems to be identical\nsince any asymmetry tends to reduce the HTC.\nB.\nRL formulation of the optimization problem\nWe now describe how we tackle our optimization prob-\nlem in the spirit of RL, which requires to formulate it\nas a sequential decision-making problem. The physical\nsystem is composed of two identical layered structures\nwhich, unless we state otherwise, contain 16 active layers\nwith a thickness of 5 nm. The two subsystems are sepa-\nrated by a gap d0 = 10 nm, each having a semi-infinite\nmetallic substrate.\nIrrespective of the employed algorithm, we define the\ncentral RL concepts as follows:\n1. Goal: our goal is to maximize the HTC via the\nmodification of the layer configuration.\n2. State: each state describes a layer configuration.\nWe define the material by an integer label, 0 for\nthe dielectric and 1 for the metal. Thus, a state is\nrepresented by a vector of 0s and 1s with 16 com-\nponents, each representing one of the 5 nm-thick\nlayers.\n3. Action: the action space is an ensemble of two de-\ncisions made concurrently, namely which layer to\nstudy and what material to consider for it. This\nincludes the possibility for the configuration to re-\nmain unchanged.\n4. Reward: the reward is the HTC corresponding to\nthe next state in units of 105 W/m2K. Thus, for\nthis problem, R = R(st+1) only. For better per-\nformance, we consider as a baseline for the reward\nvalues the HTC of our physically intuitive “best\nguess” configuration, which corresponds to the pe-\nriodic photonic crystal shown in Fig. 2(a).\nAny\npositive reward implies that we have found a higher\nHTC value.\n5. Episode Termination:\nwe impose an episodic\nformulation by defining a fixed number of actions\ntaken in a trajectory before resetting to the ini-\ntial state, containing all 0s (all dielectric layers ex-\ncept the 5 nm-thick metallic layer defining the gap).\nThis enables the network to perform much more\ntraining on known states, and to finish the opti-\nmization in an acceptable number of steps. We take\nthe length of an episode to be twice or four times\nthe number of layers of an state, so any existing\nstate is potentially reached comfortably.\nIV.\nRESULTS\nIn this section we describe the main results obtained for\nthe optimization of the NFRHT between the multilayer\nhyperbolic structures described in the previous section.\nFor didactic reasons, we present separately the results\nobtained with the different RL algorithms and in every\nsubsection we describe the basics of the corresponding\nmethod alongside with a discussion of the peculiarities\nconcerning their application to our problem.\nA.\nValue-based algorithms: SARSA, deep\nQ-learning and extensions\nAs value-based algorithms are historically the most\nwidely used and discussed in RL [33], we shall address\n6\nfirst their formulation and usage. Value-based algorithms\nare based on two core ideas.\nThe first one is tempo-\nral difference (TD) learning, which is an alternative to\nthe use of Monte Carlo sampling for gathering experi-\nences from an environment (see Sec. IV B) to estimate\nstate/state-action values. The key idea in TD learning\nis that state/state-action values are defined recursively,\nthat is, their value in a given time step is defined in\nterms of the value in the next time step.\nThis makes\nTD learning an useful method for backing up the re-\nward information from later to earlier steps through time.\nAs state/action-value functions represent an expectation\nover different trajectories, this leads to the display of a\nlower variance than Monte Carlo sampling.\nThe second idea has to do with the famous exploration-\nexploitation trade-off in RL. When the agent is learning\nan estimate of the state/state-action values, the usage of\nthis estimate can lead to better returns (exploitation).\nHowever, if one always selects actions based on current\nvalues, which might be far from the optimal ones, this\nwould lead to a deterministic behavior that can prevent\nthe agent from discovering better unknown actions (ex-\nploration).\nThis exploration-exploitation trade-off is a\nkey challenge in RL and it can be addressed employ-\ning stochastic policies, where the exploration can be dis-\ntributed along all the training and, as the estimation gets\nbetter, it gradually shifts closer to a deterministic policy.\nAn example of an stochastic policy, which will be used in\nthis work, is the ε-greedy policy, where the agent explores\nwith a probability of ε and exploits with a probability of\n1 −ε.\n1.\nSARSA\nSARSA is one of the oldest RL value-based algorithms\nand, despite its limitations (see below), it is convenient\nto start by describing its use for our problem. This al-\ngorithm is based on the estimation of the action-value\nfunction or Q-function. It employs TD learning to pro-\nduce the target state-action values or Q-values, from now\non denoted as Qtar. Therefore, it combines the reward\ngiven by the environment, rt, with the Q-value estimates\nof the next state which approximate the remaining part\nof the expected return. This is summarized in the fol-\nlowing update rule:\nQπ\ntar(st, at) = rt + γQπ(st+1, at+1).\n(8)\nNotice that the Q-value estimate depends on the follow-\ning action, as we base our estimates solely on state-action\nestimates. Over the course of many examples, the pro-\nportion of selected actions given an state will approxi-\nmate the probability distribution over all actions.\nIn practice, we employ a neural network for the ap-\nproximation of the Q-function, the Q-network, which re-\nturns the Q-value estimates of the selected state-action\npairs.\nAs a consequence, each update of the Q-value\nis not complete, as neural networks learn gradually us-\ning gradient descent, moving partially towards the target\nvalue. With all that, SARSA algorithm is summarized\nin pseudocode 1.\nThe workflow of SARSA is similar to a supervised\nlearning workflow, in which each estimate has a target\nvalue to reach and, with it, we can evaluate how well\nour neural network is performing and thus reduce the\ndiscrepancies between the values. In this sense, we use\nan iterative approach to improve the Q-value, as we can\nexplicitly see in line 3. Notice from lines 5 and 7 that,\nas SARSA is based on TD learning, only the information\nfrom the next step is required to form the target of the\ncurrent Q-value, allowing to update the Q-function in a\nbatched manner. Regarding sample efficiency, we can see\nfrom line 4 that the next action at+1 is obtained with the\nsame policy used to gather the previous action at, this\nis, ε-greedy policy over the only Q-network of the algo-\nrithm. This specific selection of the next action makes\nSARSA an on-policy algorithm, that is, an algorithm in\nwhich the information for improving the policy (at+1)\ndepends on the policy to gather data (at). Because this\non-policy behavior, each training iteration can only use\nexperiences obtained following the current policy, so each\ntime the Q-network parameters are updated, all experi-\nences must be discarded and new experiences have to be\ncollected, as reflected again with line 4 and its position\nwithin the training loop. Finally, something that arises\nfrom the use of currently collected experiences for the\nestimation of the target Q-value is the high correlation\nbetween experiences, as the data used to update the net-\nwork is often from a single episode, which can lead to\nhigh variance in different parameter updates.\nIn Fig. 3 we present a summary of the results obtained\nwith SARSA for our hyperbolic multilayer system with 16\nlayers, which includes 40 independent runs, represented\nwith their mean and standard deviation. These results\nAlgorithm 1: SARSA pseudocode [33, 34].\nInput: a differentiable action-value function\nparametrization bq : S × A × Rd →R\nInput: algorithm parameters. Initialize learning rate\nα > 0, epsilon 1 ≥ε > 0, discount rate\n1 ≥γ ≥0.\nOutput: optimized bq\n1 Initialize arbitrarily bq weights, ⃗w ∈Rd\n2 Initialize s0 ̸= terminal\n3 for training step do\n4\nGenerate a new batch of episodic experiences st,\nat, rt, st+1, at+1 with ε-greedy policy wrt. bq(st, ·)\n5\nfor experience in batch do\n6\nStore estimation(⃗w): bq(st, at)\n7\nStore target: rt + γbq(st+1, at+1)\n8\n⃗w ←⃗w + α∇⃗wloss(estimation(⃗w), target)\n9\nDecay ε\n10\nDecay learning rate\n7\nFIG. 3. Training of SARSA algorithm for our physical prob-\nlem of interest. (a) Largest HTC discovered as a function of\nthe number of found states in the problem with 16 layers ob-\ntained with SARSA algorithm. We also present the results\nobtained with the random algorithm. (b) The evolution of\nthe corresponding loss curve of SARSA algorithm. (c) Return\nobtained in a simulation of an episode with the Q-network of\nSARSA algorithm at each training step. The dashed line cor-\nresponds to the value of ε (right scale). In all panels the solid\nlines correspond to the mean value and the shaded areas to\nthe standard deviations, as obtained in 40 independent runs\nfor SARSA algorithm.\nwere obtained with the hyperparameters specified in Ta-\nble I of Appendix A. Figure 3(a) shows the largest HTC\nobtained as a function of the number of found states. To\ngauge the quality of our method, we compare SARSA\nresults in this panel with an algorithm in which different\nstates are randomly selected and the maximum HTC is\nrecorded as the algorithm progresses. This random al-\ngorithm is particularly efficient for relatively small state\nspaces, as it is forced to always find new states. There-\nfore, its results constitute a good test for the different RL\nalgorithms. To ensure reliable statistics, its mean value\nand deviation were obtained with 1000 independent runs\nin all cases.\nIn Fig. 3(b) we present the evolution of the loss func-\ntion of the Q-network along the training proceeds. Fi-\nnally, Fig. 3(c) displays the return in a greedy simulation\nof an episode with the Q-function obtained each training\nstep. Recalling the basis of RL (see Sec. II), the usual\ngoal of RL is to maximize the expected return G(τ). In\nthis sense, we can see in Fig. 3(c) that the return in-\ncreases along the exploratory phase, so SARSA algorithm\nis achieving better policies as the training proceeds, as ex-\npected. Regarding the loss function, Fig. 3(b) shows that\nits general tendency is indeed of decrease, indicating that\nthe Q-network tends to converge, as desired. However,\nsome noise during the training can be noticed. When ε is\nclose to 0.5 the average loss starts increasing, and when\nε reaches its minimum value, the loss rapidly adopts its\nlowest value. Apart from the variance arising just from\nSARSA formulation due to the high correlation between\nthe experiences of each batch, the noise seems to arise\nfrom the greedy behavior of the method in intersection\nwith the shape of the state-action space.\nIn our case,\nthe noise could indicate the Q-value of the state-actions\nvisited through the greedy policy is not that close to the\nvalue of the previously seen state-action pairs. With that,\nan overfitting of the Q-network is thought to be made to\nthe state-actions of the greedy behavior, which increases\nthe mean loss when those states are not that frequently\nvisited and gradually decreases the loss as the greedy be-\nhavior is more prominent.\nOn the other hand, Fig. 3(a) shows that SARSA is ca-\npable of finding the optimal configuration for our prob-\nlem, but it performs similarly to the random algorithm\nwith the exception of the end of the curve, where SARSA\nruns exhibit a smaller variance. Thus, the main conclu-\nsion from this analysis is that although SARSA can learn\nan improved policy, it is not sample efficient enough to\nachieve our objective for the selected environment. It is\nimportant to emphasize that, as in any machine learning\nproblem, we cannot rule out that with a better selec-\ntion of hyperparameters SARSA could clearly beat the\nrandom algorithm, especially in problems of higher di-\nmensionality.\nIn any case, as we shall see in the next\nsubsection, sample efficiency can be notably improved\nusing other types of value-based algorithms, so we shall\nnot dwell too much here with SARSA algorithm.\n2.\nQ-Learning\nAs in SARSA algorithm, Q-learning is based on TD\nlearning in order to obtain the target value. In this case,\n8\nthe update rule for the Q-values reads\nQπ\ntar(s, a) = rt + γ max\nat+1 Qπ(st+1, at+1).\n(9)\nNotice that the selection of the following action at+1 is\nmade with a max operator, which indicates that we are\ntaking the action that maximizes the Q-value of the next\nstate. This might be seen as a small change with respect\nSARSA, but it has important consequences, overcoming\nsome of SARSA limitations. With it, in Eq. (9) we are\nlearning the optimal Q-function instead of the Q-function\nof the current policy as in SARSA, improving so the sta-\nbility and speed of learning. In addition, this makes Q-\nlearning an off-policy algorithm, as the information used\nto learn the Q-value is independent of the policy used\nfor gathering data. Therefore, off-policy behavior allows\nus to learn from experiences gathered by any policy. It\nallows to reuse and decorrelate experiences, reducing the\nvariance of each update and improving the sample effi-\nciency with respect to SARSA.\nIt is interesting to underline that, for our application,\nthe reuse of experiences is of great importance as our\ngoal is to obtain the maximum HTC with as few ex-\nplored states as possible. In addition, the usage of neural\nnetworks makes this aspect even more relevant as they\nrely on gradient descent, for which each parameter up-\ndate must be small because the gradient only returns\nmeaningful information near employed parameters. This\nmakes the possibility of reusing experiences important as\nthe network’s parameters may need to be updated mul-\ntiple times.\nWe summarize the Q-learning algorithm in pseu-\ndocode 2.\nComparing lines 4 and 7, one sees that Q-\nlearning gathers experiences selecting the action at with\nan ε-greedy behavior, while it estimates the target Q-\nvalue with a greedy selection of at+1, as previously seen\nAlgorithm 2: Q-Learning pseudocode [33, 34].\nInput: a differentiable action-value function\nparametrization bq : S × A × Rd →R\nInput: algorithm parameters. Initialize learning rate\nα > 0, epsilon 1 ≥ε > 0, discount rate\n1 ≥γ ≥0.\nOutput: optimized bq\n1 Initialize arbitrarily bq weights, ⃗w ∈Rd\n2 Initialize s0 ̸= terminal\n3 for training step do\n4\nGenerate a batch of episodic experiences st, at, rt,\nst+1 with ε −greedy behavior policy wrt. bq(st, ·)\n5\nfor experience in batch do\n6\nStore estimation(⃗w): bq(st, at)\n7\nChoose at+1 with greedy target policy wrt.\nbq(st+1, ·)\n8\nStore target: rt + γbq(st+1, at+1)\n9\n⃗w ←⃗w + α∇⃗wloss(estimation(⃗w), target)\n10\nDecay ε\n11\nDecay learning rate\nin update rule of Eq. (9). Although those lines are the\nonly difference with SARSA, they enable to further im-\nplement all previously mentioned advantages like experi-\nence reuse with the extensions described below. We shall\npresent in this work only the Q-learning results obtained\nwith the most refined version considered of Q-learning\nalgorithm. Therefore, we shall postpone the discussion\nof the Q-learning results and introduce in what follows\nseveral sophistications for the naive version of this algo-\nrithm.\n3.\nQ-Learning extensions\nAs mentioned above, Q-learning is potentially better\nthan SARSA to achieve our goal because of its off-policy\nnature [33]. Building upon this nature, we shall make use\nhere of several modifications of Q-learning that have been\nproposed to enhance its sample efficiency and stability:\nExperience replay.- Introduced by Lin [53], this idea\nconsists of storing experiences in a memory in order to\nreuse them even if they were taken with old policies, al-\nlowing for more efficient learning from a reduced number\nof gathered experiences. In practice, a experience replay\nmemory stores the agent’s most recent experiences up\nto a given memory size, large enough to contain many\nepisodes, replacing the oldest experiences by the newest\nones once this size is reached. With that, every time an\nagent needs batches to be trained, it retrieves them from\nreplay memory in a random-uniformly manner. Then,\neach one of the batches is used to update the training net-\nwork. Like this, in addition to introduce higher sample\nefficiency, we ensure we have decorrelated experiences for\ntraining as they are likely to be from different policies and\nepisodes, contrary to SARSA, stabilizing the training as\nwe reduce the variance of parameter updates. Finally, to\nset a widely used notation, the combination of Q-learning\nwith the usage of Q-networks and the presented memory\nreplay technique receives the name of Deep Q-Networks\n(DQN), set by Mnih et al [54]. We use this notation along\nthe rest of the paper.\nTarget network.- Introduced by Mnih et al. [55], it fo-\ncuses on reducing the changes in the target value by\nmeans of a second network, called the target network.\nThe idea is to use a lagged copy of the training network\nwhich update frequency is less than that of the train-\ning network.\nThen, this secondary network is used to\ngenerate the state-action estimate for the target value,\nmaxat+1 Qπ(st+1, at+1), stopping its value from moving.\nThis idea addresses the issue where the target is con-\nstantly changing because of network updates, stabiliz-\ning the training and making divergences less likely as it\navoids ambiguity regarding the values the network must\napproach.\nDouble Q-learning.- Introduced by van Hasselt et\nal. [56, 57], the basic idea is using two different networks\ntrained with different experiences for the estimation of\nthe next Q-value used for obtaining the target value.\n9\nThis double estimation is computed using a network for\nretrieving the maximizing action, and the remaining net-\nwork for producing the Q-value with the selected action,\nas follows\nQπ\ntar(s, a) = rt + γQπ\n2(st+1, arg max\nat+1\nQπ\n1(st+1, at+1)).\n(10)\nIt mitigates the systematic overestimation of the state-\naction values by the deep Q-learning algorithm. This ef-\nfect arises from the use of an approximated algorithm. As\nit does not return a perfect estimation, if Q estimations\ncontain any errors, maximum state-actions are likely to\nbe positively biased, resulting in an overestimation of the\nQ-values as Hasselt et al. showed in their paper [56].\nWith that, if we introduce the usage of a second net-\nwork trained with different experiences, we can remove\nthe positive bias in the estimation. As with the introduc-\ntion of the target network we already have a second net-\nwork and we want to avoid sampling more experiences, a\ncommon practice is the usage of it as the secondary net-\nwork. Although it is just a lagged copy of the training\nnetwork, if the update frequency of the target network\nis sufficiently low, it is considered to be different enough\nfrom the training network to function as a different one.\nTaking into consideration all these modifications, the\nfinal Double DQN algorithm we use for our application is\nsummarized in the pseudocode 3. Let us emphasize the\nmain differences with respect to the Q-learning algorithm\nin pseudocode 2.\nLines 4, 6 and 8 describe the usage\nof memory replay, first initializing it to gradually add\nmore experiences at each training step. For training, B\nbatches of experiences are sampled, further leveraging\nexperiences reuse compared to not using several batches.\nNext, lines 2, 12 and 15 refer to the usage of a target\nnetwork, which is updated to the weights of the training\nnetwork with a frequency F. Finally, double estimation\nis reflected in lines 11 and 12, where at+1 is taken with\nthe training network bq, while the next state-action value\nis obtained through the target network ˜q.\nMaking use of this final Double DQN algorithm and\nthe hyperparameters listed in Table II, we obtained the\nresults summarized in Fig. 4 for our multilayer system,\nagain with 40 independent runs for Double DQN algo-\nrithm. In Fig. 4(a) we display the maximum HTC for\nDouble DQN and the random algorithm as a function of\nthe number of found states. The Double DQN algorithm\nsurpasses the random algorithm for a relatively small\namount of explored states, ∼7000 versus the ∼65000\npossible number of states of the system. To emphasize\nthe quality of these results, let us say that for the 40 runs\nof Double DQN algorithm, the top 5 best HTC values for\nthis system are found in 77.5% of the runs and the best\npossible state in 35.0% of the runs, while for the 1000\nruns of the random algorithm these values are found in\n44.3% and 12.9% of the runs, respectively.\nTo gain some insight into the training process of our\nalgorithm, we display in Fig. 4(b) the evolution of the\nloss of the Q-network. This loss decreases almost mono-\nAlgorithm 3: Double Q-learning with memory\nreplay and target network pseudocode [33, 34].\nInput: differentiable action-value function\nparametrizations bq, ˜q : S × A × Rd →R\nInput: algorithm parameters. Initialize learning rate\nα > 0, epsilon 1 ≥ε > 0, discount rate\n1 ≥γ ≥0, new experiences per episode h > 0,\nbatches per training step B > 0, target\nnetwork update frequency F > 0.\nOutput: optimized bq\n1 Initialize arbitrarily bq weights, ⃗w ∈Rd\n2 Equal target network ˜q weights to bq weights,\n⃗φ ∈Rd = ⃗w ∈Rd\n3 Initialize s0 ̸= terminal\n4 Initialize memory replay with a batch of episodic\nexperiences st, at, rt, st+1 with ε-greedy behavior\npolicy wrt. bq(st, ·)\n5 for training step do\n6\nStore in memory h episodic experiences st, at, rt,\nst+1 with ε −greedy behavior policy wrt. bq(st, ·)\n7\nfor batch B do\n8\nSample a batch of experiences from memory\n9\nfor experience in batch do\n10\nStore estimation(⃗w): bq(st, at)\n11\nChoose at+1 with greedy target policy wrt.\nbq(st+1, ·)\n12\nStore target: rt + γ˜q(st+1, at+1)\n13\n⃗w ←⃗w + α∇⃗wloss(estimation(⃗w), target)\n14\nif training step ∝F frequency then\n15\nUpdate target network ˜q weights, ⃗φ = ⃗w\n16\nDecay ε\n17\nDecay learning rate\ntonically, which indicates that our algorithm is training.\nHowever, some irregularities can be appreciated again.\nFirst, we can see some regular peaks. These just corre-\nspond to the update of the target network, which pro-\nduces the sudden change of the target. In addition, some\nnoise appears at the end of the curve, which we believe it\ncould arise from two effects. First, from the same effect\nas in SARSA’s loss, i.e., the overfitting of the Q-network.\nSecond, it could be due to the target network is not be-\ning that frequently updated, leading to a target with less\ninformation.\nTherefore, we can end again with a less\ngeneric Q-network, which can lead to higher values of\nthe loss.\nFinally, as previously discussed, an important metric\nfor the performance of the algorithm is the return in\na greedy simulation with the Q-function obtained each\ntrain step, see Fig. 4(c). Notice that the return first in-\ncreases but ends up decreasing, which could appear to be\nupsetting. Let us recall that the return is the main result\nfor RL applications with the standard algorithm’s objec-\ntive: to obtain a policy that maximizes the return of the\nsystem. However, our final objective here is slightly dif-\nferent: to explore the optimal state with as few explored\nstates as possible. This is why the observed decay in the\n10\nFIG. 4. Training of the Double DQN algorithm. (a) Largest\nHTC discovered as a function of the number of found states\nin the problem with 16 layers obtained with the Double DQN\nalgorithm. We also present the results obtained with the ran-\ndom algorithm. (b) Evolution of the corresponding loss curve\nof the Double DQN algorithm. (c) Return obtained in a sim-\nulation of an episode with the Q-network of Double DQN\nalgorithm at each training step. The dashed line corresponds\nto the value of ε (right scale). The vertical line and the green\nshaded area correspond to the training steps at which the\nhighest HTC of the runs are found. In all panels the solid\nlines correspond to the mean value and the shaded areas to\nthe standard deviations, as obtained in 40 independent runs.\nIn all cases, 4 experiences were stored per training step.\nreturn is not worrisome in our case, although it means\nthat we end up having a non-optimal policy.\nIn Fig. 4(c) we also show as a vertical green line with\na shaded area both the mean and standard deviation of\nthe training step at which the highest HTC is discovered\nin the different runs. Two things are worth remarking:\nin this region ε still has a sizeable value, so the algorithm\nFIG. 5. Same as in Fig. 4 but with 25 experiences stored per\ntraining step.\nstill has chances of discovering better states, and the re-\nturn is still growing and is higher than the return of a\ncompletely exploratory policy, so we have a policy with\nsome learning. This supports the fact that the decrease of\nthe return is something not to worry about: with selected\nhyperparameters, our algorithm uses the policy learned\nat early stages to explore better states.\nAlthough the decay of the return is not something crit-\nical in our case, it is important to understand why it\noccurs. A possible explanation is the no convergence to\nthe optimal Q-function. As mentioned in Sec. III B, our\nreward is the difference of the HTC of the periodic mul-\ntilayer system and that of the next state. In Fig. 4(c), we\ncan see that the return does not surpass the zero value,\nso we are not close to the optimal policy and, therefore,\nto the optimal Q-function. The reason why a below zero\nvalue is not close to a simulation of an episode of the op-\n11\ntimal policy is the following. Knowing the optimal state\n(see Sec. V) and given a large episode length of 64 steps,\nan intuitively good policy would imply transitioning with\nas few number of steps as possible from the start state\nto the optimal one. It would result into transitioning to\nthe optimal state in just 7 steps.\nTherefore, this pol-\nicy would output 7 unknown rewards and a reward of\n0.28 × 105 W/m2K during 57 steps. In addition to the\nbest reward, we know the worst possible one, which has\na value of −1.37 × 105 W/m2K. With it, the 7 unknown\nrewards must be equal or higher to the worst possible\nreward.\nTherefore, the good policy we have imagined\nhas a return G(τ) ≥6.37 × 105 W/m2K, which has a\npositive value. By definition, the optimal policy is such\nthat its expected return is greater or equal than any of\nthe remaining existent policies for all states [33]. With\nthis example, we have found a policy whose expected re-\nturn from the initial state is higher than zero, which is\nover the return displayed at Fig. 4(c). Thus, this demon-\nstrates that we have not reached the optimal policy in\nthat figure.\nAs we have not reached the optimal policy and there-\nfore the true value of the Q-function, we only have an\nimperfect estimation of it. This estimation can help us\nreach the optimal policy during training. However, if we\nexploit it instead of using it to continue looking for the\noptimal policy, we can end up overtraining our network\nwith experiences that a good policy is not likely to visit.\nThis puts our policy farther from the optimal Q-function,\nlosing the part of the estimation that was towards the\ngood policy and, finally, turning the policy into a worst\none because we are following non-optimal state-actions.\nAn interesting question at this stage concerns the is-\nsue of the impact of having a better estimation of the\nQ-values with the usage of more experiences and exploit\nits knowledge to obtain the optimal state. To elucidate\nthis issue, we present in Fig. 5 the results obtained for the\nDouble DQN algorithm using now 25 experiences stored\nin the memory replay per training step, rather than 4\nas presented in Fig. 4.\nIn Fig. 5(b), the loss function\nof the neural network still decays as expected with al-\nready known irregularities, while in Fig. 5(c), the return\nincreases up to a maximum value and stays there, as de-\nsired in regular applications of RL. This can mean that a\nsufficiently good policy is reached, so following it does not\nput us farther from the state-actions an optimal or sub-\noptimal policy would follow. However, something must\nbe noticed from Fig. 5(a), namely higher HTC values\nwith respect the random algorithm are now discovered\nwhen more states have been explored, distancing us from\nour true objective: obtaining the state which gives us the\nmaximum HTC with as few explored states as possible.\nThis leads to the conclusion that, although we can miss\nthe opportunity of learning a decent estimation of rele-\nvant state-action values, it is worth gathering states more\nslowly during the training as, just employing them, we\ncan discover the optimal states too. A similar behavior\nwill be also seen with policy-based algorithms.\nFinally, Fig. 5(c) shows that the highest HTC values\nare discovered early during the training.\nAgain, this\nmeans that there is no need to reach a good policy in\norder to find the good states of our application, which\nsuggests to train the Q-network with fewer states during\nmore training time.\nB.\nPolicy-based algorithms: REINFORCE\nNow we focus on the analysis of the results obtained\nwith REINFORCE [58], which is the most widely used\npolicy-based RL algorithm. In this type of algorithms,\nthe agent learns a policy function π, which in turn is\nused to produce actions and generate trajectories τ that\nmaximize the objective J(τ). REINFORCE needs three\ncomponents: (i) a parametrized policy, (ii) an objective\nto be maximized, like any other RL algorithm, and (iii) a\nmethod for updating the policy parameters. Concerning\nthe parametrized policy, this is obtained with the help\nof deep neural networks that learn a set of parameters θ.\nWe denote the policy network as πθ to emphasize that\nis parametrized by θ. Framed in this way, the process\nof learning a good policy is equivalent to searching for a\ngood set of values for θ.\nThe objective that is maximized by an agent in REIN-\nFORCE is the expected return over all complete trajec-\ntories generated by an agent:\nJ(πθ) = Eτ∼πθ [G(τ)] = Eτ∼πθ\n\" T\nX\nt=0\nγtrt\n#\n.\n(11)\nNotice that the expectation is calculated over many tra-\njectories sampled from a policy, that is, τ ∼πθ. This\nexpectation approaches the true value as more samples\nare collected, and it is specific of the policy πθ used.\nThe final component of the algorithm is the policy gra-\ndient, which formally solves the following problem:\nmax\nθ\nJ(πθ) = Eτ∼πθ [G(τ)] .\n(12)\nTo maximize the objective, we perform gradient ascent\non the policy parameters θ:\nθ ←θ + α∇θJ(πθ),\n(13)\nwhere α is the learning rate, which controls the size of the\nparameter update. The term ∇θJ(πθ) is known as the\npolicy gradient and thanks to the policy gradient theorem\n[33, 34] can be expressed as\n∇θJ(πθ) = Eτ∼πθ\n\" T\nX\nt=0\nGt(τ)∇θ log πθ(at|st)\n#\n.\n(14)\nHere, the term πθ(at|st) is the probability of the action\ntaken by the agent at time step t. The action is sampled\nfrom the policy, at ∼πθ(st).\n12\nAlgorithm 4: REINFORCE pseudocode\n[34, 58].\nInput: a differentiable policy parametrization\nπθ : S × Rd →A\nInput: algorithm parameters. Initialize learning rate\nα > 0 and discount rate 1 ≥γ ≥0\nOutput: optimized policy πθ\n1 Initialize arbitrarily πθ weights, ⃗θ ∈Rd\n2 for episode do\n3\nInitialize s0 ̸= terminal\n4\nInitialize ∇θJ(πθ) ←0\n5\nfor episode step do\n6\nTake at following πθ(at|st), observe rt, st+1\n7\nst ←st+1\n8\nfor t = 0...T do\n9\nGt(τ) ←PT\nt′=t γt′−trt′\n10\n∇θJ(πθ) ←∇θJ(πθ) + Gt(τ)∇θ log πθ(at|st)\n11\n⃗θ ←⃗θ + α∇θJ(πθ)\nIn practice, the REINFORCE algorithm numerically\nestimates the policy gradient using Monte Carlo sam-\npling. Instead of sampling many trajectories per policy,\none samples just one:\n∇θJ(πθ) ≈\nT\nX\nt=0\nGt(τ)∇θ log πθ(at|st).\n(15)\nThe REINFORCE algorithm is summarized in pseu-\ndocode 4.\nLet us emphasize how this is an episodic,\non-policy algorithm:\nevery episode we start from the\nstarting state s0 in line 3 and collect a full trajectory\nτ = (s0, a0, r0), . . . , (sT , aT , rT ) for an episode in lines 6\nand 7. Then, in line 9 the return Gt(τ) is computed for\neach time step t in the current trajectory, which is later\nused in line 10 to estimate the policy gradient along with\nthis policy’s action probabilities πθ(at|st). In line 11, the\nsum of the policy gradients for all time steps calculated in\nline 10 is used to update the policy network parameters\nθ, and the experiences are discarded.\nIt is known that the fact that the policy gradient is es-\ntimated sampling with a single trajectory typically leads\nto a high variance. One way to reduce this variance is\nto modify the returns by subtracting a suitable action-\nindependent baseline as follows\n∇θJ(πθ) ≈\nT\nX\nt=0\n[Gt(τ) −b(st)] ∇θ log πθ(at|st).\n(16)\nIn our case we chose as baseline the mean return over\nthe trajectory, namely, b = 1\nT\nPT\nt=0 Gt(τ). Our baseline\nis state-independent, being a constant value for each tra-\njectory, and centers the returns around 0. This enables\nfaster learning, correcting any imbalance between posi-\ntive and negative returns. Note that this REINFORCE-\nspecific baseline is independent of the baseline considered\nfor the HTC values in the rewards.\nLet us now describe our application of REINFORCE\nto our optimization problem. We first consider hyperpa-\nrameters given in Table III. The corresponding evolution\nof the training for 105 full episodes is shown in Fig. 6(a),\nwhere we present the return of each episode as training\nprogresses. These results corresponds to 40 independent\nruns. Notice that the return increases until it reaches a\nplateau, when the policy algorithm converges. The ver-\ntical line and the green shaded area correspond to the\nmean value and standard deviation of the training step\nat which the highest HTC of the run is found. We can\nsee that it was found in a much smaller number of steps\ncompared to the total number of training steps needed\nfor the convergence of the policy. This is a very simi-\nlar result to what we found using DQN. In Fig. 6(b) we\nshow the comparison between the best HTC found by\nREINFORCE and by a random search, also for 40 dif-\nferent runs for REINFORCE algorithm and 1000 runs\nfor the random algorithm. The random search algorithm\nis the same as in previous algorithms, so as to have a\nfair comparison. We can see how REINFORCE seems to\nbehave in a very similar fashion as the random search,\nbeing unable to significantly outperform it.\nTo improve these results, we explored the same idea\nas in Double DQN, namely to increase the learning rate\nso it can learn with a reduced number of explored states.\nIn Fig. 6(c) we show the same as in Fig. 6(a), but with a\nvalue of α = 3×10−4, 10 times larger than before. As one\ncan see, the training of the policy fails, with the return\ninitially increasing but eventually decreasing abruptly,\nwith a much larger standard deviation than before. In\na regular RL problem, we would say that training has\nfailed, and the learning rate is too high. However, if we\nlook at Fig. 6(d), same as in Fig. 6(b) but with the new\nvalue for the learning rate, REINFORCE shows the same\neffect as in Double DQN: it is able to slightly outperform\nthe random search algorithm in both the mean and the\narea covered by the standard deviation. Moreover, the\ngreen shaded area in Fig. 6(c) clearly demonstrates how\nthe best examples are found with a really small number\nof training steps, even lower than in Fig. 6(a), and the\ndecay of the return is inconsequential to our goal. This\nis also analogous to our results in Double DQN.\nWe can further identify this improvement through the\nstatistics of the runs, both from REINFORCE and the\nrandom search algorithm. Considering the 104 new ex-\namples found in Fig. 6(b), the random search algorithm is\nable to find one of the 5 configurations with highest HTC\n57.0% of the runs, with 17.3% finding the actual maxi-\nmum. On the other hand, REINFORCE is able to find\none of the 5 highest HTC configurations 60% of the runs,\nwhich is only slightly better than the random search, with\n12.5% finding the actual maximum, less than the random\nsearch. With this, we would conclude that the algorithm\nis worse than random search. But using the improved\nversion in Fig. 6(d), with a new restriction of 5000 new\nexamples, which is half the amount of new states com-\npared to Fig 6(b), the random search can only find one\n13\n \n \n(a)\nα=3×10\n−5\n(b)\n(c)\nα=3×10\n−4\n(d)\nFIG. 6. Training of a REINFORCE algorithm. (a) Evolution of the return for the trajectories as training progresses. The\nvertical line and the green shaded area correspond to the training steps at which the highest HTC of the runs are found.\nLearning rate chosen as α = 3×10−5. (b) Largest HTC discovered as a function of the number of found states for the evolution\nin (a). REINFORCE in blue versus a random search in orange. (c,d) Same as in (a,b), but with a learning rate α = 3 × 10−4.\nIn all panels the solid lines correspond to the mean value and the shaded areas to the standard deviations, as obtained in 40\nindependent runs for REINFORCE algorithm.\nof the 5 configurations with highest HTC 34.9% of the\nruns, with 9.3% finding the actual maximum, and REIN-\nFORCE can find one of the 5 configurations with highest\nHTC 42.5% of the runs, with 15% finding the actual max-\nimum, being able to surpass the random search results.\nHowever, the difference is not significant enough, and as\nsuch, REINFORCE is not the best option for solving our\nRL problem. The Monte Carlo sampling and the sam-\nple inefficiency of the algorithm prevent it from obtain-\ning comparable or even better results than other algo-\nrithms. Still, its convergence is more easily guaranteed,\nalbeit noisy, and its implementation is more straightfor-\nward than value-based algorithms, so it can still be a\ncandidate for other possible applications.\nC.\nCombined methods: Actor-Critic and PPO\nSo far, we have discussed both policy-based and value-\nbased algorithms. Now, we shall consider combined meth-\nods which learn two or more of the primary RL func-\ntions.\nTo be precise, we shall discuss both Advantage\nActor-Critic (A2C) [59] and Proximal Policy Optimiza-\ntion (PPO) [60], which are among the most widely-used\nRL algorithms for a wide variety of applications. Actor-\nCritic algorithms receive their name from the two ele-\nments that compose them: an actor, which learns a pa-\nrameterized policy like in REINFORCE; and a critic,\nwhich learns a value function to evaluate state-action\npairs, becoming a learned reinforcement signal. In sim-\nple words, the foundations of actor-critic algorithms in-\nvolve trying to learn a value function to give the policy\na more informative metric than just the rewards. When\nthe learned reinforcement signal is the advantage func-\ntion, the algorithm is called Advantage Actor-Critic. The\nadvantage function is defined as follows:\nAπ(st, at) = Qπ(st, at) −V π(st),\n(17)\nthus describing how preferable an action would be com-\npared to the average weighted by the policy in a par-\n14\nticular state.\nThis allows to rescale their values for\nall states and actions, similarly to a state-dependent\nbaseline, and presents other useful properties such as\nEa∈A[Aπ(st, a)] = 0. The actor uses this signal in place\nof the estimate of the return from the REINFORCE al-\ngorithm, performing the same gradient ascent technique.\nThat is, the actor performs the following policy optimiza-\ntion (setting πθ →π):\n∇θJ(π) = Et[Aπ\nt ∇θ log π(at|st)].\n(18)\nThe critic is tasked with estimating this advantage func-\ntion for all states and actions. In principle, this would im-\nply being able to estimate both Qπ(s, a) and V π(s), but\nthere are methods that allow the estimation of Qπ(s, a)\nthrough V π(s) over the trajectory, allowing us to only\nneed to learn the latter. There are various ways to esti-\nmate the advantage function using these value functions.\nThe first one is called n-step returns, in which we expand\nthe definition of Qπ(s, a) using the rewards obtained in\nthe current trajectory for n steps, and then take the V -\nvalue of the following one. That is, we expand:\nQπ(st, at) = Eτ∼π\n\u0002\nrt + γrt+1 + γ2rt+2 + · · · + γnrt+n\n\u0003\n+γn+1V π(st+n+1)\n≈rt + γrt+1 + γ2rt+2 + · · · + γnrt+n\n+γn+1V π(st+n+1),\n(19)\nwhich assumes accurate V π(s) estimations. This leaves\nour bias-variance trade-off explicit: the rewards from the\ntrajectory have high variance, while V π(s) is a biased\nestimation. Higher values of n present higher variance,\nso n should be chosen to balance these two effects. An-\nother way to estimate the advantage is called Generalized\nAdvantage Estimation (GAE) [61]. GAE calculates an\nexponentially-weighted average of all n-step advantages,\nintending to reduce the variance of the estimation while\nkeeping the bias low. The expression is as follows\nAπ\nGAE(st, at) =\n∞\nX\nk=0\n(γλ)kδt+k\nwhere δt = rt + γV π(st+1) −V π(st),\n(20)\nwith λ ∈[0, 1] controls the decay rate. A higher value\nintroduces higher variance, up to a value of 1 which rep-\nresents the Monte-Carlo estimate. A value of 0, on the\nother hand, computes the TD estimate of the returns.\nGAE is our estimation of choice for the advantage func-\ntion, which we use for our physical problem. Finally, we\nneed to obtain a way to estimate the V π(s) values for\neach state. Following the structure of value-based algo-\nrithms and the definition of advantage, we set the target\nof the critic network as\nV π\ntar(st) = Aπ\nGAE(st, at) + V π(st),\n(21)\nwhich we obtain in a similar manner to SARSA and Dou-\nble DQN [34]. Thus, the full A2C algorithm can be sum-\nmarized in the pseudocode 5.\nAlgorithm 5: A2C pseudocode with GAE and\nMSE [34].\nInput: a differentiable policy parametrization\nπ : S × Rd1 →A\nInput: a differentiable state-value function\nparametrization bV : S × Rd2 →R\nInput: algorithm parameters. Initialize learning rate\nα1 > 0 and α2 > 0, GAE exponential weight\n1 ≥λ ≥0 and discount rate 1 ≥γ ≥0\nOutput: optimized policy π and improved bV\n1 Initialize arbitrarily π weights, ⃗θ ∈Rd1\n2 Initialize arbitrarily bV weights, ⃗ω ∈Rd2\n3 for episode do\n4\nInitialize s0 ̸= terminal\n5\nfor episode step do\n6\nTake at following π(at|st), observe rt, st+1\n7\nst ←st+1\n8\nfor t = 0...T do\n9\nObtain bV (st)\n10\nCalculate b\nAπ\nGAE(st, at) = PT −t\nk=0(γλ)kδt+k\n11\nCalculate bV π\ntar(st) = b\nAπ\nGAE(st, at) + bV π(st)\n12\nObtain actor loss:\nLactor = −1\nT\nPT\nt=0 b\nAπ\nGAE(st, at) log π(at|st)\n13\nObtain critic MSE:\nLcritic = 1\nT\nPT\nt=0(bV π\ntar(st) −bV π(st))2\n14\nUpdate actor parameters: ⃗θ ←⃗θ + α1∇θLactor\n15\nUpdate critic parameters: ⃗ω ←⃗ω + α2∇ωLcritic\nActor-critic algorithms present a series of issues too,\nsome of the most common including performance col-\nlapse and sample inefficiency from being on-policy al-\ngorithms. PPO is one of the most popular algorithms\ndesigned to solve them, using a surrogate objective that\nensures monotonic improvements and allows to reuse off-\npolicy data samples. This new PPO objective replaces\nthe original A2C objective, and could also be applied to\nREINFORCE.\nTo understand this algorithm, we first need to consider\nthat we are performing the search of optimal policies in\nthe parameters space of Θ, while the policies are sam-\npled from the policy space Π. Thus, regular steps in the\nparameter space do not translate to regular steps in the\npolicy space, where the optimal step size may vary de-\npending on the local geometry, and might result in too\nbig or too small policy steps. This is what eventually\ncauses performance collapse.\nTo avoid this issue, we consider a constraint to the\nchange in the policy space. We define the distance in the\nobjective between policies as [60]\nJ(π′) −J(π) = Eτ∼π′\n\" T\nX\nt=0\nγtAπ(st, at)\n#\n,\n(22)\nwhere π is the original policy, which we used to calculate\nAπ, and π′ is the policy that we would obtain after the\n15\nparameter update. This is a measure of the performance\nof the new policy, and our goal would be to maximize\nit. This new maximization problem ensures that there\nis always a monotonic positive improvement, since the\nworst possible result would be to let π′ = π, without any\nmodification to the policy.\nStill, we cannot properly use this function as an ob-\njective function, because the expectation is performed\nsampling from the new policy, but the new policy is only\navailable after the update that would require said new\npolicy. To solve this issue, we perform the sampling using\nthe old policy, but including importance sampling terms,\nthe ratio between new and old policies. Thus, the surro-\ngate objective (renamed as J(π′)) becomes an expecta-\ntion over the current policy π, as desired:\nJ(π′) = Eτ∼π\n\" T\nX\nt=0\nγtAπ(st, at)π′(at|st)\nπ(at|st)\n#\n.\n(23)\nOptimization using this objective is still gradient ascent,\nso this can become the new objective for the policy gra-\ndient function. Lastly, we need to check that the error of\nthe estimation given by the approximation with impor-\ntance sampling is not big enough to no longer fulfill the\ncondition of always having a positive distance in the ob-\njective between policies. We know that, for sufficiently\nclose policies, we can bind their error by their KL di-\nvergence [62]. We only need to ensure that the policy\nimprovement is bigger than this limit to accept a change.\nThe application of this constraint can be quite straight-\nforward: we simply need to constrain this KL divergence\nto be smaller than a given value, δ. Thus, the problem\nbecomes\nmax\nθ\nE\n\u0014π′(st, at)\nπ(st, at) Aπ(st, at)\n\u0015\nensuring Et [KL(π′(st, at)||π(st, at))] ≤δ.\n(24)\nWe will solve this problem using PPO with clipped sur-\nrogate objective, a much simpler implementation doing\naway with the need to compute the KL divergence. For\nthis, we define a hyperparameter ϵ to constrain the im-\nportance sampling terms. Thus, we constrain the objec-\ntive between (1 −ϵ)Aπ\nt and (1 + ϵ)Aπ\nt . The implementa-\ntion is fairly simple, only needing to change the objective\nfunction from A2C in line 12 of pseudocode 5 by this new\nfunction:\nJCLIP(θ) = Et\n\u0014\nmin\n\u0012π′(st, at)\nπ(st, at) Aπ(st, at),\nclip\n\u0012π′(st, at)\nπ(st, at) , 1 −ϵ, 1 + ϵ\n\u0013\nAπ(st, at)\n\u0013\u0015\n.\n(25)\nLet us now describe our application of A2C and PPO\nto our optimization problem. Since these algorithms in-\nvolve a larger amount of hyperparameters, we made use\nof external packages to a more optimized implementa-\ntion.\nThe RL problems themselves were implemented\nwith the help of the library Stable-Baselines3 [63], which\nfeatures a set of reliable RL algorithms using PyTorch.\nFor the optimization of hyperparameters, we used the\nlibrary Optuna [64], an automatic hyperparameter opti-\nmization software framework. Still, fundamentally, there\nis no change in the algorithms themselves or their appli-\ncation.\nThe hyperparameters used in the implemented codes\nfor A2C and PPO are detailed in Table IV and Table V,\nrespectively. Most of them were chosen and kept fixed\nfrom the beginning, with only the last 4 of their respec-\ntive tables being optimized by the Optuna hyperparame-\nter search. The hyperparameters of said search are avail-\nable in Table VI. The training results for both A2C and\nPPO are shown in Fig. 7, where we compare again the\nbest state found by the algorithms and a random search,\n(a)\nA2C\n(b)\nPPO\nFIG. 7. Training of both A2C and PPO algorithms for our\nphysical problem of interest. (a) Evolution of largest HTC\nfound as new states are explored for the A2C algorithm. A2C\nin blue versus a random search in orange.\n(b) Same as in\n(a), but with PPO instead.\nIn both panels the solid lines\ncorrespond to the mean value and the shaded areas to the\nstandard deviations, as obtained in 11 and 14 independent\nruns for A2C and PPO algorithms, respectively.\n16\nFig. 7(a) for A2C and Fig. 7(b) for PPO. Both algorithms\nwere trained for only 4000 full episodes, since training\nwith 105 episodes like in REINFORCE yielded the exact\nsame results. Following the format from the rest of the\npaper, the algorithm is presented in blue, in dark blue the\naverage and in light blue the standard deviation, while\nthe random search is presented in orange. The random\nsearch algorithm results are always obtained using 1000\nruns, while for the RL algorithms, we only considered\nthose that were able to perform a reasonable exploration\nof new found states, resulting in 11 for A2C and 14 in\nPPO. Note that this is similar to having considered a\n“pruning” procedure over exploration, and does not con-\nsider how efficient those runs were in finding good states,\nonly new ones.\nFig. 7(a) compares A2C and the random search for a\ntotal number of 3500 states found, much smaller than\nother algorithms. A2C is able to outperform the random\nalgorithm slightly, in a similar fashion to REINFORCE,\nbut it is not decidedly better. Fig. 7(b) compares PPO\nand the random search for a total number of 1500 states\nfound, an even smaller number. PPO is able to outper-\nform both the random search and the A2C algorithm\nwith much smaller exploration. However, it also presents\na wider standard deviation compared to A2C.\nWe can more clearly describe these results by consid-\nering the statistics from the runs presented. For the 3500\nfound states in the study for A2C, the algorithm is able to\nfind one of the 5 configurations with highest HTC 36%\nof the runs, finding the best configuration 9.1% of the\nruns. In comparison, the random search is only able to\nfind one of the best 5 configurations in 26.2% of the runs,\nand the best one in 6.8% of them. For PPO, the results\nare even better than for A2C with a much lower number\nof states. PPO is able to find one of the best 5 configu-\nrations 29% of the runs, with the best configuration 21%\nof the runs. Conversely, the random search can only find\none of the top 5 configurations 12.3% of the runs, and\n3.2% the configuration with highest HTC. This makes\nPPO the best combined method for our purposes, and\na strong candidate for a RL algorithm in these kind of\nproblems.\nV.\nDISCUSSION AND CONCLUSIONS\nAll algorithms employed in this work were able to\nidentify, with differing success, the best configuration\namong the possibilities explored. This optimal 16 layer-\nconfiguration is shown in Fig. 8(a). Notice that it differs\nfrom the baseline configuration of Fig. 2(a), which corre-\nsponds to the periodic structure that one would propose\nbased on physical intuition. The best state found by our\nalgorithms does not seem to have any particular symme-\ntry, although one can identify a periodic pattern forming\na “supercell”, see Fig. 8(a). This pattern is repeated four\ntimes in the whole system when taking into account the\ngap – which is two dielectric layers wide – and both sub-\n(a)\n(b)\n(c)\nFIG. 8. Description of the best example found for the 16 layer\nproblem. (a) Schematic structure of the best state found. A\nsupercell structure is identified as a yellow shaded region on\nthe left subsystem. (b) Transmission of evanescent waves as a\nfunction of the frequency (ω) and the parallel wavevector (k)\nfor the optimal structure shown in panel (a). (c) Comparison\nbetween the spectral HTC between the best state and the\nbaseline of Fig. 2(b) at room temperature (T = 300 K).\nsystems. The baseline configuration exhibits a HTC of\n17\n1.37 × 105 W/m2 K, while the best state found has a\nHTC of 1.66 × 105 W/m2 K, a 21% higher.\nFigure 8(b) shows the transmission of evanescent waves\nas a function of the frequency ω and the parallel wavevec-\ntor k for the optimal configuration.\nThe transmission\npattern shows a series of narrow lines of values close to\nunity, which as explained in Sec. III result from the hy-\nbridization of the SPPs that are formed in the interfaces\nbetween the metallic and dielectric layers, similar to the\nbaseline case in Fig. 2(b). We can notice some differences\nwhen comparing the two: while in the baseline case the\nlines all joined to form the same structure, in the opti-\nmal configuration we find a sizeable gap between some\nof the lines, likely stemming from the supercell struc-\nture mentioned above. This implies that their radiative\nheat transfer behavior is clearly different. In Fig. 8(c) we\ncompare the spectral HTC hω of both the baseline (also\nshown in Fig. 2(c)) and the best state found. We can see\nthat both present the main central resonant peak, but\nthe one of the baseline is higher while the best state has\na higher value in many other frequencies, with a notable\nsecondary maximum at lower frequencies. The integra-\ntion of these spectra over frequency yields the total HTC\nvalues mentioned at the end of the previous paragraph.\nSo far, we have focused our study on the case of 16\nactive layers.\nThis case is complex enough for the il-\nlustration of the RL techniques while still being small\nenough that we only have ∼65 thousand possible con-\nfigurations. Thus, we can directly find out which state\nhas the highest HTC by simply analyzing all of them. To\nshow that the RL methods reported here can also be ap-\nplied to problems in which we do not know the solution\nbeforehand, we consider now the case with 24 active lay-\ners, where the total number of states is ∼17 million. We\nused the exact same techniques as before, but taking as\nbaseline the perfectly periodic case with 24 layers instead\nof 16. We solved this problem using Double DQN, the\nvalue-based algorithm that showed the highest efficiency\nin the previous section, for 20 independent runs and 2500\ntraining steps. The results for the largest HTC are shown\nin Fig. 9. They were obtained using the optimal hyper-\nparameters found for the 16-layer problem. Compared\nto the results discussed above, the RL algorithm sur-\npasses the random algorithm after a few thousand ex-\nplored states. In addition, notice that the random algo-\nrithm reaches a mean value of 1.617×105 W/m2K, while\nthe RL algorithm yields 1.673 × 105 W/m2K, which in\nturn overcomes the value of the baseline state (1.623×105\nW/m2K). This shows again that we can beat the physi-\ncal intuition with the proper use of RL, reaching an state\nwith a HTC value of 1.807×105 W/m2K. Therefore, this\nanalysis shows that the Double DQN algorithm can be\nsuccessfully applied to a state space for which the com-\nputation of the HTC of all its states is not feasible in a\nreasonable amount of time.\nSo, in summary, we have shown in this work how RL\ncan be used to tackle optimization problems in the con-\ntext of radiative heat transfer. As an illustration, we have\nFIG. 9. Largest HTC discovered as a function of the number\nof found states in the problem with 24 active layers obtained\nwith the Double DQN algorithm. We also present the results\nobtained with the random algorithm. The solid line corre-\nsponds to the mean value and the shaded area to the stan-\ndard deviations, as obtained in 20 independent runs for both\nalgorithms.\naddressed the maximization of the NFRHT between hy-\nperbolic metamaterials made of a combination of metallic\nand dielectric layers. This problem is quite generic and\ncontains the basic ingredients of most optimization prob-\nlems in the field of thermal radiation. Our work demon-\nstrates that these problems can be naturally formulated\nas a sequential decision-making problem and therefore,\nthey are susceptible to be tackled with RL methods. It\nis worth remarking that one could address inverse de-\nsign problems in the same way by simply redefining the\nobjective function.\nIn the physical problem studied in this work, we have\nshown that essentially all RL algorithms are able to find\nnear optimal solutions, albeit with different efficiencies.\nIn our case, we have found that Double DQN is the most\nefficient algorithm, with PPO also providing high quality\nresults. While PPO finds the top 5 HTC values less fre-\nquently than Double DQN, it explores substantially fewer\nstates and is still able to find the best ones much more\nreliably than the random algorithm. Therefore, both al-\ngorithms present their pros and cons in the application\nto our particular problem. In any case, we have provided\na comprehensive guide on how to utilize in practice most\nof the key RL algorithms. Thus, we hope that our work\nwill help other researchers to employ RL techniques as\npart of their toolkit for the investigation of optimization\nand inverse design problems in the context of radiative\nheat transfer and related topics.\nThe codes with the different RL algorithms used in\nthis work are available from Ref. [65].\n18\nACKNOWLEDGMENTS\nE.O.M. thanks Komorebi AI Technologies team, with\nspecial mention to its founders, for their broad coding\ninsights and discussions regarding some of the reported\nresults.\nFurthermore, she acknowledges financial sup-\nport by the Comunidad de Madrid and Komorebi AI\nTechnologies under the Program “Doctorados Industri-\nales” (reference IND2022/IND-23536). J.J.G.E. was sup-\nported by the Spanish Ministry of Science and Innovation\n(Grant No. FPU19/05281). J.B.A. acknowledges fund-\ning from the Spanish Ministry of Science and Innova-\ntion (PID2022-139995NB-I00). J.C.C. thanks the Span-\nish Ministry of Science and Innovation for financial sup-\nport through Grant No. PID2020-114880GB-I00 and the\n“Mar´ıa de Maeztu” Programme for Units of Excellence\nin R&D (CEX2023-001316-M).\n† These authors contributed equally to this work.\nAppendix A: Hyperparameters\nIn Tables I-V we summarize the hyperparameters used\nin the different algorithms employed in this work.\nIn\nparticular, we describe its meaning and role.\n[1] M. F. Modest, Radiative Heat Transfer (Elsevier, Aca-\ndemic Press, 2013).\n[2] J. R. Howell, M. P. Meng¨u¸c, K. Daun, and R. Siegel,\nThermal Radiation Heat Transfer (CRC press, Boca Ra-\nton, 2020).\n[3] Z. M. Zhang, Nano/Microscale Heat Transfer (Springer,\n2020).\n[4] J. C. Cuevas and F. J. Garc´ıa-Vidal, Radiative heat\ntransfer, ACS Photonics 5, 3896 (2018).\n[5] D. Polder and M. Van Hove, Theory of Radiative Heat\nTransfer between Closely Spaced Bodies, Phys. Rev. B\n4, 3303 (1971).\n[6] B. Song, A. Fiorino, E. Meyhofer, and P. Reddy, Near-\nfield radiative thermal transport: From theory to exper-\niment, AIP Advances 5, 053503 (2015).\n[7] S.-A. Biehs, R. Messina, P. S. Venkataram, A. W. Ro-\ndriguez, J. C. Cuevas, and P. Ben-Abdallah, Near-field\nradiative heat transfer in many-body systems, Rev. Mod.\nPhys. 93, 025009 (2021).\n[8] W. Challener, C. Peng, A. Itagi, D. Karns, W. Peng,\nY. Peng, X. Yang, X. Zhu, N. Gokemeijer, Y.-T. Hsia,\net al., Heat-assisted magnetic recording by a near-field\ntransducer with efficient optical energy transfer, Nat.\nPhotonics 3, 220 (2009).\n[9] Y. De Wilde, F. Formanek, R. Carminati, B. Gralak,\nP.-A. Lemoine, K. Joulain, J.-P. Mulet, Y. Chen, and\nJ.-J. Greffet, Thermal radiation scanning tunnelling mi-\ncroscopy, Nature 444, 740 (2006).\n[10] A. Kittel, U. F. Wischnath, J. Welker, O. Huth, F. Ruet-\ning, and S.-A. Biehs, Near-field thermal imaging of nanos-\ntructured surfaces, Appl. Phys. Lett. 93, 193109 (2008).\n[11] A. C. Jones,\nB. T. O’Callahan,\nH. U. Yang, and\nM. B. Raschke, The thermal near-field: Coherence, spec-\ntroscopy, heat-transfer, and optical forces, Progress in\nSurface Science 88, 349 (2013).\n[12] R. Carminati and J.-J. Greffet, Near-field effects in spa-\ntial coherence of thermal sources, Phys. Rev. Lett. 82,\n1660 (1999).\n[13] J.-J. Greffet, R. Carminati, K. Joulain, J.-P. Mulet,\nS. Mainguy, and Y. Chen, Coherent emission of light by\nthermal sources, Nature 416, 61 (2002).\n[14] R. Mittapally, A. Majumder, P. Reddy, and E. Mey-\nhofer, Near-field thermophotovoltaic energy conversion:\nProgress and opportunities, Phys. Rev. Appl. 19, 037002\n(2023).\n[15] W. Li and S. Fan, Nanophotonic control of thermal radi-\nation for energy applications, Optics Express 26, 15995\n(2018).\n[16] E. Rephaeli, A. Raman, and S. Fan, Ultrabroadband pho-\ntonic structures to achieve high-performance daytime ra-\ndiative cooling, Nano Lett. 13, 1457 (2013).\n[17] A. P. Raman, M. A. Anoma, L. Zhu, E. Rephaeli, and\nS. Fan, Passive radiative cooling below ambient air tem-\nperature under direct sunlight, Nature 515, 540 (2014).\n[18] J. K. Tong, X. Huang, S. V. Boriskina, J. Loomis, Y. Xu,\nand G. Chen, Infrared-transparent visible-opaque fabrics\nfor wearable personal thermal management, ACS Pho-\ntonics 2, 769 (2015).\n[19] P.-C. Hsu, A. Y. Song, P. B. Catrysse, C. Liu, Y. Peng,\nJ. Xie, S. Fan, and Y. Cui, Radiative human body cooling\nby nanoporous polyethylene textile, Science 353, 1019\n(2016).\n[20] W. Li, Y. Shi, K. Chen, L. Zhu, and S. Fan, A com-\nprehensive photonic approach for solar cell cooling, ACS\nPhotonics 4, 774 (2017).\n[21] A. Lenert, D. M. Bierman, Y. Nam, W. R. Chan,\nI. Celanovi´c, M. Soljaˇci´c, and E. N. Wang, A nanopho-\ntonic solar thermophotovoltaic device, Nat. Nanotechnol.\n9, 126 (2014).\n[22] S.-A. Biehs and P. Ben-Abdallah, Revisiting super-\nplanckian thermal emission in the far-field regime, Phys.\nRev. B 93, 165405 (2016).\n[23] V.\nFern´andez-Hurtado,\nA.\nI.\nFern´andez-Dom´ınguez,\nJ. Feist, F. J. Garc´ıa-Vidal, and J. C. Cuevas, Super-\nplanckian far-field radiative heat transfer, Phys. Rev. B\n97, 045408 (2018).\n[24] D. Thompson, L. Zhu, R. Mittapally, S. Sadat, Z. Xing,\nP. McArdle, M. M. Qazilbash, P. Reddy, and E. Mey-\nhofer, Hundred-fold enhancement in far-field radiative\nheat transfer over the blackbody limit, Nature 561, 216\n(2018).\n[25] P. Chao, B. Strekha, R. Kuate Defo, S. Molesky, and\nA. W. Rodriguez, Physical limits in electromagnetism,\nNature Reviews Physics 4, 543 (2022).\n[26] S. Molesky, Z. Lin, A. Y. Piggott, W. Jin, J. Vuckovi´c,\nand A. W. Rodriguez, Inverse design in nanophotonics,\nNat. Photonics 12, 659 (2018).\n[27] P. Mehta, M. Bukov, C.-H. Wang, A. G. Day, C. Richard-\nson, C. K. Fisher, and D. J. Schwab, A high-bias, low-\nvariance introduction to machine learning for physicists,\n19\nPhys. Rep. 810, 1 (2019).\n[28] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld,\nN. Tishby, L. Vogt-Maranto, and L. Zdeborov´a, Machine\nlearning and the physical sciences, Rev. Mod. Phys. 91,\n045002 (2019).\n[29] F. Marquardt, Machine learning and quantum devices,\nSciPost Phys. Lect. Notes , 29 (2021).\n[30] J. J. Garc´ıa-Esteban, J. Bravo-Abad, and J. C. Cuevas,\nDeep Learning for the Modeling and Inverse Design of\nRadiative Heat Transfer, Phys. Rev. Appl. 16, 064006\n(2021).\n[31] C. Zhu, E. A. Bamidele, X. Shen, G. Zhu, and B. Li, Ma-\nchine learning aided design and optimization of thermal\nmetamaterials, Chem. Rev. 124, 4258 (2024).\n[32] S. Yu, P. Zhou, W. Xi, Z. Chen, Y. Deng, X. Luo, W. Li,\nJ. Shiomi, and R. Hu, General deep learning framework\nfor emissivity engineering, Light: Science & Applications\n12, 291 (2023).\n[33] R. S. Sutton and A. G. Barto, Reinforcement learning:\nAn introduction (MIT press, 2018).\n[34] L. Graesser and W. L. Keng, Foundations of Deep Re-\ninforcement Learning: Theory and Practice in Python\n(Addison-Wesley Professional, 2019).\n[35] J. J. Garc´ıa-Esteban, J. C. Cuevas, and J. Bravo-Abad,\nGenerative adversarial networks for data-scarce radiative\nheat transfer applications, Machine Learning:\nScience\nand Technology 5, 015060 (2024).\n[36] B. Song, A. Fiorino, E. Meyhofer, and P. Reddy, Near-\nfield radiative thermal transport: From theory to exper-\niment, AIP Adv. 5, 053503 (2015).\n[37] J. C. Cuevas and F. J. Garc´ıa-Vidal, Radiative Heat\nTransfer, ACS Photonics 5, 3896 (2018).\n[38] S.-A. Biehs, R. Messina, P. S. Venkataram, A. W. Ro-\ndriguez, J. C. Cuevas, and P. Ben-Abdallah, Near-field\nradiative heat transfer in many-body systems, Rev. Mod.\nPhys. 93, 025009 (2021).\n[39] Y. Guo, C. L. Cortes, S. Molesky, and Z. Jacob, Broad-\nband super-Planckian thermal emission from hyperbolic\nmetamaterials, Appl. Phys. Lett. 101, 131106 (2012).\n[40] S.-A. Biehs, M. Tschikin, and P. Ben-Abdallah, Hyper-\nbolic Metamaterials as an Analog of a Blackbody in the\nNear Field, Phys. Rev. Lett. 109, 104301 (2012).\n[41] Y. Guo and Z. Jacob, Thermal hyperbolic metamaterials,\nOpt. Express 21, 15014 (2013).\n[42] S.-A. Biehs, M. Tschikin, R. Messina, and P. Ben-\nAbdallah, Super-Planckian near-field thermal emission\nwith phonon-polaritonic hyperbolic metamaterials, Appl.\nPhys. Lett. 102, 131106 (2013).\n[43] T. J. Bright, X. L. Liu, and Z. M. Zhang, Energy stream-\nlines in near-field radiative heat transfer between hyper-\nbolic metamaterials, Opt. Express 22, A1112 (2014).\n[44] O. D. Miller, S. G. Johnson, and A. W. Rodriguez, Ef-\nfectiveness of Thin Films in Lieu of Hyperbolic Metama-\nterials in the Near Field, Phys. Rev. Lett. 112, 157402\n(2014).\n[45] S.-A. Biehs and P. Ben-Abdallah, Near-Field Heat\nTransfer between Multilayer Hyperbolic Metamaterials,\nZeitschrift f¨ur Naturforschung A 72, 115 (2017).\n[46] H. Iizuka and S. Fan, Significant Enhancement of Near-\nField Electromagnetic Heat Transfer in a Multilayer\nStructure through Multiple Surface-States Coupling,\nPhys. Rev. Lett. 120, 063901 (2018).\n[47] J. Song, Q. Cheng, L. Lu, B. Li, K. Zhou, B. Zhang,\nZ. Luo, and X. Zhou, Magnetically Tunable Near-Field\nRadiative Heat Transfer in Hyperbolic Metamaterials,\nPhys. Rev. Appl. 13, 024054 (2020).\n[48] E. Moncada-Villa and J. C. Cuevas, Near-field radiative\nheat transfer between one-dimensional magnetophotonic\ncrystals, Phys. Rev. B 103, 075432 (2021).\n[49] S. Rytov, Theory of Electric Fluctuations and Thermal\nRadiation (Air Force Cambrige Research Center, 1953).\n[50] S. Rytov, Y. Kravtstov, and V. Tatarskii, Principles of\nStatistical Radiophysics (Springer-Verlag, 1989).\n[51] S. Basu, Z. M. Zhang, and C. J. Fu, Review of near-field\nthermal radiation and its application to energy conver-\nsion, Int. J. Energy Res. 33, 1203 (2009).\n[52] B. Caballero, A. Garc´ıa-Mart´ın, and J. C. Cuevas, Gener-\nalized scattering-matrix approach for magneto-optics in\nperiodically patterned multilayer systems, Phys. Rev. B\n85, 245103 (2012).\n[53] L.-J. Lin, Self-improving reactive agents based on re-\ninforcement learning, planning and teaching, Machine\nlearning 8, 293 (1992).\n[54] V.\nMnih,\nK.\nKavukcuoglu,\nD.\nSilver,\nA.\nGraves,\nI. Antonoglou, D. Wierstra, and M. Riedmiller, Playing\natari with deep reinforcement learning, arXiv:1312.5602.\n[55] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-\nness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski, et al., Human-level control\nthrough deep reinforcement learning, Nature 518, 529\n(2015).\n[56] H. van Hasselt, A. Guez, and D. Silver, Deep reinforce-\nment learning with double q-learning, arXiv:1509.06461.\n[57] H. Hasselt, Double q-learning, in Advances in Neural In-\nformation Processing Systems, Vol. 23, edited by J. Laf-\nferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Cu-\nlotta (Curran Associates, Inc., 2010).\n[58] R. J. Williams, Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning, Machine\nLearning 8, 229 (1992).\n[59] V. Konda and J. Tsitsiklis, Actor-critic algorithms, in\nAdvances in Neural Information Processing Systems,\nVol. 12, edited by S. Solla, T. Leen, and K. M¨uller (MIT\nPress, 1999).\n[60] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal policy optimization algorithms (),\narXiv:1707.06347.\n[61] J. Schulman, P. Moritz, S. Levine, M. Jordan, and\nP. Abbeel, High-dimensional continuous control using\ngeneralized advantage estimation (), arXiv:1506.02438.\n[62] J. Achiam, D. Held, A. Tamar, and P. Abbeel, Con-\nstrained policy optimization, arXiv:1705.10528.\n[63] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernes-\ntus, and N. Dormann, Stable-baselines3: Reliable rein-\nforcement learning implementations, Journal of Machine\nLearning Research 22, 1 (2021).\n[64] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama,\nOptuna:\nA next-generation hyperparameter optimiza-\ntion framework, arXiv:1907.10902.\n[65] Deep reinforcement learning for radiative heat transfer\noptimization problems, v1.0.0 (2024).\n20\nTABLE I. Hyperparameters of SARSA algorithm.\nVariable name\nValue\nDescription\nhidden layers\n4\nNumber of hidden layers of the neural network.\nhidden neurons\n64\nNumber of neurons of each one of the hidden layers.\nactivation function\nSELU\nActivation function of the neurons.\nloss function\nMAE\nLoss function to evaluate the performance of the neural network.\noptimizer\nAdam\nOptimizer during training.\nγ\n0.99\nDiscount rate parameter.\nepisode length\n64\nNumber of steps per episode.\nε\n1\nInitial value of epsilon parameter.\nε decay\n4 × 10−5\nValue to reduce epsilon each train step.\nε minimum\n10−3\nMinimum reachable epsilon value.\nα\n10−3\nValue of learning rate parameter.\nα decay\n0\nValue to reduce the learning rate each train step.\nbatch size\n32\nNumber of experiences to process each train step.\nTABLE II. Hyperparameters of Double DQN algorithm.\nVariable name\nValue\nDescription\nhidden layers\n4\nNumber of hidden layers of the neural network.\nhidden neurons\n64\nNumber of neurons of each one of the hidden layers.\nactivation function\nSELU\nActivation function of the neurons.\nloss function\nMAE\nLoss function to evaluate the performance of the neural network.\noptimizer\nAdam\nOptimizer during training.\nγ\n0.99\nDiscount rate parameter.\nepisode length\n64\nNumber of steps per episode.\nε\n1\nInitial value of epsilon parameter.\nε decay\n9 × 10−5\nEpsilon decay each train step.\nε minimum\n10−3\nMinimum reachable epsilon value.\nα\n10−4\nValue of learning rate parameter.\nα decay\n0\nValue to reduce the learning rate each train step.\nbatch size\n32\nNumber of experiences to process each train step.\nB\n4\nNumber of batches processed each training step.\nK\n104\nMemory size, maximum number of experiences stored in memory.\nh\n4\nNumber of new experiences added each train step.\nF\n5 × 103\nNumber of train steps to update the target network.\nTABLE III. Hyperparameters of REINFORCE algorithm.\nVariable name\nValue\nDescription\nhidden layers\n4\nNumber of hidden layers of the neural network.\nhidden neurons\n64\nNumber of neurons of each one of the hidden layers.\nactivation function\nSELU\nActivation function of the neurons.\noptimizer\nAdam\nOptimizer during training.\nα\n3 × 10−5\nValue of learning rate parameter.\nγ\n0.99\nDiscount rate parameter.\nepisode length\n32\nNumber of experiences in an episode.\nn episodes\n105\nNumber of episodes in training.\n21\nTABLE IV. Hyperparameters of A2C algorithm.\nVariable name\nValue\nDescription\nhidden layers\n4\nNumber of hidden layers of the neural network, same for actor\nand critic networks.\nhidden neurons\n100\nNumber of neurons of each one of the hidden layers, same for\nactor and critic networks.\nactivation function\nSELU\nActivation function of the neurons.\nloss function\nMAE\nLoss function to evaluate the performance of the critic network.\noptimizer\nAdam\nOptimizer during training.\nepisode length\n32\nNumber of experiences in an episode.\nn episodes\n105\nNumber of episodes in training.\ngradient clip\nTrue\nUse of gradient clipping.\nvf coef\n1/2\nConstant to weight the value function loss.\nuse rms prop\nFalse\nUse RMSProp instead of Adam.\nγ\n0.94645\nDiscount rate parameter.\nα\n3.53 × 10−4 Value of learning rate parameter.\nλ\n0.997\nValue of the GAE exponential factor.\nmax grad\n1.285\nMaximum gradient in the optimization step, gradient clipping.\nTABLE V. Hyperparameters of PPO algorithm.\nVariable name\nValue\nDescription\nhidden layers\n4\nNumber of hidden layers of the neural network, same for actor\nand critic networks.\nhidden neurons\n100\nNumber of neurons of each one of the hidden layers, same for\nactor and critic networks.\nactivation function\nSELU\nActivation function of the neurons.\nloss function\nMAE\nLoss function to evaluate the performance of the critic network.\noptimizer\nAdam\nOptimizer during training.\nepisode length\n32\nNumber of experiences in an episode.\nn episodes\n105\nNumber of episodes in training.\nϵ\n0.2\nClipping range for the importance sampling terms.\nbatch size\n32\nBatch size for the PPO algorithm training.\ngradient clip\nTrue\nUse of gradient clipping.\nvf coef\n1/2\nConstant to weight the value function loss.\nγ\n0.99988\nDiscount rate parameter.\nα\n6.24 × 10−5\nValue of learning rate parameter.\nλ\n0.871\nValue of the GAE exponential factor.\nmax grad\n4.042\nMaximum gradient in the optimization step, gradient clipping.\nTABLE VI. Hyperparameters of Optuna search algorithm for A2C & PPO.\nVariable name\nValue\nDescription\nn configs\n100\nNumber of hyperparameter configurations studied.\nn steps\n105\nNumber of experiences explored each configuration.\nsampler\nTPESampler\nSampler chosen to choose the next configuration.\npruner\nMedian\nMethod of pruning a configuration choice.\nn startup\n5\nNumber of configurations before starting the pruner.\nn warmup\n3.33 × 104\nExperiences before checking for pruning.\nγ range\n[0.9, 0.9999]\nRange of exploration values for the γ parameter.\nα range\n[10−6, 10−3]\nRange of exploration values for the α parameter.\nλ range\n[0.0, 1.0]\nRange of exploration values for the λ parameter.\nmax grad range\n[0.5, 5.0]\nRange of exploration values for the max grad parameter.\n",
  "categories": [
    "physics.optics"
  ],
  "published": "2024-08-28",
  "updated": "2024-08-28"
}