{
  "id": "http://arxiv.org/abs/2408.17316v1",
  "title": "Bridging Domain Knowledge and Process Discovery Using Large Language Models",
  "authors": [
    "Ali Norouzifar",
    "Humam Kourani",
    "Marcus Dees",
    "Wil van der Aalst"
  ],
  "abstract": "Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness.",
  "text": "Bridging Domain Knowledge and Process\nDiscovery Using Large Language Models‚ãÜ\nAli Norouzifar1 , Humam Kourani2,1 , Marcus Dees3 , and Wil van der\nAalst1\n1 RWTH University, Aachen, Germany\n{ali.norouzifar, wvdaalst}@pads.rwth-aachen.de\n2 Fraunhofer FIT, Sankt Augustin, Germany\nhumam.kourani@fit.fraunhofer.de\n3 UWV Employee Insurance Agency, Amsterdam, Netherlands\nmarcus.dees@uwv.nl\nAbstract. Discovering good process models is essential for different pro-\ncess analysis tasks such as conformance checking and process improve-\nments. Automated process discovery methods often overlook valuable do-\nmain knowledge. This knowledge, including insights from domain experts\nand detailed process documentation, remains largely untapped during\nprocess discovery. This paper leverages Large Language Models (LLMs)\nto integrate such knowledge directly into process discovery. We use rules\nderived from LLMs to guide model construction, ensuring alignment with\nboth domain knowledge and actual process executions. By integrating\nLLMs, we create a bridge between process knowledge expressed in nat-\nural language and the discovery of robust process models, advancing\nprocess discovery methodologies significantly. To showcase the usability\nof our framework, we conducted a case study with the UWV employee\ninsurance agency, demonstrating its practical benefits and effectiveness.\nKeywords: Process Mining ¬∑ Process Discovery ¬∑ Process Knowledge ¬∑\nLarge Language Models.\n1\nIntroduction\nRecorded event data within information systems provides a rich source of infor-\nmation for process mining applications, enabling organizations to gain insights\nand improve their operational processes. In the field of process mining, vari-\nous automated techniques are utilized to discover descriptive models that ex-\nplain process executions. Despite the development of numerous methodologies\nfor process discovery, the task remains inherently complex and challenging [3].\nDiscovering process models that do not align with domain knowledge presents\nsignificant challenges, particularly when these models are intended for confor-\nmance checking and process improvement.\n‚ãÜThis research was supported by the research training group ‚ÄúDataninja‚Äù (Trustworthy AI for\nSeamless Problem Solving: Next Generation Intelligence Joins Robust Data Analysis) funded by\nthe German federal state of North Rhine-Westphalia.\narXiv:2408.17316v1  [cs.AI]  30 Aug 2024\n2\nAli Norouzifar et al.\nùë≥\nùë´ùë≠ùëÆ\nCost \ncalculation\nSet of \nrules ùëπ\nCandidate cut generator \nùëíùë•ùëùùëôùëúùëüùëí(ùí¢ùêø, ùëπ)\nùë†ùë¢ùëù\nIMr\nProcess \nModel\nLLM\nFig. 1: Our proposed framework to integrate process knowledge in the IMr frame-\nwork employing LLMs.\nIn addition to the extracted event data from information systems, we often\nhave access to domain experts, process documentation, and other resources col-\nlectively referred to as domain knowledge, which cannot be directly used for pro-\ncess discovery. These valuable resources typically remain untapped when aiming\nto discover process models. Incorporating domain knowledge into the discovery\nof process models poses several challenges. For instance, domain experts usually\nhave a thorough understanding of their processes, but they can only explain them\nin natural language. Furthermore, textual process documents, although poten-\ntially rich in detail, also pose integration difficulties. In our paper, we address\nthese challenges by enabling the direct involvement of such information in pro-\ncess discovery through the use of Large Language Models (LLMs). LLMs have\ndemonstrated the ability to handle user conversations and comprehend human\nreasoning effectively.\nOur framework builds upon the IMr framework proposed in [12]. IMr is an\ninductive mining-based framework that recursively selects the process structure\nthat best explains the actual process. Within this framework, the algorithm en-\ncounters various possibilities for constructing the process structure. To guide\nthis selection, rules are introduced as inputs to prune the search space and elim-\ninate potentially suboptimal process structures. Although the concept of rules\nis broad, the Declare rule specification language is proposed as an example [11].\nDeclarative rules are advantageous due to their similarity to human reasoning\nand logic, supported by extensive literature. They are based on logical state-\nments and have specific semantics, though it is unrealistic to expect users to\nprovide these rules directly.\nOur proposed framework, illustrated in Fig. 1, leverages LLMs and prompt\nengineering to integrate domain knowledge into process discovery. Starting with\nan event log, it employs process knowledge in various forms. LLMs play a cru-\ncial role by translating textual inputs into declarative rules, which IMr then\nintegrates. This framework allows for the use of textual process descriptions\nprior to initiating process discovery, enables domain experts to provide feedback\non the discovered models, and facilitates interactive conversations with domain\nexperts to gather information and improve the models.\nBridging Domain Knowledge and Process Discovery Using LLMs\n3\n2\nRelated Work\nIn traditional process discovery, event data are often used as the primary source\nof information to create process models [3]. However, additional information\nresources, such as various forms of process knowledge, can significantly enhance\nthe quality of the discovered models [14]. When available, this supplementary\nknowledge can be utilized before discovery to filter the event log [5], during\nthe discovery phase to influence the process model structure [12], or within an\ninteractive framework [4,15]. Despite these benefits, the direct involvement of\nprocess experts is often limited due to the complexities involved in integrating\ntheir knowledge into process discovery.\nIn [12], declarative rules are used as an additional input for process discovery,\nwhich can be provided by the user or generated by automated methods. How-\never, expecting users to be proficient in declarative rule specification language\nis not always feasible. The proposed method in [4] requires users to engage at a\nlow level to position transitions and places based on guiding visualizations. The\napproach in [15] begins with an initial model discovered from a user-selected\nsubset of variants and incrementally allows adding more variants to update the\nprocess model. Some research focuses on repairing process models after discov-\nery, primarily to improve the correspondence between the process models and\nevent logs, rather than incorporating process knowledge [13]. In contrast, our pa-\nper aims to minimize the effort required from domain experts by using natural\nlanguage conversations to influence process discovery.\nTranslating the natural language to process models using natural language\nprocessing is investigated in [1]. Anomaly detection is examined in [2] by focusing\non semantic inconsistencies in event labels within event logs, utilizing natural\nlanguage processing to identify anomalous behavior. Recently, LLMs have been\nemployed for various process mining tasks. The opportunities, strategies, and\nchallenges of using LLMs for process mining and business process management\nare discussed in [16]. Additionally, several studies propose the extraction of pro-\ncess models directly from textual inputs [6,8,9]. Unlike these approaches, our\nmethod maintains the event log as the main source of information while incor-\nporating textual process knowledge into the discovery process.\n3\nBackground\nThe blue box in Fig. 1 highlights one recursion of the IMr framework [12]. Each\nrecursion extracts a Directly Follows Graph (DFG) from the event log, repre-\nsenting the set of activities Œ£ and their direct succession. The algorithm searches\nfor all binary cuts that divide Œ£ into two disjoint sets considering a structure\nspecification type, i.e., sequence, exclusive choice, concurrent, or loop type. IMr\nfilters out candidate cuts that may violate any rule r ‚ààR, where R is the set of\nrules given by the user or discovered using automated methods. While [12] in-\ncorporates declarative constraints listed in Table. 1, the framework is flexible to\nsupport other rule specification languages. Cost functions evaluate the quality of\ncandidate cuts, based on counting the number of deviating edges and estimating\n4\nAli Norouzifar et al.\nTable 1: Declarative templates supported by IMr [12].\nDeclarative Template\nDescription\nat-most(a)\na occurs at most once.\nexistence(a)\na occurs at least once.\nresponse(a, b)\nIf a occurs, then b occurs after a.\nprecedence(a, b)\nb occurs only if preceded by a.\nco-existence(a, b)\na and b occur together.\nnot-co-existence(a, b)\na and b never occur together.\nnot-succession(a, b)\nb cannot occur after a.\nresponded-existence(a, b)\nIf a occurs in the trace, then b occurs as well.\nthe number of missing edges considering parameter sup ‚àà[0, 1]. In each recur-\nsion, the algorithm selects the cut with the minimum cost, splits the event log\naccordingly, and recursively processes each sub-log until only base cases remain.\n4\nMotivating Example\nTo motivate the research question addressed in this paper, consider the follow-\ning event log extracted from a synthetic process L=[‚ü®A-created, A-canceled ‚ü©300,\n‚ü®A-created, Doc-checked, Hist-checked, A-accepted ‚ü©200, ‚ü®A-created, Hist-checked,\nDoc-checked, A-accepted ‚ü©50, ‚ü®A-created, Doc-checked, Hist-checked, A-rejected ‚ü©300,\n‚ü®A-created, Hist-checked, Doc-checked, A-rejected ‚ü©80, ‚ü®A-created, A-canceled, A-\naccepted‚ü©20, ‚ü®A-created, A-canceled, A-rejected ‚ü©15, ‚ü®A-created, Doc-checked, Hist-\nchecked, A-rejected, A-accepted ‚ü©35], where A stands for application, Doc for doc-\numents, and Hist for history.\nFigure 2a illustrates the process model discovered using the IMf algorithm\nas a state-of-the-art process discovery technique [10]. The IMr framework with\nparameter sup = 0.2 and utilizing the Declare Miner [11] with confidence = 1\ndiscovers the same process model. Consider that in addition to the provided\nevent log, we have some additional process knowledge that helps us verify this\nmodel and pinpoint the possible unexpected behavior represented in the process\nmodel. In this paper, ChatGPT refers to ChatGPT-4o. We provided a text as\nfeedback on this discovered model and asked ChatGPT to translate natural\nlanguage feedback into understandable rules for the IMr framework. Here is our\nwritten feedback:\nThe discovered process does not fully adhere to our intuitions. Specifically, if a claim\nis canceled, the application cannot be either rejected or accepted. Furthermore, a\nclaim cannot be both rejected and accepted for a single individual. Additionally, the\nhistory is always checked after the documents have been reviewed.\nThe following declarative rules, as explained in this paper, were extracted by\nChatGPT:\nBridging Domain Knowledge and Process Discovery Using LLMs\n5\nnot-co-existence(A-cancelled,A-accepted), not-co-existence(A-cancelled,A-rejected),\nnot-co-existence(A-accepted,A-rejected), response(Doc-checked,Hist-checked)\nFigure 2b presents the process model discovered using our proposed pipeline.\nIn this approach, we utilized ChatGPT to interpret the textual feedback and\ngenerate declarative constraints, which are then used as input for the IMr frame-\nwork.\n(a) Discovered model with deviations from the process knowledge. This model is dis-\ncovered using IMf with f=0.2 and IMr with sup=0.2 and rules discovered employing\nDeclare Miner [11] with confidence=1.\n(b) The desired process model considering both the event log and process knowledge.\nIMr with sup=0.2 and rules extracted from process description employing ChatGPT\ndiscover this model.\nFig. 2: Discovered models from the motivating example event log using different tech-\nniques.\n5\nDomain-Enhanced Process Discovery with LLMs\nIn this section, we present our framework that leverages LLMs to integrate do-\nmain knowledge into the process discovery task. Figure 3 illustrates an overview\nof our proposed framework. The core idea is to utilize domain knowledge to gen-\nerate a set of rules R which serves as input for the IMr framework. This can be\ndone before starting the discovery by encoding process descriptions as rules, or\nafter the process discovery by having a domain expert review the process model\nand provide feedback. Engaging in interactive conversations with LLMs in both\nscenarios helps address uncertainties and improve the quality of the extracted\nrules. An implementation of the framework is publicly available1.\nRule Validation\nTask Definition\nRule Extraction\nIMr\nframework\nBusiness Context \nInteractive Refinement\nFeedback Integration\nùë≥\nSet of \nrules ùëπ\nProcess \nModel\nFig. 3: Different components of the designed framework to bridge domain knowledge\nand process discovery using LLMs.\n1 https://github.com/aliNorouzifar/IMr-LLM.git\n6\nAli Norouzifar et al.\n5.1\nTask Definition\nAs outlined in [9], role promoting, knowledge injection, few-shot learning, and\nnegative prompting techniques have significant potential to effectively prepare\nLLMs for specific process mining tasks. In our initial prompt, we define the role\nof the LLM as an interface between the domain expert and process discovery\nframework, such that LLM should encode the domain knowledge to declarative\nconstraints as we need in IMr. Despite the similarity of declarative templates to\nhuman logic and reasoning, we observed the difficulties of LLMs in adhering to\nstrict expectations. Therefore, we explain in our prompt the set of constraints\nwe support, detailing both the syntax and the semantics of these constraints\n(cf. Table 1). We leverage the LLM‚Äôs ability to derive insights from examples by\nproviding multiple pairs of textual process descriptions and their corresponding\ndeclarative constraints. Additionally, we include instructions to avoid common\nissues, such as syntactic mistakes, and extend our learning pairs to include ex-\namples of undesirable constraints. The detailed written prompt is available in\nour GitHub repository2.\n5.2\nRule Extraction\nAfter introducing the task, the LLM is ready to receive textual input and produce\noutput as declarative constraints. As illustrated in Fig. 3, domain experts can\ncontribute in three distinct ways: providing business context, offering feedback\nafter reviewing process models, and engaging in interactive conversations with\nthe LLM. In the following sections, we explain these contributions in detail and\ndiscuss their respective roles.\nBusiness Context The domain expert can introduce the actual business pro-\ncess to the LLM, providing a general overview, detailing the relationships be-\ntween specific activities, or even including constraints written in natural lan-\nguage. This flexibility allows the domain expert to tailor the input based on\ntheir unique insights and the specifics of the process at hand. It is important\nto note that the LLM is unaware of specific activity labels used in the recorded\nevent data. The list of activities can be automatically derived from the event\nlog, ensuring that all relevant actions are accurately captured in the generated\nconstraints. Alternatively, the domain expert can provide the list of activities\nand add context to guide the LLM in relating the process description with the\nactivity labels, resulting in constraints that involve the correct activity labels.\nFeedback integration After generating the initial process model, it is pre-\nsented to the domain expert for review. The domain expert is expected to ex-\namine the process model for accuracy, completeness, and practical alignment\nwith real-life scenarios. In case of finding errors in the represented model, the\ndomain expert can provide a written feedback and explain the behaviors that\ndo not make sense in the real process. The LLM then adjusts and refines the\ndeclarative constraints based on this feedback.\n2 https://github.com/aliNorouzifar/IMr-LLM/blob/master/files/prompts.pdf\nBridging Domain Knowledge and Process Discovery Using LLMs\n7\nInteractive Refinement In typical scenarios, LLMs tend to provide answers\nthat appear confident and definitive, often without indicating any uncertainty [7].\nWe facilitate a more detailed understanding of the provided textual descriptions\nby encouraging the LLM to express uncertainty and address it by asking ques-\ntions. This stage involves a dynamic dialogue between the LLM and the domain\nexpert. Should it encounter gaps in its knowledge or find ambiguities in the\nprocess descriptions, the LLM is encouraged to formulate and pose relevant\nquestions. These questions are directed towards the domain experts, who then\nprovide responses. The quality and precision of the responses provided by do-\nmain experts play a significant role in enhancing the quality of the generated\nconstraints.\n5.3\nRule Validation\nAn essential step in the framework is checking the extracted declarative con-\nstraints from the LLM‚Äôs response. The LLM is instructed to encapsulate the\nconstraints within specific tags in the response and to write them in a prede-\nfined language with no additional text or descriptions. Following extraction, the\nconstraints undergo a validation process. This includes checking that the syntax\nof each constraint conforms to our predefined language, e.g., checking the type\nidentifier and the number of activities specified within the constraint. Addition-\nally, the labels of activities are verified against the activities recorded in the\nevent log. If any errors are detected during validation, an error-handling loop is\ninitiated. A new prompt specifies the problem and its location, prompting the\nLLM to adjust its output.\n6\nCase Study\nA case study with the UWV employee insurance agency is conducted to demon-\nstrate the usability of our approach in a real-life setting. UWV is responsible\nfor managing unemployment and disability benefits in the Netherlands. For this\ncase study, one of UWV‚Äôs claim-handling processes is selected. Figure 4 depicts\nthe normative model of this process, which was developed in collaboration with\nprocess experts who have a thorough understanding of the workflow. The event\nlog used in this study contains 144,046 cases, 16 unique activities, and 1,309,719\nevents. Our GitHub repository provides the full prompting history and more\nreadable process models3.\nFig. 4: Normative model of the UWV claim handling process, extracted manually in\ncollaboration with domain experts [12].\n3 https://github.com/aliNorouzifar/IMr-LLM/blob/master/files/prompts.pdf\n8\nAli Norouzifar et al.\n6.1\nProcess Discovery Without Including Process Knowledge\nOur initial attempt to discover a process model using the IMf algorithm with\nf = 0.2 resulted in the model shown in Fig. 5a. When compared to the normative\nmodel, significant differences are observed, e.g., Receive Claim and Start Claim\nare the first mandatory steps but the process model allows for skipping them or\nfor many other activities occurring before them. Fig. 5b illustrates the process\nmodel discovered using the IMr algorithm with sup = 0.2 and an empty set\nof input rules. Although this model shows more structural similarities to the\nnormative model, it still contains some nonsensical differences. For instance,\nBlock Claim 1 should only be relevant if the claim is planned to be accepted,\nbut this model permits it for rejected cases as well. Similarly, Receive Objection 2\nshould only occur if the claim is rejected, yet the model allows it for accepted\ncases as well.\nX\nBlock\nClaim 2\n+\nReject\nClaim\nX\nX\nReceive\nObjection 2\nReceive\nObjection 1\nX\nBlock\nClaim 1\nX\nX\nX\nReceive\nClaim\n+\nStart\nClaim\nX\n+\nAccept\nClaim\nX\nCorrect\nClaim\nWithdraw\nClaim\nBlock\nClaim 3\nRepayment\nUnblock\nClaim 1\n+\nPayment\nOrder\nX\nExecute\nPayment\nX\nUnblock\nClaim 3\nX\n(a) Discovered model with using IMf with f = 0.2.\nReceive\nClaim\nStart\nClaim\nX\nBlock\nClaim 1\nX\nCorrect\nClaim\nBlock\nClaim 2\nX\nReject\nClaim\nUnblock\nClaim 1\nX\nX\nAccept\nClaim\nX\nReceive\nObjection 2\nX\nX\nReceive\nObjection 1\nPayment\nOrder\nX\nExecute\nPayment\nX\n+\nX\nX\nX\nRepayment\nX\nBlock\nClaim 3\nX\nUnblock\nClaim 3\nWithdraw\nClaim\nX\n+\n(b) Discovered model using IMr with sup = 0.2 without any rules.\nReceive\nClaim\nStart\nClaim\nX\nBlock\nClaim 2\nBlock\nClaim 1\nX\nReject\nClaim\nCorrect\nClaim\nX\nX\nUnblock\nClaim 1\nX\nReceive\nObjection 2\nX\nAccept\nClaim\nX\nPayment\nOrder\nBlock\nClaim 3\nX\nX\nWithdraw\nClaim\nX\nReceive\nObjection 1\nExecute\nPayment\nX\nUnblock\nClaim 3\nX\nRepayment\n(c) Discovered model using IMr with sup = 0.2 and the rules provided by ChatGPT.\nReceive\nClaim\nStart\nClaim\nX\nBlock\nClaim 2\nBlock\nClaim 1\nX\nReject\nClaim\nCorrect\nClaim\nX\nX\nUnblock\nClaim 1\nX\nAccept\nClaim\nReceive\nObjection 2\nX\nX\nPayment\nOrder\nExecute\nPayment\nX\nX\nReceive\nObjection 1\nX\n+\nX\nX\nX\nUnblock\nClaim 3\nX\nRepayment\nBlock\nClaim 3\nX\nWithdraw\nClaim\nX\n+\n(d) Discovered model using IMr with sup = 0.2 and the rules provided by ChatGPT\nafter incorporating the domain expert feedback.\nFig. 5: Discovered models from UWV event log using different strategies.\n6.2\nEmploying ChatGPT to Extract the Rules\nWe experimented with Gemini and various versions of ChatGPT to translate\nthe process knowledge into declarative rules. ChatGPT-4o provided the best\nconstraints and demonstrated a superior understanding of the task. By incor-\nporating rules extracted by ChatGPT into the IMr framework, we obtained the\nprocess model shown in Fig. 5c. After iterating with feedback from domain ex-\nperts, the model is further refined and improved, resulting in the process model\ndepicted in Fig. 5d.\nBridging Domain Knowledge and Process Discovery Using LLMs\n9\nThe process knowledge for this case study is incorporated as a set of activity\nlabels and a process description provided by a domain expert. The blue boxes\nindicate the prompts given by domain experts, while the red boxes display the\nresponses generated by ChatGPT.\nOur process is designed to handle claim applications within an insurance agency,\nwhere applicants request various types of benefits, and our system processes these\napplications through a structured workflow. There are three types of blocking mech-\nanisms: Block Claim 1, Block Claim 2, and Block Claim 3, each with corresponding\nresolutions: Unblock Claim 1, Unblock Claim 2, and Unblock Claim 3. The process\nbegins once a claim is received. Block Claim 1 occurs when initial issues are de-\ntected, leading to necessary corrections before the claim is unblocked and accepted,\nresulting in the client receiving between one to three payments. Block Claim 2 is\napplied when the claim is deemed ineligible for receiving any payments, resulting in\nrejection and potentially an objection from the client. Block Claim 3 is triggered if\nthere is missing information, such as incorrect bank account details, after a claim\nis accepted; the client is notified to provide the correct details, and the claim is un-\nblocked upon receipt of this information. Additionally, clients may file an objection\nafter receiving payments, leading to the withdrawal of the claim and repayment of\nthe received money to the agency.\nFollowing this process description, we encouraged ChatGPT to ask clarifying\nquestions whenever it encountered uncertainties about the declarative rules. The\nbox below provides an example of questions posed by ChatGPT.\nQ1: Can \"Block Claim 1,\" \"Block Claim 2,\" and \"Block Claim 3\" occur in the\nsame process instance, or are they mutually exclusive? Q2: Are the corresponding\nunblocking activities (\"Unblock Claim 1,\" \"Unblock Claim 2,\" and \"Unblock Claim\n3\") strictly linked to their respective blocking activities, or can there be other se-\nquences? Q3: Is there any restriction on the order of \"Receive Objection 1\" and\n\"Receive Objection 2,\" or can they occur independently of each other? ...\nThe provided questions demonstrate the LLM‚Äôs correct approach to under-\nstanding the declarative constraints between the activities in the process. For\ninstance, the first question indicates that ChatGPT is attempting to discern\nwhether there is a co-existence or non-co-existence relationship between block\nclaim types. Question 3 is also pertinent, as the process description does not\nclearly distinguish between Receive Objection 1 and Receive Objection 2. The\ndomain expert provided the following answers to these questions:\nA1: Some cases may have both Block Claim 1 and Block Claim 3. But Block Claim 2\ncan never occur in the trace with Block Claim 1 or Block Claim 3. A2: Each blocking\nmechanism (Block Claim 1, Block Claim 2, Block Claim 3) has a corresponding\nunblocking mechanism (Unblock Claim 1, Unblock Claim 2, Unblock Claim 3), and\neach pair (Block, Unblock) occurs in a strict sequence. A3: Receive Objection 2 only\noccurs in the rejected cases and Receive Objection 1 only occurs in the accepted\n10\nAli Norouzifar et al.\ncases after they receive some payments. In general, Receive Objection 1 and Receive\nObjection 2 cannot occur together. ...\nThe set of declarative constraints extracted after answering the questions is:\nnot-co-existence(Block Claim 2, Block Claim 1), not-co-existence(Block Claim 2,\nBlock Claim 3), co-existence(Block Claim 1, Unblock Claim 1), co-existence(Block\nClaim 2, Unblock Claim 2), co-existence(Block Claim 3, Unblock Claim 3), prece-\ndence(Block Claim 1, Unblock Claim 1), precedence(Block Claim 2, Unblock Claim\n2), precedence(Block Claim 3, Unblock Claim 3), not-co-existence(Receive Ob-\njection 1, Receive Objection 2), precedence(Reject Claim, Receive Objection 2),\nprecedence(Payment Order, Receive Objection 1), at-most(Correct Claim), prece-\ndence(Block Claim 1, Correct Claim), precedence(Correct Claim, Unblock Claim 1),\nresponse(Withdraw Claim, Repayment), responded-existence(Accept Claim, Pay-\nment Order), responded-existence(Payment Order, Execute Payment)\nThese rules are validated using a predefined program to ensure that the ac-\ntivity labels are consistent with the event log and that the declarative constraints\nare free of syntax errors. Then, the rules are used as input for the IMr framework\nin addition to the event log, and the process model represented in Fig. 5c is dis-\ncovered. These rules are aligned with the process description and the follow-up\nquestions and answers. For example, the answer to the first question (A1) led\nto the extraction of not-co-existence(Block Claim 2, Block Claim 1) and not-co-\nexistence(Block Claim 2, Block Claim 3), correctly illustrating the relationship\nbetween these activities. These rules help IMr avoid the incorrect positioning of\nBlock Claim 1 observed in Fig. 5b. Another improvement is achieved by consid-\nering not-co-existence(Receive Objection 1, Receive Objection 2), which prevents\nReceive Objection 1 and Receive Objection 2 from occurring in the same trace.\nFig. 5b allows for Block Claim 3 without the existence of Unblock Claim 3. The\nrule co-existence(Block Claim 3, Unblock Claim 3) guides IMr to avoid placing\nUnblock Claim 3 as the re-do part of a loop. We presented this process model\nto domain experts for feedback. They identified some potential issues, which are\nthen provided to ChatGPT to generate a better set of declarative templates.\nThe discovered process model is interesting but we observe some issues. For example,\nReceive Objection 1 should occur after all the payments are executed. This time of\nobjection can occur at most one time per claim. Withdraw Claim can not be followed\nby another payment. Usually, after the payments are executed, the applicant has the\noption to send an objection, withdraw the claim, and repay the received benefits.\nWithdraw Claim only occurs at most once per claim.\nAfter the consideration of the domain expert input, these constraints are\nadded by ChatGPT to the previous set of constraints:\nBridging Domain Knowledge and Process Discovery Using LLMs\n11\nprecedence(Execute Payment, Receive Objection 1), at-most(Receive Objection 1),\nnot-succession(Withdraw Claim, Payment Order), not-succession(Withdraw Claim,\nExecute Payment), at-most(Withdraw Claim)\nThe discovered model using the modified set of constraints is illustrated in\nFig. 5d. While this model differs from the normative model in Fig. 4, it better\nrepresents the actual process compared to the models in Fig. 5a and Fig. 5b,\nwhich were discovered without considering process knowledge. In comparison to\nFig. 5c some improvements are achieved considering the provided feedback. The\nconstraint at-most(Receive Objection 1) prevents Receive Objection 1 from being\nincluded in a loop, and precedence(Execute Payment, Receive Objection 1) en-\nsures it is positioned after Execute Payment. Additionally, Withdraw Claim is no\nlonger in a loop due to the at-most(Withdraw Claim) constraint and is correctly\npositioned after Payment Order and Withdraw Claim because of the constraints\nnot-succession(Withdraw Claim, Payment Order) and not-succession(Withdraw\nClaim, Execute Payment).\n7\nConclusion\nThe integration of process knowledge in the discovery of process models is often\noverlooked in the literature. In this paper, we leveraged advancements in LLMs\nto demonstrate their capabilities in encoding textual domain knowledge into\ncomprehensible rules for process discovery. Our proposed framework not only\nfacilitates the integration of feedback from domain experts but also enables in-\nteractive improvement of process models. Through a comprehensive case study,\nwe demonstrated the effectiveness of our framework in generating process models\nthat better align with process knowledge. While the extracted set of declarative\nconstraints from LLMs shows great promise, there is still room for improvement\nin precision and completeness. Future work focuses on expanding the range of\ndeclarative templates within the IMr framework and developing additional rule\nspecification patterns. Additionally, providing more detailed examples in task\ndefinition steps helps LLMs capture a broader context, further enhancing the\nquality of the extracted constraints.\nReferences\n1. van der Aa, H., Ciccio, C.D., Leopold, H., Reijers, H.A.: Extracting declarative\nprocess models from natural language. In: Advanced Information Systems Engi-\nneering - 31st International Conference, CAiSE 2019, Proceedings. Lecture Notes\nin Computer Science, vol. 11483, pp. 365‚Äì382. Springer (2019)\n2. van der Aa, H., Rebmann, A., Leopold, H.: Natural language-based detection of\nsemantic execution anomalies in event logs. Inf. Syst. 102, 101824 (2021)\n3. Augusto, A., Conforti, R., Dumas, M., Rosa, M.L., Maggi, F.M., Marrella, A.,\nMecella, M., Soo, A.: Automated discovery of process models from event logs:\nReview and benchmark. IEEE Trans. Knowl. Data Eng. 31(4), 686‚Äì705 (2019)\n12\nAli Norouzifar et al.\n4. Dixit, P.M., Verbeek, H.M.W., Buijs, J.C.A.M., van der Aalst, W.M.P.: Interactive\ndata-driven process model construction. In: Conceptual Modeling - 37th Interna-\ntional Conference, ER 2018, Proceedings. Lecture Notes in Computer Science, vol.\n11157, pp. 251‚Äì265. Springer (2018)\n5. van Eck, M.L., Lu, X., Leemans, S.J.J., van der Aalst, W.M.P.: PMÀÜ2 : A process\nmining project methodology. In: Advanced Information Systems Engineering - 27th\nInternational Conference, CAiSE 2015, Proceedings. Lecture Notes in Computer\nScience, vol. 9097, pp. 297‚Äì313. Springer (2015)\n6. Grohs, M., Abb, L., Elsayed, N., Rehse, J.: Large language models can accomplish\nbusiness process management tasks. In: Business Process Management Workshops -\nBPM 2023 International Workshops, 2023, Revised Selected Papers. Lecture Notes\nin Business Information Processing, vol. 492, pp. 453‚Äì465. Springer (2023)\n7. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng,\nW., Feng, X., Qin, B., Liu, T.: A survey on hallucination in large language mod-\nels: Principles, taxonomy, challenges, and open questions. CoRR abs/2311.05232\n(2023)\n8. Klievtsova, N., Benzin, J., Kampik, T., Mangler, J., Rinderle-Ma, S.: Conversa-\ntional process modelling: State of the art, applications, and implications in prac-\ntice. In: Business Process Management Forum - BPM 2023 Forum, Proceedings.\nLecture Notes in Business Information Processing, vol. 490, pp. 319‚Äì336. Springer\n(2023)\n9. Kourani, H., Berti, A., Schuster, D., van der Aalst, W.M.P.: Process modeling with\nlarge language models. In: Enterprise, Business-Process and Information Systems\nModeling - 25th International Conference, BPMDS 2024, and 29th International\nConference, EMMSAD 2024, Proceedings. Lecture Notes in Business Information\nProcessing, vol. 511, pp. 229‚Äì244. Springer (2024)\n10. Leemans, S.J.J., Fahland, D., van der Aalst, W.M.P.: Discovering block-structured\nprocess models from event logs containing infrequent behaviour. In: Business Pro-\ncess Management Workshops - BPM 2013 International Workshops , Revised\nPapers. Lecture Notes in Business Information Processing, vol. 171, pp. 66‚Äì78.\nSpringer, Cham (2013)\n11. Maggi, F.M., Bose, R.P.J.C., van der Aalst, W.M.P.: Efficient discovery of under-\nstandable declarative process models from event logs. In: Advanced Information\nSystems Engineering - 24th International Conference, CAiSE 2012, Proceedings.\nLecture Notes in Computer Science, vol. 7328, pp. 270‚Äì285. Springer (2012)\n12. Norouzifar, A., Dees, M., van der Aalst, W.M.P.: Imposing rules in process discov-\nery: An inductive mining approach. In: Research Challenges in Information Science\n- 18th International Conference, RCIS 2024, Proceedings, Part I. Lecture Notes in\nBusiness Information Processing, vol. 513, pp. 220‚Äì236. Springer (2024)\n13. Polyvyanyy, A., van der Aalst, W.M.P., ter Hofstede, A.H.M., Wynn, M.T.:\nImpact-driven process model repair. ACM Trans. Softw. Eng. Methodol. 25(4),\n28:1‚Äì28:60 (2017)\n14. Schuster, D., van Zelst, S.J., van der Aalst, W.M.P.: Utilizing domain knowledge\nin data-driven process discovery: A literature review. Comput. Ind. 137, 103612\n(2022)\n15. Schuster, D., van Zelst, S.J., van der Aalst, W.M.P.: Cortado: A dedicated process\nmining tool for interactive process discovery. SoftwareX 22, 101373 (2023)\n16. Vidgof, M., Bachhofner, S., Mendling, J.: Large language models for business pro-\ncess management: Opportunities and challenges. In: Business Process Management\nForum - BPM 2023 Forum, 2023, Proceedings. Lecture Notes in Business Informa-\ntion Processing, vol. 490, pp. 107‚Äì123. Springer (2023)\n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2024-08-30",
  "updated": "2024-08-30"
}