{
  "id": "http://arxiv.org/abs/2409.05697v1",
  "title": "Segmentation by Factorization: Unsupervised Semantic Segmentation for Pathology by Factorizing Foundation Model Features",
  "authors": [
    "Jacob Gildenblat",
    "Ofir Hadar"
  ],
  "abstract": "We introduce Segmentation by Factorization (F-SEG), an unsupervised\nsegmentation method for pathology that generates segmentation masks from\npre-trained deep learning models. F-SEG allows the use of pre-trained deep\nneural networks, including recently developed pathology foundation models, for\nsemantic segmentation. It achieves this without requiring additional training\nor finetuning, by factorizing the spatial features extracted by the models into\nsegmentation masks and their associated concept features. We create generic\ntissue phenotypes for H&E images by training clustering models for multiple\nnumbers of clusters on features extracted from several deep learning models on\nThe Cancer Genome Atlas Program (TCGA), and then show how the clusters can be\nused for factorizing corresponding segmentation masks using off-the-shelf deep\nlearning models. Our results show that F-SEG provides robust unsupervised\nsegmentation capabilities for H&E pathology images, and that the segmentation\nquality is greatly improved by utilizing pathology foundation models. We\ndiscuss and propose methods for evaluating the performance of unsupervised\nsegmentation in pathology.",
  "text": "Segmentation by Factorization: Unsupervised Semantic\nSegmentation for Pathology by Factorizing Foundation Model\nFeatures\nJacob Gildenblat∗, Ofir Hadar∗\nDeePathology.ai\nAbstract\nWe introduce Segmentation by Factorization (F-SEG), an unsupervised segmentation method for\npathology that generates segmentation masks from pre-trained deep learning models. F-SEG allows\nthe use of pre-trained deep neural networks, including recently developed pathology foundation mod-\nels, for semantic segmentation. It achieves this without requiring additional training or fine-tuning, by\nfactorizing the spatial features extracted by the models into segmentation masks and their associated\nconcept features. We create generic tissue phenotypes for H&E images by training clustering models\nfor multiple numbers of clusters on features extracted from several deep learning models on TCGA\n[1], and then show how the clusters can be used for factorizing corresponding segmentation masks\nusing off-the-shelf deep learning models. Our results show that F-SEG provides robust unsupervised\nsegmentation capabilities for H&E pathology images, and that the segmentation quality is greatly\nimproved by utilizing pathology foundation models. We discuss and propose methods for evaluating\nthe performance of unsupervised segmentation in pathology.\n1\nIntroduction\nSegmentation models are widely used in applications of digital pathology for focusing on regions of\ninterest, such as detecting tumor regions, or performing a subsequent analysis only inside specific regions,\nor excluding non-cellular areas [2, 3]. These segmentation masks can also serve as predictive features by\nquantifying the area of a specific region or counting cells within that region (e.g., [4]). We explore the use\nof existing pre-trained neural networks for semantic segmentation, without the need to train dedicated\nsegmentation models for new datasets.\nAs research progresses in developing pre-trained models for\npathology, often referred to as foundation models when these are large models trained on large datasets,\nleveraging these off-the-shelf models can reduce barriers to create segmentation models and potentially\nenhance segmentation accuracy as the pre-trained models improve.\nThis capability is useful in various tasks, such as expediting semantic segmentation annotations\nthrough automatic annotations, proposing regions of interests in slides, quantifying objects in differ-\nent region types, or enabling interpretability by examining how model outputs interact with different\nregion types.\nOur contributions are as follows:\n1. We demonstrate that Non-Negative Matrix Factorization (NMF) [5, 6] on top of spatial activations\nextracted from deep neural networks can be used to generate consistent and meaningful semantic\nsegmentation masks for pathology. We propose two ways of achieving this. One is by applying\nNMF on top of the spatial activations to factorize them into a segmentation mask and concept\n∗These authors contributed equally to this work\narXiv:2409.05697v1  [cs.CV]  9 Sep 2024\n2\nfeatures, and then classifying the concept features with a clustering model that can be created on\na different dataset. The second way is to apply NMF where one of the matrices is fixed and set to\nbe the cluster centers, and then solve for a segmentation mask that corresponds to these clusters.\nThe achieved segmentation does not require training deep networks, allows utilizing off-the-shelf\ndeep learning models, and allows segmentation into a varying number of categories depending on\nthe need.\n2. We use this to create a generic unsupervised semantic segmentation method for H&E for a varying\nnumber of categories and for several deep learning models, and benchmark the proposed unsuper-\nvised segmentation method on H&E segmentation tasks to show that the achieved segmentation\ncorrespond with annotated categories in semantic segmentation datasets. Figure 1 illustrates an\nexample of the output segmentation.\n2\nRelated work\nImage segmentation has been a central topic in computer vision research for decades. It is typically\ncategorized into three types: semantic segmentation [7], instance segmentation [8], and panoptic segmen-\ntation [9]. This paper focuses on semantic segmentation, where the objective is to classify each pixel in\nan image into one of several categories. In supervised semantic segmentation, the process involves images\nwith known categories and detailed pixel-wise annotations. Deep learning models like U-Net [10] have\nachieved impressive results in this area and are commonly used across various domains, including medical\nimaging [11].\nIn contrast, unsupervised semantic segmentation does not rely on predefined categories or pixel-wise\nannotations.\nInstead, the goal is to discover representative categories from the data itself and then\ngenerate pixel-wise classifications based on these categories. Previous approaches often involve designing\na specific loss function for unsupervised segmentation and training a deep learning model such as U-Net\nusing this loss. For example, [12] introduces an encoder/decoder framework where the encoder performs\na k-way segmentation into k unknown categories, and the decoder reconstructs the image from features\nproduced by the encoder. They employ a soft normalized cut loss to ensure similarity within the same\ncategory and dissimilarity between different categories.\nOn the other hand, [13] uses a loss function\napplied to the output of a U-Net decoder to ensure that pixels with the same label have similar features,\nensuring spatial continuity while promoting a larger amount of clusters.\nWeakly supervised semantic segmentation, another related approach [14, 15], involves the use of\nweaker labels such as bounding boxes or image-level labels rather than pixel-wise annotations. Methods\nlike Grad-CAM [16] use these weaker labels to generate attribution maps, which are then processed\nto create pseudo-segmentation masks for training purposes. Affinity-Net [17] is one such method that\nleverages these pseudo-masks for training. Our approach shares similarities with this method in that it\nuses activations from a model to create pseudo-segmentation masks, but unlike these methods, we do not\nuse any predefined image labels.\nDeep Feature Factorization [18] applies NMF on the activations of deep neural networks to visualize\nsimilar features in different parts of an image and perform co-segmentation [19, 20] across multiple images.\nUnlike methods that rely on attribution maps, this approach does not require tile labels. Inspired by\nDeep Feature Factorization, our method aims to utilize the features of a pretrained network based solely\non its activations, without the need for annotations.\n3\nMethods\n3.1\nFactorizing spatial features of deep neural networks into concepts and\nsegmentation masks\nFollowing the approach proposed in Deep Feature Factorization [18], we apply NMF on the spatial\nactivations of a pre-trained neural network on an image. The goal is to factorize these activations into a\nconcept feature matrix and a semantic segmentation of the concepts. We start by extracting the spatial\n3\nFigure 1: An example of F-SEG semantic segmentation with the UNI foundation model and k=64 TCGA\nclusters\n4\nFigure 2: Overview of F-SEG when classifying factorized concepts into the most similar cluster\nactivations of a pre-trained network, assumed here to be non negative, resulting in a tensor of shape\nrows × cols × channels. In the case of a vision transformer model, each token in the output origins from\nan input location in the image, so we ignore the class token. We then reshape this tensor into a matrix\nA of shape (rows × cols) × channels and then factorize A with NMF into k components:\nA(rows×cols)×channels ≈W(rows×cols)×k × Hk×channels\nIn this factorization, W represents a per-pixel contribution matrix, indicating the contribution of\nevery pixel to the k concepts. For simplicity, we refer to Wijm as the contribution of the pixel ij to the\nm’th concept. H represents a concept feature matrix, storing the k representative feature vectors of the\nconcepts.\nWe assign each pixel a concept βij by identifying the concept with the largest contribution to that\npixel:\nβij = argmaxmWijm\n3.2\nMethods for achieving consistent concepts between images\nThe factorized concepts in β might have different meanings and ordering in different images. For example,\na factorized concept corresponding to \"Tumor\" might be the first concept in one image, but the third in\nanother. To achieve consistent semantic segmentation across images, we map these concepts to a common\nset of labels. This can be done using any method that assigns feature vectors into categories, for example\nby utilizing existing classification models. We achieve this by fitting a k-means clustering model [21] on\n1D features extracted from neural networks by taking the spatial average of the features from the layer\nof choice in the neural network.\nFinally, at the end of the pipeline, the factorized segmentation mask is resized to match the shape of\nthe input image. We next propose two methods for utilizing the clustering model centers for consistent\nconcepts.\n3.2.1\nAssigning factorized concepts to similar clusters\nFor each pixel i, j the semantic segmentation category ˆyij is selected by taking the cluster center with\nthe highest cosine similarity to the factorized concept feature at that pixel:\nˆ\nyij = argmax\nk\nµk · Hβij\n∥µk∥∥Hβij∥\nWhere µk is the k’th clustering center, and Hβij is the concept feature vector from the concept feature\nmatrix H\nThe pipeline for F-SEG when classifying factorized concepts into the most similar cluster is illustrated\nin figure 2.\n5\nFigure 3: Overview of F-SEG when fixing the concept matrix to be cluster centers\n3.2.2\nSolving for segmentation masks that correspond to cluster centers\nHere instead of applying NMF to solve for both the H and W matrices, we set the matrix H to be equal\nto the cluster centers, and solve only for W. This finds a segmentation mask that corresponds to these\nclusters.\nThe pipeline for F-SEG when fixing the concept matrix is illustrated in figure 3.\n3.3\nUnsupervised semantic segmentation for H&E images\nHere we apply F-SEG for unsupervised semantic segmentation of H&E histology images.\nTo create\nclustering models used by the factorization, we extract image tiles of size 256 × 256 from whole-slide\nimages (WSIs) at a magnification of 10x from the TCGA dataset.\nThe tiles are randomly sampled,\nwith 200 tiles taken per slide from a total of 11,000 slides, covering a wide variety of tissue types and\nmorphologies.\nFor clustering we use k-means with varying values of k (16, 32, 64, 128, and 256) allowing segmentation\ninto different numbers of categories. By adjusting the k-value, we can classify the image regions into\nvarying levels of granularity, allowing the segmentation to capture either broad tissue regions or finer\nanatomical structures, allowing a quick adaptation of the segmentation to different requirements.\nWe compare F-SEG with three deep learning models: the Resnet50 model [22], and two recent foun-\ndation models, the UNI model [23] and the Prov-GigaPath model [24], both of which are based on vision\ntransformers [25].\nThe Resnet50 model serves as our baseline, having been pre-trained on the Ima-\ngeNet dataset, which is commonly used for general image classification tasks. In contrast, the UNI and\nProv-GigaPath models are state-of-the-art vision transformers, designed specifically for large-scale image\nanalysis and particularly suited to extracting complex spatial relationships in digital pathology images.\nFor feature extraction using Resnet50, we observed through early experimentation that better clus-\ntering performance was achieved when using the activations from the second-to-last Resnet block, that\nhas a higher spatial resolution at it’s output, rather than the final block.\nFor the foundation vision\ntransformer models, we use the activations from the final self-attention block. However, since the outputs\nof these vision transformer models can contain negative values, in contrast to the Resnet50 model, which\nincludes a Rectified Linear Unit (ReLU) activation [26] to zero out negative values, we apply a ReLU to\nthese features as a post-processing step. This ensures that there are no negative values in the activations\nfactorized with Non Negative Matrix Factorization. However this is the most simple choice, and other\nways could be potentially explored.\nFinally we apply a Global Average Pooling (GAP) layer on top of the extracted features, and remove\ntheir spatial dimensions, resulting in 1D features suitable for clustering with k-means.\n3.4\nEvaluation Datasets\n3.4.1\nBreast Cancer Semantic Segmentation\nThe Breast Cancer Semantic Segmentation (BCSS) dataset [27] contains over 20,000 segmentation anno-\ntations of tissue regions from breast cancer images sourced from The Cancer Genome Atlas (TCGA). The\ndataset is divided into training and test sets, with pixel-wise category labels for both sets. In the training\n6\nset, the Tumor category constitutes 15.17% of the pixels, the Stroma category 34.89%, the Inflammatory\ncategory 30.37%, the Necrosis category 9.47%, and the Other category 5.86%.\nThe test set shows a different distribution, with the Tumor category making up 33.55% of the pixels,\nthe Stroma category 24.3%, the Inflammatory category 26.24%, the Necrosis category 9.23%, and the\nOther category 3.26%. The variation in pixel distribution between the training and test sets provides a\nrobust basis for evaluating the performance and generalization capabilities of segmentation models.\n3.4.2\nWSSS4LUAD\nThe Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma (WSSS4LUAD) chal-\nlenge [28] aims to perform tissue semantic segmentation in H&E stained Whole Slide Images (WSIs) for\nlung adenocarcinoma.\nFor the validation set, a total of 40 patches were manually cropped by the label review board. This\nset included 9 large patches (approximately 1500 −5000 × 1500 −5000 pixels) and 31 small patches\n(approximately 200 −500 × 200 −500 pixels). For the test set, a total of 80 patches were manually\ncropped, consisting of 14 large patches (approximately 1500 −5000 × 1500 −5000 pixels) and 66 small\npatches (approximately 200 −500 × 200 −500 pixels).\nThe distribution of pixel labels across different categories in the validation set is 42.18% pixels for the\nTumor category, 27.78% pixels for the Stroma category, 26.63% pixels for the Background category, and\n3.39% pixels for the Normal category. In the test set, the distribution is 46.25% pixels for the Tumor\ncategory, 33.63% pixels for the Stroma category, 17.66% pixels for the Background category, and 2.44%\npixels for the Normal category.\nSince the training set is labeled weakly with only image-level annotations, we focused our evaluation\non the validation and test sets, which have precise pixel-level annotations.\n3.5\nEvaluation methods\nIn this section, we outline the evaluation methods used to assess the performance of our methods. We\npropose two evaluation methods for unsupervised segmentation. First, a \"linear probing\" method specific\nto F-SEG that benchmarks the potential of the factorized features and segmentation masks, where a\nlinear classifier on top of the factorized features is used to classify them into ground truth categories,\nevaluating both the factorized features, and the segmentation quality. And a second matching-based\napproach, suitable for unsupervised segmentation methods in general, that benchmarks the unsupervised\nsegmentation by matching the predicted unsupervised categories with the ground truth categories.\n3.5.1\nLinear probing\nIn this evaluation method we replaced the clustering model with a linear classifier to classify concept\nfeatures into specific categories. The classifier is trained on top of factorized features that are found to\nbe co-occuring with ground truth categories.To train the classifier, we followed these steps:\n1. Concept-Category Association: For each tile, we identified concepts obtained with NMF that\nwere highly associated with certain categories. For each concept m, we calculated the percentage of pixels\ncorresponding to that concept which fell into a ground truth category n.\n2. Thresholding: If the percentage of pixels for concept m in category n exceeded a predefined\nthreshold, we assigned category n to that concept. We then recorded the concept feautures Hm along\nwith its label n.\n3. Training the Linear Classifier: We used the generated pairs of concept features and their\ncorresponding labels to train a linear classifier w.\n4. Generating Semantic Segmentation Labels: Finally, we utilized the trained linear classifier\nto classify the concept features, resulting in the semantic segmentation labels ˆy.\nˆ\nyij = argmax\nc=1..n\nwc · Hβij\n7\nFigure 4: Evaluating Segmentation by Factorization on the Breast Cancer Semantic Segmentation Dataset\n3.5.2\nMatching based on cluster-category frequencies\nIn this section, we describe the process of matching cluster indices with ground truth categories. Each\ncluster is associated with a single ground truth category, though a ground truth category can correspond\nto multiple clusters.\n- Matching Clusters to Ground Truth Categories: For each cluster, we calculate the frequency of\npixels from each ground truth category that fall within the cluster. We then match each cluster to the\nground truth category that has the highest frequency of pixels within that cluster.\n- Handling Unbalanced Categories: The distribution of ground truth categories can be uneven, with\nsome categories being very common and others quite rare. To address this imbalance, we provide an\nalternative matching method. In this method, we normalize the frequency of pixels in each cluster by the\ntotal number of pixels in the respective ground truth category. This normalization allows for matching\nclusters with ground truth categories when there is a high unbalance in the number of pixels belonging\nto different ground truth categories.\n4\nResults\nThe results for all models and evaluation methods are shown in Figures 4 and 5. The foundation mod-\nels demonstrate a significant improvement over the baseline Resnet50 model pretrained on ImageNet.\nSpecifically, our experiments show that the foundation models consistently outperform Resnet50 across\nall tested scenarios, achieving better performance in terms of F1 score, and overall robustness. Fixing\nthe concept matrix H leads to improved performance in all settings, compared to performing full NMF\nand classifying the factorized concepts.\n5\nDiscussion\nWe demonstrated how pre-trained deep learning models can be used for unsupervised semantic segmen-\ntation, without being required to further train or fine time them. This method allows leveraging large\n8\nFigure 5: Evaluating Segmentation by Factorization on the WSSS4LUAD Dataset\nfoundation models developed for pathology, and can be used to segment pathology images without any\nperforming any annotations, or without clear definitions of categories. We show that the unsupervised\nsegmentation correspond with meaningful tissue types on semantic datasets datasets.\nReferences\n[1]\nThe Cancer Genome Atlas Research Network. The Cancer Genome Atlas Program. Accessed: 2024-\n09-08. 2013. url: https://www.cancer.gov/tcga.\n[2]\nYan Xu et al. “Deep convolutional activation features for large scale brain tumor histopathology\nimage classification and segmentation”. In: 2015 IEEE international conference on acoustics, speech\nand signal processing (ICASSP). IEEE. 2015, pp. 947–951.\n[3]\nJun Xu et al. “A deep convolutional neural network for segmenting and classifying epithelial and\nstromal regions in histopathological images”. In: Neurocomputing 191 (2016), pp. 214–223.\n[4]\nJames A Diao et al. “Human-interpretable image features derived from densely mapped cancer\npathology slides predict diverse molecular phenotypes”. In: Nature communications 12.1 (2021),\np. 1613.\n[5]\nSuvrit Sra and Inderjit Dhillon. “Generalized Nonnegative Matrix Approximations with Breg-\nman Divergences”. In: Advances in Neural Information Processing Systems. Ed. by Y. Weiss, B.\nSchölkopf, and J. Platt. Vol. 18. MIT Press, 2005. url: https://proceedings.neurips.cc/paper_\nfiles/paper/2005/file/d58e2f077670f4de9cd7963c857f2534-Paper.pdf.\n[6]\nRashish Tandon and Suvrit Sra. “Sparse nonnegative matrix approximation: new formulations and\nalgorithms”. In: (2010).\n[7]\nSaeid Asgari Taghanaki et al. “Deep semantic segmentation of natural and medical images: a review”.\nIn: Artificial Intelligence Review 54 (2021), pp. 137–178.\n[8]\nShervin Minaee et al. “Image segmentation using deep learning: A survey”. In: IEEE transactions\non pattern analysis and machine intelligence 44.7 (2021), pp. 3523–3542.\n9\n[9]\nAlexander Kirillov et al. “Panoptic segmentation”. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. 2019, pp. 9404–9413.\n[10]\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedi-\ncal image segmentation”. In: Medical image computing and computer-assisted intervention–MICCAI\n2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18.\nSpringer. 2015, pp. 234–241.\n[11]\nReza Azad et al. “Medical image segmentation review: The success of u-net”. In: arXiv preprint\narXiv:2211.14830 (2022).\n[12]\nBrian Kulis Xide Xia. “W-Net: A Deep Model for Fully Unsupervised Image Segmentation”. In:\n2017.\n[13]\nAsako Kanezaki Wonjik Kim and Masayuki Tanaka. “Unsupervised Learning of Image Segmentation\nBased on Differentiable Feature Clustering”. In: IEEE TRANSACTIONS ON IMAGE PROCESS-\nING. 2020.\n[14]\nYunchao Wei et al. “Stc: A simple to complex framework for weakly-supervised semantic segmenta-\ntion”. In: IEEE transactions on pattern analysis and machine intelligence 39.11 (2016), pp. 2314–\n2320.\n[15]\nZilong Huang et al. “Weakly-supervised semantic segmentation network with deep seeded region\ngrowing”. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018,\npp. 7014–7023.\n[16]\nRamprasaath R Selvaraju et al. Grad-CAM: Why did you say that? 2016. eprint: arXiv:1611.07450.\n[17]\nJiwoon Ahn and Suha Kwak. “Learning Pixel-Level Semantic Affinity With Image-Level Supervision\nfor Weakly Supervised Semantic Segmentation”. In: The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR). June 2018.\n[18]\nEdo Collins, Radhakrishna Achanta, and Sabine Susstrunk. “Deep feature factorization for concept\ndiscovery”. In: Proceedings of the European Conference on Computer Vision (ECCV). 2018, pp. 336–\n352.\n[19]\nCarsten Rother et al. “Cosegmentation of image pairs by histogram matching-incorporating a global\nconstraint into mrfs”. In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’06). Vol. 1. IEEE. 2006, pp. 993–1000.\n[20]\nLopamudra Mukherjee, Vikas Singh, and Charles R Dyer. “Half-integrality based algorithms for\ncosegmentation of images”. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition.\nIEEE. 2009, pp. 2028–2035.\n[21]\nJohn A Hartigan and Manchek A Wong. “Algorithm AS 136: A k-means clustering algorithm”. In:\nJournal of the royal statistical society. series c (applied statistics) 28.1 (1979), pp. 100–108.\n[22]\nKaiming He et al. “Deep residual learning for image recognition”. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. 2016, pp. 770–778.\n[23]\nRichard J Chen et al. “Towards a General-Purpose Foundation Model for Computational Pathol-\nogy”. In: Nature Medicine (2024).\n[24]\nHanwen Xu et al. “A whole-slide foundation model for digital pathology from real-world data”. In:\nNature (2024).\n[25]\nAlexey Dosovitskiy et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale”. In: ICLR (2021).\n[26]\nAbien Fred Agarap. “Deep learning using rectified linear units (relu)”. In: arXiv preprint arXiv:1803.08375\n(2018).\n[27]\nMohamed Amgad et al. “Structured crowdsourcing enables convolutional segmentation of histology\nimages”. In: Bioinformatics 35.18 (2019), pp. 3461–3467.\n[28]\nSimon Graham et al. “Lizard: A large-scale dataset for colonic nuclear instance segmentation and\nclassification”. In: Proceedings of the IEEE/CVF international conference on computer vision. 2021,\npp. 684–693.\n10\nA\nExamples of F-SEG unsupervised semantic segmentation\nFigure 6: An example of F-SEG semantic segmentation with the Prov-GigaPath foundation model and\nk=16 TCGA clusters\n11\nFigure 7: An example of F-SEG semantic segmentation with the Prov-GigaPath foundation model and\nk=16 TCGA clusters\nFigure 8: An example of F-SEG semantic segmentation with the Prov-GigaPath foundation model and\nk=64 TCGA clusters\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2024-09-09",
  "updated": "2024-09-09"
}