{
  "id": "http://arxiv.org/abs/2409.05771v1",
  "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
  "authors": [
    "Emily Cheng",
    "Richard J. Antonello"
  ],
  "abstract": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.",
  "text": "Evidence from fMRI Supports a Two-Phase\nAbstraction Process in Language Models\nEmily Cheng∗\nUniversitat Pompeu Fabra\nBarcelona, Spain\nemilyshana.cheng@upf.edu\nRichard J. Antonello∗\nColumbia University\nNew York City, New York\nrja2163@columbia.edu\nAbstract\nResearch has repeatedly demonstrated that intermediate hidden states extracted\nfrom large language models are able to predict measured brain response to natural\nlanguage stimuli. Yet, very little is known about the representation properties that\nenable this high prediction performance. Why is it the intermediate layers, and not\nthe output layers, that are most capable for this unique and highly general transfer\ntask? In this work, we show that evidence from language encoding models in fMRI\nsupports the existence of a two-phase abstraction process within LLMs. We use\nmanifold learning methods to show that this abstraction process naturally arises\nover the course of training a language model and that the first \"composition\" phase\nof this abstraction process is compressed into fewer layers as training continues.\nFinally, we demonstrate a strong correspondence between layerwise encoding\nperformance and the intrinsic dimensionality of representations from LLMs. We\ngive initial evidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.\n1\nIntroduction\nHow do brains and machines take low-level information, such as a collection of sounds or words, and\ncompose it into the rich tapestry of ideas and concepts that can be expressed in natural language?\nThis question of composition, or abstraction, is at the heart of most studies of human language\ncomprehension. Recent work has shown that representations from large language models (LLMs)\nare able to successfully model human brain activity at varying spatial and temporal resolutions with\nonly a linear transformation [1–7]. This has led to questions about the reason for this brain-model\nsimilarity. Do LLMs and brains possess similar representations because they have similar learning\nproperties or objectives? [8, 9, 1] Or is the similarity merely a consequence of shared abstraction, the\nability to represent features not derivable from the lexical properties of language alone? [10]\nIn this work, we present new evidence that it is the abstractive, compositional properties of LLMs\nthat drive predictivity between LLMs and brains. We do this by examining an underexplored and\nunexplained phenomenon of the similarity - the tendency for intermediate hidden layers of LLMs\nto be optimal for this linear transfer task. We show that an LLM layer’s performance at predicting\nbrain activity is strongly related to intrinsic dimensionality of that layer relative to other layers in the\nsame network. Furthermore, we demonstrate that this relationship is itself an indicator that pretrained\nLLMs naturally split into an early abstraction, or composition, phase, and a later prediction, or\nextraction, phase, a result independently suggested in the LM interpretability literature [11, 12]. We\nsuggest that it is the first abstraction phase, rather than the latter prediction phase, that primarily\ndrives the observed correspondence between brains and LLMs.\n*These authors contributed equally to this work.\nPreprint. Under review.\narXiv:2409.05771v1  [cs.CL]  9 Sep 2024\n2\nMethods\nWe test the hypothesis that feature abstraction, not next-token prediction per se, drives brain-model\nsimilarity. To do so requires three observables. First, we measure the dependent variable, (1)\nbrain-model representational similarity, by scoring the prediction performance of a learned linear\nmapping from LLM representations to brain activity. Then, we compute the (2) dimensionality of\nrepresentations to measure abstract feature complexity over the LM’s layers. Finally, to test the\nalternate hypothesis that next-token prediction drives brain-LM similarity, as has been suggested by\nothers [13, 8, 1], we compute the (3) surprisal, or next-token prediction error, from each layer. In\ncontrast to prior surprisal measurement approaches [10], we compute this layerwise surprisal using\nthe TunedLens approach devised by Belrose et al. [14] to reduce measurement noise.\n2.1\nBrain-model similarity\nfMRI data\nWe used publicly available functional magnetic resonance imaging (fMRI) data col-\nlected from 3 human subjects as they listened to 20 hours of English language podcast stories over\nSensimetrics S14 headphones. Stories came from podcasts such as The Moth Radio Hour, Modern\nLove, and The Anthropocene Reviewed. Each 10-15 minute story was played during a separate scan.\nSubjects were not asked to make any responses, but simply to listen attentively to the stories. For\nencoding model training, each subject listened to roughly 95 different stories, giving 20 hours of data\nacross 20 scanning sessions, or a total of ~33,000 datapoints for each voxel across the whole brain.\nAdditional details of the MRI methods are summarized in Appendix D.\nNeural encoding model training\nTo train encoding models, we use the method described in [4].\nFor each word in the stimulus set, activations were extracted by feeding that word and its immediate\npreceding context into the LLM. A sliding window was used to ensure each word received a minimum\nof 256 tokens of context. Activations were then downsampled using a Lanczos filter and FIR delays\nof 1,2,3 and 4 TRs were added to account for the hemodynamic lag in the BOLD signal. A linear\nprojection from the downsampled, time-delayed features was trained using ridge regression. Encoding\nmodels were built using the OPT language model [15] (three sizes - 125M, 1.3B, 13B) and the 6.9B\nparameter deduped Pythia language model [16]. To study model training, 9 different Pythia model\ncheckpoints were used (at 1K, 2K, 3K, 4K, 8K, 16K, 32K, 64K, and 143K training steps).\n2.2\nDimensionality of neural manifolds\nTo measure the relationship between encoding performance and representational complexity, we\ncompute the intrinsic dimensionality Id as well as the linear effective dimensionality d of activations\nat each layer. Id and d describe different geometric properties of the representations: while the former\ndescribes the dimension of the representations’ underlying (nonlinear) manifold, the latter describes\nthe number of linear directions that explain their variance up to a threshold. When unambiguous, we\nwill use the word dimensionality to refer to both Id and d, specifying when necessary.\nWe are interested in an LM’s behavior on a representative sample of natural language, so that the\ncomputed dimensionality is informative about the LM’s linguistic processing in general. For all LMs\nmentioned in the previous section, we compute the ID on N = 10000 20-word contexts randomly\nsampled from The Pile [17], which constitute Pythia’s training data,1 for 5 random data samples.\nEach sequence is first transformed into a sequence of tokens, which are the atomic (subword) units\nthat constitute the LM’s vocabulary. Then, as the tokenization scheme may result in sequences of\nvariable length, we aggregate representations at each layer by taking that of the last token in the\nmodel’s residual stream [18]; this yields one N × D matrix of representations per layer, D being the\nmodel’s hidden dimension, or extrinsic dimension.\nNonlinear ID estimation\nTo compute Id, we apply the Generalized Ratios Intrinsic Dimension\nEstimator (GRIDE) [19], an extension of the popular TwoNN estimator [20] to general scales. GRIDE\noperates on ratios µi,2k,k := ri,2k/ri,k, where ri,j is the Euclidean distance between point i and its\njth neighbor. Assuming local uniform density up to the 2kth neighbor, the ratios µi,2k,k follow a\n1The training data for OPT are not publicly downloadable.\n2\nID\nEnc. Perf.\nComposition\nPrediction\nCorrelation with ID\nSimilarity\nNormlized ID\nLayer Num.\nLayer Num.\nRescaled EP/ID\nSurprisal\na\nb\nc\nd\nAvg. Encoding Perf.\nLayer Num.\nSurprisal\nFigure 1: Analyzing Layerwise Representational Trends: (a) Id is well correlated with encoding\nperformance across model sizes. Id is normalized here by the log of embedding size to account\nfor power law scaling. (b) The abstract-predict phase transition at layer 17 is shown for OPT-1.3b.\nAt the peak of encoding performance (red dashed line), the next-token prediction loss (blue curve)\nsharply decreases, corresponding with a decrease in encoding performance. (c) A flatmap of the\nbrain, for one subject, is shown colored voxelwise by the correlation over layers between Id and\nencoding performance. With the exception of auditory cortex (bright), which captures low-level\nspectral information, encoding performance in brain regions thought to perform higher-level linguistic\nprocessing (dark) is well-captured by representational Id.(d) The layer-wise representational similarity\ncomputed with linear CKA is shown for OPT-1.3B.\ngeneralized Pareto distribution fµi,2k,k(µ) =\nId(µId−1)k−1\nB(k,k)µId(2k−1)+1 , where B(·, ·) is the beta function.\nThe Id is then recovered by maximizing this likelihood over points i for several candidate scales k.\nFinally, in order to choose the proper Id, a scale analysis over k, which controls the neighborhood\nsize, is necessary: if k is too small, the Id likely describes local noise, and if k is too large, the\ncurvature of the manifold will produce a faulty estimate. Instead, it is recommended to choose a k for\nwhich the Id is stable [19]. We provide an example of such a scale analysis in Appendix B.\nLinear dimensionality estimation\nIn addition to nonlinear Id, we computed linear effective\ndimensionality d two ways: using PCA with variance cutoff 0.99 [21], and the Participation Ratio\n(PR), defined as (P\ni λi)2/(P\ni λ2\ni ) [22]. By definition, Id-dimensional manifolds can be embedded\nin d ≥Id dimensions, so we expect that d ≥Id.\n2.3\nMeasuring layerwise surprisal\nCausal LLMs are trained to predict the next token in context. The LLM produces a conditional\nprobability distribution pLM(·|x<i) over the next token xi given a linguistic context x<i. The loss is\ngiven by the negative log-likelihood of the ground-truth token under pLM, equivalent to minimizing\nthe information-theoretic coding length, or surprisal, of the next token given context.\n3\nOPT-125m\nOPT-1.3b\nOPT-13b\nPythia-6.9b\nGRIDE Id\n0.91\n0.96\n0.85\n0.90\nPCA d\n0.91\n0.93\n0.96\n0.86\nPR d\n0.94\n0.82\n0.85\n−0.05∗\nTable 1: The average voxelwise product-moment correlations between representational dimensionality\nand encoding performance are shown for Id, PCA-d (variance threshold of 0.99), and PR-d. Across\nmodels, the correlation is generally high no matter the dimensionality measure. All values, except\nthose marked with (*), are significant to p < 10−3, as computed by a permutation test.\nThis predictive coding objective is hypothesized to underlie sentence processing in humans, and\nhas been proposed to explain brain-LLM representational similarity. To determine whether this is\nthe case, we computed the next-token surprisal from intermediate layers using TunedLens [14] on\nThe Pile dataset. TunedLens learns an affine mapping from an intermediate layer to the vocabulary\nspace in order to predict the next token, indicating how much intermediate layers (linearly) represent\nnext-token identity. See Appendix C for implementation details.\n3\nResults\nLayerwise encoding performance and layerwise representational dimensionality across layers are\nhighly correlated, consistently across brain areas involved in linguistic processing. Table 1 shows\nthe correlation between encoding performance and dimensionality, averaged over all voxels. The\nrelationship is largely consistent across a variety of metrics for measuring dimensionality.\nFigure 1a shows the correlation between average encoding performance and normalized Id for various\nmodel sizes in from the OPT model family. The positive relationship, ρ = 0.85, between normalized\nId and encoding performance suggests that in trained language models, the Id of layer activations\ncaptures abstract linguistic feature complexity needed to support language comprehension.\nFigure 1b overlays, for OPT-1.3b, the encoding performance, Id, and next-token prediction loss\ncomputed from each layer. Observe that encoding performance peaks at layer 17, which exactly\nmarks the sharp downwards turn in prediction loss. While Cheng et al. [12] show that layers leading\nup to the Id peak extract high-level features related to syntax and semantics, our results additionally\nindicate a shift in post-Id-peak to next-token prediction. This sharp phase transition from abstraction\nto prediction is observed across model sizes, but it is more gradual for Pythia (see Appendix E.1).\nTo further verify the existence of a phase transition, we report the inter-layer representational\nsimilarity via linear Centered Kernel Alignment [23]. Figure 1d depicts at least two phases of\ninter-layer similarity (lighter is more similar): the Id peak approximately marks a junction at which\npreceding layers are no longer similar to following layers. Results generally hold across models, see\nAppendix E.3.\nFigure 1c shows the correlation of Id with encoding performance across layers at the voxelwise level\n(red is better), in a single subject. With the exception of the primary auditory cortex, which processes\nlow-level auditory information, encoding performance in brain areas thought to handle higher-level\nlinguistic processing is well-predicted by Id across layers. Results generally hold across subjects,\nmodel families, and model sizes, see Appendix E.2.\nThe relationship between encoding performance and Id arises nontrivially from learning, in a way\nthat does not simply reflect the layer position. Figure 2 plots the encoding performance and Id\nacross layers over the course of training for Pythia-6.9B (each curve is a different checkpoint). We\nconfirm an existing result from the literature that the characteristic Id peak emerges, and moreover,\nthat Id generally grows for all layers over training (Figure 2 right) [12]. Furthermore, the encoding\nperformance (left) and Id (right) increase at similar rates over training, seen by similar positions of\nthe checkpoint curves in the two plots. The two plots are globally correlated with ρ = 0.94.\nThe location of the Id peak (red dots, right), changes over the course of training, eventually settling\nat the same layers for peak encoding performance (red dots, left). This rules out the possibility that\nthe Id peak is a trivial function of the Transformer architecture, e.g., layer index.\n4\nFigure 2: Encoding Performance and Intrinsic Dimensionality Peaks Manifest Concurrently over\nTraining: (a) - The evolution of layerwise encoding performance over training of the Pythia 6.9B\nmodel is shown. A peak is reached at layer 13 of the model. (b) -Likewise, a peak in Id at layer 13\nmanifests over training. Red dots in each figure denote maximal layers for the respective metric.\n4\nDiscussion\nRecent studies of the properties of language encoding models have observed that the intermediate\nlayers of LLMs, rather than the output layers, have the highest linear similarity to measured brain\nactivity. This is true regardless of the scanning modality (be it fMRI [4], ECoG [24], or MEG [8]),\nand regardless of the chosen LLM. Despite this very frequently observed trend, little research has\nbeen dedicated to explaining this phenomenon. Yet, an understanding of this trend would greatly\nbenefit our understanding of both brains and LLMs, not least because layerwise differences in LLMs\nhave highly useful epistemic properties. LLM layers are invariant to many confounding variables -\neach layer has seen the same data in the same order, has an identical architecture, was trained on the\nsame loss term, and built using the same hyperparameters. Therefore, differences between layers can\nonly arise either as a result of the compositional nature of the transition from earlier layers to later\nones, or due to the \"time pressure\" exerted by the loss term on the final output layers.\nThese competing pressures, to first build up the most comprehensive representation of the input text\npossible, and to then ultimately use this representation to resolve towards a distribution over predicted\nnext word outputs, have opposite effects, as we demonstrate here. The composition effect leads to\na increase in encoding performance and dimensionality, whereas the prediction effect narrows the\ndimensionality to the detriment of encoding. Furthermore, we observe that as models get larger and\nmore thoroughly trained, the best layer for encoding slowly drifts to earlier in the model, perhaps\nsuggesting a saturation effect for this initial compositional phase.\nWhat conclusions should we draw from this? Firstly, that it is not likely to be the autoregressive\nnature of language models that drives brain-model similarity [9, 1, 10]. As models get more potent\nat prediction, their most predictive and most descriptive layers drift apart.2 Secondly, we can draw\nthat the multi-phase abstraction process in LLMs that has been proposed independently by other\nauthors [12, 11] is supported by evidence from the only other system known to effectively reason\nwith complex language, the human brain. As the present work only tests two model families, it will\nbe necessary to test more models for conclusions to hold in the general case.\nFrom a practical perspective, conclusions point to a potential new avenue for improving the perfor-\nmance of encoding models. If the spectral properties of different LLM layers can be measured and\nefficiently combined to produce a representation with higher Id than any individual layer, then we\nmight expect that new representation to outperform any single layerwise encoding model coming\nfrom the same LLM. As linear layerwise encoding models reach their limit, such methods may be\nnecessary to see further benefits.\n2We note that it is inherently challenging to draw affirmative conclusions about underlying mechanisms\nin the brain from from brain-model similarity alone. See Guest and Martin [25], Antonello and Huth [10] for\nfurther details.\n5\nAcknowledgements\nThis project has received funding from the European Research Council (ERC) under the European\nUnion’s Horizon 2020 research and innovation programme (grant agreement No. 101019291) as\nwell as the Dingwall Foundation and a computing gift from the Texas Advanced Computing Center\n(TACC) at The University of Texas at Austin. This paper reflects the authors’ view only, and the\nfunding agency is not responsible for any use that may be made of the information it contains.\nThe authors would like to thank Alexander Huth, Marco Baroni, Alessandro Laio, and members\nof the COLT group at Universitat Pompeu Fabra for feedback. The authors additionally thank the\norganizers of the Brains, Minds, and Machines summer course, where the project was started.\nReferences\n[1] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey,\nSamuel A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational\nprinciples for language processing in humans and deep language models. Nature neuroscience,\n25(3):369–380, 2022.\n[2] Aditya R Vaidya, Shailee Jain, and Alexander G Huth. Self-supervised models of audio\neffectively explain human cortical responses to speech. arXiv preprint arXiv:2205.14252, 2022.\n[3] Shailee Jain, Vy A Vo, Leila Wehbe, and Alexander G Huth. Computational language modeling\nand the promise of in silico experimentation. Neurobiology of Language, pages 1–65, 2023.\n[4] Richard Antonello, Aditya Vaidya, and Alexander Huth. Scaling laws for language encoding\nmodels in fmri. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,\neditors, Advances in Neural Information Processing Systems, volume 36, pages 21895–21907.\nCurran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/\npaper/2023/file/4533e4a352440a32558c1c227602c323-Paper-Conference.pdf.\n[5] Greta Tuckute, Aalok Sathe, Shashank Srikant, Maya Taliaferro, Mingye Wang, Martin\nSchrimpf, Kendrick Kay, and Evelina Fedorenko. Driving and suppressing the human language\nnetwork using large language models. bioRxiv, 2023.\n[6] SUBBAREDDY OOTA, Manish Gupta, and Mariya Toneva.\nJoint processing of lin-\nguistic properties in brains and language models.\nIn A. Oh, T. Naumann, A. Glober-\nson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Informa-\ntion Processing Systems, volume 36, pages 18001–18014. Curran Associates, Inc.,\n2023.\nURL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n3a0e2de215bd17c39ad08ba1d16c1b12-Paper-Conference.pdf.\n[7] Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D Mehta, and Nima Mesgarani.\nContextual feature extraction hierarchies converge in large language models and the brain. arXiv\npreprint arXiv:2401.17671, 2024.\n[8] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Rémi King. Evidence of a predictive\ncoding hierarchy in the human brain listening to speech. Nature Human Behaviour, pages 1–12,\n2023.\n[9] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language:\nIntegrative modeling converges on predictive processing. Proceedings of the National Academy\nof Sciences, 118(45):e2105646118, 2021.\n[10] Richard Antonello and Alexander Huth. Predictive coding or just feature discovery? an\nalternative account of why language models fit brain data. Neurobiology of Language, pages\n1–16, 2022.\n[11] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and\nAlberto Cazzaniga. The geometry of hidden representations of large transformer models. In\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in\n6\nNeural Information Processing Systems, volume 36, pages 51234–51252. Curran Associates,\nInc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\na0e66093d7168b40246af1cddc025daa-Paper-Conference.pdf.\n[12] Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Jade Yu, Alessandro Laio, and\nMarco Baroni. Emergence of a high-dimensional abstraction phase in language transformers,\n2024. URL https://arxiv.org/abs/2405.15471.\n[13] Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa,\nKohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et al. Brain-score:\nWhich artificial neural network for object recognition is most brain-like? BioRxiv, page 407007,\n2018.\n[14] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor V. Ostrovsky, Lev McKinney,\nStella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with\nthe tuned lens. ArXiv, abs/2303.08112, 2023. URL https://api.semanticscholar.org/\nCorpusID:257504984.\n[15] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and\nLuke Zettlemoyer. Opt: Open pre-trained transformer language models. (arXiv:2205.01068),\nJun 2022. doi: 10.48550/arXiv.2205.01068. URL http://arxiv.org/abs/2205.01068.\narXiv:2205.01068 [cs].\n[16] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling. In\nInternational Conference on Machine Learning, pages 2397–2430. PMLR, 2023.\n[17] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse\ntext for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n[18] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\nGanguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\nKamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\nChris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n2021. https://transformer-circuits.pub/2021/framework/index.html.\n[19] Francesco Denti, Diego Doimo, Alessandro Laio, and Antonietta Mira. The generalized ratios\nintrinsic dimension estimator. Scientific Reports, 12(11):20005, Nov 2022. ISSN 2045-2322.\ndoi: 10.1038/s41598-022-20991-1.\n[20] Elena Facco, Maria d’Errico, Alex Rodriguez, and Alessandro Laio. Estimating the intrinsic\ndimension of datasets by a minimal neighborhood information. Scientific Reports, 7(1):12140,\nSep 2017. ISSN 2045-2322. doi: 10.1038/s41598-017-11873-y.\n[21] Ian Jolliffe. Principal Component Analysis. Springer, 1986.\n[22] Peiran Gao, Eric M. Trautmann, Byron M. Yu, Gopal Santhanam, Stephen I. Ryu, Krishna V.\nShenoy, and Surya Ganguli. A theory of multineuronal dimensionality, dynamics and measure-\nment. bioRxiv, 2017. URL https://api.semanticscholar.org/CorpusID:19938440.\n[23] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural\nnetwork representations revisited. In International Conference on Machine Learning, pages\n3519–3529. PMLR, 2019.\n[24] Zhuoqiao Hong, Haocheng Wang, Zaid Zada, Harshvardhan Gazula, David Turner, Bobbi\nAubrey, Leonard Niekerken, Werner Doyle, Sasha Devore, Patricia Dugan, et al. Scale matters:\nLarge language models with billions (rather than millions) of parameters better match neural\nrepresentations of natural language. bioRxiv, pages 2024–06, 2024.\n7\nModel\nGRIDE k\nOPT-125m\n64\nOPT-1.3b\n32\nOPT-13b\n32\nPythia-6.9B\n16\nPythia (t =64000)\n16\nPythia (t =32000)\n32\nPythia (t =16000)\n32\nPythia (t =8000)\n32\nPythia (t =4000)\n64\nPythia (t =3000)\n64\nPythia (t =2000)\n16\nPythia (t =1000)\n16\nPythia (t =512)\n16\nTable B.1: Selected GRIDE scales k after performing a scale analysis for intrinsic dimension\nestimation, for all models and checkpoints tested.\n[25] Olivia Guest and Andrea E Martin. On logical inference over brains, behaviour, and artificial\nneural networks, 10 2021. URL psyarxiv.com/tbmcg.\n[26] Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscope:\nA unifying framework for inspecting hidden representations of language models. arXiv preprint\narXiv:2401.06102, 2024.\n[27] Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai, Bhavin Gupta, Allyson\nMorgenthal, Jerry Tang, Lixiang Xu, and Alexander G Huth. A natural language fmri dataset\nfor voxelwise encoding models. bioRxiv, pages 2022–09, 2022.\nA\nComputing resources\nDimensionality and surprisal computation were run on a cluster with 12 nodes with 5 NVIDIA A30\nGPUs and 48 CPUs each. Extracting and computing dimensionality on LM representations took a\nfew wall-clock hours per model. Training TunedLens took around 15 minutes per layer, so overall\n30 wall-clock hours. We parallelized all computation, and estimate the overall parallelized runtime,\nincluding preliminary experiments and failed runs to be around 6 days.\nRidge regression was performed using compute nodes with 128 cores (2 AMD EPYC 7763 64-core\nprocessors) and 256GB of RAM. In total, roughly 1,000 node-hours of compute was expended for\nthese models. Feature extraction for language models was performed on specialized GPU nodes\nsimilar to the AMD compute nodes but with 3 NVIDIA A100 40GB cards. Feature extraction\nrequired roughly 300 node-hours of compute on these GPU nodes.\nB\nID Estimation\nFor ID estimation using GRIDE, we reproduce the setup in Cheng et al. [12]. For each model,\ncheckpoint, and layer, we perform a scale analysis. The intrinsic dimension of the manifold is\nsensitive to the scale, or neighborhood size, for which it is estimated [20, 19]. Figure B.1 shows\nan example, where the GRIDE scale k varies from 20 to 212. As recommended in Denti et al. [19],\nwe choose a scale k corresponding in a range where the intrinsic dimension is stable, or plateaus,\nby visual inspection. For simplicity, we choose one scale k per model. In the particular example in\nFigure B.1, we choose k = 24, where the derivative of the curve is closest to 0 for as many layers as\npossible. Scales chosen for all models are in Table B.1.\n8\n0\n2\n4\n6\n8\n10\n12\nGRIDE Neighbor Range (log)\n10\n20\n30\n40\n50\nIntrinsic Dimension\nGRIDE Scale Analysis (pythia)\nLayer\n1\n4\n7\n10\n13\n16\n19\n22\n25\n28\n31\nFigure B.1: GRIDE scale analysis for Pythia-6.9b. The estimated intrinsic dimension (y axis) varies\naccording to the chosen scale k (x axis). It is recommended to choose a scale where the local change\nis minimal, in this case, k = 24.\nC\nSurprisal Estimation\nWe used the TunedLens implementation by Ghandeharioun et al. [26]. TunedLens ascertains the\namount of information (linearly) encoded in hidden layer t about the next token. To do so, an affine\nmapping is learned from the last-token hidden representation ht at layer t as follows:\nmin\nAt,bt DKL(f>t(ht) || LayerNorm(Atht + bt)WU).\n(C.1)\nHere, At ∈RD×D, bt ∈RD are the learnable parameters of the affine mapping. WU is the LM’s\nunembedding matrix that maps the final layer to the vocabulary. Finally, f>t(ht) is the layers of the\nLM f after layer t, producing the model’s original distribution over the vocabulary. In the provided\ncode [26], TunedLens is implemented using a direct solver numpy.linalg.lstsq on N = 8000\nrandomly sampled sequences from The Pile dataset [17], returning the least squares solution that\nminimizes the l2-norm between ht and last layer representation hT . Finally, we compute the next-\ntoken surprisals on a validation set of The Pile (N = 2000) from the TunedLens-modified hidden\nlayers.\nD\nfMRI Methods\nMRI data were collected on a 3T Siemens Skyra scanner at The University of Texas at Austin\nBiomedical Imaging Center using a 64-channel Siemens volume coil. Functional scans were collected\nusing a gradient echo EPI sequence with repetition time (TR) = 2.00 s, echo time (TE) = 30.8 ms,\nflip angle = 71°, multi-band factor (simultaneous multi-slice) = 2, voxel size = 2.6mm x 2.6mm x\n2.6mm (slice thickness = 2.6mm), matrix size = 84x84, and field of view = 220 mm. Anatomical data\nwere collected using a T1-weighted multi-echo MP-RAGE sequence with voxel size = 1mm x 1mm x\n1mm.\nIn addition to motion correction and coregistration [27], low frequency voxel response drift was\nidentified using a 2nd order Savitzky-Golay filter with a 120 second window and then subtracted\nfrom the signal. The mean response for each voxel was subtracted and the remaining response was\nscaled to have unit variance.\n9\nE\nExtended Results\nE.1\nExtended Tuned Lens Results\nPercent Layer Depth\nSurprisal\nFigure E.1: Remaining tuned lens results for OPT-125, OPT-13B, and Pythia-6.9B\n10\nE.2\nExtended Voxelwise ID Correlation Results\nCorrelation with ID\nFigure E.2: Voxelwise ID correlation results as in Figure 1c for OPT-125M\nCorrelation with ID\nFigure E.3: Voxelwise ID correlation results as in Figure 1c for OPT-13B\n11\nCorrelation with ID\nFigure E.4: Voxelwise ID correlation results as in Figure 1c for Pythia-6.9B\n12\nE.3\nExtended CKA Results\nLayer Num.\nLayer Num.\nSimilarity\nFigure E.5: CKA results as in Figure 1d for OPT-125M\nLayer Num.\nLayer Num.\nSimilarity\nFigure E.6: CKA results as in Figure 1d for OPT-13B\n13\nLayer Num.\nLayer Num.\nSimilarity\nFigure E.7: CKA results as in Figure 1d for Pythia-6.9B\n14\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-09-09",
  "updated": "2024-09-09"
}