{
  "id": "http://arxiv.org/abs/2409.05846v1",
  "title": "An Introduction to Quantum Reinforcement Learning (QRL)",
  "authors": [
    "Samuel Yen-Chi Chen"
  ],
  "abstract": "Recent advancements in quantum computing (QC) and machine learning (ML) have\nsparked considerable interest in the integration of these two cutting-edge\nfields. Among the various ML techniques, reinforcement learning (RL) stands out\nfor its ability to address complex sequential decision-making problems. RL has\nalready demonstrated substantial success in the classical ML community. Now,\nthe emerging field of Quantum Reinforcement Learning (QRL) seeks to enhance RL\nalgorithms by incorporating principles from quantum computing. This paper\noffers an introduction to this exciting area for the broader AI and ML\ncommunity.",
  "text": "An Introduction to Quantum Reinforcement\nLearning (QRL)\nSamuel Yen-Chi Chen\nWells Fargo\nNew York, NY, USA\nyen-chi.chen@wellsfargo.com\nAbstract—Recent advancements in quantum computing (QC)\nand machine learning (ML) have sparked considerable interest\nin the integration of these two cutting-edge fields. Among the\nvarious ML techniques, reinforcement learning (RL) stands out\nfor its ability to address complex sequential decision-making\nproblems. RL has already demonstrated substantial success in\nthe classical ML community. Now, the emerging field of Quantum\nReinforcement Learning (QRL) seeks to enhance RL algorithms\nby incorporating principles from quantum computing. This paper\noffers an introduction to this exciting area for the broader AI\nand ML community.\nIndex Terms—Quantum neural networks, Quantum machine\nlearning, Variational quantum circuits, Quantum reinforcement\nlearning, Quantum artificial intelligence\nI. INTRODUCTION\nQuantum computing (QC) offers the potential for substantial\ncomputational advantages in specific problems compared to\nclassical computers [1]. Despite the current limitations of\nquantum devices, such as noise and imperfections, significant\nefforts are being made to achieve quantum advantages. One\nprominent area of focus is quantum machine learning (QML),\nwhich leverages quantum computing principles to enhance\nmachine learning tasks. Most QML algorithms rely on a hybrid\nquantum-classical paradigm, which divides the computational\ntask into two components: quantum computers handle the\nparts that benefit from quantum computation, while classical\ncomputers process the parts they excel at.\nVariational quantum algorithms (VQAs) [2] form the foun-\ndation of current quantum machine learning (QML) ap-\nproaches. QML has demonstrated success in various ma-\nchine learning tasks, including classification [3]–[6], sequen-\ntial learning [7], [8], natural language processing [9]–[12],\nand reinforcement learning [13]–[19]. Among these areas,\nquantum reinforcement learning (QRL) is an emerging field\nwhere researchers are exploring the application of quantum\ncomputing principles to enhance the performance of reinforce-\nment learning agents. This article provides an introduction to\nthe concepts and recent developments in QRL.\nThe views expressed in this article are those of the authors and do not\nrepresent the views of Wells Fargo. This article is for informational purposes\nonly. Nothing contained in this article should be construed as investment\nadvice. Wells Fargo makes no express or implied warranties and expressly\ndisclaims all legal, tax, and accounting implications related to this article.\nII. QUANTUM NEURAL NETWORKS\nA. Quantum Computing\nA qubit represents the fundamental unit of quantum infor-\nmation processing. Unlike a classical bit, which is restricted\nto holding a state of either 0 or 1, a qubit can simultaneously\nencapsulate the information of both 0 and 1 due to the\nprinciple of superposition. A single qubit quantum state can\nbe expressed as |Ψ⟩= α |0⟩+ β |1⟩, where |0⟩= [1, 0]T\nand |1⟩= [0, 1]T are column vectors, and α and β are\ncomplex numbers. In an n-qubit system, the state vector has\na length of 2n. Quantum gates U are utilized to transform\na quantum state, represented as |Ψ⟩, to another state |Ψ′⟩\nthrough the operation |Ψ′⟩= U |Ψ⟩. These quantum gates\nare unitary transformations that satisfy the condition UU † =\nU †U = I2n×2n, where n denotes the number of qubits. It has\nbeen demonstrated that a small set of basic quantum gates\nis sufficient for universal quantum computation. One such set\nincludes single-qubit gates H, σx, σy, σz, Rx(θ) = e−iθσx/2,\nRy(θ) = e−iθσy/2, Rz(θ) = e−iθσz/2, and the two-qubit gate\nCNOT. In quantum machine learning (QML), rotation gates\nRx, Ry, and Rz are particularly crucial as their rotation angles\ncan be treated as trainable or learnable parameters subject to\noptimization. For quantum operations on multi-qubit systems,\nthe unitary transformation can be constructed via the tensor\nproduct of individual single-qubit or two-qubit operations,\nU = U1 ⊗U2 ⊗· · · ⊗Uk. At the final stage of a quantum\ncircuit, a procedure known as measurement is performed. A\nsingle execution of a quantum circuit generates a binary string.\nThis procedure can be repeated multiple times to determine the\nprobabilities of different computational bases (e.g., |0, · · · , 0⟩,\n· · · , |1, · · · , 1⟩) or to calculate expectation values (e.g., Pauli\nX, Y , and Z).\nB. Variational Quantum Circuits\nVariational quantum circuits (VQCs), also referred to as\nparameterized quantum circuits (PQCs), represent a special-\nized class of quantum circuits with trainable parameters. VQCs\nare extensively utilized within the current hybrid quantum-\nclassical computing framework [2] and have demonstrated\nspecific types of quantum advantages [20]–[22]. There are\nthree fundamental components in a VQC: encoding circuit,\nvariational circuit and the final measurements. As shown in\nFigure 1, the encoding circuit U(x) transforms the initial\narXiv:2409.05846v1  [quant-ph]  9 Sep 2024\nquantum state |0⟩⊗n into |Ψ⟩= U(x) |0⟩⊗n. Here the n\nrepresents the number of qubits, |0⟩⊗n represents the n-qubit\ninitial state |0, · · · , 0⟩and the U(x) represents the unitary\nwhich depends on the input value x. The measurement process\nextracts data from the VQC by assessing either a subset or all\nof the qubits, producing a classical bit sequence for further\nuse. Running the circuit once yields a bit sequence such as\n”0,0,1,1.” However, preparing and executing the circuit multi-\nple times (shots) generates expectation values for each qubit.\nMost works mentioned in this survey focus on the evaluation\nof Pauli-Z expectation values derived from measurements in\nVQCs. Generally, the mathematical expression of the VQC\ncan be expressed as −−−−→\nf(x; Θ) =\n\u0010D\nˆZ1\nE\n, · · · ,\nD\nˆZn\nE\u0011\n, where\nD\nˆZk\nE\n=\nD\n0\n\f\f\fU †(x)W †(Θ) ˆ\nZkW(Θ)U(x)\n\f\f\f 0\nE\n. In the hybrid\nquantum-classical framework, the VQC can be integrated with\nother classical components, such as deep neural networks and\ntensor networks, or with other quantum components, including\nadditional VQCs. The entire model can be optimized in an\nend-to-end manner using either gradient-based [4], [5] or\ngradient-free [14] methods. For gradient-based methods like\ngradient descent, the gradients of quantum components can\nbe computed via the parameter-shift rules [3], [23], [24].\nFig. 1. Generic Structure of a Variational Quantum Circuit (VQC).\nIII. QUANTUM REINFORCEMENT LEARNING\nA. Reinforcement Learning\nReinforcement Learning (RL) is a pivotal paradigm within\nmachine learning, where an autonomous entity known as the\nagent learns to make decisions through iterative interactions\nwith its environment [25]. The agent operates within a defined\nenvironment, represented as E, over discrete time steps. At\neach time step t, the agent receives state or observation\ninformation, denoted as st, from the environment E. Based\non this information, the agent selects an action at from a set\nof permissible actions A, guided by its policy π. The policy\nπ acts as a function that maps the current state or observation\nst to the corresponding action at. Notably, the policy can be\nstochastic, indicating that for a given state st, the action at is\ndetermined by a probability distribution π(at|st).\nUpon executing action at, the agent transitions to the sub-\nsequent state st+1 and receives a scalar reward rt. This cycle\ncontinues until the agent reaches a terminal state or fulfills\na specified stopping condition, such as a maximum number\nof steps. We define an episode as the sequence beginning\nfrom an initial state, following the described process, and\nconcluding either at the terminal state or upon meeting the\nstopping criterion. The use of quantum neural networks for\nlearning policy or value functions is referred to as quantum\nreinforcement learning (QRL). The idea of QRL is illustrated\nin Figure 2. For a comprehensive review of current QRL\ndomain, refer to the review article [18].\nFig. 2. Concept of quantum reinforcement learning (QRL).\nB. Quantum Deep Q-learning\nQ-learning [25] is a fundamental model-free RL algorithm.\nIt learns the optimal action-value function and operates off-\npolicy. The process begins with the random initialization of\nQπ(s, a) for all states s ∈S and actions a ∈A, stored in a\nQ-table. The Qπ(s, a) estimates are updated using the Bellman\nequation:\nQ (st, at) ←Q (st, at)\n+ α\nh\nrt + γ max\na\nQ (st+1, a) −Q (st, at)\ni\n.\n(1)\nThe conventional Q-learning approach offers the optimal\naction-value function but is impractical for problems requir-\ning extensive memory, especially with high-dimensional state\n(s) or action (a) spaces. In environments with continuous\nstates, storing Q(s, a) in a table is inefficient or impossible.\nTo address this challenge, neural networks (NNs) are used\nto represent Qπ(s, a), ∀s ∈S, a ∈A, leading to deep\nQ-learning. The network in this technique is known as a\ndeep Q-network (DQN) [26]. To enhance the stability of\nDQN, techniques such as experience replay and the use of\na target network are employed [26]. Experience replay stores\nexperiences as transition tuples st, at, rt, st+1 in a memory\nor buffer. After gathering sufficient experiences, the agent\nrandomly samples a batch to compute the loss and update\nDQN parameters. Additionally, to reduce correlation between\ntarget and prediction, a target network, which is a duplicate\nof the DQN, is used. The DQN parameters θ are updated\niteratively, while the target network parameters θ−are updated\nperiodically. The DQN training is done via minimizing the\nmean square error (MSE) loss function:\nL(θ) = E\nh\n(rt + γ maxa′ Q (st+1, a′; θ−) −Q (st, at; θ))2i\n(2)\nOther loss functions such as Huber loss or mean absolute\nerror (MAE) can also be used. The first VQC-based QRL\nis described in the work [13] in which a VQC is designed\nto solve environments with discrete observations such as the\nFrozen Lake and Cognitive-Radio. The design follows the\noriginal idea in classical deep Q-learning [26]. As shown in\nFigure3, there are target network and experience replay in this\nquantum DQN. They are basically two sets of quantum circuit\nparameters. The quantum agent is optimized via gradient\ndescent algorithm such as RMSProp in the hybrid quantum-\nclassical manner. Later, more sophisticated efforts in the area\nof quantum DQN take into account continuous observation\nspaces like Cart-Pole [15], [16].\nFig. 3. Quantum deep Q-learning.\nC. Quantum Policy Gradient Methods\nIn contrast to value-based RL algorithms, such as Q-\nlearning, which depend on learning a value function to guide\ndecision-making at each time step, policy gradient meth-\nods aim to optimize a policy function directly. This policy\nfunction π(a|s; θ) is parameterized by θ. The parameters\nθ are adjusted using gradient ascent on the expected total\nreturn, E[Rt]. A prominent example of a policy gradient\nalgorithm is the REINFORCE algorithm [27]. The policy\nfunction π(a|s; θ) can be implemented using a VQC, where\nthe rotation parameters serve as θ. In [28], the authors employ\nthe REINFORCE algorithm to train a VQC-based policy.\nTheir results demonstrate that VQC-based policies can achieve\nperformance comparable to or exceeding that of classical\nDNNs on several standard benchmarks. In the traditional\nREINFORCE algorithm, parameter updates for θ are based\non the gradient ∇θ log π (at|st; θ) Rt, which provides an un-\nbiased estimate of ∇θE [Rt]. However, this gradient estimate\ncan exhibit high variance, which may lead to difficulties or\ninstability during training. To address this issue and reduce\nvariance while preserving unbiasedness, a baseline term can\nbe subtracted from the return. This baseline, bt(st), is a\nlearned function of the state st. The update rule then be-\ncomes ∇θ log π (at|st; θ) (Rt −bt (st)). A typical choice for\nthe baseline bt(st) in RL is an estimate of the value function\nV π(st). Employing this baseline generally leads to a reduction\nin the variance of the policy gradient estimate [25]. The term\nRt−bt = Q(st, at)−V (st) represents the advantage A(st, at)\nof taking action at in state st. This advantage can be viewed\nas a measure of how favorable or unfavorable action at is\ncompared to the average value of the state st. This method\nis referred to as the advantage actor-critic (A2C) approach,\nwhere the policy π serves as the actor and the value function\nV acts as the critic [25]. Similar to traditional policy gradient\nmethods, the A2C algorithm can be implemented using VQCs.\nIn [29], the authors utilize VQCs to construct both the actor\n(policy function) and the critic (value function). Their study\ndemonstrates that, for comparable numbers of model param-\neters, a hybrid approach—where classical neural networks\npost-process the outputs from the VQC—achieves superior\nperformance across the tested environments. The asynchronous\nadvantage actor-critic (A3C) algorithm [30] is an enhanced\nvariant of the A2C method that utilizes multiple concurrent\nactors to learn the policy through parallel processing. This\napproach involves deploying several agents across several\ninstances of the environment, enabling them to experience a\nwide range of states simultaneously. By reducing the corre-\nlation between states or observations, this method improves\nthe numerical stability of on-policy RL algorithms like actor-\ncritic [30]. Moreover, asynchronous training eliminates the\nneed for extensive replay memory, which helps in reducing\nmemory usage [30]. A3C achieves high sample efficiency and\nrobust learning performance, making it a favored choice in RL.\nIn the context of quantum RL, asynchronous or distributed\ntraining can further boost sampling efficiency and leverage\nthe capabilities of multiple quantum computers or quantum\nprocessing units (QPUs). In [31], the authors extend the A3C\nframework to quantum settings, showing that VQC actors and\ncritics can outperform classical models when the sizes of the\nmodels are comparable.\nD. Quantum RL with Evolutionary Optimization\nOne of the significant challenges in current QML appli-\ncations is the limitation of quantum computers or quantum\nsimulation software in processing input dimensions. These\nsystems can only handle inputs up to a certain level, which is\ninsufficient for encoding larger vectors. In QRL, this constraint\nmeans the observation vector that the quantum agent can\nprocess from the environment is severely restricted. To address\nthis issue, various dimensional reduction methods have been\nproposed. Among these, a hybrid quantum-classical approach\nthat incorporates a classical learnable model with a VQC has\nshown promising results. In the work by Chen et al. [14], a\nquantum-inspired classical model based on a specific type of\ntensor network, known as a matrix product state (MPS), is\nintegrated with a VQC to function as a learnable compressor\n[4] (see Figure4). The hybrid architecture MPS-VQC, includ-\ning the tensor network and VQC, is randomly initialized, and\nthe entire model is trained in an end-to-end manner. Although\ngradient-based methods have achieved considerable success in\nRL, several challenges remain. Notably, these methods can\nbecome trapped in local minima or fail to converge to the\noptimal solution, particularly in sparse RL environments where\nthe agent frequently receives zero rewards during episodes.\nEvolutionary optimization techniques have been proposed to\naddress these challenges in classical RL and have demon-\nstrated significant success [32]. A similar approach can be\napplied to hybrid quantum-classical RL models. Specifically,\na population P of N agents, represented as parameter vectors\nΘi, i ∈1, · · · , N, is randomly initialized. In each generation,\nthe top-performing agents are selected to serve as parents for\ngenerating the next generation of agents/parameter vectors.\nThe update rules for the new parameters involve adding\nGaussian noise to the parent parameters. This method has\nbeen shown to optimize MPS-VQC models effectively and\nto outperform NN-VQC in selected benchmarks [14].\nFig. 4. Hybrid Quantum-Classical RL with Tensor Networks.\nE. Quantum RL with Recurrent Policies\nThe previously mentioned quantum RL methods primarily\nutilize various VQCs without incorporating recurrent struc-\ntures. However, recurrent connections are essential in classical\nmachine learning for retaining memory of past time steps.\nCertain RL tasks necessitate that agents have the capability\nto remember information from previous time steps to se-\nlect optimal actions. For instance, environments with partial\nobservability often require agents to make decisions based\nnot only on information from the current time step but also\non information accumulated from the past. In classical ML,\nrecurrent neural networks (RNNs), such as long short-term\nmemory (LSTM) [33], have been proposed to solve tasks\nwith temporal dependencies. The quantum version of LSTM\n(QLSTM) has been designed by replacing classical neural\nnetworks with VQCs [7]. It has been shown that QLSTM can\noutperform classical LSTM in several time-series prediction\ntasks when the model sizes are similar [7]. To address RL\nenvironments with partial observability or those requiring\ntemporal memories, QRL agents utilizing QLSTM as the value\nor policy function have been proposed in [17]. It has been\ndemonstrated that QLSTM-based value or policy functions\nenable QRL agents to outperform classical LSTM models with\na similar number of parameters.\nWhile the QLSTM-based models achieve significant results\nin several benchmarks, there is at least one major challenge\npreventing such models from wide applications. The training\nof RNNs, both in quantum and classical, requires significant\ncomputational resources due to the requirement of performing\nbackpropagation-through-time (BPTT). One might question\nwhether it is possible to leverage the capabilities of QLSTM\nwithout the need for gradient calculations with respect to\nthe quantum parameters. Indeed, it has been demonstrated\nthat a randomly initialized RNN can function as a reservoir,\ntransforming input information into a high-dimensional space.\nThe only part that requires training is the linear layer following\nthe reservoir. Quantum RNNs, such as QLSTM, can also be\nutilized as reservoirs [8]. It has been shown that even with-\nout training, the QLSTM reservoir can achieve performance\ncomparable to fully trained models [8]. To further enhance\nthe performance of QLSTM-based QRL agents and reduce\ntraining resource requirements, a randomly initialized QLSTM\ncan be employed as a reservoir in an RL agent [19]. Numerical\nsimulations have demonstrated that the QLSTM reservoir can\nachieve performance comparable to, and sometimes superior\nto, fully trained QLSTM RL agents.\nF. Quantum RL with Fast Weight Programmers\nAn alternative approach for developing a QRL model that\ncan memorize temporal or sequential dependencies without\nutilizing quantum RNNs is the Quantum Fast Weight Pro-\ngrammers (QFWP). The idea of Fast Weight Programmers\n(FWP) was originally proposed in the work of Schmidhuber\n[34], [35]. In this sequential learning model, two distinct\nneural networks (NN) are utilized: the slow programmer\nand the fast programmer. Here, the NN weights act as the\nmodel/agent’s program. The core concept of FWP involves\nthe slow programmer generating updates or changes to the\nfast programmer’s NN weights based on observations at each\ntime-step. This reprogramming process quickly redirects the\nfast programmer’s attention to salient information within the\nincoming data stream. Notably, the slow programmer does not\ncompletely overwrite the fast programmer but instead applies\nupdates or changes. This approach allows the fast programmer\nto incorporate previous observations, enabling a simple feed-\nforward NN to manage sequential prediction or control without\nthe high computational demands of recurrent neural networks\n(RNNs). The idea of FWP can be further extended into the\nhybrid quantum-classical regime as described in the work\n[36]. In the work [36], classical neural networks are used to\nconstruct the slow networks, which generate values to update\nthe parameters of the fast networks, implemented as a VQC.\nFig. 5. Quantum Fast Weight Programmers.\nAs illustrated in Figure 5, the input vector ⃗x is first pro-\ncessed by a classical neural network encoder. The encoder’s\noutput is then fed into two additional neural networks. One\nnetwork generates an output vector [Li] corresponding to the\nnumber of VQC layers, while the other produces an output\nvector [Qj] matching the number of qubits in the VQC.\nWe then calculate the outer product of [Li] and [Qj]. It\ncan be written as [Li] ⊗[Qj] = [Mij] = [Li × Qj] =\n\n\nL1 × Q1\nL1 × Q2\n· · ·\nL1 × Qn\nL2 × Q1\nL2 × Q2\n· · ·\nL2 × Qn\n...\n...\n...\nLl × Q1\nLl × Q2\n· · ·\nLl × Qn\n\n, where l is the num-\nber of learnable layers in VQC and n is the number of qubits.\nAt time t + 1, the updated VQC parameters can be calculated\nas θt+1\nij\n= f(θt\nij, Li × Qj), where f combines the previous\nparameters θt\nij with the newly computed Li × Qj. In the time\nseries modeling and RL tasks in [36], the additive update\nrule is used. The new circuit parameters are calculated as\nθt+1\nij\n= θt\nij + Li × Qj. This method preserves information\nfrom previous time steps in the circuit parameters, influencing\nthe VQC behavior with each new input ⃗x. The output from\nthe VQC can be further processed by components such as\nscaling, translation, or a classical neural network to refine the\nfinal results.\nG. Quantum RL with Quantum Architecture Search\nWhile QRL has demonstrated effectiveness across various\nproblem domains, the design of successful architectures is\nfar from trivial. Developing VQC architectures tailored to\nspecific problems requires substantial effort and expertise.\nThe field of quantum architecture search (QAS) focuses\non developing methods to identify high-performing quantum\ncircuits for specific tasks. A QAS problem is formulated by\nspecifying a particular goal (e.g., total returns in RL) and the\nconstraints of the quantum device (e.g., maximum number\nof quantum operations, set of allowed quantum gates). QAS\nhas been explored in the context of QRL. For instance, in\n[37], evolutionary algorithms are employed to search for high-\nperforming circuits. The authors define a set of candidate VQC\nblocks, including entangling blocks, data-encoding blocks,\nvariational blocks, and measurement blocks. The objective of\nthe evolutionary search is to determine an optimal sequence of\nthese blocks, given a constraint on the maximum number of\ncircuit blocks. While this approach has shown effectiveness in\nthe evaluated cases, scalability issues may arise as the search\nspace expands. Differentiable quantum architecture search\n(DiffQAS) methods, as proposed in [38], draw inspiration\nfrom differentiable neural architecture search in classical deep\nlearning to identify effective quantum circuits for RL. In\n[39], the authors apply DiffQAS to quantum deep Q-learning.\nThey parameterize a probability distribution P(k, α) for circuit\narchitecture k using α. During training, mini-batches of VQCs\nare sampled, and the weighted loss is calculated based on\nthe distribution P(k, α). Both the architecture parameter α\nand the quantum circuit parameters θ are updated using con-\nventional gradient-based methods. In [40], the authors extend\nthe DiffQAS framework to asynchronous QRL. This extension\nallows multiple parallel instances (a single instance is shown in\nFigure6 ) to optimize their own structural weights (denoted as\nw in Figure 6 ) alongside the VQC parameters. The gradients\nof these structural weights and quantum circuit parameters are\nshared across instances to enhance the training process.\nFig. 6. Differentiable Quantum Architecture Search (DiffQAS).\nIV. QUANTUM RL APPLICATIONS AND CHALLENGES\nQRL can be extended to multi-agent settings and applied\nin fields like wireless communication and autonomous control\nsystems [41]. Additionally, as discussed in SectionIII-G, QAS\ninvolves sequential decision-making and can be addressed\nthrough RL. In [42], a QRL approach is developed to discover\nquantum circuit architectures that generate desired quantum\nstates. In the NISQ era, a major challenge for QML applica-\ntions is the limited quantum resources, which complicates both\nthe training and inference phases. In [43], [44], the authors\npropose a method using a QNN to generate classical NN\nweights. For an N-qubit QNN, measuring the expectation\nvalues of individual qubits provides up to N values. However,\ncollecting the probabilities of all computational basis states\n|00 · · · 0⟩, · · · , |11 · · · 1⟩yields 2N values. These values can\nbe rescaled and used as NN weights. Thus, for an NN with\nM weights, only ⌈log2 M⌉qubits are needed to generate the\nweights. Numerical simulations demonstrate that the quantum\ncircuit can efficiently generate NN weights, achieving infer-\nence performance comparable to conventional training. Future\nresearch could further explore the trainability challenges in\nQRL models highlighted by Sequeira et al. [45], which are\nkey to enhancing their practical performance.\nV. CONCLUSION AND OUTLOOK\nThis paper introduces the concept of quantum reinforcement\nlearning (QRL), where variational quantum circuits (VQCs)\nare used as policy and value functions. It also explores\nadvanced constructs, including quantum recurrent policies,\nquantum fast weight programmers, and QRL with differen-\ntiable quantum architectures. QRL holds the potential to offer\nquantum advantages in various sequential decision-making\ntasks.\nREFERENCES\n[1] M. A. Nielsen and I. L. Chuang, Quantum computation and quantum\ninformation.\nCambridge university press, 2010.\n[2] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea,\nA. Anand, M. Degroote, H. Heimonen, J. S. Kottmann, T. Menke et al.,\n“Noisy intermediate-scale quantum algorithms,” Reviews of Modern\nPhysics, vol. 94, no. 1, p. 015004, 2022.\n[3] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, “Quantum circuit\nlearning,” Physical Review A, vol. 98, no. 3, p. 032309, 2018.\n[4] S. Y.-C. Chen, C.-M. Huang, C.-W. Hsing, and Y.-J. Kao, “An end-to-\nend trainable hybrid classical-quantum classifier,” Machine Learning:\nScience and Technology, vol. 2, no. 4, p. 045021, 2021.\n[5] J. Qi, C.-H. H. Yang, and P.-Y. Chen, “Qtn-vqc: An end-to-end learning\nframework for quantum neural networks,” Physica Scripta, vol. 99, 12\n2023.\n[6] S. Y.-C. Chen, T.-C. Wei, C. Zhang, H. Yu, and S. Yoo, “Quantum\nconvolutional neural networks for high energy physics data analysis,”\nPhysical Review Research, vol. 4, no. 1, p. 013231, 2022.\n[7] S. Y.-C. Chen, S. Yoo, and Y.-L. L. Fang, “Quantum long short-term\nmemory,” in ICASSP 2022-2022 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2022, pp.\n8622–8626.\n[8] S. Y.-C. Chen, D. Fry, A. Deshmukh, V. Rastunkov, and C. Stefanski,\n“Reservoir computing via quantum recurrent neural networks,” arXiv\npreprint arXiv:2211.02612, 2022.\n[9] S. S. Li, X. Zhang, S. Zhou, H. Shu, R. Liang, H. Liu, and L. P. Garcia,\n“Pqlm-multilingual decentralized portable quantum language model,” in\nICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2023, pp. 1–5.\n[10] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, Y. Tsao, and P.-Y. Chen, “When bert\nmeets quantum temporal convolution learning for text classification in\nheterogeneous computing,” in ICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2022, pp. 8602–8606.\n[11] R. Di Sipio, J.-H. Huang, S. Y.-C. Chen, S. Mangini, and M. Worring,\n“The dawn of quantum natural language processing,” in ICASSP 2022-\n2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2022, pp. 8612–8616.\n[12] J. Stein, I. Christ, N. Kraus, M. B. Mansky, R. M¨uller, and C. Linnhoff-\nPopien, “Applying qnlp to sentiment analysis in finance,” in 2023 IEEE\nInternational Conference on Quantum Computing and Engineering\n(QCE), vol. 2.\nIEEE, 2023, pp. 20–25.\n[13] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S.\nGoan, “Variational quantum circuits for deep reinforcement learning,”\nIEEE Access, vol. 8, pp. 141 007–141 024, 2020.\n[14] S. Y.-C. Chen, C.-M. Huang, C.-W. Hsing, H.-S. Goan, and Y.-J. Kao,\n“Variational quantum reinforcement learning via evolutionary optimiza-\ntion,” Machine Learning: Science and Technology, vol. 3, no. 1, p.\n015025, 2022.\n[15] O. Lockwood and M. Si, “Reinforcement learning with quantum vari-\national circuit,” in Proceedings of the AAAI Conference on Artificial\nIntelligence and Interactive Digital Entertainment, vol. 16, no. 1, 2020,\npp. 245–251.\n[16] A. Skolik, S. Jerbi, and V. Dunjko, “Quantum agents in the gym: a\nvariational quantum algorithm for deep q-learning,” Quantum, vol. 6, p.\n720, 2022.\n[17] S. Y.-C. Chen, “Quantum deep recurrent reinforcement learning,” in\nICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2023, pp. 1–5.\n[18] N. Meyer, C. Ufrecht, M. Periyasamy, D. D. Scherer, A. Plinge, and\nC. Mutschler, “A survey on quantum reinforcement learning,” arXiv\npreprint arXiv:2211.03464, 2022.\n[19] S. Y.-C. Chen, “Efficient quantum recurrent reinforcement learning\nvia quantum reservoir computing,” in ICASSP 2024-2024 IEEE In-\nternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2024, pp. 13 186–13 190.\n[20] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner,\n“The power of quantum neural networks,” Nature Computational Sci-\nence, vol. 1, no. 6, pp. 403–409, 2021.\n[21] M. C. Caro, H.-Y. Huang, M. Cerezo, K. Sharma, A. Sornborger,\nL. Cincio, and P. J. Coles, “Generalization in quantum machine learning\nfrom few training data,” Nature communications, vol. 13, no. 1, pp. 1–\n11, 2022.\n[22] Y. Du, M.-H. Hsieh, T. Liu, and D. Tao, “Expressive power of\nparametrized quantum circuits,” Physical Review Research, vol. 2, no. 3,\np. 033125, 2020.\n[23] M. Schuld, V. Bergholm, C. Gogolin, J. Izaac, and N. Killoran, “Eval-\nuating analytic gradients on quantum hardware,” Physical Review A,\nvol. 99, no. 3, p. 032331, 2019.\n[24] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, C. Blank, K. McK-\niernan, and N. Killoran, “Pennylane: Automatic differentiation of hy-\nbrid quantum-classical computations,” arXiv preprint arXiv:1811.04968,\n2018.\n[25] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nnature, vol. 518, no. 7540, pp. 529–533, 2015.\n[27] R. J. Williams, “Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning,” Machine learning, vol. 8, no. 3-4,\npp. 229–256, 1992.\n[28] S. Jerbi, C. Gyurik, S. Marshall, H. Briegel, and V. Dunjko,\n“Parametrized quantum policies for reinforcement learning,” Advances\nin Neural Information Processing Systems, vol. 34, pp. 28 362–28 375,\n2021.\n[29] M. K¨olle, M. Hgog, F. Ritz, P. Altmann, M. Zorn, J. Stein, and\nC. Linnhoff-Popien, “Quantum advantage actor-critic for reinforcement\nlearning,” arXiv preprint arXiv:2401.07043, 2024.\n[30] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning.\nPMLR, 2016, pp. 1928–1937.\n[31] S. Y.-C. Chen, “Asynchronous training of quantum reinforcement\nlearning,” Procedia Computer Science, vol. 222, pp. 321–330, 2023,\ninternational Neural Network Society Workshop on Deep Learning\nInnovations and Applications (INNS DLIA 2023). [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1877050923009365\n[32] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and\nJ. Clune, “Deep neuroevolution: Genetic algorithms are a competitive\nalternative for training deep neural networks for reinforcement learning,”\narXiv preprint arXiv:1712.06567, 2017.\n[33] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[34] J. Schmidhuber, “Learning to control fast-weight memories: An alterna-\ntive to dynamic recurrent networks,” Neural Computation, vol. 4, no. 1,\npp. 131–139, 1992.\n[35] ——, “Reducing the ratio between learning complexity and number\nof time varying variables in fully recurrent nets,” in ICANN’93: Pro-\nceedings of the International Conference on Artificial Neural Networks\nAmsterdam, The Netherlands 13–16 September 1993 3. Springer, 1993,\npp. 460–463.\n[36] S. Y.-C. Chen, “Learning to program variational quantum circuits with\nfast weights,” arXiv preprint arXiv:2402.17760, 2024.\n[37] L. Ding and L. Spector, “Evolutionary quantum architecture search\nfor parametrized quantum circuits,” in Proceedings of the Genetic and\nEvolutionary Computation Conference Companion, 2022, pp. 2190–\n2195.\n[38] S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and H. Yao, “Differentiable\nquantum architecture search,” Quantum Science and Technology, vol. 7,\nno. 4, p. 045023, 2022.\n[39] Y. Sun, Y. Ma, and V. Tresp, “Differentiable quantum architecture\nsearch for quantum reinforcement learning,” in 2023 IEEE International\nConference on Quantum Computing and Engineering (QCE), vol. 2.\nIEEE, 2023, pp. 15–19.\n[40] S.\nY.-C.\nChen,\n“Differentiable\nquantum\narchitecture\nsearch\nin\nasynchronous\nquantum\nreinforcement\nlearning,”\narXiv\npreprint\narXiv:2407.18202, 2024.\n[41] C. Park, W. J. Yun, J. P. Kim, T. K. Rodrigues, S. Park, S. Jung,\nand J. Kim, “Quantum multiagent actor–critic networks for cooperative\nmobile access in multi-uav systems,” IEEE Internet of Things Journal,\nvol. 10, no. 22, pp. 20 033–20 048, 2023.\n[42] S. Y.-C. Chen, “Quantum reinforcement learning for quantum archi-\ntecture search,” in Proceedings of the 2023 International Workshop on\nQuantum Classical Cooperative, 2023, pp. 17–20.\n[43] C.-Y. Liu, E.-J. Kuo, C.-H. A. Lin, J. G. Young, Y.-J. Chang, M.-H.\nHsieh, and H.-S. Goan, “Quantum-train: Rethinking hybrid quantum-\nclassical machine learning in the model compression perspective,” arXiv\npreprint arXiv:2405.11304, 2024.\n[44] C.-Y. Liu, C.-H. A. Lin, C.-H. H. Yang, K.-C. Chen, and M.-H. Hsieh,\n“Qtrl: Toward practical quantum reinforcement learning via quantum-\ntrain,” arXiv preprint arXiv:2407.06103, 2024.\n[45] A. Sequeira, L. P. Santos, and L. Soares Barbosa, “Trainability issues in\nquantum policy gradients,” Machine Learning: Science and Technology,\n2024.\n",
  "categories": [
    "quant-ph",
    "cs.AI",
    "cs.ET",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2024-09-09",
  "updated": "2024-09-09"
}