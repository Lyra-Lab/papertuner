{
  "id": "http://arxiv.org/abs/2409.07135v1",
  "title": "Unsupervised Novelty Detection Methods Benchmarking with Wavelet Decomposition",
  "authors": [
    "Ariel Priarone",
    "Umberto Albertin",
    "Carlo Cena",
    "Mauro Martini",
    "Marcello Chiaberge"
  ],
  "abstract": "Novelty detection is a critical task in various engineering fields. Numerous\napproaches to novelty detection rely on supervised or semi-supervised learning,\nwhich requires labelled datasets for training. However, acquiring labelled\ndata, when feasible, can be expensive and time-consuming. For these reasons,\nunsupervised learning is a powerful alternative that allows performing novelty\ndetection without needing labelled samples. In this study, numerous\nunsupervised machine learning algorithms for novelty detection are compared,\nhighlighting their strengths and weaknesses in the context of vibration\nsensing. The proposed framework uses a continuous metric, unlike most\ntraditional methods that merely flag anomalous samples without quantifying the\ndegree of anomaly. Moreover, a new dataset is gathered from an actuator\nvibrating at specific frequencies to benchmark the algorithms and evaluate the\nframework. Novel conditions are introduced by altering the input wave signal.\nOur findings offer valuable insights into the adaptability and robustness of\nunsupervised learning techniques for real-world novelty detection applications.",
  "text": "UNSUPERVISED NOVELTY DETECTION METHODS\nBENCHMARKING WITH WAVELET DECOMPOSITION ∗\nAriel Priarone\n†, Umberto Albertin\n‡, Carlo Cena\n‡, Mauro Martini\n, Marcello Chiaberge\nDepartment of Electronics and Telecommunications\nPolitecnico di Torino\nTurin, Italy\n{ariel.priarone, umberto.albertin, carlo.cena,\nmauro.martini, marcello.chiaberge}@polito.it\n† Corresponding author\n‡ This publication is part of the project PNRR-NGEU which has received\nfunding from the MUR – DM 351/2022 and MUR – DM 117/2023.\nABSTRACT\nNovelty detection is a critical task in various engineering fields. Numerous approaches to novelty\ndetection rely on supervised or semi-supervised learning, which requires labelled datasets for training.\nHowever, acquiring labelled data, when feasible, can be expensive and time-consuming. For these\nreasons, unsupervised learning is a powerful alternative that allows performing novelty detection\nwithout needing labelled samples. In this study, numerous unsupervised machine learning algorithms\nfor novelty detection are compared, highlighting their strengths and weaknesses in the context of\nvibration sensing. The proposed framework uses a continuous metric, unlike most traditional methods\nthat merely flag anomalous samples without quantifying the degree of anomaly. Moreover, a new\ndataset is gathered from an actuator vibrating at specific frequencies to benchmark the algorithms\nand evaluate the framework. Novel conditions are introduced by altering the input wave signal.\nOur findings offer valuable insights into the adaptability and robustness of unsupervised learning\ntechniques for real-world novelty detection applications.\nKeywords Unsupervised Machine Learning · Novelty Detection · Anomaly Detection · Predictive Maintenance ·\nArtificial Intelligence · Neural Network · Prognostics · Autoencoder · K-Means · DBSCAN · Gaussian Mixture Model ·\nOne-Class Support Vector Machine · Isolation Forest · Local Outlier Factors\n1\nIntroduction\nIn a world driven by data, it is essential to identify unexpected events. Novelty detection, i.e. the process of identifying\nnew or unknown data that deviate significantly from the expected or established patterns, is crucial in various fields, as\nit helps in identifying unusual occurrences that could indicate critical events such as system faults, fraud, or emerging\ntrends.\nFor example, in manufacturing, novelty detection can be used to spot defects in the production plant [1], while in\ncybersecurity, it can help detect unusual patterns of behaviour that might indicate a security breach [2]. Lastly, in\nfinance, anomaly detection can be vital for fraud detection, allowing institutions to prevent fraudulent transactions\nbefore they cause significant damage. Overall, the capability to recognize novelties enhances decision-making, improves\nsafety, and reduces risks across various domains.\n∗This document is a preprint as at September 10, 2024. The paper has been accepted for publication as proceeding at the 8th\nInternational Conference on System Reliability and Safety. Sicily, Italy - November 20-22, 2024 https://www.icsrs.org/index.\nhtml\narXiv:2409.07135v1  [cs.LG]  11 Sep 2024\nA PREPRINT - SEPTEMBER 10, 2024\nTraditional approaches to novelty detection often rely on statistical methods [3, 4]. These methods generally depend\non predefined thresholds or models of normal behaviour, which can struggle in their ability to generalize to evolving\npatterns and unseen anomalies.\nArtificial Intelligence (AI) has significantly advanced the field of novelty detection. Machine Learning (ML) models\ncan learn from data and identify patterns that would be difficult to specify manually. Among AI approaches, supervised\nlearning algorithms require labelled data to train models that can then predict anomalies. Notable examples are neural\nnetworks or support vector machines trained on historical data [5, 6]. Semi-supervised approaches, which use a\ncombination of labelled and unlabelled data, offer a middle ground, as they leverage the available labelled data to\nimprove the learning process while also incorporating unlabeled data to improve the model’s robustness and accuracy\n[7, 8].\nUnsupervised learning techniques, which do not require labelled data, are particularly valuable for novelty detection in\nsituations where obtaining labelled data is impractical or impossible, due to a scarcity of data or to the costs associated\nwith labelling it [7]. These methods include clustering algorithms, like K-means, autoencoders and density-based\napproaches [9, 10, 11, 12, 13, 14]. Unsupervised algorithms can discover the underlying structure of the data and\nidentify deviations. This capability makes unsupervised approaches versatile and powerful in changing environments.\nTo the best of our knowledge, the current literature lacks a comprehensive study which compares Unsupervised Machine\nLearning (UML) models able to produce a continuous degradation metric, with several preprocessing algorithms. We\nbelieve such a benchmark could provide valuable information to the novelty detection field, serving as a critical resource\nfor researchers and practitioners.\nIn this study, we benchmarked various unsupervised AI algorithms for novelty detection on a dataset developed in a\nlaboratory using a shaker to record vibrations at given frequencies, artificially generating novel conditions by changing\nthe input wave signal. Our method aims at predicting a continuous metric rather than a binary label, as this approach\nallows us to evaluate the effectiveness and adaptability of these algorithms in identifying novel patterns in complex data\nstructures. We extract a combination of statistical features and wavelet decomposition coefficients from the raw signal\nto use them as input for the UML models. Moreover, we defined a proper set of metrics to compare the performances of\nthe framework configurations. Lastly, we analyze the performance of each configuration by recording its inference time\nto measure the computational effort required. The following sections detail our methodology, experiments, and results.\n2\nRelated Works\nHere we give an overview of previous works on novelty detection, focusing mainly on unsupervised approaches.\nThe K-means algorithm is used in [12, 13] to label the degradation states of bearings. In [12], the Intelligent Maintenance\nSystems (IMS) bearing dataset [15] is clustered using Traditional Statistical Features (TSF) and Mel-frequency Cepstral\nCoefficients, and the labelled time series are subsequently used to train a Convolutional Neural Network (CNN)\nrecognition model using raw data, without the need to extract new features. Reference [13] computed TSF and\nShannon’s entropy to perform clustering on IMS and Case Western Reserve University (CWRU) [16] datasets. Pinedo\net al. converted the time series into 2D representations, as images, using a CNN, i.e. Alexnet [17], to classify the\ndegradation states.\nReference [10] proposed a method to quantify the health of bearings and validate it on the IMS dataset. They used\ntime-domain features extracted from the vibration signal and applied a cross-correlation filter to remove the redundant\nfeatures. Then, the K-means algorithm was used to select only the most relevant features through an optimization\nprocedure aimed at obtaining the most dense and separated clusters. The Self-Organizing Map algorithm is then used to\ncompute a health indicator for the bearing.\nA method to efficiently initialize the cluster centres in the K-means algorithm is proposed in [18]. It merges the Ant\nColony Optimization algorithm with K-means, and it has been validated by applying a three-layer Wavelet Packet\nDecomposition (WPD) on the CWRU dataset augmented using a sliding window method to prove the applicability of\nthe method to large datasets.\nIn [11], the authors proposed a subset-based deep autoencoder model to learn discriminative features from datasets\nautomatically. This approach has been validated on the CWRU, IMS, and Self-Priming Centrifugal Pump (SPCP) [19]\nbearing vibration datasets.\nA case study on the IMS dataset is proposed in [14]. The authors leverage the knowledge of the fault frequencies of\nthe bearings to attach labels to the data. The features considered initially are TSF and Redundant Second-Generation\nWavelet Packaged Transform coefficients, though the dimensionality of the feature space is reduced by applying\n2\nA PREPRINT - SEPTEMBER 10, 2024\nRaw Input\nFeatures Extraction\nFeatures Transformation\nFeatures Evaluation\nFigure 1: The proposed framework architecture consists of three main blocks. The feature extraction block uses WPD\nto extract wavelet coefficients. These coefficients, along with statistical measures, form the output of this block. The\nfeature transformation block employs either Autoencoder or PCA to transform the extracted features, enabling the\ndetection of different behaviours in the novelty metric computation. Finally, the feature evaluation block uses six\nunsupervised machine learning models to compute the novelty metric of the transformed features.\nPrincipal Component Analysis (PCA). Lastly, K-means, Support Vector Machine (SVM) and agglomerative clustering\nare used to perform anomaly detection and to identify the failure modes.\nAdditionally, a powerful data stream classification approach is proposed in [9]. Notably, it is not a one-class approach,\nas it manages more than one “nominal” class, and deals with the emergence of novel classes during the classification\ntask by adding, to the training set, clusters of new data points with a given level of cohesion.\nReference [1] presents a machine learning framework for real-time anomaly detection in sensor data, aiding companies\nin creating datasets for predictive maintenance. It details an optimized software architecture for efficient novelty\ndetection, validated through a digital model and real-world case studies.\nFinally, a computer vision method to detect anomalies in mechanical systems is proposed in [20]. With respect to the\nprevious methods, it has the advantage of evaluating vibrations in multiple points of interest without physical contact\nwith the observed components.\nAnother model used in the context of novelty detection is the Local Outlier Factor (LOF) [21], whose continuous metric\ndepends on the position of the point and the density of known points around it. This concept has been extended to\nconsider also the clustered structure of the data in [22], which defines the metric Cluster-Based LOF (CBLOF).\nMoreover, a simple way of quantifying the normality of data using a distance metric is proposed in [23]. The authors\npropose to compute a novelty score, that is the distance of the record from the nearest cluster, normalized by the\nstandard deviation of the distance of the known points in the cluster. This method has been tested on vibration data\ncollected on a jet engine.\nAnother distance-based method is proposed in [24], as it provides a hard classification of the data as normal, extension,\nor unknown based on the radius of the closest cluster and the position of the point to be evaluated. If the point is outside\nthe decision boundary but within a tolerance, then it is classified as an extension, and the learned model is updated. If\nthe point is outside the tolerance, then it is classified as unknown. Unknown points are kept in a buffer and used to\nupdate the model when new classes emerge within the data.\n3\nMethodology\n3.1\nNovelty detection framework architecture\nThe proposed novelty detection framework is unsupervised. The training data used reflects the system’s normal\nbehaviour without any labels. Unlike classification algorithms, our framework does not have information about the\nsystem’s modes. The trained model is then used to detect anomalies in the incoming data, assessing the deviations from\nthe nominal behaviour. The proposed architecture is shown in Fig. 1.\nThe raw input are the time series collected by an accelerometer sensor. These data are then extracted into a set of key\nfeatures. These features are then transformed into a latent space that can have lower or higher dimensions. Finally,\nthe features are evaluated with several UML models producing a novelty metric (NM) (i.e. the framework’s output).\nThe advantage of this framework is related to provide deviation’s severity instead of setting a binary flag (i.e. 0,1) for\nnormal and anomalous behaviours, which is the common approach in anomaly detection tasks. Each block is better\nexplained in the following section.\n3\nA PREPRINT - SEPTEMBER 10, 2024\nnorm\n...\n...\n...\nlevel 1\nlevel L\nConcatenation\n...\nHP Filter\nLP Filter\nDownsampling\nInput \n...\nFigure 2: Wavelet Packet Decomposition features extraction architecture. At each level of decomposition, the input\nsignal is split into two sub-band signals, each with a dimension of N/2. This process is repeated through L levels,\nresulting in 2L sub-band signals. For each sub-band signal, the l2 norm is computed, condensing each array into a\nsingle value. These values are then aggregated into a final array of dimension 2L, forming the complete WPD feature\narray that encapsulates the characteristics of the original time series data.\n3.2\nFeatures Extraction\nThe features extracted from the time series are a combination of statistical and WPD coefficients. The statistical features\nextracted for each signal are: mean, root-mean-square (RMS), peak-to-peak (P2P), standard deviation (STD), skewness\nand kurtosis.\nThe WPD is a tree-based decomposition method able to extract the frequency content of a signal. In this work, a\nDaubechies wavelet is used. The decomposition method divides the input data x ∈RN into two subsets: a set with\nhigh-frequency and the other with low-frequency content, each containing N/2 elements. This process can be applied\nan arbitrary number of times (up to log2(N)) for each subset created. The process is repeated recursively until the\ndesired number of levels L is reached. Hence, the final output is a signal decomposed into 2L vectors in which each\nvector has N/2L elements. Finally, each vector generated by the decomposition is transformed into a single value\nusing the l2 norm. This value assumes the role of a feature. The 2L computed norms form the WPD feature array. The\nstructure of the decomposition is shown in Fig.2.\nIn the end, the WPD feature vector is concatenated with the six statistical features to form the complete features array\nwith dimension (2L + 6, 1) representing the final output of the features extraction block.\nAfter the feature extraction process, the data are normalized, along the whole training dataset, removing the mean and\nscaling to unit variance in order to use them for the following steps.\nAt this stage, the extracted features can be directly used to train the models. However, additional processing can be\nimplemented to analyze the different behaviours of the unsupervised model. Specifically, two different Autoencoders\nand PCA are tested to observe the change in the novelty metric, as explained in the next section.\n3.3\nFeatures Transformation\nIn this section, the feature transformation block is explained. Three different algorithms, an undercomplete and an\nover-complete autoencoder, and PCA are considered.\n3.3.1\nAutoencoder\nThe first proposed architecture contains an autoencoder (AE), a generative neural network (NN) composed of an\nencoder that increases or reduces the input dimensionality and a decoder that tries to reconstruct the original input. In\nthis kind of model, the number of neurons in the input and in the output layers must be the same, while the hidden\n4\nA PREPRINT - SEPTEMBER 10, 2024\nEncoder\nDecoder\nInput Features\nDense\nReLU\nLeaky ReLU\nFigure 3: Architecture of the proposed undercomplete autoencoder feature transformation algorithm. The NN model is\ncomposed of fully connected layers with the following shape: NE2 < NE1 < Nfeat and NE2 < ND1 < Nfeat. The\ninput is the preprocessed data obtained after the statistical and wavelet transformation.\nEncoder\nDecoder\nInput Features\nDense\nReLU\nLeaky ReLU\nFigure 4: Architecture of the proposed overcomplete autoencoder feature transformation algorithm. The NN model is\ncomposed of fully connected layers with the following shape: Nfeat < NE1 < NE2 and Nfeat < ND1 < NE2.\nlayers can vary in shape and size. The autoencoder is considered undercomplete (AER) if the latent space has a lower\ndimension than the input and overcomplete (AEA) if the latent space has a higher dimension than the input. Formally,\nlet X ∈Rn represents the input data, n represents the input dimension, the encoder fenc : Rn →Rp, and the decoder\nfdec : Rp →Rn. The autoencoder can be represented as Xrecon = fdec(h) where h is the latent space h = fenc(X).\nHence, the complete form of the eutoencoder can be represented as Xrecon = fdec(fenc(X)). The undercomplete\nautoencoder has the latent space dimension p lower than the input (p < n), while the overcomplete autoencoder has a\nhigher dimension (p > n). Both NN models have two layers for the encoder and two layers for the decoder.\nUndercomplete Autoencoder\nThe shape of the undercomplete autoencoder is shown in Figure 3. The input data are\nthe features n = Nfeat = 2L + 6 explained and extracted in Section 3.2. The layers have the following dimensions:\nNE1, NE2, ND1 where NE2 = p and NE2 < NE1 < Nfeat. Then, the decoder transforms the latent space into the\noriginal dimension Nfeat, where NE2 < ND1 < Nfeat. The first layer for both the encoder and decoder uses a ReLU\nactivation function, while the second layer uses a LeakyReLU activation function to enhance the model’s robustness\nand prevent data saturation to zero. The model employs Mean Squared Error (MSE) as the loss function.\nOvercomplete Autoencoder\nThe same considerations are performed also for the overcomplete autoencoder shown in\nFigure 4. In this configuration, the input size is increased in the hidden layers of the autoencoder. Formally, the encoder\nis structured such that Nfeat < NE1 < NE2, while the decoder follows Nfeat < ND1 < NE2. Both the encoder and\n5\nA PREPRINT - SEPTEMBER 10, 2024\ndecoder employ ReLU activation in the first layer and LeakyReLU activation in the second layer, with MSE as the loss\nfunction, similar to the undercomplete autoencoder.\n3.3.2\nPrincipal Component Analysis\nPrincipal Component Analysis is employed to transform the features array into a lower dimensional feature space\npreserving the input variance. The number of features is reduced as a function of the ratio of variance to be preserved.\nFormally, let X ∈Rn represents the input data, where n = Nfeat. The PCA function is h = PCA(X), where h is the\nlatent space with reduced features PCA : RNfeat →RNP CA with NP CA < Nfeat.\nThe three architectures (i.e. AEA, AER, PCA) provide different numbers of features as input to the unsupervised\nmodels which produce as output the novelty metric value better explained in the next section.\n3.4\nFeatures Evaluation\nHere, the six unsupervised models that can be implemented in combination with all the previous feature transformation\nmethods (sec. 3.3) are explained in detail. The input of the unsupervised models are the features transformed by the\nautoencoders or by the PCA. The output of this block is the novelty metric. When the autoencoders are used, the\nfeatures are the latent output of the last layer of the encoder, in our case E2.\n3.4.1\nK-Means\nThe K-Means algorithm is a simple and widely used clustering algorithm. The algorithm requires as input the number\nof clusters to be created, and the data to be clustered. The algorithm first initializes k centroids and then updates them\niteratively, to better fit the data. In this paper, the variant “K-means++” [25] is used.\nThe algorithm is trained multiple times with different numbers of clusters to select the best one. The silhouette score is\nthe metric used to choose the number of cluster. Formally, the silhouette score [26] s(I) of a sample I is a measure of\nhow well the sample fits in its cluster, compared to other clusters, and is defined as:\ns(I)\n=\nb(I) −a(I)\nmax(a(I), b(I))\n(1)\na(I)\n=\n1\n|Ci| −1\nX\nJ∈Ci,I̸=J\nd(I, J)\n(2)\nb(I)\n=\nmin\nk̸=i\n1\n|Ck|\nX\nJ∈Ck\nd(I, J)\n(3)\nwhere Ci is the cluster assigned to sample I, d(I, J) is the distance between samples I and J, Ck is another cluster,\n|Ci| is the number of samples in cluster Ci, and |Ck| is the number of samples in cluster Ck. Hence, a(I) is the average\ndistance between sample I and all other samples in the same cluster, and b(I) is the average distance between sample I\nand all samples in the nearest cluster. The best number of clusters is selected on the training that achieves the highest\naverage silhouette score across all samples.\nOnce the model is trained, the centroids are known and a way of measuring how novel the new data are is needed. In\nthis paper, for the k-means models, we propose a Novelty Metric similar to the one in [23]. The approach involves\ncomputing the distance from a new sample to the closest cluster’s centroid, comparing this distance with the cluster’s\nradius, and normalizing by this radius (instead of by the standard deviation of the samples in the cluster, as done in\n[23]). The NM is defined as:\nNMKMeans(I) = d(I, ci) −ri\nri\n,\nri = max\nJ∈Ci d(J, ci)\n(4)\nwhere I is the sample to be evaluated, ci is the closest centroid to I and ri is the radius of the closest cluster. This\nnormalization accounts for the cluster size while preserving the property of the NM being negative for samples strictly\ninside the clusters, zero for samples on the boundary of a cluster, and positive for novel samples.\n3.4.2\nDBSCAN\nAnother clustering algorithm tested is the DBSCAN [27]. It works by dividing the data into “core” and “reachable”\npoints, based on the minimum cluster size MinPts and the search radius ε. It has the ability to discard some samples in\nthe training data as noise. In this case, the novelty metric for a new sample I is defined as follows:\n6\nA PREPRINT - SEPTEMBER 10, 2024\nNMDBSCAN(I) = 1\nε min\nJ∈D d(I, J)\n(5)\nWhere D is the set of samples in the train data that are not marked as noise.\n3.4.3\nGMM\nThe Gaussian Mixture Model (GMM) is a probabilistic model that assumes that the data is generated from a mixture of\nseveral Gaussian distributions. It estimates the parameters of each distribution. In this case, the novelty metric is the\nlikelihood that a given sample belongs to the distribution mixture.\n3.4.4\nnuSVM\nThe one-class Support Vector Machine (nuSVM) is a variant of the standard SVM that applies a transformation to the\ninput space in order to separate the data from the origin. New data samples that appear closer to the origin than the\ntraining samples are considered outliers.\nIn this case, we consider the distance of the sample from the separating hyperplane, as proposed by [28], as novelty\nmetric NMnuSVM(i).\n3.4.5\nIF\nThe Isolation Forest (IF) is an ensemble algorithm that trains a set of decision trees to isolate the outliers. The sample is\ncompared with a threshold at each node of the tree, until a leaf is reached. An outlier is expected to reach a leaf in a few\nsteps because it is dissimilar to known samples, while an inlier is expected to reach a leaf after many steps, because it is\nsimilar to known samples. We use as novelty metric NMIF(i) the “anomaly score” defined by the authors [29].\n3.4.6\nLOF\nThe Local Outlier Factor (LOF) is a density-based algorithm that computes the local density of a sample compared to\nthe local density of its neighbours. The algorithm then returns a metric that indicates “the degree of being an outlier”\n[30]. We consider the result of the algorithm as the NMLOF(i).\n4\nExperiments\n4.1\nDataset acquisition\nA pivotal point of our research is the realization of a dataset to test the Novelty Detection algorithms in a real environment.\nFor this purpose, a shaker (mod. n◦K2007E01) and an accelerometer (mod. n◦STM32L4R9, an evaluation board of\nST microelectronics equipped with an accelerometer sensor) were used to collect a vibration dataset. The evaluation\nboard is mounted on the shaker to gather the dataset with reduced noise and uncertainties.\nThe shaker is controlled by a PC, reproducing an audio file. Two audio files have been generated, the first reproduces a\nsignal v1(t) and the second one the signal v2(t) that we define as:\nv1(t) =\n9\nX\ni=1\nAi · sin(2π · αi · t))\n(6)\nA = {0.5, 0.5, 0.5, 1, 1, 1, 0.5, 0.5, 0.5} α = {50, 100, 150, 230, 300, 440, 460, 530, 600}\nv2(t) =\n9\nX\ni=1\nBi · sin(2π · βi · t))\n(7)\nB = [0.5, 0.5, 0.5, 1, 0.2, 1, 0.5, 0.5, 2] β = [50, 100, 150, 230, 300, 440, 460, 530, 600]\nwhere A, B are the weights and α, β are the frequencies of the harmonics. The volume setting is used to change the\nP2P amplitude of the physical signal generated by the PC.\n7\nA PREPRINT - SEPTEMBER 10, 2024\nFirstly, the signal v1 was fed to the shaker, with five different volume settings, acquiring five sets of data (set 1 - set 5).\nEvery time the volume was changed, the new P2P value was measured with the oscilloscope. Lastly, this procedure\nwas repeated using the signal v2, to record three more sets (set 6 - set 8). Each of the eight sets of vibration data\nacquired consists of a record that lasts 206 seconds and has been split into 1-second chunks to generate 206 records.\nThe setup used to collect the eight sets of vibration data is resumed in Table 1.\nThe highest frequency chosen is 600Hz, hence the sampling frequency of the evaluation board is set to fs = 1666Hz\nrespecting the Nyquist–Shannon sampling theorem.\n4.2\nFeatures extraction and normalization\nThe procedure described in Section 3.2 is applied to the experimental data collected. The depth level L of the WPD\ndecomposition tree is chosen to be 6, obtaining a WPD feature array with 64 elements. The 6 statistical features are\nalso computed, yielding a final feature array with 70 elements.\n4.3\nModels comparison\nThe performances of each architecture need to be evaluated in order to perform the benchmark analysis for each\narchitecture. Four different performance metrics are computed for each framework.\n4.3.1\nVariance\nThe variance of the novelty metric is evaluated using the remaining part of the nominal data not used for the training. A\ngood model should produce a stable novelty metric when the data represent the same normal conditions. High variance\nmeans that the model is either too sensitive to noise or cannot generalize normal behaviour well. Formally, this metric\nis defined as:\nV ariance = 1\nn\nn\nX\ni=1\n\u0000NM(i) −Ě\nNM\n\u00012\n(8)\nwhere NM(i) is the novelty metric of the i-th normal sample, Ě\nNM is the mean of the novelty metric along all the normal\nsamples, and n is the number of samples.\n4.3.2\nReactivity\nThis metric is used to compare different models under the same conditions. The reactivity of the model is defined as the\ndifference between the mean nominal and worst novelty metric. High reactivity indicates that the model can better\ndistinguish between nominal and novelty samples. Formally, the reactivity is defined as:\nReactivity = 1\nn\nn\nX\ni=1\nNM(i) −1\nm\nm\nX\nj=1\nNM(j)\n(9)\nwhere NM(i) is the novelty metric of the i-th nominal sample and NM(j) is the novelty metric of the j-th novel sample.\nTable 1: Experimental setup for data collection.\nSet name\nSignal Type\nP2P [V]\nSet 1\nv1(t)\n0.25\nSet 2\nv1(t)\n0.50\nSet 3\nv1(t)\n0.75\nSet 4\nv1(t)\n1.00\nSet 5\nv1(t)\n1.25\nSet 6\nv2(t)\n0.50\nSet 7\nv2(t)\n0.75\nSet 8\nv2(t)\n1.00\n8\nA PREPRINT - SEPTEMBER 10, 2024\nTable 2: Hyperparameters ranges chosen for the models’ optimization algorithm\nTransf.\nMethod\nNE1, ND1\nNE2\nlr\nbs\nnf\nAEA\n50 −65\n10 −45\n0.01 −0.1\n32, 64\n-\nAER\n75 −80\n85 −100\n0.01 −0.1\n32, 64\n-\nPCA\n-\n-\n-\n-\n2 −70\nTable 3: Tuned hyperparameters. AEA is the overcomplete autoencoder, AER is the undercomplete autoencoder\nModel\nNE1, ND1\nNE2\nlr\nbs\nnf\nAEA AER AEA AER AEA AER AEA AER PCA\nKMeans\n80\n61\n85\n32\n0.08\n0.03\n64\n64\n3\nDBSCAN\n75\n56\n85\n10\n0.20\n0.08\n32\n64\n2\nGMM\n79\n61\n85\n10\n0.08\n0.01\n32\n64\n2\nnuSVM\n80\n65\n93\n10\n0.09\n0.10\n32\n64\n2\nIForest\n79\n50\n85\n45\n0.02\n0.03\n64\n64\n70\nLOF\n79\n50\n88\n10\n0.03\n0.01\n32\n32\n3\n4.3.3\nInference Time\nThe inference time to evaluate a single sample (i.e. the time interval used by the UML model to compute the NM). This\nmetric is averaged by evaluating several samples. The inference time is measured using an Intel CPU i9-12900K.\n4.3.4\nFeature Percentage\nThe Feature Percentage (FP) indicates the percentage of dimensionality preserved after the feature transformation with\nrespect to the original ones. An FP of 100% means that all original features are retained. A lower FP is preferable for\nfaster computations and to avoid the challenges posed by high-dimensional data (the curse of dimensionality).\n4.4\nModels Optimization\nThe algorithms, described in section 3.3, are also optimized to select the best-performing one for each UML model.\nConcerning the autoencoder, the optimization process is performed by changing the number of neurons of the encoder\nNE1 which is the same for the decoder NE1 = ND1, the number of neurons of the latent space of the encoder NE2,\nthe learning rate, and the batch size. For the PCA, the only parameter changed during the optimization is the number\nof latent features nf. The variance is chosen as objective function because, at the time of optimization, it is the only\nvariable available without knowledge of the future of the system (e.g. reactivity). Minimizing this variance is crucial\nbecause it reflects the algorithm’s ability to model a nominal condition of the system. A lower variance indicates better\nperformance because it suggests that the model is less susceptible to noise or environmental changes during the normal\noperation of the system.\nThe optimization used is a Gaussian process-based Bayesian algorithm. It iteratively suggests values for the hyperpa-\nrameters that optimize the objective function, aiming to improve the model’s robustness and detection capabilities.\nThe first 100 samples of set 1 of Table 1 are used for training, while the remaining 106 samples of the same set are used\nto compute the variance metric. Formally:\nJ = min(σ100NM)\nσ100(NM) →NE1,D1, NE2, lr, bs, nf\nwhere J is the objective function to minimize, lr is the learning rate, bs is the batch size, and nf is the number of PCA’s\nlatent features. The sampler needs a range of values to optimize in order to reach the minimum variance. The ranges\nchosen for each variable are reported in Table 2.\nThe parameters obtained by applying the optimization process are reported in Table 3.\n9\nA PREPRINT - SEPTEMBER 10, 2024\n10−9\n10−1\nVariance\n10−5\n10−1\nMean Train\n100\nMean Test\n100\nReactivity\n10−6\n10−5\n10−4\nInf. time [s]\nKmeans DBSCAN GMM\nnuSVM\nIForest\nLOF\nOF\nAER\nAEA\nPCA\nFigure 5: Graphical representation of performance metrics reported on Table 4. All the y axes are displayed in a\nlogarithmic scale. AEA is the overcomplete autoencoder, AER is the undercomplete autoencoder and OF is the original\nfeatures.\n0\n20\n40\n60\n80\nn of transformed features\n10−6\n10−5\n10−4\nInference time\n0\n25\n50\n75\n100\n125\nTransformed features percentage\n0.7\n0.8\n0.9\nReactivity\nKMeans\nDBSCAN\nGMM\nnuSVM\nIForest\nLOF\nFigure 6: In the upper plot, the correlation between the inference time and the number of features. In the bottom plot,\nthe correlation between the reduced feature dimensionality ratio FP and the reactivity.\n10\nA PREPRINT - SEPTEMBER 10, 2024\nTable 4: Performance metrics for all the preprocessing and model combinations. OF means original features, AER\nmeans autoencoder reduced, AEA means autoencoder augmented. nf is the number of features transformed by the\nautoencoders or PCA. FP is the features percentage with respect to the original dimension. Variance defined in the\nEquation 8, and the reactivity defined in Equation 9. Train signal is set 1 of Table 1 and test signal is set 5. The symbols\n⇓and ⇑indicate if the value should ideally be minimized or maximized, respectively. The best results among all the\nUML models and features transformation combinations are highlighted with *, for each metric.\nUML model Transformation nf FP [%] ⇓Variance [10−3] ⇓\nMean\nReactivity ⇑Inference Time [µs] ⇓\nNominal signal ⇓Novelty Signal ⇑\nKMeans\nOF\n70\n100.00\n0.2680\n0.0063\n0.9686\n0.9623\n241.4\nAER\n32\n45.71\n0.1098\n0.0072\n0.9352\n0.9280\n210.8\nAEA\n85\n121.43\n0.1615\n0.0013\n0.9612\n0.9599\n212.5\nPCA\n3\n4.29\n0.0780\n0.0190\n0.8860\n0.8669\n193.1\nDBSCAN\nOF\n70\n100.00\n0.2994\n0.0119\n0.9835\n0.9716\n11.3\nAER\n10\n14.29\n0.0957\n0.0035\n0.9243\n0.9208\n7.2\nAEA\n85\n121.43\n0.0776\n0.0095\n0.9234\n0.9139\n8.0\nPCA\n2*\n2.86*\n0.0111\n0.0043\n0.9093\n0.9050\n6.5\nGMM\nOF\n70\n100.00\n0.0000 *\n0.0000 *\n0.9479\n0.9479\n22.8\nAER\n10\n14.29\n0.0009\n0.0003\n0.8850\n0.8847\n8.1\nAEA\n85\n121.43\n0.0122\n0.0005\n0.9287\n0.9282\n30.9\nPCA\n2*\n2.86*\n0.0002\n0.0006\n0.8321\n0.8316\n5.7\nnuSVM\nOF\n70\n100.00\n23.1632\n0.3165\n1.0000 *\n0.6835\n3.0\nAER\n10\n14.29\n7.9910\n0.0683\n1.0000 *\n0.9317\n0.9*\nAEA\n93\n132.86\n2.1082\n0.0580\n1.0000 *\n0.9420\n1.3\nPCA\n2*\n2.86*\n2.3468\n0.0630\n1.0000 *\n0.9370\n1.0\nIForest\nOF\n70\n100.00\n4.5572\n0.1176\n0.8893\n0.7717\n6.1\nAER\n45\n64.29\n14.9819\n0.2134\n0.9192\n0.7058\n5.3\nAEA\n85\n121.43\n16.2062\n0.1836\n0.9462\n0.7625\n5.3\nPCA\n70\n100.00\n12.2452\n0.2568\n0.9823\n0.7255\n5.9\nLOF\nOF\n70\n100.00\n0.2762\n0.0028\n0.9804\n0.9777 *\n95.9\nAER\n10\n14.29\n0.0816\n0.0029\n0.9157\n0.9128\n2.7\nAEA\n88\n125.71\n0.0522\n0.0015\n0.9448\n0.9432\n4.5\nPCA\n3\n4.29\n0.0064\n0.0032\n0.8838\n0.8805\n4.2\n4.5\nModel evaluation\nPerformance metrics are computed across all combinations of UML models and feature transformation methods.\nThe first 100 samples of set 1 are used to train the UML models. Four combinations are benchmarked for each\nUML model: a test is performed without any features transformation using only the features extracted by the features\nextraction block (we refer to this scenario with OF, which stands for “Original Features”). The other three use the\nundercomplete and overcomplete AE, and the PCA as transformation algorithms.\nThe remaining 106 samples of set 1 are used to compute the variance and the mean of the nominal signal (i.e. “mean -\nnominal signal”.\nThe 206 samples of Set 5, are used as a reference test to compute the “mean - novelty signal” and consequently the\nreactivity. Set 5 is selected as the reference novelty test because it represents the most dissimilar signal among the sets.\nThe novelty metric produced by all models is computed and normalized to a common scale where min(NM) = 0 and\nmax(NM) = 1 for a fair comparison.\nThe resulting performance metrics are reported in Table 4, where the best transformation method for each UML model,\nfor each metric, is highlighted in bold. Additionally, the best results among all the UML models and transformation\ncombinations are highlighted with an asterisk, for each metric. The same results are also visually depicted in Figure 5\nfor clearness.\nLooking at Table 4, we make some considerations about the best UML models and the best transformation methods.\n11\nA PREPRINT - SEPTEMBER 10, 2024\nRegarding the dimensionality reduction of the feature space, the PCA is almost always the transformation that selects\nthe lowest number of output features, with an exception that arises when coupled with the IForest UML model. In this\ncase, the transformation method that further reduces the dimensionality is the AER.\nThe GMM is the model that produces the least NM variance in nominal conditions (i.e. the NM produced in nominal\nconditions is almost constant). Concerning the other UML models, KMeans, DBSCAN and LOF experience the least\nvariance when coupled with PCA feature transformation, while IForest produces the most stable NM when used together\nwith original features.\nIdeally, when evaluating a nominal signal, the NM should be low. The model that minimizes this value is the GMM\nused without any feature transformation. Considering the rest of the UML models, there is no feature transformation\nthat appears to minimize this performance metric for most models.\nOn the other hand, the NM should be high when evaluating a novel signal. The nuSVM model, produces the highest\nNM regardless the transformations applied, but we observed that this property comes at the cost of a lower capability of\ngenerating a continuous NM value, as it tends to saturate to high values even when the signal differs only slightly from\nthe nominal one.\nThe reactivity appears to be the highest when the OF are used to feed the UML models, except for the nuSVM that has\nthe highest reactivity when combined with AEA. The analysis reveals that the LOF model with the original features\nshows the highest reactivity, making it the optimal model. The DBSCAN follows behind, demonstrating a similar\nreactivity value.\nThe nuSVM is the one that computes the NM the fastest. This is due to the simplicity of the inference procedure, which\nconsists of a distance calculation. Regarding the other UML models, we observe that the fastest response happens when\nthe dimensionality of the feature space is the lowest.\nSome observations can be made about the correlation of the performance metrics. Each metric has been plotted against\neach other to observe emerging patterns. The two most informative plots are shown in Fig. 6. Regarding the correlation\nbetween the inference time and the number of features, the general trend suggests that the evaluation process becomes\nslower as the dimensionality of the feature space increases. In the second plot, the trend suggests that modifying the\nfeature dimensionality (FP ̸= 100) reduces the reactivity of the UML models, except for nuSVM, which exhibits an\ninverse behaviour.\nFor completeness, the novelty metric was computed across the remaining data sets (set 1 - set 8), excluding the portion\nof set 1 used for training the models. The evolution of the novelty metric along all the sets for each combination is\nshown in Fig. 7. It appears evident that KMeans, DBSCAN, GMM and LOF provide a NM that is clearly related to the\ndissimilarity of the evaluated signal with respect to the nominal condition (at least related to the amplitude increase).\nOn the other hand, as anticipated earlier, nuSVM and IForest exhibits a tendency to saturate the NM to a constant value,\nwith an abrupt gap (almost as a binary flag). This behaviour is in contrast with the scope of our work to provide a\ncontinuously changing NM that quantifies the severity of the anomaly.\nThe last behaviour that emerges from Fig. 7 is that GMM is quite sensitive to the feature transformation applied. When\nGMM is evaluating the OF, the NM appears to be lower than when evaluating transformed features.\n4.6\nCode availability\nAll the functions and scripts used to obtain the results shown in this report will be available online2.\n5\nConclusion\nIn this work, we benchmark several unsupervised machine learning models by testing different feature transformation\nalgorithms. We also gather a real vibration dataset to evaluate all frameworks with real-world data, including noise.\nThe results indicate that a continuous Novelty Metric performs better with certain models — specifically KMeans,\nDBSCAN, GMM, and LOF — by providing an indication of novelty severity. In contrast, models like nuSVM and IF\nbehave as a binary flag. The benchmark highlights the significance of proper feature extraction and transformation\nalgorithms in changing the behaviour of unsupervised models. Additionally, we compute each framework’s inference\ntimes to assess the complexity of the frameworks in terms of time consumption. Future work will aim to deploy and test\neach framework on embedded devices to provide an EDGE benchmark. We also plan to test other feature extraction\n2https://github.com/PIC4SeR/Unsupervised-Novelty-Detection-Methods-Benchmarking-with-Wavelet-Dec\nomposition.git\n12\nA PREPRINT - SEPTEMBER 10, 2024\n0.0\n0.5\n1.0\nKMeans\n0.0\n0.5\n1.0\nDBSCAN\n0.0\n0.5\n1.0\nGMM\n0.0\n0.5\n1.0\nnuSVM\n0.0\n0.5\n1.0\nIForest\n0\n250\n500\n750\n1000\n1250\n1500\nSamples\n0.0\n0.5\n1.0\nLOF\nNormalized novelty metric\nNovelty metric for the different models\nOF\nAER\nAEA\nPCA\nFigure 7: Normalized novelty metric behaviour for all the combinations of UML models and feature transformation\ntechnique. In the plots, the steps (discontinuities) in the NM represent the instants in which the framework starts to\nevaluate a different set of data, with different characteristics. KMeans, DBSCAN and LOF appear less affected by\nthe different feature transformation techniques if compared with the other UML models. The nuSVM novelty metric\nevolution shows a clear saturation for all the samples representative of novel behaviours. Moreover, it fails to produce a\nsteady NM when evaluating samples of known modes. The Iforest model experiences almost the same saturation as the\nnuSVM. IForest appears also sensitive to the feature transformation used.\n13\nA PREPRINT - SEPTEMBER 10, 2024\nalgorithms, such as generative models like RealNVP, to determine if better results can be achieved. Finally, we will\nextend the benchmark tests to other industrial datasets to further investigate the presented study.\nAcknowledgment\nThis work has been developed with the contribution of Politecnico di Torino Interdepartmental Centre for Service\nRobotics PIC4SeR3.\nReferences\n[1] Umberto Albertin, Giuseppe Pedone, Matilde Brossa, Giovanni Squillero, and Marcello Chiaberge. A real-time\nnovelty recognition framework based on machine learning for fault detection. Algorithms, 16(2), 2023.\n[2] Adrián Vega, Ignacio Crespo-Martínez, Ángel Guerrero-Higueras, Claudia Álvarez Aparicio, Vicente Matellán,\nand Camino Fernández. Malicious traffic detection on sampled network flow data with novelty-detection-based\nmodels. Scientific Reports, 13, 09 2023.\n[3] Jeffrey Schein, Steven T Bushby, Natascha S Castro, and John M House. A rule-based fault detection method for\nair handling units. Energy and buildings, 38(12):1485–1492, 2006.\n[4] Rolf Isermann. Model-based fault-detection and diagnosis–status and applications. Annual Reviews in control,\n29(1):71–85, 2005.\n[5] Jihoon Chung, Bo Shen, and Zhenyu James Kong. Anomaly detection in additive manufacturing processes using\nsupervised classification with imbalanced sensor data based on generative adversarial network. J. Intell. Manuf.,\n35(5):2387–2406, 6 2023.\n[6] Fa Zhu, Wenjie Zhang, Xingchi Chen, Xizhan Gao, and Ning Ye. Large margin distribution multi-class supervised\nnovelty detection. Expert Systems with Applications, 224:119937, 2023.\n[7] Carlo Cena, Umberto Albertin, Mauro Martini, Silvia Bucci, and Marcello Chiaberge. Physics-Informed Real\nNVP for Satellite Power System Fault Detection. In IEEE/ASME International Conference on Advanced Intelligent\nMechatronics (AIM), 2024.\n[8] Umberto Albertin, Alessandro Navone, Mauro Martini, and Marcello Chiaberge. Semi-supervised novelty\ndetection for precise ultra-wideband error signal prediction, 2024.\n[9] Mohammad Masud, Jing Gao, Latifur Khan, Jiawei Han, and Bhavani M. Thuraisingham. Classification and\nnovel class detection in concept-drifting data streams under time constraints. IEEE Transactions on Knowledge\nand Data Engineering, 23(6):859–874, 2011.\n[10] Mohammed Chalouli, Nasr-eddine Berrached, and Mouloud Denai. Intelligent health monitoring of machine\nbearings based on feature extraction. Journal of Failure Analysis and Prevention, 17(5):1053–1066, 10 2017.\n[11] Yuyan Zhang, Xinyu Li, Liang Gao, and Peigen Li. A new subset based deep feature learning method for\nintelligent fault diagnosis of bearing. Expert Systems with Applications, 110:125–142, 2018.\n[12] Qicai Zhou, Hehong Shen, Jiong Zhao, Xingchen Liu, and Xiaolei Xiong. Degradation state recognition of rolling\nbearing based on k-means and cnn algorithm. Shock and Vibration, 2019(1):8471732, 2019.\n[13] Luis A. Pinedo-Sánchez, Diego A. Mercado-Ravell, and Carlos A. Carballo-Monsivais. Vibration analysis in\nbearings for failure prevention using cnn. Journal of the Brazilian Society of Mechanical Sciences and Engineering,\n42(12):628, 11 2020.\n[14] Cecilia Gattino, Elia Ottonello, Mario Baggetta, Roberto Razzoli, Jacek Stecki, and Giovanni Berselli. Application\nof ai failure identification techniques in condition monitoring using wavelet analysis. The International Journal of\nAdvanced Manufacturing Technology, 125(9):4013–4026, 04 2023.\n[15] J. Lee, H. Qiu, G. Yu, J. Lin, and Rexnord Technical Services. Bearing data set. IMS, University of Cincinnati,\nNASA Prognostics Data Repository, NASA Ames Research Center, Moffett Field, CA, 2007.\n[16] Case Western Reserve University. Bearing data center: Seeded fault test data. http://data-acquisition.c\nase.edu/. Accessed: 2024-06-05.\n[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural\nnetworks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information\nProcessing Systems, volume 25. Curran Associates, Inc., 2012.\n3www.pic4ser.polito.it\n14\nA PREPRINT - SEPTEMBER 10, 2024\n[18] Lanjun Wan, Gen Zhang, Hongyang Li, and Changyun Li. A novel bearing fault diagnosis method using\nspark-based parallel aco-k-means clustering algorithm. IEEE Access, 9:28753–28768, 2021.\n[19] Chen Lu, Yang Wang, Minvydas Ragulskis, and Yujie Cheng. Fault diagnosis for rotating machinery: A method\nbased on image processing. PLOS ONE, 11(10):1–22, 10 2016.\n[20] Jakub Spytek, Adam Machynia, Kajetan Dziedziech, Ziemowit Dworakowski, and Krzysztof Holak. Novelty de-\ntection approach for the monitoring of structural vibrations using vision-based mean frequency maps. Mechanical\nSystems and Signal Processing, 185:109823, 2023.\n[21] Markus Breunig, Peer Kröger, Raymond Ng, and Joerg Sander. Lof: Identifying density-based local outliers.\nvolume 29, pages 93–104, 06 2000.\n[22] Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. Pattern Recognition\nLetters, 24(9):1641–1650, 2003.\n[23] David A. Clifton, Peter R. Bannister, and Lionel Tarassenko. Learning shape for jet engine novelty detection. In\nJun Wang, Zhang Yi, Jacek M. Zurada, Bao-Liang Lu, and Hujun Yin, editors, Advances in Neural Networks -\nISNN 2006, pages 828–835, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.\n[24] Kemilly Dearo Garcia, Mannes Poel, Joost N. Kok, and André C. P. L. F. de Carvalho. Online clustering for\nnovelty detection and concept drift in data streams. In Progress in Artificial Intelligence, pages 448–459, Cham,\n2019. Springer International Publishing.\n[25] David Arthur and Sergei Vassilvitskii. K-means++: The advantages of careful seeding. volume 8, pages\n1027–1035, 01 2007.\n[26] Peter J. Rousseeuw. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of\nComputational and Applied Mathematics, 20:53–65, 1987.\n[27] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering\nclusters in large spatial databases with noise. In kdd, volume 96, pages 226–231, 1996.\n[28] Bernhard Schölkopf, Robert Williamson, Alex Smola, John Shawe-Taylor, and John Platt. Support vector method\nfor novelty detection. volume 12, pages 582–588, 01 1999.\n[29] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee international conference\non data mining, pages 413–422. IEEE, 2008.\n[30] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying density-based local\noutliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages\n93–104, 2000.\n15\n",
  "categories": [
    "cs.LG",
    "I.2.6; J.2"
  ],
  "published": "2024-09-11",
  "updated": "2024-09-11"
}