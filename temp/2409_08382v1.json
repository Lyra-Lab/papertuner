{
  "id": "http://arxiv.org/abs/2409.08382v1",
  "title": "Stochastic Reinforcement Learning with Stability Guarantees for Control of Unknown Nonlinear Systems",
  "authors": [
    "Thanin Quartz",
    "Ruikun Zhou",
    "Hans De Sterck",
    "Jun Liu"
  ],
  "abstract": "Designing a stabilizing controller for nonlinear systems is a challenging\ntask, especially for high-dimensional problems with unknown dynamics.\nTraditional reinforcement learning algorithms applied to stabilization tasks\ntend to drive the system close to the equilibrium point. However, these\napproaches often fall short of achieving true stabilization and result in\npersistent oscillations around the equilibrium point. In this work, we propose\na reinforcement learning algorithm that stabilizes the system by learning a\nlocal linear representation ofthe dynamics. The main component of the algorithm\nis integrating the learned gain matrix directly into the neural policy. We\ndemonstrate the effectiveness of our algorithm on several challenging\nhigh-dimensional dynamical systems. In these simulations, our algorithm\noutperforms popular reinforcement learning algorithms, such as soft\nactor-critic (SAC) and proximal policy optimization (PPO), and successfully\nstabilizes the system. To support the numerical results, we provide a\ntheoretical analysis of the feasibility of the learned algorithm for both\ndeterministic and stochastic reinforcement learning settings, along with a\nconvergence analysis of the proposed learning algorithm. Furthermore, we verify\nthat the learned control policies indeed provide asymptotic stability for the\nnonlinear systems.",
  "text": "Stochastic Reinforcement Learning with Stability Guarantees\nfor Control of Unknown Nonlinear Systems\nThanin Quartz1, Ruikun Zhou1, Hans De Sterck1, Jun Liu 1\n1University of Waterloo\n{tquartz, ruikun.zhou, hans.desterck, j.liu}@uwaterloo.ca\nAbstract\nDesigning a stabilizing controller for nonlinear systems is a\nchallenging task, especially for high-dimensional problems\nwith unknown dynamics. Traditional reinforcement learning\nalgorithms applied to stabilization tasks tend to drive the sys-\ntem close to the equilibrium point. However, these approaches\noften fall short of achieving true stabilization and result in\npersistent oscillations around the equilibrium point. In this\nwork, we propose a reinforcement learning algorithm that sta-\nbilizes the system by learning a local linear representation of\nthe dynamics. The main component of the algorithm is inte-\ngrating the learned gain matrix directly into the neural policy.\nWe demonstrate the effectiveness of our algorithm on several\nchallenging high-dimensional dynamical systems. In these\nsimulations, our algorithm outperforms popular reinforcement\nlearning algorithms, such as soft actor-critic (SAC) and prox-\nimal policy optimization (PPO), and successfully stabilizes\nthe system. To support the numerical results, we provide a\ntheoretical analysis of the feasibility of the learned algorithm\nfor both deterministic and stochastic reinforcement learning\nsettings, along with a convergence analysis of the proposed\nlearning algorithm. Furthermore, we verify that the learned\ncontrol policies indeed provide asymptotic stability for the\nnonlinear systems.\n1\nIntroduction\nIn recent years reinforcement learning (RL) has achieved\nconsiderable success in complex games (Silver et al. 2016),\n(Vinyals et al. 2019) and has received attention from the\ncontrol community due to its ability to learn controllers\nin complex high-dimensional control tasks (Lillicrap et al.\n2015), (Schulman et al. 2017). A central problem in this\ncontext is stability. In RL, stability is closely related to\nsafety, and safety requirements that cannot be directly\nlearned during the training phase reduce the reliability of\nthese algorithms when applied to real-world safety-critical\nproblems outside of simulations or lab environments. To\naddress these challenges in safety-critical tasks, researchers\nhave proposed solutions such as filters designed to block\nunwanted actions, which may involve human-based design\n(Saunders et al. 2017) or formal guarantees (Alshiekh et al.\n2018). A detailed overview of techniques in safe RL can\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nbe found in (Pecka and Svoboda 2014) and (Garc´ıa and\nFern´andez 2015).\nIn the control community, stability guarantees can be derived\nin the linear case and its properties are well studied. Un-\nfortunately, designing a stabilizing controller for nonlinear\nsystems and estimating its region of attraction is one of the\nmost challenging tasks, especially for systems with uncertain-\nties (Matallana, Blanco, and Bandoni 2010). In this setting,\nit is common to design a controller for the linearized system\naround some neighborhood of the equilibrium point. While\nthis formulation provides local guarantees, it restricts stability\nand performance to a small neighborhood around the equilib-\nrium point. In cases where the system is unknown or partially\nknown, finding a stabilizing controller using classical meth-\nods becomes nearly impossible. As a result, techniques that\nrely on initial stabilizing controllers, such as adaptive dy-\nnamic programming (Jiang and Jiang 2017), may not be fea-\nsible for optimal control design. To overcome the limitations\nassociated with local guarantees, we propose a reinforcement\nlearning algorithm to integrate local stabilization directly into\nthe neural policy. Our algorithm assumes no prior knowledge\nof the system dynamics. To achieve local stabilization, we\niteratively estimate the local state-space representation and\ncompute the local optimal gain matrix using LQR. In contrast\nto many classical methods, we do not assume knowledge of\nan initial stabilizing policy or a Lyapunov function as the pro-\ncess of learning and verifying a Lyapunov function is often\ncomplex and challenging (Zhou et al. 2022).\nRelated Work\nThe idea of combining reinforcement learning with control\ntheory has received considerable attention in recent years.\nFor example, the work by (Zanon and Gros 2021) and (Beck-\nenbach, Osinenko, and Streif 2019) combine reinforcement\nlearning with techniques from stabilizing model predictive\ncontrol (MPC). Nevertheless, the primary focus of merging\ntechniques in control theory and reinforcement learning has\nbeen on the development of Lyapunov-based reinforcement\nlearning methods. A detailed account on the aforementioned\nmethods can be found in (Osinenko, Dobriborsci, and\nAumer 2022). However, despite recent progress, stabilization\nremains a challenging task, and these approaches often\nassume the existence of a local stabilizing controller and\ntypically test their applicability on low-dimensional systems.\nInitial work combining Lyapunov theory and RL was pro-\nposed in (Perkins and Barto 2003) where a control policy\nlearns to switch among a number of base-level controllers\nand are designed using Lyapunov domain knowledge. More\nrecently, Lyapunov theory has been employed for safe rein-\nforcement learning, as demonstrated in (Berkenkamp et al.\n2016), with subsequent model-based advancements detailed\nin (Berkenkamp et al. 2017). An approach to local stabiliza-\ntion, assuming knowledge of the model dynamics, is dis-\ncussed in (Zoboli et al. 2021). In this approach, a global\ncontroller is learned using an actor-critic method, and the\nLQR optimal gain controller is designed to linearly domi-\nnate around the equilibrium point. The paper also formulates\nconditions for ensuring local stability in their policy design.\nAnother approach is presented in (Han et al. 2020) where the\nauthors use a Lyapunov function as the critic for guaranteeing\nstability in the mean cost of the learnt policy and prove sta-\nbilization under high probability. An alternative constrained\noptimization approach for the actor’s objective function in the\nactor-critic method was proposed in (Osinenko et al. 2020).\nThe authors demonstrated practical semi-global stabilization\nof the sample hold closed-loop controller. The same authors\nlater applied a similar technique to constrain the actor policy\nfor systems with unknown parameters, establishing closed-\nloop stability guarantees as discussed in (G¨ohrt et al. 2020).\nOur Contribution\nWe summarize the main contributions of the paper as follows:\n• We propose an algorithm that learns to stabilize un-\nknown high dimensional nonlinear systems without\nknowledge of an initial stabilizing controller. The main\ncomponent of the algorithm integrates the learned\ngained matrix directly into the neural policy. The al-\ngorithm is then shown to stabilize several challenging\nnonlinear systems.\n• We prove that reinforcement learning algorithms typi-\ncally only guarantee practical stability and we prove the\nconvergence for our learning algorithm to an asymptoti-\ncally stabilizing neural policy.\n• We verify that the learned neural policy indeed stabilizes\nthe systems using an SMT solver-based toolbox in the\nsense of Lyapunov stability.\n2\nPreliminaries\nNotation\nThe following notations will be used in the paper.\nLet R := (−∞, ∞), R≥0 := [0, ∞), N := {1, 2, . . .}. A\nfunction α : R≥0 →R≥0 is of class K if it is continu-\nous, zero at zero and strictly increasing, and it is of class\nK∞if, in addition, it is unbounded. A continuous function\nα : R2\n≥0 −→R≥0 is of class KL if for each t ∈R≥0, α(·, t)\nis of class K, and, for each s > 0, α(s, ·) is decreasing\nto zero. The Euclidean norm of a vector x ∈Rn is de-\nnoted by ∥x∥. On a compact set D we denote the ∞-norm\n∥f∥D := supx∈D |f(x)|. Let P be a real, square, and sym-\nmetric matrix, λmax(P) and λmin(P) are respectively the\nlargest and the smallest eigenvalue of P.\nDynamic Programming\nThroughout this work we consider discrete dynamical sys-\ntems generating a sequence of states {xk}∞\nk=1, x ∈D ⊂Rn,\nwhere D is a compact and convex set, starting from some\ninitial state x0 ∈Rn by the nonlinear control system\nxk+1 = f(xk, uk)\n(1)\nwhere {uk}∞\nk=1, u ∈U ⊂Rm are a sequence of control\ninputs. Moreover, we assume that the right hand side of (1)\nis Lipschitz to guarantee a unique solution.\nAssumption 1. Suppose that the vector field f(x, u) is Lips-\nchitz. That is, there exists constants Lx, Lu such that\n∥f(x, u) −f(y, w)∥≤Lx∥x −y∥+ Lu∥u −w∥,\n(2)\nfor all x, y ∈D and u, w ∈U.\nAs is common for many tasks suitable for RL, we assume\nthat we do not have explicit knowledge of the right hand side\nof (1). For this discrete system, denote an infinite sequence of\ncontrol inputs as u = (u1, u2, . . .) and consequently denote\nthe solution at the kth time state with initial condition x0 as\nΨ(k, x0, u(k)) where u(k) = uk. The function Ψ is called\nthe flow map. Given a reward function r : Rn × Rm →R\nwe define the discounted objective function.\nDefinition 1 (Objective Function). The γ-discounted objec-\ntive function, Ju\nγ : Rn →R is defined as\nJu\nγ(x0) :=\n∞\nX\nk=0\nγkr(Ψ(k, x0, u(k)), u(k)).\n(3)\nwhere γ ∈(0, 1).\nRemark 1. In this setting where transitions are deterministic,\nthe definitions of the objective function and the value function\ncoincide.\nThe objective of reinforcement learning is to identify a con-\ntroller u that maximizes the objective function.\nDefinition 2 (Optimal Value Function). The optimal value\nat a state x ∈Rn is denoted as Vγ(x0) and is defined as\nVγ(x0) := sup\nu Ju\nγ(x).\n(4)\nHowever, in most RL algorithms, estimating the action-value\nfunction leads to greater performance in the model free set-\nting.\nDefinition 3 (Action-Value Function). The action value func-\ntion Qu : Rn × Rm →R is defined as\nQu(x, u) = r(x, u) + Ju\nγ(y)\n(5)\nwhere y = f(x, u). That is, the cumulative reward at state\nx by taking action u and subsequently following the control\nsequence u at the state y.\nInfinite Horizon LQR and Stability\nConsider the discrete time linearization of f(x, u)\nxn+1 = Axn + Bun\n(6)\nabout the equilibrium point where we can assume without\nloss of generality that the origin is the equilibrium point. The\ninfinite horizon LQR problem seeks a stablizing controller\nu that minimizes a quadratic cost function, thereby ensuring\noptimal performance and stability of the closed-loop system.\nThe cost in control is dual to the reward formulation in RL.\nIn particular, the cost has the form\nJLQR =\n∞\nX\nn=0\n\u0000xT\nnQxn + uT\nnRun\n\u0001\n(7)\nwhere Q is a positive definite matrix R is a positive semi\ndefinite matrix. In (Bertsekas 2012) the optimal policy uLQR\nis derived for state system (6) with cost (7) and is given by\nthe feedback controller uLQR(x) = −Kx where\nK =\n\u0000R + BT PB\n\u0001−1 BT PA\n(8)\nand P satisfies the discrete algebraic Ricatti equation (DARE)\nP = Q + AT \u0010\nP −PB\n\u0000R + BT PB\n\u0001−1 BT P\n\u0011\nA. (9)\nIt is also well known that for linear dynamics, the controller\nu(x) = −Kx is a stabilizing controller in the sense of the\nfollowing definition.\nDefinition 4 (Asymptotic Stability). An equilibrium point\nfor the closed loop system\nxk+1 = f(xk, u(xk))\n(10)\nis said to be asymptotically stable, if the following conditions\nare satisfied:\n(1) (Lyapunov stability) For every ϵ > 0, there exists a δ > 0\nsuch that ∥x0∥< δϵ implies that xk is defined for all k and\n∥xk∥< ϵ for all k; and\n(2) (Attractivity) There exists some ρ > 0 such that, for every\nϵ > 0, there exists some N ∈N such that xk is defined for\nall k ≥N and ∥xk∥< ϵ whenever ∥xk∥< ρ and k ≥N.\nSoft Actor Critic\nThe soft actor critic algorithm (Haarnoja et al. 2018) is a pop-\nular reinforcement learning algorithm that has been success-\nfully applied to continuous control tasks. In the RL setting,\nwhen a neural network parameterizes the controller, we refer\nto the controller as a policy, denoted by πϕ, to differentiate\nit from a general admissible controller u. Additionally, we\nconsider a probabilistic distribution for πϕ which is assumed\nto belong to a Gaussian family\nπϕ(x) ∈{N(µ(x), Σ(x))},\n(11)\nwhere the mean µ and covariance matrix Σ are functions\nof the state. The SAC objective adds an entropy term\nH (π (· | x)) into the objective function to learn a stochastic\npolicy that promotes exploration\nJγ(x0, π) =\n∞\nX\nk=0\nγkE(xk,ak)∼ρπ [r (xk, ak) + αH (π (· | xk))] ,\n(12)\nwhere α is called the temperature parameter which controls\nthe stochasticity of the policy and ρπ is the trajectory dis-\ntribution of the policy πϕ. Furthermore, the SAC algorithm\nestimates the value function V πϕ\nψ (x), action-value function\nQπϕ\nθ (x, a) and learns an actor function πϕ(x) which are all\napproximated with neural networks. The value network is\ntrained to minimize the squared residual error\nJV (ψ) = Exk∼D\n\u00141\n2\n\u0010\nVψ (xk) −ˆV\n\u00112\u0015\n,\n(13)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer with the target value network\nˆV (xk) = Eak∼πϕ [Qθ (xk, ak) −log πϕ (ak | xk)] .\n(14)\nThe soft Q function is trained to minimize the soft Bellman\nresidual as its loss function\nJQ(θ) =\nE(xk,ak)∼D\n\u00141\n2\n\u0010\nQθ (xk, ak) −ˆQ (xk, ak)\n\u00112\u0015\n,\n(15)\nwith the target action-value network\nˆQ (xk, ak) = r (xk, ak) + γExk+1∼p\n\u0002\nV ¯\nψ (xk+1)\n\u0003\n.\n(16)\nwhere p is the distribution of the next state xk+1. Finally, we\ntrain the policy network to directly minimize the following\nexpected KL divergence\nJπ(ϕ) =\nExk∼D\n\u0014\nDKL\n\u0012\nπϕ (· | xk) ∥exp (Qθ (xk, ·))\nZθ (xk)\n\u0013\u0015\n.\n(17)\nThe intuition behind this formula is that higher values of\nQθ(xn, un) signal higher cumulative rewards so the policy\nwill be proportional to the current estimate of the soft action-\nvalue function. A more computable form for (17) is the fol-\nlowing\nEx∼D\n\u0002\nEa∼πϕ(·|x) [log πϕ(a | x) −Qθ(x, a)]\n\u0003\n.\n(18)\n3\nReinforcement Learning and Stability\nIn this section we describe the RL algorithm for learning\nthe policy π as well simultaneously learning local linear\ndynamics for achieving stability. In particular, we focus on\nthe Soft Actor Critic algorithm for our purposes, but the\nformulation with any reinforcement learning algorithm is\nequivalent. Since we focus on stability, the reward function\nwe implement is set to\nr(xn, un) = R −xT Qx\n(19)\nwhere R is a positive constant and Q is a positive definite\nmatrix. The design of the reward function will encourage\nthe agent to get close to the equilibrium point to maximize\nthe cumulative rewards. We could also penalize the actions\nas this will not affect any of the theoretical results derived\nin the following section, however since we already consider\nconstraints we omit this penalization to simplify the training\nprocess. Additionally, we suppose that the reward function\nsatisfies the terminal condition on the domain D.\nDefinition 5 (Terminal Condition). The reward function is\nsaid to satisfy the terminal condition on a domain D for some\nc ≥0 if\nr(xN, uN) = −c,\n(20)\nwhere N is the first iteration such that xN /∈D and\nr(xn, un) = 0 for all n > N.\nWe design the reward/environment to satisfy the terminal\ncondition to ensure that the agent collects data near the\nequilibrium point. We now describe the components of the\nalgorithm. For the full details please refer to Algorithm 1 in\nthe Appendix.\nStep 1. As a first step prior to solving the DARE we need to\nmodel the local linearization at the equilibrium point. While\nthe reinforcement learning algorithm is running we collect\ndata points whenever the agent is within a fixed η > 0 dis-\ntance from the origin. We learn a state space representation\nxn+1 = ˆAxn + ˆBun\n(21)\nby collecting data points for xn and xn+1 whenever xn ∈\nBη(0) in two different ways. Until a preset threshold has\nbeen reached, take action un = 0 and insert xn and xn+1 as\nrow vectors in the data matrices X and Y respectively. Then\nlearn ˆA as the least squares estimate\nˆA = arg min\nA ∥Y −AX∥2.\n(22)\nThen once this is completed, we similarly collect data ma-\ntrices X, Y and U where U stores the non-zero actions and\nlearn B from\nˆB = arg min\nB ∥Y −ˆAX −BU∥2.\n(23)\nStep 2. After learning a representation for (21) we calculate\nthe gain matrix K by solving the DARE.\nStep 3. Then we update the empirical loss function of (18) as\nX\nxm\nX\nak\n(log πϕ(ak | xm) −Qθ(xm, ak))\n+∥∇µ(πϕ) −K∥2 + ∥µ(πϕ)(0)∥\n(24)\nwhere µ(πϕ) : Rn →R maps the state to the mean of the\npolicy and ∇µ(πϕ) is the Jacobian of this function with\nrespect to the state x. By minimizing this loss, the policy will\nlearn to be locally stabilizing with its linear approximation\nclose to the optimal gain matrix K.\nStep 4. After training has completed, set\nπϕ(x) ←πϕ(x) −πϕ(0),\n(25)\nas this will ensure that πϕ(0) = 0 and that the origin is a\nstable equilibrium point.\nStep 5. When evaluating the policy for stabilization tasks,\nthe action taken should be the mean of the policy, that is we\nshould set\nπϕ(x) ←µ(πϕ(x))\n(26)\nas a stochastic policy will only stabilize with high probability\nshould the variance decrease to zero about the equilibrium\npoint.\nWe found that the training was more efficient when collect-\ning data while simultaneously training all the parameterized\nfunctions according to (13) - (18) rather than first learning the\nlocal dynamics and updating the actor parameters according\nto (24). This is because if knowledge of local dynamics is\nused, most states will be close to the equilibrium point and\nthe performance will not generalize well outside a neighbour-\nhood of the equilibrium point.\nEnforcing Action Bounds\nSince the Gaussian distribution has infinite support, actions\nneed to be mapped to a finite domain. To address this, we\nuse a squashing function that allows for more general actions.\nIn particular, supposing that w is a random variable with\nsupport Rd, the random variable\ny = A tanh(w) + b\n(27)\nwhere A is an invertible matrix, b is a vector and the hy-\nperbolic tangent, tanh, is applied element-wise has a sup-\nport over a parallelepiped in Rd. We will apply the change\nof variables formula to compute the log likelihoods of the\nbounded actions. Let w be a random variable with Gaus-\nsian distribution µ(w|x) and y = A tanh(w) + b. Then\nwriting z = tanh(w) we have that Jy = A and Jz =\ndiag(1 −tanh2(w1), . . . , 1 −tanh2(wd)), where Jy is the\nJacobian of y with respect to z and Jz is the Jacobian of z\nwith respect to x. From the change of variables formula,\nπ(y|x) = µ(w|x) · | det(A)|−1| det Jz|−1\nand in the case when A is a diagonal matrix, the log likelihood\nhas a convenient form\nπ(y|x) =\nµ(w|x)\nQn\ni=1 aii(1 −tanh2(wi)),\n(28)\nand the log-likelihood is given by\nlog π(y|x) = log µ(w|x) −\nn\nX\ni=1\nlog\n\u0000aii(1 −tanh2(wi))\n\u0001\n.\n(29)\nFigure 1: Overall learning algorithm of the proposed method.\n4\nTheoretical Guarantees\nRecall the system (1):\nxn+1 = f(xn, un),\nwith state x ∈Rn and input U(x) ⊂Rm where U(x) is a\nset of admissible controls (as in (Keerthi and Gilbert 1985))\nassociated to a state x. Since stabilization under discounting\nin the discrete setting is much more challenging than without\na discount factor, we make the following assumptions.\nAssumption 2. (a) For any initial condition x0 ∈Rn there\nexists an infinite length control system u⋆such that\nVγ(x) = Ju⋆\nγ (x0) = sup\nu Ju\nγ(x).\n(30)\n(b) Suppose that there exists a constant aV such that for any\nγ ∈(0, 1) and x ∈Rn\nR\n1 −γ −Vγ(x) ≤aV ∥x∥2.\n(31)\nFor (a), this assumption states that for any initial condition\nx0 ∈Rn the optimal value function in Definition 2 exists.\nConditions on the vector field in (1) and the rewards to\nensure that Assumption 2 holds are given in (Keerthi and\nGilbert 1985). For (b) a sufficient condition is for the opti-\nmal policy to be exponentially stabilizable. That is, there\nexists M > 0 and λ > 0 such that for each x ∈Rn\nthere exists an infinite length control sequence u(x) that\nsatisfies R −r(Ψ(k, x, u(k)), u(k)) ≤M∥x∥2e−λdk. For\nweaker conditions that guarantee condition (b) please refer\nto (Grimm et al. 2005).\nAssumption 3. Suppose that the policy in (11) is parameter-\nized by a neural network with smooth activation functions.\nThis assumption is necessary since in the proofs we need to\napproximate the value function with a neural policy and it\nalways holds in our implementation as we use the hyperbolic\ntangent activation. Furthermore, as we shall see in the next\nsection, reinforcement learning methods typically guarantee\npractical stability in the discrete-time with discounting set-\nting and therefore, the following assumption will allow for\nasymptotic stability.\nAssumption 4. Let K be the optimal gain matrix of the\nsolution to the local LQR problem (8). Suppose that for any\nϵK > 0, our learning algorithm converges to an estimate of\nthe optimal gain matrix ˆK such that ∥K −ˆK∥2 < ϵK.\nThis assumption is reasonable because, in practice, learn-\ning the linear state-space representation from data is often\nstraightforward, sometimes even achievable with a single\ntrajectory (Hu, Wierman, and Qu 2022). Additionally, we can\nalso empirically test whether the learned gain matrix provides\nstabilization within a neighborhood around the origin.\nDeterministic Guarantees\nWe first formulate theoretical guarantees in the deterministic\nsetting and then prove theoretical guarantees for our learning\nalgorithm. This is because we consider stochasticity in the\nour learning algorithm algorithm for exploration, but test the\neffectiveness of the policy by taking the expectation. The\nfollowing proposition establishes the value function is point-\nwise bounded and that it strictly increases along its optimal\ntrajectories.\nProposition 1. There exists comparison functions α1, α2 ∈\nK∞and γ⋆such that for any γ satisfying γ ∈(γ⋆, 1) the\nfollowing holds\nα1(x) ≤\nR\n1 −γ −Vγ(x) ≤α2(x)\n(32)\nfor any x ∈Rn, and\nVγ(x) −Vγ(y) ≤−c∥x∥2\n(33)\nfor any x ∈Rn, and y = f(x, u1(x)).\nRemark 2. In the above proposition we have that α1(s) =\nλmin(Q)s2 and α2(s) = aV s2.\nThe proposition asserts that for a sufficiently large discount\nfactor, the value function can from part of a Lyapunov func-\ntion. The proof of the proposition can be found in the Ap-\npendix. Consequently, this proposition can be used to show\nthat the optimal policy is stabilizing.\nTheorem 1. Suppose that π⋆is the optimal policy for the\nobjective (1). Then there exists γ⋆such that for any γ ∈\n(γ⋆, 1) the system\nxn+1 = f(Xn, π⋆(xn))\n(34)\nis asymptotically stable.\nThe proof of the theorem can be found in the Appendix. On\nthe other hand, given that the class of policies is parame-\nterized by neural networks, we have to restrict ourselves to\nthese class of functions. However, due to approximation er-\nrors, only practical stability in the sense of the following\ntheorem is guaranteed.\nTheorem 2. Fix ∆> 0 and let π⋆is the optimal policy for\nthe objective (1). Then, there exists ϵ > 0, γ⋆and N ∈N\nsuch that for any γ ∈(γ⋆, 1) and any neural policy ∥πϕ −\nπ⋆∥∞< ϵ such that the following holds\n∥Ψ(k, x, πϕ,k(x))∥≤∆\n∀k ≥N.\n(35)\nfor any x ∈K.\nRemark 3. An alternative formulation for practical stability\nis presented in the Appendix. The proof of this result relies\nheavily on a smooth version of the universal approximation\ntheorem.\nThe notion of stability is called practical since we are only\nguaranteed stability in a small neighborhood of the origin. To\nensure asymptotic stability our algorithm will learn a local\ngain matrix that guarantees local stability. This result will be\nformulated in the following section.\nStochastic Guarantees\nIn this section, we prove stability and convergence guarantees\nfor our learning algorithm. As a first result we show that the\ntemperature parameter in (12) can be controlled such that the\noptimal solution to the SAC objective is close in expectation\nto the solution of the optimal deterministic policy (1).\nProposition 2. Fix ϵ > 0 and consider the system (1) and the\nSAC objective. Denote the optimal solution to (12) as πSAC\nand the the optimal solution to (1) as π⋆. Then there exists\nα⋆such that for any temperature 0 < α < α⋆, we have\n|EπSAC −π⋆|D < ϵ.\n(36)\nThe value of α⋆is included in the proof as presented in the\nAppendix. Since in actual implementation, EπSAC is parame-\nterized by a neural network, by Theorem 2 we can guarantee\npractical stability for a near optimal neural policy.\nCorollary 1. Suppose that πSAC is the optimal policy to (12).\nThen for any ∆> 0, there exists ϵ, γ⋆, α⋆> 0, and N ∈N\nsuch that for any α ∈(0, α⋆), γ ∈(γ⋆, 1) and any neural\npolicy ∥E\n\u0002\nπSAC −πϕSAC\u0003\n∥D < ϵ, the following holds\n∥Ψ(k, x, πSAC\nϕ,k (x))∥≤∆\n∀k ≥N.\n(37)\nfor any x ∈D.\nHowever, if the learned neural policy satisfies a linear order\nof approximation with sufficiently small decay factor, it is\npossible to extend practical stability to asymptotic stability.\nProposition 3. Suppose that π⋆is the optimal policy to (1)\nand πSAC is the optimal policy to (12). Then, there exists\nϵ, γ⋆, α⋆> 0 such that if the neural policy that satisfies\nthe approximation error ∥E\n\u0002\nπSAC −πϕSAC\u0003\n∥D < ϵ also\nsatisfies the order of approximation\n|E\n\u0002\nπ⋆(x) −πϕ\nSAC(x)\n\u0003\n| ≤η∥x∥\n(38)\nfor any decay rate η sufficiently small, then for any α ∈\n(0, α⋆) and γ ∈(γ⋆, 1), the neural policy E\n\u0002\nπϕSAC\u0003\nis\nasymptotically stabilizing.\nThese two previous results form the bridge between stability\nand reinforcement learning. The ultimate goal of stabilizing\ncan be realized by learning a policy that approximates\nthe optimal policy well. However, due to randomness\nand optimization errors that are typically intractable, in\npractice, reinforcement learning will only manage to learn\na practically stabilizing policy as in Corollary 1. This\nhighlights the importance of our learning algorithm.\nNext we analyze the convergence of our learning algorithm.\nHowever, since the convergence of maximal entropy rein-\nforcement learning algorithms is only guaranteed in the tabu-\nlar case (Haarnoja et al. 2018) and the continuous case is still\na challenging open problem, we make the assumption that\nthe SAC algorithm can learn a neural policy that is arbitrarily\nclose to the optimal one.\nAssumption 5. Suppose that πSAC is the optimal policy to the\nSAC objective (12), then for any ϵ > 0 there exists a neural\npolicy πSAC\nϕ\nsuch that ∥E\nh\nπSAC −πSAC\nϕ\ni\n∥D < ϵ.\nIndeed, this seems to hold in practice as evidenced in the\nfollowing section. The neural policy learned by SAC will\nreach and stay in a small neighborhood around the equilib-\nrium point. Furthermore, the robustness of the LQR optimal\ngain matrix asserts a sufficient close gain matrix will be sta-\nbilizing. Therefore, combining the practical stability of the\nnear optimal RL policy and the local stabilization properties\nof learned gain matrix will show that our learning algorithm\nwill converge to an asymptotically stabilizing neural policy.\nTheorem 3. Suppose that Assumptions 4 and 5 hold for our\nlearning algorithm. Then the learning algorithm converges\nto a asymptotically stabilizing neural policy E\n\u0002\nπϕSAC\u0003\n.\n5\nNumerical Experiments\nIn this section, we present numerical examples to evaluate\nthe performance of the proposed learning algorithm. In these\nsimulations we aim to accomplish the following goals: 1)\nEvaluate the performance of our learning algorithm against\nstochastic and deterministic versions of SAC and PPO, 2)\nEvaluate whether our learning algorithm can achieve good\nperformance on high-dimensional control problems while\nstill stabilizing and satisfying the constraints, 3) Demonstrate\nformal verification of the asymptotic stability for low dimen-\nsional systems with the learned neural policy.\nComparison with Reinforcement learning methods\nWe compare our learning algorithm against deterministic and\nstochastic SAC and PPO. The ultimate goal of this task is\n(a) Inverted Pendulum\n(b) Cartpole\n(c) 2D Quadrotor\n(d) 3D Quadrotor\nFigure 2: Plots of Accumulated Costs for the Four Environments\nto learn an asymptotically stabilizing policy. Therefore, we\ncompare the accumulated cost over time where the cost is\nmeasured by the quadratic form xT Qx + uT Ru where Q\nand R are the same matrices used to learn the optimal gain\nmatrix for the LQR controller. This is different from the\ntypical comparisons in RL which average the performance of\nthe learning algorithm over the episodic training. For every\nenvironment, we evaluate the performance by uniformly\nsampling the initial states and test the aforementioned\npolicies. All hyperparameters, network architectures, and\ncosts are listed in the Appendix and the code for all\ncomparisons are provided in the supplementary material.\nIn general, stochastic policies perform poorly, whereas PPO\nperforms well on low-dimensional tasks. Deterministic SAC\nshows strong performance across all tasks, as it can reach and\nmaintain a position within a small neighborhood of the ori-\ngin, indicating that the policy can achieve practical stability.\nHowever, for true stabilization, only our learning algorithm\nsuccessfully completes this task. The plots for stabilization\nare shown in the Appendix. As a final statement, it is impor-\ntant to emphasize that our control tasks are more difficult than\nthe typical optimal control tasks as we consider constraints,\nwhereas standard optimal control methods such as LQR can-\nnot handle constraints. Additionally, our algorithm is able to\nstabilize on larger domains as compared to the recent work\nin (Meng et al. 2024).\nVerification for Asymptotic Stability\nWhen the sampling time is small, the learned neural policy\nπϕ should be able to stabilize the continuous-time dynamics.\nIn this work, we verify that the origin of the closed-loop\ncontinuous-time system ˙x = f(x, πϕ(x)) is asymptotically\nstable within its domain for low dimensional systems such\nas the inverted pendulum. This verification is done using a\nquadratic Lyapunov function and the LyZNet toolbox (Liu\net al. 2024) built upon SMT solvers. The quadratic Lyapunov\nfunction is generated by linearizing the system and then\nsolving the Lyapunov equation. With the quadratic Lyapunov\nfunction, we can first find a local region-of-attraction (ROA)\nwhere linearization dominates. Next, a reachability step is\nutilized to enlarge the ROA using the Lyapunov stability\nanalysis for the quadratic Lyapunov function with respect\nto the local ROA. In doing so, all the states in the domain\nare guaranteed to eventually converge the origin, yielding\nasymptotic stability. The verified results with the quadratic\nLyapunov function and corresponding ROA estimate can be\nfound in the Appendix.\n6\nConclusion\nIn this work, we propose an algorithm for learning a stabi-\nlizing controller for unknown nonlinear systems. By inte-\ngrating the local optimal gain matrix directly into the neural\npolicy we design a policy that satisfies classical local LQR\nstabilization and numerically shows global stability. This\napproach significantly outperforms standard RL algorithms\nwhich in the theory and implementation only guarantee prac-\ntical stability. Additionally, we provide theoretical analysis\nfor stabilization and the convergence for our algorithm to an\nasymptotically stabilizing policy, which can be verified with\nLyZNet, a toolbox based on SMT solvers. Limitations and\npotential future work are discussed in the Appendix.\nReferences\nAlshiekh, M.; Bloem, R.; Ehlers, R.; K¨onighofer, B.; Niekum,\nS.; and Topcu, U. 2018. Safe Reinforcement Learning via\nShielding. AAAI, 32(1).\nBeckenbach, L.; Osinenko, P.; and Streif, S. 2019. Model\npredictive control with stage cost shaping inspired by re-\ninforcement learning. In 2019 IEEE 58th Conference on\nDecision and Control (CDC), 7110–7115. IEEE.\nBerkenkamp, F.; Moriconi, R.; Schoellig, A. P.; and Krause,\nA. 2016. Safe learning of regions of attraction for uncertain,\nnonlinear systems with Gaussian processes. In 2016 IEEE\n55th Conference on Decision and Control (CDC), 4661–4666.\nIEEE.\nBerkenkamp, F.; Turchetta, M.; Schoellig, A. P.; and Krause,\nA. 2017.\nSafe model-based reinforcement learning with\nstability guarantees. Adv. Neural Inf. Process. Syst., 908–\n918.\nBertsekas, D. 2012. Dynamic Programming and Optimal\nControl: Volume II; Approximate Dynamic Programming.\nAthena Scientific.\nBof, N.; Carli, R.; and Schenato, L. 2018. Lyapunov Theory\nfor Discrete Time Systems. arXiv:1809.05289.\nGarc´ıa, J.; and Fern´andez, F. 2015. A comprehensive survey\non safe reinforcement learning. J. Mach. Learn. Res., 16:\n1437–1480.\nGrimm, G.; Messina, M. J.; Tuna, S. E.; and Teel, A. R.\n2005. Model predictive control: for want of a local control\nLyapunov function, all is not lost. IEEE Trans. Automat.\nContr., 50(5): 546–558.\nG¨ohrt, T.; Griesing-Scheiwe, F.; Osinenko, P.; and Streif, S.\n2020. A reinforcement learning method with closed-loop\nstability guarantee for systems with unknown parameters.\nIFAC-PapersOnLine, 53(2): 8157–8162. 21st IFAC World\nCongress.\nHaarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018. Soft\nActor-Critic: Off-Policy Maximum Entropy Deep Reinforce-\nment Learning with a Stochastic Actor. In Dy, J.; and Krause,\nA., eds., Proceedings of the 35th International Conference\non Machine Learning, volume 80 of Proceedings of Machine\nLearning Research, 1861–1870. PMLR.\nHan, M.; Zhang, L.; Wang, J.; and Pan, W. 2020. Actor-\nCritic Reinforcement Learning for Control With Stability\nGuarantee. IEEE Robotics and Automation Letters, 5(4):\n6217–6224.\nHu, Y.; Wierman, A.; and Qu, G. 2022. On the sample com-\nplexity of stabilizing lti systems on a single trajectory. Ad-\nvances in Neural Information Processing Systems, 35: 16989–\n17002.\nJiang, Y.; and Jiang, Z.-P. 2017. Robust adaptive dynamic\nprogramming. John Wiley & Sons.\nKeerthi, S.; and Gilbert, E. 1985. An existence theorem\nfor discrete-time infinite-horizon optimal control problems.\nIEEE Trans. Automat. Contr., 30(9): 907–909.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for Stochas-\ntic Optimization. In Bengio, Y.; and LeCun, Y., eds., 3rd In-\nternational Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings.\nLillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;\nTassa, Y.; Silver, D.; and Wierstra, D. 2015. Continuous\ncontrol with deep reinforcement learning.\nLiu, J.; Meng, Y.; Fitzsimmons, M.; and Zhou, R. 2024.\nTOOL LyZNet: A Lightweight Python Tool for Learning\nand Verifying Neural Lyapunov Functions and Regions of\nAttraction. In Proceedings of the 27th ACM International\nConference on Hybrid Systems: Computation and Control,\n1–8.\nMatallana, L. G.; Blanco, A. M.; and Bandoni, J. A. 2010.\nEstimation of domains of attraction: A global optimization\napproach. Mathematical and Computer Modelling, 52(3-4):\n574–585.\nMeng, Y.; Zhou, R.; Mukherjee, A.; Fitzsimmons, M.; Song,\nC.; and Liu, J. 2024. Physics-Informed Neural Network\nPolicy Iteration: Algorithms, Convergence, and Verification.\nArXiv, abs/2402.10119.\nOsinenko, P.; Beckenbach, L.; G¨ohrt, T.; and Streif, S. 2020.\nA reinforcement learning method with closed-loop stability\nguarantee. IFAC-PapersOnLine, 53(2): 8043–8048. 21st\nIFAC World Congress.\nOsinenko, P.; Dobriborsci, D.; and Aumer, W. 2022. Re-\ninforcement learning with guarantees: a review.\nIFAC-\nPapersOnLine, 55(15): 123–128.\nPecka, M.; and Svoboda, T. 2014. Safe Exploration Tech-\nniques for Reinforcement Learning – An Overview. In Mod-\nelling and Simulation for Autonomous Systems, 357–375.\nSpringer International Publishing.\nPerkins, T.; and Barto, A. 2003. Lyapunov design for safe\nreinforcement learning. J. Mach. Learn. Res., 3: 803–832.\nPinkus, A. 1999. Approximation theory of the MLP model\nin Neural Networks. Acta Numerica, 8: 143–195.\nSaunders, W.; Sastry, G.; Stuhlmueller, A.; and Evans, O.\n2017.\nTrial without Error: Towards Safe Reinforcement\nLearning via Human Intervention.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal Policy Optimization Algorithms.\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre,\nL.; van den Driessche, G.; Schrittwieser, J.; Antonoglou,\nI.; Panneershelvam, V.; Lanctot, M.; Dieleman, S.; Grewe,\nD.; Nham, J.; Kalchbrenner, N.; Sutskever, I.; Lillicrap, T.;\nLeach, M.; Kavukcuoglu, K.; Graepel, T.; and Hassabis, D.\n2016. Mastering the game of Go with deep neural networks\nand tree search. Nature, 529(7587): 484–489.\nVinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;\nDudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds, T.;\nGeorgiev, P.; Oh, J.; Horgan, D.; Kroiss, M.; Danihelka, I.;\nHuang, A.; Sifre, L.; Cai, T.; Agapiou, J. P.; Jaderberg, M.;\nVezhnevets, A. S.; Leblond, R.; Pohlen, T.; Dalibard, V.;\nBudden, D.; Sulsky, Y.; Molloy, J.; Paine, T. L.; Gulcehre,\nC.; Wang, Z.; Pfaff, T.; Wu, Y.; Ring, R.; Yogatama, D.;\nW¨unsch, D.; McKinney, K.; Smith, O.; Schaul, T.; Lillicrap,\nT.; Kavukcuoglu, K.; Hassabis, D.; Apps, C.; and Silver, D.\n2019. Grandmaster level in StarCraft II using multi-agent\nreinforcement learning. Nature, 575(7782): 350–354.\nZanon, M.; and Gros, S. 2021. Safe Reinforcement Learning\nUsing Robust MPC. IEEE Trans. Automat. Contr., 66(8):\n3638–3652.\nZhou, R.; Quartz, T.; Sterck, H.; and Liu, J. 2022. Neural Lya-\npunov control of unknown nonlinear systems with stability\nguarantees. Adv. Neural Inf. Process. Syst., abs/2206.01913.\nZoboli, S.; Andrieu, V.; Astolfi, D.; Casadei, G.; Dibangoye,\nJ. S.; and Nadri, M. 2021. Reinforcement Learning Policies\nWith Local LQR Guarantees For Nonlinear Discrete-Time\nSystems. In 2021 60th IEEE Conference on Decision and\nControl (CDC), 2258–2263. IEEE.\nA\nAppendix\nProofs of Theoretical Guarantees\nWe recall that a Ck(D) function means a k-times continuously differentiable function defined on a domain D and a C(D)\nfunction is continuous. The following lemmas will be integral to the proofs in this section.\nLemma 1 (Weierstrass M-test). Let {fn(x))} be a sequence of real valued functions defined on a set D. Suppose there exists a\nsequence of non-negative constants {Mn} such that |fn(x)| ≤Mn for all x ∈D and for all n, and that P∞\nn=1 Mn is convergent.\nThen the series P∞\nn=1 fn(x) converges uniformly on D.\nLemma 2 (Universal Approximation Theorem). Let D ⊂Rm be a compact set and suppose f : D →Rn ∈C(Rn). Then, for\nevery ϵ > 0 there exists a neural network ϕ : D →R with C(R) non-polynomial activation function such that\n∥f(x) −ϕ(x)∥D < ϵ.\n(39)\nLemma 3 (Universal Approximation Theorem - Smooth version). Let D ⊂Rm be a compact set and suppose f : D →Rn ∈\nC1(Rn). Then, for every ϵ > 0 there exists a neural network ϕ : D →R with C1(R) non-polynomial activation function such\nthat\n∥f(x) −ϕ(x)∥D < ϵ,\n(40)\nand for all i = 1, . . . , n, the following simultaneously holds\n\r\r\r\r\n∂f\n∂xi\n−∂ϕ\n∂xi\n\r\r\r\r\nD\n< ϵ.\n(41)\nThe proofs of the universal approximation theorems for neural networks can be found in (Pinkus 1999). As a preliminary result\nwe first prove the claim under Assumption 2 that if the controller is exponentially stabilizable then Assumption 2 (b) is guaranteed\nto hold.\nTheorem 4 (Hartman-Grobman). Consider the system (1) with a policy π(x). Suppose that at the equilibrium point, the\nJacobian matrix A = [∂fi/∂xj]|x=0 has no eigenvalue with real part equal to zero. Then there exists a neighbourhood N of the\nequilibrium point and a homeomorphism h : N →Rn, such that h(0) = 0 and such that in the neighbourhood N the system\nxk+1 = f(xk, π(xk)) is topologically conjugate to its linearization h(xk+1) = Ah(xk).\nWe state the result in full generality, however, for the proofs in this section we only a corollary that the system xk+1 =\nf(xk, π(xk)) is locally asymptotically stable if all eigenvalues of the Jacobian at the equilibrium point have negative real part.\nProposition 4. Suppose that the optimal controller u is exponentially stabilizable, then Assumption 2 holds with aV =\nM\n1−e−λ .\nProof. Writing out the expression for the reward and using the definition of exponential stability gives\n1\n1 −γ −\n∞\nX\nk=0\nγkr(Ψ(k, x, u(k)), u(k)) =\n∞\nX\nk=0\nγk (1 −r(Ψ(k, x, u(k)), u(k)))\n≤\n∞\nX\nk=0\n1 −r(Ψ(k, x, u(k)), u(k))\n≤\n∞\nX\nk=0\nM∥x∥2e−λk\n≤M∥x2∥\n1 −e−λ .\nThis concludes the proof.\nThe following result will be a first step in establishing\nR\n1−γ −Vγ(x) as a Lyapunov function.\nProof of Proposition 1.\nAs a preliminary step we note that there is an error in the statement of Proposition 1 as it should be\nVγ(x) −Vγ(y) ≤−c∥x∥2.\nProof. Since the rewards can be scaled by an arbitrary constant, we suppose without loss of generality that R = 1. We\nbegin by proving (32) for the optimal value function. For the lower bound, note that since xT Qx ≥λmin(Q)∥x∥2 and\n1\n1−γ −Vγ(x) ≥r(x, u(x)) = xT Qx this implies that\n1\n1 −γ −Vγ(x) ≥λmin(Q)∥x∥2.\nThe upper bound follows directly from Assumption 2 (b). Finally we prove that (33) holds. Set y = f(x, u1(x)) as the next state,\nthen by the dynamic programming principle\nVγ(x) = r(x, u(0)) + γVγ(y)\nand thus,\nVγ(x) −Vγ(y) = r(x, u(0)) −(1 −γ)Vγ(y)\n= (r(x, u(0)) −1) + (1 −γ)\n\u0012\n1\n1 −γ −Vγ(y)\n\u0013\n≤(1 −γ)α2(∥x∥) −(1 −r(x, u(0)))\n≤1 −γ\nγ\nα2(∥x∥) −xT Qx\n≤\n\u0012\n−λmin(Q) + 1 −γ\nγ\naV\n\u0013\n∥x∥2\nWe have that trajectories are decreasing up to some pertubative term. To deal with this term we set γ⋆=\naV\naV +λmin(Q). This then\nimplies that for any γ ∈(γ⋆, 1) we have that\n1 −γ\nγ\n<\nλmin(Q)\naV + λmin(Q) · aV + λmin(Q)\naV\n= λmin(Q)\naV\n.\nTherefore, it follows that there exists c > 0 such that −λmin(Q) + 1−γ\nγ aV < −c and we conclude that (33) holds.\nRemark 4. As we saw in the proof, we have that the value function strictly increases along trajectories up to some perturbative\nterm that depends on γ. In fact, this can be used to show practical stability should Assumption 2 (b) not be required. Immediately\nwe see that the discounting setting provides challenges for stability as the optimal controller need not be asymptotically stabilizing\nshould this assumption not hold.\nProof of Theorem 1.\nProof. Set γ⋆is given in the proof of Proposition 1 and if necessary adjust the value of c in Proposition 1 such that c < aV .\nDenote\nLγ(x) :=\n1\n1 −γ −Vγ(x)\nwhich will serve as our Lyapunov function and observe that (33) can be rewritten as\nLγ(y) −Lγ(x) ≤−c∥x∥2.\nAs Lγ(x) ≤aV ∥x∥2 by Assumption 2 (b) we see that\nLγ(y) −Lγ(x) ≤−c∥x∥2 ≤−c\naV\nLγ(x)\nwhich implies that Lγ(y) ≤\n\u0010\n1 −\nc\naV\n\u0011\nLγ(x) where\n\u0010\n1 −\nc\naV\n\u0011\n∈(0, 1) and so iterative this inequality gives\nLγ(Ψ(k, x, π⋆(k))) ≤\n\u0012\n1 −c\naV\n\u0013k\nLγ(x).\nThis implies that\n∥Ψ(k, x, π⋆(k))∥2 ≤\n\u0012\n1 −c\naV\n\u0013k\naV\nλmin(Q)∥x∥2\nand setting λ = −ln\n\u0010\n1 −\nc\naV\n\u0011\ngives\n∥Ψ(k, x, π⋆(k))∥2 ≤\naV\nλmin(Q)∥x∥2e−λk.\nThis proves that the optimal controller is asympotically stabilizing.\nRemark 5. In particular, we have shown that the optimal controller is exponentially stablizing.\nProof of Theorem 2.\nTo break up the length of the proof we first prove the following key lemma.\nLemma 4. Suppose that the reward function is defined according to (19) and satisfies the terminal condition (20) and the\nneural policy satisfies Assumption 11. Let π denote the optimal policy to (1). Then for any ϵ > 0 there exists δ > 0 such that\n∥π −πϕ∥D < δ implies that\n∥V π\nγ −V πϕ\nγ ∥D < ϵ.\nProof. Step 1. Show that the value function inherits the regularity from the reward function.\nNote that by definition,\nV πϕ\nγ (x) =\n∞\nX\nk=0\nγkr(Ψ(k, x, πϕ,k(x), πϕ,k(x))\nis a sum of a sequence of C2 functions {fk(x)} := {γkr(Ψ(k, x, πϕ,k(x), πϕ,k(x))}. Since the reward function is C2(D) on a\ncompact set and satisfies the terminal condition, it follows from the Weierstrass M-test that P∞\nk=0 fk and P∞\nk=0\n∂\n∂xj fk uniformly\nconverge on D and thus from the result on the derivative of uniformly converging functions (reference this later), by taking limits\nfor the derivatives, it follows that\nlim\nN→∞∇\nN\nX\nk=0\nfk = ∇V πϕ\nγ .\nBy making a similar argument for the second order partial derivatives it follows that\nlim\nN→∞∇2\nN\nX\nk=0\nfk = ∇2V πϕ\nγ .\nIt also follows that the value function is C2(D).\nStep 2. Approximate the value function by a neural network policy.\nFirstly, by the Bellman dynamic programming principle, we have that\nV π\nγ (x) = r(x, π(x)) + γV π\nγ (f(x, π(x)))\nV πϕ\nγ (x) = r(x, π(x)) + γV πϕ\nγ (f(x, π(x)))\nThe idea will be to inductively bound the value function by carefully having the same argument to the value functions to avoid an\nexponential blow up of the Lipschitz constants. Note that by the assumption on the reward function, there exists vr ∈D such that\nr(x, πϕ(x)) = r(x, π(x)) + ∇ur(x, π(x))T (π(x) −πϕ(x)) + 1\n2(π(x) −πϕ(x))T ∇2\nur(vr, π(vr))(π(x) −πϕ(x)).\nThe term with the gradient can be bounded by Cauchy-Schwarz, however the quadratic form requires a bit more work. Note that\nsince ∥π(x) −πϕ(x)∥≤δ we have that\n|δT ∇2\nur(vr)δ| = |Trace\n\u0000∇2\nur(vr, π(vr))(π(x) −πϕ(x))(π(x) −πϕ(x))T \u0001\n| ≤m∥∇2\nur(vr, π(vr))∥δ2\nand thus we see that\n|r(x, πϕ(x)) −r(x, π(x))| ≤∥∇ur∥δ + m\n2 ∥∇2\nur∥δ2,\nwhere ∥∇ur∥= supx∈D ∥∇ur(x, π(x))∥and ∥∇2\nur∥= supx∈D ∥∇2\nur(x, π(x))∥. Similarly, writing the Taylor expansion for\nthe value function, there exists vV such that\nV πϕ\nγ f(x, πϕ(x)) = V πϕ\nγ (f(x, π(x)) + ∇V πϕ\nγ (f(x, π(x)))T (f(x, π(x)) −f(x, πϕ(x)))\n+ 1\n2 (f(x, π(x)) −f(x, πϕ(x)))T ∇2V πϕ\nγ (f(vV , π(vV ))) (f(x, π(x)) −f(x, πϕ(x))) .\nCombining this identity along with the bound for the reward function and the vector field being Lipschitz in the u component we\nhave that\n|V π\nγ f(x, π(x)) −V πϕ\nγ (f(x, π(x))| ≤γ|V π\nγ (f(x, π(x))) −V πϕ\nγ (f(x, π(x)))| + ∥∇ur∥δ + m\n2 ∥∇2\nur∥δ2\n+ γLu∥∇V πϕ∥δ + γ(m + n)\n2\nL2\nu∥∇2V πϕ\nγ ∥δ2.\nwhere ∥∇V πϕ∥= supx∈D ∥∇V πϕ(f(x, π(x)))∥and ∥∇2V πϕ\nγ ∥= supx∈D ∥∇2V πϕ\nγ (f(x, π(x)))∥. By inductively arguing as\nwe have done above we obtain the following bound\n|V π\nγ f(x, π(x)) −V πϕ\nγ (f(x, π(x))| ≤lim\nk→∞\n\u0000γk|V π\nγ (f(x, π(x))) −V πϕ\nγ (f(x, π(x)))|\n\u0001\n+\n\u0012\nδ∥∇ur∥+ mδ2\n2 ∥∇2\nur∥\n\u0013 ∞\nX\nk=0\nγk\n+\n\u0012\nδLu∥∇V πϕ∥2 + (m + n)δ2\n2\nL2\nu∥∇2V πϕ\nγ ∥\n\u0013 ∞\nX\nk=1\nγk.\nThe first term in the limit will converge to 0 by Theorem 1. Subsequently, we see that |V π\nγ (x) −V πϕ\nγ (x)| can be made arbitrarily\nsmall so long as we choose δ sufficiently small. This concludes the proof of the lemma.\nNow we are ready to prove Theorem 2.\nProof. Denote max∥x∥≤δ ∥π(x)∥= Mδ which exists since π is a smooth function of the state by Assumption 3. Since πϕ(0) = 0\nwe have that Mδ →0 as δ →0. Furthermore, there exists δ > 0 such that\nLxδ + LuMδ ≤∆.\nNote from the proof of Theorem 1 and (32) of Proposition 1 that Lγ(y) < (1 −\nc\n2av )Lγ(x) holds for all x ∈D \\ Bδ(0) where\nBδ(0) is a δ neighborhood of the origin. Then, by the Bolzano-Weierstrass theorem we are guaranteed that the following\nminimum is strictly positive,\nη :=\nmin\nx∈D\\Bδ(0)\n\u0012\n1 −\nc\n2av\n\u0013\nLγ(x) −Lγ(f(x, π⋆(x)) > 0.\nTherefore, by Lemma 4, there exists ϵ1 > 0 such that if ∥π⋆−πϕ∥D < ϵ1 then ∥Vγ −V πϕ\nγ ∥D < η\n2 and thus we see that the\nfollowing inequality also holds in the place of a neural policy\nLπϕ\nγ (y) < (1 −\nc\n2av\n)Lπϕ\nγ (x) for all x ∈D \\ Bδ(0),\nwhere Lπϕ\nγ (x) :=\n1\n1−γ −V πϕ\nγ (x). By a similar argument as the one above, there exists ϵ2 > 0 for which we have that\n∥π⋆−πϕ∥D < ϵ2 implies\nλmin(Q)\n2\n∥x∥2 ≤Lπϕ\nγ (x) ≤2aV ∥x∥2 for all x ∈D \\ Bδ(0),\nand so we set ϵ = min(ϵ1, ϵ2). Thus, combining the two inequalities implies that\n∥Ψ(k, x, πϕ(k))∥2 ≤\n\u0012\n1 −\nc\n2aV\n\u0013k\n4aV\nλmin(Q)∥x∥2 for all x ∈D \\ Bδ(0).\nThis shows that all trajectories under the controller πϕ reach Bδ(0). However, since the system is in discrete time we could jump\narbitrarily far out of this ball. To bound the next state for x ∈Bδ(0) we use the Lipschitz assumption for the vector field. Indeed,\nby Assumption 1 we have for any x ∈Bδ(0) that\n∥f(x, πϕ(x))∥= ∥f(x, πϕ(x)) −f(0, πϕ(0))∥≤Lxδ + LuMδ∆.\nThis shows that the next state will always be contained in B∆(0) meaning that all trajectories eventually enter B∆(0) and stay in\nthis neighborhood. This proves that the neural policy πϕ is practically stabilizing.\nProof of Corollary 1.\nProof. By Theorem 2 there exists γ⋆and ϵ > 0 such that any neural policy πϕ satisfying ∥πϕ −π⋆∥< 2ϵ is practically\nstabilizing. Proposition 2 asserts that we can set the temperature parameter α⋆such that for any α ∈(0, α⋆) we have that\n|EπSAC −π⋆|D < ϵ.\nTherefore, we immediately see that if ∥E\n\u0002\nπSAC −πϕSAC\u0003\n∥D < ϵ then practical stability holds for the policy E\n\u0002\nπϕSAC\u0003\n.\nProof of Proposition 2.\nProof. Since the policy is a Gaussian random variable, (12) is given by\nJ\nπSAC\nϕ\nγ\n= EρπSAC\nϕ\n∞\nX\nk=0\nγkr(Ψ(k, x, πSAC\nϕ,k (x), πSAC\nϕ,k (x)) + α\n2 γk ln\n\u00002πeσ(xk)2\u0001\n=\n∞\nX\nk=0\nExk∼dk,ak∼π(ak|xk)γkr(Ψ(k, x, πSAC\nϕ,k (x), πSAC\nϕ,k (x))\n=\n∞\nX\nk=0\nExk\nh\nEak|xk\nh\nγk(1 −xT Qx) + α\n2 γk ln\n\u00002πeσ(xk)2\u0001ii\n= Eρ\n∞\nX\nk=0\nh\nγk(1 −xT\nk Qxk) + α\n2 γk ln\n\u00002πeσ(xk)2\u0001i\n=\n1\n1 −γ −Eρ\n\" ∞\nX\nk=0\nxT\nk Qxk\n#\n+ Eρ\n\" ∞\nX\nk=0\nα\n2 γk ln\n\u00002πeσ(xk)2\u0001\n#\nwhere dk is the marginal distribution of xk and ρ is the distribution of the trajectory (x0, x1, . . .). Since EπSAC\nϕ\n= π⋆maximizes\n1\n1−γ −Eρ\n\u0002P∞\nk=0 xT\nk Qxk\n\u0003\n. Moreover, as D is a compact set, supx∈D σ(x)2 := σ2\nD < ∞and we have the following bound\nEρ\n\" ∞\nX\nk=0\nα\n2 γk ln\n\u00002πeσ(xk)2\u0001\n#\n≤\nα\n2(1 −γ) ln\n\u00002πeσ2\nD\n\u0001\n.\nTherefore, by Assumption 2 (a) we have that ∥EπSAC\nϕ\n−π⋆∥∞→0 as α →0. This implies there exists α⋆> 0 such that\n∥EπSAC\nϕ\n−π⋆∥D < ϵ\nfor all α ∈(0, α⋆).\nProof of Proposition 3.\nBefore writing the proof we apologize that there is an error in the main submission for condition (38).\nThe correct condition should be\n|E\n\u0002\nπ⋆(x) −πϕ\nSAC(x)\n\u0003\n| ≤η∥x∥.\nMoreover, the value of η should be\nη <\nc\n∥∇Lγ∥2\nwhere ∥∇Lγ∥2 := supx∈D ∥∇Lγ(x)∥2.\nProof. This result will extend the practical stability of Corollary 1 to asymptotic stability. Let Lγ(x) be the Lyapunov function\nof the optimal policy π⋆as in Theorem 3. The Lyapunov function satisfies Lγ(0) = 0 and Lγ(x) > 0 for all x ∈D \\ {0}. We\nwill show that under the assumptions of the proposition that the same Lyapunov function verifies Lγ(y) −Lγ(x) < 0 for all\nx ∈D \\ {0} where y is the next state of the perturbed system\ny = f(x, π⋆(x)) + g(x)\nwhere g(x) = f(x, πSAC\nϕ\n(x)) −f(x, π⋆(x)). By the mean value theorem we have that\nLγ(f(x, π⋆(x)) + g(x)) = Lγ(f(x, π⋆(x))) + ∇Lγ(v)T g(x)\nand this implies that\nLγ(f(x, π⋆(x)) + g(x)) −Lγ(x) = Lγ(f(x, π⋆(x)) + g(x)) −Lγ(f(x, π⋆(x))) + Lγ(f(x, π⋆(x))) −Lγ(x)\n= ∇Lγ(v)T g(x) + Lγ(f(x, π⋆(x))) −Lγ(x)\n≤∥∇Lγ∥2∥g(x)∥2 −c∥x∥2\nand thus, we see for the perturbed system that\nLγ(f(x, π⋆(x)) + g(x)) −Lγ(x) < 0.\nTherefore, by Theorem 1.2 of (Bof, Carli, and Schenato 2018) we have that the Lyapunov function Lγ(x) verifies asymptotic\nstability for the system xk+1 = f(xk, πSAC\nϕ\n(xk)) and the neural policy is asymptotically stabilizing.\nRemark 6. As was mentioned previously, guaranteeing asymptotic convergence through the policy trained by RL is quite\nchallenging and to handle the limitations of practical stability, an algorithm likes ours should be used.\nProof of Theorem 3.\nThe idea of the proof entails using the neural policy to guide the system into a sufficiently small\nneighborhood of the equilibrium point. Once the system is within this region, linearization is applied to ensure local asymptotic\nstability. The following result is needed for the proof.\nProof. The linearization for the vector field f(x, πSAC\nϕ\n(x)) at the origin is given by\nA + B ˆK := ∂f\n∂x((0, πSAC\nϕ\n(0)) + ∂f\n∂u((0, πSAC\nϕ\n(0))∂π\n∂x(0) = ∂f\n∂x((0, πSAC\nϕ\n(0)) + ∂f\n∂u((0, πSAC\nϕ\n(0)) ˆK,\nwhere ˆK is the Jacobian of the neural policy. Let K be the optimal gain matrix of the actual system as a result of solving the\nDARE. By the Hartman-Grobman theorem there exists δ > 0 such that the system f(x, πSAC\nϕ\n(x)) is topologically conjugate to\nits linearization. By Assumption 4, we have that our learning algorithm converges to an estimate ˆK such that\n∥K −ˆK∥2 <\nϵK\n∥B∥2\n.\nMoreover, by Assumption 5 and Corollary 1, our learning algorithm converges to a neural policy such that is practically stabilizing\nto the neighborhood Bδ(0). Note that since the eigenvalues of a matrix are continuous functions of the entries of the matrix,\nthere exists ϵK > 0 such that for all matrices M satisfying ∥A + BK −M∥2 < ϵK, the eigenvalues λ1(M), . . . , λm(M) all\nhave negative real part. Since\n∥A + BK −(A + B ˆK)||2 ≤∥B∥2∥K −ˆK∥≤ϵK.\nwe have that the linearization for the system with the neural controller is Hurwitz. This proves local asymptotic stability of\nthe neural policy and combined with the fact that Bδ(0) is reachable for all x ∈D, this implies that the neural policy πSAC\nϕ\nis\nasymptotically stabilizing.\nAlternative Result to Theorem 2.\nTo end the section, as stated in Remark 3 we prove an alternative result involving different\nassumptions that is interesting in its own light. We provide an informal discussion in the hopes that it can lead to future work. Let\nΠNNet denote the space of neural networks.\nProposition 5. Fix ϵ > 0 and a consider a compact state space D. Let π⋆∈ΠNNet be the optimal policy that maximizes the\ndeterministic RL algorithm over ΠNNet with reward\nr(xn, un) = R −xT\nnQxn\nwhere Q is a positive definite matrix and suppose that the system xk+1 = f(xk, uk) is stabilizable with polynomial convergence\nrate\n∥xn∥≤\nC\n(n + 1)p\n(42)\nfor some p > 0 and C > 0. Furthermore, suppose that the discount factor γ satisfies\n\u0012 1 −λmin(Q)ϵ2\n1 −λmin(Q)ϵ2/4\n\u0013κ\n< γ ≤1\n(43)\nwhere\nκ =\n\u0012 ¯λϵ\n2C\n\u0013p\nand ¯λ =\nq\nλmin(Q)\nλmax(Q) . Then there exists N ∈N such that ∥xN∥< ϵ.\nProof. As a first step, we show that if the system is stabilizable then there exists a neural policy πϕ such that f(x, πϕ(x)) is\nlocally asymptotically stable. Denote the stabilizing controller as κ, the linearization at the origin is given by\n∇f π⋆\n|x=0 = A + B∇κ\nwhere A + B∇κ is Hurwitz. By the smooth version of the universal approximation theorem on the compact set D (which can be\nextended to vector value functions by placing the components of the function in a parallel architecture for the neural network)\nthere exists a sequence of neural networks πn\nϕ such that ∥∇κ −∇πn\nϕ∥D →0. This implies that\n∥∇f κ\n|x=0 −∇f πn\n|x=0∥= ∥B(∇κ −∇πn)∥\n≤∥B∥∥∇κ −∇πn∥→0\nas n →∞. Thus, it follows that the spectra converge\nσ(∇f πn\n|x=0) →σ(∇f κ\n|x=0)\nand there exists a neural policy πϕ such that A + B∇πϕ is Hurwitz and the system f(x, πϕ(x)) is asymptotically stable in\nsome neighborhood of the origin. From now on, we restrict x to this neighborhood as we can always use the previous results to\nformulate global guarantees.\nWithout loss of generality we can set R = 1 and rescale Q such that 1 −xT Qx ≥0 for all x ∈D.Now, suppose for a\ncontradiction that there exists ε > 0 such that Ψ(k, x, π⋆\nk(x)) /∈Bε(0) for all k. This implies that\nJπ⋆(x0) ≤1 −λmin(Q)ϵ2\n1 −γ\n.\nLet {xk}∞\nn=1 be the trajectory generated under the neural policy πϕ and since it is stabilizing we also suppose that it satisfies\nthe same polynomial convergence it approximates the derivatives of the stabilizing policy. By the polynomial convergence\nassumption we denote N as the first time step such that the following holds\n∥xN∥≤λmin\nλmax\nϵ.\nIn particular we have that N ≥\n\u0000 2C\n¯λϵ\n\u0001 1\np . Under this policy we can find a lower bound for the objective function\nJπϕ(x0) =\nN−1\nX\nk=1\nγn(1 −xT\nk Qxk) +\n∞\nX\nk=N\nγn(1 −xT\nk Qxk)\n>\n∞\nX\nn=N\nγn(1 −xλmax(Q)ϵ2\n4 )\n= γN (1 −λmax(Q) ϵ2\n4\n1 −γ\nand so substituting the bounds for N and γ will show that\nJπϕ > Jπ⋆\nwhich contradicts the optimality assumption for π⋆.\nRemark 7. This result can be combined with Assumption 4 to prove asymptotic convergence. The idea is to reach the region of\nattraction of the learned gain matrix that can be learned using our algorithm.\nStabilization of our learning algorithm\nThe figures below show that the algorithms presented in the numerical experiments section are also capable of learning an\nasymptotically stabilizing policy. Here we plot fifty trajectories of the positional components where the initial states are the same\nas the ones we use for the cost comparisons in Figure 2. We can also infer that the velocities must approach zero as the positional\ncomponents remain at zero.\n(a) Inverted Pendulum\n(b) Cartpole\n(c) 2D Quadrotor\n(d) Placeholder for 3D quadrotor\nFigure 3: Stabilizing Trajectories for Our Algorithm over the Four Environments\nComplete Learning Algorithm\nAlgorithm 1: Soft Actor Critic with LQR for Unknown Dynamics\n1: Initialize neural network parameters ψ, ¯ψ, θ, ϕ and replay buffer D\n2: Set input dimension (n + m), output dimension (n), discount factor (γ), entropy coefficient (α), distance tolerance (η),\npositive definite matrices (Q and R), reward scale, learning rate of each neural network\n3: Set a condition for updating A and B\n4: A, B, K ←Zero matrix, Kavailable ←False\n5: for each iteration do\n6:\nfor each environment step do.\n7:\nif ∥xk∥< η then\n8:\nTake a zero action or non-zero action ak ∼πϕ (ak | xk)\n9:\nStore (xk, ak) for (22, 23)\n10:\nelse\n11:\nak ∼πϕ (ak | xk)\n12:\nend if\n13:\nxk+1 ∼p (xk+1 | xk, ak)\n14:\nD ←D ∪{(xk, ak, r (xk, ak) , xk+1)}\n15:\nend for\n16:\nif Update A satisfied then\n17:\nUpdate A according to (22)\n18:\nend if\n19:\nif Update B Satisfied then\n20:\nUpdate B according to (23)\n21:\nCalculate K according to A, B and the DARE\n▷A is always updated before B\n22:\nKavailable ←True\n23:\nend if\n24:\nfor each gradient step do\n25:\nψ ←ψ −λV ˆ∇ψJV (ψ)\n26:\nθi ←θi −λQ ˆ∇θiJQ (θi) for i ∈{1, 2}\n27:\nif Kavailable then\n28:\nϕ ←ϕ −λπ ˆ∇ϕJ(ϕ) where the actor objective is given by (24)\n29:\nelse\n30:\nϕ ←ϕ −λπ ˆ∇ϕJπ(ϕ)\n31:\nend if\n32:\n¯ψ ←τψ + (1 −τ) ¯ψ\n33:\nend for\n34: end for\nAdditional Details on SAC and Verification of Stability\nData Collection for LQR\nSince collecting data while the agent navigates the environment can be a time consuming process,\nin many cases it is more efficient to collect state measures to get a baseline estimate of the gain matrix and improve this estimate\nas the algorithm runs.\nCost Matrices\nThe positive semidefinite matrices Q and R are always diagonal matrices. Please refer to the test files in the\ncode for the supplementary material to see the choice of Q and R.\nTarget network update\nSince SAC deals with continuous state spaces, it is common in this setting to use a separate target\nvalue function that tracks the actual value function to improve the stability of the learning algorithm. As in (Lillicrap et al. 2015),\nto update the target network, we use an exponentially moving average with smoothing constant τ. The possible values of τ range\nfrom zero to one, where a value of zero means no update to the target value network and a range of one means a complete copy\nof the current weights. We found that the same value of τ as in the original SAC paper was suitable for all the stabilization tasks\n(Haarnoja et al. 2018).\nReward Scale\nSAC is dependent on the scaling of the reward signal since this parameter affects the temperature parameter that\ncontrols the stochasticity of the policy. We found that a reward scale of two generally works well for our stabilization tasks.\nHyperparameters\nTable 1 below lists the hyperparameters that are common to all environments that are used in the comparative\nevaluation in Figure 2. We choose the hyperbolic tangent as the activation function as we believe that for continuous control\ntasks, the policy should vary smoothly with respect to the state and it also satisfies the assumptions of Theorem 2. Note that since\nPPO is an on-policy algorithm, we do not implement a replay buffer for that algorithm. Additionally, Table 2 lists the size of the\nneural networks for all stabilization tasks. We note that since stabilization requires precise actions near the origin and the policy\nmay have to learn a potentially large gain matrix at the origin, a small network may not generalize to large domains as we have in\nFigure 3. Therefore, we employ larger networks for greater approximation ability.\nTable 1: SAC Hyperparameters\nParameter\nValue\nShared\noptimizer\nAdam (Kingma and Ba 2015)\nlearning rate\n3 · 10−4\ndiscount (γ)\n0.99\nreplay buffer size\n106\nnumber of samples per minibatch\n256\nactivation\nHyperbolic Tangent\nSAC\ntarget smoothing coefficient (τ)\n0.005\ntarget update interval\n1\ngradient steps\n1\nreward scale\n2\nTable 2: SAC Environment Specific Networks\nEnvironment\nAction Dimensions\nNumber of Hidden Layers\nHidden Layer Neurons\nInverted Pendulum\n1\n2\n16\nCartpole\n1\n2\n256\nQuadrotor-2D\n2\n4\n256\nQuadrotor-3D\n4\n5\n256\nVerification for stability\nWe verify the stability of the inverted pendulum with the quadratic Lyapunov function V = xT Px = 1.02512x2\n1+0.0216179x2\n2+\n0.0379687x1x2, obtained by linearizing the continuous-time system. The matrix P is computed by solving the Lyapunov equation\nAT P + PA = −Q by setting Q = I. As illustrated in Fig. 4, the red ellipse is the ROA estimate, while the green and blue\ncurves are the level sets of the Lyapunov function.\nFigure 4: Verified ROA estimate for the inverted pendulum case, where the red curve is the ROA estimate.\nLimitations and Future Work\nConvergence Analysis: In the convergence analysis we assumed that the RL algorithm is capable of learning a near optimal\npolicy. An interesting direction for future research could be to establish conditions for convergence in the neural network setting\nand to derive results for convergence rates or finite sample approximation guarantees.\nPractical Applications: An interesting research direction would be to assess the performance improvements of our stabilizing\ncontrollers for practical safety critical tasks.\nVerification: With LyZNet, we can only verify the asymptotic stability for the low-dimensional systems with the learned neural\npolicy, due to the poor scalability of the SMT solver. In the future, we will study how to verify the stability for higher-dimensional\nsystems, given the complexity of the neural network controller.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.SY",
    "math.DS"
  ],
  "published": "2024-09-12",
  "updated": "2024-09-12"
}