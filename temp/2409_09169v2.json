{
  "id": "http://arxiv.org/abs/2409.09169v2",
  "title": "Curricula for Learning Robust Policies with Factored State Representations in Changing Environments",
  "authors": [
    "Panayiotis Panayiotou",
    "Özgür Şimşek"
  ],
  "abstract": "Robust policies enable reinforcement learning agents to effectively adapt to\nand operate in unpredictable, dynamic, and ever-changing real-world\nenvironments. Factored representations, which break down complex state and\naction spaces into distinct components, can improve generalization and sample\nefficiency in policy learning. In this paper, we explore how the curriculum of\nan agent using a factored state representation affects the robustness of the\nlearned policy. We experimentally demonstrate three simple curricula, such as\nvarying only the variable of highest regret between episodes, that can\nsignificantly enhance policy robustness, offering practical insights for\nreinforcement learning in complex environments.",
  "text": "Curricula for Learning Robust Policies with Factored\nState Representations in Changing Environments\nPanayiotis Panayiotou\nDepartment of Computer Science\nUniversity of Bath\nBath, United Kingdom\npp2024@bath.ac.uk\nÖzgür ¸Sim¸sek\nDepartment of Computer Science\nUniversity of Bath\nBath, United Kingdom\no.simsek@bath.ac.uk\nAbstract\nRobust policies enable reinforcement learning agents to effectively adapt to and\noperate in unpredictable, dynamic, and ever-changing real-world environments.\nFactored representations, which break down complex state and action spaces into\ndistinct components, can improve generalization and sample efficiency in policy\nlearning. In this paper, we explore how the curriculum of an agent using a factored\nstate representation affects the robustness of the learned policy. We experimentally\ndemonstrate three simple curricula, such as varying only the variable of highest\nregret between episodes, that can significantly enhance policy robustness, offering\npractical insights for reinforcement learning in complex environments.\n1\nIntroduction\nReinforcement learning has had remarkable success across a wide range of domains, including\nenergy management [30], robotic control [26], and strategic board games [24]. However, in many\napplications, performance is evaluated solely on the training environment, often neglecting the\nimportance of generalisation. This lack of emphasis contributes to some of the central challenges in\nreinforcement learning, including weak transferability between tasks and the brittleness of policies to\nsmall changes in environments or random seeds [32, 8, 14, 27]. Additionally, reinforcement learning\nalgorithms often suffer from low sample efficiency, requiring large amounts of data to achieve robust\nperformance.\nFactored representations [20] decompose high-dimensional, unstructured state and action spaces\ninto a few low-dimensional and high-level variables, each representing distinct and potentially\nindependent aspects of the environment. This decomposition reduces the problem’s dimensionality,\npossibly requiring fewer samples to learn a well-performing policy [29, 2]. Additionally, factored\nrepresentations can enhance a policy’s ability to generalise across different parts of the state space,\nmaking it more robust and transferable [1, 9].\nCurriculum learning [4] is a training strategy that structures the learning process, such as by organising\ndifferent subtasks in a particular sequence, with the goal of improving the learning speed or final\nperformance. This can involve progressively increasing task difficulty or transferring knowledge\nbetween tasks of similar complexity. In reinforcement learning [16, 18, 17], this strategy involves\ntraining an agent on a sequence of different tasks, enabling it to leverage the knowledge gained from\nsimpler tasks to tackle more challenging ones. This strategy can improve sample efficiency and\nenhance the robustness of the learned policies [25]. For example, Quick Chess is a simplified version\nof chess that starts with easier subgames and gradually introduces the player to the whole game [17].\nAs shown in Figure 1, early subgames can include only pawns to teach players how pawns move,\nattack and get promoted.\n17th European Workshop on Reinforcement Learning (EWRL 2024).\narXiv:2409.09169v2  [cs.LG]  19 Sep 2024\nFigure 1: Quick Chess subgames, increasing in complexity from left to right (image source: Narvekar\net al. [17]).\nThe real world is non-stationary and unpredictable, and we cannot capture all its variability in a static\ndataset or learning environment. In the real world, no two tasks are ever exactly the same, even if they\nmay seem so in a simulation environment. Therefore, we aim to train robust policies that generalise\neffectively and adapt to unseen environments caused by distributional shifts [13]. For example, a\ndomain shift might change the position of an object between different runs, or a task shift might\nchange the designated endpoint in a navigation task.\nWhile factored representations can help in learning more robust policies [1, 9], the role of curriculum\nlearning in enhancing these policies remains underexplored. In this paper, we experimentally\ninvestigate how curriculum learning can improve the generalisation and adaptability of these policies\nto novel environments. We demonstrate the following:\n1. Without factored representations, simple curricula are insufficient for training robust policies\nthat generalise well to unseen environments.\n2. Using factored representations, a curriculum of random shifts (domain randomisation) can\nenable learning robust policies.\n3. Using factored representations, a curriculum of shuffling a few diverse examples can allow\nlearning robust policies.\n4. Using factored representations, we can design a curriculum for learning robust policies by\nidentifying and adjusting the factors that cause the largest performance discrepancy (regret)\nwhen altered.\n2\nPreliminaries\nMarkov Decision Processes.\nA Markov decision process (MDP) is a mathematical framework\nused to model decision-making problems. An MDP is defined by a tuple (S, A, P, R, γ):\n• S is a set of states.\n• A is a set of actions.\n• P : S × A × S →[0, 1] is a transition probability function, where P(s′|s, a) denotes the\nprobability of transitioning to state s′ from state s after taking action a.\n• R : S × A × S →R is a reward function, where R(s, a, s′) gives the expected reward for\ntaking action a in state s and transitioning to state s′.\n• γ ∈[0, 1] is a discount factor.\nReinforcement Learning.\nMost commonly, the reinforcement learning problem is modelled as a\nMarkov Decision Process. In this framework, a policy π(a|s) represents the probability of taking\naction a when the agent is in state s. The objective is to learn a policy that maximises the expected\ncumulative return Eπ[Gt], which is the sum of discounted rewards over time when following policy\nπ. The return Gt from time step t is defined as:\nGt =\n∞\nX\nk=0\nγkrt+k+1.\n2\nwhere rt indicates the reward at time step t. An agent interacts with its environment by taking actions\nbased on its policy, receiving feedback in the form of rewards, and using this feedback to learn and\nimprove its policy over time.\nDynamic Bayesian Networks.\nA Dynamic Bayesian Network is a probabilistic graphical model\nrepresenting a set of variables and their conditional dependencies as a directed acyclic graph. It is\nspecifically designed to model sequences of variables over time. In a Dynamic Bayesian Network, the\nstate at time t, denoted Xt = (X1,t, X2,t, . . . , Xn,t), depends on the state at time t−1, denoted Xt−1.\nThe joint probability distribution over Xt is given by the product of the conditional probabilities of\neach variable given its parents in the graph, including temporal dependencies:\nP(Xt | Xt−1) =\nn\nY\ni=1\nP(Xi,t | Pa(Xi,t)),\nwhere the parents of Xi,t, denoted as Pa(Xi,t), include variables from both Xt−1 and Xt.\nFactored Representations.\nFactored representations can decompose the state and action spaces\ninto sets of variables, each representing different components of the environment. Formally, an\natomic state s is represented as a vector of high-level factors s = (x1, x2, . . . , xn), and similarly an\natomic action a = (y1, y2, . . . , ym).\nFactorisation of MDPs.\nA Factored Markov decision process is a type of MDP in which the\nstate space, action space, transition model, and sometimes the reward function are represented in\na factored form. Factored models leverage structure in the problem to manage complexity. They\ncan make solving larger MDPs more computationally feasible without losing accuracy because they\nrepresent the MDP more compactly, reducing the number of parameters. They can also generalise\nbetter in environments with large state or action spaces, allowing for more efficient policy learning\nand planning.\nThe atomic state s and atomic action a can be represented as a factored representation of high-level\nfactors. The transition probabilities P(s′|s, a) depend on a subset of state and action variables and\nare often represented using a Dynamic Bayesian Network. Similarly, the reward function can be\ndefined as the sum of local reward functions Ri, depending only on a subset of the state and action\nvariables.\nDistribution Shifts.\nDistribution shifts refer to changes in the data distribution encountered by an\nagent during different phases of learning, such as between training and testing. In reinforcement\nlearning, the environment is often characterised by a set of variables that define its state and dynamics,\nsuch as the transition probabilities, reward functions, or physical properties (e.g. grid size in a\ngrid-world task, friction coefficients in a robotic simulation, etc.). A distribution over environments\nrefers to the probabilistic distribution of these variables. By sampling from this distribution, we obtain\ndifferent instances of the environment, each with potentially different characteristics. Addressing\ndistribution shifts is critical because real-world environments are typically non-stationary, meaning\nthat the variables defining the environment can change over time.\nWe can distinguish between three different types of learning environments [13]. First, there are\nsingleton environments where the training and testing environments are identical. Secondly, there are\nindependent and identically distributed (IID) environments where training and testing environments\nare different but from the same distribution. Thirdly, there are out-of-distribution environments where\nthe training and testing environments are from different distributions.\nLow-Regret Policies.\nRegret is a measure of how much the performance of a policy (expected\ndiscounted cumulative reward) falls short of the optimal performance. Formally, the regret after T\ntime steps of following policy π from an initial state s0 can be defined as:\nRegretπ(s0, T) =\nT\nX\nt=0\n(V ∗(st) −V π(st)) ,\nwhere π∗is the optimal policy, V ∗(st) is the value function of the optimal policy at state st repre-\nsenting the expected discounted cumulative reward from that state, and V π(st) is the value function\nof the current policy π at state st.\n3\nWe consider low-regret policies robust because a low regret ensures that the performance difference\ncompared to the optimal policy is minimised, demonstrating the policy’s ability to handle various\nscenarios and adapt to changes effectively.\n3\nBackground\n“No man ever steps in the same river twice.”\n– Heraclitus\nTo effectively apply reinforcement learning in the real world, we must account for its non-stationary\nnature. Reflecting the idea of a constantly evolving environment, recent reinforcement learning\nresearch focuses on developing robust policies that can handle changing dynamics [12, 10, 11], high-\nlighting the need for policies that work in varied settings. Distribution shifts can significantly impact\nperformance, leading to poor generalisation and arbitrarily high errors [22, 21]. For reinforcement\nlearning to be successful in the real world, we must consider robustness and how shifts (e.g. an object\nchanging colour) can impact both the domain [7] and the task itself [31].\nFactored state representations, which involve breaking down the environment into distinct components,\nare an active area of research [28, 15, 3]. These representations have been shown to improve the\nsample efficiency of reinforcement learning algorithms in both tabular and deep reinforcement\nlearning methods [29, 2]. Additionally, they can help learn policies that are robust to domain shifts\n[1, 9]. It has also been proven that in scenarios where only the agent’s decisions causally influence\nthe reward (e.g. multi-armed bandits where the state does not affect the reward), all robust agents\nlearn an approximate causal model [21], which implies a factored representation.\nCurriculum learning in reinforcement learning structures an agent’s learning process by strategically\nordering tasks that the agent experiences [17]. It typically aims to enhance the agent’s performance\nand learning speed by enabling the forward transfer of skills from simpler tasks to more challenging\nones. A structured curriculum involves several key decisions: choosing the initial set of tasks,\ndefining the progression of tasks, and establishing criteria for transitioning between them. Examples\nof such curricula include the work of Silva and Costa [23], where tasks are randomly generated\nand grouped based on their “transfer potential”, and Narvekar et al. [17], where a set of source\ntasks is continuously refined to match the agent’s current abilities using methods like mistake-driven\nsubtasks, which help the agent correct erroneous behaviour. Similarly, unsupervised environment\ndesign [6] is a reinforcement learning training strategy that automatically generates a series of training\nenvironments to learn robust policies. Notable work in this area is ACCEL [19], which uses an\nevolutionary environment generator and regret-based feedback to make small edits to the environment\nand gradually introduce the agent to more complexity to train a robust policy.\n4\nThe Shifting Frozen Lake\nWe define the Shifting Frozen Lake environment, where aspects of the environment can exhibit a\nshifting behaviour, allowing us to test for out-of-distribution generalisation.\nFrozen Lake [5] is a grid-world environment where the agent navigates from a designated start cell\n(top-left) to a goal cell (bottom-right). The agent can move up, down, left, or right, and must avoid\nfalling into holes along the way. Depending on the configuration, the actions can be either stochastic\nor deterministic. For an example, refer to Figure 2.\nIn the original Frozen Lake environment, the start location, goal location, hole locations, and grid\nsize are kept constant throughout all the episodes. In Shifting Frozen Lake, the grid size N, the\npositions of the holes, the starting point, and the goal location can change from one episode to the next.\nFor simplicity, we assume that these variables remain constant during an episode despite potential\nchanges, such as warm weather that could cause the lake to start melting. Due to the changing nature\nof the environment (e.g. the start location might change), we will refer to different instances of the\nenvironment as “examples”.\nBelow is a full specification of the task:\n• Actions: Left, down, right, up with deterministic transitions.\n4\nhole\nlocations\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\ndistance\nmatrix\n6\n5\n4\n4\n5\n∞\n3\n∞\n4\n3\n2\n∞\n∞\n2\n1\n0\ngrid size\n4\nagent\nlocation\n(0, 0)\ngoal\nlocation\n(3, 3)\nFigure 2: A sample Frozen Lake environment. On the right hand side, we present a factored\nrepresentation of the state. Using this factored representation, the transition function can be factorised\nusing a Dynamic Bayesian Network (see Figure 12 in Appendix A).\n• State: This is an N × N matrix that shows where the agent, the goal, the holes and the\nfrozen squares are. We can factorise this using the 5-tuple (grid size, hole locations, agent\nlocation, goal location, distance matrix) as seen in Figure 2.\n• Start state: An initial location [xI, xI] where N × N are the dimensions of the grid, and\n0 ≤xI, yI < N.\n• Goal state: A location [xG, yG] where N × N are the dimensions of the grid, and 0 ≤\nxG, yG < N. All examples always include a possible path from the start state to the goal\nstate.\n• Rewards: −0.1 for each move, an additional +10 for reaching the goal, and an additional\n−10 for reaching a hole. The discount factor is γ = 1.\n• The episode ends if the player moves into a hole or the goal state.\nEnvironment shifts.\nThe initial location, goal location, hole locations, and grid size can change\nfrom episode to episode. The environment supports the following shifting behaviours and the\nfunctionality to switch between them:\n• No Shifting: The variables are sampled once upon the creation of the environment and\nremain constant for all episodes.\n• Random Shifting: At the start of each new episode, the environment uniformly resamples\nall variables (start location, goal location, hole locations, grid size).\n• Single Preset Variable Shifting: One variable is specified to shift. Upon the creation of the\nenvironment, all variables are sampled once. In each episode, only the chosen variable is\nresampled.\n• Single Random Variable Shifting: Upon creation, all variables are sampled. In each\nepisode, one randomly chosen variable is resampled, changed, and reverted at the end of the\nepisode.\n• Stored Examples Shifting: Upon creation, a sample of N examples is stored. For each new\nepisode, one of these examples is randomly selected and used.\nThe state can be factorised by using variables that denote the grid size, hole locations, goal location,\nthe current agent location, and a distance matrix from the goal location. However, this factorisation\nhas redundancies, e.g. the hole locations can be inferred from the distance matrix. We can optimise the\nfactored representation by retaining only the relevant variables, reducing redundancy and improving\nefficiency. For example, using only the distance matrix and the current agent location, an agent can\nlearn an optimal and robust policy by always taking the shortest path to the end.\n5\n5\nExperiments\nOur experiments include the following agents:\n• Random Action Selection: Selects action uniformly at random. Used as a baseline.\n• Optimal: Achieves the highest possible performance by using breadth-first search to pick\nthe direction with the smallest distance to the goal (without falling into a hole).\n• PPO: Without using a factored representation, we apply a convolutional neural network to\nthe grid, where each tile is one-hot encoded in a separate channel. We pad the grid with a\nspecial character so all grids have the same size.\n• PPO-F: A PPO agent using an optimised factored representation, retaining only the imme-\ndiate neighbourhood in the distance matrix, which is sufficient for the agent to act optimally\nin this task. The agent does not model the transition function or use the assumption that the\ntransition function can be factorised.\nWe run all the experiments for five agents and plot the mean and standard error of the total undis-\ncounted reward per epoch (γ = 1). Each epoch consists of 900 time steps, and each episode has a\ntimeout of 100 time steps. Performance scores around −30 indicate “stuck” behaviour, where agents\navoid losses by engaging in repetitive, looping movements, such as endlessly alternating between\nleft and right actions. Scores higher than −30 but worse than optimal performance indicate an agent\nthat solves some of the grids. For these experiments, we consider random shifting (resampling all\nvariables at the start of each episode) as a test of deep understanding and generalisation of the task\nbecause it requires agents to know how to navigate to the goal from anywhere and avoid holes.\nWe explore the following curricula, with changes in curriculum phases indicated by vertical dotted\nlines in the figures:\n(A) No Shifting to Random Shifting: Fit a single example, then shift all variables randomly to\ntest generalisation.\n(B) No Shifting to Single Random Variable Shifting: Fit a single example and then randomly\nshift only one variable per episode.\n(C) Random Shifting: Test generalisation from diverse training (domain randomisation) by\nshifting all variables randomly from the start.\n(D) Stored Examples to Random Shifting: Train a policy by shuffling a few pre-sampled\nexamples and then test generalisation by shifting all variables randomly.\n(E) Single Preset Variable Shifting to Random Shifting: Shift only one specified variable\ninitially, then shift all variables randomly to test generalisation.\nCurriculum (A): No Shifting to Random Shifting.\nWe test generalisation from a single example\nand present the results in Figure 3. When fitting a single example, the methods show a significant\nstandard error because the grid size can vastly change the reward per epoch. For instance, reaching\nthe goal in 3 steps on a 4x4 grid gives 970 points per epoch, while 20 steps on a 10x10 grid give only\n120 points per epoch. None of the trained methods demonstrate significant knowledge transfer after\nthe shift, as their performance drops to around 0. After the shift, PPO exhibits “stuck” behaviour,\nrepeatedly moving left/right or up/down. PPO-F is more active but only solves about 25% of the\nexamples right after the shift. It loses in around 9% of the examples and displays “stuck” behaviour\nin the rest.\nCurriculum (B): No Shifting to Single Random Variable Shifting.\nIn Figure 4, we see that\nboth PPO and PPO-F exhibit similarly low knowledge transfer and robustness as there is a big\nperformance drop when random shifting starts. Notably, after a few epochs with random shifting,\nPPO-F adapts quickly to the new task distribution.\nCurriculum (C): Random Shifting.\nIn Figure 5, we evaluate how well the agents can generalise\nfrom diverse training. The test and train distributions of the environment here are identical, so\nthis is IID generalisation. Note, however, that diverse training complicates the learning task. A\ncloser examination of the PPO agent behaviour reveals that it often fails to reach the goal in any\n6\nFigure 3: Curriculum (A): No Shifting to Ran-\ndom Shifting.\nFigure 4: Curriculum (B): No Shifting to Single\nRandom Variable Shifting.\nFigure 5: Curriculum (C): Random Shifting.\nFigure 6: Curriculum (D): 15 Stored Examples\nto Random Shifting.\nFigure 7: Preliminary Experiment for Curricu-\nlum (E): Fit PPO-F to a single example, then\nshift only one of the four variables.\nFigure 8: Curriculum (E): Single Preset Variable\nShifting to Random Shifting on PPO-F.\nFigure 9:\nPPO-F performance after training\nwith different numbers of stored examples, fol-\nlowed by 50 epochs of random shifting (test).\nFigure 10: Comparing Curricula for Factored\nAgents.\n7\nC\nD\nE-holes\nE-goal\n0\n2,000\n4,000\n6,000\n8,000\nRegret\nFigure 11: Comparing the regret of robust policies learnt by following Curricula C, D, E with varying\nholes, and E with a varying goal location.\nepisode, reverting to “stuck” behaviour. Factored variables, however, provide a significant advantage\nin discovering the task structure. The PPO-F agent identifies a robust policy within 75 epochs.\nBut its policy is far from optimal. We examined the agent’s performance over 50 epochs following\nstabilisation and found that 19% of its movements were repetitive, back-and-forth motions. It executed\n3.6× more moves than the optimal agent and, on average, fell into a hole 24.8 times per epoch.\nCurriculum (D): Stored Examples to Random Shifting.\nIn Figure 6, we evaluate how well the\nagent generalises using only 15 training examples. PPO exhibits “stuck” behaviour after the shift and\nshows no signs of knowledge transfer or robustness. PPO-F shows strong knowledge transfer and\nrobustness, performing on par with diverse training after seeing only 15 examples. We examined the\nagent’s performance over 50 epochs following stabilisation and found that 35% of its movements\nwere repetitive, back-and-forth motions. It executed 4.5× more moves than the optimal agent and,\non average, fell into a hole 7.8 times per epoch. This experiment demonstrates that a few diverse\nexamples are sufficient to build a robust policy over a factored state representation.\nCurriculum (D) Follow-Up.\nWe further investigate in Figure 9 how many stored examples are\nneeded to achieve good test performance under Random Shifting. We train multiple PPO-F agents\nwith different numbers of training examples and estimate their test performance by averaging over\n50 epochs under Random Shifting. Generally, we expect more diverse training with more examples\nto correlate with improved performance. However, there are diminishing returns, as fitting more\nexamples takes longer and does not necessarily result in better performance. Performance gains level\noff after fitting 13 examples, and training with more examples significantly increases the training\ntime.\nPreliminary Experiment for Curriculum (E).\nWe train PPO-F on a single example and then shift\nonly a specific variable on each episode (Single Preset Variable Shifting), as shown in Figure 7. We\nfind that shifting the goal location, start location, and hole locations leads to high regret while shifting\nthe grid size does not. In Curriculum (E), we then investigate if shifting only one of these during\ntraining is enough to learn a robust policy.\nCurriculum (E): Single Preset Variable Shifting to Random Shifting.\nIn Figure 8, we evaluate\nhow well the PPO-F agent generalises when only one variable is shifted during training. We examine\nfour training curricula, each shifting only one variable (holes, grid size, goal location, or start location).\nWe test OOD generalisation by exposing the agents to random shifts. We find that varying just one\nvariable, either hole locations or goal location, leads to learning a robust policy. Two of the three\nvariables that cause high regret are sufficient by themselves when shifted to form a curriculum for\ntraining a robust agent. We examined the agents’ performance over 50 epochs following stabilisation\nand found that when changing the holes, they executed 6.8× more moves than the optimal agent and,\n8\non average, fell into a hole 5.2 times per epoch. When changing the goal location, they executed\n8.8× more moves than the optimal agent and, on average, fell into a hole 5.4 times per epoch.\nComparing Curricula for Factored Agents.\nIn Figure 10, we compare PPO-F agents trained with\nfour robust curricula (C, D, E with varying holes, and E with varying goals) and a single-example\ncurriculum (A). All robust curricula outperform the single-example curriculum after the start of\nrandom shifting. Training with random shifts gives the best immediate post-shift performance.\nHowever, the other robust curricula perform similarly. Curricula (D) and (E) (with a varying goal)\nadapt quickly and reach the same performance as (C). The horizontal dotted line represents the “stuck”\nbehaviour observed by the single-example curriculum after the shift. Pre-shift performance is not\ncomparable between agents because each curriculum exposes agents to different environments, and\nsmaller grids lead to higher total rewards per epoch.\nRegret Analysis of Robust Policies.\nIn Figure 11, we rank the four robust curricula (C, D, E-holes,\nE-goals) by increasing regret and decreasing robustness (from left to right). This ranking also reflects\ndecreasing risk-taking and falling in holes. Curriculum (C) benefits from test and train environments\nbeing identical (IID generalisation). The two (E) curricula only modify one variable at a time but\nlearn robust policies. We suggest that changing the variable that shifts during training could further\nenhance policy robustness.\n6\nDiscussion\nFirst, our results demonstrate that methods using factored representations can help learn robust\npolicies more easily. Agents using an atomic state representation usually fail to reach the goal\nwhen the environment has distribution shifts. While a tailored curriculum could help such agents to\nlearn robust policies, simple curricula may be enough for agents that use a factored representation.\nSecondly, the curriculum used significantly impacted the robustness of the learned policy over a\nfactored state representation. The agents learned comparably robust policies with either diverse\ntraining, shuffling a few stored examples, or by shifting a single variable that caused high regret when\naltered alone (true for two out of three variables). We also quantitatively compare the robustness\nof the learned policies following each of the curricula and point out the effect of the curriculum on\nthe risk aversion and performance of the learned policies. Lastly, we believe that enabling agents to\nautonomously generate their own curricula by identifying and adjusting variables that require further\nexploration (such as those causing high regret) will lead to learning even more robust policies and\nbetter generalization across diverse environments.\nAcknowledgements\nThis work was supported by the UKRI Centre for Doctoral Training in Accountable, Responsible and\nTransparent AI (ART-AI) [EP/S023437/1]. We thank Joshua B. Evans for useful discussions.\nReferences\n[1] Mahsa Baktashmotlagh, Masoud Faraki, Tom Drummond, and Mathieu Salzmann. Learning\nfactorized representations for open-set domain adaptation. arXiv preprint arXiv:1805.12277,\n2018.\n[2] Bharathan Balaji, Petros Christodoulou, Xiaoyu lu, Byungsoo Jeon, and Jordan Bell-Masterson.\nFactoredrl: Leveraging factored graphs for deep reinforcement learning. In NeurIPS 2020\nWorkshop on Deep Reinforcement Learning, 2020.\n[3] Yoshua Bengio. Deep learning of representations: Looking forward. In International conference\non statistical language and speech processing, pages 1–37. Springer, 2013.\n[4] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.\nIn Proceedings of the 26th annual international conference on machine learning, pages 41–48,\n2009.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym, 2016.\n9\n[6] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew\nCritch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised\nenvironment design. Advances in neural information processing systems, 33:13049–13061,\n2020.\n[7] Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R Arabnia. A brief review of\ndomain adaptation. Advances in data science and information engineering: proceedings from\nICDATA 2020 and IKE 2020, pages 877–894, 2021.\n[8] Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization\nin dqn. arXiv preprint arXiv:1810.00123, 2018.\n[9] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for non-\nstationary reinforcement learning. Advances in Neural Information Processing Systems, 35:\n31957–31971, 2022.\n[10] Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srini-\nvasan, Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement\nlearning algorithms in observational health settings. arXiv preprint arXiv:1805.12298, 2018.\n[11] Jixian Guo, Mingming Gong, and Dacheng Tao. A relational intervention approach for un-\nsupervised dynamics generalization in model-based reinforcement learning. arXiv preprint\narXiv:2206.04551, 2022.\n[12] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:\nModel-based policy optimization. Advances in neural information processing systems, 32,\n2019.\n[13] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A survey of zero-shot\ngeneralisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76:\n201–264, 2023.\n[14] Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Rein-\nforcement Learning: State-of-the-Art, pages 143–173. Springer, 2012.\n[15] Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and\nMichael Tschannen. Weakly-supervised disentanglement without compromises. In International\nconference on machine learning, pages 6348–6359. PMLR, 2020.\n[16] Sanmit Narvekar. Curriculum learning in reinforcement learning. In IJCAI, pages 5195–5196,\n2017.\n[17] Sanmit Narvekar, Jivko Sinapov, Matteo Leonetti, and Peter Stone. Source task creation for\ncurriculum learning. In Proceedings of the 2016 international conference on autonomous agents\n& multiagent systems, pages 566–574, 2016.\n[18] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone.\nCurriculum learning for reinforcement learning domains: A framework and survey. Journal of\nMachine Learning Research, 21(181):1–50, 2020.\n[19] Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward\nGrefenstette, and Tim Rocktäschel. Evolving curricula with regret-based environment design.\nIn International Conference on Machine Learning, pages 17473–17498. PMLR, 2022.\n[20] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference.\nMorgan kaufmann, 1988.\n[21] Jonathan Richens and Tom Everitt. Robust agents learn causal world models. arXiv preprint\narXiv:2402.10877, 2024.\n[22] Stephane Ross and J Andrew Bagnell. Agnostic system identification for model-based rein-\nforcement learning. arXiv preprint arXiv:1203.1007, 2012.\n[23] Felipe Leno Da Silva and Anna Helena Reali Costa. Object-oriented curriculum generation for\nreinforcement learning. In Proceedings of the 17th international conference on autonomous\nagents and multiagent systems, pages 1026–1034, 2018.\n[24] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of go with deep neural networks and tree search. nature, 529(7587):484–489,\n2016.\n10\n[25] Yeeho Song and Jeff Schneider. Robust reinforcement learning via genetic curriculum. In 2022\nInternational Conference on Robotics and Automation (ICRA), pages 5560–5566. IEEE, 2022.\n[26] Peter Stone, Richard S Sutton, and Gregory Kuhlmann. Reinforcement learning for robocup\nsoccer keepaway. Adaptive Behavior, 13(3):165–188, 2005.\n[27] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A\nsurvey. Journal of Machine Learning Research, 10(7), 2009.\n[28] Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-\nJean Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable\nfactors. arXiv preprint arXiv:1708.01289, 2017.\n[29] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Rus-\nlan Salakhutdinov.\nLearning factorized multimodal representations.\narXiv preprint\narXiv:1806.06176, 2018.\n[30] Tianshu Wei, Yanzhi Wang, and Qi Zhu. Deep reinforcement learning for building hvac control.\nIn Proceedings of the 54th annual design automation conference 2017, pages 1–6, 2017.\n[31] Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning—a\ncomprehensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern\nanalysis and machine intelligence, 41(9):2251–2265, 2018.\n[32] Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep\nreinforcement learning. arXiv preprint arXiv:1804.06893, 2018.\n11\nA\nAdditional Figures\ndistance\nmatrix, t\ngrid size, t\ngoal\nlocation, t\nhole\nlocations, t\nagent\nlocation, t\ndistance\nmatrix, t + 1\ngrid size, t + 1\ngoal\nlocation, t + 1\nhole\nlocations, t + 1\nagent\nlocation, t + 1\nFigure 12: A Dynamic Bayesian Network for the factored MDP of the Shifting Frozen Lake. The\ndistance matrix (from the goal location), the grid size, the goal location and the hole locations are\nconstant throughout each episode.\n12\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-09-13",
  "updated": "2024-09-19"
}