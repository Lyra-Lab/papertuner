{
  "id": "http://arxiv.org/abs/2409.09509v1",
  "title": "Learning Nudges for Conditional Cooperation: A Multi-Agent Reinforcement Learning Model",
  "authors": [
    "Shatayu Kulkarni",
    "Sabine Brunswicker"
  ],
  "abstract": "The public goods game describes a social dilemma in which a large proportion\nof agents act as conditional cooperators (CC): they only act cooperatively if\nthey see others acting cooperatively because they satisfice with the social\nnorm to be in line with what others are doing instead of optimizing\ncooperation. CCs are guided by aspiration-based reinforcement learning guided\nby past experiences of interactions with others and satisficing aspirations. In\nmany real-world settings, reinforcing social norms do not emerge. In this\npaper, we propose that an optimizing reinforcement agent can facilitate\ncooperation through nudges, i.e. indirect mechanisms for cooperation to happen.\nThe agent's goal is to motivate CCs into cooperation through its own actions to\ncreate social norms that signal that others are cooperating. We introduce a\nmulti-agent reinforcement learning model for public goods games, with 3 CC\nlearning agents using aspirational reinforcement learning and 1 nudging agent\nusing deep reinforcement learning to learn nudges that optimize cooperation.\nFor our nudging agent, we model two distinct reward functions, one maximizing\nthe total game return (sum DRL) and one maximizing the number of cooperative\ncontributions contributions higher than a proportional threshold (prop DRL).\nOur results show that our aspiration-based RL model for CC agents is consistent\nwith empirically observed CC behavior. Games combining 3 CC RL agents and one\nnudging RL agent outperform the baseline consisting of 4 CC RL agents only. The\nsum DRL nudging agent increases the total sum of contributions by 8.22% and the\ntotal proportion of cooperative contributions by 12.42%, while the prop nudging\nDRL increases the total sum of contributions by 8.85% and the total proportion\nof cooperative contributions by 14.87%. Our findings advance the literature on\npublic goods games and reinforcement learning.",
  "text": "LEARNING NUDGES FOR CONDITIONAL COOPERATION: A\nMULTI-AGENT REINFORCEMENT LEARNING MODEL\nShatayu Kulkarni, Sabine Brunswicker\nResearch Center for Open Digital Innovation (RCODI)\nPurdue University\nWest Lafayette, IN, USA\nshatayu@alumni.purdue.edu, sbrunswi@purdue.edu\nABSTRACT\nThe public goods game describes a social dilemma in which a large proportion of agents act as\nconditional cooperators (CC): they only act cooperatively if they see others acting cooperatively\nbecause they satisfice with the social norm to be in line with \"what others are doing\" instead of\noptimizing cooperation. CCs are guided by aspiration-based reinforcement learning guided by past\nexperiences of interactions with others and satisficing aspirations. In many real-world settings,\nreinforcing social norms do not emerge in the first place, causing defect to take hold. In this paper, we\nput forward the argument that an optimizing reinforcement agent can facilitate cooperation by acting\nas a \"social planner\" using \"nudges\", i.e. indirect mechanisms for cooperation to happen. The agent’s\ngoal is to motivate CCs into cooperation through its own actions in order to create social norms that\nsignal that others are cooperating. We propose a multi-agent reinforcement learning model for public\ngoods games, with 3 CC learning agents using aspirational reinforcement learning and 1 nudging\nagent who uses deep reinforcement learning to learn \"nudges\" that optimizes cooperation in the\npublic goods game. For our nudging agent, we model two distinct reward functions, one maximizing\nthe total game return (sum DRL) and one maximizing the number of \"cooperative contributions\" -\ncontributions higher than a proportional threshold (prop DRL). Our results show that our aspiration-\nbased RL model for CC agents is consistent with empirically observed CC behavior. Furthermore,\ngames combining 3 CC RL agents and one nudging RL agent outperform the baseline consisting\nof 4 CC RL agents only. The sum DRL nudging agent increases the total sum of contributions by\n8.22% and the total proportion of cooperative contributions by 12.42%, while the prop nudging DRL\nincreases the total sum of contributions by 8.85% and the total proportion of cooperative contributions\nby 14.87%. Our findings advance the literature on public goods games and multi-agent reinforcement\nlearning with mixed incentives.\nKeywords public goods games · conditional cooperation · multi-agent reinforcement learning (MARL) · aspiration-\nbased reinforcement learning\n1\nIntroduction\nResolving intertemporal social dilemmas of collaboration is a central challenge in modern society, as social dilemmas\ncreate a trade-off between maximizing short-term individual gains and long-term collective benefits [Ostrom, 2000,\nHardin, 1968]. Intertemporal public goods games, widely studied in the fields of computational economics, biology,\nsocial science, and also machine learning, are simplified, abstracted versions of intertemporal social dilemmas of\nmulti-agent cooperation, in which players iteratively contribute individual assets (e.g., money, tokens, etc.) to a common\npool, from which each player benefits equally at the end of the multi-round game [Lang et al., 2018, Ackermann and\nMurphy, 2019]. Formal game-theoretical analysis predicts that public goods agents—natural and artificial ones—act\nindividually and fail to cooperate due to the optimization of short-term gains over long-term utility [Hardin, 1968].\nHowever, empirical studies suggest that this does not always hold true. In some cases, even though they are rare, the\ngame’s agents may actually find models to cooperate, e.g., when finding ways to sustainably use a common local fishery\narXiv:2409.09509v1  [cs.MA]  14 Sep 2024\nLearning Nudges for Conditional Cooperation\ninstead of overfishing [Jentoft et al., 2018], organizing collective food storage in preparation for a harsh winter [Hughes\net al., 2018], or succeeding in collaborative open source software development. Such studies suggest that the success of\ncollaboration in public goods games depends on the behavior of so-called conditional cooperators (CC): CC behavior\n\"depends on the subjects’ perception of future interaction\" of other group members, and such perceptions relate to\nso-called \"social norms\" [Keser and Van Winden, 2000, te Velde and Louis, 2022, Cialdini, 2007]. Social norms cast\na conforming effect on players subject to them [te Velde and Louis, 2022, Cialdini, 2007]. In public goods games,\ndescriptive social norms quickly emerge from the observed average contribution of the other group members in the\nprevious period, changing the game environment of the agents, causing positive or negative game dynamics in terms\nof cooperation [Keser and Van Winden, 2000]. Typically, human CCs, or in a more general way \"natural\" CC agents,\nwould adjust their individual aspirations to cooperate depending on the average contributions of the other players of\nthe last round, following principles of aspiration-based reinforcement learning rooted in statistical learning theories\ndeveloped in psychology and economic game theory [Mosteller, 1957, Sutton and Barto, 2020]. Emerging social norms\noften fail to create the positive game dynamics needed to increase cooperation [Ezaki et al., 2016]. However, if there\nwere a way that a social planner could indirectly modulate the emerging social norms (e.g., through its own actions), a\nmechanism also called nudging, such actions could incentivize natural CC agents to cooperate [Andı and Akesson,\n2021].\nTypically, human CCs, or in a more general way \"natural\" CC agents, would adjust their individual aspirations to\ncooperate depending on the average contributions of the other players of the last round, following principles of\naspiration-based reinforcement learning rooted in statistical learning theories developed in psychology and economic\ngame theory [Mosteller, 1957, Sutton and Barto, 2020]. Aspirational RL agents are not seeking to optimize utility but\ninstead learn based on past experiences while being guided by aspirations shaped by social norms emerging bottom-up\nfrom the contribution behavior of all agents.\nExisting work on machine learning has made significant progress in reinforcement learning (RL) algorithms that achieve\ncooperation in multi-agent reinforcement learning (MARL) among self-interested agents, who cannot be coordinated\nwith a centralized RL algorithm [Canese et al., 2021, Hughes et al., 2018]. Finding ways for self-interested agents to\nincentivize each other has shown promising results for solving the problem of cooperation among self-interested agents\nin MARL [Yang et al., 2020, Munoz de Cote et al., 2006, Hughes et al., 2018].\nThere are also recent efforts to design a social planner who leverages deep graph-based reinforcement learning to nudge\nhuman-like agents into cooperation by modulating the dyadic information exchange among agents in a social network\n[McKee et al., 2023]. However, current research has not investigated the specific problem of how to design an artificial\ndeep RL agent that acts as a social planner who seeks to optimize cooperation through its own contribution behavior to\n\"nudge\" human CCs into cooperation without formal intervention and in the absence of dyadic information exchange\namong CC agents. So the question that remains unanswered is: How can an artificial RL agent nudge natural CC\nagents into cooperative behavior to support cooperation while mitigating the risk that defection will take hold?\nTo tackle this gap, this paper studies multi-agent reinforcement learning games (MARL) for intertemporal public goods\ncombining multiple aspirational CC agents and one deep reinforcement learning agent (DRL) who seeks to optimize\ncooperation. The DRL agent learns a policy for nudging the CC agents into cooperation through its own contributions.\nWe model the learning of aspirational CC agents using aspirational reinforcement learning to emulate human CC\nbehavior. For our nudging DRL agent, we design two reward functions: one seeking to optimize the total contribution\nto the game (sum DRL), and one optimizing the percentage of cooperative contributions in each round (prop DRL).\nWe train our DRL agent using the Proximal Policy Optimization algorithm (PPO). In our experiments, we compare\nbaseline games with CC RL agents only to mixed MARL games with multiple CC RL agents and one nudging DRL\nagent. We present four major findings:\n1. Our aspiration-based RL model for CC agents is consistent with empirically observed CC behavior of humans.\n2. For our social planner, our sum DRL agent slightly outperforms the prop DRL agent during training (in terms\nof convergence, loss, and entropy).\n3. In our games with 3 CC RL agents and one nudging RL agent, the sum DRL nudging agent increases the total\nsum of contributions by 8.22% and the total proportion of cooperative contributions by 12.42% compared to\nthe baseline.\n4. In our games with 3 CC RL agents and one nudging RL agent, the prop DRL nudging agent increases the total\nsum of contributions by 8.85% and the total proportion of cooperative contributions by 14.87%, compared to\nthe baseline.\n2\nLearning Nudges for Conditional Cooperation\n2\nRelated Work\nOur research relates to the work on 1) multi-agent reinforcement learning (MARL) and cooperation and 2) aspiration-\nbased reinforcement learning in evolutionary game theory.\n2.1\nMulti-agent Reinforcement Learning and Cooperation\nAchieving cooperation in multi-agent reinforcement learning remains a difficult problem [Canese et al., 2021, Jaques\net al., 2019, Gronauer and Diepold, 2022, Zhang et al., 2021]. Prior work resorts to centralized learning strategies\nfor decentralized policies have shown to improve cooperative game outcomes [Canese et al., 2021]. However, such\nan approach is unrealistic in intertemporal social dilemma games as such games lack central coordination among\nautonomous agents. Recent MARL research has investigated new learning approaches to improve cooperation without\ncentralized learning. For example, machine learning scholars have recently drawn on the theory of social psychology\nand designed social learning algorithms, in which agents use counterfactual reasoning to assess whether their actions\nwill increase the chances of cooperation [Jaques et al., 2019]. Tampuu et al. [2017], for example, designed a deep-\nQ-network game in which two autonomous Deep Q-Learning agents learn to cooperate and compete by sharing a\nhigh-dimensional environment and being only fed with raw visual input (e.g., screen images in a video game). They\nshow that successful strategies for competition and cooperation emerge, depending on the incentives provided by the\nreward scheme. Research on MARL also specifically examines different algorithms for policy learning to tackle the\nproblem of non-stationary environments. Indeed, finding efficient algorithms for deep RL methods is challenging.\nMARL research also specifically focuses on social dilemmas: Munoz de Cote et al. [2006], for example, propose a\nQ-learning algorithm for social dilemma games, in which self-interested Q-Learning agents motivate each other using\n\"change or learn fast\" and \"change and keep\" strategies to reduce learning rates. Both strategies end up encouraging\nmore cooperation than unadjusted Q-Learning on its own. Hughes et al. [2018] design an MARL for intertemporal\nsocial dilemmas using an asynchronous advantage actor-critic (A3C) as the deep RL algorithm to study how agents\nwith inequity-averse preferences impact cooperation among the agents. Some recent work also examines specifically\nsocial network relationships and dyadic communication between agents in social dilemma games. For example, McKee\net al. [2023] designs a social planner who leverages deep graph-based reinforcement learning to break or recommend\nconnections between agents in an MARL game with social network relationships along which information is shared.\nHowever, the problem of designing a \"social planner\" leveraging deep RL to nudge cooperation among CCs using\ndescriptive social norms in the absence of dyadic information exchange has yet to be explored. This paper tackles this\ngap.\n2.2\nAspiration-based Reinforcement Learning and Economic Game Theory\nAspiration-based reinforcement learning has its roots in statistical learning theory in psychology and economics\n[Mosteller, 1957, Sutton and Barto, 2020, Cross, 1973]. Inspired by statistical learning theory in psychology, economists\nstarted to adopt Herbert Simon’s theory of \"satisficing\" [Simon, 1955, Simon et al., 1987, March, 1988] arguing that\nhumans are boundedly rational: They engage in reinforcement learning according to which humans satisfy rather than\nmaximize payoffs in the sense of economic rationality [Bendor et al., 2001]. According to this principle, humans\nmake choices based on past experiences and interactions with their decision environment that can be represented by\nprobabilistic rules: Actions that have resulted in satisfactory payoffs are more likely to be selected compared to those\nthat lead to unsatisfactory ones. To make such \"judgments\", humans are guided by aspirations that are socially defined\n(e.g., by the social norms and information available about others). Empirical studies have provided vast evidence that\nnatural agents use aspiration-based reinforcement learning guided by simple heuristics instead of utility maximization,\nin particular in environments where it is almost impossible to form a coherent model of the environment. One model that\nshaped the field of aspiration-based reinforcement learning in the field of economic game theory was the Bush-Mosteller\nmodel [Mosteller, 1957]: The Bush-Mosteller model assumes that if an agent’s action is followed by a positive payoff,\nthe probability that the action is taken increases based on simple linear probabilistic rules of learning.\nResearch on computational economics and game theory has studied aspiration-based reinforcement learning in dynamic\nmulti-agent games with the goal to facilitate cooperation. Zhang et al. [2021], for example, studied aspiration-based RL\nand the Fermi-function-based strategy adoption rule in the context of rhe Prisoner’s Dilemma and Snowdrift games.\nStahl and Haruvy [2002] examined how aspiration-based RL and reciprocity-based RL influence the dynamics in\nMARL games to empirically explain cooperation behavior. Song et al. [2022], for example, use the classical BM\nmodel in evolutionary game theory to study the effect of adaptive interaction intensity allowing direct information\nexchange between players. They find that an increased interaction intensity increases cooperation because it shapes\nthe co-evolutionary process and also the microscopic mechanisms of cooperation. Aspiration-based RL has also been\nstudied in the context of spatial public goods games [Tomassini and Antonioni, 2021]. Recently, research has also started\n3\nLearning Nudges for Conditional Cooperation\nto use aspiration-based RL to model CC behavior, and also its \"moody\" cousin. Ezaki et al. [2016], for example, used a\nrefined version of the classical RL model, the Bush-Mosteller (BM) model, to model CC behavior as aspiration-based\nRL, in which the agents’ actions are constituted by the average contribution of the players in the last round, or in other\nwords by social norms. Thus, they suggest social norms play an important role in modulating CCs’ actions. However,\nas of today, there has been little effort to investigate how a social \"planner\", modeled as a deep RL agent, seeks to\noptimize cooperation by shaping the behavior of CC agents through its contribution. This paper seeks to fill this gap.\n3\nThe Public Goods Game and the Reinforcement Learning Agents\nIn the following sections, we first introduce the public goods game. Afterwards, we specify the reinforcement learning\nmodels of our human-like CC RL agents, and the social planner, the nudging deep RL agent.\n3.1\nGame Setup\nFollowing the structure used in seminal works [Ezaki et al., 2016, Horita et al., 2017, Fischbacher et al., 2001], our\ngame involves N players (agents) participating in a total of tmax rounds. In our game, we chose N = 4 and tmax = 25.\nThree of the four players are aspirational CC agents, and one player is a nudging DRL agent. The sequential decisions\nof the players can be described as a Markov Decision Process (MDP), which is made up of a set of states S, a set of\nactions A, a transition probability function P(s, s′), and a reward function R(s, s′). The set of states S is defined as the\ncontribution at each agent can make at time period t.\nAt the onset of each round t, every player receives one token. When taking an action, they can decide to contribute\na fraction of this token to the collective pool, retaining the remainder. In a particular round t, a player i can make\na contribution ait between 0 and 1. For simplicity, we assume that the ait is continuous. After each round t, the\ncontributions in the pool are multiplied by a factor of k = 1.6, following prior public goods game models [Ezaki et al.,\n2016, Horita et al., 2017]. This amount is then evenly redistributed among all the players. Consequently, the reward rit,\nor payoff, an individual player i receives in round t is given by Equation 1:\nrit = k\nN\nN\nX\nj=1\najt + (1 −ait)\n(1)\n3.2\nThe Reinforcement Learning Agents\n3.2.1\nThe Aspirational Reinforcement Learning Agents\nWe model our human CCs as aspirational RL agents following the Bush-Mosteller (BM) model of reinforcement\nlearning [Ezaki et al., 2016, Mosteller, 1957], a probabilistic rule-based learning model (see Section 2). BM agents do\nnot seek to optimize the reward but seek to meet their aspirations using heuristics. The BM model is given in Equation\n2.\npt =\n\n\n\n\n\n\n\npt−1 + (1 −pt−1)st−1\nif at−1 ≥X and st−1 ≥0\npt−1 + pt−1st−1\nif at−1 ≥X and st−1 < 0\npt−1 −(1 −pt−1)st−1\nif at−1 < X and st−1 ≥0\npt−1 −pt−1st−1\nif at−1 < X and st−1 < 0\n(2)\nwhere pt is the expected cooperative contribution that the player makes in round t, and at−1 is the action in t −1, and\nst−1 is the stimulus that drives learning (−1 < st−1 < 1). The current action is suppressed if st−1 < 0, and reinforced\nif st−1 > 0, respectively. The stimulus is defined as:\nst−1 = tanh[β(rt−1 −A)]\n(3)\nwhere rt−1 is the reward (payoff) gained by a player in round t −1. A is the aspiration level (assumed to be fixed),\nand β is a parameter governing an agent’s sensitivity to rt−1 −A. X is a threshold used to judge whether an agent is\ncooperative or not. Once pt is computed, the actual contribution at is drawn from a Gaussian distribution with mean\nµ = pt and standard deviation σ = 0.2. The first contribution is made randomly from a uniform distribution. In this\npaper we choose A = 1.0, X = 0.4, β = 0.4, and σ = 0.2. The choices of hyperparameters follow the findings in\nEzaki et al. [2016], where the CC models were \"robustly observed if β is larger than ≈0.2, A ≤1, and 0.1 ≤X ≤0.4\".\nThe actual parameter values themselves were chosen to be similar to hyperparameters used in Ezaki et al. [2016].\n4\nLearning Nudges for Conditional Cooperation\n3.2.2\nThe Nudging Deep Reinforcement Learning (DRL) Agent\nOur social planner who seeks to nudge the CCs agents into cooperation is implemented using deep reinforcement\nlearning (DRL). Just like the aspiration-RL agent the DRL agent can take a set of actions A between 0 and 1 (for\ncomputational efficiency we discretized the actions for the DRL agents with increments of 0.01). The DRL agent\nseeks to optimize cooperation by learning a policy π(s, a) for its MDP process[Schulman et al., 2017]. We model two\ndifferent DRL agents, each seeking to optimize different reward functions. The first one seeks to optimize the total\nsum of the contributions ait of all N −1 CC agents (sum DRL). Its reward function is defined as (with the DRL agent\ndenoted with index j):\nRsum(st, a) =\nN\nX\ni=1,i̸=j\nait\n(4)\nThe second DRL agents seeks to optimize the proportion of the CC contributions ait > 0.5 (prop DRL).\nRprop(st, a) = 1\nN\nN\nX\ni=1,i̸=j\n(ait > 0.5)\n(5)\nOur DRL agents learn the corresponding standard policy π(s, a) to maximize the cumulative reward over the period of\nthe game with tmax = 25:\nEπ[s, a] =\ntmax\nX\nt=1\nRsum(st, a)\nor\nEπ[s, a] =\ntmax\nX\nt=1\nRprop(st, a)\n(6)\nWe used the PPO algorithm [Schulman et al., 2017] because it follows an actor-critic structure. Prior work suggest that\nPPO is efficient for cooperative MARL [Hughes et al., 2018, Yu et al., 2022]. The update equation for PPO is given by:\nL(θ) = ˆEt\nh\nmin\n\u0010\nrt(θ) ˆAt, clip(rt(θ), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011i\n,\nwhere rt(θ) is the probability ratio between the new and old policy, ˆAt is the advantage estimate, and ϵ is a hyperparam-\neter controlling the clipping.\n3.2.3\nThe Experiment Setup\nTo evaluate the performance of our DRL agents as a social planner, we designed a set of experiments where we compare\nbaseline games with CC agents only to games that contain a social planner that indirectly nudges the CC agents. Our\nsocial planner is a DRL agent (see Section 3.2.2)\nTable 1: Experimental Groups\nGroup\nComposition\nBaseline Games\n4 CC agents using aspiration-based RL\nNudging Games with Sum DRL\n3 CC agents, 1 sum DRL agent\nNuding Games with Prop DRL\n3 CC agents, 1 prop DRL agent\nWe trained our DRL agents for a fixed 4,000,000 steps, consisting of 1,000 batches with 4,000 steps each, within a\ngame environment that precisely mirrors their respective experimental group. During this training phase, the DRL\nagents aim to maximize their predefined reward function while playing with three CC agents. Following the training,\neach DRL agent participates in 10,000 evaluation games with three CC agents. In the baseline game, four CC agent\nengage in 10,000 evaluation games. The outcomes of these evaluation games are the primary focus of our study. The\nhyperparameter selections for the DRL agents are listed in Appendix A, Table 3.\n5\nLearning Nudges for Conditional Cooperation\nFigure 1: The relationship between three CC agents’ contributions in round t −1 and the fourth CC agents’ contribution\nin round t for CC agents in the prior study Ezaki et al. [2016] (a) and in this paper (b).\n4\nResults\n4.1\nValidation\nIn Figure 1, we examine the empirical relationship between the contributions of three CC agents in round t −1 and the\ncontribution of the fourth CC agent in round t. The charts illustrate that CC agents typically respond to high average\ngroup contributions in the prior round with high contributions of their own, and to low average group contributions with\nlow contributions. Additionally, the self-reinforcing nature of CC agents is evident: if an agent’s prior contribution was\nbelow X = 0.4, they generally follow up any group contribution with a low contribution, and with a high contribution\nif their prior contribution was above X.). The plotted lines allow us to compare our findings to previous empirical\nvalidations [Ezaki et al., 2016, Horita et al., 2017], showing consistency with prior studies using empirical data.\n4.2\nDeep Reinforcement Learning Agent Training\nFigure 2: Mean episodic reward for both the sum (a) and prop (b) DRL agents throughout the training period of\n4,000,000 games.\nFigure 2 present the training results for our deep RL agents. A step presents one game. The upward trends in Figures 2\na and 2 b indicate that each agent’s performance and efficiency in learning optimal strategies improved over time. This\nconsistent rise in rewards reflects the agents’ learned ability to adapt and optimize their decision-making processes as\ntraining progressed.\n4.3\nGame Results\nWe find that both the sum DRL and the prop DRL agent were able to increase the sums of contributions and the\nproportion of contributions over 0.5 by notable amounts compared to the baseline. We performed a Mann-Whitney\nU-test to examine the statistical difference. The results are significant (see Table 2).\nIn our games with 3 CC RL agents and one nudging RL agent, the sum DRL nudging agent increases the total sum\nof contributions by 8.22% and the total proportion of cooperative contributions by 12.42% compared to the baseline.\nIn our games with 3 CC RL agents and one nudging RL agent, the prop DRL nudging agent increases the total sum\nof contributions by 8.85% and the total proportion of cooperative contributions by 14.87%, compared to the baseline.\n6\nLearning Nudges for Conditional Cooperation\nWhile it appears that the prop DRL agent is more successful in nudging CC behavior, the difference is not statistically\nsignificant (The Mann-Whitney U-test when comparing the two distributions of game outcomes was not significant;\np = 0.139 (Prop Contribution >0.5) and p = 0.766 (Sum Contribution Mean)).\nTable 2: Game Outcomes\nMeasure\nBaseline\nSum DRL\nProp DRL\nSum Contribution (mean)\n36.995\n40.035\n40.268\nMann-Whitney U-test\n1.042 · 10−44\n1.318 · 10−58\nPercentage Change\n8.22%\n8.85%\nProp. Contribution > 0.5 (mean)\n0.491\n0.552\n0.56\nMann-Whitney U-test\n1.318 · 10−58\n1.439 · 10−64\nPercentage Change\n12.42%\n14.87%\nOverall, the results confirm that both DRL agents are successfully nudging CC. They are foster cooperation with a\nsignificant increase over the total period of the game.\nFigure 3: The mean contributions of the three other CC agents (a), the proportion of their contributions deemed\ncooperative when interacting with baseline agents and the DRL agents (b), and the average contributions of both\nbaseline and DRL agents across rounds (c).\nFigures 3a, 3b, and 3c are based on taking the mean per-round contributions across 10,000 evaluation games ran after\nthe DRL agents were finished training. Notably, the DRL agents diverge from the baseline agents’ trend early on\ndue to their substantial contributions in the initial seven rounds, which surpass those of the baseline agents. Once a\nsocial norm is established, both DRL agents stabilize their contributions at consistently higher levels than the control\nagents. This early establishment of norms by the DRL agents encourages the CC agents to assume the responsibility of\nmaintaining high contributions, thereby allowing the DRL agents to reduce their contributions while still benefiting\nfrom the established norm.\nFigure 4: The distributions of CC agents when playing with the baseline (a), the sum DRL agent (b), and the prop DRL\nagent (c) at each round represented with heat maps.\nFigures 4a and 4b show a tendency for CC agents to follow one of two paths: either the low-contribution \"doom\nloops,\" where the CC agents start contributing low and end up contributing even lower as the game progresses, or\npositive-feedback loops, where the contributions trend upward as the game progresses. The heat maps for the baseline\nagent group show a general trend towards the \"doom loop\" scenario, suggesting that the baseline (CC) agent is not very\n7\nLearning Nudges for Conditional Cooperation\neffective at preventing these situations from occurring. In contrast, the heat maps for both the prop DRL and sum DRL\nagents show a noticeable shift towards higher contributions at the later rounds, suggesting that the agents’ strategy of\ncontributing early does significantly increase the odds of creating these positive-feedback loops, as evidenced by the\nincreased brightness of the upper path across the later rounds. Of interesting note is that the CC agents’ contributions\nbegin trending upward notably around round 10, well after the DRL agents drop their contributions. This is likely the\npoint at which the positive reinforcement loop is established and the CC agents themselves have taken over upholding\nthe cooperative social norm.\nFigure 5: The difference in frequency of contributions by CC agents when playing with the sum DRL agent (a) and\nprop DRL agent (b) compared to baseline agents. These heatmaps show the change in probability of following up\ncontributions with higher or lower amounts, indicating the positive influence of both DRL agents on cooperative\nbehavior.\nFigures 5a and 5b demonstrate the increased confidence that CC agents gain when contributing higher amounts due to\nthese agents and also the lower probabilities of following low contributions with other low contributions. Taken together,\nthese charts show the positive influence both DRL agents exert upon the CC agents they play with. The detailed analysis\nof these heat maps shows that both DRL agents have a pronounced effect on encouraging high contributions when\ncompared to the baseline agent. The DRL agents’ influence is particularly notable in the increase in probability of\nCC agents following up a high contribution with another high contribution along with the decrease in probability of\nfollowing up a low contributions with another low contribution. This generally shows that the DRL agents have done a\ngood job with getting CC agents to avoid doubling down on negative trends and reinforcing positive trends.\nWhen piecing all of these charts together, we can clearly see that the strategy that both DRL agents focused on was to\nset a social norm that the CC agents felt empowered to contribute cooperatively in, and continue to nurture that over the\ncourse of the game. By this measure, both DRL agents were similarly successful, despite some variation in strategies.\nThe sum DRL agent seems to do a better job at incentivizing CC agents to follow up high contributions with more high\ncontributions (Fig. 5) However, the prop DRL agent creates a slightly more robust distribution of contributions by\nCC agents in the later round (Fig. 4), indicating that CC agents playing with the prop DRL agent tend to create the\npositive feedback loop slightly more often. Nevertheless, the differences in the two DRL agents pale in comparison\nto the differences between each agent and the baseline, with both DRL agents demonstrating a significant positive\ninfluence on the proceedings of the game after round 10 that the baseline agent was unable to replicate (Fig. 4). The\nkey reason the baseline agent could not create a trend is that it is also a CC agent, and as a result reacts to trends in\na way that DRL agents with personal incentives will not. This aligns with prior research regarding social norms and\nconditional cooperators [te Velde and Louis, 2022, Cialdini, 2007] suggesting that non-CC actors are able to establish\nnorms which CC agents will follow.\n5\nConclusion\nOur findings reveal that both DRL agents outperformed the baseline model consisting solely of CC agents. The sum\nDRL agent increased the total contributions by 8.22% and the proportion of cooperative contributions by 12.42%, while\nthe prop DRL agent increased these metrics by 8.85% and 14.87%, respectively.\nThe results indicate that the presence of a nudging agent significantly influences the behavior of CC agents, encouraging\nthem to adopt higher levels of cooperation. This extends prior research on social norms with a machine learning\nperspective [Keser and Van Winden, 2000, Cialdini, 2007]; deep RL agents acting as a \"social planner\" with an invisible\nhand can encourage cooperative behavior through social incentives.\n8\nLearning Nudges for Conditional Cooperation\nThe strategies used by both DRL agents, despite their different reward functions, suggest that the early establishment of\na cooperative norm is crucial. By contributing significantly in the initial rounds, the DRL agents set a precedent that the\nCC agents follow, thereby creating a positive feedback loop of cooperation. This finding underscores the importance of\nearly intervention in promoting collective action, a concept well-documented in the literature on social dilemmas [Keser\nand Van Winden, 2000].\nIn the machine learning space, our research builds on existing work in multi-agent reinforcement learning (MARL) and\ncooperative behavior [Jaques et al., 2019, Gronauer and Diepold, 2022]. By introducing a nudging DRL agent, we add\nto the understanding of how decentralized policies can be optimized to promote collective action. From a social science\nperspective, our study contributes to the literature on public goods games and conditional cooperation [Ezaki et al.,\n2016, Horita et al., 2017, Ledyard, 1995]. By integrating aspiration-based reinforcement learning with social norm\nnudges, we provide a novel approach to understanding and influencing cooperative behavior. Our findings support the\nnotion that strategic interventions can foster cooperation, offering practical insights for policymakers and researchers\ninterested in promoting collective action in various societal contexts [te Velde and Louis, 2022, Cialdini, 2007].\nWhile our model shows promising results, it has certain limitations. Firstly, the homogeneity of the CC agents in\nour model does not account for the diversity of behaviors observed in real-world scenarios, where free riders and\nunconditional cooperators also play a role. This research also did not explore different variants of the public goods\ngame, such as single-shot PGG. Future research should explore these dimensions to fully understand the applicability\nand limitations of AI-guided cooperation in diverse settings.\nIn conclusion, this study accentuates the profound potential of DRL models in optimizing cooperation within the public\ngoods game. It paves the way for future explorations into AI-guided social norms and their role in guiding conditionally\ncooperative actors toward collective action. Our findings serve as a pivotal reference, illustrating how AI can shape\nsocial norms and foster cooperation, thereby contributing significantly to both machine learning literature and social\nscience discourse.\nReferences\nKurt Ackermann and Ryan Murphy. Explaining Cooperative Behavior in Public Goods Games: How Preferences and\nBeliefs Affect Contribution Levels. Games, 10(1):15, March 2019. ISSN 2073-4336. doi: 10.3390/g10010015. URL\nhttps://www.mdpi.com/2073-4336/10/1/15.\nSimge Andı and Jesper Akesson.\nNudging Away False News:\nEvidence from a Social Norms Experi-\nment.\nDigital Journalism, 9(1):106–125, January 2021.\nISSN 2167-0811.\ndoi: 10.1080/21670811.2020.\n1847674.\nURL https://doi.org/10.1080/21670811.2020.1847674.\nPublisher:\nRoutledge _eprint:\nhttps://doi.org/10.1080/21670811.2020.1847674.\nJonathan Bendor, Dilip Mookherjee, and Debraj Ray. Aspiration-based reinforcement learning in repeated inter-\naction games: an overview. International Game Theory Review, 03(02n03):159–174, June 2001. ISSN 0219-\n1989. doi: 10.1142/S0219198901000348. URL https://www.worldscientific.com/doi/abs/10.1142/\nS0219198901000348. Publisher: World Scientific Publishing Co.\nLorenzo Canese, Gian Carlo Cardarilli, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Marco Re, and Sergio\nSpanò. Multi-Agent Reinforcement Learning: A Review of Challenges and Applications. Applied Sciences, 11(11):\n4948, January 2021. ISSN 2076-3417. doi: 10.3390/app11114948. URL https://www.mdpi.com/2076-3417/\n11/11/4948. Number: 11 Publisher: Multidisciplinary Digital Publishing Institute.\nRobert B. Cialdini. Descriptive Social Norms as Underappreciated Sources of Social Control. Psychometrika, 72(2):\n263–268, June 2007. ISSN 1860-0980. doi: 10.1007/s11336-006-1560-6. URL https://doi.org/10.1007/\ns11336-006-1560-6.\nJohn G. Cross. A Stochastic Learning Model of Economic Behavior*. The Quarterly Journal of Economics, 87(2):\n239–266, May 1973. ISSN 0033-5533. doi: 10.2307/1882186. URL https://doi.org/10.2307/1882186.\nTakahiro Ezaki, Yutaka Horita, Masanori Takezawa, and Naoki Masuda. Reinforcement Learning Explains Conditional\nCooperation and Its Moody Cousin. PLOS Computational Biology, 12(7):e1005034, July 2016. ISSN 1553-\n7358. doi: 10.1371/journal.pcbi.1005034. URL https://journals.plos.org/ploscompbiol/article?id=\n10.1371/journal.pcbi.1005034. Publisher: Public Library of Science.\nUrs Fischbacher, Simon Gächter, and Ernst Fehr. Are people conditionally cooperative? Evidence from a public goods\nexperiment. Economics Letters, 71(3):397–404, June 2001. ISSN 0165-1765. doi: 10.1016/S0165-1765(01)00394-9.\nURL https://www.sciencedirect.com/science/article/pii/S0165176501003949.\n9\nLearning Nudges for Conditional Cooperation\nSven Gronauer and Klaus Diepold.\nMulti-agent deep reinforcement learning: a survey.\nArtificial Intelligence\nReview, 55(2):895–943, February 2022. ISSN 1573-7462. doi: 10.1007/s10462-021-09996-w. URL https:\n//doi.org/10.1007/s10462-021-09996-w.\nGarrett Hardin. The Tragedy of the Commons. Science, 162(3859):1243–1248, December 1968. doi: 10.1126/\nscience.162.3859.1243. URL https://www.science.org/doi/10.1126/science.162.3859.1243. Publisher:\nAmerican Association for the Advancement of Science.\nYutaka Horita, Masanori Takezawa, Keigo Inukai, Toshimasa Kita, and Naoki Masuda. Reinforcement learning\naccounts for moody conditional cooperation behavior: experimental results. Scientific Reports, 7(1):39275, January\n2017. ISSN 2045-2322. doi: 10.1038/srep39275. URL https://www.nature.com/articles/srep39275.\nBandiera_abtest: a Cc_license_type: cc_by Cg_type: Nature Research Journals Number: 1 Primary_atype: Research\nPublisher: Nature Publishing Group Subject_term: Human behaviour;Social evolution Subject_term_id: human-\nbehaviour;social-evolution.\nEdward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueñez-Guzman, Antonio García Castañeda,\nIain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, Heather Roff, and Thore Graepel.\nInequity aver-\nsion improves cooperation in intertemporal social dilemmas. In Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc., 2018.\nURL https://papers.nips.cc/paper/2018/hash/\n7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html.\nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse, Joel Z. Leibo, and\nNando De Freitas. Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning. In\nProceedings of the 36th International Conference on Machine Learning, pages 3040–3049. PMLR, May 2019. URL\nhttps://proceedings.mlr.press/v97/jaques19a.html. ISSN: 2640-3498.\nSvein Jentoft, Maarten Bavinck, Enrique Alonso-Población, Anna Child, Antonio Diegues, Daniela Kalikoski, John\nKurien, Patrick McConney, Paul Onyango, Susana Siar, and Vivienne Solis Rivera. Working together in small-\nscale fisheries: harnessing collective action for poverty eradication. Maritime Studies, 17(1):1–12, April 2018.\nISSN 1872-7859, 2212-9790. doi: 10.1007/s40152-018-0094-8. URL http://link.springer.com/10.1007/\ns40152-018-0094-8.\nClaudia Keser and Frans Van Winden. Conditional Cooperation and Voluntary Contributions to Public Goods. The\nScandinavian Journal of Economics, 102(1):23–39, March 2000. ISSN 0347-0520, 1467-9442. doi: 10.1111/\n1467-9442.00182. URL https://onlinelibrary.wiley.com/doi/10.1111/1467-9442.00182.\nHannes Lang, Gregory DeAngelo, and Michelle Bongard. Explaining Public Goods Game Contributions with Rational\nAbility. Games, 9(2):36, June 2018. ISSN 2073-4336. doi: 10.3390/g9020036. URL http://www.mdpi.com/\n2073-4336/9/2/36.\nJohn O. Ledyard. Public Goods: A Survey of Experimental Research. Princeton University Press, Princeton, N.J, 1995.\nISBN 9780691042909.\nJames G. March. Variable risk preferences and adaptive aspirations. Journal of Economic Behavior & Organization, 9(1):\n5–24, January 1988. ISSN 0167-2681. doi: 10.1016/0167-2681(88)90004-2. URL https://www.sciencedirect.\ncom/science/article/pii/0167268188900042.\nKevin R. McKee, Andrea Tacchetti, Michiel A. Bakker, Jan Balaguer, Lucy Campbell-Gillingham, Richard Everett,\nand Matthew Botvinick. Scaffolding cooperation in human groups with deep reinforcement learning. Nature\nHuman Behaviour, 7(10):1787–1796, October 2023. ISSN 2397-3374. doi: 10.1038/s41562-023-01686-7. URL\nhttps://www.nature.com/articles/s41562-023-01686-7. Publisher: Nature Publishing Group.\nFrederick Mosteller. Stochastic Models for the Learning Process. Proceedings of the American Philosophical Society,\n1957. URL https://www.jstor.org/stable/985304.\nEnrique Munoz de Cote, Alessandro Lazaric, and Marcello Restelli. Learning to cooperate in multi-agent social\ndilemmas. AAMAS, pages 783–785, May 2006. doi: https://doi.org/10.1145/1160633.1160770. URL https:\n//dl.acm.org/doi/10.1145/1160633.1160770.\nDAVID Ostrom, Elinor. Conversation and Cooperation in Social Dilemmas: A Meta-Analysis of Experiments from\n1958 to 1992. Rationality and Society, 7(1):58–92, 2000. ISSN 1043-4631. doi: 10.1177/1043463195007001004.\nURL https://doi.org/10.1177/1043463195007001004.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization\nAlgorithms, August 2017. URL http://arxiv.org/abs/1707.06347. arXiv:1707.06347 [cs].\nHerbert A. Simon. A Behavioral Model of Rational Choice. The Quarterly Journal of Economics, 69(1):99–118,\nFebruary 1955. ISSN 0033-5533. doi: 10.2307/1884852. URL https://academic.oup.com/qje/article/69/\n1/99/1919737.\n10\nLearning Nudges for Conditional Cooperation\nHerbert A. Simon, George B. Dantzig, Robin Hogarth, Charles R. Plott, Howard Raiffa, Thomas C. Schelling, Kenneth A.\nShepsle, Richard Thaler, Amos Tversky, and Sidney Winter. Decision Making and Problem Solving. Interfaces, 17:\n11, October 1987. ISSN 00922102.\nZhao Song, Hao Guo, Danyang Jia, Matjaž Perc, Xuelong Li, and Zhen Wang. Reinforcement learning facilitates\nan optimal interaction intensity for cooperation. Neurocomputing, 513:104–113, November 2022. ISSN 0925-\n2312. doi: 10.1016/j.neucom.2022.09.109. URL https://www.sciencedirect.com/science/article/pii/\nS0925231222012000.\nDale O. Stahl and Ernan Haruvy.\nAspiration-Based and Reciprocity-Based Rules in Learning Dynamics for\nSymmetric Normal-Form Games. Journal of Mathematical Psychology, 46(5):531–553, October 2002. ISSN\n0022-2496. doi: 10.1006/jmps.2001.1409. URL https://www.sciencedirect.com/science/article/pii/\nS0022249601914099.\nRichard S. Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, 2 edition, 2020. ISBN\n978-0-262-19398-6. URL https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf.\nArdi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente.\nMultiagent cooperation and competition with deep reinforcement learning. PLOS ONE, 12(4):e0172395, April 2017.\nISSN 1932-6203. doi: 10.1371/journal.pone.0172395. URL https://journals.plos.org/plosone/article?\nid=10.1371/journal.pone.0172395. Publisher: Public Library of Science.\nVera L. te Velde and Winnifred Louis. Conformity to descriptive norms. Journal of Economic Behavior & Or-\nganization, 200:204–222, August 2022.\nISSN 0167-2681.\ndoi: 10.1016/j.jebo.2022.05.017.\nURL https:\n//www.sciencedirect.com/science/article/pii/S0167268122001780.\nMarco Tomassini and Alberto Antonioni. Computational behavioral models in public goods games with migration\nbetween groups. Journal of Physics: Complexity, 2(4):045013, November 2021. ISSN 2632-072X. doi: 10.1088/\n2632-072X/ac371b. URL https://dx.doi.org/10.1088/2632-072X/ac371b. Publisher: IOP Publishing.\nJiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward Hughes, and Hongyuan Zha. Learning to incentivize\nother learning agents. In Proceedings of the 34th International Conference on Neural Information Processing\nSystems, NIPS ’20, pages 15208–15219, Red Hook, NY, USA, December 2020. Curran Associates Inc. ISBN\n978-1-71382-954-6.\nChao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The Surprising\nEffectiveness of PPO in Cooperative Multi-Agent Games. Advances in Neural Information Processing Systems, 35:\n24611–24624, December 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/\n9c1535a02f0ce079433344e14d910597-Abstract-Datasets_and_Benchmarks.html.\nKaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. Multi-Agent Reinforcement Learning: A Selective Overview of\nTheories and Algorithms. In Kyriakos G. Vamvoudakis, Yan Wan, Frank L. Lewis, and Derya Cansever, editors,\nHandbook of Reinforcement Learning and Control, Studies in Systems, Decision and Control, pages 321–384.\nSpringer International Publishing, Cham, 2021. ISBN 978-3-030-60990-0. doi: 10.1007/978-3-030-60990-0_12.\nURL https://doi.org/10.1007/978-3-030-60990-0_12.\nA\nDRL Hyperparameters Used\nTable 3: Hyperparameters Used for DRL Agent Training\nParameter\nValue\nLearning Rate\n5e-5\nClipping Parameter\n0.3\nDiscount Factor (λ)\n1.0\nNumber of SGD Iterations\n30\nSGD Minibatch Size\n128\nB\nCode\nCode is available on GitHub upon request.\n11\n",
  "categories": [
    "cs.MA"
  ],
  "published": "2024-09-14",
  "updated": "2024-09-14"
}