{
  "id": "http://arxiv.org/abs/2409.13897v1",
  "title": "LLM for Everyone: Representing the Underrepresented in Large Language Models",
  "authors": [
    "Samuel Cahyawijaya"
  ],
  "abstract": "Natural language processing (NLP) has witnessed a profound impact of large\nlanguage models (LLMs) that excel in a multitude of tasks. However, the\nlimitation of LLMs in multilingual settings, particularly in underrepresented\nlanguages, remains a significant hurdle. This thesis aims to bridge the gap in\nNLP research and development by focusing on underrepresented languages. A\ncomprehensive evaluation of LLMs is conducted to assess their capabilities in\nthese languages, revealing the challenges of multilingual and multicultural\ngeneralization. Addressing the multilingual generalization gap, this thesis\nproposes data-and-compute-efficient methods to mitigate the disparity in LLM\nability in underrepresented languages, allowing better generalization on\nunderrepresented languages without the loss of task generalization ability. The\nproposed solutions cover cross-lingual continual instruction tuning,\nretrieval-based cross-lingual in-context learning, and in-context query\nalignment. Furthermore, a novel method to measure cultural values alignment\nbetween LLMs operating in different languages is proposed, ensuring cultural\nsensitivity and inclusivity. These contributions aim to enhance the\nmultilingual and multicultural alignment of LLMs in underrepresented languages,\nultimately advancing the NLP field toward greater equality and inclusiveness.",
  "text": "LLM for Everyone: Representing the\nUnderrepresented in Large Language Models\nby\nSAMUEL CAHYAWIJAYA\nA Thesis Submitted to\nThe Hong Kong University of Science and Technology\nin Partial Fulﬁllment of the Requirements for\nthe Degree of Doctor of Philosophy\nin the Department of Electronic and Computer Engineering\nAugust 2024, Hong Kong\nAuthorization\nI hereby declare that I am the sole author of the thesis.\nI authorize the Hong Kong University of Science and Technology to lend this thesis to\nother institutions or individuals for the purpose of scholarly research.\nI further authorize the Hong Kong University of Science and Technology to reproduce\nthe thesis by photocopying or by other means, in total or in part, at the request of other\ninstitutions or individuals for the purpose of scholarly research.\nSAMUEL CAHYAWIJAYA\n31 August 2024\nii\nSignature Redacted\nSignature Redacted\nLLM for Everyone: Representing the\nUnderrepresented in Large Language Models\nby\nSamuel Cahyawijaya\nThis is to certify that I have examined the above Ph.D. thesis and have found\nthat it is complete and satisfactory in all respects, and that any and all revisions\nrequired by the thesis examination committee have been made.\nProf. Pascale FUNG, Thesis Supervisor\nProf. Daniel PALOMAR, Thesis Co-Supervisor\nProf. Andrew Wing On POON, Head of Department\nThesis Examination Committee\n1. Prof. Pascale FUNG\nDepartment of Electronic and Computer Engineering\n2. Prof. Daniel PALOMAR\nDepartment of Electronic and Computer Engineering\n3. Prof. Bert Emil SHI\nDepartment of Electronic and Computer Engineering\n4. Prof. Qifeng CHEN\nDepartment of Electronic and Computer Engineering\n5. Prof. Xiaojuan MA\nDepartment of Computer Science and Engineering\n6. Prof. Dr. Hinrich Schütze\nThe Center for Information and Language Processing,\nLudwig Maximilian University of Munich\nDepartment of Electronic and Computer Engineering\nAugust 2024\niii\nHinrich SCHÜTZE\nSignature Redacted\nSignature Redacted\nSignature Redacted\nAcknowledgments\nI would never have completed this work without the help from many people. First of all,\nI thank my supervisor, Professor Pascale Fung, for her years of mentoring, advice, and\nencouragement. I have learned from her how to develop, evaluate, express, and defend\nmy ideas. These skills are important for my later in life. I also thank my co-supervisor,\nProfessor Daniel PALOMAR, for the critical way of thinking and passion about research. I\nalso thank the members of my internal and external thesis examiner committee, Professor\nBert Shi, Professor Xiaojuan Ma, and Professor Qifeng Chen, and Professor Hinrich Schutze;\nand my thesis chairperson Professor Professor Gary Shueng Han CHAN, for their insightful\ncomments on improving this work.\nSecond of all, I want to thank my wife, Holy Lovenia, and my family for their never-\nending support and encouragement throughout my PhD journey in HKUST. Studying and\nresearching at this top university wouldn’t have been possible without you all. Lastly, I\nwant to thank everyone who made my time at HKUST so vibrant and memorable. My\nfriends and colleagues, Dr. Genta Indra Winata, Andrea Madotto, Dai Wenliang, Yu\nTiezheng, Xu Yan, Lin Zhaojiang, Zihan Liu, Etsuko Ishii, Yejin Bang, Ziwei Ji Dr. Xu Peng,\nBryan Willy, Willy Ho Chun Chung, Romain Barraud, Chen Delong, Marinus Sewalt, Mac\nPasciolco, Kharis Setiasabda, Kevin Chandra, Gerry Dunda, and many others; you all made\nmy graduate study colourful inside and outside the university walls. We conquered many\nexciting projects and developed brilliant ideas together. I am forever grateful for every\nmeal, coffee break, and funny conversation we had. Without you all, my PhD journey\nwould have been a lot duller, and I am so thankful to have met such wonderful people.\niv\nTable of Contents\nTitle Page\ni\nAuthorization Page\nii\nSignature Page\niii\nAcknowledgments\niv\nTable of Contents\nv\nList of Figures\nix\nList of Tables\nxii\nAbstract\nxiv\nChapter 1\nIntroduction\n1\n1.1 Motivation and Research Problems\n1\n1.2 Thesis Outline\n3\nChapter 2\nBackground and Preliminaries\n5\n2.1 Cross-lingual Alignment\n5\n2.1.1\nClassical Cross-lingual Alignment\n5\n2.1.2\nCross-lingual Alignment in Word Embedding\n6\n2.1.3\nCross-lingual Alignment in Contextualized Embedding\n7\n2.2 Transformer and Pre-trained Language Model\n7\n2.2.1\nTransformer Model\n7\n2.2.2\nPre-trained Language Models\n10\n2.3 Large Language Models\n12\n2.3.1\nFrom Pre-trained Language Models to Large Language Models\n12\n2.3.2\nInstruction Following in Large Language Models\n12\n2.3.3\nValue Alignment in Large Language Models\n14\nv\n2.4 Related Works\n15\n2.4.1\nMultilingual Language Model\n15\n2.4.2\nMultilingual Large Language Model\n18\n2.4.3\nUnderrepresented Language Evaluation in Large Language Model\n19\nChapter 3\nLarge Language Models Evaluation in Underrepresented Languages\n22\n3.1 Introduction\n23\n3.2 Indonesian: One Country, 700+ Languages\n24\n3.2.1\nLandscape of Languages in Indonesia\n24\n3.2.2\nLanguage Diversity in Indonesia\n26\n3.3 LLMs Capability in Languages Spoken in Indonesia\n28\n3.3.1\nLanguage Under Study\n28\n3.3.2\nDataset\n29\n3.3.3\nBaseline Model\n30\n3.3.4\nEvaluation Procedure\n31\n3.4 Evaluation Results\n31\n3.4.1\nEvaluating LLM in Indonesian National Language\n31\n3.4.2\nEvaluating LLM in Local Languages Spoken in Indonesia\n33\n3.5 Analysis and Discussion\n34\n3.5.1\nDisparity Across Underrepresented Languages\n34\n3.5.2\nScaling Law in Underrepresented Languages\n36\n3.5.3\nLLM Response Quality in Underrepresented Languages\n37\n3.5.4\nCultural Evaluation in Underrepresented Languages\n40\n3.6 Conclusion\n41\nChapter 4\nMulticultural Value Alignment in Large Language Models\n42\n4.1 Introduction\n43\n4.2 Background and Preliminaries\n45\n4.3 Universal Value Representation (UniVaR)\n47\n4.3.1\nProblem Formulation\n47\n4.3.2\nValue Eliciting Question Answering\n48\n4.3.3\nMulti-view Value Embedding Learning\n49\n4.4 Experiment Design\n51\nvi\n4.4.1\nConstructing the Value Eliciting QA Training Set\n51\n4.4.2\nModel and Language Coverage\n52\n4.4.3\nTraining and Evaluation Settings\n52\n4.5 Results and Analysis\n55\n4.5.1\nEvaluation Results\n55\n4.5.2\nMap of UniVaR Representations\n57\n4.6 Conclusion\n62\nChapter 5\nUnderrepresented Languages Adaptation in Large Language Models\n63\n5.1 Introduction\n64\n5.2 Continual Cross-Lingual Instruction-Tuning\n66\n5.2.1\nOverview\n66\n5.2.2\nMethodology\n67\n5.2.3\nExperiment Setting\n69\n5.2.4\nExperiment Result\n71\n5.2.5\nAnalysis and Discussion\n74\n5.2.6\nKey Takeaways\n78\n5.3 Language Adaptation through In-Context Learning\n79\n5.3.1\nOverview\n79\n5.3.2\nMethods\n81\n5.3.3\nExperimental Settings\n85\n5.3.4\nResult and discussion\n87\n5.3.5\nKey Takeaways\n93\n5.4 Conclusion\n94\nChapter 6\nConclusion\n96\n6.1 Concluding Remarks\n96\n6.2 Limitations and Future Work\n98\nA\nHuman Annotation Guideline\n172\nB\nInstruct-Align Prompt List\n173\nC\nComparison Between LLM-int8() and Full Precision Inference\n176\nD\nInstruct-Align Datasets\n177\nE\nDetailed Experiment Results for Instruct-Align\n180\nvii\nF\nLanguage Label in Cross-lingual Alignment Experiments\n184\nG\nMonolingual Textual Similarity Experiment\n185\nH\nEffect of Machine Translation Quality to X-ICL\n186\nI\nCross-lingual In-Context Learning with BLOOM-7B1\n186\nJ\nPer Dataset Results of the Cross-Lingual In-Context Learning Experiments\n190\nK\nTranslationese Evaluation of UniVaR\n195\nL\nExtended Visualization of UniVaR Value Map\n196\nM Qualitative Analysis of UniVar\n197\nviii\nList of Figures\n2.1\nWord-level Cross-lingual Alignment\n5\n2.2\nCross-lingual Alignment in Word Embedding\n6\n2.3\nIllustration of Transformer Architecture\n8\n2.4\nScaled Dot-Product and Multi-Head Attention\n9\n2.5\nDecoder and Encoder-Decoder Transformer Architectures\n10\n2.6\nOverview of Instruction-tuning Pipeline in LLM\n13\n2.7\nOverview of Value Alignment method in LLM\n14\n3.1\nMap of Austronesian and Papuan Languages in Indonesia\n24\n3.2\nLanguage Tree of Underrepresented Languages under study\n29\n3.3\nResults on Indonesian language NLU Benchmark\n32\n3.4\nResults on Indonesian language NLU Benchmark\n33\n3.5\nResults on Indigenous Languages NLU Benchmark\n34\n3.6\nResults on Indigenous Languages NLG Benchmark\n35\n3.7\nPer Language Group Performance Breakdown on Local Indigenous Lan-\nguages\n36\n3.8\nPer Language Breakdown of Sentiment Analysis Performance\n37\n3.9\nPer Language Breakdown of Machine Translation Performance\n37\n3.10 Average Performance on Local Indigenous Languages in Indonesian\n38\n3.11 Human Rating of the Quality of Responses Generated by LLMs\n39\n3.12 Cultural evaluation of Large Language Models\n40\n4.1\nUniVaR Representations Reﬂect Distances and Similarities between Cultures 43\n4.2\nOverview of Problem Formulation and Design in UniVaR\n47\n4.3\nValue-Eliciting QA Generation Pipeline\n48\n4.4\nPerformance comparison of UniVaR between Value-Eliciting QAs and Non-\nValue-Eliciting QAs\n56\n4.5\nCultural Clusters in the Map of UniVaR Value Representation\n57\n4.6\nImpact of Translation Corpus to Cultural Relevance\n58\n4.7\nPer Dataset Visualization of UniVaR Representation\n59\n4.8\nIllustration of How UniVaR Embedding Correlate with Cultural Values\n60\nix\n4.9\nVisualization of UniVaR Representation of Phi-2 during Value Adaptation\nfrom English to Chinese LLM Values\n61\n5.1\nLinguistics Projection of World Languages\n65\n5.2\nExample of Cross-lingual Alignment through Instructions\n67\n5.3\nAverage Performance of various Instruct-Align models\n72\n5.4\nComparison of different InstructAlign Objectives\n72\n5.5\n∆Weighted F1 of InstructAlign and Number of Replay Samples (r)\n75\n5.6\nPer Language Performance of InstructAlign-tuned Models\n76\n5.7\nAlignment Quality of Instruct-Align Models\n77\n5.8\nPearson Correlation of Monolingual Semantic Similarity\n81\n5.9\nSemantic and Translation Cross-Lingual In-Context Learning\n83\n5.10 Sample Prompt for In-Context Label Alignment and Query Alignment\n85\n5.11 Performance of Different Cross-lingual In-Context Learning Methods\n88\n5.12 Semantic Cross-lingual In-Context Learning with Different Semantic Simi-\nlarity Models\n88\n5.13 Sentence Alignment Quality and Cross-Lingual In-Context Learning\n89\n5.14 Comparison of In-Context Label Alignment, Target-Only Label, and Source-\nOnly Label\n90\n5.15 ∆Weighted F1 of In-Context Label Alignment and In-Context Query Align-\nment against Non-Alignment Baseline.\n91\n5.16 Performance of XGLM-7.5B With and Without Query Alignment\n92\n5.17 Gain or Loss of Various Test-Time Adaptation Methods for Underrepre-\nsented and High-Resource Languages\n93\n5.18 Cultural understanding evaluation of in-context query alignment\n94\nA.1 Human annotation guideline in incorporated in our human evaluation.\n172\nG.2 Correlation of Monolingual Textual Similarity with Correct Labels\n186\nI.3\nBLOOM-7B1 with In-Context Label Alignment, Target-Only :abel, and\nSource-Only Label\n187\nI.4\nBLOOM-7B1 With and Without In-Context Query Alignment\n188\nI.5\nIn-context Label Alignment and In-Context Qquery Alignment against\nNon-Alignment Baseline with BLOOM-7B1\n188\nI.6\nSemantic and Translation Cross-Lingual In-Context Learning with BLOOM-\n7B1\n188\nI.7\nGain or Loss of Various Test-Time Adaptation Methods of BLOOM-7B1\n189\nx\nL.8\nGroup of languages in UniVaR value representation along with the repre-\nsentative languages within each group\n196\nL.9\nUMAP visualizations of UniVaR value embeddings.\n197\nxi\nList of Tables\n3.1\nLexical Variation of Jambi Malay\n26\n3.2\nLexical Variation of Javanese Dialects and Styles\n26\n3.3\nColloquial Indonesian code-mixing examples from social media\n27\n3.4\nWritten form Variations in several Local Languages\n28\n3.5\nDescription for all Underrepresented Languages under study\n30\n4.1\nSamples of the Generated Value Eliciting Questions\n51\n4.2\nList of LLMs Incorporated in our UniVaR Experiment\n53\n4.3\nList of All Languages covered in our UniVaR study\n54\n4.4\nValue Identiﬁcation Quality from Different Representations\n55\n5.1\nStatistics of Datasets used in Instruct-Align\n69\n5.2\nEvaluation of InstructAlign with Different Backbones\n73\n5.3\nAveraged Weighted F1-scores from various InstructAlign Objectives\n74\n5.4\nExample of in-context dictionary lookup on unseen language machine\ntranslation task across different scale of LLMs.\n84\n5.5\nDatasets and Languages used within our Cross-lingual In-Context Learning\nunder study\n86\n5.6\nList of Languages for the Cross-lingual In-Context Learning Experiments\n87\nB.1\nPrompt used for Bilingual Denoising (TLM) task\n173\nB.2\nPrompt used for Machine Translation (MT) task\n174\nB.3\nPrompt used for Crosslingual Semantic Similarity (XSS) task\n174\nB.4\nPrompt used for Monolingual Denoising (MLM) task\n175\nB.5\nPrompt used for Sentiment Analysis task\n175\nB.6\nPrompt used for Emotion Recognition task\n175\nB.7\nPrompt used for the Topic Classiﬁcation task\n175\nC.8\nComparison of Full Precision and 8-Bit Quantization\n176\nD.9 Statistics of NusaTranslation Sentiment Analysis Dataset\n177\nD.10 Statistics of NusaX Sentiment Analysis Dataset\n178\nD.11 Statistics of NusaParagraph Emotion Recognition Dataset\n178\nD.12 Statistics of NusaParagraph Topic Classiﬁcation Dataset\n179\nxii\nE.13 Sentiment Analysis Result on NusaTranslation\n180\nE.14 Sentiment Analysis Result on NusaX\n181\nE.15 Emotion Recognition Result on NusaParagraph\n182\nE.16 Topic Classiﬁcation Result on NusaParagraph\n183\nF.17 Label Set for MasakhaNews Dataset\n184\nF.18 Label Set for TweetSentiMultilingual Dataset\n184\nF.19 Label Set for NusaTranslation Dataset\n184\nF.20 Label Set for AmericasNLI Dataset\n185\nH.21 Performance of NLLB 1.3B on FLORES-200\n187\nJ.22 XGLM-7.5B Result on TweetSentiMultilingual\n190\nJ.23 XGLM-7.5B Result on MasakhaNews\n191\nJ.24 XGLM-7.5B Result on NusaTranslation\n191\nJ.25 XGLM-7.5B Result on AmericasNLI\n192\nJ.26 BLOOM-7B1 Result on TweetSentiMultilingual\n192\nJ.27 BLOOM-7B1 Result on MasakhaNews\n193\nJ.28 BLOOM-7B1 Result on NusaTranslation\n193\nJ.29 BLOOM-7B1 Result on EmricasNLI\n194\nK.30 Source Language Identiﬁcation Quality on EuroParl\n195\nM.31Samples of QAs with diverging values across different LLMs and languages.200\nM.32Samples of QAs with similar values across different LLMs and languages.\n202\nxiii\nLLM for Everyone: Representing the\nUnderrepresented in Large Language Models\nby\nSAMUEL CAHYAWIJAYA\nDepartment of Electronic and Computer Engineering\nThe Hong Kong University of Science and Technology\nABSTRACT\nNatural language processing (NLP) has witnessed a profound impact of large language\nmodels (LLMs) that excel in a multitude of tasks. However, the limitation of LLMs in\nmultilingual settings, particularly in underrepresented languages, remains a signiﬁcant\nhurdle. This thesis aims to bridge the gap in NLP research and development by focusing\non underrepresented languages. A comprehensive evaluation of LLMs is conducted\nto assess their capabilities in these languages, revealing the challenges of multilingual\nand multicultural generalization. Addressing the multilingual generalization gap, this\nthesis proposes data-and-compute-efﬁcient methods to mitigate the disparity in LLM\nability in underrepresented languages, allowing better generalization on underrepresented\nlanguages without the loss of task generalization ability. The proposed solutions cover\ncross-lingual continual instruction tuning, retrieval-based cross-lingual in-context learning,\nand in-context query alignment. Furthermore, a novel method to measure cultural values\nalignment between LLMs operating in different languages is proposed, ensuring cultural\nsensitivity and inclusivity. These contributions aim to enhance the multilingual and\nmulticultural alignment of LLMs in underrepresented languages, ultimately advancing the\nNLP ﬁeld toward greater equality and inclusiveness.\nxiv\nCHAPTER 1\nIntroduction\n1.1\nMotivation and Research Problems\nNatural Language Processing (NLP) is a burgeoning ﬁeld of research and application\nthat investigates how computers can be utilized to comprehend and manipulate natural\nlanguage for practical purposes [191, 79, 371, 198, 203]. The primary objective of NLP is to\nacquire a comprehensive understanding of how humans utilize language, thereby enabling\nthe development of appropriate tools and techniques that facilitate the comprehension and\nmanipulation of natural languages by computer systems to execute desired tasks [191, 79].\nIn its nascent stages, NLP research was primarily focused on the global lingua franca, En-\nglish, despite the existence of over 7,000 languages worldwide [108]. Other languages were\noften relegated to mere translation to English, while many others were neglected entirely.\nHowever, as NLP has advanced, it has become increasingly evident that restricting research\nto a single language is fraught with limitations, including translationese sentences [36, 134],\nsemantic ambiguity [134, 135, 257], transliteration issues [208, 409, 67, 221, 220, 252], An-\nglocentricity [228, 375, 17, 46], and monoculturalism [162, 308, 155, 196, 211, 238, 61, 214].\nOver the past decade, deep learning has brought unprecedented progress to the ﬁeld of\nnatural language processing (NLP), resulting in the development of pre-trained language\nmodels (PLMs) that exhibit remarkable performance in various NLP tasks [102, 397, 304,\n64, 57]. However, despite their impressive capabilities, existing PLMs still face a signiﬁcant\nchallenge in terms of multilingualism, as they primarily focus on learning high-resource\nlanguages such as English. Consequently, the performance of PLMs in underrepresented\nlanguages remains fairly limited, leading to a signiﬁcant disparity and inequality in access\nto state-of-the-art NLP technology. This issue highlights the urgent need to address the\ndisparity and promote equality in NLP research and development.\nIn recent years, signiﬁcant progress in Natural Language Processing (NLP) has facili-\ntated the development of multilingual large language models (LLMs), an extraordinary\n1\ntechnology that surpasses human capabilities, achieving professional-level proﬁciency\nin diverse domains such [58, 61, 272, 406, 385, 281, 16, 232, 213, 211]. The remarkable\ncapabilities of multilingual LLMs have created vast opportunities for NLP, leading to\nthe emergence of open-source and commercial multilingual LLM solutions which hold\ntremendous potential to generate a signiﬁcant impact on a global scale. However, despite\ntheir remarkable capabilities, a rigorous understanding of multilingual LLMs ability in lan-\nguages other than English is still lacking, which raises questions about their generalization\nability towards underrepresented languages, a challenge that has plagued NLP technology\nfor decades.\nBuilding upon the limited understanding of the multilingual generalization of mul-\ntilingual LLMs, this thesis presents a comprehensive evaluation that establishes a foun-\ndation for understanding the alignment capability of multilingual LLMs in underrepre-\nsented languages, speciﬁcally on Austronesian languages that are spoken in Indonesia.\nAlongside other large-scale multilingual [177, 132, 133, 320, 32, 421, 37, 5] and regional\nevaluations on underrepresented languages [7, 6, 9, 197, 219, 12, 201, 415], our thorough\nevaluations of LLMs on Austronesian languages, covering 18 underrepresented languages\nin language understanding, language generation, and cultural understanding capabilities,\nreveal the limitations of LLMs in generalizing toward multilingualism and multicultur-\nalism [397, 64, 400, 58, 60]. This underscores the urgent need for developing mitigation\nmethods to address the multilingual and multicultural generalization gap, which is critical\nfor advancing the ﬁeld of NLP.\nTo overcome this problem, we propose two approaches for improving the language\nand cultural understanding of multilingual LLMs. The ﬁrst method employs data-efﬁcient\ninstruction-tuning through cross-lingual objectives dubbed as InstructAlign. The second\nmethod is a training-free approach through in-context learning which is inspired by the\ntraditional lexicon-based [] and example-based [] machine translation approaches dubbed\nas in-context query alignment. Our approaches signify the importance of acquiring capa-\nbilities novel underrepresented languages and cultures while at the same time preventing\ncatastrophic forgetting [89] and the loss of generalization ability [414]. To this end, in this\nthesis, we formulate the following research questions and how we will approach each of\nthe research questions:\n2\n• Are Multilingual LLMs equally inclusive?\nComprehensive underrepresented languages assessment of multilingual LLMs to\nensure the inclusivity of multilingual LLMs across different level of underrepresent-\nedness.\n• Do Multilingual LLMs represent diverse cultural values?\nA robust and scalable measurement for estimating the multicultural value alignment\nin multilingual LLMs to make sure that whether multilingual LLMs represents the\ndiverse cultural values in the corresponding supported languages.\n• How to improve the inclusivity and diversity of Multilingual LLMs?\nApproaches for effectively adapt underrepresented language into existing multilin-\ngual LLMs without harming the existing multilingual and multicultural capabilities.\n1.2\nThesis Outline\nThe contents of this thesis are focused on the language and cultural inclusivity and diversity\nof multilingual LLMs. This thesis covers comprehensive evaluations of multilingual LLMs\non languages, underrepresented language adaptation methods for multilingual LLMs, and\nmulticultural value alignment in multilingual LLMs. The rest of the thesis is divided into\nfour chapters and organized as follows:\n• Chapter 2 (Preliminaries and Related Work) introduces the background and important\npreliminaries covering: 1) languages and cultures around the world, 2) transformer\nmodel and self-supervised language pre-training, 3) instruction-tuning and rein-\nforcement learning with human feedback, 4) multilingual learning and cross-lingual\nalignment, and 5) zero-shot prompting and few-shot in-context learning.\n• Chapter 3 (Large Language Models Evaluation on Underrepresented Languages)\npresents extensive evaluations on multilingual LLMs in underrepresented languages\non both language understanding and generation tasks. Additionally, we perform\nin-depth evaluations of the cultural understanding of multilingual LLMs to better\nunderstand the current state of multilingual LLMs on underrepresented language,\n3\nunderstand the effect of multilingualism on multilingual LLMs, and identify their\ndiversity across different languages.\n• Chapter 4 (Multicultural Value Alignment in Large Language Models) introduces a\nembedding-based method to understand the representation of cultures across differ-\nent languages that is obtained from value alignment process, enabling better cultural\nvalues understanding by using cultural value embedding. Using the introduced value\nembedding approach, we analyze representation of cultural values in multilingual\nLLMs across different languages, enabling us to understand the cultural diversity of\nmultilingual LLMs across different sources and languages.\n• Chapter 5 (Underrepresented Languages Adaptation in Large Language Models)\ndemonstrates cross-lingual alignment methods that enable better underrepresented\nlanguage understanding without sacriﬁcing the performance of high-resource lan-\nguages through continual cross-lingual learning and cross-lingual in-context learning.\nOur approach highlights the importance of cross-lingual alignment to improve the\ninclusivity and diversity of Multilingual multilingual LLMs\n• Chapter 6 (Conclusion) summarizes this thesis and the signiﬁcance of multilingual\nand multicultural adaptation alignment for underrepresented languages in multilin-\ngual LLMs and discusses the potential future research directions.\n4\nCHAPTER 2\nBackground and Preliminaries\nIn this chapter, we commence with a concise overview of underrepresented languages in the\nNLP ﬁeld, laying the foundation for the ensuing discussions. Subsequently, we delve into\nthe preliminary technologies pivotal to this thesis. Emphasis will be placed on cross-lingual\nalignment, transformer-based pre-trained language models (PLMs), and large language\nmodels (LLMs). In the concluding sections, we shall review related works, shedding light\non areas such as multilingualism in PLMs and LLMs, as well as underrepresented language\nevaluation in LLMs.\n2.1\nCross-lingual Alignment\nFigure 2.1: Example of the word-level cross-lingual alignment in an English-Indonesian\nparallel sentence pair.\n2.1.1\nClassical Cross-lingual Alignment\nCross-lingual alignment is ﬁrst introduced by Brown et. al. (1990) [55] along with the\nintroduction of statistical machine translation. In a classical sense, cross-lingual alignment\nconsists of two different alignment tasks, i.e., word-level alignment and sentence-level\nalignment tasks. The goal of the word-level cross-lingual alignment task is to identify\ncorrespondences between words in two parallel sentences [55, 85, 84, 123]. An example\nof the cross-lingual word alignment is shown in Figure 2.1. On the other hand, the\n5\nsentence-level cross-lingual alignment task, the goal is to retrieve correspondence pair of\nsentences across two parallel corpora [127, 122, 75]. Various works extend the sentence-\nlevel alignment to relax the strict constraint of using parallel corpora [56, 124, 119, 125, 120,\n126, 346, 344]. With these processes, we are able to induce bilingual dictionaries and phrase\ntables from parallel corpora [260, 358, 261, 121, 185]\n2.1.2\nCross-lingual Alignment in Word Embedding\nFigure 2.2: Example of cross-lingual alignment in word embedding.\nWith the introduction of word embedding methods such as word2vec [267], fast-\ntext [193], and GloVe [289], various language-speciﬁc word embeddings trained using\nlarge amount of monolingual data have been released. A number of works [266, 263] ﬁnd\nthat there are geometric similarities across different language embedding and a learnable\nlinear map is sufﬁcient to align the two embedding spaces. This process can be formulated\nas an minimization problem with the following objective:\nmin\nW\nn\nX\ni=1\nkWxi - yik\n(2.1)\nwith xi 2 Rd and yi 2 Rd denote the i-th word vector the word embedding model\nX 2 Rm⇥d and Y 2 Rm⇥d, respectively, and W 2 Rd⇥d denotes the linear transformation\nparameters. When the two embedding models are isometric (distance-preserving), this\nalignment becomes a Procrustes problem, that can be solve through a closed-form solu-\ntion [330] deﬁned as W = V.UT where U⌃V = SVD(YTX). These method enable bilingual\nlexicon induction using only monolingual data from two languages [266, 420, 321, 30].\nThis leads to the series of works in cross-lingual alignment in word embedding [29, 359,\n420, 224, 192, 223, 321, 144] which introduces similarity metrics for word embedding such\n6\nas cross-domain similarity local scaling (CSLS) [224] and relaxed cross-domain similarity\nlocal scaling (RCSLS) [192]. Despite its promise, these methods rely on the assumption of\nisomorphism between two embedding spaces, which is often violated especially when the\ntwo languages are distant [365, 288, 138]. The depiction of cross-lingual alignment in word\nembedding is shown in Figure 2.2.\n2.1.3\nCross-lingual Alignment in Contextualized Embedding\nWith the introduction of contextualized embedding models such as transformer-based\npre-trained language models, there are a number of efforts exploring the possibility of\ncontextualized embedding alignment especially in the multilingual pre-trained language\nmodels such as mBERT [102]. These methods mostly incorporate another alignment term in\nthe loss function that are heavily rely on the existence of parallel corpora [336, 66, 391, 20].\nOther line of works also analyze the cross-lingual capability of these models, and showcase\nthat these models, despite mostly trained only on monolingual data from various languages,\nit has an inherent aligned representation across different languages [354, 294, 66] and the\nalignment quality is signiﬁcantly correlated with their cross-lingual transfer capability [66,\n408, 131, 130].\n2.2\nTransformer and Pre-trained Language Model\n2.2.1\nTransformer Model\nThe Transformer [387] is a model architecture proposed for sequence modeling. Unlike,\nRNN-based models [335] such as GRU [82] and LSTM [165]), which retain only one single\nhidden state and incorporate a sequential operation to deal with long-term dependencies\nof a sequence, Transformer-based models process a sequence with a fully parallelizable\noperation based on a multi-head attention mechanism to model the long-term dependencies\nbetween input and output. This allows Transformer-based models to signiﬁcantly speed up\nboth training and inference processes showcasing their strong ability to model sequential\ndata such as natural languages [102, 304, 229, 306].\nThe illustration of the Transformer architecture is shown in Figure 2.3. The Transformer\n7\nFigure 2.3: An illustration of Transformer architecture.\nencoder and decoder are composed of a stack of Transformer layers. Each layer of the\nTransformer encoder and decoder is made up of two components: the self-attention layer\nand the feed-forward neural network, the latter of which consists of two linear layers with\nresidual connections and layer normalization [33]. In the Transformer encoder-decoder\narchitecture, an additional cross-attention layer is added between the self-attention and\nfeed-forward layers on each of the decoder layer.\nMulti-Head Attention\nThe depiction the scaled dot-product attention mechanism is\nshown in Figure 2.4. Unlike RNNs that summarize the whole natural language sequence\ninto one single hidden state, the scaled dot-product attention allows the models to maintain\nthe dimensionality of sequence length while extracting features for each token in the\nsequence. In a sequence of length L, we can obtain the hidden state Z 2 RL⇥dm, where dm\nis the dimensionality of the hidden states. The dot-product attention mechanism computes\nas follows:\n8\n0DW0XO\n6RIW0D[\n0DVN\u0003\u000bRSW\u0011\f\n6FDOH\n0DW0XO\n6FDOHG\u0003'RW\u00103URGXFW\n$WWHQWLRQ\n&RQFDW\n/LQHDU\n/LQHDU\n/LQHDU\n/LQHDU\nFigure 2.4: An illustration of the scaled dot-product attention (left) and multi-head attention\n(right). The ﬁgure is adapted from Vaswani et. al. (2024)[387].\nAttention(Q, K, V) = Softmax(QKT\np\nd\n)V,\n(2.2)\nwhere Q, K, and V are projected from the input hidden states of the Transformer layer.\nIn the scaled dot-product attention, Q represents the query vector, K represents the key\nvector, and V represents the value vector. In the self-attention layer, the entire sequence\nattends to itself, meaning all three vectors are projected from the input vector from either\nthe encoder or the decoder side. However, in the cross-attention layer, the query vector Q\nis projected from the hidden states of the decoder, while key vector K and value vector V\nare from the ﬁnal hidden states of the encoder.\nWhen the same dot-product attention function running for h times in parallel, this is\nknown as multi-head attention with h heads. Multi-head attention improves the robustness\nof the model during training resulting in an improved performance. This is done by\nallowing the model to pay attention to different input sequence features simultaneously.\nThe projection matrices are combined for different heads in practice. The projected hidden\nstates are then divided into sub-matrices and used in multi-head attention, with each\nhidden state dimension denoted as dm.\n9\n+L\n'HFRGHU\u00102QO\\\u00030RGHO\nKRZ\nDUH\n\\RX\n\"\n(26\n,\nDP\nILQH\nDQG\n\\RX\n\"\n,\nDP\nILQH\nDQG\n\\RX\n\"\n(26\n+L\nKRZ\nDUH\n\\RX\n\"\n%26\n,\nDP\nILQH\nDQG\n\\RX\n\"\n,\nDP\nILQH\nDQG\n\\RX\n\"\n(26\n(QFRGHU\n'HFRGHU\nFigure 2.5: Illustrations of (top) decoder-only and (bottom) encoder-decoder PLMs.\nMultiHead(Q, K, V) = Concat(head1, ..., headh)WO,\n(2.3)\nwhere headi = Attention(QWQ\ni , KWK\ni , VWV\ni ),\n(2.4)\nwhere WQ\ni 2 Rdm⇥d, WK\ni 2 Rdm⇥d, WV\ni 2 Rdm⇥d, and WO 2 Rhd⇥dm.\n2.2.2\nPre-trained Language Models\nPre-trained language models (PLMs), such as BERT [102] and GPT-2 [304], have achieved\ngreat success across nearly all NLP tasks. This thesis focuses on large language models\n(LLMs) which employs PLMs with decoder-only architecture for solving generative tasks in\nnatural languages. Such PLMs employ a Transformer-based architecture that can is easily\nscalable and can be pre-trained on enormous natural language corpora with self-supervised\npre-training objectives to learn the representation of the natural language residing in the\ncorpora. There are three widely-adopted architectures of PLMs, i.e., encoder-only, decoder-\nonly, and encoder-decoder. Since encoder-only PLMs, such as BERT, RoBERTa [245],\nELECTRA [87, 63], and DeBERTa [158, 157], can only be applied to classiﬁcation tasks, only\n10\ndecoder-only and encoder-decoder PLMs will be introduced further. We showcase the\ndecoder-only and encoder-decoder PLMs in Figure 2.5.\nDecoder-Only PLMs\nDecoder-only PLMs learn to take inputs and generate outputs with\na set of parameters. During pre-training, these models learn to predict successive tokens to\nmodel natural language autoregressively. In other words, given previous tokens, PLMs\nlearn to predict the next token. Given a sequence of text X = {x1, ..., xN}, decoder-only\nPLMs are pre-trained with an autoregressive causal language modeling objective:\nL(✓) = - 1\nN\nN\nX\nt=1\nlog p✓(xt|x<t),\n(2.5)\nwhere ✓denotes the parameters of the models. Decoder-only PLMs deal with inputs and\noutputs for practical use in downstream tasks by concatenating them as a single sequence.\nWe denote input and output sequences as X = {x1, ..., xM} and Y = {y1, ..., yN}, where M and\nN are lengths of the input and output sequences. As shown in Figure 2.5, a special token s\nseparates the input and output sequences – in practice, most PLMs use either the BOS or\nthe EOS tokens –, and the model recursively generates the output sequence token-by-token\ngiven the input sequence and the special token s.\nP(Y|X) =\nN\nY\nt=1\np✓(yt|x1, ..., xM, s, y1, ..., yt-1)\n(2.6)\nThe generation process stops whenever an EOS token is produced. Representative decoder-\nonly PLMs include GPT series (GPT [303], GPT-2 [304], and GPT-3 [57]), PanGu-σ [311],\nBLOOM [406], LLaMA series (LLaMA [382], LLaMA-2 [383], and LLaMA-3 [16]), etc.\nFollowing the scaling law of PLMs [199, 167, 286], these models have shown an even better\nzero-shot and few-shot in-context learning capabilities as the scale increases.\nEncoder-Decoder PLMs\nEncoder-decoder PLMs are typical Seq2Seq models that encode\ninput sequences with the encoder and predict output sequences with the decoder. The\npre-training methods of encoder-decoder PLMs vary from each other. One representa-\ntive of encoder-decoder PLMs is T5 [306, 412]. T5 is pre-trained with self-supervised\n11\nlearning through the span-level masked language modeling objective. The objective re-\nquires the model to reconstruct the masked spans from given the input while retaining\nthe overall structure of the sentence. Another commonly used encoder-decoder PLM is\nBART [229, 244], which incorporates sentence permutation and text-inﬁlling objectives for\npre-training. The sentence permutation objective requires the model to reconstruct the\npermuted sentences to the original one, while the text-inﬁlling forces the model to recover\nthe original text from the masked spans.\n2.3\nLarge Language Models\n2.3.1\nFrom Pre-trained Language Models to Large Language Models\nPLMs have shown impressive performance on various tasks. Various works [102, 304,\n397, 57, 167, 286] have displayed the positive correlation of scaling the size of PLMs to the\nlanguage understanding and generation abilities of the PLMs. In addition, the humongous\nscale of these LLMs have demonstrated emerging capability on various downstream\ntasks [62, 393]. This quality scalability leads to the rapid development of larger PLMs\nstarting from tenth-to-hundred million parameters [102, 245, 87, 158, 157] up to hundred\nbillion [187, 24, 57, 281] or even trillion parameters [360, 114] that is known as large\nlanguage model (LLM). With the extreme scale of parameters, LLMs are able to perform\ninference on an unseen data through zero-shot and few-shot prompting. This ability is\nfurther enhanced with instruction-tuning that enable LLMs to better follow instructions\neven in the zero-shot setting which will be further elaborated in §2.3.2. The ability of LLMs\nare further improved by aligning their responses to human feedback through reinforcement\nlearning with human feedback (RLHF) [80, 284]. Aside from improving response quality,\nRLHF helps to align the value adopted by LLMs that will be further described in §2.3.3\n2.3.2\nInstruction Following in Large Language Models\nInstruction following is an emergent ability [393] that LLMs have which is useful for solving\nvarious tasks in zero-shot and few-shot manner through prompting. This ability is observed\nfrom LLM with >100 billion parameters in size [57]. Instruction-tuning [323, 392, 284]\n12\n\u0017C@\u0003 KD>OPM@\u0003 <KK@<M@?\u0003 JI\u0003 OC@\u0003 R<GG\u0003 JA\u0003 <\u0003 \u0013JPI?G<I?\u0003\nNOJM@\u0003\nJI\u0003\n\u001aCTH<MF\u0003\n\u0004Q@IP@\u0003\n¡¢\n\u000bJR\u0003RJPG?\u0003TJP\u0003M@KCM<N@\u0003OC<O\u0003DI\u0003<\u0003A@R\u0003RJM?N\n\u0015@QD@R\u0003\u001a@\u0003><H@\u0003C@M@\u0003JI\u0003<\u0003\u0016<OPM?<T\u0003IDBCO\u0003<I?\u0003GP>FDGT\u0003\nDO\u0003R<NI®O\u0003<N\u0003K<>F@?\u0003<N\u0003\f\u0003OCJPBCO\u0003DO\u0003¡¢\n\u0012I\u0003<\u0003N><G@\u0003JA\u0003¼\u0003OJ\u0003À\u0003\f\u0003RJPG?\u0003BDQ@\u0003OCDN\u0003<\n\f\u0003FIJR\u0003OC<O\u0003OC@\u0003<INR@M\u0003OJ\u0003\u001aC<O\u0003O@<H\u0003?D?\u0003OC@\u0003\u0013<IOC@MN\u0003\n?@A@<O\u0003 DN\u0003 DI\u0003 \u0017C@\u0003 \u0013<IOC@MN\u0003 ADIDNC@?\u0003 OC@\u0003 M@BPG<M\u0003\nN@<NJI\u0003¡¢\u0003\u0006<I\u0003TJP\u0003O@GG\u0003H@\u0003RC<O\u0003DO\u0003DN\n\u0016PKKJN@\u0003\u0017C@\u0003=<IF@M\u0003>JIO<>O@?\u0003OC@\u0003KMJA@NNJMN\u0003<I?\u0003OC@\u0003\n<OCG@O@\u0003\u0006<I\u0003R@\u0003DIA@M\u0003OC<O\u0003\u0017C@\u0003=<IF@M\u0003>JIO<>O@?\u0003OC@\u0003\nKMJA@NNJMN\n,QVWUXFWLRQ\n\u0007J>PH@IO\u0003 \u0017C@\u0003 KD>OPM@\u0003 <KK@<M@?\u0003 JI\u0003 OC@\u0003 R<GG\u0003 JA\u0003 <\u0003\n\u0013JPI?G<I?\u0003NOJM@\u0003JI\u0003\u001aCTH<MF\u0003\u0004Q@IP@\u0003¡¢\n\u0017@SO\u0003 \u001a@\u0003 ><H@\u0003 C@M@\u0003 JI\u0003 <\u0003 \u0016<OPM?<T\u0003 IDBCO\u0003 <I?\u0003\nGP>FDGT\u0003DO\u0003R<NI®O\u0003<N\u0003K<>F@?\u0003<N\u0003\f\u0003OCJPBCO\u0003¡¢\n\u0006JIO@SO\u0003\u0017C@\u0003\u0013<IOC@MN\u0003ADIDNC@?\u0003OC@\u0003M@BPG<M\u0003N@<NJI\u0003\n¡¢\n\u0014P@NODJI\u0003\u001aC<O\u0003O@<H\u0003?D?\u0003OC@\u0003\u0013<IOC@MN\u0003?@A@<O\n\u0013M@HDN@\u0003\u0017C@\u0003=<IF@M\u0003>JIO<>O@?\u0003OC@\u0003KMJA@NNJMN\u0003<I?\u0003\nOC@\u0003<OCG@O@\n\u000bTKJOC@NDN\u0003\u0017C@\u0003=<IF@M\u0003>JIO<>O@?\u0003OC@\u0003KMJA@NNJMN\n6XPPDUL]DWLRQ\n6HQWLPHQW\u0003$QDO\\VLV\n4XHVWLRQ\u0003$QVZHULQJ\n1DWXUDO\u0003/DQJXDJH\u0003,QIHUHQFH\n\nM<AADOD\u0003<MODNO\u0003\n\u0005<IFNT\u0003DN\u0003=@GD@Q@?\u0003\nOJ\u0003=@\u0003¡¢\n¿\n\u0004MDUJI<\u0003\u0006<M?DI<GN\n\u001c@N\n//0\n0XOWL\u0010WDVN\u0003ıQH\u0010WXQLQJ\n=HUR\u0010VKRW\u0003JHQHUDOL]DWLRQ\n7DUJHW\nFigure 2.6: Overview of instruction-tuning pipeline in LLM\nenable extending this capability to smaller LLMs through multitask ﬁne-tuning using\nnatural instructions. These smaller instruction-tuned LLMs have shown remarkable zero-\nshot generalization ability to unseen tasks starting from a few billion parameters in size,\nwhile distillation can even stretch the instruction following ability to LMs with scale of\nhundred millions to a billion parameters [407].\nMore formally, given f✓as a model parameterized with ✓, while X 2 Rn and Y 2 Rm\nrespectively denote the input and the target text sequences, instruction-tuning reformulate\nthe learning process of the original ﬁne-tuning process from f✓(X) ! Y into f✓(I(X)) ! Y\nwhere I denotes a function for converting an input sequence X into a natural language\ninstruction. For example, given an English-to-Indonesian machine translation task with\nthe input X as “Hello world, good morning!”, one of the possible natural instruc-\ntion format I(X) is “Translate the sentence \"Hello world, good morning!\"\ninto Indonesian:”. In order to generalize better over different instruction formats, in\npractice, multiple instruction formats will be used to represent a single task, and zero-shot\ntask generalization emerge when scaling up this instruction-tuning process into a large\nnumber of tasks. The illustration of the instruction-tuning process is shown in Figure 2.6.\nInstruction-tuning offers improved generalization capabilities of LLMs, achieving re-\nmarkable zero-shot generalization quality on both unseen data and unseen tasks [392, 284].\nWhile instruction-following abilities are observed starting from billion parameter-range\nLLMs [379, 81], This improved generalization is showcased to outperform the standard\n13\n67(3\u0003\u0014\n+XPDQ\u0003\nGHPRQVWUDWLRQ\nGDWD\n%DVH\u0003\n//0\n6)7\u0003\n//0\n6XSHUYLVHG\nıQH\u0010WXQLQJ\n6XSHUYLVHG\u0003ıQH\u0010WXQLQJ\u0003\u000b6)7\f\n67(3\u0003\u0015\n5HLQIRUFHPHQW\u0003/HDUQLQJ\u0003:LWK\u0003+XPDQ\u0003)HHGEDFN\u0003\u000b5/+)\f\n5\u0014\u0003䘟\n5\u0015\u0003䘣\n5DWLQJ\n50\u0003IURP\u0003\nKXPDQ\u0003\nIHHGEDFN\n50\n7UDLQLQJ\n9DOXH\u0010DOLJQHG\u0003\n//0\n5/\n7UDLQLQJ\n5HLQIRUFHPHQW\u0003/HDUQLQJ\u0003:LWK\u0003$,\u0003)HHGEDFN\u0003\u000b5/$,)\f\n5\u0014\u0003䘟\n5\u0015\u0003䘣\n5DWLQJ\n50\u0003IURP\u0003\n$,\u0003\nIHHGEDFN\n50\n7UDLQLQJ\n9DOXH\u0010DOLJQHG\u0003\n//0\n5/\n7UDLQLQJ\n3UHIHUHQFH\u00037XQLQJ\n4XHVWLRQ\u0003\"\n&KRVHQ\u0003䘟\n5HMHFWHG\u0003䘣\nő\nő\nő\nő\nő\nő\n3UHIHUHQFH\u0003GDWD\n9DOXH\u0010DOLJQHG\u0003\n//0\n3UHIHUHQFH\u0003RSWLPL]DWLRQ\nFigure 2.7: Overview of value alignment method for LLMs\nﬁne-tuned counterpart on larger-scale LLMs with more than 60 billion parameters. Despite\nthe huge success, the understanding of emergent abilities in LLMs is still underdeveloped,\nsome also showcase that the emergent ability still fail to handle rare and low-resource\ntasks [60, 58, 61] and languages [415, 421], making correct and consistent elicitation of these\nabilities an open research direction.\n2.3.3\nValue Alignment in Large Language Models\nRecent LLMs such as LLaMA [382, 383, 16], ChatGPT [280] and GPT-4 [281] are pre-trained\nwith large-scale general natural language corpora that are converted to the dialogue style\nand then ﬁne-tuned through reinforcement learning with human feedback (RLHF) [80, 283].\nThese LLMs are aligned with humans to enhance their service and mitigate risks [243]. The\nmajor goal of LLMs value alignment can be divided into three fold [413], i.e., 1) Teach LLMs\nto follow human instructions [284]; 2) Align LLMs with implicit human preferences [80];\nand 3) Align LLMs to a set of pre-deﬁned principles reﬂecting human values [35]. Figure 2.7\nshowcases the overview of the LLMs value alignment that is commonly done in two phases,\ni.e., supervised ﬁne-tuning (SFT) and reinforcement learning with human or AI feedback\n14\n(RLHF/RLAIF). In SFT, the model is ﬁne-tuned by consuming a set of curated conversation\ndata complying with human desired attributes [210, 72, 273, 349]. The selection of high-\nquality, diverse data is substantial in SFT [413, 328, 210, 137, 128]. The model can be\nﬁne-tuned using a standard language modeling loss or other training paradigms such as\ncontrastive learning [13, 202] and distillation [173].\nIn the second step, RLHF [284, 34, 369] is an essential alignment technique applied by\nthe majority of recent LLMs [382, 2, 16].RLHF is achieved through reinforcement learning\nmethods such as PPO [333] where models receive feedback from a value-aligned reward\nmodel adjusting their policy. Recently, DPO [305] is introduced to alleviate the need for a\nreward model. Unlike RLHF, RLAIF generates feedback based on the model itself, reducing\nreliance on manual annotation [226, 416, 174, 240]. In RLHF, preferences are implicit as\nthey are elicited from ranking data pairs, making it difﬁcult for LLMs to generalize to\nexplicit principles. While RLHF implicitly elicit preferences from ranking data pairs, other\napproaches like Constitutional AI [35] establish explicit principles or ’constitutions’ for AI,\nenhancing model alignment to explicitly-deﬁned human values through self-critique and\nmodiﬁcation of responses.\n2.4\nRelated Works\n2.4.1\nMultilingual Language Model\nMultilingual Pre-trained Language Model\nThe development of pre-trained LMs has\ngiven rise to a new era of multilingual technology known as multilingual LMs. These\nmodels are trained on large-scale monolingual corpora in various languages, allowing\nthem to learn language representations across different linguistic contexts. Multilingual\nLMs are capable of performing cross-lingual inference without the need for any explicit\nalignment, as discussed in §2.1. This capability has signiﬁcant implications for both the\nunderstanding and generation abilities of LMs across multiple languages.\nmBERT [102, 195], a multilingual variant of BERT, can handle multiple languages\nsimultaneously, demonstrating robust cross-lingual transfer capabilities despite having no\nexplicit cross-lingual alignment. XLM-R [89] extend the monolingual data used during\npre-training while keep using masked language modeling (MLM) objective similar to\n15\nBERT while incorporating a larger pre-training corpus and more languages, achieving\nbetter performance on cross-lingual benchmarks including low-resource langyages. XLM-R\nhighlights while increasing the number of languages generally improves performance on\nlow-resource languages, it can eventually lead to the degradation of overall performance,\na phenomenon known as the curse of multilinguality. To address this issue, Goyal et. al.\n(2023) [141] demonstrates that increasing the model capacity can mitigate this degradation,\nmaintaining strong performance on both cross-lingual and high-resource language tasks.\nSimilarly, Glot500 [180] extends the language coverage of XLM-R from 100 to 500 languages\nwhile expanding the vocabulary size, thereby enhancing the inclusivity and applicability\nof multilingual LMs in diverse linguistic settings. Other line of work introduce language-\nadapter and its variants for extending the language coverage in PLMs [292, 27, 290].\nIn other line of work, various objectives for cross-lingual alignment in LMs have also\nbeen introduced. XLM [222] achieves explicit cross-lingual alignment during pretrain-\ning through translation language model (TLM) objective which leverage parallel data\nto enhance cross-lingual understanding. While other models such as LASER [31] and\nLaBSE [115] focus on sentence-level cross-lingual alignment that results in multilingual\nsentence embeddings, which enable efﬁcient cross-lingual tasks, including sentence re-\ntrieval and clustering. Another line of work [66, 218] showcase a regularization approach\nfor cross-lingual alignment through regularization between parallel samples.\nMultilingual Generative Pre-trained Language Model\nIn addition to advancements\nin encoder-only PLMs, signiﬁcant progress has been made in multilingual generative\nPLMs. XNLG [76] is a pioneering model that extends BERT and GPT architectures to\nsupport cross-lingual language generation. By leveraging cross-lingual pre-training, XNLG\nis capable of generating coherent text across multiple languages, making it suitable for\ntasks such as machine translation and cross-lingual text generation. mBART [244] is\ndesigned as a sequence-to-sequence transformer model pre-trained for multilingual text\ngeneration. It excels in machine translation and text summarization by leveraging a\ndenoising autoencoder pre-training objective. This allows mBART to generate high-quality\ntranslations and summaries across different languages, demonstrating its versatility and\neffectiveness in multilingual NLG tasks.\n16\nmT5 [412] adapts the T5 model for multilingual settings, using a text-to-text frame-\nwork that uniﬁes all tasks as text generation tasks. This approach allows mT5 to handle\na wide range of multilingual tasks with a single model, signiﬁcantly simplifying the\nprocess of multilingual NLG. The model has shown strong performance in various cross-\nlingual benchmarks, making it a powerful tool for generating text in multiple languages.\nmmT5 [291] builds upon mT5 by incorporating multimodal capabilities. This extension\nallows the model to generate and understand text that is paired with other modalities, such\nas images. By leveraging multimodal data, mmT5 enhances the generation of contextually\nrich and diverse content, pushing the boundaries of what multilingual models can achieve\nin generating and understanding complex, multi-language, and multimedia content.\nRegional-Speciﬁc Pre-trained Language Model\nThe development of regional-speciﬁc\nPLMs has become increasingly prominent, addressing the unique linguistic and cultural\nneeds of speciﬁc regions and enhancing the capabilities of language technologies for diverse\nlanguages. These models cater to languages and dialects that are often underrepresented\nin mainstream multilingual models, ensuring more equitable access to advanced language\nprocessing tools. AfroLM [18, 104] focuses on adapting language models for 23 African\nlanguages, leveraging multilingual pre-training to handle the diverse linguistic features\nof Afro-Asiatic languages. For Austronesian languages, multiple models have been intro-\nduced to address the linguistic diversity of this region. IndoNLU [397] provides pre-trained\nmodels based on BERT and a comprehensive for Indonesian NLU tasks. PhoBERT [275] is\na pre-trained language model for Vietnamese, designed to enhance language processing\ncapabilities for Vietnamese text. IndoNLG [64] extends the research in Indonesian NLP to\nNLG tasks, resulting in pre-trained LMs adapted for Indonesian languages, i.e., IndoBART\nand IndoGPT. Additionally, efforts like Cruz’s evaluation and development [96, 97] of\nLMs (e.g., Tagalog ELECTRA and Tagalog BERT) for low-resource Philippine languages\ncontribute to the advancement of NLP in the Austronesian region.\nIn the Indic language context, several models have been speciﬁcally designed to cater\nto the diverse languages spoken in the Indian subcontinent. IndicNLP Suite [197] provides\na collection of resources and pre-trained models based on ALBERT for various Indic\nlanguages, facilitating a wide range of NLP tasks. IndicNLG [219] focuses on NLG for\n17\nIndic languages, promoting the development of regional LMs based on BART and mT5 for\nsequence-to-sequence applications such as machine translation and text summarization.\n2.4.2\nMultilingual Large Language Model\nIn recent years, various multilingual LLMs have been introduced, expanding the capa-\nbilities of multilingual technology with zero-shot and few-shot generalization capability\nthrough prompting and in-context learning. Several works have further showcased the\neffectiveness of cross-lingual in-context learning [398, 404] and, even further, in-context\nalignment [376]. Several models have been developed to perform few-shot learning in a\nmultilingual context. XGLM [239] leverages extensive multilingual pre-training to excel\nin few-shot learning scenarios, demonstrating strong cross-lingual transfer capabilities.\nBLOOM [327] is a large-scale multilingual language model that supports few-shot learning\nacross 47 languages, promoting inclusivity in language technology by being able to handle\nunderrepresented languages effectively. Similarly, Falcon [24] and PolyLM [394] are LLMs\nthat excels in few-shot learning by balancing the representations across various languages,\nensuring high-quality generation and understanding across different linguistic contexts.\nTo improve the adaptability of LLMs to multilingual contexts, a few models have been\ntailored speciﬁcally for this purpose. MaLA-500 [235] focuses on adapting existing LLM to\na diverse set of languages, enhancing their cross-lingual understanding and generation\ncapabilities. This model addresses the challenge of linguistic diversity by ﬁne-tuning pre-\ntrained models on a variety of languages, thus improving their applicability in multilingual\nsettings. Furthermore, Bactrian-X [231] provides a model that adapts to low-resource\nlanguages, ensuring that even languages with limited training data are well-represented in\nmultilingual applications.\nAdditionally, other prior works have focused on improving the LLMs’ instruction-\nfollowing capability in multiple languages, leveraging their remarkable capabilities in\nzero-shot learning scenarios. BLOOMZ and mT0 [272] respectively extends the BLOOM\nand mT5 models with instruction tuning, enabling it to follow natural language instructions\nin multiple languages without requiring task-speciﬁc ﬁne-tuning. This model demonstrates\nthe potential of instruction tuning to enhance the usability of multilingual LLMs across\ndiverse tasks. Aya-101 [385] introduces a model speciﬁcally designed for zero-shot instruc-\n18\ntion following, leveraging extensive instruction tuning to perform complex multilingual\ntasks without prior task-speciﬁc training. This model has shown impressive ability on\nlow-resource and underrepresented language tasks [250].\nRegional-Speciﬁc Large Language Model\nRecently, many regional-speciﬁc LLMs have\nalso been introduced, signiﬁcantly enhancing the scope and effectiveness of language\ntechnologies for speciﬁc regions. Yi [15] is a language model designed for Chinese and\nEnglish languages, aiming to increase NLP capabilities for the Sino-Tibetan language family.\nJAIS [347] focuses on Arabic and English languages, providing robust language processing\ntools for various Arabic-speaking regions. ChatGLM citezeng2022glm,du2022glm is a\nChinese-English bilingual LM designed to handle conversational tasks, enhancing the\nquality of chatbot interactions in Chinese.\nSeaLLM [276] and SEA-LION [353] focus on Southeast Asian languages, addressing\nthe linguistic needs of this diverse and linguistically rich region. Sailor [105] is another\nmodel designed for maritime Southeast Asian languages, promoting the development\nof NLP applications for languages spoken in this area. Wangchan-Lion [293] is tailored\nfor Thai, enhancing language processing capabilities for Thai text. Cendol [61] focuses\non Indonesian and other Austronesian languages, promoting the development of NLP\napplications for languages spoken in this region. These regional-speciﬁc LLMs represent\nsigniﬁcant advancements in the ﬁeld, addressing the unique linguistic and cultural contexts\nof various regions. By focusing on the speciﬁc needs of underrepresented languages and\ndialects, these models enhance the inclusivity and applicability of language technologies,\nensuring that advanced NLP tools are accessible to a wider range of linguistic communities.\n2.4.3\nUnderrepresented Language Evaluation in Large Language Model\nCurrent LLMs perform on par or even better than state-of-the-art ﬁne-tuned models on\nEnglish [78, 81], nonetheless their capability on underrepresented languages are under\nexplored, most works in multilingual evaluation only showcase the performance in com-\nparison of other languages relative to English or to other LLMs [32, 37, 58, 385]. Recently,\nA number of recent have evaluated LLMs compared to the corresponding ﬁne-tuned\nstate-of-the-art models on the corresponding languages, nonetheless the language and task\n19\ncoverage are still limited. Adelani et. al. (2024a) [5] evaluate LLMs on 200 languages cov-\nered in NLLB [378], Cahyawijaya et. al. (2024) [61] and Lovenia et. al. (2024) [250] evaluate\nLLMs on Austronesian languages spoken in South East Asia, Adelani et. al. (2024b) [10]\nevaluate LLMs on various African languages, Adelani et. al. (2024c) [8] evaluate LLMs\non underrepresented Brazilian languages, Zhang et. al. (2024) [421] evaluate LLMs on\ncode-mixing across various languages including Spanish-English, Malayalam-English,\nTamil-English, Hinglish, and Standard-Egyptian Arabic.\nCultural Evaluation of Underrepresented Languages\nThe prevalence of Anglocentric\ntraining data in language models has raised concerns about potential cultural bias when\ngenerating texts in underrepresented languages [364, 375, 274, 155]. This bias can have\nfar-reaching consequences, creating language and cultural barriers for individuals who do\nnot speak the dominant language. Recent studies have shed light on this issue, revealing\nthat the representations learned by large language models (LLMs) often fail to reﬂect\nlocal cultural values in other languages and contexts [107, 23, 214, 238]. The disparity in\nlanguage and cultural representation poses signiﬁcant challenges for individuals from\nminority groups who do not speak the dominant language. This discrepancy creates\nlinguistic and cultural barriers to accessing technology and risks further marginalization of\nthese communities.\nFor this reason, recent studies have offered valuable insights and dug deeper to inspect\nthis problem. Various multilingual evaluations of linguistic nuances and/or cultural knowl-\nedge, such as MABL [196], M3Exam [422], and SeaEval [388], have evaluated multilingual\nmodels across a diverse set of tasks and languages and highlighted the performance gap\nbetween high-resource and low-resource languages. These evaluations also underscore\nthe importance of considering the unique characteristics of each language, the potential\npitfalls of relying solely on English-centric evaluation, as well as capturing the relevant\ncultural knowledge and cultural awareness. Further, the work by Naous et al. [274] in-\ntroduces CAMEL, a framework for measuring cross-cultural biases in LLMs. They ﬁnd\nthat models exhibit a bias toward Western entities even when operating in Arabic, leading\nto concerning cases of stereotyping and cultural unfairness. This highlights the models’\nfailure in cultural adaptation and the need for more inclusive representations. Similarly,\n20\nIndoMMLU [211] provides a multi-task language understanding benchmark, 46% of which\nfocuses on assessing proﬁciency in the Indonesian language and knowledge of nine lo-\ncal languages and cultures in Indonesia. They show that GPT-3.5’s limited Indonesian\nlanguages and cultural understanding are on par with the Indonesian primary school\nlevel, underscoring the models’ limited proﬁciency. This ﬁnding is consistent with that of\nCOPAL-ID [395], which reveals that both general multilingual models and Southeast Asian\nregional models struggle to perform well on Standard-Indonesian and Jakartan-Indonesian\nlanguage commonsense reasoning, achieving only 66.91% and 73.88% accuracy, which falls\nshort of near-perfect human performance. These studies collectively highlight that existing\nLLMs are still far from being culturally and linguistically inclusive.\n21\nCHAPTER 3\nLarge Language Models Evaluation in\nUnderrepresented Languages\nThe rapid release of open-source and commercial large language models (LLMs) including\nChatGPT [38] 1, LLaMA-2 [383], LLaMA-3 [16], Command-R 2, Aya [385, 355], etc has\nled to an increased need for a comprehensive evaluation framework to assess the quality,\nsafety and usability of LLMs. While several evaluation frameworks currently exist [177,\n132, 133, 320, 32, 421, 37, 5], they are limited in their scope, especially their evaluation\non underrepresented languages. As the number of LLMs and their coverage increases,\nso too does the necessity for robust multilingual evaluation frameworks, especially on\nunderrepresented languages, to ensure the responsible development and deployment of\nthese LLMs.\nBuilding upon the limited understanding of the underrepresented language gener-\nalization of LLMs, this chapter presents a comprehensive evaluation that establishes a\nfoundation for understanding the alignment capability of LLMs in a varying degree of\nlanguage underrepresentedness. The evaluation is focused on Austronesian languages that\nare spoken in Indonesia because of the humongous linguistic diversity in Indonesia that\ncovers more than 700 languages [17]. Alongside other large-scale regional evaluations on\nunderrepresented languages [7, 6, 9, 197, 219, 12, 201, 415], our thorough evaluations of\nLLMs on Austronesian languages reveal the limitations of LLMs in generalizing toward\nmultilingualism and multiculturalism [397, 64, 400, 58, 60] in these underrepresented lan-\nguages. This underscores the urgent need for developing mitigation methods to address\nthe multilingual and multicultural generalization gap in LLMs.\n1https://chat.openai.com/\n2https://huggingface.co/CohereForAI/c4ai-command-r-v01\n22\n3.1\nIntroduction\nLLMs have consistently pushed new frontiers in natural language processing (NLP) in\nterms of performance across a variety of benchmarks, such as MMLU [161], BIG-Bench [230]\nand HELM [51], achieving state-of-the-art results in both natural language understanding\n(NLU) and generation (NLG) tasks [38]. Various applications of LLMs have also been\nadopted in the industry bringing a signiﬁcant impact on society via technologies such\nas AI assistants, machine translations, search engines, etc. Despite its success, LLMs\nare only widely available for high-resource languages such as English and Mandarin\nChinese [418, 106, 204, 15, 382, 16], while their applicability to many languages – especially\nfor underrepresented languages – remains obscure due to the unavailability of evaluation\nsuites and benchmarks.\nIn this work, we focus on developing evaluation suites and benchmarks for languages\nin Indonesia, the second most linguistically-diverse country with 700+ languages equal to\n10% of the languages in the world [17, 108]. We compare various LLMs with pre-trained\nlanguage models (PLMs) and other baselines showcasing their limited proﬁciency in these\nlanguages. We compare the language capabilities of these LLMs in both NLU and NLG\ntasks for Indonesian (ind), the national language of Indonesia, and 17 other local languages\nspoken in Indonesia, i.e., Ambon (abs), Acehnese (ace), Mandailing (btm), Betawi (bew),\nBima (bhp), Balinese (ban), Banjarese (bjn), Buginese (bug), Javanese (jav), Madurese (mad),\nMakassarese (mak), Minangkabau (min), Musi (mui), Ngaju (nij), Rejang (rej), Sundanese\n(sun), and Toba Batak (bbc). Our results suggest that, in Indonesian (ind), existing LLMs\nperform lower to almost on par with smaller ﬁne-tuned PLMs across different tasks, and\nin some cases, slightly outperforming them. While in more underrepresented languages\nsuch as local languages spoken in Indonesia, LLMs still outcompeted by pre-trained\nlanguage models and even to classical machine learning (ML) baselines such as Logistic\nRegression [95], Naive Bayes [92], Support Vector Machine (SVM) [370] for classiﬁcation,\nwhile Bilingual Lexicon [206, 99, 331] and Phrase-Based Statistical Machine Translation\n(PBSMT) [419, 278, 45] are incorporated for machine translation.\n23\nFigure 3.1: Map of Austronesian and Papuan languages in Indonesia.\n3.2\nIndonesian: One Country, 700+ Languages\n3.2.1\nLandscape of Languages in Indonesia\nIndonesia is one of the richest countries globally in terms of linguistic diversity. More\nthan 400 of its languages belong to the Austronesian language family, while the others\nare Papuan languages spoken in the eastern part of the country. As shown in Figure 3.1,\nthe Austronesian languages in Indonesia belong to three main groups: Western-Malayo-\nPolynesian (WMP), Central-Malayo-Polynesian (CMP), and South-Halmahera-West-New-\nGuinea (SHWNG) [48]. WMP languages are Malay, Indonesian, Javanese, Sundanese,\nBalinese, and Minangkabau, among others. Languages belonging to CMP are languages of\nthe Lesser Sunda Islands from East Sumbawa (with Bimanese) onwards to the east, and\nlanguages of the central and southern Moluccas (including the Aru Islands and the Sula\nArchipelago). The SHWNG group consists of languages of Halmahera and Cenderawasih\nBay, and further-ﬂung regions such as the Mamberamo River and the Raja Ampat Islands.\n24\nMeanwhile, the Papuan languages are mainly spoken in Papua, such as Dani, Asmat,\nMaybrat, and Sentani. Some Papuan languages are also spoken in Halmahera, Timor, and\nthe Alor Archipelago [285, 319].\nMost Austronesian linguists and archaeologists agree that the original ‘homeland’\nof Austronesian languages must be sought in Taiwan and, prior to Taiwan, in coastal\nSouth China [3, 44]. In the second millennium CE, the Austronesian people moved from\nTaiwan to the Philippines. From the Philippines, they moved southward to Borneo and\nSulawesi. From Borneo, they migrated to Sumatra, the Malay Peninsula, Java, and even\nto Madagascar. From Sulawesi, they moved southward to the CMP area and eastward\nto the SHWNG area. From there, they migrated to Oceania and Polynesia, as far as New\nZealand, Easter Island, and Hawaii [145]. The people that lived in insular Southeast Asia,\nsuch as in the Philippines and Indonesia, before the arrival of Austronesians were Australo-\nMelanesians [43]. Gradual assimilation with Austronesians occurred, although some pre-\nAustronesian groups still survive, such as Melanesian people in eastern Indonesia [319, 93].\nThe Austronesian inﬂuence only affected some coastal areas of the island, for example, in\nPapua, the easternmost region in Indonesia.\nLanguage Uniﬁcation in Indonesia\nAt the time of the arrival of the ﬁrst Europeans, Malay\nhad become the major language (lingua franca) of interethnic communication in Southeast\nAsia and beyond [368, 93]. It functioned as the language of trade and the language of Islam\nbecause Muslim merchants from India and the Middle East were the ﬁrst to introduce the\nreligion into the harbor towns of Indonesia. After the arrival of Europeans, Malay was used\nby the Portuguese and Dutch to spread Catholicism and Protestantism. When the Dutch\nextended their rule over areas outside Java in the nineteenth century, the importance of\nMalay increased, and thus, the ﬁrst standardization of the spelling and grammar occurred\nin 1901, based on Classical Malay [1, 362]. In 1928, the Second National Youth Congress\nparticipants proclaimed Malay (henceforth called Indonesian) as the unifying language of\nIndonesia. During World War II, the Japanese occupying forces forbade all use of Dutch\nin favor of Indonesian, which from then onward effectively became the new national\nlanguage. From independence until the present, Indonesian has functioned as the primary\nlanguage in education, mass media, and government. Many local language speakers are\n25\nincreasingly using Indonesian with their children because they believe it will aid them to\nattain a better education and career [207].\n3.2.2\nLanguage Diversity in Indonesia\nEnglish\nMudung Laut\nDusun Teluk\nMersam\nSuo Suo\nTeluk Kuali\nLubuk Telau\nBunga Tanjung\nPulau Aro\nI/me\nsayo\naku\nawaP\nsayo\nkito, awaP\nambo\nambo\nambo\nYou\nkau, kamu\nkau\nkadn\nkamu\nkaan\nkamu\naN, kau, kayo\nbaPaN\nhe/she\ndioP\ndioP, ño\nño\nkau\nño\nño\nño\niño\nif\nkalu\njiko, kalu\nkalu\nbilao\nkalu\njiko\nkoP\nkalu\none\nsatu\nsekoP\nsekoP\nsekoP\nci3P\nsekoP\nsekoP, so\nsekoP\nTable 3.1: Lexical variation of Jambi Malay across different villages in Jambi [26].\nEnglish\nContext\nNgoko\nKrama\nWestern\nCentral\nEastern\nEastern\nI/me\nI like to eat fried rice.\ninyong, enyong\naku\naku\nkulo\nYou\nWhere will you go?\nrika, kowe, ko\nkowe, siro, sampeyan\nkoen, awakmu, sampeyan\npanjenengan\nHow\nHow do I read this?\npriwe\npiye\nyo’opo\npripun\nWhy\nWhy is this door broken?\nngapa\nngopo\nopo’o\npunapa\nWill\nWhere will you go?\narep\narep\nkate, ate\nbadhe\nNot/no\nThe calculation is not correct.\nora\nora\ngak\nmboten\nTable 3.2: Lexical variations of Javanese dialects and styles across different regions of the\nJava island. Native speakers are asked to translate the words, given the context.\nThe diversity of languages spoken in Indonesia is not only reﬂected in the large number\nof local languages but also the large number of dialects of these languages. Speakers of local\nlanguages also often mix languages in conversation, which makes colloquial Indonesian\nmore diverse. In addition, some local languages are more commonly used in conversational\ncontexts, so they do not have consistent writing forms in written media.\nDialect Variation\nIndonesian local languages often have multiple dialects, depending\non the geographical location. Local languages of Indonesian spoken in different locations\nmight be different (have some lexical variation) to one another, despite still being catego-\nrized as the same language [26, 113, 296, 200, 253, 325]. Moreover, Indonesian and its local\nlanguages have multiple styles, even within the same dialect. One factor that affects style\nis the level of politeness and formality—similar to Japanese and other Asian languages [52].\nMore polite language is used when speaking to a person with a higher social position,\nespecially to elders, seniors, and sometimes strangers. Different politeness levels manifest\n26\nColloquial Indonesian\nTranslation\nAda yang ngetag foto lawas di FB\nSomeone is tagging old photos in FB\nQuotenya Andrew Ng ini relevan banget\nThis Andrew Ng quote is very relevant\nBilo kita pergi main lagi?\nWhen will we go play again?\nIni teh aksara jawa kenapa susah banget?\nWhy is this Javanese script very difﬁcult?\nTable 3.3: Colloquial Indonesian code-mixing examples from social media. Color code:\nEnglish, Betawinese, Javanese, Minangkabau, Sundanese, Indonesian.\nin the use of different honoriﬁcs and even different lexical terms. The examples of different\ndialect variations in languages spoken in Indonesia are shown in Table 3.1 and Table 3.2.\nCode-Mixing\nCode-mixing is an occurrence where a person speaks alternately in two\nor more languages in a conversation [357, 402, 403, 103]. This phenomenon is common in\nIndonesian conversations [41, 189, 396]. In a conversational context, people sometimes mix\ntheir local languages with standard Indonesian, resulting in colloquial Indonesian [356].\nThis colloquial-style Indonesian is used daily in speech and conversation and is common\non social media [372]. Some frequently used code-mixed words (especially on social media)\nare even intelligible to people that do not speak the original local languages. Interestingly,\ncode-mixing can also occur in border areas where people are exposed to multiple languages,\ntherefore mixing them together. [154]. Furthermore, code-mixing in Indonesia not only\noccurs at the word level but also at the morpheme level [399]. The examples of code-mixing\nbetween languages spoken in Indonesia are shown in Table 3.3.\nOrthography Variation\nMany local indigenous languages spoken in Indonesia are mainly\nused in spoken settings and have no established standard orthography system. Some local\nlanguages do originally have their own archaic writing systems that derive from the\nJawi alphabet or Kawi script, and even though standard transliteration into the Roman\nalphabet exists for some (e.g., Javanese and Sundanese), they are not widely known and\npracticed [363]. Hence, some words have multiple romanized orthographies that are\nmutually intelligible, as they are pronounced the same. Some examples can be seen in\nTable 3.4. Such a variety of written forms is common in local languages in Indonesia. This\nvariation leads to a signiﬁcantly larger vocabulary size, especially for NLP systems that use\nword-based representations, and presents a challenge to constrain the representations for\n27\nLanguage\nMeaning\nWritten Variation\nIPA\nJavanese\nwhat\napa / opo\n/OpO/\n(Eastern–\nthere is\nana / ono / onok\n/OnOP/\nNgoko)\nyou\nkon / koen\n/kOn/\nBalinese\nyes\ninggih / nggih\n/PNgih/\n(Alus–\nI / me\ntiang / tyang\n/tiaN/\nSinggih)\n<greeting>\nswastyastu / swastiastu\n/swastiastu/\nSundanese\nplease / sorry\npunten / punteun\n/punt@n/\n(Badui–\nred\nbeureum / berem\n/b@r1m/\nLoma)\nsalivating\nngacai / ngacay\n/NacaI/\nTable 3.4: Written form variations in several local languages, conﬁrmed by native speakers.\ndifferent spellings of the same word to be similar. The examples of orthography variation\nin languages spoken in Indonesia are shown in Table 3.4.\n3.3\nLLMs Capability in Languages Spoken in Indonesia\n3.3.1\nLanguage Under Study\nDespite of the large language coverage, most of the languages in Indonesia are underrepre-\nsented. In this thesis, we evaluate 18 languages, one of which is the national language of\nIndonesian, i.e., Indonesian (ind). The others 17 are local indigenous languages spoken\nin Indonesia each with >500,000 speakers. Out of 17 languages, 14 come from Western\nMalayo-Polynesian (Balinese, Ngaju, Javanese, Madurese, Acehnese, Banjarese, Musi, Mi-\nnangkabau, Sundanese, Rejang Mandailing, Toba Batak, Buginese, and Makassarese), 2 are\nMalay-based creole (Betawi and Ambonese Malay), and the other is from Central Malayo-\nPolynesian (Bima). More detailed description of each language is shown in Table 3.5. It is\nimportant to note that, the number of speakers in languages under evaluation are generally\nsimilar or even higher than many higher-resource languages, e.g., Indonesian is spoken\nby 300M people similar to French and Portuguese, German is spoken by 80M speakers\nwhile there are 100M Javanese speakers, Swedish has 11M speakers while Sundanese is\nspoken by 32M speakers, Finnish is spoken by 5M speakers while Minangkabau is spoken\nby 8M, etc. However, these languages are underrepresented in the NLP community which\n28\nFigure 3.2: Language tree of all languages used within our underrepresented language\nevaluation. Some languages: Banjarese (bjn), Musi (bhp), Minangkabau (min), Betawi\n(bew), Ambonese Malay (abs) are closely-related with the national language of Indonesia,\ni.e., Indonesian (ind), all of which are under the Malay subgroup or Malay-based creole.\nis displayed by the amount of data available and the limited amount of research works in\nthese languages.\n3.3.2\nDataset\nWe assess the performance on LLMs on Indonesian (ind) and other local indigenous\nlanguages separately to measure their behaviour on different degree of underrepresented-\nness. For Indonesian language evaluation, we cover 6 tasks covering 2 NLU and 4 NLG\ntasks. For NLU, we incorporate EmoT [324] – an Indonesian emotion classiﬁcation dataset,\nSmSA [297] – an Indonesian sentiment analysis dataset. For NLG, we incorporate En!Id\nand Id!En machine translation tasks from TED [300], summarization from the extreme\nsubset of Liputan6 dataset [212], and question answering from the Indonesian subset of\n29\nISO639-3\nLanguage Name\nLanguage Family\nLanguage Group\n#Speaker\nind\nIndonesian\nAustronesian\nWMP\n300M\njav\nJavanese\nAustronesian\nWMP\n100M\nsun\nSundanese\nAustronesian\nWMP\n32M\nmin\nMinangkabau\nAustronesian\nWMP\n8M\nmad\nMadurese\nAustronesian\nWMP\n7.2M\nbjn\nBanjarese\nAustronesian\nWMP\n5.7M\nbew\nBetawi\nAustronesian\nCR\n5M\nbug\nBuginese\nAustronesian\nWMP\n4M\nace\nAcehnese\nAustronesian\nWMP\n3.4M\nban\nBalinese\nAustronesian\nWMP\n3.3M\nmak\nMakassaarese\nAustronesian\nWMP\n2.1M\nmui\nMusi\nAustronesian\nWMP\n1.6M\nbbc\nToba Batak\nAustronesian\nWMP\n1.6M\nabs\nAmbonese Malay\nAustronesian\nCR\n1.6M\nbtm\nMandailing\nAustronesian\nWMP\n1.1M\nnij\nNgaju\nAustronesian\nWMP\n890k\nbhp\nBima\nAustronesian\nCMP\n500k\nrej\nRejang\nAustronesian\nWMP\n350k\nTable 3.5: Description for all 18 languages under study. WMP denotes West Malayo-\nPolynesian, CMP denotes Central Malayo-Polynesian, and CR denotes Creole.\nTydiQA [86]. For local languages, we incorporate NLU tasks from NusaParagraph and\nNusaTranslation [60], while for NLG tasks, we incorporate XX!Id and Id!XX machine\ntranslation tasks from NusaTranslation [60] and NusaX [400].\n3.3.3\nBaseline Model\nWe incorporate 6 different LLMs with various scale in our study. Speciﬁcaly, we incorporate\nBLOOMZ [327, 272] with 7.1B parameters, LLaMA-3 [16] with 8B parameters, mT0XXL [412,\n272] and Aya-101 [385, 355] with 13B parameters, Command-R with 35B parameters, and\nGPT-3.5-Turbo [38, 279] with approximately 175B parameters. All LLMs except of LLaMA-3\nare intended for multilingual use case for languages other than English.\nWe compare these LLMs with some heuristic, statistical machine learning, and state-of-\nthe-art PLMs on the languages under study. For statistical machine learning, we emply\nLogistic Regression [95], Naive Bayes [92], Support Vector Machine (SVM) [370] for clas-\nsiﬁcation, while Bilingual Lexicon [206, 99, 331] and Phrase-Based Statistical Machine\n30\nTranslation (PBSMT) [419, 278, 45] for the machine translation task. For PLMs, we in-\ncorporate IndoBERT [397], mBERT [102], and XLM-R [89] for classiﬁcation tasks, and\nmBART [244], mT5 [412], IndoBART, and IndoGPT [64] for generation tasks.\n3.3.4\nEvaluation Procedure\nFor evaluating all LLMs except for Command-R and GPT-3.5-Turbo, we perform a zero-shot\nprompting using 3 English prompt per task. For classiﬁcation tasks, we select the prediction\nbased on the answer with the highest likelihood. For generation tasks, we employ nucleus\nsampling [172] with top-p of 0.9. We report the average performance across 3 prompts. For\nCommand-R and GPT-3.5-Turbo, we use each corresponding generation API to get the\nmodel response and extract the answer from the response. For statistical machine learning\napproaches in classiﬁcation tasks, we run grid search and take the best performance.\nEvaluation Metric\nWe employ a different evaluation metric for each task following the\nstandard evaluation metric on the corresponding task. For classiﬁcation, we report the\nMacro-F1 score, For machine translation, we report the SacreBLEU [287, 295] score. For\nsummarization, we report the ROUGEL [234] score. For QA, the F1 and exact match scores\nare reported following the original SQUAD V2 [307] evaluation metrics.\n3.4\nEvaluation Results\n3.4.1\nEvaluating LLM in Indonesian National Language\nLanguage Understanding Performance\nAs shown in Figure 3.3, the NLU performance of\nopen-source LLMs achieve ⇠45-50% F1-score, which is on par to classical ML algorithm\nwhich does not require any costly pre-training phase. this showcases the limited capa-\nbility of smaller-scale open-source LLMs on handling language understanding tasks in\nIndonesian language. Large-scale commercial LLMs and ﬁne-tuned PLMs achieve much\nhigher score of ⇠80% F1-score with the best large-scale commercial LLM, i.e., Comand-R,\noutcompete even SOTA ﬁne-tuned PLMs, i.e., XLM-RLARGE (84.89% F1-score) by achieving\na signiﬁcantly higher performance of 91.76% macro F1-score. This shows that, large-scale\n31\n\u0013\n\u0015\u0018\n\u0018\u0013\n\u001a\u0018\n\u0014\u0013\u0013\n/5\n1%\n690\n,QGR%(57\u0003EDVH\n,QGR%(57\u0003ODUJH\nP%(57\n;/0\u00105\u0003EDVH\n;/0\u00105\u0003ODUJH\n%/220=\n//D0$\u0010\u0016\nP7\u0013\u0003;;/\n$\\D\u0010\u0014\u0013\u0014\n&RPPDQG\u00105\n*37\u0010\u0016\u0011\u0018\u00107XUER\nFigure 3.3: Results on Indonesian language NLU benchmark. Smaller-scale open-source\nLLMs achieve comparable performance with classical MLs, while larger-scale commercial\nLLMs achieve similar to slightly better score to SOTA ﬁne-tuned PLMs.\ncommercial LLMs are reliable tools to be used for handling language understanding task in\nIndonesian. Nonetheless, since there is no clear information regarding the data to develop\nthese LLMs, we could reliably say that these data is really unseen by the LLMs.\nLanguage Generation Performance\nIn terms of language generation performance in\nIndonesian, as shown in Figure 3.4, open-source LLMs achieve ⇠45% average NLG score,\nwhich signiﬁcantly outperforms heuristics and SMT baselines, while slightly outperforming\nﬁne-tuned PLMNs. Larger-scale LLMs further show the superiority of LLMs yielding\nan average NLG performance of ⇠50-55%. This indicates that both open-source and\ncommercial LLMs are reliable tools to be used for handling language generation task in\nIndonesian. In conclusion, zero-shot language understanding and generation capabilities\nof existing LLMs in Indonesian are competitive to SOTA ﬁne-tuned PLMs. This makes\nLLMs a good alternative for ﬁne-tuned PLMs, as LLMs can generalize towards multiple\ntasks and the performance can be further improved through few-shot in-context learning.\n32\n\u0013\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n&RS\\\n%LOLQJXDO\u0003/H[LFRQ\n3%607\n,QGR*37\n,QGR%$57\nP7\u0018\u0003VPDOO\nP%$57\u0003ODUJH\n%/220=\n//D0$\u0010\u0016\nP7\u0013\u0003;;/\n$\\D\u0010\u0014\u0013\u0014\n&RPPDQG\u00105\n*37\u0010\u0016\u0011\u0018\u00107XUER\nFigure 3.4: Results on Indonesian language NLU benchmark. Smaller-scale open-source\nLLMs achieve comparable performance with classical MLs, while larger-scale commercial\nLLMs achieve similar to slightly better score to SOTA ﬁne-tuned PLMs.\n3.4.2\nEvaluating LLM in Local Languages Spoken in Indonesia\nLanguage Understanding Performance\nThe NLU evaluation results for Indonesian local\nindigenous languages is shown in Figure 3.5. Unlike the NLU performance in Indonesian,\nopen-source LLMs yield unsatisfactory NLU performance with ⇠40% F1-score, which is\naround 20% lower than classical ML algorithms with ⇠67% F1-score. All PLMs also fail to\noutperform classical ML approaches in indigenous languages NLU , this is because most\nof these languages, albeit related to Indonesian, are essentially unseen to all the PLMs.\nInterestingly, commercial PLMs showcase performance that are better than classical ML\nalgorithms, achieving 70.87% and 75.56% macro F1-score for GPT-3.5-Turbo and Command-\nR, respectively. Nonetheless, since there is no clear information regarding the data to\ndevelop these LLMs, we could reliably say that these data is actually unseen by LLMs.\nLanguage Generation Performance\nFigure 3.6 displays the language generation perfor-\nmance in Indonesian local indigenous languages. Both open-source and commercial LLMs\nachieve very low generation quality, yielding a performance much lower than PBSMT and\nall ﬁne-tuned PLMs. This is aligned with the ﬁnding from prior work [415, 250], where ex-\n33\n\u0013\n\u0015\u0018\n\u0018\u0013\n\u001a\u0018\n\u0014\u0013\u0013\n/5\n1%\n690\n,QGR%(57\u0003EDVH\n,QGR%(57\u0003ODUJH\nP%(57\n;/0\u00105\u0003EDVH\n;/0\u00105\u0003ODUJH\n%/220=\n//D0$\u0010\u0016\nP7\u0013\u0003;;/\n$\\D\u0010\u0014\u0013\u0014\n&RPPDQG\u00105\n*37\u0010\u0016\u0011\u0018\u00107XUER\nFigure 3.5: NLU performance on indigenous languages in Indonesia. Open-source LLMs\nare outcompeted by classical ML and ﬁne-tuned PLMs, while large-scale commercial LLMs\nslightly outperform both classical machine learning and ﬁne-tuned PLMs.\nisting LLMs have some understanding capabilities on underrepresented languages [37, 5],\nsuch as local indigenous languages in Indonesia, but fail to generate natural responses in\nthese languages. This results showcase that the existing LLMs are capable to understand\nunderrepresented languages such as local indigenous languages. Nonetheless, they are\nhave a limited capability on generating sentences on these languages.\n3.5\nAnalysis and Discussion\n3.5.1\nDisparity Across Underrepresented Languages\nWe further breakdown the local indigenous languages performance per language group.\nWe categorize the language into three different groups: low-resource, closely-related, and\nunrelated. Low-resource language group covers two indigenous languages, i.e., Javanese\nand Sundanese, which are common low-resource languages in MLLMs. Closely-related\ngroup covers ﬁve languages that are close to Indonesian (ind) as described in Figure 3.2,\ni.e., Banjarese (bjn), Musi (mui), Minangkabau (min), Betawi (bew), and Ambonese Malay\n(abs). While Unrelated group covers the others 10 local indigenous languages that are both\n34\n\u0013\n\u0014\u0013\n\u0015\u0013\n\u0016\u0013\n\u0017\u0013\n&RS\\\n%LOLQJXDO\u0003/H[LFRQ\n3%607\n,QGR*37\n,QGR%$57\nP7\u0018\u0003EDVH\nP%$57\u0010\u0018\u0013\n%/220=\n//D0$\u0010\u0016\nP7\u0013\u0003;;/\n$\\D\u0010\u0014\u0013\u0014\n&RPPDQG\u00105\n*37\u0010\u0016\u0011\u0018\u00107XUER\nFigure 3.6: NLG performance on indigenous languages in Indonesia. All LLMs are signiﬁ-\ncantly outcompeted by classical MLs and ﬁne-tuned PLMs. This showscases the inability\nof LLMs on generating sentences in underrepresented languages.\nunseen and not closely related to Indonesian (ind). As shown in Figure 3.7, the performance\non low-resource and closely-related groups are very similar from one to another, while\nthe unrelated group is performing much lower. This result shows that to improve the\nperformance of underrepresented languages, we can either: 1) proportionally cover the\nlanguage during the training of LLMs, or 2) cover more on the higher-resource language\nthat are closely-related to the underrepresented languages.\nWe further conduct a deeper analysis based on the result, speciﬁcally we analyze the\nLLM performance on the sentiment analysis and machine translation tasks from NusaX\nand NusaTranslation. In Figure 3.8, we show the average performance from all 6 open-\nsource and commercial LLMs mentioned in §3.3.3, there are disparity of performance across\ndifferent languages. The performance trend on these languages can be attributed to two\nfactors: 1) whether the language is seen during pre-training and 2) whether the language\nis closely-related to Indonesian. When a language fulﬁlls one of the criteria, i.e., Javanese\n(jav), Sundanese (sun), Minangkabau (min), Banjarese (bjn), Betawi (bew), Musi (mui) and\nAmbonese Malay (abs); the resulting performance tends to be higher in comparison to the\nother languages. Nonetheless the performance will not be as high as the high-resource\nlanguages seen during pre-training, e.g., English (eng).\n35\nFigure 3.7: Per group performance breakdown of all local language tasks. Low-Resource\ncovers Javanese and Sundanese, which are common low-resource languages in MLLMs,\nclosely-related group covers languages that are unseen but closely-related with Indonesian\n(see Fig 3.2), and unrelated group covers the other languages that are unseen and not\nclosely related to Indonesian (ind).\nThe performance trend is also consistent in machine translation tasks. As shown in\nFigure 3.9, the performance for seen or closely-related language to Indonesian (ind) tend\nto have higher performance compared to other languages. Moreover, by analyzing the\ndirection of the machine translation, English (eng) is the only language where generating\nfrom Indonesian (ind) yields a higher performance compared to generating to Indonesian\n(ind). This is because English is the only higher-resource languages in LLMs within the\nlanguages under evaluation. For other languages, generating to Indonesian (ind) yields\nconsistently higher performance than generating from Indonesian (ind) which aligns with\nprior works [415, 250] that concludes existing LLMs still facing difﬁculty to generate natural\nsentences on low-resource languages under Austronesian language family.\n3.5.2\nScaling Law in Underrepresented Languages\nWe showcase the average performance on sentiment analysis and machine translation\ntasks for only local indigenous languages in Indonesia in Figure 3.10. BLOOMZ models\ndo not show a clear pattern of scaling law on the languages under evaluation, probably\n36\nFigure 3.8: Per language breakdown of sentiment analysis performance from (top) NusaX\nand (bottom) NusaTranslation.\nlang denotes high-resource language in LLMs, lang\ndenotes the low-resource language group, and lang denotes the closely-related language\ngroup, while the others are the unrelated language group.\nFigure 3.9: Per language breakdown of machine translation performance from (top) NusaX\nand (bottom) NusaTranslation.\nlang denotes high-resource language in LLMs, lang\ndenotes the low-resource language group, and lang denotes the closely-related language\ngroup, while the others are the unrelated language group.\nbecause all of these languages are unseen to BLOOMZ. While for mT0 models, the impact\nof scaling law is more apparent, displaying a signiﬁcant performance improvement as the\nmodel size increases. Furthermore, the scaling law also observed between different model\ntype, where Aya-101, Command-R, and GPT-3.5-Turbo show an increasing trend in the\nsame order as the actual model size. This results indicate that scaling law is still observed\nfor underrepresented languages. Nonetheless, the scaling law might not hold when the\nlanguages under evaluation are strictly unseen.\n3.5.3\nLLM Response Quality in Underrepresented Languages\nWe assess the quality of generated sentence in underrepresented languages by conducting\na human evaluation of the generated sentence from LLMs. We hire native speakers on 3\nlocal indigenous languages, i.e., Javanese (jav), Sundanese (sun), and Minangkabau (min),\n37\n0DFUR\u0003)\u0014\u00106FRUH\n\u0016\u0013\n\u0018\u0013\n\u001a\u0013\n\u001c\u0013\n%/220=\u0003\u0018\u0019\u00130\n%/220=\u0003\u0014\u0011\u0014%\n%/220=\u0003\u0014\u0011\u001a%\n%/220=\u0003\u0016%\n%/220=\u0003\u001a\u0011\u0014%\nP7\u0013\u00036PDOO\nP7\u0013\u0003%DVH\nP7\u0013\u0003/DUJH\nP7\u0013\u0003;/\nP7\u0013\u0003;;/\n$\\D\u0010\u0014\u0013\u0014\n&RPPDQG\u00105\n*37\u0010\u0016\u0011\u0018\u00107XUER\n6DFUH%/(8\n\u0013\n\u0018\n\u0014\u0013\n\u0014\u0018\n\u0015\u0013\n%/220=\u0003\u0018\u0019\u00130\n%/220=\u0003\u0014\u0011\u0014%\n%/220=\u0003\u0014\u0011\u001a%\n%/220=\u0003\u0016%\n%/220=\u0003\u001a\u0011\u0014%\nP7\u0013\u00036PDOO\nP7\u0013\u0003%DVH\nP7\u0013\u0003/DUJH\nP7\u0013\u0003;/\nP7\u0013\u0003;;/\n$\\D\u0010\u0014\u0013\u0014\n&RPPDQG\u00105\n*37\u0010\u0016\u0011\u0018\u00107XUER\nFigure 3.10: Average performance on local indigenous languages in Indonesian for (left)\nsentiment analysis and (right) machine translation tasks. The scaling law is apparent\nwithin the same model type on mT0, but not for BLOOMZ. While across different model\ntypes, scaling is still clearly observed, with the largest LLM, i.e., GPT-3.5-Turbo, yield the\nbest average performance on these languages.\ndue to the difﬁculty of ﬁnding the annotators on the other languages. We incorporate\n50 generated sentences from all six LLMs under study using the data from the machine\ntranslation task. We compare the sentence generation quality with the gold translation\nlabel of the corresponding task. We ask the annotators to rate the sentence quality with a\nletter A, B, C, or D following the guideline from prior works [407, 231]. The rating of the\nhuman evaluation guideline in Appendix A. As shown in Figure 3.11, commercial LLMs\nsuch as GPT-3.5-Turbo and Command-R generate high quality responses in Indonesian,\neven outperforming the quality of the gold translation generated by human. While smaller\nopen-source LLMs display strong performance in Indonesian – especially for Llama-3 (8B)\nmodel — although there is still some room for improvement which can be solved through\nscaling in terms of the number of parameters along with improvement in the data quality\nand quantity.\nWhile for local indigenous languages, LLMs fail to yield a good rating that is showcased\nby the large amount of “C” and “D” rated responses, which is completely distinct to the\nquality rating of the gold response. Speciﬁcally, in Javanese, GPT-3.5-Turbo, Command-R,\nand Aya-101 yield almost similar score while other LLMs fail to generate a satisfactory\nresult with almost all responses are rated with “D”. For Sundanese, GPT-3.5-Turbo and\nAya-101 yield the best rating across all LLMs, with ⇠20% cumulative of “A” and “B”, and\n⇠60% of “C”. Command-R and Llama-3 (8B) show similar performance with ⇠40-50% of\n38\nFigure 3.11: Human rating of the quality of responses generated by LLMs for (top left)\nIndonesian (ind), (top right) Javanese (jav), (bottom left) Sundanese (sun), and (bottom\nright) Minnangkabau (min). The rating is letter-graded with “A” denotes highest quality\nand “D” denotes lowest quality.\nthe responses are rated “C” while the rest are “D”, while mT0XXL and BLOOMZ (7.1B)\nyield almost all “D” rated responses despite all of them are trained on multilingual data\nand have larger language coverage than Command-R and Llama-3 (8B). This is potentially\ncaused by a better quality pre-training and instruction-tuning data which are done more\nrigorously in recent LLMs [383, 16, 355, 385, 279, 24] such as LLaMA-3 and Command-R.\nFor Minangkabau, all LLMs perform poorly, with only GPT-3.5-Turbo and Aya-101 have\n⇠30% responses that are not “D” rated, while Command-R and LLaMA-3 have only ⇠10%\nresponses that are not “D” rated, while all responses of mT0XXL and BLOOMZ are “D”\nrated. This result also correlates with the number of speakers on each language where\nthe number of speakers in Minangkabau < Sundanese < Javanese < Indonesian. Our\nresult indicates that LLMs are still perform very poorly on underrepresented languages,\nespecially for languages that are more secluded and having less number of speakers. We\nestimate that the performance of LLMs will be even worse for more underrepresented\nlanguages with even smaller number of speakers.\n39\nFigure 3.12: Cultural evaluation of LLMs compared to ﬁne-tuned PLMs. Both PLMs and\nLLMs have some understanding on cultural values in Indonesian (ind) language, but still\nstruggle with understanding cultural values in local indigenous languages. ⇤The score is\nevaluated in cross-lingual manner since training data is only available in English.\n3.5.4\nCultural Evaluation in Underrepresented Languages\nLLMs not only have to understand the languages but also capture the understanding of\nlocal culture and nuances. Misrepresentation of culture in LLMs is dangerous as it might\nlead to offensive behaviors, e.g., cultural appropriation and stereotyping [112, 139]. To\nevaluate the cultural representation of LLMs on languages under study, we evaluate the\ncapability of LLMs on COPAL-ID, a dataset for Indonesian local-nuanced commonsense\nreasoning. In COPAL-ID, a scenario is provided and two options are given, one of which\nis more plausible. All scenarios in COPAL-ID are infused with Indonesian local nuances\nand context. To further extend the evaluation to local indigenous languages in Indonesia,\nwe also utilize the Indonesian, Javanese, and Sundanese subsets of MABL [196], a binary\nclassiﬁcation dataset where the LLM is required to interpret the meaning of a ﬁgure of\nspeech in a sentence. For PLMs, we only incorporate multilingual PLMs since the training\ndata in MABL is only available in English language.\nThe evaluation results are presented in Figure 3.12. On local-nuanced commonsense\nreasoning in COPAL-ID, open-source LLMs yield similar performance to ﬁne-tuned PLMs,\nwhile larger-scale commercial LLMs perform better with Command-R achieves the highest\nscore with 72.81%. On the Indonesian ﬁgure of speech understanding task from MABL,\nAll open-source LLMs are signiﬁcantly outcompeted by the best ﬁne-tuned PLMs, i.e.,\n40\nXLM-RLARGE, with >12% accuracy different. Larger-scale commercial LLMs, on the other\nhand, reduce this gap to ⇠6% accuracy score. While for the local indigenous languages\nﬁgure of speech tasks, both PLMs and LLMs perform poorly with XLM-RLARGE achieving\nthe best accuracy of ⇠60%. While all LLMs yield around 52-58% accuracy score which\nis close to the random prediction accuracy (50%). This indicates that, despite having the\nability of understanding local indigenous languages as discussed in §3.4.2, existing LLMs\nhave very limited cultural understanding in local indigenous languages and further actions\nto enhance a better cultural representation for these languages in LLMs is required.\n3.6\nConclusion\nThis chapter showcases a comprehensive evaluation of multilingual LLMs in 18 languages\nspoken in Indonesia, the second richest country in terms of linguistic diversity. The\nevaluation covers the national language of Indonesia, i.e., Indonesian (ind), and 17 local\nindigenous languages spoken in Indonesia. The evaluation showcases the promising results\nof existing multilingual LLMs as a solution for language understanding and generation in\nIndonesian (ind), the national language of Indonesia. Nonetheless, both open-source\nand commercial multilingual LLMs are still struggling on handling local indigenous\nlanguages, especially on generating sentences in these languages. Furthermore, despite\nhaving some capability on understanding sentences in these local indigenous languages,\nmultilingual LLMs are still struggle to capture the correct representation of culture in these\nlocal indigenous languages. Given the rich nature of linguistic diversity along with all the\nvariations and language mixing phenomena, the potential for LLMs to understand and\ngenerate content in local indigenous languages appears to be a crucial fascinating challenge\nfor the applicability of multilingual LLMs in Indonesia. In summary, this chapter highlight\nthe inclusivity problem of existing multilingual LLMs showcasing the limitation especially\nin terms of generating natural and culturally-relevant responses in underrepresented\nlanguages. Our work contributes to the growing understanding of multilingual LLMs in\nmultilingual societies, conﬁrming the potential of these technologies while simultaneously\nrevealing the necessary steps toward their improvement, making them a more inclusive\nand culturally sensitive tool for the beneﬁt of all speakers.\n41\nCHAPTER 4\nMulticultural Value Alignment in Large Language\nModels\nThe widespread application of Large Language Models (LLMs) across various tasks and\nﬁelds has necessitated the alignment of these models with human values and preferences.\nGiven various approaches of human value alignment, ranging from Reinforcement Learn-\ning with Human Feedback (RLHF) [80, 283], to constitutional learning [35], etc. there is\nan urgent need to understand the scope and nature of human values injected into these\nmodels before their release. Nonetheless, existing works on value alignment often only\nfocus on English language as the global lingua franca. This hinders our understanding of\nthe impact and potential of existing human value alignment to other languages. Unlike the\nexisting limitation in term of linguistic understanding in LLMs, in the cultural sense, any\nlanguage other than English can be considered as underrepresented as depicted in prior\nworks that LLMs are mostly monocultural [23] and anglocentric [107, 274, 155] despite\nhaving the multilingual generalization capability on the downstream tasks.\nIn this chapter, we propose UniVar, a high-dimensional representation, alike word\nand sentence embedding models [289, 193, 49, 194, 309, 115], for capturing human value\ndistributions in LLMs. Trained from the value-relevant information of eight multilingual\nLLMs and tested on the various open-source and commercial LLMs, we show that UniVar\nis a powerful tool to compare the distribution of human values embedded in different\nLLMs with different language sources. Through UniVar, we explore how different LLMs\nprioritize various values in different languages and cultures, shedding light on the complex\ninterplay between human values and language modeling. Furthermore, we demonstrate\nthat UniVar is able to be beneﬁcial for automatically measures the degree of multicultural\nvalue alignment in LLMs which is a crucial evaluation bottleneck that limits existing works\nin the value alignment.\n42\n4.1\nIntroduction\nFigure 4.1: UniVaR representations reﬂect distances and similarities between different\ncultures in terms of human values, across 15 LLMs and 25 languages.\nThe remarkable capabilities of Large Language Models (LLMs) have revolutionized\ngeneral-purpose AI leading to their widespread adoption in many ﬁelds [50, 410, 249, 83,\n38, 301, 59]. This newfound power comes with the responsibility of ensuring that these\nAI assistants align with human values. Numerous efforts have been made to imbue AI\nsystems with ethical principles and moral values, from designing robust frameworks for\nvalue alignment [284, 34, 35] to incorporating diverse perspectives into training data [413,\n328, 210, 137, 128]. The ability to adhere to ethical and societal values has become a critical\nfactor in developing LLMs as important as the quality and generalization on performing\ntasks effectively [107, 61, 424]. One of the most important methods to align LLMs with\nhuman values is Reinforcement Learning with Human Feedback (RLHF) [284] where a\nreward model is trained using human feedback, which is then employed as a reward\n43\nfunction to reﬁne policies via reinforcement learning (RL) to inject human preferences into\nLLMs. Another innovation, known as RLAIF [226], replaces the human annotators in RLHF\nwith an AI model. While Constitutional AI [35] uses a set of predeﬁned human-curated\nprinciples to align the LLMs explicitly. These methods ensure that LLMs are fairer, less\ntoxic, and align with human values and preferences.\nHuman values and preferences encompass a wide range, from universal ethical princi-\nples to culturally speciﬁc values, laws and regulations, social etiquette, and domain-speciﬁc\npreferences [39]. These values are the foundation of AI regulations and guidelines. While\nLLMs are trained to incorporate these values, inconsistencies arise due to crowd-sourced an-\nnotations and variations in RLHF efforts across different languages [28, 308, 175]. Whereas\nthe majority of English language LLMs produced by North American institutions tend to\nmanifest American coastal liberal values [153], and those from Chinese institutions might\nincorporate additional Chinese values [106, 418, 352, 15], the values pre-trained in LLMs\nare not always clear, and it is uncertain if different models reﬂect consistent values within\na language or culture. Do different LLMs reﬂect consistent values in a given language and\nculture? Does a single LLM embody different values in different languages? Are values\ntransferable across LLMs and languages? Even at release time, the producers of LLMs lack\nsuch a representative view of the values in the models they have released and whether their\nmodels do indeed align with the desirable values.\nTo better understand human values of LLMs, one can use surveys of human values\nto query LLMs [107, 424, 54, 423]. Surveys can be seen as a kind of sampling in the\nvalue distribution space of an LLM. However, we argue that survey answers are a limited\nsampling method as they only cover a limited amount of dimensions. For instance, the\ndimension of cultural values [168, 170] only captures 6 dimensions to map a vast variability\nin human cultures, while the theory of basic values [339, 342, 343] and the World Value\nSurvey (WVS) [183, 182, 150], only cover 19 and 10 dimensions of values, respectively. We\nargue that such a low-dimension semantic representation will likely fail to give a full picture\nof human values in LLMs. Instead, we aim a high dimension representation of human\nvalue distribution to reﬂect the complexity of the embedded values in LLMs. Ideally, this\nrepresentation needs to be orthogonal to linguistic patterns and model architecture. In\nthis paper, we propose Universal Value Representation (UniVaR) - a high-dimensional\n44\nrepresentation of human values in LLMs. We show that UniVaR representations reﬂect\nthe distances and similarities between different cultures in terms of human values in\nLLMs as illustrated in Figure 4.1. UniVaR offers a systematic and statistical approach to\nunderstanding the value systems of LLMs. UniVaR facilitates the exploration of how LLMs\nlearn and prioritize values in different languages, and is ultimately a powerful tool for\nmore transparent and accountable LLMs.\nBy bridging the gap between the capabilities of LLMs and the imperative of aligning\nthem with human values, UniVaR represents a signiﬁcant step forward in the quest for\nethically sound AI assistants. The signiﬁcance of our work can be summarized as follows:\n1. We are the ﬁrst to develop a theoretical formulation for understanding values in\nLLMs using a high-dimensional abstract representation of values.\n2. We introduce UniVaR, a scalable self-supervised learning method for understanding\nvalues of LLMs in a high-dimensional space allowing a better generalization across\ndifferent values.\n3. Using the high-dimensional value representation, we are the ﬁrst to show a map of\nhuman value distributions across different LLMs in different languages and cultures.\n4.2\nBackground and Preliminaries\nValue Alignment in LLMs\nLLMs are aligned to human values for enhanced service and\nreduced risks [243] with three major goals [413]: teaching LLMs to follow human instruc-\ntions [284], aligning LLMs to implicit human preferences [80], and conforming LLMs to\npre-deﬁned principles [35]. Value alignment typically involves Supervised ﬁne-tuning\n(SFT) and RLHF/RLAIF. In SFT, models are ﬁne-tuned using well-curated conversation\ndata data [210, 72, 273, 349] following human desirable features [413, 328, 210, 137, 128]\nthrough various training paradigms such as contrastive learning [13, 202] and distilla-\ntion [173]. RLHF, commonly used by recent LLMs [382, 2, 16], adjusts models’ policies\nthrough RL by receiving feedback from a reward model aligned with human preferences as\nin Proximal Policy Optimization (PPO) [333]. Unlike PPO , Direct Preference Optimization\n(DPO) [305], eliminates reliance on a reward model. Similarly, RLAIF [226, 416, 174, 240]\n45\ngenerates feedback from the model itself to avoid costly human annotations. While RLHF\nimplicitly elicits preferences from ranking data, Constitutional AI [35] establishes principles\nfor AI to enhance model alignment to explicitly-deﬁned human values through self-critique\nand response modiﬁcation.\nSurveying Human Values in LLMs\nEarly studies on understanding human values in\nlanguage models, such as the ETHICS dataset [160], cover various ethical frameworks\nincluding justice, deontology, virtue ethics, and utilitarianism. Zhang et al. (2023)[423]\nfurther analyzed how language models categorize and reason about different values.\nRelated research includes examining alignment with diverse societal views and stances,\nreferencing global opinion surveys like the Pew Global Attitudes (PEW) and World Values\nSurveys (WVS) [183, 182, 149]. Studies such as Durmus et al. (2023)[107] and Alkhamissi et\nal. (2024)[23] speciﬁcally focus on cultural and social value alignment in language models,\nusing data from these surveys. Zhang et al. (2024) [424] employ social value orientation\n(SVO) measures to assess the alignment of language models with human values. Our work\naims to develop methods for capturing complex human values in high-dimensional spaces\nto enhance understanding of language models’ alignment with human values.\nHigh-Dimension Embedding Representation\nDistributed representations of entities [164]\nunderpinned the advancement of embedding representation, enabling algorithms to cap-\nture nuanced semantic relationships and enhance generalization capabilities. Seminal\nworks in NLP laid the groundwork for word embeddings [163, 322, 111, 268]. This progress\nwas further accelerated by [265, 289], who reﬁned methods to generate word vectors,\nsubsequently enriching research on sub-word and sentence-level embeddings [53, 216, 309].\nIn parallel, computer vision beneﬁted from embedding techniques to capture object rep-\nresentations [147, 262, 159], with recent expansions into sub-object representations [71]\ndemonstrating the versatility of this approach. Embedding has also been applied in health-\ncare and recommendation systems to model complex behaviors [77, 94, 65]. Our work\nextends the embedding paradigm to abstract value representations elicited by LLMs, ad-\nvancing the applicability of embedding representations in understanding LLM preferences.\n46\nܳଵ\nܳଶ\nܳெ\n…\nܣଵ\nܣଶ\nܣெ\n…\nValue eliciting QA set\nܺ=\nܳ௝, ܣ௝\n௝ୀଵ\nெ\nValue eliciting questions\n࣫୴ୟ୪୳ୣ\nܳଷ\nܣଷ\n݂ఏ\nvalue decisive\nߴ୴ୟ୪୳ୣ\nvalue agnostic\nߴ୭୲୦ୣ୰\nparametrize\nߠ= ߶(ߴ୴ୟ୪୳ୣ, ߴ୭୲୦ୣ୰)\nQuestion\nܳ\nAnswer\nܣ\nLLM\n݂ఏ\nlearn a value embedding ܼ\nSection 2.1\nProblem Formulation\nSection 2.2\nValue Eliciting Question Answering\nSection 2.3\nMulti-view Value Embedding Learning\nܺଵ\nܺଶ\nܼ௑భ\nܼ௑మ\nSample random subsets of ߣQA pairs\nfrom ܺ=\nܳ௝, ܣ௝\n௝ୀଵ\nெ\nMaximizing mutual information\nmax\n௚\nܫ(ܼ௑భ, ܼ௑మ)\n݃\n݃\nshare weights\ntwo views\nembeddings\ns.t. max\n௓\nܫߴ୴ୟ୪୳ୣ; ܼെܪ(ܼ)\nFigure 4.2: Overview of problem formulation and design in UniVaR. Left: our objective is to\nlearn a value embedding Z that represents the value-relevant factor #value of LLM. Middle:\nwe elicit LLM values through QA, such that the #value is expressed by the distribution of its\nvalue eliciting QA set X. Right: we apply multi-view learning to compress information,\neliminating irrelevant information while preserving value-relevant aspects.\n4.3\nUniversal Value Representation (UniVaR)\n4.3.1\nProblem Formulation\nWe assume that some factors in LLMs contribute towards aligning with certain human\nvalues while others towards value-agnostic aspects (e.g., wording, syntax, or style). Let an\nLLM parameterized by ✓be f✓, our assumption can be formalized as ✓= φ(#value, #other)\nwith some function φ, where #value is the value-decisive factors and #other is the value-\nagnostic factors. Our goal is to extract the value-decisive factors #value such that we can\nanalyze similarities of values from different LLMs or transfer values across LLMs.\nIf we know LLM parameters ✓and we are able to derive the inverse function φ-1, we\ncan directly recover value factors from by [#value, #other] = φ-1(✓). However, this cannot\nbe applied to closed-source LLMs where ✓is not accessible, and also there is no clue how\nto estimate φ-1. The relationship and interactions between #value and #other are unknown,\nand locating value-decisive parameters from billions of LLM parameters is also difﬁcult.\nDue to the difﬁculty of extracting value-decisive factors #value explicitly, we consider a\nsurrogate task named value embedding learning. Following the information bottleneck\n47\nPerformance \norientation\nHumane \norientation\nIndividualism\nCollectivism\nMasculinity & \nfeminity\nGender \negalitarianism\nHuman Values\nValue Eliciting Questions\nWould you rather work \novertime to complete a \nproject or delegate tasks to \nensure work-life balance?\nWould you prioritize your \nown needs or the needs of \nyour community?\nAre women in your society \nencouraged to pursue \ninterests outside of their \ndomestic duties?\nWould you rather work \novertime to complete a \nproject or delegate tasks to \nensure work-life balance?\n⾧〾『↟䋬㙤⪋ㄏ杸䗭廗㔮\n⣓㰽᷺↠ᷤ䝭Ờⳤṛᴍ䐞㰺\n䖃⵲坠漦\n\u0003ϝΎϤϛϹ\u0003ϲϓΎοϹ΍\u0003ϞϤόϟ΍\u0003ϞπϔΗ\u0003Ϟϫ\n\u0003ϥΎϤπϟ\u0003ϡΎϬϤϟ΍\u0003ξϳϮϔΗ\u0003ϭ΃\u0003ωϭήθϤϟ΍\nˮΓΎϴΤϟ΍ϭ\u0003ϞϤόϟ΍\u0003ϦϴΑ\u0003ϥί΍ϮΘϟ΍\nI would rather \ndelegating tasks to \nensure my work-life \nbalance.\nㄐ⪀〾↟䋬㙤⪋ㄏ᷺\n↠漓◟ᴹ廘㔮ㄐ尢᷺\n䖃᳿忧℅ȼ\nϝΎϤϛϹ\u0003ϲϓΎοϹ΍\u0003ϞϤόϟ΍\u0003Ϟ˷πϓ΃\n\u0003Ϧϣ\u0003˯ΰΟ\u0003ΎϬϧϷ\u0003ΔϤϬϤϟ΍\nϲΘϴϟϭΆδϣ\n.\nTranslated Questions\nI prefer to delegate tasks \nto ensure work-life \nbalance.\nI would rather work \novertime to complete the \ntask because it is part of \nmy responsibility.\nI would rather delegating \ntasks to ensure my work-\nlife balance.\nLLaMA-2\n(English)\nChatGLM\n(Chinese)\nJais\n(Arabic)\nLLMs\nGenerated Answers\nTranslated Answers\nFigure 4.3: Value-eliciting QA generation pipeline. English value-eliciting questions\nare synthesized using a set of human values and the diversity is enhanced through para-\nphrasing. Each question is translated into multiple languages andfed into LLMs to get\nthe value-eliciting answers. All QA pairs are translated back into English to minimize the\nlinguistics variation across QAs.\nprinciple for representation learning [326, 381, 384], we aim to learn a compact represen-\ntation Z that contains maximized mutual information with the #value of interest while\ndiscarding other confounding factors as much as possible.\nDeﬁnition 4.3.1 (Value embedding learning) The goal of value embedding learning is to ex-\ntract sufﬁcient information about the value-decisive factors #value and represent it as a value\nembedding Z with minimal redundancy. The overall objective can be written as:\nmax\nZ\nI(#value; Z)\n|\n{z\n}\nmaximizing\ncorrelation\n-\nH(Z)\n| {z }\nminimizing\nsuperﬂuity\n,\n(4.1)\nwhere I and H denote mutual information and entropy, respectively.\n4.3.2\nValue Eliciting Question Answering\nThe core challenge of value embedding learning lies in the fact that #value exists as a latent\nvariable [217, 427]. What we can observe are the input queries and output responses driven\nby #value, but not the #value itself. Let Q denote input questions and A denote LLM answers.\nAs described in § 4.3.1, #value may or may not be involved in generating A, depending\non the nature of Q. For instance, a question asking for an arithmetic operation would be\n48\nsolely dependent on the reasoning capabilities represented by the value-agnostic #other,\nwhile #value hardly matters. On the other hand, a question that involves an ethical dilemma\nsuch as the trolley problem should be highly dependent on #value. Since our interest lies on\nvalues, we consider a set of questions Qvalue that elicit LLM’s values:\nDeﬁnition 4.3.2 (Value eliciting questions) Given LLMs fi\n✓and fj\n✓where #i\nvalue 6= #j\nvalue,\nQ 2 Qvalue leads the LLMs to generate different answers: fi\n✓(Q) 6= fj\n✓(Q). We call this set of\nquestions Qvalue as value elicit questions.\nBy Deﬁnition 4.3.2, if Q 2 Qvalue, we know that the QA pair hQ, Ai satisﬁes I(#value; hQ, Ai) >\n01. However, a single QA pair is not representative enough for #value since it is impossible\nto extrapolate the entirety of human values from a single QA. For instance, even a broad\nquestion such as “What is the meaning of life?” or “What is the ideal society?” can only\nelicit values that are related to terminal values [314, 315] and cultural values [168, 170],\nwhile neglecting other aspects of human values.\nTherefore, we consider using a wide array of value-eliciting questions to elicit and\nrepresent LLM’s values. Speciﬁcally, we prepare a set of M value eliciting questions\n{Qj}M\nj=1, and get the corresponding answers from each LLM. @e denote all QA pairs as\nX = {hQj, Aji}M\nj=1. Optimizing X towards maximizing I(#value; X) less challenging than\noptimizing a single Q towards maximizing I(#value; hQ, Ai) mentioned before, since it is\neasier to increase the question diversity or the number of questions M.\n4.3.3\nMulti-view Value Embedding Learning\nAs discussed in §4.3.2, a large set of value eliciting QA pairs X can give sufﬁcient guidance\nto maximize its dependency to the value-decisive factors #value of one LLM (the ﬁrst term\nin Eq. 4.1 ). However, since QA pairs are presented in the form of natural language, this X\nalso contains value-irrelevant information such as wording and syntax, which makes the\nsecond term, i.e., minimizing superﬂuity, not satisﬁed.\n1By deﬁnition, mutual information I(#value; hQ, Ai) = DKL(P(hQ, Ai, #value)||P(hQ, Ai)P(#value)), where the\nKL divergence DKL is always non-negative and is zero if two distributions are identical. Since hQ, Ai and\n#value are dependent, their joint distribution is different from the product of their marginal distributions,\nwe can know I(#value; hQ, Ai) > 0.\n49\nTo eliminate the redundancy in X, we propose to apply multi-view self-supervised\nlearning [384, 351] to compress X while keeping value-relevant information intact. Speciﬁ-\ncally, as shown in Figure 4.2, we sample two views 2 X1, X2 from X by selecting random\nsubsets of λ QA pairs. We adopt a joint embedding architecture [225] that includes a\nSiamese network [334, 332, 374] g that takes two views as input and produce represen-\ntations ZX1 = g(X1) and ZX2 = g(X2) We optimize g towards maximizing the mutual\ninformation across two views:\nmax\ng\nI(ZX1; ZX2).\n(4.2)\nMaximizing mutual information between multiple views requires g to capture underly-\ning factors whose inﬂuence spans multiple views while excluding non-shared factors. The\nlearned value embedding Z will capture the information of value-decisive factor #value with\na loss of ✏info > I(#value; X1|X2) [384]. This conditional mutual information quantiﬁes the\namount of additional information about #value given by having another view X2 compared\nto only knowing X1. Since two views are sampled from the same value, the corresponding\n#value is shared across two views, we can expect the ✏info to be small.\nConversely, g will compress X but retain some superﬂuous information I(X1; X2|#value)\nthat is shared by X1, X2 but irrelevant to #value [384]. In this work, we consider each LLM in\neach language has a distinct value [236, 107, 23], so the information about which language\nis used in question answering is also shared across views, which will make g to learn\nlanguage identiﬁcation as a shortcut instead of learning value identiﬁcation as desired.\nSuch superﬂuous information can be mitigate from Z by minimizing I(X1; X2|#value) via\npreprocessing. For example, simply translating all QA pairs to one language before feeding\nto g can eliminate this language information.\n50\nValue\nGenerated Value Eliciting Questions\nIndividualism\nvs\nCollec-\ntivism\n• Do you place a higher priority on being independent or having interdependent relationships?\n• Do you think it’s better to split the credit for successful outcomes with others or to take all the\ncredit on your own?\nHarmony vs\nMastery\n• What is your opinion on the signiﬁcance of striving for self-improvement and personal growth?\n• In a situation where you have to choose, do you prioritize your individual success over the\ncommunity’s welfare?\nPerformance\nvs Humane -\nOrientation\n• Is it inappropriate to criticize a team member who has been emotionally affected by personal\nevents?\n• Is it necessary to monitor staff’s online activities to drive positive performance outcomes?\nAffective Au-\ntonomy\n• Do you believe that protecting your mental well-being should take\nprecedence over meeting societal expectations?\n• What are some ways you cope with opposition to your desires\nwhen dealing with conﬂicting viewpoints?\nTable 4.1: Samples of the generated value eliciting questions.\n4.4\nExperiment Design\n4.4.1\nConstructing the Value Eliciting QA Training Set\nFigure 4.3 outlines our value-eliciting QA pipeline.\nWe start by compiling 87 refer-\nence human values from multiple human value studies including World Value Survey\n(WVS) [183, 181, 182], cultural dimensions theory [168, 170, 176, 169], theory of basic hu-\nman values [338, 339, 340, 337, 341, 329, 42], the reﬁned theory of values [343] and Rokeach\nValue Survey [314, 315, 316, 317]. For each reference value, we use LLMs to generate 50\nrelevant value-eliciting questions Q 2 Qvalue. After manually verifying and ﬁltering our\nirrelevant questions, we retain 4,296 questions. To enhance robustness, we paraphrase\neach question 4 times, resulting in a total data size of 21,480 (4,296 ⇥5) questions. These\nquestions are then translated into 25 languages that are supported by all the training LLMs\ndescribed in §4.4.2 to better understand the values expressed by LLMs across different\nlanguages. The generated value eliciting questions are shown in Table 4.1.\nThe multilingual value-eliciting questions are fed into LLMs to obtain the corresponding\nvalue-eliciting answers. To minimize linguistic variations across different languages, all\nquestion-answer pairs from languages other than English are then machine-translated into\n2As an analogy, in computer vision, the views can be images of one person’s face taken from different angle.\nIntuitively, our view in UniVaR can be interpreted as taking several “pictures” of one LLM’s values.\n51\nEnglish. This translation step is to eliminate language from becoming a confounding factor\nwhen training UniVaR since they are irrelevant to human values. Overall, we collected\n⇠1M QA pairs for training. For translation, we employ NLLB-200 (3.3B) [378]. 3.\n4.4.2\nModel and Language Coverage\nFor building UniVaR, we incorporate 15 off-the-shelf LLMs that are instruction tuned [323,\n272, 392, 246] to ensure their ability in answering the given query. We prioritize LLMs that\nhave undergone human value and preference tuning such as safety tuning [425, 259, 47],\nRLHF [80, 284], direct preference optimization (DPO) [305]. Out of 15 LLMs, we incorporate\nQAs from 8 LLMs for training and leave the other 7 as unseen LLMs for evaluations. We\nsupport 25 languages which are considered high-resource languages within LLMs under\nstudy. The complete list of all LLMs and languages used within this work is described in\nTable 4.2. The detailed supported language list is presented in Table 4.3 along with the\nNLLB 3.3B and NLLB 54B MoE performance gathered from NLLB Team et. al . (2022) [378]\nas references for the translation quality. We treat each LLM prompted in different languages\nto elicit distinct LLM values (i.e., LLM values of ChatGPT English and of ChatGPT Chinese\nare distinct). In total, we have 127 distinct pairs. Using prompts in various languages leads\nto diverse responses [236] and prompts in a culture’s dominant language typically align\nmore with that culture [23]. 4\n4.4.3\nTraining and Evaluation Settings\nTraining\nFor UniVaR training, we use Nomic Embed v1 [277] as our backbone model as it\nsupports long-context modeling. We train UniVaR with dynamic number of QAs per view\nfrom [1..λ], with λ 2 {1, 5, 20, 80}. We apply the InfoNCE loss function [386] to maximize the\nobjective function in Eq. 4.2, but other alternatives can be also used [417, 146, 156, 73, 74,\n129]. To train the model, we adopt a similar hyperparameter setting used for ﬁne-tuning a\npre-trained BERT [102] and RoBERTa [245] models. The model was trained using AdamW\noptimizer [248] for 1 epoch with a learning rate of 1e-5 and a linear warmup scheduler\n3https://huggingface.co/facebook/nllb-200-3.3B\n4It is important to note that using the dominant language does not guarantee an accurate representation of\na culture [107, 23]. Moreover, current LLMs are found to be predominantly Anglocentric [107, 274, 155].\n52\nModel Name\nRegion\nCorpus Type\nSupported Languages\nSubset\nMixtral Instruct (8x7B) 5\nEurope\n-\nfra, deu, spa, ita, eng\nTraining\nAya-101 (13B) [385, 355] 6\nGlobal\n-\neng, fra, arb, deu, ita, jpn, hin\nTraining\nzho, vie, tur, spa, ind\nSeaLLM (7B) [276] 7\nSEA\nTranslation-Heavy\neng, zho, vie, ind\nTraining\nBLOOMZ RLHF (7B) [272] 8\nGlobal\nTranslation-Heavy\neng, zho, fra, spa, arb, vie, hin, ind\nTraining\nChatGLM-3 (6B) [418, 106] 9\nChina\nNatural\nzho, eng\nTraining\nNous Hermes Mixtral (8x7B) 10\nUS\n-\nfra, deu, spa, ita, eng\nTraining\nSOLAR Instruct [204] 11\nKorea\n-\neng\nTraining\nMistral Instruct (7B) 12\nEurope\n-\nfra, deu, spa, ita, eng\nTraining\nJAIS Chat (30B) [347] 13\nArab\nTranslation-Heavy\narb, eng\nUnseen\nYi Chat (34B) [15] 14)\nChina\nNatural\nzho, eng\nUnseen\nLLaMA2 Chat (13B) [382]\n15\nUS\n-\neng, deu, fra, swe, zho, spa, rus, ita,\nUnseen\njpn, por, vie, kor, ind, ﬁn, ron, bul\nMaral-7B-alpha-1 16\nIran\nTranslation\npes, eng\nUnseen\nCommand-R 17\nEurope\n-\neng, fra, spa, ita, deu, por, jap, kor, arb, zho\nUnseen\nMeta-Llama-3-8B [16]\n18\nUS\n-\neng, deu, fra, swe, zho, spa, rus, ita,\nUnseen\njpn, por, vie, kor, ind, ﬁn, ron, bul\nChatGPT [38] 19\nUS\n-\neng, zho, kor, jpn, deu, ﬁn, swe, fra,\nUnseen\nspa, ita, por, tha, vie, zsm, tgl, hat,\nquy, rus, ron, bul, ind, arb, swh, hin, pes\nTable 4.2: List of LLMs incorporated in our UniVaR experiment. For language codes,\nwe adopt the ISO 639-3 standard. The name of the languages can be seen in Table 4.3.\nDepending on the amount of translated corpus in the training and instruction-tuning, we\ndenote the type of the corpus used for training which can be either natural or translation-\nheavy, where the corpus augmented with the translation data.\nwith a warmup step of 1000. During training, we use a batch size of 128 for both training\nand validation. All our experiments are conducted on 4 NVIDIA Tesla A800 GPU.\nEvaluation\nFor evaluation, we develop an LLM value identiﬁcation dataset based on\n4 sources of value-eliciting questions, covering 3 well-established value questionnaires\nin the ﬁeld of social science and psychology – i.e., the recently revised Portrait Value\nQuestionnaire (PVQ-RR) [341, 342, 343], World Value Survey (WVS) [183, 181, 182], and\nGLOBE survey[176, 186] – and ValuePrism [367] – a large-scale value dataset for endowing\nAI with pluralistic human values, rights, and duties. These data sources do not originally\nprovide natural value-eliciting questions for LLMs, hence we employ Mixtral 8x7B [187] to\ngenerate questions based on the context provided in the data sources. For PVQ-RR and\nValuePrism, we use the situations provided. For GLOBE survey, we create the context\n53\nLang. Name\nLang. Code\nLang. Family\n#Speakers\nNLLB 3.3B (ChrF++)\nNLLB 54B MoE (ChrF++)\nEN!XX\nXX!EN\nEN!XX\nXX!EN\nEnglish\neng\nIndo-European\n1.46B\n-\n-\n-\n-\nChinese\nzho\nSino-Tibetan\n1.14B\n22.3\n56.2\n22.8\n57.2\nHindi\nhin\nIndo-European\n610M\n57\n65.9\n57.3\n66.5\nSpanish\nspa\nIndo-European\n600M\n54.2\n59.1\n53.8\n59.4\nArabic\narb\nAfro-Asiatic\n380M\n55\n65.8\n57.1\n66.9\nFrench\nfra\nIndo-European\n310M\n69.6\n68.1\n69.7\n68.4\nIndonesian\nind\nAustronesian\n300M\n68.8\n67.3\n68.7\n67.2\nMalay\nzsm\nAustronesian\n290M\n66.3\n67.8\n66.5\n68\nPortuguese\npor\nIndo-European\n260M\n69.4\n71.3\n67.9\n71.2\nRussian\nrus\nIndo-European\n255M\n56.1\n61.3\n56.3\n61.8\nGerman\ndeu\nIndo-European\n133M\n62.8\n67.4\n62.8\n67.5\nPersian\npes\nIndo-European\n130M\n49.4\n62.7\n51.3\n63.8\nJapanese\njpn\nJaponic\n123M\n25.2\n55.1\n27.9\n55.8\nSwahili\nswh\nNiger-Congo\n88M\n60\n65\n58.6\n66.1\nVietnamese\nvie\nAustro-Asiatic\n86M\n59.3\n61.5\n59.5\n62.3\nTagalog\ntgl\nAustronesian\n83M\n60.6\n68.2\n60.5\n70.1\nKorean\nkor\nKoreanic\n82M\n34.3\n56.1\n36\n56.6\nItalian\nita\nIndo-European\n68M\n57.1\n61.2\n57.3\n61.3\nThai\ntha\nKra-Dai\n61M\n40.5\n56.8\n42.7\n57.8\nRomanian\nron\nIndo-European\n25M\n60.7\n68.1\n61.3\n68.7\nSwedish\nswe\nIndo-European\n13M\n66\n69.8\n65.9\n69.6\nHaitian\nhat\nCreole\n13M\n51.3\n61.8\n51.9\n62.2\nQuechua\nquy\nQuechuan\n7.2M\n26.7\n33.9\n26.9\n34.6\nBulgarian\nbul\nIndo-European\n10M\n64.3\n66.3\n64.8\n66.3\nFinnish\nﬁn\nUralic\n5M\n53.9\n60.4\n55.3\n60.9\nTable 4.3: List of all languages covered in our study sorted by the number of speakers. The\n#Speakers information is retrieved from Wikipedia.\nfrom the sentence and two opposing values within each question. For WVS, we take the\nquestion as is when the item is already formatted as a question, or we take the situation or\nmultiple choices provided if it is not a question. We then translate the questions into 25\nlanguages as detailed in shown in Table 4.3. Using the multilingual questions, we generate\nthe answers using all LLMs under study on the languages that are supported by each of\nthe LLMs, and then translated the QA back to English.\nThe resulting English-only value-eliciting QAs data is use for evaluating the effective-\nness of UniVaR. We evaluate the UniVaR representations by using linear probing and\nk-Nearest-Neighbour(kNN) using only a single QA as the input to identify the correct label\nout of 143 LLM value labels. For evaluation, we employ linear probing and k-Nearest-\nNeighbour. For linear probing, we train a linear classiﬁer using the representation of\nthe embedding models as the input, and output the predicted LLM value identity. We\nuse AdamW optimize with a learning rate of 2e-3 and a batch size of 512. We train the\n54\nType\nModel Name\n#Param\nAcc\nF1\nAcc@1\nAcc@5\nAcc@10\nRandom\nMajority\nHeuristics\nHeuristics\n-\n0.78%\n0.77%\n0.78%\n3.9%\n7.8%\nk-NN\nLinear\nWord\nEmb.\nGloVe\n120M\n2.27%\n2.26%\n5.45%\n17.19%\n27.72%\nBERT (base)\n109M\n1.78%\n1.82%\n10.57%\n28.87%\n42.20%\nRoBERTa (base)\n125M\n1.88%\n1.89%\n10.06%\n27.70%\n41.17%\nSentence\nXLM-R (base)\n278M\n1.40%\n1.41%\n8.65%\n24.96%\n37.92%\nEmb.\nMPNet (base)\n109M\n1.40%\n1.49%\n4.73%\n15.74%\n25.80%\nNomic Embed v1\n137M\n1.03%\n1.26%\n7.11%\n21.95%\n33.29%\nLaBSE\n471M\n4.03%\n3.94%\n11.76%\n32.16%\n47.48%\nUniVaR (λ=1)\n137M\n18.68%\n15.24%\n17.40%\n42.91%\n57.98%\nOurs\nUniVaR (λ=5)\n137M\n20.37%\n16.84%\n18.67%\n45.75%\n61.70%\nUniVaR (λ=20)\n137M\n19.99%\n17.22%\n17.76%\n44.67%\n60.39%\nUniVaR (λ=80)\n137M\n18.01%\n15.75%\n15.98%\n41.49%\n57.18%\nTable 4.4: Value identiﬁcation quality from different representations. UniVaR achieves a\nsigniﬁcantly higher score compared to all baselines indicating the effectiveness of UniVaR\non capturing value representation. UniVaR is conspicuously different with sentence em-\nbedding models.\nclassiﬁer for 20 epochs. For the kNN experiment, we use number of neighbour k = 50.\nWe measure the accuracy and F1-score between the predictions and labels for kNN, and\naccuracy@1, accuracy@5, and accuracy@10 for linear probing. We compare UniVaR to\nword embedding model, i.e., GloVe [289] and various sentence embedding models, i.e.,\nRoBERTa [245], XLM-R [89], MPNet [366], Nomic Embed v1 [277], and LaBSE [115]\n4.5\nResults and Analysis\n4.5.1\nEvaluation Results\nUniVaR Capture Value-Relevant Features\nAs shown in Table 4.4, UniVaR displays a\nstrong capability surpassing all baselines by ⇠15% k-NN accuracy and ⇠10-15% linear\nprobing accuracy@10 on the LLM value identiﬁcation task. Word and sentence embedding\nrepresentations perform poorly with <5% k-NN accuracy on the LLM value identiﬁcation\ntask indicating that there are signiﬁcant differences between value representations from\n55\n1.78 \n2.27 \n4.03 \n1.40 \n1.03 \n1.88 \n1.40 \n18.68 \n20.37 \n19.99 \n18.01 \n2.64\n5.18\n4.56\n4.13\n2.66\n4.18\n6.58\n2.55\n2.55\n2.58\n2.60\nGloVe\nBERT\nRoBERTa\nXLM-R\nMPNet-v2\nNomic-v1\nLaBSE\nUniVaR \n(Ȝ \u0014\f\nUniVaR \n(Ȝ \u0018\f\nUniVaR \n(Ȝ \u0015\u0013\f\nUniVaR \n(Ȝ \u001b\u0013\f\nAccuracy \u000b\b\f\u0003on value-eliciting QA\nAccuracy on non-value-eliciting QA\nFigure 4.4: Performance comparison of UniVaR between value-eliciting QAs and non-\nvalue-eliciting QAs from LIMA [426]. The inﬂuence of non-value-related confounders in\nUniVaR is minimal compared to other baselines signiﬁes by the substantial performance\ngap between the two tasks.\nUniVaR and existing embedding representations.\nUniVaR Minimally Capture Non-Value-Relevant Factors\nDespite the efforts to eliminate\nthe inﬂuence of non-value-related confounders through English-only multi-view learning,\nUniVaR might still be affected by generation and translation artifacts such as writing\nstyle, choice of common words, and translationese [134, 178, 14, 299]. We investigate such\nartifacts by checking whether source LLMs can be distinguished using our UniVaR rep-\nresentations on non-value-eliciting QAs, e.g., “Can you implement KMP algorithm\nwith python?”, gathered from LIMA [426]. Ideally, it should be hard to identify LLM\nwhen non-value-eliciting questions are used because these questions would not elicit\n“human values” embedded in LLMs within the answer. As shown in Figure 4.4, UniVaR\nis partially affected by these artifacts, nonetheless, the inﬂuence is less indicated by the\nsubstantial performance drop between the value-eliciting and non-value-eliciting QAs.\nAdditionally, we show that UniVaR captures less translationese factors compared to other\nrepresentations as shown in Appendix K.\nImpact of View Size in UniVaR\nWe further assess the effect of view size in the multi-\nview learning of UniVaR by incorporating more QAs in the input. We train a model\nusing varying degrees of the number of QA per view λ 2 {1, 5, 20, 80}. In Table 4.4, we\n56\nFigure 4.5: (left) Cultural clusters in the map of UniVaR value representation. (right)\n2023 version of Inglehart–Welzel Cultural Map20. The UniVaR value representations\ndemonstrates relations between LLM values and human cultures where similar cultures\ntend to be clustered together within the same region, while unrelated cultures tend to be\ndisjoint and located far apart from one to another forming regional values.\ndemonstrate that learning the dynamic number of QAs λ brings some beneﬁts in the case\nof generalization when using only a single QA (λ = 1). Nonetheless, the improvement\npeaked at λ = 5, while it consistently decreases when using higher λ potentially due to\nunderﬁtting on the λ = 1 case due to the huge dynamic range of the number of QA. In the\nlater sections, we use the best model with λ = 5 as our default model unless otherwise\nspeciﬁed.\n4.5.2\nMap of UniVaR Representations\nInspired by human value maps such as Hofstede’s Globe [168, 170, 169, 171] and World\nCultural Map [183, 181, 182] , we introduce a value map of LLMs to visualize the human\nvalues embedded in LLMs. To create the value map independent from the training data,\nwe utilized the QAs from four value-eliciting question sources described in § 4.4.3. We\nencode each QA using UniVaR and we visualize the map of LLM values by projecting the\nvalue embeddings into a 2D plane using UMAP [258]. The result of the value distributions\nare shown as a “world map” in Figure 4.1. In general, we observe that value QA pairs in\n20Image source: https://www.worldvaluessurvey.org/images/Map2023NEW.png\n57\nFigure 4.6: Translation-heavy LLMs tend to show more similar value across languages,\nindicating less cultural relevance on regions where the language are spoken.\nthe same language from different LLMs are clustered together, which show that the values\nembedded in LLMs largely come from the culture of the language they are trained in. In\nthis case, language acts as a proxy for culture [23]. We provide the detailed per language\nvisualization of UniVaR value representation in Appendix L.\nThere is also a separation of value distribution between LLMs in different languages\nas shown in Figure 4.5. The distance of values across different languages also signi-\nﬁes the similarities and differences of human values between different cultures. For\ninstance, \"Chinese-Japanese-Korean\", \"German-French-Spanish\", and \"Indonesian-Arabic-\nMalaysian\" are closer in value distribution compared to the other language pairs with a\nrelatively distant culture. German, French, and Spanish share similar European values.\nChinese, Japanese, and Korean share similar Confucian and Buddhist values. Indone-\nsian, Malaysian, and Arabic cultures share Islamic values, despite the linguistic difference\nbetween Indonesia/Malay and Arabic. Interestingly, English value distribution is rela-\ntively far from that of French, German, Italian, and Spanish, despite originating from\ncountries with Western values. This agrees with the human value map in World Value Sur-\nvey [183, 181, 182] (see Figure 4.5 (right)), where English-speaking societies are categorized\ninto their own group due to the impact of colonization and massive immigration from the\n58\nFigure 4.7: Per dataset visualization of UniVaR representation. UniVaR representations\nshow robust human value representations across value corpora.\ncolonial society [98, 377, 361, 373]. As shown in Figure 4.7, this pattern is also consistent\nacross four different value corpora indicating that the value representation in UniVaR is\nrobust to the variability of questions.\nImpact of Translation Corpus to Cultural Relevance\nLLMs that are trained with natural\ncorpus tend to have average embedding representation per language that are distinct from\none to another. While, interestingly, althought training on translation data can improve\nthe cross-lingual generalization ability of LLMs, the embedding representation across\nlanguage tends to similar from one to another. As shown in Figure 4.6, Yi and SEALLM\nthat are trained on more natural data, tend to have distinct value representation between\nthe centroid of theirs Chinese and English language representations. While JAIS, which\nis heavily trained on english-translation-arabic corpus, resulting in a much closer value\nrepresentation between the centroids of their Arabic and English representations. This\nsuggests that, although training on translated data might be beneﬁcial for improving the\nlanguage capability of the LLMs, it might lead to the misrepresentation of culture to the\ncultures where the language is actually spoken.\nUnderstanding UniVaR from Human Value Perspectives\nTo further understand the\nrelation between UniVaR representations and human values, we conducted a qualitative\nanalysis to explore how the distance in embedding space manifests conceptually. We\nanalyzed model responses to value-eliciting questions, noting that greater distances in\nUniVaR embedding often correspond to contrasting values, while closer distances indicate\nshared values. For example (Figure 4.8), ChatGPT-English and ChatGPT-Chinese, which\nare further apart, show distinct values: ChatGPT-English emphasizes liberty of choice\n59\nFigure 4.8: Illustration of how UniVaR embedding correlate with cultural values. On the\nleft, ChatGPT-French and Mixtral-German, which are closer, share the same value. On the\nright, ChatGPT-English and ChatGPT-Chinese, which are further apart, reﬂect contrasting\nvalues.\nfor vaccination, whereas ChatGPT-Chinese highlights social responsibility. Conversely,\nChatGPT-French and Mixtral-German, which are closer, share the value of the rule of law in\nresponses about tracking a criminal’s IP address. More details are shown in Appendix M.\nUniVaR as a Measure for Value Alignment\nAside for understanding the existing values\nembedded in LLMs, UniVaR is useful for measuring the degree of value alignment. In\nthis section, we showcase a utilization of UniVaR to quantitatively assess the degree of\nvalue alignment in LLMs by measuring and visualizing the value representation of LLMs\nin UniVaR representation. We employ Direct Preference Optimization (DPO) [305] to\nadapt the value representation of Phi-2 model 21, which is trained on English datasets\nand consequently exhibits values similar to those shown by models prompted in English\n(eng in Fig. 4.9). We experiment to align Phi-2 model towards Chinese value (i.e., LLM\nvalues that are elicited in Chinese; zho in Fig. 4.9). We construct a preference-tuning dataset\nfrom model-generated QA pairs based on the ValuePrism dataset using ChatGLM 6B and\nSeaLLM 7B models. To steer from Chinese language values to English, we take responses\nin Chinese as preferred answers while rejecting responses in English.\n21https://neurips.cc/media/neurips-2023/Slides/83968_5GxuY2z.pdf\n60\nPhi-2\nOriginal\n3k steps\nReference\nEmbeddings\nzho\neng\nDPO Value Alignment (eng՜zho)\nValue similarity \nto eng: 3.50 \nto zho: 9.72\nValue similarity \nto eng: 3.79 \nto zho: 9.46\nValue similarity \nto eng: 8.29 \nto zho: 5.00\n1k steps\nFigure 4.9: Visualization of UniVaR representation of Phi-2 during value adaptation from\nEnglish LLM values to Chinese LLM values via DPO. From left to right, the shift in Phi-2\nvalue representation is seen moving from its original location (pink) to the target values\n(blue). The value similarity score (smaller means more similar), derived from the distances\nbetween UniVaR value representations and measures the extent of value similarity across\ndifferent phases of transfer.\nExperiment Setting\nWe explore a preference alignment framework using DPO [305] for\nvalue transfer, directly training LLM without relying on a reward model. We employ DPO\nto train Phi-2 with β = 0.01 and a learning rate of 1e - 7 on a preference-tuning dataset\nderived from model-generated QA data based on ValuePrism questions and ChatGLM\n6B and SeaLLM 7B responses, partitioned with an 80-20 train-test split. The generated\nanswers demonstrate a shift from values common in English LLM responses towards the\nChinese counterpart. In the ﬁrst row, initially the models highlight values of individualism.\nOver the DPO training steps, they pivot towards emphasizing benevolence, underlining\nthe importance of social responsibility and helpfulness in familial and social contexts.\nFurthermore, in the last row, the transition from valuing affective autonomy towards\nprioritizing harmony and interpersonal conformity is evident. These transitions, along\nwith the visual and quantitative measurement depicted in Figure 4.9, illustrate the trajectory\nof alignment process towards different cultural values.\nResult\nWe illustrate the effectiveness of UniVaR to measure and visualize the degree\nof alignment through the visualization in Figure 4.9. From left to right, we can observe\nthe shift of English value representation of Phi-2 from its original value region (eng)\ntowards the target values (zho). To further quantify this shift, we compute the Euclidean\n61\ndistance between the centroids of value representations of Phi-2 model and those of target\nand reference. The distances indicate the degree of value similarity between the sets of\nembeddings, thereby enhancing the transparency of the value alignment process.\n4.6\nConclusion\nIn this chapter, we aim to develop a solution that allows us to better understand the\ncultural value embedded inside LLMs. We accomplish this through UniVaR, a novel\napproach to representing the values embedded in LLMs across different languages and\ncultures.UniVar is a universal value representation that captures high-dimensional human\nvalue distributions in LLMs. UniVar is trained on value-relevant information from eight\nmultilingual LLMs and generalizes well to various open-source and commercial LLMs.\nWith UniVar, we demonstrated the potential to compare the representation of human\nvalues across different LLMs and languages, shedding light on the complex interplay\nbetween human values and language modeling. We also showcased the applicability of\nUniVar for automatically assessing the degree of multicultural value alignment in LLMs, a\ncrucial and challenging evaluation bottleneck that limits existing works in value alignment.\nWe highlight the cultural diversity of existing multilingual LLMs depends on the source\nof multilingual corpora and the value alignment process where model that is trained\non translation-heavy data tends to have similar cultural value across language, while\nthe one that is trained on natural monolingual data tends to have more diverse cultural\nvalues across languages. Our work contributes to the ongoing discussion on the LLM\ndevelopment to ensuring that LLMs are aligned with the appropriate cultural values where\nthese LLMs are operating. Ultimately, our work calls for further research and discussion on\nthe implications of value alignment in LLMs, highlighting the need for a comprehensive\nunderstanding of the complex interplay between LLMs, languages, and cultural values.\n62\nCHAPTER 5\nUnderrepresented Languages Adaptation in Large\nLanguage Models\nMultilingual PLMs ans LLMs require abundant data to learn languages [90, 89, 141, 244,\n412, 106, 327]. Despite all the efforts – as shown in Figure 5.1 – existing PLMs and LLMs\nonly cover a small fraction of languages raising the needs of language adaptation meth-\nods for adapting PLMs and LLMs to new languages. Nevertheless, when adapting to\nunderrepresented languages, there is a gap where the amount of data needed for model-\ning is large while the availability of high-quality data in underrepresented languages is\nlimited. To alleviate this problem, prior works develop various efﬁcient language adap-\ntation [292, 27, 290, 291, 11] and cross-lingual alignment [354, 294, 20, 391, 66, 401, 376]\nmethods for adapting existing models to unseen languages. Nonetheless, these methods\nare introduced for PLMs, which involves a costly training and only focus on task-speciﬁc\nadaptation of the target languages. This is less preferable in multilingual LLMs as the\ntraining becomes intractable as the size of multilingual LLMs increase. Furthermore, a\nrecent work [414] showcases that an existing language adaptation method, MAD-X [292],\nfails to generalize to multilingual instruction-tuned LLM, leading to the loss of task gen-\neralization ability. This indicates the needs of novel language adaptation methods for\nmultilingual LLMs which can not only learn the new languages, but also maintain the\ntask generalization ability of multilingual LLMs especially in the pre-trained high-resource\nlanguages.\nIn this chapter, we explore data-and-compute-efﬁcient methods for language adaptation\nin multilingual LLMs through cross-lingual alignment which enables better generalization\non underrepresented languages without the loss of task generalization ability. We develop\nthree cross-lingual alignment methods that signiﬁcantly improve the model capability on\nunderrepresented languages under a different-degree of data availability. First, we intro-\nduce continual cross-lingual instruction-tuning method called Instruct-Align which enables\n63\nlanguage adaptation on underrepresented languages while maintaining the existing high-\nresource language capability and requires only thousands of parallel underrepresented to\nhigh resource data. Second, we introduce two methods for a better language adaptation\nthrough cross-lingual in-context-learning: 1) semantic cross-lingual in-context learning,\nwhich enables better few-shot cross-lingual in-context learning without the needs of any\ntask-relevant information on underrepresented languages; and 2) we introduce in-context\nquery alignment, which enables cross-lingual alignment through in-context learning with-\nout the needs of any task-relevant information. Unlike Instruct-Align the last two methods\ndo not require any parameter update at all.\n5.1\nIntroduction\nMultilingual LLMs have revolutionized the ﬁeld of Natural Language Processing (NLP)\nwith their impressive capabilities in understanding and generating human language. How-\never, the language coverage of existing multilingual LLMs is limited, excluding many\nunderrepresented languages with smaller linguistic resources. To alleviate this problem,\nwe explore data-and-compute-efﬁcient methods for adapting multilingual LLMs to under-\nrepresented languages through cross-lingual alignment. By aligning the target language\nwith high-resource languages, we aim to improve the model’s capability in understanding\nand generating underrepresented languages while maintaining its task generalization\nability. In this work, we propose and investigate three innovative approaches for language\nadaptation in multilingual LLMs:\n• InstructAlign: This method utilizes continual cross-lingual instruction tuning to\nadapt LLMs to underrepresentation languages while preserving the model’s capabil-\nity in high-resource languages. InstructAlign employs alignment-based cross-lingual\ninstruction tuning and experience replay to seamlessly align the newly adapted\nlanguage with the pre-trained high-resource languages.\n• Semantic Cross-Lingual In-Context Learning: We introduce a technique that en-\nhances few-shot cross-lingual in-context learning by leveraging semantically similar\ncross-lingual exemplars. This approach improves the model’s understanding of the\n64\ntarget language without requiring task-relevant information in the underrepresented\nlanguage.\n• In-Context Query Alignment: We propose a novel method for cross-lingual align-\nment through in-context learning, which does not require any task-relevant infor-\nmation. This approach enables the model to align the target language with the\nhigh-resource languages by utilizing in-context query alignment.\nBy evaluating these methods on various datasets and languages, we demonstrate their\neffectiveness in improving the performance of LLMs on underrepresented languages while\nmaintaining its task generalization ability. Our work highlights the importance of cross-\nlingual alignment in expanding the language repertoire of LLMs and promoting inclusivity\nand diversity in NLP technology. The following sections will provide a detailed explanation\nof each method, including their mechanisms, experimental results, and analysis. We expect\nthat these approaches will contribute to the development of more inclusive and diverse\nLLMs, making them accessible and effective for a wider range of languages.\nFigure 5.1: Linguistics projection of 4000+ languages across the globe obtained from\nURIEL [237, 255] The number of languages supported by existing multilingual LLMs\n(green region) per language family. Existing multilingual LLMs only support a fraction of\nlanguages around the globe. Most of them are within the Indo-European language family,\nwhile most other language families are underrepresented or even unexplored.\n65\n5.2\nContinual Cross-Lingual Instruction-Tuning\n5.2.1\nOverview\nTo solve the problem of adapting new languages while retaining the task generalization\ncapability of multilingual LLMs, we introduce InstructAlign, a continual instruction tuning\nframework to seamlessly align newly adapted underrepresented languages (L2) with the\npre-trained high-resource languages (L1) of an instruction-tuned LLM through crosslingual\nalignment. InstructAlign compels multilingual LLMs to perform crosslingual alignments\nbetween pre-trained and novel languages through alignment-based crosslingual instruction\ntuning, enabling the model to grasp L2 with only a limited amount of parallel data. To\nfurther prevent catastrophic forgetting, InstructAlign incorporates experience replay [70,\n318], which adds past data during the instruction tuning.\nOur work presents the following contributions:\n• We propose InstructAlign, a crosslingual continual instruction tuning method that\nallows instruction-tuned multilingual LLMs to understand L2 with minimal degrada-\ntion on L1 while retaining their zero-shot prompting capability.\n• We propose cross-lingual alignment through instructions that enables multilingual\nLLMs to align L2 to L1 allowing better L2 acquisition with only a small amount of\nparallel data.\n• We evaluate the effectiveness of InstructAlign on various underrepresented datasets,\nand demonstrate that InstructAlign can signiﬁcantly improve the performance on\nL2 by 5-10% F1 while maintaining the original performance on L1 and its multitask\ngeneralization capability.\n• We analyze the correlation between the performance of L2 and other unseen lan-\nguages (L3), suggesting the zero-shot generalization of InstructAlign to L3 particu-\nlarly when the languages are related. 1\n1We use the terms L1, L2, and L3 to denote the ﬁrst, second, and third language acquisition [151, 152]. In\nour context, L1 denotes the pre-trained languages in LLMs, L2 denotes the newly adapted languages, and\nL3denotes other languages that have not been seen after tuning with InstructAlign, which are only used in\nthe evaluation.\n66\nFigure 5.2: Example of the cross-lingual alignment through instructions, i.e., bilingual\ndenoising (TLM), machine translation (MT), and crosslingual semantic similarity (XSS) in\ncomparison to the monolingual denoising (MLM).\n5.2.2\nMethodology\nInstructAlign is a continual cross-lingual instruction tuning framework that allows the\nmodel to align high-to-low resource languages through instruction tuning. InstructAlign\nintroduces two components, i.e., 1) cross-lingual alignment through instruction tuning,\nwhich allows the model to align pre-trained languages with the new languages through\ncross-lingual alignment, and 2) continual instruction tuning, which applies continual\nlearning into instruction tuning to avoid catastrophic forgetting.\nCross-lingual Alignment through Instruction\nGiven a parallel text pair (x, y) from two languages, the goal of cross-lingual alignment is\nto learn a mapping function f(.) parameterized by ✓such that f(x, ✓) = f(y, ✓). The (x, y)\ntext pair commonly comes in the form of a word pair or a phrase pair [224, 223], but in\ntheory, it should be able to generalize to a sentence pair or even a paragraph. With the\ngoal of aligning two parallel texts from two different languages, InstructAlign deﬁnes a set\nof alignment-based crosslingual instructions by exploiting multiple alignment objectives\nthat can be achieved through a parallel sentence. Speciﬁcally, we explore three different\nobjectives, i.e., bilingual denoising / translation language modeling (TLM), machine\ntranslation (MT) and crosslingual semantic similarity (XSS).\n67\nWe ﬁrst deﬁne a parallel sentence pair (X = {x1, x2, . . . , xm}, Y = {y1, y2, . . . , yn}), where\nxi and yi denote the i-th token of the sentence X and Y, respectively. For bilingual denoising\n(TLM), we model the problem as a conditional denoising task. InstructAlign ﬁrst applies a\nperturbation function gtlm(.) to the target sentence Y that masks out part of the tokens in or-\nder to get ˜Y = gtlm(Y). The pair (X, ˜Y) is then used to generate a prompt using h(X, ˜Y, Ttlm),\nresulting in an input-output data pair for prompting (htlm(X, ˜Y, Ttlm), Y), where htlm(.)\ndenotes a bilingual denoising prompt generator and Ttlm the prompt template.\nFor the machine translation (MT) objective, we deﬁne the input-output data pair as\n(hmt(X, Tmt), Y), where hmt(.) denotes a machine translation prompt generator and Tmt\ndenotes a machine translation prompt template. As for the crosslingual semantic similarity\n(XSS) objective, we models the problem as an inference task to predict whether two parallel\nsentences X and Y are semantically similar. Speciﬁcally, we deﬁne the input-output data\npair as (hxss(X, Y, Txss), l) where hxss(.) is a semantic similarity prompt generator, Txss\ndenotes a semantic similarity prompt template and l the binary label regarding whether\nthe sentences are semantically related or not. The examples of the crosslingual alignment\nobjectives are shown in Figure 5.2.\nContinual Instruction Tuning through Experience Replay\nContinual learning is a paradigm to learn various tasks gradually allowing the model to\nacquire new knowledge over time[100]. Using a naive ﬁne-tuning approach for continual\nlearning causes the model to suffer from catastrophic forgetting (CF) [118]. Therefore,\nvarious methods have been introduced to prevent CF. Regularization-based methods [205,\n242, 21] add a regularization in the loss function to prevent the model to be updated into a\ndirection that causes CF. Replay-based methods [318, 247, 69] add samples from previous\ntasks to be incorporated during learning the new task, which helps regularize the model\nto avoid CF. Parameter isolation methods [22, 348, 256] prevent the model from CF by\nlearning new tasks using a new set of parameters while keeping the other parameters\nfrozen during ﬁne-tuning. In this work, we apply experience replay [318], which is a simple\nreplay-based method by adding tasks from previously learned languages when training\nnew languages without any loss modiﬁcation.\nWithin the continual instruction tuning phase of InstructAlign, experience replay [318]\n68\nDataset\nTask\n#Lang.\n#L1\n#L2\n#L3\n#Test\nNusaX\nSentiment Analysis\n12\n2\n7\n3\n4400\nNusaTranslation\nSentiment Analysis\n12†\n1\n3\n8\n10400\nNusaParagraph\nEmotion Recognition\n10\n0\n4\n6\n5700\nNusaParagraph\nTopic Classiﬁcation\n10\n0\n4\n6\n6250\nTable 5.1: Statistics of all datasets used in the experiments. #Lang. denotes the number\nlanguages in each dataset.\n† We use the aligned samples from the source dataset of\nNusaTranslation, i.e., EmoT [324], for the Indonesian subset of NusaTranslation.\nis employed to minimize the catastrophic forgetting problem. Experience replay works by\nstoring some of the past training data and using them during the optimization step of the\nnew data. These past data serve as a regularization term that prevents the models to forget\npast knowledge when learning from the new data. The past data is collected from the\ninstruction tuning data used when developing the corresponding instruction-tuned model,\nwhich are all supervised. During the continual instruction tuning, InstructAlign takes only\nr randomly sampled data from the past instruction tuning data. The sampled past data is\nused during continual-instruction tuning with a balanced sampling between the past data\nand new data. More formally, we deﬁne a past dataset Dold and a newly generated crosslin-\ngual instruction dataset Dcli. On each optimization step, InstructAlign samples data in an\ninterleaving manner resulting in a batch data B = {sDold\n1\n, sDcli\n1\n, sDold\n2\n, sDcli\n2\n, . . . , sDold\nn\n, sDcli\nn\n}\nwith 2n samples, where sDold\ni\nand sDcli\ni\ndenote a sample that is taken randomly from Dold\nand Dcli, respectively. Since the samples are all supervised, the optimization can be done\nby optimizing the cross-entropy loss [140] from all the samples in the batch.\n5.2.3\nExperiment Setting\nContinual-Instruction Tuning Dataset\nDuring the InstructAlign tuning, we train the model on 7 L2 languages from Malayo-\nPolynesian language family group, i.e., Sundanese (sun), Javanese (jav), Balinese (ban),\nMinangkabau (min), Buginese (bug), Acehnese (ace), and Banjarese (bjn). For the L1\nlanguages, we utilize English (eng), as English covers the majority of the pre-training data\nin most LLMs, and Indonesian (ind), as the language is closely related to the target L2\n69\nlanguages. For the dataset, we utilize FLORES-200 dataset [142, 378] as the source of the\nparallel data where we combine the validation and the test set producing a total of ⇠2000\nparallel sentences for each language pair which is orders of magnitude smaller compared\nthe data size used for language adaptation used in prior works [292, 64, 19, 414].\nModels & Hyperparameters\nWe utilize BLOOMZ [272] as the backbone model. Speciﬁcally, we explore InstructAlign on\ntwo model size, i.e., BLOOMZ-560M and BLOOMZ-1.1B. For InstructAlign, we evaluate\nthree crosslingual alignment objectives, i.e., TLM, XSS, and MT. The list of prompts\nused for instruction tuning is described in Appendix B. We use English prompts in all\nexperiments. We run all experiments with an initial learning rate of 1e-5 with a linear\nlearning rate decay and a batch size of 32 for a ﬁxed optimization step of 50,000. We run\nthe InstructAlign on a single RTX3090 GPU (24GB) using the AdamW optimizer [248] and\nmixed-precision training [264]. We use a ﬁxed number of replay samples r = 100000.\nBaselines\nFor our baselines, we conduct zero-shot prompting using four different sizes of\nBLOOMZ, i.e., BLOOMZ-560M, BLOOMZ-1.1B, and BLOOMZ-3B, without any additional\nlanguage adaptation phase. In addition, to compare the effectiveness of the crosslingual\nalignment, we add continual instruction-tuned baselines that incorporate only monolingual\ndenoising instructions, which is equivalent language adaptation using MLM [102].\nEvaluation Setting\nAfter tuning with InstructAlign, the model is then evaluated in a zero-shot crosslingual\ninference setting, in which the model has never seen the task on the target languages, but\nmight have seen the task on other seen languages. To retrieve the classiﬁcation label, we\ncompute the joint probability of the prompt with each label in the dataset and pick the label\nwhich prompt the highest joint probability. We consider 3 different prompts in English\nfor the zero-shot inference and take the average accuracy and weighted F1 scores as the\nevaluation metrics. The list of the prompts used in our evaluation is shown in Appendix B.\nWe use a single RTX1080Ti GPU (11GB) to run the evaluation for all models. To reduce\n70\nthe memory bottleneck during inference, we run the evaluations using 8-bit inference\nvia LLM.int8() [101]. We provide the performance comparison between 8-bit and 32-bit\nevaluation in Appendix C.\nZero-Shot Evaluation Datasets\nFor evaluating the effectiveness of InstructAlign, we uti-\nlize four multilingual Indonesian local languages datasets, i.e., the sentiment analysis task\nfrom NusaX (NX-S) [400], the sentiment analysis task from NusaTranslation (NT-S) [60],\nthe topic classiﬁcation task from NusaParagraph (NP-T) [60], and the paragraph-level emo-\ntion recognition task from NusaParagraph (NP-E) [60]. The detailed per-dataset statistics\nare shown in Table 5.1. 2 NusaX covers 12 languages including 2 L1 languages: English\n(eng) and Indonesian (ind), 7 L2 languages: Acehnese (ace), Balinese (ban), Buginese\n(bug), Banjarese (ban), Javanese (jav), Minangkabau (min), and Sundanese (sun), and 3 L3\nlanguages: Toba Batak (bbc), Madurese (mad), and Ngaju (nij). While NusaTranslation\ncovers 11 languages, which includes 3 L2 languages: Javanese (jav), Sundanese (sun),\nand Minangkabau (min), and 8 L3 languages: Ambon (abs), Batak (btk), Betawi (bew),\nBima (bhp), Madurese (mad), Makassarese (mak), Musi (mui), and Rejang (rej). NusaPara-\ngraph covers 10 languages, which includes 4 L2 languages: Sundanese, Javanese (jav),\nMinangkabau (min), and, and 6 L3 languages: Batak (btk), Betawi (bew), Madurese (mad),\nMakassarese (mak), Musi (mui), Rejang (rej). To expand the evaluation dataset for L1, we\nadd the Indonesian sentiment analaysis data from IndoLEM [215] 3 as the Indonesian (ind)\nsubset of NT-S. More details about each dataset can be found in Appendix D.\n5.2.4\nExperiment Result\nEffectiveness of InstructAlign\nFigure 5.3 shows the result of InstructAlign on both L1\nand L2 languages on two different scales, i.e., 560M and 1.1B. InstructAlign-tuned models\nwith XSS objective signiﬁcantly outperform the comparable-sized BLOOM and BLOOMZ\nbaselines on L2 languages while retaining a similar performance level as the original\nBLOOMZ models on L1 languages. Comparing between different InstructAlign objectives –\n2We do not use the machine translation tasks provided in both benchmarks as it will violate the zero-shot\ncrosslingual inference constraint within our experiment.\n3The source translation data of NusaTranslation\n71\nFigure 5.3: Average performance of various models across different model scales on the L1\nand L2 languages. InstructAlign improves the understanding of new languages (L2), while\nretaining the understanding of seen languages (L1)\nFigure 5.4: Comparison of different InstructAlign objectives. TLM and XSS objectives show\nimprovement against MLM, while MT does not.\nas shown in Figure 5.4, InstructAlign with TLM and XSS objectives signiﬁcantly outperform\nthe one with MLM objective. Nonetheless, this does not hold for MT objective, suggesting\nthat MT objective is less suitable for cross-lingual learning.\nWe further show the per task comparison of InstructAlign compared to all larger-scale\nLLMs, i.e., BLOOM-3B and BLOOMZ-3B. As shown in Table 5.2, InstructAlign of BLOOMZ-\n1.1B with TLM and XSS objectives are able to outperform even the BLOOMZ-3B models\non L2, while BLOOMZ-560M performs almost on part with BLOOMZ-3B. At the same\ntime, the InstructAlign-adapted models also retain their performance on L1. This indicates\nthat InstructAlign offers 3-5X efﬁciency boost on the adapted languages. Moreover, in\n72\nMethod\nL2 Weighted F1 (%)\nL1 Weighted F1 (%)\nNT-S\nNX-S\nNP-E\nNP-T\nAvg.\nNX-S En\nNX-S Id\nNT-S Id\nAvg.\nBLOOMZ Baseline\nBLOOMZ-560M\n46.83\n33.73\n2.80\n5.35\n22.18\n58.24\n55.59\n69.81\n61.21\nBLOOMZ-1.1B\n64.01\n41.50\n2.80\n5.35\n28.42\n57.41\n58.58\n80.40\n65.46\nBLOOMZ-3B\n69.41\n45.82\n2.80\n5.73\n30.94\n62.65\n63.21\n81.38\n69.08\nInstructAlign-Tuned BLOOMZ-560M\nMLM r=100k\n66.51\n42.51\n2.80\n5.52\n29.34\n60.97\n60.01\n70.93\n63.97\nMT r=100k\n66.42\n41.20\n2.82\n5.40\n28.96\n60.96\n58.09\n64.18\n61.08\nTLM r=100k\n69.24\n42.91\n2.87\n5.43\n30.11\n61.65\n58.52\n72.40\n64.19\nXSS r=100k\n68.10\n45.83\n2.84\n5.53\n30.58\n61.89\n58.22\n71.27\n63.79\nInstructAlign-Tuned BLOOMZ-1.1B\nMLM r=100k\n71.46\n45.73\n2.84\n5.49\n31.38\n61.30\n60.83\n73.25\n65.13\nMT r=100k\n66.15\n44.93\n2.84\n5.40\n29.91\n61.68\n59.18\n65.28\n62.05\nTLM r=100k\n70.29\n49.25\n3.17\n6.34\n32.26\n63.26\n60.54\n74.66\n66.15\nXSS r=100k\n71.89\n49.23\n3.08\n5.81\n32.50\n63.78\n59.34\n75.74\n66.29\nTable 5.2: Evaluation of InstructAlign with BLOOMZ-560M and BLOOMZ-1.1B backbones.\nCompared to BLOOMZ baselines, All InstructAlign-tuned models improve the zero-shot\ncrosslingual performance in L2 while also retaining the performance in L1.\nthe NusaParagraph emotion recognition (NP-E) and topic classiﬁcation (NP-T) tasks, all\nbaselines yield a very low score, suggesting that the ability to solve long text classiﬁcation\ntasks do not emerge on that scale [393]. Interestingly, InstructAlign tuned models indicate\nconsistent improvement, although marginal, on these tasks, demonstrating that an early\nemergence in L2 languages is possible through InstructAlign.\nEffect of Model Scaling\nAs shown in Table 5.2, we also observe that scaling increases\nthe zero-shot performance of BLOOMZ on both L1 and L2 indicating the generalization of\nscaling law of language models [199, 166] to instruction-tuned LLMs. Moreover, applying\nInstructAlign on larger BLOOMZ results in higher overall zero-shot performance on both\nL1 and L2. Speciﬁcally, InstructAlign-tuned models with 1.1 billion parameters yield ⇠2%\nhigher performance compared to the 560 million parameters InstructAlign-tuned models\nand even perform competitively with the original 3 billion parameters BLOOMZ model.\nThis suggests that the scaling law of language models also apply after InstructAlign where\nlarger-sized models tend to perform better compared to their smaller counterpart. Detailed\nexperiment results are described in Appendix E.\n73\nMethod\nL2\nL1\nBaselines\nRandom\n40.28\n30.88\nMajority\n32.34\n21.17\nBLOOMZ-560M\n37.66\n61.21\nSingle Objective\nMonolingual Denoising (MLM)\n36.71\n53.14\nMachine Translation (MT)\n35.43\n47.95\nBilingual Denoising (TLM)\n45.48\n53.28\nCrosslingual Semantic Similarity (XSS)\n44.55\n54.05\nMulti Objectives\nMLM + MT\n40.09\n47.67\nTLM + MT\n42.93\n48.75\nXSS + MT\n43.32\n50.66\nMLM + TLM\n43.46\n53.16\nMLM + XSS\n42.82\n53.90\nTLM + XSS\n45.83\n54.01\nTable 5.3: Averaged Weighted F1-scores from various InstructAlign objectives in the NT-S\nand NX-S datasets. We use BLOOMZ-560M as the backbone.\n5.2.5\nAnalysis and Discussion\nAlignment Objectives\nTo better understand the effectiveness of each alignment objective, we conduct experi-\nments by using a single objective, i.e., monolingual denoising (MLM), machine translation\n(MT), bilingual denoising (TLM) and crosslingual semantic similarity (XSS), as well as\nmulti objectives on various combinations. We also test zero-shot prompting without any\nadditional language adaption phase as a baseline for comparison. Note that continual\ninstruction tuning through experience replay is not applied (r=0) in these experiments\nsince we focused on the effect of alignment objectives.\nAs shown in Table 5.3, BLOOMZ 560M zero-shot performs better than the random\nbaseline on L1 while achieving a lower score on L2, showing that BLOOMZ 560M is\nunable to be directly applied to these L2 languages. For InstructAlign with a single\nobjective, similar to the result from prior work [414], applying the MLM objective decays\nthe performance of the model. Similarly, using MT objective also decreases the performance\nof both L1 and L2. Nevertheless, as shown in Table 5.2, this problem can be mitigated\n74\nby applying continual learning. On the other hand, both TLM and XSS help improve the\nmodel on L2, indicating that these objectives are effective for aligning L1 and L2 languages.\nAdditionally, the performance in L1 languages is also retained the most when using the\nTLM and XSS objectives.\nWhen combining multiple objectives during InstructAlign, we observe the highest\nscore when combining TLM and XSS. Interestingly, adding the MLM and MT objectives\nduring InstructAlign consistently yields a lower score compared to the single TLM and XSS\nobjectives for both L2 and L1 languages. These facts suggest that cross-lingual objectives\nsuch as XSS and TLM, are effective for learning new languages through cross-lingual\ninstruction-tuning with limited data.\nContinual Instruction Tuning\nFigure 5.5: ∆Weighted F1 of InstructAlign tuned BLOOMZ-560M with (left) TLM and\n(right) XSS objectives with different amount of replay samples (r) compared to the original\nBLOOMZ-560M baseline. Negative scores indicate that the model performs worse com-\npared to the baseline.\nIn order to assess the effectiveness of continual instruction tuning through experience\nreplay, we conduct an experiment exploring the effect of different numbers of replay\nsamples r used in continual instruction tuning. Speciﬁcally, we explore 4 settings of r, i.e.,\nr = [0, 1000, 10000, 100000]. Figure 5.5 shows the performance of the InstructAlign tuned\nmodels across different ranges of replay examples r. When using no experience replay\n(r=0), the performance of the pre-trained languages drops signiﬁcantly, and even further,\nthe performances on the novel languages also drop which suggests that the multitask\n75\n(a) Per language Results\n(b) Correlation of Novel (L2) and Unseen (L3)\nFigure 5.6: (Left) per language breakdown and (right) the correlation of ∆Weighted F1\nfrom the InstructAlign-tuned models to the corresponding BLOOMZ backbone models on\nnovel (L2) and unseen (L3) languages. R denotes the Pearson correlation coefﬁcient.\nprompting capability for both of these methods are degraded [414]. When r increases, a\nmuch smaller performance degradation is observed on the L1 languages. Interestingly,\nthe performance on novel languages also improved when r increases which in the end,\nincreases the performance of the model across all languages. These facts demonstrate\nthe importance of the experience replays for avoiding catastrophic forgetting in continual\ninstruction tuning.\nImpact of InstructAlign on L3 Languages\nWe further assess the impact of aligning L2 languages through InstructAlign to other unseen\nIndonesian languages which are within the same language family group (L3). To assess\nthe transferability from the L2 languages to L3 languages, we compute the correlation\ncoefﬁcient between ∆weighted F1 score on the L2 and L3 languages for each model\ncompared to the corresponding baseline, and measure the Pearson’s correlation [313, 117].\nAs shown in Figure 5.6b, the correlation between the performance improvement of L2 and\nL3 languages is high with a Pearson’s correlation coefﬁcient of 0.96. This indicates the\neffectiveness of the InstructAlign approach for not only adapting to L2 languages but also\nto related L3 languages. Nevertheless, the improvement for unseen language (L3) are lower\nthan L2 languages as shown in Figure 5.6a. The improvement also depends on the language\n76\ndistances where performance on Toba Batak (bbc) and Buginese (bug) yield much lower\nscores compares the other languages as these two languages are distantly-related with the\nother languages. This result aligns with the analysis from NusaX [400] which shows that the\nperformances of Buginese (bug) and Toba Batak (bbc) are the lowest for both the multitask\nand zero-shot crosslingual settings due to the relatively low vocabulary overlapping\ncompared to other languages in NusaX. This suggests that by performing, the model\ncan also understand unseen languages that are related to the novel-adapted language,\nindicating the generalization of the crosslingual transfer from pre-trained languages to\nnovel (L2) and unseen (L3) languages.\nCross-Lingual Alignment Quality and Downstream Performance\n(a) L2\n(b) L3\nFigure 5.7: Alignment quality of (left) novel (L2) languages and (right) unseen (L3) lan-\nguages with different Instruct-Align objectives using BLOOM-560M backbone.\nWe measure the alignment quality of Instruct-Align models using the word-level cross-\nlingual retrieval task [131, 130] that performs a similarity matching between the word\nrepresentation from the source language and the target language. We utilize the weak\nalignment variant of the task where the closest target language word is expected to be\nthe translation pair of the corresponding source language word. We utilize the bilingual\nlexicon provided from NusaX [400]4 to conduct the cross-lingual word-level retrieval. To\ngather the word representation, we simply feed the model with the corresponding word\n4https://github.com/IndoNLP/nusax/blob/main/datasets/lexicon\n77\nand take each layer last token hidden states and perform cosine similarity with of the word\nrepresentation from the source language with the word representation from the target\nlanguage. We measure the accuracy@10 as the indicator of the alignment quality.\nAs shown in Figure 5.7, the alignment quality of Instruct-Align models, even with\nthe MLM objective, signiﬁcantly outperform the corresponding BLOOM and BLOOMZ\nbaselines. The model trained with the MLM objective shows lower alignment quality\ncompared to the TLM, XSS, and MT objectives. This indicates the importance of cross-\nlingual alignment objectives for improving the cross-lingual alignment quality. The cross-\nlingual alignment quality correlated with the downstream performance of each objective\nas described in §5.2.4, except for the MT objective. Interestingly, the model trained with the\nMT objective yield a high cross-lingual alignment quality with a much lower downstream\nperformance. We conjecture that this is because every sentence pair in the MT objective\ncan only be a single task sample – unlike TLM and XSS that can produce much more\nsample variations through different perturbations on TLM and negative samples on XSS.\nThis suggests a better data efﬁciency for TLM and XSS objectives in comparison to the\nMT objective. Furthermore, the alignment quality is best on the middle layers which is\nsimilar to the cross-lingual alignment quality analysis between relatively high-resource\nlanguages in Gaschi et. al. (2023) [130]. This result indicates that measuring the cross-\nlingual alignment quality through word-level cross-lingual retrieval is consistent and\ngeneralizable to underrepresented languages within different language families.\n5.2.6\nKey Takeaways\nIn this work, we address the challenge of increasing the language coverage of instruction-\ntuned LLMs by introducing a crosslingual continual instruction tuning method, Instruc-\ntAlign. We demonstrate that InstructAlign allows an instruction-tuned LLM to effectively\nlearn novel languages through alignment-based crosslingual instruction tuning objectives\nwhile retaining the existing multitask and multilingual abilities. Based on our experiment\nresults on four Indonesian local languages datasets, InstructAlign effectively improves the\nunderstanding of novel Indonesian local languages, improving the language understand-\ning performance on novel languages by ⇠5-10% weighted F1 score and also demonstrates a\nbetter forward transfer performance to other unseen Indonesian local languages by a signif-\n78\nicant margin. In addition, we analyze various objectives of InstructAlign and demonstrate\nthe effectiveness of alignment-based crosslingual instruction tuning objectives compared\nto the traditional masked language modeling (MLM) for learning novel languages with a\nlimited amount of data. Our work contributes to the advancement of language adaptation\nmethods for instruction-tuned LLMs, especially for underrepresented languages.\n5.3\nLanguage Adaptation through In-Context Learning\n5.3.1\nOverview\nIn the previous section, we see how InstructAlign enable better language adaptation\nthrough continual cross-lingual instruction-tuning using only a few thousand of parallel\nsamples. Despite the use of small parallel data, the method rely on performing multiple\nsteps of parameter updates which still incurs huge computational budgets, particularly for\nvery large LLMs with hundreds of billion parameters. To mitigate the efﬁciency limitation,\nprior works [401, 236, 350, 421] explore cross-lingual in-context learning (X-ICL) methods,\nan extension from in-context learning (ICL), that allow LLMs to generate better response\nquality in underrepresented languages without the need for parameter tuning. In X-ICL,\nsource language exemplars are incorporated into the input context allowing the model\nto transfer the task understanding capability from the source, commonly high-resource,\nlanguage into the target language query [401, 350]. However, X-ICL still fails to compete\nwith a simple translate-test baseline, prominently for underrepresented languages. A recent\nwork [376] further enhances X-ICL through semantically similar cross-lingual exemplars\nand in-context label alignment5, yielding a large gain over the baselines on relatively\nboth underrepresented and high-resource languages such as French, Spanish, German,\nChinese, and Japanese. Nonetheless, the applicability of this method to low-resource and\nunderrepresented languages is yet to be explored.\nIn this work, we expand upon the concept of cross-lingual semantic similarity and\nin-context alignment, speciﬁcally focusing on underrepresented languages. Our hypothesis\nposits that their effectiveness may be compromised in underrepresented languages due to\n5In Tanwar et. al., (2023)[376], label alignment is referred to as task alignment. In this work, we distinguish\ntwo types of in-context alignments, i.e., in-context query alignment and in-context label alignment.\n79\nthe weak representation of the labels and sentences for the target languages. To test our\nhypothesis, we explore cross-lingual in-context learning (X-ICL) covering 25 underrepre-\nsented languages from various language families and compare them with the performance\nof 7 relatively higher resource languages, including French (fra), Spanish (spa), German\n(deu), Italian (ita), Portuguese (por), Arabic (arb), and Hindi (hin). Our result suggests\nthat the X-ICL performance decays correlate to the size of pre-training data of the target\nlanguages, which aligns with our hypothesis. Moreover, to our surprise, contrary to the\nresults reported in [376], we found that in-context label alignment does not work for all the\nlanguages under study.\nTo this end, we explore two novel variations for X-ICL method, i.e., 1) XSS X-ICL that\noffers more representative samples as the in-context learning exemplars, and 2) in-context\nquery alignment that enables cross-lingual alignment through in-context information\nwithout requiring any parameter update. We extensively analyze all these factors and their\neffect on the downstream task performance of all the languages under study. Our results\nand analysis highlight the following key takeaways:\n• We conduct a comprehensive analysis on different cross-lingual retrieval strategy for\nimproving the X-ICL capability of LLMs, incorporating cross-lingual semantic simi-\nlarity (XSS) and translation semantic similarity (TSS) to improve the understanding\ncapability on underrepresented languages.\n• Contrary to prior work [376], we found that label alignment undermines the perfor-\nmance in most languages. Keeping uniform labels from the high-resource language\noften yields the best results.\n• We introduce in-context query alignment, a novel approach that enables cross-lingual\nalignment via in-context information allowing better zero-shot prediction without\nrequiring any task-speciﬁc data.\n• We showcase the competing performance between the use of ICL and X-ICL for\nimproving LLMs’ understanding of underrepresented languages. Our ﬁndings reveal\nthe efﬁcacy of using in-context query alignment and XSS X-ICL for improving the\nlanguage generalization of LLMs, especially when there is little to no high-quality\ntask-speciﬁc data on the speciﬁed language.\n80\nFigure 5.8: Correlation of monolingual semantic similarity with the correct label for (left)\nLaBSE and (right) Multilingual MPNet sentence embedding models. Ensemble semantic\nrepresentation with word-level features such as bag-of-words and TF-IDF gives the best\nperformance trade-off on both high-resource and underrepresented languages.\n5.3.2\nMethods\nICL is effective for improving the task understanding capability of LLMs [57, 327, 421, 350]\nthrough task-speciﬁc exemplars. When handling low-resource and underrepresented\nlanguages, such task-speciﬁc exemplars might not be available due to the limited resource\nconstraint on the corresponding language. To alleviate this problem, X-ICL is introduced\nto improve the performance on underrepresented languages without the needs of task-\nspeciﬁc exemplar on the corresponding language, but instead using task-speciﬁc exemplars\nfrom other languages — commonly from high-resource languages such as English, French,\netc. In X-ICL, the exemplars are selected from a source language dataset Dsrc that are\nthen concatenated as the in-context information to better understand the task in the target\nlanguage. In this work, we showcase two approaches to improve X-ICL: 1) retrieval-based\nX-ICL and 2) in-context query alignment.\nRetrieval-Based Cross-Lingual In-Context Learning\nSemantic X-ICL\nWe denote a source language dataset as Dsrc = {(esrc\n1 , ysrc\n1 ), . . . , (esrc\nn , ysrc\nn )},\nwhere esrc\n1\nand ysrc\n1\nrespectively denote the input and label of the exemplar, and an in-\nput query qtgt. To gather the exemplars, a retrieval model is incorporated to retrieve\none or more labeled exemplars (esrc\ni , ysrc\ni\n) from Dsrc which are then concatenated with\n81\nthe qtgt during inference to improve the task understanding capability.\nMost prior\nworks [401, 32, 421, 236] incorporate random retrieval during X-ICL which takes ran-\ndom exemplars from Dsrc. We argue that this might result in exemplars that are not\nrelevant to qtgt, reducing the effectiveness of X-ICL. To alleviate this problem, we utilize\ncross-lingual semantic similarity (XSS) based retrievals, dubbed as semantic X-ICL, which\nﬁlter high-resource language exemplars using the underrepresented language query. Simi-\nlar approach has also been explored in Tanwar et. al. (2023) [376], nonetheless, it is also\nevaluated between relatively high-resource languages, such as English, Spanish, French,\nGerman, Chinese, and Japanese. In this work, we expand the exploration of semantic\nX-ICL towards underrepresented languages, shedding light towards the understanding of\nsemantic X-ICL in underrepresented languages.\nTranslation X-ICL\nIn the case of underrepresented languages, we argue that, semantic\nX-ICL might not be optimal as the semantic representation for these languages might not\nbe well aligned with the high-resource languages. Thus, we explore an alternative dubbed\nas translation X-ICL, that performs monolingual semantic similarity between qtgt and a\nsentence in target language Ltgt from a parallel dataset Dpara. We take the most similar\nsentence pair (etgt\ni\n, esrc\ni\n) in Dpara and use the esrc\ni\nto perform another monolingual semantic\nsimilarity to ﬁnd the high-resource exemplars from Dsrc. Although the monolingual\nsemantic similarity between two sentences from an underrepresented language is also\nsuboptimal, as shown in our monolingual textual similarity experiment in Figure 5.8,\nthis problem can be relieved by incorporating word-level features such as TF-IDF and\nbag-of-words. 6. We showcase the overview of semantic semantic X-ICL, and translation\nX-ICL in Figure 5.9.\nTranslate-Test ICL\nGiven the recent trend of massively multilingual PLMs and machine\ntranslation systems [390, 378, 88, 180, 235] with massive language coverage, we also in-\ntroduce a strong baseline for cross-lingual evaluation which utilize a machine translation\nsystem called translate-test ICL. Similar to the regular translate-test baseline [91, 177, 320],\nthis method translate the qtgt into high-resource language query qmt, e.g., English, and\n6The detail of the monolingual textual similarity experiment is shown in Appendix G\n82\nFigure 5.9: We explore two different cross-lingual semantic similarity methods for cross-\nlingual exemplar retrieval in X-ICL, i.e., (left) semantic X-ICL and (right) translation X-ICL.\nperform semantic similarity directly between the translated query and the high-resource\nexemplar dataset Dsrc to ﬁnd the high-resource language exemplars. In §5.3.4, we showcase\nthat translate-test ICL serves as a strong baseline for evaluating X-ICL approaches.\nIn-Context Alignment\nIn-context label alignment\nPrior works showcase the beneﬁt of cross-lingual in-context\nlearning using random exemplars that improves the zero-shot performance of LLMs on\ndownstream tasks [398, 350, 32]. More recently, Tanwar et. al. (2023) [376] introduce\ncross-lingual in-context alignment that injects a label aligner to the prompt in between the\nin-context exemplars and the input query. The label aligner provides the translation of the\nsource label set Csrc = {csrc\n1 , csrc\n2 , . . . , csrc\nk } to the target label set Ctgt = {ctgt\n1 , ctgt\n2 , . . . , ctgt\nk }.\nFor instance, given a target language Ltgt, the label aligner prompt is formatted as fol-\nlow: “In Ltgt, csrc\n1\nmeans ctgt\n1 , csrc\n2\nmeans ctgt\n2 ,. . . , and csrc\nk\nmeans ctgt\nk ”. This\nallows the model to align labels between source and target languages. We call this method\nin-context label alignment. Although cross-lingual in-context alignment has shown im-\nprovements as reported in [376], it introduces distortions to certain aspects of the Bayesian\ninference framework [411, 270] underlying in-context learning. We argue that while the\nin-context label alignment is expected to align the output distribution between the source\nand target labels, it is merely an idealistic assumption, which might not hold in real cases.\nIn-context query alignment\nLLMs are able to perform exact match word and phrase-level\nin-context dictionary lookup [376, 136, 251], allowing them to perform word and phrase-\n83\nCase Study on Artificial Qampuqi Language\nQuery: In Qampuqi language, \"hamham\" means \"eat\", \"goba\" means \"I\", \"ugyyy\" means\n\"you\", \"be\" means \"belonging to\", \"balabala\" means \"ice cream\". Translate the following\nQampuqi sentence to English: ugyyy hamham balabala be goba\nExpected Answer: You eat my ice cream\nLLM Responses\nPhi-3 Mini (3.8B): you eat ice cream belonging to me\nPhi-3 Small (7B): you eat ice cream belonging to me\nLlama-3 (8B): You eat my ice cream.\nPhi-3 Medium (14B): You eat ice cream belonging to me.\nCommand-R (35B): You are eating my ice cream!\nLlama-3 (70B): You eat ice cream belonging to me.\nCommand-R Plus (104B): You eat my ice cream\nTable 5.4: Example of in-context dictionary lookup on unseen language machine translation\ntask across different scale of LLMs.\nlevel mapping allowing a better understanding to an unseen language. We showcase\nthis behaviour using a made up language translation task as shown in Table 5.4. With\nthe continuous representation nature of LLMs, the dictionary lookup in LLMs does not\nonly work for an exact matched pattern, but also for similarly relevant patterns. Such\na behavior has been observed in prior works focusing on knowledge discovery from\nstructured data such as tabular and graph [188, 241, 269] which extends even further to a\nmultihop reasoning through dictionary look up from structured data [233].\nExploiting this capability of LLMs, we develop a novel cross-lingual alignment method\ndubbed as in-context query alignment. In-context query alignment aligns the input query\nqtgt in the target language to the source language by providing the translation of similar\nsentences to the qtgt while keeping the label set as is. To do so, we utilize the parallel exem-\nplar dataset Dpara = {(ssrc\n1 , stgt\n1 ), (ssrc\n2 , stgt\n2 ), . . . , (ssrc\nm , stgt\nm )}, where (ssrc\ni , stgt\ni\n) respectively\ndenotes to a pair of parallel source and target sentences, and select the top-k most similar\nparallel pair by maximizing the monolingual similarity between the qtgt with stgt\ni\n. Given a\ntarget language Ltgt, the parallel pairs are then formatted into an input alignment prompt,\ni.e., “stgt\n1\nmeans ssrc\n1 , stgt\n2\nmeans ssrc\n2 , . . . , and stgt\nk\nmeans ssrc\nk ”. We show the ex-\nample query for in-context label alignment and in-context query alignment in Figure 5.10.\n84\nFigure 5.10: Sample prompt for in-context label alignment and in-context query alignment.\n5.3.3\nExperimental Settings\nRetrieval and In-Context Learning Setup\nTo calculate the cross-lingual and monolingual semantic similarity, we utilize multilingual\nsentence transformers [309, 310].7 For all ICL experiments, we conduct ICL with 3-shot\nICL exemplars. We run our experiments using two LLMs: XGLM-7.5B [236] and BLOOM-\n7B [406]. To select the prediction label, we take the label that maximizes the marginal\nprobability of the prompt:\ncpred = arg max\nc\nP(Xicl, Xalign, qtgt, c)\n(5.1)\n= f(Xicl ⊕Xalign ⊕qtgt ⊕c)\n(5.2)\nwhere f(.) denotes a language model, ⊕denotes the concatenation operator, Xicl denotes\nthe ICL exemplars, Xalign denotes the alignment text, and c denotes the class label taken\nfrom the label set.\n7As our semantic similarity model, we utilize sentence-transformers/stsb-xlm-r-multilingual\n85\nDataset\n#Lang\n#Unseen\nXGLM\n#Unseen\nAya-101\n# Lang\nFamily\nRegion(s)\nDsrc\nDpara\nNusaTranslation\n6\n6\n4\n1\nSoutheast Asia\nNusaX-Senti[400]\nNusaX-MT[400]\nMasakhaNews\n9\n8\n9\n3\nAfrica\nMasakhaNews\n(Eng Train)\nMAFAND[4]\nAmericasNLI\n10\n10\n10\n8\nSouth America\nXNLI (Eng)[91]\nXNLI (Eng) L\nAmericasNLI (Dev)?\nTweet Sentiment\nMultilingual\n7\n2\n0\n2\nNorthern Africa,\nEurope, Central Asia\nTweet Sentiment\nMultilingual (Eng Train)\nTweet Sentiment\nMultilingual (Eng MT)†\nTable 5.5: The datasets and languages under study along with the Dsrc and Dpara. Our\nstudy covers 25 underrepresented languages and 7 relatively higher-resource languages\nfrom various regions. † Translated to English using NLLB [378].? We align the two datasets.\nLanguages and Datasets\nAs shown in Table 5.5, our study includes 25 underrepresented languages from three\ndifferent regions, i.e., Africa, Americas, and South-East Asia, covering 13 language families.\nNote that, many of the underrepresented languages are unseen to both XGLM and BLOOM,\nnonetheless, both models might have seen other languages under the same language\nfamily group with those underrepresented languages, e.g., both models are pre-trained on\nIndonesian, which falls under the same language family group (i.e., Malayo-Polynesian)\nto the underrepresented languages in Indonesia. We also include 7 relatively higher-\nresource languages, i.e., Arabic (arb), French (fra), German (deu), Hindi (hin), Italian\n(ita), Portuguese (por), and Spanish (spa) for comparing the behavior of X-ICL between\nthese relatively higher-resource languages and underrepresented languages. Detailed\ninformation on all the languages under study is shown in Table 5.6.\nAll the languages are spread across four different datasets, i.e., MasakhaNews (topic\nclassiﬁcation) [9], AmericasNLI (natural language inference) [109], NusaTranslation\n(sentiment analysis) [60], and TweetSentimentMultilingual (sentiment analysis) [40].\nFor each dataset, we deﬁned the ICL dataset Dsrc and parallel alignment dataset Dpara\nfrom different dataset subsets or completely different datasets. The details are shown in\nTable 5.5. For the monolingual semantic similarity baselines, we utilize the train and dev\nsets of the evaluation dataset.\n86\nLanguage\nLanguage\nDataset\nTest\nGeographic\nLanguage\nCode\nName\nName\nSize\nRegion\nFamily\nbtk\nBatak\nNusaTranslation\n1200\nSouth-East Asia\nAustronesian\nsun\nSundanese\n1200\nSouth-East Asia\nAustronesian\njav\nJavanese\n1200\nSouth-East Asia\nAustronesian\nmad\nMadurese\n1200\nSouth-East Asia\nAustronesian\nmak\nMakassarese\n1200\nSouth-East Asia\nAustronesian\nmin\nMinangkabau\n1200\nSouth-East Asia\nAustronesian\namh\nAmharic\nMasakhaNews\n376\nAfrica\nAfro-Asiatic\nhau\nHausa\n637\nAfrica\nAfro-Asiatic\nibo\nIgbo\n390\nAfrica\nNiger-Congo\nlug\nLuganda\n223\nAfrica\nNiger-Congo\npcm\nNigerian Pidgin\n305\nAfrica\nEnglish Creole\nsna\nchiShona\n369\nAfrica\nNiger-Congo\nswa\nKiswahili\n476\nAfrica\nNiger-Congo\nxho\nisiXhosa\n297\nAfrica\nNiger-Congo\nyor\nYorùbá\n411\nAfrica\nNiger-Congo\naym\nAymara\nAmericasNLI\n750\nSouth America\nAymaran\nbzd\nBribri\n750\nSouth America\nChibchan\ncni\nAsháninka\n750\nSouth America\nArawak\ngrn\nGuaraní\n750\nSouth America\nTupian\nhch\nWixarika\n750\nSouth America\nUto-Aztecan\nnah\nNahuatl\n738\nSouth America\nUto-Aztecan\noto\nOtomí\n748\nSouth America\nOto-Manguean\nquy\nQuechua\n750\nSouth America\nQuechuan\nshp\nShipibo-Konibo\n750\nSouth America\nPano-Tacanan\ntar\nRaramuri\n750\nSouth America\nUto-Aztecan\narb\nArabic\nTweet Sentiment\nMultilingual\n870\nNorthern Africa\nAfro-Asiatic\nfra\nFrench\n870\nEurope\nIndo-European\ndeu\nGerman\n870\nEurope\nIndo-European\nhin\nHindi\n870\nCentral Asia\nIndo-European\nita\nItalian\n870\nEurope\nIndo-European\npor\nPortuguese\n870\nEurope\nIndo-European\nspa\nSpanish\n870\nEurope\nIndo-European\nTable 5.6: List of languages within the datasets under study.\n5.3.4\nResult and discussion\nRetrieval-Based Cross-Lingual In-Context Learning\nWe compare the effectiveness of random X-ICL, semantic X-ICL, and translation X-ICL.\nBased on Figure 5.11, all X-ICL methods perform better than zero-shot prompting, suggest-\ning the effectiveness of these approaches for improving the task understanding of LLMs.\nSemantic X-ICL consistently outperforms random X-ICL across all higher-resource and\nunderrepresented languages displaying the beneﬁt of semantic-similarity-based even on\nunderrepresented languages. Additionally, translation X-ICL yields the highest improve-\n87\n\u0015\u0013\n\u0015\u0018\n\u0016\u0013\n\u0016\u0018\n\u0017\u0013\n\u0017\u0018\n\u0018\u0013\n=HUR\u00106KRW\n7ZHHW\u00036HQWL\u0011\n0XOWLOLQJXDO\n:HLJKWHG\u0003)\u0014\n\u0019\u0013\n\u0019\u0018\n\u001a\u0013\n;\u0010,&/\u00035DQGRP\n1XVD7UDQVODWLRQ\n\u0015\u0013\n\u0015\u0018\n\u0016\u0013\n\u0016\u0018\n;\u0010,&/\u00036%(57\n$PHULFDV1/,\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n\u001b\u0013\n7\u0010,&/\u00036%(57\u000e\n0DVDNKD1HZV\nFigure 5.11: Performance of different cross-lingual in-context learning retrievals covering\nrandom, and semantic, and translation X-ICL on (1) higher-resource languages, (2) un-\nderrepresented Indonesian languages, (3) underrepresented American languages, and (4)\nunderrepresented African languages.\nment on NusaTranslation data, but provide only marginal improvement on AmericasNLI\nand MasakhaNews and even worsen the performance on Tweet Senti Multilingual. We\nhypothesize that this problem is attributed to the error propagation of the pipelined nature\nof the translation semantic similarity system and the limited coverage of parallel exemplars\nin Dpara, suggesting the beneﬁt of using semantic X-ICL over translation X-ICL.\nImpact of Retrieval Alignment Quality and Cross-Lingual In-Context Learning\nWe\ncompare the effectiveness of varying the cross-lingual semantic similarity models for\ncross-lingual retrieval. As shown in Figure 5.12, the semantic X-ICL performance of all\nsimilarity models outperform the zero-shot baseline by 5-10% and 15-20% weighted F1\nscore for high-resource and underrepresented languages, respectively. This demonstrates\nFigure 5.12: Performance of semantic X-ICL with different semantic similarity models on\n(top) higher-resource languages and (bottom) underrepresented languages.\n88\nFigure 5.13: (left) Sentence-level alignment quality of different semantic similarity models\nunder study on bitext mining accuracy on BUCC and Tatoeba. (right) Their correlation to\nthe X-ICL performance on high-resource (HRL) and underrepresented (UL) languages.\nthe robustness and effectiveness of existing semantic similarity models for performing\nsemantic X-ICL on all supported languages. We further analyze the performance in relation\nto the cross-lingual sentence-level alignment quality.\nWe measure the sentence-level alignment quality using two bitext mining datasets;\n1) BUCC text mining dataset [428] which covers English, French, German, Russian, and\nChinese; and 2) Tatoeba [31] that covers 112 languages, most of them are underrepresented\nlanguages. We compile the sentence-level alignment quality from prior works [309, 310,\n115, 271] and calculate the correlation with the semantic X-ICL performance. The result\nis shown in Figure 5.13. The correlation between high-resource languages with BUCC\naccuracy are rather weak, unlike the correlation between high-resource languages and\nTatoeba accuracy. This signiﬁes that sentence alignment quality from a relatively small\ncoverage of languages might not enough to represent the whole high-resource languages\nunder study. Interestingly, the correlation between underrepresented languages are much\nlower compared to high-resource languages in both BUCC and Tatoeba, implying that\nsentence alignment quality is not a good proxy for estimating semantic X-ICL performance\non underrepresented languages.\nIn-Context Cross-Lingual Alignment\nInferiority of In-Context Label Alignment\nFigure 5.14 shows the comparison of in-\ncontext label alignment with uniform source-only and target-only labels. In most lan-\n89\n=HUR\u00106KRW\n;\u0010,&/\n\u0014\u0013\n\u0015\u0013\n\u0016\u0013\n\u0017\u0013\n\u0018\u0013\n\u0019\u0013\n/DEHO\u0003$OLJQPHQW\n7ZHHW\u00036HQWLPHQW\u00030XOWLOLQJXDO\n:HLJKWHG\u0003)\u0014\n=HUR\u00106KRW\n;\u0010,&/\n\u0013\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n\u001b\u0013\n\u0014\u0013\u0013\n7DUJHW\u0003/DEHO\n0DVDNKD1HZV\n=HUR\u00106KRW\n;\u0010,&/\n\u0014\u0013\n\u0014\u0018\n\u0015\u0013\n\u0015\u0018\n\u0016\u0013\n\u0016\u0018\n\u0017\u0013\n6RXUFH\u0003/DEHO\n$PHULFDV1/,\nFigure 5.14: Performance of XGLM-7.5B with in-context label alignment, target-only label,\nand source-only label on (left) higher-resource, (center) underrepresented African, and\n(right) underrepresented American languages.\nguages, in-context label alignment yields lower performance than target-only label, and\nsource-only label yields the best performance. For underrepresented African languages,\nwhile the performance of source-only label remains high, the target-only label performs\nmuch worse. We conjecture that this is due to the weak representation of these languages,\nwhich is less apparent in underrepresented Indonesian and American languages as the\ntarget labels (see Appendix F) are similar to higher-resource languages in training. Contrary\nto Tanwar et. al. (2023) [376], our results highlight the ineffectiveness of in-context label\nalignment to improve X-ICL on both higher-resource and underrepresented languages.\nIn-Context Query Alignment\nWe introduce in-context query alignment as an alternative\nto in-context label alignment in §5.3.2. To investigate how well in-context alignments can\naffect the understanding of all the languages under study, we analyze their effectiveness by\ncomparing them with the corresponding non-alignment baseline. As shown in Figure 5.15,\nin-context label alignment only improves the performance at ⇠11.54% of the time with an\nimprovement of ⇠5% weighted F1, while the rest 88.46% experiments are decreased by ⇠20%\nweighted F1. In-context query alignment, on the other hand, increases the performance\n56.25% of the time with an improvement of ⇠10% weighted F1, while the rest 43.75% of the\ntime experiences a reduction of ⇠5% weighted F1. Our results suggest that in-context query\nalignment is superior to in-context label alignment, and it improves LLMs’ understanding\nof underrepresented languages in the absence of X-ICL task-speciﬁc data, which leads to\nperformance gain.\n90\n;\u0010,&/\u0003LER\n=6\u0003\\RU\n=6\u0003VXQ\n;\u0010,&/\u0003[KR\n=6\u0003VSD\n=6\u0003DUE\n;\u0010,&/\u0003TX\\\n=6\u0003DPK\n;\u0010,&/\u0003PDN\n;\u0010,&/\u0003IUD\n;\u0010,&/\u0003LWD\n;\u0010,&/\u0003PLQ\n=6\u0003SFP\ní\u0019\u0013\ní\u0017\u0013\ní\u0015\u0013\n\u0013\n/DEHO\u0003$OLJQPHQW\nƩ\u0003:HLJKWHG\u0003)\u0014\n8QGHUSHUIRUPV\n\u001b\u001b\u0011\u0017\u0019\b\nRI\u0003WKH\u0003WLPH\n;\u0010,&/\u0003PDN\n=6\u0003PDG\n;\u0010,&/\u0003VXQ\n;\u0010,&/\u0003PDG\n;\u0010,&/\u0003OXJ\n;\u0010,&/\u0003E]G\n;\u0010,&/\u0003RWR\n;\u0010,&/\u0003GHX\n;\u0010,&/\u0003SRU\n;\u0010,&/\u0003FQL\n=6\u0003DPK\n=6\u0003RWR\n=6\u0003VKS\ní\u0014\u0013\ní\u0018\n\u0013\n\u0018\n\u0014\u0013\n\u0014\u0018\n4XHU\\\u0003$OLJQPHQW\n2XWSHUIRUPV\n\u0018\u0019\u0011\u0015\u0018\b\nRI\u0003WKH\u0003WLPH\nFigure 5.15: ∆Weighted F1 of (left) in-context label alignment and (right) in-context query\nalignment against non-alignment baseline. A score < 0 indicates the in-context alignment\ndegrades the performance.\nCombining Cross-Lingual In-Context Learning and In-Context Query Alignment\nAs shown in Figure 5.16, in-context query alignment yields similar performance with the\nbaseline (i.e., without query alignment) on higher-resource languages while improving\nzero-shot performance on underrepresented languages. Nonetheless, the improvement\nis rather marginal or even worsen the performance in the few-shot X-ICL setting. In this\ncase, we conclude that in-context query alignment can be used as an alternative to X-ICL,\nwhich is favorable when there is no available X-ICL corpus for the particular task. With\nthe recent development of large multilingual parallel corpora, such as Bloom Library [227],\nWikiMatrix [345], CC-Aligned [68, 380, 110], FLORES-200 [378], and GATITOS [190], in-\ncontext query alignment can also be a perfect complement to X-ICL for improving LLMs\nunderstanding on thousands of languages.\nImpact of In-Context Learning on Underrepresented Language Capability\nTo analyze the effectiveness of X-ICL in underrepresented languages, we compare X-ICL\nwith other inference approaches. Speciﬁcally, we compare X-ICL with 3 other baselines: 1)\nmonolingual ICL that performs inference using ICL from the same language as the query, 2)\n91\n\u0015\u0013\n\u0016\u0013\n\u0017\u0013\n\u0018\u0013\n\u0019\u0013\n7ZHHW\u00036HQWL\u0011\n0XOWLOLQJXDO\n1XVD7UDQVODWLRQ\n0DVDNKD1HZV\n$PHULFDV1/,\nZ\u0012\u00034XHU\\\u0003$OLJQ\u0011\nZ\u0012R\u00034XHU\\\u0003$OLJQ\u0011\n=HUR\u00106KRW\n:HLJKWHG\u0003)\u0014\n\u0016\u0013\n\u0016\u0018\n\u0017\u0013\n\u0017\u0018\n\u0018\u0013\n\u0018\u0018\n\u0019\u0013\nZ\u0012\u00034XHU\\\u0003$OLJQ\u0011\nZ\u0012R\u00034XHU\\\u0003$OLJQ\u0011\n;\u0010,&/\n:HLJKWHG\u0003)\u0014\nFigure 5.16: Performance of XGLM-7.5B with and without query alignment on (left)\nhigher-resource, (center) underrepresented African, and (right) underrepresented Ameri-\ncan languages.\ntranslate-test that translates the query and performs zero-shot inference in a high-resource\nlanguage, i.e., English, and 3) translate-test ICL that simply combines translate-test and\nmonolingual ICL. We measure the ∆Weighted F1 against a simple zero-shot prompting\nover all languages under study. For all experiments that include translation, we utilize MT\nmodels from NLLB [378].8\nBased on our experiment results shown in Figure 5.17, the translate-test slightly im-\nproves the performance from the zero-shot baseline in BLOOM and XGLM, while in-\ncontext query alignment only improves zero-shot performance on XGLM. This indicates\nthat alignment information only offers a limited beneﬁt to improving LLMs’ understanding.\nAdditionally, all ICL approaches improve the performance over zero-shot prompting in\nmost cases. All approaches with similarity-based retrieval, i.e., ICL Semantic and X-ICL\nSemantic achieve higher scores than random retrievals, i.e., ICL Random and X-ICL Ran-\ndom, showing the importance of semantic similarity for exemplar retrievals. Interestingly,\nX-ICL Semantic yields a similar performance to ICL Semantic, which utilizes the target\nlanguage exemplars. This indicates X-ICL can be a good alternative for underrepresented\nlanguages as the available data in the speciﬁed underrepresented language are commonly\nvery limited.\nTo conclude, we offer the following suggestions to improve the underrepresented\nlanguage performance during inference: 1) When tackling underrepresented languages,\nit is best to have a high-quality translation system accompanied by a source language\ntask-speciﬁc data for translate-test ICL; 2) When there is no machine translation (MT)\nsystem for the speciﬁed language, it is best to use either ICL or X-ICL depending on the\n8https://huggingface.co/facebook/nllb-200-distilled-1.3B\n92\n(a) Underrepresented languages\n(b) High-resource languages\nFigure 5.17: Gain/Loss of various test-time adaptation methods on (left) underrepresented\nand (right) high-resource languages.\ncorpus availability; 3) When there is a MT system, but no task-speciﬁc data, translate-test\nis still the best option; and 4) When there is no high-quality MT system nor task-speciﬁc\ndata, the best way is to use a parallel data to utilize in-context query-alignment.\nImpact of In-Context Query Alignment on Cultural Understanding\nWe further assess the effectiveness of in-context query alignment on cultural understanding\nby evaluatin Aya-101 [385] model on MABL [196], a multilingual ﬁgure of speech dataset.\nAs shown in Figure 5.18, compared to the zero-shot inference, in-context query alignment\nsigniﬁcantly improves performance on all evaluated languages. In-context query align-\nment improves the performance on underrepresented languages, i.e., Javanese (jav) and\nSundanese (sun), by 8% an 4% accuracy, respectively, while still improving the accuracy\nof a relatively higher resource language, i.e., Indonesia (ind), by ⇠2% accuracy score. Our\nresult signiﬁes the importance of adding in-context information through in-context query\nalignment and, potentially, in-context cross-lingual align for improving both the language\nand cultural understanding capabilities of MLLMs without the needs of parameter updates.\n5.3.5\nKey Takeaways\nIn this work, we systematically investigate the application of X-ICL with MLLMs, focusing\non underrepresented languages. Our comprehensive analysis sheds light on multiple facets\nof X-ICL with MLLMs. Our examination of in-context alignment reveals the limitation\n93\nFigure 5.18: Cultural understanding evaluation of in-context query alignment on a mul-\ntilingual ﬁgure of speech dataset, MABL [196]. In-context query alignment signiﬁcantly\nimproves the performance on all languages under study.\nof label alignment, thus we suggest a more effective alternative: query alignment. Our\nexploration of semantic similarity retrievals underscores the signiﬁcance of employing\ncross-lingual semantic similarity (XSS) in X-ICL. Lastly, we analyze the effectiveness of\nX-ICL in the context of underrepresented languages, highlighting the importance of X-ICL,\nespecially when there is no MT model available for the target language—a circumstance\nprevalent in underrepresented language scenarios. Additionally, our work showcases that\nour methods are not only capable of improving the language understanding quality on\nMLLMs, but also theirs cultural understanding quality, which signiﬁes the importance of\nadding relevant in-context information to alleviate the limited language proﬁciency and\ncultural understanding capability of existing MLLMs.\n5.4\nConclusion\nIn this chapter, we showcase 3 cross-lingual alignment methods that are able to enable better\nunderrepresented language adaptation to multilingual LLMs under different resource\nconstraints. First, we introduce Instruct-Align, a novel approach for continual cross-lingual\ninstruction-tuning which is able to improve 5-10% F1 score on underrepresented languages\nwithout degradation on the high-resource language using only thousands of parallel\nunderrepresented to high resource data. Second, we explore semantic cross-lingual in-\ncontext learning which enables better few-shot cross-lingual in-context learning, improving\n10-15% weighted F1-score compared to zero-shot prompting without the needs of any\n94\ntask-relevant information from the underrepresented languages. Third, we introduce in-\ncontext query alignment, which enables better cross-lingual alignment through in-context\nlearning without the needs of any task-relevant information in any languages. In-context\nquery alignment effectively improves the performance of multilingual LLMs by 3-5%\ncompared to the zero-shot baseline. Furthermore, our methods are not only capable for\nimproving the language understanding quality, but also the cultural understanding quality\nof MLLMs. This signiﬁes the importance of adding relevant in-context information to\nalleviate the limited language proﬁciency and cultural understanding capability of existing\nMLLMs. Moving forward, we anticipate further research into cross-lingual alignment\nalgorithms and methods that can better leverage limited data and training resources to\nbeneﬁt underrepresented languages in multilingual LLMs. Our work pushes the boundary\nof technological advancement while also ensuring that no one is left behind in the exciting\njourney towards the advancements of diversity and inclusivity in multilingual LLMs.\n95\nCHAPTER 6\nConclusion\n6.1\nConcluding Remarks\nIn this thesis, our primary focus lies on providing a rigorous discussion of LLMs general-\nization ability towards underrepresented languages and proposing methods to mitigate\nthe quality gap of underrepresented language understanding in LLMs. Firstly, we conduct\na meticulous evaluation on underrepresented Austronesian languages spoken in Indonesia.\nSubsequently, we introduce data-efﬁcient methods for cross-lingual alignment in LLMs\nthat enable better underrepresented language understanding in LLMs. Lastly, building\nupon the limited understanding about the cultural values embedded in LLMs, we pro-\npose a method for extracting value embedding that represents the cultural and human\nvalues embedded within LLMs. The summaries of our explorations are highlighted in the\nfollowing paragraphs.\nIn section 3, to answer the ﬁrst research question \"Are Multilingual LLMs equally\ninclusive\", we have conducted a comprehensive evaluation of the current state of LLMs in\nmultilingual and multicultural societies, with a focus on languages that are often under-\nrepresented in NLP research. Covering 18 languages spoken in Indonesia, a linguistically\ndiverse country, our evaluations reveal the strengths and limitations of existing LLMs in\nunderstanding and generating content in these languages. Moreover, existing LLMs have\nlimited cultural understanding especially for underrepresented languages. The evalua-\ntions highlight the disparity in performance between high-resource and underrepresented\nlanguages, underscoring the urgent need for improved multilingual and multicultural\ncapabilities in LLMs.\nIn section 4, we address the second research question \"Do Multilingual LLMs represent\ndiverse cultural values?\" and extensively dissect the cultural values embedded in LLMs\nand proposed UniVaR, a novel approach for comparing and aligning cultural values across\ndifferent languages and models. UniVaR enables a better understanding of the complex\n96\ninterplay between LLMs, languages, and cultural values, shedding light on the challenges\nof value alignment in NLP. We also showcase that existing multilingual LLMs can have\ndiverse cultural values across different models and languages depending on the nature of\nthe training corpus and source of value alignment data used to train the multilingual LLMs.\nSpeciﬁcally, we showcase that multilingual LLMs that are trained with more translation-\nheavy corpus shows more similar value across different languages, compared to the LLMs\nthat are trained on natural monolingual corpus. Through our value alignment method and\ncomprehensive evaluations, we contribute to the ongoing efforts to develop inclusive and\nculturally sensitive LLMs, ensuring that these powerful tools are aligned with the values\nof the communities they serve.\nIn section 5, to answer the third research question \"How to improve the inclusivity and\ndiversity of Multilingual LLMs?\", we proposed several cross-lingual alignment methods\nfor language adaptation in LLMs. These methods signiﬁcantly improve the performance of\nLLMs in underrepresented languages without sacriﬁcing the performance in high-resource\nlanguages. By leveraging regional-speciﬁc instruction tuning, continual cross-lingual learn-\ning, and cross-lingual in-context learning, our approaches enable better generalization and\nalignment of LLMs to diverse languages and cultures. We demonstrate that with limited\nresources and careful design, it is possible to develop effective strategies for enhancing the\nlanguage and cultural representation of underrepresented languages in LLMs.\nExpanding the language coverage of LLMs and addressing the challenges in underrep-\nresented languages are crucial steps towards a more inclusive and diverse NLP ecosystem.\nBy ensuring that LLMs can effectively understand and generate content in underrepre-\nsented languages, we can bridge the gap in access to state-of-the-art NLP technology and\nempower individuals and communities worldwide. This research not only contributes\nto advancing the ﬁeld of NLP but also has broader implications for cultural preservation,\nlanguage revitalization, and promoting linguistic diversity. As we continue to improve\nthe multilingual and multicultural capabilities of LLMs, we move closer to a world where\nlanguage barriers are overcome, and all individuals can fully participate in the digital age.\n97\n6.2\nLimitations and Future Work\nWhile this thesis made signiﬁcant strides in evaluating and mitigating the limitations of\nLLMs in underrepresented languages, there are some limitations and avenues for future\nwork to consider. Firstly, the evaluations and language adaptation methods focused pri-\nmarily on Austronesian languages spoken in Indonesia. Further research should extend\nthese evaluations and methods to cover a broader range of underrepresented languages\nfrom various language families and regions. Secondly, although UniVaR provides under-\nstanding of the cultural values in LLMs through value embedding, there is still room for\nimprovement in terms of capturing the nuances and complexities of different cultures other\nthan using languages as the pivot. Future work could explore more sophisticated methods\nfor cultural value alignment and consider the ethical implications of representing diverse\ncultures. Thirdly, the cross-lingual alignment methods introduced in this thesis primarily\nfocused on language understanding tasks. Future research could explore other NLP tasks,\nsuch as machine translation, text summarization, etc, to understand the generalization\ncapabilities of LLMs in underrepresented languages across a wider range of applications.\nLastly, although we have explored the effectiveness of relevant in-context information\nfor improving the language and cultural representation on underrepresented languages\nthrough cross-lingual in-context learning and in-context query alignment, there are various\nother approaches that can be explored to further improve the proposed methods; including\nmethods for better exemplar selection, methods for handling the case when parallel data\nis unavailable or very limited; extension of exploration to speech and other modalities\nfor handling oral languages, etc. It is hoped that this thesis will inspire and provide a\nfoundation for future research to address these limitations and improve the inclusivity and\ndiversity of multilingual LLMs, especially in the case of underrepresented languages.\n98\nBibliography\n[1] Husen Abas. Indonesian as a unifying language of wider communication: a historical and\nsociolinguistic perspective. Number 73 in Paciﬁc Linguistics Series D. The Australian\nNational University, Canberra, 1987.\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-\nrencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[3] Alexander Adelaar. The Austronesian languages of Asia and Madagascar: A his-\ntorical perspective. In Alexander Adelaar and Nikolaus P. Himmelmann, editors,\nThe Austronesian Languages of Asia and Madagascar, chapter 1, pages 1–42. Routledge,\nOxon, 2005.\n[4] David Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid,\nDana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen Gwadabe,\nFreshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong, Michael\nBeukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre Niy-\nongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir,\nBenjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed Ahmed, Mil-\nlicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata\nOuoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire\nMemdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmu-\nmin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and\nSam Manthalu. A few thousand translations go a long way! leveraging pre-trained\nmodels for African news translation. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pages 3053–3070, Seattle, United States, July 2022. Association for\nComputational Linguistics.\n[5] David Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Alabi, Yanke\nMao, Haonan Gao, and En-Shiun Lee. SIB-200: A simple, inclusive, and big evalua-\n99\ntion dataset for topic classiﬁcation in 200+ languages and dialects. In Yvette Graham\nand Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 226–245,\nSt. Julian’s, Malta, March 2024. Association for Computational Linguistics.\n[6] David Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael\nBeukman, Chester Palen-Michel, Constantine Lignos, Jesujoba Alabi, Shamsud-\ndeen Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula,\nRooweither Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda, Happy Buza-\naba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia Taylor, Fa-\ntoumata Kabore, Chris Chinenye Emezue, Anuoluwapo Aremu, Perez Ogayo,\nCatherine Gitau, Edwin Munkoh-Buabeng, Victoire Memdjokam Koagne, Allah-\nsera Auguste Tapo, Tebogo Macucwa, Vukosi Marivate, Mboning Tchiaze Elvis,\nTajuddeen Gwadabe, Tosin Adewumi, Orevaoghene Ahia, Joyce Nakatumba-\nNabende, Neo Lerato Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofe-\ntoluwa Oluwaseun Adeyemi, Gilles Quentin Hacheme, Idris Abdulmumin, Odunayo\nOgundepo, Oreen Yousuf, Tatiana Moteu, and Dietrich Klakow. MasakhaNER 2.0:\nAfrica-centric transfer learning for named entity recognition. In Yoav Goldberg, Zor-\nnitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 4488–4508, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics.\n[7] David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia\nKreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Ri-\njhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H.\nMuhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo,\nAremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie\nYimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo,\nJonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin\nAdewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi,\nChiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde,\nClemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane\n100\nMBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde,\nAbdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou,\nKelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin,\nTendai Marengereke, and Salomey Osei. MasakhaNER: Named entity recognition\nfor African languages. Transactions of the Association for Computational Linguistics,\n9:1116–1131, 2021.\n[8] David Ifeoluwa Adelani, A. Seza Do˘gruöz, André Coneglian, and Atul Kr. Ojha.\nComparing LLM prompting with cross-lingual transfer performance on indigenous\nand low-resource Brazilian languages. In Manuel Mager, Abteen Ebrahimi, Shruti\nRijhwani, Arturo Oncevay, Luis Chiruzzo, Robert Pugh, and Katharina von der\nWense, editors, Proceedings of the 4th Workshop on Natural Language Processing for\nIndigenous Languages of the Americas (AmericasNLP 2024), pages 34–41, Mexico City,\nMexico, June 2024. Association for Computational Linguistics.\n[9] David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Alabi, At-\nnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P.\nDossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, Sana Al-\nazzawi, Blessing Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi, Tunde\nAjayi, Tatiana Moteu, Brian Odhiambo, Abraham Owodunni, Nnaemeka Obiefuna,\nMuhidin Mohamed, Shamsuddeen Hassan Muhammad, Teshome Mulugeta Ababu,\nSaheed Abdullahi Salahudeen, Mesay Gemeda Yigezu, Tajuddeen Gwadabe, Idris\nAbdulmumin, Mahlet Taye, Oluwabusayo Awoyomi, Iyanuoluwa Shode, Tolulope\nAdelani, Habiba Abdulganiyu, Abdul-Hakeem Omotayo, Adetola Adeeko, Abeeb\nAfolabi, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari Ki-\nmotho, Onyekachi Ogbu, Chinedu Mbonu, Chiamaka Chukwuneke, Samuel Fanijo,\nJessica Ojo, Oyinkansola Awosan, Tadesse Kebede, Toadoum Sari Sakayo, Pamela\nNyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Kanda Tshinu,\nUssen Kimanuka, Thina Diko, Siyanda Nxakama, Sinodos Nigusse, Abdulmejid Jo-\nhar, Shaﬁe Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire,\nJules Jules, Ivan Ssenkungu, and Pontus Stenetorp. MasakhaNEWS: News topic\nclassiﬁcation for African languages. In Jong C. Park, Yuki Arase, Baotian Hu, Wei\nLu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi, editors, Proceedings\n101\nof the 13th International Joint Conference on Natural Language Processing and the 3rd\nConference of the Asia-Paciﬁc Chapter of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 144–159, Nusa Dua, Bali, November 2023. Association\nfor Computational Linguistics.\n[10] David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Je-\nsujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula,\nEn-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda,\nGodson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi\nSetaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Has-\nsan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, and Pontus\nStenetorp. Irokobench: A new benchmark for african languages in the age of large\nlanguage models, 2024.\n[11] Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Alham Fikri Aji, Genta Indra\nWinata, and Ayu Purwarianti. Lingualchemy: Fusing typological and geographical\nelements for unseen language generalization. CoRR, abs/2401.06034, 2024.\n[12] Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Genta Indra Winata, Pascale\nFung, and Ayu Purwarianti. IndoRobusta: Towards robustness against diverse\ncode-mixed Indonesian local languages. In Kabir Ahuja, Antonios Anastasopoulos,\nBarun Patra, Graham Neubig, Monojit Choudhury, Sandipan Dandapat, Sunayana\nSitaram, and Vishrav Chaudhary, editors, Proceedings of the First Workshop on Scaling\nUp Multilingual Evaluation, pages 25–34, Online, November 2022. Association for\nComputational Linguistics.\n[13] Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and\nJason Weston. The CRINGE loss: Learning what language not to model. In Anna\nRogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers), pages 8854–8874, Toronto, Canada, July 2023. Association for Computational\nLinguistics.\n[14] Roee Aharoni, Moshe Koppel, and Yoav Goldberg. Automatic detection of machine\ntranslated text and translation quality estimation. In Kristina Toutanova and Hua\n102\nWu, editors, Proceedings of the 52nd Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 289–295, Baltimore, Maryland, June 2014.\nAssociation for Computational Linguistics.\n[15] 01. AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang,\nHeng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang\nLiu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiao-\nhui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang,\nYuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation\nmodels, 2024.\n[16] AI@Meta. Llama 3 model card. 2024.\n[17] Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Ro-\nmadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko\nPrasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder. One country, 700+\nlanguages: NLP challenges for underrepresented languages and dialects in Indonesia.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 7226–7249, Dublin, Ireland, May 2022. Association for Computational\nLinguistics.\n[18] Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow.\nAdapting pre-trained language models to African languages via multilingual adap-\ntive ﬁne-tuning. In Proceedings of the 29th International Conference on Computational\nLinguistics, pages 4336–4349, Gyeongju, Republic of Korea, October 2022. Interna-\ntional Committee on Computational Linguistics.\n[19] Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow.\nAdapting pre-trained language models to African languages via multilingual adap-\ntive ﬁne-tuning. In Proceedings of the 29th International Conference on Computational\nLinguistics, pages 4336–4349, Gyeongju, Republic of Korea, October 2022. Interna-\ntional Committee on Computational Linguistics.\n[20] Hanan Aldarmaki and Mona Diab. Context-aware cross-lingual mapping. In Jill\n103\nBurstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 3906–3911,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[21] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne\nTuytelaars. Memory aware synapses: Learning what (not) to forget. In Computer\nVision – ECCV 2018, pages 144–161. Springer International Publishing, 2018.\n[22] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong\nlearning with a network of experts. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR). IEEE, July 2017.\n[23] Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, and Mona Diab. Inves-\ntigating cultural alignment of large language models. arXiv preprint arXiv:2402.13231,\n2024.\n[24] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,\nRuxandra Cojocaru, Merouane Debbah, Etienne Gofﬁnet, Daniel Heslow, Julien\nLaunay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme\nPenedo. Falcon-40B: an open large language model with state-of-the-art performance.\n2023.\n[25] Kwabena Amponsah-Kaakyire, Daria Pylypenko, Cristina España-Bonet, and Josef\nvan Genabith. Do not rely on relay translations: Multilingual parallel direct Europarl.\nIn Yuri Bizzoni, Elke Teich, Cristina España-Bonet, and Josef van Genabith, editors,\nProceedings for the First Workshop on Modelling Translation: Translatology in the Digital\nAge, pages 1–7, online, May 2021. Association for Computational Linguistics.\n[26] Karl Ronald Anderbeck. Malay dialects of the Batanghari river basin (Jambi, Sumatra).\nSIL International, 2008.\n[27] Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavaš,\nIvan Vuli´c, and Anna Korhonen. MAD-G: Multilingual adapter generation for efﬁ-\ncient cross-lingual transfer. In Findings of the Association for Computational Linguistics:\n104\nEMNLP 2021, pages 4762–4781, Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics.\n[28] Arnav Arora, Lucie-aimée Kaffee, and Isabelle Augenstein. Probing pre-trained\nlanguage models for cross-cultural differences in values. In Sunipa Dev, Vinodkumar\nPrabhakaran, David Adelani, Dirk Hovy, and Luciana Benotti, editors, Proceedings of\nthe First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 114–130,\nDubrovnik, Croatia, May 2023. Association for Computational Linguistics.\n[29] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning principled bilingual\nmappings of word embeddings while preserving monolingual invariance. In Pro-\nceedings of the 2016 conference on empirical methods in natural language processing, pages\n2289–2294, 2016.\n[30] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Bilingual lexicon induction through\nunsupervised machine translation. In Anna Korhonen, David Traum, and Lluís\nMàrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5002–5007, Florence, Italy, July 2019. Association for\nComputational Linguistics.\n[31] Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings\nfor zero-shot cross-lingual transfer and beyond. Transactions of the Association for\nComputational Linguistics, 7:597–610, 2019.\n[32] Akari Asai, Sneha Kudugunta, Xinyan Yu, Terra Blevins, Hila Gonen, Machel Reid,\nYulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. BUFFET: Benchmarking\nlarge language models for few-shot cross-lingual transfer. In Kevin Duh, Helena\nGomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pages 1771–1800, Mexico City, Mexico, June 2024.\nAssociation for Computational Linguistics.\n[33] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,\nabs/1607.06450, 2016.\n105\n[34] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Das-\nSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a\nhelpful and harmless assistant with reinforcement learning from human feedback.\narXiv preprint arXiv:2204.05862, 2022.\n[35] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\nAndy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\nCarol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain,\nDeep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller,\nJeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,\nMichael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma,\nRobert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,\nTom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatﬁeld-Dodds, Ben Mann,\nDario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.\nConstitutional ai: Harmlessness from ai feedback, 2022.\n[36] Mona Baker. Corpus Linguistics and Translation Studies — Implications and Applications,\npage 233. John Benjamins Publishing Company, 1993.\n[37] Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan\nShukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and\nMadian Khabsa. The belebele benchmark: a parallel reading comprehension dataset\nin 122 language variants. arXiv preprint arXiv:2308.16884, 2023.\n[38] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie,\nHoly Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale\nFung. A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning,\nhallucination, and interactivity. In Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu,\nDerry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi, editors, Proceedings of the\n13th International Joint Conference on Natural Language Processing and the 3rd Confer-\nence of the Asia-Paciﬁc Chapter of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 675–718, Nusa Dua, Bali, November 2023. Association for\nComputational Linguistics.\n106\n[39] Yejin Bang, Nayeon Lee, Tiezheng Yu, Leila Khalatbari, Yan Xu, Samuel Cahyawijaya,\nDan Su, Bryan Wilie, Romain Barraud, Elham J. Barezi, Andrea Madotto, Hayden\nKee, and Pascale Fung. Towards answering open-ended ethical quandary questions,\n2023.\n[40] Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. XLM-T: Multi-\nlingual language models in Twitter for sentiment analysis and beyond. In Proceedings\nof the Thirteenth Language Resources and Evaluation Conference, pages 258–266, Marseille,\nFrance, June 2022. European Language Resources Association.\n[41] Anab Maulana Barik, Rahmad Mahendra, and Mirna Adriani. Normalization of\nIndonesian-English code-mixed Twitter data. In Wei Xu, Alan Ritter, Tim Baldwin,\nand Afshin Rahimi, editors, Proceedings of the 5th Workshop on Noisy User-generated\nText (W-NUT 2019), pages 417–424, Hong Kong, China, November 2019. Association\nfor Computational Linguistics.\n[42] Constanze Beierlein, Eldad Davidov, Peter Schmidt, Shalom H Schwartz, and Beatrice\nRammstedt. Testing the discriminant validity of schwartz’portrait value question-\nnaire items–a replication and extension of knoppen and saris (2009). In Survey\nResearch Methods, volume 6, pages 25–36, 2012.\n[43] Peter Bellwood. Prehistory of the Indo-Malaysian Archipelago. University of Hawaii\nPress, Honolulu, 1997.\n[44] Peter Bellwood, Geoffrey Chambers, Malcolm Ross, and Hsiao-chun Hung. Are\n‘cultures’ inherited? Multidisciplinary perspectives on the origins and migrations\nof Austronesian-speaking peoples prior to 1000 BC. In Investigating archaeological\ncultures, pages 321–354. Springer, 2011.\n[45] Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and Marcello Federico. Neural\nversus phrase-based machine translation quality: a case study. In Proceedings of the\n2016 Conference on Empirical Methods in Natural Language Processing, pages 257–267,\n2016.\n[46] Aleksandrs Berdicevskis, Gerlof Bouma, Robin Kurtz, Felix Morger, Joey Öhman,\nYvonne Adesam, Lars Borin, Dana Dannélls, Markus Forsberg, Tim Isbister, Anna\n107\nLindahl, Martin Malmsten, Faton Rekathati, Magnus Sahlgren, Elena Volodina, Love\nBörjeson, Simon Hengchen, and Nina Tahmasebi. Superlim: A Swedish language un-\nderstanding evaluation benchmark. In Houda Bouamor, Juan Pino, and Kalika Bali,\neditors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-\ncessing, pages 8137–8153, Singapore, December 2023. Association for Computational\nLinguistics.\n[47] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky,\nTatsunori Hashimoto, and James Zou. Safety-tuned LLaMAs: Lessons from im-\nproving the safety of large language models that follow instructions. In The Twelfth\nInternational Conference on Learning Representations, 2024.\n[48] Robert Blust. Austronesian etymologies. Oceanic Linguistics, 19:1–181, 1980.\n[49] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching\nword vectors with subword information. Transactions of the Association for Computa-\ntional Linguistics, 5:135–146, 2017.\n[50] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Syd-\nney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brun-\nskill, et al. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258, 2021.\n[51] Rishi Bommasani, Percy Liang, and Tony Lee. Holistic evaluation of language models.\nAnnals of the New York Academy of Sciences, 2022.\n[52] Francis Bond and Timothy Baldwin. Introduction to Japanese Computational Linguistics.\nCSLI Publications, Stanford, USA, 2016.\n[53] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. Massive exploration\nof neural machine translation architectures. In Martha Palmer, Rebecca Hwa, and\nSebastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing, pages 1442–1451, Copenhagen, Denmark, September\n2017. Association for Computational Linguistics.\n108\n[54] Daniel S Brown, Jordan Schneider, Anca Dragan, and Scott Niekum. Value alignment\nveriﬁcation. In International Conference on Machine Learning, pages 1105–1115. PMLR,\n2021.\n[55] Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick\nJelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. A statistical approach\nto machine translation. Computational Linguistics, 16(2):79–85, 1990.\n[56] Peter F Brown, Jennifer C Lai, and Robert L Mercer. Aligning sentences in parallel\ncorpora. In 29th Annual Meeting of the Association for Computational Linguistics, pages\n169–176, 1991.\n[57] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,\nand H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\npages 1877–1901. Curran Associates, Inc., 2020.\n[58] Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wilie,\nFajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vin-\ncentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muham-\nmad Satrio Wicaksono, Ivan Parmonangan, Ika Alﬁna, Ilham Firdausi Putra, Samsul\nRahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie\nSuryani, Rifki Aﬁna Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya,\nMuhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito\nGhifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ich-\nwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin,\nSebastian Ruder, Pascale Fung, Herry Sujaini, Sakriani Sakti, and Ayu Purwarianti.\nNusaCrowd: Open source initiative for Indonesian NLP resources. In Anna Rogers,\nJordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for\n109\nComputational Linguistics: ACL 2023, pages 13745–13818, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics.\n[59] Samuel Cahyawijaya, Holy Lovenia, and Pascale Fung. Llms are few-shot in-context\nlow-resource language learners, 2024.\n[60] Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Dea Adhista, Emmanuel Dave, Sarah\nOktavianti, Salsabil Akbar, Jhonson Lee, Nuur Shadieq, Tjeng Wawan Cenggoro,\nHanung Linuwih, Bryan Wilie, Galih Muridan, Genta Winata, David Moeljadi,\nAlham Fikri Aji, Ayu Purwarianti, and Pascale Fung. NusaWrites: Constructing\nhigh-quality corpora for underrepresented and extremely low-resource languages.\nIn Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti,\nand Adila Alfa Krisnadhi, editors, Proceedings of the 13th International Joint Conference\non Natural Language Processing and the 3rd Conference of the Asia-Paciﬁc Chapter of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 921–945, Nusa\nDua, Bali, November 2023. Association for Computational Linguistics.\n[61] Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Rifki Aﬁna Putri, Emmanuel Dave,\nJhonson Lee, Nuur Shadieq, Wawan Cenggoro, Salsabil Maulana Akbar, Muham-\nmad Ihza Mahendra, Dea Annisayanti Putri, Bryan Wilie, Genta Indra Winata, Al-\nham Fikri Aji, Ayu Purwarianti, and Pascale Fung. Cendol: Open instruction-tuned\ngenerative large language models for indonesian languages, 2024.\n[62] Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, and Pascale Fung.\nInstructAlign: High-and-low resource language alignment via continual crosslingual\ninstruction tuning. In Derry Wijaya, Alham Fikri Aji, Clara Vania, Genta Indra\nWinata, and Ayu Purwarianti, editors, Proceedings of the First Workshop in South East\nAsian Language Processing, pages 55–78, Nusa Dua, Bali, Indonesia, November 2023.\nAssociation for Computational Linguistics.\n[63] Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia, Huan Zhong, MingQian Zhong,\nYuk-Yu Nancy Ip, and Pascale Fung. How long is enough? exploring the optimal\nintervals of long-range clinical note language modeling. In Alberto Lavelli, Eben\nHolderness, Antonio Jimeno Yepes, Anne-Lyse Minard, James Pustejovsky, and Fabio\nRinaldi, editors, Proceedings of the 13th International Workshop on Health Text Mining\n110\nand Information Analysis (LOUHI), pages 160–172, Abu Dhabi, United Arab Emirates\n(Hybrid), December 2022. Association for Computational Linguistics.\n[64] Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong\nLi, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar, Masayu Khodra,\nAyu Purwarianti, and Pascale Fung. IndoNLG: Benchmark and resources for evalu-\nating Indonesian natural language generation. In Marie-Francine Moens, Xuanjing\nHuang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, pages 8875–8898, Online and\nPunta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics.\n[65] Samuel Cahyawijaya, Tiezheng Yu, Zihan Liu, Xiaopu Zhou, Tze Wing Tiffany Mak,\nYuk Yu Nancy Ip, and Pascale Fung. SNP2Vec: Scalable self-supervised pre-training\nfor genome-wide association study. In Dina Demner-Fushman, Kevin Bretonnel Co-\nhen, Sophia Ananiadou, and Junichi Tsujii, editors, Proceedings of the 21st Workshop on\nBiomedical Language Processing, pages 140–154, Dublin, Ireland, May 2022. Association\nfor Computational Linguistics.\n[66] Steven Cao, Nikita Kitaev, and Dan Klein. Multilingual alignment of contextual\nword representations. In International Conference on Learning Representations, 2020.\n[67] Achraf Chalabi and Hany Gerges. Romanized Arabic transliteration. In Kalika Bali,\nMonojit Choudhury, and Yoh Okuno, editors, Proceedings of the Second Workshop on\nAdvances in Text Input Methods, pages 89–96, Mumbai, India, December 2012. The\nCOLING 2012 Organizing Committee.\n[68] Vishrav Chaudhary, Yuqing Tang, Francisco Guzmán, Holger Schwenk, and Philipp\nKoehn. Low-resource corpus ﬁltering using multilingual sentence embeddings. In\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham,\nBarry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Mar-\ntins, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Marco\nTurchi, and Karin Verspoor, editors, Proceedings of the Fourth Conference on Machine\nTranslation (Volume 3: Shared Task Papers, Day 2), pages 261–266, Florence, Italy, August\n2019. Association for Computational Linguistics.\n111\n[69] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.\nEfﬁcient lifelong learning with a-gem. In ICLR, 2019.\n[70] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,\nPuneet K. Dokania, Philip H. S. Torr, and Marc’Aurelio Ranzato. On tiny episodic\nmemories in continual learning, 2019.\n[71] Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, and Pascale Fung.\nSubobject-level image tokenization. arXiv preprint arXiv:2402.14327, 2024.\n[72] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma,\nYifan Yanggong, and Junbo Zhao. Maybe only 0.5% data is needed: A preliminary\nexploration of low training data instruction tuning. arXiv preprint arXiv:2305.09246,\n2023.\n[73] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple\nframework for contrastive learning of visual representations. In Proceedings of the\n37th International Conference on Machine Learning, ICML’20. JMLR.org, 2020.\n[74] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey\nHinton. Big self-supervised models are strong semi-supervised learners. In Proceed-\nings of the 34th International Conference on Neural Information Processing Systems, NIPS\n’20, Red Hook, NY, USA, 2020. Curran Associates Inc.\n[75] Percy Chi Shun Cheung and Pascale Fung. From parallel corpora to non-parallel\ncorpora, methods for bilingual sentence extraction. In LREC Workshop on Usage of\nParallel Corpora, Lisbon, May 2004, 2004.\n[76] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang.\nCross-lingual natural language generation via pre-training. In Proceedings of the AAAI\nconference on artiﬁcial intelligence, volume 34, pages 7570–7577, 2020.\n[77] Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey,\nMichael Thompson, James Bost, Javier Tejedor-Sojo, and Jimeng Sun. Multi-layer\nrepresentation learning for medical concepts. In proceedings of the 22nd ACM SIGKDD\ninternational conference on knowledge discovery and data mining, pages 1495–1504, 2016.\n112\n[78] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, Parker Schuh, Kensen Shi, Sashank Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie\nPellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele\nCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and\nNoah Fiedel. Palm: scaling language modeling with pathways. J. Mach. Learn. Res.,\n24(1), mar 2024.\n[79] Gobinda G. Chowdhury. Natural language processing. Annual Review of Information\nScience and Technology, 37(1):51–89, January 2003.\n[80] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\nAmodei. Deep reinforcement learning from human preferences. In I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n2017.\n[81] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nYunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling\ninstruction-ﬁnetuned language models. Journal of Machine Learning Research, 25(70):1–\n53, 2024.\n[82] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical\nevaluation of gated recurrent neural networks on sequence modeling. In NIPS 2014\nWorkshop on Deep Learning, December 2014, 2014.\n[83] Willy Chung, Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia, and Pascale Fung.\n113\nInstructTODS: Large language models for end-to-end task-oriented dialogue systems.\nIn Kehai Chen and Lun-Wei Ku, editors, Proceedings of the Second Workshop on Natural\nLanguage Interfaces, pages 1–21, Bali, Indonesia, November 2023. Association for\nComputational Linguistics.\n[84] Kenneth Church. Char_align: A program for aligning parallel texts at the character\nlevel. In 31st Annual Meeting of the Association for Computational Linguistics, pages 1–8,\n1993.\n[85] Kenneth Church, Ido Dagan, William Gale, Pascale Fung, Jon Helfman, and Bala\nSatish. Aligning parallel texts: Do methods developed for english-french generalize\nto asian languages? In Proceedings of PACFoCoL I (1993): Paciﬁc Asia Conference on\nFormal and Computational Linguistics, pages 1–12. Waseda University, 1993.\n[86] Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski,\nVitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: A benchmark for information-\nseeking question answering in ty pologically di verse languages. Transactions of the\nAssociation for Computational Linguistics, 8:454–470, 2020.\n[87] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra:\nPre-training text encoders as discriminators rather than generators. arXiv preprint\narXiv:2003.10555, 2020.\n[88] Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli,\nDavid Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis,\nHady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma,\nChristopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan\nMavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan,\nTuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernan-\ndez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom\nKozhevnikov, Gabriel Mejia, Robin San Roman, Christophe Touret, Corinne Wong,\nCarleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R.\nCosta-jussà, Maha Elbayad, Hongyu Gong, Francisco Guzmán, Kevin Heffernan,\nSomya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin,\nJuan Pino, Sravya Popuri, Christophe Ropers, Saﬁyyah Saleem, Holger Schwenk,\n114\nAnna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, and Mary\nWilliamson. Seamless: Multilingual expressive and streaming speech translation.\n2023.\n[89] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\npages 8440–8451, Online, July 2020. Association for Computational Linguistics.\n[90] Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining.\nCurran Associates Inc., Red Hook, NY, USA, 2019.\n[91] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bow-\nman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence\nrepresentations. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, 2018.\n[92] J. Cornﬁeld. Bayes theorem. Revue de l’Institut International de Statistique / Review of\nthe International Statistical Institute, 35(1):34, 1967.\n[93] Alexander R. Coupe and František Kratochvíl. Asia before English. In Kingsley\nBolton, Werner Botha, and Andy Kirkpatrick, editors, The Handbook of Asian Englishes,\npages 15–48. John Wiley & Sons, Inc., 2020.\n[94] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube\nrecommendations. In Proceedings of the 10th ACM conference on recommender systems,\npages 191–198, 2016.\n[95] David R Cox. The regression analysis of binary sequences. Journal of the Royal\nStatistical Society: Series B (Methodological), 20(2):215–232, 1958.\n[96] Jan Christian Blaise Cruz and Charibeth Cheng. Evaluating language model ﬁnetun-\ning techniques for low-resource languages. arXiv preprint arXiv:1907.00409, 2019.\n[97] Jan Christian Blaise Cruz and Charibeth Cheng. Establishing baselines for text\nclassiﬁcation in low-resource languages. arXiv preprint arXiv:2005.02068, 2020.\n115\n[98] David Crystal. English as a global language. Cambridge university press, 2003.\n[99] Kees De Bot and Robert Schreuder. Word production and the bilingual lexicon. The\nbilingual lexicon, 191:214, 1993.\n[100] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis,\nGreg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for-\ngetting in classiﬁcation tasks. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, pages 1–1, 2021.\n[101] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit\nmatrix multiplication for transformers at scale, 2022.\n[102] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-\ntraining of deep bidirectional transformers for language understanding.\nIn Jill\nBurstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[103] A. Seza Do˘gruöz, Sunayana Sitaram, Barbara E. Bullock, and Almeida Jacqueline\nToribio. A survey of code-switching: Linguistic and social perspectives for language\ntechnologies. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pages 1654–1666, Online, August 2021. Association for Computational\nLinguistics.\n[104] Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei,\nAbigail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, and Chris\nEmezue. AfroLM: A self-active learning-based multilingual pretrained language\nmodel for 23 African languages. In Angela Fan, Iryna Gurevych, Yufang Hou,\nZornitsa Kozareva, Sasha Luccioni, Naﬁse Sadat Moosavi, Sujith Ravi, Gyuwan Kim,\nRoy Schwartz, and Andreas Rücklé, editors, Proceedings of The Third Workshop on\nSimple and Efﬁcient Natural Language Processing (SustaiNLP), pages 52–64, Abu Dhabi,\n116\nUnited Arab Emirates (Hybrid), December 2022. Association for Computational\nLinguistics.\n[105] Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, and Min Lin.\nSailor: Open language models for south-east asia, 2024.\n[106] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie\nTang. Glm: General language model pretraining with autoregressive blank inﬁlling.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 320–335, 2022.\n[107] Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell,\nAnton Bakhtin, Carol Chen, Zac Hatﬁeld-Dodds, Danny Hernandez, Nicholas Joseph,\net al. Towards measuring the representation of subjective global opinions in language\nmodels. arXiv preprint arXiv:2306.16388, 2023.\n[108] David M. Eberhard, Gary F. Simons, and Charles D. Fennig. Ethnologue: Languages of\nthe World. Twenty-fourth edition. Dallas, Texas: SIL International., 2021.\n[109] Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis\nChiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette Rios, Ivan Vladimir\nMeza Ruiz, Gustavo Giménez-Lugo, Elisabeth Mager, Graham Neubig, Alexis Palmer,\nRolando Coto-Solano, Thang Vu, and Katharina Kann. AmericasNLI: Evaluating\nzero-shot natural language understanding of pretrained multilingual models in truly\nlow-resource languages. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 6279–6299, Dublin, Ireland,\nMay 2022. Association for Computational Linguistics.\n[110] Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, and Philipp Koehn.\nCCAligned: A massive collection of cross-lingual web-document pairs. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP\n2020), pages 5960–5969, Online, November 2020. Association for Computational\nLinguistics.\n[111] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.\n117\n[112] Leanne M Evans, Crystasany R Turner, and Kelly R Allen. \" good teachers\" with\"\ngood intentions\": Misappropriations of culturally responsive pedagogy. Journal of\nUrban Learning, Teaching, and Research, 15(1):51–73, 2020.\n[113] Andri Imam Fauzi and Dwi Puspitorini. Dialect and identity: A case study of\nJavanese use in WhatsApp and Line. In IOP Conference Series: Earth and Environmental\nScience, volume 175, page 012111. IOP Publishing, 2018.\n[114] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to\ntrillion parameter models with simple and efﬁcient sparsity, 2021.\n[115] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.\nLanguage-agnostic BERT sentence embedding. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers), pages 878–891, Dublin,\nIreland, May 2022. Association for Computational Linguistics.\n[116] Robert Darrell Firmage. A Prolegomenon to Theory of Translation. PhD thesis, The\nUniversity of Utah, 1986.\n[117] David Freedman, Robert Pisani, and Roger Purves. Statistics (international student\nedition). Pisani, R. Purves, 4th edn. WW Norton & Company, New York, 2007.\n[118] R French. Catastrophic forgetting in connectionist networks. Trends in Cognitive\nSciences, 3(4):128–135, April 1999.\n[119] Pascale Fung. A pattern matching method for ﬁnding noun and proper noun transla-\ntions from noisy parallel corpora. arXiv preprint cmp-lg/9505016, 1995.\n[120] Pascale Fung. Finding terminology translations from non-parallel corpora. In Fifth\nWorkshop on Very Large Corpora, 1997.\n[121] Pascale Fung. A statistical view on bilingual lexicon extraction, page 219–236. Springer\nNetherlands, 2000.\n[122] Pascale Fung and Percy Cheung. Multi-level bootstrapping for extracting parallel\nsentences from a quasi-comparable corpus. In COLING 2004: Proceedings of the 20th\nInternational Conference on Computational Linguistics, pages 1051–1057, 2004.\n118\n[123] Pascale Fung and Kenneth Ward Church. K-vec: A new approach for aligning parallel\ntexts. In COLING 1994 Volume 2: The 15th International Conference on Computational\nLinguistics, 1994.\n[124] Pascale Fung and Kathleen Mckeown. Aligning noisy parallel corpora across lan-\nguage groups: Word pair feature matching by dynamic time warping. In Proceedings\nof the First Conference of the Association for Machine Translation in the Americas, 1994.\n[125] Pascale Fung and Kathleen McKeown. A technical word-and term-translation aid\nusing noisy parallel corpora across language groups. Machine translation, 12:53–87,\n1997.\n[126] Pascale Fung and Lo Yuen Yee. An ir approach for translating new words from non-\nparallel, comparable texts. In 36th Annual Meeting of the Association for Computational\nLinguistics and 17th International Conference on Computational Linguistics, Volume 1,\npages 414–420, 1998.\n[127] William A. Gale and Kenneth W. Church. A program for aligning sentences in\nbilingual corpora. Computational Linguistics, 19(1):75–102, 1993.\n[128] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav\nKadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red\nteaming language models to reduce harms: Methods, scaling behaviors, and lessons\nlearned. arXiv preprint arXiv:2209.07858, 2022.\n[129] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of\nsentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing. Association for Computational Linguistics, 2021.\n[130] Felix Gaschi, Patricio Cerda, Parisa Rastin, and Yannick Toussaint. Exploring the\nrelationship between alignment and cross-lingual transfer in multilingual transform-\ners. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings\nof the Association for Computational Linguistics: ACL 2023, pages 3020–3042, Toronto,\nCanada, July 2023. Association for Computational Linguistics.\n119\n[131] Félix Gaschi, François Plesse, Parisa Rastin, and Yannick Toussaint. Multilingual\ntransformer encoders: a word-level task-agnostic evaluation. In 2022 international\njoint conference on neural networks (IJCNN), pages 1–8. IEEE, 2022.\n[132] Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Am-\nmanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu,\nMiruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus,\nOndˇrej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori\nHashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly,\nMihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati\nMahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Mar-\ntins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem,\nShashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur\nParikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anas-\ntasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant\nSubramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM bench-\nmark: Natural language generation, its evaluation and metrics. In Antoine Bosselut,\nEsin Durmus, Varun Prashant Gangal, Sebastian Gehrmann, Yacine Jernite, Laura\nPerez-Beltrachini, Samira Shaikh, and Wei Xu, editors, Proceedings of the 1st Workshop\non Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120,\nOnline, August 2021. Association for Computational Linguistics.\n[133] Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexan-\ndros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets, Ashish\nUpadhyay, Bingsheng Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin You, Craig\nThomson, Cristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi Xiong, Di Jin, Dim-\nitra Gkatzia, Dragomir Radev, Elizabeth Clark, Esin Durmus, Faisal Ladhak, Filip\nGinter, Genta Indra Winata, Hendrik Strobelt, Hiroaki Hayashi, Jekaterina Novikova,\nJenna Kanerva, Jenny Chim, Jiawei Zhou, Jordan Clive, Joshua Maynez, João Sedoc,\nJuraj Juraska, Kaustubh Dhole, Khyathi Raghavi Chandu, Laura Perez-Beltrachini,\nLeonardo F. R. Ribeiro, Lewis Tunstall, Li Zhang, Mahima Pushkarna, Mathias\nCreutz, Michael White, Mihir Sanjay Kale, Moussa Kamal Eddine, Nico Daheim,\n120\nNishant Subramani, Ondrej Dusek, Paul Pu Liang, Pawan Sasanka Ammanamanchi,\nQi Zhu, Ratish Puduppully, Reno Kriz, Rifat Shahriyar, Ronald Cardenas, Saad\nMahamood, Salomey Osei, Samuel Cahyawijaya, Sanja Štajner, Sebastien Montella,\nShailza, Shailza Jolly, Simon Mille, Tahmid Hasan, Tianhao Shen, Tosin Adewumi,\nVikas Raunak, Vipul Raheja, Vitaly Nikolaev, Vivian Tsai, Yacine Jernite, Ying Xu,\nYisi Sang, Yixin Liu, and Yufang Hou. Gemv2: Multilingual nlg benchmarking in a\nsingle line of code, 2022.\n[134] Martin Gellerstam. Translationese in swedish novels translated from english. 1986.\n[135] Martin Gellerstam. Translations as a source for cross-linguistic studies. Lund studies\nin English, 88:53–62, 1996.\n[136] Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer. Dictionary-based phrase-\nlevel prompting of large language models for machine translation, 2023.\n[137] Amelia Glaese, Nat McAleese, Maja Tr˛ebacz, John Aslanides, Vlad Firoiu, Timo\nEwalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al.\nImproving alignment of dialogue agents via targeted human judgements. arXiv\npreprint arXiv:2209.14375, 2022.\n[138] Goran Glavaš, Robert Litschko, Sebastian Ruder, and Ivan Vuli´c. How to (properly)\nevaluate cross-lingual word embeddings: On strong baselines, comparative analyses,\nand some misconceptions. In Anna Korhonen, David Traum, and Lluís Màrquez,\neditors, Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 710–721, Florence, Italy, July 2019. Association for Computational\nLinguistics.\n[139] Sergei Glotov. Intercultural ﬁlm literacy education against cultural misrepresenta-\ntion: Finnish visual art teachers’ perspectives. Journal of Media Literacy Education,\n15(1):31–43, April 2023.\n[140] I. J. Good. Rational decisions. Journal of the Royal Statistical Society. Series B (Method-\nological), 14(1):107–114, 1952.\n121\n[141] Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau.\nLarger-scale transformers for multilingual masked language modeling.\nCoRR,\nabs/2105.00572, 2021.\n[142] Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,\nDa Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.\nThe ﬂores-101 evaluation benchmark for low-resource and multilingual machine\ntranslation. 2021.\n[143] Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,\nDa Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.\nThe ﬂores-101 evaluation benchmark for low-resource and multilingual machine\ntranslation. Transactions of the Association for Computational Linguistics, 10:522–538,\n2022.\n[144] Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of\nembeddings with wasserstein procrustes. In The 22nd International Conference on\nArtiﬁcial Intelligence and Statistics, pages 1880–1890. PMLR, 2019.\n[145] Russell D. Gray and Fiona M. Jordan. Language trees support the express-train\nsequence of Austronesian expansion. Nature, 405:1052–1055, 2000.\n[146] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond,\nElena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Moham-\nmad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal\nValko. Bootstrap your own latent - a new approach to self-supervised learning. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 21271–21284. Curran\nAssociates, Inc., 2020.\n[147] Huan Gui, Jialu Liu, Fangbo Tao, Meng Jiang, Brandon Norick, and Jiawei Han.\nLarge-scale embedding learning in heterogeneous event data. In 2016 IEEE 16th\nInternational Conference on Data Mining (ICDM), pages 907–912. IEEE, 2016.\n[148] Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp\nKoehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato. The FLORES evaluation\n122\ndatasets for low-resource machine translation: Nepali–English and Sinhala–English.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 6098–6111, Hong Kong, China, November 2019. Association for\nComputational Linguistics.\n[149] Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya\nKizilova, Jaime Diez-Medrano, Marta Lagos, Pippa Norris, Eduard Ponarin, and\nBi Puranen. World values survey time-series (1981-2022) cross-national data-set,\n2022.\n[150] Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya\nKizilova, Jaime Diez-Medrano, Marta Lagos, Pippa Norris, Eduard Ponarin, and\nBi Puranen. World values survey wave 7 (2017-2022) cross-national data-set, 2022.\n[151] Björn Hammarberg. Chapter 2. roles of l1 and l2 in l3 production and acquisition.\nIn Cross-Linguistic Inﬂuence in Third Language Acquisition, pages 21–41. Multilingual\nMatters, December 2001.\n[152] Björn Hammarberg. 1. problems in deﬁning the concepts of l1, l2 and l3. In Teaching\nand Learning in Multilingual Contexts, pages 3–18. Multilingual Matters, December\n2014.\n[153] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology\nof conversational ai: Converging evidence on chatgpt’s pro-environmental, left-\nlibertarian orientation. arXiv preprint arXiv:2301.01768, 2023.\n[154] Akhmad Haryono. Perubahan dan perkembangan bahasa: Tinjauan historis dan sosiolin-\nguistik. PhD thesis, Udayana University, 2012.\n[155] Shreya Havaldar, Bhumika Singhal, Sunny Rai, Langchen Liu, Sharath Chandra Gun-\ntuku, and Lyle Ungar. Multilingual language models are not multicultural: A case\nstudy in emotion. In Jeremy Barnes, Orphée De Clercq, and Roman Klinger, editors,\nProceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment,\n& Social Media Analysis, pages 202–214, Toronto, Canada, July 2023. Association for\nComputational Linguistics.\n123\n[156] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum\ncontrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages 9726–9735, 2020.\n[157] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta\nusing electra-style pre-training with gradient-disentangled embedding sharing, 2021.\n[158] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-\nenhanced bert with disentangled attention. In International Conference on Learning\nRepresentations, 2021.\n[159] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.\nNeural collaborative ﬁltering. In Proceedings of the 26th international conference on\nworld wide web, pages 173–182, 2017.\n[160] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song,\nand Jacob Steinhardt.\nAligning ai with shared human values.\narXiv preprint\narXiv:2008.02275, 2020.\n[161] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. Measuring massive multitask language understanding.\nIn International Conference on Learning Representations, 2020.\n[162] Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Ab-\ndou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis,\nRuixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Sø-\ngaard. Challenges and strategies in cross-cultural NLP. In Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997–7013,\nDublin, Ireland, May 2022. Association for Computational Linguistics.\n[163] GE Hinton, JL McClelland, and DE Rumelhart. Distributed representations. In Parallel\ndistributed processing: explorations in the microstructure of cognition, vol. 1: foundations,\npages 77–109. 1986.\n[164] Geoffrey E Hinton. Distributed representations. 1984.\n124\n[165] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural compu-\ntation, 9(8):1735–1780, 1997.\n[166] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl,\nAidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of\ncompute-optimal large language model training. In Alice H. Oh, Alekh Agarwal,\nDanielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information\nProcessing Systems, 2022.\n[167] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl,\nAidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driess-\nche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen,\nOriol Vinyals, Jack W. Rae, and Laurent Sifre. Training compute-optimal large lan-\nguage models. In Proceedings of the 36th International Conference on Neural Information\nProcessing Systems, NIPS ’22, Red Hook, NY, USA, 2024. Curran Associates Inc.\n[168] Geert Hofstede. Culture’s consequences: Comparing values, behaviors, institutions and\norganizations across nations. Sage publications, 2001.\n[169] Geert Hofstede. Dimensionalizing cultures: The hofstede model in context. Online\nreadings in psychology and culture, 2(1):8, 2011.\n[170] Geert Hofstede, Gert Jan Hofstede, and Michael Minkov. Cultures and organizations:\nSoftware of the mind, volume 2. Mcgraw-hill New York, 2005.\n[171] Geert Hofstede and Michael Minkov. Vsm 2013. Values survey module, 2013.\n[172] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of\nneural text degeneration. In International Conference on Learning Representations, 2020.\n[173] Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, and Rui Yan. Cy-\nclealign: Iterative distillation from black-box llm to white-box models for better\nhuman alignment. arXiv preprint arXiv:2310.16271, 2023.\n125\n[174] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions:\nTuning language models with (almost) no human labor. In Anna Rogers, Jordan\nBoyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 14409–14428,\nToronto, Canada, July 2023. Association for Computational Linguistics.\n[175] Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard.\nIn The Twelfth International Conference on Learning Representations, 2024.\n[176] Robert J House, Paul J Hanges, Mansour Javidan, Peter W Dorfman, and Vipin Gupta.\nCulture, leadership, and organizations: The GLOBE study of 62 societies. Sage publications,\n2004.\n[177] Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and\nMelvin Johnson. XTREME: A massively multilingual multi-task benchmark for\nevaluating cross-lingual generalisation. In Hal Daumé III and Aarti Singh, editors,\nProceedings of the 37th International Conference on Machine Learning, volume 119 of\nProceedings of Machine Learning Research, pages 4411–4421. PMLR, 13–18 Jul 2020.\n[178] Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and Ruslan Mitkov. Identiﬁcation of\nTranslationese: A Machine Learning Approach, page 503–511. Springer Berlin Heidelberg,\n2010.\n[179] Arﬁnda Ilmania, Abdurrahman, Samuel Cahyawijaya, and Ayu Purwarianti. Aspect\ndetection and sentiment classiﬁcation using deep neural network for indonesian\naspect-based sentiment analysis. In 2018 International Conference on Asian Language\nProcessing (IALP), pages 62–67, 2018.\n[180] Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud\nJalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André Martins, François\nYvon, and Hinrich Schütze. Glot500: Scaling multilingual corpora and language\nmodels to 500 languages. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1082–1117, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics.\n126\n[181] Ronald Inglehart. Human beliefs and values: A cross-cultural sourcebook based on the\n1999-2002 values surveys. Siglo XXI, 2004.\n[182] Ronald Inglehart. Mapping global values. Comparative sociology, 5(2-3):115–136, 2006.\n[183] Ronald Inglehart, Miguel Basanez, Jaime Diez-Medrano, Loek Halman, and Ruud\nLuijkx. World values surveys and european values surveys, 1981-1984, 1990-1993,\nand 1995-1997. Ann Arbor-Michigan, Institute for Social Research, ICPSR version, 2000.\n[184] J. Iranzo-Sánchez, J. A. Silvestre-Cerdà, J. Jorge, N. Roselló, A. Giménez, A. Sanchis,\nJ. Civera, and A. Juan. Europarl-st: A multilingual corpus for speech translation\nof parliamentary debates. In ICASSP 2020 - 2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 8229–8233, 2020.\n[185] Ann Irvine and Chris Callison-Burch. A comprehensive analysis of bilingual lexicon\ninduction. Computational Linguistics, 43(2):273–310, June 2017.\n[186] Mansour Javidan and Ali Dastmalchian. Managerial implications of the globe project:\nA study of 62 societies. Asia Paciﬁc Journal of Human Resources, 47(1):41–58, 2009.\n[187] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\nHanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,\n2024.\n[188] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. Struct-\nGPT: A general framework for large language model to reason over structured data.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Processing, pages 9237–9251, Singapore,\nDecember 2023. Association for Computational Linguistics.\n[189] Ricky Chandra Johanes, Rahmad Mahendra, and Brahmastro Kresnaraman. Struc-\nturing code-switched product titles in Indonesian e-commerce platform. In Sriram\nChellappan, Kim-Kwang Raymond Choo, and NhatHai Phan, editors, Computa-\ntional Data and Social Networks, pages 217–227, Cham, 2020. Springer International\nPublishing.\n127\n[190] Alex Jones, Isaac Caswell, Ishank Saxena, and Orhan Firat. Bilex rx: Lexical data\naugmentation for massively multilingual machine translation, 2023.\n[191] Karen Sparck Jones. Natural Language Processing: A Historical Review, page 3–16.\nSpringer Netherlands, 1994.\n[192] Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave.\nLoss in translation: Learning bilingual word mapping with a retrieval criterion. In\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2979–\n2984, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics.\n[193] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and\nTomas Mikolov. Fasttext.zip: Compressing text classiﬁcation models. arXiv preprint\narXiv:1612.03651, 2016.\n[194] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks\nfor efﬁcient text classiﬁcation. In Proceedings of the 15th Conference of the European\nChapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages\n427–431. Association for Computational Linguistics, April 2017.\n[195] Karthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth. Cross-lingual ability\nof multilingual bert: An empirical study. In International Conference on Learning\nRepresentations, 2020.\n[196] Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Winata, Samuel\nCahyawijaya, Anuoluwapo Aremu, Perez Ogayo, and Graham Neubig. Multi-lingual\nand multi-cultural ﬁgurative language understanding. In Anna Rogers, Jordan Boyd-\nGraber, and Naoaki Okazaki, editors, Findings of the Association for Computational\nLinguistics: ACL 2023, pages 8269–8284, Toronto, Canada, July 2023. Association for\nComputational Linguistics.\n[197] Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhat-\ntacharyya, Mitesh M. Khapra, and Pratyush Kumar. IndicNLPSuite: Monolingual\ncorpora, evaluation benchmarks and pre-trained multilingual language models for\n128\nIndian languages. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of\nthe Association for Computational Linguistics: EMNLP 2020, pages 4948–4961, Online,\nNovember 2020. Association for Computational Linguistics.\n[198] Yue Kang, Zhao Cai, Chee-Wee Tan, Qian Huang, and Hefu Liu. Natural language\nprocessing (nlp) in management research: A literature review. Journal of Management\nAnalytics, 7(2):139–172, April 2020.\n[199] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws\nfor neural language models, 2020.\n[200] Erlin Kartikasari, Kisyani Laksono, Dian Savitri Agusniar, and Diah Yovita Suryarini.\nA study of dialectology on Javanese \"Ngoko\" in Banyuwangi, Surabaya, Magetan,\nand Solo. Humaniora, 30(2):128, 2018.\n[201] Muhammad Kautsar, Rahmah Nurdini, Samuel Cahyawijaya, Genta Winata, and\nAyu Purwarianti. IndoToD: A multi-domain Indonesian benchmark for end-to-end\ntask-oriented dialogue systems. In Derry Wijaya, Alham Fikri Aji, Clara Vania,\nGenta Indra Winata, and Ayu Purwarianti, editors, Proceedings of the First Workshop\nin South East Asian Language Processing, pages 85–99, Nusa Dua, Bali, Indonesia,\nNovember 2023. Association for Computational Linguistics.\n[202] Leila Khalatbari, Yejin Bang, Dan Su, Willy Chung, Saeed Ghadimi, Hossein Sameti,\nand Pascale Fung. Learn what not to learn: Towards generative safety in chatbots.\narXiv preprint arXiv:2304.11220, 2023.\n[203] Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. Natural language\nprocessing: state of the art, current trends and challenges. Multimedia Tools and\nApplications, 82(3):3713–3744, July 2022.\n[204] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim,\nHyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon\nYang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk\nLee, and Sunghun Kim. Solar 10.7b: Scaling large language models with simple yet\neffective depth up-scaling, 2024.\n129\n[205] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-\njardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and\nRaia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of\nthe National Academy of Sciences, 114(13):3521–3526, March 2017.\n[206] Kim Kirsner, Marilyn C Smith, RS Lockhart, ML King, and M Jain. The bilingual\nlexicon: Language-speciﬁc units in an integrated network. Journal of verbal learning\nand verbal behavior, 23(4):519–539, 1984.\n[207] Marian Klamer. Documenting the linguistic diversity of Indonesia: Time is running\nout. In Santri E.P. Djahimo, editor, Proceedings of ‘Revitalization of local languages as\nthe pillar of pluralism’, pages 1–10. APBL (Asosiasi Peneliti Bahasa-bahasa Lokal) and\nNusa Cendana University, Kupang, Satya Wacana Press, 06 2018.\n[208] Kevin Knight and Jonathan Graehl. Machine transliteration. In 35th Annual Meeting\nof the Association for Computational Linguistics and 8th Conference of the European Chapter\nof the Association for Computational Linguistics, pages 128–135, Madrid, Spain, July\n1997. Association for Computational Linguistics.\n[209] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In\nProceedings of Machine Translation Summit X: Papers, pages 79–86, Phuket, Thailand,\nSeptember 13-15 2005.\n[210] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam,\nKeith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyﬁ,\net al. Openassistant conversations-democratizing large language model alignment.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[211] Fajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models\nonly pass primary school exams in Indonesia: A comprehensive test on IndoMMLU.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, pages 12359–12374,\nSingapore, December 2023. Association for Computational Linguistics.\n130\n[212] Fajri Koto, Jey Han Lau, and Timothy Baldwin. Liputan6: A large-scale Indonesian\ndataset for text summarization. In Proceedings of the 1st Conference of the Asia-Paciﬁc\nChapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pages 598–608, Suzhou, China, December\n2020. Association for Computational Linguistics.\n[213] Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah,\nAisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata,\nNizar Habash, Preslav Nakov, and Timothy Baldwin.\nArabicmmlu: Assessing\nmassive multitask language understanding in arabic, 2024.\n[214] Fajri Koto, Rahmad Mahendra, Nurul Aisyah, and Timothy Baldwin. Indoculture:\nExploring geographically-inﬂuenced cultural commonsense reasoning across eleven\nindonesian provinces, 2024.\n[215] Fajri Koto, Afshin Rahimi, Jey Han Lau, and Timothy Baldwin. IndoLEM and\nIndoBERT: A benchmark dataset and pre-trained language model for Indonesian\nNLP. In Proceedings of the 28th International Conference on Computational Linguistics,\npages 757–770, Barcelona, Spain (Online), December 2020. International Committee\non Computational Linguistics.\n[216] Taku Kudo and John Richardson. SentencePiece: A simple and language indepen-\ndent subword tokenizer and detokenizer for neural text processing. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium, November 2018. Association for\nComputational Linguistics.\n[217] Julius Von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard\nSchölkopf, Michel Besserve, and Francesco Locatello.\nSelf-supervised learning\nwith data augmentations provably isolates content from style. In A. Beygelzimer,\nY. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information\nProcessing Systems, 2021.\n[218] Saurabh Kulshreshtha, Jose Luis Redondo Garcia, and Ching-Yun Chang. Cross-\nlingual alignment methods for multilingual BERT: A comparative study. In Trevor\n131\nCohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 933–942, Online, November 2020. Association for\nComputational Linguistics.\n[219] Aman Kumar, Himani Shrotriya, Prachi Sahu, Amogh Mishra, Raj Dabre, Ratish\nPuduppully, Anoop Kunchukuttan, Mitesh M. Khapra, and Pratyush Kumar. Indic-\nNLG benchmark: Multilingual datasets for diverse NLG tasks in Indic languages.\nIn Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pages 5363–5394,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics.\n[220] Anoop Kunchukuttan, Siddharth Jain, and Rahul Kejriwal. A large-scale evaluation\nof neural machine transliteration for Indic languages. In Paola Merlo, Jorg Tiedemann,\nand Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main Volume, pages 3469–3475, Online,\nApril 2021. Association for Computational Linguistics.\n[221] Soumyadeep Kundu, Sayantan Paul, and Santanu Pal. A deep learning based ap-\nproach to transliteration. In Nancy Chen, Rafael E. Banchs, Xiangyu Duan, Min\nZhang, and Haizhou Li, editors, Proceedings of the Seventh Named Entities Workshop,\npages 79–83, Melbourne, Australia, July 2018. Association for Computational Lin-\nguistics.\n[222] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining.\nAdvances in Neural Information Processing Systems (NeurIPS), 2019.\n[223] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato.\nUnsupervised machine translation using monolingual corpora only. In International\nConference on Learning Representations, 2018.\n[224] Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and\nHervé Jégou. Word translation without parallel data. In International Conference on\nLearning Representations, 2018.\n132\n[225] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-\n06-27. 2022.\n[226] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton\nBishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning\nfrom human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.\n[227] Colin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham\nOwodunni, and Daniel Whitenack. Bloom library: Multimodal datasets in 300+\nlanguages for a variety of downstream tasks. In Yoav Goldberg, Zornitsa Kozareva,\nand Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 8608–8621, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics.\n[228] Carsten Levisen. Biases we live by: Anglocentrism in linguistics and cognitive\nsciences. Language Sciences, 76:101173, November 2019.\n[229] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mo-\nhamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\nBart: Denoising\nsequence-to-sequence pre-training for natural language generation, translation, and\ncomprehension. In Proceedings of the 58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7871–7880, 2020.\n[230] Aitor Lewkowycz, Ambrose Slone, Anders Andreassen, Daniel Freeman, Ethan S\nDyer, Gaurav Mishra, Guy Gur-Ari, Jaehoon Lee, Jascha Sohl-dickstein, Kristen\nChiafullo, et al. Beyond the imitation game: Quantifying and extrapolating the\ncapabilities of language models. 2022.\n[231] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-\nx: Multilingual replicable instruction-following models with low-rank adaptation,\n2023.\n[232] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan\nDuan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language\nunderstanding in chinese, 2024.\n133\n[233] Ruosen Li and Xinya Du. Leveraging structured information for explainable multi-\nhop question answering and reasoning. In Houda Bouamor, Juan Pino, and Kalika\nBali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023,\npages 6779–6789, Singapore, December 2023. Association for Computational Linguis-\ntics.\n[234] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text\nSummarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for\nComputational Linguistics.\n[235] Peiqin Lin, Shaoxiong Ji, Jörg Tiedemann, André FT Martins, and Hinrich Schütze.\nMala-500: Massive language adaptation of large language models. arXiv preprint\narXiv:2401.13303, 2024.\n[236] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel\nSimig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru,\nSam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke\nZettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot\nlearning with multilingual generative language models. In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 9019–9052, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics.\n[237] Patrick Littell, David R Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and\nLori Levin. Uriel and lang2vec: Representing languages as typological, geographical,\nand phylogenetic vectors. In Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 2, Short Papers, volume 2, pages\n8–14, 2017.\n[238] Chen Cecilia Liu, Fajri Koto, Timothy Baldwin, and Iryna Gurevych. Are multilingual\nllms culturally-diverse reasoners? an investigation into multicultural proverbs and\nsayings, 2024.\n[239] Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit\nBansal, and Colin Raffel. Few-shot parameter-efﬁcient ﬁne-tuning is better and\n134\ncheaper than in-context learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[240] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi\nYang, and Soroush Vosoughi. Training socially aligned language models in simulated\nhuman society. arXiv preprint arXiv:2305.16960, 2023.\n[241] Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina Semnani, Chen Yu, and Monica\nLam. SUQL: Conversational search over structured and unstructured data with\nlarge language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors,\nFindings of the Association for Computational Linguistics: NAACL 2024, pages 4535–4555,\nMexico City, Mexico, June 2024. Association for Computational Linguistics.\n[242] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M. López, and\nAndrew D. Bagdanov. Rotate your networks: Better weight consolidation and less\ncatastrophic forgetting. In 2018 24th International Conference on Pattern Recognition\n(ICPR), pages 2262–2268, 2018.\n[243] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao\nCheng, Yegor Klochkov, Muhammad Faaiz Tauﬁq, and Hang Li. Trustworthy llms: a\nsurvey and guideline for evaluating large language models’ alignment. arXiv preprint\narXiv:2308.05374, 2023.\n[244] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad,\nMike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural\nmachine translation. Transactions of the Association for Computational Linguistics, 8:726–\n742, 2020.\n[245] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[246] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny\nZhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The ﬂan collection:\nDesigning data and methods for effective instruction tuning, 2023.\n135\n[247] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for contin-\nual learning. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems, NIPS’17, page 6470–6479, Red Hook, NY, USA, 2017. Curran\nAssociates Inc.\n[248] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations, 2019.\n[249] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung.\nNegative object presence evaluation (nope) to measure object hallucination in vision-\nlanguage models, 2023.\n[250] Holy Lovenia, Rahmad Mahendra, Salsabil Maulana Akbar, Lester James V. Miranda,\nJennifer Santoso, Elyanah Aco, Akhdan Fadhilah, Jonibek Mansurov, Joseph Marvin\nImperial, Onno P. Kampman, Joel Ruben Antony Moniz, Muhammad Ravi Shulthan\nHabibi, Frederikus Hudi, Railey Montalan, Ryan Ignatius, Joanito Agili Lopo,\nWilliam Nixon, Börje F. Karlsson, James Jaya, Ryandito Diandaru, Yuze Gao, Patrick\nAmadeus, Bin Wang, Jan Christian Blaise Cruz, Chenxi Whitehouse, Ivan Halim\nParmonangan, Maria Khelli, Wenyu Zhang, Lucky Susanto, Reynard Adha Ryanda,\nSonny Lazuardi Hermawan, Dan John Velasco, Muhammad Dehan Al Kautsar,\nWilly Fitra Hendria, Yasmin Moslem, Noah Flynn, Muhammad Farid Adilazuarda,\nHaochen Li, Johanes Lee, R. Damanhuri, Shuo Sun, Muhammad Reza Qorib, Amir-\nbek Djanibekov, Wei Qi Leong, Quyet V. Do, Niklas Muennighoff, Tanrada Pansuwan,\nIlham Firdausi Putra, Yan Xu, Ngee Chia Tai, Ayu Purwarianti, Sebastian Ruder,\nWilliam Tjhi, Peerat Limkonchotiwat, Alham Fikri Aji, Sedrick Keh, Genta Indra\nWinata, Ruochen Zhang, Fajri Koto, Zheng-Xin Yong, and Samuel Cahyawijaya.\nSeacrowd: A multilingual multimodal data hub and benchmark suite for southeast\nasian languages. arXiv preprint arXiv: 2406.10118, 2024.\n[251] Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and\nFuru Wei. Chain-of-dictionary prompting elicits translation in large language models,\n2023.\n[252] Yash Madhani, Sushane Parthan, Priyanka Bedekar, Gokul Nc, Ruchi Khapra, Anoop\nKunchukuttan, Pratyush Kumar, and Mitesh Khapra. Aksharantar: Open Indic-\n136\nlanguage transliteration datasets and models for the next billion users. In Houda\nBouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, pages 40–57, Singapore, December 2023. Association\nfor Computational Linguistics.\n[253] Putu Devi Maharani and Komang Dian Puspita Candra. Variasi leksikal bahasa Bali\ndialek kuta selatan. Mudra Jurnal Seni Budaya, 33(1):76–84, 2018.\n[254] Rahmad Mahendra, Alham Fikri Aji, Samuel Louvan, Fahrurrozi Rahman, and Clara\nVania. IndoNLI: A natural language inference dataset for Indonesian. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10511–\n10527, Online and Punta Cana, Dominican Republic, November 2021. Association\nfor Computational Linguistics.\n[255] Chaitanya Malaviya, Graham Neubig, and Patrick Littell. Learning language rep-\nresentations for typology prediction. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), Copenhagen, Denmark, September 2017.\n[256] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single\nnetwork by iterative pruning. In Proceedings of the IEEE conference on Computer Vision\nand Pattern Recognition, pages 7765–7773, 2018.\n[257] Tony McEnery, Richard Xiao, and Yukio Tono. Corpus-based language studies: An\nadvanced resource book. Taylor & Francis, 2006.\n[258] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform\nmanifold approximation and projection. The Journal of Open Source Software, 3(29):861,\n2018.\n[259] Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin,\nSiva Reddy, Yang Liu, and Dilek Hakkani-Tur. Using in-context learning to improve\ndialogue safety. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pages 11882–11910, Singapore,\nDecember 2023. Association for Computational Linguistics.\n[260] I. Dan Melamed. Automatic evaluation and uniform ﬁlter cascades for inducing\nn-best translation lexicons. In Third Workshop on Very Large Corpora, 1995.\n137\n[261] I. Dan Melamed. Models of translation equivalence among words. Computational\nLinguistics, 26(2):221–250, 2000.\n[262] Pascal Mettes and Cees GM Snoek. Spatial-aware object embeddings for zero-shot\nlocalization and classiﬁcation of actions. In Proceedings of the IEEE international\nconference on computer vision, pages 4443–4452, 2017.\n[263] Antonio Valerio Miceli Barone. Towards cross-lingual distributed representations\nwithout parallel text trained with adversarial autoencoders.\nIn Phil Blunsom,\nKyunghyun Cho, Shay Cohen, Edward Grefenstette, Karl Moritz Hermann, Laura\nRimell, Jason Weston, and Scott Wen-tau Yih, editors, Proceedings of the 1st Workshop\non Representation Learning for NLP, pages 121–126, Berlin, Germany, August 2016.\nAssociation for Computational Linguistics.\n[264] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,\nDavid Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh,\nand Hao Wu. Mixed precision training. In International Conference on Learning\nRepresentations, 2018.\n[265] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of\nword representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[266] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among\nlanguages for machine translation. arXiv preprint arXiv:1309.4168, 2013.\n[267] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed\nrepresentations of words and phrases and their compositionality. In C. J. C. Burges,\nL. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in\nNeural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.\n[268] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed\nrepresentations of words and phrases and their compositionality. Advances in neural\ninformation processing systems, 26, 2013.\n[269] Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi,\nYun Li, Nijun Li, and Qianren Wang. Exploring the impact of table-to-text methods\n138\non augmenting LLM-based question answering with domain hybrid data. In Yi Yang,\nAida Davani, Avi Sil, and Anoop Kumar, editors, Proceedings of the 2024 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 6: Industry Track), pages 464–482, Mexico City, Mexico,\nJune 2024. Association for Computational Linguistics.\n[270] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Ha-\njishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes\nin-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,\neditors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 11048–11064, Abu Dhabi, United Arab Emirates, December 2022.\nAssociation for Computational Linguistics.\n[271] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive\ntext embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors,\nProceedings of the 17th Conference of the European Chapter of the Association for Computa-\ntional Linguistics, pages 2014–2037, Dubrovnik, Croatia, May 2023. Association for\nComputational Linguistics.\n[272] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Bider-\nman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf,\net al.\nCrosslingual generalization through multitask ﬁnetuning.\narXiv preprint\narXiv:2211.01786, 2022.\n[273] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.\nWebgpt: Browser-assisted question-answering with human feedback. arXiv preprint\narXiv:2112.09332, 2021.\n[274] Tarek Naous, Michael J Ryan, and Wei Xu. Having beer after prayer? measuring\ncultural bias in large language models. arXiv preprint arXiv:2305.14456, 2023.\n[275] Dat Quoc Nguyen and Anh Tuan Nguyen. PhoBERT: Pre-trained language models\nfor Vietnamese. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the\n139\nAssociation for Computational Linguistics: EMNLP 2020, pages 1037–1042, Online,\nNovember 2020. Association for Computational Linguistics.\n[276] Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying\nCheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and\nLidong Bing. Seallms - large language models for southeast asia. 2023.\n[277] Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic\nembed: Training a reproducible long context text embedder, 2024.\n[278] Hideo Okuma, Hirofumi Yamamoto, and Eiichiro Sumita. Introducing a translation\ndictionary into phrase-based smt. IEICE transactions on information and systems,\n91(7):2051–2057, 2008.\n[279] OpenAI. Chatgpt, 2023.\n[280] OpenAI. Chatgpt: Optimizing language models for dialogue, Jan 2023.\n[281] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,\nFlorencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu,\nHaiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel\nBernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine\nBoyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin\nButton, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carl-\nson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully\nChen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory De-\ncareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan,\nSteve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David\nFarhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Ful-\nford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel\nGoh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan\nGreene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey,\n140\nWade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin\nHu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan,\nŁukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak\nKhan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik\nKirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew\nKondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\nLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe,\nPatricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv\nMarkovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok\nMehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie\nMonaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard\nNgo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe\nPalermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,\nAlex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Bel-\nbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny,\nMichelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Eliza-\nbeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond,\nFrancis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder,\nMario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt,\nDavid Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jes-\nsica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie\nStaudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas\nTezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,\nPreston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone,\nArun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang,\nBen Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel\n141\nWolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai\nXiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers,\nChong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang,\nWilliam Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.\n[282] Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris\nAbdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir Araujo,\nMeriem Beloucif, Christine De Kock, et al. Semeval-2024 task 1: Semantic textual\nrelatedness for african and asian languages. In Proceedings of the 18th International\nWorkshop on Semantic Evaluation (SemEval-2024). Association for Computational Linguis-\ntics, 2024.\n[283] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Train-\ning language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\n[284] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman,\nJacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter\nWelinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to\nfollow instructions with human feedback, 2022.\n[285] Bill Palmer. The languages and linguistics of the New Guinea area: A comprehensive guide,\nvolume 4. Walter de Gruyter GmbH, 2018.\n[286] Rohan Pandey. gzip predicts data-dependent scaling laws, 2024.\n[287] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for\nautomatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak,\nand Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for\nComputational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July 2002.\nAssociation for Computational Linguistics.\n[288] Barun Patra, Joel Ruben Antony Moniz, Sarthak Garg, Matthew R. Gormley, and\nGraham Neubig. Bilingual lexicon induction with semi-supervision in non-isometric\n142\nembedding spaces. In Anna Korhonen, David Traum, and Lluís Màrquez, editors,\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npages 184–193, Florence, Italy, July 2019. Association for Computational Linguistics.\n[289] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors\nfor word representation. In Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar, October 2014.\nAssociation for Computational Linguistics.\n[290] Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel\nArtetxe. Lifting the curse of multilinguality by pre-training modular transformers.\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 3479–3495, Seattle,\nUnited States, July 2022. Association for Computational Linguistics.\n[291] Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, and\nSebastian Ruder. mmT5: Modular multilingual pre-training solves source language\nhallucinations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pages 1978–2008, Singapore,\nDecember 2023. Association for Computational Linguistics.\n[292] Jonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-\nBased Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–\n7673, Online, November 2020. Association for Computational Linguistics.\n[293] Wannaphong Phatthiyaphaibun, Surapon Nonesung, Patomporn Payoungkhamdee,\nPeerat Limkonchotiwat, Can Udomcharoenchaikit, Jitkapat Sawatphol, Chompakorn\nChaksangchaichot, Ekapol Chuangsuwanich, and Sarana Nutanong. Wangchanlion\nand wangchanx mrc eval, 2024.\n[294] Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual\nBERT? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–\n5001, Florence, Italy, July 2019. Association for Computational Linguistics.\n143\n[295] Matt Post. A call for clarity in reporting BLEU scores. In Ondˇrej Bojar, Rajen Chat-\nterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias\nHuck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie\nNévéol, Mariana Neves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor,\neditors, Proceedings of the Third Conference on Machine Translation: Research Papers,\npages 186–191, Brussels, Belgium, October 2018. Association for Computational\nLinguistics.\n[296] Apriyani Purwaningsih. Geograﬁdialek bahasa jawa pesisiran di desa paciran\nkabupaten lamongan. In Proceeding of International Conference on Art, Language, and\nCulture, pages 594–605, 2017.\n[297] Ayu Purwarianti and Ida Ayu Putu Ari Crisdayanti. Improving bi-lstm performance\nfor indonesian sentiment analysis using paragraph vector. In 2019 International\nConference of Advanced Informatics: Concepts, Theory and Applications (ICAICTA), pages\n1–5, 2019.\n[298] Oddy Virgantara Putra, Fathin Muhammad Wasmanson, Triana Harmini, and Shof-\nﬁn Nahwa Utama. Sundanese twitter dataset for emotion classiﬁcation. In 2020\nInternational Conference on Computer Engineering, Network, and Intelligent Multimedia\n(CENIM), pages 391–395, 2020.\n[299] Daria Pylypenko, Kwabena Amponsah-Kaakyire, Koel Dutta Chowdhury, Josef van\nGenabith, and Cristina España-Bonet. Comparing feature-engineering and feature-\nlearning approaches for multilingual translationese classiﬁcation. In Marie-Francine\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language Processing, pages 8596–\n8611, Online and Punta Cana, Dominican Republic, November 2021. Association for\nComputational Linguistics.\n[300] Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neu-\nbig. When and why are pre-trained word embeddings useful for neural machine\ntranslation? In Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 2 (Short\nPapers), pages 529–535, 2018.\n144\n[301] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga,\nand Diyi Yang. Is ChatGPT a general-purpose natural language processing task\nsolver? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Processing, pages 1339–1384,\nSingapore, December 2023. Association for Computational Linguistics.\n[302] Ella Rabinovich and Shuly Wintner. Unsupervised identiﬁcation of translationese.\nTransactions of the Association for Computational Linguistics, 3:419–432, 2015.\n[303] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving\nlanguage understanding by generative pre-training. OpenAI, 2018.\n[304] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners. OpenAI blog,\n1(8):9, 2019.\n[305] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. Direct preference optimization: Your language model is\nsecretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.\n[306] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67,\n2020.\n[307] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unan-\nswerable questions for SQuAD. In Iryna Gurevych and Yusuke Miyao, editors,\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 784–789, Melbourne, Australia, July 2018. Association\nfor Computational Linguistics.\n[308] Aida Ramezani and Yang Xu. Knowledge of cultural moral norms in large lan-\nguage models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 428–446, Toronto, Canada, July 2023. Association for\nComputational Linguistics.\n145\n[309] Nils Reimers and Iryna Gurevych.\nSentence-bert: Sentence embeddings using\nsiamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing. Association for Computational Linguistics, 11 2019.\n[310] Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings\nmultilingual using knowledge distillation. In Bonnie Webber, Trevor Cohn, Yulan\nHe, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 4512–4525, Online, November 2020.\nAssociation for Computational Linguistics.\n[311] Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao\nWang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, et al.\nPangu-{\\Sigma}: Towards trillion parameter language model with sparse heteroge-\nneous computing. arXiv preprint arXiv:2303.10845, 2023.\n[312] Parker Riley, Isaac Caswell, Markus Freitag, and David Grangier. Translationese as\na language in “multilingual” NMT. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\nand Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 7737–7746, Online, July 2020. Association for\nComputational Linguistics.\n[313] Joseph Lee Rodgers and W. Alan Nicewander. Thirteen ways to look at the correlation\ncoefﬁcient. The American Statistician, 42(1):59–66, feb 1988.\n[314] Milton Rokeach. A theory of organization and change within value-attitude systems.\nJournal of social issues, 1968.\n[315] Milton Rokeach. The nature of human values. Free press, 1973.\n[316] Milton Rokeach. Some unresolved issues in theories of beliefs, attitudes, and values.\nIn Nebraska symposium on motivation. University of Nebraska Press, 1979.\n[317] Milton Rokeach. Understanding human values. Simon and Schuster, 2008.\n[318] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory\nWayne. Experience replay for continual learning. In H. Wallach, H. Larochelle,\n146\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 32. Curran Associates, Inc., 2019.\n[319] Malcolm Ross. Pronouns as a preliminary diagnostic for grouping Papuan languages.\nPapuan pasts: Cultural, linguistic and biological histories of Papuan-speaking peoples,\n572:15–65, 2005.\n[320] Sebastian Ruder, Jonathan Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo\nNicosia, Shruti Rijhwani, Parker Riley, Jean-Michel Sarr, Xinyi Wang, John Wieting,\nNitish Gupta, Anna Katanova, Christo Kirov, Dana Dickinson, Brian Roark, Bidisha\nSamanta, Connie Tao, David Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry,\nDan Garrette, Reeve Ingle, Melvin Johnson, Dmitry Panteleev, and Partha Talukdar.\nXTREME-UP: A user-centric scarce-data benchmark for under-represented languages.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 1856–1884, Singapore, December 2023.\nAssociation for Computational Linguistics.\n[321] Sebastian Ruder, Ivan Vuli´c, and Anders Søgaard. A survey of cross-lingual word\nembedding models. Journal of Artiﬁcial Intelligence Research, 65:569–631, August 2019.\n[322] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representa-\ntions by back-propagating errors. nature, 323(6088):533–536, 1986.\n[323] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari,\nCanwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike\nTian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit\nPandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella\nBiderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted\ntraining enables zero-shot task generalization. In International Conference on Learning\nRepresentations, 2022.\n[324] Mei Silviana Saputri, Rahmad Mahendra, and Mirna Adriani. Emotion classiﬁcation\n147\non Indonesian Twitter dataset. In 2018 International Conference on Asian Language\nProcessing (IALP), pages 90–95, 2018.\n[325] Gita Sarwadi, Mahsun Mahsun, and Burhanuddin Burhanuddin. Lexical variation\nof Sasak Kuto-Kute dialect in North Lombok district. Jurnal Kata: Penelitian tentang\nIlmu Bahasa dan Sastra, 3(1):155–169, 2019.\n[326] Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchin-\nsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck\ntheory of deep learning. In International Conference on Learning Representations, 2018.\n[327] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé,\net al. Bloom: A 176b-parameter open-access multilingual language model. arXiv\npreprint arXiv:2211.05100, 2022.\n[328] Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica\nChen, Kyunghyun Cho, and Ethan Perez. Training language models with language\nfeedback at scale. arXiv preprint arXiv:2303.16755, 2023.\n[329] Peter Schmidt, Sebastian Bamberg, Eldad Davidov, Johannes Herrmann, and\nShalom H Schwartz. Die messung von werten mit dem “portraits value question-\nnaire”. Zeitschrift für Sozialpsychologie, 38(4):261–275, 2007.\n[330] Peter H Schönemann. A generalized solution of the orthogonal procrustes problem.\nPsychometrika, 31(1):1–10, 1966.\n[331] Robert Schreuder and Bert Weltens. The bilingual lexicon, volume 6. John Benjamins\nPublishing, 1993.\n[332] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed em-\nbedding for face recognition and clustering. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 815–823, 2015.\n[333] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n148\n[334] Matthew Schultz and Thorsten Joachims. Learning a distance metric from relative\ncomparisons. Advances in neural information processing systems, 16, 2003.\n[335] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE\ntransactions on Signal Processing, 45(11):2673–2681, 1997.\n[336] Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. Cross-lingual alignment\nof contextual word embeddings, with applications to zero-shot dependency parsing.\nIn Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 1599–1613,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[337] Shalom Schwartz. Cultural value orientations: Nature & implications of national\ndifferences. Psychology. Journal of Higher School of Economics, 5(2):37–67, 2008.\n[338] Shalom H Schwartz. Beyond individualism/collectivism: New cultural dimensions\nof values. 1994.\n[339] Shalom H Schwartz. A theory of cultural values and some implications for work.\nApplied psychology: an international review, 1999.\n[340] Shalom H Schwartz. Mapping and interpreting cultural differences around the world.\nIn Comparing cultures, pages 43–73. Brill, 2004.\n[341] Shalom H Schwartz. An overview of the schwartz theory of basic values. Online\nreadings in Psychology and Culture, 2(1):11, 2012.\n[342] Shalom H. Schwartz. The Reﬁned Theory of Basic Values, page 51–72. Springer Interna-\ntional Publishing, 2017.\n[343] Shalom H Schwartz and Jan Cieciuch. Measuring the reﬁned theory of individual val-\nues in 49 cultural groups: psychometrics of the revised portrait value questionnaire.\nAssessment, 29(5):1005–1019, 2022.\n[344] Holger Schwenk. Filtering and mining parallel data in a joint multilingual space. In\nIryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of\n149\nthe Association for Computational Linguistics (Volume 2: Short Papers), pages 228–234,\nMelbourne, Australia, July 2018. Association for Computational Linguistics.\n[345] Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco\nGuzmán.\nWikiMatrix: Mining 135M parallel sentences in 1620 language pairs\nfrom Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main Volume, pages 1351–1361, Online, April\n2021. Association for Computational Linguistics.\n[346] Holger Schwenk and Matthijs Douze. Learning joint multilingual sentence rep-\nresentations with neural machine translation. In Phil Blunsom, Antoine Bordes,\nKyunghyun Cho, Shay Cohen, Chris Dyer, Edward Grefenstette, Karl Moritz Her-\nmann, Laura Rimell, Jason Weston, and Scott Yih, editors, Proceedings of the 2nd\nWorkshop on Representation Learning for NLP, pages 157–167, Vancouver, Canada,\nAugust 2017. Association for Computational Linguistics.\n[347] Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li,\nFajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit\nPradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong\nLiu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov,\nTimothy Baldwin, and Eric Xing. Jais and jais-chat: Arabic-centric foundation and\ninstruction-tuned open generative large language models, 2023.\n[348] Joan Serrà, Dídac Surís, Marius Miron, and Alexandros Karatzoglou. Overcoming\ncatastrophic forgetting with hard attention to the task. In ICML, 2018.\n[349] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo,\nXinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey.\narXiv preprint arXiv:2309.15025, 2023.\n[350] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush\nVosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,\nand Jason Wei. Language models are multilingual chain-of-thought reasoners. In\nThe Eleventh International Conference on Learning Representations, 2023.\n150\n[351] Ravid Shwartz Ziv and Yann LeCun.\nTo compress or not to compress—self-\nsupervised learning and information theory: A review. Entropy, 26(3), 2024.\n[352] Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An\nempirical study of instruction-tuning large language models in chinese. In The 2023\nConference on Empirical Methods in Natural Language Processing, 2023.\n[353] AI Singapore. Sea-lion (southeast asian languages in one network): A family of\nlarge language models for southeast asia. https://github.com/aisingapore/\nsealion, 2023.\n[354] Jasdeep Singh, Bryan McCann, Richard Socher, and Caiming Xiong. BERT is not\nan interlingua and the bias of tokenization. In Colin Cherry, Greg Durrett, George\nFoster, Reza Haffari, Shahram Khadivi, Nanyun Peng, Xiang Ren, and Swabha\nSwayamdipta, editors, Proceedings of the 2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019), pages 47–55, Hong Kong, China, November 2019.\nAssociation for Computational Linguistics.\n[355] Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Ma-\nhendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura\nOMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado,\nLuisa Souza Moura, Dominik Krzemi´nski, Hakimeh Fadaei, Irem Ergün, Ifeoma\nOkoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebas-\ntian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas\nMuennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara\nHooker. Aya dataset: An open-access collection for multilingual instruction tuning,\n2024.\n[356] Masitowarni Siregar, Syamsul Bahri, Dedi Sanjaya, et al. Code switching and code\nmixing in indonesia: Study in sociolinguistics. English Language and Literature Studies,\n4(1):77–92, 2014.\n[357] Sunayana Sitaram, Khyathi Raghavi Chandu, Sai Krishna Rallabandi, and Alan W\nBlack. A survey of code-switched speech and language processing. arXiv preprint\narXiv:1904.00784, 2019.\n151\n[358] Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. Translating\ncollocations for bilingual lexicons: A statistical approach. Computational Linguistics,\n22(1):1–38, 1996.\n[359] Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. Ofﬂine\nbilingual word vectors, orthogonal transformations and the inverted softmax. arXiv\npreprint arXiv:1702.03859, 2017.\n[360] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Ra-\njbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay\nKorthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer,\nXia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and\nBryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b,\na large-scale generative language model, 2022.\n[361] Vladimir M. Smokotin, Anna S. Alekseyenko, and Galina I. Petrova.\nThe phe-\nnomenon of linguistic globalization: English as the global lingua franca (eglf). Proce-\ndia - Social and Behavioral Sciences, 154:509–513, October 2014.\n[362] James Neil Sneddon. The Indonesian language: Its history and role in modern society.\nUNSW Press, Sydney, 2003.\n[363] Soeparno. Kerancuan fono-ortograﬁs dan orto-fonologis bahasa Indonesia ragam\nlisan dan tulis. Diksi, 12(2), 2015.\n[364] Anders Søgaard. Should we ban English NLP for a year?\nIn Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pages 5254–5260,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics.\n[365] Anders Sogaard, Sebastian Ruder, and Ivan Vulic. On the limitations of unsuper-\nvised bilingual dictionary induction. In Iryna Gurevych and Yusuke Miyao, editors,\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 778–788, Melbourne, Australia, July 2018. Association\nfor Computational Linguistics.\n152\n[366] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.\nMpnet: masked\nand permuted pre-training for language understanding. In Proceedings of the 34th\nInternational Conference on Neural Information Processing Systems, NIPS ’20, Red Hook,\nNY, USA, 2020. Curran Associates Inc.\n[367] Taylor Sorensen, Liwei Jiang, Jena D Hwang, Sydney Levine, Valentina Pyatkin,\nPeter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et al. Value\nkaleidoscope: Engaging ai with pluralistic human values, rights, and duties. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 38, pages 19937–\n19947, 2024.\n[368] Hein Steinhauer. Colonial history and language policy in insular Southeast Asia\nand Madagascar. In Alexander Adelaar and Nikolaus P. Himmelmann, editors, The\nAustronesian Languages of Asia and Madagascar, chapter 3, pages 65–86. Routledge,\nOxon, 2005.\n[369] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss,\nAlec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with\nhuman feedback. Advances in Neural Information Processing Systems, 33:3008–3021,\n2020.\n[370] MO Stitson, JAE Weston, A Gammerman, V Vovk, and V Vapnik. Theory of support\nvector machines. University of London, 117(827):188–191, 1996.\n[371] Shiliang Sun, Chen Luo, and Junyu Chen. A review of natural language processing\ntechniques for opinion mining systems. Information Fusion, 36:10–25, July 2017.\n[372] Bejo Sutrisno and Yessika Ariesta. Beyond the use of code mixing by social media\ninﬂuencers in Instagram. Advances in Language and Literary Studies, 10(6):143–151,\n2019.\n[373] Ana Cristina Suzina. English as lingua franca. or the sterilisation of scientiﬁc work.\nMedia, Culture & Society, 43(1):171–179, September 2020.\n[374] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing\nthe gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1701–1708, 2014.\n153\n[375] Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne\nLongpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir Radev,\nShanya Sharma, Arjun Subramonian, Jaesung Tae, Samson Tan, Deepak Tunuguntla,\nand Oskar Van Der Wal. You reap what you sow: On the challenges of bias eval-\nuation under multilingual settings. In Angela Fan, Suzana Ilic, Thomas Wolf, and\nMatthias Gallé, editors, Proceedings of BigScience Episode #5 – Workshop on Challenges\n& Perspectives in Creating Large Language Models, pages 26–41, virtual+Dublin, May\n2022. Association for Computational Linguistics.\n[376] Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, and Tanmoy Chakraborty.\nMultilingual LLMs are better cross-lingual in-context learners with alignment. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 6292–6307, Toronto, Canada, July 2023. Association for\nComputational Linguistics.\n[377] C Tardy. The role of english in scientiﬁc communication: lingua franca or tyran-\nnosaurus rex? Journal of English for Academic Purposes, 3(3):247–269, July 2004.\n[378] NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth\nHeaﬁeld, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,\nAnna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic\nBarrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley\nJarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre\nAndrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko,\nChristophe Ropers, Saﬁyyah Saleem, Holger Schwenk, and Jeff Wang. No language\nleft behind: Scaling human-centered machine translation. 2022.\n[379] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-\nshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li,\nHongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping\nHuang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu,\nZhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee\n154\nMan, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito De-\nlos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John,\nJosh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models\nfor dialog applications, 2022.\n[380] Jörg Tiedemann. Parallel data, tools and interfaces in OPUS. In Nicoletta Calzolari,\nKhalid Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan, Bente Maegaard, Joseph\nMariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the\nEighth International Conference on Language Resources and Evaluation (LREC’12), pages\n2214–2218, Istanbul, Turkey, May 2012. European Language Resources Association\n(ELRA).\n[381] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck\nprinciple. In 2015 IEEE Information Theory Workshop (ITW), pages 1–5, 2015.\n[382] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\nLlama: Open and efﬁcient foundation language models, 2023.\n[383] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan\nBikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David\nEsiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem\nKorenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin\n155\nXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\nLlama 2: Open foundation and ﬁne-tuned chat models, 2023.\n[384] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency.\nSelf-supervised learning from a multi-view perspective. In International Conference on\nLearning Representations, 2021.\n[385] Ahmet Ustun, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbe-\nmileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie\nVargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Ju-\nlia Kreutzer, and Sara Hooker. Aya model: An instruction ﬁnetuned open-access\nmultilingual language model, 2024.\n[386] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\ncontrastive predictive coding, 2019.\n[387] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,\nInc., 2017.\n[388] Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, Ai Ti Aw, and\nNancy F. Chen. Seaeval for multilingual foundation models: From cross-lingual\nalignment to cultural reasoning. NAACL, 2024.\n[389] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm:\ndeep self-attention distillation for task-agnostic compression of pre-trained transform-\ners. In Proceedings of the 34th International Conference on Neural Information Processing\nSystems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc.\n[390] Xinyi Wang, Sebastian Ruder, and Graham Neubig. Expanding pretrained models\nto thousands more languages via lexicon-based adaptation. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\n156\nPapers), pages 863–877, Dublin, Ireland, May 2022. Association for Computational\nLinguistics.\n[391] Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, and Ting Liu. Cross-lingual\nBERT transformation for zero-shot dependency parsing. In Kentaro Inui, Jing Jiang,\nVincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 5721–5727, Hong Kong, China,\nNovember 2019. Association for Computational Linguistics.\n[392] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot\nlearners. In International Conference on Learning Representations, 2022.\n[393] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori\nHashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent\nabilities of large language models. Transactions on Machine Learning Research, 2022.\nSurvey Certiﬁcation.\n[394] Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei\nLi, Yu Wan, Zhiwei Cao, Binbin Xie, et al. Polylm: An open source polyglot large\nlanguage model. arXiv preprint arXiv:2307.06018, 2023.\n[395] Haryo Akbarianto Wibowo, Erland Hilman Fuadi, Made Nindyatama Nityasya,\nRadityo Eko Prasojo, and Alham Fikri Aji. Copal-id: Indonesian language reasoning\nwith local culture and nuances. arXiv preprint arXiv:2311.01012, 2023.\n[396] Haryo Akbarianto Wibowo, Made Nindyatama Nityasya, Afra Feyza Akyürek, Suci\nFitriany, Alham Fikri Aji, Radityo Eko Prasojo, and Derry Tanti Wijaya. IndoCollex:\nA testbed for morphological transformation of Indonesian colloquial words. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021, pages 3170–3183, Online,\nAugust 2021. Association for Computational Linguistics.\n157\n[397] Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong\nLi, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, and\nAyu Purwarianti. IndoNLU: Benchmark and resources for evaluating Indonesian\nnatural language understanding. In Kam-Fai Wong, Kevin Knight, and Hua Wu,\neditors, Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association\nfor Computational Linguistics and the 10th International Joint Conference on Natural\nLanguage Processing, pages 843–857, Suzhou, China, December 2020. Association for\nComputational Linguistics.\n[398] Genta Winata, Shijie Wu, Mayank Kulkarni, Thamar Solorio, and Daniel Preotiuc-\nPietro. Cross-lingual few-shot learning on unseen languages. In Proceedings of the 2nd\nConference of the Asia-Paciﬁc Chapter of the Association for Computational Linguistics and\nthe 12th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 777–791, Online only, November 2022. Association for Computational\nLinguistics.\n[399] Genta Indra Winata. Multilingual transfer learning for code-switched language and\nspeech neural modeling. arXiv preprint arXiv:2104.06268, 2021.\n[400] Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad Mahendra, Fajri\nKoto, Ade Romadhony, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo,\nPascale Fung, Timothy Baldwin, Jey Han Lau, Rico Sennrich, and Sebastian Ruder.\nNusaX: Multilingual parallel sentiment dataset for 10 Indonesian local languages. In\nAndreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference\nof the European Chapter of the Association for Computational Linguistics, pages 815–834,\nDubrovnik, Croatia, May 2023. Association for Computational Linguistics.\n[401] Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosin-\nski, and Pascale Fung. Language models are few-shot multilingual learners. In\nDuygu Ataman, Alexandra Birch, Alexis Conneau, Orhan Firat, Sebastian Ruder, and\nGozde Gul Sahin, editors, Proceedings of the 1st Workshop on Multilingual Representation\nLearning, pages 1–15, Punta Cana, Dominican Republic, November 2021. Association\nfor Computational Linguistics.\n158\n[402] Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Code-\nswitching language modeling using syntax-aware multi-task learning. In Proceedings\nof the Third Workshop on Computational Approaches to Linguistic Code-Switching, pages\n62–67, 2018.\n[403] Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Code-\nswitched language models using neural based synthetic data from parallel sen-\ntences. In Proceedings of the 23rd Conference on Computational Natural Language Learning\n(CoNLL), pages 271–280, 2019.\n[404] Genta Indra Winata, Ruochen Zhang, and David Ifeoluwa Adelani. Miners: Multilin-\ngual language models as semantic retrievers, 2024.\n[405] Wilson Wongso, David Samuel Setiawan, and Derwin Suhartono. Causal and masked\nlanguage modeling of javanese language using transformer-based architectures. In\n2021 International Conference on Advanced Computer Science and Information Systems\n(ICACSIS), pages 1–7, 2021.\n[406] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,\nSuzana Ili´c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François\nYvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muen-\nnighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman,\nAngelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan,\nPedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Mar-\ngaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa\nAdelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan\nKim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada\nPistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin,\nIsaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian\nZhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid\nAlmubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan,\n159\nLoubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim\nMasoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank\nSingh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb,\nNishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Es-\npejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla\nAmuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López,\nRui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Sham-\nsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor,\nStanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick,\nTristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette\nLepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzer-\nling, Chenglei Si, Davut Emre Ta¸sar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y.\nLee, Abheesht Sharma, Andrea Santilli, Antoine Chafﬁn, Arnaud Stiegler, Debajyoti\nDatta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Stro-\nbelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S.\nAl-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry,\nTrishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong,\nZhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won\nChung, Jaesung Tae, Jason Phang, Oﬁr Press, Conglong Li, Deepak Narayanan,\nHatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi,\nOmar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi\nLacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena,\nSuraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Chevel-\neva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering,\nDan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina\nVoloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo,\nJekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg\nSerikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian\nGehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian\n160\nYun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada\nPruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenˇek Kasner, Alice Rueda,\nAmanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony\nHevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour,\nAzadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Car-\nlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David,\nDouwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani,\nFatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer,\nJulio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes,\nMarissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Naﬁs Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy,\nOlanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sar-\nmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade,\nTrieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano,\nAlison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine\nFourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio\nBarth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U.\nVrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David\nPosada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,\nMadeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo,\nMarianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael\nWeinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myung-\nsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus\nMuellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg,\nRobert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele\nGarda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee\nSang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo\nGigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash\nVenkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde\nBras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access\n161\nmultilingual language model, 2023.\n[407] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Al-\nham Fikri Aji. LaMini-LM: A diverse herd of distilled models from large-scale\ninstructions. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th\nConference of the European Chapter of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 944–964, St. Julian’s, Malta, March 2024. Association for\nComputational Linguistics.\n[408] Shijie Wu and Mark Dredze. Do explicit alignments robustly improve multilingual\nencoders? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Pro-\nceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 4471–4482, Online, November 2020. Association for Computational\nLinguistics.\n[409] Chai Wutiwiwatchai and Ausdang Thangthai. Syllable-based Thai-English machine\ntransliteration.\nIn A Kumaran and Haizhou Li, editors, Proceedings of the 2010\nNamed Entities Workshop, pages 66–70, Uppsala, Sweden, July 2010. Association for\nComputational Linguistics.\n[410] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming\nZhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large\nlanguage model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\n[411] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation\nof in-context learning as implicit bayesian inference. In International Conference on\nLearning Representations, 2022.\n[412] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya\nSiddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained\ntext-to-text transformer. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 483–498, Online, June 2021. Association for Computational Linguistics.\n[413] Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie. From instructions\n162\nto intrinsic human values–a survey of alignment goals for big models. arXiv preprint\narXiv:2308.12014, 2023.\n[414] Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ife-\noluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai,\nAhmed Baruwa, Genta Indra Winata, Stella Biderman, Dragomir Radev, and Vassilina\nNikoulina. Bloom+1: Adding language support to bloom for zero-shot prompting,\n2022.\n[415] Zheng Xin Yong, Ruochen Zhang, Jessica Forde, Skyler Wang, Arjun Subramonian,\nHoly Lovenia, Samuel Cahyawijaya, Genta Winata, Lintang Sutawika, Jan Chris-\ntian Blaise Cruz, Yin Lin Tan, Long Phan, Long Phan, Rowena Garcia, Thamar\nSolorio, and Alham Aji. Prompting multilingual large language models to generate\ncode-mixed texts: The case of south East Asian languages. In Genta Winata, Sudipta\nKar, Marina Zhukova, Thamar Solorio, Mona Diab, Sunayana Sitaram, Monojit\nChoudhury, and Kalika Bali, editors, Proceedings of the 6th Workshop on Computa-\ntional Approaches to Linguistic Code-Switching, pages 43–63, Singapore, December 2023.\nAssociation for Computational Linguistics.\n[416] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar,\nJing Xu, and Jason Weston.\nSelf-rewarding language models.\narXiv preprint\narXiv:2401.10020, 2024.\n[417] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins:\nSelf-supervised learning via redundancy reduction. In Marina Meila and Tong Zhang,\neditors, Proceedings of the 38th International Conference on Machine Learning, volume\n139 of Proceedings of Machine Learning Research, pages 12310–12320. PMLR, 18–24 Jul\n2021.\n[418] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi\nYang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-\ntrained model. arXiv preprint arXiv:2210.02414, 2022.\n[419] Richard Zens, Franz Josef Och, and Hermann Ney. Phrase-based statistical machine\ntranslation. In KI 2002: Advances in Artiﬁcial Intelligence: 25th Annual German Con-\n163\nference on AI, KI 2002 Aachen, Germany, September 16–20, 2002 Proceedings 25, pages\n18–32. Springer, 2002.\n[420] Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Adversarial training for\nunsupervised bilingual lexicon induction. In Regina Barzilay and Min-Yen Kan,\neditors, Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1959–1970, Vancouver, Canada, July 2017.\nAssociation for Computational Linguistics.\n[421] Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Winata, and\nAlham Aji. Multilingual large language models are not (yet) code-switchers. In\nHouda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing, pages 12567–12582, Singapore,\nDecember 2023. Association for Computational Linguistics.\n[422] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong\nBing. M3exam: A multilingual, multimodal, multilevel benchmark for examining\nlarge language models. arXiv preprint arXiv:2306.05179, 2023.\n[423] Zhaowei Zhang, Fengshuo Bai, Jun Gao, and Yaodong Yang. Measuring value\nunderstanding in language models through discriminator-critique gap. arXiv preprint\narXiv:2310.00378, 2023.\n[424] Zhaowei Zhang, Ceyao Zhang, Nian Liu, Siyuan Qi, Ziqi Rong, Song-Chun Zhu,\nShuguang Cui, and Yaodong Yang. Heterogeneous value alignment evaluation for\nlarge language models. In AAAI-2024 Workshop on Public Sector LLMs: Algorithmic\nand Sociotechnical Design, 2024.\n[425] Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, and Minlie Huang. InstructSafety:\nA uniﬁed framework for building multidimensional and explainable safety detector\nthrough instruction tuning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\nFindings of the Association for Computational Linguistics: EMNLP 2023, pages 10421–\n10436, Singapore, December 2023. Association for Computational Linguistics.\n[426] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe\nMa, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke\n164\nZettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023.\n[427] Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and\nWieland Brendel. Contrastive learning inverts the data generating process. In Marina\nMeila and Tong Zhang, editors, Proceedings of the 38th International Conference on\nMachine Learning, volume 139 of Proceedings of Machine Learning Research, pages\n12979–12990. PMLR, 18–24 Jul 2021.\n[428] Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. Overview of the second\nBUCC shared task: Spotting parallel sentences in comparable corpora. In Serge\nSharoff, Pierre Zweigenbaum, and Reinhard Rapp, editors, Proceedings of the 10th\nWorkshop on Building and Using Comparable Corpora, pages 60–67, Vancouver, Canada,\nAugust 2017. Association for Computational Linguistics.\n165\nList of Publications\nSummary\n: Over the course of 5 years of my MPhil and PhD journeys, I published 41\npapers in total in various top-level conferences including ICASSP, INTERSPEECH, AACL,\nEACL, NAACL, ACL, EMNLP, AAAI, and NeurIPS. 20 of which are ﬁrst-(co)authored\npapers and 5 of which receive awards and recognitions.\n(* denotes equal contribution)\n• Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei\nJi, Etsuko Ishii, and Pascale Fung: High-Dimension Human Value Representation in\nLarge Language Models. To appear in Proceedings of EMNLP 2024.\n• Samuel Cahyawijaya⇤, Holy Lovenia⇤, Fajri Koto⇤, Rifki Aﬁna Putri⇤, Emmanuel\nDave, Jhonson Lee, Nuur Shadieq, Tjeng Wawan Cenggoro, Salsabil Maulana Ak-\nbar, Muhammad Ihza Mahendra, Dea Annisayanti Putri, Bryan Wilie, Genta Indra\nWinata, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung: Cendol: Open Instruction-\ntuned Generative Large Language Models for Indonesian Languages. To appear in\nProceedings of ACL 2024\n• Samuel Cahyawijaya, Holy Lovenia, Pascale Fung: LLMs Are Few-Shot In-Context\nLow-Resource Language Learner. NAACL 2024: 405–433\n• Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung: Belief\nRevision: The Adaptability of Large Language Models Reasoning. To appear in\nProceedings of EMNLP 2024\n• Delong Chen, Samuel Cahyawijaya, Etsuko Ishii, Ho Shu Chan, Yejin Bang, and\nPascale Fung: The Pyramid of Captions. To appear in ACM Multimedia 2024.\n166\n• Samuel Cahyawijaya⇤, Holy Lovenia⇤, Fajri Koto⇤, Dea Adhista, Emmanuel Dave,\nSarah Oktavianti, Salsabil Akbar, Jhonson Lee, Nuur Shadieq, Tjeng Wawan Ceng-\ngoro, Hanung Linuwih, Bryan Wilie, Galih Muridan, Genta Winata, David Moeljadi,\nAlham Fikri Aji, Ayu Purwarianti, Pascale Fung: Nusawrites: Constructing high-\nquality corpora for underrepresented and extremely low-resource languages. AACL\n2023: 921–945 (Resource Award)\n• Genta Indra Winata⇤, Alham Fikri Aji⇤, Samuel Cahyawijaya⇤, Rahmad Mahendra⇤,\nFajri Koto⇤, Ade Romadhony⇤, Kemal Kurniawan⇤, David Moeljadi, Radityo Eko Pra-\nsojo, Pascale Fung: NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian\nLocal Languages. EACL 2023: 815-834 (Outstanding Paper Award)\n• Samuel Cahyawijaya⇤, Holy Lovenia⇤, Alham Fikri Aji⇤, Genta Winata⇤, Bryan\nWilie⇤, Fajri Koto⇤, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa\nVincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi,\nMuhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alﬁna, Ilham Firdausi Pu-\ntra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh\nDhole, Arie Suryani, Rifki Aﬁna Putri, Dan Su, Keith Stevens, Made Nindyatama\nNityasya, Muhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng\nYu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk\nTho, Ichwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Bald-\nwin, Sebastian Ruder, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti:\nNusaCrowd: Open Source Initiative for Indonesian NLP Resources. ACL (Findings)\n2023: 13745–13818\n• Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie,\nHoly Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale\nFung: A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning,\nHallucination, and Interactivity. AACL 2023: 675–718 (Area Chair Award)\n• Bryan Wilie, Yan Xu, Willy Chung, Samuel Cahyawijaya, Holy Lovenia, Pascale Fung:\nPICK: Polished & informed candidate scoring for knowledge-grounded dialogue\nsystems. AACL 2023: 980–995\n167\n• Willy Chung, Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia, and Pascale Fung:\n“InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue\nSystems”. Workshop NLInt 2023:1-21\n• Samuel Cahyawijaya⇤, Holy Lovenia⇤, Willy Chung⇤, Rita Frieske, Zihan Liu, and\nPascale Fung: Cross-Lingual Cross-Age Group Adaptation for Low-Resource Elderly\nSpeech Emotion Recognition. INTERSPEECH 2023: 3352-3356.\n• Holy Lovenia, Samuel Cahyawijaya, Pascale Fung: Which One Are You Referring\nTo? Multimodal Object Identiﬁcation in Situated Dialogue. EACL (Student Research\nWorkshop) 2023: 61-72\n• Yejin Bang, Nayeon Lee, Tiezheng Yu, Leila Khalatbari, Yan Xu, Samuel Cahyawijaya,\nDan Su, Bryan Wilie, Romain Barraud, Elham J Barezi, Andrea Madotto, Hayden Kee,\nPascale Fung: “Towards Answering Open-ended Ethical Quandary Questions”. AI\nfor Social Good Workshop@AAAI 2023\n• Alham Fikri Aji⇤, Genta Indra Winata⇤, Fajri Koto⇤, Samuel Cahyawijaya⇤, Ade\nRomadhony⇤, Rahmad Mahendra⇤, Kemal Kurniawan, David Moeljadi, Radityo Eko\nPrasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder. 2022: “One Country,\n700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in\nIndonesia”. ACL 2022: 7226–7249\n• Ziwei Ji, Yan Xu, I-Tsun Cheng, Samuel Cahyawijaya, Rita Frieske, Etsuko Ishii, Min\nZeng, Andrea Madotto, Pascale Fung: “VScript: Controllable Script Generation with\nVisual Presentation”. AACL/IJCNLP (System Demonstrations) 2022: 1-8\n• Yan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zihan Liu, Genta Indra Winata, Andrea\nMadotto, Dan Su, Pascale Fung: “Retrieval-Free Knowledge-Grounded Dialogue\nResponse Generation with Adapters”. DialDoc@ACL 2022: 93-107\n• Samuel Cahyawijaya⇤, Tiezheng Yu⇤, Zihan Liu⇤, Xiaopu Zhou, Tze Wing Tiffany\nMak, Nancy Y. Ip, Pascale Fung: “SNP2Vec: Scalable Self-Supervised Pre-Training for\nGenome-Wide Association Study”. BioNLP@ACL 2022: 140-154\n168\n• Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Genta Indra Winata, Pascale\nFung, and Ayu Purwarianti: “IndoRobusta: Towards Robustness Against Diverse\nCode-Mixed Indonesian Local Languages.” Scaling Up Multilingual Evaluation Work-\nshop@ACL2022: 25–34.\n• Etsuko Ishii, Bryan Wilie, Yan Xu, Samuel Cahyawijaya, Pascale Fung: “Integrating\nQuestion Rewrites in Conversational Question Answering: A Reinforcement Learning\nApproach”. ACL-SRW 2022\n• Samuel Cahyawijaya⇤, Bryan Wilie⇤, Holy Lovenia, Huan Zhong, MingQian Zhong,\nYuk-Yu Nancy Ip, Pascale Fung: “How Long Is Enough? Exploring the Optimal\nIntervals of Long-Range Clinical Note Language Modeling”.Workshop LOUHI 2022.\n• Holy Lovenia, Bryan Wilie, Willy Chung, Min Zeng, Samuel Cahyawijaya, Su Dan,\nPascale Fung: ”Clozer: Adaptable Data Augmentation for Cloze-style Reading Com-\nprehension”. Workshop Repl4NLP 2022\n• Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung,\nand Pascale Fung: “Every picture tells a story: Image-grounded controllable stylistic\nstory generation.” SIGHUM Workshop@ICCL2022: 40-52\n• Tiezheng Yu⇤, Rita Frieske⇤, Peng Xu, Samuel Cahyawijaya⇤, Cheuk Tung Shadow\nYiu, Holy Lovenia, Wenliang Dai, Elham J. Barezi, Qifeng Chen, Xiaojuan Ma, Bertram\nE. Shi, Pascale Fung: “Automatic Speech Recognition Datasets in Cantonese: A Survey\nand New Dataset”. LREC 2022\n• Holy Lovenia⇤, Samuel Cahyawijaya⇤, Genta Indra Winata⇤, Peng Xu, Xu Yan, Zihan\nLiu, Rita Frieske, Tiezheng Yu, Wenliang Dai, Elham J. Barezi and Pascale Fung.\n“ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn\nConversation.” LREC 2022\n• Wenliang Dai⇤, Samuel Cahyawijaya⇤, Tiezheng Yu⇤, Elham J. Barezi⇤, Peng Xu,\nCheuk Tung Shadow Yiu, Rita Frieske, Holy Lovenia, Genta Indra Winata, Qifeng\nChen, Xiaojuan Ma, Bertram E. Shi and Pascale Fung. “CI-AVSR: A Cantonese\nAudio-Visual Speech Dataset for In-car Command Recognition.” LREC 2022\n169\n• Etsuko Ishii, Yan Xu, Samuel Cahyawijaya, and Bryan Wilie. 2022. Can Question\nRewriting Help Conversational Question Answering?. Insights from Negative Results\nin NLP Workshop 2022: 94–99\n• Yan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zihan Liu, Genta Indra Winata, Andrea\nMadotto, Dan Su, Pascale Fung: “Retrieval-Free Knowledge-Grounded Dialogue\nResponse Generation with Adapters”. 2nd DialDoc Workshop co-located at ACL\n2022 (Best Student Paper)\n• Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya,\nAndrea Madotto, Pascale Fung: CrossNER: Evaluating Cross-Domain Named Entity\nRecognition. AAAI 2021: 13452-13460\n• Zihan Liu, Genta Indra Winata, Samuel Cahyawijaya, Andrea Madotto, Zhaojiang\nLin, Pascale Fung: On the Importance of Word Order Information in Cross-lingual\nSequence Labeling. AAAI 2021: 13461-13469\n• Etsuko Ishii, Genta Indra Winata, Samuel Cahyawijaya, Divesh Lala, Tatsuya Kawa-\nhara, Pascale Fung: “ERICA: An Empathetic Android Companion for Covid-19\nQuarantine.” SIGDIAL 2021: 257-260.\n• Samuel Cahyawijaya⇤, Genta Indra Winata⇤, Bryan Wilie⇤, Karissa Vincentio⇤, Xiao-\nhong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar, Masayu\nLeylia Khodra, Ayu Purwarianti, Pascale Fung, IndoNLG: Benchmark and Resources\nfor Evaluating Indonesian Natural Language Generation, to appear in the Proceed-\nings of EMNLP (2021)\n• Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto,\nPascale Fung. \"Are Multilingual Models Effective in Code-Switching?\" In Proceedings\nof the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,\n2021.\n• Wenliang Dai⇤, Samuel Cahyawijaya⇤, Zihan Liu, Pascale Fung. \"Multimodal End-to-\nEnd Sparse Model for Emotion Recognition.\" In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2021.\n170\n• Ye Jin Bang*, Etsuko Ishii*, Samuel Cahyawijaya*, Ziwei Ji*, Pascale Fung, \"Model\nGeneralization on COVID-19 Fake News Detection\", 1st International Workshop on\nCombating Online Hostile Posts in Regional Languages during Emergency Situation,\nCONSTRAINT 2021 co-located with 35th AAAI Conference on Artiﬁcial Intelligence,\nAAAI 2021.\n• Zhaojiang Lin, Zihan Liu, Genta Indra Winata, Samuel Cahyawijaya, Andrea Madotto,\nYejin Bang, Etsuko Ishii, and Pascale Fung. 2021. XPersona: Evaluating Multilingual\nPersonalized Chatbot. NLP4ConvAI Workshop 2021: 102–112 (Honorable Mention)\n• Andrea Madotto, Samuel Cahyawijaya, Genta Indra Winata, Yan Xu, Zihan Liu,\nZhaojiang Lin, Pascale Fung. \"Learning Knowledge Bases with Parameters for Task-\nOriented Dialogue Systems.\" In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings, 2020.\n• Bryan Wilie*, Karissa Vincentio*, Genta Indra Winata*, Samuel Cahyawijaya*, Xi-\naohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri\nBahar, Ayu Purwarianti. \"IndoNLU: Benchmark and resources for evaluating indone-\nsian natural language understanding.\" In Proceedings of the 1st Conference of the\nAsia-Paciﬁc Chapter of the Association for Computational Linguistics and the 10th\nInternational Joint Conference on Natural Language Processing, 2020.\n• Genta Indra Winata⇤, Samuel Cahyawijaya⇤, Zhaojiang Lin, Zihan Liu, Peng Xu,\nPascale Fung. \"Meta-transfer learning for code-switched speech recognition.\" In Pro-\nceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\n2020.\n• Genta Indra Winata⇤, Samuel Cahyawijaya⇤, Zihan Liu⇤, Zhaojiang Lin, Andrea\nMadotto, Peng Xu, Pascale Fung. \"Learning Fast Adaptation on Cross-Accented\nSpeech Recognition.\" In INTERSPEECH, 2020.\n• Genta Indra Winata⇤, Samuel Cahyawijaya⇤, Zhaojiang Lin, Zihan Liu, and Pascale\nFung. \"Lightweight and Efﬁcient End-to-End Speech Recognition Using Low-Rank\nTransformer.\" In ICASSP 2020-2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 6144-6148. IEEE, 2020.\n171\nAppendix\nA\nHuman Annotation Guideline\nWe adopt the human evaluation and annotation guidelines from prior works [407, 231].\nWe incorporate 50 generated sentences from all six LLMs under study, i.e., BLOOMZ [327,\n272] with 7.1B parameters, LLaMA-3 [16] with 8B parameters, mT0XXL [412, 272] and\nAya-101 [385, 355] with 13B parameters, Command-R with 35B parameters, and GPT-3.5-\nTurbo [38, 279] with approximately 175B parameters. We use the data from the machine\ntranslation task, NusaTranslation MT [60] and NusaX MT [400]. We compare the sentence\ngeneration quality with the gold translation label of the corresponding task. We ask the\nannotators to rate the sentence quality with a letter A, B, C, or D. We provide the detailed\nhuman evaluation guideline in Figure A.1.\nFigure A.1: Human annotation guideline in incorporated in our human evaluation.\n172\nB\nInstruct-Align Prompt List\nIn this section, we provide the list of the prompt used in our experiment. For InstructAlign,\nwe use 6 prompts for each objective. The prompt list for bilingual denoising (TLM), ma-\nchine translation (MT), crosslingual semantic similarity (XSS), and monolingual denoising\n(MLM) are shown in Table B.1, Table B.2, Table B.3, and Table B.4, respectively. For the\nevaluation, we employ 3 English prompts for each task. The prompt list for sentiment\nanalysis, emotion recognition, and topic classiﬁcation tasks are described in Table B.5,\nTable B.6, and Table B.7, respectively.\nPrompt in Bilingual Denoising (TLM) Task\n[INPUT_TEXT]. Denoise the previous [INPUT_LANG] text to its\nequivalent sentence in [CONTEXT_LANG]: [CONTEXT]\\n[LABEL_TEXT]\nContext in [CONTEXT_LANG]: [CONTEXT]\\nFix the following\n[INPUT_LANG] text \"[INPUT_TEXT]\" ensuring the meaning is\nequivalent with the context.\n[LABEL_TEXT]\nContext in [CONTEXT_LANG]: [CONTEXT]\\nNoisy text in [INPUT_LANG]:\n[INPUT_TEXT]\\nHow would you fix the [INPUT_LANG] sentence to make\nthe meaning the same as the context?\n[LABEL_TEXT]\n[INPUT_TEXT]. Denoise the previous [INPUT_LANG] sentence to it\nequivalent sentence:\n[CONTEXT]\\n[LABEL_TEXT]\nContext:\n[CONTEXT]\\nFix the following [INPUT_LANG] text\n\"[INPUT_TEXT]\" ensuring the meaning is equivalent with the\ncontext.\n[LABEL_TEXT]\nContext:\n[CONTEXT]\\nNoisy text in [INPUT_LANG]:\n[INPUT_TEXT]\\nHow would you fix the [INPUT_LANG] sentence\nto make the meaning the same as the [CONTEXT_LANG] sentence?\n[LABEL_TEXT]\nTable B.1: Prompt used for Bilingual Denoising (TLM) task\n173\nPrompt in Machine Translation (MT) Task\nTranslate the following text from [SOURCE_LANG] to\n[TARGET_LANG].\\nText:\n[SOURCE_TEXT]\\nTranslation:\n[TARGET_TEXT]\n[SOURCE_TEXT]\\nTranslate the text above from [SOURCE_LANG] to\n[TARGET_LANG]. [TARGET_TEXT]\nText in [SOURCE_LANG]: [SOURCE_TEXT]\\nHow would you translate\nthat in [TARGET_LANG]? [TARGET_TEXT]\nTranslate the following text to [TARGET_LANG].\\nText:\n[SOURCE_TEXT]\\nTranslation:\n[TARGET_TEXT]\n[SOURCE_TEXT]\\nTranslate the text above to [TARGET_LANG].\n[TARGET_TEXT]\nInput text:\n[SOURCE_TEXT]\\nHow would you translate that into\n[TARGET_LANG]? [TARGET_TEXT]\nTable B.2: Prompt used for Machine Translation (MT) task\nPrompt in Crosslingual Semantic Similarity (XSS) Task\n[SOURCE_LANG] sentence:\n[SOURCE_TEXT]\\n[TARGET_LANG] sentence:\n[TARGET_TEXT]\\nDo the two sentences have the same meaning?\n[LABEL]\nSentence A: [SOURCE_TEXT]\\nSentence B: [TARGET_TEXT]\\nDo sentence\nA and sentence B have the same meaning?\n[LABEL]\n[SOURCE_LANG] sentence:\n[SOURCE_TEXT]\\n[TARGET_LANG] sentence:\n[TARGET_TEXT]\\nAre the two sentences equivalent?\n[LABEL]\nSentence A: [SOURCE_TEXT]\\nSentence B: [TARGET_TEXT]\\nAre\nsentence A and sentence B equivalent?\n[LABEL]\nIs the [SOURCE_LANG] sentence \"[SOURCE_TEXT]\" equivalent to the\n[TARGET_LANG] sentence \"[TARGET_TEXT]\"?\n[LABEL]\nIs the sentence \"[SOURCE_TEXT]\" equivalent to the sentence\n\"[TARGET_TEXT]\"?\n[LABEL]\nTable B.3: Prompt used for Crosslingual Semantic Similarity (XSS) task\n174\nPrompt in Monolingual Denoising (MLM) Task\nDenoise the following noisy [SOURCE_LANG] text:\n\"[SOURCE_TEXT]\",\nto make a correct sentence.\n[TARGET_TEXT]\nFix and complete the following [SOURCE_LANG] sentence:\n[SOURCE_TEXT]\\n[TARGET_TEXT]\nSentence in [SOURCE_LANG]: [SOURCE_TEXT]\\nHow would you fix the\nsentence to make a correct sentence?\n[TARGET_TEXT]\nDenoise the following noisy text \"[SOURCE_TEXT]\" to make a\ncorrect [SOURCE_LANG] sentence.\n[TARGET_TEXT]\nFix and complete the following sentence:\n[SOURCE_TEXT]\\n[TARGET_TEXT]\nInput text:\n[SOURCE_TEXT]\\nHow would you fix the sentence to\nmake a correct [SOURCE_LANG] sentence?\n[TARGET_TEXT]\nTable B.4: Prompt used for Monolingual Denoising (MLM) task\nPrompt in Sentiment Analysis Task\n[INPUT]\\nWhat would be the sentiment of the text above?\n[OPTIONS]? [LABELS_CHOICE]\nWhat is the sentiment of this text?\\nText:\n[INPUT]\\nAnswer with\n[OPTIONS]: [LABELS_CHOICE]\nText:\n[INPUT]\\n\\nPlease classify the sentiment of above text.\nAnswer with [OPTIONS]: [LABELS_CHOICE]\nTable B.5: Prompt used for Sentiment Analysis task\nPrompt in Emotion Recognition Task\n[INPUT]\\nWhat would be the emotion of the text above?\n[OPTIONS]?\n[LABELS_CHOICE]\nWhat is the emotion of this text?\\nText:\n[INPUT]\\nAnswer with\n[OPTIONS]: [LABELS_CHOICE]\nText:\n[INPUT]\\n\\nPlease classify the emotion of above text.\nAnswer with [OPTIONS]: [LABELS_CHOICE]\nTable B.6: Prompt used for Emotion Recognition task\nPrompt in Topic Classiﬁcation Task\n[INPUT]\\nWhat would be the topic of the text above?\n[OPTIONS]?\n[LABELS_CHOICE]\nWhat is the topic of this text?\\nText:\n[INPUT]\\nAnswer with\n[OPTIONS]: [LABELS_CHOICE]\nText:\n[INPUT]\\n\\nPlease classify the topic of above text.\nAnswer with [OPTIONS]: [LABELS_CHOICE]\nTable B.7: Prompt used for the Topic Classiﬁcation task\n175\nC\nComparison Between LLM-int8() and Full Precision In-\nference\nWe run all inference within our experiment with 8-bit quantization using LLM.int8() [101].\nTo the best of our knowledge, the effectiveness of LLM.int8() [101] has never been evaluated\non zero-shot prompting in low-resource language cases. We evaluate datasets from various\nIndonesian and local languages spoken in Indonesian which are listed in IndoNLU [397]\nand NusaCrowd [58]. Speciﬁcally, we evaluate on 10 languages in NusaX [400], Javanese\nIMDB [405], IndoLEM Sentiment [215], IndoNLI [254], SmSA [297], CASA [179], and\nSundanese Twitter Dataset for Emotion [298] datasets. Based on the result shown in\nTable C.8, there is only a marginal performance different between 8-bit quantization with\nLLM.int8() compared to the full precision models, which suggests the generalization of\nLLM.int8() [101] for zero-shot prompting in low-resource languages.\nModel\nPrompt Lang.\nAcc\nMacro F1\nMacro Prec.\nMacro Rec.\nFull Precision\nBLOOMZ-560M\nEN\n47.58\n33.25\n37.97\n43.11\nBLOOMZ-560M\nID\n44.37\n29.78\n37.79\n40.28\nBLOOMZ-1B1\nEN\n52.26\n37.90\n40.48\n45.79\nBLOOMZ-1B1\nID\n52.88\n39.28\n46.42\n46.67\nBLOOMZ-1B7\nEN\n51.44\n36.90\n41.90\n45.10\nBLOOMZ-1B7\nID\n52.68\n41.20\n50.81\n48.03\n8-Bit Quantization\nBLOOMZ-560M\nEN\n47.56\n34.67\n40.94\n42.97\nBLOOMZ-560M\nID\n43.64\n33.30\n42.90\n39.68\nBLOOMZ-1B1\nEN\n50.68\n37.52\n40.37\n44.56\nBLOOMZ-1B1\nID\n51.23\n38.69\n43.53\n45.34\nBLOOMZ-1B7\nEN\n49.71\n35.05\n42.11\n43.57\nBLOOMZ-1B7\nID\n52.61\n41.87\n51.74\n48.15\nBLOOMZ-3B\nEN\n54.80\n40.78\n46.59\n48.24\nBLOOMZ-3B\nID\n56.75\n44.34\n45.16\n51.12\nTable C.8: Evaluation of full precision and 8-bit quantization on various Indonesian local\nlanguages datasets.\n176\nD\nInstruct-Align Datasets\nIn this section, we describe the statistics for each dataset use in the experiment. Table D.9\nshows the statistics for the sentiment analysis task of NusaTranslation [60]. For the Indone-\nsian subset, we take the ﬁrst fold of the IndoLEM sentiment [215], which is the Indonesian\nsentiment analysis dataset used as the source sentences in the NusaTranslation [60]. Ta-\nble D.10 shows the statistics for the sentiment analysis task of NusaX [400]. Table D.11 and\nTable D.12 display the statistics for the emotion recognition and topic classiﬁcation tasks of\nNusaParagraph [60], respectively.\nStatus\nLanguage\nTrain\nValid.\nTest\nPre-trained\nIndonesian (ind)\n3638\n399\n1011\nSeen\nJavanese (jav)\n3400\n448\n1200\nSundanese (sun)\n3400\n448\n1200\nMinangkabau (min)\n3400\n448\n1200\nUnseen\nAmbon (abs)\n250\n98\n500\nBatak (btk)\n3400\n448\n1200\nBetawi (bew)\n3400\n448\n1200\nBima (bhp)\n260\n100\n500\nMadurese (mad)\n3400\n448\n1200\nMakassarese (mak)\n3400\n448\n1200\nMusi (mui)\n250\n91\n500\nRejang (rej)\n250\n78\n500\nTable D.9: Statistics of NusaTranslation sentiment analysis dataset. Pre-trained denotes\nlanguages that are already seen before the InstructAlign tuning. Seen denotes languages\nthat are seen during the InstructAlign.Unseen denotes languages that are still unseen after\nthe InstructAlign.\n177\nStatus\nLanguage\nTrain\nValid.\nTest\nPre-trained\nEnglish (eng)\n500\n100\n400\nIndonesia (ind)\n500\n100\n400\nSeen\nAceh (ace)\n500\n100\n400\nBali (ban)\n500\n100\n400\nBanjar (bjn)\n500\n100\n400\nBugis (bug)\n500\n100\n400\nMinang (min)\n500\n100\n400\nJavanese(jav)\n500\n100\n400\nSunda (sun)\n500\n100\n400\nUnseen\nMadura (mad)\n500\n100\n400\nNgaju (nij)\n500\n100\n400\nBataknese (bbc)\n500\n100\n400\nTable D.10: Statistics of NusaX sentiment analysis dataset. Pre-trained denotes languages\nthat are already seen before the InstructAlign. Seen denotes languages that are seen during\nthe InstructAlign.Unseen denotes languages that are still unseen after the InstructAlign.\nStatus\nLanguage\nTrain\nValid.\nTest\nUnseen\nJavanese (jav)\n2800\n440\n800\nMinangkabau (min)\n2000\n357\n800\nSundanese (sun)\n2400\n400\n800\nBuginese (bug)\n87\n50\n300\nSeen\nBatak (btk)\n1150\n292\n500\nBetawi (bew)\n2700\n430\n800\nMadurese (mad)\n1000\n263\n500\nMakassarese(mak)\n1500\n304\n500\nMusi (mui)\n200\n75\n400\nRejang (rej)\n136\n50\n300\nTable D.11: Statistics of NusaParagraph emotion recognition dataset. Pre-trained denotes\nlanguages that are already seen before InstructAlign. Seen denotes languages that are seen\nduring InstructAlign.Unseen denotes languages that are still unseen after InstructAlign.\n178\nStatus\nLanguage\nTrain\nValid.\nTest\nUnseen\nJavanese (jav)\n2650\n448\n800\nMinangkabau (min)\n2400\n399\n800\nSundanese (sun)\n2800\n468\n900\nBuginese (bug)\n93\n50\n300\nSeen\nBatak (btk)\n1350\n275\n500\nBetawi (bew)\n2650\n435\n800\nMadurese (mad)\n1800\n367\n700\nMakassarese(mak)\n1500\n376\n700\nMusi (mui)\n168\n80\n400\nRejang (rej)\n105\n50\n350\nTable D.12: Statistics of NusaParagraph topic classiﬁcation dataset. Pre-trained denotes\nlanguages that are already seen before InstructAlign. Seen denotes languages that are seen\nduring InstructAlign.Unseen denotes languages that are still unseen after InstructAlign.\n179\nE\nDetailed Experiment Results for Instruct-Align\nIn this section, we provide the complete experimental result per dataset. Table E.13 shows\nthe experiment results on the sentiment analysis task of NusaTranslation. Table E.14 shows\nthe experiment results on the sentiment analysis task of NusaX [400]. Table E.15 and\nTable E.16 show the experiment results on the emotion recognition and topic classiﬁcation\ntasks of NusaParagraph, respectively.\nModel\nL1\nL2\nL3\nind\njav\nmin\nsun\nabs\nbew\nbhp\nbtk\nmad\nmak\nmui\nrej\nBLOOM 560m\n61.47\n56.09\n58.13\n58.63\n62.53\n58.42\n49.72\n56.05\n54.02\n53.97\n60.27\n55.55\nBLOOM 1b1\n58.81\n59.05\n59.33\n59.16\n47.87\n58.23\n60.85\n58.95\n58.79\n58.83\n54.92\n57.73\nBLOOM 3b\n58.30\n44.84\n45.48\n44.61\n46.08\n45.61\n44.54\n43.62\n43.36\n44.03\n45.05\n43.15\nBLOOMZ 560m\n69.81\n43.00\n50.97\n46.51\n45.23\n47.87\n33.13\n36.69\n36.84\n35.30\n61.42\n36.21\nBLOOMZ 1b1\n80.40\n61.32\n68.95\n61.75\n61.07\n66.94\n46.18\n50.20\n49.71\n50.66\n70.31\n52.01\nBLOOMZ 3b\n81.38\n68.05\n71.76\n68.43\n69.57\n69.76\n67.73\n65.09\n64.37\n63.14\n69.08\n64.05\nMLM BLOOMZ 560m\n65.68\n23.29\n21.11\n22.31\n20.86\n22.00\n20.40\n19.04\n21.10\n20.59\n28.82\n19.50\nMLM BLOOMZ 560m-r=100k\n71.93\n63.89\n69.37\n66.27\n64.46\n65.38\n56.12\n62.68\n58.20\n56.51\n67.73\n58.12\nMLM BLOOMZ 1b1-r=100k\n73.25\n71.18\n72.24\n70.95\n62.67\n67.65\n56.07\n59.22\n58.60\n60.38\n68.10\n60.27\nMT BLOOMZ 560m\n55.20\n41.87\n39.00\n38.16\n36.88\n39.29\n36.07\n34.74\n36.97\n33.81\n41.72\n37.70\nMT BLOOMZ 560m-r=100k\n74.46\n70.73\n69.94\n70.00\n66.81\n67.65\n64.58\n66.53\n65.10\n61.35\n68.43\n63.23\nMT BLOOMZ 1b1-r=100k\n70.86\n59.62\n62.22\n61.63\n54.37\n57.97\n49.95\n50.22\n51.31\n52.22\n60.25\n50.30\nTLM BLOOMZ 560m\n71.57\n66.74\n66.05\n66.94\n63.06\n65.64\n59.00\n61.07\n61.31\n61.13\n65.30\n63.17\nTLM BLOOMZ 560m-r=1k\n70.52\n61.73\n62.76\n62.01\n54.34\n56.31\n48.52\n49.44\n49.21\n47.95\n61.11\n49.22\nTLM BLOOMZ 560m-r=10k\n72.82\n66.27\n66.22\n66.76\n62.29\n63.32\n59.61\n61.27\n60.35\n60.32\n63.60\n60.49\nTLM BLOOMZ 560m-r=100k\n72.40\n61.05\n59.43\n62.11\n54.51\n56.44\n46.68\n50.72\n50.56\n45.02\n63.27\n48.39\nTLM BLOOMZ 1b1-r=100k\n75.66\n70.05\n70.12\n70.70\n64.47\n67.07\n62.92\n61.87\n60.96\n61.86\n68.11\n61.53\nXSS BLOOMZ 560m\n64.48\n57.65\n52.18\n54.13\n52.40\n53.59\n48.55\n48.06\n49.59\n44.01\n58.03\n49.43\nXSS BLOOMZ 560m-r=1k\n69.34\n63.55\n62.84\n65.45\n65.20\n64.15\n59.11\n60.53\n62.34\n61.58\n63.51\n58.36\nXSS BLOOMZ 560m-r=10k\n72.22\n67.89\n67.81\n67.25\n62.76\n64.42\n62.83\n61.98\n62.02\n62.21\n65.38\n59.27\nXSS BLOOMZ 560m-r=100k\n71.27\n68.34\n67.89\n68.07\n61.58\n68.69\n62.66\n65.73\n63.44\n58.24\n70.24\n64.92\nXSS BLOOMZ 1b1-r=100k\n76.75\n72.40\n71.40\n71.87\n63.75\n65.45\n60.27\n61.27\n60.10\n63.21\n66.16\n60.49\nTable E.13: Experiment result on the sentiment analysis task of the NusaTranslation dataset\n180\nModel\nL1\nL2\nL3\neng\nind\nace\nban\nbjn\nbug\njav\nmin\nsun\nbbc\nmad\nnij\nBLOOM 560m\n29.26\n21.13\n21.35\n21.93\n21.35\n23.21\n21.86\n21.82\n21.04\n22.28\n22.13\n21.11\nBLOOM 1b1\n22.02\n22.54\n21.47\n22.62\n22.27\n21.34\n22.97\n21.92\n21.55\n22.10\n21.65\n21.53\nBLOOM 3b\n24.03\n21.17\n21.31\n21.17\n21.18\n21.35\n21.17\n21.17\n21.17\n21.19\n21.20\n21.17\nBLOOMZ 560m\n58.24\n55.59\n31.18\n32.40\n37.17\n27.79\n35.86\n39.29\n32.44\n29.49\n32.80\n38.15\nBLOOMZ 1b1\n57.41\n58.58\n43.31\n43.02\n44.72\n31.12\n46.52\n42.59\n39.20\n26.82\n41.92\n40.76\nBLOOMZ 3b\n62.65\n63.21\n48.81\n48.40\n55.27\n23.47\n54.26\n51.11\n39.41\n32.42\n38.88\n41.68\nMLM BLOOMZ 560m\n49.99\n49.33\n31.74\n28.37\n34.32\n25.76\n33.89\n31.27\n29.20\n28.43\n32.08\n30.98\nMLM BLOOMZ 560m-R-100000\n61.32\n60.01\n42.69\n41.69\n50.95\n31.53\n44.28\n44.30\n42.11\n33.18\n41.05\n40.15\nMLM BLOOMZ 1b1-R-100000\n61.30\n59.73\n43.11\n43.02\n50.71\n31.31\n53.66\n51.05\n47.27\n31.13\n42.02\n39.83\nMT BLOOMZ 560m\n47.24\n41.41\n31.78\n33.78\n34.69\n28.44\n35.47\n35.15\n36.01\n26.86\n26.69\n27.49\nMT BLOOMZ 560m-R-100000\n60.09\n54.18\n39.11\n42.59\n46.22\n34.50\n43.37\n41.31\n41.31\n35.95\n38.54\n39.84\nMT BLOOMZ 1b1-R-100000\n59.18\n53.69\n43.97\n45.40\n50.16\n38.65\n48.37\n45.97\n41.98\n37.97\n40.90\n40.60\nTLM BLOOMZ 560m\n44.72\n46.02\n33.59\n34.26\n41.16\n25.36\n41.76\n38.72\n37.40\n25.67\n30.88\n29.98\nTLM BLOOMZ 560m-R-1000\n58.05\n54.59\n43.03\n37.06\n46.55\n34.02\n43.21\n43.24\n39.59\n33.99\n38.16\n37.39\nTLM BLOOMZ 560m-R-10000\n57.38\n57.73\n43.43\n36.76\n45.99\n35.06\n44.38\n43.30\n40.83\n34.06\n42.46\n40.00\nTLM BLOOMZ 560m-R-100000\n61.65\n56.50\n41.78\n41.36\n48.15\n31.19\n48.89\n44.12\n44.90\n33.78\n41.51\n37.90\nTLM BLOOMZ 1b1-R-100000\n64.26\n63.54\n52.22\n51.35\n58.19\n41.87\n59.48\n59.67\n56.99\n38.26\n48.11\n48.01\nXSS BLOOMZ 560m\n53.93\n53.19\n43.60\n41.73\n47.09\n37.79\n47.29\n45.36\n43.42\n32.59\n41.66\n40.79\nXSS BLOOMZ 560m-R-1000\n56.57\n54.90\n36.78\n40.28\n42.20\n28.56\n45.67\n41.33\n39.80\n27.30\n31.67\n32.20\nXSS BLOOMZ 560m-R-10000\n55.62\n57.84\n44.24\n44.03\n50.04\n32.87\n48.92\n45.55\n45.64\n36.38\n40.36\n43.12\nXSS BLOOMZ 560m-R-100000\n59.89\n58.22\n45.53\n39.57\n52.68\n36.15\n49.83\n50.61\n46.45\n35.27\n42.40\n43.39\nXSS BLOOMZ 1b1-R-100000\n60.78\n59.34\n45.83\n45.45\n53.08\n36.24\n52.24\n50.54\n47.20\n33.81\n40.99\n41.08\nTable E.14: Experiment result on the sentiment analysis task of the NusaX dataset\n181\nModel\nL2\nL3\nbug\njav\nmin\nsun\nbew\nbtk\nmad\nmak\nmui\nrej\nBLOOM 560m\n1.19\n2.42\n4.54\n3.05\n4.37\n2.56\n0.59\n1.42\n1.11\n2.66\nBLOOM 1b1\n1.19\n2.42\n4.54\n3.05\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nBLOOM 3b\n1.19\n2.42\n4.54\n3.05\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nBLOOMZ 560m\n2.36\n2.93\n4.71\n3.52\n4.35\n3.33\n1.41\n3.09\n1.28\n4.10\nBLOOMZ 1b1\n1.19\n2.42\n4.54\n3.05\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nBLOOMZ 3b\n1.19\n2.42\n4.54\n3.05\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nMLM BLOOMZ 560m\n1.19\n2.51\n4.63\n3.04\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nMLM BLOOMZ 560m-R-100000\n1.19\n2.41\n4.54\n3.05\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nMLM BLOOMZ 1b1-R-100000\n1.19\n2.42\n4.71\n3.05\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nMT BLOOMZ 1b1-R-100000\n1.60\n2.76\n4.77\n3.54\n4.27\n2.56\n0.59\n1.42\n1.12\n2.44\nMT BLOOMZ 560m\n1.41\n2.58\n9.14\n5.63\n4.45\n2.57\n0.59\n1.56\n2.10\n2.44\nMT BLOOMZ 560m-R-100000\n1.19\n2.51\n4.54\n3.04\n4.29\n2.70\n0.59\n1.42\n1.11\n2.44\nTLM BLOOMZ 560m\n1.19\n2.58\n4.88\n3.54\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nTLM BLOOMZ 560m-R-1000\n1.19\n2.50\n5.10\n3.14\n4.29\n2.57\n0.59\n1.42\n1.12\n2.44\nTLM BLOOMZ 560m-R-10000\n1.19\n2.67\n5.12\n4.34\n4.29\n2.57\n0.73\n1.42\n1.11\n2.66\nTLM BLOOMZ 560m-R-100000\n1.40\n2.42\n4.54\n3.13\n4.29\n2.71\n0.73\n1.42\n1.11\n2.44\nTLM BLOOMZ 1b1-R-100000\n1.19\n2.41\n4.63\n3.13\n4.29\n2.57\n0.59\n1.42\n1.12\n2.44\nXSS BLOOMZ 560m\n1.54\n3.12\n4.65\n3.77\n4.29\n2.71\n0.59\n1.42\n1.11\n2.44\nXSS BLOOMZ 560m-R-1000\n1.56\n2.82\n5.16\n3.54\n4.37\n2.69\n0.73\n1.42\n1.11\n2.44\nXSS BLOOMZ 560m-R-10000\n1.39\n2.84\n5.56\n4.10\n4.29\n2.57\n0.59\n1.43\n1.28\n2.44\nXSS BLOOMZ 560m-R-100000\n1.19\n2.42\n4.54\n3.05\n4.29\n2.57\n0.59\n1.42\n1.11\n2.42\nXSS BLOOMZ 1b1-R-100000\n1.19\n2.67\n4.63\n3.84\n4.29\n2.57\n0.59\n1.42\n1.11\n2.44\nTable E.15: Experiment result on the emotion recognition task of the NusaParagraph dataset\n182\nModel\nL2\nL3\nbug\njav\nmin\nsun\nbew\nbtk\nmad\nmak\nmui\nrej\nBLOOM-560m\n7.68\n3.50\n6.36\n3.80\n5.42\n7.92\n11.25\n9.07\n3.91\n5.80\nBLOOM-1b1\n7.72\n3.50\n6.36\n3.81\n5.42\n7.93\n11.26\n9.09\n3.91\n5.82\nBLOOM-3b\n7.72\n3.50\n6.36\n3.81\n5.42\n7.93\n11.26\n9.09\n3.91\n5.82\nBLOOMZ-560m\n9.13\n4.10\n6.86\n4.29\n6.07\n8.75\n11.71\n9.45\n4.09\n6.04\nBLOOMZ-1b1\n7.72\n3.50\n6.36\n3.81\n5.51\n7.93\n11.26\n9.09\n3.91\n5.82\nBLOOMZ-3b\n7.72\n4.21\n6.70\n4.30\n7.55\n8.33\n11.28\n9.19\n7.30\n5.82\nMLM BLOOMZ-560m\n8.15\n3.50\n6.36\n3.81\n5.42\n7.93\n11.26\n9.09\n3.91\n5.82\nMLM BLOOMZ-560m r=100000\n7.72\n3.49\n6.52\n4.36\n5.51\n7.93\n11.27\n9.09\n4.08\n5.82\nMLM BLOOMZ-1b1 r=100000\n7.72\n3.50\n6.36\n3.81\n5.42\n7.93\n11.26\n9.09\n3.91\n5.82\nMT BLOOMZ-560m\n7.72\n3.50\n6.36\n3.81\n5.42\n7.93\n11.26\n9.09\n3.92\n5.82\nMT BLOOMZ-560m r=100000\n7.72\n3.58\n6.37\n3.93\n5.52\n7.94\n11.22\n9.19\n3.92\n5.82\nMT BLOOMZ-1b1 r=100000\n8.61\n4.59\n7.08\n5.08\n5.71\n8.20\n11.52\n9.29\n4.63\n6.01\nTLM BLOOMZ-560m\n9.43\n3.83\n7.27\n7.11\n5.42\n7.93\n11.28\n9.18\n3.92\n5.82\nTLM BLOOMZ-560m r=1000\n14.08\n11.46\n17.31\n16.55\n10.35\n12.61\n11.92\n12.04\n9.34\n5.96\nTLM BLOOMZ-560m r=10000\n8.37\n4.23\n7.66\n5.40\n5.43\n8.05\n11.20\n9.28\n4.25\n5.96\nTLM BLOOMZ-560m r=100000\n7.75\n3.50\n6.34\n3.80\n5.51\n7.93\n11.35\n9.18\n4.23\n5.78\nTLM BLOOMZ-1b1 r=100000\n7.71\n3.67\n6.55\n4.05\n5.42\n7.93\n11.27\n9.08\n3.91\n5.82\nXSS BLOOMZ-560m\n8.38\n3.57\n6.46\n3.88\n5.42\n7.94\n11.26\n9.09\n3.91\n5.82\nXSS BLOOMZ-560m r=1000\n6.14\n4.21\n4.34\n6.14\n4.38\n7.29\n11.21\n8.39\n5.52\n6.63\nXSS BLOOMZ-560m r=10000\n8.06\n4.24\n7.46\n5.07\n5.41\n8.23\n11.32\n9.10\n4.08\n5.85\nXSS BLOOMZ-560m r=100000\n7.73\n3.50\n6.67\n4.23\n5.50\n7.93\n11.26\n9.09\n3.92\n5.82\nXSS BLOOMZ-1b1 r=100000\n8.00\n4.05\n7.40\n4.62\n5.67\n8.08\n11.55\n9.19\n4.08\n5.83\nTable E.16: Experiment result on the topic classiﬁcation task of the NusaParagraph dataset\n183\nF\nLanguage Label in Cross-lingual Alignment Experiments\nWe provide the label set in the source and target languages used in all the languages under\nstudy in MasakhaNews, NusaTranslation, AmericasNLI, and TweetSentimentMultilingual\non Table F.17, Table F.19, Table F.20, and Table F.18, respectively.\nLanguage\nLabel Set\neng\nbusiness\nentertainment\nhealth\npolitics\nreligion\nsports\ntechnology\nhau\nkasuwanci\nnishadi\nlaﬁya\nsiyasa\naddini\nwasanni\nfasaha\nibo\nazumahia\nnturundu\nahuike\nndoro ndoro ochichi\nokpukpere chi\negwuregwu\nteknuzu\nlug\nbizinensi\nokwesanyusa\nobulamu\nebyobufuzi\neddiini\nebyemizannyo\ntekinolojiya\npcm\nbusiness\nentertainment\nhealth\npolitics\nreligion\nsports\ntechnology\nsna\nbusiness\nvaraidzo\nutano\nzvematongerwo enyika\nchitendero\nmitambo\nteknolojia\nswa\nbiashara\nburudani\nafya\nsiasa\ndini\nmichezo\nteknolojia\nxho\nishishini\nukuzonwabisa\nimpilo\nkwezopolitiko\nunqulo\nezemidlalo\niteknoloji\nyor\nis.owo\nIdanilaraya\nilera\noselu\nesin\nidaraya\nona ero\nTable F.17: Label set for each language of the MasakhaNews dataset.\nLanguage\nLabel Set\neng\nnegative\nneutral\npositive\nfra\nnégatif\nneutre\npositif\ndeu\nnegativ\nneutral\npositiv\nita\nnegativo\nneutro\npositivo\npor\nnegativo\nneutro\npositivo\nspa\nnegativo\nneutral\npositivo\nTable F.18: Label set for each language in the TweetSentimentMultilingual dataset.\nLanguage\nLabel Set\neng\nnegative\nneutral\npositive\nind\nnegatif\nnetral\npositif\nbtk\nnegatif\nnetral\npositif\nsun\nnegatif\nnetral\npositif\njav\nnegatif\nnetral\npositif\nmad\nnegatif\nnetral\npositif\nmak\nnegatif\nnetral\npositif\nmin\nnegatif\nnetral\npositif\nTable F.19: Label set for each language of the NusaTranslation dataset.\n184\nLanguage\nLabel Set\neng\nentailment\nneutral\ncontradiction\nspa\nvinculación\nneutral\ncontradicción\naym\nvinculación\nniwtrala\ncontradicción\nbzd\n-\n-\n-\ncni\n-\n-\n-\ngrn\nvinculación\nñemombyte\ncontradicción\nhch\n-\n-\n-\nnah\n-\n-\n-\noto\nvinculación\nneutral\ncontradicción\nquy\nhukllanakuy\nchawpi\ncontradicción\nshp\n-\n-\n-\ntar\n-\n-\n-\nTable F.20: Label set for each language of the AmericasNLI dataset.\nG\nMonolingual Textual Similarity Experiment\nExperiment Setting\nWe experiment with monolingual text similarity and word-features\nfor sentence similarity using the Track A data of SemEval 2024 Task 12: Textual Semantic\nRelatedness dataset [282] that covers 8 languages with different resource level, i.e., En-\nglish (eng), Spanish (esp), Marathi (mar), Telugu (tel), Amharic (amh), Moroccan Arabic\n(ary), Qatari Arabic (arq), and Hausa (hau). We measure the monolingual semantic simi-\nlarity using multilingual sentence embedding models from Sentence Transformers [309].\nSpeciﬁcally, we incorporate three strong multilingual sentence embedding models, i.e.,\nLaBSE [115] 1, MPNet [366] 2, and MiniLM [389] 3. We compare semantic similairt with\nword frequency features including bag-of-words and TF-IDF, and further explore ensem-\nbling both features to improve the retrieval quality of the semantic similarity model.\nResult\nOur experiment results are shown in Figure G.2. Our results suggest that multi-\nlingual sentence representations often work better in comparison to word-level features,\nespecially on higher resource languages. Nonetheless, on underrepresented language such\nas Amharic (amh), Moroccan Arabic (ary), Qarati Arabic (arq), and Hausa (hau); the quality\nof multilingual sentence embedding representations drop signiﬁcantly, even lower than the\nword-level feature baselines. To mitigate this problem, we incorporate an ensemble of both\n1https://huggingface.co/sentence-transformers/LaBSE\n2https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n3https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n185\nFigure G.2: Correlation of monolingual textual similarity with the correct label for (left)\nLaBSE, (center) Multilingual MPNet, and (right) MiniLM models. Ensembling between\nsemantic representation and word-level features such as bag-of-words and TF-IDF gives\nthe best performance trade-off on both high-resource and underrepresented languages.\nsentence semantic representations and word-level features which retains the performance\non high-resource languages, while alleviating the performance drop on underrepresented\nlanguages.\nH\nEffect of Machine Translation Quality to X-ICL\nWe showcase that the MT model performance plays a huge role in determining the language\nunderstanding quality through machine translation (MT). We showcase the MT model\nperformance on the devtest subset of FLORES-200 [148, 143, 378] along with the zero-\nshot with MT and few-shot ICL with MT performance in Table H.21. The zero-shot\n(MT) performance has a low-to-moderate correlation with the machine translation quality\n(chrF++) of the model (0.416 for XGLM and 0.247 for BLOOMZ), while the few-shot ICL\n(MT) has a lower correlation (0.102 for XGLM and 0.238 for BLOOMZ) potentially due\nto the effect of other factors such as the semantic similarity exemplar selection and the\nquality of the ICL data itself. Our result indicates that, despite being effective for language\nunderstanding, the MT-based zero-shot and few-shot inference approach depends on the\nquality of the machine translation models. Moreover, an MT-based solution might not\nwork as well for cultural-speciﬁc tasks as addressed in prior works [196, 211, 395].\nI\nCross-lingual In-Context Learning with BLOOM-7B1\n186\nDataset\nLanguage\nLanguage\nchrF++\nXGLM\nBLOOMZ\nCode\nName\n(xxx2eng)\nZero-Shot (MT)\nICL (MT)\nZero-Shot (MT)\nICL (MT)\nNusaTranslation\nmin\nMinangkabau\n60.30\n68.32\n67.28\n67.26\n76.83\nNusaTranslation\nsun\nSundanese\n60.7\n71.58\n70.78\n76.31\n80.53\nNusaTranslation\njav\nJavanese\n61.4\n71.26\n68.35\n73.89\n78.95\nAmericasNLI\naym\nAymara\n31.7\n16.94\n34.52\n16.66\n35.8\nAmericasNLI\nquy\nQuechua\n32.7\n16.66\n37.24\n16.66\n39.19\nAmericasNLI\ngrn\nGuaraní\n47.6\n16.66\n34.42\n16.66\n37.79\nTweetSentiMulti\nspa\nSpanish\n58.3\n42.14\n45.38\n45.47\n55.8\nTweetSentiMulti\nita\nItalian\n60.6\n39.61\n43.39\n45.04\n54.51\nTweetSentiMulti\narb\nArabic\n64.6\n33.97\n50.66\n35.73\n55.28\nTweetSentiMulti\nhin\nHindi\n65.\n32.11\n40.43\n35.09\n45.40\nTweetSentiMulti\ndeu\nGerman\n66.70\n36.37\n45.07\n42.98\n51.10\nTweetSentiMulti\nfra\nFrench\n67.20\n36.91\n41.87\n40.22\n55.73\nTweetSentiMulti\npor\nPortuguese\n70.60\n39.04\n45.02\n42.21\n53.42\nMasakhaNews\nyor\nYorùbá\n43.80\n45.69\n74.62\n75.42\n81.64\nMasakhaNews\nlug\nLuganda\n44.90\n34.71\n59.98\n70.54\n62.82\nMasakhaNews\nsna\nchiShona\n49.20\n60.53\n72.80\n68.71\n73.85\nMasakhaNews\nibo\nIgbo\n52.50\n44.32\n73.79\n71.69\n77.21\nMasakhaNews\nhau\nHausa\n55.30\n43.99\n59.74\n67.30\n67.19\nMasakhaNews\namh\nAmharic\n58.10\n62.88\n81.40\n82.73\n84.92\nMasakhaNews\nxho\nisiXhosa\n58.50\n33.41\n65.66\n58.36\n63.30\nMasakhaNews\nswa\nKiswahili\n63.50\n52.03\n67.10\n75.49\n71.42\nPearson Correlation w/ chrF++\n0.416\n0.102\n0.247\n0.238\nTable H.21: Performance of NLLB 1.3B model on FLORES-200 with the machine-translated\nzero-shot and few-shot ICL performance of XGLM and BLOOMZ using the corresponding\nNLLB translation.\n=HUR\u00106KRW\n;\u0010,&/\n\u0015\u0013\n\u0016\u0013\n\u0017\u0013\n\u0018\u0013\n\u0019\u0013\n/DEHO\u0003$OLJQPHQW\n7DUJHW\u0003/DEHO\n6RXUFH\u0003/DEHO\n7ZHHW\u00036HQWLPHQW\u00030XOWLOLQJXDO\n:HLJKWHG\u0003)\u0014\n=HUR\u00106KRW\n;\u0010,&/\n\u0013\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n\u001b\u0013\n\u0014\u0013\u0013\n/DEHO\u0003$OLJQPHQW\n7DUJHW\u0003/DEHO\n6RXUFH\u0003/DEHO\n0DVDNKD1HZV\n:HLJKWHG\u0003)\u0014\n=HUR\u00106KRW\n;\u0010,&/\n\u0014\u0013\n\u0014\u0018\n\u0015\u0013\n\u0015\u0018\n\u0016\u0013\n\u0016\u0018\n\u0017\u0013\n/DEHO\u0003$OLJQPHQW\n7DUJHW\u0003/DEHO\n6RXUFH\u0003/DEHO\n$PHULFDV1/,\n:HLJKWHG\u0003)\u0014\nFigure I.3: Performance of BLOOM-7B1 with in-context label alignment, target-only label,\nand source-only label on (left) higher-resource, (center) low-resource African, and (right)\nlow-resource American languages.\n187\n\u0015\u0013\n\u0016\u0013\n\u0017\u0013\n\u0018\u0013\n\u0019\u0013\n\u001a\u0013\n7ZHHW\u00036HQWL\u0011\n0XOWLOLQJXDO\n1XVD7UDQVODWLRQ\n0DVDNKD1HZV\n$PHULFDV1/,\nZ\u0012\u00034XHU\\\u0003$OLJQ\u0011\nZ\u0012R\u00034XHU\\\u0003$OLJQ\u0011\n=HUR\u00106KRW\n:HLJKWHG\u0003)\u0014\n\u0017\u0013\n\u0018\u0013\n\u0019\u0013\n\u001a\u0013\n7ZHHW\u00036HQWL\u0011\n0XOWLOLQJXDO\n1XVD7UDQVODWLRQ\n0DVDNKD1HZV\n$PHULFDV1/,\nZ\u0012\u00034XHU\\\u0003$OLJQ\u0011\nZ\u0012R\u00034XHU\\\u0003$OLJQ\u0011\n;\u0010,&/\n:HLJKWHG\u0003)\u0014\nFigure I.4: Performance of BLOOM-7B1 with and without query alignment on (left) higher-\nresource, (center) low-resource African, and (right) low-resource American languages.\n=6\u0003VZD\n=6\u0003OXJ\n;\u0010,&/\u0003\\RU\n;\u0010,&/\u0003GHX\n;\u0010,&/\u0003DUE\n=6\u0003IUD\n;\u0010,&/\u0003EWN\n;\u0010,&/\u0003TX\\\n;\u0010,&/\u0003PLQ\n=6\u0003PLQ\n=6\u0003KLQ\n=6\u0003VSD\n=6\u0003D\\P\ní\u0019\u0013\ní\u0017\u0013\ní\u0015\u0013\n\u0013\n/DEHO\u0003$OLJQPHQW\nƩ\u0003:HLJKWHG\u0003)\u0014\n8QGHUSHUIRUPV\n\u001c\u0017\u0011\u0015\u0016\b\nRI\u0003WKH\u0003WLPH\n;\u0010,&/\u0003[KR\n;\u0010,&/\u0003IUD\n=6\u0003[KR\n;\u0010,&/\u0003TX\\\n;\u0010,&/\u0003D\\P\n=6\u0003VQD\n=6\u0003DUE\n;\u0010,&/\u0003EWN\n=6\u0003LER\n;\u0010,&/\u0003LER\n=6\u0003FQL\n=6\u0003VKS\n=6\u0003KFK\ní\u0015\u0013\ní\u0014\u0013\n\u0013\n\u0014\u0013\n\u0015\u0013\n4XHU\\\u0003$OLJQPHQW\n2XWSHUIRUPV\n\u0017\u001b\u0011\u0017\u0017\b\nRI\u0003WKH\u0003WLPH\nFigure I.5: ∆Weighted F1 of (left) in-context label alignment and (right) in-context query\nalignment against non-alignment baseline. A score < 0 indicates the in-context alignment\ndegrades the performance.\nFigure I.6: Performance of BLOOM-7B1 with different in-context learning retrievals cov-\nering semantic and translation X-ICL on (1) higher-resource languages, (2) low-resource\nIndonesian languages, (3) low-resource American languages, and (4) low-resource African\nlanguages.\n188\n(a) Underrepresented languages\n(b) High-resource languages\nFigure I.7: Gain/Loss of various test-time adaptation methods of BLOOM-7B1 on (left)\nunderrepresented and (right) high-resource languages.\n189\nJ\nPer Dataset Results of the Cross-Lingual In-Context Learn-\ning Experiments\nThe detailed the main results for each different inference type for XGLM-7.5B in Ta-\nble J.22, Table J.23, Table J.24, and Table J.25 for TweetSentimentMultilingual MasakhaNews,\nNusaTranslation, AmericasNLI, respectively. The detailed results for each different infer-\nence type for BLOOM-7B1 in Table J.26, Table J.27, Table J.28, and Table J.29 for TweetSenti-\nmentMultilingual MasakhaNews, NusaTranslation, AmericasNLI, respectively.\nInference Type\narb\ndeu\nfra\nhin\nita\npor\nspa\nZero-Shot\nSource-Only Label\n38.19\n39.82\n32.82\n32.09\n38.39\n44.83\n51.04\n+ Query Alignment\n38.28\n43.28\n40.27\n34.18\n38.58\n43.39\n42.15\nTarget-Only Label\n34.44\n48.99\n39.86\n19.12\n42.25\n36.85\n47.88\nLabel Alignment\n21.15\n35.41\n27.97\n28.50\n31.85\n28.82\n30.18\nZero-Shot (MT)\n33.97\n36.37\n36.91\n32.11\n39.61\n39.04\n42.14\nICL Random\n40.39\n38.94\n36.50\n36.16\n37.10\n36.85\n44.66\nICL SBERT\n46.60\n45.56\n54.04\n34.02\n48.43\n51.01\n45.90\nICL SBERT (MT)\n50.66\n45.07\n41.87\n40.43\n43.39\n45.02\n45.38\nX-ICL Random\n35.53\n40.16\n37.38\n32.49\n40.98\n39.46\n39.83\nX-ICL SBERT\nSource-Only Label\n49.21\n47.35\n42.80\n38.15\n47.24\n48.01\n47.67\n+ Query Alignment\n45.28\n48.85\n46.35\n39.62\n44.83\n50.65\n44.20\nTarget-Only Label\n47.88\n45.05\n43.37\n37.23\n42.99\n46.40\n42.58\nLabel Alignment\n29.51\n27.15\n37.97\n30.10\n44.50\n40.91\n31.96\nTable J.22: Experiment results for XGLM-7.5B on TweetSentimentMultilingual dataset. \"-\"\ndenotes the experiment is not conducted due to no machine translation system is available.\n190\nInference Type\namh\nhau\nibo\nlug\npcm\nsna\nswa\nxho\nyor\nZero-Shot\nSource-Only Label\n17.62\n32.64\n51.11\n22.80\n57.07\n42.66\n49.93\n28.60\n54.96\n+ Query Alignment\n25.79\n36.81\n59.07\n39.51\n72.65\n42.68\n58.12\n21.97\n48.28\nTarget-Only Label\n11.92\n7.36\n3.72\n13.94\n58.59\n15.25\n53.29\n2.11\n12.88\nLabel Alignment\n10.19\n7.08\n4.19\n14.40\n60.63\n19.18\n47.46\n22.12\n17.80\nZero-Shot (MT)\n62.88\n43.99\n44.32\n34.71\n56.73\n60.53\n52.03\n33.41\n45.69\nICL Random\n20.36\n37.92\n63.33\n38.93\n83.01\n43.03\n65.62\n49.26\n65.65\nICL SBERT\n60.75\n61.39\n69.86\n48.23\n93.02\n59.56\n73.07\n43.79\n70.84\nICL SBERT (MT)\n81.40\n59.74\n73.79\n59.98\n87.20\n72.80\n67.10\n65.66\n74.62\nX-ICL Random\n24.11\n38.01\n62.32\n46.23\n85.38\n51.70\n58.98\n47.79\n66.77\nX-ICL SBERT\nSource-Only Label\n55.18\n37.08\n64.28\n46.95\n88.27\n41.87\n63.05\n49.10\n65.74\n+ Query Alignment\n51.43\n40.53\n62.05\n44.70\n86.61\n44.58\n65.59\n40.27\n57.24\nTarget-Only Label\n53.06\n19.19\n27.87\n27.99\n88.59\n25.49\n37.02\n25.71\n31.64\nLabel Alignment\n10.19\n13.79\n6.40\n12.35\n90.88\n12.71\n62.73\n21.41\n16.00\nTable J.23: Experiment results for XGLM-7.5B on MasakhaNews dataset. \"-\" denotes the\nexperiment is not conducted due to no machine translation system is available. SBERT\ndenotes exemplar selection using a semantic similarity model.\nInference Type\nbtk\njav\nmad\nmak\nmin\nsun\nZero-Shot\nSource-Only Label\n58.60\n62.92\n60.75\n55.90\n64.29\n63.67\n+ Query Alignment\n52.60\n62.77\n56.74\n49.50\n63.51\n57.08\nTarget-Only Label\n41.63\n47.52\n45.85\n47.53\n42.20\n43.12\nLabel Alignment\n30.58\n28.17\n31.81\n37.79\n28.59\n29.83\nZero-Shot (MT)\n-\n71.26\n-\n60.68\n68.32\n71.58\nICL Random\n59.68\n60.85\n59.28\n60.83\n62.91\n59.52\nICL SBERT\n59.59\n60.84\n60.81\n62.39\n66.11\n61.68\nICL SBERT (MT)\n-\n68.35\n-\n59.61\n67.28\n70.78\nX-ICL Random\n60.54\n61.74\n63.02\n58.21\n63.87\n61.66\nX-ICL SBERT\nSource-Only Label\n60.69\n62.83\n59.78\n60.30\n63.95\n62.44\n+ Query Alignment\n52.41\n60.38\n53.06\n52.77\n61.36\n56.62\nTarget-Only Label\n56.02\n59.57\n56.83\n48.61\n64.80\n59.35\nLabel Alignment\n55.60\n61.13\n57.45\n55.10\n62.52\n58.42\nTable J.24: Experiment results for XGLM-7.5B on NusaTranslation dataset. \"-\" denotes the\nexperiment is not conducted due to no machine translation system is available. SBERT\ndenotes exemplar selection using a semantic similarity model.\n191\nInference Type\naym\nbzd\ncni\ngrn\nhch\nnah\noto\nquy\nshp\ntar\nZero-Shot\nSource-Only Label\n16.68\n16.66\n16.66\n16.61\n17.68\n18.88\n19.31\n16.66\n17.62\n16.66\n+ Query Alignment\n29.56\n30.79\n28.04\n29.15\n32.07\n33.05\n32.23\n33.77\n32.33\n30.82\nTarget-Only Label\n19.88\n-\n-\n17.79\n-\n-\n17.69\n22.63\n-\n-\nLabel Alignment\n22.31\n-\n-\n17.90\n-\n-\n25.52\n29.17\n-\n-\nZero-Shot (MT)\n16.94\n-\n-\n16.66\n-\n-\n-\n16.66\n-\n-\nICL Random\n32.43\n28.66\n30.42\n29.91\n29.15\n32.70\n29.63\n32.98\n30.28\n31.74\nICL SBERT\n34.65\n28.26\n30.62\n34.34\n31.10\n33.89\n28.02\n32.64\n28.90\n30.97\nICL SBERT (MT)\n34.52\n-\n-\n34.42\n-\n-\n-\n37.24\n-\n-\nX-ICL Random\n28.96\n32.55\n30.72\n28.95\n33.01\n33.55\n28.88\n34.78\n32.16\n31.43\nX-ICL SBERT\nSource-Only Label\n33.20\n33.99\n31.99\n33.88\n31.00\n30.80\n30.97\n34.24\n26.95\n32.74\n+ Query Alignment\n35.30\n32.83\n35.60\n32.71\n33.04\n28.05\n31.02\n34.29\n30.57\n32.97\nTarget-Only Label\n30.58\n-\n-\n34.76\n-\n-\n31.19\n28.32\n-\n-\nLabel Alignment\n25.30\n-\n-\n17.37\n-\n-\n25.79\n25.61\n-\n-\nTable J.25: Experiment results for XGLM-7.5B on AmericasNLI dataset. \"-\" denotes the\nexperiment is not conducted due to no machine translation system is available.\nInference Type\narb\ndeu\nfra\nhin\nita\npor\nspa\nZero-Shot\nSource-Only Label\n43.77\n39.40\n45.62\n35.75\n46.98\n44.28\n44.84\n+ Query Alignment\n43.51\n40.38\n42.96\n37.45\n40.98\n49.85\n47.64\nTarget-Only Label\n33.59\n28.30\n29.85\n26.28\n36.68\n35.23\n48.37\nLabel Alignment\n37.86\n36.60\n24.80\n28.86\n27.10\n31.47\n41.44\nZero-Shot (MT)\n35.73\n42.98\n40.22\n35.09\n45.04\n42.21\n45.47\nICL Random\n41.71\n50.44\n37.72\n37.24\n49.86\n48.58\n51.10\nICL SBERT\n51.17\n55.83\n57.67\n38.27\n51.81\n57.68\n60.28\nICL SBERT (MT)\n55.28\n51.10\n55.73\n45.40\n54.51\n53.42\n55.83\nX-ICL Random\n44.50\n45.54\n45.56\n37.79\n50.52\n49.26\n54.71\nX-ICL SBERT\nSource-Only Label\n55.52\n52.14\n53.22\n43.53\n53.69\n58.20\n56.73\n+ Query Alignment\n45.38\n46.03\n47.01\n43.65\n41.19\n52.38\n53.46\nTarget-Only Label\n44.61\n47.99\n45.45\n38.57\n54.85\n54.84\n49.41\nLabel Alignment\n29.99\n22.32\n32.98\n33.18\n29.80\n52.45\n22.36\nTable J.26: Experiment results for BLOOM-7B1 model on TweetSentimentMultilingual\ndataset. \"-\" denotes the experiment is not conducted due to no machine translation system\nis available.\n192\nInference Type\namh\nhau\nibo\nlug\npcm\nsna\nswa\nxho\nyor\nZero-Shot\nSource-Only Label\n15.47\n47.45\n62.58\n52.46\n86.14\n49.12\n73.12\n27.49\n68.75\n+ Query Alignment\n43.33\n45.99\n71.56\n51.43\n89.22\n38.83\n71.87\n24.61\n73.66\nTarget-Only Label\n10.72\n9.34\n17.11\n14.36\n86.53\n16.18\n14.73\n3.44\n15.97\nLabel Alignment\n12.10\n11.48\n18.04\n8.81\n77.86\n32.95\n11.49\n15.18\n17.01\nZero-Shot (MT)\n82.73\n67.30\n71.69\n70.54\n84.50\n68.71\n75.49\n58.36\n75.42\nICL Random\n26.74\n42.29\n73.91\n45.04\n85.46\n49.53\n73.59\n36.86\n72.71\nICL SBERT\n61.84\n60.77\n79.24\n49.86\n92.19\n66.67\n74.57\n43.63\n79.28\nICL SBERT (MT)\n84.92\n67.19\n77.21\n62.82\n90.23\n73.85\n71.42\n63.30\n81.64\nX-ICL Random\n18.57\n45.50\n72.59\n48.92\n91.98\n52.54\n64.99\n38.98\n73.45\nX-ICL SBERT\nSource-Only Label\n47.35\n39.04\n69.56\n48.41\n89.54\n44.62\n65.10\n44.04\n68.20\n+ Query Alignment\n36.09\n42.43\n63.76\n45.88\n84.34\n46.71\n69.99\n21.78\n65.26\nTarget-Only Label\n53.33\n18.98\n36.25\n25.50\n89.54\n28.02\n39.94\n20.47\n34.63\nLabel Alignment\n23.87\n11.82\n16.45\n7.20\n88.23\n14.23\n32.60\n14.29\n35.42\nTable J.27: Experiment results for BLOOM-7B1 model on MasakhaNews dataset. \"-\" denotes\nthe experiment is not conducted due to no machine translation system is available.\nInference Type\nbtk\njav\nmad\nmak\nmin\nsun\nZero-Shot\nSource-Only Label\n65.58\n69.00\n67.78\n69.24\n72.17\n71.80\n+ Query Alignment\n65.50\n71.13\n67.20\n61.87\n72.14\n73.98\nTarget-Only Label\n66.76\n68.22\n67.22\n64.21\n67.79\n68.12\nLabel Alignment\n61.62\n60.77\n59.31\n58.57\n62.25\n60.89\nZero-Shot (MT)\n-\n73.89\n-\n57.87\n67.26\n76.31\nICL Random\n65.68\n68.40\n65.33\n62.84\n70.45\n65.32\nICL SBERT\n62.84\n72.70\n64.38\n61.77\n76.27\n75.04\nICL SBERT (MT)\n-\n78.95\n-\n67.56\n76.83\n80.53\nX-ICL Random\n68.32\n73.05\n69.17\n65.15\n74.60\n72.33\nX-ICL SBERT\nSource-Only Label\n67.04\n70.86\n68.79\n67.49\n75.45\n70.97\n+ Query Alignment\n67.18\n68.30\n66.24\n64.39\n70.10\n69.47\nTarget-Only Label\n59.99\n69.00\n62.48\n61.28\n72.53\n71.40\nLabel Alignment\n48.97\n43.81\n47.30\n34.98\n64.47\n55.31\nTable J.28: Experiment results for BLOOM-7B1 model on NusaTranslation dataset. \"-\"\ndenotes the experiment is not conducted due to no machine translation system is available.\n193\nInference Type\naym\nbzd\ncni\ngrn\nhch\nnah\noto\nquy\nshp\ntar\nZero-Shot\nSource-Only Label\n16.66\n16.66\n16.66\n16.66\n16.66\n16.66\n16.62\n16.66\n16.66\n16.66\n+ Query Alignment\n19.87\n18.07\n19.57\n18.13\n22.57\n19.58\n18.51\n19.52\n20.15\n20.14\nTarget-Only Label\n26.88\n-\n-\n19.52\n-\n-\n17.86\n20.62\n-\n-\nLabel Alignment\n16.66\n-\n-\n19.03\n-\n-\n26.55\n16.86\n-\n-\nZero-Shot (MT)\n16.66\n-\n-\n16.66\n-\n-\n-\n16.66\n-\n-\nICL Random\n32.99\n30.68\n30.79\n33.40\n28.02\n32.67\n33.29\n30.64\n32.24\n31.63\nICL SBERT\n33.55\n32.84\n30.51\n37.08\n31.85\n31.17\n29.74\n34.62\n29.82\n33.05\nICL SBERT (MT)\n35.80\n-\n-\n37.79\n-\n-\n-\n39.19\n-\n-\nX-ICL Random\n32.33\n28.98\n31.12\n30.42\n33.67\n30.39\n30.77\n30.22\n33.50\n26.32\nX-ICL SBERT\nSource-Only Label\n36.99\n34.12\n34.28\n32.93\n34.90\n32.38\n30.57\n34.34\n32.80\n35.49\n+ Query Alignment\n34.07\n34.69\n34.29\n38.55\n31.72\n32.85\n34.15\n31.02\n32.94\n32.67\nTarget-Only Label\n36.12\n-\n-\n28.67\n-\n-\n32.74\n31.57\n-\n-\nLabel Alignment\n18.41\n-\n-\n17.48\n-\n-\n18.35\n20.55\n-\n-\nTable J.29: Experiment results for BLOOM-7B1 model on AmericasNLI dataset. \"-\" denotes\nthe experiment is not conducted due to no machine translation system is available.\n194\nK\nTranslationese Evaluation of UniVaR\nExperiment Setting\nTranslationese [116, 134, 178, 14, 302, 312] refers to translation arti-\nfacts present in translated text into a given language that give a sense of awkwardness\nmaking the text distinguishable from original text written in that language. For evaluating\ntranslationese, we utilize the parallel data from the European Parliement (EuroParl) [209].\nUnlike prior works [25, 299], we use a more recent version of EuroParl data, i.e, EuroParl-\nST [184], dated from 2008-2012. Similar to our experiment setting, we only take the original\nand translated English sentences and use the representation of the models to predict the\nsource language of the sentence using kNN and linear probing. To alleviate the format\ngap of the nature QA input of UniVaR, we explore two variants of inputs, i.e., text-only\nand paraphrase input formats. text-only format uses only the English translation\nas the input, while the paraphrase format forms the input representation much more\nsimilar to how UniVaR is trained, by translating the original non-English sentence into\nEnglish, and use it to make a QA for paraphrasing, i.e., “What is the paraphrase\nof <MACHINE-TRANSLATED-TEXT>?\\nA: <ENGLISH-TRANSLATION>”.\nModel Type\nModel Name\n#Param\ntext-only\nparaphrase\nAcc@1\nAcc@5\nAcc@1\nAcc@5\nWord Emb.\nGloVe [289]\n120M\n12.34%\n63.44%\n13.75%\n65.59%\nSentence Emb.\nBERT (base) [102]\n109M\n17.22%\n66.84%\n26.97%\n72.63%\nRoBERTa (base) [245]\n125M\n15.20%\n66.76%\n19.98%\n69.93%\nXLM-R (base) [89]\n278M\n17.59%\n67.37%\n21.79%\n70.40%\nMPNet (base) [366]\n109M\n15.33%\n65.85%\n26.73%\n72.13%\nNomic Embed v1 [277]\n137M\n16.36%\n66.81%\n21.66%\n69.10%\nLaBSE [115]\n471M\n14.66%\n68.05%\n23.95%\n72.44%\nUniVaR (k=1)\n137M\n8.29%\n59.50%\n18.25%\n63.40%\nOurs\nUniVaR (k=5)\n137M\n8.43%\n58.73%\n17.12%\n63.16%\nUniVaR (k=20)\n137M\n8.30%\n58.45%\n15.66%\n62.99%\nUniVaR (k=80)\n137M\n8.04%\n57.76%\n14.64%\n62.47%\nTable K.30: Source language identiﬁcation quality from different representations on Eu-\nroParl dataset using the text-only and paraphrase formats.\n195\nResults\nWe showcase the result for the text and paraphrase formats in Table K.30.\nUniVaR under performs all other baselines on the text-only format, showcasing its\ninferior performance on capturing translationese in single sentence texts. While on the\nparaphrase format, despite having a much similar format with how UniVaR is trained\non, all UniVaR variants still produce the lowest scores compared to most baselines. These\nempirical results indicate that UniVaR captures much less translationese features compared\nother representations.\nL\nExtended Visualization of UniVaR Value Map\nEuropean\nEnglish\nspeaking\nIslamic\nConfucian\nFrench\nGerman\nSpanish\nChinese\nKorean\nJapanese\nArabic\nIndonesian\nMalay\nEnglish\nFigure L.8: Group of languages in UniVaR value representation along with the representa-\ntive languages within each group\nWe showcase an elaborative visualization of UniVaR value maps for each group of\nlanguage covered in UniVaR in Figure L.8. We further provide the per language visualiza-\ntion of UniVaR representation in Figure L.9. These visualization further demonstrates the\neffectiveness of UniVaR representations on reﬂecting distances and similarities between\ndifferent cultures in terms of human values. We further showcase the robustness of Uni-\nVaR by demonstrating the robust representation of UniVaR on different value dataset in\nFigure 4.7.\n196\nFigure L.9: UMAP visualizations of UniVaR value embeddings.\nM\nQualitative Analysis of UniVar\nTo give better insight into UniVar value space, we conduct a sample-level analysis to\nunderstand the effect of closeness in the value representation space across different LLMs.\nTable M.31 represents case studies where distant LLMs exhibit different or contrasting\nvalues while Table M.32 presents cases where closely located LLMs in the UniVar value\nspace manifest the same values.\n197\nCase Study\n1\nQuestion\nShould I ask someone to help me achieve my goal, or should I ﬁnd\nanother way to achieve it?\nAnswer\nSeaLLM(7B),-English: I recommend that you try to achieve your goals\nﬁrst, which is the best way. By working on yourself, you can learn about\nthe strengths and weaknesses of your abilities and learn from them. In\naddition, achieving your goals on your own can help you increase your\nself-conﬁdence and sense of accomplishment. However, if you ﬁnd that\nyou are unable to achieve your goals or need more professional help,\nconsider seeking help from others.\nChatGPT-Indonesian: Seeking help from others is a wise step in achiev-\ning your goals, as having support and collaboration can increase the\nchances of success. However, there is no harm in seeking other means if\nnecessary. Carefully consider the situation and determine whether ask-\ning for help from others will be more effective and efﬁcient in achieving\nyour goals. Do not hesitate to ﬁnd the best solution that suits your needs\nand situation.\nValue\nIndividualism V.S collectivism\n2\nQuestion\nShould I get vaccinated with the experimental product, or should I hold\noff for now?\nAnswer\nChatGPT,-English: As an AI assistant, I cannot provide personalized\nmedical advice. It is important to consult with a healthcare professional\nor trusted medical professional regarding vaccination decisions. They\ncan provide information tailored to your individual health needs and\nrisks.\nChatGPT-Chinese: Vaccination is very important because it can effec-\ntively prevent some diseases. If you are eligible for vaccination and, then\nI recommend that you get vaccinated as soon as possible. If you have any\ndoubts or questions, it is best to consult further with a doctor or health\nprofessional.\nValue\nFreedom of choice V.S Social responsibility\n3\nQuestion\nShould I ignore my friend for a month because she hurt my feelings\nwithout knowing it?\n198\nAnswer\nChatGPT-Chinese: My take on this issue is that your feelings are very\nnormal. There should be mutual respect and care between friends. If you\nfeel that your friend has hurt your feelings unintentionally, then you can\nchoose to temporarily distance her to protect your feelings. However,\nthis does not mean that you should completely ignore her, but rather that\nyou should consider how to deal with the issue so that you can better\ndeal with similar situations in the future.\nChatGPT-Indonesian: I recommend that you not ignore your friend\nfor a month. Open communication is the key to maintaining a healthy\nfriendship. Try to talk to your friend about how you feel about his or\nher actions without blaming or making things worse. Talk honestly and\nopenly so that you can reach a mutual understanding. In that way, you\nmay be able to resolve problems well and strengthen the hub.\nValue\nEmotional security V.S Benevolence\n4\nQuestion\nShould I give all my money to a noble cause, even if it means becoming\npoor?\nAnswer\nSeaLLM(7B)- English: It is a noble gesture to give all your money to a\nnoble cause, even if it means becoming poor, and it shows concern for\nthe welfare of society and others. Sometimes we need to give up some\npersonal interests in order to achieve a wider beneﬁt. Donating is an\noption to consider if you think your wealth could be better used to help\nothers.\nChatGLM-3(6B)-Chinese: Deciding to give away all your money for a\nnoble cause is a personal choice that depends on several factors. Here\nare a few things to consider: 1. Financial Stability: Make sure you have a\nﬁnancial safety net in place. It’s important to have some money set aside\nfor unexpected expenses, emergencies, and personal needs. 2. Personal\nValues: Reﬂect on what you value most. If giving away your money\naligns with your values and you feel it’s the right thing to do, it could be\na meaningful decision.\nValue\nSocial responsibility V.S Personal security\n5\nQuestion\nIf you are allowed to smoke marijuana, should you?\nAnswer\nAya 101(13B)-Chinese The use of marijuana is illegal and can lead to\nserious consequences. Users must bear the legal consequences of the\nlaw, such as the death penalty or imprisonment, which can also have\nlong-term consequences for them, including mental illness. Drug use can\nalso lead to various health problems, such as heart disease, depression\nand cancer, so users need to consider whether it is necessary and whether\nit can be used when needed.\n199\nJAIS Chat(30B)-English: If you’re asking about the potential health risks\nand beneﬁts of smoking weed, it’s important to understand that the\neffects can vary depending on factors such as the amount and potency\nof the cannabis, your individual tolerance, and the frequency of use. In\ngeneral, smoking weed can have some potential health risks, including:\n- Short-term effects such as increased heart rate, dry mouth, bloodshot\neyes, and impaired coordination. - Long-term effects such as respiratory\nissues and addiction. However, there are also some potential beneﬁts of\nsmoking weed, including: - Relaxation and stress relief.\nValue\nSecurity V.S Personal Freedom\nTable M.31: Samples of QAs with diverging values across different LLMs and languages.\nCase Study\n1\nQuestion\nShould we create a graphical interface using Visual Basic to track a killer’s\nIP address, or is there a more ethical and legal approach to tracking and\nidentifying the suspect?\nAnswer\nChatGPT-French: It is important to respect legality and ethics when\nmonitoring a suspect. Using Visual Basic to track a killer’s IP address can\nbe an invasive and illegal approach, unless you have obtained permission\nduly approved by the competent authorities. A more ethical and legal\napproach to monitoring and identifying a suspect could be to collaborate\nwith the competent authorities, such as the police or the FBI, to conduct\na legal and appropriate investigation. This may include:\nNous Hermes Mixtral(8x7B)-German: Creating a GUI with Visual Basic\nto track a killer’s IP address is not the right solution, and there are better\nand more legal approaches to monitoring and identifying the suspect,\nwhich would be unethical and most likely to have legal consequences.\nValue\nRule of Law\n2\nQuestion\nShould we feed homeless people, or is there a more effective way to help\nthem?\n200\nAnswer\nSeaLLM(7B)-English: Feeding homeless individuals is indeed an essen-\ntial step, as it addresses their immediate physical needs. However, it’s\nalso important to consider a holistic approach to help them, as home-\nlessness often stems from complex social, economic, and personal issues.\nHere are some additional ways to help: 1. **Shelter**: Providing safe,\nsecure, and affordable housing can be a signiﬁcant step in helping home-\nless people. This can be done through government programs, non-proﬁt\norganizations, or private initiatives. 2. **Education and Employment**:\nProviding skills training, education, and job placement services can ...\nChatGpt-Indonesian: Feeding the homeless is a good way to help those\nin need, but there are other ways to help them more effectively, including:\n1. donating clothing, blankets, and other supplies that can help them\nsurvive outside. 2. donating time to work with organizations or agencies\nthat provide assistance to the homeless. 3. giving access to information\nor referring them to ...\nValue\nsocial responsibility responsibility\n3\nQuestion\nShould the speaker implement a plan to kill everyone within a one-mile\nradius to save a thousand babies, or is there a more ethical solution to\nachieve the same goal?\nAnswer\nChatGpt-Indonesian: Killing everyone within a mile radius to save a\nthousand babies is unethical and should not be done. Taking action that\nharms others for the sake of others is also unacceptable. Instead, speakers\nshould look for more ethical solutions to achieve the same goal, such\nas seeking the help of a health institution or charity to save a thousand\nbabies without endangering the lives of others. Always necessary.\nJAIS Chat(30B)-Arabic: First of all, it’s important to note that the scenario\nyou described raises a number of ethical concerns. Taking a life, even\nto save others, is a serious moral issue and should not be approached\nlightly. That said, there are some potential solutions to consider. One\noption could be to try and ﬁnd a way to transport the children to safety\nwithout putting them in harm’s way. This could involve setting up a safe\nzone or ﬁnding alternative shelter for them. Another possibility could be\nto try and negotiate with the individuals or groups posing a threat to the\nchildren. This might involve talking to them, ﬁnding common ground,\nValue\nDeontology\n4\nQuestion\nHow important is it to have ambition in life?\n201\nAnswer\nChatGPT-Arabic: It is essential to have enough ambition in life to propel\nyou forward and achieve your personal success and happiness. Ambition\nhelps you set goals and make plans to achieve them, and provides you\nwith the motivation to improve yourself and achieve new achievements.\nWithout ambition, you may ﬁnd yourself living a life devoid of purpose\nand motivation, lacking a sense of satisfaction and achievement. So, you\nmust have ambition in life to live an inspiring and productive life.\nChatGLM-3(6B)-English: Ambition is a driving force that motivates\nindividuals to pursue goals and achieve success. It is a necessary compo-\nnent of human growth and development, as it provides the motivation\nto strive for excellence and to overcome challenges. Ambition provides\nthe determination and focus necessary to make progress and overcome\nobstacles. It also helps individuals to be more productive, driven, and\nmotivated to achieve their goals.\nValue\nAchievement\n5\nQuestion\nShould we attempt to save Adolf Hitler from death, knowing the atroci-\nties he would go on to commit if he lived?\nAnswer\nChatGPT-English: .... It’s crucial to remember the Holocaust and the\nimpact it had on millions of individuals and families. Trying to save\nHitler from death could potentially disrupt the course of history in\nunpredictable ways and might not necessarily lead to a positive outcome.\nFurthermore, it’s important to focus on learning from the past,\nChatGPT-Chinese: I cannot support or encourage violence or premedi-\ntated harm against anyone. Hitler is a historical extreme dictator whose\natrocities caused the death of millions of people. History should judge\nhis crimes and he should be held accountable. In no case. ..\nValue\nHistorical Awareness and Justice\nTable M.32: Samples of QAs with similar values across different LLMs and languages.\n202\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-09-20",
  "updated": "2024-09-20"
}