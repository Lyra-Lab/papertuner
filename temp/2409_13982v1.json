{
  "id": "http://arxiv.org/abs/2409.13982v1",
  "title": "CUS3D :CLIP-based Unsupervised 3D Segmentation via Object-level Denoise",
  "authors": [
    "Fuyang Yu",
    "Runze Tian",
    "Zhen Wang",
    "Xiaochuan Wang",
    "Xiaohui Liang"
  ],
  "abstract": "To ease the difficulty of acquiring annotation labels in 3D data, a common\nmethod is using unsupervised and open-vocabulary semantic segmentation, which\nleverage 2D CLIP semantic knowledge. In this paper, unlike previous research\nthat ignores the ``noise'' raised during feature projection from 2D to 3D, we\npropose a novel distillation learning framework named CUS3D. In our approach,\nan object-level denosing projection module is designed to screen out the\n``noise'' and ensure more accurate 3D feature. Based on the obtained features,\na multimodal distillation learning module is designed to align the 3D feature\nwith CLIP semantic feature space with object-centered constrains to achieve\nadvanced unsupervised semantic segmentation. We conduct comprehensive\nexperiments in both unsupervised and open-vocabulary segmentation, and the\nresults consistently showcase the superiority of our model in achieving\nadvanced unsupervised segmentation results and its effectiveness in\nopen-vocabulary segmentation.",
  "text": "CUS3D: CLIP-BASED UNSUPERVISED 3D SEGMENTATION VIA OBJECT-LEVEL DENOISE\nFuyang Yu1, Runze Tian1, Zhen Wang2, Xiaochuan Wang3, Xiaohui Liang1,∗\n1 Beihang University, Beijing, China\n2 Tokyo Institute of Technology, Tokyo, Japan\n3 Beijing Technology and Business University, Beijing, China\n∗liang xiaohui@buaa.edu.cn\nABSTRACT\nTo ease the difficulty of acquiring annotation labels in 3D\ndata, a common method is using unsupervised and open-\nvocabulary semantic segmentation, which leverage 2D CLIP\nsemantic knowledge. In this paper, unlike previous research\nthat ignores the “noise” raised during feature projection from\n2D to 3D, we propose a novel distillation learning framework\nnamed CUS3D. In our approach, an object-level denosing\nprojection module is designed to screen out the “noise” and\nensure more accurate 3D feature. Based on the obtained fea-\ntures, a multimodal distillation learning module is designed\nto align the 3D feature with CLIP semantic feature space with\nobject-centered constrains to achieve advanced unsupervised\nsemantic segmentation. We conduct comprehensive experi-\nments in both unsupervised and open-vocabulary segmenta-\ntion, and the results consistently showcase the superiority of\nour model in achieving advanced unsupervised segmentation\nresults and its effectiveness in open-vocabulary segmentation.\nIndex Terms— Unsupervised Semantic Segmentation,\nPoint Cloud, Knowledge Distillation, Multimodal, Open-\nvocabulary\n1. INTRODUCTION\nPoint cloud semantic segmentation [1, 2] aiming to give a cat-\negory label for each input point is a fundamental task of 3D\nvisual understanding. Due to the challenges in acquiring and\nannotating point cloud data, there is a shortage of supervised\nsegmentation labels for point clouds, which greatly limit the\ndevelopment of this field and downstream tasks. To address\nthis shortcoming, many researchers have carried out studies\non unsupervised semantic segmentation which means train-\ning model without using annotated data.\nSeveral research works have used clustering [3, 4] to\nachieve unsupervised semantic segmentation. This approach,\nalthough applicable to a wide range of scenarios, has lim-\nited accuracy and is hard to capture deep semantic context for\ndownstream tasks. Benefiting from the development of large\nmodels such as CLIP [5] which established complete seman-\ntic space on 2D image or even biomedical image [6], many re-\nsearchers [7, 8] aimed to achieve unsupervised point cloud se-\nmantic segmentation by aligning the 3D semantic space to the\n2D semantic space of large models. The framework of their\napproaches takes the RGB-D frames as input, and implement\n2D semantic segmentation via large model like CLIP. Pixel-\nwise features can be obtained through segmented masks, then\nthe features is projected to 3D space using 2D-3D projection\nmethods [9] to obtain point-wise feature. The semantic seg-\nmentation result can be obtained by calculating the the sim-\nilarity between the point-wise features and the text embed-\nding of categories. These approaches leveraging knowledge\nof large models can achieve more accurate unsupervised se-\nmantic segmentation result and have impressive performance\non open-vocabulary tasks.\nHowever, though their methods yielded promising results\nin unsupervised and open-vocabulary semantic segmentation,\nit is noteworthy that they did not consider the accuracy of\nthe obtained 3D features. In fact, these 3D features contain\na lot of “noise”, which affects the alignment to the semantic\nspace of large model. The “noise” raised from two aspects.\n1) “Noise” is introduced by the mask-centered assignment of\nfeatures in 2D stage. 2) Fusion of features without screening\nintroduces “noise” in 3D stage. In detail, the whole image\nis divided into patches [8] or masks [7] for feature exaction\nin 2D stage. As shown in Figure 1 (Yellow), one pixel of\nthe “bag” belongs to two different masks. The big mask is\nselected as the final results and the CLIP exact its features as\n“Bed” due to the large models pay more attention on the over-\nall semantics. Masks are difficult to adaptively fit to the size\nof various objects introducing “noise”. In 3D stage, as shown\nin Figure 1 (Blue), the red point represent the same point in\n3D space and appears in three different frames. The features\nprojected from the three frames is different, when fusing the\nfeature without screening, the “noise” is raised. Shown in the\nfigure, the point is incorrectly recognized as “Bad” finally.\nWhen aligning the semantic space, these “noises” affect\nthe accuracy of the learned distributions, as illustrated in Fig-\nure 1 (Green), and further reduce segmentation accuracy. We\nvisualize the impact of “noise” on the segmentation results in\nFigure 3 (D). How to avoid these “noises” is a worthy concern\narXiv:2409.13982v1  [cs.CV]  21 Sep 2024\n2D Stage \n3D Stage \n2D Feature\n3D Feature\nFeature (Bag)\nDistribution\n\"Bag\"\n\"Bed\"\nMisclassified Point\nCorrect\ndistribution\nMisdistribution\nFusion\nmask1\nfeature\nmask2\nfeature\nSegmentation Result\nPixel-wise\nFeature\n( bed, wrong )\nCLIP\nImage\nEncoder\nFeature (Bag)\nFeature (Bed)\nFusion\nFeature (fusion)\nBed, 0.52 (wrong)\nBeg, 0.41 (expected)\nCLIP result\nFig. 1. The overview of existing methods (Up) and the analysis of the problems with existing methods (Down).\nand can significantly improve the accuracy of segmentation.\nTo address these issues and align the semantic space more ac-\ncurately, we propose a novel framework CUS3D for unsuper-\nvised and open-vocabulary semantic segmentation via object-\nlevel denoise.\nWe believe that pixels or points inside one\nobject should have the same characteristics, the “noise” can\nbe suppressed by filtering and assigning features at object-\nlevel, but it is difficult to find an accurate object mask with-\nout supervision. To eliminate these noises during projection,\nwe firstly propose a Object-level Denoising Projection(ODP)\nmodule, obtaining category to pixels as well as point mask\nby efficiently clustering and voting strategies, to screen out\nthe “noise” raised in 2D stage and 3D stage and obtain more\naccurate features. The obtained 3D features are discrete in\nsemantic space, and distillation learning enables the model to\nlearn the complete distribution from discrete sample points\nand further extends the use of the model. Thus, we further\ndesign a 3D Multimodal Distillation Learning (MDL) mod-\nule with object-level constraints, constraining the 2D and 3D\nsemantic spaces to be as close as possible centered on the ob-\nject and further screening out the effects of “noise”.\nIn summary, we propose a novel framework called\nCUS3D to efficiently align the 3D semantic space to the CLIP\nfeature space to achieve advanced unsupervised and open-\nvocabulary segmentation results, and our contributions in this\npaper are as follows: 1) We proposed an object-level fea-\nture denoise module to ensure more accurate 3D features. 2)\nWe devised a multimodal distillation learning module using\nobject-centered constrains to realize more effectively align-\nment of the 2D and 3D semantic space. 3) We conducted\ndetailed unsupervised and open-vocabulary segmentation ex-\nperiments to prove the efficiency of our approach.\n2. METHOD\nThe overall framework of our proposed method is illustrated\nin Figure 2, which consists of four main stages: 1) 2D CLIP\nfeature extraction (orange), extracting pixel-wise features; 2)\n3D feature aggregation (blue), obtaining 3D features; 3) 3D\nstudent network (green), fitting the semantic space of CLIP;\n4) CLIP textual encoder (gray), extracting textual embed-\ndings. The first and the second stage belongs to the ODP\nmodule which denoising at object level to obtain accurate 3D\nfeatures, while the third and forth stage belongs to the MDL\nmodule which fitting the CLIP’s semantic space.\n2.1. Object-level Denoising Projection\nThis module containing two sub-modules are designed to fil-\nter the “noise” in the feature at object-level to obtain more\naccurate 3D features, the two sub-models are described in de-\ntail in the following sections.\n2.1.1. 2D CLIP Feature Extraction\nWe design a 2D feature extraction method to assign pixel-\nwise features. Unlike other mask-based methods [10], we as-\nsign pixel features centered on the object, and design a 2D\nobject-level filter screening out the “noise” raised during fea-\nture excation. We first use a MaskFormer [11] to obtain candi-\ndate masks and a pixel-to-mask mapping matrix M (N × m),\nrepresenting the probability that each pixel belongs to each\nmask, where N is the number of pixels in the image and m\nis the number of masks. Then, for each predicted mask, the\ncorresponding CLIP visual features FCLIP are extracted us-\ning the CLIP ViT encoder [5]. If a probability value in M\nis greater than the threshold β, we assign the corresponding\nmask feature F maski\nCLIP to this pixel. Each raw pixel feature is\nF raw\npixelj =\nn\nF maska\nCLIP , ..., F maskn\nCLIP\no\n. For each raw feature of\neach pixel, we calculate its category label and obtain a label\nset of this pixel. The most frequent category label of the set\nis voted as the label of this pixel, to establish the connection\nbetween pixel and object. Then we filter out the CLIP feature\nthat does not belong to this category. Finally, the reserved\nCLIP features undergo an average pooling to obtain the final\npixel-wise feature Fpixel.\n2.1.2. 3D Feature Aggregation\nAs shown in the blue part of Figure 2, the pixel-wise features\nFpixel obtained from Section 2.1.1 are projected to the 3D\npoint clouds using method in 3DMV [9] to obtain raw point\nfeatures F raw\npoint. To reduce the “noise” in the features, we con-\nduct denoise by a 3D object-level filter. First, we calculate\nMaskformer\npred\nmask\nbin\nmask\nCLIP\nViT\nPointClusting\n3D Object\nMask\nCLIP\nText\nEncoder\nCos Sim\nFeature\nloss\nCos Sim\nPixel-wise\nFeature\n(clean)\n2D\n(CLIP)\n3D\n(Teacher)\nResU-Net\nPoint-wise\nFeature\n(student)\nLabel\nLoss\nThis is\na {} in\nscene\nText\n(CLIP)\nFilter\nPred\nLabel\nCLIP Pseudo\nLabel\nTextural\nEmbedding\n3D\n(Student)\n3DMV\nProjection\nPoint-wise\nFeature\n(noisy)\nPoint-wise\nFeature\n(clean)\n3D Feature\nAggregation\nObject 1\n3dMask 1\n3dMask 2\nfi\nObject 1\nObject 2\nFilter by most frequent of labels\nObject 2\nfeature of\npoint mapping\nfrom Frame i\nAvg Pool\nPoint-wise\nFeature\n(noisy)\ncos\nTextual\nEmbedding\nFill based on the mean features\nDetail of 3D Feature Aggregation\nObject-level Denoise Projection\n3D Multimodal Distillation Learning\nFig. 2. The proposed pipeline comprises two key module, which is described detailly in Section 2.\nobject masks for point clouds, a pretrained Pointclustering [3]\nis used to obtain the offset of each point cloud to its cluster\ncenter, and construct the object mask according to which clus-\nter the point cloud belongs to. Then, as shown in Figure 2\n(3D Feature aggregation), we use the generated object mask\nto denoise and aggregate the raw point feature. For each ob-\nject mask, since the raw point feature is projected from the\npixel-wise feature, it also has the CLIP semantic information;\nthus, we can measure the cosine similarity between its each\nraw point-wise feature and the CLIP text embedding, to as-\nsign a category label to each raw point-wise feature. And the\nmost frequent category label among the raw point-wise fea-\ntures is selected as the label for that object mask. We then\nscreen out any raw point-wise features that do not match this\nlabel and each point cloud remains m features. To get the fi-\nnal point-wise feature Fpoint, for each point, if m ≥1, we\nperform average pooling on these features to obtain the final\nfeature for that point; while if m = 0, we take the average of\nall retained features belonging to the same object mask with\nit and assign this average feature to that point. This screening\nprocess can effectively eliminate noisy features at object level\nand obtain more robust and accurate point-wise features.\n2.2. 3D Multimodal Distillation Learning\nThe distribution of 3D features obtained after projection is\ndiscrete, in order to align 2D CLIP semantic space more ef-\nfectively and further eliminate the effects of “noise”, we adapt\na distillation learning network with object-centered constrains\nto align to the 2D CLIP semantic space. Using the features\n(Teacher) obtained in Section 2.1.2, distillation learning can\nbe performed to guide the 3D model to encode the features\nto fit the CLIP embedding space, thus further suppressing\n“noise”. We supervise the student network (green stage in\nFigure 2) with the constrains of features and the labels. 3D\nResU-Net [12] is chosen as the backbone network because it\ncan efficiently extract point features and preserve shallow fea-\ntures. For a scene with N points, the 3D ResU-Net outputs a\nfeature map of size N × 3, where each of the N points has a\ncorresponding 3D feature vector. After the backbone network\nis deployed, we utilize three fully connected layers (MLPs) as\nprojection modules to project the point cloud features into the\nCLIP embedding space. Specifically, the output feature map\nis fed into the projection layers to align the features with the\nCLIP semantic space.\n2.2.1. Loss Function\nRelied solely on cosine similarity for supervision can lead to\nthe students network more sensitive to “noise”. Unlike pre-\nvious work, we add object-centered constrain(label loss) for\nmore effectually learning. The feature loss function main-\ntains semantic consistency between the output features and\nthe target features. The label loss function provides a soft-\nsupervise signal for training, aligning the two distributions\nwith object-centered constrains which can weak the effect of\n“noise” and learn a rubost distribution. Using both loss func-\ntions together, the model achieves better knowledge distilla-\ntion performance.\nFeature Loss is cosine similarity loss to measure the sim-\nilarity between target and output features, which is defined as\nfollows:\nLfeature = 1 −1\nN\nN\nX\ni=0\nfoi · fci\n|foi| |fci|\n(1)\nwhere foi and fci represent the features predicted by the net-\nwork and the aggregated CLIP feature of the i-th point, re-\nspectively, where N denotes the total number of point clouds.\nThis loss function constrains the semantic proximity between\ntwo features.\nLabel Loss employs the cross-entropy loss to assess the\nconsistency between the predict results of two features (net-\nwork output feature and CLIP feature) and is defined as fol-\nlows:\nLlabel = 1\nN\nN\nX\ni=0\nCrossEntropy(Predoutput\ni\n, PredCLIP\ni\n)\n(2)\nwhere N represents the number of point clouds, and\nCrossEntropy(·, ·) denotes the cross-entropy loss function.\nPredoutput\ni\nand PredCLIP\ni\nare one-hot matrixes which rep-\nresent the category prediction results of the two features cor-\nresponding to the i-th point. The matrixes are obtained by\ncosine similarity between the textual embedding and the fea-\nture. Label loss constrains the alignment of the two semantic\nspaces centered on the object category, enabling the network\nto further resist the effects of “noise” and learn a valid seman-\ntic space. The combined of the two constrains can enable the\nnetwork to more accurately align CLIP semantic space.\n3. EXPERIMENT\n3.1. Dataset and Settings\n) Dataset: ScanNetV2 [13] and S3DIS [14] are both 3D in-\ndoor dataset. ScanNetV2 provide point cloud and RGB-D\ndata of more than 1,500 scenes with objects in 20 categories.\nS3DIS provides point cloude and RGB-D data of 6 large ar-\neas, 272 rooms, and its object labeled in 13 classes.\n) Implement Details: Our experiments were carried out us-\ning a GeForce RTX 3090 graphics card with 24GB RAM. Our\nnetwork was distilled using CLIP features in the training set\n(without labels). We employ an initial learning rate of 0.001\nwith cosine annealing learning decay. We adapt the accuracy\n(Acc), mean IoU (mIoU) and harmonic IoU (hIoU) for the\nevaluation metrics.\n3.2. Quantitative results\n) Unsupervised Semantic Segmentation: We conducted\nunsupervised segmentation experiments on both ScanNetV2\nand S3DIS. In both the feature projection process and the dis-\ntillation study process, no labels were used. The model was\nsubsequently tested on ScannetV2’s validation set and S3DIS.\nThe experimental results are detailed in Table 1. While our\nmethod may not surpass existing supervised methods, it has\nattained state-of-the-art (SOTA) performance among unsuper-\nvised approaches on ScanNetV2 (57.4% vs. 54.2%). No-\ntably, we show a huge improvement in accuracy over the other\nmethods on ScanNetV2 (75.9% vs. 70.7%), which we at-\ntribute to pseudo label produced by the ODP module to guide\nthe network to learn the boundaries of the object and improve\nthe accuracy of semantic segmentation.\n) Open-vocabulary Semantic Segmentation:\nWe con-\nducted open-vocabulary experiments in two ways on Scan-\nNetV2 dataset. In the first experiment, following the settings\nof CLIP-FO3D [19], we split the labels into visible part and\ninvisible part. We only train the model using visible part and\nModel\nScanNet\nS3DIS\nmIoU\nAcc\nmIoU\nAcc\nFully-supervised methods\nMinkowskiNet [15]\n69.0\n77.5\n-\n-\nMix3D [16]\n73.6\n-\n67.2\n-\nStratified Transformer [17]\n74.3\n-\n72.0\n78.1\nUnsupervised methods\nMseg [18] Voting\n45.6\n54.4\n42.3\n51.6\nCLIP-FO3D [19]\n30.2\n49.1\n22.3\n32.8\nOpenScene [7] (LSeg)\n54.2\n66.6\n-\n-\nOpenScene [7] (OpenSeg)\n47.5\n70.7\n-\n-\nCUS3D (Ours)\n57.4\n75.9\n52.6\n72.6\nw/o pretraining\n-\n-\n25.6\n50.6\nTable 1. segmentation results on ScanNetV2 [13] (Validation\nSet) and S3DIS [20], our method achieves SOTA unsuper-\nvised segmentation performance.\nSetting\nUnseen-6\nUnseen-10\nMetric\nmIoU\nhIoU\nmIoU\nhIoU\nS\nU\nS\nU\n3DGenZ [21]\n31.2\n4.8\n8.3\n30.1\n1.4\n2.7\nTGP [22]\n55.2\n15.4\n24.1\n52.5\n9.5\n16.1\nCLIP-FO3D [19]\n67.3\n50.8\n57.9\n67.7\n40.7\n50.8\nCUS3D (Ours)\n68.3\n53.2\n59.8\n69.4\n46.2\n55.5\nTable 2. Open-vocabulary semantic segmentation on Scan-\nNetV2. “Unseen-i” indicates that there are i classes that do\nnot have labels during training. S and U represent the perfor-\nmance of seen and unseen classes, respectively.\nthen test the model on both visible and invisible labels, to see\nif our model can segment invisible objects. The invisible la-\nbels are set to 6 and 10, respectively. The results of these ex-\nperiments are presented in Table 2, and our method achieves\nSOTA in both mIoU and hIoU on Unseen-6 and Unseen-10,\nproving its robust open-vocabulary capability. In the second\nexperiment, as shown in Table 1 (w/o pretraining), we distil-\nlate our model on ScanNetV2 training set with 20 labels, and\ntest our model on S3DIS with unseen labels and scenes. It\nis noticed that our model performs better than CLIP-FO3D\n(25.6% vs. 22.3%) in same settings. We believe that our\ntwo modules weak the effect of the “noise” in feature and\nachieved more effective and robust alignment from 3D fea-\nture to 2D CLIP semantic space, thus performing better on\nonpen-vocabulary semantic segmentation.\n3.3. Ablation Studies\nIn this section, we conduct ablation experiments on the S3DIS\nand ScanNetV2 datasets, projecting CLIP features for un-\nsupvised segmentation networks distillation, and making seg-\nfloor\ntable\ncabinet\nwall\nchair\ndoor\nwindow\npicture\ncounter\ndesk\ntoilet\nsink\ncomputer\nprinter\ntrash can\nwhiteboard\nHigh\nlow\nBlule furniture\nCircular\nSleep\nWooden\nw/o 2d, w/o 3d\nw/ 2d, w/o 3d\nw/o 2d, w/ 3d\nw/ 2d, w/ 3d\nB\nC\nD\nGt\nOpenScene\nOurs\nOpenScene\nOurs\nOurs\nGt\n（1）\n（2）\n（3）\n（4）\n（1）\n（2）\n（3）\n（4）\nA\notherfurniture\nunannotated\nGt\nGt\nOurs\nFig. 3. Some Semantic segmentation visualization results.\nExperiment Settings\nS3DIS\nScanNetV2\nFeature Loss\nLabel Loss\nmIoU\nAcc\nmIoU\nAcc\n!\n%\n49.8\n70.1\n53.3\n74.7\n%\n!\n9.2\n54.3\n8.0\n58.4\n!\n!\n52.6\n72.6\n57.4\n75.9\nTable 3. Ablation experimental on loss functions.\nExperiment Settings\nTraining Set\nValidation Set\n2D FE.\n3D FA.\nSn.\nmIoU\nAcc\nmIoU\nAcc\n%\n%\n%\n27.6\n47.7\n28.2\n45.9\n!\n%\n%\n32.4\n53.2\n31.8\n53.0\n%\n!\n%\n46.7\n65.3\n47.3\n66.8\n!\n!\n%\n52.9\n75.5\n52.7\n73.9\n%\n%\n!\n36.6\n52.8\n31.3\n48.8\n!\n%\n!\n40.3\n62.3\n38.6\n59.5\n%\n!\n!\n56.3\n75.8\n51.4\n71.6\n!\n!\n!\n63.8\n79.4\n57.4\n75.9\nTable 4. Ablation experiments on ODP module. In the table,\n“2D FE.” and “3D FA.” indicate the usage of 2D feature ex-\ntraction and 3D feature aggregation module, separately. “Sn.”\nindicates whether using student network in our model.\nmentation predictions.\nWe first determine whether our loss function improves the\nmodel performance on two datasets. As shown in Table 3, the\nnetwork encounters performance drop when only using label\nloss for distillation study, while without label loss and only\nretain feature loss, the performance of the model also drops.\nUsing both label loss and feature loss can achieve more ac-\ncurate segmentation result on both S3DIS dataset (52.6% vs.\n49.8%) and ScanNetV2 dataset (57.4% vs. 53.3%). The label\nloss force the two distributions close centered on the object-\nlevel, with little attention to detail, and is therefore poorly\neffective on its own. The feature loss can bring the two dis-\ntributions as close as possible, but is susceptible to “noise”.\nWhen the two loss functions are used together, the network is\nable to effectively eliminate “noise” interference and learn a\nmore robust distribution.\nThen to investigate the effectiveness of ODP module, we\nconduct ablation experiments on the sub-modules in ODP on\nScanNetV2 dataset both on training set and validation set. As\nindicated in Table 4, the four rows above show the mIoU and\naccuracy calculating bwteen ground truth and pseudo CLIP\nfeature, while the following four rows show the mIoU and ac-\ncuracy calculate between ground truth and student network’s\noutputs. It can be found that using student network achieves\nbetter semantic segmentation results than directly leveraging\npseudo CLIP features (57.4% vs. 52.7%) due to that the stu-\ndent network can complement discrete distributions and fur-\nther resist the effects of “noise”.\nThen we investigate the efficiency of 2D feature extrac-\ntion and 3D feature aggregation. Using either 2D feature ex-\ntraction or 3D feature aggregation module can eliminate the\neffects of “noise” and improve the performance of the mod-\nels, while combining both of them can achieve even greater\nimprovement on training and validation set, regardless of\nwhether the student network is used. And compared with 2D\nfeature extraction, 3D feature aggregation has more impact on\nmodel performance.\n3.4. Segmentation Visualization\nFigure 3 gives the visualization results of some experiments.\nSubfigure A shows the unsupervised semantic segmenta-\ntion results on ScannetV2 comparing our method to Open-\nScene [7]. It can be seen that our model perform better in\ndetail of different objects.\nSubfigure B demonstrates our\nmodel’s abilities in open-vocabulary segmentation.\nCom-\npared to ground truth, our model can segment objects in un-\nseen categories, such as computers, blackboards, etc. In Sub-\nfigure C, using the text below the images, our model can\ncorrectly focus on corresponding objects, and the attention\nresults are visualized by heat maps.\nThis proves that our\nmodel is capable of exploring open-vocabulary 3D scenes by\nnot only categories but also other object properties, such as\ncolors, shapes, usages, materials, and so on. Subfigure D\nshows the visualization results of the ablation experiments\nperformed on the ODP module. This figure clearly visualizes\nthe effect of “noise” on the segmentation results, and Both the\ntwo sub-modules in ODP can weak the effect of the “noise”,\nthus improving the accuracy of the 3D features, while using\nthem both can achieve significantly better results.\n4. CONCLUTION\nThis paper proposes CUS3D aiming to align 3D features and\n2D CLIP semantic space effectually and realise advanced\nopen-vocabulary semantic segmentation. Our experimental\nresults consistently demonstrate that our approach outper-\nforms in achieving superior unsupervised segmentation re-\nsults and exhibits robust open world segmentation capabil-\nities. Small objects pose a challenge for our segmentation\nprocess, often resulting in inaccurate segmentation. This oc-\ncurs because the 2D feature extraction process overlooks the\nunique characteristics of small objects. We plan to address\nthis limitation in future work.\n5. REFERENCES\n[1] Jiaying Zhang, Xiaoli Zhao, Zheng Chen, and Zhejun\nLu, “A review of deep learning-based semantic segmen-\ntation for point cloud,” IEEE access, vol. 7, pp. 179118–\n179133, 2019.\n[2] Alok Jhaldiyal and Navendu Chaudhary, “Semantic seg-\nmentation of 3d lidar data using deep learning: a review\nof projection-based methods,” Applied Intelligence, vol.\n53, no. 6, pp. 6844–6855, 2023.\n[3] Fuchen Long, Ting Yao, Zhaofan Qiu, Lusong Li, and\nTao Mei, “Pointclustering: Unsupervised point cloud\npre-training using transformation invariance in cluster-\ning,” in CVPR, 2023, pp. 21824–21834.\n[4] Jiaxu Liu, Zhengdi Yu, Toby P Breckon, and Hubert PH\nShum, “U3ds3: Unsupervised 3d semantic scene seg-\nmentation,” arXiv preprint arXiv:2311.06018, 2023.\n[5] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.,\n“Learning transferable visual models from natural lan-\nguage supervision,” in ICML. PMLR, 2021, pp. 8748–\n8763.\n[6] Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, and\nNaira Hovakimyan, “Residual-based language models\nare free boosters for biomedical imaging,” 2024.\n[7] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea\nTagliasacchi, Marc Pollefeys, Thomas Funkhouser,\net al., “Openscene: 3d scene understanding with open\nvocabularies,” in CVPR, 2023, pp. 815–824.\n[8] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-\npeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hong-\nsheng Li,\n“Pointclip: Point cloud understanding by\nclip,” in CVPR, 2022, pp. 8552–8562.\n[9] Angela Dai and Matthias Nießner,\n“3dmv: Joint 3d-\nmulti-view prediction for 3d semantic scene segmenta-\ntion,” in ECCV, 2018, pp. 452–468.\n[10] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li,\nYinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda,\nand Diana Marculescu, “Open-vocabulary semantic seg-\nmentation with mask-adapted clip,” in CVPR, 2023, pp.\n7061–7070.\n[11] Bowen Cheng, Alex Schwing, and Alexander Kirillov,\n“Per-pixel classification is not all you need for semantic\nsegmentation,” NIPS, vol. 34, pp. 17864–17875, 2021.\n[12] Megh Bhalerao and Siddhesh Thakur,\n“Brain tumor\nsegmentation based on 3d residual u-net,” in Interna-\ntional MICCAI Brainlesion Workshop. Springer, 2019,\npp. 218–225.\n[13] Angela Dai, Angel X Chang, Manolis Savva, Maciej\nHalber, Thomas Funkhouser, and Matthias Nießner,\n“Scannet: Richly-annotated 3d reconstructions of in-\ndoor scenes,” in CVPR, 2017, pp. 5828–5839.\n[14] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang,\nIoannis Brilakis, Martin Fischer, and Silvio Savarese,\n“3d semantic parsing of large-scale indoor spaces,” in\nCVPR, 2016, pp. 1534–1543.\n[15] Christopher\nChoy,\nJunYoung\nGwak,\nand\nSilvio\nSavarese,\n“4d spatio-temporal convnets: Minkowski\nconvolutional neural networks,”\nin CVPR, 2019, pp.\n3075–3084.\n[16] Alexey Nekrasov, Jonas Schult, Or Litany, Bastian\nLeibe, and Francis Engelmann, “Mix3d: Out-of-context\ndata augmentation for 3d scenes,” in 3DV. IEEE, 2021,\npp. 116–125.\n[17] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Heng-\nshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia,\n“Stratified transformer for 3d point cloud segmentation,”\nin CVPR, 2022, pp. 8500–8509.\n[18] John Lambert, Zhuang Liu, Ozan Sener, James Hays,\nand Vladlen Koltun, “Mseg: A composite dataset for\nmulti-domain semantic segmentation,” in CVPR, 2020,\npp. 2879–2888.\n[19] Junbo Zhang, Runpei Dong, and Kaisheng Ma, “Clip-\nfo3d: Learning free open-world 3d scene representa-\ntions from 2d dense clip,” arXiv:2303.04748, 2023.\n[20] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio\nSavarese, “Joint 2d-3d-semantic data for indoor scene\nunderstanding,”\narXiv preprint arXiv:1702.01105,\n2017.\n[21] Bj¨orn Michele, Alexandre Boulch, Gilles Puy, Maxime\nBucher, and Renaud Marlet,\n“Generative zero-shot\nlearning for semantic segmentation of 3d point clouds,”\nin 3DV. IEEE, 2021, pp. 992–1002.\n[22] Runnan Chen, Xinge Zhu, Nenglun Chen, Wei Li,\nYuexin Ma, Ruigang Yang, and Wenping Wang, “Zero-\nshot point cloud segmentation by transferring geometric\nprimitives,” arXiv preprint arXiv:2210.09923, 2022.\n",
  "categories": [
    "cs.CV",
    "cs.MM"
  ],
  "published": "2024-09-21",
  "updated": "2024-09-21"
}