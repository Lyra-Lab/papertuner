{
  "id": "http://arxiv.org/abs/2409.16862v1",
  "title": "Behavior evolution-inspired approach to walking gait reinforcement training for quadruped robots",
  "authors": [
    "Yu Wang",
    "Wenchuan Jia",
    "Yi Sun",
    "Dong He"
  ],
  "abstract": "Reinforcement learning method is extremely competitive in gait generation\ntechniques for quadrupedal robot, which is mainly due to the fact that\nstochastic exploration in reinforcement training is beneficial to achieve an\nautonomous gait. Nevertheless, although incremental reinforcement learning is\nemployed to improve training success and movement smoothness by relying on the\ncontinuity inherent during limb movements, challenges remain in adapting gait\npolicy to diverse terrain and external disturbance. Inspired by the association\nbetween reinforcement learning and the evolution of animal motion behavior, a\nself-improvement mechanism for reference gait is introduced in this paper to\nenable incremental learning of action and self-improvement of reference action\ntogether to imitate the evolution of animal motion behavior. Further, a new\nframework for reinforcement training of quadruped gait is proposed. In this\nframework, genetic algorithm is specifically adopted to perform global\nprobabilistic search for the initial value of the arbitrary foot trajectory to\nupdate the reference trajectory with better fitness. Subsequently, the improved\nreference gait is used for incremental reinforcement learning of gait. The\nabove process is repeatedly and alternatively executed to finally train the\ngait policy. The analysis considering terrain, model dimensions, and locomotion\ncondition is presented in detail based on simulation, and the results show that\nthe framework is significantly more adaptive to terrain compared to regular\nincremental reinforcement learning.",
  "text": "Behavior evolution-inspired approach to walking gait reinforcement \ntraining for quadruped robots \nYu Wang1, Wenchuan Jia1, *, Yi Sun1, Dong He1 \n1 School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China. \n* Corresponding author: lovvchris@shu.edu.cn \nAbstract \nReinforcement learning method is extremely competitive in gait generation techniques for quadrupedal robot, which \nis mainly due to the fact that stochastic exploration in reinforcement training is beneficial to achieve an autonomous gait. \nNevertheless, although incremental reinforcement learning is employed to improve training success and movement \nsmoothness by relying on the continuity inherent during limb movements, challenges remain in adapting gait policy to \ndiverse terrain and external disturbance. Inspired by the association between reinforcement learning and the evolution of \nanimal motion behavior, a self-improvement mechanism for reference gait is introduced in this paper to enable incremental \nlearning of action and self-improvement of reference action together to imitate the evolution of animal motion behavior. \nFurther, a new framework for reinforcement training of quadruped gait is proposed. In this framework, genetic algorithm \nis specifically adopted to perform global probabilistic search for the initial value of the arbitrary foot trajectory to update \nthe reference trajectory with better fitness. Subsequently, the improved reference gait is used for incremental reinforcement \nlearning of gait. The above process is repeatedly and alternatively executed to finally train the gait policy. The analysis \nconsidering terrain, model dimensions, and locomotion condition is presented in detail based on simulation, and the results \nshow that the framework is significantly more adaptive to terrain compared to regular incremental reinforcement learning. \nKeywords: Gait training; Quadruped robot; Reinforcement learning; Behavior evolution; Reference trajectory; Genetic algorithm. \n \n1. Background \nIn recent years, legged robots such as quadruped robots have shown increasingly powerful locomotion capability, \nwhich is largely due to the ongoing intensive research and application of modern control techniques such as Model-\nPredictive Control [1-3]. Nevertheless, we still believe that reinforcement learning is extremely competitive in gait \ngeneration. Although the model predictive control method based on the optimal control concept and state prediction enables \nthe robot to solve the joint input condition required to reach the expected state in real time, the expected state of the robot \nstill needs to be planned in advance, and therefore the autonomy of the robot locomotion is not fundamentally improved. \nMoreover, the model prediction results are closely related to the initial value of the state, and thus are uncertain and difficult \nto reproduce accurately. In contrast, reinforcement learning theory is based on the effective imitation of the learning process \nof real behaviors, including gait learning, where fully autonomous gait can be acquired [4] after sufficient training for a \nreasonable reward target. \nReinforcement learning (RL) method [5] was first applied to discrete system, whose state space, action space, and \ntrained policy are expressed in discrete form, which motivated its debut in TV game tasks. As continuous reinforcement \nlearning methods have been proposed successively [6], not only simple physical systems in classical control can obtain the \nexpected motion, but also more complex continuous tasks such as quadruped gait policy have been trained successfully  \n[4][7]. In particular, reinforcement learning methods such as DDPG [8], A3C [9], PPO [10], and SAC [11] have been \ndemonstrated to be applicable to continuous system. \nDue to the significant increase in the state space dimension and action space dimension, reinforcement training for \ncontinuous tasks becomes difficult, and the trained movement policy is less stable than real movement. Considering that \nthe action sequences in such systems are usually naturally continuous, incremental reinforcement learning has thus been \nproposed [12], which employs the continuous characteristics of the system to improve the success rate of training and \nenhance the stability of the movement. For the quadruped gait training task, incremental reinforcement learning has been \na popular approach nowadays [13]. Kim, Shao, and Tan all achieved good results with incremental reinforcement training \nby introducing the guide mechanism as a reference or a priori [14-16]. Nevertheless, the challenge of achieving a more \nsuitable gait and adapting the quadruped robot to various terrains and resisting external disturbances through reinforcement \nlearning methods remains, which is the motivation for this study. \nSince reinforcement learning mechanism is similar to the acquisition process of animal motion behavior, we explore \nthe difference and relevance by comparing them. Then, inspired by their intrinsic relationship, the existing training \nframework based on reinforcement learning to generate gait is improved to fundamentally enhance the success rate of \ntraining and retain the autonomy of gait generation. The detailed results presented in this paper demonstrate the significant \neffect of this improvement. A comparison of animal motion behavior evolution and reinforcement learning mechanism is \nshown in Fig. 1, where the former is displayed in the left region and the latter is displayed in the right region, with a specific \ninstance of quadrupedal walking shown in the central region, separated by thick dotted lines between adjacent regions. \nFrom behavioral perspective, gait walking is classified as an instinctive behavior, which can be considered as a sequence \nof reflex behaviors, i.e., the spontaneous generation of continuous movements based on underlying perceptual information \nfrom internal and external sources, such as foot state and changes in center of gravity. Instinctive behaviors evolved from \nreflex behaviors [17-18], the latter usually referring to simple movements evoked by simple external stimulus signals, such \nas joint-level movements like the knee-jump reflex. This evolution is based on the long-term regulation of movements by \nfactors including body balance, energy consumption, and habituation. The evolved gait is capable of employing the whole \nbody to achieve movement and subsequently undergoes progressive self-improvement so that the gait is continually \nregulated to better adapt to the growing body and the various nature environment. This process also partly explains why \nreinforcement learning has difficulties in natural gait learning. For reinforcement learning, the training subject is usually a \nsimplified rigid mechanism, which is significantly different from real musculoskeletal system with good elastic motion, \nand the factors considered in the design of the reward function are usually only a subset of the many factors that influence \nthe behavior evolution (shown as the dark gray shaded area in Fig. 1), making it difficult to achieve realistic limb movement \nby relying only on training of joint movement. Incremental reinforcement learning can be understood as deriving from and \nincorporating progressive feature in behavior evolution, as shown by the red dashed line. Moreover, we believe that it is \ncrucial to introduce a self-improvement mechanism for the reference gait (shown as the red solid line in Fig. 1), where \nincremental learning and self-improvement for the gait together constitute an imitation of the evolution of the animal's \nwalking behavior. A new framework constructed based on the idea of behavioral evolution for gait reinforcement training \nis presented in this paper, which incorporates both incremental learning of gait movement and self-improvement capability, \nand its good performance in gait training of quadruped robot is also demonstrated. \n \nFig. 1. Improving existing reinforcement training processes based on evolution concept of animal motion behavior. \n2. Related Works \n2.1. Problem Statement \nReinforcement learning can usually be described as Markov Decision Process (MDP), while the motion control of a \nquadruped robot can be considered as a Partially Observable Markov Decision Processes (POMDP) described by the tuple \n(𝑆𝑆, 𝐴𝐴, 𝑃𝑃, 𝑅𝑅, 𝛾𝛾) , where the state space 𝑆𝑆  and the action space 𝐴𝐴  are assumed to be continuous, and the state transition \nprobability 𝑃𝑃: 𝑆𝑆× 𝑆𝑆× 𝐴𝐴→[0,1] denotes the probability density of the next state 𝑠𝑠𝑡𝑡+1 ∈𝑆𝑆 when the current state 𝑠𝑠𝑡𝑡∈𝑆𝑆 \nand action 𝑎𝑎𝑡𝑡∈𝐴𝐴  are set. The environment emits a reward 𝑅𝑅: 𝑆𝑆× 𝐴𝐴→[𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚, 𝑅𝑅𝑚𝑚𝑚𝑚𝑚𝑚]  on each transition, 𝛾𝛾∈[0,1] \nrepresents a discount factor. 𝜋𝜋(𝑎𝑎𝑡𝑡|𝑠𝑠𝑡𝑡): 𝑆𝑆× 𝐴𝐴→[0,1] is the action probability distribution based on the state. The stochastic \npolicy algorithm specifies an action probability distribution at each state, and the deterministic policy algorithm specifies \na certain action at each state. And Maximum entropy reinforcement learning optimizes policies 𝜋𝜋∗ to maximize both the \nexpected reward and the expected entropy of the policy, as shown in Eq. (1). \n𝜋𝜋∗= 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝐸𝐸𝜋𝜋ൣ∑𝑅𝑅(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡)\n𝑡𝑡\n+ 𝛼𝛼𝛼𝛼൫𝜋𝜋(∙|𝑠𝑠𝑡𝑡)൯൧\n(1)  \nwhere 𝛼𝛼 is temperature parameter that determines the relative importance of the entropy term versus the reward, and thus \ncontrols the stochasticity of the optimal policy. 𝐻𝐻(𝜋𝜋(∙|𝑠𝑠𝑡𝑡)) is the entropy of 𝜋𝜋 at state 𝑠𝑠𝑡𝑡, which is calculated as Eq. (2). \n𝐻𝐻൫𝜋𝜋(∙|𝑠𝑠𝑡𝑡)൯= −𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙(∙|𝑠𝑠𝑡𝑡)\n(2) \nThe core idea of maximum entropy is to explore as far as possible without missing any useful action. Compared with \ndeterministic policy that ends the learning process after finding an optimal path, maximum entropy-based reinforcement \nlearning has a stronger exploration capability because it explores various optimal possibilities in different ways to avoid \nfalling into local optima. This means that the learned policy can more easily find better patterns and be more robust under \nmultimodal rewards, and can make more reasonable adjustments when disturbed [19]. \n2.2. Soft Actor-Critic \nChallenges of some commonly used deep reinforcement learning algorithms in continuous state and action spaces \ninclude: New samples need to be collected at each gradient step, so the number of gradient steps required and the number \nof samples required at each step both increase with the complexity of the task; the effect of both off-policy learning, and \nnetwork-based approximation of high-dimensional nonlinear function on stability and convergence; the fragility and \nsensitivity of the hyperparameters result in the need to use the appropriate parameter values for different tasks. \nSoft Actor-Critic (SAC) [11] is a newly proposed off-policy maximum entropy deep reinforcement learning in recent \nyears, which can be applied to complex high-dimensional tasks. It has been shown to perform well and stably in various \ncommonly used Mujoco benchmark tasks (e.g., humanoid walking task with 21 action dimensions, and quadrupedal \njumping task with 154 observation dimensions [20]) and real robot control tasks (e.g., Minitaur learns walking gait directly \nwith eight direct-drive actuators, and a 3-finger dexterous robotic hand manipulates an object [21]). \nThe state value function 𝑉𝑉(𝑠𝑠𝑡𝑡) of SAC can be expressed by Eq. (3). \n𝑉𝑉(𝑠𝑠𝑡𝑡) = 𝐸𝐸𝜋𝜋[𝑄𝑄(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) −𝛼𝛼𝛼𝛼𝛼𝛼𝛼𝛼𝛼𝛼(𝑎𝑎𝑡𝑡|𝑠𝑠𝑡𝑡)]\n(3) \nSAC parameterizes soft Q-function 𝑄𝑄𝜃𝜃(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) and a tractable policy 𝜋𝜋𝜙𝜙(𝑎𝑎𝑡𝑡|𝑠𝑠𝑡𝑡) and the parameters of networks are \n𝜃𝜃 and ∅. The soft Q-function parameters can be trained to minimize the soft Bellman residual by Eqs. (4) to (5). \n𝐽𝐽𝑄𝑄(𝜃𝜃) = 𝐸𝐸(𝑠𝑠𝑡𝑡,𝑎𝑎𝑡𝑡)~𝐷𝐷൤\n1\n2 ቀ𝑄𝑄𝜃𝜃(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) −𝑄𝑄෠(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡)ቁ\n2\n൨\n(4) \nwith \n𝑄𝑄෠(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) = 𝑟𝑟(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) + 𝛾𝛾𝐸𝐸𝑠𝑠𝑡𝑡+1~𝑝𝑝[𝑉𝑉𝜃𝜃ഥ(𝑠𝑠𝑡𝑡+1)]\n(5) \nThe policy parameters can be learned by directly minimizing the expected KL-divergence, as shown in Eq. (6). \n𝐽𝐽𝜋𝜋(𝜙𝜙) = 𝐸𝐸𝑠𝑠𝑡𝑡~𝐷𝐷,𝑎𝑎𝑡𝑡~𝜋𝜋𝜙𝜙ቂ𝑙𝑙𝑙𝑙𝑙𝑙𝜋𝜋𝜙𝜙(𝑎𝑎𝑡𝑡|𝑠𝑠𝑡𝑡) −\n1\n𝛼𝛼𝑄𝑄𝜃𝜃(𝑠𝑠𝑡𝑡, 𝑎𝑎𝑡𝑡) + 𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙(𝑠𝑠𝑡𝑡)ቃ\n(6) \nIt is only necessary to keep collecting data and reducing the values of these two loss functions to complete training. \n2.3. Genetic Algorithm \nThe dynamic equation of the legged robot is high-order and nonlinear, which requires the gait generation by searching \nfor solutions of the parameters in a very irregular multidimensional space [22], so the standard gradient search-based \noptimization methods are not well suited for legged gait that with high degrees of freedom [23]. To obtain a natural and \nefficient gait two strategies can be followed to coordinate the movement of the legs, the first one assuming that the gait of \nthe animals is optimal (otherwise they would not survive in the wild) and thus using the real motion data of the animals to \ndrive the robot to generate the gait. However, the differences in kinematics and dynamics between animals and legged \nrobots make behavior modification necessary to be introduced [24]. The second one is to convert the gait generation of a \nlegged robot into a multi-objective optimization problem containing multiple constraints [25]. Evolutionary algorithm is a \nglobal probabilistic search based on biological evolutionary mechanism such as natural selection and genetic variation, and \nhas been shown to be effective in solving such large-scale, multi-objective, multi-constrained optimization problems. \nGenetic algorithm (GA) is one of the Evolutionary Computation (EC) solvers and is one of the most popular tools for \ngait optimization. Chernova and Veloso designed adaptive mechanisms with different mutation and crossover probabilities \nto optimize the gait of AIBO [22]. Miguel et al. described the quadruped crawling gait as a multi-objective optimization \nproblem by using GA to explore the parameter space of the CPG network to obtain the gait with minimum body vibration, \nmaximum velocity and maximum stability margin [26]. Chae et al. divided the elliptical trajectory of the foot into three \nphases and searched for the optimal intersection location of each phase to obtain a globally optimized foot trajectory based \non energy and stability criteria and using GA [27]. Evolutionary algorithm usually requires the maintenance of a population \nof candidate solutions and a large number of population iterations, and therefore, evaluating each candidate gait can be \nvery time-consuming. Especially when experimenting on realistic robots, computational efficiency becomes one of the \nbiggest limitations in EC-based gait optimization. To better achieve gait optimization, we believe it is necessary to study \nEC-based optimization algorithms instead of relying on EC alone for learning and optimization. \n3. Framework \nAlthough controllers trained by reinforcement learning have advantages that cannot be replaced by manual design, \nthe increase in complexity of the environment and task leads to the creation of a large state space and makes the \ndisadvantage of low sample utilization in traditional reinforcement learning more significant, in addition to more detailed \nreward settings that increase the difficulty of the problem. In order to achieve spontaneous rhythmic movements of the \nquadruped robot and accelerate the reinforcement learning process, we propose a new gait reinforcement training \nframework based on the idea of behavior evolution as shown in Fig. 2. This framework consists of two main parts: the \nreference action generator (the area in the dashed box) and the reinforcement training process (the light blue area). \nThe reference trajectory 𝑎𝑎𝑟𝑟𝑟𝑟𝑟𝑟(𝑡𝑡) that is generated by the reference action generator can be directly used as an a priori \nreference to accelerate the training during the reinforcement learning of quadruped gait. In a previous study [28], the \nreference action generator only produces a set of reference trajectories or actions initially, and the subsequent training tasks \nare all performed by reinforcement learning. In contrast, inspired by behavior evolution, we design the Reference Trajectory \nOptimizer inside the Reference Action Generator (as shown in the gray area in the figure) to self-improve the reference \naction, so that the reference action is constantly adjusted to better suit the environment. The detailed process of the reference \naction being self-improved is shown by Algorithm 1. This reference action generator requires two types of initial reference \ninformation, one is a rhythm signal 𝜌𝜌 generated by Central Pattern Generator (CPG) for associating gait phase, and this \n𝜌𝜌 consists of motion rhythm signals of the hip and knee joints. The Hopf oscillator with a simple form is employed in this \npaper as the Central Pattern Generator, which can also be replaced by other types of oscillators. Another one is the initial \nfoot trajectory 𝜑𝜑0, which is related to the rhythm signal 𝜌𝜌 using a Radial Basis Function Network (RBFN). RBFN has a \nsimple structure and its local mapping feature can greatly accelerate learning and avoid local minima. The state 𝑅𝑅𝑖𝑖 of the \nhidden layer neurons in RBFN is represented by Eq. (7). \n𝑅𝑅𝑖𝑖= exp ቊ−\n∑൫𝜌𝜌𝑗𝑗−𝜇𝜇𝑖𝑖,𝑗𝑗൯2\n𝜎𝜎𝑅𝑅𝑅𝑅𝑅𝑅\n2\nቋ, 𝑖𝑖= 1, … , 𝐻𝐻; 𝑗𝑗= 1,2,3,4\n(7) \nwhere 𝑖𝑖 is the index of the hidden layer neurons, 𝐻𝐻 is the total number of RBFN neurons, 𝜌𝜌 is the joint position signal, \n𝑗𝑗 is leg index, 𝜇𝜇𝑖𝑖,𝑗𝑗 is the RBFN neuron mean which is calculated by Eq. (8), and 𝜎𝜎𝑅𝑅𝑅𝑅𝑅𝑅\n2\n is the common variance for the \nfour means. \n𝜇𝜇𝑖𝑖,𝑗𝑗= 𝜌𝜌𝑗𝑗ቀ\n(𝑖𝑖−1)∗𝑇𝑇\n𝐻𝐻−1 ቁ, 𝑗𝑗= 1,2,3,4\n(8) \nwhere 𝑇𝑇 is the period of the CPG signal, and 𝐻𝐻= 20, 𝜎𝜎𝑅𝑅𝑅𝑅𝑅𝑅\n2\n= 0.04, 𝑇𝑇= 2. The linear layer output of RBFN can be \ndirectly used as the reference value for the action of each joint of the quadruped robot, as expressed in Eq. (9). \n𝑎𝑎𝑟𝑟𝑟𝑟𝑟𝑟(𝑡𝑡) = 𝑊𝑊∗𝑅𝑅𝑖𝑖(𝑡𝑡) + 𝑏𝑏\n(9) \n \nFig. 2. A reinforcement training framework for walking gait generation of quadruped robots. \n \n \nIn Reference Trajectory Optimizer, a genetic algorithm derived from evolutionary computation is introduced to \nimplement self-improvement of the reference action. It is worth noting that the genetic information used in traditional \ngenetic algorithms is usually typical gait parameters such as step length, swing height, etc. These parameters may limit the \nsearch area of the action, and the interdependence between parameters makes it difficult to be optimized. Instead, we take \na direct search for foot trajectory in the foot motion space, which is easily combined with foot trajectories generated by \nother arbitrary means (e.g., CPG) as movement priors. As shown in Fig. 3, the spatial position vectors (indicated by green \ncircles) of several uniformly distributed green markers on the footpath trajectory are each used as genetic information that \nis adjusted based on a perturbation genetic algorithm that includes crossover or mutation process to enable these position \nvectors to be adjusted to arbitrary new positions (indicated by yellow circles). The trajectory consisting of these markers \nwith new positions is then used to extend the search area of optimization, which has an untypical profile and is significantly \ndifferent from the usual elliptical profile. \n \nFig. 3. The spatial profile of the foot trajectory is modified based on evolutionary computation. \nSpecifically, the initial foot trajectory 𝜑𝜑0 contains 𝑘𝑘 markers, and the position vector corresponding to each marker \npoint is denoted as 𝑃𝑃𝐼𝐼. The set of 𝑃𝑃𝐼𝐼 is taken as the reference trajectory of the foot at that moment. Then the 𝑘𝑘 additional \nvectors 𝑣𝑣𝑎𝑎 with initial values of [0,0] are taken as incremental values of 𝑃𝑃𝐼𝐼 to be used to update 𝑃𝑃𝐼𝐼 and therefore to \nupdate the reference trajectory. We use the Evolutionary Computation Solver to update the additional vector 𝑣𝑣𝑎𝑎. To ensure \nthe breadth as well as the optimization of the exploration reference trajectory, the evolutionary computation solver updates \n𝑁𝑁EC_episode episodes each time 𝑁𝑁updateRAG  steps is completed, generating 𝑁𝑁  new sets of 𝑣𝑣𝑎𝑎  as alternatives in each \nepisode. The fitness criterion is used to evaluate the search effectiveness of each set of alternative 𝑣𝑣𝑎𝑎, i.e. to evaluate the \nperformance of the corresponding alternative reference trajectory. The evaluation process is shown in Algorithm 2. The \nmain process is to first sample the total reward in one episode (containing 𝑁𝑁EC_step steps) walking evaluation and use them \nas the individual fitness 𝐹𝐹𝑖𝑖 , then rank all the fitness to select the optimal 𝑣𝑣𝑎𝑎 and update the RBFN based on the \ncorresponding 𝑊𝑊𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 and 𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏. \n \nReinforcement learning plays an incremental learning role in the proposed framework, and we specifically use the \nSAC algorithm responsible for training the quadruped robot to interact with its environment. The reference action 𝑎𝑎𝑟𝑟𝑟𝑟𝑟𝑟(𝑡𝑡) \ngenerated by the Reference Action Generator is used as the reference action at the beginning of training, based on which \nthe joint angle increment ∆𝑎𝑎(𝑡𝑡) is trained by reinforcement learning to further optimize the gait of the quadruped robot. It \nshould be noted that both parameter updates in the Reference Action Generator and parameter updates in reinforcement \nlearning result in different behavior distributions. Therefore, to make the training more stable, the parameters of the \nreinforcement learning policy are locked when the reference action is updated, and the parameters in the Reference Action \nGenerator are locked when the parameters of the reinforcement learning policy are updated, and these two processes are \nexecuted alternately. Specifically, the parameters in Reference Action Generator are updated every 𝑁𝑁updateRAG steps, as \ndetailed in Algorithm 3. \n \nObservation Space. The input state 𝑠𝑠𝑡𝑡∈ℝ49  contains the three-dimensional linear velocity of the body 𝑣𝑣=\n[𝑣𝑣𝑥𝑥, 𝑣𝑣𝑦𝑦, 𝑣𝑣𝑧𝑧], the twelve-dimensional angular velocity of the joints 𝑞𝑞̇ = [𝑞𝑞̇0, 𝑞𝑞̇1, 𝑞𝑞̇2, . . . , 𝑞𝑞̇11], the three-dimensional pose of \nthe body 𝜓𝜓= [𝑅𝑅, 𝑃𝑃, 𝑌𝑌], the differential of the three-dimensional pose of the body 𝜑𝜑= [𝑑𝑑𝑑𝑑, 𝑑𝑑𝑑𝑑, 𝑑𝑑𝑑𝑑], the four-dimensional \nfoot touchdown state 𝑐𝑐𝑓𝑓= [𝑐𝑐0, 𝑐𝑐1, 𝑐𝑐2, 𝑐𝑐3], the twelve-dimensional foot contact force 𝐹𝐹𝑓𝑓= [𝐹𝐹𝑥𝑥0, 𝐹𝐹𝑦𝑦0, 𝐹𝐹𝑧𝑧0, . . . , 𝐹𝐹𝑥𝑥3, 𝐹𝐹𝑦𝑦3, 𝐹𝐹𝑧𝑧3], \nand the twelve-dimensional foot position relative to the body 𝑝𝑝𝑓𝑓= [𝑝𝑝𝑥𝑥0, 𝑝𝑝𝑦𝑦0, 𝑝𝑝𝑧𝑧0, . . . , 𝑝𝑝𝑥𝑥3, 𝑝𝑝𝑦𝑦3, 𝑝𝑝𝑧𝑧3]. \nAction Space. The action 𝑎𝑎𝑡𝑡∈ℝ12 output from the reinforcement learning is the position increment ∆𝑞𝑞𝑡𝑡 of 12 joints, \nwhich is added to the reference action provided by the Reference Action Generator to obtain the expected value of the joint \nposition at the current moment, as shown in Eq. (10). \n𝑞𝑞ො𝑡𝑡= 𝑞𝑞𝑡𝑡\n𝑟𝑟𝑟𝑟𝑟𝑟+ ∆𝑞𝑞𝑡𝑡\n(10) \nThe PD controller is used to calculate the joint torque τ, as shown in Eq. (11). \n𝜏𝜏𝑡𝑡= 𝐾𝐾𝑝𝑝(𝑞𝑞ො𝑡𝑡−𝑞𝑞𝑡𝑡) + 𝐾𝐾𝑑𝑑൫𝑞𝑞̇෠𝑡𝑡−𝑞𝑞̇𝑡𝑡൯\n(11) \nwhere 𝐾𝐾𝑝𝑝 and 𝐾𝐾𝑑𝑑 are kept constant in the simulation. The parameters 𝑞𝑞ො𝑡𝑡 and 𝑞𝑞̇෠𝑡𝑡 represent the expected joint position and \nthe desired joint velocity, respectively, and 𝑞𝑞̇෠𝑡𝑡 is set to 0. \nReward function. The reward function is designed to enable the robot to achieve desired target speed while \nsuccessfully learning the gait, and to remain stable and energy efficient during movement. Therefore, the reward function \nis designed as 𝜔𝜔1𝑅𝑅𝑣𝑣+ 𝜔𝜔2𝑅𝑅𝑒𝑒+ 𝜔𝜔3𝑅𝑅𝑏𝑏+ 𝜔𝜔4𝑅𝑅𝑓𝑓+ 𝜔𝜔5𝑅𝑅𝑐𝑐+ 𝜔𝜔6𝑅𝑅𝑢𝑢 , where 𝜔𝜔= [1.5,0.07,0.6,0.3,0.1,0.1] . The desired \nvelocity of the body is defined as 𝑉𝑉𝑑𝑑, the actual velocity of the body is defined as 𝑉𝑉𝑐𝑐, 𝜏𝜏 is the torque of the motor, 𝑞𝑞̇ is the \nangular velocity of the motor, 𝑘𝑘 is the number of motors, 𝑉𝑉𝑓𝑓,𝑙𝑙 denotes the foot velocity when the 𝑙𝑙th foot touches the \nground, 𝐶𝐶𝑡𝑡 denotes the number of current support legs, 𝐶𝐶𝑑𝑑 denotes the number of support legs in ideal condition, and 𝐶𝐶𝑢𝑢 \ndenotes the number of parts where unexpected contact occurs. A curriculum factor 𝑐𝑐𝑘𝑘= 1 −𝑡𝑡𝑡𝑡𝑡𝑡ℎ(4.5 ∗𝑚𝑚𝑚𝑚𝑚𝑚(𝑉𝑉𝑥𝑥−\n𝑉𝑉𝑑𝑑, 0)2) is introduced to make the robot prioritize learning the primary task (e.g., following the desired velocity) and to \nprevent the tendency to stand still [29]. As the speed of acquisition gradually reaches the target during training, 𝑐𝑐𝑘𝑘 \nincreases accordingly to raise the weight of the other objectives in the total objective; the coefficients 𝑐𝑐𝑏𝑏= 4, 𝑐𝑐𝑓𝑓= 2.5 \nare set. Then the specific components of the award in each step are shown in Table 1. \nTable 1. Reward components. \nIndex \nAward item \nSymbol \nExpression \n1 \nLinear velocity of the base \n𝑅𝑅𝑣𝑣 \n𝑅𝑅𝑣𝑣= 𝑚𝑚𝑚𝑚𝑚𝑚(𝑉𝑉𝑑𝑑, 𝑉𝑉𝑐𝑐) \n2 \nEnergy consumption \n𝑅𝑅𝑒𝑒 \n𝑅𝑅𝑒𝑒= −𝑐𝑐𝑘𝑘෍|(𝜏𝜏∗𝑞𝑞̇) ∗∆𝑡𝑡| \n3 \nBase motion \n𝑅𝑅𝑏𝑏 \n𝑅𝑅𝑏𝑏= 𝑐𝑐𝑘𝑘(𝑡𝑡𝑡𝑡𝑡𝑡ℎ(𝑐𝑐𝑏𝑏||𝜔𝜔𝑥𝑥𝑥𝑥||2) −1) \n4 \nFoot velocity variation \n𝑅𝑅𝑓𝑓 \n𝑅𝑅𝑓𝑓=\n෍\n𝑐𝑐𝑓𝑓𝑚𝑚𝑚𝑚𝑚𝑚(𝑉𝑉𝑑𝑑, 𝑉𝑉𝑐𝑐)\n𝑙𝑙∈{𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓 𝑖𝑖𝑖𝑖 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐}\n \n5 \nFoot touchdown condition \n𝑅𝑅𝑐𝑐 \n𝑅𝑅𝑐𝑐= −𝑚𝑚𝑚𝑚𝑚𝑚(𝐶𝐶𝑡𝑡−𝐶𝐶𝑑𝑑, 0) \n6 \nUnexpected contact \n𝑅𝑅𝑢𝑢 \n𝑅𝑅𝑢𝑢= −𝐶𝐶𝑢𝑢 \nThe linear velocity reward 𝑅𝑅𝑣𝑣 is used to encourage the robot to track target velocity. The energy consumption reward \n𝑅𝑅𝑒𝑒  is used both to penalize excessive joint torque to prevent damage to the joint actuators and to reduce the energy \nconsumption of the motion. The base motion reward 𝑅𝑅𝑏𝑏 is used to penalize excessive rolling movements and excessive \npitching movements to allow the body to remain stable during travel. The foot velocity variation reward 𝑅𝑅𝑓𝑓 is used to limit \nthe foot velocity to prevent the robot from slipping and excessive contact force between the foot and the ground resulting \nin structural damage. Foot contact reward 𝑅𝑅𝑐𝑐 is a crucial reward for rapid learning of the target gait, and excessive foot \ncontact is penalized to allow each leg to lift or touch the ground correctly according to the preset phase. The unexpected \ncontact 𝑅𝑅𝑢𝑢 is used to penalize unreasonable collisions and contacts, such as the contact between the body and the ground. \nExperience Replay Buffer. In the proposed framework, Reference Action Generator shares the same experience \nreplay buffer with the reinforcement training process. Since evaluation of fitness in Reference Trajectory Optimizer is \ncomputed similarly to the total reward obtained in one episode for a new trajectory reinforcement learning-based walking \ntraining task, the sample-based experience generated from each fitness evaluation is stored in the experience replay buffer, \nthus providing more samples for learner updates in reinforcement learning. This data reuse mechanism allows experience \nto be transferred from the evolutionary population to the reinforcement learning learners, providing experience that may \ncontribute to higher long-term reward while improving sampling efficiency [30]. \n4. Results \n4.1. Test Conditions \nPybullet software, with version 3.2.0, is used as the simulation platform for this study. The quadruped robot used in \nthe simulation is the GO1 quadruped robot, which contains three independent joints in each leg, thus enabling the \nmovement of each leg and body in 3D space. The position angle and angular velocity of each joint can be measured directly, \nbody posture and locomotion acceleration can be measured by IMU, and the contact force and contact state between each \nfoot and the ground can be measured by the force sensor on the foot. The parameters of the robot are shown in Table 2. \nTable 2. Robot parameters. \nParameter \nValue \nBase mass / Leg mass \n10.5 kg / 2 kg \nNumber of joints \n3*4 \nMax motor torque \n33.5 N*m \nHip length / Thigh length / Shank length \n0.1 m / 0.2 m / 0.2 m \nAbd kp/kd \n80 / 2 \nHip kp/kd \n120 / 4 \nKnee kp/kd \n90 / 3 \nInitial position \n[0, 0, 0.26 m] \nInitial motor angles \n[0, 0.9 rad, -1.8 rad] \nThe primary training goal is to make the quadruped robot walk stably in regular terrain, with expected speed and gait. \nTherefore, we design varied terrains for simulation, including basic terrains such as horizontal ground, slope, and stairs, as \nwell as several combined terrains such as continuous up-downslope and up-downstairs, which are depicted in Fig. 4. \n \nFig. 4. Base and combined terrains used in training simulation. (a) Simple base terrain. (b) Combined terrains that are built from direct \ncombination of different base terrains. Ten same combined terrain units are combined again directly for testing, i.e., N=10. Other \nparameters used to create terrain include 𝑙𝑙0 = 1 m, 𝑙𝑙1 = 1 m, 𝑙𝑙2 = 1.65 m, ℎ= 0.15 m. \nThe total number of steps for each training task is set to one million, and several groups are employed to distinguish \nthe respective goal and characteristic of these training tasks. The specific description of these groups is shown in Table 3. \nTable 3. The groups used in the test. \nGroup \nRL algorithm \nDimensions of \nstate space \nReference Trajectory \n37 \n49 \nFixed \nOptimized \nOptimization method \nGroup 0 \nSAC \n√ \n \n√ \n \n- \nGroup 1 \nSAC \n \n√ \n√ \n \n- \nGroup 2 \nSAC \n \n√ \n \n√ \nGenetic algorithm \nGroup 3 \nSAC \n \n√ \n \n√ \nUniform distribution \nGroup 4 \nSAC \n \n√ \n \n√ \nNormal distribution \nGroup 5 \nSAC \n√ \n \n \n√ \nGenetic algorithm \nThe parameters of the SAC algorithm and the neural network used in the tests are shown in Table 4. \nTable 4. SAC hyperparameters and neural network size. \nParameter \nSymbol \nValue \nbatch size \n \n256 \ninitial steps \n \n10000 \nreplay buffer size \n \n 106 \nlearning rate \n 𝑙𝑙𝑙𝑙 \n3 × 10−4 \ntemperature parameter \n 𝛼𝛼 \n0.2 \ndiscounted factor \n 𝛾𝛾 \n0.99 \nnumber of hidden layers \n \n2 \nnumber of hidden units per layer \n \n256 \nactivation \n \ntanh \nThe results and analysis of the tests by different factors are as follows. \n4.2. Observation Space \nThe dimension of the observation space can have an impact on the reinforcement learning training; it is generally \nbelieved that too few observation dimensions make the description of the current state inaccurate or missing, and too many \nobservation dimensions can lead to curse of dimensionality. We first investigate the effect of increasing only the state space \ndimension on the motion performance of the quadruped robot and the improvement of terrain adaptation from the aspect \nof reinforcement learning. For this purpose, we consider two observation spaces with different proprioception: full \nobservation and partial observation. Since training an inexperienced robot is a very difficult task, for this reason we provide \na pre-defined foot trajectory as a reference. It is worth noting that the reference trajectory used in the test are roughly given \nand not necessarily the appropriate reference. \nFull observation (49 dim): Full observation focuses on perception of both movement speed and proprioception, \nspecifically body speed 𝑣𝑣 (body state), joint angular velocity 𝑞𝑞̇ (joint state), foot contact state 𝑐𝑐𝑓𝑓, foot contact force 𝐹𝐹𝑓𝑓, \nand the relative position of the foot 𝑝𝑝𝑓𝑓. \nPartial observation (37 dim): Partial observation removes only the relative position of the foot on the basis of the \nfull observation. \nThe rewards obtained from training based on each of the two observation spaces in different terrains that as shown in \nFig. 5(a) are shown in Fig. 5(b), where the blue line corresponds to Group 0 and the red line corresponds to Group 1. In \nthe tests on all terrains, Group 0 achieves the training goal only on horizontal ground, while by introducing additional \ninformation 𝑝𝑝𝑓𝑓, which is closely related to gait movement, Group 1 obtains an increase of about 67% in average reward \nvalue on horizontal ground compared to Group 0 and obtains a higher reward value on 15° slope. In addition, the reward \nof Group 1 in 15° slope shows significant fluctuation, indirectly reflecting that Group 1 has some chance to explore the \nproper policy, which indicates that increasing the observation dimension by adding foot position information has an \nenhanced effect on training in simple terrain. In contrast, for steeper slope or more complex terrain, the richness of the \nobservation dimension does not lead to a significant improvement in the training effect. Considering that the reference \ntrajectory used in these two groups of tests is arbitrarily set, we suppose that even a suitable design of observation dimension \nand reward function may not achieve the desired training effect if it is based on an unreasonable reference trajectory. This \nimplies that improving the initial reference trajectory rather than adopting a constant reference trajectory is necessary for \ngait learning in quadruped robots. \n4.3. Reference Trajectory Optimization \n Since a greater variety of observation dimensions more or less improves the training effect, we directly test the gait \ntraining framework based on full observations. In this test, each time the reference trajectory is updated, the framework \nautomatically adjusts the reference trajectory by using a genetic algorithm to search for 10 episodes, generating 40 sets of \ncandidate solutions in each episode, and then selecting the unique optimal solution among the 10 episodes as the result, \nand using the result as the reference trajectory for incremental learning thereafter. To avoid the lack of sample richness at \nthe beginning of the training, 10,000 steps of training is performed using reinforcement learning before the reference \ntrajectory is updated for the first time and the corresponding result data is stored in the Experience Replay Buffer. From \nthe training results corresponding to Group 0 and Group 1, it can be inferred that the initial reference trajectory set by \nexperience is not entirely suitable, so the reference trajectory is updated by optimization for the first time when the total \nnumber of training steps reaches 10,000, and then updated again every 50,000 steps. \nThe training results that obtained after including the reference trajectory update mechanism are shown in Fig. 5(c), \nwhere the green line corresponds to Group 2. It can be noticed that Group 2 has significantly improved the training effect \non each terrain compared with Group 1. In the simplest horizontal ground test, the average reward value obtained by Group \n2 increased by about 147% compared to Group 1, while for all other training tests on more complex terrain, Group 2 is \nsuccessful and obtains higher rewards. Further, based on the analysis of the reward composition, it can be found that the \nreward values of Group 2 are all higher than those of Group 1. This result fully illustrates that the policy learned through \nthe introduction of the reference trajectory improvement mechanism is more likely to find better combination patterns \nunder multimodal rewards, and can effectively improve the adaptability of the quadruped robot to complex terrain. \n4.4. Trajectory Update Rule \nTraining tests based on Group 3 and Group 4 are performed and the test results are compared with the results based \non Group 2 to prove that the trajectory optimization based on genetic algorithm search is reasonable. Group 3 takes a \nuniform distribution criterion to randomly sample additional vectors in the range [0, 0.01], while Group 4 uses a standard \nnormal distribution criterion (𝜇𝜇= 0, 𝜎𝜎= 0.01) for random sampling. Two regular terrains, horizontal ground and up-\ndownslope, are used for the test, where three groups have exactly the same trajectory update frequency, number of update \nepisodes, number of candidate solutions per episode and fitness evaluation criteria. The test results are shown in Fig. 5(f), \nwhere the yellow line represents Group 3 and the pink line represents Group 4. \nThe results of the fitness evaluation show that each of the three different update methods for the additional vectors \nexplored the correct policy in the horizontal ground test, and it is noted that the trajectory updated by the genetic algorithm \nbased on the genetic algorithm shows a higher fitness compared to the other two methods. In the up-downslope tests, the \nadvantage of the genetic algorithm is more obvious, as both Group 3 and Group 4 are unable to explore a suitable reference \ntrajectory during the whole training process, thus leading to the failure of the training task, while Group 2 is quickly updated \nto a suitable reference trajectory and completes the training task. In addition, the sampling ranges of the uniform and normal \ndistributions are artificially set, and we find that the probability of failure is very high if the sampling range is set large, so \nthe regulation of the parameters of these two sampling methods needs to rely on expert experience. This shows that if a \nrandom reference trajectory is used for exploration, or if a reference trajectory distributed within a region based on some \nrules is used for exploration, it is not as effective as a reference trajectory based on genetic algorithm. \n4.5. Suitability of Reference Trajectory Optimization \nConsidering that the proposed gait training framework shows a better improvement compared to the conventional \nreinforcement learning framework in the full observation dimension test, here we continue to test this framework based on \npartial observation dimension, and the experimental results are shown in Fig. 5(d). It can be found that the training success \nrate corresponding to Group 5 is significantly higher compared to the results of Group 0, which indicates that the reference \ntrajectory optimization mechanism is generally effective for enhancing reinforcement learning-based gait training. \n4.6. Walking Performance \nAfter the training is completed, the performance of the policy in quadruped robot walking is then analyzed based on \nspecific factors such as power consumption, walking balance, and anti-disturbance during movement. The Wide Stability \nMargin (WSM) is used to evaluate the walking stability, which is defined as the distance 𝑑𝑑 from the projection point of \nthe robot's center of gravity to the intersection of the two diagonals of the support polygon. The smaller the value of 𝑑𝑑, the \nbetter the stability. In addition, the total power 𝑝𝑝𝑤𝑤 of the robot is defined as the sum of the product of the torque of each \njoint actuator and its angular velocity, as shown in Eq. (12). \n𝑝𝑝𝑤𝑤= ∑\n∑\nห𝜏𝜏𝑖𝑖𝑖𝑖𝑞𝑞̇𝑖𝑖𝑖𝑖ห\n𝑛𝑛\n𝑗𝑗=1\n𝑚𝑚\n𝑖𝑖=1\n(12) \nwhere 𝑚𝑚 is the number of legs and 𝑛𝑛 is the number of joints in each leg. \nHorizontal ground and combined terrain are used to test the locomotion performance of the quadruped robot. The \nperformance corresponding to the best policy obtained by training on horizontal ground is shown in Fig. 6(a) to (g), where \nall the data are taken from a period of time after starting the movement from the standing state. After the robot completes \nthe transition from the standing state to the walking state within 2 seconds (corresponding to the first 100 training steps), \nthe angles of all joints start to show a stable periodicity, as shown in Fig. 6(a), while the posture of the body and the height \nof the center of mass are kept to change within a certain range, and the quadruped robot keeps walking stably. Fig. 6(d) \nshows the optimization process of the foot trajectory during the training process. Since the genetic algorithm selects the \noptimal solution to update the foot position 𝑃𝑃𝐼𝐼 after a multi-round search of the additional vectors, each change in the \nspatial profile of the foot trajectory is considered to be more optimal compared to the one before the update. The optimal \nfoot trajectory profile shown in Fig. 6(d) is used as the reference trajectory, and the foot trajectory obtained after incremental \nreinforcement learning based on this reference trajectory is shown in Fig. 6(e). In addition, conditions under different \ndesired walking speeds (0.5m/s, 1.5m/s, 2.5m/s) and random disturbance are tested, and Fig. 6(b) shows the body state at \ndifferent speeds, and the disturbance is imposed as shown in Fig. 6(h). For higher desired speed, the trained policy can still \nkeep the variation of pitch angle, roll angle and center-of-mass height of the body within a certain range. Moreover, the \nmotion state can be recovered quickly after imposing random disturbance. If the desired speed is set to 0.5m/s, 1.5m/s and \n2.5m/s, the average speed after training is 0.59m/s, 1.64m/s and 2.49m/s, respectively, as shown in Fig. 6(c). Further, the \ntest results of power consumption and WSM for different desired speed conditions are shown in Fig. 6(f) and Fig. 6(g), \nrespectively. It can be observed that as the target speed increases, the power consumption will increase accordingly and the \nstability will decrease. It is worth noting that the policy trained by exerting random perturbations during training has \ninsignificant changes in power consumption, but has better walking stability. \n \nFig. 5. Rewards for training in a variety of terrains. (a) Terrains for testing. (b) Comparison of training based on different observation \ndimension, i.e., Group 0 vs Group 1. (c) Comparison of training based on whether to refer to the trajectory optimization mechanism, \ni.e., group 1 vs group 2. (d) Comparison of training based on whether to refer to the trajectory optimization mechanism in lower \ndimension condition, i.e., group 0 vs. group 5. (e) Comparison of average rewards obtained by Group 0, 1, 2, and 5. (f) Comparison of \ntraining based on different reference trajectory update methods, i.e., comparison between groups 2, 3, and 4. \n \nFig. 6. Evaluation of the locomotion performance of the gait policy acquired from training, (a) to (h) correspond to training tests on \nhorizontal terrain, and (i) to (n) correspond to training tests on upslope-downstairs terrain. (a) Optimal reference movements (light lines) \nfor both front legs within 2 seconds (100 training steps) and actual movements (dark lines) after training. (b) The roll angle, pitch angle \nand height of the CoM at different speeds, where the last is when an external force is exerted. (c) The actual average speed when the \ndesired speed is 0.5m/s, 1.5m/s and 2.5m/s respectively. (d) The evolution of the foot reference trajectory within 1 million steps, which \nis obtained based on genetic algorithm search and corresponding to a change in color from light to dark. (e) Foot trajectory of the \nquadruped robot under optimal policy. (f) Power consumption in 6s (300 training steps) with different speeds and disturbance. (g) WSN \nin 6s with different speeds and disturbance. (h) External force used to simulate random disturbance is applied at 1s intervals, each lasting \n1s. (i) Optimal reference movement of the two front legs and the actual movement in the test. (j) The roll angle, pitch angle and height \nof the CoM at a speed of 0.5m/s. (k) to (n) are similar to (a) to (g), and the difference is mainly in terrain. \n \nFig. 7. Detailed analysis of upward slope and downward stairs. \nThe performance of the quadruped robot in 6 seconds (corresponding to the first 300 training steps) during the upslope-\ndownstairs is shown in Fig. 6(i) to Fig. 6(n). The results show that the trained policy enables the quadruped robot to \nmaintain a stable rhythmic walking even in the combined terrain, as shown in Fig. 6(i) and Fig. 6(j). The reference trajectory \nof the foot is optimized over a larger area, as shown in Fig. 6(k) and Fig. 6(l). In addition, the power consumption and \nWSM are kept within a certain range, as shown in Fig. 6(m) and Fig. 6(n). The detailed process of walking in this terrain \nis shown in Fig. 7, where the power is the instantaneous value and not the average value as shown in Fig. 6(m). The sudden \nchange in power consumption first occurs during the upward slope period of about 0.6 to 1.8 seconds, when the quadruped \nrobot uses more strength to adjust its posture to maintain balance and thus adapt to the transition from flat to slope. When \ngoing uphill, the hip and knee joints of the front legs are bent, the hip and knee joints of the rear legs are extended, and the \npitching motion of the body changes greatly. Another significant change occurs during the slope-to-stairs transition from \n1.8 to 2.4 seconds, with corresponding changes in body pitch and roll movements, and the change in posture well explains \nwhy the WSM becomes larger. In the process of going down the stairs from 2.4 to 4.2 seconds, the pitch angle shows \nobvious periodic changes, and the power changes are also significant because the quadruped robot adjusts its posture to \nmaintain balance by increasing the motion amplitude of each joint in this process. Moreover, since the most critical training \nobjective is stable walking rather than energy conservation, the reward term on energy has a low weight in the total reward \nfunction, which is an important reason for the apparent power increase condition during down stairs or during terrain \ntransition. \nThe power consumption and WSM of the quadruped robot during walking on different types of terrain are shown in \nFig. 8 and Fig. 9, respectively, for a time range of 6 seconds from the start of movement, which corresponds to 300 training \ntest steps. In these tests, the power consumption of the robot basically remained within a certain range, and only during the \nprocess of going up the stairs the power consumption becomes significantly larger, which includes both the power demand \nof rapidly lifting the body and also due to the process of transitioning from the flat ground to the stairs, the robot needs \nmore strength to adjust the body posture to maintain balance. Accordingly, the WSM during upstairs is also the largest. \n \nFig. 8. Comparison of power consumption of the quadruped robot during walking on different terrain. \n \nFig. 9. Comparison of WSM of the quadruped robot during walking on different terrain. \n4.7. Parallel Training \nThe framework proposed in this paper enables parallel training to reduce the time cost of the training process, and the \nscheme we design by incorporating the parallel mechanism into the reinforcement training component is shown in Fig. 10. \nIn this scheme, the Reference Action Optimizer and Global Network are executed alternately on a main thread running on \nCPU 0, as shown in the purple area. Multiple parallel Agents responsible for reinforcement learning are built independently \nduring training and interact with independent environment to collect independent experience. We use Parl to distribute all \nthe Agents into separate CPUs, as shown in the blue area. It should be noted that simultaneous update is introduced, i.e., \nin each training episode, the Global Network waits for each Agent to complete the current episode individually, then \naggregates and averages the gradient data uploaded by these Agents to obtain a uniform gradient and updates the parameters \nof the main network with it, and finally updates all the Agents simultaneously using these parameters. The computing \nplatform used in this paper is based on the Intel(R) Core™ i9-10900X CPU @ 3.70GHz, and 64G of RAM. Based on this \nhardware configuration, we test the parallel framework in two types of terrain, horizontal ground and upslope-downstairs, \nwhich consume 8h35m and 9h52m in single process test, and 4h26m and 4h19m when using 10 Agents for parallel \ncomputation, this result shows that the elapsed time for parallel training is reduced by about 50% compared to the non-\nparallel one. If the Reference Action Generator is also designed in parallel, the training time can be further reduced. \n \nFig. 10. Parallelized training framework. \n5. Discussion \nThe constituents of the rewards obtained during training in two different terrain conditions, respectively, are shown in \nFig. 11. It can be found that in the early stage of training, the undesired contact rewards 𝑅𝑅𝑢𝑢  account for the largest \nproportion of the total reward, which indicates that at this stage the robot is mainly in a fall or self-collision state and cannot \nwalk normally. After training convergence, the largest proportion of speed rewards 𝑅𝑅𝑣𝑣 and basal movement rewards 𝑅𝑅𝑏𝑏 \nare observed. This indicates that as the reference trajectory is continuously improved and incremental learning continues, \nthe robot gradually establishes a target policy that allows it to achieve the desired tracking speed while maintaining the \nstability of the body. During the training convergence process, the continued optimization of the reference trajectory still \nhas an impact on the reward distribution. For example, in training on horizontal ground as shown on the left chart of Fig. \n11, the situation where the reference trajectory is updated at the 100,000th step to obtain a higher walking speed at the cost \nof a larger motion bump (corresponding to a smaller 𝑅𝑅𝑣𝑣 and a larger 𝑅𝑅𝑏𝑏) is re-optimized after a period of exploration \n(corresponding to increasing 𝑅𝑅𝑣𝑣 and decreasing 𝑅𝑅𝑏𝑏), and the ratio of both 𝑅𝑅𝑐𝑐 and 𝑅𝑅𝑒𝑒 also changes significantly in this \nprocess, all of which indicate that the optimization of the reference trajectory after training convergence can continue to \nmotivate the target policy to explore better combination patterns under multimodal rewards. In addition, the proportion of \nfoot contact rewards 𝑅𝑅𝑐𝑐 and energy consumption rewards 𝑅𝑅𝑒𝑒 decreases, which also shows that gait is correctly learned \nwhile energy consumption is further reduced. \n \nFig. 11. Constituents of the rewards in training. The left chart corresponds to the training task on horizontal terrain, and the right chart \ncorresponds to the task on upslope-downstairs terrain. The data for both charts are taken from the first 200,000 training steps of each. \nThe test results have sufficiently demonstrated that both enhancing the state space dimension and optimizing the \nreference trajectory are beneficial in improving the training success rate, but further observation of Fig. 5(e) clearly shows \nthat optimizing the reference trajectory can be more effective in adapting to various terrain than simply increasing the state \ndimension. This may be due to the fact that adding specific state dimensions is only applicable to particular training tasks, \nand the reasonableness of the initially provided reference trajectory greatly affects the training effect, both of which make \nthe benefits of simply adding state dimensions often not outstanding. In contrast, the reference trajectory can be improved \nby using the genetic algorithm to spontaneously perform optimal search for different objectives, and then gradually \neliminate the unreasonable factors in the initial trajectory during the reference trajectory optimization to obtain more \ntraining gains, as well demonstrated by the tests in 15° and 20° slope. \nAlthough the gait training framework proposed in this paper has obvious enhancement effects for both state spaces \nwith different dimensions, the optimized reference trajectory may be searched faster based on the rich state dimension, and \nin addition setting a more appropriate initial reference trajectory can also improve the training efficiency. \nIn addition, to validate the selection of reinforcement learning algorithm, two other popular deterministic policy \nalgorithms, DDPG and TD3, are tested and their results in the horizontal ground training are shown in Table 5. The results \nshow that the maximum entropy-based stochastic policy performs better than the deterministic policy for the proposed \napproach. \nTable 5. Average reward of different RL algorithms. \nAlgorithm \nReward \nDDPG \n291.6 \nTD3 \n7.11 \nSAC \n559.15 \n6. Conclusion \nReinforcement learning methods have been widely used to study gait generation in quadruped robots, not only because \nof their appealing random exploration and reward mechanism, but also because they are so similar to the process of learning \nto walk in humans and other animals - by continuously trying in order to eventually succeed. Although this romantic \napproach is part of many researchers' exploration of the nature of gait learning, it doesn’t go entirely smoothly due to the \ndifferences between robots and animals in various aspects. This paper reconsiders the incremental reinforcement learning \napproach that has been commonly used from a behavioral bionic perspective, and identifies one of the important reasons \nfor its staged success as the inheritance of progressive features in the evolution of motion behavior in animals. On this \nbasis, we introduce another key factor in the evolution of motion behavior in animals, that is, a self-improvement \nmechanism for the reference gait. If the reference gait is obtained from real animal data, it further reflects the orientation \nof factors such as balance, energy consumption, and habituation that are embedded in the long process of acquiring this \nbehavior in animals. Based on a more comprehensive imitation of the evolution of animal motion behavior, we construct a \nnew framework for gait training to incorporate both incremental reinforcement learning of action and self-improvement of \nreference trajectory. \nFor the two main components of the framework, the genetic algorithm and the SAC algorithm are adopted to complete \nthe final design of the framework, and the reasons for their selection are explained separately. Simulation tests in various \nterrains are performed in detail, all data results are recorded based on statistics and three main conclusions are obtained. \nFirst, compared with conventional reinforcement learning algorithms, the proposed framework achieves good results in all \nterrains under test; second, the self-improvement of the reference trajectory is the most significant in terms of its \ncontribution to successful training, in contrast to the fact that simply increasing the state-space dimension is often futile in \ntraining tasks on complex terrains; third, the gait acquired by training with the proposed framework maintains a good level \nof stability and power consumption. Therefore, the research in this paper allows the application of reinforcement learning \nto gait training and even real quadruped robot traveling tasks to be advanced. In addition, this framework is partially \nparallelized to reduce the training time by about half, which is the preliminary work to proceed with the deployment of the \nframework in real robots, and prototype experiments will be carried out in the future. \nAcknowledgment \nThis work was supported by the National Natural Science Foundation of China [Grant number 61733002]. \nReference \n[1]. J. Di Carlo, P. M. Wensing, B. Katz, G. Bledt and S. Kim, \"Dynamic locomotion in the MIT Cheetah 3 through convex \nmodel-predictive control,\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), \nMadrid, Spain, 2018, pp. 1-9. https://doi.org/10.1109/IROS.2018.8594448. \n[2]. G. Bledt, M. J. Powell, B. Katz, J. Di Carlo, P. M. Wensing and S. Kim, \"MIT Cheetah 3: design and control of a \nrobust, dynamic quadruped robot,\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems \n(IROS), Madrid, Spain, 2018, pp. 2245-2252. https://doi.org/10.1109/IROS.2018.8593885. \n[3]. D. Kim, J. Di Carlo, B. Katz, G. Bledt and S. Kim, \"Highly dynamic quadruped locomotion via whole-body impulse \ncontrol and model predictive control.\" arXiv preprint, arXiv:1909.06586 (2019). \nhttps://doi.org/10.48550/arXiv.1909.06586. \n[4]. J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun and M. Hutter, \"Learning quadrupedal locomotion over challenging \nterrain.\" Science robotics 5.47 (2020): eabc5986. https://doi.org/10.1126/scirobotics.abc5986. \n[5]. J.C.H. Christopher, P. Dayan. \"Q-learning.\" Machine learning, 8: 279-292 (1992).  \nhttps://doi.org/10.1007/BF00992698. \n[6]. V. Mnih, K. Kavukcuoglu, D. Silver, \"Human-level control through deep reinforcement learning. \" Nature 518, 529-\n533 (2015). https://doi.org/10.1038/nature14236. \n[7]. CY. Yang, K. Yuan, Q. Zhu, W.Yu and Z. Li, \"Multi-expert learning of adaptive legged locomotion.\" Science Robotics \n5.49 (2020): eabb2174. https://doi.org/10.1126/scirobotics.abb2174. \n[8]. D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra and M. Riedmiller, \"Deterministic policy gradient algorithms.\" \nInternational conference on machine learning. PMLR 32(1):387-395, 2014. \n[9]. V. Mnih, AP. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver and K. Kavukcuoglu. \"Asynchronous \nmethods for deep reinforcement learning.\" International conference on machine learning. PMLR, 2016. \n[10]. J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Kilmov. \"Proximal policy optimization algorithms.\" arXiv \npreprint arXiv:1707.06347 (2017). https://doi.org/10.48550/arXiv.1707.06347. \n[11]. T. Haarnoja, A. Zhou, P. Abbeel and S. Levine, \"Soft actor-critic: Off-policy maximum entropy deep reinforcement \nlearning with a stochastic actor.\" International conference on machine learning. PMLR, 2018. \n[12]. AG. Barto, S. Mahadevan. \"Recent advances in hierarchical reinforcement learning.\" Discrete event dynamic systems \n13.1-2 (2003): 41-77. https://doi.org/10.1023/A:1022140919877. \n[13]. D. Jain, A. Iscen and K. Caluwaerts, \"Hierarchical reinforcement learning for quadruped locomotion,\" 2019 \nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 7551-7557. \nhttps://doi.org/10.1109/IROS40897.2019.8967913. \n[14]. Y. Kim, B. Son, and D. Lee. \"Learning multiple gaits of quadruped robot using hierarchical reinforcement learning.\" \narXiv preprint arXiv:2112.04741 (2021). https://doi.org/10.48550/arXiv.2112.04741. \n[15]. Y. Shao, Y. Jin, X. Liu, W. He, H. Wang and W. Yang, \"Learning free gait transition for quadruped robots via phase-\nguided controller,\" in IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 1230-1237, April 2022. \nhttps://doi.org/10.1109/LRA.2021.3136645. \n[16]. W. Tan, X. Fang, W. Zhang, R. Song, T. Chen, Y. Zheng and Y. Li, \"A hierarchical framework for quadruped \nlocomotion based on reinforcement learning,\" 2021 IEEE/RSJ International Conference on Intelligent Robots and \nSystems (IROS), Prague, Czech Republic, 2021, pp. 8462-8468. https://doi.org/10.1109/IROS51168.2021.9636757. \n[17]. AN. Epstein. \"Instinct and motivation as explanations for complex behavior.\" The physiological mechanisms of \nmotivation (1982): 25-58. https://doi.org/10.1007/978-1-4612-5692-2_2. \n[18]. JJ. Bolhuis, LA. Giraldeau, and JA. Hogan, \"The study of animal behavior.\" The Behavior of Animals, 2nd Edition: \nMechanisms, Function and Evolution (2021): 1-11. https://doi.org/10.2307/1311704. \n[19]. W. Shi, S. Song, and C. Wu, \"Soft policy gradient method for maximum entropy deep reinforcement learning.\" arXiv \npreprint arXiv:1909.03198 (2019). https://doi.org/10.48550/arXiv.1909.03198. \n[20]. G. Bellegarda, Q. Nguyen, \"Robust quadruped jumping via deep reinforcement learning.\" arXiv preprint \narXiv:2011.07089 (2020). https://doi.org/10.48550/arXiv.2011.07089. \n[21]. T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel and S. Levine, \n\"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).  \nhttps://doi.org/10.48550/arXiv.1812.05905. \n[22]. S. Chernova and M. Veloso, \"An evolutionary approach to gait learning for four-legged robots,\" 2004 IEEE/RSJ \nInternational Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), Sendai, Japan, 2004, \npp. 2562-2567 vol.3, https://doi.org/10.1109/IROS.2004.1389794. \n[23]. D. Gong, J. Yan, and G. Zuo, \"A review of gait optimization based on evolutionary computation.\" Applied \nComputational Intelligence and Soft Computing 2010 (2010). https://doi.org/10.1155/2010/413179. \n[24]. C. Zhou, PK. Yue, J. Ni and SB. Chan, \"Dynamically stable gait planning for a humanoid robot to climb sloping \nsurface,\" IEEE Conference on Robotics, Automation and Mechatronics, 2004., Singapore, 2004, pp. 341-346 vol.1. \nhttps://doi.org/10.1109/RAMECH.2004.1438942. \n[25]. L. Hu, C. Zhou. \"EDA-Based optimization and learning methods for biped gait generation.\" Robotic Welding, \nIntelligence and Automation (2007): 541-549. https://doi.org/10.1007/978-3-540-73374-4_63. \n[26]. M. Oliveira, L. Costa, A. Rocha, C. Santos, and M. Ferreira, \"Multiobjective optimization of a quadruped robot \nlocomotion using a genetic algorithm.\" Soft Computing in Industrial Applications. Springer Berlin Heidelberg, 2011. \nhttps://doi.org/10.1007/978-3-642-20505-7_38. \n[27]. KG. Chae, JH. Park, \"Trajectory optimization with GA and control for quadruped robots.\" Journal of Mechanical \nScience and Technology 23 (2009): 114-123. https://doi.org/10.1007/s12206-008-0920-9. \n[28]. G. Bellegarda and A. Ijspeert, \"CPG-RL: learning central pattern generators for quadruped locomotion,\" in IEEE \nRobotics and Automation Letters, vol. 7, no. 4, pp. 12547-12554, Oct. 2022. \nhttps://doi.org/10.1109/LRA.2022.3218167. \n[29]. J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, \"Learning agile and dynamic \nmotor skills for legged robots.\" Science Robotics 4.26 (2019): eaau5872. https://doi/org/10.1126/scirobotics.aau5872. \n[30]. S. Khadka, K. Tumer, \"Evolutionary reinforcement learning.\" arXiv preprint arXiv:1805.07917 223 (2018). \n",
  "categories": [
    "cs.RO"
  ],
  "published": "2024-09-25",
  "updated": "2024-09-25"
}