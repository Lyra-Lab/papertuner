{
  "id": "http://arxiv.org/abs/2409.17386v1",
  "title": "Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning",
  "authors": [
    "Zhixiang Shen",
    "Shuo Wang",
    "Zhao Kang"
  ],
  "abstract": "Unsupervised Multiplex Graph Learning (UMGL) aims to learn node\nrepresentations on various edge types without manual labeling. However,\nexisting research overlooks a key factor: the reliability of the graph\nstructure. Real-world data often exhibit a complex nature and contain abundant\ntask-irrelevant noise, severely compromising UMGL's performance. Moreover,\nexisting methods primarily rely on contrastive learning to maximize mutual\ninformation across different graphs, limiting them to multiplex graph redundant\nscenarios and failing to capture view-unique task-relevant information. In this\npaper, we focus on a more realistic and challenging task: to unsupervisedly\nlearn a fused graph from multiple graphs that preserve sufficient task-relevant\ninformation while removing task-irrelevant noise. Specifically, our proposed\nInformation-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF) uses\ngraph structure refinement to eliminate irrelevant noise and simultaneously\nmaximizes view-shared and view-unique task-relevant information, thereby\ntackling the frontier of non-redundant multiplex graph. Theoretical analyses\nfurther guarantee the effectiveness of InfoMGF. Comprehensive experiments\nagainst various baselines on different downstream tasks demonstrate its\nsuperior performance and robustness. Surprisingly, our unsupervised method even\nbeats the sophisticated supervised approaches. The source code and datasets are\navailable at https://github.com/zxlearningdeep/InfoMGF.",
  "text": "Beyond Redundancy: Information-aware\nUnsupervised Multiplex Graph Structure Learning\nZhixiang Shenâˆ—,\nShuo Wangâˆ—,\nZhao Kangâ€ \nSchool of Computer Science and Engineering,\nUniversity of Electronic Science and Technology of China, Chengdu, Sichuan, China\nzhixiang.zxs@gmail.com zkang@uestc.edu.cn\nAbstract\nUnsupervised Multiplex Graph Learning (UMGL) aims to learn node representa-\ntions on various edge types without manual labeling. However, existing research\noverlooks a key factor: the reliability of the graph structure. Real-world data often\nexhibit a complex nature and contain abundant task-irrelevant noise, severely com-\npromising UMGLâ€™s performance. Moreover, existing methods primarily rely on\ncontrastive learning to maximize mutual information across different graphs, limit-\ning them to multiplex graph redundant scenarios and failing to capture view-unique\ntask-relevant information. In this paper, we focus on a more realistic and challeng-\ning task: to unsupervisedly learn a fused graph from multiple graphs that preserve\nsufficient task-relevant information while removing task-irrelevant noise. Specif-\nically, our proposed Information-aware Unsupervised Multiplex Graph Fusion\nframework (InfoMGF) uses graph structure refinement to eliminate irrelevant noise\nand simultaneously maximizes view-shared and view-unique task-relevant informa-\ntion, thereby tackling the frontier of non-redundant multiplex graph. Theoretical\nanalyses further guarantee the effectiveness of InfoMGF. Comprehensive exper-\niments against various baselines on different downstream tasks demonstrate its\nsuperior performance and robustness. Surprisingly, our unsupervised method even\nbeats the sophisticated supervised approaches. The source code and datasets are\navailable at https://github.com/zxlearningdeep/InfoMGF.\n1\nIntroduction\nMultiplex graph (multiple graph layers span across a common set of nodes), as a special type\nof heterogeneous graph, provides richer information and better modeling capabilities, leading to\nchallenges in learning graph representation [1]. Recently, unsupervised multiplex graph learning\n(UMGL) has attracted significant attention due to its exploitation of more detailed information from\ndiverse sources [2, 3], using graph neural networks (GNNs) [4] and self-supervised techniques [5].\nUMGL has become a powerful tool in numerous real-world applications [6, 7], e.g., social network\nmining and biological network analysis, where multiple relationship types exist or various interaction\ntypes occur.\nDespite the significant progress made by UMGL, a substantial gap in understanding how to take\nadvantage of the richness of the multiplex view is still left. In particular, a fundamental issue is\nlargely overlooked: the reliability of graph structure. Typically, the messaging-passing mechanism in\nGNNs assumes the reliability of the graph structure, implying that the connected nodes tend to have\nsimilar labels. All UMGL methods are graph-fixed, assuming that the original structure is sufficiently\nreliable for learning [3, 8â€“10]. Unfortunately, there has been evidence that practical graph structures\nâˆ—Equal contribution.\nâ€ Corresponding author.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2409.17386v1  [cs.LG]  25 Sep 2024\nShared information\nğ‘°(ğ‘®ğ’Š ; ğ‘®ğ’‹)\nTask-relevant\nunique information\nğ‘°ğ‘®ğ’Š ; ğ’€ ğ‘®ğ’‹)\nTask-relevant\nunique information\nğ‘°ğ‘®ğ’‹ ; ğ’€ ğ‘®ğ’Š)\nğ’€(Task)\nğ‘®ğ’Š\nğ‘®ğ’‹\n(a) Multiplex graph non-redundancy\nShared relevant edge\nUnique relevant edge\nIrrelevant edge\nGraph 1\n(b) Non-redundancy example\n(c) Empirical study on ACM\nFigure 1: (a) and (b) illustrate that in a non-redundant multiplex graph, view-specific task-relevant\nedges exist in certain graphs. The color of nodes represents class, edges between nodes of the same\nclass are considered relevant edges, and \"unique\" indicates that the edge exists only in one graph. (c)\nThe unique relevant edge ratio = (the number of unique relevant edges) / (the total number of relevant\nedges in this graph). Each graph contains a significant amount of unique task-relevant information.\nare not always reliable [11]. Multiplex graphs often contain substantial amounts of less informative\nedges characterized by irrelevant, misleading, and missing connections. For example, due to the\nheterophily in the graphs, GNNs generate poor performance [12â€“14]. Another representative example\nis adversarial attacks [15], where attackers tend to add edges between nodes of different classes.\nThen, aggregating information from neighbors of different classes degrades UMGL performance.\nDiverging from existing approaches to node representation learning, we focus on structure learning of\na new graph from multiplex graphs to better suit downstream tasks. Notably, existing Graph Structure\nLearning (GSL) overwhelmingly concentrated on a single homogeneous graph [16], marking our\nendeavor as pioneering in the realm of multiplex graphs.\nGiven the unsupervised nature, the majority of UMGL methods leverage contrastive learning mecha-\nnism [8â€“10], a typical self-supervised technique, for effective training. However, recent research has\ndemonstrated that standard contrastive learning, maximizing mutual information between different\nviews, is limited to capturing view-shared task-relevant information [17]. This approach is effective\nonly in multi-view redundant scenarios, thereby overlooking unique task-relevant information spe-\ncific to each view. In practice, the multiplex graph is inherently non-redundant. As illustrated in\nFigure 1, task-relevant information resides not only in shared areas across different graph views but\nalso in specific view-unique regions. For instance, in the real citation network ACM [18], certain\npapers on the same subject authored by different researchers may share categories and thematic\nrelevance. This characteristic, compared to the co-author view, represents view-unique task-relevant\ninformation within the co-subject view. It exposes a critical limitation in existing UMGL methods,\nwhich potentially cannot capture sufficient task-relevant information.\nMotivated by the above observations, our research goal can be summarized as follows: how can we\nlearn a fused graph from the original multiplex graph in an unsupervised manner, mitigating\ntask-irrelevant noise while retaining sufficient task-relevant information? To handle this new task,\nwe propose a novel Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF).\nGraph structure refinement is first applied to each view to achieve a more suitable graph with\nless task-irrelevant noise. Confronting multiplex graph non-redundancy, InfoMGF simultaneously\nmaximizes the view-shared and view-unique task-relevant information to realize sufficient graph\nlearning. A learnable graph augmentation generator is also developed. Finally, InfoMGF maximizes\nthe mutual information between the fused graph and each refined graph to encapsulate clean and\nholistic task-relevant information from a range of various interaction types. Theoretical analyses\nguarantee the effectiveness of our approach in capturing task-relevant information and graph fusion.\nThe unsupervised learned graph and node representations can be applied to various downstream tasks.\nIn summary, our main contributions are three-fold:\nâ€¢ Problem. We pioneer the investigation of the multiplex graph reliability problem in a\nprincipled way, which is a more practical and challenging task. To our best knowledge, we\nare the first to attempt unsupervised graph structure learning in multiplex graphs.\n2\nâ€¦â€¦\nMultiplex Graph\nOptimal \nGraph Aug.\nGraph\nLearner\nğº1\nğºğ‘‰\nShared\nGraph\nEncoder\nFused Graph\nUnique Task-\nrelevant Information\nShared Task-\nrelevant Information\nGraph Fusion\nMaximize Mutual \nInformation\nGraph Structure Refinement\nTask-relevant Information Maximizing\nğº1\nğ‘ \nğº1\nâ€²\nğºğ‘ \nğºğ‘‰\nğ‘ \nğºğ‘‰\nâ€²\nâ€¦\nâ€¦\nOptimal \nGraph Aug.\nUpdate \nParameters\nâ€¦\nâ€¦\nRefined Graph\nRefined Graph\nDownstream\nTasks\nNode Classification\nNode Clustering\nâ€¦â€¦\nFigure 2: The overall framework of the proposed InfoMGF. Specifically, InfoMGF first generates\nrefined graphs and the fused graph through the graph learner. Subsequently, it maximizes shared and\nunique task-relevant information within the multiplex graph and facilitates graph fusion. The learned\nfused graph and node representations are used for various downstream tasks.\nâ€¢ Algorithm. We propose InfoMGF, a versatile multiplex graph fusion framework that steers\nthe fused graph learning by concurrently maximizing both view-shared and view-unique\ntask-relevant information under the multiple graphs non-redundancy principle. Furthermore,\nwe develop two random and generative graph augmentation strategies to capture view-unique\ntask information. Theoretical analyses ensure the effectiveness of InfoMGF.\nâ€¢ Evaluation. We perform extensive experiments against various types of state-of-the-art\nmethods on different downstream tasks to comprehensively evaluate the effectiveness and\nrobustness of InfoMGF. Particularly, our developed unsupervised approach even outperforms\nsupervised methods.\n2\nPreliminaries\nNotation. The multiplex graph is represented by G = {G1, ..., GV }, where Gv = {Av, X} is the\nv-th graph. Av âˆˆ{0, 1}NÃ—N is the corresponding adjacency matrix and X âˆˆRNÃ—df is the shared\nfeature matrix across all graphs. Xi âˆˆRdf is the i-th row of X, representing the feature vector of\nnode i. N is the number of nodes and Dv is a diagonal matrix denoting the degree matrix of Av. Y\nis label information. For convenience, we use â€œviewâ€ to refer to each graph in the multiplex graph.\nMultiplex graph non-redundancy. Task-relevant information exists not only in the shared in-\nformation between graphs but also potentially within the unique information of certain graphs.\nFollowing the non-redundancy principle [17], we provide the formal definition of Multiplex Graph\nNon-redundancy:\nDefinition 1. Gi is considered non-redundant with Gj for Y if and only if there exists Ïµ > 0 such\nthat the conditional mutual information I(Gi; Y | Gj) > Ïµ or I(Gj; Y | Gi) > Ïµ.\nGraph structure learning. Existing GSL methods primarily focus on a single graph. Their pipeline\ncan be summarized as a two-stage framework [16]: a Graph Learner takes in the original graph\nG = {A, X} to generate a refined graph Gs = {As, X} with a new structure; a Graph Encoder uses\nthe refined graph as input to obtain node representations. Note that node features generally do not\nchange in GSL, only the graph structure is optimized. Related work is in Appendix B.\n3\nMethodology\nAs illustrated in Figure 2, our proposed InfoMGF consists of two modules: the Graph Structure\nRefinement module and the Task-Relevant Information Maximization module.\n3\n3.1\nGraph Structure Refinement\nWe first use a graph learner to generate each viewâ€™s refined graph Gs\nv = {As\nv, X}. To retain\nnode features and structure information simultaneously, we apply the widely used Simple Graph\nConvolution (SGC) [19] to perform aggregation in each view, resulting in view-specific node features\nXv. A view-specific two-layer attentive network is employed to model the varying contributions of\ndifferent features to structure learning:\nXv = ( ËœD\nâˆ’1\n2\nv\nËœAv ËœD\nâˆ’1\n2\nv\n)rX,\nHv = Ïƒ(Xv âŠ™W v\n1 ) âŠ™W v\n2\n(1)\nwhere ËœDv = Dv + I and ËœAv = Av + I. r represents the order of graph aggregation. Ïƒ(Â·) is the\nnon-linear activation function and âŠ™denotes the Hadamard product. All rows of W v\n1 are identical,\nrepresenting a learnable attention vector shared by all nodes. This strategy enables us to acquire\nview-specific features before training, thereby circumventing the time-consuming graph convolution\noperations typically required by GNN-based graph learners during training, which significantly boosts\nour modelâ€™s scalability.\nLike existing GSL methods [16, 20], we apply post-processing techniques to ensure that the adjacency\nmatrix As\nv satisfies properties such as sparsity, non-negativity, symmetry, and normalization. Specif-\nically, we use Hv to construct the similarity matrix and then sparsify it using k-nearest neighbors\n(kNN). For large-scale graphs, we utilize locality-sensitive approximation during kNN sparsification\nto reduce time complexity [21]. Afterward, operations including Symmetrization, Activation, and\nNormalization are used sequentially to generate the final As\nv. Following the refinement of each view,\nwe employ a shared Graph Convolutional Network (GCN) [22] as the graph encoder to obtain the\nnode representations Zv âˆˆRNÃ—d of each view, computed by Zv = GCN(As\nv, X).\n3.2\nMaximizing Shared Task-Relevant Information\nGs\nv should contain not only view-shared but also view-unique task-relevant information. Following\nstandard contrastive learning [23, 24], for each pair of distinct views (e.g., i and j), our approach seeks\nto maximize the mutual information 0.5I(Gs\ni; Gj) + 0.5I(Gs\nj; Gi) to capture shared task-relevant\ninformation between views. Essentially, the maximization objective can be transformed to a tractable\nlower bound I(Gs\ni; Gs\nj) [25, 26]. Considering the addition of mutual information for each pair, the\nloss term for minimization can be expressed as follows:\nLs = âˆ’\n2\nV (V âˆ’1)\nV\nX\ni=1\nV\nX\nj=i+1\nI(Gs\ni; Gs\nj)\n(2)\n3.3\nMaximizing Unique Task-Relevant Information\nMaximizing view-unique task-relevant information can be rigorously expressed as maximizing\nI(Gs\ni; Y | âˆªjÌ¸=i Gj). Then, we relax the optimization objective to the total task-relevant information\nwithin the view, I(Gs\ni; Y ). This decision is based on the following considerations: on the one hand,\ndeliberately excluding shared task-relevant information is unnecessary and would complicate the\noptimization process. On the other hand, repeated emphasis on shared task-relevant information\nencourages the model to focus more on it in the early training stage.\nThe unsupervised nature of our task dictates that we cannot directly optimize I(Gs\ni; Y ) using\nlabel information. Some typical graph learning methods often reconstruct the graph structure\nto preserve the maximum amount of information from the original data [27â€“29]. In the context of\nour task, this reconstruction-based optimization objective is equivalent to maximizing the mutual\ninformation with the original graph structure [30, 31], i.e., I(Gs\ni; Gi). However, such methods have\nsignificant drawbacks: they retain task-irrelevant information from the original data, and the graph\nreconstruction also entails high complexity. In contrast, we leverage graph augmentation to reduce\ntask-irrelevant information and retain task-relevant information without accessing Y . Following the\noptimal augmentation assumption [17, 32], we define optimal graph augmentation as:\nDefinition 2. Gâ€²\ni is an optimal augmented graph of Gi if and only if I(Gâ€²\ni; Gi) = I(Y ; Gi), implying\nthat the only information shared between Gi and Gâ€²\ni is task-relevant without task-irrelevant noise.\nTheorem 1. If Gâ€²\ni is the optimal augmented graph of Gi, then I(Gs\ni; Gâ€²\ni) = I(Gs\ni; Y ) holds.\n4\nTheorem 2. The maximization of I(Gs\ni; Gâ€²\ni) yields a discernible reduction in the task-irrelevant\ninformation relative to the maximization of I(Gs\ni; Gi).\nTheorem 1 theoretically guarantees that maximizing I(Gs\ni; Gâ€²\ni) would provide clean and sufficient\ntask-relevant guidance for learning Gs\ni. Theorem 2 demonstrates the superiority of our optimization\nobjective over typical methods in removing task-irrelevant information. Therefore, given Gâ€²\ni =\n{Aâ€²\ni, Xâ€²} for each view, where Aâ€²\ni and Xâ€² denote the augmented adjacency matrix and node features,\nrespectively, the loss term Lu is defined as:\nLu = âˆ’1\nV\nV\nX\ni=1\nI(Gs\ni; Gâ€²\ni)\n(3)\nThe key to the above objective lies in ensuring that Gâ€²\ni satisfies the optimal graph augmentation.\nHowever, given the absence of label information, achieving truly optimal augmentation is not feasible;\ninstead, we can only rely on heuristic techniques to simulate it. Consistent with most existing graph\naugmentations, we believe that task-relevant information in graph data exists in both structure and\nfeature, necessitating augmentation in both aspects. We use random masking, a simple yet effective\nmethod, to perform feature augmentation. For graph structure, we propose two versions: random\nedge dropping and learnable augmentation through a graph generator.\nRandom feature masking. For node features, we randomly select a fraction of feature dimensions\nand mask them with zeros. Formally, we sample a random vector âƒ—m âˆˆ{0, 1}df where each dimension\nis drawn from a Bernoulli distribution independently, i.e., âƒ—mi âˆ¼Bern(1 âˆ’Ï). Then, the augmented\nnode features Xâ€² is computed by Xâ€² = [X1 âŠ™âƒ—m; X2 âŠ™âƒ—m; ...; XN âŠ™âƒ—m]âŠ¤.\nRandom edge dropping (InfoMGF-RA). For a given Av, a masking matrix M âˆˆ{0, 1}NÃ—N is\nrandomly generated, where each element Mij is sampled from a Bernoulli distribution. Afterward,\nthe augmented adjacency matrix can be computed as Aâ€²\nv = Av âŠ™M.\nLearnable generative augmentation (InfoMGF-LA). Random edge dropping may lack reliability\nand interpretability. A low dropping probability might not suffice to eliminate task-irrelevant infor-\nmation, while excessive deletions could compromise task-relevant information. Therefore, we opt to\nuse a learnable graph augmentation generator. To avoid interference from inappropriate structure\ninformation, we compute personalized sampling probabilities for existing edges in each view by\nemploying a Multilayer Perceptron (MLP) in the node features. To ensure the differentiability of\nthe sampling operation for end-to-end training, we introduce the Gumbel-Max reparametrization\ntrick [33, 34] to transform the discrete binary (0-1) distribution of edge weights into a continuous\ndistribution. Specifically, for each edge ei,j in view v, its edge weight Ï‰v\ni,j in the corresponding\naugmented view is computed as follows:\nÎ¸v\ni,j = MLP ([WXi; WXj]) ,\nÏ‰v\ni,j = Sigmoid\n\u0000(log Î´ âˆ’log(1 âˆ’Î´) + Î¸v\ni,j)/Ï„\n\u0001\n(4)\nwhere [Â·; Â·] denotes the concatenation operation and Î´ âˆ¼Uniform(0, 1) is the sampled Gumbel\nrandom variate. We can control the temperature hyper-parameter Ï„ approaching 0 to make Ï‰v\ni,j\ntend towards a binary distribution. For an effective augmented graph generator, it should eliminate\ntask-irrelevant noise while retaining task-relevant information. Therefore, we design a suitable loss\nfunction for augmented graph training:\nLgen =\n1\nNV\nV\nX\ni=1\nN\nX\nj=1\n \n1 âˆ’\n(Xi\nj)âŠ¤Ë†Xi\nj\nâˆ¥Xi\njâˆ¥Â· âˆ¥Ë†Xi\njâˆ¥\n!\n+ Î» âˆ—1\nV\nV\nX\ni=1\nI(Gs\ni; Gâ€²\ni)\n(5)\nwhere Î» is a positive hyper-parameter. The first term reconstructs view-specific features using\nthe cosine error, guaranteeing that the augmented views preserve crucial task-relevant information\nwhile having lower complexity compared to reconstructing the entire graph structure. The recon-\nstructed features Ë†Xi are obtained using an MLP-based Decoder on the node representations Ziâ€²\nof the augmented view. The second term minimizes I(Gs\ni; Gâ€²\ni) to regularize the augmented views\nsimultaneously, ensuring that the augmented graphs would provide only task-relevant information as\nguidance with less task-irrelevant noise when optimizing the refined graph Gs\ni through Eq.(3). Note\nthat for InfoMGF-LA, we adopt an iterative optimization strategy to update Gs\ni and Gâ€²\ni alternatively,\nas described in Section 3.4.\n5\nAlthough previous work also employs similar generative graph augmentation [35], we still possess\nirreplaceable advantages in comparison. Firstly, they merely minimize mutual information to generate\nthe augmented graph, lacking the crucial information retention component, which may jeopardize\ntask-relevant information. Furthermore, an upper bound should ideally be used for minimization,\nwhereas they utilize a lower bound estimator for computation, which is incorrect in optimization\npractice. In contrast, we use a rigorous upper bound of mutual information for the second term of\nLgen, which is demonstrated later.\n3.4\nMultiplex Graph Fusion\nThe refined graph retains task-relevant information from each view while eliminating task-irrelevant\nnoise. Afterward, we learn a fused graph that encapsulates sufficient task-relevant information from\nall views. Consistent with the approach in Section 3.1, we leverage a scalable attention mechanism as\nthe fused graph learner:\nH = Ïƒ([X; X1; X2; Â· Â· Â· ; XV ] âŠ™W 1) âŠ™W 2,\nLf = âˆ’1\nV\nV\nX\ni=1\nI(Gs; Gs\ni)\n(6)\nwhere the node features are concatenated with all view-specific features as input. The same post-\nprocessing techniques are sequentially applied to generate the fused graph Gs = {As, X}. The\nnode representations Z of the fused graph are also obtained through the same GCN. We maximize\nthe mutual information between the fused graph and each refined graph to incorporate task-relevant\ninformation from all views, denoted as loss Lf. The total loss L of our model can be expressed as the\nsum of three terms: L = Ls + Lu + Lf.\nTheorem 3. The learned fused graph Gs contains more task-relevant information than the refined\ngraph Gs\ni from any single view. Formally, we have:\nI(Gs; Y ) â‰¥max\ni\nI(Gs\ni; Y )\n(7)\nTheorem 3 theoretically proves that the fused graph Gs can incorporate more task-relevant information\nthan considering each view individually, thus ensuring the effectiveness of multiplex graph fusion.\nOptimization. Note that all the loss terms require calculating mutual information. However, directly\ncomputing mutual information between two graphs is impractical due to the complexity of graph-\nstructured data. Since we focus on node-level tasks, we assume the optimized graph should guarantee\nthat each nodeâ€™s neighborhood substructure contains sufficient task-relevant information. Therefore,\nthis requirement can be transferred into mutual information between node representations [36], which\ncan be easily computed using a sample-based differentiable lower/upper bound. For any view i and j,\nthe lower bound Ilb and upper bound Iub of the mutual information I(Zi; Zj) are [17]:\nIlb(Zi; Zj) = Ezi,zj+âˆ¼p(zi,zj)\nzjâˆ¼p(zj)\n\u0014\nlog\nexpf(zi, zj+)\nP\nN expf(zi, zj)\n\u0015\n(8)\nIub(Zi; Zj) = Ezi,zj+âˆ¼p(zi,zj)\n\u0002\nf âˆ—(zi, zj+)\n\u0003\nâˆ’Eziâˆ¼p(zi)\nzjâˆ¼p(zj)\n\u0002\nf âˆ—(zi, zj)\n\u0003\n(9)\nwhere f(Â·, Â·) is a score critic approximated by a neural network and f âˆ—(Â·, Â·) is the optimal critic from\nIlb plugged into the Iub objective. p(zi, zj) denotes the joint distribution of node representations\nfrom views i and j, while p(zi) denotes the marginal distribution. zi and zj+ are mutually positive\nsamples, representing the representations of the same node in views i and j respectively.\nTo avoid too many extra parameters, the function f(zi, zj) is implemented using non-linear projection\nand cosine similarity. Each term in the total loss L maximizes mutual information, so we use the\nlower bound estimator for the calculation. In contrast, we use the upper bound estimator for the\ngenerator loss Lgen in InfoMGF-LA, which minimizes mutual information. These two losses can be\nexpressed as follows:\nL = âˆ’\n2\nV (V âˆ’1)\nV\nX\ni=1\nV\nX\nj=i+1\nIlb(Zi; Zj) âˆ’1\nV\nV\nX\ni=1\nIlb(Zi; Ziâ€²) âˆ’1\nV\nV\nX\ni=1\nIlb(Z; Zi)\n(10)\n6\nLgen =\n1\nNV\nV\nX\ni=1\nN\nX\nj=1\n \n1 âˆ’\n(Xi\nj)âŠ¤Ë†Xi\nj\nâˆ¥Xi\njâˆ¥Â· âˆ¥Ë†Xi\njâˆ¥\n!\n+ Î» âˆ—1\nV\nV\nX\ni=1\nIub(Zi; Ziâ€²)\n(11)\nFinally, we provide the InfoMGF-LA algorithm in Appendix C.1. In Step 1 of each epoch, we keep\nthe augmented graph fixed and optimize both the refined graphs and the fused graph using the total\nloss L, updating the parameters of Graph Learners and GCN. In Step 2, we keep the refined graphs\nfixed and optimize each augmented graph using Lgen, updating the parameters of the Augmented\nGraph Generator and Decoder. After training, Gs and Z are used for downstream tasks.\n4\nExperiments\nIn this section, our aim is to answer three research questions: RQ1: How effective is InfoMGF for\ndifferent downstream tasks in unsupervised settings? RQ2: Does InfoMGF outperform baselines\nof various types under different adversarial attacks? RQ3: How do the main modules influence the\nperformance of InfoMGF?\n4.1\nExperimental Setups\nDownstream tasks. We evaluate the learned graph on node clustering and node classification tasks.\nFor node clustering, following [8], we apply the K-means algorithm on the node representations Z of\nGs and use the following four metrics: Accuracy (ACC), Normalized Mutual Information (NMI), F1\nScore (F1), and Adjusted Rand Index (ARI). For node classification, following the graph structure\nlearning settings in [16], we train a new GCN on Gs for evaluation and use the following two metrics:\nMacro-F1 and Micro-F1.\nDatasets. We conduct experiments on four real-world benchmark multiplex graph datasets, which\nconsist of two citation networks (i.e., ACM [18] and DBLP [18]), one review network Yelp [37] and\na large-scale citation network MAG [38]. Details of datasets are shown in Appendix E.1.\nBaselines. For node clustering, we compare InfoMGF with two single-graph methods (i.e., VGAE\n[27] and DGI [39]) and seven multiplex graph methods (i.e., O2MAC [28], MvAGC [40], MCGC\n[41], HDMI [8], MGDCR [9], DMG [3], and BTGF [10]). All the baselines are unsupervised\nclustering methods. For a fair comparison, we conduct single-graph methods separately for each\ngraph and present the best results.\nFor node classification, we compare InfoMGF with baselines of various types: three supervised\nstructure-fixed GNNs (i.e., GCN [22], GAT [42] and HAN [43]), six supervised GSL methods\n(i.e., LDS [44], GRCN [45], IDGL [46], ProGNN [11], GEN [47] and NodeFormer [48]), three\nunsupervised GSL methods (i.e., SUBLIME [20], STABLE [49] and GSR [50]), and three structure-\nfixed UMGL methods (i.e., HDMI [8], DMG [3] and BTGF [10]). GCN, GAT, and all GSL methods\nare single-graph approaches. For unsupervised GSL methods, following [20], we train a new GCN\non the learned graph for node classification. For UMGL methods, following [8], we train a linear\nclassifier on the learned representations. Implementation details can be found in Appendix E.2.\n4.2\nEffectiveness Analysis (RQ1)\nTable 1 presents the results of node clustering. Firstly, multiplex graph clustering methods outperform\nsingle graph methods overall, demonstrating the advantages of leveraging information from multiple\nsources. Secondly, compared to other multiplex graph methods, both versions of our approach surpass\nexisting state-of-the-art methods. This underscores the efficacy of our proposed graph structure\nlearning, which eliminates task-irrelevant noise and extracts task-relevant information from all graphs,\nto serve downstream tasks better. Finally, InfoMGF-LA achieves notably superior results, owing to\nthe exceptional capability of the learnable generative graph augmentation in capturing view-unique\ntask-relevant information.\nTable 2 reports the node classification results. Overall, GSL methods outperform structure-fixed\nmethods, demonstrating the unreliability of the original structure in real-world data and the signifi-\ncance of graph structure learning. Particularly for various carefully designed UMGL methods, the\noriginal graphs with rich task-irrelevant noise severely limit their performance. Compared to existing\nsingle-graph GSL methods, both versions of InfoMGF outperform the supervised methods. By\n7\nTable 1: Quantitative results (%) on node clustering. The top 3 highest results are highlighted with\nred boldface, red color and boldface, respectively. The symbol â€œOOMâ€ means out of memory.\nMethod\nACM\nDBLP\nYelp\nMAG\nNMI\nARI\nACC\nF1\nNMI\nARI\nACC\nF1\nNMI\nARI\nACC\nF1\nNMI\nARI\nACC\nF1\nVGAE\n45.83\n41.36\n67.93\n68.62\n61.79\n65.56\n84.48\n83.67\n39.19\n42.57\n65.07\n56.74\nOOM\nDGI\n52.94\n47.55\n65.36\n57.34\n65.59\n70.35\n86.88\n86.02\n39.42\n42.62\n65.29\n56.79\n53.56\n42.6\n59.89\n57.17\nO2MAC\n42.36\n46.04\n77.92\n78.01\n58.64\n60.01\n83.29\n82.88\n39.02\n42.53\n65.07\n56.74\nOOM\nMvAGC\n64.49\n66.81\n87.17\n87.21\n50.39\n51.21\n78.39\n77.84\n24.39\n29.25\n63.14\n56.7\nOOM\nMCGC\n60.21\n50.72\n65.62\n54.78\n65.56\n71.51\n87.96\n87.47\n38.35\n35.17\n65.61\n57.49\nOOM\nHDMI\n65.44\n68.87\n88.11\n88.14\n64.85\n70.85\n87.39\n86.75\n60.81\n59.35\n79.56\n77.6\n48.15\n34.92\n51.78\n49.8\nMGDCR\n58.8\n55.15\n73.82\n70.34\n62.47\n62.22\n81.91\n80.16\n44.23\n46.47\n72.71\n54.43\n54.43\n43.98\n61.37\n60.53\nDMG\n64.14\n67.21\n87.11\n87.23\n69.03\n73.07\n88.45\n87.88\n65.66\n66.33\n88.26\n89.27\n48.72\n39.77\n61.61\n60.16\nBTGF\n68.92\n73.14\n90.09\n90.11\n66.28\n72.47\n88.05\n87.28\n69.97\n73.53\n91.39\n92.32\nOOM\nInfoMGF-RA\n74.89\n81.09\n92.82\n92.89\n70.19\n73.49\n88.72\n88.31\n72.67\n74.66\n91.85\n92.86\n56.65\n45.25\n64.13\n63.09\nInfoMGF-LA\n76.53\n81.49\n93.45\n93.42\n73.22\n78.49\n91.08\n90.69\n75.18\n78.91\n93.26\n94.01\nOOM\nTable 2: Quantitative results with standard deviation (% Â± Ïƒ) on node classification. Available data\nfor GSL during training is shown in the first column, supervised methods depend on Y for GSL. The\nsymbol â€œ-â€ indicates that the method is structure-fixed, which does not learn a new structure.\nAvailable\nMethods\nACM\nDBLP\nYelp\nMAG\nData for GSL\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\n-\nGCN\n90.27Â±0.59\n90.18Â±0.61\n90.01Â±0.32\n90.99Â±0.28\n78.01Â±1.89\n81.03Â±1.81\n75.98Â±0.07\n75.76Â±0.10\n-\nGAT\n91.52Â±0.62\n91.46Â±0.62\n90.22Â±0.37\n91.13Â±0.40\n82.12Â±1.47\n84.43Â±1.56\nOOM\n-\nHAN\n91.67Â±0.39\n91.47Â±0.22\n90.53Â±0.24\n91.47Â±0.22\n88.49Â±1.73\n88.78Â±1.40\nOOM\nX,Y,A\nLDS\n92.35Â±0.43\n92.05Â±0.26\n88.11Â±0.86\n88.74Â±0.85\n75.98Â±2.35\n78.14Â±1.98\nOOM\nX,Y,A\nGRCN\n93.04Â±0.17\n92.94Â±0.18\n88.33Â±0.47\n89.43Â±0.44\n76.05Â±1.05\n80.68Â±0.96\nOOM\nX,Y,A\nIDGL\n91.69Â±1.24\n91.63Â±1.24\n89.65Â±0.60\n90.61Â±0.56\n76.98Â±5.78\n79.15Â±5.06\nOOM\nX,Y,A\nProGNN\n90.57Â±1.03\n90.50Â±1.29\n83.13Â±1.56\n84.83Â±1.36\n51.76Â±1.46\n58.39Â±1.25\nOOM\nX,Y,A\nGEN\n87.91Â±2.78\n87.88Â±2.61\n89.74Â±0.69\n90.65Â±0.71\n80.43Â±3.78\n82.68Â±2.84\nOOM\nX,Y,A\nNodeFormer\n91.33Â±0.77\n90.60Â±0.95\n79.54Â±0.78\n80.56Â±0.62\n91.69Â±0.65\n90.59Â±1.21\n77.21Â±0.18\n77.08Â±0.19\nX,A\nSUBLIME\n92.42Â±0.16\n92.13Â±0.37\n90.98Â±0.37\n91.82Â±0.27\n79.68Â±0.79\n82.99Â±0.82\n75.96Â±0.05\n75.71Â±0.03\nX,A\nSTABLE\n83.54Â±4.20\n83.38Â±4.51\n75.18Â±1.95\n76.42Â±1.95\n71.48Â±4.71\n76.62Â±2.75\nOOM\nX,A\nGSR\n92.14Â±1.08\n92.11Â±0.99\n76.59Â±0.45\n77.69Â±0.42\n83.85Â±0.76\n85.73Â±0.54\nOOM\n-\nHDMI\n91.01Â±0.32\n90.86Â±0.31\n89.91Â±0.49\n90.89Â±0.51\n80.73Â±0.64\n84.05Â±0.91\n72.22Â±0.14\n71.84Â±0.15\n-\nDMG\n90.42Â±0.36\n90.31Â±0.35\n90.42Â±0.57\n91.34Â±0.49\n91.61Â±0.62\n90.24Â±0.81\n76.34Â±0.09\n76.13Â±0.10\n-\nBTGF\n91.75Â±0.11\n91.62Â±0.11\n90.71Â±0.24\n91.57Â±0.21\n92.81Â±1.12\n91.37Â±1.28\nOOM\nX,A\nInfoMGF-RA\n93.21Â±0.22\n93.14Â±0.21\n90.99Â±0.36\n91.93Â±0.29\n93.09Â±0.27\n92.02Â±0.34\n77.25Â±0.06\n77.11Â±0.06\nX,A\nInfoMGF-LA\n93.42Â±0.21\n93.35Â±0.21\n91.28Â±0.31\n92.12Â±0.28\n93.26Â±0.26\n92.24Â±0.34\nOOM\ncapturing shared and unique information from multiplex graphs, InfoMGF can integrate more com-\nprehensive task-relevant information. Finally, we can observe that the proposed InfoMGF-LA with\nlearnable augmentation indeed surpasses the random augmentation version, once again highlighting\nits advantage in exploring task-relevant information.\nWe select a subgraph from the ACM dataset with nodes in two classes (database (C1) and data mining\n(C2)) and visualize the edge weights in the original multiplex graphs and the fused graph learned\nby InfoMGF-LA. From Figure 3, the learned graph mainly consists of intra-class edges. Compared\nto the nearly fully connected PSP view, InfoMGF significantly reduces inter-class edges, reflecting\nour effective removal of task-irrelevant noise. Compared to the PAP view, InfoMGF introduces\nmore intra-class edges, benefiting from capturing shared and unique task-relevant information from\nall graphs. Furthermore, varying edge weights in Gs represent different importance levels, better\nserving downstream tasks. In summary, the above experiment results across various downstream\n(a) PAP\n(b) PSP\n(c) Gs\nFigure 3: Heatmaps of the subgraph adjacency matrices of the original and learned graphs on ACM.\n8\ntasks demonstrate the effectiveness of InfoMGF. We use the InfoMGF-LA version in the subsequent\nsections to conduct more comprehensive analyses.\n4.3\nRobustness Analysis (RQ2)\n(a) Adding edges\n(b) Deleting edges\nFigure 4: Robustness analysis on ACM.\nTo evaluate the robustness of InfoMGF against\nstructure noise, we perturb each graph on the\nACM dataset by randomly adding or remov-\ning edges. We compare InfoMGF against var-\nious baselines: structure-fixed method (GCN),\nGSL method (SUBLIME), and UMGL method\n(HDMI). From Figure 4, it is evident that with\nincreasing rates of edge perturbing, the perfor-\nmance of each method deteriorates, while the\nGSL methods (i.e., InfoMGF and SUBLIME) ex-\nhibit better robustness. Notably, our proposed\nInfoMGF consistently outperforms all other\nmethods across both experimental settings, especially when the perturbation rate is extremely high.\n4.4\nAblation Study (RQ3)\nTable 3: Performance (% Â± Ïƒ) of InfoMGF and its variants.\nVariants\nACM\nDBLP\nYelp\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\nMacro-F1\nMicro-F1\nw/o Ls\n93.05Â±0.49\n92.98Â±0.49\n90.44Â±0.45\n91.39Â±0.41\n93.15Â±0.12\n92.11Â±0.13\nw/o Lu\n92.66Â±0.53\n92.61Â±0.51\n90.13Â±0.43\n91.05Â±0.44\n92.23Â±0.27\n90.96Â±0.36\nw/o Aug.\n92.84Â±0.17\n92.81Â±0.16\n90.94Â±0.45\n91.81Â±0.41\n92.76Â±0.49\n91.63Â±0.51\nw/o Rec.\n92.91Â±0.53\n92.88Â±0.51\n91.05Â±0.27\n91.87Â±0.23\n92.65Â±0.27\n91.45Â±0.37\nInfoMGF\n93.42Â±0.21\n93.35Â±0.21\n91.28Â±0.31\n92.12Â±0.28\n93.26Â±0.26\n92.24Â±0.34\nTo verify the effectiveness of each part of InfoMGF, we design four variants and compare the\nclassification performance against InfoMGF.\nEffectiveness of loss components. Recall InfoMGF maximizes view-shared and unique task-relevant\ninformation by Ls and Lu. Thus, we design two variants (w/o Ls and w/o Lu). Table 3 shows the\nnecessity of each component. Furthermore, we can observe that removing Lu has a greater impact\ncompared to Ls, which can be explained by the fact that optimization of Lu actually maximizes the\noverall task-relevant information of each view, rather than solely view-unique aspects.\nEffectiveness of augmentation module. The InfoMGF-LA framework incorporates learnable gen-\nerative augmentation and maximizes the mutual information I(Gs\ni; Gâ€²\ni) to mine the task-relevant\ninformation. We first compare InfoMGF with maximizing the mutual information I(Gs\ni; Gi) with the\noriginal graph structure without augmentation (w/o Aug.). Furthermore, we remove the reconstruction\nloss term (w/o Rec.) of Lgen to analyze the necessity of crucial information preserving. The results\nshow that maximizing I(Gs\ni; Gi) leads to poorer performance compared to I(Gs\ni; Gâ€²\ni), consistent with\nTheorem 2. Meanwhile, deleting the reconstruction term from Lgen also results in the augmented\ngraph lacking task-relevant information, thus hurting model performance.\n5\nConclusion and Limitation\nThis paper delves into the unsupervised graph structure learning within multiplex graphs for the\nfirst time. The proposed InfoMGF refines the graph structure to eliminate task-irrelevant noise,\nwhile simultaneously maximizing both the shared and unique task-relevant information across\ndifferent graphs. The fused graph applied to downstream tasks is optimized to incorporate clean\nand comprehensive task-relevant information from all graphs. Theoretical analyses and extensive\nexperiments ensure the effectiveness of InfoMGF. A limitation of our research lies in its focus\nsolely on the pure unsupervised scenario. In some real-world scenarios where partial node labels\nare available, label information can be used to learn a better structure of multiplex graphs. Such\nsupervised or semi-supervised problems are left for future exploration.\n9\n6\nAcknowledgments and Disclosure of Funding\nThis work was supported by the National Natural Science Foundation of China (No. 62276053).\n10\nReferences\n[1] Zhixiang Shen, Haolan He, and Zhao Kang. Balanced multi-relational graph clustering. In\nACM Multimedia 2024.\n[2] Chanyoung Park, Donghyun Kim, Jiawei Han, and Hwanjo Yu. Unsupervised attributed\nmultiplex network embedding. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pages 5371â€“5378, 2020.\n[3] Yujie Mo, Yajie Lei, Jialie Shen, Xiaoshuang Shi, Heng Tao Shen, and Xiaofeng Zhu. Dis-\nentangled multiplex graph representation learning. In International Conference on Machine\nLearning, pages 24983â€“25005. PMLR, 2023.\n[4] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\ncomprehensive survey on graph neural networks. IEEE transactions on neural networks and\nlearning systems, 32(1):4â€“24, 2020.\n[5] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang.\nSelf-supervised learning: Generative or contrastive. IEEE transactions on knowledge and data\nengineering, 35(1):857â€“876, 2021.\n[6] Weifeng Zhang, Jingwen Mao, Yi Cao, and Congfu Xu. Multiplex graph neural networks for\nmulti-behavior recommendation. In Proceedings of the 29th ACM international conference on\ninformation & knowledge management, pages 2313â€“2316, 2020.\n[7] Xunqiang Jiang, Tianrui Jia, Yuan Fang, Chuan Shi, Zhe Lin, and Hui Wang. Pre-training on\nlarge-scale heterogeneous graph. In Proceedings of the 27th ACM SIGKDD conference on\nknowledge discovery & data mining, pages 756â€“766, 2021.\n[8] Baoyu Jing, Chanyoung Park, and Hanghang Tong. Hdmi: High-order deep multiplex infomax.\nIn Proceedings of the Web Conference 2021, pages 2414â€“2424, 2021.\n[9] Yujie Mo, Yuhuan Chen, Yajie Lei, Liang Peng, Xiaoshuang Shi, Changan Yuan, and Xiaofeng\nZhu. Multiplex graph representation learning via dual correlation reduction. IEEE Transactions\non Knowledge and Data Engineering, 2023.\n[10] Xiaowei Qian, Bingheng Li, and Zhao Kang. Upper bounding barlow twins: A novel filter for\nmulti-relational clustering. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 38, pages 14660â€“14668, 2024.\n[11] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang.\nGraph\nstructure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD\ninternational conference on knowledge discovery & data mining, pages 66â€“74, 2020.\n[12] Erlin Pan and Zhao Kang. Beyond homophily: Reconstructing structure for graph-agnostic\nclustering. In International Conference on Machine Learning, pages 26868â€“26877. PMLR,\n2023.\n[13] Jiong Zhu, Junchen Jin, Donald Loveland, Michael T Schaub, and Danai Koutra. How does\nheterophily impact the robustness of graph neural networks? theoretical connections and\npractical implications. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, pages 2637â€“2647, 2022.\n[14] Bingheng Li, Erlin Pan, and Zhao Kang. Pc-conv: Unifying homophily and heterophily with\ntwo-fold filtering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,\npages 13437â€“13445, 2024.\n[15] Daniel ZÃ¼gner, Oliver Borchert, Amir Akbarnejad, and Stephan GÃ¼nnemann. Adversarial\nattacks on graph neural networks: Perturbations and their patterns. ACM Transactions on\nKnowledge Discovery from Data (TKDD), 14(5):1â€“31, 2020.\n[16] Zhixun Li, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou,\nQiang Liu, Shu Wu, Liang Wang, et al. Gslb: The graph structure learning benchmark. Advances\nin Neural Information Processing Systems, 36, 2023.\n11\n[17] Paul Pu Liang, Zihao Deng, Martin Q Ma, James Y Zou, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. Factorized contrastive learning: Going beyond multi-view redundancy. Advances\nin Neural Information Processing Systems, 36, 2023.\n[18] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph\ntransformer networks. Advances in neural information processing systems, 32, 2019.\n[19] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger.\nSimplifying graph convolutional networks. In International conference on machine learning,\npages 6861â€“6871. PMLR, 2019.\n[20] Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan. Towards\nunsupervised deep graph structure learning. In Proceedings of the ACM Web Conference 2022,\npages 1392â€“1403, 2022.\n[21] Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. Slaps: Self-supervision improves\nstructure learning for graph neural networks. Advances in Neural Information Processing\nSystems, 34:22667â€“22681, 2021.\n[22] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional\nnetworks. In International Conference on Learning Representations, 2016.\n[23] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view\nredundancy, and linear models. In Algorithmic Learning Theory, pages 1179â€“1206. PMLR,\n2021.\n[24] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-\nsupervised learning from a multi-view perspective. In International Conference on Learning\nRepresentations, 2020.\n[25] Marco Federici, Anjan Dutta, Patrick ForrÃ©, Nate Kushman, and Zeynep Akata. Learning robust\nrepresentations via multi-view information bottleneck. In 8th International Conference on\nLearning Representations, 2020.\n[26] Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep\nrepresentations. Journal of Machine Learning Research, 19(50):1â€“34, 2018.\n[27] Thomas N Kipf and Max Welling. Variational graph auto-encoders. In Bayesian Deep Learning\nWorkshop (NIPS 2016, 2016.\n[28] Shaohua Fan, Xiao Wang, Chuan Shi, Emiao Lu, Ken Lin, and Bai Wang. One2multi graph\nautoencoder for multi-view graph clustering. In proceedings of the web conference 2020, pages\n3070â€“3076, 2020.\n[29] Yawen Ling, Jianpeng Chen, Yazhou Ren, Xiaorong Pu, Jie Xu, Xiaofeng Zhu, and Lifang\nHe. Dual label-guided graph refinement for multi-view graph clustering. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 37, pages 8791â€“8798, 2023.\n[30] Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient repre-\nsentation in contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16041â€“16050, 2022.\n[31] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng,\nZibin Zheng, and Weiqiang Wang. Whatâ€™s behind the mask: Understanding masked graph\nmodeling for graph autoencoders. In Proceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pages 1268â€“1279, 2023.\n[32] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What\nmakes for good views for contrastive learning? Advances in neural information processing\nsystems, 33:6827â€“6839, 2020.\n[33] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous re-\nlaxation of discrete random variables. In International Conference on Learning Representations,\n2016.\n12\n[34] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In\nInternational Conference on Learning Representations (ICLR 2017), 2017.\n[35] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation\nto improve graph contrastive learning. Advances in Neural Information Processing Systems,\n34:15920â€“15933, 2021.\n[36] Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, and Shirui Pan. Towards self-\ninterpretable graph-level anomaly detection. Advances in Neural Information Processing\nSystems, 36, 2024.\n[37] Yuanfu Lu, Chuan Shi, Linmei Hu, and Zhiyuan Liu. Relation structure-aware heterogeneous in-\nformation network embedding. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 33, pages 4456â€“4463, 2019.\n[38] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul\nKanakia. Microsoft academic graph: When experts are not enough. Quantitative Science\nStudies, 1(1):396â€“413, 2020.\n[39] Petar VeliË‡ckoviÂ´c, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio, and R Devon\nHjelm. Deep graph infomax. In International Conference on Learning Representations, 2018.\n[40] Zhiping Lin, Zhao Kang, Lizong Zhang, and Ling Tian. Multi-view attributed graph clustering.\nIEEE Transactions on Knowledge & Data Engineering, 35(02):1872â€“1880, 2023.\n[41] Erlin Pan and Zhao Kang. Multi-view contrastive graph clustering. Advances in neural\ninformation processing systems, 34:2148â€“2159, 2021.\n[42] Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, and Jiawei Han. An attention-\nbased collaboration framework for multi-view network representation learning. In Proceedings\nof the 2017 ACM on Conference on Information and Knowledge Management, pages 1767â€“1776,\n2017.\n[43] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Het-\nerogeneous graph attention network. In The world wide web conference, pages 2022â€“2032,\n2019.\n[44] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete\nstructures for graph neural networks. In International conference on machine learning, pages\n1972â€“1982. PMLR, 2019.\n[45] Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised\nconvolutional network. In Machine Learning and Knowledge Discovery in Databases: European\nConference, ECML PKDD 2020, Ghent, Belgium, September 14â€“18, 2020, Proceedings, Part\nIII, pages 378â€“393. Springer, 2021.\n[46] Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural\nnetworks: Better and robust node embeddings. Advances in neural information processing\nsystems, 33:19314â€“19326, 2020.\n[47] Ruijia Wang, Shuai Mou, Xiao Wang, Wanpeng Xiao, Qi Ju, Chuan Shi, and Xing Xie. Graph\nstructure estimation neural networks. In Proceedings of the web conference 2021, pages\n342â€“353, 2021.\n[48] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. Nodeformer: A scalable\ngraph structure learning transformer for node classification. Advances in Neural Information\nProcessing Systems, 35:27387â€“27401, 2022.\n[49] Kuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Reliable\nrepresentations make a stronger defender: Unsupervised structure refinement for robust gnn. In\nProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,\npages 925â€“935, 2022.\n13\n[50] Jianan Zhao, Qianlong Wen, Mingxuan Ju, Chuxu Zhang, and Yanfang Ye. Self-supervised\ngraph structure refinement for graph neural networks. In Proceedings of the Sixteenth ACM\nInternational Conference on Web Search and Data Mining, pages 159â€“167, 2023.\n[51] Ylli Sadikaj, Justus Rass, Yllka Velaj, and Claudia Plant. Semi-supervised embedding of\nattributed multiplex networks. In Proceedings of the ACM Web Conference 2023, pages 578â€“\n587, 2023.\n[52] Erlin Pan and Zhao Kang. High-order multi-view clustering for generic data. Information\nFusion, 100:101947, 2023.\n[53] Shima Khoshraftar and Aijun An. A survey on graph representation learning methods. ACM\nTransactions on Intelligent Systems and Technology, 15(1):1â€“55, 2024.\n[54] Liang Liu, Zhao Kang, Jiajia Ruan, and Xixu He. Multilayer graph contrastive clustering\nnetwork. Information Sciences, 613:256â€“267, 2022.\n[55] Liang Peng, Xin Wang, and Xiaofeng Zhu. Unsupervised multiplex graph learning with\ncomplementary and consistent information. In Proceedings of the 31st ACM International\nConference on Multimedia, pages 454â€“462, 2023.\n[56] Cheng Yang, Deyu Bo, Jixi Liu, Yufei Peng, Boyu Chen, Haoran Dai, Ao Sun, Yue Yu, Yixin\nXiao, Qi Zhang, et al. Data-centric graph learning: A survey. arXiv preprint arXiv:2310.04987,\n2023.\n[57] Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, and Yanfang Ye. Heterogeneous\ngraph structure learning for graph neural networks. In Proceedings of the AAAI conference on\nartificial intelligence, volume 35, pages 4697â€“4705, 2021.\n[58] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia\nMakedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2020.\n[59] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 15750â€“15758,\n2021.\n[60] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\nembeddings. In 2021 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2021, pages 6894â€“6910. Association for Computational Linguistics (ACL), 2021.\n[61] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised\npre-training for speech recognition. Interspeech 2019, 2019.\n[62] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and\nBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,\naudio and text. Advances in Neural Information Processing Systems, 34:24206â€“24221, 2021.\n[63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748â€“8763. PMLR, 2021.\n[64] Yijie Lin, Yuanbiao Gou, Zitao Liu, Boyun Li, Jiancheng Lv, and Xi Peng.\nCompleter:\nIncomplete multi-view clustering via contrastive prediction. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 11174â€“11183, 2021.\n[65] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. Graph\nself-supervised learning: A survey. IEEE transactions on knowledge and data engineering,\n35(6):5879â€“5900, 2022.\n[66] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data\naugmentation for graph neural networks. In Proceedings of the AAAI conference on artificial\nintelligence, volume 35, pages 11015â€“11023, 2021.\n[67] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of graph convolutions in\nmulti-layer networks. In The Eleventh International Conference on Learning Representations,\n2022.\n14\nA\nNotations\nTable 4: Frequently used notations.\nNotation\nDescription\nGv = {Av, X}\nThe v-th original graph.\nY\nThe label information.\nV, N, df\nThe number of graphs/nodes/features.\nAv âˆˆ{0, 1}NÃ—N\nThe adjacency matrix of v-th original graph.\nX âˆˆRNÃ—df\nThe shared feature matrix across all graphs.\nGâ€²\nv = {Aâ€²\nv, Xâ€²}\nThe v-th augmented graph.\nGs\nv = {As\nv, X}\nThe v-th refined graph.\nGs = {As, X}\nThe learned fused graph.\nHv âˆˆRNÃ—df\nThe node embeddings of the original graph from the graph learner.\nZv âˆˆRNÃ—d\nThe node representations of the refined graph of the GCN encoder.\nZ âˆˆRNÃ—d\nThe node representations of the fused graph from the GCN encoder.\nâƒ—m âˆˆ{0, 1}df\nThe random masking vector for feature masking.\nM âˆˆ{0, 1}NÃ—N\nThe random masking matrix for edge dropping.\nr\nThe order of graph aggregation in SGC.\nL\nThe number of layers in GCN.\nk\nThe number of neighbors in kNN.\nÎ»\nThe positive hyper-parameter in Lgen.\nI(Gs\ni; Gs\nj)\nThe mutual information between the i-th and j-th refined graphs.\nL\nThe total loss of InfoMGF-RA and InfoMGF-LA.\nLgen\nThe loss of augmented graph generator in InfoMGF-LA.\nâŠ™\nThe Hadamard product.\nÏƒ(Â·)\nThe non-linear activation function.\nBern(Â·)\nThe Bernoulli distribution.\n[Â·; Â·]\nThe concatenation operation.\nB\nRelated Work\nUnsupervised Multiplex Graph Learning (UMGL). Unlike supervised methods such as HAN [43]\nand SSAMN [51] which rely on label information, UMGL tackles unsupervised tasks in multiplex\ngraphs by using node features and graph structures [52]. Early UMGL methods such as MvAGC [40]\nand MCGC [41] combine graph filtering with unsupervised techniques such as spectral and subspace\nclustering to uncover underlying patterns in complex networks. With the rise of deep representation\nlearning [53], UMGL has embraced a new paradigm: Unsupervised learning of low-dimensional\nnode representations using graph neural networks (GNN) [4] and self-supervised techniques [5]\nfor downstream tasks such as node classification, node clustering, and similarity search. O2MAC\n[28] pioneered the use of GNNs in UMGL, selecting the most informative graph and reconstructing\nall graph structures to capture shared information. DMGI [2] and HDMI [8] maximize mutual\ninformation between local and global contexts, then fuse representations from different relations.\nMGCCN [54], MGDCR [9], and BTGF [10] employ various contrastive losses to align representations\nof diverse relations and prevent dimension collapse. CoCoMG [55] and DMG [3] capture complete\ninformation by learning consistency and complementarity between graphs. Despite these advances, a\ncritical factor that limits the performance of UMGL is overlooked: the reliability of graph structures,\nwhich is the focus of our research.\nGraph Structure Learning (GSL). With the advancement of graph neural networks, instead of\ndesigning complex neural architectures as model-centric approaches, some data-centric research\nhas focused on the graph data itself [56], with graph structure learning (GSL) gaining widespread\nattention for studying the reliability of graph structures. GSL, based on empirical analysis of graph\ndata, recognizes that real-world graph structures are often unreliable, thus opting to learn new\nstructures. GSLB [16] summarizes the general framework of graph structure learning: a Graph\nLearner takes in the original graph G = {A, X} and generates a refined graph Gs = {As, X}; then,\na Graph Encoder uses the refined graph to obtain node representations or perform class prediction.\nConsequently, GSL can be broadly categorized into supervised and unsupervised methods based on\nwhether label information is utilized to learn the new structure. For supervised GSL, probabilistic\nmodels like LDS [44] and GEN [47] are employed to generate graph structures; GRCN [45], IDGL\n[46], and NodeFormer [48] calculate node similarities through metric learning or scalable attention\n15\nmechanisms; while ProGNN [11] directly treats all elements in the adjacency matrix as learnable\nparameters. Meanwhile, methods like SUBLIME [20], STABLE [49], and GSR [50] introduce\nself-supervised signals through contrastive learning to learn graph structures without requiring label\ninformation. Almost all existing GSL studies concentrate on a single homogeneous graph, with only\na handful of works such as GTN [18] and HGSL [57] attempting supervised structure learning on\nheterogeneous graphs containing multiple types of nodes. There is still a lack of research concerning\nmore practically significant unsupervised graph structure learning within multiplex graphs.\nContrastive Learning and Information Theory. Contrastive learning, as an effective paradigm of\nself-supervised learning, enables representation learning without labeled information [58]. It has\nfound widespread applications across various modalities [59â€“61], particularly effective in multi-\nview or multi-modal tasks [62â€“64]. Its theoretical foundation is rooted in multi-view information\ntheory [25, 32]. Standard contrastive learning is based on the assumption of multi-view redundancy:\nshared information between views is almost exactly what is relevant for downstream tasks [17,\n23, 24]. They capture shared task-relevant information between views through contrastive pre-\ntraining, thus achieving data compression and sufficient representation learning. To successfully\napply contrastive learning to multi-modal data with task-relevant unique information, some studies\nhave improved the framework of contrastive learning and extended it to multi-view non-redundancy\n[17, 30]. Recent efforts also attempt to apply contrastive learning to graph learning tasks [65]. They\ngenerate contrastive views through graph data augmentation [66] or directly utilize different relations\nwithin graph data [41]. However, existing multi-view graph contrastive learning still suffers from\nthe limitation of multi-view redundancy, failing to extract view-unique task-relevant information\neffectively.\nC\nAlgorithm and Methodology Details\nC.1\nAlgorithm\nAlgorithm 1: The optimization of InfoMGF-RA\nInput: Original graph structure G = {G1, ..., GV }; Number of nearest neighbors k; Random\nmasking probability Ï; Number of epochs E\nOutput: Learned fused graph Gs and node representations Z\n1 Initialize parameters;\n2 Obtain view-specific node features {X1, Â· Â· Â· , XV } by Eq.(1);\n3 for e = 1, 2, 3, ..., E do\n4\nfor each view v in {1, Â· Â· Â· , V } do\n5\nGenerate refined graph Gs\nv = {As\nv, X} with graph learner by Eq.(1) and post-processors;\n6\nGenerate augmented graph Gâ€²\nv = {Aâ€²\nv, Xâ€²} with random feature masking and edge\ndropping;\n7\nend\n8\nGenerate fused graph Gs = {As, X} with graph learner by Eq.(6) and post-processors;\n9\nObtain node representations {Z1, Â· Â· Â· , ZV , Z1â€², Â· Â· Â· , ZV â€², Z} through graph encoder GCN;\n10\nCalculate the total loss L by Eq.(10) and update parameters in GCN and graph learners;\n11 end\n12 return fused graph Gs and node representations Z;\n16\nAlgorithm 2: The optimization of InfoMGF-LA\nInput: Original graph structure G = {G1, ..., GV }; Number of nearest neighbors k; Feature\nmasking probability Ï; Hyper-parameter Î»; Number of epochs E\nOutput: Learned fused graph Gs and node representations Z\n1 Initialize parameters;\n2 Obtain view-specific node features {X1, Â· Â· Â· , XV } by Eq.(1);\n3 for e = 1, 2, 3, ..., E do\n// Step 1:\nFix augmented graphs {Gâ€²\n1, Â· Â· Â· , Gâ€²\nV }\n4\nfor each view v in {1, Â· Â· Â· , V } do\n5\nGenerate refined graph Gs\nv = {As\nv, X} with graph learner by Eq.(1) and post-processors;\n6\nend\n7\nGenerate fused graph Gs = {As, X} with graph learner by Eq.(6) and post-processors;\n8\nObtain node representations {Z1, Â· Â· Â· , ZV , Z1â€², Â· Â· Â· , ZV â€², Z} through graph encoder GCN;\n9\nCalculate the total loss L by Eq.(10) and update parameters in GCN and graph learners;\n// Step 2:\nFix refined graphs and fused graph {Gs\n1, Â· Â· Â· , Gs\nV , Gs}\n10\nfor each view v in {1, Â· Â· Â· , V } do\n11\nGenerate augmented graph Gâ€²\nv = {Aâ€²\nv, Xâ€²} with random feature masking and\naugmented graph generator in Section 3.3\n12\nend\n13\nObtain node representations {Z1, Â· Â· Â· , ZV , Z1â€², Â· Â· Â· , ZV â€²} through graph encoder GCN;\n14\nObtain reconstructed features { Ë†X1, Â· Â· Â· , Ë†XV } through decoder;\n15\nCalculate Lgen by Eq.(11) and update parameters in augmented graph generator and decoder;\n16 end\n17 return fused graph Gs and node representations Z;\nC.2\nComplexity Analysis\nFirst, we analyze the time complexity of each component in InfoMGF. In this paragraph, let\nV , N, and m represent the numbers of graphs, nodes, and edges, while b1 and b2 denote the\nbatch sizes of the locality-sensitive k NN and contrastive loss computation. The layer numbers\nof graph learner, graph encoder GCN, and non-linear projector are denoted as L1, L2, and L3,\nrespectively. The feature, hidden layer, and representation dimensions are denoted as df, dh,\nand d, respectively. We analyze the complexity of kNN and GCN in scalable versions. Be-\nfore training, scalable SGC is applied with a complexity of O(V mrdf) related to the aggrega-\ntion order r. During training, we first perform a graph learner with scalable k NN that requires\nO(V NL1df + V Nb1df). For the GCN encoder and non-linear projector, the total complexity is\nO\n\u0000V mL2dh + V md + V NL2d2\nh + V Ndh(d + df) + V NL3d2\u0001\n. Within the graph augmentation\nmodule, the complexity of feature masking is O(Ndf). The learnable generative graph augmentation\nin InfoMGF-LA has a complexity of O(V Ndfdh + V mdh + V Ndfd), where the first two terms are\ncontributed by the augmented graph generator and the last one is for the decoder. For InfoMGF-RA,\nthe random edge drop requires O(V m) time complexity. For the loss computation, the complexity is\nO(V 2Nb2d).\nTo simplify the overall complexity, we denote the larger terms within L1, L2, and L3 as L, the larger\nterms between dh and d as Ë†d, the larger terms between b1 and b2 as B. Since the scalable SGC\noperation only needs to be performed once before training, its impact on training time is negligible.\nTherefore, we only consider total complexity during the training process. The overall complexity of\nboth InfoMGF-RA and InfoMGF-LA is O(V mL Ë†d + V NL Ë†d2 + V Ndf( Ë†d + L) + V NB(df + V Ë†d)),\nwhich is comparable to the mainstream unsupervised GSL models, including our baselines. For\nexample, SUBLIME [20] needs to be trained on each graph in a multiplex graph dataset, and its time\ncomplexity is O(V mL Ë†d + V NL Ë†d2 + V Ndf( Ë†d + L) + V NB(df + Ë†d)), which only has a slight\ndifference in the last term compared to the time complexity of our method.\n17\nC.3\nDetails of Post-processing Techniques\nAfter constructing the cosine similarity matrix of Hv, we employ the postprocessor to ensure that As\nv\nis sparse, nonnegative, symmetric and normalized. For convenience, we omit the subscript v in the\ndiscussion below.\nkNN for sparsity. The fully connected adjacency matrix usually makes little sense for most applica-\ntions and results in expensive computation cost. Hence, we conduct the k-nearest neighbors (kNN)\noperation to sparsify the learned graph. We keep the edges with top-k values and otherwise to 0 for\neach node and get the sparse adjacency matrix Asp.\nSymmetrization and Activation. As real-world connections are often bidirectional, we make the\nadjacency matrix symmetric. Additionally, the weight of each edge should be non-negative. With the\ninput Asp, they can be expressed as follows:\nAsym = Ïƒ(Asp) + Ïƒ(Asp)âŠ¤\n2\n(12)\nwhere Ïƒ(Â·) is a non-linear activation implemented by the ReLU function.\nNormalization. The normalized adjacency matrix with self-loop can be obtained as follows:\nAs = ( ËœDsym)âˆ’1\n2 ËœAsym( ËœDsym)âˆ’1\n2\n(13)\nwhere ËœDsym is the degree matrix of ËœAsym with self-loop. Afterward, we can obtain the adjacency ma-\ntrix As\nv for each view, which possesses the desirable properties of sparsity, non-negativity, symmetry,\nand normalization.\nC.4\nDetails of Loss Functions\nFor each view i and j, the lower and upper bound of I(Zi; Zj) in Eq.(8) and Eq.(9) can be calculated\nfor the node m:\nâ„“lb(Zi\nm, Zj\nm) = log\nesim( Ëœ\nZi\nm, Ëœ\nZj\nm)/Ï„c\nPN\nn=1 esim( Ëœ\nZi\nm, Ëœ\nZj\nn)/Ï„c\n(14)\nâ„“ub(Zi\nm, Zj\nm) = sim( ËœZi\nm, ËœZj\nm)/Ï„c âˆ’1\nN\nN\nX\nn=1\nsim( ËœZi\nm, ËœZj\nn)/Ï„c,\n(15)\nwhere ËœZi\nm is the non-linear projection of Zi\nm through MLP, sim(Â·) refers to the cosine similarity and\nÏ„c is the temperature parameter in contrastive loss. The loss Ls is computed as follows:\nLs = âˆ’\n1\nNV (V âˆ’1)\nV\nX\ni=1\nV\nX\nj=i+1\nN\nX\nm=1\n(â„“lb(Zi\nm, Zj\nm) + â„“lb(Zj\nm, Zi\nm)).\n(16)\nLikewise, we can compute Lf and Lu in the total loss L with the same approach. Upon optimizing\nL, our objective also entails the minimization of Lgen, which incorporates Î» âˆ—Lu (here we compute\nLu using the upper bound) and the loss term of the reconstruction. Lgen can be represented by:\nLgen = Î»âˆ—\n1\n2NV\nV\nX\ni=1\nN\nX\nj=1\n(â„“ub(Zi\nj, Ziâ€²\nj )+â„“ub(Ziâ€²\nj , Zi\nj))+\n1\nNV\nV\nX\ni=1\nN\nX\nj=1\n \n1 âˆ’\n(Xi\nj)âŠ¤Ë†Xi\nj\nâˆ¥Xi\njâˆ¥Â· âˆ¥Ë†Xi\njâˆ¥\n!\n(17)\nD\nProofs of Theorems\nD.1\nProperties of multi-view mutual information and representations\nIn this section, we enumerate some basic properties of mutual information used to prove the theorems.\nFor any random variables x, y and z, we have:\n(P1) Non-negativity:\nI(x; y) â‰¥0, I(x; y|z) â‰¥0\n(18)\n18\n(P2) Chain rule:\nI(x, y; z) = I(y; z) + I(x; z|y)\n(19)\n(P3) Chain rule (Multivariate Mutual Information):\nI(x; y; z) = I(y; z) âˆ’I(y; z|x)\n(20)\nWe also introduce the property of representation:\nLemma 1. [25, 26] If z is a representation of v, then:\nI(z; a|v, b) = 0\n(21)\nfor any variable (or groups of variables) a and b in the system. Whenever a random variable z is\ndefined as a representation of v, we state that z is conditionally independent of any other variable in\nthe system given v. This does not imply that z must be a deterministic function of v, but rather that\nthe source of zâ€™s stochasticity is independent of the other random variables.\nD.2\nProof of Proposition 1\nProposition 1. For any view i and j, 2I(Gs\ni; Gs\nj) is the lower bound of I(Gs\ni; Gj) + I(Gs\nj; Gi).\nProof of Proposition 1: Due to each Gs\ni is obtained from Gi through a deterministic function, which\nis independent of other variables. Thus, here Gs\ni can be regarded as a representation of Gi. For any\ntwo different views Gi and Gj, we have:\nI(Gs\ni; Gj)\n(P2)\n= I(Gs\ni; Gs\nj, Gj) âˆ’I(Gs\ni; Gs\nj|Gj)\n=âˆ—I(Gs\ni; Gs\nj, Gj)\n= I(Gs\ni; Gs\nj) + I(Gs\ni; Gj|Gs\nj)\nâ‰¥I(Gs\ni; Gs\nj)\n(22)\nwhere âˆ—follows from Lemma 1. The bound reported in this equation is tight when I(Gs\ni; Gj|Gs\nj) = 0,\nthis happens whenever Gs\nj contains all the information regarding Gs\ni (and therefore Gi). Symmetri-\ncally, we can also prove I(Gs\nj; Gi) â‰¥I(Gs\ni; Gs\nj), then we have\nI(Gs\ni; Gj) + I(Gs\nj; Gi) â‰¥2I(Gs\ni; Gs\nj)\n(23)\nProposition 1 holds.\nD.3\nProof of Theorem 1\nProof of Theorem 1. From the definition of optimal augmentation graph, we have\nI(Gâ€²\ni; Gi) = I(Y ; Gi)\n(24)\nSimilar to the proof of Proposition 1, as Gs\ni is regarded as a representation of Gi, therefore:\nI(Gs\ni; Y |Gi) = 0\n(25)\nI(Gs\ni; Gâ€²\ni|Gi) = 0\n(26)\nBased on Eq.(24) and the above two equations, then\nI(Gs\ni; Gâ€²\ni) = I(Gi; Gs\ni; Gâ€²\ni) + I(Gs\ni; Gâ€²\ni|Gi)\nEq.(26)\n=\nI(Gi; Gâ€²\ni) âˆ’I(Gi; Gâ€²\ni|Gs\ni)\nEq.(24)\n=\nI(Gi; Y ) âˆ’I(Gi; Y |Gs\ni)\n(P3)\n= I(Gi; Y ; Gs\ni)\nEq.(25)\n=\nI(Gi; Y ; Gs\ni) + I(Gs\ni; Y |Gi)\n(P3)\n= I(Gs\ni; Y )\n(27)\nIt shows that maximizing I(Gs\ni; Gâ€²\ni) and maximizing I(Gs\ni; Y ) are equivalent. Theorem 1 holds.\n19\nD.4\nProof of Theorem 2\nProof of Theorem 2. Here we theoretically compare I(Gs\ni; Gi) with I(Gs\ni; Gâ€²\ni).\nDiscussion 1. For I(Gs\ni; Gi), we have:\nI(Gs\ni; Gi) = I(Gi; Y ; Gs\ni) + I(Gs\ni; Gi|Y )\n= I(Gs\ni; Y ) âˆ’I(Gs\ni; Y |Gi) + I(Gs\ni; Gi|Y )\n= I(Gs\ni; Y ) + I(Gs\ni; Gi|Y )\n(28)\nIn the process of maximizing I(Gs\ni; Gi), not only is task-relevant information (the first term) maxi-\nmized, but task-irrelevant information (the second term) is also maximized.\nDiscussion 2. For I(Gs\ni; Gâ€²\ni), based on Theorem 1, we have:\nI(Gs\ni; Gâ€²\ni) = I(Gs\ni; Y )\n(29)\nObviously, no task-irrelevant information is maximized. Theorem 2 holds.\nD.5\nProof of Theorem 3\nProof of Theorem 3. To prove the theorem, we need to use the following three properties of entropy:\n(H1) Relationship between the mutual information and entropy:\nI(x; y) = H(x) âˆ’H(x|y)\n(30)\n(H2) Relationship between the conditional entropy and entropy:\nH(x|y) = H(x, y) âˆ’H(y)\n(31)\n(H3) Relationship between the conditional mutual information and entropy:\nI(x; y|z) = H(x|z) âˆ’H(x|y, z)\n(32)\nBy maximizing the mutual information with each refined graph, the optimized fused graph Gs would\ncontain all information from every Gs\ni. For any Gs\ni, we denote Gs\nc as the fused graph of all views\nexcept view i. Thus we have:\nH(Gs) = H(Gs\ni|Gs\nc) + H(Gs\nc|Gs\ni) + I(Gs\ni; Gs\nc)\n(33)\nwhere H(Gs\ni|Gs\nc) and H(Gs\nc|Gs\ni) indicate the specific information of Gs\nc and Gs\ni respectively, and\nI(Gs\ni; Gs\nc) indicates the consistent information between Gs\nc and Gs\ni.\nThen we have:\nH(Gs) = H(Gs\ni|Gs\nc) + H(Gs\nc|Gs\ni) + I(Gs\ni; Gs\nc)\n(H1)\n= H(Gs\ni|Gs\nc) + H(Gs\nc|Gs\ni) + H(Gs\ni) âˆ’H(Gs\ni|Gs\nc)\n(H2)\n= H(Gs\nc|Gs\ni) + H(Gs\ni, Gs\nc) âˆ’H(Gs\nc|Gs\ni)\n= H(Gs\ni, Gs\nc)\n(34)\nTherefore, for any downstream task Y , we further have:\nH(Gs, Y ) = H(Gs\ni, Gs\nc, Y ).\n(35)\nBased on the properties of mutual information and entropy, we can prove:\nI(Gs; Y ) = H(Gs) âˆ’H(Gs|Y )\n= H(Gs) âˆ’H(Gs, Y ) + H(Y )\nEq.(34)\n=\nH(Gs\nc, Gs\ni) âˆ’H(Gs\ni, Gs\nc, Y ) + H(Y )\n(36)\nBased on the properties of entropy, we have the proofs as follows:\nI(Gs\ni; Y ) = H(Gs\ni) âˆ’H(Gs\ni|Y )\n(37)\n20\nI(Gs\nc; Y |Gs\ni) = H(Gs\nc|Gs\ni) âˆ’H(Gs\nc|Gs\ni, Y )\n= H(Gs\ni, Gs\nc) âˆ’H(Gs\ni) âˆ’H(Gs\nc|Gs\ni, Y )\n(38)\nWith the equations above, we can obtain\nI(Gs\ni; Y ) + I(Gs\nc; Y |Gs\ni) = H(Gs\ni) âˆ’H(Gs\ni|Y ) + H(Gs\ni, Gs\nc) âˆ’H(Gs\ni) âˆ’H(Gs\nc|Gs\ni, Y )\n= H(Gs\ni, Gs\nc) âˆ’H(Gs\ni|Y ) âˆ’H(Gs\nc|Gs\ni, Y )\n= H(Gs\ni, Gs\nc) âˆ’H(Gs\ni, Y ) + H(Y ) âˆ’H(Gs\nc|Gs\ni, Y )\n(H2)\n= H(Gs\ni, Gs\nc) âˆ’H(Gs\ni, Y ) + H(Y ) âˆ’H(Gs\ni, Gs\nc, Y ) + H(Gs\ni, Y )\n= H(Gs\ni, Gs\nc) + H(Y ) âˆ’H(Gs\ni, Gs\nc, Y )\n(39)\nAccording to Eq.(36) and Eq.(39), we have:\nI(Gs; Y ) = I(Gs\ni; Y ) + I(Gs\nc; Y |Gs\ni).\n(40)\nAs I(Gs\nc; Y |Gs\ni) â‰¥0 (P1), then we can get\nI(Gs; Y ) â‰¥I(Gs\ni; Y ).\n(41)\nSimilarly, we can also obtain\nI(Gs; Y ) â‰¥I(Gs\nc; Y ).\n(42)\nAs Eq.(41) holds for any i, thus\nI(Gs; Y ) â‰¥max\ni\nI(Gs\ni; Y ).\n(43)\nTheorem 3 holds.\nE\nExperimental Settings\nE.1\nDatasets\nWe consider 4 benchmark datasets in total. The statistics of the datasets are provided in Table 5.\nThrough the value of â€œUnique relevant edge ratioâ€, we can observe a significant amount of view-\nunique task-relevant information present in each real-world multiplex graph dataset. It should be\nnoted that MAG is a subset of OGBN-MAG [38], consisting of the four largest classes. This dataset\nwas first organized into its current subset version in the following paper [1].\nTable 5: Statistics of datasets.\nDataset\nNodes\nRelation type\nEdges\nUnique relevant\nedge ratio (%)\nFeatures\nClasses\nTraining\nValidation\nTest\nACM\n3,025\nPaper-Author-Paper (PAP)\n26,416\n38.08\n1,902\n3\n600\n300\n2,125\nPaper-Subject-Paper (PSP)\n2,197,556\n99.05\nDBLP\n2,957\nAuthor-Paper-Author (APA)\n2,398\n0\n334\n4\n600\n300\n2,057\nAuthor-Paper-Conference-Paper-Author (APCPA)\n1,460,724\n99.82\nYelp\n2,614\nBusiness-User-Business (BUB)\n525,718\n83.12\n82\n3\n300\n300\n2,014\nBusiness-Service-Business (BSB)\n2,475,108\n97.49\nBusiness-Rating Levels-Business (BLB)\n1,484,692\n93.07\nMAG\n113,919\nPaper-Paper (PP)\n1,806,596\n64.59\n128\n4\n40,000\n10,000\n63,919\nPaper-Author-Paper (PAP)\n10,067,799\n93.48\nE.2\nHyper-parameters Settings and Infrastructure\nTable 6: Details of the hyper-parameters settings.\nDataset\nE\nlr\ndh\nd\nk\nr\nL\nÏ\nÏ„c\nRandom Aug.\nGenerative Aug.\nÏs\nlrgen\nÏ„\nÎ»\nACM\n100\n0.01\n128\n64\n15\n2\n2\n0.5\n0.2\n0.5\n0.001\n1\n0.01\nDBLP\n100\n0.01\n64\n32\n10\n2\n2\n0.5\n0.2\n0.5\n0.001\n1\n1\nYelp\n100\n0.001\n128\n64\n15\n2\n2\n0.5\n0.2\n0.5\n0.001\n1\n1\nMAG\n200\n0.005\n256\n64\n15\n3\n3\n0\n0.2\n0.5\n-\n-\n-\n21\nWe implement all experiments on the platform with PyTorch 1.10.1 and DGL 0.9.1 using an Intel(R)\nXeon(R) Platinum 8457C 20 vCPU and an L20 48GB GPU. We perform 5 runs of all experiments\nand report the average results. In the large MAG data set, InfoMGF-RA takes 80 minutes to complete\n5 runs, whereas, on other datasets, both versions of InfoMGF require less than 5 minutes.\nOur model is trained with the Adam optimizer, and Table 6 presents the hyper-parameter settings on\nall datasets. Here, E represents the number of epochs for training, and lr denotes the learning rate.\nThe hidden-layer dimension dh and representation dimension d of graph encoder GCN are tuned\nfrom {32, 64, 128, 256}. The number of neighbors k for kNN is searched from {5, 10, 15, 20, 30}.\nThe order of graph aggregation r and the number of layers L in GCN are set to 2 or 3, aligning with\nthe common layer count of GNN models [67]. The probability Ï of random feature masking is set\nto 0.5 or 0, and the temperature parameter Ï„c in contrastive loss is fixed at 0.2. For InfoMGF-RA\nusing random graph augmentation, the probability Ïs of random edge dropping is fixed at 0.5. For\nInfoMGF-LA with learnable generative graph augmentation, the generatorâ€™s learning rate lrgen is\nfixed at 0.001, the temperature parameter Ï„ in Gumbel-Max is set to 1, and the hyper-parameter Î»\ncontrolling the minimization of mutual information is fine-tuned from {0.001, 0.01, 0.1, 1, 10}. For\nthe large dataset MAG, we compute the contrastive loss for estimating mutual information in batches,\nwith a batch size of 2560.\nF\nAdditional Experiments\nF.1\nSensitivity Analysis\nWe analyze the impact of two important hyper-parameters: the number of neighbors k in kNN\nand hyper-parameter Î» controlling the influence of mutual information minimization to generate\naugmented graphs. The performance change of InfoMGF-LA with respect to k is illustrated in Figure\n5a. Overall, InfoMGF shows low sensitivity to changes in k. The model achieves optimal performance\nwhen k is set to 10 or 15. However, when k is very small (k = 5), detrimental effects may arise,\npossibly due to the limited number of beneficial neighbors. As k increases, the performance can still\nbe maintained high. Figure 5b shows the results to Î» from {0.001, 0.01, 0.1, 1, 10}. Our proposed\nmodel shows low sensitivity to changes in Î» in general, while the Î» corresponding to achieving the\nbest performance varies across different datasets.\nF.2\nRobustness\nFigure 5c shows the performance of InfoMGF and various baselines on the ACM dataset when\ninjecting random feature noise. It can be observed that InfoMGF exhibits excellent robustness against\nfeature noise, while the performance of SUBLIME degrades rapidly. As a single graph structure\nlearning method, SUBLIMEâ€™s performance heavily relies on the quality of node features. In contrast,\nour method can directly optimize task-relevant information in multi-view graph structures (e.g.,\nedges shared across multiple graphs are likely to be shared task-relevant information, which can be\ndirectly learned through Ls), thereby reducing dependence on node features. Consequently, InfoMGF\ndemonstrates superior robustness against feature noise.\n(a) The influence of k.\n(b) The influence of Î».\n(c) Robustness to feature noise.\nFigure 5: Additional experiments on sensitivity and robustness analysis.\n22\n(a) APA\n(b) APCPA\n(c) Gs\nFigure 6: Heatmaps of the subgraph adjacency matrices of the original and learned graphs on DBLP.\n(a) BUB\n(b) BSB\n(c) BLB\n(d) Gs\nFigure 7: Heatmaps of the subgraph adjacency matrices of the original and learned graphs on Yelp.\nF.3\nVisualization\nFigures 6 and 7, respectively, present the visualizations of the subgraph adjacency matrices of\nthe original multiplex graphs and the learned fused graph Gs on the DBLP and Yelp datasets. In\nDBLP, the two categories are machine learning (C1) and information retrieval (C2), while in Yelp,\nthe categories are Mexican flavor (C1) and hamburger type (C2). It can be observed that Gs not\nonly removes the inter-class edges in the original structure but also retains key intra-class edges\nwith weights, not just the shared edges. This further demonstrates the effectiveness of InfoMGF in\neliminating task-irrelevant noise while preserving sufficient task-relevant information.\nWe further visualize the learned node representations Z of the fused graph, which is used in the\nclustering task. Figure 8 shows the node correlation heatmaps of the representations, where both\nrows and columns are reordered by the node labels. In the heatmap, warmer colors signify a higher\ncorrelation between nodes. It is evident that the correlation among nodes of the same class is\nsignificantly higher than that of nodes from different classes. This is due to Gs mainly containing\nintra-class edges without irrelevant inter-class edges, which validates the effectiveness of InfoMGF\nin unsupervised graph structure learning.\n(a) ACM\n(b) DBLP\n(c) Yelp\nFigure 8: Node correlation maps of representations reordered by node labels.\n23\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.SI"
  ],
  "published": "2024-09-25",
  "updated": "2024-09-25"
}