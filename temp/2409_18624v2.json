{
  "id": "http://arxiv.org/abs/2409.18624v2",
  "title": "Unsupervised Cognition",
  "authors": [
    "Alfredo Ibias",
    "Hector Antona",
    "Guillem Ramirez-Miranda",
    "Enric Guinovart",
    "Eduard Alarcon"
  ],
  "abstract": "Unsupervised learning methods have a soft inspiration in cognition models. To\nthis day, the most successful unsupervised learning methods revolve around\nclustering samples in a mathematical space. In this paper we propose a\nstate-of-the-art, primitive-based, unsupervised learning approach for\ndecision-making inspired by a novel cognition framework. This\nrepresentation-centric approach models the input space constructively as a\ndistributed hierarchical structure in an input-agnostic way. We compared our\napproach with both current state-of-the-art unsupervised learning\nclassification, and with current state-of-the-art cancer type classification.\nWe show how our proposal outperforms previous state-of-the-art. We also\nevaluate some cognition-like properties of our proposal where it not only\noutperforms the compared algorithms (even supervised learning ones), but it\nalso shows a different, more cognition-like, behaviour.",
  "text": "IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n1\nUnsupervised Cognition\nAlfredo Ibias∗, Hector Antona, Guillem Ramirez-Miranda and Enric Guinovart\nAvatar Cognition\nBarcelona, Spain\nEmail: {alfredo∗, hector, guillem, enric}@avatarcognition.com\nEduard Alarcon\nUniversitat Polit`ecnica de Catalunya - BarcelonaTech\nBarcelona, Spain\nEmail: eduard.alarcon@upc.edu\n∗Corresponding author\nAbstract—Unsupervised learning methods have a soft inspi-\nration in cognition models. To this day, the most successful\nunsupervised learning methods revolve around clustering samples\nin a mathematical space. In this paper we propose a state-\nof-the-art, primitive-based, unsupervised learning approach for\ndecision-making inspired by a novel cognition framework. This\nrepresentation-centric approach models the input space construc-\ntively as a distributed hierarchical structure in an input-agnostic\nway. We compared our approach with both current state-of-\nthe-art unsupervised learning classification, and with current\nstate-of-the-art cancer type classification. We show how our\nproposal outperforms previous state-of-the-art. We also evaluate\nsome cognition-like properties of our proposal where it not only\noutperforms the compared algorithms (even supervised learning\nones), but it also shows a different, more cognition-like, behaviour.\nIndex Terms—Unsupervised learning, Incremental learning,\nCognitive systems, Explainable AI, Computational intelligence.\nI. INTRODUCTION\nUnsupervised learning is a huge field focused on extracting\npatterns from data without knowing the actual classes present\nin it. Due to this particularity, the field is full of methods\nthat cluster data based on its mathematical representation.\nThis hampers their applicability to data whose mathematical\nrelationships do not directly correlate with its cognitive re-\nlationships, relationships that cognitive agents (like humans)\nfind between the data. For example, the MNIST dataset has\na clear cognitive relationship between its different samples:\nthe number they represent. However, when transformed into\nnumerical values for clustering, their relationships fade out in\nfavour of relationships between their encodings, that do not\nnecessarily correspond with the cognitive ones.\nIn this field, there are multiple algorithms that focus on\nthe unsupervised classification problem. However, due to their\nsoft inspiration in cognition models, most of them address\nthe problem from an optimisation perspective. This approach\nrequires building a mapping between any input and a valid\noutput (ideally, the best output), and thus it is dividing the\ninput space into subspaces. In that regard, the representations\nManuscript received MMMM DD, 20YY; revised MMMM DD, 20YY.\nbecome spatial, in the sense that the classes are represented by\nsubspaces of an infinite space, independently of the similarity\nbetween the inputs that fall in that subspace.\nIn contrast, novel theories about how the brain works\npropose that the brain models the world in a constructive\nway, that is, it generates constructive representations of the\nworld [1]–[3]. A constructive representation would be an\nabstraction or archetype of a class, in the sense that, it would\nbe a representation to which any (or at least some) elements\nof the class are similar to. This implies that, to assign a class\nto an input, it has to be similar enough to one of the already\nlearned representations and, if it is not similar enough to any\nof them, it can not be classified. Mathematically speaking, the\ndifference between both approaches is that the first, traditional\none focuses on splitting a representation space, and the second,\nnovel one focuses on building a set of representations.\nTo empirically evaluate this new approach, we need to\ndevelop a new unsupervised learning algorithm. To develop\nsuch algorithm, we decided to follow a novel cognition frame-\nwork [4] that is based in the previously mentioned theories\nof the brain. This framework presents the Self-Projecting\nPersistence Principle (SPPP), that defines how latent infor-\nmation is present in reality, how it persist in time, and how it\nprojects itself to the world through manifestations. As such,\nthis framework presents the basic cognition task as building\nabstractions based on manifestations captured from latent in-\nformation, with the goal that such abstractions approximate the\noriginal latent information. To fulfil such task, the framework\nproposes to process manifestations into constructive repre-\nsentations through a primitive-based processing. Finally, this\nprimitive-based processing is scaled to build a whole Cognitive\nArchitecture.\nOur aim in this paper is not to develop the full Cognitive\nArchitecture, but just the Perception [5] part. This Perception\nshould recognise inputs without needing a label, and hence\nit is equivalent to an unsupervised learning algorithm. The\npreviously mentioned cognition framework [4] defines some\nrequirements for developing such an algorithm: it has to be\ninput-agnostic, primitive-based, scalable, and representation-\ncentric. With these requirements, in this paper we propose a\n0000–0000/00$00.00 © 2024 IEEE\narXiv:2409.18624v2  [cs.AI]  7 Nov 2024\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n2\nnovel unsupervised learning algorithm for decision-making we\ncall Cluster. We expect this algorithm to be a building block\ntowards a full cognition algorithm based on the previously\nmentioned cognition framework.\nIn this paper we present the fundamentals of the Cluster as\nwell as one of its modulators: the Spatial Attention modulator.\nThis modulator will auto-regulate the spatial discriminability\nof the algorithm. Additionally, we developed our proposal\nto be transparent and explainable, as it is desirable that any\nsolution can describe its representations and explain its deci-\nsions. Finally, a perk of focusing on generating constructive\nrepresentations is that our algorithm is able to state if a new\ninput does not correspond to any previously seen pattern, that\nis, it can say “I do not know”.\nWe compared our proposal to the main unsupervised classi-\nfication algorithms: K-Means for tabular data and Invariant\nInformation Clustering (IIC) [6] for image data. We com-\npared it with different configurations of K-Means and IIC\nand for four different static datasets (two tabular and two\nimage datasets), for an unsupervised classification task. The\nresults show a clear advantage of our proposal, being able to\ndeal with both tabular and image data with a decent perfor-\nmance. We also performed a sate-of-the-art comparison with\na medical dataset, in which we beat the current state-of-the-\nart for classifying cancer types. Finally, we performed some\nexperiments to evaluate cognition-like properties. In this case\nwe compared our proposal to not only K-Means and IIC, but\nalso the K-NN clustering supervised method. The comparison\nconsisted on recognising MNIST digits even when removing\nrandom pixels. In that experiment there is a clear advantage\nof our proposal over the compared algorithms that shows\nhow building constructive representations produces a different\nbehaviour, and thus has the potential to have cognition-like\nproperties. Given these results we conclude that our proposal\nis a state-of-the-art disruptive unsupervised learning algorithm\nfor decision-making, with different, more promising properties\nthan traditional algorithms.\nThe rest of the paper is organised in a related work resume\nat Section II, a proposal description in Section III, an empirical\nevaluation at Section IV, a discussion in Section V, a threats to\nvalidity analysis at Section VI, and a resume of the conclusions\nand future work at Section VII.\nII. RELATED WORK\nThere are multiple algorithms for unsupervised learning\ndeveloped along the years, from generic clustering algorithms\nlike K-Means [7], to more specific, usually Artificial Neural\nNetwork-based, algorithms that deal with only one task. In\nthis second category we can find algorithms that deal with\ntopics as unrelated as representation learning [8], [9], video\nsegmentation [10], speech recognition [11] or community\ndetection [12]. However, none of them try to build construc-\ntive representations, but instead they divide a mathematical\nrepresentation of the input space into clusters that represent\nthe different classes present in the input space.\nAmong these clustering algorithms, there are few that\nstand out, specially for the task of unsupervised classification.\nOne of them is K-Means due to its performance clustering\ntabular data. This algorithm clusters the samples based on\ntheir closeness in the mathematical space of their encodings.\nAnother one is Invariant Information Clustering (IIC) [6] due\nto its performance clustering images. This algorithm takes an\nimage, transforms it with a given transform, and then run both\nof them over two Artificial Neural Networks with the goal\nof learning what is common between them. To that effect, it\naims to maximise the mutual information between encoded\nvariables, what makes representations of paired samples the\nsame, but not through minimising representation distance like\nit is done in K-Means. In any case, both algorithms stand out\ndue to their performance in their respective domains, but none\nof them is able to obtain good accuracy across domains. Thus,\nwe will use them as baseline for comparison purposes, even\nthough they cannot be applied in all cases.\nFinally, regarding brain-inspired methods that try to model\nthe input space, the only research we are aware of is the Hier-\narchical Temporal Memory [13] (HTM) and SyncMap [14],\nalthough they are algorithms suited for learning sequences\ninstead of static data, and HTM is not unsupervised. Thus,\nas far as we are aware, ours is the first proposal of a brain-\ninspired, primitive-based, unsupervised learning algorithm for\nmodelling static data.\nIII. THE PROPOSAL\nOur proposal, based on the novel cognition framework\npresented at [4], is composed by: an Embodiment, a Cluster,\nand a Spatial Attention modulator. The goal of the Embodi-\nment is to transform the input space into Sparse Distributed\nRepresentations (SDRs), the goal of the Cluster is to process\nthose SDRs and model the input space generating constructive\nrepresentations, and the goal of the Spatial Attention modula-\ntor is to auto-regulate the Cluster.\nA. The Embodiment\nThe goal of our unsupervised learning algorithm is to\nmodel the input space generating constructive representations.\nTo do so, it requires a representation-oriented universal data\nstructure. Recent research has shown that such data structure is\nSparse Distributed Representations [13], [15] (SDRs), which\nallows for input-agnostic representations of inputs indepen-\ndently of their data type. This has been proven to be the actual\nway in which the brain processes its inputs [13], [16]. Thus,\nour algorithm will work only with SDRs.\nTo translate inputs to SDRs we need an encoder architecture.\nTo interpret the SDRs the Cluster generates we need a decoder\narchitecture too. Both architectures conform the Embodiment\nof the Cluster, as displayed in Figure 1. In our case, as in our\nexperiments we only explore tabular and image datasets, we\nonly present four kinds of encoder decoder pairs: one for float\npoint numbers, one for categorical data, one for grey images,\nand one for colour images. All these encoders are lossless and\nthus allow us to recover the encoded data.\nThe grey images translation to SDR is straightforward: a\ngrey image’s SDR is a flattened version of the image (in which\neach pixel is a dimension) with the values normalised to be\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n3\nFig. 1. Global Schema\nbetween 0 and 1. In the case of colour images, we perform\nthe same transformation to each one of the RGB channels and\nwe concatenate their SDRs to form the image SDR.\nFor floating point numbers their translation to SDR is a bit\nmore nuanced. We take the input space of the number (that\nis, the possible values it can take) and divide it into bins.\nThose bins will be the dimensions of the SDR. Then, each\nnumber will fall into one of those bins. However, in order to\nallow some overlap between numbers (what is fundamental for\nfinding similarities using the Cluster), we also activate as many\nbins around the number bin as another parameter we call bin\noverlap. With this, the SDR is a long list of zeros and some\nones around the bin where the number falls. By default, we\nuse an overlap of 10% of the number of bins, that by default\nis set to 100 bins. In the case of categorical data we create\none bin per category and set the overlap to 0%, following a\none-hot encoding.\nHaving these representations, we define the SDR represen-\ntation of a tabular input as a concatenation of the SDR repre-\nsentations of each entry, adjusting the indices to put one entry\nrepresentation after another. Using this same methodology, we\ncan compose multiple input types into one SDR. Although\nthis is not the goal of this paper, these compositions could\npotentially help our algorithm to deal with datasets in which\nthe input has many different types (for example, keyboard keys\nand video, like the recently released MineRL dataset [17]),\nalthough proving that would be matter of future work.\nHere it is important to remark that, due to the transformation\nof any input and any output into SDRs, the algorithm always\ndeals with the same data structures, and thus it is optimised to\nlearn them independently of what they represent. Moreover,\nthis makes our algorithm input-agnostic, as any kind of data\nis potentially transformable into an SDR.\nB. The Cluster\nOnce we have an SDR representation of the inputs (and\na way to recover the values from the SDR representation),\nwe need to process them. To that end, and following the\nrequirements outlined at Section I, we need a primitive-based\nprocessing. We defined a basic primitive we called Footprint,\nand we defined its scalability grouping multiple Footprints\ninto a set we called Cell, and grouping multiple Cells into a\nhierarchy we called Cluster, following the terminology present\nat [4].\n1) The Footprint: Our most basic processing unit, that is,\nour primitive, is the Footprint. A Footprint is an internal\nrepresentation with two basic functions. A Footprint contains\na data SDR, and can contain (for evaluation purposes only) a\nAlgorithm 1 Footprint Update (all ops are element-wise)\nRequire: FP: a Footprint, In: an input\n1: fp ←FP.SDR\ninp ←In.SDR\n2: n ←FP.N {Recall #inputs mixed into the Footprint}\n3: tmp ←fp ∗n\n4: tmp ←tmp + inp\n5: FP.SDR ←tmp/(n + 1)\n6: FP.N ←n + 1\nAlgorithm 2 Footprint Activation (all ops are element-wise)\nRequire: FP: a Footprint, In: an input\n1: fp ←FP.SDR\ninp ←In.SDR\n2: FP.ARCHETY PE ←(inp + fp)/2\n3: FP.PROJECTION ←(inp ∗2) −fp\n4: return FP.ARCHETY PE, FP.PROJECTION\nFig. 2.\n(left) An example of Footprint: the combination of the 1’s of the\nMNIST dataset.\n(right) An example of Cell: the Footprints of the 60,000 samples of the MNIST\ndataset.\nmetadata SDR (i.e. an SDR representing a label). A Footprint\nalso has an updating function and an activation function. The\nupdating function modifies the data and metadata SDRs when\nnecessary mixing the Footprint SDRs with an external SDR\n(Algorithm 1), while the activation function computes the\nArchetype and Projection of such Footprint (Algorithm 2). An\nexample of a Footprint is displayed at Figure 2 left.\nThe Archetype and the Projection are the two outputs of a\nFootprint after processing an input, and they are derived from\nthe SPPP [4]. The Archetype aims to be an abstracted version\nof the input, while the Projection aims to be a concreted\nversion of an Archetype. Both are the manifestations (and\nthus self-projections) of the Footprint at different perspectives,\nand they allow for the Footprint to be connected to other\nFootprints.\nFinally, as it is clear from Algorithm 1, our approach to\nbuild constructive representations consists on merging similar\ninputs to build the final representation. This is an aggregative\napproach to abstractions that is not very common on the field,\nbut whose benefits will be clear in the following sections.\n2) The Cell: The main limitation of Footprints is that\nthey can only store one representation. Thus, to be able to\nhave multiple representations, we needed to create multiple\nFootprints. To organise them, we grouped them inside a\nstructure called Cell, that coordinates the Footprints to avoid\nredundancies. Whenever a Cell receives an input, it has to\ndecide which Footprint will process it to produce the outputs,\nand it does so trough similarity: the most similar Footprint is\nthe one that is activated, and thus the one that will process\nthe input, and whose Archetype and Projection will be the\nCell’s Archetype and Projection. Thus, a Cell contains a set\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n4\nFig. 3. An example of Cluster: the Cells of the hierarchy present in the 60,000\nsamples of the MNIST dataset. The Seed Cell is the central black node.\nof Footprints and a threshold common to all its Footprints.\nThis threshold is used to decide whether an input is similar to\nan existing Footprint or not, and will be deeply defined in the\nnext section. An example of a Cell is displayed at Figure 2\nright.\nTo decide if a new input is considered similar to an existing\nFootprint, we use a Similarity Function to get an score of the\nsimilarity of both SDRs, and if that score is over the Cell’s\nthreshold, then both the input and the Footprint are considered\nsimilar to each other. When there are more than one Footprint\nwith a similarity over the Cell’s threshold, we consider similar\nonly the one with higher similarity. This similar Footprint is\nthen the active Footprint.\nTo compute the previously mentioned similarity score be-\ntween SDRs we need a Similarity Function that compares\nSDRs. This Similarity Function should take two SDRs and\nreturn a similarity value stating how similar we consider them\nto be. The specific Similarity Function we developed for this\npaper is a variation of the euclidean distance, but we also\ntested using the euclidean distance between vectors and the\ndifferences in results are minimal. An important remark here\nis that the Similarity Function should use only the data part of\nboth SDRs to compute the similarity in order for our method\nto be fully unsupervised, leaving the metadata part outside of\nany decision making.\n3) The Cluster: The core of our algorithm revolves around\nproperly building and organising Footprints. To organise them,\nthey are grouped inside Cells, but a Cell only allows us to\nhave representations at the same level of abstraction. To have\nmultiple levels of abstractions, we need to have multiple Cells\norganised in a tree-like hierarchical structure, and that is what\nwe call Cluster. An example of Cluster is displayed at Figure 3.\nAt the beginning of training there is only one Cell in\nthe Cluster (which we call Seed Cell), that starts with no\nFootprints. When new inputs are processed during training,\nnew Footprints are generated and the Seed Cell grows. And\nwhen an input is considered similar to an existing Footprint,\nthen the need for a hierarchy arises. In such a case, a new\nCell is created as a child of the Seed Cell, and it is associated\nwith that Footprint. Thus, a Cell can have as many children\nas Footprints, and each child Cell has an associated parent\nFootprint.\nWith this organisation, a Cluster’s goal is to organise Cells\nin a subset hierarchy, in which the Seed Cell contains the\nFootprints representing more inputs, and the leaf Cells contain\nFootprints representing only one input. The idea is that any\nchild Cell will subdivide the subset of inputs represented by its\nparent Footprint. To that end, is fundamental the fact that each\nCell has its own similarity threshold, what allows for a better\ndiscrimination policy as we will see in following sections.\n4) Processing an Input: Now let us show how a new input\nis processed by the Cluster. To follow this description, a\ngeneral schema of this algorithm is displayed at Figure 4.\nAs we can see in the schema, our input processing method\nhas three phases. The first phase is a filtering one, in which\nwe look for the Footprint most similar to the input. The\nsecond phase is an abstracting one, in which we go up the\nCluster generating an abstraction of the input using the Cell’s\nArchetype. In this phase is where the Footprint update also\nhappens. Finally, the third phase is a concreting one, in which\nwe take the generated abstraction from the previous phase and\nfilter it down the Cluster to find the Footprint most similar to\nthe abstraction of the input using the Cell’s Projection.\nWe start the filtering phase (left side of Figure 4) with\nan SDR that is a new input (this SDR contains both a data\npart and a metadata part). This input is then compared to all\nthe Footprints present in the Cluster, and we select, from the\nFootprints that surpass their Cell’s similarity threshold, the\nFootprint that gets the highest similarity. If a Footprint is\nselected, then we check if it has a child Cell, and if it has\nit, we create a new Footprint copy of the input in such Cell.\nIf the Footprint does not have a child Cell, then one is created\nthat contains two new Footprints: one is a copy of the input,\nthe other is a copy of the parent Footprint. If no Footprint is\nselected, then a new Footprint copy of the input will be created\nin the Seed Cell and will be selected. If we are in evaluation\nmode, no Cell or Footprint are created in this phase.\nIn the abstracting phase (centre side of Figure 4), the\nselected Footprint is activated and executes its updating and\nactivation functions in that order. The Archetype produced by\nthe activation function is the output of the Cell in this phase.\nThis output is then passed up as input to its parent Cell, where\nthe parent Footprint is activated and executes its updating and\nactivation functions with its child Cell’s Archetype. This way\na Footprint update is performed with an aggregation of the\ninput and the active child Footprints. This process is repeated\nuntil it has been done in the Seed Cell, and the Archetype\ngenerated by the Seed Cell is considered the Archetype of the\nCluster. If we are in evaluation mode, no Footprint is updated\nin this phase.\nFinally, in the concreting phase (right side of Figure 4),\nthe Cluster’s Archetype is used to perform the inverse of\nthe abstracting phase, but this time without learning. That is,\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n5\nFig. 4. The Cluster’s Training Schema\nthe Archetype is provided to the Seed Cell as input to be\ncompared against the existing Footprints, and if any Footprint\nis activated, that is, its similarity with the input is over\nthe Cell’s threshold, its activation function is executed. The\nProjection produced by the activation function is the output of\nthe Cell in this phase. This output is then passed down as input\nto the corresponding child Cell to repeat the process. When\nthere is no child Cell, or no Footprint is activated, the SDRs\nof the last activated Footprint are considered the Projection of\nthe Cluster. If there is no Footprint activated in the Seed Cell\nthen there is no Projection and an “I do not know” answer is\nprovided.\nThe final Projection is supposed to be a very concrete\nversion of the Cluster’s Archetype, which is an abstraction of\nthe original input. Then, the output of the whole method is this\nProjection, that includes a data SDR with a representation of\nthe input and a metadata SDR with the additional information\nabout the data SDR provided (like a label). This Projection is\nlater processed by the decoder of the Embodiment to retrieve\nthe final label of the input. A resume of the global algorithm\nis displayed at Figure 1. It is important to note that, if we are\nin evaluation mode, then the update function of the Footprints\nis not executed to not modify the internal representations, and\nno new Footprints neither Cells are created. Thus, if there is\nno Footprint activated in the Seed Cell in the third phase then\nthere is no Projection and our method answers an “I do not\nknow”.\nHere it is important to remark the relevance of the second\nand third phases: they allow generalisation. Without them, our\nalgorithm would be a set of copies of the inputs, deciding the\nclass by the most similar one. That is, it would be a convoluted\nimplementation of the K-Nearest Neighbours algorithm with\nK = 1. Adding these phases allows us to build constructive\nrepresentations, that in turn may be more similar to potential\ninputs than the already processed ones. Doing the updates in\nan intelligent way, we have more potential for generalisation.\nThis also allows us to avoid overfitting in subsequent epochs,\nbecause the aggregated representation will never be equivalent\nto an individual input.\nTo end with the Cluster, it is important to note that, during\ntraining, the label is provided in the metadata SDR to be\nincluded in the Footprints, but it is not used for comparing\nFootprints, and thus it is avoided for building and organising\nthe internal representations. Later, when in evaluation mode,\nthe label of each Footprint would be a mixture of the labels\nof the inputs that updated such Footprint. When classifying,\nthe returned label will be the strongest label of the Projection.\nAlgorithm 3 Spatial Attention\nRequire: FPS: a Cell’s Footprints\nRequire: SA: a Cell’s Spatial Attention\n1: SA ←0.0\n2: l ←length(FPS)\n3: n ←0\n4: maxSim(fp) = max([similarity(fp, ot) for ot in FPS])\n5: minSim(fp) = min([similarity(fp, ot) for ot in FPS])\n6: for i in 1 : l do\n7:\nfp ←FPS[i].SDR\n8:\nSA ←SA + ((maxSim(fp) + minSim(fp))/2)\n9:\nn ←n + 1\n10: end for\n11: SA ←SA/n\n12: return SA\nFinally, it is important to note how the Cluster is\nrepresentation-centric, as the whole algorithm focuses on\ngenerating the right internal representations of the inputs,\nwithout explicit guide by the classification goal. We aim that\nthese representations would produce the right Projections for\nclassification, but we do not base our updating in how well\nthey are classifying. Instead, we focus our learning on building\nrepresentations that make sense and can be considered proper\nabstractions of the individual inputs that made them. This is\nuseful to detect patterns, as different instances of the same\npattern will eventually collide into one representation, building\nan abstraction of such pattern.\nC. The Spatial Attention Modulator\nIn the previous Section, a threshold was used to decide if\ntwo SDRs were considered similar or not. This threshold can\nbe set arbitrarily, but that would hamper the performance of the\nCluster and would generate multiple extra parameters of the\nmodel. Thus, a way to automatise the threshold selection was\nneeded, and that is the role of the Spatial Attention Modulator.\nThe whole role of this modulator is to measure the variability\nof the input space of the Cell and dynamically set a similarity\nthreshold. The algorithm we developed to set such threshold\nis the average of the mean similarities between the Footprints\nof the Cell, and its pseudo-code is displayed at Algorithm 3.\nThe rationale behind using this approach is that any input\nspace has a certain variability, and the right threshold will\nbe that one that sits in the middle of such variability. Such\nvariability is unknown, but the Footprints have captured part\nof it in the form of aggregations. Thus, each aggregation (that\nis, each Footprint) represents a class and the average of the\nsimilarities between them is the “captured” variability.\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n6\nTABLE I\nCHARACTERISTICS OF THE EXPERIMENTAL SUBJECTS\nName\nType\n# Features\n# Samples\nWisconsin Breast Cancer\nTabular (Numerical)\n30\n569\nPima Indians Diabetes\nTabular (Numerical)\n8\n768\nMNIST\nImage (B&W)\n28 × 28\n60, 000 + 10, 000\nCIFAR10\nImage (RGB)\n32 × 32\n50, 000 + 10, 000\nCancer Type\nTabular (Numerical)\n1500\n398\nThis actually allows for child Cells to have higher thresholds\nthan their parent Cells, as their input space are limited to those\ninputs that are similar to their associated parent Footprint. This\ngenerates an increase in discrimination power the further down\nthe Cluster a Footprint is. In turn, this develops a distributed\nhierarchy, in which each Cell processes a different subdomain\nof the input domain.\nIV. EMPIRICAL EVALUATION\nTo evaluate our proposal, we performed three different\nexperiments: a comparison in classification task versus other\nunsupervised learning algorithms, a state-of-the-art compari-\nson over a medical dataset, and a comparison in cognition-\nlike capabilities versus other clustering algorithms. All the\nexperiments were run in an Ubuntu laptop with an Intel Core\ni9-13900HX at 2.60GHz with 32 cores, 32Gb of memory, and\na NVIDIA GeForce RTX 4060 with 8Gb of VRAM.\nA. Experimental Subjects\nOur experimental subjects for these experiments were five\ndatasets: two tabular datasets full of numerical values, two\nimage datasets, and a medical data dataset. Those datasets\nare the widely known Wisconsin Breast Cancer dataset [18],\n[19], Pima Indians Diabetes dataset [20], MNIST dataset [21],\nCIFAR10 dataset [22], and Cancer Type dataset (extracted\nfrom The Cancer Genome Atlas (TCGA) database [23]). The\ndifferent properties of these datasets are presented in Table I.\nWe divided these datasets into a training set and a test set.\nFor Wisconsin Breast Cancer and Pima Indians Diabetes we\nsplit the samples into 70% for the training set and 30% for the\ntest set. In the case of the MNIST and CIFAR10 datasets, they\ncome with 10, 000 samples for test. Thus, we took as training\nset all the samples from the training dataset and the test set\nare those 10, 000 test samples. The used Embodiments are\nthe ones described in Section III-A, with an overlap of 10%\nfor Wisconsin Breast Cancer and of 20% for Pima Indians\nDiabetes due to their respective characteristics.\nB. Experiments\n1) Learning Curves Experiment: The first experiment we\nperformed aimed to test how well our proposal deals with\na classification task compared to other unsupervised learning\nalgorithms. For tabular data we compared to K-Means with\nas many centroids as labels, and with the number of centroids\nthat the elbow method [24], [25] proposes. To evaluate its\nclassification power, each cluster was assigned the label that\nwas most repeated between the training elements of that\ncluster. In the case of our proposal, the label selected is the one\nassociated to the Projection. When comparing over the image\ndatasets (MNIST and CIFAR10), the Invariant Information\nClustering (IIC) algorithm was computed. In this case, the\nIIC algorithm was setup with the recommended parameters\nset by the authors for each dataset, and we compared different\nnumber of epochs (1, 10, 100) because we could not try the\nauthor recommended number of epochs (3, 200 for MNIST\nand 2, 000 for CIFAR10) or any higher number of epochs due\nto resource constraints.\nTo compare these algorithms, we executed them over the\nexperimental subjects computing the learning curve. That\nmeans, we trained the algorithms with the first 150 samples of\nthe training set, we evaluated them with the samples used to\ntrain, and then we tested them over the whole test set. Then\nwe trained them with the first 151 samples of the training\nset, evaluated them with the samples used to train, and tested\nthem over the whole test set again, and so on and so forth.\nWe repeated this process, adding 1 training sample each time,\nuntil the whole training set was used for training. We display\nthe resulting learning curves in Figure 5. Due to the size of\nthe datasets, when computing the image datasets results we\nexecuted the experiments for the first 200 samples, from then\non each 100 samples until the 2, 000 sample, from then on\neach 1, 000 samples until the 10, 000 sample, and from then\non we computed the result with the whole dataset.\nThe results of this experiment clearly show that our proposal\nis a better option for unsupervised classification. As we can\nobserve, for tabular data our alternative is on par with K-\nMeans for the Pima Indian Diabetes dataset (loosing by a\n1.3%) and for the Wisconsin Breast Cancer dataset (winning\nby a 1.17%). When we move to image data, we can observe\nhow our proposal is on par with IIC (loosing by 1.75% for\nMNIST with 100 epochs, but winning by 6.05% for CIFAR10\nand by 7.07% for MNIST with 10 epochs).\nWe want to explicitly remark the fact that our proposal is\nable to obtain very good accuracies with fewer samples. For\ncontrast, IIC needs around 1, 700 training samples to obtain an\nstable accuracy over 70% in test MNIST, while our proposal\nneeds less than 200 samples. Moreover, our proposal does\nnot need multiple epochs to obtain such results: it only goes\nthrough the training samples once, although more epochs also\nimprove results (as shown by the “10 epochs” lines). Finally,\nif we compare with IIC with only 1 training epoch, then IIC\nis not able to overcome our proposal in any scenario, what\nshows the performance improvement and data efficiency of\nour approach.\n2) State-Of-The-Art Experiment: Now, for our state-of-the-\nart experiment, we compared our algorithm over the Can-\ncer Type dataset with the state-of-the-art methods evaluated\nat [26]. We performed exactly the same experiment: cancer\ntype classification with five-fold cross-validation. Here being\nunsupervised was not a requisite, but even with our unsuper-\nvised approach we managed to get the results displayed at\nTable II, where it is clear how our proposal outperforms the\nother algorithms (except for the F1 macro measure).\nThis experiment is crucial to show how our proposal can\nproduce state-of-the-art results in certain scenarios, even when\ncompeting against Artificial Neural Network-based models. It\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n7\nWisconsin Breast Cancer\nPima Indians Diabetes\nMNIST\nCIFAR10\nFig. 5. Learning curves comparison for the different datasets\nTABLE II\nRESULTS OF MULTI-CLASS CLASSIFICATION BETWEEN CANCER\nSUB-TYPES. OTHER METHODS RESULTS ARE FROM [26]\nModel\nAccuracy\nF1 weighted\nF1 macro\nGCN [27]\n0.73\n0.721\n0.525\nGAT [28]\n0.733\n0.725\n0.552\nMOGONET [29]\n0.712\n0.717\n0.614\nMVGNN [26]\n0.735\n0.725\n0.636\nUnsupervised Cognition\n0.746\n0.737\n0.513\nalso shows how it can be used in real-world scenarios and not\nonly in toy examples like the ones used for the learning curves\nexperiment.\n3) Cognition-like Capabilities Experiment: Finally, in our\nlast experiment we wanted to explore the cognition-like\ncapabilities of our proposal, compared to other clustering\nalgorithms, using noise distortion [30]. To that end, inspired\nby [31], we devised an experiment using the MNIST dataset\nthat consists on training the algorithms with the first 10, 000\nsamples of the training set, and then take the 10, 000 samples\nfrom the test set and start taking out pixels. That is, for\ndifferent percentages (from 0% to 100% with a step of 2%), we\nremove that percentage of pixels (that is, we set them to black)\nfrom all the samples of the test set. Then, we evaluate all the\nalgorithms over that test set and compute both the accuracy\ncurve and the area under such curve. We did the same exper-\niment also using the 10, 000 train samples, in order to also\nevaluate such distortion curve over the already experienced\nsamples. The selected clustering algorithms are: our proposal,\nour proposal capped to have only 1 Cell, K-Means with 10\ncentroids, K-Means with 105 centroids, IIC with 1 epoch,\nIIC with 100 epochs, K-NN with 11 neighbours and K-NN\nwith 1 neighbour. We display the resulting distortion curves\nat Figure 6.\nThe idea behind this experiment is that, even after removing\nFig. 6.\nDistortion curves comparison for the different clustering\nalgorithms\nsome pixels from an image, humans are able to recognise\nnumbers. Moreover, if given some specific pixels, and after\nbeing told that such pixels represent a number, humans are\nable to fill in the number. Thus, we understand that recognising\nand/or reconstructing numbers is a capability of cognitive\nsystems, one that we would desire in any Artificial Intelligence\nalgorithm. Therefore, the goal here is to analyse how well\neach algorithm is able to recognise and reconstruct numbers\nfrom a set of pixels. As the pixels are removed at random,\nit is expected that after some removal percentage not even\nhumans are able to recognise them, but the more pixels are\nremoved, the better the concept of a number is understood\nif the number is correctly recognised. Thus, this experiment\nis expected to set a difference between those algorithms that\nhave an optimisation approach and those that have a modelling\none.\nAs we can observe, our proposal has better distortion curves\nthan any other alternative. In numbers, our proposal obtains an\nArea Under the Curve (AUC) of 92.60 over train and of 76.23\nover test. This AUC is way ahead of the next best one, obtained\nby the IIC with 100 epochs, that obtains an AUC for both train\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n8\nand test of 72.28. Moreover, as we can observe in Figure 6,\nthe other tested methods have a steep descend when there is\nmore than 50 −60% of pixels removed, while our proposal\nkeeps getting high accuracy until the very end, when it is\nalmost impossible to recognise the numbers even for a human\n(around an 80% of pixels removed). This contrast allows us\nto conclude that the behaviour presented by our proposal is\nfundamentally different than the behaviour of the alternatives.\nWe understand that this is the effect of developing the\nalgorithm around the idea of modelling the input space gen-\nerating constructive representations, as this is the fundamental\ndifference between the algorithms. This effect also implies\nthat our algorithm is finding a different kind of relationships\nbetween the samples than the pure numerical or pattern based\nones, showing cognition-like properties. Analysing it more\nin deep, we think that one of the keys of this behaviour is\nthe discriminative hierarchy of patterns. This hierarchy comes\nfrom the subdivision of the input space into subspaces through\nthe automatic replication of the Cells, and it allows for a more\nrobust representation of concepts, and thus a better adaptation\nto noise. Having different levels of representations allows for\na better matching between noisy samples and the internal\nrepresentations, as they can be more similar to some of the\nintermediate representations than to the lower, literal ones.\nC. Ablation Studies\nIn this Section we analyse the effects of the different\nparameters of our proposal. Let us start by stating that the\nmain “parameter” of our proposal is the Embodiment. In this\npaper we have presented very basic Embodiments, and our\nalgorithm works decently well with them. However, a fine\ntuned embodiment can cause huge increases in performance.\nFor example, during our experiments with the Pima Indian\nDiabetes dataset we discovered that our initial embodiment\n(with a 10% overlap) did not obtain the best results. Thus,\nafter testing multiple overlaps we settled in the 20% overlap.\nIn general, for numerical data, the overlap between numbers is\na fundamental parameter, because usually with a 0% overlap\nthe performance is low, then it quickly raises with a small\noverlap and eventually falls down when the overlap is too big.\nOther “parameters” of our proposal are the similarity and\nSpatial Attention functions. We presented the ones that we\nhave discovered that produce the best results, after trying a\nlot of alternatives like the similarity between the input and\nthe aggregation of the Footprints of the Cell for the Spatial\nAttention function or the Jaccard distance for the similarity\nfunction. Furthermore, we understand that alternative functions\ncan be developed with the potential to improve furthermore the\nresults, but looking for those improved functions is matter of\nfuture work.\nFinally, we would like to reflect on the idea that our proposal\ndoes not have real parameters, at least not in the sense of\nthe ones one can find in other Artificial Intelligence methods.\nThere is no “magic” number that needs to be fine tuned,\nbut instead all parameters of this kind correspond to the\nEmbodiment and the input preprocessing. We consider this\nto be a huge advantage of our proposal, as this allows it to be\nused with much less required knowledge and expertise.\nV. DISCUSSION AND LIMITATIONS\nIn this Section we would like to discuss the transparency\nand explainability of our algorithm, its capability of saying “I\ndo not know”, and its limitations.\nRegarding transparency and explainability, it is fundamental\nto note that, as our algorithm has an internal hierarchical\norganisation of Sparse Distributed Representations (SDRs), it\nis possible to recall how our algorithm decided which label\ncorresponds to the input. To that effect, we need the decoder\nfrom the Embodiment to transform the internal SDRs into\nunderstandable outputs. Thus, we can interpret any decision\nas a filtering from the Seed Cell, based on its Footprints, and\ndown the hierarchy until the last Footprint that was activated.\nThen, its representation is the Projection, and the strongest\nlabel of that Projection is the selected label.\nRegarding the capability of our algorithm to say “I do not\nknow”, it is easily derived from our threshold setup. If a new\ninput does not surpass the threshold for any Footprint, that is,\nits similarity with each one of the Footprints in the Cluster\nis lower than their associated thresholds, then our algorithm\nreturns a value stating it cannot associate that input to any\nknowledge it has learned. This is in fact used during training\nto generate new Footprints. Moreover, that answer is not only\nan “I do not know the label”, but it actually means that it\ndoes not have a model for such input, so it can not return\nany Projection of it neither. This is an important and novel\nfeature in an unsupervised learning algorithm. Its importance\nlies in the fact that saying “I do not know” ensures the user\nunderstands that the algorithm was not trained to recognise the\npattern that was given, instead of falsely providing an answer\nand hallucinating [32], [33].\nFinally, regarding the limitations of our proposal, its main\none is the high memory costs involved compared to other\nalternatives due to the storage of a huge number of SDRs.\nWe are aware that this limitation can hamper its scalability\nand applicability over very huge datasets and we are working\nin ways to diminish it, from developing growth inhibition and\ndeath mechanisms for the Footprints and Cells, to improving\nour Embodiments to generate smaller SDRs.\nA secondary but also important limitation is the fact that\nour proposal is not an optimisation method. This implies that\nits focus is not to generate the best answer, or to cluster in\nthe best way possible, like other algorithms. Instead, it is\nfocused on building meaningful representations, that are useful\nto represent the input space, and we expect that this focus will\nproduce, incidentally, a good classifier. This in turn hampers\nour classification capabilities, and thus our results, but anyway\nwe managed to obtain the good results presented in this paper.\nVI. THREATS TO VALIDITY\nIn this section we discuss the possible threats to the validity\nof our results. The first kind are the threats to internal validity,\nthat can explain our results due to uncontrolled factors. The\nmain threat in this category is the possibility of having a\nfaulty code. To reduce this threat we have carefully tested each\npiece of code used in our experiments and developed unit tests\nfor them, and we have relied on widely tested libraries like\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n9\nscikit for the K-Means and K-NN algorithms, and the authors\nimplementation for IIC. Another threat in this category is the\nimpact of randomisation in the comparison results. As our\nproposal is fully deterministic, randomisation does not affect\nit, and any subsequent run arises the exact same values. Thus,\nfor the compared methods we ran them multiple times and\nprovided the results from the best execution. Finally, the last\nthreat in this category is the use of unsupervised learning\nalgorithms for a classification task, task usually associated\nto supervised learning ones. We are aware that the K-Means\nalgorithms was not developed for classification tasks, but at\nthe same time we needed a way to compare the results of our\nalgorithm with theirs, having into account that characterising\nthe “clusters” that ours generates is not viable. Thus, we\ndecided to use classification tasks, as they allowed to compare\nhow well the algorithms detected the underlying relationship\nbetween the inputs, even if none of them uses the label for\ntraining purposes.\nThe second kind of threats are the ones to external validity,\nthat hamper the generality of our results to other scenarios. In\nour case the only threat in this category is the small scale\nexperimental setup, having compared against two methods\nover four datasets. However, we have performed small com-\nparisons with other methods and datasets too, and obtained\nsimilar results. Moreover, we have included also a state-of-\nthe-art comparison for a medical dataset to show the capability\nof our proposal of obtaining state-of-the-art results beating\nmultiple other algorithms, include Artificial Neural Network-\nbased ones. Nonetheless, the comparison of our proposal to\nless well known algorithms is matter of future work.\nFinally, the last kind of threats are the construction validity\nones, hampering the extrapolation of our results to real-world\nscenarios. In this case, the range of possible scenarios is\npotentially infinite, and this threat cannot be fully addressed,\nbut as explained before, the exploration of how our proposal\nbehaves in other scenarios is matter of future work.\nVII. CONCLUSIONS\nCurrent well known unsupervised learning methods have a\ndim capability of extracting cognition-like relationships due\nto their optimisation oriented setup. The biggest exponent of\nthis field is K-Means, that clusters samples based only on the\nmathematical distance between them. In this paper we have\nproposed an alternative, input-agnostic, representation-centric,\nunsupervised learning algorithm for decision-making that ex-\ntracts cognition-like relationships between samples through\nconstructive representations.\nOur proposal transforms the inputs into SDRs, and then\ngenerates an internal representation of those SDRs in order to\nlater recall that representation when asked about the class of\nan specific input. We tested our proposal against K-Means and\nIIC for unsupervised classification tasks in four datasets, and\nshow that our proposal is equivalent to them, even although it\nonly process each sample once. Moreover, we have compared\nit with the state-of-the-art for identifying cancer types, and\novercome them. Finally, we have evaluated how well it can dis-\ncover cognition-like relationships compared to other clustering\nalgorithms, and we have found that it is better than the three\nmain clustering algorithms: K-Means, IIC and K-NN. This is\nimportant because it means that our proposal does not only\nhave a different, better behaviour than unsupervised learning\nalgorithms, but also than supervised learning clustering ones.\nAs future work, we would like to explore how our proposal\nperforms in other datasets and against other unsupervised\nlearning algorithms, and perform an in deep analysis of the\nrelevance of each “parameter” of our model. We would also\nlike to develop new Embodiments for different input types, like\nsound. We would like to explore the extension of our algorithm\nwith other modulators too, like a conditioning modulator that\nallows us to have a reinforcement learning-like algorithm, or\na temporal modulator that allows us to process sequences.\nWe would like to explore different algorithms to compute the\nsimilarity function or the spatial attention function too. Finally,\nwe would like to extend our proposal with growth inhibition\nand death mechanisms for the Footprints and Cells, in order\nto reduce its memory costs.\nACKNOWLEDGMENTS\nWe want to thank Daniel Pinyol for his help building the\ncode of our proposal, and Daniel Pinyol and Pere Mayol for\nour insightful discussions about the topic.\nThe results shown here are in part based upon data generated\nby the TCGA Research Network: https://www.cancer.gov/tcga.\nREFERENCES\n[1] J. Hawkins and S. Blakeslee, On Intelligence.\nUSA: Times Books,\n2004.\n[2] J. Z. Leibo, J. Cornebise, S. G´omez, and D. Hassabis, “Approximate\nhubel-wiesel modules and the data structures of neural computation,”\nCoRR, vol. abs/1512.08457, 2015. [Online]. Available: http://arxiv.org/\nabs/1512.08457\n[3] D. Yon, C. Heyes, and C. Press, “Beliefs and desires in the predictive\nbrain,” Nature Communications, vol. 11, no. 1, p. 4404, Sep 2020.\n[4] A. Ibias, G. Ramirez-Miranda, E. Guinovart, and E. Alarc´on, “From\nmanifestations to cognitive architectures: A scalable framework,” in\nArtificial General Intelligence - 17th International Conference, AGI\n2024, Seattle, WA, USA, August 13-16, 2024, Proceedings, ser. Lecture\nNotes in Computer Science, vol. 14951.\nSpringer, 2024, pp. 89–98.\n[5] J. E. Laird, C. Lebiere, and P. S. Rosenbloom, “A standard model of\nthe mind: Toward a common computational framework across artificial\nintelligence, cognitive science, neuroscience, and robotics,” AI Mag.,\nvol. 38, no. 4, pp. 13–26, 2017.\n[6] X. Ji, A. Vedaldi, and J. F. Henriques, “Invariant information cluster-\ning for unsupervised image classification and segmentation,” in 2019\nIEEE/CVF International Conference on Computer Vision, ICCV 2019,\nSeoul, Korea (South), October 27 - November 2, 2019.\nIEEE, 2019,\npp. 9864–9873.\n[7] S. P. Lloyd, “Least squares quantization in PCM,” IEEE Trans. Inf.\nTheory, vol. 28, no. 2, pp. 129–136, 1982.\n[8] F. Wang, H. Liu, D. Guo, and F. Sun, “Unsupervised representation\nlearning by invariance propagation,” in Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,\n2020.\n[9] Y. Shin, C. Tran, W. Shin, and X. Cao, “Edgeless-gnn: Unsupervised\nrepresentation learning for edgeless nodes,” IEEE Trans. Emerg. Top.\nComput., vol. 12, no. 1, pp. 150–162, 2024.\n[10] N. Araslanov, S. Schaub-Meyer, and S. Roth, “Dense unsupervised\nlearning for video segmentation,” in Advances in Neural Information\nProcessing Systems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,\n2021, pp. 25 308–25 319.\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, VOL. X, NO. X, MMM YYYY\n10\n[11] A. Baevski, W. Hsu, A. Conneau, and M. Auli, “Unsupervised speech\nrecognition,” in Advances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, 2021, pp. 27 826–27 839.\n[12] J. Gao, J. Chen, B. M. Oloulade, R. Al-Sabri, T. Lyu, J. Zhang, and\nZ. Li, “Commgnas: Unsupervised graph neural architecture search for\ncommunity detection,” IEEE Trans. Emerg. Top. Comput., vol. 12, no. 2,\npp. 444–454, 2024.\n[13] Y. Cui, S. Ahmad, and J. Hawkins, “The HTM spatial pooler - A\nneocortical algorithm for online sparse distributed coding,” Frontiers\nComput. Neurosci., vol. 11, p. 111, 2017.\n[14] D. V. Vargas and T. Asabuki, “Continual general chunking problem and\nsyncmap,” in Thirty-Fifth AAAI Conference on Artificial Intelligence,\nAAAI 2021, Thirty-Third Conference on Innovative Applications of Arti-\nficial Intelligence, IAAI 2021, The Eleventh Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2021, Virtual Event, February\n2-9, 2021.\nAAAI Press, 2021, pp. 10 006–10 014.\n[15] S. Ahmad and J. Hawkins, “How do neurons operate on sparse\ndistributed representations? A mathematical theory of sparsity, neurons\nand active dendrites,” CoRR, vol. abs/1601.00720, 2016. [Online].\nAvailable: http://arxiv.org/abs/1601.00720\n[16] P. Foldiak, “Sparse coding in the primate cortex,” The handbook of brain\ntheory and neural networks, 2003.\n[17] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso,\nand R. Salakhutdinov, “Minerl: A large-scale dataset of minecraft\ndemonstrations,” in Proceedings of the Twenty-Eighth International Joint\nConference on Artificial Intelligence, IJCAI 2019, Macao, China, August\n10-16, 2019.\nijcai.org, 2019, pp. 2442–2448.\n[18] D. Dua and C. Graff, “UCI machine learning repository,” 2017.\n[Online]. Available: http://archive.ics.uci.edu/ml\n[19] W. Wolberg, W. Street, and O. Mangasarian, “Breast Cancer Wisconsin\n(Diagnostic),” UCI Machine Learning Repository, 1995.\n[20] J. W. Smith, J. E. Everhart, W. Dickson, W. C. Knowler, and R. S.\nJohannes, “Using the adap learning algorithm to forecast the onset\nof diabetes mellitus,” in Proceedings of the annual symposium on\ncomputer application in medical care.\nAmerican Medical Informatics\nAssociation, 1988, p. 261.\n[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning\napplied to document recognition,” Proc. IEEE, vol. 86, no. 11, pp. 2278–\n2324, 1998.\n[22] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features\nfrom tiny images,” 2009.\n[23] J. N. Weinstein, E. A. Collisson, G. B. Mills, K. R. Shaw, B. A.\nOzenberger, K. Ellrott, I. Shmulevich, C. Sander, and J. M. Stuart,\n“The cancer genome atlas pan-cancer analysis project,” Nature genetics,\nvol. 45, no. 10, pp. 1113–1120, 2013.\n[24] R. L. Thorndike, “Who belongs in the family?” Psychometrika, vol. 18,\nno. 4, pp. 267–276, Dec 1953.\n[25] F. Liu and Y. Deng, “Determine the number of unknown targets in open\nworld based on elbow method,” IEEE Trans. Fuzzy Syst., vol. 29, no. 5,\npp. 986–995, 2021.\n[26] Y. Ren, Y. Gao, W. Du, W. Qiao, W. Li, Q. Yang, Y. Liang, and G. Li,\n“Classifying breast cancer using multi-view graph neural network based\non multi-omics data,” Frontiers in Genetics, vol. 15, p. 1363896, 2024.\n[27] X. Li, J. Ma, L. Leng, M. Han, M. Li, F. He, and Y. Zhu, “Mogcn: a\nmulti-omics integration method based on graph convolutional network\nfor cancer subtype analysis,” Frontiers in Genetics, vol. 13, p. 806842,\n2022.\n[28] X. Xing, F. Yang, H. Li, J. Zhang, Y. Zhao, M. Gao, J. Huang, and J. Yao,\n“An interpretable multi-level enhanced graph attention network for\ndisease diagnosis with gene expression data,” in 2021 IEEE International\nConference on Bioinformatics and Biomedicine (BIBM).\nIEEE, 2021,\npp. 556–561.\n[29] T. Wang, W. Shao, Z. Huang, H. Tang, J. Zhang, Z. Ding, and K. Huang,\n“Mogonet integrates multi-omics data using graph convolutional net-\nworks allowing patient classification and biomarker identification,” Na-\nture communications, vol. 12, no. 1, p. 3445, 2021.\n[30] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understand-\ning deep learning (still) requires rethinking generalization,” Commun.\nACM, vol. 64, no. 3, pp. 107–115, 2021.\n[31] M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. G. Rabbat,\nY. LeCun, and N. Ballas, “Self-supervised learning from images with\na joint-embedding predictive architecture,” in IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC,\nCanada, June 17-24, 2023.\nIEEE, 2023, pp. 15 619–15 629.\n[32] P. A. Ortega, M. Kunesch, G. Del´etang, T. Genewein, J. Grau-Moya,\nJ. Veness, J. Buchli, J. Degrave, B. Piot, J. P´erolat, T. Everitt, C. Tallec,\nE. Parisotto, T. Erez, Y. Chen, S. E. Reed, M. Hutter, N. de Freitas, and\nS. Legg, “Shaking the foundations: delusions in sequence models for\ninteraction and control,” CoRR, vol. abs/2110.10819, 2021. [Online].\nAvailable: https://arxiv.org/abs/2110.10819\n[33] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,\nA. Madotto, and P. Fung, “Survey of hallucination in natural language\ngeneration,” ACM Comput. Surv., vol. 55, no. 12, 2023.\nAlfredo Ibias received B.Sc. degrees in Computer Science and in Mathematics\nfrom Complutense University of Madrid, Spain, and an M.Sc. degree in\nFormal Methods in Computer Science and a Ph.D. degree in Computer Science\nfrom the same university. For 4 years he did research around the development\nof AI methods for uncommon scenarios. Currently he is working as an AI\nresearcher at Avatar Cognition. His main research area is the development of\na general AI based on novel theories of the brain.\nHector Antona received B.Sc. degrees in Computer Science and in Telecom-\nmunications Engineering from Universitat Polit`ecnica de Catalunya (UPC),\nSpain, and an M.Sc. degree in Advanced Telecomunications Technologies\nfrom the same university. Currently he is working as an AI researcher at\nAvatar Cognition. His main research area is the applicability of novel AI\nmethods.\nGuillem Ramirez-Mirandaz received B.Sc. degree in Computer Science from\nUniversitat Polit´ecnica de Barcelona, Spain. For 3 years he did research\non performance analysis and optimisation of high-performance computing\napplications. Currently, he is working as a developer and researcher at Avatar\nCognition and pursuing a B.A. degree in Philosophy at Universidad Nacional\nde Estudios a Distancia (UNED), Spain. His main research area is the\ndevelopment of a general AI based on novel theories of the brain.\nEnric Guinovart received B.Sc. degree in Computer Science from Universitat\nPolit`ecnica de Catalunya (UPC), Spain. He has been working in the industry\nfor 20 years as AI consultant (among other roles). In 2018 he funded Avatar\nCognition, where he currently works as co-CEO, CTO and CRO. His main\nresearch area is the development of a general AI based on novel theories of\nthe brain.\nEduard Alarcon received the M.Sc. (National award) and Ph.D. degrees\n(honors) in Electrical Engineering from the Technical University of Catalunya\n(UPC BarcelonaTech), Spain, in 1995 and 2000, respectively. Since 1995\nhe has been with the Department of Electronics Engineering at the School\nof Telecommunications at UPC, where he became Associate Professor in\n2000 and is currently full professor. Visiting professor at CU Boulder and\nKTH. He has co-authored more than 450 scientific publications, 6 books, 8\nbook chapters and 12 patents. He has been involved in different National,\nEuropean (H2020 FET-Open, Flag-ERA, ESA) and US (DARPA, NSF,\nNASA) R&D projects within his research interests including the areas of\non-chip energy management and RF circuits, energy harvesting and wireless\nenergy transfer, nanosatellites and satellite architectures for Earth Observa-\ntion, nanotechnology-enabled wireless communications, Quantum computing\narchitectures and Artificial Intelligence chip architectures. He has received the\nGOOGLE Faculty Research Award (2013), SAMSUNG Advanced Institute of\nTechnology Global Research Program gift (2012), and INTEL Doctoral Stu-\ndent Honor Programme Fellowship (2014). Professional officer responsibilities\ninclude elected member of the IEEE CAS Board of Governors (2010-2013)\nand Vice President for Technical Activities of IEEE CAS (2016-2017, and\n2017-2018). Editorial duties include Senior founding Editorial Board of the\nIEEE Journal on Emerging topics in Circuits and Systems, of which he was\nEditor-in-Chief (2018-2019).\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-09-27",
  "updated": "2024-11-07"
}