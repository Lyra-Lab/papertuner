{
  "id": "http://arxiv.org/abs/2410.00704v1",
  "title": "Contrastive Abstraction for Reinforcement Learning",
  "authors": [
    "Vihang Patil",
    "Markus Hofmarcher",
    "Elisabeth Rumetshofer",
    "Sepp Hochreiter"
  ],
  "abstract": "Learning agents with reinforcement learning is difficult when dealing with\nlong trajectories that involve a large number of states. To address these\nlearning problems effectively, the number of states can be reduced by abstract\nrepresentations that cluster states. In principle, deep reinforcement learning\ncan find abstract states, but end-to-end learning is unstable. We propose\ncontrastive abstraction learning to find abstract states, where we assume that\nsuccessive states in a trajectory belong to the same abstract state. Such\nabstract states may be basic locations, achieved subgoals, inventory, or health\nconditions. Contrastive abstraction learning first constructs clusters of state\nrepresentations by contrastive learning and then applies modern Hopfield\nnetworks to determine the abstract states. The first phase of contrastive\nabstraction learning is self-supervised learning, where contrastive learning\nforces states with sequential proximity to have similar representations. The\nsecond phase uses modern Hopfield networks to map similar state representations\nto the same fixed point, i.e.\\ to an abstract state. The level of abstraction\ncan be adjusted by determining the number of fixed points of the modern\nHopfield network. Furthermore, \\textit{contrastive abstraction learning} does\nnot require rewards and facilitates efficient reinforcement learning for a wide\nrange of downstream tasks. Our experiments demonstrate the effectiveness of\ncontrastive abstraction learning for reinforcement learning.",
  "text": "Contrastive Abstraction for Reinforcement Learning\nVihang Patil∗\nMarkus Hofmarcher\nElisabeth Rumetshofer\nSepp Hochreiter\nLIT AI Lab, Ellis Unit Linz and Institute for Machine Learning\nJohannes Kepler University Linz\n{patil, hofmarch, rumetshofer, hochreit}@ml.jku.at\nAbstract\nLearning agents with reinforcement learning is difficult when dealing with long\ntrajectories that involve a large number of states. To address these learning problems\neffectively, the number of states can be reduced by abstract representations that\ncluster states. In principle, deep reinforcement learning can find abstract states, but\nend-to-end learning is unstable. We propose contrastive abstraction learning to\nfind abstract states, where we assume that successive states in a trajectory belong\nto the same abstract state. Such abstract states may be basic locations, achieved\nsubgoals, inventory, or health conditions. Contrastive abstraction learning first\nconstructs clusters of state representations by contrastive learning and then applies\nmodern Hopfield networks to determine the abstract states. The first phase of\ncontrastive abstraction learning is self-supervised learning, where contrastive\nlearning forces states with sequential proximity to have similar representations. The\nsecond phase uses modern Hopfield networks to map similar state representations\nto the same fixed point, i.e. to an abstract state. The level of abstraction can\nbe adjusted by determining the number of fixed points of the modern Hopfield\nnetwork. Furthermore, contrastive abstraction learning does not require rewards\nand facilitates efficient reinforcement learning for a wide range of downstream\ntasks. Our experiments demonstrate the effectiveness of contrastive abstraction\nlearning for reinforcement learning.\n1\nIntroduction\nKey in reinforcement learning (RL) is to learn proper representation of the environment. If the\nstate space is small and trajectories are short, RL can efficiently construct plans and solve tasks.\nIn particular, for Markov decision processes (MDPs) [74] with few states and short trajectories,\nan agent can readily learn world models, value functions, and policies. Therefore, the main goal\nin representation learning for RL is to find clusters of similar states, that form abstract states. If\nwe know how to transit from one abstract state to another one, then state abstraction transforms a\ncomplex problem into a simpler problem, e.g. an MDP with many states into an MDP with few states.\nDeep reinforcement learning gave us the hope that it automatically identifies abstract states, however,\nend-to-end learning of representations is not stable [56, 52, 92]. Therefore, recent work revisited\nauxiliary losses and data augmentation to obtain good representations [27]. Also, learning proper\nstate abstractions is computationally expensive, since a proper clustering of states must be identified\nin large set of possible clusterings. However, if state abstractions are learned reward free they can be\nused for many RL tasks, thus amortizing the cost of finding these state clusterings.\nA promising direction to learn proper RL representations without using rewards is self-supervised\nlearning, where a learning system is trained to capture the mutual dependencies between its inputs\n[57]. The data stream and not the model designer should determine good representations, which will\n∗Corresponding Author: patil@ml.jku.at\nNeurIPS 2023 Workshop on Generalization in Planning (GenPlan 2023).\narXiv:2410.00704v1  [cs.LG]  1 Oct 2024\n(a)\nInfoNCE\nCNN\nCNN\n(b)\nβ-Network\nβ\nModern Hopﬁeld Network\nState Representation\nFixed Points\n(c)\nReinforcement Learning\nFigure 1: Contrastive abstraction learning. (a) The first phase applies self-supervised learning via\ncontrastive learning to represent states with sequential proximity in a similar way. Using the InfoNCE\nobjective, two sequentially close states are forced to have a similar representation and non-close\nstates dis-similar representation. (b) In the second phase, a modern Hopfield network maps similar\nrepresentations to the same fixed point which constitutes an abstract state. (c) In the last phase,\ndownstream tasks are solved in the reduced state space.\nemerge under appropriate training schemes [100]. We leverage such reward free learning to obtain\nabstract states to make learning efficient in downstream tasks.\nIntuition for our approach.\nTypically, an agent remains in the same abstract state after a state\ntransition, while leaving the abstract state is a rare event. Such abstract states may be basic locations,\nachieved subgoals, current demands, inventory, or health conditions. If the agent is in the office\nand wants to buy from the supermarket, then abstract states of the location might be office building,\nstreet, and supermarket. The first subgoal would be to leave the office building to the street, the\nsecond subgoal to walk on the street to the supermarket, and the final subgoal to buy from the\nsupermarket. These abstract states may be identified by both sequential proximity of original states in\nthe trajectories and similarities of state representations. Therefore, original states should be clustered\nto create abstract states. In the next step, abstract actions have to be constructed that allow to transit\nfrom one abstract state to another. These abstract actions can be subagents, like a subagent for leaving\nthe building to the street. Given abstract states, subagents can be readily trained via reinforcement\nlearning. A start-goal-subagent is placed in a state that belongs to the start abstract state and receives\na reward if it reaches a state that belongs to the goal abstract state.\nWe propose to learn abstractions, i.e. proper state representations, through self-supervised learning.\nStates with sequential proximity in a trajectory are assumed to belong to the same abstract state.\nTherefore, in the first phase we use contrastive learning to map states with sequential proximity\nto a similar representation. In the second phase, similar representations are mapped by a modern\nHopfield network (MHN) [80] to the same fixed point. Such a fixed point constitutes an abstract state.\nMHNs allow abstraction at different levels since a temperature parameter determines the number of\nfixed points. Therefore, the same MHN with the same stored representations enables finer or coarser\nabstractions. Further, we learn the temperature parameter of the MHN. This is useful as some tasks\nneed to distinguish between states while in some cases we can unify them. Our main contributions\nare:\n• We propose a novel method called \"contrastive abstraction learning\" to learn abstractions\nin state space without rewards using contrastive learning in the first phase and MHNs in the\nsecond phase. Contrastive learning captures the structure of the environment by constructing\nstate clusters, while MHNs supply the abstract states given state clusters.\n• Contrastive abstraction learning controls the level of abstraction by a temperature parameter\nof the MHN. The control function adjusts the temperature based on the current state and is\ntrained by contrastive learning.\n• We study the resulting abstractions in different environments of varying complexity and\nevaluate the performance of our contrastive abstraction learning in downstream tasks.\n2\n2\nMethod\nWe propose the novel contrastive abstraction learning to find abstract states for RL. After learning,\noriginal states are mapped to the same abstract state if they are in sequential proximity in the training\ntrajectories. For example, in a grid world, all states within a room would be mapped to the same\nabstract state. Therefore the number of states is reduced to the number of rooms. Next, we have\nto construct a small number of sub-policies to move from one room to another room. Importantly,\nfinding such an abstract state does not depend on a goal or reward and, therefore, can be used for\nvarious downstream RL tasks. In the grid world example, representing the environment by the rooms\nhelps to solve any task where the agent has to visit specific rooms.\nFigure 1 shows an overview of our contrastive abstraction learning, which consists of three phases.\nIn the first phase, we sample two states from a sequence of states based on their sequential proximity\nand use contrastive learning to learn a representation that maps sequentially proximal states to a\nsimilar representation. The training trajectories might be expert examples, recorded examples, or\nexamples sampled from a random policy. In the second phase, MHNs reduce the number of states to\na small number of abstract states by mapping states to fixed points. In order to control the level of\nabstraction, the temperature parameters of MHNs can be learned to be controlled. In the final phase,\na policy on this small set of abstract states is learned very efficiently since there are only few abstract\nstates. To learn the policy, we have to learn sub-policies that move the agent from one abstract state\nto another. These sub-policies serve as actions for an MDP built with the abstract states.\n2.1\nContrastive Learning of Sequential Proximal States\nWith the advent of large corpora of unlabeled data in vision and language, contrastive learning\nmethods have become highly successful to learn expressive representations that can be adapted to\nvarious downstream tasks [45, 18, 75, 32, 90]. Similarly, contrastive learning can be applied to data\nof trajectories to construct rich state representations.We utilize contrastive learning to map states with\nsequential proximity to a similar representation. Given an MDP, the resulting representation encodes\nthe transition structure of the MDP. For example, if an agent transits from state s1 to s2 after selecting\nan action a, then the representation of s1 should be closer to s2 than other states st.\nWe define our problem setting as a finite MDP without reward to be a 3-tuple of (S, A, p) of finite\nset S with states s, A with actions a, transitions dynamics p(st+1 = s′ | st = s, at = a). Here, t is\nthe position in a state-action trajectory (s1, a1, . . . , sT , aT ) of length T. The agent selects actions\na ∼π(st = s) based on the policy π, which depends on the current state s. For contrastive learning,\npairs of states (st, st+k) from the same trajectory with small absolute value of k are selected as\npositive pairs with high probability, while other pairs are negative pairs, which also include states\nfrom different trajectories. We sample a set of positive pairs for each mini-batch of trajectories in\nthe following way. First, we uniformly sample a time index ti from a trajectory τ0:T . Then, we\nsample the time index tj from the same trajectory e.g. by centering a Gauss or Laplace distribution\non ti. The Laplace distribution can be approximated by using a discount factor and normalizing\nthe discounts, where states farther away from i are more discounted and, therefore, have lower\nprobability. Hence, the samples of a positive pair are temporally related. Negative pairs are samples\nfrom different trajectories or those with a large sequential distance. With N as the number of samples\nin the mini-batch, our contrastive objective [98] is\nLInfoNCE = −1\nN\nN\nX\ni=1\nln\nexp(τ −1 sT\ntistj)\nPN\nk=1 exp(τ −1 sT\ntistk)\n.\n(1)\n2.2\nSampling a Positive Pair\nThe sampling strategy to construct a positive pair for the contrastive objective has a profound impact\non the learned representation. Given a trajectory, the timestep ti of the first state of a positive pair is\nsampled uniformly. Then, we center a distribution at the timestep ti and then sample the timestep tj\nof the second state to complete the positive pair. We explore the following four distributions located\nat ti for sampling tj: (I) uniform, (II) Gaussian, (III) Laplace, and (IV) exponential. With sampling\nfrom a uniform distribution, the second timestep tj is sampled uniformly from the same trajectory,\nthus not using sequential proximity. For sampling from a Gaussian distribution, we center a Gaussian\nwith standard deviation σ at ti and select tj that is closest to a sample that can be from both the future\n3\nand the past. Analog for sampling from a Laplace distribution with scale parameter b. Also analog\nfor sampling from an exponential distribution inverse scale γ. Since only positive values are sampled,\nonly future states are sampled. For additional details on sampling strategies see Appendix Sec. (B).\n2.3\nAbstractions via Modern Hopfield Networks\nAfter learning state representations based on sequential proximity, we reduce their number by\nmapping them to abstract states. With a small state space RL can efficiently solve an MDP for a given\nreward function. MHNs [80] map states to fixed points that correspond to abstract states. Hopfield\nnetworks, introduced in the 1980s, are binary associative memories that played a significant role in\npopularizing artificial neural networks [3, 48, 49]. These networks were designed to store and retrieve\nsamples [17, 73, 7, 35, 1, 50, 13, 54]. In contrast to traditional binary memory networks, we employ\ncontinuous associative memory networks with exceptionally high storage capacity. These MHNs,\nutilized in deep learning architectures, have an energy function with continuous states and possess\nthe ability to retrieve samples with just a single update [80]. The update rule of MHNs is guaranteed\nto converge to a fixed point. These fixed points correspond to attractors in the network’s energy\nlandscape, and the network dynamics converge towards these attractors when starting at an input\npattern. MHNs have already been demonstrated to be successful in diverse fields [101, 87, 102, 69, 32].\nIn summary, MHNs store and retrieve patterns similar to the query (the input).\nWe leverage the mechanism of MHNs to map every input pattern to a fixed point. The MHNs have\nN state representations ui as stored patterns U = (u1, . . . , uN). The update rule of MHNs with\ninverse temperature β is\nξt+1 = U softmax(βU T ξt) ,\n(2)\nwhich is applied until convergence. The query x of the MHN is a state representation and serves\nas initial state ξ0 = x. To determine all fixed points of the MHN, we query with the ith stored\nrepresentations ξ0 = ui. Therefore, we obtain N fixed points of the MHNs. After removing\nduplicates, we get all M different fixed points (y1, . . . , yM) of the MHN that stores U.\nBy adjusting the inverse temperature β, we can control the number of fixed points y, and thus the level\nof abstraction. For β = 0 there is only one fixed point y, which is the average of all stored patterns.\nConversely, for large β the stored pattern is retrieved that is closest to the query. For intermediate β\nvalues, we obtain meta-stable states, that means several stored patterns converge to a fixed point they\nshare. The larger β the smaller the meta-stable states, i.e. less stored patterns are mapped to the same\nfixed point. Thus, small values for β result in a small number of fixed points while high values of β\nin a larger number of fixed points. Therefore, adjusting β to control the number of fixed points, and\nthus the granularity of our abstract representation, is crucial.\n2.4\nControlling the Level of Abstraction\nFor a given state s, we first obtain its state representation x, which serves as input to the MHN. Then\nwe obtain the fixed point y = y(x), which is the abstract state. Controlling the level of abstraction\nis essential to generate representations that are useful for a large number of tasks. MHNs are well\nsuited to achieve this control via the inverse temperature β.\nTraining a β-network.\nTo determine a suitable β for a given state representation x we train a\nseparate β-network (see overview Figure (1)). This β-network is trained using a contrastive setup.\nOne branch of the contrastive model applies random dropout to a given state representation x to\nobtain a masked representation x′. By masking features in the input, the network is forced to adjust\nthe level of abstraction in such a way, that even masked representations are assigned to the correct\nfixed point. A second branch consists of the β-network followed by a MHN. The original x is input\nto the β-network, a small FNN, which regresses a β value. This β value is then used in the MHN\nto map the state representation x to a fixed point y. The output of the two branches, the masked\nrepresentation x′ and the fixed point y, form a positive pair. Other masked representations from the\nmini-batch serve as negatives. The contrastive model is trained with the InfoNCE objective. See\nAppendix Sec. (C).\n4\nLaplace\nUniform\nExponential\nGaussian\nFigure 2: Visualization of learned representation of the states for different sampling techniques for\ncontrastive learning. Left: Two trajectories (⋆and ▲) in the Maze2D environment. Right: Different\nsampling techniques of positive pairs for contrastive learning. Red points are states in the dataset,\nwhile blue and green are the trajectories from left. Sampling from the Laplace distribution leads to\nrepresentation well suited for abstraction, while Gaussian does not yield clear clusters.\n2.5\nUsing Abstraction for Downstream Tasks\nAbstraction of states can be a powerful tool for improving the efficiency and effectiveness of planning\nand RL tasks. Abstract states can be used in several ways for downstream applications. The following\nthree approaches demonstrate ways in which abstraction can be applied to downstream tasks. Each\noffers unique advantages in terms of reduced state space and efficient learning. The specific approach\ncan be chosen depending on the characteristics of the task and the available resources.\nLearning a policy from abstract states (Abstract Policy).\nBy reducing the state space to a set of\nabstract states, we also reduce the complexity of the problem. Instead of operating in the original\nstate space, the policy can now focus on making decisions based on abstract states. We refer this\napproach as Abstract Policy. For this approach, we assume that the optimal policy selects the same\naction for all original states mapping to one abstract state. While this is a strong assumption we show\nexperimentally, that this approach is viable.\nLearning sub-policies as meta-actions (Meta Policy).\nSub-policies allow the agent to transition\nfrom one abstract state to another, thereby acting as meta-actions. Such sub-policies can be learned\nfrom existing trajectories or newly generated trajectories using imitation learning or other RL\nalgorithms. For example, the agent can learn to imitate trajectories between abstract states via\nBehavior Cloning (BC). The sub-policy for the transition from abstract state y1 to y2 starts in a\nstate mapped to y1 and receives a reward if it arrives at a state which maps to y2. Subsequently, a\nmeta-policy is trained to select the appropriate sub-policy based on the current abstract state. The\nadvantage of this approach is that it focuses on learning and selecting from the small abstract state\nspace, leading to faster and more targeted learning. We refer to this approach as Meta Policy.\nGoal-conditioned planning from a graph over abstract states (Planning).\nIn a third approach, a\ngraph is constructed over the abstract states, representing their relationships and connectivity. This\ngraph can then be utilized for planning and decision-making. Given a goal state, the agent can\nplan a sequence of abstract states that lead to the desired outcome using graph search algorithms.\nAdditionally, learned sub-policies can be employed to execute actions within the environment,\nfacilitating the agent’s progress towards the goal state. This approach combines the benefits of\nabstraction, graph-based planning, and learned sub-policies to enable effective goal-directed behavior.\n3\nExperiments\nIn order to show the effectiveness of contrastive abstraction learning we perform a series of experi-\nments. First, we verify that abstract representations encode the structure of the environment. Using\n5\nMaze2D\nRedBlueDoor\nRoom 2\nRoom 1\n(a)\n(a)\n(c)\n(b)\n(b)\nFigure 3: Learned representations (blue) and fixed points (red) of a MHN for Maze2D and Red-\nBlueDoor. The parameter β of the MHN increases from left to right, thus increasing the number\nof fixed points. The learned representation forms clusters of states where corridors (top) or rooms\n(bottom) are close. These clusters are mapped to abstract states via an MHN. In the rooms example\n(bottom row), (a) corresponds to states where the agent is in Room 1 and the red door is closed, (b)\ncorresponds to the red door being open, (c) corresponds to agent being in Room 2.\ntrajectories collected from a diverse set of environments we visualize the abstract representations\nvia fixed points. Next, we investigate the connection between β and the level of abstraction. Finally,\nwe show experiments for each of the three approaches of using contrastive abstraction learning for\ndownstream tasks detailed in the previous section.\n3.1\nEnvironments\nWe perform all experiments on a set of environments with increasing complexity selected or designed\nto showcase the effectiveness of a learned abstract representation. Furthermore, for learning a\ncontrastive representation as well as for learning the abstract representation we require a dataset of\ntrajectories. Therefore, we select a set of trajectories for each environment from existing datasets if\navailable or by sampling from a random policy in case no dataset is available. Alternatively, a dataset\ncan easily be acquired with an iterative approach by interacting with the environment but we found\nthat the representation learned from randomly sampled trajectories is often sufficient.\n(I) CifarEnv is an environment with images and classes from the CIFAR-100 dataset. It is used to\nverify if the method obtains abstract representations. We select 10 classes with defined transition\nstructures for various goals and tasks. The agent remains in the same state for a specified number\nof timesteps (e.g., 8), receiving images from the corresponding class. After the timesteps, the agent\nselects an action to transition to the next state. We sample 100,000 timesteps from this environment\nto create a dataset for contrastive abstraction learning. (II) Maze2D-large is an environment from\nMujoco Suite [96]. A robot must navigate to a given goal state from a random starting position. We\nuse trajectories from the D4RL dataset [31] for our experiments. (III) RedBlueDoor is part of the\nMinigrid library [20] and is a partially observable environment. We generate a dataset by storing all\ninteractions during training of a policy with Proximal Policy Optimization (PPO) [83]. We use the\nenvironment and the dataset of human demonstrations released for the MineRL competition [38].\n3.2\nContrastive and Abstract Representation\nIn order to show that contrastive abstraction learning results in a representation that encodes the\nstructure of the environment we present qualitative analysis for the various environments. We\nvisualize the learned representation after contrastive learning and the abstract representation obtained\n6\nMemory\nFixed Points\n(a) Fixed beta\nMemory\nFixed Points\n(b) Learned beta\nFigure 4: Visualization of all states (blue) in the memory, with the resulting fixed points (red) in\nCifarEnv. We compare the number of fixed points when the temperature parameter is fixed (a) and\nwhen the temperature parameter is learned (b). The learned temperature parameter reduces the state\nspace to a number of fixed points equal to the number of Cifar classes in the environment.\nvia fixed points from MHNs. We use PCA to downproject both the representation after contrastive\npre-training as well as the abstract representation.\nSampling strategies for contrastive pre-training.\nWe visualize the learned representation after\ncontrastive learning of sequentially proximal states for different sampling strategies (2.2) in Figure\n2. We visualize all samples after contrastive pre-training and highlight two trajectories. While on\nfirst look the representation obtained when sampling positive pairs using the Gaussian strategy seems\ngood, for subsequent abstraction this strategy is not well suited. The representation almost perfectly\nmirrors the original state space, therefore similar states are not closer to each other than without\ncontrastive pre-training. As expected, the Uniform strategy results in unclear clusters and thus a\nsub-par representation. From the remaining two we prefer sampling with the Laplace strategy as\nclusters of states emerge and states where both trajectories overlap are closer in the representation.\nControlling the level of abstraction through β.\nNext, we visualize abstract states obtained via\nfixed points of the MHN. First, we show the effect of β on the number of abstract states. In Figure 3 we\nshow three different settings of beta for three environments, namely Maze2D-large and RedBlueDoor.\nFrom left to right we increase β and therefore the number of abstract states that emerge. This shows\nthat finding a suitable value for β is crucial for solving downstream tasks.\nLearning β.\nNext, we show on CifarEnv that learning β works well and results in a set of abstract\nstates that is both small and covers all modes of the state space. Figure 4 (a) shows a manually\nselected value for β that at first glance results in a good abstract representation. However, the set of\nabstract states is still much larger than necessary. In Figure 4 (b) the abstract states, represented by\nfixed points of the MHN, are much sparser while still covering the state space. Manually tuning β for\neach environment would be time-consuming, therefore learning an good value for β is crucial.\n3.3\nAbstract Policy\nThe first approach for using the abstract representation for downstream tasks we investigate is training\nan Abstract Policy. We assume that the optimal policy selects the same action for every state belonging\nto an abstract state. As this assumption holds in the CifarEnv we train a policy using PPO. At every\ntimestep t the original state st is first mapped to it’s corresponding abstract state y. This abstract state\nis then used by the policy network. The original action space is not changed. Figure 5 shows that\nAbstract Policy can solve all tasks in this environment, only outperformed by the next Meta Policy.\nFor comparison we include two additional baselines. Contrastive Representation is trained with PPO\nusing the representation after contrastive pre-training, thus without abstract states. Original State is a\nPPO baseline on the original state space without constrastive abstraction learning.\n7\n0\n25000\n50000\n75000\n100000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReturn\nGoSchool\n0\n25000\n50000\n75000\n100000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGoOﬃce\n0\n25000\n50000\n75000\n100000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGoMarket\n0\n25000\n50000\n75000\n100000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGoHouse\nMeta Policy\nAbstract Policy\nATC\nOriginal State\nFigure 5: We show training of policies from abstract states to solve four tasks in CifarEnv. Abstract\nPolicy uses abstract states but the original action space. Meta Policy selects actions from an abstract\naction space and executes sub-policies. Contrastive Representation uses state representations from\ncontrastive pre-training. For Original State both action and state space are not changed. Both methods\nusing abstract states can solve all tasks.\nbed\nhouse\nbus\nroad school\nroad market\nmarket\nschool\nroad oﬃce\nbicycle\noﬃce\n(a) Original graph (CifarEnv)\nbed\nhouse\nbus\nroad school\nroad market\nmarket\nschool\nroad oﬃce\nbicycle\noﬃce\n(b) Graph with fixed points (CifarEnv)\nFigure 6: We compare the actual underlying transition graph (a) between states and the graph we\nconstruct using fixed points (b). Such a graph can be used for high-level planning in the environment.\n3.4\nMeta Policy\nNext, we train the Meta Policy. In contrast to the Abstract Policy, the original action space is replaced\nby an abstract action space where each action represents a transition from one abstract state to another.\nThese abstract actions are learned, either by interacting with the environment or, as in our case, via\nBC. Thus, each abstract action is a sub-policy that imitates behavior from the dataset. When an\nabstract action is selected, the corresponding sub-policy is executed in the environment. The Meta\nPolicy is again trained with PPO. In Figure 5 we show results using this approach in the CifarEnv.\nMeta Policy outperforms all other approaches and baselines.\n3.5\nPlanning\nThe third approach for obtaining a policy from abstract states is Planning. Here, we extract a\ngraph from the abstract states describing an environment and then use it for planning. The graph is\nconstructed by calculating the cosine similarity of all abstract states with each other. Then, a threshold\nis applied to remove connections between dissimilar abstract states. In Figure 6 we compare the true\ngraph and the graph constructed from the abstract states. While there is a some additional edges, the\nconstructed graph is sufficient to solve almost all tasks. In this experiment, we randomly sample goal\nstates and plan a path towards it using the graph. We find the shortest path from the current abstract\nstate to the abstract state of the goal on the graph. Similar to Meta Policy, we use sub-policies to\ntransition from one abstract state to another. These are again learned with BC from the dataset.\nIn addition to CifarEnv we evaluate the robustness of constructing a valid graph from abstract states.\nFor each environment in Table 1 we sample 1000 start and goal states. Using a graph, we find a plan\nto transition from the start to goal state and execute this plan in the environment. We then record if the\ngoal state is reached within a certain number of timesteps. For more details see Appendix Sec. (D).\n8\nTable 1: Planning over graphs. For each environment 1000 abstract start and goal states are sampled.\nA graph is constructed and used for planning a policy to transition from start to goal state. We show\nthe rate of success of reaching the goal state when the plan is executed in the environment.\nEnvironment\nCifarEnv\nMaze2D-Large\nMaze2D-Medium\nSuccess (%)\n86\n88\n92\n4\nRelated Work\nRecently, contrastive learning was very successful at constructing powerful representations with\nclusters. In the embedding space, contrastive learning brings representations of paired inputs closer to\none another while moving unpaired inputs farther away from each other [18, 75, 82]. Following this\nsuccess, Contrastive Learning has been used in RL to learn representations [91, 27]. CURL [91] learns\nrepresentations on images by pairing a state with its augmented version. CoBerl [8] masks elements\nof trajectories in the input and then tries to predict them, analogously to BERT in the language domain.\n[27] pairs state-actions with future state-actions in a trajectory. Contrastive learning has been used as\nunsupervised pretraining for RL. APS [42] and ATS [43] learn representations by maximizing entropy\nduring an unsupervised phase and fine-tuning for a subsequent task. ATC [94] pairs state with a state\nthat is k steps ahead in the trajectory. SPR [84, 85], KSL [64] and [4] predict the representation of\nk future steps and bring it closer to the actual representation. Proto-RL [105] tries similarly to use\nideas from Swav [15] and offline RL [? 86] to first explore the environment in pre-training to learn\nrepresentation from diverse samples and then fine-tunes on downstream task.\nAbstraction in RL is a well-studied area [10, 36, 6, 81, 95, 59, 22, 5, 99, 55, 72, 26, 12]. Bisimulation\n[36, 29, 16, 106] tries to simplify MDPs by identifying states which lead to similar behaviors\n(bisimilar states). Methods based on bisimulation learn abstractions that are dependent on a particular\nreward function. Contrastive abstraction learning enables the identification of sub-goals and sub-\ntasks, thus making it relevant to hierarchical reinforcement learning (HRL) approaches such as the\noption framework [95], the MAXQ framework [25], and the recursive composition of option models\n[89]. Several approaches have been proposed to tackle the problem of learning good options. One\napproach is selecting frequently observed solution states as targets [93]. Another strategy involves\nemploying gradient-based techniques to enhance the termination function for options [21, 63, 58].\n[58] employed policy gradient optimization to learn a unified policy comprising intra-option policies,\noption termination conditions, and an option selection policy.\nParametrized options can be learned by treating the termination functions as hidden variables and\napplying expectation maximization [22]. Intrinsic rewards have also been utilized to learn policies\nwithin options, while extrinsic rewards are used to learn the policy over options [55]. [5] proposed\na method that jointly learns options and their associated policies using the policy gradient theorem.\nAdditionally, a manager module operating on a slow time scale has been introduced to learn sub-goals,\nwhich are subsequently achieved by a worker module operating on a fast time scale [99].\n5\nConclusion\nWe propose contrastive abstraction learning, a novel approach for state abstraction in RL. Abstract\nstates drastically reduce the number of states for planning in RL. We use contrastive learning to learn\na representation where states in sequential proximity are mapped to similar representations. Then\nwe use MHNs to determine abstract states via fixed points. Furthermore, we can adjust the level of\nabstraction by controlling the number of fixed points. We show how to learn the level of abstraction\nvia contrastive learning. We propose three approaches for using such an abstract representation in RL\nfor efficiently learning policies. Our experiments show, that our approach is able to find a small set of\nabstract states in several diverse environments and that learning policies based on this representation\ncan be efficient.\n9\nAcknowledgments and Disclosure of Funding\nThe ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal\nState Upper Austria. We thank the projects AI-MOTION (LIT-2018-6-YOU-212), DeepFlood (LIT-\n2019-8-YOU-213), Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064),\nPRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), EPILEPSIA\n(FFG-892171), AIRI FG 9-N (FWF-36284, FWF-36235), AI4GreenHeatingGrids(FFG- 899943),\nINTEGRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-\nCL6-2021-CLIMATE-01-01). We thank Audi.JKU Deep Learning Center, TGW LOGISTICS\nGROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF\nFriedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund\nAG, GLS (Univ. Waterloo) Software Competence Center Hagenberg GmbH, TÜV Austria, Frauscher\nSensonic, TRUMPF and the NVIDIA Corporation.\nReferences\n[1] L. F. Abbott and Y. Arian. Storage capacity of generalized networks. Physical Review A, 36:\n5091–5094, 1987. doi: 10.1103/PhysRevA.36.5091.\n[2] S. Agarwal, G. Krueger, J. Clark, A. Radford, J. W. Kim, and M. Brundage. Evaluating\nCLIP: Towards characterization of broader capabilities and downstream implications. ArXiv,\n2108.02818, 2021.\n[3] S.-I. Amari. Learning patterns and pattern sequences by self-organizing nets of threshold\nelements. IEEE Transactions on Computers, C-21(11):1197–1206, 1972. doi: 10.1109/T-C.\n1972.223477.\n[4] A. Anand, J. Walker, Y. Li, E. Vértes, J. Schrittwieser, S. Ozair, T. Weber, and J. Hamrick.\nProcedural generalization by planning with self-supervised world models. arXiv, 2111.01587,\n2021. URL https://arxiv.org/abs/2111.01587.\n[5] P. L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the\nThirty-First AAAI Conference on Artificial Intelligence, page 1726–1734. AAAI Press, 2017.\ndoi: 10.5555/3298483.3298491.\n[6] R. Balaraman. An algebraic approach to abstraction in reinforcement learning. 2003.\n[7] P. Baldi and S. S. Venkatesh. Number of stable points for spin-glasses and neural networks of\nhigher orders. Physical Review Letters, 58:913–916, 1987. doi: 10.1103/PhysRevLett.58.913.\n[8] A. Banino, A. Badia, J. Walker, T. Scholtes, J. Mitrovic, and C. Blundell. Coberl: Contrastive\nbert for reinforcement learning. arXiv, 2107.05431, 2022.\n[9] D. Bau, A. Andonian, A. Cui, Y Park, A. Jahanian, A. Oliva, and A. Torralba. Paint by word.\nArXiv, 2103.10951, 2021.\n[10] D.P. Bertsekas and D.A. Castanon. Adaptive aggregation methods for infinite horizon dynamic\nprogramming. IEEE Transactions on Automatic Control, 34(6):589–598, 1989. doi: 10.1109/\n9.24227.\n[11] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.\nOpenai gym. ArXiv, 2016.\n[12] Vittorio Caggiano, Guillaume Durandau, Huwawei Wang, Alberto Chiappa, Alexander\nMathis, Pablo Tano, Nisheet Patel, Alexandre Pouget, Pierre Schumacher, Georg Martius,\nDaniel Haeufle, Yiran Geng, Boshi An, Yifan Zhong, Jiaming Ji, Yuanpei Chen, Hao Dong,\nYaodong Yang, Rahul Siripurapu, Luis Eduardo Ferro Diez, Michael Kopp, Vihang Patil,\nSepp Hochreiter, Yuval Tassa, Josh Merel, Randy Schultheis, Seungmoon Song, Massimo\nSartori, and Vikash Kumar. Myochallenge 2022: Learning contact-rich manipulation us-\ning a musculoskeletal hand. In Marco Ciccone, Gustavo Stolovitzky, and Jacob Albrecht,\neditors, Proceedings of the NeurIPS 2022 Competitions Track, volume 220 of Proceed-\nings of Machine Learning Research, pages 233–250. PMLR, 28 Nov–09 Dec 2022. URL\nhttps://proceedings.mlr.press/v220/caggiano23a.html.\n10\n[13] B. Caputo and H. Niemann. Storage capacity of kernel associative memories. In Proceedings\nof the International Conference on Artificial Neural Networks (ICANN), page 51–56, Berlin,\nHeidelberg, 2002. Springer-Verlag.\n[14] N. Carlini and A. Terzis. Poisoning and backdooring contrastive learning. ArXiv, 2106.09667,\n2021.\n[15] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 9912–9924. Curran Associates, Inc., 2020.\n[16] P. S. Castro. Scalable methods for computing state similarity in deterministic Markov decision\nprocesses. ArXiv, 1911.09291, 2019.\n[17] H. H. Chen, Y. C. Lee, G. Z. Sun, H. Y. Lee, T. Maxwell, and C. Lee Giles. High order\ncorrelation model for associative memory. AIP Conference Proceedings, 151(1):86–99, 1986.\ndoi: 10.1063/1.36224.\n[18] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\nof visual representations. In H. Daumé and A. Singh, editors, Proceedings of the International\nConference on Machine Learning (ICML), pages 1597–1607, 2020.\n[19] X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the\nConference on Computer Vision and Pattern Recognition (CVPR), pages 15750–15758, 2021.\n[20] M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for\ngymnasium, 2018. URL https://github.com/Farama-Foundation/Minigrid.\n[21] G. Comanici and D. Precup. Optimal policy switching algorithms for reinforcement learning.\nIn Proceedings of the International Joint Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), volume 2, pages 709–714, 2010. doi: 10.1145/1838206.1838300.\n[22] C. Daniel, H. vanHoof, J. Peters, and G. Neumann.\nProbabilistic inference for deter-\nmining options in reinforcement learning. Machine Learning, 104, 2016. doi: 10.1007/\ns10994-016-5580-x.\n[23] F. Deng, I.Jang, and S. Ahn. Dreamerpro: Reconstruction-free model-based reinforcement\nlearning with prototypical representations. arXiv, 2021.\n[24] B. Devillers, R. Bielawski, B. Choski, and R. VanRullen. Does language help generalization\nin vision models? ArXiv, 2104.08313, 2021.\n[25] T. G. Dietterich. An experimental comparison of three methods for constructing ensembles of\ndecision trees: Bagging, boosting, and randomization. Machine Learning, 40(2):1, 2000.\n[26] Youssef Diouane, Aurelien Lucchi, and Vihang Prakash Patil. A globally convergent evolu-\ntionary strategy for stochastic constrained optimization with applications to reinforcement\nlearning. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings\nof The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of\nProceedings of Machine Learning Research, pages 836–859. PMLR, 28–30 Mar 2022. URL\nhttps://proceedings.mlr.press/v151/diouane22a.html.\n[27] B. Eysenbach, T. Zhang, R. Salakhutdinov, and S. Levine. Contrastive learning as goal-\nconditioned reinforcement learning. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors,\nAdvances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id=vGQiU5sqUe3.\n[28] H. Fang, P. Xiong, L. Xu, and Y. Chen. CLIP2Video: Mastering video-text retrieval via image\nCLIP. ArXiv, 2106.11097, 2021.\n[29] N. Ferns, P. S. Castro, D. Precup, and P. Panangaden. Methods for computing state similarity\nin Markov decision processes. ArXiv, 1206.6836, 2012.\n11\n[30] K. Frans, L. B. Soros, and O. Witkowski. CLIPDraw: Exploring text-to-drawing synthesis\nthrough language-image encoders. ArXiv, 2106.14843, 2021.\n[31] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven\nreinforcement learning, 2020.\n[32] A. Fürst, E. Rumetshofer, J. Lehner, V. Tran, F. Tang, H. Ramsauer, D. Kreil, M. Kopp,\nG. Klambauer, Q. Bitto-Nemling, and S. Hochreiter. CLOOB: modern Hopfield networks with\nInfoLOOB outperform CLIP. ArXiv, 2110.11316, 2021.\n[33] F. A. Galatolo, M. G. C. A. Cimino, and G. Vaglini. Generating images from caption and vice\nversa via CLIP-guided generative latent space search. ArXiv, 2102.01645, 2021.\n[34] T. Gao, X. Yao, and D. Chen. SimCSE: Simple contrastive learning of sentence embeddings.\nArXiv, 2104.08821, 2021.\n[35] E. Gardner. Multiconnected neural network models. Journal of Physics A, 20(11):3453–3464,\n1987. doi: 10.1088/0305-4470/20/11/046.\n[36] R. Givan, T. Dean, and M. Greig. Equivalence notions and model minimization in Markov de-\ncision processes. Artificial Intelligence, 147(1):163–223, 2003. doi: 10.1016/S0004-3702(02)\n00376-4.\n[37] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch,\nB. Ávila Pires, Z. D. Guo, M. Gheshlaghi Azar, B. Piot, K. Kavukcuoglu, R. Munos, and\nM. Valko. Bootstrap your own latent - a new approach to self-supervised learning. In\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural\nInformation Processing Systems (NeurIPS), pages 21271–21284, 2020.\n[38] W. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. P. Mohanty, D. P.\nLiebana, R. Salakhutdinov, N. Topin, M. Veloso, and P. Wang. The MineRL competition on\nsample efficient reinforcement learning using human priors. arXiv, 2019.\n[39] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov.\nMineRL: A large-scale dataset of Minecraft demonstrations. In Proc. of the 28th Int. Joint\nConf. on Artificial Intelligence (IJCAI’19), 2019. URL http://minerl.io.\n[40] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent\nimagination. arXiv, abs/1912.01603, 2019.\n[41] T. Han, W. Xie, and A. Zisserman. Self-supervised co-training for video representation learning.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in\nNeural Information Processing Systems (NeurIPS), pages 5679–5690, 2020.\n[42] L. Hao and P. Abbeel. Aps: Active pretraining with successor features. In In Advances in\nNeural Information Processing Systems, page 34:18459–73, 2021.\n[43] L. Hao and P. Abbeel. Behavior from the void: Unsupervised active pre-training. In In\nProceedings of the 38th International Conference on Machine Learning, PMLR, page 6736–47,\n2021.\n[44] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau,\nE. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk,\nM. Brett, A. Haldane, J. F. del Río, M. Wiebe, P. Peterson, P. Gérard-Marchant, K. Sheppard,\nT. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with\nNumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2.\n[45] K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick. Momentum contrast for unsupervised visual\nrepresentation learning. In Proceedings of the Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020.\n[46] O. J. Hénaff, A. Srinivas, J. DeFauw, A. Razavi, C. Doersch, S. M. A. Eslami, and A. vanDenO-\nord. Data-efficient image recognition with contrastive predictive coding. ArXiv, 1905.09272,\n2019.\n12\n[47] M. L. Henderson, R. Al-Rfou, B. Strope, Y.-H. Sung, L. Lukács, R. Guo, S. Kumar, B. Miklos,\nand R. Kurzweil. Efficient natural language response suggestion for smart reply. ArXiv,\n1705.00652, 2017.\n[48] J. J. Hopfield. Neural networks and physical systems with emergent collective computational\nabilities. Proceedings of the National Academy of Sciences, 79(8):2554–2558, 1982.\n[49] J. J. Hopfield. Neurons with graded response have collective computational properties like\nthose of two-state neurons. Proceedings of the National Academy of Sciences, 81(10):3088–\n3092, 1984. doi: 10.1073/pnas.81.10.3088.\n[50] D. Horn and M. Usher. Capacities of multiconnected memory models. Journal of Physics\nFrance, 49(3):389–395, 1988. doi: 10.1051/jphys:01988004903038900.\n[51] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9\n(3):90–95, 2007. doi: 10.1109/MCSE.2007.55.\n[52] I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep\nreinforcement learning from pixels. ArXiv, 2004.13649, 2020.\n[53] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis,\nDeptartment of Computer Science, University of Toronto, 2009.\n[54] D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. In D. D.\nLee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems (NeurIPS), pages 1172–1180, 2016.\n[55] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and Josh J. Tenenbaum. Hierarchical deep re-\ninforcement learning: Integrating temporal abstraction and intrinsic motivation. In D. Lee,\nM. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 29, pages 3675–3683. Curran Associates, Inc., 2016.\n[56] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning\nwith augmented data. ArXiv, 2004.14990, 2020.\n[57] Y. LeCun. A path towards autonomous machine intelligence. Openreview, 2022. URL\nhttps://openreview.net/forum?id=BZ5a1r-kVsf.\n[58] K. Y. Levy and N. Shimkin. Unified inter and intra options learning using policy gradient\nmethods. In S. Sanner and M. Hutter, editors, Recent Advances in Reinforcement Learning,\npages 153–164. Springer Berlin Heidelberg, 2012. doi: 10.1007/978-3-642-29946-9_17.\n[59] L. Li, T. J. Walsh, and M. L. Littman. Towards a unified theory of state abstraction for MDPs.\nIn Ninth International Symposium on Artificial Intelligence and Mathematics (ISAIM), 2006.\n[60] H. Liu, T. Zahavy, V. Mnih, and S. Singh. Palm up: Playing in the latent manifold for\nunsupervised pretraining. arXiv, abs/2210.10913, 2022.\n[61] L. Logeswaran and H. Lee. An efficient framework for learning sentence representations. In\nInternational Conference on Learning Representations (ICLR). OpenReview, 2018.\n[62] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li. CLIP4Clip: An empirical study\nof CLIP for end to end video clip retrieval. ArXiv, 2104.08860, 2021.\n[63] D. J. Mankowitz, T. A. Mann, and S. Mannor. Adaptive skills adaptive partitions (ASAP).\nIn D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 29, pages 1588–1596. Curran Associates, Inc., 2016.\n[64] T. McInroe, L. Schäfer, and S. Albrecht. Learning temporally-consistent representations for\ndata-efficient reinforcement learning. arxiv, abs/2110.04935, 2021.\n[65] T. Milbich, K. Roth, S. Sinha, L. Schmidt, M. Ghassemi, and B. Ommer. Characterizing\ngeneralization under out-of-distribution shifts in deep metric learning. ArXiv, 2107.09562,\n2021.\n13\n[66] J. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon,\nand L. Schmidt. Accuracy on the line: On the strong correlation between out-of-distribution\nand in-distribution generalization. ArXiv, 2107.04649, 2021.\n[67] I. Misra and L. vanDerMaaten. Self-supervised learning of pretext-invariant representations.\nIn Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[68] M. Narasimhan, A. Rohrbach, and T. Darrell. CLIP-It! Language-guided video summarization.\nArXiv, 2107.00650, 2021.\n[69] F. Paischer, T. Adler, V. Patil, A. Bitto-Nemling, M. Holzleitner, S. Lehner, H. Eghbal-Zadeh,\nand S. Hochreiter. History compression via language models in reinforcement learning. In\nK. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings\nof the 39th International Conference on Machine Learning, pages 17156–17185, 2022.\n[70] D. Pakhomov, S. Hira, N. Wagle, K. E. Green, and N. Navab. Segmentation in style: Unsuper-\nvised semantic image segmentation with stylegan and CLIP. ArXiv, 2107.12518, 2021.\n[71] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-\njani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative\nstyle, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 32, pages 8024–8035. Curran Associates, Inc., 2019.\n[72] Vihang Patil, Markus Hofmarcher, Marius-Constantin Dinu, Matthias Dorfer, Patrick M\nBlies, Johannes Brandstetter, José Arjona-Medina, and Sepp Hochreiter. Align-RUDDER:\nLearning from few demonstrations by reward redistribution. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of\nthe 39th International Conference on Machine Learning, volume 162 of Proceedings of\nMachine Learning Research, pages 17531–17572. PMLR, 17–23 Jul 2022. URL https:\n//proceedings.mlr.press/v162/patil22a.html.\n[73] D. Psaltis and H. P. Cheol. Nonlinear discriminant functions and associative memories. AIP\nConference Proceedings, 151(1):370–375, 1986. doi: 10.1063/1.36241.\n[74] M. L. Puterman. Markov Decision Processes. John Wiley & Sons, Inc., 2005. ISBN 978-0-\n471-72782-8.\n[75] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from\nnatural language supervision. In Proceedings of the International Conference on Machine\nLearning (ICML), 2021.\n[76] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from\nnatural language supervision. ArXiv, 2103.00020, 2021.\n[77] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah\nDormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of\nMachine Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/\n20-1364.html.\n[78] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional\nimage generation with CLIP latents. ArXiv, 2204.06125, 2022. URL https://openai.com/\ndall-e-2/.\n[79] H. Ramsauer, B. Schäfl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, M. Pavlovi´c,\nG. K. Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter.\nHopfield networks is all you need. ArXiv, 2008.02217, 2020.\n14\n[80] H. Ramsauer, B. Schäfl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, M. Pavlovi´c,\nG. K. Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter.\nHopfield networks is all you need. In International Conference on Learning Representations\n(ICLR). OpenReview, 2021.\n[81] B. Ravindran and A. G. Barto. SMDP homomorphisms: An algebraic approach to abstraction in\nsemi-Markov decision processes. In Proc. of the 18th Int. Joint Conf. on Artificial Intelligence\n(IJCAI’03), pages 1011–1016, San Francisco, CA, USA, 2003. Morgan Kaufmann Publishers\nInc.\n[82] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10684–10695, 2022.\n[83] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. ArXiv, 2018.\n[84] M. Schwarzer, A. Anand, R. Goel, D. Hjelm, A. Courville, and P. Bachman. Data-efficient\nreinforcement learning with self-predictive representations. arXiv, 2007.05929, 2020. URL\nhttps://arxiv.org/abs/2007.05929.\n[85] M. Schwarzer, N. Rajkumar, M. Noukhovitch, A. Anand, L. Charlin, D. Hjelm, P. Bachman,\nand A. Courville. Pretraining representations for data-efficient reinforcement learning. arXiv,\n2106.04799, 2021. URL https://arxiv.org/abs/2106.04799.\n[86] Kajetan Schweighofer, Marius-constantin Dinu, Andreas Radler, Markus Hofmarcher, Vi-\nhang Prakash Patil, Angela Bitto-nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter. A\ndataset perspective on offline reinforcement learning. In Sarath Chandar, Razvan Pascanu,\nand Doina Precup, editors, Proceedings of The 1st Conference on Lifelong Learning Agents,\nvolume 199 of Proceedings of Machine Learning Research, pages 470–517. PMLR, 22–24\nAug 2022. URL https://proceedings.mlr.press/v199/schweighofer22a.html.\n[87] P. Seidl, P. Renz, N. Dyubankova, P. Neves, J. Verhoeven, J. K. Wegner, M. Segler, S. Hochre-\niter, and G. Klambauer. Improving few-and zero-shot reaction template prediction using\nmodern Hopfield networks. Journal of Chemical Information and Modeling, 2022.\n[88] S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K.-W. Chang, Z. Yao, and K. Keutzer.\nHow much can CLIP benefit vision-and-language tasks? ArXiv, 2107.06383, 2021.\n[89] D. Silver and K. Ciosek. Compositional planning using optimal option models. In Proceedings\nof the 29th International Conference on Machine Learning (ICML), volume 2, 2012. arXiv\n1206.6473.\n[90] Rahul Siripurapu, Vihang Prakash Patil, Kajetan Schweighofer, Marius-Constantin Dinu,\nThomas Schmied, Luis Eduardo Ferro Diez, Markus Holzleitner, Hamid Eghbal-zadeh,\nMichael K Kopp, and Sepp Hochreiter. InfODist: Online distillation with informative rewards\nimproves generalization in curriculum learning. In Deep Reinforcement Learning Workshop\nNeurIPS 2022, 2022. URL https://openreview.net/forum?id=9CvMkA8oi8O.\n[91] A. Srinivas, M. Laskin, and P. Abbeel. Curl: Contrastive unsupervised representations for\nreinforcement learning. arXiv, 2004.04136, 2020.\n[92] Christian Alexander Steinparz, Thomas Schmied, Fabian Paischer, Marius-constantin Dinu,\nVihang Prakash Patil, Angela Bitto-nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter.\nReactive exploration to cope with non-stationarity in lifelong reinforcement learning. In Sarath\nChandar, Razvan Pascanu, and Doina Precup, editors, Proceedings of The 1st Conference\non Lifelong Learning Agents, volume 199 of Proceedings of Machine Learning Research,\npages 441–469. PMLR, 22–24 Aug 2022. URL https://proceedings.mlr.press/v199/\nsteinparz22a.html.\n[93] M. Stolle and D. Precup. Learning options in reinforcement learning. In Lecture Notes in\nComputer Science, volume 2371, pages 212–223, 2002. doi: 10.1007/3-540-45622-8_16.\n15\n[94] A. Stooke, L. Kimin, P. Abbeel, and M. Laskin. Decoupling representation learning from\nreinforcement learning. arXiv, 2009.08319, 2021.\n[95] R. S. Sutton, D. Precup, and S. P. Singh. Between MDPs and Semi-MDPs: A framework\nfor temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181–211,\n1999.\n[96] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033.\nIEEE, 2012.\n[97] Unknown. Stable diffusion repository on github. Github, 2022. URL https://github.com/\nCompVis/stable-diffusion.\n[98] A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive\ncoding. ArXiv, 1807.03748, 2018.\n[99] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and\nK. Kavukcuoglu.\nFeUdal networks for hierarchical reinforcement learning.\narXiv,\nabs/1703.01161, 2017.\n[100] H. Wang, E. Miahi, M. White, M. C. Machado, Z. Abbasr, R. Kumaraswamy, V. Liu, and\nA. White. Investigating the properties of neural network representations in reinforcement\nlearning. ArXiv, 2203.15955, 2022.\n[101] M. Widrich, B. Schäfl, M. Pavlovi´c, H. Ramsauer, L. Gruber, M. Holzleitner, J. Brandstetter,\nG. K. Sandve, V. Greiff, S. Hochreiter, and G. Klambauer. Modern Hopfield networks and\nattention for immune repertoire classification. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems (NeurIPS),\npages 18832–18845, 2020.\n[102] M. Widrich, M. Hofmarcher, V. P. Patil, A. Bitto-Nemling, and S. Hochreiter. Modern Hopfield\nnetworks for return decomposition for delayed rewards. In Deep RL Workshop at NeurIPS\n2021, 2021. URL https://openreview.net/forum?id=t0PQSDcqAiy.\n[103] M. Wortsman, G. Ilharco, M. Li, J. W. Kim, H. Hajishirzi, A. Farhadi, H. Namkoong, and\nL. Schmidt. Robust fine-tuning of zero-shot models. ArXiv, 2109.01903, 2021.\n[104] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In Proceedings of the Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3733–3742, Los Alamitos, CA, USA, 2018. doi: 10.1109/CVPR.\n2018.00393.\n[105] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Reinforcement learning with prototypical\nrepresentations. arXiv, 2102.11271, 2021. URL https://arxiv.org/abs/2102.11271.\n[106] A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine. Learning invariant representations\nfor reinforcement learning without reconstruction. ArXiv, 2006.10742, 2020.\n[107] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models.\nArXiv, 2109.01134, 2021.\n16\nContents of the Appendix\nA Extended Related Work\n18\nA.1\nContrastive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.2\nContrastive Learning in Reinforcement Learning\n. . . . . . . . . . . . . . . . . .\n18\nA.3 Abstraction in Reinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . .\n19\nB\nContrastive Learning of Sequential Proximity\n19\nB.1\nHyperparameters for Constrastive Learning\n. . . . . . . . . . . . . . . . . . . . .\n19\nB.2\nHyperparameters for Sampling Strategies\n. . . . . . . . . . . . . . . . . . . . . .\n20\nC Abstraction using Modern Hopfield Networks\n20\nC.1\nReview of Modern Hopfield Networks . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.2\nHyperparameters for Hopfield Network\n. . . . . . . . . . . . . . . . . . . . . . .\n21\nC.3\nLearned Beta Training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nD Policy and Planning Experiments\n22\nD.1\nEnvironment Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nD.2\nPolicy experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nD.3\nLearning Policy between fixed points . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nD.4\nPlanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nE\nCompute and Software Libraries\n23\nE.1\nCompute Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nE.2\nSoftware Libraries\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n17\nA\nExtended Related Work\nA.1\nContrastive Learning\nNon-Contrastive learning objectives for self-supervised learning, such as those of BYOL [37] and\nSimSiam [19] do not require negative samples. However, most self-supervised learning methods\nuse contrastive learning. The most popular constrative objective is InfoNCE [98]. InfoNCE pairs an\nanchor sample with a positive sample (positive pair) and then contrasts this pair to pairs of the anchor\nsample with negative samples (negative pairs). InfoNCE has been utilized in transfer learning [46],\naiding natural language response suggestions [47], acquiring sentence representations from unlabelled\ndata [61], and facilitating unsupervised feature learning by maximizing distinctions between instances\n[104]. Furthermore, InfoNCE has been effectively employed for learning visual representations in\nPretext-Invariant Representation Learning (PIRL) [67], Momentum Contrast (MoCo) [45], and\nSimCLR [18]. Given the success of the InfoNCE [98] with sequential data [47, 61], we use it as an\nobjective for contrastive learning. Specifically, we use contrastive learning using the InfoNCE [98]\nobjective.\nUsing contrastive learning for constructing powerful representations is well established. Contrastive\nPredictive Coding (CPC) [98] learns abstract representations in an unsupervised way. The representa-\ntion build by CPC was general enough that it allowed for transfer learning [46]. Contrastive learning\nhas been applied to natural language tasks to learn sentence representations from unlabelled data\n[47, 61] and to unsupervised feature learning [104]. Also, SimCSE learns sentence embeddings [34]\nIn vision, Pretext-Invariant Representation Learning (PIRL) contrasts representations of transformed\nversions of the same image with representations of other images [67]. Momentum Contrast (MoCo)\nwas very successful for unsupervised visual representation learning [45]. SimCLR is one of the\nbest-known contrastive learning methods for visual representations, which was highly effective for\ntransfer learning [18]. Vision applications have been extended to video representation learning [41].\nA milestone in contrastive learning was Contrastive Language-Image Pre-training (CLIP), which\nyielded very impressive results at zero-shot transfer learning [75, 76]. CLIP learns expressive image\nembeddings directly from raw text, thereby leverages a much richer source of supervision than just\nlabels. The representation learned by CLIP are so powerful that a plethora of follow-up publications\nused CLIP’s representations. The CLIP model is used in Vision-and-Language tasks [88]. The\nCLIP model guided generative models via an additional training objective [9, 33, 30] and improved\nclustering of latent representations [70]. It is used in studies of out of distribution performance\n[24, 65, 66], of fine-tuning robustness [103], of zero-shot prompts [107] and of adversarial attacks to\nuncurated datasets [14]. It stirred discussions about more holistic evaluation schemes in computer\nvision [2]. Multiple methods utilize the CLIP model in a straightforward way to perform text-to-video\nretrieval [28, 62, 68]. Latent diffusion models [82] lead to stable diffusion models version 1 [97] and\nversion 2 [97], which are a recent open-source image generation models comparable to proprietary\nmodels such as DALL-E 2 [78]. Stable diffusion model version 1 uses CLIP for textual guiding, while\nversion 2 uses OpenCLIP. The representation learned by CLIP is key to gernerate impressive images\nby stable diffusion models. Stable diffusion models hat high impact on the scientific community as\nwell as in the public.\nA.2\nContrastive Learning in Reinforcement Learning\nCURL[91] learns representation on raw pixels by bringing closer the representation of a state and\nits augmented version. CoBerl[8] uses contrastive learning to learn representations by using the\nmasked prediction objective of Bert in the time domain for reinforcement learning. [27] attempt\nto move the representation of a state-action closer to another state if it is present in the future and\naway from another state if it is not present in the future. Some methods use contrastive learning\nduring an unsupervised phase before online learning. APS [42] and ATS [43] learn representations\nby maximizing entropy during an unsupervised phase and fine-tuning for a subsequent task. ATC\n[94] brings closer the representation of a state to another state which is k steps in the future. SPR\n[84, 85], KSL [64] and [4] predict the representation of k future steps and bring it closer to the\nactual representation. DreamerPro [23] combines the ideas from Dreamer [40] and Swav [15] to do\nModel-Based Reinforcement learning without reconstruction. Proto-RL [105] tries similarly to use\nideas from Swav to first explore the environment in pre-training to lean representation from diverse\nsamples and then fine-tunes on downstream task. Palm Up [60] connect large-scale vision models to\nreinforcement learning by using them as an environment to learn representations.\n18\nA.3\nAbstraction in Reinforcement Learning\nAbstraction in RL is a well-studied area [10, 36, 6, 81, 95, 59, 22, 5, 99, 55]. Bisimulation [36, 29,\n16, 106] tries to simplify MDPs by identifying states which lead to similar behaviors (bisimilar states).\nMethods based on bisimulation learn abstractions that are dependent on a particular reward function.\nContrastive abstraction learning enables the identification of sub-goals and sub-tasks, thus making it\nrelevant to hierarchical reinforcement learning (HRL) approaches such as the option framework [95],\nthe MAXQ framework [25], and the recursive composition of option models [89]. Several approaches\nhave been proposed to tackle the problem of learning good options. One approach is selecting\nfrequently observed solution states as targets [93]. Another strategy involves employing gradient-\nbased techniques to enhance the termination function for options [21, 63, 58]. [58] employed policy\ngradient optimization to learn a unified policy comprising intra-option policies, option termination\nconditions, and an option selection policy. Parametrized options can be learned by treating the\ntermination functions as hidden variables and applying expectation maximization [22]. Intrinsic\nrewards have also been utilized to learn policies within options, while extrinsic rewards are used\nto learn the policy over options [55]. [5] proposed a method that jointly learns options and their\nassociated policies using the policy gradient theorem. Additionally, a manager module operating on a\nslow time scale has been introduced to learn sub-goals, which are subsequently achieved by a worker\nmodule operating on a fast time scale [99].\nB\nContrastive Learning of Sequential Proximity\nB.1\nHyperparameters for Constrastive Learning\nIn the maze experiments, a four-layer fully connected network was employed, comprising of 256,\n128, 64, and 32 neurons in each respective layer. The network utilized ReLU activations. The τ value\nwas set to 30 for 1. A batch size of 4096 was used, and the training process consisted of 1000 epochs.\nThe learning rate was set to 1e-3, and a weight decay of 1e-5 was applied.\nIn the CIFAR environment experiments, a network architecture consisting of three CNN layers\nfollowed by three fully connected layers was utilized. The CNN layers had output channels of 64, 32,\nand 32, with kernel sizes of 4, 4, and 3 respectively. A stride and padding value of 2 was applied\nthroughout. The fully connected layers had sizes of 256, 64, and 8 neurons respectively, employing\nReLU activations. The τ value was set to 30 for 1. A batch size of 4096 was used, and the training\nprocess spanned 8000 epochs. The learning rate was annealed using cosine annealing, starting from\n1e-3 and gradually decreasing to a minimum value of 1e-6. A weight decay of 1e-5 was applied.\nIn the MiniGrid experiments, a network architecture comprising of three CNN layers followed by\ntwo fully connected layers was employed. The CNN layers had output channels of 64 and 32, with\nkernel sizes of 4, 4, and 2 respectively. Stride and padding values of 2 were uniformly used. The\nfully connected layers had sizes of 64 and 32 neurons respectively, employing ReLU activations.\nFollowing these layers, an LSTM layer and another fully connected layer were added, both with an\noutput size of 32. The τ value was set to 30 for 1. A batch size of 4096 was used, and the training\nprocess spanned 8000 epochs. The learning rate was annealed using cosine annealing, starting from\n1e-3 and decreasing gradually to a minimum value of 1e-6. Additionally, a weight decay of 1e-5 was\napplied.\nIn the Minecraft experiments, the network architecture involves processing a sequence of the 32 most\nrecent frames as input. The first stage of the network consists of four batch-normalized convolution\nlayers with ReLU activation functions. These layers are structured as follows: Conv-Layer-1 with 16\nfeature maps, a kernel size of 4, a stride of 2, and zero padding of 1; Conv-Layer-2 with 32 feature\nmaps, a kernel size of 4, a stride of 2, and zero padding of 1; Conv-Layer-3 with 64 feature maps, a\nkernel size of 3, and a stride of 2; and Conv-Layer-4 with 32 feature maps, a kernel size of 3, and a\nstride of 2. The resulting flattened latent representation (dimension: R32×288) from the convolution\nstage is then fed into an LSTM layer with 256 units for further processing. The inverse tau value\nis set to 30 for the contrastive loss. A batch size of 1024 is used, and the training process involves\n300 epochs. The learning rate is annealed using cosine annealing, starting from 1e-3 and gradually\ndecreasing to a minimum value of 1e-6. Additionally, a weight decay of 1e-5 is applied.\n19\nB.2\nHyperparameters for Sampling Strategies\nFor the Gaussian sampling strategy, standard deviation (σ) of 15 is used for all experiments. For\nexponential distribution, we use a γ of 0.99 for all experiments.\nC\nAbstraction using Modern Hopfield Networks\nC.1\nReview of Modern Hopfield Networks\nWe briefly review continuous MHNs that are used for deep learning architectures. MHNs [80] map\nstates to fixed points that correspond to abstract states. Hopfield networks, introduced in the 1980s,\nare binary associative memories that played a significant role in popularizing artificial neural networks\n[3, 48, 49]. These networks were designed to store and retrieve samples [17, 73, 7, 35, 1, 50, 13, 54].\nSince they exhibit continuity and differentiability, these functions are compatible with gradient descent\nin deep architectures. As they can be updated in a single step, they can be activated similarly to other\nlayers in deep learning. With their exponential storage capacity, they are capable of handling large-\nscale problems effectively. In contrast to traditional binary memory networks, we employ continuous\nassociative memory networks with exceptionally high storage capacity. These MHNs, utilized in\ndeep learning architectures, have an energy function with continuous states and possess the ability to\nretrieve samples with just a single update [80]. The update rule of MHNs is guaranteed to converge\nto a fixed point. These fixed points correspond to attractors in the network’s energy landscape, and\nthe network dynamics converge towards these attractors when starting at an input pattern. MHNs\nhave already been demonstrated to be successful in diverse fields [101, 87, 102, 69, 32].\nWe assume a set of patterns {u1, . . . , uN} ⊂Rd that are stacked as columns to the matrix U =\n(u1, . . . , uN) and a state pattern (query) ξ ∈Rd that represents the current state. The largest norm\nof a stored pattern is M = maxi ∥ui∥. Continuous MHNs with state ξ have the energy\nE = −β−1 log\n N\nX\ni=1\nexp(βuT\ni ξ)\n!\n+ β−1 log N + 1\n2 ξT ξ + 1\n2 M 2 .\n(A1)\nFor energy E and state ξ, the update rule\nξnew = f(ξ; U, β) = U p = U softmax(βU T ξ)\n(A2)\nhas been proven to converge globally to stationary points of the energy E, which are almost always\nlocal minima [79, 80]. The update rule Eq. (A2) is also the formula of the well-known transformer\nattention mechanism [79, 80], therefore Hopfield retrieval and transformer attention coincide.\nThe separation ∆i of a pattern ui is defined as its minimal dot product difference to any of the\nother patterns: ∆i = minj,j̸=i\n\u0000uT\ni ui −uT\ni uj\n\u0001\n. A pattern is well-separated from the data if\n∆i ≥\n2\nβN + 1\nβ log\n\u00002(N −1)NβM 2\u0001\n.\nIn the case where the patterns ui exhibit good separation, the iteration defined by Eq. (A2) converges\nto a stable point that is in close proximity to one of the stored patterns. However, if certain patterns\nare similar to each other and lack clear separation, the update rule Eq. (A2) converges to a stable point\nthat is close to the mean of those similar patterns. This particular fixed point represents a metastable\nstate of the energy function, effectively averaging over the similar patterns.\nAccording to the next theorem, when the patterns are well separated, the update rule defined by\nEq. (A2) typically achieves convergence within a single update. Additionally, the theorem asserts\nthat the retrieval error is exponentially small with respect to the separation ∆i between the patterns.\nTheorem A1 (Modern Hopfield Networks: Retrieval with One Update [80]). With query ξ, after\none update the distance of the new point f(ξ) to the fixed point u∗\ni is exponentially small in the\nseparation ∆i. The precise bounds using the Jacobian J = ∂f(ξ)\n∂ξ\nand its value Jm in the mean value\ntheorem are:\n∥f(ξ) −u∗\ni ∥⩽∥Jm∥2 ∥ξ −u∗\ni ∥,\n(A3)\n∥Jm∥2 ⩽2 β N M 2 (N −1) exp(−β (∆i −2 max{∥ξ −ui∥, ∥u∗\ni −ui∥} M)) .\n(A4)\nFor given ϵ and sufficient large ∆i, we have ∥f(ξ) −u∗\ni ∥< ϵ, that is, retrieval with one update.\nThe retrieval error ∥f(ξ) −ui∥of pattern ui is bounded by\n∥f(ξ) −ui∥⩽2 (N −1) exp(−β (∆i −2 max{∥ξ −ui∥, ∥u∗\ni −ui∥} M)) M . (A5)\n20\nFor a proof see [79, 80].\nOur objective is to store a potentially extensive collection of embeddings. To accomplish this, we\ninitially establish the concept of storing and retrieving patterns within a contemporary Hopfield\nnetwork.\nDefinition A1 (Pattern Stored and Retrieved [80]). We assume that around every pattern ui a sphere\nSi is given. We say ui is stored if there is a single fixed point u∗\ni ∈Si to which all points ξ ∈Si\nconverge, and Si ∩Sj = ∅for i ̸= j. We say ui is retrieved for a given ϵ if iteration (update rule)\nEq. (A2) gives a point ˜xi that is at least ϵ-close to the single fixed point u∗\ni ∈Si. The retrieval error\nis ∥˜xi −ui∥.\nSimilar to classical Hopfield networks, we focus on patterns that lie on a sphere, meaning patterns\nwith a constant norm. In the case of randomly selected patterns, the number of patterns that can be\nstored increases exponentially with the dimensionality d of the pattern space (ui ∈Rd).\nTheorem A2 (Modern Hopfield Networks: Exponential Storage Capacity [80]). We assume a failure\nprobability 0 < p ⩽1 and randomly chosen patterns on the sphere with radius M := K\n√\nd −1. We\ndefine a :=\n2\nd−1(1 + ln(2βK2p(d −1))), b := 2K2β\n5\n, and c :=\nb\nW0(exp(a+ln(b)), where W0 is the\nupper branch of the Lambert W function 4.13, and ensure c ≥\n\u0010\n2\n√p\n\u0011\n4\nd−1 . Then with probability\n1 −p, the number of random patterns that can be stored is\nN ≥√p c\nd−1\n4\n.\n(A6)\nTherefore it is proven for c ≥3.1546 with β = 1, K = 3, d = 20 and p = 0.001 (a + ln(b) > 1.27)\nand proven for c ≥1.3718 with β = 1, K = 1, d = 75, and p = 0.001 (a + ln(b) < −0.94).\nFor a proof see [79, 80].\nThe validity of employing continuous modern Hopfield networks in substituting retrieved embeddings\ninstead of the original embeddings for large batch sizes is supported by this theorem. Even with a\nsubstantial number of embeddings, reaching into the hundreds of thousands, the continuous modern\nHopfield network demonstrates its capability to retrieve the embeddings, provided that the dimension\nof the embeddings is sufficiently large.\nC.2\nHyperparameters for Hopfield Network\nFor the fixed β visualisation experiments Fig 3 on Maze2D environment, we use β values of 20, 25\nand 35.\nFor the fixed β visualisation experiments, Fig 3 on RedBlueDoor environment, we use β values of\n100, 150 and 200 (left to right in Fig 3.\nFor the fixed β visualisation experiments, Fig 3 on Minecraft environment, we use β values of 30, 35\nand 50.\nWe currently do not have any learnable weights in the hopfield, but a hopfield network can be trained\nwith learnable weights also.\nC.3\nLearned Beta Training\nFor all our experiments we use a single fully connected network as the β network. This β network\nhas a sigmoid output neuron, which predicts a value between 0 and 1. Further, we multiply this output\nwith βmax, which is the maximum value the β can have. Thus, bounding the β value. For all our\nexperiments we use the βmax value of 200.\nWe use a single fully connected network after hopfield network. In the experiment, the learning rate\nwas set to 1e-3. A weight decay of 1e-5 was applied to control the regularization of the model’s\nweights. The training process spanned a total of 1000 epochs, with each epoch representing a\ncomplete iteration through the dataset. Furthermore, a masking ratio of 0.3 was employed, indicating\nthe proportion of the input or data that was randomly masked or hidden during training.\n21\nD\nPolicy and Planning Experiments\nD.1\nEnvironment Details\nCifarEnv\nis an environment consisting of images and classes from the CIFAR-100 [53] dataset.\nThis environment is designed to verify the abstract representation as the true number of abstract states\nand their relation is known. We select 10 classes and define a transition structure for various goals and\ntasks (see Figure 6(a)). For example, in GoSchool the agent starts in state bed and receives reward if\nit reaches the state school. Other tasks include, GoMarket, GoOffice, GoHouse. The agent stays in the\nsame state for n timesteps (where n = 8 for our experiments) in this environment, and is presented\nwith an image sampled from the respective class in each timestep. After n timesteps the agent then\nselects an action in order to transition to the next state. For example, if the agent is in the state house\nthen n different images of this class are sampled from CIFAR-100 sequentially. We randomly sample\n100, 000 timesteps from this environment and build a dataset for constrastive abstraction learning.\nMaze2D-large\nis an environment from Mujoco Suite [96]. A robot must navigate to a given goal\nstate from a random starting position. The state is an 4-dimensional vector and the action space is\ncontinuous, controlling the agents directional movement. We use trajectories from the D4RL dataset\n[31] for our experiments.\nRedBlueDoor\nis part of the Minigrid library [20] and is a partially observable environment. The\nagent has access to a partial view of the environment and can select from 7 discrete actions. We\ngenerate a dataset by storing all interactions during training of a policy with Proximal Policy\nOptimization (PPO) [83].\nMinecraft\nis an environment based on the popular sandbox game Minecraft. The environment\nwas released together with a dataset of demonstrations from human players performing various tasks\nfor the MineRL competition [38]. Several tasks, such as mining a diamond, were defined for the\ncompetition. We use the environment and the dataset of human demonstrations released for the\nMineRL competition [38].\nD.2\nPolicy experiments\nWe use the same PPO hyperparameters across all the methods. The clip coefficient was set to 0.2,\nwhich limits the size of the policy update to prevent drastic changes. The entropy coefficient was set\nto 0.0001, encouraging exploration by adding an entropy term to the objective function. The learning\nrate was set to 1e-5, indicating the step size used in the optimization process. There was no annealing\napplied to the learning rate or clipping parameter throughout the training process. Additionally,\nto mitigate the impact of large gradients, the gradient was clipped if its norm exceeded 0.5. The\nexperiments were averaged over 10 different seeds.\nD.3\nLearning Policy between fixed points\nWe learn policy between fixed points using behavior cloning. We first map each state to the cor-\nresponding fixed point. Further, we pair the state with the next fixed point in the sequence, thus\nobtaining a dataset where each sample has the following information, (st, ξt, ξt′, at). Further, we\ncondition the policy network with the current state st and future fixed point ξt′ and predict the action\nat in the dataset.\nThese policy between fixed points are used for both MetaPolicy and Planning experiments.\nD.4\nPlanning\nFor the CIFAR experiments, we use a threshold of 0.68 to prune the connections in the graph. For\nMaze2D experiments, we use a threshold of 0.6. We also remove self-connection by removing most\nsimilar connections to the current node using another threshold (0.97, 0.9 respectively). Instead of\nusing a threshold, we can also use the dataset to prune connections. We can look for consecutive\nfixed points in the entire dataset and remove connections which were not found.\n22\nE\nCompute and Software Libraries\nE.1\nCompute Details\nFor all experiments, during development 2 to 3 nodes each with 4 GPUs of an internal GPU cluster\nwere used for roughly six months of GPU compute time (Nvidia A100).\nE.2\nSoftware Libraries\nWe are thankful towards the developers of Stable-Baselines [77], PyTorch [71], OpenAI Gym [11],\nNumpy [44], Matplotlib [51] and Minecraft [39]. We also used code from [72].\n23\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-10-01",
  "updated": "2024-10-01"
}