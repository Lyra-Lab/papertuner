{
  "id": "http://arxiv.org/abs/2410.01431v1",
  "title": "Scalable Reinforcement Learning-based Neural Architecture Search",
  "authors": [
    "Amber Cassimon",
    "Siegfried Mercelis",
    "Kevin Mets"
  ],
  "abstract": "In this publication, we assess the ability of a novel Reinforcement\nLearning-based solution to the problem of Neural Architecture Search, where a\nReinforcement Learning (RL) agent learns to search for good architectures,\nrather than to return a single optimal architecture. We consider both the\nNAS-Bench-101 and NAS- Bench-301 settings, and compare against various known\nstrong baselines, such as local search and random search. We conclude that our\nReinforcement Learning agent displays strong scalability with regards to the\nsize of the search space, but limited robustness to hyperparameter changes.",
  "text": "Scalable Reinforcement Learning-based Neural\nArchitecture Search\nAmber Cassimon1*, Siegfried Mercelis1 and Kevin Mets1\n1*IDLab - Faculty of Applied Engineering, University of Antwerp - imec,\nSint-Pietersvliet 7, Antwerp, 2000, Antwerp, Belgium.\n*Corresponding author(s). E-mail(s): amber.cassimon@uantwerpen.be;\nContributing authors: siegfried.mercelis@uantwerpen.be;\nkevin.mets@uantwerpen.be;\nAbstract\nIn this publication, we assess the ability of a novel Reinforcement Learning-based\nsolution to the problem of Neural Architecture Search, where a Reinforcement\nLearning (RL) agent learns to search for good architectures, rather than to return\na single optimal architecture. We consider both the NAS-Bench-101 and NAS-\nBench-301 settings, and compare against various known strong baselines, such as\nlocal search and random search. We conclude that our Reinforcement Learning\nagent displays strong scalability with regards to the size of the search space, but\nlimited robustness to hyperparameter changes.\nKeywords: Neural Architecture Search, AutoML, Deep Learning, Reinforcement\nLearning\n1 Introduction\nOver the past decade, deep learning has made great strides in several fields, ranging\nfrom computer vision to natural language processing and RL [13] [5]. Much of this\nsuccess has been driven by the search for better neural network architectures [10]\n[29], which has in turn significantly increased the complexity of state-of-the-art neural\nnetwork architectures. This trend has led to the creation of automated methods for\nfinding optimal neural network architectures.\n1\narXiv:2410.01431v1  [cs.LG]  2 Oct 2024\nThis set of methods is usually referred to as Neural Architecture Search (NAS).\nNAS has been done using a wide variety of techniques, from evolutionary algorithms\n[6] to reinforcement learning [24] and continuous relaxation [17].\nMost NAS methods used today are single-use methods, where an algorithm is\nran once for multiple hours or days, and yields a single architecture. If any of the\nparameters of the search are changed, such as the search space, the target applica-\ntion,. . . the search must be repeated, which can be computationally costly. Typically,\ncomputational costs also scale with the search space, with larger search spaces requir-\ning significantly more computational resources In this publication, we take the first\nstep towards building a NAS system that scales efficiently with the size of the search\nspace, to limit the necessary computational resources.\nWe aim to achieve this by learning a searching behaviour, rather than trying to find\nan optimal architecture for any given problem. In this paper, we focus on the problem\nsetting and the design of the agent. We examine the agent in the NAS-Bench-101 and\nNAS-Bench-301 settings.\nMore precisely, we make the following contributions:\n1. We propose a novel Reinforcement Learning-based NAS methodology based on the\nincremental improvement of neural network architectures\n2. We investigate the effectiveness of our NAS methodology on two established\nbenchmarks: NAS-Bench-101 and NAS-Bench-301\n3. We compare against several known strong baseline algorithms including random\nsearch and local search, as well as several state-of-the-art algorithms.\n2 Related Work\nOver the years, several types of algorithms have been used to attempt to tackle the\nNAS problem. One popular type of NAS algorithm are evolutionary algorithms. They\nhave been used by Real et al. [26], and Elsken et al. [6] to succesfully find architectures\nthat can match or exceed the performance of state-of-the-art hand-designed neural\nnetwork architectures. A more recent example of the evolutionary approach is the\nwork of Hendrickx et al. [11]. They use a modified version of NSGA-Net [18] to opti-\nmize convolutional neural networks to with the aim of steering a nanodrone towards a\nflower so the flower can be pollinated. They modified NSGA-Net to include depthwise\nseparable operations, and started their searches from known, well-performing archi-\ntectures, such as MobileNetV2. Using this approach, they showed that starting from\nknown well-performing architectures can increase search performance, leading to NAS\nalgorithms finding good solutions quicker.\nAnother approach that has proved popular is continuous relaxation. This was first\nintroduced in DARTS [17]. DARTS transforms the discrete set of operations that\ncan be assigned to the edges of a computational graph into a continuous one. This\nallows them to reframe the problem of finding the optimal operation to assign to\neach edge as a bi-level optimization problem, where the first level constitutes the\ntrainable parameters of the underlying neural network, and the second level constitutes\nthe architectural parameters that parameterize the choice of operation for each edge.\nUsing gradient descent, they are able to find strong performing architectures, with a\n2\nreasonable computational budget. DARTS has seen many adaptation in recent years,\nincluding BHE-DARTS [2], Fair DARTS [3], . . .\nMonte-Carlo Tree Search (MCTS) is another approach that has gained popularity\nin recent years [31] [39]. Usually, in these approaches, an architecture is broken down\ninto a sequence of decisions. One decision is made at every level of the search tree,\nuntil a complete architecture has been sampled. A very recent example is GCNAS\n[39], which uses MCTS to find a strong architecture for the semantic segmentation of\nmultispectral LiDAR point clouds.\n2.1 Reinforcement Learning-Based NAS\nNext, we examine several RL-based NAS algorithms in more detail. In their paper\non MetaQNN [1], Baker et al. showcase a tabular Q-learning-based methodology for\niteratively designing Convolutional Neural Networks (CNNs) in a chain-structured,\nmacro search space. They consider three different computer vision target domains:\nCIFAR-10, SVHN and MNIST. This approach models the design process for a neu-\nral network as a sequential decision making problem. A complete neural network is\nconsidered to consist of a sequence of layers. MetaQNN decides the type of the layer,\nand some of its parameters. Using this approach, they were able to achieve compa-\nrable performance to some state-of-the-art networks of the time. The experiments in\nthis publication were reported to take 8-10 days using 10 GPUs, resulting in a total\ncomputational cost of 80-100 GPU-days.\nZoph et al. published their paper on performing Neural Architecture Search with\nReinforcement Learning in 2017 [41]. Contrary to MetaQNN, Zoph et al. used a Long\nShort-Term Memory (LSTM)-based Reinforcement Learning controller to build neu-\nral networks. Their agent was capable of operating both in a chain-structured macro\nsearch space for designing CNNs for CIFAR-10, and in a cell-based search space for\ndesigning recurrent cells for use on Penn Treebank. Compared to Baker et al.’s incre-\nmental approach, Zoph et al. designed entire neural networks in a single timestep,\nand used their validation accuracy as a reward signal. Using their algorithm, Zoph et\nal. were able to achieve state-of-the-art performance or close to it for both CIFAR-10\nand Penn Treebank. One of the downsides of their methodology was its large com-\nputational cost of 800 GPUs over 28 days [42]. Especially compared to the 80-100\nGPU-days of Baker et al. [1], this was a large hurdle for any researcher without access\nto large computational clusters.\nFrom this sprang efforts from various researchers, among who were Pham et al., to\nlower the amount of necessary computational resources to conduct NAS research. In\ntheir paper “Efficient Neural Architecture Search via Parameter Sharing” [24], they\nproposed a method to speed up the search process. Similar to Zoph et al., Pham et\nal. considered the CIFAR-10 and Penn Treebank problems. Pham et al. were able to\nachieve a speed up by a factor of 1000x compared to Zoph et al.’s first publication\n[41], resulting in an overall computational cost of between 8 (CIFAR-10, Macro Search\nSpace) and 10 GPU-hours (Penn Treebank and CIFAR-10 Micro Search Space). Pham\net al. were able to achieve this speed-up through the use of parameter sharing, or\nweight sharing. Realizing that the major bottleneck in [41] was the training of neural\narchitectures from scratch to convergence, Pham et al. attempted, through the use of\n3\nweight-sharing, to accelerate this training process, and thus limit the amount of time\nnecessary to sufficiently train each sampled architecture.\nDespite being originally published in 2018, LSTM-based controllers are still often\nused in applications of NAS. One example of this is Li et al.’s work in [15], where an\nLSTM-based controller is used for designing Graph Neural Networks (GNNs) capable\nof transferring between different graph-based tasks. They do not use weight-sharing,\nbut rather use a performance prediction model to even further increase computational\nefficiency, and avoid some pitfalls of weight sharing specific to GNNs [40].\nA more recent method for performing NAS using RL is GraphPNAS [14]. In this\nwork, a probabilistic graph generator is trained using the REINFORCE algorithm [37].\nThe authors show strong performance on various benchmarks, with a computational\ncost of 16 GPU-days for the Tiny-ImageNet with Oracle Evaluator setting and 12\nGPU-hours in the ENAS Macro search space setting on CIFAR-10.\n3 Methods\nOriginally defined in [7], we consider a solution to a NAS problem to consist of three\ncomponents: a search space, a search strategy and a performance estimation strategy.\nIn this section, we will consider these three aspects for our method. For a more thor-\nough overview of the different types of search spaces, search strategies, performance\nestimation strategies etc., we refer interested readers to [36].\nBesides these aspects, we would also like to briefly make a note of the algorithm we\nuse to identify isomorphic graphs. We adopt a modified version of the graph hashing\nalgorithm used in [38] to detect isomorphic graphs. This algorithm is used for quick\nlook-ups into the NAS-Bench-101 dataset, it is used to check the uniqueness of archi-\ntectures when generating neighbours, it is used in our local search and random search\nalgorithms to identify architectures, etc. Concretely, we improved the performance of\nthe algorithm by removing many of the string ↔bytes conversions. We also replaced\nthe MD5 hashing algorithm in the original with the blake2s hashing algorithm with a\n32-byte digest, due to its increased speed. We verified that the new hashing algorithm\ndoesn’t cause any collisions, by computing the hash of every architecture in the NAS-\nBench-101 benchmark to verify that our changes do not create any hash collisions.\nFinally, we also verified our algorithm produces the same outcomes on the unit tests\nthat were a part of the original NAS-Bench-101 codebase. We also note that the new\n32-byte digest is twice as long as the 16-byte digest of the original MD5 algorithm,\nwhich should in theory reduce the chance of hash collisions.\n3.1 Search Space\nSince Neural Architecture Search is a search problem, we will start by defining the\nsearch spaces considered in this paper. Specifically, we consider two search spaces, the\nfirst is that from NAS-Bench-101 [38], and the second is that from NAS-Bench-301\n[28].\nThe NAS-Bench-101 search space is a cell-based, operations-on-nodes search space.\nIt consists of 423624 unique directed acyclic graphs, with at most 7 vertices and 9\nedges, which represend the computational Directed Acyclic Graph (DAG) of one cell\n4\nc_{k-1}\nc_{k-2}\n2\n1\n0\n3\nsep_conv_3x3\nsep_conv_3x3\nsep_conv_3x3\nskip_connect\nsep_conv_3x3\nsep_conv_3x3\nskip_connect\ndil_conv_3x3\nc_{k}\nc_{k-1}\nc_{k-2}\nc_{k}\n3\ndil_conv_3x312\nskip_connect11\n+13\n0\nsep_conv_3x33\nsep_conv_3x32\n+4\n1\nsep_conv_3x35\nsep_conv_3x36\n+7\n2\nskip_connect8\nsep_conv_3x39\n+10\nFig. 1 An example of a “operations-on-edges” architecture (left) converted to a “operations-on-\nnodes” representation (right). All edges with associated operations are converted to nodes. Each of\nthese nodes is given an in-edge from the source of the original edge, and an out-edge to the destination\nof the original edge. The nodes in the original architecture are replaced by reduction operations, a\nsummation in this case. Finally, as was the case before, all reduction operations are given an edge to\nthe output operation.\nin a neural network. Each vertex can carry one of five labels: “input”, “output”, “conv-\n1x1”, “conv-3x3” and “max-pool-3x3”. Each graph has 1 vertex labelled as “input”\nwith index 0, and the vertex with the highest index is assigned the ”output” label.\nAll vertices in between can be assigned “conv-1x1”, “conv-3x3” or “max-pool-3x3”.\nEvery graph is required to have at least 1 path going from the vertex labelled “input”\nto the vertex labelled “output”, and all vertices must have an in- and out-degree of at\nleast 1 (With exceptions for vertices labelled “input” or “output”).\nThe NAS-Bench-301 search space is a cell-based, operations-on-edges search space\nthat contains roughly 1018 architectures. Each cell has 7 vertices, with the first two\nbeing labelled as “input” and the last one being labelled as “output”. The cell also\ncontains 4 intermediate vertices, each with 2 incoming edges. Finally, all 4 interme-\ndiate vertices also have 1 edge going to the output vertex, where the feature maps\nfrom each intermediate vertex are concatenated along the depth dimension. Each of\nthe 8 incoming edges for the 4 intermediate vertices will have an operation assigned\nto it, picked from “avg pool 3x3”, “dil conv 3x3”, “dil conv 5x5”, “max pool 3x3”,\n“sep conv 3x3”, “sep conv 5x5” or “skip connect”. Each of these edges can use as its\nsource any node with an index lower than its destination node (to ensure the com-\nputational graph is acyclic.) The two edges incident to one reduction node must also\noriginate from different sources. In total, the described search space contains around\n109 architectures. This is raised to 1018 by searching for two architectures from this\nsearch space, a normal and a reduction cell. Both of these cells are then used to\nconstruct the final neural network. When encoding NAS-Bench-301 architectures for\n5\nour agents, we convert the operations-on-edges architecture to a operations-on-nodes\narchitecture. An example of this conversion is displayed in figure 1.\n3.2 Search Strategy\nThe second component of a NAS algorithm is the strategy used to traverse the search\nspace. In this section, we will consider this search strategy by first giving a description\nof the problem our RL agent is tasked with solving, along with a brief description of\nthe agent.\n3.2.1 Incremental Problem Formulation\nWe will describe our problem formulation in more detail. For this formulation, we will\nuse the Markov Decision Process (MDP) framework, which is usually used to describe\nsequential decision making problems, especially in the field of RL. An MDP is an\n6-tuple (S, A, T, γ, µ, R) where:\n• S is the state space\n• A is the action space\n• T : S × A × S →[0, 1] is a probabilistic transition function.\n• γ ∈[0, 1) is a discount factor\n• µ : S →[0, 1] is a probability distribution over initial states\n• R : S × A × S →R is a reward function.\nSequential decision making problems in RL are usually played out in episodes. At\nthe start of an episode, an initial state, s0 is selected from the overall state space\nS using the distribution µ. The agent then observes this state s0, and selects an\naction, a0 based on this observation. Once an action is selected, the next state, s1 is\ndetermined using the transition function T, and a reward is computed using R. This\nsequence of observing the current state, selecting an action, and determining the next\nstate and reward is generally referred to as a timestep. The agent plays out one or\nmore timesteps, until either a terminal state is reached, where the episode naturally\nterminates, or until the episode is truncated for training purposes.\nIn our case, the state space, S is equivalent to the architecture search space, each\narchitecture (or pair of architectures, in the case of NAS-Bench-301) represents one\nstate, s in the MDP. In each state s, an agent has a set of actions it can take. An\naction a, in our case, corresponds to another architecture or tuple of architectures\nin the search space that the agent can move to. The agent is only presented with\nactions that are reachable by making one change to the current state (For example:\nChanging the operation of one node into another). The total number of actions that\nthe agent is presented with is limited at a maximum of N. If there are less than N\nneighbours, the invalid actions will be masked-off, and they will not be considered\nfor transition by the MDP. The agent is also presented with one additional action\nthat terminates the episode at the current state. Our transition function T is entirely\ndeterministic, the next state is the state the agent indicates through its action. γ is a\ndiscounting factor that determines what behaviour is considered optimal. Generally, γ\nis somewhere in the [0, 1) range, with 0.9, 0.95 and 0.99 being common values for the\n6\n0\n1\n2\n3\n4\n5\n0\n1\n3\n4\n5\n0\n1\n3\n4\n5\n0\n1\n3\n4\n5\n1\n2\n3a\n3b\n0\n1\n2\n3\n4\n5\n0\n0\n1\n2\n3\n4\n5\n0\n1\nFig. 2 Vertex Removal Process. 1) A graph consisting of 5 vertices, where we want to remove the\nvertex with index 2. 2) After removing vertex 2, we generate all possible edges connecting the source\nof in-edges to vertex 2 to the destination of out-edges to vertex 2. 3a) An invalid selection of generated\nedges, leaving vertex 1 with an out-degree of 0. 3b) A valid selection of edges, leaving none of the\nother vertices disconnected from the graph.\nparameter. Higher values of γ bias policies to more strongly consider future rewards\nover immediate rewards, while policies found under lower values of γ tend to favour\nimmediate and short-term rewards over long-term rewards. µ determines how the\ninitial state of an episode is selected. In our case, we select an architecture at random\nfrom the search space, using the same sampling logic used for our random search\nalgorithm in section 3.2.3. Finally, R is the reward function. In our case, our reward\nfunction is defined as the difference between the (validation) accuracy of the previous\narchitecture, and the current architecture. For the first timestep of an episode, since\nthere is no previous architecture, the reward is 0. This difference is computed after\napplying reward shaping, described in 3.2.2.\nIn essence, we have reframed the NAS problem as a graph search problem, where\nevery node in a graph represents an architecture, and every edge represents a relation\nbetween architectures. Then, the task our agents must learn to perform, is to find the\ngraph node with the highest accuracy.\nNext, we will describe the process of neighbour generation. Each timestep, an\nagent is presented with the current architecture, and N of its neighbours, and has to\nchoose one of these architectures as the next state of the MDP. If the agent chooses\nthe current architecture, the episode is terminated. These neighbours are generated\nby making small alterations to the current architecture. Below is a list of the different\nalterations we use to generate neighbours.\n• Remove a vertex. When removing a vertex, edges are selected such that the source\nof in-edges to the removed vertex are connected to the destination of out-edges to\nthe removed vertex. Edges are selected such that all vertices involved have an in-\nand out-degree of at least 1, to ensure no invalid graphs are generated, this process\nis illustrated in figure 2. Every possible arrangement of vertices is considered to be\na distinct neighbour (after accounting for isomorphism.)\n7\n• Add a vertex. The vertex will be connected to one of the preceding and one of\nthe succeeding vertices with an in- and an out-edge respectively. When generating\nneighbours, all possible ways of connecting the new vertex are generated.\n• Change a vertex label. We change the label of a vertex to a different label. This\noperation can never change a vertex into an input or an output, or change an input\nor output into a different operation.\n• Remove an edge. Only edges that, when removed, don’t break the graph’s\nconnectivity are considered for removal.\n• Add an edge. Edges can only be added between existing vertices, they can not\ngenerate cycles, and the total number of edges can never exceed the maximum\nnumber of edges of the search space.\nIn [33], White et al. also consider the NAS problem through a similar lens. There\nare some differences though between their framing and ours. In their work, [33] assume\nthat neighbourhoods are a symmetric relation. That is, if A is in B’s neighbourhood,\nthen B must also be in A’s neighbourhood. Under our formulation, this isn’t necessarily\nthe case. An example of this is shown in figure 2. While the architecture in 3b is\nconsidered to be a neighbour of the architecture in 1 (Through removing vertex 2), the\narchitecture in 1 is not a direct neighbour of the architecture in 3b, instead requiring\nthe addition of a vertex, and the addition and removal of several edges. This partly\ninvalidates [33]’s definition of a branching factor, and thus makes it impossible to apply\ntheir theoretical results with regards to the number of local minima etc. to our problem\nformulation. We also note that our overall search space graph also does not have a\nregular degree in the way [33]’s does. A possible solution to resolve these discrepancies\nwould be the inclusion of a “zeroize” operation in the NAS-Bench-101 search space,\nakin to the “zeroize” operation in NAS-Bench-301. This would allow NAS-Bench-\n101 to be formulated as a NAS problem with a fixed topology, where the removal of\ncertain vertices and edges is achieved through the use of “zeroize” operations, similar\nto NAS-Bench-301.\nIn some settings, like NAS-Bench-301, more than one cell is designed at a time. In\nthis case, all cells (two in the NAS-Bench-301 case) are shown to the agent at the same\ntime. In order to generate neighbours in this case, we generate the neighbours of each\ncell individually, and then select up to N tuples, each consisting of 1 neighbour for each\ncell to form the final selection of neighbours. This leads to a significant increase in the\ntotal number of neighbours for benchmarks that search multiple cells at a time. (In the\nNAS-Bench-301 setting, we noticed that each cell has around 70 neighbours, leading\nto a total set of around 4900 neighbours, of which 50 are selected for presentation to\nthe agent).\n3.2.2 Reward Shaping\nIn order for our agent to converge, we utilize reward shaping. In reinforcement learn-\ning, reward shaping changes the reward function that is used to train an agent, to\nfacilitate the training process. In our case, we employ reward shaping due to the\ninherent distribution of rewards in the benchmark problems we consider. In NAS-\nBench-101, the average validation accuracy across all random initializations for all\n8\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95 100\nNASBench-101 Validation Accuracy [%]\n100\n101\n102\n103\n104\n105\nOccurrences [/]\nOriginal accuracy distribution\nNormalized shaped accuracy distribution (\n= 6)\nFig. 3 Histogram of the validation accuracy of the architectures included in NAS-Bench-101, across\nall random initializations. The original accuracy distirbution is shown in blue, while the distribution\nafter reward shaping is shown in red. Note the logarithmic Y-axis.\narchitectures is 90.24% If we plot a histogram of the validation accuracy of all\narchitectures and all random initializations, we also see that the vast majority of\narchitectures have accuracies in the range of [85%, 94%], as demonstrated in figure 3.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nOriginal Reward [/]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nShaped Reward [/]\nNo Reward Shaping\n= 2\n= 6\n= 10\n= 32\nFig. 4 The reward shaping that was used in this\npaper. Experiments on NAS-Bench-101 used α =\n6, experiments on NAS-Bench-301 used α = 32.\nOther values were used in ablation studies.\nThis means that, even though\nthe\nreward for our agent is technically nor-\nmalized between 0 and 1, the majority\nof architectures will fall into this small\nrange, resulting in very small reward\nvalues, after the difference between\ntwo consecutive architectures is taken.\nThus, we apply an exponential function,\neα·R, to the validation accuracy of the\narchitectures, and use this exponential\naccuracy in the difference calculation\nbetween subsequent architectures. By\nselecting an appropriate value for α,\nwe can tune the reward function based\non the problem definition, to always\nensure a good distribution of rewards.\nFigure 3 shows the difference between\nthe original accuracy distribution, and\nthe distribution after reward shaping.\nWe demonstrate the reward shaping\nfunctions used in this publication in\nfigure 4.\n9\n2\n3\n4\n5\n6\n7\nNumber of Vertices [/]\n100\n101\n102\n103\n104\n105\nOccurrences [/]\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNumber of Edges [/]\n100\n101\n102\n103\n104\n105\nOccurrences [/]\n2\n3\n4\n5\n6\n7\nNumber of Vertices [/]\n100\n101\n102\n103\n104\n105\nOccurrences [/]\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNumber of Edges [/]\n100\n101\n102\n103\n104\n105\nOccurrences [/]\n2\n3\n4\n5\n6\n7\nNumber of Vertices [/]\n100\n101\n102\n103\n104\n105\nOccurrences [/]\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNumber of Edges [/]\n100\n101\n102\n103\n104\n105\nOccurrences [/]\nUniform Sampler\nBANANAS Random Cell Adj Sampler\nOur Sampler\nFig. 5 Histogram of the number of vertices and edges in a random sample of 10000 architectures,\ncompared between a uniform sample from the NAS-Bench-101 dataset, the “Random Cell Adj”\nsampler from the BANANAS repository, and our sampler. Note the logarithmic Y-axis.\n3.2.3 Agents\nIn order to get a good sense of the performance of our proposed agent, we have also\nimplemented several baseline algorithms. In this section, we will elaborate on the\ndetails of each of the agents we evaluate.\nRandom Search\nThe most basic agent we include is a random search agent. This agent does not fol-\nlow the incremental problem formulation we intend to use, but rather just selects an\narchitecture at random from the entire search space, similar to the random search\nagent used in [14]. It serves an indication of how much of a change in difficulty or\ncomplexity the use of the incremental problem formulation creates. If the incremen-\ntal problem is significantly more difficult or easy than just picking one architecture\nfrom the search space, we expect to see a performance difference between the random\nsearch and random walk agent. Our random sampling procedure is slightly different\nfrom that used in BANANAS [34]. We start by sampling the size of each graph, in this\ncase from a uniform distribution. Then, we proceed to sample in a manner similar to\nBANANAS, where we sample random adjacency matrices and vertex labelings, and\nsimply reject the invalid ones until we have a valid sample, with the added require-\nment that an adjacency matrix must have the desired number of vertices. Figure 5\nshows a comparison between a uniform sample from the NAS-Bench-101 dataset, the\nBANANAS random sampler ours is based on, and our sampling algorithm, in terms\nof the number of vertices of the generated architectures. We note that our sampler\nproduces a quasi-uniform distribution for both the number of vertices and the number\nof edges, which ensures that our RL agent has sufficient experience with both large\narchitectures (many vertices) and smaller architectures (few vertices), thus ensuring\nconsistent performance across the entire search space.\n10\n1\n3\n...\n0\n1\n0\n0\n0\n...\n...\n...\n...\n...\n...\n0\n1\n1\n1\n...\n...\n...\n...\n1\n0\n...\n1\n3\n...\nTransformer\nEncoder\nLinear-ReLU\nT:T\nARCHITECTURE ENCODING\nADVANTAGE HEAD\nLinear-ReLU\n|o|:T\nARCHITECTURE\nINDEX\nA\nLinear-ReLU\nT:T\nLinear-ReLU\nT:T\nLinear\nT:1\nLinear\nT:N\nV\nSTATE VALUE\nESTIMATE\nPositional\nEncoding\nx 3\nx 2\nx 2\nVALUE HEAD\nOBSERVATION PREPARATION\nFig. 6 The architecture of our transformer agent. The top row shows the process of preparing the\nobservations, while the bottom row represents the learnable RL agent.\nRandom Walk\nWe also include a random agent that follows the incremental problem formulation we\ndefined earlier in section 3.2. This agent randomly selects a next neighbour according\nto a uniform distribution. It can additionally also opt to terminate the episode instead\nof selecting a next neighbour.\nLocal Search\nSome publications [33][4] note that local search is a strong NAS algorithm, especially\nin smaller search spaces like NAS-Bench-101. With this in mind, we also include local\nsearch as one of the algorithms we benchmark against. Our local search algorithm\ndoesn’t have any of the enhancements that [33] used. The local search agent is pre-\nsented with N architectures, it compares the validation accuracy for all of them, and\nselects the one with the highest validation accuracy. If no architecture has a valida-\ntion accuracy higher than that of the current architecture, the episode is terminated.\nWe note that, different from [33], our neighbour relation is non-symmetrical, meaning\nthat A being a neighbour of B, doesn’t guarantee that B will also be a neighbour of\nA, as we explained in section 3.2.\nRL-based Agent\nOur RL agent, shown in figure 6, uses a transformer encoder as the core of its architec-\nture, similar to Vision Transformer (ViT) [5]. The agent is presented with at most N\ndifferent architectures. Before the architectures are presented to the agent, they must\nbe prepared. We take the lower-triangular half of the adjacency matrix (To ensure\nacyclicity), and flatten it by concatenating all of the rows. This flattened adjacency\n11\nmatrix is then concatenated to the vertex labels, encoded in a one-hot fashion, result-\ning in one long binary vector. In order to be able to encode architectures with varying\nsizes, the original adjacency matrix and vertex labels are first padded to the size of the\nlargest architecture. If multiple architectures need to be searched (As in the case of\nNAS-Bench-301), the architectures are concatenated after preparation. Before send-\ning the architectures to be encoded, we prepend the the current architecture to the\nsequence of up to N neighbours.\nAfter preparation, an architecture encoding network transforms the prepared\nobservation to a 256-dimensional latent space (T = 256), using several fully-connected\nlayers with ReLU activations. These latent space vectors then have a positional encod-\ning applied to them, to ensure that the permutation-invariant transformer architecture\nremains aware of the order of the latent space vectors. While the exact ordering of\nthe architectures is unimportant, the agent must still be aware of their ordering, since\neach action the agent can perform corresponds to one of the architectures observed.\nThus, if the agent loses all awareness of the ordering of all architectures, it will be\nunable to select the appropriate action.\nThe current architecture is prepended to all possible neighbours, similar to the\n“classification token” in [5]. Correspondingly, we use the first output of the transformer\nencoder as the input for the rest of the agent. At this point, we have a duelling head\nconsisting of two branches [32]. One uses the transformer output to compute advantage\nvalues for each architecture that was presented to the transformer encoder, which is\nused to determine which action the agent will take. The second branch computes a\nstate-value, and is used to stabilize the RL procedure.\nThe agent is trained using the Ape-X algorithm [12], a variant of Q-Learning\ndesigned for a high throughput of experience collection. We enhance Ape-X using\nPartial Episode Bootstrapping (PEB) [23] and 3-step bootstrapping to be able to train\nthe agent in finite-length episodes, while obtaining a behaviour suitable for episodes\nof infinite length. Our Q-learning network makes use of a target network [21], duelling\nhead [32] and double Q-learning [9].\n3.3 Performance Estimation\nFor this publication, we consider two benchmark problems: NAS-Bench-101 and NAS-\nBench-301. We selected NAS-Bench-101, since it is the largest tabular benchmark\nwe are currently aware of. Being a tabular benchmark, it eliminates some possible\nconfounding factors that may arise when using other performance estimation strate-\ngies. We also included NAS-Bench-301, since it covers a very large and commonly\nused search space (≈1018 architectures). Contrary to NAS-Bench-101, NAS-Bench-\n301 uses a machine learning model fitted on a subset of the search space to predict\nthe performance (classification accuracy, in this case) for the entire search space. All\nagents we trained use the same performance estimator, in order to elimate the choice\nof performance estimator as a possible confounding factor.\n12\n4 Experiments\nIn order to evaluate the effectiveness of RL agent, we will evaluate it on two NAS\nbenchmarks: NAS-Bench-101 [38] and NAS-Bench-301 [28]. We start by evaluating on\nNAS-Bench-101. In their work, White et al. [33] note that because the search space\nfor NAS-Bench-101 is relatively small, relatively simple algorithms like local search\ntend to perform fairly well. It is with this in mind that we selected NAS-Bench-301\nas a second benchmark. Since it uses the DARTS [17] search space, which many other\nNAS algorithms also use, it should give us a clearer view of how our agent performs\ncompared to algorithms like local search.\n4.1 NAS-Bench-101\n4.1.1 Experiment Configuration\nWhen tackling NAS-Bench-101, we train our agents for 10 × 106 timesteps of expe-\nrience. As a reward signal, we use the mean of all three validation accuracies after\n108 epochs of training (Corresponding to the reduced noise setting in [35]). We train\n5 agents with seeds ranging from 0 to 4 (inclusive), using version 1.13 of the RLLib\nframework [16]. We set γ = 0.9 with a learning rate of 4 × 10−5. Our replay buffer has\na capacity of 25 × 103 entries. Our replay buffer is a prioritized replay buffer [27] with\nα = 0.6 and β = 0.4. Exploration is done using a per-worker epsilon-greedy strategy.\nWe use the same parameters as section 4.1 of the Ape-X paper [12]: Each individ-\nual worker uses an epsilon-greedy exploration strategy, with ϵi = ϵ1+(α·\ni\nN−1). We set\nϵ = 0.4 and α = 7, these are the RLLib default settings. We train our agent using\nepisodes of 16 timesteps, and evaluate it with episodes consisting of 32 timesteps. We\nrefer to figure 6 in [38], plotting the Random Walk Autocorrelation (RWA) in the NAS-\nBench-101 search space. We note that after a 6-10 timestep random walk through the\nsearch space, RWA flattens out close to zero, indicating that there should be no more\nlocality effects. This implies that, in order for agents to make significant alterations,\nthe maximum episode length should be greater than 6, supporting our choice for 16.\nAs mentioned in section 3.2.2, we use reward shaping with α = 6. Our experimental\nsetup allocates 8 threads for 8 workers to gather experience, 4 threads for 4 shards of\nthe replay buffer, and 2 threads for the driver, as well as 1 GPU for the driver, but not\nfor the workers. Different experiments were carried out using either 1 NVIDIA Tesla\nV100 GPU, or 1 NVIDIA Quadro RTX4000 GPU. Each worker also uses a vectorized\nversion of our environments, that allows it to operate on 32 environments at a time.\nFor evaluation, we randomly selected 5 sets of 1 × 104 architectures from the search\nspace. These are used as the initial state for each episode of the evaluation process.\nEach of the 5 trained agents is evaluated on each of the 5 sets of 1 × 104 initial states,\nresulting in a total of 2.5 × 105 episodes of evaluation data per algorithm. For evalua-\ntion of NAS-Bench-101, we use the image classification accuracy on the test set after\n108 epochs of training.\n13\nFig. 7 The initial accuracy of the agent vs the improvement in accuracy. The diagonal dashed lines\nrepresent a final accuracy (at the end of an episode) of 85%, 90% and 95% respectively. The diagonal\nline delimiting red region in the top-right corner represents an accuracy of 100%.\n4.1.2 Results\nIn the NAS-Bench-101 setting, the mean training time was 92.57h (σ = 20.89h), with\na minimum of 78.27h, and a maximum of 133.97h. During evaluation, using only\nCPUs (no GPUs), completing an episode took an average of 0.50s (σ = 0.49 s), with\na minimum of 0.00s and a maximum of 10.14s.\nWe start by evaluating the ability of our agents to improve from a given starting\naccuracy. We do this by plotting the improvement in accuracy over an entire episode\nagainst the accuracy of the initial state. An ideal agent would generate a diagonal\nline, as close as possible to the top right corner. How close this diagonal can be to\n14\nthe top-right corner is determined by the global optimum within the search space.\nAn agent that consistently manages to end up at the same architecture (Regardless\nof its performance), will have all datapoints on a perfect diagonal line that intersects\nthe X-axis at the accuracy of this final architecture. Thus, the spread of an agent’s\ndatapoints from a perfect diagonal line can be used to gauge how consistently an agent\nmanages to find the same architecture. A deviation from a diagonal line indicates an\nagent that doesn’t always perform consistently, and can optimize some architectures\nbetter than others. These results are shown in figure 7. From this, we can see that every\ntested policy displays some degree of inconsistency, as indicated by none of the policies\nforming a perfect diagonal line. As expected, this is greatest for random policies,\nwhile local search is the most consistent. Our reinforcement learning agent is fairly\nconsistent in improving the accuracy of the architectures it is given, but sometimes\nfails to improve, and makes things worse. This can be an issue when using the agent as\npart of a hyperparameter tuning pipeline, but the agent improves consistently enough\nthat this should be a relatively rare occurrence, as evidenced by some of the other\ndata in this section.\nWe can also look at this data from a different perspective, by evaluating how often\neach agent is able to improve a given architecture, and how often an agent returns a\nworse architecture. We perform this evaluation by creating a histogram showing the\ndistribution of the difference in accuracy between the start of an episode, and the end\nof that episode, where positive values indicate an improvement from the initial to the\nfinal architecture. Since the agent decides when to terminate the episode, it should\nideally always terminate an episode when the accuracy is higher than the initial state.\nWe show these results in figure 8.\nWe also show the median value for each histogram using a solid, vertical black line.\nBesides this, we also show symmetrical intervals around the median. For instance,\nthe interval marked 50% contains the values between the 25-th and 75-th percentile,\ni.e., 50% of values concentrated around the median. From this histogram, we can\nimmediately see that, in terms of consistent improvements, local search is undisputedly\nthe best algorithm (Median improvement: +3.77%), followed by Ape-X (+1.15%),\nRandom Walks (+0.26%) and Random Search (+0.00%). This is in line with our\nexpectations, since the local search algorithm is formulated in such a way that it should\nalways improve an architecture. We do note that this isn’t always the case, since local\nsearch optimizes validation accuracy, but is evaluated on test accuracy. We also note\nthat there are several high peaks in the local search histogram, with one bin even\ncontaining around 20% of the episodes. We found that in this bin, the final architecture\nis the same in the vast majority of episodes, implying that this architecture is a local\noptimum. We show this architecture in figure 9. Quantitatively, we can also see this\nreflected in the skew value for each distribution. Negative skew values imply that the\ntail of the distribution is on the left side, while positive skew values indicate the tail is\non the right side. Or in other words, negative skew implies that most probability mass\nis on the right side of the distribution, while positive skew implies the majority of\nprobability mass is on the left side of the distribution. Local search exhibits the lowest\nskew, with −2.091, followed by Ape-X with −1.821, Random Walks at 0.091 and finally\nrandom search with a skew of 0.219. It also shows that our RL agent most commonly\n15\nNAS-Bench-101 Test Accuracy Difference [%]\n0\n5\n10\n15\n20\n25\n30\nRelative Occurrence [%]\n50%\n75%\n95%\nApe-X (N=248208)\nNAS-Bench-101 Test Accuracy Difference [%]\nRelative Occurrence [%]\n50%\n75%\n95%\nLocal Search (N=247695)\n10\n5\n0\n5\n10\nNAS-Bench-101 Test Accuracy Difference [%]\n0\n5\n10\n15\n20\n25\n30\nRelative Occurrence [%]\n50%\n75%\n95%\nRandom Search (N=247458)\n10\n5\n0\n5\n10\nNAS-Bench-101 Test Accuracy Difference [%]\nRelative Occurrence [%]\n50%\n75%\n95%\nRandom Walk (N=248053)\nFig. 8 A histogram showing the distribution of the improvement in accuracy over an entire episode.\nWe limited the range of improvements shown between -12.5% and +12.5%, which leads to a small\namount of outlying samples (Roughly 2000-3000 samples, or 0.8% - 1.2% of all samples) being excluded\nfrom the histogram (Not included in the extreme bins.) The solid vertical line indicates the median\nimprovement. Dashed vertical lines indicate the edges of a symmetrical range around the median\ncontaining a certain percentage of the total samples.\nimproves an architecture, but on rare occasions fails to do so. Finally, one more artifact\nfrom this data is the fact that random walks actually outperformed random search,\nwith a median that skews slightly towards the positive side. This becomes even more\nobvious when we consider the skew by the intervals noted in the histogram. While for\nrandom search, most intervals are roughly symmetrical around the median, for random\nwalks, the intervals skew heavily towards the side of positive improvements, with the\n95% interval ranging from -5% to +10%. Initially, we hypothesized that there may be\n16\n1\n50\n100\n150\n200\n250\n300\nQueries [/]\n91.0\n91.5\n92.0\n92.5\n93.0\n93.5\n94.0\n94.5\n95.0\nNAS-Bench-101 Test Accuracy [%]\nApe-X (N=500)\nLocal Search (N=500)\nRandom Search (N=500)\nRandom Walk (N=500)\nRandom (GraphPNAS, 2023)\nNAO (SemiNAS, 2020)\nSemiNAS (2020)\nGraphPNAS (2023)\nFig. 10 The accuracy of the best architecture found so far plotted against the number of queries.\nWe evaluated each algorithm for up to 300 queries. For baselines indicated by black lines, we used\nthe numbers reported in their publications. The shaded area (for our experiments) and the error bars\n(for other baselines) represents a 95% confidence interval around the mean.\na correlation between an architecture’s test accuracy, and the number of neighbours it\nhas. This data is shown in a scatter plot in figure 11, but unfortunately reveals only a\nvery weak correlation, with Pearson’s r at -0.014, and Kendall’s τ at -0.157. We were\nunable to find another explanation for this performance difference.\nFinally, we also consider the question of how many queries are required for an\nagent to be able to find well-performing architectures. We consider a setting where an\nalgorithm can make up to 300 queries, and assess the agent by looking at the highest\naccuracy architecture that has been found after a given number of queries. We note\nthat, for all algorithms except local search, 1 episode is considered to be 1 query. For\nlocal search, we count every neighbour at every timestep as a query, since the local\nsearch agent needs to know the accuracy of each neighbour, before deciding on which\naction to take. The results of this are shown in figure 10. We also compare against\na number of state-of-the-art publications that also reported their results after 300\nqueries: NASWOT [20], GraphPNAS [14], Random Search (from GraphPNAS), NAO\n(Reported by SemiNAS) [19] and SemiNAS [19]. For both our own experiments, and\nthe state-of-the-art algorithms, we computed a 95% confidence interval, to determine\nif there is a statistically significant difference in performance between algorithms. The\n95% confidence interval for our own experiments was computed using bootstrapping\nwith 5 × 103 bootstrap samples, while the confidence intervals for other baselines\n(Black lines in figure 10) were computed using a closed-form expression that assumes\na normal distribution, since we don’t always have access to the full dataset, but most\n17\nFig. 11 The test accuracy of a NAS-Bench-101 architecture vs. the number of neighbours of the\narchitecture.\npublications do report a mean and standard deviation. In figure 10, we see that our\nRL agent performs fairly well for low query budgets, but as soon as the total number\nof queries exceeds 75-100, it is outperformed by a local search agent. Local search is\nknown to perform well in the NAS-Bench-101 setting, due to the relatively limited size\nof the search space [33]. Interestingly, at lower query budgets (< 50 queries, random\nwalks and random search actually outperform local search. We attribute this to the fact\nthat we allow agents to observe at most 50 neighbours. Thus, if local search is presented\nwith an architecture that has 50 or more neighbours, it first must make 50 queries (one\nfor each neighbour) before it is able to realize an improvement. This also explains the\njump in performance that occurs at the 50 query mark for local search. Interestingly, we\nnote that, when considering the number of queries, random walks actually outperform\nrandom search quite significantly, and even approach the performance of our RL agent\nas the number of queries increases. This may be another indication that not all forms\nof random search are created equal, and the way a NAS problem is formulated can\nhave an impact on how well different random algorithms are able to perform. Our first\nhypothesis for this discrepancy was that larger architectures (More vertices and edges)\ntend to have both more neighbours and more trainable parameters, and thus a higher\naccuracy. However, when plotting the number of neighbours an architecture has vs.\nits test accuracy, it becomes clear that the correlation is rather weak, implying there\nis likely at least one more factor contributing to random walks strong performance.\nUnfortunately, we were unable to find any other parameters that may contribute to\nrandom walks’ strong performance.\nTable 1 shows the numerical results for the best NAS-Bench-101 test accuracy\nfound by our RL agent after a specified number of queries. We selected 50 as the best\nresult for a low query budget, 100 as the cross-over point where local search starts\noutperforming our algorithm, 150 as the halfway point to 300 queries, and 300 since\nits the most commonly used value by most state-of-the-art methods.\n4.2 NAS-Bench-301\n18\n50 Queries\n100 Queries\n150 Queries\n300 Queries\nAlgorithm\n#Q\nµ ± σ\n95% CI\n#Q\nµ ± σ\n95% CI\n#Q\nµ ± σ\n95% CI\n#Q\nµ ± σ\n95% CI\nRandom (NASWOT) [20]\nN/A\n100\n90.38%\n± 5.51%\n[79.55%,\n101.21%]\nN/A\nN/A\nNASWOT [20]\nN/A\n100\n91.77%\n± 0.05%\n[91.67%,\n91.87%]\nN/A\nN/A\nRandom (GraphPNAS) [14]\nN/A\nN/A\nN/A\n300\n93.59%\n± 0.42%\n[92.63%,\n94.54%]\nNAO [19]\nN/A\nN/A\nN/A\n300\n93.69%\n± 0.06%\n[93.57%,\n93.81%]\nSemiNAS [19]\nN/A\nN/A\nN/A\n300\n93.89%\n± 0.06%\n[93.77%,\n94.01%]\nGraphPNAS [14]\nN/A\nN/A\nN/A\n300\n94.19%\n± 0.07%\n[94.03%,\n94.36%]\nRandom Search (Ours)\n50\n93.48%\n± 0.15%\n[93.47%,\n93.50%]\n100\n93.56%\n± 0.14%\n[93.55%,\n93.58%]\n150\n93.61%\n± 0.15%\n[93.60%,\n93.63%]\n300\n93.69%\n± 0.16%\n[93.68%,\n93.71%]\nRandom Walk (Ours)\n50\n93.62%\n± 0.20%\n[93.60%,\n93.64%]\n100\n93.73%\n± 0.18%\n[93.71%,\n93.74%]\n150\n93.79%\n± 0.17%\n[93.78%,\n93.80%]\n300\n93.88%\n± 0.14%\n[93.87%,\n93.89%]\nLocal Search (Ours)\n50\n93.61%\n± 0.55%\n[93.57%,\n93.66%]\n100\n93.85%\n± 0.33%\n[93.81%,\n93.87%]\n150\n93.95%\n± 0.26%\n[93.93%,\n93.97%]\n300\n94.05%\n± 0.17%\n[94.04%,\n94.07%]\nApe-X (Ours)\n50\n93.73%\n± 0.16%\n[93.71%,\n93.74%]\n100\n93.82%\n± 0.15%\n[93.80%,\n93.83%]\n150\n93.86%\n± 0.14%\n[93.85%,\n93.87%]\n300\n93.94%\n± 0.13%\n[93.93%,\n93.95%]\nTable 1 Numerical test accuracy values on NAS-Bench-101 (mean ± std.) and 95% confidence intervals at various query budgets. Since we may not have data\nfor a specific query number , we use the data of the last preceding improvement.\n19\nHK-1\nconv-3x31\nconv-3x32\nconv-3x33\nHK\nFig. 9 The local optimum found by local search\nin the NAS-Bench-101 setting in slightly more than\n20% of cases.\nAs mentioned at the start of section 4,\nwe will also evaluate our agent on the\nNAS-Bench-301 setting [28], due to its\nincreased complexity compared to NAS-\nBench-101.\n4.2.1 Configuration\nFor NAS-Bench-301, our hyperparam-\neter setup is similar to that of NAS-\nBench-101. We train our agents for\nlonger on NAS-Bench-301, for up to\n15 × 106 timesteps. We sampled a new\nset of initial states for evaluation (Since\nNAS-Bench-301 uses a different search\nspace) with the same size as the NAS-\nBench-101 set of initial states. We set\nreward shaping exponent to 32, rather\nthan 6 as was the case with NAS-Bench-\n101. The NAS-Bench-301 agent also\nuses a 512-dimensional latent space,\ncompared to 256 dimensions for NAS-\nBench-101, due to the increased size of\nthe observations, and the fact that it\nneeds to encode two architectures (Nor-\nmal and Reduction Cell) in one latent\nspace. We use version 1.0 of NAS-Bench-301, with the included ensemble of XGB\nestimators, without using noisy predictions (i.e., taking the mean of all predictors in\nthe ensemble), following the recommendation from [35] to reduce noise in architec-\nture evaluation pipelines (Denoised setting). We made some minor alterations to the\noriginal NAS-Bench-301 code to allow for batched inference of the estimator to speed\nup the training procedure. All experiments were carried out 1 NVIDIA Tesla V100\nGPU. The vectorization of our environment was disabled to reduce the overall time\ntaken for a single training iteration. Since the NAS-Bench-301 XGB ensemble is only\ntrained to predict validation accuracy, we both train and evaluate our agents using\nthe validation accuracy.\n4.2.2 Results\nWe will be evaluating our agent using the same analysis and metrics that we did for\nNAS-Bench-101, and the evaluation procedure also remains unchanged.\nOne thing that is immediately noticeable in the improvement histograms for NAS-\nBench-301, is the quasi-normal shaped improvement distribution, for all algorithms\nconsidered. This differs considerably from NAS-Bench-101, where the improvement\ndistributions, especially for random search, were much more uniformly distributed. To\nverify this hypothesis, we conducted a normality test with a p-value of 5 × 10−3. For\nall algorithms, the null-hypothesis that the data is normally distributed was rejected,\n20\nFig. 12 The initial accuracy of the agent vs the improvement in accuracy.\nwith the test reporting p-values less than 1 × 10−6 for all algorithms. We used scipy’s\n“scipy.stats.normaltest” [30] test, which combines skew and kurtosis to test the nor-\nmality of a sample. Following these normality tests, we also conducted a test to see\nif the data might follow a t-distribution. We used “scipy’s scipy.stats.goodness of fit”\n[30] with 1000 Monte-Carlo samples. Once again, we use a p-value of 5 × 10−3 as our\nthreshold for rejecting the null hypothesis that our data follows a t-distribution. For\nall algorithms, we rejected the null hypothesis that the data follows a t-distribution,\nwith each test returning a p-value of 9.99×10−4 for Ape-X, Local Search and Random\nWalks, and a p-value of 1.998 × 10−3 for Random Search.\nThe confidence intervals displayed in figure 14 were computed in the same way\nas in our NAS-Bench-101 experiments. Figure 14 shows us a slightly different picture\n21\nNAS-Bench-301\nValidation Accuracy Difference [%]\n0\n2\n4\n6\n8\n10\n12\n14\n16\nRelative Occurrence [%]\n50%\n75%\n95%\nApe-X (N=250000)\nNAS-Bench-301\nValidation Accuracy Difference [%]\nRelative Occurrence [%]\n50%\n75%\n95%\nLocal Search (N=250000)\n4\n2\n0\n2\n4\nNAS-Bench-301\nValidation Accuracy Difference [%]\n0\n2\n4\n6\n8\n10\n12\n14\n16\nRelative Occurrence [%]\n50%\n75%\n95%\nRandom Search (N=250000)\n4\n2\n0\n2\n4\nNAS-Bench-301\nValidation Accuracy Difference [%]\nRelative Occurrence [%]\n50%\n75%\n95%\nRandom Walk (N=250000)\nFig. 13 A histogram showing the distribution of the improvement in accuracy over an entire episode\nfrom figure 10 in the NAS-Bench-101 setting. With a 300-query budget, our RL agent\nactually outperforms local search. Another interesting difference is the near-identical\nperformance of random search and random walks. While in the NAS-Bench-101 set-\nting, these two showed a significant performance difference, in the NAS-Bench-301\ncase, they both achieve nearly identical performance.\nSince some publications use larger query budgets for NAS-Bench-301 (Ranging\nfrom 150 to 10000), we also evaluate our agent with a budget of 1000 queries. When we\nextend the query budget up to 1000 queries, it becomes clear that, given a sufficiently\nlarge computational budget, local search remains a strong search algorithm, surpassing\nour RL algorithm around the 400 query mark. We also see that, just as in the 300-\nquery case, performance for random search and random walks remains nearly identical,\n22\n1\n50\n100\n150\n200\n250\n300\nQueries [/]\n91\n92\n93\n94\n95\n96\nNAS-Bench-301 Validation Accuracy [%]\nApe-X (N=500)\nLocal Search (N=500)\nRandom Search (N=500)\nRandom Walk (N=500)\nRandom Search (CR-LSO, 2022)\nCMA-ES (CR-LSO, 2022)\nNAO (CR-LSO, 2022)\nTree-Structured Parzen Estimator (CR-LSO, 2022)\nCR-LSO (2022)\nFig. 14 The accuracy of the best architecture found so far plotted against the number of queries.\nWe evaluated each algorithm for up to 300 queries.\nsuggesting that the difference in performance in the NAS-Bench-101 case can likely\nbe attributed to a difference between the benchmarks.\nWe also provide numerical data on the best mean validation accuracy obtained\nafter a specific number of queries in table 2. The shown query budgets were chosen to\ncoincide with the baselines shown in figure 14.\nFinally, we consider the time it takes to train and evaluate our RL agent. The\nmean training time for NAS-Bench-301 was 486.16h (σ = 40.12h), with a minimum of\n446.75h, and a maximum of 553.48h. During evaluation, using only CPUs (no GPUs),\ncompleting an episode took an average of 16.30s (σ = 9.52 s), with a minimum of\n0.00s and a maximum of 69.83s.\nWe note an important difference in the training and evaluation time between\nthe NAS-Bench-101 and NAS-Bench-301 setting. The average training time for NAS-\nBench-101 is around 4 days, while that for NAS-Bench-301 is around 21 days (5.25×),\ndespite NAS-Bench-301 only requiring 1.5× as much training experience, resulting\nin the conclusion that in the NAS-Bench-301 setting, we are on average 3.5× slower\nto train. When looking at the evaluation times, we see an even starker contrast of\n0.5s/Episode for NAS-Bench-101, and 16.3s/Episode for NAS-Bench-301 (32×). We\nattribute this difference in training time primarly to the time it takes to evaluate both\nreward functions. Since evaluating a reward function is a fairly common occurrence in\nRL (It occurs at least once per timestep), a slow reward function will inevitably slow\ndown the whole training process. Because NAS-Bench-301 requires the evaluation of\n10 different gradient boosted trees, its reward function is significantly slower than that\nof NAS-Bench-101, which consists of simply a table look-up. We can also see this in\n23\n150 Queries\n200 Queries\n300 Queries\n400 Queries\nAlgorithm\n#Q\nµ ± σ\n95% CI\n#Q\nµ ± σ\n95% CI\n#Q\nµ ± σ\n95% CI\n#Q\nµ ± σ\n95% CI\nRandom Search (GPTP) [8]\nN/A\nN/A\nN/A\nN/A\nSynflow (GPTP) [8]\nN/A\nN/A\nN/A\nN/A\nCL-fine-tune (GPTP) [8]\nN/A\nN/A\nN/A\nN/A\nRandom Search (CR-LSO) [25]\nN/A\n200\n94.17%\n± 0.06%\n[94.00%,\n94.34%]\nN/A\n400\n94.23%\n± 0.05%\n[94.09%,\n94.37%]\nTPE (CR-LSO) [25]\nN/A\n200\n94.50%\n± 0.06%\n[94.33%,\n94.67%]\nN/A\n400\n94.65%\n± 0.03%\n[94.57%,\n94.73%]\nCMA-ES (CR-LSO) [25]\nN/A\n200\n94.37%\n± 0.09%\n[94.12%,\n94.62%]\nN/A\n400\n94.60%\n± 0.06%\n[94.43%,\n94.77%]\nNAO (CR-LSO) [25]\nN/A\n200\n94.49%\n± 0.01%\n[94.46%,\n94.52%]\nN/A\n400\n94.55%\n± 0.04%\n[94.44%,\n94.66%]\nCR-LSO [25]\nN/A\n200\n94.53%\n± 0.04%\n[94.32%,\n94.64%]\nN/A\n400\n94.80%\n± 0.06%\n[94.63%,\n94.97%]\nRandom Search (Ours)\n149\n94.26%\n± 0.12%\n[94.25%,\n94.28%]\n198\n94.30%\n± 0.11%\n[94.29%,\n94.31%]\n297\n94.34%\n± 0.09%\n[94.33%,\n94.35%]\n400\n94.36%\n± 0.09%\n[94.35%,\n94.37%]\nRandom Walk (Ours)\n150\n94.28%\n± 0.12%\n[94.26%,\n94.29%]\n198\n94.31%\n± 0.11%\n[94.29%,\n94.32%]\n300\n94.35%\n± 0.11%\n[94.33%,\n94.36%]\n400\n94.37%\n± 0.11%\n[94.35%,\n94.38%]\nLocal Search (Ours)\n150\n94.71%\n± 0.26%\n[94.68%,\n94.74%]\n200\n94.78%\n± 0.19%\n[94.75%,\n94.80%]\n300\n94.85%\n± 0.13%\n[94.84%,\n94.87%]\n400\n94.89%\n± 0.11%\n[94.87%,\n94.90%]\nApe-X (Ours)\n150\n94.83%\n± 0.05%\n[94.83%,\n94.84%]\n199\n94.85%\n± 0.05%\n[94.84%,\n94.85%]\n300\n94.86%\n± 0.05%\n[94.86%,\n94.87%]\n398\n94.88%\n± 0.04%\n[94.87%,\n94.88%]\n800 Queries\n1000 Queries\nAlgorithm\n#Q\nµ ± σ\n95% CI\n#Q\nµ ± σ\n95% CI\nRandom Search (GPTP) [8]\n800\n94.75%\n± 0.08%\n[94.53%,\n94.97%]\nN/A\nSynflow (GPTP) [8]\n800\n94.60%\n± 0.11%\n[94.29%,\n94.91%]\nN/A\nCL-fine-tune (GPTP) [8]\n800\n94.83%\n± 0.06%\n[94.66%,\n95.00%]\nN/A\nRandom Search (CR-LSO) [25]\n800\n94.31%\n± 0.06%\n[94.14%,\n94.48%]\nN/A\nTPE (CR-LSO) [25]\n800\n94.76%\n± 0.04%\n[94.65%,\n94.87%]\nN/A\nCMA-ES (CR-LSO) [25]\n800\n94.82%\n± 0.08%\n[94.60%,\n95.04%]\nN/A\nNAO (CR-LSO) [25]\n800\n94.72%\n± 0.08%\n[94.50%,\n94.94%]\nN/A\nCR-LSO [25]\n800\n94.89%\n± 0.06%\n[94.72%,\n95.06%]\nN/A\nRandom Search (Ours)\n798\n94.41%\n± 0.08%\n[94.40%,\n94.42%]\n1000\n94.43%\n± 0.08%\n[94.42%,\n94.44%]\nRandom Walk (Ours)\n797\n94.42%\n± 0.10%\n[94.41%,\n94.43%]\n1000\n94.44%\n± 0.09%\n[94.43%,\n94.45%]\nLocal Search (Ours)\n800\n94.94%\n± 0.09%\n[94.93%,\n94.96%]\n1000\n94.95%\n± 0.08%\n[94.94%,\n94.96%]\nApe-X (Ours)\n800\n94.90%\n± 0.04%\n[94.90%,\n94.91%]\n1000\n94.91%\n± 0.04%\n[94.90%,\n94.91%]\nTable 2 Numerical validation accuracy values on NAS-Bench-301 (mean ± std.) and 95% confidence intervals at various query budgets. Since we may not have\ndata for a specific query number, we use the data of the last preceding improvement.\n24\n1\n250\n500\n750\n1000\nQueries [/]\n91\n92\n93\n94\n95\n96\nNAS-Bench-301 Validation Accuracy [%]\nApe-X (N=250)\nLocal Search (N=250)\nRandom Search (N=250)\nRandom Walk (N=250)\nRandom Search (CR-LSO, 2022)\nSynflow (GPTP, 2023)\nNAO (CR-LSO, 2022)\nRandom (GPTP, 2023)\nTree-Structured Parzen Estimator (CR-LSO, 2022)\nCMA-ES (CR-LSO, 2022)\nCL-fine-tune (GPTP, 2023)\nCR-LSO (2022)\nFig. 15 The accuracy of the best architecture found so far plotted against the number of queries.\nWe evaluated each algorithm for up to 1000 queries.\n21\n23\n25\n27\nBatch Size [/]\n10\n1\n100\n101\n102\nTime / Step / Sample [ms]\nNAS-Bench-301\nNAS-Bench-101\n21\n23\n25\n27\nBatch Size [/]\n101\n102\n103\n104\nSample-Steps per Second [1/s (Hz)]\nNAS-Bench-301\nNAS-Bench-101\n21\n23\n25\n27\nBatch Size [/]\n10\n2\n10\n1\n100\n101\n102\n103\nTotal Time [s]\nNAS-Bench-301\nNAS-Bench-101\nFig. 16 A comparison of the throughput of our environment for NAS-Bench-101 and NAS-Bench-\n301, with various degrees of vectorization. Note the logarithmic axes.\nfigure 16, which compares the performance of our environment when using the NAS-\nBench-101 objective versus the NAS-Bench-301 objective. To generate these numbers,\nwe stripped our environment down to its bare essentials, so we can focus solely on the\neffect that these objectives have on the throughput. The environment no longer uses\nthe incremental formulation we’ve used so far, and no longer need to do things like\ngenerate neighbours. We simply feed the environment an architecture, as our action,\nand it returns the reward. We can clearly see that, when using the NAS-Bench-101\n25\n0\n2M\n4M\n6M\n8M\n10M\nTimesteps Trained [/]\n91.0\n91.5\n92.0\n92.5\n93.0\nMean NAS-Bench-101\nValidation Accuracy [%]\n= 0.90\n= 0.95\n= 0.99\nFig. 17 Mean validation accuracy over time for γ = 0.9, 0.95 and 0.99 on NAS-Bench-101. A simple\nmoving average filter with N=10 was used to smooth out the results.\nobjective, our environment tends to be around two orders of magnitude quicker than\nwhen using NAS-Bench-301, regardless of the degree of vectorization used.\n5 Ablation Studies\nWe also perform several ablation studies to study the effect of certain parameter\nchoices on our overall system. We use the NAS-Bench-101 setting for this, since it is\nthe quickest to train, allowing us to conduct more experiments. We compare different\nparameter choices by looking at the mean validation accuracy of the architectures\nproduced throughout the training procedure.\n5.1 Gamma\nFirst, we consider the γ parameter that is used in RL to determine how heavily future\nrewards should be discounted compared to current rewards. Lower values for γ discount\nfuture rewards more, and thus focus more on immediate rewards, and less on future\nrewards. We consider three values for γ : 0.9, 0.95 and 0.99. In figure 17 we show the\nmean accuracy of the architectures generated as a function of the number of timesteps\nthe agent has been trained on. The figure shows that, during the training procedure,\nonly the agent trained with γ = 0.9 is able to make a noticable improvement. The\nagents with γ = 0.95 and 0.99 both fail to significantly improve the accuracy of the\ngenerated architectures, and the agent with γ = 0.99 even ends up with slightly\nworse performance. During training, we noted that, on average, every architecture has\naround 25 neighbours, this means that, N steps into the future, an agent can be at\nany of 25N different states. Since the number of possible states that can be reached\nin relatively few steps grows very quickly, making an accurate determination of the\nfuture discounted reward becomes very difficult, thus hindering convergence when\nlarger values of γ are used.\n26\n0\n2M\n4M\n6M\n8M\n10M\nTimesteps Trained [/]\n91.0\n91.5\n92.0\n92.5\n93.0\nMean NAS-Bench-101\nValidation Accuracy [%]\n25 Neighbours\n50 Neighbours\n100 Neighbours\nFig. 18 Mean validation accuracy over time for N = 25, 50 and 100 on NAS-Bench-101. A simple\nmoving average filter with N=10 was used to smooth out the results.\n5.2 Number of Neighbours\nOur environment limits the number of neighbours that our agent can choose from.\nThe agent is presented with up N neighbours, or less if the total number of neighbours\nis less than N. We consider three values for this parameter: 25, 50 and 100. From\nthis, we can see that agents observing 25 or 50 neighbours both converge to roughly\nthe same performance, while observing up to 100 neighbours leads to slightly worse\nperformance. We hypothesize that this is likely a function of model capacity. The\nsame model was used in all three experiments, but the size of the observations, and\nthe number of architectures that needs to be considered quadrupled between N = 25\nand N = 100. These results also show that, in a search space such as NAS-Bench-101\nwhich is fairly densely connected, raising the total number of neighbours beyond a\ncertain threshold doesn’t help agent performance.\n5.3 Reward Shaping\nIn the NAS-Bench-101 setting, the majority of architectures have an accuracy around\n90%, with only a small minority having accuracies far below 90%. Because of this, we\nintroduced a reward shaping scheme in section 3.2.2. In this section, we will consider\nthe effect that different values of the shaping parameter α have on the performance of\nour agent. We consider four settings for this reward shaping mechanism: No reward\nshaping (No exponential functions are applied), and three values for the reward shap-\ning coefficient: 2, 6 and 10. The results of this experiment are shown in figure 19. In\nthis figure, we see that the experiments without reward shaping, or with only slight\nreward shaping (α = 2) achieve worse performance than experiments with stronger\nreward shaping (α = 6, 10). From this, we conclude that, in order to achieve good\nperformance, a sufficient level of reward shaping is necessary. We also note that the\nexperiment with α = 10 is slightly quicker to experience a rise in accuracy, however,\nwe do not believe this to be significant, since in our ablations for γ, we used α = 6,\nand saw results similar to α = 10 when γ = 0.9.\n27\n0\n2M\n4M\n6M\n8M\n10M\nTimesteps Trained [/]\n91.0\n91.5\n92.0\n92.5\n93.0\nMean NAS-Bench-101\nValidation Accuracy [%]\nNo Reward Shaping\n= 2\n= 6\n= 10\nFig. 19 Mean validation accuracy over time for no reward shaping, and α = 2, 6 and 10 on NAS-\nBench-101. A simple moving average filter with N=10 was used to smooth out the results.\n6 Conclusions\nIn this publication, we outline a new, incremental framing of the NAS problem. We also\nintroduce a method of tackling this problem using a RL-based NAS agent. We show\nthat our RL agent is competitive with state-of-the-art NAS algorithms and baselines\nthat are known to have strong performance. Our NAS agent outperforms all other\nbenchmarks considered when query budgets are low, but starts to be overtaken by\nother algorithms as query budgets increase. We also perform an ablation study and\nconclude that our agent has a high sensitivity to some hyperparameters, while being\nrather insensitive to others.\n6.1 Training Times\nWhen comparing training times between the NAS-Bench-101 and NAS-Bench-301\nsettings, we note a significant difference between both (≈5×) in terms of training\ntime, even after accounting for the additional training required for NAS-Bench-301\n(≈3.5×). We attribute this difference primarily to the difference in time it takes to\nevaluate each reward function, with NAS-Bench-301 being significantly slower, due to\nthe need to evaluate 10 gradient boosted trees. This presents a significant barrier to the\nadoption of our RL based method, but it can be overcome by improving the sample-\nefficiency of the RL agent, or using surrogate models that are quicker to evaluate than\nthe ensemble of gradient boosted trees used in this paper.\n6.2 Random Search\nIn the NAS-Bench-101 setting, we observe a noticable difference between the random\nsearch and random walk algorithms. When comparing both algorithms in the NAS-\nBench-301 setting, we note that they achieve near identical performance. This indicates\nthat the difference in performance between both algorithms is likely a result of a\npecularity of the NAS-Bench-101 benchmark. This does draw into question the validty\nof comparing the results of algorithms across problem formulations, since, in some\n28\ncases, formulating the problem in a different way can lead to significantly different\nperformance, even for random algorithms.\n6.3 Scalability\nDespite an increase in the size of the search space from 4.23×105 to 1018, the amount\nof samples required to train our RL agent only increased from 1.0 × 107 to 1.5 × 107,\nwhile retaining strong performance in both benchmarks. This shows that a RL-based\napproach can scale well as the search space we operate in becomes larger.\n6.4 Ablation Studies\nOur ablation studies show mixed results with regards to robustness to hyperparameter\nchanges. Some hyperparameters (like the number of neighbours the agent is presented\nwith) seem to have relatively little effect on the agent’s ability to converge, while\nothers (such as the degree of reward shaping and reward discounting) seem to have an\nalmost-binary effect on convergence: Sufficiently high or low values must be selected\nin order to ensure convergence of the agent.\n7 Future Work\n7.1 Re-usability evaluation\nAn important distinction between our transformer-based controller and earlier RL-\nbased NAS controllers is its re-usability. The traditional RL-based NAS paradigm\ninvolves training an RL agent to output a single architecture. This means that, once\nthe training procedure is finished, the optimal architecture is found, but the RL agent\nserves no purpose beyond this point. Accordingly, the time and compute cost for train-\ning this agent has gone entirely towards finding a single optimal architecture. Through\nthe re-use our agent offers, this time-cost can be amortized over many searches, since\nwe didn’t learn the optimal solution, but rather how to find it.\n7.2 Domain Generalization\nIn this work, we onyl considered the image classification domain. This unfortunately\nmeans that a new agent must be trained every time we wish to tackle a new problem\ndomain. A truly re-usable NAS agent should be re-usable on a completely new domain\nwith little to no adaptation work. There are several avenues that could enable this. One\nof these, is the use of a training-free, domain-independent performance estimator such\nas Neural Tangent Kernel-based metrics like Label-Gradient Alignment (LGA) [22], or\nthe number of linearly separable regions [20]. Using metrics like these as performance\nestimators would create an agent that can theoretically operate in any search space,\nregardless of the target domain, assuming that the used metrics correlate strongly\nwith actual domain-performance across domains.\nAcknowledgments.\nThis research received funding from the Flemish Government\n(AI Research Program).\n29\nThis work was supported by the Research Foundation Flanders (FWO) under Grant\nNumber 1SC8821N.\nReferences\n[1] Baker B, Gupta O, Naik N, et al (2017) Designing neural network architec-\ntures using reinforcement learning. In: International Conference on Learning\nRepresentations, URL https://openreview.net/forum?id=S1c2cvqee\n[2] Cai Z, Chen L, Liu HL (2023) Bhe-darts: Bilevel optimization based on hyper-\ngradient estimation for differentiable architecture search. In: ICASSP 2023 -\n2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp 1–5, https://doi.org/10.1109/ICASSP49357.2023.10095940\n[3] Chu X, Zhou T, Zhang B, et al (2020) Fair darts: Eliminating unfair advantages\nin differentiable architecture search. In: Vedaldi A, Bischof H, Brox T, et al (eds)\nComputer Vision – ECCV 2020. Springer International Publishing, Cham, pp\n465–480\n[4] Den Ottelander T, Dushatskiy A, Virgolin M, et al (2021) Local search is a\nremarkably strong baseline for neural architecture search. In: Ishibuchi H, Zhang\nQ, Cheng R, et al (eds) Evolutionary Multi-Criterion Optimization. Springer\nInternational Publishing, Cham, pp 465–479\n[5] Dosovitskiy A, Beyer L, Kolesnikov A, et al (2021) An image is worth 16x16 words:\nTransformers for image recognition at scale. In: International Conference on\nLearning Representations, URL https://openreview.net/forum?id=YicbFdNTTy\n[6] Elsken T, Metzen JH, Hutter F (2019) Efficient multi-objective neural architec-\nture search via lamarckian evolution. In: International Conference on Learning\nRepresentations, URL https://openreview.net/forum?id=ByME42AqK7\n[7] Elsken T, Metzen JH, Hutter F (2019) Neural architecture search: A survey.\nJournal of Machine Learning Research 20(55):1–21\n[8] Han FX, Mills KG, Chudak F, et al (2023) A general-purpose transferable predic-\ntor for neural architecture search. In: Proceedings of the 2023 SIAM International\nConference on Data Mining (SDM), SIAM, pp 721–729\n[9] Hasselt H (2010) Double q-learning. In: Lafferty J, Williams C, Shawe-Taylor\nJ, et al (eds) Advances in Neural Information Processing Systems, vol 23.\nCurran Associates, Inc., URL https://proceedings.neurips.cc/paper files/paper/\n2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf\n[10] He K, Zhang X, Ren S, et al (2016) Deep residual learning for image recogni-\ntion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)\n30\n[11] Hendrickx L, Van Ranst W, Goedem´e T (2022) Hot-started nas for task-\nspecific embedded applications. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) Workshops, pp 1971–1978\n[12] Horgan D, Quan J, Budden D, et al (2018) Distributed prioritized experience\nreplay. In: International Conference on Learning Representations, URL https:\n//openreview.net/forum?id=H1Dy---0Z\n[13] Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with\ndeep convolutional neural networks. In: Pereira F, Burges C, Bottou L,\net al (eds) Advances in Neural Information Processing Systems, vol 25.\nCurran Associates, Inc., URL https://proceedings.neurips.cc/paper/2012/file/\nc399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n[14] Li M, Liu JY, Sigal L, et al (2022) Graphpnas: Learning distribution of good\nneural architectures via deep graph generative models. 2211.15155\n[15] Li\nY,\nWu\nJ,\nDeng\nT\n(2023)\nMeta-gnas:\nMeta-reinforcement\nlearn-\ning\nfor\ngraph\nneural\narchitecture\nsearch.\nEngineering\nApplications\nof\nArtificial\nIntelligence\n123:106300.\nhttps://doi.org/https://doi.org/10.1016/j.\nengappai.2023.106300, URL https://www.sciencedirect.com/science/article/pii/\nS0952197623004840\n[16] Liang E, Liaw R, Nishihara R, et al (2018) RLlib: Abstractions for distributed\nreinforcement learning. In: Dy J, Krause A (eds) Proceedings of the 35th Inter-\nnational Conference on Machine Learning, Proceedings of Machine Learning\nResearch, vol 80. PMLR, pp 3053–3062, URL https://proceedings.mlr.press/v80/\nliang18b.html\n[17] Liu H, Simonyan K, Yang Y (2019) DARTS: Differentiable architecture search. In:\nInternational Conference on Learning Representations, URL https://openreview.\nnet/forum?id=S1eYHoC5FX\n[18] Lu Z, Whalen I, Boddeti V, et al (2019) Nsga-net: neural architecture search using\nmulti-objective genetic algorithm. In: Proceedings of the Genetic and Evolution-\nary Computation Conference. Association for Computing Machinery, New York,\nNY, USA, GECCO ’19, p 419–427, https://doi.org/10.1145/3321707.3321729,\nURL https://doi.org/10.1145/3321707.3321729\n[19] Luo R, Tan X, Wang R, et al (2020) Semi-supervised neural architecture search.\nAdvances in Neural Information Processing Systems 33:10547–10557\n[20] Mellor J, Turner J, Storkey A, et al (2021) Neural architecture search without\ntraining. In: Meila M, Zhang T (eds) Proceedings of the 38th International Con-\nference on Machine Learning, Proceedings of Machine Learning Research, vol 139.\nPMLR, pp 7588–7598, URL https://proceedings.mlr.press/v139/mellor21a.html\n31\n[21] Mnih V, Kavukcuoglu K, Silver D, et al (2013) Playing atari with deep rein-\nforcement learning. CoRR abs/1312.5602. URL http://arxiv.org/abs/1312.5602,\n1312.5602\n[22] Mok J, Na B, Kim JH, et al (2022) Demystifying the neural tangent kernel from\na practical perspective: Can it be trusted for neural architecture search without\ntraining? In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp 11861–11870\n[23] Pardo F, Tavakoli A, Levdik V, et al (2018) Time limits in reinforcement learning.\nURL https://openreview.net/forum?id=HyDAQl-AW\n[24] Pham H, Guan M, Zoph B, et al (2018) Efficient neural architecture search\nvia parameters sharing. In: Proceedings of the 35th International Conference\non Machine Learning, pp 4095–4104, URL https://proceedings.mlr.press/v80/\npham18a.html\n[25] Rao X, Zhao B, Yi X, et al (2022) Cr-lso: Convex neural architecture optimization\nin the latent space of graph variational autoencoder with input convex neural\nnetworks. 2211.05950\n[26] Real E, Moore S, Selle A, et al (2017) Large-scale evolution of image classifiers.\nIn: Precup D, Teh YW (eds) Proceedings of the 34th International Conference\non Machine Learning, Proceedings of Machine Learning Research, vol 70. PMLR,\npp 2902–2911, URL https://proceedings.mlr.press/v70/real17a.html\n[27] Schaul T, Quan J, Antonoglou I, et al (2016) Prioritized experience replay [c/ol].\nIn: Proceedings of the 4th Inter national Conference on Learning Representations,\nICLR\n[28] Siems JN, Zimmer L, Zela A, et al (2021) {NAS}-bench-301 and the case for\nsurrogate benchmarks for neural architecture search. URL https://openreview.\nnet/forum?id=1flmvXGGJaa\n[29] Szegedy C, Liu W, Jia Y, et al (2015) Going deeper with convolutions. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR)\n[30] Virtanen P, Gommers R, Oliphant TE, et al (2020) SciPy 1.0: Fundamental\nAlgorithms for Scientific Computing in Python. Nature Methods 17:261–272.\nhttps://doi.org/10.1038/s41592-019-0686-2\n[31] Wang L, Zhao Y, Jinnai Y, et al (2020) Neural architecture search using deep\nneural networks and monte carlo tree search. Proceedings of the AAAI Conference\non Artificial Intelligence 34(06):9983–9991. https://doi.org/10.1609/aaai.v34i06.\n6554, URL https://ojs.aaai.org/index.php/AAAI/article/view/6554\n32\n[32] Wang Z, Schaul T, Hessel M, et al (2016) Dueling network architectures for\ndeep reinforcement learning. In: Balcan MF, Weinberger KQ (eds) Proceedings of\nThe 33rd International Conference on Machine Learning, Proceedings of Machine\nLearning Research, vol 48. PMLR, New York, New York, USA, pp 1995–2003,\nURL https://proceedings.mlr.press/v48/wangf16.html\n[33] White C, Nolen S, Savani Y (2020) Local search is state of the art for nas\nbenchmarks. CoRR abs/2005.02960. URL https://arxiv.org/abs/2005.02960\n[34] White C, Neiswanger W, Savani Y (2021) Bananas: Bayesian optimization with\nneural architectures for neural architecture search. In: Proceedings of the AAAI\nConference on Artificial Intelligence\n[35] White C, Nolen S, Savani Y (2021) Exploring the loss landscape in neural\narchitecture search. In: de Campos C, Maathuis MH (eds) Proceedings of the\nThirty-Seventh Conference on Uncertainty in Artificial Intelligence, Proceed-\nings of Machine Learning Research, vol 161. PMLR, pp 654–664, URL https:\n//proceedings.mlr.press/v161/white21a.html\n[36] White C, Safari M, Sukthanker R, et al (2023) Neural architecture search: Insights\nfrom 1000 papers. arXiv preprint arXiv:230108727 URL https://arxiv.org/abs/\n2301.08727\n[37] Williams RJ (1992) Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine learning 8:229–256\n[38] Ying C, Klein A, Christiansen E, et al (2019) NAS-bench-101: Towards repro-\nducible neural architecture search. In: Chaudhuri K, Salakhutdinov R (eds)\nProceedings of the 36th International Conference on Machine Learning, Proceed-\nings of Machine Learning Research, vol 97. PMLR, Long Beach, California, USA,\npp 7105–7114, URL http://proceedings.mlr.press/v97/ying19a.html\n[39] Zhang Q, Peng Y, Zhang Z, et al (2023) Semantic segmentation of spectral\nlidar point clouds based on neural architecture search. IEEE Transactions on\nGeoscience and Remote Sensing 61:1–11. https://doi.org/10.1109/TGRS.2023.\n3284995\n[40] Zhou K, Huang X, Song Q, et al (2022) Auto-gnn: Neural architecture search of\ngraph neural networks. Frontiers in big Data 5:1029307\n[41] Zoph B, Le Q (2017) Neural architecture search with reinforcement learning. In:\nInternational Conference on Learning Representations, URL https://openreview.\nnet/forum?id=r1Ue8Hcxg\n[42] Zoph B, Vasudevan V, Shlens J, et al (2018) Learning transferable architec-\ntures for scalable image recognition. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp 8697–8710\n33\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-02",
  "updated": "2024-10-02"
}