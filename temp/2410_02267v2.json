{
  "id": "http://arxiv.org/abs/2410.02267v2",
  "title": "Unsupervised Meta-Learning via Dynamic Head and Heterogeneous Task Construction for Few-Shot Classification",
  "authors": [
    "Yunchuan Guan",
    "Yu Liu",
    "Ketong Liu",
    "Ke Zhou",
    "Zhiqi Shen"
  ],
  "abstract": "Meta-learning has been widely used in recent years in areas such as few-shot\nlearning and reinforcement learning. However, the questions of why and when it\nis better than other algorithms in few-shot classification remain to be\nexplored. In this paper, we perform pre-experiments by adjusting the proportion\nof label noise and the degree of task heterogeneity in the dataset. We use the\nmetric of Singular Vector Canonical Correlation Analysis to quantify the\nrepresentation stability of the neural network and thus to compare the behavior\nof meta-learning and classical learning algorithms. We find that benefiting\nfrom the bi-level optimization strategy, the meta-learning algorithm has better\nrobustness to label noise and heterogeneous tasks. Based on the above\nconclusion, we argue a promising future for meta-learning in the unsupervised\narea, and thus propose DHM-UHT, a dynamic head meta-learning algorithm with\nunsupervised heterogeneous task construction. The core idea of DHM-UHT is to\nuse DBSCAN and dynamic head to achieve heterogeneous task construction and\nmeta-learn the whole process of unsupervised heterogeneous task construction.\nOn several unsupervised zero-shot and few-shot datasets, DHM-UHT obtains\nstate-of-the-art performance. The code is released at\nhttps://github.com/tuantuange/DHM-UHT.",
  "text": "UNSUPERVISED\nMETA-LEARNING\nVIA\nDYNAMIC\nHEAD AND HETEROGENEOUS TASK CONSTRUCTION\nFOR FEW-SHOT CLASSIFICATION\nYunchuan Guan∗\nHuazhong Univerisity\nof Science and Technology\nYu Liu\nHuazhong Univerisity\nof Science and Technology\nKetong Liu\nHuazhong Univerisity\nof Science and Technology\nKe Zhou\nHuazhong Univerisity\nof Science and Technology\nZhiqi Shen\nNanyang Technological\nUniversity\nABSTRACT\nMeta-learning has been widely used in recent years in areas such as few-shot learn-\ning and reinforcement learning. However, the questions of why and when it is\nbetter than other algorithms in few-shot classiﬁcation remain to be explored. In\nthis paper, we perform pre-experiments by adjusting the proportion of label noise\nand the degree of task heterogeneity in the dataset. We use the metric of Singular\nVector Canonical Correlation Analysis to quantify the representation stability of\nthe neural network and thus to compare the behavior of meta-learning and clas-\nsical learning algorithms. We ﬁnd that beneﬁting from the bi-level optimization\nstrategy, the meta-learning algorithm has better robustness to label noise and het-\nerogeneous tasks. Based on the above conclusion, we argue a promising future for\nmeta-learning in the unsupervised area, and thus propose DHM-UHT, a dynamic\nhead meta-learning algorithm with unsupervised heterogeneous task construction.\nThe core idea of DHM-UHT is to use DBSCAN and dynamic head to achieve het-\nerogeneous task construction and meta-learn the whole process of unsupervised\nheterogeneous task construction. On several unsupervised zero-shot and few-shot\ndatasets, DHM-UHT obtains state-of-the-art performance. The code is released\nat https://github.com/tuantuange/DHM-UHT.\n1\nINTRODUCTION\nMeta-learning has emerged as a powerful paradigm for learning to adapt to unseen tasks Vanschoren\n(2018). As an example, the optimization-based meta-learning algorithm Finn et al. (2017); Raghu\net al. (2020); Nichol et al. (2018) has been shown to demonstrate excellent generalization perfor-\nmance in few-shot learning and reinforcement learning. In these areas, the more commonly used\npre-train and ﬁne-tune strategy exhibits disadvantages regarding training overhead, reliance on mas-\nsive samples, and accuracy.\nHowever, in recent years, new research has shown that models pre-trained by the classical Whole-\nClass Training (WCT) strategy exhibit comparable or even better accuracy on multiple few-shot\nimage classiﬁcation datasets Tian et al. (2020); Chen et al. (2021). The inconsistent conclusions\ndescribed above confuse us about the nature of meta-learning, and in turn hinder us from devel-\noping the area. Most of the current theoretical research on meta-learning focuses on estimating\nthe upper bound of the generalization error of meta-learning Jose & Simeone (2021a); Chen et al.\n(2020). However, the conclusions given by these researches cannot be directly used to improve the\nperformance of meta-learning algorithms nor extend the range of their applications. Two simple but\nimportant questions remain to be answered – why and when is meta-learning better than other\nalgorithms in few-shot classiﬁcation?\n∗Email: hustgyc@hust.edu.cn\n1\nOur contribution can be summarized as follows:\n• We answer the question of why and when meta-learning is better than the classical learning\nalgorithms in few-shot classiﬁcation.\n• To exploit meta-learning’s robustness to label noise and heterogeneous tasks, we propose\nDHM-UHT. We are the ﬁrst to treat the whole process of unsupervised task construction as\nthe meta-objective.\n• Through comparative experiment, ablation experiment, and sensitive experiment, we\ndemonstrate the superiority of DHM-UHT on unsupervised zero-shot and few-shot learning\nscenarios.\n2\nWHY AND WHEN IS META-LEARNING BETTER IN FEW-SHOT\nCLASSIFICATION?\nIn this chapter we answer the two questions of why and when meta-learning is better than other\nalgorithms in few-shot classiﬁcation. We reveal that the strength of meta-learning algorithm lies in\nthe fact that it is robust to label noise and heterogeneous tasks, and that meta-learning should have\nbetter performance in unsupervised area under the same annotation ability constraints.\n2.1\nROBUSTNESS TO LABEL NOISE\nCurrently, the controversy about meta-learning focuses on few-shot classiﬁcation scenarios. Several\nstudies have shown that pre-trained models obtained by classical Whole Class Training (WCT) can\nachieve similar or even better results than meta-learning algorithms Tian et al. (2020); Luo et al.\n(2023); Chen et al. (2021). However, such a straightforward comparison is unfair because under\nsupervised conditions, the annotation effort of WCT requires distinguishing all categories in the\ndataset, whereas meta-learning only requires distinguishing a few categories in the meta-task. Since\ndata annotation for classical supervised pre-training algorithm is more costly than meta-learning\nwhen obtaining similar performance, it is reasonable to speculate that meta-learning algorithm may\nperform better when annotation ability or label error rate are the same.\nPerformance Evaluation. First, we evaluate the classiﬁcation accuracy of two meta-learning al-\ngorithms and classical WCT pre-training algorithm on Omniglot and Mini-Imagenet datasets with\n5-way 1-shot task. We use the same neural network architecture and learning conﬁguration as Finn\net al. (2017); Raghu et al. (2020). The dataset setup is detailed in A.1. As shown in Table 1 and\nTable 2, we compared two meta-learning algorithms with WCT on the original datasets. The results\nare the same as in Tian et al. (2020): under the condition of using the same network architecture,\nthere is almost no difference in performance between them. However, the meta-learning algorithms\nANIL and MAML achieved signiﬁcantly higher classiﬁcation accuracy when we introduced 15%\nlabel noise to samples in the dataset. The difference in performance is even more pronounced when\nthe label noise comes to 30%.\nRepresentation Stability Analyse. Second, we analyse the behavior of meta-learning and WCT\non neural network. Speciﬁcally, by employing SVCCA Raghu et al. (2017b) as metric, we propose\na quantitative approach to measure representation stability rsi\nt. By visualizing the representation\nstability of each neural network layer, we can analyse the characteristic of meta-learning and other\nalgorithms during training process. The learned representation of i −th layer can be written as\nθi(D), where D is A ﬁxed batch of test sample. At epoch t, the representation stability of the i −th\nlayer of the network at is deﬁne as:\nrsi\nt = SV CCA(θi\nt(D), θi\nt−1(D)).\n(1)\nSVCCA is the metric to measure representation similarity. It is often used to compare the behavioral\nsimilarity of different models. In meta-learning, it is used by us to explain whether fast adaptation\nin MAML is essentially feature reuse Raghu et al. (2020). In this paper, we use SVCCA to calculate\nthe representation stability of the same neural network component at different training stages, i.e.,\nrsi\nt. This metric helps us observe the behavior of the training algorithm at all opponents of the\nmodel during the training process. As shown in Figure 2, in Omniglot dataset with 15% noise,\nlayers trained by WCT show a relatively low representation stability. At the same time, we can\n3\nTable 1: Comparison of Meta-Learning’s and\nWCT’s resistance to label noise on the Omniglot\ndataset with 5-way 1-shot task. The metric is\nAccuracy%.\n0% label noise\n15% label noise\n30% label noise\nWCT\n94.51\n82.44\n64.65\nANIL\n94.35\n89.83\n76.36\nMAML\n94.46\n89.79\n76.34\nTable 2: Comparison of Meta-Learning’s and\nWCT’s resistance to label noise on the Mini-\nImagenet dataset with 5-way 1-shot task. The\nmetric is Accuracy%.\n0% label noise\n15% label noise\n30% label noise\nWCT\n47.04\n38.92\n29.68\nANIL\n46.77\n41.69\n37.45\nMAML\n46.81\n41.63\n37.51\nANIL\nMAML\nL0\nL1\nL2\nL3\nL4\n1.0\n0.8\n0.6\n0.4\n0.2\n0  \n25   50  75  100\nnoise\nw/o noise\n1.0\n0.8\n0.6\n0.4\n0.2\n0  \n25   50  75  100\n0  \n25   50  75  100\nEpoch/500\nCCA\nWCT\nFigure 2: Representation stability of WCT and meta learning algorithms on Omniglot dataset with\n5-way 1-shot tasks. The x-axis is the epoch and the y-axis is rsi\nt value. Higher y-axis value means\nhigher stability. L4 is neural network’s head, L0 is the input layer, and L1-L3 is body.\nnotice that the representational stability gradually decreases from the bottom to the top layers (L4\nto L0), which is consistent with the general pattern of neural networks Raghu et al. (2017b). The\nabove phenomenon shows that WCT is difﬁcult to learn effective representations when training\nwith label noise. In contrast, the representation of ANIL sacriﬁces stability in the bottom layer\n(i.e., head), to maintain high level representation stability on the middle layers (i.e., body). We are\nnot surprised by the lower representational stability of the head. According to ANIL’s principle,\nthe head’s parameters are involved in the update of the inner-loop, which makes it easy to ﬁt with\ndifferent data-label dependencies for different tasks. For the body of the network, when updating\nwithin the outer-loop, it is able to extract inter-task common representations since heads have been\nadapted to the sample-label dependencies of the current tasks. In this process, the meta-learning\nalgorithm uses the header as a ﬁlter to avoid label noise. It is worth noting that MAML learns\nthe same representation patterns as ANIL, even though it does not explicitly distinguish between\nhead and body. We believe this is because MAML is essentially optimizing the ﬁne-tuning process,\nand a good neural network pattern for ﬁne-tuning should be the same as ANIL, MAML just ﬁnds\nit! Back to WCT, WCT seems to treat the whole neural network as a head, and thus may suffer\nfrom different sample-data dependencies.\n2.2\nROBUSTNESS TO HETEROGENEOUS TASK\nIn the study of meta-learning, previous researchers were obsessed with ﬁxed networks and ﬁxed loss\nfunctions, thus focusing only on the learning of homogeneous tasks. For example, for classiﬁcation\ntasks, previous researchers constructed tasks with the same classiﬁcation way. This approach allows\ndifferent tasks to use the same network structure and identical function, which in turn simpliﬁes\nthe algorithm and saves overhead. However, such an approach may make meta-learning overﬁt\nto unnecessary information about “way of classiﬁcation” on training sets with homogeneous\ntasks, and underperform on test sets with heterogeneous tasks.\nTo extend the heterogeneous adaptability of meta-learning, we combine the bi-level optimization\ntraining strategy of meta-learning and the dynamic head trick of multi-task learning to achieve dy-\nnamic head meta-learning (DHM). Speciﬁcally, as shown in Algorithm 1, we improve the operations\nrelated to the network’s head. For each task Ti, we reinitialize the network’s head. We will show ex-\n4\nTable 3: Comparison of Accuracy% on\nHeterogeneous Tasks.\nOmniglot\nMini-Imagenet\nDynamic Head\n93.27\n44.09\nStatic Head\n92.86\n41.63\nMulti Task\n72.95\n35.86\nTable 4: Comparison of Accuracy% on\n5-way 1-shot Homogeneous Tasks.\nOmniglot\nMini-Imagenet\nDynamic Head\n94.41\n46.77\nStatic Head\n94.46\n46.81\nCCA\nMTL\nL0\nL1\nL2\nL3\n1.0\n0.8\n0.6\n0.4\n0.2\n0  \n25   50  75  100\nEpoch /500\n0  \n25   50  75  100\n0  \n25   50  75  100\nSHM\nDHM\nFigure 3: Representation stability of DHM, SHM, and MTL on Omniglot dataset with 5-way 1-shot\ntasks.\nperimentally that dynamic meta-learning can provide better generalization without loss of accuracy\ncompared to classical multi-task learning and classical meta-learning algorithms.\nIn datasets composed of heterogeneous tasks, we compare the performance of dynamic head meta-\nlearning, classical static head meta-learning (SHM), and multi-task learning (MTL). Speciﬁcally,\nwe perform the experiment on Omniglot and Mini-Imagenet datasets. On Omniglot dataset, the\nclassiﬁcation way of heterogeneous tasks varies from 5-20, and this number on Mini-Imagenet is\n5-10. The setup is detailed in A. For SHM, we train a model for each way of tasks, and ultimately\ntake the average testing performance of the models. For DHM and MTL, we train the model with\nthe train set consisting of a mixture of the heterogeneous tasks, and ultimately evaluating its perfor-\nmance directly on the test set. As shown in Table 3, we can ﬁnd that the accuracy in descending is\nDHM, SHM, and classical MTL. Among them, DHM and SHM perform similarly and much better\nthan MTL. Note that the degree of heterogeneity of Omniglot is higher than Mini-Imagenet, so the\nperformance gap between each algorithm is much larger on Omniglot datasets. This result demon-\nstrates the robustness of DHM for heterogeneous tasks. In addition, we compare the performance of\nDHM with SHM learning on homogeneous 5-way 1-shot test tasks. We use the same raw data for\nboth of them. We construct heterogeneous tasks for DHM and construct 5-way 1-shot homogeneous\ntasks for SHM. As shown in Table 4, the performance of the DHM is comparable to SHM, with\naccuracy difference at most 0.05%.\nTo further support the above experimental result, we also visualize the representation stability of\nthe above three models under the task heterogeneous condition. As shown in Figure 3, during the\nlearning process, the body of DHM obtains the most stable representation.\n3\nDHM-UHT\nSince meta-learning has better robustness to label noise and heterogeneous tasks, the converse\nis true: meta-learning should have better performance under the same unsupervised annotation\ncapacity constraints. As a result, we propose DHM-UHT, a dynamic head meta-learning algorithm\nwith unsupervised heterogeneous task construction.\nThe overview of the proposed method is\nshown in Figure 1. The core idea of our method is to meta-learn the process of Unsupervised\nHeterogeneous Task Construction. In other words, we put the process of UHT in the inner-loop.\nIn addition, in order to accommodate and utilize heterogeneous tasks construct by DBSCAN,\nwe re-initialize a dynamic head for each task. Note that UHT is similar to DeepCluster, with\nthe difference that we substitute K-means to DBSCAN there. To avoid overﬁtting to sampling\nnoise, we dropout the cluster with relatively small scale, when the training epoch exceeds a certain\n5\nthreshold (hyperparameter ”min sample” in DBSCAN). We have two reasons to use DBSCAN. On\nthe one hand, DBSCAN has higher ﬂexibility compared to K-means due to the unﬁxed number\nof clusters.\nOn the other hand, learning on the heterogeneous tasks constructed by DBSCAN\neffectively exploits the robustness of meta-learning to heterogeneous.\nTraining and Testing. During training phase, by sampling the dataset D, we obtain {T1, T2, ..., Tn}\nas the input of the meta-learning model fθb. For each Ti, we copy a body fθb\ni from fθb and initialize\na head fθh\ni to learn the task in an unsupervised manner. When all the inner loop and all the task Ti\nare learned 1, we perform update to optimize fθb and ﬁnish one step of outer loop. The optimization\nproblem corresponding to train fθb can be written as:\narg min\nθb\n\u0001\nTi∈D\nLUHT (fθb, Ti),\n(2)\nand one iterative solution can be written as:\nfθb = fθb −η∇\n\u0001\nTi∈D\nLUHT (fθb, Ti).\n(3)\nWhen all outer loops are ﬁnished, we obtain a well trained neural network fθb∗.\nDuring test-\ning phase, our method can evaluate in an unsupervised manner. Depending on the requirements\nof the few-shot and zero-shot, we can choose to perform or not to perform inner loop to ﬁne-tune fθb.\nUnsupervised Heterogeneous Task Construction – UHT. The process of UHT is shown in the\nbubble frame. For the samples in Ti, we use fθb to project them in a embedding space, and then\nuse DBSCAN to divide the embedding representation into multiple clusters. Since meta-learning\nmodel don’t care the data-label dependency category Yin et al. (2020), we take the serial number\nof the cluster as the samples pseudo-labels. Finally, to achieve gradient computation and back-\npropagation, we initialize a fully connect layer as the head of neural network, and use CrossEntropy\n(CE) as loss function. Now we have sample, label, loss function and meta-objective Hospedales\net al. (2021) (i.e., the process of UHT), so we can perform inner-loop to update fθi and outer-loop\nto update fθb. The loss used in the outer loop, i.e., loss of the process of UHT can be written as:\nLUHT i = Linner(fθ\u0002\ni, Ti)\n(4)\nwhere fθi = fθb\ni ◦fθh\ni , and fθ\u0002\ni is the updated base learner:\nfθ\u0002\ni = fθi −α∇Linner(fθi, Ti),\n(5)\nwhere the loss of inner loop can be written as:\nLinner(fθi, Ti) =\n\u0001\nxi∈Ti\nCE(fθb\ni ◦fθh\ni (xi), fc ◦fθh\ni (xi)).\n(6)\nNote that we use a MAML-style update strategy for base learner (i.e., Equation 5), for efﬁciency\nreasons, we can also use an ANIL-style update strategy. We will compare these two approaches\nin Section 4.4. Also note that during the above process, we can utilize the classical support set &\nquery set split to calculate LUHT and Linner, or just use Ti to calculate both of them Nichol et al.\n(2018).\nDynamic Head Meta-Learning – DHM. In the meta-learning phase, we use gradient base meta-\nlearning as training framework, e.g., MAML and ANIL. As shown in the right part of Figure 1, we\ndivide the neural network θ to two part, body θb and head θh. The body θb is ﬁxed and can be shared\nby each task. The heads θh are dynamic and are customized to different tasks. In the scenarios of\nfew-shot classiﬁcation, the heterogeneity among tasks come from the difference in the number of\nclassiﬁcation ways, so for these heterogeneous tasks we use fully connected layers as heads with\ndifferent length. For the same reason as DBSCAN, we use dynamic head here is to accommodate\nheterogeneous tasks and to exploit the robustness of meta-learning to heterogeneous tasks.\n1For convenience, we roughly take a batch of data Ti as a task.\n6\nAlgorithm 1 Dynamic Meta-Learning\n1: Require: Dataset D; Neural network body\nfθb; Cluster algorithm fc.\n2: while not done do\n3:\nfor Ti ∈D do\n4:\nfor Inner loop do\n5:\nInitialize head: fθh\ni\n6:\nfθb\ni ←fθb\n7:\nLinner(fθi, Ti) ←Equation 6\n8:\nfθ\u0002\ni ←fθi −α∇Linner(fθi, Ti)\n9:\nend for\n10:\nLUHT i ←Linner(fθ\u0002\ni, Ti)\n11:\nend for\n12:\nfθb ←fθb −η∇\u0001\nTi∈D LUHT (fθb, Ti)\n13: end while\nFigure 4: Unsupervised Datasets Description\nDatasets\nTesting Task\nCifar10\n10w-0s\nCifar100\n100w-0s\nSTL10\n10w-0s\nImageNet\n1000w-0s\nTiny-Imagenet\n200w-0s\nDomainNet\n345w-0s\nOmniglot\n5w-1s/ 5w-5s/ 20w-1s/ 20w-5s\nMini-Imagenet\n5w-1s/ 5w-5s/ 5w-20s/ 5w-50s\nIt is important to state here that our approach is fundamentally different from other unsupervised\nmeta-learning algorithms. DHM-UHT are the ﬁrst to treat the whole process of unsupervised\nheterogeneous task construction as a meta-object for meta-learning, which implies it directly\noptimized the ability to annotation. Other methods, represented by the CACTUs, use pre-trained\nfeature extractors that are not trained by means of meta-learning. This implies, that such training\nmethod cannot optimize the process of generate pseudo-labels, cannot optimize test objective di-\nrectly, and thus may perform sub-optimally. The process of DHM-UHT is is outlined in Algorithm 1\n(non meta-batch version).\n4\nEXPERIMENT\nIn this section, we answer the following questions:\n• Is DHM-UHT superior compared to the mainstream unsupervised few-shot and zero-shot\nclassiﬁcation algorithms?\n• How effective are the components of DHM-UHT?\n• How sensitive is DHM-UHT to newly introduced hyperparameters?\nAll our experiment results are mean values from ﬁve replicate experiments.\n4.1\nUNSUPERVISED ZERO-SHOT CLASSIFICATION\nWe compare DHM-UHT with several unsupervised representation learning algorithms, i.e.,\nReSSL Zheng et al. (2022), IIC Ji et al. (2019), DeepCluster Caron et al. (2018), BiGAN Donahue\net al. (2017b), MAE He et al. (2022), and NVAE Vahdat & Kautz (2020). For all of these algo-\nrithms, we use K-means to perform downstream classiﬁcation. As shown in Table 4, to demonstrate\nthe unsupervised zero-shot classiﬁcation ability of DHM-UHT, we compare the models on Cifar10,\nCifar100, STL10, Imagenet, Tiny-Imagenet, and DomainNet datasets. The dataset and algorithm\nsetup is detailed in Appendix A.\nTable 5 shows the accuracy of each models on the six datasets. Compared to state-of-the-art meth-\nods ReSSL, DHM-UHT obtains a higher accuracy of 5.37% on average. One of the obvious com-\nparisons is between DHM-UHT, DeepCluster, and IIC. All three methods use a clustering-based\nclassiﬁcation strategy during training phase, however Deepcluster and IIC do not perform the meta-\nlearning process, and thus cannot directly optimize for the ability to perform a few-shot, let alone a\nzero-shot learning. As a result, they may be more susceptible to biased interference from samples\nin a zero-shot scenario. MAE, NVAE, and BiGAN exhibit similar performance. They are essen-\ntially structured as auto-encoders that learn features indirectly by learning reconstruction process.\nIn contrast to DHM-UHT, they are not only unable to optimize directly for downstream task objec-\ntives, but also unable to obtain resistance to noise from few-shot or zero-shot setting. ReSSL is a\n7\nTable 5: Accuracy in % on unsupervised zero-shot scenario\nCIFAR-10\nCIFAR-100\nSTL-10\nImageNet\nTiny-MINIST\nDomainNet\nDHM-UHT\n72.15 ± 1.09\n42.34 ± 1.41\n59.74 ± 1.35\n30.12 ± 1.06\n85.45 ± 1.28\n21.68 ± 0.91\nReSSL\n70.27 ± 1.28\n41.48 ± 1.60\n58.52 ± 1.31\n31.25 ± 1.13\n83.17 ± 1.24\n21.42 ± 0.92\nIIC\n64.05 ± 1.02\n36.23 ± 1.27\n53.78 ± 1.30\n25.07 ± 0.88\n79.21 ± 1.54\n18.18 ± 0.74\nMAE\n68.83 ± 1.19\n39.11 ± 1.52\n56.19 ± 1.47\n27.32 ± 1.14\n81.03 ± 1.36\n20.53 ± 1.03\nNVAE\n67.43 ± 1.37\n38.29 ± 1.45\n55.78 ± 1.22\n27.21 ± 0.98\n81.52 ± 1.61\n19.84 ± 0.79\nDeepCluster\n63.02 ± 1.14\n35.05 ± 1.11\n52.21 ± 1.42\n24.83 ± 0.95\n78.63 ± 1.68\n18.09 ± 0.88\nBiGAN\n67.61 ± 1.24\n38.78 ± 1.19\n55.24 ± 1.34\n26.85 ± 1.07\n80.09 ± 1.27\n19.23 ± 0.95\nTable 6: Accuracy in % on unsupervised few-shot scenario\nOmniglot\nMini-Imagenet\n(way, shot)\n(5, 1)\n(5, 5)\n(20, 1)\n(20, 5)\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\nDHM-UHT\n93.75 ± 0.46\n97.71 ± 0.37\n82.15 ± 0.41\n91.88 ± 0.40\n44.73 ± 1.01\n56.54 ± 0.78\n67.30 ± 0.95\n70.23 ± 1.07\nPsCo\n93.25 ± 0.59\n97.56 ± 0.34\n82.06 ± 0.43\n91.01 ± 0.45\n42.90 ± 0.95\n54.87 ± 0.94\n65.66 ± 1.05\n69.94 ± 1.11\nMeta-GMVAE\n93.81 ± 0.75\n96.85 ± 0.50\n81.29 ± 0.62\n89.00 ± 0.51\n41.78 ± 1.13\n54.15 ± 0.87\n62.11 ± 1.14\n67.11 ± 1.10\nUMTRA\n82.97 ± 0.68\n94.84 ± 0.60\n73.51 ± 0.53\n91.22 ± 0.59\n39.14 ± 1.02\n49.21 ± 0.90\n57.66 ± 1.02\n59.68 ± 1.17\nCACTUs-MA-DC\n67.98 ± 0.80\n87.07 ± 0.63\n47.48 ± 0.59\n72.21 ± 0.54\n39.11 ± 1.08\n53.40 ± 0.88\n63.00 ± 1.06\n68.62 ± 1.12\nCACTUs-Pr-DC\n67.08 ± 0.72\n82.97 ± 0.64\n46.32 ± 0.51\n65.75 ± 0.62\n38.47 ± 1.14\n53.01 ± 0.91\n61.05 ± 1.09\n62.82 ± 1.08\nCACTUs-MA-Bi\n57.84 ± 0.75\n78.12 ± 0.67\n34.98 ± 0.57\n57.75 ± 0.58\n36.13 ± 1.07\n50.45 ± 0.90\n60.97 ± 1.16\n66.34 ± 1.14\nCACTUs-Pr-Bi\n53.58 ± 0.65\n71.21 ± 0.68\n32.79 ± 0.53\n50.12 ± 0.51\n36.05 ± 1.06\n49.87 ± 0.92\n58.47 ± 0.09\n62.56 ± 1.10\nMAML (oracle)\n94.46\n98.83\n84.6\n96.29\n46.81\n62.13\n71.03\n75.54\nProtoNets (oracle)\n98.35\n99.58\n95.31\n98.81\n46.56\n62.29\n70.05\n72.04\nrelation-based self-supervised algorithm, since it focuses on the relationships between different in-\nstances rather than instance level information. Similar to the Siamese network in few-shot learning,\nit achieves sub-optimal performance here.\n4.2\nUNSUPERVISED FEW-SHOT CLASSIFICATION\nWe compare our DHM-UHT with several state-of-the-art few-shot unsupervised meta-learning clas-\nsiﬁcation algorithms, i.e., CACTUs Hsu et al. (2019), UMTRA Khodadadeh et al. (2019), Meta-\nGMVAE Lee et al. (2021), and PsCo Jang et al. (2023a). Note that by varying the meta-learning\ntraining approach and the unsupervised embedding algorithm, there are four implementations of\nCACTUs, i.e., CACTUs-MA-DC, CACTUs-Pr-DC, CACTUs-MA-Bi, and CACTUs-Pr-Bi. Pr rep-\nresents ProtoNet Snell et al. (2017), MA represents MAML, DC represents DeepCluster, Bi repre-\nsents BiGAN. As shown in Table 4, we evaluate the models on Omniglot and Mini-Imagenet datasets\nwith several few-shot tasks. The datasets and algorithm setup is detailed in Appendix A.\nTable 6 shows the accuracy of each models on the two datasets. Comparing to state-of-the-art al-\ngorithm, DHM-UHT obtains a higher accuracy of 3.52% on average. It can be seen that none of\nother algorithms include the process of unsupervised task construction in inner-loop, thus cannot\noptimize the ability of pseudo labels annotation. Among them, PsCo obtains relatively good per-\nformance due to its use of Pseudo-supervised Contrast, to target the meta-learning reliance on the\nimmutable pseudo-labels. Note that UMTRA obtains the worst results on the Mini-Imagenet dataset.\nThe reason is that UMTRA essentially only implements the 1-shot task construct during training.\nAlthough it uses various data augmentation methods to increase the number of shots in the con-\nstructed task, it still suffers from overﬁtting. We can ﬁnd that the difference in performance between\nDHM-UHT and UMTRA increases as the number of shots increases. On the other hand, since UM-\nTRA underlies a statistical assumption on sample labeling, its performance degrades rapidly due to\nmislabel in scenarios where the total number of classes is small and the number of classes in a single\ntask is big.\n4.3\nABLATION STUDY\nWe perform ablation experiment on both unsupervised few-shot and zero shot datasets, i.e., Om-\nniglot, Cifar100, and STL10 datasets. In the ﬁrst control group (G1), we use K-means to generate\nhomogeneous tasks and use static head to learn these tasks. In the second group (G2), we don’t\nmeta-learn the whole process of unsupervised heterogeneous tasks construction, instead, we gen-\n8\nTable 8: Comparison of UHT-MAML and UHT-ANIL on performance-overhead trade off. We\nrecorded the computational time (second) required for both strategy to reach the same classiﬁcation\naccuracy, on Omniglot dataset with 5-way 1-shot setting.\nAccuracy%\nUHT-ANIL\nUHT-MAML\n50.0\n655\n748\n60.0\n1429\n1681\n70.0\n2730\n2992\n80.0\n5564\n5642\n90.0\n13987\n14723\n93.5\n/\n25717\nOmniglot dataset with 5-way 1-shot task, UHT updated by ANIL strategy can be calculated at a\nmuch faster rate. However, UHT-MAML can reach higher Accuracy ultimately. Therefore, in prac-\ntice, we need to choose the update strategy based on the accuracy and overhead requirements. Note\nthat the above results are somewhat different from those in Raghu et al. (2020), which may be due to\nthe complexity of the unsupervised task. Updating only the head in the inner loop cannot adequately\nlearn the process of unsupervised task construction. In scenarios where absolute high accuracy is\ndesirable, it seems more effective to use a MAML-style update strategy.\n5\nRELATED WORK\nTheoretical analysis of meta-learning The exploration of meta-learning theories has progressed\nin the last few years. For example, Raghu et al. (2020); Goldblum et al. (2020) unrevealed the\nnature of MAML’s fast adaption, Tian et al. (2020) argue that learning a good embedding may\noutperforms meta-learning in few-shot classiﬁcation scenario. Luo et al. (2023) empirically proved\nthat meta-training algorithm and the adaptation algorithm can be completely disentangled. Chen\net al. (2019) found that baseline can outperform meta-learning in the area of few-shot classiﬁcation\nunder speciﬁc conditions of domain difference and backbone network architecture. Chen et al.\n(2021) discuss the effect of class generalization and novel class generalization on meta-learning.\nGuan & Lu (2022); Jose & Simeone (2021b); Maurer (2005) estimated the generalization error\nupper bound of meta-learning. In this paper, we answer two simple but important questions – why\nand when meta-learning is better than other algorithms in few-shot classiﬁcation?\nUnsupervised meta-learning. Unsupervised meta-learning Hsu et al. (2019); Khodadadeh et al.\n(2019); Lee et al. (2021); Jang et al. (2023a); Ye et al. (2023); Jang et al. (2023b); Lee et al. (2023);\nDong et al. (2022); Khodadadeh et al. (2021) aims to link meta-learning and unsupervised learn-\ning by constructing synthetic tasks and extracting the meaningful information from unlabeled data.\nOur proposed DHM-UHT is the ﬁrst algorithm to meta-learn the whole process of heterogeneous\nunsupervised task construction.\n6\nCONCLUSION\nIn this paper, we answer the question of why and when meta-learning is better than classical learning\nalgorithm in few-shot classiﬁcation. The answer is that meta-learning is more robust to label noise\nand heterogeneous tasks, and that meta-learning has better unsupervised performance under the\nsame constraints of annotation ability. We propose a quantitative approach to measure representation\nstability, and further to analyse the manner of meta-learning and other learning algorithms during\ntraining process. In the pre-experiment we ﬁnd that meta-learning algorithm is more robust to label\nnoise and task heterogeneous, cause it can train neural network in a more rational way, i.e., bi-\nlevel optimization. To utilize the robustness of meta-learning, we propose DHM-UHT, a dynamic\nhead meta-learning algorithm with unsupervised heterogeneous task construction. It’s the ﬁrst meta-\nlearning algorithm treat the whole process of unsupervised heterogeneous task construction as meta-\nobjective, and exhibit state-of-the-art performance on unsupervised zero-shot and few-shot datasets.\n10\nREFERENCES\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-\npervised learning of visual features. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu,\nand Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Ger-\nmany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes in Computer\nScience, pp. 139–156. Springer, 2018.\nJiaxin Chen, Xiao-Ming Wu, Yanke Li, Qimai Li, Li-Ming Zhan, and Fu-Lai Chung. A closer look\nat the training strategy for modern meta-learning. In Hugo Larochelle, Marc’Aurelio Ranzato,\nRaia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer\nlook at few-shot classiﬁcation. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\nYinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, and Xiaolong Wang. Meta-baseline: Ex-\nploring simple meta-learning for few-shot learning. In 2021 IEEE/CVF International Conference\non Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pp. 9042–9051.\nIEEE, 2021.\nJeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net, 2017a.\nJeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net, 2017b.\nXingping Dong, Jianbing Shen, and Ling Shao. Rethinking clustering-based pseudo-labeling for un-\nsupervised meta-learning. In Shai Avidan, Gabriel J. Brostow, Moustapha Ciss´e, Giovanni Maria\nFarinella, and Tal Hassner (eds.), Computer Vision - ECCV 2022 - 17th European Conference,\nTel Aviv, Israel, October 23-27, 2022, Proceedings, Part XX, volume 13680 of Lecture Notes in\nComputer Science, pp. 169–186. Springer, 2022.\nMartin Ester, Hans-Peter Kriegel, J¨org Sander, and Xiaowei Xu. A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. In Evangelos Simoudis, Jiawei Han,\nand Usama M. Fayyad (eds.), Proceedings of the Second International Conference on Knowledge\nDiscovery and Data Mining (KDD-96), Portland, Oregon, USA, pp. 226–231. AAAI Press, 1996.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70, pp. 1126–1135. PMLR, 2017.\nMicah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Gold-\nstein. Unraveling meta-learning: Understanding feature representations for few-shot tasks. In\nProceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July\n2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 3607–3616.\nPMLR, 2020.\nJiechao Guan and Zhiwu Lu. Task relatedness-based generalization bounds for meta learning. In\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022. OpenReview.net, 2022.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross B. Girshick. Masked\nautoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 15979–15988.\nIEEE, 2022. doi: 10.1109/CVPR52688.2022.01553.\nTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural\nnetworks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):\n5149–5169, 2021.\n11\nKyle Hsu, Sergey Levine, and Chelsea Finn.\nUnsupervised learning via meta-learning.\nIn 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net, 2019.\nHuiwon Jang, Hankook Lee, and Jinwoo Shin. Unsupervised meta-learning via few-shot pseudo-\nsupervised contrastive learning. In The Eleventh International Conference on Learning Repre-\nsentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a.\nHuiwon Jang, Hankook Lee, and Jinwoo Shin. Unsupervised meta-learning via few-shot pseudo-\nsupervised contrastive learning. In The Eleventh International Conference on Learning Repre-\nsentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b.\nXu Ji, Andrea Vedaldi, and Jo˜ao F. Henriques. Invariant information clustering for unsupervised\nimage classiﬁcation and segmentation. In 2019 IEEE/CVF International Conference on Computer\nVision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 9864–9873. IEEE,\n2019.\nSharu Theresa Jose and Osvaldo Simeone. Information-theoretic generalization bounds for meta-\nlearning and applications. Entropy, 23(1):126, 2021a.\nSharu Theresa Jose and Osvaldo Simeone. An information-theoretic analysis of the impact of task\nsimilarity on meta-learning. In IEEE International Symposium on Information Theory, ISIT 2021,\nMelbourne, Australia, July 12-20, 2021, pp. 1534–1539. IEEE, 2021b.\nSiavash Khodadadeh, Ladislau B¨ol¨oni, and Mubarak Shah. Unsupervised meta-learning for few-\nshot image classiﬁcation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 10132–10142, 2019.\nSiavash Khodadadeh, Sharare Zehtabian, Saeed Vahidian, Weijia Wang, Bill Lin, and Ladislau\nB¨ol¨oni. Unsupervised meta-learning through latent-space interpolation in generative models. In\n9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net, 2021.\nDong Bok Lee, Dongchan Min, Seanie Lee, and Sung Ju Hwang. Meta-gmvae: Mixture of gaussian\nVAE for unsupervised meta-learning. In 9th International Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\nDong Bok Lee, Seanie Lee, Kenji Kawaguchi, Yunji Kim, Jihwan Bang, Jung-Woo Ha, and Sung Ju\nHwang.\nSelf-supervised set representation learning for unsupervised meta-learning.\nIn The\nEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net, 2023.\nXu Luo, Hao Wu, Ji Zhang, Lianli Gao, Jing Xu, and Jingkuan Song. A closer look at few-shot\nclassiﬁcation again. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\nSivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning\nResearch, pp. 23103–23123. PMLR, 2023.\nAndreas Maurer. Algorithmic stability and meta-learning. J. Mach. Learn. Res., 6:967–994, 2005.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. volume\nabs/1803.02999, 2018.\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?\ntowards understanding the effectiveness of MAML. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. SVCCA: singular vector\ncanonical correlation analysis for deep learning dynamics and interpretability. In Isabelle Guyon,\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\nRoman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, pp. 6076–6085, 2017a.\n12\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. SVCCA: singular vector\ncanonical correlation analysis for deep learning dynamics and interpretability. In Isabelle Guyon,\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\nRoman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, pp. 6076–6085, 2017b.\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning.\nIn Advances in Neural Information Processing Systems 30: Annual Conference on Neural Infor-\nmation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4077–4087,\n2017.\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and Phillip Isola. Rethinking\nfew-shot image classiﬁcation: A good embedding is all you need?\nIn Andrea Vedaldi, Horst\nBischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIV, volume 12359\nof Lecture Notes in Computer Science, pp. 266–282. Springer, 2020.\nArash Vahdat and Jan Kautz.\nNVAE: A deep hierarchical variational autoencoder.\nIn Hugo\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nJoaquin Vanschoren. Meta-learning: A survey. CoRR, abs/1810.03548, 2018.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. Advances in neural information processing systems, 29, 2016.\nHan-Jia Ye, Lu Han, and De-Chuan Zhan. Revisiting unsupervised meta-learning via the character-\nistics of few-shot tasks. IEEE Trans. Pattern Anal. Mach. Intell., 45(3):3721–3737, 2023. doi:\n10.1109/TPAMI.2022.3179368.\nMingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning\nwithout memorization. In 8th International Conference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\nMingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang\nXu. Relational self-supervised learning. CoRR, abs/2203.08717, 2022.\nA\nAPPENDIX\nA.1\nDATASET SETUP\nOmniglot. The raw dataset contains 1628 classes, we split the classes of training set, evaluation\nset, test set into 800: 400: 432. We use Omniglot in three scenario. The ﬁrst scenario is in 2.1.\nWe perform supervised few-shot learning with label noise. We randomly mask the labels of the\nsamples in the training set according to the noise ratio (i.e., 0%, 15%, 30%). Depending on the\ntraining method, we can construct these raw data into task by Finn et al. (2017), or use them directly\nfor whole class training by Tian et al. (2020). The second scenario is in Section 2.2. We perform\nsupervised few-shot learning with heterogeneous tasks. When constructing heterogeneous tasks, we\nsample a variable number of classes, to ensure the difference in the way of tasks (i.e., 5-20 way), and\nfurther to ensure the heterogeneity. The third scenario is in 4.2. We perform unsupervised few-shot\nlearning. We follow the protocol given by Hsu et al. (2019).\nMini-Imagenet. The raw Mini-Imagenet contains 100 classes, we the split classes of training set,\nevaluation set, test set into 64: 16: 20. We use Mini-Imagenet in three scenario. The details of\n13\nthe setup of the three experimental scenarios are the same as Omniglot. With the except that we\nconstruct 5-10 way heterogeneous task in Section 2.2.\nCIFAR-10, CIFAR-100, STL-10, Imagennet, and Tiny Imagenet. For CIFAR-10, CIFAR-100,\nSTL-10, Imagennet, and Tiny Imagenet datasets, we follow the protocol given by Zheng et al.\n(2022). They are used for unsupervised zero-shot learning, so we mask all the labels in training\nset.\nDomainNet. DomainNet is a domain adaption dataset. We use it to evaluate algorithms’ ability of\nunsupervised zero-shot domain adaption. It contains 6 domain with 345 classes for each domain.\nWe use one domain for test and the remain 5 domain for both training and validating. Note that\nwhen constructing tasks, we sample classes from the same domain and we mask all the labels in\ntraining set.\nA.2\nALGORITHM SETUP\nDHM-UHT We use DHM-UHT in both unsupervised zero-shot and few-shot scenario.\nIn\nunsupervised\nfew-shot\ndatasets,\nwe\nfollow\nthe\nsame\nbackbone\narchitecture\ngiven\nby github.com/dragen1860/MAML-Pytorch.\nWe set epoch, inner-loop learning rate, outer-loop\nlearning rate, meta-batch size, adaption steps for evaluation and sub-sample size, as 30000, 0.05,\n0.001, 8, 50, and 100 respectively. For DBSCAN in UHT, we set min samples and eps as 15 and 1.0,\nrespectively. In unsupervised zero-shot datasets (except of DomainNet), we follow the same back-\nbone architecture given by github.com/xu-ji/IIC, i.e., ResNet and VGG11. We set epoch, inner-loop\nlearning rate, outer-loop learning rate, meta-batch size, adaption steps for evaluation and sub-sample\nsize, as 80000, 0.001, 0.001, 8, 0, and 100 respectively. For DBSCAN in UHT, we set min samples\nand eps as 15 and 1.0, respectively. For DomainNet dataset, we use ResNet-9 as backbone archi-\ntecture, which is the same as github.com/liyunsheng13/DRT. The other conﬁguration is the same as\nother unsupervised zero-shot datasets.\nWCT, MAML, ANIL, and MTL In Section 2, we use WCT, MAML, ANIL, and MTL to perform\npre-experiment, on Omniglot and Mini-Imagenet datasets. For MAML, we use embedding function\nused by Vinyals et al. (2016), which has 4 modules with a 3 × 3 convolutions and 64 ﬁlters, followed\nby batch normalization, a ReLU nonlinearity, and 2 × 2 max-pooling. The Omniglot images are\ndownsampled to 28 × 28, so the dimensionality of the last hidden layer is 64. As in the baseline\nclassiﬁer used by 2, the last layer is fed into a softmax. For Omniglot, we used strided convolutions\ninstead of max-pooling. For ANIL, we only adjust the update strategy. For WCT, we use the same\nneural network architecture and learning conﬁguration, with the except of the last layer (whose\ndimensions are the same as the number of categories). For MTL, we maintain the same setting with\nANIL, except for bi-level optimization strategy.\nPsCo, Meta-GMVAE, UMTRA, and CACTUs. We reuse the entire the conﬁguration give by Jang\net al. (2023a), Lee et al. (2021), Khodadadeh et al. (2019), Hsu et al. (2019), cause our test scenarios\nare the same as them.\nReSSL and IIC In CIFAR-10, CIFAR-100, STL-10, ImageNet, and Tiny ImageNet datasets, we\nreuse the entire the conﬁguration describe in Zheng et al. (2022) and Ji et al. (2019). In Domain-\nNet dataset, for a fair comparison, we use ResNet-9 as backbone and maintain the same learning\nconﬁguration as mentioned above.\nMAE and NVAE. Due to computational resource constraints, and in order to replicate the two\napproaches as much as possible, we use ViT-base (instead of ViT-Large) as backbone. In DomainNet\ndataset, we also use ResNet-9 as backbone. The learning conﬁguration is in line with original.\nDeepCluster. We run DeepCluster for each unsupervised zero-shot dataset, which we respectively\nrandomly crop and resize to the appropriate image size. We modify the ﬁrst layer of the AlexNet\narchitecture used by the authors to accommodate this input size. We follow the authors and use the\ninput to the (linear) output layer as the embedding. These are 4096-dimensional, so we follow the\nauthors and apply PCA to reduce the dimensionality to 256, followed by whitening. Our conﬁgu-\nration is built upon github.com/facebookresearch/ deepcluster. In DomainNet dataset, we also use\nResNet-9 as backbone.\n14\nBiGAN. We follow the BiGAN authors and specify a uniform 50-dimensional prior on the unit\nhypercube for the latent. They use a 200 dimensional version of the same prior for their ImageNet\nexperiments, so we follow suit for our unsupervised zero-shot dataset. They randomly crop to 64\n× 64 and use the AlexNet-inspired architecture used by Donahue et al. (2017a) for their Imagenet\nresults. Our conﬁguration is built upon github.com/jeffdonahue/bigan. In DomainNet dataset, we\nalso use ResNet-9 as backbone.\n15\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-03",
  "updated": "2024-10-13"
}