{
  "id": "http://arxiv.org/abs/2410.02521v2",
  "title": "Methods of Automatic Matrix Language Determination for Code-Switched Speech",
  "authors": [
    "Olga Iakovenko",
    "Thomas Hain"
  ],
  "abstract": "Code-switching (CS) is the process of speakers interchanging between two or\nmore languages which in the modern world becomes increasingly common. In order\nto better describe CS speech the Matrix Language Frame (MLF) theory introduces\nthe concept of a Matrix Language, which is the language that provides the\ngrammatical structure for a CS utterance. In this work the MLF theory was used\nto develop systems for Matrix Language Identity (MLID) determination. The MLID\nof English/Mandarin and English/Spanish CS text and speech was compared to\nacoustic language identity (LID), which is a typical way to identify a language\nin monolingual utterances. MLID predictors from audio show higher correlation\nwith the textual principles than LID in all cases while also outperforming LID\nin an MLID recognition task based on F1 macro (60%) and correlation score\n(0.38). This novel approach has identified that non-English languages (Mandarin\nand Spanish) are preferred over the English language as the ML contrary to the\nmonolingual choice of LID.",
  "text": "Methods for Automatic Matrix Language Determination of Code-Switched\nSpeech\nOlga Iakovenko and Thomas Hain\nThe University of Sheffield\n{oiakovenko,t.hain}@sheffield.ac.uk\nAbstract\nCode-switching (CS) is the process of speakers\ninterchanging between two or more languages\nwhich in the modern world becomes increas-\ningly common. In order to better describe CS\nspeech the Matrix Language Frame (MLF) the-\nory introduces the concept of a Matrix Lan-\nguage, which is the language that provides the\ngrammatical structure for a CS utterance. In\nthis work the MLF theory was used to develop\nsystems for Matrix Language Identity (MLID)\ndetermination. The MLID of English/Mandarin\nand English/Spanish CS text and speech was\ncompared to acoustic language identity (LID),\nwhich is a typical way to identify a language in\nmonolingual utterances. MLID predictors from\naudio show higher correlation with the textual\nprinciples than LID in all cases while also out-\nperforming LID in an MLID recognition task\nbased on F1 macro (60%) and correlation score\n(0.38). This novel approach has identified that\nnon-English languages (Mandarin and Spanish)\nare preferred over the English language as the\nML contrary to the monolingual choice of LID.\n1\nIntroduction\nCode-switching (CS) is the process of speakers\nswitching between two or more languages in spo-\nken or written language (Table 1). Spoken CS\ndata is scarce, and thus models for processing CS\nspeech often yield poor performance in comparison\nto monolingual models. Given that in many coun-\ntries CS is widespread (e.g. India, South Africa,\nNigeria) (Diwan et al., 2021; Ncoko et al., 2000;\nRufai Omar, 1983), it is essential to develop sys-\ntems for understanding and modelling CS speech.\nOne of the critical tasks in analyzing code-switched\nspeech is determining the matrix language (ML), or\nthe dominant language, which serves as the struc-\ntural framework for the utterance. Accurate identi-\nfication of ML is essential for various applications\nas well as sociolinguistic studies.\nTable 1: An example of a CS utterance transcription\nfrom the SEAME dataset of colloquial Singaporean\nlanguage.\næ¯•ä¸šè¿‡åurh ä½ çš„study life è·Ÿä½ çš„working\nlife æœ‰ä»€ä¹ˆdifference å—\nThe linguistic Matrix Language Frame (MLF)\ntheory (Myers-Scotton, 1997) provides a model\nfor CS production and introduces the concept of a\nmain, i.e. dominant language and a secondary, in-\nserted language in CS utterances. These languages\nare ML and Embedded Language (EL), respec-\ntively. The MLF theory introduces two methods\nfor ML determination:\n1. The Morpheme Order Principle - ML will pro-\nvide the surface morpheme order for a CS\nutterance if it consists of singly occurring EL\nlexemes and any number of ML morphemes\n2. The System Morpheme Principle - all system\nmorphemes which have grammatical relations\nexternal to their head constituent will come\nfrom ML\nThe morphemes as units within the MLF frame-\nwork were first introduced by Myers-Scotton in\n1997 (Myers-Scotton, 1997) and were split into\ncontent and system morphemes. Some common\nexamples of system morphemes are quantifiers,\npossessives and tense/aspect determiners, while\ncontent morphemes include nouns, pronouns, ad-\njectives, verbs and prepositions.\nMatrix language identity (MLID) is the iden-\ntity of the language providing the grammatical\nframe for the utterance and it can be defined for\nboth monolingual and CS utterances. Moreover,\nthe existence of ML implies a certain token distri-\nbution following the System Morpheme Principle\n(Myers-Scotton, 2002) which is highlighted in fur-\nther Myers-Scotton works in the 4-M model. Over-\nall, MLID provides insight into the grammatical\narXiv:2410.02521v2  [cs.CL]  14 Nov 2024\nproperties of the utterance and a computational im-\nplementation of an MLID would be able to reduce\nthe amount of manual annotation.\nIn this paper three MLID systems for CS text\nand audio were implemented. MLF theory for-\nmulates The Morpheme Order Principle and the\nSystem Morpheme Principle, which were imple-\nmented into three systems for MLID determina-\ntion from text (P1.1, P1.2 and P2) and from au-\ndio (ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1, ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 and ğ‘€ğ¿ğ¼ğ·ğ‘ƒ2). An\nextensive correlation analysis and comparison of\nan MLID determination system and a traditional\nacoustic language identities (LID) were carried out.\nRecognised MLID and LID from CS texts and au-\ndio were compared to ground truth ML annotation\nand the quality of ML recognition was measured\nin terms of F1 macro and Matthewâ€™s Correlation\nCoefficient (MCC). To conclude the findings, the\ndistributions of textual LIDs were compared to the\ntextual MLID distributions of the CS data.\nThe remainder of the paper is structured as fol-\nlows. The next section reviews the relevant re-\nsearch presented previously on MLF and LID in\nCS text and speech. The third section provides\na detailed description of the methods used. This\nis followed by a section on experiments, which\nprovides information on datasets, detailed imple-\nmentation, experiment descriptions as well as a\ndiscussion of results. Conclusions summarise and\ncomplete the paper.\n2\nRelated work\nMLF theory has rarely been used to automatically\nanalyse speech or text. Up until now it was only\nused for text augmentation (Bhat et al., 2016; Yil-\nmaz et al., 2018; Lee et al., 2019) or for Language\nModel (LM) adaptation for code-switching. For\nexample additional grammatical information was\nused during LM construction (Adel et al., 2015;\nSoto and Hirschberg, 2019) or a self-supervised\ntraining procedure was set up which encouraged\ngeneration of CS utterances (Chang et al., 2019).\nMLID classification for CS text was carried out in\nBullock et al. (2018) where the ML was identified\nbased on the token and system POS majorities.\nSimultaneously in the speech processing domain\na common technique to separate languages in CS\nis LID. LID of a whole CS utterance may be per-\nformed when CS is regarded as a separate language\n(Mary N J et al., 2020), in this case the component\nperforms both LID and CS detection. A multi-\nlingual ASR system with an utterance-wise LID\ncomponent as an auxiliary task was tested for CS\nutterances in Toshniwal et al. (2018) but the model\nwas not able to generate CS text as a result. LID\nand language segmentation (LIS) systems make\ndecisions based on similarity to the data they were\ntrained on (Muralikrishna et al., 2021) and, to the\nbest of our knowledge, no study was done to de-\ntermine if pretrained LID/LIS are able to predict a\ndominant language in a CS utterance.\nNeither MLID nor LID were previously used for\nCS analysis. Furthermore, ML determination prin-\nciples were never fully implemented and compared.\nHowever, statistical methods were introduced be-\nfore (GuzmÃ¡n et al., 2017) which can assess the\nnature of CS. Among the statistical methods only\nthe M-index (Multilingual Index) quantifies the\nrepresentation of the languages in multilingual cor-\npora. While the M-index is useful to learn about\nthe balance of the token LIDs, it might be insuffi-\ncient to learn about the utterance LID and MLID\ndistributions.\nThe above indicates that theoretical methods to\nidentify ML from text exist but previously there\nwas only one attempt to determine MLID which\nwas not based on the two ML determination prin-\nciples. Furthermore, there are no existing MLID\npredictors from audio to the best of our knowl-\nedge. Therefore the objective of this study is to\nadvance technologies for multilingual understand-\ning and analysis by describing the implementation,\ncomparison and performance of automatic MLID\npredictors from CS audio and text based on the ML\ndetermination principles from the MLF theory.\n3\nPrinciples for ML determination\nThe MLF theory principles mentioned above have\nto be implemented in order to compare the main lan-\nguage recognised by an LID system to the MLID.\nEach of them provides estimates for MLID but is\nconditioned by different evidence in the utterance.\nThe Morpheme Order Principle is separated into\ntwo implementations of the 1st Principle (P1.1 and\nP1.2). The implementations of the principle deduc-\ntion as per MLF theory are described below.\n3.1\nPrinciple 1.1: The singleton principle\nThe ML provides the context for singly occurring\nwords from the EL, which will be further referred\nto as \"singleton insertions\". Although the original\nprinciple operates on the level of lexemes, the cur-\nFigure 1: Detection error tradeoff (DET) curve for possible log ğ›¼values. Thin diamond is the default value of\nlog ğ›¼= 0, thick diamond - result of log ğ›¼estimation, red star - ground truth log ğ›¼.\nrent implementation operates on the level of words.\nSuppose there is a CS utterance y of length ğ‘›, then\n(yâ€², lâ€²) = ((ğœ€, ğ‘™ğœ€), (ğ‘¦1, ğ‘™1), .., (ğ‘¦ğ‘›, ğ‘™ğ‘›), (ğœ€, ğ‘™ğœ€)) are\nmorphemes with corresponding language ID la-\nbels, ğœ€is an empty morpheme and ğ‘™ğœ€is an empty\nlanguage morpheme tag from an empty language\nğ¿ğœ€. If ((ğ‘¦ğ‘–, ğ‘™ğ‘–), .., (ğ‘¦ğ‘—, ğ‘™ğ‘—)) constitute a word where\n0 < ğ‘–< ğ‘—< ğ‘›+ 1, âˆ€ğ‘˜that ğ‘–< ğ‘˜< ğ‘—| ğ‘™ğ‘˜= ğ¿2\nand ğ‘™ğ‘–âˆ’1, ğ‘™ğ‘—+1 âˆˆğ¿1, ğ¿ğœ€then the language of the\ncontext ğ¿1 is the ML while ğ¿2 is the embedded\nlanguage. For example in â€œå“¦ä½ post åœ¨ä½ çš„é‚£ä¸ª\nblogâ€œ Mandarin will be ML since it is a context for\nEnglish singleton insertions.\n3.2\nPrinciple 1.2: The token order principle\nFigure 2: Pipeline of the morpheme order-based princi-\nple for ML determination P1.2.\nIn P1.2 the second part of the Morpheme Order\nPrinciple is implemented which postulates that the\nmorpheme order is determined by the ML. For ex-\nample, in â€œä½ è§‰å¾—æˆ‘ä»¬speak clear enough å—â€œ the\nEnglish translation of the auxiliary Mandarin verb\nå—will never appear at the end of an utterance in\nEnglish, signifying that Mandarin is ML in this\nutterance. Assume languages (ğ¿1, ğ¿2) âŠ‚ğ¿are\npresent in a bilingual utterance where ğ¿are all lan-\nguages, then the original CS utterance y can be\ntranslated into two monolingual utterances Ë†yğ¿1 and\nË†yğ¿2. Ë†yğ¿1 and Ë†yğ¿2 are obtained from the original ut-\nterance y by a Neural Machine Translation (NMT)\nsystems ğ‘”ğ¿1 and ğ‘”ğ¿2. Consider an LM ğ‘ƒ(ğ¿|y)\nwhich is used to provide a probability of the utter-\nance belonging to a language ğ¿, then given the two\nlanguages ğ¿1 and ğ¿2 classification leads to:\nğ‘ƒ(y|ğ¿1)ğ‘ƒ(ğ¿1)\nğ‘ƒ(y)\nâ‹šğ‘ƒ(y|ğ¿2)ğ‘ƒ(ğ¿2)\nğ‘ƒ(y)\n(1)\nIn the above the denominator may be eliminated.\nThe probability ğ‘ƒ(y|ğ¿) may be estimated using in-\ndependent monolingual LMs ğ‘ƒğ¿(y) and translation\nË†yğ¿defined above ğ‘ƒ(y|ğ¿) â‰ˆğ‘ƒğ¿(Ë†yğ¿) resulting in\nfollowing:\nğ‘ƒğ¿1(Ë†yğ¿1)\nğ‘ƒğ¿2(Ë†yğ¿2) â‹šğ›¼\n(2)\nWhere ğ›¼is the scaling factor for weighing the\nimpact of the models. Taking the log 2 leads to:\nlog ğ‘ƒğ¿1(Ë†yğ¿1) âˆ’log ğ‘ƒğ¿2(Ë†yğ¿2) â‹šlog ğ›¼\n(3)\nAssume\nthe\ndifference\nof\nlanguage\nlog-\nprobabilities can be expressed in terms of a factor\nğ›¼. This factor may be estimated by calculating the\nexpectation of log-probabilities using utterances\nscored by monolingual LMs:\nlog ğ›¼= E{log ğ‘ƒğ¿1(yğ¿1)} âˆ’E{log ğ‘ƒğ¿2(yğ¿2)} (4)\nAll of the above leads to the following decision\nfunction:\nTable 2: Monolingual dataset splits used for LM training in P1.2.\nUnit\nCallhome\nEnglish\nCallhome\nMandarin\nCallhome\nSpanish\nToken set size\ntokens\n6160\n6853\n3236\nTrain\n20029\n15827\n19672\nValid\nutterances\n6030\n3959\n5500\nTest\n2609\n1775\n2665\nML =\n(\nğ¿1, log ğ‘ƒğ¿1(Ë†yğ¿1) âˆ’log ğ‘ƒğ¿2(Ë†yğ¿2) â‰¥log ğ›¼\nğ¿2, otherwise\n(5)\nA visual representation of the resulting algorithm\nis shown in Figure 2.\n3.3\nPrinciple 2: The system word principle\nFrom the examples in Section 1 it is evident\nthat there exists an overlap of content/system\nmorphemes duality with the traditional con-\ntent/function words opposition defined in linguis-\ntics, although they are not equivalent and the tra-\nditional classifications also not strictly distinguish-\nable. Therefore in the implementation of the 2\nPrinciple (P2) for ML determination instead of\ncontent/system morpheme duality (Myers-Scotton,\n2002) a content/function Part of Speech (POS) du-\nality is considered.\nSystem POS are identified, namely determiners,\nauxiliaries, subordinating conjunctions and coor-\ndinating conjunctions, while the rest of POS are\nconsidered as content POS. ML is determined in a\nCS utterance if one of the participating languages\nprovided function POS for the utterance and the\nother language did not. The language that pro-\nvided the function POS is determined as the ML.\nAlthough CS POS taggers exist (Feng et al., 2022;\nBansal et al., 2022) none of them are available in\nopen-source and since training a POS tagger is not\na goal of this work a monolingual POS tagger is\nused instead. For example in the utterance â€œim\nokay with the è›‹é»„â€œ determiner â€œtheâ€œ is used and\ntherefore ML is determined as English.\n4\nExperiments\n4.1\nDatasets\nThe experiments using the algorithms described\nabove are carried out using monolingual Call-\nhome subsets and 2 CS datasets: SEAME (Lyu\net al., 2010) and Miami subcorpus from the Miami-\nBangor corpus1.\n4.1.1\nMonolingual data\nMonolingual LM training for P1.2 was carried out\nusing Callhome datasets for English2, Mandarin3\nand Spanish4. Pretrained LMs were not used in\nthis work because they do not provide likelihood\nscoring of morpheme units. The summary of the\ndatasets is presented in Table 2.\n4.1.2\nCS data\nCS spoken language corpora SEAME (120 hours)\nand Miami (35 hours) are used for analysis and\nacoustic MLID training. Agreement analysis is car-\nried out for CS utterances from the SEAME and\nMiami corpora and monolingual SEAME and Mi-\nami utterances are used for estimating the scaling\nfactor ğ›¼(Table 4). The monolingual subsets of\nSEAME and Miami are also used for training the\nmapping from the LID outputs to English, Man-\ndarin and Spanish posteriors (ğ¿ğ¼ğ·ğ‘šğ‘ğ‘). Mandarin\ncharacters in the SEAME corpus are word seg-\nmented which is helpful when applying P1.1, a\nprinciple that operates with words. All the intro-\nduced principles require morpheme-level LID tag\nannotation which is available for Miami and is au-\ntomatically determined for SEAME based on the\nscript (latin vs logographic). Finally, additional\nMLID-annotated 91 CS Miami utterances were\nused to measure the quality of MLID prediction\nfrom text and audio. The annotated MLID labels\nwere assigned to the CS utterance transcriptions on\nthe basis of determiner-noun-adjective complexes\n(Parafita Couto and Gullberg, 2017).\n1https://biling.talkbank.org/access/Bangor/Miami.html\n2LDC97T14\n3LDC96T16\n4LDC96T17\nTable 3: Examples of applying the principles.\nUtterance\nbaseline\nML P1.1\nML P1.2\nML P2\ni thought all trains éƒ½æ˜¯via jurongeast å»åˆ°pasirris\nen\nen\nen\nen\nbut ä»–è›®zai çš„right\nen\nzh\nen\nen\nbut æˆ‘çš„parents éƒ½æ²¡æœ‰sponsor æˆ‘\nzh\nzh\nzh\nen\nè¿˜æœ‰chicken noodles\nen\nen\nen\nzh\nTable 4: CS dataset splits.\nUnits\nSEAME\nMiami\nMonolingual\n53086\n38401\nCS raw\nutterances\n56951\n2425\nCS annotated\n-\n91\n4.2\nApplying ML Principles to utterance\ntranscriptions\nP1.1 ML only applies to utterance transcriptions\nwith singleton insertions, therefore resulting in a\nsmall data coverage: only for 36% (SEAME) and\n60% (Miami) of the CS data the ML is determined.\nIn P2 POS tags are computed for constituent mono-\nlingual islands (segments) of a CS utterance using\na pretrained CNN-based POS tagger (Svenstrup\net al., 2017). P2 covered 31% (SEAME) and 58%\n(Miami) of all of the CS examples. Furthermore,\na baseline MLID determiner from text is imple-\nmented which is based on the token LID count\nfollowing Bullock et al. (2018). Examples of run-\nning the resulting principles implementations is\npresented in the Table 3.\nThe implementation of P1.2 includes three com-\nponents: a Machine Translation (MT) system, a\npseudomorpheme tokeniser and a language model\n(LM). CS utterances are translated word by word\nusing Wiktionary5 to preserve the token order. The\nEnglish and Spanish LMs are trained on the to-\nkenised English and Spanish Callhome datasets.\nThe tokenisation was carried out using a stemmer\nwhere stem and affix would be separated. For the\nMandarin Callhome dataset separate characters are\nregarded as morphemes. The two Transformer-\nbased (Vaswani et al., 2017) LMs with 2 lay-\ners, 2 attention heads per layer are trained for 25\nepochs with negative log-likelihood loss on one\n3080 Nvidia GPU for 1 hour. Validation and test\nperplexities for the three languages are presented\nin Table 5.\n5https://www.wiktionary.org/\nTable 5: Perplexities calculated for the validation and\ntest subsets of monolingual Callhome data.\nEnglish\nMandarin\nSpanish\nValid\n48.97\n94.98\n57.76\nTest\n57.61\n98.16\n52.30\nMoreover, a preliminary experiment is carried\nout to evaluate if the trained LMs have the ability\nto detect the original word order (WO) among its\npermuted variants (up to 20 word permutations).\nThe sequence of tokens for which the probability\nwas the highest was chosen as the predicted original\nWO. Comparing the sequence with chosen WO to\nthe original WO leads to 37% accuracy for SEAME\nand 60% for Miami.\nTable 7: Outcomes of ğ›¼estimation. \"-ğ›¼MCC\" is the\ncorrelation measured between the MLID determined by\nthe unscaled P1.2 approach and MLID labels from other\nprinciples (+ true MLID labels for Miami). \"+ğ›¼MCC\"\nare the correlation measurements with the scaled P1.2.\nSEAME\nMiami\nP1.1\nP2\nP1.1\nP2\ntrue\n-ğ›¼MCC\n0.31\n0.33\n0.36\n0.08\n0.41\n+ğ›¼MCC\n0.36\n0.31\n0.38\n0.09\n0.37\nOutputs of the pre-trained monolingual LMs\nhave different probability distributions, therefore,\nas described in Section 3, the factor ğ›¼is used to\nallow for scale changes. ğ›¼is derived from expecta-\ntions of the probabilities yielded on monolingual\nexamples and their translations following Equation\n4. As a result of ğ›¼estimation the MCC of SEAME\nP1.1/P1.2, Miami P1.1/P1.2 and Miami P2/P1.2\nhas increased (Table 7). Additionally, a \"true\" ğ›¼\nvalue is calculated using ground truth MLID for\nMiami and P1.1 and P2 MLID for SEAME, and\nthey are compared to the estimated ğ›¼. DET plots\nand highlighted thresholds in Figure 1 demonstrate\nthat by using the estimated ğ›¼the amount of False\nPositives (FP) and False Negatives (FN) becomes\nmore balanced for SEAME. For Miami the ğ›¼es-\ntimation does not lead to more balanced FP and\nTable 6: Experimental results for SEAME. First three columns and last three rows (P1.1, P1.2 and P2) refer to\nthe ML determination principles from text. \"Coverage\" row presents the percentage of all CS examples being\nprocessed. \"% English\" row displays the percentage of utterances recognised as \"English\" LID or MLID. MCC\nBaseline refers to the word LID majority implementation (Bullock et al., 2018). \"ğ¿ğ¼ğ·\" is a pretrained LID system,\n\"ğ¿ğ¼ğ·ğ‘šğ‘ğ‘\" column is a mapping trained on monolingual utterances from SEAME. ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1, ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 and\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ2 are trained mappings similar to ğ¿ğ¼ğ·ğ‘šğ‘ğ‘but trained on CS data and labels generated from transcriptions\nby corresponding principles. ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1, ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 and ğ‘€ğ¿ğ¼ğ·ğ‘ƒ2 contain correlation values with the target MLID\ndetermined from text (italic) and correlations with other MLID targets.\nP1.1\nP1.2\nP2\nğ¿ğ¼ğ·\nğ¿ğ¼ğ·ğ‘šğ‘ğ‘\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ2\nCoverage\n36%\n100%\n31%\n100%\n100%\n100%\n100%\n100%\n% English\n24%\n46%\n49%\n18%\n43%\n42%\n44%\n45%\nMCC Baseline\n0.99\n0.28\n0.69\n0.33\n0.33\n0.5\n0.38\n0.46\nMCC P1.1\n1\n0.36\n0.83\n0.41\n0.5\n0.67\n0.47\n0.52\nMCC P1.2\n0.36\n1\n0.31\n0.09\n0.14\n0.17\n0.3\n0.16\nMCC P2\n0.83\n0.31\n1\n0.33\n0.45\n0.49\n0.4\n0.6\nFN but this improvement is not observed due to\nthe limited test set and other reasons which will be\ndiscussed later (Section 4.5.2).\n4.3\nLanguage Identification\nIf one assumes that is a \"dominant\" language that\nmost acoustically resembles the spoken CS utter-\nance, then a conventional LID system can be used\nas an ML determiner. An ECAPA-TDNN (Desplan-\nques et al., 2020) model pretrained on Voxlingua-\n107 (Valk and AlumÃ¤e, 2020) was used to automati-\ncally detect the dominant language from audio data\n(Table 6 and 8, column ğ¿ğ¼ğ·). The ECAPA-TDNN\nmodel was trained to recognise a large number of\nlanguages. In order to limit the models to binary\ntask a mapping function was trained from the out-\nputs based on a fully-connected neural network\n(Multi-Layer Perceptron, MLP) classifier.\nThe\nmapping function is trained to map 107 language\noutput posteriors to the binary output of the lan-\nguages participating in CS. LID is a challenging\ntask for accented data such as monolingual subsets\nfrom SEAME and Miami but still achieves 82%\nand 79% F1-macro respectively on cross-validation\namong 5 splits.\n4.4\nML identification from audio\nOne can train an MLP mapping model using the\nLID posterior distribution to also predict P1.1, P1.2\nand P2 from audio. Due to the different coverage\nrates of P1.1, P1.2 and P2 of the CS data the amount\nof training data would vary greatly: 16582 for P1.1,\n43068 for P1.2 and 23868 for P2. The resulting\nsystems will be further referred to as ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1,\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 and ğ‘€ğ¿ğ¼ğ·ğ‘ƒ2.\n4.5\nCorrelation analysis\nThe agreement between the implemented princi-\nples is measured using the MCC metric since the\nMLID generated by the principles are not human\nannotation and are automatically generated. F1-\nmacro is computed only in cases when the human-\nannotated Miami subset is compared to the MLID\napproaches.\n4.5.1\nCorrelation between P1.1, P1.2 and P2\nTable 9: Correlation values for SEAME with unlabelled\nsentences given a third \"unknown MLID P1.1\", \"un-\nknown MLID P1.2\" and \"unknown MLID P2\" class la-\nbels for the three principle implementations accordingly.\nThis approach ensures 100% coverage of all utterances\nsacrificing the MCC.\nP1.1\nP1.2\nP2\nMCC P1.1\n1\n0.03\n0.09\nMCC P1.2\n0.03\n1\n0.1\nMCC P2\n0.09\n0.1\n1\nP1.1, P1.2 and P2 were applied to CS text data and\nthe agreement analysis is presented in Table 6 and\nTable 8 for SEAME and Miami respectively in the\nfirst three columns. P1.1 and P2 have to meet cer-\ntain conditions to be applied, therefore they do not\nhave full coverage of CS data: 36% and 31% for\nSEAME, 60% and 58% for Miami. Measuring the\ncorrelations only for the utterances for which the\nMLID is determined is performed to measure the\nagreement of the implementations of the linguistic\nTable 8: Experimental results for Miami. \"ğ¿ğ¼ğ·ğ‘šğ‘ğ‘\" column is a mapping trained on monolingual utterances from\nMiami. \"F1-macro true\" and \"MCC true\" are the metric values when comparing the outputs of the systems to ground\ntruth ML annotation for Miami.\nP1.1\nP1.2\nP2\nğ¿ğ¼ğ·\nğ¿ğ¼ğ·ğ‘šğ‘ğ‘\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2\nğ‘€ğ¿ğ¼ğ·ğ‘ƒ2\nCoverage\n60%\n100%\n58%\n100%\n100%\n100%\n100%\n100%\n% English\n45%\n31%\n31%\n43%\n30%\n31%\n42%\n31%\nF1-macro true\n100%\n67%\n93%\n56%\n53%\n56%\n60%\n56%\nMCC true\n1.0\n0.37\n0.86\n0.27\n0.35\n0.27\n0.24\n0.38\nMCC Baseline\n0.99\n0.28\n0.67\n0.59\n0.81\n0.83\n0.42\n0.8\nMCC P1.1\n1\n0.38\n0.81\n0.45\n0.42\n0.85\n0.43\n0.82\nMCC P1.2\n0.38\n1\n0.09\n0.26\n0.34\n0.35\n0.53\n0.34\nMCC P2\n0.81\n0.09\n1\n0.7\n0.86\n0.87\n0.51\n0.82\nprinciples. Higher correlation indicates that the\nprinciples in agreement may be used to generate\nground truth MLID labels for downstream tasks.\nCalculating MCC with unknown labels leads to ex-\ntremely low correlation due to a large portion of\nunknown labels which makes it impossible to as-\nsess the slight changes in correlation of the labeled\ndata (Tables 9 and 10).\nAmong the three principles P1.1 and P2 have the\ngreatest correlation (0.82 for SEAME and 0.81 for\nMiami), P1.1/P1.2 demonstrates less correlation\n(0.36 and 0.38), while the least correlation is ob-\nserved between P1.2 and P2 (0.31 and 0.09). P1.1\nand the baseline have almost identical behavior\nwhich is expected (0.99 and 1.0), whereas less cor-\nrelation is observed of the baseline with P2 (0.69\nand 0.67) and P1.2 (0.28 and 0.28).\nTable 10: Correlation values for Miami with unlabelled\nsentences given the \"unknown ML\" class.\nP1.1\nP1.2\nP2\nF1-macro true\n55%\n67%\n48%\nMCC true\n0.44\n0.37\n0.56\nMCC P1.1\n1\n0.1\n0.25\nMCC P1.2\n0.1\n1\n0.16\nMCC P2\n0.25\n0.16\n1\nThe high correlation values for P1.1 and P2\nprove that the MLF framework can reliably pre-\ndict the structure and behavior of CS text. This\nenables to use the MLIDs generated by the rule-\nbased principles as pseudo-labels in applications.\n4.5.2\nCorrelation of P1.1/P1.2/P2 and the\nacoustic LID/MLID\nThe ML determined from CS text is compared to\nthe LID computed from the corresponding audio.\nThe procedure for the LID experiments is described\nin the previous subsection. Columns 4 and 5 in\nTables 6 and 8 show the amount of correlation be-\ntween MLID derived from text and the recognised\nLID classes. The same columns for Miami in Table\n8 also include F1 macro and MCC for an annotated\nMLID subset. Training ğ¿ğ¼ğ·ğ‘šğ‘ğ‘on the monolin-\ngual utterances seems to increase the MCC (from\n0.27 to 0.35) but decrease the F1 macro (56% from\n53%) for the CS Miami data.\nSuppose a conventional LID system determines\nthe dominant language in a CS audio based on the\nmajority of time the language is spoken. Then the\ntrue annotation may be approximated by counting\nthe textual token LIDs in a CS utterance (Baseline).\nHowever, correlation analysis shows that ğ‘€ğ¿ğ¼ğ·\nsystems are better predictors of the token LID ma-\njority (columns 4-5 vs 6-8 row MCC Baseline in\nTables 6 and 8).\nFurther experimentation comprises of compar-\ning ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1, ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 and ğ‘€ğ¿ğ¼ğ·ğ‘ƒ2 with\nP1.1, P1.2 and P2. Upon observing the results for\nSEAME data ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 leads to overall highest\nvalue out-of-domain MCC scores (0.47+0.4=0.87)\nfor textual principles P1.1 and P2. A similar in-\nspection of the Miami results shows the biggest\nMCC scores for ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1 (0.35+0.87). For the\nannotated subset of Miami data ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 leads\nto the biggest F1 macro among all systems (60%),\nwhile ğ‘€ğ¿ğ¼ğ·ğ‘ƒ2 leads to the biggest MCC score\n(0.38).\nTable 11: Distributions of languages in CS corpora. Utterance level LID for monolingual subsets is in the \"Utterance\nLID\" row, token level LID for CS is in the \"Token LID\" row and utterance level textual ML for CS are in rows\nP1.1/P1.2/P2.\nSEAME\nMiami\nEnglish\nMandarin\nEnglish\nSpanish\nUtterance LID (mono)\n54%\n46%\n68%\n32%\nToken LID (CS)\n42%\n58%\n66%\n34%\nP1.1 (CS)\n23%\n77%\n45%\n55%\nP1.2 (CS)\n44%\n56%\n31%\n69%\nP2 (CS)\n49%\n51%\n31%\n69%\nFigure 3: Correlations between acoustic ğ¿ğ¼ğ·and\nğ‘€ğ¿ğ¼ğ·outputs and textual P1.1, P1.2 and P2 for CS\nSEAME data. Each bar segment represents the amount\nof correlation for a LID or MLID model with textual\nprinciples, therefore the whole bar represents the sum\nof the correlations.\nFigure 4: Correlations between acoustic ğ¿ğ¼ğ·and\nğ‘€ğ¿ğ¼ğ·outputs and textual P1.1, P1.2 and P2 for CS\nMiami data.\nLastly, the MCC scores between the textual and\nacoustic MLID determiners are summed up for\nevery ğ¿ğ¼ğ·/ğ‘€ğ¿ğ¼ğ·approach (Figures 3 and 4). Re-\nsults show that the correlation of the proposed ap-\nproaches with the MLID is higher than in ğ¿ğ¼ğ·\nsystems in all occasions apart from ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 for\nCS Miami data. The latter is due to the word order\nof the English and Spanish languages being similar\nin contrast to English/Mandarin CS. This leads to\nthe morpheme order having a better discrimination\npower in the case of English/Mandarin CS than in\nEnglish/Spanish CS.\n4.5.3\nP1.1/P1.2/P2 distribution analysis\nAt the last step of analysis the distributions of\nlanguages are measured on utterance level LID\nfor monolingual (Utterance LID row in Table 11),\ntoken level LID for CS (Token LID row in Ta-\nble 11) and utterance level textual MLID for CS\n(P1.1/P1.2/P2 rows in Table 11). The numbers re-\nveal that although the majority of the monolingual\nutterances are English in both corpora (54% for\nSEAME and 68% for Miami), it is not the pre-\nferred ML when CS occurs in the utterance for all\nprinciples. The token LID distribution also does\nnot seem to be correlated with the choice of the\nML in these corpora. In SEAME there seems to\nbe a strong preference towards using Mandarin as\nan ML (77%) when EL insertions are single words\n(P1.1). The preference is not as strong for Span-\nish in the CS Miami subset (55%) but it is still a\nbig difference in comparison to the monolingual\ndistributions (32%). P1.2 and P2 show a similar\ndistribution of MLIDs with the numbers indicating\nthe preference of speakers to use the non-English\nlanguage as the grammatical frame for a CS utter-\nance.\n5\nConclusion\nTo the best of our knowledge this is the first work\nthat precisely carries out the Matrix Language (ML)\ndetermination of a code-switched (CS) utterance\nbased on the Matrix Language Frame (MLF) the-\nory and that compares Matrix Language Identity\n(MLID) to acoustic Language Identity (LID). Three\nmethods for ML determination in text and audio\nare implemented using the ideas and the concepts\nof the MLF theory (Myers-Scotton, 1997). An ex-\ntensive correlation analysis of the MLID systems\nfrom text and speech is carried out. A pretrained\nLID system ğ¿ğ¼ğ·is adapted to the data by train-\ning a mapping function ğ¿ğ¼ğ·ğ‘šğ‘ğ‘, while also map-\nping functions ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.1, ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2, ğ‘€ğ¿ğ¼ğ·ğ‘ƒ2\nfor MLID are trained. ğ‘€ğ¿ğ¼ğ·consistently outper-\nforms ğ¿ğ¼ğ·for ML determination from audio based\non Matthewâ€™s Correlation Coefficient (MCC). Com-\nparing the results to the ground truth ML annotation\nshows that the trained ğ‘€ğ¿ğ¼ğ·ğ‘ƒ1.2 and ğ‘€ğ¿ğ¼ğ·ğ‘ƒ2\noutperform ğ¿ğ¼ğ·in terms of F1-macro and MCC\nrespectively. Finally, this approach reveals that\ndespite English dominating as the utterance LID\nfor the monolingual utterances, non-English (Man-\ndarin or Spanish) languages set the grammatical\nframe for CS utterances.\nThe proposed approaches can be used for ac-\ncurate automatic analysis of CS text and audio.\nIt can provide insight into the nature of CS for\nwhole datasets but also separate speakers and even\nutterances. Further work will explore the useful-\nness of the MLID implementations in Natural Lan-\nguage Processing and Automatic Speech Recogni-\ntion (ASR) applications, namely in language and\ndialogue modelling and also in end-to-end multi-\ntask ASR the MLID component will be used as a\npart of the ASR setup. Additionally, further devel-\nopment of P2 is required where the system mor-\nphemes would be automatically determined from a\ngiven set of CS data rather than using a closed set\nof POS tags.\nLimitations\nThe main limitation of the method is related to data\navailability: there is limited ML-annotated CS data\nopenly available to date. Therefore it is problem-\natic to assess the quality of ML classification. ML\nidentity can be determined in CS data using the\nP1.1 but the principle can only be applied in case\nof singleton EL insertions. Since there is no ML\nannotation, correlation was measured for most of\nthe experiments which is difficult to assess. Fi-\nnally, although providing valuable insight into the\nCS data, the usefulness of the method is yet to be\ntested in NLP and ASR applications.\nAcknowledgements\nMany thanks to Maria del Carmen Parafita Couto\nand Diana Carter for sharing the ML annotation for\nthe Miami-Bangor corpus. We would also like to\nthank Tom Pickard and Carolina Scarton for the\nvery helpful comments on the paper. This work was\nsupported by Engineering and Physical Sciences\nResearch Council [grant number 2676033].\nReferences\nHeike Adel, Ngoc Thang Vu, Katrin Kirchhoff, Dominic\nTelaar, and Tanja Schultz. 2015. Syntactic and se-\nmantic features for code-switching factored language\nmodels. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 23:431â€“440.\nSrijan Bansal, Suraj Tripathi, Sumit Agarwal, Teruko\nMitamura, and Eric Nyberg. 2022. PRO-CS : An\ninstance-based prompt composition technique for\ncode-switched tasks.\nIn EMPNLP 2022, pages\n10243â€“10255, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nGayatri Bhat, Monojit Choudhury, and Kalika Bali.\n2016. Grammatical constraints on intra-sentential\ncode-switching: From theories to working models.\narXiv preprint arXiv:1612.04538.\nBarbara Bullock, Wally GuzmÃ¡n, Jacqueline Serigos,\nVivek Sharath, and Almeida Jacqueline Toribio. 2018.\nPredicting the presence of a matrix language in code-\nswitching. In Proceedings of the Third Workshop\non Computational Approaches to Linguistic Code-\nSwitching, pages 68â€“75, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nChing-Ting Chang, Shun-Po Chuang, and Hung yi Lee.\n2019. Code-switching sentence generation by gener-\native adversarial networks and its application to data\naugmentation. In INTERSPEECH.\nBrecht Desplanques, Jenthe Thienpondt, and Kris De-\nmuynck. 2020. ECAPA-TDNN: Emphasized chan-\nnel attention, propagation and aggregation in TDNN\nbased speaker verification.\nIn Interspeech 2020.\nISCA.\nAnuj Diwan, Rakesh Vaideeswaran, Sanket Shah,\nAnkita Singh, Srinivasa Raghavan, Shreya Khare,\nVinit Unni, Saurabh Vyas, Akash Rajpuria, Chi-\nranjeevi Yarra, et al. 2021. Multilingual and code-\nswitching asr challenges for low resource indian lan-\nguages. arXiv preprint arXiv:2104.00235.\nYukun Feng, Feng Li, and Philipp Koehn. 2022. To-\nward the limitation of code-switching in cross-lingual\ntransfer. In EMNLP 2022, pages 5966â€“5971, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nGualberto A. GuzmÃ¡n, Joseph Ricard, Jacqueline Seri-\ngos, Barbara E. Bullock, and Almeida Jacqueline\nToribio. 2017. Metrics for modeling code-switching\nacross corpora. In INTERSPEECH.\nGrandee Lee, Xianghu Yue, and Haizhou Li. 2019. Lin-\nguistically motivated parallel data augmentation for\ncode-switch language modeling. In INTERSPEECH.\nDau-Cheng Lyu, Tien Ping Tan, Chng Eng Siong, and\nHaizhou Li. 2010. Seame: a mandarin-english code-\nswitching speech corpus in south-east asia. In IN-\nTERSPEECH.\nMetilda Sagaya Mary N J, Vishwas M. Shetty, and\nS. Umesh. 2020. Investigation of methods to improve\nthe recognition performance of tamil-english code-\nswitched data in transformer framework. ICASSP\n2020, pages 7889â€“7893.\nH. Muralikrishna, S. Kapoor, Dileep Aroor Dinesh, and\nPadmanabhan Rajan. 2021. Spoken language identi-\nfication in unseen target domain using within-sample\nsimilarity loss. ICASSP 2021, pages 7223â€“7227.\nC. Myers-Scotton. 1997. Duelling Languages: Gram-\nmatical Structure in Codeswitching.\nClarendon\nPress.\nCarol Myers-Scotton. 2002. Contact linguistics: Bilin-\ngual encounters and grammatical outcomes. OUP.\nSOS Ncoko, Ruksana Osman, and Kate Cockcroft. 2000.\nCodeswitching among multilingual learners in pri-\nmary schools in south africa: An exploratory study.\nInternational Journal of Bilingual Education and\nBilingualism, 3(4):225â€“241.\nMaria Carmen Parafita Couto and Marianne Gullberg.\n2017. Code-switching within the noun phrase: Evi-\ndence from three corpora. International Journal of\nBilingualism, 23.\nMadaki Rufai Omar. 1983. A linguistic and pragmatic\nanalysis of Hausa-English code-switching (Nigeria).\nUniversity of Michigan.\nVÃ­ctor Soto and Julia Hirschberg. 2019. Improving\ncode-switched language modeling performance using\ncognate features. In INTERSPEECH.\nDan Svenstrup, Jonas Meinertz Hansen, and Ole\nWinther. 2017. Hash embeddings for efficient word\nrepresentations. Preprint, arXiv:1709.03933.\nShubham Toshniwal, Tara N. Sainath, Ron J. Weiss,\nBo Li, Pedro Moreno, Eugene Weinstein, and Kan-\nishka Rao. 2018.\nMultilingual speech recogni-\ntion with a single end-to-end model.\nPreprint,\narXiv:1711.01694.\nJÃ¶rgen Valk and Tanel AlumÃ¤e. 2020. Voxlingua107:\na dataset for spoken language recognition. Preprint,\narXiv:2011.12998.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Preprint, arXiv:1706.03762.\nEmre Yilmaz, Astik Biswas, Ewald van der Westhuizen,\nFebe de Wet, and Thomas R. Niesler. 2018. Building\na unified code-switching asr system for south african\nlanguages. ArXiv, abs/1807.10949.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-10-03",
  "updated": "2024-11-14"
}