{
  "id": "http://arxiv.org/abs/2410.03446v1",
  "title": "On Uncertainty In Natural Language Processing",
  "authors": [
    "Dennis Ulmer"
  ],
  "abstract": "The last decade in deep learning has brought on increasingly capable systems\nthat are deployed on a wide variety of applications. In natural language\nprocessing, the field has been transformed by a number of breakthroughs\nincluding large language models, which are used in increasingly many\nuser-facing applications. In order to reap the benefits of this technology and\nreduce potential harms, it is important to quantify the reliability of model\npredictions and the uncertainties that shroud their development.\n  This thesis studies how uncertainty in natural language processing can be\ncharacterized from a linguistic, statistical and neural perspective, and how it\ncan be reduced and quantified through the design of the experimental pipeline.\nWe further explore uncertainty quantification in modeling by theoretically and\nempirically investigating the effect of inductive model biases in text\nclassification tasks. The corresponding experiments include data for three\ndifferent languages (Danish, English and Finnish) and tasks as well as a large\nset of different uncertainty quantification approaches. Additionally, we\npropose a method for calibrated sampling in natural language generation based\non non-exchangeable conformal prediction, which provides tighter token sets\nwith better coverage of the actual continuation. Lastly, we develop an approach\nto quantify confidence in large black-box language models using auxiliary\npredictors, where the confidence is predicted from the input to and generated\noutput text of the target model alone.",
  "text": "Back\nFront\nOn Uncertainty In Natural Language Processing ∙ Dennis Ulmer ∙ 2024\nOn Uncertainty In \nNatural Language Processing\nDennis Ulmer\nThe last decade in deep learning has brought on increasingly capable systems that are deployed on a wide\nvariety of applications. In natural language processing, the field has been transformed by a number of\nbreakthroughs including large language models, which are used in increasingly many user-facing applications.\nIn order to reap the benefits of this technology and reduce potential harms, it is important to quantify the\nreliability of model predictions and the uncertainties that shroud their development.\nThis thesis studies how uncertainty in natural language processing can be characterized from a linguistic,\nstatistical and neural perspective, and how it can be reduced and quantified through the design of the\nexperimental pipeline. We further explore uncertainty quantification in modeling by theoretically and\nempirically investigating the effect of inductive model biases in text classification tasks. The corresponding\nexperiments include data for three different languages (Danish, English and Finnish) and tasks as well as a\nlarge set of different uncertainty quantification approaches. Additionally, we propose a method for calibrated\nsampling in natural language generation based on non-exchangeable conformal prediction, which provides\ntighter token sets with better coverage of the actual continuation. Lastly, we develop an approach to quantify\nconfidence in large black-box language models using auxiliary predictors, where the confidence is predicted\nfrom the input to and generated output text of the target model alone. \narXiv:2410.03446v1  [cs.AI]  4 Oct 2024\nOn Uncertainty In Natural\nLanguage Processing\nDENNIS ULMER\nDepartment of Computer Science\nIT University of Copenhagen\nDissertation Submitted in Partial Fulfillment of the\nRequirements for the Degree of\nDoctor of Philosophy\nJune 14, 2024\nCommittee\nAdvisor\nDr. Christian Hardmeier\nIT Universitetet i København\nCo-Advisor\nDr. Jes Frellsen\nDanmarks Tekniske Universitet\nMembers\nDr. Leon Derczynski\nIT Universitet i København\nProf. Dr. Ole Winther\nDanmarks Tekniske Universitet\nProf. Dr. Mário A. T. Figueiredo\nInstituto Superior Técnico\n1\nAbstract\nThe last decade in deep learning has brought on increasingly\ncapable systems that are deployed on a wide variety of applications.\nIn natural language processing, the field has been transformed by\na number of breakthroughs including large language models, which\nare used in increasingly many user-facing applications. In order to\nreap the benefits of this technology and reduce potential harms, it\nis important to quantify the reliability of model predictions and\nthe uncertainties that shroud their development.\nThis thesis studies how uncertainty in natural language process-\ning can be characterized from a linguistic, statistical and neural\nperspective, and how it can be reduced and quantified through\nthe design of the experimental pipeline. We further explore uncer-\ntainty quantification in modeling by theoretically and empirically\ninvestigating the effect of inductive model biases in text classifica-\ntion tasks. The corresponding experiments include data for three\ndifferent languages (Danish, English and Finnish) and tasks as well\nas a large set of different uncertainty quantification approaches.\nAdditionally, we propose a method for calibrated sampling in nat-\nural language generation based on non-exchangeable conformal\nprediction, which provides tighter token sets with better coverage\nof the actual continuation. Lastly, we develop an approach to quan-\ntify confidence in large black-box language models using auxiliary\npredictors, where the confidence is predicted from the input to and\ngenerated output text of the target model alone.\ni\nResumé\nDet sidste årti i deep learning har medført stadig mere dygtige\nsystemer, der anvendes på mange forskellige områder.\nFeltet\nnatural language processing (naturlig sprogbehandling) er blevet\ntransformeret af en række gennembrud, herunder store sprogmod-\neller, som bruges i stadigt flere anvendelser med menneskelige\nbrugere. For at udnytte fordelene ved denne teknologi og reducere\npotentielle skader, er det vigtigt at kvantificere pålideligheden\naf modelforudsigelser og de usikkerheder, der omkranser deres\nudvikling.\nDette afhandling undersøger, hvordan usikkerhed i natural lan-\nguage processing kan karakteriseres ud fra et sprogligt, statistisk\nog neuralt perspektiv, og hvordan den kan reduceres og kvantifi-\nceres gennem design af den eksperimentelle pipeline. Vi udforsker\nyderligere kvantificering af usikkerhed i modellering ved teoretisk\nog empirisk at undersøge effekten af modellers induktive bias i\ntekstklassificeringsopgaver. De tilsvarende eksperimenter omfatter\ndata for tre forskellige sprog (dansk, engelsk og finsk) og opgaver\nsamt et stort sæt forskellige tilgange til kvatificering af usikker-\nheder. Derudover foreslår vi en metode til kalibreret sampling i\nnaturlig sproggenerering baseret på non-exchangeable conformal\nprediction, der giver smallere tokensæt med bedre dækning af\nden faktiske fortsættelse. Til sidst udvikler vi en tilgang til at\nkvantificere tillid i store black-box sprogmodeller ved hjælp af\nsåkaldte hjælpeprædiktorer, hvor tilliden forudsiges ud fra input\ntil og genereret outputtekst fra sprogmodellen alene.\nii\nAcknowledgements\n“There’s a ruinous misconception that a PhD must be smart.\nThis can’t be true.\nA smart person would know better than to get a PhD.”\n—Matt Might\nSurely almost every PhD, across disciplinary boundaries, can\nattest the uniqueness of this degree. You seize being a student\ndespite continuing to study. However, any pre-defined curricula\ncease to exist and there is an almost overwhelming number of\npaths forward. The idea of working on an open, unsolved problem\nis as exciting as it is terrifying—it might not known whether a\nsolution even exists, and a million possible solutions are waiting to\nbe explored. Success depends on whether an idea actually proves\nto work in practice, and even if one is lucky enough to experience\nthis feeling, publication is not guaranteed. Even a great paper\nmight be reject in the reviewing lottery or drowned out in the\ncurrent deluge of research. Almost by design, these and other\nfactors create an emotional rollercoaster that is easy to glorify by\nthe time one writes the acknowledgements of one’s thesis, but\ncan be a formidable test of resilience in the moment. I am deeply\ngrateful to all the people that have allowed me to come this far\nand supported me along this way, and this section is dedicated to\nthem.\nFirstly, I would like to thank Natalie Schluter for making it\npossible for me to start my PhD journey. I highly appreciate you\ntaking a chance on me and giving me full flexibility. Secondly,\nI owe a lot of gratitude to my supervisors Christian Hardmeier\nand Jes Frellsen. You were incredibly supportive and gave me\nthe freedom to explore any direction I desired, and immensely\nenjoyed collaborating with you. I also would like to thank the\nmember of my PhD committee, namely Leon Derczynski from\nITU and NVIDIA, Ole Winther from DTU, and Mário A. T.\nFigueiredo from IST in Lisbon.\nDuring the last three and a\nhalf years, I also had the pleasure to collaborate with and learn\niii\nAcknowledgements\niv\nfrom a number of outstanding people, namely Giovanni Cinà at\nPacmed, André F. T. Martins at the Instituto Superior Técnico in\nLisbon, Elman Mansimov at Amazon Web Services, and Seong\nJoon Oh at Parameter Lab. A special and repeated thanks goes\nto Dieuwke Hupkes, who together with my other former master\nthesis co-supervisor Elia Bruni were instrumental in putting me\non this path in the first place. In addition, I want to thank all the\nother collaborators across several projects during my PhD that I\nwas lucky enough to work with and learn from.\nI could not have done this either without the unwavering\nsupport from my partner Bea; you have been with me through the\nlows and you were there to celebrated the highs, and I will never\nbe able to accurately thank you for your presence and patience.\nI also owe a lot to my family for their support over the course\nof many years of university education without which I would\nhave never made it this far; thank you to my parents Michaela\nand Siegfried, my sister Anna and my grandma Ingrid for your\nencouragement (Oma, I am sorry that I still haven’t gotten a\nregular, stable job). A large amount of gratitude is owed to all the\nfriends in and outside of academia for their their support and role\nin making this an overall unforgettable time. Thank you António,\nAtilla, Ben, Christina, Chryssa, Constanza, Daniel, David, Doro,\nElisa, Ellie, Hannes, Isra, Jon, Joris, Joscha, Karolina, Kristoffer,\nLaura, Mareike, Marija, Mark, Martin, Max, Mike, Nuno, Putri,\nRita, Rui, Sean, Yova.\nThis thank you is generally extended\nto all the members of the NLPnorth at ITU, CoAStal at KU,\nSARDINE at IST and Jes’ group at DTU. In addition, a thank\nyou Daniel, Max, and Mike as well as Yiyi and Laura for helping\nwith the organization of Beers with NLPeers. The list above to is\nimpossible to make comprehensive, so should your name not be in\nit, please be assured that I value and appreciate your part in this\nchapter of my life.\nLastly, I want to thank Lotti from ITU’s high performance\ncomputing cluster for her swift help with any technical problems,\nand the members of ITU’s IT department, facility management,\nHR, finance, front desk and PhD school for all other problems\nsmall and large that arose over the course of my PhD. I am also\ngrateful to Ricardo B. Ponce (@pixel.flux on Instagram) for letting\nme license his artwork for this thesis. Last but not least, thanks\nto the members of Café Analog and producers of Club Mate for\nproviding much needed morale and caffeine boosts throughout my\nPhD.\nDeclaration of Work\nI, Dennis Ulmer, declare that this thesis—submitted in partial\nfulfillment of the requirements conferral of a PhD from the IT\nUniversity of Copenhagen—is solely my own work unless otherwise\nreferenced or attributed.\nNeither the thesis nor its content\nhave been submitted (or published) for qualifications at another\nacademic institution.\n—Dennis Ulmer\nv\nContents\nAbstract\ni\nResumé\nii\nAcknowledgements\niii\nDeclaration of Work\nv\nNotation\nxv\n1\nIntroduction\n1\n1.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nApplications . . . . . . . . . . . . . . . . . . . . . .\n3\n1.3\nChallenges in Natural Language Processing . . . . .\n5\n1.4\nObjectives . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.5\nPublications . . . . . . . . . . . . . . . . . . . . . .\n8\n1.6\nStructure\n. . . . . . . . . . . . . . . . . . . . . . .\n11\n2\nBackground\n13\n2.1\nWhat Is Uncertainty, anyway? . . . . . . . . . . . .\n13\n2.1.1\nThe Frequentist Perspective . . . . . . . . .\n14\n2.1.2\nThe Bayesian Perspective\n. . . . . . . . . .\n19\n2.1.3\nThe Linguistic Perspective: Underspecifica-\ntion, Ambiguity & Vagueness\n. . . . . . . .\n26\n2.1.4\nThe Linguistic Perspective: Expressing Un-\ncertainty . . . . . . . . . . . . . . . . . . . .\n30\n2.1.5\nA Pragmatic Answer . . . . . . . . . . . . .\n32\n2.2\nUncertainty in Deep Learning . . . . . . . . . . . .\n33\n2.2.1\nFrequentist Neural Networks . . . . . . . . .\n35\n2.2.2\nBayesian Neural Networks . . . . . . . . . .\n40\n2.2.3\nEvidential Neural Networks\n. . . . . . . . .\n48\n2.2.4\nOther Approaches . . . . . . . . . . . . . . .\n56\n2.3\nUncertainty in Natural Language Processing . . . .\n59\n2.4\nUncertainty & Trust\n. . . . . . . . . . . . . . . . .\n64\n2.5\nCommunicating Uncertainty . . . . . . . . . . . . .\n66\n2.6\nApplications of Uncertainty\n. . . . . . . . . . . . .\n67\n2.7\nSummary\n. . . . . . . . . . . . . . . . . . . . . . .\n69\n3\nAddressing Uncertainty in Experimental Design\n70\n3.1\nExperimental Standards for NLP\n. . . . . . . . . .\n72\n3.1.1\nData . . . . . . . . . . . . . . . . . . . . . .\n73\n3.1.2\nCodebase & Models . . . . . . . . . . . . . .\n77\n3.1.3\nExperiments & Analysis . . . . . . . . . . .\n79\n3.1.4\nDiscussion . . . . . . . . . . . . . . . . . . .\n81\nvi\nCONTENTS\nvii\n3.2\nStatistical Hypothesis Testing . . . . . . . . . . . .\n83\n3.2.1\nAlmost Stochastic Order . . . . . . . . . . .\n84\n3.2.2\nExperimental Comparison . . . . . . . . . .\n87\n3.2.3\nCase study: Question-Answering with Large\nLanguage Models . . . . . . . . . . . . . . .\n89\n3.2.4\nDiscussion . . . . . . . . . . . . . . . . . . .\n95\n4\nUncertainty in Text Classification\n96\n4.1\nTheoretical Pitfalls in Classification . . . . . . . . .\n98\n4.1.1\nPreliminaries\n. . . . . . . . . . . . . . . . .\n99\n4.1.2\nMonotonicity & Polytopes . . . . . . . . . . 101\n4.1.3\nConvergence of Predictions on OOD Data\n. 102\n4.1.4\nConvergence of Uncertainty Metrics on OOD\nData . . . . . . . . . . . . . . . . . . . . . . 106\n4.1.5\nSynthetic Data Experiments . . . . . . . . . 108\n4.2\nUncertainty & Calibration in Low-Resource NLP\n. 110\n4.2.1\nMethodology\n. . . . . . . . . . . . . . . . . 111\n4.2.2\nDataset Selection & Creation\n. . . . . . . . 112\n4.2.3\nModel Training . . . . . . . . . . . . . . . . 114\n4.2.4\nEvaluation . . . . . . . . . . . . . . . . . . . 114\n4.2.5\nExperiments . . . . . . . . . . . . . . . . . . 116\n4.2.6\nDependence on Training Data . . . . . . . . 117\n4.2.7\nInstance Analysis . . . . . . . . . . . . . . . 118\n4.2.8\nDiscussion . . . . . . . . . . . . . . . . . . . 119\n4.3\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . 120\n5\nUncertainty in Natural Language Generation\n123\n5.1\nConformalizing Natural Language Generation\n. . . 124\n5.2\nBackground . . . . . . . . . . . . . . . . . . . . . . 125\n5.3\nMethod\n. . . . . . . . . . . . . . . . . . . . . . . . 127\n5.4\nExperiments . . . . . . . . . . . . . . . . . . . . . . 129\n5.4.1\nEvaluating Coverage . . . . . . . . . . . . . 129\n5.4.2\nCoverage Under Shift . . . . . . . . . . . . . 133\n5.4.3\nGeneration Quality . . . . . . . . . . . . . . 135\n5.5\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . 137\n5.6\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . 139\n6\nUncertainty in Large Language Models\n140\n6.1\nCalibrating LLMs with Auxiliary Models . . . . . . 141\n6.1.1\nPrompting the Target LLM\n. . . . . . . . . 143\n6.1.2\nSetting Calibration Targets\n. . . . . . . . . 146\n6.1.3\nTraining the Auxiliary Model\n. . . . . . . . 147\n6.2\nExperiments . . . . . . . . . . . . . . . . . . . . . . 148\n6.2.1\nSetting Calibration Targets by Clustering . . 149\n6.2.2\nCalibrating White and Black-Box Models . . 150\n6.2.3\nAblation Study . . . . . . . . . . . . . . . . 154\n6.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . 155\n6.4\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . 156\nCONTENTS\nviii\n7\nDiscussion\n157\n7.1\nDiscussion of Research Questions\n. . . . . . . . . . 157\n7.2\nOpen Questions & Future Research Directions . . . 165\n7.2.1\nModeling Uncertainty . . . . . . . . . . . . . 165\n7.2.2\nLimits of Uncertainty Quantification\n. . . . 167\n7.2.3\nEvaluating Uncertainty . . . . . . . . . . . . 168\n7.2.4\nCommunicating Uncertainty . . . . . . . . . 169\n8\nConclusion\n170\nBibliography\n175\na\nTheoretical Appendix\n276\na.1 Relationship between Beta and Gamma function . . 276\na.2 Expectation of the Dirichlet Distribution . . . . . . 278\na.3 Entropy of the Dirichlet Distribution . . . . . . . . 279\na.4 Expected Entropy of the Dirichlet Distribution . . . 280\na.5 Kullback-Leibler Divergence between two Dirichlets 281\na.6 Mutual Information for Dirichlet Networks . . . . . 282\na.7 Connection between Softmax and Sigmoid . . . . . 282\na.8 Construction of Polytopal Regions . . . . . . . . . . 283\na.9 Proof of Proposition 1\n. . . . . . . . . . . . . . . . 284\na.10 Proof of Proposition 2\n. . . . . . . . . . . . . . . . 287\na.11 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . 288\na.12 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . 288\na.13 Proof of Lemma 8 . . . . . . . . . . . . . . . . . . . 289\na.14 Proof of Lemma 9 . . . . . . . . . . . . . . . . . . . 290\nb\nExperimental Appendix\n292\nb.1 Additional Error Rate Experiments . . . . . . . . . 292\nb.2\nSynthetic Data Experiments . . . . . . . . . . . . . 294\nb.3\nSub-Sampling of Training Sets . . . . . . . . . . . . 296\nb.4\nSelection of OOD Test Sets\n. . . . . . . . . . . . . 298\nb.5 Additional Scatter Plots\n. . . . . . . . . . . . . . . 299\nb.6\nQualitative Analysis\n. . . . . . . . . . . . . . . . . 302\nb.7 Additional Coverage Results . . . . . . . . . . . . . 303\nb.8 Ablating Neighborhood Size and Desired Coverage . 303\nb.9 Additional Clustering Results\n. . . . . . . . . . . . 306\nb.10 Additional Calibration Results . . . . . . . . . . . . 307\nc\nReproducibility Appendix\n325\nc.1\nOpen Source Software\n. . . . . . . . . . . . . . . . 325\nc.2\nEnvironmental Impact . . . . . . . . . . . . . . . . 326\nc.3 ASO Test Implementation Details . . . . . . . . . . 327\nc.4\nHyperparameters Search . . . . . . . . . . . . . . . 329\nc.4.1\nIris Example . . . . . . . . . . . . . . . . . . 329\nc.4.2\nSynthetic Data Experiments . . . . . . . . . 329\nc.4.3\nText Classification Experiments . . . . . . . 330\nc.4.4\nAuxiliary Calibrator Experiments . . . . . . 332\nc.5\nPre-processing for Text Classification Benchmark\n. 333\nc.6\nImplementation Details of Text Classification Bench-\nmark . . . . . . . . . . . . . . . . . . . . . . . . . . 333\nc.7\nConvergence on Clinc Plus . . . . . . . . . . . . . . 334\nc.8 Temperature Search . . . . . . . . . . . . . . . . . . 335\nAbbreviations\n337\nIndex\n340\nList of Figures\nFigure 1.1\nNumber of published open-source mod-\nels, Bert latent trajectory, size of different\nWikipedias. . . . . . . . . . . . . . . . . . .\n5\nFigure 2.1\nDifferent Beta prior and posterior shapes. .\n21\nFigure 2.2\nHighest density intervals and maximum a\nposteriori estimates for different Beta pos-\nteriors. . . . . . . . . . . . . . . . . . . . .\n23\nFigure 2.3\nParse trees for a sentence with structural\nambiguity.\n. . . . . . . . . . . . . . . . . .\n27\nFigure 2.4\nParse trees for a garden path sentence, be-\nfore and after the last word.\n. . . . . . . .\n29\nFigure 2.5\nDouble triangle of language production. . .\n29\nFigure 2.6\nTaxonomy of semantic uncertainties. . . . .\n31\nFigure 2.7\nTaxonomy of uncertainty quantification ap-\nproaches in deep learning. . . . . . . . . . .\n34\nFigure 2.8\nComparison of MC dropout, deep ensembles\nand prior networks on the Iris dataset. . . .\n50\nFigure 2.9\nExamples of the probability simplex for a\nK = 3 classification problem. . . . . . . . .\n53\nFigure 2.10\nJuxtaposition of a prior network and credal\nsets constructed from the convex hull of\nensemble and MC dropout predictors. . . .\n58\nFigure 3.1\nPublished papers at NLP venues from 2012\nto today. . . . . . . . . . . . . . . . . . . .\n70\nFigure 3.2\nSchematic representation of the scientific\nmethod in deep learning. . . . . . . . . . .\n72\nFigure 3.3\nExamples of (almost) stochastic order be-\ntween two CDFs. . . . . . . . . . . . . . . .\n86\nFigure 3.4\nPlot of distributions used to benchmark\nsignificance tests.\n. . . . . . . . . . . . . .\n87\nFigure 3.5\nComparisons of type I error rates. . . . . .\n88\nFigure 3.6\nSetup for the QA case study. . . . . . . . .\n89\nix\nLIST OF FIGURES\nx\nFigure 3.7\nResults of LLM case study using the ASO\ntest. . . . . . . . . . . . . . . . . . . . . . .\n93\nFigure 4.1\nUncertainty and linear regions of a ReLU\nclassifier trained on example data. . . . . .\n98\nFigure 4.2\nDependencies between theoretical results in\nSection 4.1. . . . . . . . . . . . . . . . . . . 102\nFigure 4.3\nUncertainty landscapes of ReLU classifiers\ntrained on the half-moon dataset.\n. . . . . 109\nFigure 4.4\nSchematic of text classification experiments. 114\nFigure 4.5\nScatter plots showing the connection be-\ntween model performance and uncertainty\nquality on Dan+.\n. . . . . . . . . . . . . . 117\nFigure 4.6\nPredictive entropy estimates for a single sen-\ntence from Dan+ and mutual information\nfor Finnish UD.\n. . . . . . . . . . . . . . . 119\nFigure 5.1\nSchematic\nrepresentation\nof\nnon-\nexchangeable conformal language genera-\ntion through nearest neighbors. . . . . . . . 124\nFigure 5.2\nConditional converage results on the de →\nen MT task. . . . . . . . . . . . . . . . . . 132\nFigure 5.3\nCoverage, average set size and ˆq based on\nthe noise level on the de →en MT task and\nopen text generation task.\n. . . . . . . . . 134\nFigure 6.1\nIllustration of APRICOT\n. . . . . . . . . 142\nFigure 6.2\nFull overview over APRICOT\n. . . . . . . 143\nFigure 6.3\nIllustration of the prompting strategies used\nto generate the input data for the auxiliary\ncalibrator.\n. . . . . . . . . . . . . . . . . . 144\nFigure 6.4\nReliability diagrams for our different meth-\nods for Vicuna v1.5 on TriviaQA.\n. . . . . 153\nFigure A.1\nIllustrating the interplay of softmax proba-\nbilities between components for K = 2 in\nR2. . . . . . . . . . . . . . . . . . . . . . . 284\nFigure B.1\nComparing test score distributions for dif-\nferent tests and distributions as a function\nof sample size. . . . . . . . . . . . . . . . . 293\nFigure B.2\nMeasuring the Type II error rate of the con-\nsidered tests on normal and normal mixture\ndistributions as a function of sample size. . 294\nFigure B.3\nUncertainty measured by different metrics\nfor single-instance models and their gradient\nmagnitude. . . . . . . . . . . . . . . . . . . 308\nFigure B.4\nUncertainty measured by different metrics\nfor multi-instance models and the gradient\nof the uncertainty score w.r.t. to the input. 309\nLIST OF FIGURES\nxi\nFigure B.5\nComparing the relative type frequency in\nthe original and sub-sampled training sets.\n310\nFigure B.6\nComparing the relative sequence length fre-\nquency in the original and sub-sampled\ntraining sets. . . . . . . . . . . . . . . . . . 310\nFigure B.7\nComparing the relative label frequency in\nthe original training set, compared to sub-\nsampled training sets. . . . . . . . . . . . . 311\nFigure B.8\nComparison of the relative class frequencies\nbetween original training set compared to\nthe OOD test set. . . . . . . . . . . . . . . 311\nFigure B.9\nComparison of sequence length distribution\nbetween the original training set and the\nOOD test set.\n. . . . . . . . . . . . . . . . 312\nFigure B.10\nComparison of the relative frequencies of\nthe top 25 types in the original training set\ncompared to the OOD test set. . . . . . . . 312\nFigure B.11\nScatter plots showing the difference between\nmodel performance and the quality of un-\ncertainty estimates. . . . . . . . . . . . . . 313\nFigure B.12\nScatters plot showing the difference between\nmodel performance and the quality of un-\ncertainty estimates. . . . . . . . . . . . . . 314\nFigure B.13\nScatter plot showing the difference between\nmodel performance and the quality of un-\ncertainty estimates on a token-level.\n. . . 314\nFigure B.14\nScatter plot showing the difference between\nmodel performance and the quality of un-\ncertainty estimates on a sequence-level.\n. . 315\nFigure B.15\nAdditional examples for uncertainty esti-\nmates on single sequences on the Dan+\ndataset. . . . . . . . . . . . . . . . . . . . . 316\nFigure B.16\nAdditional examples for uncertainty esti-\nmates on single sequences on the Finnish\nUD dataset.\n. . . . . . . . . . . . . . . . . 317\nFigure B.17\nAdditional conditional coverage plots for\nthe MT and LM dataset using our non-\nexchangeable conformal natural generation. 318\nFigure B.18\nBar plot of cluster sizes found. . . . . . . . 318\nFigure B.19\nDensity plot of calibration targets generated\nthrough the clustering procedure for the two\nLLMs and TriviaQA / CoQA.\n. . . . . . . 319\nFigure B.20\nIllustrating questions from TriviaQA along\nwith their assigned confidence targets for\nthe two LLMs. . . . . . . . . . . . . . . . . 320\nFigure B.21\nReliability diagrams for all methods for Vi-\ncuna v1.5 7B on TriviaQA. . . . . . . . . . 321\nFigure B.22\nReliability diagrams for all methods for Vi-\ncuna v1.5 7B on CoQA. . . . . . . . . . . . 322\nFigure B.23\nReliability diagrams for all methods for\nGPT-3.5 on TriviaQA.\n. . . . . . . . . . . 323\nFigure B.24\nReliability diagrams for all methods for\nGPT-3.5 on CoQA. . . . . . . . . . . . . . 324\nList of Tables\nTable 4.1\nDatasets used for text classification experi-\nments.\n. . . . . . . . . . . . . . . . . . . . 112\nTable 4.2\nResults for text classification experiments. . 122\nTable 5.1\nCoverage results for the de →en and ja →\nen MT tasks. . . . . . . . . . . . . . . . . . 131\nTable 5.2\nCoverage results for the LM task.\n. . . . . 132\nTable 5.3\nRelationship between prediction entropy\nand set sizes per noise level on the de →en\ntask.\n. . . . . . . . . . . . . . . . . . . . . 134\nTable 5.4\nGeneration results for the de →en and ja\n→en translation tasks. . . . . . . . . . . . 136\nTable 5.5\nGeneration results for the open text gener-\nation. . . . . . . . . . . . . . . . . . . . . . 137\nTable 6.1\nComparison of appealing attributes that\nLLM confidence quantification techniques\nshould fulfill. . . . . . . . . . . . . . . . . . 141\nTable 6.2\nResults of evaluation of found clusters on\nTriviaQA and CoQA. . . . . . . . . . . . . 150\nTable 6.3\nCalibration results for Vicuna v1.5 and\nGPT-3.5 on TriviaQA and CoQA. . . . . . 152\nTable 6.4\nConsistency of verbalized uncertainty meth-\nods for Vicuna v1.5 and GPT-3.5 on Trivi-\naQA and CoQA. . . . . . . . . . . . . . . . 154\nTable 6.5\nCalibration results for Vicuna v1.5 and\nGPT-3.5 on TriviaQA and CoQA using the\nauxiliary (clustering) method.\n. . . . . . . 155\nTable A.1\nCorrespondences between sections of the\ntheoretical appendix and thesis chapters.\n. 276\nTable B.1\nCorrespondences between sections of the\nempirical appendix and thesis chapters. . . 292\nxii\nLIST OF TABLES\nxiii\nTable B.2\nType I error rates for samples drawn from a\nnormal distribution as a function of sample\nsize and different rejection thresholds. . . . 295\nTable B.3\nType II error rates for normal samples as a\nfunction of sample size and different rejec-\ntion thresholds.\n. . . . . . . . . . . . . . . 296\nTable B.4\nType II error rates for normal samples as\na function of mean difference and different\nrejection thresholds. . . . . . . . . . . . . . 297\nTable B.5\nType I error rates for normal mixture sam-\nples as a function of sample size and differ-\nent rejection thresholds. . . . . . . . . . . . 298\nTable B.6\nType II error rates for normal mixture sam-\nples as a function of sample size and differ-\nent rejection thresholds. . . . . . . . . . . . 299\nTable B.7\nType II error rates for normal mixture sam-\nples as a function of mean difference be-\ntween two of the mixture components and\ndifferent rejection thresholds. . . . . . . . . 300\nTable B.8\nType I error rates for samples drawn from a\nLaplace distribution as a function of sample\nsize and different rejection thresholds. . . . 301\nTable B.9\nType I error rates for samples drawn from a\nRayleigh distribution as a function of sam-\nple size and different rejection thresholds. . 302\nTable B.10\nResults of using an interpolated Kneser-Ney\nn-gram language model on selected datasets,\nincluding sub-sampled training splits and\nthe OOD test set. . . . . . . . . . . . . . . 303\nTable B.11\nResults for varying values of α using differ-\nent models and datasets.\n. . . . . . . . . . 304\nTable B.12\nResults for varying neighborhood sizes K\nusing different models and datasets. . . . . 304\nTable B.13\nContents of some randomly sampled clus-\nters that result from the clustering proce-\ndure for TriviaQA. . . . . . . . . . . . . . . 306\nTable B.14\nContents of some randomly sampled clus-\nters that result from the clustering proce-\ndure for CoQA.\n. . . . . . . . . . . . . . . 306\nTable C.1\nCorrespondences between reproducibility\nappendix sections and thesis chapters. . . . 325\nTable C.2\nList of open-source repositories for the con-\ntents of this thesis.\n. . . . . . . . . . . . . 326\nTable C.3\nBest hyperparameters found on the half-\nmoon dataset. . . . . . . . . . . . . . . . . 330\nLIST OF TABLES\nxiv\nTable C.4\nDistributions or options that hyperparame-\nters were sampled from during the random\nhyperparameter search. . . . . . . . . . . . 330\nTable C.5\nList of searched hyperparameters for the\ntext classification experiments. . . . . . . . 331\nTable C.6\nList of model hyperparameters by dataset\nfor the text classification experiments. . . . 336\nTable C.7\nChosen hyperparameters for our model on\ndifferent datasets and for different calibra-\ntion targets.\n. . . . . . . . . . . . . . . . . 336\nNotation\n“It is time for mathematicians, physicists, and computer\nscientists to forget their differences and admit that nobody\nreally has a clue about what’s going on in high dimensions.”\n—Clément Canonne\nIn the following, we generally follow the notational guidelines used\nin the book by Goodfellow et al. (2016) and by other organizations\nsuch as the Transactions on Machine Learning Research (TMLR)\njournal, with some modifications. These include the use of the\nfollowing:\n• Lowercase latin and greek letters for scalars, e.g. a, b, c and\nα, β, γ.\n• Bold lowercase latin and greek letters for vectors, e.g. a, b, c\nand α, β, γ.\n• Bold uppercase latin and greek letters for matrices, e.g. A, W\nand Θ, Ψ.\n• Uppercase letters such as A and D to denote sets. Sometimes,\ncalligraphical letters like C might be used to denote sets when\nthe notation might conflict with common conventions (e.g. C\nusually denoting the set of complex numbers.).\n• {xi}N\ni=1 to denote a set of elements {x1, . . . , xN}. We also use\nthe condensed shorthand {xij}M,N\ni,j=1 to denote a set of elements\n{x1,1, . . . , xM,1, . . . xM,N} indexed along two dimensions.\n• [K] to denote an set {1, 2, . . . , K}, or more formally, for any\nK ∈N+, [K] = {n | n ∈N+ and n ≤K}.\nWe denote an element-wise multiplication for vectors and ma-\ntrices by ◦, and the same symbol may be used in some contexts to\ndenote function compositions, i.e. (f ◦g)(x) = g(f(x)).\nxv\nNotation\nxvi\nDefinitions\nNeural Network.\nSome concepts occur often enough to warrant\ntheir separate definitions. Since this thesis revolves around neural\nnetworks, we denote θ as the (flattened) vector of network param-\neters and Θ as the space of all possible weight parameters. Neural\nnetworks usually comprise a number of linear layers, consisting\nof a weight matrix W and a bias term b, transforming inputs x\ninto hidden encodings z. A superscript or index might be added\nto indicate one of these objects belonging to a specific layer l ∈[L]\nor to a specific time step t ∈[T]. Furthermore, we indicate with\na index θ when a function is parameterized by θ (or some other\nset of parameters). This is generally done to reduce clutter and\nmake equations more readable, but might be made explicit with\nconditioning when it is important in a statistical context. For\ninstance, the probability distribution over classes k ∈[K] of a\nneural classifier will be denoted as pθ(y | x) ≡p(y | x, θ). In the\nsame fashion, we denote fθ(x) as the logits, i.e. the unnormalized\noutput of a neural classifier, and use fθ(x)k to refer to the k-th\nlogit.\nNeural Network Functions.\nThere also exist several functions\nthat play a specific role in the neural network context. On of these\nis the sigmoid function defined as\nσ(x) =\n1\n1 + exp(−x),\n(0.1)\nas well as its multivariate generalization, the softmax function:\nsoftmax(x)k ≡¯σ(x)k =\nexp(xk)\nPK\nk=1 exp xk\n,\n(0.2)\nwhere we sometimes will use the notation ¯σ(·) to avoid visual\nclutter.\nIndicator function.\nThe indicator function takes as input some\ncondition, and evaluates as\n1\n\u0000condition\n\u0001\n=\n\n\n\n1\nif condition is true\n0\nelse\n(0.3)\nIn some cases it is useful to apply the indicator function element-\nwise to the contents of a vector. In that case, we use a bolded\nversion, namely 1(·), which will be a vector of the same dimension-\nality. Take the example of a vector x whose elements are compared\nagainst a threshold τ. Then\nNotation\nxvii\n1(x > τ)i =\n\n\n\n1\nif xi > τ\n0\nelse\n(0.4)\nStatistics.\nIn the context of statistics, we use the Dirac delta\nfunction, which is defined as 0 everywhere except for the origin,\nwhere it is +∞:\nδ(x) =\n\n\n\n+∞\nif x = 0\n0\nelse\n(0.5)\nIn addition, its integral over the entire real number line is 1.\nAnother set of definitions denotes common statistical concepts as\nthe expectation of a random variable x\nE[x] =\nX\nx\nP(x)x\nor\nZ\nx\np(x)dx.\n(0.6)\nIn this case, we also use P to denote probability mass functions\nand p to denote probability density functions. From this, we can\nalso define the variance as\nVar[x] = E\n\u0002\n(x −E[x])2\u0003\n(0.7)\nas well as the Shannon entropy\nH[x] = −\nX\nx\nP(x) log P(x)\nor\n−\nZ\np(x) log p(x)dx.\n(0.8)\nSpecial Functions.\nAnother set of definitions is dedicated to\nsome mathematical functions, including the Gamma function Γ(·),\nwhich is a continuous version of the factorial and defined as\nΓ(z) =\nZ ∞\n0\ntz−1 exp(−t)dt.\n(0.9)\nAnother important function is the Beta function:\nB(α1, α2) = Γ(α1)Γ(α2)\nΓ(α1 + α2) .\n(0.10)\nIn some cases, we will consider the Beta function with an\narbitrary number of α values. In that case, we collect them in a\nvector α = [α1, . . . , αK]T and write the Beta function as\nB(α) =\nQK\nk=1 Γ(αk)\nΓ\n\u0010 PK\nk=1 αk\n\u0011.\n(0.11)\n1\n|\nIntroduction\n“Forudsigelse er meget vanskelig, især om fremtiden.”\n“Prediction is very difficult, especially about the future.”\n—Niels Bohr\n1.1\nMotivation\nEvery person’s life is full of decisions. Is this restaurant really as\ngood as the reviews suggest? Should I take a job here or take a\nmore interesting job in a city far away? These decisions can be\nhard to evaluate, since not all necessary information is known\nbeforehand: Restaurant reviews might be fraudulent or biased, and\na promising job opportunity might turn out to be different than\nadvertised. Compare that with the example of making a move in a\ngame of chess: Chess is called a game with perfect information, so\nall the positions and possible moves of the pieces on the board\nare known, and one could in theory make the optimal move at\nevery step (assuming good chess-playing abilities). However, in\nreal life we often do not have all the information necessary to\nmake a perfect decision. As such, humans take into account the\nuncertainty that permeates their decision-making in order to\nmanage risk.\nIn this way, machines are (or should be) no different. The\ndecades-old research in machine learning (ML)—and especially\nthe most recent advances in the last decade or so—have produced\nsystems that make decisions from the mundane (“is this a picture\nof a cat or an airplane?”) to the potentially risky (“what treatment\nshould be recommended to this patient?”). This trend has been\naccelerated by the paradigm of deep learning, which allows us\nto build evermore complex systems that could solve increasingly\ncomplex tasks. The complexity of these systems through comes\nat the cost of losing a detailed understanding of all the “cogs and\ngears” involved due to the sheer size of models (including millions,\nbillions and sometimes even trillions of such “gears”). This fact has\nspurred numerous lines of research to develop methods to make\n1\n1.1 motivation\n2\ndeep learning systems more robust, fair and safe.\nOne such line of research is concerned with uncertainty\nquantification, i.e. reflecting the degree of trustworthiness of a\nprediction.\nIn systems with automatic decision-making, such\nscores can for instance be used to withhold a prediction or request\nhuman oversight. One popular example is autonomous driving:\nConsider an important traffic sign that cannot be accurately\nevaluated by the onboard computer, or a traffic situation that is\nhard to analyze. In these cases, a human driver might appreciate\nthe opportunity to intervene with the car, e.g. by reducing its\nspeed in the face of uncertainty, instead of the car sticking to a\nwrong assessment and endangering the driver’s or other traffic\nusers’ lives.\nAt this point, the reader might be rightfully wonder whether\nsuch high-risk scenarios also exist for language applications. And\nindeed, such problems can arise in sometimes more, sometimes less\nobvious places. An intuitive application with these considerations\nis healthcare: More and more work has recently gone into building\nartificial intelligence (AI) systems that provide decision-support for\nmedical staff. For instance, models could analyze text written by\na user to detect signs of mental illness or triage (i.e., prioritize) pa-\ntients when resources are limited (Cohan et al., 2016; Rozova et al.,\n2022; Stewart et al., 2022). In this case, uncertainty can serve as a\nsignal to request an additional human review of a case. Confident\nbut wrong predictions here can lead to a waste of resources, a loss\nof trust of the medical professionals in the system, and, in the worst\ncase, leaving urgent cases untreated. As another example, natural\nlanguage systems are also used to assist in legal deliberations\n(Chalkidis et al., 2019a; Martinez-Gil, 2023; Chalkidis, 2023).\nWhile the scenario of a “robo judge” is usually ruled out, there\nstill remain risks where models used for legal discovery or research\nmight overlook relevant or produce misleading or incorrect outputs.\nUncertainty quantification is an active research area for systems\nthat operate for instance on images or tabular data, but it has\nonly recently started to receive attention in the natural language\nprocessing (NLP) community. This thesis gives an introduction to\nuncertainty quantification in machine learning and natural language\nprocessing for novices, summarizes the current state of progress in\nthe field, and presents some novel and relevant methods for some\nof the most pressing problems for automated languages processing:\nThese include for instance determining the most viable methods\nin text classification and proposing new approaches to calibrated\n1.2 applications\n3\nsampling for natural language generation, as well as confidence\nestimation for black-box models.\n1.2\nApplications\nA lot of research on uncertainty quantification makes only super-\nficial statements or tacit assumptions about its usefulness. The\nfollowing, non-exhaustive list of aspects therefore underline poten-\ntial practical use-cases.\nSafety.\nIn general, uncertainty estimates can improve safety\nwhenever a system with automated decision capabilities could\npotentially have real-world effects. Some of these situations are\nstudied in the AI safety literature (see e.g. Amodei et al., 2016):\nThey can include preventing an intelligent agent from exploring\nunsafe options, or acting in a risky manner as its environment\nchanges from the version it was trained with, which is often referred\nto as distributional shift (Shimodaira, 2000; Moreno-Torres et al.,\n2012). In these cases, uncertain options can either be outright\nrejected or decisions can be delegated to a human user.\nTrust.\nIn order to reap the benefits of automation and the abil-\nity to extract intricate patterns from large amounts of data, users\nhave to trust the system’s output, or otherwise run the danger\nof being mislead. In the worst case, they might grow to ignore\nor even antagonize an automatic system. Since our systems are\ninanimate—and often inscrutable—building trust between humans\nand machines can be a tricky endeavor. Nevertheless, there exists\na notion of trust that can be built by consistency (i.e., knowing\nwhat to expect from a system) and by using uncertainty to under-\nstand the behavior of a model (Jacovi et al., 2021). We dedicate\nSection 2.4 to discuss this connection in more detail.\nFairness.\nA long line of works has demonstrated how modern\ndeep learning systems have a tendency to discriminate against\nsubpopulations in the dataset and how to mitigate these effects\n(see Caton and Haas, 2024; Mehrabi et al., 2021 for an overview).\nAdditional studies have argued that this is the result of human\nbiases in the machine learning pipeline (Waseem et al., 2021) as\nwell as biases and underrepresentation of groups in the training\ndataset (Meng et al., 2022a). In the latter case, specific uncertainty\nquantification methods can indicate whenever the correct prediction\nis uncertain due to a lack of similar training data (see Sections 2.2.2\nand 2.2.3). In other instances, unfairness might occur when models\nfavor a prediction corresponding to a majority group in the dataset\n1.2 applications\n4\nin the face of an inherently ambiguous input. Consider the example\nof machine translation system that is supposed to translate “the\ndoctor is here” into Spanish. In English, we do not have to specify\nthe gender of doctor, while this is necessary in Spanish.\nAnd\nthus, without any additional context, two translations are equally\nplausible (“el doctor está aqui” versus “la doctora está aqui”). Deep\nlearning systems have an inclination to prefer the version that has\nappeared more often in the training data, which due to real-world\nhuman biases might be el doctor (Vanmassenhove et al., 2018). By\nexposing the inherent uncertainty however, we can delegate a series\nof decisions to the user or other specialized systems and avoid such\npitfalls.\nEfficiency.\nNot all inputs a deep learning system faces are\nequally difficult. Imagine a system that has been trained to distin-\nguish images of lions and tigers. Upon receiving an picture of a lion\nsimilar to its training instances, we would expect a well-trained\nmodel to come to a confident (and correct) prediction. Many of\nour contemporary deep learning systems have grown to include\nfrom millions up to billions and sometimes trillions of learnable\nparameters, and thus incur considerable computational cost for\nevery prediction. Therefore, some works have explored whether\nwe can use notions of uncertainty to detect when a model has\narrived at a secure prediction in order to skip unnecessary com-\nputations (i.e. Schuster et al., 2021, 2022). Conversely, consider\nthat our fictional lion vs. tiger detector is faced with a liger, or\nan albino tiger displaying differently-colored fur.1 In light of these\ndifficult examples, we could use uncertainty to trigger additional\ncomputations to come to a conclusion (see an example for such a\nmechanism for machine translation by van der Poel et al., 2022).\nThere is evidence that the human brain operators in a similar\nfashion, for instance when the reading time in human subjects\nincreases when confronted with a surprising sentence structure\n(Ferreira and Henderson, 1991).\nInterpretability.\nDue to the scale of modern architectures, the\nmechanisms in which they arrive at a prediction can be opaque\nand hard to deduce for humans. Here, research also has produced\na variety of approaches to tackle this problem (see for instance\nMadsen et al., 2023 for a non-exhaustive selection). Uncertainty\ncan be used as an additional angle to understand when the model\nmight behave unreasonably confident or uncertain, with some\nstudies already conducted for natural language generation (Ott\net al., 2018; Xu et al., 2020; Xiao and Wang, 2021; Chen and Ji,\n1 A liger is a tiger / lion hybrid, see https://en.wikipedia.org/wiki/Liger.\n1.3 challenges in natural language processing\n5\n2022).\nDespite the variety of useful applications, there are a number\nof challenges to UQ that are very common or even unique in NLP,\nand distinguish this line of research from similar works on images\nor tabular data.\n1.3\nChallenges in Natural Language Processing\n(a) Number of models published on the\nHuggingface Hub.\n(b) Bert latent representations for a\nsentence.\n(c) Number of Wikipedia articles by language (log-scale).\nFigure 1.1: (a) Number of models published per month on the Hug-\ngingFace Hub (gathered on 17.06.2024). (b) Trajectory of the first two\nsentences of Turing (1950) using in the latent space of the uppermost\nlayer of Bert (Devlin et al., 2019), after projecting them into two-\ndimensional space using PCA and whitening them. Time is indicated by\ncolor, reaching from dark (first token) to light (last token). (c) Number\nof articles by Wikipedia, log-scale (gathered on 11.04.2024). Shown are\nthe top ten languages, and then ten randomly chosen languages of the\nremaining four quantiles of the distribution, each. All figures are best\nviewed in color and digitally.\nThe research in this thesis aims to fill a literature gap: While\nuncertainty quantification is a vibrant research field in machine\nlearning, the availability of methods for natural language data\n1.3 challenges in natural language processing\n6\nis limited, and very few works develop solutions for this purpose\nspecifically. This is disconcerting for the following reasons:\nChallenges of Natural Language.\nIn contrast to other ma-\nchine learning problems, processing language is a rather messy\naffair. First of all, language is incredibly diverse, displaying vast\ndifferences between languages, dialects, demographics, domains\nor even individual speakers (Bender, 2011; Plank, 2016; Zampieri\net al., 2020; van Esch et al., 2022). It is secondly embedded in a\nsocial and cultural context that is often necessary to understand\nits meaning (Hershcovich et al., 2022), and due to its paraphrastic\nnature, the idea same idea can often be expressed in a multitude\nof ways (Baan et al., 2023). Thirdly, the sequential nature often\nbreaks the i.i.d. assumption that is a fundamental underlying as-\nsumption for many algorithms. One might assume that language\ndata could just be treated as a time series and apply corresponding\nmethods for uncertainty quantification (see e.g. Zhu and Laptev,\n2017; Wang et al., 2020a; Blasco et al., 2024).\nUnfortunately\nthough, encodings of language usually behave very erratically, as\nFigure 1.1b demonstrates. Modeling techniques for time series\nhowever subtly assume a certain behavior of the underlying data,\ne.g. a limit in the allowed rate of change encountered between two\ntime steps.2 The sometimes abrupt token-level changes encoun-\ntered during language processing therefore prevent the application\nof time series modeling techniques.\nData Scarcity.\nLarge amounts of both unstructured and\nannotated data exist for English, but most of the world’s 7000+\nlanguages are not blessed with such resources (Ruder, 2020;\nJoshi et al., 2020).\nFigure 1.1c shows the number of articles\nof a variety of Wikipedias on a log-scale. Due to its openness,\nWikipedia remains a popular source of training data in NLP,\nhowever high-resource languages like English and German provide\nexponentially more potential training data compared to languages\nsuch as Afrikaans, Amharic or N’Ko.3 This runs contrary to the\nstrength of modern deep learning architectures: Weak architectural\ninductive biases such as in transformers enable us to learn complex\nmeaning representations, but only when enough data is supplied\n(Tay et al., 2023).\nIn the case of low-resource languages for\ninstances, such data is often not available, and thus we can end\n2 This trait can for instance be formalized through the Lipschitz constant of the\ntrue data-generating function (Qu et al., 2022).\n3 Cebuano, the second-most spoken language in the Philippines, features the\nsecond-largest Wikipedia due to a bot called Lsjbot, which tries to create\nCebuano Wikipedia articles for all living creatures. This makes around 99.6%\nof its articles bot-generated (Wikipedia contributors, 2024).\n1.4 objectives\n7\nup with a model that is underspecified (D’Amour et al., 2022):\nThe fewer training data points are available, the more possible\nmodels are able to fit them. While this might not lead to any\nproblems on inputs similar to the training data, models might\nbehave unpredictably on out-of-distribution data in ways that\nmight not be immediately detectable by a user.\nTrust & Safety.\nIn machine learning, much of the research on\nuncertainty quantification is motivated by concerns regarding the\ntrust in and the safety of automation. Despite this, similar research\nhas until recently mostly remained nascent in NLP, despite being\nequally as relevant. Furthermore, the rapid developments in NLP\nwith respect to large language models (Kalyan et al., 2021; Sevilla\net al., 2022) have accelerated their adoption for a variety of applica-\ntions by end users, albeit without appropriate techniques to ensure\ntheir safety. This trend is illustrated by Figure 1.1a, showing the\nnumber of models by month uploaded to the HuggingFace Hub, a\nplatform to share open-source models. After a drop in submissions\nafter its initial release in 2022, numbers have steadily increased\nto around 60k models in mid 2024. This, alongside a number\nof available proprietary models that can accessed through web\ninterfaces and APIs, lowers the barrier of access to language models.\n1.4\nObjectives\nThis thesis analyzes the current state of uncertainty quantification\nresearch in deep learning and connects it to the methodological chal-\nlenges that arise when they are applied to language data. As such,\nit aims to familiarize the reader with the most popular strategies\nfor uncertainty quantification, as well as giving an intuition about\ntheir limitations. In this thesis, we seek to answer the following\nresearch questions:\nRQ1: How can uncertainty in NLP be characterized?\nUncertainty can be a somewhat vague concept, and its defini-\ntion is often passed over in different research works. Therefore,\nthis thesis tries to gain a multi-disciplinary perspective on the\nmatter, investigating different perspectives on the concept\nand how they are related.\nRQ2: How can choices in experimental design help to reduce\nand quantify uncertainty?\nAnother overlooked factor in empirical research in NLP is\n1.5 publications\n8\nthe role of experimental design. Specifically, this work in-\nvestigates how more conscious design decisions can not only\nhelp to reduce and quantify uncertainty, but also open new\nways to model it.\nRQ3: How do inductive model biases influence uncertainty\nquantification?\nThe inductive biases of a model usually refers to a set of\nimplicit or explicit assumptions made by a learning algorithm\n(Hüllermeier et al., 2013). For instance, linear regression\nassumes that the target variable can be recovered as a linear\ncombination of its input variables, and neural networks have\nan inductive bias to non-linear higher-order combinations of\ninput features. The behavior of uncertainty in neural models\nis a priori often idealized (e.g. the model always displaying\nuncertainty on OOD), with this expectation sometimes being\nunfulfilled in practice. The reasons for this however are not\nso well understood, and thus this thesis sheds some light on\nthe interaction of model biases and uncertainty.\nRQ4: How can we address some of the challenges of uncer-\ntainty quantification in NLP?\nSection 1.3 has listed some of the unique hurdles that UQ on\nNLP produces. Therefore, this thesis puts forth some new\ninsights along with methodological advances to tackle these\nchallenges.\n1.5\nPublications\nThe following works were produced during the PhD and are dis-\ncussed in detail in this thesis (ordered chronologically). In all cases,\nthe author’s contributions amount to the main or complete share\nof the conception, implementation and description of ideas and\nexperiments and the writing of the resulting publications, unless\nshared authorship is indicated.\n1. Ulmer, Dennis∗, and Giovanni Cinà∗. “Know Your Lim-\nits: Uncertainty Estimation with ReLU Classifiers Fails at\nReliable OOD Detection.” In: Uncertainty in Artificial Intel-\nligence. PMLR (2021) (discussed in Section 4.1).\n2. Ulmer, Dennis, Christian Hardmeier, and Jes Frellsen.\n“deep-significance-Easy and Meaningful Statistical Signifi-\ncance Testing in the Age of Neural Networks.”\nIn: The\n∗Equal Contribution.\n1.5 publications\n9\nWorkshop on Machine Learning Evaluation Standards at\nICLR (2022) (discussed in Section 3.2).\n3. Ulmer, Dennis, Elisa Bassignana, Max Müller-Eberstein,\nDaniel Varab, Mike Zhang, Rob van der Goot, Christian\nHardmeier, and Barbara Plank. “Experimental Standards for\nDeep Learning in Natural Language Processing Research.” In:\nFindings of the Association for Computational Linguistics:\nEMNLP (2022), pp. 2673–2692 (discussed in Section 3.1).\n4. Ulmer, Dennis, Jes Frellsen, and Christian Hardmeier.\n“Exploring Predictive Uncertainty and Calibration in NLP:\nA Study on the Impact of Method & Data Scarcity.” In:\nFindings of the Association for Computational Linguistics:\nEMNLP (2022), pp. 2707—2735 (discussed in Section 4.2).\n5. Ulmer, Dennis, Christian Hardmeier, and Jes Frellsen.\n“Prior and Posterior Networks: A Survey on Evidential Deep\nLearning Methods for Uncertainty Estimation.” In: Transac-\ntions on Machine Learning Research. JMLR (2023) (discussed\nin Section 2.2.3).\n6. Ulmer, Dennis, Chrysoula Zerva, André F. T. Martins:\n“Non-Exchangeable Conformal Language Generation with\nNearest Neighbors”.\nIn: Findings of the Association for\nComputational Linguistics: EACL (2024), pp. 1909–1929\n(discussed in Chapter 5).\n7. Ulmer, Dennis, Martin Gubri, Hwaran Lee, Sangdoo Yun,\nSeong Joon Oh. “Calibrating Large Language Models Using\nTheir Generations Only”. In: Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) (discussed in Chapter 6).\nThe following works were produced during the PhD, but will\nnot be discussed in detail, either since the author was not the main\nauthor, or because they were not a good fit for the topic of this\nthesis:\n8. Baan, Joris∗, Nico Daheim∗, Evgenia Ilia∗, Dennis Ulmer∗,\nHaau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sen-\nnrich, Chrysoula Zerva, Wilker Aziz. “Uncertainty in Natural\nLanguage Generation: From Theory to Applications.” Under\nreview, 2024.\n9. Hupkes,\nDieuwke,\nMario\nGiulianelli,\nVerna\nDankers,\nMikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos\nChristodoulopoulos, Karim Lasri, Naomi Saphra, Arabella\n1.5 publications\n10\nSinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar\nBatsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Rita\nFrieske, Ryan Cotterell, Zhijing Jin: “A Taxonomy and Re-\nview of Generalization Research in NLP”. In: Nature Machine\nIntelligence 5 (10), p. 1161–1174.\n10. Farinhas, António, Chrysoula Zerva, Dennis Ulmer, André\nF.T. Martins. “Non-exchangeable Conformal Risk Control”.\nIn: Proceedings of the International Conference on Learning\nRepresentations, 2024.\n11. Ulmer, Dennis, Elman Mansimov, Kaixiang Lin, Justin\nSun, Xibin Gao, Yi Zhang. “Bootstrapping LLM-based Task-\nOriented Dialogue Agents via Self-Talk”. In: Findings of the\nAssociation for Computational Linguistics: ACL 2024.\n12. Gubri, Martin, Dennis Ulmer, Hwaran Lee, Sangdoo Yun,\nSeong Joon Oh.\n“TRAP: Targeted Random Adversarial\nPrompt Honeypot for Black-Box Identification”. In: Findings\nof the Association for Computational Linguistics: ACL 2024.\nAnother published document concerns the proceedings of the\nUncertaiNLP workshop, in which the author was involved as an\neditor and workshop co-organizer. The workshop was co-located\nwith the European meeting of the Association for Computational\nLinguistics (EACL) in St. Julians, Malta, in 2024:\n• Vázquez, Raúl, Hande Celikkanat, Dennis Ulmer, Jörg\nTiedemann, Swabha Swayamdipta, Wilker Aziz, Barbara\nPlank, Joris Baan, and Marie-Catherine de Marneffe. Pro-\nceedings of the 1st Workshop on Uncertainty-Aware NLP\n(UncertaiNLP 2024).\nAll of the above publications are accompanied by open-source\ncode, that is listed in detail in Appendix C.1. However, some of\nthe more important open-source contributions are highlighted here:\n1. nlp-uncertainty-zoo: A Python package implementing dif-\nferent methods for uncertainty quantification in sequence\nclassificationand sequence labeling in NLP.\n2. deep-significance: A Python package including many\nfunctions to simplify statistical significance testing in deep\nlearning.\n3. awesome-experimental-standards-deep-learning:\nA\npointer to useful resources in order to improve experimental\nstandards as well as reproducibility and replicability in Deep\nLearning experiments.\n1.6 structure\n11\n1.6\nStructure\nThis thesis is structured as to provide a comprehensive overview\nover the topic of uncertainty from both a statistical and\nlinguistic point of view.\nBoth perspectives are then woven\ntogether in a overview over uncertainty quantification in deep\nlearning and natural language processing. This part serves as\na foundation for later chapters about the uncertainty in the\nexperimental design in NLP, before concretely tackling specific\nproblem scenarios: Uncertainty in text classification problems,\nuncertainty in language generation problems and uncertainty in lat-\nter problems specifically involving the use of large language models.\nTo be more detailed, Chapter 2 introduces the reader to\ndifferent concepts in uncertainty quantification and related\nliterature.\nIt begins with a definition of uncertainty from a\nvariety of perspectives, for instance frequentist and Bayesian\nstatistics, linguistics, and several popular approaches in deep\nlearning. In this context, we also discuss Ulmer et al. (2023), which\nsurveys works related to a novel class of uncertainty quantification\nmethods called evidential deep learning. In the end, this includes a\ndiscussion of the relationship of uncertainty quantification with\nthe end-user with both a motivation in trust and communication.\nWhile most of the research that makes up this thesis is focused\non uncertainty in modeling language, uncertainty also occurs in\nthe experimental design and execution of day-to-day research.\nTherefore, Chapter 3 presents an interlude on challenges with the\nnotions of reproducibility & replicability in deep learning, their\nconnection to uncertainty, and the use of statistical hypothesis\ntesting, all of which inform the methodology of later chapters. This\nencompasses the published works of Ulmer et al. (2022a), giving\nan account of ongoing discussions about experimental methods\nin deep learning, as well as Ulmer et al. (2022c), introducing a\npackage for better statistical hypothesis testing and its application\nto a case study with large language models.\nIn the subsequent Chapter 4, we tackle the problem of\nuncertainty in classification problems.\nFirst we demonstrate\nthe\npitfalls\nof\nuncertainty\nquanitification\nfor\nclassification\nusing simple ReLU networks, drawing from Ulmer and Cinà\n(2021).\nAfterwards, we discuss uncertainty quantification in\nthe context of different classification problems specific to NLP,\nbased on Ulmer et al. (2022b). Here we show how well exisiting\nmethods for NLP fare on different languages and tasks, and\n1.6 structure\n12\nhow much that performance—including the reliability of uncer-\ntainty estimates—depends on the amount of available training data.\nIn Chapter 5, we move to the problem of natural language\ngeneration, and develop a new calibrated sampling method based\non conformal prediction (Papadopoulos et al., 2002; Vovk et al.,\n2005). In language generation, we often restrict the set of possible\ncandidate tokens to generate to a subset of (hopefully) plausible\ncontinutations. Using the theoretical underpinning of conformal\nprediction, we introduce a novel method that does so with statisti-\ncal guarantees. This chapter is based on Ulmer et al. (2024c), and\nwe demonstrate how this novel way to construct prediction sets\nis theoretically sound and produces flexibel prediction sets that\ncome with guarantees about containing plausible tokens to generate.\nIn Chapter 6, we discuss a new method for quantifying the\nconfidence of LLMs originally published in Ulmer et al. (2024a),\nthat tries to circumvent many of the pratical constraints that\ncome with large model sizes. Compared to other alternatives, this\nmethod is furthermore applicable to black-box models that do\nnot allow any internal access to model states or weights, and is\ncomparatively lightweight to train.\nThe thesis continues with a general discussion of the overall\nresults in Chapter 7. There, we answer the overarching research\nquestions stated in Section 1.4 and reflect on current research\ndirections in the field. Lastly, Chapter 8 takes on a bird’s-eye view\nby contextualizing uncertainty quantification in the current zeitgeist\nand discussing its relationship with contemporary policies.\nIn\naddition, the thesis comprises an appendix with theoretical results\n(Appendix A) and one with experimental details (Appendix B)\nthat were omitted from the main text. Details that are necessary\nfor an accurate reproduction of experimental results are bundled\nin Appendix C.\n2\n|\nBackground\n“Le doute n’est pas une état bien agréable, mais l’assurance\nest un état ridicule.”\n“Doubt is not a pleasant condition, but certainty is absurd.”\n—Voltaire\nUncertainty is a common occurrence in everyone’s life, and thus\nmost people have an intuitive understanding of the concept. To\ndefine it concretely, however, can be challenging. Colloquially, we\nmight define uncertainty as a phenomenon or state that is filled\nwith doubts, lack knowledge or that is simply hard to predict. In\nresearch papers, the term uncertainty often only remains vaguely\ndefined, either building on an intuitive definition or presupposing\na certain school of thinking.\nThe aim of this chapter is to bring some clarity to the different\nways uncertainty is defined, and to give an fairly comprehensive\naccount of its applications. This entails a journey from its origins\nin statistics (Sections 2.1.1 and 2.1.2) and linguistics (Sections 2.1.3\nand 2.1.4) to its implementation with neural networks, specifically\nin deep learning (Section 2.2) and natural language processing\n(Section 2.3). In the latter contexts, this comes with a focus on\nmodeling uncertainty, and this is indeed also where many of the\nresearch papers in the field end. Therefore, an additional goal of\nthis chapter is to not take uncertainty modeling as the ultimate\ngoal per se, but to see beyond it and grasp the bigger picture.\nAs uncertainty quantification is often motivated by increasing\ntrustworthiness and safety, we take a closer look at the relationship\nbetween uncertainty and trust in Section 2.4, as well as how to\ncommunicate uncertainty in Section 2.5. Furthermore, the chapter\noutlines diverse applications of uncertainty in Section 2.6.\n2.1\nWhat Is Uncertainty, anyway?\nWe start by defining the most central concept in this thesis: Un-\ncertainty. Since this thesis is focused on NLP, we aim to define\n13\n2.1 what is uncertainty, anyway?\n14\nthe concept from all the perspectives modern NLP touches on.\nThis includes building up some basic concepts from frequentist and\nBayesian statistics as well from different parts of linguistics.\n2.1.1\nThe Frequentist Perspective\n“Statistical inference is serious business.”\n—Bradley Efron, Robert J. Tibshirani in An Introduction to\nthe Bootstrap (Tibshirani and Efron, 1993)\nFrequentist statistics in an approach to statistics that aims to make\ninferences and draw conclusions from sampled data, alone. The\nterm is based on the fact that probabilities are seen as equivalent\nto the observed frequencies of events in the data, assuming\n(potentially infinitely) many repetitions of an experiment (Willink\nand White, 2011). Let us reason about the popular example of a\ncoin flip here to illustrate this notion. We are given a coin and\nwould like to estimate the probability of heads, which we define as\nthe parameter of interest to estimate and will denote by θ. We do\nnot know whether the coin is fair, so we flip it a number of times\nand count the heads and tails to estimate this probability. We\nobtain the following five coin flips:\n1\n1\n1\nBased on this experiment, we then estimate the probability of\nheads as ˆθ =\n#heads\n#coin flips = 2\n5 = 0.4. However, how can we be sure\nthat this reflects the actual probability of heads? We thus repeat\nthe experiment three more times, and obtain:\n1\n1\n1\n→ˆθ2 = 2\n5 = 0.4\n1\n1\n1\n→ˆθ3 = 2\n5 = 0.4\n1\n1\n→ˆθ4 = 3\n5 = 0.6\nAs we gather more and more samples and take their average,\nwe will provably converge to the true value of θ in the limit due\nto the law of large numbers (Dekking et al., 2005). But in light of\na limited number of samples like above, how can we quantify the\nuncertainty of our estimate?\n2.1 what is uncertainty, anyway?\n15\nConfidence Intervals.\nOne common approach to compute some\nfrequentist uncertainty estimate is the use of confidence intervals\n(Neyman, 1937). Confidence intervals try to capture a range of\nvalues for the parameter θ such that, if we were to repeat our\nexperiment, the computed confidence intervals would cover the\ntrue value with some probability (e.g. 95%). We can do this by\nassuming that any estimates ˆθ for θ are independently and normally\ndistributed. The normality assumption holds in this case since the\ncentral limit theorem applies, stating that if we were to repeat\nthis experiment over and over, the sample mean (which is our\nestimate ˆθ) will be normally distributed. Possessing the knowledge\nabout the estimate being normally distributed lets us define the\nconfidence intervals. The procedure is as follows: We know that\nour samples are normally distributed according to some specific\n(constant but unknown) mean θ and standard deviation. In our\nexample, θ would correspond to the true probability of heads that\nwe are interested in. For convenience, we now standardize this\ndistribution. We can achieve this by simply subtracting the mean\nθ from the mean of our estimates ¯θ and dividing by an estimate of\nthe standard deviation denoted as s, which we obtain as:\n¯θ = 1\nN\nN\nX\ni=1\nˆθi\n(2.1)\ns2 =\n1\nN −1\nN\nX\ni=1\n(ˆθi −¯θ)2.\n(2.2)\nAccording to the central limit theorem, the estimate of the\nstandard deviation improves in accuracy by a factor of\n1\n√\nN , leading\nto the standard error s/\n√\nN, and we thus arrive at:\nt =\n¯θ −θ\ns/\n√\nN\n.\n(2.3)\nNow, we would like to know how the statistic in Equation (2.3)\nis distributed in order to identify confidence intervals for θ. We\nalready know that ¯θ is distributed according to a Normal distri-\nbution, and assuming that the sample variance s2 is distributed\naccording to a χ2 distribution with N degrees of freedom, we obtain\na Student’s-t distribution with N −1 degrees of freedom. We can\nnow determine the bounds such that p(−c ≤t ≤c) = 0.95. Using\nsome intermediate steps, we find that\n2.1 what is uncertainty, anyway?\n16\np(−c ≤t ≤c)\n(2.4)\n=p\n\u0010\n−c ≤\n¯θ −θ\ns/\n√\nN\n≤c\n\u0011\n(2.5)\n=p\n\u0010\n−cs\n√\nN\n≤¯θ −θ ≤cs\n√\nN\n\u0011\n(2.6)\n=p\n\u0010\n−cs\n√\nN\n−¯θ ≤−θ ≤cs\n√\nN\n−¯θ\n\u0011\n(2.7)\n=p\n\u0010\n¯θ −cs\n√\nN\n≤θ ≤¯θ + cs\n√\nN\n\u0011\n.\n(2.8)\nTherefore, we know that our unknown mean θ of the distribution\nwill be contained within these bounds. Lastly, we need to choose c\nsuch that the proposed interval corresponds to 95% (or some other\ndesired amount) of the total probability density. Since the shape of\nthe Student’s-t distribution is known, we can choose c to correspond\nto the 97.5-th percentile (leaving 2.5% of the total density to\neither side). This number can be easily computed through the\ndistributions’s inverse cumulative distribution function.4 In our\nexample above, we have N = 4, ¯θ = 1\n4(2\n5 + 2\n5 + 2\n5 + 3\n5) = 0.45 and\ns2 = 1\n3(0.052 + 0.052 + 0.052 + 0.152) ≈0.0111. With c ≈3.182,\nthis gives us a confidence interval of [0.31, 0.59]. This interval\ncan be interpreted in the following way: If we were to repeat this\nexperiment, say, 100 times, the resulting confidence intervals would\ncontain the true value 95 times. For the samples shown above a\nfair coin was used, and thus the confidence interval contains the\ntrue value of 0.5. It should be noted here that confidence intervals\nare sometimes available in closed form, for instance for the normal\ndistribution.5 This simplistic example also assumed confidence\nintervals to be both symmetrical and two-sided (i.e. having a\nlower and upper bound). For a more thorough and comprehensive\ntreatment of confidence intervals we refer to other works such as\nZech (2002); Smithson (2003).\nBootstrapping and Jackknife.\nIn the previous example,\nwe were able to successfully obtain a confidence interval for the\n4 Using a software library such as scipy, we can for instance compute\nscipy.stats.t.ppf(0.975, df=3).\n5 The reasoning goes as follows: By Cochran’s theorem, if the distribution is\nnormal, the sample mean and variance are independent (Cochran, 1934). But\nthe reverse is also true, so with an independent sample mean and variance we\ncan assume that the underlying distribution is normal. We can again construct\nthe t-statistic in Equation (2.3) to obtain confidence intervals e which are\nθ ∈[¯θ −tn−1,1−α/2\ns\n√\nN , ¯θ + tn−1,1−α/2\ns\n√\nN ], where tN−1,1−α/2 stands for the\n1 −α/2-th quantile of a t-distribution with N −1 degrees of freedom and a\n1 −α confidence level (Krishnamoorthy, 2006, p. 130).\n2.1 what is uncertainty, anyway?\n17\nprobability of heads.\nHowever, this required us to repeat the\ncoin flipping experiment multiple times. In the case of flipping a\ncoin, this is rather straightforward—nevertheless, we might also\ninterested in quantifying the uncertainty about the estimate in\ncases where obtaining a new sample is difficult or expensive (e.g.\nwhen an experiment require expensive computational hardware).\nA tool for these cases is given in the form of bootstrapping (Efron,\n1992; Tibshirani and Efron, 1993):\nInstead of collecting new\ndata, we can instead perform inferences from our existing sample\nthrough re-sampling: We sample randomly from our initial set\nof coin flips with replacement,6 and obtain a number of new\n(pseudo-)samples. We then use these to estimate the confidence\nintervals of our estimate in a similar fashion to the confidence\nintervals in the previous paragraph: Drawing N = 10 re-samples,\nusing the same procedure as in Equations (2.1) and (2.4), we\nobtain a confidence interval of [0.26, 0.62]. Nevertheless, there are\nknown problems with the bootstrap: When our sample size small\n(just five coin flips), the sample might not be representative, and\nbootstrap samples can amplify any bias present in the sample.\nHere, having two heads and three tails differs slightly from the\nactual probability of 0.5, which is then carried over into the\nbootstrap samples. Another, similar estimator is the jackknife\n(Quenouille, 1949; Tukey, 1958), where we do not resample, but\ninstead create new samples by leaving out one observation at\na time. Therefore, the original set of coin flips would yield the\nfollowing new pseudo-samples:\n1\n1\n→ˆθ−1 = 2\n4 = 0.5\n1\n1\n→ˆθ−2 = 2\n4 = 0.5\n1\n1\n1\n→ˆθ−3 = 1\n4 = 0.25\n1\n1\n1\n→ˆθ−4 = 1\n4 = 0.25\n1\n1\n→ˆθ−5 = 2\n4 = 0.5\nWe again repeat our procedure in Equations (2.1) and (2.4) to\nobtain the confidence interval of [0.25, 0.55]. In order to make a\nprediction about a new coin flip, in all of three cases we would\n6 Sampling with replacement implies that our new samples might contain dupli-\ncates.\n2.1 what is uncertainty, anyway?\n18\nsimply declare head with a probability of the estimated ˆθ or hedge\nour bets using the range of values contained in the confidence\ninterval.\nLikelihood Functions.\nA useful tool to evaluate the fit of a\nparameter estimate for the data are likelihood functions.\nThe\nlikelihood p(D | θ) quantifies how well the choice of a value for\nθ “explains” the observations D, meaning how likely the value is\nto have generated the data or how consistent the data are with\nthe chosen value. Accordingly, a high likelihood expresses that a\nvalue of θ is consistent with the observations, while a low likelihood\nsuggest that θ is unlikely to have generated the data. For the coin\nflipping example, we can choose a Bernoulli likelihood:\nBernoulli(x | θ) = θx(1 −θ)(1−x).\n(2.9)\nGiven the probability of heads θ, it assigns a probability of\nan outcome x, i.e. heads or tails. In line with the intuition that\nmore suitable values of θ assign higher likelihoods to the data, a\nquick derivation reveals that the mean ˆθ we used is actually the\nparameter that maximizes the likelihood of our sample:\np(D | θ) =\nN\nY\ni=1\np(xi | θ) =\nN\nY\ni=1\nθx(1 −θ)(1−x)\n(2.10)\nlog p(D | θ) =\nN\nX\ni=1\nxi log(θ) + (1 −xi) log(1 −θ)\n(2.11)\n∂\n∂θ log p(D | θ) =\nN\nX\ni=1\nxi\nθ −1 −xi\n1 −θ\n!= 0\n(2.12)\n0 =\nN\nX\ni=1\nxi(1 −θ)\nθ(1 −θ) −(1 −xi)θ\nθ(1 −θ)\n(2.13)\n=\nN\nX\ni=1\nxi −\u001a\u001a\nxiθ −θ + \u001a\u001a\nxiθ\n(2.14)\nθMLE = 1\nN\nX\ni=1\nxi.\n(2.15)\nHere, we used the i.i.d. assumption (identically, independently\ndistributed) to argue that since observations xi are independent,\nwe can factor the joint distribution p(D | θ) ≡p(x1, . . . , xN | θ)\ninto a product of its individual likelihoods. Then, we transfer\nthe computation into log-space for convenience, and identify the\nestimate by taking the derivative w.r.t. θ, setting to zero, and\n2.1 what is uncertainty, anyway?\n19\nsolving for it. This is referred to as the maximum likelihood estimate\n(MLE).\nRecap.\nThe above methods have illustrated the viewpoint of\nfrequentist statistics: Parameter estimates are derived from the\nactually observed data, and our uncertainty about the estimates\ncan expressed through confidence intervals, in which we expect our\ntrue value to fall. These can be obtained by relating estimates\nof the parameter for instance to the Student’s-t distribution by\nexploiting the central limit theorem. Furthermore, other estimates\ncan be obtained by collecting more data or through procedures such\nas the bootstrap or the jackknife. In our case, we knew that the\nused coin was fair, and that the initial sample simply ended up not\nrepresentative due to using an uneven number of observations. In\na similar but more realistic scenario, we might not know anything\nabout the properties of the coin (or the phenomenon of interest),\nbut might still suspect, at least without any other information\navailable, that it is fair. The frequentist framework does not give\nus any means to incorporate this belief into our reasoning, but the\nBayesian view presented in the next section does.\n2.1.2\nThe Bayesian Perspective\n“There is a valid defence of using non-Bayesian methods,\nnamely incompetence.”\n—John Skilling in Fundamentals of MaxEnt in Data Analysis\n(Skilling and Sibisi, 1990).\nBayesian statistics delineates itself from frequentist statistics by\nseeing probability itself as more than just the mere relative fre-\nquency of an event, and instead as the degree of belief in the\noccurrence of an event.7 This difference has caused (and is still\ncausing) ideological chasms among statisticians, as illustrated by\nthe quote above. The name of Bayesian statistics is derived from\nThomas Bayes, an English presbytarian minister in the 18th cen-\ntury who first formulated the eponymous Bayes’ theorem. It should\nbe noted however that Bayes only formulated his theory in a very\nspecific setting,8 and that a general version of Bayesian statistics\nwas instead pioneered by Pierre-Simon Laplace (McGrayne, 2011;\nLeonard, 2014). The theorem can be formulated as follows: Given a\n7 Even though there are also subtle nuances to this definition, see for instance\nGood (1971).\n8 Namely, using a uniform prior. See Equation (2.16) and onward.\n2.1 what is uncertainty, anyway?\n20\nset of observations D and a parameter of interest θ, we can express\nthe probability of the parameter given the observational data as\np(θ | D) = p(D | θ)p(θ)\np(D)\n,\n(2.16)\nwhere the different parts of the equation are commonly referred\nto as the posterior p(θ | D), the likelihood p(D | θ), the prior p(θ),\nand the evidence p(D). We already discussed likelihoods in the\nprevious section. The prior p(θ) is a probability distribution over\npossible values of θ, and thus allows us to express our prior belief\nby attributing higher probability to values of θ we deem more\nlikely. This also implies a philosophical difference with frequentist\nstatistics: While θ was treated as an unknown constant before, it is\nnow seen as another random variable. The evidence p(D) encodes\nthe general probability of the observed data under any value of\nθ. This somewhat hidden interpretation becomes more clear when\nrewriting the term:\np(D) =\nZ\np(D, θ)dθ =\nZ\np(D | θ)p(θ)dθ.\n(2.17)\nWe can therefore interpret the evidence as the likelihood of the\ndata averaged over all possible parameter values of θ, weighed by\ntheir prior probabilities. Lastly, the posterior p(θ | D) describes a\nprobability distribution over values of θ given our observations. We\ncan think of the posterior as starting with our prior belief, using\nthe data to update it and arriving at a final distribution that takes\nboth of these into account. This has several advantages: We can\nnow choose to encode our suspicions about the value of the target\nparameter into the prior. But as we will see, obtaining more and\nmore data points results in outweighing the prior belief, completely\nrelying on the observations in the limit.\nCoin Flipping Redux.\nWe now illustrate these concepts using\nthe coin flipping example from Section 2.1.1, showing how uncer-\ntainty is modeled from the Bayesian perspective. In order to do\nso, we first have to make some design choices, i.e. the choice of\nlikelihood and prior function as well as prior parameters. We again\nuse the Bernoulli likelihood from the previous section, and now\nwould like to define a prior over θ. A good choice for a prior for\nthe Bernoulli distribution is the Beta distribution:\nBeta(θ; α1, α2) =\n1\nB(α1, α2)θα1−1(1 −θ)α2−1\n(2.18)\nB(α1, α2) = Γ(α1)Γ(α2)\nΓ(α1 + α2) ,\n(2.19)\n2.1 what is uncertainty, anyway?\n21\n(a) Beta priors.\n(b) Beta posteriors.\nFigure 2.1: Plots of different choices of (a) Beta prior distributions and\ntheir resulting (b) posterior distributions after observing our initial set\nof coin flips. Juxtaposing the two plots illustrates how the choice of prior\nbelief can influence the shape of the resulting posterior distribution.\nwith Γ(·) denoting the Gamma function, a generalization of the\nfactorial to the real numbers. The distribution has its support on\n[0, 1] and possesses two shape parameters α1, α2 ∈R+, which we\ncan use to encode our prior belief about θ. A few examples for the\nresulting distribution are shown in Figure 2.1a. Choosing the Beta\ndistribution as a prior comes in handy when analytically deriving\nthe posterior distribution, since it is conjugate to the likelihood.\nConjugacy here means that using a Beta prior together with a\nBernoulli likelihood as in Equation (2.10), the posterior has the\nform of a Beta distribution.9 Bayes’ rule contains the unwieldy\nevidence term, which we established in Equation (2.17) can in some\ncases be evaluated analytically using an integral over parameters.\nHowever, we can notice that the evidence p(D) does not depend on\nthe parameters directly, and only scales the posterior p(θ | D) by a\nconstant. As such, we can declare the posterior to be proportional\nto the product of the likelihood and prior:\np(θ | D) = p(D | θ)p(θ)\np(D)\n∝p(D | θ)p(θ) =\nN\nY\ni=1\np(xi | θ)p(θ), (2.20)\n9 Conjugate priors are available for distributions that can be generalized to a\nparticular form which is referred to as exponential families, including popular\ndistributions such as the Normal, Poisson, Bernoulli, and categorical distri-\nbution and more (Bishop and Nasrabadi, 2006; Gelman et al., 2021; Efron,\n2022).\n2.1 what is uncertainty, anyway?\n22\nwhich simplifies solving for the posterior parameters. We now\nsubstitute the expressions in Equations (2.9) and (2.18) into Bayes’\nrule in Equation (2.16) and continue in log-space for convenience:\nlog p(θ | D) ∝\nN\nX\ni=1\nlog p(xi | θ) + log p(θ)\n(2.21)\n=\nN\nX\ni=1\nxi log θ + (1 −xi) log(1 −θ)\n+ (α1 −1) log θ + (α2 −1) log(1 −θ)\n(2.22)\n=\n\u0010\nα1 +\nN\nX\ni=1\nxi −1\n\u0011\nlog θ\n+\n\u0010\nα2 + N −\nN\nX\ni=1\nxi −1\n\u0011\nlog(1 −θ),\n(2.23)\nwhere we can see that in the end—after dropping the log-Beta\nfunction as it is just a constant—we obtain the form of a Beta\ndistribution, but this time with the new shape parameters α(N)\n1\n=\nα1 + PN\ni=1 xi and α(N)\n2\n= α2 + N −PN\ni=1 xi. Compared to the\nprior parameter values, they now contain information about the\nobservations that we have made. We can use these new parameters\nto visualize our posterior for our initial set of coin flips and our\ninitial choices of priors in Figure 2.1b. Similar to the frequentist\nconfidence intervals of the previous sections, the uncertainty about\nthe true value of θ is encoded in the spread of the posterior distri-\nbution. As we gather more observations, we expect the posterior\nto become more and more narrow around one (or few) values of θ.\nSimilarly to the maximum likelihood estimate in Equation (2.15)\nthat helps us determine the parameter value which is most likely to\nhave generated our observations, we can derive a similar quantity in\nthe Bayesian setting. This is referred to as the posteriori estimate\n(MAP), and can be interpreted as the most likely value of θ given\nthe data and a choice of prior. We can derive the MAP using the\nposterior in Equation (2.21) and solving for θ:\n∂\n∂θ log p(θ | D)\n!= 0\n(2.24)\n(1 −θ)\n\u0010\nα1 +\nN\nX\ni=1\nxi −1\n\u0011\n= θ\n\u0010\nα2 + N −\nN\nX\ni=1\nxi −1\n\u0011\n(2.25)\nˆθMAP = α1 + PN\ni=1 xi −1\nα1 + α2 + N −2 .\n(2.26)\n2.1 what is uncertainty, anyway?\n23\nFigure 2.2: Highest density intervals (gray regions) and maximum a\nposteriori estimates (red vertical lines) for different Beta posteriors.\nHighest Density Intervals.\nOne way to now quantify the\nuncertainty about our estimate for θ is to create the Bayesian\ncounterpart of confidence intervals: The highest density interval\n(HDI; also referred to as the credible interval). The HDI describes\nthe ranges of values of the posterior distribution that covers 95%\n(or some other number) of the total density. Thus, our estimate has\na posterior probability of 95% to fall within this interval. For the\nprior and posterior distributions in Figure 2.1, we obtain ˆθ1 ≈0.44,\nHDI1 ≈[0.16, 0.76], ˆθ2 ≈0.43, HDI2 ≈[0.13, 0.77], and ˆθ3 ≈0.49,\nHDI3 ≈[0.18, 0.80], with the HDIs and MAP estimates shown in\nFigure 2.2. Since the second prior places less belief on a value of\nθ = 0.5, the slightly skewed initial sample of coins ˆθ = 0.4 shifts\nthe posterior estimate and HDI slightly towards the left. In the\nthird case, our prior belief is highly biased towards higher values\nof θ, which is also reflected in the obtained posterior estimate, the\nMAP estimate θMAP and its HDI. However, confidence intervals\nfrom Section 2.1.1 and the HDIs have very different interpretations,\nwhich echo the differences in frequentist and Bayesian thinking: The\nconfidence intervals imply that, if we were to repeat our experiment\n100 times, the true value for θ would be covered by the CIs 95 out\nof 100 times. In contrast, the HDI draws a conclusion about the\nrange of values be believe the true parameter value to lie in, based\non our prior belief updated using our actual observations.\nPredictive Uncertainty.\nSo far, we have discussed the un-\ncertainty in our parameter estimate, but Bayesian statistics also\nprovides a useful tool to reason about new observations: Predictive\ndistributions. Let us assume we would like to make a prediction\nabout the observation x′ stemming from a new coin flip. We can\nwrite this probability as follows:\np(x′) =\nZ\nΘ\np(x′ | θ)p(θ)dθ.\n(2.27)\n2.1 what is uncertainty, anyway?\n24\nThis is referred to as the prior predictive distribution, which\ngives us an instrument to reason about the outcome using the\nspecified prior alone, disregarding any observations. One way to\ninterpret this distribution is as a weighted aggregate of predictions\nfor x′ using different values of θ, which are weighed according to\nour prior belief. In the case of the Bernoulli and Beta distribution\nin the coin flip sample, this distribution has an analytical form:\nP(x′) =\nZ\nΘ\nP(x′ | θ)p(θ)dθ\n(2.28)\n=\nZ 1\n0\nθx′(1 −θ)(1−x′)\n1\nB(α1, α2)θα1−1(1 −θ)α2−1dθ\n(2.29)\n=\n1\nB(α1, α2)\nZ 1\n0\nθα1+x′−1(1 −θ)(α2−x′)dθ\n(2.30)\n= B(α1 + x′, α2 −x′ + 1)\nB(α1, α2)\n,\n(2.31)\nwhere the last step used the fact the Beta function can be expressed\nas B(α1, α2) =\nR 1\n0 xα−1(1 −x)α2−1dx (see Appendix A.1 for more\ndetails). While it is useful to check whether a chosen prior is\nsuitable for a given task, the prior predictive does not take any\nobservations into account yet, so we would usually consider a\npredictive distribution given some available data.\nThis is the\npurpose of the posterior predictive distribution, defined as\np(x′ | D) =\nZ\nΘ\np(x′ | θ)p(θ | D)dθ.\n(2.32)\nAgain, we arrive at a prediction by “averaging” predictions\nmade using different values of θ. Since not all values of θ are\nequally plausible given our data, they are furthermore weighted\nby their probability under the posterior p(θ | D). In a frequentist\nanalysis, we only consider a single point estimation of ˆθ instead of\na distribution. In terms of Equation (2.32), this can be expressed\nwith the help of a Dirac delta function:\np(x′ | D) ≈\nZ\nΘ\np(x′ | θ)δ(θ −ˆθ)dθ = p(x′ | ˆθ),\n(2.33)\nwhere we recover using only a single estimate ˆθ for our prediction.\nBack to the coin flip example, we can apply a similar argument as\nin Equation (2.28) with the posterior instead of the prior to arrive\nat\nP(x′ | D) = B(x′ + α(N)\n1\n, 1 −x′ + α(N)\n2\n)\nB(α(N)\n1\n, α(N)\n2\n)\n.\n(2.34)\n2.1 what is uncertainty, anyway?\n25\nInterestingly, we can interpret the two terms in the right-hand\nside of Equation (2.32) as two different sources of uncertainty:\nThe aforementioned uncertainty about the true value of θ given\nobserved data is encoded in p(θ | D), and the uncertainty about x′\ngiven a fixed parameter value in p(x′ | θ). This interpretation gives\nrise to the distinction of data (or aleatoric) uncertainty and model\n(or epistemic) uncertainty. The former usually refers to irreducible\nuncertainty that is inherent to the phenomenon we would like to\nmodel, like inherent ambiguity or unavoidable noise, and refers to\np(x′ | θ). The latter describes our uncertainty about the correct\nmodel parameters and resides in p(θ | D).10 The more data we\ngather, the more we assume the posterior to be concentrated on only\nthe most plausible parameter values, and thus the uncertainty is\nreduced. In the frequentist approach, tools like confidence intervals\ncan only tell us about the total uncertainty of our estimate. In the\nBayesian approach however, these different notions of uncertainty\nare represented by different distributions. These considerations\nare the basis for Bayesian deep learning methods, which we will\ndiscuss more in Section 2.2.2.\nRecap.\nWe have seen in this section how Bayesian statistics\ntakes a very different approach to uncertainty than frequentist\nstatistics: In frequentist statistics, probabilities are seen as relative\nfrequencies of an event as we repeat an experiment. In Bayesian\nstatistics, this interpretation is abandoned in favor of an viewpoint\nthat sees probabilities as the degree of belief in an event, and\nparameters of interest becoming random variables instead of\nunobserved constants. It allows us to specify a prior belief which is\nupdated using observations, and which diminishes in importance\nas we encounter more and more data. Furthermore, we can use\npredictive distributions to reason about unseen outcomes.\nIn\nthe posterior predictive distribution, we can also distinguish two\nkinds of uncertainty: Irreducible data uncertainty and model\nuncertainty, reducible by obtaining more data.\nSo far we have only discussed view of uncertainty from the\nperspective of statistics, defining models that explain observations\nand make new predictions. Despite their usefulness, it is no obvious\nhow to apply these statistical models to phenomena as complex as\nhuman language, which we turn to next.\n10Here, this categorization is approached from a general standpoint. We will\ndiscuss how these notions of uncertainty materialize in a language context in\nSection 2.3 and point out some problems and nuances.\n2.1 what is uncertainty, anyway?\n26\n2.1.3\nThe Linguistic Perspective:\nUnderspecification, Ambiguity &\nVagueness\nLinguistics can be categorized into multiple sub-disciplines that\nare concerned with different aspects of human language (Akmajian\net al., 2017). This thesis focuses on written language, which is why\nwe will not discuss any uncertainty in e.g. phonetics and phonology\n(the studies of the production of sounds and how they are organized\nin a language). Instead, we focus on the following three levels: se-\nmantics, syntax and pragmatics. In linguistics, uncertainty appears\nthrough different phenomena, for instance ambiguity or polysemy\n(Tuggy, 1993; Kennedy, 2011), underspecification (Pustejovsky,\n1991, 2017) and vagueness (Tuggy, 1993; Brown, 2005; Kennedy,\n2011), which manifests in different ways in different linguistic levels.\nThis creates uncertainty by creating multiple different interpre-\ntations of a sentence, which are often—but not always—resolved\nthrough additional context, either linguistic, situational or from\nworld knowledge. Describing this interplay between uncertainty\nand resolve on different linguistic levels is goal of this chapter.\nUncertainty in Semantics.\nThe field of semantics is concerned\nwith the literal meaning of words and the ways in which these are\ncombined (Kearns, 2017). One way in which uncertainty arises in\nsemantics is polysemy, a phenomenon where two or more distinct\nsenses are associated with the same word (Gries, 2015). Gries for\ninstance mentions the examples of “I emptied the glass” compared\nto “I drank a glass”, where glass corresponds in the first case to a\ncontainer, and to its content in the second. A more subtle case of\npolysemy is exemplified by the examples\n(a) Jocelyn walked to the school.\n(b) The concerned mother talked to the school.\nwhere “school” in the former refers to the physical building, and\nthe latter to the an administrative unit inside the organization\nthat operates within the school building (Frisson, 2009). Resolving\nthese cases can be highly non-trivial, leading in NLP to the field\nof word sense disambiguation (see e.g. Schütze, 1997; Agirre and\nEdmonds, 2007; Navigli, 2009). Another case is homonymy, where\ntwo unrelated meanings map onto the same form (Devos, 2003), as\nin the case of bank as a financial institute, a place for sitting, or the\nterrain alongside a river bed. Vagueness can be defined in contrast\nto these notions as whether “a piece of semantic information is\npart of the underlying semantic structure of the item, or the\n2.1 what is uncertainty, anyway?\n27\nresult of a contextual specification” or simply “the notion that\ncertain features are not expressed in a representation” (Frisson,\n2009; Geeraerts, 1993). In their example, they show how for “my\nneighbor is a civil servant”, neighbor is not ambiguous since it does\nnot require disambiguation in the given context, despite the word\nbeing underspecified (i.e., the neighbor’s gender is for instance\nunderspecified). Vagueness and underspecification are ubiquitous\nin language, since terms like tall or red are gradual and highly\nsubjective terms (Brown, 2005) or simply because a speaker (or\nlistener) is lacking information (Williamson, 2002). In addition,\nthe meaning of some words might be underspecified unless or\nbecause it is combined with other words (Pustejovsky, 1991). The\nprinciple of compositionality states that the meaning of a more\ncomplex expression depends—completely or at least in part—on\nthe meaning of its constituents (Fodor, 2001; Szabó, 2004; Brown,\n2005). While composition of simpler to more complex expressions\ncan help to resolve underspecification (“my female neighbor is\na civil servant”), it can also create new underspecification, for\ninstance through multiple quantifiers or prepositional phrases with\nmultiple attachments (Pustejovsky, 2017, see next paragraph for\nan example).\nS\nNP\nI\nVP\nV\nsaw\nNP\nPron\nher\nN\nduck\n(a) Parsing duck as a noun.\nS\nNP\nI\nVP\nVP\nV\nsaw\nNP\nPron\nher\nVP\nV\nduck\n(b) Parsing duck as a verb.\nFigure 2.3: Two equally valid parse trees for the sentence “I saw her\nduck” using a constituency grammar.\nUncertainty in Syntax.\nSyntax describes the machinery that\ncombines the meaning of words and subwords into bigger units,\nsuch as phrases and sentences (Koeneman and Zeijlstra, 2017). In\norder to model this system, different grammatical formalisms have\nbeen proposed (Varile et al., 1997; section 3.3), which describe\nsets of rules that analyze a sentence in terms of a hierarchical\nstructure that describes the relationship between words. These\ninclude constituency grammars, which will be used for illustrative\n2.1 what is uncertainty, anyway?\n28\npurposes here. The core idea of this concept lies in observation\nthat words can behave as either single units, or clump together\nto comprise units of meanings, called constituents (Jurafsky and\nMartin, 2022). Constituency grammars describe the rules according\nto which these constituents combine into more and more complex\nunits of meanings. For instance, the phrase “the duck” consists of\na determiner (Det), or article, “the”, as well as a noun (N), “duck”.\nTogether, they are denoted as a noun phrase, or simply NP. In the\nsame fashion, we can assign categories like pronoun (Pron), verb (V)\nand verb phrase (VP), that culminate in a sentence (S). An example\nof an analysis using a constituency grammar is given in Figure 2.3:\nHere, the words in the sentence “I saw her duck” are combined along\nthese rules.11 However, the word duck can be read both as the\naction of suddenly crouching and a word describing aquatic fowl.\nIn this former interpretation, “her” is read as an object instead\nof a possessive pronoun. The corresponding parse tree is given in\nFigure 2.3a. The alternative reading as a possessive pronoun is\nshown in Figure 2.3b. By themselves, the two parse trees might be\nequally valid grammatical analyses given a constituency grammar.\nThis implies that this structural ambiguity is unresolvable without\nany further context or world knowledge. Structural ambiguity can\narise in a variety of situations depending on the language in question\n(see for instance Taha, 1983 for examples in English). Figure 2.3\ndepicts an attachment ambiguity: It is unclear whether her and\nduck attach as a combined NP to the VP of saw, or whether all\nthree parts are equal constituents of a combined VP. Other popular\nexamples include the attachment of (specifically) propositional\nphrases (“I saw the man with the telescope”; Schütze, 1995; Hindle\nand Rooth, 1990) or coordination (“old men and women”; Frazier\net al., 2000; Engelhardt and Ferreira, 2010). Uncertainty can also\nappear in the processing of language when awaiting additional\ncontext. A famous example of this are garden path sentences,\ni.e. sentences that contain surprising syntactical elements that\nrequire a re-analysis of the sentence structure thus far (Sturt et al.,\n1999). The most famous example is the sentence “the horse raced\npast the barn fell”, for which the corresponding syntactical parse\ntrees are shown in Figure 2.4. Before observing the last word, the\nsentence in Figure 2.4a exhibits a simple structure of subject (“the\nhorse”), verb (“raced”) and a prepositional phrase (“past the barn”).\nAfter encountering “fell”, we realize that “raced” was indeed not\nthe main verb of the sentence, and instead is used to describe\n11These rules can also defined more formally in the form of a context-free\ngrammar, where rules are applied regardless of a context. The exact rules are\nomitted here for the sake of clarity, but it should be noted that is a simplifying\nassumption, as natural language is not context-free (Savitch et al., 2012).\n2.1 what is uncertainty, anyway?\n29\nthat the horse that fell did so after having raced past the barn.\nStructurally, this requires the VP of “raced” in Figure 2.4b to be\ngrouped under the subject NP, and a new VP to be created for\n“fell”. Experimental evidence has shown that such ambiguous or\nchallenging constructions can lead to an increase in human reading\nand processing times (see e.g. Milne, 1982; Ferreira and Henderson,\n1991; Swets et al., 2008), suggesting that some form of re-analysis\nmight occur.12\nS\nNP\nDet\nthe\nNN\nhorse\nVP\nV\nraced\nPP\nIN\npast\nNP\nDet\nthe\nNN\nbarn\n(a) Parse before the last word.\nS\nNP\nNP\nDet\nthe\nNN\nhorse\nVP\nV\nraced\nPP\nIN\npast\nNP\nDet\nthe\nNN\nbarn\nVP\nV\nfell\n(b) Parse after the last word.\nFigure 2.4: Parse trees for the garden path sentence “The horse raced\npast the barn fell”, before and after adding the last word, prompting a\nre-analysis of the sentence, where “raced past the barn” attached to the\nNP and “fell” becomes the new main verb.\nFigure 2.5: Double triangle of language production by Baan et al. (2023)\nas an extension of the triangle of reference by (Ogden and Richards,\n1923).\nUncertainty in Pragmatics.\nPragmatics can be defined as the\nstudy of language in use, especially in social interactions and speech\n12Interestingly, similar effects have been observed in neural models (Van Schijndel\nand Linzen, 2018; Irwin et al., 2023), although the relationship is weaker in\nrecent transformer models (Oh and Schuler, 2023; Oh et al., 2024).\n2.1 what is uncertainty, anyway?\n30\n(Mey, 2006; Huang, 2014). Compared to semantics, it also studies\nhow word meanings are affected in the context of a specific utterance\n(Kearns, 2017). Baan et al. (2023) demonstrate its connection to\nuncertainty, specifically in natural language generation, through\nan extension of the “triangle of reference” by Ogden and Richards\n(1923), which is shown in Figure 2.5: Given an input to the speaker,\nthere is a potentially wide set of possible inferred meanings; this\ncan be caused by errors, underspecification (for instance where in\nsome language the gender of a subject is not specified explicitly)\nor ambiguities of syntactical or semantic nature, as discussed in\nthe previous paragraphs. This mapping from utterance to meaning\nis therefore not one-to-one, but rather one-to-many (Grice, 1957;\nKennedy, 2011). As the speaker prepares their utterance, they\nthen choose one of a variety of similar or even equivalent meaning\nto express the intended utterance. This production process is\ninfluenced by the speaker’s social and cognitive idiosyncrasies\n(Levelt, 1993). We will refer to these two sources of uncertainty\nas input and output variability or paraphrasticity in the rest of\nthesis. This describes an important difference between language\nand other modalities: Since language is paraphrastic, there are\n(almost) equally valid ways to express the same intended meaning,\nwhich however might differ completely in their realizations, i.e.\nwordings.13\n2.1.4\nThe Linguistic Perspective: Expressing\nUncertainty\nBesides paraphrastic language, a different type of uncertainty lies\nin explicit uncertainty expressions by the speaker. This spans\nthe overall tone of a series of utterances to the usage of diverse\nlinguistic expression (see e.g. Rubin, 2006 pp. 21–40; Lorson et al.,\n2023, Zhou et al., 2023), for instance hedges (Lakoff, 1973; Fraser,\n1975; Prince et al., 1982; Holmes, 1982), i.e. words or phrases\nto express ambiguity or uncertainty. Additionally, uncertainty\nexpressions might also be chosen circumstantially, for instance\nbased on whether the other speaker is cooperative or uncooperative\n(Lorson et al., 2021), politeness (Sirota and Juanchich, 2015;\nHoltgraves and Perdew, 2016) or power differences between\n13Some works argue against the concept that two expressions can be fully\nequivalent; for instance Widoff (2022) points out how two expressions can be\nequal in some form, but unequal in others (e.g. “the water in the glass” and\n“the glass is half-full” convey a similar meaning, but the second one does not\nspecify the content) or how for instance instruments like passive voice can be\nbe used to convey intent (“Hans beats Peter” vs. “Peter is beaten by Hans”).\n2.1 what is uncertainty, anyway?\n31\nspeakers (Bonnefon and Villejoubert, 2006).\nSemantic Uncertainty\nEpistemic\nLack of knowledge;\nNeither true or false\nHypothetical\nPossibilities that\ncan be true or false\nParadoxical\nInvestigative\nUncertain until\ninvestigated\nConditional\nUncertainty about\nconditions; if / else\nexpressions\nNon-epistemic\nDoxastic\nExpression\nof beliefs\nDynamic\nDuties, plans,\ndesires\nFigure 2.6: Taxonomy of different semantic uncertainties adapted from\nKolagar and Zarcone (2024), originally based on the work by Szarvas\net al. (2012).\nFigure 2.6 shows a taxonomy of semantic uncertainties by\nKolagar and Zarcone (2024), based on the works of Szarvas\net al. (2012); Vincze (2014).\nIt proposes a categorization of\nexpressed uncertainty based on the truth value of an utterance.\nThe taxonomy divides semantic uncertainty first into epistemic,14\nwhere the speaker expresses worlds which are neither true or\nuntrue and do not coincide with their actual world. To make\nthis notion less abstract, consider the following example: In the\nsentence “This is the best dessert I have ever had”, we take the\nsentence to be a fact, and therefore assign a positive truth value.\nNow, we can instead use a modal verb to say “This may be the best\ndessert I have ever had”. While one can imagine a possible world\nin which this statement is true, we cannot assign it a definitive\ntruth value per se. The alternative branch in the taxonomy are\nhypotheticals, which can also be uncertain, but in contrast to\nepistemic uncertainty, also have the possibility of being evaluated\nas true or false. One bifurcation, paradoxical, refers to cases in\nwhich the truth value depends on another propositions, for instance\nif / else expressions (conditional) or cases in which the truth value\ncan only be evaluated after further examination (investigative).\nThe other fork, non-epistemic, describes circumstances in which a\nspeaker expresses beliefs (doxastic) and duties, plans or desires\n14Not be confused with the epistemic or model uncertainty in a statistical sense\nin Section 2.1.2.\n2.1 what is uncertainty, anyway?\n32\n(dynamic).\nRecap.\nWe have now discussed a variety of sources of uncertainty\nin linguistics, located on different levels of language use, including\nsemantics, syntax and pragmatics. These discussions were mostly\ninformed by phenomena in the English language and are thus\nlimited, as other types of ambiguity exists that were not discussed\nhere (see for instance Li et al., 2024a). We can nevertheless distill\ncertain insights: On the one hand, uncertainty arises as an inherent\nproperty of language, through polysemy, structural ambiguities or\npossible paraphrases. On the other hand, uncertainty is a tool that\nbe employed by a speaker to express their own uncertainty and to\nexpress the state of potential worlds. In both cases, this creates\nchallenges for any processing system that operates on language\nand tries to infer its meaning.\n2.1.5\nA Pragmatic Answer\nThe astute reader might have noticed that while the title of\nSection 2.1 was “what is uncertainty, anyway?”, it might appear\nthat we have thus far been tiptoeing around this question,\nenumerating and explaining different perspectives to it without\ngiving a satisfying answer.\nIn the end, uncertainty is a multifaceted and perhaps vague\nconcept, whose definition varies based on the phenomenon of\ninterest. At its very core, it describes a lack of knowledge about\nthe true state of the world among competing alternative states.\nThe definitions of these world states can differ tremendously on\nthe context, and can include all the possible interpretations of the\nsentence “I saw her duck” to plausible values of a data-generating\nparameter θ. For the purpose of this thesis, we reduce its definition\nto the following aspects: Firstly, there is the uncertainty that is\ninherent to language described in Section 2.1.3, describing how\ninterpretation and production are not a one-to-one processes of\nmeaning; Secondly, the statistical models we apply to language\nare themselves faced with multiple possible specifications and\ncan produce different potential predictions. As these models are\nat best informative but incomplete abstractions of reality that\nare fit on finite data, we accept their uncertainty as the price for\npracticality. While the last two points refer to uncertainty as\nphenomena, however and thirdly, uncertainty is also a tool: It\nenables us to reason about and express our own knowledge about\npossible states, and convey our lack thereof. This notion captures\n2.2 uncertainty in deep learning\n33\nboth the statistical sense, like considering different parameter\nvalues in the posterior predictive distribution, as well as making\nconditional statements or using linguistic modifiers to convey our\nbeliefs in natural language. As NLP involves different kinds of\nuncertainty both in its modeling tools and data modality, this\ncreates an intricate interplay between these uncertainties.\nSo far, we have looked at uncertainty in a fashion that is\ncompletely independent from neural networks, the core modeling\ntool of this thesis and main workhorse of contemporary artificial\nintelligence. Natural language processing specifically has adopted\nthe use of large neural models operating on language inputs (and\nsometimes also outputs). It thus lies in the intersection of linguistics\nand statistics, and we will review the implications on uncertainty\nmodeling next.\n2.2\nUncertainty in Deep Learning\n“In the 1950s and 60s, scientists built a few working\nperceptrons, as these artifical brains were called. [. . . ] This\nperceptron is being trained to recognize the difference between\nmales and females. [. . . ] After training on lots of examples,\nit [. . . ] is able to successfully distinguish male from female.\nIt has learned. While promising, this approach to machine\nintelligence has virtually died out.”\n—Clip about AI research in the 1950s and 60s, date unknown.\nIn contrast to the interviewer’s quote in the epigraph, the very\npromising approach of using computational models of neurons did\nnot completely die out, but rather remained dormant for decades.\nFirst known as cybernetics at the time of the first models of artificial\nneurons (McCulloch and Pitts, 1943; Rosenblatt, 1958; Rosenblatt\net al., 1962), it became known as connectionism in the 1980–1990s,\nbefore assuming its current name deep learning in 2006 (Goodfellow\net al., 2016). Nowadays, deep learning is commonly and vaguely\ndefined as a family of machine learning networks that employ artifi-\ncial neural networks of increasing depth (Goodfellow et al., 2016).15\nUncertainty in deep learning materializes in a wide variety of ap-\nproaches, as depicted as a hierarchical taxonomy in Figure 2.7: As\nshown in Section 2.2.1, the frequentist school uses neural networks\n15One might argue that modern NLP might be at least in a large part subsumed\nby this definition; for this purpose of this thesis we will treat them as overlap-\nping but different disciplines due to their history (see for instance Chapter 1 of\nJurafsky and Martin, 2022) and the peculiarity of language as a data modality,\nespecially compared to images or tabular data.\n2.2 uncertainty in deep learning\n34\nUncertainty Quant.\nin Deep Learning\nFrequentist\nConfidence\nCalibration\nPrediction Sets\nConformal Prediction\nBayesian\nEnsembling\nSampling\nMarkov Chain\nMonte Carlo\nStochastic Langevin\nDynamics\nLaplace\nApproximations\nVariational\nStochastic\nRegularizers\nBayes-by-backprop\nDeep Kernel Learning\nEvidential\nPrior Networks\nPosterior Networks\nOther\nHeuristic\nDirect Prediction\nDensity-based\nUncertainty Heads\nAuxiliary Models\nNeural SDEs\nCredal Sets\nNLP-specific\nBlack-box methods\nVerbalized Uncertainty\nFigure 2.7: Hierarchical Taxonomy, showing the different methods\ndiscussed in Section 2.2. Note that these shown categories are not\nnecessarily disjoint, as different methods can sometimes be placed into\nmultiple categories at once.\n2.2 uncertainty in deep learning\n35\nas powerful estimators of predictive parameters, which can be inter-\npreted similarly to the models in frequentist statistics we discussed\nearlier. In the same way, Bayesian methods can be applied to\nneural networks to quantify uncertainty through parameter poste-\nrior and posterior predictive distributions (Section 2.2.2). Other\napproaches take inspiration from the Dempster-Shafer theory of\nevidence (Section 2.2.3) or draw from entirely different ideas such\nas framing uncertainty quantification as a supervised learning task,\nstochastic differential equations, and more (Section 2.2.4).\n2.2.1\nFrequentist Neural Networks\nBefore we turn to how frequentist methods allow the quantification\nof uncertainty in neural networks, we first review the similarities\nin parameter estimation when applied to neural predictors. In the\nfollowing, we term the application of frequentist methods to neural\nnetwork as frequentist neural networks.\nAs introduced in Section 2.1.1, frequentist statistics refers to an\ninterpretation of probability as the relative frequency of an event.\nIn a neural network setting, the estimation of the parameter(s) of\ninterest, in this case the network’s parameters θ, is analogous to\nthe maximum likelihood estimation in Section 2.1.1. The main\ndifferences are that firstly, instead of parameterizing a distribution\nwith θ directly, we parameterize it with the prediction obtained\nfrom a neural net with parameters θ. Whereas in the coin flipping\nexample, θ referred to the probability of heads, a neural network in\na binary classification setting is equipped with some parameters θ\nnow predicts the probability of the positive class ˆp. And secondly,\ndue to the model’s non-linear and hierarchical dependencies, the\nsolution to θ is not available in closed form anymore. Instead,\nwe iteratively optimize θ through procedures such as gradient\ndescent, where we compute the gradient of some loss function\nw.r.t. the parameters and take a step in the direction of the (anti-\n)gradient.\nThe loss functions vary depending on the intended\npurpose, but in some cases can be directly related to maximum\nlikelihood estimation. In analogy to the coin flipping in the previous\nsection, a network trained on a binary classification task for instance\npredicts ˆp = σ(fθ(x)) (with σ(·) denoting the sigmoid function)\nand is then optimized using the binary cross-entropy loss (here for\na single input using a gold label y ∈{0, 1}):\nLBCE(y, ˆp) = −y log ˆp + (1 −y) log(1 −ˆp).\n(2.35)\n2.2 uncertainty in deep learning\n36\nThe resemblance to the Bernoulli log-likelihood in Equation (2.9)\nis no coincidence, and we can see that the loss is minimized when\nthe network prediction ˆp correspond to the actual probability p,\ne.g. the relative occurrence of the positive class. Thus we view\nmodel predictions, at least for classification problems, through a\nsimilar, frequentist lens.16\nConfidence & Calibration.\nTo illustrate frequentist uncer-\ntainty estimation further, we now move from a binary classification\nproblem to a multi-class classification problem. Formally, consider\na neural predictor fθ, a function mapping from an input space RD\nto an output space RK and with parameter vector θ. Here, K\ntypically refers to the number of classes in a classification problem\nand the output of the network is referred to as logits. These logits\nare then normalized, typically by using the softmax function ¯σ(·),\nto produce a categorical probability distribution over classes. Since\neach class is now associated with a probability score, we can refer\nto each of these probabilities as the confidence of fθ regarding a\ncertain class, or more formally\nˆpk = Pθ(y = k | x) ≡¯σ(fθ(x))k.\n(2.36)\nAlso, let\nˆy = argmax\nk∈[K]\nˆpk;\nˆp = max\nk∈[K] ˆpk\n(2.37)\nbe the class predicted by the model and its corresponding proba-\nbility, respectively. Ideally, a predicted probability of e.g. 45% for\nsome class would thus indicate that this is the correct prediction,\nin 45 out of 100 times, if we were to repeat the experiment. We\ncan formulate this requirement as\np\n\u0000y = ˆy | ˆp\n\u0001\n= ˆp.\n(2.38)\nThe degree to which this requirement is violated is measured\nthrough the expected calibration error (ECE; Naeini et al., 2015),\nwhich is defined as\nECE = E\nh\f\fp\n\u0000y = ˆy | ˆp\n\u0001\n−ˆp\n\f\f\ni\n.\n(2.39)\nThis expectation can for instance be computed by grouping N test\npredictions into M equally wide bins according to their confidence\n16Here, we mainly focus on classification problems, which tend to be more\nfrequent in NLP. Nevertheless, we can also consider a prediction from a trained\nregressor as frequentist by considering it as the mean of normal distribution\nwith a variance equal to some (inverse and homoskedastic) noise, see for\ninstance Bishop and Nasrabadi (2006), chapter 3.1.\n2.2 uncertainty in deep learning\n37\nˆp. Defining Bm as the set indices that belong to bin m, we can\nwrite the ECE as\nECE ≈\nM\nX\nm=1\n|Bm|\nN\n\f\f\f\n1\n|Bm|\nX\ni∈Bm\n1\n\u0000ˆyi = yi\n\u0001\n|\n{z\n}\nBin accuracy (target)\n−\n1\n|Bm|\nX\ni∈Bm\nˆpi\n|\n{z\n}\nAvg. bin confidence\n\f\f\f, (2.40)\nwhere 1\n\u0000ˆyi = yi\n\u0001\nis the indicator function showing whether the\nprediction was correct. As Guo et al. (2017) note, the terms in\nthe difference approximate the left-hand and right-hand side in\nEquation (2.38) per bin, respectively. However, the ECE has also\ndrawn several points of criticisms: The number of bins can also\ndistort the results when test points are unequally distributed or\nan unsuitable number of bins is chosen. Also, it is not a proper\nscoring rule (Savage, 1971; Gneiting and Raftery, 2007), meaning\nthat it is not necessarily minimized by the true distribution. For\nproper scoring rules, the true distribution should constitute the\nminimum, but for the ECE we can often minimize through a\nuniform distribution instead. Therefore, many other alternatives\nto the ECE have been proposed (e.g. Nixon et al., 2019; Kumar\net al., 2019; Zhang et al., 2020a; Gruber and Buettner, 2022;\nKirchenbauer et al., 2022; Roelofs et al., 2022; Błasiok and\nNakkiran, 2023; Chidambaram et al., 2024).\nUnfortunately, several works have shown that neural network\nmodels tend to be miscalibrated in general, with a tendency to be\noverconfident (e.g. Guo et al., 2017; Minderer et al., 2021; Zhu et al.,\n2023). Therefore, a vast library of methods has been proposed to\nimprove the calibration of neural networks. This includes post-\nprocessing of predictions, for instance by retraining or adjusting\nthe logits through additional scale and shift parameters (Platt\net al., 1999; Guo et al., 2017; Mozafari et al., 2019; Kull et al., 2019;\nWenger et al., 2020; Gupta et al., 2021; Ma and Blaschko, 2021).\nOthers have introduced custom loss functions (Mukhoti et al.,\n2020b; Karandikar et al., 2021; Bohdal et al., 2021; Ghosh et al.,\n2022; Hebbalaguppe et al., 2022; Tao et al., 2023) that are meant\nto disincentivize overconfidence on a specific class. This is since\nperforming maximum likelihood estimation of network parameters\nθ with objectives such as Equation (2.35) only seeks to maximize\nthe probability of the true class, but does not incentivize calibration\nper se. Other strategies involve tempering with the training data.\nThrough label smoothing (Szegedy et al., 2016; Müller et al., 2019;\nLukasik et al., 2020; Lienen and Hüllermeier, 2021b; Zhang et al.,\n2021a; Liu et al., 2022a; Park et al., 2023), where probability mass\nis dispersed from the ground truth class onto other classes, the\n2.2 uncertainty in deep learning\n38\nnetwork is taught to not assign maximal confidence to the ground\ntruth. For further regularization, mixup can be used (Zhang et al.,\n2018b; Thulasidasan et al., 2019; Maroñas et al., 2021; Zhang et al.,\n2022a; Noh et al., 2023; Wang et al., 2023a), where the network\nis trained on interpolations of two inputs. In this case, both the\ninput representations as well as their gold label distributions are\nmixed. Since miscalibration might also stem from a lack of training\ndata, an intuitive way to improve models is data augmentation\n(Hendrycks et al., 2020; Patel et al., 2021). It has also been observed\nthat ensembling (Lakshminarayanan et al., 2017; Wen et al., 2020a;\nAshukha et al., 2020; Zhang et al., 2020a; Wu and Gales, 2021;\nRahaman and Thiéry, 2021; Wen et al., 2021; Seligmann et al.,\n2024) and Bayesian modeling approaches (Mitros and Namee, 2019;\nMaroñas et al., 2020; Izmailov et al., 2021; Fortuin et al., 2022)\ncan improve calibration (see Section 2.2.2).\nPrediction Sets & Conformal Prediction.\nInstead of simply\npresenting a single prediction ˆy, we can also present the most likely\noutcomes in a prediction set instead, similar to a confidence interval.\nLet α ∈[0, 1] be a hyperparameter controlling the desired width of\na prediction set by defining a cutoff for probabilities. We can then\ndefine the prediction set C for a new point x′ as\nC(x′) =\nn\nyπ−1(1), . . . , yπ−1(k′)\no\n(2.41)\nk′ = sup\nn\nk\n\f\f\f\nk′\nX\nj=1\nˆpπ−1(j) < 1 −α\no\n+ 1.\n(2.42)\nThe above formulation includes a sorting function π(·) that\nsorts indices k by their corresponding class probabilities ˆpk, in a\ndescending order, encompasses the most classes while staying under\nthe probability threshold 1 −α, and adds one to avoid empty sets.\nTherefore, a more intuitive construction is the following: We sort\nall predicted probabilities from highest to lowest, and the select the\nclasses for the prediction set until their sum exceeds a threshold of\n1 −α. Ideally, we would like prediction sets to fulfil two criteria:\nThey should contain the correct answer (coverage) and they should\nbe as tight as possible.17 1−α corresponds to the desired probability\nwith which the correct answer should be contained in the prediction\nset in expectation, similarly how frequentist confidence scores\ncorrespond to a probability of correctness under many repetitions\nof an experiment. In this way, we can also interpret the width of the\nset as a proxy for confidence; The wider the set, the more uncertain\n17Since one can always contain the correct answer by having the widest possible\nprediction sets, evaluating coverage alone is usually not meaningful.\n2.2 uncertainty in deep learning\n39\nthe underlying model and the more classes it has to include in\norder to fulfill a coverage probability of 1 −α. Unfortunately, and\nvery similar to confidence scores, prediction sets are usually not\ncalibrated by default (Kompa et al., 2021). The analogous solution\nto the calibration of prediction sets is conformal prediction (Vovk\net al., 2005; Papadopoulos et al., 2002; Angelopoulos and Bates,\n2021): By using a calibration set of data points and following the\nalgorithm shown in Algorithm 1,18 we can determine a probability\nthreshold ˆq in the following way: First, we collect a number of non-\nconformity scores si on a held-out calibration set that reflect the\ncorrectness of the model. The design of these scores is arbitrary,\nbut should reflect the correctness of a model’s prediction for a\npoint, e.g. s(xi) = 1 −pθ(yi | xi). Afterwards we choose ˆq as the\n⌈(N + 1)(1 −α)/N⌉-th quantile of the empirical score distribution.\nUsing ˆq, our prediction sets now provably contain the correct\nprediction in expectation with a probability of 1 −α. One simple\nway is to include all classes with a probability higher than ˆq:\nC(x′) =\n\b\nyk\n\f\f ˆpk ≥ˆq\n\t\n,\n(2.43)\notherwise we can also repeat the construction in Equation (2.41),\nbut replace the 1 −α threshold by ˆq. Prediction sets in this way\nthen fulfil the following guarantee:\np\n\u0000y′ ∈C(x′)\n\u0001\n≥1 −α.\n(2.44)\nAlgorithm 1 Conformal Prediction\nRequire: Calibration data set {(xi, yi)}N\ni=1, predictor pθ, non-\nconformity function s : RD →R .\n▷1. Retrieve non-conformity scores for calibration points, e.g.\nsi = s(xi) = 1 −pθ(yi | xi)\n▷2. Find quantile ˆq using empirical inverse CDF F −1\nS\nˆq ←F −1\nS\n\u0000⌈(N + 1)(1 −α)/N⌉\n\u0001\n▷3. Create prediction set, e.g.\nC(x′) ←{yk | ˆpk ≥ˆq}\n18This algorithm displays split conformal prediction, which can be applied to\nalready trained predictors. Full conformal prediction however requires the\nre-training of the predictor on all the leave-one-out subsets of the training\nset, and is therefore infeasible for many modern settings. See for instance\nAngelopoulos and Bates (2021), section 6.\n2.2 uncertainty in deep learning\n40\nConformal prediction has enjoyed great interest in recent years,\nsince it is agnostic to the form of the underlying predictor and\ncan therefore easily be applied to neural networks. Recent work\nhas for instance be dedicated to apply conformal prediction for\ntime series (Xu and Xie, 2021; Stankeviciute et al., 2021; Lin et al.,\n2022b; Zaffran et al., 2022) and other non-i.i.d. settings (Gibbs and\nCandès, 2021; Oliveira et al., 2022; Bhatnagar et al., 2023; Barber\net al., 2023; Farinhas et al., 2024). It should also be noted that the\nconformal guarantee in Equation (5.2) can be rewritten in terms\nof the indicator function:\np\n\u0010\n1\n\u0000y′ ∈C(x′)\n\u0001\u0011\n≥1 −α.\n(2.45)\nThis fact is exploited by Angelopoulos et al. (2023) and sub-\nsequent works (Fisch et al., 2022; Farinhas et al., 2024; Xu et al.,\n2023b) to generalize this guarantee to families of functions that\ngo beyond coverage, for instance controlling for false-negative rate\n(Angelopoulos et al., 2023; Fisch et al., 2022; Farinhas et al., 2024;\nXu et al., 2023b).\nUncertainty Quantification in Frequentist Networks.\nIn\nthe case of prediction sets, their width can be interpreted as a\nconfidence score: When the probability distribution is more uni-\nform, more classes have to be added to the set to reach a specific\nprobability threshold, and thus the set size grows. Without pre-\ndiction sets, we turn to the (calibrated) confidence score, which\nis usually taken to be the maximum probability among all classes\n(Hendrycks and Gimpel, 2017). Alternatively, a popular measure of\nuncertainty is to compute the Shannon entropy of the distribution,\nwhich is given by\nH\n\u0002\nPθ(y | x)\n\u0003\n= −\nK\nX\nk=1\nPθ(y = k | x) log Pθ(y = k | x).\n(2.46)\nThe entropy is maximal when the distribution is uniform, and\nconversely its value is minimal when all the probability mass rests\non a single outcome.\n2.2.2\nBayesian Neural Networks\nAfter reviewing the frequentist approach to neural networks in the\nprevious section, the question naturally arises whether we can also\napply Bayesian thinking in a neural network setting. This question\ncan be answered affirmatively and has been studied since the 1990s\n2.2 uncertainty in deep learning\n41\n(see e.g. Tishby and Solla, 1989; MacKay, 1992a; Neal, 1995).19\nWe start by Bayesian parameter estimation for a neural network\nparameterized by weights θ. By placing a prior p(θ) over the\nweights, we obtain a posterior using Bayes’ rule in Equation (2.16):\np(θ | D) ∝p(D | θ)p(θ).\n(2.47)\nWe can again find the maximum a posteriori estimate like in\nSection 2.1.2, but due to the nature of neural networks, have to\nresort to an iterative optimization procedure to find the parameters\nlike for the neural maximum likelihood estimate in the previous\nSection 2.2.1. Luckily, we can optimize for p(θ | D) by simply\nusing a loss function such as in the previous section, and either\nexplicitly or implicitly define a prior p(θ). Explicitly, this can be\nperformed by for instance sampling the initial values of θ from some\nprior distribution, or implicitly through regularization.20 While\nthat makes it comparatively easy to find the parameters θ that\nmaximize Equation (2.47), it is much harder to find the analytical\nform of the posterior p(θ | D) or to sample from it. This is because\nthe full form of Equation (2.47) derived via Bayes’ rule includes\nthe evidence term p(D) as normalizing constant, which as shown\nin Equation (2.17), involves the marginalization over θ.\nThis\nsame infeasible marginalization also appears in the corresponding\npredictive distribution:\np(x′ | D) =\nZ\nΘ\np(x′ | θ)p(θ | D)dθ .\n(2.48)\nWhy is this marginalization prohibitive? Compared to the\nconjugacy that allowed for the elegant solutions in Equations (2.15),\n(2.28) and (2.34), neural networks typically involve non-linear\ncomponents in the form of activation functions, which enable their\nflexibility and modeling power as their depth increases (Hornik\net al., 1989; Barron, 1994; Lu et al., 2017).21 Formulating the\nlikelihood p(D | θ), this non-linear dependence of parameters\nmakes it impossible to marginalize the parameters out. Numerical\nintegration is also usually not feasible, since network parameters\n19Due to the volume of the corresponding literature, we will restrict ourselves\nto some core ideas and important works, a brief history of the field can for\ninstance be found in Gal (2016), pp. 20–23.\n20Regularizing the l2-norm of the network parameters for instance corresponds\nto the use of a isotropic normal prior (see e.g. Bishop and Nasrabadi, 2006;\nSection 3.3.1).\n21As an illustrative counter-example, consider a simple two-layer network without\nnon-linear activation functions in the form of\nf(x) =\nh\nw5\nw6\ni \u0012 \"\nw1\nw2\nw3\nw4\n# \"\nx1\nx2\n# \u0013\n,\n2.2 uncertainty in deep learning\n42\nare real-valued and typically high-dimensional.\nHowever, that\ndoes not mean that Bayesian inference with neural networks is\nimpossible, it rather means that we have to employ a number of\ndifferent strategies. A common red thread between them is that\nevaluating Equation (2.48) does not require us to have access to\nthe distribution itself, only (high-quality) samples. As such, we\ncan approximate the integral using Monte Carlo sampling:\np(x′ | D) =\nZ\nΘ\np(x′ | θ)p(θ | D)dθ ≈1\nM\nM\nX\nm=1\np(x′ | θ(m)),\n(2.49)\nwhere we assume access to a set {θ(m)}M\nm=1 of M sampled param-\neter vectors. This Monte Carlo integration approximates Equa-\ntion (2.48) with an error 1/\n√\nM, that decreases as a function of\nthe number of samples. It should be noted however that this ap-\nproximation will be only asymptotically correct for samples from\nthe true (and not an approximate) posterior, which we can obtain\nusing the now following methods.\nMarkov Chain Monte Carlo & Stochastic Gradient\nLangevin Dynamics.\nIn order to obtain representative samples\nfrom the posterior, we do not necessarily need the analytical form of\nthe posterior. This idea is used by techniques such as Markov chain\nMonte Carlo (MCMC) and stochastic gradient Langevin dynamics\n(SGLD). In the case of MCMC, the core insight is that as long\nas we can evaluate p(θ | D) up to the pesky evidence term p(D),\nwe can, in relative terms, determine whether one sample is more\nlikely under the posterior than another. That means that upon\nformulating a suitable update rule, we can construct a chain of sam-\nples that leads from unlikely samples from p(θ | D) to more likely\nones. A thorough introduction to and overview over this family of\nmethods is out of scope for this section, which is why we instead\nrefer to (Robert et al., 1999) and the corresponding chapters in\nBishop and Nasrabadi (2006); Gelman et al. (2021). The technique\nhas found numerous applications for neural networks, e.g. Andrieu\net al. (2003); Neal (1995); Cobb and Jalaian (2021); Li and Zhang\n(2023). Stochastic Gradient Langevin dynamics (SGLD) follows\na similar intuition (Welling and Teh, 2011), however instead of\nformulating probabilistic transition rules, the constructed chain of\nsamples follows the gradient of the prior and log-likelihood to seek\nposterior modes, similar to gradient descent. Trying to combine\nwhich we can rewrite as f(x) = aT x with a1 = w1w5 + w3w6 and a2 =\nw2w5 + w4w6. Therefore, despite using two linear layers, we effectively obtain\na single linear layer in practice, thus providing motivation for non-linear\nactivation functions.\n2.2 uncertainty in deep learning\n43\nthe advantages of both methods has even birthed SGLD / MCMC\nhybrids (Ma et al., 2015; Chen et al., 2016; Liu et al., 2016; Zhang\net al., 2020b). In all cases, sampling methods remain challenging\ndue to the high dimensional parameter space of neural networks\nand the often multi-modal nature of the weight posterior p(θ | D).\nVariational Inference.\nInstead of trying to sample from the\nposterior p(θ | D), we can instead sample from an easier proposal\ndistribution q(θ | ϕ) with parameters ϕ. For this proposal distri-\nbution to reasonably represent the original weight posterior, we try\nto minimize the difference between the two (Hinton and Van Camp,\n1993; Graves, 2011). At first glance, this seems paradoxical—how\ncan we minimize the distance from the posterior if we do not know\nits form? However, using the Kullback-Leibler (KL) divergence,\nwe can rewrite this difference as follows:\nmin\nϕ KL\n\u0002\nq(θ | ϕ)\n\f\f\f\f p(θ | D)\n\u0003\n(2.50)\n= min\nϕ −\nZ\nΘ\nq(θ | ϕ) log q(θ | ϕ)\np(θ | D)dθ\n(2.51)\n= min\nϕ\nZ\nΘ\nq(θ | ϕ) log\nq(θ | ϕ)\np(D | θ)p(θ)dθ\n(2.52)\n= min\nϕ KL\n\u0002\nq(θ | ϕ)\n\f\f\f\f p(θ)\n\u0003\n−Eq(θ|ϕ)\n\u0002\np(D | θ)\n\u0003\n.\n(2.53)\nTo derive this expression, we exploited the fact that in Equa-\ntion (2.52), the expectation of the evidence p(D) under q(θ | ϕ)\nis a constant that does not influence the result of the optimiza-\ntion problem. Since this term is missing from the expression in\nEquation (2.53), we refer to the result as the evidence lower bound\nor ELBO. We can now evaluate the KL divergence in closed form\nwhen the proposal distribution and prior are chosen in a convenient\nform (e.g. Gaussian distributions), and the respective integrals can\nagain be approximated via Monte Carlo approximation, and the\nparameters ϕ be optimized via gradient descent. The only missing\ncomponent is that sampling θ from q(θ | ϕ) must be differentiable,\nwhich is achieved via the reparameterization trick (Opper and Ar-\nchambeau, 2009; Kingma and Welling, 2014; Rezende et al., 2014).\nTo show this, let ϕ = {µ, ρ} be the parameters of a Gaussian\nproposal distribution. Then we can obtain differentiable samples\nby\nε ∼N(0, I);\nθ = µ + ρ ◦ε .\n(2.54)\nAfter training, networks parameters can be sampled from q(θ |\nϕ) directly to facilitate Bayesian neural networks and evaluate the\n2.2 uncertainty in deep learning\n44\npredictive distribution in Equation (2.49). Examples for variational\nmethods for Bayesian neural networks are given by Blundell et al.\n(2015); Hernández-Lobato and Adams (2015); Louizos and Welling\n(2016); Krueger et al. (2017); Pawlowski et al. (2017); Zhang et al.\n(2018a).\nStochastic Regularizers.\nAnother line of research has been\nconcerned with the interpretation of neural network regularizers\nas sources for stochastic network parameter samples. For instance,\ndropout (Srivastava et al., 2014) regularizes neural network weights\nby setting a random subset of them to zero. This is implemented by\nsampling a mask from a Bernoulli distribution with dropout prob-\nability pdropout and multiplying it with the corresponding weight\nmatrix W ∈RM×N:22\nWdropout = W ◦M;\n{Mij}M,N\ni,j=1 ∼Bernoulli(pdropout).\n(2.55)\nAs Gal and Ghahramani (2016b) argues, we can actually in-\nterpret a set of parameters θdropout with dropout masks applied\nto it as a sample from a variational posterior; therefore, by using\ndropout at inference time (as opposed to just training time in its\noriginal form), we obtain a set of samples {θ(m)\ndropout}M\nm=1 that can be\ninserted back into the MC estimate of the predictive distribution\nin Equation (2.49). This technique is referred to as Monte Carlo\ndropout (or MC dropout) and has found a number of extensions\nover the years (Gal and Ghahramani, 2016a; Li and Gal, 2017; Gal\net al., 2017a; Nalisnick et al., 2019a; Boluki et al., 2020; Durasov\net al., 2021). A similar reasoning can be applied to batch normal-\nization (Ioffe and Szegedy, 2015): Batch normalization works by\nnormalizing the input z(l) to a layer via an estimate of its mean\nand variance\nz(l)\nBN =\nz(l) −E[z(l)]\np\nVar[z(l)] + ε\n,\n(2.56)\nwhere ε is a small value added to avoid numerical issues, and\nthe mean and variance statistics are estimated empirically during\ntraining. Similar to MC dropout, Teye et al. (2018); Mukhoti et al.\n(2020a) re-interpret this as a source of stochasticity: By sampling\na single batch from the training set at inference time, we can use it\nto set our batch statistics for expectation and variance. By using\n22While the intuition of dropout lies in severing neural connections randomly, in\npractice it is often realized as an additional layer that is applied by zeroing\nout parts of activations. For instance, the parallel work of Blum et al. (2015)\nexplores a variational objective using dropout that is applied directly to the\nactivations.\n2.2 uncertainty in deep learning\n45\nthis batch mean and variance for our current inference, Teye et al.,\nwe sample different hidden representations, than we interpreted\nas a result of the randomness in the underlying weights. In both\ncases, the advantages are obvious: These regularization components\nare already part of many deep learning architectures, and the\nonly overhead added is by running multiple forward passes per\ntest input, which—for smaller models—might only add negligible\noverhead. The more subtle downside lies in the fact variational\ninference techniques, as these techniques are counted as, tend to\nonly explore limited regions of the posterior distribution (Wilson\nand Izmailov, 2020). As such, obtained samples might simply not\nbe very representative of p(θ | D) and lead to subpar predictions\nand uncertainty estimates.\nLaplace Approximations.\nThe idea of Laplace approximations\ncan indeed by traced back to the eponymous Pierre-Simon Laplace\n(Laplace, 1774) and has been applied to deep learning first by\nMacKay (1992b). In order to approximate a complex distribution\np(θ | D), we first obtain a MAP estimate of the network parameters\nθMAP as described in the beginning of this section. We then consider\na second-order Taylor expansion for the loss function L(D, θ) at\nθMAP:\nL(D, θ) ≈\nL(D, θMAP) + 1\n2(θ −θMAP)T \u0000∇2\nθL(D, θ)\n\f\f\nθMAP\n\u0001\n(θ −θMAP).\n(2.57)\nBy assuming that L(D, θMAP) is negligible for a fully trained\nnetwork, we can identify\np(θ | D) ≈N\n\u0010\nθ\n\f\f\f θMAP, −∇2\nθL\n\u0000D, θ)\n\f\f\nθMAP\n\u0001−1\u0011\n.\n(2.58)\nUnfortunately, the computation of the covariance matrix quickly\nbecomes infeasible for larger models due to the quadratic nature of\nthe Hessian. Therefore, different compromises have been proposed\n(Daxberger et al., 2021a), including last-layer approximations (Kris-\ntiadi et al., 2020; Snoek et al., 2015), approximation on subsets of\nweights (Daxberger et al., 2021b), factorizing the Hessian (Ritter\net al., 2018a,b; Kristiadi et al., 2020; Yu et al., 2024; Bergamin\net al., 2024), or variational approximations (Ortega et al., 2023).\nAt inference time, network parameters can be drawn from the\napproximate posterior as with previous methods.\n2.2 uncertainty in deep learning\n46\nEnsembling.\nA long-existing method to boost predictive\nperformance has been to train multiple predictors on a problem\nand to ensemble their outputs (Bauer and Kohavi, 1999; Dietterich,\n2000).\nCombining predictions has already been studied since\nthe late 1960s, e.g. in Bates and Granger (1969); Clemen (1989),\nwith some works on neural network ensembles already in the\n1990s (Hansen and Salamon, 1990; Levin et al., 1990; Liu and\nYao, 1999; Zhou et al., 2002). After the deep learning revival,\nLakshminarayanan et al. (2017) discovered that deep ensembles do\nnot only improve generalization, but also tend to be well-calibrated\nand produce high-quality estimates of predictive uncertainty.\nWhile Lakshminarayanan et al. (2017) frame deep ensembles\nexplicitly as non-Bayesian, Fort et al. (2019); Wilson and Izmailov\n(2020) later argued that ensembling actually is a form of Bayesian\nmodel averaging. Since the members of an ensemble are usually\ntrained independently, they are better at converging to different\nsolutions in the parameter space. Therefore, ensembles are argued\nto better represent the often multi-modal weight posterior than\nsome of the methods discussed earlier like MC dropout or Laplace\napproximations, which rely on local approximations (Fort et al.,\n2019).\nNaturally, the disadvantage of ensembling lies in having to train\nmultiple predictors, which can be costly for modern, large neural\nneural networks. A flurry of research works has investigated alter-\nnatives to this costly procedure, such as having ensemble members\nshare weights (Antorán et al., 2020; Liu et al., 2022b; Durasov\net al., 2021; Laurent et al., 2023) or ensembling checkpoints of\na model collected over the training (Izmailov et al., 2018; Mad-\ndox et al., 2019; Izmailov et al., 2019; Wilson and Izmailov, 2020;\nYashima et al., 2022). As our understanding of neural loss land-\nscapes improves, works such as Garipov et al. (2018); Cha et al.\n(2021) have suggested to create ensembles along low-loss basins.\nOther ways to curb computational inference costs involve efficient\nweight factorization techniques (Wenzel et al., 2020; Wen et al.,\n2020b; Dusenberry et al., 2020) or distilling properties of an entire\nensemble into a single predictor (Malinin et al., 2020; Kim et al.,\n2024a). Even when ensemble members are trained independently,\nthey can converge to similar solutions, offsetting their advantage.\nSeveral methods to improve the diversity in ensembles have been\nproposed (Jain et al., 2020; D’Angelo and Fortuin, 2021; El-Laham\net al., 2023), including the ensembling of different architectures\n(Zaidi et al., 2021). Notable are also other explicitly Bayesian ways\nof ensembling (Pearce et al., 2020; Deng et al., 2022) or connections\nto mixture-of-experts models (Allingham et al., 2022).\n2.2 uncertainty in deep learning\n47\nDeep Kernel Learning.\nGaussian processes (GP; Kolmogoroff,\n1941; Wiener, 1949; Williams and Rasmussen, 2006) are (typically)\nnon-parametric models that predict targets and corresponding\nuncertainties based on the similarity between training and test\npoints. These similarities are computed through covariance or\nkernel functions. In theory, this creates appealing properties for\nuncertainty quantification, as unusual inputs should be labeled as\nuncertain because of their dissimilarity with the observed data.\nNevertheless, scaling Gaussian processes to large amounts of data\nin known to be challenging (see e.g. discussions in Williams and\nRasmussen, 2006 or in Bishop and Nasrabadi, 2006, Chapter 6).\nTherefore, deep kernel learning (Wilson et al., 2016) fits a Gaussian\nprocess layer on top of a deep neural feature extractor. This has\ncreated a number of follow-up works using deep kernel learning\nfor UQ (Bradshaw et al., 2017; Daskalakis et al., 2020; Liu et al.,\n2021; van Amersfoort et al., 2021), however several authors have\nnoted shortcomings with the approach due to the challenging\njoint optimization of the GP and neural feature extractor (Ober\net al., 2021; van Amersfoort et al., 2021; Schwöbel et al., 2022):\nThis includes overfitting and feature collapse, where OOD data\npoints are mapped to similar regions of the latent space as training\npoints. On top of deep kernel learning, there are several connections\nbetween neural networks and GPs are given through deep Gaussian\nprocesses (Damianou and Lawrence, 2013; Dunlop et al., 2018;\nJakkala, 2021) and the theoretical links between neural networks\nand GPs (Neal, 1995; Williams, 1998; Hensman and Lawrence,\n2014; Dutordoir et al., 2021).\nUncertainty Quantification in Bayesian Networks.\nSo\nfar, we have discussed multiple different methods how to obtain\nsamples from the (approximate) weight posterior, but without\nmentioning how this aids in obtaining new, useful and disentangled\nuncertainties. One way to assess epistemic uncertainty in this\nframework is to measure disagreement between predictions for the\nsame input. Since models tend to be underspecified on OOD inputs,\nthis is where the predictions from different models will disagree\nthe most in case of high model uncertainty. In classification, this\ncan be done for instance using the variation ratio (Freeman, 1965;\nGal, 2016): Assuming a set of B samples from the weight posterior,\nlet ˆy(b) the predicted label using each set of weights and let y∗\ndenote the most commonly predicted label among these. Then,\nthe variation-ratio is defined as\nVR = 1 −1\nB 1\n\u0000ˆy(b) = y∗\u0001\n.\n(2.59)\n2.2 uncertainty in deep learning\n48\nAnother way to measure the disagreement between predictions\nis to simply quantify the average variance of predictions per class:\n¯σ2 = 1\nK\nK\nX\nK=1\nEp(θ|D)\n\u0002\nPθ(y = k | x)2\u0003\n−Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u00032.\n(2.60)\nA more theoretically motivated approach to isolate epistemic\nuncertainty is to consider the mutual information between model\nparameters and a data sample (Depeweg et al., 2018; Smith and\nGal, 2018):\nI\n\u0002\ny, θ\n\f\f D, x\n\u0003\n|\n{z\n}\nModel uncertainty\n= H\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\n|\n{z\n}\nTotal uncertainty\n−Ep(θ|D)\nh\nH\n\u0002\nPθ(y | x)\n\u0003i\n|\n{z\n}\nData uncertainty\n.\n(2.61)\nThe term itself can be interpreted as the gain in information\nabout the ideal model parameters and correct label upon receiving\nan input. If we can only gain a little, that implies that parameters\nare already well-specified and that the epistemic uncertainty is low.\nIn both cases of Equations (4.3) and (4.4) the expectation can be\napproximated through Monte Carlo approximation, i.e.\nEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\n≈1\nB\nB\nX\nb=1\nP(y = k | x, θ(b)).\n(2.62)\n2.2.3\nEvidential Neural Networks\nThe following work is based on Ulmer et al. (2023).\nIn the last section, we explored many different approaches to\nquantify different kinds of uncertainty by obtaining samples from\nthe weight posterior p(θ | D). However, we saw that this can\nbe a challenging endeavor, since samples might be expensive to\nobtain or not very representative of the actual posterior distribution.\nAlternatively, we can factorize Equation (2.48) further and use a\npoint estimate for the weights to obtain a tractable form:\n2.2 uncertainty in deep learning\n49\nP(y | x, D) =\nZ\np(x′ | θ)p(θ | D)dθ\n(2.63)\n=\nZZ\nP(y | π)\n|\n{z\n}\nAleatoric\np(π | x, θ)\n|\n{z\n}\nDistributional\np(θ | D)\n|\n{z\n}\nEpistemic\ndπdθ\n(2.64)\n≈\nZ\nP(y | π) p(π | x, ˆθ)\n|\n{z\n}\np(θ|D)≈δ(θ−ˆθ)\ndπ .\n(2.65)\nIn the last step, Malinin and Gales replace p(θ | D) by a point\nestimate ˆθ using the Dirac delta function, i.e. a single trained\nneural network, to get rid of the intractable integral.23 This fac-\ntorization contains another type of uncertainty, which Malinin\nand Gales (2018) call the distributional uncertainty; uncertainty\ncaused by the mismatch of training and test data distributions.\nAlthough another integral remains, retrieving the uncertainty from\nthis predictive distribution actually has a closed-form analytical\nsolution, as we will see later. The advantage of this approach is\nfurther that it allows us to distinguish uncertainty about a data\npoint because it is ambiguous, from uncertainty caused by a point\ncoming from an entirely different data distribution. This approach\nto UQ it called evidential deep learning (EDL), and originates\nfrom the work of Sensoy et al. (2018). They originally base their\nmotivation on the theory of evidence (Dempster, 1968; Audun,\n2018): Within the theory, belief mass is assigned to set of possible\nstates, e.g. class labels, and can also express a lack of evidence, i.e.\nan “I don’t know”. We can apply this idea to the predicted output\nof a neural classifier using the Dirichlet distribution, allowing us to\nexpress a lack of evidence through a uniform Dirichlet. In this way,\nthe neural network does not parameterize a single (categorical)\ndistribution, but a distribution over distributions, also referred to\nas a second-order distribution. This is different from a uniform\n(first-order) categorical distribution, which does not distinguish an\nequal probability for all classes from a lack of evidence, or differ-\nently phrased: One cannot distinguish whether the distribution is\nuniform due to uncertainty, or confidently uniform due to ambi-\nguity. In the following, we define EDL as a family of approaches\nin which a neural network can fall back onto a uniform prior for\nunknown inputs. While neural networks usually parameterize like-\n23In the context of Equation (2.63), it should be noted that restricting oneself to\na point estimate of the network parameters prevents the epistemic uncertainty\nestimation through the weight posterior p(θ | D), as discussed in the previous\nsection. However, there are works like Haussmann et al. (2019); Zhao et al.\n(2020) that combine both approaches.\n2.2 uncertainty in deep learning\n50\n1\n2\n3\n(a) Iris setosa\n(b) Iris versicolor\n(c) Iris virginica\nFigure 2.8: Illustration of different approaches to uncertainty quantifica-\ntion on the Iris dataset, with examples for the classes given on the left\n(Figures 2.8a to 2.8c). On the right, the data is plotted alongside some\npredictions of a prior network (lighter colors indicate higher density) and\nan ensemble and MC dropout model on the probability simplex, with\n50 predictions each. Iris images were taken from Wikimedia Commons,\n2022a,b,c.\nlihood functions, approaches in this survey parameterize prior or\nposterior distributions instead, as we will show next.\nAn Illustrating Example: The Iris Dataset.\nWe train a deep\nneural network ensemble (Lakshminarayanan et al., 2017) with 50\nmodel instances, a model with MC Dropout (Gal and Ghahramani,\n2016b) with 50 predictions and a prior network (Sensoy et al.,\n2018), an example of EDL, on all available data points, and plot\ntheir predictions on three test points on the probability simplex in\nFigure 2.8.24 On these simplices, each point signifies a categorical\ndistribution, with the proximity to one of the corners indicating\n24For information about training and model details, see Appendix C.4.1.\n2.2 uncertainty in deep learning\n51\na higher probability for the corresponding class. EDL methods\nfor classification do not predict a single output distribution, but\nan entire density over output distributions. Test point 3 lies in\na region of overlap between instances of Iris versicolor and Iris\nvirginica, thus inducing high aleatoric uncertainty. In this case, we\ncan see that the prior network places all of its density around the\nvertex between these two classes, similar to most of the predictions\nof the ensemble and MC dropout (bottom right). However, some\nof the latter predictions still land in the center of the simplex. The\npoint 1 is located in an area without training examples between\ninstances of Iris versicolor and setosa, as well as close to a single\nvirginica outlier. As shown in the top left, ensemble and MC\ndropout predictions agree that the point belongs to either the\nsetosa or versicolor class, with a slight preference for the former.\nThe prior network concentrates its prediction on versicolor, but\nadmits some uncertainty towards the two other choices. The last\ntest point 2 is placed in an area of the feature space devoid of any\ndata, roughly equidistant from the three clusters of flowers. Similar\nto the previous example, the ensemble and MC dropout predictions\non the top right show a preference for Iris setosa and versicolor,\nalbeit with higher uncertainty. The prior network however shows\nan almost uniform density, admitting distributional uncertainty\nabout this particular input. This simple example provides some\ninsights into the potential advantages of EDL: First of all, the prior\nnetwork was able to provide reasonable uncertainty estimates in\ncomparison with Bayesian model averaging methods. Secondly,\nthe prior network is able to admit its lack of knowledge for the\nOOD data point by predicting an almost uniform prior, something\nthat the other models are not able to. Lastly, training the prior\nnetwork only required a single model, which is a noticeable speed-up\ncompared to MC dropout and especially the training of ensembles.\nParameterization.\nWe start from a categorical distribution\nover classes, defined as:\nCategorical(y | π) =\nK\nY\nk=1\nπ\n1\n\u0000y=k\n\u0001\nk\n,\n(2.66)\nin which K denotes the number of categories or classes, and the\nclass probabilities are expressed using a vector π ∈[0, 1]K with\nP\nk πk = 1, and 1\n\u0000·\n\u0001\nis the indicator function. In this setting, the\nDirichlet distribution arises as a suitable prior and multivariate\ngeneralization of the Beta distribution (and is thus also called the\nmultivariate Beta distribution):\n2.2 uncertainty in deep learning\n52\nDir(π; α) =\n1\nB(α)\nK\nY\nk=1\nπαk−1\nk\n;\nB(α) =\nQK\nk=1 Γ(αk)\nΓ(α0)\n;\nα0 =\nK\nX\nk=1\nαk,\n(2.67)\nwhere αk ∈R+ and the Beta function B(·) is defined for K shape\nparameters compared to Equation (2.18).\nThe distribution is\ncharacterized by its concentration parameters α, the sum of which,\noften denoted as α0, is called the precision.25 The Dirichlet is\na conjugate prior for such a categorical likelihood, meaning that\naccording to Bayes’ rule, it produces a Dirichlet posterior with\nparameters β, given a data set D = {(xi, yi)}N\ni=1 of N observations\nwith corresponding labels:\np(π | D, α) ∝p\n\u0000{yi}N\ni=1 | π, {xi}N\ni=1\n\u0001\np(π | α)\n=\nN\nY\ni=1\nK\nY\nk=1\nπ\n1\n\u0000yi=k\n\u0001\nk\n1\nB(α)\nK\nY\nk=1\nπαk−1\nk\n(2.68)\n=\nK\nY\nk=1\nπ\n\u0000 PN\ni=1 1\n\u0000yi=k\n\u0001\u0001\nk\n1\nB(α)\nK\nY\nk=1\nπαk−1\nk\n(2.69)\n=\n1\nB(α)\nK\nY\nk=1\nπNk+αk−1\nk\n∝Dir(π; β),\n(2.70)\nwhere β is a vector with βk = αk + Nk, with Nk denoting the\nnumber of observations for class k. Intuitively, this implies that the\nprior belief encoded by the initial Dirichlet is updated using the\nactual data, sharpening the distribution for classes for which many\ninstances have been observed. The Dirichlet is a distribution over\ncategorical distributions on the K −1 probability simplex—while\na neural classifier is usually realized as a function fθ : RD →RK,\nmapping an input x ∈RD to logits for each class. Followed by a\nsoftmax function, this then defines a categorical distribution over\nclasses with a vector π with πk ≡Pθ(y = k | x). The same under-\nlying architecture can be used without any major modification to\ninstead parameterize a Dirichlet, predicting a distribution over cat-\negorical distributions p(π | x, ˆθ) as in Equation (2.67).26 In order\nto classify a data point x, a categorical distribution is created from\n25The precision is analogous to the precision of a Gaussian, where a larger α0\nsignifies a sharper distribution.\n26The only thing to note here is that the every αk has to be strictly positive,\nwhich can for instance be enforced by using an additional softplus, exponential\nor ReLU function (Sensoy et al., 2018; Malinin and Gales, 2018; Sensoy et al.,\n2020).\n2.2 uncertainty in deep learning\n53\n(a) Confident pre-\ndiction.\n(b) Aleatoric un-\ncertainty.\n(c) Epistemic un-\ncertainty.\n(d) Distributional\nuncertainty.\nFigure 2.9: Examples of the probability simplex for a K = 3 classification\nproblem, where every corner corresponds to a class and every point to a\ncategorical distribution, and brighter colors correspond to higher density.\nShown is the (desired) Behavior of Dirichlet in different scenarios by\nMalinin and Gales (2018): (a) For a confident prediction, the density is\nconcentrated in the corner of the simplex corresponding to the assumed\nclass. (b) In the case of aleatoric uncertainty, the density is concentrated\nin the center, and thus uniform categorical distributions are most likely.\n(c) In the case of model uncertainty, the density may still be concentrated\nin a corner, but more spread out, expressing the uncertainty about the\nright prediction. (d) In the case of an OOD input, a uniform Dirichlet\nexpresses that any categorical distribution is equally likely, since there\nis no evidence for any known class.\nthe predicted concentration parameters of the Dirichlet as follows\n(this corresponds to the mean of the Dirichlet, see Appendix A.2):\nα = exp\n\u0000fθ(x)\n\u0001\n;\nπk = αk\nα0\n;\nˆy = argmax\nk∈[K]\nπ1, . . . , πK.\n(2.71)\nUncertainty Quantification in EDL.\nLet us now turn our\nattention to how to estimate the aleatoric, epistemic and distribu-\ntional uncertainty within the Dirichlet framework. In Figure 2.9,\nwe show different (ideal) shapes of a Dirichlet distribution param-\neterized by a neural network, corresponding to different cases of\nuncertainty, where each point on the simplex represents a cate-\ngorical distribution, with proximity to a corner indicating a high\nprobability for the corresponding class. However, since we do not\nwant to inspect Dirichlets visually, we instead use closed-form ex-\npressions to quantify uncertainty. To obtain a measure of data\nuncertainty, we can evaluate the expected entropy of the data\ndistribution P(y | π). As the entropy captures the “peakiness” of\nthe output distribution, a lower entropy indicates that the model\nis concentrating most probability mass on a single class, while high\nentropy characterizes a more uniform distribution—the model is\nundecided about the right prediction. For Dirichlet networks, this\nquantity has a closed-form solution (for the full derivation, refer to\nAppendix A.4):\n2.2 uncertainty in deep learning\n54\nEp(π|x,ˆθ)\nh\nH\n\u0002\nP(y | π)\n\u0003i\n= −\nK\nX\nk=1\nαk\nα0\n\u0012\nψ(αk+1)−ψ(α0+1)\n\u0013\n, (2.72)\nwhere ψ denotes the digamma function, defined as ψ(x) =\nd\ndx log Γ(x), and H the Shannon entropy.\nAs we saw in Equa-\ntion (2.63), we can avoid the intractable integral over network\nparameters θ by using a point estimate ˆθ.27\nThis means that\ncomputing the model uncertainty via the weight posterior p(θ | D)\nlike in Section 2.2.2 is not possible. Nevertheless, a key property\nof Dirichlet networks is that epistemic uncertainty is expressed\nthrough the spread of the Dirichlet distribution (for instance in\nFigure 2.9 (c) and (d)). Therefore, the epistemic uncertainty can be\nquantified considering the concentration parameters α that shape\nthis distribution: Charpentier et al. (2020) simply consider the\nmaximum αk as a score akin to the maximum probability score by\nHendrycks and Gimpel (2017), while Sensoy et al. (2018) compute\nit by K/ PK\nk=1(αk + 1) or simply α0 (Charpentier et al., 2020). In\nboth cases, the underlying intuition is that larger αk produce a\nsharper density, and thus indicate increased confidence in a predic-\ntion. Lastly, the distributional uncertainty can be quantified by\ncomputing the difference between the total amount of uncertainty\nand the data uncertainty (similar to the reasoning behind Equa-\ntion (4.4)), which can be expressed through the mutual information\nbetween the label y and its categorical distribution π:\nI\n\u0002\ny, π\n\f\f x, D\n\u0003\n= H\nh\nEp(π|x,D)\n\u0002\nP(y | π)\n\u0003i\n|\n{z\n}\nTotal Uncertainty\n−Ep(π|x,D)\nh\nH\n\u0002\nP(y | π)\n\u0003i\n|\n{z\n}\nData Uncertainty\n.\n(2.73)\nThis quantity expresses how much information we would receive\nabout π if we were given the label y, conditioned on the new input\nx and the training data D. In regions in which the model is well-\ndefined, receiving y should not provide much new information\nabout π—and thus the mutual information would be low. Yet,\nsuch knowledge should be very informative in regions in which few\ndata have been observed, and there this mutual information would\nindicate higher distributional uncertainty. Given that E[πk] = αk\nα0\n(Appendix A.2) and assuming the point estimate p(π | x, D) ≈\np(π | x, ˆθ) to be sufficient (Malinin and Gales, 2018), we obtain\nan expression very similar to Equation (2.72):\n27When the distribution over parameters in Equation (2.63) is retained, alternate\nexpressions of the aleatoric and epistemic uncertainty are derived by Woo\n(2022).\n2.2 uncertainty in deep learning\n55\nI\n\u0002\ny, π\n\f\f x, D\n\u0003\n= −\nK\nX\nk=1\nαk\nα0\n\u0010\nlog αk\nα0\n−ψ(αk+1)+ψ(α0+1)\n\u0011\n. (2.74)\nWe mentioned before how Figure 2.9 illustrates idealized be-\nhaviors of the Dirichlet distributions. Therefore, any closed-form\nexpressions of different uncertainties can only be effective when\nthe desired shape of the distribution is attained. Similarly, the\nnaive parameterization in Equation (2.71) is not to guaranteed\nto succeed in this goal, and the literature has proposed different\nmethods to attain this goal. They can broadly be classified into\ntwo families: Prior networks, which parameterize the Dirichlet\nprior distribution and employ custom training procedures and reg-\nularizers, and posterior networks, which instead parameterize a\nDirichlet posterior like in Equation (2.68) instead.28\nPrior Networks.\nPrior networks can be further subcategorized\ninto two sets, namely OOD-free approaches or OOD-dependent\napproaches. In the first case, we regulate the behavior of the\nDirichlet distribution on OOD inputs by adding a regularizer that\npenalizes any density allocated to regions that do not correspond to\nthe gold label. One such option is to decrease the Kullback-Leibler\ndivergence from a uniform Dirichlet (see Appendix A.5):\nKL\n\u0002\np(π | α)\n\f\f\f\f p(π | 1)\n\u0003\n= log Γ(K)\nB(α) +\nK\nX\nk=1\n(αk−1)\n\u0000ψ(αk)−ψ(α0)\n\u0001\n.\n(2.75)\nOther options are the use of Rényi divergences (Tsiligkaridis,\n2019), regularizers derived from PAC-bounds (Haussmann et al.,\n2019), or lp-norms (Sensoy et al., 2018; Tsiligkaridis, 2019). Alter-\nnatively, some works also try to transfer the uncertainty from a set\nof Bayesian predictors into a single prior network (Malinin et al.,\n2020; Fathullah and Gales, 2022) using knowledge distillation (Hin-\nton et al., 2015). When OOD data is available, we also explicitly\ntrain the prior network to maximize its entropy on such exam-\nples (Malinin and Gales, 2018, 2019; Nandy et al., 2020), which\ncan for instance be implemented using the closed-form solution in\nAppendix A.3:\n28We now give a brief overview over these approaches with a focus on classification\nproblems. For a more comprehensive account that also includes regression\nproblems, refer to Ulmer et al. (2023).\n2.2 uncertainty in deep learning\n56\nH\n\u0002\np(π | α)\n\u0003\n= log B(α) + (α0 −K)ψ(α0) −\nK\nX\nk=1\n(αk −1)ψ(αk).\n(2.76)\nUnfortunately though, it should be noted that such data is often\nnot available or in the first place, or cannot guarantee robustness\nagainst other kinds of unseen OOD data, of which infinite types\nexist in a real-valued feature space.29\nPosterior Networks.\nWhen parameterizing Equation (2.68)\ninstead of the Dirichlet prior, the neural networks now predicts\nthe update Nk instead, and the prior parameters α are typically\nset to be uniform. Nevertheless, we still need to gently guide\nthe resulting Dirichlet posterior to attain its desired uncertainty\nbehavior. Similar to prior networks, this can be done with an\nentropy regularizer (Sensoy et al., 2018) or additional training\nobjective on OOD examples, including works that create synthetic\nOOD inputs using additional generative models (Sensoy et al.,\n2020; Hu et al., 2021). More interestingly, Charpentier et al. (2020);\nStadler et al. (2021); Charpentier et al. (2022) use normalizing\nflows (Rezende and Mohamed, 2015) trained on the model’s latent\nrepresentations to compute the update Nk. By modeling the latent\ndensity, this allows us to update the uniform prior by a lot when\nthe latent encoding is familiar, and leave the prior ignorance intact\nwhen it is not, and is therefore assigned a low probability by the\nnormalizing flow.\n2.2.4\nOther Approaches\nA number of other methods for UQ do not neatly fall into the\ncategories we discussed so far. This includes for instance some\nworks that see the layer-wise transformations happening inside a\nneural network as a dynamical system that can be modeled through\nneural stochastic differential equations (SDEs; Kong et al., 2020b;\nWang et al., 2021c; Wang and Yao, 2021; Xu et al., 2022). By\nparameterizing the drift and diffusion terms of a SDE by neural\nnetworks, the diffusion network can be used to predict model\nuncertainty.\nMa et al. (2023) parameterize a layer-wise mean\nand covariance instead, but do not embed these in a SDE. In a\ncompletely different approach, Hu et al. (2022) obtain a sequence\nof probabilities for a specific input from different model snapshots\nduring training, and then quantify the uncertainty in the frequency\n29The same applies to the synthetic OOD data in Chen et al. (2018); Shen et al.\n(2020); Sensoy et al. (2020).\n2.2 uncertainty in deep learning\n57\ndomain after applying a discrete Fourier transform. Papernot and\nMcDaniel (2018); Jiang et al. (2018) compare the output of a\npredictor to that of a simple nearest-neighbor classifier to quantify\nuncertainty, and Anirudh and Thiagarajan (2021) compare latent\nembeddings to a number of anchor points.\nDirect Uncertainty Prediction.\nSo far, we have treated\nuncertainty as something to be extracted from a model that, in\ngeneral, is performing a different task, such as classification or\nregression. But what if we can just treat UQ as a supervised\nlearning task, learning to predict an uncertainty score from an\ninput? For instance, Geifman and El-Yaniv (2019) propose to add\nanother prediction head to a model which predicts when the model\nshould abstain from a potentially false output. The same option\nis instead parameterized as an additional class in a classification\nproblem by Liu et al. (2019). Alternatively, the the confidence\nof a network can also be obtained from an independent network\n(Corbière et al., 2019, 2021; Luo et al., 2021; Fathullah et al.,\n2024; Liu et al., 2024b), which is also what Chapter 6 discusses\nin the context of LLMs. This model can also take the shape of a\nGaussian process, as demonstrated by Qiu and Miikkulainen (2022).\nInstead of setting up this additional model as a classifier, we can\nalso employ a density estimator to derive the uncertainty of a target\nmodel, similar to posterior networks in Section 2.2.3. This again\nfollows the idea that a density estimator would be able to indicate\nwhen a given test point lies outside of the known training distribu-\ntion. As estimation of density can be achieved through Gaussian\ndiscriminant analysis on the latent representations (Mukhoti et al.,\n2021; Franchi et al., 2022), distances between latent features (Huang\net al., 2021), kernel density estimators (Kotelevskii et al., 2022; Sun\net al., 2024) or normalizing flows (Lahlou et al., 2023). Some of\nthese methods are benchmarked by Postels et al. (2022), showing\nsome sensitivity to distributional shifts nevertheless.\nCredal Sets.\nCredal sets are based on the theory of imprecise\nprobabilities (Boole, 1854; Keynes, 1921; Walley, 1991). The theory\nfocuses on the idea that while there might a model that precisely\ndescribes a probability of interest, it may not be known, for instance\ndue to vague, conflicting or scarce data (Caprio et al., 2023). One\noption to model this impreciseness is the use credal sets, which\nare sets of credible probability distributions. Like EDL methods\nin Section 2.2.3, they are defined on the probability simplex, but\nin contrast are not distributions, but convex sets instead. More\nintuitively, we can see a label y as a sample from the conditional\n2.2 uncertainty in deep learning\n58\n(a) Prior network prediction.\n(b) Credal sets from convex hulls.\nFigure 2.10: Juxtaposition of a prior network and credal sets constructed\nfrom the convex hull of ensemble and MC dropout predictors.\ndistribution y ∼P(y | x). Now, let the probability simplex for a\nclassification problem with K classes be defined as\n∆K−1 = {λ = (λ1, . . . , λK)T | λk ≥0, || λ ||1 = 1} ⊂RK,\n(2.77)\nand thus we can see that every P(y | x) ∈∆K−1. A credal set Q\nis now a convex subset of this simplex, i.e. Q ⊆∆K−1. As with\nevidential methods, ignorance about a prediction can be represented\nthrough including the whole simplex, so Q = ∆K−1. Since the\ncombination with neural models is still a nascent field of research,\nlearning credal sets can be challenging. Existing ideas include self-\nsupervised learning (Lienen and Hüllermeier, 2021a; Lienen et al.,\n2023), or creating a convex hull around predictions produced by\nBayesian methods such as ensembles (Mortier et al., 2022). We show\nan example of this for the second test point from the Iris dataset\nexample from Section 2.2.3 in Figure 2.10. It is also possible to\nlearn credal sets from Dirichlet networks when target distributions\n(instead of labels) are available (Javanmardi et al., 2024), using\ninterval neural networks (which produce intervals over predictions\nand activations; Wang et al., 2024b). A more complex approach\ninvolves defining credal sets for priors and likelihood functions, from\nwhich credal sets of posterior distributions can be learned using\nvariational inference (Caprio et al., 2023). In terms of uncertainty\nquantification, Mortier et al. (2022) develop several metrics to\nassess the calibration of credal predictors, and Hüllermeier et al.\n(2022); Sale et al. (2023b) investigate different uncertainty metrics.\nIt should be mentioned that while a notion of volume of the credal\nsets appears as an intuitive metric (analogous to prediction set\nsize), this intuition is flawed for multi-class classification problems\n(Sale et al., 2023b). In this regard, Hüllermeier et al. (2022) offer\n2.3 uncertainty in natural language processing\n59\nalternative metrics based class dominance (whether a certain class\nin more likely than all others for all the distributions in the credal\nset), which also allows to distinguish aleatoric from epistemic\nuncertainty.\n2.3\nUncertainty in Natural Language\nProcessing\nMany of the approaches of uncertainty in the previous sections\nhave also been applied to natural language processing, and we thus\nonly mention some of the relevant works briefly: Calibration for\ninstance has been investigated for classification (Desai and Durrett,\n2020; Dan and Roth, 2021; Xiao et al., 2022; Ulmer et al., 2022b;\nAhuja et al., 2022; Park and Caragea, 2022; Holm et al., 2023;\nChen et al., 2023b; Zhu et al., 2023; Li et al., 2024c; Ye et al., 2024;\nPlaut et al., 2024). It has also been looked into in the context of\ngeneration tasks like language modeling (Zhu et al., 2023), machine\ntranslation (Wang et al., 2020b), and especially question-answering\n(Zhang et al., 2021c; Si et al., 2022, 2023; Lin et al., 2022a; Huang\net al., 2023; Zhang et al.; Geng et al., 2023; Detommaso et al.,\n2024; Ulmer et al., 2024a). Conformal prediction has also been\napplied to NLP in various ways (see e.g. Campos et al. (2024) for\na more comprehensive survey): These applications include natural\nlanguage generation (Schuster et al., 2022; Ravfogel et al., 2023;\nDeutschmann et al., 2024; Ulmer et al., 2024c), prompt selection\n(Zollo et al., 2023), planning problems with LLMs (Ren et al.,\n2023), and behavioral alignment, i.e. the avoidance of toxic or\notherwise undesired behaviors (Gui et al., 2024). Furthermore,\nsome works have also sought out applications of evidential deep\nlearning in NLP (Shen et al., 2020; He et al., 2023a), however with\nno application to language generation at the time of writing of this\nthesis.\nToken- and Sequence-Level Uncertainty.\nDue to the se-\nquentiality of language, uncertainty in NLP can be quantified on\ndifferent scales. On the one hand, we might be interested in quanti-\nfying uncertainty on a (subword-)token level in order to e.g. identify\nmistranslations or factual errors. On the other hand, sequence-level\nuncertainties are of interest when the whole generation might be\nunreliable, or when we are trying to assess its usefulness for a\ndownstream task. Similarly, there are potential applications to\nquantify uncertainty even on a paragraph-, document-, or dialogue-\nlevel. In order to now quantify the uncertainty on these scales, one\nmight intuitively resort to the approaches for frequentist networks\n2.3 uncertainty in natural language processing\n60\nin Section 2.2.1, i.e. take the probability of the most likely token or\nthe likelihood of a generated sequence as confidence. This runs into\nmultiple problems: Due to the paraphrasticity of language (Sec-\ntion 2.1.3), a distribution over tokens might simply be uncertain due\nto the natural variability of language, not due to the uncertainty\nof the model.30 Since we would like confidence scores to reflect\nsome notion of correctness or reliability, using the likelihood of a\ngenerated sequence is also problematic; for one, token probabilities\nlikely do not reflect confidence by themselves, but there is even a\nmismatch between the frequency of generated sequences compared\nto the (true) human distribution (Ott et al., 2018; LeBrun et al.,\n2022; Ji et al., 2023a), implying that sequence likelihoods are not\neven representative as the expected relative frequency of a gen-\nerated sentence. This rules out their use to for instance reliably\nidentify anomalous outputs. More importantly, there is no explicit\ninductive bias in modern architecture or training procedures that\nmodels the variability directly (Baan et al., 2024) or would push\nsequence likelihoods to reflect confidence per se (see for instance\nthe results by Xue et al., 2024a; Becker and Soatto, 2024). While\ncalibrating these likelihoods (Ulmer et al., 2024a; Xie et al.) or\nreweighing token probabilities in a sequence (Lin et al., 2024) can\nlead to some success, ECE results might also be misleading when\ncomparing models with humans in a language context (Ilia and\nAziz, 2024). Therefore, uncertainty on a sequence-level has instead\nbeen investigated by resampling generations (see next paragraph;\nOtt et al., 2018; Aina and Linzen, 2021). On a token-level, several\napproaches have emerged, for instance computing uncertainty given\nspecific claims (Fadeeva et al., 2024), predicting the confidence\nbased on the quantiles of the token distribution (Gupta et al., 2024),\nor training an additional prediction head (Kadavath et al., 2022).\nIn order to compare a wide variety of different such uncertainty\nmetrics, Huang et al. (2024) proposed the use of rank calibration,\ni.e. testing whether higher certainty indeed implies higher genera-\ntion quality. Some works also exists that quantify uncertainty for\nlong texts, for instance based on the entailment probabilities of\nsegments (Zhang et al., 2024a), and Sicilia et al. (2024) model the\nuncertainty inherent in long conversations (but therefore not the\nuncertainty of the model processing the conversation itself).\nSelf-consistency, Prompt Ensembling and Output Diversity.\nWhile there has been some research over the years into Bayesian\nmethods (Xiao et al., 2020; Malinin and Gales, 2021; Gidiotis\nand Tsoumakas, 2022; Xiong et al., 2023), these have become less\n30Neural models also have been show to be ill-calibrated towards the human\nword distribution, see Liu et al., 2024a; Ilia and Aziz, 2024.\n2.3 uncertainty in natural language processing\n61\napplicable in the era of large language models due to their sheer\nsize.31 Therefore, a number of works ensemble predictions for the\nsame input (also referred to as self-consistency; Wang et al., 2023b;\nManakul et al., 2023; Chen and Mueller, 2023; Li et al., 2024b), from\nthe same prompt with different pieces of additional information\n(Hou et al., 2023a), or from different prompts altogether (Li et al.,\n2023b; Hou et al., 2023b; Pitis et al., 2023; Gao et al., 2024b)\ninstead of predictions from different parameter sets. The intuition\nremains similar to Bayesian methods in Section 2.2.2: If similar\nprompts for the same input produce vastly different predictions,\nthe network must be uncertain. We can therefore interpret prompt\nensembling techniques as evaluating a predictive distribution over\ndistribution of prompts p(ρ) and in-context samples p(C):\nEp(ρ,C)\n\u0002\np(y | x, θ, ρ, C)\n\u0003\n=\nZZ\np(y | θ, x, ρ, C)p(ρ)p(C)dρ dC.\n(2.78)\nAny disagreement in responses however can also be influenced\nby the generation hyperparameters, and thus this method does\nnot admit a clean distinction between aleatoric and epistemic\nuncertainty like in Equation (4.4).32\nFurthermore, Ling et al.\n(2024) investigate how the choice of in-context samples can also\ninduce additional uncertainty into the LLMs generation. Kuhn et al.\n(2023) base their idea of semantic entropy on a similar intuition:\nTrough the use of a bi-directional entailment classifier, generations\nare clustered by meaning.33 Instead of the Shannon entropy over\nclasses in Equation (4.2), we evaluate entropy over all the sequences\ns given some M out of M clustered meaning classes:\nSE(x) = −\nM\nX\nm=1\np(Mm | x) log p(Mm | x)\n(2.79)\n= −\nM\nX\nm=1\n\u0010 X\ns∈Mm\np(s | x)\n\u0011\nlog\n\u0010 X\ns∈Mm\np(s | x)\n\u0011\n(2.80)\n≈−1\nM\nM\nX\nm=1\nlog\n\u0010 X\ns∈Mm\np(s | x)\n\u0011\n,\n(2.81)\nwhere the last step is obtained through Monte Carlo integration.\nAichberger et al. (2024) improve on this estimator by producing\n31This comes with the exception of methods like Yang et al., 2023; Onal et al..\nBesides, Papamarkou et al. (2024) sketch avenues with which Bayesian methods\ncan still provide advantages in the age of large-scale methods.\n32In contrast to the claims of Hou et al., 2023a.\n33The idea is that if the classifier indicates that a generation implies another\nand vice versa, they must (ought to) be equivalent.\n2.3 uncertainty in natural language processing\n62\nmore variable generations through targeted token substitutions.\nInstead of computing the entropy over hard meaning clusters,\nNikitin et al. (2024) propose to instead compute the entropy using\nsemantic kernels that measure the similarity in meaning between\nmodel responses, replacing hard clusters.\nVerbalized Uncertainty.\nOriginating from works like T5 (Raf-\nfel et al., 2020), natural language has become a general interface\nfor modern NLP models. This refers both to embedding other,\ntraditionally non-generative tasks such as sequence classification\ninto a sequence-to-sequence task, but also to users increasingly\ninteracting with language models through prompting. Mielke et al.\n(2022) already demonstrated that pre-trained models could be\nfinetuned to express different levels of uncertainty in words. This\nhowever required finetuning on human-annotated data, while mod-\nern approaches simply prompt the LLM to express its uncertainty\nin words (Kadavath et al., 2022; Xiong et al., 2023; Tian et al.,\n2023; Chen et al., 2023a), often through percentage values (“Confi-\ndence: 96 %”) or confidence expressions (“Confidence: Very high”),\nwhich are then mapped back onto numerical values for evaluation\npurposes. Tian et al. (2023) for instance find that through the\ncombination of suitable prompts and temperature-scaling, the cali-\nbration error of such methods can be noticeably reduced. However,\nthey also find that the distributions of confidence expressions are\nhighly skewed—while it does differ between datasets, the tested\nGPT models (GPT-3.5 and GPT-4) tend to mostly confident ex-\npressions, likely due to the unequal usage of these terms in their\ntraining data. This finding is corroborated by Yona et al. (2024);\nSingh et al. (2024a); Krause et al. (2023), indicating that LLMs\nalways generate decisive answer even for uncertain questions, and\nthat this is challenging to change through prompting alone. When\nresults are strong, this might coincide with cases in which the\ndataset is too easy and the skewed confidence expression distribu-\ntion actually conforms to the results (as for instance for TriviaQA\nin Ulmer et al., 2024a; Xue et al., 2024a). Lin et al. (2022a) also\nfinetune an LLM to verbalize its uncertainty, but do so on automat-\nically generated confidence targets that are obtained by checking\nthe model’s performance on some sub-category of a task, like differ-\nent question types for mathematical reasoning. A similar approach\nis taken by Zhang et al. (2023a), finetuning them to admit their\nuncertainty for incorrect answers. In the case of Kadavath et al.\n(2022), the LLM is simply asked directly whether its answer was\ntrue or false. Band et al. (2024) finetune verbalized uncertainty\nfrom a Bayesian decision-making standpoint, increasing factual-\nity. Zhou et al. (2023) investigate the general use of linguistic\n2.3 uncertainty in natural language processing\n63\nconfidence expressions in LLMs, and show that accuracy can be\ninfluenced through the use of such expressions in the prompt.\nUncertainty for Black-box Models.\nThe commercialization\nof LLM-based chatbots such as ChatGPT (OpenAI, 2022) also\ncreated a trend of black-box models, which are shielded by an\nAPI. As such, any UQ method has to do without any access to\nmodel latent representations, logits or output probabilities. The\nquestion of whether and how uncertainty can be estimated from\ntext generations alone therefore also has become an active area of\nresearch. Such approaches include predicting confidence directly\nfrom the generated text using an auxiliary model (Chapter 6; Ulmer\net al., 2024a), verbalized uncertainty methods from the previous\nparagraph, or comparing the similarity of generations given the\nsame input (Lin et al., 2023). Su et al. (2024) further show that\nLLM predictions can be conformalized even without access to\nthe probabilities through repeated sampling and word frequencies\nanalysis alone.\nReward Modeling.\nAs part of the contemporary language\nmodel pipeline, models are first pre-trained on large amounts of\ntext using a language modeling objective (Devlin et al., 2019;\nRadford et al., 2019), then finetuned on a number of instructions,\nand finally undergo a step that aims to align their behavior with\ngeneral human values (Ouyang et al., 2022). This last step is\noften performed using reinforcement learning from human feedback\n(RLHF; Christiano et al., 2017; Stiennon et al., 2020). This involves\nthe use of a trained reward model, that predicts the quality of a\ngeneration based on human preference data. While it has been\nfound that this step can hurt model calibration (Zhu et al., 2023),\nthe reward modeling itself has also been characterized as brittle, and\nthus a number of works have proposed Bayesian approaches to the\ntarget model finetuning or the reward model to increase robustness\n(Zhai et al., 2024; Yang et al., 2024; Zhang et al., 2024b,c).\nHuman Label Variation.\nCompared to other input modalities,\nthe variability, ambiguity and underspecification of language (Sec-\ntion 2.1.3) calls the validity of a single ground truth for training\ninto question. Indeed, there have been several calls to embrace\nthis diversity for classification (Basile et al., 2021; Plank, 2022;\nBaan et al., 2022; Gruber et al., 2024) and language generation\ntasks (Baan et al., 2023). Importantly, this opens up new avenues\nfor better modeling and representing of the uncertainty in the\nunderlying data (Nie et al., 2020; Zhou et al., 2022; Uma et al.,\n2021; Davani et al., 2022; Wu et al., 2023a), modeling annotators\n2.4 uncertainty & trust\n64\n(Deng et al., 2023), and to learn from fewer instances (Gruber\net al., 2024). Training on single labels or references has for instance\nbeen hypothesized to cause the miscalibration of neural models\nto human language variability (Giulianelli et al., 2023; Ilia and\nAziz, 2024), and to potentially be responsible for the inadequacy of\ngreedy decoding in natural language generation (Eikema and Aziz,\n2020; Eikema, 2024). The variation of labels should therefore be\nreframed as an opportunity, as it for instance also allows to more\neasily learn second-order predictors like evidential neural networks\nor credal sets (Javanmardi et al., 2024).\n2.4\nUncertainty & Trust\nEven though we have already discussed several applications of\nuncertainty quantification in Section 1.2 in the first chapter, it\nis useful to zoom in on the aspect of trust, why it matters, and\nhow quantifying the uncertainty of a ML system can help. The\nreason for this is the following: The main promise of machine\nlearning algorithm lies in its ability to analyze and process large\nswaths of data, identifying potential patterns that remain elusive\nfor even the most astute humans. As such, it promises to either\nreplace or support human decision-makers. However, even if a\npart of the deliberation for a decision is taken over by a machine,\npeople are the ones that remain affected by it. This is true for\nall the examples of decision support including for medical staff,\nself-driving cars or automated translation systems. Trust is the\nsocial mechanism that governs this relationship, and is a necessary\nrequirement for it to have a positive effects. If trust is not present,\nwe run the risk of alienating the people affected, leading to them\nignoring the automation and thus foregoing any benefits, or\neven creating negative consequences. Indeed, Inie (2024) finds\nin a diverse survey that participants perceive AI systems as less\ntrustworthy when problems they are trying to solve or the models\nthemselves are complex, and when no human expert is in the loop.\nJacovi et al. (2021) formalize this dynamic using notions of\ninterpersonal trust from sociology. They thereby define two roles:\nThe trustor (i.e. the person trusting someone) and the trustee (i.e.\nthe person being trusted). In order to make this distinction clearer\nin our context, we will notate these roles by trustor\nand trustee\n. They employ the following definition of interpersonal trust:\nDefinition 1 (Interpersonal Trust; Mayer et al., 1995). If a trustor\nbelieves that a trustee\nwill act in their best interest and\n2.4 uncertainty & trust\n65\naccepts vulnerability to the trustee\n’s actions, then the trustor\ntrusts the trustee\n.\nThe authors admit that this definition is somewhat simplistic:\nAI systems are not people, and as such, terms such as reliance\n(i.e., the trust put into an object) might be more applicable (Baier,\n1986). However, users often show tendencies to anthropomorphize\nAI systems (Miller, 2019; Jacovi and Goldberg, 2021). And thus,\nwe can use a variation of Definition 1 to define human-AI trust.\nJacovi et al. here use the notion of contract between the trustor\nand the trustee\n, which in the human-AI case has to be\nexplicit instead of implicit. Such contracts define certain properties\nor behaviors that model is expected to uphold. This can include\nthings as for instance robustness, fairness w.r.t. certain group in the\ndatasets, or interpretability and finally leads us to the definition of\nhuman-AI trust:\nDefinition 2 (Human-AI Trust; Jacovi et al., 2021). A trustee\nin the form of an AI model is trustworthy if it is capable of\nmaintaining a specific contract with the trustor\n.\nJacovi et al. further distinguish two kinds of trust: Intrinsic\ntrust, when the decision process of the trustee\nis observable and\nmatches the trustor\n’s own priors. This is possible in a decision\ntree, but very hard for neural networks, as their size can obscure\nthe decision process. Therefore, we focus here on extrinsic trust,\nwhich is built by observing symptoms of a trustworthy model. A\nsymptom of a trustworthiness can for example be its (consistent)\nperformance of the trustee\nmodel, as for instance explored by\nYin et al. (2019); Rechkemmer and Yin (2022). While the above\nassumed the trustor\nto be human and the trustee\nto be\nan AI system, recent work has also started exploring whether AI\nsystems can exhibit human trust behaviors (Xie et al., 2024).\nIt can be argued that one such tool for building extrinsic trust\ncan be uncertainty quantification methods: Using them, the trustee\ncan communicated how much weight should be assigned to\nits predictions, and when they are better to be ignored. Further,\nexplicit contracts like in Definition 2 can be formed by providing\nmodel cards that for instance report the calibration of a model on\nspecific datasets. Overall, Liao and Sundar (2022) describe that\nsuch trust in automation is not inherent, and that additional care\nhas to be put into how to design the trust cues for an end user.\nFor this reason, we will discuss ways of communicating uncertainty\nnext.\n2.5 communicating uncertainty\n66\n2.5\nCommunicating Uncertainty\nUnderstanding the usefulness of a model can be challenging\nfor laypeople and experts alike. Even when possessing technical\ndomain knowledge, NLP practitioners for instance struggle to se-\nlect the best encoder model for a task (Bassignana et al., 2022).\nEven accuracy scores or other performance metrics can be hard\nto interpret, especially when they may unknowingly degrade un-\nder distributional shift in an application. The previous sections\nhave demonstrated the diversity of ways in which uncertainty is\nmeasured, often requiring knowledge about the model, methods\nor entire schools of thought (as in the frequentist vs. the Bayesian\nexample). In practice, requiring such knowledge from laypeople\nis unrealistic; furthermore, the interpretation of such measure is\nalso influenced by human numeracy (Zikmund-Fisher et al., 2007;\nGalesic and Garcia-Retamero, 2010) and cognitive biases (Reyna\nand Brainerd, 2008; Daniel, 2017; Spiegelhalter, 2017). There-\nfore, Bhatt et al. (2021) advocate that in practice, uncertainty\nmeasure should be tailored to and tested with the different stake-\nholders they are targeted towards. This includes an arsenal of\nways such as communicating numerical values, graphical means\nor the verbalized uncertainty from Section 2.3. However, the best\nway of communicating uncertainty in an NLP context remains\napplication-dependent and underexplored. One promising avenue\nis the verbalized uncertainty in Section 2.3, although this approach\nat its current stage remains quite simplistic: Usually, uncertainties\nare communicated as percentage values or values on a discrete scale,\ninstead of making use of the rich variety in human uncertainty\nexpressions (Section 2.1.4).\nEffects of Communicating Uncertainty.\nSome works have\ninvestigated how communicated uncertainty influences the trust\nof human users. For instance, Zhang et al. (2020d) show how\ndisplaying confidence scores can help to calibrate people’s trust in\na model, but that it may not necessarily improve the outcomes of AI-\nassisted decision making, whereas Kim et al. (2024b) find a positive\neffect on accuracy in a human study with LLMs. Paradoxically,\nVodrahalli et al. (2022) show how these outcomes can be improved\neven when the underlying confidence scores are not calibrated. In\nanother experiment with human participants, Dhuliawala et al.\n(2023) showcase how misleading uncertainty can produce lose-lose\nsituations. In their study, they quantify human trust in uncertainty\nestimates through monetary bets on a model’s answers in a question-\nanswering task. They find two things in the face of unreliable\nuncertainty estimates: Firstly, a smaller overall pay-off for the\n2.6 applications of uncertainty\n67\nparticipants and a loss of trust in the system, both caused due to\nor signified by more conservative bets. In general, it should also\nbe noted that notions like trust are notoriously hard to isolate in\nhuman experiments, and that any stated results also presuppose a\nspecific model between model predictions and their influence on\nhuman decision-making.\n2.6\nApplications of Uncertainty\nPrevious sections have focused on characterizing and quantifying\nuncertainty that one encounters in machine learning and natural\nlanguage processing. This is not a purely intellectual quest, and\nwe have already touched on some potential use-cases in Section 1.2.\nThere is exists a trove of research works on several downstream\napplications that uncertainty quantification can be used for, a\n(non-exhaustive) list of which we present here.\nFairness.\nAlgorithmic fairness has recently increased in popular-\nity as a field that studies systematic biases and mitigation strategies\nin AI algorithms (Pessach and Shmueli, 2023). In this regard, some\nworks have researched Bayesian treatments of fairness metrics (Ji\net al., 2020; Kuzucu et al., 2023; Barrainkua et al., 2024). Others\nhave argued that uncertainty can be a source of unfairness (Singh\net al., 2021; Ali et al., 2021; Tahir et al., 2023; Wang et al., 2024a;\nCooper et al., 2024) and propose its quantification as a way to\nreduce bias during training (Stone et al., 2022). The relationship\nbetween debiasing techniques and UQ has further been investigated\nby Kuzmin et al. (2023).\nError Detection.\nSince uncertainty estimates are usually hard\nto evaluate due to the lack of ground truth, and thus error detection\nis both a downstream application as well as an evaluation strategy.\nThe intuition lies in the fact that predictions with higher uncertainty\nshould assumed to be more likely to be wrong. Examples for this are\nfor instance the works of Kong et al. (2020a); Ashukha et al. (2020);\nVazhentsev et al. (2022); Thuy and Benoit (2023); Vazhentsev et al.\n(2023), among many others. In the context of LLMs, uncertainty\nquantification has also been applied specifically to hallucination\ndetection (Xiao and Wang, 2021; Manakul et al., 2023; Zhang et al.,\n2023b; Band et al., 2024; Detommaso et al., 2024).\nOut-of-distribution Detection.\nOut-of-distribution detection\nfollows a similar logic as error detection. As inputs different from\nthe training data of a model should could lead to unexpected\npredictions since the model is underspecified on them (i.e. different\n2.6 applications of uncertainty\n68\nmodels that fit the training data will create disagreeing predictions\non unseen data; D’Amour et al., 2022), we want the model to be\ngenerally more uncertain about its prediction. In contrast to error\ndetection however, this applications focuses on model uncertainty,\nsince errors can be caused by high model uncertainty or inherent\ndifficulty alike. This assumptions has been shown to be formally\nincorrect for some simple ReLU networks (Section 4.1; Hein et al.,\n2019; Ulmer and Cinà, 2021), and the ability of uncertainty to\ndetect OOD inputs has been investigated in a larger number of\nworks (see, among many others, DeVries and Taylor, 2018; Snoek\net al., 2019; Liu et al., 2023; Ulmer et al., 2020; Kong et al., 2020a;\nStadler et al., 2021; Arora et al., 2021; Ulmer et al., 2022b; Uppal\net al., 2024).\nConditional Computation.\nUncertainty can also be used as\na signal to switch the intended way of processing for an input,\nwhich can be motivated by cognitive reasons (e.g. based on system\n1 and system 2 in humans; Daniel, 2017), boosting performance\n(Gerych et al., 2024) or to improve efficiency (Schuster et al., 2022;\nVarshney and Baral, 2022). Gerych et al. (2024) for instance use\nconfidence scores to route inputs to a pool of models to find the\nbest-performing one, and Zheng et al. (2019) use uncertainty to\ndetermine the right module from a mixture of experts. In NLG,\nvan der Poel et al. (2022) use mutual information to switch the\ndecoding algorithm, and Xiao and Wang (2021) adapt beam search\nbased on uncertainty in order to alleviate hallucinations. Another\nusage of uncertainty enables the early exciting from a model, i.e.\nwhere not all layers of a deep learning model are used (Schuster\net al., 2022; Fei et al., 2022; Bajpai and Hanawal, 2024). Lastly,\nuncertainty has also been utilized in model cascades, where we try\nto select one of a pool of increasingly-sized model based on the\ndifficulty of an input (Teerapittayanon et al., 2016; Varshney and\nBaral, 2022; Jitkrittum et al., 2024; Gupta et al., 2024).\nActive Learning.\nActive learning describes a field of machine\nlearning in which an algorithm selects unlabeled instances that are\ngiven to a human for labeling, and are subsequently added to the\nalgorithm’s training data (Settles, 2009). The use of uncertainty\nmeasures for this purpose has long predated deep neural networks\n(e.g. Lewis and Gale, 1994; Lewis and Catlett, 1994; Scheffer et al.,\n2001), and has found many applications since their revival (Ren\net al., 2021; Zhang et al., 2022c). When using uncertainty to\nidentify samples of interest, there also exists a colorful bouquet\nof approaches: Frequentist methods usually rely on some measure\nof model confidence (Wang and Shang, 2014; Matiz and Barner,\n2.7 summary\n69\n2019; Ebrahimi et al., 2020; Zhang and Plank, 2021; Wang and\nPlank, 2023), Bayesian methods quantify metrics such as mutual\ninformation (Gal et al., 2017b; Kirsch et al., 2019; Kim et al., 2021;\nKirsch and Gal, 2022; Smith et al., 2023) and evidential methods\nutilize distributional uncertainty (Zhu et al., 2021; Park et al., 2022;\nHemmer et al., 2022).\nRequesting Human Oversight.\nActive learning is a specific\ncase of human-in-the-loop problems that is focused on resource-\nefficient data labeling, but can be seen as just one instance of a\nclass of applications in which human oversight or intervention is re-\nquested upon uncertainty. Other examples include for for instance\nplanning problems in reinforcement learning (Singi et al., 2023), in-\ndustrial applications (Treiss et al., 2021), clarifying uncertain parts\nin image segmentation for remote sensing (García Rodríguez et al.,\n2020), text moderation (Andersen and Maalej, 2022; Andersen and\nZukunft, 2022), and co-annotation of data (Li et al., 2023a). In\ngeneral, these applications promise to alleviate the workload that\nwould be otherwise assigned to human experts, and only request\ntheir assistance in the case of difficult inputs.\n2.7\nSummary\nThis chapter has given a fairly comprehensive account of uncertainty\nand its relevant concepts, definitions, methods and applications for\ndeep learning and natural language processing. It has provided an\noverview over the different definitions of uncertainty in statistics,\ni.e. the frequentist and Bayesian viewpoints, and how uncertainty in\nlinguistics plays a layered role as an inherent feature of language on\nthe one side, and a tool for communication of one’s world state on\nthe other. These different notions crystallize in their applications to\nneural networks: Statistical uncertainties permeate model training\nand inference, and linguistic uncertainties influence the processing\nof natural language inputs. Not only is the quantification of these\nuncertainties challenging and methods to do so are multifarious, but\nthe adequate communication of uncertainty is equally difficult. This\nlast step is pivotal to enable human-AI collaboration, in which\ntrust relationships are formed between users and their silicate\ncollaborators. As with human relationships, this trust can be built\nbut also lost, which suggests more research is needed to understand\nthis dynamic better.\n3\n|\nAddressing Uncertainty\nin Experimental Design\n“When you run an experiment, you take notes, think for a\nwhile, then publish your results. If you don’t publish, nobody\nwill learn from your experience. The whole idea is to save\nother from repeating what you’ve done.”\n—Clifford Stoll in The Cuckoo’s Egg: Tracking a Spy\nThrough the Maze of Computer Espionage.\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\nYear\n0\n200\n400\n600\n800\n1000\n1200\n1400\nPublications\nEMNLP\nEMNLP Findings\nTACL\nCL\nCoNLL\nNAACL\nEACL\nACL\nACL Findings\nCOLING\nAACL\nFigure 3.1: Published papers at NLP venues. Gaps are due to some\nvenues not producing proceedings in any given year. Notably, this plot\ndoes not include NLP papers published at venues such as NeurIPS,\nICML, or ICLR.\nBefore returning to the uncertainty in NLP models, we will\nfirst engage in another, wider perspective on where uncertainty\nhides in the NLP pipeline.\nDL in general, and NLP in its\ncurrent form, are largely empirical sciences:\nWe obtain new\nknowledge by forming hypotheses, running experiments, and\nthen analyzing results to come to a conclusion about our initial\nsuppositions. In the the last decade or so, this field has ballooned\nin size: In Figure 3.1, we show the number of published conference\n70\naddressing uncertainty in experimental design\n71\npapers in NLP venues since 2012, which has more than quadrupled.\nWhile such growth is remarkable, it comes at a cost: Akin to\nconcerns in other disciplines (John et al., 2012; Jensen et al., 2021),\nseveral authors have noted major obstacles to reproducibility\n(Gundersen and Kjensmo, 2018; Belz et al., 2021) and a lack of\nhypothesis testing (Marie et al., 2021) or published results not\ncarrying over to different experimental setups, for instance in\ntext generation (Gehrmann et al., 2022) and with respect to new\nmodel architectures (Narang et al., 2021). Others have questioned\ncommonly-accepted experimental protocols (Gorman and Bedrick,\n2019; Søgaard et al., 2021; Bouthillier et al., 2021; van der Goot,\n2021) as well as the (negative) impacts of research on society\n(Hovy and Spruit, 2016; Mohamed et al., 2020; Bender et al.,\n2021; Birhane et al., 2022) and environment (Strubell et al., 2019;\nSchwartz et al., 2020; Henderson et al., 2020). Lastly, the adoption\nof large language models that are also possibly closed-source\nhave exacerbated problems about experimental protocols further\n(Mizrahi et al., 2024; Balloccu et al., 2024). These problems have\nnot gone unnoticed—many of the mentioned works have proposed\na cornucopia of solutions.\nIn a quickly-moving environment\nhowever, keeping track and implementing these proposals becomes\nchallenging.\nThis chapter addresses these issue in two ways: On the one hand,\nopen issues in reproducibility and replicability are woven together\ninto a cohesive set of guidelines for gathering stronger experimental\nevidence, that can be implemented with reasonable effort and which\nare discussed in Section 3.1. On the other hand, we zoom into the\nquestion of hypothesis testing (Section 3.2), with a specific focus\non the almost stochastic order test (ASO; del Barrio et al., 2018a;\nDror et al., 2019) in Section 3.2.1 and its application to question-\nanswering with LLMs in Section 3.2.2. The core thesis of this\nchapter is that increased efforts in reproducibility and replicability\nare intricately linked to the question of uncertainty in empirical\nresearch: For example, transparent and diligent data curation\nenables better modeling of uncertainty (referring to the discussion\non language paraphrasticity in Section 2.1.3 and human label\nvariation in Section 2.3), and a more rigorous experimental protocol\nand statistical hypothesis testing can help to unveil the uncertainty\nlingering in results, aiding the development of better methods and\nbringing more clarity to the research landscape. Therefore, we\nbuild these ideas up from the scientific method and show their\nimplementation in the experimental pipeline.\n3.1 experimental standards for nlp\n72\n3.1\nExperimental Standards for NLP\nThe following work is based on Ulmer et al. (2022a).\nFigure 3.2: Schematic representation of the scientific method in Deep\nLearning. After forming hypotheses, we conduct our experiments by\nmodeling some data of interest and analyzing the results to obtain some\nevidence to support or reject our initial assumptions. While reproducibil-\nity entails the reproduction of evidence based on the hypotheses and\na description of the experiments, replicability refers to a step-by-step\ncopy of the pipeline using the original data, model, and analyses.\nThe Scientific Method.\nKnowledge can be obtained through\nseveral ways including theory building, qualitative methods, and\nempirical research (Kuhn, 1970; Simon, 1995). Here, we focus on\nthe latter aspect, in which (exploratory) analyses lead to falsifiable\nhypotheses that can be tested and iterated upon (Popper, 1934).34\nThis process requires that anyone must be able to back or dispute\nthese hypotheses in the light of new evidence.\nIn the following, we focus on the evidence-based evaluation\nof hypotheses and how to ensure the scientific soundness of the\nexperiments which gave rise to the original empirical evidence,\nwith a focus on replicability and reproducibility. In computational\nliterature, one term requires access to the original code and data\nin order to re-run experiments exactly, while the other requires\nsufficient information in order to reproduce the original findings\neven in the absence of code and original data (see also Figure 3.2).35\n34While such hypothesis-driven science is not always applicable or possible\n(Carroll, 2019), it is a strong common denominator that encompasses most\nempirical ML research.\n35Strikingly, these central terms already lack agreed-upon definitions (Peng,\n2011; Fokkens et al., 2013; Liberman, 2015; Cohen et al., 2018), however we\nfollow the prevailing definitions in the NLP community (Drummond, 2009;\nDodge and Smith, 2020) as the underlying ideas are equivalent.\n3.1 experimental standards for nlp\n73\nReplicability.\nWithin DL, we take replicability to mean the\n(near-)exact replication of prior reported evidence. In a compu-\ntational environment, access to the same data, code and tooling\nshould be sufficient to generate prior results. However, many fac-\ntors, such as hardware differences, make exact replication difficult\nto achieve. Nonetheless, we regard experiments to be replicable if\na practitioner is able to re-run them to produce the same evidence\nwithin a small margin of error dependent on the environment,\nwithout the need to approximate or guess experimental details.\nReproducibility.\nIn comparison, we take reproducibility to\nmean the availability of all necessary and sufficient information\nsuch that an experiment’s findings can be independently reaffirmed\nwhen the same research question is asked. As discussed later,\nthe availability of all components for replicability is rare—even\nin a computational setting. An experiment then is reproducible\nif anyone with access to the publication is able to re-identify the\noriginal evidence, i.e. exact results differing, but patterns across\nexperiments being equivalent. This is illustrated by Figure 3.2,\nwhere replicability involves access to all data, modeling and analy-\nsis steps, whereas reproducibility only involves knowledge of the\nhypotheses, a description of the experiments, as well as their results.\nWe assume that the practitioner aims to follow these principles\nin order to find answers to a well-motivated research question\nby gathering the strongest possible evidence for or against their\nhypotheses. The guidelines in the following sections therefore aim\nto model or reduce uncertainty in each step of the experimental\npipeline through enhancing its reproducibility and / or replicability.\n3.1.1\nData\nFrequently, it is claimed that a model solves a particular cognitive\ntask, however in reality it merely scores higher than others on some\nspecific dataset according to some predefined metric (Schlangen,\n2021). Of course, the broader goal is to improve systems more\ngenerally by using individual datasets as proxies. Admitting that\nour experiments cover only a small slice of the real-world sample\nspace will help more transparently measure progress towards this\ngoal. In light of these limitations and as there will always be\nprivate or otherwise unavailable datasets which violate replicability,\na practitioner must ask themselves: Which key information about\nthe data must be known in order to reproduce an experiment’s\nfindings? In this section we define requirements for putting this\nquestion into practice during dataset creation and usage such that\n3.1 experimental standards for nlp\n74\nanyone can draw the appropriate conclusions from a published\nexperiment.\nChoice of Dataset.\nThe choice of dataset arises from the need\nto answer a specific research question within the limits of the\navailable resources. Such answers typically come in the form of\ncomparisons between different experimental setups while using the\nequivalent data and evaluation metrics. Using a publicly available,\nwell-documented dataset will likely yield more comparable work,\nand thus stronger evidence. In absence of public data, creating a\nnew dataset according to guidelines which closely follow prior work\ncan also allow for useful comparisons. Should the research question\nbe entirely unexplored, creating a new dataset will be necessary. In\nany case, the data itself must contain the information necessary to\ngenerate evidence for the researcher’s hypothesis. For example, a\nmodel for a classification task will not be learnable unless there are\ndistinguishing characteristics between data points and consistent\nlabels for evaluation. Therefore, an exploratory data analysis is\nrecommended for assessing data quality and anticipating problems\nwith the research setup. Simple baseline methods such as regression\nanalyses or simply manually verifying random samples of the data\nmay provide indications regarding the suitability and difficulty of\nthe task and associated dataset (Kreutzer et al., 2022). On the flip\nside, a lower-quality dataset runs the danger of introducing noise\nand therefore aleatoric uncertainty into the dataset (Baan et al.,\n2023).\nMetadata.\nAt a higher level, data sheets and statements (Ge-\nbru et al., 2021; Bender and Friedman, 2018) aim to standardize\nmetadata for dataset authorship in order to inform future users\nabout assumptions and potential biases during all levels of data\ncollection and annotation—including the research design (Hovy and\nPrabhumoye, 2021). Simultaneously, they encourage reflection on\nwhether the authors are adhering to their own guidelines (Waseem\net al., 2021). Generally, higher-level documentation should aim\nto capture the dataset’s representativeness with respect to the\nglobal population. This is especially crucial for “high-stakes” envi-\nronments in which subpopulations may be disadvantaged due to\nbiases during data collection and annotation (He et al., 2019; Sap\net al., 2022). Even in lower-stake scenarios, a model trained on\nonly a subset of the global data distribution can have inconsistent\nbehavior when applied to a different target data distribution and\ndisplay high model uncertainty (D’Amour et al., 2022; Koh et al.,\n2021). For instance, domain differences have a noticeable impact on\nmodel performance (White and Cotterell, 2021; Ramesh Kashyap\n3.1 experimental standards for nlp\n75\net al., 2021). Increased data diversity can improve the ability of\nmodels to generalize to new domains and languages (Benjamin,\n2018), however diversity is difficult to quantify (Gong et al., 2019)\nand full coverage is unachievable. This highlights the importance\nof documenting representativeness in order to ensure reproducibil-\nity—even in absence of the original data. For replicability using\nthe original data, further considerations include long-term storage\nand versioning, as to ensure equal comparisons in future work.\nInstance Annotation.\nAchieving high data quality requires\nthat the data must be accurate and relevant for the task to enable\neffective learning (Pustejovsky and Stubbs, 2012; Tseng et al.,\n2020) and reliable evaluation (Bowman and Dahl, 2021; Basile\net al., 2021). Since most datasets involve human annotation, a\ncareful annotation design is crucial (Pustejovsky and Stubbs, 2012;\nPaun et al., 2022). Ambiguity in natural language poses inherent\nchallenges and disagreement is genuine (see Sections 2.1.3 and 2.3\nor Basile et al., 2021; Specia, 2021; Uma et al., 2021; Plank, 2022).\nAs insights into the annotation process are valuable, yet often\ninaccessible, we recommend to release datasets with individual-\ncoder annotations, as also put forward by Basile et al. (2021);\nPrabhakaran et al. (2021); Plank (2022) and to complement data\nwith insights like statistics on inter-annotator coding (Paun et al.,\n2022), e.g., over time (Braggaar and van der Goot, 2021), or coder\nuncertainty (Bassignana and Plank, 2022). When creating new\ndatasets such information strengthens the reproducibility of future\nfindings, as they transparently communicate the inherent variability\ninstead of obscuring it. Furthermore, this opens up new avenues to\nmodel distributions instead of single gold labels to more accurately\nreflect uncertainty (Javanmardi et al., 2024; Gruber et al., 2024)\nor modeling single annotators (Deng et al., 2023).\nPre-processing.\nGiven a well-constructed or well-chosen\ndataset, the first step of an experimental setup will be the process\nby which a model takes in the data. This must be well documented\nor replicated—most easily by publishing the associated code—as\nperceivably tiny pre-processing choices can lead to huge accuracy\ndiscrepancies (Fokkens et al., 2013) and influences model uncer-\ntainty during inference.36 Typically, this involves decisions such as\nsentence segmentation, tokenization and normalization. In general,\n36One could for instance imagine a case where data uncertainty is created by\nnot removing certain characters like rare symbols or fragments of code, or\nincreasing model uncertainty through suboptimal tokenization of a language,\nfor instance through another language’s or multilingual tokenizer (Rust et al.,\n2021).\n3.1 experimental standards for nlp\n76\nthe data setup pipeline should ensure that a model “observes” the\nsame kind of data across comparisons. Next, the dataset must be\nsplit into representative subsamples which should only be used for\ntheir intended purpose, i.e. model training, tuning and evaluation\n(see Section 3.1.3). In order to support claims about the generality\nof the results, it is necessary to use a test split without overlap\nwith other splits. Alternatively, a tuning / test set could consist of\ndata that is completely foreign to the original dataset (Ye et al.,\n2021), ideally even multiple sets (Bouthillier et al., 2021), which\nis also essential when trying to quantify any model uncertainty\nin the face of distributional drifts (see Section 4.2). It should be\nnoted that even separate, static test splits are prone to unconscious\n“overfitting”, if they have been in use for a longer period of time, as\npeople aim to beat a particular benchmark (Gorman and Bedrick,\n2019). If a large variety of resources are not available, it is also pos-\nsible to construct challenging test sets from existing data (Ribeiro\net al., 2020; Kiela et al., 2021; Søgaard et al., 2021). Finally, the\nmetrics by which models are evaluated should be consistent across\nexperiments and thus benefit from standardized evaluation code\n(Dehghani et al., 2021). For some tasks, metrics may be driven\nby community standards and are well-defined (e.g. classification\naccuracy). In other cases, approximations must stand in for hu-\nman judgment (e.g. in machine translation). In either case—but\nespecially in the latter—dataset authors should inform users about\ndesirable performance characteristics and recommended metrics.\nAppropriate Conclusions.\nThe results a model achieves on a\ngiven data setup should first and foremost be taken as just that.\nAppropriate, broader conclusions can be drawn using this evidence\nprovided that biases or incompleteness of the data are addressed\n(e.g., results only being applicable to a subpopulation). Even with\nstatistical tests for the significance of comparisons, properties such\nas the size of the dataset and the distributional characteristics of\nthe evaluation metric may influence the statistical power of any\nevidence gained from experiments (Card et al., 2020). In experi-\nments with large models, practitioners might decide to only run\nthe model on a subset of the data. But again, such a sample might\nnot be powerful enough and not enable fair comparisons with other\nmodels (Balloccu et al., 2024). It is therefore important to keep in\nmind that in order to claim the reliability of the obtained evidence,\nfor example, larger performance differences are necessary on less\ndata than what might suffice for a large dataset, or across multiple\ncomparisons (see Section 3.1.3). Finally, a practitioner should be\naware that a model’s ability to achieve high scores on a certain\ndataset may not be directly attributable to its capability of simu-\n3.1 experimental standards for nlp\n77\nlating a cognitive ability, but rather due to spurious correlations\nin the input (Ilyas et al., 2019; Schlangen, 2021; Nagarajan et al.,\n2021). By for instance only exposing models to a subset of features\nthat should be inadequate to solve the task, we can sometimes\ndetect when they take unexpected shortcuts (Fokkens et al., 2013;\nXenos et al., 2023). Communicating the limits of the data helps\nfuture work in reproducing prior findings more accurately.\nBest Practices: Data\n⋄Consider dataset & experimental limitations (Schlangen, 2021);\n⋄Document task adequacy, representativeness and pre-processing\n(Bender and Friedman, 2018);\n⋄Split the data such as to avoid spurious correlations;\n⋄Publish the dataset accessibly & indicate changes;\n⋆Perform exploratory data analyses to ensure task adequacy\n(Kreutzer et al., 2022);\n⋆Publish the dataset with individual-coder annotations;\n⋆Consider the dataset’s statistical power (Card et al., 2020).\n3.1.2\nCodebase & Models\nThe NLP community has historically taken pride in promoting\nopen access to papers, data, code, and documentation, but some\nhave also noted room for improvement (Wieling et al., 2018; Belz\net al., 2021). The benefit of such a repository is in its ability to\nenable direct replication, helping to reduce uncertainty in modeling\nwhen building upon others work. In DL however, full datasets\ncan be large and impractical to share. Due to their importance\nhowever, it is essential to carefully consider how one can share\nthe data with researchers in the future. Therefore, repositories\nfor long-term data storage backed by public institutions should\nbe preferred (e.g. LINDAT / CLARIN by Váradi et al., 2008).\nNevertheless, practitioners often can not distribute data due to\nprivacy, legal, or storage reasons. In such cases, practitioners must\ninstead carefully consider how to distribute data and tools to allow\nfuture research to produce accurate replications of the original data\n(Zong et al., 2020).\nHyperparameter Search.\nHyperparameter tuning strategies\nremain an open area of research (e.g. Bischl et al., 2023), but are\ncentral to the replication of contemporary models. Well-chosen\nhyperparameters promote stability in model predictions, while ill-\n3.1 experimental standards for nlp\n78\nchosen parameters induce additional additional uncertainty.37 The\nfollowing rules of thumb exist: Grid search or Bayesian optimization\ncan be applied if few parameters can be searched exhaustively under\nthe computation budget. Otherwise, random search is preferred, as\nit explores the search space more efficiently (Bergstra and Bengio,\n2012). Advanced methods like Bayesian optimization (Snoek et al.,\n2012) and bandit search-based approaches (Li et al., 2017) can be\nused as well if applicable (Bischl et al., 2023). To avoid unnecessary\nguesswork, the following information is expected: Hyperparameters\nthat were searched per model (including options and ranges), the\nfinal hyperparameter settings used, number of trials, and settings of\nthe search procedure if applicable. As tuning of hyperparameters is\ntypically performed using specific parts of the dataset, it is essential\nto note that any modeling decisions based on them automatically\ninvalidate their use as test data.\nModels.\nContemporary models (e.g. Vaswani et al., 2017; Devlin\net al., 2019; Dosovitskiy et al., 2021; Chen et al., 2021; Touvron\net al., 2023a,b; AI@Meta, 2024; Jiang et al., 2023a; Groeneveld\net al., 2024) have very large computational and memory footprints.\nTo avoid retraining models, and more importantly, to allow for\nreplicability, it is recommended to save and share model weights.\nThis may face similar challenges as those of datasets (namely, large\nfile sizes), but it remains an impactful consideration. In most cases,\nsimply sharing the best or most interesting model could suffice,\nalthough sharing multiple models enables more robust significance\ntesting and allows for modeling of uncertainty through ensembling\n(Section 2.2.2). It should be emphasized that distributing model\nweights should always complement a well-documented repository\nas libraries and hosting sites might not be supported in the future.\nModel Evaluation.\nThe exact model and task evaluation pro-\ncedure can differ significantly (e.g. Post, 2018). It is important to\neither reference the exact evaluation script used (including param-\neters, citation, and version, if applicable) or include the evaluation\nscript in the codebase. Moreover, to ease error or post-hoc anal-\nyses, we highly recommend saving model predictions whenever\npossible and making them available at publication (Card et al.,\n2020; Gehrmann et al., 2022) and using standardized and tested\nimplementations (e.g. Von Werra et al., 2022). Using single metrics\n37Whether such uncertainty would be aleatoric or epistemic is difficult to decide;\nwhile more data could compensate for suboptimal hyperparameter values, it is\nintuitive that a model will be unlikely to converge and reduce its uncertainty\nfor e.g. adversarially chosen values. This reinforces the argument by Baan et al.\n(2023) that data and model uncertainty should not be seen as a dichotomy,\nbut rather as a spectrum.\n3.1 experimental standards for nlp\n79\ncan also distort results or paint a restrictive picture, which is why\nusing multiple different evaluation metrics is commendable (Marie\net al., 2021).\nModel Cards.\nApart from quantitative evaluation and optimal\nhyperparameters, Mitchell et al. (2019) propose model cards: A\ntype of standardized documentation, as a step towards responsible\nML and AI technology, accompanying trained ML models that\nprovide benchmarked evaluation in a variety of conditions, across\ndifferent cultural, demographic, or phenotypic and intersectional\ngroups that are relevant to the intended application domains. They\ncan be reported in the paper or project, and can help to collect\nimportant information for reproducibility, such as preprocessing\nand evaluation results. We refer to Mitchell et al. (2019); Menon\net al. (2020) for examples of model cards.\nBest Practices: Codebase & Models\n⋄Publish a code repository with documentation and license;\n⋄Report all details about hyperparameter search and model train-\ning;\n⋄Specify the hyperparameters for replicability;\n⋄Publish model predictions and evaluation scripts.;\n⋄Use multiple, complementary evaluation metrics;\n⋆Use model cards;\n⋆Publish models;\n3.1.3\nExperiments & Analysis\nExperiments and their analyses constitute the core of most scientific\nworks, and empirical evidence is valued especially highly in ML\nresearch (Birhane et al., 2022). However, there are common issues\nthat practitioners are faced with model training and experimental\nanalyses, for which we discuss counter-strategies here.\nModel Training.\nFor model training, it is advisable to set a\nrandom seed for replicability, and train multiple initializations\nper model in order to obtain a sufficient sample size for later\nstatistical tests. The number of runs should be adapted based\non the observed variance: Using for instance bootstrap power\nanalysis, existing model scores are raised by a constant compared\nto the original sample using a significance test in a bootstrapping\nprocedure (Yuan and Hayashi, 2003; Tufféry, 2011; Henderson\net al., 2018). If the percentage of significant results is low, we\n3.1 experimental standards for nlp\n80\nshould collect more scores.38\nBouthillier et al. (2021) further\nrecommend to vary as many sources of randomness in the training\nprocedure as possible (i.e., data shuffling, data splits etc.) to obtain\na closer approximation of the true model performance. When\ntraining more runs is not feasible such as in the case of LLMs,\nwe can for instance obtain additional observations by varying the\ngeneration process (see for instance the case study in Section 3.2.3).\nNevertheless, any drawn conclusion are still surrounded by a degree\nof statistical uncertainty, which can be combated by the use of\nstatistical hypothesis testing.\nSignificance Testing.\nUsing deep neural networks, a number\nof (stochastic) factors such as the random seed (Dror et al., 2019)\nor even the choice of hardware (Yang et al., 2018) or framework\n(Leventi-Peetz and Östreich, 2022) can influence performance and\nneed to be taken into account. First of all, the size of the dataset\nshould support sufficiently powered statistical analyses (see Sec-\ntion 3.1.1). Secondly, an appropriate significance test should be\nchosen. We give a few rules of thumb based on Dror et al. (2018):\nWhen the distribution of scores is known, for instance a normal\ndistribution for the Student’s-t test, a parametric test should be\nchosen. Parametric tests are designed with a specific distribution\nfor the test statistic in mind, and have strong statistical power (i.e.\na lower Type II error). The underlying assumptions can sometimes\nbe hard to verify (see Dror et al., 2018, Section 3.1), thus when in\ndoubt non-parametric tests can be used. This category features\ntests like the bootstrap, employed in case of a small sample size,\nor the Wilcoxon signed-rank test (Wilcoxon, 1992), when plenty\nobservations are available. Depending on the application, the usage\nof specialized tests might furthermore be desirable (Dror et al.,\n2019; Agarwal et al., 2021). We also want to draw attention to the\nfact that comparisons between multiple models and / or datasets,\nrequire an adjustment of the confidence level, for instance using\nthe Bonferroni correction (Bonferroni, 1936), which is a safe and\nconservative choice and easily implemented for most tests (Dror\net al., 2017; Ulmer et al., 2022c). Sadeqi Azer et al. (2020) provide\na guide on how to adequately word insights when a statistical test\nwas used, and Greenland et al. (2016) list common pitfalls and\nmisinterpretations of results. Due to spatial constraints, we refer to\nSection 3.2 for a slightly more technical introduction to the topic.\nCurrent trends surrounding LLMs further make significance testing\nchallenging, as training and evaluating multiple different model\n38The resulting tensions with modern DL hardware requirements are discussed\nin Section 5.5.\n3.1 experimental standards for nlp\n81\nruns can be prohibitively expensive. We explore different strategies\nin this restrictive setting in the case study in Section 3.2.3.\nCritiques & Alternatives.\nAlthough statistical hypothesis\ntesting is an established tool in many disciplines, its (mis-)use has\nreceived criticism for decades (Berger and Sellke, 1987; Demšar,\n2008; Ziliak and McCloskey, 2008).\nFor instance, Wasserstein\net al. (2019) criticize the p-value as reinforcing publication bias\nthrough the dichotomy of “significant” and “not significant”, i.e.\nby favoring positive results (Locascio, 2017). Instead, Wasserstein\net al. (2019) propose to report it as a continuous value and with\nthe appropriate scepticism.39 In addition to statistical significance,\nanother approach advocates for reporting effect size (Berger and\nSellke, 1987; Lin et al., 2013), so for instance the mean difference, or\nthe absolute or relative gain in performance for a model compared\nto a baseline.\nThe effect size can be modeled using Bayesian\nanalysis (Kruschke, 2013; Benavoli et al., 2017), which better fit\nthe uncertainty surrounding experimental results, but requires\nthe specification of a plausible statistical model producing the\nobservations40 and potentially the usage of markov chain Monte\nCarlo sampling (Brooks et al., 2011; Gelman et al., 2021). Benavoli\net al. (2017) give a tutorial for applications to ML and supply an\nimplementation of their proposed methods in a software package\nand guidelines for reporting details are given by Kruschke (2021),\nincluding for instance the choice of model and priors.\nBest Practices: Experiments & Analysis\n⋄Report mean & standard dev. over multiple runs;\n⋄Perform significance testing or Bayesian analysis and motivate\nyour choice of method;\n⋄Carefully reflect on the amount of evidence regarding your initial\nhypotheses.\n3.1.4\nDiscussion\nPrevious sections have emphasized the need to overhaul some\nexperimental standards and have describes their interactions\n39Or, as Wasserstein et al. (2019) note: “statistically significant—don’t say it\nand don’t use it”.\n40Here, we are not referring to a neural network, but instead to a process gener-\nating experimental observations, specifying a prior and likelihood for model\nscores. Conclusions are drawn from the posterior distribution over parameters\nof interest (e.g. the mean performance), as demonstrated by Benavoli et al.\n(2017).\n3.1 experimental standards for nlp\n82\nwith reducing and modeling uncertainty. But specifically with\nregard to statistical significance in Section 3.1.3, there is a stark\nconflict between the hardware requirements of modern methods\n(Sevilla et al., 2022) and the computational budget of the average\nresearcher.\nOnly the best-funded research labs can afford the\nincreasing computational costs to account for the statistical\nuncertainty of results and to reproduce prior works (Hooker, 2021).\nUnder these circumstances, it becomes difficult to judge whether\nthe results obtained via larger models and datasets actually\nconstitute substantial progress or just statistical flukes. While\nwe present some alternatives in Section 3.2.3, this environment\nalso make the use of traditional Bayesian DL techniques like\nin Section 2.2.2 more challenging. For this reason, researchers\nshould embrace data variability as a new avenues for modeling and\nreducing uncertainty in large contemporary models (as discussed\nin Section 3.1.1).\nEchoing our fundamental deliberations about the scientific pro-\ncess in Section 3.1, being able to (re-)produce empirical findings is\ncritical for scientific progress, particularly in fast-growing fields like\nNLP (Manning, 2015). To reduce the risks of a reproducibility cri-\nsis and unreliable research findings (Ioannidis, 2005), experimental\nrigor is imperative. Being aware of possible harmful implications\nand to avoid them is therefore important, since every step can carry\npossible biases (Hovy and Prabhumoye, 2021; Waseem et al., 2021).\nThis chapter aims at providing a toolbox of actionable recommen-\ndations, and a reflection and summary of the ongoing broader\ndiscussion. To improve the experimental standard in the field over-\nall, we can distill the following suggestions: As researchers, we\ncan start implementing the recommendations in this work in order\nto drive bottom-up change and reach a critical mass (Centola et al.,\n2018). As reviewers, we can shift focus from results to more\nrigorous methodologies (Rogers and Augenstein, 2021), and allow\nmore critiques and reproductions of past works and meta-reviews\nto be published (Birhane et al., 2022; Lampinen et al., 2021). As\na community, we can change the incentives around research and\nexperiment with new initiatives. With concrete best practices to\nraise awareness and a call for uptake, we hope to aid researchers\nin their empirical endeavors. The rest of this chapter is dedicated\nto the practice of statistical hypothesis testing and its challenges\nin the era of LLMs.\n3.2 statistical hypothesis testing\n83\n3.2\nStatistical Hypothesis Testing\nThe following work is based on Ulmer et al. (2022c).\nIn this part of the chapter, we are discussing statistical hy-\npothesis testing with an application to comparing two models or\nalgorithms. While terms like model or algorithms will be used\nalmost synonymously in the rest of this thesis, it will aid the rest\nof this chapter to define these notions better.\nDefinition 3 (Model). We define a model fθ to be the element of\nsome hypothesis class fθ ∈H. Here, the hypothesis class is loosely\ndefined as all neural predictors trained using the same architecture\nand training data.\nImportantly, the above definition does not imply that all predic-\ntors H comprise the same parameter values—they can be influenced\nby factors such as random seeds or the order of training samples,\nand in the case of LLMs, the use of different generation hyperpa-\nrameters or prompt templates.\nDefinition 4 (Metric & Observation). Let us define ϕ : H ×\nP(D) →R to be a function measuring the performance of a\npredictor fθ on some dataset D ∈P(D) in form of a real number\ns ∈R, called observation or score, with ϕ called the metric.\nWe will assume in the following that a higher number for s\nindicates a more desirable behavior. Now, we let SA denote a\nset of observations obtained from different instances of a specific\nhypothesis class A. Ideally for deep neural networks, obtaining a\nset of observations SA would involve training multiple instances\nof a network with the same architecture using different sets of\nhyperparameters and random initializations. Since the former part\noften becomes computationally infeasible in practice, we follow the\nadvice of Bouthillier et al. (2021) and assume that it is obtained\nby fixing one set of hyperparameters after a prior search and\nvarying as many other random elements as possible. Here, we only\ngive a very brief introduction into statistical hypothesis testing\nusing p-values, and refer the reader to resources such as Japkowicz\nand Shah (2011); Dror et al. (2018); Raschka (2018); Sadeqi Azer\net al. (2020); Dror et al. (2020); Riezler and Hagmann (2021) for\na more comprehensive overview. Using the introduced notation,\nwe can define a one-sided test statistic δ(SA, SB) based on the\ngathered observations. An example of such test statistics is for\ninstance the difference in observation means δ(SA, SB) = ˆµA −ˆµB\nwith µ(·) =\n1\n|S(·)|\nP\nsi∈S(·) si. We then formulate the following null\nhypothesis:\n3.2 statistical hypothesis testing\n84\nH0 : δ(SA, SB) ≤0.\n(3.1)\nThe null hypothesis H0 assumes the opposite of our desired\ncase, namely that A is not better than B, but equally as good or\nworse, as indicated by the value of the test statistic. Usually, the\ngoal becomes to reject this null hypothesis. p-value testing is a\nfrequentist method in the realm of statistical hypothesis tests. It\nintroduces the notion of data that could have been observed if we\nwere to repeat our experiment again using the same conditions,\nwhich we will write with superscript rep in order to distinguish them\nfrom our actually observed scores (Gelman et al., 2021). We then\ndefine the p-value as the probability that, under the null hypothesis\nH0, the test statistic using replicated observations is larger than or\nequal to the observed test statistic:\np(δ(Srep\nA , Srep\nB ) ≥δ(SA, SB) | H0).\n(3.2)\nWe can interpret this expression as follows: Assuming that A\nis not better than B, the test assumes a corresponding distribution\nof statistics that δ is drawn from. So how does the observed test\nstatistic δ(SA, SB) fit in here? This is what the p-value expresses:\nWhen the probability is high, δ(SA, SB) is in line with what we\nexpected under the null hypothesis, so we cannot reject the null\nhypothesis, or in other words, we cannot conclude A to be better\nthan B. If the probability is low, that means that the observed\nδ(SA, SB) is quite unlikely under the null hypothesis and that the\nreverse case is more likely—i.e. that it is likely larger—and we\nconclude that A is indeed better than B. In summary, the question\nthat a p-value asks can be stated as follows: Assuming the null\nhypothesis to be true, how likely is a test statistic to be at least as\nextreme as observed? Note that the p-value does not express\nwhether the null hypothesis is true. To make our decision\nabout whether or not to reject the null hypothesis, we typically\ndetermine a threshold—the significance level α, often set to 0.05—\nthat the p-value has to fall below. However, it has been argued that\na better practice involves reporting the p-value alongside the results\nwithout a pigeonholing of results into significant and non-significant\n(Wasserstein et al., 2019).\n3.2.1\nAlmost Stochastic Order\nDeep neural networks are known to be highly non-linear models (Li\net al., 2018), having their performance depend to a large extent on\nthe choice of hyperparameters, random seeds and other (stochastic)\nfactors (Bouthillier et al., 2021). This makes comparisons between\n3.2 statistical hypothesis testing\n85\nalgorithms more difficult, as illustrated by the motivating example\nbelow by Dror et al. (2019):\nExample 1 (Part-of-Speech tagging).\nConsider the results for Part-of-Seech-\ntagging given in the table on the right,\ntaken over 3898 and 1822 observations\nusing different hyperparameter configu-\nrations and random seeds, respectively.\nOptimizing with Adam (Kingma and Ba,\n2015) gives a higher average word-level\naccuracy than using RMSprop (Tiele-\nman and Hinton, 2012), however the\nmedian favors the latter. Furthermore,\nthe minimum across a few runs favor\nAdam, but the maximum is higher for\nRMSprop. So, which algorithm do we\nconsider to be better?\nAdam\nRMSprop\nMean\n.9224\n.9190\nStd. dev.\n.0604\n.0920\nMedian\n.9319\n.9349\nMin.\n.1746\n.1420\nMax.\n.9556\n.9573\nTherefore, Dror et al. (2019) propose almost stochastic order\n(ASO) for Deep Learning models based on the work by del Barrio\net al. (2018a).41\nIt is based on a relaxation of the concept of\nstochastic order by Lehmann (1955): A random variable xA is\ndefined to be stochastically larger than xB (denoted xA ⪰xB)\nif ∀x : F(x) ≤G(x), where F and G denote the cumulative\ndistribution functions (CDF) of the two random variables. The\nCDF is defined as F(t) = p(x ≤t), while the empirical CDF given\na sample {x1, . . . , xn} is defined as\nFn(t) = 1\nn\nn\nX\ni=1\n1\n\u0000xi ≤t\n\u0001\n,\nwith 1\n\u0000·\n\u0001\nbeing the indicator function. In practice, since we do\nnot know the real score distributions p(xA) and p(xB), we cannot\nuse the precise CDFs in subsequent calculations, and we rely on\nthe empirical CDFs FN and GM. A case of stochastic order is illus-\ntrated in Figure 3.3a, using the CDFs of two normal distributions.\nHowever, in cases such Figure 3.3b we would still like to declare\none of the algorithms superior, even though the stochastic order of\nthe underlying CDFs is partially violated. Several ways to quantify\nthe violation of stochastic dominance exist (Álvarez-Esteban et al.,\n2017; del Barrio et al., 2018b), but here we elaborate on the optimal\ntransport approach by del Barrio et al. (2018a). They propose a\nthe following expression quantifying the distance of each random\nvariables from being stochastically larger than the other:\n41Implementation details and pseudo-code are given in Appendix C.3.\n3.2 statistical hypothesis testing\n86\n(a) Stochastic order with red ⪰green. (b) Almost stochastic order, with blue\n≿green.\nFigure 3.3: Examples for stochastic order (a) and almost stochastic\norder (b), illustrated using the CDFs of two normal random variables.\nBecause stochastic order is too strict to be practical, almost stochastic\norder allows for some degree of violation of the order (gray area in (b)).\nεW2(F, G) =\nR\nVx(F −1(t) −G−1(t))2dt\n(W2(F, G))2\n,\n(3.3)\nwith the violation ratio εW2(F, G) ∈[0, 1] and a violation set\nVx =\n\b\nt ∈(0, 1) : F −1(t) < G−1(t)\n\t\n, i.e. where the stochastic\norder is being violated.\nEquation (3.3) contains the following\ncomponents: Firstly, the quantile functions F −1(t) and G−1(t)\nassociated with the corresponding CDFs:\nF −1(t) = inf\n\b\nx : t ≤F(x)\n\t\n,\nt ∈(0, 1).\nThe quantile functions allow us to define stochastic order via\nX ⪰Y ⇐⇒∀t ∈(0, 1) : F −1(t) ≥G−1(t). Secondly, it comprises\nthe univariate l2-Wasserstein distance:\nW2(F, G) =\nsZ 1\n0\n\u0000F −1(t) −G−1(t)\n\u00012dt,\n(3.4)\nwhich for univariate functions can be expressed through their\ninverse CDFs (De Angelis and Gray, 2021). Finally, del Barrio\net al. (2018a); Dror et al. (2019) define a hypothesis test based on\nthis quantity by formulating the following hypotheses:\nH0 : εW2(F, G) ≥τ\nH1 : εW2(F, G) < τ,\nfor a pre-defined threshold τ > 0, for instance 0.5 or lower (see\ndiscussion in Appendix B.1 about the choice of threshold). Fur-\nther, Álvarez-Esteban et al. (2017); Dror et al. (2019) produce a\nfrequentist upper bound to this quantity, defining the minimal εW2\nfor which we can reject the null hypothesis with a confidence of\n1 −α as\n3.2 statistical hypothesis testing\n87\nFigure 3.4: Plot of distributions used to empirically test the Type I and\nType II error of significance tests in Section 3.2.2.\nεmin(FN, GM, α) = εW2(FN, GM) −\nr\nN + M\nNM\nˆσN,MΦ−1(α). (3.5)\nThe variance term ˆσN,M is estimated using a bootstrapping\nestimator (as introduced in Section 2.1.1) for the variance, with F ∗\nN\nand G∗\nM denoting empirical CDFs based on sets of scores resampled\nfrom original sets of model scores, similar to re-sampling procedure\nin other tests like the bootstrap (Efron and Tibshirani, 1994) or\npermutation-randomization test (Noreen, 1989):\nˆσ2\nN,M = Var\n\u0014r\nNM\nN + M\n\u0000εW2(F ∗\nN, G∗\nM) −εW2(FN, GM)\n\u0001\u0015\n.\n(3.6)\nThus, if εmin(FN, GM, α) < τ, we can reject the null hypothesis\nand claim that algorithm A is better than B, with a growing\ndiscrepancy in performance the smaller the value becomes.\n3.2.2\nExperimental Comparison\nWe compare ASO to established significance tests such as the\nStudent’s-t, the bootstrap (Efron and Tibshirani, 1994), and the\npermutation-randomization test (Noreen, 1989), along with the\nWilcoxon signed-rank (Wilcoxon, 1992) and Mann-Whitney U test\n(Mann and Whitney, 1947) on different types of distributions, which\nare plotted in Figure 3.4. We plot the Type I error rate per 500\nsimulations for ASO and 1000 simulations for the other tests as\na function of sample size in Figure 3.5, where we sample both\nsets of observation from the same distribution. For Figure 3.5a,\nwe sample from N(0, 1.52) and try a bimodal normal mixture in\n3.2 statistical hypothesis testing\n88\n(a) Rates for normal samples.\n(b) Rates for normal mixture samples.\n(c) Rates for Laplace samples.\n(d) Rates for Rayleigh samples.\nFigure 3.5: Comparing type I error rates for different tests and distribu-\ntions as a function of sample size. Decisions are made using a confidence\nthreshold of α = 0.05 and τ = 0.2 for εmin.\nFigure 3.5b (using the same parameter for the second component,\nand N(−0.5, 0.252) with mixture weights π1 = 0.75 and π2 = 0.25).\nTo test the behavior of tests on non-normal distributions, we also\nsample from a Laplace(0, 1.52) distribution in Figure 3.5c, which\npossesses a different behavior around the main, as well as the\nRayleigh distribution with Rayleigh(1) in Figure 3.5d, which has\na heavy tail. We can see that ASO performs either en par or\nbetter than other tests in all scenarios, achieving lower error rates\nthe more samples are available, while other tests score around\nthe expected type I error of 5%. In Appendix B.1, Type II error\nexperiments reveal that the test produces comparatively higher\nerror rates for ASO, though. This can be explained by the fact that\nwe use the upper bound εmin instead of εW2 to evaluate the null\nhypothesis, which makes the test act more conservatively. We also\nfind in Appendix B.1 that a decision threshold of τ = 0.2 strikes\nan acceptable balance between Type I and II error rates across\ndifferent scenarios. Overall, we argue that a lower Type I error\nis more advantageous in the context of empirical research, and\nthat a decreasing error rate w.r.t. higher sample sizes constitutes\nan appealing property when used on arbitrary distributions. In\nthese experiments, the score distributions were determined a priori\n3.2 statistical hypothesis testing\n89\nin order to create rigid experimental conditions.\nNaturally, a\npractitioner would not know these distribution in a typical setting,\nwhich is why we illustrate the usage of the test in the next section.\n3.2.3\nCase study: Question-Answering with\nLarge Language Models\n(a) Setup for question-answering task.\n(b) Strategies to produce varying answers.\nFigure 3.6: Setup for the question-answering case study. In (a), we depict\nthe general task setup: Questions are given to an LLM, which produces\nanswers that are scored against reference answers using ROUGE-L.\nThe scores for every question-answer pair are compared against a pre-\ndefined threshold, which determines whether an answer is considered\ncorrect, and an accuracy score can be computed. (b) In order to produce\ndifferent answers for the same question, we can vary different factors,\nincluding the prompt format, the in-context demonstrations, generation\nhyperparameters, or all of these factors together.\nWe apply the ASO test to a very relevant problem in NLP:\nComparing the results from different LLMs, where models are\nalready trained and multiple different seeds are not available. Here,\nwe explore ways in which we can still enable statistical hypothesis\ntesting despite the more restrictive setup. While we do not quantify\nany uncertainty in model predictions here, introducing variability\nand employing hypothesis testing enables us to quantify uncertainty\nin model results, therefore aiding model selection.\nSetup.\nWe use three popular open-source models, namely\nMosaicAI MPT 7B (MosaicML NLP Team, 2023), Mistral 7B\n3.2 statistical hypothesis testing\n90\n(Jiang et al., 2023a), and OLMo 7B (Groeneveld et al., 2024),42\nand compare them on a closed-book question-answering task on\nTriviaQA (Joshi et al., 2017). The general task setup is shown\nin Figure 3.6a: Given a number of questions from the TriviaQA\ntest set, we obtain the LLM’s answers, which are scored against\nreference answers using ROUGE-L (Lin, 2004), which is a measure\nbased on n-gram overlap.\nIf the obtained score surpasses a\npre-defined threshold, we score an answer as correct. From this,\nwe obtain a single accuracy score for the whole test set. In each\ncase, we use their default generation methods set for the model on\nthe HuggingfaceHub and 10 other instances as in-context examples.\nThe goal is to show that even when we operate with monolithic\nmodels, we can still facilitate meaningful comparisons using statis-\ntical hypothesis testing. The default option usually consist of just\ncomparing the two accuracies (scalar comparison), however this\ndoes not take any uncertainty in the results into account. Instead,\nwe might compare the population of instance-level scores in Fig-\nure 3.6a before thresholding (instance-level comparison), or use a\nbootstrap estimator on the instance-level scores to obtain multiple\naccuracy scores, similar to our estimation of the probability of heads\nusing a sample of bootstrapped coin flips in Section 2.1.1 (boot-\nstrapping comparison). Another approach is to vary the factors\nthat produce an LLM’s answer, which are depicted in Figure 3.6b:\nWe can for instance change the prompt formatting (multi-prompt\ncomparison), change the in-context demonstrations by re-sampling\nthem from the training set for each inference (varying in-context\nsamples), or modify the hyperparameters that influence the mod-\nels generation (generation hyperparameters). Lastly, we can also\ncombine prompt formatting, varying in-context examples and gen-\neration hyperparameters by changing them jointly for every test\ninstance (mix). We briefly discuss each of these options in more\ndetail.\nScalar Comparison.\nWe first consider the potentially most\ncommon form of comparison, namely single scalars. For this pur-\npose, we compute the accuracy per model on the given test set of\nquestions. To judge whether a question has been answered cor-\nrectly, we use the same heuristic as employed by Kuhn et al. (2023),\nwhere we compute the ROUGE-L score (Lin, 2004) as implemented\nby the evaluate package43 between a given model answer and\n42More precisely, we use mosaicml/mpt-7b, mistral-community/Mistral-7B-\nv0.2, and allenai/OLMo-1.7-7B-hf.\n43See https://huggingface.co/docs/evaluate/index.\n3.2 statistical hypothesis testing\n91\ngold answer. When the resulting score surpasses a value of 0.3, an\nanswer is considered correct.\nInstance-level Comparison.\nInstead of aggregating the mea-\nsurements on all test instances into a single score, we can instead\nlook at them as a set of observations. This enables us to com-\npare larger populations of observations, as opposed to having only\none single observation per model. For question-answering, we use\nthe ROUGE-L scores, but without applying a threshold. A key\ndifference to the other tested approaches is that this comparison\nanswers a subtly different question about the models: Instead of\nconsidering which hypothesis class of model is better by evaluating\ndifferent model instances after training them with distinct ran-\ndom seeds, we instead ask which trained model instance tends to\ngive better-scored answers in general (as judged by the ROUGE-L\nheuristic).\nBootstrapping Comparison.\nIn Section 2.1.1, we discussed\nbootstrapping as a way to quantify the uncertainty about a quantity\nof interest. We can apply the same technique to the accuracy by\nbootstrapping samples of observations from the existing set of\nanswered questions, and computing the accuracy on these pseudo-\nsamples. These scores can then be used to compute the standard\nerror and to run them through the ASO test.\nMulti-prompt Comparison.\nLLMs can be very sensitive to\nthe chosen prompt format (Mizrahi et al., 2024; Sclar et al., 2023).\nTherefore, instead of evaluating predictions from models trained\nwith different random seeds, we can instead consider predictions\nfrom the same model but using different prompts, and treat the re-\nsulting accuracies as observations. Specifically, we test the following\nprompt templates:\n1. Q: {question} A:\n2. Question:\n{question} Answer:\n3. Take the following question:\n’{question}’.\nGive\nthe correct answer:\nVarying In-context Examples.\nVarious studies have pointed\nout the importance of in-context samples for task-specific model ca-\npabilities (Xie et al., 2022; Min et al., 2022; Hendel et al., 2023). For\nthis reason, we run four additional evaluations where we randomly\nsample a new set of in-context samples.\n3.2 statistical hypothesis testing\n92\nGeneration Hyperparameters.\nWe also consider generating\nanswers using different generation parameters. Specifically, we try\nthe default approach for the three models, greedy decoding, as well\nas Nucleus sampling (Holtzman et al., 2020) with p = 0.9, top-k\nsampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al.,\n2019) with k = 60 or beam search with three beams. Lastly, we\nalso combine this with multiple different prompts and different\nin-context samples, where we answer every question 5 times, each\ntime sampling a different prompt, generation configuration, and\nin-context demonstrations randomly.\nResults.\nAll accuracies for the different methods including stan-\ndard deviations are shown in Figure 3.7a, with an overview of all\nthe εmin values calculated by the ASO test in Figure 3.7b. Recall\nthat according to Section 3.2.1, we would declare one model supe-\nrior to another when εmin < τ, which empirically τ = 0.2 to provide\na good trade-off between Type I and Type II error. We can see\nthat the ordering of models largely agrees across settings, but can\nprovide subtle differences. All models usually generate through\ngreedy sampling. When using different generation hyperparame-\nters, we can observe a noticeable degradation in results, although\nthe OLMo model seems to be most robust to changes in generation\nparameters. Interestingly, the εmin values in Figure 3.7b show that\nall evaluations mostly agree in their result; however the comparison\nof instance-level scores seems to underestimate the degree of almost\nstochastic dominance (as shown through larger εmin values for the\nbest models). A noticeable exception for this agreement is the\nexperiment using different generation hyperparameters, where the\nsevere loss in performance renders none of the results significant.\nIn the end, the mixture of a random prompt template, generation\nhyperparameters and in-context examples seems to portray the\nclearest picture of the model rankings through the εmin values.\nFormalization.\nIn this case study, we discussed a number of\nways we can use to perform statistical hypothesis testing using\nLLMs, assuming access to an already trained model. All of these\nare subtly different in what the kinds of uncertainties they take\ninto account to compare models. To investigate the differences, we\nformalize the problem: Let x be shorthand for an input sequence,\ny for a generated sequence and ϕ a function mapping a generated\nsequence to an evaluation score (e.g. an indicator function decid-\ning whether an answer is correct). Further, let ρ be a prompt\ntemplate, γ a set of generation parameters, C a set of in-context\ndemonstrations and λ a set of training hyperparameters (including\narchitecture, optimizer, regularization, finetuning strategy etc.).\n3.2 statistical hypothesis testing\n93\nAccuracy\nModel\nScalar\nBootstrapping\nMulti-prompt\nGeneration\nIn-context\nMix\nMosaicAI MPT 7B\n.49\n.49 ± .00\n.51 ± .01\n.02 ± .02\n.51 ± .01\n.30 ± .02\nMistral 7B v0.2\n.37\n.37 ± .00\n.40 ± .04\n.15 ± .09\n.43 ± .04\n.33 ± .03\nOLMo 7B v1.7\n.51\n.51 ± .01\n.57 ± .04\n.17 ± .02\n.59 ± .03\n.40 ± .03\n(a) Evaluation results, given in accuracy. Best results are bolded, significant\ndifferences according to the ASO test are underlined. Shown are results\nfrom a scalar comparison (Scalar), bootstrapping instance-level observations\n(Bootstrapping), trying different prompt templates (Multi-prompt), generation\nhyperparameters (Generation), in-context demonstrations (In-context), or\nrandomly sampling a prompt, generation settings and in-context examples for\neach input (Mix). Instance-level results were only used to perform hypothesis\ntesting for the scalar results, and are therefore not included as a column.\n(b) εmin values comparing the LLMs using different sets of observations.\nFigure 3.7: Results for the case study. Given are (a) accuracy scores,\neither as single scalar or accuracy scores with confidence intervals as a\nresult of bootstrapping or using multiple-prompts. Further shown are\n(b) the εmin scores based on the instance-level observations, bootstrap-\nping observations as well using multiple prompts, different generation\nparameters, different in-context demonstrations or a combination of the\nlast tree (Mix).\n3.2 statistical hypothesis testing\n94\nWe are then interested in two quantities: Aggregate metrics such\nas accuracy, which we can formulate as the expected value of ϕ\nunder the model on a given dataset, and the expected accuracy\narising when varying all the other factors mentioned above, forming\nanother expectation:\nEp(γ,ρ,C)\nh\nEp(y|x,θ,ρ,γ)\n\u0002\nϕ(y)\n\u0003i\n=\nZ\n. . .\nZ\nϕ(y)\n|{z}\nScore\np(y | θ, x, ρ, γ, C)\n|\n{z\n}\nLLM Predictive Dist.\np(ρ)p(γ)p(C)\n|\n{z\n}\nGeneration Priors\np(θ | D, λ)p(λ)dy dθ dγ dρ dCdλ .\n(3.7)\nWe can use this to analyze all the test setups above by applying\nDirac delta functions (as previously used in Equation (2.33)) and\nMonte Carlo integration (see Equation (2.49)) to evaluate Equa-\ntion (3.7). For instance, the scalar comparison assume an single\nprompt ˆρ, set of generation parameters ˆγ, in-context samples ˆC\nand weights ˆθ and thus Equation (3.7) becomes\nEp(γ,ρ,C)\nh\nEp(y|x,θ,ρ,γ)\n\u0002\nϕ(y)\n\u0003i\n≈\nZ\nϕ(y)p(y | θ, x, ρ, γ, C)δ(ρ −ˆρ)δ(γ −ˆγ)δ(θ −ˆθ)δ(C −ˆC)dy\n=\nZ\nϕ(y)p(y | ˆθ, x, ˆρ, ˆγ, ˆC)dy.\n(3.8)\nThe same assumptions are also applied for the instance-level\ncomparison, with the difference that we only evaluate the outer\nexpectation in Equation (3.7).\nFurther, we can interpret the\nbootstrapping procedure as a different outer expectation in Equa-\ntion (3.7), where we instead evaluate the expectation over all\npossible samples (with replacement) of our original set of generated\nsequences. The conclusion we can draw from this is the following:\nTo evaluate the overall performance of a model, we would like\nto approximate Equation (3.7) as closely as possible, ideally by\nperforming a full ancestral sampling scheme. For LLMs, this is\nnot feasible, since we often have to work with a single, already\ntrained model. Bouthillier et al. (2021) have unveiled the perhaps\ncounter-intuitive intuition that increasing amount of randomness\nin our experiments actually helps to decrease the variance of our\nestimate of Equation (3.7). We follow this idea and vary as many\naspects as possible, which in this case study produces a clear rank-\ning of the robustness of a model. For language models, this implies\nrunning the model over the dataset multiple times, but sampling\ndifferent generation parameters and prompt templates like in our\nmix variant (as advocated for by Mizrahi et al., 2024). In cases\n3.2 statistical hypothesis testing\n95\nwhere running the model multiple times for each input might still\nbe prohibitively expensive, we can always fall back onto a bootstrap\nestimator.\n3.2.4\nDiscussion\nThe previous sections have demonstrated the advantages of the\nASO test in an neural network setting. Nevertheless, using these\ntechniques in practice comes with limitations as well, which the\nend user should be aware of. The first line of limits comes with\nASO itself.\nMultiple steps of the procedure require different\nkinds of approximations or properties that are only guaranteed\nto hold in the infinite-sample limit, e.g. the bootstrap estimator\nof the variance in Equation (3.6). Furthermore, significance tests\nin general are known to sometimes provide unreliable results\nwith small (Reimers and Gurevych, 2018) or very large sample\nsizes (Lin et al., 2013), are prone to misinterpretation (Gibson,\n2021; Greenland et al., 2016), and encourages binary significant\n/ non-significant thinking (Wasserstein et al., 2019; Sadeqi Azer\net al., 2020). Bayesian analysis (Kruschke, 2013; Benavoli et al.,\n2017; Gelman et al., 2021) is therefore an attractive alternative to\nstatistical hypothesis testing, where the user draws conclusions\nfrom posterior distributions over quantities of interest. A potential\ndrawback of this methodology is that it often comes at the cost of\nhaving to use Markov chain Monte Carlo methods, which require\nexperience from the user to validate convergence and defining\nappropriate models and model priors.\nFor the application to LLMs, Section 3.2.3 has demonstrated\nthat even with a fully trained model, we can still perform mean-\ningful statistical hypothesis testing by either using bootstrapping\nor by varying prompt templates, generation hyperparameters and\nin-context demonstrations. Some of these methods for model com-\nparison will now be used in the remaining chapters of this thesis.\n4\n|\nUncertainty in Text\nClassification\n“Two roads diverged in a yellow wood,\nAnd sorry I could not travel both\nAnd be one traveler, long I stood\nAnd looked down one as far as I\ncould\nTo where it bent in the undergrowth;\nAnd both that morning equally lay\nIn leaves no step had trodden black.\nOh, I kept the first for another day!\nYet knowing how way leads on to\nway,\nI doubted if I should ever come back.\nThen took the other, as just as fair,\nAnd having perhaps the better claim,\nBecause it was grassy and wanted\nwear;\nThough as for that the passing there\nHad worn them really about the same,\nI shall be telling this with a sigh\nSomewhere ages and ages hence:\nTwo roads diverged in a wood, and I—\nI took the one less traveled by,\nAnd that has made all the difference.”\n—The Road Not Taken by Robert Frost (1915).\nAssume we would like to automate the moderation of postings on\na social media platform. While it would be preferable to always\nuse human moderators, this is often not feasible due to the deluge\nof posts, and also not desirable due to the psychological impact\nthat the moderation of harmful content can have. After having\ntrained a classifier on some labeled training instances, we are ready\nto deploy. And while we expect a large number of the flagged cases\nto be clear positives, there will inadvertently be instances for which\nthe classifier struggles, for example sentences in which an toxic\nremark is quoted or lacks context. The sentence below was taken\nfrom the Wikitalk dataset (Wulczyn et al., 2017; Borkan et al.,\n2019), which includes discussions among Wikipedia editors:\n“I was responding to a post by AndyTheGrump at Talk: Commu-\nnist terrorism, section ‘Marxism is not the only ‘communism”,\nwhere he called me ‘idiot’ and then refused to retract his remark\nwhen I requested him to do so.”\nThe mention of “idiot” here might already set off the toxicity\nclassifier, even though the sentence just quotes another user’s\nremark. In this case, we might want to defer to the decision to\na human moderator when the classifier shows uncertainty. To\n96\nuncertainty in text classification\n97\nmake the task of moderation easier, we could also employ another\nsystem to label the spans of text that contain harmful speech.\nHere, we might only show the parts that the system is most\nuncertain about to limit the exposure to toxicity, and potentially\nask the moderator to label them in order to improve the training\ndata for future model updates. To illustrate this, let us look at\nanother (truncated) example from the dataset, labeling it in two\ndifferent ways (assuming simplified tokenization):\nI’d\nlike\nto\noffer\nyou\na\ngreat\nbig\nglass\nof\nshut-the-f∗@#-up\njuice\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n+\n−\nO\nO\nO\nO\nO\nB-TOX\nI-TOX\nI-TOX\nI-TOX\nI-TOX\nI-TOX\nI-TOX\nIn the first annotation, we focus on whether single words\ncould be considered toxic or not, while in the second annotation,\nwe capture an entire toxic span using BIO-tags (which indicate\nthe beginning, inside or outside of such a phrase).\nWhat we\noutlined above are instances of classic NLP task formats, namely\nsequence classification and sequence labeling, respectively.\nIn\nthe former we simply assign a label to an entire sequence,\nwhereas in the latter we label or classify parts of a sequence.\nIn this thesis, we will refer to both jointly as text classification.\nSequence labeling subsumes tasks such as part-of-speech tagging,44\nwhere the labels can e.g. be noun, verb or adverb, or named\nentity recognition, in which we identify named entities such\nas people, organization or locations.\nIn this work we will use\nthe terms label and class interchangeably to refer to a cate-\ngory from a set of categories that is assigned to (part of) an input.45\nHowever, quantifying the uncertainty in these decisions is chal-\nlenging. Uncertainty is not always present in predictions when we\nmight expect them, and might be present if it is unwarranted. This\nchapter aims to understand this behavior, both from a theoretical\nand empirical perspective. Therefore, we demonstrate some short-\ncomings of UQ with ReLU networks in the next section, before\nreturning to text classification in Section 4.2.\n44PoS tagging also is common preprocessing step for parser that produce parse\ntrees as the ones shown in Section 2.1.3.\n45Even though this chapter focuses on multi-class classification, there is a subtle\ndifference to multi-label classification: Multi-class means that we have multiple\nchoices, but only one of them will be considered correct at a time, which\nmakes sense when trying to choose from mutually exclusive options. In the\nmulti-label classes, we choose for each possible label whether it is applicable\nor not, allowing multiple labels to be correct at the same time. For instance,\nwhen classifying legal judgments according to which human right articles are\nbeing violated, we can find that each judgement can violate multiple articles\nat once (Chalkidis et al., 2019b).\n4.1 theoretical pitfalls in classification\n98\n4.1\nTheoretical Pitfalls in Classification\nThe following work is based on Ulmer and Cinà (2021).\n(a) Predictive entropy.\n(b) Polytopal regions.\n(c) Magnitude of predic-\ntive entropy gradient.\nFigure 4.1: Uncertainty and linear regions of a ReLU classifier trained\non example data. (a) Uncertainty measured by predictive entropy on\nsynthetic data, illustrated by increasing shades of purple, with white\ndenoting absolute certainty. (b) Polytopal, linear regions in the feature\nspace induced by the same classifier (as introduced by Arora et al., 2018,\nplotted using the code by Jordan et al., 2019). (c) Gradient norm of the\npredictive entropy plotted in shades of green—small perturbations in\nthe input have a decreasing influence on the uncertainty of the network\nas we stray away from the training data, creating large areas in which\nuncertainty levels are overgeneralized.\nIt is well-known that neural network classifiers tend to be\noverconfident in their predictions (Guo et al., 2017; see more\nrelated work in Section 2.2.1). In addition, they can exhibit high\nlevels of certainty when this is unwarranted, and often fail to\ncorrectly identify OOD samples (Snoek et al., 2019; Nalisnick\net al., 2019b). Ulmer et al. (2020) showed that even techniques\nspecifically developed to quantify the model’s uncertainty struggle\nat detecting OOD samples for a relatively simple classification\ntask. Crucially, it was shown that neural discriminators tend to\nproject vast areas of high certainty far away from the training\ndistribution—a behavior that seems completely at odds with\nreliable OOD detection.\nThese observations are replicated\nin Figure 4.1: In Figure 4.1a, we can see that the predictive\nentropy of a ReLU classifier displays low uncertainty in large\nregions behind the observed data clusters. As Arora et al. (2018)\nshowed, ReLU classifiers induce polytopal linear regions in the\nfeature space shown in Figure 4.1b, which was used by the\nprevious work of (Hein et al., 2019) to show that the network’s\nconfidence is an unsuitable measure of uncertainty to detect OOD\ninputs. However, the reasons for this behavior in a classification\n4.1 theoretical pitfalls in classification\n99\nsetting are less studied, and thus we study this behavior on addi-\ntional uncertainty metrics such as predictive entropy in Figure 4.1c.\nIn this chapter, we present a theoretical argument to explain\nsuch phenomena, showing that certainty levels are generalized on\nsub-spaces defined by the network (see Figures 4.1b and 4.1c).\nWe do this by simulating covariate shift for single feature values\nof real variables and studying the asymptotic behavior of the\nmodel. Our first result shows that, under mild assumptions about\nthe network’s behavior on certain subspaces, ReLU-based neural\nnetwork classifiers coupled with widely used uncertainty metrics\nalways converge to a fixed uncertainty level on OOD samples.\nWe extend this result by proving that variational inference-based\nand ensembling methods in combination with several uncertainty\nestimation techniques suffer from the same problem (Theorem 1).\nThis phenomenon is illustrated and discussed on synthetic data.\nThese results entail that, when the conditions of the theorem are\nmet, these models cannot be used to reliably detect OOD: since\nthe level of certainty is generalized from seen to unseen data, the\nmodels are unable to differentiate between the two. The findings\nof this chapter have bearings on OOD detection for several critical\napplications using neural classifiers with ReLU activation functions,\nand I will also discuss the impact on the following experiments for\nNLP.\n4.1.1\nPreliminaries\nWe first introduce some relevant definitions for the rest of this\nchapter.\nOut-of-distribution Data.\nA Although there exist many differ-\nent notions of dataset shift (Shimodaira, 2000; Moreno-Torres et al.,\n2012; Hupkes et al., 2023), we particularly focus on covariate shift,\nin which the distribution of feature values—the covariates—differs\nfrom the original training distribution p(x). We focus on this kind\nof shift as it is especially common in non-stationary environments\nlike healthcare (Curth et al., 2019), where distributional drifts over\ntime are very common. To simulate covariate shift, we obtain OOD\nsamples by shifting points away from the training distribution by\nmeans of a scaling factor. This approach is in line with recent ex-\nperiments on covariate shift and OOD detection (Snoek et al., 2019;\nUlmer et al., 2020). We would expect a reliable OOD detection\nmodel to display increasing uncertainty as points stray further and\nfurther away from the mass of p(x), thus we study the behavior\n4.1 theoretical pitfalls in classification\n100\nof OOD detection models in the limit, when the scaling factor is\nallowed to grow indefinitely in at least one dimension.\nUncertainty Metrics.\nWe begin by first defining a neural dis-\ncriminator in the form of a ReLU classifier, which we assume to\nfollow common architectural conventions. Thus, it consist of a\nseries of affine transformations with ReLU (Glorot et al., 2011) acti-\nvation functions, defined by ReLU(x) = max(0, x). Together with\na final softmax function (Bridle, 1990) as defined in Equation (0.2),\nit parameterizes a categorical distribution over classes.46\nDefinition 5 (ReLU Classifier). Let x ∈RD be an input vector\nand K the number of classes in a classification problem. The\nunnormalized output of the network after L layers is a function\nfθ : RD →RK with the final output following after an additional\nsoftmax function ¯σ(·) s.t. Pθ = ¯σ◦fθ, so Pθ(y = k | x) ≡¯σ(fθ(x))k.\nThus, the discriminator is represented by a function Pθ : RD →\n[0, 1]K, which is parametrized by a vector θ.\nWe will consider a set of popular uncertainty metrics, which\nwe introduced in Sections 2.2.1 and 2.2.2 and restate them here.\nFirstly, Hendrycks and Gimpel (2017) introduce a simple baseline,\nwhich is the highest probability observed for any class, also referred\nto as confidence:\nˆp = max\nk∈[K] Pθ(y = k | x).\n(4.1)\nIdeally, the model’s predictive distribution would become more\nuniform for challenging inputs (e.g. in areas of class overlap) and\nthus produce a lower confidence score ˆp, which is why we measure\nuncertainty by 1 −ˆp. Another approach lies in measuring the\nShannon entropy H of the predictive distribution:\nH\n\u0002\nPθ(y | x)\n\u0003\n= −\nK\nX\nk=1\nPθ(y = k | x) log Pθ(y = k | x).\n(4.2)\nThe entropy here is minimal when all probability mass is cen-\ntered on a single class, and maximal when the predictive distri-\nbution is uniform. The other uncertainty estimation techniques\nare based on the idea of Bayesian deep learning, where, the more\npredictions between different parameter sets disagree, the larger the\nuncertainty. One straightforward way to measure this disagreement\n46The following proofs also hold for binary classifiers which are parameterized\nthrough a sigmoid function. For the connection between the softmax and\nsigmoid function, refer to Appendix A.7.\n4.1 theoretical pitfalls in classification\n101\nis the average variance of the predicted probability per class, as\ndone in Smith and Gal (2018):\n¯σ2 = 1\nK\nK\nX\nK=1\nEp(θ|D)\n\u0002\nPθ(y = k | x)2\u0003\n−Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u00032.\n(4.3)\nMaximum softmax and predictive entropy only capture the\ntotal uncertainty, and while the class variance aims to quantify\nmodel uncertainty, it does so rather heuristically. Thus, we also\nconsider the mutual information between model parameters and\na data sample (Depeweg et al., 2018; Smith and Gal, 2018) as a\nmore theoretically-motivated measure of epistemic uncertainty:\nI\n\u0002\ny, θ\n\f\f D, x\n\u0003\n|\n{z\n}\nModel uncertainty\n= H\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\n|\n{z\n}\nTotal uncertainty\n−Ep(θ|D)\nh\nH\n\u0002\nPθ(y | x)\n\u0003i\n|\n{z\n}\nData uncertainty\n.\n(4.4)\nThe term itself can be interpreted as the gain in information\nabout the ideal model parameters and correct label upon receiving\nan input. If we can only gain a little, that implies that parameters\nare already well-specified and that the epistemic uncertainty is low.\nEspecially when an input is OOD, we therefore expect this metric\nto display high uncertainty.\n4.1.2\nMonotonicity & Polytopes\nBefore developing the main results, we introduce some concepts\nthat will become central to the proofs in the next sections. This\nincludes the definition of unbounded polytopes on which the model\nbehaves linearly, and the monotonicity of multivariate functions,\nwhich lets us make statements about the output of the network\nwhen scaling its input.\nIn the univariate case, we call a function strictly increasing on\nan interval I = [a, b] with a < b and a, b ∈R if its derivative is\nstrictly positive on the whole interval:\n∀x′ ∈I :\n∂\n∂xf(x)\n\f\f\nx=x′ > 0,\n(4.5)\nwhere ·|x=x′ refers to evaluating the value of the derivative of f at\nx′. This definition can also be extended to multivariate functions\nby requiring strict monotonicity (strictly increasing or decreasing)\nin all dimensions:\n4.1 theoretical pitfalls in classification\n102\nDefinition 6 (Monotonicity in Multivariate Functions). We call a\nmultivariate function f : RD →R strictly monotonic on a subspace\nP ⊆RD if it holds that the function is either strictly increasing or\ndecreasing in every direction:\n∀d ∈[D] :\n∀x′ ∈P :\n\u0000∇xf(x)\n\f\f\nx=x′\n\u0001\nd < 0\nor\n∀x′ ∈P :\n\u0000∇xf(x)\n\f\f\nx=x′\n\u0001\nd > 0,\n(4.6)\nwhere (·)d refers to ∂f(xd)\n∂xd |xd=x′\nd, i.e. the d-th component of the\ngradient ∇xf(x) evaluated at x′. We call a multivariate function\nf : RD →RK component-wise strictly monotonic if the above defi-\nnition holds for the gradient of every output component ∇xf(x)k.\nWe note here that the softmax function, whose probabilistic\noutput is used for the discussed uncertainty metrics, is an example\nfor a component-wise strictly monotonic function. As later lemmas\ninvestigate the behavior of functions in the limit, it is furthermore\nuseful to define regions of the feature space that are unbounded\nin at least one direction. We call a partially-unbounded polytope\n(henceforth abbreviated by PUP) a convex subspace of RD that\nis unbounded in at least one dimension d, i.e. if the polytope’s\nprojection onto d is either left-bounded by −∞or right-bounded\nby ∞, or both.\n4.1.3\nConvergence of Predictions on OOD Data\nLemma 1\nLemma 2\nProposition 2\nProof of Hein et al.\n(2019)\nProposition 1\nConvergence of gradient\nLemma 4\nExpectation of softmax\nprobability.\nLemma 5\nSoftmax variance\nLemma 6\nPredictive entropy\nLemma 7\nMutual information\nTheorem 1\nConvergence of\nuncertainty.\nLemma 3\nMax. softmax probability\nFigure 4.2: Dependencies between theoretical results. Information in\nparentheses denotes the section in the document.\nIn this section we will show that, moving the input to the\nextremes of the feature space, a ReLU classifier will converge\nto a fixed prediction. To demonstrate this, we must establish\nhow the distance from the training data affects the network’s\nlogits. To this end, we utilize a known result stating that neural\nnetworks employing piece-wise linear activation functions partition\nthe input space into polytopes (such as in Figure 4.1b; Arora\n4.1 theoretical pitfalls in classification\n103\net al., 2018). Given the saturating nature of the softmax, we\nconclude in Proposition 1 that even for extreme feature values in\nthe limit, the output distribution of the model will not change\nanymore. In order to help the reader untangle the interdependence\nof upcoming results, we provide a flow chart in Figure 4.2. We first\ndescribe how to re-write a ReLU network—or any other network\nwith piece-wise linear activation functions—as a piece-wise affine\ntransformation, borrowing from Croce and Hein (2018) and Hein\net al. (2019). We start with the common form of fθ as a series of\naffine transformations, interleaved with ReLU activation functions,\nwhich we will denote by ϕ:\nfθ(x) = WL ϕ\n\u0000WL−1 ϕ\n\u0000. . . ϕ\n\u0000W1 x + b1\n\u0001\n. . .\n\u0001\n+ bL−1\n\u0001\n+ bL .\n(4.7)\nIn the following, let f l\nθ(x) denote the output of layer l before\napplying an activation function. We now define a layer-specific\ndiagonal matrix Φl ∈Rnl×nl in the following way, where nl denotes\nthe hidden units in layer l:\nΦl(x) =\n\n\n1\n\u0000f l\nθ(x)1 > 0\n\u0001\n· · ·\n0\n...\n...\n...\n0\n· · ·\n1\n\u0000f l\nθ(x)nl > 0\n\u0001\n\n.\n(4.8)\nThis allows us to rewrite Equation (4.7) by replacing the usage\nof ϕ with a matrix multiplication using Φl:\nfθ(x) = WL ΦL−1(x)\n\u0000WL−1 ΦL−2(x)\n\u0000. . . Φ1(x)\n\u0000W1 x + b1\n\u0001\n. . .\n\u0001\n+ bL−1\n\u0001\n+ bL .\n(4.9)\nWe can now distribute the matrix products inside-out, we which\ndemonstrate below using a three-layer network:\nfθ(x) = W3 Φ2(x)\n\u0000W2 Φ1(x)\n\u0000W1 x + b1) + b2) + b3\n(4.10)\n= W3 Φ2(x)\n\u0000W2 Φ1(x) W1 x + W2 Φ1(x) b1) + b2) + b3\n(4.11)\n= W3 Φ2(x) W2 Φ1(x) W1\n|\n{z\n}\n= V(x)\nx\n+ W3 Φ2(x) W2 Φ1(x) b1 + W3 Φ2(x) b2 + b3\n|\n{z\n}\n= a(x)\n.\n(4.12)\nThis result lets rewrite the network as a single affine transfor-\nmation fθ(x) = V(x) x + a(x) with\n4.1 theoretical pitfalls in classification\n104\nV(x) = WL\n\u0012 L−1\nY\nl=1\nΦl(x) WL−l\n\u0013\n(4.13)\na(x) = bL +\nL−1\nX\nl=1\n\u0012 L−l\nY\nl′=1\nWL+1−l′ ΦL−l′(x)\n\u0013\nbl .\n(4.14)\nNote that the definition of V(x) corresponds to the Jacobian of\nfθ(x), meaning that vkd = ∂fθ(x)k\n∂xd\n. This is very useful, as it allows\nus to quickly check whether a network fθ is component-wise strictly\nmonotonic by checking V(x) for entries containing zeros. As Hein\net al. (2019) show, this formulation can also be used to characterize\na set of polytopes Q = {Q1, . . . , QM} induced by fθ and that within\neach polytope, the function has a unique representation as an affine\ntransformation. For this reason, we drop the dependence of V and\na on x when we refer to a specific polytope. Such polytopes are\nconstructed by first retrieving the half-spaces induced by each of\nthe network’s neurons and then intersecting all said half-spaces to\ngenerate convex regions or polytopes.47 We are especially interested\nin polytopes that are unbounded in at least one direction. The\nresults of Croce and Hein (2018) and Hein et al. (2019) show that\nthere is a finite number of polytopes corresponding to the given\nnetwork, and their Lemma 3.1 proves the existence of at least one\nunbounded polytope. Furthermore, under a mild condition on V,\nwe can ascertain that fθ will be component-wise strictly monotonic\non any polytope.\nLemma 1. Suppose fθ is a ReLU network according to Definition\n5. Then fθ is a component-wise strictly monotonic function on\nevery of its polytopes Q ∈Q, as long as its corresponding matrix\nV has no zero entries.\nProof. Let Q be one such polytope. As discussed, when restricted\nto Q, the network corresponds to an affine transformation fθ(x) =\nV x + a with V ∈RK×D and a ∈RK. fθ(x)k thus corresponds to\nthe dot product of the k-th row of V and x plus the k-th element\nof a. It follows that the partial derivative of fθ(x)k with respect\nto a dimension d equals the element vkd in V. This entails that, if\nvkd ̸= 0, at any point x ∈Q the gradient will be always positive or\nalways negative.\n47We refer the reader to Appendix A.8 or Hein et al. (2019) for details on the\nconstruction, since it is not central to our reasoning.\n4.1 theoretical pitfalls in classification\n105\nWe note here that the component-wise strict monotonicity of fθ\nand softmax do not entail the same property for Pθ.48 Nonetheless,\nthe monotonic behavior of fθ is sufficient to drive the logits to plus\nor minus infinity in the limit, a phenomenon that constrains the\noutput of pθ as we scale a data sample away from training data.\nWe begin our investigation of behavior in the limit by showing\nthat if we scale a vector only in a single dimension, we eventually\nalways remain within a unique PUP.\nLemma 2. Let x′ ∈RD and Q = {Q1, . . . , QM} be the finite set\nof polytopes generated by a network fθ. Let α ∈RD be a vector\ns.t. ∀d′ ̸= d, αd′ = 1. There exist a value β > 0 and m ∈1, . . . , M\nsuch that for all αd > β, the product x′ ◦α lies within Qm.\nProof. The proof mirrors the proof of Lemma 3.1 in Hein et al.\n(2019), so we only provide the intuition. By contradiction, suppose\nthat there is no unique polytope and thus the point x′ ◦α must\ntraverse different polytopes as we scale up αd. Since there are\nfinitely many polytopes, eventually the same polytope Qm will\nhave to be traversed twice. Since the polytopes are convex, all the\npoints on the line connecting the locations of where the boundary\nof Qm was crossed the first and second time must lie within Qm,\nbut this contradicts the fact that the scaled point traverses different\npolytopes.\nFrom here onward, we adapt the following shorthand to simplify\nnotation: Given a scaling vector α ∈RD s.t. ∀d′ ̸= d, αd′ = 1, we\nuse P(x′, d) to denote the PUP that x′ lands in when scaling it with\nαd in the limit. This definition implies that we can only scale paral-\nlel to the basis vectors and not arbitrary directions (for a discussion\non how restrictive this is, see Section 4.1.5). Finally, in the next\nlemma we establish that the output distribution converges to a fixed\npoint using the l2-norm of the gradient ∇xPθ(y = k | x). Generally,\nin regions of the feature space where the classifier predicts the same\nprobability distribution over classes, small perturbations in the\ninput x will not change the prediction. Therefore, the gradient in\nthese regions w.r.t. the input will be small and potentially even\ncorrespond to the zero vector, with a norm of (or close to) zero.\nProposition 1 (Convergence of predictions in the limit). Suppose\nthat fθ is a ReLU-network. Let x′ ∈RD, suppose α is a scaling\n48To see a counterexample, the reader can check that even assuming component-\nwise strict monotonicity for fθ, if the matrix V associated to fθ on a specific\npolytope has a column d filled with the same value a, then the resulting pθ will\nhave a gradient of 0 at dimension d, regardless of what class we are considering.\nThis is because the partial derivatives of the softmax, when all multiplied by\nthe same constant a, add up to zero.\n4.1 theoretical pitfalls in classification\n106\nvector and that the associated PUP P(x′, d) has a corresponding\nmatrix V with no zero entries. Then it holds that\n∀k ∈[K] :\nlim\nαd→∞\n\f\f\f\f∇xPθ(y = k | x)\n\f\f\nx=α ◦x′\n\f\f\f\f\n2 = 0.\n(4.15)\nThe whole proof can be found in Appendix A.9, so we present\nthe main intuitions here. Because of Lemma 2, we know the scaled\npoint α ◦x′ will end up in a unique PUP. The assumption on fθ\nthen triggers Lemma 1, from which we can infer that scaling the\ninput in a single dimension leads all logits to ±∞. Because of the\nsaturating property of the softmax, this will in turn provoke the\noutput of pθ to converge to a fixed point. As an aside, we recast\nTheorem 3.1 by Hein et al. (2019) in our framework, showing that\nthe model becomes increasingly certain in a single class, placing all\nits probability mass on it in the limit. The proof of this additional\nproposition is in Appendix A.10.\nProposition 2. Let fθ be ReLU network. Let x′ ∈RD, suppose\nα is a scaling vector and that the associated PUP P(x′, d) has a\ncorresponding matrix V with no zero entries. Assume the d-th\ncolumn of V has no duplicate entries. Then there exists a class k\nsuch that\nlim\nαd→∞¯σ(fθ(α ◦x′))k = 1.\nIn conclusion, we have shown in this section that the output\nprobabilities of ReLU networks are less and less sensitive to small\nperturbations of the input in the limit and, under the assumptions\nof Proposition 2, will converge to favor a single class with very high\nconfidence. In the next section we prove that all other uncertainty\nmetrics also converge to fixed values in the limit.\n4.1.4\nConvergence of Uncertainty Metrics on\nOOD Data\nIn Proposition 1, we have established how the prediction of a model\nconverges to a fixed point when feature values become extreme.\nWe now show a similar property about the uncertainty estimation\ntechniques introduced in Section 4.1.1. The fact that this the same\npathologies appear for more complex metrics is not immediately\nobvious, and one might assume that we can curb the deficiency of\nthe simple confidence score by using more sophisticated metrics\nand Bayesian deep learning techniques. To this end, we have to\nestablish how the predictions coming from multiple model instances\n4.1 theoretical pitfalls in classification\n107\ninteract, a point we analyze in Lemma 4. Then, we demonstrate\nhow the uncertainty metrics also converge to a fixed value in the\nlimit by proving the case for each of them in turn, before bundling\nour results in Theorem 1. We start with the easiest metric, which\nalso applies to a single ReLU network.\nLemma 3 (Maximum softmax probability). Suppose that fθ is a\nReLU network. Let x′ ∈RD, suppose α is a scaling vector and that\nthe associated PUP P(x′, d) has a corresponding matrix V with no\nzero entries. Then\nlim\nαd→∞\n\f\f\f\f∇x max\nk∈[K] Pθ(y = k | x)\n\f\f\nx=α ◦x′\n\f\f\f\f\n2 = 0.\nProof. The gradient of the max function will be a specific ∇xPθ(y =\nk | x), which reduces this to the case already proven in Proposi-\ntion 1.\nNote that for this metric, the combination with Proposition 2\nshows that the model is fully confident in a single class in the limit.\nFor our following lemmas, we consider uncertainty scores that are\nbased on multiple instances, e.g. different ensemble members or\nforward passes using re-sampled dropout masks. What all of these\napproaches have in common is that for every b in 1, . . . , B, the\nnetwork parameters θ(b) will differ, and thus also the polytopal\ntesselation of the feature space. Hence, we have to adjust our\nassumptions accordingly. For every instance b, let us denote the\naffine function on a polytope Q(b) as f (b)\nθ (x) = V(b) x + a(b). In\norder for our previous strategy to hold, we now assume for all\nb ∈[B] that P(b)(x′, d) has a matrix V(b) which does not have any\nzero entries. Note that even though this assumption has to hold\nfor all b, this does not mean that the matrices have to be identical.\nLemma 4 (Convergence of aggregated predictions in the limit).\nSuppose that f (1)\nθ , . . . , f (K)\nθ\nare ReLU networks. Let x′ ∈RD, sup-\npose α is a scaling vector and that for all k, the associated PUP\nP(k)(x′, d) has a corresponding matrix V(k) with no zero entries.\nThen\nlim\nαd→∞\n\f\f\f\f∇x Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\f\f\nx=α ◦x′\n\f\f\f\f\n2 = 0.\nThe full proof of this lemma can be found in Appendix A.11.\nThe analogous lemmas for the remaining uncertainty metrics—\npredictive entropy, class variance and mutual information—are\nstated and proved in Appendices A.12 to A.14. The proof strategy\nfor all further metrics is to simplify and reduce the uncertainty\nmetrics such that Lemma 4 or Proposition 1 can be applied. All of\nthese results combined now pave the way for our central theorem.\n4.1 theoretical pitfalls in classification\n108\nTheorem 1 (Convergence of uncertainty level in the limit). Sup-\npose that f (1)\nθ , . . . , f (B)\nθ\nare ReLU networks. Let x′ ∈RD, suppose α\nis a scaling vector and that for all b, the associated PUP P(b)(x′, d)\nhas a corresponding matrix V(b) with no zero entries. Then, when-\never uncertainty is measured via either of the following metrics\n1. Maximum softmax probability in Equation (4.1);\n2. Predictive entropy in Equation (4.2);\n3. Class variance in Equation (4.3);\n4. Approximate mutual information in Equation (4.4);\nthe network(s) will converge to fixed uncertainty scores for x′ ◦α\nin the limit of αd →∞.\nProof. The four parts of the theorem are proven separately by\nLemmas 3 and 7 to 9 and Appendix A.12 in Appendix A.\nWhat follows from this result is that methods based on multiple\ninstances of ReLU classifiers will suffer from the aforementioned\nproblem as long as uncertainty is estimated with one of the tech-\nniques listed above. Next we demonstrate how these assumptions\nand results apply on synthetic data.\n4.1.5\nSynthetic Data Experiments\nTo illustrate our findings, we plot the uncertainty surfaces and the\ngradient magnitudes of different models and uncertainty metric\npairings on the half moons dataset, which we generate using the\ncorresponding function in the scikit-learn package (Pedregosa\net al., 2011). Detailed information about the procedure can be\nfound in Appendix C.4.2 along with additional plots.\nFor a single network, we can observe in Figure 4.3a that there\nexist vast open-ended regions of stable confidence, confirming the\nfindings of Theorem 1. However, in the right part of Figure 4.3a\nwe can observe green regions with high gradient magnitude which\ndo not seem to comply with our findings. In this case, we can\nsee that these regions follow the decision boundaries. Due to the\nexponential function in the softmax, it is intuitive that small\nperturbation in these areas would have a large impact on the\nuncertainty score, resulting in a high gradient magnitude. But\nwhy does the magnitude not decrease in the limit as predicted\nby Theorem 1? We formulated our scaling vector α in way that\nonly allows scaling along one of the coordinate axes. Therefore, if\n4.1 theoretical pitfalls in classification\n109\n(a) Neural discriminator with max-\nimum probability (Hendrycks and\nGimpel, 2017).\n(b) MC Dropout (Gal and Ghahra-\nmani, 2016b) with mutual information\n(Smith and Gal, 2018).\n(c)\nNeural\nensemble\n(Lakshmi-\nnarayanan et al., 2017) with class\nvariance.\n(d) Anchored ensemble (Pearce et al.,\n2020) with mutual information (Smith\nand Gal, 2018).\nFigure 4.3: Uncertainty on the half-moon dataset, including the\nbinary classification AUROC. (Left plots) The uncertainty surface\nis represented with increasingly darker shades of purple, with white\nbeing the lowest uncertainty. Open-ended regions of static certainty\nappear across different models and metrics, and are extrapolated\nto unseen data (see Figures 4.3a to 4.3d); this phenomenon is less\napparent in some instances (Figure 4.3d). (Right plots) Increasing\nshades of green indicate the magnitude of the gradient of the uncer-\ntainty score w.r.t. the input. All metrics show open ended regions\nwhere the magnitude approaches zero.\nthe decision boundaries are not parallel to the axes, by scaling\nwe eventually escape the green areas and arrive at an area with\ngradient of magnitude zero. If the green regions were parallel\nto the axes then this would result in a violation of our main\nassumption.\nTraversing the input space parallel to a decision\nboundary in direction d will not influence the prediction within\nthe polytope, meaning that there will be entries vcd = 0.49\nTurning to predictions aggregated from multiple network\ninstances in Figures 4.3b to 4.3d, we again observe large regions of\nconstant uncertainty. The high-confidence region in the plots using\nmutual information (Figure 4.3d) displays a different behavior from\nthe others. As this metric aims to isolate epistemic uncertainty, it\nmakes sense that uncertainty would be lowest around the training\n49A decision boundary in a polytope is not the only way in which this assumption\ncan be broken, but it still appears to hold reasonably often. For instance, just\naround 6.3% of plotted points in Figure 4.1 possess a matrix V with at least\none zero entry—all located in the PUP in the top right corner.\n4.2 uncertainty & calibration in low-resource nlp\n110\ndata, i.e. where the model is best specified. The character of the\ngreen regions in the bottom part of Figure 4.3c and Figure 4.3d\ncan again be explained by decision boundaries: In these cases, we\nhave multiple instances with parameters θ(k), all with their own\npolytopal structure. When they overlap, the regions of the feature\nspace where the assumption of our theorem is violated can either\ngrow (Figure 4.3c) or shrink (Figure 4.3d), depending on the\ndiversity among instances. The fact that the anchored ensemble in\nFigure 4.3d does not exhibit such uniform regions of uncertainty\nlike the vanilla ensemble could be explained by the fact that its\ntraining procedure encourages diversification between members.\nThe difference between MC Dropout and ensemble models can be\nelucidated using recent insights that variational methods tend to\nonly explore a single mode of the weight posterior p(θ | D), while\nensemble members often spread across multiple modes (Wilson\nand Izmailov, 2020).\nOverall, we have seen that our theorem can explain why an\novergeneralization of uncertainty scores beyond the training data\nresults in failure in OOD detection. We also explored the cases in\nwhich our assumptions are violated, i.e. by multiple, diverse model\ninstances. In such scenarios, identification of OOD samples could\nin theory succeed, but often fails to do so reliably, see e.g. Snoek\net al. (2019); Ulmer et al. (2020). These insights can also help\nexplain many other empirical findings in this regard on a variety\nof real-world datasets, e.g. Smith and Gal (2018); Kompa et al.\n(2021).\n4.2\nUncertainty & Calibration in\nLow-Resource NLP\nThe following work is based on Ulmer et al. (2022b).\nThe previous section looked at a somewhat simplified setting\nusing ReLU networks.\nIn practice, most contemporary NLP\narchitectures are based on much more complex architectures like\nthe transformer (Vaswani et al., 2017). Theoretical arguments\nlike Theorem 1 then become harder, since making monotonicity\narguments with model components such as multi-head attention\nis not trivial. Additionally, the proof strategy of scaling a single\nfeature value into the limit is not applicable, because the input\nchanges from single feature vectors to a series of subword token em-\nbeddings. For this reason, we turn to an empirical approach instead.\n4.2 uncertainty & calibration in low-resource nlp\n111\nWhile there exist many works on images (Lakshminarayanan\net al., 2017; Snoek et al., 2019) and tabular data (Ruhe et al., 2019;\nUlmer et al., 2020; Malinin et al., 2021), the quality of uncertainty\nestimates provided by neural networks remains underexplored in\nNLP. In addition, as model underspecification due to insufficient\ndata presents a risk (D’Amour et al., 2022), the increasing interest\nin less-researched languages with limited resources raises the ques-\ntion of how reliably uncertain predictions can be identified. This\nmotivates the following research questions:\n1. What are the best approaches in terms of uncertainty quality\nand calibration?\n2. How are models impacted by the amount of available training\ndata?\n3. What are differences in how the different approaches estimate\nuncertainty?\nContributions.\nWe address these questions by conducting a\ncomprehensive empirical study of eight different models for uncer-\ntainty estimation for classification and evaluate their effectiveness\non three languages spanning distinct NLP tasks, involving sequence\nlabeling and classification. We show that while approaches based\non pre-trained models and ensembles achieve the best results over-\nall, the quality of uncertainty estimates on OOD data can become\nworse using more data. In an analysis on an instance-level, we also\ndiscover that a model’s total uncertainty seems to mostly consist\nof its data uncertainty.\n4.2.1\nMethodology\nModels.\nWe choose a variety of models that cover a range of\ndifferent approaches based on the two most prominently used\narchitectures in NLP: Long-short term memory networks (LSTMs;\nHochreiter and Schmidhuber, 1997) and transformers (Vaswani\net al., 2017). Inside the first family, we use the variational LSTM\n(Gal and Ghahramani, 2016a) based on MC dropout (Gal and\nGhahramani, 2016b), the Bayesian LSTM (Fortunato et al., 2017)\nimplementing Bayes-by-backprop (Blundell et al., 2015) and the\nST-τ LSTM (Wang et al., 2021a), modeling transitions in a finite-\nstate automaton, as well as an ensemble (Lakshminarayanan et al.,\n2017). In the second family, we count the variational transformer\n(Xiao et al., 2020), also using MC dropout, the SNGP transformer\n(Liu et al., 2023), using a Gaussian Process output layer, and the\ndeep deterministic uncertainty transformer (DDU; Mukhoti et al.,\n4.2 uncertainty & calibration in low-resource nlp\n112\n2021), fitting a Gaussian mixture model on extracted features. We\nelaborate on implementation details in Appendix C.6.\nUncertainty Metrics.\nWe test the same metrics as introduced\nin Section 4.1.1, but add a few additional ones. One of them is the\nsoftmax gap (Tagasovska and Lopez-Paz, 2019), i.e. the difference\nbetween the two largest probabilities of the classifier’s output\ndistribution. As another metric, we consider the Dempster-Shafer\nmetric (Sensoy et al., 2018), defined as K/(K + PK\nk=1 exp(zk)),\nwhere zk denotes the logit corresponding to class k. Since this\nmetric considers logits, it might be able to avoid the saturation on\nOOD shown by Hein et al. (2019) or in Section 4.1.4. While all\nmetrics so far can be mixed and matched with all the tested models,\nthere are also a few model-specific metrics. For instance, the DDU\ntransformer by Mukhoti et al. (2021) uses the log-probability of\nthe last layer network activation under a Gaussian mixture model\nfitted on the training set as an additional metric. Since all others\nmodels are trained or fine-tuned as classifiers, they cannot assign\nlog-probabilities to sequences. Lastly, since some tasks require\npredictions for every time step of a sequence, we determine the\nuncertainty of a whole sequence in these cases by taking the mean\nover all step-wise uncertainties.50 A more principled approach for\nsequences is for instance provided by Malinin and Gales (2021) in\nthe context of NLG, and we leave the extension and exploration of\nsuch methods for different uncertainty metrics, models and tasks\nto future work.\n4.2.2\nDataset Selection & Creation\nLang.\nTask\nDataset\nOOD Test Set\n# ID / OOD\nTraining Sizes\nEN\nIntent\nClassification\nClinc Plus\n(Larson et al., 2019)\nOut-of-scope\nvoice commands\n15k/1k\n15k/12.5k/10k\nDA\nNamed Entity\nRecognition\nDan+ News\n(Plank et al., 2020)\nTweets\n4382/109\n4k/2k/1k\nFI\nPoS Tagging\nFinnish UD Treebank\n(Haverinen et al., 2014;\nPyysalo et al., 2015;\nKanerva and Ginter, 2022)\nHospital records,\nonline forums,\ntweets, poetry\n12217/2122\n10k/7.5k/5k\nTable 4.1: Datasets used for our experiments. The original and sub-\nsampled number of sequences for experiments are given on the right.\n50We also just considered the maximum uncertainty over a sequence, with similar\nresults.\n4.2 uncertainty & calibration in low-resource nlp\n113\nIn-Distribution Training Sets.\nIn our experiments, we test\nthree different languages combined with one NLP task, each. For\nthe languages, we choose English (Clinc Plus; Larson et al., 2019),\nDanish in the form of the Dan+ dataset (Plank et al., 2020) based\non news texts from PAROLE-DK (Bilgram and Keson, 1998),\nFinnish (UD Treebank; Haverinen et al., 2014; Pyysalo et al.,\n2015; Kanerva and Ginter, 2022). These datasets correspond to\nthe NLP tasks of sequence classification, named entity recognition\nand part-of-speech tagging, respectively. An overview over the\ndatasets is given in Table 4.1, with the preprocessing detailed\nin Appendix C.5.\nWe use low-resource languages in the case\nof Finnish and Danish, and simulate a low-resource setting\nusing English data.51 Starting with a sufficiently-sized training\nset and then sub-sampling allows us to create training sets of\narbitrary sizes.\nWe employ a specific sampling scheme that\ntries to maintain the sequence length and class distribution\nof the original corpus, which we explain and verify in Appendix B.3.\nOut-Of-Distribution Test Sets.\nWe create OOD test sets\nfrom data sources that are qualitatively different from the in-\ndistribution training data: Out-of-scope voice commands by users\nin Larson et al. (2019),52 the Twitter split of the Dan+ dataset\n(Plank et al., 2020), and the Finnish OOD treebank (Kanerva and\nGinter, 2022). In similar works for the image domain, OOD test\nsets are often chosen to be convincingly different from the training\ndistribution, for instance MNIST versus Fashion-MNIST (Nalisnick\net al., 2019b; van Amersfoort et al., 2021). While there exist a\nvariety of taxonomies for distributional shifts(Moreno-Torres et al.,\n2012; Wald et al., 2021; Arora et al., 2021; Federici et al., 2021;\nHupkes et al., 2023), it is often hard to determine if and what kind\nof shift is taking place. Winkens et al. (2020) define near OOD\nas a scenario in which the training and outlier distribution are\nmeaningfully related, and far OOD as a case in which they are\nunrelated. Unfortunately, this distinction is somewhat arbitrary\nand hard to apply in a language context, where OOD could bde\n51The definition of low-resource actually differs greatly between works. One\ndefinition by Bird (2022) advocates the usage for (would-be) standardized\nlanguages with a large amount of speakers and a written tradition, but a\nlack of resources for language technologies. Another way is a task-dependent\ndefinition: For dependency parsing, Müller-Eberstein et al. (2021) define low-\nresource as providing less than 5000 annotated sentences in the Universal\nDependencies Treebank. Hedderich et al. (2021); Lignos et al. (2022) lay out\na task-dependent spectrum, from a several hundred to thousands of instances.\n52Since all instances in this test set correspond to out-of-scope inputs and not\nto classes the model was trained on, we cannot evaluate certain metrics in\nTable 4.2.\n4.2 uncertainty & calibration in low-resource nlp\n114\ndefined as anything ranging from a different language or dialect to\na different demographic of an author or speaker or a new genre.\nTherefore, we use a similar methodology to the validation of the\nsub-sampled training sets to make an argument that the selected\nOOD splits are sufficiently different in nature from the training\nsplits. The exact procedure along some more detailed results is\ndescribed in Appendix B.4. Mainly, we examine the distribution\nof sequence lengths and labels, and score the OOD test set using\nthe perplexity of a language model training on the training split.\n4.2.3\nModel Training\nFigure 4.4: Schematic of our text classification experiments. Training\nsets are sub-sampled and used to train LSTM-based models and fine-\ntune transformer-based ones, which are evaluated on in- and out-of-\ndistribution test data.\nUnfortunately, our datasets do not contain enough data to\ntrain transformer-based models from scratch. Therefore, we only\nfully train LSTM-based models, and use pre-trained transformers,\nnamely Bert (English; Devlin et al., 2019), Danish Bert (Hvingelby\net al., 2020), and FinBert (Finnish; Virtanen et al., 2019), for the\nother approaches. The whole procedure is depicted in Figure 4.4.\nThe way we optimize models as well as training hardware and\nhyperparameter information are listed in Appendix C.4.3, with the\nenvironmental impact described in Appendix C.2.\n4.2.4\nEvaluation\nIn addition to evaluating models on the task performance, we also\nevaluate the following calibration and uncertainty, painting a multi-\nfaceted picture of the reliability of models. In all cases, we use the\nalmost stochastic order test (ASO; del Barrio et al., 2018a; Dror\net al., 2019) as described in Section 3.2.1 for significance testing.\nEvaluation of Calibration.\nFirst, we measure the calibration\nof models using the expected calibration error (ECE; Naeini et al.,\n4.2 uncertainty & calibration in low-resource nlp\n115\n2015; Guo et al., 2017), which we already discussed in Section 2.2.1.\nIn the same chapter, we introduced the frequentist measure of\ncoverage (Larry, 2004; Kompa et al., 2021). Coverage here based\non the (non-conformalized) prediction set of a classifier given an\ninput, which includes the most likely classes adding up to or\nsurpassing 1 −α probability mass. A well-tuned classifier should\ncontain the correct class in this prediction set, while minimizing its\nwidth. The extent to which this property holds can be determined\nby the coverage percentage, i.e. the number of times the correct\nclass in indeed contained in the prediction set, and its cardinality,\ndenoted simply as width.\nEvaluation of Uncertainty.\nWe compare uncertainty scores\non the ID and OOD test set and measure the area under the\nreceiver-operator curve (AUROC; evaluating the trade-off between\nsensitivity and specificity) and under the precision-recall curve\n(AUPR), assuming that uncertainty will generally be higher on\nsamples from the OOD test set.53 An ideal model should create\nvery different distributions of confidence scores on ID and OOD\ndata, thus maximizing AUROC and AUPR (as opposed to the\nsaturating confidence scores that we observed in Section 4.1.5).\nHowever, we also want to find out to what extent uncertainty can\ngive an indication of the correctness of the model, which is why\nwe propose a new way to evaluate the discrimination property\nproposed by Alaa and van der Schaar (2020) based on Leonard\net al. (1992): A good model should be less certain for inputs\nthat incur a higher loss. To measure this both on a token and\nsequence level, we utilize Kendall’s τ (Kendall, 1938), which, given\ntwo lists of measurements, determines the degree to which they\nare concordant—that is, to what extent the rankings of elements\naccording to their measured values agree. This is expressed by\na value between −1 and 1, with the latter expressing complete\nconcordance. In our case, these measurements correspond to the\nuncertainty estimate and the actual model loss, either for tokens\n(Token τ) or sequences (Sequence τ).\n4.2 uncertainty & calibration in low-resource nlp\n116\n4.2.5\nExperiments\nWe present the results from our experiments using the largest\ntraining set sizes per dataset in Table 4.2.54\nTask Performance.\nAcross datasets and models, we can iden-\ntify several trends: some of the Bert-based models unsurprisingly\nperform better than LSTM-based models, which can be explained\nby the fact that they are pretrained on large datasets. We observe\nworse performance for some LSTM and Bert-variants, in particular\nthe variational, Bayesian and ST-τ LSTM, as well the SNGP Bert.\nIn accordance with the ML literature (see e.g. Lakshminarayanan\net al., 2017; Snoek et al., 2019) and the discussions in Section 2.2.2,\nLSTM ensembles actually perform very strongly and on par or\nsometimes better than fine-tuned Berts.\nCalibration.\nWe also see that Bert models generally achieve\nlower calibration errors across all metrics measured, which is in\nline with previous works (Desai and Durrett, 2020; Dan and Roth,\n2021). It is interesting that the correct prediction is almost always\ncontained in the 0.95 confidence set across all models, however\nthese number have to be interpreted in the context of the set’s\nwidth: It becomes apparent that for instance LSTMs achieve this\ncoverage by spreading probability mass over many classes, while\nonly Bert-based models, LSTM ensembles as well as the Bayesian\nLSTM (on Danish) and the Variational LSTM (on Finnish) are\nconfidently correct.\nUncertainty Quality.\nLSTM-based model seem to struggle to\ndistinguish in- from out-of-distribution data based on predictive\nuncertainty. For Danish, only Berts perform visibly above chance-\nlevel. For Finnish, the AUPR results suggest that although some\nOOD instances are quickly identified as uncertain, many other\nOOD inputs remain undetected among in-distribution samples.\nFor English, OOD samples are detected more effectively, which\ncan be explained by them consisting of unknown voice commands,\nrepresenting a potential instance of semantic shift, which has been\nshown to be easier to detect by classifiers (Arora et al., 2021).\nFurthermore, it is striking that uncertainty and loss on a token-\n53We thus formulate a pseudo-binary classification task as common in the\nliterature, using the model’s uncertainty score to try to distinguish the two\ntest sets. Note that we do not advocate for actually using uncertainty for\nOOD detection, but only use it for evaluation purposes, since uncertainty on\nOOD examples should be high due to model uncertainty.\n54For English, some models were omitted due to convergence issues, which are\ndiscussed in Appendix C.7.\n4.2 uncertainty & calibration in low-resource nlp\n117\nlevel (Token τ) is only positive correlated for some models, using\nmetrics such as the maximum probability score, softmax gap or\nthe Dempster-Shafer metric, which are all entirely based on the\ncategorical output distributions. On a sequence-level (Sequence τ),\nthe correlation is often negative, meaning that higher uncertainty\ngoes hand in hand with a higher loss. This is the antithesis of the\ndesired outcome and the opposite of the trend on the token-level,\nand suggests that few tokens-level scores distort the sequence-\nlevel aggregation of uncertainties. Lastly, it should be noted that\ndifferent uncertainty metrics yield diverse outcomes: There does\nnot seem to be one superior metric across all experimental settings,\nas seen by the variety of markers shown in Table 4.2, which signify\nthe best-performing uncertainty metrics per model and result.\n4.2.6\nDependence on Training Data\n0.1\n0.2\n0.3\n−0.2\n0.0\n0.2\n0.4\n0.6\nToken-level Kendall’s tau\n1000 instances\n0.1\n0.2\n0.3\n2000 instances\n0.1\n0.2\n0.3\n4000 instances\nDempster-Shafer\nMutual Inf.\nSoftmax gap\nLog. Prob.\nMax. Prob.\nPred. Entropy\nVariance\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\nFigure 4.5: Scatter plot showing the difference between model perfor-\nmance (measured by macro F1 and the quality of uncertainty estimates\non a token-level (measured by Kendall’s τ). Shown are different mod-\nels and uncertainty metrics and several training set sizes of the Dan+\ndataset. Arrows indicate changes between the in-distribution and out-\nof-distribution test set. Best viewed electronically and in color.\nAfter presenting the best results for the biggest training set sizes\nin Table 4.2, we now continue to analyze the difference between\nmodels and metrics in a more fine-grained way. In Figure 4.5, we\nshow differences for the token-level correlation between a model’s\nloss and its uncertainty measured by Kendall’s τ, with arrows\nindicating the shift from measurements on the in- to the out-of-\ndistribution test set. Here, we see the same trend of more training\ndata having a larger influence on Bert models. Peculiarly, we also\nobserve that the uncertainty of pre-trained models correlates less\nwith their losses on the OOD data, while this property stays relative\nconstant for LSTMs. We can recognize this trend also for the other\ndatasets in Figure 4.5 and to a lesser degree on a sequence level\nFigure B.14a in Appendix B.5, albeit with a negative correlation in\ngeneral in the latter case. In Figures B.11 and B.12 in Appendix B.5,\n4.2 uncertainty & calibration in low-resource nlp\n118\nwe show the AUROC and AUPR of different model-uncertainty\nmetric combinations for all datasets and training set sizes. In both\ncases, we can notice that pre-trained models profit more from an\nincrease in available training data than LSTM-based models that\nare trained from scratch. This improvement is observed both in\ntask performance, as well as in the model’s ability to discern ID\nfrom OOD data using its uncertainty, but more so for the Danish\nthan English or Finnish. Like in the previous section, we often see\nthat uncertainty metrics of the same model perform quite similarly.\nThese results outline a seeming paradox: Pre-trained and then\nfine-tuned models (often) perform better on the task at hand, and\nprovide better uncertainty estimates, but only on in-distribution\ndata. Models trained from scratch that have seen less data overall,\nhowever provide more reliable uncertainty estimates on OOD data,\nbut are also worse calibrated (Section 4.2.5), with the exception\nof ensembles. This effect appears to largest on Danish, containing\nthe least data.\n4.2.7\nInstance Analysis\nWe investigate the development of uncertainty estimates over the\ncourse of a single sequence for different datasets, models, and un-\ncertainty metrics. Two examples are showcased in Figure 4.6, with\nmore examples in Appendix B.6. By looking at the predictive\nentropy of models in Section 4.2.7, we can observe multiple things:\nFirst of all, we can observe some degree of agreement between mod-\nels and their uncertainty: Uncertainty is higher for subword tokens,\nand the total uncertainty always appears to reduce considerably\non punctuation. Interestingly, the highest uncertainty seems to be\nproduced by the DDU and variational Bert models as well as the\nensembles. In Figure 4.6b, we compare the estimates for predictive\nentropy and mutual information, the latter of which is supposed to\nonly express model uncertainty. Here, uncertainty is generally low,\nindicating a large part of the total uncertainty might actually be\nof an aleatoric nature (which is the gap between triangle and cross\nmarkers of the same color, due to Equation (4.4)). These insights\nindicate that while aleatoric uncertainty might be a constant factor\nfor all models, epistemic uncertainty expectedly differs noticeably\nbetween them. We use all of these insights to discuss the choice of\nmodel next.\n4.2 uncertainty & calibration in low-resource nlp\n119\ndenne (O)\ngang (O)\ni (O)\nfølgeskab (O)\nmed (O)\nj (B-PER)\n##ørn (-100)\nmiddel (I-PER)\n##hede (-100)\n, (O)\nligeledes (O)\nfra (O)\nkold (B-LOC)\n##ing (-100)\n. (O)\n4\n3\n2\n1\n0\n1\n2\n3\nNormalized Uncertainty\nPred. Entropy\nLSTM\nDDU Bert\nBayesian LSTM\nVariational Bert\nSNGP Bert\nLSTM Ensemble\nST-tau LSTM\nVariational LSTM\n(a) Predictive entropy over the sentence “This time in company\nwith Jørn Middelhede, also from Kolding”.\nIlmi (NOUN)\n##ö (-100)\nkesti (VERB)\nkuitenkin (ADV)\nniin (ADV)\nlyhyen (ADJ)\naikaa (NOUN)\n, (PUNCT)\nettei (_)\nettä (SCONJ)\nei (AUX)\nPe (PROPN)\n##kalla (-100)\nollut (AUX)\nmahdollisuutta (NOUN)\nsen (PRON)\ntodista (NOUN)\n##miseen (-100)\n. (PUNCT)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nUnnormalized Uncertainty\nPred. Entropy\nMutual Inf.\nSNGP Bert\nVariational Bert\nLSTM Ensemble\nVariational LSTM\n(b) Predictive entropy and mutual information over the sentence\n“However, the phenomenon lasted for such a short time that Pekka\ndid not have a chance to prove it”.\nFigure 4.6: Uncertainty estimates on single sequences, for (a) predictive\nentropy of different models on Danish and (b) predictive entropy and\nmutual information for multi-prediction models on Finnish (Figure 4.6b).\n4.2.8\nDiscussion\nOur experiments in the previous sections have uncovered interest-\ning nuances about uncertainty quantification in text classification.\nWith respect to the first research question, we observed that\nfine-tuning Berts and training LSTM ensembles on different\nlanguages produces high task scores with low calibration errors\nand high-quality uncertainty estimates, but only on in-distribution\ndata. On OOD data, uncertainty estimates from fine-tuned models\nactually become less indicative of potential model loss compared\nto LSTM-based models.\nWe also find that among the variety\nof uncertainty metrics proposed, there does not appear to be a\nsuperior metric, i.e. most able to hint at mispredictions and OOD\n4.3 summary\n120\ndata. Differences in Kendall’s τ on a token and sequence level\nsuggest that loss and uncertainties fluctuate over the course of\nsequence.\nAnswering the second research question, more training data\nparadoxically decreases the quality of uncertainty estimates on\nOOD data for pre-trained models. We speculate that fine-tuning\nmodels increasingly lets them forget relevant features that would\nproduce higher uncertainty. This might explain why for this effect\nis smaller for LSTM-type models, which are trained from scratch.\nLastly, we conclude about the third research question that\nall the total uncertainty of models behaves somewhat similarly,\npotentially due to the strong influence of aleatoric uncertainty.\nFrom these insights, we summarize that the approaches using\npre-trained models overall give the best trade-off between task\nperformance, uncertainty quality and calibrations, however their\nfailure on OOD samples opens up further directions of research.\nEnsembles can provide an alternative here in data-scarce settings,\nwhen the task is sufficiently learnable without the need for pre-\ntraining.\nLimitations.\nEven though the experiments test a large array\nof models and metrics, the collection here shown is by no means\nexhaustive, and only a selection of popular models or approaches\nfrom very different families were considered.\nAnother glaring\nshortcoming is the focus on only three European languages: By\ncomparing members of the Uralic, North Germanic and West\nGermanic families, we only scratch the surface when it comes to\nthe morphological diversity of human language, as for instance\nillustrated in Figure 1.1c. Further, we only focused on languages\nwith a Latin writing systems, as well as specific text domains and\ntasks. This is due to resource constraints and the availability of\nsuitable OOD test sets. We hope that follow-up works will refine\nour insights on a more representative sample of natural languages.\n4.3\nSummary\nThis chapter explored some perspectives on uncertainty quan-\ntification in classification.\nSection 4.1 demonstrated how the\ninductive bias of ReLU networks produces uncertainty estimates\nthat are not indicative of the familiarity of data to the model;\ninstead, they converge to fix points in the limit. We were able to\nprove this formally and get an intuition of potential pitfalls in\npractice in Section 4.1.5, however text classification models in\n4.3 summary\n121\nNLP possess different and more complex architecture, for which\nsimilar arguments are not easily applicable. Therefore, we followed\nup with an empirical investigation into many popular models\nand uncertainty metrics on three different languages and tasks in\nSection 4.2. This came with some surprising insights, for instance\nthat uncertainty can be unreliable on OOD data, and that more\ntraining or finetuning data can lead to decreased uncertainty\nquality.\nAs we argued in the introduction in Section 1.3, data scarcity\nand the complexity of language are two core features that differ-\nentiate uncertainty quantification in NLP from other modalities.\nIn this chapter we discussed the arguably easier setting of text\nclassification: In text classification, we can treat predictions on\na sequence-level as i.i.d., and the set of classes is usually much\nsmaller than the number of tokens in a vocabulary. This is why\nwe now turn our attention to the more challenging problem of\nlanguage generation in the next chapter.\n4.3 summary\n122\nTask\n\u0000ID\n\u000e\nOOD\n\u0001\nCalibration\n\u0000ID\n\u000e\nOOD\n\u0001\nUncertainty\n\u0000ID\n\u000e\nOOD\n\u0001\nModel\nAcc.↑\nF1 ↑\nECE↓\n% Cov.↑\n∅Width↓\nAUROC↑\nAUPR↑\nToken τ ↑\nSeq. τ ↑\nLSTM\n.79\n±.00\n.62\n±.01\n.78\n±.00\n1.00\n±.00\n144.00\n±.00\n.88'\n±.01\n.60'\n±.01\n.75⃝\n±.01\nBayesian\nLSTM\n.59\n±.06\n.46\n±.05\n.78\n±.00\n.88\n±.00\n41.99\n±1.94\n.86△\n±.01\n.59$\n±.01\n.66⃝\n±.02\nLSTM\nEnsemble\n.81\n±.00\n.64\n±.00\n0.77\n±.00\n.87\n±.00\n4.27\n±.05\n.92'\n±.00\n.71'\n±.01\n.732\n±.01\nVar.\nBert\n.45\n±.16\n.34\n±.13\n.78\n±.00\n1.00\n±.00\n115.11\n±11.38\n.80$\n±.01\n.53$\n±.01\n.57⃝\n±.09\nEnglish\nDDU\nBert\n.79\n±.00\n.64\n±.01\n.77\n±.00\n.82\n±.00\n1.46\n±.04\n.88⃝\n±.00\n.62⃝\n±.01\n.87⃝\n±.00\nLSTM\n.93\n±.00\n\u000e\n.92\n±.00\n.26\n±.01\n\u000e\n.19\n±.01\n.17\n±.00\n\u000e\n.17\n±.00\n1.00\n±.00\n\u000e\n1.00\n±.00\n19.00\n±.00\n\u000e\n19.00\n±.00\n.50⃝\n±.02\n.14⃝\n±.01\n.50⃝\n±.01\n\u000e\n.47⃝\n±.00\n−.26'\n±.02\n\u000e\n−.28⃝\n±.05\nVar.\nLSTM\n.90\n±.02\n\u000e\n.90\n±.02\n.08\n±.02\n\u000e\n.09\n±.02\n.17\n±.00\n\u000e\n.17\n±.00\n.99\n±.01\n\u000e\n.98\n±.01\n6.62\n±.37\n\u000e\n6.68\n±.33\n.60'\n±.04\n.21'\n±.02\n.23⃝\n±.06\n\u000e\n.23⃝\n±.05\n−.04$\n±.02\n\u000e\n−.022\n±.05\nST-τ\nLSTM\n.92\n±.00\n\u000e\n.92\n±.00\n.12\n±.00\n\u000e\n.09\n±.00\n.17\n±.00\n\u000e\n.17\n±.00\n1.00\n±.00\n\u000e\n.99\n±.00\n7.10\n±.07\n\u000e\n7.03\n±.08\n.54'\n±.01\n.15'\n±.01\n.50⃝\n±.00\n\u000e\n.48⃝\n±.00\n−.052\n±.03\n\u000e\n−.012\n±.05\nBayesian\nLSTM\n.93\n±.00\n\u000e\n.93\n±.00\n.07\n±.00\n\u000e\n.07\n±.00\n.17\n±.00\n\u000e\n.17\n±.00\n1.00\n±.00\n\u000e\n1.00\n±.00\n1.68\n±.04\n\u000e\n1.70\n±.05\n.65D\n±.17\n.31D\n±.30\n.53⃝\n±.01\n\u000e\n.55⃝\n±.01\n−.012\n±.07\n\u000e\n−.02'\n±.04\nLSTM\nEnsemble\n.95\n±.00\n\u000e\n.94\n±.00\n.33\n±.01\n\u000e\n.25\n±.01\n0.16\n±.00\n\u000e\n0.16\n±.00\n.98\n±.00\n\u000e\n.97\n±.00\n1.62\n±.00\n\u000e\n1.58\n±.01\n.602\n±.02\n.182\n±.01\n.442\n±.00\n\u000e\n.452\n±.00\n−.19'\n±.01\n\u000e\n−.282\n±.01\nSNGP\nBert\n.22\n±.35\n\u000e\n.19\n±.34\n.03\n±.03\n\u000e\n.02\n±.02\n.17\n±.00\n\u000e\n0.17\n±.00\n1.00\n±.00\n\u000e\n1.00\n±.00\n18.84\n±.32\n\u000e\n18.83\n±.34\n.86△\n±.06\n.49△\n±.12\n.172\n±.09\n\u000e\n.262\n±.14\n.29$\n±.03\n\u000e\n.442\n±.11\nVar.\nBert\n.94\n±.00\n\u000e\n.89\n±.00\n.29\n±.01\n\u000e\n.17\n±.00\n0.16\n±.00\n\u000e\n0.16\n±.00\n.99\n±.00\n\u000e\n.98\n±.00\n2.25\n±.01\n\u000e\n3.86\n±.08\n.86'\n±.01\n.46'\n±.02\n.42⃝\n±.00\n\u000e\n.17D\n±.00\n−.352\n±.01\n\u000e\n−.412\n±.01\nDanish\nDDU\nBert\n.92\n±.00\n\u000e\n.89\n±.00\n.25\n±.00\n\u000e\n.17\n±.00\n0.16\n±.00\n\u000e\n0.16\n±.00\n.99\n±.00\n\u000e\n.99\n±.00\n3.48\n±.01\n\u000e\n4.04\n±.03\n.86⃝\n±.01\n.39⃝\n±.02\n.56⃝\n±.00\n\u000e\n.25⃝\n±.01\n−.24⃝\n±.01\n\u000e\n−.38⃝\n±.03\nLSTM\n.75\n±.00\n\u000e\n.69\n±.00\n.57\n±.00\n\u000e\n.53\n±.00\n.07\n±.00\n\u000e\n.07\n±.00\n1.00\n±.00\n\u000e\n1.00\n±.00\n16.00\n±.00\n\u000e\n16.00\n±.00\n.63△\n±.01\n.69'\n±.01\n.29⃝\n±.00\n\u000e\n.19⃝\n±.01\n−.28'\n±.02\n\u000e\n−.27'\n±.02\nVar.\nLSTM\n.27\n±.00\n\u000e\n.26\n±.00\n.03\n±.00\n\u000e\n.03\n±.00\n.07\n±.00\n\u000e\n.07\n±.00\n.97\n±.00\n\u000e\n.96\n±.00\n1.35\n±.23\n\u000e\n1.37\n±.21\n.51'\n±.01\n.59'\n±.01\n.00△\n±.01\n\u000e\n.00D\n±.00\n.01△\n±.03\n\u000e\n.012\n±.01\nST-τ\nLSTM\n.76\n±.00\n\u000e\n.71\n±.00\n.58\n±.00\n\u000e\n.55\n±.00\n.06\n±.00\n\u000e\n.06\n±.00\n.97\n±.00\n\u000e\n.96\n±.00\n3.32\n±.01\n\u000e\n3.57\n±.01\n.62△\n±.01\n.69'\n±.01\n.31⃝\n±.00\n\u000e\n.21⃝\n±.01\n−.14'\n±.02\n\u000e\n−.122\n±.04\nBayesian\nLSTM\n.27\n±.00\n\u000e\n.26\n±.00\n.03\n±.00\n\u000e\n.03\n±.00\n.07\n±.00\n\u000e\n.07\n±.00\n1.00\n±.00\n\u000e\n1.00\n±.00\n16.00\n±.00\n\u000e\n16.00\n±.00\n.51D\n±.01\n.60$\n±.00\n.00D\n±.00\n\u000e\n.00D\n±.00\n.01⃝\n±.01\n\u000e\n.04'\n±.00\nLSTM\nEnsemble\n.81\n±.00\n\u000e\n.75\n±.00\n.62\n±.00\n\u000e\n.57\n±.00\n.06\n±.00\n\u000e\n.06\n±.00\n.99\n±.00\n\u000e\n.98\n±.00\n3.46\n±.01\n\u000e\n3.80\n±.01\n.67'\n±.01\n.74'\n±.01\n.29⃝\n±.00\n\u000e\n.19⃝\n±.01\n−.28'\n±.01\n\u000e\n−.31'\n±.01\nVar.\nBert\n.87\n±.00\n\u000e\n.81\n±.00\n.74\n±.00\n\u000e\n.70\n±.00\n.06\n±.00\n\u000e\n.06\n±.00\n.99\n±.00\n\u000e\n.99\n±.00\n4.68\n±.03\n\u000e\n5.19\n±.02\n.64△\n±.01\n.70⃝\n±.01\n.14⃝\n±.00\n\u000e\n.08'\n±.00\n−.19$\n±.00\n\u000e\n−.16$\n±.01\nSNGP\nBert\n.18\n±.10\n\u000e\n.17\n±.10\n.07\n±.02\n\u000e\n.08\n±.02\n.07\n±.00\n\u000e\n.07\n±.00\n1.00\n±.00\n\u000e\n.99\n±.01\n15.00\n±.00\n\u000e\n15.00\n±.00\n.54△\n±.05\n.63△\n±.04\n.152\n±.04\n\u000e\n.152\n±.03\n.122\n±.05\n\u000e\n.142\n±.02\nFinnish\nDDU\nBert\n.87\n±.00\n\u000e\n.81\n±.00\n.72\n±.03\n\u000e\n.68\n±.03\n.06\n±.00\n\u000e\n.06\n±.00\n.94\n±.00\n\u000e\n.91\n±.00\n2.16\n±.06\n\u000e\n2.31\n±.06\n.61⃝\n±.02\n.69⃝\n±.02\n.39⃝\n±.04\n\u000e\n.26⃝\n±.03\n−.07⃝\n±.05\n\u000e\n−.16⃝\n±.04\nTable 4.2: Results on the tested datasets. Task performance is measured\nby macro F1 and accuracy, calibration by different calibration errors,\nthe coverage percentage the average prediction set width. For every\nresult, and value on the ID and OOD test set is shown. For English,\nOOD scores are not available since the OOD set does not contain\ngold labels, and Token τ is missing due to CLINC being a sequence\nclassification task. Uncertainty quality is evaluated using its ability to\ndiscriminate between ID and OOD data, quantified by AUROC and\nAUPR. Furthermore, Kendall’s τ is measured between the uncertainty\nand losses on a sequence- and token-level. Displayed are mean and\nstandard deviation over five random seeds, with bolding and underlining\nindicating almost stochastic dominance with εmin ≤0.3 over all other\nmodels. For last section, the best value over uncertainty metrics is\ngiven, with symbols indicating the type of metric achieving it: ⃝Max.\nprobability, △Predictive entropy. 2 Class variance. D Softmax gap.\n'\nDempster-Shafer.\n$ Mutual information.\n5\n|\nUncertainty in Natural\nLanguage Generation\n“Obviously, a computer program that succeeded in generating\nsentences of a language would be, in itself, of no scientific\ninterest unless it also shed some light on the kinds of\nstructural features that distinguish languages from arbitrary,\nenumerable sets.”\n—Noam Chomsky in Formal properties of grammars (1963).\nNatural language generation (NLG) is a multi-faceted field spanning\napplications such as machine translation (MT), language modeling\n(LM), summarization, question-answering and dialogue generation.\nOwing to the recent success of large language models (LLMs) such\nas GPT-4 (OpenAI, 2023), Bloom (Scao et al., 2022) or Llama\n(Touvron et al., 2023a), natural language is increasingly used as an\ninterface for end users to interact with models. In order to generate\nthe tokens in a sentence, models typically predict a distribution over\nsubword tokens at every step of the generation process. Due to the\nparaphrastic nature of language discussed in Section 2.1.3, there is a\nlarge uncertainty about which token to select, since there might not\nbe a single “correct” token. Futhermore, just using the most likely\ntoken often results in text of low-quality (Holtzman et al., 2020; See\net al., 2019; Eikema and Aziz, 2020; Zhang et al., 2021b; Eikema,\n2024). For this reason, this uncertain decision is often realized\nthrough specialized sampling procedures. However, it has been\nshown that sampling from the tail of the token distribution also\nnegatively impacts text quality, which is why token distributions are\noften truncated in practice (Holtzman et al., 2020; Fan et al., 2018;\nMeister et al., 2023). While this kind of sampling allows for more\nfluent and varied text, there are no guarantees about the plausibility\nof the generated text. This is particularly relevant for generation\nscenarios where pre-trained models are applied to new data whose\ndistribution is different from the training data, increasing the risk of\ngenerating erroneous, misleading, and potentially harmful text (Ji\net al., 2023b; Guerreiro et al., 2023b; Pan et al., 2023; Alkaissi and\n123\n5.1 conformalizing natural language generation\n124\n{            }\nFigure 5.1: Schematic representation of our approach. A decoder hidden\nrepresentation zt is used during inference to retrieve the nearest neigh-\nbors and their non-conformity scores sk. Their relevance is determined\nby using their distance to compute weights wk, resulting in the quantile\nˆq that forms conformal prediction sets.\nMcFarlane, 2023; Azamfirei et al., 2023). Therefore, this chapter\nintroduces a way of creating calibrated prediction sets to sample\nfrom for natural language generation, imbued with the guarantees\nof conformal prediction.\n5.1\nConformalizing Natural Language\nGeneration\nThe following work is based on Ulmer et al. (2024c).\nConformal prediction (Vovk et al., 2005; Papadopoulos et al.,\n2002; Angelopoulos and Bates, 2021), has recently gained popular-\nity by providing calibrated prediction sets that are equipped with\nstatistical guarantees about containing the correct solution (see for\ninstance the introduction in Section 2.2.1). Nevertheless, applying\nconformal prediction to NLG is not trivial: The autoregressive\ngeneration process breaks the independence and identical distribu-\ntion (i.i.d.) assumption underlying conformal prediction techniques,\nsince new predictions are conditioned on the sequence generated\nso far. We tackle this problem by drawing inspiration from recent\nadvances in nearest-neighbor language modeling (Khandelwal et al.,\n2020; He et al., 2021a; Xu et al., 2023a) and machine translation\n(Khandelwal et al., 2021; Zheng et al., 2021; Meng et al., 2022b;\nMartins et al., 2022). This way, we can dynamically generate cali-\nbration sets during inference that maintain statistical guarantees.\nWe schematically illustrate non-exchangeable conformal nucleus\nsampling in Figure 5.1: In the first step, we obtain a (sorted)\nprobability distribution over tokens and a latent representation\nzt for the current generation step from the model. In a second\nstep, we use the latent representation to query a datastore for\n5.2 background\n125\nsimilar, previously stored representations and their corresponding\nnon-conformity scores, si. In the same way as in the standard con-\nformal prediction algorithm, these non-conformity scores indicate\nhow much a prediction conforms to the rest of the calibration set\nand its difficulty for the model. These scores are then used to\ncompute a threshold ˆq based on the theory of non-exchangeable\nconformal prediction (Barber et al., 2023), which defines a smaller\nset of tokens that is sampled from.55 The extension by Barber et al.\nallows us to compensate a lack of i.i.d. data by instead defining\nrelevance weights between the test point and the calibration set.\nContributions.\nWe present a general-purpose extension of the\nconformal framework to NLG by tackling the problems above. Our\ncontributions are as follows: First, to the best of our knowledge, we\nare the first to present a novel technique based on non-exchangeable\nconformal prediction and to apply it to language generation to\nproduce calibrated prediction sets using a theoretically sound mo-\ntivation. Secondly, we validate the effectiveness of the method in\na language modeling and machine translation context, evaluating\nthe coverage of the calibrated prediction sets and showing that our\nmethod is on par with or even outperforms other sampling-based\ntechniques in terms of generation quality, all while maintaining\ntighter prediction sets and better coverage. Lastly, we demonstrate\nthat these properties are also maintained under distributional shift\ninduced by corrupting the model’s latent representations.\n5.2\nBackground\nWe already discussed the basic formulation of conformal predic-\ntion in Section 2.2.1: We first define a non-conformity score that\nprovides an estimate of the distance of the test point to the rest\nof the data. Then, we determine ˆq as the\n\u0006\n(N + 1)(1 −α)/N\n\u0007\n-th\nquantile of the non-conformity scores on a held-out set. Finally,\nwe can create calibrated prediction sets of the form\nC(x′) =\n\b\ny\n\f\f Pθ(y | x′) ≥1 −ˆq\n\t\n.\n(5.1)\nHere, x′ is a new test point for which we could like to construct\na prediction set. If a test point x′ and the calibration set are i.i.d.,\nthen this set fulfils the conformal guarantee\np\n\u0000y′ ∈C(x′)\n\u0001\n≥1 −α.\n(5.2)\n55For simplicity, the figure depicts the simplest form of prediction sets used in\nconformal prediction. In practice, we use the adaptive prediction sets explained\nin Section 5.3.\n5.2 background\n126\nNevertheless, this formulation is not directly applicable to NLG,\nas autoregressive generation violates the i.i.d. assumption: If we\ncompare the token distributions at different time steps and different\nsequences, they will hardly be comparable.\nNon-exchangeable Conformal Prediction.\nBarber et al.\n(2023) address this shortcoming: When a test point and the cal-\nibration data are not i.i.d.,56 the distributional drift causes any\npreviously found ˆq to be miscalibrated, so the intended coverage\nbound of 1 −α can no longer be guaranteed. However, we can still\nperform conformal prediction by assigning a weight wi ∈[0, 1] to\nevery calibration data point, reflecting its relevance—i.e. assigning\nlower weights to points far away from the test distribution. Then,\nby normalizing the weights with ˜wi = wi/(1 + PN\ni=1 wi), we define\nthe quantile as\nˆq = inf\n\b\nq\n\f\f\nN\nX\ni=1\n˜wi1\n\u0000si ≤q\n\u0001\n≥1 −α\n\t\n.\n(5.3)\nThe construction of the prediction sets then follows the same\nsteps as before. Most notably, the coverage guarantee in Equa-\ntion (5.2) now changes to\np\n\u0000y′ ∈C(x′)\n\u0001\n≥1 −α −\nN\nX\ni=1\n˜wiεi,\n(5.4)\nwith an extra term including the total variation distance (dTV)\nbetween the distribution of a calibration and a test point, εi =\ndTV\n\u0000(xi, yi), (x′, y′)\n\u0001\n.57 Unfortunately, this term is hard to esti-\nmate or bound, nevertheless, the selection of appropriate weights\nthat captures the relevance of calibration points to the test set\nshould moderate both the impact of the distant data points on\nthe estimation of the prediction set and the impact of dTV on the\ncoverage bound. In other words, for large dTV values we expect\nto have smaller weights, that allow us to achieve coverage close\nto the desired values. We show in our experiments that the loss\nof coverage when using weights derived from the distance to near-\nest neighbor is limited, and revisit the practical implications in\nSection 5.5.\n56In fact, the coverage guarantee in Equation (5.2) applies to the case where the\ndata is exchangeable, a weaker requirement than i.i.d. Specifically, a series of\nrandom variables is exchangeable if their joint distribution is unaffected by a\nchange of their order. The work by Barber et al. (2023) allows us to also forgo\nthis requirement.\n57In this expression, (xi, yi) and (x′, y′) denote random variables and the total\nvariation distance is between the two underlying distributions. See Barber\net al. (2023) for details.\n5.3 method\n127\n5.3\nMethod\nWe now present a novel method to apply conformal predic-\ntion in NLG by synthesizing the non-exchangeable approach\nof Barber et al. (2023) with k-NN search-augmented neural\nmodels (Khandelwal et al., 2021, 2020). In the latter case, the\ntoken distribution at the current generation step is interpolated\nwith the predictive distributions of nearest neighbors in a datastore.\nA related approach for conformal prediction for NLG by Rav-\nfogel et al. (2023) calibrates prediction sets using the standard\nconformal procedure described in Section 5.2. In order to improve\nits effectiveness, the authors also determine multiple ˆq values based\non the entropy of the token distribution, grouping inputs into one\nof multiple bins. However, this implies that we would use semanti-\ncally unrelated (sub-)sequences to calibrate the model—in fact, we\nshow experimentally that this approach generally obtains trivial\ncoverage by producing extremely wide prediction sets. Instead,\nwe propose to perform a dynamic calibration step during model\ninference, only considering the most relevant data points from the\ncalibration set. We do this in the following way: Given a dataset\n{(x(i), y(i))} of sequences x(i) = (x(i)\n1 , . . . , x(i)\nS ) and corresponding\nreferences consisting of gold tokens y(i) = (y(i)\n1 , . . . , y(i)\nT ), we extract\nthe model’s decoder activations z(i)\nt\n∈Rd and conformity scores\ns(i)\nt .58 We save those in an optimized datastore, allowing for fast\nand efficient nearest-neighbor search using the FAISS method by\nJohnson et al. (2019) through techniques such as quantization and\nGPU acceleration. In the inference phase, during every decoding\nstep, we then use the decoder hidden state z′\nt to query the data\nstore for the K nearest neighbors and their non-conformity scores\nand record their distances. We use the squared l2 distance to\ncompute the weight wk as\nwk = exp\n\u0000−\n\f\f\f\f zt −zk\n\f\f\f\f2\n2 / τ\n\u0001\n,\n(5.5)\nwhere τ corresponds to a temperature hyperparameter.59 This\nformulation is equivalent to a radial basis function kernel with\nscale parameter τ. Finally, we use the weights to compute the\n58In this phase, we do not let the model generate freely, but feed it the gold\nprefix during the decoding process to make sure that conformity scores can be\ncomputed correctly.\n59Using this formulation of the weights wk that depends on the data deviates\nfrom the assumptions of original proof, as discussed in Barber et al. (2023),\nsection 4.5. Nevertheless, our results in Section 5.4 and those by Farinhas et al.\n(2024) show that the obtained bound in Equation (5.4) still remains useful.\n5.3 method\n128\nquantile ˆq as in Equation (5.3). The entire algorithm is given in\nAlgorithm 2.\nAlgorithm 2 Non-exchangeable Conformal Language Generation\nwith Nearest Neighbors\nRequire: Sequence x, model fθ, datastore DS(·) with model acti-\nvations collected from held-out set, temperature τ\nwhile generating do\n▷1. Extract latent encoding for current input\nzt ←fθ(xt; y<t)\n▷2. Retrieve K neighbors & non-conformity scores\n{(z1, s1), . . . (zK, sK)} ←DS(zt)\n▷3. Compute weights wk and normalize\nwk ←exp(−|| zt −zk||2\n2 / τ)\n˜wk ←wk/(1 + PK\nk=1 wk)\n▷4. Find quantile ˆq\nˆq ←inf{q | PN\ni=1 ˜wi1\n\u0000si ≤q\n\u0001\n≥1 −α}\n▷5. Create prediction set\nˆc ←sup{c′ | Pc′\nj=1 Pθ(y = π(j) | xt, y<t) < ˆq} + 1\nC(xt) ←{π(1), . . . , π(ˆc)}\n▷6. Generate next token\nyt ←generate(C(xt))\nend while\nAdaptive Prediction Sets.\nThe efficacy of conformal predic-\ntion hinges on the choice of non-conformity score, with the simple\nnon-conformity score si = 1 −Pθ(yt | x, y<t) known to undercover\nhard and overcover easy subpopulations of the data (Angelopoulos\nand Bates, 2021). Due to the diverse nature of language, we there-\nfore opt for adaptive prediction sets (Angelopoulos et al., 2021;\nRomano et al., 2020). Adaptive prediction sets redefine the non-\nconformity score as the cumulative probability over classes (after\nsorting in descending order) necessary to reach the correct class.\nIntuitively, this means that we include all classes whose cumulative\nprobability does not surpass ˆq. Compared to the simple conformity\nscore, this produces wider predictions sets for hard inputs, en-\ncompassing more potentially plausible continuations in a language\ncontext. More formally, let π be a permutation function mapping\n5.4 experiments\n129\nall possible output tokens [C] to the indices of a permuted version\nof the set, for which tokens are sorted in descending oder by their\nprobability under the model. We define the non-conformity score\nas\nsi =\nπ(yt)\nX\nj=1\nPθ(π−1(j) | x, y<t).\n(5.6)\nSince we only include the cumulative mass up until the gold\nlabel, the summation stops at π(y). The prediction sets are then\ndefined as\nC(x, y<t) =\nn\nπ−1(1), . . . , π−1(ˆc)\no\n,\n(5.7)\nwith ˆc = sup{c′ | Pc′\nj=1 Pθ(π−1(j) | x, y<t) < ˆq} + 1, where we add\none extra class to avoid empty sets.\n5.4\nExperiments\nIn the following sections, we conduct experiments in both language\nmodeling and machine translation. For machine translation we\nopt for the 400 million and 1.2 billion parameter versions of the\nM2M100 model (Fan et al., 2021) on the WMT-2022 shared task\ndatasets for German to English and Japanese to English (Bojar\net al., 2017). For language modeling, we use the 350 million and 1.3\nbillion parameter versions of the OPT model (Zhang et al., 2022b)\nand replicate the setup by Ravfogel et al. (2023): We calibrate our\nmodel on 10000 sentences from a 2022 English Wikipedia dump\n(Wikimedia Foundation, 2022) and test coverage and generation\non 1000 sentences from OpenWebText (Gokaslan et al., 2019).60\nAll models are used in a zero-shot setup without extra training or\nfinetuning. For the datastore, we use the implementation of the\nFAISS library (Johnson et al., 2019), computing 2048 clusters in\ntotal and probing 32 clusters per query. We also summarize the\nenvironmental impact of our experiments in Appendix C.2.\n5.4.1\nEvaluating Coverage\nFirst of all, we demonstrate that the retrieved information from the\ndata store enables us to successfully obtain calibrated prediction\n60Data obtained through the Hugging Face datasets package (Lhoest\net al., 2021):\nhttps://huggingface.co/datasets/wikipedia and https:\n//huggingface.co/datasets/stas/openwebtext-10k.\n5.4 experiments\n130\nsets. Coverage is an important notion in conformal prediction,\nreferring to the correct label being included in a prediction set or\ninterval. Since we can always achieve coverage trivially by choosing\nthe largest possible prediction set, an ideal method strikes a balance\nbetween high coverage and small prediction sets. While it is not\npossible to measure coverage in a free generation setting (see next\nsection), we can assess whether the correct class is contained in the\nprediction set if we feed the actual reference tokens into the decoder\nand check whether we include the true continuation.61 For our MT\ntask, this is reminiscent of an interactive translation prediction\nsetup (Knowles and Koehn, 2016; Peris et al., 2017; Knowles et al.,\n2019), where we propose possible continuations to a translator,\nsuggesting the next word from a set of words that (a) contains\nplausible options and (b) is limited in size, in order to restrict the\ncomplexity for the end user. Before we run our experiments, we\nneed to determine τ, which we tune on the calibration set using a\nstochastic hill-climbing procedure described in Appendix C.8. We\ncompare our non-exchangeable conformal nucleus sampling (Non-\nEx. CS) with the following sampling methods: Nucleus sampling\n(Nucleus; Holtzman et al., 2020), which includes all tokens up to a\npre-defined cumulative probability mass, and the conformal nucleus\nsampling (Conf.; Ravfogel et al., 2023) discussed earlier. The latter\nbins predictions on a calibration set by the entropy of the output\ndistribution, and compute one ˆq per such entropy bin using the\nstandard conformal procedure given in the beginning of Section 5.2.\nEvaluation.\nWe measure the total coverage using different dis-\ntance metrics, namely, squared l2 distance, normalized inner prod-\nuct, and cosine similarity (see Tables 5.1 and 5.2),62 as well as\nbinning predictions by set size and then measuring the per-bin\ncoverage in Figure 5.2 (more results given in Appendix B.7). We\nalso summarize the plots in Figure 5.2 via the expected coverage\ngap (ECG)63 that we define as\nECG =\nB\nX\nb=1\n|Bb|\nN max\n\u0010\n1 −α −Coverage\n\u0000Bb\n\u0001\n, 0\n\u0011\n,\n(5.8)\n61We emphasize that access to gold tokens is not required by our method and\nonly done here to measure the actual coverage.\n62For inner product and cosine similarity, we follow the same form as Equa-\ntion (5.5), omitting the minus. We normalize the inner product by the square\nroot of the latent dimension.\n63This is inspired by the expected calibration error (Guo et al., 2017), comparing\ncoverage to 1 −α, where overcoverage is not penalized due to Equation (5.2)’s\nlower bound.\n5.4 experiments\n131\nwhere Bb denotes a single bin and N the total number of considered\npredictions in the dataset.64 The ECG thus captures the average\nweighted amount of undercoverage across bins. In our experiments,\nwe use 75 bins in total. The same bins are used to also evaluate\nthe size-stratified coverage metric (SSC) proposed by Angelopoulos\net al. (2021), with a well-calibrated method resulting in a SCC\nclose to the desired coverage 1 −α:\nSCC =\nmin\nb∈{1,...,B} Coverage\n\u0000Bb\n\u0001\n.\n(5.9)\nWe can therefore understand the SCC as the worst-case coverage\nacross all considered bins.\nWe present some additional exper-\niments where we assess the impact of key hyperparameters in\nAppendix B.8.\nde →en\nja →en\nMethod\nDist.\nτ\n% Cov.\n∅Width ↓\nScc ↑\nEcg ↓\nτ\n% Cov.\n∅Width ↓\nScc ↑\nEcg ↓\nM2M100(400M)\nNucleus\n–\n–\n.9207\n.48\n.25\n.00\n–\n.9261\n.54\n.41\n.02\nConf.\n–\n–\n.9951\n.94\n.33\n.03\n–\n.9950\n.96\n.14\n.00\nNon-Ex.\nCS\nIP\n3.93\n.8251\n.16\n.63\n.26\n11.90\n.8815\n.24\n.67\n.03\nl2\n512.14\n.8334\n.17\n.60\n.06\n419.91\n.8468\n.18\n.61\n.05\ncos\n2.54\n.8371\n.17\n.63\n.06\n3.53\n.8540\n.17\n.62\n.04\nM2M100(1.2B)\nNucleus\n–\n–\n.8339\n.38\n.00\n.08\n–\n.7962\n.42\n.03\n.10\nConf.\n–\n–\n.9993\n.99\n.34\n.00\n–\n.9998\n.99\n.60\n.00\nNon-Ex.\nCS\nIP\n15.79\n.8861\n.25\n.71\n.03\n10.45\n.9129\n.38\n.72\n.00\nl2\n1123.45\n.8874\n.25\n.72\n.03\n605.97\n.8896\n.30\n.76\n.01\ncos\n3.21\n.8858\n.25\n.72\n.03\n1.48\n.8897\n.30\n.75\n.01\nTable 5.1: Coverage results for the de →en and ja →en MT tasks. We\nreport the best found temperature τ while keeping the confidence level\nα and number of neighbors k = 100 fixed. We also show the coverage\npercentage along with the avg. prediction set size as a proportion of\nthe entire vocabulary (∅Width) as well as ECG and SSC. Tested\ndistance metrics are inner product (IP), (squared) l2 distance, and\ncosine similarity (cos).\nResults.\nThe results are shown in Tables 5.1 and 5.2.\nWe\nfound that our method missed the desired coverage of 90% for\nMT by only 8% or less. Beyond the best values shown in the\ntables, we were not able to further increase coverage by varying\nthe temperature parameter without avoiding trivial coverage (i.e.,\ndefaulting to very large set sizes). This likely due to inherent\ncoverage gap in Equation (5.4) that is due to distributional drift\n64Since conformal prediction produces a lower bound on the coverage, we do\nnot include overcoverage in Equation (5.8).\n5.4 experiments\n132\nOpenWebText\nMethod\nDist.\nτ\n% Cov.\n∅Width ↓\nScc ↑\nEcg ↓\nOPT(350M)\nNucl. Sampl.\n-\n-\n.8913\n.05\n.71\n.01\nConf. Sampl.\n–\n–\n.9913\n.90\n.91\n.00\nNon-Ex. CS\nIP\n4.99\n.9352\n.19\n.80\n.00\nl2\n.31 × 104\n.9425\n.17\n.80\n.00\ncos\n4.98\n.9370\n.15\n.83\n.00\nOPT(1.3B)\nNucl. Sampl.\n–\n–\n.8952\n.05\n.00\n.01\nConf. Sampl.\n–\n–\n.9905\n.88\n0.95\n.00\nNon-Ex. CS\nIP\n.48\n.9689\n.59\n.84\n.00\nl2\n1.55 × 104\n.9539\n.20\n.83\n.00\ncos\n.11\n.9512\n.20\n.875\n.00\nTable 5.2: Coverage results for the LM task. We report the best found\ntemperature τ while keeping the confidence level α and number of\nneighbors k = 100 fixed. We also show the coverage percentage along\nwith the avg. prediction set size as a proportion of the entire vocabulary\n(∅Width) as well as the ECG and SSC metrics. Tested distance\nmetrics are inner product (IP), (squared) l2 distance and cos. similarity\n(cos).\n0\n20000\n40000\n60000\n80000\n100000\n120000\nSet Size\n0.2\n0.4\n0.6\n0.8\n1.0\nCoverage\n0\n500\n1000\n1500\n2000\n2500\nNumber of Points\n(a) Nucleus Sampling on de →en.\n0\n20000\n40000\n60000\n80000\n100000 120000\nSet Size\n0.2\n0.4\n0.6\n0.8\n1.0\nCoverage\n0\n5000\n10000\n15000\n20000\nNumber of Points\n(b) Conformal Nucleus Sampling on\nde →en.\n0\n20000\n40000\n60000\n80000\n100000 120000\nSet Size\n0.2\n0.4\n0.6\n0.8\n1.0\nCoverage\n0\n2000\n4000\n6000\n8000\nNumber of Points\n(c) Non-Ex. Conformal Sampling on\nde →en.\n0\n20000\n40000\n60000\n80000\n100000\n120000\nSet Size\n0.2\n0.4\n0.6\n0.8\n1.0\nCoverage\n0\n500\n1000\n1500\n2000\n2500\nNumber of Points\n(d) Non-Ex.\nCS on de →en with\nM2M100(1.2B).\nFigure 5.2: Conditional coverage for the M2M100 on de →en with the\nsmall 418M model (Figures 5.2a to 5.2c) and using the bigger 1.2B model\n(Figure 5.2d). We aggregate predictions by set size using 75 equally-\nspaced bins in total. The blue curve shows the conditional coverage per\nbin, whereas red bars show the number of binned predictions.\nand is challenging to estimate directly.\n5.4 experiments\n133\nMost notably, our method was able to achieve better SCC\nscores while maintaining considerably smaller prediction sets\nthan the baselines on average. The reason for this is illustrated\nin Figure 5.2: while standard nucleus sampling produces some\nprediction sets that are small, the total coverage seems to mostly\nbe achieved by creating very large prediction sets between 60k–80k\ntokens. The behavior of conformal nucleus sampling by Ravfogel\net al. (2023) is even more extreme in this regard, while our method\nproduces smaller prediction sets, with the frequency of larger set\nsizes decreasing gracefully. In Figure 5.2d, we can see that the\nlarger M2M100 models also tend to produce larger prediction\nsets, but still noticeably smaller than the baselines. Importantly,\nfor both M2M100 models, even very small prediction sets (size\n≤1000) achieve non-trivial coverage, unlike the baseline methods.\nFor LM, we always found the model to slightly overcover. This\ndoes not contradict the desired lower bound on the coverage in\nEquation (5.4) and suggests a more negligible distributional drift.\nWhile nucleus sampling produces the smallest average prediction\nsets, we can see that based on the SCC values some strata remain\nundercovered. Instead, our method is able to strike a balance\nbetween stratified coverage and prediction set size. With respect\nto distance measures, we find that the difference between them\nis minimal, indicating that the quality largely depends on the\nretrieved local neighborhood of the decoder encoding and that\nfinding the right temperature can help to tune the models to\napproximate the desired coverage. We would now like to find out\nwhether this neighborhood retrieval mechanism can prove to be\nrobust under distributional shift as well. Since we did not observe\nnotable differences between the distance metrics, we continue with\nthe l2 distance.\n5.4.2\nCoverage Under Shift\nTo demonstrate how the retrieval of nearest neighbors can help\nto maintain coverage under distributional shift, we add Gaussian\nnoise of increasing variance—and therefore intensity—to the last\ndecoder hidden embeddings (for MT) and the input embeddings\n(LM).65 This way, we are able to simulate distributional drift\nwhile still keeping the original sequence of input tokens intact,\nallowing us to measure the actual coverage. We show the achieved\ncoverage along with the average set size (as a percentage of the\n65A similar approach can be found for instance in the work of Hahn and Choi\n(2019); Zhang et al. (2023c) or by Snoek et al. (2019); Hendrycks and Dietterich\n(2019) in a computer vision context.\n5.4 experiments\n134\nNone\n0.025\n0.05\n0.075\n0.1\n0.6\n0.8\n1.0\nCoverage\nNone\n0.025\n0.05\n0.075\n0.1\n0.25\n0.50\n0.75\n1.00\nSet Size as % of Vocabulary\nNone\n0.025\n0.05\n0.075\n0.1\n0.6\n0.8\n1.0\nˆq\nNone\n0.025\n0.05\n0.075\n0.1\n0.4\n0.6\n0.8\n1.0\nCoverage\nNone\n0.025\n0.05\n0.075\n0.1\n0.0\n0.5\n1.0\nSet Size as % of Vocabulary\nNone\n0.025\n0.05\n0.075\n0.1\n0.85\n0.90\n0.95\n1.00\nˆq\nConformal Nucleus Sampling\nNucleus Sampling\nNon-Exchangeable Conformal Nucleus Sampling\nFigure 5.3: Coverage, average set size and ˆq based on the noise level on\nthe de →en MT task (top) and open text generation task (bottom).\nError bars show one standard deviation.\nNoise level\nNone\n.025\n.05\n.075\n.1\n∅Entropy\n8.46\n8.71\n9.20\n9.71\n10.08\nNucl. Sampl. (ρ)\n.87\n.86\n.84\n.82\n.81\nConf. Sampl. (ρ)\n.60\n.60\n.60\n.57\n.55\nNon-Ex. CS (ρ)\n−.14\n−.18\n−.27\n−.37\n−.45\nTable 5.3: Average entropy of 400M M2M100 model on de →en per noise\nlevel as well as the Spearman’s ρ correlation coefficients between the\npredictive entropy and the prediction set size of the different methods.\nAll results are significant with p < 0.0001.\ntotal vocabulary) and the average quantile ˆq in Figure 5.3. We can\nsee that the conformal sampling method deteriorates into returning\nthe full vocabulary as a prediction set. Thus it behaves similarly\nto simple sampling as indicated by the ˆq values being close to\n1. Nucleus sampling provides smaller prediction sets compared\nto conformal sampling, but they seem invariant to noise.\nAs\nsuch, the method is not robust to noise injection in the open text\ngeneration task, and the obtained coverage deteriorates with noise\nvariance ≥0.025. Instead, the use of nearest neighbors allows\nfor the estimation of prediction sets that are small but amenable\nto increase, such that the obtained coverage remains close to the\ndesired one. We can specifically observe that the prediction set\nsize increases considerably to mitigate the injected noise in the\nopen-text generation case.\n5.4 experiments\n135\nNeighbor Retrieval.\nWe further analyze how the retrieval en-\nables this flexibility by relating it to the entropy of the output\ndistribution of the 400M parameters M2M100 on German to En-\nglish. Intuitively, the baseline methods, faced by high-entropy\noutput distributions, need to produce wide prediction sets in order\nto maintain coverage. In fact, we report such results by correlat-\ning entropy levels and prediction set sizes using Spearman’s ρ in\nTable 5.3, showing strong positive correlations. Our method in\ncontrast consistently shows an anticorrelation between these two\nquantities, enabled by decoupling the creation of prediction sets\nfrom statistics of the output distribution to instead considering\nthe non-conformity scores of similar subsequences. The fact that\nthe prediction set size is not just dependent on the entropy of the\npredictions while maintaining coverage demonstrates the value of\nthe nearest neighbors: In this way, model uncertainty becomes\nmore flexible and is corroborated by evidence gained from similar\ninputs.\n5.4.3\nGeneration Quality\nCrucially, our method should not degrade and potentially even\nimprove generation quality. Thus, we evaluate the generation qual-\nity for the same tasks without supplying the gold prefix, instead\nemploying standard language generation procedures. For language\nmodeling, we follow Ravfogel et al. (2023) and use the first 35\ntokens from the original sentence as input. We compare against a\nset of generation strategies including top-k sampling (Fan et al.,\n2018; Holtzman et al., 2018; Radford et al., 2019), nucleus sampling\nand conformal nucleus sampling. We also test a variant of our\nmethod using constant weights wk = 1 for retrieved neighbors\n(Const. Weight CS) to assess the impact of the weighted neigh-\nbor retrieval procedure. We further compare with beam search\n(Medress et al., 1977; Graves, 2012) with a softmax temperature of\n0.1, and greedy decoding. Evaluation is performed using BLEU\n(Papineni et al., 2002), COMET-22 (Rei et al., 2020, 2022) and\nchrF (Popović, 2017) for MT, where COMET-22 is a trained neural\nmetric. For text generation, we use MAUVE (Pillutla et al., 2021)\nand BERTscore (Zhang et al., 2020c).66 MAUVE is a neural met-\nric that measures the divergence from human-written text, while\nBERTscore involves a fine-tuned Bert model that aims to predict\nhuman quality judgments.\n66All metrics except for COMET were used through Hugging Face evaluate.\nMAUVE uses gpt2 as a featurizer.\n5.4 experiments\n136\nde →en\nja →en\nMethod\nBleu ↑\nComet ↑\nChrF ↑\nBleu ↑\nComet ↑\nChrF ↑\nM2M100(400m)\nBeam search\n28.53\n.88\n55.58\n11.37\n.63\n37.74\nGreedy\n27.81\n.90\n54.9\n10.73\n.58\n36.5\nNucleus Sampling\n27.63 ±.03\n.89 ±.01\n54.80 ±.07\n10.61 ±.15\n.59 ±.01\n36.52 ±.19\nTop-k Sampling\n27.63 ±.03\n.89 ±.01\n54.79 ±.07\n10.61 ±.15\n.59 ±.01\n36.52 ±.19\nConf. Sampling\n27.63 ±.03\n.89 ±.01\n54.80 ±.07\n10.61 ±.15\n.59 ±.01\n36.52 ±.19\nConst. Weight CS\n27.63 ±.03\n.89 ±.01\n54.80 ±.07\n10.61 ±.15\n.59 ±.01\n36.52 ±.19\nNon-Ex. CS\n27.65 ±.10\n.90 ±.01\n54.82 ±.14\n10.74 ±.11\n.59 ±.01\n36.61 ±.08\nM2M100(1.2B)\nBeam search\n30.89\n.90\n56.8\n13.76\n.63\n40.43\nGreedy\n29.52\n.90\n55.67\n12.94\n.60\n39.91\nNucleus Sampling\n29.37 ±.12\n.90 ±.00\n55.55 ±.11\n10.61 ±.15\n.59 ±.01\n36.52 ±.19\nTop-k Sampling\n29.53 ±.00\n.90 ±.00\n55.67 ±.00\n12.91 ±.08\n.60 ±.01\n39.95 ±.00\nConf. Sampling\n29.37 ±.12\n.90 ±.00\n55.55 ±.11\n12.91 ±.08\n.60 ±.00\n39.95 ±.08\nConst. Weight CS\n29.37 ±0.12\n.90 ±.00\n55.55 ±.11\n12.91 ±.08\n.60 ±.01\n39.95 ±.08\nNon-Ex. CS\n29.37 ±0.12\n.90 ±.00\n55.55 ±.11\n12.91 ±.08\n.60 ±.01\n39.95 ±.08\nTable 5.4: Generation results for the de →en and ja →en translation\ntasks. We report performance using 5 beams for beam-search, top-k\nsampling with k = 10, and nucleus sampling with p = 0.9. Conformal\nmethods all use α = 0.1, with non-exchangeable variants retrieving 100\nneighbors, and sampling uses a softmax temperature of 0.1. Results\nusing 5 different seeds that are stat. significant according to the ASO\ntest (del Barrio et al., 2018a; Dror et al., 2019; Ulmer et al., 2022c) with\na confidence level of 0.95 and threshold εmin ≤0.3 are underlined.\nResults.\nWe show the results for the different methods in Ta-\nbles 5.4 and 5.5. We see that beam search outperforms all sampling\nmethods for MT. This corroborates previous work by Shaham and\nLevy (2022) who argue that (nucleus) sampling methods, by prun-\ning only the bottom percentile of the token distribution, introduce\nsome degree of randomness that is beneficial for open text genera-\ntion but may be less optimal for conditional language generation,\nwhere the desired output is constrained and exact matching gener-\nations are preferred (which is the case for MT). Among sampling\nmethods, we find nucleus sampling and conformal sampling to\nperform similarly (being in agreement with the findings of Ravfogel\net al., 2023) but are sometimes on par or even outperformed by our\nnon-exchangeable conformal sampling for MT. For text generation,\nour method performs best for the smaller OPT model, but is slightly\nbeaten by conformal nucleus sampling in terms of MAUVE. When\nusing constant weights, performance deteriorates to the conformal\n5.5 discussion\n137\nOpenWebText\nMethod\nMAUVE ↑\nBERTscore F1 ↑\nOPT(350M)\nBeam search\n.12\n.79\nGreedy\n.02\n.79\nNucleus Sampling\n.91 ±.02\n.80 ±.00\nTop-k Sampling\n.90 ±.03\n.80 ±.00\nConf. Sampling\n.91 ±.02\n.80 ±.00\nConst. Weight CS\n.91 ±.02\n.80 ±.00\nNon-Ex. CS\n.92 ±.01\n.80 ±.00\nOPT(1.3B)\nBeam search\n.17\n.80\nGreedy\n.05\n.79\nNucleus Sampling\n.91 ±.02\n.80 ±.00\nTop-k Sampling\n.93 ±.01\n.81 ±.00\nConf. Sampling\n.93 ±.01\n.80 ±.00\nConst. Weight CS\n.91 ±.02\n.80 ±.00\nNon-Ex. CS\n.92 ±.01\n.81 ±.00\nTable 5.5: Generation results for the open text generation. We report\nperformance using 5 beams for beam-search, top-k sampling with k = 10,\nand nucleus sampling with p = 0.9. Conformal methods all use α = 0.1,\nwith non-exchangeable variants retrieving 100 neighbors. Results using\n5 different seeds that are stat. significant according to the ASO test\n(del Barrio et al., 2018a; Dror et al., 2019; Ulmer et al., 2022c) with a\nconfidence level of 0.95 and threshold εmin ≤0.3 are underlined.\nsampling setup, emphasizing the importance of not considering all\nconformity scores equally when computing ˆq, even though the effect\nseems to be less pronounced for larger models. This illustrates\nthe benefit of creating flexible prediction sets that are adapted on\ntoken-basis, suggesting that both the latent space neighborhoods\nas well as the conformity scores are informative.\n5.5\nDiscussion\nOur experiments have shown that despite the absence of i.i.d.\ndata in NLG and the loss in coverage induced by using dynamic\ncalibration sets, the resulting coverage is still close to the pre-\nspecified desired level for both LM and MT. Additionally, even\nthough the coverage gap predicted by the method of Barber et al.\n(2023) is infeasible to compute for us, we did not observe any\ncritical degradation in practice. Further, we demonstrated how\n5.5 discussion\n138\nsampling from these calibrated prediction sets performs similarly\nor better than other sampling methods. Even though our method\nis still outperformed by beam search in the MT setting, previous\nwork such as minimum Bayes risk decoding has shown how multiple\nsamples can be re-ranked to produce better outputs (Kumar and\nByrne, 2002, 2004; Eikema and Aziz, 2020; Fernandes et al., 2022;\nFreitag et al., 2023). Additionally, recent dialogue systems based\non LLMs use sampling instead of beam search for generation (e.g.\nOpenAI, 2023; AI@Meta, 2024). Since our prediction sets are more\nflexible and generally tighter, our results serve as a starting point\nfor future work. For instance, our technique could be used with non-\nconformity scores that do not consider token probabilities alone (e.g.\nMeister et al., 2023) or using prediction set widths as a proxy for\nuncertainty (Angelopoulos et al., 2021). Furthermore, the extension\nwith conformal risk control (Angelopoulos et al., 2023; Farinhas\net al., 2024) enables guarantees with respect to a wider family\nof function than just coverage. This opens up other directions,\nfor instance defining functions that assess the desirability of the\ncurrent generation, analogous to on-the-fly alignment procedures\n(Yang and Klein, 2021; Qin et al., 2022; Mudgal et al., 2023; Gao\net al., 2024a).\nLimitations.\nWe highlight two main limitations of our work\nhere: Potential issues arising from different kinds of dataset shift as\nwell as efficiency concerns. Even though any loss of coverage due\nto the term quantifying distributional drift in Equation (5.4) was\nlimited in our experiments (see Sections 5.4.1 and 5.4.2), this might\nnot hold across all possible setups. As long as we cannot feasibly\napproximate the shift penalty, it is impossible to determine a priori\nwhether the loss of coverage might prove to be detrimental, and\nwould have to be checked in a similar way as in our experiments.\nFurthermore, we only consider shifts between the models’ training\ndistributions and test data distributions here, while many other,\nunconsidered kinds of shifts exist (Moreno-Torres et al., 2012;\nHupkes et al., 2023). Additionally, even using optimized tools such\nas FAISS (Johnson et al., 2019), moving the conformal prediction\ncalibration step to inference incurs additional computational cost\nduring generation. Nevertheless, works such as He et al. (2021a);\nMartins et al. (2022) show that there are several ways to improve\nthe efficiency of k-NN approaches, and we leave such explorations\nto future work.\n5.6 summary\n139\n5.6\nSummary\nIn this chapter, we successfully demonstrated the application of\na non-exchangeable variant of conformal prediction to machine\ntranslation and language modeling with the help of k-NN retrieval.\nBy retrieving a calibration set on the fly, one can create prediction\nsets for language generation based on the non-exchangeable\nconformal prediction algorithm by (Barber et al., 2023).\nWe\ndemonstrated that this method best maintains the desired coverage\nacross different dataset strata while keeping prediction sets smaller\nthan other sampling methods, all while providing theoretical cover-\nage guarantees about coverage that other comparable methods lack.\nHowever, this method has multiple shortcomings:\nExcept\nthrough the width of prediction sets, it does not explicitly quantify\nthe uncertainty of the model, adds computational overhead to the\ninference process and furthermore requires access to the internal\nstates of the model. This becomes problematic when trying to\napply to larger models than for instance M2M100(1.2B): Many of\nthe contemporary open-source models (like those in the case study\nin Section 3.2.3) comprise 7 billion, 40 billion or even more param-\neters. In addition, commercial closed-source models that can only\nbe accessed through an API are estimated to be even larger.67 The\nnature of the API-only access further exacerbated this problem,\nas no information internal to the model, sometimes not even the\ntoken distribution, can be accessed. The next chapter therefore\nproposes a method that operates within this very challenging and\nrestrictive setup.\n67For example, GPT-4’s parameter count is rumored to be 1.76 trillion (The\nDecoder, 2023).\n6\n|\nUncertainty in Large\nLanguage Models\n“In desperation I asked Fermi whether he was not impressed\n[. . . ]. He replied “How many arbitrary parameters did you\nuse for your calculations?” I [. . . ] said “Four.” He said: “I\nremember my friend Johnny von Neumann used to say, with\nfour parameters I can fit an elephant, and with five I can\nmake him wiggle his trunk.” With that, the conversation was\nover.”\n—Freeman Dyson in A Meeting with Enrico Fermi (2006).\nThe following work is based on Ulmer et al. (2024a).\nWhen given a case description of “A man superglued his face to\na piano and he says it’s making it hard to get a full night of sleep”,\na medical LLM was found to list a plethora of potential causes in\nits diagnosis, including narcolepsy, sleep apnea and others.68 This,\nof course, ignores the seemingly obvious reason for the patient’s\ncomplaints. While humorous, this example illustrates the pitfalls\nof practical LLM applications: Despite often looking convincing\non the surface—especially to non-experts—model responses can\nbe wrong or unreliable, leading to potentially harmful outcomes\nor a loss of trust in the system, foregoing its benefits. Indeed,\nconsistent behavior (imagine e.g. reliably indicating a lack of\nconfidence for unsure responses) has been argued as one way\nto build trust in automated systems (Jacovi et al., 2021), while\nmisleading predictions have been empirically shown to lead to a\nloss of trust that can be hard to recover from (Dhuliawala et al.,\n2023).\nThe introductory example shows that as large language\nmodels (LLMs) are increasingly deployed in user-facing appli-\ncations, building trust and maintaining safety by accurately\nquantifying a model’s confidence in its prediction becomes even\n68https://x.com/spiantado/status/1620459270180569090\n(last\naccessed\nNov. 7, 2023).\n140\n6.1 calibrating llms with auxiliary models\n141\nmore important.\nHowever, finding effective ways to calibrate\nLLMs—especially when the only interface to the models is their\ngenerated text—remains a challenge. Most previously discussed\nmethods to calibrate model predictions, such as the ones in\nSection 2.2.1 or even the non-exchangeable conformal language\ngeneration from the previous Chapter 5, require some degree\nof retraining or at least access to model hidden states and / or logits.\nIn this chapter, we introduce APRICOT\n(auxiliary prediction\nof confidence targets): A method to set targets to calibrate con-\nfidence scores to and train an additional model that predicts an\nLLM’s confidence based on its textual input and output alone. This\napproach has several advantages: It is conceptually simple, does\nnot require access to the target model beyond its output, does not\ninterfere with the language generation, and has a multitude of po-\ntential usages, for instance by verbalizing the predicted confidence\nor adjusting the given answer based on the confidence. We show\nhow our approach performs competitively in terms of calibration\nerror for white-box and black-box LLMs on closed-book question-\nanswering to detect incorrect LLM answers. Our contributions\nare as follows: We propose to obtain calibration targets for LLM\nconfidence scores without requiring any additional information\nabout LLM internals or question metadata. We show that using\nauxiliary models on the target LLM’s input and output is sufficient\nto predict a useful notion of confidence for question-answering on\nTriviaQA (Joshi et al., 2017) and CoQA (Reddy et al., 2019). We\nalso perform additional studies to identify which parts of the LLM’s\noutput are most useful to predict confidence.\n6.1\nCalibrating LLMs with Auxiliary Models\nMethod\nBlack-box LLM?\nConsistent?\nCalibrated?\nSequence likelihoods\n✗\n✔\n✗\nVerbalized uncertainty\n✔\n✗\n✗\nAPRICOT\n(ours)\n✔\n✔\n✔\nTable 6.1: Comparison of appealing attributes that LLM confidence\nquantification techniques should fulfill. They should ideally be applicable\nto black-box LLMs, be consistent (i.e. always elicit a response that\nindicates confidence in contrast to an unrelated response), and produce\ncalibrated estimates of confidence.\n6.1 calibrating llms with auxiliary models\n142\nWhat is the largest EU\ncountry by population?\nIt's Germany.\nAuxiliary Model\nTarget LLM\nUser\nQuestion\nLLM Answer\nConfidence: 0.63   \nInput\nFigure 6.1: Illustration of APRICOT\n: We train an auxiliary model to\npredict a target LLM’s confidence based on its input and the generated\nanswer.\nEstimating the confidence of an LLM can be challenging, since their\nsize rules out many traditional techniques that require finetuning\nor access to model parameters. In this light, using the likelihood of\nthe generated sequence might seem like an appealing alternative;\nhowever, it may not actually reflect the reliability of the model’s\nanswer and often cannot be retrieved when using black-box models,\nwhere the only output is the generated text. Verbalized uncertainty,\ni.e. prompting the LLM to express its uncertainty in words, can be\na solution when the model is powerful enough. But as we later\nshow in Section 5.4, the generated confidence expressions are\nnot very diverse, and results are not always consistent, meaning\nthat the model does not always generate a desired confidence\nself-assessment. We will later see how for verbalized uncertainty\nfor instance, models sometimes respond with unrelated answers,\neven when prompted to express their uncertainty. As we illustrate\nin Table 6.1, our method, APRICOT\n, fulfills all of these criteria:\nThrough a one-time finetuning procedure of an auxiliary model\non the target LLMs outputs, we have full control over a cali-\nbrated model that gives consistent and precise confidence estimates.\nIn Figure 6.2 we give an overview of APRICOT\n, which con-\nsists of three main steps: Firstly, we prompt the target LLM to\ngenerate training data for our auxiliary model (Section 6.1.1). Sec-\nondly, we set calibration targets in a way that does not require ac-\ncess to the target LLM beyond its generated outputs (Section 6.1.2).\nLastly, we train the auxiliary calibrator to predict the target LLM’s\n6.1 calibrating llms with auxiliary models\n143\nTarget LLM\nAuxiliary Model\nQuestion\nLLM Answer\nQuestion\nLLM Answer\nCluster Accuracy\nQuestion\nEmbeddings\n(0.63   - 0.75)2\n\"What is the capital of France?\"\n\"Capital of Italy?\"\nFigure 6.2: Full overview over APRICOT\n. We collect a LLM’s answer\nto a set of questions and embed the latter using an embedding model.\nAfter clustering similar questions and identifying the LLM’s accuracy\non them, we can use this value as reference when training to predict the\nconfidence from a question-answer pair.\nconfidence for a given question (Section 6.1.3).69 Thereby, we add\ntwo parts that are agnostic to the LLM in question: A method\nthat determines calibration targets, and their prediction through\nthe auxiliary model. Note that we use the terms auxiliary model\nor calibrator interchangeably in the following sections.\n6.1.1\nPrompting the Target LLM\nIn the first step, we generate finetuning data for the auxiliary model\nby prompting the target LLM on the given task. Here, we explore\ndifferent variations to see which model response might provide the\nbest training signal for the auxiliary calibrator. More concretely,\nwhile the original prompt and model generation might already\nsuffice to predict the model’s confidence, we also ask the model\nto elaborate on its answer using chain-of-thought prompting (Wei\net al., 2022). We hypothesize that including additional reasoning\n69In general, using secondary neural models to predict properties of the generated\ntext also has connections to other tasks such as translation quality estimation\n(Blatz et al., 2004; Quirk, 2004; Wang et al., 2019; Glushkova et al., 2021;\nZerva et al., 2022), toxicity classification (Maslej-Krešňáková et al., 2020) or\nfine-grained reward modeling (Wu et al., 2023b).\n6.1 calibrating llms with auxiliary models\n144\n(a) Default prompting.\n(b) Chain-of-though prompting.\n(c) Prompting with verbalized confidence.\nFigure 6.3: Illustration of the prompting strategies used to generate the\ninput data for the auxiliary calibrator. Note that (c) can also involve\nconfidence expressed in words (“My confidence level is low”) and that\n(b) and (c) can be combined.\nsteps exposes signals that are useful for the calibrator.70\nWe\nfurthermore take a model’s assessment of its confidence into account,\ntoo. Recent works on verbalized uncertainty (Lin et al., 2022a; Tian\net al., 2023) investigated how to elicit such an assessment as a\npercentage value, e.g. “I am 95 % confident in my answer”, or using\nlinguistic expressions such as “My confidence is somewhat low”.\nWhile previous studies like Zhou et al. (2024) have demonstrated\nthe difficulty in obtaining reliable self-assessments, we can just treat\nverbalized uncertainties as additional input features, and let their\nimportance be determined through the auxiliary model training.\nWe illustrate the different prompting strategies in Figure 6.3 and\nelaborate on the prompts in the following.\n70We do this while acknowledging evidence by Turpin et al. (2023) that shows\nthat any chain-of-thought reasoning might not reflect the actual reasons for\na specific model response. Nevertheless, even if chain-of-thought reasoning\ndoes not unveil the actual process of the LLM, it can provide useful textual\nfeatures to the auxiliary model, including unexpected intermediate result or\nlinguistic markers of uncertainty.\n6.1 calibrating llms with auxiliary models\n145\nPrompt Design.\nWe use a simple prompt for question-\nanswering, where we fill in a template of the form “Question:\n{Question} Answer:”.\nFor in-context samples, we prepend the\ndemonstrations to the input, using the sample template as above.\nIn the case of chain-of-thought prompting, we use the prompting\nbelow:\nQA Chain-of-thought prompt\nBriefly answer the following question by thinking step by step.\nQuestion: {Question} Answer:\nIn the case where the question is supposed to be answered given\nsome context, we slightly change the prompt design:\nChain-of-thought prompt with context\nContext: {Context}\nInstruction: Briefly answer the following question by thinking\nstep by step.\nQuestion: {Question}\nAnswer:\nHere, the passage that questions are based on is given first, and\nchain-of-thought prompting is signaled through the “Instruction”\nfield. When no chain-of-thought prompting is used, this field is\nomitted.\nFor the verbalized uncertainty, we use the following\nprompts, in which case we omit any in-context samples:\nVerbalized uncertainty prompt (quantitative)\n{Question} {Model answer} Please provide your confidence in\nthe answer only as one of ’Very Low’ / ’Low’ / ’Somewhat Low’\n/ ’Medium’ / ’Somewhat High’ / ’High’ / ’Very High’:\nVerbalized uncertainty prompt (qualitative)\n{Question} {Model answer} Please provide your confidence in\nthe answer only in percent (0–100 %):\nWe follow Kuhn et al. (2023) and use 10 in-context samples for\nthe original answer, which are randomly sampled from the training\nset (but in contrast to Kuhn et al., we sample different examples\nfor each instance). When prompting for verbalized uncertainty, we\nremove these in-context samples. Additionally, verbalized uncer-\n6.1 calibrating llms with auxiliary models\n146\ntainty expressions such as ‘very low’ or ‘high’ are mapped back onto\nthe following numerical values for evaluation purposes (in the order\nof appearance in the template above): 0, 0.3, 0.45, 0.5, 0.65, 0.7, 1.71\n6.1.2\nSetting Calibration Targets\nAfter explaining the inputs to the auxiliary model, the question\nnaturally arises about what the calibrator should be trained to\npredict. The work by Mielke et al. (2022) introduces an additional\nmodel that simply predicts the correctness of an individual answer\n(and does so by using the target model’s internal hidden states,\nwhich is not possible for black-box models). We test this type of\noutput in Section 6.2.2, but we also show that we can produce\nbetter calibration targets through clustering.\nRecall the notion of calibration and calibration error from\nSection 2.2.1, where we saw that the expected calibration error can\nbe approximated by binning points into buckets Bm (Naeini et al.,\n2015) by confidence:\nM\nX\nm=1\n|Bm|\nN\n\f\f\f\n1\n|Bm|\nX\ni∈Bm\n1\n\u0000ˆyi = yi\n\u0001\n|\n{z\n}\nBin accuracy (target)\n−\n1\n|Bm|\nX\ni∈Bm\nˆpi\n|\n{z\n}\nAvg. bin confidence\n\f\f\f,\n(6.1)\nwhere ˆpi corresponds to some confidence score. Our key insight is\nhere that we can optimize the expected calibration error through\na similar approximation as in Equation (6.1), without changing\nthe LLM’s original answers or access to token probabilities. We\ncan abstract the idea in Equation (6.1) as aggregating samples\nin homogeneous groups (in the above case, groups of similar\nconfidence), and measuring the group-wise accuracy. But now,\ninstead of creating bins Bm by confidence, which is not possible in\na black-box setting, we create clustered sets Cm of questions with\nsimilar sentence embeddings. Calibration targets are then obtained\nby using the average accuracy of the LLMs answers per question\nset Cm. This is similar to the method of Lin et al. (2022a), who\nconsider the accuracy per question category (e.g. multiplication or\naddition math questions). Yet in the absence of such additional\n71The choice of these specific numbers is admittedly somewhat arbitrary, and\nother works such as Lin et al. (2022a) have also employed similarly heuristic\nscales. Tian et al. (2023) motivate their mapping from expression to probabili-\nties to a social media survey by Fagen-Ulmschneider (2015), where respondents\nwere asked to assign probabilities to different expressions. However, these\nvalues vary greatly between participants, and thus assigning a single numerical\nvalue is still challenging.\n6.1 calibrating llms with auxiliary models\n147\ncategorization data, we expect good embedding and clustering\nalgorithms to roughly group inputs by category.\nHöltgen and\nWilliamson (2023) also echo a similar idea of more generalized\ngrouping choices, describing how ECE’s grouping by confidence\ncan be abstracted to other kinds of similarities.\nThey also\nprovide a proof that the calibration error of a predictor based on a\nk-nearest neighbor clustering tends to zero in the infinite data limit.\nImplementation.\nPractically, we embed questions into a latent\nspace using a light-weight model such as SentenceBert (Reimers\nand Gurevych, 2019), normalize the embeddings along the fea-\nture dimension (Timkey and van Schijndel, 2021), and then use\nHDBSCAN (Campello et al., 2013), an unsupervised, bottom-up\nclustering algorithm, to cluster them into questions of similar topic.\nThe use of HDBSCAN has multiple advantages: Compared to e.g.\nk-means, we do not have to determine the numbers of clusters in\nadvance, and since the clustering is conducted bottom-up, clusters\nare not constrained to a spherical shape. Furthermore, compared\nto its predecessor DBSCAN (Ester et al., 1996), HDBSCAN does\nnot require one to determine the minimum distance between points\nfor clustering manually. We evaluate this procedure in Section 6.2.1\nand Appendix B.9.\n6.1.3\nTraining the Auxiliary Model\nAfter determining the input and the training targets for the auxil-\niary model in the previous sections, we can now describe the actual\ntraining procedure that makes it predict the target LLM’s confi-\ndence. To start, we feed the questions alongside some in-context\nsamples into our target LLM. We retain the generated answers and\ncreate a dataset that combines the question (without in-context\nsamples) and the target model’s answers. These are used to train\nthe auxiliary calibrator to predict the calibration targets obtained\nby the clustering procedure above. In our experiments, we use\nDeBERTaV3 (He et al., 2023b), an improvement on the original\nDeBERTa model (He et al., 2021b) using variety of improvements\nwith respect to its architecture and pre-trainign objective. We then\nfinetune it using the AdamW optimizer (Loshchilov and Hutter,\n2018) in combination with a cosine learning rate schedule. We min-\nimize the following mean squared error, where ˆpi is the predicted\n6.2 experiments\n148\nconfidence, C(i) the cluster that the input question with index i\nbelongs to, and ˆaj an answer given by the target LLM:\nL\n\u0000ˆpi, C(i)\n\u0001\n=\n\u0010\nˆpi −\n1\n|C(i)|\nX\nj∈C(i)\n1\n\u0000ˆaj is correct\n\u0001\n|\n{z\n}\nCluster accuracy (target)\n\u00112\n.\n(6.2)\nWe also explore a variant that simply predicts whether the LLM’s\nanswer is expected to be correct or incorrect, so in this case, we\nsimply optimize a binary cross-entropy loss:\nL\n\u0000ˆpi, ˆai\n\u0001\n= 1\n\u0000ˆai is correct\n\u0001\nlog ˆpi+\n\u00001−1\n\u0000ˆai is correct\n\u0001\u0001\nlog(1−ˆpi).\n(6.3)\nAlthough omitted here for clarity, the actual loss also uses loss\nweights to balance the unequal distribution of correct and incorrect\nlanguage model answers.72 Finally, we select the final model via\nthe best loss on the validation set. We determine the learning rate\nand weight decay term through Bayesian hyperparameter search\n(Snoek et al., 2012), picking the best configuration by validation\nloss. We detail search ranges and found values in Appendix C.4.4.\nTraining hardware and the environmental impact are discussed in\nAppendix C.2.\n6.2\nExperiments\nWe now demonstrate how APRICOT\nprovides a simple yet\neffective solution to calibrate LLMs. Before assessing the quality of\nthe unsupervised clustering to determine calibration targets from\nSection 6.1.2, we first introduce the dataset and models.\nDatasets.\nWe employ TriviaQA (Joshi et al., 2017), a common\n(closed-book) question-answering dataset. Open-ended question-\nanswering is an ideal testbed for natural language generation tasks,\nsince it is comparatively easy to check whether an answer is correct\nor not, so calibration has an intuitive interpretation. To preprocess\nTriviaQA, we create a training set of 12k examples and choose\nanother 1.5k samples as a validation and test split, respectively.73\nSecondly, we run experiments on CoQA (Reddy et al., 2019), a\nconversational question-answering dataset in which the model is\nquizzed about the information in a passage of text. We treat the\n72The loss weights are based on scikit-learn’s implementation using the “bal-\nanced” mode, see https://scikit-learn.org/stable/modules/generated/\nsklearn.utils.class_weight.compute_class_weight.html.\n73Since the original test split does not include answers, we generate the validation\nand test split from the original validation split.\n6.2 experiments\n149\ndataset as an open-book dataset, where the model is shown the\npassage and then asked one of the corresponding questions at a\ntime. We extract a subset of the dataset to match the split sizes\nof TriviaQA.\nModels.\nWe test two models settings: A white-box setting,\nwhere the model can be run locally and we have full access to its\ninternals, and a black-box setting, where the model is only available\nthrough an API, drastically reducing the options for uncertainty\nquantification methods. For our white-box model experiments,\nwe choose a 7 billion parameter variant of the Vicuna v1.5 model\n(Zheng et al., 2023),74 an instruction-finetuned model originating\nfrom Llama 2 (Touvron et al., 2023a). For the black-box model,\nwe opt for OpenAI’s GPT-3.5 (OpenAI, 2022).75 Despite recent\nAPI changes granting access to token probabilities,76 creating\nmethods for black-box confidence estimation is still relevant for\nmultiple reasons: Token probabilities are not available for most\nblack-box models, they might be removed again to defend against\npotential security issues; and they are not always a reliable proxy\nfor confidence.\n6.2.1\nSetting Calibration Targets by Clustering\nBefore beginning our main experiments, we would like to verify\nthat our proposed methodology in Section 6.1.2 is sound.\nIn\nparticular, clustering the embeddings of questions and computing\nthe calibration confidence targets rests on the assumption that\nsimilarly-themed questions are collected in the same cluster. Ideally,\nwe would like to check this using metadata, which however is usually\nnot available.\nSetup.\nInstead, we evaluate this through different means: We\nfirst use the all-mpnet-base-v2 model from the sentence trans-\nformers package (Reimers and Gurevych, 2019) and HDBSCAN\nwith a minimum cluster size of 3 to cluster questions. We then ana-\nlyze the textual and semantic similarity of questions in a cluster by\ncomputing the average pair-wise ROUGE-L score (semantic; Lin,\n2004)77 between questions, and cosine similarities between question\nembeddings of the same cluster (semantic). Since we assume the\n74https://huggingface.co/lmsys/vicuna-7b-v1.5.\n75Specifically, using version gpt-3.5-turbo-0125.\n76https://x.com/OpenAIDevs/status/1735730662362189872 (last accessed\non 16.01.24).\n77As implemented by the evaluate package, see https://huggingface.co/\ndocs/evaluate/index.\n6.2 experiments\n150\nTriviaQA\nCoQA\nTextual\nSemantic\nTextual\nSemantic\nRandom\n.11 ±.08\n.00 ±.08\n.08 ±.12\n.00 ±.12\nClustering\n.39 ±.28\n.60 ±.14\n.47 ±.25\n.70 ±.17\nTable 6.2: Results of evaluation of found clusters on TriviaQA and\nCoQA, including one standard deviation. Textual refers to similarity\nscores computed using ROUGE-L, and semantic scores based on cosine\nsimilarities of question embeddings of the same cluster. Here, we use\nrandom comparisons between questions in the dataset as a baseline.\nsentence embedding model to capture the meaning of a sentence,\nthe expect the semantic similarity to be high when questions are\nsimilar in topic, but might differ in their choice of words. Since\nperforming this evaluation on the entire dataset is computationally\nexpensive, we approximate the score by using 5 pairwise compar-\nisons per cluster, with 200 comparisons for ROUGE-L and 1000 for\ncosine similarity in total, respectively. As a control for our method\n(clustering), we also compute values between unrelated questions\nthat are not in the same cluster (random).\nResults.\nWe show the results of this analysis in Table 6.2. We\nobserve noticeable differences between the random baseline and\nthe similarity for the clustering scores, both on a textual and\nsemantic level. While there is smaller difference on a textual level\ndue to the relatively similar wording of questions, the semantic\nsimilarity based on the encoded questions is very notable. We\nprovide deeper analyses of this part in Appendix B.9, showing\nthat this method creates diverse ranges of calibration confidence\ntargets. This suggests two things: On the one hand, our proposed\nmethodology is able to identify fine-grained categories of questions.\nOn the other hand, the diversity in calibration targets shown in\nAppendix B.9 indicates that we detect sets of questions on which the\nLLM’s accuracy varies—and that this variety should be reflected.\nWe test the ability of different methods to do exactly this next.\n6.2.2\nCalibrating White and Black-Box Models\nNext, we test whether auxiliary models can reliably predict the\ntarget LLM’s confidence. We describe our experimental conditions\nbelow.\n6.2 experiments\n151\nEvaluation metrics.\nAside from reporting the accuracy on\nthe question-answering task, we also report several calibration\nmetrics, including the expected calibration error (ECE;Naeini et al.,\n2015) using 10 bins. In order to address any distortion of results\nintroduced by the binning procedure, we use smooth ECE (smECE;\nBłasiok and Nakkiran, 2023), which avoids the binning altogether\nby smoothing observations using a radial basis function kernel. We\nalso consider Brier score (Brier, 1950), which can be interpreted as\nmean-squared error for probabilistic predictions. We further show\nhow indicative the predicted confidence is for answering a question\nincorrectly by measuring the AUROC. The AUROC treats the\nproblem as a binary error detection task based on the confidence\nscores, aggregating the results over all possible decision thresholds.\nIn each case, we report the result alongside a bootstrap estimate\nof the standard error (Efron and Tibshirani, 1994) estimated from\n100 samples and test for significance using the almost stochastic\norder test (del Barrio et al., 2018a; Dror et al., 2019; Ulmer et al.,\n2022c) with τ = 0.35 and a confidence level of α = 0.1.\nBaselines.\nTo contextualize the auxiliary calibrator results, we\nconsider the following baselines: We consider the raw (length-\nnormalized) sequence likelihoods (Seq. likelihood) as well as variant\nusing Platt scaling (Platt et al., 1999): Using the raw likelihood\nˆp ∈[0, 1] and the sigmoid function σ, we fit two additional scalars\na, b ∈R to minimize the mean squared error on the validation set\nto produce a calibrated likelihood ˆq = σ(aˆp + b) while keeping all\nother calibrator parameters fixed. We also compare it to the recent\nmethod of verbalized uncertainty (Lin et al., 2022a; Tian et al.,\n2023), where we ask the model to assess its confidence directly.\nWe do this by asking for confidence in percent (Verbalized %)\nand using a seven-point scale from “very low” to “very high”, and\nwhich is mapped back to numeric confidence scores (Verbalized\nQual.). Where applicable, we also distinguish between baselines\nwith and without chain-of-thought prompting (CoT; Wei et al.,\n2022). For our approach, we distinguish between confidence targets\nobtained through the procedure in Section 6.1.2 (clustering) and\nsimply predicting whether the given answer is correct or incorrect\n(binary).\nResults.\nVicuna v1.5 7B achieves 58% accuracy on TriviaQA\nand 44% on CoQA, while GPT-3.5 obtains 85% and 55% accuracy,\n6.2 experiments\n152\nTriviaQA\nCoQA\nMethod\nBrier↓\nECE↓\nsmECE↓\nAUROC↑\nBrier↓\nECE↓\nsmECE↓\nAUROC↑\nVicuna v1.5 (white-box)\nSeq. like.\n.22 ±.01\n.05 ±.00\n.03 ±.00\n.79 ±.01\n.32 ±.01\n.08 ±.00\n.08 ±.00\n.69 ±.01\nSeq. like. (CoT)\n.25 ±.01\n.04 ±.00\n.04 ±.00\n.70 ±.01\n.35 ±.01\n.04 ±.00\n.05 ±.00\n.61 ±.01\nPlatt\n.24 ±.00\n.08 ±.00\n.07 ±.00\n.70 ±.01\n.30 ±.00\n.03 ±.00\n.03 ±.00\n.69 ±.01\nPlatt (CoT)\n.24 ±.00\n.12 ±.00\n.11 ±.00\n.79 ±.01\n.30 ±.00\n.02 ±.00\n.02 ±.00\n.61 ±.01\nVerb. Qual.\n.38 ±.03\n.02 ±.00\n.02 ±.00\n.62 ±.03\n.45 ±.01\n.00 ±.00\n.00 ±.00\n.48 ±.01\nVerb. Qual. (CoT)\n.39 ±.02\n.01 ±.00\n.01 ±.00\n.60 ±.02\n.45 ±.01\n.00 ±.00\n.00 ±.00\n.48 ±.01\nVerb. %\n.39 ±.01\n.38 ±.00\n.27 ±.00\n.52 ±.01\n.49 ±.01\n.48 ±.00\n.32 ±.00\n.53 ±.01\nVerb. % (CoT)\n.39 ±.01\n.38 ±.00\n.26 ±.00\n.49 ±.01\n.48 ±.01\n.06 ±.00\n.06 ±.00\n.55 ±.01\nAux. (binary)\n.20 ±.01\n.16 ±.01\n.15 ±.01\n.81 ±.01\n.20 ±.01\n.16 ±.01\n.15 ±.01\n.82 ±.01\nAux. (clustering)\n.18 ±.00\n.09 ±.01\n.09 ±.01\n.83 ±.01\n.18 ±.00\n.04 ±.01\n.04 ±.01\n.82 ±.01\nGPT-3.5 (black-box)\nSeq. like.\n.15 ±.01\n.04 ±.00\n.04 ±.00\n.69 ±.02\n.29 ±.01\n.11 ±.00\n.11 ±.00\n.70 ±.01\nSeq. like. (CoT)\n.14 ±.00\n.05 ±.00\n.05 ±.00\n.60 ±.02\n.25 ±.00\n.01 ±.00\n.02 ±.00\n.52 ±.02\nPlatt\n.15 ±.00\n.04 ±.00\n.04 ±.00\n.69 ±.02\n.26 ±.01\n.03 ±.00\n.03 ±.00\n.70 ±.01\nPlatt (CoT)\n.15 ±.00\n.12 ±.00\n.12 ±.00\n.60 ±.02\n.25 ±.00\n.06 ±.00\n.06 ±.00\n.52 ±.02\nVerb. Qual.\n.14 ±.01\n.07 ±.00\n.04 ±.00\n.61 ±.02\n.27 ±.00\n.07 ±.00\n.05 ±.00\n.52 ±.01\nVerb. Qual. (CoT)\n.15 ±.00\n.04 ±.00\n.03 ±.00\n.63 ±.02\n.30 ±.01\n.08 ±.01\n.04 ±.00\n.50 ±.01\nVerb. %\n.13 ±.01\n.01 ±.00\n.01 ±.00\n.63 ±.02\n.34 ±.01\n.25 ±.00\n.22 ±.00\n.54 ±.01\nVerb. % (CoT)\n.13 ±.01\n.00 ±.00\n.01 ±.00\n.63 ±.02\n.37 ±.01\n.09 ±.01\n.06 ±.00\n.49 ±.02\nAux. (binary)\n.14 ±.00\n.14 ±.01\n.14 ±.01\n.65 ±.02\n.19 ±.01\n.13 ±.01\n.13 ±.01\n.81 ±.01\nAux. (clustering)\n.12 ±.01\n.06 ±.01\n.06 ±.01\n.72 ±.02\n.18 ±.00\n.02 ±.01\n.02 ±.00\n.81 ±.01\nTable 6.3: Calibration results for Vicuna v1.5 and GPT-3.5 on Trivi-\naQA and CoQA. We bold the best results per dataset and model, and\nunderline those that are statistically significant compared to all other\nresults assessed via the ASO test. Results are reported along with a\nbootstrap estimate of the standard error.\nrespectively.78\nWe present the calibration results in Table 6.3.\nAPRICOT\nachieves the highest AUROC in all settings and\namong the lowest Brier scores and calibration errors.\nOn the\nlatter metric, verbalized confidence beats our method, but often\nat the cost of a higher worst-case calibration error and lower\nAUROC. The effect of CoT prompting on calibration, however,\nremains inconsistent across different baselines. Lastly, APRICOT\nwith clustering beats the use of binary targets for Vicuna v1.5\nand GPT-3.5 on both TriviaQA and CoQA. We also juxtapose\nreliability diagrams for the different methods for Vicuna v1.5 on\nTriviaQA in Figure 6.4 (we show the other reliability diagrams,\n78We use the same heuristic based on thresholded ROUGE-L scores as in\nSection 3.2.3 or Kuhn et al. (2023) to determine whether an answer is correct.\nSince GPT-3.5 is a closed-source model, it is hard to say whether the higher\naccuracy scores are due to better model quality, test data leakage, or overlap\nin questions in the case of TriviaQA (Lewis et al., 2021).\n6.2 experiments\n153\n(a) Seq. likelihood.\n(b) Seq. likelihood (CoT).\n(c) Platt scaling.\n(d) Platt scaling (CoT).\n(e) Verbalized Qual.\n(f) Verbalized %.\n(g) Auxiliary (binary). (h)\nAuxiliary\n(cluster-\ning).\nFigure 6.4: Reliability diagrams for our different methods using 10 bins\neach for Vicuna v1.5 on TriviaQA. The color as well as the percentage\nnumber within each bar indicate the proportion of total points contained\nin each bin.\nincluding for GPT-3.5, in Appendix B.10). Here it becomes clear\nthat verbalized uncertainties approaches usually do not emit a wide\nvariety of confidence scores. This is in line with observations by\nZhou et al. (2023), who hypothesize the distribution of expressions\ngenerated by verbalized uncertainty heavily depend on the mention\nof e.g. percentage values in the model’s training data.\nWhile\nFigure B.23 shows that GPT-3.5 provides more variety in this\nregard, the overall phenomenon persists.\nConsistency of Verbalized Uncertainty.\nWhile verbalized\nuncertainties often perform well according to calibration error,\n6.2 experiments\n154\nVicuna v1.5\nGPT-3.5\nMethod\nTriviaQA\nCoQA\nTriviaQA\nCoQA\nVerb. Qual.\n.19\n.66\n1.00\n1.00\nVerb. Qual. (CoT)\n.25\n.73\n1.00\n1.00\nVerb. %\n1.00\n.99\n1.00\n1.00\nVerb. % (CoT)\n1.00\n.99\n.99\n.58\nTable 6.4: Consistency of verbalized uncertainty methods for Vicuna\nv1.5 and GPT-3.5 on TriviaQA and CoQA.\nthese results have to be taken with a grain of salt: Especially for\nthe relatively small 7B Vicuna v1.5 model, the generations do\nnot always contain the desired confidence expression, as visible\nby the low consistency in Table 6.4.\nCoT prompting seems\nto increase the success rate of verbalized uncertainty, and the\nadditional results on GPT-3.5 suggests that this ability might also\nbe dependent on model size. But even when taking the generated\nconfidence expression, their ability to distinguish potentially\ncorrect from incorrect LLM responses remains at or close to\nrandom level. This suggests that due to the skewed distribution\nof confidence expressions, they can only be well-calibrated\non datasets which are easy for the underlying model, which,\nnaturally, is not known a priori. Next, we conduct some addi-\ntional analyses based on the clustering-based variant of our method.\n6.2.3\nAblation Study\nThe previous results pose the question of which parts of input the\nauxiliary model actually learns from. So, analogous to the different\nprompting strategies in Figure 6.3, we explore different input\nvariants: First, we test a question-only setting, where the target\nLLM’s answer is omitted completely. We also test the performance\nof the calibrator when given more information, for instance the\nmodel answer with and without chain-of-thought prompting, which\ncould potentially expose flaws in the LLM’s response.79 Finally, we\nalso expose the verbalized uncertainty of the LLM to the calibrator.\nResults.\nWe show these results in Table 6.5 in Appendix B.10.\nInterestingly, we can observe that even based on the question to\n79Based on the recent study by Turpin et al. (2023), we assume that CoT does\nnot expose the LLM’s actual reasoning. Nevertheless, it provides more context\nabout the given answer.\n6.3 discussion\n155\nAuxiliary Model Input\nTriviaQA\nCoQA\nQuest.\nAns.\nCoT\nVerb.\nBrier↓\nECE↓\nsmECE↓\nAUROC↑\nBrier↓\nECE↓\nsmECE↓\nAUROC↑\nVicuna v1.5 (white-box)\n✔\n✗\n✗\n✗\n.21 ±.00\n.07 ±.01\n.06 ±.01\n.74 ±.01\n.22 ±.00\n.03 ±.01\n.03 ±.00\n.70 ±.01\n✔\n✔\n✗\n✗\n.18 ±.00\n.09 ±.01\n.09 ±.01\n.83 ±.01\n.18 ±.00\n.04 ±.01\n.04 ±.01\n.82 ±.01\n✔\n✔\n✗\nQual.\n.18 ±.00\n.08 ±.01\n.08 ±.01\n.82 ±.01\n.19 ±.00\n.04 ±.01\n.04 ±.01\n.79 ±.01\n✔\n✔\n✗\n%\n.18 ±.00\n.07 ±.01\n.07 ±.01\n.82 ±.01\n.18 ±.00\n.03 ±.01\n.03 ±.01\n.80 ±.01\n✔\n✔\n✔\n✗\n.19 ±.01\n.07 ±.01\n.07 ±.01\n.80 ±.01\n.21 ±.00\n.04 ±.01\n.03 ±.01\n.74 ±.01\n✔\n✔\n✔\nQual.\n.19 ±.00\n.08 ±.01\n.08 ±.01\n.80 ±.01\n.22 ±.00\n.03 ±.01\n.03 ±.01\n.70 ±.01\n✔\n✔\n✔\n%\n.18 ±.00\n.07 ±.01\n.07 ±.01\n.81 ±.01\n.20 ±.00\n.03 ±.01\n.03 ±.00\n.75 ±.01\nGPT-3.5 (black-box)\n✔\n✗\n✗\n✗\n.12 ±.01\n.05 ±.01\n.05 ±.01\n.71 ±.03\n.21 ±.00\n.03 ±.01\n.03 ±.01\n.72 ±.01\n✔\n✔\n✗\n✗\n.12 ±.01\n.06 ±.01\n.06 ±.01\n.72 ±.02\n.18 ±.01\n.04 ±.02\n.04 ±.02\n.82 ±.02\n✔\n✔\n✗\nQual.\n.12 ±.01\n.03 ±.01\n.03 ±.01\n.72 ±.03\n.18 ±.01\n.02 ±.01\n.02 ±.00\n.80 ±.01\n✔\n✔\n✗\n%\n.12 ±.01\n.03 ±.01\n.03 ±.01\n.72 ±.02\n.18 ±.00\n.04 ±.01\n.03 ±.00\n.80 ±.01\n✔\n✔\n✔\n✗\n.12 ±.01\n.06 ±.01\n.06 ±.01\n.72 ±.02\n.21 ±.00\n.03 ±.01\n.03 ±.01\n.72 ±.01\n✔\n✔\n✔\nQual.\n.12 ±.01\n.04 ±.01\n.04 ±.01\n.73 ±.02\n.21 ±.00\n.04 ±.01\n.04 ±.01\n.72 ±.01\n✔\n✔\n✔\n%\n.12 ±.01\n.04 ±.01\n.04 ±.01\n.64 ±.02\n.21 ±.00\n.02 ±.01\n.02 ±.00\n.72 ±.01\nTable 6.5: Calibration results for Vicuna v1.5 and GPT-3.5 on TriviaQA\nand CoQA using the auxiliary (clustering) method. We bold the best\nresults per dataset, method and model.\nthe LLM alone, APRICOT\ncan already achieve respectable\nperformance across all metrics. This suggests that the calibrator at\nleast partially learns to infer the difficulty of the LLM answering\na question from the type of question alone.\nNevertheless, we\nalso find that adding the LLM’s actual answer further improves\nresults, with additional gain when using CoT prompting. In some\ncases, the calibration error can be improved when using the LLM’s\nverbalized uncertainties; in this sense, we can interpret the role\nof the calibrator as mapping the model’s own assessment to a\ncalibrated confidence score.\n6.3\nDiscussion\nDespite the difficulty of predicting the LLM’s confidence from its\ngenerated text alone, our experiments have shown that APRICOT\ncan be used to produce reasonable scores even under these strict\nconstraints. We showed in the past sections that the auxiliary\nmodel can be finetuned to learn from multiple signals. On the\none hand, the auxiliary calibrator learns a mapping from a latent\ncategory of question to the expected difficulty for a target LLM. On\nthe other hand, including the answer given through CoT prompting\nand including the LLM’s own assessment of its uncertainty helped\nto further improve results. While sometimes beaten in terms of\n6.4 summary\n156\ncalibration error, our method consistently outperforms our base-\nlines in error detection AUROC, meaning that it can provide the\nbest signal to detect wrong LLM answers. Compared to other\napproaches, this yields some desirable properties: APRICOT\nis\navailable when sequence likelihood is not; it is more reliable than\nverbalized uncertainty; and it only needs a light finetuning once,\nadding negligible inference overhead. Compared to other methods\nsuch as Kuhn et al. (2023); Lin et al. (2023) in Section 2.3, it also\ndoes not require more generations for the same input, reducing the\nmore expensive LLM inference costs.\nLimitations.\nWhile yielding generally positive results in our\ncase, the clustering methodology from Section 6.1.2 requires access\nto a sufficiently expressive sentence embedding model and a large\nenough number of data points. When this is not given, we show that\nthe binary approach—tuning the auxiliary model to predict errors—\nis a viable alternative. As any neural model, the auxiliary calibrator\nis vulnerable to distributional shift and out-of-distribution data.\nFurther research could help to understand how this issue can\nbe reduced and which parts of the input the model identifies to\npredict confidence scores in order to unveil potential shortcut\nlearning (Du et al., 2023). Our experiments focused on open-ended\nquestion-answering tasks, which provide a fairly easy way to check\nanswer correctness. In other types of language generation such as\nsummarization, translation or open text generation, this notion of\ncorrectness is not given.\n6.4\nSummary\nIn this chapter, we presented APRICOT\n, a general method to\nobtain confidence scores from any language model on the input\nand text output alone. We showed that it is possible to compute\ncalibration targets through the clustering of question embeddings.\nThrough the subsequent finetuning of a smaller language model, we\nthen outperform other methods to distinguish incorrect from correct\nanswers with competitive calibration scores, on different models\nand datasets. While we only presented a first, more fundamental\nversion this approach in this work, it lends itself naturally to a whole\nbody of research that aims to improve the calibration of pretrained\nlanguage models (Desai and Durrett, 2020; Jiang et al., 2021; Chen\net al., 2023b). Lastly, future studies might also investigate the\nuncertainty of the auxiliary model itself and use techniques such\nas conformal prediction in Section 2.2.1 to produce estimates of\nLLM confidence intervals.\n7\n|\nDiscussion\n“When Sha Monk opened up a scroll of scripture that the\nother two disciples were clutching, his eyes perceived only\nsnow-white paper without a trace of so much as half a letter\non it. Hurriedly he presented it to Tripitaka, saying, ‘Master,\nthis scroll is wordless!’ Pilgrim also opened a scroll and it,\ntoo, was wordless. Then Eight Rules opened still another\nscroll, and it was also wordless. ‘Open all of them!’ cried\nTripitaka. Every scroll had only blank paper.”\n—The Journey to the West (西游记), Ch. 94, as translated\nand edited by Anthony C. Yu (1977).\nThe last chapters have explored the various different definitions of\nand perspectives on uncertainty and how they materialize in the\nfields of machine learning and natural language processing. Despite\nthe usefulness of uncertainty quantification for a whole spectrum of\napplications (Section 2.6) and its importance to avoid negative out-\ncomes and to build trust in automation (Section 2.4), a somewhat\nfractured research landscape emerges: Uncertainty still remains a\nvery under-defined and under-researched topic, especially in natural\nlanguage processing. Uncertainty within the experimental pipeline\noften stays unaddressed or outright ignored; Uncertainty modeling\nposes a challenge under the current large language model paradigm\nand the successes and failures of uncertainty quantification are\nequally poorly understood. The efforts described in Chapters 3 to 6\ncan only work as a step to mitigate this fact, and thus dedicate this\nchapter to revisit the initial research goals defined in this thesis,\nand discuss a number of fundamental open questions and research\ndirections.\n7.1\nDiscussion of Research Questions\nThis thesis gave an overview over different notions of uncertainty\nfrom the perspectives of statistics, linguistics, deep learning and\nNLP in Chapter 2, also discussing how uncertainty can be commu-\nnicated and how it interacts with human-AI trust. The influence\n157\n7.1 discussion of research questions\n158\nof uncertainty on the experimental pipeline was analyzed in Chap-\nter 3, where we could see how more careful experimental design\nallows to quantify uncertainty in results, reduce it, and even open\nup new avenues for modeling it. Some of the limits of uncertainty\nquantification for text classification were demonstrated in Chap-\nter 4 using the theoretical case of ReLU networks and a large\nvariety of different models applied to text classification tasks in\nEnglish, Danish and Finnish. Lastly, non-exchangeable conformal\nprediction enables us to develop a method to obtain calibrated\ntoken sets for generation in Chapter 5 and APRICOT\n, a method\nto obtain calibrated confidence scores from black-box LLMs in\nChapter 6. Based on this research, we now return to the research\nquestions posed in Section 1.4 and discuss them in turn.\nRQ1: How can uncertainty in NLP be characterized?\nIn Chapter 2 we discussed the multi-faceted views on un-\ncertainty from a variety of perspectives, all of which coalesce\nin modern NLP applications.\nThis includes the linguistic\nuncertainties present in the input data, interacting with the\nstatistical uncertainties lingering in the modeling aspect.\nLinguistically, uncertainty materializes as an inherent property\nof language in the form of underspecification, ambiguity and\nvagueness (Section 2.1.3), but also as a tool for humans to express\ntheir state of knowledge about the world (Section 2.1.4; this can\nalso be used by language models to communicate uncertainty, see\nSection 2.5). Statistically, uncertainty is treated differently in\nthe frequentist and Bayesian school of thought: Frequentists see\nprobabilities as the relative frequency of an event under continued\nrepetitions of an experiment.\nBayesians interpret them as a\ndegree of belief, with the parameter of interest turning from an\nunobserved constant into a random variable. Both perspectives\nare echoed in the corresponding neural approaches: Calibration\ntechniques and conformal prediction on the one hand allow us\nto create confidence scores that reflect the correctness of the\nmodel, or prediction sets contain the ground truth in expectation.\nApproximating the neural weights posterior or parameterizing\nhigher-order distributions on the other hand permit a decoupling\nof different notions of uncertainty.\nThe latter notions mostly refer to predictive uncertainty and\nare for example quantified in terms such as the total, data, model\nand distributional uncertainties.\nAs Baan et al. (2023) point\n7.1 discussion of research questions\n159\nout, these can be seen as a spectrum, in contrast to a fixed set\nof discrete categories. This means that steps like data collection\ncan be a source of model uncertainty when data is scarce, and\ncan be reduced when more data is collected.\nHowever it can\nalso produce data uncertainty which, in some instances, can be\nreduced through e.g. better annotation guidelines. In this light,\nthe choice of method can be informed by the kind of uncertainty\nmost useful to the problem at hand, and if necessary and possible,\nthe experimental pipeline can be adapted to reduce uncertainty\nfurther or to enable better modeling of it (see next\nRQ2). For\nactive learning for instance, we might care most about epistemic\nor distributional uncertainty and therefore refer to Bayesian or\nevidential methods, while for error detection we might be satisfied\nwith easy-to-implement estimators of total uncertainty.\nIt should be noted though that almost all methods discussed\nso far quantify uncertainty statistically rather than linguistically.\nWhile verbalized uncertainty (Section 2.5) is a step towards ex-\npressing uncertainty in words, it (thus far) ignores the rich shades\nof meaning that are at a human speaker’s disposal (Figure 2.6).\nCommunicating uncertainty to humans can be challenging (Sec-\ntion 2.4), so more natural verbalized uncertainty could prove to be\na fruitful avenue of research.\nRQ2: How can choices in experimental design help to reduce\nand quantify uncertainty?\nIn Chapter 3, we discussed the role of uncertainty in experimen-\ntal design in NLP. There, we argued that careful data collection\ncan help to reduce uncertainty caused by noise, and enable new\nmodeling options through multiple annotations.\nFurthermore,\nhypothesis testing can help to quantify the uncertainty in results\nand aid model selection.\nUncertainty manifests in different stages of the experimental\nprocess and is often overlooked outside of the modeling stage;\nhowever, steps that are undertaken to increase reproducibility\ncan help to rein in uncertainty and open modeling options. In\nNLP, this is exemplified by publishing all instance annotations\n(instead of an aggregate) and embracing human disagreements\nwhich arise from the ambiguities in language (Section 2.3; Plank,\n2022; Baan et al., 2023).\nAs we discuss in Section 7.2, this\ncould for instance be combined with recent advances in eviden-\n7.1 discussion of research questions\n160\ntial deep learning to learn higher-order distributions (Section 2.2.3).\nAdditionally, comparing different models, prompts or other\nsettings can be difficult due to the non-linear nature of neural\nnetwork and their increasing model sizes. In Section 3.2.1, we\nshowed how to quantify this uncertainty in modeling results using\nthe ASO test. As the test is non-parametric, we do not require any\nknowledge of the underlying distribution of scores. In the case study\nin Section 3.2.3, we furthermore demonstrated that even though\nmodern LLMs tend to be pretrained, monolithic models, we can\nperform statistical hypothesis testing by obtaining observations\nfrom different prompts and thereby assessing their robustness\n(Mizrahi et al., 2024; Sclar et al., 2023). We also formalized the\ndifferent distributions that are compared—in the LLM setting\nfor instance, we keep the model architecture, pretraining data\nand hyperparameters constant while varying other factors such\nas prompt design and generation hyperparameters.\nGenerally\nspeaking, all of these settings vary a certain number of variables\non which the output is conditioned on, while keeping others fixed.\nAlthough many variations of this setup are plausible, we believe it is\nimportant to make underlying assumptions more explicit and vary\nas many variables as feasible in order to arrive at a well-rounded\nestimate of model performance.\nRQ3: How do inductive model biases influence\nuncertainty quantification?\nInductive biases describe the modeling assumptions present\nin a model’s architecture and training procedure. As we saw in\nChapter 4, this can have unintuitive effects on the efficacy of\nuncertainty estimates, where models may act confidently when\nfaced with OOD inputs.\nMany methods for uncertainty quantification equip a model\nwith some sort of metric that operates on the model’s output\nand translates it into a usually scalar measure of its uncertainty.\nWhile these have some expected or desired behaviors—such as the\npredictive entropy being high on OOD data—this is often not true\nin practice. This was illustrated for instance using ReLU networks\nin Section 4.1: Due to the inductive bias of the architecture, the\nnetwork induces linear decision regions in the feature space, leading\nuncertainty metrics to provably converge to fix points in the limit\n(instead of being sensitive to the degree of familiarity with an input).\n7.1 discussion of research questions\n161\nOne might criticize the argument about ReLU networks for\nbeing too simplistic, since modern deep learning architecture are\nmuch more complex; and while it is true that this fact prevents\nsimilar proofs, we empirically identified similar problems on a\nlarge variety of text classification models in Section 4.2.\nWe\nexplicitly tested a low-resource setting (simulated for English),\nwhere training data is scarce and behavior on OOD might be\nunreliable.\nBy testing on OOD test sets, we could show that\nsimilar failures occur in practice and that uncertainty measures are\nunable to effective distinguish in-distribution from foreign data.\nHow can we explain this behavior? One possible hypothesis\nis to look at this problem through the lens of the information\nbottleneck principle (Tishby et al., 2000; Tishby and Zaslavsky,\n2015; Saxe et al., 2018):\nNeural predictors often map input\nrepresentations into lower-dimensional latent spaces. This way,\nthey are incentivized during training firstly to recover the correct\nprediction, and secondly to compress the input in a way that\nsupports the first goal.\nIntuitively, we can assume that this\nlearned compression will favor features that are most useful\nto the predictive task, not necessarily ones that are useful to\nindicate uncertainty. Indeed, some works in anomaly detection\nhave noted that neural models might fail to encode novel, unseen\nfeatures that might indicate that a test point is out-of-distribution\n(Dietterich and Guyer, 2022; Sivaprasad and Fritz, 2023).\nIn\naddition, other works have noted how in- and out-of-distribution\nfeatures overlap in latent space (van Amersfoort et al., 2021). But\nthese features are exactly what should indicate model uncertainty,\nsince the model is likely to be misspecified on points different from\nthe training distribution! This means that this dynamic might\nmake uncertainty quantification unreliable in cases where we\ncannot obtain good estimates of epistemic uncertainty, or where\nepistemic uncertainty accounts for a large portion of the total\nuncertainty. In the theoretical analysis in Section 4.1, uncertainty\nestimates can still be useful in regions of class overlap (hence,\naleatoric uncertainty), but fail to be informative in regions without\nmodel training data due to their convergence to fix points. In\nthe empirical study in Section 4.2, we observe that the quality\nof uncertainty estimates can decrease as we add more training\ndata, potentially due to the selective compression phenomenon.\nFrom this we can deduce that the inductive biases of standard\narchitectures are insufficient for reliable uncertainty quantification,\nand better inductive biases are needed.\n7.1 discussion of research questions\n162\nOne possible solution of this lies in directly modeling the\ndata density.\nLanguage models do this already by assigning\nprobabilities to entire sequences; however, Section 6.2.2 and\nKumar and Sarawagi (2019) showed that sequence likelihoods are\ninsufficient for error prediction, and other studies such as Ren et al.\n(2022) have demonstrated their failure on OOD detection. This\ncan be explained by the fact that language models are trained on\nonly a single sequence in a combinatorically large space of possible\ncontinuations. This automatically implies a sort of data scarcity,\nwhere the model fails to adequately capture the paraphrasticity of\nlanguage (see Section 2.1.3). LeBrun et al. (2022) discovered how\nlanguage models tend to overestimate the probability of frequent\nsequences and underestimate the ones coming from the tail end of\nthe sequence distribution, with similar findings by Ilia and Aziz\n(2024); Liu et al. (2024a).80\nAs another approach to better inductive biases for UQ, one\nmight choose to model the distribution of latent representations\ninstead. This is for instance done through normalizing flows in the\ncase of posterior networks (Section 2.2.3) or some methods regard-\ning direct uncertainty prediction (Section 2.2.4). But since these\ncomponents are trained on the latent encodings of an underlying\nmodel, they can only learn the distribution of latent features that\nare learned by the main model, and might thus fall into the same\ntrap of not modeling features indicative of model uncertainty that\nwere “compressed away”. This can explain why the DDU Bert in\nSection 4.2.5 does not attain its best results on OOD detection\nthrough the log probability of its latent density estimator, and why\nposterior networks have been shown to not always detect OOD\nreliably (Kopetzki et al., 2021).\nRQ4: How can we address some of the challenges of\nuncertainty quantification in NLP?\nIn this thesis, we addressed multiple of the challenges that we\nlaid out in Section 1.3, including data scarcity and sequentiality.\nFor clarity, we will discuss them here in turn and the corresponding\ninsights gained from this work.\nChallenges of Natural Language.\nIn this thesis, we mainly\nworked towards solving two of the challenges that come with natural\n80This phenomenon might also be the culprit behind the inadequacy of sampling\nfrom the mode in NLG, see for instance Eikema and Aziz (2020); Holtzman\net al. (2020); Eikema (2024).\n7.1 discussion of research questions\n163\nlanguage data, namely its diversity and sequentiality. On the one\nhand, Section 4.2 tested different uncertainty methods for text\nclassification on three different languages and OOD test sets that\nintroduce novel domains. While general trends are visible across\nall settings, we can also see that the best uncertainty quality in\nterms of model and corresponding metric differs across datasets.\nThis suggests that there might be complex underlying interactions\nbetween the model and the types of uncertainty that OOD data\nevokes in it, the uncertainty quantification method, and language-\nspecific characteristics.81\nFor the non-exchangeable conformal\nlanguage generation in Chapter 5, we also tested on German and\nJapanese as different source language for the machine translation\ntask. We measured coverage, namely whether conformal prediction\nsets contain the ground truth continuation, and translation quality,\nbut found only minor differences between languages, with similar\ntrends across tested methods. Importantly, this method addresses\nthe sequentiality issue in natural language: Even though it is\npossible to conformalize language generation on a sequence-level\nwhere the i.i.d. assumption is maintained (see Quach et al., 2023),\nwe were able to provide a method on a token-level that provides\na well-motivated framework. This is different compared to cases\nlike Ravfogel et al. (2023), who operate on a token-level but have\nto make strong assumptions about the underlying data that might\nnot be realistic in practice.\nData Scarcity.\nIn Section 4.2, we explicitly tested low-resource\nsettings by using under-resourced languages such as Finnish and\nDanish, and by testing the relationship between training set size\nand uncertainty quality.\nUnsurprisingly, we showed that task\nperformance increases with the amount of data. More surprisingly,\nwe showed that increased amount of training data can have adverse\neffects on uncertainty quality on OOD inputs, for possible reasons\nwe discussed in the answer for\nRQ3.\nTrust & Safety.\nFirstly, this thesis introduced non-exchangeable\nconformal language generation in Chapter 5, which provides a way\nto produce sets of token for generation with conformal guaran-\ntees. Similarly to standard prediction sets in Section 2.2.1, other\nways of truncating the predictive distribution over tokens do not\nprovide any guarantees of containing the correct continuation. Nev-\n81The ability to model linguistic idiosyncrasy’s can to some degree also be\ninfluenced by the quality of tokenization and therefore the models’ uncertainty.\nFor investigation into the first point, refer e.g. to Graën et al. (2018); Virtanen\net al. (2019); Singh et al. (2019); Rust et al. (2021); Pfeiffer et al. (2021);\nMielke et al. (2021); Maronikolakis et al. (2021).\n7.1 discussion of research questions\n164\nertheless, these prediction sets can be conformalized through our\ncalibration method that utilizes information from nearest neigh-\nbors from a datastore. Not only does the generation process now\n(approximately) fulfill conformal guarantees, this also opens up new\npossibilities through the extension of (non-exchangeable) confor-\nmal risk control (Angelopoulos et al., 2023; Farinhas et al., 2024):\nFuture approaches could provide bounds on a wider family of\nfunctions, more instance measuring toxicity, veracity or alignment\nwith human values, similar to the works of Mohri and Hashimoto\n(2024); Gui et al. (2024). The latter has already been explored\nas an on-the-fly procedure (albeit, not conformal) instead of an\nadditional finetuning stage (Yang and Klein, 2021; Qin et al., 2022;\nMudgal et al., 2023; Gao et al., 2024a). The fact that conformal\nmethods can provide statistical guarantees for otherwise unwieldy\nlanguage models has also spurred additional work on the subject,\nfor instance conformalizing generation on a sequence-level (Quach\net al., 2023), for prompt selection (Zollo et al., 2023), conditional\ncomputation (Schuster et al., 2022; Ren et al., 2023), planning for\nLLM agents (Liang et al., 2024), and for black-box models (Su\net al., 2024). Secondly, for the most restrictive setup in which we\nare dealing with a black-box LLM and only have access to its input\nand generated text, we proposed APRICOT\nin Chapter 6. We\ndemonstrated that even in this context, using a secondary auxiliary\nmodel enables us to predict the target LLMs confidence reliably.\nWe also showed that by clustering the latent presentation of inputs,\nwe can use these clusters to obtain more fine-grained information\nabout the expected performance of the LLM on a certain category\nof inputs. While we leave further exploration of this question to\nfuture work, it is intuitive to assume that this very extreme setup\nhas limits on the reliability of confidence estimates. In this way,\nwe can order different methods on a spectrum from full access\nto the model, including latent representations, to access to logits\nand the predictive distribution to text-only access. Some works\nhave found that OOD inputs are detectable based on the model’s\nhidden representations (Yoo et al., 2022; Ren et al., 2022), with\nsimilar insights for hallucination detection (Ferrando et al., 2022;\nGuerreiro et al., 2023a; CH-Wang et al., 2023; Duan et al., 2024)\nand general uncertainty quantification (Vazhentsev et al., 2023; Liu\net al., 2024b), potentially suggesting a link back to the discussion\nabout encoded and undecoded latent features from the previous\nRQ3.\n7.2 open questions & future research directions\n165\n7.2\nOpen Questions & Future Research\nDirections\nThe answers to\nRQs1 to 4 can only provide partial steps towards\nsolving any of these complex questions. As this thesis has argued,\nthe topic of uncertainty quantification in NLP lies in the intersection\nof multiple different fields such as statistics, linguistics and deep\nlearning. It has only recently started to garner more attention,\nas for instance demonstrated by the first UncertaiNLP workshop\n(Vázquez et al., 2024), related surveys (Baan et al., 2023; Hu et al.,\n2023b; Geng et al., 2023; Campos et al., 2024) or other dissertations\n(He, 2024). This creates ample space for future research, which we\noutline next.\n7.2.1\nModeling Uncertainty\nOne focus of research about uncertainty in deep learning is—and\nhas been—its modeling. Despite the manifold of works in this\ndirection however, a number of many open directions of research\nremain. This includes everything from the modeling uncertainty on\ndifferent input scales, obtaining guarantees, and how to properly\nrepresent and explain it.\nInfluence of Experimental Design.\nChapter 3 has argued\nhow careful experimental design can reduce or help to quantify\nuncertainty, for instance by providing clearer annotation guidelines\nor model selection through statistical hypothesis testing. An of-\nten overlooked aspect is how retaining multiple human labels per\ntraining instance also opens up new avenues for better modeling of\nuncertainty and paraphrasticity (Plank, 2022; Baan et al., 2022,\n2023).\nUncertainty with Guarantees.\nPivotally, uncertainty quan-\ntification can only increase trust in ML systems when the estimate\nof uncertainty is itself reliable. As for instance Dhuliawala et al.\n(2023) showed, unreliable estimates can lead to a loss of trust in\nthe model that can be hard to recover from. Thus, conformal\nprediction currently is a very promising research direction, since\nit supplies statistical guarantees about predictions that are fur-\nthermore agnostic to the underlying predictor. This flexibility\nhas enables the flurry of conformal works in NLP (e.g. Schuster\net al., 2022; Ravfogel et al., 2023; Quach et al., 2023; Zollo et al.,\n2023; Su et al., 2024; Ulmer et al., 2024c; Campos et al., 2024).\nConformal prediction however comes with two caveats: Coverage\n7.2 open questions & future research directions\n166\nis only guaranteed in expectation, and is marginal rather than\nconditional, i.e. the guarantee is p(y′ ∈C(x′)) ≥1 −α rather than\np(y′ ∈C(x′) | x′) ≥1 −α. Unfortunately, conditional coverage\nis generally deemed unachievable under finite samples, with the\nguarantee approximately being fulfilled in some situations (Vovk,\n2012; Foygel Barber et al., 2021; Gibbs et al., 2023). Other ways\nto circumvent this issue lie in partitioning the dataset (similar to\nthe binning in the ECE, see Feldman et al., 2021; Gibbs et al.,\n2023; Jin and Ren, 2024) or conditioning on the label y∗instead of\nthe input (see mondrian conformal predictors; Vovk et al., 2005).\nTherefore, future research could investigate conformalizing other\nuncertainty methods or extending existing guarantees.\nHierarchical Uncertainty.\nCompared to other input modali-\nties such as images, uncertainty in NLP exists on different scales.\nStarting from (subword-)token uncertainty, uncertainty can also\nexist on a sequence, utterance, or paragraph or even dialogue-level.\nSo far, most uncertainty quantification techniques operate on a\ntoken-level or sequence-level, with pioneering work on higher scales\nsuch as the dialogue-level (Sicilia et al., 2024). While there are\nsome theoretical frameworks like Malinin and Gales (2021) to model\nhow uncertainty from tokens affects the uncertainty in sequences,\nthis is only given for certain metrics. Therefore, an open question\nremains how to estimate uncertainty on these different levels and\nhow uncertainty can be decomposed into smaller units.\nRepresenting Uncertainty.\nIn this thesis, we have mostly fo-\ncused on representing uncertainty in the form of single scalars or\nprediction sets. However, uncertainty can also be represented in\nmany other ways, for instance in the form of a posterior distribu-\ntion or the highest density interval in Section 2.1.2, uncertainty\nin the latent space (Kingma and Welling, 2014; Rezende et al.,\n2014; Daxberger and Hernández-Lobato, 2019; Kong et al., 2020b;\nMiani et al., 2022), or even linguistically (see discussion in Sec-\ntion 7.2.4). The representation of uncertainty should therefore not\nbe overly restrictive, embrace the richness in options and explore\nnew representations.\nQuantifying Human Uncertainty.\nMost of this thesis was\nfocused on modeling and quantifying the uncertainty in models\noperating on language data, but one might also want to model the\nhuman uncertainty underlying the data directly. First advances\nin this direction have been made by estimating the uncertainty in\nhuman labels (Northcutt et al., 2021; Jiang et al., 2023b; Gruber\net al., 2024), analyzing annotator disagreement (Baan et al., 2022,\n7.2 open questions & future research directions\n167\n2024) or comparing the variability of humans to that of NLG\nsystems (Giulianelli et al., 2023; Lee et al., 2023; Ilia and Aziz,\n2024). Furthermore, a number works try to model the uncertainty\nin humans using neural language models (Hu et al., 2023a) or try\nto detect linguistic uncertainty in text (Szarvas et al., 2012; Vincze,\n2014; Kolagar and Zarcone, 2024).\nExplaining Uncertainty.\nThe answer to\nRQ3 suggest a\nhypothesis with which the general behavior of uncertainty is in-\nfluenced by neural inductive biases. Nevertheless, there also lies\ntremendous value in understanding how uncertainty actually arises\nfor a specific input. This can for instance highlight erroneous or\nnoisy parts of an input or help to understand model failure cases\n(see e.g. Xu et al., 2020 for an application to text summarization).\nTo this extent, some works have began to apply interpretability\ntechniques to understand predictive uncertainty, including Shap-\nley values (Chen and Ji, 2022; Watson et al., 2024) or feature\nattribution methods (Bley et al., 2024).\n7.2.2\nLimits of Uncertainty Quantification\nAnother often overlooked aspect of uncertainty is defining or ex-\nploring the boundaries in which the model’s uncertainty is expected\nto operate; this includes in particular cases in which uncertainty\nestimates themselves might be uncertain, ill-defined, limited, or\nreductive, and which are open for further exploration.\nLimits of the Aleatoric–Epistemic Dichotomy.\nUncer-\ntainty, in a statistical sense, is traditionally delineated along data\n(aleatoric) and model (epistemic) uncertainty (Hora, 1996; Der Ki-\nureghian and Ditlevsen, 2009; Hüllermeier and Waegeman, 2021).\nHowever, recent works such as Baan et al. (2023); Gruber et al.\n(2023) have advocated to reject this dichotomy in favor of placing\nuncertainties and their sources on a spectrum. This dichotomy\nbecomes blurred further when considering that more far-reaching\ndecompositions are possible (for instance adding distributional\nuncertainty like in Section 2.2.3), and that estimates of epistemic\nuncertainty might be in themselves uncertain (Wimmer et al.,\n2023).\nHigher Order Uncertainties.\nEvidential deep learning (Sec-\ntion 2.2.3) and credal learning (Section 2.2.4) offer methods to\nmodel higher-order probability distributions or sets and quantify\ntheir uncertainty. Having said that, evidential deep learning in\nparticular has been criticized for not providing loss functions that\n7.2 open questions & future research directions\n168\ncan provably achieve well-behaved epistemic uncertainties in the\nmodel (Bengs et al., 2023), however alternative methods have been\nproposed for credal predictors (Hüllermeier et al., 2022; Sale et al.,\n2023a, 2024; Hofman et al., 2024).\nFeatures for Uncertainty Quantification.\nThe previously\nmentioned methods quantify uncertainty based on properties of\nthe underlying probability distribution parameterized by a neural\nnetwork. However, the considerations in\nRQ3 might prompt one\nto consider whether this should be the only source from which we\nshould deduce uncertainty. In the previous section we discussed for\ninstance modeling uncertainty in the latent space, and Section 6.2.3\nillustrated how, to some extent, we can infer uncertainty solely\nfrom the input to a model and train a secondary predictor to output\nuncertainty in a supervised learning task. Thus there remain many\navenues to explore to find the best features that can be used to\nobtain uncertainty estimates, which are already being explored by\nworks such as Fathullah et al. (2024); Liu et al. (2024b).\n7.2.3\nEvaluating Uncertainty\nOne common conundrum in the research surrounding uncertainty\nquantification is the lack of ground truth about a predictors\nuncertainty. Therefore—and in this regard Chapters 4 and 6 are\nno different—one has to instead defer to approximations and\nproxy tasks. For frequentists methods like confidence scores we\ncan measure calibration errors, but have to make do with binning,\nkernel estimators or other approximations.\nOtherwise we fall\nback other problems like error or OOD detection or measure\ncorrelations between predictive error and uncertainty.\nThese\nanalyses need to be multi-dimensional to be cogent and can be\ngamed; for example the SNGP Bert in Section 4.2.5 achieves\nhigh correlation between sequence uncertainties and loss by not\nconverging properly, and verbalized uncertainty by GPT-3.5 in\nSection 6.2.2 is well-calibrated on TriviaQA since the dataset is\ntoo easy, despite only articulating the same (high) confidence scores.\nYet when multiple annotations are available, we can actually use\nthis to our advantage to create a ground truth for uncertainty, as\ndone for instance by Baan et al. (2022); Ilia and Aziz (2024). Here,\nthe paraphrasticity of language can help to create ground truth\ndistributions whose uncertainty can be measure and compared\nagainst.\n7.2 open questions & future research directions\n169\n7.2.4\nCommunicating Uncertainty\nCommunicating uncertainty is difficult—Section 2.5 described how\ncommunicating uncertainty to different social groups while being\nboth understandable and precise is challenging, and how the process\ncan affect human-machine cooperations in sometimes unintuitive\nways. In this light, verbalized uncertainty (Section 2.3) seems\nlike an attractive tool for humanly intuitive ways of expressing\nuncertainty. But the experiments in Section 6.2.2 and studies such\nas (Tian et al., 2023) exemplified that such expressions tend to\ndisplay lopsided distributions of confidence that are not desirable.\nZhou et al. (2023) show how this behavior might be rooted in\nthe unequal distribution of these confidence expression (in their\ncase, percentage values) in the training data. This is not to say\nthat this approach is moribund: Works like Mielke et al. (2022);\nStengel-Eskin et al. (2024) train language models to produce more\ncomplex verbalized expressions of uncertainty, and Section 2.1.4\noutlines the richness of human uncertainty expressions that can\nserve as a guide for future research.\n8\n|\nConclusion\n“These intelligent agents are the only way to sift through the\noceans of data we are producing at an exponential rate [. . . ].\nIt is important if you find this terrifying or wonderful because\npublic sentiment drives education, investment and regulation.\nIf people find the rapid advance of intelligent machines\nterrifying instead of wonderful it won’t stop it, but it could\nmake the outcome worse for us all.”\n—Garry Kasparov in Deep Thinking (Kasparov, 2017).\nOn May 6th 2023, a document submitted to the United States\nDistrict Court of the Southern District of New York (The United\nStates District Court for the S.D.N.Y., 2013) reads:\n“The Court is presented with an unprecedented circumstance. A submission\nfiled by plaintiff’s counsel in opposition to a motion to dismiss is replete with\ncitations to non-existent cases. When the circumstance was called to the\nCourt’s attention by opposing counsel, the Court issued Orders requiring\nplaintiffs counsel to provide an affidavit [. . . ]. Six of the submitted cases\nappear to be bogus judicial decisions with bogus quotes and bogus internal\ncitations.”\nThe document was submitted by the judge in the case of\nRoberto Mata versus the Columbian airline Avianca. As it was\nrevealed later, the plaintiff’s lawyers used OpenAI’s ChatGPT to\nfind other relevant cases for their argument, which turned out\nto be non-existent.82 This curious case represents three different\naspects about AI in modern society at once: Firstly, AI in general\nand LLMs specifically are increasingly permeating society and\nculture.\nThis can be shown through their growing adoption\n(Humlum and Vestergaard, 2024), their impact on art (Zulić, 2019;\nDu, Wenda, and Han, Qing, 2021; Sivertsen et al., 2024) and by\nbecoming an progressively political issue (Hovy and Spruit, 2016;\n82See\nfor\nexample\nthe\ncorresponding\narticles\nby\nthe\nVerge\n(https:\n//www.theverge.com/2023/5/27/23739913/chatgpt-ai-lawsuit-\navianca-airlines-chatbot-research)\nor\nthe\nNew\nYork\nTimes\n(https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-\nlawsuit-chatgpt.html). Both were accessed last on 17-05-2024.\n170\nconclusion\n171\nMohamed et al., 2020; Zuboff, 2023; Devenot, 2023). Secondly,\ncurrent language models are prone to producing hallucinations, i.e.\nseemingly plausible but fabricated generations. While detection\nand mitigation of hallucinations are very active areas of research\n(Ji et al., 2023b), some have argued that it is an unavoidable\nfeature of current models (Kalai and Vempala, 2024; Xu et al.,\n2024).\nThirdly, the way language models work remains too\ntechnical and opaque to most people and LLM-based chatbots are\nconceptualized as search engines rather than extremely powerful\nword predictors. This becomes even more blatant when examining\nthe details of the above case through one of the lawyers’ affidavit:\nIn order to verify the veracity of the (later to be found fictitious)\ncited case studies, they asked ChatGPT questions such as “Is\nvarghese a real case”, to which the language model answered\naffirmatively.\nThe bitter lesson (Sutton, 2019) states that “general methods\nthat leverage computation are ultimately the most effective, and\nby a large margin”. In the past, it has proven time and time again\nthat sophistication in AI research is outperformed by sheer scale.\nWhich, given the content of thesis, prompts the question of whether\nresearch on UQ is necessary or yet another piece of unnecessary\nornamentation on the road to more intelligent systems.\nDo We actually Need UQ?\nLet us assume the role of a devil’s\nadvocate for a moment. In this position, we can pose several\ncounter-arguments to the necessity of UQ, starting with\n“Current cutting-edge models work so well that UQ is not\nnecessary.”\nWhile it is true that the bitter lesson keeps materializing in\ncurrent models, even an ever-increasing coverage of topics and\ntasks through larger amounts of training data does not shield them\nfrom an infinitely-large space of possible inputs, on which their\nbehavior is hard to predict. This phenomenon is referred to as\nmodel underspecification. Furthermore, increasing generalization\nby obtaining more and more training data is expensive; estimations\nby works such as Villalobos et al. (2022) suggest that we are\nalready starting to deplete the stock of high-quality language data\nto train on. Counter-strategies to this problem have been to simply\nallocate resources to human data creation,83 to repeatedly use the\n83See for instance reporting about OpenAI’s strategy to employ workers\nin Kenya to create new training data and improve existing data quality,\ne.g. https://time.com/6247678/openai-chatgpt-kenya-workers/ (last ac-\ncessed 19.05.2024).\nconclusion\n172\nsame training data (Xue et al., 2024b) or to use synthetic training\ndata, where the latter has shown mixed results (Guo et al., 2023;\nAlemohammad et al., 2023; Briesch et al., 2023; Bohacek and Farid,\n2023; Gulcehre et al., 2023; Shumailov et al., 2023; Feng et al.;\nUlmer et al., 2024b). However, this also ignores the inequality of\navailable data in different languages (Singh et al., 2024b). Being\nable to guarantee robust model behavior on different topics, tasks\nand language this way thus appears unlikely.\n“Model capabilities have consistently improved with model size and\nthe amount of available training data, and in the same way a\nmodel’s uncertainty estimates will become more reliable by itself.”\nWhile there is some evidence that e.g. a model’s calibration in-\ncreases with the available training data (Dan and Roth, 2021; Chen\net al., 2023b; Tian et al., 2023; Zhu et al., 2023; Ulmer et al., 2024a),\none can hypothesize that the increased coverage of training cases\nsimply enables the model to better learn the actual distributions\nover targets (be it class labels or token distributions) for the most\nfrequent types of input. For LLMs, there is some evidence that\nverbalized uncertainty in its current form improves with model and\ntraining data size, but the distribution of uncertainty expressions\nstill remains skewed (Tian et al., 2023; Ulmer et al., 2024a).\n“Smarter models will become better at admitting when they do not\nknow an answer.”\nCompared to the previous question, here we wouldn’t rely on\nadditional uncertainty estimates to refuse a potential unreliable\nprediction, but assume that a smarter model would learn to refuse\ndirectly. We can reason through this argument by realizing that\nin order to achieve these model refusals, they would either have\nto be explicit contents of their training data, or be the result\nof of some subsequent finetuning / alignment process. The first\ncase is unrealistic or at least conceptually misguided: We would\nlike models to respond to certain instructions by admitting their\nignorance because the answer would otherwise likely be incorrect,\nnot because they learned a mapping from certain instructions to\nthese admissions—in the end, we still want models to learn to solve\na given task! This entails that such a behavior would be acquired\nduring additional finetuning steps (instead of the pre-training phase,\nsuch as instruction finetuning, alignment using human feedback,\netc.), but in order to do so, one requires knowledge about when\nthese statements are necessary.\nThis could come from signals\nfrom the model itself—however we have seen that models do not\nalways know when they do not know—or from human or automatic\nconclusion\n173\nevaluation, which seems infeasible to perform on a comprehensive\nscale. Thus, we can likely only adopt these behaviors for more\ncommon instructions, even though they would matter most on\nunseen or rare ones.\n“Current UQ quantification approaches are useless since they are\nnot reliable themselves.”\nThis is not an entirely unfair criticism, and we dedicated parts\nof Chapter 7 to the limits and failure cases of current UQ meth-\nods. One could explain the recent soaring in interest in conformal\nprediction methods that they, in contrast to their alternatives,\ncan provide formal guarantees. Even though these might still be\ninsufficient for many practical applications, we can expect future\nresearch to improve them further. Furthermore, there is a case to\nmade where the overall utility of UQ with even somewhat deficient\nguarantees exceeds the loss in utility without any UQ whatso-\never. Given this thought, one might wonder why we haven’t seen\nwide-spread adoption of UQ techniques in user-facing applications.\nWhat Hinders UQ in User-Facing Products?\nThis point\ncan only be answered speculatively, but what is true is that none of\nthe large commercially available LLMs at the time of this writing\noffer any degree of uncertainty quantification.84 One potential\nreason could be that there is simply no or not enough demand; this\ncould be because models usually work sufficiently well for users\non their specific use cases or that customers are not aware of the\nproblem (or of UQ as a possible solution). Another reason could\nbe that UQ in its current form does not work reliably enough and\nwould expose a company to too many risks; an unreliable prediction\nthat is accompanied with a high confidence value could potentially\ncreate PR and legal liability issues when found to have caused\nreal-world harm.\nHow does UQ Relate to Current Developments in the\nField?\nAt the time of writing of the author’s master thesis in\n2019, the field of NLP was experiencing an acceleration. After the\ninvention of the transformer two years prior (Vaswani et al., 2017),\nmodels like Bert (Devlin et al., 2019) and GPT-2 (Radford et al.,\n2019) were heralding a paradigm shift in the field, as increasingly\nlarge models were demonstrating hitherto unseen abilities. In this\ncontext, part of the conclusion of Ulmer (2019) reads\n84This includes Anthropic, Cohere, OpenAI, Google and Mistral. OpenAI’s\nAPI does allow access to token probabilities (https://x.com/OpenAIDevs/\nstatus/1735730662362189872, last accessed on 16.01.24), however they are\nnot framed as confidence scores directly, confidence estimation is just mentioned\nas one possible application.\nconclusion\n174\n“On the flip side, these [language] models require huge amounts of data and\ncomputational resources. [. . . ] This has several, worrying implications: First,\nwith these resource requirements, scientific papers become hard to reproduce.\nThese costs only allow training of these models in the context of well-funded\ninstitutions, namely top-tier universities and affluent tech giants. Secondly,\nthe reliance on large-scale hardware produces a high electricity consumption\nalong with a worrisome carbon footprint, which bears a certain irony: These\nmodels try to (loosely) imitate the human brain, a biological computer that is\nactually very energy efficient (Schwartz et al., 2019). Lastly, scaling up data\nsets and the number of parameters does not necessarily increase the semblance\nto human cognition.”\nIt is interesting to re-examine these thoughts in the light of\ncurrent trends. First of all, the size of language models and their\ntraining set sizes has risen tremendously. Devlin et al.’s largest\nBert model comprised 340 million parameters, and was trained\non around 3.3 billion words. For comparison, the largest Llama\n3 model comprises 90 billion parameters and was trained on 15\ntrillion tokens (AI@Meta, 2024), with GPT-4 rumored to be 1.76\ntrillion parameters large (The Decoder, 2023).\nThe fact that\nGPT-4’s parameter count is not public and that details about\nthe training data for both GPT-4 and Llama 3 are unknown\naccentuate the most recent trend in language model development\nand echo some of the thoughts above: With a few exceptions such\nlike OLMo (Groeneveld et al., 2024), it has become infeasible for\nnon-industry actors to train language models from scratch. At\nthe same time, companies have started to hide training details\nthat they deem strategically important, hindering replication and\nresearch even when the final models become openly available. This\nalso makes it hard to assess for which kind of inputs we can expect\nmodels to behave reliably. This is exacerbated by the fact that any\nsemblance of human intelligence is still controversial—while recent\nmodels have displayed impressive abilities (Bubeck et al., 2023),\nsome argue that outputs are “haphazardly stitch[ed] together\nsequences of linguistic forms [the language model] has observed\nin its vast training data, according to probabilistic information\nabout how they combine, but without any reference to meaning”\n(Bender et al., 2021). The consequence of this is that language\nmodels might fail in ways that are unpredictable and unintuitive\nto humans. And as the introductory examples in this chapter and\nChapter 6 show, the more convincing generations appear, the\nharder any failures become to spot.\nPolicy and Societal Implications.\nThe increased adoption\nof AI models has prompted a response from different regulatory\nbodies. One instance of this is the EU AI act (Madiega, 2021).\nThe act sorts different applications into a four tier system, ranging\nconclusion\n175\nfrom minimal risk to unacceptable risk. While unacceptable risk\napplications are outright prohibited (e.g. social scoring systems,\nfacial recognition etc.), there also exists a tier of high-risks systems\nwith applications in law enforcement, education or medicine that\nare allowed under strict regulations. One prerequisite for high-\nrisk systems is human oversight, meaning that the system can be\n“effectively overseen by natural persons during the period in which\nthe AI system is in use” and to “prevent or minimize the risks to\nhealth, safety or fundamental rights that may emerge” (Article\n14). It should be clear that techniques like anomaly detection\nand UQ can help to fulfill these criteria by deferring decisions to\nhuman overseers and flagging inputs on which the system could\nbehave abnormally. Thus, in order to create commercial high-\nrisk AI applications in the EU, the development of UQ methods\nwith stronger guarantees might be one potential avenue. Similar\npolicies are still pending in the United States, where the Biden\nadministration enacted an executive order on the development and\nuse of AI (Biden, 2023). In its opening paragraph, it states\n“Harnessing AI for good and realizing its myriad benefits requires mitigating\nits substantial risks. This endeavor demands a society-wide effort that\nincludes government, the private sector, academia, and civil society.”\nAI is a powerful technology, unfolding in an unequal world and\nalready reshaping societies. As researchers, we can help advance\ndirections like UQ alongside others such as generalization, bias\nmitigation, fairness, interpretability and many more in order to help\nmitigate the risk of modern AI systems, so that any transformation\nmay be a positive one.\nBibliography\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C.\nCourville, and Marc G. Bellemare. 2021. Deep Reinforcement\nLearning at the Edge of the Statistical Precipice. In Advances in\nNeural Information Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 29304–29320. (Cited on\npage 80)\nEneko Agirre and Philip Edmonds. 2007. Word Sense Disam-\nbiguation: Algorithms and Applications, volume 33. (Cited on\npage 26)\nKabir Ahuja, Sunayana Sitaram, Sandipan Dandapat, and Monojit\nChoudhury. 2022. On the Calibration of Massively Multilingual\nLanguage Models. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pages 4310–\n4323. (Cited on page 59)\nLukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, and\nSepp Hochreiter. 2024. How many Opinions Does Your LLM\nHave? Improving Uncertainty Estimation in NLG. In ICLR 2024\nWorkshop on Secure and Trustworthy Large Language Models.\n(Cited on page 61)\nAI@Meta. 2024. Llama 3 Model Card. (Cited on pages 78, 138,\nand 174)\nLaura Aina and Tal Linzen. 2021. The Language Model Understood\nthe Prompt Was Ambiguous: Probing Syntactic Uncertainty\nthrough Generation. In Proceedings of the Fourth BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Networks for\nNLP, pages 42–57. (Cited on page 60)\nAdrian Akmajian, Ann K. Farmer, Lee Bickmore, Richard A. De-\nmers, and Robert M. Harnish. 2017. Linguistics: An Introduction\nto Language and Communication. (Cited on page 26)\nAhmed M. Alaa and Mihaela van der Schaar. 2020. Discrimina-\ntive Jackknife: Quantifying Uncertainty in Deep Learning via\n176\nBIBLIOGRAPHY\n177\nHigher-order Influence Functions. In Proceedings of the 37th In-\nternational Conference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 165–174. (Cited on page 115)\nSina Alemohammad,\nJosue Casco-Rodriguez,\nLorenzo Luzi,\nAhmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali\nSiahkoohi, and Richard Baraniuk. 2023. Self-consuming Genera-\ntive Models Go MAD. In The Twelfth International Conference\non Learning Representations. (Cited on page 172)\nJunaid Ali, Preethi Lahoti, and Krishna P. Gummadi. 2021. Ac-\ncounting for Model Uncertainty in Algorithmic Discrimination.\nIn Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,\nand Society, pages 336–345. (Cited on page 67)\nHussam Alkaissi and Samy I. McFarlane. 2023. Artificial Halluci-\nnations in ChatGPT: Implications in Scientific Writing. Cureus,\n15(2). (Cited on page 123)\nJames Urquhart Allingham, Florian Wenzel, Zelda E. Mariet, Basil\nMustafa, Joan Puigcerver, Neil Houlsby, Ghassen Jerfel, Vincent\nFortuin, Balaji Lakshminarayanan, Jasper Snoek, Dustin Tran,\nCarlos Riquelme Ruiz, and Rodolphe Jenatton. 2022. Sparse\nMoEs meet Efficient Ensembles. Transaction on Machine Learn-\ning Research, 2022. (Cited on page 46)\nP.C. Álvarez-Esteban, Eustasio del Barrio, Juan Antonio Cuesta-\nAlbertos, and C. Matrán. 2017. Models for the Assessment of\nTreatment Improvement: The Ideal and the Feasible. Statistical\nScience, 32(3):469–485. (Cited on pages 85 and 86)\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,\nJohn Schulman, and Dan Mané. 2016. Concrete Problems in AI\nSafety. ArXiv preprint, abs/1606.06565. (Cited on page 3)\nJakob Smedegaard Andersen and Walid Maalej. 2022. Efficient,\nUncertainty-based Moderation of Neural Networks Text Classi-\nfiers. In Findings of the Association for Computational Linguis-\ntics: ACL 2022, pages 1536–1546. (Cited on page 69)\nJakob Smedegaard Andersen and Olaf Zukunft. 2022. More Sustain-\nable Text Classification via Uncertainty Sampling and a Human-\nin-the-loop. In International Conference on Agents and Artificial\nIntelligence, pages 201–225. Springer. (Cited on page 69)\nChristophe Andrieu, Nando De Freitas, Arnaud Doucet, and\nMichael I. Jordan. 2003. An Introduction to MCMC for Machine\nLearning. Machine learning, 50:5–43. (Cited on page 42)\nBIBLIOGRAPHY\n178\nAnastasios Nikolas Angelopoulos and Stephen Bates. 2021.\nA\nGentle Introduction to Conformal Prediction and Distribution-\nfree Uncertainty Quantification. ArXiv preprint, abs/2107.07511.\n(Cited on pages 39, 124, and 128)\nAnastasios Nikolas Angelopoulos, Stephen Bates, Adam Fisch, Li-\nhua Lei, and Tal Schuster. 2023. Conformal Risk Control. In The\nTwelfth International Conference on Learning Representations.\n(Cited on pages 40, 138, and 164)\nAnastasios Nikolas Angelopoulos, Stephen Bates, Michael I. Jordan,\nand Jitendra Malik. 2021. Uncertainty Sets for Image Classifiers\nUsing Conformal Prediction. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. (Cited on pages 128, 131, and 138)\nRushil Anirudh and Jayaraman J. Thiagarajan. 2021. Delta-UQ:\nAccurate Uncertainty Quantification via Anchor Marginalization.\nArXiv preprint, abs/2110.02197. (Cited on page 57)\nJavier Antorán, James Urquhart Allingham, and José Miguel\nHernández-Lobato. 2020.\nDepth Uncertainty in Neural Net-\nworks. In Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Sys-\ntems 2020, NeurIPS 2020, December 6-12, 2020, virtual. (Cited\non page 46)\nRaman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukher-\njee. 2018. Understanding Deep Neural Networks with Rectified\nLinear Units. In 6th International Conference on Learning Rep-\nresentations, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. (Cited on pages 98,\n102, and 103)\nUdit Arora, William Huang, and He He. 2021. Types of Out-of-\ndistribution Texts and how to Detect Them. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 10687–10701. (Cited on pages 68, 113, and 116)\nArsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and\nDmitry P. Vetrov. 2020.\nPitfalls of In-domain Uncertainty\nEstimation and Ensembling in Deep Learning. In 8th Inter-\nnational Conference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. (Cited on pages 38\nand 67)\nJsang Audun. 2018. Subjective Logic: A Formalism for Reasoning\nunder Uncertainty. (Cited on page 49)\nBIBLIOGRAPHY\n179\nRazvan Azamfirei, Sapna R. Kudchadkar, and James Fackler. 2023.\nLarge Language Models and the Perils of Their Hallucinations.\nCritical Care, 27(1):1–2. (Cited on page 124)\nJoris Baan, Wilker Aziz, Barbara Plank, and Raquel Fernandez.\n2022. Stop Measuring Calibration when Humans Disagree. In\nProceedings of the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1892–1915. (Cited on pages 63,\n165, 166, and 168)\nJoris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing\nLi, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula\nZerva, and Wilker Aziz. 2023. Uncertainty in Natural Language\nGeneration: From Theory to Applications.\nArXiv preprint,\nabs/2307.15703. (Cited on pages 6, 29, 30, 63, 74, 78, 158, 159,\n165, and 167)\nJoris Baan, Raquel Fernández, Barbara Plank, and Wilker Aziz.\n2024. Interpreting Predictive Probabilities: Model Confidence or\nHuman Label Variation? In Proceedings of the 18th Conference\nof the European Chapter of the Association for Computational\nLinguistics, EACL 2024 - Volume 2: Short Papers, St. Julian’s,\nMalta, March 17-22, 2024, pages 268–277. Association for Com-\nputational Linguistics. (Cited on pages 60 and 167)\nAnnette Baier. 1986. Trust and Antitrust. Ethics, 96(2):231–260.\n(Cited on page 65)\nDivya Jyoti Bajpai and Manjesh Kumar Hanawal. 2024. CEEBERT:\nCross-Domain Inference in Early Exit BERT. ArXiv preprint,\nabs/2405.15039. (Cited on page 68)\nSimone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondrej\nDusek. 2024. Leak, Cheat, Repeat: Data Contamination and\nEvaluation Malpractices in Closed-source LLMs. In Proceedings\nof the 18th Conference of the European Chapter of the Association\nfor Computational Linguistics, EACL 2024 - Volume 1: Long\nPapers, St. Julian’s, Malta, March 17-22, 2024, pages 67–93.\nAssociation for Computational Linguistics. (Cited on pages 71\nand 76)\nNeil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto.\n2024. Linguistic Calibration of Language Models. ArXiv preprint,\nabs/2404.00474. (Cited on pages 62 and 67)\nDan Baras. 2023. Carbon Offsetting. Ethics, Policy & Environment,\npages 1–18. (Cited on page 327)\nBIBLIOGRAPHY\n180\nRina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, and\nRyan J. Tibshirani. 2023. Conformal Prediction beyond Ex-\nchangeability. The Annals of Statistics, 51(2):816–845. (Cited\non pages 40, 125, 126, 127, 137, and 139)\nAinhize Barrainkua, Paula Gordaliza, Jose A. Lozano, and Novi\nQuadrianto. 2024. Uncertainty Matters: Stable Conclusions\nunder Unstable Assessment of Fairness Results. In International\nConference on Artificial Intelligence and Statistics, pages 1198–\n1206. PMLR. (Cited on page 67)\nAndrew R. Barron. 1994. Approximation and Estimation Bounds\nfor Artificial Neural Networks. Machine learning, 14:115–133.\n(Cited on page 41)\nValerio Basile, Michael Fell, Tommaso Fornaciari, Dirk Hovy, Silviu\nPaun, Barbara Plank, Massimo Poesio, and Alexandra Uma.\n2021. We Need to Consider Disagreement in Evaluation. In\nProceedings of the 1st Workshop on Benchmarking: Past, Present\nand Future, pages 15–21. (Cited on pages 63 and 75)\nElisa Bassignana, Max Müller-Eberstein, Mike Zhang, and Barbara\nPlank. 2022. Evidence > Intuition: Transferability Estimation\nfor Encoder Selection. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022,\npages 4218–4227. (Cited on page 66)\nElisa Bassignana and Barbara Plank. 2022. CrossRE: A Cross-\ndomain Dataset for Relation Extraction. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2022, pages\n3592–3604. (Cited on page 75)\nJohn M. Bates and Clive W. J. Granger. 1969. The Combination of\nForecasts. Journal of the Operational Research Society, 20(4):451–\n468. (Cited on page 46)\nEric Bauer and Ron Kohavi. 1999. An Empirical Comparison\nof Voting Classification Algorithms: Bagging, Boosting, and\nVariants. Machine learning, 36:105–139. (Cited on page 46)\nEvan Becker and Stefano Soatto. 2024. Cycles of Thought: Mea-\nsuring LLM Confidence through Stable Explanations. ArXiv\npreprint, abs/2406.03441. (Cited on page 60)\nAnya Belz, Shubham Agarwal, Anastasia Shimorina, and Ehud\nReiter. 2021. A Systematic Review of Reproducibility Research\nin Natural Language Processing. In Proceedings of the 16th\nBIBLIOGRAPHY\n181\nConference of the European Chapter of the Association for Com-\nputational Linguistics: Main Volume, pages 381–393. (Cited on\npages 71 and 77)\nAlessio Benavoli, Giorgio Corani, Janez Demšar, and Marco Zaf-\nfalon. 2017. Time for a Change: A Tutorial for Comparing\nMultiple Classifiers through Bayesian Analysis. The Journal of\nMachine Learning Research, 18(1):2653–2688. (Cited on pages 81\nand 95)\nEmily M. Bender. 2011. On Achieving and Evaluating Language-\nindependence in NLP. Linguistic Issues in Language Technology,\n6. (Cited on page 6)\nEmily M. Bender and Batya Friedman. 2018. Data Statements for\nNatural Language Processing: Toward Mitigating System Bias\nand Enabling Better Science. Transactions of the Association\nfor Computational Linguistics, 6:587–604. (Cited on pages 74\nand 77)\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and\nShmargaret Shmitchell. 2021. On the Dangers of Stochastic\nParrots: Can Language Models Be too Big?\nIn Proceedings\nof the 2021 ACM Conference on Fairness, Accountability, and\nTransparency, pages 610–623. (Cited on pages 71 and 174)\nViktor Bengs, Eyke Hüllermeier, and Willem Waegeman. 2023. On\nSecond-order Scoring Rules for Epistemic Uncertainty Quantifi-\ncation. In International Conference on Machine Learning, pages\n2078–2091. PMLR. (Cited on page 168)\nMartin Benjamin. 2018.\nHard Numbers: Language Exclusion\nin Computational Linguistics and Natural Language Process-\ning. In Proceedings of the LREC 2018 Workshop “CCURL2018–\nSustaining Knowledge Diversity in the Digital Age, pages 13–18.\n(Cited on page 75)\nFederico Bergamin, Pablo Moreno-Muñoz, Søren Hauberg, and\nGeorgios Arvanitidis. 2024. Riemannian Laplace Approximations\nfor Bayesian Neural Networks. Advances in Neural Information\nProcessing Systems, 36. (Cited on page 45)\nJames O. Berger and Thomas Sellke. 1987. Testing a Point Null Hy-\npothesis: The Irreconcilability of p-values and Evidence. Journal\nof the American Statistical Association, 82(397):112–122. (Cited\non page 81)\nBIBLIOGRAPHY\n182\nJames Bergstra and Yoshua Bengio. 2012. Random Search for\nHyper-Parameter Optimization. Journal of Machine Learning\nResearch, 13(2). (Cited on pages 78 and 330)\nAadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. 2023.\nImproved Online Conformal Prediction via Strongly Adaptive\nOnline Learning. In International Conference on Machine Learn-\ning, pages 2337–2363. PMLR. (Cited on page 40)\nUmang Bhatt, Javier Antorán, Yunfeng Zhang, Q Vera Liao,\nPrasanna Sattigeri, Riccardo Fogliato, Gabrielle Melançon, Ran-\nganath Krishnan, Jason Stanley, Omesh Tickoo, et al. 2021.\nUncertainty as a Form of Transparency: Measuring, Commu-\nnicating, and Using Uncertainty. In Proceedings of the 2021\nAAAI/ACM Conference on AI, Ethics, and Society, pages 401–\n413. (Cited on page 66)\nJoseph R. Biden. 2023.\nExecutive Order on the Safe, Secure,\nand Trustworthy Development and Use of Artificial Intelligence.\n(Cited on page 175)\nLukas Biewald. 2020.\nExperiment Tracking with Weights and\nBiases. Software available from wandb.com. (Cited on pages 326\nand 332)\nThomas Bilgram and Britt Keson. 1998. The Construction of a\nTagged Danish Corpus. In Proceedings of the 11th Nordic Con-\nference of Computational Linguistics (NoDaLiDa 1998), pages\n129–139. (Cited on page 113)\nSteven Bird. 2022. Local Languages, Third Spaces, and other High-\nResource Scenarios. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), pages 7817–7829. (Cited on page 113)\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew,\nRavit Dotan, and Michelle Bao. 2022. The Values Encoded in\nMachine Learning Research. In FAccT ’22: 2022 ACM Con-\nference on Fairness, Accountability, and Transparency, Seoul,\nRepublic of Korea, June 21 - 24, 2022, pages 173–184. ACM.\n(Cited on pages 71, 79, and 82)\nBernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob\nRichter, Stefan Coors, Janek Thomas, Theresa Ullmann, Marc\nBecker, Anne-Laure Boulesteix, Difan Deng, and Marius Lin-\ndauer. 2023. Hyperparameter Optimization: Foundations, Al-\ngorithms, Best Practices, and Open Challenges. WIREs Data.\nMining. Knowl. Discov., 13(2). (Cited on pages 77 and 78)\nBIBLIOGRAPHY\n183\nChristopher M. Bishop and Nasser M. Nasrabadi. 2006. Pattern\nRecognition and Machine Learning, volume 4. (Cited on pages 21,\n36, 41, 42, and 47)\nTxus Blasco, J. Salvador Sánchez, and Vicente García. 2024. A\nSurvey on Uncertainty Quantification in Deep Learning for Fi-\nnancial Time Series Prediction. Neurocomputing, 576:127339.\n(Cited on page 6)\nJarosław Błasiok and Preetum Nakkiran. 2023. Smooth ECE:\nPrincipled Reliability Diagrams via Kernel Smoothing. ArXiv\npreprint, abs/2309.12236. (Cited on pages 37 and 151)\nJohn Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,\nCyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing.\n2004. Confidence Estimation for Machine Translation. In COL-\nING 2004: Proceedings of the 20th International Conference on\nComputational Linguistics, pages 315–321. (Cited on page 143)\nFlorian Bley, Sebastian Lapuschkin, Wojciech Samek, and Grégoire\nMontavon. 2024. Explaining Predictive Uncertainty by Exposing\nSecond-Order Effects. ArXiv preprint, abs/2401.17441. (Cited\non page 167)\nAvrim Blum, Nika Haghtalab, and Ariel D. Procaccia. 2015. Vari-\national Dropout and the Local Reparameterization Trick. In\nAdvances in Neural Information Processing Systems 28: Annual\nConference on Neural Information Processing Systems 2015, De-\ncember 7-12, 2015, Montreal, Quebec, Canada, pages 2575–2583.\n(Cited on page 44)\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan\nWierstra. 2015. Weight Uncertainty in Neural Network. In\nProceedings of the 32nd International Conference on Machine\nLearning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of\nJMLR Workshop and Conference Proceedings, pages 1613–1622.\n(Cited on pages 44, 111, and 330)\nMatyas Bohacek and Hany Farid. 2023. Nepotistically Trained\nGenerative-AI Models Collapse. ArXiv preprint, abs/2311.12202.\n(Cited on page 172)\nOndrej Bohdal, Yongxin Yang, and Timothy Hospedales. 2021.\nMeta-calibration: Learning of Model Calibration Using Dif-\nferentiable Expected Calibration Error.\nArXiv preprint,\nabs/2106.09613. (Cited on page 37)\nBIBLIOGRAPHY\n184\nOndřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette\nGraham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp\nKoehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo\nNegri, Matt Post, Raphael Rubino, Lucia Specia, and Marco\nTurchi. 2017.\nFindings of the 2017 Conference on Machine\nTranslation (WMT17). In Proceedings of the Second Conference\non Machine Translation, pages 169–214. (Cited on page 129)\nShahin Boluki, Randy Ardywibowo, Siamak Zamani Dadaneh,\nMingyuan Zhou, and Xiaoning Qian. 2020. Learnable Bernoulli\nDropout for Bayesian Deep Learning. In The 23rd International\nConference on Artificial Intelligence and Statistics, AISTATS\n2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume\n108 of Proceedings of Machine Learning Research, pages 3905–\n3916. (Cited on page 44)\nCarlo Bonferroni. 1936. Teoria Statistica delle Classi e Calcolo delle\nProbabilita. Pubblicazioni del Instituto Superiore di Scienze Eco-\nnomiche e Commericiali di Firenze, 8:3–62. (Cited on page 80)\nJean-François Bonnefon and Gaëlle Villejoubert. 2006. Tactful\nor Doubtful? Expectations of Politeness Explain the Severity\nBias in the Interpretation of Probability Phrases. Psychological\nScience, 17(9):747–751. (Cited on page 31)\nGeorge Boole. 1854. An Investigation of the Laws of Thought:\nOn which Are Founded the Mathematical Theories of Logic and\nProbabilities, volume 2. (Cited on page 57)\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2019. Nuanced Metrics for Measuring\nUnintended Bias with Real Data for Text Classification. In\nCompanion Proceedings of the 2019 World Wide Web Conference,\npages 491–500. (Cited on page 96)\nXavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov,\nBrennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepah-\nvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. 2021.\nAccounting for Variance in Machine Learning Benchmarks. Pro-\nceedings of Machine Learning and Systems, 3. (Cited on pages 71,\n76, 80, 83, 84, and 94)\nSamuel R. Bowman and George Dahl. 2021. What Will It Take\nto Fix Benchmarking in Natural Language Understanding? In\nProceedings of the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 4843–4855. (Cited on page 75)\nBIBLIOGRAPHY\n185\nJohn Bradshaw, Alexander G. de G. Matthews, and Zoubin Ghahra-\nmani. 2017. Adversarial Examples, Uncertainty, and Transfer\nTesting Robustness in Gaussian Process Hybrid Deep Networks.\nArXiv preprint, abs/1707.02476. (Cited on page 47)\nAnouck Braggaar and Rob van der Goot. 2021. Challenges in Anno-\ntating and Parsing Spoken, Code-switched, Frisian-Dutch Data.\nIn Proceedings of the Second Workshop on Domain Adaptation\nfor NLP, pages 50–58. (Cited on page 75)\nJohn S. Bridle. 1990. Probabilistic Interpretation of Feedforward\nClassification Network Outputs, with Relationships to Statistical\nPattern Recognition. In Neurocomputing, pages 227–236. (Cited\non pages 100 and 282)\nGlenn W. Brier. 1950. Verification of Forecasts Expressed in Terms\nof Probability. Monthly Weather Review, 78(1):1–3. (Cited on\npage 151)\nMartin Briesch, Dominik Sobania, and Franz Rothlauf. 2023.\nLarge Language Models Suffer from Their own Output: An\nAnalysis of the Self-consuming Training Loop. ArXiv preprint,\nabs/2311.16822. (Cited on page 172)\nSteve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng.\n2011.\nHandbook of Markov Chain Monte Carlo.\n(Cited on\npage 81)\nKeith Brown. 2005. Encyclopedia of Language and Linguistics,\nvolume 1. (Cited on pages 26 and 27)\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes\nGehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee,\nYuanzhi Li, Scott Lundberg, et al. 2023. Sparks of Artificial\nGeneral Intelligence: Early Experiments with GPT-4. ArXiv\npreprint, abs/2303.12712. (Cited on page 174)\nTimothy Campbell. 2021. Offsetting, Denialism, and Risk. (Cited\non page 327)\nRicardo J.G.B. Campello, Davoud Moulavi, and Jörg Sander. 2013.\nDensity-based Clustering Based on Hierarchical Density Esti-\nmates. In Pacific-Asia Conference on Knowledge Discovery and\nData Mining, pages 160–172. Springer. (Cited on pages 147\nand 338)\nMargarida M. Campos, António Farinhas, Chrysoula Zerva, Mário\nA. T. Figueiredo, and André F. T. Martins. 2024. Conformal\nBIBLIOGRAPHY\n186\nPrediction for Natural Language Processing: A Survey. ArXiv\npreprint, abs/2405.01976. (Cited on pages 59 and 165)\nMichele Caprio, Souradeep Dutta, Kuk Jang, Vivian Lin, Radoslav\nIvanov, Oleg Sokolsky, and Insup Lee. 2023. Credal Bayesian\nDeep Learning.\nArXiv preprint, abs/2302.09656.\n(Cited on\npages 57 and 58)\nDallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia,\nKyle Mahowald, and Dan Jurafsky. 2020. With Little Power\nComes Great Responsibility. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 9263–9274. (Cited on pages 76, 77, and 78)\nSean M. Carroll. 2019. Beyond Falsifiability: Normal Science in\na Multiverse. Why Trust a Theory, pages 300–314. (Cited on\npage 72)\nSimon Caton and Christian Haas. 2024.\nFairness in Machine\nLearning: A Survey. ACM Comput. Surv., 56(7):166:1–166:38.\n(Cited on page 3)\nDamon Centola, Joshua Becker, Devon Brackbill, and Andrea\nBaronchelli. 2018. Experimental Evidence for Tipping Points\nin Social Convention. Science, 360(6393):1116–1119. (Cited on\npage 82)\nSky CH-Wang, Benjamin Van Durme, Jason Eisner, and Chris\nKedzie. 2023. Do Androids Know They’re only Dreaming of\nElectric Sheep? (Cited on page 164)\nJunbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho,\nSeunghyun Park, Yunsung Lee, and Sungrae Park. 2021. SWAD:\nDomain Generalization by Seeking Flat Minima. In Advances in\nNeural Information Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 22405–22418. (Cited on\npage 46)\nIlias Chalkidis. 2023. ChatGPT May Pass the Bar Exam Soon, but\nHas a Long Way to Go for the LexGLUE Benchmark. ArXiv\npreprint, abs/2304.12202. (Cited on page 2)\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019a.\nNeural Legal Judgment Prediction in English. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational\nLinguistics, pages 4317–4323. (Cited on page 2)\nBIBLIOGRAPHY\n187\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019b.\nNeural Legal Judgment Prediction in English. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational\nLinguistics, pages 4317–4323. (Cited on page 97)\nBertrand Charpentier, Oliver Borchert, Daniel Zügner, Simon\nGeisler, and Stephan Günnemann. 2022. Natural Posterior Net-\nwork: Deep Bayesian Predictive Uncertainty for Exponential\nFamily Distributions. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022. (Cited on page 56)\nBertrand Charpentier, Daniel Zügner, and Stephan Günnemann.\n2020. Posterior Network: Uncertainty Estimation without OOD\nSamples via Density-based Pseudo-counts. In Advances in Neu-\nral Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual. (Cited on pages 54, 56, and 279)\nChangyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, and\nLawrence Carin. 2016. Stochastic Gradient MCMC with Stale\nGradients. In Advances in Neural Information Processing Sys-\ntems 29: Annual Conference on Neural Information Processing\nSystems 2016, December 5-10, 2016, Barcelona, Spain, pages\n2937–2945. (Cited on page 43)\nHanjie Chen and Yangfeng Ji. 2022. Explaining Prediction Uncer-\ntainty of Pre-trained Language Models by Detecting Uncertain\nWords in Inputs. ArXiv preprint, abs/2201.03742. (Cited on\npages 4 and 167)\nJiuhai Chen and Jonas Mueller. 2023. Quantifying Uncertainty in\nAnswers from Any Language Model via Intrinsic and Extrinsic\nConfidence Assessment. ArXiv preprint, abs/2308.16175. (Cited\non page 61)\nJustin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2023a.\nReconcile: Round-table Conference Improves Reasoning via Con-\nsensus among Diverse LLMs. ArXiv preprint, abs/2309.13007.\n(Cited on page 62)\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya\nGrover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and\nIgor Mordatch. 2021.\nDecision Transformer: Reinforcement\nLearning via Sequence Modeling. In Advances in Neural Infor-\nmation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 15084–15097. (Cited on page 78)\nBIBLIOGRAPHY\n188\nWenhu Chen, Yilin Shen, Hongxia Jin, and William Wang. 2018.\nA Variational Dirichlet Framework for Out-of-distribution De-\ntection. ArXiv preprint, abs/1811.07308. (Cited on pages 56\nand 281)\nYangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng\nJi. 2023b. A Close Look into the Calibration of Pre-trained\nLanguage Models. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages\n1343–1367. (Cited on pages 59, 156, and 172)\nMuthu Chidambaram, Holden Lee, Colin McSwiggen, and Semon\nRezchikov. 2024. How Flawed Is ECE? An Analysis via Logit\nSmoothing. ArXiv preprint, abs/2402.10046. (Cited on page 37)\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane\nLegg, and Dario Amodei. 2017. Deep Reinforcement Learning\nfrom Human Preferences. In Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural Informa-\ntion Processing Systems 2017, December 4-9, 2017, Long Beach,\nCA, USA, pages 4299–4307. (Cited on pages 63 and 339)\nRobert T. Clemen. 1989. Combining Forecasts: A Review and\nAnnotated Bibliography. International journal of forecasting,\n5(4):559–583. (Cited on page 46)\nclimeworks. 2022. Climeworks. https://climeworks.com/. Ac-\ncessed: 2022-06-22. (Cited on page 327)\nAdam D. Cobb and Brian Jalaian. 2021. Scaling Hamiltonian Monte\nCarlo Inference for Bayesian Neural Networks with Symmetric\nSplitting. In Proceedings of the Thirty-Seventh Conference on\nUncertainty in Artificial Intelligence, UAI 2021, Virtual Event,\n27-30 July 2021, volume 161 of Proceedings of Machine Learning\nResearch, pages 675–685. (Cited on page 42)\nWilliam G. Cochran. 1934. The Distribution of Quadratic Forms\nin a Normal System, with Applications to the Analysis of Covari-\nance. In Mathematical Proceedings of the Cambridge Philosoph-\nical Society, volume 30, pages 178–191. Cambridge University\nPress. (Cited on page 16)\nArman Cohan, Sydney Young, and Nazli Goharian. 2016. Triag-\ning Mental Health Forum Posts. In Proceedings of the Third\nWorkshop on Computational Linguistics and Clinical Psychology,\npages 143–147. (Cited on page 2)\nBIBLIOGRAPHY\n189\nK. Bretonnel Cohen, Jingbo Xia, Pierre Zweigenbaum, Tiffany\nCallahan, Orin Hargraves, Foster Goss, Nancy Ide, Aurélie\nNévéol, Cyril Grouin, and Lawrence E. Hunter. 2018. Three\nDimensions of Reproducibility in Natural Language Process-\ning. In Proceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018). (Cited on\npage 72)\nA. Feder Cooper, Katherine Lee, Madinha Zahrah Choksi, Barocas\nSolon, Christopher De Sa, James Grimmelmann, Jon Kleinberg,\nSiddhartha Sen, and Baobao Zhang. 2024. Is My Prediction Arbi-\ntrary? The Confounding Effects of Variance in Fair Classification\nBenchmarks. (Cited on page 67)\nCharles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord,\nand Patrick Pérez. 2019. Addressing Failure Prediction by Learn-\ning Model Confidence. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 2898–2909. (Cited on page 57)\nCharles Corbière, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu,\nMatthieu Cord, and Patrick Perez. 2021. Confidence Estimation\nvia Auxiliary Models. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 44(10):6043–6055. (Cited on page 57)\nFrancesco Croce and Matthias Hein. 2018. A Randomized Gradient-\nfree Attack on ReLU Networks. In German Conference on Pat-\ntern Recognition, pages 215–227. Springer. (Cited on pages 103\nand 104)\nAlicia Curth, Patrick Thoral, Wilco van den Wildenberg, Peter\nBijlstra, Daan de Bruin, Paul W. G. Elbers, and Mattia Fornasa.\n2019. Transferring Clinical Prediction Models across Hospitals\nand Electronic Health Record Systems. In Machine Learning and\nKnowledge Discovery in Databases - International Workshops\nof ECML PKDD 2019, Würzburg, Germany, September 16-20,\n2019, Proceedings, Part I, volume 1167 of Communications in\nComputer and Information Science, pages 605–621. (Cited on\npage 99)\nAndreas C. Damianou and Neil D. Lawrence. 2013. Deep Gaus-\nsian Processes. In Proceedings of the Sixteenth International\nConference on Artificial Intelligence and Statistics, AISTATS\n2013, Scottsdale, AZ, USA, April 29 - May 1, 2013, volume 31\nof JMLR Workshop and Conference Proceedings, pages 207–215.\n(Cited on page 47)\nBIBLIOGRAPHY\n190\nAlexander D’Amour, Katherine A. Heller, Dan Moldovan, Ben Ad-\nlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan\nDeaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormoz-\ndiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthike-\nsalingam, Mario Lucic, Yi-An Ma, Cory Y. McLean, Diana\nMincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek\nNatarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Ra-\nman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin\nSeneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch,\nMax Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yad-\nlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. 2022. Under-\nspecification Presents Challenges for Credibility in Modern Ma-\nchine Learning. Journal of Machine Learning Research (JMLR),\n23:226:1–226:61. (Cited on pages 7, 68, 74, and 111)\nSoham Dan and Dan Roth. 2021. On the Effects of Transformer\nSize on In- and Out-of-domain Calibration. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2021, pages\n2096–2101. (Cited on pages 59, 116, and 172)\nFrancesco D’Angelo and Vincent Fortuin. 2021. Repulsive Deep\nEnsembles Are Bayesian. In Advances in Neural Information\nProcessing Systems 34: Annual Conference on Neural Informa-\ntion Processing Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pages 3451–3465. (Cited on page 46)\nKahneman Daniel. 2017. Thinking, Fast and Slow. (Cited on\npages 66 and 68)\nConstantinos Daskalakis, Petros Dellaportas, and Aristeidis Panos.\n2020. Scalable Gaussian Processes, with Guarantees: Kernel\nApproximations and Deep Feature Extraction. (Cited on page 47)\nAida Mostafazadeh Davani, Mark Díaz, and Vinodkumar Prab-\nhakaran. 2022. Dealing with Disagreements: Looking beyond\nthe Majority Vote in Subjective Annotations. Transactions of\nthe Association for Computational Linguistics, 10:92–110. (Cited\non page 63)\nErik Daxberger and José Miguel Hernández-Lobato. 2019. Bayesian\nVariational Autoencoders for Unsupervised Out-of-distribution\nDetection. ArXiv preprint, abs/1912.05651. (Cited on page 166)\nErik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Es-\nchenhagen, Matthias Bauer, and Philipp Hennig. 2021a. Laplace\nRedux - Effortless Bayesian Deep Learning. In Advances in\nNeural Information Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021, NeurIPS 2021,\nBIBLIOGRAPHY\n191\nDecember 6-14, 2021, virtual, pages 20089–20103. (Cited on\npage 45)\nErik A. Daxberger, Eric T. Nalisnick, James Urquhart Alling-\nham, Javier Antorán, and José Miguel Hernández-Lobato. 2021b.\nBayesian Deep Learning via Subnetwork Inference. In Proceed-\nings of the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Pro-\nceedings of Machine Learning Research, pages 2510–2521. (Cited\non page 45)\nMarco De Angelis and Ander Gray. 2021. Why the 1-Wasserstein\nDistance Is the Area between the two Marginal CDFs. ArXiv\npreprint, abs/2111.03570. (Cited on page 86)\nMostafa Dehghani, Yi Tay, Alexey A Gritsenko, Zhe Zhao, Neil\nHoulsby, Fernando Diaz, Donald Metzler, and Oriol Vinyals.\n2021. The Benchmark Lottery. (Cited on page 76)\nFrederik Michel Dekking, Cornelis Kraaikamp, Hendrik Paul Lop-\nuhaä, and Ludolf Erwin Meester. 2005. A Modern Introduction to\nProbability and Statistics: Understanding why and how, volume\n488. (Cited on page 14)\nEustasio del Barrio, Juan A. Cuesta-Albertos, and Carlos Matrán.\n2018a.\nAn Optimal Transportation Approach for Assessing\nAlmost Stochastic Order. In The Mathematics of the Uncertain,\npages 33–44. (Cited on pages 71, 85, 86, 114, 136, 137, and 151)\nEustasio del Barrio, Juan A. Cuesta-Albertos, and Carlos Matrán.\n2018b. Some Indices to Measure Departures from Stochastic\nOrder. ArXiv preprint, abs/1804.02905. (Cited on page 85)\nArthur P. Dempster. 1968. A Generalization of Bayesian Inference.\nJournal of the Royal Statistical Society: Series B (Methodologi-\ncal), 30(2):205–232. (Cited on page 49)\nJanez Demšar. 2008. On the Appropriateness of Statistical Tests\nin Machine Learning. In Workshop on Evaluation Methods for\nMachine Learning in conjunction with ICML, page 65. Citeseer.\n(Cited on page 81)\nNaihao Deng, Xinliang Frederick Zhang, Siyang Liu, Winston Wu,\nLu Wang, and Rada Mihalcea. 2023. You Are What You Anno-\ntate: Towards Better Models through Annotator Representations.\nIn Findings of the Association for Computational Linguistics:\nEMNLP 2023, Singapore, December 6-10, 2023, pages 12475–\n12498. Association for Computational Linguistics. (Cited on\npages 64 and 75)\nBIBLIOGRAPHY\n192\nZhijie Deng, Feng Zhou, Jianfei Chen, Guoqiang Wu, and Jun\nZhu. 2022. Deep Ensemble as a Gaussian Process Approximate\nPosterior. ArXiv preprint, abs/2205.00163. (Cited on page 46)\nStefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-\nVelez, and Steffen Udluft. 2018. Decomposition of Uncertainty in\nBayesian Deep Learning for Efficient and Risk-sensitive Learning.\nIn Proceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,\nJuly 10-15, 2018, volume 80 of Proceedings of Machine Learning\nResearch, pages 1192–1201. (Cited on pages 48 and 101)\nArmen Der Kiureghian and Ove Ditlevsen. 2009.\nAleatory or\nEpistemic? Does It Matter? Structural safety, 31(2):105–112.\n(Cited on page 167)\nShrey Desai and Greg Durrett. 2020. Calibration of Pre-trained\nTransformers. In Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP), pages\n295–302. (Cited on pages 59, 116, and 156)\nGianluca Detommaso, Martin Bertran, Riccardo Fogliato, and\nAaron Roth. 2024. Multicalibration for Confidence Scoring in\nLLMs. ArXiv preprint, abs/2404.04689. (Cited on pages 59\nand 67)\nNicolas Deutschmann, Marvin Alberts, and María Rodríguez\nMartínez. 2024. Conformal Autoregressive Generation: Beam\nSearch with Coverage Guarantees. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 38, pages 11775–\n11783. (Cited on page 59)\nNeşe Devenot. 2023. Tescreal Hallucinations: Psychedelic and\nAI Hype as Inequality Engines. Journal of Psychedelic Studies,\n7(S1):22–39. (Cited on page 171)\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. BERT: Pre-Training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages 4171–4186.\n(Cited on pages 5, 63, 78, 114, 173, 174, 333, and 337)\nFilip Devos. 2003. Semantic Vagueness And Lexical Polyvalence.\nStudia Linguistica, 57(3):121–141. (Cited on page 26)\nBIBLIOGRAPHY\n193\nTerrance DeVries and Graham W. Taylor. 2018. Learning Con-\nfidence for Out-of-distribution Detection in Neural Networks.\nArXiv preprint, abs/1802.04865. (Cited on page 68)\nShehzaad Dhuliawala, Vilém Zouhar, Mennatallah El-Assady, and\nMrinmaya Sachan. 2023. A Diachronic Perspective on User Trust\nin AI Under Uncertainty. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing, EMNLP\n2023, Singapore, December 6-10, 2023, pages 5567–5580. (Cited\non pages 66, 140, and 165)\nThomas G. Dietterich. 2000. Ensemble Methods in Machine Learn-\ning. In International workshop on multiple classifier systems,\npages 1–15. Springer. (Cited on page 46)\nThomas G. Dietterich and Alex Guyer. 2022. The Familiarity\nHypothesis: Explaining the Behavior of Deep Open Set Methods.\nPattern Recognition, 132:108931. (Cited on page 161)\nJesse Dodge and Noah A. Smith. 2020.\nReproducibility at\nEMNLP 2020.\nhttps://2020.emnlp.org/blog/2020-05-20-\nreproducibility. (Cited on page 72)\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkor-\neit, and Neil Houlsby. 2021. An Image Is Worth 16X16 Words:\nTransformers for Image Recognition at Scale. In 9th International\nConference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. (Cited on page 78)\nRotem Dror, Gili Baumer, Marina Bogomolov, and Roi Reichart.\n2017. Replicability Analysis for Natural Language Processing:\nTesting Significance with Multiple Datasets. Transactions of the\nAssociation for Computational Linguistics, 5:471–486. (Cited on\npage 80)\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018.\nThe Hitchhiker’s Guide to Testing Statistical Significance in\nNatural Language Processing. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1383–1392. (Cited on pages 80\nand 83)\nRotem Dror, Lotem Peled-Cohen, Segev Shlomov, and Roi Reichart.\n2020.\nStatistical Significance Testing for Natural Language\nProcessing. Synthesis Lectures on Human Language Technologies,\n13(2):1–116. (Cited on page 83)\nBIBLIOGRAPHY\n194\nRotem Dror, Segev Shlomov, and Roi Reichart. 2019. Deep Dom-\ninance - How to Properly Compare Deep Neural Models. In\nProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 2773–2785. (Cited on pages 71,\n80, 85, 86, 114, 136, 137, 151, 328, and 329)\nChris Drummond. 2009. Replicability Is not Reproducibility: Nor\nIs It Good Science.\nProceedings of the Evaluation Methods\nfor Machine Learning Workshop at the 26th ICML. (Cited on\npage 72)\nMengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia\nHu. 2023.\nShortcut Learning of Large Language Models in\nNatural Language Understanding. Communications of the ACM,\n67(1):110–120. (Cited on page 156)\nDu, Wenda, and Han, Qing. 2021. Research on Application of\nArtificial Intelligence in the Movie Industry.\nvolume 12076.\n(Cited on page 170)\nHanyu Duan, Yi Yang, and Kar Yan Tam. 2024. Do LLMs Know\nabout Hallucination? An Empirical Investigation of LLM’s Hid-\nden States. ArXiv preprint, abs/2402.09733. (Cited on page 164)\nMatthew M. Dunlop, Mark A. Girolami, Andrew M. Stuart, and\nAretha L. Teckentrup. 2018. How Deep Are Deep Gaussian\nProcesses? Journal of Machine Learning Research, 19(54):1–46.\n(Cited on page 47)\nNikita Durasov, Timur M. Bagautdinov, Pierre Baqué, and Pascal\nFua. 2021. Masksembles for Uncertainty Estimation. In IEEE\nConference on Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 13539–13548. (Cited on\npages 44 and 46)\nMichael Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-An Ma,\nJasper Snoek, Katherine A. Heller, Balaji Lakshminarayanan,\nand Dustin Tran. 2020. Efficient and Scalable Bayesian Neural\nNets with Rank-1 Factors. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 2782–2792. (Cited on page 46)\nVincent Dutordoir, James Hensman, Mark van der Wilk, Carl Hen-\nrik Ek, Zoubin Ghahramani, and Nicolas Durrande. 2021. Deep\nNeural Networks as Point Estimates for Deep Gaussian Processes.\nIn Advances in Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing Systems 2021,\nBIBLIOGRAPHY\n195\nNeurIPS 2021, December 6-14, 2021, virtual, pages 9443–9455.\n(Cited on page 47)\nSayna Ebrahimi, William Gan, Dian Chen, Giscard Biamby, Kam-\nyar Salahi, Michael Laielli, Shizhan Zhu, and Trevor Darrell.\n2020. Minimax Active Learning. ArXiv preprint, abs/2012.10467.\n(Cited on page 69)\nBradley Efron. 1992. Bootstrap Methods: Another Look at the\nJackknife. In Breakthroughs in statistics: Methodology and dis-\ntribution, pages 569–593. (Cited on page 17)\nBradley Efron. 2022. Exponential Families in Theory and Practice.\n(Cited on page 21)\nBradley Efron and Robert J. Tibshirani. 1994. An Introduction to\nthe Bootstrap. (Cited on pages 87 and 151)\nBryan Eikema. 2024.\nThe Effect of Generalisation on the In-\nadequacy of the Mode.\nIn Proceedings of the 1st Workshop\non Uncertainty-Aware NLP (UncertaiNLP 2024), pages 87–92.\n(Cited on pages 64, 123, and 162)\nBryan Eikema and Wilker Aziz. 2020. Is MAP Decoding all You\nNeed? The Inadequacy of the Mode in Neural Machine Trans-\nlation. In Proceedings of the 28th International Conference on\nComputational Linguistics, pages 4506–4520. (Cited on pages 64,\n123, 138, and 162)\nYousef El-Laham, Niccolò Dalmasso, Elizabeth Fons, and Svitlana\nVyetrenko. 2023. Deep Gaussian Mixture Ensembles. In Uncer-\ntainty in Artificial Intelligence, pages 549–559. PMLR. (Cited\non page 46)\nPaul E Engelhardt and Fernanda Ferreira. 2010. Processing Coor-\ndination Ambiguity. Language and speech, 53(4):494–509. (Cited\non page 28)\nPiero Esposito. 2020.\nBlitz - Bayesian Layers in Torch Zoo\n(a Bayesian Deep Learing Library for Torch).\nhttps://\ngithub.com/piEsposito/blitz-bayesian-deep-learning/.\n(Cited on page 333)\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu.\n1996. A Density-based Algorithm for Discovering Clusters in\nLarge Spatial Databases with Noise. In Proceedings of the Second\nInternational Conference on Knowledge Discovery and Data Min-\ning (KDD-96), Portland, Oregon, USA, pages 226–231. (Cited\non pages 147 and 338)\nBIBLIOGRAPHY\n196\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov,\nSergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsym-\nbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin,\net al. 2024. Fact-checking the Output of Large Language Mod-\nels via Token-level Uncertainty Quantification. ArXiv preprint,\nabs/2403.04696. (Cited on page 60)\nWade Fagen-Ulmschneider. 2015.\nPerception of Probabil-\nity Words.\nhttps://waf.cs.illinois.edu/visualizations/\nPerception-of-Probability-Words/. Accessed: 2024-06-10.\n(Cited on page 146)\nAhmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Chukwunyere Osi,\nPrateek Sharma, Fan Chen, and Lei Jiang. 2023. LLMCarbon:\nModeling the End-to-end Carbon Footprint of Large Language\nModels. In The Twelfth International Conference on Learning\nRepresentations. (Cited on page 326)\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed\nEl-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guil-\nlaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch,\nVitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand\nJoulin. 2021. Beyond English-centric Multilingual Machine Trans-\nlation. Journal of Machine Learning Research (JMLR), 22:107:1–\n107:48. (Cited on page 129)\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical\nNeural Story Generation. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 889–898. (Cited on pages 92, 123,\nand 135)\nAntónio Farinhas, Chrysoula Zerva, Dennis Ulmer, and André F. T.\nMartins. 2024. Non-exchangeable Conformal Risk Control. In\nThe Twelfth International Conference on Learning Representa-\ntions. (Cited on pages 40, 127, 138, and 164)\nYassir Fathullah and Mark J. F. Gales. 2022. Self-distribution\nDistillation: Efficient Uncertainty Estimation. In Uncertainty\nin Artificial Intelligence, Proceedings of the Thirty-Eighth Con-\nference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5\nAugust 2022, Eindhoven, The Netherlands, volume 180 of Pro-\nceedings of Machine Learning Research, pages 663–673. (Cited\non page 55)\nYassir Fathullah, Puria Radmard, Adian Liusie, and Mark J. F.\nGales. 2024. Efficient Estimation of Sequence-level Attributes\nBIBLIOGRAPHY\n197\nwith Proxies. In Proceedings of the 18th Conference of the Euro-\npean Chapter of the Association for Computational Linguistics,\nEACL 2024 - Volume 1: Long Papers, St. Julian’s, Malta, March\n17-22, 2024, pages 1478–1496. Association for Computational\nLinguistics. (Cited on pages 57 and 168)\nMarco Federici, Ryota Tomioka, and Patrick Forré. 2021.\nAn\nInformation-theoretic Approach to Distribution Shifts. In Ad-\nvances in Neural Information Processing Systems 34: Annual\nConference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages 17628–17641.\n(Cited on page 113)\nZhengcong Fei, Xu Yan, Shuhui Wang, and Qi Tian. 2022. DeeCap:\nDynamic Early Exiting for Efficient Image Captioning.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022,\npages 12206–12216. (Cited on page 68)\nShai Feldman, Stephen Bates, and Yaniv Romano. 2021. Improving\nConditional Coverage via Orthogonal Quantile Regression. In\nAdvances in Neural Information Processing Systems 34: Annual\nConference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages 2060–2071.\n(Cited on page 166)\nYunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, and\nJulia Kempe. A Tale of Tails: Model Collapse as a Change\nof Scaling Laws. In ICLR 2024 Workshop on Navigating and\nAddressing Data Problems for Foundation Models. (Cited on\npage 172)\nPatrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de\nSouza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022.\nQuality-aware Decoding for Neural Machine Translation. In\nProceedings of the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 1396–1412. (Cited on page 138)\nJavier Ferrando, Gerard I. Gállego, Belen Alastruey, Carlos Es-\ncolano, and Marta R. Costa-jussà. 2022. Towards Opening the\nBlack Box of Neural Machine Translation: Source and Target\nInterpretations of the Transformer. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Process-\ning, pages 8756–8769. (Cited on page 164)\nBIBLIOGRAPHY\n198\nFernanda Ferreira and John M. Henderson. 1991. Recovery From\nMisanalyses Of Garden-Path Sentences. Journal of Memory and\nLanguage, 30(6):725–745. (Cited on pages 4 and 29)\nAdam Fisch, Tal Schuster, Tommi S. Jaakkola, and Regina Barzilay.\n2022. Conformal Prediction Sets with Limited False Positives.\nIn International Conference on Machine Learning, ICML 2022,\n17-23 July 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages 6514–6532.\n(Cited on page 40)\nJerry Fodor. 2001.\nLanguage, Thought and Compositionality.\nRoyal Institute of Philosophy Supplements, 48:227–242. (Cited\non page 27)\nAntske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen,\nPiek Vossen, and Nuno Freire. 2013. Offspring from Reproduction\nProblems: What Replication Failure Teaches Us. In Proceedings\nof the 51st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1691–1701. (Cited\non pages 72, 75, and 77)\nStanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. 2019.\nDeep Ensembles: A Loss Landscape Perspective. ArXiv preprint,\nabs/1912.02757. (Cited on page 46)\nVincent Fortuin, Adrià Garriga-Alonso, Sebastian W. Ober, Florian\nWenzel, Gunnar Rätsch, Richard E. Turner, Mark van der Wilk,\nand Laurence Aitchison. 2022. Bayesian Neural Network Priors\nRevisited. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022.\n(Cited on page 38)\nMeire Fortunato, Charles Blundell, and Oriol Vinyals. 2017.\nBayesian\nRecurrent\nNeural\nNetworks.\nArXiv\npreprint,\nabs/1704.02798. (Cited on page 111)\nRina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, and\nRyan J. Tibshirani. 2021. The Limits of Distribution-free Con-\nditional Predictive Inference.\nInformation and Inference: A\nJournal of the IMA, 10(2):455–482. (Cited on page 166)\nGianni Franchi, Xuanlong Yu, Andrei Bursuc, Emanuel Aldea,\nSeverine Dubuisson, and David Filliat. 2022. Latent Discriminant\nDeterministic Uncertainty. In European Conference on Computer\nVision, pages 243–260. Springer. (Cited on page 57)\nBruce Fraser. 1975. Hedged Performatives. In Speech Acts, pages\n187–210. (Cited on page 30)\nBIBLIOGRAPHY\n199\nLyn Frazier, Alan Munn, and Charles Clifton. 2000. Processing\nCoordinate Structures. Journal of Psycholinguistic Research,\n29:343–370. (Cited on page 28)\nLinton C. Freeman. 1965. Elementary Applied Statistics: For\nStudents in Behavioral Science. (Cited on page 47)\nMarkus Freitag, Behrooz Ghorbani, and Patrick Fernandes. 2023.\nEpsilon Sampling Rocks: Investigating Sampling Strategies for\nMinimum Bayes Risk Decoding for Machine Translation. In Find-\nings of the Association for Computational Linguistics: EMNLP\n2023, Singapore, December 6-10, 2023, pages 9198–9209. Associ-\nation for Computational Linguistics. (Cited on page 138)\nSteven Frisson. 2009. Semantic Underspecification in Language\nProcessing. Language and Linguistics Compass, 3(1):111–127.\n(Cited on pages 26 and 27)\nYarin Gal. 2016. Uncertainty in Deep Learning. (Cited on pages 41\nand 47)\nYarin Gal and Zoubin Ghahramani. 2016a.\nA Theoretically-\ngrounded Application of Dropout in Recurrent Neural Networks.\nIn Advances in Neural Information Processing Systems 29: An-\nnual Conference on Neural Information Processing Systems 2016,\nDecember 5-10, 2016, Barcelona, Spain, pages 1019–1027. (Cited\non pages 44, 111, and 332)\nYarin Gal and Zoubin Ghahramani. 2016b. Dropout as a Bayesian\nApproximation: Representing Model Uncertainty on Deep Learn-\ning. In Proceedings of the 33nd International Conference on\nMachine Learning, ICML 2016, New York City, NY, USA, June\n19-24, 2016, volume 48 of JMLR Workshop and Conference Pro-\nceedings, pages 1050–1059. (Cited on pages 44, 50, 109, and 111)\nYarin Gal, Jiri Hron, and Alex Kendall. 2017a. Concrete Dropout.\nIn Advances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pages 3581–3590.\n(Cited on page 44)\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017b. Deep\nBayesian Active Learning with Image Data. In Proceedings of\nthe 34th International Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70\nof Proceedings of Machine Learning Research, pages 1183–1192.\n(Cited on page 69)\nBIBLIOGRAPHY\n200\nMirta Galesic and Rocio Garcia-Retamero. 2010. Statistical Numer-\nacy for Health: A Cross-Cultural Comparison with Probabilistic\nNational Samples. Archives of internal medicine, 170(5):462–468.\n(Cited on page 66)\nBolin Gao and Lacra Pavel. 2017. On the Properties of the Softmax\nFunction with Application in Game Theory and Reinforcement\nLearning. ArXiv preprint, abs/1704.00805. (Cited on page 284)\nSongyang Gao, Qiming Ge, Wei Shen, Shihan Dou, Junjie Ye, Xiao\nWang, Rui Zheng, Yicheng Zou, Zhi Chen, Hang Yan, et al.\n2024a. Linear Alignment: A Closed-form Solution for Align-\ning Human Preferences without Tuning and Feedback. ArXiv\npreprint, abs/2401.11458. (Cited on pages 138 and 164)\nXiang Gao, Jiaxin Zhang, Lalla Mouatadid, and Kamalika Das.\n2024b. SPUQ: Perturbation-Based Uncertainty Quantification\nfor Large Language Models. In Proceedings of the 18th Conference\nof the European Chapter of the Association for Computational\nLinguistics, EACL 2024 - Volume 1: Long Papers, St. Julian’s,\nMalta, March 17-22, 2024, pages 2336–2346. Association for\nComputational Linguistics. (Cited on page 61)\nCarlos García Rodríguez, Jordi Vitrià, and Oscar Mora. 2020.\nUncertainty-based Human-in-the-loop Deep Learning for Land\nCover Segmentation. Remote Sensing, 12(22):3836. (Cited on\npage 69)\nJacob R. Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel,\nand Andrew Gordon Wilson. 2018. GPyTorch: Blackbox Matrix-\nmatrix Gaussian Process Inference with GPU Acceleration. In\nAdvances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018,\nNeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages\n7587–7597. (Cited on page 333)\nTimur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P.\nVetrov, and Andrew Gordon Wilson. 2018. Loss Surfaces, Mode\nConnectivity, and Fast Ensembling of DNNs. In Advances in\nNeural Information Processing Systems 31: Annual Conference\non Neural Information Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montréal, Canada, pages 8803–8812. (Cited\non page 46)\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-\nnifer Wortman Vaughan, Hanna M. Wallach, Hal Daumé III,\nand Kate Crawford. 2021. Datasheets for Datasets. Commun.\nACM, 64(12):86–92. (Cited on page 74)\nBIBLIOGRAPHY\n201\nDirk Geeraerts. 1993. Vagueness’s Puzzles, Polysemy’s Vagaries.\n(Cited on page 27)\nSebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2022.\nRepairing the Cracked Foundation:\nA Survey of Obstacles\nin Evaluation Practices for Generated Text. ArXiv preprint,\nabs/2202.06935. (Cited on pages 71 and 78)\nYonatan Geifman and Ran El-Yaniv. 2019. Selectivenet: A Deep\nNeural Network with an Integrated Reject Option. In Proceedings\nof the 36th International Conference on Machine Learning, ICML\n2019, 9-15 June 2019, Long Beach, California, USA, volume 97\nof Proceedings of Machine Learning Research, pages 2151–2159.\n(Cited on page 57)\nAndrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson,\nAki Vehtari, Donald B. Rubin, John Carlin, Hal Stern, Donald\nRubin, and David Dunson. 2021. Bayesian Data Analysis Third\nEdition. (Cited on pages 21, 42, 81, 84, and 95)\nJiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav\nNakov, and Iryna Gurevych. 2023.\nA Survey of Language\nModel Confidence Estimation and Calibration. ArXiv preprint,\nabs/2311.08298. (Cited on pages 59 and 165)\nWalter Gerych, Yara Rizk, Vatche Isahagian, Vinod Muthusamy,\nEvelyn Duesterwald, and Praveen Venkateswaran. 2024. Who\nKnows the Answer? Finding the Best Model and Prompt for\neach Query Using Confidence-based Search. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 38, pages\n18065–18072. (Cited on page 68)\nArindam Ghosh, Thomas Schaaf, and Matthew Gormley. 2022.\nAdaFocal: Calibration-aware Adaptive Focal Loss. Advances in\nNeural Information Processing Systems, 35:1583–1595. (Cited\non page 37)\nIsaac Gibbs and Emmanuel J. Candès. 2021. Adaptive Conformal\nInference under Distribution Shift. In Advances in Neural In-\nformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 1660–1672. (Cited on page 40)\nIsaac Gibbs, John J. Cherian, and Emmanuel J. Candès. 2023. Con-\nformal Prediction with Conditional Guarantees. ArXiv preprint,\nabs/2305.12616. (Cited on page 166)\nBIBLIOGRAPHY\n202\nEric W. Gibson. 2021. The Role of p-values in Judging the Strength\nof Evidence and Realistic Replication Expectations. Statistics\nin Biopharmaceutical Research, 13(1):6–18. (Cited on page 95)\nAlexios Gidiotis and Grigorios Tsoumakas. 2022. Should We Trust\nThis Summary? Bayesian Abstractive Summarization to the\nRescue. In Findings of the Association for Computational Lin-\nguistics: ACL 2022, pages 4119–4131. (Cited on page 60)\nMario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernández, and\nBarbara Plank. 2023. What Comes Next? Evaluating Uncer-\ntainty in Neural Text Generators against Human Production\nVariability. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2023, Singa-\npore, December 6-10, 2023, pages 14349–14371. Association for\nComputational Linguistics. (Cited on pages 64 and 167)\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep\nSparse Rectifier Neural Networks. In Proceedings of the Four-\nteenth International Conference on Artificial Intelligence and\nStatistics, pages 315–323. PJMLR Workshop and Conference\nProceedings. (Cited on page 100)\nTaisiya Glushkova, Chrysoula Zerva, Ricardo Rei, and André F. T.\nMartins. 2021. Uncertainty-aware Machine Translation Evalua-\ntion. In Findings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 3920–3938. (Cited on page 143)\nTilmann Gneiting and Adrian E. Raftery. 2007. Strictly Proper\nScoring Rules, Prediction, and Estimation. Journal of the Ameri-\ncan Statistical Association, 102(477):359–378. (Cited on page 37)\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex.\n2019. OpenWebText Corpus. http://Skylion007.github.io/\nOpenWebTextCorpus. (Cited on page 129)\nGold Standard. 2024.\nThe Gold Standard Marketplace.\nLast\naccessed 21.06.24. (Cited on page 327)\nZhiqiang Gong, Ping Zhong, and Weidong Hu. 2019. Diversity\nin Machine Learning. IEEE Access, 7:64323–64350. (Cited on\npage 75)\nIrving John Good. 1971. 46656 Varieties of Bayesians. American\nStatistician, 25(5):62. (Cited on page 19)\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep\nlearning. MIT press. (Cited on pages xv and 33)\nBIBLIOGRAPHY\n203\nKyle Gorman and Steven Bedrick. 2019. We Need to Talk about\nStandard Splits. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages 2786–2791.\n(Cited on pages 71 and 76)\nJohannes Graën, Mara Bertamini, Martin Volk, Mark Cieliebak,\nDon Tuggener, and Fernando Benites. 2018. Cutter–A Universal\nMultilingual Tokenizer. In CEUR Workshop Proceedings, 2226,\npages 75–81. CEUR-WS. (Cited on page 163)\nAlex Graves. 2011.\nPractical Variational Inference for Neural\nNetworks. In Advances in Neural Information Processing Systems\n24: 25th Annual Conference on Neural Information Processing\nSystems 2011. Proceedings of a meeting held 12-14 December\n2011, Granada, Spain, pages 2348–2356. (Cited on page 43)\nAlex Graves. 2012. Sequence Transduction with Recurrent Neural\nNetworks. arXiv preprint arXiv:1211.3711. (Cited on page 135)\nSander Greenland, Stephen J. Senn, Kenneth J. Rothman, John B.\nCarlin, Charles Poole, Steven N. Goodman, and Douglas G.\nAltman. 2016. Statistical Tests, p-values, Confidence Intervals,\nand Power: A Guide to Misinterpretations. European journal of\nepidemiology, 31(4):337–350. (Cited on pages 80 and 95)\nH. Paul Grice. 1957. Meaning. The Philosophical Review, 66(3):377–\n388. (Cited on page 30)\nStefan Th. Gries. 2015. Polysemy. Handbook of cognitive linguistics,\n39:472–490. (Cited on page 26)\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney\nKinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian\nMagnusson, Yizhong Wang, et al. 2024. OLmo: Accelerating the\nScience of Language Models. ArXiv preprint, abs/2402.00838.\n(Cited on pages 78, 90, and 174)\nCornelia Gruber, Katharina Hechinger, Matthias Aßenmacher,\nGöran Kauermann, and Barbara Plank. 2024. More Labels or\nCases? Assessing Label Variation in Natural Language Inference.\nIn Proceedings of the Third Workshop on Understanding Implicit\nand Underspecified Language, pages 22–32. (Cited on pages 63,\n64, 75, and 166)\nCornelia Gruber, Patrick Oliver Schenk, Malte Schierholz, Frauke\nKreuter, and Göran Kauermann. 2023. Sources of Uncertainty\nin Machine Learning–A Statisticians’ View.\nArXiv preprint,\nabs/2305.16703. (Cited on page 167)\nBIBLIOGRAPHY\n204\nSebastian Gruber and Florian Buettner. 2022. Better Uncertainty\nCalibration via Proper Scores for Classification and Beyond.\nAdvances in Neural Information Processing Systems, 35:8618–\n8632. (Cited on page 37)\nNuno M. Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry\nHaddow, Alexandra Birch, Pierre Colombo, and André F.T.\nMartins. 2023a. Hallucinations in Large Multilingual Translation\nModels.\nTransactions of the Association for Computational\nLinguistics, 11:1500–1517. (Cited on page 164)\nNuno M. Guerreiro, Elena Voita, and André F.T. Martins. 2023b.\nLooking for a Needle in a Haystack: A Comprehensive Study of\nHallucinations in Neural Machine Translation. In Proceedings of\nthe 17th Conference of the European Chapter of the Association\nfor Computational Linguistics, pages 1059–1075.\n(Cited on\npage 123)\nYu Gui, Ying Jin, and Zhimei Ren. 2024. Conformal Alignment:\nKnowing when to Trust Foundation Models with Guarantees.\nArXiv preprint, abs/2405.10301. (Cited on pages 59 and 164)\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia\nKonyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant,\nAlex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced\nSelf-training (ReST) for Language Modeling. ArXiv preprint,\nabs/2308.08998. (Cited on page 172)\nOdd Erik Gundersen and Sigbjørn Kjensmo. 2018. State of the\nArt: Reproducibility in Artificial Intelligence. In Proceedings of\nthe Thirty-Second AAAI Conference on Artificial Intelligence,\n(AAAI-18), the 30th innovative Applications of Artificial Intelli-\ngence (IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018, pages 1644–1651. (Cited\non page 71)\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017.\nOn Calibration of Modern Neural Networks. In Proceedings of\nthe 34th International Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70\nof Proceedings of Machine Learning Research, pages 1321–1330.\n(Cited on pages 37, 98, 115, and 130)\nYanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and Chloé\nClavel. 2023.\nThe Curious Decline of Linguistic Diversity:\nTraining Language Models on Synthetic Text. ArXiv preprint,\nabs/2311.09807. (Cited on page 172)\nBIBLIOGRAPHY\n205\nKartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas\nMensink, Cristian Sminchisescu, and Richard Hartley. 2021. Cal-\nibration of Neural Networks Using Splines. In 9th International\nConference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. (Cited on page 37)\nNeha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum,\nAnkit Singh Rawat, Aditya Krishna Menon, and Sanjiv Ku-\nmar. 2024. Language Model Cascades: Token-level Uncertainty\nand Beyond. ArXiv preprint, abs/2404.10136. (Cited on pages 60\nand 68)\nSangchul Hahn and Heeyoul Choi. 2019. Self-knowledge Distillation\nin Natural Language Processing. In Proceedings of the Inter-\nnational Conference on Recent Advances in Natural Language\nProcessing (RANLP 2019), pages 423–430. (Cited on page 133)\nLars Kai Hansen and Peter Salamon. 1990. Neural Network En-\nsembles. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 12(10):993–1001. (Cited on page 46)\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt,\nRalf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser,\nJulian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern,\nMatti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew\nBrett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe,\nPearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler\nReddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke,\nand Travis E. Oliphant. 2020. Array Programming with NumPy.\nNature, 585(7825):357–362. (Cited on page 325)\nManuel Haussmann, Sebastian Gerwinn, and Melih Kandemir. 2019.\nBayesian Evidential Deep Learning with PAC Regularization.\nArXiv preprint, abs/1906.00816. (Cited on pages 49 and 55)\nKatri Haverinen, Jenna Nyblom, Timo Viljanen, Veronika Laippala,\nSamuel Kohonen, Anna Missilä, Stina Ojala, Tapio Salakoski,\nand Filip Ginter. 2014. Building the Essential Resources for\nFinnish: The Turku Dependency Treebank. Language Resources\nand Evaluation, 48:493–531. Open access. (Cited on pages 112\nand 113)\nJianfeng He. 2024. Uncertainty Estimation on Natural Language\nProcessing. (Cited on page 165)\nJianfeng He, Linlin Yu, Shuo Lei, Chang-Tien Lu, and Feng Chen.\n2023a. Uncertainty Estimation on Sequential Labeling via Un-\ncertainty Transmission. ArXiv preprint, abs/2311.08726. (Cited\non page 59)\nBIBLIOGRAPHY\n206\nJianxing He, Sally L. Baxter, Jie Xu, Jiming Xu, Xingtao Zhou, and\nKang Zhang. 2019. The Practical Implementation of Artificial\nIntelligence Technologies in Medicine. Nature medicine, 25(1):30–\n36. (Cited on page 74)\nJunxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021a.\nEfficient Nearest Neighbor Language Models. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 5703–5714. (Cited on pages 124 and 138)\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023b. DeBER-\nTaV3: Improving DeBERTa Using ElectrasStyle Pre-training\nwith Gradient-disentangled Embedding Sharing. In The Eleventh\nInternational Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. (Cited on page 147)\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.\n2021b. DeBERTa: Decoding-enhanced BERT with Disentangled\nAttention. In 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\n(Cited on page 147)\nRamya Hebbalaguppe, Jatin Prakash, Neelabh Madan, and Chetan\nArora. 2022. A Stitch in Time Saves Nine: A Train-time Reg-\nularizing Loss for Improved Neural Network Calibration. In\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022,\npages 16060–16069. (Cited on page 37)\nMichael A. Hedderich, Lukas Lange, Heike Adel, Jannik Strötgen,\nand Dietrich Klakow. 2021. A Survey on Recent Approaches\nfor Natural Language Processing in Low-resource Scenarios. In\nProceedings of the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 2545–2568. (Cited on page 113)\nMatthias Hein, Maksym Andriushchenko, and Julian Bitterwolf.\n2019. Why ReLU Networks Yield High-confidence Predictions\nFar Away from the Training Data and How to Mitigate the\nProblem. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 41–50. (Cited on pages 68, 98, 103, 104, 105, 106,\n112, and 283)\nPatrick Hemmer, Niklas Kühl, and Jakob Schöffer. 2022. DEAL:\nDeep Evidential Active Learning for Image Classification. Deep\nLearning Applications, Volume 3, pages 171–192. (Cited on\npage 69)\nBIBLIOGRAPHY\n207\nRoee Hendel, Mor Geva, and Amir Globerson. 2023. In-context\nlearning creates task vectors. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, Singapore, December\n6-10, 2023, pages 9318–9333. Association for Computational\nLinguistics. (Cited on page 91)\nPeter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan\nJurafsky, and Joelle Pineau. 2020.\nTowards the Systematic\nReporting of the Energy and Carbon Footprints of Machine\nLearning. Journal of Machine Learning Research, 21(248):1–43.\n(Cited on page 71)\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau,\nDoina Precup, and David Meger. 2018. Deep Reinforcement\nLearning that Matters. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-18), the 30th\ninnovative Applications of Artificial Intelligence (IAAI-18), and\nthe 8th AAAI Symposium on Educational Advances in Artificial\nIntelligence (EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 3207–3214. (Cited on page 79)\nDan Hendrycks and Thomas G. Dietterich. 2019. Benchmark-\ning Neural Network Robustness to Common Corruptions and\nPerturbations. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. (Cited on page 133)\nDan Hendrycks and Kevin Gimpel. 2017. A Baseline for Detect-\ning Misclassified and Out-of-distribution Examples in Neural\nNetworks. In 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\n(Cited on pages 40, 54, 100,\nand 109)\nDan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph,\nJustin Gilmer, and Balaji Lakshminarayanan. 2020. AugMix:\nA Simple Data Processing Method to Improve Robustness and\nUncertainty. In 8th International Conference on Learning Rep-\nresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. (Cited on page 38)\nJames Hensman and Neil D. Lawrence. 2014.\nNested Varia-\ntional Compression in Deep Gaussian Processes. arXiv preprint\narXiv:1412.1370. (Cited on page 47)\nJosé Miguel Hernández-Lobato and Ryan P. Adams. 2015. Proba-\nbilistic Backpropagation for Scalable Learning of Bayesian Neural\nNetworks. In Proceedings of the 32nd International Conference\nBIBLIOGRAPHY\n208\non Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference Proceedings, pages\n1861–1869. (Cited on page 44)\nDaniel\nHershcovich,\nStella\nFrank,\nHeather\nLent,\nMiryam\nde Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele\nBugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang\nCui, Constanza Fierro, Katerina Margatina, Phillip Rust, and\nAnders Søgaard. 2022.\nChallenges and Strategies in Cross-\ncultural NLP. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), pages 6997–7013. (Cited on page 6)\nDonald Hindle and Mats Rooth. 1990. Structural Ambiguity and\nLexical Relations. In Speech and Natural Language: Proceedings\nof a Workshop Held at Hidden Valley, Pennsylvania, June 24-\n27,1990. (Cited on page 28)\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the\nKnowledge in a Neural Network. ArXiv preprint, abs/1503.02531.\n(Cited on page 55)\nGeoffrey E. Hinton and Drew Van Camp. 1993. Keeping the Neural\nNetworks Simple by Minimizing the Description Length of the\nWeights.\nIn Proceedings of the Sixth Annual Conference on\nComputational learning Theory, pages 5–13. (Cited on page 43)\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong Short-\nterm Memory. Neural computation, 9(8):1735–1780. (Cited on\npages 111 and 338)\nPaul Hofman, Yusuf Sale, and Eyke Hüllermeier. 2024. Quantifying\nAleatoric and Epistemic Uncertainty with Proper Scoring Rules.\nArXiv preprint, abs/2404.12215. (Cited on page 168)\nAndreas Nugaard Holm, Dustin Wright, and Isabelle Augenstein.\n2023. Revisiting Softmax for Uncertainty Approximation in Text\nClassification. Information, 14(7):420. (Cited on page 59)\nJanet Holmes. 1982. Expressing Doubt and Certainty in English.\nRELC journal, 13(2):9–28. (Cited on page 30)\nBenedikt Höltgen and Robert C. Williamson. 2023. On the Richness\nof Calibration. In Proceedings of the 2023 ACM Conference on\nFairness, Accountability, and Transparency, pages 1124–1138.\n(Cited on page 147)\nBIBLIOGRAPHY\n209\nThomas Holtgraves and Audrey Perdew. 2016. Politeness and the\nCommunication Of Uncertainty. Cognition, 154:1–10. (Cited on\npage 30)\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi.\n2020. The Curious Case of Neural Text Degeneration. In 8th\nInternational Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n(Cited on\npages 92, 123, 130, and 162)\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David\nGolub, and Yejin Choi. 2018. Learning to Write with Cooperative\nDiscriminators. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1638–1649. (Cited on pages 92 and 135)\nSara Hooker. 2021. The Hardware Lottery. Communications of\nthe ACM, 64(12):58–65. (Cited on page 82)\nStephen C. Hora. 1996. Aleatory and Epistemic Uncertainty in\nProbability Elicitation with an Example from Hazardous Waste\nManagement. Reliability Engineering & System Safety, 54(2-\n3):217–223. (Cited on page 167)\nKurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989.\nMulti-layer Feedforward Networks are Universal Approximators.\nNeural networks, 2(5):359–366. (Cited on page 41)\nBairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang,\nand Yang Zhang. 2023a. Decomposing Uncertainty for Large\nLanguage Models through Input Clarification Ensembling. ArXiv\npreprint, abs/2311.08718. (Cited on page 61)\nBairu Hou, Joe O’Connor, Jacob Andreas, Shiyu Chang, and Yang\nZhang. 2023b. Promptboosting: Black-box Text Classification\nwith ten Forward-passes. In International Conference on Machine\nLearning, pages 13309–13324. PMLR. (Cited on page 61)\nDirk Hovy and Shrimai Prabhumoye. 2021. Five Sources of Bias in\nNatural Language Processing. Language and Linguistics Com-\npass, 15(8):e12432. (Cited on pages 74 and 82)\nDirk Hovy and Shannon L. Spruit. 2016. The Social Impact of\nNatural Language Processing. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 591–598. (Cited on pages 71\nand 170)\nBIBLIOGRAPHY\n210\nHengtong Hu, Lingxi Xie, Xinyue Huo, Richang Hong, and Qi Tian.\n2022. Vibration-based Uncertainty Estimation for Learning from\nLimited Supervision.\nIn European Conference on Computer\nVision, pages 160–176. Springer. (Cited on page 56)\nJennifer Hu, Roger Levy, Judith Degen, and Sebastian Schus-\nter. 2023a. Expectations over Unspoken Alternatives Predict\nPragmatic Inferences. Transactions of the Association for Com-\nputational Linguistics, 11:885–901. (Cited on page 167)\nMengting Hu, Zhen Zhang, Shiwan Zhao, Minlie Huang, and\nBingzhe Wu. 2023b. Uncertainty in Natural Language Process-\ning: Sources, Quantification, and Applications. ArXiv preprint,\nabs/2306.04459. (Cited on page 165)\nYibo Hu, Yuzhe Ou, Xujiang Zhao, Jin-Hee Cho, and Feng Chen.\n2021. Multidimensional Uncertainty-aware Evidential Neural\nNetworks. In Thirty-Fifth AAAI Conference on Artificial In-\ntelligence, AAAI 2021, Thirty-Third Conference on Innovative\nApplications of Artificial Intelligence, IAAI 2021, The Eleventh\nSymposium on Educational Advances in Artificial Intelligence,\nEAAI 2021, Virtual Event, February 2-9, 2021, pages 7815–7822.\n(Cited on page 56)\nHaiwen Huang, Joost van Amersfoort, and Yarin Gal. 2021. Decom-\nposing Representations for Deterministic Uncertainty Estimation.\nArXiv preprint, abs/2112.00856. (Cited on page 57)\nXinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed\nHassani, Insup Lee, Osbert Bastani, and Edgar Dobriban. 2024.\nUncertainty in Language Models: Assessment through Rank-\ncalibration. ArXiv preprint, abs/2404.03163. (Cited on page 60)\nYan Huang. 2014. Pragmatics. (Cited on page 30)\nYuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, and\nLei Ma. 2023. Look before You Leap: An Exploratory Study of\nUncertainty Measurement for Large Language Models. ArXiv\npreprint, abs/2307.10236. (Cited on page 59)\nEyke Hüllermeier, Sébastien Destercke, and Mohammad Hossein\nShaker. 2022. Quantification of Credal Uncertainty in Machine\nLearning: A Critical Analysis and Empirical Comparison. In\nUncertainty in Artificial Intelligence, Proceedings of the Thirty-\nEighth Conference on Uncertainty in Artificial Intelligence, UAI\n2022, 1-5 August 2022, Eindhoven, The Netherlands, volume\n180 of Proceedings of Machine Learning Research, pages 548–557.\n(Cited on pages 58 and 168)\nBIBLIOGRAPHY\n211\nEyke Hüllermeier, Thomas Fober, and Marco Mernberger. 2013.\nInductive Bias, pages 1018–1018. (Cited on page 8)\nEyke Hüllermeier and Willem Waegeman. 2021. Aleatoric and\nEpistemic Uncertainty in Machine Learning: An Introduction to\nConcepts and Methods. Mach. Learn., 110(3):457–506. (Cited\non page 167)\nAnders Humlum and Emilie Vestergaard. 2024. The Adoption of\nChatGPT. University of Chicago, Becker Friedman Institute for\nEconomics Working Paper, (2024-50). (Cited on page 170)\nDieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe,\nYanai Elazar, Tiago Pimentel, Christos E. Christodoulopoulos,\nKarim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer,\nFlorian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Kous-\ntuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan\nCotterell, and Zhijing Jin. 2023. A taxonomy and review of\ngeneralization research in NLP. Nature Machine Intelligence,\n5(10):1161–1174. (Cited on pages 99, 113, and 138)\nRasmus Hvingelby, Amalie Brogaard Pauli, Maria Barrett,\nChristina Rosted, Lasse Malm Lidegaard, and Anders Søgaard.\n2020. DaNe: A Named Entity Resource For Danish. In Proceed-\nings of the Twelfth Language Resources and Evaluation Confer-\nence, pages 4597–4604. (Cited on pages 114 and 333)\nEvgenia Ilia and Wilker Aziz. 2024. Predict the Next Word: <\nHumans Exhibit Uncertainty in this Task and Language Models\n_____>. In Proceedings of the 18th Conference of the Euro-\npean Chapter of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 234–255. (Cited on pages 60,\n64, 162, 167, and 168)\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan En-\ngstrom, Brandon Tran, and Aleksander Madry. 2019. Adversarial\nExamples Are not Bugs, They Are Features. In Advances in\nNeural Information Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages 125–136.\n(Cited on page 77)\nNanna Inie. 2024. What Motivates People to Trust ’AI’ Systems?\n(Cited on page 64)\nJohn P. A. Ioannidis. 2005. Why most Published Research Findings\nAre False. PLOS Medicine, 2(8):null. (Cited on page 82)\nBIBLIOGRAPHY\n212\nSergey Ioffe and Christian Szegedy. 2015. Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Co-\nvariate Shift. In Proceedings of the 32nd International Conference\non Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference Proceedings, pages\n448–456. (Cited on page 44)\nTovah Irwin, Kyra Wilson, and Alec Marantz. 2023. BERT Shows\nGarden Path Effects. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Computational\nLinguistics, pages 3220–3232. (Cited on page 29)\nPavel Izmailov, Wesley Maddox, Polina Kirichenko, Timur Garipov,\nDmitry P. Vetrov, and Andrew Gordon Wilson. 2019. Subspace\nInference for Bayesian Deep Learning. In Proceedings of the\nThirty-Fifth Conference on Uncertainty in Artificial Intelligence,\nUAI 2019, Tel Aviv, Israel, July 22-25, 2019, volume 115 of\nProceedings of Machine Learning Research, pages 1169–1179.\n(Cited on page 46)\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P.\nVetrov, and Andrew Gordon Wilson. 2018. Averaging Weights\nLeads to Wider Optima and Better Generalization. In Proceed-\nings of the Thirty-Fourth Conference on Uncertainty in Artificial\nIntelligence, UAI 2018, Monterey, California, USA, August 6-10,\n2018, pages 876–885. (Cited on page 46)\nPavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and An-\ndrew Gordon Wilson. 2021. What Are Bayesian Neural Network\nPosteriors really Like? In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine Learning\nResearch, pages 4629–4640. (Cited on page 38)\nAlon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Inter-\npretations with their Social Attribution. Transactions of the\nAssociation for Computational Linguistics, 9:294–310. (Cited on\npage 65)\nAlon Jacovi, Ana Marasović, Tim Miller, and Yoav Goldberg. 2021.\nFormalizing Trust in Artificial Intelligence: Prerequisites, Causes\nand Goals of Human Trust in AI. In Proceedings of the 2021\nACM conference on fairness, accountability, and transparency,\npages 624–635. (Cited on pages 3, 64, 65, and 140)\nSiddhartha Jain, Ge Liu, Jonas Mueller, and David Gifford. 2020.\nMaximizing Overall Diversity for Improved Uncertainty Esti-\nmates in Deep Ensembles. In The Thirty-Fourth AAAI Confer-\nBIBLIOGRAPHY\n213\nence on Artificial Intelligence, AAAI 2020, The Thirty-Second In-\nnovative Applications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2020, New York, NY, USA,\nFebruary 7-12, 2020, pages 4264–4271. (Cited on page 46)\nKalvik Jakkala. 2021. Deep Gaussian Processes: A Survey. ArXiv\npreprint, abs/2106.12135. (Cited on page 47)\nNathalie Japkowicz and Mohak Shah. 2011. Evaluating Learning\nAlgorithms: A Classification Perspective. (Cited on page 83)\nAlireza Javanmardi, David Stutz, and Eyke Hüllermeier. 2024.\nConformalized\nCredal\nSet\nPredictors.\nArXiv\npreprint,\nabs/2402.10723. (Cited on pages 58, 64, and 75)\nFrederick Jelinek. 1980. Interpolated Estimation of Markov Source\nParameters from Sparse Data. In Proc. Workshop on Pattern\nRecognition in Practice, 1980. (Cited on page 297)\nTheis Ingerslev Jensen, Bryan T. Kelly, and Lasse Heje Pedersen.\n2021. Is there a Replication Crisis in Finance? Technical report,\nNational Bureau of Economic Research. (Cited on page 71)\nDisi Ji, Padhraic Smyth, and Mark Steyvers. 2020. Can I Trust\nMy Fairness Metric? Assessing Fairness with Unlabeled Data\nand Bayesian Inference. In Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual. (Cited on page 67)\nHaozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, and Minlie\nHuang. 2023a. Tailoring Language Generation Models under\nTotal Variation Distance. In The Eleventh International Confer-\nence on Learning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net. (Cited on page 60)\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu,\nEtsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.\n2023b. Survey of Hallucination in Natural Language Generation.\nACM Computing Surveys, 55(12):1–38. (Cited on pages 123\nand 171)\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Florian\nBressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\net al. 2023a. Mistral 7B. ArXiv preprint, abs/2310.06825. (Cited\non pages 78 and 90)\nBIBLIOGRAPHY\n214\nHeinrich Jiang, Been Kim, Melody Y. Guan, and Maya R. Gupta.\n2018. To Trust or not to Trust a Classifier. In Advances in\nNeural Information Processing Systems 31: Annual Conference\non Neural Information Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montréal, Canada, pages 5546–5557. (Cited\non page 57)\nNan-Jiang Jiang, Chenhao Tan, and Marie-Catherine de Marneffe.\n2023b. Understanding and Predicting Human Label Variation\nin Natural Language Inference through Explanation. ArXiv\npreprint, abs/2304.12443. (Cited on page 166)\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig.\n2021. How Can We Know When Language Models Know? On\nthe Calibration of Language Models for Question Answering.\nTransactions of the Association for Computational Linguistics\n(TACL), 9:962–977. (Cited on page 156)\nYing Jin and Zhimei Ren. 2024. Confidence on the Focal: Con-\nformal Prediction with Selection-conditional Coverage. ArXiv\npreprint, abs/2403.03868. (Cited on page 166)\nWittawat Jitkrittum, Neha Gupta, Aditya K. Menon, Harikrishna\nNarasimhan, Ankit Rawat, and Sanjiv Kumar. 2024. When Does\nConfidence-Based Cascade Deferral Suffice? Advances in Neural\nInformation Processing Systems, 36. (Cited on page 68)\nLeslie K. John, George Loewenstein, and Drazen Prelec. 2012.\nMeasuring the Prevalence of Questionable Research Practices\nwith Incentives for Truth Telling. Psychological science, 23(5):524–\n532. (Cited on page 71)\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale\nSimilarity Search with GPUs. IEEE Transactions on Big Data,\n7(3):535–547. (Cited on pages 127, 129, 138, and 338)\nTaejong Joo, Uijung Chung, and Min-Gwan Seo. 2020. Being\nBayesian about Categorical Probability. In Proceedings of the\n37th International Conference on Machine Learning, ICML 2020,\n13-18 July 2020, Virtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 4950–4961. (Cited on page 281)\nMatt Jordan, Justin Lewis, and Alexandros G. Dimakis. 2019.\nProvable Certificates for Adversarial Examples: Fitting a Ball in\nthe Union of Polytopes. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 14059–14069. (Cited on page 98)\nBIBLIOGRAPHY\n215\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.\n2017. TriviaQA: A Large Scale Distantly Supervised Challenge\nDataset for Reading Comprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1601–1611. (Cited on pages 90,\n141, and 148)\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and\nMonojit Choudhury. 2020. The State and Fate of Linguistic\nDiversity and Inclusion in the NLP World. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational\nLinguistics, pages 6282–6293. (Cited on page 6)\nDaniel Jurafsky and James H. Martin. 2022. Speech and Language\nProcessing: An Introduction to Natural Language Processing,\nComputational Linguistics, and Speech Recognition, 3rd Ed. Draft.\n(Cited on pages 28, 33, and 333)\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,\nDawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds,\nNova DasSarma, Eli Tran-Johnson, et al. 2022.\nLanguage\nModels (Mostly) Know What They Know.\nArXiv preprint,\nabs/2207.05221. (Cited on pages 60 and 62)\nAdam Tauman Kalai and Santosh S. Vempala. 2024. Calibrated\nLanguage Models Must Hallucinate. In Proceedings of the 56th\nAnnual ACM Symposium on Theory of Computing, STOC 2024,\nVancouver, BC, Canada, June 24-28, 2024, pages 160–171. ACM.\n(Cited on page 171)\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivane-\nsan Sangeetha. 2021. AMMUS: A Survey of Transformer-based\nPretrained Models in Natural Language Processing.\nArXiv\npreprint, abs/2108.05542. (Cited on page 7)\nJenna Kanerva and Filip Ginter. 2022. Out-of-domain Evaluation\nof Finnish Dependency Parsing. In Proceedings of the Thirteenth\nLanguage Resources and Evaluation Conference, pages 1114–1124.\n(Cited on pages 112 and 113)\nArchit Karandikar, Nicholas Cain, Dustin Tran, Balaji Laksh-\nminarayanan, Jonathon Shlens, Michael C. Mozer, and Becca\nRoelofs. 2021. Soft Calibration Objectives for Neural Networks.\nIn Advances in Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages 29768–29779.\n(Cited on page 37)\nBIBLIOGRAPHY\n216\nGarry Kasparov. 2017. Deep Thinking: Where Machine Intelligence\nEnds and Human Creativity Begins. (Cited on page 170)\nKate Kearns. 2017. Semantics. (Cited on pages 26 and 30)\nMaurice G. Kendall. 1938. A New Measure of Rank Correlation.\nBiometrika, 30(1/2):81–93. (Cited on page 115)\nChristopher Kennedy. 2011.\nAmbiguity and Vagueness:\nAn\nOverview. Semantics: An International Handbook of Natural\nLanguage Meaning, 1:507–535. (Cited on pages 26 and 30)\nJohn Maynard Keynes. 1921. A Treatise on Probability. (Cited on\npage 57)\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer,\nand Mike Lewis. 2021. Nearest Neighbor Machine Translation.\nIn 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021. (Cited on\npages 124 and 127)\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer,\nand Mike Lewis. 2020. Generalization through Memorization:\nNearest Neighbor Language Models. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. (Cited on pages 124 and 127)\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, At-\nticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad,\nAmanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush,\nSebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia,\nMohit Bansal, Christopher Potts, and Adina Williams. 2021.\nDynabench: Rethinking Benchmarking in NLP. In Proceedings\nof the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, pages 4110–4124. (Cited on page 76)\nHyunsu Kim, Jongmin Yoon, and Juho Lee. 2024a.\nFast En-\nsembling with Diffusion Schrödinger Bridge. ArXiv preprint,\nabs/2404.15814. (Cited on page 46)\nKwanyoung Kim, Dongwon Park, Kwang In Kim, and Se Young\nChun. 2021. Task-aware Variational Adversarial Active Learning.\nIn IEEE Conference on Computer Vision and Pattern Recogni-\ntion, CVPR 2021, virtual, June 19-25, 2021, pages 8166–8175.\n(Cited on page 69)\nSunnie S. Y. Kim, Q. Vera Liao, Mihaela Vorvoreanu, Stephanie\nBallard, and Jennifer Wortman Vaughan. 2024b. I’m Not Sure,\nBIBLIOGRAPHY\n217\nBut...: Examining the Impact of Large Language Models’ Un-\ncertainty Expression on User Reliance and Trust. In The 2024\nACM Conference on Fairness, Accountability, and Transparency,\nFAccT 2024, Rio de Janeiro, Brazil, June 3-6, 2024, pages\n822–835. ACM. (Cited on page 66)\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for\nStochastic Optimization. In 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings. (Cited on pages 85,\n329, and 332)\nDiederik P. Kingma and Max Welling. 2014. Auto-encoding Vari-\national Bayes. In 2nd International Conference on Learning\nRepresentations, ICLR 2014, Banff, AB, Canada, April 14-16,\n2014, Conference Track Proceedings. (Cited on pages 43 and 166)\nJohn Kirchenbauer, Jacob Oaks, and Eric Heim. 2022.\nWhat\nIs Your Metric Telling You? Evaluating Classifier Calibration\nunder Context-specific Definitions of Reliability. ArXiv preprint,\nabs/2205.11454. (Cited on page 37)\nAndreas Kirsch and Yarin Gal. 2022. Unifying Approaches in\nActive Learning and Active Sampling via Fisher Information\nand Information-theoretic Quantities. Transactions on Machine\nLearning Research (TMLR), 2022. (Cited on page 69)\nAndreas Kirsch, Joost van Amersfoort, and Yarin Gal. 2019. Batch-\nBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian\nActive Learning. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 7024–7035. (Cited on page 69)\nRebecca Knowles and Philipp Koehn. 2016. Neural Interactive\nTranslation Prediction. In Conferences of the Association for\nMachine Translation in the Americas: MT Researchers’ Track,\npages 107–120. (Cited on page 130)\nRebecca Knowles, Marina Sanchez-Torron, and Philipp Koehn.\n2019. A User Study of Neural Interactive Translation Prediction.\nMachine Translation, 33:135–154. (Cited on page 130)\nOlaf Koeneman and Hedde Zeijlstra. 2017. Introducing Syntax.\n(Cited on page 27)\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie,\nMarvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro\nBIBLIOGRAPHY\n218\nYasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne\nDavid, Ian Stavness, Wei Guo, Berton Earnshaw, Imran S. Haque,\nSara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,\nSergey Levine, Chelsea Finn, and Percy Liang. 2021. WILDS: A\nBenchmark of In-the-Wild Distribution Shifts. In Proceedings of\nthe 38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 5637–5664.\n(Cited on\npage 74)\nZahra Kolagar and Alessandra Zarcone. 2024. Aligning Uncertainty:\nLeveraging LLMs to Analyze Uncertainty Transfer In Text Sum-\nmarization. In Proceedings of the 1st Workshop on Uncertainty-\nAware NLP (UncertaiNLP 2024), pages 41–61. (Cited on pages 31\nand 167)\nAndrey Kolmogoroff. 1941. Interpolation und Extrapolation von\nStationären Zufälligen Folgen. Izvestiya Rossiiskoi Akademii\nNauk. Seriya Matematicheskaya, 5(1):3–14. (Cited on page 47)\nBenjamin Kompa, Jasper Snoek, and Andrew L Beam. 2021. Em-\npirical Frequentist Coverage of Deep Learning Uncertainty Quan-\ntification Procedures. Entropy, 23(12):1608. (Cited on pages 39,\n110, and 115)\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao,\nand Chao Zhang. 2020a. Calibrated Language Model Fine-tuning\nfor In- and Out-of-distribution Data. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1326–1340. (Cited on pages 67 and 68)\nLingkai Kong, Jimeng Sun, and Chao Zhang. 2020b. SDE-Net:\nEquipping Deep Neural Networks with Uncertainty Estimates.\nIn Proceedings of the 37th International Conference on Machine\nLearning, ICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research, pages 5405–\n5415. (Cited on pages 56 and 166)\nAnna-Kathrin Kopetzki, Bertrand Charpentier, Daniel Zügner,\nSandhya Giri, and Stephan Günnemann. 2021. Evaluating Ro-\nbustness of Predictive Uncertainty Estimation: Are Dirichlet-\nbased Models Reliable? In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine Learning\nResearch, pages 5707–5718. (Cited on page 162)\nNikita Kotelevskii, Aleksandr Artemenkov, Kirill Fedyanin, Fedor\nNoskov, Alexander Fishkov, Artem Shelmanov, Artem Vazhent-\nBIBLIOGRAPHY\n219\nsev, Aleksandr Petiushko, and Maxim Panov. 2022. Nonparamet-\nric Uncertainty Quantification for Single Deterministic Neural\nNetwork. Advances in Neural Information Processing Systems,\n35:36308–36323. (Cited on page 57)\nLea Krause, Wondimagegnhue Tufa, Selene Báez Santamaría, Angel\nDaza, Urja Khurana, and Piek Vossen. 2023. Confidently Wrong:\nExploring the Calibration and Expression of (Un-)Certainty\nof Large Language Models in a Multilingual Setting. In Pro-\nceedings of the Workshop on Multimodal, Multilingual Natural\nLanguage Generation and Multilingual WebNLG Challenge (MM-\nNLG 2023), pages 1–9. (Cited on page 62)\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van\nEsch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subra-\nmani, Artem Sokolov, Claytone Sikasote, Monang Setyawan,\nSupheakmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara\nRivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pe-\ndro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo\nRubungo, Toan Q. Nguyen, Mathias Müller, André Müller,\nShamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda\nMnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira,\nColin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite,\nMathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile\nDlamini, Nisansa de Silva, Sakine Çabuk Ballı, Stella Bider-\nman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi\nBaljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and\nMofetoluwa Adeyemi. 2022. Quality at a Glance: An Audit of\nWeb-crawled Multilingual Datasets. Transactions of the Associa-\ntion for Computational Linguistics, 10:50–72. (Cited on pages 74\nand 77)\nKalimuthu Krishnamoorthy. 2006. Handbook of Statistical Distri-\nbutions with Applications. (Cited on page 16)\nAgustinus Kristiadi, Matthias Hein, and Philipp Hennig. 2020.\nBeing Bayesian, even just a bit, Fixes Overconfidence in ReLU\nNetworks. In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual\nEvent, volume 119 of Proceedings of Machine Learning Research,\npages 5436–5446. (Cited on page 45)\nDavid Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner,\nAlexandre Lacoste, and Aaron Courville. 2017. Bayesian Hyper-\nnetworks. ArXiv preprint, abs/1710.04759. (Cited on page 44)\nBIBLIOGRAPHY\n220\nJohn K. Kruschke. 2013. Bayesian Estimation Supersedes the t\ntest. Journal of Experimental Psychology: General, 142(2):573.\n(Cited on pages 81 and 95)\nJohn K. Kruschke. 2021. Bayesian Analysis Reporting Guidelines.\nNature human behaviour, 5(10):1282–1291. (Cited on page 81)\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic\nUncertainty: Linguistic Invariances for Uncertainty Estimation\nin Natural Language Generation. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023.\n(Cited on pages 61, 90, 145, 152,\nand 156)\nThomas S. Kuhn. 1970. The Structure of Scientific Revolutions,\nvolume 111. (Cited on page 72)\nMeelis Kull, Miquel Perelló-Nieto, Markus Kängsepp, Telmo\nde Menezes e Silva Filho, Hao Song, and Peter A. Flach. 2019.\nBeyond Temperature Scaling: Obtaining Well-calibrated Multi-\nclass Probabilities with Dirichlet Calibration. In Advances in\nNeural Information Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages 12295–\n12305. (Cited on page 37)\nAnanya Kumar, Percy Liang, and Tengyu Ma. 2019. Verified\nUncertainty Calibration. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 3787–3798. (Cited on\npage 37)\nAviral Kumar and Sunita Sarawagi. 2019. Calibration of Encoder\nDecoder Models for Neural Machine Translation. ArXiv preprint,\nabs/1903.00802. (Cited on page 162)\nShankar Kumar and William Byrne. 2002. Minimum Bayes-risk\nWord Alignments of Bilingual Texts. In Proceedings of the 2002\nConference on Empirical Methods in Natural Language Processing\n(EMNLP 2002), pages 140–147. (Cited on page 138)\nShankar Kumar and William Byrne. 2004. Minimum Bayes-risk\nDecoding for Statistical Machine Translation. In Proceedings of\nthe Human Language Technology Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics:\nHLT-NAACL 2004, pages 169–176. (Cited on page 138)\nBIBLIOGRAPHY\n221\nMorton Kupperman. 1964.\nProbabilities of Hypotheses and\nInformation-statistics in Sampling from Exponential-class Pop-\nulations. Selected Mathematical Papers, 29(2):57. (Cited on\npage 278)\nGleb Kuzmin, Artem Vazhentsev, Artem Shelmanov, Xudong\nHan, Simon Suster, Maxim Panov, Alexander Panchenko, and\nTimothy Baldwin. 2023. Uncertainty Estimation for Debiased\nModels: Does Fairness Hurt Reliability?\nIn Proceedings of\nthe 13th International Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific Chapter of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), pages 744–770. (Cited on page 67)\nSelim Kuzucu, Jiaee Cheong, Hatice Gunes, and Sinan Kalkan.\n2023. Uncertainty-based Fairness Measures. ArXiv preprint,\nabs/2312.11299. (Cited on page 67)\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and\nThomas Dandres. 2019.\nQuantifying the Carbon Emissions\nof Machine Learning. Workshop on Tackling Climate Change\nwith Machine Learning at NeurIPS 2019. (Cited on page 326)\nSalem Lahlou, Moksh Jain, Hadi Nekoei, Victor Butoi, Paul Bertin,\nJarrid Rector-Brooks, Maksym Korablyov, and Yoshua Bengio.\n2023. DEUP: Direct Epistemic Uncertainty Prediction. Transac-\ntions on Machine Learning Research, 2023. (Cited on page 57)\nGeorge Lakoff. 1973. Hedges: A Study in Meaning Criteria and\nthe Logic of Fuzzy Concepts. Journal of Philosophical Logic,\n2(4):458–508. (Cited on page 30)\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.\n2017. Simple and Scalable Predictive Uncertainty Estimation\nUsing Deep Ensembles. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural Information\nProcessing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, pages 6402–6413. (Cited on pages 38, 46, 50, 109, 111,\nand 116)\nAndrew Kyle Lampinen, Stephanie C. Y. Chan, Adam Santoro,\nand Felix Hill. 2021. Publishing Fast and Slow: A Path Toward\nGeneralizability in Psychology and AI. (Cited on page 82)\nPierre-Simon Laplace. 1774. Mémoires de Mathématique et de\nPhysique. (Cited on page 45)\nWassennan Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. (Cited on page 115)\nBIBLIOGRAPHY\n222\nStefan Larson, Anish Mahendran, Joseph J. Peper, Christopher\nClarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld,\nKevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason\nMars. 2019. An Evaluation Dataset for Intent Classification and\nOut-of-scope Prediction. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 1311–1316. (Cited on pages 112\nand 113)\nOlivier Laurent, Adrien Lafage, Enzo Tartaglione, Geoffrey Daniel,\nJean-Marc Martinez, Andrei Bursuc, and Gianni Franchi. 2023.\nPacked Ensembles for Efficient Uncertainty Estimation. In The\nEleventh International Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\n(Cited on page 46)\nBenjamin LeBrun, Alessandro Sordoni, and Timothy J. O’Donnell.\n2022. Evaluating Distributional Distortion in Neural Language\nModeling. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022.\n(Cited on pages 60 and 162)\nNoah Lee, Na Min An, and James Thorne. 2023.\nCan Large\nLanguage Models Capture Dissenting Human Voices? In The\n2023 Conference on Empirical Methods in Natural Language\nProcessing. (Cited on page 167)\nErich Leo Lehmann. 1955. Ordered Families of Distributions. The\nAnnals of Mathematical Statistics, pages 399–419. (Cited on\npage 85)\nJ. A. Leonard, Mark A. Kramer, and L. H. Ungar. 1992.\nA\nNeural Network Architecture that Computes Its own Reliability.\nComputers & chemical engineering, 16(9):819–835. (Cited on\npage 115)\nThomas Hoskyns Leonard. 2014. A Personal History of Bayesian\nStatistics.\nWiley Interdisciplinary Reviews:\nComputational\nStatistics, 6(2):80–115. (Cited on page 19)\nWillem JM Levelt. 1993. Speaking: From Intention to Articulation.\n(Cited on page 30)\nA.-M. Leventi-Peetz and T. Östreich. 2022.\nDeep Learning\nReproducibility and Explainable AI (XAI).\nArXiv preprint,\nabs/2202.11452. (Cited on page 80)\nBIBLIOGRAPHY\n223\nEsther Levin, Naftali Tishby, and Sara A. Solla. 1990. A Statistical\nApproach to Learning and Generalization in Layered Neural\nNetworks. Proceedings of the IEEE, 78(10):1568–1574. (Cited\non page 46)\nDavid D. Lewis and Jason Catlett. 1994. Heterogeneous Uncer-\ntainty Sampling for Supervised Learning. In Machine learning\nproceedings 1994, pages 148–156. (Cited on page 68)\nDavid D. Lewis and William A. Gale. 1994. A Sequential Algorithm\nfor Training Text Classifiers. (Cited on page 68)\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Ques-\ntion and Answer Test-train Overlap in Open-domain Question\nAnswering Datasets. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1000–1008. (Cited on page 152)\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Ab-\nhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chau-\nmond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison,\nMario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Bran-\ndeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry,\nAngelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clé-\nment Delangue, Théo Matussière, Lysandre Debut, Stas Bekman,\nPierric Cistac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets:\nA Community Library for Natural Language Processing. In\nProceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pages\n175–184. (Cited on page 129)\nBolian Li and Ruqi Zhang. 2023. Entropy-MCMC: Sampling from\nflat basins with ease. In NeurIPS 2023 Workshop on Symmetry\nand Geometry in Neural Representations. (Cited on page 42)\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom\nGoldstein. 2018. Visualizing the Loss Landscape of Neural Nets.\nIn Advances in Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing Systems 2018,\nNeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages\n6391–6401. (Cited on page 84)\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh,\nand Ameet Talwalkar. 2017. Hyperband: A Novel Bandit-based\nApproach to Hyperparameter Optimization. The Journal of\nMachine Learning Research, 18(1):6765–6816. (Cited on pages 78\nand 330)\nBIBLIOGRAPHY\n224\nMargaret Y. Li, Alisa Liu, Zhaofeng Wu, and Noah A. Smith.\n2024a.\nA Taxonomy of Ambiguity Types for NLP.\nArXiv\npreprint, abs/2403.14072. (Cited on page 32)\nMinzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F.\nChen, Zhengyuan Liu, and Diyi Yang. 2023a. CoAnnotating:\nUncertainty-guided Work Allocation between Human and Large\nLanguage Models for Data Annotation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10, 2023, pages\n1487–1505. Association for Computational Linguistics. (Cited\non page 69)\nMoxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and\nTat-Seng Chua. 2024b. Think Twice before Assure: Confidence\nEstimation for Large Language Models through Reflection on\nMultiple Answers. ArXiv preprint, abs/2403.09972. (Cited on\npage 61)\nXiang Lisa Li, Urvashi Khandelwal, and Kelvin Guu. 2024c.\nFew-Shot Recalibration of Language Models. ArXiv preprint,\nabs/2403.18286. (Cited on page 59)\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang\nLou, and Weizhu Chen. 2023b. Making Language Models Better\nReasoners with Step-aware Verifier. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5315–5333. (Cited on page 61)\nYingzhen Li and Yarin Gal. 2017. Dropout Inference an Bayesian\nNeural Networks with Alpha-divergences.\nIn Proceedings of\nthe 34th International Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70\nof Proceedings of Machine Learning Research, pages 2052–2061.\n(Cited on page 44)\nKaiqu Liang, Zixu Zhang, and Jaime Fernández Fisac. 2024. Intro-\nspective Planning: Guiding Language-enabled Agents to Refine\ntheir Own Uncertainty. ArXiv preprint, abs/2402.06529. (Cited\non page 164)\nQ Vera Liao and S Shyam Sundar. 2022. Designing for Responsible\nTrust in AI Systems: A Communication Perspective. In 2022\nACM Conference on Fairness, Accountability, and Transparency,\npages 1257–1268. (Cited on page 65)\nMark Liberman. 2015. Replicability vs. Reproducibility — Or Is It\nthe other Way around? https://languagelog.ldc.upenn.edu/\nnll/?p=21956. Accessed: 21.02.2022. (Cited on page 72)\nBIBLIOGRAPHY\n225\nJulian Lienen, Caglar Demir, and Eyke Hullermeier. 2023. Confor-\nmal Credal Self-supervised Learning. In Conformal and Prob-\nabilistic Prediction with Applications, pages 214–233. PMLR.\n(Cited on page 58)\nJulian Lienen and Eyke Hüllermeier. 2021a. Credal Self-supervised\nLearning. In Advances in Neural Information Processing Systems\n34: Annual Conference on Neural Information Processing Sys-\ntems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages\n14370–14382. (Cited on page 58)\nJulian Lienen and Eyke Hüllermeier. 2021b. From Label Smooth-\ning to Label Relaxation. In Thirty-Fifth AAAI Conference on\nArtificial Intelligence, AAAI 2021, Thirty-Third Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2021, The\nEleventh Symposium on Educational Advances in Artificial In-\ntelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages\n8583–8591. (Cited on page 37)\nConstantine Lignos, Nolan Holley, Chester Palen-Michel, and Jonne\nSälevä. 2022. Toward more Meaningful Resources for Lower-\nresourced Languages. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, pages 523–532. (Cited on\npage 113)\nChin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation\nof Summaries. In Text Summarization Branches Out, pages 74–\n81. (Cited on pages 90, 149, and 339)\nJiayu Lin. 2016. On the Dirichlet Distribution. Master’s Report.\n(Cited on pages 278, 279, and 281)\nMingfeng Lin, Henry C. Lucas Jr., and Galit Shmueli. 2013. Re-\nsearch Commentary—Too Big to Fail: Large Samples and the\np-value Problem. Information Systems Research, 24(4):906–917.\n(Cited on pages 81 and 95)\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022a. Teaching\nModels to Express their Uncertainty in Words. Transactions on\nMachine Learning Research (TMLR)., 2022. (Cited on pages 59,\n62, 144, 146, and 151)\nZhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2022b. Conformal\nPrediction Intervals with Temporal Dependence. Transactions\non Machine Learnin Research, 2022. (Cited on page 40)\nZhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating\nwith Confidence: Uncertainty Quantification for Black-box Large\nBIBLIOGRAPHY\n226\nLanguage Models. ArXiv preprint, abs/2305.19187. (Cited on\npages 63 and 156)\nZhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024. Contextu-\nalized Sequence Likelihood: Enhanced Confidence Scores For\nNatural Language Generation. ArXiv preprint, abs/2406.01806.\n(Cited on page 60)\nChen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun,\nXuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie\nJi, et al. 2024. Uncertainty Decomposition and Quantification for\nIn-context Learning of Large Language Models. ArXiv preprint,\nabs/2402.10189. (Cited on page 61)\nBingyuan Liu, Ismail Ben Ayed, Adrian Galdran, and Jose Dolz.\n2022a. The Devil Is in the Margin: Margin-based Label Smooth-\ning for Network Calibration.\nIn IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022, New\nOrleans, LA, USA, June 18-24, 2022, pages 80–88. (Cited on\npage 37)\nChang Liu, Jun Zhu, and Yang Song. 2016. Stochastic Gradient\nGeodesic MCMC Methods. In Advances in Neural Information\nProcessing Systems 29: Annual Conference on Neural Informa-\ntion Processing Systems 2016, December 5-10, 2016, Barcelona,\nSpain, pages 3009–3017. (Cited on page 43)\nHaitao Liu, Yew-Soon Ong, Xiaomo Jiang, and Xiaofang Wang.\n2021. Deep Latent-variable Kernel Learning. IEEE Transactions\non Cybernetics, 52(10):10276–10289. (Cited on page 47)\nJeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen,\nGhassen Jerfel, Zachary Nado, Jasper Snoek, Dustin Tran, and\nBalaji Lakshminarayanan. 2023. A Simple Approach to Improve\nSingle-model Deep Uncertainty via Distance-awareness. Journal\nof Machine Learning Research (JMLR), 24:42:1–42:63. (Cited\non pages 68, 111, 300, 332, 334, and 339)\nJiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, and\nHannaneh Hajishirzi. 2024a. Infini-Gram: Scaling Unbounded\nn-gram Language Models to a Trillion Tokens. ArXiv preprint,\nabs/2401.17377. (Cited on pages 60 and 162)\nLinyu Liu, Yu Pan, Xiaocheng Li, and Guanting Chen. 2024b.\nUncertainty Estimation and Quantification for LLMs: A Simple\nSupervised Approach. ArXiv preprint, abs/2404.15993. (Cited\non pages 57, 164, and 168)\nBIBLIOGRAPHY\n227\nShiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen,\nGhada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang\nWang, and Decebal Constantin Mocanu. 2022b. Deep Ensem-\nbling with no Overhead for either Training or Testing: The\nAll-round Blessings of Dynamic Sparsity. In The Tenth Inter-\nnational Conference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. (Cited on page 46)\nYong Liu and Xin Yao. 1999. Ensemble Learning via Negative\nCorrelation.\nNeural networks, 12(10):1399–1404.\n(Cited on\npage 46)\nZiyin Liu, Zhikang Wang, Paul Pu Liang, Ruslan Salakhutdinov,\nLouis-Philippe Morency, and Masahito Ueda. 2019. Deep Gam-\nblers: Learning to Abstain with Portfolio Theory. In Advances\nin Neural Information Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada, pages\n10622–10632. (Cited on page 57)\nJoseph J. Locascio. 2017. Results Blind Science Publishing. Basic\nand applied social psychology, 39(5):239–246. (Cited on page 81)\nAlexandra Lorson, Chris Cummins, and Hannah Rohde. 2021.\nStrategic Use of (Un-)Certainty Expressions. Frontiers in Com-\nmunication, 6:635156. (Cited on page 30)\nAlexandra Lorson, Hannah Rohde, and Chris Cummins. 2023.\nEpistemicity and Communicative Strategies. Discourse Processes,\n60(8):556–593. (Cited on page 30)\nIlya Loshchilov and Frank Hutter. 2018. Fixing Weight Decay\nRegularization in Adam. (Cited on page 147)\nKadan Lottick, Silvia Susai, Sorelle A. Friedler, and Jonathan P.\nWilson. 2019. Energy Usage Reports: Environmental Awareness\nas Part of Algorithmic Accountability. Workshop on Tackling\nClimate Change with Machine Learning at NeurIPS 2019. (Cited\non page 326)\nChristos Louizos and Max Welling. 2016. Structured and Efficient\nVariational Deep Learning with Matrix Gaussian Posteriors. In\nProceedings of the 33nd International Conference on Machine\nLearning, ICML 2016, New York City, NY, USA, June 19-24,\n2016, volume 48 of JMLR Workshop and Conference Proceedings,\npages 1708–1716. (Cited on page 44)\nBIBLIOGRAPHY\n228\nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei\nWang. 2017. The Expressive Power of Neural Networks: A View\nfrom the Width. In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Process-\ning Systems 2017, December 4-9, 2017, Long Beach, CA, USA,\npages 6231–6239. (Cited on page 41)\nMichal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and\nSanjiv Kumar. 2020. Does Label Smoothing Mitigate Label\nNoise? In Proceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\nvolume 119 of Proceedings of Machine Learning Research, pages\n6448–6458. (Cited on page 37)\nYan Luo, Yongkang Wong, Mohan S. Kankanhalli, and Qi Zhao.\n2021. Learning to Predict Trustworthiness with Steep Slope\nLoss. In Advances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Processing Systems\n2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 21533–\n21544. (Cited on page 57)\nHengyuan Ma, Yang Qi, Li Zhang, Wenlian Lu, and Jianfeng Feng.\n2023. Probabilistic Computation with Emerging Covariance:\nTowards Efficient Uncertainty Quantification. ArXiv preprint,\nabs/2305.19265. (Cited on page 56)\nXingchen Ma and Matthew B. Blaschko. 2021. Meta-Cal: Well-\ncontrolled Post-hoc Calibration by Ranking. In Proceedings of\nthe 38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 7235–7245.\n(Cited on\npage 37)\nYi-An Ma, Tianqi Chen, and Emily B. Fox. 2015. A Complete\nRecipe for Stochastic Gradient MCMC. In Advances in Neural\nInformation Processing Systems 28: Annual Conference on Neu-\nral Information Processing Systems 2015, December 7-12, 2015,\nMontreal, Quebec, Canada, pages 2917–2925. (Cited on page 43)\nDavid J.C. MacKay. 1992a. A Practical Bayesian Framework for\nBackpropagation Networks. Neural computation, 4(3):448–472.\n(Cited on page 41)\nDavid J.C. MacKay. 1992b. Bayesian Interpolation. Neural com-\nputation, 4(3):415–447. (Cited on page 45)\nWesley J. Maddox, Pavel Izmailov, Timur Garipov, Dmitry P.\nVetrov, and Andrew Gordon Wilson. 2019. A Simple Baseline\nBIBLIOGRAPHY\n229\nfor Bayesian Uncertainty in Deep Learning. In Advances in\nNeural Information Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages 13132–\n13143. (Cited on page 46)\nTambiama Madiega. 2021. Artificial Intelligence Act. European\nParliament: European Parliamentary Research Service. (Cited\non page 174)\nAndreas Madsen, Siva Reddy, and Sarath Chandar. 2023. Post-hoc\nInterpretability for Neural NLP: A Survey. ACM Comput. Surv.,\n55(8):155:1–155:42. (Cited on page 4)\nAndrey Malinin, Neil Band, Yarin Gal, Mark J. F. Gales, Alexan-\nder Ganshin, German Chesnokov, Alexey Noskov, Andrey\nPloskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal\nRaina, Vyas Raina, Denis Roginskiy, Mariya Shmatova, Panagi-\notis Tigas, and Boris Yangel. 2021. Shifts: A Dataset of Real\nDistributional Shift Across Multiple Large-Scale Tasks. In Pro-\nceedings of the Neural Information Processing Systems Track on\nDatasets and Benchmarks 1, NeurIPS Datasets and Benchmarks\n2021, December 2021, virtual. (Cited on page 111)\nAndrey Malinin and Mark J. F. Gales. 2018. Predictive Uncertainty\nEstimation via Prior Networks. In Advances in Neural Infor-\nmation Processing Systems 31: Annual Conference on Neural\nInformation Processing Systems 2018, NeurIPS 2018, Decem-\nber 3-8, 2018, Montréal, Canada, pages 7047–7058. (Cited on\npages 49, 52, 53, 54, 55, 280, and 282)\nAndrey Malinin and Mark J. F. Gales. 2019. Reverse KL-Divergence\nTrainingoOf Prior Networks: Improved Uncertainty and Adver-\nsarial Robustness. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 14520–14531. (Cited on page 55)\nAndrey Malinin and Mark J. F. Gales. 2021. Uncertainty Estima-\ntion in Autoregressive Structured Prediction. In 9th International\nConference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. (Cited on pages 60, 112, and 166)\nAndrey Malinin, Bruno Mlodozeniec, and Mark J. F. Gales. 2020.\nEnsemble Distribution Distillation. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. (Cited on pages 46 and 55)\nBIBLIOGRAPHY\n230\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023.\nSelfCheckGPT: Zero-resource black-box hallucination detection\nfor generative large language models.\nIn Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10, 2023, pages\n9004–9017. Association for Computational Linguistics. (Cited\non pages 61 and 67)\nHenry B. Mann and Donald R. Whitney. 1947. On a Test of\nwhether one of two Random Variables Is Stochastically Larger\nthan the other. The annals of mathematical statistics, pages\n50–60. (Cited on page 87)\nChristopher D. Manning. 2015.\nLast Words:\nComputational\nLinguistics and Deep Learning.\nComputational Linguistics,\n41(4):701–707. (Cited on page 82)\nLei Mao. 2019. Introduction to Exponential Family. Accessed April\n2022. (Cited on page 278)\nBenjamin Marie, Atsushi Fujita, and Raphael Rubino. 2021. Sci-\nentific Credibility of Machine Translation Research: A Meta-\nevaluation of 769 Papers. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 7297–7306. (Cited\non pages 71 and 79)\nJuan Maroñas, Roberto Paredes, and Daniel Ramos. 2020. Cali-\nbration of Deep Probabilistic Models with Decoupled Bayesian\nNeural Networks.\nNeurocomputing, 407:194–205.\n(Cited on\npage 38)\nJuan Maroñas, Daniel Ramos, and Roberto Paredes. 2021. On\nCalibration of Mixup Training for Deep Neural Networks. In\nStructural, Syntactic, and Statistical Pattern Recognition: Joint\nIAPR International Workshops, S+ SSPR 2020, Padua, Italy,\nJanuary 21–22, 2021, Proceedings, pages 67–76. Springer. (Cited\non page 38)\nAntonis Maronikolakis, Philipp Dufter, and Hinrich Schütze. 2021.\nWine Is not V I N. On the Compatibility of Tokenizations Across\nLanguages. In Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2382–2399. (Cited on page 163)\nJorge Martinez-Gil. 2023. A Survey On Legal Question-answering\nSystems. Computer Science Review, 48:100552. (Cited on page 2)\nBIBLIOGRAPHY\n231\nPedro Henrique Martins, Zita Marinho, and André F. T. Martins.\n2022. Chunk-based Nearest Neighbor Machine Translation. In\nProceedings of the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4228–4245. (Cited on pages 124\nand 138)\nViera Maslej-Krešňáková, Martin Sarnovsk`y, Peter Butka, and\nKristína Machová. 2020. Comparison of Deep Learning Models\nand Various Text Pre-processing Techniques for the Toxic Com-\nments Classification. Applied Sciences, 10(23):8631. (Cited on\npage 143)\nSergio Matiz and Kenneth E. Barner. 2019. Inductive Conformal\nPredictor for Convolutional Neural Networks: Applications to\nActive Learning for Image Classification. Pattern Recognition,\n90:172–182. (Cited on page 68)\nRoger C. Mayer, James H. Davis, and F. David Schoorman. 1995.\nAn Integrative Model of Organizational Trust.\nAcademy of\nmanagement review, 20(3):709–734. (Cited on page 64)\nWarren S. McCulloch and Walter Pitts. 1943. A Logical Calculus\nof the Ideas Immanent in Nervous Activity. The bulletin of\nmathematical biophysics, 5:115–133. (Cited on page 33)\nSharon Bertsch McGrayne. 2011. The Theory that Would not\nDie: How Bayes’ Rule Cracked the Enigma Code, Hunted down\nRussian Submarines, & Emerged Triumphant from two Centuries\nof Controversy. (Cited on page 19)\nMark F. Medress, Franklin S. Cooper, Jim W. Forgie, C. C. Green,\nDennis H. Klatt, Michael H. O’Malley, Edward P. Neuburg, Allen\nNewell, D. R. Reddy, B. Ritea, et al. 1977. Speech Understanding\nSystems: Report of a Steering Committee. Artificial Intelligence,\n9(3):307–316. (Cited on page 135)\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina\nLerman, and Aram Galstyan. 2021.\nA Survey on Bias and\nFairness in Machine Learning. ACM Computing Surveys (CSUR),\n54(6):1–35. (Cited on page 3)\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell.\n2023. Locally Typical Sampling. Transactions of the Association\nfor Computational Linguistics, 11:102–121. (Cited on pages 123\nand 138)\nChuizheng Meng, Loc Trinh, Nan Xu, James Enouen, and Yan Liu.\n2022a. Interpretability and Fairness Evaluation of Deep Learning\nBIBLIOGRAPHY\n232\nModels on MIMIC-IV Dataset. Scientific Reports, 12(1):7166.\n(Cited on page 3)\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tian-\nwei Zhang, and Jiwei Li. 2022b. Fast Nearest Neighbor Machine\nTranslation. In Findings of the Association for Computational\nLinguistics: ACL 2022, pages 555–565. (Cited on page 124)\nSachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and\nCynthia Rudin. 2020. PULSE: Self-supervised Photo Upsam-\npling via Latent Space Exploration of Generative Models. In\n2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020,\npages 2434–2442. (Cited on page 79)\nJ. L. Mey. 2006. Pragmatics: Overview. Concise Encyclopedia of\nPragmatics, pages 786–797. (Cited on page 30)\nMarco Miani, Frederik Warburg, Pablo Moreno-Muñoz, Nicki\nSkafte, and Søren Hauberg. 2022. Laplacian Autoencoders for\nLearning Stochastic Representations. Advances in Neural Infor-\nmation Processing Systems, 35:21059–21072. (Cited on page 166)\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel,\nManan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y\nLee, Benoît Sagot, et al. 2021. Between Words and Characters:\nA Brief History of Open-vocabulary Modeling and Tokenization\nin NLP. ArXiv preprint, abs/2112.10508. (Cited on page 163)\nSabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau.\n2022. Reducing Conversational Agents’ Overconfidence through\nLinguistic Calibration. Transactions of the Association for Com-\nputational Linguistics, 10:857–872. (Cited on pages 62, 146,\nand 169)\nJeffrey W. Miller. 2011. (ML 7.7.A2) Expectation of a Dirichlet\nRandom Variable. (Cited on page 278)\nTim Miller. 2019. Explanation in Artificial Intelligence: Insights\nfrom the Social Sciences. Artificial intelligence, 267:1–38. (Cited\non page 65)\nRobert William Milne. 1982. Predicting Garden Path Sentences.\nCognitive Science, 6(4):349–373. (Cited on page 29)\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,\nHannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking\nthe Role of Demonstrations: What Makes In-context Learning\nWork?\nIn Proceedings of the 2022 Conference on Empirical\nBIBLIOGRAPHY\n233\nMethods in Natural Language Processing, pages 11048–11064.\n(Cited on page 91)\nMatthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hu-\nbis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic.\n2021. Revisiting the Calibration of Modern Neural Networks.\nIn Advances in Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages 15682–15694.\n(Cited on page 37)\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes,\nLucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deb-\norah Raji, and Timnit Gebru. 2019. Model Cards for Model\nReporting. In Proceedings of the conference on fairness, account-\nability, and transparency, pages 220–229. (Cited on page 79)\nJohn Mitros and Brian Mac Namee. 2019. On the Validity Of\nBayesian Neural Networks for Uncertainty Estimation. In Pro-\nceedings for the 27th AIAI Irish Conference on Artificial In-\ntelligence and Cognitive Science, Galway, Ireland, December\n5-6, 2019, volume 2563 of CEUR Workshop Proceedings, pages\n140–151. (Cited on page 38)\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Sha-\nhaf, and Gabriel Stanovsky. 2024. State of What Art? A Call for\nMulti-prompt LLM Evaluation. ArXiv preprint, abs/2401.00595.\n(Cited on pages 71, 91, 94, and 160)\nShakir Mohamed, Marie-Therese Png, and William Isaac. 2020.\nDecolonial AI: Decolonial Theory as Sociotechnical Foresight in\nArtificial Intelligence. Philosophy & Technology, 33(4):659–684.\n(Cited on pages 71 and 171)\nChristopher Mohri and Tatsunori Hashimoto. 2024. Language\nModels with Conformal Factuality Guarantees. ArXiv preprint,\nabs/2402.10978. (Cited on page 164)\nJose G. Moreno-Torres, Troy Raeder, Rocío Alaíz-Rodríguez,\nNitesh V. Chawla, and Francisco Herrera. 2012. A Unifying View\non Dataset Shift in Classification. Pattern Recognit., 45(1):521–\n530. (Cited on pages 3, 99, 113, and 138)\nThomas Mortier, Viktor Bengs, Stijn Luca, and Willem Waegeman.\n2022. On Calibration of Ensemble-based Credal Predictors. stat,\n1050:20. (Cited on page 58)\nBIBLIOGRAPHY\n234\nMosaicML NLP Team. 2023. Introducing MPT-7B: A New Stan-\ndard for Open-Source, Commercially Usable LLMs. Accessed:\n2023.05.05. (Cited on page 89)\nAzadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Leão, and\nChristian Gagné. 2019. Unsupervised Temperature Scaling: An\nUnsupervised Post-Processing Calibration Method of Deep Net-\nworks. ArXiv preprint, abs/1905.00174. (Cited on page 37)\nSidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao\nWang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael\nCollins, Jilin Chen, et al. 2023. Controlled Decoding from Lan-\nguage Models. In Socially Responsible Language Modelling Re-\nsearch. (Cited on pages 138 and 164)\nJishnu Mukhoti, Puneet K. Dokania, Philip Torr, and Yarin Gal.\n2020a. On Batch Normalisation for Approximate Bayesian Infer-\nence. In Third Symposium on Advances in Approximate Bayesian\nInference. (Cited on page 44)\nJishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S.\nTorr, and Yarin Gal. 2021. Deterministic Neural Networks with\nAppropriate Inductive Biases Capture Epistemic and Aleatoric\nUncertainty. ArXiv preprint, abs/2102.11582. (Cited on pages 57,\n111, 112, and 338)\nJishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart\nGolodetz, Philip H. S. Torr, and Puneet K. Dokania. 2020b. Cal-\nibrating Deep Neural Networks Using Focal Loss. In Advances in\nNeural Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual. (Cited on page 37)\nRafael Müller, Simon Kornblith, and Geoffrey E. Hinton. 2019.\nWhen Does Label Smoothing Help? In Advances in Neural In-\nformation Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pages 4696–4705. (Cited\non page 37)\nMax Müller-Eberstein, Rob van der Goot, and Barbara Plank. 2021.\nGenre as Weak Supervision for Cross-lingual Dependency Parsing.\nIn Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4786–4802. (Cited on\npage 113)\nMahdi Pakdaman Naeini,\nGregory F. Cooper,\nand Milos\nHauskrecht. 2015. Obtaining Well-calibrated Probabilities Us-\ning Bayesian Binning.\nIn Proceedings of the Twenty-Ninth\nBIBLIOGRAPHY\n235\nAAAI Conference on Artificial Intelligence, January 25-30, 2015,\nAustin, Texas, USA, pages 2901–2907. (Cited on pages 36, 114,\n146, and 151)\nVaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur.\n2021. Understanding the Failure Modes of Out-of-distribution\nGeneralization. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. (Cited on page 77)\nEric T. Nalisnick, José Miguel Hernández-Lobato, and Padhraic\nSmyth. 2019a. Dropout as a Structured Shrinkage Prior. In\nProceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California,\nUSA, volume 97 of Proceedings of Machine Learning Research,\npages 4712–4722. (Cited on page 44)\nEric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Görür,\nand Balaji Lakshminarayanan. 2019b. Do Deep Generative Mod-\nels Know What They Don’t Know? In 7th International Con-\nference on Learning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. (Cited on pages 98 and 113)\nJay Nandy, Wynne Hsu, and Mong-Li Lee. 2020. Towards Max-\nimizing the Representation Gap between In-Domain & Out-\nOf-Distribution Examples. In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual. (Cited on page 55)\nSharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault\nFevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam\nShazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake\nMarcus, Adam Roberts, and Colin Raffel. 2021. Do Transformer\nModifications Transfer Across Implementations and Applica-\ntions? In Proceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 5758–5773. (Cited on\npage 71)\nRoberto Navigli. 2009. Word Sense Disambiguation: A Survey.\nACM computing surveys (CSUR), 41(2):1–69. (Cited on page 26)\nRadford M. Neal. 1995. Bayesian Learning for Neural Networks,\nvolume 118. (Cited on pages 41, 42, and 47)\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994. On Structur-\ning Probabilistic Dependences in Stochastic Language Modelling.\nComputer Speech & Language, 8(1):1–38. (Cited on page 297)\nBIBLIOGRAPHY\n236\nJerzy Neyman. 1937. Outline of a Theory of Statistical Estimation\nBased on the Classical Theory of Probability. Philosophical\nTransactions of the Royal Society of London. Series A, Math-\nematical and Physical Sciences, 236(767):333–380. (Cited on\npage 15)\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What Can We\nLearn from Collective Human Opinions on Natural Language\nInference Data?\nIn Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP),\npages 9131–9143. (Cited on page 63)\nAlexander Nikitin, Jannik Kossen, Yarin Gal, and Pekka Marttinen.\n2024. Kernel Language Entropy: Fine-grained Uncertainty Quan-\ntification for LLMs from Semantic Similarities. ArXiv preprint,\nabs/2405.20003. (Cited on page 62)\nJeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen\nJerfel, and Dustin Tran. 2019. Measuring Calibration in Deep\nLearning. In IEEE Conference on Computer Vision and Pattern\nRecognition Workshops, CVPR Workshops 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 38–41. (Cited on page 37)\nJongyoun Noh, Hyekang Park, Junghyup Lee, and Bumsub Ham.\n2023. RankMixup: Ranking-based Mixup Training for Network\nCalibration. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1358–1368. (Cited on\npage 38)\nEric W. Noreen. 1989. Computer Intensive Methods for Hypothesis\nTesting: An Introduction. Wiley, New York, 19:21. (Cited on\npage 87)\nCurtis Northcutt, Lu Jiang, and Isaac Chuang. 2021. Confident\nLearning: Estimating Uncertainty in Dataset Labels.\nJour-\nnal of Artificial Intelligence Research, 70:1373–1411. (Cited on\npage 166)\nSebastian W. Ober, Carl E. Rasmussen, and Mark van der Wilk.\n2021. The Promises and Pitfalls of Deep Kernel Learning. In\nProceedings of the Thirty-Seventh Conference on Uncertainty in\nArtificial Intelligence, UAI 2021, Virtual Event, 27-30 July 2021,\nvolume 161 of Proceedings of Machine Learning Research, pages\n1206–1216. (Cited on page 47)\nCharles Kay Ogden and Ivor Armstrong Richards. 1923.\nThe\nMeaning of Meaning: A Study of the Influence of Thought and\nof the Science of Symbolism. (Cited on pages 29 and 30)\nBIBLIOGRAPHY\n237\nByung-Doh Oh and William Schuler. 2023. Why Does Surprisal\nfrom Larger Transformer-based Language Models Provide a\nPoorer Fit to Human Reading Times?\nTransactions of the\nAssociation for Computational Linguistics, 11:336–350. (Cited\non page 29)\nByung-Doh Oh, Shisen Yue, and William Schuler. 2024. Frequency\nExplains the Inverse Correlation of Large Language Models’ Size,\nTraining Data Amount, and Surprisal’s Fit to Reading Times.\nIn Proceedings of the 18th Conference of the European Chapter\nof the Association for Computational Linguistics, EACL 2024 -\nVolume 1: Long Papers, St. Julian’s, Malta, March 17-22, 2024,\npages 2644–2663. Association for Computational Linguistics.\n(Cited on page 29)\nRoberto I. Oliveira, Paulo Orenstein, Thiago Ramos, and João Vi-\ntor Romano. 2022. Split Conformal Prediction for Dependent\nData. ArXiv preprint, abs/2203.15885. (Cited on page 40)\nEmre Onal, Klemens Flöge, Emma Caldwell, Arsen Sheverdin,\nand Vincent Fortuin. Gaussian Stochastic Weight Averaging\nfor Bayesian Low-rank Adaptation of Large Language Mod-\nels. In Sixth Symposium on Advances in Approximate Bayesian\nInference-Non Archival Track. (Cited on page 61)\nOpenAI. 2022. Introducing ChatGPT. (Cited on pages 63 and 149)\nOpenAI. 2023. GPT-4 Technical Report. (Cited on pages 123\nand 138)\nManfred Opper and Cédric Archambeau. 2009.\nThe Varia-\ntional Gaussian Approximation Revisited. Neural computation,\n21(3):786–792. (Cited on page 43)\nLuis A. Ortega, Simón Rodríguez Santana, and Daniel Hernández-\nLobato. 2023. Variational Linearized Laplace Approximation\nfor Bayesian Deep Learning. ArXiv preprint, abs/2302.12565.\n(Cited on page 45)\nMyle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato.\n2018. Analyzing Uncertainty in Neural Machine Translation. In\nProceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,\nJuly 10-15, 2018, volume 80 of Proceedings of Machine Learning\nResearch, pages 3953–3962. (Cited on pages 4 and 60)\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll\nWainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal,\nBIBLIOGRAPHY\n238\nKatarina Slama, Alex Ray, et al. 2022. Training Language Models\nto Follow Instructions with Human Feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\n(Cited on\npage 63)\nYikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen\nKan, and William Yang Wang. 2023. On the Risk of Misinfor-\nmation Pollution with Large Language Models. ArXiv preprint,\nabs/2305.13661. (Cited on page 123)\nHarris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex\nGammerman. 2002. Inductive Confidence Machines for Regres-\nsion. In Machine Learning: ECML 2002: 13th European Confer-\nence on Machine Learning Helsinki, Finland, August 19–23, 2002\nProceedings 13, pages 345–356. Springer. (Cited on pages 12, 39,\nand 124)\nTheodore Papamarkou, Maria Skoularidou, Konstantina Palla,\nLaurence Aitchison, Julyan Arbel, David Dunson, Maurizio\nFilippone, Vincent Fortuin, Philipp Hennig, Aliaksandr Hubin,\net al. 2024. Position Paper: Bayesian Deep Learning in the Age\nof Large-scale AI. ArXiv preprint, abs/2402.00809. (Cited on\npage 61)\nNicolas Papernot and Patrick McDaniel. 2018. Deep k-Nearest\nNeighbors: Towards Confident, Interpretable and Robust Deep\nLearning. ArXiv preprint, abs/1803.04765. (Cited on page 57)\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.\n2002. BLEU: A Method for Automatic Evaluation of Machine\nTranslation. In Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, pages 311–318. (Cited\non pages 135 and 337)\nHyekang Park, Jongyoun Noh, Youngmin Oh, Donghyeon Baek,\nand Bumsub Ham. 2023.\nACLS: Adaptive and Conditional\nLabel Smoothing for Network Calibration. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 3936–3945. (Cited on page 37)\nSeo Yeon Park and Cornelia Caragea. 2022. On the Calibration\nof Pre-trained Language Models Using Mixup Guided by Area\nunder the Margin and Saliency.\nIn Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5364–5374. (Cited on page 59)\nYounghyun Park, Wonjeong Choi, Soyeong Kim, Dong-Jun Han,\nand Jaekyun Moon. 2022. Active Learning For Object Detection\nBIBLIOGRAPHY\n239\nwith Evidential Deep Learning and Hierarchical Uncertainty Ag-\ngregation. In The Eleventh International Conference on Learning\nRepresentations. (Cited on page 69)\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\nBradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Na-\ntalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf,\nEdward Yang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\nand Soumith Chintala. 2019. PyTorch: An Imperative Style,\nHigh-performance Deep Learning Library. In Advances in Neu-\nral Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages 8024–8035.\n(Cited on page 325)\nKanil Patel, William Beluch, Dan Zhang, Michael Pfeiffer, and\nBin Yang. 2021. On-manifold Adversarial Data Augmentation\nImproves Uncertainty Calibration. In 2020 25th International\nConference on Pattern Recognition (ICPR), pages 8029–8036.\nIEEE. (Cited on page 38)\nSilviu Paun, Ron Artstein, and Massimo Poesio. 2022. Statistical\nMethods for Annotation Analysis. Synthesis Lectures on Human\nLanguage Technologies, 15(1):1–217. (Cited on page 75)\nNick Pawlowski, Andrew Brock, Matthew CH Lee, Martin Rajchl,\nand Ben Glocker. 2017. Implicit Weight Uncertainty in Neural\nNetworks. ArXiv preprint, abs/1711.01297. (Cited on page 44)\nTim Pearce, Felix Leibfried, and Alexandra Brintrup. 2020. Un-\ncertainty in Neural Networks: Approximately Bayesian Ensem-\nbling. In The 23rd International Conference on Artificial Intelli-\ngence and Statistics, AISTATS 2020, 26-28 August 2020, Online\n[Palermo, Sicily, Italy], volume 108 of Proceedings of Machine\nLearning Research, pages 234–244. (Cited on pages 46 and 109)\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent\nMichel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter\nPrettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-\nLearn: Machine Learning in Python. the Journal of machine\nLearning research, 12:2825–2830.\n(Cited on pages 108, 325,\nand 329)\nRoger D. Peng. 2011. Reproducible Research in Computational\nScience. Science, 334(6060):1226–1227. (Cited on page 72)\nBIBLIOGRAPHY\n240\nÁlvaro Peris, Miguel Domingo, and Francisco Casacuberta. 2017.\nInteractive Neural Machine Translation. Computer Speech &\nLanguage, 45:201–220. (Cited on page 130)\nDana Pessach and Erez Shmueli. 2023. Algorithmic Fairness. In\nMachine Learning for Data Science Handbook: Data Mining\nand Knowledge Discovery Handbook, pages 867–886. (Cited on\npage 67)\nJonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder.\n2021.\nUNKs Everywhere: Adapting Multilingual Language\nModels to New Scripts. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, pages\n10186–10203. (Cited on page 163)\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thick-\nstun, Sean Welleck, Yejin Choi, and Zaïd Harchaoui. 2021.\nMAUVE: Measuring the Gap between Neural Text and Human\nText Using Divergence Frontiers. In Advances in Neural Infor-\nmation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, Decem-\nber 6-14, 2021, virtual, pages 4816–4828. (Cited on pages 135\nand 338)\nSilviu Pitis, Michael R Zhang, Andrew Wang, and Jimmy Ba. 2023.\nBoosted Prompt EnsemblesfFor Large Language Models. ArXiv\npreprint, abs/2304.05970. (Cited on page 61)\nBarbara Plank. 2016. What to do About Non-standard (or Non-\ncanonical) Language in NLP. In Proceedings of the 13th Confer-\nence on Natural Language Processing, KONVENS 2016, Bochum,\nGermany, September 19-21, 2016, volume 16 of Bochumer Lin-\nguistische Arbeitsberichte. (Cited on page 6)\nBarbara Plank. 2022. The “Problem” of Human Label Variation:\nOn Ground Truth in Data, Modeling and Evaluation. In Proceed-\nings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10671–10682. (Cited on pages 63,\n75, 159, and 165)\nBarbara Plank, Kristian Nørgaard Jensen, and Rob van der Goot.\n2020. DaN+: Danish Nested Named Entities and Lexical Normal-\nization. In Proceedings of the 28th International Conference on\nComputational Linguistics, pages 6649–6662. (Cited on pages 112\nand 113)\nJohn Platt et al. 1999. Probabilistic Outputs for Support Vector\nMachines and Comparisons to Regularized Likelihood Methods.\nBIBLIOGRAPHY\n241\nAdvances in large margin classifiers, 10(3):61–74. (Cited on\npages 37 and 151)\nBenjamin Plaut, Khanh Nguyen, and Tu Trinh. 2024. Softmax\nProbabilities (Mostly) Predict Large Language Model Correct-\nness on Multiple-choice Q&A. ArXiv preprint, abs/2402.13213.\n(Cited on page 59)\nMaja Popović. 2017. ChrF++: Words Helping Character n-grams.\nIn Proceedings of the Second Conference on Machine Translation,\npages 612–618. (Cited on page 135)\nKarl Popper. 1934. Logik der Forschung. (Cited on page 72)\nMatt Post. 2018. A Call for Clarity in Reporting BLEU Scores.\nIn Proceedings of the Third Conference on Machine Translation:\nResearch Papers, pages 186–191. (Cited on page 78)\nJanis Postels, Mattia Segù, Tao Sun, Luca Daniel Sieber, Luc Van\nGool, Fisher Yu, and Federico Tombari. 2022. On the Practi-\ncality of Deterministic Epistemic Uncertainty. In International\nConference on Machine Learning, ICML 2022, 17-23 July 2022,\nBaltimore, Maryland, USA, volume 162 of Proceedings of Ma-\nchine Learning Research, pages 17870–17909. (Cited on page 57)\nVinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark\nDiaz. 2021. On Releasing Annotator-level Labels and Informa-\ntion in Datasets. In Proceedings of the Joint 15th Linguistic\nAnnotation Workshop (LAW) and 3rd Designing Meaning Repre-\nsentations (DMR) Workshop, pages 133–138. (Cited on page 75)\nEllen F. Prince, Joel Frader, Charles Bosk, et al. 1982. On Hedging\nin Physician-Physician Discourse. Linguistics and the Professions,\n8(1):83–97. (Cited on page 30)\nJames Pustejovsky. 1991. The Generative Lexicon. Computational\nLinguistics, 17(4):409–441. (Cited on pages 26 and 27)\nJames Pustejovsky. 2017. The Semantics of Lexical Underspeci-\nfication. Folia linguistica, 51(s1000):1–25. (Cited on pages 26\nand 27)\nJames Pustejovsky and Amber Stubbs. 2012. Natural Language\nAnnotation for Machine Learning: A Guide to Corpus-building\nfor Applications. (Cited on page 75)\nSampo Pyysalo, Jenna Kanerva, Anna Missilä, Veronika Laippala,\nand Filip Ginter. 2015. Universal Dependencies For Finnish. In\nBIBLIOGRAPHY\n242\nProceedings of the 20th Nordic Conference of Computational Lin-\nguistics (NoDaLiDa 2015), pages 163–172. (Cited on pages 112\nand 113)\nLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022.\nCold Decoding: Energy-based Constrained Text Generation with\nLangevin Dynamics. Advances in Neural Information Processing\nSystems, 35:9538–9551. (Cited on pages 138 and 164)\nXin Qiu and Risto Miikkulainen. 2022. Detecting Misclassification\nErrors in Neural Networks with a Gaussian Process Model. In\nThirty-Sixth AAAI Conference on Artificial Intelligence, AAAI\n2022, Thirty-Fourth Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2022, The Twelveth Symposium\non Educational Advances in Artificial Intelligence, EAAI 2022\nVirtual Event, February 22 - March 1, 2022, pages 8017–8027.\n(Cited on page 57)\nEric Qu, Xufang Luo, and Dongsheng Li. 2022. Data Continuity\nMatters: Improving Sequence Modeling with Lipschitz Regu-\nlarizer. In The Eleventh International Conference on Learning\nRepresentations. (Cited on page 6)\nVictor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho\nSohn, Tommi S. Jaakkola, and Regina Barzilay. 2023. Conformal\nlanguage modeling. In The Twelfth International Conference on\nLearning Representations. (Cited on pages 163, 164, and 165)\nMaurice H. Quenouille. 1949. Approximate Tests of Correlation\nIn Time-series. In Mathematical Proceedings of the Cambridge\nPhilosophical Society, volume 45, pages 483–484. Cambridge\nUniversity Press. (Cited on page 17)\nChristopher B. Quirk. 2004. Training a Sentence-level Machine\nTranslation Confidence Measure. In Proceedings of the Fourth\nInternational Conference on Language Resources and Evaluation\n(LREC’04). (Cited on page 143)\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. 2019.\nLanguage Models Are\nUnsupervised Multitask Learners. OpenAI blog, 1(8):9. (Cited\non pages 63, 92, 135, and 173)\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.\nLiu. 2020. Exploring the Limits of Transfer Learning with a\nUnified Text-to-text Transformer. Journal of Machine Learning\nResesarch (JMLR), 21:140:1–140:67. (Cited on page 62)\nBIBLIOGRAPHY\n243\nRahul Rahaman and Alexandre H. Thiéry. 2021.\nUncertainty\nQuantification and Deep Ensembles. In Advances in Neural In-\nformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 20063–20075. (Cited on page 38)\nAbhinav Ramesh Kashyap, Devamanyu Hazarika, Min-Yen Kan,\nand Roger Zimmermann. 2021. Domain Divergences: A Survey\nand Empirical Analysis. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, pages\n1830–1849. (Cited on page 74)\nSebastian Raschka. 2018.\nModel Evaluation, Model Selection,\nand Algorithm Selection in Machine Learning. ArXiv preprint,\nabs/1811.12808. (Cited on page 83)\nShauli Ravfogel, Yoav Goldberg, and Jacob Goldberger. 2023.\nConformal Nucleus Sampling. In Findings of the Association\nfor Computational Linguistics: ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pages 27–34. Association for Computational\nLinguistics. (Cited on pages 59, 127, 129, 130, 133, 135, 136,\n163, and 165)\nAmy Rechkemmer and Ming Yin. 2022. When Confidence Meets\nAccuracy: Exploring the Effects of Multiple Performance Indi-\ncators on Trust in Machine Learning Models. In Proceedings of\nthe 2022 chi conference on human factors in computing systems,\npages 1–14. (Cited on page 65)\nSiva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQa:\nA Conversational Question Answering Challenge. Transactions of\nthe Association for Computational Linguistics, 7:249–266. (Cited\non pages 141 and 148)\nRicardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva,\nAna C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur,\nand André F. T. Martins. 2022.\nCOMET-22: Unbabel-IST\n2022 Submission for the Metrics Shared-task. In Proceedings of\nthe Seventh Conference on Machine Translation (WMT), pages\n578–585. (Cited on page 135)\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.\nCOMET: A Neural Framework for MT Evaluation. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2685–2702. (Cited on\npages 135 and 337)\nBIBLIOGRAPHY\n244\nNils Reimers and Iryna Gurevych. 2018. Why Comparing Single\nPerformance Scores Does not Allow to Draw Conclusions about\nMachine Learning Approaches. ArXiv preprint, abs/1803.09578.\n(Cited on page 95)\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence\nEmbeddings Using Siamese BERT-networks. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages 3982–3992.\n(Cited on pages 147 and 149)\nAllen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh,\nStephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei\nXia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and\nAnirudha Majumdar. 2023. Robots that ask for help: Uncertainty\nalignment for large language model planners. In Conference on\nRobot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA,\nUSA, volume 229 of Proceedings of Machine Learning Research,\npages 661–682. PMLR. (Cited on pages 59 and 164)\nJie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad\nSaleh, Balaji Lakshminarayanan, and Peter J. Liu. 2022. Out-of-\ndistribution Detection and Selective Generation for Conditional\nLanguage Models. In The Eleventh International Conference on\nLearning Representations. (Cited on pages 162 and 164)\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li,\nBrij B Gupta, Xiaojiang Chen, and Xin Wang. 2021. A Survey\nof Deep Active Learning. ACM computing surveys (CSUR),\n54(9):1–40. (Cited on page 68)\nValerie F. Reyna and Charles J. Brainerd. 2008. Numeracy, Ra-\ntio Bias, and Denominator Neglect in Judgments of Risk and\nProbability. Learning and individual differences, 18(1):89–107.\n(Cited on page 66)\nDanilo Jimenez Rezende and Shakir Mohamed. 2015. Variational\nInference with Normalizing Flows. In Proceedings of the 32nd\nInternational Conference on Machine Learning, ICML 2015,\nLille, France, 6-11 July 2015, volume 37 of JMLR Workshop and\nConference Proceedings, pages 1530–1538. (Cited on page 56)\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.\n2014. Stochastic Backpropagation and Approximate Inference in\nDeep Generative Models. In Proceedings of the 31th International\nConference on Machine Learning, ICML 2014, Beijing, China,\nBIBLIOGRAPHY\n245\n21-26 June 2014, volume 32 of JMLR Workshop and Conference\nProceedings, pages 1278–1286. (Cited on pages 43 and 166)\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer\nSingh. 2020.\nBeyond Accuracy: Behavioral Testing of NLP\nModels with CheckList.\nIn Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages\n4902–4912. (Cited on page 76)\nStefan Riezler and Michael Hagmann. 2021. Validity, Reliability,\nand Significance. (Cited on page 83)\nHippolyt Ritter, Aleksandar Botev, and David Barber. 2018a. A\nScalable Laplace Approximation for Neural Networks. In 6th In-\nternational Conference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. (Cited on page 45)\nHippolyt Ritter, Aleksandar Botev, and David Barber. 2018b. On-\nline Structured Laplace Approximations for Overcoming Catas-\ntrophic Forgetting. In Advances in Neural Information Processing\nSystems 31: Annual Conference on Neural Information Process-\ning Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,\nCanada, pages 3742–3752. (Cited on page 45)\nChristian P. Robert, George Casella, and George Casella. 1999.\nMonte Carlo Statistical Methods, volume 2. (Cited on page 42)\nRebecca Roelofs, Nicholas Cain, Jonathon Shlens, and Michael C.\nMozer. 2022. Mitigating Bias in Calibration Error Estimation. In\nInternational Conference on Artificial Intelligence and Statistics,\nAISTATS 2022, 28-30 March 2022, Virtual Event, volume 151\nof Proceedings of Machine Learning Research, pages 4036–4054.\n(Cited on page 37)\nAnna Rogers and Isabelle Augenstein. 2021.\nHow to Review\nfor ACL Rolling Review?\nhttps://aclrollingreview.org/\nreviewertutorial. Accessed: 2022.02.21. (Cited on page 82)\nAlex Rogozhnikov. 2022. Einops: Clear and Reliable Tensor Manip-\nulations with Einstein-like Notation. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual\nEvent, April 25-29, 2022. (Cited on page 325)\nYaniv Romano, Matteo Sesia, and Emmanuel J. Candès. 2020.\nClassification with valid and Adaptive Coverage. In Advances in\nNeural Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual. (Cited on page 128)\nBIBLIOGRAPHY\n246\nFrank Rosenblatt. 1958. The Perceptron: A Probabilistic Model for\nInformation Storage and Organization in the Brain. Psychological\nreview, 65(6):386. (Cited on page 33)\nFrank Rosenblatt et al. 1962. Principles of Neurodynamics: Percep-\ntrons and the Theory of Brain Mechanisms, volume 55. (Cited\non page 33)\nVlada Rozova, Katrina Witt, Jo Robinson, Yan Li, and Karin\nVerspoor. 2022. Detection of Self-harm And Suicidal Ideation\nin Emergency Department Triage Notes. Journal of the Ameri-\ncan Medical Informatics Association, 29(3):472–480. (Cited on\npage 2)\nVictoria L. Rubin. 2006. Identifying Certainty in Texts. Unpublished\nDoctoral Thesis, Syracuse University, Syracuse, NY. (Cited on\npage 30)\nSebastian Ruder. 2020. Why You Should Do NLP Beyond English.\nhttp://ruder.io/nlp-beyond-english. (Cited on page 6)\nDavid Ruhe, Giovanni Cina, Michele Tonutti, Daan de Bruin,\nand Paul Elbers. 2019. Bayesian Modelling in Practice: Using\nUncertainty to Improve Trustworthiness in Medical Applications.\nArXiv preprint, abs/1906.08619. (Cited on page 111)\nPhillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and\nIryna Gurevych. 2021.\nHow Good Is Your Tokenizer?\nOn\nthe Monolingual Performance of Multilingual Language Models.\nIn Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long\nPapers), pages 3118–3135. (Cited on pages 75 and 163)\nErfan Sadeqi Azer, Daniel Khashabi, Ashish Sabharwal, and Dan\nRoth. 2020. Not all Claims Are Created Equal: Choosing the\nRight Statistical Approach to Assess Hypotheses. In Proceedings\nof the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 5715–5725. (Cited on pages 80, 83, and 95)\nYusuf Sale, Viktor Bengs, Michele Caprio, and Eyke Hüllermeier.\n2023a. Second-order Uncertainty Quantification: A Distance-\nbased Approach. ArXiv preprint, abs/2312.00995. (Cited on\npage 168)\nYusuf Sale, Michele Caprio, and Eyke Hüllermeier. 2023b.\nIs\nthe Volume of a Credal Set a Good Measure for Epistemic\nUncertainty?\nIn Uncertainty in Artificial Intelligence, pages\n1795–1804. PMLR. (Cited on page 58)\nBIBLIOGRAPHY\n247\nYusuf Sale, Paul Hofman, Lisa Wimmer, Eyke Hüllermeier, and\nThomas Nagler. 2024.\nSecond-order Uncertainty Quantifica-\ntion: Variance-based Measures. ArXiv preprint, abs/2401.00276.\n(Cited on page 168)\nMaarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou,\nYejin Choi, and Noah A. Smith. 2022. Annotators with Attitudes:\nHow Annotator Beliefs and Identities Bias Toxic Language De-\ntection. In Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, pages 5884–5906. (Cited\non page 74)\nLeonard J. Savage. 1971. Elicitation of Personal Probabilities and\nExpectations. Journal of the American Statistical Association,\n66(336):783–801. (Cited on page 37)\nWalter J. Savitch, Emmon Bach, W. E. Marsh, and Gila Safran-\nNaveh. 2012.\nThe Formal Complexity of Natural Language,\nvolume 33. (Cited on page 28)\nAndrew M. Saxe, Yamini Bansal, Joel Dapello, Madhu Advani,\nArtemy Kolchinsky, Brendan D. Tracey, and David D. Cox.\n2018. On the Information Bottleneck Theory of Deep Learning.\nIn 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. (Cited on page 161)\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,\nSuzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha\nLuccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom:\nA 176B-Parameter Open-access Multilingual Language Model.\nArXiv preprint, abs/2211.05100. (Cited on page 123)\nTobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001.\nActive Hidden Markov Models for Information Extraction. In\nInternational symposium on intelligent data analysis, pages 309–\n318. Springer. (Cited on page 68)\nDavid Schlangen. 2021. Targeting the Benchmark: On Methodology\nin Current Natural Language Processing Research. In Proceed-\nings of the 59th Annual Meeting of the Association for Computa-\ntional Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 2: Short Papers), pages\n670–674. (Cited on pages 73 and 77)\nVictor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam\nConell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle\nBIBLIOGRAPHY\n248\nFriedler, and Sasha Luccioni. 2021. Codecarbon: Estimate and\nTrack Carbon Emissions from Machine Learning Computing.\n(Cited on page 326)\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara\nBahri, Vinh Tran, Yi Tay, and Donald Metzler. 2022. Confident\nAdaptive Language Modeling. Advances in Neural Information\nProcessing Systems, 35:17456–17472. (Cited on pages 4, 59, 68,\n164, and 165)\nTal Schuster, Adam Fisch, Tommi Jaakkola, and Regina Barzilay.\n2021. Consistent Accelerated Inference via Confident Adaptive\nTransformers. In Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages 4962–4979.\n(Cited on page 4)\nCarson T. Schütze. 1995. PP Attachmenta And Argumenthood.\nMIT Working Papers in Linguistics, 26(95):151.\n(Cited on\npage 28)\nHinrich Schütze. 1997. Ambiguity Resolution in Language Learning.\nCSLI Lecture Notes, 71. (Cited on page 26)\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni.\n2020.\nGreen AI.\nCommun. ACM, 63(12):54–63.\n(Cited on\npage 71)\nPola Schwöbel, Martin Jørgensen, Sebastian W. Ober, and Mark\nvan der Wilk. 2022. Last-layer Marginal Likelihood for Invariance\nLearning. In International Conference on Artificial Intelligence\nand Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event,\nvolume 151 of Proceedings of Machine Learning Research, pages\n3542–3555. (Cited on page 47)\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023.\nQuantifying Language Models’ Sensitivity to Spurious Features\nin Prompt Design or: How I learned to Start Worrying about\nPrompt Formatting. In The Twelfth International Conference\non Learning Representations. (Cited on pages 91 and 160)\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and\nChristopher D. Manning. 2019. Do Massively Pretrained Lan-\nguage Models Make Better Storytellers? In Proceedings of the\n23rd Conference on Computational Natural Language Learning\n(CoNLL), pages 843–861. (Cited on page 123)\nFlorian Seligmann, Philipp Becker, Michael Volpp, and Gerhard\nNeumann. 2024. Beyond Deep Ensembles: A Large-scale Evalu-\nBIBLIOGRAPHY\n249\nation of Bayesian Deep Learning under Distribution Shift. Ad-\nvances in Neural Information Processing Systems, 36. (Cited on\npage 38)\nMurat Sensoy, Lance M. Kaplan, Federico Cerutti, and Maryam\nSaleki. 2020. Uncertainty-aware Deep Classifiers Using Genera-\ntive Models. In The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innovative Appli-\ncations of Artificial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances in Artificial\nIntelligence, EAAI 2020, New York, NY, USA, February 7-12,\n2020, pages 5620–5627. (Cited on pages 52 and 56)\nMurat Sensoy, Lance M. Kaplan, and Melih Kandemir. 2018. Ev-\nidential Deep Learning to Quantify Classification Uncertainty.\nIn Advances in Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing Systems 2018,\nNeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages\n3183–3193. (Cited on pages 49, 50, 52, 54, 55, 56, 112, 281,\nand 329)\nBurr Settles. 2009.\nActive Learning Literature Survey.\nCom-\nputer Sciences Technical Report 1648, University of Wisconsin–\nMadison. (Cited on page 68)\nJaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Mar-\nius Hobbhahn, and Pablo Villalobos. 2022. Compute Trends\nAcross Three Eras of Machine Learning. In 2022 International\nJoint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.\n(Cited on pages 7 and 82)\nUri Shaham and Omer Levy. 2022. What Do You Get when You\nCross Beam Search with Nucleus Sampling? In Proceedings of\nthe Third Workshop on Insights from Negative Results in NLP,\npages 38–45. (Cited on page 136)\nYilin Shen, Wenhu Chen, and Hongxia Jin. 2020. Modeling Token-\nlevel Uncertainty to Learn Unknown Concepts in SLU via Cal-\nibrated Dirichlet Prior RNN. ArXiv preprint, abs/2010.08101.\n(Cited on pages 56 and 59)\nHidetoshi Shimodaira. 2000. Improving Predictive Inference under\nCovariate Shift by Weighting the Log-likelihood Function. Jour-\nnal of statistical planning and inference, 90(2):227–244. (Cited\non pages 3 and 99)\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas\nPapernot, and Ross Anderson. 2023. Model Dementia: Generated\nBIBLIOGRAPHY\n250\nData Makes Models Forget. arXiv e-prints, pages arXiv–2305.\n(Cited on page 172)\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jian-\nfeng Wang, Jordan L. Boyd-Graber, and Lijuan Wang. 2023.\nPrompting GPT-3 to be reliable. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net. (Cited on page 59)\nChenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-Graber. 2022.\nRe-examining Calibration: The Case of Question Answering.\nIn Findings of the Association for Computational Linguistics:\nEMNLP 2022, pages 2814–2829. (Cited on page 59)\nAnthony Sicilia, Hyunwoo Kim, Khyathi Raghavi Chandu, Malihe\nAlikhani, and Jack Hessel. 2024. Deal, or no Deal (or Who\nKnows)? Forecasting Uncertainty in Conversations Using Large\nLanguage Models. ArXiv preprint, abs/2402.03284. (Cited on\npages 60 and 166)\nHerbert A. Simon. 1995. Artificial Intelligence: An Empirical\nScience. Artificial Intelligence, 77(1):95–127. (Cited on page 72)\nAniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam\nDhakal, and Chandra Dhakal. 2024a. Do Large Language Models\nShow Human-like Biases? Exploring Confidence—competence\nGap in AI. Information, 15(2):92. (Cited on page 62)\nAshudeep Singh, David Kempe, and Thorsten Joachims. 2021.\nFairness in Ranking under Uncertainty. In Advances in Neu-\nral Information Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 11896–11908. (Cited on\npage 67)\nJasdeep Singh, Bryan McCann, Richard Socher, and Caiming Xiong.\n2019. BERT is not an Interlingua and the Bias of Tokenization. In\nProceedings of the 2nd Workshop on Deep Learning Approaches\nfor Low-Resource NLP (DeepLo 2019), pages 47–55. (Cited on\npage 163)\nShivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F Karlsson,\nAbinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel,\nDeividas Mataciunas, Laura OMahony, et al. 2024b. Aya Dataset:\nAn Open-access Collection for Multilingual Instruction Tuning.\nArXiv preprint, abs/2402.06619. (Cited on page 172)\nBIBLIOGRAPHY\n251\nSiddharth Singi, Zhanpeng He, Alvin Pan, Sandip Patel, Gunnar A.\nSigurdsson, Robinson Piramuthu, Shuran Song, and Matei Cio-\ncarlie. 2023. Decision Making for Human-in-the-loop Robotic\nAgents via Uncertainty-aware Reinforcement Learning. ArXiv\npreprint, abs/2303.06710. (Cited on page 69)\nMiroslav Sirota and Marie Juanchich. 2015. A Direct and Compre-\nhensive Test of two Postulates of Politeness Theory Applied to\nUncertainty Communication. Judgment and Decision Making,\n10(3):232–240. (Cited on page 30)\nSarath Sivaprasad and Mario Fritz. 2023. Going beyond Famil-\niar Features for Deep Anomaly Detection.\nArXiv preprint,\nabs/2310.00797. (Cited on page 161)\nChristian Sivertsen, Guido Salimbeni, Anders Sundnes Løvlie,\nSteven David Benford, and Jichen Zhu. 2024. Machine Learning\nProcesses as Sources of Ambiguity: Insights from AI Art. In Pro-\nceedings of the CHI Conference on Human Factors in Computing\nSystems, CHI ’24. (Cited on page 170)\nJohn Skilling and S. Sibisi. 1990. Fundamentals of Maxent in Data-\nanalysis. In Institute of Physics Conference Series, 107, pages\n1–21. IOP PUBLISHING LTD TEMPLE CIRCUS, TEMPLE\nWAY, BRISTOL BS1 6BE, ENGLAND. (Cited on page 19)\nFreddie Bickford Smith, Andreas Kirsch, Sebastian Farquhar, Yarin\nGal, Adam Foster, and Tom Rainforth. 2023. Prediction-oriented\nBayesian Active Learning. In International Conference on Artifi-\ncial Intelligence and Statistics, pages 7331–7348. PMLR. (Cited\non page 69)\nLewis Smith and Yarin Gal. 2018. Understanding Measures of\nUncertainty for Adversarial Example Detection. In Proceedings\nof the Thirty-Fourth Conference on Uncertainty in Artificial\nIntelligence, UAI 2018, Monterey, California, USA, August 6-10,\n2018, pages 560–569. (Cited on pages 48, 101, 109, and 110)\nMichael Smithson. 2003. Confidence Intervals. 140. (Cited on\npage 16)\nJasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practi-\ncal Bayesian Optimization of Machine Learning Algorithms. In\nAdvances in Neural Information Processing Systems 25: 26th An-\nnual Conference on Neural Information Processing Systems 2012.\nProceedings of a meeting held December 3-6, 2012, Lake Tahoe,\nNevada, United States, pages 2960–2968. (Cited on pages 78,\n148, and 332)\nBIBLIOGRAPHY\n252\nJasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshmi-\nnarayanan, Sebastian Nowozin, D. Sculley, Joshua V. Dillon,\nJie Ren, and Zachary Nado. 2019. Can You Trust Your Model’s\nUncertainty? Evaluating Predictive Uncertainty under Dataset\nShift. In Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing Sys-\ntems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 13969–13980. (Cited on pages 68, 98, 99, 110,\n111, 116, and 133)\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur\nSatish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat,\nand Ryan P. Adams. 2015. Scalable Bayesian Optimization Using\nDeep Neural Networks. In Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille, France,\n6-11 July 2015, volume 37 of JMLR Workshop and Conference\nProceedings, pages 2171–2180. (Cited on page 45)\nAnders Søgaard, Sebastian Ebert, Jasmijn Bastings, and Katja\nFilippova. 2021. We Need to TalkaAbout Random Splits. In\nProceedings of the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main Volume,\npages 1823–1832. (Cited on pages 71 and 76)\nLucia Specia. 2021. Disagreement in Human Evaluation: Blame\nthe Task, not the Annotators. NoDaLiDa Keynote. (Cited on\npage 75)\nDavid Spiegelhalter. 2017. Risk and Uncertainty Communication.\nAnnual Review of Statistics and Its Application, 4:31–60. (Cited\non page 66)\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to\nPrevent Neural Networks from Overfitting.\nThe Journal of\nMachine Learning Research, 15(1):1929–1958. (Cited on page 44)\nMaximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel\nZügner, and Stephan Günnemann. 2021. Graph Posterior Net-\nwork: Bayesian Predictive Uncertainty for Node Classification.\nIn Advances in Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages 18033–18048.\n(Cited on pages 56 and 68)\nKamile Stankeviciute, Ahmed M. Alaa, and Mihaela van der Schaar.\n2021. Conformal Time-series Forecasting. In Advances in Neu-\nral Information Processing Systems 34: Annual Conference on\nBIBLIOGRAPHY\n253\nNeural Information Processing Systems 2021, NeurIPS 2021, De-\ncember 6-14, 2021, virtual, pages 6216–6228. (Cited on page 40)\nElias Stengel-Eskin, Peter Hase, and Mohit Bansal. 2024. LACIE:\nListener-aware Finetuning for Confidence Calibration in Large\nLanguage Models. ArXiv preprint, abs/2405.21028. (Cited on\npage 169)\nJonathon Stewart, Juan Lu, Adrian Goudie, Glenn Arendts, Shiv A.\nMeka, Sam Freeman, Katie Walker, Peter Sprivulis, Frank San-\nfilippo, Mohammed Bennamoun, et al. 2022. Applications of\nNatural Language Processing at Emergency Department Triage:\nA Systematic Review. medRxiv, pages 2022–12. (Cited on page 2)\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan\nLowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F.\nChristiano. 2020. Learning to Summarize with Human Feed-\nback. In Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual. (Cited on\npages 63 and 339)\nAndreas Stolcke. 2002. SRILM - An Extensible Language Modeling\nToolkit. In Seventh international conference on spoken language\nprocessing. (Cited on page 297)\nRebecca S. Stone, Nishant Ravikumar, Andrew J. Bulpitt, and\nDavid C. Hogg. 2022. Epistemic Uncertainty-weighted Loss for\nVisual Bias Mitigation. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, CVPR Workshops\n2022, New Orleans, LA, USA, June 19-20, 2022, pages 2897–\n2904. (Cited on page 67)\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. 2019.\nEnergy and Policy Considerations for Deep Learning in NLP. In\nProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3645–3650. (Cited on page 71)\nPatrick Sturt, Martin J Pickering, and Matthew W Crocker. 1999.\nStructural Change and Reanalysis Difficulty in Language Com-\nprehension. Journal of Memory and Language, 40(1):136–150.\n(Cited on page 28)\nJiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. 2024. API\nIs Enough: Conformal Prediction for Large Language Models\nwithout Logit-access. ArXiv preprint, abs/2403.01216. (Cited\non pages 63, 164, and 165)\nBIBLIOGRAPHY\n254\nHao Sun, Boris van Breugel, Jonathan Crabbé, Nabeel Seedat, and\nMihaela van der Schaar. 2024. What Is Flagged in Uncertainty\nQuantification? Latent Density Models for Uncertainty Catego-\nrization. Advances in Neural Information Processing Systems,\n36. (Cited on page 57)\nRichard Sutton. 2019. The Bitter Lesson. Incomplete Ideas (blog),\n13:12. (Cited on page 171)\nBenjamin Swets, Timothy Desmet, Charles Clifton, and Fernanda\nFerreira. 2008.\nUnderspecification of Syntactic Ambiguities:\nEvidence from Self-paced Reading. Memory & Cognition, 36:201–\n216. (Cited on page 29)\nZoltán Gendler Szabó. 2004. Compositionality. (Cited on page 27)\nGyörgy Szarvas, Veronika Vincze, Richárd Farkas, György Móra,\nand Iryna Gurevych. 2012. Cross-genre and Cross-domain De-\ntection of Semantic Uncertainty.\nComputational Linguistics,\n38(2):335–367. (Cited on pages 31 and 167)\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon\nShlens, and Zbigniew Wojna. 2016. Rethinking the Inception\nArchitecture for Computer Vision. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016, Las\nVegas, NV, USA, June 27-30, 2016, pages 2818–2826. (Cited on\npage 37)\nNatasa Tagasovska and David Lopez-Paz. 2019.\nSingle-model\nUncertainties for Deep Learning. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pages 6414–6425. (Cited\non page 112)\nAbdul Karim Taha. 1983. Types of Syntactic Ambiguity in English.\n(Cited on page 28)\nAnique Tahir, Lu Cheng, and Huan Liu. 2023. Fairness through\nAleatoric Uncertainty. In Proceedings of the 32nd ACM Interna-\ntional Conference on Information and Knowledge Management,\npages 2372–2381. (Cited on page 67)\nLinwei Tao, Minjing Dong, and Chang Xu. 2023.\nDual Focal\nLoss for Calibration. In International Conference on Machine\nLearning, pages 33833–33849. PMLR. (Cited on page 37)\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung,\nWilliam Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran,\nBIBLIOGRAPHY\n255\nDani Yogatama, and Donald Metzler. 2023. Scaling Laws vs.\nModel Architectures: How does Inductive Bias Influence Scaling?\nIn Findings of the Association for Computational Linguistics:\nEMNLP 2023, Singapore, December 6-10, 2023, pages 12342–\n12364. Association for Computational Linguistics. (Cited on\npage 6)\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung.\n2016. Branchynet: Fast Inference via Early Exiting from Deep\nNeural Networks.\nIn 2016 23rd international conference on\npattern recognition (ICPR), pages 2464–2469. IEEE. (Cited on\npage 68)\nMattias Teye, Hossein Azizpour, and Kevin Smith. 2018. Bayesian\nUncertainty Estimation for Batch Normalized Deep Networks.\nIn Proceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,\nJuly 10-15, 2018, volume 80 of Proceedings of Machine Learning\nResearch, pages 4914–4923. (Cited on pages 44 and 45)\nThe Decoder. 2023. GPT-4 Architecture, Datasets, Costs and more\nLeaked. [Online; accessed 02-May-2024]. (Cited on pages 139\nand 174)\nThe United States District Court for the S.D.N.Y. 2013. Roberto\nMata v. Aviance Inc. (Cited on page 170)\nSunil Thulasidasan, Gopinath Chennupati, Jeff A. Bilmes, Tanmoy\nBhattacharya, and Sarah Michalak. 2019. On Mixup Training:\nImproved Calibration and Predictive Uncertainty for Deep Neural\nNetworks. In Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 13888–13899. (Cited on page 38)\nArthur Thuy and Dries F. Benoit. 2023. Explainability through Un-\ncertainty: Trustworthy Decision-making with Neural Networks.\nEuropean Journal of Operational Research. (Cited on page 67)\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael\nRafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Man-\nning. 2023. Just Ask for Calibration: Strategies for Eliciting\nCalibrated Confidence Scores from Language Models Fine-tuned\nwith Human Feedback. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing, EMNLP\n2023, Singapore, December 6-10, 2023, pages 5433–5442. (Cited\non pages 62, 144, 146, 151, 169, and 172)\nBIBLIOGRAPHY\n256\nRobert J. Tibshirani and Bradley Efron. 1993. An Introduction to\nthe Bootstrap. Monographs on statistics and applied probability,\n57(1):1–436. (Cited on pages 14 and 17)\nTijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-RMSprop:\nDivide the Gradient by a Running Average of Its Recent Mag-\nnitude. COURSERA: Neural networks for machine learning,\n4(2):26–31. (Cited on page 85)\nWilliam Timkey and Marten van Schijndel. 2021. All Bark and\nno Bite: Rogue Dimensions in Transformer Language Models\nObscure Representational Quality. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language Process-\ning, pages 4527–4546. (Cited on page 147)\nTishby and Solla. 1989. Consistent Inference of Probabilities in\nLayered Networks: Predictions and Generalizations. In Interna-\ntional 1989 joint conference on neural networks, pages 403–409.\nIEEE. (Cited on page 41)\nNaftali Tishby, Fernando C. Pereira, and William Bialek. 2000. The\nInformation Bottleneck Method. arXiv preprint physics/0004057.\n(Cited on page 161)\nNaftali Tishby and Noga Zaslavsky. 2015. Deep Learning and\nthe Information Bottleneck Principle. In 2015 ieee information\ntheory workshop (itw), pages 1–5. IEEE. (Cited on page 161)\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Na-\nman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama:\nOpen and Efficient Foundation Language Models. ArXiv preprint,\nabs/2302.13971. (Cited on pages 78, 123, and 149)\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad\nAlmahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Ba-\ntra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2:\nOpen Foundation and Fine-tuned Chat Models. ArXiv preprint,\nabs/2307.09288. (Cited on page 78)\nAlexander Treiss, Jannis Walk, and Niklas Kühl. 2021.\nAn\nUncertainty-based Human-in-the-loop System for Industrial Tool\nWear Analysis. In Machine Learning and Knowledge Discovery\nin Databases. Applied Data Science and Demo Track: European\nConference, ECML PKDD 2020, Ghent, Belgium, September\n14–18, 2020, Proceedings, Part V, pages 85–100. Springer. (Cited\non page 69)\nBIBLIOGRAPHY\n257\nTina Tseng, Amanda Stent, and Domenic Maida. 2020. Best Prac-\ntices for Managing Data Annotation Projects. ArXiv preprint,\nabs/2009.11654. (Cited on page 75)\nTheodoros Tsiligkaridis. 2019. Information Robust Dirichlet Net-\nworks for Predictive Uncertainty Estimation. ArXiv preprint,\nabs/1910.04819. (Cited on page 55)\nStéphane Tufféry. 2011. Data Mining and Statistics for Decision\nMaking. (Cited on page 79)\nDavid Tuggy. 1993. Ambiguity, Polysemy, and Vagueness. (Cited\non page 26)\nJohn Tukey. 1958. Bias and Confidence in not Quite Large Samples.\nAnn. Math. Statist., 29:614. (Cited on page 17)\nA.M. Turing. 1950. Computing Machinery and Intelligence. (Cited\non page 5)\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman.\n2023. Language Models Don’t Always Say What They Think:\nUnfaithful Explanations in Chain-of-Thought Prompting. In\nAdvances in Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\n(Cited on pages 144 and 154)\nDennis Ulmer. 2019. Recoding Latent Sentence Representations–\nDynamic Gradient-based Activation Modification in RNNs.\nArXiv preprint, abs/2101.00674. (Cited on page 173)\nDennis Ulmer, Elisa Bassignana, Max Müller-Eberstein, Daniel\nVarab, Mike Zhang, Rob van der Goot, Christian Hardmeier,\nand Barbara Plank. 2022a. Experimental Standards for Deep\nLearning in Natural Language Processing Research. In Findings\nof the Association for Computational Linguistics: EMNLP 2022,\npages 2673–2692. (Cited on pages 11, 72, and 325)\nDennis Ulmer and Giovanni Cinà. 2021. Know Your Limits: Uncer-\ntainty Estimation with ReLU Classifiers Fails at Reliable OOD\nDetection. In Proceedings of the Thirty-Seventh Conference on\nUncertainty in Artificial Intelligence, UAI 2021, Virtual Event,\n27-30 July 2021, volume 161 of Proceedings of Machine Learning\nResearch, pages 1766–1776. (Cited on pages 11, 68, and 98)\nDennis Ulmer, Jes Frellsen, and Christian Hardmeier. 2022b. Ex-\nploring Predictive Uncertainty and Calibration in NLP: A Study\non the Impact of Method & Data Scarcity. In Findings of the\nBIBLIOGRAPHY\n258\nAssociation for Computational Linguistics: EMNLP 2022, pages\n2707–2735. (Cited on pages 11, 59, 68, and 110)\nDennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and\nSeong Joon Oh. 2024a. Calibrating Large Language Models\nUsing Their Generations only. arXiv preprint 2403.05973. (Cited\non pages 12, 59, 60, 62, 63, 140, and 172)\nDennis Ulmer, Christian Hardmeier, and Jes Frellsen. 2022c. Deep-\nsignificance: Easy and Meaningful Signifcance Testing in the Age\nof Neural Networks. In ML Evaluation Standards Workshop at\nthe Tenth International Conference on Learning Representations.\n(Cited on pages 11, 80, 83, 136, 137, and 151)\nDennis Ulmer, Christian Hardmeier, and Jes Frellsen. 2023. Prior\nand Posterior Networks: A Survey on Evidential Deep Learning\nMethods for Uncertainty Estimation. Transactions on Machine\nLearning Research. (Cited on pages 11, 48, and 55)\nDennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun,\nXibin Gao, and Yi Zhang. 2024b. Bootstrapping LLM-based\nTask-oriented Dialogue Agents via Self-talk. ArXiv preprint,\nabs/2401.05033. (Cited on page 172)\nDennis Ulmer, Lotta Meijerink, and Giovanni Cinà. 2020. Trust\nIssues: Uncertainty Estimation Does not Enable Reliable OOD\nDetection on Medical Tabular Data. In Machine Learning for\nHealth, pages 341–354. PMLR. (Cited on pages 68, 98, 99, 110,\nand 111)\nDennis Ulmer, Chrysoula Zerva, and André F. T. Martins. 2024c.\nNon-exchangeable conformal language generation with nearest\nneighbors. In Findings of the Association for Computational\nLinguistics: EACL 2024, St. Julian’s, Malta, March 17-22, 2024,\npages 1909–1929. Association for Computational Linguistics.\n(Cited on pages 12, 59, 124, and 165)\nAlexandra N. Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun,\nBarbara Plank, and Massimo Poesio. 2021. Learning from Dis-\nagreement: A Survey. Journal of Artificial Intelligence Research,\n72:1385–1470. (Cited on pages 63 and 75)\nAnshuk Uppal, Kristoffer Stensbo-Smidt, Wouter Boomsma, and\nJes Frellsen. 2024.\nImplicit Variational Inference for High-\ndimensional Posteriors. Advances in Neural Information Pro-\ncessing Systems, 36. (Cited on page 68)\nBIBLIOGRAPHY\n259\nJoost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key,\nand Yarin Gal. 2021. On Feature Collapse and Deep Kernel\nLearning for Single Forward-pass Uncertainty. ArXiv preprint,\nabs/2102.11409. (Cited on pages 47, 113, 161, and 333)\nRob van der Goot. 2021. We Need to Talk about Train-dev-test\nSplits. In Proceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4485–4494. (Cited on\npage 71)\nLiam van der Poel, Ryan Cotterell, and Clara Meister. 2022. Mutual\nInformation Alleviates Hallucinations in Abstractive Summariza-\ntion. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 5956–5965. (Cited on\npages 4 and 68)\nDaan van Esch, Tamar Lucassen, Sebastian Ruder, Isaac Caswell,\nand Clara Rivera. 2022. Writing System and Speaker Metadata\nfor 2,800+ Language Varieties. In Proceedings of the Thirteenth\nLanguage Resources and Evaluation Conference, pages 5035–5046.\n(Cited on page 6)\nMarten Van Schijndel and Tal Linzen. 2018. Modeling Garden\nPath Effects without Explicit Hierarchical Syntax. In CogSci.\n(Cited on page 29)\nEva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018.\nGetting Gender Right in Neural Machine Translation. In Pro-\nceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3003–3008. (Cited on page 4)\nTamás Váradi, Steven Krauwer, Peter Wittenburg, Martin Wynne,\nand Kimmo Koskenniemi. 2008. CLARIN: Common Language\nResources and Technology Infrastructure.\nIn Proceedings of\nthe Sixth International Conference on Language Resources and\nEvaluation (LREC’08). (Cited on page 77)\nGiovanni Battista Varile, Ronald Cole, Ronald Allan Cole, Antonio\nZampolli, Joseph Mariani, Hans Uszkoreit, and Annie Zaenen.\n1997. Survey of the State of the Art in Human Language Tech-\nnology. (Cited on page 27)\nNeeraj Varshney and Chitta Baral. 2022. Model Cascading: To-\nwards Jointly Improving Efficiency and Accuracy of NLP Systems.\nIn Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 11007–11021. (Cited on\npage 68)\nBIBLIOGRAPHY\n260\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polo-\nsukhin. 2017. Attention Is All You Need. In Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neu-\nral Information Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pages 5998–6008. (Cited on pages 78,\n110, 111, 173, and 338)\nArtem Vazhentsev, Gleb Kuzmin, Artem Shelmanov, Akim Tsvi-\ngun, Evgenii Tsymbalov, Kirill Fedyanin, Maxim Panov, Alexan-\nder Panchenko, Gleb Gusev, Mikhail Burtsev, Manvel Avetisian,\nand Leonid Zhukov. 2022. Uncertainty Estimation of Transformer\nPredictions for Misclassification Detection. In Proceedings of\nthe 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8237–8252. (Cited\non page 67)\nArtem Vazhentsev, Gleb Kuzmin, Akim Tsvigun, Alexander\nPanchenko, Maxim Panov, Mikhail Burtsev, and Artem Shel-\nmanov. 2023. Hybrid Uncertainty Quantification for Selective\nText Classification in Ambiguous Tasks. In Proceedings of the\n61st Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 11659–11681. (Cited\non pages 67 and 164)\nRaúl Vázquez, Hande Celikkanat, Dennis Ulmer, Jörg Tiedemann,\nSwabha Swayamdipta, Wilker Aziz, Barbara Plank, Joris Baan,\nand Marie-Catherine de Marneffe. 2024. Proceedings of the 1st\nWorkshop on Uncertainty-Aware NLP (UncertaiNLP 2024). In\nProceedings of the 1st Workshop on Uncertainty-Aware NLP\n(UncertaiNLP 2024). (Cited on page 165)\nPablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu,\nMarius Hobbhahn, and Anson Ho. 2022. Will We Run out of\nData? An Analysis of the Limits of Scaling Datasets in Machine\nLearning. ArXiv preprint, abs/2211.04325. (Cited on page 171)\nVeronika Vincze. 2014. Uncertainty Detection in Natural Lan-\nguage Texts. Ph.D. thesis, Szegedi Tudomanyegyetem (Hungary).\n(Cited on pages 31 and 167)\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani\nLuotolahti, Tapio Salakoski, Filip Ginter, and Sampo Pyysalo.\n2019. Multilingual Is not Enough: BERT for Finnish. ArXiv\npreprint, abs/1912.07076. (Cited on pages 114, 163, and 333)\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haber-\nland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu\nBIBLIOGRAPHY\n261\nPeterson, Warren Weckesser, Jonathan Bright, Stéfan J. van\nder Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman,\nNikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert\nKern, Eric Larson, C. J. Carey, İlhan Polat, Yu Feng, Eric W.\nMoore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert\nCimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris,\nAnne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul\nvan Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python.\nNature Methods, 17:261–272. (Cited on page 325)\nKailas Vodrahalli, Tobias Gerstenberg, and James Y Zou. 2022.\nUncalibrated Models Can Improve Human-AI Collaboration.\nAdvances in Neural Information Processing Systems, 35:4004–\n4016. (Cited on page 66)\nLeandro Von Werra, Lewis Tunstall, Abhishek Thakur, Sasha Luc-\ncioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen\nRajani, Victor Mustar, and Helen Ngo. 2022. Evaluate & Eval-\nuation on the Hub: Better Best Practices for Data and Model\nMeasurements. In Proceedings of the 2022 Conference on Empir-\nical Methods in Natural Language Processing: System Demon-\nstrations, pages 128–136. (Cited on page 78)\nVladimir Vovk. 2012. Conditional Validity of Inductive Conformal\nPredictors. In Asian conference on machine learning, pages\n475–490. PMLR. (Cited on page 166)\nVladimir Vovk, Alexander Gammerman, and Glenn Shafer. 2005.\nAlgorithmic Learning in a Random World, volume 29. (Cited on\npages 12, 39, 124, 166, and 338)\nYoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. 2021.\nOn Calibration And Out-of-domain Generalization. In Advances\nin Neural Information Processing Systems 34: Annual Confer-\nence on Neural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pages 2215–2227. (Cited on\npage 113)\nPeter Walley. 1991. Statistical Reasoning with Imprecise Probabil-\nities. (Cited on page 57)\nBin Wang, Tianrui Li, Zheng Yan, Guangquan Zhang, and Jie Lu.\n2020a. DeepPipe: A Distribution-free Uncertainty Quantification\nApproach for Time Series Forecasting. Neurocomputing, 397:11–\n19. (Cited on page 6)\nBIBLIOGRAPHY\n262\nCheng Wang, Carolin Lawrence, and Mathias Niepert. 2021a. Un-\ncertainty Estimation and Calibration with Finite-state Prob-\nabilistic RNNs. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. (Cited on page 111)\nDan Wang and Yi Shang. 2014. A New Active Labeling Method for\nDeep Learning. In 2014 International joint conference on neural\nnetworks (IJCNN), pages 112–119. IEEE. (Cited on page 68)\nDeng-Bao Wang, Lanqing Li, Peilin Zhao, Pheng-Ann Heng, and\nMin-Ling Zhang. 2023a. On the Pitfall of Mixup for Uncertainty\nCalibration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 7609–7618.\n(Cited on page 38)\nHao Wang, Luxi He, Rui Gao, and Flavio Calmon. 2024a. Aleatoric\nand Epistemic Discrimination: Fundamental Limits of Fairness\nInterventions. Advances in Neural Information Processing Sys-\ntems, 36. (Cited on page 67)\nKaizheng Wang, Keivan Shariatmadar, Shireen Kudukkil Manchin-\ngal, Fabio Cuzzolin, David Moens, and Hans Hallez. 2024b.\nCreINNs:\nCredal-set Interval Neural Networks for Uncer-\ntainty Estimation in Classification Tasks.\nArXiv preprint,\nabs/2401.05043. (Cited on page 58)\nShuo Wang, Yang Liu, Chao Wang, Huanbo Luan, and Maosong\nSun. 2019. Improving Back-translation with Uncertainty-based\nConfidence Estimation. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 791–802. (Cited on page 143)\nShuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu. 2020b.\nOn the Inference Calibration of Neural Machine Translation. In\nProceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 3070–3079. (Cited on page 59)\nXinpeng Wang and Barbara Plank. 2023. ACTOR: active learning\nwith annotator-specific classification heads to embrace human\nlabel variation. In Proceedings of the 2023 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023, pages 2046–2052. Association\nfor Computational Linguistics. (Cited on page 69)\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.\nChi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.\nBIBLIOGRAPHY\n263\n2023b. Self-consistency Improves Chain-of-thought Reasoning\nin Language Models. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net. (Cited on page 61)\nYingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Sha-\nposhnik. 2021b. Understanding how Dimension Reduction Tools\nWork: An Empirical Approach to Deciphering t-SNE, UMAP,\nTriMAP, And PacMAP for Data Visualization. Journal of Ma-\nchine Learning Research, 22:201:1–201:73. (Cited on page 320)\nYongguang Wang, Huobin Tan, and Shuzhen Yao. 2021c. Curved\nSDE-Net Leads to Better Generalization for Uncertainty Esti-\nmates of DNNs. In International Conference on Artificial Neural\nNetworks, pages 248–259. Springer. (Cited on page 56)\nYongguang Wang and Shuzhen Yao. 2021. Neural Stochastic Dif-\nferential Equations with Neural Processes Family Members for\nUncertainty Estimation in Deep Learning. Sensors, 21(11):3708.\n(Cited on page 56)\nZeerak Waseem, Smarika Lulz, Joachim Bingel, and Isabelle Au-\ngenstein. 2021. Disembodied Machine Learning: On the Illusion\nof Objectivity in NLP. ArXiv preprint, abs/2101.11974. (Cited\non pages 3, 74, and 82)\nRonald L. Wasserstein, Allen L. Schirm, and Nicole A. Lazar. 2019.\nMoving to a World Beyond “p <0.05”. (Cited on pages 81, 84,\nand 95)\nDavid Watson, Joshua O’Hara, Niek Tax, Richard Mudd, and Ido\nGuy. 2024. Explaining Predictive Uncertainty with Informa-\ntion Theoretic Shapley Values. Advances in Neural Information\nProcessing Systems, 36. (Cited on page 167)\nRobert Watt. 2021. The Fantasy of Carbon Offsetting. Environ-\nmental Politics, 30(7):1069–1088. (Cited on page 327)\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei\nXia, Ed Chi, Quoc V. Le, Denny Zhou, et al. 2022. Chain-of-\nthought Prompting Elicits Reasoning in Large Language Models.\nAdvances in Neural Information Processing Systems, 35:24824–\n24837. (Cited on pages 143, 151, and 338)\nMax Welling and Yee Whye Teh. 2011. Bayesian Learning via\nStochastic Gradient Langevin Dynamics. In Proceedings of the\n28th International Conference on Machine Learning, ICML 2011,\nBellevue, Washington, USA, June 28 - July 2, 2011, pages 681–\n688. (Cited on page 42)\nBIBLIOGRAPHY\n264\nYeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusen-\nberry, Jasper Snoek, Balaji Lakshminarayanan, and Dustin Tran.\n2020a. Improving Calibration of Batchensemble with Data Aug-\nmentation. Workshop on Uncertainty and Ro-Bustness in Deep\nLearning. (Cited on page 38)\nYeming Wen, Ghassen Jerfel, Rafael Muller, Michael W. Dusen-\nberry, Jasper Snoek, Balaji Lakshminarayanan, and Dustin Tran.\n2021. Combining Ensembles and Data Augmentation Can Harm\nYour Calibration. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. (Cited on page 38)\nYeming Wen, Dustin Tran, and Jimmy Ba. 2020b. Batchensemble:\nAn Alternative Approach to Efficient Ensemble and Lifelong\nLearning. In 8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n(Cited on page 46)\nJonathan Wenger, Hedvig Kjellström, and Rudolph Triebel. 2020.\nNon-parametric Calibration for Classification. In The 23rd In-\nternational Conference on Artificial Intelligence and Statistics,\nAISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily,\nItaly], volume 108 of Proceedings of Machine Learning Research,\npages 178–190. (Cited on page 37)\nFlorian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Je-\nnatton. 2020. Hyperparameter Ensembles for Robustness and\nUncertainty Quantification. In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual. (Cited on page 46)\nJennifer C. White and Ryan Cotterell. 2021. Examining the Induc-\ntive Bias of Neural Language Models with Artificial Languages.\nIn Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long\nPapers), pages 454–463. (Cited on page 74)\nAndreas Widoff. 2022. Equivalence and Polyvalence: A Case for\nthe Stratification of Semantics. Public Journal of Semiotics,\n10(1):1–25. (Cited on page 30)\nMartijn Wieling, Josine Rawee, and Gertjan van Noord. 2018.\nReproducibility in Computational Linguistics: Are We Willing\nto Share? Computational Linguistics, 44(4):641–649. (Cited on\npage 77)\nBIBLIOGRAPHY\n265\nNorbert Wiener. 1949. Extrapolation, Interpolation, and Smooth-\ning of Stationary Time Series: With Engineering Applications.\n(Cited on page 47)\nWikimedia Commons. 2022a. Iris Setosa. (Cited on page 50)\nWikimedia Commons. 2022b. Iris Versicolor. (Cited on page 50)\nWikimedia Commons. 2022c. Iris Virginica. (Cited on page 50)\nWikimedia Foundation. 2022.\nWikimedia Downloads.\nhttps:\n//dumps.wikimedia.org/. (Cited on page 129)\nWikipedia contributors. 2024.\nWikipedia Statistics.\nhttps:\n//stats.wikimedia.org/EN/BotActivityMatrixCreates.htm.\n[Online; accessed 12.04.2024]. (Cited on page 6)\nFrank Wilcoxon. 1992. Individual Comparisons by Ranking Meth-\nods. In Breakthroughs in Statistics, pages 196–202. (Cited on\npages 80 and 87)\nChristopher K. I. Williams. 1998. Computation with Infinite Neural\nNetworks. Neural Computation, 10(5):1203–1216. (Cited on\npage 47)\nChristopher K.I. Williams and Carl Edward Rasmussen. 2006.\nGaussian Processes for Machine Learning, volume 2. (Cited on\npage 47)\nTimothy Williamson. 2002. Vagueness. (Cited on page 27)\nRobin Willink and Rod White. 2011. Disentangling Classical and\nBayesian Approaches to Uncertainty Analysis. Measurement\nStandards Laboratory, PO Box, 31310. (Cited on page 14)\nAndrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and\nEric P. Xing. 2016.\nDeep Kernel Learning.\nIn Proceedings\nof the 19th International Conference on Artificial Intelligence\nand Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016,\nvolume 51 of JMLR Workshop and Conference Proceedings, pages\n370–378. (Cited on page 47)\nAndrew Gordon Wilson and Pavel Izmailov. 2020. Bayesian Deep\nLearning and a Probabilistic Perspective of Generalization. In\nAdvances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual. (Cited on pages 45,\n46, and 110)\nBIBLIOGRAPHY\n266\nLisa Wimmer, Yusuf Sale, Paul Hofman, Bernd Bischl, and Eyke\nHüllermeier. 2023. Quantifying Aleatoric and Epistemic Un-\ncertainty in Machine Learning: Are Conditional Entropy and\nMutual Information Appropriate Measures?\nIn Uncertainty\nin Artificial Intelligence, pages 2282–2292. PMLR. (Cited on\npage 167)\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth,\nVivek Natarajan, Joseph R. Ledsam, Patricia MacWilliams,\nPushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al.\n2020. Contrastive Training for Improved Out-of-distribution\nDetection. ArXiv preprint, abs/2007.05566. (Cited on page 113)\nJohn Michael Winn. 2004. Variational Message Passing and Its\nApplications. (Cited on page 278)\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault,\nRemi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers: State-of-the-\nart Natural Language Processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 38–45. (Cited on page 325)\nJae Oh Woo. 2022. Analytic Mutual Information in Bayesian Neural\nNetworks. In IEEE International Symposium on Information\nTheory, ISIT 2022, Espoo, Finland, June 26 - July 1, 2022, pages\n300–305. (Cited on page 54)\nBen Wu, Yue Li, Yida Mu, Carolina Scarton, Kalina Bontcheva,\nand Xingyi Song. 2023a. Don’t waste a single annotation: Im-\nproving single-label classifiers through soft labels. In Findings\nof the Association for Computational Linguistics: EMNLP 2023,\nSingapore, December 6-10, 2023, pages 5347–5355. Association\nfor Computational Linguistics. (Cited on page 63)\nXixin Wu and Mark Gales. 2021. Should Ensemble Members Be\nCalibrated? ArXiv preprint, abs/2101.05397. (Cited on page 38)\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithvi-\nraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Han-\nnaneh Hajishirzi. 2023b. Fine-grained Human Feedback Gives\nBetter Rewards For Language Model Training. In Thirty-seventh\nConference on Neural Information Processing Systems. (Cited\non page 143)\nBIBLIOGRAPHY\n267\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.\nEx\nMachina: Personal Attacks Seen at Scale. In Proceedings of\nthe 26th International Conference on World Wide Web, WWW\n2017, Perth, Australia, April 3-7, 2017, pages 1391–1399. (Cited\non page 96)\nAlexandros Xenos, Themos Stafylakis, Ioannis Patras, and Georgios\nTzimiropoulos. 2023. A Simple Baseline for Knowledge-based\nVisual Question Answering. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language Processing,\nEMNLP 2023, Singapore, December 6-10, 2023, pages 14871–\n14877. Association for Computational Linguistics. (Cited on\npage 77)\nTim Z. Xiao, Aidan N. Gomez, and Yarin Gal. 2020. Wat Zei\nJe? Detecting Out-of-distribution Translations with Variational\nTransformers.\nArXiv preprint, abs/2006.08344.\n(Cited on\npages 60, 111, and 333)\nYijun Xiao and William Yang Wang. 2021. On Hallucination and\nPredictive Uncertainty in Conditional Language Generation. In\nProceedings of the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main Volume,\npages 2734–2744. (Cited on pages 4, 67, and 68)\nYuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger,\nRuslan Salakhutdinov, and Louis-Philippe Morency. 2022. Un-\ncertainty Quantification with Pre-trained Language Models: A\nLarge-scale Empirical Analysis. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages 7273–7284.\n(Cited on page 59)\nChengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel\nBibi, Ziniu Hu, Philip Torr, Bernard Ghanem, and Guohao Li.\n2024. Can Large Language Model Agents Simulate Human Trust\nBehaviors? ArXiv preprint, abs/2402.04559. (Cited on page 65)\nJohnathan Xie, Annie S. Chen, Yoonho Lee, Eric Mitchell, and\nChelsea Finn.\nCalibrating Language Models with Adaptive\nTemperature Scaling. In ICLR 2024 Workshop on Secure and\nTrustworthy Large Language Models. (Cited on page 60)\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu\nMa. 2022. An Explanation of In-context Learning as Implicit\nBayesian Inference. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-\n29, 2022. (Cited on page 91)\nBIBLIOGRAPHY\n268\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian\nHe, and Bryan Hooi. 2023. Can LLMs Express Their Uncer-\ntainty? An Empirical Evaluation of Confidence Elicitation in\nLLMs. In The Twelfth International Conference on Learning\nRepresentations. (Cited on pages 60 and 62)\nChen Xu and Yao Xie. 2021. Conformal Prediction Interval for\nDynamic Time-series. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine Learning\nResearch, pages 11559–11569. (Cited on page 40)\nFrank F. Xu, Uri Alon, and Graham Neubig. 2023a. Why do nearest\nneighbor language models work? In International Conference\non Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,\nHawaii, USA, volume 202 of Proceedings of Machine Learning\nResearch, pages 38325–38341. PMLR. (Cited on page 124)\nJiacheng Xu, Shrey Desai, and Greg Durrett. 2020. Understanding\nNeural Abstractive Summarization Models via Uncertainty. In\nProceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6275–6281. (Cited\non pages 4 and 167)\nWinnie Xu, Ricky T. Q. Chen, Xuechen Li, and David Duvenaud.\n2022. Infinitely Deep Bayesian Neural Networks with Stochastic\nDifferential Equations. In International Conference on Artificial\nIntelligence and Statistics, AISTATS 2022, 28-30 March 2022,\nVirtual Event, volume 151 of Proceedings of Machine Learning\nResearch, pages 721–738. (Cited on page 56)\nYunpeng Xu, Wenge Guo, and Zhi Wei. 2023b. Conformal Risk Con-\ntrol for Ordinal Classification. In Proceedings of the Thirty-Ninth\nConference on Uncertainty in Artificial Intelligence, volume 216\nof Proceedings of Machine Learning Research, pages 2346–2355.\n(Cited on page 40)\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024. Hallucination\nIs Inevitable: An Innate Limitation of Large Language Models.\nArXiv preprint, abs/2401.11817. (Cited on page 171)\nBoyang Xue, Hongru Wang, Weichao Wang, Rui Wang, Sheng\nWang, Zeming Liu, and Kam-Fai Wong. 2024a. A Comprehensive\nStudy of Multilingual Confidence Estimation on Large Language\nModels. ArXiv preprint, abs/2402.13606. (Cited on pages 60\nand 62)\nBIBLIOGRAPHY\n269\nFuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and\nYang You. 2024b. To Repeat or not to Repeat: Insights from\nScaling LLM under Token-crisis. Advances in Neural Information\nProcessing Systems, 36. (Cited on page 172)\nAdam Yang, Maxime Robeyns, Xi Wang, and Laurence Aitchison.\n2023. Bayesian Low-rank adaptation for Large Language Models.\nIn Socially Responsible Language Modelling Research. (Cited on\npage 61)\nAdam X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang,\nHaitham Bou-Ammar, and Laurence Aitchison. 2024. Bayesian\nReward\nModels\nfor\nLLM\nAlignment.\nArXiv\npreprint,\nabs/2402.13210. (Cited on page 63)\nJie Yang, Shuailong Liang, and Yue Zhang. 2018. Design Challenges\nand Misconceptions in Neural Sequence Labeling. In Proceedings\nof the 27th International Conference on Computational Linguis-\ntics, pages 3879–3889. (Cited on page 80)\nKevin Yang and Dan Klein. 2021. FUDGE: Controlled Text Gen-\neration with Future Discriminators. In Proceedings of the 2021\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies,\npages 3511–3535. (Cited on pages 138 and 164)\nShingo Yashima, Teppei Suzuki, Kohta Ishikawa, Ikuro Sato, and\nRei Kawakami. 2022. Feature Space Particle Inference for Neural\nNetwork Ensembles. In International Conference on Machine\nLearning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\nUSA, volume 162 of Proceedings of Machine Learning Research,\npages 25452–25468. (Cited on page 46)\nFanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang,\nDerek F Wong, Emine Yilmaz, Shuming Shi, and Zhaopeng\nTu. 2024. Benchmarking LLMs via Uncertainty Quantification.\nArXiv preprint, abs/2401.12794. (Cited on page 59)\nNanyang Ye, Kaican Li, Lanqing Hong, Haoyue Bai, Yiting Chen,\nFengwei Zhou, and Zhenguo Li. 2021.\nOOD-Bench: Bench-\nmarking and Understanding Out-of-distribution Generalization\nDatasets and Algorithms.\nArXiv preprint, abs/2106.03721.\n(Cited on page 76)\nMing Yin, Jennifer Wortman Vaughan, and Hanna M. Wallach.\n2019. Understanding the Effect of Accuracy on Trust in Machine\nLearning Models. In Proceedings of the 2019 CHI Conference\non Human Factors in Computing Systems, CHI 2019, Glasgow,\nScotland, UK, May 04-09, 2019, page 279. (Cited on page 65)\nBIBLIOGRAPHY\n270\nGal Yona, Roee Aharoni, and Mor Geva. 2024. Can Large Language\nModels Faithfully Express their Intrinsic Uncertainty in Words?\narXiv preprint arXiv: 2405.16908. (Cited on page 62)\nKiyon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak. 2022. Detec-\ntion of Adversarial Examples in Text Classification: Benchmark\nand Baseline via Robust Density Estimation. In Findings of\nthe Association for Computational Linguistics: ACL 2022, pages\n3656–3672. (Cited on page 164)\nHanlin Yu, Marcelo Hartmann, Bernardo Williams Moreno Sanchez,\nMark Girolami, and Arto Klami. 2024. Riemannian laplace\napproximation with the fisher metric. In International Conference\non Artificial Intelligence and Statistics, 2-4 May 2024, Palau\nde Congressos, Valencia, Spain, volume 238 of Proceedings of\nMachine Learning Research, pages 820–828. PMLR. (Cited on\npage 45)\nKe-Hai Yuan and Kentaro Hayashi. 2003. Bootstrap Approach to\nInference and Power Analysis Based on three Test Statistics for\nCovariance Structure Models. British Journal of Mathematical\nand Statistical Psychology, 56(1):93–110. (Cited on page 79)\nMargaux Zaffran, Olivier Féron, Yannig Goude, Julie Josse, and\nAymeric Dieuleveut. 2022. Adaptive Conformal Predictions for\nTime Series. In International Conference on Machine Learning,\nICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume\n162 of Proceedings of Machine Learning Research, pages 25834–\n25866. (Cited on page 40)\nSheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C. Holmes,\nFrank Hutter, and Yee Whye Teh. 2021. Neural Ensemble Search\nfor Uncertainty Estimation and Dataset Shift.\nIn Advances\nin Neural Information Processing Systems 34: Annual Confer-\nence on Neural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pages 7898–7911. (Cited on\npage 46)\nMarcos Zampieri, Preslav Nakov, and Yves Scherrer. 2020. Natural\nLanguage Processing for Similar Languages, Varieties, and Di-\nalects: A Survey. Natural Language Engineering, 26(6):595–612.\n(Cited on page 6)\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014.\nRecurrent Neural Network Regularization.\narXiv preprint\narXiv:1409.2329. (Cited on page 332)\nGünter Zech. 2002. Frequentist and Bayesian Confidence Intervals.\nEPJ direct, 4:1–81. (Cited on page 16)\nBIBLIOGRAPHY\n271\nChrysoula Zerva, Taisiya Glushkova, Ricardo Rei, and André F.T.\nMartins. 2022. Better Uncertainty Quantification for Machine\nTranslation Evaluation. arXiv e-prints, pages arXiv–2204. (Cited\non page 143)\nYuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng,\nBo Ding, and Huaimin Wang. 2024. Uncertainty-penalized Rein-\nforcement Learning from Human Feedback with Diverse Reward\nLoRA Ensembles. ArXiv preprint, abs/2401.00243. (Cited on\npage 63)\nCaiqi Zhang, Fangyu Liu, Marco Basaldella, and Nigel Collier.\n2024a. LUQ: Long-text Uncertainty Quantification for LLMs.\nArXiv preprint, abs/2403.20279. (Cited on page 60)\nChang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei,\nQi Han, Zhen Li, and Ming-Ming Cheng. 2021a. Delving Deep\ninto Label Smoothing. IEEE Transactions on Image Processing,\n30:5984–5996. (Cited on page 37)\nGuodong Zhang, Shengyang Sun, David Duvenaud, and Roger B.\nGrosse. 2018a. Noisy Natural Gradient as Variational Inference.\nIn Proceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,\nJuly 10-15, 2018, volume 80 of Proceedings of Machine Learning\nResearch, pages 5847–5856. (Cited on page 44)\nHanlin Zhang, Yifan Zhang, Yaodong Yu, Dhruv Madeka, Dean Fos-\nter, Eric P. Xing, Himabindu Lakkaraju, and Sham M. Kakade.\nA Study on the Calibration of In-context Learning. In I Can’t\nBelieve It’s Not Better Workshop: Failure Modes in the Age of\nFoundation Models. (Cited on page 59)\nHanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian,\nXingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2023a.\nR-Tuning: Teaching Large Language Models to Refuse Unknown\nQuestions. ArXiv preprint, abs/2311.09677. (Cited on page 62)\nHongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David\nLopez-Paz. 2018b. Mixup: Beyond Empirical Risk Minimization.\nIn 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. (Cited on page 38)\nHugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind\nNeelakantan. 2021b. Trading off Diversity and Quality in Natural\nLanguage Generation. In Proceedings of the Workshop on Human\nEvaluation of NLP Systems (HumEval), pages 25–33. (Cited on\npage 123)\nBIBLIOGRAPHY\n272\nJize Zhang, Bhavya Kailkhura, and Thomas Yong-Jin Han. 2020a.\nMix-N-Match: Ensemble and Compositional Methods for Un-\ncertainty Calibration in Deep Learning. In Proceedings of the\n37th International Conference on Machine Learning, ICML 2020,\n13-18 July 2020, Virtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 11117–11128. (Cited on pages 37\nand 38)\nLinjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou.\n2022a. When and how Mixup Improves Calibration. In Interna-\ntional Conference on Machine Learning, ICML 2022, 17-23 July\n2022, Baltimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 26135–26160. (Cited on\npage 38)\nMike Zhang and Barbara Plank. 2021. Cartography Active Learning.\nIn Findings of the Association for Computational Linguistics:\nEMNLP 2021, pages 395–406. (Cited on page 69)\nRuqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and\nAndrew Gordon Wilson. 2020b. Cyclical Stochastic Gradient\nMCMC for Bayesian Deep Learning. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. (Cited on page 43)\nShujian Zhang, Chengyue Gong, and Eunsol Choi. 2021c. Know-\ning more about Questions Can Help: Improving Calibration in\nQuestion Answering. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages 1958–1970.\n(Cited on page 59)\nShun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing\nSun, and Chuang Gan. 2024b. Improving Reinforcement Learning\nfrom Human Feedback with Efficient Reward Model Ensemble.\nArXiv preprint, abs/2401.16635. (Cited on page 63)\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya\nChen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,\nXi Victoria Lin, et al. 2022b. OPT: Open Pre-trained Trans-\nformer Language Models. ArXiv preprint, abs/2205.01068. (Cited\non page 129)\nTianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang,\nZheng Zhang, Chenghu Zhou, Xinbing Wang, and Luoyi Fu.\n2023b. Enhancing Uncertainty-based Hallucination Detection\nwith Stronger Focus. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP\nBIBLIOGRAPHY\n273\n2023, Singapore, December 6-10, 2023, pages 915–932. Associa-\ntion for Computational Linguistics. (Cited on page 67)\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,\nand Yoav Artzi. 2020c. BERTscore: Evaluating Text Generation\nwith BERT. In 8th International Conference on Learning Rep-\nresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. (Cited on page 135)\nXiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang,\nand Yang Liu. 2024c. Overcoming Reward Overoptimization via\nAdversarial Policy Optimization with Lightweight Uncertainty\nEstimation. ArXiv preprint, abs/2403.05171. (Cited on page 63)\nXinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui\nWang, Zhongjie Ba, and Kui Ren. 2023c. Text-CRS: A general-\nized certified robustness framework against textual adversarial\nattacks. In 2024 IEEE Symposium on Security and Privacy (SP),\npages 53–53. IEEE Computer Society. (Cited on page 133)\nYunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020d.\nEffect of Confidence and Explanation on Accuracy and Trust\nCalibration in AI-Assisted Decision Making. In Proceedings of\nthe 2020 conference on fairness, accountability, and transparency,\npages 295–305. (Cited on page 66)\nZhisong Zhang, Emma Strubell, and Eduard Hovy. 2022c.\nA\nSurvey of Active Learning for Natural Language Processing.\nIn Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 6166–6190. (Cited on\npage 68)\nXujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. 2020. Un-\ncertainty Aware Semi-Supervised Learning on Graph Data. In\nAdvances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual. (Cited on page 49)\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,\nZhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng\nLi, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\n2023. Judging LLM-as-a-judge with MT-bench and chatbot\narena. In Advances in Neural Information Processing Systems 36:\nAnnual Conference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\n2023. (Cited on page 149)\nBIBLIOGRAPHY\n274\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing\nChen, Weihua Luo, and Jiajun Chen. 2021. Adaptive Nearest\nNeighbor Machine Translation. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 368–374. (Cited on\npage 124)\nZhuobin Zheng, Chun Yuan, Xinrui Zhu, Zhihui Lin, Yangyang\nCheng, Cheng Shi, and Jiahui Ye. 2019. Self-supervised Mixture-\nof-experts by Uncertainty Estimation.\nIn The Thirty-Third\nAAAI Conference on Artificial Intelligence, AAAI 2019, The\nThirty-First Innovative Applications of Artificial Intelligence\nConference, IAAI 2019, The Ninth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2019, Honolulu,\nHawaii, USA, January 27 - February 1, 2019, pages 5933–5940.\n(Cited on page 68)\nKaitlyn Zhou, Jena D. Hwang, Xiang Ren, and Maarten Sap.\n2024.\nRelying on the Unreliable: The Impact of Language\nModels’ Reluctance to Express Uncertainty. ArXiv preprint,\nabs/2401.06730. (Cited on page 144)\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023. Nav-\nigating the Grey Area: Expressions of Overconfidence and Un-\ncertainty in Language Models. arXiv e-prints, pages arXiv–2302.\n(Cited on pages 30, 62, 153, and 169)\nXiang Zhou, Yixin Nie, and Mohit Bansal. 2022. Distributed NLI:\nLearning to Predict Human Opinion Distributions for Language\nReasoning. In Findings of the Association for Computational\nLinguistics: ACL 2022, pages 972–987. (Cited on page 63)\nZhi-Hua Zhou, Jianxin Wu, and Wei Tang. 2002. Ensembling\nNeural Networks: Many Could Be Better than All. Artificial\nintelligence, 137(1-2):239–263. (Cited on page 46)\nChiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, and\nZhendong Mao. 2023. On the calibration of large language models\nand alignment. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 9778–9795. Association for Computational Linguistics.\n(Cited on pages 37, 59, 63, and 172)\nDaniel Zhu, Arnaud Martin, Yolande Le Gall, Jean-Christophe\nDubois, and Vincent Lemaire. 2021. Evidential Nearest Neigh-\nbours in Active Learning. In Worksop on Interactive Adaptive\nLearning (IAL)-ECML-PKDD. (Cited on page 69)\nBIBLIOGRAPHY\n275\nLingxue Zhu and Nikolay Laptev. 2017.\nDeep and Confident\nPrediction for Time Series at Uber. In 2017 IEEE International\nConference on Data Mining Workshops (ICDMW), pages 103–\n110. IEEE. (Cited on page 6)\nBrian J. Zikmund-Fisher, Dylan M. Smith, Peter A. Ubel, and\nAngela Fagerlin. 2007. Validation of the Subjective Numeracy\nScale: Effects of Low Numeracy on Comprehension of Risk Com-\nmunications and Utility Elicitations. Medical Decision Making,\n27(5):663–671. (Cited on page 66)\nSteve Ziliak and Deirdre Nansen McCloskey. 2008. The Cult of\nStatistical Significance: How the Standard Error Costs Us Jobs,\nJustice, and Lives. (Cited on page 81)\nThomas Zollo, Todd Morrill, Zhun Deng, Jake Snell, Toniann\nPitassi, and Richard Zemel. 2023. Prompt risk control: A rig-\norous framework for responsible deployment of large language\nmodels. In Socially Responsible Language Modelling Research.\n(Cited on pages 59, 164, and 165)\nShi Zong, Ashutosh Baheti, Wei Xu, and Alan Ritter. 2020.\nExtracting Covid-19 Events from Twitter.\nArXiv preprint,\nabs/2006.02567. (Cited on page 77)\nShoshana Zuboff. 2023. The Age of Surveillance Capitalism. In\nSocial Theory Re-wired, pages 203–213. (Cited on page 171)\nHarun Zulić. 2019. How AI Can Change/Improve/Influence Music\nComposition, Performance and Education: Three Case Studies.\nIn INSAM Journal of Contemporary Music, Art and Technology,\n2, pages 100–114. (Cited on page 170)\nA\n|\nTheoretical Appendix\n“Physics is searching for a theory of everything. Deep\nlearning is searching for a theory of anything.”\n—Zachary Lipton on Twitter.\nThesis\nAppendix\nSection 2.1.2\nAppendix A.1\nSection 2.2.3\nAppendices A.2, A.3, A.5 and A.6\nSection 4.1.1\nAppendix A.7\nSection 4.1.3\nAppendices A.9 and A.10\nSection 4.1.4\nAppendices A.6 and A.11 to A.13\nTable A.1: Correspondences between sections of the theoretical appendix\nand thesis chapters.\nThis appendix contains additional derivations and proofs for\nsome of the main chapters in this thesis.\nTable A.1 gives an\noverview over the correspondences between thesis chapters and\nsections in this appendix.\nA.1\nRelationship between Beta and Gamma\nfunction\nHere, we further elaborate on the connection between the Beta and\nthe Gamma function, used to derive the predictive prior and pos-\nterior distribution of a Beta distribution with Bernoulli likelihood\nin Equations (2.28) and (2.34). The Beta function is commonly\ndefined in terms of Gamma functions, namely\nB(α, β) = Γ(α)Γ(β)\nΓ(α + β) ,\n(A.1)\nand recall the definition of the Gamma function as\nΓ(α) =\nZ ∞\n0\nxα−1 exp(−x)dx.\n(A.2)\nAlternatively, the Beta function can be stated as\n276\nA.1 relationship between beta and gamma function\n277\nB(α, β) =\nZ 1\n0\nxα−1(1 −x)β−1dx.\n(A.3)\nThis connection arises by evaluating the following product:\nΓ(α)Γ(β) =\n\u0010 Z ∞\n0\nxα−1 exp(−x)dx\n\u0011\u0010 Z ∞\n0\nyβ−1 exp(−y)dy\n\u0011\n(A.4)\n=\nZ ∞\n0\nZ ∞\n0\nxα−1yβ−1 exp\n\u0000−(x + y)\n\u0001\ndxdy.\n(A.5)\nIn order to simplify the integration, we apply a change of\nvariables by substituting x = uv and y = u(1 −v). To account\nfor the change of variables during the integration, we also need to\nevaluate the determinant of the Jacobian as\n|J| =\n\f\f\f\f\f\n∂x\n∂u\n∂x\n∂v\n∂y\n∂u\n∂y\n∂v\n\f\f\f\f\f =\n\f\f\f\f\f\nv\nu\n1 −v\n−u\n\f\f\f\f\f = −uv −u(1 −v) = −u.\n(A.6)\nBy writing u and v in terms of x and y, we obtain that u = x+y\nand v = x/(x + y), which implies that the limits for the integration\nremain 0 to ∞for u and become 0 to 1 for v. Using all of these\ninsights, we can now show that\nΓ(α)Γ(β) =\nZ ∞\n0\nZ ∞\n0\nxα−1yβ−1 exp\n\u0000−(x + y)\n\u0001\ndxdv\n(A.7)\n=\nZ 1\n0\nZ ∞\n0\n(uv)α−1\u0000u(1 −v)\n\u0001β−1\nexp\n\u0000−(uv + u(1 −v))\n\u0001\n| −u|dudv\n(A.8)\n=\nZ 1\n0\nZ ∞\n0\nuα−1vα−1uβ−1(1 −v)β−1 exp(−u)ududv\n(A.9)\n=\nZ 1\n0\nZ ∞\n0\nuα+β−1vα−1uβ−1(1 −v)β−1 exp(−u)dudv\n(A.10)\n=\n\u0010 Z 1\n0\nvα−1(1 −v)β−1dv\n\u0011\u0010 Z ∞\n0\nuα+β−1 exp(−u)du\n\u0011\n(A.11)\n=B(α, β)Γ(α + β),\n(A.12)\nfrom which the connection between the two definition follows.\nA.2 expectation of the dirichlet distribution\n278\nA.2\nExpectation of the Dirichlet Distribution\nHere, we show results for the quantities E[πk] and E[log πk] that\nappear in Section 2.2.3. For the first, we follow the derivation by\nMiller (2011). Another proof is given by Lin (2016).\nE[πk] =\nZ\n· · ·\nZ\nπk\nΓ(α0)\nQK\nk′=1 Γ(α′\nk)\nK\nY\nk′=1\nπ\nαk′−1\nk′\ndπ1 . . . dπK.\n(A.13)\nMoving παk−1\nk\nout of the product:\n=\nZ\n· · ·\nZ\nΓ(α0)\nQK\nk′=1 Γ(αk′)\nπαk−1+1\nk\nY\nk′̸=k\nπ\nαk′−1\nk′\ndπ1 . . . dπK.\n(A.14)\nFor the next step, we define a new set of Dirichlet parameters with\nβk = αk + 1 and ∀k′ ̸= k : βk′ = αk′. For those new parameters,\nβ0 = P\nk βk = 1 + α0. So by virtue of the Gamma function’s\nproperty that Γ(β0) = Γ(α0 + 1) = α0Γ(α0), replacing all terms in\nthe normalization factor yields\n=\nZ\n· · ·\nZ αk\nα0\nΓ(β0)\nQK\nk′=1 Γ(βk′)\nK\nY\nk′=1\nπ\nβk′−1\nk′\ndπ1 . . . dπK = αk\nα0\n,\n(A.15)\nwhere in the last step we obtain the final result, since the Dirichlet\nwith new parameters βk must nevertheless integrate to 1, and the\nintegrals do not regard αk or α0. For the expectation E[log πk],\nwe first rephrase the Dirichlet distribution in terms of the ex-\nponential families (Kupperman, 1964). The exponential families\nencompass many commonly-used distributions, such as the normal,\nexponential, Beta or Poisson, which all follow the form\np(x; η) = h(x) exp\n\u0000ηT u(x) −A(η)\n\u0001\n,\n(A.16)\nwith natural parameters η, sufficient statistic u(x), and log-\npartition function A(η).\nFor the Dirichlet distribution, Winn\n(2004) provides the sufficient statistic as u(π) = [log π1, . . . , πK]T\nand the log-partition function\nA(α) =\nK\nX\nk=1\nlog Γ(αk) −log Γ(α0).\n(A.17)\nBy Mao (2019), we also find that by the moment-generating\nfunction that for the sufficient statistic, its expectation can be\nderived by\nA.3 entropy of the dirichlet distribution\n279\nE[u(x)k] = ∂A(η)\n∂ηk\n.\n(A.18)\nTherefore, we can evaluate the expected value of log πk (i.e. the\nsufficient statistic) by inserting the definition of the log-partition\nfunction in Equation (A.17) into Equation (A.18):\nE[log πk] =\n∂\n∂αk\nK\nX\nk=1\nlog Γ(αk) −log Γ(α0) = ψ(αk) −ψ(α0),\n(A.19)\nwhich corresponds precisely to the definition of the digamma func-\ntion as ψ(x) =\nd\ndx log Γ(x).\nA.3\nEntropy of the Dirichlet Distribution\nThe following derivation for the entropy of the Dirichlet which\nappears in Section 2.2.3 is adapted from Lin (2016), with the result\nstated in Charpentier et al. (2020) as well.\nH[p(π | α)] = −E[log p(π | α)]\n(A.20)\n= −E\nh\nlog\n\u0010\n1\nB(α)\nK\nY\nk=1\nπαk−1\nk\n\u0011i\n(A.21)\n= −E\nh\n−log B(α) +\nK\nX\nk=1\n(αk −1) log πk\ni\n(A.22)\n= log B(α) −\nK\nX\nk=1\n(αk −1)E[log πk].\n(A.23)\nUsing Equation (A.19):\n= log B(α) −\nK\nX\nk=1\n(αk −1)\n\u0000ψ(αk) −ψ(α0)\n\u0001\n(A.24)\n= log B(α) +\nK\nX\nk=1\n(αk −1)ψ(α0) −\nK\nX\nk=1\n(αk −1)ψ(αk)\n(A.25)\n= log B(α) + (α0 −K)ψ(α0) −\nK\nX\nk=1\n(αk −1)ψ(αk).\n(A.26)\nA.4 expected entropy of the dirichlet distribution\n280\nA.4\nExpected Entropy of the Dirichlet\nDistribution\nThe following derivation for the expected entropy of the Dirichlet\nwhich appears in Section 2.2.3 is adapted from Malinin and Gales\n(2018) appendix section C.4. In the following, we assume that\n∀k ∈K : πk > 0:\nEp(π|x, ˆθ)\nh\nH\n\u0002\nP(y | π)\n\u0003i\n=\nZ\np(π | x, ˆθ)\n\u0010\n−\nK\nX\nk=1\nπk log πk\n\u0011\ndπ\n(A.27)\n= −\nK\nX\nk=1\nZ\np(π | x, ˆθ)\n\u0000πk log πk\n\u0001\ndπ.\n(A.28)\nInserting the definition of p(π| x, ˆθ) ≈p(π | x, D):\n= −\nK\nX\nk=1\n \nΓ(α0)\nQK\nk′=1 Γ(αk′)\nZ\nπk log πk\nK\nY\nk′=1\nπ\nαk′−1\nk′\ndπ\n!\n.\n(A.29)\nSingling out the factor πk:\n= −\nK\nX\nk=1\n \nΓ(α0)\nΓ(αk) Q\nk′̸=k Γ(αk′)παk−1\nk\nZ\nπk log πk\nY\nk′̸=k\nπ\nαk′−1\nk′\ndπ\n!\n.\n(A.30)\nAdjusting the normalizing constant (this is the same trick used in\nAppendix A.2):\n= −\nK\nX\nk=1\n \nαk\nα0\nZ\nΓ(α0 + 1)\nΓ(αk + 1) Q\nk′̸=k Γ(αk′)παk−1\nk\nlog πk\nY\nk′̸=k\nπ\nαk′−1\nk′\ndπ\n!\n.\n(A.31)\nUsing the identity E[log πk] = ψ(αk) −ψ(α0) (Equation (A.19)).\nSince the expectation here is w.r.t. to a Dirichlet with concentration\nparameters αk + 1, we obtain\n= −\nK\nX\nk=1\nαk\nα0\n\u0012\nψ(αk + 1) −ψ(α0 + 1)\n\u0013\n.\n(A.32)\nA.5 kullback-leibler divergence between two dirichlets\n281\nA.5\nKullback-Leibler Divergence between two\nDirichlets\nThe following result appearing in Section 2.2.3 is presented using\nan adapted derivation by Lin (2016) and appears in Chen et al.\n(2018) and Joo et al. (2020) as a starting point for their variational\nobjective. In the following we use Dir(π; α) to denote distribu-\ntion to be optimized, and Dir(π; γ) for the reference or target\ndistribution.\nKL\n\u0002\np(π | α)\n\f\f\f\f p(π | γ)\n\u0003\n= E\nh\nlog p(π | α)\np(π | γ)\ni\n= E\n\u0002\nlog p(π | α)\n\u0003\n−E\n\u0002\nlog p(π | γ)\n\u0003\n(A.33)\n= E\nh\n−log B(α) +\nK\nX\nk=1\n(αk −1) log πk\ni\n−E\nh\n−log B(γ) +\nK\nX\nk=1\n(γk −1) log πk\ni\n.\n(A.34)\nDistributing and pulling out B(α) and B(γ) out of the expectation\n(they don’t depend on π):\n= −log B(γ)\nB(α) + E\nh\nK\nX\nk=1\n(αk −1) log πk −(γk −1) log πk\ni\n(A.35)\n= −log B(γ)\nB(α) + E\nh\nK\nX\nk=1\n(αk −γk) log πk\ni\n.\n(A.36)\nMoving the expectation inward and using the identity E[πk] =\nψ(αk) −ψ(α0) from Appendix A.2:\n= −log B(γ)\nB(α) +\nK\nX\nk=1\n(αk −γk)\n\u0000ψ(αk) −ψ(α0)\n\u0001\n.\n(A.37)\nThe KL divergence is also used by some works as regularizer by\npenalizing the distance to a uniform Dirichlet with γ = 1 (Sensoy\net al., 2018). In this case, the result above can be derived to be\nKL\n\u0002\np(π | α)\n\f\f\f\f p(π | 1)\n\u0003\n= log Γ(K)\nB(α) +\nK\nX\nk=1\n(αk−1)\n\u0000ψ(αk)−ψ(α0)\n\u0001\n,\n(A.38)\nwhere the log Γ(K) term can also be omitted for optimization\npurposes, since it does not depend on α.\nA.6 mutual information for dirichlet networks\n282\nA.6\nMutual Information for Dirichlet Networks\nAs stated in Section 2.2.3, mutual information is a measure of\ndistributional uncertainty in Dirichlet networks.\nTo derive its\nclosed-form expression, we start from Equation (2.73):\nI\nh\ny, π\n\f\f\f x, D\ni\n= H\nh\nEp(π|x,D)\n\u0002\nP(y | π)\n\u0003i\n−Ep(π|x,D)\nh\nH\n\u0002\nP(y | π)\n\u0003i\n.\n(A.39)\nGiven that E[πk] = αk\nα0 (Appendix A.2) and assuming that point\nestimate p(π | x, D) ≈p(π | x, ˆθ) is sufficient (Malinin and\nGales, 2018), we can identify the first term as the Shannon entropy\n−PK\nk=1 πk log πk = −PK\nk=1\nαk\nα0 log αk\nα0 . Furthermore, the second\npart we already derived in Appendix A.4, and thus we obtain:\n= −\nK\nX\nk=1\nαk\nα0\nlog αk\nα0\n+\nK\nX\nk=1\nαk\nα0\n\u0010\nψ(αk + 1) −ψ(α0 + 1)\n\u0011\n(A.40)\n= −\nK\nX\nk=1\nαk\nα0\n\u0010\nlog αk\nα0\n−ψ(αk + 1) + ψ(α0 + 1)\n\u0011\n.\n(A.41)\nA.7\nConnection between Softmax and Sigmoid\nIn this section we briefly outline the connection between the softmax\nand the sigmoid function, in order to show the applicability of\nresults in Section 4.1 to both binary and multi-class classification\nproblems. This connection was originally shown in Bridle (1990).\nLet the sigmoid function be defined as\nσ(x) =\nexp(x)\n1 + exp(x),\n(A.42)\nand softmax according to the definition in Equation (0.2). The\noutput of fθ in a multi-class classification problem with K classes\ncorresponds to a K-dimensional column vector that is based on\nan affine transformation of the network’s last intermediate hidden\nrepresentation xL, such that fθ(x) = WL xL.85 Correspondingly,\nthe output of fθ for a single class c can be written as the dot product\nbetween xL and the corresponding row vector of WL denoted as\nw(c)\nL , such that fθ(x)k ≡w(k)T\nL\nxL. For a classification problem\n85The bias term bL was omitted here for clarity.\nA.8 construction of polytopal regions\n283\nwith K = 2 classes, we can now rewrite the softmax probabilities\nin the following way:86\nPθ(y = 1 | x) =\nexp(w(1)\nL\nT xL)\nexp(w(0)\nL\nT xL) + exp(w(1)\nL\nT xL)\n.\n(A.43)\nSubtracting a constant from the weight term inside the expo-\nnential function does not change the output of the softmax function.\nUsing this property, we can show the sigmoid function to be a\nspecial case of the softmax for binary classification:\nPθ(y = 1 | x) =\nexp((w(1)\nL −w(0)\nL )T xL)\nexp((w(0)\nL −w(0)\nL )T xL) + exp((w(1)\nL −w(0)\nL )T xL)\n(A.44)\n=\nexp((w(1)\nL −w(0)\nL )T xL)\n1 + exp((w(1)\nL −w(0)\nL )T xL\n=\nexp(w∗\nL\nT xL)\n1 + exp(w∗\nL\nT xL),\n(A.45)\nwhere w∗\nL = w(1)\nL −w(0)\nL corresponds to the new parameter vector\nwhich is used to parametrize a single output unit for a network in\nthe binary classification setting.\nA.8\nConstruction of Polytopal Regions\nIn this section, we reiterate the reasoning by Hein et al. (2019)\nbehind the construction the polytopal regions mentioned in Sec-\ntion 4.1.3.\nFor this purpose, the authors define an additional\ndiagonal matrix ∆l(x) per layer l:\n∆l(x) =\n\n\nsign(f l\nθ(x)1)\n· · ·\n0\n...\n...\n...\n0\n· · ·\nsign(f l\nθ(x)nl)\n\n.\n(A.46)\nTogether with the linearization of the network at x explained\nin Equation (4.13), this is used to define a set of half-spaces for\nevery neuron in the network:\nHl,i(x) =\n\b\nz ∈Rd \f\f ∆l(x)\n\u0000Vl(x)i z + al(x)i\n\u0001\n≥0\n\t\n.\n(A.47)\nHere, Vl(x)i and bl(x)i denote the parts of the affine transfor-\nmation obtained for the i-th neuron of the l-th layer, so the i-th row\n86The following argument holds without loss of generality for Pθ(y = 0 | x).\nA.9 proof of Proposition 1\n284\nFigure A.1: Illustration taken from the work of Gao and Pavel (2017),\nillustrating the interplay of softmax probabilities between components\nfor K = 2 in R2.\nvector in Vl(x) and the i-th scalar in bl(x), respectively. Finally,\nthe polytope Q containing x is obtained by taking the intersection\nof all half-spaces induced by every neuron in the network:\nQ(x) =\n\\\nl∈1,...,L\n\\\ni∈1,...,nl\nHl,i(x).\n(A.48)\nA.9\nProof of Proposition 1\nThis section provides the proof of Proposition 1 in Section 4.1.3.\nWe proceed to analyze the behavior of gradients in the limit via\ntwo more lemmas; First, we establish the saturating property of\nthe softmax in Lemma 5, i.e. the model doesn’t change its decision\nanymore in the limit.\nLemma 5. Let k, k′ ∈[K] be two arbitrary classes. It then holds\nfor their corresponding output components (logits) that\nlim\nfθ(x)k→±∞\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k = 0.\n(A.49)\nProof. Here, we first begin by evaluating the derivative of one\ncomponent of the function w.r.t. to an arbitrary component:\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k =\n∂\n∂fθ(x)k′\nexp(fθ(x)k)\nP\nk′′∈[K] exp(fθ(x)k′′)\n(A.50)\n= 1\n\u0000k = k′\u0001\nexp(fθ(x)k)\nP\nk′′∈[K] exp(fθ(x)k′′) −exp(fθ(x)k) exp(fθ(x)k′)\n\u0000 P\nk′′∈[K] exp(fθ(x)k′′)\n\u00012.\n(A.51)\nA.9 proof of Proposition 1\n285\nThis implies that\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k =\n\n\n\n\n\n\n\n\n\n\n\n−\nexp(2fθ(x)k)\n\u0000 P\nk′′∈[K] exp(fθ(x)k′′)\n\u00012 +\nexp(fθ(x)k)\nP\nk′′∈[K] exp(fθ(x)k′′)\nif k = k′\n−exp(fθ(x)k + fθ(x)k′)\n\u0000 P\nk′′∈[K] exp(fθ(x)k′′)\n\u00012\nif k ̸= k′\n(A.52)\nor more compactly:\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k = ¯σ(fθ(x))k\n\u00001\n\u0000k = k′\u0001\n−¯σ(fθ(x))k′\u0001\n.\nBased on Equation (A.52), we can now investigate the asymp-\ntotic behavior for fθ(x)k →∞more easily, starting with the k = k′\ncase:\nlim\nfθ(x)k→∞\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k\n=\nlim\nfθ(x)k→∞−\nexp(fθ(x)k)\nP\nk′′∈[K] exp(fθ(x)k′′)\nexp(fθ(x)k)\nP\nk′′∈[K] exp(fθ(x)k′′)\n|\n{z\n}\n-1\n+\nlim\nfθ(x)k→∞\nexp(fθ(x)k)\nP\nk′′∈[K] exp(fθ(x)k′′)\n|\n{z\n}\n1\n= 0.\n(A.53)\nWith the numerator and denominator being dominated by the\nexponentiated fθ(x)k in Equation (A.53), the first term will tend to\n−1, while the second term will tend to 1, resulting in a derivative\nof 0. The case k ̸= k′ can be analyzed the following way:\nlim\nfθ(x)k→∞\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k =\nlim\nfθ(x)k→∞\n\u0012\n−\nexp(fθ(x)k)\nP\nk′′∈[K] exp(fθ(x)k′′)\n\u0013\n|\n{z\n}\n−1\n\u0012\nexp(fθ(x)k′)\nP\nk′′∈[K] exp(fθ(x)k′′)\n\u0013\n|\n{z\n}\n0\n= 0.\n(A.54)\nAgain, we factorize the fraction in Equation (A.54) into the\nproduct of two softmax functions, one for component k, one for k′.\nThe first factor will again tend to −1 as in the other case, however\nthe second will approach 0, as only the sum in the denominator\nwill approach infinity. As the limit of a product is the products\nof its limits, this lets the whole expression approach 0 in the limit.\nWhen fθ(x)k →−∞, both cases approach 0 due to the exponential\nfunction, which proves the lemma.\nA.9 proof of Proposition 1\n286\nHow the interplay between different softmax components pro-\nduces zero gradients in the limit is illustrated in Figure A.1. In\nLemma 6, we compare the rate of growth of different components\nof Pθ. We show that for the decomposed function Pθ, the rate at\nwhich the softmax function converges to its output distribution in\nthe limit outpaces the change in the underlying logits w.r.t. the\nnetwork input.\nLemma 6. Suppose that fθ is a ReLU-network. Let x′ ∈RD,\nsuppose α is a scaling vector and that the associated PUP P(x′, d)\nhas a corresponding matrix V with no zero entries. Then it holds\nfor all k′ ∈[K] that\nlim\nαd→∞\n\u0010\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k\n\u0011−1\f\f\f\nx=α ◦x′−\n\u0010 ∂\n∂xd\nfθ(x)k′\n\u0011\f\f\f\nx=α ◦x′ = ∞.\n(A.55)\nProof. We evaluate the first term of Equation (A.55) to show that\nit grows exponentially in the limit. By Lemma 2, we know that\nin the limit αd →∞the vector α ◦x′ will remain within P(x′, d).\nSince the matrix associated with this PUP has no zero entries, we\nknow by Lemma 1 that the gradient of fθ(x)k on dimension d is\neither always positive or negative, hence fθ(x)k →±∞. Given\nLemma 5 describing the asymptotic behavior in the limit, it follows\nthat\nlim\nfθ(x)k→±∞\n\u0010\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k\n\u0011−1\n= ∞,\n(A.56)\nwhere we can see that the result is a symmetrical function displaying\nexponential growth in the limit of fθ(x)k →±∞. We now show\nthat because we assumed fθ to be a neural network consisting of L\naffine transformations with ReLU activation functions, the output\nof the final layer is only going to be a linear combination of its\ninputs.87 This can be proven by induction. Let us first look at\nthe base case L = 1. In the rest of this proof, we denote xl as the\ninput to layer l, with x1 ≡x, and Wl, bl the corresponding layer\nparameters. al signifies the result of the affine transformation that\nis then fed into the activation function.\nfθ(x) = ϕ(a1) = ϕ(W1 x1 + b1)\n∂fθ(x)\n∂x1\n= ∂ϕ(a1)\n∂a1\n∂a1\n∂x1\n= 1(x1 > 0)T W1\n∂fθ(x)\n∂x1d\n= 1\n\u0000xd > 0\n\u0001\nw1d,\n(A.57)\nwhere 1(x1 > 0) = [1\n\u0000x11 > 0\n\u0001\n, . . . , 1\n\u0000x1d > 0\n\u0001\n]T, w1d denoting\nthe d-th column of W1. This is a linear function, which proves the\n87Here we make the argument for the whole function fθ : RD →RK, but the\nconclusions also applies to every output component of the function fθ(x)k.\nA.10 proof of Proposition 2\n287\nbase case. Let now ∂xl\n∂x1 denote the partial derivative of the input\nto the l-th layer w.r.t. to the input and suppose that it is linear by\nthe inductive hypothesis. Augmenting the corresponding network\nby another linear adds another term akin to the second expression\nin Equation (A.57) to the chain of partial derivatives:\n∂xl+1\n∂x1\n= ∂xl+1\n∂xl\n∂xl\n∂x1\n,\n(A.58)\nwhich is also a linear function, proving the induction step. Because\nwe know that both terms of the product in Equation (A.58) are\nlinear, the second term of the Equation (A.55) is as well. Together\nwith the previous insight that the first term is exponential, this\nimplies that it will outgrow the second in the limit, creating an\ninfinitely-wide gap between them and thereby proving the lemma.\nEquipped with the results of Lemmas 5 and 6, we can finally\nprove Proposition 1:\nProof. We show that one scalar factor contained in the factorization\nof the gradient ∇xPθ(y = k | x) tends to zero under the given\nassumptions, having the whole gradient become the zero vector in\nthe limit. We begin by again factorizing the gradient ∇xPθ(y =\nk | x) using the multivariate chain rule:\n∇xPθ(y = k | x) =\nK\nX\nk′=1\n∂\n∂fθ(x)k′ ¯σ(fθ(x))k∇xfθ(x)k′.\n(A.59)\nBy Lemmas 1 and 2 we know that fθ is a component-wise strictly\nmonotonic function on P(x′, d), which implies for the limit of\nαd →∞that ∀k ∈[K] : fθ(x)k →±∞. Then, Lemma 5 implies\nthat the first factor of every part in the sum of Equation (A.59)\nwill tend to zero in the limit. Lemma 6 ensures that the first factor\napproximates zero quicker than every component of the gradient\n∇xfθ(x)k′ potentially approaching infinity, causing the product to\nresult in the zero vector. As this results in a sum over K zero\nvectors in the limit, this proves the lemma.\nA.10\nProof of Proposition 2\nThis section contains the proof of Proposition 2 in Section 4.1.3.\nProof. We start by rewriting the softmax probability for the k-th\nlogit:\n¯σ(fθ(x))k =\nexp(fθ(x)k)\nP\nk′∈[K] exp(fθ(x)k′) = 1 −\nP\nk′′∈[K]\\{k} exp(fθ(x)k′′)\nP\nk′∈[K] exp(fθ(x)k′)\n.\n(A.60)\nA.11 proof of Lemma 4\n288\nBy Lemmas 1 and 2, we have shown that fθ is a component-wise\nstrictly monotonic function on P(x′, d), so we know that for all\nk′ ∈[K] :\nfθ(x)k′ →±∞as αd →∞. We now treat the two\nlimits ±∞in order. Because of the assumption that d-column\nof V has no duplicate entries, this implies that there must be a\nk ∈[K] s.t. ∀k′ ̸= k : vkd > vk′d. Thus, in the limit of fθ(x)k →∞,\nthe sum in the denominator of the fraction including the logit of\nk will tend to infinity faster than the the sum in the numerator\nnot including k’s logit, and thus the fraction itself will tend to 0,\nproving this case. In the case of fθ(x)k →−∞, the numerator of\nthe fraction will tend to 0 faster than the denominator, having the\nfraction approach 0 in the limit as well, proving the second case\nand therefore the lemma.\nA.11\nProof of Lemma 4\nThis section contains the proof of Lemma 4 in Section 4.1.4.\nProof.\nlim\nα→∞\n\f\f\f\f∇xEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\f\f\nx=α ◦x′\n\f\f\f\f\n2\n(A.61)\n= lim\nα→∞\n\f\f\f\fEp(θ|D)\n\u0002\n∇xPθ(y = k | x)\n\u0003\f\f\f\f\nx=α ◦x′\n\f\f\f\f\n2\n(A.62)\n≤lim\nα→∞Ep(θ|D)\n\u0002 \f\f\f\f∇xPθ(y = k | x)\n\f\f\nx=α ◦x′\n\f\f\f\f\n2\n|\n{z\n}\n= 0 (Proposition 1)\n\u0003\n= 0.\n(A.63)\nBecause the last expression is an upper bound to the original\nexpression and the l2 norm is lower-bounded by 0, this proves the\nlemma.\nA.12\nProof of Lemma 7\nThis section contains the proof of Lemma 7 that is part of the\nproof of Theorem 1 in Section 4.1.4.\nLemma 7. (Asymptotic behavior with softmax variance) Suppose\nthat f (1)\nθ , . . . , f (K)\nθ\nare ReLU networks. Let x′ ∈RD, suppose α is\na scaling vector and that for all k, the associated PUP P(k)(x′, d)\nhas a corresponding matrix V(k) with no zero entries. It holds that\nlim\nαd→∞\n\f\f\f\n\f\f\f∇x\n1\nK\nK\nX\nk=1\nEp(θ|D)\n\u0002\nPθ(y = k | x)2\u0003\n−Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u00032\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2 = 0.\n(A.64)\nA.13 proof of Lemma 8\n289\nProof.\nlim\nαd→∞\n\f\f\f\n\f\f\f∇x\n1\nK\nK\nX\nk=1\nEp(θ|D)\n\u0002\nPθ(y = k | x)2\u0003\n−Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u00032\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.65)\n= lim\nαd→∞\n\f\f\f\n\f\f\f 1\nK\nK\nX\nk=1\n∇xEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u00032\n−∇xEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u00032\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.66)\nApply triangle inequality ||x + y|| ≤||x|| + ||y|| to sum over all k:\n≤lim\nαd→∞\n1\nK\nK\nX\nk=1\n\f\f\f\f∇xEp(θ|D)\n\u0002\nPθ(y = k | x)2\u0003\n−∇xEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u00032\f\f\nx=α ◦x′\n\f\f\f\f\n2\n(A.67)\nOn the first term use linearity of gradients and apply chain rule,\ndo it in the reverse order on the second term:\n= lim\nαd→∞\n1\nK\nK\nX\nk=1\n\f\f\f\fEp(θ|D)\n\u0002\n2Pθ(y = k | x) ∇xPθ(y = k | x)\n\f\f\nx=α ◦x′\n|\n{z\n}\n= 0 (Proposition 1)\n\u0003\n−2Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\nEp(θ|D)\n\u0002\n∇xPθ(y = k | x)\n\f\f\nx=α ◦x′\n|\n{z\n}\n= 0 (Proposition 1)\n\u0003\f\f\f\f\n2 = 0.\n(A.68)\nWe can see that due to an intermediate result of Proposition 1, i.e.\nthat ∇xPθ(y = k | x) approaches the zero vector in the limit, the\ninnermost gradients tend to zero, bringing the whole expression to\nzero. Because the final is an upper bound to the original expression\nand because the l2 norm has a lower bound of 0, this proves the\nlemma.\nA.13\nProof of Lemma 8\nThis section contains the proof of Lemma 8 that is part of the\nproof of Theorem 1 in Section 4.1.4.\nLemma 8. (Asymptotic behavior for predictive entropy) Suppose\nthat f (1)\nθ , . . . , f (K)\nθ\nare ReLU networks. Let x′ ∈RD, suppose α is\na scaling vector and that for all k, the associated PUP P(k)(x′, d)\nhas a corresponding matrix V(k) with no zero entries. It holds that\nlim\nαd→∞\n\f\f\f\n\f\f\f∇xH\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2 = 0.\n(A.69)\nA.14 proof of Lemma 9\n290\nProof.\nlim\nαd→∞\n\f\f\f\n\f\f\f∇xH\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.70)\n= lim\nαd→∞\n\f\f\f\n\f\f\f∇x\n\u0010\nK\nX\nk=1\nEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\n· log Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\u0011\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.71)\n= lim\nαd→∞\n\f\f\f\n\f\f\f\nK\nX\nk=1\n∇x\n\u0010\nEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\n· log Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\u0011\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.72)\n= lim\nαd→∞\n\f\f\f\n\f\f\f\nK\nX\nk=1\n∇xEp(θ|D)\n\u0002\npθ(y = c| x)\n\u0003\n+ ∇x\n\u0010\nEp(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\u0011\nlog Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.73)\n= lim\nαd→∞\n\f\f\f\n\f\f\f\nK\nX\nk=1\n∇xEp(θ|D)\n\u0002\npθ(y = k | x)\n\u0003\n·\n\u0010\n1 + log Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\u0011\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.74)\nApply triangle inequality to sum over all k:\n≤lim\nαd→∞\nK\nX\nk=1\n\f\f\f\f∇xEp(θ|D)\n\u0002\npθ(y = k | x)\n\u0003\n·\n\u00001 + log Ep(θ|D)\n\u0002\nPθ(y = k | x)\n\u0003\u0001\f\f\nx=α ◦x′\n\f\f\f\f\n2\n(A.75)\n= lim\nαd→∞\nK\nX\nk=1\n\u0010\n1 + log Ep(θ|D)\n\u0002\npθ(y = k | x)\n\u0003\u0011\n·\n\f\f\f\f∇xEp(θ|D)\n\u0002\npθ(y = k | x)\n\u0003\f\f\nx=α ◦x′\n\f\f\f\f\n2\n|\n{z\n}\n= 0 (Lemma 4)\n= 0.\n(A.76)\nAs the final result is an upper bound to the original expression\nand is lower-bounded by 0 due to the l2 norm, this proves the\nlemma.\nA.14\nProof of Lemma 9\nThis section contains the proof of Lemma 9 that is part of the\nproof of Theorem 1 in Section 4.1.4.\nA.14 proof of Lemma 9\n291\nLemma 9. (Asymptotic behavior for approximate mutual informa-\ntion) Suppose that f (1)\nθ , . . . , f (K)\nθ\nare ReLU networks. Let x′ ∈RD,\nsuppose α is a scaling vector and that for all k, the associated PUP\nP(k)(x′, d) has a corresponding matrix V(k) with no zero entries. It\nholds that\nlim\nαd→∞\n\f\f\f\n\f\f\f∇x\n\u0010\nH\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\n−Ep(θ|D)\nh\nH\n\u0002\nPθ(y | x)\n\u0003i\u0011\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2 = 0.\n(A.77)\nProof.\nlim\nαd→∞\n\f\f\f\n\f\f\f∇x\n\u0010\nH\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\n−Ep(θ|D)\nh\nH\n\u0002\nPθ(y | x)\n\u0003i\u0011\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.78)\n= lim\nαd→∞\n\f\f\f\n\f\f\f\n\u0010\n∇xH\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\n−Ep(θ|D)\nh\n∇xH\n\u0002\nPθ(y | x)\n\u0003i\u0011\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.79)\nApplying chain rule and intermediate result of Proposition 1:\n= lim\nαd→∞\n\f\f\f\n\f\f\f∇xH\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\n−Ep(θ|D)\nh\nK\nX\nk=1\n\u00001 + log Pθ(y = k | x)\n\u0001\n∇xPθ(y = k | x)\n|\n{z\n}\n= 0 (Proposition 1)\ni\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n(A.80)\n= lim\nαd→∞\n\f\f\f\n\f\f\f∇xH\nh\nEp(θ|D)\n\u0002\nPθ(y | x)\n\u0003i\f\f\f\nx=α ◦x′\n\f\f\f\n\f\f\f\n2\n|\n{z\n}\n(Lemma 8)\n= 0.\n(A.81)\nAs the final result is an upper bound to the original expression and\nthe l2 norm provides a lower bound of 0, this proves the lemma.\nB\n|\nExperimental Appendix\n“Machine learning has become alchemy.”\n—Ali Rahimi in his NIPS 2017 Test of Time Award Talk.\nThesis\nAppendix\nSection 3.2.2\nAppendix B.1\nSection 4.1.5\nAppendix C.4.2\nSection 4.2.2\nAppendices B.3 and B.4\nSection 4.2.6\nAppendix B.5\nSection 4.2.7\nAppendix B.6\nSection 5.4.1\nAppendices B.7 and B.8\nSection 6.2\nAppendix B.9\nSection 6.2.2\nAppendix B.10\nTable B.1: Correspondences between sections of the empirical appendix\nand thesis chapters.\nThis appendix involves a collection of additional empirical\nresults stemming from the experiments in the different chapters.\nAn overview over the contents and their correspondence to thesis\nchapters is given in Table B.1. For more details regarding the\nreproducibility of experiments (hyperparameters, experimental\nsettings etc.) refer to Appendix C.\nB.1\nAdditional Error Rate Experiments\nWe use this section to further shed light on the results in Figure 3.5.\nTest Score Distributions.\nInstead of showing the Type I error\nrates based on thresholded test results, we instead plot the distri-\nbutions over test scores in Figure B.1. We can observe that the\nlower ends of the interquartile range of εmin distributions are either\nthe same or higher than the ones for p-values (they do not need\nto be centered around 0.5 since εmin is an upper bound to εW2),\nexplaining the lower Type I error rate.\n292\nB.1 additional error rate experiments\n293\n(a) Dists. for normal samples.\n(b) Dists. for normal mixture samples.\n(c) Dists. for Laplace samples.\n(d) Dists. for Rayleigh samples.\nFigure B.1: Comparing test score distributions for different tests and\ndistributions as a function of sample size.\nType II Error Rate Experiments.\nWe furthermore test the\nType II error rates on samples from different distributions in\nFigure B.2, sampling the score samples 500 times for ASO and\n1000 times for the other tests from N(0.5, 1.52) and N(0, 1.52),88\nrespectively, for a p-value threshold of 0.05 and εmin threshold of 0.2.\nWe see that the Type II error rate decreases with increasing sample\nsize (Figures B.2a and B.2c), but is less sensitive for increasing mean\ndifference than other tests (Figures B.2b and B.2d). Generally,\nwe can observe the behavior to be very similar to Student’s-t and\nMann-Whitney U test.\nError Rates by Rejection Threshold.\nLastly, we report the\nType I and II error rates on the tested distributions using different\nType I / II error rates. In Tables B.2, B.5, B.8 and B.9, we see\nthat ASO achieves lower error rates than other tests in almost all\nscenarios when faced with the fame threshold. Naturally, these\nthresholds cannot be interpreted the same for ASO and the other\nsignificance tests. Nevertheless, we can see that a threshold of\nτ = 0.2 seems to roughly correspond to a p-value threshold of 0.05\nin terms of Type I error rate. Type II error rates are given in\n88For the normal mixture, only the second mixture component is varied.\nB.2 synthetic data experiments\n294\n(a) Type II error as a function of sam-\nple size.\n(b) Type II error rate as a function\nof mean difference.\n(c) Type II error as a function of sam-\nple size.\n(d) Type II error rate as a function\nof mean difference.\nFigure B.2: Measuring the Type II error rate of the considered tests on\nnormal and normal mixture distributions as a function of sample size\nFigures B.2a and B.2c and mean differences Figures B.2b and B.2d.\nTables B.3, B.4, B.6 and B.7. Here the difference between ASO\nand the other tests is not quite as pronounced, however, it always\nincurs higher error rates.\nB.2\nSynthetic Data Experiments\nThis sections provides more details on the results in Section 4.1.5.\nAll of the plots produced can be found in Figures B.3 and B.4,\nwhere uncertainty values where plotted for different ranges depend-\ning on the metric (variance: 0-0.25; (negative) entropy: 0-1; mutual\ninformation: 4 −5; (1−) max. prob: 0 −0.5), with deep purple\nsignifying high uncertainty and white signifying low uncertainty /\nhigh certainty. We can see in Figure B.3 that maximum softmax\nprobability and predictive entropy behave quite similarly, forming\na tube-like region of high uncertainty along what appear to be the\ndecision boundary. In both cases, the region appears to be sharper\nin the case of maximum softmax probability (right column) and\nalso more defined after additional temperature scaling (bottom\nrow). For all models and metrics, we see that the gradient magni-\ntude decreases and approaches zero away from the training data\nB.2 synthetic data experiments\n295\nSample Size\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n5\n.05\n.020\n.048\n.085\n.029\n.029\n.056\n.10\n.034\n.093\n.149\n.079\n.088\n.085\n.20\n.006\n.212\n.241\n.197\n.160\n.159\n.30\n.094\n.299\n.322\n.286\n.236\n.284\n.40\n.146\n.396\n.403\n.370\n.315\n.348\n.50\n.216\n.483\n.483\n.468\n.490\n.498\n10\n.05\n.004\n.055\n.077\n.058\n.051\n.048\n.10\n.014\n.103\n.130\n.110\n.113\n.100\n.20\n.038\n.196\n.215\n.201\n.192\n.194\n.30\n.084\n.282\n.300\n.285\n.261\n.272\n.40\n.138\n.394\n.398\n.395\n.387\n.378\n.50\n.204\n.409\n.486\n.491\n.499\n.479\n15\n.05\n.002\n.059\n.072\n.057\n.051\n.052\n.10\n.014\n.106\n.123\n.104\n.095\n.113\n.20\n.042\n.198\n.215\n.199\n.186\n.196\n.30\n.080\n.303\n.309\n.303\n.295\n.304\n.40\n.136\n.395\n.400\n.392\n.371\n.368\n.50\n.190\n.482\n.485\n.479\n.470\n.468\n20\n.05\n.004\n.046\n.058\n.047\n.043\n.047\n.10\n.006\n.095\n.105\n.093\n.085\n.092\n.20\n.028\n.181\n.196\n.177\n.171\n.183\n.30\n.074\n.280\n.290\n.289\n.284\n.273\n.40\n.120\n.384\n.389\n.381\n.372\n.394\n.50\n.170\n.479\n.478\n.473\n.477\n.481\nTable B.2: Type I error rates for samples drawn from a normal distribu-\ntion as a function of sample size and different rejection thresholds.\n(yellow / green plots), except for the cases discussed in Section 4.1.5.\nIn the next figure, Figure B.4, we observe the uncertainty\nsurfaces for models using multiple network instances. For the\nremaining models it is interesting to see that class variance (left\ncolumn) didn’t seem to produce significantly different values across\nthe feature space except for the anchored ensemble. For predictive\nentropy (central column), we can see a similar behavior compared\nto the single-instances models. Interestingly, the “fuzziness” of the\nhigh-uncertainty region increases with the ensemble and becomes\nincreasing large with its anchored variant. Nevertheless, regions\nwith static levels of certainty still exist in this case. For the mutual\ninformation plots (right column), epistemic uncertainty is lowest\naround the training data, where the model is best specified, which\ncreates another tube-like region of high confidence even where\nthere is no training data, an effect that is reduced with the neural\nensemble and almost completely solved by the anchored ensemble.\nFor all metrics, we see a magnitude close to zero for the uncertainty\ngradient away from the training data, except for the decision\nboundaries, as discussed in Section 4.1.5.\nB.3 sub-sampling of training sets\n296\nSample Size\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n5\n.05\n.942\n.883\n.796\n.918\n.925\n.875\n.10\n.916\n.786\n.714\n.802\n.792\n.819\n.20\n.870\n.623\n.585\n.649\n.691\n.694\n.30\n.792\n.512\n.480\n.521\n.597\n.539\n.40\n.714\n.399\n.309\n.421\n.498\n.470\n.50\n.650\n.302\n.315\n.318\n.387\n.391\n10\n.05\n.978\n.836\n.791\n.853\n.864\n.840\n.10\n.950\n.703\n.695\n.737\n.743\n.741\n.20\n.868\n.580\n.551\n.58\n.595\n.576\n.30\n.802\n.428\n.41\n.429\n.462\n.453\n.40\n.708\n.330\n.328\n.327\n.347\n.329\n.50\n.604\n.223\n.223\n.229\n.272\n.251\n15\n.05\n.984\n.769\n.734\n.781\n.788\n.787\n.10\n.905\n.643\n.615\n.646\n.672\n.639\n.20\n.840\n.470\n.455\n.480\n.493\n.481\n.30\n.716\n.348\n.340\n.350\n.355\n.365\n.40\n.610\n.244\n.245\n.246\n.276\n.261\n.50\n.486\n.177\n.176\n.175\n.185\n.192\n20\n.05\n.976\n.732\n.709\n.736\n.750\n.747\n.10\n.946\n.601\n.586\n.601\n.614\n.610\n.20\n.848\n.406\n.396\n.410\n.421\n.410\n.30\n.704\n.277\n.268\n.272\n.299\n.289\n.40\n.508\n.200\n.201\n.198\n.221\n.206\n.50\n.444\n.144\n.144\n.147\n.156\n.152\nTable B.3: Type II error rates for normal samples as a function of sample\nsize and different rejection thresholds.\nB.3\nSub-Sampling of Training Sets\nSince we sub-sample some of the data splits in Table 4.1, this\nbears the dangers of producing unnatural samples of text. For that\nreason, we use this appendix to describe the sampling strategies\nused for the methodology in Section 4.2.2 in more detail.\nSub-Sampling Procedure.\nThe procedure for sub-sampling\ntext is that sequences are first placed into buckets of the same\nlabel, then into sub-buckets of the same length. Then, the sampling\nprocedure consists of first drawing a label based on the observed\nlabel frequencies, after which the draw of sequence length, propor-\ntional to the frequency of this length inside the bucket, determines\nthe final bucket from which a sequence is again drawn uniformly.\nLastly, the process for token classification involves the grouping\ninto sequences by length at the highest level. Inside a bucket, a\nsequence is not drawn uniformly, but with a probability according\nto the alignment of the sequence’s labels with the overall corpus\nlabel distribution. This alignment is calculated for each sequence\nby evaluating the expected log-probability of the sequence’s la-\nB.3 sub-sampling of training sets\n297\nDifference\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n.25\n.05\n.984\n.925\n.857\n.941\n.945\n.930\n.10\n.954\n.846\n.781\n.859\n.844\n.881\n.20\n.914\n.705\n.659\n.721\n.761\n.768\n.30\n.872\n.585\n.554\n.606\n.680\n.622\n.40\n.800\n.482\n.462\n.489\n.594\n.548\n.50\n.714\n.381\n.387\n.394\n.480\n.465\n.50\n.05\n.966\n.888\n.805\n.918\n.920\n.883\n.10\n.932\n.784\n.700\n.811\n.794\n.830\n.20\n.870\n.616\n.570\n.652\n.696\n.698\n.30\n.812\n.500\n.477\n.523\n.602\n.535\n.40\n.722\n.406\n.397\n.426\n.505\n.466\n.50\n.606\n.313\n.315\n.326\n.411\n.401\n.75\n.05\n.934\n.822\n.707\n.883\n.885\n.822\n.10\n.896\n.699\n.610\n.725\n.710\n.764\n.20\n.798\n.514\n.469\n.561\n.599\n.607\n.30\n.702\n.407\n.370\n.421\n.515\n.455\n.40\n.590\n.308\n.300\n.325\n.406\n.375\n.50\n.482\n.223\n.222\n.237\n.303\n.295\n1.00\n.05\n.870\n.739\n.609\n.850\n.850\n.743\n.10\n.796\n.585\n.488\n.678\n.655\n.659\n.20\n.712\n.386\n.327\n.449\n.497\n.487\n.30\n.580\n.257\n.232\n.289\n.388\n.307\n.40\n.504\n.178\n.170\n.194\n.278\n.229\n.50\n.384\n.115\n.115\n.128\n.189\n.176\nTable B.4: Type II error rates for normal samples as a function of mean\ndifference and different rejection thresholds.\nbel distribution w.r.t. to the label distribution of the corpus (i.e.,\nthe cross-entropy). The scores for all same-length sequences in a\nbucket are then normalized into a [0, 1] interval in order to enable\nsampling, which is similar to the two-stage procedure used in the\nsequence classification case.\nValidation of Sub-Sampled Training Sets.\nWe take multiple\nsteps to validate the representativeness of our sub-sampled data\nsplits. First, we plot the distributions of the 50 most frequent types\nin the original corpus in Figure B.5, where we see that distributions\nconverge with increasing sample size. Secondly, we plot sentence\nlength distributions in Figure B.6, where we also see increasing\nalignment with sample size. We plot the class distributions in\nFigure B.7. Lastly, we train an interpolated trigram Kneser-Ney\nlanguage model (Jelinek, 1980; Ney et al., 1994) with uniform\ninterpolation weights trained on the original training set using\nthe SRILM tool (Stolcke, 2002) and sub-word tokens produced by\nthe corresponding Bert tokenizer, sub-sample multiple splits and\ncompare their perplexity scores to those of the original corpus in\nTable B.10. While n-gram perplexities of sub-sampled training\nB.4 selection of ood test sets\n298\nSample Size\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n5\n.05\n.000\n.000\n.012\n.028\n.026\n.003\n.10\n.000\n.013\n.035\n.079\n.085\n.004\n.20\n.000\n.069\n.104\n.179\n.153\n.049\n.30\n.008\n.169\n.213\n.281\n.208\n.160\n.40\n.024\n.338\n.358\n.363\n.305\n.244\n.50\n.058\n.494\n.493\n.483\n.484\n.478\n10\n.05\n.000\n.007\n.018\n.059\n.049\n.011\n.10\n.000\n.031\n.050\n.110\n.109\n.030\n.20\n.004\n.102\n.121\n.205\n.188\n.109\n.30\n.008\n.221\n.229\n.302\n.273\n.211\n.40\n.034\n.347\n.349\n.398\n.379\n.351\n.50\n.070\n.511\n.515\n.506\n.491\n.495\n15\n.05\n.000\n.006\n.007\n.055\n.048\n.004\n.10\n.000\n.022\n.033\n.106\n.097\n.017\n.20\n.002\n.103\n.118\n.194\n.202\n.095\n.30\n.006\n.215\n.220\n.301\n.308\n.208\n.40\n.028\n.356\n.366\n.415\n.404\n.328\n.50\n.082\n.501\n.499\n.496\n.502\n.501\n20\n.05\n.000\n.006\n.007\n.048\n.045\n.005\n.10\n.000\n.019\n.027\n.088\n.085\n.021\n.20\n.000\n.104\n.109\n.200\n.187\n.097\n.30\n.006\n.214\n.218\n.307\n.289\n.221\n.40\n.032\n.363\n.369\n.412\n.390\n.349\n.50\n.082\n.494\n.495\n.492\n.496\n.485\nTable B.5: Type I error rates for normal mixture samples as a function\nof sample size and different rejection thresholds.\nsets do lie over the ones of the original data, they are still upper-\nbounded by the in-distribution test set perplexities. Furthermore,\nthis verification was not aimed to give the most precise results, as\nalso the scoring using an n-gram model can be rather crude. Thus,\nwith all these results, we conclude that our sub-sampling procedure\nproduces sufficiently representative samples of the original data for\nthe different tasks discussed.\nB.4\nSelection of OOD Test Sets\nIn this appendix section, we present additional evidence that the\nOOD test splits shown in Table 4.1 are sufficiently different from the\ntraining data—meaning, out-of-distribution—to enable our chosen\nmethodology. To that end, we re-use similar ideas as described in\nAppendix B.3, but with the opposite goal. In Figure B.9, we plot\nthe distribution of sequence lengths of the training set compared\nwith the OOD test set, with the same done for the most frequent\n25 types in Figure B.10 and class labels in Figure B.8. Lastly, we\nagain use a interpolated Kneser-Ney trigram language model to\ncompute the perplexity of the training compared to the OOD test\nB.5 additional scatter plots\n299\nSample Size\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n5\n.05\n1.000\n.999\n.964\n.892\n.897\n.995\n.10\n1.000\n.962\n.874\n.728\n.697\n.985\n.20\n.994\n.747\n.640\n.474\n.525\n.870\n.30\n.976\n.476\n.422\n.299\n.426\n.579\n.40\n.896\n.252\n.234\n.206\n.326\n.414\n.50\n.748\n.117\n.118\n.122\n.222\n.280\n10\n.05\n1.000\n.908\n.831\n.552\n.635\n.926\n.10\n.996\n.721\n.641\n.354\n.419\n.730\n.20\n.954\n.390\n.354\n.186\n.247\n.407\n.30\n.828\n.191\n.180\n.108\n.156\n.219\n.40\n.642\n.089\n.087\n.068\n.089\n.107\n.50\n.452\n.034\n.031\n.037\n.056\n.052\n15\n.05\n.996\n.829\n.757\n.352\n.441\n.864\n.10\n.990\n.568\n.517\n.213\n.272\n.628\n.20\n.928\n.251\n.234\n.087\n.129\n.298\n.30\n.774\n.099\n.091\n.033\n.058\n.116\n.40\n.498\n.027\n.026\n.019\n.034\n.044\n.50\n.276\n.009\n.010\n.010\n.013\n.014\n20\n.05\n1.000\n.653\n.580\n.204\n.279\n.666\n.10\n.980\n.359\n.333\n.105\n.162\n.392\n.20\n.848\n.107\n.101\n.035\n.064\n.147\n.30\n.586\n.038\n.035\n.013\n.022\n.047\n.40\n.344\n.010\n.010\n.008\n.013\n.017\n.50\n.130\n.003\n.003\n.004\n.006\n.006\nTable B.6: Type II error rates for normal mixture samples as a function\nof sample size and different rejection thresholds.\nset in Table B.10. In all cases, OOD n-gram perplexities lie much\nover the training or sub-sampled data perplexities. Except for\nFinnish, they are also widely different from the test set perplexities.\nIn that exceptional cases, an explanation could be given by the\nhighly agglutinative nature of Finnish, increasing the sparsity of\nthe language despite the subword tokenization.\nB.5\nAdditional Scatter Plots\nThis section provides some additional scatter plots for the exper-\niments in Section 4.2.6. For all plots presented here as well as\nFigure 4.5, some slight jitter sampled from N(0, 0.01) was added to\nx and y-coordinates to increase readability of overlapping points.\nClinc Plus.\nIn Figures B.11a and B.12a, we can see that the\nvariational Bert model actually degrades in performance as the more\ntraining data is added, both on a task and uncertainty dimensions,\nwhile other models stay relatively constant. The same trend can be\ndetected using the sequence-level Kendall’s τ for Clinc Plus. We\nsuspect that the smallest training size of 10k examples does already\nB.5 additional scatter plots\n300\nDiff.\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n.25\n.05\n1.000\n.997\n.988\n.958\n.962\n.998\n.10\n.998\n.988\n.960\n.894\n.882\n.994\n.20\n.996\n.903\n.856\n.754\n.792\n.945\n.30\n.978\n.762\n.727\n.643\n.724\n.814\n.40\n.940\n.594\n.576\n.530\n.621\n.704\n.50\n.886\n.424\n.424\n.444\n.532\n.563\n.50\n.05\n.998\n.999\n.980\n.931\n.932\n.996\n.10\n.998\n.978\n.931\n.820\n.802\n.990\n.20\n.996\n.849\n.775\n.647\n.695\n.905\n.30\n.976\n.659\n.603\n.511\n.611\n.724\n.40\n.928\n.458\n.438\n.407\n.504\n.577\n.50\n.840\n.284\n.287\n.310\n.395\n.449\n.75\n.05\n1.000\n.997\n.966\n.912\n.915\n.993\n.10\n.998\n.966\n.901\n.769\n.746\n.985\n.20\n.994\n.802\n.707\n.553\n.623\n.886\n.30\n.974\n.547\n.497\n.397\n.516\n.651\n.40\n.922\n.355\n.337\n.286\n.407\n.485\n.50\n.824\n.191\n.191\n.198\n.305\n.363\n1.00\n.05\n1.000\n1.000\n.961\n.890\n.894\n.995\n.10\n1.000\n.958\n.868\n.714\n.682\n.989\n.20\n.996\n.715\n.617\n.445\n.505\n.872\n.30\n.962\n.432\n.380\n.291\n.419\n.545\n.40\n.870\n.253\n.235\n.204\n.308\n.408\n.50\n.702\n.120\n.120\n.132\n.208\n.263\nTable B.7: Type II error rates for normal mixture samples as a function\nof mean difference between two of the mixture components and different\nrejection thresholds.\nprovide enough data for models to converge to similar solutions\neven after adding more data, and that the variational Bert alone\nmight be prone to overfitting in this case.\nDan+.\nResults for the Danish dataset are shown in Fig-\nures B.11b and B.12b. It is apparent that LSTM-based models stay\nmostly constant in their predictive performance, with the largest\ngains observed by the LSTM ensemble. We can also observe the\nDDU and variational Bert to increase both in task performance\nand uncertainty quality with increasing training data. Interest-\ningly, we can see for the SNGP Bert that uncertainty estimates\nbecome more indicative of OOD with more training samples, but\nmostly only using predictive entropy and the maximum probability\nscore. This might indicate that in these cases, the model actually\nachieves the desired distance-awareness posed by Liu et al. (2023).\nIn Figure B.14b, we can see a similar behavior of the SNGP-Bert\nand its metrics w.r.t. to the sequence-level correlation. Also, we\nB.5 additional scatter plots\n301\nSample Size\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n5\n.05\n.022\n.053\n.110\n.048\n.046\n.066\n.10\n.038\n.117\n.164\n.106\n.116\n.097\n.20\n.088\n.223\n.261\n.208\n.187\n.169\n.30\n.124\n.319\n.343\n.295\n.234\n.286\n.40\n.154\n.427\n.445\n.398\n.322\n.379\n.50\n.218\n.509\n.510\n.491\n.506\n.508\n10\n.05\n.004\n.059\n.077\n.060\n.046\n.051\n.10\n.012\n.114\n.142\n.111\n.106\n.098\n.20\n.056\n.218\n.236\n.216\n.202\n.199\n.30\n.104\n.314\n.330\n.318\n.290\n.291\n.40\n.164\n.404\n.407\n.398\n.378\n.400\n.50\n.238\n.475\n.475\n.473\n.481\n.486\n15\n.05\n.000\n.052\n.066\n.048\n.048\n.048\n.10\n.012\n.100\n.117\n.103\n.100\n.101\n.20\n.028\n.204\n.220\n.199\n.199\n.187\n.30\n.070\n.311\n.319\n.303\n.296\n.294\n.40\n.120\n.404\n.409\n.402\n.378\n.394\n.50\n.194\n.510\n.514\n.511\n.504\n.519\n20\n.05\n.004\n.044\n.047\n.048\n.057\n.052\n.10\n.010\n.099\n.113\n.104\n.103\n.101\n.20\n.030\n.214\n.232\n.215\n.199\n.202\n.30\n.064\n.312\n.325\n.308\n.297\n.307\n.40\n.138\n.414\n.413\n.415\n.381\n.405\n.50\n.220\n.507\n.505\n.501\n.485\n.496\nTable B.8: Type I error rates for samples drawn from a Laplace distri-\nbution as a function of sample size and different rejection thresholds.\nsee that the other Bert models and LSTM-Ensemble actually loose\nin uncertainty quality as more data is added.\nFinnish UD.\nIn Figures B.11c and B.12c, we observe that the\nAUROC and AUPR scores of different models and metrics stay\nlargely constant across dataset sizes, which could be explained\nwith the larger amount of training data supplied compared to\nDan+. On the token-level correlation between uncertainty and\nloss in Figure B.13, we see the DDU Bert profiting most from\nmore data. On a sequence-level, as depicted in Figure B.14c, the\ncorrelation appears mostly static across training set sizes, with\nonly small gaps between in-distribution and out-of-distribution\ndata.\nOverall, it seems that the range of dataset sizes for Dan+ show\nthe most critical differences between models, while for the dataset\nsizes used for Finnish UD and Clinc Plus, enough data seems to be\nsupplied for changes to be more miniscule. This result is particularly\nrelevant for low-resource setting, although the dependency on the\ntask can not be disentangled from these results.\nB.6 qualitative analysis\n302\nSample Size\nτ\nASO\nStudent’s t\nBootstrap\nPermutation\nWilcoxon\nMann-Whitney U\n5\n.05\n.012\n.054\n.107\n.028\n.028\n.054\n.10\n.034\n.108\n.147\n.089\n.096\n.088\n.20\n.076\n.203\n.235\n.187\n.162\n.165\n.30\n.110\n.319\n.342\n.302\n.229\n.291\n.40\n.146\n.423\n.435\n.415\n.331\n.360\n.50\n.198\n.532\n.539\n.523\n.530\n.524\n10\n.05\n.012\n.046\n.062\n.043\n.039\n.041\n.10\n.018\n.087\n.107\n.093\n.094\n.084\n.20\n.044\n.187\n.206\n.180\n.172\n.187\n.30\n.064\n.295\n.314\n.297\n.265\n.284\n.40\n.114\n.401\n.405\n.399\n.373\n.412\n.50\n.180\n.507\n.514\n.505\n.500\n.508\n15\n.05\n.004\n.050\n.064\n.049\n.050\n.054\n.10\n.010\n.100\n.115\n.100\n.103\n.104\n.20\n.036\n.194\n.201\n.182\n.187\n.187\n.30\n.070\n.295\n.302\n.287\n.294\n.291\n.40\n.114\n.386\n.394\n.379\n.371\n.373\n.50\n.198\n.481\n.484\n.487\n.472\n.497\n20\n.05\n.002\n.054\n.064\n.059\n.055\n.052\n.10\n.004\n.115\n.121\n.113\n.103\n.113\n.20\n.030\n.195\n.205\n.202\n.187\n.204\n.30\n.058\n.281\n.287\n.277\n.283\n.291\n.40\n.130\n.377\n.386\n.375\n.368\n.384\n.50\n.190\n.489\n.493\n.493\n.468\n.469\nTable B.9: Type I error rates for samples drawn from a Rayleigh\ndistribution as a function of sample size and different rejection thresholds.\nB.6\nQualitative Analysis\nThis section provides more examples for the qualitative analysis in\nSection 4.2.7.\nDan+.\nWe show more examples of the predictive entropies on\nsamples from the Dan+ dataset in Figure B.15, where uncertainty\nvalues where jointly normalized by subtracting the mean and di-\nviding by the standard deviation over all models and time steps.\nWe can make the following observations: Firstly, uncertainty seems\nto decrease on punctuation marks such as commas and full-stops.\nSecondly, uncertainty appears higher on sub-word tokens and some\nnamed entities. Thirdly, DDU Bert and the LSTM ensemble pro-\nduce the highest uncertainty values, which are also two of the best\nperforming models on the task.\nFinnish UD.\nHere, we give more examples of the analysis on\nthe Finnish UD dataset in Figure B.16. First of all, we see that the\nvariational LSTM and SNGP Bert seem to produce almost constant\nuncertainty scores, which can be explained by their suboptimal\nB.7 additional coverage results\n303\nSub-sampled Train ppl.↓\nLanguage\nTrain ppl.↓\nn = 100\nn = 500\nn = 1000\nTest ppl.↓\nOOD Test ppl.↓\nEnglish\n31.54\n43.97 ± 2.46\n44.50 ± 0.68\n44.9 ± 0.4\n53.11\n120.32\nDanish\n112.73\n252.52 ± 13.25\n247.09 ± 3.3\n249.27 ± 3.15\n418.71\n524.32\nFinnish\n116.49\n257.67 ± 10.96\n257.66 ± 4.7\n260.36 ± 5.36\n1374.76\n1284.82\nTable B.10: Results of using an interpolated Kneser-Ney n-gram lan-\nguage model on selected datasets, including sub-sampled training splits\nand the OOD test set. Scores of sub-sampled training sets were obtained\nover five different attempts.\nperformance in task, as shown by their results in Table 4.2. But\neven for the models that perform better, such as the variational\nBert and the LSTM ensemble, the decomposition of predictive\nentropy into aleatoric and epistemic uncertainty reveals that model\nuncertainty generally remains low, and is overshadowed to a larger\nextent by the aleatoric uncertainty. We can observe that similar\nto Danish, uncertainty seems to be low on punctuation marks and\nhigh on subword tokens. Furthermore, aleatoric uncertainty seems\nto be higher on nouns and pronouns. This could be due to the\nsheer number of possible nouns and pronouns that could fill such a\ngap in a sentence.\nB.7\nAdditional Coverage Results\nWe show additional plots for the experiments in Section 5.4.1,\nillustrating the coverage per set size-bins in Figure B.17. We can\nsee the counterparts for Figure 5.2 using the larger M2M100(1.2B)\nmodel in Figures B.17a and B.17b: Instead of leveling off like for\nthe smaller model, most prediction set sizes are either in a very\nsmall range or in a size of a few ten thousand. In Figures B.17c\nand B.17d, we show similar plots for the two different OPT model\nsizes. Since in both cases, most prediction set sizes are rather small,\nwe zoom in on the the sizes from 1 to 100. Here, we can observe a\nsimilar behavior to the smaller M2M100(400m), gradually leveling\noff. We do not show similar plots for other distance metrics as they\nshow similar trends.\nB.8\nAblating Neighborhood Size and Desired\nCoverage\nIn this section, we present experiments surrounding the two most\npivotal parameters of our method in Section 5.3: The desired\nconfidence level α, as well as the number of neighbors.\nB.8 ablating neighborhood size and desired coverage\n304\nα\n% Cov.\n∅Width ↓\nScc ↑\nEcg ↓\nM2M100(400M) / de →en\n.1\n.9442\n.31\n.8702\n.0011\n.2\n.8767\n.18\n.7906\n8.63 × 10−5\n.3\n.7963\n.12\n.0000\n.0016\n.4\n.7058\n.09\n.1393\n.0082\n.5\n.6081\n.07\n.2836\n.0055\n.6\n.5017\n.06\n.1393\n.0082\n.7\n.3896\n.05\n.0000\n.0091\n.8\n.2800\n.05\n.0000\n.0090\n.9\n.1762\n.04\n.0000\n.0071\nM2M100(400M) / ja →en\n.1\n.7453\n.15\n.3080\n.1511\n.2\n.5579\n.07\n.2728\n.2446\n.3\n.4277\n.04\n.2770\n.2779\n.4\n.3438\n.03\n.1212\n.2438\n.5\n.2749\n.03\n.0455\n.1883\n.6\n.2175\n.02\n.0000\n.1207\n.7\n.1685\n.02\n.0000\n.0560\n.8\n.1309\n.01\n.0000\n.0117\n.9\n.0989\n.02\n.0000\n.0099\nOPT(350M) / OpenWebText\n.1\n.9460\n.26\n.8\n1.85 × 10−5\n.2\n.8937\n.16\n.8000\n.000\n.3\n.8392\n.10\n.5000\n8.74 × 10−6\n.4\n.7782\n.08\n.6667\n.000\n.5\n.7171\n.06\n.0000\n1.19 × 10−5\n.6\n.6559\n.06\n.6033\n.000\n.7\n.5945\n.05\n.000\n8.21 × 10−6\n.8\n.5349\n.05\n.4462\n.000\n.9\n.4757\n.05\n.3580\n.000\nTable B.11: Results for varying\nvalues of α using different models\nand datasets.\nK\n% Cov.\n∅Width ↓\nScc ↑\nEcg ↓\nM2M100(400M) / de →en\n10\n.9923\n.39\n.9728\n.0000\n25\n.9563\n.37\n.8877\n.0011\n50\n.9504\n.32\n.8870\n.0006\n75\n.9444\n.32\n.8641\n.0014\n100\n.9442\n.31\n.8702\n.0011\n200\n.9422\n.31\n.8125\n.0016\n300\n.9404\n.31\n.8483\n.0019\n500\n.9389\n.31\n.8214\n.0023\nM2M100(400M) / ja →en\n10\n.8013\n.17\n.2995\n.1606\n25\n.7353\n.17\n.2994\n.1438\n50\n.7540\n.17\n.3023\n.1603\n75\n.7368\n.16\n.3019\n.1603\n100\n.7453\n.15\n.3072\n.1529\n200\n.7295\n.14\n.2938\n.1787\n300\n.7192\n.13\n.2948\n.1788\n500\n.7110\n.13\n.2756\n.1867\nOPT(350M) / OpenWebText\n10\n.9438\n.35\n.8824\n.0019\n25\n.9522\n.33\n.8333\n2.06 × 10−5\n50\n.9442\n.27\n.0000\n1.86 × 10−5\n75\n.9477\n.27\n.8000\n1.03 × 10−5\n100\n.9460\n.26\n.8000\n1.86 × 10−5\n200\n.9487\n.28\n.8571\n6.20 × 10−5\n300\n.9500\n.28\n.8181\n1.86 × 10−5\n500\n.9508\n.29\n.8181\n1.86 × 10−5\nTable B.12: Results for varying\nneighborhood sizes K using dif-\nferent models and datasets.\nCoverage Threshold.\nIn Table B.11, we investigate the impact\nof different values on α on our evaluation metrics. We show that\nthe increase in α does indeed produce the expected decrease in\ncoverage, however with a certain degree of overcoverage for the de\n→en MT and the LM task. The loss in coverage always goes hand\nin hand with a decrease in the average prediction set width as well,\nas the model can allow itself to produce tighter prediction sets at\nthe cost of higher miscoverage. As this also produces bin in which\nall contained instances are uncovered, this produces zero values for\nthe SCC, while we cannot discern clear trends for the ECG.\nNeighborhood Size.\nIn Table B.12, we vary the effect of the\nchosen neighborhood size (with 100 being the value we use in our\nmain experiments). We make the following, interesting observa-\nB.9 additional clustering results\n305\ntions: Coverage on the MT task seems to decrease with an increase\nin the neighborhood size as prediction set widths get smaller on\naverage, with a neighborhood size around 100 striking a balance\nbetween coverage, width, computational cost and SCC / ECG. For\nLM, coverage seems to be mostly constant, with prediction set\nwidth hitting an inflection point for 100 neighbors. We speculate\nthat initially there might be a benefit to considering more neighbors\nto calibrate ˆq, but that considering too large neighborhoods might\nintroduce extra noise. While we found 100 to be a solid choice for\nthe purpose of our experiments, we leave more principled ways to\ndetermine the neighborhood size to future work.\nB.9 additional clustering results\n306\nB.9\nAdditional Clustering Results\nCluster 1120\nHow many fluid ounces are in one quarter\nof an imperial pint?\nHow many fluid ounces in one Imperial\npint?\nHow many fluid ounces in half an Imperial\npint?\nCluster 920\nWhich famous US outlaw shot the cashier\nof a savings bank in Gallatin Missouri in\n1869?\nWhat famous outlaw committed the Wild\nWest’s first train robbery on July 21, 1873\nin Adair, Iowa?\nOn July 21, 1873, Jesse James and the\nJames-Younger gang pulled off the first\nsuccessful what of the American West, in\nAdair Iowa?\nCluster 984\nIn what country was the game of golf\ninvented?\nWhich ball game was invented by Dr\nJames Naismith in Massachusetts USA in\n1891?\nIt is generally accepted that the game of\ngolf originated in what country?\nWhat’s a country where most people love\nplaying rugby?\nWhat’s a country where most people love\nplaying golf?\nCluster 811\nHow many colors are there in the spec-\ntrum when white light is separated?\nWhich part of the eye contains about 137\nmillion light-sensitive cells in one square\ninch?\nWhich of the retina’s cells can distinguish\nbetween different wavelengths of light?\nIn four colour process printing, which is\nalso known as CMYK, which are the only\nfour colours that are used?\nHow many colours are in the rainbow?\nIn art,\nwhat are the three primary\ncolours?\nWhat color consists of the longest wave-\nlengths of lights visible by the human eye?\nWhat are the three primary colours of\nlight?\nTable B.13: Contents of some ran-\ndomly sampled clusters that result\nfrom the clustering procedure for\nTriviaQA.\nCluster 823\nWhere in Europe is it located?\nIs it in the European Plain?\nWhich region of Europe is it in?\nCluster 1176\nDid she have children?\nDoes she have any children?\nDid she have any children?\nDid she have any other children?\nCluster 2244\nWho won the Kentucky Derby?\nas he won the Derby before?\nHas he raced in the Derby before?\nWhat were the winning horse’s odds?\nHow many Derbys have their been?\nCluster 11\nAre they densities of everything the\nsame?\nWhat is the densest elements at regular\nconditions?\nWhat is density of a substance?\nWhat is another symbol for density?\nWho gives weight per unit volume as the\ndefinition?\nWhere is density the same value as it’s\nmass concentration?\nTo make comparisons easier what stands\nin for density?\nWhat is the relative density of something\nthat floats?\nCluster 1081\nWho was murdered?\nwho was killed?\nWho committed this murder?\nwho was killed?\nWho was killed?\nwho was killed?\nTable B.14: Contents of some ran-\ndomly sampled clusters that result\nfrom the clustering procedure for\nCoQA.\nB.10 additional calibration results\n307\nIn this section we take a closer look at the results of the\nclustering procedure described in Section 6.1.2.\nIn our exper-\niments, we run HDBSCAN using a minimum cluster size of\nthree, since preliminary experiments showed this number to\nproduce the best trade-off between the coherence of cluster\ncontents (as evaluated in Table 6.2) and a diversity in cluster\ntargets. This setting yields a distribution of cluster sizes shown\nin Figure B.18.\nWe can see that the majority of cluster sizes\nare rather small, including questions on specific topics, some\nof which we display in Tables B.13 and B.14.\nNot shown are\ncluster sizes over 20 since the distribution quickly levels off,\nas well the set of all points that could not be sorted into any cluster.\nAfter clustering and computing the average accuracy per cluster,\nwe obtain a distribution over calibration targets, which we show\nwith density plots in Figure B.19.\nSince most clusters are of\nsize three, we can see clear modes around 0, 0.33, 0.66 and 1 for\nVicuna v1.5 in Figure B.19a. For GPT-3.5 in Figure B.19b these are\nhowever less pronounced: We see that targets are often concentrated\non 0 or 1, respectively. Similar spikes like in Figure B.19a are\nobservable for both models on CoQA in Figures B.19c and B.19d.\nThis trend is also visible when plotting the assigned calibration\ntargets per data point in Figure B.20: While we can spot more\ntransitionary colors between the blue and red extremes in the\nmanifold for Figure B.20a, the colors tend more to either of the\noptions Figure B.20b. These mode trends continue for CoQA in\nFigure B.20c and Figure B.20d.\nB.10\nAdditional Calibration Results\nAdditional reliability plots.\nWe show the all available relia-\nbility diagrams for the experiments in Section 6.2.2 for Vicuna-v1.5\nfor TriviaQA in Figure 6.4 and CoQA in Figure B.22, as well as\nthe corresponding plots for GPT-3.5 in Figures B.23 and B.24.\nSequence likelihood can be well-calibrated already, but this fact\ndepends strongly on the dataset in question. And while our version\nof Platt scaling can improve results, it also narrows the range\nof confidence values to a narrow window. Verbalized uncertainty\nin both of variants also is not able to produce a wide variety of\nresponses, even though this effect is slightly less pronounced for\nGPT-3.5. The auxiliary model is able to predict a wider array\nof confidence values in all settings, with the clustering variant\nachieving better calibration overall.\nB.10 additional calibration results\n308\nPredictive Entropy\nMaximum probability\nNeural Network\nPlatt scaling\nFigure B.3: Uncertainty measured by different metrics for single-instance\nmodels (purple plots) and their gradient magnitude (yellow / green\nplots).\nB.10 additional calibration results\n309\nClass variance\nPredictive Entropy\nMutual Information\nMC Dropout\nNeural Ensemble\nAnchored Ensemble\nFigure B.4: Uncertainty measured by different metrics for multi-instance\nmodels (purple plots) and the gradient of the uncertainty score w.r.t. to\nthe input (yellow / green plot).\nB.10 additional calibration results\n310\nblevet\ndag\nnye\nførste\nuden\nsiger\nhele\ngang\nsammen\nsiden\nkr\ndanske\ngar\nma\nnar\nfa\nar\nogsa\nsa\npa\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nRelative frequency\nDan+\nOriginal\n100\n1000\np¨aiv¨an¨a\nsaa\nottaa\nyliopiston\ntuli\nOn\nsai\npit¨a¨a\nJos\nprosenttia\nEU\nSuomen\nT¨am¨a\nTurun\nSe\nvuonna\nH¨an\nEuroopan\nn\n–\nFinnish UD\nadd\ngas\ngood\nnumber\nsong\nphone\nﬂight\nshopping\nreservation\nset\noil\nlong\ntime\nbank\nchange\ncar\nlist\naccount\ncredit\ncard\nClinc Plus\nType Rank\nFigure B.5: Comparing the relative frequency of types in the original and\nsub-sampled training sets. Shown are the top 20 types in the original\ntraining set, compared to sub-sampled training sets of 100 and 1000\nsequences for Dan+, Finnish UD and Clinc Plus. It is shown that while\nthe type frequencies differ noticeably for the small dataset, already 1000\nsequences suffice to approximate the original frequencies. Numbers,\nstopwords and the most common punctuation were removed.\n0\n5\n10\n15\n20\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nRelative frequency\nDan+\nOriginal\n100\n1000\n0\n5\n10\n15\n20\nFinnish UD\n0\n5\n10\n15\n20\nClinc Plus\nSequence length\nFigure B.6: Comparing the relative frequency of sequence lengths in\nthe original and sub-sampled training sets. Shown are sequence lengths\nbetween 0 and 25 in the original test, compared to OOD test sets for\nDan+, Finnish UD, Clinc Plus. Not the whole distribution is shown\nin all cases, with many of the OOD sentences for Dan+ being very\nlong. For Dan+ and Finnish UD, the sentence length distributions are\nnoticeably different. For Clinc Plus, they are very similar.\nB.10 additional calibration results\n311\nB-PERderiv\nB-ORGderiv\nB-PERpart\nB-MISCpart\nI-LOC\nB-ORGpart\nI-MISC\nI-ORG\nB-ORG\nI-PER\nB-PER\nO\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency\nDan+\nOriginal\n100\n1000\nSYM\nADP\nSCONJ\nNUM\nCCONJ\nAUX\nPROPN\nPRON\nADV\nVERB\nPUNCT\nNOUN\nFinnish UD\nLabel Rank\nFigure B.7: Comparing the relative frequency of labels in the original\ntraining set, compared to sub-sampled training sets. Shown are frequen-\ncies for 100 and 1000 sequences. For Danish, the most frequent label by\nfar is the neutral label indicating that no named entity is present.\nB-PERderiv\nB-ORGderiv\nB-PERpart\nB-MISCpart\nI-LOC\nB-ORGpart\nI-MISC\nI-ORG\nB-ORG\nI-PER\nB-PER\nO\n0.0\n0.2\n0.4\n0.6\n0.8\nRelative frequency\nDan+\nID\nOOD\nSYM\nADP\nSCONJ\nNUM\nCCONJ\nAUX\nPROPN\nPRON\nADV\nVERB\nPUNCT\nNOUN\nFinnish UD\nLabel Rank\nFigure B.8: Comparison of the relative class frequencies between original\ntraining set compared to the OOD test set. The proportions stay largely\nthe same for Danish, while different more for Finnish.\nB.10 additional calibration results\n312\n0\n5\n10\n15\n20\n25\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nRelative frequency\nDan+\nID\nOOD\n0\n5\n10\n15\n20\n25\nFinnish UD\n0\n5\n10\n15\n20\n25\nClinc Plus\nSequence length\nFigure B.9: Comparison of sequence length distribution between the\noriginal training set and the OOD test set. For English, the distribu-\ntion of lengths of voice assistant commands is quite similar, while the\ndifferences for Dan+ and Finnish UD are more pronounced.\ntidga\nsidste\nsagde\nsamme\nblevet\ndag\nnye\nførste\nuden\nsiger\nhele\ngang\nsammen\nsidenkr\ndanske\ngarma\nnarfaar\nogsasapa\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nRelative frequency\nDan+\nID\nOOD\ntehd¨a\nKun\nyhteis¨onEi\nhuomioon\np¨aiv¨an¨a\nsaa\nottaa\nyliopiston\ntuliOnsai\npit¨a¨a\nJos\nprosenttia\nEU\nSuomen\nT¨am¨a\nTurunSe\nvuonna\nH¨an\nEuroopann–\nFinnish UD\ncheck\nmarch\norder\nlimit\ntires\nadd\ngas\ngood\nnumber\nsong\nphone\nﬂight\nshopping\nreservationsetoil\nlong\ntime\nbank\nchangecarlist\naccount\ncredit\ncard\nClinc Plus\nType Rank\nFigure B.10: Comparison of the relative frequencies of the top 25 types\nin the original training set compared to the OOD test set. Even among\nthe most frequent and therefore usually common tokens, the plots show\ndifferences between the in-distribution train and out-of-distribution\ntest set. Numbers, stopwords and the most common punctuation were\nremoved.\nB.10 additional calibration results\n313\n0.0\n0.2\n0.4\n0.6\n0.5\n0.6\n0.7\n0.8\n0.9\nID / OOD AUROC\n10000 instances\n0.0\n0.2\n0.4\n0.6\n12500 instances\n0.0\n0.2\n0.4\n0.6\n15000 instances\nVariance\nDempster-Shafer\nPred. Entropy\nLog. Prob.\nSoftmax gap\nMax. Prob.\nMutual Inf.\nLSTM\nLSTM Ensemble\nBayesian LSTM\nDDU Bert\nVariational Bert\nMacro F1 score\n(a) Scatter plot for the Clinc Plus dataset.\n0.1\n0.2\n0.3\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nID / OOD AUROC\n1000 instances\n0.1\n0.2\n0.3\n2000 instances\n0.1\n0.2\n0.3\n4000 instances\nDempster-Shafer\nMutual Inf.\nSoftmax gap\nLog. Prob.\nMax. Prob.\nPred. Entropy\nVariance\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\n(b) Scatter plot for the Dan+ dataset.\n0.0\n0.2\n0.4\n0.6\n0.45\n0.50\n0.55\n0.60\n0.65\nID / OOD AUROC\n5000 instances\n0.0\n0.2\n0.4\n0.6\n7500 instances\n0.0\n0.2\n0.4\n0.6\n10000 instances\nPred. Entropy\nMax. Prob.\nSoftmax gap\nDempster-Shafer\nMutual Inf.\nVariance\nLog. Prob.\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\n(c) Scatter plot for the Finnish UD dataset.\nFigure B.11: Scatter plots showing the difference between model perfor-\nmance (measured by macro F1) and the quality of uncertainty estimates\nusing AUROC. Shown are different models and uncertainty metrics and\nseveral training set sizes on the used datasets.\nB.10 additional calibration results\n314\n0.0\n0.2\n0.4\n0.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nID / OOD AUPR\n10000 instances\n0.0\n0.2\n0.4\n0.6\n12500 instances\n0.0\n0.2\n0.4\n0.6\n15000 instances\nVariance\nDempster-Shafer\nPred. Entropy\nLog. Prob.\nSoftmax gap\nMax. Prob.\nMutual Inf.\nLSTM\nLSTM Ensemble\nBayesian LSTM\nDDU Bert\nVariational Bert\nMacro F1 score\n(a) Scatter plot for the Clinc Plus dataset.\n0.1\n0.2\n0.3\n0.1\n0.2\n0.3\n0.4\n0.5\nID / OOD AUPR\n1000 instances\n0.1\n0.2\n0.3\n2000 instances\n0.1\n0.2\n0.3\n4000 instances\nDempster-Shafer\nMutual Inf.\nSoftmax gap\nLog. Prob.\nMax. Prob.\nPred. Entropy\nVariance\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\n(b) Scatter plot for the Dan+ dataset.\n0.0\n0.2\n0.4\n0.6\n0.55\n0.60\n0.65\n0.70\nID / OOD AUPR\n5000 instances\n0.0\n0.2\n0.4\n0.6\n7500 instances\n0.0\n0.2\n0.4\n0.6\n10000 instances\nPred. Entropy\nMax. Prob.\nSoftmax gap\nDempster-Shafer\nMutual Inf.\nVariance\nLog. Prob.\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\n(c) Scatter plot for the Finnish UD dataset.\nFigure B.12: Scatters plot showing the difference between model perfor-\nmance (measured by macro F1) and the quality of uncertainty estimates\nusing AUPR. Shown are different models and uncertainty metrics and\nseveral training set sizes on the used datasets.\n0.0\n0.2\n0.4\n0.6\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nToken-level Kendall’s tau\n5000 instances\n0.0\n0.2\n0.4\n0.6\n7500 instances\n0.0\n0.2\n0.4\n0.6\n10000 instances\nPred. Entropy\nMax. Prob.\nSoftmax gap\nDempster-Shafer\nMutual Inf.\nVariance\nLog. Prob.\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\nFigure B.13: Scatter plot showing the difference between model perfor-\nmance (measured by macro F1) and the quality of uncertainty estimates\non a token-level (measured by Kendall’s τ). Results are shown for\ndifferent models and uncertainty metrics and several training set sizes\non the Finnish UD dataset. Arrows indicate changes between the in-\ndistribution and out-of-distribution test set.\nB.10 additional calibration results\n315\n0.0\n0.2\n0.4\n0.6\n0.0\n0.2\n0.4\n0.6\n0.8\nSequence-level Kendall’s tau\n10000 instances\n0.0\n0.2\n0.4\n0.6\n12500 instances\n0.0\n0.2\n0.4\n0.6\n15000 instances\nVariance\nDempster-Shafer\nPred. Entropy\nLog. Prob.\nSoftmax gap\nMax. Prob.\nMutual Inf.\nLSTM\nLSTM Ensemble\nBayesian LSTM\nDDU Bert\nVariational Bert\nMacro F1 score\n(a) Scatter plot for the Clinc Plus dataset.\n0.1\n0.2\n0.3\n−0.4\n−0.2\n0.0\n0.2\n0.4\nSequence-level Kendall’s tau\n1000 instances\n0.1\n0.2\n0.3\n2000 instances\n0.1\n0.2\n0.3\n4000 instances\nDempster-Shafer\nMutual Inf.\nSoftmax gap\nLog. Prob.\nMax. Prob.\nPred. Entropy\nVariance\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\n(b) Scatter plot for the Dan+ dataset.\n0.0\n0.2\n0.4\n0.6\n−0.4\n−0.3\n−0.2\n−0.1\n0.0\n0.1\n0.2\nSequence-level Kendall’s tau\n5000 instances\n0.0\n0.2\n0.4\n0.6\n7500 instances\n0.0\n0.2\n0.4\n0.6\n10000 instances\nPred. Entropy\nMax. Prob.\nSoftmax gap\nDempster-Shafer\nMutual Inf.\nVariance\nLog. Prob.\nLSTM\nLSTM Ensemble\nST-tau LSTM\nBayesian LSTM\nVariational LSTM\nDDU Bert\nVariational Bert\nSNGP Bert\nMacro F1 score\n(c) Scatter plot for the Finnish UD dataset.\nFigure B.14: Scatter plot showing the difference between model perfor-\nmance (measured by macro F1) and the quality of uncertainty estimates\non a sequence-level (measured by Kendall’s τ). Results are shown for\ndifferent models and uncertainty metrics and several training set sizes\non the Finnish UD and Clinc Plus dataset. Arrows indicate changes\nbetween the in-distribution and out-of-distribution test set.\nB.10 additional calibration results\n316\ntværtimod (O)\ner (O)\ndet (O)\nen (O)\naf (O)\nrus (B-PER)\n##lands (-100)\nfa (O)\nsucceshistorie (O)\n##r (-100)\n, (O)\nder (O)\noptræder (O)\nnar (O)\nrock (O)\n##gruppen (-100)\ngo (B-ORG)\n##rk (-100)\n##y (-100)\npark (I-ORG)\nindleder (O)\nderes (O)\ndanmark (B-LOCpart)\n##s (-100)\n- (-100)\nturne (-100)\ni (O)\nde (O)\nsmukke (O)\nsøer (O)\n##s (-100)\nby (O)\n. (O)\n3\n2\n1\n0\n1\n2\n3\nNormalized Uncertainty\nPred. Entropy\nLSTM\nDDU Bert\nBayesian LSTM\nVariational Bert\nSNGP Bert\nLSTM Ensemble\nST-tau LSTM\nVariational LSTM\n(a) Predictive entropy over the sentence “On the contrary, it is one of Russia’s\nfew success stories that performs when the rock group Gorky Park begins their\nDanish tour in the city of the beautiful lakes”.\n' (O)\nmen (O)\nvi (O)\nhavde (O)\nikke (O)\npræcise (O)\ninformationer (O)\nom (O)\naftalen (O)\n, (O)\nder (O)\nvar (O)\nindg (O)\n##aet (-100)\n. (O)\n2\n1\n0\n1\n2\n3\n4\n5\nNormalized Uncertainty\nPred. Entropy\nLSTM\nDDU Bert\nBayesian LSTM\nVariational Bert\nSNGP Bert\nLSTM Ensemble\nST-tau LSTM\nVariational LSTM\n(b) Predictive entropy over the sentence “However, we did not have precise\ninformation about what was agreed upon”.\ndæmon (O)\n##iserede (-100)\nhad (O)\n- (-100)\nytringer (-100)\ninspirerer (O)\nde (O)\nmarginal (O)\n##iserede (-100)\n, (O)\npsykisk (O)\nustabile (O)\n( (O)\n! (O)\n) (O)\nmænd (O)\npa (O)\nden (O)\nyderste (O)\nhøjrefløj (O)\ntil (O)\nat (O)\ngribe (O)\ntil (O)\nvold (O)\nmod (O)\nmuslimer (O)\ndet (O)\nskriver (O)\nel (B-PER)\n##vir (-100)\n, (O)\nder (O)\n2\n0\n2\n4\nNormalized Uncertainty\nPred. Entropy\nLSTM\nDDU Bert\nBayesian LSTM\nVariational Bert\nSNGP Bert\nLSTM Ensemble\nST-tau LSTM\nVariational LSTM\n(c) Predictive entropy over the sentence “Demonizing hate speech inspires the\nmarginalized, PSYCHOLOGY UNSTABLE (!) Men on the far right to resort\nto violence against Muslims. This writes Elvir, who...”.\nFigure B.15: Further examples for uncertainty estimates on single\nsequences. Taken from the Dan+ dataset.\nB.10 additional calibration results\n317\n@ (PROPN)\nToni (-100)\n##L (-100)\n##ot (-100)\n##jonen (-100)\n@ (PROPN)\nharr (-100)\n##ikum (-100)\n##pula (-100)\n##ine (-100)\nSe (PRON)\non (AUX)\ntotta (ADJ)\n, (PUNCT)\nettä (SCONJ)\nehkä (ADV)\ntällaisia (ADJ)\nLatvia (PROPN)\n- (PUNCT)\nVenäjä (PROPN)\n- (ADJ)\ntyyppisiä (-100)\npelejä (NOUN)\nkaipaisi (VERB)\nlisää (ADV)\ntällaiseen (ADJ)\narvot (NOUN)\n##a (-100)\n##pahtuma (-100)\n##an (-100)\n. (PUNCT)\n[UNK] (NOUN)\nsalibandy (-100)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nUnnormalized Uncertainty\nPred. Entropy\nMutual Inf.\nSNGP Bert\nVariational Bert\nLSTM Ensemble\nVariational LSTM\n(a) Predictive entropy over the sentence “@ToniLotjonen @harrikumpulaine It\nis true that I’d maybe like to see more of such Latvia–Russia type games in\nthese kinds of major sports events. #floorball”.\nToivon (VERB)\n, (PUNCT)\nettä (SCONJ)\nmiehet (NOUN)\nantaisi (VERB)\n##vat (-100)\nomat (ADJ)\nvinkki (NOUN)\n##nsä (-100)\nkommen (NOUN)\n##teissa (-100)\n. (PUNCT)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nUnnormalized Uncertainty\nPred. Entropy\nMutual Inf.\nVariational LSTM\nVariational Bert\nSNGP Bert\nLSTM Ensemble\n(b) Predictive entropy over the sentence “I hope that the procedures done on\nthe person in question stop and he gives his body (and mind) time to recover\nfrom that poisoning!”.\nEhkä (ADV)\nhat (NOUN)\n##ulla (-100)\ntai (CCONJ)\nsen (PRON)\npäähäni (NOUN)\njoutu (NOUN)\n##misella (-100)\nei (AUX)\nole (AUX)\nmitään (PRON)\nmerkitystä (NOUN)\n. (PUNCT)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nUnnormalized Uncertainty\nPred. Entropy\nMutual Inf.\nVariational LSTM\nVariational Bert\nSNGP Bert\nLSTM Ensemble\n(c) Predictive entropy over the sentence “Maybe the hat or how it got on my\nhead doesn’t matter”.\nFigure B.16: Further examples for uncertainty estimates on single\nsequences. Taken from the Finnish UD dataset.\nB.10 additional calibration results\n318\n0\n20000\n40000\n60000\n80000\n100000 120000\nSet Size\n0.6\n0.7\n0.8\n0.9\n1.0\nCoverage\n0\n1000\n2000\n3000\n4000\n5000\nNumber of Points\n(a) Conditional coverage of\nM2M100(1.2B) for de →en.\n0\n20000\n40000\n60000\n80000\n100000 120000\nSet Size\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nCoverage\n0\n200\n400\n600\n800\n1000\n1200\nNumber of Points\n(b) Conditional coverage of\nM2M100(1.2B) for ja →en.\n0\n20\n40\n60\n80\n100\nSet Size\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\nCoverage\n0\n100\n200\n300\n400\n500\n600\n700\nNumber of Points\n(c) Conditional coverage for\nOPT(350M) on Language Modelling.\n0\n20\n40\n60\n80\n100\nSet Size\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nCoverage\n0\n100\n200\n300\n400\nNumber of Points\n(d) Conditional coverage for\nOPT(1.3B) on Language Modelling.\nFigure B.17: Additional conditional coverage plots for the MT and\nLM dataset using our non-exchangeable conformal prediction method,\naggregating predictions by prediction set size. The blue curve shows\nthe conditional coverage per bin, whereas red bars show the number of\npredictions per bin. For Figures B.17c and B.17d, we zoom in on the\nprediction set sizes from 1 and 100.\n(a) Cluster sizes on TriviaQA.\n(b) Cluster sizes on CoQA.\nFigure B.18: Bar plot of cluster sizes found. The plot is truncated at\nsize 20.\nB.10 additional calibration results\n319\n(a) Vicuna v1.5 on TriviaQA.\n(b) GPT-3.5 on TriviaQA.\n(c) Vicuna v1.5 on CoQA.\n(d) GPT-3.5 on CoQA.\nFigure B.19: Density plot of calibration targets generated through the\nclustering procedure for the two LLMs and TriviaQA / CoQA.\nB.10 additional calibration results\n320\n(a) Vicuna v1.5 on TriviaQA.\n(b) GPT-3.5 on TriviaQA.\n(c) Vicuna v1.5 on CoQA.\n(d) GPT-3.5 on CoQA.\nFigure B.20: Illustrating questions from TriviaQA along with their\nassigned confidence targets for the two LLMs, signified through their\ncolor from dark blue (0) to dark red (1). To avoid clutter, we subsampled\n40% of the combined datasets to be shown here and used PacMAP (Wang\net al., 2021b) to transform their sentence embeddings into 2D space.\nB.10 additional calibration results\n321\n(a) Seq. likelihood.\n(b) Seq. like. (CoT).\n(c) Platt scaling.\n(d) Platt scaling (CoT).\n(e) Verbalized Qual.\n(f) Verb. Qual. (CoT).\n(g) Verbalized %\n(h) Verb. % (CoT).\n(i) Auxiliary (binary).\n(j) Aux. (clustering).\nFigure B.21: Reliability diagrams for all methods using 10 bins each\nfor Vicuna v1.5 7B on TriviaQA. The color as well as the percentage\nnumber within each bar indicate the proportion of total points contained\nin each bin.\nB.10 additional calibration results\n322\n(a) Seq. likelihood.\n(b) Seq. like. (CoT).\n(c) Platt scaling.\n(d) Platt scaling (CoT).\n(e) Verbalized Qual.\n(f) Verb. Qual. (CoT).\n(g) Verbalized %.\n(h) Verb. % (CoT).\n(i) Auxiliary (binary).\n(j) Aux. (clustering).\nFigure B.22: Reliability diagrams for all methods using 10 bins each for\nVicuna v1.5 7B on CoQA. The color as well as the percentage number\nwithin each bar indicate the proportion of total points contained in each\nbin.\nB.10 additional calibration results\n323\n(a) Seq. likelihood.\n(b) Seq. like. (CoT).\n(c) Platt scaling.\n(d) Platt scaling (CoT).\n(e) Verbalized Qual.\n(f) Verb. Qual. (CoT).\n(g) Verbalized %.\n(h) Verb. % (CoT).\n(i) Auxiliary (binary).\n(j) Aux. (clustering).\nFigure B.23: Reliability diagrams for all methods using 10 bins each\nfor GPT-3.5 on TriviaQA. The color as well as the percentage number\nwithin each bar indicate the proportion of total points contained in each\nbin.\nB.10 additional calibration results\n324\n(a) Seq. likelihood.\n(b) Seq. like. (CoT).\n(c) Platt scaling.\n(d) Platt scaling (CoT).\n(e) Verbalized Qual.\n(f) Verb. Qual. (CoT).\n(g) Verbalized %.\n(h) Verb. % (CoT).\n(i) Auxiliary (binary).\n(j) Aux. (clustering).\nFigure B.24: Reliability diagrams for all methods using 10 bins each for\nGPT-3.5 on CoQA. The color as well as the percentage number within\neach bar indicate the proportion of total points contained in each bin.\nC\n|\nReproducibility\nAppendix\n“grad student descent: (machine learning, humorous) The\nprocess of choosing hyperparameters manually and in an\nad-hoc manner, typical of work assigned to a graduate\nstudent.”\n—Wiktionary definition.\nThesis\nAppendix\nSection 3.2.1\nAppendix C.3\nSection 2.2.3\nAppendix C.4.1\nSection 4.2.2\nAppendix C.5\nSection 4.2.1\nAppendix C.6\nSection 4.2.5\nAppendix C.7\nSection 5.4.1\nAppendix C.8\nTable C.1: Correspondences between sections of the reproducibility\nappendix and thesis chapters.\nThis appendix contains additional information for reproducibil-\nity purposes, according to the guidelines by Ulmer et al. (2022a).\nIn Appendix C.1, a number of open-source software projects that\nwere used in the making of this thesis are listed. Appendix C.2\ndiscusses the compute hardware and environmental impact of the\nconducted experiments and other aspects. In Table C.1, we give\nan overview over the correspondence between thesis chapters and\nsections in this appendix.\nC.1\nOpen Source Software\nThis thesis would not have been possible without the usage of\nopen-source tools and software. Deep learning models where im-\nplemented with NumPy (Harris et al., 2020), SciPy (Virtanen et al.,\n2020), scikit-learn (Pedregosa et al., 2011), einops (Rogozh-\nnikov, 2022), PyTorch (Paszke et al., 2019) and the transformers\nlibrary (Wolf et al., 2020). Experimental tracking and hyperparam-\n325\nC.2 environmental impact\n326\nRepository\nChapters\ngithub.com/Kaleidophon/phd-thesis\nSections 1.3, 2.1.2, 2.2.4 and 3.2.1\ngithub.com/Kaleidophon/evidential-deep-learning-survey\nSection 2.2.3\ngithub.com/Kaleidophon/awesome-experimental-standards-deep-learning\nChapter 3\ngithub.com/Kaleidophon/evidential-deep-learning-survey\nSection 3.2.1\ngithub.com/Kaleidophon/know-your-limits\nSection 4.1\ngithub.com/Kaleidophon/nlp-low-resource-uncertainty\ngithub.com/Kaleidophon/nlp-uncertainty-zoo\nSection 4.2\ngithub.com/Kaleidophon/non-exchangeable-conformal-language-generation\nChapter 5\ngithub.com/parameterlab/apricot\nChapter 6\nTable C.2: List of open-source repositories for the contents of this thesis.\neter search was facilitated via Weights & Biases (Biewald, 2020),\nand tracking carbon emissions with codecarbon (Schmidt et al.,\n2021; Lacoste et al., 2019; Lottick et al., 2019). The code for the\nplots and experiments in this thesis is itself available open-source,\nand corresponding online repositories are listed in Table C.2.\nC.2\nEnvironmental Impact\nHere, we discuss the environmental impact of the experiments\nin the different chapters.\nIn all cases, carbon emissions have\nbeen estimated using codecarbon (Schmidt et al., 2021; Lacoste\net al., 2019; Lottick et al., 2019), although it should be noted that\nsince the time of writing, more advanced tools like LLMCarbon\n(Faiz et al., 2023) have been developed to more accurately es-\ntimate the carbon footprint of language model training, specifically.\nFor Chapter 4, the carbon efficiency was estimated to be\n0.61 kgCO2eq / kWh. 735 hours of computation were performed\non a Tesla V100 GPU. This includes hyperparameter search,\nfailed runs, debugging, and discarded runs. As a rough upper\nbound, we estimate the compute time for a single replication of\nall experiments in Chapter 4 to take around 73 hours.89 Total\nemissions were estimated to be 52.45 kgCO2eq.\nFor Chapter 5, the carbon efficiency was estimated to be 0.12\nkgCO2eq / kWh. 159.5 hours of computation were performed on a\nNVIDIA RTX A6000. Total emissions are estimated to be 6.99\nkgCo2eq. All of these values are upper bound including debugging\nas well as failed or redundant runs, and thus any replication of\n89Note that this number could be reduced further by using better hardware\nacceleration, larger batch sizes, and slightly reducing the training duration for\nsome models. Most importantly, this number also includes compute used for\nhyperparameter search.\nC.3 aso test implementation details\n327\nresults will likely be shorter and incur fewer carbon emissions.\nFor Chapter 6, all experiments were run on a single V100\nNVIDIA GPU. We estimate finetuning the auxiliary calibrator to\namount to 0.05 kgCO2eq of emissions, with an estimated carbon\nefficiency of 0.46 kgCO2eq / kWH. Therefore, we estimate total\nemissions of around 1 kgCO2eq to replicate all the experiments in\nthis chapter.\nCarbon Offsetting.\nCarbon offsetting is a controversial topic\n(Watt, 2021; Campbell, 2021; Baras, 2023), and avoiding emission\nshould always be the preferred option compared to post-hoc offset-\nting. Nevertheless, the author believes in mitigating the impact\nof their emissions as best as possible. The tracked carbon emis-\nsions from all the chapter in this thesis are 60.44 kgCO2eq. An\nadditional 20% is added to this number to account for variation in\ntracking, untracked debug runs or failed experiments, amounting to\n72.53 kgCO2eq. Furthermore, over the course of almost four years,\nthe author attended a number of conferences during their PhD\nprogram, and conducted industrial internships as well as a research\nstay. The travels related to these activities produced an estimated\ntotal of 12088 kgCO2eq in emissions. Direct air capture by clime-\nworks (climeworks, 2022) was used to offset the emissions from\nthe experiments, and carbon credits stemming from wind energy\nprojects in Thailand and India were purchased through the Gold\nStandard Marketplace (Gold Standard, 2024) for travel-related\nemissions.\nC.3\nASO Test Implementation Details\nThis section details the Python implementation of the ASO test\nin Section 3.2.1. The full algorithm to compute the εmin score is\ngiven in Algorithm 3, and will now be explained in full detail. We\nshow how the violation ratio in Equation (3.3) can be compute in\nPython:\ndef compute_violation_ratio(scores_a: np.array, scores_b:\nnp.array, dt: float) -> float:\n,→\nquantile_func_a = get_quantile_function(scores_a)\nquantile_func_b = get_quantile_function(scores_b)\nt = np.arange(dt, 1, dt)\n# Points we integrate over\nf = quantile_func_a(t)\n# F-1(t)\ng = quantile_func_b(t)\n# G-1(t)\ndiff = g - f\nsquared_wasserstein_dist = np.sum(diff ** 2 * dt)\nC.3 aso test implementation details\n328\nAlgorithm 3 Almost Stochastic Order (ASO) Significance Test\nRequire: Sets of observations SA and SB, integration interval ∆t,\nnumber of bootstrap iterations B, desired confidence level 1 −α.\nεW2(Fn, Gm) = compute_violation_ratio(SA, SA, ∆t)\n▷Bootstrapping\nfor i ∈0, . . . , B do\nS∗\nA = bootstrap_sample(SA)\nS∗\nB = bootstrap_sample(SB)\n▷Store value below in list\nε∗\nW2(Fn, Gm) = compute_violation_ratio(S∗\nA, S∗\nA, ∆t)\nend for\n▷Compute value below based on variance of all the values in list\nˆσ2\nn,m = Var\n\u0014p mn\nn+m\n\u0000εW2(F ∗\nn, G∗\nm) −εW2(Fn, Gm)\n\u0001\u0015\nεmin(Fn, Gm, α) = εW2(Fn, Gm) −\nq\nn+m\nnm ˆσn,mΦ−1(α)\nreturn εmin(Fn, Gm, α)\n# Now only consider points where stochastic order is being\nviolated and set the rest to 0\n,→\ndiff[f >= g] = 0\nint_violation_set = np.sum(diff[1:] ** 2 * dt)\n# Ignore t = 0\nsince t in (0, 1)\n,→\nviolation_ratio = int_violation_set / squared_wasserstein_dist\nreturn violation_ratio\nWe can see that the integration over the violation set VX in\nEquation (3.3) is being performed by masking out values for which\nthe stochastic order is honored (i.e. where F −1\nn (t) ≥G−1\nn (t)). Com-\nputing the violation ratio involves building the empirical inverse\ncumulative distribution function or empirical quantile function, the\nsame method as in Dror et al. (2019) is used, with the corresponding\nPython code given below:\ndef get_quantile_function(scores: np.array) -> Callable:\ndef _quantile_function(p: float) -> float:\ncdf = np.sort(scores)\nnum = len(scores)\nindex = int(np.ceil(num * p))\nreturn cdf[np.clip(index - 1, 0, num - 1))]\nC.4 hyperparameters search\n329\nreturn np.vectorize(_quantile_function)\nThis function is also used inside the bootstrap sampling proce-\ndure, the last missing part of the implementation. We again follow\nthe implementation by Dror et al. (2019) and employ the inverse\ntransform sampling procedure, in which we draw p ∼U[0, 1] and\nrun it through a quantile function to create a sample.\nC.4\nHyperparameters Search\nHere we list the hyperparameter search procedures, ranges and\nfound values for the different experiments in this thesis, in the\norder of appearance.\nC.4.1\nIris Example\nThis section describes the details for the Iris dataset example\nin Figure 2.8 in Section 2.2.3. All models use three layers with\n100 hidden units and ReLU activations each. We furthermore\noptimized all of the models with a learning rate of 0.001 using the\nAdam optimizer (Kingma and Ba, 2015) with its default parameter\nsettings. We also regularize the ensemble and MC Dropout model\nwith a dropout probability of 0.1 each.\nPrior Network Specifics.\nWe choose the expected l2 loss by\nSensoy et al. (2018) and regularize the network using the KL\ndivergence w.r.t. to a uniform Dirichlet as in Sensoy et al. (2018).\nIn the regularization term, we do not use the original concentration\nparameters α, but a version in which the concentration of the\nparameter αk corresponding to the correct class is removed using\na one-hot label encoding y by ˜α = (1 −α) ◦α + y ◦α. The\nregularization term is added to the loss using a weighting factor of\n0.05.\nC.4.2\nSynthetic Data Experiments\nThis sections gives more details on the synthetic data experiments\nin Section 4.1.5. We perform our experiments on the half-moons\ndataset, using the corresponding function to generate the dataset\nin scikit-learn (Pedregosa et al., 2011), producing 500 samples\nfor training and 250 samples for validation using a noise level\nof .125. We do hyperparameter search using the ranges listed\nin Table C.4, settling on the values given in Table C.3 after 200\nC.4 hyperparameters search\n330\nevaluation runs per model (for neural networks and MC dropout;\nthe hyperparameters found for neural networks were then used\nfor Platt scaling, anchored ensembles and neural ensembles as\nwell). We also performed a similar hyperparameter search for the\nBayes-by-backprop (Blundell et al., 2015) model, which seemed\nto not have yielded a suitable configuration even after extensive\nsearch, which is why results were omitted here. All models were\ntrained with a batch size of 64 and for 20 epochs at most using\nearly stopping with a patience of 5 epochs and the Adam optimizer.\nModel\nHyperparameter\nValue\nNeural Network\nHidden size\n[25, 25, 25]\nNeural Network\nDropout prob.\n.014552\nNeural Network\nLearning rate\n.000538\nMC Dropout\nHidden sizes\n[25, 25, 25, 25]\nMC Dropout\nDropout prob.\n.205046\nMC Dropout\nLearning rate\n.000526\nTable C.3: Best hyperparameters found on the half-moon dataset.\nHyperparameter\nChosen from\nHidden layers\n1–5 layers of 15, 20, 25\nLearning rate\nU(log 10−4, log 0.1)\nDropout rate\nU(0.1, 0.5)\nTable C.4: Distributions or options that hyperparameters were sampled\nfrom during the random hyperparameter search.\nC.4.3\nText Classification Experiments\nHere, we detail the hyperparameter search conditions for the exper-\niments in Section 4.2.5. We perform hyperparameter search using\nrandom sampling (Bergstra and Bengio, 2012) using hyperband\nscheduling (Li et al., 2017)90 on the entire training set, even if\nmodels are trained on sub-sampled training sets later. This has\nthe advantage of ensuring comparability between runs and elim-\ninating suboptimal hyperparameter choices as a source of worse\nuncertainty estimation. We do 80 trials for LSTM-based models,\nand 30 for Bert-based models. Furthermore, the hyperparameters\nfor the LSTM are identical for the LSTM ensemble (10 instances\nare used per ensemble). Hyperparameters were picked by best final\nvalidation loss over search trials.\n90Trials might be terminated using hyperband after 10k steps.\nC.4 hyperparameters search\n331\nChosen Hyperparameters.\nWe summarize some common hy-\nperparameters here and show the rest in Table C.6. We commonly\nuse a batch size of 32, and sequence lengths of 35 for LSTM-based\nand 128 for Bert-based models. All LSTM-based models are trained\nusing 2 layers, with the exception of the vanilla LSTM and the\nLSTM-ensemble on Clinc Plus with 3 layers. Their hidden size and\nembedding sizes are set to 650. For all models, gradient clipping\nis set to 10. For models using multiple predictions to compute\nuncertainty estimates, 10 predictions are used at a time.\nName\nTuned for\nSearch space\nLearning rate\nLSTM, LSTM Ensemble,\nBayesian LSTM, ST-τ LSTM\nVariational LSTM\nU(0.1, 0.5)\nLearning rate\nDDU BERT, SNGP BERT,\nVariational BERT\nlog U(10−5, 10−3)\nSpectral norm upper bound\nDDU BERT, SNGP BERT\nU(0.95, 0.99)\nKernel amplitude\nSNGP BERT\nlog U(0.01, 0.5)\nβ weight decay\nSNGP BERT\nlog U(10−3, 0.5)\nWeight decay\nLSTM, LSTM Ensemble,\nST-τ LSTM, Variational BERT\nU(0.1, 0.5)\nLayers\nLSTM, LSTM Ensemble\n{2, 3}\nDropout\nLSTM, LSTM Ensemble,\nST-τ LSTM, Variational BERT\nU(0.1, 0.4)\nLayer Dropout\nVariational LSTM\nU(0.1, 0.4)\nTime Dropout\nVariational LSTM\nU(0.1, 0.4)\nEmbedding Dropout\nVariational LSTM\nU(0.1, 0.4)\nHidden size\nLSTM, LSTM Ensemble\n{350, 500, 650}\nPrior σ1\nBayesian LSTM\nlog U(−0.8, 0.1)\nPrior σ2\nBayesian LSTM\nlog U(−0.8, 0.1)\nPrior π\nBayesian LSTM\nlog U(0.1, 0.9)\nPosterior µ init\nBayesian LSTM\nU(−0.6, 0.6)\nPosterior ρ init\nBayesian LSTM\nU(−8, −2)\nInit weight\nLSTM\nU(0.1, 0.4)\nNumber of centroids\nST-τ LSTM\n{5, 10, 20, 30, 40}\nTable C.5: List of searched hyperparameters. LSTM Ensemble hyper-\nparameters are not searched, but simply copied from the found LSTM\nhyperparameters.\nWe further include some notes about the optimization of models\nfor the experiments in Section 4.2.5. To make sure that all models\nare trained for the same number of steps regardless of the the\nsize of (sub-sampled) training set, we set the training duration to\nthe number of steps corresponding to a number of epochs using\nC.4 hyperparameters search\n332\nthe original training set size, and name it epoch-equivalents in the\nfollowing. Due to the imbalance of classes in Finnish UD and\nDan+, all models were trained using loss-weights that are inverse\nto the frequency of a label in the dataset.\nOptimization of LSTMs.\nWe adopt different optimization\nschemes for transformer- and LSTM-based models. For LSTMs,\nwe choose stochastic gradient descent with a decaying learning rate\nschedule, decaying by .8695 after the equivalent of 14 epochs for\nevery following epoch-equivalent for 55 epoch-equivalents in total.\nThis corresponds to the setup in Gal and Ghahramani (2016a),\nmodified from the setup in Zaremba et al. (2014).\nOptimization of Berts.\nWe fine-tune Bert models using the\nshorter duration of 20 epoch-equivalents, corresponding to the\nNLP experiments in Liu et al. (2023). Adam (Kingma and Ba,\n2015) is used for optimization with default parameters β1 = .9 and\nβ2 = .999 alongside a triangular learning rate, using the first 10%\nof the training duration as warm-up.\nC.4.4\nAuxiliary Calibrator Experiments\nThis sections explains the hyperparameter tuning for the exper-\niments in Section 6.2.2. We conduct suites of hyperparameter\nsearches per target LLM, dataset and type of calibration targets\n(binary and clustering) corresponding to the results in Table 6.3,\nresulting in eight different suites. We then use these found hyper-\nparameters for the results in Table 6.5.\nSearch Method and Ranges.\nFor the search, we opt for\nBayesian hyperparameter search (Snoek et al., 2012) as imple-\nmented by Weights & Biases (Biewald, 2020). We optimize only\ntwo hyperparameters: Learning rate and weight decay. The learn-\ning rate is samples from a log-uniform distribution log U[10−5, 0.01]\nand weight decay from a uniform distribution U[10−4, 0.05] for a\ntotal of 50 runs and 250 training steps each. The final hyperpa-\nrameters selected are given in Table C.7.\nOther Hyperparameters.\nWhen obtaining the responses from\nVicuna v1.5 7B, we use a batch size of 4 and generate for a maximum\nof 50 tokens and stop generation when the model tries to generate\nparts of the prompt, such as “Question:” / “Q:” or “Answer:” / “A:”.\nWe also use 10 in-context samples for TriviaQA, but no in-context\nsamples for CoQA. For the auxiliary calibrator, we use a context\nC.5 pre-processing for text classification benchmark\n333\nsize of 512 tokens, batch size of 32, and gradient clipping with a\nmaximum norm of 10.\nC.5\nPre-processing for Text Classification\nBenchmark\nThis sections explains the data preprocessing for the datasets used\nin Section 4.2.5.\nTokenization.\nWe use the corresponding Bert tokenizer for each\nlanguage, including for LSTM-based models to ensure compatibility.\nFor English, this corresponds to the original SentencePiece tokenizer\nused by Devlin et al. (2019), while we use the tokenizer of the\nDanish Bert (Hvingelby et al., 2020) and Finnish Bert (Virtanen\net al., 2019) for those languages, respectively.\nTags for Sub-Word Tokens.\nFor named entity recognition\nand part-of-speech tagging, we follow Jurafsky and Martin (2022),\nchapter 11.3.3 to deal with sub-word tokens: For every token that\nis split into sub-word tokens, we assign the tag only to the first\nsub-word token, and −100 for the rest, which ignores them for\nevaluation purposes.\nC.6\nImplementation Details of Text\nClassification Benchmark\nThis section gives additional implementation details for the models\nused in the text classification benchmark in Section 4.2.5.\nResources.\nIn addition to the resources in Appendix C.1, the\nBayesian LSTM was developed using the Blitz package (Esposito,\n2020) for PyTorch and the SNGP transformer using gpytorch\n(Gardner et al., 2018).\nModels.\nFor the DUE transformer, we used principal component\nanalysis on the latent representations for Clinc Plus to reduce the\nmemory usage of the Gaussian discriminant analysis by reducing\ndimensionality to 64. We initially also experimented with the usage\nof the DUE transformer by van Amersfoort et al. (2021), however\nfound that it was not trivial to create the inducing points for the\nGaussian process output layer in a sequential setting. For the vari-\national transformer (Xiao et al., 2020), the authors do not specify\nexactly how MC dropout is used. We use the existing dropout\nlayers in the corresponding model, and use a number of forward\nC.7 convergence on clinc plus\n334\npasses with different dropout masks to make predictions. Since\nthe number of classes is prohibitive for the original formulation\nof the SNGP transformer, we use the extension proposed by Liu\net al. (2023) in Appendix A.1 and only store one ˆΣ−1 matrix for all\nclasses. Furthermore, we update the matrix continuously during\ntraining and not just during the last epoch, in order to allow track-\ning of the predictive performance over the training time. Lastly,\nwe also evaluate predictions using Monte Carlo approximations\ninstead of the mean-field approach, since this allows us to compute\na wider variety of uncertainty metrics.\nEvaluation.\nWhen computing uncertainty estimates and losses\nfor evaluation purposes, the measurements for a number of tokens\nwere discarded. These include the ignore token with ID −100,\nas well as the IDs corresponding to the [EOS], [SEP], [CLS] and\n[PAD] token, which might differ between tokenizers of different\nlanguages. For computing the ECE, we use 10 bins.\nModel Comparison.\nWe facilitate the comparison of models\nusing the almost stochastic order test from Section 3.2.1. We use\nthe test with a confidence level α = 0.05 and a decision threshold\nof τ = 0.3.\nC.7\nConvergence on Clinc Plus\nHere, we briefly address the models missing from the English Clinc\nPlus experiments in Section 4.2.5. For the ST-τ and variational\nLSTM, we could not identify clear reasons on why models did\nnot converge. Even after extensive hyperparameter searches and\nmanual fine-tuning of hyperparameters (including different learning\nrate schedules and optimizers), we did not find a combination of\noptions that resulted in convergence. We also observed strange\nbehavior for the Bayesian LSTM, which, after reaching a valida-\ntion accuracy of 0.5, would suddenly return to its initial training\nperformance. This could potentially be explained by the model\naccidentally escaping a low-loss basin due to a learning rate that\nis still too high, and thus we changed the model to only be trained\nfor 18 epoch-equivalents and initiate the learning rate decay after\nseven epoch-equivalents. The puzzling fact is that SNGP Bert did\nnot converge on Clinc Plus, since the authors successfully used\nthe dataset in their own work (Liu et al., 2023). We put forth\nthe following explanations: First of all, we observed the model to\ngenerally possess a high variance, as demonstrated by the standard\ndeviation on the Danish and Finnish data. Secondly, we make at\nleast two changes to their implementation: Instead of using the\nC.8 temperature search\n335\nmean-field approximation to the predictive distribution, we use the\nMonte Carlo approximation in order to compute metrics such as\nmutual information. Also, we update the covariance matrix ˆΣ over\nthe whole training time in order to track the predictive performance\nfor our experiments, and not just during the last epoch.\nC.8\nTemperature Search\nThis sections explains the temperature search procedure for the\nparameter τ in Equation (5.5) in Section 5.3 further. To determine\nthe temperaturein Equation (5.5) for the different distance metrics\nin Table 5.1, we adopt a variation of a simple hill-climbing proce-\ndure. Given user-defined bounds for the temperature search τmin\nand τmax, we sample an initial candidate τ0 ∼U[τmin, τmax], and\nthen evaluate the coverage of the method given the candidate on\nthe first 100 batches of the calibration dataset. The next candidate\nthen is obtained via\nτt+1 = τt + η · ε · sgn\n\u00001 −α −Coverage(τt)\n\u0001\n;\nε ∼N(0, τmax −τmin),\n(C.1)\nwhere η is a predefined step size (in our case 0.1) and Coverage(τt)\nthe achieved coverage given a candidate τt. The final temperature\nis picked after a fixed number of steps (t = 20 in our work) based\non the smallest difference between achieved and desired coverage.\nOverall, we found useful search ranges to differ greatly between\nexperimental settings, as illustrated by the reported values in\nTable 5.1 and Table 5.2. In general, the stochastic hill-climbing\ncould also be replaced by a grid search, even though we sometimes\nfound the best temperature to be “hidden” in a very specific value\nrange. It also has to be noted that temperature for the l2 distance\nis the highest by far since FAISS returns squared l2 distances by\ndefault.\nC.8 temperature search\n336\nModel\nHyperparameter\nEnglish\nDanish\nFinnish\nLSTM\nWeight decay\n.001337\n.001357\n.001204\nLearning rate\n.4712\n.4931\n.2205\nInit. weight\n.2830\n.5848\n.5848\nDropout\n.3379\n.2230\n.1392\nVariational LSTM\nWeight decay\n–\n10−7\n.01953\nLearning rate\n–\n.3031\n.7817\nInit. weight\n–\n.1097\n.5848\nEmbedding Dropout\n–\n.1207\n.3566\nLayer Dropout\n–\n.1594\n.3923\nTime Dropout\n–\n.1281\n.1646\nBayesian LSTM\nWeight decay\n.001341\n.003016\n.03229\nLearning rate\n.1704\n01114\n.1549\nDropout\n.3410\n.3868\n.331\nPrior σ1\n.9851\n.7664\n.3246\nPrior σ2\n.5302\n.851\n.5601\nPrior π\n1\n1\n.1189\nPosterior µ init\n−.005537\n−.0425\n.4834\nPosterior ρ init\n−7\n−6\n.1124\nST-τ LSTM\nWeight decay\n–\n.001189\n.0007857\nLearning rate\n–\n.01979\n.3601\nDropout\n–\n.1867\n.1737\nNum. centroids\n–\n5\n30\nDDU Bert\nLearning Rate\n.003077\n.00006168\n.001825\nSpectral norm upper bound\n.9753\n.9211\n.9410\nWeight decay\n.0039 = 0\n.1868\n.09439\nVariational BERT\nLearning Rate\n.0002981\n.00009742\n.00003483\nWeight decay\n.01591\n.02731\n.09927\nDropout\n.2382\n.4362\n.4364\nSNGP Bert\nLearning Rate\n–\n.0002332\n.0002919\nSpectral norm upper bound\n–\n.99\n.96\nBeta Weight decay\n–\n.001619\n.002438\nBeta length scale\n–\n2.467\n2.254\nKernel amplitude\n–\n.3708\n.2466\nTable C.6: List of used model hyperparameters by dataset.\nTriviaQA\nCoQA\nBinary\nClustering\nBinary\nClustering\nVicuna v1.5\nlearning rate\n1.4 × 10−5\n3.37 × 10−5\n9.58 × 10−5\n8.84 × 10−5\nweight decay\n.03184\n.008936\n.005793\n7.42 × 10−4\nGPT-3.5\nlearning rate\n2.96 × 10−5\n1.62 × 10−5\n5.12 × 10−5\n5.59 × 10−5\nweight decay\n.01932\n.01362\n.03327\n.03495\nTable C.7: Chosen hyperparameters for our model on different datasets\nand for different calibration targets.\nAbbreviations\nk-NN k-nearest neighbors, the idea to use k points most similar\nto a point of interest for purposes such as classification or\nclustering.\nAI Artificial intelligence, the field that develops and studies meth-\nods that enables machines to learn and take actions in a\nway that imitates human intelligence.\nAPI Application programming interface, a way for computer pro-\ngrams to communicate with each other through a specified\ninterfance.\nAPRICOT Auxiliary prediction of confidence targets. Method\nto create calibrated confidence scores for black-box LLMs\nthrough an external secondary model, discussed in Chapter\n6.\nAUPR Area under the precision-recall curve, an evaluation metric\nthat measures the trade-off between the precision and recall\nunder varying decision thresholds for a binary classification\nproblem.\nAUROC Area under the receiver-operator characteristic, an eval-\nuation metric that measures the trade-off between the true\npositive rate and false positive rate under varying decision\nthresholds for a binary classification problem.\nBert Bidirectional encoder representations from transformers by\nDevlin et al. (2019), a transformer-based language trained\nto predict a masked-out token given some context, with\npredicting the order of two subsequent sentences as an\nauxiliary task.\nBLEU Bilingual evaluation understudy, an evaluation metric ini-\ntially proposed by Papineni et al. (2002) to evaluate ma-\nchine translation methods based on the n-gram overlap\nbetween the translation and a reference.\nCOMET Crosslingual optimized metric for evaluation of trans-\nlation, a metric proposed by Rei et al. (2020) that tries\n337\n338\nto predict the quality of a machine-generated translation\nusing a neural model.\nCoT Chain-of-through prompting, a prompting technique orig-\ninally proposed by Wei et al. (2022), where an LLM is\ninstructed to solve a task by performing step-by-step rea-\nsoning.\nCP Conformal prediction, a technique orginally developed by Vovk\net al. (2005) that creates prediction sets (classification) or\nintervals (regression) that include the correct prediction\nwith a pre-defined probability, given that a test point is\nfrom the same distribution as the calibration data used to\nconstruct the prediction sets / intervals.\nDDU Deep deterministic uncertainty transformer Mukhoti et al.,\n2021, a type of transformer for which a Gaussian discrimi-\nnant analysis model is fit on its latent features in order to\nquantify uncertainty.\nDL Deep learning, the field concerned with the study of artificial\nneural network of increased depth. Can be considered a\nsubfield of machine learning.\nFAISS (Meta’s) Fundamental AI similarity search, a software\nlibrary proposed by Johnson et al. (2019) to quickly find\nthe nearest neighbors for a vector in a datastore.\nHDBSCAN Hierarchical density-based spatial clustering of appli-\ncations with noise (Campello et al., 2013), an improvement\non the earlier DBSCAN clustering algorithm (Ester et al.,\n1996). The algorithm is unsupervised, i.e. does not require\na specification of the number of clusters a priori, and works\nby merging points into cluster by distance in a bottom-up\nfashion.\nLLM Large language model or foundation model; typically a\nlarge neural model based on the transformer architecture\n(Vaswani et al., 2017), that has been trained on huge swaths\nof data to model the statistical distribution of words.\nLM Language model or language modeling (i.e. the process or task\nof modeling the statistical distribution of words underlying\nlanguage). More general than LLMs, since language models\ncan also be based on recurrent or n-gram models.\nLSTM Long-short term memory network (Hochreiter and Schmid-\nhuber, 1997), a type of recurrent neural architecture used\nfor sequential data.\nMAUVE Neural metric to assess the quality of machine-generated,\ngeneral text by Pillutla et al. (2021).\n339\nML Machine learning, the subfield of artificial intelligence con-\ncerned with the development and study of statistical algo-\nrithms that can learn from data and generalize to unseen\ndata.\nMT Machine translation, the study or task of automatically trans-\nlating text or speech with the help of computers.\nNLG Natural language generation, a set of tasks involving the gen-\neration of language, including language modeling, machine\ntranslation, question-answering, and image captioning.\nNLP Natural language processing, an interdisciplinary subfield of\nartificial intelligence and linguistics, primarily concerned\nwith providing computers the ability to process data en-\ncoded in natural language.\nOOD Out-of-distribution or out-of-domain, used to refer to test\ninputs to a machine learning model that are different come\nfrom a different distribution than the training data the\nmodel was originally fit on.\nPAC Probably approximately correct learning, a framework for\nthe mathematical analysis of machine learning.\nReLU Rectifier linear unit, a non-linear activation function defined\nas ϕ(x) = max(0, x), often used on the activations between\nneural network layers.\nRLHF Reinforcement learning from human feedback (Christiano\net al., 2017; Stiennon et al., 2020), a technique to optimize\nneural models based on human preference data.\nROUGE An evaluation metric initially proposed by (Lin, 2004) to\ntext summarization methods based on the n-gram overlap\nbetween the summarization and a reference.\nSDE Stochastic differential equation, a type of equation regarding\nthe derivative of some function in which one or more of the\nterms is a stochastic process.\nSNGP Spectrally-normalized Gaussian process transformer (Liu\net al., 2023), a type of transformer architecture whose\nweights are regularized through spectral normalization and\nwhich features a Gaussian process output layer..\nUQ Uncertainty Quantification; methods to assess the reliability or\ntrustworthiness of the predictions of (in this thesis) neural\nmodels.\nIndex\nχ2 distribution, 15\nn-gram, 297, 299\nActive learning, 68\nAmbiguity, 26, 30, 32, 49, 63,\n75, 158, 159\nAttachment, 28\nStructural, 28, 32\nAnomaly detection, 161, 175\nAPRICOT, 141, 142, 148,\n152, 155, 156\nArtificial intelligence, 2, 33,\n64, 79, 170, 171, 174,\n175\nAUPR, 115, 116\nAUROC, 115, 151, 152, 156\nBatch normalization, 44\nBayes’ theorem, see Bayes’\nrule, 19, 21, 41, 52\nBayes-by-backprop, 111, 330\nBayesian model averaging, 51\nBernoulli distribution, 18, 20,\n21, 36, 44, 276\nBert, 114, 116, 117, 119, 173,\n174, 297, 302, 303,\n330–334\nSentence, 147\nBERTscore, 135\nBeta distribution, 20–22, 51,\n278\nBeta function, xvii, 22, 24,\n276\nBIO-tags, 97\nBitter lesson, 171\nBLEU, 135\nBonferroni correction, 80\nBootstrap, 17, 19, 79, 80, 87,\n91, 329\nBrier score, 151\nCalibration, 37, 39, 59, 63,\n114, 116, 141, 146,\n156, 158, 172, 332\nCategorical distribution, 100\ncategorical distribution, 21,\n36, 49, 51\nCentral limit theorem, 15, 19\nchrF, 135\nClass variance, 48, 101, 107,\n295\nClassification, 11, 120\nBinary, 35, 36, 116, 282,\n283\nMulti-class, 36, 58, 97,\n282\nMulti-label, 97\nSequence, 113, 297\nText, 2, 97, 119–121, 331,\n333\nCochran’s theorem, 16\nCOMET, 135\nCompositionality, 27\nComputational efficiency, 4,\n68, 138\nConditional computation, 68\nConfidence, 36, 39, 40, 57, 60,\n62, 66, 68, 98, 106,\n108, 140–143, 150,\n153, 155, 164, 168,\n173, 295\nConfidence interval, 15, 22,\n23, 25\n340\nINDEX\n341\nConformal prediction, 12, 39,\n40, 59, 124, 125, 127,\n130, 156, 158, 165,\n173\nNon-exchangeable, 125,\n139\nSplit, 39\nConformal risk control, 138,\n164\nConjugacy, 21, 52\nConnectionism, 33\nConstituency grammar, 27,\n28\nCoverage, 38, 115, 125, 126,\n130, 131, 133, 135,\n137, 138, 163,\n303–305\nConditional, 166\nMarginal, 166\nCredal sets, 57, 58, 64, 168\nCumulative distribution\nfunction, 85\nEmpirical, 85, 87, 328\nInverse, 16, 86, 328\nCybernetics, 33\nData scarcity, 6, 57, 120, 162,\n163\nDeBERTa, 147\nDeep kernel learning, 47\nDeep learning, 1, 33, 46, 69,\n70, 73, 77, 157, 165,\n325\nEvidential, 49, 50, 53, 57,\n59, 64, 69, 160, 167\nDempster-Shafer metric, 112\nDempster-Shafer theory of\nevidence, 35, 49\nDigamma function, 54, 279\nDirac delta function, xvii, 24,\n49\nDirichlet distribution, 49, 51,\n53–55, 278–282, 329\nDropout, 44, 329, 333\nMonte Carlo, 44, 46, 50,\n51, 110, 111, 329, 333\nEnsemble, see Ensembling\nEnsembling, 38, 46, 50, 51,\n61, 78, 99, 110, 111,\n116, 118, 119, 295,\n302, 303, 329, 330\nEntropy, 279\nCross-, 35, 297\nExpected, 53, 280\nPredictive, 98, 100, 101,\n107, 118, 160, 294,\n295, 303\nSemantic, 61\nShannon, 40, 61, 100\nEnvironmental impact, 325,\n326\nError detection, 67, 141, 151,\n156\nEvidence, 20, 21, 41\nExpected calibration error,\n36, 37, 60, 114, 146,\n151, 166, 334\nSmooth, 151\nExpected coverage gap, 130,\n304, 305\nExperimental design, 8, 71,\n82, 158, 159, 165\nExponential distribution, 278\nExponential families, 21, 278\nFairness, 3, 67, 175\nGamma function, xvii, 21,\n276\nGarden path sentences, 28\nGaussian discriminant\nanalysis, 333\nGaussian distribution, see\nNormal distribution\nGaussian process, 47, 57, 111\nHallucination, 67, 68, 171\nHDBSCAN, 147, 149, 307\nHedging, 30\nHessian, 45\nHighest density interval, 23,\n166\nINDEX\n342\nHomonymy, 26\nHuman label variation, 63, 71\nHypothesis testing, 71, 76,\n80–84, 89, 90, 92, 95,\n114, 159, 160, 165\nHypotheticals, 31\nImprecise probabilities, 57\nIn-context example, see\nIn-context learning\nIn-context learning, 91, 92,\n94, 145, 147, 332\nIndicator function, xvi\nInductive bias, 6, 8, 60,\n160–162\nInformation bottleneck\nprinciple, 161\nInterpretability, 4, 175\nJackknife, 17, 19\nJacobian, 104, 277\nKendall’s τ, 115, 117, 120\nKullback-Leibler divergence,\n43, 55, 281, 329\nLabel smoothing, 37\nLangevin dynamics, 42\nLanguage generation, see\nNatural language\ngeneration\nLanguage modeling, 59, 123,\n125, 129, 133, 135,\n137, 304, 305\nNearest-neighbor, 124\nLaplace approximation, 45\nLaplace distribution, 88\nLarge language model, 7, 12,\n62, 66, 71, 80, 89, 92,\n94, 123, 140–142, 146,\n148, 155–157, 160,\n164, 170–172, 332\nLikelihood, 18, 20, 21, 36, 41,\n50, 52, 58, 60, 81, 276\nSequence, 142, 151, 156,\n162, 307\nLinguistics, 11, 26, 32, 33,\n157, 165\nLipschitz, 6\nLog-partition function, 278\nLog-uniform distribution, 332\nLong-short term memory\nnetwork, 111, 114,\n116, 117, 119, 302,\n303, 330–333\nBayesian, 111, 116, 333,\n334\nST-τ, 111, 116, 334\nVariational, 116, 302, 334\nLow-resource language, 6,\n113, 163, 301\nMachine learning, 1, 64, 67,\n68, 72, 79, 116, 157\nMachine translation, 4, 59,\n76, 123, 125, 129, 131,\n133, 135–137, 156,\n163, 304, 305\nInteractive, 130\nMann-Whitney U test, 87,\n293\nMarkov chain Monte Carlo,\n42, 43, 81, 95\nMAUVE, 135\nMaximum a posterior\nestimate, 22, 23, 41\nMaximum likelihood estimate,\n19, 22, 35, 37, 41\nMixture of experts, 68\nModel cascade, 68\nMonotonicity, 101, 110\nComponent-wise, 105\nComponent-wise strict,\n102, 287, 288\nStrict, 101\nMonte Carlo estimation,\n42–44, 61, 94\nMutual information, 48, 54,\n68, 69, 101, 107, 109,\n118, 282, 295\nINDEX\n343\nNamed entity recognition, 97,\n113\nNatural language generation,\n3, 4, 12, 30, 59, 64,\n68, 121, 123–125, 127,\n156, 163\nNatural language processing,\n2, 7, 26, 33, 36, 59,\n66, 67, 69, 70, 72, 89,\n99, 111, 157, 165, 173,\n332\nNatural parameter, 278\nNeural network, xvi, 33, 37,\n41, 56, 68, 80, 81, 84,\n98\nBayesian, 40, 43\nFrequentist, 35, 60\nInterval, 58\nNon-conformity score, 39,\n125, 127, 128, 135,\n138\nNon-exchangeable conformal\nlanguage generation,\n141, 163\nNormal distribution, 16, 21,\n41, 43, 45, 52, 80, 85,\n278\nNormalizing flow, 56, 162\nNucleus sampling, 130,\n134–136\nConformal, 130, 133–136\nNon-exchangeable\nconformal, 124, 130,\n136\nNull hypothesis, 83, 84, 87\nOOD detection, 98–100, 116\nOptimal transport, 85\nOut-of-distribution data, 7,\n47, 51, 55, 56, 67, 98,\n99, 113, 119, 156, 160,\n161, 163, 298\nFar, 113\nNear, 113\nParaphrasticity, 30, 32, 60,\n71, 162, 165, 168\nPart-of-speech tagging, 97,\n113, 333\nPermutation-randomization\ntest, 87\nPhonetics, 26\nPhonology, 26\nPlatt scaling, 151, 307, 330\nPoisson distribution, 21, 278\nPolysemy, 26, 32\nPolytope, 102, 104, 107, 283,\n284\nPartially-unbounded, 101,\n102, 105, 286, 288,\n289, 291\nPosterior distribution, 20–22,\n24, 35, 50, 56, 166,\n276\nPosterior network, 55–57, 162\nPragmatics, 26, 29, 32\nPrediction set, 38, 39, 115,\n116, 124, 125, 133,\n135, 158, 163, 166,\n303–305\nAdaptive, 125, 128\nPredictive distribution, 23,\n163\nPosterior, 24, 33, 35, 41,\n44, 48, 61\nPrior, 24\nPrincipal component analysis,\n333\nPrior distribution, 20, 21, 41,\n50, 56, 81, 276\nPrior network, 50, 51, 55, 329\nPrompting, 61–63, 91, 92,\n145, 160\nChain-of-thought, 143,\n145, 151, 154, 155\nProper scoring rule, 37\nQuantile function, see\nCumulative\ndistribution function,\nInverse\nINDEX\n344\nQuestion-answering, 59, 66,\n71, 90, 123, 141, 156\nRayleigh distribution, 88\nReinforcement learning from\nhuman feedback, 63,\n172\nReLU, 97, 99, 100, 120, 158,\n160, 286, 289, 291\nReparameterization trick, 43\nReplicability, 71–73, 77, 79\nReproducibility, 71–73, 79,\n292, 325\nROUGE, 90, 91, 149, 152\nRényi divergence, 55\nScientific method, 71, 72\nSemantics, 26, 30, 32\nSequence classification, 97\nSequence labeling, 97\nShannon entropy, xvii\nShapley value, 167\nShift\nCovariate, 99\nDistributional, 3, 49, 57,\n66, 76, 99, 113, 125,\n126, 133, 138, 156\nSigmoid function, xvi, 35,\n100, 151, 282\nSignificance testing, see\nHypothesis testing\nSize-stratified coverage, 131,\n133, 304, 305\nSoftmax function, xvi, 36, 52,\n100, 102, 103, 106,\n108, 112, 282–285\nStandard error, 15, 91\nStatistical power, 76\nStatistics, 33, 157, 165\nBayesian, 11, 19, 23, 25,\n158\nFrequentist, 11, 14, 20,\n25, 35, 158\nStochastic differential\nequation, 56\nStochastic order, 85, 328\nAlmost, 71, 85, 87, 89, 91,\n92, 114, 151, 160, 293,\n327, 334\nStudent’s-t distribution, 15,\n16, 19\nStudent’s-t test, 80, 87, 293\nSufficient statistic, 278, 279\nSyntax, 26, 27, 32\nText classification, 158, 163\nText summarization, 123, 156\nTime series, 6\nTop-k sampling, 135\nTotal variation distance, 126\nTransformer, 110, 114, 173,\n332\nDDU, 112, 118, 302\nDUE, 333\nSentence, 149\nSNGP, 111, 116, 302, 333,\n334\nVariational, 111, 116, 118,\n303, 333\nTriangle of reference, 30\nTrust, 3, 64, 66, 69, 140, 157,\n165\nExtrinsic, 65\nHuman-AI, 65, 157\nInterpersonal, 64\nIntrinsic, 65\nType I error, 87, 88, 92, 292,\n293\nType II error, 80, 88, 92, 293\nUncertainty, 13, 26–30, 32,\n33, 57, 59, 62, 70, 71,\n108, 114, 118, 156,\n157, 294, 295\nAleatoric, 25, 51, 53, 54,\n59, 61, 78, 118, 120,\n158, 161, 167, 303\nCommunication of, 66,\n169\nConditional, 31\nDistributional, 49, 53, 54,\n69, 158, 282\nINDEX\n345\nDoxastic, 31\nDynamic, 32\nEpistemic, 25, 31, 47, 48,\n53, 54, 59, 61, 68, 75,\n78, 101, 109, 118, 158,\n161, 162, 167, 295,\n303\nInvestigative, 31\nLinguistic, 167\nNon-epistemic, 31\nParadoxical, 31\nPredictive, 23\nSemantic, 31\nTotal, 101, 118, 159\nVerbalized, 62, 63, 66,\n142, 144, 145, 151,\n153, 154, 156, 159,\n168, 169, 172, 307\nUncertainty metric, 40, 47,\n53, 58, 60, 100, 107,\n112, 117, 118, 121,\n334\nUncertainty quantification, 2,\n35, 47, 49, 57, 58, 63,\n65, 67, 119, 120, 157,\n160, 161, 165, 171,\n173, 175\nUnderspecification, 26, 27, 30,\n63, 158\nModel, 111, 171\nUniform distribution, 332\nVagueness, 26, 27, 158\nVariability, 30, 60, 63\nVariation ratio, 47\nVariational inference, 43, 45,\n99, 281\nWilcoxon signed-rank test, 80,\n87\nWord sense disambiguation,\n26\n",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "published": "2024-10-04",
  "updated": "2024-10-04"
}