{
  "id": "http://arxiv.org/abs/2410.03706v1",
  "title": "Topological Foundations of Reinforcement Learning",
  "authors": [
    "David Krame Kadurha"
  ],
  "abstract": "The goal of this work is to serve as a foundation for deep studies of the\ntopology of state, action, and policy spaces in reinforcement learning. By\nstudying these spaces from a mathematical perspective, we expect to gain more\ninsight into how to build better algorithms to solve decision problems.\nTherefore, we focus on presenting the connection between the Banach fixed point\ntheorem and the convergence of reinforcement learning algorithms, and we\nillustrate how the insights gained from this can practically help in designing\nmore efficient algorithms. Before doing so, however, we first introduce\nrelevant concepts such as metric spaces, normed spaces and Banach spaces for\nbetter understanding, before expressing the entire reinforcement learning\nproblem in terms of Markov decision processes. This allows us to properly\nintroduce the Banach contraction principle in a language suitable for\nreinforcement learning, and to write the Bellman equations in terms of\noperators on Banach spaces to show why reinforcement learning algorithms\nconverge. Finally, we show how the insights gained from the mathematical study\nof convergence are helpful in reasoning about the best ways to make\nreinforcement learning algorithms more efficient.",
  "text": "Topological Foundations of Reinforcement Learning\nKrame Kadurha David\ndavid.krame@aims-cameroon.org\nAfrican Institute for Mathematical Sciences (AIMS)\nCameroon\nSupervised by: Dr. Ya´e U. Gaba\nAIMS-Research & Innovation Center (AIMS RIC)\nKigali, Rwanda\n18 May 2024\nSubmitted in Partial Fulfillment of a Structured Masters Degree at AIMS-Cameroon\narXiv:2410.03706v1  [cs.LG]  25 Sep 2024\nAbstract\nThe goal of this work is to serve as a foundation for deep studies of the topology of state, action, and\npolicy spaces in reinforcement learning. By studying these spaces from a mathematical perspective, we\nexpect to gain more insight into how to build better algorithms to solve decision problems. Therefore,\nwe focus on presenting the connection between the Banach fixed point theorem and the convergence of\nreinforcement learning algorithms, and we illustrate how the insights gained from this can practically help\nin designing more efficient algorithms. Before doing so, however, we first introduce relevant concepts\nsuch as metric spaces, normed spaces and Banach spaces for better understanding, before expressing\nthe entire reinforcement learning problem in terms of Markov decision processes. This allows us to\nproperly introduce the Banach contraction principle in a language suitable for reinforcement learning,\nand to write the Bellman equations in terms of operators on Banach spaces to show why reinforcement\nlearning algorithms converge. Finally, we show how the insights gained from the mathematical study\nof convergence are helpful in reasoning about the best ways to make reinforcement learning algorithms\nmore efficient.\nKeywords : Markov Decision Process; Reinforcement Learning; Contraction mapping; Fixed Point;\nBanach Space; Bellman Operators; Q-Learning.\nDeclaration\nI, the undersigned, hereby declare that the work contained in this essay is my original work, and that\nany work done by others or by myself previously has been acknowledged and referenced accordingly.\nKrame Kadurha David, 18 May 2024.\ni\nContents\nAbstract\ni\nList of Figures\niii\n1\nIntroduction\n1\n2\nMetric spaces and the Banach fixed point\n2\n2.1\nComplete metric spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.2\nContraction mapping and fixed-point . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nApplication of the Banach Contraction Principle - An example . . . . . . . . . . . . . .\n7\n3\nOverview on Reinforcement Learning\n11\n3.1\nMarkov Decision Processes in the Reinforcement Learning setting\n. . . . . . . . . . . .\n11\n3.2\nOptimal Policies and Optimal Value Functions . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.3\nSolving Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4\nBellman Operators and Convergence of RL algorithms\n25\n4.1\nRefinement of the Banach Contraction Principle . . . . . . . . . . . . . . . . . . . . . .\n25\n4.2\nBellman Optimality Operators\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.3\nBellman Expectation Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.4\nPolicy evaluation and iteration in operators setting\n. . . . . . . . . . . . . . . . . . . .\n30\n5\nContributions, implementations and analysis\n32\n5.1\nAlternatives to the Bellman Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.2\nPractical implementation and analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n6\nConclusion\n42\nAcknowledgements\n43\nReferences\n46\nii\nList of Figures\n2.1\nNumerical solution to x′(t) = 1/2.x(t) −t using the Banach Contraction Principle\n. .\n10\n3.1\nAgent-environment interaction in an MDP . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.2\nPolicy iteration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.3\nValue Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n5.1\nThe 3 operators on MountainCar environment.\n. . . . . . . . . . . . . . . . . . . . . .\n39\n5.2\nThe 3 operators on CartPole environment. . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.3\nThe 3 operators on Acrobot environment. . . . . . . . . . . . . . . . . . . . . . . . . .\n40\niii\n1. Introduction\nThere is not much work on the foundations of Reinforcement Learning (RL). Especially from a topological\npoint of view. This work is, to the best of our knowledge, the first attempt to unify the basic concepts of\nReinforcement Learning in a coherent and well-ordered way, specifying all the underlying mathematical\nconcepts.\nSome researchers have instead been interested in related questions, trying to explore RL concepts from a\nmathematical point of view, with the aim of improving the effectiveness of RL algorithms. For example:\nIn the paper [12], the authors studied the geometric and topological properties of the value functions in\nMarkov decision processes with finite number of states and actions. Their characterization of the value\nfunction space as a general polytope provides an understanding of the relationships between policies and\nvalue functions. This perspective enhances the theoretical understanding of RL dynamics. The authors\nof [5] also explored the concept of Lipschitz continuity in model-based RL, emphasizing the importance\nof learning Lipschitz continuous models for efficient value function estimation. This establishes some\nerror bounds and demonstrates the benefits of controlling the Lipschitz constant in RL setting. The work\ndone in [34] extends model-free RL algorithms to continuous state-action spaces using a Q-learning-\nbased approach, and that advancement opens avenues for applying RL techniques to a broader range of\nreal-world applications with continuous state spaces. A very deep study is then done in [21], introducing\na unified formalism for defining state similarity metrics in RL. The authors claim to address the challenge\nof generalization in continuous-state systems by establishing hierarchies among metrics and examining\ntheir implications on RL algorithms.\nThe aforementioned works can be difficult to understand or even less precise if the foundations are not\nwell clarified. Therefore, the main goal of our work is to serve as a first stone in studying and organising\nthe basic concepts, while presenting Reinforcement Learning in a simple and concise way, to help future\nresearchers to quickly dive into the foundations of RL and to study more deeply, in particular the\ntopology of state, action, and policy spaces. By studying these spaces from a mathematical perspective,\nwe expect to gain more insight into how to build better algorithms to solve decision problems.\nAt the end of this work, we even illustrate the latter assumption by showing that, with the mathematical\ninvestigation we have done, we are able to make very strong and general conjectures which merit to be\ntaken seriously into account. This work is organized then as follows :\nAfter this introduction, in chapter 2, we discuss metric spaces and their completeness, as well as the\nBanach Fixed Point Theorem. We end the chapter giving an application of the Banach Fixed Point\nTheorem to a concrete example.\nIn chapter 3, we formally present the Markov Decision Process framework and directly after that, we\npresent the concept of optimality in Reinforcement Learning. At the end, we then discuss some methods\nwhich are at the core of how optimality is attained in Reinforcement Learning.\nIn chapter 4, we express reinforcement in term of Operators to have a simple way of proving why\nReinforcement Learning Algorithms work, using the Banach Contraction Principle. In fact, this chapter\ndiscusses about Bellman Operators and shows how the optimality can be reached using operators’\nlanguage.\nFinally, before the final conclusion, in chapter 5 we present some limitations of the classical Bellman\noperator and, using some insights gained from the mathematical investigation done before, we propose\nan alternative to the classical Bellman operator which shows good behaviour in terms of optimality and\nefficiency.\n1\n2. Metric spaces and the Banach fixed point\nIn this chapter, we present the fundamental concepts of metric spaces and explore their completeness,\na crucial property in understanding the behavior of sequences within these spaces. We also study the\nBanach fixed point theorem, a result which establishes the existence and uniqueness of fixed points\nfor mappings in complete metric spaces. We elucidate the significance of this theorem in providing\na theoretical foundation for iterative algorithms and solution techniques in various mathematical and\ncomputational contexts. Furthermore, we culminate our discussion by illustrating the practical relevance\nof the Banach Fixed Point Theorem through a concrete example, demonstrating its applicability in solving\nreal-world problems and providing insight into its computational implications.\n2.1\nComplete metric spaces\nMetrics and metric spaces provide a structured framework for precisely measuring the relationship be-\ntween elements within a set. This facilitates deeper insights into their interconnections and dependencies,\nenabling pattern recognition, association identification, and informed decision-making across diverse do-\nmains. In this section, we delve into complete metric spaces, which offer a comprehensive framework\nfor understanding the entirety of a set’s elements and their interrelations.\n2.1.1 Definition (Metric and metric space). A metric on a set M is a function d : M × M →R, such\nthat for x, y, z ∈M:\n(i) [Non-negativity] d(x, y) ≥0 and d(x, x) = 0\n(ii) [Identity of indiscernibles] d(x, y) = 0 =⇒x = y\n(iii) [Symmetry] d(x, y) = d(y, x)\n(iv) [Triangle Inequality] d(x, y) ≤d(x, z) + d(z, y)\nThe pair (M, d) is called a metric space.\nLet’s now give some interesting examples of metric spaces.\n2.1.2 Example.\n1) Let M = Rn be the set of real n-uplets. For x = (x1, x2, · · · , xn) , y = (y1, y2, · · · , yn) and\nz = (z1, z2, · · · , zn) in Rn let’s define :\ndp(x, y) =\n\u0010\nn\nX\ni=1\n|xi −yi|p\u00111/p\nwith p ≥1\nThe conditions (i), (ii) and (iii) hold due to the properties of the absolute value and the triangle\ninequality also holds because it is the result of the Minkowsky’s inequality [19, 20].\nIndeed\nassuming that ai = xi −zi and bi = zi −yi. We can write the triangle inequality as follows :\n\u0010\nn\nX\ni=1\n|ai + bi|p\u00111/p\n≤\n\u0010\nn\nX\ni=1\n|ai|p\u00111/p\n+\n\u0010\nn\nX\ni=1\n|bi|p\u00111/p\n(2.1.1)\n2\nSection 2.1. Complete metric spaces\nPage 3\nSo,\n\u0010\nRn, dp\n\u0011\nis a metric space. When p = 1 we get the manhattan distance, if p = 2 we have\nthe classical euclidian distance. But one case which is for a particular interest in this work is the\ncase when p →∞. We get the infinity distance :\nd∞(x, y) = max\n1≤i≤n |xi −yi|\n(2.1.2)\n2) Let S be any non-empty set and B(S), the space of all bounded real-valued functions on S.\nConsider f and g in B(S), and define :\nd(f, g) = sup\nx∈S\n|f(x) −g(x)|\n(2.1.3)\nThe pair\n\u0010\nB(S), d\n\u0011\nis a metric space. This metric is called the supremum metric or uniform\nmetric [20].\nHaving presented some interesting examples of metric spaces, we give some definitions and propositions,\nnecessary to introduce the completeness of metric spaces.\n2.1.3 Definition (Sequences). A sequence in a metric space (X, d) is a function u : N →X. It can be\nwritten {un}n∈N or just {un}.\n2.1.4 Definition (Convergence). A sequence {xn} of points in (X, d) is said to be convergent if there\nis a point x∗∈X such that, for any ε > 0, there exists a positive integer N0 such that d(xn, x∗) < ε\nwhenever n > N0. The point x∗is termed the limit of the sequence and we write xn →x∗.\n2.1.5 Proposition. The limit of a sequence in a metric space is unique.\nProof. The proof of that can be seen in [31]. It leverages the property that metric spaces belong to the\nfamily of T2 (or Hausdorff) topological spaces\nCauchy sequences play a fundamental role in the study of completeness within metric spaces. A sequence\nis considered Cauchy if, for any arbitrarily small positive distance, there exists a point in the sequence\nbeyond which all subsequent points are within that distance from each other. Completeness, on the\nother hand, refers to the property of a metric space wherein every Cauchy sequence converges to a\nlimit within the space itself. Together, Cauchy sequences and completeness provide essential tools for\nunderstanding the convergence behavior and structural integrity of metric spaces.\n2.1.6 Definition (Cauchy sequences). A sequence {xn} in a metric space\n\u0010\nX, d\n\u0011\nis called a Cauchy\nsequence if for any given ε > 0, we can find N0 ∈N such that whenever min{m, n} > N0 we have\nd(xm, xn) < ε.\nWhile every convergent sequence is a Cauchy sequence, the reverse of this is only true in ‘complete\nmetric spaces’.\n2.1.7 Definition (Completeness). The metric space\n\u0010\nX, d\n\u0011\nis said complete if every Cauchy sequence\nconverges in X.\n2.1.8 Example. Using the standard metric (absolute value), R is complete. Using also the associated\nstandard metric, Rn is complete [20].\nSection 2.1. Complete metric spaces\nPage 4\n2.1.9 Definition (Uniformly Cauchy sequence of functions). We say that a sequence fn : X →R is\nuniformly Cauchy if for a given ε > 0, there exists N ∈N with the following property:\n|fn(x) −fm(x)| < ε for all x ∈X and for all m, n ≥N.\nNormed spaces are a subset of metric spaces, where vectors have associated norms. They are useful\nbecause they provide a framework for measuring distances and magnitudes in vector spaces, aiding in\nthe analysis of mathematical structures.\n2.1.10 Definition (Norms and normed spaces). Let’s V be a linear space over a field K. A norm on\nV is a map || · || : V × V →R+ such that for any x, y ∈V and α ∈K :\n(i) [Positivity]\n||x|| > 0 for every x ̸= 0\n(ii) [Homogeneity]\n||αx|| = |α|||x||\n(iii) [Triangle inequality]\n||x + y|| ≤||x|| + ||y||\nThe pair\n\u0010\nV, || · ||\n\u0011\nis called a normed space.\n2.1.11 Remark. While normed spaces focus on vector spaces and their associated norms, metric spaces\nprovide a broader context for studying distances and relationships between points in arbitrary sets. If\n|| · || is a norm, we can get a metric d from it by doing d(x, y) = ||x −y||. This is called an induced\nmetric.\n2.1.12 Example. Let X be any nonempty set. Let us consider the linear space B(X) of all bounded\nreal valued functions on X under the sup norm ||·||∞. In B(X), a sequence {fn} is Cauchy if and only\nif it is uniformly Cauchy.\nProof. Detailed proof in [38].\nBanach spaces are a special class of normed vector spaces that are also complete metric spaces. This\nproperty of completeness ensures that every Cauchy sequence in the space will converge to a limit within\nthe space itself, making Banach spaces crucial in various areas of analysis and functional analysis. Let\nnow define the concept of Banach Space which will be important in the last chapter.\n2.1.13 Definition. A Banach space is a complete normed space.\n2.1.14 Example. The normed space (B(X), || · ||∞) as defined in the Example 2.1.12 is a Banach\nspace.\nProof. The proof can be found in [20]. We here just sketch it.\nLet {fn} ∈B(X) be a Cauchy sequence. By Example 2.1.12, the sequence {fn} is uniformly Cauchy.\nLet’s fix x ∈X and consider the sequence of scalars {fn(x)}. Since R is complete, the sequence\nconverges to a real number f(x) ∈R.\nFirst, we have to show that f is bounded on X and,\nSecondly we have to show that {fn} converges uniformly to f on X. That will complete the proof.\nMetric spaces are not always complete. We here present a way to make complete an incomplete metric\nspace.\nSection 2.2. Contraction mapping and fixed-point\nPage 5\n2.1.15 Completion of Metric Spaces [20, 31]. Let (X, d) be a non-complete metric space. It is\nalways possible to complete the space into a larger space in such a way that every Cauchy sequence in X\nhas a limit in the completion. To do this, we should add new points to (X, d) and extend d to all these\nnew points so that each non-convergent Cauchy sequences in X find limits among these new points.\n2.1.16 Definition (Completion). Let (X, d) be a metric space. A complete metric space (X∗, d∗) is\nconsidered as a completion of the metric space (X, d) if :\n(i) X is a subspace of X∗\n(ii) Every point of X∗is the limit of some sequence in X\n2.1.17 Theorem. Let (X, d) be a metric space. Then there exists a completion of (X, d).\nProof. For the proof, see [20].\nLet us now present the contraction mapping and the Banach Contraction Principle. Those are the core\nconcepts that we will use throughout the work.\n2.2\nContraction mapping and fixed-point\n2.2.1 Definition. Let X be an nonempty set and f : X →X be a mapping on that set :\n- A point x is said to be a fixed point of f if f(x) = x.\n- We will write Fix(f) = {x ∈X : f(x) = x} the set of fixed points of f on X.\n2.2.2 Proposition. Let X be a nonempty set and f : X →X a mapping defined on it. If x ∈X is a\nunique fixed point of fn with fn = f ◦f ◦· · · ◦f\n|\n{z\n}\nn−times\nfor any n > 1, then it is the unique fixed point of f\nand reversely :\nFix(fn) = {x} ⇐⇒Fix(f) = {x}\nProof. Let x be a fixed point for fn in X. Then we can write :\nf(x) = f (fn(x)) = fn+1(x) = fn (f(x)) =⇒\nf(x) is a fixed point of fn\nSince x is the only one fixed point for fn, then f(x) = x, i.e. x is also a fixed point for f. Let’s now\nshow that the aforementioned fixed point is unique also for f :\nLet’s take y ̸= x another fixed point of f, and compute y = f(y) = f2(y) = · · · = fn(y) or\nfn(y) = y ⇐⇒y = x. Then, we’ve clearly seen that x is the unique fixed point for f.\nBefore stating the Banach Contraction Principle, let’s define all the important notions related to that\nspecific theorem.\n2.2.3 Definition. [13, 14, 26, 29] Let (X, d) be a metric space and f : X →X a mapping on X. f is\nsaid to be Lipschitz continuous if there exist α > 0 such that :\nd (f(x), f(y)) ≤α · d(x, y)\n∀x, y ∈X\n(2.2.1)\nSection 2.2. Contraction mapping and fixed-point\nPage 6\n- If α ∈[0, 1), f is said to be a contraction.\n- If α = 1 then f is said non-expansive.\n- If d (f(x), f(y)) < d(x, y), ∀x ̸= y then f is contractive.\n2.2.4 Proposition. Let (X, d) be a metric space and f : X →X a contraction mapping with α ∈(0, 1).\nIf f has a fixed point, that point is unique.\nProof. Suppose we have two fixed points x and y for f with always x ̸= y. Because f is a contraction,\nwith α ∈(0, 1) we can write :\n0 ̸= d(x, y) = d (f(x), f(y)) ≤α · d(x, y),\nwhich is impossible (contradictory). So, the fixed point is unique.\nAs we have now a formal definition of fixed point, contractions and some obvious implications, let’s now\nstate the Banach Contraction Principle.\n2.2.5 Theorem (Banach Contraction Principle : BCP [37]). Let (X, d) be a complete metric space,\nf : X →X a contraction. Then f has a unique fixed point x∗and for each x ∈X, lim\nn→∞fn(x) = x∗.\nMoreover,\nd (fn(x), x∗) ≤\nαn\n1 −αd (x, f(x)) .\n.\nProof. Let us construct a sequence {xn}, starting on x0 and using the following recurrence :\nxn = f(xn−1) ∀n ∈N\ni.e.\nxn = fn(x0)\nWe show that {xn} is a Cauchy-Sequence. In fact, since f is a contraction, we should have :\nd(xm+1, xm) = d (f(xm), f(xm−1)) ≤α · d(xm, xm−1)\n= α · d (f(xm−1), f(xm−2)) ≤α2 · d(xm−1, xm−2)\n...\n= αm−1 · d (f(x1), f(x0)) ≤αm · d(x1, x0)\n(2.2.2)\nNow, for any given m and n, positive and with n < m, the triangle inequality applied recursively gives :\nd(xm, xn) ≤d(xm, xm−1) + d(xm−1, xm−2) + · · · + d(xn−1, xn)\n≤\n\u0000αm−1 + αm−2 + · · · + αn\u0001\n· d(x1, x0)\nfrom 2.2.2\n≤αn \u0000αm−n−1 + αm−n−2 + · · · + 1\n\u0001\n· d(x1, x0)\n≤\nαn\n1 −α · d(x1, x0)\n(2.2.3)\nAs we can now see from lim\nn→∞αn = 0 and d(x1, x0) is fixed, we can say d(xm, xn) →0 as m, n →∞.\nSo, {xn} is a Cauchy sequence and since X is complete, there exits only one x∗∈X such that xn →x∗.\nSo, by continuity of contractions, we can write :\nx∗= lim\nn→∞xn+1 = lim\nn→∞f(xn) = f\n\u0010\nlim\nn→∞xn\n\u0011\n= f\n\u0010\nlim\nn→∞fn(x0)\n\u0011\n= f(x∗)\nSection 2.3. Application of the Banach Contraction Principle - An example\nPage 7\nAnd the proposition above (Proposition 2.2.2), we have a way to find this unique fixed point indepen-\ndently on the starting point.\nAnd to show a good estimate of how we are approaching the limit we can rewrite the Expression 2.2.3\nlike this :\nd (fn(x), x∗) ≤\nαn\n1 −αd (x, f(x))\n≡\nd (xn, x∗) ≤\nαn\n1 −αd (x0, x1)\n2.2.6 Remark. The beauty of the Banach Contraction Principle is that it only require completeness\nfor the metric space and contraction property on the mapping. Also, we can get closer and closer to\nthe fixed point, starting from any point x ≡x0 ∈X by just applying recursively the mapping on that\nspecific point.\n2.3\nApplication of the Banach Contraction Principle - An example\nThe Banach Fixed Point principle or Banach Contraction Principle (BCP) is very important in many\nfields, in mathematics as well as in its applications [3, 24, 38, 43]. We here introduce a problem whose\nsolution is guaranteed by the Picard-Lindelof Theorem, the uniqueness of solutions of a first order\nOrdinary Differential Equation (ODE). We show that its solution can be obtained using the BCP.\nLet’s say we have the following initial value problem (which is here an ODE with initial conditions) :\ny′ = F(x, y)\n;\ny(x0) = y0\nwith y′ = dy\ndx\n(2.3.1)\nThis problem is in fact equivalent to the following :\ny(x) −y(x0) =\nZ x\nx0\nF\n\u0010\nt, y(t)\n\u0011\ndt as we have y0 = y(x0)\n⇒y(x) = y0 +\nZ x\nx0\nF\n\u0010\nt, y(t)\n\u0011\ndt\n(2.3.2)\nSo, the two problems 2.3.1 and 2.3.2 are exactly the same. And from this last Expression 2.3.2 we can\ndefine a sequence given by :\nyn+1(x) = y0 +\nZ x\nx0\nF\n\u0010\nt, yn(t)\n\u0011\ndt\nwith y0 = yn(x0) ∀n.\n(2.3.3)\nThe problem can be seen as demonstrating the Picard-Lindelof Theorem which states the following :\n2.3.1 Theorem (Picard-Lindelof Theorem [27]). Let D ⊂R × Rn be an open set, F : D →Rn a\ncontinuous function and (x0, y0) ∈D. Then the initial value problem\ndy\ndx = y′ = f(x, y),\ny(x0) = y0\nhas a unique solution y(x) on closed intervals I = [x0 −ε, x0 + ε] with ε > 0.\nTo easily show the usefulness of the BCP for the problem 2.3.1, we can enunciate the Picard-Lindelof\nTheorem in especially the context of that problem as follows and then prove the existence and uniqueness\nof the solution :\nSection 2.3. Application of the Banach Contraction Principle - An example\nPage 8\n2.3.2 Proposition. [43]\nAssume that :\n1. With S a rectangle domain defined as follows\nS = {(x, w) ∈R2 : |x −x0| ≤a and |w −y0| ≤b} with a, b ∈R and w = y(x)\nThe function F : S →R, given in the ODE 2.3.1 above, is continuous, and there exists c ∈R+\nsuch that |F(x, y)| ≤c.\n2. F(x, y) satisfies the Lipschitz condition with respect to y on S, i.e. there exists L ≥0 such\nthat :\n|F(x, y1(x)) −F(x, y2(x))| ≤L · |y1(x) −y2(x)|,\nfor all (x, y1), (x, y2) ∈S\n3. The real number h satisfies : 0 < h ≤a,\nhc ≤b,\nhL ≤1\nThe following holds :\n(i) The sequence {yn} constructed in Relation 2.3.3 converges to a certain function y∗.\n(ii) The initial value problem stated in 2.3.1 and equivalently in its integrale form given by the Ex-\npression 2.3.2 has a unique solution which is exactly y∗.\n(iii) For n ∈N and k := hL we have the following estimates :\n• ||yn −y∗||∞≤\nkn\n1 −k · ||y1 −y0||∞see the last Expression 2.2.3.\n• ||yn+1 −y∗||∞≤\nk\n1 −k · ||yn+1 −yn||∞\nWe will not prove this proposition explicitly. We will follow a process, inspired with the assumptions\nmade in the Proposition 2.3.2 and that will help us to demonstrate immediately (i) and (ii). The point\n(iii) about the estimates will follow immediately, due to the Expression 2.2.3.\nProof. Let R be the rectangular domain :\nR = {(x, w) ∈R2 : |x −x0| ≤h and |w −y0| ≤h.c} with h, c ∈R and w = y(x)\nFollowing the assumptions made for this Proposition, we know that 0 < h ≤a,\nhc ≤b, i.e. R ⊂S.\nLet X be the set of all real-valued continuous functions y = y(x) on [x0 −h, x0 + h]. Then X is a\nclosed subset of the normed space C ([x0 −h, x0 + h]) with the sup norm || · ||∞.\nLet’s now define an operator (a functional) like this (inspired by the second formulation of our problem,\ngiven in 2.3.2) :\nT : X →X\ng(x) 7→h(x) = Tg = y0 +\nZ x\nx0\nF\n\u0010\nt, g(t)\n\u0011\ndt\n(2.3.4)\nSince\nd\n\u0010\nh(x), y0\n\u0011\n= sup\nx\n\f\f\f\f\nZ x\nx0\nF\n\u0010\nt, g(t)\n\u0011\ndt\n\f\f\f\f =\n\f\f\f\f\n\f\f\f\f\nZ x\nx0\nF\n\u0010\nt, g(t)\n\u0011\ndt\n\f\f\f\f\n\f\f\f\f\n∞\n≤c(x −x0) ≤c · h ≤b,\nSection 2.3. Application of the Banach Contraction Principle - An example\nPage 9\nwe can say that the operator T in 2.3.4 is well defined.\nNow, we have to prove that this operator is a contraction mapping. For that, let’s take g, g1 ∈X. We’ll\nhave :\nd (Tg, Tg1) = d(h, h1) =\n\f\f\f\f\n\f\f\f\f\nZ x\nx0\n\u0010\nF (t, g(t)) −F (t, g1(t))\n\u0011\ndt\n\f\f\f\f\n\f\f\f\f\n∞\n≤\nZ x\nx0\n\f\f\f\n\f\f\f\n\u0010\nF (t, g(t)) −F (t, g1(t))\n\u0011\f\f\f\n\f\f\f\n∞dt\n≤L ·\nZ x\nx0\n|g(t) −g1(t)| dt\nusing the second assumption.\n≤Lh · d (g, g1) = k · d (g, g1) .\n(2.3.5)\nIn 2.3.5, from the third assumption, we know that k < 1, then the operator T is a contraction.\nHence now T is a contraction mapping and X is a closed subset of a metric space, X is also a complete\nmetric space and by the Banach Fixed Point principle, T has one unique fixed point g which is the\nexactly the unique solution to the given ODE.\n2.3.3 Example. Consider the following first order initial value problem\nx′(t) = 1\n2x(t) −t,\nx(0) = 0.\nFrom the Picard-Lindelof Theorem above, we are sure that we’ll get a solution, and to converge to it, we\nstarted with a random function and used an iterative approach, as suggested by the Banach contraction\nprinciple that we used to prove the theorem (through the Proposition 2.3.2). We obtained the following\nresults :\nSection 2.3. Application of the Banach Contraction Principle - An example\nPage 10\nFigure 2.1: Numerical solution to x′(t) = 1/2.x(t) −t applying insights from the Banach Fixed Point\nTheorem\nIn the Figure 2.1 we can see that the algorithm converges quickly to the true function (just after 5\niterations on [0, 4]. To make it work, we implemented the operator 2.3.4 and used the same kind of\niterative process as suggested by the Banach contraction principle. The full implementation we did, as\nwell as the use of other methods for validation and stability, can be seen in our Github [1]. The Example\n2.3.3 has also been treated in [16] and the interested reader can compare our results with those found\nin that article.\nChapter Conclusion\nIn this chapter we have discussed contraction mappings in a metric space and the Banach contraction\nprinciple (or Banach fixed point theorem).\nWe have also shown how the theorem can be used to\nprove the existence and uniqueness of solutions to first-order ordinary differential equations with initial\nconditions, and even to find the solution. In the next chapter we will give an overview of Reinforcement\nLearning, which will help us to relate it to the theories introduced in this chapter.\n3. Overview on Reinforcement Learning\nIn this chapter, we will first formally present the Markov Decision Process framework and how it relates\nto Reinforcement Learning. Second, we will talk about optimality in Reinforcement Learning, and finally\nwe will discuss some methods that are at the core of how optimality is realised in Reinforcement Learning.\n3.1\nMarkov Decision Processes in the Reinforcement Learning setting\nWe here present a formal definition of the Markov Decision Process. But before that, we discuss about\nMarkov Chain because it allows us to present the fact that, there are situations in which the previous\nobservation contains all the information about the past, in such a way that we don’t need to look at\nthe whole history.\n3.1.1 Definition (Markov chain [22]). Let S (a subset of Rn for example) represent the state space.\nThe discrete-time dynamic system {St}t∈N ∈S is a Markov chain if :\nP(St+1 = s|St, St−1, · · · , S0) = P(St+1 = s|St),\n(3.1.1)\ni.e. everything we need to predict (in probability) the future is contained in the current state (this\nis what we call Markov property). Given an initial state S0 ∈S, a Markov chain is defined by the\nfollowing transition probability :\np(s′|s) = Pr(St+1 = s′|St = s).\n(3.1.2)\nWe can now clearly define the Markov Decision Process and related concepts, important for Reinforce-\nment Learning.\n3.1.2 Definition (Markov Decision Process [22, 32]). Formally, an Markov Decision Process (MDP) is\na 5-uplet M = ⟨S, A, p, r, γ⟩where :\n• S represents the state space. It can be finite, countable infinite or continuous.\n• A represents the action space which could also be finite, countable infinite or continuous.\n• p(s, a, s′) is the transition probability (called environment dynamics) defined by :\np(s, a, s′) ≡p(s′|s, a) = Pr(St+1 = s′|St = s, At = a),\n(3.1.3)\nwith s, s′ ∈S and a ∈A. This represents the probability of observing a next state s′ when the\naction a is taken in the state s.\n• r(s, a, s′) is the reward function called also a reinforcement obtained when taking the action\na, in the state s and the next state observed is s′. So :\nr : S × A × S →R\n• The transition probability p and the reward function r defines the environment model.\n11\nSection 3.1. Markov Decision Processes in the Reinforcement Learning setting\nPage 12\n• γ ∈[0, 1) is the discount factor used to define (let’s say for discrete time here for illustration)\nthe expected cumulative reward :\nG =\n∞\nX\nt=0\nγt · r(St, At, St+1).\nIf we have a problem which can be modeled as an MDP, the goal is generally to maximize the quantity\nG by choosing at each time and for each state the action At. That choice is defined by a function\nπ : S →∆A called policy. Where ∆A represents a probability distribution under actions.\n3.1.3 Definition (Policy or decision rule). Policy is a mapping of states to actions, which can be :\n• Deterministic : in the same conditions, the same action is always chosen.\n• Stochastic : at each time, actions are only chosen with a certain probability. In this case the\npolicy is a mapping from the state space to a probability distribution over actions.\nThe policy can also be varying as the time evolve (Non-stationary policy) or constant (Stationary policy).\nWe have given the foundational definitions, let us now reformulate everything in the Reinforcement\nLearning setting :\n1. The agent-environment interface\nMarkov Decision Processes (MDPs) are indeed a simple way to frame the problem of learning\nfrom interaction to achieve a goal.\nWe are here define some important terms in the field of\nReinforcement Learning :\n• The element that has to learn and make decisions is called agent.\n• Everything with which the agent interacts, including any entity outside the agent, is called\nthe environment.\n• The agent and the environment are in constant interaction, the former selecting actions, the\nlatter responding to those actions and presenting new situations to the agent. These new\nsituations are called states of the environment.\n• The agent also receives rewards (special numerical values) from the environment and tries\nto maximise these over time through its choice of actions.\n• We call action the way the agent chooses to interact with its environment. Or simply, the\ninteraction of the agent with its environment.\nLet us now call St, At and Rt respectively the state, the action and the reward at a specific\ntime-step t.The MDP and the agent will now produce the following kind of sequences :\nS0, A0, R1 ,\nS1, A1, R2 ,\nS2, A2, R3 , ...\nThe set of rewards, actions and states will be respectively written as R, A and S. We’ll also call\nA(s) the actions which are feasible in a specific state St = s.\nThe Figure 3.1 shows schematically all the above concepts and how they are related.\nSection 3.1. Markov Decision Processes in the Reinforcement Learning setting\nPage 13\nFigure 3.1: Agent-environment interaction in an MDP [36]\n2. Model of the environment\nLet us now suppose that we are able to have access to all the dynamic of our environment. Then,\nwe can generally define a function which gives the probability of transiting in a certain state St\nand receive a certain reward Rt if we were previously in the state St−1 and made the action At−1.\nThat kind of function, defined for the whole action and state spaces is called the model of the\nenvironment. It can be generally defined as follows :\np(s′, r|s, a) ≡Pr\n\u0010\n{St = s′, Rt = r|St−1 = s, At−1 = a}\n\u0011\n∀s, s′ ∈S , r ∈R; a ∈A(s) (3.1.4)\nMost of the time, it is assumed that, the next values depend only on the immediate previous\nvalues (Markov property) [22]. The Markov Property can be formalized as follows :\np(s′, r|s, a) = Pr\n\u0010\n{St, Rt|St−1, At−1, St−2, At−2, ...}\n\u0011\n= Pr\n\u0010\n{St = s′, Rt = r|St−1 = s, At−1 = a}\n\u0011\nThis function defines the dynamic of a Markov decision process.\n3. Other important definitions from the model of the environment\nMDPs are powerful tools for modeling and solving problems where decision making involves un-\ncertainty and sequential actions. We are here giving other formal definitions which shows how we\ncan derive well-known concepts from the general Definition given by the Expression 3.1.4 [36].\n(a) Second form of the environment model\nThis definition is the commonly used but is not as general as the one we gave in Relation\n3.1.4. The reward is not explicitly considered :\np(s′|s, a) ≡Pr\n\b\nSt = s′|St−1 = s, At−1 = a\n\t\n=\nX\nr∈R\np(s′, r|s, a)\n(3.1.5)\n(b) Definition of the reward\nHere we define the reward without considering the next state of the environment:\nr(s, a) ≡E [Rt|St−1 = s, At−1 = a] =\nX\nr∈R\n \nr ·\nX\ns′∈S\np(s′, r|s, a)\n!\n(3.1.6)\nSection 3.1. Markov Decision Processes in the Reinforcement Learning setting\nPage 14\n(c) Reward depending on next state\nHere we consider the next state in our definition of reward. That captures the most similar\nto our usual view of the reward function in real life:\nr(s, a, s′) ≡E\n\u0002\nRt|St−1 = s, At−1 = a, St = s′\u0003\n=\nX\nr∈R\n\u0012\nr · p(s′, r|s, a)\np(s′|s, a)\n\u0013\n(3.1.7)\n4. Formalization of the concepts of goals, rewards and returns\nIn Reinforcement Learning (RL), the purpose or goal of the agent is formalized in terms of a special\nsignal called a “reward” as explained previously, which is sent to the agent from the environment.\nIn other words, given the reward at any given time as just a real number, the “goal” of the agent is\nto maximize the total amount of rewards it receives. This is not a maximization of the immediate\nreward, but of the cumulative reward in the long run. With this in mind, if we call Rt the reward\nwe get at time t, we can generally define a function that we’ll call return at that time like this :\nGt = Rt+1 + γ · Rt+2 + γ2 · Rt+3 + · · · =\n∞\nX\nk=0\nγk · Rt+k+1\nwith γ ∈[0, 1)\n(3.1.8)\nIn this Expression 3.1.8, the discount factor γ can also be taken equal to one but this will lead to\nthe divergence of the return function in infinite cases. This definition is mathematically consistent,\nand it is also consistent in real-life. In fact :\n• Mathematically : Taking γ in the interval [0, 1) ensures the convergence of the series if only\nthe reward function is bounded.\n• In real life : This definition of the return function is consistent with the concept that imme-\ndiate rewards are more appreciated than rewards deferred into the future.\n5. Policies and Value Functions\nAlmost all RL algorithms involve estimating value functions, which are functions of states (or\nof state-action pairs) that estimate how good it is for an agent to be in a particular state (or to\nperform a particular action in a particular state).\nThe reward that an agent can expect to receive in the future depends on which action will be\ntaken. Accordingly, value functions are defined with respect to particular ways of acting, called\npolicies. Formally, a policy is a mapping from states to action space. In general, it quantifies\nthe probability of selecting a certain action in a specific state. So, the policy can be defined as\nfollows:\nπ (a|s) ≡π (At = a|St = s) = Pr {a ∈A(s)|s} ,\n(3.1.9)\nwhere A(s) represents the set of possible actions when we are in the state s.\nThe value function now, for a state s, under a policy π, denoted vπ(s), is the expected payoff\nof starting in that state and then following that policy. For MDPs we can formally say that the\nvalue function is given by :\nvπ(s) ≡Eπ [Gt|St = s] = Eπ\n\" ∞\nX\nk=0\nγk · Rt+k+1|St = s\n#\n(3.1.10)\nWe can similarly define the value function of taking an action a while following the same policy,\nstarting in the state s, which is called the Q-value :\nqπ(s, a) ≡Eπ [Gt|St = s, At = a] = Eπ\n\" ∞\nX\nk=0\nγk · Rt+k+1|St = s, At = a\n#\n(3.1.11)\nSection 3.2. Optimal Policies and Optimal Value Functions\nPage 15\nThe equations 3.1.10 and 3.1.11 correspond respectively to the state-value function and the\naction-value function. Also, the state value function can be seen as the average of all the values\nthat can be achieved by acting from a given state, i.e., we can see the state value function as a\nweighted average of the action-value functions that we can obtain starting from that state. So,\nformally :\nvπ(s) ≡\nX\na∈A(s)\nqπ(s, a) · π(a|s)\n(3.1.12)\nNow the following holds between the value of s and the value of its possible successor states for\nany policy π and any state s :\nvπ(s) = Eπ [Gt|St = s]\n= Eπ [Rt+1 + γ · Gt+1|St = s]\n=\nX\na\nπ(a|s)\n X\ns′\nX\nr\np(s′, r|s, a)\nh\nr + γ · Eπ\n\u0002\nGt+1|St+1 = s′\u0003 i!\n(3.1.13)\n⇒vπ(s) =\nX\na\nπ(a|s)\n\nX\ns′,r\np(s′, r|s, a)\nh\nr + γ · vπ(s′)\ni\n\n\n(3.1.14)\nusing then the equation 3.1.6\n⇒vπ(s) =\nX\na\nπ(a|s) ·\n \nr(s, a) + γ ·\nX\ns′\np(s′|s, a)vπ(s′)\n!\n(3.1.15)\nThe Relation 3.1.13 is explained by the definition of the reward function given by the equation\n3.1.6, and also by the fact that we normally have to do a weighted summation under the action\nset A to account for all actions. The equation 3.1.15 is then called Bellman equation for the\nvalue function vπ.\nWe can define in the same way the Bellman equation corresponding to the action-value function\nas follows [18] :\nqπ(s, a) = r(s, a) + γ ·\nX\ns′\np(s′|s, a) ·\n X\na′\nπ(a′|s′) · qπ(s′, a′)\n!\n(3.1.16)\nFrom now on we can notice that the MDP framework is a considerable abstraction of the problem of\ngoal-directed learning from interaction.\n3.2\nOptimal Policies and Optimal Value Functions\nHere we present the optimal value functions and discuss how they can be used to find the optimal policy\n[36]. In fact, the value function defines a partial ordering over policies. More precisely, given two policies\nπ and π′, we can say,\nπ ≥π′ ⇐⇒vπ(s) ≥vπ′(s)\n∀s ∈S,\n(3.2.1)\nbut we are not always able to compare two value functions. So, the policy π is defined to be better\nthan or equal to π′ if its expected return is greater than or equal to that of π′ for all states.\nSection 3.2. Optimal Policies and Optimal Value Functions\nPage 16\nAnd now, if π∗is the optimal policy we have :\n( v∗(s)\n≡\nmax\nπ\nvπ(s)\nq∗(s, a)\n≡\nmax\nπ\nqπ(s, a) = E [Rt+1 + γ · v∗(St+1)|St = s, At = a]\n(3.2.2)\n3.2.1 Bellman optimality equation. Since we take the optimal action at each time (following the\noptimal policy π∗), we can write v∗(s) = max\na∈A(s) qπ∗(s, a) , and this will be the best achievable expected\nreward at each time. So :\nv∗(s) = max\na∈A(s) qπ∗(s, a)\n= max\na\nEπ∗[Gt|St = s, At = a]\n= max\na\nEπ∗[Rt+1 + γ · Gt+1|St = s, At = a]\n= max\na\nEπ∗[Rt+1 + γ · v∗(St+1)|St = s, At = a]\n= max\na\nX\ns′,r\np(s′, r|s, a) ·\n\u0002\nr + γ · v∗(s′)\n\u0003\n= max\na\n\"\nr(s, a) + γ ·\nX\ns′\np(s′|s, a)v∗(s′)\n#\n(3.2.3)\nThe Expression (3.2.3) follows the definition of the model of the environment given in point 2 on page\n13. We can then show the consistency with the connection given in Expression (3.1.12) by deducing\nq∗(s, a) as follows :\nq∗(s, a) = E\n\u0014\nRt+1 + γ · max\na′\nq∗(s′, a′)|St = s, At = a,\nSt+1 = s′, At+1 = a′\n\u0015\n=\nX\ns′,r\np(s′, r|s, a) ·\n\u0014\nr + γ · max\na′\nq∗(s′, a′)\n\u0015\n⇒q∗(s, a) = r(s, a) + γ ·\nX\ns′\np(s′|s, a) · max\na′\nq∗(s′, a′)\n⇒q∗(s, a) = r(s, a) + γ ·\nX\ns′\np(s′|s, a) · v∗(s′)\n(3.2.4)\nEquations (3.2.3) and (3.2.4) are called Bellman optimality equations. For a finite MDP, the Bellman\noptimality equation has a unique solution (in the case of infinite MDPs, the Bellman optimality equation\nmay not have a unique solution because infinite MDPs can have multiple optimal policies that lead to\nthe same value function).\nAnd having the optimal value function v∗, one can determine an optimal policy (because the value\nfunction defines a partial order on the policy space).\nFinally, even if we have a complete and accurate model of the environment (the dynamics), it is\nusually not possible to compute an optimal policy simply by solving the Bellman equation. Therefore,\napproximate methods are developed and used [33].\nSection 3.3. Solving Markov Decision Processes\nPage 17\n3.3\nSolving Markov Decision Processes\nSolving an MDP means try to find an optimal value function v∗or q∗, which corresponds to find the\noptimal policy π∗.\nThere are many ways of doing that, and algorithms were developed for specific challenges. The most used\nclassification consists in distinguishing model-based and model-free algorithms, direct or indirect.\nGenerally, model-based algorithms concern what is called Dynamic Programming (DP). Model-based\nalgorithms assume that the model of the environment is well known and can be used to find value\nfunctions and policies using the Bellman equations.\nModel-free algorithms now, is considered as true-RL algorithms. They don’t require the availability of a\nperfect model (we suppose that, the environment is not totally known). This method rely on interaction\nwith the environment. That interaction is done by simulating the policy and generating samples of state\ntransitions and rewards. Then, the samples are used to estimate value functions and even to improve\nthe used policy.\nIn model-free algorithms, since no model of the MDP is known, the agent must explore the MDP to\nobtain information. This introduces a exploration-exploitation trade-off that must be balanced to\nobtain an optimal policy.\nWe can also talk about model-based reinforcement learning algorithms in which the agent does not\npossess, at the beginning, a model of the environment, but estimates it after a certain amount of time.\nOnce a reasonable model of the environment has been induced, the agent can then apply dynamic\nprogramming algorithms to compute the optimal policy.\n3.3.1 Dynamic Programming based algorithms or Model-based algorithms. Taking a complex\nproblem, breaking it down into simpler components and solving them recursively is the main idea of\ndynamic programming.\nThere are two main Dynamic Programming algorithms from which we can derive others. Policy iteration\n[17] and value iteration [9]. These two algorithms are based on two principal concepts (policy evaluation\nand policy improvement) below explained :\n1. Policy evaluation :\nThe partial order defined by the Relation 3.2.1 gives a way of comparing two policies. Now, by\nestablishing an order between value functions, we can find the best policy. But before doing that,\nwe have to evaluate each specific policy.\nPolicy evaluation consists on finding the value function using the function vπ defined in Relation\n3.1.15 iteratively until convergence. This is done using the following discrete version of the value\nfunction :\nvk+1\nπ\n(s) ←−\nX\na\nπ(a|s) ·\n \nr(s, a) + γ ·\nX\ns′\np(s′|s, a) · vk\nπ(s′)\n!\n(3.3.1)\nThe algorithm 1 below describes how the values of each state are computed iteratively to evaluate\nthe policy [18] :\nSection 3.3. Solving Markov Decision Processes\nPage 18\nAlgorithm 1 Policy Evaluation Algorithm\n1: Policy (π), Discount rate (γ), Threshold to stop (ε)\n▷Input\n2: vπ(s) = 0 for all s ∈S\n▷Initialization\n3: The estimated state-value function for the policy π (all v(s))\n▷Output\n4: while not converge do\n5:\nδ = 0\n6:\nfor each state s ∈S do\n7:\nv ←vπ(s)\n8:\nvπ(s) ←P\na π(a|s) · [r(s, a) + γ · P\ns′ p(s′|s, a) · vπ(s′)]\n9:\nδ ←max(δ, |v −vπ(s)|)\n10:\nend for\n11:\nif δ < ε then\n12:\nbreak\n13:\nend if\n14: end while\n2. Policy improvement :\nIn RL, the final goal is to find the optimal policy. Since the policy space is very huge, the task\nbecomes infeasible. This is why, the policy improvement algorithm, as a solution to that problem,\naim to find iteratively the optimal policy, by making small improvements on the current one.\nThe idea is to modify the current policy greedily with respect to the value function. But, here we\nshould definitely use the action-value function instead of the state one because the policy is about\nchoosing an action. This will result in a deterministic policy (i.e. the probabilities for each case\nare 0 or 1). The only remaining problem is to compute at each time the action-value function\nbut we don’t have an estimate of it. Fortunately, as we have already a way to get iteratively\nthe state-value function (using policy evaluation), we can now use it in the expression of the\naction-value function.\nKnowing the relation between q and v, (Relation 3.1.12), we can modify the Expression 3.1.16 as\nfollows :\nqπ(s, a) = r(s, a) + γ ·\nX\ns′\np(s′|s, a) · vπ(s′)\n(3.3.2)\nSo, the algorithm 2 below gives with more details how to improve gradually the policy [18] :\nSection 3.3. Solving Markov Decision Processes\nPage 19\nAlgorithm 2 Policy Improvement Algorithm\n1: Input: Estimated value function vπ , discount rate (γ)\n2: Initialization: qπ(s, a) = 0 for all s ∈S and a ∈A\n3: Output: Improved deterministic policy π′\n▷Calculation of the state-action value function using the estimated state value function\n4: for each state s ∈S do\n5:\nfor each action a ∈A(s) do\n6:\nqπ(s, a) ←r(s, a) + γ · P\ns′ p(s′|s, a) · vπ(s′)\n7:\nend for\n8: end for\n▷Calculation of an improved deterministic policy\n9: for each state s ∈S do\n10:\na∗= argmax\na\nqπ(s, a)\n11:\nfor each action a ∈A(s) do\n12:\nif a = a∗then\n13:\nπ′(a|s) ←1\n14:\nelse\n15:\nπ′(a|s) ←0\n16:\nend if\n17:\nend for\n18: end for\nNow we can look easily on the value iteration and the policy iteration, which are our goals for\nthese Dynamic Programming techniques of solving Markov Decision Processes.\n1. Policy iteration :\nPolicy iteration is used to find the optimal policy in a Markov decision process (MDP). This\nalgorithm consists of two main steps, policy evaluation and policy improvement. Those two steps\nare iteratively applied to refine the policy until it converges to the optimal one.\nFigure 3.2: Policy iteration [33]\nSection 3.3. Solving Markov Decision Processes\nPage 20\n2. Value iteration :\nThis value iteration algorithm is like an improvement of policy iteration.\nIn fact, the latter\nalgorithm works effectively and find the optimal policy but it can take a lot of time because\nat each epoch it computes the policy evaluation and the policy improvement. Value iteration\ncombines policy evaluation and policy improvement into one step. The idea is to replace the\ngeneral expression of the value function for a given policy (Expression 3.1.15) by the one giving\nthe optimal value function (Expression 3.2.3) in the Expression 3.3.2.\nFigure 3.3: Value Iteration [28]\nGenerally, DP algorithms provide a powerful framework for solving reinforcement learning problems when\na perfect model of the environment is known. It decomposes the problem into simpler subproblems and\nthen uses the Bellman equations to compute the value functions.\nOptimality is reached when the\nexpected cumulative reward is maximised.\nUnfortunately, the model of the environment is not always available.\nAlso, DP algorithms can be\ncomputationally expensive, especially for problems with large state and action spaces. For those reasons,\nother RL methods have been developed, such as Monte Carlo methods and temporal difference learning\n[35].\n3.3.2 Model-free algorithms : Monte Carlo. Dynamic Programming (DP) assumes that the agent\nhas a perfect access to the model of the environment (dynamics of the environment). But most of the\ntime, we don’t have access to that. Additionally, sometimes the state and/or action spaces are very\nhuge in such a way that DP algorithm can be inefficient.\nMonte Carlo refers to the method of using experience to estimate value functions by averaging sample\nreturns. Statistically it is proven to converge to the true estimate of the returns [10]. It uses sequences\nof state-action-reward samples generated by the agent by interacting with the environment.\nThere are always two important steps to be taken into account, the evaluation step and the improvement\nstep. The idea of Monte Carlo (MC) policy evaluation for example is, following a certain policy π, to\ngenerate a sample of state-action-reward-state or state-action-reward-state-action starting from a certain\nstate. After generating a large amount for many episodes (because here we are talking about processes\nwhich have a terminating state, i.e. they can end in a finite number of steps), the expected return is\nobtained by averaging the returns from a large amount of complete sample episodes. This gives the\ncorresponding value for that state, given that specific policy. As the number increases, the average will\nconverges to the true value of vπ [10].\nThe previous text just gives a general idea on how MC works. But, there are many implementations of\nthis idea :\nSection 3.3. Solving Markov Decision Processes\nPage 21\n• First-visit MC policy evaluation (using value function or action-value function)\n• Every-visit MC policy evaluation (using value function or action-value function)\n• Incremental MC policy evaluation (using value function or action-value function)\n• Monte Carlo Control (Monte Carlo Policy Improvement).\nHere is a succinct presentation of those main MC approaches :\n1. First-visit Monte Carlo Policy Evaluation\nThe idea of this method is to generate a lots of samples, compute the returns at each step within\neach episode and update the estimate of the state value function for each state (at the end of\nthe considered episode). The update is done by observing whether it is the first time to visit\nthe state in each episode and then update the return and the number of visits. At the end, the\nexpected return will be the average of the returns. What we have just explained is summarised in\nthe following algorithm [18] :\nAlgorithm 3 First-visit Monte Carlo Policy Evaluation for vπ\n1: Policy to evaluate (π), Discount rate (γ), Number of episodes (K)\n▷Input\n2: i = 0, vπ(s) = 0, G(s) = 0, N(s) = 0 for all s ∈S\n▷Initialization\n3: The estimated state-value function for π (all v(s))\n▷Output\n4: while i < K do\n5:\nGenerate a sample episode τ by following the policy π, where τ = S0, R1, S1, R2, ...\n6:\nCompute the return Gt for every time step t = 1, 2, 3, 4, ... in episode τ\n7:\nfor each time step t till the end of episode τ do\n8:\nif it’s the first time state St is visited in episode τ then\n9:\nN(St) ←N(St) + 1\n▷Increment counter of total visits\n10:\nG(St) ←G(St) + Gt\n▷Increment total return\n11:\nvπ(St) ←G(St)/N(St)\n▷Update estimated value\n12:\nend if\n13:\nend for\n14:\ni = i + 1\n15: end while\n2. Every-visit Monte Carlo Policy Evaluation\nA visit to a state s is each time the agent passes to that specific state. In the first-visit algorithm,\nonly the first time is considered in each episode while updating the return. For the every-visit,\nall the visits, even within an episode are taken into account. The former method is unbiased but\nhas an higher variance, the latter is biased with a lower variance. The trade-off is now taken\nby considering that aspect. Sometime even a combination of them is envisioned. The algorithm\ncomes from a small modification on the one for first-visit [18, 36].\n3. Incremental Monte Carlo Policy Evaluation\nThe update method used line 11 of the previous algorithm 3 is still perfectible. In fact, many\nresearches have shown that the incremental updates converges better than the simple update as\ndone in the previous algorithm [18]. So, instead of vπ(St) ←−G(St)/N(St), the incremental\nSection 3.3. Solving Markov Decision Processes\nPage 22\napproach consists on doing :\nvπ(St) ←−vπ(St) +\n1\nN(St) · (G(St) −vπ(St))\n|\n{z\n}\nincremental error\n(3.3.3)\nAll these algorithms, can also be used by considering the action-value function instead of the state value\nfunction. We just have to change mutatus mutandi everything. For example, for the incremental update\nshown in Relation 3.3.3, we simply do :\nqπ(St, At) ←−qπ(St, At) +\n1\nN(St, At) ·\n\u0010\nGt −qπ(St, At)\n\u0011\n(3.3.4)\nAlso, exactly as for Dynamic Programming, the control step can be considered. The control step consists\nnow on changing greedily the policy. For this case, there is for example Monte Carlo Control or Policy\nImprovement. For this, we can even try to do not always choose the best value but combine exploration\nand exploitation in a good way for better results [40].\n3.3.3 Model-free algorithms : Temporal Difference. MC is limited to episodic problems. Temporal\nDifference (TD) methods generalize MC by dealing with both episodic and continuing (no natural ending\nor terminating state for the considered task) or infinite RL problems.\nSo, MC has two main drawbacks : it only takes into account episodic problems and we have to wait\nfor the end of an episode to do any update on the estimated values. This is called Temporal Difference\nbecause the agent update its estimates based on the immediate reward and the estimated value for the\nnext state without waiting for the end, even if these values will change another time during the episode.\nThe main idea is a continuous updating of values, a guess from guesses.\nLet’s give the most important implementations which really show how TD works :\n1. TD Policy evaluation :\nFor a given policy, samples are generated and at the same time incremental update is applied in\nthe following way, without waiting for the end :\nvπ(St) ←−vπ(St) + λ · (Gt −vπ(St))\n←−vπ(St) + λ ·\n\u0010\n[Rt + γ · vπ(St+1)] −vπ(St)\n\u0011\n(3.3.5)\nWe here show the corresponding algorithm :\nAlgorithm 4 Temporal Difference TD(0) (policy evaluation for vπ)\n1: Input: Policy (π), Discount rate (γ), Step size λ, Number of episodes (K)\n2: Sample initial state S0. Initialization of i = 0, vπ(s) = 0 for all s ∈S.\n▷Initialization\n3: while i < K do\n4:\nSample action At for St following the policy π\n5:\nChose the action At in the environment and get Rt while observing St+1\n6:\ni = i + 1\n7:\nCompute TD target:\n8:\nδt =\n\u001a Rt\nif the next state St+1 is a terminal state\nRt + γ · vπ(St+1)\notherwise, we are bootstrapping as shown here\n9:\nUpdate estimated value vπ(St) ←vπ(St) + (δt −vπ(St))\n10:\nSt = St+1\n11: end while\nSection 3.3. Solving Markov Decision Processes\nPage 23\nIf we need to work with action-value functions, we just have to replace vπ by qπ coherently in\nequation 3.3.5. One obtains :\nqπ(St, At) ←−qπ(St, At) + λ · (Gt −qπ(St, At))\n←−qπ(St, At) + λ ·\n\u0010\n[Rt + γ · qπ(St+1, At+1)] −qπ(St, At)\n\u0011\n(3.3.6)\nAt the end of the algorithm, we’ll update the state as well as the action in case we are using the\nExpression 3.3.6.\nBased on this idea, the update can go far than one step as in the Expression 3.3.5. In fact, we can\nreplace Gt = Rt + γ · vπ(St+1) by Gt = Rt + γ · Rt+1 + · · · + γn · vπ(St+n). Depending on when\nwe stop, it gives us different levels of bootstrapping. This generalizes the TD(0) algorithm we\nhave been illustrating in this part. So, in general we have just an n-step TD Policy Evaluation\nthat we choose to write TD(n) with n+1 the number of steps ahead in the formula [18, 36].\n2. TD Control : SARSA\nSARSA stands literally for State-Action-Reward-State-Action. This method uses in fact the general\npolicy iteration template that we have shown in the Dynamic Programming part to find the optimal\npolicy. But it uses specific methods to manage exploration and exploitation while searching for\nthe optimal policy. In fact, from the previous policy iteration that we saw in the DP part, we have\nto take at each time the one which gives the maximum return (exploitation). But for this case,\nwe can sometimes take a suboptimal action (exploration) with a certain probability ε. This is\nwhat we call an ε-greedy selection method.\nSimply, the main idea is to loop on all the states for all the actions (state-action pairs) and then\nuse an ε-greedy policy to select the best action. But note that there are many ways of managing\nthe exploration-exploitation trade-off [18].\nIn general also, we can use n-step TD Policy Evaluation in the evaluation part of this policy\niteration. It now gives raise to all other implementations of SARSA n-step SARSA [36].\n3. TD Control : Q-Learning\nThe SARSA algorithm previously introduced is in fact a family of general policy iteration algorithm\nwhich estimates the state-action value function qπ for a policy π and uses an ε-greedy policy to\ncompute a new, better policy π′. But, as we did for value iteration algorithm, we can skip the\nstep of estimating values for a policy π by directly making an estimation using the optimal value\nfunction formula, so that we’ll be directly estimating the optimal policy. For Q-Learning, we use\nthe same idea by changing the Relation 3.3.6 by :\nq(St, At) ←−q(St, At) + λ ·\n\u0012\u0014\nRt + γ · max\na′\nq(St+1 = s′, a′)\n\u0015\n−q(St, At)\n\u0013\n(3.3.7)\nOne advantage of this algorithm is that it always uses the maximum value q(St+1, a′) across all\nvalid actions for the successor state St+1 = s′, regardless of which action is chosen by the policy.\nWe should note that, even if Q-Learning is one of the most used in real-world problems, it may be\nbiased. This is why another method called Double Q-Learning was suggested to alleviate that\nproblem [36, 39].\nIn many of the tasks the state space is very huge. In such cases, even with the most efficient algorithms\nwe have described above, we cannot expect to find an optimal policy or the optimal value function even\nin the limit of infinite time and data. Most of the time then, the goal is to find a good approximate of\nthe function we are looking for. Using linear or non-linear approximates. For more information about\nSection 3.3. Solving Markov Decision Processes\nPage 24\nthat, see [18, 30, 36].\nNow, as we have seen the main approaches to solve an MDP, which in fact resume the key points of\nRL foundational algorithms, we can now discuss on how it is related to the fixed point theory.\nChapter Conclusion\nIn this chapter, we have shown the fundamentals of Reinforcement Learning and how it is connected\nto Markov Decision Processes. After that, we have discussed about the optimal value functions and\noptimal policies and then at the end we have shown how generally the optimality is accomplished in RL.\nThe next chapter will now formalize what we have just presented in this chapter as formula to find the\noptimal policy in terms of operators. This will now allow us to show the connection with the Banach\nContraction Principle and how it can help understand deeply why and how RL algorithms work.\n4. Bellman Operators and Convergence of RL\nalgorithms\nIn this chapter, we will express reinforcement in terms of operators so that we have a simple way to\nprove why reinforcement learning algorithms work well and find the optimal policy. To do this, we will\nfirst write the Banach contraction principle on normed spaces in a way that makes it easy to use in our\nnext proofs, then we will discuss the Bellman operators, and finally we will talk about the core methods\nfor finding optimality introduced in the previous chapter (but now using the language introduced in this\nchapter).\n4.1\nRefinement of the Banach Contraction Principle\nWe need to state the Banach Contraction Principle presented in the first chapter in such a way that\neverything in this part will be expressed in the same way, using the same language.\n4.1.1 Contraction mapping and fixed-point. Let us now refine everything in term of norms in such\na way that we will have a consistent notation until the end of this work. In fact, as we can see in this\nwork, all the concepts related to reinforcement learning are defined in term of norms. So,\n4.1.2 Definition. Let (X, || · ||) be a normed space. A mapping T : X →X is a γ−contraction\nmapping if, there exists a γ ∈[0, 1) such that, for any x1, x2 ∈X we have\n||T x1 −T x2|| ≤γ · ||x1 −x2||\nFrom this, each contraction is Lipchitz and thus continuous [20]. It implies that :\nIf xn −→\n||·|| x\nthen\nT xn −→\n||·|| T x,\nwhere −→\n||·|| denotes the convergence using that norm || · ||. Now, by definition, an element x∗∈X is\ncalled fixed point for T if we have :\nT x∗= x∗\n4.1.3 Proposition. Let (X, ||·||) be a Banach space and\nT : X →X\na\nγ −contraction mapping.\nThen :\n1. T has a unique fixed point x∗∈X.\n2. For every starting point x0 ∈X, the sequence recursively defined by xn+1 = T xn converges to\nx∗in a geometric fashion.\nProof. For the first part, see the proof of the Banach fixed point Theorem 2.2.5. Let’s now only justify\nthe fact that the convergence is in geometric fashion :\nT ||xn−1 −x∗|| = ||T xn−1 −T x∗|| = ||T xn−1 −x∗|| = ||xn −x∗||,\nand as we are working with a contraction mapping, we also have ||T xn−1 −T x∗|| ≤γ · ||xn−1 −x∗||,\nthen we can conclude directly ||xn −x∗|| ≤γ · ||xn−1 −x∗||. Which implies that we get, by iteratively\n25\nSection 4.2. Bellman Optimality Operators\nPage 26\napplying the mapping on the second member of this previous inequality, ||xn −x∗|| ≤γn · ||x0 −x∗||\nthus\nlim\nn→∞||xn −x∗|| ≤lim\nn→∞(γn · ||x0 −x∗||) = 0 ⇒lim\nn→∞||xn −x∗|| = 0\nAs we may have noticed, the Proposition 4.1.3, is the Banach Contraction Principle as given by the\nTheorem 2.2.5. We restrain it on Banach spaces because this is sufficient for the RL setting. In fact, in\nmany instances encountered in Reinforcement Learning (RL), the vector space X is frequently identified\nas Rd or the space of real bounded functions, and the most relevant norms are mostly || · ||2 and || · ||∞.\nSo, stating the theorem on Banach Spaces is sufficient in this setting.\n4.2\nBellman Optimality Operators\nOperators are maps in space of functions. Here we will be defining them with respect to the associated\nRL equations. Let’s first define the whole mathematical framework that we’ll be using here.\nLet M = ⟨S, A, p, r, γ⟩be a Markov Decision Process (MDP), Vs ≡V the space of bounded real-valued\nfunctions over S and Q the space of bounded real-valued functions over S × A. With that we will call :\n• V :\nthe space of all state-value functions,\n• Q :\nthe space of all action-value functions,\n• (T π\nv ) : V →V\nthe Bellman Expectation Operator for the state value function,\n•\n\u0010\nT π\nQ\n\u0011\n: Q →Q :\nthe Bellman Expectation Operator for the action-value function,\n• (T ∗\nv ) : V →V\nthe Bellman Optimality Operator for the state value function,\n•\n\u0010\nT ∗\nQ\n\u0011\n: Q →Q :\nthe Bellman Optimality for the action-value function\n• π : S →∆(A)\nthe policy (a conditional probability defined by π(a|s) = Pr{a|s}) with ∆(A)\ndenoting a probability distribution of actions.\nInspired by the definition given in the Expression 2.1.3 and the Example 2.1.14 we know that, with\n||f(s)||∞≡max\ns |f(s)|, the tuple (V, ||.||∞) is a Banach Space.\nAlso, with ||f(s, a)||∞≡max\ns,a |f(s, a)| we can also see that (Q, ||.||∞) is a Banach Space. Those two\nsupremum norms are different but we will be writing them in the same way because, the real formula\nwill depend on the space we are working with.\n4.2.1 Definition. Let’s recall the fact that, the Bellman Optimality equation for the value function\n(Relation 3.2.3) is given by the following expression :\nv∗(s) = max\na\n\u0010\nr(s, a) + γ ·\nX\ns′\np(s′|s, a) · v∗(s′)\n\u0011\nSection 4.2. Bellman Optimality Operators\nPage 27\nBy analogy, we will define, point-wise, the Bellman Optimality Operator for state value function\n(T ∗\nv ) : V →V by :\n(T ∗\nv f) (s) ≡max\na\n\"\nr(s, a) + γ ·\nX\ns′\np(s′|s, a) · f(s′)\n#\n∀f ∈V\n(4.2.1)\n4.2.2 Properties of this operator. The convention we’ll take here is to write (T ∗\nv ) just (T ∗), knowing\nthat the space will be written V by default. Now, what is very interesting is that, the operator, defined\nin Relation 4.2.1 has these properties [2] :\n• T ∗is a γ−contraction with respect to the supremum norm || · ||∞on V :\n||T ∗u −T ∗v||∞≤γ · ||u −v||∞∀u, v ∈V\nwith γ ∈[0, 1)\n• T ∗is monotonic :\n∀u, v ∈V s.t u ≤v, in any state, T ∗u ≤T ∗v\nProof. First, let us rewrite simply the Bellman Optimality Operator in Relation 4.2.1 like this :\n(T ∗\nv f) (s) ≡(T ∗f) (s) ≡max\na\nh\nr(s, a) + γ · Es′|s,af(s′)\ni\n(4.2.2)\nNow, let us prove the two properties of the Bellman Optimality Operator T ∗:\n1. Using the definition of a contraction mapping and Bellman Optimality Operator we expand :\n|T ∗u(s) −T ∗v(s)| =\n\f\f\f\fmax\na\n\u0002\nr(s, a) + γ · Es′|s,au(s′)\n\u0003\n−max\nb\n\u0002\nr(s, b) + γ · Es′|s,bv(s′)\n\u0003\f\f\f\f\nSince for f, g real-valued functions, |max\nx f(x) −max\nx g(x)| ≤max\nx |f(x) −g(x)|, it follows that :\n|T ∗u(s) −T ∗v(s)| ≤max\na\n\f\f \u0002\nr(s, a) + γ · Es′|s,au(s′)\n\u0003\n−\n\u0002\nr(s, a) + γ · Es′|s,av(s′)\n\u0003 \f\f\n= max\na\n\f\fγ · Es′|s,au(s′) −γ · Es′|s,av(s′)\n\f\f\n= max\na\n\u0000γ ·\n\f\fEs′|s,a\n\u0000u(s′) −v(s′)\n\u0001 \f\f\u0001\n≤max\ns′\n\f\fγ ·\n\u0000u(s′) −v(s′)\n\u0001 \f\f\n= γ · max\ns\n\f\f (u(s) −v(s))\n\f\f\n⇒|T ∗u(s) −T ∗v(s)| ≤γ · ||u −v||∞\n⇒max\ns\n|T ∗u(s) −T ∗v(s)| ≤γ · ||u −v||∞\n⇒||T ∗u(s) −T ∗v(s)||∞≤γ · ||u −v||∞\n(4.2.3)\n2. We can now prove the monotonicity very simply :\nAssume that for all s, v(s) ≤u(s). Then r(s, a) + γ · Es′|s,av(s′) ≤r(s, a) + γ · Es′|s,au(s′).\nSection 4.2. Bellman Optimality Operators\nPage 28\nLet us evaluate T ∗v(s) −T ∗u(s) and see what happens :\nT ∗v(s) −T ∗u(s) = max\na\n\u0002\nr(s, a) + γ · Es′|s,av(s′)\n\u0003\n−max\na\n\u0002\nr(s, a) + γ · Es′|s,au(s′)\n\u0003\n≤max\na\n\u0010\u0002\nr(s, a) + γ · Es′|s,av(s′)\n\u0003\n−\n\u0002\nr(s, a) + γ · Es′|s,au(s′)\n\u0003\n|\n{z\n}\n≤\n0\n\u0011\nT ∗v(s) −T ∗u(s) ≤\n0\n⇒T ∗v(s)\n≤\nT ∗u(s)\n∀s\n(4.2.4)\nNow, using the Banach Contraction Principle as refined by the Proposition 4.1.3, we can conclude that,\nthe Bellman Optimality Operator T ∗on (V, || · ||∞) has the following behaviors :\n• T ∗has a unique fixed point f∗∈V.\n• For any starting point (function) f0, we can find the fixed point f∗by looking for the convergence\nof the sequence defined by fn+1 = T fn.\nAlso, the fixed point that we will find will be the optimal value function, since it’ll be the maximum\n(in fact, this sequence is monotonically increasing by our definition of the problem).\nThis fixed point f∗is the overall optimal value function because here the equation use exactly the\noptimal policy π∗.\nWe can, in the same way, define the operator for action-value functions.\n4.2.3 Definition. As we know, the Bellman Optimality equation for Q-values (Relation 3.2.4) is given\nby this expression :\nq∗(s, a) = r(s, a) + γ ·\nX\ns′\np(s′|s, a) · max\na′\nq∗(s′, a′)\nNow, giving the same MDP M, let Q be the space of bounded real-valued functions over S × A.\nWe can also define, point-wise, the Bellman Optimality Operator for the Q-Value T ∗\nQ : Q →Q as\nfollows :\n\u0000T ∗\nQf\n\u0001\n(s, a) ≡r(s, a) + γ ·\nX\ns′\n\u0014\np(s′|s, a) · max\na′∈A(s′)f(s′, a′)\n\u0015\n∀f ∈Q\n(4.2.5)\nThe same properties can be stated for this Bellman optimality operator for the Q-value, using the same\nkind of proofs.\nThe Bellman optimality operators as defined here are particular cases of what we will refer to as Bellman\nExpectation Operators.\nIn this context, we considered the scenario of selecting the action that\nmaximises the expected reward, without consideration of alternative policies. We now consider the\npolicy in more general terms. This does not lead only to the optimal value function, but also to the\nvalue function corresponding to the chosen policy.\nSection 4.3. Bellman Expectation Operators\nPage 29\n4.3\nBellman Expectation Operators\nWe will start with the state-value Bellman Expectation Operator. We know that, the Bellman Expec-\ntation equation is given by the Relation 3.1.15 :\nvπ(s) =\nX\na\nπ(a|s) ·\n \nr(s, a) + γ ·\nX\ns′\np(s′|s, a)vπ(s′)\n!\nWe can define by analogy the Bellman Expectation Operator by :\n(T π\nv f) (s) =\nX\na\nπ(a|s) ·\n \nr(s, a) + γ ·\nX\ns′\np(s′|s, a)f(s′)\n!\n∀f ∈V\n(4.3.1)\n4.3.1 Proposition. We claim that, the operator T π\nv , as defined by the Expression 4.3.1 is :\n1. A γ−contraction mapping.\n2. A monotonic mapping.\nProof. Let consider two value functions u and v in V.\n1. Let us compute the absolute value between the transformations of those two functions and see\nwhat happens :\n|T π\nv u(s) −T π\nv u(s)| =\n\f\f\f\f\f\nX\na\nπ(a|s) ·\n \nγ ·\nX\ns′\np(s′|s, a)u(s′) −γ ·\nX\ns′\np(s′|s, a)v(s′)\n!\f\f\f\f\f\n= γ ·\n\f\f\f\f\f\nX\na\nπ(a|s) ·\n X\ns′\np(s′|s, a)u(s′) −\nX\ns′\np(s′|s, a)v(s′)\n!\f\f\f\f\f\n= γ ·\n\f\f\f\f\f\nX\na\nπ(a|s) ·\n\u0000Es′|s,au(s′) −Es′|s,av(s′)\n\u0001\n\f\f\f\f\f\n≤γ · max\ns′\n\f\fu(s′) −v(s′)\n\f\f\n⇒|T π\nv u(s) −T π\nv u(s)| ≤γ · max\ns\n|u(s) −v(s)|\n⇒max\ns\n|T π\nv u(s) −T π\nv u(s)| ≤γ · ||u(s) −v(s)||∞\n⇒||T π\nv u(s) −T π\nv u(s)||∞≤γ · ||u(s) −v(s)||∞\n(4.3.2)\nThis proves that the Bellman Expectation Operator for the state value function is a contraction\nmapping.\n2. Let us now prove the monotonicity :\nSection 4.4. Policy evaluation and iteration in operators setting\nPage 30\nAssume that u(s) ≤v(s) in all states, then we will get :\nT π\nv u(s) −T π\nv v(s) = γ ·\nX\na\nπ(a|s) ·\n\u0000Es′|s,au(s′) −Es′|s,av(s′)\n\u0001\n= γ ·\nX\na\nπ(a|s) ·\n\n\nEs′|s,a\n\u0000u(s′) −v(s′)\n\u0001\n|\n{z\n}\nu(s)≤v(s) ∀s\n\n\n\n= γ ·\nX\na\nπ(a|s) ·\n\u0000Es′|s,a\n\u0000u(s′) −v(s′)\n\u0001\u0001\n≤0\n⇒T π\nv u(s) −T π\nv v(s) ≤0\n⇒T π\nv u(s) ≤T π\nv v(s)\ni.e. T π\nv is a monotonic mapping\n(4.3.3)\nFrom the properties we’ve just proven, using the Banach Contraction Principle as refined in the Propo-\nsition 4.1.3, we can conclude that :\n• T π\nv has a unique fixed point fπ ∈V.\n• For any starting point f0, we can find the fixed point fπ by looking for the convergence of the\nsequence defined by fπ\nn+1 = T fπ\nn .\nThis fixed point fπ is the optimal value function with respect to the policy π we are following.\n4.3.2 Remark. For the action value function, the Bellman Expectation Operator T π\nQ and the Bellman\nOptimality Operator T ∗\nQ as defined by the Expression 4.2.5 have exactly the same properties.\nNote also that, the Bellman Expectation Operator, by analogy with the formula 3.1.16 is given by :\n\u0000T π\nQ f\n\u0001\n(s, a) = r(s, a) + γ ·\nX\ns′\np(s′|s, a)\n X\na′\nπ(a′|s′) · f(s′, a′)\n!\n(4.3.4)\nHere we have presented in a condensed way, the Bellman Operators. Now, we are able to say precisely\nhow the Reinforcement Learning algorithms work really. This is why, in the following section, we are\ntalking about the policy evaluation and policy iteration methods but now in the operator setting and\nknowing all the properties of those operators, it shows us why the RL algorithm come to work.\n4.4\nPolicy evaluation and iteration in operators setting\nFrom these properties (of the Bellman Operator), we can now deduce two ways of searching for the\nstate-value function and the best policy. There are the well-known foundational policy evaluation\nand policy improvement (but this last one will be illustrated through policy iteration) algorithms (as\npresented in the previous chapter) :\n1. Policy evaluation :\nThe policy evaluation, is a method used to find the true state-value function associated to an MDP\nwhile following a specific policy. It’s based on the properties of the Bellman operator. Starting\nfrom a random distribution of value functions, until reaching the true one as follows :\nSection 4.4. Policy evaluation and iteration in operators setting\nPage 31\n• Start with v0 a random initial value function\n• Update the value function using : vk+1 ←T πvk\nBy the Banach Fixed-point Principle, it will converge to vπ as k →∞\n2. Policy iteration :\nThe policy iteration algorithm is used to find the best policy, by updating concurrently the policy\n(policy improvement) as well as the associated value-functions. It is done in the following way :\n• Start with a random policy π0\n• Concurrent iterations :\n* Policy evaluation : vk+1 ←T πivk\n* Policy improvement : Greedily improve the policy using : πi+1 = argmax\na∈A(s)\nqπi(s, a)\nAgain, using the Banach Fixed-point Principle, it will converge to the best policy π and the\nassociated value function vπ as k →∞.\nThis is sufficient to justify how Reinforcement Learning algorithms work because it explains how the two\nfoundational methods work.\nChapter Conclusion\nIn this chapter we have talked about Bellman operators. We first presented the Bellman optimality op-\nerators (for both the state and the state-action value functions), before presenting a general formulation\nvia the Bellman expectation operators (again for both the state and the state-action value functions).\nWe have shown that these operators are γ−contraction and monotonic mappings, i.e. they have only\none fixed point and that fixed point is the optimum. This is very important because it justifies why and\nhow RL algorithms work from a fundamental point of view. In the next chapter we will now discuss\nsome limitations of the Bellman operators in their classical expression and show some alternatives that\nseem interesting, even if they require further investments.\n5. Contributions, implementations and analysis\nIn this chapter we discuss some limitations of the classical Bellman operators as introduced in the\nprevious chapter. We present how we have to make a trade-off between optimality and efficiency, and\nfinally we show some practical results that we have obtained through experiments, which suggest that\nwe need to think more about some refinements that should be made either to the Bellman operators or\nto the associated value functions.\n5.1\nAlternatives to the Bellman Operator\nAs we have seen, value-based reinforcement learning algorithms solve decision making problems through\niterative application of a convergent operator, and an initial value function is recursively improved.\nMany other research works have examined alternatives to the Bellman operator [4, 6, 8, 11, 23, 42],\nbut there are two which seem to be more promising, this is the reason why we will take inspiration\nfrom them. The first one is the consistent Bellman Operator [8], and the second one is the family\nof Robust Stochastic Operators [23]. These two operators appear to be more interesting than those\nsuggested before [23], but for the second, we’ll suggest a refinement of it which will remain good\n(we postulate) even without the stochasticity, improving the stability.\n5.1.1 Motivation [8, 23]. The motivation for new expressions of the Bellman Operator is simple.\nWhile Q-learning based methods continue to be successfully used in RL to determine the optimal policy,\nthere is always a need to improve the convergence speed, accuracy and robustness of these algorithms.\nNow considering the fact that most of the time there are intrinsic error approximation (for example in\nthe usual case of using discrete MDP to approximate a continuous system), the optimal state-action\nvalue function obtained through the Bellman operator does not always describe the value of stationary\npolicies. And most importantly, when the differences between the optimal state-action value function\nand the suboptimal state-action value functions are small, this can lead to errors in identifying the true\noptimal actions. This is why, even if the classical Bellman Operators can solve problems in a perfect\ndiscrete setting, we need to refine it to be general because we know that for many real-world problems\nof our interest, discrete-time systems come from approximating continuous systems. So, there is always\nan intrinsic error, thus a need to integrate the corrector in the Operator.\nLet us now study the effectiveness of the two aforementioned operators and find some insights on the\ngeneral method of determining optimal policies in RL.\n5.1.2 The Consistent Bellman Operator. This operator, is primarily defined by the authors of [8] for\nthe action-value function. Here is the definition :\nTcf(s, a) = r(s, a) + γ ·\nX\ns′\np(s′|s, a) ·\n\u0014\nI{s̸=s′}max\na′\nf(s′, a′) + I{s=s′}f(s, a)\n\u0015\nwith f ∈Q\n(5.1.1)\nKnowing that I is the indicator function, we claim also that, the following properties holds for the\nconsistent Bellman Operator given by the Expression 5.1.1 :\n1. Tc is a contraction mapping,\n2. Tc is monotonic.\nProof. First, let’s do some refinements :\n32\nSection 5.1. Alternatives to the Bellman Operator\nPage 33\n• We will replace for simplicity\nX\ns′\np(s′|s, a) · (f)\nby\nEP(f)\n• We will refine the action-value function as follows :\nfs(s′, a′) =\n\u001a f(s′, a′)\nif\ns ̸= s′\nf(s, a)\nif\ns = s′\nfor\nf ∈Q\n(5.1.2)\nNow we can rewrite the consistent operator in 5.1.1 as follows :\nTcf(s, a) = r(s, a) + γ · EP\n\u0014\nmax\na′\nfs(s′, a′)\n\u0015\nwith f ∈Q\n(5.1.3)\nWe can now start the proof :\n1. Let u, v ∈Q, we will have :\n|Tcu(s, a) −Tcv(s, a)| =\n\f\f\f\fγ · EP\n\u0012\nmax\na′\nus(s′, a′)\n\u0013\n−γ · EP\n\u0012\nmax\nb′\nvs(s′, b′)\n\u0013\f\f\f\f\n= γ ·\n\f\f\f\fEP\n\u0012\nmax\na′\nus(s′, a′)\n\u0013\n−EP\n\u0012\nmax\na′\nvs(s′, a′)\n\u0013\f\f\f\f\n≤γ ·\n\f\f\f\fmax\na′\nEP\n\u0000us(s′, a′) −vs(s′, a′)\n\u0001\f\f\f\f\n≤γ · max\na′\n\f\fEP\n\u0000us(s′, a′) −vs(s′, a′)\n\u0001\f\f\n≤γ · max\ns′,a′\n\f\fus(s′, a′) −vs(s′, a′)\n\f\f\n= γ · max\ns,a\n|us(s, a) −vs(s, a)|\n= γ · ||us(s, a) −vs(s, a)||∞\n⇒|Tcu(s, a) −Tcv(s, a)| ≤γ · ||us(s, a) −vs(s, a)||∞\n⇒||Tcu(s, a) −Tcv(s, a)||∞≤γ · ||us(s, a) −vs(s, a)||∞\n(5.1.4)\n2. Now, for us to prove the monotonicity, we have to consider two state-action value functions u and\nv such that u(s, a) ≤v(s, a) ∀(s, a) ∈S×A. From the definition we gave through the Expression\n5.1.2, this also means that us(s, a) ≤vs(s, a). Based now on the proof we just provided for the\nmonotonicity, we have :\nTcu(s, a) −Tcv(s, a) ≤γ · max\na′\n\n\nEP\n\u0000us(s′, a′) −vs(s′, a′)\n\u0001\n|\n{z\n}\nus≤vs\n\n\n\n⇒Tcu(s, a) −Tcv(s, a) ≤0\n⇒Tcu(s, a) ≤Tcv(s, a)\n∀u, v ∈Q\n(5.1.5)\nSection 5.1. Alternatives to the Bellman Operator\nPage 34\nFrom this proof, we can conclude that Tc has only one fixed point and that fixed point is optimal for this\nsetup (that is the setup in which we are using the consistent Bellman equation instead of the classical\none).\nThe question that remains is how this fixed point relates to the one we have found using the\nclassical Bellman operator.\nThe most important thing is that, we are sure to converge to a unique fixed-point. As we don’t yet\nhave the mathematical tool to say something about the relation between that fixed point and the one\nwhich will be found using the classical Bellman Operator, we can just wait and see how the operator\nwill behave in practice in comparison to the Bellman Operator.\n5.1.3 Modified Robust Stochastic Operator. Here, the proposed operator take inspirations in both\nthe articles [23] and [8] but mostly that former article. This is more general (expectation operator) and\ndifferent from the Robust Stochastic Operator suggested in [23].\nWe know that, as an RL algorithm get better, if we express vπ(s) by :\nvπ(s) =\nX\na\nπ(a|s) · qπ(s, a),\nthe difference that we will call advantage learning, given by\nA(s, a) = qπ(s, a) −vπ(s),\nalways gives an idea on how well is our policy (which should be learning how to often choose better\nactions) as well as the value function. The quantity |A(s, a)| should get smaller as we are learning the\ngood policy.\nNow, let’s say we modify the Bellman Expectation Operator for the action-value function as defined\nby the Relation 4.3.4 in such a way that we integrate the idea of the advantage learning directly in\nthe Bellman Operator instead of waiting applying it in the learning process as for some policy gradient\nmethods [15]. Now, if we call that new operator Ta, it will be defined for all f ∈Q as follows :\n(Taf) = r(s, a)+γ ·\nX\ns′\np(s′|s, a)\n X\na′\nπ(a′|s′) · f(s′, a′)\n!\n+β ·\n\"\nf(s, a) −\nX\na\nπ(a|s)f(s, a)\n#\n|\n{z\n}\nA(s,a): advantage learning\n(5.1.6)\nWe can now study the properties of the operator defined by this Expression 5.1.6. The coefficient β is\nany real now but we will define it properly at the end of this theoretical investigation.\n5.1.4 Proposition. The operator Ta as defined by the Relation 5.1.6 is not a contraction mapping.\nProof. Let u(s, a) ≡u and v(s, a) ≡v be two elements of Q :\n|Tau −Tav| =\n\f\f\f\f\fγEP\n X\na′\nπ(a′|s′)\n\u0000u(s′, a′) −v(s′, a′)\n\u0001\n!\n+ β\n\"\n(u −v) −\nX\na\nπ(a|s)(u −v)\n#\f\f\f\f\f\n> β ·\n\f\f\fu(s, a) −v(s, a)\n\f\f\f\nfor some couples (u, v) and\nβ\n⇒||Tau −Tav||∞> β · ||u −v||∞\n(5.1.7)\nSo, as we can see, whatever β will be, there exist situations where the operator Ta is not a contration.\nSection 5.1. Alternatives to the Bellman Operator\nPage 35\nSince the operator Ta is not a contraction, we cannot assess its convergence. But, even if we are not\nsure about the convergence, we can still say something about its behavior. For that, inspired by the\ndefinitions given in [23, 8], let’s define two important properties that we will call the properties of a\nwell-behaving operator. There are the optimality preservation and the gap increasing properties.\nNote : in the following definitions, we’ll change a little bit our conventions by writing value functions\nin capital letters. This will help us to write properly operators as indices.\n5.1.5 Definition (Optimality preservation). Let call Ta an alternative to the Bellman operator and Tb\nthe classical Bellman operator. We will say that Ta preserves the optimality property if :\nQk,Tb < Vk,Tb ⇒Qk,Ta < Vk,Ta\nwhen k →∞\nQk,T represents the action value function that we find at the iteration k, using the operator T and Vk,T\nthe associated state value function.\n5.1.6 Definition (Gap increasing). Using the same notation as previously, we will say that an alternative\noperator Ta to the Bellman one Tb induces the gap increasing property if, for each state s ∈S and each\nfeasible action in that state a ∈A(s) :\n\f\f\f\f lim\nk→∞[Qk,Tb −Vk,Tb]\n\f\f\f\f ≤\n\f\f\f\f lim\nk→∞[Qk,Ta −Vk,Ta]\n\f\f\f\f\nKnowing that at each state, Vk,T is an average of all Qk,T that we can get by making a certain action\nfrom that state.\nThe optimality preservation and the gap increasing properties represent respectively :\n• How well the operator is likely to behave while looking for the fixed point and,\n• How is the model able to differentiate the values of suboptimal actions with those for optimal\nactions.\n5.1.7 Proposition. We claim that, the operator defined by the Expression 5.1.6, even if it is not a\ncontraction, is both optimality preserving and gap increasing.\nProof. Let’s prove those properties in the same order :\n1. Optimality preservation : We know that Qk,T = T Qk−1 and Vk,Tb =\nX\na\nπ(a|s).Bell(Qk−1).\nNow, using the Definition 5.1.5, we assume that Qk,Tb < Vk,Tb and we will deduce the relation\nbetween Qk,Ta and Vk,Ta. To make the notation easy to manage, we will do this replacement :\nr(s, a) + γ · EP\n X\na′\nπ(a′|s′) · Qk−1(s′, a′)\n!\n= Bell (Qk−1)\nNow we can start (knowing that we are working with k →∞) :\nQk,Tb < Vk,Tb\n⇒TbQk−1 < Vk,Tb\n⇒Bell (Qk−1) <\nX\na\nπ(a|s) · Bell (Qk−1)\nSection 5.1. Alternatives to the Bellman Operator\nPage 36\nwithout a loss of generality, let K = mina\n\u0010\nQk−1(s, a) −Vk−1(s)\n\u0011\n. Then\nBell (Qk−1) <\nX\na\nπ(a|s) · Bell (Qk−1)\n⇒Bell (Qk−1) + β · K <\nX\na\nπ(a|s) · Bell (Qk−1) + β · K · 1\n⇒Bell (Qk−1) + β · K <\nX\na\nπ(a|s) · Bell (Qk−1) + β · K ·\nX\na\nπ(a|s)\n⇒Bell (Qk−1) + β · K <\nX\na\nπ(a|s) · Bell (Qk−1) + β ·\nX\na\nπ(a|s) · K\nwe know that K ≤P\na π(a|s)·(Qk−1(s, a) −Vk−1(s)) ∀(s, a) ∈S ×A, and when k →∞as we\nare getting better policy, we have almost surely that (Qk−1 −Vk−1) ≤P\na π(a|s)·(Qk−1 −Vk−1)\nBell (Qk−1) + β · K <\nX\na\nπ(a|s) · Bell (Qk−1) + β ·\nX\na\nπ(a|s) · K\n⇒Bell (Qk−1) + β · (Qk−1 −Vk−1) <\nX\na\nπ(a|s) · Bell (Qk−1) + β ·\nX\na\nπ(a|s) · (Qk−1 −Vk−1)\n⇒Bell (Qk−1) + β · (Qk−1 −Vk−1)\n|\n{z\n}\nQk,Ta\n<\nX\na\nπ(a|s) ·\nh\nBell (Qk−1) + β · (Qk−1 −Vk−1)\ni\n⇒Qk,Ta <\nX\na\nπ(a|s) · Qk,Ta(s, a)\n⇒Qk,Ta < Vk,Ta\nas k →∞\n(5.1.8)\n2. Gap increasing : Using the Definition 5.1.6, we have now to show that :\n\f\f\f\f lim\nk→∞[Qk,Tb −Vk,Tb]\n\f\f\f\f ≤\n\f\f\f\f lim\nk→∞[Qk,Ta −Vk,Ta]\n\f\f\f\f\nIn fact, we know that, using the Bellman Operator Tb, if we are improving the policy at the same\ntime, the state value function Vk,Tb will converge to the action value function Qk,Tb as the policy\nget better. So, for optimal actions we will have :\nlim\nk→∞[Qk,Tb −Vk,Tb] = 0,\ni.e. also using Ta, we can say surely for optimal actions :\n\f\f\f\f lim\nk→∞[Qk,Tb −Vk,Tb]\n\f\f\f\f = 0\n≤\n\f\f\f\f lim\nk→∞[Qk,Ta −Vk,Ta]\n\f\f\f\f\nFor other actions, the inequality also holds because on convergence (Qk−1 −Vk−1) < 0 almost\nsurely and from the proof of optimality preservation we have seen that :\nBell (Qk−1) + β · (Qk−1 −Vk−1) < P\na π(a|s) · Bell (Qk−1) + β · P\na π(a|s) · (Qk−1 −Vk−1) ,\ni.e. also\n\f\f\f\f lim\nk→∞[Qk,Tb −Vk,Tb]\n\f\f\f\f ≤\n\f\f\f\f lim\nk→∞[Qk,Ta −Vk,Ta]\n\f\f\f\f for any (s, a) ∈S × A.\nSection 5.1. Alternatives to the Bellman Operator\nPage 37\nSo, the operator that we suggested through the Expression 5.1.6 is a well behaving operator, even if\nwe just assume the convergence (modulo the value of β).\nLet us now give some comments about the possibility of finding a fixed point for this operator. But,\nbefore doing so, we can first say something about the continuity and the boundedness of that operator\nbecause without that, we cannot talk about fixed points.\n5.1.8 Proposition. Let r(s, a), the reward function, be bounded and continuous. The operator Ta as\ndefined by the Expression 5.1.6, will also be bounded and continuous.\nProof. We can split our operator in three parts for simplicity :\n(Taf) = r(s, a)\n| {z }\npart 1\n+ γ ·\nX\ns′\np(s′|s, a)\n X\na′\nπ(a′|s′) · f(s′, a′)\n!\n|\n{z\n}\npart 2\n+ β ·\n\"\nf(s, a) −\nX\na\nπ(a|s)f(s, a)\n#\n|\n{z\n}\nA(s,a): advantage learning :: part 3\n1. Part 1 : the reward function is assumed to be continuous and bounded.\n2. Part 2 : If we take that part as an operator by itself, it will be bounded (because it is a contraction)\nand also linear. And being a bounded linear operator implies that it is also continuous. So, if we\ncall this new operator Tsmall written as follows :\n\u0010\nTsmallf\n\u0011\n(s, a) = γ ·\nX\ns′\np(s′|s, a)\n X\na′\nπ(a′|s′) · f(s′, a′)\n!\n= γ · EP\n X\na′\nπ(a′|s′) · f(s′, a′)\n!\n,\nWe can firstly prove the linearity, assuming that u, v ∈Q and α, β ∈R. We have to show that\nTsmall\n\u0010\nα · u + β · v\n\u0011\n= α · Tsmall u + β · Tsmall v\nAnd, that is true because the operator is just formed with average operations, which are in fact\nlinear. So, Tsmall is linear.\nLet’s now prove the contraction. For that, using again u and v defined previously, we have :\n\f\f\fTsmall u −Tsmall v\n\f\f\f =\n\f\f\fγ · EP\nX\na′\nπ(a′|s′) ·\n\u0000u(s′, a′) −v(s′, a′)\n\u0001 \f\f\f\n≤γ · EP\n\f\f\f\nX\na′\nπ(a′|s′) ·\n\u0000u(s′, a′) −v(s′, a′)\n\u0001 \f\f\f\n≤γ · EP\nX\na′\nπ(a′|s′) ·\n\f\f\f\n\u0000u(s′, a′) −v(s′, a′)\n\u0001 \f\f\f\n≤γ · max\ns′,a′\n\f\f\f\n\u0000u(s′, a′) −v(s′, a′)\n\u0001 \f\f\f\n= γ · max\ns,a\n\f\f\f (u(s, a) −v(s, a))\n\f\f\f\n⇒\n\f\f\fTsmall u −Tsmall v\n\f\f\f ≤γ · max\ns,a\n\f\f\f (u(s, a) −v(s, a))\n\f\f\f\n⇒max\ns,a\n\f\f\fTsmall u −Tsmall v\n\f\f\f ≤γ · max\ns,a\n\f\f\f (u(s, a) −v(s, a))\n\f\f\f\n⇒||Tsmall u −Tsmall v||∞≤γ · || (u(s, a) −v(s, a)) ||∞\n(5.1.9)\nThus, the second part is also continuous and bounded.\nSection 5.1. Alternatives to the Bellman Operator\nPage 38\n3. Part 3 : We know that, if f is continuous and bounded, then f(s, a) −P\na π(a|s)f(s, a) will\nalso be continuous and bounded. We just need to ensure that β has good properties, ensuring\nconvergence.\nFinally, knowing that the summation of continuous and bounded operators is also continuous and\nbounded, we have proved the assertion, and Ta is continuous and bounded (the notion of good behavior\nfor β is just to be precised).\nFrom the fact that Ta is continuous and bounded (modulo the behavior of β), we are sure that it will\nnot diverge if we set well the values of β. And, looking carefully on that operator, we can now say\nsomething about β.\nWe will always need an operator which is well behaving (improving speed and optimality) but also close\nenough to the classical Bellman Operator as we are evolving. That is the only way we can ensure\nthat we will get closer enough to the same fixed point while maintaining some nice properties of this\nalternative operator.\nIf now we consider a family of operators based on Ta by varying β, and the case of a β which changes\nper iteration in one operator during the learning process, to ensure convergence, they should fulfill the\nfollowing conditions :\n\n\n\n\n\n\n\n∞\nP\nj=1\nβi,j\n<\n∞\n{βi,j}\n→\n0\nwith j fixed.\ni : the operator,\nj : iteration\n(5.1.10)\nFor an operator which has a varying β across training, as it will be applied iteratively, we can directly\nnotice that after many iterations, we will have some terms like this\nX\nβi,j · (u(s, a) −v(s, a))j ≡\nX\nβi,j · Aj,\nand as the difference (u(s, a) −v(s, a))j is bounded, we can replace it by its bound. Let’s call the\nbound of (u(s, a) −v(s, a))j ≡Aj simply A. Then :\nX\nβi,j · Aj ≤A ·\nX\nj\nβi,j < ∞⇐⇒\nX\nj\nβi,j < ∞\nSo, for one operator, with varying β across iterations, the sum of values should be finite.\nAlso, with the conditions we gave, it allows us to construct directly a sequence of operators converging\nto the classical Bellman Operator. So, that’s why we need :\n{βi,j} →0\nwith j fixed.\n5.1.9 Example. One example of a family of operators with the form given by the Expression 5.1.6 and\nfulfilling the conditions on β given by 5.1.10, can be :\n1. Operator 1 : β is a sequence given by {β1,j} with β1,j = 1\nj2\n2. Operator 2 : β is a sequence given by {β2,j} with β2,j = 1\nj3\nSection 5.2. Practical implementation and analysis\nPage 39\n3. Operator k : β is a sequence given by {βk,j} with βk,j =\n1\njk+1 .\nAs last comment on this part we can observe : during our diverse proofs, we just require to the mappings\nto be contractions and monotonic in order to establish the existence of an optimal fixed point. The\nuse of specific definitions of value functions was not really important. This is why we can conjecture\nthat :\n5.1.10 Conjecture. Any monotonic contraction mapping integreting a certain notion of policy is a suit-\nable candidate as operator in Reinforcement Learning (we just have to refine accordingly the definitions\nof the associated value functions for them to be suitable to the RL framework).\n5.2\nPractical implementation and analysis\nTo prove the effectiveness of the suggested Modified Robust Stochastic Operator, we have conducted\nexperiments on 3 groups of classical problems in reinforcement learning using the Q-Learning algorithm,\nin Open-AI Gymnasium environments. The implementation of Q-Learning we used is inspired by\n[41] and our own implementation with all the parameters is in our Github Repository [1].\nHere are the necessary details on the environments we used :\n5.2.1 Environments and results.\n1. Mountain Car environment : The theory about this environment is presented in [25]. The state\nvector is 2-dimensional, continuous with a total of three possible actions. As long as the goal is\nnot yet reached, depending on the action, a negative reward is given to the agent, until it reaches\nthe goal. Following [23], we have discretized the state space into a 40×40 grid, but differently we\ndid 10000 training steps, with 10000 episodes each. The following Figure 5.1 shows the averages\nacross episodes.\nFigure 5.1: The 3 operators on MountainCar environment.\n2. Cart Pole environment : The theory about Cart Pole is presented in [7]. The state vector is\n4-dimensional and continuous with a total of two possible actions. The aim is to keep the pole\nSection 5.2. Practical implementation and analysis\nPage 40\nupright for as long as possible, with a reward of +1 for each step up to the failure, including the\nfinal step. So, the reward is positive at the end. Here, we have discretized the state space into\na 150 × 150 × 150 × 150 grid and again we did 10000 training steps, with 10000 episodes each.\nThe following Figure 5.2 shows the averages across episodes.\nFigure 5.2: The 3 operators on CartPole environment.\n3. Acrobot environment : The theory about Acrobot is presented in [35]. The state vector is 6-\ndimensional and continuous with a total of three possible actions. The goal is to have the free\nend of the Acrobot to reach the target height (represented by a horizontal line) in as few steps\nas possible, with each step not reaching the target being rewarded with -1. So, the reward is\nnegative again as for Mountain Car. Here, we have discretized the state space into a 30 × 30 ×\n30×30×30×30 grid, due to the limitations in memory allocation of the computer we was using,\nand again we did 10000 training steps, with 10000 episodes each. The following Figure 5.3 shows\nthe averages across episodes.\nFigure 5.3: The 3 operators on Acrobot environment.\nSection 5.2. Practical implementation and analysis\nPage 41\n5.2.2 Interpretation and discussion.\n1. Mountain Car : From the problem presented above, we know that the reward for mountain car can\nbe negative because it depends with how long the agent takes to reach the goal or to be stopped.\nNow, looking on the Figure 5.1 we can directly see how the average reward is better using the\nModified Robust Stochastic Operator than using the Bellman Operator. Also, the classical and\nthe consistent operators are performing exactly the same for this specific case. So, overall, the\nsuggested Operator finishes the episodes with higher reward than for the other two operators (the\nclassical and the consistent).\n2. Cart Pole : For Cart Pole, we can see in Figure 5.2 that the higher reward is reached again\nwhile using the Modified Bellman (suggested Bellman) compared to the other operators. And the\ndifference in terms of performances is really clear.\n3. Acrobot : Acrobot was more challenging and as we can see in Figure 5.3, the results using those 3\noperators are about the same. So, we think this is due to the fact that we were not able to make\na finer discretization for the Acrobot environment. In fact this was due to the memory limitations.\nSo, this experiment needs further investigations to establish clearly what is the great attainable\ndifference between those operators.\nSo, generally we can say that the Consistent Bellman Operator gives about the same result as the\nclassical one, but our modified version of the Robust Stochastic Operator gives most of the time better\nresults comparing to the other previous two operators.\nChapter Conclusion\nIn this last chapter, we have introduced the Consistent Bellman Operator as the first alternative to the\nclassical Bellman Operator presented in the previous chapter, and we showed that it fulfills exactly the\nsame properties as the classical one (uniqueness of the optimal fixed point), although this raises the\nquestion of the relation between a fixed point found by the Consistent Bellman and the one we can\nfind by the classical Bellman. Secondly, we presented our version of the robust stochastic operator.\nWe modified the one proposed in the literature in such a way that it no longer needs stochasticity, but\nstill preserves some good properties (gap increasing and optimality preservation), even if it loses the\nproperties of a classical Bellman operator. Finally, with results from a practical implementation, we\nclearly show that our proposed operator must be taken seriously, since it unexpectedly outperforms the\nclassical Bellman operator on some difficult tasks, as hypothesised.\n6. Conclusion\nThis work serves as a foundational effort in the deep and precise mathematical study of Reinforcement\nLearning concepts. The aim is to facilitate breakthrough discoveries and subsequent investigations, and\nto enable even pure mathematicians to contribute to this subject by suggesting insightful adjustments\nnecessary for improving Reinforcement Learning algorithms. This has been achieved through the various\nchapters in this work as follows:\nAfter the general introduction, in Chapter 2, we primarily discussed contraction mappings in a metric\nspace and the Banach Fixed Point Theorem. We also demonstrated how the Banach Fixed Point The-\norem can be used to prove the existence and uniqueness of solutions to first-order Ordinary Differential\nEquations with initial conditions.\nIn Chapter 3, we expressed the Reinforcement Learning problem in terms of a Markov Decision Process.\nThis allowed us to introduce different expressions of the value function, both in terms of state and\nstate-action pairs. Subsequently, we presented the concepts of Bellman Equations, which served as the\nfirst step in introducing the Bellman Operators.\nChapter 4 was dedicated to Bellman Operators. We have demonstrated that these operators are γ-\ncontraction and monotonic mappings, implying that they have a unique fixed point that is the opti-\nmum. This justification sheds light on why and how Reinforcement Learning algorithms work from a\nfundamental perspective.\nAt the end, in chapter 5, we presented the Consistent Bellman Operator as the first alternative to\nthe classical Bellman Operator. After that, we showed that it fulfills exactly the same properties as\nthe classical Bellman Operator (uniqueness of the optimal fixed point). However, this raised questions\nabout the relationship between a fixed point found through the Consistent Bellman Operator and one\nfound through the classical Operator. We then introduced a modification of the Robust Stochastic\nOperator, which is deterministic but defined in a more general and precise way. This demonstrates that\nstochasticity is no longer necessary to achieve better results than the classical Bellman Operator. Finally,\nusing an implementation in Python and OpenAI Gymnasium environments, we clearly demonstrated that\nour proposed operator merits serious consideration, as it outperforms the classical Bellman Operator in\nmany of the considered tasks.\nBased on the foundations laid by this work, the perspectives are numerous. The mathematical properties\nof value function spaces have been clearly presented (value functions can indeed be properly studied in\nBanach spaces), but future researchers can explore in depth the state space and the action space as\nwell as the policy space. They can also extend the analysis of the efficiency of our proposed operator.\nThe most relevant attempts we have seen during our research are in [12], where the geometric and\ntopological properties of value functions for Markov decision processes are explored, and in [21], where a\nunified formalism for defining state similarity metrics in Reinforcement Learning is introduced, hierarchies\namong metrics are established, and their implications for Reinforcement Learning algorithms are studied.\nWe expect that these can be well understood and extended with the help of the investigations done in\nthis work.\nThis work enables researchers, even those not currently involved in the field, to quickly and meaningfully\nengage with Reinforcement Learning, as the most important foundations are mathematically clarified.\n42\nAcknowledgements\n“It is not that we think we are qualified to do anything on our own. Our qualification comes\nfrom God.” 2 Corinthians 3:5\n“Transire suum pectus mundoque potiri. Deus ex machina.“\nMy sincere thanks to all the esteemed individuals who have made AIMS such a beautiful and transforma-\ntive experience for me, especially Professor Neil Turok, Professor Mama Foupouagnigni, and Dr. Daniel\nDuviol. This work would not have been possible without the invaluable guidance of my supervisor, Dr.\nYa´e Ulrich Gaba, who, despite his many responsibilities, was always there to guide, advise, and help me;\nand working with him was truly amazing. My heartfelt thanks go to my mentor, Sir Domini Leko, whose\nsimplicity, advice, and willingness to help went beyond science and always inspired me. I also extend my\ngratitude to my beloved Sifa Shamavu Rose, whose unwavering support, love, and patience constantly\nremind me that I am on the right path. Many thanks to all my friends at AIMS, especially Sinenkhosi\nMamba, who made me feel at home. Thank you to everyone who has been a constant support on my\njourney. May this work also serve as a testament to the effort you have put into helping me and the\nkindness you have shown me.\n43\nReferences\n[1] . https://github.com/DavidKrame/rl-essay-aims-cameroon.\n[2] DeepMind x UCL — Deep Learning Lecture Series 2021, 2021. Accessed on 13/03/2024.\n[3] Qamrul Hasan Ansari and Daya Ram Sahu. Fixed Point Theory and Variational Principles in Metric\nSpaces. Cambridge University Press, 2023.\n[4] Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.\nIn International Conference on Machine Learning, pages 243–252. PMLR, 2017.\n[5] Kavosh Asadi, Dipendra Misra, and Michael L. Littman.\nLipschitz continuity in model-based\nreinforcement learning. CoRR, abs/1804.07193, 2018.\n[6] Mohammad Gheshlaghi Azar, Remi Munos, Mohammad Ghavamzadeh, and Hilbert Kappen.\nSpeedy q-learning. In Advances in neural information processing systems, 2011.\n[7] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that\ncan solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics,\n(5):834–846, 1983.\n[8] Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip Thomas, and R´emi Munos. Increasing\nthe action gap: New operators for reinforcement learning. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 30, 2016.\n[9] RICHARD Bellman. Dynamic programming, princeton univ. Press Princeton, New Jersey, 39,\n1957.\n[10] Dimitri Bertsekas and John N Tsitsiklis. Introduction to probability, volume 1. Athena Scientific,\n2008.\n[11] Dimitri P Bertsekas and Huizhen Yu.\nQ-learning and enhanced policy iteration in discounted\ndynamic programming. Mathematics of Operations Research, 37(1):66–94, 2012.\n[12] Robert Dadashi, Adrien Ali Ta¨ıga, Nicolas Le Roux, Dale Schuurmans, and Marc G. Bellemare.\nThe value function polytope in reinforcement learning. CoRR, abs/1901.11524, 2019.\n[13] Pradip Debnath, Nabanita Konwar, and Stojan Radenovi´c. Metric fixed point theory. Springer,\n2021.\n[14] Dhananjay Gopal, Poom Kumam, and Mujahid Abbas. Background and recent developments of\nmetric fixed point theory. 2017.\n[15] Laura Graesser and Wah Loon Keng. Foundations of deep reinforcement learning: theory and\npractice in Python. Addison-Wesley Professional, 2019.\n[16] Emirhan Hacıo˘glu, Faik G¨ursoy, Samet Maldar, Yunus Atalan, and Gradimir V Milovanovi´c. Iterative\napproximation of fixed points and applications to two-point second-order boundary value problems\nand to machine learning. Applied Numerical Mathematics, 167:143–172, 2021.\n[17] Ronald A Howard. Dynamic programming and markov processes. 1960.\n[18] Michael Hu. The art of reinforcement learning.\n44\nREFERENCES\nPage 45\n[19] Leonid Vital’evich Kantorovich and Gleb Pavlovich Akilov. Functional analysis. Elsevier, 2016.\n[20] Somaskandan Kumaresan. Topology of metric spaces. Alpha Science Int’l Ltd., 2005.\n[21] Charline Le Lan, Marc G. Bellemare, and Pablo Samuel Castro. Metrics and continuity in rein-\nforcement learning. CoRR, abs/2102.01514, 2021.\n[22] A Lazaric. Markov decision processes and dynamic programming, 2013.\n[23] Yingdong Lu, Mark S Squillante, and Chai Wah Wu. A general family of robust stochastic operators\nfor reinforcement learning. arXiv preprint arXiv:1805.08122, 2018.\n[24] Md Abdul Mannan, Md R Rahman, Halima Akter, Nazmun Nahar, and Samiran Mondal.\nA\nstudy of banach fixed point theorem and it’s applications. American Journal of Computational\nMathematics, 11(2):157–174, 2021.\n[25] Andrew William Moore.\nEfficient memory-based learning for robot control.\nTechnical report,\nUniversity of Cambridge, Computer Laboratory, 1990.\n[26] Arch W Naylor and George R Sell. Linear operator theory in engineering and science. Springer\nScience & Business Media, 1982.\n[27] Aurelien Noupelah and Leonel Mouassom. Ordinary differential equations (ode). Course materials,\nSeptember 2023.\n[28] Stack Overflow.\nPolicy iteration vs value iteration.\nhttps://stackoverflow.com/questions/\n43728781/policy-iteration-vs-value-iteration, 2017.\n[29] Vittorino Pata et al. Fixed point theorems and applications, volume 116. Springer, 2019.\n[30] Rafael Ris-Ala. Fundamentals of Reinforcement Learning. Springer Nature, 2023.\n[31] Satish Shirali and Harkrishan Lal Vasudeva. Metric spaces. Springer Science and Business Media,\n2005.\n[32] Olivier Sigaud and Olivier Buffet. Markov decision processes in artificial intelligence. John Wiley\n& Sons, 2013.\n[33] David Silver. Rl course at ucl by david silver. UCL Course on RL, 2015.\n[34] Zhao Song and Wen Sun. Efficient model-free reinforcement learning in metric spaces. CoRR,\nabs/1905.00475, 2019.\n[35] Richard S Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse\ncoding. Advances in neural information processing systems, 8, 1995.\n[36] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[37] Anita Tomar and M.C Joshi. Fixed Point Theory and its Applications to Real World Problems.\nNova Science Publishers, New York, 2021.\n[38] Gordon Tran.\nAMATH 731: Advanced Computational Finance and Risk Management, 2019.\nSyllabus + Course Materials.\n[39] Hado Van Hasselt et al. Double q-learning. In NIPS, volume 23, pages 2613–2621, 2010.\nREFERENCES\nPage 46\n[40] Martijn Van Otterlo and Marco Wiering. Reinforcement learning and markov decision processes.\nIn Reinforcement learning: State-of-the-art, pages 3–42. Springer, 2012.\n[41] vmayoral.\nBasic Reinforcement Learning.\nhttps://github.com/vmayoral/basic reinforcement\nlearning/tree/master, 2024.\n[42] Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.\n[43] Eberhard Zeidler. Applied functional analysis: applications to mathematical physics, volume 108.\nSpringer Science and Business Media, 2012.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "math.FA",
    "68T05"
  ],
  "published": "2024-09-25",
  "updated": "2024-09-25"
}