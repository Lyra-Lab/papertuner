{
  "id": "http://arxiv.org/abs/2410.05191v1",
  "title": "LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation",
  "authors": [
    "Zhijie Wang",
    "Zhehua Zhou",
    "Jiayang Song",
    "Yuheng Huang",
    "Zhan Shu",
    "Lei Ma"
  ],
  "abstract": "Building on the advancements of Large Language Models (LLMs) and Vision\nLanguage Models (VLMs), recent research has introduced Vision-Language-Action\n(VLA) models as an integrated solution for robotic manipulation tasks. These\nmodels take camera images and natural language task instructions as input and\ndirectly generate control actions for robots to perform specified tasks,\ngreatly improving both decision-making capabilities and interaction with human\nusers. However, the data-driven nature of VLA models, combined with their lack\nof interpretability, makes the assurance of their effectiveness and robustness\na challenging task. This highlights the need for a reliable testing and\nevaluation platform. For this purpose, in this work, we propose LADEV, a\ncomprehensive and efficient platform specifically designed for evaluating VLA\nmodels. We first present a language-driven approach that automatically\ngenerates simulation environments from natural language inputs, mitigating the\nneed for manual adjustments and significantly improving testing efficiency.\nThen, to further assess the influence of language input on the VLA models, we\nimplement a paraphrase mechanism that produces diverse natural language task\ninstructions for testing. Finally, to expedite the evaluation process, we\nintroduce a batch-style method for conducting large-scale testing of VLA\nmodels. Using LADEV, we conducted experiments on several state-of-the-art VLA\nmodels, demonstrating its effectiveness as a tool for evaluating these models.\nOur results showed that LADEV not only enhances testing efficiency but also\nestablishes a solid baseline for evaluating VLA models, paving the way for the\ndevelopment of more intelligent and advanced robotic systems.",
  "text": "LADEV: A Language-Driven Testing and Evaluation Platform for\nVision-Language-Action Models in Robotic Manipulation\nZhijie Wang1, Zhehua Zhou1, Jiayang Song1, Yuheng Huang2, Zhan Shu1, and Lei Ma2,1\nAbstract‚Äî Building on the advancements of Large Language\nModels (LLMs) and Vision Language Models (VLMs), recent\nresearch has introduced Vision-Language-Action (VLA) models\nas an integrated solution for robotic manipulation tasks.\nThese models take camera images and natural language task\ninstructions as input and directly generate control actions\nfor robots to perform specified tasks, greatly improving both\ndecision-making capabilities and interaction with human users.\nHowever, the data-driven nature of VLA models, combined\nwith their lack of interpretability, makes the assurance of their\neffectiveness and robustness a challenging task. This highlights\nthe need for a reliable testing and evaluation platform. For this\npurpose, in this work, we propose LADEV, a comprehensive\nand efficient platform specifically designed for evaluating VLA\nmodels. We first present a language-driven approach that\nautomatically generates simulation environments from natural\nlanguage inputs, mitigating the need for manual adjustments\nand significantly improving testing efficiency. Then, to further\nassess the influence of language input to the VLA models,\nwe implement a paraphrase mechanism that produces diverse\nnatural language task instructions for testing. Finally, to expedite\nthe evaluation process, we introduce a batch-style method for\nconducting large-scale testing of VLA models. Using LADEV, we\nconducted experiments on several state-of-the-art VLA models,\ndemonstrating its effectiveness as a tool for evaluating these\nmodels. Our results showed that LADEV not only enhances\ntesting efficiency but also establishes a solid baseline for\nevaluating VLA models, paving the way for the development of\nmore intelligent and advanced robotic systems.\nI. INTRODUCTION\nRecent research has demonstrated the application of Large\nLanguage Models (LLMs) in various robotic domains [1],\n[2], where they are employed to tackle complex tasks\nthat usually require human-like cognitive abilities, such as\nplanning [3]‚Äì[5], task comprehension [6]‚Äì[8], and intention\nunderstanding [9]‚Äì[11]. Building on these advancements,\na growing number of recent works also employ Vision\nLanguage Models (VLMs) [12] to enhance robots with\nthe ability to process visual inputs [13]‚Äì[15]. It enables\nrobots to interpret their surrounding environments and identify\ninteractable objects, facilitating autonomous decision-making\nprocesses necessary for task completion.\nThis progress has given rise to a new class of end-\nto-end models known as Vision-Language-Action (VLA)\n1Zhijie Wang, Zhehua Zhou, Jiayang Song, and Zhan Shu are with\nthe University of Alberta, Edmonton, AB, Canada {zhijie.wang,\nzhehua1, jiayan13, zshu1}@ualberta.ca\n2Yuheng\nHuang\nand\nLei\nMa\nare\nwith\nThe\nUniversity\nof\nTokyo,\nTokyo,\nJapan.\nLei\nMa\nis\nalso\nwith\nthe\nUniversity\nof\nAlberta\nyuhenghuang42@g.ecc.u-tokyo.ac.jp,\nma.lei@acm.org. The code of this paper will be made available\nonline\nsoon.\nAdditional\ninformation\nand\nmaterials\nare\nprovided\nin https://sites.google.com/view/ladev.\nVisual Input\nCamera Image\nText Tokenizer \n‚Ä¶\nVisual Encoder \n‚Ä¶\nTransformer\nAction\nHead\n[‚àÜùë•, ‚àÜùúÉ, ‚àÜùëîùëüùëñùëù]ùëá\nVision-Language-Action Model\nText Input\nNatual Language \nTask Instruction\nPick the Coke Can \nbeside the Orange\nFig. 1: The VLA model takes camera images and natural language task\ninstructions as inputs. Using a transformer-based encoding and decoding\nprocess, the VLA model directly generates control commands for the robots.\nmodels [16], [17], primarily designed for robotic manipulation\ntasks [18]‚Äì[20]. The inputs to the VLA models consist\nof images captured by cameras and user-provided natural\nlanguage instructions that describe the desired task. The VLA\nmodels then directly generate control commands, e.g., the\npose of the end-effector, to guide the robotic manipulator\nin completing the assigned tasks based on the inputs [18]\n(see Fig. 1). These large pre-trained VLA models offer a\nnovel approach to robotic control that not only mitigates the\nneed for programming low-level task and motion controllers\nbut also fosters direct interaction between robots and users\nthrough natural language instructions. This innovation marks\na promising advancement toward achieving higher levels of\nrobot intelligence, bringing us closer to the realization of\nfully autonomous and intelligent robotic systems [21].\nHowever, the data-driven nature of VLA models introduces\nseveral challenges. For example, the effectiveness of task\nexecution is highly reliant on the quality of the training\ndata used to develop these models [16]. Moreover, the\nlimited interpretability of VLA models raises concerns about\ntheir reliability, robustness, and trustworthiness [22]. These\nchallenges underscore the need for a comprehensive platform\nto test and evaluate the performance of VLA models across\na variety of manipulation tasks and scenarios.\nUnfortunately, as an emerging field, related quality as-\nsurance methods are still at a very early stage, and there\nis currently no platform specifically designed to test and\nevaluate VLA models automatically. This gap in evaluation\nframeworks highlights the need for reliable tools to measure\nthe performance and robustness of these models. In response\nto this need, a simulation platform called SimplerEnv was\nintroduced in\n[23]. Built on the SAPIEN simulator [24]\narXiv:2410.05191v1  [cs.RO]  7 Oct 2024\n[Environment Description]\n‚Ä¢\nNumber and details of \nObjects\n‚Ä¢\nEnvironmental Setup\n[Task Instruction]\n‚Ä¢\nObjects\n‚Ä¢\nActions\nLLM\nObject \nDatabase\nLanguage-Driven Simulation Environment Generation\nNatural Language Task Instruction Paraphrase\nNatural Language Input\nObject \nConfiguration\nPick up the Apple\nGrasp the Apple\nTask\nInstructions\nSimulation Scene\nHigh-level Testing Commend \nGenerate X testing scenes \nabout picking up an object\nTask\nInstructions\nA Set of \nSimulation \nScenes\nUser\nVLA Model\nCan\nBottle\nEnvironmental\nSetup\nCamera\nLighting\nSimulator\nBatch-Style Evaluation\n‚Ä¶\nBasic Performance\nRobustness (Visual Input Variation)\nRobustness (Textual Input Variation)\nSemantics\nValidation\nOriginal Instruction\nVaried Instructions\nFig. 2: Overview of LADEV. LADEV proposes: (1) Language-driven simulation environment generation; (2) Natural language task instruction paraphrase;\n(3) Batch-style evaluation. For details about the prompts used in this work, please refer to the preprint version or the supplementary website of this paper.\nand the ManiSkill2 benchmark [25], it includes multiple\ntypical pick-and-place scenarios and various VLA models.\nBy generating simulation environments that replicate real-\nworld training conditions, SimplerEnv is able to assess the\nperformance of VLA models in a simulated setting. However,\nmodifying the simulation environments in SimplerEnv re-\nquires manual adjustments, which can be labor-intensive when\ntesting numerous different environments for comprehensive\nevaluations. Moreover, SimplerEnv alters only the simulated\nmanipulation scenes, e.g., the objects in the environment,\nwhich solely affect the visual input to the VLA models.\nThe natural language task instruction, i.e., a crucial input\ncomponent that specifies the manipulation tasks, remains\nunchanged in SimplerEnv. To thoroughly test and evaluate\nVLA models, it is essential not only to efficiently generate\na wide range of manipulation scenarios but also to create\ndiverse natural language task instructions. These instructions\nshould describe different tasks or express the same task using\nvarying sentence structures and vocabulary to effectively test\nthe language input aspect of the VLA models.\nTo achieve this, we propose in this work a compre-\nhensive language-driven testing and evaluation platform\ncalled LADEV, which is specifically designed for VLA\nmodels. Building on SimplerEnv, we introduce three major\nadvancements in LADEV: (1) Language-driven Simulation\nEnvironment Generation: instead of manual adjustments, we\nintroduce an automated mechanism to generate simulation\nenvironments based on simple language descriptions of\nthe desired manipulation scenarios. Using LLMs, these\ndescriptions are translated into environmental configurations\ncompatible with the simulator for constructing the simulation\nenvironment (see Fig. 2). To further expand simulation\ndiversity and incorporate a wide variety of objects, we also\nintegrate LADEV with the YCB object dataset [26], enabling\nthe automatic selection and inclusion of appropriate object\nmodels in the simulation based on the given language input.\n(2) Natural Language Task Instruction Paraphrase: in addition\nto generating simulation environments, we also propose a\nmethod for paraphrasing natural language task instructions.\nGiven an original instruction for the desired manipulation task,\nwe use LLMs to create alternative sentences that convey the\nsame task but with different sentence structures and wording\n(see Fig. 2). (3) Batch-Style Evaluation: to further streamline\nthe testing and evaluation process, we implement a batch-style\ngeneration mechanism capable of creating numerous distinct\ntest environments from a single command input. Specifically,\nwe ask an LLM to generate a complete testing script with\ndescriptions of diverse manipulation scenes, which are then\npassed to the scene generation process to assess the VLA\nmodel‚Äôs performance in each individual scenario.\nThe contributions of this paper are summarized as follows:\n‚Ä¢ We propose a novel language-driven approach that au-\ntonomously generates simulation environments from natural\nlanguage descriptions of the desired manipulation tasks.\nThis fully automated process greatly improves the efficiency\nof testing and evaluating VLA models, providing a solid\nfoundation for comprehensive performance assessment.\n‚Ä¢ We present a paraphrase mechanism that transforms the\ngiven natural language task instruction into various forms,\nenabling a comprehensive assessment of VLA models‚Äô\nability to handle diverse language inputs. This capability\nfills a gap in prior evaluations of VLA models, which\nfocused exclusively on simulation environments while\nneglecting the essential role of language input.\n‚Ä¢ We introduce a batch-style generation approach that is able\nto construct a diverse range of manipulation scenarios from\na single input command. This ‚Äúone-line‚Äù testing command\nenables rigorous large-scale testing and evaluation of VLA\nmodels in an efficient way.\n‚Ä¢ Using the proposed LADEV platform, we conduct a\nthorough and extensive evaluation of multiple state-of-the-\nart VLA models. Specifically, we examined the performance\nof seven VLA models on four robotic manipulation tasks\nusing over 4,000 distinct scenes, showcasing their actual\ncapabilities in different scenarios.\nII. RELATED WORK\nA. LLM and VLM in Robotics\nIn recent research, LLMs have been applied to various\nrobotic tasks, such as decision-making [6], [8], [10] and\nreasoning [7], [9], [27]. For instance, [6] leverages LLMs‚Äô\nsemantic capabilities to process natural language instructions,\nenabling robots to perform tasks assigned by humans through\na value function. Similarly, [9] utilizes LLMs to evaluate the\nfeasibility of task plans in a dialogue-based format, allowing\nrobots to correct their actions as needed. Other works have\nexplored using LLMs for task and motion planning [3],\n[28]‚Äì[32]. For example, [28] uses LLMs to guide object\nrearrangement, improving both autonomy and efficiency.\nMeanwhile, [5] explores the potential of LLMs with a\nself-refinement mechanism for long-horizon sequential task\nplanning, increasing task success rates compared to a zero-\nshot LLM approach. The incorporation of LLMs significantly\nadvances robotic intelligence, enhancing both autonomy and\ninteraction with human users.\nExtended from LLMs, an increasing number of studies now\nhave utilized VLMs to equip robotic systems with the ability\nto process visual inputs [13], [33]. One common application\nof VLMs in robotics is reasoning about the environment\nand identifying interactable objects [34]‚Äì[38]. For instance,\n[35] combines VLM and LLM to generate 3D affordance\nand constraint maps that guide robotic manipulation tasks.\nSimilarly, [36] proposes a physically grounded VLM to\nimprove the interaction between the robot and the object. [37]\nintroduces a VLM-based navigation approach for determining\nthe robot‚Äôs motion in human-centered environments. The\nintegration of visual processing capabilities further enhances\nrobots‚Äô understanding of tasks and environments, opening up\nthe potential for achieving general robotic intelligence [17].\nB. VLA Models in Robotics\nVLA models are end-to-end multi-modality foundation\nmodels evolved from VLMs [17], [39], [40]. Currently,\nmost VLA models are designed for robotic manipulation\ntasks, such as pick-and-place and grasping [16], [18], [20],\n[41]‚Äì[43]. One of the pioneering works in VLA models\nis RT-1 [18], which combines a FiLM EfficientNet and a\ntransformer to learn control policies from 130k real-world\nrobot demonstrations. RT-2 [41] advances RT-1 by introducing\nco-fine-tuning, integrating low-level control policies with\nhigh-level task planners to create a more comprehensive\nrobotic system. Since the release of the Open X-Embodiment\ndataset [16], a series of VLA models have been developed by\neither training or fine-tuning on this dataset, such as Open-\nVLA [20], Octo [42], and LLaRA [43]. These models have\ndemonstrated strong performance in their respective training\nenvironments, showing great potential for enabling intelligent\nrobotic manipulation using only image and language inputs.\nHowever, ensuring the reliability and robustness of VLA\nmodels is challenging, as their performance heavily relies\non the quality of the training data [2]. This necessitates an\nextensive testing and evaluation platform specifically designed\nfor VLA models. As previously mentioned, the SimplerEnv,\nintroduced in [23], provides valuable simulation environments.\nHowever, it requires manual adjustments for environment\nconstruction and neglects the impact of language inputs. To\novercome these limitations, we therefore propose LADEV\nin this work, which enables a more efficient, comprehensive,\nand automated evaluation process for VLA models.\nIII. LADEV\nIn this section, we introduce details about the proposed\nLADEV platform. First, we describe how LLMs are leveraged\nto generate simulation environments from natural language\ndescriptions of the desired manipulation scenarios. Next, we\nintroduce a paraphrase mechanism that alters the given natural\nlanguage task instructions. Lastly, we present a batch-style\nevaluation method that greatly accelerates the evaluation\nprocess with improved efficiency. Due to the page limit,\ndetailed information about the prompts used in this work\nis presented in the pre-print version and the website of this\npaper: https://sites.google.com/view/ladev.\nA. Language-Driven Simulation Environment Generation\nThe core concept behind the automated generation of\nsimulated manipulation environments is to convert natural lan-\nguage descriptions into simulator-compatible environmental\nconfigurations by leveraging LLMs. To achieve this, we use\na fixed structure for the natural language description, which\nincludes the following components (see also Fig. 3):\n‚Ä¢ Number and details of objects: First, we specify the total\nnumber of objects and provide additional details, such\nas their specific types and poses, to be included in the\nsimulation environment. If object details are not needed,\nthis part can be left blank.\n‚Ä¢ Environmental setup: Then, we describe the environmental\nsetup, including the lighting condition and camera pose. If\nnot specified, predefined default values will be used.\nUsing this structured description, we apply a two-step\nprocess to separately handle the object configuration and\nenvironmental setup during the generation process.\n1) Object Configuration: We begin by using the de-\nscriptions of the number and details of objects to select\nappropriate models from LADEV‚Äôs object model database,\nwhich combines the YCB object dataset [26] and the default\ndataset from SimplerEnv [23]. This is achieved by providing\na predefined list of all available objects, along with the natural\nlanguage description, to the LLM, which then generates a\nlist of object addition operations. The length of this list\ncorresponds to the specified number of objects, and each entry\nrepresents the addition of an object model to the simulation\nenvironment (see Fig. 3). If detailed object specifications\nare provided, the LLM prioritizes selecting models that best\nmatch the criteria. For example, if the user requests a coke\ncan, LADEV searches for the relevant model and adds it if\navailable. When no specific details are given, random objects\nare selected. Similarly, if specific object poses are provided,\nthey are translated into corresponding coordinates. Otherwise,\nLADEV assigns random values within a predefined range.\n2) Environmental Setup: We then prompt the LLM with\nthe description of the environmental setup to configure the\nsimulation parameters. In the current version of LADEV,\ntwo environmental configurations are considered: the lighting\ncondition and the camera pose. If a specific value is provided\nfor the lighting condition, the LLM generates an operation\ncommand to adjust the scene‚Äôs lighting intensity accordingly.\nLLM\nSimulator\nNatural Language Input\n[Object Configuration]\nCreate a testing scene, which includes \nfour different objects, objects should \n√Ö√ê\u0003ƒ®ƒÉ¬≠√Ü√ê√å\u0003ƒÆ√êƒ®¬≠ƒ´¬≠ƒ¥√êƒÉ≈ô»¶\n[Environmental Setup]\nThe camera should be placed at the \nƒ¥ƒèƒ®\u0003ƒè√•\u0003ƒ¥√¨√ê\u0003ƒè√Ö√æ√ê√Üƒ¥ƒÆ»¶\nSimulation Environment \nConfiguration \n{\"model_id\": \"opened_7up_can\", \n…áƒèƒ´√∞√êƒäƒ¥¬≠ƒ¥√∞ƒèƒä…á»§\u0003¬ô¬ô¬ô»£\u0003…ÄƒÉ…Ä»§\u0003¬ô¬ô¬ô\u0003\n…Äcamera_angle…Ä»§\u0003¬ô¬ô¬ô»¥ »¶\nDatabase\nFig. 3: Example of language-driven simulation environment generation.\nSimilarly, if a camera pose is specified, the LLM generates an\noperation to move or rotate the camera to match the desired\npose, ensuring proper visual inputs for the VLA models. If\nno information is given, predefined default values are applied.\nOnce the object addition and environmental adjustment\noperations are generated, we pass them to the simulator\nto construct the corresponding simulation environment. To\nenhance the accuracy of the LLM‚Äôs translations, we employ\nfew-shot in-context learning [44] in our prompts. This\napproach also ensures that the LLM outputs are formatted to\nbe compatible with the simulator. In LADEV, these operations\nare specified in JSON configuration formats. This language-\ndriven testing automation substantially reduces the time\nand effort required to construct simulation environments.\nMoreover, it enables an efficient evaluation of VLA models\nacross diverse manipulation scenarios. To further optimize\nour workflow, we later propose executing these evaluations in\na batch-style process, allowing for more efficient assessment.\nB. Natural Language Task Instruction Paraphrase\nTo evaluate the performance of VLA models in processing\ndiverse language inputs, we also propose a paraphrase\nmechanism that generates varied natural language task in-\nstructions. The paraphrase mechanism consists of two phases:\na generation phase and a validation phase (see Fig. 2).\nThe input to the generation phase is an original task\ninstruction that follows the standard format used in previous\nworks [18], [20], [41], [42], such as using ‚Äúpick up apple\"\nto describe a task involving picking up an apple. The goal\nof the generation phase is to produce a predefined number,\nk, of alternative instructions that convey the same meaning\nbut differ in sentence structure and wording. For example,\nthe original instruction ‚Äúpick up apple\" could be rephrased\nas ‚Äúgrasp apple‚Äù, ‚Äúlet‚Äôs pick the apple‚Äù, or ‚Äúcan you lift the\napple‚Äù, etc. This is achieved by prompting an LLM with the\noriginal instruction and guidelines for generating alternative\nsentences. The LLM then outputs k variations of the task\ninstructions with distinct wordings and structures.\nAfter generating a set of candidate sentences, we implement\na validation phase to ensure that each sentence accurately\nretains the same meaning as the original, ensuring the\nvalidity of the paraphrased sentences. This is achieved by\nusing a sentence BERT model [45] for similarity checking.\nSpecifically, we utilize the sentence BERT model to compute\nembeddings for each language task instruction and measure\nthe semantic similarity between the original ones and the can-\ndidate variations. If the similarity value exceeds a predefined\nthreshold, the varied task instruction is considered to have the\nsame meaning and is deemed valid. These valid instructions\nare then used to evaluate the VLA models‚Äô performance in\nhandling diverse language inputs.\nBy utilizing the proposed natural language task instruction\nparaphrase mechanism, we significantly enhance the diversity\nof language inputs for VLA models. This approach addresses\na crucial gap in the overall evaluation, focusing on the previ-\nously overlooked aspect of assessing how natural language\ntask instructions impact the performance of VLA models.\nC. Batch-Style Evaluation\nThrough the methods proposed in Sec.III-A and Sec.III-B,\nwe can generate a single manipulation scenario with multiple\ndiverse language task instructions that convey the same task.\nHowever, for a comprehensive evaluation of the VLA model,\nit is crucial to test its performance across various scenarios\nand tasks. To expedite this process, we introduce a batch-style\nevaluation approach that automatically generates a specified\nnumber of distinct manipulation scenarios from a single\nnatural language input.\nSpecifically, we instruct the LLM to automatically generate\ndiverse language inputs for executing both the simulation\nenvironment generation and task instruction paraphrase pro-\ncesses (see Fig. 2). Suppose the user wishes to create n\ntest scenes with k task instructions per scene. In this case,\nthe LLM is prompted to randomly generate n sets of natural\nlanguage inputs that specify objects and environmental setups,\nalong with n original task instructions following the standard\nformat. Each original task instruction is then used in the\nparaphrase process to generate k variations, resulting in a\ntotal of n √ó k language inputs. Due to the limited space, the\ndetailed prompt templates and examples used for in-context\nlearning are provided on the website accompanying this paper.\nThe proposed batch-style evaluation approach enables\nefficient large-scale testing of VLA models, facilitating a more\ncomprehensive and reliable assessment of their robustness\nand effectiveness. In the next section, we apply this approach\nto evaluate the performance of multiple state-of-the-art VLA\nmodels across various manipulation scenarios.\nIV. EXPERIMENTS\nIn this section, we first present the details of our experimen-\ntal setup. Then, we utilize a concrete example to illustrate\nhow LADEV generates a simulation environment from a\nnatural language input and paraphrases the task instruction.\nFinally, we present our large-scale experimental results in\nassessing the performance of multiple state-of-the-art VLA\nmodels, which demonstrate the efficiency and effectiveness\nof the proposed LADEV for VLA models‚Äô evaluation.\nA. Experimental Setup\nWe consider four robotic manipulation tasks in LADEV\nfor evaluating VLA models: (1) Pick up an object; (2) Move\nobject A near object B; (3) Put object A on object B; and\n(4) Put object A inside object B. For performance evaluation,\nwe assess the impact of the following factors:\n[Environment Description]\nGenerate a scene with \nthree objects. One object is \nthe plastic bottle.\n[Task Instruction]\nPick up the plastic bottle.\nNatural Language Input\nSimulation Scene\nVaried Natural Language Task Instructions\nGrasp the plastic bottle\nFetch the plastic bottle\nCan you pick up the plastic bottle\nLet‚Äôs pick up the plastic bottle\nVLA Model Evaluation\nFig. 4: Example of simulation scene generation and natural language instruction paraphrase with LADEV. See also the supplementary video for more details.\n‚Ä¢ Number of objects: We first investigated how the number\nof objects in the simulation scene affects performance.\nMore objects introduce additional obstacles, increasing the\ndifficulty for VLA models in correctly identifying the target\nobject. To evaluate this, we generated 100 test scenes for\neach task, with each scene containing 1 to 5 objects. Only\nbasic task instructions were used to focus on the effect of\nthe number of objects.\n‚Ä¢ Task instructions: For each task, we generated 100 test\nscenes with 1 to 4 randomly selected objects. Each scene\nwas evaluated using both the basic task instructions and\nthose paraphrased by LADEV to examine the influence of\nvaried language inputs. For each task, we randomly chose\none from ten paraphrased task instructions.\n‚Ä¢ Unseen objects: The LADEV‚Äôs object model database is the\ncombination of the SimplerEnv [23] (18 objects) and the\nYCB [46] (65 objects). Objects in SimplerEnv are generally\nconsidered part of the VLA models‚Äô training dataset, while\nobjects from the YCB can be regarded as unseen. We\nevaluated the effect of unseen objects by generating two\ngroups of 100 test scenes for each task, randomly including\n1 to 4 objects from either the SimplerEnv or the YCB.\n‚Ä¢ Environmental conditions: To assess the impact of environ-\nmental conditions, for each task, we created three sets of\n100 test scenes with 1 to 4 objects: one with default lighting\nand camera settings, one with randomly adjusted lighting\nconditions, and one with altered camera poses. To ensure\nall objects remain visible and recognizable, only small\nadjustments were made to lighting conditions (increasing\nor decreasing the lighting intensities by a maximum value\nof 0.5) and camera poses (a maximum rotation angle of\n5‚ó¶and a maximum moving distance of 5 cm).\nWe compare the following state-of-the-art VLA models:\nRT-1-1k, RT-1-58k, RT-1-400k [18], RT-1-X [16], Octo-base,\nOcto-small [42], and OpenVLA-7b [20]. We use GPT-4o as\nour LLM. All experiments are conducted on a server with an\nAMD 5955WX CPU and two NVIDIA RTX A6000 GPUs.\nFor more details about the experimental setup, please refer\nto the supplementary website of this paper.\nB. Environment Generation and Command Mutation\nWe first use a concrete example to demonstrate the\nprocesses of simulation environment generation and natural\nlanguage task instruction paraphrase. As shown in Fig. 4,\nthe natural language input instructs LADEV to create a\nsimulation environment with three objects, one of which is a\nplastic bottle. After searching the model database, LADEV\ngenerates the required environment. The basic task instruction,\n‚Äúpick up the plastic bottle\", is also paraphrased into four\nvariations by LADEV. These task instructions, along with the\ngenerated environment, are used to evaluate the performance\nof VLA models. This individual process can also be scaled\nusing the proposed batch-style evaluation mechanism, which\nautomatically generates multiple manipulation scenes, each\nwith varied task instructions, to comprehensively test the VLA\nmodels‚Äô performance.\nC. Performance Evaluation of VLA Models\nUsing the proposed LADEV, we performed a large-scale\nevaluation of VLA models by considering the aforementioned\nfactors. The success rates for completing the given tasks\nunder various conditions are shown in Table I-Table IV. We\nhighlight the following key observations:\n‚Ä¢ Number of objects: As shown in Table I, all VLA models\nperformed best when only one object was present, i.e., only\nthe target object. The performance of the VLA models\ndecreased as the number of objects increased. When five\nobjects were included, almost all models performed poorly\nacross all test scenes and tasks. Model-wise, RT-1-58k,\nRT-1-400k, and RT-1-X outperformed the other models on\nthe Pick Up task, while OpenVLA-7b achieved the highest\nperformance on the Move Near task. However, for the Put\nOn and Put In tasks, all VLA models showed poor results,\nwith success rates below 5%.\n‚Ä¢ Task instructions: From Table II, we observed a significant\nperformance drop when using paraphrased instructions\ncompared to the basic ones in the Pick Up and Move Near\ntasks. However, the performance differences in the Put On\nand Put In tasks were marginal, as the basic instructions\nalready resulted in poor performance.\n‚Ä¢ Unseen objects: As shown in Table III, VLA models per-\nformed worse with YCB objects compared to SimplerEnv\nobjects. For instance, models such as RT-1-58k, RT-1-\n400k, RT-1-X, and OpenVLA-7b performed relatively well\nin the Pick Up and Move Near tasks with SimplerEnv\nobjects. However, when manipulating YCB objects, their\nperformance dropped by 10% to 30%.\n‚Ä¢ Environmental conditions: Table IV reveals that even small\nchanges in environmental conditions could largely affect\nmodel performance. For lighting condition changes, the\nRT-1 and RT-1-X models were more adversely affected\nTABLE I: VLA models‚Äô performance with different numbers of objects.\nVLA Model\nPick Up\nMove Near\nPut On\nPut In\n1\n2\n3\n4\n5\nAvg.\n1\n2\n3\n4\n5\nAvg.\n1\n2\n3\n4\n5\nAvg.\n1\n2\n3\n4\n5\nAvg.\nRT-1-1k\n0%\n1%\n0%\n0%\n0%\n0.2%\n2%\n2%\n1%\n1%\n1%\n1.4%\n0%\n0%\n0%\n0%\n0%\n0.0%\n0%\n0%\n0%\n0%\n0%\n0.0%\nRT-1-58k\n36%\n41%\n27%\n23%\n21%\n29.6%\n11%\n11%\n9%\n10%\n6%\n9.4%\n0%\n0%\n0%\n0%\n0%\n0.0%\n0%\n0%\n0%\n0%\n0%\n0.0%\nRT-1-400k\n44%\n37%\n35%\n33%\n26%\n35.0%\n12%\n14%\n4%\n5%\n3%\n7.6%\n0%\n0%\n0%\n0%\n0%\n0.0%\n0%\n0%\n0%\n1%\n0%\n0.2%\nRT-1-X\n26%\n30%\n19%\n16%\n9%\n20.0%\n7%\n12%\n4%\n2%\n4%\n5.8%\n3%\n1%\n1%\n1%\n2%\n1.6%\n0%\n0%\n0%\n1%\n0%\n0.2%\nOcto-small\n2%\n0%\n1%\n0%\n0%\n0.6%\n3%\n1%\n6%\n0%\n0%\n2.0%\n4%\n5%\n6%\n1%\n2%\n3.6%\n0%\n3%\n0%\n0%\n2%\n1.0%\nOcto-base\n1%\n0%\n0%\n0%\n0%\n0.2%\n0%\n0%\n0%\n1%\n0%\n0.2%\n0%\n0%\n5%\n0%\n1%\n1.2%\n0%\n1%\n3%\n0%\n1%\n1.0%\nOpenVLA-7b\n12%\n7%\n8%\n7%\n2%\n7.2%\n23%\n18%\n12%\n8%\n2%\n12.6%\n1%\n5%\n1%\n1%\n2%\n2.0%\n5%\n1%\n1%\n4%\n0%\n2.2%\nTABLE II: Basic task instructions vs. paraphrased (Para.) task instructions.\nVLA Model\nPick Up\nMove Near\nPut On\nPut In\nBasic\nPara.\nBasic\nPara.\nBasic\nPara.\nBasic\nPara.\nRT-1-1k\n0%\n2%\n3%\n1%\n0%\n0%\n0%\n0%\nRT-1-58k\n28%\n17%\n12%\n6%\n0%\n1%\n1%\n0%\nRT-1-400k\n36%\n22%\n7%\n3%\n0%\n1%\n0%\n0%\nRT-1-X\n20%\n13%\n7%\n4%\n2%\n0%\n1%\n0%\nOcto-base\n0%\n0%\n2%\n0%\n2%\n3%\n3%\n1%\nOcto-small\n0%\n1%\n2%\n5%\n4%\n3%\n1%\n1%\nOpenVLA-7b\n8%\n7%\n12%\n4%\n2%\n4%\n1%\n2%\nTABLE III: Objects from SimplerEnv (SE) vs. objects from YCB.\nVLA Model\nPick Up\nMove Near\nPut On\nPut In\nSE\nYCB\nSE\nYCB\nSE\nYCB\nSE\nYCB\nRT-1-1k\n0%\n0%\n3%\n0%\n0%\n0%\n0%\n1%\nRT-1-58k\n28%\n2%\n12%\n7%\n0%\n0%\n1%\n0%\nRT-1-400k\n36%\n5%\n7%\n3%\n0%\n2%\n0%\n0%\nRT-1-X\n20%\n3%\n7%\n0%\n2%\n2%\n1%\n1%\nOcto-base\n0%\n0%\n2%\n0%\n2%\n1%\n3%\n0%\nOcto-small\n0%\n0%\n2%\n0%\n4%\n0%\n1%\n3%\nOpenVLA-7b\n8%\n0%\n12%\n6%\n2%\n0%\n1%\n0%\ncompared to the others. However, for camera pose adjust-\nments, no notable performance differences were observed.\nV. DISCUSSION\nA. Pros and Cons of VLA Models\nBy combining visual, linguistic, and action-based infor-\nmation, VLA models offer several advantages to robotic\nsystems. For instance, they enable robots to better inter-\npret their surroundings and execute tasks using natural\nlanguage instructions, reducing the dependence on hardcoded\nor structured inputs. This leads to more intuitive human-\nrobot communication, enhancing interaction flexibility while\nboosting the robots‚Äô autonomy and intelligence. However,\nVLA models also face multiple challenges. For example,\ntraining these models requires large, multi-modal datasets\nand significant computational resources. Unfortunately, high-\nquality datasets that align visual inputs, language descrip-\ntions, and corresponding actions are scarce. Moreover, our\nexperiments show that current VLA models still struggle\nwith even simple manipulation tasks under varied conditions,\nhighlighting the need for deeper exploration in this area to\nachieve better performance.\nB. Limitations and Future Work\nOne limitation of the current version of LADEV is that\nit only considers four manipulation tasks. This is primarily\ndue to the fact that state-of-the-art VLA models are still\nonly trained for simple tasks. Another drawback is that\nour evaluations are conducted solely in simulations, as\nTABLE IV: Influence of different environmental conditions: default (De.),\nmutated lighting (Li.), and mutated camera poses (Ca.).\nVLA Model\nPick Up\nMove Near\nPut On\nPut In\nDe.\nLi.\nCa.\nDe.\nLi.\nCa.\nDe. Li. Ca.\nDe. Li. Ca.\nRT-1-1k\n0%\n0%\n1%\n3%\n3%\n6%\n0% 0% 2%\n0% 0% 0%\nRT-1-58k\n28% 23% 31%\n12% 12% 11%\n0% 0% 0%\n1% 1% 0%\nRT-1-400k\n36% 20% 30%\n7%\n6%\n8%\n0% 0% 0%\n0% 0% 0%\nRT-1-X\n20%\n9% 14%\n7%\n8%\n7%\n2% 2% 1%\n1% 1% 1%\nOcto-base\n0%\n0%\n1%\n2%\n2%\n3%\n2% 2% 3%\n3% 3% 2%\nOcto-small\n0%\n1%\n0%\n2%\n2%\n2%\n4% 4% 2%\n1% 1% 2%\nOpenVLA-7b\n8% 12% 14%\n12% 12% 15%\n2% 2% 1%\n1% 1% 1%\nperforming comprehensive real-world experiments is difficult\nand resource-intensive. To reduce the gap between simulation\nand reality, we adopt the same approach as SimplerEnv by\nusing real-world images as backgrounds for the visual inputs\nto the VLA models. However, further research is needed\nto better minimize the simulation-to-reality gap and develop\nmore efficient methods for assessing the performance of VLA\nmodels in real-world conditions.\nFor future work, we plan to expand the LADEV platform by\nincorporating more object models and manipulation scenarios,\nwhich would greatly increase its diversity and effectiveness.\nAnother potential direction is to compare the performance of\nVLA models in both simulation and real-world environments\nacross several representative tasks, serving as an indicator\nof their reliability in practical applications. However, how\nto select the most appropriate and representative tasks will\nrequire further in-depth research.\nVI. CONCLUSION\nIn this work, we propose LADEV, a testing and evaluation\nplatform for VLA models in robotic manipulation tasks.\nBy introducing a language-driven framework, we efficiently\ngenerate simulation environments from simple natural lan-\nguage inputs, mitigating the need for manual adjustments.\nTo assess the impact of language instructions on VLA\nmodels, we also present a task instruction paraphrase approach\nthat automatically generates diverse sentences to enrich the\nlanguage input. Moreover, to further improve the evaluation\nefficiency, we develop a batch-style mechanism that creates\nmultiple testing scenarios from a single command, enabling a\ncomprehensive and streamlined assessment of VLA models‚Äô\nperformance. Our platform notably improves the evaluation\nprocess and establishes a strong baseline for advancing VLA\nmodels, paving the way for more intelligent robotic systems\nwith enhanced autonomy and decision-making capabilities.\nREFERENCES\n[1] F. Zeng, W. Gan, Y. Wang, N. Liu, and P. S. Yu, ‚ÄúLarge language\nmodels for robotics: A survey,‚Äù arXiv preprint arXiv:2311.07226, 2023.\n[2] J. Wang, Z. Wu, Y. Li, H. Jiang, P. Shu, E. Shi, H. Hu, C. Ma, Y. Liu,\nX. Wang et al., ‚ÄúLarge language models for robotics: Opportunities,\nchallenges, and perspectives,‚Äù arXiv preprint arXiv:2401.04334, 2024.\n[3] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, ‚ÄúLanguage models\nas zero-shot planners: Extracting actionable knowledge for embodied\nagents,‚Äù in International Conference on Machine Learning.\nPMLR,\n2022.\n[4] S. S. Kannan, V. L. Venkatesh, and B.-C. Min, ‚ÄúSmart-llm: Smart\nmulti-agent robot task planning using large language models,‚Äù arXiv\npreprint arXiv:2309.10062, 2023.\n[5] Z. Zhou, J. Song, K. Yao, Z. Shu, and L. Ma, ‚ÄúIsr-llm: Iterative self-\nrefined large language model for long-horizon sequential task planning,‚Äù\nin 2024 IEEE International Conference on Robotics and Automation\n(ICRA).\nIEEE, 2024, pp. 2081‚Äì2088.\n[6] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al., ‚ÄúDo as i can,\nnot as i say: Grounding language in robotic affordances,‚Äù arXiv preprint\narXiv:2204.01691, 2022.\n[7] J. Song, Z. Zhou, J. Liu, C. Fang, Z. Shu, and L. Ma, ‚ÄúSelf-refined\nlarge language model as automated reward function designer for deep\nreinforcement learning in robotics,‚Äù arXiv preprint arXiv:2309.06687,\n2023.\n[8] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg, ‚ÄúText2motion:\nFrom natural language instructions to feasible plans,‚Äù arXiv preprint\narXiv:2303.12153, 2023.\n[9] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y. Chebotar et al., ‚ÄúInner monologue:\nEmbodied reasoning through planning with language models,‚Äù arXiv\npreprint arXiv:2207.05608, 2022.\n[10] S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen, D.-A.\nHuang, E. Aky√ºrek, A. Anandkumar et al., ‚ÄúPre-trained language mod-\nels for interactive decision-making,‚Äù Advances in Neural Information\nProcessing Systems, vol. 35, pp. 31 199‚Äì31 212, 2022.\n[11] W. Street, ‚ÄúLlm theory of mind and alignment: Opportunities and risks,‚Äù\narXiv preprint arXiv:2405.08154, 2024.\n[12] Y. Du, Z. Liu, J. Li, and W. X. Zhao, ‚ÄúA survey of vision-language\npre-trained models,‚Äù arXiv preprint arXiv:2202.10936, 2022.\n[13] J. Zhang, J. Huang, S. Jin, and S. Lu, ‚ÄúVision-language models for\nvision tasks: A survey,‚Äù IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2024.\n[14] C. Cui, Y. Ma, X. Cao, W. Ye, Y. Zhou, K. Liang, J. Chen, J. Lu,\nZ. Yang, K.-D. Liao et al., ‚ÄúA survey on multimodal large language\nmodels for autonomous driving,‚Äù in Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, 2024, pp.\n958‚Äì979.\n[15] Z. Long, G. Killick, R. McCreadie, and G. Aragon-Camarasa, ‚ÄúRobollm:\nRobotic vision tasks grounded on multimodal large language models,‚Äù\nin 2024 IEEE International Conference on Robotics and Automation\n(ICRA).\nIEEE, 2024, pp. 12 428‚Äì12 435.\n[16] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Ir-\npan, A. Khazatsky, A. Rai, A. Singh, A. Brohan et al., ‚ÄúOpen x-\nembodiment: Robotic learning datasets and rt-x models,‚Äù arXiv preprint\narXiv:2310.08864, 2023.\n[17] Y. Ma, Z. Song, Y. Zhuang, J. Hao, and I. King, ‚ÄúA survey\non vision-language-action models for embodied ai,‚Äù arXiv preprint\narXiv:2405.14093, 2024.\n[18] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., ‚ÄúRt-1:\nRobotics transformer for real-world control at scale,‚Äù arXiv preprint\narXiv:2212.06817, 2022.\n[19] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart,\nS. Welker, A. Wahid et al., ‚ÄúRt-2: Vision-language-action models\ntransfer web knowledge to robotic control,‚Äù in Conference on Robot\nLearning.\nPMLR, 2023, pp. 2165‚Äì2183.\n[20] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair,\nR. Rafailov, E. Foster, G. Lam, P. Sanketi et al., ‚ÄúOpenvla: An open-\nsource vision-language-action model,‚Äù arXiv preprint arXiv:2406.09246,\n2024.\n[21] Z. Xu, K. Wu, J. Wen, J. Li, N. Liu, Z. Che, and J. Tang, ‚ÄúA survey on\nrobotics with foundation models: toward embodied ai,‚Äù arXiv preprint\narXiv:2402.02385, 2024.\n[22] D. Myers, R. Mohawesh, V. I. Chellaboina, A. L. Sathvik, P. Venkatesh,\nY.-H. Ho, H. Henshaw, M. Alhawawreh, D. Berdik, and Y. Jararweh,\n‚ÄúFoundation and large language models: fundamentals, challenges,\nopportunities, and social impacts,‚Äù Cluster Computing, vol. 27, no. 1,\npp. 1‚Äì26, 2024.\n[23] X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu,\nI. Lunawat, I. Sieh, S. Kirmani et al., ‚ÄúEvaluating real-world robot\nmanipulation policies in simulation,‚Äù arXiv preprint arXiv:2405.05941,\n2024.\n[24] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang,\nY. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su,\n‚ÄúSAPIEN: A simulated part-based interactive environment,‚Äù in The\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2020.\n[25] J. Gu, F. Xiang, X. Li, Z. Ling, X. Liu, T. Mu, Y. Tang, S. Tao, X. Wei,\nY. Yao et al., ‚ÄúManiskill2: A unified benchmark for generalizable\nmanipulation skills,‚Äù arXiv preprint arXiv:2302.04659, 2023.\n[26] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and\nA. M. Dollar, ‚ÄúBenchmarking in manipulation research: The ycb\nobject and model set and benchmarking protocols,‚Äù arXiv preprint\narXiv:1502.03143, 2015.\n[27] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker,\nF. Tombari, A. Purohit, M. Ryoo, V. Sindhwani et al., ‚ÄúSocratic models:\nComposing zero-shot multimodal reasoning with language,‚Äù arXiv\npreprint arXiv:2204.00598, 2022.\n[28] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, ‚ÄúTask and motion planning\nwith large language models for object rearrangement,‚Äù arXiv preprint\narXiv:2303.06247, 2023.\n[29] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\nJ. Thomason, and A. Garg, ‚ÄúProgprompt: Generating situated robot\ntask plans using large language models,‚Äù in International Conference\non Robotics and Automation.\nIEEE, 2023, pp. 11 523‚Äì11 530.\n[30] T. Silver, S. Dan, K. Srinivas, J. B. Tenenbaum, L. P. Kaelbling, and\nM. Katz, ‚ÄúGeneralized planning in pddl domains with pretrained large\nlanguage models,‚Äù arXiv preprint arXiv:2305.11014, 2023.\n[31] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-P√©rez,\nand L. P. Kaelbling, ‚ÄúPddl planning with pretrained large language\nmodels,‚Äù in NeurIPS 2022 Foundation Models for Decision Making\nWorkshop, 2022.\n[32] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone,\n‚ÄúLlm+ p: Empowering large language models with optimal planning\nproficiency,‚Äù arXiv preprint arXiv:2304.11477, 2023.\n[33] Y. Du, K. Konyushkova, M. Denil, A. Raju, J. Landon, F. Hill,\nN. de Freitas, and S. Cabi, ‚ÄúVision-language models as success\ndetectors,‚Äù arXiv preprint arXiv:2303.07280, 2023.\n[34] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, ‚ÄúR3m: A\nuniversal visual representation for robot manipulation,‚Äù arXiv preprint\narXiv:2203.12601, 2022.\n[35] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, ‚ÄúVoxposer:\nComposable 3d value maps for robotic manipulation with language\nmodels,‚Äù arXiv preprint arXiv:2307.05973, 2023.\n[36] J. Gao, B. Sarkar, F. Xia, T. Xiao, J. Wu, B. Ichter, A. Majumdar, and\nD. Sadigh, ‚ÄúPhysically grounded vision-language models for robotic\nmanipulation,‚Äù in IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2024.\n[37] D. Song, J. Liang, A. Payandeh, X. Xiao, and D. Manocha, ‚ÄúVlm-\nsocial-nav: Socially aware robot navigation through scoring using\nvision-language models,‚Äù arXiv preprint arXiv:2404.00210, 2024.\n[38] P. Liu, Y. Orru, C. Paxton, N. M. M. Shafiullah, and L. Pinto, ‚ÄúOk-\nrobot: What really matters in integrating open-knowledge models for\nrobotics,‚Äù arXiv preprint arXiv:2401.12202, 2024.\n[39] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing,\nW. Zhang, H. Liu et al., ‚ÄúVision-language foundation models as\neffective robot imitators,‚Äù arXiv preprint arXiv:2311.01378, 2023.\n[40] A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong,\nP. Wohlhart, S. Kirmani, B. Zitkovich, F. Xia et al., ‚ÄúOpen-world\nobject manipulation using pre-trained vision-language models,‚Äù arXiv\npreprint arXiv:2303.00905, 2023.\n[41] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choro-\nmanski, T. Ding, D. Driess, A. Dubey, C. Finn et al., ‚ÄúRt-2: Vision-\nlanguage-action models transfer web knowledge to robotic control,‚Äù\narXiv preprint arXiv:2307.15818, 2023.\n[42] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees,\nS. Dasari, J. Hejna, T. Kreiman, C. Xu et al., ‚ÄúOcto: An open-source\ngeneralist robot policy,‚Äù arXiv preprint arXiv:2405.12213, 2024.\n[43] X. Li, C. Mata, J. Park, K. Kahatapitiya, Y. S. Jang, J. Shang,\nK. Ranasinghe, R. Burgert, M. Cai, Y. J. Lee et al., ‚ÄúLlara: Super-\ncharging robot learning data for vision-language policy,‚Äù arXiv preprint\narXiv:2406.20095, 2024.\n[44] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., ‚ÄúLanguage models\nare few-shot learners,‚Äù Advances in Neural Information Processing\nSystems, vol. 33, pp. 1877‚Äì1901, 2020.\n[45] N. Reimers and I. Gurevych, ‚ÄúSentence-bert: Sentence embeddings\nusing siamese bert-networks,‚Äù in Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019, pp. 3982‚Äì3992.\n[46] B. Calli, A. Singh, A. Walsman, S. Srinivasa, P. Abbeel, and A. M.\nDollar, ‚ÄúThe ycb object and model set: Towards common benchmarks\nfor manipulation research,‚Äù in 2015 international conference on\nadvanced robotics (ICAR).\nIEEE, 2015, pp. 510‚Äì517.\n",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "published": "2024-10-07",
  "updated": "2024-10-07"
}