{
  "id": "http://arxiv.org/abs/2410.07564v1",
  "title": "Boosting Deep Ensembles with Learning Rate Tuning",
  "authors": [
    "Hongpeng Jin",
    "Yanzhao Wu"
  ],
  "abstract": "The Learning Rate (LR) has a high impact on deep learning training\nperformance. A common practice is to train a Deep Neural Network (DNN) multiple\ntimes with different LR policies to find the optimal LR policy, which has been\nwidely recognized as a daunting and costly task. Moreover, multiple times of\nDNN training has not been effectively utilized. In practice, often only the\noptimal LR is adopted, which misses the opportunities to further enhance the\noverall accuracy of the deep learning system and results in a huge waste of\nboth computing resources and training time. This paper presents a novel\nframework, LREnsemble, to effectively leverage effective learning rate tuning\nto boost deep ensemble performance. We make three original contributions.\nFirst, we show that the LR tuning with different LR policies can produce highly\ndiverse DNNs, which can be supplied as base models for deep ensembles. Second,\nwe leverage different ensemble selection algorithms to identify high-quality\ndeep ensembles from the large pool of base models with significant accuracy\nimprovements over the best single base model. Third, we propose LREnsemble, a\nframework that utilizes the synergy of LR tuning and deep ensemble techniques\nto enhance deep learning performance. The experiments on multiple benchmark\ndatasets have demonstrated the effectiveness of LREnsemble, generating up to\n2.34% accuracy improvements over well-optimized baselines.",
  "text": "Boosting Deep Ensembles with Learning Rate Tuning\nHongpeng Jin\nYanzhao Wu\nFlorida International University\nMiami, FL, United States\n{hjin008, yawu}@fiu.edu\nAbstract\nThe Learning Rate (LR) has a high impact on deep learning training performance. A com-\nmon practice is to train a Deep Neural Network (DNN) multiple times with different LR policies\nto find the optimal LR policy, which has been widely recognized as a daunting and costly task.\nMoreover, multiple times of DNN training has not been effectively utilized. In practice, often\nonly the optimal LR is adopted, which misses the opportunities to further enhance the overall\naccuracy of the deep learning system and results in a huge waste of both computing resources\nand training time. This paper presents a novel framework, LREnsemble, to effectively leverage\neffective learning rate tuning to boost deep ensemble performance. We make three original contri-\nbutions. First, we show that the LR tuning with different LR policies can produce highly diverse\nDNNs, which can be supplied as base models for deep ensembles. Second, we leverage differ-\nent ensemble selection algorithms to identify high-quality deep ensembles from the large pool\nof base models with significant accuracy improvements over the best single base model. Third,\nwe propose LREnsemble, a framework that utilizes the synergy of LR tuning and deep ensem-\nble techniques to enhance deep learning performance. The experiments on multiple benchmark\ndatasets have demonstrated the effectiveness of LREnsemble, generating up to 2.34% accuracy\nimprovements over well-optimized baselines.\n1\nIntroduction\nDeep learning has been widely applied in many real-world applications, represented by the recent\nsuccess of Large Language Models [10, 36, 27, 39, 40, 6, 45] to achieve human-like performance.\nThe Learning Rate (LR) is a critical hyperparameter in deep learning with high impacts on the Deep\nNeural Network (DNN) training performance [42, 41, 1]. Good learning rates can train deep learn-\ning models with high accuracy or reach accuracy thresholds fast with low training costs [52, 16].\nConversely, too small or too large learning rates may result in slow convergence or model divergence\nduring deep learning training [42, 1, 53]. Hence, finding a good learning rate is challenging, and\nrequires meticulous tuning. Even though numerous efforts have been devoted to designing learn-\ning rate policies to leverage decaying, cyclic, or composite functions to specify LR values during\ntraining to facilitate LR tuning [33, 42, 52], it still requires multiple times of deep learning training\nwith trial-and-errors to find the optimal LR policy, making the LR tuning not only tedious but also\ntime-consuming. It is still an open research challenge to identify the optimal LR policy. In practice,\noften only the optimal LR policy will be selected to train the deep learning model and shared across\nthe deep learning community. For a new learning task, a new dataset, or a new model, the LR tuning\nprocess will repeat and cause a huge waste of computing resources. We bring up a novel research\nquestion in this study, i.e., how to effectively utilize the sub-optimal LR tuning outcomes to improve\ndeep learning performance.\n1\narXiv:2410.07564v1  [cs.LG]  10 Oct 2024\nFigure 1: Visualization of deep learning\ntraining paths with different LR poli-\ncies: different LR policies can lead to\ndifferent optimization trajectories and\nproduce diverse deep learning models.\nDeep neural network Ensembles (Deep Ensembles)\nrepresent a category of emerging techniques, which\nhold the potential to combine multiple diverse DNNs\nto enhance the predictive performance over individual\nDNNs [30, 19, 51, 49, 50, 48].\nBoth good individual\nmodel accuracy and high ensemble diversity contribute\nto the potential accuracy improvements by deep ensem-\nbles [30, 48, 49, 51, 34, 58, 50]. LR tuning will employ\ndifferent LR policies to train DNNs, resulting in differ-\nent optimization trajectories, each represented by differ-\nent colors in Figure 1. Multiple diverse DNNs produced\nthrough this process can be potentially combined through\ndeep ensembles to enhance the overall deep learning ac-\ncuracy. We show the accuracy of individual DNNs ob-\ntained with LR tuning in blue or gray bars in Figure 2a\nfor training WRN-28-10 [56] on CIFAR-100 [28], where\neach bar corresponds to a data point in Figure 2b with dif-\nferent LR policies and different initial LR values. We ob-\nserve that these DNNs have varying model accuracy, exhibiting a certain degree of diversity, making\nthem potential candidates for creating high-quality deep ensembles. In Figure 2a, the high-quality\nensemble (red bar) achieves the accuracy of 85.32%, which significantly outperforms the best single\nDNN of 83.58% accuracy. On the other hand, we also found that simply combining all available\nDNNs will introduce high ensemble execution costs but may not produce optimal accuracy. For\nexample, the red ensemble in Figure 2a with 11 member DNNs (blue bars) outperforms the entire\nensemble of all 16 DNNs with 85.23% accuracy. Hence, given the large number of models produced\nby LR tuning, it is non-trivial to identify high-quality ensembles to improve deep learning accuracy\nand reduce the ensemble cost.\n(a) LREnsemble vs. Individual models\n(b) Individual models by LR tuning\nFigure 2:\nAccuracy improvements by LREnsemble\n(WRN-28-10 on CIFAR-100): LREnsemble can lever-\nage Learning Rate (LR) tuning to generate diverse in-\ndividual models (blue or gray bars in Figure 2a) and\nselect the complementary member models (blue bars:\nselected models) to boost ensemble accuracy (red bar:\nLREnsemble with over 1.74% accuracy improvements).\nFigure 2b shows the accuracy of individual models\ntrained using different LR policies with different initial\nLR values using our LREnsemble framework.\nIn this paper, we propose LREnsem-\nble, a novel framework to leverage effec-\ntive LR tuning to boost deep Ensemble\nperformance, which addresses all three\nresearch challenges mentioned above: (1)\nhow to perform effective LR tuning to\nidentify optimal LR policies for train-\ning individual DNNs for delivering high\naccuracy, (2) how to utilize sub-optimal\nmodels generated by LR tuning to en-\nhance deep learning accuracy and avoid\nthe waste of computing resources, and\n(3) how to efficiently build high-quality\ndeep ensembles from these models pro-\nduced by LR tuning to further enhance\noverall prediction performance. We make\nthree novel contributions. First, we show\nthat leveraging different LR policies to\nperform LR tuning can not only iden-\ntify optimal LR policies but also produce\nhighly diverse DNNs to facilitate ensem-\n2\nble learning. Second, we demonstrate that the sub-optimal models produced by LR tuning can\nbe effectively leveraged to develop deep ensembles to enhance the overall predictive performance.\nThird, we introduce LREnsemble, a novel framework that exploits the synergy between LR tuning\nand deep ensemble to enhance deep learning performance. We conduct comprehensive experiments\non multiple benchmark datasets and DNNs, including ViT [15] and a representative Large Language\nModel (LLM), LLaMA [45], to demonstrate the effectiveness of our LREnsemble framework. To\nthe best of our knowledge, this is the first to utilize the sub-optimal models produced by LR tuning\nto improve deep learning performance.\n2\nRelated Work\nIn this section, we summarize and discuss related studies in three respects: (1) Learning Rate Tuning,\n(2) Deep Ensembles, and (3) Large Language Models.\nLearning Rate Tuning. The Learning Rate (LR) is one of the most critical hyperparameters in\ndeep learning training. Different from other static hyperparameters, such as the batch size and the\nnumber of DNN layers, LR can be adjusted dynamically during DNN training, which provides\nhigh flexibility but also presents challenges in LR tuning. An LR policy (Î·(t)) defines the LR\nvalue for iteration/epoch t. Several existing studies [53, 52, 25] present an in-depth summary of\nrepresentative LR policies and analyze the high impacts of LR tuning on deep learning training\nperformance. Decaying LRs present one category of LR policies defined by a decaying function to\nstart with a high LR value and decrease the LR throughout the training. For example, MultiStepLR\nis specified by a step function with l indicating multiple step lengths and reducing the LR value\nby a factor of Î³ each step [56, 49]. Another category of popular LR policies is cyclic LRs, which\nare characterized by periodically increasing and decreasing the LR values during DNN training,\nrepresented by CLRs [42], SGDR [33], and OneCycleLR [43]. OneCycleLR simply uses one cycle\nfrom a cyclic function, which will first increase the LR value to its upper bound and then decrease the\nLR to help the model converge fast close to the end of training. The cyclic LRs have been leveraged\nin [24] to generate multiple DNN snapshots during one training to develop the snapshot ensemble,\nwhich is different from the proposed LREnsemble to develop deep ensembles from multiple diverse\nLR policies. To the best of our knowledge, this is the first to study how to leverage multiple sub-\noptimal DNNs produced by LR tuning to enhance deep learning predictive performance.\nDeep Ensembles. Ensemble learning is a popular Machine Learning (ML) method to combine\nmultiple diverse ML models to improve the overall predictive performance [13, 7, 3, 5, 4, 14, 51],\nrepresented by bagging [3], boosting [5], and random forests [4]. Deep neural network Ensembles\n(Deep Ensembles) hold the potential to enhance deep learning predictive performance by combining\nmultiple diverse DNNs [30, 19, 51], where the high diversity is the key to the improved performance.\nA few existing studies have been devoted to promoting diversity to produce high-quality deep en-\nsembles such as using different random initialization [30] or using different levels of weight decay\nand label smoothing [49]. To the best of our knowledge, we are the first to examine whether these\nmodels generated during LR tuning can reveal sufficient diversity to facilitate deep ensembles.\nLarge Language Models. Large Language Models (LLMs) represent a type of auto-regressive\ngenerative models trained on massive corpora of texts, which demonstrate remarkable performance\nto adapt to new tasks and perform human-like conversations merely on textual instructions [6, 45,\n35, 46]. Fine-tuning is an efficient and practical method to further enhance a pre-trained LLM\nperformance for a specific learning task [44, 9]. Unlike traditional DNNs, research on LLMs is still\nin the early stages. In this study, we evaluate LREnsemble using LLMs and demonstrate that LLM\nensembles can effectively enhance LLM predictive capabilities.\n3\n3\nProblem Statement\nWe formally describe the research problems in this section. Following [52], the learning rate tuning\nis a subproblem of hyperparameter tuning. Concretely, given the optimizer O, deep neural network\nF, loss function L, training dataset Xtrain, and validation dataset Xvalidation, the goal of LR tuning is\nto identify an optimal LR policy Î· to minimize the loss function LxâXval(x; FOÎ·(Xtrain)), where FOÎ·\nis the trained DNN by using the optimizer O and LR policy Î·. Formally, the LR tuning problem can\nbe defined as an optimization problem in Formula 1:\nËÎ· = argmin\nÎ·âP\nLxâXval(x; FOÎ·(Xtrain))\n(1)\nwhere P represents a set of all possible LR policies. In practice, LR tuning will identify the optimal\nLR policy from a finite set of s candidate LR policies, P = {Î·1(t), Î·2(t), . . . , Î·s(t)}. The LR policy\nÎ·i(t) specifies the LR value for each training iteration/epoch t. For example, Î·i(t) can be a constant\nvalue as Î·i(t) = 0.1 for all training iterations/epochs. We can also use a cyclic function, such as a\ncosine function to define the LR policy, such as Î·i(t) = 0.1 Â· cos\n\u0000t\n200Ï\n\u0001\n, a classic cosine learning\nrate policy with a total of 200 training epochs and 0.1 as the initial LR value. We refer curious\nreaders to [53] for the formal definitions of different LR policies.\nIn order to identify the optimal LR policy, the common practice through trial-and-error is to\nenumerate all candidate LR policies Î·i(t) âP to train the DNN, producing s model parameters,\nwhere each Î¸i = OÎ·i and FÎ¸i is the trained DNN using an LR policy Î·i. The optimal LR policy\nÎ·âcorresponds to the best trained DNN FOÎ·âevaluated on the validation dataset, which can be\neasily identified after LR tuning. In addition, we also obtain multiple sub-optimal DNNs trained\nby other sub-optimal LR policies, i.e., FOÎ·i where Î·i Ì¸= Î·â. For simplicity, in total, we obtain s\nDNNs F1, F2, . . ., Fs from LR tuning including the optimal DNN F â. Often only the optimal LR\npolicy and DNN will be adopted for real-world applications while other sub-optimal DNNs will\nbe discarded, causing a huge waste of computing resources and training time. In this paper, we\nintroduce a new research question: can we effectively utilize these sub-optimal DNNs to enhance\ndeep learning performance?\nOne of the potential solutions is to integrate these diverse models generated during LR tun-\ning, which aligns well with the fundamental principle of deep ensembles [30, 19, 51, 34]. Deep\nensembles typically follow three steps to integrate multiple DNNs to potentially enhance the over-\nall predictive performance: (1) base model collection, which creates a base model pool M to in-\nclude multiple diverse base models through ensemble training or open-source pre-trained models,\n(2) member model selection, which identifies sub-sets of diverse yet complementary base models\nto potentially form high-quality deep ensembles, and (3) ensemble consensus, which integrates the\noutputs of multiple member models to produce the ensemble prediction, such as using soft voting\n(model averaging) or majority voting. Here, the next question is: will these models trained using dif-\nferent LR policies during LR tuning provide sufficient diversity to serve as ensemble base models?\nMoreover, given the high variance in the individual model accuracy and a large number of these\nmodels from LR tuning (see blue and gray bars in Figure 2a), it is also non-trivial to effectively\nidentify high-quality deep ensembles to boost the overall deep learning predictive performance.\n4\nLREnsemble Overview\nWe propose the LREnsemble framework to address these research challenges by leveraging LR\ntuning to boost deep Ensemble performance.\nFigure 3 presents an overview of LREnsemble.\n4\nLREnsemble provides functional components to effectively support both LR tuning and deep ensem-\nbles. We below describe how to leverage LREnsemble to perform LR tuning and create high-quality\ndeep ensembles.\nFigure 3: Overview of LREnsemble architecture\nIn LREnsemble, the goals of learning rate\ntuning include (1) identifying an optimal LR\npolicy to optimize deep learning training and\n(2) generating diverse individual models to sup-\nply the base model pool for building high-\nquality deep ensembles.\nThese two goals\nare complementary: by default, we configure\nLREnsemble with a diverse set of LR policies\nto explore the large search space of possible LR\nvalues, which enables effective identification of\nthe optimal LR policy and meanwhile produces\na set of diverse individual models for creating\ndeep ensembles (see our experimental analysis\nin Section 6). We implement a variety of popu-\nlar LR policies, including decaying LRs, cyclic\nLRs, and composite LRs [47, 26, 52, 53], and\nLR tuning strategies, such as grid search, ran-\ndom search, and Bayesian search. In this study, we primarily leverage the grid search and four rep-\nresentative LR policies: (1) MultiStepLR, a popular LR policy defined by a step function [56, 49],\n(2) WarmupCosineAnnealing (Cosine Annealing LR with Warm-up), a frequently used LR policy,\nespecially for training Transformer-based models [15, 8], (3) OneCycleLR, an LR policy using one\ncycle of a cyclic function [12, 11] and (4) Composite, a hybrid LR policy consisting of multiple\ndifferent cyclic functions to facilitate deep learning training at different phrases [52]. The LR policy\ndatabase in LREnsemble stores the LR tuning results organized by the LR policy, dataset, model\nand learning task, which facilitate the LR tuning to recommend the top N best LR policies.\nAfter LR tuning, multiple diverse individual models will be added to the base model pool, which\nsupplies the base models to build high-quality deep ensembles. Given the base models are generated\nvia LR tuning with varying model accuracy, some of them with low diversity may not complement\neach other well. Moreover, simply combining all available base models to form the entire ensemble\nmay not produce the optimal accuracy (see Table 2 in Section 6). We also provide multiple ensemble\nselection methods in LREnsemble to effectively identify high quality ensembles with high accuracy,\nincluding (1) Brute Force Selection, which enumerates all possible combinations of base models and\nselects the ensemble with the highest accuracy on the validation set, (2) Greedy Selection, which\nconstructs the ensemble incrementally by initially selecting the best single model, and subsequently\nadding models that offer the largest performance improvement [7], (3) Random Selection, which\nbuilds the ensemble by randomly choosing base models, potentially subject to certain constraints,\nsuch as a fixed team size or a minimum accuracy threshold, and (4) Focal Selection, which leverages\nfocal diversity on the validation set to effectively identify high-quality ensembles [54].\nLREnsemble is currently implemented with PyTorch [37]. We adopt a module design for LREns-\nemble, which can be easily extended to support new LR policies, new deep learning frameworks, and\nnew ensemble learning methods. LREnsemble can also be integrated with existing general hyperpa-\nrameter tuning tools, such as Ray Tune [32], to leverage other hyperparameter tuning strategies.\n5\n5\nTheoretical Analysis\nIn this section, we provide a brief theoretical analysis to explain the improved predictive perfor-\nmance by LREnsemble. Given SGD [2] as the optimizer for training DNN F, we have Î¸t+1 =\nÎ¸t âÎ·tâL, where Î¸t and Î·t represent the model parameters and learning rate value respectively at\niteration t and âL denotes the gradients. We then analyze the impact of learning rate variance on\nmodel parameters. Let Âµg and Ï2\ng denote the mean and variance of the gradients âL. Assume the\nlearning rate Î·t has a mean ÂµÎ· and variance Ï2\nÎ·. Formally, we have E[âL] = Âµg, Var(âL) = Ï2\ng,\nE[Î·t] = ÂµÎ·, and Var(Î·t) = Ï2\nÎ·.\nTo obtain the variance of model parameters Var(Î¸t+1), we use the law of total variance in For-\nmula 2.\nVar(Î¸t+1) = E[Var(Î¸t+1 | Î¸t)] + Var(E[Î¸t+1 | Î¸t])\n(2)\nGiven Î¸t+1 = Î¸t âÎ·tâL, we have Formula 3.\nVar(Î¸t+1 | Î¸t) = Var(Î¸t âÎ·tâL | Î¸t) = Var(Î·tâL)\n= E[Î·t]2Var(âL) + E[âL]2Var(Î·t) + Var(Î·t)Var(âL)\n= Âµ2\nÎ·Ï2\ng + Âµ2\ngÏ2\nÎ· + Ï2\nÎ·Ï2\ng\n(3)\nGiven Var(Î·tâL) does not depend on Î¸t, the expectation of the conditional variance is For-\nmula 4.\nE[Var(Î¸t+1 | Î¸t)] = Âµ2\nÎ·Ï2\ng + Âµ2\ngÏ2\nÎ· + Ï2\nÎ·Ï2\ng\n(4)\nNext, we calculate Var(E[Î¸t+1 | Î¸t]). Since Î·t and âL are independent, based on the SGD\nupdate rule, we have Formula 5.\nE[Î¸t+1 | Î¸t] = E[Î¸t âÎ·tâL | Î¸t] = Î¸t âE[Î·tâL]\n= Î¸t âE[Î·t]E[âL]\n= Î¸t âÂµÎ·Âµg\n(5)\nHence, the variance of this conditional expectation is Formula 6.\nVar(E[Î¸t+1 | Î¸t]) = Var(Î¸t âÂµÎ·Âµg) = Var(Î¸t)\n(6)\nCombining Formulas 2, 4, and 6, we have Formula 7.\nVar(Î¸t+1) = Âµ2\nÎ·Ï2\ng + Âµ2\ngÏ2\nÎ· + Ï2\nÎ·Ï2\ng + Var(Î¸t)\n(7)\nFormula 7 shows that the variance of the model parameters Î¸ depends on both the variance\nof the learning rate Ï2\nÎ· and the variance of the gradients Ï2\ng, as well as their means. The term\nÂµ2\ngÏ2\nÎ· +Ï2\nÎ·Ï2\ng highlights that the high variance of the learning rate will contribute to the high variance\nof model parameters, which can be accumulated across training iterations to significantly increase\nthe variance of model parameters and ultimately produce highly diverse DNNs. Moreover, we also\nutilize diversity-based ensemble selection methods in LREnsemble to further identify highly diverse\nand complementary DNNs with improved ensemble accuracy and reduced ensemble execution cost.\n6\n6\nExperimental Analysis\nWe perform comprehensive experiments to evaluate the proposed LREnsemble. The datasets we\nused in this study include CIFAR-10 [28], CIFAR-100 [28], Tiny ImageNet [31], and Stanford-\nalpaca instruction-following data [44]. The DNN models for evaluation include WRN-28-10 [56],\nResNeXt50 [55], Vision Transformer (ViT) [15], and a large language model, LLaMA-7b [45]. We\ntrain the WRN-28-10 model on the CIFAR-10 and CIFAR-100 datasets, and the ResNeXt50 and ViT\nmodels on the Tiny ImageNet dataset. We provide the training and dataset details in Section A and C\nof the appendix. We fine-tune LLaMA-7b on the Stanford-alpaca instruct data for three epochs with\ndifferent learning rate policies, and evaluate the fine-tuned models and their ensembles using the\nmetrics of ARC-Challenge [10], Hellaswag [57], and MMLU [23].\n6.1\nPerformance and Diversity of Base Models\nTable 1: Accuracy comparison of 16 learning rate policies for training on four tasks\nLearning Rate Policy\nAccuracy (%)\nLearning Rate Function\nk0\nkvit\n0\nk1\nkvit\n1\nÎ³\nl\nlcycle\nCIFAR-10\nCIFAR-100\nTiny ImageNet\nTiny ImageNet\nWRN-28-10\nWRN-28-10\nResNeXt50\nViT\nMultiStepLR\n0.2\n0.002\n0.2\n[0.3, 0.6, 0.8]\n96.30\n82.14\n63.19\n90.83\nMultiStepLR\n0.1\n0.001\n0.2\n[0.3, 0.6, 0.8]\n97.12\n82.66\n69.31\n91.16\nMultiStepLR\n0.05\n0.0005\n0.2\n[0.3, 0.6, 0.8]\n97.13\n82.60\n71.15\n91.05\nMultiStepLR\n0.01\n0.0001\n0.2\n[0.3, 0.6, 0.8]\n96.73\n81.04\n70.22\n87.81\nOneCycleLR\n0.4\n0.004\n0\n0\n95.86\n80.82\n63.36\n90.21\nOneCycleLR\n0.2\n0.002\n0\n0\n96.92\n83.20\n68.01\n90.65\nOneCycleLR\n0.1\n0.001\n0\n0\n97.00\n83.18\n71.00\n91.03\nOneCycleLR\n0.05\n0.0005\n0\n0\n96.97\n82.70\n71.90\n91.08\nWarmupCosineAnnealing\n0.2\n0.002\n0\n0\n96.63\n83.01\n66.18\n90.90\nWarmupCosineAnnealing\n0.1\n0.001\n0\n0\n97.04\n83.58\n69.99\n91.01\nWarmupCosineAnnealing\n0.05\n0.005\n0\n0\n97.34\n83.11\n72.41\n91.09\nWarmupCosineAnnealing\n0.01\n0.0001\n0\n0\n96.82\n81.38\n70.49\n88.83\nComposite\n1.0\n0.01\n0.2\n0.002\n0.1\n[0.45, 0.9]\n[3,2,1]\n95.40\n77.85\n51.07\n90.28\nComposite\n0.5\n0.005\n0.1\n0.001\n0.1\n[0.45, 0.9]\n[3,2,1]\n96.31\n81.94\n65.71\n90.75\nComposite\n0.25\n0.0025\n0.05\n0.0005\n0.1\n[0.45, 0.9]\n[3,2,1]\n96.70\n82.51\n67.83\n90.97\nComposite\n0.05\n0.0005\n0.01\n0.0001\n0.1\n[0.45, 0.9]\n[3,2,1]\n97.01\n81.93\n71.37\n90.59\nThe table shows the specific settings we deploy in the training or fine-tuning for each task with different LR policies,\nincluding 4 types of LR functions with different settings. k0 is the initial LR value for MultiStepLR and\nWarmupCosineAnnealing, and is the maximum learning rate for OneCycleLR and Composite LRs. k1 is the minimum LR\nvalue. The LR value of the ViT fine-tuning task is shown as kvit\n0\n(=0.01 Ã k0) and kvit\n1\n(=0.01 Ã k1). The l represents the\ntime/stages when the learning rate multiplies a factor Î³ in terms of the number of iterations/epochs, and lcycle is the cycle\nlength of cyclic LRs for the Composite LRs for the three stages of l. All the learning rate policies are visualized in\nSection B in the appendix.\nThe first set of experiments investigates whether training models with different learning rate\npolicies in LR tuning can produce diverse individual DNN models. Table 1 presents the experimen-\ntal results of training DNNs with different LR policies. We highlight three interesting observations.\nFirst, different learning rate policies can generate different training results for all the tasks. Espe-\ncially, the ResNeXt50 models trained on Tiny ImageNet have up to 21.34% difference in accuracy.\nThe effect of the learning rate policy is non-negligible and brings diversity to the trained DNN mod-\nels, which provides potential opportunities for forming high-quality ensembles. Second, the best\nlearning rate policies on three training tasks (WRN-28-10 and ResNeXt50) are WarmupCosineAn-\nnealing policies, while the MultiStepLR policies achieve second and third places in these training\ntasks and achieve the best in the ViT fine-tuning task. To find the optimal learning rate policy,\nbesides tuning the initial learning rate value, it is worth exploring multiple learning rate policies\nin depth. Third, the MultiStepLR policies with an initial learning rate value of 0.1 have a similar\nperformance to another MultiStepLR policy with an initial LR of 0.05, which is not a small gap in\nthe initial LR values. A total of 6 LR policies can train the models to reach 97% accuracy on the\n7\ntest dataset for CIFAR-10, which shows that different LR policies can train DNNs with similar high\naccuracy. Hence, LR tuning can potentially produce multiple good LR policies.\n(a) CIFAR-10 & WRN-28-10\n(b) CIFAR-100 & WRN-28-10\n(c) Tiny ImageNet & ResNeXt50\n(d) Tiny ImageNet & ViT\nFigure 4: Performance distribution across all ensemble team sizes in different tasks.\nFrom previous experiments, we learned that models trained with different LR policies can ex-\nhibit varying predictive performance. However, these models produced by LR tuning also have high\nvariance in their accuracy and may not complement each other to boost ensemble performance. We\nthen study the potential of leveraging these different models produced by different LR policies to\nform high-quality ensembles. Figure 4 visualizes the accuracy distributions of every possible en-\nsemble combination for different tasks, where each box presents a performance distribution of a\nspecific team size. A team size of one indicates a single model, while a team size of 16 repre-\nsents the entire ensemble of all 16 models. We find three interesting observations. First, it shows\nthat many ensembles can outperform the best single model for each task, demonstrating the ef-\nfectiveness of our LREnsemble in creating high-quality ensembles. For training WRN-28-10 on\nCIFAR-100, the ensembles can consistently outperform the best single model when the team size\nis larger than 3, a similar case also happens for training ResNeXt50 on Tiny ImageNet when the\nsize is larger than 11. Second, among these ensembles, the maximum accuracy shows an increase of\n3.4% in training ResNeXt50 on the Tiny ImageNet dataset and that is 2.33% in training WRN-28-10\non the CIFAR-100 dataset, which demonstrates the high potential of leveraging LR tuning to effec-\ntively boost ensemble performance. Third, for all four tasks, we can identify at least one ensemble\nthat consistently outperforms the best single model, which shows that it is beneficial to leverage even\nsub-optimal models produced in LR tuning to further enhance deep learning performance. However,\nthe wide distribution of deep ensemble accuracy also presents a critical challenge: many ensembles\nmay not outperform the best single model. Moreover, simply choosing the entire ensemble may not\noutperform the best single model as well, such as the entire ensemble of 16 WRN-28-10 models on\nCIFAR-10. Therefore, to fully harness the potential of deep ensembles, we need to identify an ef-\nfective ensemble selection method, which can efficiently identify complementary ensemble member\nmodels to form high-quality ensembles and deliver robust and accurate predictive performance.\n6.2\nPerformance of Ensemble Selection\nGiven the large pool of base models obtained from LR tuning, random selection or using all models\nwithout selection may not produce a good ensemble predictor, since many ensemble teams underper-\nform the best single model as shown in Figure 4. Therefore, we provide multiple ensemble selection\nmethods in our LREnsemble framework. Table 2 presents experimental results by using four dif-\nferent ensemble selection methods to select high-quality ensembles using the validation dataset: (1)\nRandom Selection, (2) Brute Force Selection, (3) Greedy Selection [7], and (4) Focal Selection [54].\n8\nTable 2: Accuracy comparison of 3 selection methods performance with different team sizes\nTeam Size\nAccuracy (%) of CIFAR-100 & WRN-28-10\nAccuracy (%) of Tiny ImageNet & ResNeXt50\nRandom\nBrute\nGreedy\nFocal\nRandom\nBrute\nGreedy\nFocal\nSelection\nForce\nSelection\nSelection\nSelection\nForce\nSelection\nSelection\nBest Single\n83.58\n72.41\n2\n84.12Â± 0.32\n84.52\n84.52\n84.52\n71.24Â± 2.08\n73.48\n73.48\n72.88\n3\n84.07Â± 0.50\n84.56\n84.91\n84.48\n70.75Â± 1.65\n74.05\n74.28\n73.66\n4\n84.76Â± 0.19\n84.69\n84.96\n84.96\n72.22Â± 1.41\n74.06\n74.22\n74.27\n5\n84.69Â± 0.34\n85.02\n85.12\n84.91\n72.76Â± 0.89\n74.55\n74.55\n73.90\n6\n84.77Â± 0.29\n85.19\n85.01\n85.06\n73.07Â± 0.48\n74.75\n74.75\n73.93\n7\n84.86Â± 0.11\n85.13\n84.99\n84.92\n72.93Â± 0.56\n74.45\n74.45\n74.32\n8\n84.95Â± 0.12\n85.53\n85.17\n84.93\n73.13Â± 0.47\n74.58\n74.58\n73.50\n9\n85.04Â± 0.18\n85.19\n85.08\n84.82\n73.40Â± 0.39\n74.64\n74.28\n74.13\n10\n85.13Â± 0.12\n85.11\n84.99\n85.19\n73.50Â± 0.39\n74.33\n74.33\n74.14\n11\n85.12Â± 0.15\n85.17\n84.95\n85.32\n73.27Â± 0.35\n74.30\n74.30\n73.92\n12\n85.11Â± 0.08\n85.18\n85.07\n85.26\n73.52Â± 0.29\n74.05\n74.05\n73.76\n13\n85.15Â± 0.11\n85.16\n85.05\n85.18\n73.60Â± 0.22\n74.16\n74.16\n73.43\n14\n85.13Â± 0.06\n85.12\n84.99\n85.22\n73.57Â± 0.14\n73.95\n73.95\n73.89\n15\n85.20Â± 0.03\n85.05\n85.05\n85.27\n73.63Â± 0.11\n73.84\n73.84\n73.91\nEntire Ensemble\n85.23\n73.61\nWe highlight four interesting observations. First, as depicted in the subplot for training ResNeXt50\non Tiny ImageNet task in Figure 4, when the team size ranges from 2 to 4, around half of the en-\nsembles exhibit lower accuracy than the best single model, with this proportion nearly holding when\nthe team size is 5. Despite the wide range of accuracy distribution and many ensembles under-\nperforming the best single model, all methods except random selection can still select high-quality\nensembles. In contrast, random selection fails to achieve similar results under these conditions. Its\naverage performance is lower than the performance of the best single model when the team size is\nbetween 2 and 4. Moreover, random selection exhibits a lower average performance along with a\nnon-negligible variance compared to the other three selection methods. Second, many ensembles\ncomposed of fewer models outperform the entire ensemble of all base models in both tasks, indicat-\ning that the best ensemble is often a subset of the entire ensemble, and thus, the ensemble selection\nor pruning methods would be an important factor for improving the ensemble efficiency and perfor-\nmance. Third, as shown in Table 2, three selection methods, brute force, greedy selection, and focal\nselection, have good performance, with no single method proving to be dominant. Considering the\nfocal selection method based on the diversity score, a more explainable and generalization method\nthan the other two methods, we choose this method in the following experiments, and we provide a\ndetailed explanation in the ablation study in Section F in the appendix. Fourth, excluding random\nselection method, the worst case has a 0.9% and 0.47% increase compared to the best single model,\nunderlined in Table 2. All the ensembles, regardless of the team size, achieve higher accuracy than\nthe best single model. This demonstrates that the models trained by different LR policies with proper\nensemble selection can boost overall performance.\n6.3\nLREnsemble Performance Comparison to Other Methods\nOur method achieves good performance by leveraging the model diversity produced by learning\nrate tuning and effective ensemble selections. There are other ensemble learning methods using\ndifferent diversifying methods or selection methods. Here, we compare our LREnsemble with other\nstate-of-the-art ensemble learning methods for training WRN-28-10 on the CIFAR-10 and CIFAR-\n100 datasets with 200 epochs. Table 3 presents the performance comparison of the ensemble of size\n4 by our LREnsemble and the best ensemble by using other deep ensemble methods, our LREnsem-\n9\nble can consistently outperform other ensemble methods with 0.8% accuracy improvement over\nTable 3: Accuracy comparison of LREnsemble and other state-of-\nthe-art methods for training WRN-28-10\nMethod\nCIFAR-10\nCIFAR-100\nAccuracy\nTeam size\nAccuracy\nTeam size\nFast Geometric Ensemble [22]\n96.64\n12\n83.12\n12\nSnapshot Ensemble [22, 24]\n96.69\n12\n83.03\n12\nBatchEnsemble [48]\n95.94\n4\n80.32\n4\nRank-1 BNN [17]\n96.5\n4\n82.4\n4\nDeepEns [30, 49]\n96.2\n4\n82.6\n4\nHyper-Deep Ens [49]\n96.5\n4\n82.8\n4\nLREnsemble - Focal (4)\n97.46\n4\n84.96\n4\nLREnsemble - Focal (best)\n97.49\n2\n85.32\n11\nthe best method on CIFAR-\n10 and 2.2% accuracy en-\nhancement on CIFAR-100.\nThe promising results indi-\ncate that LR tuning can pro-\nduce a pool of diverse and\naccurate base models, which\ncan be effectively leveraged\nto build high quality ensem-\nbles through ensemble selec-\ntion. Moreover, this set of ex-\nperiments also demonstrates\nthat our LREnsemble can effectively leverage LR tuning to boost deep ensemble performance to\nfurther enhance deep learning prediction accuracy.\n6.4\nPerformance of LREnsemble on LLM Fine-tuning\nWith the rapid rise in popularity and influence of Large Language Models, as well as motivated by\nthe encouraging results of fine-tuning ViT in Table 1 and Figure 4, which is also a Transformer-based\nmodel, we study the performance impacts of LREnsemble on fine-tuning Large Language Models.\nDifferent from the image classification, we leverage another voting method to combine multiple\nLLMs, which is provided in Section E in the appendix. Table 4 shows the performance comparison\nof the LLM ensemble and single LLM on ARC-Challenge [10], Hellaswag [57], and MMLU [23]\nbenchmarks. Fine-tuning can significantly increase the pre-trained LLaMA performance by 4.12%\non ARC-Challenge, 1.43% on Hellaswag, and 7.28% on MMLU. The LR tuning by LREnsemble\nalso outperforms the default learning rate setting in Stanford Alpaca [44] in ARC-Challenge and\nTable 4: Evaluation results of LLaMA fine-tuning task\nModels\nLLM Evaluation (%)\nARC(25)\nHellaSwag(10)\nMMLU(2)\nOrigial LLaMA 1 [45]\n50.39\n77.81\n34.08\nFine-tuned LLaMA by Stanford Alpaca [44]\n52.02\n78.03\n41.36\nBest single fine-tuned by LREnsemble\n54.51\n79.24\n41.36\nBest ensemble fine-tuned by LREnsemble\n54.59\n79.28\n41.80\nHellaswag benchmarks, also\nindicating the high impact of\nadjusting the learning rates.\nFurthermore, the results in-\ndicate that the best ensem-\nble team of fine-tuned LLMs\nidentified by LREnsemble\ncan achieve additional per-\nformance gains, with an in-\ncrease of 0.08% on ARC-Challenge, 0.04% on Hellaswag, and 0.44% on MMLU in addition to\nthe improvements obtained from fine-tuning. Considering the few iterations in fine-tuning, this\nimprovement shows the potential of our LREnsemble to further boost ensemble performance in\ntraining/fine-tuning Large Language Models.\n6.5\nVisualization of LREnsemble Performance\nFigure 5 visualizes the default voting mechanism in LREnsemble. Here, we average the prediction\nprobability vectors from all the member models to compute the final ensemble prediction. Three\ninteresting observations should be highlighted. First, the models trained by using different learning\nrate policies have significantly different prediction distributions, showing that the different LR poli-\ncies can introduce diversity to DNNs. Second, this voting mechanism by averaging member model\n10\nFigure 5: Two image examples to illustrate the voting processes in LREnsemble for training WRN-\n28-10 on CIFAR-10. The ensemble used here is from Table 3 with the team size of 4. All four\nmember models are listed in Table 1, which are ordered from model 1 to model 4: MultiStepLR\nwith k0 = 0.1, OneCycleLR with k0 = 0.2, WarmupCosineAnnealing with k0 = 0.1, and Warmup-\nCosineAnnealing with k0 = 0.05.\npredictions can effectively mitigate biases by individual ensemble members and produce a smoother\noutcome, which can effectively correct member model mistakes. Third, although two models in\nthe ensemble make incorrect predictions in each case and no single model is correct for both cases,\nthe final ensemble prediction is still correct. This demonstrates that the models within a carefully\nselected ensemble by LREnsemble can complement each other effectively.\n7\nConclusion\nThis paper makes three novel contributions. First, we show that different LR policies can be lever-\naged to produce highly diverse DNNs to boost deep ensemble performance, which significantly en-\nhances the resource utilization during LR tuning. Second, we propose a novel framework, LREnsem-\nble, to exploit the complementary capability of LR tuning and deep ensembles to improve overall\npredictive performance. Third, we conduct systematic experiments on multiple benchmark datasets\nand DNNs, including ViT and LLM models, to demonstrate the effectiveness of LREnsemble. To\nthe best of our knowledge, we are the first to leverage sub-optimal DNNs from LR tuning to improve\ndeep learning accuracy.\nReferences\n[1] Yoshua Bengio. 2012. Practical Recommendations for Gradient-Based Training of Deep Ar-\nchitectures. Springer Berlin Heidelberg, Berlin, Heidelberg, 437â478.\n[2] LÂ´eon Bottou. 2012. Stochastic Gradient Descent Tricks. Springer Berlin Heidelberg, Berlin,\nHeidelberg, 421â436. https://doi.org/10.1007/978-3-642-35289-8_25\n[3] Leo Breiman. 1996. Bagging predictors. Machine Learning 24, 2 (Aug 1996), 123â140.\nhttps://doi.org/10.1007/BF00058655\n[4] Leo Breiman. 2001. Random Forests. Machine Learning 45, 1 (2001), 5â32.\nhttps:\n//doi.org/10.1023/A:1010933404324\n[5] Leo Breiman et al. 1998. Arcing classifier (with discussion and a rejoinder by the author). The\nannals of statistics 26, 3 (1998), 801â849.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, and et al. 2020.\nLanguage Models are Few-\nShot Learners. In Advances in Neural Information Processing Systems, H. Larochelle,\n11\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc.,\n1877â1901. https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[7] Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. 2004. Ensemble se-\nlection from libraries of models. In Proceedings of the Twenty-First International Conference\non Machine Learning (Banff, Alberta, Canada) (ICML â04). Association for Computing Ma-\nchinery, New York, NY, USA, 18. https://doi.org/10.1145/1015330.1015432\n[8] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. 2022. When Vision Transformers Outper-\nform ResNets without Pre-training or Strong Data Augmentations. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net. https://openreview.net/forum?id=LtKcMgGOeLt\n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.\nVicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https:\n//lmsys.org/blog/2023-03-30-vicuna/\n[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2\nReasoning Challenge. arXiv:1803.05457 [cs.AI]\n[11] DeepSpeed Contributors. 2024. â1-Cycle Scheduleâ. https://www.deepspeed.ai/\ntutorials/one-cycle/. [Online; accessed 02-Feb-2024].\n[12] PyTorch Contributors. 2023.\nâONECYCLELRâ.\nhttps://pytorch.org/docs/\nstable/generated/torch.optim.lr_scheduler.OneCycleLR.html.\n[On-\nline; accessed 02-Feb-2024].\n[13] Thomas G. Dietterich. 2000. Ensemble Methods in Machine Learning. In Multiple Classifier\nSystems. Springer Berlin Heidelberg, Berlin, Heidelberg, 1â15.\n[14] Xibin Dong, Zhiwen Yu, Wenming Cao, Yifan Shi, and Qianli Ma. 2020. A survey on ensemble\nlearning. Frontiers of Computer Science 14, 2 (2020), 241â258.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. 2021.\nAn Image is Worth 16x16 Words: Transform-\ners for Image Recognition at Scale. In 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\nhttps:\n//openreview.net/forum?id=YicbFdNTTy\n[16] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for On-\nline Learning and Stochastic Optimization. J. Mach. Learn. Res. 12 (July 2011), 2121â2159.\nhttp://dl.acm.org/citation.cfm?id=1953048.2021068\n[17] Michael W. Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-An Ma, Jasper Snoek, Katherine\nHeller, Balaji Lakshminarayanan, and Dustin Tran. 2020. Efficient and scalable Bayesian neu-\nral nets with rank-1 factors. In Proceedings of the 37th International Conference on Machine\nLearning (ICMLâ20). JMLR.org, Article 261, 11 pages.\n12\n[18] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2021.\nSharpness-\naware Minimization for Efficiently Improving Generalization. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-\nview.net. https://openreview.net/forum?id=6Tm1mposlrM\n[19] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. 2020.\nDeep Ensembles: A Loss\nLandscape Perspective. arXiv:1912.02757 [stat.ML] https://arxiv.org/abs/1912.\n02757\n[20] Leo Gao. 2021.\nMultiple Choice Normalization in LM Evaluation.\nhttps://blog.\neleuther.ai/multiple-choice-normalization/\n[21] Leo Gao, Jonathan Tow, Baber Abbasi, and et al. 2024. A framework for few-shot language\nmodel evaluation. https://doi.org/10.5281/zenodo.12608602\n[22] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wil-\nson. 2018.\nLoss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. In Ad-\nvances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates,\nInc. https://proceedings.neurips.cc/paper_files/paper/2018/file/\nbe3087e74e9100d4bc4c6268cdbe8456-Paper.pdf\n[23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. 2021.\nMeasuring Massive Multitask Language Understanding.\narXiv:2009.03300 [cs.CY] https://arxiv.org/abs/2009.03300\n[24] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger.\n2017.\nSnapshot Ensembles: Train 1, Get M for Free. In 5th International Conference on\nLearning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track\nProceedings. OpenReview.net. https://openreview.net/forum?id=BJYwwY9ll\n[25] H. Jin, W. Wei, X. Wang, W. Zhang, and Y. Wu. 2023. Rethinking Learning Rate Tuning in\nthe Era of Large Language Models. In 2023 IEEE 5th International Conference on Cognitive\nMachine Intelligence (CogMI). IEEE Computer Society, Los Alamitos, CA, USA, 112â121.\nhttps://doi.org/10.1109/CogMI58952.2023.00025\n[26] Yuchen Jin, Tianyi Zhou, Liangyu Zhao, Yibo Zhu, Chuanxiong Guo, Marco Canini, and\nArvind Krishnamurthy. 2021. AutoLRS: Automatic Learning-Rate Schedule by Bayesian Op-\ntimization on the Fly. In 9th International Conference on Learning Representations, ICLR\n2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\nhttps://openreview.\nnet/forum?id=SlrqM9_lyju\n[27] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large\nScale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings\nof the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Linguistics, Vancouver, Canada, 1601â1611.\nhttps://doi.org/10.18653/v1/P17-1147\n[28] Alex Krizhevsky and Geoffrey Hinton. 2009.\nLearning multiple layers of fea-\ntures from tiny images.\n(2009).\nhttps://www.cs.toronto.edu/Ëkriz/\nlearning-features-2009-TR.pdf\n13\n[29] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. 2021. ASAM: Adap-\ntive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks. In\nProceedings of the 38th International Conference on Machine Learning (Proceedings of Ma-\nchine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 5905â5914.\nhttps://proceedings.mlr.press/v139/kwon21b.html\n[30] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017.\nSimple and\nScalable Predictive Uncertainty Estimation using Deep Ensembles. In Advances in Neu-\nral Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wal-\nlach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates,\nInc. https://proceedings.neurips.cc/paper_files/paper/2017/file/\n9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf\n[31] Ya Le and Xuan S. Yang. 2015. Tiny ImageNet Visual Recognition Challenge. Technical\nReport. Stanford University. http://vision.stanford.edu/teaching/cs231n/\nreports/2015/pdfs/yle_project.pdf\n[32] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E. Gonzalez, and Ion\nStoica. 2018.\nTune: A Research Platform for Distributed Model Selection and Training.\narXiv:1807.05118 [cs.LG] https://arxiv.org/abs/1807.05118\n[33] Ilya Loshchilov and Frank Hutter. 2017.\nSGDR: Stochastic Gradient Descent with Warm\nRestarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.\nhttps://\nopenreview.net/forum?id=Skq89Scxx\n[34] Luis A. Ortega, Rafael CabaËnas, and Andres Masegosa. 2022.\nDiversity and Generaliza-\ntion in Neural Network Ensembles. In Proceedings of The 25th International Conference on\nArtificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 151),\nGustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera (Eds.). PMLR, 11720â11743.\nhttps://proceedings.mlr.press/v151/ortega22a.html\n[35] Long\nOuyang,\nJeffrey\nWu,\nXu\nJiang,\nand\net\nal.\n2022.\nTraining\nlanguage\nmodels\nto\nfollow\ninstructions\nwith\nhuman\nfeedback.\nIn\nAdvances\nin\nNeural\nIn-\nformation\nProcessing\nSystems,\nS.\nKoyejo,\nS.\nMohamed,\nA.\nAgarwal,\nD.\nBel-\ngrave,\nK.\nCho,\nand\nA.\nOh\n(Eds.),\nVol.\n35.\nCurran\nAssociates,\nInc.,\n27730â\n27744.\nhttps://proceedings.neurips.cc/paper_files/paper/2022/\nfile/b1efde53be364a73914f58805a001731-Paper-Conference.pdf\n[36] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for\nAutomatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting\nof the Association for Computational Linguistics. Association for Computational Linguistics,\nPhiladelphia, Pennsylvania, USA, 311â318. https://doi.org/10.3115/1073083.\n1073135\n[37] PyTorch Developers. 2017. Tensors and Dynamic neural networks in Python with strong GPU\nacceleration. https://pytorch.org/. [Online; accessed 04-Dec-2017].\n[38] PyTorch Developers. 2017. VIT B 16.\n\"https://pytorch.org/vision/main/\nmodels/generated/torchvision.models.vit_b_16.html\"\n14\n[39] Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQA: A Conversational Ques-\ntion Answering Challenge. Transactions of the Association for Computational Linguistics 7\n(2019), 249â266. https://doi.org/10.1162/tacl_a_00266\n[40] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. 2020.\nSuperGlue: Learning Feature Matching With Graph Neural Networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n[41] Tom Schaul, Sixin Zhang, and Yann LeCun. 2013. No more pesky learning rates. In Pro-\nceedings of the 30th International Conference on Machine Learning (Proceedings of Machine\nLearning Research, Vol. 28), Sanjoy Dasgupta and David McAllester (Eds.). PMLR, Atlanta,\nGeorgia, USA, 343â351.\nhttps://proceedings.mlr.press/v28/schaul13.\nhtml\n[42] Leslie N. Smith. 2017. Cyclical Learning Rates for Training Neural Networks. In 2017 IEEE\nWinter Conference on Applications of Computer Vision (WACV). 464â472. https://doi.\norg/10.1109/WACV.2017.58\n[43] Leslie N. Smith and Nicholay Topin. 2018. Super-Convergence: Very Fast Training of Neural\nNetworks Using Large Learning Rates. arXiv:1708.07120 [cs.LG] https://arxiv.org/\nabs/1708.07120\n[44] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following\nLLaMA model. https://github.com/tatsu-lab/stanford_alpaca.\n[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\nothÂ´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Ef-\nficient Foundation Language Models. arXiv:2302.13971 [cs.CL] https://arxiv.org/\nabs/2302.13971\n[46] Hugo Touvron, Louis Martin, Kevin Stone, and et al. 2023. Llama 2: Open Foundation and\nFine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] https://arxiv.org/abs/2307.\n09288\n[47] Jia Wei, Xingjun Zhang, Zhimin Zhuo, Zeyu Ji, Zheng Wei, Jingbo Li, and Qianyang Li.\n2023. Leader population learning rate schedule. Information Sciences 623 (2023), 455â468.\nhttps://doi.org/10.1016/j.ins.2022.12.039\n[48] Yeming Wen, Dustin Tran, and Jimmy Ba. 2020. BatchEnsemble: an Alternative Approach\nto Efficient Ensemble and Lifelong Learning. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\nhttps://openreview.net/forum?id=Sklf1yrYDr\n[49] Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. 2020. Hyperparameter\nensembles for robustness and uncertainty quantification. Advances in Neural Information Pro-\ncessing Systems 33 (2020), 6514â6527.\n[50] Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, and Ling Liu. 2024. Hierarchical Pruning of Deep\nEnsembles with Focal Diversity. ACM Trans. Intell. Syst. Technol. 15, 1, Article 15 (Jan. 2024),\n24 pages. https://doi.org/10.1145/3633286\n15\n[51] Yanzhao Wu and Ling Liu. 2021.\nBoosting deep ensemble performance with hierarchical\npruning. In 2021 IEEE International Conference on Data Mining (ICDM). IEEE, 1433â1438.\n[52] Yanzhao Wu and Ling Liu. 2023. Selecting and Composing Learning Rate Policies for Deep\nNeural Networks. ACM Trans. Intell. Syst. Technol. 14, 2, Article 22 (feb 2023), 25 pages.\nhttps://doi.org/10.1145/3570508\n[53] Y. Wu, L. Liu, J. Bae, K. Chow, A. Iyengar, C. Pu, W. Wei, L. Yu, and Q. Zhang. 2019.\nDemystifying Learning Rate Policies for High Accuracy Training of Deep Neural Networks.\nIn 2019 IEEE International Conference on Big Data (Big Data). IEEE Computer Society,\nLos Alamitos, CA, USA, 1971â1980. https://doi.org/10.1109/BigData47090.\n2019.9006104\n[54] Yanzhao Wu, Ling Liu, Zhongwei Xie, Ka-Ho Chow, and Wenqi Wei. 2021. Boosting En-\nsemble Accuracy by Revisiting Ensemble Diversity Metrics. In 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR). 16464â16472. https://doi.org/\n10.1109/CVPR46437.2021.01620\n[55] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. 2017. Aggregated\nResidual Transformations for Deep Neural Networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR).\n[56] Sergey\nZagoruyko\nand\nNikos\nKomodakis.\n2017.\nWide\nResidual\nNetworks.\narXiv:1605.07146 [cs.CV] https://arxiv.org/abs/1605.07146\n[57] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag:\nCan a Machine Really Finish Your Sentence?. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics.\n[58] Wentao Zhang, Jiawei Jiang, Yingxia Shao, and Bin Cui. 2020. Efficient Diversity-Driven En-\nsemble for Deep Neural Networks. In 2020 IEEE 36th International Conference on Data En-\ngineering (ICDE). 73â84. https://doi.org/10.1109/ICDE48307.2020.00014\n16\nA\nSource Codes and Data Setting Details\nAll the training datasets are split into two components: (1) 90% for training and (2) 10% for val-\nidation. The data processing contains Random Crop, Random Horizontal Flip, Cutout, and Label\nSmoothing for all the datasets. In addition, we resize the picture size from 64*64 to 224*224 and\nadd level 2 Random Augment for the Tiny ImageNet dataset.\nB\nVisualization of LR Policies\n(a) MultiStepLR\n(b) MultiStepLR (ViT)\n(c) OneCycleLR\n(d) OneCycleLR (ViT)\n(e) WarmupCosineAnnealing\n(f)\nWarmupCosineAnnealing\n(ViT)\n(g) Composite\n(h) Composite (ViT)\nFigure 6: Visualization of LR policies presented in Table 1.\nC\nTraining Details\nMost tasks are trained from scratch with the batch size of 64 and 200 total epochs, except ViT. The\nViT model is ViT-B/16 pre-trained on ImageNet-1k [38], and we fine-tune it on the Tiny ImageNet\nfor 100 epochs with the 256 batch size. All the tasks use ASAM [18, 29] as the optimizer, setting\nlabel smoothing as 0.1, we set all the related seeds as 0 to control the randomness.\nD\nComputational Cost Details\nWe present a summary of the total computational costs for four training tasks in Table 5. The experi-\nments are conducted on a server equipped with an Intel(R) Xeon(R) Gold 6258R CPU, 1.5TiB RAM,\nand an NVIDIA A100 GPU. For the CIFAR-10 and CIFAR-100 datasets, the training/validation/test-\n17\nTable 5: Cost comparison for LREnsemble\nTask\nInferencing (sec / million / %)\nTraining\nSelection\nBest Single\nLREnsemble\nEntire Ensemble\n(min)\n(sec)\nTime\n#Params\nACC\nTime\n#Params\nACC\nTime\n#Params\nACC\nCIFAR-10 & WRN-28-10\n5279.04\n2211.86\n2.26\n46\n97.34\n4.30\n184\n97.49\n34.28\n736\n97.25\nCIFAR-100 & WRN-28-10\n5543.52\n2965.35\n2.21\n46\n83.58\n23.62\n506\n85.32\n34.19\n736\n85.23\nTiny ImageNet & ResNeXt50\n17886.40\n9115.52\n5.28\n25\n72.41\n29.55\n175\n74.32\n66.48\n400\n73.61\nTiny ImageNet & ViT\n34863.52\n5299.86\n21.87\n86\n91.16\n65.43\n258\n91.37\n345.36\n1376\n91.23\ning sets contain 45,000/5,000/10,000 images respectively. For the Tiny ImageNet dataset, the train-\ning/validation/testing sets consist of 90,000/10,000/10,000 images respectively.\nLREnsemble involves selecting models from the base model pool formed during LR tuning.\nConsequently, the training time cost reflects the cost of training a single model with LR tuning.\nAdditional costs arise from the model selection and inference phases, as shown in Table 5. These\nadditional costs are considerably minor compared to the training expenses. Moreover, these high-\nquality ensembles identified by LREnsemble significantly enhance the predictive performance of\nsingle models and also substantially reduce the ensemble execution cost by the entire ensemble.\nE\nEnsemble Voting of LLMs\nWe follow three steps to combine the results from different LLMs fine-tuned by different learning\nrate policies: First, we get the detailed log file of each model after the evaluation by the Language\nModel Evaluation Harness [21]. We only retain the questions from the three evaluations that are\nsingle-choice questions with four options. Second, we get the negative likelihood of each question\nand each model from the logs, and then transform the results of each model in the log using byte-\nlength normalization [20]. Third, considering the evaluations as four-class classification problems,\nwe use ensemble voting to evaluate the ensemble performance.\nE.1\nPerformance of LREnsemble on ViT Fine-tuning\nDifferent from training from scratch, fine-tuning tasks usually only have a small dataset, a few\niterations/epochs for updating the model parameter, and smaller learning steps [15, 8]. In our ex-\nTable 6: Performance on ViT fine-tuning task\nTeam Size\nAccuracy (%)\nRandom\nBrute\nGreedy\nFocal\nSelection\nForce\nSelection\nSelection\n2\n91.00Â± 0.24\n91.25\n91.25\n91.25\n3\n91.04Â± 0.17\n91.10\n91.14\n91.37\n4\n91.14Â± 0.10\n91.29\n91.27\n91.36\n5\n91.15Â± 0.12\n91.17\n91.27\n91.33\n6\n91.25Â± 0.09\n91.25\n91.24\n91.33\n7\n91.22Â± 0.09\n91.26\n91.26\n91.23\n8\n91.24Â± 0.11\n91.25\n91.22\n91.31\n9\n91.21Â± 0.07\n91.36\n91.26\n91.31\n10\n91.22Â± 0.08\n91.25\n91.30\n91.23\n11\n91.25Â± 0.06\n91.31\n91.26\n91.21\n12\n91.23Â± 0.08\n91.27\n91.27\n91.16\n13\n91.23Â± 0.05\n91.29\n91.29\n91.26\n14\n91.25Â± 0.04\n91.34\n91.28\n91.29\n15\n91.23Â± 0.03\n91.27\n91.20\n91.17\nEntire Ensemble\n91.23\nperiments, the ViT fine-tuning only\ntakes\nhalf\nthe\ntraining\ntime\nof\nResNeXt50 on Tiny ImageNet with\nlearning rates that are two orders of\nmagnitude smaller. However, in Ta-\nble 1 and Figure 4, the experimental\nresults of the ViT fine-tuning present\nsimilar patterns to the results of the\nResNeXt50 training task. First, Ta-\nble 6 shows the ViT ensembles identi-\nfied by LREnsemble can outperform\nthe best single ViT. This observation\nshows that our LREnsemble is effec-\ntive in fine-tuning tasks. Second, be-\nyond the distinctions in model struc-\nture and training settings, the perfor-\n18\nmance of the ViT fine-tuning is significantly higher than that of the ResNeXt50 training task, but the\neffectiveness of LREnsemble is reflected in these different situations, demonstrating the generality\nof this method.\nF\nAblation Study\nWe conduct two ablation studies to better understand the function of learning rate tuning in diversi-\nfying models and the role of focal ensemble selection.\nWe calculate the pairwise cosine similarity of the model parameters, except the output layer, for\neach WRN-28-10 model trained on CIFAR-10 and CIFAR-100, comparing every possible pair of\nmodels across all epochs, and summarize the aggregated results shown in Figure 7a. The solid lines,\naccompanied by confidence boundaries, illustrate the trend of cosine similarity scores for model\nparameters throughout the training period. The red line represents the average value of similarity\nacross all pairs of models trained on the CIFAR-10 dataset, the blue line corresponds to models\ntrained on CIFAR-100, and the orange line depicts the similarity between model pairs with one\nmodel trained on CIFAR-10 and the other on CIFAR-100. The dashed line in green represents the\nmost similar case at the end of the training, and the purple represents the least similar case. Both\ndashed lines emerged coincidentally from an inner comparison of CIFAR-10.\nWe find three interesting observations from Figure 7a. First, after 25 epochs, the average differ-\nence between models with different LR policies is larger than the different model parameter initials\nin both red and blue lines. Both red and blue lines end at the numbers close to the orange line,\nwhich is the pairwise cosine similarity between different initialization and different training tasks. It\ndemonstrates that the cumulative effect of differences in learning rate policies across each iteration\ncan significantly magnify the divergence in model parameters. Second, The purple dashed repre-\nsents the least similar case, which has a 9.16% similarity score at the end of the training. It is from\ntwo WRN-28-10 models trained on CIFRA-10, but the difference exceeds the average difference be-\ntween WRN-28-10 trained on different datasets, showing the range of model parameter difference\nbrought from learning rate policies can be very large. Third, the most similar cases shown by the\ngreen dashed line, from two models trained on CIFAR-10 dataset, only have a 49.21% similarity\nscore, which shows that the different LR policies will significantly and robustly make the mod-\nels different and diverse. Those above observations, derived from a different analytical perspective\ncompared to the final prediction comparison in Table 1, strongly suggest that the learning rate tuning\nleads to high diversity in the trained models, underlining the potential for performance enhancement\nthrough the ensemble of these models.\nWe discuss in the experiment part that our LREnsemble method can create a diverse and accurate\nindividual model pool for building a strong ensemble team, which outperforms the best single model\nin each task and the current methods proposed in other papers in WRN-28-10 model trained on the\nCIFAR-10 and CIFAR-100 datasets. By default, we choose the Focal selection in this paper to find\nthe ensemble team for the final evaluation and comparison, even though other selection methods are\nalso applicable based on the result in Table 2.\nBoth Greedy Selection and Brute Force Selection have generalization problems due to their over-\nreliance on the validation set [7], so they might have higher performance on the validation dataset but\nlack consistent stability and generalizability. However, instead of using the validation set to calculate\noverall accuracy as the selection metric, the focal selection measures the complementarity among the\nensemble members in an ensemble team according to their negative sample independence, in which\nthe FQ-GD score is one of the metrics evaluating the complementarity. Figure 7b and Figure 7c\npresents a scatter plot that visualizes the relationship between the FQ-GD score and the accuracy\n19\n(a) Model parameter comparison between\ntrained WRN-28-10s\n(b) CIFAR-10 & WRN-28-10\n(c) Tiny ImageNet & ResNeXt50\nFigure 7: Figure (a) shows the cosine difference between the parameters of each pair of WRN-28-10\nmodels trained on CIFAR-10 or CIFAR-100. Figure (b) and Figure (c) show the relationship between\nFQ-GD score and accuracy (%) of the WRN-28-10 Training on CIFAR-10 and the ResNeXt50\nTraining on Tiny ImageNet.\nfor two models: the WRN-28-10 model trained on the CIFAR-10 dataset and the ResNeXt50 model\ntrained on the Tiny ImageNet dataset. There is a negative correlation across all the team sizes\nbetween the FQ-GD score and the test accuracy in the graphs in both tasks, showing the good\ngeneralization of this method. Based on the plot displayed, we can select the ensemble team with a\nlower FQ-GD score in a given ensemble team size as our output ensemble.\n20\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-10",
  "updated": "2024-10-10"
}