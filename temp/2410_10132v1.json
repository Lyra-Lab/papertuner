{
  "id": "http://arxiv.org/abs/2410.10132v1",
  "title": "Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning",
  "authors": [
    "Hung Le",
    "Kien Do",
    "Dung Nguyen",
    "Sunil Gupta",
    "Svetha Venkatesh"
  ],
  "abstract": "Effective decision-making in partially observable environments demands robust\nmemory management. Despite their success in supervised learning, current\ndeep-learning memory models struggle in reinforcement learning environments\nthat are partially observable and long-term. They fail to efficiently capture\nrelevant past information, adapt flexibly to changing observations, and\nmaintain stable updates over long episodes. We theoretically analyze the\nlimitations of existing memory models within a unified framework and introduce\nthe Stable Hadamard Memory, a novel memory model for reinforcement learning\nagents. Our model dynamically adjusts memory by erasing no longer needed\nexperiences and reinforcing crucial ones computationally efficiently. To this\nend, we leverage the Hadamard product for calibrating and updating memory,\nspecifically designed to enhance memory capacity while mitigating numerical and\nlearning challenges. Our approach significantly outperforms state-of-the-art\nmemory-based methods on challenging partially observable benchmarks, such as\nmeta-reinforcement learning, long-horizon credit assignment, and POPGym,\ndemonstrating superior performance in handling long-term and evolving contexts.",
  "text": "Stable Hadamard Memory: Revitalizing\nMemory-Augmented Agents for Reinforcement\nLearning\nHung Le, Kien Do, Dung Nguyen, Sunil Gupta, and Svetha Venkatesh\nApplied AI Institute, Deakin University, Geelong, Australia\nthai.le@deakin.edu.au\nAbstract\nEffective decision-making in partially observable environments demands robust\nmemory management. Despite their success in supervised learning, current deep-\nlearning memory models struggle in reinforcement learning environments that are\npartially observable and long-term. They fail to efficiently capture relevant past\ninformation, adapt flexibly to changing observations, and maintain stable updates\nover long episodes. We theoretically analyze the limitations of existing memory\nmodels within a unified framework and introduce the Stable Hadamard Memory,\na novel memory model for reinforcement learning agents. Our model dynamically\nadjusts memory by erasing no longer needed experiences and reinforcing crucial\nones computationally efficiently. To this end, we leverage the Hadamard product\nfor calibrating and updating memory, specifically designed to enhance memory\ncapacity while mitigating numerical and learning challenges. Our approach sig-\nnificantly outperforms state-of-the-art memory-based methods on challenging par-\ntially observable benchmarks, such as meta-reinforcement learning, long-horizon\ncredit assignment, and POPGym, demonstrating superior performance in handling\nlong-term and evolving contexts.\n1\nIntroduction\nReinforcement learning agents necessitate memory. This is especially true in Partially Observable\nMarkov Decision Processes (POMDPs [Kaelbling et al., 1998]), where past information is crucial for\nmaking informed decisions. However, designing a robust memory remains an enduring challenge,\nas agents must not only store long-term memories but also dynamically update them in response to\nevolving environments. Memory-augmented neural networks (MANNs)–particularly those devel-\noped for supervised learning [Graves et al., 2016, Vaswani et al., 2017], while offering promising\nsolutions, have consistently struggled in these dynamic settings. Recent empirical studies [Morad\net al., 2023, Ni et al., 2024] have shown that MANNs exhibit instability and underperform simpler\nvector-based memory models such as GRU [Chung et al., 2014] or LSTM [Hochreiter, 1997]. The\nissue is exacerbated in complex and sparse reward scenarios where agents must selectively retain\nand erase memories based on relevance. Unfortunately, existing methods fail to provide a memory\nwriting mechanism that is simultaneously efficient, stable, and flexible to meet these demands.\nIn this paper, we focus on designing a better writing mechanism to encode new information into\nthe memory. To this end, we introduce the Hadamard Memory Framework (HMF), a unified model\nthat encompasses many existing writing methods as specific cases. This framework highlights the\ncritical role of memory calibration, which involves linearly adjusting memory elements by multi-\nplying the memory matrix with a calibration matrix and then adding an update matrix. By leverag-\ning Hadamard products that operate element-wise on memory matrices, we allow memory writing\nPreprint. Under review.\narXiv:2410.10132v1  [cs.LG]  14 Oct 2024\nwithout mixing the memory cells in a computationally efficient manner. More importantly, the cali-\nbration and update matrices are dynamically computed based on the input at each step. This enables\nthe model to learn adaptive memory rules, which are crucial for generalization. For instance, in\nmeta-reinforcement learning with varying maze layouts, a fixed memory update rule may work for\none layout but fail in another. By allowing the calibration matrix to adjust according to the current\nmaze observation, the agent can learn to adapt to any layout configuration. A dynamic calibration\nmatrix also enables the agent to flexibly forget and later recall information as needed. For example,\nan agent navigating a room may need to remember the key’s location, retain it during a detour, and\nlater recall it when reaching a door, while discarding irrelevant detour events.\nAlthough most current memory models can be reformulated within the HMF, they tend to be either\noverly simplistic with limited calibration capabilities [Katharopoulos et al., 2020, Radford et al.,\n2019] or unable to manage memory writing reliably, suffering from gradient vanishing or exploding\nissues [Ba et al., 2016, Morad et al., 2024]. To address these limitations, we propose a specific in-\nstance of HMF, called Stable Hadamard Memory, which introduces a novel calibration mechanism\nbased on two key principles: (i) dynamically adjusting memory values in response to the current\ncontext input to selectively weaken outdated or enhance relevant information, and (ii) ensuring the\nexpected value of the calibration matrix product remains bounded, thereby preventing gradient van-\nishing or exploding. Through extensive experimentation on POMDP benchmarks, including meta\nreinforcement learning, long-horizon credit assignment, and hard memorization games, we demon-\nstrate that our method consistently outperforms state-of-the-art memory-based models in terms of\nperformance while also delivering competitive speed. We also provide comprehensive ablation stud-\nies that offer insights into the components and internal workings of our memory models.\n2\nBackground\n2.1\nReinforcement Learning Preliminaries\nA Partially Observable Markov Decision Process (POMDP) is formally defined as a tuple\n⟨S, A, O, R, P, γ⟩, where S is the set of states, A is the set of actions, O is the observation space,\nR : S ×A →R is the reward function, P : S ×A →∆(S) defines the state transition probabilities,\nand γ ∈[0, 1) is the discount factor. Here, the agent does not directly observe the true environment\nstate st. Instead, it receives an observation ot ∼O(st), which provides partial information about\nthe state, often not enough for optimal decision making. Therefore, the agent must make decisions\nbased on its current observation ot and a history of previous observations, actions, and rewards\n(a0, r0, o1, . . . , at−1, rt−1, ot). The history may exclude past rewards or actions.\nLet us denote the input context at timestep t as xt = (ot, at−1, rt−1) and assume that we can encode\nthe sequence of contexts into a memory Mt = f\n\u0010\n{xi}t\ni=1\n\u0011\n. The goal is to learn a policy π(at|Mt)\nthat maximizes the expected cumulative discounted reward:\nJ(π) = Eπ\n\" ∞\nX\nt=1\nγtR(st, at)|at ∼π(at|Mt), st+1 ∼P(st+1|st, at), ot ∼O(st)\n#\n(1)\nThus, a memory system capable of capturing past experiences is essential for agents to handle the\npartial observability of the environment while maximizing long-term rewards.\n2.2\nMemory-Augmented Neural Networks\nWe focus on matrix memory M, and to simplify notation, we assume it is a square matrix. Given a\nmemory M ∈RH×H, we usually read from the memory as:\nht = Mtq (xt)\n(2)\nwhere q is a query network q : RD 7→RH to map an input context xt ∈RD to a query vector.\nThe read value ht later will be used as the input for policy/value functions. Even more important\nthan the reading process is the memory writing: How can information be written into the memory\nto ensure efficient and accurate memory reading? A general formulation for memory writing is\nMt = f (Mt−1, xt) with f as the update function that characterizes the memory models.\n2\nThe simplest form of memory writing traces back to Hebbian learning rules: Mt = Mt−1 + xt ⊗xt\nwhere ⊗is outer product [Kohonen and Ruohonen, 1973, Hebb, 2005]. Later, researchers have\nproposed “fast weight” memory [Marr and Thach, 1991, Schmidhuber, 1992, Ba et al., 2016]:\nMt = Mt−1λ + ηg (Mt−1, xt) ⊗g (Mt−1, xt)\n(3)\nwhere g is a non-linear function that take the previous memory and the current input data as the input;\nλ and η are constant hyperparameters. On the other hand, computer-inspired memory architectures\nsuch as Neural Turing Machine (NTM, [Graves et al., 2014]) and Differentiable Neural Computer\n(DNC, [Graves et al., 2016]) introduce more sophisticated memory writing:\nMt = Mt−1 ⊙(1 −w (Mt−1, xt) ⊗e (Mt−1, xt)) + w (Mt−1, xt) ⊗v (Mt−1, xt)\n(4)\nwhere w, e and v are non-linear functions that take the previous memory and the current input data\nas the input to produce the writing weight, erase and value vectors, respectively. ⊙is the Hadamard\n(element-wise) product.\nThe problem with non-linear f w.r.t M is that the computation must be done in recursive way, and\nthus being slow. Therefore, recent memory-based models adopt simplified linear dynamics. For\nexample, Linear Transformer’s memory update reads [Katharopoulos et al., 2020]:\nMt = Mt−1 + v (xt) ⊗\nϕ (k (xt))\nP\nt ϕ (k (xt))\n(5)\nwhere ϕ is an activation function; k and v are functions that transform the input to key and value.\nRecently, Beck et al. [2024] have proposed matrix-based LSTM (mLSTM):\nMt = f (xt) Mt−1 + i (xt) v (xt) ⊗k (xt)\n(6)\nwhere f and i are the forget and input gates, respectively. In another perspective inspired by neuro-\nscience, Fast Forgetful Memory (FFM, [Morad et al., 2024]) employs a parallelable memory writing,\nwhich processes a single step update as follows:\nMt = Mt−1 ⊙γ +\n\u0000v (xt) ⊗1⊤\u0001\n⊙γt−n\n(7)\nwhere γ is a trainable matrix, v is an input transformation function, and n is the last timestep.\n3\nMethods\nIn this section, we begin by introducing a unified memory writing framework that incorporates\nseveral of the memory writing approaches discussed earlier. Next, we examine the limitations of\ncurrent memory writing approaches through an analysis of this framework. Following this analysis,\nwe propose specialized techniques to address these limitations. For clarity and consistency, all\nmatrix and vector indices will be referenced starting from 1, rather than 0. Constant matrices are\ndenoted by bold numbers.\n3.1\nHadamard Memory Framework (HMF)\nWe propose a general memory framework that uses the Hadamard product as its core operation. The\nmemory writing at time step t is defined as:\nMt = Mt−1 ⊙Cθ (xt)\n| {z }\nCt\n+ Uφ (xt)\n| {z }\nUt\n(8)\nwhere Cθ : RD 7→RH×Hand Uφ : RD 7→RH×H are parameterized functions that map the current\ninput xt to two matrices Ct (calibration matrix) and Ut (update matrix). Here, θ and φ are referred\nto as calibration and update parameters. Intuitively, the calibration matrix Ct determines which\nparts of the previous memory Mt−1 should be weakened and which should be strengthened while\nthe update matrix Ut specifies the content to be encoded into the memory. We specifically choose\n3\nthe Hadamard product (⊙) as the matrix operator because it operates on each memory element\nindividually. We avoid using the matrix product to prevent mixing the content of different memory\ncells during calibration and update. Additionally, the matrix product is computationally slower.\nThere are many ways to design Ct and Ut. Given proper choices of Ct and Ut, Eqs. 3-7 can be\nreformulated into Eq. 8. Inspired by prior “fast weight” works, we propose a simple update matrix:\nUφ (xt) = ηφ (xt) [v (xt) ⊗k (xt)]\n(9)\nwhere k and v are trainable neural networks that transform the input xt to key and value representa-\ntions. ηφ : RD 7→R is a parameterized function that maps the current input xt to an update gate that\ncontrols the amount of update at step t. For example, if ηφ (xt) = 0, the memory will not be updated\nwith any new content, whereas ηφ (xt) = 1 the memory will be fully updated with the content from\nthe t-th input. We implement ηφ (xt) as a neural network with sigmoid activation function.\nWe now direct our focus towards the design of Ct, which is the core contribution of our work.\nThe calibration matrix selectively updates the memory by erasing no longer important memories\nand reinforcing ongoing critical ones. In a degenerate case, if Ct = 1 for all t, the memory will\nnot forget or strengthen any past information, and will only memorize new information over time,\nsimilar to the Hebbian Rule and Transformers. To analyze the role of the calibration matrix, it is\nuseful to unroll the recurrence, leading to the closed-form equation (see proof in Appendix A.1):\nMt = M0\ntY\ni=1\nCi +\nt\nX\ni=1\nUi ⊙\ntY\nj=i+1\nCj\n(10)\nwhere Q represents element-wise products.\nThen, ht = Mtq (xt) = M0\nQt\ni=1 Ciq (xt) +\nPt\ni=1 Ui ⊙Qt\nj=i+1 Cjq (xt). Calibrating the memory is important because without calibration\n(Ct = 1 ∀t), the read value becomes: ht = M0q (xt) + Pt\ni=1 Uiq (xt) . In this case, mak-\ning ht to reflect a past context at any step j requires that q (xt) ̸= 0 and P\ni̸=j Uiq (xt) =\nP\ni̸=j ηφ (xi) v (xi) [k (xi) · q (xt)] ≈0, which can be achieved if we can find q (xt) such that\nk (xi) · q (xt) ≈0 ∀i ̸= j. Yet, this becomes hard when T ≫H and ηφ (xi) ̸= 0 as it leads to an\noverdetermined system with more equations than variables. We note that avoiding memorizing any\ni-th step with ηφ (xi) = 0 is suboptimal since xi may be required for another reading step t′ ̸= t.\nTherefore, at a certain timestep t, it is critical to eliminate no longer relevant timesteps from Mt by\ncalibration, i.e., Ui ⊙Qt\nj=i+1 Cj ≈0 for unimportant i (forgetting). For example, an agent may\nfirst encounter an important event, like seeing a color code, before doing less important tasks, such\nas picking apples. When reaching the goal requiring to identify a door matching the color code, it\nwould be beneficial for the agent to erase memories related to the apple-picking task, ensuring a\nclean retrieval of relevant information–the color code. Conversely, if timestep i becomes relevant\nagain at a later timestep t′, we need to recover its information, ensuring Ui ⊙Qt′\nj=i+1 Cj ̸= 0\n(strengthening), just like the agent, after identifying the door, may return to collecting apple task.\nRemark 1. In the Hadamard Memory Framework, calibration should be enabled (Ct ̸= 1) and\nconditioned on the input context.\nRegarding computing efficiency, if Ct and Ut are not functions of M<t, we can compute the memory\nusing Eq. 10 in parallel, ensuring fast execution. In particular, the set of products\nnQt\nj=i+1 Cj\not\ni=1\ncan be calculated in parallel in O (log t) time. The summation can also be done in parallel in\nO (log t) time. Additionally, since all operations are element-wise, they can be executed in par-\nallel with respect to the memory dimensions. Consequently, the total time complexity is O (log t).\nAppendix Algo. 1 illustrates an implementation supporting parallelization.\nRemark 2. In the Hadamard Memory Framework, with optimal parallel implementation, the time\ncomplexity for processing a sequence of t steps is O (log t). By contrast, without parallelization, the\ntime complexity is O\n\u0000tH2\u0001\n.\n4\n3.2\nChallenges on Memory Calibration\nThe calibration matrix enables agents to either forget or enhance past memories. However, it com-\nplicates learning due to the well-known issues of gradient vanishing or exploding. This can be\nobserved when examining the policy gradient over T steps, which reads:\n∇ΘJ (πΘ) = Es,a∼πΘ\nT\nX\nt=0\n∇Θ log πΘ (at|Mt)\n|\n{z\n}\nGt(Θ)\nAdv (st, at, γ)\n(11)\nwhere Θ is the set of parameters, containing {θ, φ}, Adv represents the advantage function, which\nintegrates reward information R (st, at), and Gt (Θ) =\n∂log πΘ(at|Mt)\n∂Mt\n∂Mt\n∂Θ captures information\nrelated to the memory. Considering main gradient at step t, required to learn θ and φ:\n∂Mt\n∂θ\n= M0\n∂Qt\ni=1 Cθ(xi)\n∂θ\n+\nt\nX\ni=1\nUφ (xi) ⊙\n∂Qt\nj=i+1 Cθ(xj)\n∂θ\n|\n{z\n}\nG1(i,t,θ)\n;\n(12)\n∂Mt\n∂φ =\nt\nX\ni=1\n∂Uφ (xi)\n∂φ\n⊙\ntY\nj=i+1\nCθ(xj)\n|\n{z\n}\nG2(i,t,θ)\n(13)\nWe collectively refer G1 (i, t, θ) and G2 (i, t, θ) as G1,2 (i, t, θ) ∈RH×H. These terms are critical as\nthey capture the influence of state information at timestep i on the learning parameters θ and φ. The\ntraining challenges arise from these two terms as the number of timesteps t increases: (i) Numerical\nInstability (Gradient Exploding): if ∃m.k ∈[1, H] s.t. G1,2 (i, t, θ) [m, k] →∞, this leads to\noverflow, causing the gradient to become “nan”; (ii) Learning Difficulty (Gradient Vanishing): if\nt ≫i0, ∥G1,2 (i, t, θ)∥≈0 ∀i < i0, meaning no signal from timesteps i < i0 contributes to\nlearning the parameters. This is suboptimal, especially when important observations occur early in\nthe episode, and rewards are sparse and given at the episode end, .i.e., R (st, at) = 0 ∀t ̸= T.\nHow to design the calibration matrix Cθ (x) to overcome the training challenges? A common\napproach is to either fix it as hyperparameters or make it learnable parameters independent on the\ninput xt (e.g., Eqs. 3 and 7). Unfortunately, we can demonstrate that this leads to either numerical\ninstability or learning difficulties as formalized in Proposition 3. In the next section, we will provide\na better design for the calibration matrix.\nProposition 3. If calibration is enabled Cθ (xt) ̸= 1, yet the calibration matrix is fixed, independent\nof the input xt (∀t : Cθ (xt) = θ ∈RH×H), numerical instability or learning difficulty will arise.\nProof. See Appendix A.2\n3.3\nStable Hadamrad Memory (SHM)\nTo avoid numerical and learning problems, it is important to ensure each element of Ct is not al-\nways greater than 1 or smaller than 1, which ends up in their product will be bounded such that\nE\nhQT\nt=1 Ct\ni\n̸= {0, ∞} as T →∞. At the same time, we want Ct to be a function of xt to enable\ncalibration conditioned on the current context. To this end, we propose the calibration matrix:\nCθ (xt) = 1 + tanh (θt ⊗vc (xt))\n(14)\nwhere vc : RD 7→RH is a mapping function, and θt ∈RH represents the main calibration parame-\nters. Here, we implement vc as a linear transformation to map the input to memory space. Notably,\nthe choice of θt determines the stability of the calibration. We propose to design θt as trainable\nparameters that is randomly selected from a set of parameters θ. In particular, given θ ∈RL×H,\n5\nwe sample uniformly a random row from θ, θt = θ [lt] where lt ∼U (1, L) where L is the number\nof possible θt. We name the design as Stable Hadamard Memory (SHM). Given the formulation,\nthe range of an element zm,k\nt\n= Cθ (xt) [m, k] is [0, 2] where m, k ∈[1, H]. We can show one\nimportant property of this choice is that E\nhQT\nt=1 Ct\ni\n≈1 under certain conditions.\nProposition 4. Assume: (i) xt ∼N (0, Σt), (ii) vf is a linear transformation, i.e.,vc (xt) = Wxt,\n(iii) {zt = θt ⊗vc (xt)}T\nt=1 are independent across t. Given Cθ (xt) defined in Eq. 14 then ∀T ≥\n0, 1 ≤m, k ≤H:\nE\n\" T\nY\nt=1\nzm,k\nt\n#\n= E\n\" T\nY\nt=1\nCθ (xt) [m, k]\n#\n= 1\nProof. see Appendix A.3\nAssumption (i) is reasonable as xt can be treated as being drawn from a Gaussian distribution, and\nLayerNorm can be applied to normalize xt to have a mean of zero. Assumption (ii) can be realized\nas we implement vc as a linear transformation. Assumption (iii) is more restrictive because {xt}T\nt=1\nare often dependent in RL setting, which means {zt = θt ⊗vc (xt)}T\nt=1 are not independent and\nthus, E\nhQT\nt=1 zm,k\nt\ni\n̸= 1. However, by reducing the linear dependence between zt through random\nselection of θt, we can make E\nhQT\nt=1 zm,k\nt\ni\ncloser to 1, and thus being bounded. Specifically,\nwe will prove that by using θt = θ [lt] ; lt ∼U (1, L) the Pearson Correlation Coefficient between\ntimesteps is minimized, as stated in the following proposition:\nProposition 5. Let zm,k\nt\n= um\nt vk\nt where zm,k\nt\n= (θt ⊗vc (xt)) [m, k], um\nt\n= θt [m] and vk\nt =\nvc (xt) [k]. Given the Pearson correlation coefficient of two random variables X and Y is defined\nas ρ (X, Y ) =\nCov(X,Y )\n√\nVar(X)√\nVar(Y ), then ∀vk\nt , vk\nt′:\n\f\fρ\n\u0000um\nt vk\nt , um\nt′ vk\nt′\n\u0001\f\f ≤\n\f\fρ\n\u0000vk\nt , vk\nt′\n\u0001\f\f\nThe equality holds when um\nt = βum\nt′ .\nProof. See Appendix A.4\nAs a result, our choice of θt outperforms other straightforward designs for minimizing dependencies\nbetween timesteps. For instance, a fixed θt (θt = θ ∈RH) results in higher dependencies because\nρ(θ [m] vk\nt , θ [m] vk\nt′) = ρ(vk\nt , vk\nt′) ≥ρ\n\u0000um\nt vk\nt , um\nt′ vk\nt′\n\u0001\n. In practice, even when E\nhQT\nt=1 zm,k\nt\ni\nis bounded, the cumulative product can occasionally become very large for certain episodes and\ntimesteps, leading to overflow and disrupting the learning process. This can be avoided by clipping\nthe gradients. In experiments, we implement SHM using nonparallel recursive form (Eq. 8). The\nmemory is then integrated into policy-gradient RL algorithms to optimize Eq. 1, with the read value\nht (Eq. 2) used as input for value/policy networks. Code will be available upon publication.\n4\nExperimental Results\nWe evaluate our method alongside notable memory-augmented agents in POMDP environments.\nUnless stated otherwise, the context consists of the observation and previous action. All training\nuses the same hardware (single NVDIA H100 GPU), RL architecture, algorithm, training protocol,\nand hyperparameters. We focus only on model-free RL algorithms. The baselines differ only in\ntheir memory components: GRU [Chung et al., 2014], FWP [Schlag et al., 2021], GPT-2 [Radford\net al., 2019], S6 [Gu and Dao, 2023], mLSTM [Beck et al., 2024], FFM [Morad et al., 2024] and\nSHM (Ours). For tasks with a clear goal, we measure performance using the Success Rate, defined\nas the ratio of episodes that reach the goal to the total number of evaluation episodes. For tasks in\nPOP-Gym, we use Episode Return as the evaluation metric. We fix SHM’s number of possible θt,\nL = 128, across experiments.\n6\nFigure 1: Meta-RL: Wind and Point Robot learning curves. Mean ± std. over 5 runs.\nFigure 2: Credit Assignment: Visual Match, Key-to-Door learning curves. Mean ± std. over 3 runs.\n4.1\nSample-Efficient Meta Reinforcement Learning\nMeta-RL targets POMDPs where rewards and environmental dynamics differ across episodes, rep-\nresenting various tasks [Schmidhuber, 1987, Thrun and Pratt, 1998]. To excel in all tasks, memory\nagents must learn general memory update rules that can adapt to any environments. We enhanced\nthe Wind and Point Robot environments from Ni et al. [2022] to increase difficulty. In these environ-\nments, the observation consists of the agent’s 2D position pt, while the goal state pg is hidden. The\nagent takes continuous actions at by moving with 2D velocity vector. The sparse reward is defined\nas R(pt+1, pg) = 1(∥pt+1 −pg∥2 ≤r) where r = 0.01 is the radius. In Wind, the goal is fix\npg = [0, 1], yet there are noises in the dynamics: pt+1 = pt + at + w, with the “wind” w sampled\nfrom U[−0.1, 0.1] at the start and fixed thereafter. In Point Robot, the goal varies across episodes,\nsampled from U[−10, 10]. To simulate real-world conditions where the training tasks are limited,\nwe create 2 modes using different number of training and testing tasks: [50, 150] and [10, 190],\nrespectively. Following the modifications, these simple environments become significantly more\nchallenging to navigate toward the goal, so we set the episode length to 100.\nWe incorporate the memory methods to the Soft Actor Critic (SAC, [Haarnoja et al., 2018]), using\nthe same code base introduced in Ni et al. [2022]. We keep the SHM model sizes and memory\ncapacities small, at around 2MB for the checkpoint and 512 memory elements, which is roughly\nequivalent to a GRU (see Appendix C.1). We train all models for 500,000 environment steps, and\nreport the learning curves in Fig. 1. In the easy mode (50-150), our SHM method consistently\nachieves the best performance, with a near-optimal success rate, while other methods underperform\nby 20-50% on average in Wind and Point Robot, respectively. In the hard mode (10-190), SHM\ncontinues to outperform other methods by approximately 20%, showing earlier signs of learning.\n4.2\nLong-term Credit Assignment\nIn this task, we select the Visual Match and Key-to-Door environments, the most challenge tasks\nmentioned in Ni et al. [2024]. Both have observation as the local view of the agent, discrete actions\nand sparse rewards dependent on the full trajectory, requiring long-term memorization. In particular,\nthe pixel-based Visual Match task features an intricate reward system: in Phase 1, observing color\ncodes yields no rewards, while in Phase 2, picking an apple provides immediate reward of one,\n7\nTask\nLevel\nGRU\nFFM\nSHM (Ours)\nAutoencode\nEasy\n-37.9±7.7\n-32.7±0.6\n49.5±23.3\nMedium\n-43.6±3.5\n-32.7±0.6\n-28.8±14.4\nHard\n-48.1±0.7\n-47.7±0.5\n-43.9±0.9\nBattleship\nEasy\n-41.1±1.0\n-34.0±7.1\n-12.3±2.4\nMedium\n-39.4±0.5\n-37.1±3.1\n-16.8±0.6\nHard\n-38.5±0.5\n-38.8±0.3\n-21.2±2.3\nConcentration\nEasy\n-10.9±1.0\n10.7±1.2\n-1.9±2.4\nMedium\n-21.4±0.5\n-24.7±0.1\n-21.0±0.8\nHard\n-84.0±0.3\n-87.5±0.5\n-83.3±0.1\nRepeatPrevious\nEasy\n99.9±0.0\n98.4±0.3\n88.9±11.1\nMedium\n-34.7±1.7\n-24.3±0.4\n48.2±7.2\nHard\n-41.7±1.8\n-33.9±1.0\n-19.4±9.9\nAverage\nAll\n-28.4±1.3\n-24.2±1.2\n-5.1±6.3\nTable 1: PopGym: Mean return ± std. (×100) at the end of training over 3 runs. The range of return\n(×100) is [−100, 100].\nrelying on short-term memory. The final reward–a bonus for reaching the door with the matching\ncolor code is set to 10. Key-to-Door also involves multiple phases: finding the key, picking apples,\nand reaching the door. The terminal reward is given if the key was acquired in the first phase and used\nto open the door. Both tasks can be seen as decomposed episodic problems with noisy immediate\nrewards, requiring that in the final phase, the agent remembers the event in the first phase. We create\n2 configurations using different episode steps in the second phases: 250 and 500, respectively.\nStill following Ni et al. [2022], we use SAC-Discrete [Christodoulou, 2019] as the RL algorithm.\nWe use the same set of baselines as in Sec. 4.1 and train them for 2 million environment steps. The\nresults in Fig. 2 clearly show that, our method, SHM, significantly outperforms all other methods in\nterms of success rate. Notably, SHM is the only method that can perfectly solve both 250 and 500-\nstep Visual Match while the second-best performer, FFM, achieves only a 77% and 25% success rate,\nrespectively. In Key-To-Door, our method continues showing superior results with high success rate.\nBy contrast, no meaningful learning is observed from the other methods, which perform similarly to\nGPT-2, as also noted by Ni et al. [2024].\n4.3\nPOPGym Hardest Games\nWe evaluate SHM on the POPGym benchmark [Morad et al., 2023], the largest POMDP benchmark\nto date. Following previous studies [Morad et al., 2023, Samsami et al., 2024], we focus on the most\nmemory-intensive tasks: Autoencode, Battleship, Concentration and RepeatPrevious. These tasks\nrequire ultra long-term memorization, with complexity increasing across Easy, Medium, and Hard\nlevels. All tasks use categorical action and observation spaces, allowing up to 1024 steps.\nFor comparison, we evaluate SHM against state-of-the-art model-free methods, including GRU and\nFFM. Other memory models, such as DNC, Transformers, FWP, and SSMs, have been reported to\nperform worse. The examined memory models are integrated into PPO [Schulman et al., 2017],\ntrained for 15 million steps using the same codebase as Morad et al. [2023] to ensure fairness.\nThe models differ only in their memory, controlled by the memory dimension hyperparameter H.\nWe tune it for each baseline, adjusting it to optimize performance, as larger H values typically\nimprove results. The best-performing configurations are reported in Table 1, where SHM demon-\nstrates a relative improvement of ≈10-12% over FFM and GRU on average. Notably, the learning\ncurves in Appendix Fig. 4 show that only SHM demonstrates signs of learning in several tasks,\nincluding RepeatPrevious-Medium/Hard, and Autoencode-Easy/Medium. Detailed hyperparameter\nsetting and additional results are provided in Appendix C.2.\nIn terms of running time, the average batch inference time in milliseconds for GRU, FFM, and SHM\nis 1.6, 1.8, and 1.9, respectively, leading to a total of 7, 8, and 9 hours of training per task. While\nSHM is slightly slower than GRU and FFM, the difference is a reasonable trade-off for improved\nmemory management in partially observable environments. Last but not least, our model’s runtime\nwas measured using a non-parallel implementation, while GRU benefits from hardware optimization\nin the PyTorch library. SHM’s running time could be further improved with proper parallelization.\n8\n(a)\n(b)\n(c)\n-0.5\n0\n0.5\n1.0\n0.2\n0.4\n0.6\n0.8\n0 0\n100\nAutoencode-Easy\nBattleship-Easy\n0\n2\n-100\n100\nM\nC\nForget Remember\nImportant memory \nPhase 1\nPhase 2\nPhase 3\nt=0\nt=15\nt=20\nt=50\nt=80\nt=110\nt=115\nt=120\nt=124\nt\nt\nt\nFigure 3: (a) Left: Return of calibration designs over 3 runs; Right: Calibration matrix cumulative\nproduct over 100 episodes. (b) Return of memory sizes H on Autoencode-Easy (left) and Battleship-\nEasy (right). (c) Memory (M, top) and calibration (C, bottom) matrices over timesteps in Visual\nMatch: SHM erases memory that are no longer required and strengthens the important ones.\n4.4\nModel Analysis and Ablation Study\nChoice of Calibration Prop. 5 suggests that selecting random θt ∈RH in Eq. 14 will reduce the\ndependencies between Ct, bringing QT\nt=1 Ct closer to 1 to avoid gradient issues. In this section, we\nempirically verify that by comparing our proposed Random θt with the following designs: C = 1,\nno calibration is used; Random C, where a random calibration matrix is sampled from normal\ndistribution at each timestep, having E\nhQT\nt=1 zm,k\nt\ni\n= 1 under mild assumptions, but preventing\nlearning meaningful calibration rules; Fixed C, a simpler method for learning calibration, but prone\nto gradient problems (Prop. 3); Fixed θt, where we learn fixed parameter θt, which is proven to be\nless effective than Random θt in reducing linear dependencies between timesteps (Prop. 5); Neural\nθt, where θt = FFW (xt), generated by a feedforward neural network like mLSTM, but with no\nguarantee of reducing timestep dependencies. We trained RL agents using the above designs of the\ncalibration matrix with H = 72 on the Autoencode-Easy task and present the learning curves in\nFig. 3 (a, left). The results show that our proposed Random θt outperforms the other baselines by a\nsubstantial margin, with at least a 30% improvement in performance. This confirms the effectiveness\nof our calibration design in enhancing the final results.\nVanishing Behavior In practice, exploding gradients can be mitigated by clipping. Thus, we focus\non the vanishing gradient, which depends on the cumulative product Cj = Qj\nt=1 Ct. Our theory\nsuggests that Random θt should be less susceptible to the vanishing phenomenon compared to other\nmethods such as Fixed C, Fixed θt and Neural θt. To verify that, for each episode, we compute the\naverage value of elements in the matrix Cj that are smaller than 1 (Cj [< 1]), as those larger than\n1 are prone to exploding and are not appropriate for averaging with the vanishing values. We plot\nCj [< 1] for j = 1, 2, ...100 over 100 episodes in Fig 3 (a, right).\nThe results are consistent with theoretical predictions: Fixed C leads to rapid vanishing of the\ncumulative product in just 10 steps. Neural θt also perform badly, likely due to more complex\ndependencies between timesteps because zm,k\nt\nnow becomes non-linear function of xt, causing\nQT\nt=1 zm,k\nt\nto deviate further from 1. While Fixed θt is better than Neural θt, it still exhibits quicker\nvanishing compared to our approach Random θt. As expected, Random C shows little vanishing,\nbut like setting C = 1, it fails to leverage memory calibration, resulting in underperformance (Fig.\n3 (a, left)). Random θt, although its Cj also deviates from 1, shows the smallest deviation among the\ncalibration learning approaches. Additionally, the vanishing remain manageable after 100 timesteps,\nallowing gradients to still propagate effectively and enabling the calibration parameters to be learned.\n9\nMemory Size The primary hyperparameter of our method is H, determining the memory capacity.\nWe test SHM with H ∈{24, 72, 128, 156} on the Autoencode-Easy and Battleship-Easy tasks.\nFig. 3 (b) shows that larger memory generally results in better performance. In terms of speed,\nthe average batch inference times (in milliseconds) for different H values are 1.7, 1.8, 1.9, and 2.1,\nrespectively. We choose H = 128 for other POPGym tasks to balance performance and speed.\nForgetting and Remembering Here, we investigate the learned calibration strategy of SHM on\nVisual Match with 15,100, and 10 steps in Phase 1, 2 and, 3, respectively. We sample several repre-\nsentative Mt and Ct from 3 phases and visualize them in Fig. 3 (c). In Phase 1, the agent identifies\nthe color code and stores it in memory, possibly in two columns of M, marked as “important mem-\nory”. In Phase 2, unimportant memory elements are erased where Ct ≈0 (e.g., those within the\nblue rectangle). However, important experiences related to the Phase 1’s code are preserved across\ntimesteps until Phase 3 (e.g., those within the red rectangle where Ct ≳1), which is desirable.\n5\nRelated works\nNumerous efforts have been made to equip RL agents with memory mechanisms. Two main research\ndirections focus on inter-episode and intra-episode memory approaches. While episodic control with\na global, tabular memory enhances sample efficiency by storing experiences across episodes [Blun-\ndell et al., 2016, Le et al., 2021, 2022a], it falls short in recalling specific events within individual\nepisodes. Similarly, global memory mechanisms can support exploration or optimization [Badia\net al., 2020, Le et al., 2022b, 2024], but are not designed to address the challenges of memoriza-\ntion within a single episode. In contrast, to address the challenges of partially observable Markov\ndecision processes (POMDPs), RL agents often leverage differentiable memory, which is designed\nto capture the sequence of observations within an episode, and can be learned by policy gradient\nalgorithms [Wayne et al., 2018].\nDifferentiable memory models can be broadly categorized into vector-based and matrix-based ap-\nproaches. Vector-based memory, like RNNs [Elman, 1990], processes inputs sequentially and stores\npast inputs in their hidden states. While RNNs are slower to train, they are efficient during infer-\nence. Advanced variants, such as GRU and LSTM, have shown strong performance in POMDPs,\noften outperforming more complex RL methods [Ni et al., 2022, Morad et al., 2023]. Recently,\nfaster alternatives like convolutional and structured state space models (SSM) have gained attention\n[Bai et al., 2018, Gu et al., 2020], though their effectiveness in RL is still under exploration. Initial\nattempts with models like S4 underperformed in POMDP tasks [Morad et al., 2023], but improved\nSSM versions using S5, S6 or world models have shown promise [Lu et al., 2024, Gu and Dao,\n2023, Samsami et al., 2024]. Despite these advancements, vector-based memory is limited, as com-\npressing history into a single vector makes it challenging to scale for high-dimensional memory\nspace.\nMatrix-based memory, on the other hand, offers higher capacity by storing history in a matrix\nbut at the cost of increased complexity. Attention-based models, such as Transformers [Vaswani\net al., 2017], have largely replaced RNNs in SL, also delivering good results in standard POMDPs\n[Parisotto et al., 2020]. However, their quadratic memory requirements limit their use in envi-\nronments with long episodes. Empirical studies have also shown that Transformers struggle with\nlong-term memorization and credit assignment tasks [Ni et al., 2024].\nWhile classic memory-\naugmented neural networks (MANNs) demonstrated good performance in well-crafted long-term\nsettings [Graves et al., 2016, Hung et al., 2019, Le et al., 2020], they are slow and do not scale well\nin larger benchmarks like POPGym [Morad et al., 2023]. New variants of LSTM [Beck et al., 2024],\nincluding those based on matrices, have not been tested in reinforcement learning settings and lack\ntheoretical grounding to ensure stability.\nSimplified matrix memory models [Katharopoulos et al., 2020, Schlag et al., 2021], offer scalable so-\nlutions but have underperformed compared to simple RNNs in the POPGym benchmark, highlight-\ning the challenges of designing efficient matrix memory for POMDPs. Recently, Fast and Forgetful\nMemory (FFM, [Morad et al., 2024]), incorporating inductive biases from neuroscience, has demon-\nstrated better average results than RNNs in the benchmark. However, in the most memory-intensive\nenvironments, the improvement remains limited. Compared to our approach, these matrix-based\nmemory methods lack a flexible memory calibration mechanism and do not have robust safeguards\nto prevent numerical and learning issues in extremely long episodes.\n10\n6\nDiscussion\nIn this paper, we introduced the Stable Hadamard Framework (SHF) and its effective instance, the\nStable Hadamard Memory (SHM), a novel memory model designed to tackle the challenges of\ndynamic memory management in partially observable environments. By utilizing the Hadamard\nproduct for memory calibration and update, SHM provides an efficient and theoretically grounded\nmechanism for selectively erasing and reinforcing memories based on relevance. Our experiments\non the POPGym and POMDP benchmarks demonstrate that SHM significantly outperforms state-of-\nthe-art memory-based models, particularly in long-term memory tasks, while being fast to execute.\nAlthough our theory suggests that SHM should be more stable and mitigate gradient learning issues\nby reducing linear dependencies between timesteps, this stability is not guaranteed to be perfect.\nFurther theoretical investigation is needed to validate and refine these properties in future work.\nReferences\nJimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast\nweights to attend to the recent past. In Advances in Neural Information Processing Systems,\npages 4331–4339, 2016.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven\nKapturowski, Olivier Tieleman, Mart´ın Arjovsky, Alexander Pritzel, Andew Bolt, et al. Never\ngive up: Learning directed exploration strategies. arXiv preprint arXiv:2002.06038, 2020.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\nand recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\nMaximilian Beck, Korbinian P¨oppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova,\nMichael Kopp, G¨unter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended\nlong short-term memory. arXiv preprint arXiv:2405.04517, 2024.\nCharles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,\nJack Rae, Daan Wierstra, and Demis Hassabis.\nModel-free episodic control.\narXiv preprint\narXiv:1606.04460, 2016.\nPetros Christodoulou. Soft actor-critic for discrete action settings. arXiv preprint arXiv:1910.07207,\n2019.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nJeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\nNeural turing machines.\narXiv preprint\narXiv:1410.5401, 2014.\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi´nska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,\net al. Hybrid computing using a neural network with dynamic external memory. Nature, 538\n(7626):471–476, 2016.\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv\npreprint arXiv:2312.00752, 2023.\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R´e. Hippo: Recurrent memory\nwith optimal polynomial projections. Advances in neural information processing systems, 33:\n1474–1487, 2020.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International confer-\nence on machine learning, pages 1861–1870. PMLR, 2018.\nDonald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology\npress, 2005.\n11\nS Hochreiter. Long short-term memory. Neural Computation MIT-Press, 1997.\nChia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale,\nArun Ahuja, and Greg Wayne. Optimizing agent behavior over long time scales by transporting\nvalue. Nature communications, 10(1):5223, 2019.\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra.\nPlanning and acting in\npartially observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are\nrnns: Fast autoregressive transformers with linear attention. In International conference on ma-\nchine learning, pages 5156–5165. PMLR, 2020.\nTeuvo Kohonen and Matti Ruohonen. Representation of associated data by matrix operators. IEEE\nTransactions on Computers, 100(7):701–702, 1973.\nHung Le, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory. In International\nconference on machine learning, pages 5682–5691. PMLR, 2020.\nHung Le, Thommen Karimpanal George, Majid Abdolshah, Truyen Tran, and Svetha Venkatesh.\nModel-based episodic memory induces dynamic hybrid controls. Advances in Neural Information\nProcessing Systems, 34:30313–30325, 2021.\nHung Le, Majid Abdolshah, Thommen K George, Kien Do, Dung Nguyen, and Svetha Venkatesh.\nEpisodic policy gradient training. In Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 36, pages 7317–7325, 2022a.\nHung Le, Thommen Karimpanal George, Majid Abdolshah, Dung Nguyen, Kien Do, Sunil Gupta,\nand Svetha Venkatesh. Learning to constrain policy optimization with virtual trust region. Ad-\nvances in Neural Information Processing Systems, 35:12775–12786, 2022b.\nHung Le, Kien Do, Dung Nguyen, and Svetha Venkatesh. Beyond surprise: Improving exploration\nthrough surprise novelty. In Proceedings of the 23rd International Conference on Autonomous\nAgents and Multiagent Systems, pages 1084–1092, 2024.\nChris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and\nFeryal Behbahani. Structured state space models for in-context reinforcement learning. Advances\nin Neural Information Processing Systems, 36, 2024.\nDavid Marr and W Thomas Thach. A theory of cerebellar cortex. In From the Retina to the Neocor-\ntex, pages 11–50. Springer, 1991.\nSteven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok. Popgym:\nBenchmarking partially observable reinforcement learning. arXiv preprint arXiv:2303.01859,\n2023.\nSteven Morad, Ryan Kortvelesy, Stephan Liwicki, and Amanda Prorok. Reinforcement learning\nwith fast and forgetful memory. Advances in Neural Information Processing Systems, 36, 2024.\nTianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent model-free rl can be a\nstrong baseline for many pomdps.\nIn International Conference on Machine Learning, pages\n16691–16723. PMLR, 2022.\nTianwei Ni, Michel Ma, Benjamin Eysenbach, and Pierre-Luc Bacon. When do transformers shine\nin rl? decoupling memory from credit assignment. Advances in Neural Information Processing\nSystems, 36, 2024.\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,\nMax Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers\nfor reinforcement learning. In International conference on machine learning, pages 7487–7498.\nPMLR, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n12\nMohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar. Mastering\nmemory tasks with world models. arXiv preprint arXiv:2403.04253, 2024.\nImanol Schlag, Kazuki Irie, and J¨urgen Schmidhuber. Linear transformers are secretly fast weight\nprogrammers.\nIn International Conference on Machine Learning, pages 9355–9366. PMLR,\n2021.\nJ¨urgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to\nlearn: the meta-meta-... hook. PhD thesis, Technische Universit¨at M¨unchen, 1987.\nJ¨urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent\nnetworks. Neural Computation, 4(1):131–139, 1992.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nSebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to\nlearn, pages 3–17. Springer, 1998.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing\nSystems, 2017. URL https://api.semanticscholar.org/CorpusID:13756489.\nGreg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska-\nBarwinska, Jack Rae, Piotr Mirowski, Joel Z Leibo, Adam Santoro, et al. Unsupervised predictive\nmemory in a goal-directed agent. arXiv preprint arXiv:1803.10760, 2018.\n13\nAppendix\nA\nTheoretical Results\nA.1\nClosed-form Memory Update\nGiven\nMt = Mt−1 ⊙Ct + Ut\n(15)\nthen\nMt = M0\ntY\ni=1\nCi +\nt\nX\ni=1\nUi ⊙\ntY\nj=i+1\nCj\n(16)\nProof. We prove by induction.\nBase case: for t = 1, the equation becomes for both Eqs. 15 and 16:\nM1 = M0 · C1 + U1\nThus, the equation holds for t = 1.\nInductive hypothesis: Assume the equation holds for t = n:\nMn = M0\nn\nY\ni=1\nCi +\nn\nX\ni=1\nUi ⊙\nn\nY\nj=i+1\nCj\nInductive step: We now prove the equation holds for t = n + 1. From the update rule in Eq. 15:\nMn+1 = Mn ⊙Cn+1 + Un+1\nSubstituteMn using the inductive hypothesis:\nMn+1 =\n\nM0\nn\nY\ni=1\nCi +\nn\nX\ni=1\nUi ⊙\nn\nY\nj=i+1\nCj\n\n⊙Cn+1 + Un+1\n= M0\nn+1\nY\ni=1\nCi +\nn\nX\ni=1\nUi ⊙\nn+1\nY\nj=i+1\nCj + Un+1\n= M0\nn+1\nY\ni=1\nCi +\nn+1\nX\ni=1\nUi ⊙\nn+1\nY\nj=i+1\nCj\nThis matches the form of the closed-form Eq. 16 for t = n + 1, completing the proof by induction.\nA.2\nProposition 3\nDefinition 6. Critical Memory Gradients: In the Hadamard Memory Framework, we define the\ncritical memory gradients of memory rules as follows:\nG1 (i, t, θ) =\n∂Qt\nj=i+1 Cθ(xj)\n∂θ\n=\nt\nX\nj=i+1\n∂Cθ (xj)\n∂θ\n⊙\ntY\nk=i+1,k̸=j\nCθ(xk)\nG2 (i, t, θ) =\ntY\nj=i+1\nCθ(xj)\nNow we can proceed to the proof for Proposition 3.\n14\nProof. In the case Cθ (xt) = θ ∈RH×H, the critical gradients read:\nG1 (i, t, θ) =\nt\nX\nj=i+1\ntY\nk=i+1,k̸=j\nθ\n= (t −i) θt−i−1\nG2 (i, t, θ) = θt−i\nIf ∃0 ≤m, k < H s.t. |θ [m, k]| > 1, G1,2 (i, t, θ) [m, k] ∼o\n\u0010\nθ [m, k]t−i−1\u0011\n, and thus →∞, i.e.,\nnumerical problem arises. Let ∥·∥denote the infinity norm, we also have:\n∥G1 (i, t, θ)∥=\n\r\r(t −i) θt−i−1\r\r ≤(t −i) ∥θ∥t−i−1\n∥G2 (i, t, θ)∥=\n\r\rθt−i\r\r ≤∥θ∥t−i\nNote that if ∀m, k |θ [m, k]| < 1 , ∥θ∥< 1, both terms become 0 as t −i increases, thus learning\nproblem always arises. In conclusion, in this case, to avoid both numerical and learning problems,\n∀m, k |θ [m, k]| = 1, which is not optimal in general.\nA.3\nProposition 4\nProof. Let zm,k\nt\n= um\nt vk\nt where zm,k\nt\n= (θt ⊗vc (xt)) [m, k], um\nt\n= θt [m] and vk\nt = vc (xt) [k].\nUsing assumption (i) and (ii), vk\nt is a Gaussian variable, i.e., vk\nt ∼N\n\u00000, µk\nt\n\u0001\n. By definition, um\nt is\na categorical random variable that can take values {θ [m, lt]}L\nlt=1 with equal probability 1/L. For\nnow, we drop the subscripts m, k and t for notation ease. The PDF of z = uv can be expressed\nas a mixture distribution since u is categorical and can take discrete values {ul}L\nl=1. The PDF of z,\ndenoted as f (z), is given by:\nf(z) =\nL\nX\nl=1\nP(u = ul)fulv(z)\n= 1\nL\nL\nX\nl=1\nfulv(z)\nwhere fulv(z) is the PDF of ulv, and ul is a constant for each l. Thus, fulv(z) is the scaled PDF of\nu:\nfulv(z) =\n1\n|ul|fv\n\u0012 z\nul\n\u0013\nSince v ∼N (0, µ), the PDF of v, denoted as fv(x), is symmetric about 0, we have:\nfulv(z) =\n1\n|ul|fv\n\u0012 z\nul\n\u0013\n=\n1\n|ul|fv\n\u0012−z\nul\n\u0013\n= fulv(−z).\nThis shows that fulv(z) is symmetric around 0 for each l. Therefore, the PDF f(z) is also symmet-\nric:\nf(z) = 1\nL\nL\nX\nl=1\nfulv(z) = 1\nL\nL\nX\nl=1\nfulv(−z) = f(−z)\nSince tanh is an odd function and the PDF of zm,k is symmetric about 0, E\n\u0002\ntanh\n\u0000zm,k\u0001\u0003\n= 0 and\nthus E\n\u0002\n1 + tanh\n\u0000zm,k\u0001\u0003\n= 1. Finally, using assumption (iii), E\nhQT\nt=1 zm,k\nt\ni\n= QT\nt=1 E\nh\nzm,k\nt\ni\n=\n1.\n15\nTask\nInput type\nPolicy/Value networks\nRL algorithm\nBatch size\nMeta-RL\nVector\n3-layer FFW + 1 Memory\nSAC\n32\nlayer (128, 128, 128, H)\nCredit\nImage\n2-layer CNN + 2-layer FFW\nSAC-D\n32\nAssignment\n+ 1 Memory layer (128, 128, H)\nPOPGym\nVector\n3-layer FFW + 1 Memory\nPPO\n65,536\nlayer (128, 64, H, 64)\nTable 2: Network architecture shared across memory baselines.\nA.4\nProposition 5\nProof. Without loss of generalization, we can drop the indice m and k for notation ease. Since\nθt = θ [lt] ; lt ∼U (1, L), it is reasonable to assume that each of {ut, ut′}, {ut, vt}, {ut, vt′},\n{ut′, vt}, {ut′, vt′} are independent. In this case, let us denote X = utvt and X′ = ut′vt′, we have\nCov(X, X′) = Cov(utvt, ut′vt′)\n= E[utvt · ut′vt′] −E[utvt] · E[ut′vt′]\n= E(utvt)E(ut′vt′) −E(ut)E(vt)E(ut′)E(vt′)\n= E(ut)E(vt)[E(ut′vt′) −E(ut′)E(vt′)]\n= E[ut]E[ut′]Cov(vt, vt′)\nThe variances read:\nVar(X) = Var(utvt) = E[u2\ntv2\nt ] −E[utvt]2 = E[u2\nt]\n\u0000E[v2\nt ] −E[vt]2\u0001\n= E[u2\nt] · Var(vt)\nSimilarly:\nVar(X′) = Var(ut′vt′) = E[u2\nt′] · Var(vt′)\nGiven ρ as the Pearson Correlation Coefficient, consider the ratio:\n|ρ(X, X′)|\n|ρ(vt, vt′)| =\n|E[ut]E[ut′]Cov(vt,vt′)|\n√\nE[u2\nt ]·Var(vt)·E[u2\nt′]·Var(vt′)\n|Cov(vt,vt′)|\n√\nVar(vt)·Var(vt′)\n= |E[ut]E[ut′]|\np\nE[u2\nt]E[u2\nt′]\nBy the independence of ut and ut′ and the Cauchy-Schwarz inequality:\n|E[ut]E[ut′]| = |E[utut′]| ≤\nq\nE[u2\nt]E[u2\nt′],\nwhich implies\n|ρ(utvt, ut′vt′)|\n|ρ(vt, vt′)|\n≤1 ⇐⇒|ρ (utvt, ut′vt′)| ≤|ρ (vt, vt′)|\nThe equality holds when ut = βut′.\nB\nDetails on Methodology\nIn this section, we describe RL frameworks used across experiments, which are adopted exactly\nfrom the provided benchmark. Table 2 summarizes the main configurations. Further details can be\nfound in the benchmark papers [Ni et al., 2022, Morad et al., 2023].\n16\nAlgorithm 1: Theoretical Parallel Hadamard Memory Framework.\nInput: M0 ∈RB×H×H, C ∈RB×T ×H×H, U ∈RB×T ×H×H\nOperator: ⊕parallel prefix sum, ⊗parallel prefix product, ⊙Hadamard product\nOuput: M = {M}n\nt=1 ∈RB×T ×H×H\n/* Parallel prefix product along T. Complexity:\nO(log(t)).\n*/\n1 Cp = ⊗(C, dim = 1)\n/* Concatenation along T. Complexity:\nO(1).\n*/\n2 D = concat([M0, C] , dim = 1)\n/* Parallel prefix product along T. Complexity:\nO(log(t)).\n*/\n3 Dp = ⊗(D, dim = 1)\n/* Parallel Hadamard product (Cp ̸= 0).\nComplexity:\nO(1).\n*/\n4 E = U ⊙\n1\nCp\n/* Parallel prefix sum along T. Complexity:\nO(log(t)).\n*/\n5 Ep = ⊕(E, dim = 1)\n/* Parallel sum.\nComplexity:\nO(1).\n*/\n6 M = Dp[:, 1 :] + Ep\nTask\nURL\nLicense\nMeta-RL\nhttps://github.com/twni2016/pomdp-baselines\nMIT\nCredit Assignment\nhttps://github.com/twni2016/Memory-RL\nMIT\nPOP-Gym\nhttps://github.com/proroklab/popgym\nMIT\nTable 3: Benchmark repositories used in our paper.\nC\nDetails of Experiments\nWe adopt public benchmark repositories to conduct our experiments. The detail is given in Table 3\nC.1\nSample-Efficiency in Meta Reinforcement Learning and Long-term Credit Assignment\nDetails\nWe report the choice of memory hyperparameter H in Table. 4. Due to the lack of efficient parallel\nprocessing in the codebase from Ni et al. [2022], running experiments with larger memory sizes is\nprohibitively slow. As a result, we were only able to test models with 512 memory elements, limit-\ning the potential performance of our SHM. This constraint contrasts with the POPGym benchmark\nwith better parallel processing, where our method scales comfortably with larger memory sizes, as\ndemonstrated later in C.2.\nC.2\nPOPGym Hardest Games Details\nIn this experiment, we tuned the memory model size to ensure optimal performance. Specifically, for\nGRU, we tested hidden sizes of 256, 512, 1024, and 2048 on the Autoencode-Easy task. Increasing\nGRU’s hidden size did not lead to performance gains but resulted in a significant rise in computation\ncost. For instance, at H = 2048, the average batch inference time was 30 milliseconds, compared\nto 1.6 milliseconds at H = 256. Thus, we set GRU’s hidden size to 256, as recommended by the\nPOPGym documentation. For FFM, we set the context size to c = 4, as the authors suggest that\na larger context size degrades performance. We tuned the trace size m ∈{32, 128, 256, 512} on\nthe Autoencode-Easy task and found that m = 128 provided slightly better results, so we used this\nModel\nGRU\nFWP\nGPT-2∗\nS6\nmLSTM\nFFM\nSHM (Ours)\nH\n512\n24\n512\n512\n24\n128\n24\nMemory elements\n512\n512\n512 × T\n512\n512\n512\n512\nTable 4: Memory dimension for Meta-RL and Credit Assignment tasks. GRU and S6 use vector\nmemory of size H. GPT-2 does not have a fixed size memory and attend to all previous T timesteps\nin the episode. H = 512 is the dimension of Transformer’s hidden layer. FFM’s memory shape is\n2 × m × c where c = 4, H = m = 128. FWP and SHM’s memory shape is H × H.\n17\nFigure 4: POPGym learning curves: Mean ± std. over 3 runs.\nvalue for all experiments. For our method, we tested H ∈{24, 72, 128, 156} on the same task and\nobserved that larger values of H led to better performance, as shown in the ablation study in Sec.\n4.4. However, to balance runtime, memory cost, and performance, we set H = 128 for all POPGym\nexperiments. The learning curves are given in Fig. 4.\n18\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2024-10-14",
  "updated": "2024-10-14"
}