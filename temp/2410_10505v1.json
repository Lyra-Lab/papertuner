{
  "id": "http://arxiv.org/abs/2410.10505v1",
  "title": "Comparison of deep learning and conventional methods for disease onset prediction",
  "authors": [
    "Luis H. John",
    "Chungsoo Kim",
    "Jan A. Kors",
    "Junhyuk Chang",
    "Hannah Morgan-Cooper",
    "Priya Desai",
    "Chao Pang",
    "Peter R. Rijnbeek",
    "Jenna M. Reps",
    "Egill A. Fridgeirsson"
  ],
  "abstract": "Background: Conventional prediction methods such as logistic regression and\ngradient boosting have been widely utilized for disease onset prediction for\ntheir reliability and interpretability. Deep learning methods promise enhanced\nprediction performance by extracting complex patterns from clinical data, but\nface challenges like data sparsity and high dimensionality.\n  Methods: This study compares conventional and deep learning approaches to\npredict lung cancer, dementia, and bipolar disorder using observational data\nfrom eleven databases from North America, Europe, and Asia. Models were\ndeveloped using logistic regression, gradient boosting, ResNet, and\nTransformer, and validated both internally and externally across the data\nsources. Discrimination performance was assessed using AUROC, and calibration\nwas evaluated using Eavg.\n  Findings: Across 11 datasets, conventional methods generally outperformed\ndeep learning methods in terms of discrimination performance, particularly\nduring external validation, highlighting their better transportability.\nLearning curves suggest that deep learning models require substantially larger\ndatasets to reach the same performance levels as conventional methods.\nCalibration performance was also better for conventional methods, with ResNet\nshowing the poorest calibration.\n  Interpretation: Despite the potential of deep learning models to capture\ncomplex patterns in structured observational healthcare data, conventional\nmodels remain highly competitive for disease onset prediction, especially in\nscenarios involving smaller datasets and if lengthy training times need to be\navoided. The study underscores the need for future research focused on\noptimizing deep learning models to handle the sparsity, high dimensionality,\nand heterogeneity inherent in healthcare datasets, and find new strategies to\nexploit the full capabilities of deep learning methods.",
  "text": " \n \n1 \nComparison of deep learning and conventional methods for disease onset \nprediction \nLuis H. John, MSc1, Chungsoo Kim, PhD2, Jan A. Kors, PhD1, Junhyuk Chang, PharmD3, Hannah Morgan-\nCooper, MSc4, Priya Desai, MSc4, Chao Pang, PhD5, Peter R. Rijnbeek, PhD1, Jenna M. Reps, PhD1,6, Egill A. \nFridgeirsson, PhD1 \n1Department of Medical Informatics, Erasmus University Medical Center, Rotterdam, The Netherlands \n2Section of Cardiovascular Medicine, Department of Internal Medicine, Yale School of Medicine, New \nHaven, CT, United States \n3Department of Biomedical Informatics, Ajou University Graduate School of Medicine, Suwon, Republic \nof Korea \n4Stanford School of Medicine and Stanford Health Care, Palo Alto, CA, United States \n5Department of Biomedical Informatics, Columbia University Irving Medical Center, New York, NY, \nUnited States \n6Janssen Research and Development, Titusville, NJ, United States \nContact Information \nLuis H. John (corresponding author): l.john@erasmusmc.nl; +31 10 704 44 81; Dr. Molewaterplein 40, \n3015 GD Rotterdam, The Netherlands \nChungsoo Kim: chungsoo@ohdsi.org \nJan A. Kors: j.kors@erasmusmc.nl \nJunhyuk Chang: wkd9504@ajou.ac.kr \nHannah Morgan-Cooper: hannahmc@stanford.edu \nPriya Desai: prd@stanford.edu \nChao Pang: cp3016@cumc.columbia.edu \nPeter R. Rijnbeek: p.rijnbeek@erasmusmc.nl \nJenna M. Reps: jreps@its.jnj.com \nEgill A. Fridgeirsson: e.fridgeirsson@erasmusmc.nl \nResearch in context \nEvidence before this study \nConventional methods like logistic regression and gradient boosting have been widely used \nfor their robustness and interpretability in disease onset prediction. Deep learning methods \nhave faced challenges with structured observational healthcare data due to data sparsity and \nhigh dimensionality. Recently, adaptions of ResNet and Transformer models have become \navailable, leading to renewed interest in their application to tabular data. \nAdded value of this study \nThis study is the first to perform a large-scale evaluation of deep learning and conventional \nprediction methods across 11 diverse international databases. The focus is on external \n \n \n2 \nvalidation to evaluate model transportability, which is an aspect not commonly investigated \nfor deep learning. By focusing on clinically relevant outcomes including lung cancer, \ndementia, and bipolar disorder, this study provides generalizable insights. It highlights the \ncomparative discrimination and calibration performance, and scalability of these methods, \nemphasizing the strengths and limitations of each method for disease onset prediction. \nImplications of all the available evidence \nOur findings suggest that conventional models like logistic regression and gradient boosting \nremain highly effective, particularly with smaller datasets and during external validation. This \nsuggests these methods should continue to be used in clinical practice for their reliability and \nease of use. While deep learning shows potential, more research and development are \nneeded to optimize these models for structured observational data. Future work should also \nfocus on finding new strategies to exploit the full capabilities of deep learning methods for \ndisease onset prediction. \nSummary \nBackground: Conventional prediction methods such as logistic regression and gradient boosting have \nbeen widely utilized for disease onset prediction for their reliability and interpretability. Deep learning \nmethods promise enhanced prediction performance by extracting complex patterns from clinical data, \nbut face challenges like data sparsity and high dimensionality. \nMethods: This study compares conventional and deep learning approaches to predict lung cancer, \ndementia, and bipolar disorder using observational data from eleven databases from North America, \nEurope, and Asia. Models were developed using logistic regression, gradient boosting, ResNet, and \nTransformer, and validated both internally and externally across the data sources. Discrimination \nperformance was assessed using the area under the receiver operating characteristic curve (AUROC), \nand calibration was evaluated using Eavg. \nFindings: Across eleven datasets, conventional methods (logistic regression and XGBoost) generally \noutperformed deep learning methods (ResNet and Transformer) in terms of discrimination \nperformance, particularly during external validation, highlighting their better transportability. Learning \ncurve analysis suggested that deep learning models require substantially larger datasets to reach the \nsame performance levels as conventional methods. Calibration performance was also better for \nconventional methods, with ResNet showing the poorest calibration among all models. \nInterpretation: Despite the potential of deep learning models to capture complex patterns in structured \nobservational healthcare data, conventional models remain highly competitive for disease onset \nprediction, especially in scenarios involving smaller datasets and if lengthy training times need to be \navoided. The study underscores the need for future research focused on optimizing deep learning \nmodels to handle the sparsity, high dimensionality, and heterogeneity inherent in healthcare datasets, \nand find new strategies to exploit the full capabilities of deep learning methods. \nFunding: This project has received funding from the Innovative Medicines Initiative 2 Joint Undertaking \n(JU) under grant agreement No. 806968. The JU receives support from the European Union’s Horizon \n2020 research and innovation programme and EFPIA. \n1 Introduction \nIdentifying individuals at high risk of disease at an early stage allows for improved care and risk-factor \n \n \n3 \ntargeted intervention. Prognostic models can provide such risk estimates, aiding healthcare providers in \nmaking informed clinical decisions which can ultimately improve patient outcomes.(1, 2) \nConventional approaches such as logistic regression and gradient boosting have long served as reliable \ntools for predictive modeling in the clinical domain.(3-5) For example, Framingham Risk Scores quantify \nthe risk of developing cardiovascular conditions, including coronary heart disease, stroke, and heart \nfailure over a period of ten years.(6) However, the continuous advancement of deep learning methods \noffers the promise of improved prediction accuracy and the ability to extract intricate patterns from \ncomplex clinical data.(7) \nWhile the impact of deep learning has been primarily observed in domains that involve unstructured \ndata, such as imaging and natural language processing, its application to structured observational \nhealthcare data has been met with challenges.(7-9) Sparsity, high dimensionality, and heterogeneity \nhave been identified as inherent characteristics of observational healthcare data that limit the efficacy \nof deep learning methods.(7, 10-12) As a result, conventional prediction methods, such as logistic \nregression, continue to achieve comparable performance to deep learning methods, despite the \ncomplexity of the latter.(7) \nHowever, recent work in deep learning revisits the application to structured tabular data, yielding \npromising new approaches.(13-15) This study presents a comparison of conventional and deep learning \nmethods to predict three clinically relevant health outcomes using observational healthcare data. \nDefinitions of health outcomes are taken from published clinical articles and include dementia in \npersons aged 55 – 84, bipolar disorder in patients newly diagnosed with major depressive disorder, and \nlung cancer in patients aged 45 – 65 who have been cancer-free.(16-18) Moreover, transportability of \nthe models is evaluated across a large network of observational databases through external validation, \nan aspect not widely studied for deep learning. \n2 Methods \n2.1 Source of data \nThis retrospective study uses structured observational healthcare data from administrative claims and \nelectronic health records (EHR) from the Observational Health Data Sciences and Informatics (OHDSI) \ncollaborative network. Table 1 presents the eleven longitudinal observational databases that were \nincluded in this study. The databases include data from the United States, Europe, and Asia-Pacific \nregions, covering primary and tertiary care center and ranging in population from fewer than 3 million \nto more than 170 million person records. Further details about the databases are presented in the \nappendix (pp 2–4). Databases were mapped to the Observational Medical Outcomes Partnership \nCommon Data Model (OMOP CDM).(19) The OMOP CDM provides a standardized data structure and \nvocabulary, which facilitates sharing and execution of analysis packages across data sites. \nTable 1. Data sources that are in part used for model development and all used for external validation. \nDatabase \nAcronym \nPerson \ncount \nUsage  \nCountry \nData \ntype \nTime \nperiod \nIntegrated Primary Care Information \n(version N) \nIPCI \n2·7M Development, \nValidation \nNetherlands \nEHR \n(GP) \n01/2006 – \n12/2022 \nAjou University School of Medicine \n(version 535) \nAUSOM \n2·7M Development, \nValidation \nSouth Korea \nEHR \n01/1994 – \n02/2023 \n \n \n4 \nOptum® de-identified Electronic Health \nRecord dataset (version 2541) \nOptum EHR \n111·4M Development, \nValidation \nUnited \nStates \nEHR \n01/2007 – \n12/2022 \nOptum’s de-identifed Clinformatics® Data \nMart Database (version 2559) \nClinformatics \n97·3M Development, \nValidation \nUnited \nStates \nClaims \n05/2000 – \n03/2023 \nStanford Medicine Research Data \nRepository OMOP (version 24_03_21) \nSTARR-\nOMOP \n3·9M Development, \nValidation \nUnited \nStates \nEHR \n2008 – \n2024 \nColumbia University Irving Medical Center \n(version 2023q4r2) \nCUIMC \n6·1M Development, \nValidation \nUnited \nStates \nEHR \n1985 – \n2023 \nIQVIA® Disease Analyzer Germany (version \n2784) \nGerman DA \n32·8M Validation \nGermany \nEHR \n(GP) \n10/2013 – \n09/2023 \nJapan Medical Data Center (version 2906) \nJMDC \n17·7M Validation \nJapan \nClaims \n01/2005 – \n06/2023 \nMerative® MarketScan® Multi-State \nMedicaid (version 2888) \nMDCD \n36·0M Validation \nUnited \nStates \nClaims \n01/2000 – \n01/2024 \nMerative® MarketScan® Medicare \nSupplemental Database (version 2886) \nMDCR \n11·3M Validation \nUnited \nStates \nClaims \n01/2000 – \n01/2024 \nMerative® MarketScan® Commercial \nClaims and Encounters Database (version \n2887) \nCCAE \n172·3M Validation \nUnited \nStates \nClaims \n01/2000 – \n01/2024 \nEHR: electronic health records \nGP: general practitioner data included \n2.2 Prediction problems \nA patient-level prediction model quantifies a person’s risk of developing a health outcome during a \nspecified time-at-risk period following an index date, using information collected in an observation \nwindow prior to index (Figure 1). Health outcomes of interest include: \n• \nDementia: A progressive neurodegenerative disorder characterized by declining cognitive \nfunction, including memory loss, impaired thinking, and behavioral changes. Early identification \nof dementia can facilitate timely intervention, potentially slowing disease progression and \nimproving quality of life.(20) Dementia is predicted in a target cohort of persons aged 55 – 84 \nduring a time-at-risk of 1,825 days as defined in a published clinical article.(18) \n• \nBipolar disorder: A mental health condition marked by recurring episodes of mania or \nhypomania and depression, significantly impacting mood, energy, and activity levels. A \nconsiderable fraction of major depressive disorder patients later receive a corrected diagnosis of \nbipolar disorder, making early prediction important for effective management.(21-24) The \ntransition to bipolar disorder is predicted in a target cohort of persons diagnosed with major \ndepressive disorder during a time-at-risk of 365 days as defined in a published clinical \narticle.(16) \n• \nLung cancer: The leading cause of cancer mortality in the United States, with a 5-year survival \nrate of only 22% due to late-stage diagnosis.(25) Despite the proven benefits of screening, \nuptake is low, and many patients diagnosed with lung cancer do not meet current screening \ncriteria.(26-28) Early detection of lung cancer can significantly improve treatment outcomes and \nsurvival rates. Lung cancer is predicted in a target cohort of persons aged 45 – 65 during a time-\nat-risk of 1,095 days as defined in a published clinical article.(17) \n \n \n5 \n \nFigure 1. Onset prediction time windows for a person in a study population.(29) \nFor all prediction tasks, a visit record that marks an interaction with a healthcare provider, serves as the \nindex date, allowing for the practical application of the model. To use recent data, but at the same time \neliminate pandemic-related confounding effects on healthcare systems and patient behavior, we chose \nto utilize pre-COVID data right before the pandemic (before 1 January 2020). Given the respective time-\nat-risk periods this means that the index date for dementia falls into the year of 1 Jan 2014 – 31 Dec \n2014, for lung cancer into the year of 1 Jan 2016 – 31 Dec 2016, and for bipolar into the years from 1 Jan \n2017 – 31 Dec 2019. \nParticipants require 365 days of continuous observation time before the index date (excluding the index \ndate), in which candidate predictors are assessed (Figure 1). This relatively short period is consistent \nwith other models in literature and, as opposed to all-time lookback, was also found to have only small \nimpact on discrimination and calibration as all-time lookback can vary strongly across patients.(30) As \ncandidate predictors, we use patient’s age, sex, and Charlson Comorbidity Index at the index date. \nDuring the observation window we use dichotomized diagnoses and drug prescriptions. Even though \nthis information is recorded at multiple time points, for analysis purposes it is flattened into a tabular \nformat; a necessary step for prediction methods not designed to handle sequence data. Additionally, \nfollowing empirical recommendations on handling patients lost to follow-up, the study allows \nparticipants to exit the cohorts at any time during the time-at-risk period, provided they have at least \none day of time-at-risk after the index date.(31) \n2.3 Prediction methods \nClinical predictive modeling has undergone significant progress over recent decades, evolving from \ntraditional statistical methods to modern machine learning techniques.(3, 5) One of the earliest well-\nknown models is the Framingham Risk Score, developed in the late 1990s using logistic regression to \npredict ten-year cardiovascular risk.(6) Logistic regression has been widely adopted due to its simplicity, \nrobustness, and interpretability, predicting outcomes by applying a logistic function to a weighted sum \nof input features.(4, 32) Incorporating L1 regularization enabled effective feature selection and further \nimproved the models’ scalability on large datasets, making it optimal for clinical prediction on \nobservational data.(33) \nMore recently, gradient boosting strategies have demonstrated high efficacy in handling feature \ninteractions and missing values by sequentially combining weak learners, typically decision trees, to \nminimize errors.(34) Gradient boosting has gained widespread popularity for structured data and is \nextensively being used in machine learning competitions.(14) Notable implementations include Extreme \nGradient Boosting (XGBoost), which for example has been used in a framework for predicting smoking-\ninduced noncommunicable diseases, and in models predicting the mortality of patients with acute \nkidney injury in intensive care.(35-37) \n \n \n6 \nIn contrast to these conventional approaches, neural networks, which serve as the foundation for deep \nlearning, were conceptualized as early as 1958 with the introduction of the perceptron.(38) The field \nadvanced significantly in 1986 with the backpropagation algorithm being introduced, enabling effective \ntraining of Multi-Layer Perceptrons (MLPs).(39) Residual Networks (ResNet) further addressed \nchallenges such as the vanishing gradient by incorporating residual connections, allowing for \nincreasingly deeper architectures.(40) ResNets have demonstrated diagnostic performance equivalent \nto that of health-care professionals in detecting disease from medical imaging.(9) The transformer \nrepresents another significant leap in deep learning architecture, particularly for natural language \ntasks.(41-44) It relies on self-attention mechanisms, enabling efficient parallel processing and improved \nhandling of long-range dependencies.(45) \nAs the focus shifts back to tabular data, adaptions of the ResNet and Transformer have become \navailable, potentially more suitable for clinical prediction on observational healthcare data.(14) We \ncompare logistic regression, gradient boosting (XGBoost), ResNet, and Transformer to evaluate their \nefficacy for disease onset prediction across eleven observational databases. (4, 14, 35) The comparison \nincludes internal validation, external validation, and a learning curve analysis to assess scalability with \nincreasing data set size.(46) Model parameters and hyperparameter space for training conventional and \ndeep learning methods are presented in the appendix (p 4). For deep learning methods, this parameter \nchoice is based on previously established recommendations.(14) For L1 regularized logistic regression \nwe use an adaptive search method to automatically tune the degree of regularization.(4) XGBoost uses a \ngrid search in combination with a holdout set to automatically determine its hyperparameters.(2) \n2.4 Statistical analysis \nFor internal validation, each dataset is partitioned into a 75% training set and a 25% test set. Model \nperformance on the test set is assessed using the area under the receiver operating characteristic curve \n(AUROC) for discrimination and the Eavg for calibration.(47) AUROC measures the probability that a \nrandomly selected patient with the outcome will have a higher predicted risk than one without.(48) The \n95% confidence interval (CI) is computed for each AUROC using stratified bootstrapped replicates.(49) \nCalibration is typically visualized through plots showing agreement between predicted and observed risk \nacross deciles. To simplify comparison of calibration across models and data sources, the single value \nmetric Eavg is used, which indicates the average absolute difference between observed and predicted \nprobabilities.(50, 51) For external validation, performance is evaluated on the full external datasets by \napplying the models to persons matching the target cohort definitions in the external data sources. \nWe use Friedman's test to detect ranking differences of the discrimination and calibration performances \nof the different prediction methods.(52) If the null hypothesis for no difference in ranks between the \nmethods is rejected (p-value less than 0·05), we proceed with a post-hoc test to examine all pairwise \ndifferences, controlling for multiplicity.(53) The results are plotted in a critical difference (CD) diagram of \nthe Nemenyi test, which shows the mean ranks of each prediction method. \n2.5 Role of the funding source \nThe funding source of this study had no role in study design, data collection, data analysis, data \ninterpretation, or writing and submission of the report. \n3 Results \nThe analysis includes several steps to develop and validate clinical prediction models, as outlined in \nFigure 2. Data are extracted for the outcomes of dementia, bipolar disorder, and lung cancer from \neleven databases. Logistic regression, gradient boosting, ResNet, and Transformer models are \n \n \n7 \ndeveloped on data from six of these databases and validated across all eleven databases. We evaluate \ndiscrimination using AUROC and calibration using Eavg. \n \nFigure 2. Study overview.(54) \n3.1 Study populations \nFor dementia prediction, all study populations yield sufficient outcomes for prediction (Table 2). \nOutcome rates vary between 1·1% in AUSOM and 4·8% in Clinformatics. The two largest data sets from \nOptum EHR and Clinformatics also provide the largest study populations and most outcomes of \ndementia. The median time-at-risk for persons in the study populations is the full 1,825 days of the \nprediction problem definition, except for Clinformatics with a median time-at-risk of 1,746 days. Female \nrepresentation is notably higher than male representation in Optum EHR, STARR-OMOP, and CUIMC. \nTable 2. Dementia study populations for model development. \n \nIPCI \nAUSOM \nOptum EHR \nClinformatics \nSTARR-OMOP \nCUIMC \nPopulation, n \n186,767 \n54,723 \n7,811,078 \n2,838,303 \n124,045 \n92,610 \nOutcomes, n \n(%) \n3,094 (1·7) \n587 (1·1) \n306,923 (3·9) \n136,018 (4·8) \n2,209 (1·8) \n2,499 (2·7) \nMedian time-\nat-risk, days \n(IQR) \n1,825 (443) \n1,825 (645) \n1,825 (575) \n1,746 (1233) \n1,825 (0) \n1,825 (0) \nSex \nFemale, n (%) \n100,438 (53·8) \n28,359 (51·8) \n4,524,507 \n(57·9) \n1,527,140 \n(53·8) \n72,548 (58·5) \n54,484 (58·8) \nMale, n (%) \n86,329 (46·2) \n26,364 (48·2) \n3,286,571 \n(42·1) \n1,311,163 \n(46·2) \n51,497 (41·5) \n38,117 (41·2) \nTrain\nT\nO\nT\nO\nPrediction problems\n• Dementia in persons\n   aged 55 - 84\n• Bipolar disorder in persons\n   newly diagnosed with\n   major depressive disorder\n• Lung cancer in persons\n   aged 45 - 65\nDatabases\n \n• IPCI\n• AUSOM\n• Optum EHR\n• Clinformatics\n• STARR-OMOP\n• CUIMC\nDatabases\n• IPCI\n• AUSOM\n• Optum EHR\n• Clinformatics\n• STARR-OMOP\n• CUIMC\n• German DA\n• JMDC\n• MDCD\n• MDCR\n• CCAE\nPrediction methods\n• Logistic regression\n• XGBoost\n• Residual network\n• Transformer\nEvaluation metrics\n• Discrimination: Area under\n   the receiver operating\n   characteristic curve\n• Calibration: Average\n   absolute difference\n   between observed and\n   predicted probabilities\nStep 1\nPrediction problem\nDefinition of target-outcome\npairs for onset prediction.\n  T   Target cohort\n O   Outcome cohort\nStep 2\nDatabase extraction\nExtract target and outcome\ncohort from database. Label\nintersection of cohorts as\npersons with the outcome in\nthe target.\nStep 3\nModel development\nPartition data into training\nand test set. Develop models\nfor various prediction\nmethods on training set.\nStep 4\nInternal validation\nEvaluate discrimination and\ncalibration performance of\nmodels on test set.\nStep 5\nExternal validation\nEvaluate discrimination and\ncalibration performance of\nmodels on external data\nsources.\nTraining set\nTest set\n \n \n8 \nFor lung cancer prediction, outcome rates are substantially lower despite large study populations (Table \n3). The median time-at-risk for persons in the study populations was the full 1,095 days, except for \nClinformatics with a median time-at-risk of 916 days. Female representation is notably higher than male \nrepresentation in Optum EHR, STARR-OMOP and CUIMC. \nTable 3. Lung cancer study population for model development. \n \nIPCI \nAUSOM \nOptum EHR \nClinformatics \nSTARR-OMOP \nCUIMC \nPopulation \n249,294 \n77,256 \n5,987,928 \n1,435,988 \n92,346 \n45,765 \nOutcomes, n \n(%) \n520 (0·2) \n41 (0·1) \n5,937 (0·1) \n1,334 (0·1) \n92 (0·1) \n21 (0·0) \nMedian time-\nat-risk (IQR) \n1,095 (0) \n1,095 (0) \n1,095 (315) \n916 (751) \n1,095 (0) \n1095 (0) \nSex \nFemale, n (%) \n134,228 (53·8) \n35,992 (46·6) \n3,537,268 \n(59·1) \n738,731 (51·4) \n59,923 (64·9) \n28,590 (62·5) \nMale, n (%) \n115,066 (46·2) \n41,264 (53·4) \n2,450,660 \n(40·9) \n697,257 (48·6) \n32,420 (35·1) \n17,171 (37·5) \n \nFor bipolar disorder prediction, the largest study populations are again seen in the largest databases, \nOptum EHR and Clinformatics. The IPCI and AUSOM cohorts were excluded from the study due to having \nfewer outcomes than the minimum cell count required for publication. Median time-at-risk was the full \n365 days for all cohorts. Female representation is notably higher than male representation across the \nremaining study populations. \nTable 4. Bipolar disorder study population for model prediction. \n \nIPCI \nAUSOM \nOptum EHR \nClinformatics \nSTARR-OMOP \nCUIMC \nPopulation \n<5 outcomes \n<5 outcomes \n842,194 \n398,452 \n16,354 \n10,566 \nOutcomes, n \n(%) \n12,133 (1·4) \n4,654 (1·2) \n103 (0·6) \n166 (1·6) \nMedian time-\nat-risk (IQR) \n365 (0) \n365 (34) \n365 (0) \n365 (0) \nSex \nFemale, n (%) \n \n \n541,082 (64·2) \n237,183 (59·5) \n11,226 (68·6) \n7,007 (66·3) \nMale, n (%) \n301,112 (35·8) \n161,269 (40·5) \n5,120 (31·3) \n3,555 (33·6) \n \n3.2 Discrimination performance \nInternal and external discrimination performance for logistic regression, XGBoost, ResNet, and \nTransformer for eleven observational databases is presented in Figure 3. Internal validation \nperformance is assessed on the test set partition, whereas external validation performance is assessed \non the whole data of each respective validation database and statistically summarized. Models generally \nperform better during internal validation. For development datasets with few outcomes, AUROC \nconfidence intervals tend to be wider. Complete discrimination results are presented in the appendix (p \n \n \n9 \n5). \n \nFigure 3. Discrimination performance (AUROC) for internal validation with 95% CI and for external validation across the \nrespective external databases. \nFor discrimination measures across outcomes, databases, internal and external validation, the Friedman \ntest yields a test statistic (Q) of 38·429 with 3 degrees of freedom and rejects the null hypothesis of no \ndifference in ranks (p ≪ 0·0001). This suggests that at least one of the prediction methods differs \nsignificantly from the others in discrimination performance. The CD diagram in Figure 4A reveals that \nXGBoost and Logistic Regression show no significant difference in performance. Similarly, no significant \nperformance difference is observed between Transformer and ResNet. However, conventional models \nhave lower rank compared to deep learning models, indicating a significant performance difference \nbetween the methods. Looking only at internal prediction performance, the Friedman test reveals no \nsignificant difference between the prediction methods (Q(3) = 6·225, P = 0·10), and, thus, no post-hoc \ntest is performed. However, a significant difference between models shows for external validation \nperformance (Q(3) = 34·555, P ≪ 0·0001), and the CD diagram (Figure 4B) reveals the same ranking as \nacross all discrimination measures. Conventional models developed on the smaller datasets of IPCI, \nAUSOM, STARR-OMOP, and CUIMC and validated across all data sources except Clinformatics and \nOptum EHR (Figure 4C) perform better than deep learning models (Q(3) = 40·622, P ≪ 0·0001), while on \nthe larger data sets Clinformatics and Optum EHR conventional and deep learning models perform the \nsame (Q(3) = 1·725, P = 0·63). \n \n \n \n10 \n \nFigure 4. Ranking of prediction method based on discrimination performance (AUROC) for (A) internally and externally \nvalidated models, (B) externally validated models, (C) models developed on small datasets IPCI, AUSOM, STARR-OMOP, and \nCUIMC and validated across all data sources except Optum EHR and Clinformatics. \n3.3 Calibration performance \nCalibration performance of the models is evaluated using the Eavg. Complete calibration results are \npresented in the appendix (p 6). For measures across outcomes, databases, internal and external \nvalidation, the Friedman test yields a test statistic (Q) of 116·79 with 3 degrees of freedom. The p-value \nis much less than 0·0001, indicating a significant result. The CD diagram in Figure 5A reveals that logistic \nregression, XGBoost, and Transformer show better calibration than ResNet. The same does not apply to \ninternal validation performance (Q(3) = 29·325, P ≪ 0·0001) where the ResNet is calibrated equally well \nas the Transformer, but for the external (Q(3) = 95·139, P ≪ 0·0001) validation the ResNet continues to \nget outperformed (Figure 5B and Figure 5C, respectively). When looking only at models that achieve at \nleast 0·7 AUROC discrimination performance there is no significant difference between them (Q(3) = \n4·0621, P = 0·25). For models developed on small datasets IPCI, AUSOM, STARR-OMOP, and CUIMC and \nvalidated across all data sources except Clinformatics and Optum EHR the ResNet is outperformed (Q(3) \n= 118·47, P ≪ 0·0001) as evident in Figure 5D. For models developed and validated on large datasets \nClinformatics and Optum EHR (Figure 5E) there is a significant performance difference observed \nbetween XGBoost and ResNet (Q(3) = 15·409, P = 0·0015). \n \n \n11 \n \nFigure 5. Ranking of prediction method based on calibration performance (Eavg) for (A) internally and externally validated \nmodels, (B) internally validated models, (C) externally validated models, (D) models developed on small datasets IPCI, \nAUSOM, STARR-OMOP, and CUIMC and validated across all data sources except Optum HER and Clinformatics, (E) models \ndeveloped and validated on large datasets Optum EHR and Clinformatics. \n \n3.4 Learning curves \nLearning curves illustrate how prediction performance changes with increasingly larger subsets of the \ntraining data (Figure 6). Previous work has shown that for logistic regression and clinical prediction tasks \nwith imbalanced classes, the outcome count correlates more closely with performance than training set \nsize.(46) Thus, performance is plotted against outcome count rather than subset size. Given the \nextensive training times for deep learning models, we reuse the optimal hyperparameters from the \nmodel trained on the full training set, rather than performing a new search for each subset. \nLearning curves for conventional models require less data to reach the performance plateau. However, \nwith sufficient data performance for all methods appears to converge. ResNet can show a large degree \nof instability as compared to the other methods, which only diminishes with increasing subset size. \n \n \n \n12 \n \nFigure 6. Discrimination performance (AUROC) on the test set for increasingly larger subsets of the training set. \n4 Discussion \nIn this study, we compared the performance of conventional and deep learning methods for disease \nonset prediction using observational healthcare data. Our large-scale analysis included logistic \nregression, XGBoost, ResNet, and Transformer models. We performed internal and external validation \nacross eleven databases in North America, Europe, and Asia, each with different data capture processes \nand population sizes. Evaluation was performed across three clinical prediction problems with \nconsistent evaluation metrics to ensure robustness and generalizability. Our findings highlight the \ncurrent capabilities and limitations of deep learning models in the context of structured observational \nhealthcare data. \nThe study populations reveal considerable differences in cohort size and outcome count, with the \nlargest cohorts observed in Optum EHR and Clinformatics. For bipolar disorder, insufficient data were \navailable for IPCI and AUSOM, which were hence excluded from the analysis. Notably, female \nrepresentation was higher across the EHR data sets from the United States, which for dementia aligns \nwith existing literature indicating a higher prevalence in women, and in general with the gender \ndifference in utilization of healthcare and preventive care services in the United States.(55-57) Median \ntime-at-risk was the full time-at-risk for all data sets, with the exception of Clinformatics which for \ndementia is 1,746 days and for lung cancer 916 days. This indicates that few persons are lost to follow \nup.(31) \nInternal and external discrimination performance was evaluated using AUROC across all prediction \nmethods. The CD plot in Figure 4A shows that overall (internal- and externally), conventional methods \noutperform deep learning methods. This trend is also observed for external validation, indicating limited \ntransportability of the deep learning methods. Interestingly, internal validation shows no statistical \ndifference between methods. Further investigation using learning curves confirms this observation, \nwhere in the dementia dataset with the largest cohort sizes, the performance of all prediction methods \nconverges, with the ResNet being slightly outperformed. Analyses for lung cancer and bipolar disorder \nshowed that the learning curves were still in the upwards trending phase and no plateau was reached \n \n \n13 \ndue to insufficient data. Although learning curves agree with the Friedman test that models perform \nequally on their development data, deep learning models require substantially larger datasets to reach \nequivalent performance levels, explaining their lag in smaller datasets. From the learning curves we \nobserve that ResNet shows greater instability in discrimination performance as compared to the other \nmethods. This may suggest that ResNet is more sensitive to hyperparameter choices, as these were \nderived from the model trained on the full dataset instead of being optimized for each learning curve \nsubset due to lengthy training times. This limitation may also affect the learning curve of the \nTransformer. \nOther notable observations include the absence of several models for which no performance could be \nmeasured, likely due to insufficient data or failure to achieve model convergence, and as a result the \nunavailability of a developed model for external validation. Lack of sufficient data also lead to poor \nmodel performance (AUROC close to 0·50) for several models in the lung cancer and bipolar disorder \ncohorts, whereas dementia results were complete. AUROC values below 0·50 are observed during some \nexternal validations indicating systematic misclassification likely caused by overfitting to insufficient \ntraining data during model development. Another potential reason for poor external validation \nperformance is database heterogeneity. Although, the conversion to the OMOP CDM provides data \nstandardization and harmonization, limitations apply such as incomplete data capture, variable coding \npractices, and differences in patient populations across databases. An example of the latter is dementia \nmodels consistently performing poorly on CCAE, which can be explained by CCAE not providing the \nrequired age range for the target cohort, but only from 55 – 65 years of age. On the other end of the \nperformance range several above average performing models warrant further investigation into \npotential anomalies. For example, the AUSOM Transformer for lung cancer performs well internally with \n0.85 AUROC with wide confidence intervals (0·79 – 0·91) likely driven by a low outcome count of 41, and \nas a result, does not generalize well to the remaining databases. The Optum EHR Transformer for lung \ncancer achieves an AUROC of 0.88 at CUIMC. However, the low outcome count of 21 and wide \nconfidence intervals (0·68 – 1·00) warrant cautious interpretation. \nInternal calibration performance is good across all models (appendix p 5). Calibration on external \ndatabases deteriorates and is worst for dementia, which can be explained by the differences in \ndemographics across databases as age is a driving predictor. Calibration is also found to be excellent for \nmany models that discriminate poorly. We believe that calibration should not be assessed in this case \nand additionally assess calibration for models that achieve AUROC ≥ 0·70. We present this threshold \nwithout specific label, acknowledging that it is somewhat arbitrary and based on digit preference to \ndistinguish from a model with AUROC that predicts not better than chance.(58, 59) Since there is no \nsignificant difference found for this subset of models, we conclude that models that perform above 0·70 \nAUROC are equally well calibrated. \nOther insights from this study include the lengthy training times and advanced hardware requirements, \nsuch as graphics processing units (GPU), for developing deep learning models. Due to computational \nconstraints on cloud infrastructure with high GPU costs, our GPU hours were limited. Consequently, we \nprioritized external validation across more databases, as it is less computationally intensive and more \ncost-effective than model development. These challenges are less pronounced in studies using \nconventional models and highlights the complexities and costs of deep learning studies. If for a study \ndeep learning models are preferred and given ResNet’s poor calibration on small data and instability \nobserved in discrimination-based learning curves, we recommend the use of Transformers if its lengthy \ntraining times are acceptable. However, based on the tested architectures, we currently recommend \nlogistic regression or XGBoost over deep learning methods for disease onset prediction on structured \nobservational data, especially for smaller datasets and when external validation is important. \n \n \n14 \nOur finding establishes a critical baseline and highlights the current limitations of deep learning methods \nwhen applied to structured observational data. These methods are more complex, but do not achieve \nbetter performance than conventional methods. Therefore, it is necessary to better understand the \ncontext in which deep learning may offer advantages. Structured observational data likely does not \nexploit the full capabilities of deep learning methods. Specifically, the removal of temporal information \nto achieve compatibility with conventional prediction methods is unnecessary for a Transformer model \nwhich can effectively use sequence data. Also, the use of potentially more informative features should \nbe considered. For example, measurement data or medical procedures may offer a more comprehensive \npicture of condition severity. Moreover, pre-training on large databases, as demonstrated by models like \nMed-BERT, BEHRT, and CEHR-BERT, has shown considerable promise. Pre-training allows models to \ncapture a wide range of medical concepts and patterns, thereby improving generalizability.(60-62) \nOur comparison reveals that conventional models remain highly competitive for disease onset \nprediction using structured observational data. While deep learning holds the potential to excel in more \nadvanced data contexts, conventional prediction methods continue to offer reliable performance and \ngood transportability especially with smaller datasets. \nContributors \nL.H.J. and E.A.F. lead and C.K. and J.M.R. contributed to the conception and design of the work. L.H.J., \nC.K., J.M.R, and E.A.F prepared the formal analysis. L.H.J., C.K., J.C., H.M.C., C.P. and E.A.F. accessed, \ncurated, and verified the underlying data. J.A.K. and P.R.R. provided critical feedback on methodology. \nL.H.J., C.K., J.C., H.M.C., P.D., C.P., and E.A.F. carried out the formal analysis. L.H.J. took the lead in \nwriting the manuscript. All authors provided critical feedback on the manuscript and helped shape the \nmanuscript. All authors read and approved the final manuscript. \nData sharing \nData dictionaries defining each field in the set are publicly available as part of the Observational Medical \nOutcomes Partnership Common Data Model (https://ohdsi.github.io/CommonDataModel/). Requests \nfor analysis results must be formally addressed to L.H.J. The study repository is publicly available at \nhttps://github.com/ohdsi-studies/DeepLearningComparison and includes the protocol and study \nparameters, as well as source code to generate disseminated results. The source code of the analysis \nframework is publicly available at https://github.com/OHDSI/PatientLevelPrediction and \nhttps://github.com/OHDSI/DeepPatientLevelPrediction. \nDeclaration of interests \nJ.M.R is an employee of Janssen Research & Development and shareholder of Johnson & Johnson. L.H.J., \nJ.A.K., P.R.R., J.M.R., and E.A.F. work for a research group that in the past three years receives/received \nunconditional research grants from Chiesi, UCB, Amgen, Johnson and Johnson, Innovative Medicines \nInitiative and the European Medicines Agency. None of these grants result in a conflict of interest to the \ncontent of this paper. \n5 References \n1. \nAlonzo TA. Clinical prediction models: a practical approach to development, validation, and updating: by \nEwout W. Steyerberg. Oxford University Press; 2009. \n2. \nReps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design and implementation of a standardized \nframework to generate and evaluate patient-level prediction models using observational healthcare data. J Am Med \nInform Assoc. 2018;25(8):969-75. \n3. \nYang C, Kors JA, Ioannou S, John LH, Markus AF, Rekkas A, et al. Trends in the conduct and reporting of \nclinical prediction model development and validation: a systematic review. J Am Med Inform Assoc. 2022;29(5):983-\n \n \n15 \n9. \n4. \nSuchard MA, Simpson SE, Zorych I, Ryan P, Madigan D. Massive parallelization of serial inference algorithms \nfor a complex generalized linear model. ACM Transactions on Modeling and Computer Simulation (TOMACS). \n2013;23(1):1-17. \n5. \nGoldstein BA, Navar AM, Pencina MJ, Ioannidis J. Opportunities and challenges in developing risk prediction \nmodels with electronic health records data: a systematic review. J Am Med Inform Assoc. 2017;24(1):198-208. \n6. \nWilson PW, D’Agostino RB, Levy D, Belanger AM, Silbershatz H, Kannel WB. Prediction of coronary heart \ndisease using risk factor categories. Circulation. 1998;97(18):1837-47. \n7. \nChen D, Liu S, Kingsbury P, Sohn S, Storlie CB, Habermann EB, et al. Deep learning and alternative learning \nstrategies for retrospective real-world clinical data. NPJ digital medicine. 2019;2(1):43. \n8. \nWu S, Roberts K, Datta S, Du J, Ji Z, Si Y, et al. Deep learning in clinical natural language processing: a \nmethodical review. J Am Med Inform Assoc. 2020;27(3):457-70. \n9. \nLiu X, Faes L, Kale AU, Wagner SK, Fu DJ, Bruynseels A, et al. A comparison of deep learning performance \nagainst health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. \nLancet Digit Health. 2019;1(6):e271-e97. \n10. \nWang F, Casalino LP, Khullar D. Deep learning in medicine—promise, progress, and challenges. JAMA \ninternal medicine. 2019;179(3):293-4. \n11. \nXiao C, Choi E, Sun J. Opportunities and challenges in developing deep learning models using electronic \nhealth records data: a systematic review. J Am Med Inform Assoc. 2018;25(10):1419-28. \n12. \nMiotto R, Wang F, Wang S, Jiang X, Dudley JT. Deep learning for healthcare: review, opportunities and \nchallenges. Brief Bioinform. 2018;19(6):1236-46. \n13. \nArik SÖ, Pfister T, editors. Tabnet: Attentive interpretable tabular learning. AAAI; 2021. \n14. \nGorishniy Y, Rubachev I, Khrulkov V, Babenko A. Revisiting deep learning models for tabular data. Adv \nNeural Inf Process Syst. 2021;34:18932-43. \n15. \nRubachev I, Alekberov A, Gorishniy Y, Babenko A. Revisiting pretraining objectives for tabular deep learning. \narXiv preprint arXiv:220703208. 2022. \n16. \nNestsiarovich A, Reps JM, Matheny ME, DuVall SL, Lynch KE, Beaton M, et al. Predictors of diagnostic \ntransition from major depressive disorder to bipolar disorder: a retrospective observational network study. \nTranslational Psychiatry. 2021;11(1):642. \n17. \nChandran U, Reps J, Yang R, Vachani A, Maldonado F, Kalsekar I. Machine learning and real-world data to \npredict lung cancer risk in routine care. Cancer Epidemiol Biomarkers Prev. 2023;32(3):337-43. \n18. \nJohn LH, Fridgeirsson EA, Kors JA, Reps JM, Williams RD, Ryan PB, et al. Development and validation of a \npatient-level model to predict dementia across a network of observational databases. BMC Med. 2024;22(1):1-12. \n19. \nOverhage JM, Ryan PB, Reich CG, Hartzema AG, Stang PE. Validation of a common data model for active \nsafety surveillance research. J Am Med Inform Assoc. 2012;19(1):54-60. \n20. \nLivingston G, Huntley J, Sommerlad A, Ames D, Ballard C, Banerjee S, et al. Dementia prevention, \nintervention, and care: 2020 report of the Lancet Commission. Lancet. 2020;396(10248):413-46. \n21. \nMusliner K, Østergaard S. Patterns and predictors of conversion to bipolar disorder in 91 587 individuals \ndiagnosed with unipolar depression. Acta Psychiatr Scand. 2018;137(5):422-32. \n22. \nAngst J, Sellaro R, Stassen HH, Gamma A. Diagnostic conversion from depression to bipolar disorders: results \nof a long-term prospective study of hospital admissions. J Affect Disord. 2005;84(2-3):149-57. \n23. \nKessing LV, Willer I, Andersen PK, Bukh JD. Rate and predictors of conversion from unipolar to bipolar \ndisorder: A systematic review and meta-analysis. Bipolar Disorders. 2017;19(5):324-35. \n24. \nLi C-T, Bai Y-M, Huang Y-L, Chen Y-S, Chen T-J, Cheng J-Y, et al. Association between antidepressant \nresistance in unipolar depression and subsequent bipolar disorder: cohort study. The British Journal of Psychiatry. \n2012;200(1):45-51. \n25. \nSiegel RL, Miller KD, Fuchs HE, Jemal A. Cancer statistics, 2022. CA Cancer J Clin. 2022;72(1). \n26. \nFedewa SA, Bandi P, Smith RA, Silvestri GA, Jemal A. Lung cancer screening rates during the COVID-19 \npandemic. Chest. 2022;161(2):586-9. \n27. \nWang Y, Midthun DE, Wampfler JA, Deng B, Stoddard SM, Zhang S, et al. Trends in the proportion of patients \nwith lung cancer meeting screening criteria. JAMA. 2015;313(8):853-5. \n28. \nFaselis C, Nations JA, Morgan CJ, Antevil J, Roseman JM, Zhang S, et al. Assessment of lung cancer risk among \n \n \n16 \nsmokers for whom annual screening is not recommended. JAMA oncology. 2022;8(10):1428-37. \n29. \nJohn LH. Time windows for disease onset prediction. Figshare2024. \n30. \nHardin J, Reps JM. Evaluating the impact of covariate lookback times on performance of patient-level \nprediction models. BMC Med Res Methodol. 2021;21(1):1-9. \n31. \nReps JM, Rijnbeek P, Cuthbert A, Ryan PB, Pratt N, Schuemie M. An empirical analysis of dealing with \npatients who are lost to follow-up when developing prognostic models using a cohort design. BMC Med Inform Decis \nMak. 2021;21:1-24. \n32. \nHosmer Jr DW, Lemeshow S, Sturdivant RX. Applied logistic regression: John Wiley & Sons; 2013. \n33. \nFridgeirsson EA, Williams R, Rijnbeek P, Suchard MA, Reps JM. Comparing penalization methods for linear \nmodels on large observational health data. J Am Med Inform Assoc. 2024;31(7):1514-21. \n34. \nFriedman JH. Greedy function approximation: a gradient boosting machine. Annals of statistics. 2001:1189-\n232. \n35. \nChen T, Guestrin C, editors. Xgboost: A scalable tree boosting system. Proceedings of the 22nd acm sigkdd \ninternational conference on knowledge discovery and data mining; 2016. \n36. \nDavagdorj K, Pham VH, Theera-Umpon N, Ryu KH. XGBoost-based framework for smoking-induced \nnoncommunicable disease prediction. Int J Environ Res Public Health. 2020;17(18):6513. \n37. \nLiu J, Wu J, Liu S, Li M, Hu K, Li K. Predicting mortality of patients with acute kidney injury in the ICU using \nXGBoost model. PLoS One. 2021;16(2):e0246306. \n38. \nRosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. \nPsychol Rev. 1958;65(6):386. \n39. \nRumelhart DE, Hinton GE, Williams RJ. Learning representations by back-propagating errors. Nature. \n1986;323(6088):533-6. \n40. \nHe K, Zhang X, Ren S, Sun J, editors. Deep residual learning for image recognition. Proceedings of the IEEE \nconference on computer vision and pattern recognition; 2016. \n41. \nRogers A, Kovaleva O, Rumshisky A. A primer in BERTology: What we know about how BERT works. \nTransactions of the Association for Computational Linguistics. 2021;8:842-66. \n42. \nDevlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language \nunderstanding. arXiv preprint arXiv:181004805. 2018. \n43. \nRadford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-\ntraining. 2018. \n44. \nLee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation \nmodel for biomedical text mining. Bioinformatics. 2020;36(4):1234-40. \n45. \nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Adv Neural \nInf Process Syst. 2017;30. \n46. \nJohn LH, Kors JA, Reps JM, Ryan PB, Rijnbeek PR. Logistic regression models for patient-level prediction \nbased on massive observational data: Do we need all data? Int J Med Inform. 2022;163:104762. \n47. \nCollins GS, Reitsma JB, Altman DG, Moons KG. Transparent reporting of a multivariable prediction model \nfor individual prognosis or diagnosis (TRIPOD) the TRIPOD statement. Circulation. 2015;131(2):211-9. \n48. \nHanley JA, McNeil BJ. The meaning and use of the area under a receiver operating characteristic (ROC) \ncurve. Radiology. 1982;143(1):29-36. \n49. \nRobin X, Turck N, Hainard A, Tiberti N, Lisacek F, Sanchez J-C, et al. pROC: an open-source package for R and \nS+ to analyze and compare ROC curves. BMC Bioinformatics. 2011;12:1-8. \n50. \nAustin PC, Steyerberg EW. The Integrated Calibration Index (ICI) and related metrics for quantifying the \ncalibration of logistic regression models. Stat Med. 2019;38(21):4051-65. \n51. \nHarrell FE. Regression modeling strategies: with applications to linear models, logistic regression, and \nsurvival analysis: Springer; 2001. \n52. \nFriedman M. A comparison of alternative tests of significance for the problem of m rankings. The annals of \nmathematical statistics. 1940;11(1):86-92. \n53. \nDemšar J. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine learning \nresearch. 2006;7:1-30. \n54. \nJohn LH. Development and validation of clinical prediction models across a network of observational \ndatabases. Figshare2024. \n \n \n17 \n55. \nVaidya V, Partha G, Karmakar M. Gender differences in utilization of preventive care services in the United \nStates. J Womens Health. 2012;21(2):140-5. \n56. \nBertakis KD, Azari R, Helms LJ, Callahan EJ, Robbins JA. Gender differences in the utilization of health care \nservices. J Fam Pract. 2000;49(2). \n57. \nPrince M, Bryce R, Albanese E, Wimo A, Ribeiro W, Ferri CP. The global prevalence of dementia: a systematic \nreview and metaanalysis. Alzheimers Dement. 2013;9(1):63-75. e2. \n58. \nWhite N, Parsons R, Collins G, Barnett A. Evidence of questionable research practices in clinical prediction \nmodels. BMC Med. 2023;21(1):339. \n59. \nde Hond AA, Steyerberg EW, van Calster B. Interpreting area under the receiver operating characteristic \ncurve. Lancet Digit Health. 2022;4(12):e853-e5. \n60. \nLi Y, Rao S, Solares JRA, Hassaine A, Ramakrishnan R, Canoy D, et al. BEHRT: transformer for electronic \nhealth records. Sci Rep. 2020;10(1):7155. \n61. \nRasmy L, Xiang Y, Xie Z, Tao C, Zhi D. Med-BERT: pretrained contextualized embeddings on large-scale \nstructured electronic health records for disease prediction. NPJ digital medicine. 2021;4(1):86. \n62. \nPang C, Jiang X, Kalluri KS, Spotnitz M, Chen R, Perotte A, et al., editors. CEHR-BERT: Incorporating temporal \ninformation from structured EHR data to improve prediction tasks. Machine Learning for Health; 2021: PMLR. \n \n \n \n1 \nSupplementary material \n \nComparison of deep learning and conventional methods for disease onset prediction \nLuis H. John, MSc1, Chungsoo Kim, PhD2, Jan A. Kors, PhD1, Junhyuk Chang, PharmD3, Hannah Morgan-Cooper, \nMSc4, Priya Desai, MSc4, Chao Pang, PhD5, Peter R. Rijnbeek, PhD1, Jenna M. Reps, PhD1,6, Egill A. Fridgeirsson, \nPhD1 \n1Department of Medical Informatics, Erasmus University Medical Center, Rotterdam, The Netherlands \n2Section of Cardiovascular Medicine, Department of Internal Medicine, Yale School of Medicine, New Haven, CT, \nUnited States \n3Department of Biomedical Informatics, Ajou University Graduate School of Medicine, Suwon, Republic of Korea \n4Stanford School of Medicine and Stanford Health Care, Palo Alto, CA, United States \n5Department of Biomedical Informatics, Columbia University Irving Medical Center, New York, NY, United States \n6Janssen Research and Development, Titusville, NJ, United States \nTable of contents \nSupplementary Methods \n2 \nSource of data \n2 \nIntegrated Primary Care Information (IPCI) \n2 \nAjou University School of Medicine (AUSOM) \n2 \nOptum® de-identified Electronic Health Record dataset (Optum EHR) \n2 \nOptum’s de-identifed Clinformatics® Data Mart Database (Clinformatics) \n2 \nStanford Medicine Research Data Repository OMOP (STARR-OMOP) \n2 \nColumbia University Irving Medical Center (CUIMC) \n3 \nIQVIA® Disease Analyzer Germany (German DA) \n3 \nJapan Medical Data Center (JMDC) \n3 \nMerative® MarketScan® Multi-State Medicaid Database (MDCD) \n3 \nMerative® MarketScan® Medicare Supplemental Database (MDCR) \n3 \nMerative® MarketScan® Commercial Claims and Encounters Database (CCAE) \n4 \nPrediction methods \n4 \nSupplementary Figures \n5 \nDiscrimination performance \n5 \nCalibration performance \n6 \n \n \n \n \n \n2 \nSupplementary Methods \nSource of data \nSeveral databases from the United States, Europe, and Asia-Pacific regions are used for the analysis. \nIntegrated Primary Care Information (IPCI) \nThe Integrated Primary Care Information (IPCI) database is a longitudinal observational database containing \nroutinely collected data from computer-based patient records of a selected group of general practitioners (GP) \nthroughout the Netherlands.(1) IPCI was started in 1992 by the department of Medical Informatics of the Erasmus \nUniversity Medical Center in Rotterdam. The current database includes patient records from 2006 on, when the size \nof the database started to increase significantly. In 2016, IPCI was certified as Regional Data Center. Since 2019 the \ndata is also standardized to the OMOP CDM. \nAjou University School of Medicine (AUSOM) \nThe Ajou University School of Medicine (AUSOM) database is the EHR database of the Ajou University Medical \nCenter from 1994. Ajou University Medical Center in South Korea is a tertiary teaching hospital with 1,108 beds, 33 \nmedical departments, and 23 operating rooms. The AUSOM database is standardized to the OMOP CDM. \nOptum® de-identified Electronic Health Record dataset (Optum EHR) \nOptum’s longitudinal EHR repository (Optum EHR) is derived from dozens of healthcare provider organizations in \nthe United States. The data is certified as de-identified by an independent statistical expert following HIPAA \nstatistical de-identification rules and managed according to Optum® customer data use agreements . Clinical, claims \nand other medical administrative data is obtained from both Inpatient and Ambulatory electronic health records \n(EHRs), practice management systems and numerous other internal systems. Information is processed, normalized, \nand standardized across the continuum of care from both acute inpatient stays and outpatient visits. Optum® data \nelements include demographics, medications prescribed and administered, immunizations, allergies, lab results \n(including microbiology), vital signs and other observable measurements, clinical and inpatient stay administrative \ndata and coded diagnoses and procedures. In addition, Optum® uses natural language processing (NLP) computing \ntechnology to transform critical facts from physician notes into usable datasets. The NLP data provides detailed \ninformation regarding signs and symptoms, family history, disease related scores (i.e. RAPID3 for RA, or CHADS2 \nfor stroke risk), genetic testing, medication changes, and physician rationale behind prescribing decisions that might \nnever be recorded in the EHR. \nOptum’s de-identifed Clinformatics® Data Mart Database (Clinformatics) \nOptum’s Clinformatics® Data Mart (Clinformatics) is derived from a database of administrative health claims for \nmembers of large commercial and Medicare Advantage health plans. Clinformatics® Data Mart is statistically de-\nidentified under the Expert Determination method consistent with HIPAA and managed according to Optum® \ncustomer data use agreements. Clinformatics administrative claims submitted for payment by providers and \npharmacies are verified, adjudicated and de-identified prior to inclusion. This data, including patient-level \nenrollment information, is derived from claims submitted for all medical and pharmacy health care services with \ninformation related to health care costs and resource utilization. The population is geographically diverse, spanning \nall 50 states. \nStanford Medicine Research Data Repository OMOP (STARR-OMOP) \nThe Stanford Medicine Research Data Repository OMOP (STARR-OMOP) is Stanford’s second-generation clinical \ndata warehouse dataset designed to enhance access to healthcare data for research purposes. Launched in 2019, \nSTARR-OMOP contains electronic health records data from Stanford Health Care and Stanford Children's Hospital, \nthe TriValley Hospital, and associated clinics for around four million patients from 2008. The dataset is refreshed \nmonthly and contains demographics, labs, diagnoses, drugs, and procedure information, as well as clinical notes. \nSeveral flowsheet fields are also mapped to the OMOP measurements table, including vitals such as blood pressure, \noxygen level, heart rate, respiratory rate, measurements from the Sequential Organ Failure Assessment (SOFA) \nscore, etc. The structured and unstructured data is anonymized using a combination of Safe Harbor and other \ntechniques. Based on guidelines from our University Privacy Office (UPO), our location data contains zip5 data for \n \n \n3 \nover 77% of the population. Our data and ETL processes are entirely hosted on the cloud. \nColumbia University Irving Medical Center (CUIMC) \nThe Columbia University Irving Medical Center (CUIMC) database holds electronic health records for more than \nsix million patients, with data collection starting in 1985. CUIMC, located in the northeast US, is a quaternary care \ncenter that provides primary care in northern Manhattan and nearby areas, covering both inpatient and outpatient \nservices. The database encompasses a wide array of data types, including patient demographics, visit details for \ninpatient and outpatient care, conditions (billing diagnoses and problem lists), medications (outpatient prescriptions \nand inpatient medication orders and administrations), medical devices, clinical measurements (such as laboratory \ntests and vital signs), and other clinical observations like symptoms. The data is sourced from a variety of systems, \nincluding current and former electronic health record (EHR) systems such as the homegrown Clinical Information \nSystem, WebCIS, Allscripts Sunrise Clinical Manager, Allscripts TouchWorks, and Epic Systems. It also includes \ndata from administrative systems like IBM PCS-ADS, Eagle Registration, IDX Systems, and Epic Systems, as well \nas ancillary systems such as the homegrown Laboratory Information System (LIS), Sunquest, and Cerner \nLaboratory. The CUIMC data has been standardized to the Observational Medical Outcomes Partnership Common \nData Model (OMOP CDM). \nIQVIA® Disease Analyzer Germany (German DA) \nIQVIA® Disease Analyzer Germany (German DA) is a longitudinal patient database providing anonymized \ninformation from continuing physician and patient interaction on consultations, diagnoses and treatment within \nPrimary Care. It contains a data from approximately 2,500 office based doctors in Germany. The contents of the \ndatabase document the management of patients by General Practitioners as well as some specialists and include \ncomprehensive records of diagnosis information; the management of the diagnosis, be it prescription issues, hospital \nadmission or other tertiary care; specialist referrals; laboratory test results and administrative activities. All the \nevents are date stamped, with diagnosis/note/test information collected. Prescriptions, issued by GPs using either the \ngeneric substance or drug name are captured exactly as written, including information on indication, dose, strength \nand dosage instruction and cost. \nJapan Medical Data Center (JMDC) \nJapan Medical Data Center (JMDC) database consists of data from more than 250 Health Insurance Associations \ncovering workers aged less than 75 and their dependents. The proportion who are younger than 66 years old in \nJMDC is approximately the same as the proportion in the whole nation. JMDC data includes data on membership \nstatus of the insured people and claims data provided by insurers under contract. Claims data are derived from \nmonthly claims issued by clinics, hospitals and community pharmacies. The size of JMDC population is about 10% \nof people in the whole nation. \nMerative® MarketScan® Multi-State Medicaid Database (MDCD) \nThe Merative® MarketScan® Multi-State Medicaid Database (MDCD) reflects the healthcare service use of \nindividuals covered by Medicaid programs in numerous geographically dispersed states. The database contains the \npooled healthcare experience of Medicaid enrollees, covered under fee-for-service and managed care plans. It \nincludes records of inpatient services, inpatient admissions, outpatient services, and prescription drug claims, as well \nas information on long-term care. Data on eligibility and service and provider type are also included. In addition to \nstandard demographic variables such as age and gender, the database includes variables such as federal aid category \n(income based, disability, Temporary Assistance for Needy Families) and race. \nMerative® MarketScan® Medicare Supplemental Database (MDCR) \nThe Merative® MarketScan® Medicare Supplemental Database (MDCR) represents the health services of retirees \nin the United States with Medicare supplemental coverage through employer-sponsored plans. This database \ncontains primarily fee-for-service plans and includes health insurance claims across the continuum of care (e.g. \ninpatient, outpatient and outpatient pharmacy). \n \n \n4 \nMerative® MarketScan® Commercial Claims and Encounters Database (CCAE) \nThe Merative® MarketScan® Commercial Claims and Encounters Database (CCAE) includes health insurance \nclaims across the continuum of care (e.g. inpatient, outpatient, outpatient pharmacy, carve-out behavioral healthcare) \nas well as enrollment data from large employers and health plans across the United States who provide private \nhealthcare coverage for employees, their spouses, and dependents. This administrative claims database includes a \nvariety of fee- for-service, preferred provider organizations, and capitated health plans. \nPrediction methods \nThe hyperparameter space that is explored during training is presented in Table S1, Table S2, Table S3, and Table \nS4 for logistic regression, XGBoost, ResNet, and Transformer, respectively. For developing ResNet and \nTransformer models we follow the common approach of sampling 100 combinations from the hyperparameter space \nto reduce computational costs.(2) \nTable S1. Hyperparameter space of logistic regression models. \nHyperparameter \nValues \nPrior distribution starting variance (lower limit) \n0.01 \nLower prior variance limit for grid-search \n0.01 \nUpper prior variance limit for grid-search \n20 \nTable S2. Hyperparameter space of XGBoost models. \nHyperparameter \nValues \nNumber of trees \n100, 300 \nMaximum depth of each tree \n4, 6, 8 \nBoosting learning rate \n0.05, 0.1, 0.3 \nTable S3. Hyperparameter space of ResNet models. \nHyperparameter \nValues \nSize of embedding layer \n64, 128, 256, 512 \nNumber of layers \n1, 2, 3, 4, 5, 6, 7, 8 \nNumber of neurons in each default layer \n64, 128, 256, 512, 1024 \nFactor to grow the number of neurons in each residual layer \n1, 2, 3, 4 \nDropout after first linear layer in residual layer \n0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 \nDropout after last linear layer in residual layer \n0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 \nTable S4. Hyperparameter space of Transformer models. \nHyperparameter \nValues \nNumber of transformer blocks \n2, 3, 4 \nEmbedding dimensions \n64, 128, 256, 512 \nNumber of attention heads \n2, 4, 8 \nDropout for attention \n0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 \nDropout for feedforward block \n0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 \nDropout for residual connections \n0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 \nDimension of the feedforward block as a ratio of the embedding \ndimensions \n0.75 \n \n \n5 \nSupplementary Figures \nDiscrimination performance \nFigure S1 presents the internal and external discrimination performances of all developed models using the area under the receiver operating characteristic curve (AUROC). The \n95% confidence intervals (CI) is provided alongside these results. \n \nFigure S1. Internal and external discrimination performance (AUROC) with 95% CI across prediction methods and prediction problems. \n \n \n0.71\n(0.66,0.76)\n0.74\n(0.67,0.82)\n0.68\n(0.67,0.68)\n0.72\n(0.71,0.73)\n0.59\n(0.54,0.65)\n0.74\n(0.63,0.86)\n0.75\n(0.71,0.79)\n0.65\n(0.63,0.67)\n0.69\n(0.67,0.70)\n0.67\n(0.55,0.79)\n0.68\n(0.66,0.69)\n0.55\n(0.52,0.57)\n0.74\n(0.66,0.83)\n0.54\n(0.53,0.55)\n0.53\n(0.52,0.55)\n0.49\n(0.45,0.54)\n0.62\n(0.50,0.74)\n0.62\n(0.58,0.65)\n0.59\n(0.58,0.61)\n0.52\n(0.51,0.54)\n0.54\n(0.41,0.68)\n0.51\n(0.50,0.53)\n0.67\n(0.64,0.70)\n0.42\n(0.31,0.53)\n0.64\n(0.62,0.65)\n0.66\n(0.64,0.68)\n0.55\n(0.50,0.59)\n0.63\n(0.48,0.78)\n0.62\n(0.58,0.67)\n0.53\n(0.51,0.55)\n0.68\n(0.66,0.69)\n0.66\n(0.54,0.77)\n0.64\n(0.62,0.65)\n0.74\n(0.71,0.76)\n0.73\n(0.66,0.81)\n0.72\n(0.71,0.72)\n0.76\n(0.73,0.78)\n0.65\n(0.60,0.70)\n0.72\n(0.61,0.83)\n0.71\n(0.68,0.75)\n0.67\n(0.66,0.69)\n0.73\n(0.71,0.74)\n0.66\n(0.55,0.77)\n0.71\n(0.70,0.72)\n0.50\n(0.50,0.50)\n0.50\n(0.50,0.50)\n0.83\n(0.82,0.85)\n0.77\n(0.75,0.79)\n0.79\n(0.79,0.79)\n0.79\n(0.79,0.79)\n0.76\n(0.75,0.77)\n0.73\n(0.72,0.74)\n0.74\n(0.74,0.75)\n0.66\n(0.65,0.68)\n0.70\n(0.70,0.70)\n0.70\n(0.70,0.70)\n0.52\n(0.52,0.52)\n0.83\n(0.82,0.83)\n0.78\n(0.75,0.81)\n0.79\n(0.78,0.79)\n0.78\n(0.78,0.78)\n0.76\n(0.75,0.77)\n0.73\n(0.72,0.74)\n0.73\n(0.73,0.74)\n0.65\n(0.64,0.67)\n0.68\n(0.68,0.69)\n0.69\n(0.68,0.69)\n0.53\n(0.53,0.54)\n0.83\n(0.83,0.84)\n0.78\n(0.76,0.79)\n0.82\n(0.81,0.82)\n0.81\n(0.81,0.81)\n0.78\n(0.77,0.79)\n0.76\n(0.75,0.77)\n0.75\n(0.75,0.75)\n0.67\n(0.65,0.68)\n0.73\n(0.73,0.74)\n0.74\n(0.74,0.74)\n0.61\n(0.60,0.61)\n0.83\n(0.83,0.84)\n0.78\n(0.76,0.79)\n0.81\n(0.81,0.81)\n0.82\n(0.82,0.82)\n0.78\n(0.77,0.79)\n0.75\n(0.74,0.76)\n0.75\n(0.75,0.75)\n0.68\n(0.67,0.70)\n0.73\n(0.73,0.74)\n0.74\n(0.74,0.74)\n0.62\n(0.61,0.62)\n0.83\n(0.83,0.84)\n0.77\n(0.76,0.79)\n0.80\n(0.80,0.80)\n0.80\n(0.80,0.80)\n0.78\n(0.76,0.80)\n0.75\n(0.74,0.76)\n0.75\n(0.75,0.75)\n0.67\n(0.65,0.68)\n0.71\n(0.71,0.71)\n0.72\n(0.72,0.72)\n0.59\n(0.59,0.60)\n0.83\n(0.83,0.84)\n0.77\n(0.75,0.79)\n0.79\n(0.79,0.79)\n0.80\n(0.80,0.80)\n0.77\n(0.76,0.78)\n0.76\n(0.74,0.78)\n0.75\n(0.75,0.75)\n0.66\n(0.65,0.68)\n0.71\n(0.71,0.71)\n0.71\n(0.71,0.71)\n0.58\n(0.57,0.58)\n0.76\n(0.75,0.77)\n0.74\n(0.74,0.75)\n0.72\n(0.67,0.77)\n0.72\n(0.69,0.76)\n0.55\n(0.52,0.59)\n0.57\n(0.47,0.66)\n0.66\n(0.66,0.67)\n0.63\n(0.58,0.68)\n0.68\n(0.67,0.69)\n0.73\n(0.72,0.73)\n0.76\n(0.75,0.77)\n0.70\n(0.65,0.75)\n0.69\n(0.65,0.73)\n0.59\n(0.56,0.63)\n0.66\n(0.57,0.75)\n0.64\n(0.64,0.65)\n0.70\n(0.66,0.75)\n0.71\n(0.70,0.71)\n0.65\n(0.65,0.66)\n0.70\n(0.70,0.71)\n0.68\n(0.57,0.79)\n0.67\n(0.63,0.71)\n0.56\n(0.52,0.60)\n0.65\n(0.56,0.74)\n0.60\n(0.60,0.61)\n0.65\n(0.60,0.70)\n0.65\n(0.64,0.66)\n0.62\n(0.62,0.63)\n0.62\n(0.61,0.63)\n0.62\n(0.57,0.68)\n0.66\n(0.58,0.75)\n0.56\n(0.52,0.60)\n0.49\n(0.39,0.59)\n0.59\n(0.58,0.59)\n0.62\n(0.57,0.67)\n0.59\n(0.58,0.60)\n0.71\n(0.67,0.75)\n0.69\n(0.61,0.77)\n0.67\n(0.66,0.68)\n0.71\n(0.70,0.72)\n0.60\n(0.55,0.65)\n0.67\n(0.55,0.79)\n0.74\n(0.71,0.78)\n0.63\n(0.61,0.65)\n0.68\n(0.66,0.69)\n0.66\n(0.56,0.76)\n0.66\n(0.65,0.67)\n0.74\n(0.71,0.76)\n0.69\n(0.61,0.77)\n0.74\n(0.73,0.75)\n0.76\n(0.75,0.77)\n0.67\n(0.61,0.72)\n0.71\n(0.59,0.82)\n0.71\n(0.68,0.75)\n0.68\n(0.67,0.70)\n0.74\n(0.72,0.75)\n0.72\n(0.61,0.83)\n0.72\n(0.71,0.73)\n0.73\n(0.71,0.75)\n0.74\n(0.66,0.82)\n0.71\n(0.70,0.72)\n0.75\n(0.72,0.77)\n0.62\n(0.57,0.67)\n0.73\n(0.61,0.85)\n0.71\n(0.67,0.74)\n0.67\n(0.65,0.69)\n0.73\n(0.72,0.74)\n0.68\n(0.56,0.79)\n0.71\n(0.70,0.72)\n0.52\n(0.49,0.55)\n0.52\n(0.41,0.62)\n0.55\n(0.54,0.56)\n0.53\n(0.51,0.55)\n0.71\n(0.61,0.82)\n0.46\n(0.32,0.61)\n0.45\n(0.40,0.49)\n0.57\n(0.55,0.59)\n0.52\n(0.50,0.53)\n0.53\n(0.40,0.65)\n0.53\n(0.52,0.54)\n0.83\n(0.82,0.84)\n0.77\n(0.75,0.79)\n0.79\n(0.79,0.79)\n0.79\n(0.79,0.79)\n0.76\n(0.75,0.77)\n0.73\n(0.72,0.74)\n0.75\n(0.74,0.75)\n0.66\n(0.65,0.68)\n0.70\n(0.70,0.70)\n0.70\n(0.70,0.70)\n0.53\n(0.53,0.54)\n0.82\n(0.82,0.83)\n0.77\n(0.73,0.80)\n0.79\n(0.79,0.79)\n0.79\n(0.78,0.79)\n0.76\n(0.75,0.77)\n0.72\n(0.71,0.73)\n0.73\n(0.73,0.74)\n0.65\n(0.64,0.67)\n0.69\n(0.69,0.69)\n0.69\n(0.69,0.69)\n0.56\n(0.55,0.56)\n0.83\n(0.83,0.84)\n0.78\n(0.76,0.80)\n0.82\n(0.82,0.82)\n0.82\n(0.81,0.82)\n0.79\n(0.78,0.80)\n0.77\n(0.76,0.78)\n0.75\n(0.75,0.76)\n0.69\n(0.68,0.71)\n0.74\n(0.73,0.74)\n0.74\n(0.74,0.74)\n0.66\n(0.65,0.66)\n0.83\n(0.83,0.84)\n0.78\n(0.76,0.80)\n0.81\n(0.81,0.81)\n0.82\n(0.82,0.82)\n0.78\n(0.77,0.79)\n0.75\n(0.75,0.76)\n0.75\n(0.75,0.76)\n0.69\n(0.68,0.71)\n0.73\n(0.73,0.73)\n0.74\n(0.74,0.74)\n0.67\n(0.67,0.68)\n0.83\n(0.82,0.83)\n0.78\n(0.76,0.79)\n0.80\n(0.80,0.80)\n0.79\n(0.79,0.80)\n0.78\n(0.76,0.80)\n0.74\n(0.73,0.75)\n0.74\n(0.74,0.74)\n0.67\n(0.65,0.68)\n0.70\n(0.70,0.70)\n0.71\n(0.71,0.72)\n0.60\n(0.60,0.61)\n0.83\n(0.82,0.83)\n0.77\n(0.75,0.79)\n0.80\n(0.79,0.80)\n0.80\n(0.79,0.80)\n0.77\n(0.76,0.78)\n0.77\n(0.75,0.78)\n0.75\n(0.75,0.75)\n0.66\n(0.65,0.68)\n0.71\n(0.71,0.71)\n0.72\n(0.72,0.72)\n0.62\n(0.62,0.63)\n0.76\n(0.76,0.77)\n0.75\n(0.74,0.75)\n0.71\n(0.67,0.76)\n0.75\n(0.71,0.79)\n0.56\n(0.53,0.60)\n0.58\n(0.50,0.67)\n0.67\n(0.67,0.68)\n0.66\n(0.62,0.71)\n0.68\n(0.68,0.69)\n0.72\n(0.71,0.72)\n0.75\n(0.74,0.77)\n0.70\n(0.65,0.75)\n0.67\n(0.63,0.71)\n0.57\n(0.54,0.61)\n0.67\n(0.58,0.75)\n0.64\n(0.64,0.65)\n0.70\n(0.66,0.75)\n0.71\n(0.70,0.71)\n0.64\n(0.64,0.65)\n0.65\n(0.64,0.66)\n0.66\n(0.60,0.71)\n0.65\n(0.56,0.74)\n0.54\n(0.51,0.57)\n0.58\n(0.51,0.65)\n0.58\n(0.58,0.59)\n0.61\n(0.56,0.66)\n0.59\n(0.59,0.60)\n0.59\n(0.54,0.65)\n0.52\n(0.44,0.60)\n0.56\n(0.55,0.57)\n0.60\n(0.59,0.62)\n0.53\n(0.46,0.60)\n0.68\n(0.55,0.80)\n0.63\n(0.60,0.67)\n0.55\n(0.52,0.57)\n0.61\n(0.59,0.62)\n0.59\n(0.48,0.70)\n0.58\n(0.57,0.60)\n0.57\n(0.55,0.60)\n0.70\n(0.53,0.87)\n0.55\n(0.54,0.56)\n0.56\n(0.55,0.58)\n0.50\n(0.45,0.55)\n0.55\n(0.42,0.69)\n0.65\n(0.61,0.69)\n0.54\n(0.52,0.56)\n0.54\n(0.52,0.55)\n0.55\n(0.41,0.68)\n0.53\n(0.52,0.55)\n0.72\n(0.70,0.74)\n0.59\n(0.50,0.67)\n0.73\n(0.71,0.74)\n0.73\n(0.72,0.75)\n0.68\n(0.62,0.73)\n0.67\n(0.53,0.81)\n0.70\n(0.66,0.74)\n0.64\n(0.62,0.66)\n0.72\n(0.71,0.73)\n0.72\n(0.58,0.85)\n0.70\n(0.69,0.71)\n0.69\n(0.67,0.72)\n0.63\n(0.53,0.72)\n0.66\n(0.65,0.66)\n0.71\n(0.68,0.74)\n0.66\n(0.61,0.71)\n0.75\n(0.64,0.85)\n0.65\n(0.62,0.69)\n0.60\n(0.58,0.62)\n0.71\n(0.69,0.72)\n0.69\n(0.58,0.80)\n0.67\n(0.66,0.68)\n0.47\n(0.45,0.50)\n0.47\n(0.39,0.55)\n0.52\n(0.51,0.52)\n0.53\n(0.51,0.54)\n0.66\n(0.53,0.78)\n0.49\n(0.39,0.60)\n0.45\n(0.41,0.48)\n0.53\n(0.51,0.55)\n0.52\n(0.51,0.54)\n0.51\n(0.38,0.64)\n0.52\n(0.51,0.53)\n0.59\n(0.57,0.61)\n0.61\n(0.54,0.67)\n0.55\n(0.54,0.55)\n0.54\n(0.52,0.55)\n0.49\n(0.42,0.56)\n0.78\n(0.62,0.95)\n0.67\n(0.64,0.70)\n0.51\n(0.49,0.53)\n0.53\n(0.51,0.54)\n0.47\n(0.35,0.59)\n0.53\n(0.52,0.54)\n0.79\n(0.78,0.81)\n0.71\n(0.68,0.73)\n0.75\n(0.75,0.75)\n0.75\n(0.75,0.75)\n0.73\n(0.72,0.74)\n0.70\n(0.69,0.71)\n0.69\n(0.69,0.70)\n0.62\n(0.60,0.63)\n0.67\n(0.67,0.68)\n0.67\n(0.67,0.67)\n0.50\n(0.49,0.50)\n0.68\n(0.67,0.69)\n0.71\n(0.67,0.75)\n0.69\n(0.69,0.69)\n0.69\n(0.69,0.69)\n0.67\n(0.66,0.68)\n0.63\n(0.62,0.64)\n0.63\n(0.63,0.63)\n0.58\n(0.57,0.60)\n0.62\n(0.61,0.62)\n0.62\n(0.62,0.62)\n0.48\n(0.48,0.49)\n0.82\n(0.82,0.83)\n0.77\n(0.75,0.79)\n0.82\n(0.82,0.82)\n0.82\n(0.82,0.82)\n0.79\n(0.78,0.80)\n0.78\n(0.77,0.78)\n0.75\n(0.75,0.75)\n0.68\n(0.67,0.70)\n0.74\n(0.74,0.74)\n0.75\n(0.74,0.75)\n0.67\n(0.67,0.68)\n0.82\n(0.82,0.83)\n0.76\n(0.74,0.78)\n0.81\n(0.81,0.81)\n0.82\n(0.82,0.82)\n0.78\n(0.77,0.79)\n0.75\n(0.75,0.76)\n0.74\n(0.74,0.74)\n0.69\n(0.67,0.70)\n0.74\n(0.73,0.74)\n0.75\n(0.74,0.75)\n0.67\n(0.66,0.67)\n0.72\n(0.71,0.73)\n0.63\n(0.61,0.66)\n0.72\n(0.71,0.72)\n0.74\n(0.74,0.74)\n0.71\n(0.69,0.73)\n0.69\n(0.67,0.70)\n0.62\n(0.61,0.62)\n0.60\n(0.59,0.62)\n0.65\n(0.65,0.65)\n0.66\n(0.66,0.67)\n0.58\n(0.57,0.58)\n0.74\n(0.73,0.75)\n0.67\n(0.65,0.69)\n0.74\n(0.74,0.74)\n0.76\n(0.76,0.76)\n0.71\n(0.70,0.72)\n0.73\n(0.71,0.75)\n0.70\n(0.69,0.70)\n0.61\n(0.59,0.62)\n0.68\n(0.68,0.69)\n0.68\n(0.67,0.68)\n0.58\n(0.58,0.58)\n0.76\n(0.75,0.76)\n0.74\n(0.74,0.75)\n0.70\n(0.65,0.76)\n0.75\n(0.71,0.78)\n0.52\n(0.48,0.56)\n0.55\n(0.46,0.64)\n0.66\n(0.65,0.66)\n0.63\n(0.58,0.67)\n0.68\n(0.67,0.68)\n0.71\n(0.71,0.72)\n0.74\n(0.73,0.75)\n0.67\n(0.62,0.72)\n0.70\n(0.66,0.74)\n0.60\n(0.56,0.63)\n0.60\n(0.51,0.68)\n0.64\n(0.64,0.65)\n0.64\n(0.58,0.69)\n0.68\n(0.67,0.69)\n0.57\n(0.56,0.57)\n0.59\n(0.58,0.60)\n0.55\n(0.42,0.67)\n0.56\n(0.52,0.61)\n0.52\n(0.48,0.56)\n0.51\n(0.43,0.60)\n0.52\n(0.52,0.53)\n0.50\n(0.45,0.55)\n0.52\n(0.51,0.53)\n0.54\n(0.54,0.55)\n0.53\n(0.53,0.54)\n0.54\n(0.48,0.60)\n0.52\n(0.43,0.62)\n0.53\n(0.49,0.57)\n0.49\n(0.40,0.58)\n0.53\n(0.52,0.53)\n0.51\n(0.46,0.56)\n0.52\n(0.52,0.53)\n0.66\n(0.60,0.71)\n0.61\n(0.52,0.70)\n0.57\n(0.56,0.57)\n0.62\n(0.61,0.64)\n0.35\n(0.30,0.40)\n0.64\n(0.51,0.77)\n0.71\n(0.67,0.75)\n0.50\n(0.48,0.52)\n0.60\n(0.59,0.62)\n0.58\n(0.44,0.72)\n0.57\n(0.56,0.59)\n0.53\n(0.51,0.56)\n0.85\n(0.79,0.91)\n0.53\n(0.52,0.54)\n0.55\n(0.53,0.56)\n0.53\n(0.48,0.58)\n0.63\n(0.49,0.77)\n0.59\n(0.55,0.63)\n0.53\n(0.51,0.55)\n0.52\n(0.51,0.54)\n0.47\n(0.35,0.60)\n0.52\n(0.50,0.53)\n0.69\n(0.67,0.72)\n0.56\n(0.47,0.64)\n0.71\n(0.70,0.73)\n0.71\n(0.69,0.72)\n0.69\n(0.64,0.74)\n0.65\n(0.52,0.79)\n0.69\n(0.65,0.73)\n0.64\n(0.62,0.66)\n0.70\n(0.69,0.72)\n0.65\n(0.51,0.79)\n0.68\n(0.67,0.70)\n0.66\n(0.63,0.68)\n0.57\n(0.48,0.67)\n0.61\n(0.61,0.62)\n0.66\n(0.63,0.70)\n0.50\n(0.45,0.54)\n0.71\n(0.60,0.81)\n0.66\n(0.62,0.70)\n0.52\n(0.50,0.54)\n0.66\n(0.64,0.67)\n0.66\n(0.55,0.77)\n0.63\n(0.62,0.64)\n0.41\n(0.39,0.43)\n0.36\n(0.27,0.46)\n0.49\n(0.48,0.49)\n0.46\n(0.45,0.48)\n0.72\n(0.61,0.83)\n0.38\n(0.24,0.51)\n0.34\n(0.30,0.37)\n0.43\n(0.41,0.46)\n0.45\n(0.44,0.47)\n0.51\n(0.38,0.63)\n0.49\n(0.47,0.50)\n0.64\n(0.61,0.66)\n0.69\n(0.60,0.78)\n0.55\n(0.55,0.56)\n0.60\n(0.58,0.61)\n0.36\n(0.31,0.40)\n0.80\n(0.68,0.92)\n0.70\n(0.66,0.73)\n0.52\n(0.50,0.54)\n0.55\n(0.53,0.56)\n0.48\n(0.35,0.60)\n0.55\n(0.54,0.56)\n0.82\n(0.80,0.83)\n0.75\n(0.73,0.77)\n0.77\n(0.77,0.77)\n0.78\n(0.78,0.78)\n0.75\n(0.74,0.76)\n0.72\n(0.71,0.73)\n0.72\n(0.71,0.72)\n0.65\n(0.64,0.67)\n0.69\n(0.69,0.70)\n0.68\n(0.68,0.68)\n0.56\n(0.56,0.57)\n0.54\n(0.53,0.55)\n0.63\n(0.59,0.68)\n0.55\n(0.55,0.55)\n0.58\n(0.58,0.59)\n0.55\n(0.54,0.56)\n0.57\n(0.56,0.58)\n0.57\n(0.57,0.57)\n0.56\n(0.54,0.57)\n0.51\n(0.50,0.51)\n0.56\n(0.56,0.56)\n0.57\n(0.57,0.58)\n0.83\n(0.82,0.83)\n0.77\n(0.75,0.78)\n0.82\n(0.82,0.83)\n0.82\n(0.82,0.82)\n0.79\n(0.78,0.80)\n0.78\n(0.77,0.79)\n0.75\n(0.74,0.75)\n0.68\n(0.67,0.70)\n0.74\n(0.74,0.74)\n0.75\n(0.74,0.75)\n0.66\n(0.66,0.67)\n0.83\n(0.82,0.84)\n0.78\n(0.76,0.79)\n0.81\n(0.81,0.81)\n0.82\n(0.82,0.82)\n0.79\n(0.78,0.80)\n0.76\n(0.75,0.77)\n0.74\n(0.74,0.75)\n0.69\n(0.68,0.71)\n0.74\n(0.74,0.74)\n0.75\n(0.75,0.75)\n0.68\n(0.68,0.68)\n0.77\n(0.76,0.78)\n0.72\n(0.70,0.74)\n0.76\n(0.76,0.76)\n0.76\n(0.76,0.77)\n0.76\n(0.74,0.78)\n0.70\n(0.69,0.71)\n0.67\n(0.67,0.67)\n0.62\n(0.61,0.64)\n0.68\n(0.67,0.68)\n0.68\n(0.68,0.68)\n0.58\n(0.58,0.59)\n0.73\n(0.73,0.74)\n0.70\n(0.68,0.72)\n0.75\n(0.75,0.75)\n0.76\n(0.76,0.76)\n0.73\n(0.72,0.74)\n0.73\n(0.71,0.75)\n0.70\n(0.70,0.70)\n0.63\n(0.61,0.64)\n0.68\n(0.68,0.69)\n0.68\n(0.68,0.68)\n0.61\n(0.60,0.61)\n0.76\n(0.75,0.77)\n0.75\n(0.74,0.75)\n0.73\n(0.68,0.78)\n0.73\n(0.69,0.77)\n0.54\n(0.50,0.58)\n0.50\n(0.41,0.59)\n0.67\n(0.67,0.68)\n0.65\n(0.61,0.70)\n0.68\n(0.67,0.68)\n0.73\n(0.72,0.73)\n0.75\n(0.73,0.76)\n0.69\n(0.63,0.74)\n0.71\n(0.68,0.75)\n0.57\n(0.54,0.61)\n0.58\n(0.49,0.68)\n0.65\n(0.65,0.66)\n0.69\n(0.65,0.74)\n0.69\n(0.69,0.70)\n0.60\n(0.60,0.61)\n0.63\n(0.62,0.64)\n0.64\n(0.54,0.75)\n0.60\n(0.56,0.64)\n0.55\n(0.51,0.59)\n0.49\n(0.41,0.58)\n0.57\n(0.56,0.57)\n0.53\n(0.48,0.57)\n0.58\n(0.57,0.59)\n0.59\n(0.58,0.59)\n0.58\n(0.57,0.59)\n0.60\n(0.55,0.65)\n0.65\n(0.55,0.75)\n0.53\n(0.49,0.56)\n0.55\n(0.45,0.65)\n0.55\n(0.54,0.55)\n0.60\n(0.55,0.65)\n0.54\n(0.54,0.55)\nLogistic regression\nXGBoost\nResNet\nTransformer\nLung cancer\nDementia\nBipolar disorder\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nValidation database\nDevelopment database\nAUROC\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n \n \n6 \n \nCalibration performance \nFigure S2 presents the internal and external calibration performances of all developed models using the average absolute difference between observed and predicted probabilities \n(Eavg). \n \n \nFigure S2. Internal and external calibration performance (Eavg) across prediction methods and prediction problems. \n5.9e-4\n1.1e-3\n1.0e-3\n8.3e-4\n6.7e-4\n1.1e-3\n1.5e-3\n9.4e-4\n1.5e-3\n1.5e-3\n1.2e-3\n1.6e-3\n5.0e-4\n4.6e-4\n4.3e-4\n6.4e-4\n1.8e-4\n3.1e-4\n4.0e-4\n2.6e-3\n1.4e-3\n1.1e-4\n1.2e-3\n5.9e-4\n6.4e-4\n9.7e-4\n3.0e-4\n5.6e-4\n8.7e-4\n1.6e-4\n2.5e-3\n5.9e-4\n3.4e-4\n1.2e-3\n2.9e-4\n2.5e-4\n5.0e-4\n4.3e-4\n6.3e-4\n7.2e-4\n1.0e-4\n1.7e-3\n6.4e-4\n1.6e-4\n0.0e+0\n0.0e+0\n1.4e-3\n2.4e-3\n2.2e-2\n3.2e-2\n3.2e-3\n9.9e-3\n2.8e-2\n1.3e-3\n6.3e-2\n2.2e-2\n2.4e-3\n7.4e-3\n2.2e-3\n2.8e-2\n3.6e-2\n6.3e-3\n1.4e-2\n3.4e-2\n8.8e-4\n6.7e-2\n2.7e-2\n1.3e-3\n1.8e-2\n1.6e-2\n3.3e-3\n8.4e-3\n1.6e-2\n1.2e-2\n7.3e-3\n5.3e-3\n2.8e-2\n1.2e-2\n3.5e-3\n1.9e-2\n1.9e-2\n3.0e-3\n2.9e-3\n2.0e-2\n1.5e-2\n5.7e-3\n7.9e-3\n2.4e-2\n2.0e-2\n4.5e-3\n4.8e-3\n2.9e-3\n2.0e-2\n2.6e-2\n2.2e-3\n7.1e-3\n2.7e-2\n1.8e-3\n5.8e-2\n1.5e-2\n1.2e-3\n5.7e-3\n7.2e-3\n1.5e-2\n2.1e-2\n5.0e-3\n2.9e-3\n2.5e-2\n5.5e-3\n5.2e-2\n9.9e-3\n3.5e-3\n2.0e-3\n2.0e-3\n5.7e-3\n1.7e-3\n1.1e-2\n2.2e-2\n1.4e-2\n9.7e-4\n1.8e-3\n2.3e-3\n1.1e-3\n6.5e-3\n3.6e-3\n7.3e-3\n2.5e-2\n1.3e-2\n5.2e-4\n1.6e-3\n7.3e-3\n5.9e-3\n2.2e-3\n9.2e-3\n3.7e-3\n3.2e-2\n2.5e-2\n1.5e-3\n8.7e-3\n2.8e-3\n3.4e-3\n7.7e-3\n2.6e-3\n1.9e-2\n1.6e-2\n1.8e-2\n7.9e-3\n2.7e-3\n5.3e-4\n1.2e-3\n1.0e-3\n1.1e-3\n8.6e-4\n1.5e-3\n1.7e-3\n9.9e-4\n7.3e-4\n1.1e-3\n1.4e-3\n1.1e-3\n2.7e-4\n1.4e-4\n1.3e-4\n2.8e-4\n5.4e-4\n7.7e-4\n1.0e-4\n1.5e-3\n6.9e-4\n2.9e-4\n1.2e-3\n3.6e-4\n1.8e-4\n2.7e-4\n2.4e-4\n4.8e-4\n7.3e-4\n2.1e-4\n1.7e-3\n6.9e-4\n3.5e-4\n1.5e-3\n9.2e-4\n7.8e-4\n7.2e-4\n3.4e-4\n9.0e-4\n1.2e-3\n5.7e-4\n2.8e-3\n1.5e-3\n6.1e-4\n8.1e-4\n3.5e-3\n2.1e-2\n3.1e-2\n3.4e-3\n1.0e-2\n2.7e-2\n2.4e-3\n6.3e-2\n2.0e-2\n3.3e-3\n8.0e-3\n1.9e-3\n2.7e-2\n3.6e-2\n6.1e-3\n1.4e-2\n3.3e-2\n1.1e-3\n6.7e-2\n2.7e-2\n8.4e-4\n1.4e-2\n1.3e-2\n8.8e-4\n1.0e-2\n1.3e-2\n7.9e-3\n1.1e-2\n6.3e-3\n3.0e-2\n6.8e-3\n4.0e-3\n2.1e-2\n1.9e-2\n2.5e-3\n9.5e-4\n2.1e-2\n1.8e-2\n1.1e-2\n5.7e-3\n2.1e-2\n2.3e-2\n2.6e-3\n3.7e-3\n2.5e-3\n1.9e-2\n2.6e-2\n1.5e-3\n8.0e-3\n2.6e-2\n9.2e-4\n5.6e-2\n1.6e-2\n1.1e-3\n5.8e-3\n6.7e-3\n1.5e-2\n2.1e-2\n4.5e-3\n3.8e-3\n2.6e-2\n5.7e-3\n4.9e-2\n1.1e-2\n4.9e-3\n1.4e-3\n2.4e-3\n5.1e-3\n2.1e-3\n9.5e-3\n2.2e-2\n1.6e-2\n1.2e-3\n2.2e-3\n2.1e-3\n1.1e-3\n6.0e-3\n4.1e-3\n6.4e-3\n2.5e-2\n1.4e-2\n1.1e-3\n1.6e-3\n2.3e-3\n3.4e-3\n8.5e-3\n3.0e-3\n1.4e-2\n1.8e-2\n1.6e-2\n7.7e-3\n1.5e-3\n3.1e-3\n3.3e-3\n3.5e-3\n2.9e-3\n3.0e-3\n3.6e-3\n4.4e-3\n2.5e-3\n5.9e-3\n5.2e-3\n2.8e-3\n2.8e-3\n3.4e-3\n3.0e-3\n3.6e-3\n2.8e-3\n3.3e-3\n3.5e-3\n4.5e-3\n3.7e-3\n5.1e-3\n3.9e-3\n6.9e-4\n6.6e-4\n3.5e-4\n2.6e-4\n5.1e-4\n7.8e-4\n1.1e-3\n4.6e-4\n1.2e-3\n7.4e-4\n3.7e-4\n7.0e-4\n9.7e-4\n8.8e-4\n4.2e-4\n8.7e-4\n1.2e-3\n1.2e-3\n7.8e-4\n1.0e-3\n6.7e-4\n6.0e-4\n3.0e-2\n3.4e-2\n3.2e-2\n3.1e-2\n3.2e-2\n3.2e-2\n3.4e-2\n3.1e-2\n2.9e-2\n3.0e-2\n3.1e-2\n2.5e-1\n3.1e-1\n2.8e-1\n2.9e-1\n2.6e-1\n2.5e-1\n2.2e-1\n2.9e-1\n3.0e-1\n3.1e-1\n2.9e-1\n6.3e-3\n7.7e-3\n2.0e-2\n3.1e-2\n8.8e-3\n1.4e-2\n3.0e-2\n3.4e-3\n6.1e-2\n2.4e-2\n3.1e-3\n1.8e-2\n2.2e-2\n2.2e-2\n2.6e-2\n2.3e-2\n2.4e-2\n3.2e-2\n1.6e-2\n5.4e-2\n3.6e-2\n9.2e-3\n1.6e-2\n1.2e-2\n2.1e-3\n5.2e-3\n1.6e-2\n1.2e-2\n1.3e-2\n5.0e-3\n2.0e-2\n1.3e-2\n3.8e-3\n1.9e-2\n1.9e-2\n6.0e-3\n3.1e-3\n2.3e-2\n2.0e-2\n1.6e-2\n5.5e-3\n1.4e-2\n2.2e-2\n3.5e-3\n6.7e-3\n6.5e-3\n1.7e-2\n2.4e-2\n4.5e-3\n1.1e-2\n2.8e-2\n5.3e-3\n5.5e-2\n2.0e-2\n4.1e-3\n7.1e-3\n1.4e-2\n1.1e-2\n1.5e-2\n9.2e-3\n6.4e-3\n2.3e-2\n9.9e-3\n4.3e-2\n7.7e-3\n6.5e-3\n3.3e-3\n2.8e-3\n7.9e-3\n4.4e-3\n1.5e-2\n2.6e-2\n1.3e-2\n2.7e-3\n3.6e-3\n3.9e-3\n1.9e-3\n5.9e-3\n5.4e-3\n1.5e-2\n2.1e-2\n1.3e-2\n1.3e-3\n3.9e-3\n4.9e-1\n4.8e-1\n5.0e-1\n4.9e-1\n5.3e-1\n4.7e-1\n4.7e-1\n4.8e-1\n4.9e-1\n5.1e-1\n5.2e-1\n5.2e-1\n5.1e-1\n5.6e-1\n5.1e-1\n5.0e-1\n5.2e-1\n5.2e-1\n9.7e-4\n6.0e-4\n6.9e-4\n3.5e-4\n6.5e-4\n6.3e-4\n7.3e-4\n2.4e-4\n1.7e-3\n2.2e-3\n5.2e-4\n2.0e-3\n4.5e-4\n6.9e-4\n5.9e-4\n6.1e-4\n1.4e-4\n1.2e-4\n4.4e-4\n2.7e-3\n1.7e-3\n2.5e-4\n1.2e-3\n3.6e-4\n2.2e-4\n3.4e-4\n5.6e-4\n3.5e-4\n6.6e-4\n4.3e-4\n1.9e-3\n1.3e-3\n1.7e-4\n1.2e-3\n4.5e-4\n6.9e-4\n4.3e-4\n9.4e-4\n3.8e-4\n4.5e-4\n6.0e-4\n1.9e-3\n1.2e-3\n3.3e-4\n1.9e-3\n7.3e-4\n7.4e-4\n7.8e-4\n5.5e-4\n6.0e-4\n5.0e-4\n6.9e-4\n2.9e-3\n1.3e-3\n4.5e-4\n1.7e-3\n2.3e-4\n5.4e-4\n4.5e-4\n7.6e-4\n3.6e-4\n2.3e-4\n4.1e-4\n2.6e-3\n1.4e-3\n1.1e-4\n5.9e-3\n1.1e-2\n1.9e-2\n2.8e-2\n1.2e-2\n1.5e-2\n2.9e-2\n3.5e-3\n5.6e-2\n2.3e-2\n2.9e-3\n1.5e-2\n6.1e-3\n3.4e-2\n4.0e-2\n1.5e-2\n2.2e-2\n4.0e-2\n8.6e-3\n7.3e-2\n3.9e-2\n4.7e-3\n6.2e-3\n5.7e-3\n7.1e-3\n1.7e-2\n6.0e-3\n5.0e-3\n1.9e-2\n1.5e-3\n3.9e-2\n7.8e-3\n1.1e-3\n1.4e-2\n1.5e-2\n5.0e-3\n5.0e-3\n1.9e-2\n1.6e-2\n1.7e-2\n3.0e-3\n2.1e-2\n1.5e-2\n1.1e-3\n7.2e-3\n4.8e-3\n2.0e-2\n2.6e-2\n6.2e-3\n1.3e-2\n2.7e-2\n2.7e-3\n5.5e-2\n2.5e-2\n8.6e-4\n5.9e-3\n3.9e-3\n2.0e-2\n2.5e-2\n5.8e-3\n8.6e-3\n3.0e-2\n5.0e-3\n5.2e-2\n1.8e-2\n3.3e-3\n2.0e-3\n4.4e-3\n3.2e-3\n3.7e-3\n8.2e-3\n3.0e-2\n1.8e-2\n1.4e-3\n5.0e-3\n8.3e-3\n3.6e-3\n1.0e-2\n5.9e-3\n1.3e-2\n1.7e-2\n1.1e-2\n1.7e-3\n6.8e-3\n1.2e-2\n8.2e-3\n5.5e-3\n1.1e-2\n4.4e-3\n4.0e-2\n2.9e-2\n4.0e-3\n1.3e-2\n1.2e-2\n6.8e-3\n6.4e-3\n1.7e-2\n1.8e-2\n2.8e-2\n2.8e-2\n2.9e-3\n1.2e-2\nLogistic regression\nXGBoost\nResNet\nTransformer\nLung cancer\nDementia\nBipolar disorde\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nGerman DA\nJMDC\nMDCD\nMDCR\nCCAE\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nIPCI\nAUSOM\nOptum EHR\nClinformatics\nSTARR-OMOP\nCUIMC\nValidation database\nDevelopment database\nEavg\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n \n \n7 \nReferences \n1. \nde Ridder MA, de Wilde M, de Ben C, Leyba AR, Mosseveld BM, Verhamme KM, et al. Data resource \nprofile: the integrated primary care information (IPCI) database, The Netherlands. Int J Epidemiol. 2022;51(6):e314-\ne23. \n2. \nGorishniy Y, Rubachev I, Khrulkov V, Babenko A. Revisiting deep learning models for tabular data. Adv \nNeural Inf Process Syst. 2021;34:18932-43. \n \n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-14",
  "updated": "2024-10-14"
}