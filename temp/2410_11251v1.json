{
  "id": "http://arxiv.org/abs/2410.11251v1",
  "title": "Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning",
  "authors": [
    "Jiaheng Hu",
    "Zizhao Wang",
    "Peter Stone",
    "Roberto Martín-Martín"
  ],
  "abstract": "A hallmark of intelligent agents is the ability to learn reusable skills\npurely from unsupervised interaction with the environment. However, existing\nunsupervised skill discovery methods often learn entangled skills where one\nskill variable simultaneously influences many entities in the environment,\nmaking downstream skill chaining extremely challenging. We propose Disentangled\nUnsupervised Skill Discovery (DUSDi), a method for learning disentangled skills\nthat can be efficiently reused to solve downstream tasks. DUSDi decomposes\nskills into disentangled components, where each skill component only affects\none factor of the state space. Importantly, these skill components can be\nconcurrently composed to generate low-level actions, and efficiently chained to\ntackle downstream tasks through hierarchical Reinforcement Learning. DUSDi\ndefines a novel mutual-information-based objective to enforce disentanglement\nbetween the influences of different skill components, and utilizes value\nfactorization to optimize this objective efficiently. Evaluated in a set of\nchallenging environments, DUSDi successfully learns disentangled skills, and\nsignificantly outperforms previous skill discovery methods when it comes to\napplying the learned skills to solve downstream tasks. Code and skills\nvisualization at jiahenghu.github.io/DUSDi-site/.",
  "text": "Disentangled Unsupervised Skill Discovery\nfor Efficient Hierarchical Reinforcement Learning\nJiaheng Hu\nUniversity of Texas at Austin\njiahengh@utexas.edu\nZizhao Wang\nUniversity of Texas at Austin\nzizhao.wang@utexas.edu\nPeter Stone†\nUniversity of Texas at Austin, Sony AI\npstone@cs.utexas.edu\nRoberto Martín-Martín†\nUniversity of Texas at Austin\nrobertomm@cs.utexas.edu\nAbstract\nA hallmark of intelligent agents is the ability to learn reusable skills purely from\nunsupervised interaction with the environment. However, existing unsupervised\nskill discovery methods often learn entangled skills where one skill variable si-\nmultaneously influences many entities in the environment, making downstream\nskill chaining extremely challenging. We propose Disentangled Unsupervised\nSkill Discovery (DUSDi), a method for learning disentangled skills that can be\nefficiently reused to solve downstream tasks. DUSDi decomposes skills into dis-\nentangled components, where each skill component only affects one factor of the\nstate space. Importantly, these skill components can be concurrently composed\nto generate low-level actions, and efficiently chained to tackle downstream tasks\nthrough hierarchical Reinforcement Learning. DUSDi defines a novel mutual-\ninformation-based objective to enforce disentanglement between the influences of\ndifferent skill components, and utilizes value factorization to optimize this objective\nefficiently. Evaluated in a set of challenging environments, DUSDi successfully\nlearns disentangled skills, and significantly outperforms previous skill discovery\nmethods when it comes to applying the learned skills to solve downstream tasks.\nCode and skills visualization at jiahenghu.github.io/DUSDi-site/.\n1\nIntroduction\nReinforcement learning (RL) algorithms have achieved many successes in challenging tasks, including\nmagnetic plasma control [11], automobile racing [54], and robotics [47]. However, applying existing\nRL algorithms to every new task in a tabula rasa manner often results in low sample efficiency that\nlimits RL’s broader applicability [18]. Unsupervised skill discovery holds the promise of improving\nthe sample efficiency of Reinforcement Learning, by learning a set of reusable skills through reward-\nfree interaction with the environment that can be later recombined to tackle multiple downstream tasks\nmore efficiently. In practice, prior unsupervised RL skills are represented as a policy that conditions\non a skill variable to generate diverse behaviors, and have led to successful and efficient learning of\ndownstream tasks when combined with skill fine-tuning or hierarchical RL skill selection [13, 24, 58].\nDespite prior successes, a common limitation of the skills learned by existing unsupervised RL\nmethods is that they are entangled: any change in the skill variable causes the agent to induce changes\nin multiple dimensions of the state space simultaneously. Learning to use and recombine these\nentangled skills can be extremely hard for an agent trying to solve downstream tasks, especially in\n†Equal supervision.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2410.11251v1  [cs.LG]  15 Oct 2024\nPrior Works\nDUSDi (ours)\nFigure 1: Consider an agent practicing driving skills by learning to control a car’s speed (length of orange arrow),\nsteering (curvature of orange arrow), and headlights (blue symbol), (Left) previous unsupervised skill discovery\nmethods learn entangled skills, where a change in the skill variable can cause all three environment factors to\nchange (Right) DUSDi learns disentangled skills with concurrent components, where each skill component only\naffects one factor of the state space, enabling efficient downstream task learning with hierarchical RL.\ncomplex domains like multi-agent systems or household humanoid robots, where the agent needs to\nconcurrently change multiple independent dimensions of the state to complete the task. For example,\nconsider an agent learning to operate a car: if a single skill variable simultaneously changes the speed,\nsteering, and headlights of the car, it will be extremely challenging for the agent to learn how to\nturn on/off the headlights while keeping the car at the right speed and direction. In contrast, humans\nnaturally have the ability to concurrently and independently adjust the car’s acceleration, steering,\nand headlights based on the car’s current speed, surroundings, and lighting conditions. In other words,\nhumans naturally obtain disentangled skill components where each component only affects one or\nfew state variables, and can easily recombine these skill components into compositional skills [2] to\ncontrol multiple factors simultaneously.\nIn this work, we aim to create such a mechanism for artificial agents to learn disentangled skills\nthat facilitate solving downstream tasks. We introduce Disentangled Unsupervised Skill Discovery\n(DUSDi), a novel method for unsupervised discovery of disentangled skills. A key insight of\nDUSDi is to take advantage of state factorization that is naturally available in unsupervised RL\nenvironments [13, 35, 17] (e.g. speed, direction, and lighting conditions of the car in the driving\nexample; the state of different objects in a household environment). These factored state spaces\nprovide a natural inductive bias we leverage for disentanglement: DUSDi decomposes skills into\ndisentangled components, and encourages each skill component to affect only one state factor while\ndiscouraging it from affecting any other factors. To that end, DUSDi designs a novel intrinsic reward\nfor unsupervised skill learning based on mutual information (MI) between disentangled skills and\nstate factors: the learning agent receives high rewards for 1) increasing the MI between a state\nfactor and the skill component assigned to change it, and 2) for decreasing the MI between that skill\ncomponent and all other state factors.\nDUSDi introduces a set of technical innovations to tractably and efficiently optimize the proposed\nmutual information objective. Once the DUSDi skills are learned, they can be used as the low-level\npolicy in a hierarchical reinforcement learning (HRL) setting to tackle downstream tasks. Compared\nto using entangled skills, a key benefit of using the disentangled DUSDi skills is that they guarantee\nmore efficient exploration during downstream task learning and therefore often lead to significantly\nbetter performance. Furthermore, the structured skill space of DUSDi opens up additional possibilities\nto inject domain knowledge into the learning process to further improve the efficiency of both skill\nlearning and downstream task learning.\nDUSDi is easy to implement and can be integrated into any MI-based unsupervised skill discovery\napproach. In our experiments, we integrate DUSDi with DIAYN [13] and evaluate the performance\non four domains: a 2D agent navigation domain, a DMC walker domain, a large-scale multi-agent\nparticle domain, and a 3D realistic simulated robotics domain. Our experiments indicate that DUSDi\ncan indeed learn disentangled skills, and significantly outperforms other Unsupervised Reinforcement\nLearning methods on solving complex downstream tasks with HRL.\n2\nreward\n(# state factors)\nRL\ntraining\nenv\n(a)\n(b)\nintrinsic\ndisentangled Q prediction for state factor\nFigure 2: Two learning stages of DUSDi: (a) in disentangled skill learning stage, DUSDi creates a one-to-one\nmapping between state factors and skill components — each disentangled skill component zi only influences\nstate factor si. DUSDi designs a novel mutual-information-based intrinsic reward to enforce disentanglement\nand utilize Q-value decomposition to learn the skill policy πθ efficiently. (b) in the task learning stage, the skill\npolicy is used as a frozen low-level policy and a high-level policy πhigh is learned to select skill z for every L\nsteps, by maximizing the task reward rtask.\n2\nPreliminaries\nFactored Markov Decision Process (f-MDP)\nIn this work, we consider unsupervised skill discov-\nery in a reward-free Factored Markov Decision Process. Following Osband and Van Roy [33], Mohan\net al. [32], we define a Factored Markov Decision Process by the tuple M = (S, A, P), where\nS = S1 × · · · × SN is a factored state space with N factors such that each state s ∈S consists of\nN state factors: s = (s1, . . . , sN), si ∈Si. A is the action space, and P is an unknown Markovian\ntransition model, S × A →S. Notice that a factored state space is often naturally available in\ndomains used by prior works [13, 24, 35, 17, 9, 49] as it can naturally represent environments with\nseparate elements (e.g., objects) that can be changed independently. DUSDi leverages the property\nthat factors often have sparse dynamics dependencies, which opens up the possibility of learning\ndisentangled skills to control the state of each factor. Moreover, many downstream tasks are defined\nby specific changes in one or a few factors (e.g., changing the state of a single object and not others),\nwhich are easier to learn with disentangled skills. In domains with only image-based (unfactored)\nobservations, a factored state space can be extracted using disentangled representation learning or\nobject-centric representation learning methods [31, 20], which we empirically evaluated in Sec. 4.5.\nMutual-Information-Based Skill Discovery\nMutual-information-based skill discovery methods,\nsuch as the paradigmatic DIAYN [13], specify the skills with a latent variable z ∈Z, and learns a\nskill-conditioned policy π(a|s, z). The optimization objective these methods use to learn the skills\nis to maximize the mutual information (MI) between the state, s, and the skill latent variable, z:\nI(S; Z), which incentivizes the agent to reach diverse and distinguishable states. One popular way to\ndetermine the MI, I(S; Z), is to decompose it as I(S; Z) = H(Z) −H(Z | S), where H denotes\nentropy. Since the skill variable is typically sampled from a fixed distribution, H(Z) can be assumed\nconstant: maximizing I(S; Z) is thus equivalent to minimizing H(Z | S). Following the definition\nof conditional entropy, −H(Z | S) = Es,z[log p(z|s)], DIAYN proposes to approximate p(z|s) with\na learned discriminator q(z|s) that predicts the skill latent, z, given the state, s.\nAfter discovering the skills, mutual-information-based methods apply them to learn downstream\nreward-supervised tasks. Many methods (e.g., DIAYN) adopt a hierarchical RL structure for this\nsecond phase, where the skill policy is used as a low-level “frozen” element, and a high-level policy\nπhigh(z|s) learns to sequentially activate skill z based on observations. The high-level policy is trained\nto maximize the provided task reward, R, with Z as the action space.\n3\nLearning Disentangled Skills with DUSDi\nSimilar to prior works in unsupervised skill discovery, DUSDi implements a two-stage learning\nprocedure for the agents: in the first phase, DUSDi develops a library of skills without external\nreward (Sec. 3.1). The key to DUSDi’s success is to encourage disentanglement between different\nskill components through a novel learning objective that restricts the effect of each disentangled skill\n3\nAlgorithm 1 DUSDi Skill Learning\n1: Initialize skill policy πθ, discriminators qi\nϕ, qi\nψ and value function Qi for each state factor Si.\n2: for each skill training episode do\n3:\nSample skill z ∼p(z).\n4:\nCollect state transitions with actions from πθ(a|s, z).\n5:\nSample a batch of (s, a, z) from the replay buffer.\n6:\nfor i = 1, . . . , N do\n7:\nUpdate qi\nϕ(zi|si) and qi\nψ(zi|s¬i) with discrimination losses.\n8:\nCalculate ri based on Eq. 4\n9:\nUpdate Qi(s, a, z) with reward ri using SAC.\n10:\nend for\n11:\nUpdate πθ with Q = PN\ni=1 Qi using SAC.\n12: end for\ncomponent to independent factors. In the second phase, DUSDi leverages the learned skills to solve\ndownstream tasks through Hierarchical Reinforcement Learning, achieving higher returns P Rtask\nthan methods with entangled skills (Sec. 3.3). In practice, learning disentangled skills in environments\nwith many factors can be challenging. To address this challenge, we introduce improvements to\nDUSDi’s first phase based on Q-function decomposition (Sec. 3.2). We present the entire DUSDi\npipeline in Fig. 2, and the pseudo-code in Alg. 1.\n3.1\nDisentangled Skill Spaces and Learning Objective\nDUSDi aims to create disentangled skill components that can be easily recombined to solve down-\nstream tasks. To that end, DUSDi proposes a novel factorization of the latent skill conditioning\nvariable, z, into N independent disentangled components such that the latent space Z becomes\nZ = Z1 × · · · × ZN. We equate N to the number of state factors and consider zi ∈Zi the disentan-\ngled skill component that affects state factor i. The skill policy π(a|s, z) takes in z ∈Z, which is a\ncomposition of the skill components.\nWhile the factored latent space Z could be discrete or continuous, we consider discrete skill space in\nthis paper, and discuss how DUSDi can be applied to continuous skills in Appendix A. We can then\nassume that each disentangled component zi takes the form of an integer, zi ∈[1, k], resulting in a\ncompositional skill, z, with the form of a N-dimensional multi-categorical vector with kN possible\nvalues. During skill learning, we independently sample each disentangled component zi from a fixed\nuniform distribution p(zi), similar to previous works [13, 35].\nGiven this factored skill space, our goal is to learn a skill policy network, πθ : S × Z 7→A,\nsuch that each disentangled component Zi affects and only affects the value of a state factor,\nSi. For each disentangled component and state factor pair (Zi, Si), we encourage diverse and\ndistinguishable behaviors by maximizing their mutual information I(Si; Zi). While this objective\nenables a disentanglement skill component to affect the corresponding factor, it does not restrict the\ncomponent from affecting other factors. This is undesirable since the resulting skill components\nwould still be entangled in their effects. To prevent that, we propose to ensure that each skill\ncomponent, Zi, minimally affects the rest of the state factors, S¬i, where S¬i denotes the subspace\nformed by all other state factor spaces except Si: S1 × . . . , Si−1 × Si+1 × · · · × SN. Specifically,\nwe incorporate an entanglement penalty to minimize, I(S¬i; Zi), which corresponds to the mutual\ninformation between a skill component and all other state factors that it should not affect.\nFormally, the skill policy aims to maximize the following objective:\nJ (θ) =\nN\nX\ni=1\nI(Si; Zi) −λI(S¬i; Zi),\n(1)\nwhere λ < 1 is a hyperparameter that controls the importance of the entanglement penalty relative to\nthe skill-factor association. We restrict λ to be smaller than one for the following reason: in some\nenvironments, due to intrinsic dynamical dependencies between state factors themselves, controlling\n4\na state factor, Si, has to introduce some association between Zi and other factors in S¬i, e.g., when\ncontrolling an object whose manipulation requires the agent to use other objects as tools. In these\ncases, as the policy learns to maximize the MI between a skill and a factor, I(Si, Zi), the MI with\nother factors, I(S¬i; Zi), may also increase. For these cases, the use of λ < 1 will ensure that the\nentanglement penalty does not overpower the association reward, and the policy is still incentivized\nto learn disentangled skill components that change Si distinguishably while introducing minimal\nchanges on other factors. In practice, we simply set λ = 0.1 in all our experiments.\nOptimizing DUSDi’s Objective: Directly maximizing the objective in Eq. 1 is intractable. Alter-\nnatively, we propose to approximate the objective using a variational lower bound of the mutual\ninformation [1]:\nI(Si; Zi) = H(Zi) −H(Zi|Si) ≥C + Ez,s log qi\nϕ(zi|si),\n(2)\nwhere C represents the constant value of H(Zi), the entropy of the prior distribution over the skill\nlatent variable, which does not change during training, and qi\nϕ is a variational distribution.\nSimilarly, we can approximate the MI in the entanglement penalty by:\nI(S¬i; Zi) ≥C + Ez,s log qi\nψ(zi|s¬i),\n(3)\nwhere qi\nψ is another variational distribution3. Importantly, when these q approximations perfectly\nrecover the posterior distribution of zi, we obtain equality in Eq. 2 and Eq. 3. In DUSDi, we\nimplement the variational distributions, qϕ and qψ, as neural network discriminators mapping input\nstate factor(s) to the predicted disentangled component values, zi.\nTo optimize J (θ), we alternate between two steps: 1) performing variational inference to train the\ndiscriminators qi\nϕ and qi\nψ through gradient ascent, and 2) using qi\nϕ and qi\nψ to learn a disentangled skill\npolicy πθ through RL by maximizing the following intrinsic reward approximating Eq. 1:\nrz(s, a) ≜\nN\nX\ni=1\nqi\nϕ(zi|si) −λqi\nψ(zi|s¬i)\n(4)\nInterestingly, the decomposed nature of our intrinsic reward allows a convenient avenue for shaping\nskill behaviors based on domain knowledge. In particular, we can restrict a state factor si to only\ntake certain values by constraining qi\nϕ(zi|si) accordingly. While not the main focus of this work, we\nbriefly explore this further optimization enabled by DUSDi in Appendix H.\n3.2\nAccelerating Skill Learning through Q Decomposition\nWhen using reinforcement learning (RL) to optimize the intrinsic reward function defined in Eq. 4,\nstandard RL algorithms treat the reward function as a black box and learn a single value function\nfrom the mixture of intrinsic reward terms. While this approach may be sufficient for environments\nwith few state factors, doing so for complex environments with many state factors (large N) often\nleads to suboptimal solutions. A key reason is that the mixture of 2N reward terms leads inevitably\nto high variance in the reward, making the value of the Q function oscillate. Furthermore, the sum of\nreward terms obscures information about each term’s value, which hinders credit assignment.\nDUSDi overcomes this issue by leveraging the fact that the intrinsic reward function in Eq. 4 is\na linear sum over terms associated with each disentangled component. Thanks to the linearity of\n3While lower-bounding J (θ) requires upper-bounding I(S¬i; Zi), we stick with the variational lower bound\nfor this term because of the complexity in upper bounding MIs [41].\n5\nexpectation, we can decompose the Q function into N disentangled Q functions as follows:\nQπ(s, a, z) = Eθ[\n∞\nX\nt=0\nγtrt]\n= Eθ[\n∞\nX\nt=0\nγt\nN\nX\ni=1\nqi\nϕ(zi|si) −λqi\nψ(zi|s¬i)]\n=\nN\nX\ni=1\nEθ[\n∞\nX\nt=0\nγt(qi\nϕ(zi|si) −λqi\nψ(zi|s¬i))]\n=\nN\nX\ni=1\nQi(s, a, z)\n(5)\nwhere Qi represents each disentangled Q function, one for each disentangled component. The\ndisentangled Q functions can be then updated only with their corresponding intrinsic reward terms,\nri ≜qi\nϕ(zi|si) −λqi\nψ(zi|s¬i). During policy learning, we sum all disentangled Q functions together\nto recover the global critic, Qπ, as shown in Fig. 2 (a), top. Compared to learning Qπ directly from all\n2N reward terms, learning disentangled Q functions significantly reduces reward variance, allowing\nQπ to converge faster and more stably.\n3.3\nDownstream Task Learning\nSimilar to Eysenbach et al. [13], in DUSDi we utilize hierarchical RL to solve reward-supervised\ndownstream tasks with the discovered skills, as depicted in Fig.2 (b). The skill policy, πθ : S × Z →\nA, acts as the low-level policy and is kept constant while a high-level policy, πhigh : S →Z, learns\nto select which skill to execute for L steps using the skill latent variable, z. Thus, the skill latent\nconditioning space, Z, acts as the action space of the high-level policy, πhigh. As extensively evaluated\nin our experiments, without any additional “ingredient”, performing downstream task learning in the\naction space formed by DUSDi skills often results in significantly superior performance compared to\nan action space formed by entangled skills. We show that the superior performance of DUSDi can be\nexplained by more efficient exploration when using the DUSDi skills for hierarchical RL, which we\nelaborate on in Appendix B, through analyzing the benefits and search complexity of DUSDi’s skill\nspace over DIAYN’s.\nDepending on the nature of the downstream tasks, we can often take further advantage of the\ndisentangled skills learned by DUSDi through leveraging its structure. One such scenario is when\nthe downstream task has a composite reward function consisting of multiple terms. Previous works\n[16, 46] have shown that when the causal dependencies from action dimensions to reward terms are\navailable (e.g., the reward for speed only depends on actions that affect speed), one can use Causal\nPolicy Gradient (CPG) to decompose the policy update (e.g., only the “speed actions” get updated\nby the speed reward) and greatly improve sample efficiency, especially when the dependencies are\nsparse. In downstream task learning, with an action space (of the high-level policy) consisting of\nthe skills learned by DUSDi, we have a convenient way of applying causal policy gradient, where\nthe causal dependencies between the action dimensions (i.e., skill components) and reward terms\nare often sparse and can be easily obtained by examining the state factor that a skill component is\nassociated with, which we evaluate empirically in Sec. 4.6.\n4\nExperimental Evaluation\nIn the evaluation of DUSDi, we aim to answer the following questions: Q1: Are skills learned by\nDUSDi truly disentangled (Sec. 4.2)? Q2: Can Q-decomposition improve skill learning efficiency\n(Sec. 4.3)? Q3: Do our disentangled skills perform better when solving downstream tasks compared\nto other unsupervised reinforcement learning methods (Sec. 4.4)? Q4: Can DUSDi be extended to\nimage observation environments (Sec.4.5)? Q5: Can we leverage the structured skill space of DUSDi\nto further improve downstream task learning efficiency (Sec.4.6)?\n6\nTable 1: Evaluation of skill disentanglement based on the DCI metric, shown as mean and standard deviation\nacross skill policies trained with 3 random seeds.\n2D GUNNER\nMULTI-PARTICLE\nIGIBSON\nDUSDi (ours)\nDIAYN-MC\nDUSDi (ours)\nDIAYN-MC\nDUSDi (ours)\nDIAYN-MC\nDisentanglement (↑)\n0.864 ± 0.018\n0.016 ± 0.002\n0.705 ± 0.037\n0.002 ± 0.000\n0.833 ± 0.022\n0.017 ± 0.006\nCompleteness (↑)\n0.864 ± 0.017\n0.024 ± 0.004\n0.750 ± 0.041\n0.003 ± 0.000\n0.834 ± 0.021\n0.019 ± 0.005\nInformativeness (↑)\n0.897 ± 0.012\n0.821 ± 0.010\n0.849 ± 0.052\n0.791 ± 0.032\n0.854 ± 0.006\n0.752 ± 0.015\n4.1\nEvaluation Environments\nPrevious works [13, 34, 35, 44, 23] extensively rely on standard RL environments such as DMC [48]\nand OpenAI Fetch [4] to evaluate unsupervised RL methods. However, unlike previous unsupervised\nskill discovery methods, DUSDi focuses on learning a set of disentangled skill components that\ncan be concurrently executed and re-combined to complete downstream tasks. As such, it only\nmakes sense to examine the performance of DUSDi in challenging tasks that require concurrent\ncontrol of many environment entities (e.g. multi-agent systems, complex household robots). Previous\nenvironments lack this property: in DMC for example, while the state and action space can be very\ncomplex, the predominant downstream tasks are just to move the center-of-mass of the agent to\ndifferent places. In such cases, there is no need for concurrent skill components, and therefore we\ndo not expect large gains from using DUSDi’s disentangled skills. Nevertheless, we include an\nevaluation on the DMC-Walker [48] environment to demonstrate that our method is also applicable\nto those environments, but focus the majority of our evaluation on environments that DUSDi is\ndesigned for, including 2D Gunner, Multi-Particle [30], and iGibson [26].\nThe 2D gunner is a relatively simple domain, where a point agent can navigate inside a continuous\n2D plane, collecting ammo and shooting at targets. Multi-Particle is a multi-agent domain modified\nbased on [30]. In this domain, a centralized controller simultaneously controls 10 heterogenous\npoint-mass agents to interact with 10 stations, where each agent can only interact with a specific\nstation. We evaluate in this domain to test the scalability of our methods to a large number of state\nfactors. iGibson [26] is a challenging simulated robotics domain with the same action space and\ncomplexity as real-world robots, where a mobile manipulator can navigate in a room, inspect the\nroom using its head camera, and interact with electric appliances in the room by pointing a remote\ncontrol to them and switching them on/off. We evaluate in this domain to examine whether our\nmethod can handle home-like environments with complex dynamics. We provide visualizations and\nadditional information about each of the environments in Appendix C.\n4.2\nEvaluating Skill Disentanglement\nFirst, we examine whether the skills learned by DUSDi are truly disentangled (Q1) using the DCI\nmetric proposed by Eastwood and Williams [12]. The DCI metric consists of three terms, namely\ndisentanglement, completeness, and informativeness, explained in detail in Appendix F. In the\noriginal work, measuring DCI requires knowing the ground truth generative factors. In our case,\nthe generative factors are simply the state factors, and we only need to discretize the value of each\nstate factor to make it compatible for evaluation. For each method on each domain, we collect 100K\nrollout steps using the learned skill policy, π(s, z), where the skill is (re)sampled from the uniform\nprior distribution, p(z), every 50 steps. These (state, skill) pairs are then used to calculate DCI.\nWe compare against DIAYN-MC (Multi-channel DIAYN) that uses the same skill representation as\nDUSDi but optimizes the DIAYN objective of I(S; Z), and show results in Table 1. Unsurprisingly,\nDUSDi significantly outperforms DIAYN-MC, especially on Disentanglement and Completeness,\nacross all three environments. These results indicate that DUSDi learns truly disentangled skills,\nenabling efficient downstream task learning, as we will show in Sec. 4.4. We encourage the readers\nto visit our project website for a qualitative visualization of the learned skills.\n4.3\nEvaluating Skill Learning Efficiency with Q-decomposition\nTo examine the importance of Q-decomposition (Q2), we measure the performance of optimizing the\nDUSDi objective during skill learning with and without a decomposed Q network. We compare the\nclassification accuracy of the skill discriminators qi\nϕ(zi|si), averaged over all skill channels, which\n7\n(a) 2D Gunner\n(b) Multi-Particle\n(c) iGibson\nFigure 3: Evaluation of the effect of Q-decomposition in skill learning. The plots depict the mean and standard\ndeviation of accuracy (↑) when predicting the skill component zi based on the state factor si, computed across 3\ntraining processes. The higher prediction accuracy indicates that the policy learns to control more state factors in\nmore distinguishable ways, leading to more efficient downstream task learning.\nindicates progress towards discovering diverse and distinguishable skills, with higher accuracy being\nbetter. We depict our results in Fig. 3. We observe that Q-decomposition has a similar performance to\nthe regular Q network in the simplest 2D gunner domain, but significantly outperforms the regular Q\nnetwork in domains with more state factors (Multi-Particle) and more complex dynamics (iGibson),\nsuggesting that Q-decomposition is necessary for scaling towards complex domains.\n4.4\nEvaluating Downstream Task Learning\nThe promise of DUSDi is to incorporate disentanglement into skills so that the skills can be effectively\nused in downstream task learning. Therefore, the most critical evaluation of our work focuses on\ncomparing the performance of different unsupervised RL methods on task learning (Q3). We\ncompare against existing state-of-the-art unsupervised reinforcment learning algorithms, including\nDIAYN [13], CIC [24], CSD [35], METRA [36], ICM [37], RND [5], ELDEN [51], and Vanilla\nRL [14], where these baselines are further explained in Appendix E.\nSimilar to the evaluation setting in the URLB benchmark [23], we allow each method to train for\n4 million steps without access to reward (i.e., pretraining phase) before the reward is revealed to\nthe agent and the downstream learning takes place. During the pre-training phase, all methods use\nsoft actor-critic (SAC) [14] to optimize the intrinsic reward. For all skill discovery methods (i.e.,\nDUSDi, DIAYN, CIC, CSD, METRA), a skill-conditioned policy, πθ(a|s, z), is learned during the\npretraining phase. During downstream learning, the skill network is fixed, whereas an upper policy,\nπhigh(z|s), is trained using proximal policy optimization (PPO) [43] to optimize the task reward.\nSimilar to previous works [13, 44], we omit proprioceptive states from the MI optimization for all\nskill discovery methods to facilitate more meaningful explorations. For exploration methods (i.e.,\nRND, ICM, ELDEN), a policy πθ(a|s) is learned during the pretraining phase on intrinsic reward\nand fine-tuned using the task reward during the downstream learning phase. The hyperparameters are\nspecified in Appendix G.\nWe evaluate all methods in four environments and 13 downstream tasks, detailed in Appendix D. The\nresults are depicted in Fig. 4. As expected, DUSDi performs similarly to previous unsupervised RL\nmethods in the DMC walker environment due to the simplicity in terms of its downstream objectives\n(all related to center-of-mass locomotion), but significantly outperforms all previous methods on\ndomains where downstream tasks require coordinative control of multiple state factors. The most\ncrucial comparison is between DUSDi and DIAYN. DIAYN is a special case of DUSDi where\nthere is only one state factor (consisting of the entire state) and one skill component. Therefore\ncomparing against DIAYN offers a straightforward examination of the effect of disentangled skills\nfor downstream task learning. DUSDi significantly outperforms DIAYN in all downstream tasks,\ndemonstrating the effectiveness of using disentangled skills. In general, we found exploration-based\nmethods to be less capable than skill discovery methods, possibly due to their lack of temporal\nabstraction. CIC performs very poorly, likely because the CIC objective does not explicitly encourage\ndistinguishable skills and instead generates the intrinsic reward solely based on state entropy, making it\nvery hard for the upper policy to select the right skill. This result again shows the importance of having\na proper skill representation. DUSDi also outperforms CSD and METRA on most downstream tasks,\nespecially on the more complex and high-dimensional domains, like Multi-Particle. This superiority is\nperhaps surprising considering that in our experiments, DUSDi only relies on the simple DIAYN-style\nintrinsic reward for skill discovery, but further demonstrates the importance of learning a disentangled\nskill space. It is important to notice that many techniques proposed to improve skill discovery quality\n8\n(a) Walker-run\n(b) Walker-goal\n(c) 2DG-unlim\n(d) 2DG-lim\n(e) MP-seq-easy\n(f) MP-seq-medium\n(g) MP-seq-hard\n(h) MP-fp-easy\n(i) MP-fp-medium\n(j) MP-fp-hard\n(k) IG-look\n(l) IG-housekeep\n(m) IG-inspect\nFigure 4: Training curves of DUSDi and baselines on multiple downstream tasks (reward supervised second\nphase). The plots depict the mean and standard deviation of the return of each method over 3 random seeds.\nDUSDi outperforms all baselines that learn entangled skills, converging faster and to higher returns.\n(e.g., Baumli et al. [3], Zhao et al. [57]), can be seamlessly incorporated into DUSDi. Therefore, we\nexpect our method to perform even better as new advances are made in unsupervised skill discovery.\n4.5\nExtending DUSDi to Image Space\nAlthough this paper primarily focuses on applying DUSDi to factored state space, we can straightfor-\nwardly extend it to image space through existing works in factored / object-centric representation\nlearning [29, 20, 53, 28, 55] (Q4). We empirically illustrate this capability in the Multi-Particle envi-\nronment, where we replace the low-dimensional state observation with 64 × 64 image observations.\nSpecifically, we first pretrain an object-centric encoder following Yang et al. [55], and then use our\nmethod on top of the extracted representation to learn disentangled skills. Hence, essentially, the\nskill policy uses images as observation. As shown in Fig. 5, when learning from image observation,\nDUSDi achieves similar performance to learning from state space, whereas the baseline methods are\nunable to learn these two tasks even when learning from the low-dimensional state space as in Fig. 4.\n4.6\nLeveraging Structure of DUSDi Skills\nWhile DUSDi can already learn downstream tasks quite efficiently, it is possible to further improve\nthe sample efficiency of downstream task learning through leveraging the structured skill space of\nDUSDi (Q5), as described in the second paragraph of Sec.3.3. Specifically, we apply Causal Policy\nGradient [16] to the Multi-Particle domain, where the causal dependencies between state factors and\nreward terms are easy to identify. We present our results in Fig. 6, where the sample efficiency of\ndownstream task learning is greatly improved thanks to the structured skill space of DUSDi.\n5\nRelated Work\nUnsupervised Skill Discovery In unsupervised skill discovery, the goal of an agent is to learn\ntask-agnostic skills without external rewards. To learn such skills, previous methods propose various\nforms of intrinsic reward: (1) maximizing the mutual information between visited states and the skill\nvariables [13, 44, 6, 24], (2) maximizing the traveled distance along the direction specified by the\nskill variables [34–36], (3) learning to reach a diverse set of goals [52, 40, 38]. These skills can be\nused to boost the sample efficiency of downstream task learning, for example, (1) using hierarchical\nRL where a high-level policy learns to select which skill to execute [13], or (2) using the skill policy\nto initialize the task solving policy and then fine-tuning it [24].\n9\n(a) MP-fp-medium\n(b) MP-fp-hard\nFigure 5: Performance of DUSDi with image ob-\nservations on two multi-particle downstream tasks\nover three random seeds. With the help of dis-\nentangled representation learning, DUSDi effec-\ntively learns skills based only on image observa-\ntions and leverages the skills to solve challenging\ndownstream tasks where baseline methods fail.\n(a) MP-fp-medium\n(b) MP-fp-hard\nFigure 6: Performance of DUSDi in two multi-\nparticle downstream tasks when combined with\nCausal Policy Gradient (CPG, orange). The disen-\ntangled skills of DUSDi provide opportunities for\nleverage structure and speed up downstream task\nlearning, greatly improving the sample efficiency\nwhen learning downstream tasks.\nState Space Factorization in RL In RL, there is a long history of leveraging state factorization,\nincluding learning a world model between state factors for planning [22, 50], augmenting data [39],\nand providing intrinsic rewards [42, 17, 9]. Relevant to our work are skill discovery methods that\nlearn to either reach a goal for each controllable object [19, 9] or achieve interactions between a\npair of specified objects [8]. Though these methods achieve disentanglement by influencing one or\na pair of objects during a skill, they do not apply to tasks that require controlling multiple objects\nsimultaneously, like driving where we need to control the car’s speed and heading directions at the\nsame time. In contrast, our method can combine disentangled skill components into concurrent skills\n[10] to solve a wide range of tasks.\nDisentanglement in Skill Learning Inspired by the benefits of compositionality, disentanglement\nhas been extensively studied, mainly in learning image representations [28]. There are a few works\ninvestigating disentanglement in unsupervised skill discovery. Lee et al. [25] consider a special case\nof disentangled skills — for a multi-arm robot, learning independent skills for each arm. However,\nthey rely on manually factored action spaces which is an assumption that often limits the behavior\nof the agent. Kim et al. [21] encourage the disentanglement between different dimensions of the\nskill variable by regularizing it with β-VAE objective [15], but Locatello et al. [27] point out that\nsuch regularization is impossible to achieve disentanglement. To learn disentangled skills, Song et al.\n[45] learns a decoder from skill variables to state trajectories and their generation factors, which is\nthen used to train the skill policy through imitation learning. However, their training of the decoder\nrequires pre-collected trajectories and corresponding generation factors, whereas our method is fully\nunsupervised with no expert data.\n6\nConclusion\nWe present DUSDi, an unsupervised skill discovery method for learning disentangled skills by lever-\naging the factorization of the state space. DUSDi designs a skill space that exploits the factorization\nof the state space and learns a skill-conditioned policy where each sub-skill affects only one state\nfactor. DUSDi enforces disentanglement through an intrinsic reward based on mutual information,\nand shows superior performance on a set of downstream tasks with naturally factored state spaces\ncompared to baselines and state-of-the-art unsupervised RL methods.\nOne limitation of DUSDi is the assumption of access to a factored state space. While a factored state\nspace is naturally available in many existing RL environments, and can be extracted from images\nas we have shown in our experiment (Sec. 4.5), we believe that future advances in disentangled\nrepresentation learning will greatly broaden the applicability of DUSDi towards partially observable,\npixel-based environments. Secondly, DUSDi primarily focuses on learning a structured skill space\nfor more efficient downstream learning, and its exploration capability during skill learning is largely\ndetermined by the specific algorithm used to optimize for our mutual information objective. While\nwe used DIAYN [13] in this work due to its simplicity, it would be interesting to examine extending\nthe idea of learning disentangled skills to other skill discovery methods, e.g., Zhao et al. [57], Laskin\net al. [24], including those that are not based on mutual information [35, 56].\n10\nAcknowledgements\nThis work took place at the Learning Agents Research Group (LARG) and\nthe Robot Interactive Intelligence Lab (RobIn) at UT Austin. RobIn is supported in part by DARPA\nTIAMAT program (HR0011-24-9-0428). LARG research is supported in part by NSF (FAIN-\n2019844, NRT-2125858), ONR (N00014-18-2243), ARO (W911NF-23-2-0004), Lockheed Martin,\nand UT Austin’s Good Systems grand challenge. Peter Stone serves as the Executive Director of\nSony AI America and receives financial compensation for this work. The terms of this arrangement\nhave been reviewed and approved by the University of Texas at Austin in accordance with its policy\non objectivity in research.\nReferences\n[1] David Barber and Felix Agakov. Information maximization in noisy channels: A variational\napproach. Advances in Neural Information Processing Systems, 16, 2003.\n[2] André Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Aygün, Philippe Hamel,\nDaniel Toyama, Shibl Mourad, David Silver, Doina Precup, et al. The option keyboard:\nCombining skills in reinforcement learning. Advances in Neural Information Processing\nSystems, 32, 2019.\n[3] Kate Baumli, David Warde-Farley, Steven Hansen, and Volodymyr Mnih. Relative variational\nintrinsic control. In Proceedings of the AAAI conference on artificial intelligence, volume 35,\npages 6732–6740, 2021.\n[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[5] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random\nnetwork distillation, 2018.\n[6] Víctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giró-i Nieto, and\nJordi Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In\nInternational Conference on Machine Learning, pages 1317–1327. PMLR, 2020.\n[7] Jongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang Shane Gu. Varia-\ntional empowerment as representation learning for goal-based reinforcement learning. arXiv\npreprint arXiv:2106.01404, 2021.\n[8] Jongwook Choi, Sungtae Lee, Xinyu Wang, Sungryull Sohn, and Honglak Lee. Unsupervised\nobject interaction learning with counterfactual dynamics models. In Workshop on Reincarnating\nReinforcement Learning at ICLR 2023, 2023.\n[9] Caleb Chuck, Kevin Black, Aditya Arjun, Yuke Zhu, and Scott Niekum. Granger-causal\nhierarchical skill discovery. arXiv preprint arXiv:2306.09509, 2023.\n[10] Cédric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Autotelic agents with\nintrinsically motivated goal-conditioned reinforcement learning: a short survey, 2022.\n[11] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco\nCarpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al.\nMagnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):\n414–419, 2022.\n[12] Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of\ndisentangled representations. In International conference on learning representations, 2018.\n[13] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you\nneed: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.\n[14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In International\nconference on machine learning, pages 1861–1870. PMLR, 2018.\n11\n[15] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. In International conference on learning representations,\n2016.\n[16] Jiaheng Hu, Peter Stone, and Roberto Martín-Martín. Causal policy gradient for whole-body\nmobile manipulation. arXiv preprint arXiv:2305.04866, 2023.\n[17] Jiaheng Hu, Zizhao Wang, Peter Stone, and Roberto Martin-Martin. Elden: Exploration via\nlocal dependencies. arXiv preprint arXiv:2310.08702, 2023.\n[18] Jiaheng Hu, Rose Hendrix, Ali Farhadi, Aniruddha Kembhavi, Roberto Martin-Martin, Peter\nStone, Kuo-Hao Zeng, and Kiana Ehsan. Flare: Achieving masterful and adaptive robot policies\nwith large-scale reinforcement learning fine-tuning. arXiv preprint arXiv:2409.16578, 2024.\n[19] Xing Hu, Rui Zhang, Ke Tang, Jiaming Guo, Qi Yi, Ruizhi Chen, Zidong Du, Ling Li, Qi Guo,\nYunji Chen, et al. Causality-driven hierarchical structure discovery for reinforcement learning.\nAdvances in Neural Information Processing Systems, 35:20064–20076, 2022.\n[20] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin Ahn. Object-centric slot diffusion. arXiv\npreprint arXiv:2303.10834, 2023.\n[21] Jaekyeom Kim, Seohong Park, and Gunhee Kim. Unsupervised skill discovery with bottleneck\noption learning. arXiv preprint arXiv:2106.14305, 2021.\n[22] Thomas Kipf, Elise Van der Pol, and Max Welling. Contrastive learning of structured world\nmodels. arXiv preprint arXiv:1911.12247, 2019.\n[23] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang,\nLerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark, 2021.\n[24] Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter\nAbbeel. Cic: Contrastive intrinsic control for unsupervised skill discovery. arXiv preprint\narXiv:2202.00161, 2022.\n[25] Youngwoon Lee, Jingyun Yang, and Joseph J Lim. Learning to coordinate manipulation skills\nvia skill behavior diversification. In International conference on learning representations, 2019.\n[26] Chengshu Li, Fei Xia, Roberto Martín-Martín, Michael Lingelbach, Sanjana Srivastava, Bokui\nShen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, C. Karen\nLiu, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese. igibson 2.0: Object-centric\nsimulation for robot learning of everyday household tasks, 2021.\n[27] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard\nSchölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning\nof disentangled representations. In international conference on machine learning, pages 4114–\n4124. PMLR, 2019.\n[28] Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and\nMichael Tschannen. Weakly-supervised disentanglement without compromises. In International\nConference on Machine Learning, pages 6348–6359. PMLR, 2020.\n[29] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg\nHeigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with\nslot attention, 2020.\n[30] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent\nactor-critic for mixed cooperative-competitive environments. Neural Information Processing\nSystems (NIPS), 2017.\n[31] Sindy Löwe, Phillip Lippe, Francesco Locatello, and Max Welling. Rotating features for object\ndiscovery. arXiv preprint arXiv:2306.00600, 2023.\n[32] Aditya Mohan, Amy Zhang, and Marius Lindauer. Structure in reinforcement learning: A\nsurvey and open problems. arXiv preprint arXiv:2306.16021, 2023.\n12\n[33] Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps.\nAdvances in Neural Information Processing Systems, 27, 2014.\n[34] Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, and Gunhee Kim. Lipschitz-\nconstrained unsupervised skill discovery. arXiv preprint arXiv:2202.00914, 2022.\n[35] Seohong Park, Kimin Lee, Youngwoon Lee, and Pieter Abbeel. Controllability-aware unsuper-\nvised skill discovery. arXiv preprint arXiv:2302.05103, 2023.\n[36] Seohong Park, Oleh Rybkin, and Sergey Levine. Metra: Scalable unsupervised rl with metric-\naware abstraction. arXiv preprint arXiv:2310.08887, 2023.\n[37] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven explo-\nration by self-supervised prediction, 2017.\n[38] Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain\nexploration for long horizon multi-goal reinforcement learning. In International Conference on\nMachine Learning, pages 7750–7761. PMLR, 2020.\n[39] Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally\nfactored dynamics. Advances in Neural Information Processing Systems, 33:3976–3990, 2020.\n[40] Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey\nLevine.\nSkew-fit: State-covering self-supervised reinforcement learning.\narXiv preprint\narXiv:1903.03698, 2019.\n[41] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational\nbounds of mutual information. In International Conference on Machine Learning, pages\n5171–5180. PMLR, 2019.\n[42] Cansu Sancaktar, Sebastian Blaes, and Georg Martius. Curious exploration via structured\nworld models yields zero-shot object manipulation. Advances in Neural Information Processing\nSystems, 35:24170–24183, 2022.\n[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms, 2017.\n[44] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-\naware unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019.\n[45] Wonil Song, Sangryul Jeon, Hyesong Choi, Kwanghoon Sohn, and Dongbo Min. Learning\ndisentangled skills for hierarchical reinforcement learning through trajectory autoencoder with\nweak labels. Expert Systems with Applications, page 120625, 2023.\n[46] Thomas Spooner, Nelson Vadori, and Sumitra Ganesh. Factored policy gradients: Leveraging\nstructure for efficient learning in momdps, 2021.\n[47] Chen Tang, Ben Abbatematteo, Jiaheng Hu, Rohan Chandra, Roberto Martín-Martín, and Peter\nStone. Deep reinforcement learning for robotics: A survey of real-world successes. arXiv\npreprint arXiv:2408.03539, 2024.\n[48] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David\nBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin\nRiedmiller. Deepmind control suite, 2018.\n[49] Zizhao Wang, Jiaheng Hu, Caleb Chuck, Stephen Chen, Roberto Martín-Martín, Amy Zhang,\nScott Niekum, and Peter Stone. Skild: Unsupervised skill discovery guided by local depen-\ndencies. In Workshop on Reinforcement Learning Beyond Rewards@ Reinforcement Learning\nConference 2024.\n[50] Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal dynamics learning for\ntask-independent state abstraction. arXiv preprint arXiv:2206.13452, 2022.\n[51] Zizhao Wang, Jiaheng Hu, Peter Stone, and Roberto Martín-Martín. Elden: exploration via\nlocal dependencies. Advances in Neural Information Processing Systems, 36, 2024.\n13\n[52] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and\nVolodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv\npreprint arXiv:1811.11359, 2018.\n[53] Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, and Animesh Garg. Slotformer: Unsuper-\nvised visual dynamics simulation with object-centric models. arXiv preprint arXiv:2210.05861,\n2022.\n[54] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,\nThomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.\nOutracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):\n223–228, 2022.\n[55] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae:\nDisentangled representation learning via neural structural causal models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 9593–9602, 2021.\n[56] Rushuai Yang, Chenjia Bai, Hongyi Guo, Siyuan Li, Bin Zhao, Zhen Wang, Peng Liu, and\nXuelong Li. Behavior contrastive learning for unsupervised skill discovery, 2023.\n[57] Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state\nintrinsic control. arXiv preprint arXiv:2103.08107, 2021.\n[58] Yifeng Zhu, Peter Stone, and Yuke Zhu. Bottom-up skill discovery from unsegmented demon-\nstrations for long-horizon robot manipulation. IEEE Robotics and Automation Letters, 7(2):\n4126–4133, 2022.\n14\nA\nExtension to Continuous Skills\nFor continuous skills, we can simply define each skill component zi as a m-dimensional continuous\nvector (and therefore the length of the skill would be m × N, where N is the total number of skill\ncomponents). Now we can define the prior distribution for each skill component distribution p(zi) as\nan m-dimensional continuous uniform distribution (e.g. U[−1, 1]). Notice that our objective remains\nunchanged, as the MI is well-defined no matter whether the skill is discrete or continuous. Lastly,\nwe need to change the output head of our skill prediction networks q, such that instead of outputting\na categorical distribution, it will output a continuous probability distribution (e.g. a multivariate\nGaussian distribution with diagonal covariance).\nB\nEntangled vs. Disentangled Skill Components for Policy Learning\nCompared to entangled skills, the advantages of using disentangled components mainly reside in\nan easier exploration in the skill space. For skill spaces of equivalent capacity, the DIAYN latent\nskill variable is a single integer between 1 and kN, and the DUSDi skill variable is a N-dimensional\nvector with each dimension representing a disentangled component with k possible values. In this\nsection, we analyze the benefits and search complexity of DUSDi’s space over DIAYN’s for two main\ncases: when there are no dynamical dependencies between state factors (optimal case for disentangled\ncomponents) and where there are intrinsic dependencies between state factors.\nState Factors without Dynamical Dependencies:\nIn this case, for DIAYN to find the correct\nskill to execute at the current time step, in the worst case, it needs to iterate through all skills,\nresulting in 1-step exploration sample-efficiency of O(kN). In contrast, for DUSDi, as disentangled\ncomponents are independent of each other, with one skill trial, the agent can simultaneously observe\nthe effects of setting each disentangled component as Zi = zi. Hence, for an intelligent agent, to\nunderstand the effects of each disentangled component at the current state, it only needs to sweep\nthrough each disentangled component space with k trials (e.g., setting all disentangled components\nZi = 1, . . . , k). After that, as the effects of each disentangled component are independent, by\ncompositing disentangled components in novel ways, the agent has the ability to imagine the effects\nof all skills, leading to O(k) exploration efficiency.\nState Factors with Dynamical Dependencies:\nWhen there are dynamical dependencies, we denote\nPAi as parent indices of state factors that Si depends on, e.g., when moving a mouse (Si), SPAi\ndenotes the hand. In such cases, the effect of Zi is conditioned on the value of ZPAi, and we\nneed to iterate through all (Zi, ZPAi) pairs to observe all possible influences on Si. As a result,\nthe exploration is constrained by the state factor with the largest number of parents. Denoting\n|PAi| as the number of parent factors for Si, the exploration sample-efficiency is O(k1+maxi |PAi|).\nWe can see that the O(k) efficiency when there is no dynamical dependencies is a special case\nof maxi |PAi| = 0. Despite lower efficiency than O(k), in many environments, the dynamics of\neach state factor only depend on a small number of other factors, i.e., maxi |PAi| ≪N. Hence,\nexploration with disentangled components is still more sample-efficient than using entangled skills.\nC\nEnvironment Details\nWe test DUSDi on four environments, where a visualization of each of the environments is presented\nin Fig. 7.\n2D Gunner: Shown in Fig. 7 (a), the blue star marks the position of the agent, the blue line marks\nits shooting direction, the red diamond marks ammo location, and the orange cross marks the target\nposition. The agent has a 7-dimensional observation space, consisting of 3 state factors: [Agent\nPosition, Ammo State, Target State]. The action is 5-dimensional, 2 for agent movement, 2 for ammo\npickup, and 1 for shooting direction.\nDMC-Walker: Shown in Fig. 7 (b), a 6 degree-of-freedom robot can locomote on a 2D plane through\njoint motions. The agent has a 26-dimensional observation space consisting of 3 state factors: [Body\nPosition, Body Velocity, Robot Proprioception].\n15\n(a) 2D Gunner\n(b) DMC Walker\n(c) Multi-Particle\n(d) iGibson\nFigure 7: Environments Visualization\nMulti-Particle: Shown in Fig. 7 (c), the agents are marked by small circles, while the stations are\nmarked by large circles. Only stations and agents of the same color can interact with each other. The\nMulti-Particle environment has a 70-dimensional observation space, consisting of 20 state factors.\nThe state factors include states for each landmark and states for each agent. The action space is\n50-dimensional, with 5 dimensions per agent that control their motions and interactions with the\nlandmarks.\niGibson: Shown in Fig. 7 (d), iGibson has 42-dimensional observation space consisting of 4 state\nfactors, including [Agent Location, Electric Appliances State, Object(s) in View, Robot Propriocep-\ntion]. The action space is 11-dimensional, consisting of base velocity (2D), head motion (2D), arm\nmotion (6D), and gripper motion (1D).\nD\nDownstream Tasks\nDMC-Walker (Walker):\n• Run: In this task, the walker agent is rewarded for moving forward at a particular velocity.\n• Goal Reaching: In this downstream task, the agent has to reach randomly generated goal positions.\n2D Gunner (2DG):\n• Unlimited Ammo (unlim): In this downstream task, a set of targets will randomly appear, where\nthe agent needs to navigate to a position close to the target and shoot them in order to score. The\nammo is unlimited so the agent does not need to worry about picking up ammo.\n• Limited Ammo (lim): This downstream task is different from the “unlimited ammo” in that the\nagent starts with no ammo and needs to pick up ammo in order to shoot. Everything else is identical.\nMulti-Particle (MP):\n• Sequential interaction (seq) (easy, medium, hard): In this task, agents need to sequentially interact\nwith their corresponding station following an instruction sequence given at the start of each episode.\nInteracting with stations in the wrong order will be penalized. The easy version of this task has a\nsequence length of 2, while medium and hard have a sequence length of 5 and 8 respectively.\n• Food-poison (fp) (easy, medium, hard): In this downstream task, each station will offer either\nfood or poison to the corresponding agent. Each agent needs to decide whether to interact with\nits corresponding station based on a sequence of binary indicators provided to the agents. The\ndifficulty level has the same meaning as in the sequential interaction task.\niGibson (IG):\n• Look around: In this task, the robot needs to look at objects in the room sequentially.\n• Appliances inspection: In this task, the robot needs to navigate to different electric appliances,\nand test whether each of them is working correctly by pointing a remote control towards it.\n• Housekeeping: In this task, the robot needs to manage the electric appliances intelligently. Specifi-\ncally, the robot needs to first look at a screen to receive instructions. Depending on the instruction,\nthe robot needs to turn on / off certain electric appliances using the remote control.\n16\nE\nBaseline Methods\nDuring downstream task evaluation, we compared against the following state-of-the-art unsupervised\nRL methods:\n• DIAYN [13] represents skill variable z as an integer between 1 to kN and learns skills by maximiz-\ning I(S; Z), the MI between Z and all state factors S.\n• CIC [24] learns a state representation with contrastive learning and learns skills by maximizing\ntransition entropy in the representation space.\n• CSD [35] learns skills maximizing distance traveled along the direction of z in the state space,\nwhere distance is measured in a controllability-aware manner.\n• METRA [36] learn a set of behaviors that collectively cover as much of the state space as possible\nthrough optimizing a Wasserstein variant of the state-skill Mutual Information.\n• ICM [37]: encourages visiting novel states by using prediction errors of action consequences as\nintrinsic rewards.\n• RND [5] encourages visiting novel states by using prediction errors of features computed from a\nrandomly initialized network as intrinsic rewards.\n• ELDEN [17] operates in a factored state space similar to our approach, and encourages visiting\nstates that induce novel factor dependencies.\n• SAC [14] where no pretraining is used, and vanilla RL is directly applied to tackle the downstream\ntasks.\nF\nEvaluating Skill Disentanglement Details\nThe DCI metric consists of three terms, namely disentanglement, completeness, and informative-\nness. In the context of this work, disentanglement (↑) measures, on average, to what extent each skill\ncomponent only affects a single state factor. Completeness score (↑) measures, on average, to what\nextent each state factor is only influenced by a single skill component. Informativeness score (↑)\nmeasures the repeatability of learned skills: given the skill z, how accurately we can predict which\nstates will be visited. We refer the reader to the work by Eastwood and Williams [12] for a detailed\ndiscussion of these metrics and how they are calculated.\nG\nHyperparameters\nSkill Dimensions: For all skill learning methods with discrete skills (i.e. DUSDi, DIAYN), we make\nsure that they have equivalent capacity. Specifically, for igibson and 2D gunner, each DUSDi skill\nconsists of 3 skill components, each component with 5 possible values. As a result, DIAYN skill is an\ninteger between 1 to 125 in these two domains. The only exception is Multi-Particle, where DUSDi\nhas ten sub-skills, each with 5 possible values. Since skill as an integer between 1 and 510 = 9765625\nis obviously challenging for DIAYN to converge, we set the number of discrete skills to be 4096 for\nDIAYN. For continuous skills (i.e. CSD, CIC, METRA), we follow the skill dimensions specified in\nthe original papers (64D for CIC, 3D for CSD and METRA), which were shown to be effective for\nthe respective methods.\nSkill Learning Parameters: All skill learning methods in our baselines use SAC to optimize for\nthe intrinsic reward, with the same policy and value network architecture. DUSDi applies additional\ndecomposition and masking to the value networks, as described in Section. 3.2, which is not applicable\nto the baseline methods. Due to Q-decomposition, when using the same value network architecture,\nDUSDi’s value network capacity is N times of the capacity of other methods’ value networks\n(including when comparing the variations of DUSDi, i.e., no decomposition). For a fair comparison,\nwe also tried to increase value network capacity for other methods to match the capacity for DUSDi,\nbut found that their skill/task learning performances do not improve significantly. This suggests\n(1) that, for skill learning, reward variance, rather than network capacity, is the key reason for no\nQ-composition variation of DUSDi to converge slowly, and (2) that, for task learning, disentangled\nskills, rather than network capacity, is what make DUSDi significantly outperform baselines.\nWe present the hyperparameters for SAC in Table. 2. All methods use a low-level step size of L = 50.\n17\nTable 2: Hyperparameters of Skill Learning.\nName\nValue\nSAC\noptimizer\nAdam\nactivation functions\nReLu\nlearning rate\n1 × 10−4\nbatch size\n1024\ncritic target τ\n0.01\nMLP size\n[1024, 1024]\nsteps per update\n2\n# of environments\n4\nTemperature α\n0.02\nlog std bounds\n[-10, 2]\nDownstream Hierarhical Learning: For all skill discovery methods, downstream learning of the\nskill selection policy is implemented with PPO. We used the same hyperparameters for all methods\nacross all tasks, as specified in Table. 3.\nTable 3: Hyperparameters of Downstream Learning.\nName\nValue\nPPO\noptimizer\nAdam\nactivation functions\nTanh\nlearning rate\n1 × 10−4\nbatch size\n32\nclip ratio\n0.1\nMLP size\n[128, 128]\nGAE λ\n0.98\ntarget steps\n250\nn steps\n20\n# of environments\n4\n# of low-level steps L\n50\nDownstream Finetuning: For all non-skill discovery methods, downstream learning is done using\nthe same hyperparameters as pretraining (table. 2), replacing the intrinsic reward with the task reward.\nH\nBehavior Restriction of Skills via Domain Knowledge\nDue to the decomposable nature of the intrinsic reward of DUSDi, we can conveniently restrict the\nbehavior of skills by constraining the skill predictor qi\nϕ(zi|si) for a particular state factor i. For\nexample, if we want si to stay within a certain range, we can set qi\nϕ(zi|si) to be a uniform distribution\nfor all si not within this range, effectively discouraging the agent from going out of range. In the\nextreme case, we can fully specify the mapping between zi and si, essentially resulting in performing\ngoal-conditioned RL for state i (as also pointed out in [7]) while performing DUSDi for the rest of\nthe state factors.\nWe qualitatively examine this idea in the iGibson domain. By restricting a mobile manipulator to only\nlocomote in regions that are close to a whiteboard, our robot successfully learns diverse board-wiping\nbehaviors which are otherwise extremely hard to learn. Visualizations of the learned skills can be\nseen on our project website.\n18\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: This paper’s main contribution is on unsupervised reinforcement learning for\nefficient downstream task learning, which is also the focus of the abstract and introduction\nsections.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The limitations are clearly discussed in the last paragraph of conclusion.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n19\nAnswer: [Yes]\nJustification: The main results are empirical, while the theoretical results are proofed in the\npaper.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provided all hyperparameters in Appendix G, as well as our codebase.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n20\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We provided our codebase with detailed instructions. This paper does not use\nany existing dataset.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: All hyperparameters are specified in the Appendix G.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: We include standard deviation for all quantitative our results (Sec. 4).\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n21\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We provide computer resources in the Appendix G.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: It conforms with the NeurIPS Code of Ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: This paper presents work whose goal is to advance the field of Machine\nLearning. There are many potential societal consequences of our work, none which we feel\nmust be specifically highlighted here.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n22\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We explicitly cites all environments that we used in this paper.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n23\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This paper does not release new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: the paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: the paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n24\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2024-10-15",
  "updated": "2024-10-15"
}