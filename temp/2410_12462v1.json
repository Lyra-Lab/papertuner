{
  "id": "http://arxiv.org/abs/2410.12462v1",
  "title": "Bridging the Language Gaps in Large Language Models with Inference-Time Cross-Lingual Intervention",
  "authors": [
    "Weixuan Wang",
    "Minghao Wu",
    "Barry Haddow",
    "Alexandra Birch"
  ],
  "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE.",
  "text": "Bridging the Language Gaps in Large Language Models with\nInference-Time Cross-Lingual Intervention\nWeixuan Wang1†\nMinghao Wu2†\nBarry Haddow1\nAlexandra Birch1\n1School of Informatics, University of Edinburgh\n2Monash University\n{weixuan.wang, bhaddow, a.birch}@ed.ac.uk\nminghao.wu@monash.edu\nAbstract\nLarge Language Models (LLMs) have shown\nremarkable capabilities in natural language\nprocessing but exhibit significant performance\ngaps among different languages. Most existing\napproaches to address these disparities rely on\npretraining or fine-tuning, which are resource-\nintensive. To overcome these limitations with-\nout incurring significant costs, we propose\nInference-Time Cross-Lingual Intervention\n(INCLINE), a novel framework that enhances\nLLM performance on low-performing (source)\nlanguages by aligning their internal represen-\ntations with those of high-performing (target)\nlanguages during inference.\nINCLINE ini-\ntially learns alignment matrices using paral-\nlel sentences from source and target languages\nthrough a Least-Squares optimization, and then\napplies these matrices during inference to trans-\nform the low-performing language represen-\ntations toward the high-performing language\nspace. Extensive experiments on nine bench-\nmarks with five LLMs demonstrate that IN-\nCLINE significantly improves performance\nacross diverse tasks and languages, compared\nto recent strong baselines. Our analysis demon-\nstrates that INCLINE is highly cost-effective\nand applicable to a wide range of applications.\nIn addition, we release the code to foster re-\nsearch along this line.1\n1\nIntroduction\nLarge Language Models (LLMs) have achieved\nremarkable success across a variety of natural lan-\nguage processing tasks, demonstrating strong capa-\nbilities in language understanding and generation\n(OpenAI, 2023; Dubey et al., 2024; Mesnard et al.,\n2024; Anthropic, 2024; OpenAI, 2024a,b). How-\never, despite these advancements, most state-of-the-\nart LLMs remain predominantly English-centric,\nexhibiting significant performance gaps among dif-\nferent languages (Petrov et al., 2023; Kumar et al.,\n† Equal contribution.\n1https://github.com/weixuan-wang123/INCLINE\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\nprojection on direction1\n8\n6\n4\n2\n0\n2\n4\nprojection on direction2\nlang\nen\npt\n(a) Before intervention\n2\n0\n2\n4\nprojection on direction1\n8\n6\n4\n2\n0\n2\nprojection on direction2\nlang\nen\npt\n(b) After intervention\nFigure 1: Bivariate kernel density estimation plots dis-\nplaying the representations (hidden states of the last\ntoken) from 100 random examples in English (blue)\nand their Portuguese translations (orange) from XCOPA\n(Ponti et al., 2020). After intervention using INCLINE,\nthe Portuguese representations are aligned closer to the\nEnglish representations.\n2024), which can adversely affect user experience\nand potentially exclude large portions of the global\npopulation from accessing advanced AI services\n(Lai et al., 2023a; Wang et al., 2024a).\nAddressing the performance gaps across lan-\nguages is highly challenging. Recent approaches\nare mostly data-driven, such as multilingual super-\nvised fine-tuning or continued pre-training (Üstün\net al., 2024; Cui et al., 2023; Kuulmets et al., 2024).\nHowever, collecting and annotating large-scale\ndatasets for numerous underrepresented languages\nis both time-consuming and resource-intensive (Lai\net al., 2023b). Furthermore, training LLMs on mul-\ntilingual data requires substantial computational\nresources, limiting their practicality for widespread\napplications, especially in resource-constrained set-\ntings (Muennighoff et al., 2023; Li et al., 2023a).\nGiven these limitations, a natural question arises:\nHow can we bridge the performance gaps between\nhigh-performing and low-performing languages\nwithout incurring prohibitive costs?\nInspired by Lample et al. (2018) showing that\nword embeddings in different languages can be\n1\narXiv:2410.12462v1  [cs.CL]  16 Oct 2024\naligned to a shared representation space through\nlearned rotations for word translation, we pro-\npose Inference-Time Cross-Lingual Interven-\ntion (INCLINE). This novel framework utilizes\na group of learned alignment matrices that trans-\nform the representations (e.g., hidden states) of\na low-performing (source) language into those of\na high-performing (target) language during infer-\nence. Our framework comprises two main steps.\nFirst, we train the alignment matrices for each\nlayer of LLM using parallel sentences from the\nsource and target languages. The learning process\nis formulated as a Least-Squares optimization prob-\nlem, where these alignment matrices are learned\nby minimizing the distance between the projected\nsource language representations and their corre-\nsponding target language representations, without\nthe need for extensive retraining or fine-tuning the\nLLM. Second, we apply the learned alignment ma-\ntrices to transform the source language input repre-\nsentations into the target language representation\nspace at each layer during inference. By integrating\nthese steps, INCLINE leverages the rich represen-\ntations learned from high-performing languages\nto enhance performance on downstream tasks in-\nvolving low-performing languages. As shown in\nFigure 1, INCLINE effectively aligns the input\nrepresentations in Portuguese to their parallel rep-\nresentations in English.\nIn this study, we conduct extensive experiments\nto validate the effectiveness of INCLINE on nine\nwidely used benchmarks using five LLMs. Our\nresults demonstrate that aligning internal represen-\ntations using INCLINE significantly improves per-\nformance on diverse tasks among languages.\nOur contributions are summarized as follows:\n• We propose INCLINE, a cross-lingual in-\ntervention approach that enhances LLMs by\ntransforming source language representations\ninto a target language representation space\nduring inference without requiring additional\ntraining of LLMs (see Section 3).\n• We conduct extensive evaluations across five\ndiscriminative tasks and four generative tasks,\ncovering 21 languages. Our experimental re-\nsults show that INCLINE significantly im-\nproves model performance, boosting average\naccuracy by up to +4.96 compared to strong\nbaselines (see Section 4).\n• Our detailed analysis indicates that INCLINE\nis highly cost-effective, as it requires minimal\ncomputational resources while delivering sub-\nstantial performance improvements (see Sec-\ntion 5). Moreover, we demonstrate that IN-\nCLINE is effective with regard to LLM back-\nbones, model sizes, and in-context learning,\nunderscoring its general applicability and po-\ntential for broader use in enhancing LLMs for\nunderrepresented languages (see Section 6).\n2\nRelated Work\nMultilingual LLMs\nLLMs are pivotal in multi-\nlingual NLP tasks, typically leveraging external par-\nallel datasets for training (Xue et al., 2021; Muen-\nnighoff et al., 2023; Chung et al., 2024). For low-\nresource languages, data augmentation techniques\ngenerate parallel data by mining sentence pairs or\ntranslating monolingual text using machine transla-\ntion tools (Edunov et al., 2018; Zhao et al., 2021;\nRanaldi et al., 2023). However, these methods\nheavily rely on robust parallel corpora. To reduce\ndata costs, studies have shifted toward Parameter-\nEfficient Fine-Tuning (PEFT) techniques (Pfeiffer\net al., 2020; Parovi´c et al., 2022; Agrawal et al.,\n2023; Wu et al., 2024) and cross-lingual embed-\ndings mapping methods (Mikolov et al., 2013; Or-\nmazabal et al., 2019; Wang et al., 2022), which still\ndemand considerable computational resources.\nMultilingual Prompting\nThere is a growing in-\nterest in methods that do not require parameter ad-\njustments. Prompting techniques have emerged, uti-\nlizing LLMs with multilingual prompts (Lin et al.,\n2021c, 2022; Shi et al., 2022b; Huang et al., 2023).\nHowever, these strategies face challenges like poor\ntranslation quality and prompt framing interference\n(Wang et al., 2024c). Additionally, their effective-\nness varies by task, as recent research indicates that\nfew-shot learning may not outperform zero-shot\nlearning in translation tasks (Hendy et al., 2023).\nIntervention\nTo address these challenges, we\nexplore inference-time intervention techniques as\ncost-effective and efficient alternatives to tradi-\ntional fine-tuning. Prior research in style trans-\nfer (Subramani et al., 2022; Turner et al., 2023),\nknowledge editing (Meng et al., 2022), and truthful-\nness shifting (Li et al., 2023b; Rimsky et al., 2024)\ndemonstrates the potential of linear probe-based\ninterventions. However, these methods have been\nlargely limited to monolingual contexts. Our goal\nis to design a novel cross-lingual inference-time\nintervention that effectively aligns representations\nacross languages, aiming to improve performance\n2\nLately, with gold prices \nup more than 300% \nover the last decade, it \nis harder than ever. \nEnglish text\nUltimamente, com os \npreços do ouro a \nsubirem mais de 300% \nna última década, a \nsituação está mais \ndifícil do que nunca.\nPortuguese text\n𝑾𝒍\n∗\nQuestion: O artigo foi \nembalado em plástico \nbolha. Qual a causa? \nA: Era frágil. \nB: Era pequeno. \nAnswer: \nTest Input\nLayer 1\nLayer 2\nLayer N\nrep.\n…\n…\n…\n…\n…\nLLM\nLLM\nIntervention\nUnaffected states\nIntervened states\nLLM\nLayer    l-1\nℎ𝑙\n𝑠\n𝛼෠ℎ𝑙\n𝑡\n𝛼\n+\nℎ𝑙\n𝑚𝑖𝑥\nB\nA\n(a) Learning the Cross-Lingual Alignment\n(b) Inference-Time Transformation\nLayer    l\n…\n…\nFrozen parameters\nrepresentation in Portuguese\nrepresentation in English\nTransformation\n𝑊1\n∗\n𝑊2\n∗\n𝑊𝑁\n∗\n…\nFigure 2: Framework of INCLINE. INCLINE involves two steps: (a) Learning the Cross-Lingual Alignment:\nsentence representations from a parallel dataset are used to train alignment matrices that map source (Portuguese)\nrepresentations to the target (English) representations. (b) Inference-Time Transformation: this step adapts the\nsource representations from downstream tasks into the target representation space using the alignment matrices.\nacross multiple languages.\n3\nMethodology\nIn Figure 2, we illustrate the framework of IN-\nCLINE, which enhances LLMs through inference-\ntime cross-lingual intervention. Our approach com-\nprises two main steps:\n• Learning the Cross-Lingual Alignment: Us-\ning parallel corpora, we train alignment ma-\ntrices for each layer to map ource language\nrepresentations totarget language representa-\ntions (see Section 3.1).\n• Inference-Time Transformation: During in-\nference, we utilize the learned alignment ma-\ntrices to transform input representations from\nthe source language into the target language\nrepresentation space, thereby improving the\nLLM’s performance on tasks in the source\nlanguage (see Section 3.2).\nBy minimizing the distance between the source\nlanguage representations and their corresponding\ntarget language representations, we effectively re-\nduce cross-lingual representation gaps and align\nrepresentation spaces across languages.\n3.1\nLearning the Cross-Lingual Alignment\nInspired by Schuster et al. (2019) that align embed-\ndings across languages with learned linear trans-\nformations, we aim to learn a cross-lingual align-\nment matrix Wl that aligns sentence representa-\ntions from the source language to the target lan-\nguage at the l-th layer of LLM. Given a parallel\ndataset D = {(xxxs\ni,xxxt\ni)}N\ni=1, where each xxxs\ni is the\ni-th source sentence and xxxt\ni is its corresponding\ntranslation in the target language. Both xxxs\ni and xxxt\ni\nare sequences of tokens. From these sequences,\nwe extract sentence representations by taking the\nhidden state of the last token in each sequence, de-\nnoted as hhhs\ni,l ∈Rd and hhht\ni,l ∈Rd for the source\nand target sentence, respectively, where d is the\ndimensionality of the hidden states.\nTo minimize the difference between the pro-\njected source sentence representations and the tar-\nget sentence representations, our objective can be\ndefined as a Least-Squares optimization problem:\nW ∗\nl = argmin\nWl\nN\nX\ni=1\n\r\rWlhhhs\ni,l −hhht\ni,l\n\r\r2\n(1)\nThis problem seeks the optimal W ∗\nl that aligns the\nsource representations with the target representa-\ntions by minimizing the distance between them.\nHence, the closed-form solution to this optimiza-\ntion problem is:\nW ∗\nl =\n N\nX\ni=1\n(hhhs\ni,l)⊤hhhs\ni,l\n!−1  N\nX\ni=1\n(hhhs\ni,l)⊤hhht\ni,l\n!\n(2)\nBy applying the learned alignment matrix W ∗\nl to\nthe source sentence representations, we effectively\nmap them into the target language’s representation\nspace. This alignment reduces cross-lingual rep-\nresentation discrepancies, allowing the model to\n3\nleverage knowledge from the target language to im-\nprove performance on tasks in the source language.\n3.2\nInference-Time Transformation\nWith the learned alignment matrix W ∗\nl , we can\nenhance the LLM’s processing of source language\ninputs by transforming their representations to the\ntarget representation space during inference.\nWe denote the hidden state of the last token of\nthe test input qqqs in the source language at the l-\nth layer of the LLM as hhhs\nq,l and then project this\nsource language representation into the target rep-\nresentation space using the alignment matrix W ∗\nl :\nˆhhh\nt\nq,l = W ∗\nl hhhs\nq,l\n(3)\nTo perform the cross-lingual intervention at the l-th\nlayer using the intervention vector ˆhhh\nt\nq,l, we adjust\nthe original hidden state in source language hhhs\nq,l by\nblending it with the projected hidden state in target\nlanguage ˆhhh\nt\nq,l. This adjustment is controlled by a\nhyperparameter α, which balances the influence\nbetween the source and target hidden states:\nhhhmix\nq,l = hhhs\nq,l + αˆhhh\nt\nq,l\n(4)\nHere, Equation 4 represents a shift of representa-\ntion of source language towards target language\nrepresentation by a magnitude of α times.\nDecoding with Minimal Intervention\nIn this\nwork, we only conduct one single intervention on\nthe last token of qqqs by replacing hhhs\nq,l with hhhmix\nq,l for\nthe test input qqqs at the l-th layer of LLM. In such a\nway, we can effectively intervene the model output\nwhile preserve the features in the source language.\nComparison with ITI and CAA\nRecently, ITI\n(Li et al., 2023b) and CAA (Rimsky et al., 2024)\nhave been proposed as interventions in the model\nbehaviors by manipulating the selected attention\nheads and hidden states, respectively. INCLINE\nis distinct from ITI and CAA due to three pri-\nmary differences. Firstly, ITI and CAA utilize a\nlearned static intervention vector to alter model\nbehaviors, whereas INCLINE leverages a set of\nalignment matrices to dynamically align input rep-\nresentations from the source language to the target\nlanguage. Secondly, ITI and CAA apply the inter-\nvention vector across all token positions following\nthe instruction, potentially causing excessive per-\nturbation during inference. In contrast, INCLINE\nperforms a single intervention solely on the last\ntoken of the input. Additionally, unlike ITI and\nCAA, which target on only a limited number of lay-\ners, INCLINE modifies the representations across\nall layers. These modifications enable the LLMs\nto comprehensively leverage their target language\ncapabilities for multilingual prediction.\n4\nExperiments\nIn this section, we introduce our experimental setup\n(Section 4.1) and present our results in Section 4.2.\n4.1\nExperimental Setup\nWe present our evaluation tasks, model backbones,\nimplementation details of INCLINE, and baselines\nin this section.\nEvaluation Tasks\nWe conduct extensive evalua-\ntions across nine diverse downstream tasks, catego-\nrized into two groups:\n• Discriminative Tasks: XCOPA (Ponti et al.,\n2020), XStoryCloze (Lin et al., 2021b),\nXWinograd (Lin et al., 2021b), XCSQA (Lin\net al., 2021a), XNLI (Conneau et al., 2018);\n• Generative Tasks:\nMZsRE (Wang et al.,\n2024b),\nFlores\n(Goyal\net\nal.,\n2021),\nWMT23 (Kocmi et al., 2023), MGSM (Shi et al.,\n2022a).\nThese tasks covers 21 languages including English\n(en), Arabic (ar), German (de), Greek (el), Spanish\n(es), Estonian (et), French (fr), Hindi (hi), Indone-\nsian (id), Italian (it), Japanese (ja), Dutch (nl), Por-\ntuguese (pt), Russian (ru), Swahili (sw), Tamil (ta),\nThai (th), Turkish (tr), Ukrainian (uk), Vietnamese\n(vi), and Chinese (zh). We include more details of\nthese tasks in Appendix A.\nModel Backbones\nIn this work, we mainly\nuse BLOOMZ-7B1-MT as our model backbone\nfor all the baseline approaches, unless other-\nwise specified.\nTo demonstrate the effective-\nness of INCLINE across various model back-\nbones, we include four additional LLMs: LLAMA3-\n8B-INSTRUCT (Dubey et al., 2024), LLAMA2-\n7B-CHAT (Touvron et al., 2023), MISTRAL-7B-\nINSTRUCT (Jiang et al., 2023), FALCON-7B-\nINSTRUCT (Almazrouei et al., 2023). We present\nthese results in Section 6. For the MGSM task, we\nemploy the MATHOCTOPUS (Chen et al., 2023),2\na specialized model fine-tuned from LLAMA2-7B\nfor mathematical reasoning tasks, as the backbone.\n2https://huggingface.co/Mathoctopus/Parallel_7B\n4\nXCOPA\nXStoryCloze\nXWinograd\nXCSQA\nXNLI\nµALL\nµSEEN\nµUNSEEN\nµALL\nµSEEN\nµUNSEEN\nµALL\nµSEEN\nµUNSEEN\nµALL\nµSEEN\nµUNSEEN\nµALL\nµSEEN\nµUNSEEN\nBASELINE\n61.62\n69.00\n52.40\n74.96\n77.83\n57.78\n57.05\n59.71\n53.06\n47.35\n55.31\n34.62\n46.48\n50.04\n41.48\nMT-GOOGLE\n73.31†\n73.52\n73.05†\n76.63\n76.05\n80.08†\n57.63\n57.12\n57.90†\n58.52†\n54.84\n64.40†\n50.72\n49.80\n52.00\nMT-LLM\n59.84\n67.16\n50.70\n79.41\n82.23\n62.48\n43.02\n41.67\n45.04\n30.73\n35.38\n23.30\n43.64\n47.83\n37.77\nIntervention Methods\nITI\n60.91\n67.56\n52.60\n76.38\n79.33\n58.70\n48.24\n58.37\n33.06\n46.32\n55.33\n31.92\n46.32\n49.51\n41.84\nCAA\n63.96\n71.80\n54.15\n78.16\n80.92\n61.61\n58.42\n60.70\n55.01\n47.97\n56.01\n35.10\n46.17\n50.92\n39.52\nINCLINE\n65.22\n(+3.60)\n72.56\n(+3.56)\n56.05\n(+3.65)\n79.92\n(+4.96)\n82.03\n(+4.20)\n67.24\n(+9.46)\n59.35†\n(+2.30)\n62.04†\n(+2.33)\n55.32\n(+2.26)\n48.45\n(+1.10)\n56.45†\n(+1.14)\n35.64\n(+1.02)\n48.12\n(+1.64)\n51.44\n(+1.40)\n43.47\n(+1.99)\nSFT\n66.89\n76.84\n54.45\n87.36\n89.50\n74.52\n43.78\n48.63\n36.50\n42.18\n47.95\n32.96\n69.68\n76.76\n59.76\nSFT +INCLINE\n69.24\n(+2.35)\n79.28†\n(+2.44)\n61.22\n(+6.77)\n88.11†\n(+0.75)\n90.00†\n(+0.50)\n76.77\n(+2.25)\n49.84\n(+6.06)\n57.58\n(+8.95)\n38.24\n(+1.74)\n42.55\n(+0.37)\n48.38\n(+0.43)\n33.22\n(+0.26)\n71.17†\n(+1.49)\n77.83†\n(+1.07)\n61.84†\n(+2.08)\nTable 1: Main results of discriminative tasks. All the tasks are evaluated using accuracy. † denotes the best results.\nµALL, µSEEN, and µUNSEEN indicate the macro-average of results across all the languages, the seen languages, and the\nunseen languages, respectively.\nINCLINE (Ours)\nIn this work, we mainly focus\non aligning the low-performing language (source)\nrepresentations closer to the English (target) rep-\nresentations, as LLMs are predominantly English-\ncentric. For training the alignment matrices be-\ntween languages, we randomly sample 500 parallel\nsentence pairs for each language pair involving En-\nglish and other languages. These pairs are sourced\nfrom the News Commentary v16 dataset (Barrault\net al., 2019), and for languages not covered by this\ndataset, we use the CCAligned dataset (El-Kishky\net al., 2020). Following Rimsky et al. (2024), the\nvalue of the α controlling the intervention strength\nis in the range from -1 to 1 and determined by the\nvalidation results for each language across tasks.\nWe use one A100 GPU (40G) for all experiments.\nBaselines\nWe compare INCLINE against sev-\neral established techniques: (1) BASELINE in-\ndicates the predictions given by the original\nBLOOMZ-7B1-MT; (2) MT-GOOGLE utilizes\nGOOGLE TRANSLATE to translate non-English\nquestions into English; (3) MT-LLM leverages\nBLOOMZ-7B1-MT to translate questions in non-\nEnglish languages into English, employing the\nstructured prompt template “{Source Language}:\n{Inputs} English:”; (4) SFT represents the task-\nspecific supervised fine-tuning (SFT) involving up-\ndating all parameters of the LLM on the English\ntraining set for each downstream task individually\nwith the hyperparameters described in Appendix B\nand evaluating the resulting model on the multi-\nlingual test sets; (5) ITI (Li et al., 2023b) is an\nintervention method that identifies attention heads\nwith high linear probing accuracy for truthfulness\nand adjusts activations along these truth-correlated\ndirections during inference. Originally used to shift\nmodels from generating false statements to truthful\nones, we adapt it to encourage the generation of\nEnglish text over non-English text. (6) CAA (Rim-\nsky et al., 2024) employs the mean difference in\nhidden states between positive and negative exam-\nples from additional training data as an intervention\nvector to adjust the model’s behavior towards the\ndesired direction. Initially designed for monolin-\ngual alignment-relevant tasks, we utilize it to shift\nthe model’s output from non-English to English.\n4.2\nResults\nIn this section, we present our results on the dis-\ncriminative tasks (Table 1) and generative tasks\n(Table 2). Furthermore, we also categorize the lan-\nguages involved in the downstream tasks into two\ngroups based on the training data of BLOOMZ-\n7B1-MT: seen languages (ar, es, fr, hi, id, pt, sw,\nta, vi, and zh) and unseen languages (de, el, et, it,\nja, nl, ru, th, tr, and uk). The breakdown results are\nprovided in Table 7 (see Appendix C).\nINCLINE significantly improves discrimina-\ntive task performance.\nThe experimental re-\nsults in Table 1 clearly demonstrate the effective-\nness of INCLINE. Although methods like SFT,\nMT-GOOGLE, and MT-LLM achieve high perfor-\nmance, they come with substantial costs, including\nthe need for extensive fine-tuning of LLMs and\nreliance on third-party tools. Activation interven-\ntion methods, such as ITI and CAA, offer a more\ncost-effective solution but yield only minimal im-\nprovements, indicating a potential inadequacy in\ncapturing the complexities of multilingual tasks.\nIn contrast, INCLINE provides significant perfor-\nmance gains by enhancing multilingual representa-\ntion alignment at inference time without requiring\nextensive resources or dependencies. This results\nin a more efficient improvement in multilingual\n5\nMZsRE\nFlores\nWMT23\nMGSM\nµALL\nµSEEN\nµUNSEEN\nµALL\nµSEEN\nµUNSEEN\nµALL\nµSEEN\nµUNSEEN\nµALL\nµSEEN\nµUNSEEN\nBASELINE\n39.96\n45.79\n32.67\n46.09\n58.57\n21.12\n13.78\n14.39\n13.63\n39.35\n39.80\n38.90\nMT-GOOGLE\n73.56†\n72.76†\n74.56†\n-\n-\n-\n-\n-\n-\n46.70†\n47.70†\n45.70†\nMT-LLM\n33.18\n39.25\n25.61\n-\n-\n-\n-\n-\n-\n21.40\n30.00\n12.80\nIntervention Methods\nITI\n36.31\n41.72\n29.54\n2.85\n2.97\n1.95\n2.34\n3.16\n2.13\n40.50\n41.90\n39.10\nCAA\n42.88\n50.17\n33.78\n47.87\n60.63\n16.75\n13.74\n14.86\n13.46\n39.43\n40.85\n38.00\nINCLINE\n43.22\n(+3.26)\n50.21\n(+4.42)\n34.49\n(+1.82)\n48.19†\n(+2.10)\n61.28†\n(+2.71)\n22.00†\n(+0.88)\n14.23†\n(+0.45)\n15.05†\n(+0.66)\n14.02†\n(+0.39)\n42.85\n(+3.50)\n43.30\n(+3.50)\n42.40\n(+3.50)\nTable 2: Main results of generative tasks. † denotes the best results. µALL, µSEEN, and µUNSEEN indicate the macro-\naverage of results across all the languages, the seen languages, and the unseen languages, respectively. We use\nExact Match (EM) to evaluate MZsRE, use BLEU to evaluate Flores and WMT23, and use accuracy to evaluate MGSM.\nperformance. For example, INCLINE increases\nthe average accuracy by +4.96 on XStoryCloze.\nAdditionally, it delivers improvements of +4.20\nand +9.46 for seen and unseen languages, respec-\ntively. Moreover, INCLINE can further improve\nthe performance of the task-specific SFT.\nINCLINE significantly enhances generative\ntask performance.\nThe experimental results pre-\nsented in Table 2 suggest the effectiveness of IN-\nCLINE in enhancing performance across gener-\native tasks. Unlike ITI and CAA, which show\nonly marginal improvements similar to those ob-\nserved in discriminative tasks, INCLINE appears\nto achieve substantial advancements. Notably, ITI\nseems to struggle significantly in machine transla-\ntion tasks, such as Flores and WMT23, highlighting\nits limitations. Furthermore, INCLINE reportedly\nboosts accuracy in the MGSM task by up to +3.50\nacross various languages. This finding suggests\nthat, although the mathematical capabilities are in-\ndependent from the languages, understanding the\nquestions written in different languages still re-\nquires language-specific knowledge. INCLINE\nsuccessfully transfers the LLMs’ natural language\nunderstanding capabilities from English to other\nlanguages. It is important to note that SFT is not\nevaluated on generative tasks because there are no\ntraining sets associated with these tasks.\nIn summary, these results demonstrate that IN-\nCLINE offers a significant improvement in both\ndiscriminative and generative tasks by effectively\naligning multilingual representations.\n5\nAnalysis\nIn this section, we conduct an in-depth analysis of\nINCLINE, focusing on four key aspects: compu-\ntational costs, enhanced consistency after interven-\ntion, the impacts of the intervened components of\nLLMs, and the choice of intervention strength α.\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\nNumber of training data\n30\n40\n50\n60\n70\nAccuracy\n200\n400\nSeconds\naccuracy\ntime\n(a) Training cost\nar es hi\nid ru sw zh\nLanguage\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCPC\nbaseline\nINCLINE\n(b) Prediction consistency\nFigure 3: (a) Training costs of INCLINE with regard\nto the number of parallel sentences and time used for\ntraining alignment matrices. INCLINE is evaluated on\nXStoryCloze in Swahili. (b) Correct Prediction Con-\nsistency (CPC) between non-English and English on\nXStoryCloze for the model using INCLINE.\nThis analysis provides a comprehensive understand-\ning of how INCLINE operates and its implications\nfor model performance and efficiency.\nINCLINE is highly efficient for training and in-\ntroduces only marginal overhead for inference.\nTo analyze the relationship between computational\ncosts and accuracy, we measure both the training\nand inference costs of our method, INCLINE, us-\ning the XStoryCloze task in Swahili. As shown in\nFigure 3(a), increasing the amount of training data\ndoes not necessarily lead to improved accuracy,\neven though the training time is directly propor-\ntional to the number of samples. In our study, we\nempirically determine that using 500 samples for\ntraining the alignment matrices provides the best\nbalance between performance gains and compu-\ntational costs. Consequently, the training process\ntakes only 172 seconds. During inference, our ap-\nproach involves a single intervention at the last\ntoken, resulting in a time complexity of O(1). This\nmethod incurs only a 12% increase in inference\ntime, taking 0.80 seconds per item compared to\n6\nXCOPA\nXCSQA\nFlores\nMGSM\nBASELINE\n61.60\n47.35\n46.09\n39.35\nINCLINE\nINCLINE-HIDDEN\n65.22\n48.45\n48.19\n42.85\nINCLINE-ATTN\n63.87\n48.18\n47.54\n41.55\nINCLINE-FFN\n64.20\n47.96\n46.10\n41.80\nINCLINE-EMB\n63.16\n47.59\n39.23\n40.90\nTable 3: The averaged results of XStoryCloze, XCSQA,\nFlores, MGSM tasks with four configurations for IN-\nCLINE given by BLOOMZ-7B1-MT.\n0.71 seconds without it, thereby maintaining a low\ninference cost.\nINCLINE effectively enhances the consistency\nof correct predictions between non-English lan-\nguages (source) and English (target).\nRecent\nnon-English test sets are commonly translated from\ntheir English versions, either by humans or ma-\nchines, creating parallel datasets. To quantify the\nalignment between non-English languages (source)\nand English (target), we propose using the Cor-\nrect Prediction Consistency (CPC) rate. This met-\nric measures the proportion of questions correctly\nanswered in both languages, with a higher CPC\nrate indicating better alignment. The results in\nFigure 3(b) demonstrate that CPC significantly im-\nproves after intervention by INCLINE, suggest-\ning that INCLINE effectively aligns non-English\nrepresentations with English ones for more accu-\nrate predictions. Notably, CPC for Swahili (sw)\nincreases from 0.54 to 0.65 with INCLINE, show-\ning its effectiveness for low-resource languages.\nIntervening on hidden states yields the great-\nest performance improvements.\nWe apply IN-\nCLINE to various components of LLMs, including\nthe hidden states (INCLINE-HIDDEN), the outputs\nof attention heads (INCLINE-ATTN), the outputs\nof FFN blocks (INCLINE-FFN), and the embed-\ndings (INCLINE-EMB). The results presented\nin Table 3 indicate that intervening on the hidden\nstates (INCLINE-HIDDEN) leads to the most sig-\nnificant improvements across multilingual tasks.\nThis finding suggests that hidden states can capture\ncomprehensive semantic information that is cru-\ncial for cross-lingual alignment. While INCLINE-\nATTN, INCLINE-FFN, and INCLINE-EMB also\nenhance performance, their performance gains vary\nacross different tasks. These findings justify our\ndesign choice of using hidden states in INCLINE.\n1\n0\n1\n87\n88\n89\n90\nes\nINCLINE\nbaseline\n1\n0\n1\n86\n88\n90\nzh\nstrength \naccuracy\nFigure 4: The accuracy changed with hyperparameter α\non the XStoryCloze task with BLOOMZ-7B1-MT.\nar\nes\nhi\nid\nru\nsw\nzh\nAVG\nBLOOMZ-7B1-MT\nBASELINE\n79.22\n87.89\n76.37\n84.45\n57.78\n50.50\n88.55\n74.96\nINCLINE\n83.12\n90.60\n81.47\n86.10\n67.24\n59.70\n91.20\n79.92\nLLAMA3-8B-INSTRUCT\nBASELINE\n86.50\n91.73\n84.84\n37.46\n66.98\n54.00\n92.39\n73.41\nINCLINE\n87.36\n92.39\n85.31\n64.53\n73.73\n55.66\n92.72\n78.81\nLLAMA2-7B-CHAT\nBASELINE\n49.37\n47.25\n39.25\n48.18\n34.94\n0.93\n55.53\n39.35\nINCLINE\n51.42\n56.65\n47.25\n49.97\n41.03\n17.67\n60.69\n46.38\nMISTRAL-7B-INSTRUCT\nBASELINE\n18.33\n81.34\n24.95\n76.64\n83.65\n2.58\n90.07\n53.94\nINCLINE\n36.71\n84.23\n35.77\n80.18\n85.13\n25.71\n90.34\n62.58\nFALCON-7B-INSTRUCT\nBASELINE\n53.61\n58.31\n53.21\n55.59\n54.60\n51.16\n54.00\n54.35\nINCLINE\n54.33\n61.81\n54.33\n58.04\n57.91\n53.47\n59.70\n57.09\nTable 4: The results of XStoryCloze dataset with five\nLLM backbones.\nThe value of α varies across languages and de-\npends on language relatedness.\nIn this study, we\nintroduce α to control the strength of intervention\nin Equation 4. To investigate the impact of α, we\nconduct a grid search to find the optimal α values\nacross the languages in XStoryCloze. We present\nthe results for Spanish and Chinese in Figure 4.\nWe observe that the optimal α values for these two\nlanguages are opposite: positive for Spanish and\nnegative for Chinese. These findings suggest that\nthe value of α is likely to depend on language relat-\nedness, as both Spanish and English belong to the\nIndo-European language family, while Chinese be-\nlongs to the Sino-Tibetan language family. Results\nfor more languages are provided in Appendix D.\n6\nDiscussions\nIn this section, we conduct a series of experiments\nto investigate how variations in LLMs, model sizes,\nin-context learning, and the data used for training\nalignment matrices affect our results. Addition-\nally, we also explore using French as the target\nlanguage (Appendix E) and examine the effects of\nlayer-specific intervention (Appendix F).\n7\nar\nel\nes\nfr\nhi\nru\ntr\nvi\nzh\nAVG\nBASELINE\n66.59\n15.30\n48.52\n67.86\n71.97\n35.66\n12.38\n40.40\n56.11\n46.09\nINCLINE\n68.68\n15.63\n50.79\n69.93\n76.92\n37.95\n12.42\n43.11\n58.27\n48.19\nINCLINE-FDEV\n73.95\n15.76\n56.11\n75.84\n77.85\n39.33\n12.92\n46.49\n60.19\n50.94\nTable 5: The BLEU results of Flores dataset with INCLINE and INCLINE-FDEV.\n0.56b\n1b\n3b\n7.1b\n0\n10\n20\n30\n40\nEM\nbaseline\nINCLINE\n6\n8\n10\npercentage\nimprovement\nimprovement\nmodel size\n(a) Various model sizes\n0-shot\n2-shot\n4-shot\n0\n20\n40\n60\nEM\n39.96\n47.82\n54.97\n43.22\n49.11\n55.99\nbaseline\nINCLINE\nexample counts\n(b) In-context learning\nFigure 5: (a) Exact Match (left y-axis) and relative\nimprovements over the baseline (right y-axis) on MZsRE\nwith respect to various model sizes of BLOOMZ. (b)\nExact Match score for MZsRE dataset with INCLINE\nbased on the zero-shot setting and few-shot settings\ngiven by BLOOMZ-7B1-MT.\nINCLINE consistently enhances performance\nacross multiple LLMs.\nTo demonstrate the ver-\nsatility of INCLINE across different LLMs, we\napply it to another four high-performing models on\nthe XStoryCloze task. As shown in Table 4, IN-\nCLINE consistently enhances performance com-\npared to the BASELINE. Specifically, we observe\nincreases of +4.96 for BLOOMZ-7B1-MT, +5.40\nfor LLAMA3-8B-INSTRUCT, +7.03 for LLAMA2-\n7B-CHAT, +8.64 for MISTRAL-7B-INSTRUCT, and\n+2.74 for FALCON-7B-INSTRUCT.\nLarger LLMs benefit more from INCLINE.\nBuilding on the work of Wang et al. (2024b), who\ndemonstrates a scaling relationship between the\nsize of backbone models and their performance,\nwe evaluate the impact of different model sizes\nwithin the BLOOMZ series on the MZsRE dataset.\nOur findings, illustrated in Figure 5(a), show that\nthe relative performance gain of INCLINE over\nthe baseline increases with the size of the back-\nbone model. Specifically, the Exact Match (EM)\nscores (in the stacked columns) and the improve-\nment percentages (in the line chart) indicate that\nlarger models in the BLOOMZ series exhibit more\nsignificant enhancements when INCLINE is ap-\nplied. This observation demonstrates that larger\nLLMs can benefit more from INCLINE.\nINCLINE can further enhance model perfor-\nmance when combined with in-context learn-\ning.\nIn-context learning (ICL) has been shown to\nimprove the performance of LLMs on the MZsRE\ntask (Wang et al., 2024b). Building upon this find-\ning, we evaluate the effectiveness of combining\nINCLINE with ICL. As illustrated in Figure 5(b),\nINCLINE demonstrates enhanced performance,\nachieving an additional increase of +1.02 in aver-\nage Exact Match (EM) score with four in-context\nexamples compared to the baseline using ICL alone.\nWhile this improvement is smaller than the +3.26\nincrease observed in the zero-shot setting, it sug-\ngests that the benefits of INCLINE and ICL are\ncomplementary, with both methods capturing fea-\ntures from different perspectives. This highlights\nthe versatility of INCLINE in various applications.\nHigh-quality parallel sentences improve align-\nment in INCLINE.\nWe explore how the qual-\nity of parallel sentences affects the performance\nof INCLINE. By default, the alignment matri-\nces of INCLINE are trained using 500 random\nsamples from the News Commentary dataset. To\nassess the impact of sentence quality, we also\ntrain the alignment matrices using 500 high-quality\nparallel sentences from the development set of\nFlores, which are carefully translated by profes-\nsional human translators. We refer to this vari-\nant as INCLINE-FDEV. In Table 5, INCLINE-\nFDEV significantly outperforms both the standard\nINCLINE and BASELINE, highlighting the impor-\ntance of high-quality parallel sentences.\n7\nConclusion\nIn this paper, we introduce Inference-Time Cross-\nLingual Intervention (INCLINE), an innovative\nframework that bridges the performance gaps be-\ntween high-performing and low-performing lan-\nguages in LLMs. By training alignment matrices to\ntransform source low-performing language repre-\nsentations into the target high-performing language\nrepresentation space, INCLINE enhances perfor-\nmance on underrepresented languages without re-\nquiring additional training or fine-tuning of LLMs.\n8\nExtensive experiments across nine benchmarks and\nfive LLMs demonstrate that, INCLINE delivers\nsignificant improvements by up to +4.96 in terms\nof accuracy compared to strong baselines, while it\nonly requires minimal computational costs.\n8\nLimitations\nWhile INCLINE demonstrates significant enhance-\nment for the multilingual tasks with cross-lingual\nintervention, the alignment matrices are trained for\nspecific pairs of source and target languages. Fu-\nture work will focus on developing multilingual\nalignment matrices that can accommodate multiple\nlanguages simultaneously, reducing the need for\nlanguage pair-specific training and enhancing scal-\nability. Implementing INCLINE requires access\nto the internal layers and representations of LLMs.\nFor proprietary or closed-source models, or models\naccessible only through APIs without exposure of\ninternal representations (e.g., GPT-4o), applying\nthis method may not be feasible. How to perform\ncross-lingual alignment as a plug-and-play tool for\nall LLMs, including those with restricted access,\nrequires further investigation.\nReferences\nPriyanka Agrawal, Chris Alberti, Fantine Huot, Joshua\nMaynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,\nDipanjan Das, and Mirella Lapata. 2023. Qameleon:\nMultilingual qa with only 5 examples. Transactions\nof the Association for Computational Linguistics,\n11:1754–1771.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMérouane Debbah, Étienne Goffinet, Daniel Hesslow,\nJulien Launay, Quentin Malartic, Daniele Mazzotta,\nBadreddine Noune, Baptiste Pannier, and Guilherme\nPenedo. 2023. The falcon series of open language\nmodels. CoRR, abs/2311.16867.\nAnthropic. 2024. Claude 3.5 sonnet.\nLoïc Barrault, Ondˇrej Bojar, Marta R. Costa-jussà,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias Müller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine trans-\nlation (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\nNuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming\nGong, Yangqiu Song, Dongmei Zhang, and Jia Li.\n2023. Breaking language barriers in multilingual\nmathematical reasoning: Insights and observations.\nCoRR, abs/2310.20246.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2024. Scaling instruction-finetuned language models.\nJournal of Machine Learning Research, 25(70):1–53.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for chinese llama and\nalpaca. CoRR, abs/2304.08177.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurélien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozière, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Grégoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR,\nabs/2407.21783.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 489–500. Association for Computational Lin-\nguistics.\nAhmed El-Kishky, Vishrav Chaudhary, Francisco\nGuzmÃ¡n, and Philipp Koehn. 2020. CCAligned:\n9\nA massive collection of cross-lingual web-document\npairs. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2020), pages 5960–5969. Association for\nComputational Linguistics.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2021. The flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Has-\nsan Awadalla. 2023. How good are GPT models\nat machine translation? A comprehensive evaluation.\nCoRR, abs/2302.09210.\nHaoyang Huang, Tianyi Tang, Dongdong Zhang,\nWayne Xin Zhao, Ting Song, Yan Xia, and Furu\nWei. 2023.\nNot all languages are created equal\nin llms:\nImproving multilingual capability by\ncross-lingual-thought prompting.\narXiv preprint\narXiv:2305.07004.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nthée Lacroix, and William El Sayed. 2023. Mistral\n7b. CoRR, abs/2310.06825.\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden,\nOndrej Bojar, Anton Dvorkovich, Christian Fed-\nermann, Mark Fishel, Markus Freitag, Thamme\nGowda, Roman Grundkiewicz, Barry Haddow,\nPhilipp Koehn, Benjamin Marie, Christof Monz,\nMakoto Morishita, Kenton Murray, Makoto Nagata,\nToshiaki Nakazawa, Martin Popel, Maja Popovic,\nand Mariya Shmatova. 2023. Findings of the 2023\nconference on machine translation (WMT23): llms\nare here but not quite there yet. In Proceedings of the\nEighth Conference on Machine Translation, WMT\n2023, Singapore, December 6-7, 2023, pages 1–42.\nAssociation for Computational Linguistics.\nSomnath Kumar, Vaibhav Balloli, Mercy Ranjit, Kabir\nAhuja, Tanuja Ganu, Sunayana Sitaram, Kalika Bali,\nand Akshay Nambi. 2024. Bridging the gap: Dy-\nnamic learning strategies for improving multilingual\nperformance in llms. CoRR, abs/2405.18359.\nHele-Andra Kuulmets, Taido Purason, Agnes Luhtaru,\nand Mark Fishel. 2024. Teaching llama a new lan-\nguage through cross-lingual knowledge transfer. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2024, Mexico City, Mexico, June\n16-21, 2024, pages 3309–3325. Association for Com-\nputational Linguistics.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben\nVeyseh, Hieu Man, Franck Dernoncourt, Trung Bui,\nand Thien Huu Nguyen. 2023a. Chatgpt beyond en-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 13171–13189. Association for Computational\nLinguistics.\nViet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo,\nThuat Nguyen, Franck Dernoncourt, Ryan A. Rossi,\nand Thien Huu Nguyen. 2023b. Okapi: Instruction-\ntuned large language models in multiple languages\nwith reinforcement learning from human feedback.\nIn Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2023 - System Demonstrations, Singapore, December\n6-10, 2023, pages 318–327. Association for Compu-\ntational Linguistics.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio Ran-\nzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In 6th In-\nternational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings. OpenRe-\nview.net.\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji,\nand Timothy Baldwin. 2023a. Bactrian-x : A multi-\nlingual replicable instruction-following model with\nlow-rank adaptation. CoRR, abs/2305.15011.\nKenneth Li, Oam Patel, Fernanda B. Viégas, Hanspeter\nPfister, and Martin Wattenberg. 2023b. Inference-\ntime intervention: Eliciting truthful answers from a\nlanguage model. In Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neu-\nral Information Processing Systems 2023, NeurIPS\n2023, New Orleans, LA, USA, December 10 - 16,\n2023.\nBill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and\nXiang Ren. 2021a.\nCommon sense beyond en-\nglish: Evaluating and improving multilingual lan-\nguage models for commonsense reasoning. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1274–1287,\nOnline. Association for Computational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona T. Diab, Veselin\nStoyanov, and Xian Li. 2021b.\nFew-shot learn-\ning with multilingual language models.\nCoRR,\nabs/2112.10668.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021c.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\n10\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022.\nFew-shot learning with multilingual generative lan-\nguage models. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 9019–9052.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in gpt. Advances in Neural Information Pro-\ncessing Systems, 35:17359–17372.\nThomas Mesnard, Cassidy Hardin, Robert Dadashi,\nSurya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivière, Mihir Sanjay Kale, Juliette Love,\nPouya Tafti, Léonard Hussenot, Aakanksha Chowdh-\nery, Adam Roberts, Aditya Barua, Alex Botev, Alex\nCastro-Ros, Ambrose Slone, Amélie Héliou, Andrea\nTacchetti, Anna Bulanova, Antonia Paterson, Beth\nTsai, Bobak Shahriari, Charline Le Lan, Christo-\npher A. Choquette-Choo, Clément Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya,\nEric Ni, Eric Noland, Geng Yan, George Tucker,\nGeorge-Cristian Muraru, Grigory Rozhdestvenskiy,\nHenryk Michalewski, Ian Tenney, Ivan Grishchenko,\nJacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan,\nJeremy Chen, Johan Ferret, Justin Chiu, and et al.\n2024. Gemma: Open models based on gemini re-\nsearch and technology. CoRR, abs/2403.08295.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for machine\ntranslation. arXiv preprint arXiv:1309.4168.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n15991–16111. Association for Computational Lin-\nguistics.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nOpenAI. 2024a. Hello gpt-4o.\nOpenAI. 2024b. Learning to reason with llms.\nAitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor\nSoroa, and Eneko Agirre. 2019. Analyzing the limi-\ntations of cross-lingual word embedding mappings.\narXiv preprint arXiv:1906.05407.\nMarinela Parovi´c, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2022. Bad-x: Bilingual adapters improve\nzero-shot cross-lingual transfer. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1791–1799.\nAleksandar Petrov, Emanuele La Malfa, Philip H. S.\nTorr, and Adel Bibi. 2023. Language model tok-\nenizers introduce unfairness between languages. In\nAdvances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Pro-\ncessing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer.\narXiv\npreprint arXiv:2005.00052.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXcopa: A multilingual dataset for causal common-\nsense reasoning. arXiv preprint arXiv:2005.00333.\nLeonardo Ranaldi, Giulia Pucci, and André Fre-\nitas. 2023.\nEmpowering cross-lingual abili-\nties of instruction-tuned large language models\nby translation-following demonstrations.\nCoRR,\nabs/2308.14186.\nNina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong,\nEvan Hubinger, and Alexander Matt Turner. 2024.\nSteering llama 2 via contrastive activation addition.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2024, Bangkok, Thailand, August\n11-16, 2024, pages 15504–15522. Association for\nComputational Linguistics.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of contex-\ntual word embeddings, with applications to zero-shot\ndependency parsing. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short\nPapers), pages 1599–1613. Association for Compu-\ntational Linguistics.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, Dipanjan\nDas, and Jason Wei. 2022a. Language models are\nmultilingual chain-of-thought reasoners. Preprint,\narXiv:2210.03057.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022b.\nLanguage models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057.\nNishant Subramani, Nivedita Suresh, and Matthew E.\nPeters. 2022. Extracting latent steering vectors from\npretrained language models. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 566–581.\nAssociation for Computational Linguistics.\n11\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. CoRR, abs/2307.09288.\nAlexander Matt Turner, Lisa Thiergart, David Udell,\nGavin Leech, Ulisse Mini, and Monte MacDiarmid.\n2023. Activation addition: Steering language models\nwithout optimization. CoRR, abs/2308.10248.\nAhmet Üstün, Viraat Aryabumi, Zheng Xin Yong, Wei-\nYin Ko, Daniel D’souza, Gbemileke Onilude, Neel\nBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,\nFreddie Vargus, Phil Blunsom, Shayne Longpre,\nNiklas Muennighoff, Marzieh Fadaee, Julia Kreutzer,\nand Sara Hooker. 2024. Aya model: An instruction\nfinetuned open-access multilingual language model.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2024, Bangkok, Thailand, August\n11-16, 2024, pages 15894–15939. Association for\nComputational Linguistics.\nBin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao,\nYang Ding, AiTi Aw, and Nancy Chen. 2024a. Seae-\nval for multilingual foundation models: From cross-\nlingual alignment to cultural reasoning. In Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers), NAACL 2024, Mexico City, Mexico,\nJune 16-21, 2024, pages 370–390. Association for\nComputational Linguistics.\nWeixuan Wang, Barry Haddow, and Alexandra Birch.\n2024b. Retrieval-augmented multilingual knowledge\nediting. In Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 335–354, Bangkok,\nThailand. Association for Computational Linguistics.\nWeixuan Wang, Barry Haddow, Alexandra Birch, and\nWei Peng. 2024c. Assessing factual reliability of\nlarge language model knowledge. In Proceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Pa-\npers), NAACL 2024, Mexico City, Mexico, June 16-21,\n2024, pages 805–819. Association for Computational\nLinguistics.\nXinyi Wang, Sebastian Ruder, and Graham Neubig.\n2022. Expanding pretrained models to thousands\nmore languages via lexicon-based adaptation. arXiv\npreprint arXiv:2203.09435.\nMinghao Wu, Thuy-Trang Vu, Lizhen Qu, George F.\nFoster, and Gholamreza Haffari. 2024. Adapting\nlarge language models for document-level machine\ntranslation. CoRR, abs/2401.06468.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 483–498. Association\nfor Computational Linguistics.\nWei Zhao, Steffen Eger, Johannes Bjerva, and Isabelle\nAugenstein. 2021. Inducing language-agnostic mul-\ntilingual representations. In Proceedings of *SEM\n2021: The Tenth Joint Conference on Lexical and\nComputational Semantics, *SEM 2021, Online, Au-\ngust 5-6, 2021, pages 229–240. Association for Com-\nputational Linguistics.\n12\nA\nDetails of Datasets\nThe tasks and the corresponding output format,\nprompt template, evaluation metrics, the number\nof languages are shown in Table 6.\nB\nHyperparameters for SFT\nWe fine-tune all parameters of LLMs using the\nAdamW optimizer with a learning rate of 2 × 10−6\nand a batch size of 4. This process is conducted\nover three epochs on four NVIDIA A100 GPUs\n(80GB). During training, we use a linear learning\nrate schedule with a warm-up phase that constitutes\n10% of the total training steps.\nC\nDetailed Results of Intervention\nThe detailed results of BASELINE, MT-GOOGLE,\nMT-LLM, SFT, ITI, CAA INCLINE and SFT\n+INCLINE for each languages across discrimina-\ntive and generative tasks are shown in Table 7.\nD\nThe value of α across languages\nWe explore the optimal value of α for each lan-\nguage in XStoryCloze using grid search, as shown\nin Figure 6.\nE\nProjection to Non-English\nWe have demonstrated the effectiveness of IN-\nCLINE in aligning representations from non-\nEnglish to English. To further prove the generaliz-\nability of INCLINE with another high-performing\nlanguage, we conduct an ablation study aligning\nrepresentations of various languages with French.\nAs shown in Table 8, INCLINE enhances transla-\ntion performance to non-English languages, with\nan average BLEU score increase of +5.35. This fur-\nther demonstrates that INCLINE can effectively\nalign representations across different languages.\nF\nLayer-Specific Intervention\nTo examine the effects of layer-specific interven-\ntions, we conduct a study applying interventions\nacross different layers and evaluated the results\nusing the MZsRE Portuguese test set.\nThe find-\nings, shown in Figure 7, demonstrate how accu-\nracy varies with interventions at different layers.\nIntervening in a single layer (scoring less than 50)\nresulted in lower performance compared to inter-\nventions across all layers (52.09 in Table 7). Ac-\ncording to the trends in Figure 7, interventions in\nthe higher layers lead to greater improvements than\nthose in the lower layers, likely because they miti-\ngate information forgetting. Notably, interventions\nin the hidden states outperform other types signif-\nicantly. However, not every intervention leads to\nperformance gains; both INCLINE-HIDDEN and\nINCLINE-FFN show substantial declines when in-\ntervening in the middle layers. The mechanisms\nunderlying these effects merit further investigation.\nG\nDetails of Visualizing\nFollowing Li et al. (2023b), we use Linear Regres-\nsion to examine multilingual input representations.\nFor each English and corresponding Portuguese\nsample from the News Commentary dataset (a total\nof 500 items), we extract the hidden states at the\nlast token to create a probing dataset for each layer.\nWe randomly divide this dataset into training and\nvalidation sets in a 4:1 ratio and fit a binary linear\nclassifier to the training set. Similar to principal\ncomponent analysis (PCA), we train a second lin-\near probe on the same dataset, constrained to be\northogonal to the first probe. This orthogonality\nensures that the two probes capture distinct aspects\nof the data. Finally, we project the hidden states\nof each sample in the MZsRE test set onto the di-\nrections defined by the probes from the last layer,\nallowing us to visualize and analyze the multilin-\ngual representations effectively.\n13\nDataset\nOutput\nprompt\nMetric\n|L|\nXCOPA\n2-way class\nHere is a premise: \"{premise}\". A: \"{choice1}\" B: \"{choice2}\"\nWhat is the {question}? \"A\" or \"B\"?\nacc.\n10\nXStoryCloze\n2-way class\n{input} What is a possible continuation for the story given the\nfollowing options? A: {quiz1} B: {quiz2}’\nacc.\n8\nXWinograd\n2-way class\n{input} Replace the _ in the above sentence with the correct option:\n- {option1} - {option2}\nacc.\n6\nXNLI\n3-way class\nTake the following as truth: {premise} Then the following statement:\n\"{hypothesis}\" is \"true\", \"false\", or \"inconclusive\"?\nacc.\n13\nXCSQA\nmulti-choice\nQuestion: {question} {choice} Answer:\nacc.\n14\nMZsRE\nanswer\n{context} Quesion: {question} Answer:\nEM\n10\nFlores\nanswer\nTranslate the following sentence from {language} to English: {input}\nBLEU\n10\nWMT23\nanswer\nTranslate the following sentence from {language} to English: {input}\nBLEU\n5\nMGSM\nanswer\nWrite a response that appropriately completes the request in {language}.\nPlease answer in {language}. ### Instruction: {query}### Response:\nEM\n9\nTable 6: The nine datasets used to evaluate multilingual intervention. |L| indicates the number of languages. EM is\nthe Exact Match score and acc. represents the accuracy.\n1\n0\n1\n30\n40\n50\n60\n70\n80\nar\nINCLINE\nbaseline\n1\n0\n1\n87\n88\n89\n90\nes\n1\n0\n1\n60\n65\n70\n75\nhi\n1\n0\n1\n82\n84\n86\nid\n1\n0\n1\n30\n40\n50\n60\nru\n1\n0\n1\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\nsw\n1\n0\n1\n86\n88\n90\nzh\nstrength \naccuracy\nFigure 6: The accuracy changed with hyperparameter α on the XStoryCloze task.\n0\n5\n10\n15\n20\n25\n30\n10\n20\n30\n40\n50\n15\n20\n25\n30\n46\n48\nINCLINE-hidden\nINCLINE-attn\nINCLINE-ffn\nbaseline\nLayer\naccuracy\nFigure 7: The accuracy changed with layer-specific\nintervention, where INCLINE-hidden, INCLINE-attn,\nINCLINE-ffn represents the intervention in the hidden\nstates, in the output of attention heads, in the output of\nthe FFN block.\n14\nDiscriminative tasks\nXCOPA\nen\net\nid\nit\nsw\nta\nth\ntr\nvi\nzh\nAVG\nBASELINE\n76.40\n50.80\n69.60\n58.60\n55.20\n71.60\n50.60\n49.60\n71.20\n77.40\n61.62\nMT-GOOGLE\n-\n75.40† 75.00\n76.00† 76.20†\n62.20\n62.40† 78.40† 76.40\n77.80\n73.31†\nMT-LLM\n-\n44.80\n69.80\n59.40\n60.20\n71.20\n47.40\n51.20\n61.60\n73.00\n59.84\nSFT\n86.40\n50.60\n78.40\n67.80\n59.00\n77.20\n47.60\n53.00\n83.00\n84.60\n66.80\nITI\n-\n50.80\n70.80\n60.00\n55.40\n63.20\n49.00\n50.60\n69.00\n79.40\n60.91\nCAA\n-\n51.20\n72.20\n61.20\n59.20\n73.00\n52.20\n52.00\n74.80\n79.80\n63.96\nINCLINE\n-\n55.40\n73.40\n62.80\n59.80\n73.40\n52.60\n53.40\n76.20\n80.00\n65.22\nSFT +INCLINE\n-\n53.20\n81.20† 65.80\n60.80\n85.00† 54.40\n53.40\n84.40† 85.00†\n69.24\nXStoryCloze\nen\nar\nes\nhi\nid\nru\nsw\nzh\nAVG\nBASELINE\n91.46\n79.22\n87.89\n76.37\n84.45\n57.78\n50.50\n88.55\n74.96\nMT-GOOGLE\n-\n79.48\n81.34\n50.69\n80.81\n80.08† 77.04\n86.96\n76.63\nMT-LLM\n-\n81.80\n86.83\n82.59\n83.59\n62.48\n73.66\n84.91\n79.41\nSFT\n94.11\n90.47\n92.85\n88.22\n91.59\n74.52\n81.14\n92.72\n87.36\nITI\n-\n78.23\n90.54\n80.28\n85.70\n58.70\n52.55\n88.68\n76.38\nCAA\n-\n86.04\n90.47\n79.15\n88.22\n61.61\n52.61\n89.01\n78.16\nINCLINE\n-\n83.12\n90.60\n81.47\n86.10\n67.24\n59.70\n91.20\n79.92\nSFT +INCLINE\n-\n90.93† 92.98† 89.08† 91.99†\n76.77\n81.93† 93.05†\n88.11†\nXWinograd\nen\nfr\nja\npt\nru\nzh\nAVG\nBASELINE\n73.76\n59.04\n51.51\n57.80\n54.60\n62.30\n57.05\nMT-GOOGLE\n-\n61.45\n58.39† 59.32† 57.41\n50.60\n57.63\nMT-LLM\n-\n54.22\n47.86\n33.08\n42.22\n37.70\n43.02\nSFT\n78.06\n62.65\n14.91\n43.35\n58.09\n39.89\n43.78\nITI\n-\n54.22\n51.51\n57.79\n14.60\n63.10\n48.24\nCAA\n-\n60.24\n52.87\n58.17\n57.14\n63.69\n58.42\nINCLINE\n-\n63.86† 53.18\n58.56\n57.46\n63.69†\n59.35†\nSFT +INCLINE\n-\n63.86† 16.48\n46.39\n60.00†\n62.50\n49.84\nXCSQA\nen\nar\nde\nes\nfr\nhi\nit\nja\nnl\npt\nru\nsw\nvi\nzh\nAVG\nBASELINE\n76.50\n52.40\n33.90\n64.30\n63.30\n48.50\n41.30\n36.00\n28.70\n61.30\n33.20\n40.50\n55.20\n57.00\n47.35\nMT-GOOGLE\n-\n61.60† 65.00† 68.00† 67.20†\n32.10\n68.70† 57.30† 66.50† 66.90† 64.50† 19.60\n62.90† 60.40† 58.52†\nMT-LLM\n-\n32.30\n26.30\n42.70\n42.30\n30.40\n25.60\n25.60\n17.40\n39.90\n21.60\n24.00\n31.60\n39.80\n30.73\nSFT\n65.70\n48.20\n32.90\n54.10\n53.60\n43.10\n40.40\n32.60\n29.00\n53.60\n29.90\n31.80\n48.40\n50.80\n42.18\nITI\n-\n52.10\n34.20\n64.50\n63.70\n48.10\n40.00\n25.90\n26.00\n61.20\n33.50\n40.90\n54.90\n57.20\n46.32\nCAA\n-\n52.80\n34.10\n64.50\n63.30\n48.40\n42.20\n36.40\n29.30\n62.80\n33.50\n41.90\n56.00\n58.40\n47.97\nINCLINE\n-\n53.20\n34.90\n65.00\n63.80\n48.80† 42.90\n36.80\n29.80\n62.60\n33.80\n42.20† 57.30\n58.70\n48.45\nSFT +INCLINE\n-\n48.50\n33.30\n54.40\n53.70\n43.90\n40.60\n33.00\n29.30\n53.70\n29.90\n32.50\n49.10\n51.20\n42.55\nXNLI\nen\nar\nde\nel\nes\nfr\nhi\nru\nsw\nth\ntr\nvi\nzh\nAVG\nBASELINE\n54.81\n53.63\n43.33\n41.04\n51.36\n50.54\n50.16\n47.80\n45.01\n40.32\n34.93\n49.68\n49.92\n46.48\nMT-GOOGLE\n-\n51.46\n53.13\n52.71\n51.84\n50.82\n41.58\n51.68\n50.54\n50.50\n52.00\n51.94\n50.42\n50.72\nMT-LLM\n-\n46.87\n43.25\n36.29\n52.12\n51.40\n45.31\n42.08\n43.43\n34.07\n33.17\n47.23\n48.42\n43.64\nSFT\n86.37\n77.17\n68.10\n59.48\n82.71\n81.48\n72.42\n66.87\n67.15\n54.55\n49.80\n77.62\n78.76\n69.68\nITI\n-\n53.69\n45.37\n41.36\n50.18\n51.20\n50.34\n47.74\n43.35\n38.98\n35.77\n48.96\n48.86\n46.32\nCAA\n-\n53.59\n44.67\n41.62\n52.83\n52.75\n50.28\n34.40\n45.75\n40.48\n36.41\n50.32\n50.92\n46.17\nINCLINE\n-\n53.89\n47.74\n41.96\n54.33\n53.11\n50.50† 49.22\n45.99\n41.28\n37.17\n51.12\n51.16†\n48.12\nSFT +INCLINE\n-\n78.44† 71.02† 61.22† 83.07†\n82.14† 73.85† 69.68† 69.14† 55.69† 51.60† 78.64† 79.52†\n71.17†\nGenerative tasks\nMZsRE\nen\nde\nes\nfr\npt\nru\nth\ntr\nvi\nzh\nAVG\nBASELINE\n96.23\n55.05\n48.86\n49.53\n45.49\n30.55\n6.33\n38.76\n51.68\n33.38\n39.96\nMT-GOOGLE\n-\n78.73† 76.18† 75.50† 71.74†\n63.66† 78.47† 77.39† 60.97† 79.41†\n73.56†\nMT-LLM\n-\n49.13\n54.78\n51.28\n6.86\n2.69\n9.69\n40.92\n34.72\n48.59\n33.18\nITI\n-\n53.84\n44.41\n43.34\n41.99\n19.11\n6.59\n38.63\n46.70\n32.17\n36.31\nCAA\n-\n57.07\n53.30\n52.36\n52.76\n31.49\n7.13\n39.43\n55.05\n38.36\n42.99\nINCLINE\n-\n57.20\n53.30\n51.82\n52.09\n31.49\n7.40\n41.86\n55.32\n38.49\n43.22\nFlores\nen\nar\nel\nes\nfr\nhi\nru\ntr\nvi\nzh\nAVG\nBASELINE\n-\n66.59\n15.30\n48.52\n67.86\n71.97\n35.66\n12.38\n40.40\n56.11\n46.09\nITI\n-\n2.39\n2.34\n3.71\n4.40\n3.31\n2.44\n3.03\n3.64\n0.37\n2.85\nCAA\n-\n67.88\n15.92\n54.85\n68.16\n72.98\n38.99\n12.09\n43.01\n56.93\n47.87\nINCLINE\n-\n73.95† 15.79† 56.11† 75.84†\n77.85† 39.33† 12.92† 48.62† 60.19†\n51.18†\nWMT23\nen\nde\nja\nru\nuk\nzh\nAVG\nBASELINE\n-\n18.26\n10.17\n14.73\n11.36\n14.39\n11.78\nITI\n-\n2.75\n1.79\n2.32\n1.66\n3.16\n2.34\nCAA\n-\n16.96\n10.22\n15.11\n11.54\n14.86\n13.74\nINCLINE\n-\n18.85† 10.30† 15.24† 11.71†\n15.05†\n14.23†\nMGSM\nen\nde\nes\nfr\nja\nru\nsw\nth\nzh\nBASELINE\n51.20\n46.40\n42.40\n42.40\n35.20\n38.40\n34.80\n35.60\n39.60\n39.35\nMT-GOOGLE\n-\n46.00\n50.40† 47.20† 44.40†\n46.80† 45.60† 45.60† 47.60†\n46.70†\nMT-LLM\n-\n20.40\n38.80\n32.40\n10.80\n18.40\n22.00\n1.60\n26.80\n21.40\nITI\n-\n46.00\n43.20\n44.80\n35.60\n40.00\n36.80\n34.80\n42.80\n40.50\nCAA\n-\n42.40\n42.00\n40.00\n34.40\n40.80\n36.20\n34.40\n45.20\n39.43\nINCLINE\n-\n48.40† 46.80\n45.20\n37.60\n44.80\n38.00\n38.80\n43.20\n42.85\nTable 7: The overall results of nine NLP tasks with multilingual intervention. † denotes the best results.\n15\nen\nar\nel\nes\nhi\nru\ntr\nvi\nzh\nAVG\nBASELINE\n45.11\n44.70\n15.37\n39.37\n50.18\n36.99\n10.51\n38.77\n42.20\n35.91\nINCLINE\n52.36\n52.33\n15.62\n51.37\n55.40\n39.69\n10.94\n46.48\n47.14\n41.26\nTable 8: INCLINE on the Many-to-French translation task.\n16\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-10-16",
  "updated": "2024-10-16"
}