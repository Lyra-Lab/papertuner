{
  "id": "http://arxiv.org/abs/2410.12598v2",
  "title": "Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach",
  "authors": [
    "Henrique Donâncio",
    "Antoine Barrier",
    "Leah F. South",
    "Florence Forbes"
  ],
  "abstract": "In deep Reinforcement Learning (RL) models trained using gradient-based\ntechniques, the choice of optimizer and its learning rate are crucial to\nachieving good performance: higher learning rates can prevent the model from\nlearning effectively, while lower ones might slow convergence. Additionally,\ndue to the non-stationarity of the objective function, the best-performing\nlearning rate can change over the training steps. To adapt the learning rate, a\nstandard technique consists of using decay schedulers. However, these\nschedulers assume that the model is progressively approaching convergence,\nwhich may not always be true, leading to delayed or premature adjustments. In\nthis work, we propose dynamic Learning Rate for deep Reinforcement Learning\n(LRRL), a meta-learning approach that selects the learning rate based on the\nagent's performance during training. LRRL is based on a multi-armed bandit\nalgorithm, where each arm represents a different learning rate, and the bandit\nfeedback is provided by the cumulative returns of the RL policy to update the\narms' probability distribution. Our empirical results demonstrate that LRRL can\nsubstantially improve the performance of deep RL algorithms for some tasks.",
  "text": "DYNAMIC LEARNING RATE FOR DEEP REINFORCEMENT\nLEARNING: A BANDIT APPROACH\nPREPRINT\nHenrique Donˆancio1, Antoine Barrier2, Leah F. South3, and Florence Forbes1\n1Univ. Grenoble Alpes, Inria, CNRS, Grenoble, France\n2Univ. Grenoble Alpes, Inserm, U1216, Grenoble Institut Neurosciences, GIN, Grenoble, France\n3School of Mathematical Sciences and Centre for Data Science, Queensland University of\nTechnology, Brisbane, Australia\nFebruary 4, 2025\nABSTRACT\nIn deep Reinforcement Learning (RL) models trained using gradient-based techniques, the choice of\noptimizer and its learning rate are crucial to achieving good performance: higher learning rates can\nprevent the model from learning effectively, while lower ones might slow convergence. Additionally,\ndue to the non-stationarity of the objective function, the best-performing learning rate can change\nover the training steps. To adapt the learning rate, a standard technique consists of using decay\nschedulers. However, these schedulers assume that the model is progressively approaching conver-\ngence, which may not always be true, leading to delayed or premature adjustments. In this work,\nwe propose dynamic Learning Rate for deep Reinforcement Learning (LRRL), a meta-learning ap-\nproach that selects the learning rate based on the agent’s performance during training. LRRL is\nbased on a multi-armed bandit algorithm, where each arm represents a different learning rate, and\nthe bandit feedback is provided by the cumulative returns of the RL policy to update the arms’\nprobability distribution. Our empirical results demonstrate that LRRL can substantially improve the\nperformance of deep RL algorithms for some tasks.\n1\nIntroduction\nReinforcement Learning (RL), when combined with function approximators such as Artificial Neural Networks\n(ANNs), has shown success in learning policies that outperform humans in complex games by leveraging exten-\nsive datasets (see, e.g., 37, 20, 43, 44). While ANNs were previously used as value function approximators [32], the\nintroduction of Deep Q-Networks (DQN) by Mnih et al. [25, 26] marked a significant breakthrough by improving\nlearning stability through two mechanisms: the target network and experience replay.\nExperience replay (see 23) stores the agent’s interactions within the environment, allowing sampling of past interac-\ntions in a random way that disrupts their correlation. The target network further stabilizes the learning process by\nperiodically copying the parameters of the learning network. This strategy is necessary because the Bellman update\n—using estimations to update other estimations— would otherwise occur using the same network, potentially causing\ndivergence. By leveraging the target network, gradient steps are directed towards a periodically fixed target, ensur-\ning more stability in the learning process. Additionally, the learning rate1 hyperparameter controls the magnitude of\nthese gradient steps in optimizers such as the Stochastic Gradient Descent (SGD) algorithm, affecting the training\nconvergence.\n1The terms “learning rate” and “step-size” are used interchangeably in the literature and they technically refer to the same\nconcept.\narXiv:2410.12598v2  [cs.LG]  3 Feb 2025\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nThe learning rate is one of the most important hyperparameters, with previous work demonstrating that decreasing its\nvalue during policy finetuning can enhance performance by up to 25% in vanilla DQN [3]. Determining the appropriate\nlearning rate is essential for achieving good model performance: higher values can prevent the agent from learning,\nwhile lower values can lead to slow convergence (see 15, 7, 46). However, finding a learning rate value that improves\nthe model performance requires extensive and computationally expensive testing. In order to adapt its initial choice\nduring training, optimizers such as Adam [18] and RMSProp [41] employ an internal scheme that dynamically adjusts\nthe learning rate, considering, for instance, past gradient information. Therefore, various learning rate scheduling\nstrategies can be combined with the optimizer to decrease the learning rate and improve the convergence over the\ntraining steps.\nStandard learning rate schedulers typically decrease the learning rate based on training progress using, e.g., linear\nor exponential decay strategies [35, 46]. Unlike supervised learning, RL usually involves generating data by trading\noff between exploration (discovery of new states) and exploitation (refining of the agent’s knowledge). As the policy\nimproves, the data distribution encountered by the agent becomes more concentrated, but this evolution occurs at a\ndifferent pace than the overall training progress. For instance, some environments require extensive exploration due\nto the sparseness of rewards, while others need more exploitation to refine the policy to the complexity of the task.\nConsequently, a more sophisticated decaying learning rate strategy that accounts for policy performance rather than\ntraining steps can significantly enhance learning in deep RL.\nContributions.\nIn this work, we propose dynamic Learning Rate for deep Reinforcement Learning (LRRL), a\nmethod to select the learning rate on the fly for deep RL. Our approach acknowledges that different learning phases\nrequire different learning rates, and as such, instead of scheduling the learning rate decay using some blanket approach,\nwe dynamically choose the learning rate using a Multi-Armed Bandit (MAB) algorithm, which accounts for the current\npolicy’s performance. Our method has the advantage of being algorithm-agnostic and applicable to any optimizer. By\nexploiting different configurations, we aim to illustrate the use cases of LRRL and assess its robustness. We provide\nexperiments with the Adam and RMSProp optimizers, with different sets of arms, and two deep RL algorithms (DQN\nand IQN). Tests are carried out across a variety of Atari games, using baselines provided in the Dopamine framework\n(see 9). We also show the effectiveness of LRRL using a standard SGD optimizer in a non-RL optimization context.\nRelated work is detailed in Appendix A. Our main contributions and results can be summarized as follows:\n• We introduce LRRL, a novel approach that leverages a MAB algorithm to dynamically select the learning\nrate during training in deep RL.\n• We assess LRRL robustness across a set of Atari games, using different configurations, optimizers and deep\nRL algorithms. Our empirical results demonstrate that LRRL achieves performance competitive with or su-\nperior to standard learning using fixed learning rates or decay schedulers. We further highlight its advantages\nby considering an SGD optimizer on stationary non-convex landscapes.\n• More generally, we show that LRRL matches the scheduler’s behavior by dynamically choosing lower learn-\ning rate values as the training progresses.\n2\nPreliminaries\nThis section introduces the RL and MAB frameworks, defining supporting notation.\n2.1\nDeep Reinforcement Learning\nAn RL task is defined by a Markov Decision Process (MDP), that is by a tuple (S, A, P, R, γ, T), where S denotes\nthe state space, A the set of possible actions, P : S × A × S →[0, 1] the transition probability, R : S × A →R the\nreward function, γ ∈[0, 1] the discount factor, and T the horizon length in episodic settings (see, e.g., 39 for details).\nIn RL, starting from an initial state s0, a learner called the agent interacts with the environment by picking, at time t,\nan action at depending on the current state st. In return, it receives a reward rt = R(st, at), reaching a new state st+1\naccording to the transition probability P(·|st, at). The agent’s objective is to learn a policy π(·|s) that maps a state s\nto a probability distribution over actions in A, aiming to maximize expected returns Qπ(s, a) = Eπ\n\u0002PT\nt=0 γtrt | s0 =\ns, a0 = a\n\u0003\n.\nTo learn how to perform a task, value function-based algorithms coupled with ANNs [25, 26] approximate the qual-\nity of a given state-action pair Q(s, a) using parameters θ to derive a policy πθ(·|s) = 1a∗(s)(·) where 1a∗(s) de-\nnotes the uniform distribution over a∗(s) with a∗(s) = arg maxa∈A Qθ(s, a). By storing transitions (s, a, r, s′) =\n2\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\n(st, at, rt, st+1) into the replay memory D, the objective is to minimize the loss function defined by:\nJ (θ) = E(s,a,r,s′)∼D\nh\nr + γ max\na′∈A Qθ−(s′, a′) −Qθ(s, a)\ni2\n,\n(1)\nwhere θ−are the target network parameters used to compute the target of the learning network y\n= r +\nγ maxa′∈A Qθ−(s′, a′). The parameters θ−are periodically updated by copying the parameters θ, leveraging sta-\nbility during the learning process by fixing the target y. The minimization, and hence the update of the parameters θ,\nis done according to the optimizer’s routine. A simple possibility is to use SGD using mini-batch approximations of\nthe loss gradient:\nθn+1 ←θn −η∇θJ (θn) , where\n∇θJ (θn) ≈1\n|B|\nX\n(s,a,r,s′)∈B\n2\n\u0000Qθ(s, a) −y\n\u0001\n∇θQθ(s, a),\n(2)\nwith B being a mini-batch of transitions sampled from D and η is a single scalar value called the learning rate. Unlike\nin supervised learning, where the loss function J (θ) is typically stationary, RL presents a fundamentally different\nchallenge: the policy is continuously evolving, leading to shifting distributions of states, actions, and rewards over\ntime. This continuous evolution introduces instability during learning, which deep RL mitigates by employing a large\nreplay memory and calculating the target using a frozen network with parameters θ−. However, stability also depends\non how the parameters θ change during each update. This work aims to adapt to these changes by dynamically\nselecting the learning rate η over the training steps.\n2.2\nMulti-Armed Bandits\nMAB provide an elegant framework for making sequential decisions under uncertainty (see for instance 21). MAB can\nbe viewed as a special case of RL with a single state, where at each round n, the agent selects an arm kn ∈{1, . . . , K}\nfrom a set of K arms and receives a feedback (reward) fn(kn) ∈R. Like RL, MAB algorithms must balance the\ntrade-off between exploring arms that have been tried less frequently and exploiting arms that have yielded higher\nrewards up to time n.\nTo account for the non-stationarity of RL rewards, we will consider in this work the MAB setting of adversarial\nbandits [6]. In this setting, at each round n, the agent selects an arm kn according to some distribution pn while the\nenvironment (the adversary) arbitrarily (e.g., without stationary constraints) determines the rewards fn(k) for all arms\nk ∈K. MAB algorithms are designed to minimize the pseudo-regret GN after N rounds defined by:\nGN = E\n\" N\nX\nn=1\nmax\nk∈{1,...,K} fn(k) −\nN\nX\nn=1\nfn(kn)\n#\n,\nwhere the randomness of the expectation depends on the MAB algorithm and on the adversarial environment,\nPN\nn=1 maxk∈{1,...,K} fn(k) represents the accumulated reward of the (pontentially changing) single best arm, and\nPN\nn=1 fn(kn) is the accumulated reward obtained by the algorithm. A significant component in the adversarial setting\nis to ensure that each arm k has a non-zero probability pn(k) > 0 of being selected at each round n: this guarantees\nexploration, which is essential for the algorithm’s robustness to environment changes.\n3\nDynamic Learning Rate for Deep RL\nIn this section, we tackle the challenge of selecting the learning rate over the training steps by introducing a dynamic\nLRRL. LRRL is a meta-learning approach designed to dynamically select the learning rate in response to the agent’s\nperformance. LRRL couples with SGD optimizers and adapts the learning rate based on the reward achieved by the\npolicy πθ using an adversarial MAB algorithm. As the agent interacts with the environment, the average of observed\nrewards is used as bandit feedback to guide the selection of the most appropriate learning rate throughout the training\nprocess.\n3.1\nSelecting the Learning Rate Dynamically\nOur problem can be framed as selecting a learning rate η for policy updates —specifically, when updating the param-\neters θ after λ interactions with the environment.\nBefore training, a set K = {η1, . . . , ηK} of K learning rates are defined by the user. Then, during training, a MAB\nalgorithm selects, at every round n —that is, at every κ interactions with the environment— an arm kn ∈{1, . . . , K}\n3\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\naccording to a probability distribution pn defined based on previous rewards, as explained in the next section. The\nparameters θ are then updated using the sampled learning rate ηkn. The steps involved in this meta-learning approach\nare summarized in Algorithm 1.\nAlgorithm 1 dynamic Learning Rate for deep Reinforcement Learning (LRRL)\nParameters:\nSet of learning rates K = {η1, . . . , ηK}\nNumber of episodes M\nHorizon length T\nUpdate window λ for the learning network θ\nUpdate window τ for the target network θ−\nUpdate window κ for arm probabilities p\nInitialize:\nParameters θ and θ−\nArm probabilities p0 ←( 1\nK , . . . , 1\nK )\nMAB round n ←0\nCumulative reward R ←0\nEnvironment interactions counter C ←0\nfor episode m = 1, 2, 3, . . . , M do\nfor timestep t = 1, 2, 3, . . . , T do\nChoose action at following the policy πθ(s) with probability 1 −ϵ\n▷(ϵ-greedy strategy)\nPlay action at and observe reward rt\nAdd rt to cumulative reward R ←R + rt\nIncrease environment interactions counter C ←C + 1\nif C mod λ ≡0 then\nif C ≥κ then\nCompute average of the last C rewards fn ←R\nC\nIncrease MAB round n ←n + 1\nCompute weights wn and arm probabilities pn using Equations (3, 4)\nSample arm kn with distribution pn\nReset R ←0 and C ←0\nend if\nUpdate network parameter θ using the optimizer update rule with learning rate ηkn\nEvery τ steps update the target network θ−←θ\nend if\nend for\nend for\nNote that the same algorithm might be used with learning rates schedulers, that is with K = {η1, . . . , ηK} where\nηk : N →R+ is a predefined function, usually converging towards 0 at infinity. If so, the learning rate used at round\nn of the optimization is ηkn(n).\n3.2\nUpdating the Probability Distribution\nAs we expect that the agent’s performance —and hence, the cumulative rewards— will improve over time, the MAB\nalgorithm should receive non-stationary feedback. To take this non-stationary nature of the learning into account,\nwe employ a modified version of the Exponential-weight algorithm for Exploration and Exploitation (Exp3, see 6\nfor an introduction). At round n, Exp3 chooses the next arm (and its associated learning rate) according to the arm\nprobability distribution pn which is based on weights (wn(k))1≤k≤K updated recursively. Those weights incorporate\na time-decay factor δ ∈(0, 1] that increases the importance of recent feedback, allowing the algorithm to respond\nmore quickly to changes of the best action and to improvements in policy performance.\nSpecifically, after picking arm kn at round n, the RL agent interacts C times with the environment and the MAB algo-\nrithm receives a feedback fn corresponding to the average reward of those C interactions. Based on Moskovitz et al.\n[27], this feedback is then used to compute the improvement in performance, denoted by f ′\nn, obtained by subtracting\nthe average of the past j bandit feedbacks from the most recent one fn:\nf ′\nn = fn −1\nj\nj−1\nX\ni=0\nfn−i .\n4\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nThe improvement in performance allows computation of the next weights wn+1 as follows, where initially w1 =\n(0, . . . , 0):\n∀k ∈{1, . . . , K},\nwn+1(k) =\n(\nδ wn(k) + α\nf ′\nn\newn(k)\nif k = kn\nδ wn(k)\notherwise ,\n(3)\nwhere α > 0 is a step-size parameter. The distribution pn+1, used to draw the next arm kn+1, is\n∀k ∈{1, . . . , K},\npn+1(k) =\newn+1(k)\nPK\nk′=1 ewn+1(k′) .\n(4)\nThis update rule ensures that as the policy πθ improves the cumulative reward, the MAB algorithm continues to\nfavor learning rates that are most beneficial under the current policy performance, thereby effectively handling the\nnon-stationarity inherent in the learning process.\n4\nExperiments\nIn the following sections, we examine whether combining LRRL with Adam or RMSProp —two widely used opti-\nmization algorithms— can enhance cumulative returns in deep RL algorithms. We compare LRRL against learning\nmethods both with and without schedulers, using baseline algorithms provided in the Dopamine [9] framework. The\nexperiments involve testing LRRL across various configurations and a selection of Atari games, reporting the mean\nand one-half standard deviation of returns over 5 independent runs. As testing the full Atari benchmark is extremely\ncostly, we choose a representative subset of games guided by their baseline learning curves, which offer insights into\nreward sparsity, stochasticity, and exploration demands. Further details on the evaluation metrics and hyperparameters\nused in these experiments are provided in Appendices C and D. In addition, although our primary target is RL tasks, we\nalso test LRRL using SGD to minimize stationary objective functions to provide a simpler evaluation, as performance\nin RL is highly multi-factorial.\n4.1\nComparing LRRL with Standard Learning\nIn our first experiment, we consider a set of 5 learning rates. We set different configurations of LRRL using these\nvalues and compare them against the DQN agent reaching the best performance, in terms of maximum average return,\namong the possible single rates. More specifically, the learning rates considered are\nK(5) =\n\b\n1.5625 × 10−5, 3.125 × 10−5, 6.25 × 10−5, 1.25 × 10−4, 2.5 × 10−4\t\n.\nThe whole set K(5) of rates is used by one LRRL version, while others are based on the 3 lowest (Klowest(3)), 3 middle\n(Kmiddle(3)), 3 highest (Khighest(3)) values, and the last one (Ksparse(3)) taking the lowest/middle/highest values. All\nexperiments use the Adam optimizer, and we report the return based on the same number of environment iterations.\nThe results in Figure 1 demonstrate that LRRL using the full set K(5) substantially outperforms the best-performing\nsingle learning rate in 2 out of 6 games (Seaquest and Breakout) while remaining competitive in the others. As shown\nin Figure 7 in Appendix B.1, the baseline DQN performance is highly sensitive to the learning rate choice, requiring\nextensive hyperparameter tuning to identify the optimal rate. In contrast, LRRL dynamically selects learning rates\nduring training, avoiding the need for such tuning. Additionally, the 3-arm bandit variants highlight that while LRRL\ncan explore multiple learning rates within a single run, the specific set of rates available still impacts performance.\nOverall, these results illustrate the advantage of LRRL, which achieves similar or better performance than the best-\ntuned DQN baseline while eliminating the computational cost of manual learning rate tuning.\nNext, we illustrate how LRRL adapts during training in response to non-stationary bandit feedback as policy perfor-\nmance improves. Figure 2 shows the systematic sampling of pulled arms (learning rates) and corresponding returns\nover training steps from a single run of LRRL K(5). In most of the tested environments, LRRL behaves similarly to\ntime-decay schedulers by selecting higher learning rates during the early stages of training, gradually shifting toward\narms with lower rates as training progresses.\n4.2\nCombining and Comparing Schedulers with LRRL\nIn the following experiments, LRRL is combined with learning rate schedulers. Specifically, we employ schedulers\nwith exponential decay rate of the form η(n) = η0×e−d n, where η0 is a fixed initial value (common to each scheduler\nand equal to 6.25 × 10−5 in our experiment), d is the exponential decay rate and n is the number of policy updates\n(i.e., of MAB rounds). We define a set of 3 schedulers Ks, where each arm represents a scheduler using a different\n5\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nFigure 1: Comparison between LRRL and standard DQN learning: 5 variants of LRRL with different learning rate sets\n(provided in Appendix D) are tested again the DQN algorithm reaching best performance among the possible learning\nrates. The mean and one-half standard deviations over 5 runs are respectively represented by the curves and shaded\nareas.\nFigure 2: Systematic sampling of normalized learning rates and returns over the training steps using LRRL K(5) with\nAdam optimizer, through a single run. For each episode, we show the selected learning rate using different colors.\nLower rates are increasingly selected over time.\n6\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\ndecay rate d = {1, 2, 3} × 10−7, and compare the results of LRRL with each scheduler individually, using the Adam\noptimizer.\nFigure 3 shows that LRRL combined with schedulers could once again substantially increase the final performance of\n2 out of 6 tasks compared to using a fixed scheduler. In the remaining tasks, LRRL remains at least competitive, while\nfor most of them, it shows a better jumpstart performance. The dashed black line represents the maximal average return\nachieved by Adam without learning rate decay, resulting in slightly worse performance compared to using schedulers,\naligning with findings in previous work by Andrychowicz et al. [4], who linearly decay the learning rate to 0.\nIn all the experiments conducted so far, we have set the update window κ parameter to 1, which allows the observation\nof a full episode before updating the learning rate. In Appendix B.3, we address how this value could influence the\nperformance of LRRL by allowing the observation of more interactions.\nFigure 3: LRRL with arms that select learning rate schedulers: LRRL vs 3 individual schedulers (d = 1, 2, 3). The\ndashed black line represents the max average return achieved by Adam with a constant learning rate set to 6.25×10−5.\n4.3\nComparing RMSProp and Adam Optimizers\nAnother widely used optimizer for deep RL models is RMSProp, which, like Adam, features an adaptive learning rate\nmechanism. Adam builds upon RMSProp by retaining exponential moving averages to give more weight to recent\ngradients while incorporating momentum. Although the standard RMSProp does not feature momentum, we found\nthat adding momentum to RMSProp can increase both the performance of DQN and LRRL, aligning with findings\nin the literature [30, 4]. In the following experiment, we compare the performance of RMSProp with Nesterov’s\nmomentum (RMSProp-M), and Adam when coupled with either LRRL or the best-performing single learning rate\nwhen using DQN. Since the results were consistent in previous sections, we extend the experiments to other 3 Atari\ngames.\nFigure 4 shows that LRRL coupled with Adam consistently outperforms our configuration using RMSProp-M and the\nbaseline using standard DQN. Moreover, LRRL (RMSProp-M) underperforms compared to DQN without LRRL in\ntwo out of three tasks due to its slow convergence despite better jumpstart performance. Future work should investigate\nwhether this slow convergence is due to some optimizer features such as the absence of bias correction in the first and\nsecond moment estimates.\n4.4\nLRRL with a Distributional RL Algorithm\nLastly, we illustrate the versatility of LRRL by combining it with the distributional RL algorithm Implicit Quantile\nNetworks (IQN) [11], which exhibits the best performance in many Atari games among the algorithms available in\nDopamine. Adam is used as the optimizer. Figure 5 shows that LRRL outperforms standard learning with a single\n7\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nFigure 4: A comparison between Adam and RMSProp with momentum, using either DQN or LRRL.\nlearning rate across all the tasks evaluated. More generally, LRRL can be seamlessly integrated with any RL algorithm\nthat employs function approximators based on SGD, improving its performance and mitigating hyperparameter tuning.\nFigure 5: Comparison between LRRL and standard IQN learning: IQN combined with LRRL using 3 different learning\nrates is tested against the 3 variants of standard IQN using each possible learning rate.\n4.5\nSGD and Stationary Non-convex Loss Landscapes\nAlthough RL tasks are a challenging showcase to illustrate the effectiveness of our approach in the non-stationary\ncase, there are many confounding factors to analyze their impact on the results, such as the optimizer adaptive routine,\nthe environment stochasticity, and the reward sparseness. To disentangle these factors from our evaluation, in the\nfollowing experiments we employ the SGD optimizer, which has a single scalar learning rate without adaptive scheme\nor momentum to find the global minima of 6 non-convex loss landscapes. Since these functions represent a stationary\ncase, we employ the stochastic bandit Minimax Optimal Strategy in the Stochastic case (MOSS) algorithm (see details\nin Appendix B.2), using as feedback the loss plus a small constant ξ, such that fn =\n1\n|loss(n)|+ξ, observed at each\ngradient step n to update the arm’s probabilities.\nFigure 6 illustrates the performance of LRRL against SGD with a single learning rate, starting from the same initial\npoint and for the same amount of steps. The final steps are indicated by a star icon with a different color. The results\nshow that LRRL, with its “all-in-one” strategy, efficiently navigates intricate landscapes. It is effective in combining\nhigher learning rates to accelerate convergence and lower ones for more fine-grained optimization steps when needed,\nas evidenced in the loss landscapes depicted in Figures 6a, 6b, and 6c. In addition, Figures 6d, 6e, and 6f illustrate that\nLRRL can be also effective in converging towards the best-performing learning rate within the arms, showcased by its\nsteady convergence towards the global minima.\n5\nConclusion\nIn this work, we introduced LRRL, a meta-learning approach for selecting optimizers’ learning rates on the fly. In\nany optimization task, the best-performing learning rate or rate scheduler is typically unknown, requiring multiple\nruns to test each candidate in a wasteful computational effort. LRRL mitigates this cost by employing a multi-armed\nbandit algorithm to explore a set of learning rates within a single run, significantly reducing the need for extensive\nhyperparameter tuning while achieving better or competitive results with minimal computational overhead. Using\n8\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 6: Comparison between LRRL and fixed learning rates in a stationary setting with SGD. LRRL with 3 learning\nrates is tested against the 3 constant rates for 6 non-convex losses (shown in Figure 10 in Appendix B.4). LRRL is\neffective in either converging to the best performing rate or combining the set to achieve better results.\nRL and non-convex optimization tasks, our empirical results show that combining LRRL with the Adam or SGD\noptimizers could enhance the performance, outperforming baselines and learning rate schedulers.\nFuture investigations could extend LRRL ideas to other hyperparameters, such as mini-batch size, which also plays a\nkey role in convergence. Moreover, although LRRL selects rates based on policy performance, alternative feedback\nmechanisms could be explored, such as using gradient information to select block-wise (e.g., per-layer) learning rates,\nextending these ideas to supervised learning and applying them to other non-stationary objective functions, including\nthose encountered in Continual Learning [33, 19, 1].\nLimitations. The primary limitation of LRRL lies in the need to define a set of arms for the MAB algorithm, as\nthe choice of this set can directly influence the performance of the underlying learning algorithm. However, in com-\nputationally intensive settings such as RL tasks, LRRL offers a significant advantage: it enables the simultaneous\nevaluation of multiple learning rates within a single training run, eliminating the need to test each rate individually.\nBy employing an “all-in-one” strategy, LRRL not only achieves competitive results while reducing the burden of\nhyperparameter optimization but also can outperform standard learning by combining multiple learning rates.\nBroader Impact Statement\nThis work aims to contribute to the advancement of the field of Machine Learning. While we did not identify any direct\nnegative social impacts associated with this research, we encourage future studies building on it to remain vigilant and\ncritically evaluate potential ethical and societal implications.\nAcknowledgment\nThis work was granted access to the HPC resources of IDRIS under the allocation 2024-A0171015651 made by\nGENCI. HD and FF acknowledge funding from Inria Project WOMBAT. LFS is supported by a Discovery Early\nCareer Researcher Award from the Australian Research Council (DE240101190)\n9\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nReferences\n[1] D. Abel, A. Barreto, B. Van Roy, D. Precup, H. P. van Hasselt, and S. Singh. A Definition of Continual Rein-\nforcement Learning. In Advances in Neural Information Processing Systems, volume 36, pages 50377–50407.\nCurran Associates, Inc., 2023.\n[2] R. Agarwal, D. Schuurmans, and M. Norouzi. An Optimistic Perspective on Offline Reinforcement Learning.\nIn Proceedings of the 37th International Conference on Machine Learning (ICML 2017), volume 119, pages\n104–114. PMLR, 2020.\n[3] R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare. Reincarnating reinforcement learn-\ning: Reusing prior computation to accelerate progress. In Advances in Neural Information Processing Systems,\nvolume 35, pages 28955–28971. Curran Associates, Inc., 2022.\n[4] M. Andrychowicz, A. Raichuk, P. Sta´nczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin,\nM. Michalski, et al. What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study. In 9th\nInternational Conference on Learning Representations, ICLR 2021, 2021.\n[5] J. Audibert and S. Bubeck. Minimax Policies for Adversarial and Stochastic Bandits. In Proceedings of the 22nd\nConference on Learning Theory (COLT 2009). PMLR, 2009.\n[6] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The Nonstochastic Multiarmed Bandit Problem. SIAM\nJournal on Computing, 32(1):48–77, 2002. doi: 10.1137/S0097539701398375.\n[7] L. Blier, P. Wolinski, and Y. Ollivier. Learning with random learning rates. In Machine Learning and Knowledge\nDiscovery in Databases: European Conference, ECML PKDD 2019, pages 449–464. Springer, 2019.\n[8] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,\nS. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018.\nURL http://github.com/google/jax. version 0.3.13.\n[9] P. S. Castro, S. Moitra, C. Gelada, S. Kumar, and M. G. Bellemare. Dopamine: A Research Framework for Deep\nReinforcement Learning, 2018. Preprint, arXiv:1812.06110.\n[10] J. Coullon, L. South, and C. Nemeth. Efficient and generalizable tuning strategies for stochastic gradient MCMC.\nStatistics and Computing, 33(3), 2023. doi: 10.1007/s11222-023-10233-3.\n[11] W. Dabney, G. Ostrovski, D. Silver, and R. Munos. Implicit Quantile Networks for Distributional Reinforcement\nLearning. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018), pages 1104–\n1113. PMLR, 2018.\n[12] G. DeepMind. The DeepMind JAX Ecosystem, 2020. URL http://github.com/google-deepmind.\n[13] T. Degris, K. Javed, A. Sharifnassab, Y. Liu, and R. S. Sutton. Step-size optimization for continual learning,\n2024. Preprint, arXiv:2401.17401.\n[14] S. Fujimoto, D. Meger, and D. Precup.\nOff-Policy Deep Reinforcement Learning without Exploration.\nIn\nProceedings of the 36th International Conference on Machine Learning (ICML 2019), volume 97, pages 2052–\n2062. PMLR, 2019.\n[15] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.\n[16] K. G. Jamieson and A. Talwalkar. Non-stochastic Best Arm Identification and Hyperparameter Optimization.\nIn Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016,\nvolume 51, pages 240–248. JMLR, 2016.\n[17] Z. Karnin, T. Koren, and O. Somekh. Almost Optimal Exploration in Multi-Armed Bandits. In Proceedings of\nthe 30th International Conference on Machine Learning (ICML 2013), volume 28(3), pages 1238–1246. PMLR,\n2013.\n[18] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In 3rd International Conference on\nLearning Representations, ICLR 2015, 2015.\n[19] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ra-\nmalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic\nforgetting in neural networks. CoRR, 2016. arXiv:1612.00796.\n[20] G. Lample and D. S. Chaplot. Playing FPS Games with Deep Reinforcement Learning. In Proceedings of the\nAAAI Conference on Artificial Intelligence, 2017.\n[21] T. Lattimore and C. Szepesv´ari. Bandit Algorithms. Cambridge University Press, 2020.\n10\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\n[22] L. Li, K. G. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: a novel bandit-based\napproach to hyperparameter optimization.\nJournal of Machine Learning Research, 18(1):6765–6816, 2018.\nISSN 1532-4435.\n[23] L. J. Lin. Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching. Machine\nLearning, 8:293–321, 1992.\n[24] R. Liu, T. Wu, and B. Mozafari.\nAdam with Bandit Sampling for Deep Learning.\nIn Advances in Neural\nInformation Processing Systems, volume 33, pages 5393–5404. Curran Associates, Inc., 2020.\n[25] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller. Playing Atari\nwith Deep Reinforcement Learning. CoRR, 2013. arXiv:1312.5602.\n[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):\n529–533, 2015.\n[27] T. Moskovitz, J. Parker-Holder, A. Pacchiano, M. Arbel, and M. Jordan. Tactical Optimism and Pessimism\nfor Deep Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 34, pages\n12849–12863. Curran Associates, Inc., 2021.\n[28] G. Ostrovski, P. S. Castro, and W. Dabney. The Difficulty of Passive Learning in Deep Reinforcement Learning.\nIn Advances in Neural Information Processing Systems, volume 34, pages 23283–23295, 2021.\n[29] J. Parker-Holder, V. Nguyen, and S. J. Roberts. Provably Efficient Online Hyperparameter Optimization with\nPopulation-Based Bandits. In Advances in Neural Information Processing Systems, volume 33, pages 17200–\n17211. Curran Associates, Inc., 2020.\n[30] N. Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12(1):145–151,\n1999. doi: 10.1016/S0893-6080(98)00116-6.\n[31] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. Adaptive Computation and Machine\nLearning series. MIT Press, 2005. ISBN 9780262182539.\n[32] M. Riedmiller. Neural fitted q iteration – first experiences with a data efficient neural reinforcement learning\nmethod. In Proceedings of the 16th European Conference on Machine Learning, pages 317–328, Berlin, Heidel-\nberg, 2005. Springer-Verlag. ISBN 3540292438. doi: 10.1007/11564096 32.\n[33] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Had-\nsell. Progressive Neural Networks. CoRR, 2016. arXiv:1606.04671.\n[34] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In 4th International Conference\non Learning Representations, ICLR 2016, 2016.\n[35] A. Senior, G. Heigold, M. Ranzato, and K. Yang. An empirical study of learning rates in deep neural networks\nfor speech recognition. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP), 2013.\n[36] X. Shang, E. Kaufmann, and M. Valko. A simple dynamic bandit algorithm for hyper-parameter tuning. In\nWorkshop on Automated Machine Learning at International Conference on Machine Learning (AutoML@ICML\n2019), 2019.\n[37] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search.\nNature, 529(7587):484–489, 2016.\n[38] R. S. Sutton. Adapting bias by gradient descent: an incremental version of delta-bar-delta. In Proceedings of the\nTenth National Conference on Artificial Intelligence, pages 171–176. AAAI Press, 1992.\n[39] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA,\nUSA, 2018. ISBN 0262039249.\n[40] M. E. Taylor and P. Stone. Transfer Learning for Reinforcement Learning Domains: A Survey. Journal of\nMachine Learning Research, 10:1633–1685, 2009.\n[41] T. Tieleman and G. Hinton. Lecture 6.5 - RMSProp: Divide the gradient by a running average of its recent\nmagnitude. Coursera: Neural Networks for Machine Learning, 2012. URL https://www.cs.toronto.edu/\n~tijmen/csc321/slides/lecture_slides_lec6.pdf.\n[42] H. van Hasselt, A. Guez, and D. Silver. Deep Reinforcement Learning with Double Q-Learning. In Proceedings\nof the AAAI conference on artificial intelligence, 2016.\n11\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\n[43] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds,\nP. Georgiev, et al. Grandmaster level in StarCraft ii using multi-agent reinforcement learning. Nature, 575(7782):\n350–354, 2019.\n[44] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J. Walsh, R. Capobianco, A. Devlic,\nF. Eckert, F. Fuchs, et al. Outracing champion Gran Turismo drivers with deep reinforcement learning. Nature,\n602(7896):223–228, 2022.\n[45] Z. Xu, H. P. van Hasselt, M. Hessel, J. Oh, S. Singh, and D. Silver. Meta-Gradient Reinforcement Learning\nwith an Objective Discovered Online. In Advances in Neural Information Processing Systems, volume 33, pages\n15254–15264. Curran Associates, Inc., 2020.\n[46] K. You, M. Long, J. Wang, and M. I. Jordan. How does learning rate decay help modern neural networks?, 2019.\nPreprint, arXiv:1908.01878.\n[47] K. Young, B. Wang, and M. E. Taylor. Metatrace Actor-Critic: Online Step-Size Tuning by Meta-gradient De-\nscent for Reinforcement Learning Control. In Proceedings of the Twenty-Eighth International Joint Conference\non Artificial Intelligence, IJCAI-19, pages 4185–4191, 2019. doi: 10.24963/ijcai.2019/581.\n12\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nA\nRelated Work\nThis section presents works closely related to ours. We first introduce approaches that use MAB for hyperparameter\nselection and later other methods that aim to adapt the learning rate over the training process.\nMulti-armed bandit for (hyper)parameter selection.\nDeep RL is known to be overly optimistic in the face of\nuncertainty [28], and many works have addressed this issue by proposing conservative policy updates [42, 14, 2].\nHowever, when the agent can interact with the environment, this optimism might encourage exploration, potentially\nleading to the discovery of higher returns. Building on this idea, Moskovitz et al. [27] proposes Tactical Optimism and\nPessimism (TOP), an approach that uses an adversarial MAB algorithm to trade-off between pessimistic and optimistic\npolicy updates based on the agent’s performance over the learning process. Although our work shares similarities with\nTOP, such as employing an adversarial bandit algorithm and using performance improvement as feedback, there are\nkey differences. First, TOP is closely tied to actor-critic methods, making it unclear how it could be extended to other\nRL paradigms. Moreover, it also introduces an exploration strategy which depends on epistemic uncertainty (captured\nthrough ensembles) and aleatoric uncertainty (estimated using a distributional RL algorithm). Second, TOP trades-off\nbetween optimism and pessimism by adding new variables (arms), which are not inherently tied to the learning process\nand also require tuning. In contrast, LRRL offers an algorithm-agnostic solution by directly optimizing the learning\nrate, a core hyperparameter in all SGD-based methods, making it simpler and more broadly applicable. Also close\nto our work, Liu et al. [24] propose Adam with Bandit Sampling (ADAMBS), which employs the Exp3 algorithm to\nenhance sample efficiency by incorporating importance sampling within the Adam optimizer. ADAMBS prioritizes\ninformative samples by keeping a probability distribution over all samples, although the feedback is provided by the\ngradient computed over a mini-batch, unlike previous works in RL that are capable of assigning per-sample importance\nby using their computed TD-error (see 34).\nIn order to select the learning rate for stochastic gradient MCMC, Coullon et al. [10] employ an algorithm based on\nSuccessive Halving [17, 16], a MAB strategy that promotes promising arms and prunes suboptimal ones over time.\nIn the context of hyperparameter optimization, Successive Halving has also been used in combination with infinite-\narm bandits to select hyperparameters for supervised learning [22, 36]. A key difference between these approaches\nto hyperparameter optimization for supervised learning and our work is that we focus on selecting the best learning\nrate on the fly from a predefined set, rather than performing an extensive and computationally expensive search over\nthe hyperparameter space. Close to our work, Parker-Holder et al. [29] propose population-based training which\nemploys a Gaussian Process (see details in 31) with a time-varying kernel to represent the upper confidence bound in a\nbandit setting. The idea is to have many instances of agents learning a task in parallel with different hyperparameters,\nreplacing those that are underperforming within a single run.\nLearning rate adapters/schedulers.\nOptimizers such as RMSProp [41] have an adaptive mechanism to update a set\nof parameters θ by normalizing past gradients, while Adam [18] also incorporates momentum to smooth gradient steps.\nHowever, despite their widespread adoption, these algorithms have inherent limitations in non-stationary environments\nsince they do not adapt to changes in the objective function over time (see 13). Increment-Delta-Bar-Delta (IDBD),\nintroduced by Sutton [38], has an adaptive mechanism based on the loss to adjust the learning rate ηi for each sample\nxi for linear regression and has been extended to settings including RL [47]. Learning rate schedulers with time\ndecay [35, 46] are coupled with optimizers, assuming gradual convergence to a good solution, but often require task-\nspecific manual tuning. A meta-gradient RL method is proposed in Xu et al. [45], composed of a two-level optimization\nprocess: one that uses the agent’s objective and the other to learn meta-parameters of the objective function. Our work\ndiffers from these methods by employing a MAB approach to dynamically select the learning rate over the training\nprocess, specifically targeting RL settings.\nB\nSupplementary Experiments\nB.1\nBaseline Evaluation with Varying Learning Rates\nTo establish a baseline to compare learning without our approach LRRL, we run individual arm values as baseline\nlearning rate using the Adam optimizer. The results presented in Figure 7 align with common expectations by showing\nthat higher learning rates fail to learn for most environments while lower ones can lead to worse jumpstart performance\nand slow convergence.\n13\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nFigure 7: DQN performance using the Adam optimizer with varying learning rates across 6 Atari games. Shaded\nregion represents one-half standard deviation. The results show that the best performing learning rate varies across\ngames.\nB.2\nA Comparison between Multi-Armed Bandits Algorithms\nAdversarial MAB algorithms are designed for environments where the reward distribution changes over time. In\ncontrast, stochastic MAB algorithms assume that rewards are drawn from fixed but unknown probability distributions.\nTo validate our choice and address our method’s robustness, we compare Exp3 with the stochastic MAB algorithm\nMOSS (Minimax Optimal Strategy in the Stochastic case) [5]. MOSS trades off exploration and exploitation by pulling\nthe arm k with highest upper confidence bound given by:\nBk(n) = ˆµk(n) + ρ\nv\nu\nu\ntmax\n\u0010\nlog\nn\nKnk(n), 0\n\u0011\nnk(n)\nwhere ˆµk(n) is the empirical average reward for arm k and nk(n) is the number of times it has been pulled up to round\nn.\nIn Figure 8, we use different bandit step-sizes α for Exp3 and parameter ρ, which balances exploration and exploitation\nin MOSS. Additionally, the bandit feedback used in MOSS is the average cumulative reward R\nC . The results indicate\nthat while Exp3 performs better overall, MOSS can still achieve competitive results depending on the amount of\nexploration, demonstrating the robustness of our approach regarding the MAB algorithm employed.\nB.3\nDealing with Noise and Lack of Feedback\nLRRL relies entirely on performance improvements to update the arm probabilities. However, some RL environments\ncan be highly stochastic, with sparse or delayed rewards, which increases the difficulty of learning tasks and, con-\nsequently, the ability of LRRL to make optimal choices. To mitigate this limitation, we introduce the variable κ in\nAlgorithm 1, which determines the number of interactions to observe before making a decision.\nWhen setting κ, two factors should be considered:\n• The network’s update rate: Selecting new learning rates more frequently than updating the model leads to\ninaccurate feedback for the bandit mechanism since no model update might occur after pulling some arm.\n• The number of environment steps: In some tasks, positive or negative outcomes of a sequence of actions\nare only perceived at the end of an episode, and capturing sufficient information about the agent performance\nis essential to updating the arms’ probabilities.\n14\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nFigure 8: A comparison between adversarial (Exp3) and stochastic (MOSS) MAB algorithms. Shaded region repre-\nsents one-half standard deviation. The results show that Exp3 performs better overall.\nIn the experiments presented in this work, we set κ = 1 episode, allowing the observation of a complete episode\nbefore updating the arms’ probabilities. To explore the impact of this parameter further, we conduct an ablation study\nusing different κ values to evaluate potential performance improvements when combined with the schedulers shown\nin Figure 3. As suggested by Figure 9, the choice of κ is task-dependent: while some tasks benefit from more reactive\nlearning rate updates, others show marginal improvements with increased observation counts. Ultimately, setting\nκ = 1 seems relatively robust for Atari games, although higher values could slightly increase the performance in some\ntasks.\nFigure 9: Results of the ablation study evaluating the impact of varying κ with LRRL using schedulers. The dashed\nblack line represents the max average return achieved by the best performing scheduler.\n15\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nB.4\nSupplementary Results using SGD\nIn Section 4.5, we tested the SGD optimizer in 6 non-convex loss landscapes show in Figure 10. These functions were\nselected since they represent common benchmarks in optimization, each one offering different challenges due to their\ndistinct shapes. By using SGD on these landscapes, we minimize the influence of confounding factors, such as the\nhigh stochasticity inherent in some RL tasks and the complexity of more robust optimizers like Adam.\nFigure 11 presents the normalized loss and the arms selected at each optimization step. The results demonstrate\nLRRL’s dynamic adaptation mechanism, either converging to the most effective learning rate or combining them to\nachieve superior outcomes.\nFigure 10: The 6 stationary non-convex loss landscapes used to assess LRRL with SGD optimizer.\nFigure 11: Learning rates and logarithmic scale of losses over the gradient steps using LRRL with SGD optimizer.\n16\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nC\nEvaluation Metrics\nIn this section, we describe the evaluation metrics that can be used to evaluate the agent’s performance as it interacts\nwith an environment. Based on Dopamine, we use the evaluation step-size “iterations”, defined as a predetermined\nnumber of episodes. Figure 12 illustrates the evaluation metrics used in this work, as defined in Taylor and Stone [40]:\n• Max average return: The highest average return obtained by an algorithm throughout the learning process.\nIt is calculated by averaging the outcomes across multiple individual runs.\n• Final performance: The performance of an algorithm after a predefined number of interactions. While two\nalgorithms may reach the same final performance, they might require different amounts of data to do so. This\nmetric captures the efficiency of an algorithm in reaching a certain level of performance within a limited\nnumber of interactions. In Figure 12, the final performance overlaps with the max average return, represented\nby the black dashed line.\n• Jumpstart performance: The performance at the initial stages of training, starting from a policy with ran-\ndomized parameters θ. In Figure 12, Algorithm A exhibits better jumpstart performance but ultimately\nachieves lower final performance than Algorithm B. A lower jumpstart performance can result from fac-\ntors such as a lower learning rate, although this work demonstrates that this does not necessarily lead to\nworse final performance.\nFigure 12: Performance curves of two RL algorithms (adapted from 40).\nD\nImplementation Details\nIn the following, we list the set of arms, optimizer and the bandit step-size used in each experiment.\nSection 4.1 – Comparing LRRL with Standard Learning.\n• Optimizer: Adam\n• Bandit step-size: α = 0.2\n• Considered sets of learning rates:\nK(5) =\n\b\n1.5625 × 10−5, 3.125 × 10−5, 6.25 × 10−5, 1.25 × 10−4, 2.5 × 10−4\t\nKlowest(3) =\n\b\n1.5625 × 10−5, 3.125 × 10−5, 6.25 × 10−5\t\nKmiddle(3) =\n\b\n3.125 × 10−5, 6.25 × 10−5, 1.25 × 10−4\t\nKhighest(3) =\n\b\n6.25 × 10−5, 1.25 × 10−4, 2.5 × 10−4\t\nKsparse(3) =\n\b\n1.5625 × 10−5,\n6.25 × 10−5,\n2.5 × 10−4\t\nSection 4.2 – Combining and Comparing Schedulers with LRRL.\n• Optimizer: Adam\n• Bandit step-size: α = 0.2\n• Initial learning rate of schedulers: η0 = 6.25 × 10−5\n17\nDynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach\nPREPRINT\nSection 4.3– Comparing RMSProp and Adam Optimizers\n• Optimizer: RMSProp for DQN baselines\n• Bandit step-size: α = 0.2\n• LRR-DQN set of learning rates: K =\n\b\n1.5625 × 10−5, 3.125 × 10−5, 6.25 × 10−5\t\nSection 4.4– LRRL with a Distributional RL Algorithm.\n• Optimizer: Adam\n• Bandit step-size: α = 0.2\n• LRRL-IQN set of learning rates: K =\n\b\n2.5 × 10−5, 5 × 10−5, 1 × 10−4\t\nSection B.2 – A Comparison between Multi-Armed Bandit Algorithms.\n• Optimizer: Adam\n• MOSS/Exp3 set of learning rates: K =\n\b\n3.125 × 10−5, 6.25 × 10−5, 1.25 × 10−4\t\nHyperparameters of the optimizers and deep RL algorithms.\nFor reproductibility, the list of hyperparameters\nof the optimizers and deep RL algorithms used for our experiments is provided in Table 1. The optimizers from the\nOptax library [12] are employed alongside the JAX [8] implementation of the DQN [25, 26] and IQN [11] algorithms,\nas provided by the Dopamine framework [9].\nDQN and IQN hyperparameters\nValue\nSticky actions\nTrue\nSticky actions probability\n0.25\nDiscount factor (γ)\n0.99\nFrames stacked\n4\nMini-batch size (B)\n32\nReplay memory start size\n20 000\nLearning network update rate (λ)\n4 steps\nMinimum environment steps (κ)\n1 episode\nTarget network update rate (τ)\n8000 steps\nInitial exploration (ϵ)\n1\nExploration decay rate\n0.01\nExploration decay period (steps)\n250 000\nEnvironment steps per iteration (steps)\n250 000\nReward clipping\n[-1, 1]\nNetwork neurons per layer\n32, 64, 64\nHardware (GPU)\nV100\nAdam hyperparameters\nβ1 decay\n0.9\nβ2 decay\n0.999\nEps (DQN)\n1.5×10−4\nEps (IQN)\n3.125×10−4\nRMSProp hyperparameters (DQN)\nDecay\n0.9\nMomentum (if True)\n0.999\nCentered\nFalse\nEps\n1.5×10−4\nTable 1: Hyperparameters used in the experiments.\n18\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-16",
  "updated": "2025-02-03"
}