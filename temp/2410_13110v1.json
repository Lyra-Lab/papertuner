{
  "id": "http://arxiv.org/abs/2410.13110v1",
  "title": "Deep Learning-based Software Engineering: Progress, Challenges, and Opportunities",
  "authors": [
    "Xiangping Chen",
    "Xing Hu",
    "Yuan Huang",
    "He Jiang",
    "Weixing Ji",
    "Yanjie Jiang",
    "Yanyan Jiang",
    "Bo Liu",
    "Hui Liu",
    "Xiaochen Li",
    "Xiaoli Lian",
    "Guozhu Meng",
    "Xin Peng",
    "Hailong Sun",
    "Lin Shi",
    "Bo Wang",
    "Chong Wang",
    "Jiayi Wang",
    "Tiantian Wang",
    "Jifeng Xuan",
    "Xin Xia",
    "Yibiao Yang",
    "Yixin Yang",
    "Li Zhang",
    "Yuming Zhou",
    "Lu Zhang"
  ],
  "abstract": "Researchers have recently achieved significant advances in deep learning\ntechniques, which in turn has substantially advanced other research\ndisciplines, such as natural language processing, image processing, speech\nrecognition, and software engineering. Various deep learning techniques have\nbeen successfully employed to facilitate software engineering tasks, including\ncode generation, software refactoring, and fault localization. Many papers have\nalso been presented in top conferences and journals, demonstrating the\napplications of deep learning techniques in resolving various software\nengineering tasks. However, although several surveys have provided overall\npictures of the application of deep learning techniques in software\nengineering, they focus more on learning techniques, that is, what kind of deep\nlearning techniques are employed and how deep models are trained or fine-tuned\nfor software engineering tasks. We still lack surveys explaining the advances\nof subareas in software engineering driven by deep learning techniques, as well\nas challenges and opportunities in each subarea. To this end, in this paper, we\npresent the first task-oriented survey on deep learning-based software\nengineering. It covers twelve major software engineering subareas significantly\nimpacted by deep learning techniques. Such subareas spread out the through the\nwhole lifecycle of software development and maintenance, including requirements\nengineering, software development, testing, maintenance, and developer\ncollaboration. As we believe that deep learning may provide an opportunity to\nrevolutionize the whole discipline of software engineering, providing one\nsurvey covering as many subareas as possible in software engineering can help\nfuture research push forward the frontier of deep learning-based software\nengineering more systematically.",
  "text": "SCIENCE CHINA\nInformation Sciences\n. REVIEW .\nDeep Learning-based Software Engineering:\nProgress, Challenges, and Opportunities*\nXiangping CHEN2* , Xing HU3* , Yuan HUANG4 , He JIANG5* , Weixing JI6 ,\nYanjie JIANG1* , Yanyan JIANG7* , Bo LIU6 , Hui LIU6 , Xiaochen LI5 , Xiaoli LIAN8* ,\nGuozhu MENG9* , Xin PENG10* , Hailong SUN11* , Lin SHI11* , Bo WANG12* ,\nChong WANG10 , Jiayi WANG7 , Tiantian WANG13* , Jifeng XUAN14* , Xin XIA15 ,\nYibiao YANG7* , Yixin YANG11 , Li ZHANG8 , Yuming ZHOU7* & Lu ZHANG1*\n1\nKey Laboratory of High\nConfidence Software\nTechnologies\n(Peking\nUniversity), Ministry of Education;\nSchool of Computer Science, Peking\nUniversity, Beijing 100871,\nChina;\n2School of Journalism and Communication, Sun\nYat-sen\nUniversity,\nGuangzhou 510275, China;\n3School of Software\nTechnology, Zhejiang\nUniversity, Hangzhou 310058,\nChina;\n4School of Software Engineering, Sun\nYat-sen\nUniversity,\nGuangzhou 510275,\nChina;\n5School of Software, Dalian\nUniversity\nof Technology, Dalian 116024,\nChina;\n6School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China;\n7State Key Laboratory for Novel Software Technology, Nanjing\nUniversity, Nanjing 210023,\nChina;\n8School of Computer Science and Engineering, Beihang\nUniversity, Beijing 100191,\nChina;\n9Institute of Information Engineering,\nChinese Academy of Sciences,\nBeijing 100864, China;\n10 School of Computer Science, Fudan\nUniversity, Shanghai 200433, China;\n11 State Key Laboratory of Complex & Critical Software Environment (CCSE);\nSchool of Software, Beihang\nUniversity,\nBeijing 100191, China;\n12 School of Computer and Information Technology, Beijing Jiaotong\nUniversity, Beijing 100044, China;\n13 School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China;\n14 School of Computer Science,\nWuhan\nUniversity,\nWuhan 430072,\nChina;\n15 Huawei Technologies, Hangzhou, China\nAbstract\nResearchers have recently achieved significant advances in deep learning techniques, which in\nturn has substantially advanced other research disciplines, such as natural language processing, image pro-\ncessing, speech recognition, and software engineering. Various deep learning techniques have been successfully\nemployed to facilitate software engineering tasks, including code generation, software refactoring, and fault\nlocalization. Many papers have also been presented in top conferences and journals, demonstrating the appli-\ncations of deep learning techniques in resolving various software engineering tasks. However, although several\nsurveys have provided overall pictures of the application of deep learning techniques in software engineering,\nthey focus more on learning techniques, that is, what kind of deep learning techniques are employed and\nhow deep models are trained or fine-tuned for software engineering tasks. We still lack surveys explaining\nthe advances of subareas in software engineering driven by deep learning techniques, as well as challenges\nand opportunities in each subarea.\nTo this end, in this paper, we present the first task-oriented survey\non deep learning-based software engineering.\nIt covers twelve major software engineering subareas signifi-\ncantly impacted by deep learning techniques. Such subareas spread out the through the whole lifecycle of\nsoftware development and maintenance, including requirements engineering, software development, testing,\nmaintenance, and developer collaboration. As we believe that deep learning may provide an opportunity to\nrevolutionize the whole discipline of software engineering, providing one survey covering as many subareas\nas possible in software engineering can help future research push forward the frontier of deep learning-based\nsoftware engineering more systematically. For each of the selected subareas, we highlight the major advances\nachieved by applying deep learning techniques with pointers to the available datasets in such a subarea. We\nalso discuss the challenges and opportunities concerning each of the surveyed software engineering subareas.\nKeywords\nDeep Learning, Software Engineering, Software Benchmark, Software Artifact Representation,\nSurvey\nCitation\n* The authors are displayed in alphabetical order.\n* Corresponding\nauthor\n(email:\nchenxp8@mail.sysu.edu.cn,\nxinghu@zju.edu.cn,\njianghe@dlut.edu.cn,\nyanjiejiang@pku.edu.cn,\nSci\nChina Inf Sci\n2\n1\nIntroduction\nIn recent years, deep learning, first proposed by Hintonetal. [1] in 2006, has achieved highly impressive\nadvances [2]. Because of the unsupervised-layer-wise training proposed by Hinton et al. [3], the\nmajor obstacle to the training of deep neural networks has been removed.\nSince then, most artificial\nintelligence (AI) researchers have turned to deep learning, constructing deep neural networks containing\ndozens, thousands, and even millions of layers [4].\nThey have also proposed various novel structures\nof deep neural networks, such as convolutional neural networks (CNNs) [5], recurrent neural networks\n(RNNs) [6], long short-term memory networks (LSTMs) [7], bidirectional LSTM [8], and Transformer [9].\nWith the advances and popularity of deep learning, hardware vendors like NVIDIA release more powerful\ncomputing devices specially designed for deep learning. All of these together significantly push forward\nmachine learning techniques and make deep learning-based AI one of the most promising techniques in\nthe 21st century.\nGiven significant advances in deep learning, various deep learning techniques have been employed to\nfulfill software engineering tasks [10]. Although natural language processing (NLP), image and video pro-\ncessing, and speech processing are the major targets of current deep learning techniques, deep learning has\nbeen successfully applied to a wide range of various domains, including data mining [11], machine manu-\nfacturing [12], biomedical engineering [13] and information security [14]. Concerning software engineering,\nresearchers have successfully exploited various deep learning techniques for various important tasks, such\nas code generation [15], code completion [16], code summarization [17], software refactoring [18], code\nsearch [19], fault localization [20], automated program repair [21], vulnerability detection [22], and soft-\nware testing [23].\nIn all such tasks, deep learning techniques have been proven useful, substantially\nimproving the state-of-the-art. One possible reason for the success of deep learning-based software engi-\nneering is the significant advances in deep learning techniques. Another possible reason is that various and\nmassive software engineering data are publicly available for training advanced neural models. With the\npopularity of open-source software applications, developers share massive software requirements, source\ncode, documents, bug reports, patches, test cases, online discussions, and logs and trace relationships\namong different artifacts. All such data make training specialized deep neural models for software engi-\nneering tasks feasible. To our knowledge, there have already been several surveys on deep learning for\nsoftware engineering (e.g., Yanget al. [10], Watson et al. [24] and Niu etal. [25]). Although these surveys\nprovide some overall pictures of the applications of deep learning for software engineering, there is still a\nlack of detailed analyses of the progress, challenges, and opportunities of deep learning techniques from\nthe perspective of each subarea of software engineering influenced by deep learning.\nIn this paper, we present a detailed survey covering the applications of deep learning techniques in\nmajor software engineering subareas. We choose to provide one survey to cover the technical research\nof deep learning for the whole discipline of software engineering instead of several surveys for different\nindividual subareas for the following reasons. First, software engineering has one central objective, and\nresearchers follow a divide-and-conquer strategy to divide software engineering into different subareas.\nHowever, the advances of deep learning may provide usan opportunity to break the boundaries of research\nin different subareas to push forward the software engineering discipline as a whole. That is, deep learning\nmay provide a common means to revolutionize software engineering in the future. Therefore, we believe\nthat a survey of deep learning in all software engineering subareas may be more beneficial for software\nengineering researchers.\nSecond, deep learning belongs to representation learning, and deep learning\ntechniques for software engineering are thus highly specific to different artifacts in software development.\nAs different subareas of software engineering typically share some common artifacts, putting different\nsubareas into one survey would help researchers from different subareas understand the strengths and\nweaknesses of different deep learning techniques. For example, a deep learning technique based on source\ncode may impact all subareas related to the comprehension, generation, and modification of codes.\nOne issue that raises in preparing this survey is that software engineering is a big discipline and the\napplications of deep learning techniques may thus touch some subareas in an uninfluential way. When\nfacing such a subarea, where there are very few deep learning papers, we feel that these papers can\nhardly reflect the essence of deep learning research in that subarea. Therefore, instead of covering all\nsubareas of software engineering, we focus on subareas where deep learning has already had significant\njyy@nju.edu.cn,\nlianxiaoli@buaa.edu.cn,\nmengguozhu@iie.ac.cn,\npengxin@fudan.edu.cn,\nsunhl@buaa.edu.cn,\nshilin@buaa.edu.cn,\nwangbo cs@bjtu.edu.cn,\nwangtiantian@hit.edu.cn,\njxuan@whu.edu.cn\n,\nyangyibiao@nju.edu.cn,\nzhouyuming@nju.edu.cn,\nzhanglucs@pku.edu.cn)\nSci\nChina Inf Sci\n3\nTable 1\nSoftware Engineering Tasks Covered by the Survey\nSoftware Engineering Tasks\nNumber of Surveyed Papers\n1\nRequirements Engineering\n28\n2\nCode Generation\n46\n3\nCode Search\n40\n4\nCode Summarization\n55\n5\nSoftware Refactoring\n19\n6\nCode Clone Detection\n53\n7\nSoftware Defect Prediction\n32\n8\nBug Finding\n114\n9\nFault Localization\n42\n10\nProgram Repair\n64\n11\nBug Report Management\n51\n12\nDeveloper Collaboration\nTotal\n57\n601\nimpacts. Fortunately, deep learning has deeply impacted software engineering, and through the subareas\nwe selected for surveying in this paper, we can already form a general and insightful picture of deep\nlearning for software engineering. To achieve our goal, for each of the selected subareas, we highlight\nmajor technical advances, challenges, and opportunities. As far as we know, our survey is the first task-\noriented survey on deep learning-based software engineering, providing a technical overview of software\nengineering research driven by deep learning.\nAnother issue is the alignment between software engineering and deep learning. From the perspective\nof software engineering, research efforts are primarily divided into five phases (i.e., requirements, design,\nimplementation, testing, and maintenance) according to the software development lifecycle, where each\nphase maybe further divided into different software development activities. Meanwhile, from the perspec-\ntive of deep learning, research efforts are grouped by the target learning tasks, where the most distinctive\ncharacteristic of a learning task is its input and output as deep learning is typically of an end-to-end\nfashion. That is, using the same form of input–output pairs is typically viewed as being the same task,\nbut there may be several different tasks for achieving the same goal. To balance the two perspectives,\nwe grouped the research efforts according to their goals. That is, we viewed research for achieving the\nsame goal or a set of similar goals as being of one subarea, and accordingly divided the research efforts\ninto 12 subareas (Table 1 for the numbers of surveyed papers in each subarea): requirements engineering,\ncode generation, code search, code summarization, software refactoring, code clone detection, software\ndefect prediction, bug finding, fault localization, program repair, bug report management, and developer\ncollaboration. We believe that this organization is friendly to readers from both software engineering and\ndeep learning fields. Each subarea is of clear semantics in software engineering, and the tasks in each\nsubarea are highly related from the perspective of deep learning. We further arranged the subareas in an\norder consistent with the order of the five phases of software development. An additional benefit is that\nthis organization naturally prevents one subarea from being overcrowded.\nHowever, this organization\nalso demonstrates the following drawbacks. From the software development lifecycle perspective, many\nsubareas belong to software maintenance, but fewer subareas belong to other phases in our survey. We\nwould like to emphasize that although this may roughly reflect that more research efforts have been de-\nvoted to software maintenance, it does not mean that software maintenance is of more importance than\nthe other phases in software engineering.\nIn collecting papers for our survey, we focused on publications in major conferences and journals\non software engineering and artificial intelligence between 2000 and 2023. Tables 2 and 3 present the\nconferences and journals we searched for papers on deep learning for software engineering. Notably, as we\nare not performing a systematic literature review, we did not follow a strict procedure for data collection\n(i.e., paper collection). Therefore, although Tables 2 and 3 provide the conferences and journals from\nwhere papers were surveyed, the selection of papers for our survey also underwent subjective judgment\nfrom the authors for appropriation. That is, the primary goal of our survey is to provide readers with a\ngeneral picture of technical advances in deep learning-based software engineering but not to characterize\na precise distribution of research efforts across different software engineering subareas. Because there\nSci\nChina Inf Sci\n4\nTable 2\nConferences Covered by the Survey\n# ID\nAbbreviation\nConference\n1\nAAAI\nAAAI Conference on Artificial Intelligence\n2\nACL\nAnnual Meeting of the Association for Computational Linguistics\n3\nAPSEC\nAsia-Pacific Software Engineering Conference\n4\nASE\nIEEE/ACM International Conference on Automated Software Engineering\n5\nCOMPSAC\nIEEE Annual Computers, Software, and Applications Conference\n6\nEASE\nEvaluation and Assessment in Software Engineering\n7\nESEC/FSE\nACM Joint European Software Engineering Conference\nand Symposium on the Foundations of Software Engineering\n8\nICDM\nIEEE International Conference on Data Mining\n9\nICLR\nInternational Conference on Learning Representations\n10\nICMLA\nInternational Conference on Machine Learning and Applications\n11\nICML\nInternational Conference on Machine Learning\n12\nICPC\nInternational Conference on Program Comprehension\n13\nICSE\nIEEE/ACM International Conference on Software Engineering\n14\nICSME\nIEEE International Conference on Software Maintenance and Evolution\n15\nICSR\nInternational Conference on Social Robotics\n16\nICST\nIEEE International Conference on Software Testing, Verification and Validation\n17\nICTAI\nIEEE International Conference on Tools with Artificial Intelligence\n18\nIJCAI\nInternational Joint Conference on Artificial Intelligence\n19\nIJCNN\nInternational Joint Conference on Neural Networks\n20\nInternetware\nAsia-Pacific Symposium on Internetware\n21\nISSRE\nIEEE International Symposium on Software Reliability Engineering\n22\nISSTA\nACM SIGSOFT International Symposium on Software Testing and Analysis\n23\nIWoR\nInternational Workshop on Refactoring\n24\nIWSC\nInternational Workshop on Software Clones\n25\nMSR\nInternational Conference on Mining Software Repositories\n26\nNeurIPS\nAnnual Conference on Neural Information Processing Systems\n27\nNIPS\nAdvances in Neural Information Processing Systems\n28\nOOPSLA\nObject-Oriented Programming, Systems, Languages, and Applications\n29\nQRS\nInternational Conference on Software Quality, Reliability and Security\n30\nRE\nInternational Conference on Requirements Engineering\n31\nSANER\nInternational Conference on Software Analysis, Evolution and Reengineering\n32\nSEKE\nInternational Conference on Software Engineering and Knowledge Engineering\n33\nS&P\nIEEE Symposium on Security and Privacy\n34\nWCRE\nWorking Conference on Reverse Engineering\n35\nWW W\nWeb Conference\nSci\nChina Inf Sci\n5\nTable 3\nJournals Covered by the Survey\n#ID\nAbbreviation\nJournal\n1\nASE\nAutomated Software Engineering\n2\nCOLA\nJournal of Computer Languages\n3\nCSUR\nACM Computing Surveys\n4\nESE\nEmpirical Software Engineering\n5\nFCS\nFrontiers in Computer Science\n6\nIETS\nIET Software\n7\nIST\nInformation and Software Technology\n8\nJSS\nJournal of Systems and Software\n9\nNCA\nNeural Computing and Applications\n10\nPACMPL\nACM on Programming Languages\n11\nRE\nRequirements Engineering\n12\nSCIS\nScience China Information Sciences\n13\nTC\nIEEE Transactions on Computers\n14\nTOSEM\nACM Transactions on Software Engineering and Methodology\n15\nTSE\nIEEE Transactions on Software Engineering\n16\nTSUSC\nIEEE Transactions on Sustainable Computing\n17\n–\nSoft Computing\n18\nIEEE Transactions on Reliability\n19\nIEEE Access\n20\n–\nExpert Systems with Applications\n21\nJournal of Software: Evolution and Process\n22\nAdvances in Engineering Software\nare quite many researchers interested in deep learning-based software engineering, the number of papers\nincreases quickly. Consequently, the number of papers discussed in this survey has already significantly\nsurpassed the number of surveyed papers in recent systematic literature reviews (i.e., Yang et al. [10]\nand Watson et al. [24]).\nFurther, this survey notably belongs to the theme of AI4SE (where various\ntechniques of artificial intelligence are applied to software engineering), whereas there is also intensive\nresearch in another related theme of SE4AI (where techniques for software engineering are applied to\nenhance artificial intelligence systems). An important goal of our subjective judgment is to avoid the\ninclusion of SE4AI papers in our paper collection.\n2\nRelated Work\nRecently, Yanget al. [10] performed a systematic literature review to summarize, classify, and analyze rel-\nevant papers in the field of software engineering that leverage deep learning techniques. They collected in\ntotal 250 relevant papers published in 32 major software engineering conferences and journals since 2006.\nBased on the papers, they analyzed the development trends of deep learning, provided a classification of\ndeep learning models, and summarized the research topics tackled by these relevant papers. The major\ntask of the systematic literature review is to figure out which and how deep learning techniques have been\napplied to software engineering. Their findings suggest that four categories of DNNs (CNN,LSTM, RNN,\nand FNN) were frequently employed by more than 20 studies. In addition, they summarized three types\nof DNN-based model selection strategies, i.e., characteristic-based selection, prior study-based selection\nbased on, and using multiple feasible DNNs where the first strategy (characteristic-based selection) is by\nfar the most popular one. Our survey differs from the systematic literature review by Yang et al. [10] in\nthat Yang et al. focus more on deep learning techniques whereas we focus more on software engineering\ntasks.\nYang et al.\nexplain what deep learning techniques have been applied to software engineering\nwhereas we analyze each software engineering task to explain how deep learning techniques could provide\nhelp for the task, and what kind of challenges could be encountered. In total, we analyze more than 500\npapers that leverage deep learning techniques for software engineering, providing a more comprehensive\nSci\nChina Inf Sci\n6\nview on this emerging research field.\nWatson et al. [24] conducted another systematic literature review on the application of deep learning\ntechniques in software engineering. They collected and analyzed 128 papers from software engineering and\ndeep learning conferences and journals. The major task of the paper is to answer five questions, i.e., what\ntypes of software engineering tasks have been addressed by deep learning techniques, what deep learning\ntechniques have been applied to software engineering, how requested training data are collected, how well\nsoftware engineering tasks have been addressed by deep learning techniques, and what common factors\ninfluence the replicability of deep learning applied to software engineering tasks. Their findings suggest\nthat deep learning-based techniques had been applied in a diverse set of tasks where program synthesis,\ncode comprehension, and source code generation are the most prevalent. They also found that a number\nof different data preprocessing techniques have been utilized. Tokenization and neural embeddings are\nthe two most frequently employed data pre-processing techniques. Besides that, their analysis revealed\nseven major types of deep learning architectures that have been employed for software engineering tasks,\nincluding Recurrent Neural Networks (RNNs), Encoder-Decoder Models, Convolutional Neural Networks\n(CNNs), Feed-Forward Neural Networks (FNNs), AutoEncoders, Siamese Neural Networks, and highly\ntailored architectures.\nOur survey differs from Watson et al. [24] in that Watson et al.\ntake software\nengineering as a whole to discuss the advances and challenges in applying deep learning techniques to\nsoftware engineering. In contrast, we discuss the advances and challenges for each software engineering\ntask concerning how deep learning techniques could be employed to resolve the given software engineering\ntask.\nThe survey conducted by Niu et al. [25] reviews pre-trained models used in software engineering. In\ntotal, they identified and analyzed 20 pre-trained models developed for software engineering tasks. They\nclassified the models with four dimensions, i.e., the underlying network architecture, the number of input\nmodalities, the tasks used for pretraining, and whether they are pre-trained on a single programming\nlanguages or multiple programming languages. It also investigated how the models were pretrained for\ndifferent software engineering tasks. Their goal is to raise the awareness of AI technologies, including the\ndevelopment and use of pre-trained models and the successful applications of such models in resolving\nsoftware engineering tasks. Their findings suggest that code pre-training models area promising approach\nto a wide variety of software engineering tasks. Our survey differs from the survey conducted by Niu et\nal. [25] in that the survey by Niu et al. is confined to pre-trained models whereas our survey covers all\ndeep neural networks employed for software engineering tasks.\nZhang et al. [26] conducted a systematic survey to summarize the current state-of-the-art research in\nthe LLM-based SE community. They summarized 30 representative LLMs of source code across three\nmodel architectures, 15 pre-training objectives across four categories, and 16 downstream tasks across\nfive categories. They presented a detailed summarization of the recent SE studies for which LLMs are\ncommonly utilized. Furthermore, they summarized existing attempts to empirically evaluate LLMs in\nSE, such as benchmarks, empirical studies. Finally, they discussed several critical aspects of optimization\nand applications of LLMs in SE. Our survey differs from the survey conducted by Zhang et al. [26] in\nthat our survey focus on all deep neural networks employed for software engineering tasks. However, the\nsurvey conducted by Zhang et al. specifically concentrates on different representative LLMs.\nAs a conclusion, although a few surveys have been made concerning the synergy between software\nengineering and deep learning, we still lack a clear picture of the advances, potentials, and challenges\nconcerning deep learning-driven attempts to various software engineering tasks.\nTo this end, in this\npaper, we select the most fundamental and most challenging software engineering tasks, and for each of\nthem we analyze the advances, potentials, and challenges of its deep learning-based solutions.\n3\nRequirements Engineering\nRequirements engineering (RE) is the process of eliciting stakeholder needs and desires and developing\nthem into an agreed-upon set of detailed requirements that can serve as a basis for all subsequent de-\nvelopment activities.\nUnlike solution-oriented SE tasks (such as software design and program repair)\nthat aim to ensure “doing the thing right”, requirements engineering is a problem-oriented SE task\nthat aims to ensure “doing the right thing”, i.e., to make the problem that is being stated clear and\ncomplete, and to guarantee that the solution is correct, reasonable, and effective [27].\nRecently, more and more RE researchers employ deep learning techniques to elicit, analyze, trace,\nSci\nChina Inf Sci\n7\nvalidate, and manage software requirements. In this section, we will introduce the progress of DL-related\nRE literature from three perspectives, i.e., RE task taxonomy, datasets, and DL models. Then, we will\nsummarize the challenges and opportunities faced by DL-related RE literature.\n3.1\nRequirements Elicitation\nThe task of requirements elicitation aims to gather accurate and complete information about what the\nsystem should do, how it should behave, and what constraints and limitations it should adhere to.\nThis process is critical in software development because the success of a project depends on clearly\nunderstanding requirements and translating them into solutions that meet stakeholder needs.\nHuang et al. [28] proposed a convolution neural network (CNN)-based approach for automatically\nclassifying sentences into different categories of intentions: feature request, aspect evaluating, problem\ndiscovery, solution proposal, information seeking, and information giving, and meaningless. They sped\nup the training process by integrating batch normalization. They also optimized the hyper-parameters\nthrough an automatic hyper-parameter tuning method in order to improve accuracy. Pudlitz et al. [29]\npresented an automated approach for extracting system states from natural language requirements using\na self-trained Named-entity Recognition model with Bidirectional LSTMs and CNNs. They presented\na semi-automated technique to extract system state candidates from natural language requirements for\nthe automotive domain. The results show that their automated approach achieves a F1-score of 0.51,\nwith only 15 minutes of manual work, while the iterative approach achieves an F1-score of 0.62 with\n100 minutes.\nFurthermore, manual extraction took nine hours, demonstrating that machine learning\napproaches can be applied with a reasonable amount of efforts to identify system states for requirements\nanalysis and verification.\nLi etal. [30] proposed a technique called DEMAR based on deep multi-task learning, which can discover\nrequirements from problem reports, and solve the limitations that requirements analysis tasks usually\nrely on manually coded rules or insufficient training data. Through the three steps of data augmentation,\nmodel building, and model training, their experimental results show that the multi-task learning mode of\nDEMAR has higher performance than the single-task mode. Meanwhile, DEMAR also outperforms the\nother selected existing techniques. DEMAR provides directions for exploring the application of multi-\ntask learning to other software engineering problems. Guo et al. [31] proposed Caspar, a technique for\nextracting and synthesizing app question stories from app reviews. Caspar first extracts ordered events\nfrom acquired app reviews and ranks them using NLP techniques. Caspar then classifies these events\ninto user actions or application questions, and synthesizes action-question pairs.\nFinally, an inference\nmodel is trained on the operation-problem pairs.\nFor a given user action, Caspar can infer possible\nprogram problem events, thus enabling developers to understand possible problems in advance to improve\nprogram quality. They experimented with SVM, USE+SVM, and Bi-LSTM networks, respectively; and\nthe results show that the Bi-LSTM model performs better than the other two. Mekala et al. [32] proposed\na deep learning-supported artificial intelligence pipeline that can analyze and classify user feedback.\nThis technique includes a sequence classifier.\nTheir experimental results show that the BERT-based\nclassifier performs the best overall and achieves good performance. Their experimental results further\ndemonstrate that pre-trained embeddings of large corpora are a very effective way to achieve state-of-the-\nart accuracy in the context of low-capacity datasets. Tizard et al. [33] proposed a technique for linking\nforum posts, issue trackers, and product documentation to generate corresponding requirements. They\nscraped product forum data for VLC and Firefox, performed similarity calculations using the Universal\nSentence Encoder (USE), and matched forum posts to issue trackers. Furthermore, they demonstrated\nthat applying clustering to USE results in impressive performance on matching forum posts with product\ndocuments.\nShi et al. [34] proposed a technique called FRMiner to detect feature requests with high\naccuracy from a large number of chat messages via deep Siamese networks.\nIn particular, they used\nSpacy to perform word segmentation, lemmatization, and lowercase processing on sentences, randomly\nselected 400 conversations in three open-source projects for data sampling, and annotated them with the\nhelp of an inspection team. Then, they built a context-aware dialogue model using a bidirectional LSTM\nstructure, and used a Siamese network to learn the similarity between dialogue pairs.\nExperimental\nresults show that FRMiner significantly outperforms two-sentence classifiers and four traditional text\nclassification techniques. The results confirm that FRMiner can effectively detect hidden feature requests\nin chat messages, thus helping obtain comprehensive requirements from a large amount of user data in\nan automated manner.\nSci\nChina Inf Sci\n8\nPanetal. [35] introduced an automated developer chat information mining technique called F2CHAT,\nwhich aims to solve the challenge of retrieving information from online chat rooms. They constructed\na thread-level taxonomy of nine categories by identifying different types of messages in developer chats.\nFurthermore, they proposed an automatic classification technique F2CHAT,which combines hand-crafted\nnon-textual features with deep textual features extracted by neural models.\nThe results show that\nF2CHAT outperforms FRMiner and achieves high performance in cross-project verification, indicating\nthat F2CHAT can be generalized across various projects.\n3.2\nRequirements Generation\nThe task of requirements generation aims to recommend requirements drafts on top of very limited\ninformation (e.g., keywords or structured models).\nMost existing studies on automated requirements generation concentrate on transforming (semi-) struc-\ntured models (e.g., business process models [36,37], i* framework [38,39], KAOS and Objectiver [40–42],\nUML models [43–45], or other representations like security goals in [46]) into specific syntactic pattern-\noriented natural language requirements specifications, based on a set of pre-defined rules. Recently, Zhao\net al. [47] proposed an approach called ReqGen that aims to recommend requirements drafts based on\na few given keywords. Specifically, they first selected keyword-oriented knowledge from the domain on-\ntology and injected it into the basic Unified Pre-trained Language Model (UniLM). Next, they designed\na copy mechanism to ensure the occurrence of keywords in the generated statements.\nFinally, they\nproposed a requirement-syntax-constrained decoding technique to minimize the semantic and syntax dis-\ntance between candidate and reference specifications. They evaluated ReqGen on two public datasets\nand demonstrated its superiority over six existing natural language generation approaches. Koscinski et\nal. [48] investigated the usage of Relational Generative Adversarial Networks (RelGAN) in automatically\nsynthesizing security requirements specifications, and demonstrated promising results with a case study.\n3.3\nRequirements Analysis\nThe purpose of requirements analysis is to systematically identify, understand, and document the software\nrequirements for a given system or software project.\nLi et al. [49] proposed a new technique named RENE, which uses the LSTM-CRF model for require-\nments entity extraction and introduces general knowledge to reduce the need for labeled data.\nThis\ntechnique can more efficiently extract requirements entities from textual requirements, thereby reducing\nlabor costs. They introduced the construction and training process of RENE, and evaluated RENE on an\nindustrial requirements dataset, showing good results. Furthermore, they explored the value of general-\npurpose corpora and unlabeled data, and provided an effective practical approach that can inspire how\nto further improve performance on specific tasks.\n3.4\nSmelly Requirements Detection\nThe task of smelly requirements detection is to identify the potential issues in software requirements that\nmight threaten the success of software projects.\nCasillo et al. [50] proposed to utilize a pre-trained convolutional neural network (CNN) to identify\npersonal, and private disclosures from short texts to extract features from user stories. They constructed\na user-story privacy classifier by combining the extracted features with those obtained from a privacy\ndictionary.\nEzzini et al. [51] proposed an automated approach for handling anaphoric ambiguity in\nrequirements, addressing both ambiguity detection and anaphora interpretation.\nThey developed six\nalternative solutions based on the choices of (1) whether to use hand-crafted language features, word\nembeddings or a combination thereof for classification, (2) whether pre-trained language models like\nBERT are a viable replacement for the more traditional techniques, and (3) whether a mashup of existing\n(and often generic) NLP tools would be adequate for specific RE tasks. Wang et al. [52,53] proposed\na deep context-wise semantic technique to resolve entity co-reference in natural-language requirements,\nwhich consists of two parts. One is a deep fine-tuned BERT context model for context representation, and\nthe other is a Word2Vec-based entity network for entity representation. Then they proposed to utilize\na multi-layer perceptron (MLP) to fuse two representations. The input of the network is a requirement\ncontextual text and related entities, and the output is a predicted label to infer whether the two entities\nare co-referent. Ezzini et al. [54] proposed QAssist, a question-answer system that provides automated\nSci\nChina Inf Sci\n9\nassistance to stakeholders during the analysis of NL requirements. The system retrieves relevant text\npassages from the analyzed requirements document as well as external domain knowledge sources and\nhighlights possible answers in each retrieved text passage. When domain knowledge resources are missing,\nQAssist automatically constructs a domain knowledge resource by mining Wikipedia articles. QAssist\nis designed to detect incompleteness and other quality issues in requirements without the tedious and\ntime-consuming manual process.\n3.5\nRequirements Classification\nThe purpose of requirements classification is to categorize and organize the collected requirements based\non certain criteria or characteristics. By classifying requirements, the goal is to improve the understand-\ning, management, and communication of requirements throughout the software development process.\nBaker et al. [55] proposed to classify software requirements into five categories by using machine learn-\ning techniques:\nmaintainability, operability, performance, security, and availability.\nThey conducted\nexperimental evaluations on two widely used software requirements datasets for convolutional neural net-\nwork (CNN) and artificial neural network (ANN) evaluation.\nThe results show that this technique is\neffective, reaching high precision and recall. In addition, they explored potential applications of the tech-\nnique in software engineering life cycle, including automating the process of analyzing NFRs, reducing\nhuman errors and misunderstandings, and reducing potential requirements-related defects and errors in\nsoftware systems. Tobias et al. [56] proposed an automated requirements classification technique called\nNoRBERT, which uses the transfer learning capabilities of BERT. They evaluated NoRBERT on different\ntasks, including binary classification, multi-class classification, and classification in terms of quality. Their\nexperimental results show that NoRBERT performs well when dealing with natural language requirements\nand can effectively improve the performance of automatic classification methods. Luo et al. [57] proposed\na new requirements classification technique named PRCBERT, which is based on BERT’s pre-trained\nlanguage model and applies flexible prompt templates to achieve accurate classification. They also con-\nducted experiments on a large-scale demand dataset to compare it with other techniques, showing that\nthe technique significantly improves accuracy and efficiency. Winkler et al. [58] proposed an automated\napproach for classifying natural language requirements by their potential verification techniques. They\nproposed to use a convolutional neural network for text classification, and trained it on a dataset created\nby industry requirements specifications. They performed 10-fold cross-validation on a dataset containing\nabout 27,000 industrial requirements, achieving a high F1 score. AlDhafer et al. [59] used Bidirectional\nGated Recurrent Neural Networks (BiGRU) to classify requirements into binary labels (functional and\nnon-functional) and multiple labels (Operational, Performance, Security, Usability, etc).\n3.6\nRequirements Traceability\nThe task of requirements traceability is to build the associations between software requirements and\nother artifacts, which may be different types of requirements, design artifacts, source code, test cases and\nother artifacts. Compared to other tasks in RE, much more research uses DL techniques for requirements\ntraceability.\nTo the best of our knowledge, the first DL for requirements traceability research was from Guo et al. [60]\nin 2017. They proposed the technique of TraceNN to build the links between software requirements and\ndesign documents. In particular, they modeled the task of traceability construction as a classification\nproblem. They embedded the features of text sequences based on the Recurrent neural network (RNN)\nand used the multi-layer perceptron (MLP) to conduct the classification task. They evaluated two types\nof RNN models, namely long short-term memory (LSTM) and gated recurrent unit (GRU) on large-scale\nindustrial datasets, and showed GRU achieved better mean average precision (MAP). In contrst, RNNs\ncan only encode one side of contextual information, which will be weakened for long sequences [61]. With\nthe wide application of BERT since 2018, different variants such as BioBERT [62] and CodeBERT [63]\nhave been developed for various domains. Lin et al. proposed TraceBERT [64] to construct the trace\nlinks between requirements and source code based on CodeBERT, and modeled the traceability as a\ncode search problem. They designed a three-fold procedure of pre-training, intermediate-training and\nfine-tuning towards their tasks. Particularly, they first pre-trained CodeBERT on source code to build\nTraceBERT. Then in the intermediate-training phase, they provided adequate labeled training examples\nto train the model to address the code search problem with the expectation that the model can learn\ngeneral traceability knowledge. Finally, in the fine-tuning phase, they applied the model to the specific\nSci China Inf Sci\n10\nFigure\n1\nThe deep learning models involved in the related work\n“issue (natural language)-commit (programming language) tracing” problem to improve the tracing effect.\nThey evaluated three commonly used BERT architectures (i.e., single, twin, and Siamese) on open-source\nprojects. Their experimental results showed that the single architecture achieves the best accuracy, while\nthe Siamese architecture achieves similar accuracy with faster training time. Tian et al. [65] proposed\na technique named DRAFT to build the traceability between new requirements and other requirements\nin different abstraction levels, during the system evolution process.\nThey performed a second-phase\npre-training on BERT based on the project-related corpus for the purpose of project-related knowledge\ntransformation.\nThen, they designed 11 heuristic features and embedded them with requirements text.\nThe performance of DRAFT has been evaluated with eight open-source projects. Lin et al. [66] explored\nthe performance of information retrieval and deep learning techniques on building trace links in 14\nEnglish-Chinese projects. The involved approaches include Vector Space Model (VSM), Latent Semantic\nIndexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono- and cross-\nlingual word embeddings with the Generative Vector Space Model (GVSM), and adeep-learning approach\nbased on a BERT language model. Theyed show that their TraceBERT performed best in large projects,\nwhile IR-based GVSM worked best on small projects.\nFrom the perspective of paper distribution over different RE tasks, the most four tasks using DL tech-\nniques are requirements elicitation, requirements traceability, smelly requirements detection and require-\nments classification, with 9, 7, 5 and 5 papers, respectively. For the other tasks, including requirements\ngeneration and analysis, the involved papers are few. We did not find any papers related to requirements\nmanagement and requirements validation.\n3.7\nDL Models\nFigure 1 shows the DL models involved in our surveyed studies on requirements engineering. We can see\nthat there are over 10 different kinds of models utilized in these DL4RE studies. Among these models,\nBERT is the most widely-used one, followed by CNN, and the number of papers related to other models\nis far smaller than these two. This may be because BERT and CNN are relatively mature and universal\ndeep learning models, which can adapt to different RE scenarios and datasets with high performance and\nreproducibility. It may also be due to the active and extensive research community of BERT, which can\nprovide rich reference materials and the latest developments, stimulating the interest and innovation of\nresearchers.\nWe can also see that the numbers of papers on the transformer-based model and NN-based model are 11\nand 13 respectively, indicating that they have certain competition and complementarity in requirements\nengineering, and have certain research value and practical significance. There is no absolute difference\nbetween good and bad, but it is necessary to select or design appropriate in-depth learning models\naccording to the specific RE tasks.\nUsage of DL models. Figure 2 illustrates the ways of using deep learning models in requirements\nengineering. Primarily, there are three types: direct-training, pre-training, and fine-tuning. Among them,\n43% of the studies directly build neural networks from training. The advantage of direct training is that\nit can train a model from scratch according to the dataset of the target task, without being limited to the\npre-trained model, and can customize the network structure and parameters towards specific domains\nand tasks. The disadvantage is that if the dataset of the target task is not large enough, direct training\nSci China Inf Sci\n11\nFigure 2\nThe proportion of different usage of DL models\nmay lead to problems such as model non-convergence, overfitting, and low generalization ability, etc.\nConsidering that most of the studies use the way of direct training, it may indicate that this way is\nrelatively simple and effective.\nAnother possible reason is the lack of suitable pre-trained models or\ndomain knowledge.\n36% of studies use pre-trained DL models. The advantage is that it can use the model parameters\ntrained by large-scale datasets, saving time and computing resources, and improving computational\nefficiency and accuracy. The disadvantage is that if the dataset of the pre-trained model and that of\nthe target task are not highly similar, the effect of the pre-trained model may not be satisfactory, because\ndifferent tasks need to extract different features.\n21% of studies use fine-tuned DL models. The advantage of fine-tuning is that it can adjust part or all\nof the parameters based on the pre-trained model according to the dataset of the target task, retaining\nthe ability of the pre-trained model to extract general features and increasing the ability of the model\nto adapt to new task features. The disadvantage is that it needs to choose a suitable pre-trained model,\na suitable fine-tuning level and range, a suitable learning rate, and other hyperparameters. Otherwise,\nit may affect the effect of fine-tuning.\nPre-training and fine-tuning each account for approximately a\nquarter, possibly because the use of pre-training and fine-tuning has certain limitations and complexity.\nPre-training needs to select appropriate pre-trained models and objective functions, and the matching\ndegree and difference between pre-training models and RE problems or data need to be considered.\nFine-tuning needs to select appropriate fine-tuning strategies and parameter settings, and consider the\nsimilarities and differences between different RE tasks or datasets.\nPerformance of DL models. Most papers use three indicators to measure the performance of deep\nlearning models: precision, recall and F1. We find that the precision and recall of deep learning models\nare often above 80%, and the F1 score is often above 75%. The performance of deep learning models\nis related to many factors, such as the structure and parameters of the model, the training methods,\nand the size and quality of the dataset. Researchers often collect as many data as possible when using\ndeep learning models to improve the expected performance of the models. However, it is not necessarily\ntrue that the larger the dataset is, the better the model performance is. The size of the selected dataset\nin [54] is only 387, but the average recall and precision of its model are over 90% and 84%. The dataset\nsize used in [55] is 914, with a precision between 82% and 94%, a recall between 76% and 97%, and an\nF1-score between 82% and 92%. In [35], the size of the used dataset is 2959, the largest one among these\nthree work, but its F1-score is only 62.8%, smaller than the other two. We can see that there are many\nfactors that affect the performance of deep learning models, and the size of the dataset is just one of\nthem. Different factors also affect and constrain each other. We need to analyze and experiment based\non specific situations.\n3.8\nDatasets\nDatasets are a very important part of deep learning, as they are the foundation for training deep learning\nmodels.\nWe focus on the selected datasets for the DL4RE studies and summarize the results into a\nbubble chart, as shown in Figure 3. The x-axis indicates the RE tasks that the studies focused on, and\nthe y-axis shows the size of data entries used in training and testing the involved DL models. The size\nSci China Inf Sci\n12\nFigure 3\nThe size distribution of the datasets involved in the DL4RE studies.\nof the bubble indicates the number of publications. As shown in Figure 3, we can see that 61% of the\nselected publications have a dataset size of less than 6,000, which may be due to the difficulty and cost\nof data collection. In some fields, data may be difficult to obtain and require professional equipments,\npersonnel, or licenses.\nWe can also find that the datasets used in the tasks of requirements detection, requirement generation,\nand requirement classification are relatively smaller, which may be due to the phenomenon of data-\nhungriness in the RE community. As software requirements often embed the value that software products\naim to deliver to their users, the requirements documents are typically private and confidential. Thus,\nit is difficult for DL4RE studies to obtain enough data for training effective DL models. Requirements\ngeneration is a text generation problem that can be improved by utilizing pre-trained language models\nwithout requiring a large amount of domain data. Besides, some techniques, such as data enhancement,\ntransfer learning, and miniaturization networks, can be leveraged to reduce corpus size. In the meanwhile,\nrequirements elicitation studies have larger datasets since it is a comprehensive task involving knowledge\nin multiple fields and multiple information sources.\nMoreover, many studies leverage the data in the\nopen-source community, such as feature requests or live chats, to build their deep-learning models.\n3.9\nChallenges and Opportunities\n3.9.1\nChallenges\n• Low transparency of public requirements.\nUnlike other software development activities,\nthe publicly accessible data for requirements typically are small in volume and lack details. For\nproprietary software, since requirements reflect the delivered value of the software, organizations\nusually consider requirements as confidential assets and are reluctant to open them.\nFor open-\nsource software (OSS), their requirements are scattered in massive informal online discussions, such\nas issue reports [34] or live community chats [30]. Although there are some widely-used requirements\nbenchmarks, such as PROMISE and MODIS, the scale cannot be compared to that of open-source\ncode in GitHub. Besides, many publicly accessible benchmarks depict requirements in textual and\nentry-oriented formats, lacking details on project-level information, such as detailed background,\nstakeholder preferences, and graphic presentations. Thus, it becomes difficult for RE researchers\nto comprehensively understand the rationale behind the textual requirements, as well as efficiently\ntrain DL models for RE.\n• High diversity in representations. Software requirements are typically depicted in a variety of\nformats, including textual formats, data-entry formats, user stories, use cases, as well as (semi-)\nstructured models. Notably, the representation within each format can also vary significantly. For\ninstance, when it comes to modeling requirements, representations might take the form of Uni-\nfied Modeling Language (UML), Systems Modeling Language (SysML), Goal-oriented models, etc.\nSci China Inf Sci\n13\nAlthough guideline standards like ISO/IEC/IEEE 29148:2018 [67] and Easy Approach to Require-\nments Syntax (EARS) [68] are provided, their adoption in the industry is limited. It is reported\nthat nearly half (47%) of RE practitioners are unaware of these standards [69]. Consequently, the\nrepresentation of requirements often aligns with individual writing practices rather than standard-\nized guidelines. This significant diversity in representations poses a considerable challenge for DL\nmodels in accurately interpreting requirements. Additionally, it presents a notable obstacle in train-\ning DL models too, compounding the complexity of achieving reliable performance across varied\nformats.\n• Hidden domain-specific knowledge and experiences. Many requirements-related tasks (such\nas specification, modeling, and analysis) hinge on domain-specific knowledge and the accumu-\nlated experience of experts. For example, it is commonly accepted that high-quality requirements\nmust embody three key characteristics: Correctness, Consistency, and Completeness — collectively\nknown as the ‘3Cs’. Yet, these attributes are not clearly and formally defined in the industry, nor\nare there universally recognized detailed metrics to evaluate whether a set of requirements meets\nthe 3C standard. Consequently, this aspect of the work often depends on the nuanced insights of\nexperienced engineers. Regrettably, requirements engineers typically do not document this critical\ndomain knowledge or their hands-on engineering experiences. This documentation is not typically\nrequired by project management protocols. Moreover, the challenge of accurately capturing the ob-\njective essence of such experiential knowledge remains unresolved. This gap means that DL models\nare currently difficult to absorb and apply such valuable expertise to support RE tasks.\n3.9.2\nOpportunities\n• Needs for research on piecing scattered OSS requirements into a big picture.\nThe\ninfluence and momentum of OSS on other SE tasks, like code generation and repair, is substantial\nand revolutionary. To advance AI4RE research, we appeal to construct evolving requirements for\nOSS. As the above discussion, OSS requirements are scattered in massive informal online discussions.\nTherefore, it is crucial to embark on research focused on pinpointing discrete pieces of requirements\ninformation, assembling them into coherent and intelligible requirements documents, and ensuring\ntheir evolution in tandem with changes in the code base.\n• Exploring the feasibility and usability of AI-based RE approaches in real industrial\nsettings.\nWhile AI-based approaches have demonstrated efficacy in a multitude of tasks, their\neffectiveness for RE-tasks in real-world industrial settings remains uncertain.\nIndeed, there was\neven evidence that practicing engineers are reluctant to rely on AI models for higher-level design\ngoals [70].\nThis presents a significant opportunity to delve into and expand upon this uncharted\narea of RE research.\n• Leveraging the interaction capability of LLMs to enhance requirements activities. High-\nquality requirements lay the foundation for robust software products.\nIn essence, the success of\nLLMs in various SE tasks is contingent upon the quality of these underlying software requirements.\nGiven the impressive interactive capabilities of LLMs, a promising avenue is to harness these models\nto engage with diverse stakeholders intelligently. Through such interactions, LLMs can be leveraged\nto efficiently carry out tasks like requirements discovery, negotiation, and validation, anchored by\nrapid prototyping.\n4\nCode Generation\nIn practice, a highly efficient code generation model may significantly boost developers’ productivity,\nenabling them to complete programming tasks by simply inputting target descriptions into code genera-\ntors [71]. Therefore, automated code generation can be deemed as an important objective for developers,\nhighlighting the crucial role of the code generation task in the software engineering (SE) field. In addition,\ncode completion is one of the most widely used code generation tasks in integrated development environ-\nments (IDEs) [72]. According to the study on the Eclipse IDE by Murphy et al. [73], code completion\nis one of the top ten commands used by developers. Recent years have seen increasing interest in code\ncompletion systems using deep learning techniques [74,75].\nSci China Inf Sci\n14\nTo achieve this goal, within the academic community, a range of studies [76] have put forth deep\nlearning (DL)-based models for automatically generating code fragments from input utterances. To gain\na deeper understanding of these advanced techniques, in this section, we survey different studies on DL-\nbased code generation. In particular, we survey the following three types of research: 1) research on\nproposing new techniques, 2) research on empirical evaluations, and 3) research on constructing datasets.\nDue to the specific characteristics of code, it is necessary to invent specialized deep learning techniques\nto deal with code. Therefore, we can see a number of influential deep learning techniques for code stem\nfrom the area of code generation. Table 4 highlights the main technical contributions in code generation.\nSince code completion is one of the most important code generation applications, we also survey deep\nlearning based code completion.\nThe primary contributions of the analyzed studies can be classified into nine distinct categories, high-\nlighting the diverse range of advancements and innovations within code generation.\n4.1\nEnhancing Code Structure Information\nSix studies highlight the oversight of rich structural information in many code generators [77]. To address\nthis concern, diverse approaches have been proposed for incorporating additional code structure infor-\nmation into their DL-based models, aiming to enhance the overall performance. Rabinovich et al. [78]\nintroduced abstract syntax networks, a DL-based model that incorporates supplementary structural in-\nformation from abstract syntax trees (ASTs).\nTheir model utilizes an encoder-decoder BiLSTM with\nhierarchical attention, aiming to generate well-formed and executable Python code fragments. Jiang et\nal. [79] observed that the standard Seq2Tree model translates the input natural language description into\na sequence based on the pre-order traversal of an Abstract Syntax Tree (AST). However, this traversal\norder might not be optimal for handling multi-branch nodes in certain cases. To address this, they put\nforward the idea of enhancing the Seq2Tree model with a context-based Branch Selector, enabling it to\ndynamically determine the optimal expansion orders for multi-branch nodes.\n4.2\nSpecial Code Generation\nFour studies employ DL techniques to generate code fragments for less common programming languages\nor in unusual logical forms. Yuetal. [84] introduced a novel DL model specifically designed for generating\nSQL code for test code scenarios. Yang et al. [86] introduced a pre-trained model to generate assembly\ncode from NL descriptions.\n4.3\nMulti-mode based Code Generation\nThe studies in this category construct the code generators by taking into account multiple code artifacts\nin a comprehensive manner. Le et al. [90] noticed that most code generators overlook certain crucial yet\npotentially valuable code specifications, such as unit tests, which frequently leads to subpar performance\nwhen addressing intricate or unfamiliar coding tasks. They thus introduced a new generation procedure\nwith a critical sampling strategy that allows a model to automatically regenerate programs based on\nfeedback from example unit tests and NL descriptions. Wang et al. [91] integrated NL descriptions with\nthe specific attributes of programming languages, such as token types, to construct CodeT5, which is a\nunified pre-trained encoder-decoder Transformer model designed for generating code fragments.\n4.4\nCompilability\nThree studies concentrate on the development of DL-based code generators with the objective of gen-\nerating executable code fragments across multiple programming languages. Sun et al. [92] identified a\nsignificant number of inaccuracies and non-executable SQL code generated as a result of the mismatch\nbetween question words and table contents. To mitigate this problem, they took into account the table\nstructure and the SQL language syntax to train a DL-based SQL generator. Wang et al. and Poesia et\nal. [93,94] leveraged the powerful capability of large language models (LLMs) to improve the compilability\nof generated code.\nSci China Inf Sci\n15\nTable 4\nCharacteristics of studies within the Design Research category.\nContribution\nModel Type\nProgramming Language (PL)\nReference\nEnhanced Code Structural Info\nDL\nDL\nDL\nDL\nPre-trained\nPython\nJava\nJava, Python\nPython, SQL\nPython\n[78]\n[80]\n[81]\n[82]\n[79]\nSpecial Code Generation\nDL\nDL\nDL\nPre-trained\nConditional Statement\nSQL (Test Code)\nPseudo-code\nAssembly, Python\n[83]\n[84]\n[85]\n[86]\nMulti-mode based\nDL\nPre-trained\nPre-trained\nJava, Python\nPython\nGo, Java, JavaScript, PHP, Python, Ruby\n[87,88]\n[89,90]\n[91]\nCompilability\nDL\nPre-trained\nPre-trained\nSQL\nPython\nSQL, Vega-Lite, SMCalFlow\n[92]\n[93]\n[94]\nDual Learning based\nDL\nPre-trained\nPre-trained\nJava, Python\nJava\nPython, SQL\n[95]\n[96]\n[97]\nSearch-based\nDL\nDL\nPre-trained\nPython\nC++\nJava, Python\n[98]\n[99]\n[100]\nContext-aware\nDL\nPre-trained\nJava\nPython\n[101,102]\n[103]\nPracticality\nDL\nDL\npython, SQL\nJavascript\n[104]\n[105]\nLong Dependency Problem\nDL\nPython\n[77,106,107]\nSci China Inf Sci\n16\n4.5\nDual-Learning based Code Generation\nThree studies capitalize on the dual connections between code generation (CG) and code summarization\n(CS) to generate accurate code fragments from NL descriptions. Wei et al. [95] exploited the duality\nbetween code generation and code summarization tasks to propose a DL-based dual training framework to\ntrain the two tasks simultaneously. Ahmad etal. and Yeetal. [96,97] leveraged the inherent relationship\nbetween CG and CS and utilized pre-trained models to enhance the accuracy of the code generation task.\n4.6\nCode Generation on Top of Existing Code\nSeveral studies have observed that generating code based on existing related code fragments can yield\nsuperior performance compared to generating code from scratch. As a result, these studies incorporate\ncode search techniques and leverage DL to construct their code generators. For instance, Hashimoto et\nal.\nand Kulal et al. [98,99] employed DL-based retrieval models to generate Python and C++ code.\nAdditionally, Parvez et al. [100] utilized information retrieval techniques along with pre-trained models\nto develop a code generator capable of generating code in multiple programming languages, such as Java\nand Python.\n4.7\nContext-aware Code Generation\nRecognizing that the code fragments generated by many existing code generators may not be directly\napplicable in software, several studies have proposed to incorporate code contexts to enhance the accuracy\nof code generation.\nGuo et al. [102] introduced a context-aware encoder-decoder model with a latent\nvariable in their code generator, enabling it to incorporate the contextual environment during code\ngeneration. Li et al. [103] proposed SKCODER, an approach for sketch-based code generation, aiming\nto simulate developers’ code reuse behavior. SKCODER retrieves a similar code snippet based on an\nNL requirement, extracts relevant parts as a code sketch, and then modifies the sketch to generate the\ndesired code.\n4.8\nPracticality\nTwo relevant studies aim to improve the practicality of code generators.\nDong et al. [104] introduced\na structure-aware neural architecture for code generation that exhibits adaptability to diverse domains\nand representations. Shen et al. [105] proposed a task augmentation technique that integrates domain\nknowledge into code generation models, making their model the first domain-specific code generation\nsystem adopted in industrial development environments.\n4.9\nLong Dependency\nMany deep learning-based code generators are trained using recurrent neural networks (RNNs) such as\nLSTM [108], BiLSTM [109], and GRU [110].\nTo overcome the long-term dependency problem, three\nstudies introduce novel techniques to tackle this challenge. Sun et al. [77] proposed a novel tree-based\nneural architecture and applied the attention mechanism of Transformers to alleviate the long-dependency\nproblem. Xie et al. [107] utilized mutual distillation learning to train a code generator in order to avoid\nthe occurrence of this problem.\n4.10\nCode Completion\nTable 5 shows code completion techniques published in the premier publication venues (i.e., ASE,ICSE,\nand FSE) from 2020 to 2023. Since 2020, there have been six papers focusing on the code completion task,\nin which three of them exploit the pre-trained models or large language models.\nGPT-2 is widely used\nin completing source code.\nAmong all programming languages, Java and Python are the most popular\nprogramming languages. Wang et al. [111] conducted an empirical study that investigated developers’\nperspectives on code completion. Liu et al. [112] and Izadi et al. [113] integrated the multi-task learning\ntechniques by learning different types of information of the source code (e.g., token sequences and ASTs).\nTang et al. [114] introduced the retrieval-augmented language model to conduct domain adaption and\nimprove the performance of existing LLMs (e.g., ChatGPT and UniXcoder) on the code completion task.\nTheir results show that retrieval techniques can be seamlessly integrated with black-box code completion\nmodels and as a plugin to further enhance the performance of LLMs.\nSci China Inf Sci\n17\nTable 5\nDeep Learning based Code Completion Studies\nYear\nVenue\nLiterature\nLanguage\nModel\n2020\nASE\n[112]\nJava, TypeScript\nMulti-task learning, Pre-trained models\n2022\nICSE\n[113]\nPython\nMulti-task learning, GPT-2\n2023\nASE\n[114]\nJava, Python\nRetrieval-augmented language model\n2023\nFSE\n[115]\nJava, Python\nGPT-2, CodeT5\n2023\nFSE\n[111]\n-\nEmpirical Study\n2023\nICSE\n[116]\nJava\nTransformer\n4.11\nEmpirical Studies\nFour studies [117–120] undertake empirical investigations to examine the characteristics of existing code\ngenerators. Dahal et al. [117] leveraged text-to-tree, linearized tree-to-tree, and structured tree-to-tree\ncode generation models to perform an empirical analysis of the significance of input forms for code\ngeneration.\nThey found that using a structure-aware model improves the performance of models on\nboth two datasets. Norouzi et al. [118] examined whether a generic transformer-based Seq2Seq model\ncan achieve competitive performance with the minimal design of code-generation-specific inductive bias.\nThey observed that it is possible for a transformer-based Seq2Seq model with minimal specific prior\nknowledge to achieve results that are superior to or on par with state-of-the-art models specifically\ntailored for code generation. Mastropaolo et al. [119] presented an empirical study to investigate whether\ndifferent but semantically equivalent NL descriptions yield the same code fragments. The experimental\nresults demonstrate that modifying the description leads to generating different code in approximately\n46% of cases. Furthermore, differences in semantically equivalent descriptions can have an impact on\nthe correctness of the generated code (±28%). Xu et al. [120] conducted a comprehensive user study\non code generation and retrieval within an integrated development environment (IDE), developing an\nexperimental harness and framework for analysis. They noticed that developers raise concerns about the\npotential side effects of code generators on their workflow, encompassing aspects such as time efficiency,\ncode correctness, and code quality.\n4.12\nDatasets\nThere are four studies that construct available benchmark datasets for the code generation task. Table 6\nprovides detailed information about the studies within the dataset construction category. Specifically, Iyer\net al. [101] emphasized the significance of code contexts in the code generation task. To achieve accurate\ncode fragment generation based on corresponding code contents and natural language descriptions, they\ndeveloped CONCODE, a dataset including over 100,000 examples comprising Java classes sourced from\nonline code repositories. Lianget al. [121] introduced a novel code generation task: to generate a program\nin a base imperative language with an embedded declarative language, given a natural language comment.\nTo support this task, they created a dataset (i.e., Lyra) consisting of 2,000 carefully annotated database\nmanipulation programs extracted from real-world projects.\nEach program is associated with both a\nChinese comment and an English comment. To accurately assess the performance of code generation,\nHendrycks et al. [122] introduced APPS, a benchmark specifically tailored for code generation in more\nrestricted settings compared with the prior benchmarks. Their benchmark comprises 10,000 problems,\nspanning from simple online solutions to substantial algorithmic challenges. Lu et al. [123] developed a\ncomprehensive dataset known as CodeXGLUE. This dataset covers a diverse set of 10 tasks across 14\ndifferent datasets, encompassing eight programming languages, and it serves as a platform for evaluating\nand comparing models in the field of code generation.\n4.13\nChallenges and Opportunities\nIn this section, we highlight some challenges and opportunities for future research in deep learning\ntechniques for code generation:\nSci China Inf Sci\n18\nTable 6\nThe detailed information of studies within the Dataset Construction category.\nPL\nDataset Name\nReference\nJava\nCONCODE\n[101]\nPython\nLyra\n[121]\nPython\nAPPS\n[122]\nPython, Java, PHP, JavaScript, Ruby, Go, C/C++\nCodeXGLUE\n[123]\n4.13.1\nChallenges\n• Unsafe Code Generation. Deep learning techniques, especially recently proposed large language\nmodels (LLMs), are (pre-)trained on massive code bases and then applied to code generation. The\ncode bases may include vulnerable code snippets that lead to generation of unsafe code. Thus, how\nto generate functionally correct and safe code is challenging.\n• Benchmarks. Existing benchmarks for code generation mainly include hand-written programming\nproblems and their corresponding solutions (such as HumanEval). However, there is a huge differ-\nence between these human-written benchmarks and real projects. In addition, the human-written\nbenchmarks are time-consuming and strongly dependent on experts’ knowledge. Thus, constructing\na benchmark from real projects automatically is important for code generation.\n• Hallucination of LLMs. Recently, many LLMs have been exploited for code generation, such\nas Copilot. Existing studies have shown that LLMs, such as ChatGPT, often generate fabricated\nor inaccurate responses, which are commonly referred to as the hallucination phenomena [124].\nHallucination makes LLMs not always reliable in generating code snippets. It is crucial to combine\nthe capabilities of ChatGPT with human expertise to ensure the quality and reliability of the\ngenerated code.\n4.13.2\nOpportunities\n• Knowledge-Augmented Code Generation.\nExisting studies have shown that recently pro-\nposed LLMs can generate code effectively. To better adapt LLMs to generate code for a specific\ndomain, knowledge-augmented code generation is helpful. It thus is important to integrate different\ninformation, such as project information and similar code snippets, to boost existing LLMs.\n• Managing datasets as software.\nDatasets (including the training data and benchmarks) are\nimportant in training and evaluating a code generation model. As more and more datasets have\nbeen proposed in recent years, we need to better manage the datasets for code generation models.\nNowadays, the datasets are also evolved and hosted in collaborating platforms, such as GitHub and\nHugging Face. Similar to software, we should improve dataset productivity, quality, and security.\n5\nCode Search\nCode search is the process of finding relevant code snippets from online or local code repositories based\non query statements, typically expressed in natural language or code itself.\n5.1\nNatural Language based Code Search\n5.1.1\nInformation Retrieval\nEarly code search engines primarily relied on Information Retrieval (IR) techniques to match query key-\nwords with code snippets. These techniques assess the relevance of queries and code based on their\ntextual similarity [125–128]. Subsequent approaches enhance code search by delving into the structural\naspects of source code. They consider diverse relationships between code entities and sought matches\nbetween relevant APIs. A notable trend is the representation of code as directed graphs, effectively trans-\nforming the search task into a graph exploration problem. For instance, McMillan et al. [129] introduced\nPortfolio, a tool that pinpoints potential methods containing query keywords within an API call graph.\nThe tool then ranks the results using a combination of PageRank scores to evaluate node importance and\nSci China Inf Sci\n19\nSpreading Activation Network (SAN) scores to gauge query relevance. In a similar vein, Li et al. [130]\npresented RACS, a technique founded on relations. It parses natural language queries into action graphs\nand code into relation invocation graphs. This enables a structural alignment between the two graph\nrepresentations, thereby elevating the accuracy of matches. However, these approaches encounter limita-\ntions stemming from the pronounced disparities between programming languages and natural languages.\nConsequently, the comprehension of semantics remains challenging for IR-based approaches.\n5.1.2\nDeep Learning\nTo establish more robust semantic connections between natural language queries and code, researchers\nhave increasingly turned to deep learning models to tackle code search tasks. The fundamental approach\ninvolves encoding both the query statement and code into separate vector representations, then assessing\nthe semantic correlation between the two through vector similarity analysis. Gu et al. [19] innovatively\nemployed perceptrons and Recurrent Neural Networks (RNNs) to embed various code elements such as\nmethod names, API call sequences, and code sequences into a shared high-dimensional space. This lays\nthe foundation for DeepCS,a code search tool. Sachdevetal. [131] introduced NCS (Neural Code Search),\ntailored for extensive code repositories. NCS combines word embeddings with TF-IDF to generate vector\nrepresentations for code snippets and query statements. It then gauges relevance through vector distances,\nsimulating the significance of code snippets in relation to queries.\nAs deep learning progresses, subsequent research integrates more complex representations and more\nsophisticated models for code vectorization. Ling and Zou [132] introduced a novel source code search\ntechnique employing graph embedding. It involves creating a code graph from a software project’s source\ncode, representing code elements using graph embedding, and then utilizing this structure to answer nat-\nural language queries by returning relevant subgraphs composed of code elements and their relationships.\nGu et al. [133] proposed CRaDLe, a novel approach for code retrieval based on statement-level semantic\ndependency learning.\nCRaDLe distills code representations by merging dependency and semantic in-\nformation at the statement level, ultimately learning unified vector representations for code-description\npairs to model their matching relationship. Wanetal. [134] introduced MMAN,a Multi-Modal Attention\nNetwork designed for semantic source code retrieval. They created a holistic multi-modal representation\nby utilizing LSTM for sequential tokens, Tree-LSTM for code’s AST, and GGNN for its CFG, followed\nby a multi-modal attention fusion layer that combines and assigns weights to different components for\nan integrated hybrid representation.\nLing et al. [135] introduced an end-to-end deep graph matching\nand searching (DGMS) model for semantic code retrieval. They represented query texts and code snip-\npets as unified graph-structured data, and used the DGMS model to retrieve the most relevant code\nsnippet by capturing structural information through graph neural networks and fine-grained similarity\nthrough cross-attention based semantic matching operations. Liu et al. [136] presented GraphSearchNet,\na neural network framework that improves source code search accuracy by simultaneously learning from\nsource code and natural language queries. They introduced bidirectional GGNN (BiGGNN) to create\ngraphs for code and queries, capturing local structural details, and enhanced BiGGNN using a multi-\nhead attention module to incorporate global dependencies for enhanced learning capacity. Li et al. [137]\nintroduced CodeRetriever, which obtains function-level code semantic representations via extensive code-\ntext contrastive pre-training. This involves unimodal contrastive learning that uses function names and\ndocumentation to build code pairs, and bimodal contrastive learning that utilizes code comments and\ndocumentation for code-text pairs, both contributing to effective pre-training using a vast code corpus.\nJiang et al. [138] introduced ROSF, a technique that enhances code snippet recommendations by com-\nbining information retrieval and supervised learning.\nThe approach involves two stages: generating a\ncandidate set using information retrieval and then re-ranking the candidates based on probability values\npredicted by a trained model, resulting in improved code snippet recommendations for developers.\nIn recent years, significant progress has been made in the realm of large pre-trained models based on\nthe Transformer architecture, driving advancements across numerous NLP tasks. This progress leads to\nthe emergence of code understanding pre-training models leveraging transformers, fostering the growth\nof code intelligence. For instance, Feng et al. [63] introduced CodeBERT, a pioneering large-scale pre-\ntrained model that integrates natural language and programming language understanding across multiple\nprogramming languages. CodeBERT harnesses Masked Language Modeling (MLM) to capture the se-\nmantic relationship between natural language and code. Researchers have explored the incorporation of\nmultiple modal representations of source code into the Transformer paradigm to gain a comprehensive\nSci China Inf Sci\n20\nunderstanding. Guo et al. [139] developed the GraphCodeBERT model, seamlessly combining the vari-\nable sequence from data flow graphs with the code token sequence. This model undergoes training via\nMLM, Edge Prediction (EP), and Node Alignment (NA) tasks to encompass both code structures and\ndata dependencies.\nSimilarly, Guo et al. [140] introduced UnixCoder, which fuses serialized Abstract\nSyntax Trees (ASTs) with comment text sequences. By utilizing MLM, Unidirectional Language Model-\ning (ULM), DeNoiSing (DNS), Multi-modal Contrastive Learning (MCL), and Cross-Modal Generation\n(CMG), this model enriches its comprehension of code syntax and semantics. Some researchers further\nleveraged contrastive learning to enhance model performance. Shi et al. [141] introduced CrossCS,a tech-\nnique that improves code search through cross-modal contrastive learning. They devised a novel objective\nconsidering both inter- and intra-modality similarity, used data augmentation for semantic consistency,\nand boosted pre-trained models by ranking code snippets with weighted similarity scores based on re-\ntrieval and classification scores. Bui et al. [142] presented Corder, a self-supervised contrastive learning\nframework for source code models. It aims to reduce the need for labeled data in code retrieval and sum-\nmarization tasks by training the model to differentiate between similar and dissimilar code snippets using\ncontrastive learning and semantic-preserving transformations. Additionally, Shi et al. [143] introduced\nCoCoSoDa, which employs contrastive learning for code search, incorporating soft data augmentation\nand negative samples.\nThey also applied multimodal contrastive learning to enhance code-query pair\nrepresentations.\n5.1.3\nQuery Expansion and Refinement\nSignificant differences in expression and vocabulary between natural languages and code are key fac-\ntors contributing to the mismatch between high-level intents implied in natural languages and low-level\ncode implementations [19], impacting the accuracy of code search. Improving the query statement or\nthe candidate code has been proved to be an essential approach for enhancing code search effectiveness.\nBajracharya et al. [144] introduced Sourcerer, an open-source code search engine that extracts detailed\nstructural information from code and stores it in a relational model.\nThis information facilitates the\nimplementation of CodeRank and supports search forms beyond traditional keyword-based searches. Lu\net al. [145] introduced an approach that extends queries using synonyms from WordNet, which involves\nextracting natural language phrases from source code identifiers, matching expanded queries with these\nphrases, and sorting the search results. Fei et al. [146] introduced CodeHow, a code search technique\ncapable of recognizing potential APIs referenced in a user query. After identifying relevant APIs, Code-\nHow expands the query with these APIs and performed code retrieval using the extended boolean model,\nincorporating both text similarity and potential APIs for improved search. Mohammad et al. [147] used\ncontext-awareness and data analysis to apply appropriate term weighting in query reformulation, thereby\nenhancing code search. Hill etal. [148] presented a search technique based on method signature analysis,\ninvolving the rewriting of code method names and subsequent matching of the altered method names\nwith queries to facilitate the search process. Additionally, Liu et al. [149] introduced the NQE model,\nwhich predicts keywords related to the query keywords in the corpus based on natural language queries.\nThis technique expands query statements and subsequently improves code search effectiveness. Alongside\nutilizing identifiers in source code, researchers explores leveraging search logs from platforms like stack\noverflow to enhance code search. Cao et al. [150] analyzed large-scale search logs from stack overflow to\nidentify patterns in query reformulation. They constructed a corpus encompassing both original queries\nand their reconstructed versions, and then trained a model using this corpus. The trained model can\ngenerate a list of candidate reconstructed queries when provided with a user query, offering improved\nsearch options. Li etal. [151] introduced a generation-augmented query expansion framework that utilizes\ncode generation models to enhance code retrieval. Instead of relying solely on documentation queries,\nthe approach involves augmenting queries with generated code snippets from the code generation model,\ndrawing inspiration from the human retrieval process of sketching an answer before searching.\n5.2\nCode-to-Code Search\nIn addition to searching for code based on natural language input, code snippets are also utilized as\ninput for code search, divided into searching within the same programming language and across different\nprogramming languages.\nA notable work for searching within the same language is Aroma proposed\nby Luan et al. [152]. Aroma takes incomplete code snippets as input and searches for similar complete\ncode snippets from pre-indexed open-source projects. Compared to searching within the same language,\nSci China Inf Sci\n21\nTable 7\nNatural Language based Code Search Datasets\nDataset\nLanguage\nSize\nSource\nRelease Year\nStaQC\n[133]\nCoNaLa [135]\nFB-Java [141]\nPython, SQL\nPython, Java\nJava\nPython, Java,\n267k\n2.8k\n287\nSO\nSO\nSO, GitHub\n2018\n2018\n2019\nCodeSearchNet [142]\nRuby, Go, PHP,\nJavaScript\n2M\nGitHub\n2019\nSO-DS [143]\nCosBench [151]\nCodeXGLUE [128]\nCoSQA [157]\nPython\nJava\nPython\nPython\nC#, C++, C, D,\nGo, Haskell, Java,\n2.2k\n52\n281k\n20k\nSO,\nGitHub\nSO,\nGitHub\nBing, GiHub\nBing, GitHub\n2020\n2020\n2020\n2021\nXCodeEval [158]\nJavascript, Kotlin,\nOcaml, Pascal, Perl,\nPHP, Python, Ruby,\nRust, Scala\n11k\nCodeforces\n2023\ncross-language code search is more challenging due to syntactic and semantic differences across languages.\nMathew et al. [153] introduced the COSAL approach, which performs non-dominated sorting based on\nsimilarities between code snippets, including AST structures and input-output behaviors, to facilitate\ncode search within the same language and across languages. Additionally, cross-language search is used\nfor code translation, such as converting Java code into Python code with the same functionality. Perez et\nal. [154] employed LSTM networks to model clone similarity between cross-language code snippets based\non ASTs, and Nguyen et al. [155] utilized the API2Vec model, inspired by Word2Vec, to embed APIs\ninto high-dimensional vectors for cross-language code translation. Chen et al. [156] introduced BigPT,\na technique for interactive cross-language retrieval from Big Code, involving a predictive transformation\nmodel based on auto-encoders to aid program translation using retrieved code representations. Users are\nable to further refine the retrieval results to improve the process.\n5.3\nDatasets\nThe following datasets in Table 7 are commonly used for natural language based code search.\nThe StaQC dataset [133] is tailored for predicting the suitability of code snippets in addressing specific\nqueries. Comprising (question, code) pairs, it was curated by filtering Python and SQL Stack Overflow\nposts tagged with “how-to” questions, resulting in 147,546 Python pairs and 119,519 SQL pairs.\nThe CoNaLa dataset [135] consists of 2,379 training and 500 test examples, manually annotated with\nnatural language intents and corresponding Python snippets.\nThe FB-Java dataset [141] comprises 287 natural language queries and relevant code snippet answers\nfrom Stack Overflow threads tagged with “java” or “android”.\nAdditionally, it includes code snippet\nexamples from the search corpus, sourced from public repositories on GitHub, that correctly answer the\ncorresponding queries.\nThe CodeSearchNet corpus [142] is an extensive collection of approximately 6 million functions auto-\nmatically gathered from open-source code spanning six programming languages (Go, Java, JavaScript,\nPHP, Python, and Ruby). It includes 2 million functions with query-like natural language descriptions\nobtained via scraping and preprocessing associated function documentation. Furthermore, it contains\n99 natural language queries with around 4,000 expert relevance annotations of likely results from the\nCodeSearchNet Corpus.\nThe SO-DS corpus [143] consists of code snippets mined from Stack Overflow posts with the most\nupvoted posts labeled with “python” and tags related to data science libraries such as “tensorflow,”\n“matplotlib,” and “beautifulsoup.” The ground truth is collected by creating queries from duplicate\nStack Overflow posts, resulting in 2,225 annotated queries.\nSci China Inf Sci\n22\nThe CosBench corpus [151] comprises 475,783 Java files and 4,199,769 code snippets (Java methods)\nextracted from the top-1000 popular Java projects on GitHub. It includes 52 queries with ground truth\ncode sbippets indicating three types of intentions: bug resolution, code reuse, and API learning, chosen\nfrom Stack Overflow.\nThe CodeXGLUE [128] serves as a benchmark dataset and an open challenge for code intelligence,\nencompassing various code intelligence tasks. For NL-based code search, it includes two sub-datasets:\nAdvTest and WebQueryTest. AdvTest, constructed from the CodeSearchNet corpus [142], uses the first\nparagraph of documentation as a query for the corresponding function. WebQueryTest is a testing set of\nPython code questions answered with 1,046 query-code pairs and expert annotations.\nThe CoSQA dataset [157] is based on real user queries collected from Microsoft’s Bing search engine\nlogs. It encompasses 20,604 labels for pairs of natural language queries and code snippets, each annotated\nby at least 3 human annotators.\nThe xCodeEval [158] is recognized as one of the most extensive executable multilingual multitask\nbenchmarks, encompassing seven code-related tasks that span across 17 programming languages. Derived\nfrom a pool of 25 million openly available samples from codeforces.com, a platform hosting competitive\nprogramming contests, this dataset comprises 7,514 distinct problems. In terms of code retrieval, xCodeE-\nval introduces a novel and more demanding task, specifically centered on matching a natural language\nproblem description to the most relevant and accurate code within a candidate pool containing similar\nsolutions. To facilitate this, all submitted code snippets and their associated test cases are aggregated for\neach programming language, creating a retrieval corpus and a suite of test cases. The primary objective\nis to evaluate the correctness of these code snippets against the provided test cases. In this context, the\nnatural language problems serve as queries, and the correct solutions, verified by successful execution\noutcomes (PASSED), are considered as the ground truth.\nFor code-to-code search, existing datasets designed for code clone detection, featuring clusters of se-\nmantically equivalent implementations, can be utilized. One such dataset is the BigCloneBench provided\nwithin CodeXGLUE [128]. In these datasets, a group typically comprises variations of the same imple-\nmentation. In practice, one implementation within a group can serve as a query, while the remaining\nimplementations within the cluster act as the ground truth.\nAdditionally, xCodeEval [158]\noffers a\ndataset specifically tailored for code-to-code tasks. This dataset consists of 9,508 queries, created from\ncorrect submitted solutions to the same natural language problems, adding diversity to the evaluation of\ncode-to-code search capabilities.\n5.4\nChallenges and Opportunities\n5.4.1\nChallenges\n• Quality Assurance of Search Results. Ensuring the quality of code search results goes beyond merely\nmatching search intent. Factors such as correctness, security, and timeliness must be considered to\nguarantee the reliability and suitability of the returned code snippets.\n• Long Tail Issues. Addressing the challenges posed by less common, long-tail issues is essential. Code\nsearch systems need to effectively handle diverse and infrequent queries, ensuring comprehensive\ncoverage across a spectrum of coding scenarios.\n• Result Interpretability. Achieving interpretability in code search results is also a challenge. Only\nlittle research effort (e.g., [159]) on this direction. It involves presenting search outcomes in a clear\nand understandable manner, aiding developers in comprehending the context and relevance of the\nreturned code snippets.\n• Integrating Retrieved Code into Development Context.\nEffectively utilizing code search results\nposes a significant challenge.\nTypically, developers expend considerable effort in adapting and\nintegrating a retrieved code snippet into the current context of their code development.\n• Ambiguity in Search Intents. The inherent ambiguity in certain search intents poses a challenge.\nCode search systems need to navigate and interpret vague or imprecise queries to provide relevant\nand accurate results.\n• Dataset Quality Issues. Existing datasets used for training and evaluating code search models, such\nas CodeSearchNet [142], often consider method comments as search queries, which fails to align\nSci China Inf Sci\n23\nwell with real-world code search scenarios [160]. While this can lead to strong model performance\nduring evaluation, it may not be effectively transferred into practical use.\n• Efficiency vs. Effectiveness. Learning-based code search has shown promising search accuracy, but\nit often demands substantial computing and storage resources. Although certain approaches [161,\n162] have been proposed to address efficiency concerns in learning-based code search, there is still\nsignificant room for improving the trade-off between efficiency and effectiveness.\n5.4.2\nOpportunities\n• Incorporating Richer Input Information.\nExpanding the scope of input information beyond the\nquery itself presents an opportunity.\nIncorporating details from IDE development contexts, his-\ntorical patterns, and personal preferences can enhance the accuracy and relevance of code search\nresults.\n• Enhanced Code Search with LLMs. Leveraging advancements in LLMs offers an opportunity to\naugment code search capabilities. Focusing on result interpretability, code auto-adaptation, and\nharnessing the advanced natural language and code semantic understanding abilities of LLMs can\nelevate the efficiency of code search systems.\n• Improved Intent Clarification Techniques.\nAdvancing techniques for Intent clarification in code\nsearch represents an opportunity.\nDeveloping techniques to better understand and refine user\nsearch queries contributes to a more streamlined and effective code search experience.\n• High-quality Dataset Construction. This presents an opportunity for the community to enhance\nthe quality of current datasets [160] or create more reliable ones for code search evaluation.\n6\nCode Summarization\nCode summarization, also known as code comment generation, is a process that aims to enhance the\nunderstanding and documentation of source code by automatically generating concise and informative\nsummaries for software artifacts [63,163–165]. It helps address the challenge of comprehending large and\ncomplex code repositories by providing developers with high-level descriptions that capture the code’s\nessential functionality and usage patterns.\nCode summarization has been a hot research topic in software engineering in recent years. Initially,\nresearchers explore template-based methods like SWUM [166] and Stereotypes [167] for generating code\ncomments automatically; Meanwhile, information retrieval-based techniques such as VSM [168] and\nLSI [169] are also applied for code summarization. However, with the rapid advancements in deep neural\nnetworks within machine learning, deep learning-based approaches have gained momentum and become\npredominant in code summarization research. Typically, researchers leverage deep learning models to\ncapture implicit relationships between relevant information within source code and natural language de-\nscriptions. These approaches have significantly contributed to the development of code summarization,\nfacilitating more effective comprehension and documentation of software products.\nDeep learning-based approaches for comment generation primarily mimic the Neural Machine Trans-\nlation (NMT) [170] models in natural language processing. However, compared to translation tasks in\nnatural language, source code typically has a much greater length than comments and contains rich\nstructural information. Most deep learning-based research takes the source code token sequence as the\ninput to the model, while some studies also consider other information sources, such as Abstract Syntax\nTrees (ASTs), API, and so on. We divide these studies into five categories based on different sources of\ninformation: techniques that utilize source code sequences as the model input, techniques that employ\nAbstract Syntax Tree (AST) sequences as the model input, techniques that use tree structures as the\nmodel input, techniques that utilize graph structures as the model input, and techniques that consider\nother sources of information. These techniques generate comments for code snippets (class level, function\nlevel or line level), as well as comments for code commits (i.e., commit messages).\nSci China Inf Sci\n24\n6.1\nUsing source code sequences as model input\nA source code sequence refers to a simple stitching of a code snippet into a sequence with a token as a\nbasic unit. Using source code sequences as model input is simple and convenient and could preserve the\nmost original semantic information of the code.\nIyer et al. [17] proposed CODE-NN, the first deep learning model in code summarization, which\nuses LSTM network structures and attention mechanisms to generate natural language descriptions of\nC# code and SQL code.\nAllamanis et al. [171] applied convolutional neural networks (CNNs) with\nattention mechanisms to an encoder that helps detect long-range topical attention features and local\ntime-invariant features of code sequences. Ahmad etal. [172] first used the Transformer model for source\ncode summarization, innovatively adding an attention layer to the encoder for replicating rare tokens of\nthe source code. Wang et al. [173] proposed Fret, which combines Transformer and BERT to bridge the\ngap between source code and natural language descriptions and alleviate the problem of long dependencies.\nZhang et al. [174] tried to fuse two techniques: deep learning and information retrieval. Specifically, they\nproposed Rencos, which first trains an encoder-decoder model based on a training corpus. Subsequently,\ntwo code segments are selected from the training corpus according to the syntax and semantic similarity.\nFinally, the input code segment and the retrieved two similar code segments are encoded and decoded\nto generate comments. LeClair et al. [175] explored the orthogonality of different code summarization\ntechniques and proposed an integration model to exploit this for better overall performance. Gong et\nal. [176] proposed SCRIPT, which first obtains the structural relative position matrix between tokens\nby parsing the AST of the source code, and then encodes this matrix during the computation of the\nself-attention score after the source code sequence is input into the encoder.\nSome research considers both comment generation and code generation tasks. Chen et al. [177] focused\non both code retrieval and comment generation tasks, and they proposed a framework, BVAE, which\nallows a bidirectional mapping between code and natural language descriptions. The approach attempts to\nconstruct two VAEs (variational autoencoders), where C-VAE mainly models code and L-VAE primarily\nmodels the natural language descriptions in comments.\nThe technique jointly trains these two VAEs\nto learn the semantic vectors of code and natural language representation.\nSimilarly, Wei et al. [95]\nconsidered code summarization and code generation as dyadic tasks, as there is a correlation between\nthe two tasks.\nThey proposed a dual framework to train both tasks simultaneously.\nThey exploited\nthe pairwise nature and the duality between probability and attention weights.\nThen they designed\ncorresponding regularization terms to constrain this duality. Clement et al. [89] focused on the Python\nlanguage and proposed PYMT-5. They also focused on dual tasks: code generation from signatures and\ndocumentation generation from method code.\nSome researchers focused on comment generation for commits. Jiang et al. [178,179] used NMT to\ngenerate concise summaries of commits while designing a filter to ensure that the model is trained only on\nhigher-quality commit messages. Jiang et al. [180] preprocessed code changes into more concise inputs,\nexplicitly using a code semantic analysis approach for the dataset, which can significantly improve the\nperformance of the NMT algorithm.\nLiu et al. [181] used a modified sequence-to-sequence model to\nautomatically generate PR descriptions based on submission information and source code comments\nadded in pull requests (PRs).\nBansal et al. [182] proposed a project-level encoder to generate vector\nrepresentations of selected code snippets in software projects to improve existing code summarization\nmodels. Xie et al. [183] considered method names as refined versions of code summaries. Their approach\nfirst uses the prediction of method names as an auxiliary training task and then feeds the generated\nand manually written method names into the encoder separately. Finally, the outputs are fused into the\ndecoder.\n6.2\nUsing AST sequences as model input\nAn Abstract Syntax Tree (AST) is a hierarchical representation of the syntactic structure of a program\nor code snippet. An AST represents the structure of the code by breaking it down into its constituent\nparts and organizing them in a tree-like format.\nASTs are commonly used in computer science and\nprogramming language theory to analyze and manipulate code. Each node in an AST corresponds to a\nsyntactic element of the code, such as a statement, expression, or declaration.\nHu et al. [184] proposed Deepcom, a technique to preserve the structural information of the code\nintact by parsing the source code into an AST. The authors designed a new traversal strategy, SBT\nSci China Inf Sci\n25\n(Structure-based Traversal), which solves the problem that the source code cannot be effectively restored\nfrom the AST sequence. Subsequently, they proposed Hybrid-DeepCom [185] based on DeepCom, which\nmainly improves DeepCom in three aspects. First, it uses a combination of code information and AST\nsequence information. Second, the OOV problem is mitigated by subdividing the identifier into multiple\nwords based on the camel naming convention. Finally, Hybrid-DeepCom uses beam search to generate\ncode comments. Huang et al. [186] proposed a statement-level AST traversal approach that preserves\nboth code text information and AST structure information, and achieves good results in code snippet-\noriented comment generation tasks.\nTang et al. [187] proposed AST-Trans, which exploits two node\nrelationships in ASTs: ancestor-descendant and sibling relationships. The authors applied the attention\nof a tree structure to assign weights to related nodes dynamically.\nLiu et al. [188] proposed ATOM,\nwhich explicitly incorporates an AST of code changes and utilizes a hybrid sorting module to prioritize\nthe most accurately retrieved and generated messages based on a single code change.\nSome approaches receive both AST and source code sequences as input. Wan et al. [189] combined\nan LSTM that receives code sequences and an LSTM that receives ASTs to extract a hybrid vector\nrepresentation (named Hybrid-DRL) of the target code synthetically.\nIt further uses a reinforcement\nlearning framework (i.e., actor-critic network) to obtain better performance. LeClair et al. [190] proposed\nast-attendgru, which also considers two representations of the code: a word-based text sequence and an\nAST-based tree structure. It processes each data source as a separate input and later merges the vectors\nproduced by the attention layer.\nXu et al. [191] proposed CoDiSum to extract AST structures and\ncode semantics from source code changes and then jointly model these two sources of information to\nlearn the representation of code changes better. Li et al. [192] designed a new semantic parser, SeCNN,\nusing two CNN components that receive source code and AST, respectively, and proposed a new AST\ntraversal technique ISBT to encode structural information more sufficiently. Specifically, they used the\nserial number of the AST via pre-order traversal to replace the brackets in the SBT sequence.\nGao\net al. [193] proposed M2TS, which uses cross-modal fusion further to combine AST features with the\nmissing semantic information and highlight the key features of each module. Zhou et al. [192] proposed\nGSCS, which uses a graphical attention network to process AST sequences and a multi-head attention\nmechanism to learn features of nodes in different representation subspaces.\n6.3\nUsing tree structure as model input\nUnlike the approaches discussed in the previous sub-section, which transforms the AST into a sequence,\napproaches discussed in this sub-section retain the tree structure of the AST directly as input.\nLiang et al. [193] proposed Code-RNN based on tree-LSTM, which is for the case where a node has\nmultiple children, thus overcoming the restriction of converting ASTs into binary trees. For decoding,\ncode-GRU is used. Wanget al. [194] built a tree structure based on code indentation, where the nodes of\nthe tree are statements in the code, and statements with the same indentation are sibling nodes. They\nthen fed this tree structure into a tree-transformer-based encoder.\nLin et al. [195] partitioned the AST\ninto several subtrees according to the control flow graph of the method, and then fed the subtrees into\na tree LSTM for pre-training to obtain their vector representations. They used these representations in\nthe subsequent comment generation task. Similarly, Shi et al. [196] used user-defined rules to split the\nAST tree hierarchically. The model learns the representation of each subtree using a tree-based neural\nmodel, i.e., RvNN. The difference is that RvNN finally combines the representations of all subtrees by\nreconstructing the split AST to capture the structural and semantic information of the whole tree.\n6.4\nUsing graph structure as model input\nA graph is a versatile and powerful data structure that captures complex relationships and intercon-\nnections among entities. Some approaches treat the source tokens as graph vertices and represent the\nrelationships between tokens by edges.\nFernandes et al. [197] added graph information to sequence encoding. Source code is modeled as a graph\nstructure, which helps infer long-distance relationships in weakly structured data (e.g., text). LeClair\net al. [198] used a graph neural network (GNN) based encoder to model the graph form of an AST and\nan RNN-based encoder to model the code sequences. Liu et al. [199] constructed a code property graph\n(CPG) based on an AST while augmenting it with CPGs of ASTs of the retrieved similar code snippets.\nThen, the CPGs are input into a graph neural network for training. Liu et al. [200] proposed a graph\nconvolutional neural network (GCN) based on a hierarchical attention mechanism for encoding graphs\nSci China Inf Sci\n26\nTable 8\nDatasets for code summarization\nDataset\nLiterature\nLanguage\nSize\nTL-CodeSum\n[205]\nJava\n87,136\nDeepcom\n[185]\nJava\n588,108\nFuncom\n[190]\nJava\n2.1 million\nCodeSearchNet\n[157]\nGo, Java, JavaScript, PHP, Python, and Ruby\n2 million\ncode-docstring-corpus\n[214]\nPython\n150,370\nSCGen\n[215]\nJava\n600,243\ncommitMessage\n[179]\nJava\n2,027,734\nparsed from code sequences. The code encoded by the sequence encoder is further combined with the\ndocument text information for decoding. Cheng et al. [201] designed three encoders that receive source\ncode sequences, code structure information, and code context information.\nA bipartite graph is used\nto represent the structure information evolved from the AST, with the addition of a keyword guidance\nmodule.\nGuo et al. [202] proposed CODESCRIBE, which models a code snippet’s hierarchical syntactic structure\n(i.e., AST) by introducing new triadic positions. Then, they used a graph neural network and Transformer\nto preserve the structural and semantic information of the code, respectively. Ma et al. [203] proposed\nMMF3, which uses a graph convolutional network to encode AST graph embeddings while fusing the\nsequence of source code features to determine the matching relationship between each token in the code\nsequence and each leaf node in the AST by comparing the position order. Wang et al. [204] proposed\nGypSum to introduce specific edges associated with the control flow of code snippets into the AST for\nbuilding graphs, and designed two encoders for learning from the graph and source code sequences.\n6.5\nConsidering other sources of information\nOther sources of information include APIs, control flow graphs, unified modeling languages, and byte-\ncode,etc. Hu etal. [205] argued that APIs called within code may provide certain information, and they\nproposed TL-CodeSum, which first trains the mapping relationship between APIs and code comments\nand subsequently migrates the learned knowledge to the code summarization task. Shahbazi et al. [206]\ngenerated comments using API documentation, code snippets, and abstract syntax trees. They showed\nthat API documentation is an external knowledge source, and the performance improvement is negligible.\nGao et al. [207] proposed GT-SimNet, a code semantic modeling approach based on local application pro-\ngramming interface (API) dependency graphs (local ADG). This approach is accomplished by computing\nthe correlation coefficients between dependency graphs and AST nodes.\nZhou et al. [208] proposed ContextCC to obtain ASTs by parsing code to find methods and their\nassociated dependencies (i.e., contextual information) and then generate code comments by combining\nthe filtered contextual information.\nWang et al.\n[209] constructed a type-augmented abstract syntax\ntree (Type-augmented AST) and extracted control flow graphs (CFGs) as an alternative syntax-level\nrepresentation of the code, with a hierarchical attention network to encode this data. Wang et al. [210]\nintroduced class names and associated Unified Modelling Languages (UMLs) for method comment gener-\nation, where the UMLs are fed into the graph neural network as graph forms. Son et al. [211] found that\nProgram Dependency Graphs (PDGs) can represent the structure of code snippets more effectively than\nASTs, proposed an enhancement module (PBM) that encodes PDGs as graph embeddings, and designed\na framework for implementing PBMs with existing models.\nZhang et al. [212] proposed Re Trans to\nenhance structural information by adding data flow and control flow edges to the AST and using GCN\nto encode the entire AST.\nHuanget al. [213] explored the feasibility of using bytecode as a source of information to generate code\ncomments. They used pre-order traversal to serialize the bytecode control flow graph, and combined it\nwith a bytecode token sequence as model input to achieve automatic code summarization in a scenario\nwithout available source code.\n6.6\nDatasets\nThe following datasets in Table 8 are commonly used for automatic code summarization.\nSci China Inf Sci\n27\nTL-CodeSum [205] comprises 69,708 method-comment pairs obtained by crawling Java projects devel-\noped between 2015 and 2016, each having a minimum of 20 stars on GitHub. The average lengths of\nJava methods, API sequences, and comments are 99.94, 4.39, and 8.86, respectively.\nDeepcom [185] is collected from GitHub’s Java repositories created from 2015 to 2016 considering only\nthose having more than 10 stars to filter out low quality repositories.\nIt uses the first sentences of the\nJavadoc as the target comments and excludes the setter, getter, constructor and test methods. After the\npreprocessing, there are 588,108 method-comment pairs in total.\nFuncom [190] constitutes a compilation of 2.1 million method-summary pairs derived from the Sourcerer\nrepository. After the removal of auto-generated code and exact duplicates, the dataset is partitioned into\ntraining, validation, and test sets by project.\nCodeSearchNet [157] is a large well-formatted dataset collected from open source libraries hosted on\nGitHub. It contains 2 milllion code-summary pairs and about another 4 million functions without an\nassociated documentation, spanning six programming languages (i.e., Go, Java, JavaScript, PHP, Python,\nand Ruby).\ncode-docstring-corpus [214] is a dataset downloaded from repositories on GitHub, retaining Python 2.7\ncode. The dataset contains 150,370 code-comment pairs. The vocabulary size of code and comment is\n50,400 and 31,350, respectively.\nSCGen [215] is a dataset of Java code snippets constructed from 959 Java projects of GitHub. Data\ncleaning is performed to filter out invalid data according to templates, such as comments in setter and\ngetter methods or comments generated by the template predefined in the IDE comment plugin. Using a\ncomment scope detection approach, 600,243 code snippet-comment pairs are collected.\ncommitMessage [179] contains 967 commits from the exsiting work and all the commits from the top\n1,000 popular Java projects in Github. The rollback commits, merge commits and the commits with\nmessages that are empty or have non-English letters are filtered. In the end, there are 2,027,734 commits\nin the dataset.\n6.7\nChallenges and Opportunities\n6.7.1\nChallenges\n• High-quality Dataset.\nThe code summarization techniques based on deep learning need a high-\nquality dataset to improve their performance. Although several datasets have been published in\nthis area, the data is selected from the perspective of the project popularity. As a result, there may\nbe duplicate data and machine-generated comments in the dataset. Developing practical techniques\nto identify high-quality comments that really reflect the code intent is challenging.\n• Evaluation Metrics. Many studies employ the BLEU metric to evaluate the performance of the code\nsummarization models. However, there are many variations of BLEU, which results in different ways\nof calculation, such as BLEU-L and BLEU-C. On one hand, due to the different emphasis of each\nvariation of BLEU, the performance of the same model shows significant differences under different\nBLEU metrics. On the other hand, it is possible that the BLEU score does not accurately reflect\nthe actual effect of the generated comment because the BLEU score is based on the repetition of\ntokes in two sentences. Two sentences with the same semantics but different words have a low\nBLEU score, which is unreasonable.\n• Adaptation Ability : Code summarization needs to be adaptable to various programming languages,\neach with its own syntax and semantics. Developing a universal summarization model that performs\nwell across diverse languages is a significant challenge.\n6.7.2\nOpportunities\nAt present, most code summarization models cannot be directly applied to the production practice, and\nare still in the experimental prototype. It comes down to the fact that the model is not powerful enough\nto apply. There is still a lot of room for improvement.\n• Utilizing more implementation information: When the information at the source code level (e.g.,\nASTs, tokens, APIs, CFGs) is almost mined, a feasible way maybe to mine more useful information\nfrom outside the source code (e.g., bytecode, API documents, design documents) to characterize\nSci China Inf Sci\n28\nthe internal patterns of the code, and further improve the performance of code comment generation\nmodels.\n• Considering richer information in the comments: Most code summarization datasets take the first\nsentences of comment or commit message as the code summary because the first sentences are\nconsidered to describe the functionalities of Java methods according to Javadoc guidance. With\nthe development of deep learning models, other information in the summary (e.g., the intent or\nrationale of the code) may be extracted and used for summary generation.\n7\nSoftware Refactoring\nSoftware refactoring is to improve software quality by changing its internal structures whereas its external\nbehaviors are kept intact [216]. Ever since Opdyke proposed the concept of software refactoring in 1992,\nresearchers have tried to automate software refactorings aiming to reduce the cost of software refactoring\nand to improve the safety of refactorings.\nThanks to such hard work, automatic or semi-automatic\nsoftware refactoring has been provided as a default feature in all mainstream IDEs, such as Eclipse,\nIntelliJ IDEA, and Visual Studio, thus significantly increasing the popularity of software refactorings.\nVarious techniques have been exploited for software refactorings [217–220] whereas recently deep learn-\ning techniques have become the main force in this field [221,222].\nTraditional software refactoring\nheavily depends on static code analysis, code metrics, and expert-defined heuristics to identify code\nsmells [217,218](i.e., what should be refactored) and to recommend refactoring opportunities. However,\nit is challenging to formalize complex refactorings with human-defined simple heuristics, making tra-\nditional heuristics-based refactoring less accurate. In contrast, deep learning techniques with complex\nnetworks and numerous weights, have the potential to learn complex refactorings [219]. Consequently,\nvarious deep learning techniques have been recently employed for software refactoring.\nHowever, applying deep learning techniques to software refactorings is nontrivial, encountering a se-\nquence of challenges. The first challenge is to collect a large number of high-quality items requested for\ntraining. Since deep models often contain a large number of parameters, they usually request a large\nnumber of labeled items as training data.\nHowever, we lack such large-scale high-quality datasets in\nthe field of software refactoring. The second challenge is to figure out how deep models (deep learning\ntechniques) could be adapted for different categories of software refactorings. There are various deep\nlearning techniques (e.g., CNN, LSTM, and GNN), which are originally designed for tasks (e.g., natural\nlanguage processing or image processing) other than software refactoring. Consequently, such techniques\nshould be substantially adapted for this specific task.\n7.1\nDetection of Code Smells\nCode smell detection is often taken as the first step in software refactoring because code smells often\nindicate the problems of source code as well as their solutions (i.e., refactorings). Traditional approaches\nusually distinguish software entities associated with code smells from smell-free code by code metrics,\ntaking the task of code smell detection as a binary classification problem. Since deep learning techniques\nhave proved effective in classification tasks [223,224], it is reasonable to investigate deep learning-based\ndetection of code smells.\nTo the best of our knowledge, the automated approach to detecting feature-envy smells proposed by\nLiu et al. [219] is the first attempt at deep learning-based code smell detection. They exploited traditional\ncode metrics, e.g., coupling between code entities, by a CNN, and exploited the identifiers of code entities\n(i.e., names of the to-be-tested method and names of its enclosing class as well as its potential target class)\nby another CNN. The outputs of the two CNNs are fed into a dense layer (Fully-Connected Network,\nFCN) and its output generates how likely the method should be moved from its enclosing class to the\ngiven target class. Their evaluation results suggest that trained with automatically generated data, the\ndeep learning-based approach is more accurate than traditional approaches that do not leverage deep\nlearning techniques.\nBased on the success, they [222] expanded the deep learning-based detection to\nadditional categories of code smells (i.e., long methods, large classes, and misplaced classes), and their\nevaluation results suggest that deep learning techniques have the potential to improve the state of the\nart in code smell detection.\nSci China Inf Sci\n29\nBarbez et al. [225] applied CNN to capture the evolution of God classes. For a class to be tested,\nthe proposed approach (called CAME) extracts the latest n versions of this class, and for each version,\nit extracts the selected code metrics (e.g., the complexity of the class).\nAs a result, it expresses the\nevolution of the class as a matrix Xn,m where n is the number of versions and m is the number of\ninvolved code metrics. This matrix is fed into a CNN whose output is forwarded to a MLP (Multilayer\nPerceptron). The MLP will make the final prediction, i.e., whether the class under test is a God class.\nNotably, although this approach depends on deep neural networks, it leverages only 71 real-world God\nclasses for training and testing.\nYu et al. [226] employed a Graph Neural Network (GNN) to detect feature-envy smells. They rep-\nresented methods as nodes in the graph and the calling relationships among methods as edges. They\nleveraged a GNN technique to extract features and vectorize the nodes, and finally classified the nodes\n(methods) as smelly methods or smell-free ones. Their evaluation results suggest that their GNN-based\napproach is more accurate than the CNN and FCN-based approach proposed by Liu et al. [222] for\ndetecting feature-envy smells.\nZarina et al. [220] proposed a hybrid approach to identifying feature-envy smells by leveraging both\ndeep learning techniques and traditional machine learning techniques (i.e., SVM). The approach first\nrepresents methods and classes as vectors with Code2Vec [227]. Code2Vec parses a method into an\nAST, and represents each path between two AST leaves as a vector.\nBased on the resulting vectors\nthat represent patches within the AST, Code2Vec represents the whole method as a vector. A class is\npresented as a vector that equals the average of the vectors of methods within this class. The vector\nof the to-be-tested method and the vector of its potential target class are fed into SVM-based binary\nclassifier to predict whether the method should be moved to the given target class. Note that, the deep\nlearning model employed by this approach (i.e., Code2Vec) is unsupervised. Consequently, it does not\nrequest a large number of labeled training data, which is a significant advance of the hybrid approach.\nSimilarly, Di et al. [228] also leveraged Code2Vec (or Code2Seq) to turn a method (i.e., AST) into a vector,\nand employed graph embedding techniques to represent its dependency with other methods. With the\nresulting embeddings, they also leveraged a traditional machine learning technique (Naive Bayes) to make\npredictions.\nCode smell detection, if taken as a binary classification, often encounters serious class imbalances\nbecause software entities associated with code smells (noted as positive items) are often significantly fewer\nthan smell-free entities (noted as negative items). Fuzzy sampling, proposed by Yedida and Menzies [229],\nis a novel technique to handle class imbalance. It adds points concentrically outwards from points (items)\nof the less popular class. The oversampling thus may push the decision boundary away from these points\nif the newly added points belong to the same class. As a result, the classifier trained with additional items\nmay learn better to identify similar items belonging to the less popular class. Yedida and Menzies [230]\nvalidated whether this novel technique can boost deep learning-based code smell detection. Their results\ndemonstrate that fuzzy sampling boosts the deep learning-based approaches (proposed by Liu et al.\n[222]) to detect feature envy, long methods, large class, and misplaced classes by fuzzy sampling. That is\nto say, the results suggest that fuzzy sampling does improve the state of the art in deep learning-based\ncode smell detection.\n7.2\nRecommendation of Refactoring Opportunities\nAlthough we may suggest where and which refactorings should be applied by automated detection of code\nsmells as introduced in the preceding section, we may also need to recommend refactoring opportunities\n(and even detailed refactoring solutions) directly without code smell detection.\nFor example, Liu et\nal. [231] proposed an automated approach to recommending renaming opportunities based on renaming\nrefactorings that developers recently conducted. In this approach, they do not detect any specific code\nsmells but recommend refactorings similar to what has been conducted recently. The approach proposed\nby Liang et al. [232] is similar to the approach by Liu et al. [231] in that both approaches recommend\nrenaming opportunities according to the evolution history of the source code. The key difference is that\nLiang et al. [232] employed deep learning techniques whereas Liu et al. [231] depended on heuristics\nand static source code analysis. Notably, Liang et al.’s approach recommends renaming opportunities\non methods only.\nFor a given method, it leverages BERT [233] and textCNN [234] to vectorize the\nmethod. After that, it employs an MLP classifier to predict whether the method should be renamed.\nThe predicted method is renamed only if at least one of its closely related entities has been renamed\nSci China Inf Sci\n30\nrecently.\nLiu et al. [235] employed unsupervised deep learning techniques to identify and recommend\nrenaming opportunities.\nFirst, for each method in the given corpus, it vectorizes method names and\nmethod bodies by CNN and Paragraph Vector [236], respectively. For a given method whose method\nname is mn, and whose method body is md, it retrieves the top k most similar method names from\nthe corpus and top k method names whose corresponding method bodies are the most similar to md. If\nthe two sets of method names are highly similar, method name mn is consistent with the corresponding\nmethod body md. Otherwise, they are inconsistent, and thus it selects a method name from the latter\nset with a set of heuristics and recommends replacing mn with it.\nSince useful refactorings should be frequently applied by various developers on various software ap-\nplications, it is likely that we may infer such refactorings from the rich evolution histories of software\napplications without knowing exactly what the refactorings are in advance. Consequently, by applying\nadvanced learning or mining techniques to evolution histories, we may learn (discover) some less-known\nrefactorings, and can even learn where and how such refactorings could be applied. For example, Tufano\net al. [237] proposed a deep learning-based technique to learn from pull requests and to infer how code is\nchanged. Among the most frequent changes learned by this approach, refactorings are dominating. After\ntraining the deep neural model with various pull requests, the technique applies the resulting model to\npredict expected changes on a given application. Most of the predicted changes are refactorings, and thus\nthe prediction could be viewed as an automated recommendation of refactorings.\nNyamawe et al. [238] suggested that refactoring activities, as well as other software development ac-\ntivities, should be traced to software requirements as well as their changes. Based on this assumption,\nthey proposed a novel technique to recommend refactorings based on feature requests. It first associates\nfeature requests with code smells and refactorings by mining software evolution histories.\nFor a new\nfeature request, it employs various machine learning-based models to predict the required refactorings\nbased on the feature request as well as code smells associated with related source code.\nAlOmar etal. [239] proposed the first just-in-time recommendation approach for extract-method refac-\ntorings based on copy-and-paste actions. When developers copy and paste a piece of source code, the\napproach determines whether the copied fragment of source code should be extracted as a new method.\nIt leverages a large number of code metrics and uses a CNN to classify code fragments based on the code\nmetrics.\nChi et al. [240] proposed a novel and more reliable approach called ValExtractor to conduct extract-\nlocal-variable refactorings. The primary challenge in automating extract-local-variables refactorings is\nthe efficient identification of side effects and potential exceptions between extracted expressions and their\ncontexts without resorting to time-consuming dynamic program execution. ValExtractor addresses this\nchallenge by utilizing lightweight static source code analysis to validate the side effects of the selected\nexpressions. It also identifies occurrences of the selected expression that can be extracted together without\nintroducing program semantics or potential exceptions.\nBesides the generic approaches that could be applied to various software applications, deep learning-\nbased refactoring recommendation has also been proposed for some special domains, e.g., microservices.\nDesai et al. [241] proposed a deep learning-based approach to recommending refactoring opportunities,\ni.e., extracting some classes from a monolith application as micro-services.\nThe approach represents\nclasses as nodes and invocation among them as edges. It also identifies entry points (i.e., APIs of web\napplications) and represents such information as attributes of the classes. It then employs a graph neural\nnetwork to cluster the nodes (i.e., classes), aiming to minimize the effect of outlier nodes. The resulting\noutlier nodes are finally recommended to be extracted as micro-services.\nWe conclude that both supervised and unsupervised deep learning techniques have been applied to rec-\nommending generic and domain-specific refactoring opportunities. However, we also notice that existing\napproaches support only a limited number of refactoring categories, and thus it is potentially fruitful to\nrecommend more categories of refactoring opportunities by deep learning techniques in the future. We\nalso notice that some latest advances in deep learning techniques, like large language models (e.g., GPT)\nhave not yet been fully exploited in automated software refactoring approaches.\n7.3\nDatasets\nLacking of large-scale and high-quality training datasets is one of the biggest obstacles to deep learning-\nbased software refactoring.\nNotably, most existing datasets for software refactorings are built manu-\nally [242] or built by mining refactoring histories [243]. For example, to validate the automated move-\nSci China Inf Sci\n31\nmethod refactorings, JMove [218] requested developers to manually check the suggested refactoring op-\nportunities, and thus such manually confirmed items could serve as training or testing dataset for future\nresearch in this line. However, such a manually constructed dataset is often too small for sufficient training\nof deep neural networks. It is also challenging to enlarge such datasets because the manual identification\nof refactoring opportunities is time-consuming and error-prone. Another way to construct datasets of soft-\nware refactorings is to leverage automated refactoring miners to discover actually conducted refactorings\nrecorded in open-accessed version control systems. A few approaches, e.g., RefactoringMiner [244], RefD-\niff [245], and Ref-Finder [246], have been proposed for such purpose. Although such mining tools could\nidentify a large number of refactorings from real-world applications, they often result in false positives,\nmaking the resulting dataset unsuitable for model training.\nTo this end, Liu et al. [219] proposed a novel approach to generating large-scale training data for\nthe training of deep learning-based refactoring models.\nIn the paper, the authors focused on a single\ncategory of code smells (i.e., feature envy) and its corresponding refactoring (i.e., move-method refactor-\ning). To create positive items (i.e., methods associated with feature envy smells), they randomly moved\nmethods across classes (with precondition checking of Eclipse move method refactoring), and took the\nmoved methods as positive items because they had better be moved back to the original place (i.e., their\nenclosing classes before the movement). Methods that could be moved (i.e., satisfying the precondition of\nmove method refactorings) but have not been moved could be taken as negative items, i.e., methods not\nassociated with feature envy smells. By applying this novel approach to high-quality open-source appli-\ncations, they generated huge data sets of refactorings (code smells) where the ratio of positive (negative)\nitems could be accurately controlled as well. Based on the generated data, they trained a CNN-based\ndeep neural model to detect feature envy smells and to suggest solutions (i.e., where the associated\nmethods should be moved). Their evaluation results suggest that the resulting model significantly out-\nperforms the state-of-the-art approaches. Later, Liu et al. [222] successively expanded this approach to\nmore categories of code smells, i.e., long methods, large classes, and misplaced classes. Long methods\nare created by automated inline refactorings that merge multiple methods into a single one, large classes\nare created by merging multiple classes whereas misplaced classes are created by moving classes across\npackages. Their evaluation results suggest that employing such automatically generated large datasets\nto train deep neural networks could significantly improve the state of the art in code smell detection and\nautomated recommendation of refactoring opportunities [222]. Currently, this automated data generation\nhas been employed by almost all deep learning-based refactoring approaches that request labeled training\nitems [230,247].\nAlthough the quantity of automatically generated training items is satisfying, their quality may still\nbe questionable. Because the code smells (i.e., positive items) are automatically generated, they could be\nsignificantly different from code smells introduced unconsciously by developers. As a result, deep neural\nmodels trained with such generated artificial data may learn only how to identify artificial smells instead\nof real-world code smells. To this end, Liu et al. [243] aimed to improve the precision of refactoring miners,\nand thus their discovered real-world code smells and refactorings could be taken directly as high-quality\ntraining data. The key to their approach is to leverage a sequence of heuristics and machine learning-based\nclassifiers to exclude false positives. Notably, they employed a traditional machine learning technique\n(i.e., decision trees) instead of deep learning techniques to exclude false positives. The major reason for\nthe selection is that traditional machine learning techniques may work well with small (but high-quality)\ntraining data whereas deep learning ones often request much larger dataset that they were unable to\nprovide. Their evaluation results suggest that by filtering out false positives with their approach, the\nprecision of the employed refactoring miner (RefactoringMiner [244]) is able to reach a high level compa-\nrable to human experts in discovering move-method refactorings. Compared to the artificial feature-envy\nmethods automatically generated by previous approach [219], such real-world feature-envy methods dis-\ncovered by the proposed approach could significantly improve the performance of deep learning-based\nmodel in detecting feature-envy smells and in recommending move-method opportunities.\nMost of the current refactoring detection approaches often result in non-negligible false positives and\nfalse negatives. To solve this problem, Liu et al. [248] proposed a novel refactoring detection approach\n(called ReExtractor).\nThe rationale of ReExtractor is that an entity matching algorithm takes full\nadvantage of the qualified names, the implementations, and the references of software entities. Compared\nagainst the state of the art, it improves the accuracy of entity matching between two successive versions\nand thus substantially reduces false positives and false negatives in refactoring detecting.\nBased on the preceding analysis, we conclude that large-scale and high-quality training data are critical\nSci China Inf Sci\n32\nfor deep learning-based refactoring, and data collection remains an open question that deserves further\ninvestigation.\n7.4\nChallenges and Opportunities\nBased on the preceding analysis of deep learning-based software refactoring, we present here a list of\npotential challenges and opportunities for future research in deep learning-based software refactorings.\n7.4.1\nChallenges\n• Large-scale high-quality dataset.\nIt remains challenging to collect large-scale and high-quality\nrefactoring data to train deep learning models. Although generating refactorings to be automatically\nreversed as suggested by Liu et al. [219] should result in large-scale refactoring data, the quality\nand representativeness are in question. In contrast, discovering refactoring histories in open-source\napplications may result in high-quality real-world refactoring data, such data are often small and\nlack diversity.\n• Generalization across different paradigms. Most of the code bases are written in various program-\nming languages, each with its own syntax and semantics. Developing deep learning models that\ngeneralize well across different languages is a considerable challenge. Currently, most of the studies\nin deep learning based refactoring use applications written in Java. Thus, there is little or slow\nadoption in other programming languages. Another challenge concerning the generalization is to\nmake the approaches applicable to all categories of code smells. The identification of code smells is\nvery crucial in the process of software refactoring. It has been proven challenging to have a general\ndeep learning model to detect code smells for refactoring as different models behave differently for\nspecific smells.\n• Generic classification and feature engineering.\nIt is very challenging to design general classifier\nwhich may be used for the process of software engineering as different features may be needed for\ndifferent refactoring processes.\n• Complexity of Code Patterns. Code bases often contain complex patterns and structures. Captur-\ning and representing these patterns effectively for training deep learning models can be difficult,\nespecially when dealing with large and diverse code bases.\n• Context Sensitivity.\nRefactoring decisions are often context-dependent, considering the broader\nsystem architecture, design patterns, and usage scenarios. Deep learning models might struggle to\ncapture and understand such context-sensitive information.\n• Interpretability. Deep learning models are known for their “black-box” nature, making it challenging\nto understand the rationale behind their refactoring recommendations. This lack of interpretability\ncan be a significant hurdle for developers who need to trust and adopt these suggestions.\n7.4.2\nOpportunities\n• Large language model-based refactoring.\nLarge language models have great potential in smell\ndetection and refactoring suggestions.\nUp to date, various deep learning techniques have been\nproposed to detect code smells, and/or to suggest solutions to identified code smells. However, to\nthe best of my knowledge, large language models have not yet been applied to such tasks. Since large\nlanguage models are good at understanding and generating source code, it is likely that they can\nbe employed to format source code and to identify abnormal parts of source code (i.e., code smells).\nThey may also be employed to discover (and measure) the relationship (like coupling, similarity,\ncohesion, and dependency) among software entities. Such a relationship, currently measured by\nstatistic code metrics only, is critical for the detection of code smells as well as suggestion of\nrefactoring solutions.\n• Automated discovery of new refactorings.\nUp to date, the academic community focus on well-\nknown code smells and refactorings. All such smells and refactorings are coined by human experts.\nHowever, with the development in programming languages and new paradigms, new categories of\ncode smells as well as new categories of refactorings are emerging. One possible way to discover\nSci China Inf Sci\n33\nsuch new smells and new refactorings is to mine the evolution history of open-source applications.\nWith advanced deep learning techniques, it is potentially feasible.\n8\nCode Clone Detection\nCode clones are code fragments that have the same or similar syntax and semantics. The wide presence\nof code clones in open-source and industrial software systems makes clone detection fundamental in many\nsoftware engineering tasks, e.g., software refactoring, evolution analysis, quality management, defect pre-\ndiction, bug/vulnerability detection, code recommendation, plagiarism detection, copyright protection,\nand program comprehension.\nThe recent research in code clones has attracted the use deep learning techniques. As shown in Figure\n4, most of the research focuses on source code clone detection and source code representation for clone\ndetection. A little of research focuses on the binary clone detection, cross language clone detection, clone\nevaluation and validation.\n4\n9\n5\n32\n3\nFigure 4\nCode clone research tasks where deep learning has been applied.\n8.1\nSource code clone detection\nSource code clones are typically classified as follows based on their degree of similarity.\n1. Type1: Duplicate code fragments, except for differences in white space, comments, and layout.\n2. Type2:\nSyntactically identical code fragments, except for differences in variable names, literal\nvalues, white space, formatting, and comments.\n3. Type3: Syntactically similar code fragments with statements added, modified, or deleted.\n4. Type4: Syntactically different code fragments implementing the same functionality.\nAs the boundary between Type3 and Type4 clones is often ambiguous, in benchmarks like Big-\nCloneBench [249] researchers further divide these two clone types into three categories: strongly Type3,\nmoderately Type3, and Weakly Type3/Type4. Each type is harder to detect than the former one.\nWeak\nType3 and Type4 clones are usually called semantic clones.\nThe research efforts on source code clone detection are shown in Table 9. As can be seen from the table,\nmost of recent research tries to leverage deep neural networks to effectively capture complex semantic\ninformation in code fragments, so as to improve the effectiveness of semantic clone detection.\nThe code clone detection process begins by modeling the semantic of the source code. To achieve\nthis goal, diverse program representations such as tokens, Abstract Syntax Trees (ASTs), Control Flow\nGraph (CFGs), Data Flow Graphs (DFGs), Program Dependency Graphs (PDGs) are being used to learn\nprogram features.\nVarious deep learning models, such as CodeBERT, GraphCodeBERT, Graph Neural Network (GNN),\nGraph Attention Network, Convolutional Neural Network (CNN), Graph Convolutional Network (GCN),\nSource Code Clone Detection\nBinary Code Clone Detection\nCross-Language Clone Detection\nSource code representation for clone detection etc.\nClone Evaluation and Validation\nSci China Inf Sci\n34\nASTNN, Tree-Based Convolutional Neural Network (TBCNN), Recursive Autoencoders (RAE), Recur-\nrent Neural Network (RNN), Recursive Neural Network(RvNN), LSTM, have been used.\nThese studies have achieved higher recall and better precision than the best classical approaches.\nTable 9\nSource Code Clone Detection\nYear\nVenue\nLiterature\nLanguage\nCode Representation\nDeep Learning Models\nClone Type\nBenchmark\nBaseline\n2022\nICSME\nSSCD [250]\nJava, C/C++\ntoken\nCodeBERT,GraphBERT\nType 3 and Type 4\nBigCloneBench,CompanyC, CompanyC++\nSourcererCC [251]\n2022\nIWSC\n[252]\nJava\ntoken\nCodeBERT\nAll\nBigCloneBench,SemanticCloneBench etc.\nNA\n2022\nTSE\nHolmes [253]\nJava\nPDG\nGNN,LSTM\nALL\nGoogleCodeJam, SeSaMe,BigCloneBench\nTBCCD [254]\n2022\nICSR\nSEED [255]\nJava,C ,C++\nSemantic graph\nGMN\nType 4\nOJClone, BigCloneBench\nTBCCD [254],ASTNN [256]\n2022\nADES\nCCCD-DL [257]\nJava\nAST,CFG,FCG\nDNN\nType 4\nBigCloneBench,opensource projects\nLV-CCD [25 8], FCCA [259], and TBCNN [260]\n2021\nQRS\nCSEM [261]\nC\nEvent Embedding Tree\nGAT\nType3,Type4\nOJClone\nCCLearner\n[262],Deckard [263], CloneWork [264], SourcerCC [251]\n2021\nAPSEC\n[265]\nJava,C\nAST\nBiC-CNN\nType3 and Type4\nBigCloneBench, OJClone\nDeckard [263], DLC [266],CDLH [267],Deepsim [268], ASTNN [256] etc.\n2021\nApplied Sciences\n[260]\nJava\nAST\nTBCNN\nAll\nBigCloneBench\nDeckard [263],RtvNN [266], CDLH [267], DeepSim [268]\n2020\nIEICE T INF SYST\n[269]\nJava\nAST\nCNN,Siamese\nALL\nBigCloneBench\nNiCad [270]\n2020\nASE\nSCDetector [271]\nJava,C / C++\nCFG\nGRU,Siamese\nAll\nGCJ ,BigCloneBench\nRtvNN [266],ASTNN [256], Deckard [263], SourcererCC [251]\n2020\nAPSEC\nSia-RAE [272]\nJava\nAST\nrecursive autoencoders (RAE) ,Siamese\nAll\nBigCloneBench\nDeepSim [268], CDLH [267],weighted RAE [273]\n2020\nDSA\n[274]\nJava\nCFG\nBi-RNN,GCN\nType4\nopensource projects\nNicad [270]\n2020\nT-R\nFCCA [259]\nJava\ntoken,AST,CFG\nRNN\nAll\nBigCloneBench\nDeckard [263], CDLH [267], DeepSim [268],DLC [266], SourcerCC [251], TBCCDcitecd-Yu2019\n2020\nSANER\nFA- AST -G MN [2 75 ]\nJava,C/C++\nFA-AST\nGNN\nAll\nGCJ,BigCloneBench\nDeckard\n[263], RtvNN [266],CDLH [267], ASTNN [256]\n2020\nISSTA\n[276]\nC++\nAST,CFG\nDNN\nType4\nOJClone\nDeckard [263],DLC [266],CDLH [267], ASTNN [256], DeepSim [268]\n2020\nIEEE Access\n[277]\nJava\nAST\nCNN\nType1,Type2\nBigCloneBench\nSourcererCC [251],NiCad [270],Deckard [263], CClearner [262], Oreo [278]\n2020\nComplexity\n[279]\nJava,C/C++\nAST\nBiLSTM\nAll\nOJClone,BigCloneBench\nRAEcd-White2016,CDLH [267], ASTNN [256]\n2019\nIEEE Access\nweighted RAE [273]\nJava\nAST\nRAE\nAll\nBigCloneBench\nOreo [278], DeepSim [268], CCLearner [262],CDLH [267],Nicad [270]\nDeckard [263] SourcerCC [251], CloneWorks [264]\n2019\nAAAI\nACD [280]\nJava,C\nAST\nLSTM\nAll\nOJClone,BigCloneBench\nDeckard [263] SourcerCC [251],CDLH [267]\n2019\nSANER\n[281]\nJava\nAST\nRvNN,Siamese Network\nType4\nBigCloneBench\nDeckard [263]\n2019\nICPC\nTBCCD [254]\nJava,C\nAST,token\nPACE\nAll\nOJClone,BigCloneBench\nCDLH [267],Deckard [263],SourcerCC [251], DLC [266]\n2019\nISSTA\nGo-Clone [282]\nGolang\nLSFG\nDNN\nNA\nfrom Github\nNA\n2019\nTI I\n[283]\nC\nAST\nGCN\nNA\nopen source projects\nVUD D Y [2 8 4 ] , LS T M\n2018\nESEC/FSE\nOreo [278]\nJava\nmetrics,token,\nDNN with Siamese architecture\nAll\nopensource projects\nNicad\n[270], Deckard [263], SourcerCC [251], CloneWorks [264]\n2018\nESEC/FSE\nDeepSim [268]\nJava\nSemantic features matrix\nFeed-forward nerual network\nType3,Type4\nGCJ , BigCloneBench\nCDLH [267], Deckard [263], RtvNN [266] etc.\n2018\nICML A\nCCDLC [285]\nJava\nBDG,PDG,AST\nCNN\nNA\nopen source projects\n2018\nIJCAI\nCDOU [2 86]\nJava,C\nAST\nLSTM.word2vec\nAll\nOJClone,BigCloneBench\nCDLH [267],Deckard [263] ,SourcerCC [251]\n,RtvNN [266] etc.\n2017\nIJCAI\nCDLH [267]\nJava,C\nAST\nLSTM\nAll\nOJClone,BigCloneBench\nDeckard [263], SourcerCC [251] ,RtvNN [266] etc.\n2017\nICSME\nCCLearner [262]\nJava\ntoken\nDNN\nType1,Type2,Type3\nBigCloneBench\nDeckard [263], SourcerCC [251], Nicad [270]\n2016\nASE\nRtvNN [266]\nJava\nAST\nRvNN\nAll\nopen source projects\nDeckard [263]\n2016\nICML A\n[287]\nJava\nmetrics\nMLP\nAll\nBigCloneBench\nDeckard [263], SourcerCC [251],Nicad [270] ,CCFiner,IClone\n8.2\nCode representation learning for clone detection\nThe studies in Table 10 focus on learning source code representation, so as to automatically capture\nthe syntactic and semantic information from source code. Then the embedding is applied to code clone\ndetection, code classification tasks etc. Code similarities can be learned from diverse representations of\nthe code, such as identifiers, tokens, ASTs, CFGs, DFGs, and bytecode.\nSiowetal. [288] performed an empirical study on code representation. They found that the graph-based\nrepresentation is superior to the other selected techniques across these tasks. Different tasks require task-\nspecific semantics to achieve their highest performance; however, combining various program semantics\nfrom different dimensions such as control dependency, data dependency can still produce promising\nresults. Tufano et al. [289] demonstrated that combined models relying on multiple representations can\nbe effective in code clone detection and classification.\nTable 10\nSource Code Representation Learning for Clone Detection\nYear\nVenue\nLiterature\nLanguage\nCode Representation\nDeep Learning Models\nClone Type\nBechmark\nBaseline\n2022\nICSE\n[290]\nJava,C/C++\ntoken\nCodeBERT,GraphCodeBERT\nAll\nOJClone,BigCloneBench\nCDLH [267],FA-AST-GMN [275],TBCCDcitecd-Yu2019 ect.\n2022\nSANER\n[288]\nC\nfeatures,token,AST,CPG\nBiLSTM,LSTM,Transformer,Tree-LSTM,Code2Vec,GAT,GCN,GGNN\nNA\nOJClone\nNA\n2022\nEMSE\n[291]\nJava\nAST\ncode2vec\nNA\nopensource projects\nNA\n2021\nICSE\nInferCode [292]\nJava,C/C++\nAST\nTBCNN\nAll\nOJClone,BigCloneBench\ncode2vec,code2seq,Deckard [263],SourcererCC [251],RtvNN [266]\n2021\nICONIP\nHGCR [293]\nC,Java\nSG, EDFG\nT-GCN,E-GAT\nAll\nGCJ,BigCloneBench,\nASTNN [256],FA-AST ,FCCA\n2019\nICSE\nASTNN [256]\nJava,C\nAST\nASTNN\nAll\nOJClone,BigCloneBench\nTBCNN [294] ,CDLH [267] ,RAE [266],etc .\n2019\nICCF\nTBCAA [295]\nJava,C\nAST\nTree-based Convolution\nAll\nOJClone,BigCloneBench\nCDLH [267],Deckard [263] SourcerCC [251],DLC [266]\n2019\nICSME\nTECCD [296]\nJava\nAST\nSentence2Vec\nType3\nBigCloneBench,opensource projects\nCCLearner [262],Nicad [270],CCAligner [297]\n2018\nICSE\n[289]\nJava\nidentifier,ASTs, CFGs, and Bytecode\nRNN\nAll\nQualitas.class Corpus [298]\nNA\n8.3\nCross-language code clone detection\nThe above work on source code clone detection focuses on clones in a single programming language.\nHowever, software systems are increasingly developed on a multi-language platform on which similar\nfunctionalities are implemented across different programming languages [154,299].\nThe main challenge of cross-language code clone detection is how to reduce the feature gap between\ndifferent programming languages.\nThe studies on cross-language code clone detection are shown in Table 11. These studies focus on\nextracting syntactic and semantic features of different programming languages.\nNafi et al. [300] used\na Siamese architecture to learn the metric features.\nPerez et al. [154] used an unsupervised learning\napproach for learning token-level vector representations and an LSTM-based neural network to predict\nclones.\nBui et al. [301] proposed a Bi-NN framework to learn the semantic features of two different\nSci China Inf Sci\n35\nprogramming languages.\nWang et al. [302] proposed a Unified Abstract Syntax Tree neural network.\nYahya et al. [299] used AST embeddings from InferCode [292] as input of the Siamese architecture.\nTable 11\nCross-Language Code Clone Detection\nYear\nVenue\nLiterature\nLanguage\nCode Representation\nDeep Learning Models\nBechmark\nBaseline\n2023\nComputers\nCLCD-I [299]\nJava,Python\nAST\nSiamese architecture [303]\ncoe from programming competition\nLSTM\n2022\nICPC\nUAST [302]\nC,C++,Java,Python, JavaScript\ntoken\nBi-LSTM,GCN\nJC, dataset collected from leecode\nInferCode [292]\n2019\nSANER\n[301]\nJava,C++\nAST\nBi-NN\nOJClone,opensource projects\nTBCNN [294] etc.\n2019\nMSR\n[154]\nJava,Python\ntoken,AST\ntree-based skip-gram,LSTM\ncode from programming competition\nsequential input model\n2019\nASE\nCLCDSA [300]\nJava,Python, C#\nMetrics\nDNN\ncode from programming competition\nLICCA [304],CLCMiner [305], [154]\n8.4\nBinary code clone detection\nBinary code clone detection can be used in the context of cross-platforms as well as legacy applications\nthat are already deployed in several critical domains.\nResearch on binary code clone detection is shown in Table 12.\nXue et al. [306] combined program slicing and a deep learning based binary code clone modeling\nframework to identify pointer-related binary code clones.\nXu et al. [307] proposed a neural network-\nbased approach to compute the embedding based on the control flow graph of each binary function, and\nthen to measure the distance between the embeddings for two binary functions. Marastoni et al. [308]\ntackled the problem of binary code similarity using deep learning applied to binary code visualization\ntechniques. They found that it is important to further investigate how to build a suitable mapping from\nexecutables to images.\nTable 12\nBinary Code Clone Detection\nYear\nVenue\nLiterature\nCode Representation\nDeep Learning Models\nBechmark\nBaseline\n2018\nMASES\n[308]\nvisualization graph\nCNN\nGoogleCodeJam etc.\nShallow Neural Net\n2018\nFEAST\nClone-Slicer [306]\nCFG,DDG\nRNN\nSPEC2006\nCloneHunter [309]\n2017\nCCS\nGemini [307]\nACFG\nStructure2vec\ndataset from [310] etc.\nGenius [310]\n8.5\nClone evaluation and validation\nMostaeen et al. [311] proposed a machine learning based approach for predicting the user code clone\nvalidation patterns. The proposed method works on top of any code clone detection tools for classifying\nthe reported clones as per user preferences. The automatic validation process can accelerate the overall\nprocess of code clone management.\nSaini et al. [312] presented a semi-automated approach to facilitating precision studies of clone detection\ntools. The approach merges automatic mechanisms of clone classification with manual validation of clone\npairs, so as to reduce the number of clone pairs that need human validation during precision experiments.\nLiu et al. [313] proposed an evaluation methodology that can systematically measure the cross-\nfunctionality generalizability of neural clone detection. They also conducted an empirical study and the\nresults indicate that the studied neural clone detectors cannot generalize well as expected. They found\nthat the performance loss on unseen functionalities can be reduced by addressing the out-of-vocabulary\nproblem and increasing training data diversity.\nYuetal. [314] presented an experimental study to show that BigCloneBench typically includes semantic\nclone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs.\nTo alleviate these issues, they abstracted a subset of the identifier names (including type, variable, and\nmethod names) in BigCloneBench to result in AbsBigCloneBench and used AbsBigCloneBench to better\nassess the effectiveness of deep learning models on the task of detecting semantic clones.\nJens and Ragkhitwetsagul [315] performed a manual investigation on BigCloneBench. They demon-\nstrated that the way BigCloneBench being constructed makes it problematic to use BigCloneBench as\nthe ground truth for learning code similarity. BigCloneBench fails to label all clone pairs. Moreover,\nonly a small set of true negatives has been created and, for most of the possible pairs in the dataset, the\nground truth is unknown. This leads to a strong impact on the validity of the ground truth for Weakly\nType3 and Type4 clone pairs, threatening the validity of results for evaluations in the Weakly Type3 and\nType4 category and approaches to learning code similarity.\nSci China Inf Sci\n36\nTable 13\nSource Code Clone Detection\nDataset\nLiterature\nLanguage\nBigCloneBench\n[249]\nJava\nOJClone\n[294]\nC\nGoogleCodeJam (GCJ)\n[268]\nJava\nSemanticCloneBench\n[316]\nJava, C, C#, Python\nSeSaMe\n[317]\nJava\nJC\n[301]\nJava, C++\nLeetcode\n[302]\nC, C++, Java, Python, JavaScript\n8.6\nDatasets\nWe summarize datasets used in clone detection in Table 13.\nBigCloneBench is a benchmark of inter-project clones from IJaDataset [249], a big Java source code\nrepository. It has about 8M labeled clone pairs, as well as 260,000 false clone pairs, covering 43 func-\ntionalities. BigCloneBench divides Type3 and Type4 clones into four categories: Very-Strongly Type3,\nStrongly Type3, Moderately Type3, and Weakly Type3/Type4.\nOJClone is generated based on OJ dataset [294] covering 104 functionalities. Each functionality is a\nprogramming question with 500 verified solutions written in C, submitted by students. Two solutions to\nthe same question can be considered as a clone pair.\nGoogleCodeJam (GCJ) is a benchmark similar to OJClone. It contains 1,669 solutions written in Java\nfor 12 functionalities collected from GoogleCodeJam.\nSemanticCloneBench is a benchmark of semantic clone pairs [316]. It consists of four thousand clone\npairs, each for four programming languages (i.e., Java, C, C#, and Python). The clone pairs are col-\nlected from the StackOverflow answers. The method pairs to the same questions on Stack Overflow are\nconsidered as semantic clones.\nThe SeSaMe dataset consists of 857 semantically similar method pairs mined from 11 open-source Java\nrepositories [317].\nThe JC dataset includes 10 categories of programs crawled by Bui etal. [301] from GitHub. It contains\n5822 Java files and 7019 C++ files. The code files for each category implement the same function.\nThe Leetcode dataset contains 50 categories of programs from Leetcode, each of which contains 400\nsemantically similar solutions of five different programming languages, with a total of 20000 files [302].\n8.7\nChallenges and Opportunities\nThis section presents the challenges and opportunities for further work on code clone detection.\n8.7.1\nChallenges\n• Challenge to build comprehensive learning-oriented code clone detection datasets.\nMost of the\nexisting code clone datasets are of a limited scale due to the effort required in manually constructing\nthe benchmarks. BigCloneBench is a large-scale dataset and becomes a standard to evaluate and\ncompare the performance of clone detection tools. Many researchers also use it to train deep learning\nmodels. However, as pointed by Jens and Ragkhitwetsagul, the incomplete ground truth and the\nbias and imbalance of the ground truth will have a strong impact on deep learning approaches for\ncode similarity that learned from Big-CloneBench’s ground truth [315].\nBesides, there still lack\nstandard datasets for cross-language clone detection. Also, there is no clear taxonomy about cross-\nlanguage clones. Therefore, it is necessary to build large, cross-language, learning oriented clone\ndatasets.\n• Challenge to cross-functionality generalizability of deep learning-based clone detection. Most of the\ndeep learning based code clone detection approaches are proposed for detecting semantic clones, and\nthey have achieved impressive results based on the tested benchmarks. However, according to the\nresearch by Liu et al., these studies are limited in detecting clones whose functionalities have never\nbeen previously observed in the training dataset [313]. Further research on deep learning algorithms\nSci China Inf Sci\n37\nLabel extraction\nm1\n...\nmn\nb\n...\n...\n1\n...\n...\n0\n...\n...\n1\nTraining set\nModel\nm1\n...\nmn\nb\n...\n...\n?\n...\n...\n?\n...\n...\n?\nTest set\nTest result\nLabel\nextraction\nLabel\noracle\nFigure 5\nThe overall structure of supervised defect prediction\nfor improving the cross-functionality generalizability is required. Moreover cross-functionality gen-\neralizability should also be considered in detection result evaluation.\n• Challenge to selecting suitable source code representation. Current deep learning based clone de-\ntection approaches use different representations of source code like tokens, AST, CFG, PDG. These\nrepresents vary in clone detection scalability, efficiency and effectiveness. For example, approaches\nrepresenting code fragments as sequences of tokens may be more efficient, but they may not be\ngeneralized to source code having out-of-vocabulary tokens, as vocabulary of tokens is unlimited.\nApproaches representing code fragments as PDGs help in semantic analysis. However the low ef-\nficiency may prevent them from being used on a large code base. Further research on source code\nrepresentation is still needed to deeply analyze the scalability, efficiency and effectiveness.\n8.7.2\nOpportunities\n• Opportunity to explore deep learning in more clone related tasks. A software may contain a lot of\nclones but not all clones need to be manipulated. History of clone management helps extract useful\nfeatures that can help take various decisions of clone related tasks. For example, deep learning based\nrecommendation can help automatically identify important clones for refactoring. Deep learning\nbased methods can also help predict the code clone quality, detect clone related bugs, and reduce\nthe maintenance cost caused by harmful or risky clones. However, only a few existing studies focus\non these tasks. Research can be further conducted in these areas.\n• Opportunity to explore large language model in detecting cross-language code clones.\nIn recent\nyears, large language models have experienced rapid development.\nBy pre-training on a large\namount of corpus, the model is able to “remember” knowledge from the corpus, including syn-\ntax, semantics, etc., thus typically being able to understand the syntax and semantics of different\nprogramming languages. This lays a good foundation for detecting cross-language code clones.\n9\nSoftware Defect Prediction\nSoftware Defect Prediction (SDP) aims to forecast potential defect locations in a software project, predict-\ning which modules (such as files, classes, and functions) may contain defects. This predictive information\nm1\n...\nmn\nb\n...\n...\n0\n...\n...\n1\n...\n...\n1\nSupervised\nlearner\nTarget\nversions (projects)\nSource\nversions (projects)\nTest\nTrain\nEvaluate\nPerformance\nreport\n>\nV\n>\nSci China Inf Sci\n38\nis crucial for the software quality assurance process. On one hand, it prioritizes modules awaiting inspec-\ntion or testing, facilitating early identification of defective modules in the software project under test. On\nthe other hand, it guides testing personnel to allocate code review or testing resources more efficiently\nand sensibly for each module. Allocating more review resources to modules with a higher likelihood of\ndefects can help quality assurance personnel discover as many software defects as possible within the\ngiven budget.\nAs depicted in Fig. 5, in the supervised defect prediction scenario, a learner is utilized to establish\nthe connections between features and labels using the training set. Subsequently, the learned model is\napplied to the test set to predict defect-proneness. For each instance in the test set, the corresponding\nfeatures are used to calculate the probability of being defect-prone. If the probability surpasses a pre-\ndefined threshold (typically set at 0.5), the instance is classified as “buggy”; otherwise, it is labeled as\n“clean”. Traditionally, numerous manually crafted features have been employed, such as size metrics,\ncomplexity metrics, cohesion metrics, and coupling metrics. However, these conventional hand-crafted\nfeatures primarily rely on syntactic information. Consequently, they fail to capture semantic information,\nresulting in limited predictive capability for defect-proneness. To address this limitation, a variety of deep\nlearning techniques have been adopted to generate powerful semantic features for defect prediction.\nTable 14 provides a summary of notable studies that employ deep learning techniques in defect predic-\ntion. The table includes information on the publication year and the type of defect prediction addressed\nin the first and second columns, respectively. To simplify the presentation, “WPDP” represents Within-\nProject Defect Prediction, while “CPDP” represents Cross-Project Defect Prediction. The third column\npresents the granularity at which defect prediction is conducted, spanning from file-level to statement-\nlevel. Note that GDRC (Graph of Defect Region Candidates) corresponds to an area of the code file.\nThe fourth column specifies the utilized deep learning technique. The fifth to ninth columns outline the\ninputs provided to the deep learning model for extracting powerful semantic features. An entry marked\nwith “•” or non-blank signifies that the corresponding information is used as input, while a blank entry\nindicates its absence. The last column provides the reference for each study.\nFrom Table 14, we can see that numerous studies have emerged, employing a variety of deep learning\ntechniques for defect prediction. Among these techniques, CNN, RNN (including LSTM as a variant),\nand GCN stand out as the most prominent ones, each offering distinct capabilities that researchers find\nadvantageous for defect prediction. CNN proves highly effective in capturing localized patterns and spatial\ndependencies within the data. RNN excels at capturing sequential and long-term dependencies, which are\nparticularly valuable in understanding the temporal aspects of defect occurrences. GCN demonstrates\nremarkable proficiency in capturing the structural information and intricate inter-dependencies within\ncode, attributes that are crucial for accurate defect prediction. Overall, the adoption of deep learning\ntechniques has significantly advanced defect prediction research, offering valuable insights into defect\ndetection.\nOverall, in the defect prediction community, significant attention has been dedicated to utilizing\ndeep learning in order to generate expressive features and enhance the effectiveness of defect predic-\ntion.\nThrough the application of deep learning, remarkable progress has been made in automatically\ngenerating features that are both highly informative and discriminative.\nThis transition from manual\nfeature engineering to automated feature extraction using deep learning signifies a fundamental change\nin approaches to predicting defect-proneness in software systems. The objective is to construct models\ncapable of comprehending the fundamental traits and complexities of the code, thereby enabling more\ndependable and accurate defect predictions.\n9.1\nUsing manually crafted features as the input\nIn the literature, numerous manually crafted features have been proposed for defect prediction. How-\never, these features often focus on specific characteristics of a module, and their high-level semantic\nrelationships are not adequately captured, limiting their defect prediction capability.\nTo address this\nlimitation, Yang et al. [318] utilized DBN to generate expressive features from a set of initial change\nfeatures. This approach results in more powerful change-level defect prediction models. Similarly, Tong\net al. employed SDAEs to extract expressive features from traditional software metrics [323]. Following\ntheir work, a range of deep learning techniques have been explored for this purpose, including deep for-\nest [326], DNN [327,339], Layered RNN [328], DAE-CNN [332], Bi-LSTM [334], MDA [344], SSDAE [345],\nand TCN [347].\nSci China Inf Sci\n39\nTable 14\nAn overview of prominent defect prediction models utilizing deep learning techniques\nYear Type\nGranularity\nDeep\nlearning\ntechnique\nInput to deep learning\nReference\nHand-\ncrafted AST\nfeatures\nCFG/\nCPG\nRaw\ncode\n(token)\nOther\n2015 WPDP\nChange\nDBN\n•\nYang et al. [318]\n2017 WPDP\nFile\nCNN\nCFG\nPhan et al. [319]\nWPDP\nFile\nCNN\n•\nLi et al. [320]\n2018\nWPDP/CPDP File\nCNN\n•\nComments Huo et al. [321]\nWPDP\nFile\nRNN\n•\nChange\nhistory\nLiu et al. [322]\nWPDP\nFile/Function\nSDAEs\n•\nTong et al. [323]\n2019\nCPDP\nFile\nCNN\n•\nQiu et al. [324]\nWPDP\nChange\nCNN\ncommit log\n+ change\nHoang et al. [325]\nWPDP\nFile/Function Deep Forest\n•\nZhou et al. [326]\nWPDP\nFile\nDNN\n•\nXu et al. [327]\nWPDP\nFile\nLayered RNN •\nTurabieh et al. [328]\nWPDP/CPDP File\nLSTM\n•\nDam et al. [329]\nCPDP\nFile\nBi-LSTM\n•\nLi et al. [330]\n2020\nWPDP/CPDP File\nCNN\n•\n(im-\nage)\nChen et al. [331]\nWPDP/CPDP Change\nDAE-CNN\n•\nZhu et al. [332]\nWPDP/CPDP File/Change\nDBN\n•\nWang et al. [333]\nWPDP\nFile\nBi-LSTM\n•\nDeng et al. [334]\nWPDP/CPDP File\nBi-LSTM\n+Attention\n•\nShi et al. [335]\nWPDP\nStatement\nLSTM\n•\n•\nMajd et al. [336]\nWPDP\nFile\nLSTM\n•\nChange\nhistory\nWen et al. [337]\n2021\nWPDP/CPDP File\nCNN\n•\nShi et al. [338]\nCPDP\nFile\nDNN\n•\nXu et al. [339]\nWPDP\nFile\nGCN\n•\nXu et al. [340]\nWPDP\nFile\nGCN\n•\nCDN\nZeng et al. [341]\nCPDP\nGDRC\nGCN\nCPG\nXu et al. [342]\nWPDP\nFile\nLSTM\n•\nWang et al. [343]\nCPDP\nFile\nMDA\n•\nZou et al. [344]\nWPDP\nFile/Function\nSSDAE\n•\nZhang et al. [345]\n2022 WPDP/CPDP File\nBi-LSTM\n•\nUddin et al. [346]\nWPDP\nChange\nTCN\n•\nArdimento\net\nal. [347]\n2023 WPDP\nCode line\nHAN\n•\nPornprasitetal. [348]\nWPDP\nFile\nCNN\n•\nQiu et al. [349]\nSci China Inf Sci\n40\n9.2\nUsing raw source code as the input\nTraditional manually crafted features in source code analysis tend to be syntax-based, disregarding the\nvaluable semantic information embedded in the code. To overcome this limitation, Chen et al. introduced\na novel technique that visualizes the source code of programs as images and then employs CNN to extract\nimage features [331]. These extracted features are utilized for building more effective defect prediction\nmodels. Uddin et al. leveraged Bi-LSTM to capture expressive features from the embedded token vectors\nobtained through BERT applied to the source code [346]. This technique allows effectively harnessing\nthe semantic information within the code for defect prediction.\nFurthermore, Pornprasit et al. [348]\nadopted HAN to learn semantic features from the surrounding tokens and lines of code. This approach\nenables prediction of defective lines more accurately by considering the context and relationships between\ndifferent parts of the code.\n9.3\nUsing abstract syntax trees as the input\nOne significant drawback of using raw source code as input is the disregard for crucial structural infor-\nmation when extracting expressive features. To tackle this issue, Li et al. introduced a novel approach,\nutilizing a program’s AST (Abstract Syntax Tree) as input to generate semantic features [320]. In their\napproach, Li et al. initially extracted token vectors from the program’s AST and then transformed them\ninto numerical vectors through mapping and word embedding. Subsequently, they fed these numerical\nvectors into a CNN, allowing the model to learn both semantic and structural features for defect predic-\ntion. Following this breakthrough, several other deep learning techniques have been explored for the same\npurpose, including LSTM [329,343], Bi-LSTM [330,335], DBN [333], and GCN [340]. These approaches\naim to enhance defect prediction by considering the inherent structural characteristics of the source code,\nleading to more effective and accurate models.\n9.4\nUsing graphical representations as the input\nPhan etal. emphasized the importance of utilizing precise graphs that represent program flows to capture\nthe complex semantics of programs accurately [319]. They argued that tree structures like ASTs might\nfall short in this regard. To address this issue, Phan et al. adopted CFGs (Control Flow Graphs) and\napplied GCN to extract expressive features. By leveraging CFGs, they were able to capture intricate\nprogram dependencies and interactions more effectively. The GCN allows the generation of expressive\nfeatures that incorporate both local and global information from the program’s control flow. As a result,\ntheir approach improves the accuracy of defect prediction models by considering the intricate semantics\nof the code. In a related study, Xu et al. proposed an alternative approach using CPGs (Code Property\nGraphs) as input to GCN [342]. This approach aims to extract even more expressive features for defect\nprediction, further enhancing the capabilities of the model in predicting software defects.\n9.5\nUsing hybrid-source information as the input\nIn addition to source code, other information such as comments, commit logs, and change history contains\nvaluable insights for defect prediction. Recognizing this, researchers have proposed innovative approaches\nto leverage this diverse information for more accurate defect prediction models. Huo et al. introduced\na technique that jointly learns semantic features from both source code and comments for defect pre-\ndiction [321]. Likewise, Liu et al. utilized the historical version sequence of manually crafted features\nfrom continuous software versions as input for RNN in defect prediction [322]. Subsequent studies follow\na similar path, combining different sources of information.\nSome explore the joint use of change and\ncommit logs [325], while others investigate the combination of manually crafted features and CDN (Class\nDependency Networks) [341]. Integrating these diverse data sources provides richer and more expressive\nsemantic features, leading to further improvements in defect prediction capabilities.\n9.6\nDatasets\nThe field frequently relies on datasets like NASA, PROMISE, AEEEM, and Relink, valued for their\nextensive coverage of Java and C, the primary programming languages [326,345]. These datasets pre-\ndominantly focus on file-level granularity and typically encompass fewer than 1,000 instances within a\nproject.\nSci China Inf Sci\n41\n• Programming language. The majority of datasets comprise open-source projects, prominently\nfeaturing programming languages such as Java, C, and C++ [326]. Typically, these studies encom-\npass around 10 projects, with over 80% utilizing Apache projects.\n• Prediction granularity. The datasets exhibit varying levels of prediction granularity, spanning\nfrom file-level, change-level, function-level, to statement-/line-level.\nFile-level granularity is the\nmost prevalent (nearly 80%), whereas statement-/line-level granularity receives less emphasis [336,\n348].\n• Training dataset size. In the majority of studies, within-version defect prediction is the norm,\nleading to training sets typically comprising fewer than 1,000 instances. Notably, only a handful of\nchange-level defect prediction studies leverage notably larger datasets, with instance counts reaching\ntens of thousands [332].\nIn summary, existing studies primarily concentrate on projects involving a small number of program-\nming languages, mainly focusing on coarse-grained defect prediction. In particular, the training set sizes\nfor deep learning are often limited.\n9.7\nChallenges and Opportunities\nWhile numerous studies have put forward different defect prediction methodologies, several significant\nchallenges hinder the development of accurate and cost-effective approaches for defect prediction. These\nchallenges encompass issues like model interpretability, thorough assessment, and the replication of exper-\niments. Simultaneously, there are promising opportunities on the horizon that could potentially address\nthese challenges in future studies on defect prediction.\n9.7.1\nChallenges\nThe application of deep learning to defect prediction presents the following main challenges that need to\nbe addressed:\n• Limited interpretability of control- and data-flow in relation to defect-proneness. Prac-\ntitioners highly value understanding the root causes within code that contribute to defect-proneness,\nparticularly concerning control- and data-flow dynamics. Such understanding is pivotal in compre-\nhending the occurrence of defects and subsequently addressing them. However, the opacity of deep\nlearning models presents a challenge, as they are often perceived as black boxes, impeding the\nability to interpret the rationale behind their predictions. This lack of transparency restricts the\ninterpretability of the models, posing a barrier to the adoption in defect prediction, an area where\ninterpretability holds critical importance.\n• Insufficient assessment of the effectiveness in comparison to traditional models. Many\nstudies evaluating new deep learning models tend to solely compare them against earlier deep learn-\ning approaches, often neglecting comprehensive comparisons with established traditional models.\nThis oversight results in a significant gap in understanding the true advancements brought about\nby deep learning techniques in defect prediction. Consequently, it becomes challenging to gauge\nthe tangible progress and assess how deeply these adopted deep learning methodologies truly drive\nthe field forward compared to well-established traditional models.\n• Substantial challenges in replicating the reported findings externally.\nThe absence of\nstandardized practices for distributing replicated packages, encompassing datasets and their cor-\nresponding scripts, poses a significant challenge.\nIn practice, it is well known that even minor\nvariations in the re-implementation of prior models can lead to substantial differences in defect\nprediction performance. This lack of transparency and accessibility hinders external validation of\ninsights derived from past studies. As a result, ensuring the robustness and applicability of reported\nconclusions becomes progressively more challenging.\nFuture studies should aim to address the aforementioned challenges to accurately gauge the progress\nmade in the field of defect prediction. Failing to do so will hamper our understanding of the extent to\nwhich advancements have been made. Furthermore, there is a risk of unintentionally drawing misleading\nconclusions regarding the advancement of the state-of-the-art, potentially leading to missed opportunities\nfor further progress in defect prediction.\nSci China Inf Sci\n42\n9.7.2\nOpportunities\nThere are several valuable opportunities available to address the challenges associated with deep learning-\nbased defect prediction.\n• Development of interpretable techniques. One promising opportunity lies in the development\nof novel techniques that enhance the interpretability of deep learning models in defect prediction.\nResearchers can explore methods such as feature attribution,saliency mapping, and attention mech-\nanisms to gain insights into the factors contributing to defect-proneness. These techniques would\nenable better understanding of model behavior and facilitate the identification of critical features\nand patterns related to defects.\n• Comparative evaluation frameworks. A key opportunity is the establishment of comprehensive\nevaluation frameworks for deep learning-based defect prediction. This involves designing rigorous\ncomparative studies that systematically compare the performance of deep learning models with\ntraditional approaches on diverse datasets. By incorporating various evaluation metrics and statis-\ntical analysis, researchers can provide robust evidence on the effectiveness and advantages of deep\nlearning models.\n• Replication and external validation studies. Another important opportunity is to conduct\nreplication and external validation studies to verify the reported findings in deep learning-based\ndefect prediction.\nCollaboration among researchers, sharing of datasets and code, and adopting\nopen science practices can facilitate the replication of experiments across different research groups.\nExternal validation studies conducted on independent datasets can provide further validation of the\ngeneralizability and reliability of the reported results.\nBy embracing these opportunities, researchers can address the challenges faced in deep learning-based\ndefect prediction and pave the way for advancements in model interpretability, performance evaluation,\nand the overall reliability of defect prediction systems.\n10\nBug Finding\nThe term “bug” originates from a literal bug (i.e., moth) stuck in the relay of the Mark II computer, and\nhas been used to denote any defect that violates its specification: unexpected crashes, incorrect results,\nand information leakages, etc. Bug finding is a process that involves examining software artifacts (source\ncode repositories, documentation, and existing test inputs, etc.) and generating a list of potential bugs\nwith explanations.\nThe process of bug finding goes beyond mere “wild guesses”. Unlike defect prediction (Section 9), which\nidentifies loose correlations between software metrics and bugs, bug finding techniques should provide\nconcrete bug certificates. Certificates can aid developers in accurately identifying the actual presence and\nroot cause of a bug. Such certificates could be a hint, like a bug anti-pattern, or a specific test case that\ntriggers a program error. This section discusses the three mainstream approaches to bug finding:\n1. Static analysis. Bugs reside within source code, and bugs can be identified by “reading” the source\ncode and its associated artifacts, without the need to execute the program in a real environment.\nStatic bug does not require resources (such as hardware platform, computational power, environ-\nment, and dependencies) to bootstrap the software, making it accessible to any party involved in\na software’s life cycle. From Lint tools [350] to bug finders [351,352], static analysis serves as a\nfundamental gate-keeping procedure in the software development process.\n2. Dynamic testing. More reliable certificates provide undeniable evidence of a bug’s existence, with\ntest cases [353] being the most commonly used such certificates. Developers create unit and system\ntest cases, and software is also extensively validated using machine-generated test cases. Passing\ntest cases enhance the confidence that the software functions under various circumstances.\n3. Formal verification. In theory, one can exhaustively test a (finite-state) system to prove its bug-\nfreedom. To make this procedure practical, one can leverage path-based abstraction and employ a\nSci China Inf Sci\n43\nconstraint solver to manage the search space [354,355]. Alternatively, one can provide a machine-\ncheckable proof to a proof assistant.\nThere have been successful reports on the verification of\ncompilers [356], operating system kernels [357], and other software systems [358].\nThe three mainstream approaches have undergone decades of evolution, particularly the classical (for-\nmal) algorithmic bug-finding techniques. These techniques rely on algorithms, logical procedures that can\nbe mechanically implemented over simple axioms, to identify bugs [359]. With the advent of deep learn-\ning, probabilistic techniques based on machine learning have gained popularity, because learned models\ncan effectively digest inputs from the chaotic and complex human world.\nThe algorithmic and probabilistic paradigms complement each other in the deep-learning era. This is\nbecause software, which projects the requirements of the probabilistic human world onto the algorithmic\ncomputing world, intersects both of these realms [360,361]. To provide a bug certificate, one must possess\nnot only a thorough understanding of the requirements but also the ability to engage in complex logical\nreasoning.\nTherefore, we argue that significant research efforts should be invested to answer how to leverage learned\n(albeit imperfect) domain-knowledge to facilitate an effective algorithmic bug-finding procedure. This\napproach builds upon the success of AlphaGo [362], harnessing the potential of AI-in-the-(algorithmic)-\nloop”, akin to having “experienced human experts” consistently offering insights into software artifacts\nand intermediate results whenever decisions are required in static checking, dynamic testing, and formal\nverification. On the other hand, it would also be equally interesting to explore whether bug-finding can\nbe driven by an autonomous agent, like the chain of thought [363,364], by automatically exploiting the\nexisting algorithmic bug-finding techniques.\nIn addition to the classical papers that shed light on the fundamentals of bug finding, this section also\ncomprehensively surveys recent papers by conducting Google Scholar search using the combinations of\nthe following two sets of keywords:\n• S1 : Testing, validation, verification, fuzzing, program analysis, static analysis, dynamic analysis,\nsymbolic execution, formal methods;\n• S2 : Neural network (NN),deep learning (DL),machine learning (ML),reinforcement learning (RL),\nlarge language model (LLM), transformer.\nWe collect recent (2018–2023) papers and narrow our selection to papers from top-tier conferences and\njournals in the fields of software engineering, programming languages, and computer systems. Addition-\nally, we include widely-cited papers with 30 or more citations. In total, we select 72 papers for this section\n(and more in the case study on vulnerability detection in Section 10.4). These papers are categorized\nand discussed as follows.\n10.1\nStatic Bug-finding: Program Analysis\n10.1.1\nCode proofreading\nEven if we do not check against any specification on application behavior, we expect that source code\nreads like natural language texts, following the famous quote from student’s first programming class [365]:\nPrograms are meant to be read by humans and only incidentally for computers to execute;\nReadability implies that the code is easier to maintain, and researchers do find that programs resemble\nnatural-language texts to some extent [366]. This “naturalness” of software serves as the foundation of\ncode proofreading (by either a human or a checker of probabilistic distribution) by identifying anomalies\nthat correlate to bugs.\nSuch “proof reading” can be algorithmic, i.e., checking against formal best-practice specifications\n[367,368] or lint rules [350]. Adding a bit of probability to linters yields interesting chemical reactions:\nany “odd” (infrequent) pattern may suggest a bug [369]. This “may belive” approach paves a way for\nharnessing the power of both formal templates and code-level counter-coincidental couplings, leading to\nthe family of bug-finding techniques that mine frequent items and consider minor items as bugs, e.g.,\nCP-miner [370].\nToday’s deep learning models, which are fine-tuned over billion lines of code (e.g., llama2-code), are far\nmore capable than finding these counter-coincidental couplings between lexical tokens. Large language\nSci China Inf Sci\n44\nmodels exhibit strong usefulness in finding (and even fixing) the “unnatural” bugs, even for semantics\nbugs that violate application specifications.\nFor example, GitHub Copilot is an AI pair programmer\nwhich facilitates the finding and repairing of bugs through its context-sensitive recommendations [371].\nBUGLAB is a self-supervised approach that trains bug detectors by co-training a bug selector, which\nproduces bugs that are elusive to detect [372].\nSeemingly surprising, the intuition behind these papers is straightforward: a small code snippet is\n“almost completely determined” by its surrounding context. Based on the fact that most of the bugs are\nnot “new” and there are similar bug samples in the training data, a language model can well predict the\nexistence of them–code proofreading is very useful as a gate-keeping procedure for improving software\nquality. On the other hand, we still cannot fully rely on it, either algorithmically or probabilistically, to\nfind bugs. Probability distributions like ChatGPT are always happy to provide proofreading results and\nexplanations on the bug, but many times “talks nonsense with a straight face”. Algorithmic lint tools\nproduce an excessive number of false alarms. These fundamental limitations can be alleviated, but is not\nexpected to be resolved in the near future [360,373].\n10.1.2\nSemantic analysis\nTo catch subtle semantic bugs that appear to be correct and escape proofreading, it is necessary to\nunderstand precisely what a piece of program can and cannot do (i.e., conducting a semantic analysis)\n[374]. Observing that each program statement’s behavior has been rigorously defined as a part of the\nlanguage specification, it would be theoretically possible to predict a program’s all possible behaviors by\nan algorithmic procedure (see Section 10.3; however, the general problem is undecidable [375]). On the\nother hand, while today’s language models can somehow find semantic bugs, their probabilistic nature\nmake them less predictable (and thus less reliable) than algorithms.\nClassical semantic analysis strives to over-approximate the programs semantics to obtain sound predic-\ntions of program outcomes. In practice, static analysis techniques have to trade off between usefulness,\nthoroughness, and engineering efforts. For decades, the data-flow and abstract interpretation framework\ndominates the field, and numerous properties about programs can be automatically proved under rig-\norous logical reasoning upon the formal operational semantics [376], and scaling to millions of lines of\ncode [352].\nStill, static analyzers are limited to finding bugs of limited “general” types of semantic bugs like pointer\nerrors and data races, which are neutral to the application logic. It seems still a long way for today’s\nanalyzers to automatically prove arbitrary developer’s assertions about program states. Certificates from\nstate analysis are also imperfect for bug-finding. Static analyzers have to draw indefinite conclusions like\n“two variables may have the same value” (but actually not) to achieve scalability, resulting in false bug\ncertificates.\nThe paths for algorithmic and probabilistic static analysis have long been diverged, and the progress\nto unite them is still exploratory.\nProbabilistic distributions are useful in providing heuristic hints,\ne.g., control of the degree of analysis sensitivity to meet resource constraints [377]. [378] proposes a\ntechnique to learn effective state-selection heuristics from data, in order to keep track of a small number of\nbeneficial program states in Infer [352]. LLift [379] complements semantic analysis by providing speculate\nconclusions on undecided (timeout) symbolic-execution paths. Static analyses can also be complemented\nby learned features over reduced programs [380], or leveraging anomaly detection techniques to learn a\nbalance between precision and scalability [381].\nMachine learning can also be incorporated with specific static analysis techniques for better effectiveness\nand precision. [382,383] address the challenge of manually developing cost-effective analysis heuristics for\npointer analysis using an algorithm for heuristic learning.\nLAIT [384]\nutilizes an iterative learning\nalgorithm that develops a neural policy to identify and eliminate redundant constraints throughout the\nsequence in order to produce a faster and more scalable numerical analysis.\nToday’s deep learning models are fundamentally limited in predicting the execution results of even a\nsmall code snippet [385], and we believe that static analysis is a field to be revolutionized by deep learning.\nThe potential lies in the naturalness of software, which “informally” reflects program semantics, like the\nfollowing piece of code, which is difficult to be fully (rigorously) analyzed. On the other hand, a language\nmodel can speculate variable’s types or even possible values at a specific program point [386,387], even\nif the static analyzer cannot prove it.\nThese results may further benefit analyses on other parts of\nthe program, which may be useful in developing efficient analyses.\nUnfortunately, there still lacks a\nSci China Inf Sci\n45\nframework that can simultaneously exploit the power of rigorous and long logical reasoning and the\npower of understanding the human-world semantics of programs from LLMs [388,389].\nAnother promising direction is approximating program semantics via deep-learning models. For exam-\nple, neural program smoothing [390,391] approximates programs as differentiable function of inputs and\naccordingly generates new test inputs using gradient descent. [392] constructs a benchmark suite with\n28 open-source projects and proposes PreFuzz, which guides fuzzing by a resource-efficient edge selec-\ntion mechanism and a probabilistic byte selection mechanism. However, the efficacy of neural program\nsmoothing remains an area for further exploration according to extensive evaluation [393].\nEnabling AI-in-the-loop for static analysis brings another challenge: machine learning models can be as\nlarge as billions of parameters, and it is impractical for static analyzers that frequently query the model.\nToday’s AI-aided static analysis is still limited to manual feature engineering, in which simple classifiers\nlike gradient boosting [378] are used. Deep learning model inference is considerably more expensive, and\nthe problem remains open.\n10.2\nDynamic Bug-finding: Software Testing\n10.2.1\nTest oracle\nBefore one can test a program, a fundamental question should be answered first: what do we mean by a\nprogram to be “correct”? Some correctness criteria are obvious: a program should not crash, should be\nrace-free, memory-safe, assertion-pass, and, most importantly, functional. A somehow desperate fact is\nthat as long as software reflects a physical-world procedure (i.e., requirements), being functional becomes\na myth1), and this is referred to as the “specification crisis” [394].\nIn the context of testing, such a specification, which decides whether each test case passes, is called a\ntest oracle. General programming errors, e.g., semantic errors discussed in Section 10.1, can be a part\nof a test oracle and can be effectively checked at runtime.\nSanitizers [395,396] are famous for being\neffective in bug-finding for practical systems. However, finding a generic test oracle that decides whether\na program is functional remains as an open problem.\nDespite that the research community has a strong focus on modeling and lightweight formal methods\n[397], we argue that the test oracle problem may be one of the first problem to be effectively solved\nby deep learning. A key observation is that, while programs are generally hard for neural networks to\nunderstand, an execution trace really looks like a (maybelong) story consisting of events. The implication\nof this observation is twofold:\n1. Deep learning models can mimic a human developers that write testcases or directly act as a human\ntest operator, and simultaneously generate expected program behavior (test oracle) [398], following\nthat a test case describes a “natural” procedure in the human world, e.g., generating natural faults\nfor mutation testing [399].\n2. Deep learning models can digest software’s execution traces. Traces can be projected to the human\ndomain, and a language model can find “common sense violation” cases [400].\nTest oracles for unit testing is a promising direction to be solved by machine learning [401,402].\nAthenaTest [403] is a method designed to generate unit test cases through a sequence-to-sequence\nlearning model, trained first on a large unsupervised Java corpus for denoising and fine-tuned on real-\nworld methods and developer-written test cases.\nAtlas [404] is a Neural Machine Translation (NMT)\nbased approach designed to predict a meaningful assert statement to assess the correctness of the focal\nmethod. [405] leverages a transformer model, first pre-trained on an English textual corpus and then\nsemi-supervised trained on a substantial source code corpus, culminating in finetuning for generating\nassert statements in unit tests.\nCallMeMaybe [406] is a technique employing natural language pro-\ncessing (NLP) to analyze Javadoc comments for identifying temporal constraints, thereby aiding test\ncase generators in executing method call sequences that adhere to these constraints. TOGA [407] is a\nunified transformer-based neural method designed to deduce exceptional and assertion test oracles from\nthe context of the focal method, effectively addressing units with unclear, absent documentation, or even\nmissing implementations. ChatUniTest [408] is a ChatGPT-based automated unit test generation tool\nwhich creates adaptive focal contexts from projects, uses them in prompts for ChatGPT, and then val-\nidates and repairs the generated tests using rule-based and ChatGPT-based approaches. A3Test [409]\n1) For example, more types of genders are recognized by the society over time, but software systems often fall behind.\nSci China Inf Sci\n46\nis a DL-based approach that addresses the limitations of AthenaTest [403] by incorporating assertion\nknowledge with a mechanism to verify naming consistency and test signatures. CodaMosa [398] lever-\nages an LLM to generate unit tests (oracles) to reach a designated program part. To generate test oracles\nfor games, Neatest [410] navigates through a program’s statements, constructing neural networks that\nmanipulate the program for dependable statement coverage, essentially learning to strategically explore\nvarious code segments. One can also model the test generation problem as a completion problem [116].\nExisting studies show that the quality of LLM-generated test cases (including oracles) still have signif-\nicant room for improvement [411–414]. [415] points out that unit tests generated by ChatGPT still suffer\nfrom correctness issues, including diverse compilation errors and execution failures.\nThe challenge of implementing a “neural test oracle” is that program traces are too verbose for today’s\ntransformer architecture to digest. Neural networks fall short on finding long-range logical dependencies\nin the trace. The probabilistic nature of deep learning models also implies that they are imperfect test\noracles. Nevertheless, we can always generate more test cases, and deep neural networks are not likely\n(though still possible) to make false-negative predictions on all test cases that trigger the same bug.\n10.2.2\nTest input generation\nEven if the specification crisis is resolved, bug-finding is still a challenge. The number of test inputs is\nastronomically high, and we do not have the resources to examine all of them. This issue, which we refer\nto as the “search-space curse,” is akin to finding needles in a haystack.\nConsequently, we only afford\nsampling useful test inputs that exercise diverse program behaviors (in hope of revealing bugs). A natural\nidea is to decompose the input space over its structure:\n1. Connecting the search space, based on the observation that useful (generally available, e.g., regres-\nsion tests [416,417]) inputs can be slightly mutated (modified) to obtain another useful input, which\nmay manifest different program outcomes. Then, the input space can be decomposed into a graph,\nwhere vertices (inputs) are connected by mutation operators. The exploration procedure can be\nguided, e.g., by leaning towards test inputs that cover new code [418–420]. Machine learning is also\nuseful in creating mutants [391,399,421].\n2. Partitioning the search space, and only sample representative test input(s) are selected in the equiv-\nalent classes.\nTo alleviate the search-space curse, symbolic execution essentially merges all test\ninputs that share the same control-flow path. Then, for every path, we only care about whether\nit is reachable or not. The search is postponed, and a clever constraint solver may quickly draw a\nconclusion.\nWe see solid progress in improving how we decompose (and explore) the input space. Learned probability\ndistributions over test inputs, traces, and execution logs are undoubtedly useful in whenever decisions\nshould be made, e.g., in ranking the mutants [422,423] or providing useful “golden” seeds [424,425].\nDeep learning-based generative models, particularly LLMs, exhibit exceptional performance in code\ngeneration tasks and are thus frequently employed in generating seeds for testing compilers or deep\nlearning libraries. Deng et al. presented TitanFuzz as the first technique to directly leveraging LLMs\nto generate seeds for fuzzing DL libraries [424].\nFuzzGPT [425] follows up the work by using LLMs\nto synthesize unusual programs for DL fuzzing. WhiteFox [426] uses LLMs to produce test programs\nwith source-code information to fuzz compilers.\nFuzz4All [427] is the first universal fuzzer capable\nof targeting a wide range of input languages and their various features.\nIt capitalizes on LLMs for\ninput generation and mutation, generating diverse and realistic inputs for any language.\nCOMFORT\n[428] is another compiler fuzzing framework designed to identify bugs in JavaScript engines, utilizing\nadvancements in deep learning-based language models for automatic JS test code generation. DeepSmith\n[429] uses generative models to generate tens of thousands of realistic programs, thereby accelerating the\nfuzzing process of compilers.\nMachine-learning can also provide heuristic decisions for accelerating fuzzing. Specific work includes\nRegFuzz, a directed fuzzing approach that employs a linear regression model to predict seed effective-\nness, thereby allocating more energy and fuzzing opportunities to efficient seeds [430]. HATAFL [431]\nutilizes pre-trained LLMs to construct grammars for protocol message types and assist in mutating\nmessages for protocol fuzzing.\nSEAMFUZZ [421] learns effective mutation strategies by capturing the\ncharacteristics of individual seed inputs. [422] proposes a reinforcement learning-based hierarchical seed\nSci China Inf Sci\n47\nscheduling strategy for greybox fuzzing. RLF [432] models the fuzzing process of smart contracts as a\nMarkov decision process and uses a specially designed reward system that considers both vulnerability\nand code coverage. [433] leverages the Monte Carlo Tree Search (MCTS) to drive DL model genera-\ntion, thus improves the quality of DL inference engines. [434] employs machine learning models and\nmeta-heuristic search algorithms to strategically guide the fuzzing of actuators, aiming to maneuver a\nCyber-Physical System (CPS) into various unsafe physical states. NeuFuzz [423] utilizes deep neural\nnetworks for intelligent seed selection in graybox fuzzing which learns vulnerability patterns in program\npaths. Some studies also utilize machine learning models to integrate fuzzing with symbolic execution.\nFor example, [435] predicts the timing for switching between concrete and symbolic executions. [436]\ntrains a neural network-based fuzzing policy on the dataset generated by symbolic execution, enabling\nthe application of the learned policy to fuzz new programs. JOpFuzzer [437] learns the relationships\nbetween code features and optimization choices to direct seed mutation for JIT compiler fuzzing.\nThe challenge here is similar to incorporating deep learning within static analysis: excessive amount\nof expensive queries may outweigh simply exercising more test cases.\nMachine learning models generally perform better for domain-specific test input generation. For exam-\nple, WebExplor [438] leverages a curiosity-driven reinforcement learning to generate high-quality action\nsequences (testcases) for web testing. FIGCPS [439] adopts deep reinforcement learning to interact with\nthe Cyber-Physical Systems (CPS) under test and effectively searches for failure-inducing input guided by\nrewards. Mobile applications provide a natural human interface, which can be effectively understood by\nmachine-learning models. QTypist [440] utilizes a pre-trained LLM for generating text inputs based on\nthe context of a mobile application’s GUI. [441] proposes Deep GUI, which enhances black-box testing\nby utilizing deep learning to generate intelligent GUI inputs.\nAdaT [442] is a lightweight image-based\napproach that uses a deep learning model to dynamically adjust inter-event times in automated GUI\ntesting based on the GUI’s rendering state. Badge [443] is an approach for automated UI testing which\nuses a hierarchical multi-armed bandit model to prioritize UI events based on their exploration value\nand exploration diversity. Q-testing [444] employs a curiosity-driven strategy to focus on unexplored\nfunctionalities and uses a neural network as a state comparison module to efficiently differentiate between\nfunctional scenarios. [445] introduces Avgust, a system that automates the creation of usage-based tests\nfor mobile apps by using neural models for image understanding.\nMachine learning models are also capable of understanding “stories”–API call sequences.APICAD\n[446] and NLPtoREST [447] are tools that enhance REST API testing by applying NLP techniques\nfrom API documents and specifications. [448] describes an adaptive REST API testing technique that\nemploys reinforcement learning to prioritize API operations and parameters, using dynamic analysis of\nrequest and response data and a sampling-based strategy to efficiently process API feedback.\n10.3\nProving Bug-freedom: Formal Verification\n10.3.1\nSearching for needles in the haystack\nTo the extreme end of testing, one can theoretically test over all possible inputs to verify that a program\nis bug-free (or to find all bugs). Exhaustive enumeration is the ultimate victim of the search-space curse.\nWhile we cannot leverage the small explanation hypothesis in verification (we cannot leave any corner case\nunchecked), the idea of search space decomposition still applies2). For example, one can separately verify\neach program path, in which each verification is essentially a smaller search problem that can be solved\nby a constraint solver. Search spaces may have their own structures and pruning opportunities [449],\nwhich can be accelerated by machine learning [450].\nOn the other hand, path-based verification is not a silver bullet. Loops, even nested with simple control\nflow, post significant scalability challenges to a symbolic verifier. For example, a loop-based popcount\nimplementation, which sequentially checks each bit of a 32-bit integer and increments a counter when\nthe bit is set, consists of 232 distinct program paths, and off-the-shelf symbolic execution engines fail to\nverify it. Sometimes we may rewrite the above function into one formula that can be recognized by a\nconstraint solver [451], e.g.,\n2) Decomposition is not limited to input spaces.\nOne can also decompose the search space consisting of program states for\nmodel checking.\nSci China Inf Sci\n48\nto “offload” the 232 paths to the constraint solver.\nGenerally, we do not have this luck for most of the\npractical cases, and symbolic program verification is still limited to small programs.\nThe complexity\nissues raised by control flows, pointers, memory allocation, libraries, and environments, are all challenges\nto verify practical programs [452].\nDynamic symbolic execution is a path-based program verification technique, and many learning-based\ntechniques have emerged to ease the search-space curse in symbolic execution. Most studies focus on\nemploying machine learning techniques to devise an optimized search strategy, thereby reducing the\ntime and space overhead of path enumeration.\nLearch [453] utilizes a machine learning model to\npredict the potential of a program state, specifically its capability to maximize code coverage within\na given time budget. [454–456] dynamically adapt search heuristics through a learning algorithm that\ndevelops new heuristics based on knowledge garnered during previous testing. There are also techniques to\nprune the search space. Homi [457] identifies promising states by a learning algorithm that continuously\nupdates the probabilistic pruning strategy based on data accumulated during the testing process. Others\noptimize symbolic execution from different perspectives, including the prediction and optimization of\npath constraints [458,459], as well as the fine-tuning of search parameters [460], and transformation of\ntarget code [461].\nHow can probabilistic techniques be useful in software verification, i.e., an exhaustive search over the\ninput space? The use of machine learning model must be sound, i.e., the checking results remain correct\neven under prediction errors. This problem remains open today.\n10.3.2\nProviding a checkable proof\nProving bug-freedom does not really require an exhaustive enumeration. Programs are rigorous mathe-\nmatical objects: programs can be regarded as a function taking an input and produces an execution. One\nwould always provide such a program with a logical proof that asserts all produced executions satisfying\nthe specification, like we prove the correctness of any algorithm, e.g., bubble sort indeed gives a sorted\narray after n −1 iterations. The validity of such proofs can be checked by a proof assistant like Coq [462]\nor Isabelle/HOL [463,464], to provide a certificate that the proof is correct [356,357].\nFully automatic theorem proving is hard, even for short mathematical proofs. We could also search for\nthe proof, carrying the search-space curse, and exploiting deep neural networks for heuristics. In contrast\nto programs that implement a human-world requirement, mechanical proofs are quite “unnatural”, and\nunderstanding a proof usually requires a careful examination of the proof stack, while code in mainstream\nprogramming languages reads much more like a natural language text. This implies that training deep\nlearning models for creating proofs is considerably more challenging [465,466], and learning to accelerate\nsearch for proof tactics is still in preliminary stage [467].\nWe argue that proofs for verifying software systems have a considerably different structure compared\nwith proofs for mathematical theorems (the focus of today’s research [468]), and the research community\nmay have a paradigm shift: the core of a proof is invariants, which “summarizes” what happens in the\nintermediates of program execution, to form inductive hypotheses for machine-checkable proofs, and we\nidentify strong patterns for program invariants. They can be done by humans, and we see opportunities\nthat human work can be replaced by deep learning, e.g., machine-learning models can rank generated\ninvariants [469].\nAn interesting observation is that we are trying to prove that a program satisfies a specification\nregardless of the input space size and the program execution length. However, the input space can be\nhuge (or even infinite), and the program execution can be lengthy!\nBoth the program and the proof\nseem much more concise compared with the set of all possible program execution traces, and proof\nchecking can be done reasonably efficient. This phenomenon, which connects to the small explanation\nhypothesis, suggests that the program’s execution space (inputs and their corresponding traces) follows a\nsomehow simple structure that can be described algorithmically, and we might avoid a costly exhaustive\nenumeration. We speculate that practical software implementations are of the same magnitude of the\nminimum specification-satisfying implementation, like the “Kolmogorov complexity” of software.\nThe\nimplications of this phenomenon also remain open.\n10.4\nCase Study: Vulnerability Detection\nFollowing the above framework, this section further describes how deep learning techniques can improve\nthe effectiveness and efficiency of finding a specific kind of bugs–security vulnerabilities, which can be\nSci China Inf Sci\n49\nexploited by attackers to gain unauthorized access, perform malicious activities, or steal sensitive data.\n10.4.1\nStatic Vulnerability Detection\nIn modern times, the dominant model for software development revolves around library-based program-\nming. The primary objective is to enhance development efficiency, minimize program complexity, and\nstreamline operations such as development and maintenance. Program documentation plays a crucial role\nin providing a natural language description of the program, aiding users in comprehending and utilizing\nit effectively. Within a code base, an API serves as an interface that enables users to access its various\nfunctions. These APIs subject to certain security constraints, such as manually releasing function return\npointers, among others. These security constraints, known as security protocols, are documented by the\ncode base developers within the program documentation. By documenting these protocols, developers\noffer users of the code base a valuable point of reference and guidance. During a call to the API, the\ndevelopers must comply with the constraints of API calls.\nOtherwise, API misuse can occur, leading\nto serious software security issues, such as NULL pointer dereference, pointer use after free, and logical\nbugs, etc.\nIn recent years, many researchers have used text analysis to find various security problems automat-\nically, including access control configuration errors, wrong access requests, and logic flaws, etc.\nFor\nexample, application developers provide privacy policies and notify users, but users cannot tell whether\nthe application’s natural behavior is consistent with their privacy policies. In response to this problem,\nZimmeck et al. [470] proposed a systematic solution to automate the analysis of privacy policies to de-\ntect inconsistencies between them and application-specific behavior. Tools such as WHYPER [471] and\nAUTOCOG [472] examine whether Android applications correctly describe usage permissions in appli-\ncation descriptions. Similarly, Liu et al. [473] used text categorization and rule-based analysis to test for\nconsistency between standard EU data protection regulations and applicable privacy policies.\nThe approaches above operate under the assumption that the documentation is accurate and do not\ncontain errors. Consequently, if the code contains defects related to an API that lacks documentation,\nthese defects cannot be detected. Cindy et al. [474] examined 52 filesystems and discovered discrepancies\nbetween the error codes returned by functions and those recorded in the documentation. This investi-\ngation revealed over 1,700 undocumented error codes. Tan et al. [475] utilized a series of rule templates\nand a pre-trained decision tree to filter out comments from code that described API usage specifica-\ntions.\nThe program was then analyzed with user-provided function names (e.g., lock/unlock function\nnames) to identify inconsistencies between the comments and the code. TCOMMENT [476] focuses on\nparameter values in Java comments and verifies the consistency of exceptions thrown under those values\nwith the actual types of exceptions thrown by the code.\nWen et al. [477] performed data mining on\n1,500 software code submissions and manually analyzed 500 to classify inconsistencies between code and\ncomments. They also discussed the degree to which a code submission necessitates concurrent modifi-\ncation of comments, guiding for identifying and resolving inconsistencies between code and comments.\nPandita et al. [478] employed machine learning models to filter out sentences in documents that describe\nAPI usage timing. They subsequently employed traditional natural language processing techniques to\ntransform these sentences into first-order logical expressions. They further identified code defects that\ndeviate from these specifications by constructing a semantic diagram of the document statements and\ninferring API usage timing specifications.\nRen et al.\n[479] extracted an API Declaration Graph from\nsemi-structured API declarations and derived usage specifications from the natural language descriptions\nwithin API documents. They then used this information to generate a knowledge graph encompassing\nAPI usage constraints, facilitating the detection of API misuse.\nLv et al. [480] introduced Advance,\nthe first comprehensive API misuse detection tool, employing document analysis and natural language\nprocessing techniques.\nSeveral studies summarize security protocols from many code usage examples for vulnerability detec-\ntion. One intuitive way to do this is to automate the analysis of a large amount of code, then take a\nmajority vote and use the most frequent code used as a reference for API usage. For example, Apisan [481]\nextracts usage patterns from a large amount of code through parameter semantics, causality and then\nextracts API usage references from usage patterns based on a majority vote. Thus, Apisan no longer\nneeds to define defect patterns manually.\nApex [482] finds criteria for the API return value range on\nthis branch based on fewer observations of code branch statements that handle errors and then infers\nthe API’s Error Specification based on the principle that most people are right and diagnosing defects\nSci China Inf Sci\n50\nfor handling code snippets that do not follow the error definition for API return values.\nLike APEx,\nAres [483] uses heuristic rules to identify error-handling blocks of code.\nA majority vote on the entry\ncriteria for these blocks and a range merge results in an API error definition and diagnosing a defect by\nchecking the return value of the API against a check that violates the error definition. Apisan, Apex,\nand Ares all rely on the majority vote, but the majority vote is only sometimes right, which leads to the\nfallacy of the inferred specification itself.\nDeep learning-based large language models (LLMs), represented by the Transformer structure, are\nbeing applied to vulnerability detection tasks, mainly for static code analysis.\nGiven a piece of code\nsnippet, LLMs are asked through a question-answering dialogue whether it detects any vulnerabilities\nin the code and provides an explanation. However, LLMs still cannot handle various types of sensitive\ndetection (including flow-sensitive, domain-sensitive, context-sensitive, etc). Therefore, it is necessary to\nfine-tune the large model or introduce additional knowledge through prompts to guide LLMs to gradually\ncorrect its analysis results. For specific field program vulnerability detection, such as smart contracts\nand shell scripts, due to their short length and low complexity, LLMs usually have a more accurate\nperformance.\n10.4.2\nDynamic Vulnerability Detection\nIn the field of vulnerability detection, fuzzy testing is an efficient dynamic detection technique. It explores\nand detects vulnerabilities in programs by continuously constructing unexpected abnormal data and\nproviding them to the target program for execution while monitoring program execution anomalies [484].\nDuring security testing, a large amount of data can be produced and further employed, such as test cases,\nexecution traces, system states, software implementation specifications, and vulnerability descriptions.\nThis information can be analyzed with deep learning techniques to improve fuzzy testing. For example,\nnatural language processing can be used to understand text descriptions related to vulnerabilities, which\ncan assist in generating test cases [485].\nThrough the strong fitting ability of deep learning models,\nmappings between program inputs and states can be accurately established, which can guide test case\nmutation [390]. Models trained and generated from existing testcases can automatically learn some input\nspecifications to facilitate input generation [486]. At the same time, with the trained model, guidance\nand reasoning can be performed at a relatively low cost, which helps use deep learning techniques in\nreal-time during fuzzy testing.\nAs for the objectives, the application of deep learning in fuzzy testing can be divided into two categories:\nreducing human processing overhead and increasing decision intelligence. The former includes reducing\npreparation work before testing, such as input model inference and mutation operation customization; the\nlatter includes tasks such as seed file scheduling, mutation operation scheduling, and test case filtering.\nResearchers have proposed to use deep learning algorithms to learn a generation model from existing\ntest cases for input model inference or to enhance existing input models by automatically understand-\ning auxiliary information such as input specifications using machine learning algorithms [487,488]. On\nthe other hand, researchers have used machine learning techniques to customize mutation operations for\ndifferent programs. Angora [489] first uses taint analysis technology to obtain input positions that affect\nspecific branches in a program. By converting branch conditions into input functions, Angora uses gra-\ndient descent to mutate corresponding input positions to generate test cases covering specified branches.\nThis adaptive gradient-based mutation operation does not require manual setting and outperforms ran-\ndomly using existing mutation operations in experiments.\nSubsequently, researchers [390–392] further propose to use a single function to fit the input to the\ncorresponding branch coverage of a program, followed by selecting input positions affecting specified\nbranches based on the gradient information of the function and performing targeted mutations accord-\ningly.\nAccording to the Universal Approximation Theorem [490], neural networks have strong function\nfitting capabilities and can approximate any function. Additionally, they have good generalization ability\nand easy calculation of gradients. Therefore, researchers propose to use neural networks as a function\nto fit program behaviors, with the principle of larger gradients indicating greater impact on the corre-\nsponding edge as the basis for the automatic selection of mutation positions and adaptive mutation based\non the size and direction of gradients.\nThese techniques alleviate the cost of expert-designed mutation\noperations to some extent while also having adaptability for different programs.\nDue to the inherent randomness of mutation operations, generated test cases may not meet specific\ntest input generation standards for fuzzy testing. The main cost of a mutation-based fuzzy testing tool\nSci China Inf Sci\n51\nlies in the execution of test cases [491,492]. If deep learning can be used to filter inputs before execution,\nit can reduce unnecessary running costs of target programs and improve the efficiency of fuzzy testing.\nDeep learning-based directed fuzzy testing, such as Neufuzz [390] and Fuzzguard [493], provides a novel\napproach to filtering redundant test cases. This approach collects a large number of test samples and\nuses whether they are reachable on a sensitive path as the classification standard to train deep learning\nmodels to classify and predict the reachability of future test samples. However, one limitation of this\napproach is whether the model can accurately understand the code logic. To overcome this limitation,\nwe need to combine the semantics of the code itself so that the model can correctly understand the logic\nof the code and make accurate judgments about test cases.\n10.5\nDatasets\nConsidering the naturalness and complexity of modern software systems, it is not likely that anyone can\ntrain neural network models from scratch. Therefore, a mainstream approach to bug-finding is embracing\npre-trained models for static analysis, dynamic testing, and formal verification [398,424]. To train domain-\nspecific machine-learning models, e.g., for GUI trace understanding or heurstic decision-making, datasets\nare needed [390,404]. Alternatively, one may randomly select a subset of program execution results to\nserve as training datasets [453]. A unified, large-scale dataset has not been identified, hence it is not\nexplored in this discussion.\n10.6\nChallenges and Opportunities\n10.6.1\nChallenges\nIn pursuing effective bug-finding techniques, the challenges for either static analysis, dynamic testing, or\nformal verification, all point to the specification crisis and the search-space curse. Interestingly, both the\ncrisis and the curse arise from the formal aspect of programs and the algorithmic nature of the bug-finding\ntechniques.\nMachine learning, particularly deep learning techniques, serves as a bridge between the algorithmic\nrealm and the human realm.\nIn short term, even if today’s deep learning models are still superficial\nand fall short on long chain of logical inference, they are extremely good at digesting software artifacts.\nWhenever there is a need for heuristics, deep learning models have potential to perform significantly better\nthan hand-crafted heuristics. The effectiveness of heuristics is multiplicative: among a huge number of\ndecisions, even small improvements may result in magnitudes of significant efficiency improvements.\n• To the probabilistic end, while deep learning models are generally replacing human beings in con-\nducting simple, fast jobs like writing unit tests, the context-length of today’s transformers makes\nit fundamentally limited in understanding large-scale systems–we still need an effective mechanism\nto simplify or reduce large systems such that neural networks can handle various analysis tasks on\nthese systems.\n• To the algorithmic end, we dream one day, a compiler3) is sufficiently powerful to automatically\ngenerate a proof for arbitrary assertions, even in natural language, or provide a counter-example.\nAll programs that compile will automatically be “provably correct” to some extent.\nHowever,\nproviding a prove, particularly for large-scale programs, are far beyond the capability of today’s\nverifiers (model checkers) and automatic theorem provers.\n10.6.2\nOpportunities\nLooking back at the academia’s main theme of bug finding in the past decades, we see the thrive of\nboth fully automatic bug-finding algorithms and end-to-end models.\nThis is partly because for both\nparts we have available benchmarks for the push-button, reproducible evaluation. To go even further,\nperhaps we have overlooked the fact that we have developers, Q/A teams, who are also “probabilistic”\nand whose performance varies day by day in the loop of software development. We could be more open\nto semi-automated techniques that invoke humans, exploit humans, and tolerate biases and errors. Such\n“humans” will eventually be replaced by a deep learning model on the availability of data.\n3) The term “compiler” may no longer be appropriate at that time. Better to call it a “terminator” that kills programmers.\nSci China Inf Sci\n52\n• To the probabilistic end, it remains open and interesting whether there is an effective chain-of-\nthought to draw useful conclusions on software with the ability to understand both the software\nartifacts and analysis results from algorithmic tools.\n• To the algorithmic end, we may hit a balance in the middle: in case the compiler is not powerful\nenough to prove an assertion, compilation will fail and the program should improve the code to\nmake the program easier to verify.\nThe Rust programming language is an early-stage attempt\nfollowing this pathway [494].\n11\nFault Localization\nFault localization (FL) in software engineering is the process of identifying the specific code elements (i.e.,\nstatements or functions) in a program where defects occur [495,496]. Currently, due to the requirement of\noracles, FL techniques mainly focus on functional bugs, which could be found by correctness specifications\nsuch as unit tests.\nTraditionally, FL techniques involve manual debugging techniques [497], such asprint statements, code\ninspection, and step-wise program execution. While these techniques have been widely used, they can\nbe time-consuming, error-prone, and inefficient, particularly in large and complex software systems. To\novercome these limitations, researchers from the software engineering community have explored automated\nFL techniques. These techniques aim to leverage various information sources, such as program execution\ntraces, test cases, and code coverage information, to identify the locations in the code base that are most\nlikely responsible for the observed failures.\nAutomated FL techniques can be broadly categorized into Heuristic FL and Statistical FL approaches.\nHeuristic FL techniques rely on pre-defined heuristic rules to locate the bugs that share similar buggy\nbehavior. For example, some utilize dynamic dependencies (i.e., program slices) [498], some utilize stack\ntraces [499], and some mutate the crisis values during test execution [500]. On the other hand, statistical\nFL techniques [501–503] build statistical models of buggy programs to analyze the relationships between\ncode elements and failures. Most statistical FL techniques utilize program spetra [504–506], a kind of\ncoverage information collected from test execution, to learn a model or a formula and then use it to rank\nthe code elements. Some use other information sources, such as mutation analysis [507,508]. The former\nfamily is called spectrum-based FL (SBFL), while the latter is called mutation-based FL (MBFL).\nRecently, with the rapid development of deep learning, deep-learning-based fault localization (DLFL)\ntechniques have shown the potential to automate and improve the accuracy of fault localization.\nBy\nutilizing neural networks and sophisticated learning algorithms, these approaches can effectively identify\nfault-prone regions of code, prioritize debugging efforts, and accelerate the resolution of software defects.\nIn the following, we divide deep learning-based fault localization into two categories: techniques for\ndirectly enhancing fault localization and techniques for augmenting input data for fault localization.\n11.1\nFault Localization Approaches\nBefore the rise of deep learning, researchers had already attempted to establish a connection between\nthe coverage information of test executions and the test results, in order to predict the location of\nfaulty code. As early as 2011, Wong et al. proposed back-propagation neural network models for defect\nlocalization [509]. Subsequently, Wong et al. made improvements by using a more complex Radial Basis\nFunction network [510].\nTo the best of our knowledge, Zheng et al.\n[511] and Zhou et al. [512] first proposed to adopt deep\nlearning approaches into FL, in 2016 and 2017, respectively. They used a simple full-connection deep\nneural network (DNN) that is trained against the same input of the SBFL. The primary evaluation results\nshow the potential of DNN, which significantly outperforms the DStar approach, which was recognized\nas one of the most effective SBFL techniques at that time.\nDeepFL, the milestone of DLFL, gained huge attention in the field of software engineering [513]. To\naddress the limitations of traditional fault localization techniques, DeepFL utilizes various deep learning\narchitectures, such as MLP, and RNN, to capture different aspects of the software system and learn\nintricate patterns and correlations. By training on labeled data consisting of program execution traces,\ntestcases, and associated fault information, DeepFL is able to make accurate predictions on the likelihood\nof specific code locations being responsible for failures.\nThe complex DNN models enable DeepFL to\nSci China Inf Sci\n53\nhandle multiple kinds of information (including SBFL features, MBFL features, and code-complexity\nfeatures) and complex run-time traces.\nZhou et al.\nproposed CNN-FL [514], which utilizes a tailored CNN model to process data for FL.\nFirst, CNNs are capable of effectively learning local features of the code, leading to more accurate fault\nlocalization. Second, CNN-FL can handle large-scale software systems and exhibits good scalability on\nextensive code repositories. Finally, this approach does not rely on specific feature extraction techniques\nbut rather automatically learns the most relevant features through the network.\nLi et al.\nproposed DEEPRL4FL [515], a fault localization approach for buggy statements/methods.\nDEEPRL4FL exploits the image classification and pattern recognition capability of the CNN to apply\nto the code coverage matrix.\nCNNs are capable of learning the relationships among nearby cells via\na small filter and can recognize the visual characteristic features to discriminate faulty and non-faulty\nstatements/methods.\nTo date, DEEPRL4FL still holds the best performance in terms of the Top-1\nmetric.\nLou et al. proposed GRACE [516], a method-level FL approach based on the graph-neural network\n(GNN). GRACE represents aprogram by a graph, where nodes represent code elements or tests, and edges\nrepresent coverage relationships or code structures. By leveraging the power of graph representations and\nlearning latent features, the approach enhances fault localization accuracy and helps identify faulty code\nlocations more effectively.\nQian et al. utilized graph convolutional neural networks (GCN) to improve localization accuracy and\nproposed AGFL [517].\nAGFL represents abstract syntax trees by adjacent matrix and program tokens\nby word2vec, and then combines these features to further train GCN models. AGFL applies attention\nand GCN to classify whether an AST node is buggy.\nQian et al.\nproposed GNet4FL, which is based on the GCN [518].\nTo improve the performance,\nGNet4FL collects both static features based on code structure and dynamic features based on test results.\nIt utilizes GraphSAGE to generate node representations for source codes and performs feature fusion\nfor entities consisting of multiple nodes, preserving the graph’s topological information.\nThe entity\nrepresentations are then fed into a multi-layer perceptron for training and ranking.\nZhang et al.\nproposed CAN, a context-aware FL approach based on GNN [519].\nCAN represents\nthe failure context by constructing a program dependency graph, which shows how a set of statements\ninteract with each other. Then, CAN utilizes GNNs to analyze and incorporate the context ( e.g., the\ndependencies among the statements) into suspiciousness evaluation.\nLi et al.\nproposed FixLocator [520], a DLFL approach that can locate faulty statements in one or\nmultiple methods that need to be modified accordingly in the same fix. FixLocator utilizes dual-task\nlearning with a method-level model and a statement-level model.\nSimilarly, Dutta et al.\ndesigned a\nhierarchical FL approach that uses two three-layer DNNs to first localize a function and then localize a\nstatement [521].\nYu et al. proposed CBCFL, a context-based cluster approach that aims to alleviate the influence of\ncoincidental correctness (CC) tests [522].\nCBCFL uses the failure context, which includes statements\nthat affect the output of failing tests, as input for cluster analysis to improve CC test identification. By\nchanging the labels of CC tests to failing tests, CBCFL incorporates this context into fault localization\ntechniques.\nLi et al. proposed a two-phase FL approach based on GNN [523]. It extracts information from both\nthe control flow graph and the data flow graph via GNN. The localization process is divided into two\nphases: (1) computing the suspiciousness score of each method and generating a ranking list, and (2)\nhighlighting potential faulty locations inside a method using a fine-grained GNN.\nYosofvandet al. proposed to treat the FL problem as anode classification problem, where the Gumtree\nalgorithm is used to label nodes in graphs comparing buggy and fixed code [524].\nThis paper uses\nGraphSAGE, a GNN model that handles big graphs with big neighborhoods well.\nWu et al. proposed GMBFL which improves MBFL via GNN [525]. Existing GNN-based approaches\nmainly focus on SBFL, while GMBFL first represents mutants and tests by a graph. The nodes of a graph\nare code elements, mutants, and tests. The edges are the mutation relationship between a code element\nand a mutant, the killed relationship between a mutant and a test, and the code structural relationship\nbetween two code elements of different hierarchies. Then GMBFL trains a Gated Graph Attention Neural\nNetwork model to learn useful features from the graph.\nIn addition to test-based fault localization methods, there are also approaches that localize the bugs\nfrom change sets. BugPecker [526] is the first to encode the commits and bug reports into revision graphs.\nSci China Inf Sci\n54\nCiborowska et al. proposed to fine-tune the BERT model for locating the buggy change set [527].\nTo evaluate and compare the performance of different DNN models, Zhang et al. processed a large-\nscale empirical study [528], which involves CNN, RNN, and multi-layer perceptron. The evaluation results\nshow that CNNs perform the best in terms of identifying real faults.\n11.2\nData Augmentation and Data Processing Approaches for Fault Localization\nData augmentation refers to the technique of artificially increasing the size and diversity of a dataset\nby applying various transformations or modifications to the original data.\nIt plays a crucial role in\nimproving the performance and robustness of deep learning models. In the context of fault localization,\ndata augmentation can be used to enhance the effectiveness of deep learning-based approaches to fault\nlocalization.\nZhong and Mei proposed CLAFA [529], which employs word embedding techniques to process the\nnames within code. Then it compares program dependency graphs from buggy and fixed code to locate\nbuggy nodes and extracts various graph features for training a classifier.\nZhang et al. address the data imbalance problem in FL [530], which is caused by the fact that the\nnumber of failing test cases is much smaller than that of passing test cases. This paper employs test\ncase resampling to representative localization models using deep learning, and improves the accuracy of\nDLFL.\nXie et al. proposed Aeneas [531], which employs a revised principal component analysis (PCA) to\ngenerate a reduced feature space, resulting in a more concise representation of the original coverage matrix.\nThis reduction in dimensionality not only improves the efficiency of data synthesis but also addresses the\nissue of imbalanced data. Aeneas tackles the imbalanced data problem by generating synthesized failing\ntest cases using a conditional variational autoencoder (CVAE) from the reduced feature space.\nHu et al.\nproposed Lamont, which uses revised linear discriminant analysis (LDA) to reduce the\ndimensionality of the original coverage matrix and leverages synthetic minority over-sampling (SMOTE)\nto generate the synthesized failing tests [532].\nLei et al. proposed BCL-FL [533], a data augmentation approach based on between-class learning. By\nleveraging the characteristics of real failed test cases, BCL-FL produces synthesized samples that closely\nresemble real test cases. The mixing ratio of original labels is used to assign a continuous value between\n0 and 1 to the synthesized samples. This ensures a balanced input dataset for FL techniques.\nLei et al. proposed CGAN4FL [534], a data augmentation approach to address the data imbalance\nproblem in FL. CGAN4FL employs program dependencies to create a context that exposes the root causes\nof failures. Subsequently, CGAN4FL harnesses a generative adversarial network (GAN) to examine this\nfailure-inducing context and generate test cases that belong to the minority class (i.e., failing test cases).\nUltimately, CGAN4FL integrates the synthesized data into the existing test cases to obtain a balanced\ndataset suitable for FL.\nZhang et al. proposed UNITE [535], which utilizes context information of trace data. UNITE combines\nthree sources of information (i.e., a statement, a test case, and all test cases of a test suite), and then\ncomputes the influence relationship of the statements by program dependencies. In evaluation, UNITE\nsignificantly improves the state-of-the-art DLFL approaches.\nAlso, researchers try to improve DLFL from other aspects.\nTian et al.\nproposed to use DNNs to\nextract deep semantic changes to construct better mutants to improve FL performance\n[536]. Zhang et\nal. proposed to synthesize failing tests to improve the performance of DLFL [537].\n11.3\nEvaluation Metrics\nEvaluation metrics play a crucial role in assessing the performance of FL techniques. Similar to traditional\nFL approaches, DLFL studies adopt the commonly used metrics, shown as follows:\n1. Top-N. This metric measures the number of the model in identifying the correct faulty code element\nwithin the top-N ranked elements.\n2. MAR. MAR (Mean Average Rank) is the mean of the average rank.\n3. MFR. MFR (Mean First Rank) is the mean of the first faulty statement’s rank of all faults using\na localization approach.\nSci China Inf Sci\n55\nTable 15\nThe Frequently Used Evaluation Metrics.\nMetric Name\nUsed Times\nReference\nTop-N\n12\n[515] [513] [516] [517] [518] [520] [522] [526] [527] [534] [535] [525]\nMAR(Mean Average Rank)\n8\n[515] [513] [516] [517] [518] [534] [535] [525]\nMFR(Mean First Rank)\n8\n[515] [513] [516] [522] [534] [535] [523] [525]\nEXAM\n6\n[509] [510] [511] [512] [514] [521] [523]\nRImp(Relative Improvement)\n6\n[512] [514] [519] [522] [534] [535]\nMAP(Mean Average Precision)\n2\n[526] [527]\nMRR(Mean Reciprocal Rank)\n2\n[526] [527]\nHit-N(multi-defects)\n1\n[520]\n4. EXAM. EXAM stands for Expected Maximum Fault Localization, which measures the expected\nrank of the first correct fault location in a ranked list of code elements.\n5. RImp. RImp (Relative Improvement) is to compare the total number of statements that need to\nbe examined to find all faults using one FL approach versus the number that need to be examined\nby without using the FL approach.\n6. MAP. MAP (Mean Average Rank) first computes the average precision for each fault, then calcu-\nlates the mean of the average precision.\n7. MRR. MRR (Mean Reciprocal Rank) metric calculates the mean of the reciprocal position at\nwhich the first relevant method is found.\n8. Hit-N. Hit-N is a metric designed for multi-defects, which measures the number of bugs that the\npredicted set contains at least N faulty statements.\nTable 15 summarizes the commonly used metrics. The most popular metrics are Top-N and EXAM,\nwhich are used eight times and six times, respectively. RImp, MFR, and MAR are used four times, which\nare less common. MAP and MRR are used in only two studies, while the Hit-N metric is used only once\nto measure the multi-defect FL approaches.\n11.4\nDatasets\nThe datasets used in the DLFL field are largely similar to those in the program repair and other related\nfields. Current approaches primarily utilize Java programs for evaluation, with a smaller portion using\nC language programs.\nIn Java, the Defects4J [538] benchmark is the most extensively used. Defects4J is a widely recognized\nbenchmark dataset in the FL field. This dataset is well-maintained and continues to be updated. Early FL\ntechniques uses Defects4J v1.2, primarily evaluating against six projects within it. Recent work employs\nDefects4J v2.0, which includes more open-source projects. Additionally, the BEARS [539] benchmark\nand nanoxml from the SIR [540] dataset are also employed.\nIn C, some C programs from the SIR dataset, as well as the ManyBugs [541] dataset, are more frequently\nused. Existing DLFL research has utilized programs like space from the SIR dataset, as well as python,\ngzip, libtiff, and others from the ManyBugs dataset.\n11.5\nChallenges and Opportunities\nThis section summarizes the challenges and highlights opportunities for future work in fault localization.\n11.5.1\nChallenges\nDeep learning-based fault localization techniques have shown great potential in improving the accuracy\nand effectiveness of fault localization. However, they also face the following challenges that need to be\naddressed to fully exploit their capabilities.\nSci China Inf Sci\n56\n• The risk of overfitting.\nThe current existing DLFL approaches are mainly evaluated on the\npopular benchmark Defects4J [538], which consists of Java projects and hundreds of bugs and is used\nas an important dataset in debugging-related fields. Most papers use Defects4J v1.2, which only\ncontains 395 bugs from six Java projects. Worse still, some approaches discard the Closure project\nand only use 224 of the bugs. This leads to the risk that the conclusions of most existing methods\nmight be overfitting to this particular dataset or the Java programming language. Currently, there\nexist a few studies that target Python or JavaScript programs and we suggest evaluating the novel\napproaches across multiple languages and benchmarks.\n• Inadequate availability of high-quality labeled data. The current research is limited to the\nDefects4J dataset due to a shortage of high-quality labeled data.\nThis scarcity not only affects\nthe evaluation of DLFL approaches but also impacts the training of deep learning models.\nIn\naddition, existing FL data suffer from imbalance, i.e., the data from passing tests overwhelms the\ndata from failing tests. This characteristic poses challenges for many learning-based approaches.\nDeep learning models require a large amount of accurately labeled data for training, which can be\ndifficult to obtain, especially when dealing with real-world bugs across different languages.\n• Interpretability of deep learning models. The lack of interpretability has long been a challenge\nfor deep learning and similarly affects fault localization based on deep learning, which makes it\ndifficult to analyze the results of fault localization, specifically the relationship between failing\ntests, the code elements, and the traces.\n• Occasionally low efficiency. The current fault localization systems struggle to achieve real-time\nfault localization. This is partly due to their reliance on test runs to collect trace information and\npartly because large-scale deep-learning models could slow training and prediction.\n11.5.2\nOpportunities\nDespite these challenges, deep learning-based fault localization techniques present the following promising\nopportunities.\n• Data augmentation. As discussed in current challenges, existing DLFL approaches are suffering\nfrom low-quality data that are imbalanced or of limited scale. Thus data augmentation methods\noffer avenues for addressing the challenge.\n• Enhancing multiple FL. Existing DLFL approaches generally assume that there is only one\nbug in the target project. However, a real-world buggy project often contains multiple bugs. The\ninteraction between these bugs makes it more difficult to train and predict using traditional learning\napproaches, highlighting potential opportunities for improvement in this direction.\n• The integration of domain-specific knowledge.\nFL relies on code syntax structures, code\nsemantics, and execution traces, which require a deep understanding of the underlying programming\nlanguages and software engineering principles. The integration of domain-specific knowledge, such\nas code smells, development experience, and debugging heuristics, can significantly improve the\neffectiveness of FL systems. Especially, representing and learning context information of the buggy\ncode is promising, because it enables the models to better understand the specific scenarios in which\nfaults occur and how they relate to the text execution.\n12\nProgram Repair\nProgram repair refers to the process of identifying and fixing software defects in programs. Program repair\nrequires a large number of time costs and human resources from the project development team [542]. Due\nto the growing demand for efficient software maintenance, automatic program repair techniques have\nemerged as a solution [543].\nAutomatic program repair allows developers to automate (or nearly automate) defect detection and\ncorrection.\nThis makes program repair efficient, reliable, and cost-effective [544]. In recent years, the\ndevelopment and implementation of automatic program repair techniques have been widely recognized\nby the software development community [545]. The success of deep learning in recent years makes it a\nSci China Inf Sci\n57\npromising approach for locating and repairing buggy programs. The family of deep learning approaches\nto program repair develops and applies deep learning techniques to identify software defects and generate\npatches [546]. Deep learning models have been widely applied to repair a wide range of program errors.\nWe divide existing work into three categories, including compilation error repair, runtime error repair,\nand specific domain error repair.\n12.1\nCompilation Error Repair\nA compilation error is an error that can be detected during compilation.\nPrograms with compilation\nerrors fail in program compilation or linking; meanwhile, programs with compilation errors cannot be\ndirectly handled by program analysis tools [547–549].\nCompilation errors prevent source code from being transformed into executable machine code during\ncompilation. These compilation errors contain issues like type errors, undefined variables, syntax errors,\nand other errors that violate programming language standards. Studies in the early stage have proposed\nerror-correcting parsers to achieve program repair of faulty code [550–553]. Recently, researchers have\nexplored the advantage of deep learning techniques for automatically fixing compilation errors. These\ntechniques use deep neural networks to analyze extensive repair datasets and recommend precise code\nfixes [545].\nTo avoid misleading messages returned by compilers, Gupta et al. [554] trained a deep neural network,\nnamed DeepFix, to identify incorrect locations in source code and provide the corresponding repaired\nstatements. They collected 6,971 erroneous C programs from code written by students for 93 programming\ntasks and found that DeepFix is able to completely repair 27% and partially repair 19% of these erroneous\nprograms. Bhatia et al. [555] presented RLAssist, a technique that combines recurrent neural networks\n(RNNs) with constraint-based reasoning to fix programming assignments with syntax errors.\nAhmed\net al. [556] proposed an end-to-end system, called Tracer, for fixing code with multiple errors. In the\nsame year of 2018, Santoset al. [557] proposed an approach to correcting syntax errors using n-gram and\nLSTM models. They evaluated the approach on the BlackBox dataset [558].\nMesbah etal. [559] proposed Deepdelta, which converts an Abstract Syntax Tree (AST) into a domain-\nspecific language before feeding the converted tree into a Neural Machine Translation (NMT) network.\nDeepdelta achieves a success rate of 50% in generating correct repairs for missing symbols and mismatched\nmethod signatures. Gupta et al. [560] proposed a programming language correction framework that uses\nreinforcement learning to assist novice programmers with syntactic errors. This framework outperforms\ntheir previous work DeepFix [554] in 2017. Wu et al. [561] trained a deep learning model on the DeepFix\ndataset. The model, called graph-based grammar fix, combines token sequences and graph structures\nbased on ASTs to predict the error position and generate correct tokens. To integrate program-feedback\ngraphs and a self-supervised learning framework, Yasunaga et al. [562] proposed DrRepair, a program\nrepair technique based on graph attention networks.\nWith the rapid development of deep learning, the effectiveness of automatic program repair has been\nfurther improved.\nHajipour et al. [563] proposed a generative model for fixing compilation errors in\nC programs.\nTheir model learned a distribution over potential fixes and encourages diversity over a\nsemantic embedding space.\nAllamanis et al. [372] proposed to train a selector concurrently with the\nmodel that locates and fixes errors in source code. The selector was utilized to automatically generate\nfaulty code, which is used to enrich the training set for the original model.\nTo make the generated\nfaulty code closely resemble real-world error scenarios, Yasunaga et al. [564] proposed BIFI, an iterative\ntraining approach for fixing syntax errors. They trained two models, including the breaker and the fixer,\nin an iterative manner. The breaker creates faulty code that closely resembles real-world errors while the\nfixer converts the faulty code into the correct version. Ahmed et al. [565] presented a lenient parser for\nimperfect code (i.e., the union of fragmentary code, incomplete code, and ill-formed code) and proposed\nan indirectly supervised approach for training the parser.\nTo fix the parser errors in programming\nlanguages, Sakkas et al. [566] proposed a language-agnostic neurosymbolic technique in 2022. Their\ntechnique, called Seq2Parse, combined symbolic error correcting parsers and neural networks. Seq2Parse\nwas evaluated on 1.1 million Python programs. Li et al. [567] proposed TransRepair, which utilizes a\nTransformer-based neural network via considering both the context of erroneous code and the feedback by\ncompilers to fix compilation errors in C programs. Ahmed et al. [568] introduced SynShine,a three-stage\napproach that combines the feedback of Java compiler with models based on a Robustly Optimized BERT\n(Roberta) [569]. They indicated that SynShine achieves 75% of effectiveness in fixing the single-line errors\nSci China Inf Sci\n58\nof the code in the Blackbox dataset [558].\n12.2\nRuntime Error Repair\nRuntime errors, also known as dynamic errors, occur when the program executes. Runtime errors result in\ncrashes or incorrect behaviors during program execution. Automatic program repair techniques generate\npatches for faulty code based on test cases, crashes, references, contracts, etc [570].\nApproaches to runtime error repair can help save time costs and effort in software development. These\nrepair approaches can be roughly divided into search-based repair [571,572], constraint-based repair [573–\n575], template-based repair [576–578], and learning-based repair [579–581]. With the support by deep\nneural networks, learning-based repair approaches can generate high-quality code patches. The framework\nof repair approaches based on deep learning for runtime errors generally consist of five steps: fault location,\ndata pre-processing, feature extraction, patch generation, and patch selection.\nLong et al. [582] proposed a hybrid repair system, called Prophet. Prophet integrates a probabilistic\nmodel trained on the benchmark presented by Goues et al. [583] and ranks code patches to fix runtime\nerrors. Tufano et al. [584] investigated the possibility of learning patches through the translation model\nbased on neural networks.\nTheir model achieves a prediction rate of 9% via training on the GitHub\nrepositories.4) Sun et al. [585] developed a sequence-to-sequence service based on the attention techniques.\nWhite et al. [579] presented a deep learning model, DeepRepair, to produce patches that cannot be\nsearched by redundancy-based techniques. Tufano et al. [237] explored the potential of an NMT model\nto create code changes made by developers during adding pull requests.\nResearchers have been exploring new ways of tackling issues in program repair. Ding et al. [586] con-\nducted a study that investigates the differences between sequence-to-sequence models and translation\nmodels for program repair. They proposed a strategy based on the empirical findings and development\nknowledge in patch generation. Yanget al. [587] proposed an automatic model to locate faults and gener-\nate patches. They scored the ranks between bug reports and source code based on Convolutional Neural\nNetworks (CNNs) and auto-encoder. Then, they created patches through the Seq-GAN algorithm [588].\nLutellier et al. [589] proposed CoCoNuT, which leverages the strength of CNNs and NMTs to achieve\nmulti-language repair for Java, C, Python, and JavaScript programs. Li et al. [21] proposed DLFix, a\ndual-level deep learning model designed to address the limitations of learning-based program repair. The\nfirst layer of DLFix is a tree-based RNN model that captures the context of fixed code while the second\nlayer utilizes the output from the first layer to learn and apply code patches. The validation experiments\nare conducted on the datasets of Defects4J 5) [590] and Bugs.jar 6) [591]. Tian et al. [592] explored differ-\nent deep learning-based approaches. Their experimental results show that embeddings from pre-trained\nand re-trained neural networks are beneficial to reason and generate correct patches. Dinella et al. [593]\nproposed Hoppity, a Javascript-targeted automatic repair model. This learning-based model focuses on\ngraph structures of faulty code.\nTang et al. [594] proposed a grammar-based approach to syntax correct patch generation. Huang et\nal. [595] discussed the use of a pyramid encoder in seq2seq models to reduce computational and memory\ncosts while achieving a similar repair rate to their non-pyramid counterparts. Their study focuses on\nautomatic correction of logic errors. Jiang et al. [581] presented a three-stage based NMT model, called\nCURE. They first pre-trained a programming language model to incorporate the real-world coding style\nand then proposed a technique to expand the search space.\nThey integrated subword tokenization,\na technique used in natural language processing that splits words into smaller units called subword\ntokens, to generate precise patches. Based on the Long Short-Term Memory (LSTM) network and the\nbidirectional recurrent neural network, Rahman et al. [596] presented BiLSTM to identify and classify the\nfaulty code and generate possible fixes. Chen et al. [580] proposed SequenceR, a sequence-to-sequence-\nbased deep learning system. The model adapts a copy mechanism to deal with large-scale code instances.\nBerabi et al. [597] proposed TFix, a pre-trained and fine-tuned language model that generates improved\npatches for JavaScript programs. Tang et al. [598] proposed a Graph-to-Sequence learning model called\nGrasP, based on code structure. Szalontai et al. [599] focused on the uncommon parts of Python code.\nThey constructed a neural network model to classify and generate replacement code snippets.\n4) https://github.com/.\n5) https://github.com/rjust/defects4j.\n6) https://github.com/bugs-dot-jar/bugs-dot-jar.\nSci China Inf Sci\n59\nLi etal. [600] proposed DEAR, a deep learning-based repair approach, which attempts to locate multi-\nbugs and generate patches to fix multiple errors in a single program. Xu et al. [601] presented M3V,\nan approach that integrates LSTM and GNN models for fault location and patch generation.\nTheir\napproach primarily focuses on null-pointer exceptions and out-of-bounds exceptions in Java programs.\nMeng et al. [602] introduced Transfer, a technique that mines deep semantic information. Their model\nintegrates multiple features to rank patches, including semantic-based features, spectrum-based features,\nand mutation-based features. Kim etal. [603] focused on locations of software defects. They improved the\naccuracy of deep learning approaches in program repair via using genetic algorithms to obtain the precise\ndefective code. Wardat et al. [604] proposed DeepDiagnosis, which focuses on defects in deep learning\napplications. Using well trained models, DeepDiagnosis classifies faults into eight categories and provides\nfeasible patches for each category of faults. Yao et al. [605] proposed Bug-Transformer, a context-based\ndeep learning model. Bug-Transformer retains the contextual information of the faulty code and uses a\ntransformer model for training. Yanet al. [606] proposed CREX, a transfer-learning-based technique [607]\nfor validating C program patch correctness. Their model aims to learn the code semantic similarity to\nimprove the accuracy validation. Chakraborty et al. [608] proposed CODIT, a tree-based deep learning\nmodel for generating code change suggestions. Ye et al. [609] proposed RewardRepair, an NMT-based\nmodel with fine-tuned loss functions. RewardRepair incorporates compilation information and test cases\ninto the calculation of the loss functions. Ye et al. [610] also proposed ODS, a deep learning system for\npredicting patch correctness. They extracted code features in ASTs from patches and buggy code. Then,\nthey employed supervised learning to determine the correctness of patches. Another technique proposed\nby Yeetal. [611] is SelfAPR, a self-supervised model based on employing perturbation-generated data for\npatch generation. Xia et al. [612] proposed AlphaRepair, a CodeBERT based model for code generation.\nAlphaRepair uses zero-shot learning techniques rather than training models with historical erroneous\nand corrected code. Kim etal. [613] investigated the efficacy of deep learning-based repair techniques for\nJava-to-Kotlin conversion programs. Their technique enhances the defect fixing performance by applying\ntransfer learning techniques. Tian et al. [614] proposed BATS, a learning-based model designed to predict\nthe correctness of patches. BATS is an unsupervised model that repairs faulty programs by detecting\nprogram behavior during failed testcases. Yuan et al. [615] proposed CIRCLE, a T5-based model for patch\ngeneration. CIRCLE employs continual learning techniques and is able to work on multiple programming\nlanguages. They also designed the Prompt feature to make it capable of understanding natural language\ncommands. Chen et al. [616] trained an iterative model, which aims to learn from generated patches and\ntest execution.\n12.3\nSpecific Domain Error Repair\nDeep learning has also been applied to domain-specific tasks related to automated program repair. The\nspecific domain repair refers to the application of specialized knowledge or techniques from a specific\ndomain to improve the effectiveness and efficiency of program repair.\nTest repair aims at fixing errors in test cases that are unusable due to software updates. Stocco et\nal. [617] fixed web test cases by analyzing visual features based on an image processing approach. Panet\nal. [618] presented Meter, a computer vision based technique for fixing test cases in the Graphical User\nInterface (GUI) of mobile applications.\nBuild scripts are crucial components in the automatic building of software systems. Program repair\nfor build scripts refers to the process of automatically detecting and fixing faults in build scripts [619].\nHassan et al. [620] built a benchmark of 175 build failures and the relevant remedies for Gradle.7) They\npresented HireBuild as the first model of patch generation for fixing build scripts in Gradle. Based on\nthe previous work of HireBuild, Lou et al. [621] extended the benchmark from Top-1000 GitHub projects\nand proposed an improved search model, which generates patches according to current test projects and\nexternal resources. Recently, Loriot et al. [622] proposed STYLER, an approach based on the LSTM\nneural network to generate patches for code violations against format rules.\nSoftware vulnerability refers to security weaknesses that can compromise the integrity and availability\nof software systems.Ma et al. [623] developed a tool called VuRLE that automatically locates and fixes\nvulnerabilities in source code. Their tool generates repair templates and selectes patches based on the\nASTs of source code. Harer et al. [624] proposed a technique based on Generative Adversarial Networks\n(GANs) that generates corrupted data and uses correct-incorrect pairs to train an NMT model. Zhou et\n7) Gradle, https://gradle.org/\nSci China Inf Sci\n60\nTable 16\nEvaluation Datasets for Deep Learning Based Program Repair Tools\nYear\nDataset\nLanguage\nType\nSize\nURL\n2014\nDefect4J\nJava\nRuntime Error\n835\nhttps://github.com/rjust/defecrrorts4j\n2014\nBlackbox\nJava\nHybrid Error\nOver 2,000,000,000\nhttp://www.cs.kent.ac.uk/ ~ nccb/blackbox\n2016\nIntroClassJava\nJava\nRuntime Error\n297\nhttps://github.com/Spirals-Team/IntroClassJava\n2017\nDroixBench\nJava\nHybrid Error\n24\nhttps://droix2017.github.io\n2018\nBugs.jar\nJava\nRuntime Error\n1,158\nhttps://github.com/bugs-dot-jar/bugs-dot-jar\n2018\nSantos et al.\nJava\nCompilation Error\n1,715,313\nhttps://archive.org/details/sensibility-saner2018\n2019\nBugSwarm\nJava\nBuild Failure\n3,091\nhttps://github.com/BugSwarm/bugswarm\n2019\nPonta\nJava\nVulnerability\n1,068\nhttps://github.com/SAP/project-kb\n2022\nVul4J\nJava\nVulnerability\n79\nhttps://github.com/tuhh-softsec/vul4j\n2015\nManyBugs\nC\nHybrid Error\n185\nhttps://repairbenchmarks.cs.umass.edu\n2015\nIntroClass\nC\nRuntime Error\n998\nhttps://repairbenchmarks.cs.umass.edu\n2016\nPrutor\nC\nCompilation Error\n6,971\nhttps://www.cse.iitk.ac.in/users/karkare/prutor\n2017\nDBGBENCH\nC\nRuntime Error\n27\nhttps://dbgbench.github.io\n2017\nCodeFlaws\nC\nHybrid Error\n3,902\nhttps://codeflaws.github.io\n2018\nTRACER\nC\nCompilation Error\n16,985\nhttps://github.com/umairzahmed/tracer\n2019\nTEGCER\nC\nCompilation Error\n15,579\nhttps://github.com/umairzahmed/tegcer\n2020\nBig-Vul\nC/C++\nVulnerability\n3,754\nhttps://github.com/ZeoVan/MSR-20-Code-vulnerability-CSV-Dataset\n2021\nCVEfixes\nC/C++\nVulnerability\n5,495\nhttps://github.com/secureIT-project/CVEfixes\n2021\nBugsCpp\nC/C++\nRuntime Error\n215\nhttps://github.com/Suresoft-GLaDOS/bugscp\n2017\nQuixBugs\nJava/Python\nRuntime Error\n40\nhttps://github.com/jkoppel/Quixbugs\n2017\nHireBuild\nBuild Script\nBuild Failure\n175\nhttps://sites.google.com/site/buildfix2017\n2019\nBugsJS\nJavaScript\nRuntime Error\n453\nhttps://bugsjs.github.io\n2019\nDefexts\nKotlin/Groovy/etc.\nRuntime Error\n654\nhttps://github.com/ProdigyXable/defexts\n2019\nRefactory\nPython\nHybrid Error\n1783\nhttps://github.com/githubhuyang/refactory\n2020\nTANDEM\nJava/C/SQL/etc.\nHybrid Error\n125\nhttps://github.com/belene/tandem\n2022\nRing\n6 Languages\nHybrid Error\n1,200\nhttps://github.com/microsoft/prose-benchmarks\n2022\nCrossVul\n40 Languages\nVulnerability\n5,131\nhttps://zenodo.org/record/4734050\nal. [625] proposed SPVF, which combines ASTs, security properties, and the attention mechanism into\nan integrated neural network for both C/C++ and Python programs.\nHuang et al. [626] attempted to\nfix vulnerabilities by leveraging pre-trained large language models. They reported an accuracy rate of\n95.47% for single-line errors and 90.06% for multiple-line errors. Chen et al. [627] hypothesized that there\ncould be a correlation between program repairing and vulnerability fixing. They proposed VRepair, a\ntransfer learning model designed to solve security vulnerabilities in C programs with limited data. Due\nto the increasing number of reported vulnerabilities, Chi et al. [628] developed SeqTrans, an NMT-based\ntool that automatically fixed vulnerabilities.\nTheir approach involves learning from historical patches\nand contextual features of source code.\n12.4\nDatasets\nThe diverse range of programming languages leads to the creation of various types of datasets for program\nrepair. In light of existing research, there are numerous available datasets that are specifically tailored\nfor the application of automatic program repair tools.\nThe evaluation datasets are listed in Table 16. We divide the datasets into three categories according\nto the programming languages:\nin Java, in C/C++, and in other programming languages.\nWe briefly\nintroduce typical datasets as follows. Prutor [629] is a tutorial system, which helps students solve pro-\ngramming problems.\nPrutor is used in the introductory programming course in IIT Kanpur.\nThus,\nPrutor collects many pieces of C code, including the buggy code and the correct code.\nBlackbox [558] is a project since 2013. It collects data from the BlueJ IDE,8) a tutorial environment\nfor JAVA learners. After five years of collection, the Blackbox dataset has amassed over two terabytes\nof data [630].\nIn 2018, Santos et al. [557] refined the Blackbox dataset to evaluate their Sensibility\napproach. They selected 1,715,312 program pairs of previous and current versions from the Blackbox\ndataset, including 57.39% pairs with one syntax error and 14.48% with two syntax errors.\nDefects4J [631] is one of the most widely-used benchmark in program repair [589,609,632,633]. The\nlatest version of Defects4J is a collection of 835 reproducible bugs from 17 open-source Java projects.\nEach bug in Defects4J corresponds to a set of test cases that can trigger the bugs. GrowingBugs [634] is\n8) BlueJ IDE, https://www.bluej.org/\nSci China Inf Sci\n61\nhighly similar to Defects4J in that bug-irrelevant changes in bug-fixing commits have been excluded from\nthe patches. The current version of GrowingBugs contains 1,008 real-world bugs collected from open-\nsource applications. The only difference between GrowingBugs and Defects4J is that the latter excludes\nbug-irrelevant changes from bug-fixing commits manually, whereas the former does it automatically by\nBugBuilder [635,636].\nVul4J [637] is a dataset of reproducible Java vulnerabilities.\nAll Vulnerabilities in Vul4J correspond\nto human patches and Proof-of-Vulnerability (POV) test cases. CrossVul [638] contains vulnerabilities\nover 40 programming languages. Each file is corresponding to an ID of Common Vulnerability Exposures\n(CVEs) and its source repository.\nThis dataset also contains commit messages, which may serve as\nhuman-written patches.\n12.5\nChallenges and Opportunities\nWe present challenges and opportunities in deep learning for program repair in this section.\n12.5.1\nChallenges\nThere are several challenges in deep learning for program repair. These challenges reveal that there is\nstill a long way to go in applying deep learning techniques to automatic program repair.\nTraining data. Deep learning requires a considerable number of data to train learnable models. In\nprogram repair, labeled data of buggy code and patches are limited. Obtaining high-quality erroneous\nand patched code is a severe challenge due to the limited datasets of program repair. Another challenge\nis to determine how to use buggy code and patches in model training. A deep learning model can involve\nthe location of buggy code, its specific type, its context, its syntax, and semantics. The mapping between\nbuggy code and patched code is a key step in training models in program repair. To date, there is no\ntheoretical analysis for such a challenge.\nModel interpretability. The lack of interpretability for deep learning models makes it difficult to\nensure the correctness of the generated patches. The original goal of program repair is to assist the real-\nworld developers. Thus, it may be difficult to persuade developers to use deep learning-based program\nrepair in real-world development.\nSelf-validation of data. Models of deep learning are built on training data.\nIf the training data\ncontains errors, these errors may propagate to the generated patches.\nThis hurts the effectiveness of\nautomatic program repair. Developers are unable to confirm the reliability of patches generated by deep\nlearning.\n12.5.2\nOpportunities\nDespite the challenges, we identify the following opportunities in the field of deep learning for program\nrepair.\nData collection and generation. Automatic data collection and generation can benefit program\nrepair. Researchers can train deep learning models to handle a wide range of programming scenarios\nbased on high-quality data. Additionally, data generation methods such as program synthesis may be\nable to enrich the existing data.\nHybrid approaches. Hybrid approaches that combine deep learning with other existing techniques,\nlike symbolic reasoning, template-based repair, and rule-based search, have shown promising results in\nprogram repair. These hybrid approaches can improve the effectiveness and efficiency of current program\nrepair tools by integrating the strength of multiple techniques.\nModel optimization.\nResearchers are able to explore ways to optimize deep learning models for\nprogram repair. Such optimization contains model re-sizing knowledge distillation, neural architecture\nsearch, and so on. Model optimization can help researchers transfer available knowledge from a large and\ncomplex model to new-coming or unknown scenarios.\nIn conclusion, the opportunities in deep learning for program repair are vast and exciting. Continuous\nresearch and development in this field may lead to more advanced and effective approaches for program\nrepair.\nSci China Inf Sci\n62\nBug 170801 - Converting image from grayscale to black&white is\npainfully slow\nStatus:\nProduct:\nComponent:\nVersion:\nHardware:\nRESOLVED\nFIXED\nGIMP\nGeneral\n2.2.x\nOther All\nReported:\nModified:\nCC List:\nSee Also:\n2005-03-18 14:46 UTC\nby Xuan Baldauf\n2008-01-15 12:50 UTC\n(History)\n1 users (show)\nImportance: Normal normal\nGIMP Bugs\nTo:\nXuan Baldauf 2005-03-18 14:46:31 UTC\nDescription\n1. Open a large grayscale image of your choice (e.g. ….\n2. Use “Tools/Color Tools/Threshold” to apply some threshold choosen.\n3. Now you have a 8bit grayscale image, which acturally consists only of color values “0”\nand color values “255”. ….\nThis slow speed is not acceptable for interactive image processing, and this slowness\nis not nessary at all.\n……………\nManish Singh 2005-03-19 17:48:16 UTC\nRevision 1.156. ….\nif (palette_type == GIMP_WEB_PALETTE ||\npalette_type == GIMP_MONO_PALETTE ||\n……………\nFigure 6\nAn example of bug report and its lifecycle.\n13\nBug Report Management\nGiven the intricate nature of software systems, bugs are unavoidable. A previous study shows that a\ncollection of 606 software failures reported in 2017 has affected approximately 3.7 billion users and caused\nfinancial losses of $1.7 trillion. Therefore, efficiently fixing bugs becomes a fundamental step for software\nprojects [639].\nTo fix bugs, most software projects use bug reports to manage bugs. A bug report is used to record\nspecific details of the bug such as the title, the descriptions (e.g., reproducible steps and stack traces), the\nproperty fields, and the comments, which assist developers in identifying and rectifying buggy code [640].\nFigure 6 is an example of a bug report from the Gnome project with bug report ID 170801 9).\nIn this\nexample, Xuan Baldauf issued a bug report titled “converting image from grayscale to black&white is\npainfully slow”, and provided detailed information on how to reproduce the bug in the description field\n(e.g., “Open a large ... at all”).\nAdditionally, the reporter specified the property of the bug, such as\nthe product and component containing the bug.\nAfter the submission, other participants contributed\ncomments in the comment field. The bug report and its associated comments are typically utilized by\nsoftware developers to facilitate subsequent software activities.\nThe general activities to manage bug reports for bug fixing involve six steps [641] (as shown in Figure 6).\nUpon the submission of anew bug report, its initial status is labeled as new. Subsequently, the bug report\nundergoes manual scrutiny to verify its validity and prevent duplication. Once the bug is confirmed, the\nbug report is assigned to a developer. The developer then proceeds to fix the bug and sets its status\nto resolved after fixing the bug. For resolved bugs, additional developers conduct code review to verify\nthe bug resolution. If the bug resolution is verified, developers can close this bug; otherwise, they label\nthe bug report as reopen, which means that the bug is not successfully fixed. In a typical bug report\nmanagement (BRM) cycle, these processes are mainly carried out manually by software developers.\nHowever, with the exponential growth in the number of bug reports submitted to many large-scale\nsoftware projects, the size and complexity of bug report repositories increase significantly. It becomes\na laborious task to manually manage bug reports, which consumes a significant amount of time for\ndevelopers [642]. For instance, the Eclipse project received a total of 552,334 bug reports in recent 20 years\n(from January 2003 to December 2022), averaging around 76 new bug reports daily. Additionally, given\nthe varied reporting experience of bug reporters [640], not all bug reports provide sufficient information\nto assist developers with bug fixing.\nDevelopers have to spend tremendous time understanding and\nmanaging these bug reports.\nTo tackle this challenge, many studies propose to utilize text mining and machine learning (ML) [639,\n643,644] to automate BRM. These studies employ classical ML techniques such as Na¨ıve Bayes, Random\n9) https://bugzilla.gnome.org/show bug.cgi?id=170801\nNew\nReopen\nResolved\nVerified\nClosed\nAssigned\nDescription\nComment\nProperty\nTitle\nAssigned\nSci China Inf Sci\n63\nTable 17\nBRM tasks addressed by deep learning techniques.\n#\nTasks\nDescription\nTotal\n1\nBug report refinement\nGenerate a high-quality bug report by enriching/modifying an existing one.\n1\n2\nDuplicate bug detection\nDetect duplicate or similar bug reports in bug repositories.\n8\n3\nBug assignment\nRecommend the most appropriate developers to fix the bug.\n5\n4\nBug severity/priority prediction\nPredict the severity/priority of a bug report before fixing.\n2\n5\nBug fixing time prediction\nPredict how long it will take to fix the bug.\n1\n6\nBug report summarization\nSummarize a bug report into a much shorter form.\n6\n7\nBug localization\nLocate relevant source code files or methods that possibly contain the bug\nbased on the bug report.\n16\n8\nBug-Commit linking\nLink bug reports with bug fixing commits or bug inducing commits.\n3\nForest, Support Vector Machine, and k-nearest neighbors to automatically detect duplicate bug reports,\ntriage bug reports, and identify reopened bug reports.\nHowever, the effectiveness of these classical ML techniques is limited [527,645].\nTherefore, recent\nstudies have utilized deep learning (DL) techniques to enhance the automation of BRM. DL uses its\npowerful feature engineering capability to deeply analyze bug reports. The exponential growth number\nof bug reports also becomes an important source to effectively train DL models for different classification\nand regression tasks. For example, Li et al. [646] introduced an autoencoder architecture that extracts\nmultiple features from bug reports to generate bug report summary. Fang et al. [647] proposed the use\nof Recurrent Neural Network (RNN) to capture sequential information in bug reports and source code\nfor bug localization. These studies provide compelling evidence of the effectiveness of DL in improving\nautomated BRM. In this section, we survey the BRM tasks improved by DL techniques and discuss the\nchallenges and opportunities in this area.\nTable 17 summarizes the main BRM tasks addressed by DL techniques, including the task name, the\ntask description, and the number of related papers for each task. In total, researchers have applied DL\ntechniques for eight tasks in BRM.\n13.1\nBug report refinement\nWhen a bug report is submitted, bug report refinement aims to refine the bug report and improve its\nquality for better understanding.\nFor example, we can enrich the bug report with more information\ncollected from similar bug reports or reformulate the content of the bug report.\nDL can be used to\nimprove the refinement process. Zhou et al. [648] reformulated a bug report as a query representation by\nleveraging multi-level embeddings through Convolutional Neural Networks (CNNs) with the self-attention\nmechanism. The reformulation is used to help developers understand the bug report and retrieve similar\nbug reports in the repository.\n13.2\nDuplicate bug detection\nDuplicate bug report detection aims to identify whether a given bug report is a duplicate of an existing\none.\nThe detection not only eliminates redundant data but also provides additional insights into the\nreported issues. To automate this task, different types of deep neural networks have been used to train\nduplicate bug report detection models, such as simple DNN, CNN, RNN, and LSTM [649–651].\nFor\nexample, Deshmukh et al. [652] used CNN and LSTM to retrieve duplicate bug reports, achieving a\nprecision value of 90% and a recall value of 80%.\nThe main goal of these DL techniques is to represent the content of bug reports for distinguishing them.\nEvaluation demonstrates that DL techniques such as DNN-based 2D embedding and BERT show higher\nperformance in detecting duplicate bug reports than traditional language models (e.g., n-gram based\nmodel) [653–655]. Despite the promising performance, a recent study [645] analyzed the effectiveness of\nstate-of-the-art duplicate bug report detection approaches in a realistic setting using industrial datasets.\nThe results of the study shows that a simple technique already adopted in practice can achieve comparable\nresults as a recently proposed research tool.\n13.3\nBug assignment\nA submitted bug report should be assigned to a developer for resolution.\nThis assignment process\nis known as bug assignment (or triaging), which can be automated to reduce the manual effort.\nAn\nSci China Inf Sci\n64\nexisting empirical study shows that DL techniques outperform the other traditional ML techniques for\nthis task [656]. For bug assignment, DL models are employed to extract word sequences, semantic and\nsyntactic features from bug report textual contents [657]. The features contain discriminative information\nthat indicates who should fix the bug.\nLee et al. [658] proposed an automatic bug triager using CNN and word embedding.\nThe results\ndemonstrate benefits in both industrial and open source projects.\nMani et al. [659] proposed a novel\nbug report representation algorithm using an attention-based deep bidirectional recurrent neural network\n(DBRNN-A), which addresses the problem that existing bag-of-words models fail to consider syntacti-\ncal and sequential word information in unstructured texts. The empirical results show that DBRNN-A\nprovides higher rank-10 average accuracy. Liu et al. [660] proposed BT-RL model, which uses deep rein-\nforcement learning for bug triaging. It utilizes deep multi-semantic feature fusion for high-quality feature\nrepresentation, and an online dynamic matching model employing reinforcement learning to recommend\ndevelopers for bug reports.\n13.4\nBug severity/priority prediction\nThe severity/priority of bug reports indicates the importance of fixing a bug, which is often manually\ndecided by developers. Bug severity/priority prediction aims to automatically assign the severity/priority\nproperty of a bug report based on the knowledge of historical bug reports. Han et al. [661] used word\nembeddings and a one-layer shallow CNN to automatically capture discriminative word and sentence\nfeatures in bug report descriptions for predicting the severity. Gomes et al [662] conducted a survey on\nbug severity/priority prediction, which also confirms the effectiveness of DL techniques on this task.\n13.5\nBug fixing time prediction\nThis task predicts the time to be taken to fix a bug, which helps software team better allocate the work\nof developers. Previously, many popular ML methods such as KNN and Decision trees have been used to\npredict the fixing time of bug reports. In recent years, DL has also been employed. For example, Noyori\net al. [663] adopted a CNN and gradient-based visualization approach for extracting bug report features\nrelated to bug fixing time from comments of bug reports. These features can significantly improve the\neffectiveness of bug fixing time prediction models.\n13.6\nBug report summarization\nThe common practice for software developers to fix newly reported bugs is to read the bug report and\nsimilar historical bug reports. Statistics show that nearly 600 sentences have to be read on average if\na developer refers to only 10 historical bug reports during bug fixing [646]. Bug report summarization\nautomatically identifies important sentences in a bug report or directly generates a short, high-level\nsummary of the bug report [664]. Regarding DL techniques, Huang et al. [28] proposed a CNN-based\napproach to analyze the intention of each sentence in bug report for the ease of reading the content. Li\net al. [646] and Liu et al. [665] proposed auto-encoder networks with different structures for informative\nsentence extraction in bug reports. In recent studies, DL based language models are also employed to\nautomatically generate titles of bug reports [666,667].\n13.7\nBug localization\nBug localization is the main BRM task facilitated by DL techniques, where 16 related papers are found.\nFor this task, DL techniques are used to bridge the gap between the natural language in bug reports and\nthe source code [668,669]. For example, DL networks such as CNNs can be used to extract semantic\ncorrelations between bug reports and source files. Then, these features are merged and passed to a\nclassification layer to compute the relevancy scores between bug reports and source files [670].\nThe\nrelevancy scores can be combined with the scores computed from other ML or information retrieval\ntechniques [671–673] (such as learning to rank [674]) to better associate source code with a bug report.\nIn recent years, advanced DL techniques such as BERT [527], adversarial transfer learning [675], and\nattention mechanism [676] have been employed to improve the localization effectiveness.\nUsing these\npowerful DL techniques, a bug report can be located to the source code on different levels (e.g., file\nlevel [677], component level [678], and method level [526]). These techniques also enable both within-\nproject bug localization and cross-project bug localization [675,679].\nSci China Inf Sci\n65\nTable 18\nDatasets used for BRM tasks.\n#\nTasks\n<1000\n1000–10000\n10001–30000\n30001-100000\n>100000\n1\nBug report refinement\n*\n*\n*\n*\n1\n2\nDuplicate bug detection\n*\n*\n*\n*\n5\n3\nBug assignment\n*\n*\n*\n1\n2\n4\nBug severity/priority prediction\n*\n*\n*\n1\n*\n5\nBug fixing time prediction\n*\n*\n*\n1\n*\n6\nBug report summarization\n2\n*\n*\n1\n1\n7\nBug localization\n1\n3\n10\n*\n1\n8\nBug-Commit linking\n1\n*\n2\n*\n*\n13.8\nBug-Commit linking\nBug-commit linking associates bug reports with bug fixing commits or bug inducing commits. Conse-\nquently, developers can better understand which commit fixes the bug and why/how/when the bug is\nintroduced [639].\nA variant of this task is to link bug reports with reviews or comments in software\nengineering forums (e.g., APP reviews) [680]. For this task, DL techniques (e.g., word embedding and\nRNN) learn the semantic representation of natural language descriptions and code in bug reports and\ncommits, as well as the semantic correlation between bug reports and commits [681,682].\n13.9\nDatasets\nThe type of datasets used for BRM tasks is determined by the nature of each BRM task. For the majority\nof BRM tasks, the main datasets are the bug reports (also called issue reports). Such BRM tasks include\nbug report refinement, duplicate bug detection, bug assignment, bug severity/priority prediction, bug\nfixing time prediction, and bug report summarization.\nRegarding bug localization and bug-commit\nlinking, datasets require bug reports, source code, and commit messages for analysis. Existing studies\nevaluate their DL techniques using datasets from different software repositories, such as Open Office,\nEclipse, Mozilla, and Net Beans [652,653,683,684]. Since many software projects nowadays are managed\nby distributed collaboration platforms such as GitHub, bug reports from GitHub are also important\ndataset sources for BRM tasks, such as TensorFlow, Docker, Bootstrap, and VS Code [28].\nTable 18 shows the sizes of datasets used for different BRM tasks. The number in each cell represents\nthe number of datasets with the certain size used for a BRM task. All BRM tasks have datasets with\nmore than 10,000 BRM-related items (e.g., bug reports).\nFor five out of eight BRM tasks, they have\nconstructed large datasets with more than 100,000 BRM-related items. The large datasets for BRM tasks\nnot only improve the reliability of the evaluation, but also facilitate the training of DL techniques.\n13.10\nChallenges and opportunity\nBased on the preceding analysis of deep learning-based BRM, we present the challenges and opportunities\nfor future research.\n13.10.1\nChallenges\n• Computation cost. Computation cost of deep learning is one of the major concerns in BRM. Deep\nlearning works well for many tasks but generally costs a large amount of computation resources.\nSometimes the training time lasts for hundreds of hours of CPU/GPU time, especially for complex\nnetwork architectures. The less computation cost is important for using DL in real BRM scenario\nin industry, because the long computation time leads to power consumption and heat dissipation\nissues, which increase the total financial cost of software companies [685].\n• Training datasets.\nThe size of a training set is important for DNNs.\nBiswas et al. [686] found\nthat there are sometimes no performance increases for domain-specific training of DL, due to the\nsmall-scale training set. Nizamani et al. [687] also observed a trend of performance improvement\nfor deep learning when the training set size is increased. For deep learning applications in BRM,\nsmall training sets often lead performance decline and over-fitting.\n• Interpretability. The interpretability is important in BRM. Results provided by machine learning\nalgorithms are sometimes difficult to be understood.\nDL techniques are even worse due to the\nSci China Inf Sci\n66\ncomplex network architectures. Therefor, more interpretable DL techniques are important to help\ndevelopers get trustworthy prediction results.\n13.10.2\nOpportunities\n• DL acceleration in the BRM context. SE studies accelerate the network training with optimization\nstrategies in AI, e.g., batch gradient descent and RMSprop. For a given BRM task, some studies\nselect faster neural networks as substitutes for the slow ones.\nFor example, CBOW is a faster\nmodel than Skip-gram in Word2Vec. Li et al. [688] compared the two models and used CBOW for\ntheir task, as the two models achieve similar performance. Meanwhile, existing studies also reduce\nthe computation cost by using the distributed model training [689] and dynamic GPU memory\nmanager [690].\n• BRM data enrichment.\nThe challenge of training datasets can be alleviated as the exponen-\ntial growth in the number of bug reports. BRM studies also adopt DL techniques such as fine-\ntuning [654] and transfer learning [675] to address this problem. In addition, we can automatically\ngenerate (relatively low-quality) artificial data to enlarge the training set. Typically, data genera-\ntion is treated in a case-by-case manner. For instance, Morgan et al. [691] used APP screenshots\nand the labeled GUI-component names in screenshots to train DNNs for GUI-component classifica-\ntion. They synthesized APP screenshots by placing GUI-components of specified types on a single\nscreen with randomized sizes and values. They also performed color perturbation on the images to\nfurther enlarge the training set.\n• Interpretable DL techniques for BRM tasks. Existing studies try to interpret the prediction results\nof DL by visualization, including t-SNE and heat map visualizations.\nThe t-SNE (t-distributed\nStochastic Neighbor Embedding) technique projects high-dimensional vectors into two-dimensional\nspaces. It is useful to understand the embedding (vectors) generated by DNNs. With t-SNE, we\ncan understand the semantically related APIs and SE terms [692] calculated by deep learning. A\nheat map is usually used to visualize network parameters. By visualizing parameters of a layer, we\ncan understand which part of information on which the network focuses more. A heat map assumes\nthe more important (in deeper color) a region is, the more weight the network assigns to features in\nthat area. SE studies use heat maps to visualize the important part in SE images for classification\nand the attention of RNN [693].\n14\nDeveloper Collaboration\nSoftware development usually relies on highly collaborative efforts among developers and is widely known\nas a type of social-technical activity. Hence effective collaboration among software developers is one of\nthe most important factors that greatly benefit productive software development. Brooks highlights the\ncollaboration cost in software development in his famous book, The Mythical Man-Month [694].\nOn\none hand, for large-scale software projects inside an organization, hundreds or thousands of developers\ncould be involved, in which developers may not know each other well, and it is a great challenge to\nestablish effective collaboration among developers. On the other hand, open source and crowdsourcing\nprojects are becoming increasingly prevalent software development paradigms, and millions of developers\nare loosely organized in Internet-based development platforms, where both the development tasks and\ndevelopers are characterized by variety and scale, and how to support collaborative development in such\nopen environments is another big challenge. While developer collaboration may involve many aspects,\ndevelopment tasks are the pivotal points. Thus, the core issues for developer collaboration include: (1)\nunderstanding developers, and (2) assigning a task to one or multiple proper developers. In this regard,\nthanks to the availability of the large volume of software development data, machine learning (especially\ndeep learning) has been employed to analyze the data and provide intelligent support for facilitating\ndeveloper collaboration. In particular, we survey developer collaboration from the following three angles:\ndeveloper expertise profiling, intelligent task assignment, and development team forming.\nSci China Inf Sci\n67\n14.1\nDeveloper Expertise Profiling\nDeveloper expertise profiling mainly aims at realizing certain forms of representation of the skills that a\ndeveloper has mastered, which is the basis for collaborative development. Profiling developer expertise\nhas received much attention in the software engineering community [695–699]. The typical approach is to\nmine developers’ past experience data to measure their expertise with machine learning and data mining\ntechniques.\nWe first review the research efforts with traditional machine learning techniques. For instance,\n[695]\npresents an approach, Expertise Browser, to measure expertise of developers with the data in change\nmanagement systems, and [698] studies how developers learn their expertise by quantifying the develop-\nment fluency. [699–701] evaluate developers by graph-based algorithms. [702] present a conceptual theory\nof software development expertise for programming mainly by a large-scale survey of real-world software\ndevelopers. [703] evaluate developers’ contributions by development values consisting of the effect of code\nreuse and the impact on development. Expertise profiling is also used for modeling developers in open\nsource software community. For example, Venkataramaniet al. [704] captured the expertise of developers\nby mining their activities from the open source code repositories. Saxena et al. [705] annotated GitHub\ncode with tags in Stack Overflow and then created a detailed technology skill profile of a developer\nbased on code repository contributions of the developer. Considering single-community data could be\ninsufficient for accurately characterizing developers, several techniques have been proposed to connect\nusers in different software communities\n[706–712]. In [709], the authors conducted an empirical study\nof user interests across GitHub and Stack Overflow and they found that developers do share common\ninterests in the two communities. Huang et al. [710] proposed CPDScorer to model the programming\nability of developers across CQA sites and OSS communities. They first analyzed the answers posted\nin CQA and the projects submitted in OSS to score developer expertise in the two communities, re-\nspectively. They then computed the final expertise by summing up the two scores. However, they did\nnot consider the interactions among developers, which have implications for evaluating the expertise of\ndevelopers. Furthermore, most approaches to developer expertise profiling ignore the fact that developer\nexpertise evolves over time due to learning or forgetting. To fill this gap, Yan et al. [711] and Song et\nal. [713] proposed heterogeneous information network-based approaches to profiling developer expertise\nwith GitHub and Stack Overflow data, where there are four types of nodes including developers, skills,\nquestions, and projects, and nine types of edges in the network. As a result, the problem of profiling de-\nvelopers is formulated as estimating the distance of developer nodes and skill nodes. That work combines\nthe historical contributions of developers, the dynamics of expertise due to forgetting, and collaborations\namong developers, which is particularly featured by incorporating the collaboration relationships into the\nestimation of developers’ expertise. Montandon et al. [712] employed data from social coding platforms\n(i.e., Stack Overflow and GitHub), built three different machine-learning models to identify the technical\nroles of open source developers such as backend, frontend, full-stack, etc, and they simply leveraged the\ndata from Stack Overflow to build a ground truth for evaluating the performance of their approach.\nApart from traditional machine learning, deep learning techniques are also introduced in profiling\ndeveloper expertise as vectors. The vectors can be utilized for predicting or recommending downstream\ntasks to developers.\nDey et al. [714] proposed a deep neural network based approach to generating\nvectorized representation of software developers based on the APIs they have used.\nSpecifically, the\nauthors collected the open source data with WoC (World of Code) [715], extracted the mappings of\nprojects, developers and the APIs, and then used Doc2Vec [236] to obtain the vectors representing APIs,\ndevelopers and projects. Similarly, Dakhel et al.\n[716] also used Doc2Vec to generate vector embeddings\nof developers, but they incorporated more data including the textual data of repositories and issues\nbeyond APIs. Javeedetal. [717] proposed to use LSTM and convolutional neural networks to train deep\nlearning models to classify whether a developer is an expert or a novice according to six attributes of\nsource code, including security, reliability, complexity, lines of code, maintainability and duplication.\n14.2\nIntelligent Task Assignment\nGiven a software development task, one important issue is how to find appropriate developers to handle\nthe task, which is also known as development task assignment.\nIn the following, we summarize how\nmachine learning and deep learning are used to support development task assignments in various scenarios\nincluding crowdsourcing software development, open source development, bug triage etc.\nSci China Inf Sci\n68\n14.2.1\nCrowdsourcing developer recommendation\nCrowdsourcing software development usually adopts the open-call mode to solicit workers (i.e., developers)\nfor various tasks published online by requesters. Developers hope to find appropriate tasks by considering\nthe task factors such as reward, difficulty, and needed efforts while requesters need to find capable\ndevelopers to complete the tasks with guaranteed quality. As a result, recommending suitable developers\nto a task, or vice versa, becomes an important research topic in crowdsourcing software development.\nAs prior studies often overlook the skill improvement of developers over time, Wang et al. [718] pro-\nposed a new technique for crowdsourcing developer recommendations.\nAfter conducting an empirical\nstudy of 74 developers on Topcoder and re-calculating developers’ scores, they found that skill improve-\nment of developers fits well with the negative exponential learning curve. Based on the learning curve,\na skill prediction technique is designed and a skill improvement-aware framework for recommending de-\nvelopers is proposed.\nZhang et al. [719] proposed a meta-learning-based policy model to address the\nchallenge of identifying developers who are most likely to win a given task in crowdsourcing software\ndevelopment. This model firstly filters out developers unlikely to participate or submit to a given task,\nthen recommends the top k developers with the highest possibility of winning. Yu et al. [720] proposed\na new deep model, which is a cross-domain developer recommendation algorithm using feature matching\nbased on collaborative filtering, for T-shaped expert finding. Their recommendation model leverages the\ndata from software crowdsourcing platforms (i.e., ZhuBaJie and Joint Force), solves the problem of data\nsparsity, and finally improves the recommendation performance to some extent. Wang et al. [721] pre-\nsented PTRec, a context-aware task recommendation technique, capturing in-process progress-oriented\ninformation and crowdworkers’ traits through a testing context model. Using random forest, it dynam-\nically recommends tasks aligned with worker skills and interests. The evaluation shows its excellence in\nprecision and recall, saving efforts and enhancing bug detection. Furthermore, Wanget al. [722] presented\niRec2.0, which integrates dynamic testing context modeling, learning-based ranking, and multi-objective\noptimization for crowdworker recommendations in crowdtesting. It aims to detect bugs earlier, shorten\nnon-yielding windows, and alleviate recommendation unfairness, demonstrating the potential to improve\ncost-effectiveness.\n14.2.2\nReviewer recommendation\nCode review is one of the important tasks for ensuring code quality, which relies on professional developers,\nknown as reviewers, to identify defects by reading source code. Thus finding appropriate reviewers is\na core issue for achieving effective code reviews.\nTo address this issue, researchers have conducted\nextensive studies on recommending reviewers for code review tasks, especially in the context of open\nsource development.\nYing et al. [723] proposed a reviewer recommendation approach (EARec) for a given pull request,\nconsidering developer expertise and authority simultaneously. Jianget al. [724] provided an approach to\nrecommending developers to comment a PullRequest in social-coding platforms like GitHub. Zhang et\nal. [725] presented CORAL, a reviewer recommendation technique using a socio-technical graph and a\ngraph convolutional neural network. Trained on 332 Microsoft repositories, CORAL identifies qualified\nreviewers missed by traditional systems, excelling in large projects, while traditional systems perform\nbetter in smaller ones. Rebai etal. [726] framed code reviewer recommendation as a multi-objective search\nproblem, balancing expertise, availability, and collaboration history. Validation on 9 open-source projects\nconfirms its superiority over existing approaches.\nZanjan et al. [727] introduced cHRev, an approach\nfor automatic reviewer recommendation based on historical contributions of reviewers in their previous\nreviews. Due to leveraging specific previous review information, cHRev outperforms existing approaches\non three open-source systems and a Microsoft code base. Hannebauer et al. [728] empirically compared\nsix modification expertise-based algorithms and two reviews expertise-based algorithms on four FLOSS\nprojects. The study concludes that review expertise-based algorithms, particularly the Weighted Review\nCount (WRC), are more effective. Ronget al. [729] introduced HGRec, a recommendation system utilizing\nhypergraph techniques to model complex relationships involving multiple reviewers per pull-request in\ncode review. Evaluated on 12 OSS projects, HGRec demonstrates superior accuracy, emphasizing the\npotential of hypergraphs in this field. Kovalenko et al. [730] evaluated a reviewer recommendation system\nin a company setting, covering over 21,000 code reviews. Despite the relevance of recommendations, it\nidentifies no evidence of influence on user choices, highlighting the need for more user-centric design\nand evaluation in reviewer recommendation tools. Ahasanuzzamanet al. [731] proposed KUREC,a code\nSci China Inf Sci\n69\nreviewer recommender utilizing Java programming language knowledge units (KUs) to generate developer\nexpertise profiles and select reviewers.\nEvaluated against baselines, KUREC is found to be equally\neffective but more stable.\nBesides, combining KUREC with baselines further enhances performance.\nGon¸calves et al. [732] identified 27 competencies vital for code review through expert validation and\nranked them using a survey of 105 reviewers. The findings reveal that technical skills are essential and\ncommonly mastered, but improvements are needed in clear communication and constructive feedback.\n14.2.3\nOther tasks\nBesides the aforementioned tasks, there are also a series of other development tasks that benefit from\nmachine learning and deep learning technologies,e.g. bug assignment (Section 13.3), question answering\ntasks in online Q&A sites, and various development tasks for open source contributors.\nHuanget al. [733] proposed an approach to recommending appropriate answerers for questions posted to\nQ&A sites. Specifically, they leveraged graph attention networks to represent the interactive information\nbetween candidate answerers, and an LDA topic model to capture the text information. It was verified by\nexperiments that the approach outperforms the state-of-the-art techniques of that time.\nJin et al. [734]\nproposed CODER, a graph-based code recommendation framework for OSS developers, that models user-\ncode and user-project interactions via a heterogeneous graph to predict developers’ future contributions.\nCODER has shown superior performance in various experimental settings, including intra-project and\ncross-project recommendations.\nXiao et al. [735] introduced RecGFI, an approach for recommending\n“Good First Issues” to newcomers in open-source projects. Utilizing features from content, background,\nand dynamics.\nEmploying an XGBoost classifier, RecGFI achieves up to 0.853 AUC in evaluation,\ndemonstrating superiority over alternative models.\nSantos [736] proposed an automatic open issue labeling strategy to assist OSS contributors in selecting\nsuitable tasks and helping OSS communities attract and retain more contributors. The technique uses\nAPI-domain tags to label issues and relies on qualitative studies to formulate recommendation strategies\nand quantitative investigations to analyze the relevance between API-domain labels and contributors. The\nresults show that the predicted labels have an average precision of 75.5%, demonstrating the superiority\nof the technique.\nCosta et al. [737] presented TIPMerge, an approach for recommending participants\nfor collaborative merge sessions within large development projects with multiple branches. TIPMerge\nbuilds a ranked list of developers appropriate to collaborate by considering their changes in previous\nhistory, branches and dependencies, and recommends developers with complementary knowledge. The\napproach demonstrates a mean normalized improvement of 49.5% for joint knowledge coverage compared\nto selecting the top developers.\n14.3\nDevelopment Team Formation\nComplex development tasks often ask for a team of developers. Thus how to find a cohort of developers\nwho can collaboratively handle a complex task is an important issue in software development.\nIn order to address the complexity of finding collaborators with shared interests in large open-source\nsoftware, Constantinoetal. [738] proposed a visual and interactive web application tool named CoopFinder.\nThey further presented and evaluated two collaborator recommendation strategies based on co-changed\nfiles [739]. The strategies utilize TF-IDF scheme to estimate the importance of files modified by developers\nand measure developers’ similarity using the Cosine metric. Through an extensive survey of 102 real-\nworld developers, the strategies show up to an 81% acceptance rate, enhancing collaboration efficiency\namong developers.\nSurian et al. [740] introduced a technique to find compatible collaboration among\ndevelopers. They first created a collaboration network using information of developers and projects from\nSourceforge.Net, then recommended collaborators for developers based on their programming skills and\npast projects through a random walk with restart procedures. Canfora et al. [741] introduced Yoda, a\ntechnique for recommending mentors to newcomers in software projects. By mining data from mailing\nlists and versioning systems, developers with experience meanwhile actively interacting with newcomers\nare selected as their mentors. Evaluation on five open-source projects and surveys with developers shows\nthe potential usefulness of Yoda in supporting newcomers in a team and indicates that top committers\nare not always the best mentors. Ye et al. [742] introduced a personalized teammate recommendation\napproach for crowdsourcing developers. Through an empirical study on Kaggle, three factors influencing\ndevelopers’ teammate preferences are identified and a linear programming-based technique is proposed to\nSci China Inf Sci\n70\ncompute developers’ teammate preferences. Finally, a recommendation approach with an approximation\nalgorithm to maximize collaboration willingness is designed.\n14.4\nDatasets\nHere, we present some commonly used datasets in the three main kinds of research efforts toward intel-\nligent developer collaboration. For developer expertise profiling, data primarily originate from collabo-\nrative software development platforms such as GitHub, encompassing version control data from various\nopen-source projects with diverse developer information [743] [744] [716]. For intelligent task assignment,\ndatasets are sourced from crowdsourcing platforms like Topcoder and Baidu CrowdTest, where historical\ndata contain developer information and the corresponding task categories [718] [721].\nIn the case of\ndevelopment team formation, datasets are predominantly obtained from platforms like SourceForge and\nKaggle, showcasing richer collaboration patterns among developers [740] [742]. These datasets provide\nabundant information for studying developer behavior, intelligent task assignment, and team formation.\n• Developer expertise profiling.\nThe World of Code (WoC) dataset [743] is a versioned and\nexpansive repository of version control data from Free/Libre and Open Source Software (FLOSS)\nprojects using Git.\nThe dataset, collected in March 2020, contains 7.9 billion blobs, 2 billion\ncommits, 8.3 billion trees, 17.3 million tags, 123 million distinct repositories, and 42 million unique\nauthor IDs. WoC supports various research tasks, including developer expertise profiling. Using\nthe WoC dataset, Fry et al. [744] proposed a technique to identify all author IDs belonging to a\nsingle developer in the entire dataset, revealing aliases. Using machine learning, Fryetal. processed\naround 38 million author IDs, identifying 14.8 million with aliases linked to 5.4 million developers.\nThis dataset enhanced models of developer behavior at the global open-source software ecosystem\nlevel, facilitating rapid resolution of new author IDs.\nMeanwhile, Dakhel et al. [716] collected a\ndataset to determine the domain expertise of developers using information from GitHub.\nTheir\ndata collection process involved three main types of information: repositories that developers have\ncontributed to, issue-resolving history, and API calls involved in a commit. This dataset contained\ninformation about 1,272 developers with expertise labels in five job roles. The dataset consisted\nof textual information from 58,000 repositories, issue-resolving history from 60,000 issues, and API\ncalls from 21 million commits across different GitHub repositories. The dataset aimed to provide\ncomprehensive insights into developers’ expertise by considering their contributions to repositories,\nissue resolution history, and API usage in commits across diverse projects on GitHub.\n• Intelligent task assignment. Crowdsourcing platforms provide the feasibility of data collection\nfor intelligent task assignments.\nFor instance, Topcoder is a competition-based crowdsourcing\nsoftware development platform.\nTopcoder offers various types of tasks, such as “Test Suites,”\n“Assembly,” and “Bug Hunt,” each representing a category of challenges. Challenges are instances\nof task types, and developers choose whether to participate in these challenges. In this respect,\nWang et al. [718] collected a dataset from Topcoder.\nTheir dataset involved 32,565 challenges,\n7,620 developers, and 59,230 submissions spanning from 2006 to 2016. The dataset focused on 100\ndevelopers with over 100 submissions, containing the evolution of the skills of each developer. After\nfiltering out submissions with a final score of 0 (indicating unfinished submissions), there were 74\ndevelopers left in the dataset. There has also been a lot of work on dataset building using other\ncrowdsourcing platforms. For example, Wanget al. [721] collected a dataset from Baidu CrowdTest.\nThe dataset involved 2,404 crowdworkers and comprised 80,200 submitted reports. For each testing\ntask, comprehensive information was gathered, including task-related details and all submitted test\nreports with associated information, such as submitter and device.The dataset serves as a valuable\nresource for analyzing crowdtesting dynamics and outcomes.\n• Development team formation. Work related to development team formation is often obtained\nfrom open-source software development platforms with collaborative and social nature, such as\nSourceForge and Kaggle. For example, Surian et al. [740] collected a dataset by analyzing the\nSourceForge database, which contained information about projects, project categories, and pro-\ngramming languages. There were 209,009 developers associated with 151,776 projects. The dataset\nincluded details such as 354 project categories and information about the usage of 90 different pro-\ngramming languages within the projects. Meanwhile, Yeetal. [742] collected a dataset by crawling\nSci China Inf Sci\n71\ndata from Kaggle, including details from 275 competitions and 74,354 developers. The data spanned\nfrom April 2010 to January 2018 and encompassed 191,300 submissions. Additionally, developers’\nsocial data from Kaggle’s communities were crawled, enhancing the dataset with information about\ninteractions and discussions among developers.\n14.5\nChallenges and Opportunities\nIn general, software is becoming more complex as the major enabling force for the infrastructure of var-\nious information systems. Consequently, more developers participate in software projects, and software\ndevelopment is increasingly exhibiting a social-technical characteristic, which requires more effective col-\nlaboration among developers. Thus, more research efforts are needed to produce new theories, techniques,\nand tools to further improve collaborative development. To that end, we summarize the challenges and\nopportunities in this research area.\n14.5.1\nChallenges\nThe use of deep learning to improve developer collaboration faces the following research challenges in\nterms of collaborative tasks, software development data, and evaluation benchmarks.\n• Complex collaboration among multiple developers. Existing research mainly considers the\ncollaboration between two developers. In other words, one developer posts a task requirement, and\nanother developer is required to fulfill the task. However, at higher levels (e.g., from a project’s\nperspective), we must consider the collaboration among a group of developers, where global collab-\noration effects and constraints should be of greater concern.\n• Continuously growing data size and heterogeneity. On the one hand, developers generate\nmore data, which are often distributed across various platforms, including personal IDEs, propri-\netary enterprise environments, open-source platforms (e.g., GitHub), and public forums (e.g., Stack\nOverflow). On the other hand, software development data are essentially heterogeneous, involving\nnatural language data, source code, AI models, and even graphical data. Dealing with highly dis-\npersed, heterogeneous, and large-scale development data is a great challenge for using deep learning\nto achieve more effective and efficient collaborative development.\n• Lack of benchmarks for effective evaluation. As the effectiveness of developer collaboration\nusually cannot be observed in a short time, evaluating a newly proposed technique is difficult.\nTherefore, having benchmark datasets on one or multiple collaborative development tasks is highly\ndesirable.\n14.5.2\nOpportunities\nAlthough deep learning has shown promising results in improving developer collaboration, there are still\nabundant research opportunities for advancing this direction.\n• Application of deep learning to a wider spectrum of collaborative development tasks. As collabo-\nrative activities are pervasive in software development processes, deep learning can be introduced\nto deal with more tasks other than those in existing studies.\n• Incorporating advanced deep learning technologies.\nAlthough deep learning has developed dra-\nmatically for nearly two decades, new technologies are still being put forward continually, such as\ngraph neural networks and large language models. Applying these technologies to handle large-scale\nheterogeneous development data embraces more opportunities for intelligent collaborative develop-\nment.\n• Novel collaborative development activities enabled by deep learning. Large language models like\nChatGPT are becoming more powerful in handling development tasks, such as code generation\nand program repair. It is possible to see novel collaborative development activities among human\ndevelopers and deep learning-based AI models.\nSci China Inf Sci\n72\n15\nConclusion\nIn this paper, we present the first task-oriented survey on deep learning-based software engineering. Our\nsurvey focuses on twelve of the important tasks in software development and maintenance: requirements\nengineering, code generation, code search, code summarization, software refactoring, code clone detection,\nsoftware defect prediction, bug finding, fault localization, program repair, bug report management, and\ndeveloper collaboration. For each of the selected tasks, we summarize the mostrecent advances concerning\nthe application of deep learning for the given task, as well as relevant challenges and future opportunities.\nOn the basis of the surveyed deep learning-based subareas of software engineering, we make the following\nobservations:\n• Widespread applications and impressive results. Deep learning techniques have been widely\napplied across various subareas of software engineering and have achieved impressive results. First,\ndeep learning techniques typically improve the performance of most tasks.\nFor example, deep\nlearning-based code clone detection always achieves higher recall and better precision than the best\nclassical approaches. An existing empirical study also shows that deep learning techniques outper-\nform other traditional machine learning techniques for bug assignments. Moreover, in requirements\nengineering, most studies report precision and recall exceeding 80%, and the F1 score is often above\n75%. Second, the powerful feature engineering capability of deep learning enables the capture of\nsemantic information. Typically, some tasks (e.g., software defect prediction and software refactor-\ning) involve complex feature engineering. Deep learning helps release researchers and practitioners\nfrom tedious feature engineering. Third, deep learning can further contribute to the automation of\nsoftware engineering processes. Traditional techniques may rely on complex preprocessing and/or\npostprocessing techniques to automate the whole process. Deep learning can turn the whole process\nin an end-to-end manner. For example, deep learning techniques can directly deal with bug reports\nand help automate the process of bug report management.\n• Challenges in high-quality training data acquisition. The primary challenge faced by most\nsubareas of software engineering is obtaining high-quality training data. First, the sizes of datasets\nare often limited.\nFor example, publicly accessible data for requirements typically are small in\nvolume and lack some details, making it difficult to train effective deep learning models. Second,\nthe quality of datasets is often unguaranteed. For example, in code generation, it is unclear whether\ndatasets contain vulnerable code snippets that may result in unsafe codes. Third, some existing\ndatasets are the results of specially designed data production activities and may not well align with\nreal-world scenarios. Although this can lead to strong model performance during evaluation, the\nperformance may not be effectively transferred into practical use.\n• Interpretability challenge. The interpretability of deep learning models is also a common chal-\nlenge across various subareas of software engineering.\nFirst, the lack of interpretability for deep\nlearning models makes ensuring correctness difficult.\nFor example, ensuring whether generated\npatches are correct is difficult. Second, deep learning models are known for their “black-box” na-\nture, making it challenging for developers to understand their rationale. Therefore, it is hard for\ndevelopers to work with deep learning models.\n• Generalization across languages. Developing deep learning models that generalize well across\ndifferent languages is a considerable challenge. Different programming languages have their own\nsyntax and semantics, making it difficult to develop a universal model that performs well across\ndiverse languages.\nTherefore, the current practice is to deal with each programming language\nindividually.\nFor example, for different programming languages, researchers often need to train\ndifferent models for the same code summarization task.\nFurthermore, to incorporate additional\ncode structure information in deep learning models, researchers must parse code snippets with their\ncorresponding parsers, thus resulting in significant differences for different programming languages.\nOur survey demonstrates that deep learning-based software engineering has achieved significant ad-\nvances recently and has the potential for further improvement. However, some critical challenges should\nbe resolved before deep-learning based software engineering can reach its maximal potential. We believe\nthat future research should focus on resolving these challenges.\nSci China Inf Sci\n73\nAcknowledgements\nWe\nthank the following\npersons for their prior contributions to the manuscript preparation\n(in\nalphabetical order):\nYuze Guo\n(Beihang University), Ruiqi Hong (Beihang University), Mingwei Liu (Fudan University),\nXiaofan Liu (Wuhan University), Di Wu (Beihang University), Hongjun Yang (Beihang University), Yanming Yang (Zhejiang\nUniversity), Binquan Zhang (Beihang University), and Zhuang Zhao (Wuhan University) .\nReferences\n1\nGeoffrey E Hinton and Ruslan R Salakhutdinov.\nReducing\nthe dimensionality of data with neural networks.\nscience ,\n313(5786):504–507, 2006 .\n2\nLi Liu, Wanli\nOuyang, Xiaogang Wang,\nPaul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietik¨ainen.\nDeep learning for\ngeneric object detection: A survey.\nInternational\nJournal of Computer\nVision, 128:261–318, 2020 .\n3\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh.\nA fast learning algorithm for deep belief nets.\nNeural Computation,\n18(7):1527–1554, 2006 .\n4\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural networks.\nCommunications of the ACM, 60(6):84–90, 2017 .\n5\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner.\nGradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278–2324, 1998 .\n6\nJeffrey L Elman.\nFinding structure in time.\nCognitive Science, 14(2):179–211, 1990 .\n7\nSepp Hochreiter and J¨urgen Schmidhuber.\nLong short-term memory.\nNeural\nComputation, 9(8):1735–1780, 1997 .\n8\nMike Schuster and Kuldip K Paliwal.\nBidirectional recurrent neural networks.\nIEEE\nTransactions\non Signal Processing ,\n45(11):2673–2681, 1997 .\n9\nAshish Vaswani,\nNoam\nShazeer,\nNiki\nParmar,\nJakob\nUszkoreit, Llion Jones, Aidan N Gomez, L–ukasz Kaiser, and Illia\nPolosukhin.\nAttention is all you need.\nAdvances\nin neural information processing systems, 30, 2017 .\n10\nYanming Yang, Xin Xia, David Lo, and John Grundy. A survey on deep learning for software engineering.\nACM Computing\nSurveys (CSUR), 54(10s):1–73, 2022 .\n11\nGiang Nguyen, Stefan Dlugolinsky, Martin Bob´ak,\nViet Tran,\n´Alvaro L´opez\nGarc´ıa, Ignacio Heredia, Peter Mal´ık, and\nLadislav Hluch`y.\nMachine learning and deep learning\nframeworks and libraries for large-scale data\nmining:\na survey.\nArtificial Intelligence Review, 52:77–124, 2019 .\n12\nJinjiang Wang, Yulin Ma, Laibin Zhang, Robert X Gao, and Dazhong Wu.\nDeep learning for smart manufacturing:\nMethods\nand applications.\nJournal of Manufacturing Systems, 48:144–156, 2018 .\n13\nDinggang Shen, Guorong Wu, and Heung-Il Suk.\nDeep learning in medical image analysis.\nAnnual Review of Biomedical\nEngineering, 19:221–248, 2017 .\n14\nDaniel S Berman, Anna L Buczak, Jeffrey S Chavis, and Cherita L Corbett.\nA survey of deep learning methods for cyber\nsecurity.\nInformation, 10(4):122, 2019 .\n15\nTriet HM Le, Hao Chen, and Muhammad Ali Babar.\nDeep learning for source code modeling and generation:\nModels,\napplications, and challenges.\nACM\nComputing Surveys\n(CSUR), 53(3):1–38, 2020 .\n16\nAlexey Svyatkovskiy, Ying Zhao,\nShengyu Fu,\nand Neel Sundaresan.\nPythia:\nAi-assisted\ncode completion system.\nIn\nProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , KDD 2019,\npages 2727–2735, 2019 .\n17\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer.\nSummarizing source code using a neural attention\nmodel.\nIn Proceedings of the\n54th Annual Meeting of the Association for\nComputational Linguistics, ACL 2016, August\n7-12, 2016, Berlin, Germany,\nVolume 1:\nLong Papers.\nThe Association for Computer Linguistics, 2016 .\n18\nMauricio Aniche, Erick Maziero, Rafael Durelli, and Vinicius HS Durelli.\nThe effectiveness of supervised machine learning\nalgorithms in predicting software refactoring.\nIEEE\nTransactions\non Software Engineering, 48(4):1432–1450, 2020 .\n19\nXiaodong\nGu,\nHongyu\nZhang,\nand Sunghun Kim.\nDeep\ncode\nsearch.\nIn\nMichel\nChaudron,\nIvica Crnkovic,\nMarsha\nChechik, and Mark Harman, editors, Proceedings\nof the 40th International\nConference\non\nSoftware Engineering,\nICSE\n2018,\nGothenburg,\nSweden, May 27 - June 03, 2018, pages 933 –944. ACM, 2018 .\n20\nMohammad Wardat, Wei Le, and Hridesh Rajan.\nDeeplocalize:\nFault localization for deep neural networks.\nIn Proceedings\nof the IEEE/ACM 43rd International Conference on Software Engineering, ICSE 2021, pages 251–262 . IEEE, 2021 .\n21\nYi\nLi,\nShaohua\nWang, and Tien N. Nguyen.\nDlfix:\ncontext-based code transformation learning for automated program\nrepair.\nIn ICSE\n’20:\n42nd\nInternational\nConference\non Software Engineering, Seoul, South Korea, 27 June - 19 July,\n2020, pages 602–614 . ACM, 2020 .\n22\nDeqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, and Hai Jin.\nμvuldeepecker: A deep learning-based system for multiclass\nvulnerability detection.\nIEEE\nTransactions\non Dependable and Secure Computing, 18(5):2224–2236, 2019 .\n23\nNargiz Humbatova, Gunel Jahangirova, and Paolo Tonella.\nDeepcrime:\nmutation testing of deep\nlearning\nsystems\nbased\non real faults.\nIn Proceedings of the 30th ACM SIGSOFT International Symposium on Software\nTesting and Analysis ,\nISSTA 2021, pages 67–78, 2021 .\n24\nCody Watson, Nathan Cooper, David Nader Palacio, Kevin Moran, and Denys Poshyvanyk.\nA systematic literature review\non the use of deep learning in software engineering research.\nACM Transactions on Software Engineering and Methodology\n(TOSEM), 31(2):1–58, 2022.\n25\nChangan Niu, Chuanyi Li, Bin Luo, and Vincent Ng.\nDeep learning meets software engineering:\nA survey on pre-trained\nmodels of source code.\narXiv preprint arXiv:2205.11739, 2022 .\n26\nQuanjun Zhang, Chunrong Fang, Yang Xie, Yaxin Zhang, Yun Yang, Weisong Sun, Shengcheng Yu, and Zhenyu Chen.\nA\nsurvey on large language models for software engineering.\narXiv preprint arXiv:2312.15223, 2023 .\n27\nZhi Jin.\nEnvironment\nModeling-Based\nRequirements\nEngineering for\nSoftware Intensive\nSystems.\nMorgan\nKaufmann\nPublishers Inc., San Francisco, CA, USA, 1st edition, 2017 .\n28\nQiao\nHuang,\nXin Xia, David Lo, and Gail C. Murphy.\nAutomating\nintention mining.\nIEEE\nTransactions\non\nSoftware\nEngineering, 46(10):1098–1119, 2020 .\n29\nFlorian Pudlitz, Florian Brokhausen, and Andreas Vogelsang.\nExtraction of system states from natural language require-\nments.\nIn 2019 IEEE 27th International Requirements Engineering\nConference\n(RE), pages 211–222, 2019 .\n30\nMingyang Li,\nLin Shi, Ye Yang,\nand\nQing Wang.\nA deep\nmultitask\nlearning\napproach\nfor requirements discovery and\nannotation from open forum.\nIn Proceedings of the 35th IEEE/ACM International Conference on Automated Software\nEngineering, ASE ’20, page 336–348, New York, NY, USA, 2021 . Association for Computing Machinery.\nSci China Inf Sci\n74\n31\nHui Guo and Munindar P. Singh.\nCaspar:\nExtracting\nand synthesizing user stories of problems from app reviews.\nIn\nProceedings\nof the ACM/IEEE 42nd International\nConference\non\nSoftware\nEngineering, ICSE ’20, page 628–640, New\nYork, NY, USA, 2020 . Association for Computing Machinery.\n32\nRohan Reddy Mekala, Asif Irfan, Eduard C. Groen, Adam Porter, and Mikael Lindvall.\nClassifying\nuser\nrequirements\nfrom online feedback in small dataset environments using deep learning.\nIn 2021 IEEE 29th International Requirements\nEngineering\nConference (RE), pages 139–149, 2021 .\n33\nJames Tizard, Peter Devine, Hechen Wang, and Kelly Blincoe.\nA software requirements ecosystem:\nLinking forum,\nissue\ntracker, and faqs for requirements management.\nIEEE\nTransactions\non Software Engineering, 49(4):2381–2393, 2023 .\n34\nLin Shi, Mingzhe Xing, Mingyang Li, Yawen Wang, Shoubin Li, and Qing Wang.\nDetection of hidden feature requests from\nmassive chat messages via deep siamese network.\nIn\nProceedings\nof the\nACM/IEEE\n42nd International\nConference\non\nSoftware Engineering, ICSE ’20, page 641–653, New York, NY, USA, 2020 . Association for Computing Machinery.\n35\nShengyi Pan, Lingfeng Bao, Xiaoxue Ren, Xin Xia, David Lo, and Shanping Li.\nAutomating developer chat mining.\nIn\n2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 854–866, 2021 .\n36\nOktay T¨uretken, Onur Su, and Onur Demir¨ors.\nAutomating software requirements generation from business process models.\nIn 1st Conf.\non the Principles of Software Eng.(PRISE’04),\nBuenos Aires, Argentina, 2004 .\n37\nKarl Cox, Keith Phalp, Steven J. Bleistein, and June M. Verner.\nDeriving requirements from process models via the problem\nframes approach.\nInf. Softw.\nTechnol. , 47(5):319–337, 2005 .\n38\nNeil A. M. Maiden, Sharon Manning, Sara Jones, and John Greenwood.\nGenerating requirements from systems models using\npatterns: a case study.\nRequir.\nEng. ,\n10(4):276 –288 , 2005 .\n39\nEric S. K. Yu, Philippe Du Bois, Eric Dubois, and John Mylopoulos.\nFrom organization models to system requirements:\nA\n’cooperating agents’ approach.\nIn Proceedings of the\nThird International\nConference on Cooperative Information Systems\n(CoopIS-95), May 9-12, pages 194–204, 1995 .\n40\nEmmanuel Letier and Axel van Lamsweerde.\nDeriving operational software specifications from system goals.\nIn Proceedings\nof the\nTenth\nACM SIGSOFT Symposium\non Foundations\nof Software Engineering 2002,\nCharleston,\nSouth\nCarolina,\nUSA, November 18-22, 2002, pages 119 –128. ACM, 2002 .\n41\nRenaud De Landtsheer, Emmanuel Letier, and Axel van Lamsweerde.\nDeriving tabular\nevent-based specifications from\ngoal-oriented requirements models.\nRequir. Eng. , 9(2):104–120, 2004 .\n42\nAxel Van Lamsweerde.\nGoal-oriented\nrequirements\nenginering:\na roundtrip\nfrom\nresearch\nto\npractice\n[enginering read\nengineering] .\nIn Proceedings.\n12th IEEE International Requirements Engineering\nConference, 2004. , pages 4–7 .\nIEEE,\n2004.\n43\nAxel van Lamsweerde and Laurent Willemet.\nInferring declarative requirements specifications from operational scenarios.\nIEEE Trans.\nSoftware Eng. , 24(12):1089 –1114, 199 8 .\n44\nFarid Meziane, Nikos Athanasakis, and Sophia Ananiadou.\nGenerating\nnatural language specifications from UML class\ndiagrams.\nRequir. Eng. , 13(1):1–18, 2008.\n45\nBrian Berenbach.\nThe automated extraction of requirements from UML models.\nIn\n11th IEEE International\nConference\non Requirements Engineering\n(RE\n2003),\n8-12\nSeptember 2003,\nMonterey\nBay,\nCA,\nUSA,\npage\n287 .\nIEEE\nComputer\nSociety, 2003 .\n46\nAmina Souag, Ra´ul Mazo, Camille Salinesi, and Isabelle Comyn-Wattiau.\nUsing the aman-da method to generate security\nrequirements: A case study in the maritime domain.\nRequir.\nEng. , 23(4):557–580, nov 2018 .\n47\nZiyan Zhao, Li Zhang, Xiaoli Lian, Xiaoyun Gao, Heyang Lv, and Lin Shi.\nReqgen: Keywords-driven software requirements\ngeneration.\nMathematics, 11(2), 2023 .\n48\nKoscinski\nViktoria,\nHashemi\nSara,\nand\nMirakhorli\nMehdi.\nOn-demand\nsecurity\nrequirements\nsynthesis\nwith\nrelational\ngenerative adversarial networks.\nIn\n2023 IEEE/ACM 45th\nInternational\nConference\non Software Engineering\n(ICSE) ,\npages 1613–1625, 2023 .\n49\nMingyang Li, Ye Yang, Lin Shi, Qing Wang, Jun Hu, Xinhua Peng, Weimin Liao, and Guizhen Pi.\nAutomated extraction\nof requirement entities by leveraging lstm-crf and transfer learning.\nIn 2020 IEEE International Conference on Software\nMaintenance and Evolution (ICSME), pages 208–219, 2020 .\n50\nFrancesco Casillo, Vincenzo Deufemia, and Carmine Gravino.\nDetecting\nprivacy requirements from user stories with nlp\ntransfer learning models.\nInformation\nand Software\nTechnology, 146:106853, 2022 .\n51\nSaad Ezzini, Sallam Abualhaija, Chetan Arora, and Mehrdad Sabetzadeh.\nAutomated handling of anaphoric ambiguity\nin requirements:\nA multi-solution study.\nIn\n2022\nIEEE/ACM 44th\nInternational\nConference\non\nSoftware\nEngineering\n(ICSE), pages 187–199, 2022 .\n52\nYawen Wang, Lin Shi, Mingyang Li, Qing Wang, and Yun Yang.\nDetecting coreferent entities in natural language require-\nments.\nRequirements\nEngineering, 27:351–373, 2022 .\n53\nYawen\nWang,\nLin\nShi, Mingyang Li, Qing Wang, and Yun Yang.\nA deep context-wise method for coreference detection\nin natural language requirements.\nIn 2020 IEEE 28th International Requirements Engineering\nConference\n(RE), pages\n180–191, 2020 .\n54\nSaad Ezzini,\nSallam\nAbualhaija,\nChetan\nArora,\nand\nMehrdad Sabetzadeh.\nAi-based\nquestion answering assistance for\nanalyzing natural-language requirements, 2023 .\n55\nCody Baker, Lin Deng, Suranjan Chakraborty, and Josh Dehlinger.\nAutomatic multi-class non-functional software require-\nments classification using neural networks.\nIn 2019 IEEE 43rd Annual Computer Software and Applications\nConference\n(COMPSAC), volume 2, pages 610–615, 2019 .\n56\nTobias Hey, Jan Keim, Anne Koziolek, and Walter F. Tichy.\nNorbert:\nTransfer learning for requirements classification.\nIn\n2020 IEEE 28th International Requirements Engineering Conference (RE), pages 169–179, 2020 .\n57\nXianchang\nLuo,\nYinxing\nXue,\nZhenchang\nXing, and Jiamou Sun.\nPrcbert:\nPrompt learning for requirement classifica-\ntion using bert-based pretrained language models.\nIn\nProceedings\nof the 37th IEEE/ACM International\nConference\non\nAutomated Software Engineering, ASE ’22, New York, NY, USA, 2023 . Association for Computing Machinery.\n58\nJonas Paul Winkler, Jannis Gr¨onberg, and Andreas Vogelsang.\nPredicting how to test requirements: An automated approach.\nIn 2019 IEEE 27th International Requirements Engineering Conference (RE), pages 120–130, 2019 .\n59\nOsamah AlDhafer, Irfan Ahmad, and Sajjad Mahmood.\nAn end-to-end deep learning system for requirements classification\nusing recurrent neural networks.\nInformation\nand Software\nTechnology, 147:106877, 2022 .\n60\nJin Guo, Jinghui Cheng, and Jane Cleland-Huang.\nSemantically enhanced software traceability using deep learning tech-\nniques.\nIn 2017 IEEE/ACM 39th International\nConference on Software Engineering (ICSE), pages 3–14, 2017 .\n61\nMuhammad Shah Jahan, Habib Ullah Khan, Shahzad Akbar, Muhammad Umar Farooq, Sarah Gul, Anam Amjad, and\nSci China Inf Sci\n75\nFabrizio Riguzzi.\nBidirectional language modeling:\nA systematic literature review.\nSci.\nProgram. , 2021, jan 2021 .\n62\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBioBERT:\na pre-trained biomedical language representation model for biomedical text mining.\nBioinformatics,\n36(4):1234–1240, 09\n2019.\n63\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin\nJiang, and Ming Zhou.\nCodebert: A pre-trained model for programming and natural languages.\nIn Trevor Cohn, Yulan He,\nand Yang Liu, editors, Findings of the Association for Computational Linguistics:\nEMNLP\n2020,\nOnline Event,\n16-20\nNovember 2020, volume EMNLP 2020 of Findings of ACL, pages 1536–1547 .\nAssociation for Computational Linguistics,\n2020.\n64\nJinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang.\nTraceability transformed:\nGenerating more\naccurate links with pre-trained bert models.\nIn 2021 IEEE/ACM 43rd International\nConference on Software Engineering\n(ICSE), pages 324–335, 2021 .\n65\nJiahao Tian, Li Zhang, and Xiaoli Lian.\nA cross-level requirement trace link update model based on bidirectional encoder\nrepresentations from transformers.\nMathematics, 11(3):623, Jan 2023 .\n66\nJinfeng Lin, Yalin Liu,\nand Jane Cleland-Huang.\nInformation\nretrieval\nversus\ndeep\nlearning\napproaches for generating\ntraceability links in bilingual projects.\nEmpirical Software Engineering, 27, 2022 .\n67\nIso/iec/ieee international standard - systems and software engineering – life cycle processes – requirements engineering.\nISO/IEC/IEEE 29148:2018(E), pages 1–104, 2018 .\n68\nAlistair Mavin, Philip Wilkinson, Adrian Harwood, and Mark Novak.\nEasy approach to requirements syntax (ears) .\nIn 2009\n17th IEEE International Requirements Engineering Conference, pages 317–322, 2009 .\n69\nXavier\nFranch,\nMartin Glinz, Daniel Mendez, and Norbert Seyff.\nA study\nabout the\nknowledge and use of requirements\nengineering standards in industry.\nIEEE\nTransactions\non Software Engineering, 48(9):3310–3325, 2022 .\n70\nJenny T. Liang, Chenyang Yang, and Brad A. Myers.\nA large-scale survey on the usability of ai programming assistants:\nSuccesses and challenges, 2023 .\n71\nSteven Kelly and Juha-Pekka Tolvanen.\nDomain-specific Modeling:\nenabling\nFull\nCode\nGeneration.\nJohn Wiley & Sons,\n2008.\n72\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton.\nA survey of machine learning for big code and\nnaturalness.\nACM\nComputing Surveys\n(CSUR), 51(4):1–37, 2018.\n73\nGail C Murphy, Mik Kersten, and Leah Findlater.\nHow are java software developers using the elipse ide?\nIEEE software ,\n23(4):76–83, 2006.\n74\nMarcel Bruch, Martin Monperrus, and Mira Mezini.\nLearning from examples to improve code completion systems.\nIn\nProceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium\non the foundations of software engineering, pages 213–222, 2009 .\n75\nTihomir\nGvero,\nViktor\nKuncak,\nIvan Kuraj,\nand Ruzica Piskac.\nComplete\ncompletion\nusing\ntypes\nand\nweights.\nIn\nProceedings of the 34th ACM SIGPLAN conference on Programming language design and implementation, pages 27–38,\n2013.\n76\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong,\nShan Wang,\nYufei Xue,\nZihan Wang,\nLei\nShen,\nAndi\nWang, Yang Li,\net al.\nCodegeex:\nA pre-trained model for code generation with multilingual evaluations on humaneval-x.\narXiv preprint\narXiv:2303.17568, 2023 .\n77\nZeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang.\nTreegen:\nA tree-based transformer architecture\nfor code generation.\nIn Proceedings\nof the AAAI Conference\non Artificial Intelligence, volume 34, pages 8984–8991, 2020 .\n78\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\nAbstract syntax networks for code generation and semantic parsing.\nIn\nRegina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics, ACL 2017,\nVancouver,\nCanada,\nJuly\n30 - August 4,\nVolume\n1:\nLong\nPapers, pages 1139–1149 . Association\nfor Computational Linguistics, 2017 .\n79\nHui Jiang, Chulun Zhou, Fandong Meng, Biao Zhang, Jie Zhou, Degen Huang, Qingqiang Wu, and Jinsong Su.\nExploring\ndynamic selection of branch expansion orders for code generation.\nIn Chengqing\nZong, Fei Xia, Wenjie Li, and Roberto\nNavigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 5076–5085 . Association for Computational Linguistics, 2021 .\n80\nSrinivasan Iyer, Alvin Cheung, and Luke Zettlemoyer.\nLearning\nprogrammatic\nidioms for scalable semantic parsing.\nIn\nKentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-\nIJCNLP 2019,\nHong\nKong,\nChina,\nNovember\n3-7,\n2019,\npages\n5425–5434 .\nAssociation\nfor\nComputational Linguistics,\n2019.\n81\nPengcheng Yin and Graham Neubig.\nA syntactic neural model for general-purpose code generation.\nIn Regina Barzilay and\nMin-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL\n2017,\nVancouver,\nCanada,\nJuly\n30\n- August\n4,\nVolume 1:\nLong Papers, pages 440–450 . Association for\nComputational\nLinguistics, 2017 .\n82\nPengcheng Yin and Graham Neubig.\nTRANX: A transition-based neural abstract syntax parser for semantic parsing and\ncode generation.\nIn\nEduardo\nBlanco and Wei Lu, editors,\nProceedings\nof\nthe\n2018\nConference\non\nEmpirical\nMethods\nin Natural Language Processing, EMNLP 2018:\nSystem Demonstrations, Brussels, Belgium,\nOctober 31 - November 4,\n2018, pages 7–12 . Association for Computational Linguistics, 2018 .\n83\nLi Dong and Mirella Lapata.\nLanguage to logical form with neural attention.\nIn\nProceedings\nof the\n54th Annual Meeting\nof the Association for\nComputational Linguistics, ACL 2016, August\n7-12,\n2016,\nBerlin,\nGermany,\nVolume\n1:\nLong\nPapers. The Association for Computer Linguistics, 2016 .\n84\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\nRoman, Zilin Zhang, and Dragomir R. Radev.\nSpider:\nA large-scale human-labeled dataset for complex and cross-domain\nsemantic parsing and text-to-sql task.\nIn\nEllen\nRiloff,\nDavid\nChiang,\nJulia\nHockenmaier,\nand\nJun’ichi\nTsujii,\neditors,\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\nOctober\n31 - November 4, 2018, pages 3911–3921 . Association for Computational Linguistics, 2018 .\n85\nAkshay Sethi, Anush Sankaran, Naveen Panwar, Shreya Khare, and Senthil Mani.\nDlpaper2code: Auto-generation of code\nfrom deep learning research papers.\nIn Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 32, 2018 .\n86\nGuang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang, Tingting Han, and Taolue Chen.\nExploitgen:\nTemplate-augmented\nSci China Inf Sci\n76\nexploit code generation based on codebert.\nJournal of Systems and Software, 197:111577, 2023 .\n87\nWang\nLing,\nPhil\nBlunsom,\nEdward\nGrefenstette,\nKarl\nMoritz Hermann,\nTom´as Kocisk´y, Fumin Wang, and Andrew W.\nSenior.\nLatent predictor networks for code generation.\nIn Proceedings\nof the\n54th Annual Meeting\nof the Association for\nComputational Linguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany,\nVolume\n1:\nLong\nPapers.\nThe Association\nfor Computer Linguistics, 2016.\n88\nChen Lyu, Ruyun Wang, Hongyu Zhang, Hanwen Zhang, and Songlin Hu.\nEmbedding api dependency graph for neural code\ngeneration.\nEmpirical Software\nEngineering, 26:1–51, 2021 .\n89\nColin B. Clement,\nDawn Drain,\nJonathan Timcheck,\nAlexey\nSvyatkovskiy,\nand\nNeel Sundaresan.\nPymt5:\nMulti-mode\ntranslation of natural language and python code with transformers.\nIn Bonnie Webber, Trevor Cohn, Yulan He, and Yang\nLiu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,\nOnline, November 16-20, 2020, pages 9052–9065 . Association for Computational Linguistics, 2020 .\n90\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi.\nCoderl:\nMastering code gen-\neration through pretrained models and deep reinforcement learning.\nAdvances in Neural Information Processing Systems ,\n35:21314–21328, 2022 .\n91\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi.\nCodet5:\nIdentifier-aware unified pre-trained encoder-decoder\nmodels for code understanding and generation.\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau\nYih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,\nVirtual Event / Punta Cana, Dominican Republic,\n7-11 November, 2021, pages 8696–8708 .\nAssociation for Computational\nLinguistics, 2021 .\n92\nYibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and Ming Zhou.\nSemantic\nparsing with syntax- and table-aware SQL generation.\nIn Iryna Gurevych and Yusuke Miyao, editors,\nProceedings\nof the\n56th Annual Meeting\nof the Association for Computational Linguistics, ACL 2018, Melbourne, Australia,\nJuly\n15-20,\n2018,\nVolume\n1:\nLong Papers, pages 361–372.\nAssociation for Computational Linguistics, 2018 .\n93\nXin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu. Compilable\nneural code generation with compiler feedback.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,\nFindings of the Association for Computational Linguistics:\nACL\n2022,\nDublin, Ireland, May 22-27, 2022, pages 9–19 .\nAssociation for Computational Linguistics, 2022 .\n94\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani.\nSynchromesh:\nReliable code generation from pre-trained language models.\nIn\nThe\nTenth\nInternational\nConference\non Learning Repre-\nsentations, ICLR 2022,\nVirtual Event, April 25-29, 2022 . OpenReview.net, 2022 .\n95\nBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin.\nCode generation as a dual task of code summarization.\nAdvances\nin\nneural information processing systems, 32, 2019 .\n96\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\nUnified pre-training for program understanding\nand generation.\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T¨ur, Iz Beltagy, Steven Bethard,\nRyan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American\nChapter of the Association for\nComputational Linguistics:\nHuman Language\nTechnologies, NAACL-HLT 2021,\nOnline,\nJune 6-11, 2021, pages 2655–2668 . Association for Computational Linguistics, 2021 .\n97\nWei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang.\nLeveraging code generation to improve\ncode retrieval and summarization via dual learning. In Proceedings of The Web Conference 2020, pages 2309–2319, 2020 .\n98\nTatsunori\nB\nHashimoto,\nKelvin\nGuu,\nYonatan\nOren,\nand\nPercy S Liang.\nA\nretrieve-and-edit\nframework for predicting\nstructured outputs.\nAdvances\nin Neural Information Processing Systems, 31, 2018 .\n99\nSumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang.\nSpoc:\nSearch-\nbased pseudocode to code.\nAdvances\nin Neural Information Processing Systems, 32, 2019 .\n100\nMd. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\nRetrieval augmented code\ngeneration and summarization.\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,\nFindings\nof the Association for\nComputational Linguistics:\nEMNLP 2021,\nVirtual\nEvent\n/ Punta\nCana,\nDominican\nRepublic,\n16-20 November, 2021, pages 2719–2734 . Association for Computational Linguistics, 2021 .\n101\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in programmatic context.\nIn Ellen Riloff, David Chiang,\nJulia Hockenmaier, and Jun’ichi Tsujii, editors,\nProceedings\nof\nthe\n2018\nConference\non\nEmpirical Methods\nin Natural Language\nProcessing,\nBrussels,\nBelgium,\nOctober\n31\n- November\n4,\n2018,\npages\n1643–\n1652. Association for Computational Linguistics, 2018 .\n102\nDaya Guo, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin.\nCoupling retrieval and meta-learning for context-dependent\nsemantic parsing.\nIn Anna\nKorhonen, David R. Traum, and Llu´ıs M`arquez, editors, Proceedings of the 57th Conference\nof the Association for\nComputational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,\nVolume\n1:\nLong\nPapers, pages 855–866 . Association for Computational Linguistics, 2019 .\n103\nJia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu.\nSkcoder:\nA sketch-based\napproach for automatic code\ngeneration.\narXiv preprint\narXiv:2302.06144, 2023 .\n104\nLi Dong and Mirella Lapata.\nCoarse-to-fine decoding for neural semantic parsing.\nIn Iryna Gurevych and Yusuke Miyao,\neditors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018,\nVolume 1:\nLong Papers, pages 731–742 . Association for Computational Linguistics, 2018 .\n105\nSijie Shen, Xiang Zhu, Yihong Dong,\nQizhi\nGuo, Yankun Zhen,\nand Ge Li.\nIncorporating\ndomain\nknowledge\nthrough\ntask augmentation for front-end javascript code generation.\nIn Proceedings\nof the\n30th\nACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering, pages 1533–1543, 2022 .\n106\nZeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu Zhang.\nA grammar-based structural cnn decoder for code\ngeneration.\nIn Proceedings\nof the AAAI conference on artificial intelligence, volume 33, pages 7055–7062, 2019 .\n107\nBinbin Xie, Jinsong Su, Yubin Ge, Xiang Li, Jianwei Cui, Junfeng Yao, and Bin Wang.\nImproving tree-structured decoder\ntraining\nfor code generation via mutual learning.\nIn\nProceedings\nof\nthe\nAAAI\nConference\non\nArtificial\nIntelligence ,\nvolume 35, pages 14121–14128, 2021 .\n108\nhttps://en.wikipedia.org/wiki/Long short-term memory.\n109\nhttp://www.gabormelli.com/RKB/Bidirectional LSTM (BiLSTM) Model.\n110\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.\nEmpirical evaluation of gated recurrent neural\nnetworks on sequence modeling.\narXiv preprint arXiv:1412.3555, 2014 .\n111\nChaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng. Practitioners’\nexpectations on code completion.\narXiv preprint\narXiv:2301.03846, 2023 .\nSci China Inf Sci\n77\n112\nFang Liu, Ge Li, Yunfei Zhao, and Zhi Jin.\nMulti-task learning based pre-trained language model for code completion.\nIn\nProceedings\nof the\n35th\nIEEE/ACM International\nConference\non\nAutomated\nSoftware\nEngineering,\nASE\n’20,\npage\n473–485, New York, NY, USA, 2021 . Association for Computing Machinery.\n113\nMaliheh Izadi,\nRoberta\nGismondi, and Georgios Gousios.\nCodefill:\nMulti-token code completion by jointly learning from\nstructure and naming sequences.\nIn\nProceedings\nof the\n44th\nInternational\nConference\non\nSoftware\nEngineering,\npages\n401–412, 2022 .\n114\nZe Tang, Jidong\nGe,\nShangqing\nLiu,\nTingwei\nZhu,\nTongtong\nXu,\nLiguo\nHuang,\nand\nBin\nLuo.\nDomain\nadaptive code\ncompletion via language models and decoupled domain databases.\narXiv preprint arXiv:2308.09313, 2023 .\n115\nZhensu Sun, Xiaoning Du, Fu Song, and Li Li.\nCodemark:\nImperceptible watermarking for code datasets\nagainst\nneural\ncode completion models.\narXiv preprint\narXiv:2308.14401, 2023 .\n116\nPengyu Nie, Rahul Banerjee, Junyi Jessy Li, Raymond J Mooney, and Milos Gligoric.\nLearning deep semantics for test\ncompletion.\narXiv preprint\narXiv:2302.10166, 2023 .\n117\nSamip Dahal, Adyasha Maharana, and Mohit Bansal.\nAnalysis of tree-structured architectures for code generation.\nIn\nFindings of the Association for\nComputational Linguistics:\nACL-IJCNLP 2021, pages 4382–4391, 2021 .\n118\nSajad Norouzi, Keyi Tang, and Yanshuai Cao.\nCode generation from natural language with less prior knowledge and more\nmonolingual data.\nIn Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 2:\nShort Papers), pages 776–785, 2021 .\n119\nAntonio\nMastropaolo,\nLuca\nPascarella,\nEmanuela\nGuglielmi,\nMatteo\nCiniselli,\nSimone\nScalabrino,\nRocco\nOliveto,\nand\nGabriele Bavota.\nOn the robustness of code generation techniques: An empirical study on github copilot.\narXiv preprint\narXiv:2302.00438, 2023 .\n120\nFrank F Xu, Bogdan Vasilescu, and Graham Neubig.\nIn-ide code generation from natural language:\nPromise and challenges.\nACM Transactions on Software Engineering and Methodology (TOSEM), 31(2):1–47, 2022.\n121\nQingyuan Liang, Zeyu Sun, Qihao Zhu, Wenjie Zhang, Lian Yu, Yingfei Xiong, and Lu Zhang.\nLyra:\nA\nbenchmark\nfor\nturducken-style code generation.\nIn Luc De Raedt, editor, Proceedings of the\nThirty-First International Joint\nConference\non Artificial Intelligence, IJCAI 2022,\nVienna, Austria, 23-29 July 2022, pages 4238–4244 . ijcai.org, 2022 .\n122\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik,\nHorace He, Dawn Song, and Jacob Steinhardt.\nMeasuring coding challenge competence with APPS. In Joaquin Vanschoren\nand Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks\n1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021 .\n123\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin\nJiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel\nSundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\nCodexglue:\nA machine learning benchmark dataset for code\nunderstanding and generation.\nIn Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information\nProcessing\nSystems\nTrack\non\nDatasets\nand\nBenchmarks\n1, NeurIPS\nDatasets\nand\nBenchmarks\n2021,\nDecember\n2021,\nvirtual, 2021 .\n124\nXinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang.\nIn chatgpt we trust?\nmeasuring and characterizing the\nreliability of chatgpt.\narXiv preprint arXiv:2304.08979, 2023 .\n125\nStacy K. Lukins, Nicholas A. Kraft, and Letha H. Etzkorn.\nSource code retrieval for bug localization using latent dirichlet\nallocation.\nIn Ahmed E. Hassan, Andy Zaidman, and Massimiliano Di Penta, editors,\nWCRE 2008, Proceedings of the\n15th\nWorking\nConference on Reverse Engineering, Antwerp, Belgium,\nOctober 15-18, 2008, pages 155–164 .\nIEEE Computer\nSociety, 2008 .\n126\nShaunak Chatterjee, Sudeep Juvekar, and Koushik Sen.\nSNIFF: A search engine for java using free-form queries. In Marsha\nChechik and Martin Wirsing, editors, Fundamental Approaches to Software Engineering, 12th International Conference,\nFASE 2009, Held as Part of the Joint European Conferences on\nTheory and Practice of Software, ETAPS 2009,\nYork,\nUK, March 22-29, 2009 . Proceedings, volume 5503 of Lecture Notes in Computer Science, pages 385–400 . Springer, 2009 .\n127\nEmily Hill, Manuel Roldan-Vega, Jerry Alan Fails, and Greg Mallet.\nNl-based query refinement and contextualized code\nsearch results: A user study.\nIn Serge Demeyer, Dave W. Binkley, and Filippo Ricca, editors, 2014 Software Evolution\nWeek\n- IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014, Antwerp,\nBelgium, February 3-6, 2014, pages 34–43. IEEE Computer Society, 2014 .\n128\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin\nJiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel\nSundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\nCodexglue:\nA machine learning benchmark dataset for code\nunderstanding and generation.\nIn Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information\nProcessing\nSystems\nTrack\non\nDatasets\nand\nBenchmarks\n1, NeurIPS\nDatasets\nand\nBenchmarks\n2021,\nDecember\n2021,\nvirtual, 2021 .\n129\nCollin McMillan, Mark Grechanik, Denys Poshyvanyk, Qing Xie, and Chen Fu.\nPortfolio:\nfinding relevant functions and\ntheir usage.\nIn Richard N. Taylor, Harald C. Gall, and Nenad Medvidovic, editors, Proceedings of the 33rd International\nConference on Software Engineering, ICSE 2011,\nWaikiki, Honolulu , HI,\nUSA, May 21-28, 2011, pages 111–120 . ACM,\n2011.\n130\nXuan Li,\nZerui\nWang,\nQianxiang\nWang,\nShoumeng\nYan,\nTao\nXie,\nand\nHong\nMei.\nRelationship-aware code search for\njavascript\nframeworks.\nIn\nThomas\nZimmermann,\nJane\nCleland-Huang,\nand\nZhendong\nSu, editors,\nProceedings\nof\nthe\n24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016, Seattle,\nWA,\nUSA,\nNovember 13-18, 2016, pages 690–701 . ACM, 2016 .\n131\nSaksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish Chandra.\nRetrieval on source code: a\nneural code search.\nIn Justin Gottschlich and Alvin Cheung, editors, Proceedings of the 2nd ACM SIGPLAN International\nWorkshop on Machine Learning and Programming Languages, MAPL@PLDI 2018, Philadelphia, PA,\nUSA, June 18-22,\n2018, pages 31–41 . ACM, 2018 .\n132\nYanzhen\nZou,\nChunyang\nLing,\nZeqi Lin, and Bing Xie.\nGraph embedding\nbased code search in software\nproject.\nIn\nProceedings of the 10th Asia-Pacific Symposium on Internetware, pages 1–10, 2018 .\n133\nZiyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun.\nStaqc:\nA systematically mined question-code dataset from stack\noverflow.\nIn\nProceedings\nof the\n2018\nWorld\nWide\nWeb\nConference\non\nWorld\nWide\nWeb,\nWWW 2018,\nLyon, France,\nApril 23-27, 2018, pages 1693–1703 . ACM, 2018 .\n134\nYao\nWan,\nJingdong\nShu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip S. Yu.\nMulti-modal attention net-\nwork learning for semantic source code retrieval.\nIn 34th IEEE/ACM International Conference on Automated Software\nSci China Inf Sci\n78\nEngineering, ASE 2019, San Diego, CA,\nUSA, November 11-15, 2019, pages 13–25.\nIEEE, 2019 .\n135\nPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig.\nLearning to mine aligned code and\nnatural language pairs from stack overflow.\nIn Andy\nZaidman, Yasutaka Kamei, and Emily Hill, editors,\nProceedings\nof\nthe 15th International Conference on Mining Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018 ,\npages 476–486 . ACM, 2018 .\n136\nShangqing Liu, Xiaofei Xie, Lei Ma, Jing Kai Siow, and Yang Liu.\nGraphsearchnet:\nEnhancing gnns via capturing global\ndependency for semantic code search.\nCoRR, abs/2111.02671, 2021 .\n137\nXiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and\nNan Duan.\nCoderetriever:\nUnimodal and bimodal contrastive learning.\narXiv preprint arXiv:2201.10866, 2022 .\n138\nHe Jiang, Liming Nie, Zeyi Sun, Zhilei Ren, Weiqiang Kong, Tao Zhang, and Xiapu Luo.\nROSF:\nleveraging information\nretrieval and supervised learning for recommending code snippets.\nIEEE\nTrans.\nServ.\nComput. , 12(1):34–46, 2019 .\n139\nDaya Guo,\nShuo\nRen,\nShuai\nLu,\nZhangyin\nFeng,\nDuyu\nTang,\nShujie\nLiu,\nLong Zhou, Nan Duan, Alexey Svyatkovskiy,\nShengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,\nand Ming Zhou.\nGraphcodebert:\nPre-training code representations with data flow.\nIn\n9th\nInternational\nConference\non\nLearning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021.\n140\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin.\nUnixcoder: Unified cross-modal pre-training for\ncode representation.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics\n(Volume\n1:\nLong\nPapers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 7212–7225 . Association for Computational Linguistics, 2022 .\n141\nHongyu Li, Seohyun Kim, and Satish Chandra.\nNeural code search evaluation dataset.\nCoRR, abs/1908.09804, 2019 .\n142\nHamel Husain,\nHo-Hsiang\nWu,\nTiferet\nGazit,\nMiltiadis\nAllamanis,\nand\nMarc\nBrockschmidt.\nCodesearchnet\nchallenge:\nEvaluating the state of semantic code search.\nCoRR, abs/1909.09436, 2019 .\n143\nGeert Heyman and Tom Van Cutsem.\nNeural\ncode\nsearch revisited:\nEnhancing code snippet retrieval through natural\nlanguage intent.\nCoRR, abs/2008.12193, 2020 .\n144\nSushil\nKrishna\nBajracharya, Trung Chi Ngo, Erik Linstead, Yimeng Dou, Paul Rigor, Pierre Baldi, and Cristina Videira\nLopes.\nSourcerer: a search engine for open source code supporting structure-based search.\nIn Peri L. Tarr and William R.\nCook, editors, Companion to the 21th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems,\nLanguages, and Applications, OOPSLA 2006, October 22-26, 2006, Portland, Oregon,\nUSA, pages 681–682 . ACM, 2006 .\n145\nMeili Lu, Xiaobing Sun,\nShaowei Wang, David Lo, and Yucong Duan.\nQuery\nexpansion via wordnet\nfor effective code\nsearch.\nIn Yann-Ga¨el Gu´eh´eneuc, Bram Adams, and Alexander Serebrenik, editors, 22nd IEEE International Conference\non\nSoftware\nAnalysis,\nEvolution,\nand\nReengineering,\nSANER\n2015,\nMontreal,\nQC,\nCanada,\nMarch 2-6,\n2015,\npages\n545–549. IEEE Computer Society, 2015 .\n146\nFei Lv, Hongyu Zhang, Jian-Guang Lou,\nShaowei Wang, Dongmei Zhang, and Jianjun Zhao.\nCodehow:\nEffective\ncode\nsearch based on API understanding and extended boolean model\n(E) .\nIn\nMyra\nB.\nCohen,\nLars\nGrunske,\nand\nMichael\nWhalen, editors, 30th IEEE/ACM International\nConference\non Automated Software Engineering, ASE 2015,\nLincoln,\nNE,\nUSA, November 9-13,\n2015, pages 260–270. IEEE Computer Society, 2015 .\n147\nMohammad Masudur Rahman.\nSupporting code search with context-aware, analytics-driven, effective query reformulation.\nIn Joanne M. Atlee, Tevfik Bultan, and Jon Whittle, editors, Proceedings of the 41st International\nConference on Software\nEngineering:\nCompanion Proceedings,\nICSE 2019,\nMontreal,\nQC,\nCanada,\nMay\n25-31,\n2019,\npages\n226–229 .\nIEEE\n/\nACM, 2019 .\n148\nEmily Hill, Lori L. Pollock, and K. Vijay-Shanker.\nImproving source code search with natural language phrasal represen-\ntations of method signatures.\nIn Perry Alexander, Corina S. Pasareanu, and John G. Hosking, editors, 26th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE 2011), Lawrence, KS,\nUSA, November 6-10, 2011 ,\npages 524–527. IEEE Computer Society, 2011 .\n149\nJason Liu, Seohyun Kim, Vijayaraghavan Murali, Swarat Chaudhuri, and Satish Chandra.\nNeural query expansion for code\nsearch.\nIn Tim Mattson, Abdullah Muzahid, and Armando Solar-Lezama, editors, Proceedings of the\n3rd ACM SIGPLAN\nInternational\nWorkshop\non\nMachine\nLearning\nand\nProgramming Languages,\nMAPL@PLDI 2019,\nPhoenix, AZ,\nUSA,\nJune 22, 2019, pages 29–37 . ACM, 2019 .\n150\nKaibo Cao, Chunyang Chen, Sebastian Baltes, Christoph Treude, and Xiang Chen.\nAutomated query reformulation for\nefficient search based on query logs from stack overflow.\nIn\n2021 IEEE/ACM 43rd International Conference on Software\nEngineering\n(ICSE), pages 1273–1285 . IEEE, 2021 .\n151\nShuhan Yan, Hang Yu, Yuting Chen, Beijun Shen, and Lingxiao Jiang. Are the code snippets what we are searching for? A\nbenchmark and an empirical study on code search with natural-language queries.\nIn Kostas Kontogiannis, Foutse Khomh,\nAlexander Chatzigeorgiou, Marios-Eleftherios Fokaefs, and Minghui Zhou, editors,\n27th\nIEEE\nInternational\nConference\non Software Analysis, Evolution and Reengineering, SANER 2020, London,\nON,\nCanada, February\n18-21, 2020, pages\n344–354. IEEE, 2020 .\n152\nSifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra.\nAroma: code recommendation via structural code\nsearch.\nProc.\nACM Program.\nLang. , 3(OOPSLA):152:1–152:28, 2019 .\n153\nGeorge Mathew and Kathryn T. Stolee.\nCross-language code search using static and dynamic analyses.\nIn Diomidis Spinellis,\nGeorgios\nGousios,\nMarsha\nChechik,\nand\nMassimiliano\nDi\nPenta,\neditors,\nESEC/FSE\n’21:\n29th\nACM\nJoint\nEuropean\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering, Athens,\nGreece, August\n23-28,\n2021, pages 205–217 . ACM, 2021 .\n154\nDaniel Perez and Shigeru Chiba.\nCross-language clone detection by learning over abstract syntax trees.\nIn 2019 IEEE/ACM\n16th International Conference on Mining Software Repositories (MSR), pages 518–528, 2019 .\n155\nTrong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and Tien N. Nguyen.\nExploring API embedding for API usages and\napplications.\nIn Sebasti´an Uchitel, Alessandro Orso, and Martin P. Robillard, editors, Proceedings of the 39th International\nConference\non Software Engineering, ICSE 2017,\nBuenos Aires, Argentina,\nMay 20-28,\n2017, pages 438–449 .\nIEEE\n/\nACM, 2017 .\n156\nBinger Chen and Ziawasch Abedjan.\nInteractive cross-language code retrieval with auto-encoders.\nIn 2021 36th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE), pages 167–178 . IEEE, 2021 .\n157\nJunjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan.\nCosqa:\n20, 000+\nweb queries for code search and question answering.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6,\nSci China Inf Sci\n79\n2021, pages 5690–5700 . Association for Computational Linguistics, 2021 .\n158\nMohammad Abdullah Matin Khan, M. Saiful Bari, Xuan Long Do, Weishi Wang, Md. Rizwan Parvez, and Shafiq R. Joty.\nxcodeeval:\nA\nlarge scale multilingual multitask benchmark for code understanding, generation, translation and retrieval.\nCoRR, abs/2303.03004, 2023 .\n159\nChong Wang, Xin Peng, Zhenchang Xing, Yue Zhang, Mingwei Liu, Rong Luo, and Xiujie Meng.\nXcos:\nExplainable code\nsearch based on query scoping and knowledge graph.\nACM\nTrans.\nSoftw.\nEng. Methodol. , 32(6):140:1–140:28, 2023 .\n160\nZhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li.\nOn the importance of building high-quality training datasets for neural\ncode search.\nIn 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA,\nUSA, May 25-27, 2022, pages 1609–1620 . ACM, 2022 .\n161\nAkhilesh Deepak Gotmare, Junnan Li, Shafiq R. Joty, and Steven C. H. Hoi.\nCascaded fast and slow models for efficient\nsemantic code search.\nCoRR, abs/2110.07811, 2021 .\n162\nWenchao\nGu,\nYanlin Wang,\nLun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Michael R. Lyu.\nAccelerating\ncode\nsearch with deep hashing and code classification.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,\nProceedings\nof the\n60th\nAnnual\nMeeting\nof the\nAssociation for\nComputational\nLinguistics\n(Volume\n1:\nLong\nPapers),\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages 2534–2544 . Association for Computational Linguistics, 2022 .\n163\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\nA neural attention model for abstractive sentence summarization.\nIn Llu´ıs M`arquez,\nChris\nCallison-Burch,\nJian\nSu,\nDaniele Pighin, and Yuval Marton, editors,\nProceedings of the\n2015\nConference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21,\n2015, pages 379–389 . The Association for Computational Linguistics, 2015 .\n164\nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav.\ncode2seq:\nGenerating sequences from structured representations\nof code.\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,\nUSA,\nMay 6-9,\n2019.\nOpenReview.net, 2019 .\n165\nKun Xu,\nLingfei Wu,\nZhiguo Wang,\nYansong Feng,\nand Vadim Sheinin.\nGraph2seq:\nGraph\nto\nsequence\nlearning with\nattention-based neural networks.\nCoRR, abs/1804.00823, 2018 .\n166\nGiriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay-Shanker.\nTowards automatically generating\nsummary comments for java methods.\nIn Charles Pecheur,\nJamie Andrews, and Elisabetta Di Nitto, editors, ASE 2010,\n25th\nIEEE/ACM International\nConference\non Automated Software Engineering, Antwerp, Belgium, September 20-24,\n2010, pages 43–52 . ACM, 2010 .\n167\nNahla J. Abid, Natalia Dragan, Michael L. Collard, and Jonathan I. Maletic.\nUsing stereotypes in the automatic generation\nof natural language summaries for C++ methods.\nIn Rainer Koschke, Jens Krinke, and Martin P. Robillard, editors,\n2015\nIEEE International\nConference on Software Maintenance and Evolution, ICSME 2015, Bremen,\nGermany, September\n29 -\nOctober 1, 2015, pages 561 –565 . IEEE Computer Society, 2015 .\n168\nSonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus.\nOn the use of automated text summarization techniques for\nsummarizing source code.\nIn Giuliano Antoniol, Martin Pinzger, and Elliot J. Chikofsky, editors, 17th\nWorking Conference\non Reverse Engineering,\nWCRE\n2010,\n13-16\nOctober\n2010,\nBeverly, MA,\nUSA, pages 35–44 . IEEE\nComputer\nSociety,\n2010.\n169\nSonia Haiduc, Jairo Aponte, and Andrian Marcus.\nSupporting program comprehension with source code summarization.\nIn\nJeff Kramer, Judith Bishop, Premkumar T. Devanbu, and Sebasti´an Uchitel, editors, Proceedings of the 32nd ACM/IEEE\nInternational\nConference\non Software\nEngineering - Volume\n2,\nICSE\n2010,\nCape\nTown,\nSouth\nAfrica,\n1-8\nMay\n2010 ,\npages 223–226. ACM, 2010 .\n170\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le.\nSequence to sequence learning with neural networks.\nIn Zoubin Ghahramani,\nMax Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Pro-\ncessing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal,\nQuebec,\nCanada, pages 3104–3112, 2014 .\n171\nMiltiadis\nAllamanis,\nHao\nPeng,\nand\nCharles\nSutton.\nA convolutional\nattention network for extreme summarization of\nsource code.\nIn Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International\nConference\non Machine Learning, ICML 2016,\nNew\nYork\nCity,\nNY,\nUSA,\nJune\n19-24,\n2016,\nvolume 48 of\nJMLR\nWorkshop\nand\nConference Proceedings, pages 2091–2100 . JMLR.org, 2016 .\n172\nWasi\nUddin\nAhmad,\nSaikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\nA transformer-based\napproach\nfor source\ncode summarization.\nIn Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the\n58th\nAnnual Meeting of the Association for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 4998–5007 .\nAssociation for Computational Linguistics, 2020 .\n173\nRuyun Wang, Hanwen Zhang, Guoliang Lu, Lei Lyu, and Chen Lyu.\nFret:\nFunctional reinforced transformer with BERT\nfor code summarization.\nIEEE Access, 8:135591–135604, 2020 .\n174\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu.\nRetrieval-based neural source code summarization.\nIn\nGregg Rothermel and Doo-Hwan Bae, editors, ICSE\n’20: 42nd International Conference on Software Engineering, Seoul,\nSouth Korea, 27 June - 19 July, 2020, pages 1385–1397 . ACM, 2020 .\n175\nAlexander LeClair, Aakash Bansal, and Collin McMillan.\nEnsemble models for neural source code summarization of subrou-\ntines.\nIn IEEE International Conference on Software Maintenance and Evolution, ICSME 2021, Luxembourg, September\n27 -\nOctober 1, 2021, pages 286–297 . IEEE, 2021 .\n176\nZi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng, and Zenglin Xu.\nSource code summarization with structural\nrelative position guided transformer.\nIn IEEE International\nConference on Software Analysis, Evolution and Reengineer-\ning, SANER 2022, Honolulu, HI,\nUSA, March\n15-18, 2022, pages 13–24 . IEEE, 2022 .\n177\nQingying Chen and Minghui Zhou.\nA\nneural framework for retrieval and summarization of source code.\nIn\nMarianne\nHuchard, Christian K¨astner, and Gordon Fraser, editors, Proceedings of the 33rd ACM/IEEE International\nConference on\nAutomated Software Engineering, ASE 2018, Montpellier, France, September 3-7, 2018, pages 826–831 . ACM, 2018 .\n178\nSiyuan Jiang, Ameer Armaly, and Collin McMillan.\nAutomatically generating commit messages from diffs using neural\nmachine translation.\nIn\nGrigore\nRosu,\nMassimiliano\nDi\nPenta,\nand\nTien\nN.\nNguyen,\neditors,\nProceedings\nof\nthe\n32nd\nIEEE/ACM International Conference on Automated Software Engineering, ASE 2017,\nUrbana, IL,\nUSA,\nOctober 30 -\nNovember 03, 2017, pages 135–146. IEEE Computer Society, 2017 .\n179\nSiyuan Jiang and Collin McMillan.\nTowards automatic generation of short summaries of commits.\nIn Giuseppe Scanniello,\nDavid Lo, and Alexander Serebrenik, editors, Proceedings of the 25th International Conference on Program Comprehen-\nsion, ICPC 2017, Buenos Aires, Argentina, May 22-23, 2017, pages 320–323 . IEEE Computer Society, 2017 .\n180\nShuyao Jiang.\nBoosting neural commit message generation with code semantic analysis.\nIn 34th IEEE/ACM International\nSci China Inf Sci\n80\nConference on Automated Software Engineering, ASE 2019, San Diego,\nCA,\nUSA,\nNovember 11-15, 2019, pages 1280–\n1282. IEEE, 2019 .\n181\nZhongxin Liu, Xin Xia, Christoph Treude, David Lo, and Shanping Li.\nAutomatic generation of pull request descriptions.\nIn 34th IEEE/ACM International\nConference\non Automated Software Engineering, ASE 2019,\nSan Diego,\nCA,\nUSA,\nNovember 11-15, 2019, pages 176–188. IEEE, 2019 .\n182\nAakash Bansal, Sakib Haque, and Collin McMillan.\nProject-level encoding for\nneural source code summarization of sub-\nroutines.\nIn 29th IEEE/ACM International Conference on Program\nComprehension, ICPC 2021, Madrid, Spain, May\n20-21, 2021, pages 253–264 . IEEE, 2021 .\n183\nRui Xie, Wei Ye, Jinan Sun, and Shikun Zhang.\nExploiting method names to improve code summarization:\nA deliberation\nmulti-task learning approach.\nIn\n29th IEEE/ACM International\nConference\non Program\nComprehension,\nICPC 2021,\nMadrid, Spain, May 20-21, 2021, pages 138–148 . IEEE, 2021 .\n184\nXing Hu,\nGe Li, Xin Xia,\nDavid Lo, and Zhi Jin.\nDeep code comment generation.\nIn Foutse Khomh,\nChanchal K. Roy,\nand Janet Siegmund, editors, Proceedings of the 26th Conference on Program Comprehension, ICPC 2018,\nGothenburg,\nSweden, May 27-28, 2018, pages 200–210 . ACM, 2018 .\n185\nXing\nHu,\nGe\nLi,\nXin\nXia,\nDavid\nLo, and Zhi Jin.\nDeep\ncode\ncomment generation with hybrid lexical and syntactical\ninformation.\nEmpir.\nSoftw.\nEng. , 25(3):2179–2217, 2020 .\n186\nYuan Huang, Shaohao Huang, Huanchao Chen, Xiangping Chen, Zibin Zheng, Xiapu Luo, Nan Jia, Xinyu Hu, and Xiaocong\nZhou.\nTowards automatically generating block comments for code snippets.\nInf.\nSoftw.\nTechnol. , 127:106373, 2020 .\n187\nZe Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, Zheling Zhu, and Bin Luo.\nAst-trans:\nCode summarization\nwith efficient tree-structured attention.\nIn 44th IEEE/ACM 44th\nInternational\nConference\non\nSoftware\nEngineering,\nICSE 2022, Pittsburgh, PA,\nUSA, May 25-27, 2022, pages 150–162.\nACM, 2022 .\n188\nShangqing Liu, Cuiyun Gao, Sen Chen, Lun Yiu Nie, and Yang Liu.\nATOM: commit message generation based on abstract\nsyntax tree and hybrid ranking.\nIEEE\nTrans. Software\nEng. , 48(5):1800–1817, 2022 .\n189\nYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu.\nImproving automatic source code\nsummarization via deep reinforcement learning.\nIn Marianne Huchard, Christian K¨astner, and Gordon Fraser, editors, Pro-\nceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018, Montpellier,\nFrance,\nSeptember 3-7, 2018, pages 397 –407. ACM, 2018 .\n190\nAlexander\nLeClair,\nSiyuan\nJiang,\nand\nCollin\nMcMillan.\nA neural\nmodel\nfor\ngenerating\nnatural language summaries of\nprogram subroutines.\nIn Joanne M. Atlee, Tevfik Bultan, and Jon Whittle, editors, Proceedings of the 41st International\nConference\non\nSoftware\nEngineering,\nICSE\n2019,\nMontreal,\nQC,\nCanada,\nMay\n25-31,\n2019,\npages\n795–806 .\nIEEE\n/\nACM, 2019 .\n191\nShengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, Hanghang Tong, and Jian Lu.\nCommit message generation for source\ncode changes.\nIn\nSarit\nKraus, editor,\nProceedings\nof the\nTwenty-Eighth\nInternational\nJoint\nConference\non\nArtificial\nIntelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 3975–3981 . ijcai.org, 2019 .\n192\nZheng Li, Yonghao Wu, Bin Peng, Xiang Chen, Zeyu Sun, Yong Liu, and Deli Yu.\nSecnn:\nA semantic CNN parser for code\ncomment generation.\nJ. Syst. Softw. , 181:111036, 2021 .\n193\nYuexiu Gao and Chen Lyu.\nM2TS: multi-scale multi-modal approach based on transformer for source code summarization.\nIn Ayushi Rastogi, Rosalia Tufano, Gabriele Bavota, Venera Arnaoudova, and Sonia Haiduc, editors,\nProceedings\nof the\n30th IEEE/ACM International\nConference on Program\nComprehension, ICPC 2022,\nVirtual Event,\nMay 16-17, 2022 ,\npages 24–35 . ACM, 2022.\n194\nWenhua Wang, Yuqun\nZhang,\nZhengran Zeng, and Guandong Xu.\nTransˆ3:\nA transformer-based framework for unifying\ncode summarization and code search.\nCoRR, abs/2003.03238, 2020 .\n195\nChen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li, and Rongxin Wu.\nImproving code summarization\nwith block-wise abstract syntax tree splitting.\nIn 29th IEEE/ACM International\nConference on Program\nComprehension,\nICPC 2021, Madrid, Spain, May 20-21, 2021, pages 184–195 . IEEE, 2021 .\n196\nEnsheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun.\nCAST: enhancing code\nsummarization with hierarchical splitting and reconstruction of abstract syntax trees.\nIn Marie-Francine Moens, Xuanjing\nHuang,\nLucia\nSpecia,\nand\nScott\nWen-tau\nYih,\neditors,\nProceedings\nof\nthe\n2021\nConference\non\nEmpirical\nMethods\nin\nNatural Language Processing, EMNLP 2021,\nVirtual Event / Punta\nCana, Dominican Republic,\n7-11 November, 2021 ,\npages 4053–4062 . Association for Computational Linguistics, 2021 .\n197\nPatrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt.\nStructured neural summarization.\nIn\n7th\nInternational\nConference on Learning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019 .\nOpenReview.net, 2019.\n198\nAlexander\nLeClair,\nSakib\nHaque,\nLingfei\nWu,\nand\nCollin\nMcMillan.\nImproved\ncode\nsummarization\nvia\na graph\nneural\nnetwork.\nIn\nICPC\n’20:\n28th\nInternational\nConference\non\nProgram\nComprehension,\nSeoul,\nRepublic\nof\nKorea,\nJuly\n13-15, 2020, pages 184–195 . ACM, 2020 .\n199\nShangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. Retrieval-augmented generation for code summarization\nvia hybrid GNN. In 9th International Conference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May\n3-7, 2021 . OpenReview.net, 2021.\n200\nXuye Liu, Dakuo Wang, April Yi Wang, Yufang Hou, and Lingfei Wu.\nHaconvgnn:\nHierarchical attention based convolu-\ntional graph neural network for code documentation generation in jupyter notebooks.\nIn Marie-Francine Moens, Xuanjing\nHuang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics:\nEMNLP\n2021,\nVirtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 4473–4485 . Association for Com-\nputational Linguistics, 2021 .\n201\nWuyan\nCheng,\nPo\nHu,\nShaozhi\nWei,\nand\nRan\nMo.\nKeyword-guided\nabstractive\ncode\nsummarization\nvia incorporating\nstructural and contextual information.\nInf.\nSoftw.\nTechnol. , 150:106987, 2022 .\n202\nJuncai\nGuo,\nJin Liu, Yao Wan, Li Li, and Pingyi Zhou.\nModeling hierarchical syntax structure with triplet position for\nsource code summarization.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings\nof the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 486–500 . Association for Computational Linguistics, 2022 .\n203\nZheng Ma, Yuexiu Gao, Lei Lyu, and Chen Lyu.\nMMF3:\nneural code summarization based on\nmulti-modal fine-grained\nfeature fusion.\nIn Fernanda Madeiral, Casper Lassenius, Tayana Conte, and Tomi M¨annist¨o, editors, ESEM\n’22: ACM /\nIEEE International Symposium on Empirical Software Engineering and Measurement, Helsinki Finland, September 19\n- 23, 2022, pages 171–182. ACM, 2022 .\n204\nYu Wang, Yu Dong, Xuesong Lu, and Aoying Zhou.\nGypsum:\nlearning hybrid representations for code summarization.\nIn\nSci China Inf Sci\n81\nAyushi Rastogi, Rosalia Tufano, Gabriele Bavota, Venera Arnaoudova, and Sonia Haiduc, editors, Proceedings of the 30th\nIEEE/ACM International Conference on Program Comprehension, ICPC 2022,\nVirtual Event, May 16-17, 2022, pages\n12–23. ACM, 2022 .\n205\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin.\nSummarizing source code with transferred API knowledge.\nIn\nJ´erˆome Lang, editor, Proceedings of the\nTwenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI\n2018, July 13-19, 2018, Stockholm, Sweden, pages 2269–2275 . ijcai.org, 2018 .\n206\nRamin Shahbazi, Rishab Sharma, and Fatemeh H. Fard.\nApi2com:\nOn the improvement of automatically generated code\ncomments using API documentations.\nIn 29th IEEE/ACM International\nConference on Program Comprehension, ICPC\n2021, Madrid, Spain, May 20-21, 2021, pages 411 –421 . IEEE, 2021 .\n207\nXuejian Gao, Xue Jiang,\nQiong Wu, Xiao Wang,\nChen Lyu, and Lei Lyu.\nGt-simnet:\nImproving code automatic summa-\nrization via multi-modal similarity networks.\nJ. Syst.\nSoftw. , 194:111495, 2022 .\n208\nYu Zhou, Xin Yan, Wenhua Yang, Taolue Chen, and Zhiqiu Huang.\nAugmenting java method comments generation with\ncontext information based on neural networks.\nJ. Syst. Softw. , 156:328–340, 2019 .\n209\nWenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip S. Yu, and Guandong Xu.\nReinforcement-\nlearning-guided source code summarization using hierarchical attention.\nIEEE\nTrans.\nSoftware\nEng. , 48(2):102–119, 2022.\n210\nYanlin Wang, Lun Du, Ensheng Shi, Yuxuan Hu, Shi Han, and Dongmei Zhang.\nCocogum:\nContextual code summarization\nwith multi-relational gnn on umls.\nMicrosoft,\nTech.\nRep.\nMSR-TR-2020-16, 2020 .\n211\nJikyoeng Son, Joonghyuk Hahn, HyeonTae Seo, and Yo-Sub Han.\nBoosting code summarization by embedding code struc-\ntures.\nIn\nNicoletta Calzolari,\nChu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo\nRyu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun\nHahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings\nof the 29th\nInternational\nConference\non\nComputational Linguistics,\nCOLING\n2022,\nGyeongju,\nRepublic\nof Korea,\nOctober\n12-17,\n2022, pages 5966–5977 . International Committee on Computational Linguistics, 2022 .\n212\nChunyan Zhang, Qinglei Zhou, Meng Qiao, Ke Tang, Lianqiu Xu, and Fudong Liu.\nRe trans:\nCombined\nretrieval and\ntransformer model for source code summarization.\nEntropy, 24(10):1372, 2022 .\n213\nYuan Huang, Jinbo Huang, Xiangping Chen, Kunning He, and Xiaocong Zhou.\nBcgen:\na comment generation method for\nbytecode.\nAutom.\nSoftw.\nEng. , 30(1):5, 2023 .\n214\nAntonio\nValerio\nMiceli Barone and Rico Sennrich.\nA parallel corpus of python functions and documentation strings for\nautomated code documentation and code generation.\nIn\nGreg\nKondrak and Taro Watanabe, editors,\nProceedings\nof the\nEighth International Joint Conference on Natural Language Processing, IJCNLP 2017,\nTaipei,\nTaiwan, November 27 -\nDecember 1, 2017, Volume 2: Short Papers, pages 314–319 . Asian Federation of Natural Language Processing, 2017 .\n215\nHanyang Guo, Xiangping Chen, Yuan Huang, Yanlin Wang, Xi Ding, Zibin Zheng, Xiaocong Zhou, and Hong-Ning Dai.\nSnippet comment generation based on code context expansion.\nACM\nTrans.\nSoftw.\nEng.\nMethodol. , 33(1), nov 2023 .\n216\nMartin Fowler, Kent Beck, John Brant, William Opdyke, and Don Roberts.\nRefactoring:\nImproving the Design of Existing\nCode. Addison-Wesley, 1999 .\n217\nNikolaos Tsantalis and Alexander Chatzigeorgiou.\nIdentification of move method refactoring opportunities.\nIEEE\nTrans-\nactions on Software Engineering, 35(3):347–367, 2009 .\n218\nRicardo Terra, Marco Tulio Valente, Sergio Miranda, and Vitor Sales.\nJMove:\nA novel heuristic and tool to detect move\nmethod refactoring opportunities.\nJournal of Systems and Software, 138:19–36, 2018 .\n219\nHui Liu, Zhifeng Xu, and Yanzhen Zou.\nDeep learning based feature envy detection.\nIn Proceedings of the 33rd ACM/IEEE\nInternational Conference on Automated Software Engineering, ASE 2018, pages 385–396 . ACM, 2018 .\n220\nZarina Kurbatova, Ivan Veselov, Yaroslav Golubev, and Timofey Bryksin.\nRecommendation of move\nmethod refactoring\nusing path-based rrepresentation of code.\nIn Proceedings of the 4th International\nWorkshop on Refactoring , IWoR 2020,\npages 315–322. ACM, 2020 .\n221\nTushar Sharma, Vasiliki Efstathiou, Panos Louridas, and Diomidis Spinellis.\nCode smell detection by deep direct-learning\nand transfer-learning.\nJournal of Systems and Software, 176:110936, 2021 .\n222\nHui Liu, Jiahao Jin, Zhifeng Xu, Yanzhen Zou, Yifan Bu, and Lu Zhang.\nDeep learning based code smell detection.\nIEEE\nTransactions\non Software Engineering, 47(9):1811–1837, 2021 .\n223\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep learning.\nnature, 521(7553):436–444, 2015 .\n224\nXizhao Wang, Yanxia Zhao, and Farhad Pourpanah.\nRecent advances in deep learning.\nInternational\nJournal\nof Machine\nLearning and Cybernetics, 11:747–750, 2020 .\n225\nAntoine Barbez, Foutse Khomh, and Yann-Ga¨el Gu´eh´eneuc.\nIn\nDeep Learning Anti-patterns from\nCode Metrics History ,\nICSME 2019, pages 114–124 . IEEE, 2019 .\n226\nDongjin Yu, Yihang Xu, Lehui Weng, Jie Chen, Xin Chen, and Quanxin Yang.\nIn Detecting\nand Refactoring Feature Envy\nbased on Graph Neural Network, ISSRE 2022, pages 458–469 . IEEE, 2022 .\n227\nUri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.\nCode2vec:\nLearning distributed representations of code.\nPro-\nceedings of the ACM on Programming Languages, 3(POPL):40:1–40:29, 2019 .\n228\nDi Cui, Siqi Wang, Yong Luo, Xingyu Li, Jie Dai, Lu Wang, and Qingshan Li.\nRMove:\nRecommending move method refac-\ntoring opportunities using structural and semantic representations of code.\nIn Proceedings\nof the 2022 IEEE International\nConference on Software Maintenance and Evolution, ICSME 2022, pages 281–292 . IEEE, 2022 .\n229\nRahul Yedida and Tim Menzies.\nOn\nthe value of oversampling\nfor deep learning in software defect prediction.\nIEEE\nTransactions\non Software Engineering, 48(8):3103–3116, 2022 .\n230\nRahul Yedida and Tim Menzies.\nHow\nto\nimprove\ndeep\nlearning for software\nanalytics:\n(a case\nstudy with code smell\ndetection) .\nIn\nProceedings\nof\nthe\n19th\nInternational\nConference\non Mining\nSoftware\nRepositories,\nMSR\n2022,\npages\n156–166. ACM, 2022 .\n231\nHui Liu, Qiurong Liu, Yang Liu, and Zhouding Wang.\nIdentifying renaming opportunities by expanding conducted rename\nrefactorings.\nIEEE\nTransactions\non Software Engineering, 41(9):887–900, 2015 .\n232\nJiahui Liang, Weiqin Zou, Jingxuan Zhang, Zhiqiu Huang, and Chenxing Sun.\nA\ndeep method renaming prediction and\nrefinement approach for Java projects.\nIn Proceedings of the 21st International Conference on Software Quality, Reliability\nand Security), QRS 2021, pages 404–413 . IEEE, 2021 .\n233\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova.\nBert:\nPre-training of deep bidirectional transformers\nfor language understanding.\nIn Proceedings\nof the\n2019\nConference\nof the North American\nChapter of the Association\nfor Computational Linguistics:\nHuman Language\nTechnologies, NAACL-HLT 2019, pages 4171–4186 . ACL, 2019 .\n234\nSara Rosenthal, Noura Farra, and Preslav Nakov.\nSemeval-2017 task 4:\nSentiment analysis in twitter.\nIn Proceedings of\nSci China Inf Sci\n82\nthe 11th International\nWorkshop on Semantic Evaluation, SemEval 2017, pages 502–518. ACL, 2017 .\n235\nKui Liu, Dongsun Kim, Tegawend´e F Bissyand´e, Taeyoung Kim, Kisub Kim, Anil Koyuncu, Suntae Kim, and Yves Le Traon.\nLearning to spot and refactor inconsistent method names.\nIn Proceedings of the 41st International\nConference on Software\nEngineering, ICSE 2019, pages 1–12 . IEEE, 2019 .\n236\nQuoc Le and Tomas Mikolov.\nDistributed representations of sentences and documents.\nIn Proceedings\nof the\n31st Interna-\ntional Conference on Machine Learning, PMLR 2014, pages 1188–1196 . IMLS, 2014 .\n237\nMichele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk.\nOn\nlearning meaningful\ncode changes via neural machine translation.\nIn Proceedings of the 41st International Conference on Software Engineering,\nICSE 2019, Montreal, QC, Canada, May 25-31, 2019, pages 25–36 . IEEE / ACM, 2019 .\n238\nAlly S Nyamawe, Hui Liu, Nan Niu, Qasim Umer, and Zhendong Niu.\nFeature requests-based recommendation of software\nrefactorings.\nEmpirical Software\nEngineering, 25:4315–4347, 2020 .\n239\nEman Abdullah AlOmar, Anton Ivanov, Zarina Kurbatova, Yaroslav Golubev, Mohamed Wiem Mkaouer, Ali Ouni, Timofey\nBryksin, Le Nguyen, Amit Kini, and Aditya Thakur.\nJust-in-time code duplicates extraction.\nInformation and Software\nTechnology, 158:107169, 2023 .\n240\nChi Xiaye, Liu Hui, Li Guangjie, Wang Weixiao, Xia Yunni, Jiang Yanjie, Zhang Yuxia, and Ji Weixing.\nAn automated\napproach to extracting local variables.\nIn Proceedings of the\n31th ACM Joint European Software Engineering Conference\nand\nSymposium\non\nthe\nFoundations\nof Software\nEngineering,\nESEC/FSE\n2023,\nSan\nFrancisco,\nCalifornia,\nUSA,\n2023.\nACM.\n241\nUtkarsh Desai, Sambaran Bandyopadhyay, and Srikanth Tamilselvam. Graph neural network to dilute outliers for refactoring\nmonolith application.\nIn Proceedings\nof the AAAI Conference\non Artificial Intelligence, volume 35 of AAAI 2021, pages\n72–80, 2021 .\n242\nLech Madeyski and Tomasz Lewowski.\nMlcq:\nIndustry-relevant code smell data set.\nIn Proceedings\nof the\n24th Evaluation\nand Assessment in Software Engineering, EASE 2010, pages 342–347 . ACM, 2020 .\n243\nLiu Bo, Liu Hui, Li Guangjie, Niu Nan, Xu Zimao, Wang Yifan, Xia Yunni, Zhang Yuxia, and Jiang Yanjie.\nDeep learning\nbased feature envy detection boosted by real-world examples.\nIn Proceedings of the 31th ACM Joint European Software\nEngineering\nConference\nand\nSymposium\non the Foundations\nof Software Engineering , ESEC/FSE 2023, San Francisco,\nCalifornia, USA, 2023 . ACM.\n244\nNikolaos Tsantalis, Ameya Ketkar, and Danny Dig.\nRefactoringMiner 2.0 .\nIEEE\nTransactions\non Software Engineering ,\n48(3):930–950, 2022 .\n245\nDanilo Silva, Jo˜ao Silva, Gustavo Jansen De Souza Santos, Ricardo Terra, and Marco Tulio O Valente.\nRefDiff 2.0:\nA\nmulti-language refactoring detection tool.\nIEEE\nTransactions\non Software Engineering, 47(12):2786–2802, 2021 .\n246\nMiryung Kim, Matthew Gee, Alex Loh, and Napol Rachatasumrit.\nRef-Finder:\nA refactoring reconstruction tool based on\nlogic query templates.\nIn Proceedings of the\n18th ACM SIGSOFT International Symposium on Foundations\nof Software\nEngineering, FSE 2010, pages 371–372, Santa Fe, NM, USA, 2010 . ACM.\n247\nXin Yin,\nChongyang\nShi, and Shuxin Zhao.\nLocal\nand global feature based explainable feature envy detection.\nIn\n2021\nIEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021, pages 942–951 . IEEE, 2021 .\n248\nLiu Bo, Liu Hui, Li Guangjie, Niu Nan, Zhang Yuxia, Li Guangjie, and Jiang Yanjie.\nAutomated software entity matching\nbetween successive versions.\nIn\nProceedings\nof the\n38th\nIEEE/ACM\nInternational\nConference\non\nAutomated\nSoftware\nEngineering, ASE 2023, Kirchberg, Luxembourg, 2023 . IEEE.\n249\nJeffrey Svajlenko, Judith F. Islam, Iman Keivanloo, Chanchal K. Roy, and Mohammad Mamun Mia.\nTowards a big data\ncurated benchmark of inter-project code clones.\nIn 2014 IEEE International\nConference\non Software Maintenance\nand\nEvolution, pages 476–480, 2014 .\n250\nMuslim Chochlov, Gul Aftab Ahmed, James Vincent Patten, Guoxian Lu, Wei Hou, David Gregg, and Jim Buckley.\nUsing a\nnearest-neighbour, bert-based approach for scalable clone detection.\nIn\n2022 IEEE International\nConference on Software\nMaintenance and Evolution (ICSME), pages 582–591, 2022 .\n251\nHitesh Sajnani, Vaibhav Saini, Jeffrey Svajlenko, Chanchal K. Roy, and Cristina V. Lopes.\nSourcerercc:\nScaling code clone\ndetection to big-code.\nICSE ’16, page 1157–1168, New York, NY, USA, 2016 . Association for Computing Machinery.\n252\nSaad Arshad, Shamsa Abid, and Shafay Shamail.\nCodebert for code clone detection:\nA replication study.\nIn 2022 IEEE\n16th International\nWorkshop on Software\nClones\n(IWSC), pages 39–45, 2022 .\n253\nNikita\nMehrotra,\nNavdha\nAgarwal,\nPiyush Gupta, Saket Anand, David Lo, and Rahul Purandare.\nModeling functional\nsimilarity in source code with graph-based siamese networks.\nIEEE\nTransactions on Software Engineering, 48(10):3771–\n3789, 2022 .\n254\nHao Yu, Wing Lam,\nLong Chen,\nGe Li,\nTao Xie,\nand\nQianxiang Wang.\nNeural\ndetection\nof\nsemantic code clones via\ntree-based convolution.\nIn 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC), pages\n70–80, 2019 .\n255\nZhipeng Xue, Zhijie Jiang, Chenlin Huang, Rulin Xu, Xiangbing Huang, and Liumin Hu.\nSeed:\nSemantic graph based deep\ndetection for type-4 clone.\nIn Gilles Perrouin, Naouel Moha, and Abdelhak-Djamel Seriai, editors,\nReuse\nand\nSoftware\nQuality, pages 120–137, Cham, 2022 . Springer International Publishing.\n256\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu.\nA novel neural source code rep-\nresentation based on abstract syntax tree.\nIn 2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE), pages 783–794, 2019 .\n257\nS. Karthik and B. Rajdeepa.\nA collaborative method for code clone detection using a deep learning model.\nAdvances\nin\nEngineering Software, 174:103327, 2022 .\n258\nMing Wu, Pengcheng Wang, Kangqi Yin, Haoyu Cheng, Yun Xu, and Chanchal K. Roy.\nLvmapper:\nA large-variance clone\ndetector using sequencing alignment approach. IEEE Access, 8:27986–27997, 2020 .\n259\nWei Hua, Yulei Sui, Yao Wan, Guangzhong Liu, and Guandong Xu.\nFcca:\nHybrid code representation for functional clone\ndetection using attention networks.\nIEEE\nTransactions\non Reliability, 70(1):304–318, 2021 .\n260\nYoung-Bin\nJo,\nJihyun\nLee,\nand\nCheol-Jung\nYoo.\nTwo-pass technique\nfor clone detection\nand type classification using\ntree-based convolution neural network.\nApplied Sciences, 11(14), 2021 .\n261\nBingzhuo Li, Chunyang Ye, Shouyang Guan, and Hui Zhou.\nSemantic code clone detection via event embedding tree and\ngat network.\nIn 2020 IEEE 20th International\nConference on Software\nQuality, Reliability and Security\n(QRS), pages\n382–393, 2020 .\n262\nLiuqing Li, He Feng, Wenjie Zhuang, Na Meng, and Barbara Ryder.\nCclearner:\nA deep\nlearning-based clone detection\napproach.\nIn\n2017 IEEE\nInternational\nConference\non\nSoftware\nMaintenance\nand Evolution\n(ICSME), pages 249–260,\nSci China Inf Sci\n83\n2017.\n263\nLingxiao\nJiang,\nGhassan Misherghi, Zhendong Su,\nand Stephane Glondu.\nDeckard:\nScalable and accurate\ntree-based\ndetection of code clones.\nIn 29th International\nConference on Software Engineering\n(ICSE’07), pages 96–105, 2007 .\n264\nJeffrey Svajlenko and Chanchal K. Roy.\nFast and flexible large-scale clone detection with cloneworks.\nIn\n2017 IEEE/ACM\n39th International Conference on Software Engineering Companion (ICSE-C), pages 27–30, 2017 .\n265\nAiping Zhang, Kui Liu, Liming Fang, Qianjun Liu, Xinyu Yun, and Shouling Ji.\nLearn to align:\nA code alignment network\nfor code clone detection.\nIn 2021\n28th Asia-Pacific Software Engineering Conference\n(APSEC), pages 1–11, 2021 .\n266\nMartin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk.\nDeep learning code fragments for code clone\ndetection.\nIn Proceedings\nof the\n31st IEEE/ACM International\nConference\non Automated Software Engineering, ASE\n’16, page 87–98, New York, NY, USA, 2016. Association for Computing Machinery.\n267\nHui-Hui Wei and\nMing\nLi.\nSupervised\ndeep\nfeatures\nfor\nsoftware functional clone detection by exploiting\nlexical\nand\nsyntactical information in source code.\nIn Proceedings of the 26th International Joint Conference on Artificial Intelligence ,\nIJCAI’17, page 3034 –3040 . AAAI Press, 2017 .\n268\nGang Zhao and Jeff Huang.\nDeepsim:\nDeep learning code functional similarity.\nESEC/FSE 2018, page 141–151, New York,\nNY, USA, 2018 . Association for Computing Machinery.\n269\nDong Kwan KIM.\nA\ndeep\nneural\nnetwork-based\napproach\nto\nfinding similar code segments.\nIEICE\nTransactions\non\nInformation and Systems, E103.D(4):874–878, 2020 .\n270\nChanchal K. Roy and James R. Cordy.\nNicad: Accurate detection of near-miss intentional clones using flexible pretty-printing\nand code normalization.\nIn 2008\n16th IEEE International Conference on Program Comprehension, pages 172–181, 2008 .\n271\nYueming Wu, Deqing Zou, Shihan Dou, Siru Yang, Wei Yang, Feng Cheng, Hong Liang, and Hai Jin.\nScdetector:\nSoftware\nfunctional clone detection based on semantic tokens analysis.\nIn\n2020\n35th\nIEEE/ACM\nInternational\nConference\non\nAutomated Software Engineering (ASE), pages 821–833, 2020 .\n272\nChenhui Feng, Tao Wang, Yue Yu, Yang Zhang, Yanzhi Zhang, and Huaimin Wang.\nSia-rae:\nA siamese network based on\nrecursive autoencoder for effective clone detection.\nIn 2020 27th Asia-Pacific Software Engineering\nConference (APSEC) ,\npages 238–246, 2020 .\n273\nJie Zeng, Kerong Ben, Xiaowei Li, and Xian Zhang.\nFast code clone detection based on weighted recursive autoencoders.\nIEEE Access, 7:125062–125078, 2019 .\n274\nYuan Yuan, Weiqiang Kong,\nGang Hou, Yan Hu,\nMasahiko Watanabe, and Akira Fukuda.\nFrom local to global semantic\nclone detection.\nIn\n2019\n6th\nInternational\nConference\non\nDependable\nSystems\nand\nTheir\nApplications\n(DSA),\npages\n13–24, 2020 .\n275\nWenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin.\nDetecting code clones with graph neural network and flow-augmented\nabstract syntax tree.\nIn 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering\n(SANER), pages 261–271, 2020 .\n276\nChunrong Fang, Zixi Liu, Yangyang Shi, Jeff Huang, and Qingkai Shi.\nFunctional\ncode clone detection with syntax and\nsemantics fusion learning.\nIn Proceedings of the 29th ACM SIGSOFT International Symposium\non Software Testing and\nAnalysis, ISSTA 2020, page 516–527, New York, NY, USA, 2020 . Association for Computing Machinery.\n277\nChenkai Guo, Hui Yang, Dengrong Huang, Jianwen Zhang, Naipeng Dong, Jing Xu, and Jingwen Zhu.\nReview sharing via\ndeep semi-supervised code clone detection.\nIEEE Access, 8:24948–24965, 2020 .\n278\nVaibhav Saini, Farima Farmahinifarahani, Yadong Lu, Pierre Baldi, and Cristina V. Lopes.\nOreo:\nDetection of clones in the\ntwilight zone.\nESEC/FSE 2018, page 354–365, New York, NY, USA, 2018 .\nAssociation for Computing Machinery.\n279\nYao\nMeng\nand Long Liu.\nA deep\nlearning approach for a source code detection model using self-attention.\nComplexity,\n2020:24948–24965, 2020 .\n280\nYan\nYa\nZhang and Ming Li.\nFind\nme if you can:\nDeep software clone detection by exploiting the contest between the\nplagiarist and the detector. volume 33, pages 5813–5820, 2019 .\n281\nLutz B¨uch and Artur Andrzejak.\nLearning-based recursive aggregation of abstract syntax trees for code clone detection.\nIn\n2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER), pages 95–104,\n2019.\n282\nCong Wang, Jian Gao, Yu Jiang, Zhenchang Xing, Huafeng Zhang, Weiliang Yin, Ming Gu, and Jiaguang Sun.\nGo-clone:\nGraph-embedding based clone detector for golang.\nIn Proceedings of the 28th ACM SIGSOFT International Symposium\non\nSoftware\nTesting\nand\nAnalysis,\nISSTA\n2019,\npage\n374–377,\nNew York, NY, USA, 2019 .\nAssociation for Computing\nMachinery.\n283\nHeyuan Shi, Runzhe Wang, Ying Fu, Yu Jiang, Jian Dong, Kun Tang, and Jiaguang Sun.\nVulnerable code clone detection for\noperating system through correlation-induced learning.\nIEEE Transactions on Industrial Informatics, 15(12):6551–6559,\n2019.\n284\nSeulbae Kim, Seunghoon Woo, Heejo Lee, and Hakjoo Oh.\nVuddy: A scalable approach for vulnerable code clone discovery.\nIn 2017 IEEE Symposium on Security and Privacy (SP), pages 595–614, 2017 .\n285\nAbdullah Sheneamer.\nCcdlc detection framework-combining clustering with deep learning classification for semantic clones.\nIn 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 701–706, 2018 .\n286\nHui-Hui Wei and Ming Li. Positive and unlabeled learning for detecting software functional clones with adversarial training.\nIn\nProceedings\nof the\n27th International Joint\nConference\non Artificial\nIntelligence, IJCAI’18, page 2840–2846 .\nAAAI\nPress, 2018 .\n287\nAbdullah Sheneamer and Jugal Kalita.\nSemantic clone detection using machine learning.\nIn 2016 15th IEEE International\nConference on Machine Learning and Applications (ICMLA), pages 1024–1028, 2016 .\n288\nJing Kai Siow, Shangqing Liu, Xiaofei Xie, Guozhu Meng, and Yang Liu.\nLearning program semantics with code represen-\ntations: An empirical study.\nIn 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering\n(SANER), pages 554–565, 2022 .\n289\nMichele Tufano,\nCody Watson,\nGabriele Bavota,\nMassimiliano Di Penta,\nMartin White,\nand Denys Poshyvanyk.\nDeep\nlearning similarities from different representations of source code.\nIn\n2018 IEEE/ACM 15th International\nConference on\nMining Software Repositories\n(MSR), pages 542–553, 2018 .\n290\nDeze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao.\nBridging pre-trained models\nand downstream tasks for source code understanding.\nIn\n2022\nIEEE/ACM 44th\nInternational\nConference\non\nSoftware\nEngineering\n(ICSE), pages 287–298, 2022 .\n291\nS. Karakatiˇc, A. Miloˇseviˇc, and T Heriˇcko. Software system comparison with semantic source code embeddings.\nEmpirical\nSoftware Engineering, 27(70), 2022 .\nSci China Inf Sci\n84\n292\nNghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang.\nInfercode:\nSelf-supervised learning of code representations by predicting\nsubtrees.\nIn 2021 IEEE/ACM 43rd International\nConference on Software Engineering\n(ICSE), pages 1186–1197, 2021 .\n293\nQiong Wu, Xue Jiang, Zhuoran Zheng, Xuejian Gao, Chen Lyu, and Lei Lyu.\nCode representation based on hybrid graph\nmodelling.\nIn Teddy Mantoro, Minho Lee, Media Anugerah Ayu, Kok Wai Wong, and Achmad Nizar Hidayanto, editors,\nNeural Information Processing, pages 298–306, Cham, 2021 . Springer International Publishing.\n294\nLili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.\nConvolutional neural networks over tree structures for programming\nlanguage processing. AAAI’16, page 1287–1293 . AAAI Press, 2016 .\n295\nLong Chen, Wei Ye, and Shikun Zhang.\nCapturing source code semantics via tree-based convolution over api-enhanced ast.\nIn Proceedings of the 16th ACM International Conference on Computing Frontiers, CF ’19, page 174–182, New York, NY,\nUSA, 2019 . Association for Computing Machinery.\n296\nYi Gao, Zan Wang, Shuang Liu, Lin Yang, Wei Sang, and Yuanfang Cai.\nTeccd:\nA tree embedding approach for code clone\ndetection.\nIn\n2019\nIEEE International\nConference\non\nSoftware\nMaintenance\nand Evolution\n(ICSME), pages\n145–156,\n2019.\n297\nPengcheng Wang, Jeffrey Svajlenko, Yanzhao Wu, Yun Xu, and Chanchal K. Roy.\nCcaligner:\nA token based large-gap clone\ndetector.\nIn 2018 IEEE/ACM 40th International\nConference on Software Engineering\n(ICSE), pages 1066–1077, 2018 .\n298\nRicardo Terra, Luis Fernando Miranda, Marco Tulio Valente, and Roberto S. Bigonha.\nQualitas.class corpus:\nA compiled\nversion of the qualitas corpus.\nSIGSOFT Softw.\nEng.\nNotes, 38(5):1–4, aug 2013 .\n299\nMohammad A. Yahya and Dae-Kyoo Kim.\nClcd-i:\nCross-language clone detection by using deep learning with infercode.\nComputers, 12(1), 2023 .\n300\nKawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K. Roy, and Kevin A. Schneider.\nClcdsa:\nCross language\ncode clone detection using syntactical features and api documentation.\nIn 2019 34th IEEE/ACM International\nConference\non Automated Software Engineering\n(ASE), pages 1026–1037, 2019 .\n301\nNghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang.\nBilateral dependency\nneural networks for cross-language algorithm clas-\nsification.\nIn 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering\n(SANER) ,\npages 422–433, 2019 .\n302\nKesu Wang, Meng Yan, He Zhang, and Haibo Hu.\nUnified abstract syntax tree representation learning for cross-language\nprogram classification.\nIn 2022 IEEE/ACM 30th International\nConference\non Program\nComprehension\n(ICPC), pages\n390–400, 2022 .\n303\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah.\nSignature verification using a ”siamese”\ntime delay neural network.\nIn Proceedings of the 6th International Conference on Neural Information Processing Systems ,\nNIPS’93, page 737–744, San Francisco, CA, USA, 1993 . Morgan Kaufmann Publishers Inc.\n304\nTijana Vislavski, Gordana Raki´c, Nicol´as Cardozo, and Zoran Budimac.\nLicca:\nA tool for cross-language clone detection. In\n2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER), pages 512–516,\n2018.\n305\nXiao Cheng, Zhiming Peng, Lingxiao Jiang, Hao Zhong, Haibo Yu, and Jianjun Zhao.\nMining revision histories to detect\ncross-language clones without intermediates.\nIn 2016 31st IEEE/ACM International\nConference\non Automated Software\nEngineering\n(ASE), pages 696–701, 2016 .\n306\nHongfa Xue,\nGuru Venkataramani,\nand Tian Lan.\nClone-slicer:\nDetecting\ndomain\nspecific\nbinary code clones through\nprogram slicing.\nIn\nProceedings\nof\nthe\n2018\nWorkshop\non\nForming\nan\nEcosystem\nAround\nSoftware\nTransformation ,\nFEAST ’18, page 27–33, New York, NY, USA, 2018 . Association for Computing Machinery.\n307\nXiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song.\nNeural network-based graph embedding for cross-\nplatform binary code similarity detection.\nCCS ’17, page 363–376, New York, NY, USA, 2017 .\nAssociation for Computing\nMachinery.\n308\nNiccol`o\nMarastoni,\nRoberto\nGiacobazzi,\nand\nMila\nDalla\nPreda.\nA deep\nlearning\napproach\nto program similarity.\nIn\nProceedings\nof the\n1st\nInternational\nWorkshop\non\nMachine\nLearning\nand\nSoftware\nEngineering\nin Symbiosis, MASES\n2018, page 26–35, New York, NY, USA, 2018 . Association for Computing Machinery.\n309\nHongfa Xue, Guru Venkataramani, and Tian Lan.\nClone-hunter:\nAccelerated bound checks elimination via binary code clone\ndetection.\nIn Proceedings of the 2nd ACM SIGPLAN International\nWorkshop on Machine Learning and Programming\nLanguages, MAPL 2018, page 11–19, New York, NY, USA, 2018 . Association for Computing Machinery.\n310\nQian Feng, Rundong Zhou, Chengcheng Xu, Yao Cheng, Brian Testa, and Heng Yin.\nScalable graph-based bug search for\nfirmware images.\nIn Proceedings\nof the\n2016 ACM SIGSAC\nConference\non\nComputer\nand\nCommunications\nSecurity ,\nCCS ’16, page 480–491, New York, NY, USA, 2016 . Association for Computing Machinery.\n311\nGolam Mostaeen, Jeffrey Svajlenko, Banani Roy, Chanchal K. Roy, and Kevin A. Schneider.\nOn the use of machine learning\ntechniques towards the design of cloud based automatic code clone validation tools.\nIn\n2018\nIEEE\n18th\nInternational\nWorking Conference on Source Code Analysis and Manipulation (SCAM), pages 155–164, 2018 .\n312\nVaibhav Saini, Farima Farmahinifarahani, Yadong Lu, Di Yang, Pedro Martins, Hitesh Sajnani, Pierre Baldi, and Cristina V.\nLopes.\nTowards automating precision studies of clone detectors.\nIn 2019 IEEE/ACM 41st International Conference on\nSoftware Engineering (ICSE), pages 49–59, 2019 .\n313\nChenyao Liu, Zeqi Lin, Jian-Guang Lou, Lijie Wen, and Dongmei Zhang.\nCan neural clone detection generalize to unseen\nfunctionalities?\nIn 2021\n36th IEEE/ACM International\nConference on Automated Software Engineering\n(ASE), pages\n617–629, 2021 .\n314\nHao Yu, Xing Hu,\nGe Li, Ying Li,\nQianxiang Wang, and Tao Xie.\nAssessing\nand\nimproving an evaluation dataset for\ndetecting semantic code clones via deep learning.\nACM\nTrans.\nSoftw.\nEng.\nMethodol. , 31(4), jul 2022 .\n315\nJens Krinke and Chaiyong Ragkhitwetsagul.\nBigclonebench considered harmful for machine learning.\nIn 2022 IEEE 16th\nInternational\nWorkshop on Software\nClones\n(IWSC), pages 1–7, 2022 .\n316\nFarouq Al-Omari, Chanchal K. Roy, and Tonghao Chen.\nSemanticclonebench:\nA semantic code clone benchmark using\ncrowd-source knowledge.\nIn 2020 IEEE\n14th International\nWorkshop on Software\nClones\n(IWSC), pages 57–63, 2020 .\n317\nMarius Kamp, Patrick Kreutzer, and Michael Philippsen.\nSesame: A data set of semantically similar java methods.\nIn 2019\nIEEE/ACM 16th International Conference on Mining Software Repositories (MSR), pages 529–533, 2019 .\n318\nXinli Yang,\nDavid Lo, Xin Xia, Yun Zhang,\nand Jianling Sun.\nDeep learning for just-in-time defect prediction.\nIn\n2015\nIEEE International Conference on Software Quality, Reliability and Security, pages 17–26. IEEE, 2015 .\n319\nAnh Viet Phan, Minh Le Nguyen, and Lam Thu Bui.\nConvolutional neural networks over control flow graphs for software\ndefect prediction.\nIn 2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI), pages 45–52,\n2017.\nSci China Inf Sci\n85\n320\nJian Li, Pinjia He, Jieming Zhu, and Michael R. Lyu.\nSoftware defect prediction via convolutional neural network.\nIn 2017\nIEEE International Conference on Software Quality, Reliability and Security (QRS), pages 318–328, 2017 .\n321\nXuan Huo, Yang Yang,\nMing Li,\nand De-Chuan Zhan.\nLearning semantic features for software defect prediction by code\ncomments embedding.\nIn 2018 IEEE international conference\non data mining\n(ICDM), pages 1049–1054 . IEEE, 2018 .\n322\nYibin Liu, Yanhui Li, Jianbo Guo, Yuming Zhou, and Baowen Xu.\nConnecting software metrics across versions to predict\ndefects.\nIn 2018 IEEE 25th International\nConference\non\nSoftware\nAnalysis,\nEvolution\nand\nReengineering\n(SANER) ,\npages 232–243, 2018 .\n323\nHaonan Tong, Bin Liu, and Shihai Wang.\nSoftware defect prediction using stacked denoising autoencoders and two-stage\nensemble learning.\nInf.\nSoftw.\nTechnol. , 96(C):94–111, apr 2018 .\n324\nShaojian Qiu, Lu Lu, Ziyi Cai, and Siyu Jiang.\nCross-project defect prediction via transferable deep learning-generated and\nhandcrafted features.\nIn International\nConference on Software Engineering and Knowledge Engineering, 2019 .\n325\nThong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi.\nDeepjit:\nAn end-to-end deep learning\nframework for just-in-time defect prediction.\nIn 2019 IEEE/ACM\n16th International\nConference\non\nMining\nSoftware\nRepositories (MSR), pages 34–45, 2019 .\n326\nTianchi Zhou, Xiaobing Sun, Xin Xia, Bin Li, and Xiang Chen.\nImproving defect prediction with deep forest.\nInf.\nSoftw.\nTechnol., 114(C):204–216, oct 2019 .\n327\nZhou Xu, Shuai Li, Jun Xu, Jin Liu, Xiapu Luo, Yifeng Zhang, Tao Zhang, Jacky Keung, and Yutian Tang.\nLdfr: Learning\ndeep feature representation for software defect prediction.\nJ. Syst. Softw. , 158(C), dec 2019 .\n328\nHamza\nTurabieh,\nMajdi\nMafarja,\nand\nXiaodong Li.\nIterated\nfeature selection\nalgorithms with layered recurrent neural\nnetwork for software fault prediction.\nExpert Syst.\nAppl. , 122(C):27–42, may 2019 .\n329\nHoa Khanh Dam, Trang Pham, Shien Wee Ng, Truyen Tran, John Grundy, Aditya Ghose, Taeksu Kim, and Chul-Joo Kim.\nLessons learned from using a deep tree-based model for software defect prediction in practice.\nIn Proceedings of the 16th\nInternational Conference on Mining Software Repositories, MSR ’19, page 46–57 . IEEE Press, 2019 .\n330\nHao\nLi,\nXiaohong\nLi,\nXiang\nChen,\nXiaofei\nXie,\nYanzhou\nMu,\nand\nZhiyong\nFeng.\nCross-project\ndefect\nprediction\nvia\nasttoken2vec and blstm-based neural network.\nIn 2019 International\nJoint\nConference\non\nNeural\nNetworks\n(IJCNN) ,\npages 1–8, 2019 .\n331\nJinyin Chen, Keke Hu, Yue Yu, Zhuangzhi Chen, Qi Xuan, Yi Liu, and Vladimir Filkov.\nSoftware visualization and deep\ntransfer learning for effective software defect prediction.\nIn Proceedings\nof the ACM/IEEE 42nd international conference\non software engineering, pages 578–589, 2020 .\n332\nKun Zhu, Nana Zhang, Shi Ying, and Dandan Zhu.\nWithin-project and cross-project just-in-time defect prediction based\non denoising autoencoder and convolutional neural network.\nIET Software, 14(3):185–195, 2020 .\n333\nSong Wang, Taiyue Liu, Jaechang Nam, and Lin Tan.\nDeep semantic feature learning for software defect prediction.\nIEEE\nTransactions\non Software Engineering, 46(12):1267–1293, 2020 .\n334\nJiehan Deng, Lu Lu, and Shaojian Qiu.\nSoftware defect prediction via lstm.\nIET software, 14(4):443–450, 2020 .\n335\nKe Shi, Yang Lu, Jingfei Chang, and Zhen Wei.\nPathpair2vec:\nAn ast path pair-based code representation method for defect\nprediction.\nJournal of Computer Languages, 59:100979, 2020 .\n336\nAmirabbas\nMajd,\nMojtaba\nVahidi-Asl,\nAlireza\nKhalilian,\nPooria\nPoorsarvi-Tehrani,\nand\nHassan\nHaghighi.\nSldeep:\nStatement-level software defect prediction using deep-learning model on static code features.\nExpert Syst.\nAppl. , 147(C),\njun 2020 .\n337\nMing Wen, Rongxin Wu, and Shing-Chi Cheung.\nHow well do change sequences predict defects?\nsequence learning from\nsoftware changes.\nIEEE\nTransactions\non Software Engineering, 46(11):1155–1175, 2018 .\n338\nKe Shi, Yang Lu, Guangliang Liu, Zhenchun Wei, and Jingfei Chang.\nMpt-embedding:\nan unsupervised representation\nlearning of code for software defect prediction.\nJournal of Software:\nEvolution and Process, 33(4):e2330, 2021 .\n339\nZhou Xu, Kunsong Zhao, Tao Zhang, Chunlei Fu, Meng Yan, Zhiwen Xie, Xiaohong Zhang, and Gemma Catolino.\nEffort-\naware just-in-time bug prediction for mobile apps via cross-triplet deep feature embedding.\nIEEE\nTransactions on Relia-\nbility, 71(1):204–220, 2022 .\n340\nJiaxi Xu, Fei Wang, and Jun Ai.\nDefect prediction with semantics and context features of codes based on graph representation\nlearning.\nIEEE\nTransactions\non Reliability, 70(2):613–625, 2020 .\n341\nCheng\nZeng,\nChun\nYing\nZhou,\nSheng\nKai\nLv, Peng He, and Jie Huang.\nGcn2defect :\nGraph convolutional networks\nfor smotetomek-based software defect prediction.\nIn\n2021 IEEE 32nd International Symposium\non Software Reliability\nEngineering\n(ISSRE), pages 69–79, 2021 .\n342\nJiaxi Xu, Jun Ai, Jingyu Liu, and Tao Shi.\nAcgdp:\nAn augmented code graph-based system for software defect prediction.\nIEEE\nTransactions on Reliability, 71(2):850–864, 2022 .\n343\nHao Wang, Weiyuan Zhuang, and Xiaofang Zhang.\nSoftware\ndefect prediction based on gated hierarchical lstms.\nIEEE\nTransactions\non Reliability, 70(2):711–727, 2021 .\n344\nQuanyi Zou, Lu Lu, Zhanyu Yang, Xiaowei Gu, and Shaojian Qiu.\nJoint feature representation\nlearning and progressive\ndistribution matching for cross-project defect prediction.\nInformation\nand Software\nTechnology, 137:106588, 2021 .\n345\nNana Zhang, Shi Ying, Kun Zhu, and Dandan Zhu.\nSoftware defect prediction based on stacked sparse denoising autoencoders\nand enhanced extreme learning machine.\nIET Software, 16(1):29–47, 2022 .\n346\nMd Nasir Uddin, Bixin Li, Zafar Ali, Pavlos Kefalas, Inayat Khan, and Islam Zada.\nSoftware defect prediction employing\nbilstm and bert-based semantic feature.\nSoft\nComputing, 26(16):7877–7891, 2022 .\n347\nPasquale Ardimento, Lerina Aversano, Mario Luca Bernardi, Marta Cimitile, and Martina Iammarino.\nJust-in-time software\ndefect prediction using deep temporal convolutional networks.\nNeural\nComputing and Applications, 34(5):3981–4001, 2022 .\n348\nChanathip Pornprasit and Chakkrit Kla Tantithamthavorn.\nDeeplinedp:\nTowards a deep learning approach for line-level\ndefect prediction.\nIEEE\nTransactions\non Software Engineering, 49(1):84–98, 2023 .\n349\nShaojian Qiu, Huihao Huang, Wenchao Jiang, Fanlong Zhang, and Weilin Zhou.\nDefect prediction via tree-based encoding\nwith hybrid granularity for software sustainability.\nIEEE\nTransactions\non Sustainable\nComputing, pages 1–12, 2023 .\n350\nStephen C Johnson. Lint, a C program checker.\n1977.\n351\nDavid Hovemeyer and William Pugh.\nFinding bugs is easy.\n2004.\n352\nFacebook.\nInfer:\nA tool to detect bugs in java and c/c++/objective-c code before it ships, 2015 .\n353\nAlessandro Orso and Gregg Rothermel. Software testing: a research travelogue (2000–2014) . 2014.\n354\nCristian Cadar, Daniel Dunbar, Dawson R Engler, et al.\nKlee:\nUnassisted and automatic generation of high-coverage tests\nfor complex systems programs. 2008.\n355\nLuke Nelson, Helgi Sigurbjarnarson, Kaiyuan Zhang, Dylan Johnson, James Bornholt, Emina Torlak, and Xi Wang.\nHyper-\nSci China Inf Sci\n86\nkernel: Push-button verification of an os kernel.\n2017.\n356\nXavier Leroy.\nFormal verification of a realistic compiler.\n2009.\n357\nGerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai\nEngelhardt, Rafal Kolanski, Michael Norrish, et al.\nseL4:\nFormal verification of an OS kernel.\n2009.\n358\nVijay D’silva, Daniel Kroening, and Georg Weissenbacher.\nA survey of automated techniques for formal software verification.\n2008.\n359\nDonald E. Knuth.\nThe Art\nof Computer Programming,\nVolume\n1 (3rd Ed.):\nFundamental Algorithms.\n1997.\n360\nXinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.\nLarge language models for software engineering: A systematic literature review.\n2023.\n361\nAngela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang.\nLarge language\nmodels for software engineering: Survey and open problems. 2023.\n362\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.\nMastering the game of go with deep neural networks and\ntree search.\n2016.\n363\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen.\nReasoning with language model prompting: A survey. 2022.\n364\nJie Huang and Kevin Chen-Chuan Chang.\nTowards reasoning in large language models: A survey. 2022.\n365\nHarold Abelson and Gerald Jay Sussman.\nStructure\nand interpretation\nof computer programs.\n1996.\n366\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.\nOn the naturalness of software.\n2016.\n367\nGuido Van Rossum, Barry Warsaw, and Nick Coghlan.\nPEP 8–style guide for python code.\n2001.\n368\nAchut Reddy et al.\nJava coding style guide.\n2000.\n369\nDawson Engler, David Yu Chen, Seth Hallem, Andy Chou, and Benjamin Chelf.\nBugs\nas deviant behavior:\nA general\napproach to inferring errors in systems code. 2001.\n370\nZhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan Zhou.\nCP-Miner:\nFinding copy-paste and related bugs in large-scale\nsoftware code.\n2006.\n371\nOpenAI and GitHub.\nGithub copilot:\nYour AI pair programmer, 2021 .\n372\nMiltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt.\nSelf-supervised bug detection and repair.\nIn\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n34:\nAnnual\nConference\non\nNeural Information\nProcessing\nSystems\n2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages 27865 –27876, 2021 .\n373\nTushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, Indira Vats, Hadi Moazen, and Federica Sarro.\nA survey\non machine learning techniques for source code analysis. 2021.\n374\nYanjie Jiang, Hui Liu, Yuxia Zhang, Weixing Ji, Hao Zhong, and Lu Zhang.\nDo bugs lead to unnaturalness of source code?\n2022.\n375\nHenry Gordon Rice.\nClasses of recursively enumerable sets and their decision problems. 1953.\n376\nBenjamin Livshits, Manu Sridharan, Yannis Smaragdakis, Ondˇrej Lhot´ak, J Nelson Amaral, Bor-Yuh Evan Chang, Samuel Z\nGuyer, Uday P Khedker, Anders Møller, and Dimitrios Vardoulakis.\nIn defense of soundiness:\nA manifesto.\n2015.\n377\nKihong Heo, Hakjoo Oh, and Hongseok Yang.\nResource-aware program analysis via online abstraction coarsening.\n2019.\n378\nYoonseok Ko and Hakjoo Oh.\nLearning to boost disjunctive static bug-finders.\n2023.\n379\nHaonan\nLi,\nYu\nHao, Yizhuo Zhai, and Zhiyun Qian.\nThe hitchhiker’s guide to program analysis:\nA journey\nwith large\nlanguage models.\n2023.\n380\nKwonsoo Chae, Hakjoo Oh, Kihong Heo, and Hongseok Yang.\nAutomatically\ngenerating features for learning program\nanalysis heuristics for c-like languages. 2017.\n381\nKihong Heo, Hakjoo Oh, and Kwangkeun Yi.\nMachine-learning-guided selectively unsound static analysis.\n2017.\n382\nMinseok Jeon, Myungho Lee, and Hakjoo Oh.\nLearning graph-based heuristics for pointer analysis without handcrafting\napplication-specific features. 2020.\n383\nSehun Jeong, Minseok Jeon, Sungdeok Cha, and Hakjoo Oh.\nData-driven context-sensitivity for points-to analysis.\n2017.\n384\nJingxuan He, Gagandeep Singh, Markus P¨uschel, and Martin Vechev.\nLearning fast and precise numerical analysis. 2020.\n385\nWojciech Zaremba and Ilya Sutskever.\nLearning to execute.\n2014.\n386\nRabee Sohail Malik, Jibesh Patra, and Michael Pradel.\nNL2Type:\ninferring javascript function types from natural language\ninformation.\n2019.\n387\nKevin Jesse, Premkumar T Devanbu, and Toufique Ahmed.\nLearning type annotation:\nis big data enough?\n2021 .\n388\nDongran Yu, Bo Yang, Dayou Liu, Hui Wang, and Shirui Pan.\nA survey on neural-symbolic learning systems.\n2023.\n389\nWenguan\nWang\nand\nYi\nYang.\nTowards\ndata-and\nknowledge-driven\nartificial\nintelligence:\nA survey\non\nneuro-symbolic\ncomputing.\n2022.\n390\nDongdong She, Kexin Pei, Dave Epstein, Junfeng Yang, Baishakhi Ray, and Suman Jana.\nNeuzz:\nEfficient\nfuzzing with\nneural program smoothing.\nIn 2019 IEEE Symposium\non Security and Privacy (SP), pages 803–817 . IEEE, 2019 .\n391\nDongdong She,\nRahul Krishna,\nLu Yan,\nSuman\nJana,\nand\nBaishakhi\nRay.\nMTFuzz:\nfuzzing\nwith\na multi-task\nneural\nnetwork.\nIn Proceedings of the 28th ACM joint meeting on European software\nengineering conference and symposium on\nthe foundations of software engineering, pages 737–749, 2020 .\n392\nMingyuan Wu, Ling Jiang, Jiahong Xiang, Yuqun Zhang, Guowei Yang, Huixin Ma, Sen Nie, Shi Wu, Heming Cui, and Ling-\nming Zhang.\nEvaluating and improving neural program-smoothing-based fuzzing.\nIn Proceedings\nof the 44th International\nConference\non Software Engineering, pages 847–858, 2022 .\n393\nMaria-Irina Nicolae, Max Eisele, and Andreas Zeller.\nRevisiting neural program smoothing for fuzzing.\n2023.\n394\nAndreas Zeller.\nMining specifications:\nA roadmap.\n2011.\n395\nKonstantin\nSerebryany,\nDerek\nBruening,\nAlexander\nPotapenko, and Dmitriy Vyukov.\nAddressSanitizer:\nA\nfast address\nsanity checker.\n2012.\n396\nKonstantin Serebryany and Timur Iskhodzhanov.\nThreadSanitizer:\ndata race detection in practice.\n2009.\n397\nDaniel Jackson.\nSoftware\nAbstractions:\nlogic,\nlanguage, and analysis.\n2012.\n398\nCaroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, and Siddhartha Sen.\nCODAMOSA: Escaping coverage plateaus\nin test generation with pre-trained large language models.\n2023.\n399\nAhmed Khanfir, Renzo Degiovanni, Mike Papadakis, and Yves Le Traon.\nEfficient mutation testing via pre-trained language\nmodels.\n2023.\n400\nZhuangbin Chen, Jinyang Liu, Wenwei Gu, Yuxin Su, and Michael R Lyu.\nExperience report:\nDeep learning-based system\nlog analysis for anomaly detection.\n2021.\nSci China Inf Sci\n87\n401\nJunjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang.\nSoftware testing with large language\nmodel: Survey, landscape, and vision.\n2023.\n402\nVinicius\nHS\nDurelli, Rafael S Durelli, Simone S Borges, Andre T Endo, Marcelo M Eler, Diego RC Dias, and Marcelo P\nGuimar˜aes.\nMachine learning applied to software testing:\nA systematic mapping study.\n2019.\n403\nMichele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan.\nUnit test case generation with\ntransformers and focal context.\n2020.\n404\nCody Watson, Michele Tufano, Kevin Moran,\nGabriele Bavota, and Denys Poshyvanyk.\nOn\nlearning\nmeaningful assert\nstatements for unit test cases. 2020.\n405\nMichele Tufano, Dawn Drain, Alexey Svyatkovskiy, and Neel Sundaresan.\nGenerating accurate assert statements for unit\ntest cases using pretrained transformers. 2022.\n406\nArianna Blasi, Alessandra Gorla, Michael D Ernst, and Mauro Pezz`e.\nCall Me Maybe:\nUsing NLP to automatically generate\nunit test cases respecting temporal constraints.\n2022.\n407\nElizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and Shuvendu K Lahiri.\nToga: A neural method for test oracle generation.\n2022.\n408\nZhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, and Jianwei Yin.\nChatUniTest:\na chatgpt-based automated unit\ntest generation tool.\n2023.\n409\nSaranya Alagarsamy, Chakkrit Tantithamthavorn, and Aldeida Aleti.\nA3Test:\nAssertion-augmented\nautomated test case\ngeneration.\n2023.\n410\nPatric Feldmeier and Gordon Fraser.\nNeuroevolution-based generation of tests and oracles for games. 2022.\n411\nMax Sch¨afer, Sarah Nadi, Aryaz Eghbali, and Frank Tip.\nAdaptive test generation using a large language model.\n2023.\n412\nMohammed\nLatif Siddiq,\nJoanna\nSantos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and Vinicius Carvalho\nLopes.\nExploring the effectiveness of large language models in generating unit tests. 2023.\n413\nSoneya Binta Hossain, Antonio Filieri, Matthew B Dwyer, Sebastian Elbaum, and Willem Visser.\nNeural-based test oracle\ngeneration: A large-scale evaluation and lessons learned. 2023.\n414\nZhongxin Liu, Kui Liu, Xin Xia, and Xiaohu Yang.\nTowards\nmore\nrealistic evaluation for neural test oracle generation.\n2023.\n415\nZhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, and Xin Peng.\nNo more manual tests?\nevaluating and improving chatgpt for unit test generation.\n2023.\n416\nW Eric Wong, Joseph R Horgan, Saul London, and Hiralal Agrawal. A study of effective regression testing in practice.\n1997.\n417\nShin Yoo and Mark Harman.\nRegression testing minimization, selection and prioritization:\na survey.\n2012.\n418\nValentin JM Man`es, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J Schwartz, and Maverick\nWoo.\nThe art, science, and engineering of fuzzing:\nA survey. 2019.\n419\nXiaogang Zhu, Sheng Wen, Seyit Camtepe, and Yang Xiang.\nFuzzing:\na survey for roadmap.\n2022.\n420\nJun Li, Bodong Zhao, and Chao Zhang. Fuzzing: a survey. 2018.\n421\nMyungho Lee, Sooyoung Cha, and Hakjoo Oh.\nLearning seed-adaptive mutation strategies for greybox fuzzing.\n2023.\n422\nJinghan Wang, Chengyu Song, and Heng Yin.\nReinforcement learning-based hierarchical seed scheduling for greybox fuzzing.\n2021.\n423\nYunchao Wang, Zehui Wu, Qiang Wei, and Qingxian Wang.\nNeufuzz:\nEfficient fuzzing with deep neural network.\n2019.\n424\nYinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming Zhang. Large language models are zero-shot\nfuzzers: Fuzzing deep-learning libraries via large language models. 2023.\n425\nYinlin\nDeng,\nChunqiu\nSteven\nXia,\nChenyuan\nYang,\nShizhuo\nDylan\nZhang,\nShujing Yang,\nand Lingming\nZhang.\nLarge\nlanguage models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt. 2023.\n426\nChenyuan Yang, Yinlin Deng, Runyu Lu, Jiayi Yao, Jiawei Liu, Reyhaneh Jabbarvand, and Lingming Zhang.\nWhite-box\ncompiler fuzzing empowered by large language models. 2023.\n427\nChunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang.\nUniversal fuzzing via large\nlanguage models.\n2023.\n428\nGuixin Ye, Zhanyong Tang, Shin Hwei Tan, Songfang Huang, Dingyi Fang, Xiaoyang Sun, Lizhong Bian, Haibo Wang, and\nZheng Wang.\nAutomated conformance testing for javascript engines via deep compiler fuzzing.\n2021.\n429\nChris Cummins, Pavlos Petoumenos, Alastair Murray, and Hugh Leather.\nCompiler fuzzing through deep learning.\n2018.\n430\nMingmin Lin, Yingpei Zeng, and Yang Li.\nRegFuzz:\nA linear\nregression-based approach for seed scheduling in directed\nfuzzing.\n2023.\n431\nRuijie Meng, Martin Mirchev, Marcel B¨ohme, and Abhik Roychoudhury.\nLarge\nlanguage model guided protocol fuzzing.\n2024.\n432\nJianzhong Su, Hong-Ning Dai, Lingjun Zhao, Zibin Zheng, and Xiapu Luo.\nEffectively generating vulnerable transaction\nsequences in smart contracts with reinforcement learning-guided fuzzing.\n2022.\n433\nWeisi Luo, Dong Chai, Xiaoyue Ruan, Jiang Wang, Chunrong Fang, and Zhenyu Chen.\nGraph-based fuzz testing for deep\nlearning inference engines. 2021.\n434\nYuqi Chen, Christopher M Poskitt, Jun Sun, Sridhar Adepu, and Fan Zhang.\nLearning-guided network fuzzing for testing\ncyber-physical system defences. 2019.\n435\nLing Jiang, Hengchen Yuan, Mingyuan Wu, Lingming Zhang, and Yuqun Zhang.\nEvaluating and improving hybrid fuzzing.\n2023.\n436\nJingxuan He, Mislav Balunovi´c, Nodar Ambroladze, Petar Tsankov, and Martin Vechev.\nLearning to fuzz from symbolic\nexecution with application to smart contracts.\n2019.\n437\nHaoxiang Jia,\nMing Wen,\nZifan Xie, Xiaochen Guo, Rongxin Wu, Maolin Sun, Kang Chen, and Hai Jin.\nDetecting JVM\nJIT compiler bugs via exploring two-dimensional input spaces.\n2023.\n438\nYan Zheng, Yi Liu, Xiaofei Xie, Yepang Liu, Lei Ma, Jianye Hao, and Yang Liu.\nAutomatic web testing using curiosity-driven\nreinforcement learning.\n2021.\n439\nShaohua Zhang, Shuang Liu, Jun Sun, Yuqi Chen, Wenzhi Huang, Jinyi Liu, Jian Liu, and Jianye Hao.\nFIGCPS: Effective\nfailure-inducing input generation for cyber-physical systems with deep reinforcement learning.\n2021.\n440\nZhe Liu, Chunyang Chen, Junjie Wang, Xing Che, Yuekai Huang, Jun Hu, and Qing Wang.\nFill in the blank:\nContext-aware\nautomated text input generation for mobile GUI testing.\n2023.\n441\nFaraz YazdaniBanafsheDaragh and Sam Malek.\nDeep GUI: Black-box GUI input generation with deep learning.\n2021.\n442\nSidong Feng, Mulong Xie, and Chunyang Chen.\nEfficiency\nmatters:\nSpeeding\nup\nautomated testing with GUI rendering\ninference.\n2023.\nSci China Inf Sci\n88\n443\nDezhi Ran, Hao Wang, Wenyu Wang, and Tao Xie.\nBadge:\nPrioritizing ui events with hierarchical multi-armed bandits for\nautomated ui testing.\n2023.\n444\nMinxue Pan, An Huang, Guoxin Wang, Tian Zhang, and Xuandong Li.\nReinforcement learning based curiosity-driven testing\nof android applications.\n2020.\n445\nYixue\nZhao,\nSaghar Talebipour, Kesina Baral, Hyojae Park, Leon Yee, Safwat Ali Khan, Yuriy Brun, Nenad Medvidovi´c,\nand Kevin Moran.\nAvgust:\nautomating usage-based test generation from videos of app executions.\n2022.\n446\nXiaoke Wang and Lei Zhao.\nAPICAD: Augmenting API misuse detection through specifications from code and documents.\n2023.\n447\nMyeongsoo Kim, Davide Corradini,\nSaurabh Sinha, Alessandro Orso, Michele Pasqua, Rachel Tzoref-Brill, and Mariano\nCeccato.\nEnhancing REST API testing with NLP techniques.\n2023.\n448\nMyeongsoo Kim, Saurabh Sinha, and Alessandro Orso.\nAdaptive REST API testing with reinforcement learning.\n2023.\n449\nTasniem Nasser Alyahya, Mohamed El Bachir Menai, and Hassan Mathkour.\nOn the structure of the boolean satisfiability\nproblem: A survey.\n2022.\n450\nWenxuan\nGuo,\nHui-Ling Zhen, Xijun Li, Wanqian Luo, Mingxuan Yuan, Yaohui Jin, and Junchi Yan.\nMachine learning\nmethods in solving the boolean satisfiability problem.\n2023.\n451\nThanassis Avgerinos, Alexandre Rebert, Sang Kil Cha, and David Brumley.\nEnhancing symbolic execution with veritesting.\n2014.\n452\nRoberto Baldoni, Emilio Coppa, Daniele Cono D’elia, Camil Demetrescu, and Irene Finocchi.\nA survey of symbolic execution\ntechniques.\n2018.\n453\nJingxuan He, Gishor Sivanrupan, Petar Tsankov, and Martin Vechev.\nLearning to explore paths for symbolic execution.\n2021.\n454\nSooyoung Cha and Hakjoo Oh.\nConcolic testing with adaptively changing search heuristics.\n2019.\n455\nSooyoung Cha, Seongjoon Hong, Junhee Lee, and Hakjoo Oh. Automatically generating search heuristics for concolic testing.\n2018.\n456\nTianqi Zhang, Yufeng Zhang, Zhenbang Chen, Ziqi Shuai, and Ji Wang.\nEfficient multiplex symbolic execution with adaptive\nsearch strategy.\n2020.\n457\nSooyoung Cha and Hakjoo Oh.\nMaking symbolic execution promising by learning aggressive state-pruning strategy.\n2020.\n458\nZhenbang Chen, Zehua Chen, Ziqi Shuai, Guofeng Zhang, Weiyu Pan, Yufeng Zhang, and Ji Wang.\nSynthesize solving\nstrategy for symbolic execution.\n2021.\n459\nSicheng Luo, Hui Xu, Yanxiang Bi, Xin Wang, and Yangfan Zhou.\nBoosting symbolic execution via constraint solving time\nprediction (experience paper) . 2021.\n460\nSooyoung Cha, Myungho Lee, Seokhyun Lee, and Hakjoo Oh.\nSymTuner:\nmaximizing the power of symbolic execution by\nadaptively tuning external parameters.\n2022.\n461\nJunjie Chen, Wenxiang Hu, Lingming Zhang, Dan Hao, Sarfraz Khurshid, and Lu Zhang.\nLearning to accelerate symbolic\nexecution via code transformation.\n2018.\n462\nThe Coq development team.\nThe Coq proof assistant, 1984 .\n463\nThe Isabelle development team. Isabelle, 1986 .\n464\nLawrence C Paulson.\nNatural deduction as higher-order resolution.\n1986.\n465\nGuillaume Lample, Timothee Lacroix,\nMarie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril,\nGabriel\nEbner, and Xavier Martinet.\nHypertree proof search for neural theorem proving.\n2022.\n466\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy.\nAuto-\nformalization with large language models.\n2022.\n467\nEmily First and Yuriy Brun.\nDiversity-driven automated formal verification.\n2022.\n468\nKaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima\nAnandkumar.\nLeanDojo:\nTheorem proving with retrieval-augmented language models.\n2023.\n469\nSaikat\nChakraborty,\nShuvendu\nK\nLahiri,\nSarah\nFakhoury,\nMadanlal\nMusuvathi,\nAkash\nLal,\nAseem\nRastogi,\nAditya\nSenthilnathan, Rahul Sharma, and Nikhil Swamy.\nRanking llm-generated loop invariants for program verification.\n2023.\n470\nSebastian Zimmeck,\nZiqi Wang,\nLieyong\nZou,\nRoger\nIyengar,\nBin\nLiu,\nFlorian\nSchaub,\nShomir\nWilson,\nNorman\nSadeh,\nSteven Bellovin, and Joel Reidenberg.\nAutomated analysis of privacy requirements for mobile apps.\nIn\n2016 AAAI Fall\nSymposium Series, 2016 .\n471\nAfsaneh Mahanipour and Hossein Nezamabadi-Pour.\nGsp:\nan automatic programming technique with gravitational search\nalgorithm.\nApplied Intelligence, 49:1502–1516, 2019 .\n472\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.\nDistributed representations of words and phrases\nand their compositionality.\nAdvances\nin neural information processing systems, 26, 2013 .\n473\nShuang Liu, Baiyang Zhao, Renjie Guo, Guozhu Meng, Fan Zhang, and Meishan Zhang.\nHave you been properly notified?\nautomatic compliance analysis of privacy policy text with gdpr article 13 .\nIn\nProceedings\nof the\nWeb\nConference\n2021 ,\npages 2154–2164, 2021 .\n474\nCindy Rubio-Gonz´alez and Ben Liblit.\nExpect the\nunexpected: Error\ncode\nmismatches\nbetween documentation and the\nreal world.\nIn Proceedings of the 9th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and\nengineering, pages 73 –80, 2010 .\n475\nLin Tan, Ding Yuan, Gopal Krishna, and Yuanyuan Zhou.\n/* icomment:\nBugs\nor bad comments?* .\nIn\nProceedings\nof\ntwenty-first ACM SIGOPS symposium on Operating systems principles, pages 145–158, 2007 .\n476\nShin Hwei Tan, Darko Marinov, Lin Tan, and Gary T Leavens.\n@ tcomment:\nTesting javadoc comments to detect comment-\ncode inconsistencies.\nIn 2012 IEEE Fifth International\nConference\non\nSoftware\nTesting,\nVerification\nand\nValidation ,\npages 260–269 . IEEE, 2012 .\n477\nFengcai Wen, Csaba Nagy, Gabriele Bavota, and Michele Lanza.\nA large-scale empirical study on code-comment inconsis-\ntencies.\nIn\n2019 IEEE/ACM 27th International\nConference\non\nProgram\nComprehension\n(ICPC), pages 53–64 .\nIEEE,\n2019.\n478\nRahul Pandita, Kunal Taneja, Laurie Williams, and Teresa Tung.\nIcon:\nInferring temporal constraints from natural language\napi descriptions.\nIn 2016 IEEE international conference on software maintenance and evolution (ICSME), pages 378–388 .\nIEEE, 2016 .\n479\nXiaoxue\nRen,\nXinyuan\nYe,\nZhenchang\nXing,\nXin Xia, Xiwei Xu,\nLiming Zhu, and Jianling Sun.\nApi-misuse\ndetection\ndriven by fine-grained api-constraint knowledge graph.\nIn Proceedings of the 35th IEEE/ACM International Conference\non Automated Software Engineering, pages 461–472, 2020 .\nSci China Inf Sci\n89\n480\nTao\nLv,\nRuishi\nLi,\nYi Yang,\nKai\nChen, Xiaojing Liao, XiaoFeng Wang,\nPeiwei Hu,\nand Luyi Xing.\nRtfm!\nautomatic\nassumption discovery and verification derivation from library document for api misuse detection.\nIn Proceedings of the 2020\nACM SIGSAC conference on computer and communications security, pages 1837–1852, 2020 .\n481\nInsu Yun, Changwoo Min, Xujie Si, Yeongjin Jang, Taesoo Kim, and Mayur Naik.\nApisan:\nSanitizing api usages through\nsemantic cross-checking. In Usenix Security Symposium, pages 363–378, 2016 .\n482\nYuan Kang, Baishakhi Ray, and Suman Jana.\nApex:\nAutomated inference of error specifications for c apis.\nIn\nProceedings\nof the 31st IEEE/ACM International Conference on Automated Software Engineering, pages 472–482, 2016 .\n483\nChi Li, Min Zhou, Zuxing Gu, Ming Gu, and Hongyu Zhang.\nAres:\nInferring error specifications through static analysis.\nIn 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 1174–1177 . IEEE,\n2019.\n484\nAri\nTakanen,\nJared\nD\nDemott,\nCharles\nMiller,\nand\nAtte Kettunen.\nFuzzing\nfor\nsoftware\nsecurity\ntesting\nand\nquality\nassurance. Artech House, 2018 .\n485\nWei You,\nPeiyuan\nZong, Kai Chen, XiaoFeng Wang, Xiaojing Liao, Pan Bian, and Bin Liang.\nSemfuzz:\nSemantics-based\nautomatic generation of proof-of-concept exploits.\nIn\nProceedings\nof the\n2017 ACM\nSIGSAC\nConference\non\nComputer\nand Communications Security, pages 2139–2154, 2017 .\n486\nPatrice\nGodefroid,\nHila\nPeleg,\nand Rishabh\nSingh.\nLearn&fuzz:\nMachine\nlearning\nfor\ninput\nfuzzing.\nIn\n2017\n32nd\nIEEE/ACM International Conference on Automated Software Engineering (ASE), pages 50–59 . IEEE, 2017 .\n487\nXiao Liu, Xiaoting Li, Rupesh Prajapati, and Dinghao Wu.\nDeepfuzz:\nAutomatic generation of syntax valid c programs for\nfuzz testing.\nIn Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 33, pages 1044–1051, 2019 .\n488\nSuyoung Lee, HyungSeok Han, Sang Kil Cha, and Sooel Son.\nMontage:\nA neural network language model-guided javascript\nengine fuzzer.\nIn Proceedings\nof the 29th USENIX Conference on Security Symposium, pages 2613–2630, 2020 .\n489\nPeng Chen and Hao Chen.\nAngora:\nEfficient\nfuzzing\nby principled search.\nIn\n2018\nIEEE\nSymposium\non\nSecurity\nand\nPrivacy (SP), pages 711–725 . IEEE, 2018 .\n490\nKen-Ichi\nFunahashi.\nOn\nthe\napproximate\nrealization\nof\ncontinuous\nmappings\nby\nneural\nnetworks.\nNeural\nnetworks ,\n2(3):183–192, 1989 .\n491\nStefan Nagy and Matthew Hicks.\nFull-speed fuzzing:\nReducing fuzzing overhead through coverage-guided tracing.\nIn\n2019\nIEEE Symposium on Security and Privacy (SP), pages 787–802 . IEEE, 2019 .\n492\nChijin Zhou, Mingzhe Wang, Jie Liang, Zhe Liu, and Yu Jiang.\nZeror:\nSpeed up fuzzing with coverage-sensitive tracing and\nscheduling.\nIn Proceedings of the\n35th IEEE/ACM International Conference on Automated Software Engineering, pages\n858–870, 2020 .\n493\nPeiyuan Zong, Tao Lv, Dawei Wang, Zizhuang Deng, Ruigang Liang, and Kai Chen.\nFuzzguard:\nFiltering out unreachable\ninputs in directed grey-box fuzzing through deep learning.\nIn\nProceedings\nof the 29th\nUSENIX\nConference\non\nSecurity\nSymposium, pages 2255–2269, 2020 .\n494\nRalf Jung, Jacques-Henri Jourdan, Robbert Krebbers, and Derek Dreyer.\nSafe systems programming in Rust.\n2021.\n495\nW\nEric\nWong,\nRuizhi\nGao, Yihao Li, Rui Abreu, and Franz Wotawa.\nA\nsurvey on software fault localization.\nIEEE\nTransactions on Software Engineering, 42(8):707–740, 2016 .\n496\nAbubakar Zakari,\nSai\nPeck Lee,\nRui\nAbreu,\nBabiker\nHussien Ahmed,\nand\nRasheed Abubakar Rasheed.\nMultiple\nfault\nlocalization of software programs: A systematic literature review.\nInformation and Software\nTechnology, 124:106312, 2020 .\n497\nXiaoyuan Xie,\nZicong\nLiu,\nShuo Song, Zhenyu Chen, Jifeng Xuan, and Baowen Xu.\nRevisit of automatic\ndebugging via\nhuman focus-tracking analysis.\nIn\nProceedings\nof\nthe\n38th\nInternational\nConference\non\nSoftware\nEngineering, pages\n808–819, 2016 .\n498\nH. Agrawal, J.R. Horgan, S. London, and W.E. Wong.\nFault localization using execution slices and dataflow tests.\nIn\nProceedings of Sixth International Symposium on Software Reliability Engineering.\nISSRE’95, pages 143–151, 1995 .\n499\nChu-Pan Wong, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, and Hong Mei.\nBoosting bug-report-oriented fault\nlocalization with segmentation and stack-trace analysis.\nIn 2014 IEEE International Conference on Software Maintenance\nand Evolution, pages 181 –190, 201 4 .\n500\nXiangyu Zhang, Neelam Gupta, and Rajiv Gupta.\nLocating faults through automated predicate switching.\nIn\nProceedings\nof the\n28th\nInternational\nConference\non\nSoftware\nEngineering,\nICSE\n’06,\npage 272–281,\nNew\nYork,\nNY,\nUSA,\n2006 .\nAssociation for Computing Machinery.\n501\nJames A Jones, Mary Jean Harrold, and John Stasko.\nVisualization of test information to assist fault localization.\nIn\nProceedings of the 24th international conference on Software engineering, pages 467–477, 2002 .\n502\nBen Liblit, Mayur Naik, Alice X Zheng, Alex Aiken, and Michael I Jordan.\nScalable statistical bug isolation.\nAcm Sigplan\nNotices, 40(6):15–26, 2005 .\n503\nRui Abreu,\nPeter\nZoeteweij,\nRob Golsteijn, and Arjan JC Van Gemund.\nA\npractical evaluation of spectrum-based\nfault\nlocalization.\nJournal of Systems and Software, 82(11):1780–1792, 2009 .\n504\nXiaoyuan Xie, Tsong Yueh Chen, Fei-Ching Kuo, and Baowen Xu.\nA theoretical analysis of the risk evaluation formulas for\nspectrum-based fault localization.\nACM\nTransactions\non\nsoftware\nengineering\nand\nmethodology\n(TOSEM), 22(4):1–40,\n2013.\n505\nDaming\nZou,\nJingjing\nLiang,\nYingfei Xiong,\nMichael D Ernst, and Lu Zhang.\nAn empirical study of fault\nlocalization\nfamilies and their combinations.\nIEEE\nTransactions\non Software Engineering, 47(2):332–347, 2019 .\n506\nRatnadira Widyasari, Gede Artha Azriadi Prana, Stefanus A Haryono, Yuan Tian, Hafil Noer Zachiary, and David Lo.\nXai4fl:\nenhancing spectrum-based fault localization with explainable artificial intelligence.\nIn Proceedings of the 30th IEEE/ACM\nInternational Conference on Program Comprehension, pages 499–510, 2022 .\n507\nSeokhyeon Moon, Yunho Kim, Moonzoo Kim, and Shin Yoo.\nAsk the mutants: Mutating faulty programs for fault localiza-\ntion.\nIn 2014 IEEE Seventh International Conference on Software\nTesting,\nVerification\nand\nValidation, pages 153–162 .\nIEEE, 2014 .\n508\nMike Papadakis and Yves Le Traon.\nMetallaxis-fl:\nmutation-based fault localization.\nSoftware\nTesting,\nVerification\nand\nReliability, 25(5-7):605–628, 2015 .\n509\nW Eric Wong and Yu Qi.\nBp neural network-based effective fault localization.\nInternational\nJournal\nof Software\nEngi-\nneering and Knowledge Engineering, 19(04):573–597, 2009 .\n510\nW. E. Wong, V. Debroy, R. Golden, Xiaofeng Xu, and B. Thuraisingham.\nEffective software fault localization using an rbf\nneural network.\nIEEE\nTransactions\non Reliability, 61(1):149–169, 2012 .\n511\nWei Zheng, Desheng Hu, and Jing Wang. Fault localization analysis based on deep neural network.\nMathematical Problems\nin Engineering, 2016:1 –11, 01 2016 .\nSci China Inf Sci\n90\n512\nZhuo ZHANG, Yan LEI, Qingping TAN, Xiaoguang MAO, Ping ZENG, and Xi CHANG.\nDeep learning-based fault local-\nization with contextual information.\nIEICE\nTransactions on Information and Systems, E100.D(12):3027–3031, 2017 .\n513\nXia Li, Wei Li, Yuqun Zhang, and Lingming Zhang.\nDeepfl:\nIntegrating multiple fault diagnosis dimensions for deep fault\nlocalization.\nIn Proceedings of the 28th ACM SIGSOFT international symposium on software testing and analysis, pages\n169–180, 2019 .\n514\nZhuo, Zhang, Yan, Lei, Xiaoguang, Mao, Panpan, and Li.\nCnn-fl:\nAn effective approach for localizing faults using convolu-\ntional neural networks.\nIn 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering\n(SANER) .\n515\nYi Li, Shaohua Wang, and Tien Nguyen.\nFault localization with code coverage representation learning.\nIn 2021 IEEE/ACM\n43rd International Conference on Software Engineering (ICSE), pages 661–673 . IEEE, 2021 .\n516\nYiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao, Lu Zhang, and Lingming Zhang.\nBoosting coverage-based\nfault localization via graph-based representation learning.\nIn\nProceedings\nof the\n29th ACM Joint\nMeeting\non European\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering, pages 664–676, 2021 .\n517\nJie Qian, Xiaolin Ju, Xiang Chen, Hao Shen, and Yiheng Shen.\nAgfl: a graph convolutional neural network-based method\nfor fault localization.\nIn 2021 IEEE 21st International Conference on Software\nQuality, Reliability and Security (QRS) ,\npages 672–680 . IEEE, 2021 .\n518\nJie Qian, Xiaolin Ju, and Xiang Chen.\nGnet4fl:\neffective fault localization via graph convolutional neural network.\nAuto-\nmated Software Engineering, 30(2):16, Apr 2023 .\n519\nZhuo Zhang, Yan Lei, Xiaoguang Mao, Meng Yan, Xin Xia, and David Lo.\nContext-aware neural fault localization.\nIEEE\nTransactions on Software Engineering, pages 1–17, 2023 .\n520\nYi Li, Shaohua Wang, and Tien N. Nguyen.\nFault localization to detect co-change fixing locations.\nIn Proceedings\nof the\n30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering ,\nESEC/FSE 2022, page 659–671, New York, NY, USA, 2022 . Association for Computing Machinery.\n521\nArpita Dutta,\nRicha\nManral, Pabitra Mitra, and Rajib Mall.\nHierarchically localizing software faults using dnn.\nIEEE\nTransactions on Reliability, 69(4):1267–1292, 2020 .\n522\nJunji Yu, Yan Lei, Huan Xie, Lingfeng Fu, and Chunyan Liu.\nContext-based cluster fault localization.\nIn Proceedings of the\n30th IEEE/ACM International\nConference on Program\nComprehension, ICPC ’22, page 482–493, New York, NY, USA,\n2022. Association for Computing Machinery.\n523\nZhengmin Li,\nEnyi Tang, Xin Chen,\nLinzhang Wang,\nand Xuandong Li.\nGraph\nneural\nnetwork\nbased\ntwo-phase fault\nlocalization approach.\nIn Proceedings\nof the 13th Asia-Pacific Symposium on Internetware, pages 85–95, 2022 .\n524\nLeila Yousofvand, Seyfollah Soleimani, and Vahid Rafe.\nAutomatic bug localization using a combination of deep learning\nand model transformation through node classification.\nSoftware\nQuality\nJournal, pages 1–19, 2023 .\n525\nShumei Wu,\nZheng Li, Yong Liu, Xiang\nChen, and Mingyu Li.\nGmbfl:\nOptimizing mutation-based fault localization via\ngraph representation.\nIn 2023 IEEE International\nConference\non Software Maintenance and Evolution\n(ICSME), pages\n245–257. IEEE, 2023 .\n526\nJunming Cao, Shouliang Yang, Wenhui Jiang, Hushuang Zeng, Beijun Shen, and Hao Zhong.\nBugpecker:\nLocating faulty\nmethods with deep learning on revision graphs.\nIn 2020 35th IEEE/ACM International Conference on Automated Software\nEngineering\n(ASE), pages 1214–1218, 2020 .\n527\nAgnieszka Ciborowska and Kostadin Damevski.\nFast changeset-based bug localization with bert.\nIn Proceedings\nof the 44th\nInternational\nConference\non\nSoftware Engineering, ICSE ’22, page 946–957, New York, NY, USA, 2022 .\nAssociation for\nComputing Machinery.\n528\nZhuo Zhang, Yan Lei, Xiaoguang Mao, Meng Yan, Ling Xu, and Xiaohong Zhang.\nA study of effectiveness of deep learning\nin locating real faults.\nInformation\nand Software\nTechnology, 131:106486, 2021 .\n529\nHao Zhong and Hong Mei.\nLearning a graph-based classifier for fault localization.\nScience\nChina\nInformation Sciences ,\n63:1–22, 2020.\n530\nZhuo Zhang, Yan Lei, Xiaoguang Mao, Meng Yan, Ling Xu, and Junhao Wen.\nImproving deep-learning-based fault local-\nization with resampling.\nJournal of Software:\nEvolution and Process, 33(3):e2312, 2021 .\n531\nHuan Xie, Yan Lei, Meng Yan, Yue Yu, Xin Xia, and Xiaoguang Mao.\nA universal data augmentation approach for fault\nlocalization.\nIn Proceedings\nof the 44th International\nConference\non Software Engineering, ICSE ’22, page 48–60, New\nYork, NY, USA, 2022 . Association for Computing Machinery.\n532\nJian Hu, Huan Xie, Yan Lei, and Ke Yu.\nA light-weight data augmentation method for fault localization.\nInformation and\nSoftware Technology, 157:107148, 2023 .\n533\nYan\nLei,\nChunyan\nLiu, Huan Xie, Sheng Huang, Meng Yan, and Zhou Xu.\nBcl-fl:\nA\ndata augmentation approach with\nbetween-class learning for fault localization.\nIn 2022 IEEE International\nConference on Software Analysis, Evolution and\nReengineering\n(SANER), pages 289–300, 2022 .\n534\nYan\nLei,\nTiantian Wen,\nHuan Xie, Lingfeng Fu, Chunyan Liu, Lei Xu, and Hongxia Sun.\nMitigating the effect of class\nimbalance in fault localization using context-aware generative adversarial network.\nIn Proceedings of the\n31st IEEE/ACM\nInternational Conference on Program Comprehension, ICPC ’23, 2023.\n535\nZhuo Zhang, Yan Lei, Ting Su, Meng Yan, Xiaoguang Mao, and Yue Yu.\nInfluential global and local contexts guided trace\nrepresentation for fault localization.\nACM\nTransactions on Software Engineering and Methodology, 32(3):1–27, 2023.\n536\nZhao Tian, Junjie Chen, Qihao Zhu, Junjie Yang, and Lingming Zhang.\nLearning to construct better mutation faults.\nIn\n37th IEEE/ACM International Conference on Automated Software Engineering, pages 1–13, 2022 .\n537\nZhuo Zhang, Yan Lei, Xiaoguang Mao, Meng Yan, and Xin Xia.\nImproving fault localization using model-domain synthesized\nfailing test generation.\nIn 2022 IEEE International\nConference on Software Maintenance and Evolution (ICSME), pages\n199–210, 2022 .\n538\nRen´e Just, Darioush Jalali, and Michael D Ernst.\nDefects4j:\nA database of existing faults to enable controlled testing studies\nfor java programs.\nIn Proceedings\nof the\n2014\ninternational symposium on software testing and analysis, pages 437–440,\n2014.\n539\nFernanda Madeiral,\nSimon Urli,\nMarcelo\nMaia,\nand Martin Monperrus.\nBears:\nAn\nextensible java\nbug benchmark for\nautomatic program repair studies.\nIn\n2019\nIEEE\n26th\nInternational\nConference\non\nSoftware\nAnalysis,\nEvolution\nand\nReengineering\n(SANER), pages 468–478 . IEEE, 2019 .\n540\nHyunsook Do, Sebastian Elbaum, and Gregg Rothermel.\nSupporting controlled experimentation with testing techniques:\nAn infrastructure and its potential impact.\nEmpirical Software\nEngineering, 10:405–435, 2005 .\n541\nClaire Le Goues, Neal Holtschulte, Edward K Smith, Yuriy Brun, Premkumar Devanbu, Stephanie Forrest, and Westley\nSci China Inf Sci\n91\nWeimer.\nThe manybugs and introclass benchmarks for automated repair of c programs.\nIEEE Transactions on Software\nEngineering, 41(12):1236–1256, 2015 .\n542\nCathrin Weiß, Rahul Premraj, Thomas Zimmermann, and Andreas Zeller.\nHow long will it take to fix this bug?\nIn\nFourth\nInternational\nWorkshop on Mining Software Repositories, MSR 2007 (ICSE\nWorkshop), Minneapolis, MN,\nUSA,\nMay\n19-20, 2007, Proceedings, page 1 . IEEE Computer Society, 2007 .\n543\nLuca Gazzola, Daniela Micucci, and Leonardo Mariani.\nAutomatic software repair:\nA survey.\nIEEE\nTrans.\nSoftware Eng. ,\n45(1):34–67, 2019.\n544\nJifeng Xuan, Zhilei Ren, Ziyuan Wang, Xiaoyuan Xie, and Jiang He.\nProgress on approaches to automatic program repair.\nJournal of Software, 27(4):771 –784, 2016 .\n545\nMartin Monperrus.\nThe living review on automated program repair.\nResearch Report hal-01956501, HAL Archives Ouvertes,\n2018.\nVersion: 5 .\n546\nMichele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk.\nAn empirical\nstudy on learning bug-fixing patches in the wild via neural machine translation.\nACM\nTrans.\nSoftw.\nEng.\nMethodol. ,\n28(4):19:1–19:29, 2019 .\n547\nChristian Kern.\nAutomatic\nerror\ncorrection of java\nprograms.\nIn\nKai\nBollue,\nDominique\nG¨uckel,\nUlrich\nLoup,\nJacob\nSp¨onemann, and Melanie Winkler, editors, Proceedings of the Joint\nWorkshop\nof the\nGerman Research\nTraining\nGroups\nin Computer Science, Algorithmic synthesis of reactive and discrete-continuous systems, AlgoSyn 2010, May 31 - June\n2, 2010, page 155 . Verlagshaus Mainz, Aachen, Germany, 2010 .\n548\nYuchi Tian and Baishakhi Ray.\nAutomatically diagnosing and repairing error handling bugs in C.\nIn Eric Bodden, Wilhelm\nSch¨afer, Arie van Deursen, and Andrea Zisman, editors, Proceedings\nof the\n2017 11th\nJoint Meeting\non Foundations of\nSoftware Engineering, ESEC/FSE 2017, Paderborn, Germany, September 4-8, 2017, pages 752–762 . ACM, 2017 .\n549\nAntonio\nCarvalho,\nWelder\nPinheiro Luz, Diego Marcilio, Rodrigo Bonif´acio, Gustavo Pinto, and Edna Dias Canedo.\nC-\n3PR: A bot for fixing static analysis violations via pull requests.\nIn\nKostas\nKontogiannis,\nFoutse\nKhomh,\nAlexander\nChatzigeorgiou, Marios-Eleftherios Fokaefs, and Minghui Zhou, editors, 27th IEEE International Conference on Software\nAnalysis,\nEvolution\nand\nReengineering,\nSANER\n2020,\nLondon,\nON,\nCanada,\nFebruary\n18-21,\n2020,\npages\n161–171 .\nIEEE, 2020 .\n550\nAlfred V. Aho and Thomas G. Peterson.\nA minimum distance error-correcting parser for context-free languages.\nSIAM J.\nComput., 1(4):305–312, 1972 .\n551\nSusan L. Graham and Steven P. Rhodes.\nPractical syntactic error recovery.\nIn Patrick C. Fischer and Jeffrey D. Ullman,\neditors,\nConference Record\nof the ACM Symposium on Principles\nof Programming Languages, Boston, Massachusetts,\nUSA,\nOctober 1973, pages 52 –58. ACM Press, 1973 .\n552\nStuart Oliver Anderson and Roland Carl Backhouse.\nLocally least-cost error recovery in early’s algorithm.\nACM\nTrans.\nProgram.\nLang.\nSyst. , 3(3):318–347, 1981 .\n553\nMichael G. Burke.\nA\nPractical\nMethod for Lr\nand\nLl Syntactic Error Diagnosis\nand Recovery.\nPhD thesis, New York\nUniversity, USA, 1983 .\n554\nRahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade.\nDeepfix:\nFixing\ncommon\nC language errors by deep\nlearning.\nIn Satinder Singh and Shaul Markovitch, editors, Proceedings of the\nThirty-First AAAI Conference on Artificial\nIntelligence, February 4-9, 2017, San Francisco, California,\nUSA, pages 1345–1351 . AAAI Press, 2017 .\n555\nSahil Bhatia, Pushmeet Kohli, and Rishabh Singh.\nNeuro-symbolic program corrector for introductory programming assign-\nments.\nIn Proceedings\nof the 40th International\nConference on Software Engineering, ICSE 2018,\nGothenburg, Sweden,\nMay 27 - June 03, 2018, pages 60–70. ACM, 2018 .\n556\nUmair\nZ.\nAhmed,\nPawan Kumar, Amey Karkare, Purushottam Kar, and Sumit Gulwani.\nCompilation error repair:\nfor\nthe student programs, from the student programs.\nIn\nProceedings\nof\nthe\n40th\nInternational\nConference\non\nSoftware\nEngineering:\nSoftware\nEngineering Education and\nTraining, ICSE\n(SEET) 2018,\nGothenburg, Sweden, May 27 - June\n03,\n2018, pages 78–87 . ACM, 2018 .\n557\nEddie Antonio\nSantos,\nJoshua\nCharles\nCampbell,\nDhvani\nPatel,\nAbram\nHindle,\nand\nJos´e Nelson Amaral.\nSyntax and\nsensibility:\nUsing language models to detect and correct syntax errors.\nIn\n25th\nInternational\nConference on\nSoftware\nAnalysis,\nEvolution\nand\nReengineering,\nSANER\n2018,\nCampobasso,\nItaly,\nMarch\n20-23,\n2018,\npages\n311–322 .\nIEEE\nComputer Society, 2018 .\n558\nNeil Christopher Charles Brown, Michael K¨olling, Davin McCall, and Ian Utting.\nBlackbox:\na large scale repository of\nnovice programmers’ activity.\nIn\nThe 45th ACM Technical Symposium on Computer Science Education, SIGCSE 2014,\nAtlanta, GA,\nUSA, March 5-8, 2014, pages 223–228.\nACM, 2014 .\n559\nAli Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso, and Edward Aftandilian.\nDeepdelta:\nlearning to repair compi-\nlation errors.\nIn Proceedings of the ACM Joint Meeting on European Software Engineering Conference and Symposium\non the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019,\nTallinn, Estonia, August 26-30, 2019, pages\n925–936. ACM, 2019 .\n560\nRahul Gupta, Aditya Kanade, and Shirish K. Shevade.\nDeep reinforcement\nlearning for syntactic error repair in student\nprograms.\nIn\nThe\nThirty-Third AAAI Conference\non Artificial Intelligence, AAAI 2019,\nThe\nThirty-First\nInnovative\nApplications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019, pages 930–937 . AAAI Press,\n2019.\n561\nLiwei Wu,\nFei Li, Youhua Wu,\nand Tao\nZheng.\nGGF:\nA\ngraph-based\nmethod\nfor\nprogramming\nlanguage syntax error\ncorrection.\nIn\nICPC\n’20:\n28th\nInternational\nConference\non\nProgram\nComprehension,\nSeoul,\nRepublic\nof Korea,\nJuly\n13-15, 2020, pages 139–148 . ACM, 2020 .\n562\nMichihiro Yasunaga and Percy Liang.\nGraph-based, self-supervised program repair from diagnostic feedback.\nIn Proceedings\nof the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of\nProceedings\nof Machine Learning Research, pages 10799–10808 . PMLR, 2020 .\n563\nHossein Hajipour, Apratim Bhattacharyya,\nCristian-Alexandru Staicu, and Mario Fritz.\nSamplefix:\nLearning to generate\nfunctionally diverse fixes.\nIn\nMachine\nLearning\nand\nPrinciples\nand\nPractice\nof Knowledge Discovery\nin\nDatabases\n-\nInternational\nWorkshops\nof ECML\nPKDD\n2021,\nVirtual\nEvent,\nSeptember\n13-17,\n2021,\nProceedings,\nPart\nII, volume\n1525 of Communications in Computer and Information Science, pages 119–133 . Springer, 2021 .\n564\nMichihiro Yasunaga and Percy Liang.\nBreak-it-fix-it:\nUnsupervised learning for program repair.\nIn Proceedings\nof the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 11941–11952 . PMLR, 2021 .\nSci China Inf Sci\n92\n565\nToufique\nAhmed,\nPremkumar\nT. Devanbu, and Vincent J. Hellendoorn.\nLearning lenient parsing & typing via indirect\nsupervision.\nEmpir.\nSoftw. Eng. , 26(2):29, 2021 .\n566\nGeorgios Sakkas, Madeline Endres, Philip J. Guo, Westley Weimer, and Ranjit Jhala.\nSeq2parse:\nneurosymbolic parse error\nrepair.\nProc.\nACM Program.\nLang. , 6(OOPSLA2):1180–1206, 2022 .\n567\nXueyang Li, Shangqing Liu, Ruitao Feng, Guozhu Meng, Xiaofei Xie, Kai Chen, and Yang Liu.\nTransrepair:\nContext-aware\nprogram repair for compilation errors.\nIn 37th IEEE/ACM International Conference on Automated Software Engineering,\nASE 2022, Rochester, MI,\nUSA,\nOctober 10-14,\n2022, pages 108:1–108:13 . ACM, 2022 .\n568\nToufique Ahmed, Noah Rose Ledesma, and Premkumar T. Devanbu.\nSynshine:\nImproved fixing of syntax errors.\nIEEE\nTrans.\nSoftware Eng. , 49(4):2169–2181, 2023 .\n569\nZhuang Liu, Wayne Lin, Ya Shi, and Jun Zhao.\nA robustly optimized BERT pre-training approach with post-training.\nIn\nSheng Li, Maosong Sun, Yang Liu, Hua Wu, Kang Liu, Wanxiang Che, Shizhu He, and Gaoqi Rao, editors, Chinese\nCom-\nputational Linguistics - 20th China National Conference,\nCCL 2021, Hohhot,\nChina, August 13-15, 2021, Proceedings ,\nvolume 12869 of Lecture Notes in Computer Science, pages 471–484 . Springer, 2021 .\n570\nYongfeng\nGu,\nPing\nMa,\nXiangyang\nJia,\nHe\nJiang, and Jifeng Xuan.\nProgress\non software crash research.\nSCIENTIA\nSINICA Informationis, 49(11):1383–1398, 2019 .\n571\nClaire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer.\nGenprog:\nA generic\nmethod\nfor automatic\nsoftware repair. IEEE\nTrans.\nSoftware\nEng. , 38(1):54–72, 2012 .\n572\nChu-Pan Wong, Priscila Santiesteban, Christian K¨astner, and Claire Le Goues.\nVarfix:\nbalancing edit expressiveness and\nsearch effectiveness in automated program repair.\nIn Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massimil-\niano Di Penta, editors, ESEC/FSE ’21:\n29th ACM Joint European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering, Athens, Greece, August 23-28, 2021, pages 354–366 . ACM, 2021 .\n573\nHoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra.\nSemfix:\nprogram repair via semantic\nanalysis.\nIn\nDavid\nNotkin,\nBetty\nH.\nC.\nCheng,\nand\nKlaus\nPohl,\neditors,\n35th\nInternational\nConference\non\nSoftware\nEngineering, ICSE\n’13, San Francisco,\nCA,\nUSA,\nMay 18-26, 2013, pages 772–781 . IEEE Computer Society, 2013 .\n574\nSergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury.\nAngelix:\nscalable multiline program patch synthesis via symbolic\nanalysis.\nIn\nLaura\nK.\nDillon,\nWillem\nVisser,\nand\nLaurie\nA.\nWilliams,\neditors,\nProceedings\nof\nthe\n38th\nInternational\nConference on Software Engineering, ICSE 2016, Austin,\nTX,\nUSA, May\n14-22, 2016, pages 691–701 . ACM, 2016 .\n575\nJifeng Xuan, Matias Martinez, Favio Demarco, Maxime Clement, Sebastian R. Lamelas Marcote, Thomas Durieux, Daniel Le\nBerre, and Martin Monperrus.\nNopol:\nAutomatic repair of conditional statement bugs\nin java programs.\nIEEE\nTrans.\nSoftware Eng. , 43(1):34–55, 2017 .\n576\nShin Hwei Tan and Abhik Roychoudhury.\nrelifix:\nAutomated repair of software regressions.\nIn Antonia Bertolino, Gerardo\nCanfora, and Sebastian G. Elbaum, editors, 37th IEEE/ACM International Conference on Software Engineering, ICSE\n2015, Florence, Italy, May 16-24, 2015,\nVolume\n1, pages 471–482 . IEEE Computer Society, 2015 .\n577\nSeemanta Saha, Ripon K. Saha, and Mukul R. Prasad.\nHarnessing evolution for multi-hunk program repair.\nIn Joanne M.\nAtlee, Tevfik Bultan, and Jon Whittle, editors, Proceedings of the 41st International\nConference on Software Engineering,\nICSE 2019, Montreal, QC, Canada, May 25-31, 2019, pages 13–24 . IEEE / ACM, 2019 .\n578\nKui Liu, Anil Koyuncu, Dongsun Kim, and Tegawend´e F. Bissyand´e.\nTbar: revisiting template-based automated program\nrepair.\nIn Dongmei Zhang and Anders Møller, editors, Proceedings of the\n28th ACM SIGSOFT International Symposium\non Software\nTesting and Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019, pages 31–42. ACM, 2019 .\n579\nMartin White,\nMichele\nTufano,\nMatias Martinez,\nMartin Monperrus, and Denys Poshyvanyk.\nSorting and transforming\nprogram repair ingredients via deep learning code similarities. In 26th IEEE International Conference on Software Analysis,\nEvolution\nand Reengineering, SANER 2019, Hangzhou, China, February 24-27, 2019, pages 479–490 . IEEE, 2019 .\n580\nZimin Chen, Steve Kommrusch, Michele Tufano, Louis-No¨el Pouchet, Denys Poshyvanyk, and Martin Monperrus.\nSequencer:\nSequence-to-sequence learning for end-to-end program repair.\nIEEE\nTrans. Software Eng. , 47(9):1943–1959, 2021 .\n581\nNan Jiang, Thibaud Lutellier, and Lin Tan.\nCURE: code-aware neural machine translation for automatic program repair.\nIn 43rd IEEE/ACM International\nConference\non Software Engineering, ICSE 2021, Madrid,\nSpain, 22-30 May 2021 ,\npages 1161–1173 . IEEE, 2021 .\n582\nFan Long and Martin C. Rinard.\nAutomatic patch generation by learning correct code.\nIn Proceedings\nof the 43rd Annual\nACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2016, St.\nPetersburg, FL,\nUSA,\nJanuary 20 - 22, 2016, pages 298–312 . ACM, 2016 .\n583\nClaire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer.\nA systematic study of automated program\nrepair:\nFixing 55 out of 105 bugs for\n$8 each.\nIn\n34th\nInternational\nConference\non Software\nEngineering, ICSE 2012,\nJune 2-9, 2012, Zurich, Switzerland, pages 3–13 . IEEE Computer Society, 2012 .\n584\nMichele\nTufano,\nCody\nWatson,\nGabriele\nBavota,\nMassimiliano\nDi\nPenta,\nMartin\nWhite,\nand\nDenys Poshyvanyk.\nAn\nempirical investigation into learning bug-fixing patches in the wild via neural machine translation.\nIn\nProceedings\nof\nthe\n33rd ACM/IEEE International\nConference\non\nAutomated\nSoftware\nEngineering,\nASE 2018,\nMontpellier,\nFrance,\nSeptember 3-7, 2018, pages 832–837 . ACM, 2018 .\n585\nZhiyu Sun, Chao Xin, and Yanchun Sun.\nAn automatic semantic code repair service based on deep learning for programs\nwith single error.\nIn\n2019\nIEEE\nWorld\nCongress\non\nServices,\nSERVICES\n2019,\nMilan,\nItaly,\nJuly\n8-13, 2019,\npages\n360–361. IEEE, 2019 .\n586\nYangruibo Ding, Baishakhi Ray, Premkumar T. Devanbu, and Vincent J. Hellendoorn.\nPatching as translation: the data and\nthe metaphor.\nIn 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020, Melbourne,\nAustralia, September 21-25, 2020, pages 275–286 . IEEE, 2020 .\n587\nGeunseok Yang, Kyeongsic Min, and Byungjeong Lee.\nApplying deep learning algorithm to automatic bug localization and\nrepair.\nIn SAC\n’20:\nThe 35th ACM/SIGAPP Symposium on Applied Computing, online event, Brno,\nCzech Republic,\nMarch 30 - April 3, 2020, pages 1634–1641 . ACM, 2020 .\n588\nLantao Yu, Weinan Zhang,\nJun Wang,\nand Yong Yu.\nSeqgan:\nSequence\ngenerative adversarial nets with policy gradi-\nent.\nIn Proceedings\nof the\nThirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco,\nCalifornia,\nUSA, pages 2852–2858 . AAAI Press, 2017 .\n589\nThibaud Lutellier,\nHung Viet Pham,\nLawrence Pang, Yitong Li, Moshi Wei, and Lin Tan.\nCoconut:\ncombining context-\naware neural translation models using ensemble for program repair.\nIn\nISSTA\n’20:\n29th ACM SIGSOFT International\nSymposium on Software\nTesting and Analysis,\nVirtual Event, USA,\nJuly\n18-22, 2020, pages 101–114. ACM, 2020 .\n590\nMatias Martinez, Thomas Durieux, Romain Sommerard, Jifeng Xuan, and Martin Monperrus. Automatic repair of real bugs\nin java:\na large-scale experiment on the defects4j dataset.\nEmpir.\nSoftw. Eng. , 22(4):1936–1964, 2017 .\nSci China Inf Sci\n93\n591\nRipon K. Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul R. Prasad.\nBugs.jar:\na large-scale, diverse dataset of\nreal-world java bugs.\nIn Proceedings\nof the\n15th International\nConference\non Mining Software Repositories, MSR 2018,\nGothenburg, Sweden, May 28-29, 2018, pages 10–13. ACM, 2018 .\n592\nHaoye Tian, Kui Liu, Abdoul Kader Kabor´e, Anil Koyuncu, Li Li, Jacques Klein, and Tegawend´e F. Bissyand´e.\nEvaluating\nrepresentation learning of code changes for predicting patch correctness in program repair.\nIn 35th IEEE/ACM International\nConference on Automated Software Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020, pages 981–992 .\nIEEE, 2020 .\n593\nElizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang.\nHoppity:\nLearning graph transformations\nto detect and fix bugs in programs.\nIn\n8th\nInternational\nConference\non\nLearning\nRepresentations,\nICLR\n2020,\nAddis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net, 2020 .\n594\nYu\nTang,\nLong\nZhou,\nAmbrosio Blanco,\nShujie Liu,\nFuru Wei, Ming Zhou, and Muyun Yang.\nGrammar-based\npatches\ngeneration for automated program repair.\nIn Findings of the Association for Computational Linguistics:\nACL/IJCNLP\n2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 1300–1305 .\nAssociation for\nComputational Linguistics, 2021 .\n595\nShan Huang, Xiao Zhou, and Sang Chin.\nApplication of seq2seq\nmodels on code correction.\nFrontiers\nArtif.\nIntell. ,\n4:590215, 2021.\n596\nMd. Mostafizer Rahman, Yutaka Watanobe, and Keita Nakamura.\nA bidirectional LSTM language model for code evaluation\nand repair.\nSymmetry, 13(2):247, 2021 .\n597\nBerkay Berabi, Jingxuan He, Veselin Raychev, and Martin T. Vechev.\nTfix:\nLearning to fix coding errors with a text-to-text\ntransformer.\nIn Proceedings of the 38th International Conference on Machine Learning, ICML 2021,\n18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine Learning Research, pages 780–791 . PMLR, 2021 .\n598\nBen Tang, Bin Li, Lili Bo, Xiaoxue Wu, Sicong Cao, and Xiaobing Sun.\nGrasp:\nGraph-to-sequence learning for automated\nprogram repair. In 21st IEEE International Conference on Software Quality, Reliability and Security, QRS 2021, Hainan,\nChina, December 6-10, 2021, pages 819 –828 . IEEE, 2021 .\n599\nBal´azs Szalontai, Andr´as Vad´asz, Zsolt Rich´ard Borsi, Ter´ez A. V´arkonyi, Bal´azs Pint´er, and Tibor Gregorics.\nDetecting\nand fixing nonidiomatic snippets in python source code with deep learning.\nIn\nIntelligent\nSystems\nand\nApplications\n-\nProceedings\nof the 2021 Intelligent Systems\nConference, IntelliSys 2021, Amsterdam,\nThe Netherlands,\n2-3 September,\n2021, Volume 1, volume 294 of Lecture Notes in Networks and Systems, pages 129–147 . Springer, 2021 .\n600\nYi Li, Shaohua Wang, and Tien N. Nguyen. DEAR: A novel deep learning-based approach for automated program repair.\nIn\n44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA,\nUSA, May 25-27,\n2022, pages 511–523 . ACM, 2022 .\n601\nXuezheng\nXu,\nXudong\nWang,\nand\nJingling Xue.\nM3V:\nmulti-modal\nmulti-view\ncontext\nembedding for repair operator\nprediction.\nIn IEEE/ACM International Symposium on Code\nGeneration and Optimization,\nCGO 2022, Seoul, Korea,\nRepublic of, April 2-6, 2022, pages 266–277 . IEEE, 2022 .\n602\nXiangxin Meng, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu.\nImproving fault localization and program repair\nwith deep semantic features and transferred knowledge.\nIn 44th IEEE/ACM 44th International Conference on Software\nEngineering, ICSE 2022, Pittsburgh, PA,\nUSA, May 25-27, 2022, pages 1169–1180 . ACM, 2022 .\n603\nMisoo Kim, Youngkyoung Kim, Jinseok Heo, Hohyeon Jeong, Sungoh Kim, and Eunseok Lee.\nImpact of defect instances\nfor successful deep learning-based automatic program repair.\nIn IEEE International Conference on Software Maintenance\nand Evolution, ICSME 2022, Limassol, Cyprus, October 3-7, 2022, pages 419–423 . IEEE, 2022 .\n604\nMohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan.\nDeepdiagnosis:\nAutomatically diagnosing faults and\nrecommending actionable fixes in deep learning programs.\nIn 44th IEEE/ACM 44th International\nConference on Software\nEngineering, ICSE 2022, Pittsburgh, PA,\nUSA, May 25-27, 2022, pages 561–572 .\nACM, 2022 .\n605\nJie Yao, Bingbing Rao, Weiwei Xing, and Liqiang Wang.\nBug-transformer:\nAutomated program repair using attention-based\ndeep neural network.\nJ.\nCircuits Syst.\nComput. , 31(12):2250210:1 –2250210:26, 20 22 .\n606\nDapeng Yan,\nKui\nLiu,\nYuqing\nNiu, Li Li, Zhe Liu, Zhiming Liu, Jacques Klein, and Tegawend´e F.\nBissyand´e.\nCrex:\nPredicting patch correctness in automated repair of C programs through transfer learning of execution semantics.\nInf.\nSoftw.\nTechnol. , 152:107043, 2022 .\n607\nKexin Pei, Zhou Xuan, Junfeng Yang, Suman Jana, and Baishakhi Ray.\nLearning approximate execution semantics from\ntraces for binary function similarity.\nIEEE\nTrans.\nSoftware Eng. , 49(4):2776–2790, 2023 .\n608\nSaikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray.\nCODIT: code editing with tree-based neural\nmodels.\nIEEE\nTrans.\nSoftware Eng. , 48(4):1385–1399, 2022 .\n609\nHe Ye,\nMatias Martinez,\nand Martin Monperrus.\nNeural program repair with execution-based backpropagation.\nIn\n44th\nIEEE/ACM 44th\nInternational\nConference\non\nSoftware\nEngineering,\nICSE\n2022,\nPittsburgh,\nPA,\nUSA,\nMay\n25-27,\n2022, pages 1506–1518 . ACM, 2022 .\n610\nHe Ye, Jian Gu, Matias Martinez, Thomas Durieux, and Martin Monperrus.\nAutomated classification of overfitting patches\nwith statically extracted code features.\nIEEE\nTrans.\nSoftware\nEng. , 48(8):2920–2938, 2022 .\n611\nHe Ye, Matias Martinez, Xiapu Luo, Tao Zhang, and Martin Monperrus.\nSelfapr:\nSelf-supervised program repair with test\nexecution diagnostics.\nIn\n37th\nIEEE/ACM International\nConference\non\nAutomated Software Engineering, ASE 2022,\nRochester, MI,\nUSA,\nOctober 10-14,\n2022, pages 92:1 –92:13 . ACM, 2022 .\n612\nChunqiu Steven Xia and Lingming Zhang.\nLess training, more repairing please:\nrevisiting automated program repair via\nzero-shot learning.\nIn Abhik Roychoudhury, Cristian Cadar, and Miryung Kim, editors, Proceedings of the 30th ACM Joint\nEuropean Software Engineering\nConference\nand\nSymposium\non\nthe\nFoundations\nof Software\nEngineering,\nESEC/FSE\n2022, Singapore, Singapore, November 14-18, 2022, pages 959–971 . ACM, 2022 .\n613\nMisoo Kim, Youngkyoung Kim, Hohyeon Jeong, Jinseok Heo, Sungoh Kim, Hyunhee Chung, and Eunseok Lee.\nAn empirical\nstudy of deep transfer learning-based program repair for kotlin projects.\nIn Proceedings\nof the\n30th\nACM Joint Euro-\npean Software Engineering\nConference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022,\nSingapore, Singapore, November 14-18, 2022, pages 1441–1452 . ACM, 2022 .\n614\nHaoye Tian, Yinghua Li, Weiguo Pian, Abdoul Kader Kabor´e, Kui Liu, Andrew Habib, Jacques Klein, and Tegawend´e F.\nBissyand´e.\nPredicting patch correctness based on the similarity of failing test cases.\nACM\nTrans.\nSoftw.\nEng.\nMethodol. ,\n31(4):77:1–77:30, 2022 .\n615\nWei Yuan, Quanjun Zhang, Tieke He, Chunrong Fang, Nguyen Quoc Viet Hung, Xiaodong Hao, and Hongzhi Yin.\nCIRCLE:\ncontinual repair across programming languages.\nIn ISSTA\n’22:\n31st ACM SIGSOFT International Symposium on Software\nTesting and Analysis,\nVirtual Event, South Korea, July\n18 - 22, 2022, pages 678–690. ACM, 2022 .\nSci China Inf Sci\n94\n616\nLiushan Chen, Yu Pei, Minxue Pan, Tian Zhang, Qixin Wang, and Carlo A. Furia.\nProgram repair with repeated learning.\nIEEE Trans.\nSoftware Eng. , 49(2):831–848, 2023 .\n617\nAndrea Stocco, Rahulkrishna Yandrapally, and Ali Mesbah.\nVisual web test repair.\nIn Proceedings\nof the 2018 ACM Joint\nMeeting\non\nEuropean\nSoftware\nEngineering\nConference\nand\nSymposium\non\nthe\nFoundations\nof Software\nEngineering,\nESEC/SIGSOFT FSE 2018, Lake Buena\nVista, FL,\nUSA, November 04-09,\n2018, pages 503–514 . ACM, 2018 .\n618\nMinxue Pan, Tongtong Xu, Yu Pei, Zhong Li, Tian Zhang, and Xuandong Li.\nGui-guided test script repair for mobile apps.\nIEEE Trans.\nSoftware Eng. , 48(3):910–929, 2022 .\n619\nZhilei Ren, Shiwei Sun, Jifeng Xuan, Xiaochen Li, Zhide Zhou, and He Jiang.\nAutomated patching for unreproducible\nbuilds.\nIn 44th IEEE/ACM 44th International\nConference on Software Engineering, ICSE 2022, Pittsburgh, PA,\nUSA,\nMay 25-27, 2022, pages 200–211 . ACM, 2022 .\n620\nFoyzul Hassan and Xiaoyin Wang.\nHirebuild:\nan automatic approach to history-driven repair of build scripts.\nIn Proceedings\nof the 40th International Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018 ,\npages 1078–1089 . ACM, 2018 .\n621\nYiling Lou,\nJunjie\nChen, Lingming Zhang, Dan Hao, and Lu Zhang.\nHistory-driven build failure fixing:\nhow far are we?\nIn Proceedings\nof the 28th ACM SIGSOFT International Symposium on Software\nTesting\nand Analysis,\nISSTA\n2019,\nBeijing, China, July 15-19, 2019, pages 43–54 . ACM, 2019.\n622\nBenjamin Loriot, Fernanda Madeiral, and Martin Monperrus.\nStyler:\nlearning formatting conventions to repair checkstyle\nviolations.\nEmpir.\nSoftw.\nEng. , 27(6):149, 2022.\n623\nSiqi Ma, Ferdian Thung, David Lo, Cong Sun, and Robert H. Deng.\nVurle: Automatic vulnerability detection and repair by\nlearning from examples.\nIn\nComputer Security - ESORICS 2017 - 22nd European Symposium on Research in\nComputer\nSecurity, Oslo, Norway, September 11-15, 2017, Proceedings, Part II, volume 10493 of Lecture Notes in Computer Science ,\npages 229–246 . Springer, 2017 .\n624\nJacob Harer, Onur Ozdemir, Tomo Lazovich, Christopher P. Reale, Rebecca L. Russell, Louis Y. Kim, and Peter Chin.\nLearning to repair software vulnerabilities with generative adversarial networks.\nIn\nAdvances\nin\nNeural\nInformation\nProcessing Systems 31:\nAnnual\nConference\non Neural Information Processing Systems 2018, NeurIPS 2018, December\n3-8, 2018, Montr´eal,\nCanada, pages 7944–7954, 2018 .\n625\nZhou Zhou, Lili Bo, Xiaoxue Wu, Xiaobing Sun, Tao Zhang, Bin Li, Jiale Zhang, and Sicong Cao.\nSPVF: security property\nassisted vulnerability fixing via attention-based models.\nEmpir.\nSoftw.\nEng. , 27(7):171, 2022 .\n626\nKai Huang, Su Yang, Hongyu Sun, Chengyi Sun, Xuejun Li, and Yuqing Zhang.\nRepairing security vulnerabilities using\npre-trained programming language models.\nIn 52nd Annual IEEE/IFIP International Conference on Dependable Systems\nand Networks, DSN Workshops 2022, Baltimore, MD,\nUSA,\nJune 27-30, 2022, pages 111–116 . IEEE, 2022 .\n627\nZimin Chen, Steve Kommrusch, and Martin Monperrus.\nNeural transfer learning for repairing security vulnerabilities in C\ncode.\nIEEE\nTrans.\nSoftware Eng. , 49(1):147–165, 2023 .\n628\nJianlei Chi, Yu Qu, Ting Liu, Qinghua Zheng, and Heng Yin. Seqtrans:\nAutomatic vulnerability fix via sequence to sequence\nlearning.\nIEEE\nTrans.\nSoftware\nEng. , 49(2):564–585, 2023 .\n629\nRajdeep\nDas,\nUmair\nZ.\nAhmed, Amey Karkare,\nand Sumit Gulwani.\nPrutor:\nA system for tutoring cs1 and collecting\nstudent programs for analysis.\narXiv preprint\narXiv:1608.03828, 2016 .\n630\nNeil C. C. Brown, Amjad Altadmri, Sue Sentance, and Michael K¨olling.\nBlackbox, five years on: An evaluation of a large-\nscale programming data collection project.\nIn\nProceedings\nof the\n2018\nACM\nConference\non\nInternational\nComputing\nEducation Research, ICER ’18, page 196–204, New York, NY, USA, 2018 . Association for Computing Machinery.\n631\nRen´e Just, Darioush Jalali, and Michael D. Ernst. Defects4j:\na database of existing faults to enable controlled testing studies\nfor java programs.\nIn Corina S. Pasareanu and Darko Marinov, editors, International Symposium on Software\nTesting and\nAnalysis, ISSTA ’14, San Jose,\nCA,\nUSA\n- July 21 - 26, 2014, pages 437–440. ACM, 2014 .\n632\nManish Motwani, Sandhya Sankaranarayanan, Ren´e Just, and Yuriy Brun.\nDo automated program repair techniques repair\nhard and important bugs?\nIn Michel Chaudron, Ivica Crnkovic, Marsha Chechik, and Mark Harman, editors, Proceedings of\nthe 40th International Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018 ,\npage 25 . ACM, 2018 .\n633\nMatias Martinez, Thomas Durieux, Romain Sommerard, Jifeng Xuan, and Martin Monperrus. Automatic repair of real bugs\nin java:\nA large-scale experiment on the defects4j dataset.\nCoRR, abs/1811.02429, 2018 .\n634\nGrowingbugs, 2024 . https://github.com/jiangyanjie/GrowingBugs.\n635\nYanjie Jiang, Hui Liu, Nan Niu, Lu Zhang, and Yamin Hu.\nExtracting concise bug-fixing patches from human-written patches\nin version control systems.\nIn Proceedings of the 43rd International Conference on Software Engineering\n(ICSE’21), 2021 .\n636\nYanjie Jiang, Hui Liu, Xiaoqing Luo, Zhihao Zhu, Xiaye Chi, Nan Niu, Yuxia Zhang, Yamin Hu, Pan Bian, and Lu Zhang.\nBugbuilder: An automated approach to building bug repository.\nIEEE\nTransactions\non Software Engineering, pages 1–22,\n2022.\n637\nQuang-Cuong Bui, Riccardo Scandariato, and Nicol´as E. D´ıaz Ferreyra.\nVul4j: A dataset of reproducible java vulnerabilities\ngeared towards the study of program repair techniques.\nIn\n2022 IEEE/ACM 19th International\nConference\non Mining\nSoftware\nRepositories (MSR), pages 464–468, 2022 .\n638\nGeorgios Nikitopoulos, Konstantina Dritsa, Panos Louridas, and Dimitris Mitropoulos.\nCrossvul:\nA\ncross-language vul-\nnerability dataset with commit data.\nIn Proceedings\nof the\n29th ACM Joint Meeting on European Software Engineering\nConference\nand Symposium on the Foundations\nof Software Engineering, ESEC/FSE 2021, page 1565–1569, New York,\nNY, USA, 2021 . Association for Computing Machinery.\n639\nWeiqin\nZou,\nDavid\nLo, Zhenyu Chen, Xin Xia, Yang Feng, and Baowen Xu.\nHow\npractitioners\nperceive automated bug\nreport management techniques.\nIEEE\nTransactions\non Software Engineering, 46(8):836–862, 2018 .\n640\nNicolas Bettenburg, Sascha Just, Adrian Schr¨oter, Cathrin Weiss, Rahul Premraj, and Thomas Zimmermann.\nWhat makes\na good bug report?\nIn\nProceedings\nof the\n16th\nACM SIGSOFT International\nSymposium\non Foundations\nof software\nengineering, pages 308–318, 2008 .\n641\nDong-Gun Lee and Yeong-Seok Seo.\nSystematic review of bug report processing techniques to improve software management\nperformance.\nJ. Inf.\nProcess.\nSyst. , 15(4):967–985, 2019 .\n642\nJohn Anvik.\nAutomating\nbug report\nassignment.\nIn\nProceedings\nof\nthe\n28th\ninternational\nconference\non\nSoftware\nengineering, pages 937 –940, 2006 .\n643\nHe Jiang, Xiaochen Li, Zhilei Ren, Jifeng Xuan, and Zhi Jin.\nToward better summarizing bug reports with crowdsourcing\nelicited attributes.\nIEEE\nTransactions on Reliability, 68(1):2–22, 2018 .\n644\nYoushuai Tan, Sijie Xu, Zhaowei Wang, Tao Zhang, Zhou Xu, and Xiapu Luo.\nBug severity prediction using question-and-\nSci China Inf Sci\n95\nanswer pairs from stack overflow.\nJournal of Systems and Software, 165:110567, 2020 .\n645\nTing\nZhang,\nDongGyun\nHan,\nVenkatesh\nVinayakarao,\nIvana\nClairine\nIrsan,\nBowen\nXu,\nFerdian\nThung,\nDavid\nLo,\nand\nLingxiao Jiang.\nDuplicate\nbug\nreport\ndetection:\nHow far are we?\nACM\nTransactions\non\nSoftware\nEngineering\nand\nMethodology, 32(4):1–32, 2023 .\n646\nXiaochen Li, He Jiang, Dong Liu, Zhilei Ren, and Ge Li.\nUnsupervised deep bug report summarization.\nIn\nProceedings\nof\nthe 26th Conference on Program Comprehension, pages 144–155, 2018 .\n647\nFan Fang, John Wu, Yanyan Li, Xin Ye, Wajdi Aljedaani, and Mohamed Wiem Mkaouer. On the classification of bug reports\nto improve bug localization.\nSoft\nComputing, 25:7307–7323, 2021 .\n648\nCheng Zhou, Bin Li, Xiaobing Sun, and Sheng Yu.\nLeveraging\nmulti-level embeddings for knowledge-aware bug report\nreformulation.\nJournal of Systems and Software, page 111617, 2023 .\n649\nJianjun He, Ling Xu, Meng Yan, Xin Xia, and Yan Lei.\nDuplicate bug report detection using dual-channel convolutional\nneural networks.\nIn Proceedings\nof the 28th International\nConference on Program Comprehension, pages 117–127, 2020 .\n650\nGuanping Xiao, Xiaoting Du, Yulei Sui, and Tao Yue.\nHindbr:\nHeterogeneous\ninformation\nnetwork based duplicate bug\nreport prediction.\nIn\n2020\nIEEE\n31st\nInternational\nSymposium\non\nSoftware\nReliability\nEngineering\n(ISSRE),\npages\n195–206. IEEE, 2020 .\n651\nQi Xie, Zhiyuan Wen, Jieming Zhu, Cuiyun Gao, and Zibin Zheng.\nDetecting duplicate bug reports with convolutional\nneural networks.\nIn 2018 25th Asia-Pacific Software Engineering\nConference\n(APSEC), pages 416–425 .\nIEEE, 2018 .\n652\nJayati Deshmukh, KM Annervaz, Sanjay Podder, Shubhashis Sengupta, and Neville Dubash.\nTowards accurate duplicate bug\nretrieval using deep learning techniques.\nIn\n2017 IEEE International conference\non software maintenance and evolution\n(ICSME), pages 115–124 . IEEE, 2017 .\n653\nAmar Budhiraja, Kartik Dutta, Raghu Reddy, and Manish Shrivastava.\nDWEN: deep word embedding network for duplicate\nbug report detection in software repositories.\nIn Proceedings of the 40th International Conference on software engineering:\ncompanion proceeedings, pages 193–194, 2018 .\n654\nHaruna\nIsotani,\nHironori\nWashizaki,\nYoshiaki\nFukazawa,\nTsutomu\nNomoto,\nSaori\nOuji,\nand\nShinobu\nSaito.\nDuplicate\nbug report detection by using sentence embedding and fine-tuning.\nIn 2021 IEEE International Conference on Software\nMaintenance and Evolution (ICSME), pages 535–544 . IEEE, 2021 .\n655\nYuan Jiang, Xiaohong Su, Christoph Treude, Chao Shang, and Tiantian Wang. Does deep learning improve the performance\nof duplicate bug report detection? an empirical study.\nJournal of Systems and Software, page 111607, 2023 .\n656\nUgur Koc, Shiyi Wei, Jeffrey S Foster, Marine Carpuat, and Adam A Porter.\nAn empirical assessment of machine learning\napproaches for triaging reports of a Java static analysis tool.\nIn\n2019 12th ieee conference on software testing, validation\nand verification (icst), pages 288–299 . IEEE, 2019 .\n657\nAdrian-C˘at˘alin Florea, John Anvik, and R˘azvan Andonie.\nParallel implementation of a bug report assignment recommender\nusing deep learning.\nIn Artificial Neural Networks\nand Machine Learning–ICANN 2017:\n26th International\nConference\non Artificial Neural Networks, Alghero, Italy, September 11-14,\n2017, Proceedings, Part II 26, pages 64–71 .\nSpringer,\n2017.\n658\nSun-Ro Lee, Min-Jae Heo, Chan-Gun Lee, Milhan Kim, and Gaeul Jeong.\nApplying\ndeep\nlearning based automatic bug\ntriager to industrial projects.\nProceedings\nof the 2017 11th Joint Meeting\non Foundations of Software Engineering, 2017 .\n659\nSenthil Mani, Anush Sankaran, and Rahul Aralikatte.\nDeeptriage:\nExploring the effectiveness of deep\nlearning for bug\ntriaging.\nIn Proceedings of the ACM India Joint International Conference on Data Science and Management of Data ,\npage 171–179, New York, NY, USA, 2019 . Association for Computing Machinery.\n660\nYong Liu, Xuexin Qi, Jiali Zhang, Hui Li, Xin Ge, and Jun Ai.\nAutomatic bug triaging via deep reinforcement learning.\nApplied Sciences, 2022 .\n661\nZhuobing Han, Xiaohong Li, Zhenchang Xing, Hongtao Liu, and Zhiyong Feng.\nLearning to\npredict\nseverity of software\nvulnerability using only vulnerability description.\nIn 2017 IEEE International conference on software maintenance and\nevolution (ICSME), pages 125–136 . IEEE, 2017 .\n662\nLuiz Alberto Ferreira Gomes, Ricardo da Silva Torres, and Mario L´ucio Cˆortes.\nBug report severity level prediction in open\nsource software: A survey and research opportunities.\nInformation\nand software technology, 115:58–78, 2019 .\n663\nYuki Noyori, Hironori Washizaki, Yoshiaki Fukazawa, Keishi Ooshima, Hideyuki Kanuka, and Shuhei Nojiri.\nDeep learning\nand gradient-based extraction of bug report features related to bug fixing time.\nFrontiers in\nComputer Science , 5:1032440,\n2023.\n664\nHaoran Liu, Yue Yu, Shanshan Li, Mingyang Geng, Xiaoguang Mao, and Xiangke Liao.\nHow to cherry pick the bug report\nfor better summarization?\nEmpirical Software\nEngineering, 26:1–36, 2021 .\n665\nHaoran Liu, Yue Yu, Shanshan Li, Yong Guo, Deze Wang, and Xiaoguang Mao.\nBugsum:\nDeep context understanding\nfor bug report summarization.\nIn\nProceedings\nof the 28th International\nConference\non Program\nComprehension, pages\n94–105, 2020.\n666\nSongqiang Chen, Xiaoyuan Xie, Bangguo Yin, Yuanxiang Ji, Lin Chen, and Baowen Xu.\nStay\nprofessional and efficient:\nautomatically generate titles for your bug reports.\nIn Proceedings\nof the 35th IEEE/ACM International\nConference on\nAutomated Software Engineering, pages 385–397, 2020 .\n667\nHao Lin, Xiang Chen, Xuejiao Chen, Zhanqi Cui, Yun Miao, Shan Zhou, Jianmin Wang, and Zhan Su.\nGen-FL: Quality\nprediction-based filter for automated issue title generation.\nJournal of Systems and Software, 195:111513, 2023 .\n668\nYan Xiao, Jacky Keung, Kwabena E Bennin, and Qing Mi.\nImproving bug localization with word embedding and enhanced\nconvolutional neural networks.\nInformation\nand Software\nTechnology, 105:17–29, 2019 .\n669\nYan\nXiao,\nJacky Keung,\nQing Mi, and Kwabena E Bennin.\nImproving\nbug\nlocalization with an enhanced convolutional\nneural network.\nIn 2017 24th Asia-Pacific Software Engineering\nConference\n(APSEC), pages 338–347 . IEEE, 2017 .\n670\nBei Wang, Ling Xu, Meng Yan, Chao Liu, and Ling Liu.\nMulti-dimension convolutional neural network for bug localization.\nIEEE Transactions on Services Computing, 15(3):1649–1663, 2020 .\n671\nAn\nNgoc\nLam,\nAnh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen.\nBug localization with combination of deep\nlearning and\ninformation retrieval.\nIn\n2017\nIEEE/ACM\n25th\nInternational\nConference\non\nProgram\nComprehension\n(ICPC), pages 218 –229 . IEEE, 2017 .\n672\nShasha Cheng, Xuefeng Yan, and Arif Ali Khan.\nA\nsimilarity\nintegration method based information retrieval and word\nembedding in bug localization.\nIn 2020 IEEE 20th International Conference on Software\nQuality, Reliability and Security\n(QRS), pages 180–187 . IEEE, 2020 .\n673\nAn\nNgoc\nLam,\nAnh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen.\nCombining deep learning with information\nretrieval to localize buggy files for bug reports (n) .\nIn\n2015\n30th\nIEEE/ACM International\nConference\non\nAutomated\nSci China Inf Sci\n96\nSoftware Engineering\n(ASE), pages 476–481 . IEEE, 2015 .\n674\nPablo Loyola, Kugamoorthy Gajananan, and Fumiko Satoh.\nBug localization by learning to rank and represent bug inducing\nchanges.\nIn Proceedings\nof the 27th ACM International\nConference on Information and Knowledge Management, pages\n657–665, 2018 .\n675\nZiye Zhu, Yun Li, Hanghang Tong, and Yu Wang.\nCooba:\nCross-project bug localization via adversarial transfer learning.\nIn IJCAI, 2020 .\n676\nJiaxuan Han, Cheng Huang, Siqi Sun, Zhonglin Liu, and Jiayong Liu.\nbjXnet:\nan improved bug localization model based\non code property graph and attention mechanism.\nAutomated Software\nEngineering, 30(1):12, 2023.\n677\nHongliang Liang, Dengji Hang, and Xiangyu Li.\nModeling function-level interactions for file-level bug localization.\nEmpirical\nSoftware Engineering, 27(7):186, 2022.\n678\nMorakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Chaiyong Ragkhitwetsagul, and Aditya Ghose.\nAu-\ntomatically recommending components for issue reports using deep learning.\nEmpirical\nSoftware\nEngineering,\n26:1–39,\n2021.\n679\nXuan Huo, Ferdian Thung, Ming Li, David Lo, and Shu-Ting Shi.\nDeep transfer bug localization.\nIEEE\nTransactions\non\nsoftware engineering, 47(7):1368 –1380, 2019 .\n680\nMarlo Haering,\nChristoph\nStanik, and Walid Maalej.\nAutomatically matching bug reports with related app reviews.\nIn\n2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 970–981 . IEEE, 2021 .\n681\nHang Ruan, Bihuan Chen, Xin Peng, and Wenyun Zhao.\nDeepLink:\nRecovering issue-commit links based on deep learning.\nJournal of Systems and Software, 158:110406, 2019 .\n682\nRui Xie, Long Chen, Wei Ye, Zhiyu Li, Tianxiang Hu, Dongdong Du, and Shikun Zhang.\nDeeplink:\nA code knowledge graph\nbased deep learning approach for issue-commit link recovery.\nIn\n2019 IEEE 26th International\nConference\non\nSoftware\nAnalysis, Evolution and Reengineering (SANER), pages 434–444 . IEEE, 2019 .\n683\nSun-Ro Lee, Min-Jae Heo, Chan-Gun Lee, Milhan Kim, and Gaeul Jeong.\nApplying\ndeep\nlearning based automatic bug\ntriager to industrial projects.\nIn Proceedings of the 2017 11th Joint Meeting on foundations of software engineering, pages\n926–931, 2017 .\n684\nShengqu Xi, Yuan Yao, Xusheng Xiao, Feng Xu, and Jian Lu.\nAn effective approach for routing the bug reports to the right\nfixers.\nIn Proceedings\nof the\n10th Asia-Pacific Symposium on Internetware, pages 1–10, 2018 .\n685\nWei Fu and Tim Menzies.\nEasy over hard:\nA case study on deep learning.\nIn Proceedings of the 2017 11th Joint Meeting on\nFoundations of Software Engineering, ESEC/FSE 2017, page 49–60, New York, NY, USA, 2017 .\nAssociation for Computing\nMachinery.\n686\nEeshita Biswas, K. Vijay-Shanker, and Lori Pollock.\nExploring word embedding techniques to improve sentiment analysis\nof software engineering texts. MSR ’19, page 68–78 . IEEE Press, 2019 .\n687\nZeeshan Ahmed Nizamani, Hui Liu, David Matthew Chen, and Zhendong Niu.\nAutomatic approval prediction for software\nenhancement requests.\nAutomated\nSoftware Engineering, 25:347–381, 2018 .\n688\nXiaochen Li, He Jiang, Yasutaka Kamei, and Xin Chen.\nBridging semantic gaps between natural languages and APIs with\nword embedding.\nIEEE\nTransactions\non Software Engineering, 46(10):1081–1097, 2018 .\n689\nMinsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W. Keckler.\nVDNN: Virtualized deep neural\nnetworks for scalable, memory-efficient neural network design.\nIn\nThe 49th Annual IEEE/ACM International Symposium\non Microarchitecture, MICRO-49, 2016 .\n690\nLinnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons:\nDynamic\nGPU memory management for training deep neural networks.\nIn\nProceedings\nof\nthe\n23rd ACM\nSIGPLAN\nSymposium\non\nPrinciples\nand\nPractice\nof Parallel\nProgramming,\nPPoPP\n’18,\npage 41–53,\nNew York,\nNY, USA, 2018 .\nAssociation for Computing Machinery.\n691\nKevin Moran, Carlos Bernal-C´ardenas, Michael Curcio, Richard Bonett, and Denys Poshyvanyk.\nMachine learning-based\nprototyping of graphical user interfaces for mobile apps.\nIEEE\nTransactions on Software Engineering, 46(2):196–221, 2018 .\n692\nTrong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and Tien N Nguyen.\nExploring API embedding for API usages\nand applications.\nIn 2017 IEEE/ACM 39th International\nConference on Software Engineering\n(ICSE), pages 438–449.\nIEEE, 2017 .\n693\nYao Wan,\nZhou\nZhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu.\nImproving automatic source\ncode summarization via deep reinforcement learning.\nIn Proceedings\nof the 33rd ACM/IEEE international conference on\nautomated software engineering, pages 397–407, 2018 .\n694\nFrederick P. Brooks.\nThe mythical man-month.\nIn Reliable\nSoftware, 1975 .\n695\nAudris Mockus and James D. Herbsleb. Expertise browser:\nA quantitative approach to identifying expertise.\nIn Proceedings\nof the 24th International Conference on Software Engineering, ICSE ’02, pages 503–512, New York, NY, USA, 2002 . ACM.\n696\nJohn\nAnvik,\nLyndon\nHiew,\nand Gail C. Murphy.\nWho should fix this bug?\nIn\nProceedings\nof the\n28th\nInternational\nConference on Software Engineering, ICSE ’06, pages 361–370, New York, NY, USA, 2006 . ACM.\n697\nD. Ma, D. Schuler, T. Zimmermann, and J. Sillito.\nExpert recommendation with usage expertise.\nIn\n2009 IEEE Interna-\ntional Conference on Software Maintenance, pages 535–538, Sept 2009 .\n698\nMinghui Zhou and Audris Mockus.\nDeveloper fluency: Achieving true mastery in software projects.\nIn Proceedings of the\nEighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE ’10, pages 137–146,\nNew York, NY, USA, 2010 . ACM.\n699\nThomas\nFritz,\nGail\nC.\nMurphy,\nEmerson\nMurphy-Hill,\nJingwen\nOu,\nand Emily Hill.\nDegree-of-knowledge:\nModeling a\ndeveloper’s knowledge of code.\nACM\nTrans.\nSoftw.\nEng.\nMethodol. , 23(2):14:1–14:42, April 2014 .\n700\nMitchell Joblin, Wolfgang Mauerer, Sven Apel, Janet Siegmund, and Dirk Riehle.\nFrom developer\nnetworks to verified\ncommunities:\na\nfine-grained approach.\nIn\nProceedings\nof the\n37th\nInternational\nConference\non\nSoftware\nEngineering-\nVolume 1, pages 563–573 . IEEE Press, 2015 .\n701\nXiaozhu\nMeng,\nBarton P Miller, William R Williams,\nand Andrew R Bernat.\nMining software repositories for accurate\nauthorship.\nIn Software Maintenance (ICSM), 2013 29th IEEE International Conference on, pages 250–259 . IEEE, 2013 .\n702\nSebastian Baltes and Stephan Diehl.\nTowards a theory of software development expertise.\nCoRR, abs/1807.06087, 2018 .\n703\nJinglei Ren, Hezheng Yin, Qingda Hu, Armando Fox, and Wojciech Koszek.\nTowards quantifying the development value of\ncode contributions.\nIn Proceedings\nof the\n2018 26th ACM Joint Meeting on European Software Engineering\nConference\nand Symposium on the Foundations of Software Engineering, pages 775–779. ACM, 2018 .\n704\nRahul Venkataramani, Atul Gupta, Allahbaksh Asadullah, Basavaraju Muddu, and Vasudev Bhat.\nDiscovery of technical\nexpertise from open source code repositories.\nIn Proceedings of the 22nd International Conference on\nWorld\nWide\nWeb ,\nSci China Inf Sci\n97\npages 97–98 . ACM, 2013 .\n705\nRohit Saxena and Niranjan Pedanekar.\nI know what you coded last summer:\nMining candidate expertise from github repos-\nitories.\nIn Companion of the 2017 ACM Conference on Computer Supported Cooperative\nWork\nand Social\nComputing ,\npages 299–302 . ACM, 2017 .\n706\nSiyuan Liu, Shuhui Wang, Feida Zhu, Jinbo Zhang, and Ramayya Krishnan.\nHydra:\nLarge-scale social identity linkage via\nheterogeneous behavior modeling.\nIn Proceedings of the 2014 ACM SIGMOD international conference on Management\nof data, pages 51–62 . ACM, 2014 .\n707\nErik Kouters, Bogdan Vasilescu, Alexander Serebrenik, and Mark GJ van den Brand.\nWho’s who in gnome:\nUsing lsa to\nmerge software repository identities.\nIn\nSoftware\nMaintenance\n(ICSM),\n2012\n28th IEEE International\nConference\non ,\npages 592–595 . IEEE, 2012 .\n708\nWenkai\nMo,\nBeijun\nShen,\nYuting\nChen,\nand\nJiangang\nZhu.\nTbil:\nA tagging-based\napproach to identity linkage across\nsoftware communities.\nIn Software Engineering\nConference\n(APSEC), 2015 Asia-Pacific, pages 56–63. IEEE, 2015 .\n709\nRoy Ka-Wei Lee and David Lo.\nGithub and stack overflow:\nAnalyzing developer interests across multiple social collaborative\nplatforms.\nIn Social Informatics -\n9th International\nConference,\nSocInfo\n2017,\nOxford,\nUK,\nSeptember\n13-15,\n2017,\nProceedings, Part II, pages 245–256, 2017 .\n710\nWeizhi Huang, Wenkai Mo, Beijun Shen, Yu Yang, and Ning Li. Cpdscorer:\nModeling and evaluating developer programming\nability across software communities.\nIn SEKE, pages 87–92, 2016 .\n711\nJiafei Yan, Hailong Sun, Xu Wang, Xudong Liu, and Xiaotao Song.\nProfiling developer expertise across software communities\nwith heterogeneous information network analysis.\nIn Proceedings of the\nTenth Asia-Pacific Symposium on Internetware,\nInternetware 2018, Beijing, China, September 16-16, 2018, pages 2:1–2:9 . ACM, 2018.\n712\nJo˜ao Eduardo Montandon, Marco Tulio Valente, and Luciana L Silva.\nMining the technical roles of github users.\nInformation\nand Software\nTechnology, 131:106485, 2021 .\n713\nXiaotao\nSong,\nJiafei Yan,\nYuexin Huang, Hailong Sun, and Hongyu Zhang.\nA collaboration-aware\napproach to profiling\ndeveloper expertise with cross-community data.\n2022 IEEE 22nd International Conference on Software Quality, Reliability\nand Security (QRS), pages 344–355, 2022 .\n714\nTapajit Dey, Andrey Karnauch, and Audris Mockus.\nRepresentation of developer expertise in open source software.\n2021\nIEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 995–1007, 2020 .\n715\nYuxing\nMa,\nChris\nBogart,\nSadika\nAmreen,\nRussell L.\nZaretzki, and Audris Mockus.\nWorld\nof code:\nAn infrastructure\nfor mining the universe of open source vcs data.\n2019\nIEEE/ACM\n16th\nInternational\nConference\non\nMining\nSoftware\nRepositories (MSR), pages 143–154, 2019 .\n716\nArghavan Moradi Dakhel, Michel C. Desmarais, and Foutse Khomh.\nDev2vec:\nRepresenting domain expertise of developers\nin an embedding space. Inf. Softw.\nTechnol. , 159:107218, 2022 .\n717\nFarooq Javeed, Ansar Siddique, Akhtar Munir, Basit Shehzad, and Muhammad Ikram Lali. Discovering software developer’s\ncoding expertise through deep learning.\nIET Softw. , 14:213–220, 2020 .\n718\nZizhe Wang, Hailong Sun, Yang Fu, and Luting Ye.\nRecommending crowdsourced software developers in consideration of\nskill improvement.\nIn 2017 32nd IEEE/ACM International Conference on Automated Software Engineering\n(ASE), pages\n717–722, 2017 .\n719\nZhenyu Zhang, Hailong Sun, and Hongyu Zhang.\nDeveloper recommendation for topcoder through a meta-learning based\npolicy model.\nEmpirical Software\nEngineering, 25:859 – 889, 2019 .\n720\nXu Yu, Yadong He, Yu Fu, Yu Xin, Junwei Du, and Weijian Ni.\nCross-domain developer recommendation algorithm based on\nfeature matching.\nIn\nCCF\nConference on\nComputer Supported Cooperative\nWork and Social\nComputing, pages 443–457 .\nSpringer, 2019 .\n721\nJunjie Wang, Ye Yang, Song Wang, Chunyang Chen, Dandan Wang, and Qing Wang.\nContext-aware personalized crowdtest-\ning task recommendation.\nIEEE\nTransactions\non Software Engineering, 48:3131–3144, 2021 .\n722\nJunjie Wang, Ye Yang, Song Wang, Jun Hu, and Qing Wang.\nContext- and fairness-aware in-process crowdworker recom-\nmendation.\nACM\nTransactions on Software Engineering and Methodology\n(TOSEM), 31:1 – 31, 2022 .\n723\nHaochao Ying, Liang Chen, Tingting Liang, and Jian Wu.\nEarec:\nleveraging expertise and authority for pull-request reviewer\nrecommendation in github.\nIn Proceedings of the 3rd International\nWorkshop on CrowdSourcing in Software Engineering ,\npages 29–35 . ACM, 2016.\n724\nJing Jiang, Yun Yang, Jiahuan He, Xavier Blanc, and Li Zhang.\nWho\nshould\ncomment on this pull request?\nanalyzing\nattributes for more accurate commenter recommendation in pull-based development.\nInformation and Software\nTechnology ,\n2017.\n725\nJiyang\nZhang,\nChandra Shekhar Maddila, Ramakrishna Bairi,\nChristian Bird, Ujjwal Raizada, Apoorva Agrawal, Yamini\nJhawar, Kim Herzig, and Arie van Deursen.\nUsing large-scale heterogeneous graph representation learning for code review\nrecommendations at microsoft.\n2023\nIEEE/ACM\n45th\nInternational\nConference\non\nSoftware\nEngineering:\nSoftware\nEngineering in Practice (ICSE-SEIP), pages 162–172, 2022 .\n726\nSoumaya Rebai, Abderrahmen Amich,\nSomayeh Molaei, Marouane Kessentini, and Rick Kazman.\nMulti-objective\ncode\nreviewer recommendations:\nbalancing expertise, availability and collaborations.\nAutomated Software Engineering, 27:301\n– 328, 2020 .\n727\nMotahareh Bahrami Zanjani, Huzefa H. Kagdi, and Christian Bird.\nAutomatically recommending peer reviewers in modern\ncode review.\nIEEE\nTransactions\non Software Engineering, 42:530–543, 2016 .\n728\nChristoph Hannebauer, Michael Patalas, Sebastian St¨unkel, and Volker Gruhn.\nAutomatically recommending code reviewers\nbased on their expertise:\nAn empirical comparison.\n2016\n31st\nIEEE/ACM\nInternational\nConference\non\nAutomated\nSoftware Engineering\n(ASE), pages 99–110, 2016 .\n729\nGuoping Rong, Yifan Zhang, Lanxin Yang, Fuli Zhang, Hongyu Kuang, and He Zhang.\nModeling review history for reviewer\nrecommendation:\nA hypergraph approach.\n2022\nIEEE/ACM 44th\nInternational\nConference\non\nSoftware\nEngineering\n(ICSE), pages 1381–1392, 2022 .\n730\nVladimir Kovalenko, Nava Tintarev, Evgeny Pasynkov, Christian Bird, and Alberto Bacchelli.\nDoes reviewer recommenda-\ntion help developers.\nIEEE\nTransactions\non Software Engineering, 46:710–731, 2020 .\n731\nMd Ahasanuzzaman, Gustavo Ansaldi Oliva, and Ahmed E. Hassan.\nUsing knowledge units of programming languages to\nrecommend reviewers for pull requests: An empirical study.\nArXiv, abs/2305.05654, 2023 .\n732\nPavl´ına Wurzel Gon¸calves, G¨ul C¸alikli, Alexander Serebrenik, and Alberto Bacchelli.\nCompetencies for code review.\nPro-\nceedings of the ACM on Human-Computer Interaction, 7:1 – 33, 2023 .\n733\nYuexin\nHuang\nand\nHailong\nSun.\nBest\nanswerers\nprediction\nwith topic\nbased gat\nin q&a sites.\nIn\n12th\nAsia-Pacific\nSci China Inf Sci\n98\nSymposium on Internetware, pages 156–164, 2020 .\n734\nYiqiao\nJin,\nYunsheng\nBai,\nYanqiao\nZhu,\nYizhou Sun, and Wei Wang.\nCode recommendation for open source\nsoftware\ndevelopers. Proceedings of the\nACM Web Conference 2023, 2022 .\n735\nWenxin Xiao, Hao He, Weiwei Xu, Xin Tan,\nJinhao Dong, and Minghui Zhou.\nRecommending good first issues in github\noss projects.\n2022 IEEE/ACM 44th International\nConference\non Software Engineering\n(ICSE), pages 1830–1842, 2022 .\n736\nF´abio Santos.\nSupporting the task-driven skill identification in open source project issue tracking systems.\nACM SIGSOFT\nSoftware Engineering Notes, 48:54 – 58, 2022 .\n737\nCatarina Costa, Jair Figueirˆedo, Jo˜ao Felipe Pimentel, Anita Sarma, and Leonardo Murta.\nRecommending participants for\ncollaborative merge sessions.\nIEEE\nTransactions\non Software Engineering, 47(6):1198–1210, 2021 .\n738\nKattiana Constantino and Eduardo Figueiredo.\nCoopfinder:\nFinding collaborators based on co–changed files.\nIn 2022 IEEE\nSymposium on\nVisual Languages and Human-Centric\nComputing (VL/HCC), pages 1–3, 2022 .\n739\nKattiana Fernandes Constantino, Fabiano Muniz Bel´em, and Eduardo Figueiredo.\nDual analysis for helping developers to\nfind collaborators based on co-changed files: An empirical study. Software: Practice and Experience, 53:1438 – 1464, 2023 .\n740\nDidi Surian, Nian Liu, David Lo, Hanghang Tong, Ee-Peng Lim, and Christos Faloutsos.\nRecommending people in developers’\ncollaboration network.\nIn 2011\n18th\nWorking\nConference\non Reverse Engineering, pages 379–388, 2011 .\n741\nGerardo Canfora, Massimiliano Di Penta, Rocco Oliveto, and Sebastiano Panichella.\nWho is going to mentor newcomers\nin open source projects?\nIn\nProceedings\nof the\nACM\nSIGSOFT 20th\nInternational Symposium\non the Foundations\nof\nSoftware Engineering, FSE ’12, New York, NY, USA, 2012 . Association for Computing Machinery.\n742\nLuting Ye, Hailong Sun, Xu Wang, and Jiaruijue Wang.\nPersonalized teammate recommendation for crowdsourced software\ndevelopers.\nIn 2018\n33rd IEEE/ACM International\nConference on Automated Software Engineering\n(ASE), pages 808–\n813, 2018 .\n743\nYuxing\nMa,\nChris\nBogart,\nSadika Amreen,\nRussell Zaretzki, and Audris Mockus.\nWorld of code:\nAn infrastructure for\nmining the universe of open source vcs data.\nIn\n2019\nIEEE/ACM\n16th\nInternational\nConference\non\nMining\nSoftware\nRepositories (MSR), pages 143–154, 2019 .\n744\nTanner Fry, Tapajit Dey, Andrey Karnauch, and Audris Mockus.\nA dataset and an approach for identity resolution of 38\nmillion author ids extracted from 2b git commits.\nCoRR, abs/2003.08349, 2020 .\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2024-10-17",
  "updated": "2024-10-17"
}