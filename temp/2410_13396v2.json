{
  "id": "http://arxiv.org/abs/2410.13396v2",
  "title": "Linguistically Grounded Analysis of Language Models using Shapley Head Values",
  "authors": [
    "Marcell Fekete",
    "Johannes Bjerva"
  ],
  "abstract": "Understanding how linguistic knowledge is encoded in language models is\ncrucial for improving their generalisation capabilities. In this paper, we\ninvestigate the processing of morphosyntactic phenomena, by leveraging a\nrecently proposed method for probing language models via Shapley Head Values\n(SHVs). Using the English language BLiMP dataset, we test our approach on two\nwidely used models, BERT and RoBERTa, and compare how linguistic constructions\nsuch as anaphor agreement and filler-gap dependencies are handled. Through\nquantitative pruning and qualitative clustering analysis, we demonstrate that\nattention heads responsible for processing related linguistic phenomena cluster\ntogether. Our results show that SHV-based attributions reveal distinct patterns\nacross both models, providing insights into how language models organize and\nprocess linguistic information. These findings support the hypothesis that\nlanguage models learn subnetworks corresponding to linguistic theory, with\npotential implications for cross-linguistic model analysis and interpretability\nin Natural Language Processing (NLP).",
  "text": "Linguistically Grounded Analysis of Language Models\nusing Shapley Head Values\nMarcell Fekete\nDepartment of Computer Science\nAalborg University\nCopenhagen, Denmark\nmrfe@cs.aau.dk\nJohannes Bjerva\nDepartment of Computer Science\nAalborg University\nCopenhagen, Denmark\njbjerva@cs.aau.dk\nAbstract\nUnderstanding how linguistic knowledge is en-\ncoded in language models is crucial for im-\nproving their generalisation capabilities. In this\npaper, we investigate the processing of mor-\nphosyntactic phenomena, by leveraging a re-\ncently proposed method for probing language\nmodels via Shapley Head Values (SHVs). Us-\ning the English language BLiMP dataset, we\ntest our approach on two widely used models,\nBERT and RoBERTa, and compare how lin-\nguistic constructions such as anaphor agree-\nment and filler-gap dependencies are handled.\nThrough quantitative pruning and qualitative\nclustering analysis, we demonstrate that atten-\ntion heads responsible for processing related\nlinguistic phenomena cluster together. Our re-\nsults show that SHV-based attributions reveal\ndistinct patterns across both models, providing\ninsights into how language models organize\nand process linguistic information. These find-\nings support the hypothesis that language mod-\nels learn subnetworks corresponding to linguis-\ntic theory, with potential implications for cross-\nlinguistic model analysis and interpretability in\nnatural language processing (NLP).\n1\nIntroduction\nLanguage models gain knowledge of grammati-\ncal phenomena during pretraining. However, ex-\nactly how this knowledge is encoded is not well\nunderstood. While there is prior research on prob-\ning language models for morphosyntactic construc-\ntions (Finlayson et al., 2021; Mueller et al., 2022;\nStanczak et al., 2022; Ács et al., 2023), it is not\nwell established if this information is crucial to\nthe model itself, or if it is merely learned as a by-\nproduct. We perform extensive analysis and offer\nevidence for a hypothesis that language models\nlearn separate subnetworks which we can ground\nin linguistic theory, and are crucial to the model\nprocessing. To our knowledge this is the first paper\nthat uses quantitative as well as qualitative analysis\nLanguage model\nparameters\nLoRA\nClassifier\nInterpretation\nAttribution\nFine-tuning\nShapley Head Values\nPruning\nClustering\nLinguistic analysis\nSuperlative quant. 1\nSuperlative quant. 2\nQuantifiers\nFigure 1: In the fine-tuning step, we train a classifier on\na grammaticality judgement task. We carry out Shapley\nHead Value (SHV) attributions, and in the interpretation\nstep, we carry out quantitative analysis using pruning as\nwell as qualitative experiments using linguistic ground-\ning.\ngrounded in linguistic theory in assessing which\nlanguage model components are responsible for\ntaking care of the processing of specific linguistic\nphenomena.\nWe calculate Shapley Head Values (SHVs) fol-\nlowing the methodology of Held and Yang (2023).\nWhile they use SHVs to find attention heads that\nhave a high contribution to certain NLP tasks such\nas natural language inference to improve perfor-\nmance, we calculate SHVs using a grammaticality\nclassification task across 13 different phenomena\nand 67 different constructions from the BLiMP\nbenchmark (Warstadt et al., 2020, see Table 1).\nHence, we offer a novel, linguistically grounded ap-\nproach for isolating components of language mod-\nels responsible for processing specific morphosyn-\ntactic phenomena. We cluster constructions based\non SHVs, and assess the success of isolating heads\nresponsible for processing aspects of morphosyn-\ntax using pruning based on relative importance of\nattention heads and linguistic analysis (see Figure\n1).\nIn this paper, we perform an in-depth analysis us-\ning the BLiMP dataset to explore how widely used\nlanguage models, such as BERT and RoBERTa,\narXiv:2410.13396v2  [cs.CL]  25 Feb 2025\nhandle diverse morphosyntactic phenomena, in-\ncluding anaphor agreement, filler-gap dependen-\ncies, and island effects. Through a combination of\nquantitative pruning and qualitative linguistic anal-\nysis, we demonstrate that certain attention heads\nwithin these models are systematically responsi-\nble for processing distinct linguistic phenomena.\nCrucially, we show that related phenomena often\ncluster together within the models, suggesting that\nlanguage models develop internal subnetworks cor-\nresponding to theoretical linguistic categories.\nWe make the following contributions:\n1. We use Shapley Head Values (SHVs) to probe\nattention heads for their role in processing\nmorphosyntactic phenomena, across two lan-\nguage models.\n2. We apply SHV-based clustering, showing that\nlanguage models develop subnetworks corre-\nsponding to related linguistic categories.\n3. We provide qualitative linguistic analyses ex-\nplaining the clustering of morphosyntactic\nphenomena based on SHV attributions.\n4. Quantitative pruning validates our clusters by\nshowing localized performance impacts when\nrelevant attention heads are removed.\n5. We release an implementation of our SHV-\nbased probing and pruning methods.\n2\nBackground\n2.1\nProbing\nPrevious work in attribution focuses on identify-\ning the role that either training features or model\ncomponents play with respect to various phenom-\nena. Training data attribution is used to investi-\ngate, e.g., to what extent language-specific sub-\nnetworks rely on in-language examples from the\ntraining data when making predictions (Choenni\net al., 2023). Foroutan et al. (2022) uses itera-\ntive magnitude pruning (Frankle and Carbin, 2019)\nto isolate sub-networks specific to languages and\ntasks like masked language modelling, named en-\ntity recognition, and natural language inference.\nStructured pruning is used for identifying attention\nheads important for dialogue summarisation (Liu\nand Chen, 2023) and cross-lingual natural language\ninference, e.g., with Shapley Head Values, (Held\nand Yang, 2023). Extrinsic probing efforts such\nas from Ács et al. (2023) target whether morpho-\nlogical information is encoded by language mod-\nels. On the other hand, work in intrinsic probing\naims to reveal how exactly linguistic information is\nstructured within a model (Dalvi et al., 2019; Tor-\nroba Hennigen et al., 2020; Stanczak et al., 2022).\nAnother popular technique is causal mediation\nanalysis (CMA), a causal method of identifying\nthe importance of a model component to a target\nphenomenon (Pearl, 2001). Under CMA, two ef-\nfects on the prediction of a model are compared: a\ndirect effect of input intervention – such as a text\nedit – and an indirect effect. The relative degree\nof the indirect effect compared to the direct effect\nreveals the significance of the target component to\nthe target phenomenon. CMA is used to investi-\ngate the role of individual neurons and attention\nheads in mediating gender bias (Vig et al., 2020),\nand to isolate neurons responsible for subject-verb\nagreement in English (Finlayson et al., 2021), and\nacross a number of other languages (Mueller et al.,\n2022). A limitation of CMA, however, is that text\nedit operations are necessarily word-level to isolate\nother factors such as sensitivity to specific syntactic\nfeatures. This makes it challenging to accommo-\ndate a number of morphosyntactic constructions\nsuch as island effects (see Table 1). Instead, we use\nSHVs which are particularly well-suited to probe\nfor subnetworks responsible for processing of mor-\nphosyntactic phenomena.\nOurs is certainly not the first paper to carry out\nthe analysis of language model skills using linguis-\ntic grounding, but we believe our work is differenti-\nated by the wider coverage in terms of morphosyn-\ntactic phenomena, as well as our efforts to localise\nlinguistic knowledge. Previous work explores the\nrelation between self-attention of input tokens and\ndependency links (Htut et al., 2019; Clark et al.,\n2019). Linzen and Baroni (2021) analyse the capa-\nbilities of LSTM and GRU models on long-distance\ndependencies, while Wilcox et al. (2024) use GPT-\n2 and GPT-3 to look at long-distance dependencies\nand island constraints.\n2.2\nShapley Head Values\nShapley Values originate from game theory, de-\nvised to fairly distribute a given reward among a\nset of players based on their relative contribution\nto a certain outcome (Shapley, 1953; Mosca et al.,\n2022). They are used in model interpretability re-\nsearch thanks to their properties as attribution meth-\nPhenomenon\nN\nAcceptable Example\nUnacceptable Example\nANAPHOR AGR.\n2\nMany girls insulted themselves.\n*Many girls insulted herself.\nARG. STRUCTURE\n7\nRose wasn’t disturbing Mark.\n*Rose wasn’t boasting Mark.\nBINDING THEORY\n7\nCarlos said that Lori helped him.\n*Carlos said that Lori helped himself.\nCONTROL/RAISING\n5\nThere was bound to be a fish escaping.\n*There was unable to be a fish escaping.\nDET.-NOUN AGR.\n8\nRachelle had bought that chair.\n*Rachelle had bought that chairs.\nELLIPSIS\n2\nAnne’s doctor cleans one important\nbook and Stacey cleans a few.\n*Anne’s doctor cleans one book\nand Stacey cleans a few important.\nFILLER-GAP DEP.\n7\nBrett knew what many waiters find.\n*Brett knew that many waiters find.\nIRREG. FORMS\n2\nAaron broke the unicycle.\n*Aaron broken the unicycle.\nISLAND EFFECTS\n8\nWhose hat should Tonya wear?\n*Whose should Tonya wear hat?\nNPI LICENSING\n7\nThe truck has clearly tipped over.\n*The truck has ever tipped over.\nQUANTIFIERS\n4\nNo boy knew fewer than six guys.\n*No boy knew at most six guys.\nS-SELECTION\n2\nCarrie isn’t listening to Jodi.\n*That movie theater isn’t listening to Jodi.\nSUBJ.-VERB AGR.\n6\nThese casseroles disgust Kayla.\n*These casseroles disgusts Kayla.\nTable 1: Minimal pairs from the thirteen linguistic phenomena BLiMP paradigms are categorised into with the\nnumber of paradigms in each category (N). Differences are in bold text and, following convention, ungrammatical\nsentences are marked with an asterisk. Table adapted from Warstadt et al. (2020).\nods and satisfying theoretical properties of local\naccuracy, missingness and consistency (Shapley,\n1953; Ghorbani and Zou, 2020; Held and Yang,\n2023). Shapley Values have the advantage over\nalternative, gradient-based attribution methods in\nthat they do not need evaluation functions to be\ndifferentiable, allowing them to be applied directly.\nThey are also meaningfully signed with positive\nvalues reflecting positive contribution of the com-\nponent and negative values reflecting the opposite\n(Held and Yang, 2023).. We use Shapley Head\nValues (SHVs) to measure the mean marginal con-\ntributions of attention heads in a language model in\na linguistically grounded scenario, approximating\nthese values following Held and Yang (2023). Con-\ncretely, we cluster morphosyntactic phenomena\nbased on the similarity of their associated SHVs,\nthen analyse the resulting cluster with regards to\ntheir correspondence to linguistic theory.\n2.3\nPruning\nAccording to the Lottery Ticket Hypothesis (Fran-\nkle and Carbin, 2019), neural models contain both\nharmful and beneficial connections between model\ncomponents with respect to a target scenario. This\nmeans that pruning – the removing or turning off\nindividual neurons or attention heads – can help\nus isolate subnetworks (‘winning tickets’) within\nlanguage models (Pfeiffer et al., 2024). The com-\nmon technique of pruning is to stop signals from\npassing through specific model components using\na binary mask. Where the goal is improving model\nperformance, the mask will impact components\nthat contribute negatively to the target task, lan-\nguage, or other use cases. Pruning may also be\nutilised to discern how localised processing of a\nmorphosyntactic phenomenon is in the model, and\nhow generalisable this ability is to other phenom-\nena. In our work, pruning is thus a quantitative\nmetric to evaluate the cohesion of SHV clusters\nand the success of isolating components of a sub-\nnetwork encoding the same aspects of linguistic\nknowledge.\n3\nMethodology\n3.1\nDeriving SHVs\nFollowing Held and Yang (2023), we define SHV\nφh, for a single attention head Atth ∈A1, to rep-\nresent the mean performance improvement on the\ncharacteristic function V – as derived from perfor-\nmance on the evaluation metric described in Sec-\ntion 3.2.2 – if Atth contributes to the inference. To\nbe able to remove or add attention heads at will, we\naugment our target models with a gate Gh = {0, 1}\nfor each attention head. When Gh = 0, the head\nAtth is removed from the inference and does not\ncontribute to the output of the transformer. The\nderivation of φh requires the contribution of the\nhead Atth to be measured across all Q permuta-\ntions of the other gates, see Equation 1.\nφh =\n1\n|Q|\nX\nA∈Q\nV (A ∪h) −V (A)\n(1)\n1Representing the set of all attention heads.\nCalculating Equation 1 for all of N attention\nheads requires 2N evaluations, which is intractable\nwith the number of heads language models contain.\nWe can facilitate the computation through a num-\nber of steps (Ghorbani and Zou, 2020; Held and\nYang, 2023). First, we can replace the full permu-\ntation set Q in Equation 1 by a randomly sampled\nsubset of permutations via Monte Carlo simula-\ntions (Castro et al., 2009). Additionally, since SHV\nestimates are low-variance and computationally ex-\npensive to derive, we can speed up convergence\nby applying stopping criteria. One of these criteria\nis a truncation heuristic that determines stopping\nthe sampling of the marginal contributions of a\nhead once < 50% of attention heads remain in the\npermutation (Held and Yang, 2023).\nThe other stopping criterion is rooted in multi-\narmed bandit sampling. We want to stop sampling\nthe marginal contributions of a head Atth when\nwe reach a decrease in the variance range in the\napproximated ˆφh of Atth. A low variance range in-\ndicates that we can be fairly confident in the degree\nof impact Atth has for the characteristic function\nV . Our confidence interval is derived by Empiri-\ncal Bernstein Bounds variance estimation (Maurer\nand Pontil, 2009). Given t samples with observed\nvariance σt and a maximum variance range of R,\nthe difference between the observed mean ˆµ and\nthe true mean µ falls in range given by Equation 2\n(Mnih et al., 2008) with a probability of 1 −δ.\n|ˆµ −µ| ≤σt\nr\n2 log(3/δ)\nt\n+ 3R log(3/δ)\nt\n(2)\nSampling stops when the bound is less than\n|µ −0| as this identifies the SHV as positive or\nnegative with probability 1 −δ, which is an ef-\nficient way to determine which heads contribute\npositively to the target task. Following Held and\nYang (2023), we set R = 1 and δ = 0.1.\n3.2\nExperimental Setup\nIn the following, we introduce our experimental\nsetup, including the dataset we use, the target task\nto derive SHVs, and our fine-tuning setup.\n3.2.1\nBLiMP Dataset\nThe Benchmark of Linguistic Minimal Pairs\n(BLiMP) is a challenge set for English that aims to\nmeasure the amount of linguistic knowledge lan-\nguage models have on a range of morphosyntactic\nphenomena (Warstadt et al., 2020). Constructions\nare organised into 67 minimal pair paradigms each\ncontaining 1,000 sentence pairs, and the individual\nparadigms are organised into 13 larger categories\n(see Table 1).2 While similar datasets exist for Chi-\nnese with 38 (Song et al., 2022), for Japanese with\n39 (Someya and Oseki, 2023), and Russian with\n45 individual paradigms (Taktasheva et al., 2024),\nBLiMP is the largest in its category.\n3.2.2\nGrammaticality Judgement Task\nWe define the evaluation metric mentioned in Sec-\ntion 3.1 for the SHV derivation as accuracy on a\ncustom grammaticality judgement task: a classi-\nfier has to output a binary label signifying which\nelement of a sentence pair (s1, s2) is grammati-\ncal. The same metric is used to derive pruning\nperformance. Both sentences are drawn from the\nsame BLiMP paradigm p, member of the set of all\nparadigms P, p shuffled prior to the selection of\n(s1, s2). Shuffling is done to facilitate the classi-\nfier focusing on the underlying morphosyntactic\nphenomenon represented – or violated – in the sen-\ntences, as opposed to merely the surface features\nof (s1, s2).\nThe performance of the classifier is measured\nin terms of accuracy, and it assigns label 0 or 1,\ndepending on if s1 or s2 is grammatical:\nBinaryClass(s1, s2) =\n(\n0\nif s1 is grammatical,\n1\nif s2 is grammatical.\nThe advantage of this novel task formulation is\nthat – unlike other approaches such as simple text\nedits – it can flexibly incorporate morphosyntactic\nphenomena where the grammatical and ungram-\nmatical sentences differ in more than a single word\n(see e.g., ELLIPSIS or ISLAND EFFECTS in Table\n1).\n3.2.3\nFine-Tuning Setup\nWe carry out our experiments and our analysis us-\ning the monolingual English models BERT (De-\nvlin et al., 2019) and RoBERTa (Liu et al., 2019).\nBERT has 110 million parameters and was trained\non 16GB of data, while RoBERTa has 125 million\nparameters and was trained on 160GB. We follow\nHeld and Yang (2023) in deriving SHVs (see Sec-\ntion 3.1). However, rather than fully fine-tuning\nour target model weights for the target task, we use\n2For a full list of paradigms, see Appendix A.\n0\n10\n20\n30\n40\n50\n60\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nNumber of clusters\nInertia\nFigure 2: Clustering is done to try to optimise inertia\nand cluster count, resulting in an attempt at 10 clusters.\nlow-rank adapters (LoRA; Hu et al. 2021) imple-\nmented with the PEFT library.3 The main advan-\ntages of LoRA modules are their speed of training,\nand, more importantly, that they merely emphasise\ninformation that is already present in the original\nweights as opposed to reshaping the original model\nweights (Hu et al., 2021). This has the added ad-\nvantage that it enables us to isolate and evaluate the\npretrained linguistic knowledge of the models we\nconsider.\nWe merge a subset of the sentence pairs from\nthe individual BLiMP paradigms p ∈P to create\nthe training set for training LoRA and the classifier,\nand merge a smaller subset of pairs as a develop-\nment set. For each p, 800 pairs go to the training\nset, and 100 pairs are assigned to the development\nset. After splitting the sentence pairs – but before\nmerging into the training and development sets –\nwe permute the set of ungrammatical sentences\nto avoid exact minimal pairs in order to better fo-\ncus on the underlying grammatical construction\n(see Section 3.2.2). Finally, we shuffle the order\nof grammatical and ungrammatical sentences to\ncreate appropriate binary training examples, and\nmerge the selections into the training and develop-\nment sets. We retain the remaining 100 sentence\npairs per paradigm to derive paradigm-specific at-\ntributions.\n3.3\nInterpretation\nIn this section, we describe the way we cluster\nBLiMP paradigms using SHVs, followed by how\nwe interpret these clusters both with qualitative and\nquantitative means to assess how successfully we\n3See https://huggingface.co/docs/peft/index.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of top heads to prune\n0.18\n0.16\n0.14\n0.12\n0.10\n0.08\n0.06\n0.04\nMean  accuracy\nFigure 3: Mean ∆accuracy values drop in a near-linear\nfashion when pruning up to the top n heads across\nparadigms\ncan identify subnetworks encoding specific linguis-\ntic knowledge across BERT and RoBERTa.\nSHVs represent the mean marginal contribu-\ntions of each attention head to the performance\non the grammaticality judgement task defined in\nSection 3.2.2. For each BLiMP paradigm p ∈P,\nthese SHV attributions are represented as vectors\nvp ∈Rd where d is 144, i.e., the count of all atten-\ntion heads in BERT and RoBERTa. We scale these\nvectors and cluster them using k-means clustering,\ngrouping paradigms together based on how simi-\nlarly the individual attention heads contribute to\nprocessing the target constructions. We decide an\noptimal number of clusters k by calculating inertia\n(see Figure 2). Based on empirical results, we pick\nk = 10, a small enough number to also facilitate\nin-depth analysis.\nThe goal of our qualitative, linguistic analysis is\nto understand the potential links between diverse\nparadigms that are clustered together, thus vali-\ndating how successfully we identified the subnet-\nworks responsible for these constructions. BLiMP\nparadigms are assigned into one of twelve cate-\ngories reflecting morphosyntactic phenomena (see\nTable 1). On the highest level, our qualitative analy-\nsis is guided by the category membership of the in-\ndividual paradigms – clusters containing paradigms\nfrom the same BLiMP category are likely to be\ncohesive in terms of morphosyntactic phenomena\nthey represent. Clusters may be cohesive, however,\neven where category membership is not homoge-\nneous. Categories may represent aspects of some of\nthe same morphosyntactic phenomena – like how\nboth BINDING THEORY and ANAPHOR AGR. are\nBERT\nRoBERTa\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\nFigure 4: Distribution of baseline accuracy levels with-\nout pruning across the BERT and RoBERTa models.\nconcerned with the distribution of anaphors with\nrespect to their antecedents. Category-level analy-\nsis can be complemented well by a sentence-level\none.\nOur quantitative analysis relies on pruning. We\ngenerate binary masks for each p paradigm, mask-\ning the top n attention heads in terms of their SHV,\nthen apply this mask across all other paradigms\nP. We do this to evaluate how cohesive the result-\ning clusters are, i.e., to what degree they represent\nthat the language model has to apply some of the\nsame set of morphosyntactic knowledge to pro-\ncess paradigms of the cluster. It stands to reason\nthat if the cluster is well-defined, pruning using\nmasks within cluster should have a larger impact\nthan when utilising these masks for paradigms that\nare out-of-cluster. This is because out-of-cluster\nparadigms likely do not require the same set of mor-\nphosyntactic knowledge that is encoded in the rele-\nvant attention heads. As Figure 3 shows, pruning\nthe top 10 heads already results in a large impact\non accuracy across various paradigms. Since we\ntarget only 7% of all heads, we can observe how\nmuch of the processing of the various phenomena\nis localised as opposed to distributed more widely\nin the language model. This way we can verify how\nsuccessfully we isolated the relevant components\nof a given subnetwork.\n4\nResults\nWe cluster the BLiMP paradigms into 10 individual\nclusters using the k-means algorithm based on the\nSHV vectors across BERT and RoBERTa. While\n4There are two different Binding clusters in BERT, the\nparadigms in the smaller cluster are underlined.\nBLiMP Paradigm\nLinguistics Term\nM\nNPI Cluster\nNPI present (1)\nNPI LICENSING\n♢\nNPI present (2)\n♦\nNPI scope (‘only’)\n♦\nNPI scope (sentential negation)\n♢\nNPI licensor present (‘only’)\n♢\n\"\n(matrix question)\n♢\n\"\n(sentential negation)\n♢\nIrregular past participle verbs\nIRREGULAR FORMS\n♦\nIsland Effects\nAdjunct island\nISLAND EFFECTS\n♢\nComplex NP island\n♢\nCoordinate structure constraint\n(complex left branch)\n♢\n\"\n(object extraction)\n♦\nLeft branch island (simple question)\n♢\nLeft branch island (echo question)\n♢\nWh-island\n♦\nQuan-\ntifiers\nSuperlative quantifiers 1\nQUANTIFIERS\n♢\nSuperlative quantifiers 2\n♢\nBinding*\nAnaphor gender agreement\nANAPHOR AGR.\n♦\nAnimate subject trans.\nS-SELECTION\n♦\nPrinciple A (case 1)\nBINDING\n♦\n\"\n(domain 1)\n♢\n\"\n(domain 2)\n♢\n\"\n(domain 3)\n♢\n\"\n(c-command)\n♢\nFiller-Gap\nEllipsis N-bar (2)\nELLIPSIS\n♦\nExistential ‘there’\n(subject raising)\nCONTROL/RAISING\n♦\nTough vs raising (2)\n♦\nPrinciple A (case 1)\nBINDING\n♦\n\"\n(case 2)\n♢\n\"\n(reconstruction)\n♢\nWh-questions (object gap)\nFILLER-GAP DEP.\n♢\n\"\n(subject gap)\n♢\n\"\n(subject gap, long distance)\n♢\nWh vs ‘that’ (no gap)\n♢\n\"\n(no gap, long distance)\n♢\nWh vs ‘That’\nInchoative\nARG. STRUCTURE\n♦\nIntransitive\n♦\nTough vs raising (1)\nCONTROL/RAISING\n♦\nWh vs ‘that’ (with gap)\nFILLER-GAP DEP.\n♢\n\"\n(with gap, long distance)\n♢\nTable 2: Clusters that emerge in both models with shared\nparadigms marked with ♢. A subset of paradigms only\nappear in in BERT (♦) or RoBERTa (♦). Clusters are\nnamed by the authors based on majority membership.4\nBLiMP Paradigm\nLinguistics Term\nM\nDet.-Noun Cluster\nDeterminer-noun agr. (1)\nDET.-NOUN AGR.\n♦\n\"\n(2)\n♦\n\"\n(irregular, 1)\n♦\n\"\n(irregular, 2)\n♦\n\"\n(with adjective, 1)\n♦\n\"\n(with adjective, 2)\n♦\n\"\n(with adjective,\nirregular 1)\n♦\n\"\n(with adjective,\nirregular 2)\n♦\nDistractor agreement\n(relational noun)\nSUBJECT-VERB AGR.\n♦\nRegular plural subject-verb agr. (1)\n♦\nRegular plural subject-verb agr. (2)\n♦\nTransitive\nARG. STRUCTURE\n♦\nTable 3:\nParadigms in the Det.-Noun Cluster in\nRoBERTa (♦).\nNPI\nIsland\nQuantifiers\nBinding\nFiller-gap\nWh vs `that'\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\n Accuracy\n*\n*\n*\nType\nIn-cluster\nOut-cluster\nFigure 5: Impact in terms of ∆in accuracy in-cluster\nversus out-of-cluster across six BERT clusters when\npruning the top 10 attention heads. Asterisks (*) show\nwhere the difference in distribution of the delta values\nis significant at α ≤0.001 after applying Bonferroni\ncorrection (Dunn, 1961) (see Appendix D).\nclustering is stochastic, independent runs of the\nalgorithm do not seem to differ significantly from\none another.5 As Table 2 demonstrates, six of the\nten clusters show significant correspondence across\nthe two language models. Some clusters, however,\nare less generalisable across the two models. The\nRoBERTa cluster in Table 3 contains paradigms\nrepresenting a number of diverse linguistic phenom-\nena, and the BERT cluster featuring DET.-NOUN\nAGREEMENT paradigms contains an even wider\nrange of constructions.6\nFigure 5 compares the relative impact of pruning\nin- and out-of-cluster across the BERT clusters in\nTable 2, i.e., the ones that have related RoBERTa\nclusters. This impact is measured in the change (∆)\nfrom the baseline values in terms of accuracy on\nthe grammaticality judgement task when pruning\nthe top 10 attention heads. When in-cluster pruning\nin general results in a more dramatic negative ∆\nthan pruning out-of-cluster, we can surmise that\nthe top 10 attention heads contribute significantly\nto the processing of the target paradigms, thus we\nsuccessfully identified the most important parts of\na subnetwork. In other cases, e.g., the Island or\nQuantifiers cluster, either the target phenomenon\nis not localised enough for the top 10 heads to\ncause a significant impact, or the clusters are not\nhomogeneous or exclusive enough with respect to\nthe phenomena they contain.\n5See Appendix B for analysis.\n6See all clusters in Appendix C.\n5\nDiscussion\nIn the following, we discuss our most important\nfindings regarding the clusters created via SHV\nattributions. The membership of these clusters in-\ndicates which morphosyntactic phenomena are pro-\ncessed using the same subnetworks by BERT and\nRoBERTa, thus showing how the models generalise\nover these constructions.\nConsistency across models\nCluster membership\nacross the BERT and RoBERTa models largely\ncorresponds between the majority – 6 out of 10 –\nof the clusters (see Table 2).\nSuccessful grouping of linguistic categories\nIn\nmany cases, BLiMP paradigms from the same lin-\nguistic categories appear in their own clusters. This\nis especially true for the clusters that are consistent\nacross the language models. For instance, NPI LI-\nCENSING or BINDING paradigms mostly appear\nin their NPI or Binding cluster, respectively (see\nTable 2). DET.-NOUN AGR. paradigms are in their\nown – though less homogeneous – cluster in the\nRoBERTa model (Table 3) and the BERT model as\nwell (see Table 7 in Appendix C).\nCommon ground across linguistic categories\nWhile clusters do tend to collect BLiMP paradigms\nfrom the same categories, exceptions, i.e., the pres-\nence of paradigms from other linguistic categories,\ncan often be explained by linguistic analysis. Take\nthe Filler-gap cluster (Table 2).\nParadigms in\nFILLER-GAP DEP. typically represent the fronting\nof linguistic material that can be analysed to have\noriginated in a different clause (1). Similarly, rel-\nevant CONTROL/RAISING (2) and BINDING (3)\nparadigms also deal with the licensing of raised\nmaterial. Finally, ELLIPSIS can be analysed as a\nlink between an antecedent and consequent clause\nwhich allows the omission of linguistic material (e\nin 4).7\n(1)\nWayne has revealed who/*that most hospi-\ntals admired t.\n(2)\nThere was bound/*unable to be a fish es-\ncaping.\n(3)\nIt’s himself that this cashier attacked t /*It’s\nhimself that attacked this cashier.\n7The symbol t for trace is used as a convention to indicate\nwhere the raised material originates from, while e represents\nthe omitted material.\n(4)\nThis print scares a lot of busy senators and\nBenjamin scares a few e /*This print scares\na lot of senators and Benjamin scares a few\nbusy.\nThe Binding cluster contains BLiMP paradigms\nrelated to phenomena concerning anaphors. BIND-\nING paradigms represent the various licensing re-\nstrictions of anaphors which is often represented by\nthe presence or lack of gender agreement between\nthe anaphor and the antecedent noun (5), similarly\nto the relevant ANAPHOR AGR. paradigm (6).\n(5)\nGina explains Alan fires himself/*Alan ex-\nplains Gina fires himself.\n(6)\nA girl couldn’t reveal herself/*himself.\nFinally, the DET.-NOUN AGR. and SUBJECT-\nVERB AGR. paradigms in the RoBERTa cluster in\nTable 3 are connected by the fact that both phenom-\nena concern number agreement (7a and 7b).\n(7)\na. Raymond\nis\nselling\nthis\nsketch/*sketches.\nb. The students/*student perform.\nLinguistic knowledge is often strongly localised\nThe difference between in-cluster and out-of-\ncluster pruning impact on ∆accuracy indicates\nwe identified the majority of attention heads repre-\nsenting the full construction-specific subnetwork\n(see Figure 5). In a subset of cases this is not the\ncase: either the subnetwork is larger, or the relevant\nknowledge is more widely distributed across model\ncomponents. Three out of the six clusters show a\nsignificant difference between in-cluster and out-of-\ncluster ∆accuracy. This is very unlikely to occur\nrandomly: in our experiments with 125 random\nclusters of varying sizes, we found it to happen\nonly in two cases.\nDifferent degrees of sensitivity between mod-\nels\nSHVs prove useful to cluster related BLiMP\nparadigms into clusters, but it is clear that the\nRoBERTa model is somewhat more discerning with\nregards to morphosyntactic phenomena. In the case\nof BERT, 25 of all 67 paradigms are assigned to\na single cluster representing 9 different linguistic\ncategories (see BERT 3 in Table 7, Appendix C).\nRoBERTa has no such cluster that collects this num-\nber of unrelated linguistics constructions. The dis-\ncrepancy between the two language models may\nlie in the comparably higher degree of linguistic\nknowledge obtained by RoBERTa thanks to the\nfact that it is pretrained on vastly more data than\nBERT for more training steps, and that it has more\nmodel parameters (see Section 3.2.3). This is also\nshown how its performance surpasses BERT on\nmany metrics (Liu et al., 2019).\nThese points show that language models en-\ncode at least a subset of all morphosyntactic\nknowledge in subnetworks that our methodology\ncan identify.\nThis enables us to evaluate the\ngeneralisation\nabilities\nof\nlanguage\nmodels\nbetween related linguistic constructions. We can\nadditionally find that the success of generalisation\nis impacted by the depth of pretraining, i.e., the\nsize of pretraining data and the number of training\nsteps.\n6\nConclusion\nWe apply intrinsic probing on two commonly used\nlanguage models, BERT and RoBERTa, in order\nto investigate how linguistic knowledge is repre-\nsented and organised internally. We show that attri-\nbutions based on SHVs can be used to identify at-\ntention heads of subnetworks that generalise across\nrelated morphosyntactic phenomena, and allow us\nto carry out a linguistically grounded analysis. In\nthis, we showcase that SHVs are well-suited to pro-\nvide linguistically interpretable insights into the\ninner workings of language models, beyond the\ntask-specific investigations carried out by Held and\nYang (2023). Additionally, our pruning analysis\ndemonstrates that in many cases, the identified at-\ntention heads are crucial components of the iden-\ntified subnetworks; we find that switching these\nattention heads off severely impacts the grammati-\ncality judgement of the language models. In future\nwork, our methods can prove valuable in describing\nsubnetworks specific to linguistic knowledge in lan-\nguage models. This might be particularly valuable\nin cross-lingual settings.\nLimitations\nA major limitation for our work is the difficulty\nof scaling it to new languages and to new lan-\nguage models. First, the coverage of datasets of\nlinguistic minimal pairs for languages other than\nEnglish lags behind that of BLiMP. Next, linguisti-\ncally grounded clustering analysis requires an in-\ndepth understanding of the specific morphosyntac-\ntic phenomena in a language, as well as consider-\nable manual effort. This effort is a bottleneck on\nhow readily the analysis can be carried out for other\nlanguage models and, of course, other languages.\nAdditionally, sentences in the BLiMP dataset are\ngenerated from expert templates using a predefined\nvocabulary, which means they tend to be semanti-\ncally empty as well as heavily formulaic. However,\nsince this is true for both grammatical and ungram-\nmatical sentences, this fact is not likely to cause\nproblems for our attributions. Furthermore, the\nderivation of SHVs requires the use of sampling\ntechniques and truncation heuristics to make com-\nputation tractable. These techniques introduce un-\ncertainty, which means there is a slight chance they\nmight influence the accuracy of the attribution pro-\ncess. Moreover, the binary gates used to zero atten-\ntion head activations may harm model performance\nsince these zero-values are out-of-distribution for\nthe language model. This means the actual im-\nportance of attention heads somewhat difficult to\ndiscombobulate from the impact of the zero ab-\nlations. Finally, we are carrying out attributions\non the level of the attention head, rather than on\nthe level of the neuron. This improves tractability\nand facilitates the qualitative analysis, but it may\nresult in a loss of granularity. In future work, it\nmight be worth exploring what we can gain from\nneuron-level attributions.\nEthics Statement\nThis paper deals with a linguistically grounded\nanalysis of the inner workings of language models.\nOur work adheres to the ACL ethics guidelines.\nAcknowledgements\nMF and JB are funded by the Carlsberg Foundation,\nunder the Semper Ardens: Accelerate programme\n(project nr. CF21-0454).\nReferences\nJudit Ács, Endre Hamerlik, Roy Schwartz, Noah A.\nSmith, and Andras Kornai. 2023. Morphosyntac-\ntic probing of multilingual BERT models. Natural\nLanguage Engineering, pages 1–40.\nA Asudeh and M Dalrymple. 2006. Binding Theory.\nIn Encyploedia of Language & Linguistics, second\nedition. Elsevier Ltd.\nA Carnie. 2006. Island Constraints. In Encyploedia\nof Language & Linguistics, second edition. Elsevier\nLtd.\nJavier Castro, Daniel Gómez, and Juan Tejada. 2009.\nPolynomial calculation of the Shapley value based\non sampling. Computers & Operations Research,\n36(5):1726–1730.\nRochelle Choenni, Ekaterina Shutova, and Dan Gar-\nrette. 2023.\nExamining Modularity in Multilin-\ngual LMs via Language-Specialized Subnetworks.\nArXiv:2311.08273 [cs].\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Be-\nlinkov, Anthony Bau, and James Glass. 2019. What\nis one grain of sand in the desert? analyzing indi-\nvidual neurons in deep nlp models. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 6309–6317.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nS.W. Dubinsky and W.D. Davies. 2006. Control and\nRaising. In Encyclopedia of Language & Linguistics,\nsecond edition, pages 131–139. Elsevier Ltd.\nOlive Jean Dunn. 1961. Multiple Comparisons among\nMeans. Journal of the American Statistical Associa-\ntion, 56(293):52–64.\nY N Falk. 2006. Long-Distance Dependencies. In Ency-\nclopedia of Language & Linguistics, second edition.\nElsevier Ltd.\nMatthew\nFinlayson,\nAaron\nMueller,\nSebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021.\nCausal analysis of syntactic\nagreement mechanisms in neural language models.\nIn Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1:\nLong\nPapers), pages 1828–1843, Online. Association for\nComputational Linguistics.\nNegar Foroutan, Mohammadreza Banaei, Rémi Lebret,\nAntoine Bosselut, and Karl Aberer. 2022. Discov-\nering language-neutral sub-networks in multilingual\nlanguage models. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7560–7575, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nJonathan Frankle and Michael Carbin. 2019. The Lot-\ntery Ticket Hypothesis: Finding Sparse, Trainable\nNeural Networks. ArXiv:1803.03635 [cs].\nAmirata Ghorbani and James Y Zou. 2020. Neuron\nShapley: Discovering the Responsible Neurons. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 5922–5932. Curran Associates,\nInc.\nWilliam Held and Diyi Yang. 2023. Shapley head prun-\ning: Identifying and removing interference in mul-\ntilingual transformers. In Proceedings of the 17th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 2416–\n2427, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nJ Hoeksema. 2006. Polarity Items. In Encyclopedia\nof Language & Linguistics, second edition. Elsevier\nLtd.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel\nR.\nBowman.\n2019.\nDo\nAttention\nHeads in BERT Track Syntactic Dependencies?\nArXiv:1911.12246.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\nof Large Language Models. ArXiv:2106.09685 [cs].\nTal Linzen and Marco Baroni. 2021. Syntactic Structure\nfrom Deep Learning. Annual Review of Linguistics,\n7(1):195–212.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv:1907.11692 [cs].\nZhengyuan Liu and Nancy F. Chen. 2023. Picking the\nUnderused Heads: A Network Pruning Perspective of\nAttention Head Selection for Fusing Dialogue Coref-\nerence Information. In ICASSP 2023 - 2023 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 1–5, Rhodes Is-\nland, Greece. IEEE.\nAndreas Maurer and Massimiliano Pontil. 2009. Em-\npirical Bernstein Bounds and Sample Variance Penal-\nization. ArXiv:0907.3740 [stat].\nVolodymyr Mnih, Csaba Szepesvári, and Jean-Yves Au-\ndibert. 2008. Empirical Bernstein stopping. In Pro-\nceedings of the 25th international conference on Ma-\nchine learning - ICML ’08, pages 672–679, Helsinki,\nFinland. ACM Press.\nEdoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel\nGallagher, and Georg Groh. 2022. SHAP-based ex-\nplanation methods: A review for NLP interpretabil-\nity. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 4593–\n4603, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nAaron Mueller, Yu Xia, and Tal Linzen. 2022. Causal\nanalysis of syntactic agreement neurons in multi-\nlingual language models.\nIn Proceedings of the\n26th Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 95–109, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nJudea Pearl. 2001. Direct and indirect effects. In Pro-\nceedings of the Seventeenth Conference on Uncer-\ntainty in Artificial Intelligence, UAI’01, pages 411–\n420, San Francisco, CA, USA. Morgan Kaufmann\nPublishers Inc. Event-place: Seattle, Washington.\nJonas Pfeiffer, Sebastian Ruder, Ivan Vuli´c, and\nEdoardo Maria Ponti. 2024. Modular Deep Learning.\nArXiv:2302.11529 [cs].\nL. S. Shapley. 1953. 17. A Value for n-Person Games.\nIn Harold William Kuhn and Albert William Tucker,\neditors, Contributions to the Theory of Games (AM-\n28), Volume II, pages 307–318. Princeton University\nPress, Princeton.\nTaiga Someya and Yohei Oseki. 2023.\nJBLiMP:\nJapanese benchmark of linguistic minimal pairs. In\nFindings of the Association for Computational Lin-\nguistics: EACL 2023, pages 1581–1594, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nYixiao Song, Kalpesh Krishna, Rajesh Bhatt, and Mo-\nhit Iyyer. 2022. SLING: Sino linguistic evaluation\nof large language models. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 4606–4634, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nKarolina Stanczak, Edoardo Ponti, Lucas Torroba Hen-\nnigen, Ryan Cotterell, and Isabelle Augenstein. 2022.\nSame neurons, different languages: Probing mor-\nphosyntax in multilingual pre-trained models. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1589–1598, Seattle, United States. Association\nfor Computational Linguistics.\nEkaterina Taktasheva, Maxim Bazhukov, Kirill Kon-\ncha, Alena Fenogenova, Ekaterina Artemova, and\nVladislav Mikhailov. 2024.\nRuBLiMP: Rus-\nsian Benchmark of Linguistic Minimal Pairs.\nArXiv:2406.19232 [cs].\nLucas Torroba Hennigen, Adina Williams, and Ryan\nCotterell. 2020. Intrinsic probing through dimension\nselection. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 197–216, Online. Association for\nComputational Linguistics.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020. Investigating Gender Bias in Lan-\nguage Models Using Causal Mediation Analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12388–12401. Curran Associates,\nInc.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics 2020, pages\n409–410, New York, New York. Association for Com-\nputational Linguistics.\nEthan Gotlieb Wilcox, Richard Futrell, and Roger Levy.\n2024. Using Computational Models to Test Syntactic\nLearnability. Linguistic Inquiry, 55(4):805–848.\nS Winkler. 2006. Ellipsis. In Encyclopedia of Language\n& Linguistics, second edition. Elsevier Ltd.\nA\nAll BLiMP Paradigms\nSee Table 4 for a list of all minimal pair paradigms\nand examples in the BLiMP dataset (Warstadt et al.,\n2020), organised into their relevant linguistic cate-\ngories.\nB\nCluster Purity\nClusters\nPurity: µ (σ)\nBERT REFERENCE\nBERT clusters\n0.497 (0.045)\nRoBERTa clusters\n0.530 (0.046)\nRandom clusters\n0.277 (0.027)\nROBERTA REFERENCE\nBERT clusters\n0.532 (0.044)\nRoBERTa clusters\n0.765 (0.063)\nRandom clusters\n0.285 (0.028)\nTable 5: Mean (µ) purity scores and standard deviations\n(σ) across k-means runs measured against our BERT\nand RoBERTa reference clusters. Each purity score\nrepresents comparison with 100 individual runs, within-\nmodel comparisons are underlined.\nSince k-means clustering is a stochastic process,\nSHV clusters may differ between runs of the algo-\nrithm. We focus on showing how qualitative and\nquantitative analysis can reveal shared patterns be-\ntween clusters in general, thus ensuring clustering\nconsistency is not our goal. Nevertheless, we eval-\nuate do carry out intrinsic evaluation of the method\nusing purity as a metric. Given N data points, a\nset of clusters Ω= {ω1, ω2, . . . , ωK}, and a set of\nclasses C = {c1, c2, . . . , cJ}, purity is calculated\nthrough assigning each cluster to the class most fre-\nquent in the cluster, and then measuring the number\nof correctly assigned data points divided by N, see\nEquation 3.\npurity(Ω, C) = 1\nN\nX\nk\nmaxj |ωk ∩cj|\n(3)\nPurity falls between 0 (no match between clus-\nters) and 1 (perfect match), and it is typically calcu-\nlated against a gold standard cluster set. As we do\nnot have gold clusters, we measure the purity using\nreference clusters for both BERT and RoBERTa\n(see the clusters in Section 4). Since cluster labels\nmay also change between runs, we cannot simply\nuse cluster IDs as gold labels even using the ref-\nerence clusters. Instead, we aim to align clusters\nBLiMP Paradigm\nGrammatical/Ungrammatical Examples\nANAPHOR AGR.\nAnaphor gender agreement\nA girl couldn’t reveal herself/*himself.\nAnaphor number agreement\nThomas complained about himself/*themselves.\nARG. STRUCTURE\nCausative\nAaron breaks/*appeared the glass.\nDrop argument\nTravis is touring/*Travis is revealing.\nInchoative\nPatricia had changed./*Patricia had forgotten.\nIntransitive\nThe screen does brighten/*resemble.\nPassive 1\nTracy isn’t fired/*muttered by Jodi’s daughter.\nPassive 2\nSteve isn’t disliked/*lied.\nTransitive\nDiane watched/*screamed Alan.\nBINDING\nPrinciple A (c-command)\nA girl that wouldn’t watch Omar questions herself/*himself.\n\"\n(case 1)\nThe teenagers explain that they/*themselves aren’t breaking all glasses.\n\"\n(case 2)\nEric imagines himself taking every rug/*Eric imagines himself took every rug.\n\"\n(domain 1)\nCarla had explained that Samuel has discussed her/*herself.\n\"\n(domain 2)\nJames says Kayla helped herself/*himself.\n\"\n(domain 3)\nGina explains Alan fires himself/*Alan explains Gina fires himself.\n\"\n(reconstruction)\nIt’s himself that this cashier attacked/*It’s himself that attacked this cashier.\nCONTROL/RAISING\nExistential ‘there’ (object raising)\nFrank judged /*compelled there to be a photograph of Michael looking like Sherry.\n\"\n(subject raising)\nThere was bound to be a fish escaping/*There was unable to be a fish escaping.\nExpletive ‘it’ object raising\nThis cashier had ascertained/*can’t press it to be not so interesting that Anna painted.\n‘Tough’ vs raising (1)\nJulia wasn’t fun/*unlikely to talk to.\n\"\n(2)\nBruce was sure/*annoying to remember Gerald.\nDET.-NOUN AGR.\nDeterminer-noun agreement (1)\nRaymond is selling this sketch/*sketches.\n\"\n(2)\nTracy passed these/*this art galleries.\n\"\n(irregular 1)\nThe driver reveals these/*this mice.\n\"\n(irregular 2)\nNatalie describes this/*these child.\n\"\n(with adjective 1)\nMany men have these messy cups/*cup.\n\"\n(with adjective 2)\nDonna might hire this/*these serious actress.\n\"\n(with adjective, irregular 1)\nHeidi returns to that big woman/*women.\n\"\n(with adjective, irregular 2)\nDenise did confuse that/*those important women.\nELLIPSIS\nEllipsis N-bar (1)\nThis print scares a lot of busy senators and Benjamin scares a few/*This print scares a lot of senators and Benjamin scares a few busy.\n\"\n(2)\nVincent wore one shirt and Matt wore some big shirt/*Vincent wore one hidden shirt and Matt wore some big.\nFILLER-GAP DEP.\nWh-questions (object gap)\nJoel discovered the vase that Patricia might take/*Joel discovered what Patricia might take the vase.\n\"\n(subject gap)\nLeslie remembered some guest that has bothered women./*Leslie remembered who some guest has bothered women.\n\"\n(subject gap, long distance)\nRegina sees that candle that Steve lifts that might impress every doctor./*Regina sees who that candle that Steve lifts might impress every doctor.\nWh vs ‘that’ (no gap)\nMark figured out that/*who most governments appreciate Steven.\n\"\n(no gap, long distance)\nEva discovered that/*who all pedestrians that have performed upset Candice.\n\"\n(with gap)\nWayne has revealed who/*that most hospitals admired.\n\"\n(with gap, long distance)\nKenneth investigated who/*that the cashiers that perform cared for.\nIRREGULAR FORMS\nIrregular past participle (adjectives)\nThe broken/*broke mirrors were blue.\n\"\n(verbs)\nThe Borgias wore/*worn a lot of scarves.\nISLAND EFFECTS\nAdjunct island\nWho should Derek hug after shocking Richard?/*Who should Derek hug Richard after shocking?\nComplex NP island\nWhat can’t a guest who would like some actor argue about?/*What can’t some actor argue about a guest who would like?\nCoordinate structure constraint (complex left branch)\nWhich teenagers had Tamara hired and Grace fired?/*Which hard Tamara hired teenagers and Grace fired?\n\"\n(object extraction)\nWhat had Russel and Douglas attacked?/*What had Russel attacked and Douglas?\nLeft branch island (echo question)\nIrene had messed up whose rug?/*Whose had Irene messed up rug?\n\"\n(simple question)\nWhose museums had Dana alarmed?/*Whose had Dana alarmed museums?\nSentential subject island\nWho should pedestrians’ curing Deanna scare/*Who should pedestrians’ curing scare Deanna.\nWh-island\nWho isn’t Craig realising he/*who kisses?\nNPI LICENSING\nNPI licensor present (matrix question)\nHad Bruce ever played? / *Bruce had ever played.\n\"\n(‘only’)\nOnly/*Even Bill would ever complain.\n\"\n(sentential negation)\nTeresa had not/*probably every sold a movie theater.\nNPI present (1)\nEven Suzanne has really/*ever joked around.\n\"\n(2)\nTamara really/*ever exited these mountains.\nNPI scope (‘only’)\nOnly many people who George likes ever clashed./*Many people who only George likes ever clashed.\n\"\n(sentential negation)\nEvery coat that did scare Nina has not ever wrinkled/*Every coat that did not scare Nina has ever wrinkled.\nQUANTIFIERS\nExistential ‘there’ quantifiers (1)\nThere was a/*each documentary about music irritating Allison.\n\"\n(2)\nAll dancers are there talking to Pamela./*There are all dancers talking to Pamela.\nSuperlative quantifiers (1)\nNo girl attacked fewer/*at most than two waiters.\n\"\n(2)\nThe/*No hospital had fired at most four people.\nS-SELECTION\nAnimate subject (passive)\nLisa was kissed by the boys/*blouses.\n\"\n(transitive)\nPhillip/*This pasta can talk to those waitresses.\nSUBJECT-VERB AGR.\nDistractor agreement (relational noun)\nA story about the Balkans doesn’t/*don’t irritate a person.\n\"\n(relative clause)\nBoys that aren’t disturbing Natalie suffer/*suffers.\nIrregular plural subject verb agreement (1)\nThis goose isn’t/*weren’t bothering Edward.\n\"\n(2)\nThe people/*person conspire.\nRegular plural subject verb agreement (1)\nThe cups alarm/*alarms Angela.\n\"\n(2)\nThe students/*student perform.\nTable 4: A list of all BLiMP minimal pair paradigms and examples, organised according to their respective linguistic\ncategories.\nusing the Hungarian (or Kuhn-Munkres) algorithm\nthat matches clusters by maximising shared data-\npoints between them.\nTable 5 shows purity scores between BERT and\nRoBERTa clusters and three sets of 100 different\ncluster sets: these include other runs of k-means\nclustering on the BERT and RoBERTa SHVs, and\nrandomly assigned clusters. It is clear that the co-\nhesion of clusters as measured in terms of purity\nacross and within models far exceeds the cohesion\nbetween the reference clusters and the random clus-\nters.\nC\nAll Clusters\nSee Tables 6 and 7 that list all BERT and RoBERTa\nclusters, representing both matching and (under-\nlined in the tables) and not matching ones.\nD\nT-Test on Pruning In- and\nOut-of-Cluster\nCluster\nT-stat.\nP-value\nNPI\n−4.809\n1.12e−05\nIsland\n0.995\n0.329\nQuantifiers\n−2.053\n0.113\nBinding\n−8.719\n7.97e−11\nFiller-gap\n−4.885\n2.94e−06\nWh vs ‘that’\n−1.855\n0.157\nTable 8: Results of the T-test between in-cluster and\nout-of-cluster pruning with the BERT model.\nTable 8 shows T-test statistics and P-values –\nadjusted using the Bonferroni correction (Dunn,\n1961) – between accuracy changes in the in-cluster\nand out-of-cluster pruning of relevant clusters with\nthe BERT model. The NPI, Binding, and Filler-\ngap clusters pass the T-test, i.e., the differences\nbetween the distribution of in- and out-of-cluster\nvalues are statistically significant. This indicates\na more drastic consequence of pruning within\nBLiMP paradigms in these clusters than when us-\ning prune masks from paradigms outside of the\nclusters.\nThis is not the case in the other three clusters.\nHowever, both the Quantifiers and Wh vs ‘that’\nclusters contain only 2-2 paradigms, meaning that\nin-cluster pruning involves only four datapoints.\nThis makes the success of any statistical analysis\nquestionable. Finally, the Island cluster contains\nonly 5 out of 8 ISLAND EFFECTS paradigms. Addi-\ntionally, it is also possible that the attention heads\nresponsible for processing these paradigms are not\nso well localisable as with other paradigms. This\nmay reduce the impact of pruning only the top 10\nattention heads.\nE\nGlossary of Linguistic Terms\nIn this section, we include a glossary of some lin-\nguistic terms relevant to BLiMP that merit defini-\ntion.\nAnaphors and binding theory\nBinding theory\nconditions the distribution of nominals, particularly\npronouns and anaphors, i.e., reflexive pronouns\n(Asudeh and Dalrymple, 2006). The main con-\nstraint occurs with respect to a potential antecedent\nnominal that co-refers with the target nominal. In\nthe following discussion, this co-reference is sig-\nnalled by indexing with i and j. Anaphors can only\noccur if there is a valid antecedent in the sentence,\nsee the example in (5) restated in (8):\n(8)\nGinai explains Alanj fires himselfj.\nPronouns can have a valid antecedent but not in\nthe same clause as them:\n(9)\nGinai explains Alani fires heri.\nControl and raising\nBoth constructions involve\na noun phrase (NP) in a main clause determining a\ncovert reference of a subject of a subordinate clause.\nThe key difference is that under raising this main\nclause NP (in M) receives its semantic role from\nthe verb of the subordinate clause ‘expected’ (in\nS), see in (10). Under control, on the other hand,\nthe NP receives its semantic role from the verb of\nthe main clause ‘persuaded’ (11).\n(10)\n[M The teacher expected the students ]M\n[S to help the visitors ]S.\n(11)\n[M The teacher persuaded the students ]M\n[S to help the visitors ]S.\nIn practice, however, both constructions involve\na subordinate that is seemingly lacking a subject\n(Dubinsky and Davies, 2006).\nEllipsis\nEllipsis refers to the \"omission of lin-\nguistic material, structure, and sound\" (Winkler,\n2006). Certainly ellipsis is not an unbounded phe-\nnomenon, and typically the omitted material stands\nin a co-reference relation with some overt element\nin the sentence.\nBLiMP Paradigm\nLinguistics Term\nM\nNPI Cluster\nNPI present (1)\nNPI LICENSING\n♢\nNPI present (2)\n♦\nNPI scope (‘only’)\n♦\nNPI scope (sentential negation)\n♢\nNPI licensor present (‘only’)\n♢\n\"\n(matrix question)\n♢\n\"\n(sentential negation)\n♢\nIrregular past participle verbs\nIRREGULAR FORMS\n♦\nIsland Effects\nAdjunct island\nISLAND EFFECTS\n♢\nComplex NP island\n♢\nCoordinate structure constraint (complex left branch)\n♢\n\"\n(object extraction)\n♦\nLeft branch island (simple question)\n♢\nLeft branch island (echo question)\n♢\nWh-island\n♦\nQuan-\ntifiers\nSuperlative quantifiers 1\nQUANTIFIERS\n♢\nSuperlative quantifiers 2\n♢\nBinding*\nAnaphor gender agreement\nANAPHOR AGR.\n♦\nAnimate subject trans.\nS-SELECTION\n♦\nPrinciple A (case 1)\nBINDING\n♦\n\"\n(domain 1)\n♢\n\"\n(domain 2)\n♢\n\"\n(domain 3)\n♢\n\"\n(c-command)\n♢\nFiller-Gap\nEllipsis N-bar (2)\nELLIPSIS\n♦\nExistential ‘there’ (subject raising)\nCONTROL/RAISING\n♦\nTough vs raising (2)\n♦\nPrinciple A (case 1)\nBINDING\n♦\n\"\n(case 2)\n♢\n\"\n(reconstruction)\n♢\nWh-questions (object gap)\nFILLER-GAP DEP.\n♢\n\"\n(subject gap)\n♢\n\"\n(subject gap, long distance)\n♢\nWh vs ‘that’ (no gap)\n♢\n\"\n(no gap, long distance)\n♢\nWh vs ‘That’\nInchoative\nARG. STRUCTURE\n♦\nIntransitive\n♦\nTough vs raising (1)\nCONTROL/RAISING\n♦\nWh vs ‘that’ (with gap)\nFILLER-GAP DEP.\n♢\n\"\n(with gap, long distance)\n♢\nBERT 1\nAnaphor gender agreement\nANAPHOR AGR.\n♦\nDistractor agreement (relational noun)\nSUBJECT-VERB AGR.\n♦\n\"\n(relative clause)\n♦\nIrregular plural subject verb agreement (1)\n♦\nExistential ‘there’ (object raising)\nCONTROL/RAISING\n♦\nExpletive ‘it’ (object raising)\n♦\nNPI scope (‘only’)\nNPI LICENSING\n♦\nSentential subject island\nISLAND EFFECTS\n♦\nBERT 2\nIrregular past participle adjectives\nIRREGULAR FORMS\n♦\nPassive (1)\nARG. STRUCTURE\n♦\nPassive (2)\n♦\nRoBERTa 1\nAnimate subject (passive)\nS-SELECTION\n♦\n\"\n(transitive)\n♦\nPassive (1)\nARG. STRUCTURE\n♦\nTable 6: BLiMP paradigms in the relevant BERT (♦) and RoBERTa (♦) clusters. Paradigms that appear in the\nclusters of both models are marked with ♢. Clusters without obvious organising principles are marked with model\nnames and numbers instead of labels.\nBLiMP Paradigm\nLinguistics Term\nM\nBERT 3\nAnaphor number agreement\nANAPHOR AGR.\n♦\nAnimate subject (passive)\nS-SELECTION\n♦\nCausative\nARG. STRUCTURE\n♦\nDrop argument\n♦\nInchoative\n♦\nIntransitive\n♦\nTransitive\n♦\nCoordinate structure constraint (object extraction)\nISLAND EFFECTS\n♦\nWh-island\n♦\nDeterminer-noun agr. (1)\nDET.-NOUN AGR.\n♦\n\"\n(2)\n♦\n\"\n(irregular, 1)\n♦\n\"\n(irregular, 2)\n♦\n\"\n(with adjective, 1)\n♦\n\"\n(with adjective, 2)\n♦\n\"\n(with adjective, irregular 1)\n♦\n\"\n(with adjective, irregular 2)\n♦\nEllipsis N-bar (1)\nELLIPSIS\n♦\nExistential ‘there’ quantifiers (1)\nQUANTIFIERS\n♦\n\"\n(2)\n♦\nIrregular plural subject-verb agreement (2)\nSUBJECT-VERB AGR.\n♦\nRegular plural subject-verb agreement (1)\n♦\n\"\n(2)\n♦\n‘Tough’ vs raising (1)\nCONTROL/RAISING\n♦\n\"\n(2)\n♦\nDet.-Noun Cluster (RoBERTa)\nDeterminer-noun agr. (1)\nDET.-NOUN AGR.\n♦\n\"\n(2)\n♦\n\"\n(irregular, 1)\n♦\n\"\n(irregular, 2)\n♦\n\"\n(with adjective, 1)\n♦\n\"\n(with adjective, 2)\n♦\n\"\n(with adjective, irregular 1)\n♦\n\"\n(with adjective, irregular 2)\n♦\nDistractor agreement (relational noun)\nSUBJECT-VERB AGR.\n♦\nRegular plural subject-verb agr. (1)\n♦\nRegular plural subject-verb agr. (2)\n♦\nTransitive\nARG. STRUCTURE\n♦\nRoBERTa 2\nAnaphor number agreement\nANAPHOR AGR.\n♦\nCausative\nARG. STRUCTURE\n♦\nDrop argument\n♦\nPassive (2)\n♦\nEllipsis N-bar (1)\nELLIPSIS\n♦\n\"\n(2)\n♦\nIrregular past participle (adjectives)\nIRREGULAR FORMS\n♦\n\"\n(verbs)\n♦\nIrregular plural subject-verb agreement (1)\nSUBJECT-VERB AGR.\n♦\n\"\n(2)\n♦\nNPI present (2)\nNPI LICENSING\n♦\nSentential subject island\nISLAND EFFECTS\n♦\nRoBERTa 3\nDistractor agreement (relative clause)\nSUBJECT-VERB AGR.\n♦\nExistential ‘there’ object raising\nCONTROL/RAISING\n♦\nExistential ‘there’ quantifiers (1)\nQUANTIFIERS\n♦\n\"\n(2)\n♦\nExistential ‘there’ subject raising\nCONTROL/RAISING\n♦\nExpletive ‘it’ object raising\n♦\nTable 7: BLiMP paradigms in the relevant BERT (♦) and RoBERTa (♦) clusters (continued). Paradigms that appear\nin the clusters of both models are marked with ♢. Clusters without obvious organising principles are marked with\nmodel names and numbers instead of labels.\nFiller-gap\ndependencies\nAlso\ncalled\nlong-\ndistance dependencies, ‘filler’ refers to a fronted\nelement and the ‘gap’ is the position with which it\nis semantically or syntactically related (Falk, 2006).\nExamples for such dependencies involve wh ques-\ntions, exclamatives, topicalisation, cleft, and more,\nsee the examples in (12).\n(12)\na. Which painting does the artist believe\nthe curator said the gallery owner hung\nt on the wall?\nb. What a painting the artist believes the\ncurator said the gallery owner hung t\non the wall!\nc. This painting, the artist believes the cu-\nrator said the gallery owner hung t on\nthe wall.\nd. It is the painting that the artist believes\nthe curator said the gallery owner hung\nt on the wall.\nIsland effects\nUnder certain (generative) linguis-\ntic analyses, various elements of a sentence ma-\nterialise in a different location than where they\nwere generated in (Carnie, 2006). For instance,\nwh-movement, i.e., the displacement of a question\nword is a common example for this, see (13).\n(13)\na. Mary does love pineapples.\nb. What does Mary love t?\nThe trace t indicates where the question word\nWhat originated from under the movement anal-\nysis. Islands, on the other hand, are a collection\nof constraints on movement. These typically con-\ntain restrictions on moving across various clauses.\nThere are many such constraints, but one exam-\nple is the so-called Coordinate Structure Constraint\nthat disallows extracting members of a conjunction,\nsee the examples in (14).\n(14)\na. I have eaten the salad and the pizza.\nb. *What have you eaten the salad and t?\nc. *What have you eaten t and the pizza?\nNPI\nNegative Polarity Items (NPI) – like any,\never, or yet – are words that appear only in a limited\nnumber of contexts. These contexts are among all,\nthe scope of negation, as complements of negative\npredicates, in comparative clauses, in questions,\nand in the scope of negative quantifiers and adverbs\nsuch as few/little, rarely, or only (Hoeksema, 2006).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-10-17",
  "updated": "2025-02-25"
}