{
  "id": "http://arxiv.org/abs/2410.14069v1",
  "title": "Rethinking Optimal Transport in Offline Reinforcement Learning",
  "authors": [
    "Arip Asadulaev",
    "Rostislav Korst",
    "Alexander Korotin",
    "Vage Egiazarian",
    "Andrey Filchenkov",
    "Evgeny Burnaev"
  ],
  "abstract": "We propose a novel algorithm for offline reinforcement learning using optimal\ntransport. Typically, in offline reinforcement learning, the data is provided\nby various experts and some of them can be sub-optimal. To extract an efficient\npolicy, it is necessary to \\emph{stitch} the best behaviors from the dataset.\nTo address this problem, we rethink offline reinforcement learning as an\noptimal transportation problem. And based on this, we present an algorithm that\naims to find a policy that maps states to a \\emph{partial} distribution of the\nbest expert actions for each given state. We evaluate the performance of our\nalgorithm on continuous control problems from the D4RL suite and demonstrate\nimprovements over existing methods.",
  "text": "Rethinking Optimal Transport in\nOffline Reinforcement Learning\nArip Asadulaev1,2 Rostislav Korst4 Alexander Korotin3,1\nVage Egiazarian5,6 Andrey Filchenkov2 Evgeny Burnaev3,1\n1AIRI\n2ITMO\n3Skoltech\n4MIPT\n5Yandex\n6HSE University\naripasadulaev@airi.net\nAbstract\nWe propose a novel algorithm for offline reinforcement learning using optimal\ntransport. Typically, in offline reinforcement learning, the data is provided by\nvarious experts and some of them can be sub-optimal. To extract an efficient policy,\nit is necessary to stitch the best behaviors from the dataset. To address this problem,\nwe rethink offline reinforcement learning as an optimal transportation problem.\nAnd based on this, we present an algorithm that aims to find a policy that maps\nstates to a partial distribution of the best expert actions for each given state. We\nevaluate the performance of our algorithm on continuous control problems from\nthe D4RL suite and demonstrate improvements over existing methods.\n1\nIntroduction\nDeep reinforcement learning (RL) has shown remarkable progress in complex tasks such as strategy\ngames [51], robotics [37], and dialogue systems [39]. In online RL, agents operate in the environment\nand receive rewards that are used to update policies. However, in some domains, such as healthcare,\nindustrial control, and robotics, interactions with the environment can be costly and risky. For these\nreasons, offline RL algorithms that use historical data of expert interactions with the environment are\nmore applicable. Given only a dataset of experiences, offline RL algorithms learn a policy without\nany online actions [33].\nDue to the off-policy nature of offline RL, naive training of online RL algorithms in offline settings\ndoes not provide effective solutions [33]. This is primarily due to a distribution shift between the\nlearned and the behavior policy [15], which causes the instability of the critic function during off-\npolicy evaluation. To solve this problem and make actions of the learned policy more similar to the\none on which critic function was trained, a Behavior Cloning (BC) objectives was proposed [52, 33].\nAs an example of BC loss, Optimal Transport (OT) [50] distance has been introduced for RL. This\napplication of OT is particularly relevant because by minimizing the OT distance between policies, we\ncan efficiently clone the behavior [52, 11, 40, 34, 20, 9, 35]. But there is another problem, in offline\nRL, the dataset often consists of various experts demonstrations. Some of these demonstrations may\nbe incorrect, or efficient only for certain parts of the environment. Consequently, cloning the behavior\npolicy in such scenarios may limit policy improvement [30]. To solve these types of problems, RL\nalgorithms need to apply stitching, which means that the algorithm selects the best action for each\nstate and recombines different trajectories provided by multiple experts [30].\nIn our paper, we rethink OT as a framework for offline reinforcement learning that aims to stitch the\nbest trajectories, rather than clone the policy. Unlike previous OT-based approaches in RL, we do\nnot introduce a new OT-based regularization or a reward function [11, 40]. Instead, we propose a\nnovel perspective that views the entire offline RL problem as an optimal transport problem\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2410.14069v1  [cs.LG]  17 Oct 2024\nbetween state and actions distribution (M3). By utilizing the Q-function as a transport cost and the\npolicy as an optimal transport map, we formalize offline reinforcement learning as a Maximin OT\noptimization problem. This opens doors for applying recent advantages of OT methods to RL directly.\nIn particular, we propose an algorithm that trains a policy to identify the partial distribution of the\nbest actions for each given state.\nContribution: Based on optimal transport, we introduce Partial Policy Learning (PPL) algorithm,\nwhich efficiently identifies the best action for each state in the dataset. We evaluated our method\nacross various environments using the D4RL benchmark suite, achieving superior performance\ncompared to existing OT-based offline RL methods and current state-of-the-art model-free offline RL\ntechniques.\n2\nBackground and Related Work\n2.1\nOptimal Transport\nPrimal form of the optimal transport problem was first proposed by Monge [50]. Suppose there\nare two probability distributions µ and ν over measurable spaces X and Y respectively, where\nX, Y ⊂RD. We want to find a measurable map T : X →Y such that the mapped distribution is\nequal to the target ν, for a cost function c : X × Y →R, the OT problem between µ, ν:\nmin\nT ♯µ=ν Ex∼µ\n\u0002\nc(x, T(x))\n\u0003\n.\n(1)\nwhere T♯is the push forward operator and respectively T♯µ = ν represents the mass preserving\ncondition. Informally, we can say that the cost is a measure of how hard it is to move a mass piece\nbetween points x ∈X and y ∈Y from distributions µ and ν correspondingly. That is, an OT\nmap T shows how to optimally move the mass of µ to ν, i.e., with the minimal effort. A widely\nrecognized alternative formulation for optimal transport was introduced by Kantorovich [22]. Unlike\nthe Monge’s OT problem formulation, this alternative allows for mass splitting. The Kantorovich OT\nproblem can be written as:\nmin\ng∈Π(µ,ν) E(x,y)∼g\n\u0002\nc(x, y)\n\u0003\n.\n(2)\nIn this case, the minimum is obtained over the transport plans g, which refers to the couplings Π with\nthe respective marginals being µ and ν. The optimal g∗belonging to Π(µ, ν) is referred to as the\noptimal transport plan.\nDual form of optimal transport, following the Kantorovich-Rubinstein duality [49], can be obtained\nfrom (2) and written as:\nmax\nf\nEx∼µ\n\u0002\nf c(x)\n\u0003\n+ Ey∼ν\n\u0002\nf(y)\n\u0003\n,\n(3)\nwhere f : X →R and f c(x)=min\ny∈ν\n\u0002\nc(x, y)−f(y)\n\u0003\nis referred to as the c-transform of potential\nf [50]. For cost function c(x, y) = ∥x−y∥2, the resulted OT cost is called the Wasserstein-1 distance,\nsee [50, M1] or [44, M1, 2]. It was shown that to compute Wasserstein-1 distance, instead of computing\nconjugate function f c we can simply consider f to be from the set of 1-Lipschitz functions[50, 1].\nMaximin formulation for simultaneously computing the OT distance and recovering the OT map\nT was recently proposed. According to [27], by applying the Rockafellar interchange theorem [43,\nTheorem 3A], we can replace the optimization over points y ∈ν with an optimization over functions\nT : X →Y, which reformulates the problem (3) as a saddle-point optimization problem for the\npotential f and map T:\nmax\nf\nmin\nT\nEx∼µ\n\u0002\nc(x, T(x)) −f(T(x))\n\u0003\n+ Ey∼ν\n\u0002\nf(y)\n\u0003\n(4)\nUsing this formulation, significant progress has been achieved in utilizing neural networks for\ncomputing OT maps to address strong [36, 24, 18], weak [27], and general [2] OT problems. Our\nwork is inspired by these developments, as we leverage neural optimal transport methods to improve\noffline reinforcement learning.\nPartial OT: In cases where it is necessary to ignore some data and map the input to part of the target\ndistribution, techniques like unbalanced or partial optimal transport are commonly employed [12, 4,\n2\n17]. For example, partial optimal transport is useful in resource allocation problems where only a\nsubset of resources needs to be optimally allocated to match a subset of demands [12].\nIn our paper, we consider the partial OT formulation proposed by [17]. This framework, named\nextremal OT, solves the partial alignment between the full input distribution and part of the target\ndistribution. Essentially, for Euclidean cost functions such as ℓ2, partial transport maps can be seen as\na tool for finding nearest neighbors (maximally close) to the input samples from the target according\nto the cost function. Please see Figure 1 in [17]. Formally, this method can be written as:\nmin\nT ♯µ≤wν Ex∼µ\n\u0002\nc(x, T(x))\n\u0003\n.\n(5)\nWe can call w a coefficient of unbalance. The intuition behind this formulation is as follows: when\nw = 1, the constraint T♯µ ≤wν is equivalent to T♯µ = ν. This equivalence holds because there is\nonly one probability measure that is less than or equal to ν, and it is ν itself. However, with values of\nw ≥1, the mass of the second measure is scaled. Consequently, it is sufficient to match µ only to\npart of the second measure ν to transfer the full mass contained in µ. Since the cost c is our objective,\nT will tend to map to the samples from ν that are closest to the input distribution. It was shown that\nthis formulation can also be considered in maximin settings [17], which is actually the core of our\nmethod (Section 3.2).\n2.2\nOffline Reinforcement Learning\nReinforcement Learning is a well-established framework for decision-making processes in environ-\nments modeled as Markov Decision Processes (MDPs). The MDP is defined as M = (S, A, P, r, γ)\nby the state space S, action space A, transition probability P(s′ | s, a), reward function r(s, a), and\ndiscount factor γ. The goal in RL is to find a policy π(a|s) that maximizes the expected cumulative\nreward over time t: J(π)\ndef\n= Eπ[P∞\nt=0 γtr(st, at)]. Also we define the distribution over the state\nspace following policy π as dπ(s). To estimate of the expected cumulative reward following a given\npolicy π the critic function Qπ is used:\nQπ (s, a) = r (s, a) + γEs′∼T (s,a),a′∼π(s′) [Qπ (s′, a′))\n\u0003\n.\n(6)\nIn deep RL neural networks are used to parameterize critic Qπ\nϕ(s, a) and policy πθ. Critic can\nbe learned by minimizing the mean squared Bellman error over the experience replay dataset\nD = {(si, ai, s′\ni, ri)}, which consist of trajectories implied by policy π:\nmin\nQπ\nϕ\nE(s,a,s′)∼D\nh\u0000r(s, a) + γEa′∼πθ(s′)\n\u0002\nQπ\nϕ(s′, a′)\n\u0003\n−Qπ\nϕ(s, a)\n\u00012i\n.\n(7)\nUsing a trained critic function we can recover a near-optimal policy, for example, using Deterministic\nPolicy Gradient (DPG) [45] or its improved version named Twin Delayed Deep Deterministic Policy\nGradient (TD3) [15].\nOffline RL is a fully data-driven approach for decision-making problems. Conventional offline RL\nalgorithms use the (7) formula to recover the Q function, using data D collected according to the\nexpert policy β. However, due to distribution shift between actions from D and those induced by\npolicy π, the Q function may suffer from inefficiency and overestimation bias during evaluation [15].\nTo mitigate this issue, various offline RL algorithms with behavior cloning objectives was proposed.\nSee the related work section (M6) for more details.\nOT in offline RL has been proposed primarily for efficient behavior cloning. The simplest method of\nintegrating OT into offline RL is by utilizing the Wasserstein-1 distance as the measure of dissimilarity\nbetween the learned and expert policies. This method is known as the W-BRAC algorithm [52].\nGiven the critic function Q(s, a), the Wasserstein-1 distance defined by f, and the behavior cloning\ncoefficient α, we have\nminπ max∥f∥L≤1 Es∼D,a∼π(s)\n\u0002\n−Qπ(s, a)\n|\n{z\n}\nCritic\n\u0003\n+ α\n\u0000E(s,a)∼D\n\u0002\nf(s, a)\n\u0003\n−Es∼D,a∼π(s)\n\u0002\nf(s, a)\n\u0003\n|\n{z\n}\nWasserstein-1 distance\n\u0001\n(8)\nIn real-world applications, collecting large and high-quality datasets may be too costly or impractical.\nDue to that, the provided data it is often collected by many expert playing several near-optimal policies\nand only a single expert can be optimal in dataset D. The formulation (8) has no mechanism to infer\nthe importance of each action, its just clone the entire data. Moreover, the problem is that computing\nWasserstein-1 distance is typically complicated [25], because the potential f is required to satisfy the\n1-Lipschitz condition [1]. It is also important to note that the coefficient α is task-dependent[6].\n3\nAlgorithm 1 Partial Policy Learning\nInput: Dataset D(s, a, r, s′)\nInitialize Qϕ, πθ, fω, β\nfor k in 1...N do\n(s, a, r, s′) ←D: sample a batch of transitions from the dataset.\nQk+1 ←Update cost function Qπ\nϕ using the Bellman update in (2).\nf k+1\n←\nUpdate\nfω\nusing\noutputs\nof\nπθ\nand\nsamples\nfrom\ndataset:\narg minf −Es∼D,a∼πk(s)[f k(s, a)] + wEs,a∼D[f k(s, a)]\nπk+1 ←Update policy πθ as a transport map: arg minπ Es∼D,a∼πk(s)[−Qk(s, a) −f k(s, a)].\nend for\n3\nRethinking OT in RL\n3.1\nConsidering RL as OT problem\nTo extend a connection and apply OT methods in RL, we can view the entire offline RL prob-\nlem as an optimal transport problem. For this purpose, let’s consider (1). By replacing the cost\nfunction c(x, y) with the critic Qπ(s, a) and treating our policy π as a map that moves mass\nfrom the state distribution dβ(s), (d(s) for shorthand), to the corresponding distribution of ac-\ntions given by the behavior policy β(·|s), we obtain a primal state-conditional Monge OT problem.\nEs∼D,a∼π(s) minπ♯d(s)=β(·|s)[−Qπ(s, a)] which is equal to:\nmin\n\b\nπ♯d(s)=β(·|s)\n\t Es∼D,a∼π(s)\n\u0002\n−Qπ(s, a)\n\u0003\n.\n(9)\nThe objective is to minimize the expectation of the negative critic function Qπ, (we can also consider\nQβ) while mapping exclusively to the distribution of actions given by the expert policy β.\nRemark 1: In this formulation, the imposed equality constraints force the policy to mimic the\nbehavior of the expert policy. However, since the provided dataset often contains sub-optimal paths,\nthis formulation remains inappropriate for avoiding inefficient actions provided by the expert policy.\nIn the following subsection, we introduce an algorithm that integrates Partial Optimal Transport [17]\nfor deep RL.\n3.2\nStitching with Partial OT\nIn many real-world applications, collecting high-quality datasets may be costly or impractical.\nTherefore, offline RL algorithms need to select the best actions provided by the experts and ignore\nsub-optimal ones. Our idea is that partial OT can help us deal with sub-optimal trajectories by\nmapping states only to the most relevant parts of the action distribution with respect to the critic\nfunction Q To obtain partial formulation of 3, instead of equality constraints, we need to consider\noptimization problem with the following inequality constraints [17]:\nmin\n\b\nπ♯d(s)≤w(β(·|s))\n\t Es∼D,a∼π(s)\n\u0002\n−Qπ(s, a)\n\u0003\n.\n(10)\nAs discussed in Section 2.1, the parameter w scales the mass contained in the target distribution. For\nvalues of w ≥1, the mass of the second measure is increased. Consequently, it is not necessary to\ncover the full distribution β(·|s) to transport the Dirac mass given by d(s). As a result, a learned\npolicy (map) matches the state only to part of the action distribution β(·|s). This is particularly\nrelevant for offline RL, as the resulting policy will not clone all actions but will select only the most\noptimal ones, where optimality is defined by the critic function Q.\nProposition 1: (Informal) for a critic-based cost function Qβ, an offline Policy trained by (10)\nimproves over the behavior policy β. Proofs are given an Appendix (M9.1).\nTo find a practical solution to the given problem using neural networks, we can also consider a\ndual and maximin formulation for (10). The dual form for the partial transport problem[17] can be\nexpressed as\nmax\nf≥0 Es∼D,a∼π\n\u0002\nf c(s, a)\n\u0003\n+ wEs∼D,a∼β\n\u0002\nf(s, a)\n\u0003\n,\n(11)\n4\nTo obtain the maximin formulation, we expand the c-transform (3) using the function Q as the cost\nc. This operation can be represented as Es∼D,a∼π[f c(s, a)] = Es∼D[mina {−Qπ(s, a) −f(s, a)}].\nSubsequently, by applying the Rockafellar interchange theorem [43, Theorem 3A], we replace the\noptimization over points a with an equivalent optimization over functions π : S →A. The solution\ncan be computed using neural approximations, which is useful for RL tasks.\nmax\nf≥0 min\nπ Es∼D,a∼π(s)\n\u0002\n−Qπ(s, a)\n|\n{z\n}\nCost\n−f(s, a)\n\u0003\n+ wE(s,a)∼D\n\u0002\nf(s, a)\n\u0003\n|\n{z\n}\nConstraints\n.\n(12)\nRemark 2: In the [17, Proposition 4] it was shown that f ∗(s, a) = 0 vanishes on the outliers i.e\non the samples outside the scaled target distribution. The policy update in (12), can be written as\nminπ Es∼D,a∼π(s)[−Q(s, a) −f(s, a)]. This reveals that if f(s, a) = 0 exclusively on the outlier\nactions, it then applies extra weight to correct actions, which reduces the value of −Q(s, a)−f(s, a).\nRemark 3: The value of w controls the size of the action space in which we map; the higher the\nvalue of w, the smaller the subset[17]. When the actions are provided by a single expert, scaling w\ncan be detrimental to performance.\nThe primary distinction between our method (12), and previous OT for RL approaches, particularly\n(8), is as follows:\n• OT is not treated as a component of the problem or as a regularization term with a coefficient\nof α. Instead, the entire policy optimization is considered as OT.\n• Our formulation for computing OT does not necessitate 1-Lipschitz function constraints\non f, as it does not compute the Wasserstein-1 distance. Rather, it addresses a maxmin\nOT problem with a critic-based cost function, where f can be any arbitrary scalar function\nsatisfying f ≥0.\n• Unlike traditional OT approaches that fully aligns two distributions, our method maps only\nto the part of the target distribution containing the most relevant actions with respect to the\ncritic.\nIn practice, we use neural networks πθ : S →A and fω : S × A →R to parameterize π and\nf respectively. These neural networks serve as function approximators that can capture complex\nmappings of states. We train these neural networks using stochastic gradient ascent-descent (SGAD)\nby sampling random batches from the dataset D. At each training step, we sample a batch of\ntransitions from the offline dataset D and then adjust the value function Q, and the potential f, Then\nfor the k update step, given the last policy value function Qk, and f k, we update the policy, see\nAlgorithm 1.\n4\nToy Experiments\n(a)\n(b)\n(c)\nFigure 1: Toy experiments. (a) Left point S0 denoting start and ST is the only rewarded, target\nlocation. Black curves visualize behavior trajectories β. (b) Best behavior policy β∗according to the\ndata, and the optimal policy π∗that provides the optimal (shortest path) solution. (c) Comparison of\nthe policy β trained by minimizing the objective −Qβ(s, π(s))+BC and our policy π trained by the\nPPL algorithm 1.\nTo demonstrate the ability of our algorithm to extract the best policy from the sub-optimal behavior\ntrajectories, we constructed a simplified offline RL dataset. The goal in this setup is to find the\nshortest path from a starting location S0 to a reward point ST , navigating through 50 intermediate\nstates St uniformly spaced along the x-axis from -1.3 to 0. The action space is continuous, with the\nagent’s task being to determine the optimal y-values corresponding to each x-coordinate. Only the\n5\nfinal state ST yields a reward of 1, while all other intermediate states St have a reward of zero. The\noffline RL dataset consisted of three sub-optimal expert trajectories, as shown in Figure 1(a). The\nbest policy that can be extracted by recombining the expert trajectories and an actual optimal policy\nπ∗that provides the shortest path are highlighted in Figure 1(b).\nWe compared our method with the offline RL algorithm based on behavior cloning. For this, we\ntrained the critic Qβ by (2), using actions drawn from the observed behavior trajectories β presented\nin the dataset D. Given Qβ, we minimize Es,a∼D[−Qβ(s, β(s)) + ∥β(s) −a∥2] to obtain the policy\nβ.1 Next, using the given critic function Qβ, we trained the policy π(s) using our algorithm with\nparameter w = 8. Our approach successfully identified an optimal straight trajectory, see Figure 1(c).\nCompared to offline RL 1(b), which produced average expert solutions, our method demonstrated\nsuperior performance by extracting and exploiting the most strategic insights from the data, ignoring\nall inefficient actions.\nIn all experiments, a single-layer neural network with 32 neurons and ReLU activation was used as\nthe configuration for both the critic and policy networks. For our method, we used a Lagrangian\nnetwork f(s, a) with similar parameters. There are 5000 steps was done for each network, with\nAdam [23] optimizer with a learning rate (lr) of 1e-3.\n5\nD4RL Experiments\n5.1\nBenchmarks and Baselines\nWe evaluate our proposed method using the Datasets for Deep Data-Driven Reinforcement Learning\n(D4RL) [13] benchmark suite, which is a collection of diverse datasets designed for training and\nevaluating deep RL agents in a variety of settings. It includes tasks in continuous control, discrete\ncontrol, and multi-agent settings with a variety of reward structures and observation spaces. First, we\ntested our method on the Gym’s MuJoCo-v2 environments, such as Walker, Hopper, and HalfCheetah.\nWe also tested our method on the complex Antmaze-v2 and Android-v1 environments.\nTo compare the performance of our proposed method, we selected four state-of-the-art offline RL\nalgorithms. These include Conservative Q-Learning (CQL) [32], Twin Delayed Deep Deterministic\nPolicy Gradient with behavior Cloning (TD3+BC) [14], Implicit Q-Learning IQL [28], ReBRAC [47],\nand IQL with Optimal Transport Reward Labeling (IQL+OTR) [35]. More details on the methods are\ngiven in (M6). The results of the supervised behavior cloning are also included.\n5.2\nSettings\nPPL. First, to be consistent with Proposition 1, we considered our method in the One-Step RL\nsettings. Following the original setting provided by One-Step RL [6], we pre-trained β for 500k\nsteps. Next, we pre-trained Qβ for 2 million steps. Then, to avoid overestimation bias, we applied a\nsimplified conservative update to Qβ (see [32, Eq.1]). Finally, to improve beyond the behavior policy\nβ, we trained the policy π for the 100k steps using our method 1. For these experiments, a two-layer\nfeed-forward network with a hidden layer size of 1024 and a learning rate of 0.001 was used with\nthe Adam optimizer [23]. We tested this approach for the Mujoco environment see Table 4, for the\ncomparison with the OneStep RL method.\nPPLCQL Second, we tested our method in conjunction with the CQL method on the Antmaze\nproblems. In this setting, no pre-trained models were used; we used the code provided by the CORL\nlibrary [48], with all hyperparameters and architectures set identical to those originally used in CORL\nfor the CQL method. We trained the algorithm for 1M steps, with w set to 8 for all experiments. See\nthe comparison with CQL in Table 1.\nPPLR Finally, we coupled our method with the improved version of TD3+BC called ReBRAC [47].\nIn these experiments, for a cost function, the TD3+BC objective was used, which combines the Q\nfunction with a supervised loss. The same hyperparameters proposed by the authors for ReBRAC\nwere kept in our experiments. No pre-trained models were used, we trained the agent from scratch for\n1M steps. We have tested our method on the Mujoco, Android, and Antmaze datasets. While the\n1We also tested direct minimization of −Qβ(s, a). However, even in toy settings this method suffered from\noverestimation bias, causing the resulting policy to consistently predict values equal to 1.\n6\nReBRAC framework recommends task-specific hyperparameters [47], the results for the Antmaze\nare also reported for the best w hyperparameter. The results can be found in the tables 2, 3 and 1\nrespectively.\n5.3\nResults\nIn the Table 1 we consider a side-by-side comparison, and the color purple indicates the best results\nbetween two columns. In the Tables 2 and 3 the color indicates the top 3 results. We compared the\nresults with those reported in the ReBRAC [47] and OTR+IQL papers [35]. For the Antmaze datasets,\nthe main comparison is with the oracle method to which our method is coupled. For comparison,\nwe reproduced the CQL and ReBRAC results using the code provided in CORL. The CQL score\ncurves are provided in Appendix 3. For completeness, IQL and OTR+IQL results are also included in\nTable 1.\nTable 1: Averaged normalized scores on Antmaze-v2 tasks. Reported scores are the results of the\nfinal 100 evaluations and 5 random seeds.\nDataset\nIQL\nOTR+IQL\nCQL\nPPLCQL(Ours)\nReBRAC\nPPLR(Ours)\numaze\n87.5 ± 2.6\n83.4 ± 3.3\n86.3 ±3.7\n90±2.6\n97.8 ± 1.0\n98.0 ±14\numaze-diverse\n62.2 ± 13.8\n68.9 ± 13.6\n34.6 ±20.9\n40±2.6\n88.3 ± 13.0\n93.6±6.1\nmedium-play\n71.2 ± 7.3\n70.5 ± 6.6\n63.0 ±9.8\n67.3±10.1\n84.0 ± 4.2\n90.2 ±3.1\nmedium-diverse\n70.0 ± 10.9\n70.4 ± 4.8\n59.6 ±3.5\n65.3±8.0\n76.3 ± 13.5\n84.8 ±14.7\nlarge-play\n39.6 ± 5.8\n45.3 ± 6.9\n20.0 ±10.8\n25.6±3.7\n60.4 ± 26.1\n76.8 ±4.0\nlarge-diverse\n47.5 ± 9.5\n45.5 ± 6.2\n20.0 ±5.1\n23.6 ±11.0\n54.4 ± 25.1\n76.6 ± 7.4\nTotal\n378\n384\n283.5\n311.8\n461.2\n520\nTable 2: Averaged normalized scores on MuJoCo tasks. Reported scores are the results of the final 10\nevaluations and 5 random seeds.\nDataset\nBC\nOne-RL\nCQL\nIQL\nOTR+IQL\nTD3+BC\nReBRAC\nPPLR(Ours)\nM\nHalf.\n42.6\n48.4\n44.0\n47.4\n43.3\n48.3\n65.6\n64.95±0.2\nHopper\n52.9\n59.6\n58.5\n66.3\n78.7\n59.3\n102.0\n93.49±7.2\nWalker\n75.3\n81.1\n72.5\n78.3\n79.4\n65.5\n82.5\n85.66±0.6\nMR\nHalf.\n36.6\n38.1\n45.5\n44.2\n41.3\n44.6\n51.0\n51.1±0.3\nHopper\n18.1\n97.5\n95.0\n94.7\n84.8\n60.9\n98.1\n100.0±2\nWalker\n26.0\n49.5\n77.3\n73.9\n66.0\n81.8\n77.3\n78.66±2.0\nME\nHalf.\n55.2\n93.4\n91.6\n86.7\n89.6\n90.7\n101.1\n104.85±0.1\nHopper\n52.5\n103.3\n105.4\n91.5\n93.2\n98.0\n107.0\n109.0±1.2\nWalker\n107.5\n113.0\n108.8\n109.6\n109.3\n110.1\n112.3\n111.74±1.1\nTotal\n467.7\n684.9\n698.6\n692.6\n685.6\n659.2\n796.9\n799.45\nIn offline RL, the problems are noisy, complex, and diverse, and task-specific hyperparameter search is\nrequired. Despite this fact, our method shows promising results. Our method consistently outperforms\nmethods on which it is based. The most important result is a significant improvement in the achieved\nscores for the Antmaze problems (Table 1). In particular, our method provides state-of-the-art results\nfor all datasets on this task, and gives a significant improvement of (+16) and (+21) points for the\ncomplicated large-play-v2 and large-play-diverse-v2 environments. Importantly, we consistently\noutperform the previous best OT-based offline RL algorithm, OTR+IQL.\nWe can interpret that our method lies between behavior cloning and direct maximization of the Q\nfunction. Recent studies have shown that direct maximization can lead to sub-optimal results due to\noverestimation bias [16]. Conversely, being too close to the expert’s policy prevents improvement [30].\nIntuitively, our extremal formulation allows us to strike a balance that maximizes Q by taking actions\nfrom the expert action distribution that are not radically different from those on which the Q function\nwas trained.\nOur formulation aligns the state space with only a portion of the action space. This property is\nparticularly relevant for offline datasets that contain a mixture of expert demonstrations. Our method\ncan help to select the best possible action from a set of expert trajectories [33]. In other words,\n7\nTable 3: Averaged normalized scores on Android tasks. Reported scores are the results of the final\n10 evaluations and 5 random seeds.\nDataset\nBC\nTD3+BC\nCQL\nIQL\nOTR+IQL\nReBRAC\nPPLR(Ours)\nPen\nHuman\n34.4\n81.8\n37.5\n81.5\n66.82\n103.5\n108.43\nCloned\n56.9\n61.4\n39.2\n46.8\n78.7\n91.8\n113.97\nExpert\n85.1\n146.0\n107.0\n133.6\n-\n154.1\n152.33\nDoor\nHuman\n0.5\n-0.1\n9.9\n3.1\n5.9\n0.0\n1.0\nCloned\n-0.1\n0.1\n0.4\n0.8\n0.01\n1.1\n0.44\nExpert\n34.9\n86.4\n101.5\n105.3\n-\n104.6\n104.66\nHammer\nHuman\n1.5\n0.4\n4.4\n2.5\n1.79\n0.2\n2.0\nCloned\n0.8\n0.5\n2.1\n1.1\n0.8\n6.7\n4.74\nExpert\n125.6\n117.0\n86.7\n106.5\n-\n133.8\n130.2\nRelocate\nHuman\n0.0\n-0.2\n0.2\n0.1\n-0.2\n0.0\n0.23\nCloned\n-0.1\n-0.1\n-0.1\n0.2\n0.1\n0.9\n0.45\nExpert\n101.3\n107.3\n95.0\n106.5\n-\n106.6\n92.99\nTotal\n340.9\n600.5\n484.8\n588.5\n-\n703.2\n711.44\noptimal f is to encourage the discovery of new actions that can yield high rewards. To illustrate the\ntask-specific dependence of the parameter w, we performed an ablation study for different values, see\n(M5.4)).\nRun time: The code is implemented in the PyTorch [41] and JAX frameworks and will be publicly\navailable along with the trained networks. Our method converges within 2–3 hours on Nvidia\n1080 (12 GB) GPU. We used WanDB [5] for babysitting training process. The code is available in\nsupplementary materials.\n5.4\nParameter w\n(a) umaze-v2\n(b) umaze-diverse-v2\n(c) medium-play-v2\n(d) medium-diverse-v2\n(e) large-play-v2\n(f) large-diverse-v2\nFigure 2: Exponential moving average (coef. 0.3) curves of the normalized score curves for the\nAntmaze. Different colors of the curves represent results for w = 3, w = 5, w = 8, w = 12.\nThe parameter w influences the action selection process by determining the support range over which\nthe policy operates. With the parameter w equal to 1, our method considers all actions provided in the\ndataset for each state. By increasing this value, we consider the smaller subspace of possible actions,\nwhich can be useful when it is necessary to select the best one from the data. In a perfect scenario,\nwe want to find as few actions as possible that maximize the score, so we usually favor the higher\nvalues of w.\n8\nFor some datasets, reducing the action subspace is unnecessary. We studied different parameters\nw=[3,5,8,12] for the range of tasks, results for the Antmaze are presented in Figure 2. We can see\nthat the biggest effect of the parameter was accrued for the large-play and large-diverse task s2 (c).\nThis means that selecting the subspace of actions is important for this environments, and our method\nallows for this nuanced control depending on the task.\n6\nRelated work\nThere is a rich family of offline RL algorithms. To solve the distribution shift problem, several\nmethods have been proposed [6, 29, 53, 7, 31, 14, 28]. The simplest approach is the behavior\nregularized actor-critic (BRAC) [52]. Minimizing some divergence D, between the learned π and the\nexpert policy β, the BRAC framework can be written as distance regularized policy optimization:\nWhere D(π(·|s), β(·|s)) is BC loss. Based on BRAC framework [14] proposed a TD3+BC approach\nthat combines standard TD3 [15] with BC loss minimization between policy and expert actions, and\nshowed strong performance.\nPolicy Gradient from Arbitrary Experience via DICE [38] proposes to incorporate f-divergence\nregularization into the critic function. Based on a similar concept, Conservative Q-Learning [32] and\nAdversarial Trained Actor-Critic [8] extends the critic loss with pessimistic terms that minimize the\nQ values on the samples of a learned policy and maximize the values of the dataset actions.\nMethods such as one-step RL [6], solves stitching by approximating the behavior Qβ function and\nextract the corresponding policy π by weighting actions using the advantage function Aβ(s, a) to\nfind the best action for the given state. [46]. Based on this concept, Implicit Q-Learning (IQL) [28]\napproximates the policy improvement step by treating the state value function V β(s) as a random\nvariable and taking a state-conditional upper expectile to estimate the value of the best actions in the\nstate.\nApplication of optimal transport in offline RL was wiedly explored for behavioral cloning. For\nexample it was used to construct a pseudo-reward function [11, 40, 34, 20, 9]. Primal Wasserstein\nImitation Learning [11] and Sinkhorn Imitation Learning [40] use the OT distance between the expert\nand imitator to create a reward function. Recently, the Optimal Transport Reward Labeling (OTR)\nmethod [35] was proposed to generate a reward-annotated dataset that can then be used by various\noffline RL algorithms. However, like W-BRAC, these methods primarily use OT as an additional loss\nfunction.\n7\nConclusion and Discussion\nIn this paper, we have established the novel algorithm for offline reinforcement learning. Our work\nintroduces the concept using the action-value function and the policy as components of the OT\nproblem between states and actions. To demonstrate the potential of our formulation, we propose the\npractical algorithm that penalize the policy to avoid inefficient actions provided in the dataset. By\napplying partial optimal transport, our algorithm effectively selects and maps the best expert actions\nfor each given state, ensuring efficient policy extraction from noisy or sub-optimal datasets. The\nlimitation of our method is that parameter w is task dependent.\nThis work is a step towards our larger vision towards deepening the connection between OT and\noffline RL RL. Using our formulation other OT methods also can be integrated into RL. For example,\nvarious regularizations [26, 10] and general costs [42, 2] can be used to incorporate task-specific\ninformation into the learned map, which can be particularly relevant in hierarchical RL problems [3].\nAdditionally Weak Neural OT [27] can be relevant in RL where stochastic behavior is preferred for\nexploration in the presence of multimodal goals [19].\n7.1\nReproducibility\nTo reproduce our experiment we provide source code in supplementary materials. Details on used\nhyperparameters are presented in settings (M5.2).\n8\nSocietal Impact\nThis paper presents research aimed at advancing the field of RL. While our work may have various\nsocietal implications, we do not believe any particular ones need to be specifically emphasized here.\n9\nReferences\n[1] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial\nnetworks. In International conference on machine learning, pages 214–223. PMLR, 2017.\n[2] Arip Asadulaev, Alexander Korotin, Vage Egiazarian, and Evgeny Burnaev. Neural optimal\ntransport with general cost functionals. arXiv preprint arXiv:2205.15403, 2022.\n[3] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings\nof the AAAI conference on artificial intelligence, volume 31, 2017.\n[4] Yikun Bai, Bernhard Schmitzer, Matthew Thorpe, and Soheil Kolouri. Sliced optimal partial\ntransport. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13681–13690, 2023.\n[5] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from\nwandb.com.\n[6] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without\noff-policy evaluation. Advances in neural information processing systems, 34:4933–4946, 2021.\n[7] Xi Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea\nFinn, and Chongjie Zhang. Latent-variable advantage-weighted policy optimization for offline\nrl. arXiv preprint arXiv:2203.08949, 2022.\n[8] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor\ncritic for offline reinforcement learning. In International Conference on Machine Learning,\npages 3852–3878. PMLR, 2022.\n[9] Samuel Cohen, Brandon Amos, Marc Peter Deisenroth, Mikael Henaff, Eugene Vinitsky, and\nDenis Yarats. Imitation learning from pixel observations for continuous control. 2021.\n[10] Nicolas Courty, Rémi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for\ndomain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–\n1865, 2016.\n[11] Robert Dadashi, Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein\nimitation learning. arXiv preprint arXiv:2006.04678, 2020.\n[12] Alessio Figalli. The optimal partial transport problem. Archive for rational mechanics and\nanalysis, 195(2):533–560, 2010.\n[13] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for\ndeep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[14] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\nAdvances in neural information processing systems, 34:20132–20145, 2021.\n[15] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error\nin actor-critic methods. In International conference on machine learning, pages 1587–1596.\nPMLR, 2018.\n[16] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning\nwithout exploration. In International conference on machine learning, pages 2052–2062.\nPMLR, 2019.\n[17] Milena Gazdieva, Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Extremal\ndomain translation with neural optimal transport. arXiv preprint arXiv:2301.12874, 2023.\n[18] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.\nImproved training of Wasserstein GANs. In Advances in Neural Information Processing\nSystems, pages 5767–5777, 2017.\n[19] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning\nwith deep energy-based policies. In International conference on machine learning, pages\n1352–1361. PMLR, 2017.\n10\n[20] Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and Lerrel Pinto. Watch and match: Su-\npercharging imitation with regularized optimal transport. arXiv preprint arXiv:2206.15469,\n2022.\n[21] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.\nIn Proceedings of the Nineteenth International Conference on Machine Learning, pages 267–\n274, 2002.\n[22] Leonid Kantorovitch. On the translocation of masses. Management Science, 5(1):1–4, 1958.\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[24] Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev.\nWasserstein-2 generative networks. In International Conference on Learning Representations,\n2021.\n[25] Alexander Korotin, Alexander Kolesov, and Evgeny Burnaev.\nKantorovich strikes back!\nwasserstein gans are not optimal transport? arXiv preprint arXiv:2206.07767, 2022.\n[26] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Kernel neural optimal transport.\narXiv preprint arXiv:2205.15269, 2022.\n[27] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport.\narXiv preprint arXiv:2201.12220, 2022.\n[28] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement\nlearning with fisher divergence critic regularization. In International Conference on Machine\nLearning, pages 5774–5783. PMLR, 2021.\n[29] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-\npolicy q-learning via bootstrapping error reduction. Advances in Neural Information Processing\nSystems, 32, 2019.\n[30] Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. When should we prefer offline\nreinforcement learning over behavioral cloning? arXiv preprint arXiv:2204.05618, 2022.\n[31] Aviral Kumar, Sergey Levine, Yevgen Chebotar, et al. Dasco: Dual-generator adversarial support\nconstrained offline reinforcement learning. In Advances in Neural Information Processing\nSystems, 2022.\n[32] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for\noffline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–\n1191, 2020.\n[33] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:\nTutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[34] Zhuo Li, Xu Zhou, Taixin Li, and Yang Liu. An optimal-transport-based reinforcement learning\napproach for computation offloading. In 2021 IEEE Wireless Communications and Networking\nConference (WCNC), pages 1–6. IEEE, 2021.\n[35] Yicheng Luo, Zhengyao Jiang, Samuel Cohen, Edward Grefenstette, and Marc Peter Deisenroth.\nOptimal transport for offline imitation learning. arXiv preprint arXiv:2303.13971, 2023.\n[36] Ashok Vardhan Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason D Lee. Optimal\ntransport mapping via input convex neural networks. arXiv preprint arXiv:1908.10962, 2019.\n[37] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n[38] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Al-\ngaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.\n11\n[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730–27744, 2022.\n[40] Georgios Papagiannis and Yunpeng Li. Imitation learning with sinkhorn distances. arXiv\npreprint arXiv:2008.09167, 2020.\n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in neural information processing\nsystems, 32, 2019.\n[42] François-Pierre Paty and Marco Cuturi. Regularized optimal transport is ground cost adversarial.\nIn International Conference on Machine Learning, pages 7532–7542. PMLR, 2020.\n[43] R Tyrrell Rockafellar. Integral functionals, normal integrands and measurable selections. In\nNonlinear operators and the calculus of variations, pages 157–207. Springer, 1976.\n[44] Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-\n63):94, 2015.\n[45] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.\nDeterministic policy gradient algorithms. In International conference on machine learning,\npages 387–395. PMLR, 2014.\n[46] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[47] Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the\nminimalist approach to offline reinforcement learning. arXiv preprint arXiv:2305.09836, 2023.\n[48] Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.\nCORL: Research-oriented deep offline reinforcement learning library.\nIn 3rd Offline RL\nWorkshop: Offline RL as a ”Launchpad”, 2022.\n[49] Cédric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc.,\n2003.\n[50] Cédric Villani. Optimal transport: old and new, volume 338. Springer Science & Business\nMedia, 2008.\n[51] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wo-\njciech M Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al.\nAlphastar: Mastering the real-time strategy game starcraft ii. DeepMind blog, 2, 2019.\n[52] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement\nlearning. arXiv preprint arXiv:1911.11361, 2019.\n[53] Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline\nreinforcement learning. In Conference on Robot Learning, pages 1719–1735. PMLR, 2021.\n12\n9\nAppendix\n9.1\nProofs\nProposition 9.1 (Policy Improvement with Partial Policy Learning). For any policy π, let’s define\nits performance as J(π)\ndef\n= Eπ[P∞\nt=0 γtr(st, at)] over the trajectories obtained by following the\npolicy π, (s0 ∼S, at ∼π(st), st+1 ∼P(· | st, at)). Let β be the policy of an expert and π is the\nsolution to Eq. 10 with the Qβ cost function. Then it holds that:\nJ(π) ≥J(β).\nProof. According to [21], to compare the performance of any two policies π and β we can use the\nPerformance difference lemma:\nJ(π) −J(β) =\n1\n1 −γ E\ns∼dπ\n\u0002\nAβ(s, π)\n\u0003\n(13)\nBy applying [17, Theorem 3], the solution of (10) as the parameter w →∞, converges to the\nsolution of the following problem:\nmin\n\b\nπ(·|s)⊂Supp(β(·|s))\n\t Es∼D,a∼π[−Q(s, a)].\n(14)\nHere support (Supp) is the set of points where the probability mass of the distribution lives. In our\nsettings Supp(β(·|s)) indicates the best action from β which maximizes Q.\nNow, to show the improvement over the behavior policy, let’s consider the difference between the\nbehavior policy β and the new policy π obtained after the update of β using our Eq.14, then we have:\nJ(π) −J(β) =\n1\n1 −γ E\ns∼dπ\n\u0002\nQβ(s, π) −V β(s)\n\u0003\n=\n(15)\n1\n1 −γ E\ns∼dπ[Qβ(s,\nmax\na⊂Supp(β(·|s))[Qβ(s, a)]) −Ea∼β(s)[Qβ(s, a)]] ≥0\n(16)\nWhere in first we used the definition of π, and then noted that the maximum over the all actions given\nby the expert is always greater than the average over some actions from the experts policy β. □\nThis result is analogous to the classic policy iteration improvements proof. The difference is that we\ntakes max over the Supp(β(·|s)) rather than max over the all possible actions A.\n9.2\nAdditional Illustrations\nFor completeness, we provide an additional illustration of the results of the proposed method.\nSpecifically, we show the scores for the one-step settings Table 4 and in this subsection we show the\nnormalized score curves of the CQL and our method during training Figure 3.\nTable 4: Averaged normalized scores on MuJoCo tasks. Results of the final 10 evaluations and 5\nrandom seeds.\nDataset\nOneStep-RL\nPPL\nMedium\nHalfCheetah\n48.4\n51.4±0.2\nHopper\n59.6\n80.4±7.4\nWalker\n81.1\n84.3±0.6\nMedium-\nReplay\nHalfCheetah\n38.1\n44.8±0.5\nHopper\n97.5\n92.1±10.8\nWalker\n49.5\n86.6±4.8\nMedium-\nExpert\nHalfCheetah\n93.4\n74.4±17.0\nHopper\n103.3\n110.2±1.9\nWalker\n113.0\n111.2±0.7\nParameters settings As mentioned, we considered different values of w for the ReBRAC-based\nmethod. The parameters can be seen in the Table 5. For most tasks, higher values of w are preferable,\nwhich means that some sort of suboptimal trajectories are present in the datasets.\n13\n(a) Umaze-diverse\n(b) Medium-play\n(c) Medium-diverse\n(d) Large-play\n(e) Large-diverse\nFigure 3: Normalized score curves on the Antmaze tasks, IPLCQL algorithm is blue, CQL is red\nw\nEnvironments\n8\nhopper-medium, pen-cloned, relocate-cloned,\nrelocate-expert, antmaze-large-diverse,\nhalfcheetah-medium, walker-medium,\nhalfcheetah-medium-replay, hopper-medium-replay,\nwalker-medium-replay, walker-medium-expert,\numaze, door-cloned, door-expert, hammer-cloned\n12\nhopper-medium, pen-cloned, relocate-cloned,\nrelocate-expert, antmaze-large-diverse\n5\nantmaze-large-play, antmaze-medium-play,\nantmaze-medium-diverse, pen-expert, hammer-expert\n3\numaze-diverse, pen-human, relocate-human\nTable 5: Optimal w values for different environments\n14\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: For each contribution listed in abstract and introduction we provide the links\nto the sections about them.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We discuss the limitations of our solver in Appenix 7.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: We provide the proofs and the assumptions in Appendix ??.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide a full list of the experimental details. We use publicly available\ndatasets. The code for the experiments is provided in supplementary material.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: The code for our solver is included in supplementary material and will be\nmade public after acceptance of our paper. We use publicly available datasets.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We provide the experimental details in Experiments section5.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: We have provided comparisons in tables on the range of different environments,\nwhich is usually sufficient for comparing algorithms in the offline RL domain.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\n15\nJustification: The computational resources are discussed in results section.\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The research conforms with NeurIPS Code of Ethics. We discuss the societal\nimpact of our paper in Appendix ??.\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We discuss the societal impact of our paper in 8.\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The research does not release data or models that have a high risk for misuse\nand does not need safeguards.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We cited the creators all of the used assets.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: The code is included in supplementary material. The license for the code will\nbe provided after the paper acceptance.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not include crowdsourcing experiments or research with human\nsubjects.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not include crowdsourcing experiments or research with human\nsubjects.\n16\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-17",
  "updated": "2024-10-17"
}