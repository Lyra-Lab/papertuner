{
  "id": "http://arxiv.org/abs/2410.14135v1",
  "title": "Inverse Reinforcement Learning from Non-Stationary Learning Agents",
  "authors": [
    "Kavinayan P. Sivakumar",
    "Yi Shen",
    "Zachary Bell",
    "Scott Nivison",
    "Boyuan Chen",
    "Michael M. Zavlanos"
  ],
  "abstract": "In this paper, we study an inverse reinforcement learning problem that\ninvolves learning the reward function of a learning agent using trajectory data\ncollected while this agent is learning its optimal policy. To address this\nproblem, we propose an inverse reinforcement learning method that allows us to\nestimate the policy parameters of the learning agent which can then be used to\nestimate its reward function. Our method relies on a new variant of the\nbehavior cloning algorithm, which we call bundle behavior cloning, and uses a\nsmall number of trajectories generated by the learning agent's policy at\ndifferent points in time to learn a set of policies that match the distribution\nof actions observed in the sampled trajectories. We then use the cloned\npolicies to train a neural network model that estimates the reward function of\nthe learning agent. We provide a theoretical analysis to show a complexity\nresult on bound guarantees for our method that beats standard behavior cloning\nas well as numerical experiments for a reinforcement learning problem that\nvalidate the proposed method.",
  "text": "Inverse Reinforcement Learning from Non-Stationary Learning Agents\nKavinayan P. Sivakumar, Yi Shen, Zachary Bell, Scott Nivison, Boyuan Chen and Michael M. Zavlanos\nAbstract— In this paper, we study an inverse reinforcement\nlearning problem that involves learning the reward function\nof a learning agent using trajectory data collected while this\nagent is learning its optimal policy. To address this problem, we\npropose an inverse reinforcement learning method that allows\nus to estimate the policy parameters of the learning agent which\ncan then be used to estimate its reward function. Our method\nrelies on a new variant of the behavior cloning algorithm, which\nwe call bundle behavior cloning, and uses a small number\nof trajectories generated by the learning agent’s policy at\ndifferent points in time to learn a set of policies that match\nthe distribution of actions observed in the sampled trajectories.\nWe then use the cloned policies to train a neural network model\nthat estimates the reward function of the learning agent. We\nprovide a theoretical analysis to show a complexity result on\nbound guarantees for our method that beats standard behavior\ncloning as well as numerical experiments for a reinforcement\nlearning problem that validate the proposed method.\nI. INTRODUCTION\nUnderstanding the intentions of autonomous agents [1] has\nimportant implications in a variety of applications, ranging\nfrom cyber-physical systems [2] to strategic games [3], [4].\nIf an agent follows a fixed policy, its intentions remain\nconsistent and can be learned by sampling a sufficiently\nlarge number of state-action pairs [5], [6]. However, in\npractice, sampling enough trajectories to learn a fixed optimal\npolicy can be time consuming. Moreover, from an observer’s\nperspective, it is not always easy to determine whether the\nobserved agent’s policy is fixed or not. Therefore, it is\nimportant to be able to estimate an agent’s intentions while\nthis agent is interacting with its environment to learn its\noptimal policy and before the policy has converged.\nDoing so using only state-action data is not straightfor-\nward. A common way to predict the intentions of an agent in\ndeep reinforcement learning is assuming the policy weights\nare known. If the policy weights are known during learning,\nthe distribution of agent actions at any given state can also be\nestimated [7]. Nevertheless, assuming that the structure of the\nagent’s policy is known is not ideal in practice [8]. Inverse\nreinforcement learning (IRL) instead aims to understand an\nagent’s intentions by learning their reward function [9]. A\ncommon approach in existing IRL literature is to use the\nmaximum entropy to select a distribution of actions at a\nstate that matches an expected distribution given observations\nof state-action pairs. By maximizing the entropy, one can\naccount for all the sampled distributions of state-action pairs\nat highly visited states, thus making it more reliable to\nrecover the reward function as no additional assumptions are\nmade on the behavior at less visited states [10], [11].\nAn important common assumption in the IRL literature\ndiscussed above is that the trajectories used to learn the\nreward functions are generated from a stationary expert\npolicy. This assumption does not hold when trajectory data\nare collected as the agent interacts with its environment to\nlearn its optimal policy. In this case, existing IRL methods\nare not straightforward to apply. To address this challenge,\n[12] proposes a way to learn an agent’s reward function from\ndata generated by this agent’s non-expert policy while this\npolicy is being updated and assuming that policy updates\nare done using stochastic gradient descent. However, the\ntheoretical analysis of the approach in [12] requires that\nthe reward function of the learner agent is approximated\nby a set of linear feature functions. In practice, selecting\nan expressive enough set of feature functions is challenging.\nFor example, even in the case of radial basis functions that\nare commonly used as feature functions, choosing proper\ncenters and bandwidth parameters is not straightforward [13].\nIn IRL, these parameters must be chosen to ensure that each\nstate action pair has a unique feature and that no state action\npairs are favored over others. If these criteria are not met,\nthe learned reward function may be skewed towards the state\naction pairs that are favored more.\nMotivated by the approach in [12], in this paper we\npropose an IRL method to learn the reward function of a\nlearning agent as it learns its optimal policy. As in [12],\nwe assume that the agent updates its policy using stochastic\ngradient descent, but we do not assume that the agent’s\nreward function is approximated by linear feature functions;\nit can be approximated by any nonlinear function, e.g., a\nneural network. Since the agent does not share its policy\nparameters explicitly, the challenge lies in estimating the\nagent’s policy parameters as they are being updated during\nlearning. To do so, we propose a new variant of behavior\ncloning, which we term bundle behavior cloning, that uses a\nsmall number of trajectories generated by the learner agent’s\npolicy at different points in time to learn a set of policies for\nthis agent that match the distribution of actions observed in\nthe sampled trajectories. These cloned policies are then used\nto train a neural network model that estimates the reward\nfunction of the learning agent. We provide error bounds of\nthe policies learned by bundle behavior cloning and standard\nbehavior cloning that allow us to choose an appropriate\nbundle size which validates bundle behavior cloning as a\nsuperior method for the problem we discuss. Moreover, we\npresent numerical experiments on simple motion planning\nproblems that show that the proposed approach is effective\nin learning the reward function of the learning agent. In\ncomparison, empirical results presented in [12] assume that\nthe agent’s policy parameters are known so that behavior\ncloning is not needed to estimate them.\narXiv:2410.14135v1  [cs.LG]  18 Oct 2024\nThe rest of the paper is organized as follows. In Section\nII, we formulate the proposed IRL problem and introduce\nsome preliminaries. In Section III, we develop our proposed\nalgorithm. In section IV we present theoretical results that\nsupport our proposed method. Finally, in Section V, we\nnumerically validate our method on simple motion planning\nproblems.\nII. PROBLEM DEFINITION\nConsider an agent with state space S and action space A.\nWe model this system as a finite Markov Decision Process\n(MDP), defined as M = (S, A, P, γ, R), where P(St+1 =\nst+1|St = st, At = at) is the transition function defined over\nthe agent’s state st ∈S and action at ∈A, γ ∈(0, 1] is the\ndiscount factor, and R(st, at, st+1) is the reward received\nwhen the agent transitions from state st to state st+1 using\nthe action at. Let also πθ(at|st) →[0, 1], denote the agent\npolicy, which is the probability of choosing an action at at\nstate st, and is parameterized by the policy parameter θ. The\nobjective of the agent is to maximize its accumulated reward,\ni.e.,\nmax\nθ\nJ(θ) =\nE\ns0∼ρu[\nT\nX\nt=0\nγtR(st, at, st+1)],\n(1)\nwhere ρu is the initial state distribution of the agent. In what\nfollows, we assume that the agent does not share its policy\nparameters or rewards. Then, in this paper, we address the\nfollowing problem:\nProblem 1: (Inverse reinforcement learning from a learn-\ning\nagent)\nLearn\nthe\nreward\nfunction\nof\na\nlearning\nagent using only trajectories of state action pairs τ\n=\n[(s0, a0), ..., (sT , aT )] generated as the agent interacts with\nits environment to learn its optimal policy πθ that solves (1).\nTo solve Problem 1 we make the following assumption.\nAssumption 1: The agent updates its policy using the\nstochastic gradient descent (SGD) update θt+1 = θt+α∇θJ.\nWe note that many popular policy gradient methods use\nstochastic gradient descent, e.g., REINFORCE, Actor-Critic,\netc. Specifically, in this paper, we use the REINFORCE\nalgorithm to update the policy parameters of agent i as\nθt+1 = θt + αγtR(st+1)∇ln π(at|st, θ),\n(2)\nwhere θ is the policy parameter of the agent, α is the learning\nrate, and we denote R(st+1) = R(st, at, st+1). We call\nthis the forward learning problem and make the following\nassumption on the learning rate.\nAssumption 2: The learning rate of the agent α is known\nand is sufficiently small.\nFinally, we make the following assumption on the infor-\nmation that is available in order to learn the reward of the\nagent.\nAssumption 3: All states and actions of the agent in the\nenvironment can be observed.\nThe main challenge with solving Problem 1 is that without\nthe policy parameters of the agent at every timestep, it is\ndifficult to recover R(st+1). In the next section, we detail\nhow we can recover R(st+1) using IRL and a new variant\nof behavior cloning that we propose, that we call bundle\nbehavior cloning, to estimate the agent’s policy.\nIII. METHOD\nAssuming it is known that the learning agent uses the\nupdate (2) to update its policy parameters, to recover the\nreward at a state st+1 we need to know the policy parameters\nθt and θt+1, as well as ∇ln π(at|st, θ). As the policy\nstructure, e.g., the structure of the policy neural network,\nand policy weights of the agent are considered unknown, it\nis not straightforward to estimate these policy parameters, or\nthe gradient of the objective function. Here, we propose a\nnovel variant of behavior cloning to learn these quantities,\nusing only trajectory data collected during learning.\nBehavior cloning is a supervised learning method used to\nlearn a policy πθ from a set of trajectories {τ} sampled\nfrom an expert policy. A number of applications ranging\nfrom autonomous driving [14], video games [15], and traffic\ncontrol [16] have relied on behavior cloning to learn the\ndesired expert policies. Generally, the assumption in these\nworks is that the expert policy used to generate the data is\nstationary. However, if the expert policy changes as in this\npaper, it is not easy to assign trajectory data {τ} consisting of\nstate and action pairs τ = {(s0, a0), (s1, a1), ..., (sT , aT )},\nto specific policies πθ. We address this challenge by instead\nbundling together trajectories and learning a single cloned\npolicy for each bundle. Given Assumption 2, if the learning\nrate α is small enough, then consecutive policy parameters\nθt+1 and θt will be close to each other. Therefore, for an\nappropriate bundle size, it is reasonable to expect that the\ntrajectories in each bundle are generated by approximately\nthe same policy. We call the proposed method Bundle\nBehavior Cloning, which we describe below.\nWe denote the cloned policy corresponding to the kth\nbundle bk in the set [b1, ..., bM] by πk\nψ where ψ is the\npolicy parameter. We assume that every bundle contains\nB trajectories. Given a total number of training episodes\nE from forward learning, the total number of bundles is\ndefined as M = E −B + 1 if a sliding window is used\nto construct the bundles, or M = E/B rounded up to the\nnearest integer for non-overlapping sets of episodes. Here,\nthe size of each bundle B constitutes a hyperparameter. For\neach bundle bk consisting of B trajectories, our goal is to\ngenerate a distribution of the agent’s actions for each state\ndenoted by ρbk(s) that approximates the distribution of the\nagent’s actions in the policy we wish to clone. Then, to train\nthe cloned policy we rely on the Mean Squared Error (MSE)\nloss\nloss(ψk) =\nX\ns∈S\nMSE(πk\nψ, ρbk(s)).\n(3)\nThe proposed bundle behavior cloning method is outlined in\nAlgorithm 1. Specifically, line 5 in Algorithm 1 adds the loss\nover all the states in the state space so that the kth cloned\npolicy parameter ψk can accurately imitate the true agent\nAlgorithm 1: Bundle Behavior Cloning\ninput : List of trajectories of state-action pairs τ\nfrom entire learning, size of each bundle B,\nmean-squared error loss function MSE,\nuninitialized policy ˜πψ\noutput: Set of cloned policies {˜π1\nψ, ˜π2\nψ, ... ˜πM\nψ }\n1 for bundle index k from [1 : M] do\n2\nInitialize ˜πk\nψ = ˜πk−1\nψ\nor ˜πk\nψ = ˜πψ if k = 1;\n3\nDefine bk as the trajectory set of episode indices\nfrom (k −1)B (inclusive) to kB (exclusive);\n4\nCalculate sampled distribution ρbk(s) of agent\nactions at state s for bundle of trajectories bk\nfor each s ∈S;\n5\nCalculate the loss using (3);\n6\nUpdate policy ˜πk\nψ using backpropagation;\nFig. 1.\nA visual illustration of Bundle Behavior Cloning. (a) Set of\ntrajectories {τ} from total number of E episodes during the forward step.\n(b) The first trajectory τ0 containing state action pairs for T timesteps. (c)\nThe last trajectory τE containing state action pairs for the last episode of\nlearning. (d) The bundle bk of M trajectories that are used to clone for a\npolicy ˜πk\nψ. (e) The bundle of trajectories bk are sampled to get a distribution\nof individual agent actions per state s, ρbk(s).\npolicy parameters θ during the episodes. Fig. 1 illustrates\nBundle Behavior Cloning visually.\nUsing bundle behavior cloning, we can estimate the\nagent’s policy parameters θ at various points in time during\nlearning, which we can use to further estimate the gradient\n∇ψ ˜J using the gradient of the cloned policy associated with\nbundle bk. Given the cloned policy parameters and gradients,\nwe can train a neural network β(st+1) to approximate the\nagent’s reward function using the loss function obtained from\n(2):\nloss =\nX\nZ\n(β(st+1)∇πk\nψ(st, at)γt −(ψk+1 −ψk)),\n(4)\nwhere Z is the batch size of existing state action pairs from\nthe forward case, ∇πk\nψ(at|st) is the estimated gradient of\nthe cloned policy from the kth bundle corresponding to the\nstate action pair (st, at), and (ψk+1 −ψk) is the difference\nin policy parameters between the kth bundle and the k + 1\nbundle. This loss is backpropagated through β to predict\nrewards at states st+1 that will minimize the loss. The full\nalgorithm for training β is seen in Algorithm 2 and the\ncomplete pipeline detailing all the steps is shown in Fig.\n2.\nAlgorithm 2: Learning REINFORCE Reward Func-\ntion of Agent\ninput : List of trajectories of state-action pairs τ\nfrom entire learning, batch size Z, number\nof training episodes F, set of cloned policies\nof agent’s true policies {˜π1\nψ, ˜π2\nψ, ... ˜πM\nψ }\noutput: Our neural network estimate of the agent’s\nreward function β(s)\n7 for f in F do\n8\nfor z in Z do\n9\nSample a state s from τ;\n10\nCalculate the loss using (4) at s and add it to\noverall loss;\n11\nUpdate policy β using backpropagation;\nIV. COMPLEXITY ANALYSIS\nIn this section, we provide a sample complexity result and\nshow that under certain conditions bundle behavior cloning\nachieves tighter complexity bounds than conventional behav-\nior cloning. Specifically, given a set of trajectories of state-\naction pairs {τ 1, ..., τ B}, where τ i represents the trajectory\nunder the fixed policy πi, we show that bundle behavior\ncloning can be used to estimate the true policy π1(·|s) of\nthe agent at time step 1 using ˆπ1:B(·|s), the estimated policy\nthat best fits the distribution sampled from the bundle of\ntrajectories {τ 1, ..., τ B} from timestep 1 to timestep BT and\nT is the number of samples in one trajectory. Note that we\nonly provide the analysis for π1 and the results below hold\nfor all time steps t.\nObserve that if the polices in the bundle are significantly\ndifferent from each other, we do not expect a larger bundle\nsize to facilitate policy estimation. As a result, we make the\nfollowing assumptions:\nAssumption 4: For every consecutive policy pair, there\nexists a constant ϵ such that for any state s, the following\ninequality holds:\n||πt(s) −πt+1(s)||tv ≤ϵ,\nwhere the total variation norm between two distributions µ\nand ν is defined as TV (µ, ν) := sup\nA∈B\n|µ(A) −ν(A)| [17],\nwhere B denotes the class of Borel sets.\nThe state visitation dπ for a fixed policy π is defined as\ndπ = (1 −γ) P∞\nt=0 γtPrπ(st = s), where Prπ(st = s) is\nthe probability that st = s after starting at state s0 and\nfollowing π thereafter. The single policy behavior cloning\nsample complexity has been studied in [18] and is presented\nbelow.\nLemma 1 (Theorem 21 in [18]): With probability at least\n1 −δ, we have that\nEs∼dπ||ˆπ(·|s) −π(·|s)||2\ntv ≤2ln(|Π|/δ)\nT\n,\nFig. 2.\nThe full pipeline of the algorithm presented in this paper. Note that the learner does not need to finish optimizing its policy before its reward\nfunction can be estimated using the algorithm.\nwhere T is the number of samples of state action pairs from a\nsingle trajectory, Π is the policy class Π = {π : S →∆(A)}\nwhich is discrete with size |Π|.\nBefore applying Lemma 1 to τ 1 (standard behavior\ncloning) and {τ 1, · · · , τ B} (bundle behavior cloning), we\nassume the policies that generate {τ 1, · · · , τ B} are indepen-\ndent. Then, defining the resultant cloned policies for both sets\nof trajectories ˆπ1(·|s) and ˆπ1:B(·|s) respectively, we have\nthat, with probability 1 −δ:\nEs∼dπ1||ˆπ1(·|s) −π1(·|s)||2\ntv ≤2ln(|Π|/δ)\nT\n(5)\nEs∼dπ1:B ||ˆπ1:B(·|s) −π1:B(·|s)||2\ntv ≤2ln(|Π|/δ)\nBT\n.\n(6)\nIn what follows, our goal is to provide a bound on\nEs∼dπ1||ˆπ1:B(·|s) −π1(·|s)||2\ntv, which measures the differ-\nence between the policy returned by bundle behavior cloning\nand the true policy π1. Note that, this expectation is taken\nwith respect to the state visitation of π1, which is different\nfrom (6). To address this challenge, we first present a\nresult from [19] that shows that the difference between the\ndistributions dπ1 and dπ1:B can be bounded.\nLemma 2 (Lemma 3 in [19]): The\ndivergence\nbetween\ndiscounted state visitation distribution is bounded by an\naverage divergence of the policies:\n||dπ′ −dπ||1 ≤\n2γ\n1 −γ Es∼dπ||π′(·|s) −π(·|s)||tv,\n(7)\nwhere γ is the discount factor and dπ′, dπ are two state\nvisitation distributions associated with π′(·|s), π(·|s) respec-\ntively.\nBefore proving our main theorem, we provide some prop-\nerties of the policies within the bundle.\nLemma 3: Let Assumption 4 hold. For any two policies\nπi, πj ∈{π1, ..., πB}, we have that:\n||πi −πj||tv ≤(B −1)ϵ\n||dπi −dπj||1 ≤2γ(B −1)ϵ\n1 −γ\nProof:\nThe first property follows from the triangle\ninequality of total variations. According to Lemma 3, we\nhave\n\r\r\rdπi −dπj\r\r\r\n1 ≤\n2γ\n1−γ Es∼dπj\n\r\rπi(·|s) −πj(·|s)\n\r\r\ntv ≤\n2γ\n1−γ Es∼dπj (B −1)ϵ = 2γ(B−1)ϵ\n1−γ\n.\nCorollary 1: Policies π1 and π1:B satisfy the inequalities\nin Lemma 3.\nProof:\nWe use the definition π1:B =\n1\nB (PB\ni=1 πi)\nand substitute this into the left hand side norm of the\nfirst equation in Lemma 3 to get ||π1 −\n1\nB\nPB\ni=1 πi|| =\n1\nB (PB\ni=2 ||π1−πi||). Using Lemma 3 we can obtain an upper\nbound as ||π1 −π1:B||tv ≤1\nB (PB\ni=2 |i −1|)ϵ. Since |i −1|\nis upper bounded by B −1, we further get ||π1 −π1:B||tv ≤\n1\nB (PB\ni=2 B −1)ϵ =\n(B−1)2\nB\nϵ < (B −1)ϵ. We can then\nsubstitute this into Lemma 2 and obtain ||dπ1 −dπ1:B||1 ≤\n2γ(B−1)ϵ\n1−γ\n.\nWe provide the sample complexity for bundle behavior\ncloning below.\nTheorem 1: With probability at least (1 −δ)2, we have\nthat\nEs∼dπ1||ˆπ1:B(·|s) −π1(·|s)||2\ntv\n≤4γ(B −1)ϵ\n1 −γ\n+ 4ln(|Π|/δ)\nBT\n+ 2ϵ2(B −1)2.\n(8)\nProof: For simplicity of notation we hereby define ρi =\nπi(·|s). First we rewrite the expectation on the left hand side\nof (8) as Es∼dπ1||ˆρB −ρB +ρB −ρ1||2\ntv. Using the inequality\n||x−z||2 ≤2||x−y||2+2||y−z||2 we obtain an upper bound\non the difference between the distribution of the true policy\nand that of the policy cloned from bundle behavior cloning\nas:\nEs∼dπ1||ˆρB −ρB + ρB −ρ1||2\ntv ≤\n2Es∼dπ1||ˆρB −ρB||2\ntv + 2Es∼dπ1||ρB −ρ1||2\ntv.\nThe right hand bound can then be transformed as:\n2Es∼dπ1||ˆρB −ρB||2\ntv −2Es∼dπB ||ˆρB −ρB||2\ntv\n+2Es∼dπB ||ˆρB −ρB||2\ntv + 2Es∼dπ1||ρB −ρ1||2\ntv.\n(9)\nUsing Corollary 1 we obtain an upper bound on (9):\n≤2[Es∼dπ1||ˆρB −ρB||2\ntv −Es∼dπB ||ˆρB −ρB||2\ntv]\n+ 4ln(|Π|/δ)\nBT\n+ 2ϵ2(B −1)2\n≤2[\nX\ns\n(Pdπ1(s) −PdπB (s))||ˆρB −ρB||2\ntv]\n+ 4ln(|Π|/δ)\nBT\n+ 2ϵ2(B −1)2\n≤2[\nX\ns\n|Pdπ1(s) −PdπB (s)|] + 4ln(|Π|/δ)\nBT\n+ 2ϵ2(B −1)2.\nUsing Lemma 2, we further have that:\n≤4γ(B −1)ϵ\n1 −γ\n+ 4ln(|Π|/δ)\nBT\n+ 2ϵ2(B −1)2\n(10)\nwhich completes the proof\nNote that with the same probability (1−δ)2, we have with\nprobability at least (1 −δ)2, Es∼dπ1||ˆπ1(·|s) −π1(·|s)||2\ntv ≤\nFig. 3.\nColormaps representing the true and learned reward functions\nusing Algorithm 2 with Bundle Behavior Cloning (a) Agent’s normalized,\ntrue reward function. (b) Agent’s normalized, learned reward function with\nthe same neural network structure.\n2ln(|Π|/(2δ−δ2))\nT\naccording to (5). Note also that there exists\nan optimal B that minimizes the upper bound in (10). If the\nminimum value of (10) is strictly less than 2ln(|Π|/(2δ−δ2))\nT\nas\nderived from (5), then Theorem 1 guarantees that the policy\nlearned from the bundle achieves a tighter bound on the dif-\nference between the distribution of the cloned policy and that\nof the true policy than the same bound achieved in standard\nbehavior cloning. Of course, the minimum value of the bound\nin (10) also depends on ϵ, so bundle behavior cloning may\nperform worse than conventional behavior cloning if ϵ is\nlarge.\nV. EXPERIMENTAL RESULTS\nA. Testing Environment\nIn this section, we demonstrate the proposed IRL algo-\nrithm on a Gridworld environment, where the learning agent\nseeks to reach a goal state. The environment is a 7 by 7\ngrid, with the agent starting at the top left square. It has\nthree possible actions, moving to the right, moving to the\nleft, and moving down. The agent receives the maximum\nreward of 20 when it reaches the goal state, located at the\nbottom right square in the grid, but different squares on the\ngrid result in different, negative rewards for the agent ranging\nfrom −5 to −2. All experiments are run using PyTorch [20]\non a Windows system with an RTX 3080 GPU.\nB. Learning Rewards\nWe use our proposed IRL method with bundle behavior\ncloning to learn the reward function of the agent. The forward\npolicy is modeled by a 2 layer neural network with 16 hidden\nnodes and ReLU activation functions on the first layer. The\nneural network used to clone the policies is the same as that\nin the forward case. The bundle has size B = 15. There\nare M = 333 total bundles defined across 5000 trajectories\nand each trajectory consists of 15 timesteps. As a result,\neach bundle contains 225 state action pairs. The learning\nTABLE I\nAVERAGED REWARDS FROM LEARNED POLICY\nDescription\nAverage Reward\nForward Policy: 2 layer, 16 hidden nodes\n57.461 ± 4.199\nBBC: 2 layer, 16 hidden nodes (same structure)\n57.840 ± 0.946\nBBC: 2 layer, 16 hidden nodes, independent policies\n54.551 ± 1.777\nBBC: 2 layer, 8 hidden nodes\n57.923 ± 1.248\nBBC: 2 layer, 24 hidden nodes\n57.912 ± 1.262\nBBC: 2 layer, 32 hidden nodes\n46.855 ± 3.865\nBBC: 3 layer, 16 hidden nodes\n52.190 ± 3.037\nBBC: 5 layer, 16 hidden nodes\n54.934 ± 1.304\nrate used during training of the forward and cloned policies\nis 0.00075 and γ is set to 0.999. The neural network used\nto model the estimated reward β is a 2 layer neural network\nwith 20 hidden nodes with a ReLU activation function on\nthe first layer. The batch size used to train β is 100 and the\ntotal number of training episodes to is 5000.\nAs neural networks’ weights are trained stochastically\n[21], we use only the last layer of weights from the cloned\npolicies to train β in order to minimize the variance in\nestimated gradients from layers before the output layer.\nThe last layer in a neural network is used to output the\ndistribution of actions, and thereby we can minimize the\neffect of multiple combinations of neural network weights\nthat could output the same estimated distribution.\nTo compare the reward β learned using bundle behavior\ncloning and the proposed IRL method to the true agent\nreward, we first normalize and scale the reward β. Fig. 3\nshows the comparison results on a normalized color map.\nThe recovered reward function is able to localize the goal\nstate in the environment as well as identify states on the\noptimal path.\nTo further validate the predicted rewards, we use them to\ntrain a new policy which we then compare to the optimal\nforward policy learned from the true reward function. The\naverage rewards and standard deviations of these forward and\ninverse learned policies are shown in Table I. The learned\npolicy is able to produce the same optimal trajectory as\nthe forward optimal policy and achieves the same possible\nreward.\nIn Fig. 4 we train the neural network β 10 times. We\nnormalize and scale the results as before, and present the\nlower and upper bound reward functions for a 95% con-\nfidence interval. In Fig. 4a we see the state distribution\nfrom the forward learning trajectories. States that are visited\nmore have a smaller range of estimated rewards within this\nconfidence interval, whereas states like the bottom left corner\nhave the largest range.\nC. Independent Policies\nIn the forward case, future policy weights depend on pre-\nvious ones, so the assumption that the policies that generate\n{τ 1, · · · , τ B} are independent in the theoretical analysis in\nSection IV may not necessarily hold in practice. Simulating\nindependent policies is possible as shown in [22], [23], and\nwe do so by doubling the size of the bundle but skipping\nevery other state action pair during sampling. As a result,\nFig. 4.\nThe state distribution in the forward case can affect the variance in reward estimation. (a) The state distribution of the forward trajectories in\npercentages. (b) Lower bound and (c) upper bound of predicted scaled, normalized rewards for 95% confidence measure. States with higher percentages\nof visitation correlate with less variance in estimated rewards.\nthe number of state action pairs per bundle is kept constant\nand subsequent policies are less dependent on each other.\nThis sampling strategy still allows to reach the optimal goal\nposition as seen in the first part of Table I.\nD. Testing Different Neural Network Structures\nWe test different neural network structures to measure\nrobustness of bundle behavior cloning to different network\nstructures when the true neural network structure of the\nagent’s forward policy is not known. The second part of\nTable I shows the average rewards of the learned policy for\ndifferent neural network structures. In our experiments, we\nassume we know the activation functions used in between\nlayers (ReLU). We leave the analysis of the impact of\ndifferent activation functions on the performance of the\nalgorithm to future work. We observe that it is possible\nto still learn the reward function of the agent even if a\ndifferent neural network structure is used, compared to that\nof the agent’s forward policy. Nevertheless, large deviations\nbetween the learnt and true policy networks have an effect on\nthe reward estimation; the 32 hidden nodes network, results\nin a larger error in reward estimation.\nFig. 5.\nOnly using the first 50 bundles (750 episodes) to learn the reward\nfunction.\nE. Learning While Learning\nIn this experiment, we test the ability of our method to\nlearn the reward function of the learning agent while it is\nFig. 6.\nWe see that the higher the bundle size B is, the faster the last\nlayer of weights in the cloned policies change across bundles. We’ve chosen\nB = 15 in our experiments as a good balance between allowing enough\nsamples for each bundle but also keeping Assumption 4.\ninteracting with its environment to learn its optimal policy.\nSpecifically, we test our method with just the first 50 bundles\nrather than the full set of 333 bundles. In Fig. 5, we show\nthe recovered reward function using these 50 bundles, which\ncompose only 750 episodes from the forward learning. We\nsee that with less than a sixth of the total episodes from\nthe forward learning, we can start to recover the shape of\nthe reward function. We conclude that, compared to regular\nbehavior cloning that requires an optimal policy to imitate,\nbundle behavior cloning does not.\nF. Deviation of Cloned Policies in Between Bundles\nFinally, we empirically test the effect of the bundle choice\non the performance of the proposed method. Fig. 6 shows the\nnorm of the last layer weights of the cloned policies across\nbundles for different bundle sizes. As the bundle size gets\nlarger, the weights of the last layer change much faster across\nthe bundles, which may violate Assumption 4. Although the\nnorm of the weights of a neural network is not a substitute\nfor the distribution of the policy itself, this graph does show\nthat adjusting bundle size B can help satisfy the theoretical\nrequirement for ϵ in Assumption 4 as it results in policy\nnetworks that are closer to each other across consecutive\nbundles.\nREFERENCES\n[1] N. C. Rabinowitz, F. Perbet, H. F. Song, C. Zhang, S. M. A.\nEslami, and M. Botvinick, “Machine theory of mind,” CoRR, vol.\nabs/1802.07740, 2018. [Online]. Available: http://arxiv.org/abs/1802.\n07740\n[2] K. Zhang, Z. Yang, and T. Basar, “Multi-agent reinforcement\nlearning: A selective overview of theories and algorithms,” CoRR,\nvol. abs/1911.10635, 2019. [Online]. Available: http://arxiv.org/abs/\n1911.10635\n[3] D. Xie and X. Zhong, “Deep deterministic policy gradients with\ntransfer learning framework in starcraft micromanagement,” in 2019\nIEEE International Conference on Electro Information Technology\n(EIT), 2019, pp. 410–415.\n[4] N. K. Adhikari, S. J. Louis, and S. Liu, “Multi-objective cooperative\nco-evolution of micro for rts games,” in 2019 IEEE Congress on\nEvolutionary Computation (CEC), 2019, pp. 482–489.\n[5] A. Y. Ng and S. J. Russell, “Algorithms for inverse reinforcement\nlearning,” in Proceedings of the Seventeenth International Conference\non Machine Learning, ser. ICML ’00.\nSan Francisco, CA, USA:\nMorgan Kaufmann Publishers Inc., 2000, p. 663–670.\n[6] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and\nJ. Peters, “An algorithmic perspective on imitation learning,” CoRR,\nvol. abs/1811.06711, 2018. [Online]. Available: http://arxiv.org/abs/\n1811.06711\n[7] Y. Li and C. Tan, “A survey of the consensus for multi-agent\nsystems,” Systems Science & Control Engineering, vol. 7, no. 1, pp.\n468–482, 2019. [Online]. Available: https://doi.org/10.1080/21642583.\n2019.1695689\n[8] A. Servin and D. Kudenko, Multi-agent Reinforcement Learning for\nIntrusion Detection, 02 2008, pp. 211–223.\n[9] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse\nreinforcement\nlearning,”\nin\nProceedings\nof\nthe\nTwenty-First\nInternational Conference on Machine Learning, ser. ICML ’04.\nNew York, NY, USA: Association for Computing Machinery, 2004,\np. 1. [Online]. Available: https://doi.org/10.1145/1015330.1015430\n[10] D. Li and J. Du, “Maximum entropy inverse reinforcement learning\nbased on behavior cloning of expert examples,” in 2021 IEEE 10th\nData Driven Control and Learning Systems Conference (DDCLS),\n2021, pp. 996–1000.\n[11] A. J. Snoswell, S. P. N. Singh, and N. Ye, “Revisiting maximum\nentropy inverse reinforcement learning: New perspectives and algo-\nrithms,” in 2020 IEEE Symposium Series on Computational Intelli-\ngence (SSCI), 2020, pp. 241–249.\n[12] G. Ramponi, G. Drappo, and M. Restelli, “Inverse reinforcement learn-\ning from a gradient-based learner,” in Advances in Neural Information\nProcessing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F.\nBalcan, and H. Lin, Eds., vol. 33.\nCurran Associates, Inc., 2020, pp.\n2458–2468.\n[13] N. Jafarpisheh, E. Jalaeian, M. Teshnehlab, H. Karimipour, R. Parizi,\nand G. Srivastava, “A deep neural network combined with radial\nbasis function for abnormality classification,” Mobile Networks and\nApplications, pp. 1–11, 09 2021.\n[14] W. Farag and Z. Saleh, “Behavior cloning for autonomous driving\nusing convolutional neural networks,” in 2018 International Confer-\nence on Innovation and Intelligence for Informatics, Computing, and\nTechnologies (3ICT), 2018, pp. 1–7.\n[15] B. Chen, S. Tandon, D. Gorsich, A. Gorodetsky, and S. Veerapaneni,\n“Behavioral cloning in atari games using a combined variational\nautoencoder and predictor model,” in 2021 IEEE Congress on Evolu-\ntionary Computation (CEC), 2021, pp. 2077–2084.\n[16] X. Li, P. Ye, J. Jin, F. Zhu, and F.-Y. Wang, “Data augmented deep\nbehavioral cloning for urban traffic control operations under a parallel\nlearning framework,” IEEE Transactions on Intelligent Transportation\nSystems, pp. 1–10, 2021.\n[17] D. Rosenberg, “Distances between probability measures,” Berkeley\nUniversity STAT C206A / MATH C223A, 2007.\n[18] A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun, “Flambe:\nStructural complexity and representation learning of low rank mdps,”\nAdvances in neural information processing systems, vol. 33, pp.\n20 095–20 107, 2020.\n[19] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy op-\ntimization,” in International conference on machine learning. PMLR,\n2017, pp. 22–31.\n[20] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,\nZ. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differen-\ntiation in pytorch,” 2017.\n[21] J. Gawlikowski, C. R. N. Tassi, M. Ali, J. Lee, M. Humt, J. Feng,\nA. M. Kruspe, R. Triebel, P. Jung, R. Roscher, M. Shahzad, W. Yang,\nR. Bamler, and X. X. Zhu, “A survey of uncertainty in deep neural\nnetworks,” CoRR, vol. abs/2107.03342, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2107.03342\n[22] P. Mcquighan, “Simulating the poisson process,” University of Chicago\nMath, 2010.\n[23] B. Hunt and D. Levermore, “Simulating an iid sequence from an\narbitrary distribution,” University of Maryland AMSC/MATH 420,\n2013.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-10-18",
  "updated": "2024-10-18"
}