{
  "id": "http://arxiv.org/abs/2410.14383v3",
  "title": "MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based Inter-Robot Negotiation",
  "authors": [
    "Toby Godfrey",
    "William Hunt",
    "Mohammad D. Soorati"
  ],
  "abstract": "Multi-agent reinforcement learning is a key method for training multi-robot\nsystems over a series of episodes in which robots are rewarded or punished\naccording to their performance; only once the system is trained to a suitable\nstandard is it deployed in the real world. If the system is not trained enough,\nthe task will likely not be completed and could pose a risk to the surrounding\nenvironment. We introduce Multi-Agent Reinforcement Learning guided by\nLanguage-based Inter-Robot Negotiation (MARLIN), in which the training process\nrequires fewer training episodes to reach peak performance. Robots are equipped\nwith large language models that negotiate and debate a task, producing plans\nused to guide the policy during training. The approach dynamically switches\nbetween using reinforcement learning and large language model-based action\nnegotiation throughout training. This reduces the number of training episodes\nrequired, compared to standard multi-agent reinforcement learning, and hence\nallows the system to be deployed to physical hardware earlier. The performance\nof this approach is evaluated against multi-agent reinforcement learning,\nshowing that our hybrid method achieves comparable results with significantly\nreduced training time.",
  "text": "MARLIN: Multi-Agent Reinforcement Learning Guided by\nLanguage-Based Inter-Robot Negotiation\nToby Godfrey1, William Hunt1 and Mohammad D. Soorati1\nAbstract— Multi-agent\nreinforcement\nlearning\nis\na\nkey\nmethod for training multi-robot systems over a series of\nepisodes in which robots are rewarded or punished according\nto their performance; only once the system is trained to a\nsuitable standard is it deployed in the real world. If the\nsystem is not trained enough, the task will likely not be\ncompleted and could pose a risk to the surrounding envi-\nronment. We introduce Multi-Agent Reinforcement Learning\nguided by Language-based Inter-Robot Negotiation (MARLIN),\nin which the training process requires fewer training episodes\nto reach peak performance. Robots are equipped with large\nlanguage models that negotiate and debate a task, producing\nplans used to guide the policy during training. The approach\ndynamically switches between using reinforcement learning\nand large language model-based action negotiation throughout\ntraining. This reduces the number of training episodes required,\ncompared to standard multi-agent reinforcement learning, and\nhence allows the system to be deployed to physical hardware\nearlier. The performance of this approach is evaluated against\nmulti-agent reinforcement learning, showing that our hybrid\nmethod achieves comparable results with significantly reduced\ntraining time.\nI. INTRODUCTION\nMutli-robot\nsystems\nhave\nemerged\nas\na\npromising\nparadigm for tackling complex tasks in various domains.\nAdvanced decision-making algorithms are essential to co-\nordinate and control these systems effectively. Multi-Agent\nReinforcement Learning (MARL) is a powerful framework\nfor training agents to collaborate, or compete [1], and\nhas become an integral technique for multi-robot systems’\ndevelopment and deployment. However, the complexity of\nreal-world scenarios often requires sophisticated learning\nand reasoning capabilities, which can be difficult and time-\nconsuming to train. Recently, pre-trained Large Language\nModels (LLMs) have demonstrated few-shot performance\nin a range of tasks [2], [3]. This knowledge distillation\nand reasoning are useful in enhancing the decision-making\ncapabilities of multi-robot systems.\nBy taking actions in an environment and learning through\npunishment and reward, agents learn policies through ex-\nperience and exploration [1]. Proximal Policy Optimisation\n(PPO) is a type of reinforcement learning policy that has be-\ncome popular in robotics applications [4]. Due to the clipped\nsurrogate objective function, the size of policy updates is\nlimited, resulting in more stable training. PPO was extended\nThis work was done as part of FAST-PI in UKRI Trustworthy Au-\ntonomous Systems Hub [EP/V00784X/1] and also supported by UKRI\nMINDS CDT [EP/S024298/1].\n1School of Electronics & Computer Science, University of Southampton,\nSouthampton, SO17 1BJ, United Kingdom\n{t.godfrey, w.hunt, m.soorati}@soton.ac.uk\nto multi-agent learning in [5] by adding a centralised critic\nto aid in training. Multi-Agent PPO (MAPPO) maintains the\nclipped surrogate objective function, and hence the stability,\nbut also follows the centralised training, decentralised exe-\ncution pattern, which has shown promise in overcoming the\nnonstationarity problem commonly found in MARL [6].\nWhen training multi-robot systems using MARL, sig-\nnificant time and data are often required to train models\nbefore they can be deployed onto robots. This paper aims to\naddress the amount of training required to deploy a MARL\npolicy to robots in the real world. This problem can cause\ndelays in the deployment of novel multi-robot systems due\nto training requirements, or because robots become damaged\nfrom prematurely deploying a policy. This work could have\nwide-reaching impacts by reducing the number of episodes\nit takes for a policy to perform well enough such that it can\nreliably be applied to hardware without adding significant\nrisk to the surrounding environment or the integrity of robots.\nLLMs are trained on vast corpora of text and other\ninternet-scale data, and so contain prior knowledge and\nreasoning skills unseen in other approaches. Since their\ninception, the community has seen an increasing number of\nLLM-integrated robotic systems, alongside many attempts at\nmitigating and overcoming common pitfalls.\nThis work harnesses the reasoning skills and flexibil-\nity of LLMs to improve the efficiency at which multi-\nrobot\nsystems\nare\ntrained\nusing\nMARL.\nMulti-Agent\nReinforcement Learning guided by Language-Based, Inter-\nAgent Negotiation (MARLIN) is proposed; a method where\nagents negotiate how to solve a given task using natural\nlanguage plans, which are then used to help train the\nMARL policy. This balances quick, robust MARL with\nprior-knowledge-based, but slower inter-agent negotiation.\nTo save training time, off-the-shelf language models without\nany additional fine-tuning are used. This approach leverages\nencoded reasoning and natural language understanding to\ngenerate more performant plans. As a result, policies can\nbe deployed to physical robots after fewer training episodes,\ncompared to a standard MARL implementation.\nThe primary contribution of this work is the development\nof a novel hybrid approach which augments MARL with di-\nalogical LLMs that utilise inter-agent negotiation to generate\nnatural-language plans which are then used to improve the\nin-training performance of cooperative robots, with no loss in\naccuracy compared to conventional MARL approaches. This\napproach is evaluated against MAPPO and an LLM baseline\nin a series of cooperative tasks where robots must navigate a\nnarrow corridor and swap positions, both in simulation and\narXiv:2410.14383v3  [cs.RO]  4 Mar 2025\non physical robots.\nII. RELATED WORK\nMulti-robot systems have become more common in the\nlandscape due to their increased robustness and flexibility\nwhen compared to single robot systems [7]. MARL is\nused extensively for multi-robot systems to learn a desired\nbehaviour. A more recent development is LLMs, which are\ncontinually being improved upon [8], [9], and have recently\nbeen integrated into robotics [10]. These models have been\napplied to multi-robot systems to build plans in natural\nlanguage [11]. When language models are extended with\naction tokens and visual processing capabilities, they have\nbeen used to develop end-to-end robot control with improved\ngenerality and semantic reasoning [12]. Similarly, PaLM-\nE was developed as an embodied multi-modal language\nmodel capable of mapping information from the textual-\nvisual domain to real-world actions [13]. When LLMs are\ncapable of interacting with other LLMs, they have shown\nthe ability to mimic complex human behaviours, such as\nreflection on past experiences and forming opinions [14].\nInteracting LLMs can also increase the accuracy of results\nthrough multi-agent debate [15].\nThe intersection of LLMs and MARL is less developed\nthan that of language models and robotic agents. Sun et\nal. detailed the current landscape of LLM-based MARL and\ndiscussed the potential for language models to be used in the\ntraining stage of MARL [16]. LLMs may be able to increase\nthe sample efficiency and explainability of MARL [17].\nIn [18], an LLM was combined with MARL to generate safe\nMARL policies. Natural language constraints were translated\ninto semantic embeddings for use in the MARL training\nprocess. Their framework performed comparably to conven-\ntional MARL models but with fewer constraint violations.\nLLMs have also been used to interpret natural language\nnavigation instructions and pass them to a reinforcement\nlearning or MARL model, resulting in a policy that can act\non human instructions [19], [20].\nZhang et al. used an LLM to generate an intrinsic reward\nfunction to help alleviate the credit assignment problem\nin MARL environments with sparse rewards [21]. Their\nintegration of models showed improved sample efficiency\nover a standard Deep Q-Learning implementation. Tarakli\net al. used a language model to solicit human feedback\nused to guide MARL training in order to improve per-\nformance [22]. Liu et al. developed the eSpark system to\nintegrate LLMs (one as a generator and one as a critic)\ninto MARL exploration [23]. The eSpark system directly\ngenerates exploration functions using one of the language\nmodels and then masks out potential actions. These functions\nare then applied and the best policy is selected. Their method\nincreased performance, compared to a standard MARL im-\nplementation, however, the eSpark system is only applicable\nto homogeneous agents and dense environments.\nOur system differs from the aforementioned works by us-\ning inter-agent negotiation to produce plans to guide MARL\ntraining. This is less centralised and utilises the prior knowl-\nedge encoded into the language models to solve the problem\nin fewer episodes. Actions are generated through inter-agent\nnegotiation and debate instead of using the approach of\neSpark to centrally generate functions for all agents. As\nour negotiation-based action generation system makes no\nassumptions regarding robot morphologies, MARL rewards,\nor the underlying MARL architecture, MARLIN can be used\nin a wider array of systems.\nIII. METHOD\nMAPPO is used as the base MARL model for several\nreasons. It achieves stable training due to the clipping of\nthe surrogate objective function [5]. It also follows the\ncentralised training, decentralised execution paradigm which\nhas been shown to achieve state-of-the-art performance [24].\nMAPPO is commonly used in multi-robot systems, allowing\nMARLIN to be applicable to a wider range of systems.\nAs the agents are cooperative, parameters are shared be-\ntween agents to decrease the number of trainable parameters.\nMAPPO is extended with an additional method for action\ngeneration to increase performance throughout training. Our\nsystem is comprised of two action-generation methods: (1)\nsampling the MARL action distribution (standard MAPPO),\nand (2) natural language inter-agent negotiation (LLM plan-\nner). At each inference step one of these methods is used,\nand the generated actions control the robots during training.\nSituations where LLMs are likely to generate superior plans\ncompared to the action distribution are capitalised on through\nruntime switching between action-generation functions. Irre-\nspective of how actions are generated, the MARL model ben-\nefits by training on the trajectory. Both the standard MAPPO\nalgorithm and our novel approach were implemented using\nthe Ray RLlib library [25]. A key facet of our work is that\nno fine-tuning is performed on language models. Fine-tuning\nmay yield performance gains, but MARLIN was designed for\nscenarios where robots are in use during training, resulting\nin limited opportunities for LLM fine-tuning before training.\nThe system can be modelled as a partially observable\nMarkov decision process (S,A,O,T,Ω,R) where S denotes\nthe state space, A = {W,F,B,L,R} is the set of actions\navailable to agents, corresponding to waiting (W), moving\nforwards (F), backwards (B), left (L) or right (R). The\nobservation space O ⊆Z4 for each agent is constructed\nsuch that o ∈O is a 4-tuple (x,y,xg,yg), representing the\nagent’s current position and its goal position. The transition\nfunction T : S × A × S →[0,1] specifies the probability of\ntransitioning from state s to state s′ under action a and\nthe observation function Ω: S × A × O →[0,1] returns the\nprobability of observing o ∈O for a given agent in state s\ntaking action a. The reward function is R : S×Ai →R such\nthat R(i) = w·max(0, di\nDi )−ρ where w ∈N is a constant to\nweight the performance over penalties, di is the Manhattan\ndistance from agent i’s current position to its goal position,\nDi is the Manhattan distance from agent i’s starting position\nto its goal position and ρ represents penalties for collisions\nwith other agents or the boundaries.\n(a)\n(b)\n(c)\n(d)\n(e)\nFig. 1.\nDiagrams of the scenarios used for evaluation; (a) Asymmetrical\nTwo Slot Corridor, (b) Symmetrical Two Slot Corridor, (c) Single Slot\nCorridor, (d) Two Path Corridor, (e) Maze Like Corridor\nThe environment consists of narrow corridors in which\ntwo agents cannot move past each other, and a way for the\nagents to either let the other pass or move around each other.\nDiagrams of the environments can be seen in Fig. 1. The\nSymmetrical Two Slot Corridor (Fig. 1(b)) is based on the\nenvironment used in [26]. This idea was expanded to other\ncorridors for the rest of the environments. The goal of all\nenvironments is for the agents to swap positions from one\nend of the corridor to the other. The task was kept simple to\naid in reproducibility.\nA feedforward Neural Network (NN) is trained to generate\nactions for each robot. Each robot has a NN with a Z4 input\nvector, two fully connected hidden layers, each with 256\nunits and tanh activation functions, and a R5 output vector\nrepresenting the action logits.\nMAPPO follows the centralised training, decentralised ex-\necution paradigm, and so during training there is a centralised\nvalue function. For a system with two robots, the feedforward\nNN to predict the value function has a Z13 input vector (own\nobservation, opponent’s observation, and opponent’s one-hot\nencoded action), one fully connected hidden layer with 16\nunits and tanh activation functions, and an R1 output.\nA. Generation Methods\nA generator function, G, is used, such that G : S →An\nwhere S is the set of states, A is the set of all actions and n\nis the total number of agents. These generator functions gen-\nerate the actions for all agents for every step. Our approach\nuses two distinct generators, Action Distribution Sampling\n(based on RL), and Inter-Agent Negotiation (using LLMs).\nAction Distribution Sampling: Sampling from the policy’s\naction distribution is the standard way to generate actions for\nMAPPO. The selected actions depend on the output of the\nmodel’s network and are based on expected reward, gradually\nconverging towards the best actions for a given state [27].\nInter-Agent Negotiation: While generating actions from the\nmodel’s action distribution works well once the model has\nadequate experience, it can take significant time and data to\nreach such a standard. This leaves the model selecting poor-\nperforming actions, especially at the beginning of training or,\nin the case of a dynamic environment, after the environment\nchanges. By integrating each agent with an LLM instance,\nthey can discuss the next actions to take. The inter-agent\nnegotiation system is extended from [11] in which agents\ntake turns discussing and critiquing solutions to a task.\nRobot 1 (R1)\n1. Task: [R1] is at (1, 0) and [R2] is at (1, 7).\n[R1]'s goal is (1, 7) and [R2]'s goal is (1, 0).\nWhat moves should [R1] and [R2] take to reach their\ngoals?\nRobot 2 (R2)\n2. TLP: [R1] moves towards her goal by getting out\nof the way and then moves along the x=1 column,\nwhile [R2] moves towards his goal by getting out of\nthe way and then moves along the x=1 column.\n[R2]: @SOUTH (1, 7) -> (1, 6) - Since [R2]'s goal\nis at (1, 0), he needs to move down to reach it,\nand there's no other agent in the way at (1, 6).\n[R1]: @NORTH (1, 0) -> (1, 1) - Since [R1]'s goal\nis at (1, 7), she needs to move up to reach it, and\nmoving to (1, 1) will temporarily get her out of\nthe way of [R2].\n3. [R1]: @WEST (1, 1) -> (0, 1) - To clear the path\nand make way for [R2], [R1] will move to (0, 1),\ntemporarily getting out of the way.\n[R2]: @SOUTH (1, 6) -> (1, 5) - [R2] will keep\nmoving down the x=1 column to reach his goal.\nAgreed [R1] Action: @WEST (1, 0) -> (0, 0)\nAgreed [R2] Action: @SOUTH (1, 6) -> (1, 5)\n@AGREE\n1. Task: [R1] is at (1, 0) and [R2] is at (1, 6).\n[R1]'s goal is (1, 7) and [R2]'s goal is (1, 0).\nWhat moves should [R1] and [R2] take to reach their\ngoals?\nThe previous move resulted in an error for [R1].\n[R1] cannot take action @WEST from (1, 0) as they\ncollide with the wall. [R1] must pick a different\naction. Do not do this again.\n2. TLP: [R1] moves towards her goal by getting out\nof the way and then moves along the x=1 column,\nwhile [R2] moves towards his goal by getting out of\nthe way and then moves along the x=1 column.\n[R2]: @SOUTH (1, 6) -> (1, 5) - Since [R2]'s goal\nis at (1, 0), he needs to move down to reach it,\nand there's an agent in the way at (1, 5).\n[R1]: @EAST (1, 0) -> (2, 0) - Since [R1]'s goal is\nat (1, 7), she needs to move up to reach it, and\nmoving to (2, 0) will temporarily get her out of\nthe way of [R2] and allow him to move down.\nYour name is [NAME] and [BOSS] is the boss.\n**Actions**\n* @NORTH (x, y) -> (x, y+1)\n* @SOUTH ...\n**Rules**\n1. No two agents can occupy the same grid square.\n(However, they can be in the same row or column, just\nnot the same grid square)\n2. ...\n**Grid Layout**\n      (1, 7)\n      (1, 6)(2, 6)\n            (2, 5)\n(0, 4)      (2, 4)\n(0, 3)      (2, 3)\n(0, 2)(1, 2)(2, 2)\n            (2, 1)\n      (1, 0)(2, 0)\nThere are several obstructions in the environment.\nThe main path is in the x=2 two column but this only\nhas space for one agent at a time. Obstructions are\nshown as blank areas on the grid above. Any other \nfree space from the main route can be used to move\nout the way of the other agent and let them use the\nonly clear path through the space. One agent will\nhave to use the area at (1, 2) to temporarily move\nout of the way of the other agent then you can move\nback into the x=2 column once the other agent has\nmoved past you. If you do not move into this area,\nyou will collide at some point and neither of you\nwill be able to complete your tasks.\n**Response Format**\n* TLP: Briefly describe the high-level plan for each\nagent (do not specify multiple actions).\n* [NAME]: <NEXT ACTION> - <REASON/THOUGHT PROCESS>\n* [PARTNER NAME]: <FEEDBACK/AGREEMENT>\nExample TLP: <NAME> moves towards their goal by ...\nthen moves along the <COL> column to ...\n**Agreement**\nOnce you agree on the actions, respond with:\n* Agreed [NAME] Action: <AGREED [NAME] ACTION>\n* Agreed [PARTNER NAME] Action: <AGREED [PARTNER\nNAME] ACTION>\n* @AGREE ~AGREE\nSystem Prompt\n...\nSim\nMoves\nFig. 2.\nA diagram of the inter-agent negotiation mechanism. Both agents\nshare the same system prompt, then alternate in suggesting a Top-Level Plan\n(TLP) and moves for both agents. Critiques can be provided to improve\nmoves until they are agreed by the pair. Moves are then simulated and the\nnext moves are discussed.\nB. Inter-Robot Negotiation\nTo generate actions using language models, the robots\ndiscuss the next step to take. Due to the nature of LLMs,\nthese discussions are conducted in natural language and\nhence are understandable by human observers. The language\nmodels act as planners instead of controllers. A lower layer\nis used to translate the high-level actions, e.g. @NORTH, into\nlinear and angular velocities for the robot wheels. This also\nallows MARLIN to be applied to other discrete planning\ntasks by simply writing a translator from high-level to low-\nlevel actions, and redefining the LLMs’ actions. In this\nwork, the LLMs were hosted remotely and accessed over\nthe Internet. At the beginning of a negotiation round, one\nof the robots is chosen to be the leader, this can either be\ndeterministic (used in our experiments) or a random choice\nfrom all robots. This helps to mitigate agents yielding the\ndecision to another agent, which assistant-style LLMs are\nprone to doing [28]. Then the robots alternate to discuss (in\npairs) the best actions for both agents until they either come\nto a consensus or reach the message limit. Once an action\nhas been agreed, the movements are stored and simulated,\nand the robots are given their updated positions. This process\nrepeats until either all robots reach their goals, or they reach\nthe maximum number of moves. Plans are cached for use\nin later training, or until they are replaced with higher-\nperforming ones. In the system prompt, the robots are given\nthe format in which responses should be written. This format\ncan then be parsed to extract the agreed moves. If the models\nproduce an output in a different format, they are prompted\n0.00\n0.25\n0.50\n0.75\n1.00\nPerformance\nAsymmetrical Two Slot Corridor\n0\n100\n200\n1400\n1500\n1600\nSingle Slot Corridor\n0\n100\n200\n1400\n1500\n1600\nEpisode\nSymmetrical Two Slot Corridor\n0\n100\n200\n1400\n1500\n1600\nEpisode\n0.00\n0.25\n0.50\n0.75\n1.00\nPerformance\nTwo Path Corridor\n0\n100\n200\n1400\n1500\n1600\nEpisode\nMaze Like Corridor\n0\n250\n500\n1400\n1500\n1600\nLLM\nMARL\nMARLIN\nFig. 3.\nMedian performance of the MARLIN and MARL systems for different scenarios in simulation. The boxplot shows the distribution of performance\nscores for trials using the LLM-based negotiation.\nto correct it. Operator preferences could also be included in\nthe system prompt to further influence robot behaviour. A\ndiagram of the negotiation mechanism can be seen in Fig. 2.\nC. Action Selection Process\nAlgorithm 1 MARLIN Algorithm\nRequire: N is the set of agents\n1: while episode ≤episodemax do\n2:\np ←mean(pastPerformanceBuffer)\n3:\nwhile step < stepmax do\n4:\nP ←loadPlan(state)\n5:\npplan ←perf(P)\n6:\nif step = 0 then\n7:\nif episode < m then\n8:\nG ←GADS\n9:\nelse if episode < 2m then\n10:\nG ←GIAN\n11:\nelse\n12:\nG ←\n\n\n\n\n\nGIAN,\nif pplan = 1\nGIAN,\nif p < pLLM\nGADS,\notherwise\n13:\nif P = ∅and G = GIAN then\n14:\nP ←makePlan()\n15:\nif step = stepmax\n2\nand rand() ≤0.1 then\n16:\nG ←toggleGenerator(G)\n17:\nactions ←genActionSingleStep(G,step)\n18:\ntrainStep(actions)\n▷Update env. & model\n19:\np ←\n∑|N|\ni=1 max(0,1−di\nDi )\n|N|\n▷Evaluate performance\n20:\npastPerformanceBuffer ←p\nThe algorithm for training using MARLIN is shown in Al-\ngorithm 1. An initialisation phase of 2m episodes establishes\nthe initial performance of both generation methods. At the\nstart of all proceeding episodes, it is dynamically determined\nwhich action generator, G, is to be used. GIAN and GADS\nindicate the Inter-Agent Negotiation-based generator and the\nAction Distribution Sampling-based generator respectively;\npplan is the performance of the plan for the current state,\np is the average performance of the past 5 episodes and\npLLM is the average performance for the LLM-only system\nin that state. For the performance metric, di is the Manhattan\ndistance from agent i’s end position to its goal position, and\nDi is the Manhattan distance from agent i’s starting point to\nits goal position; thus a successful trial has a performance\nof 1. This is similar but differs from the reward function of\nthe MARL model, which includes penalties for collisions.\nIf GIAN is selected, the best plan for the current state is\nretrieved from memory. If a plan for the current state does not\nyet exist, or the retrieved plan has worse performance than\nthe current rolling average, a new plan is created to ensure\npoor plans do not disrupt the training later in the process.\nIf the regenerated plan has a higher performance than the\ncached version, the cache is updated with the new plan. To\nfurther enhance the effectiveness of training, a probabilistic\nelement was implemented whereby there is a 10% chance\nthat the generation technique of any given episode will be\nswitched halfway through the episode. This helps to gather\nexperience from elsewhere in the environment.\nIV. EVALUATION\nThe effectiveness of our approach is evaluated by testing\neach component and our combined hybrid system in each\nscenario. Episode length was limited to 50 moves for all tests\nand all common configurations were kept the same between\nthe three systems. The Llama 3.1 8B Instruct model [9] was\nchosen as the LLM for all evaluation experiments, and the\nperformance metric from Algorithm 1 was used to evaluate\neach trial. Models were trained for 1600 episodes.\nSimulation-Based Evaluation: The first phase of experi-\nmentation was to test the performance of MARLIN against\nthe MAPPO and LLM benchmarks in simulation. Fig. 3\nshows that for all the scenarios, MARLIN outperforms\nboth the benchmark MARL implementation and the system\nonly using the inter-robot negotiation. Furthermore, in most\nscenarios the plans generated by the off-the-shelf LLMs\nstill have difficulty navigating around the other robot. How-\never, robot movement beyond the 50% performance mark\ngreatly increased the likelihood of task completion. When\ncomparisons are made to the MAPPO benchmark, it is\nseen that peak performance is reached by MARLIN after\nfewer training episodes for most scenarios. This shows that\nplans generated through inter-robot discussion are effective\nin providing beneficial actions to help train the MARL\npolicy. For all but the Maze-Like Corridor (see Fig. 1(e)),\nour method reached peak median performance earlier than\nthe MARL system. When trying to solve the Maze-Like\nCorridor, MARL reached peak performance from episode\n1000, but with some slight variation in the results. MARLIN\nplateaued slightly below the perfect score at around 0.95 but\nreached this score at episode 750, earlier than MARL.\nTABLE I\nAVERAGE PERFORMANCE THROUGHOUT TRAINING. BOLD INDICATES\nSIGNIFICANCE (p < 0.001 OVER A 50 RUN PAIRED T-TEST)\nScenario\n100\n850\n1600\nMARL MARLIN MARL MARLIN MARL MARLIN\nAsymmetrical\n0.8543\n1.0000\n0.9814\n0.9957\n0.9843\n0.9929\nTwo Slot Corr.\nMaze Like\n0.4300\n0.4271\n0.8529\n0.9643\n0.8214\n1.0000\nCorridor\nSingle Slot\n0.7157\n0.9071\n0.9329\n0.9943\n0.9700\n1.0000\nCorridor\nSymmetrical\n0.8671\n0.9714\n0.9929\n1.0000\n0.9857\n0.9952\nTwo Slot Corr.\nTwo Path Corr. 0.7771\n0.9271\n0.9929\n1.0000\n1.0000\n0.9667\nThese data show equal or higher performance by our\napproach, as compared to a standard MAPPO model, and an\nincreased sample efficiency. These data highlight how, most\nof the time, our hybrid approach delivers better performance\nthan using LLMs alone. Table I shows that MARLIN outper-\nforms MARL in most cases. Only for the Maze Like Corridor\n(see Fig. 1(e)) did the MARL have better performance from\nthe outset, but the performance of MARLIN was significantly\nhigher later in the training period. This slower start may be\ndue to the more complex environmental reasoning required\nby the LLM. As progress is made, the LLMs gain a better\nunderstanding of how to move around the environment, and\nthe performance of their plans increases.\nPhysical Robot Experiments: To validate our findings\nfrom the simulations and to close the reality gap, further\nexperiments were conducted on real hardware. For this, two\nTurtleBot3 robotic platforms were used, running ROS2 Hum-\nble Hawksbill. As the Maze-Like Corridor was the hardest\nscenario to solve in the simulation-based experiments, it\nwas focused on for the physical testing (see Fig. 4(a))1.\nThe performance of the system for both MARLIN and the\nMARL system was evaluated by testing the policy every\n250 episodes, starting from episode 100. Fig. 5 shows the\nperformance of our approach and the benchmark on phys-\nical hardware. This highlights that real-world performance\nfollows the same trend as that seen in Fig. 3, with MARLIN\nreaching near-perfect performance earlier than the MARL\n1Experimental videos can be found at https://sooratilab.com/z/marlin.php.\n(a) Environment setup\n(b) TurtleBot3 robot\nFig. 4.\nThe environment and robot platform used for the physical robot\nexperiments in the Maze-Like Corridor. Results are reported in Fig. 5.\n100\n350\n600\n850\n1100\n1350\n1600\nEpisode\n0.00\n0.25\n0.50\n0.75\n1.00\nPerformance\nMaze Like\nCorridor\nMARL\nMARLIN\nLLM\nFig. 5.\nMedian performance of the system for the Maze-Like Corridor\nwhen executed on physical hardware. The boxplot shows the performance\ndistribution when the LLM-only system was evaluated on physical hardware.\nsystem, but with the MARL approach slightly outperforming\nour system after a considerable amount of training. These\ndata reinforce our previous results that LLM-based, inter-\nagent negotiation can provide valuable information during\nMARL training to reduce the amount of training required.\nScalability Experiments: In order to ensure our inter-agent\nnegotiation system scales to larger environments with more\nagents, we developed the environment shown in Fig. 6.\nMARLIN’s negotiation system is designed for pairs of robots\nto discuss a task, therefore for larger systems, such a nego-\ntiation mechanism needs to be triggered only when robots\nmust negotiate. For this proof-of-concept, one agent starts\nat each grey tile and moves randomly until either: (1) the\nagent reaches the green square and leaves the environment,\nor (2) the agent finds another agent in the same area and\nthey cannot move past each other. If two agents are in\nFig. 6.\nA more complex scenario with a larger number of robots is\nused to test our method at a large scale. Agents negotiate in pairs to\npass corridors with no alternative route (e.g., blue areas highlighting our\nexamined corridors). The green cell is the exit and the grey cells indicate\nstarting positions.\none of these areas, moves are generated using our inter-\nagent negotiation system. This highlights how our dyadic\nnegotiation approach can be deployed to systems with more\nagents by detecting when negotiation pairs must be formed.\nThis LLM-based negotiation system also enables its use in\nenvironments which may change during runtime. However,\nefficiently scaling LLM-based approaches is still an open\nchallenge in multi-robot systems.\nV. CONCLUSIONS\nMARLIN augments MARL training with inter-agent, natu-\nral language negotiation. Throughout MARL training, agents\nnegotiate using the reasoning skills of off-the-shelf LLMs\nto generate actions which guide the MARL policy. This\napproach was evaluated in both simulation and the real-\nworld and showed that, on average, MARLIN reaches peak\nperformance in fewer training episodes than the MAPPO\nbaseline. Therefore, systems trained using MARLIN can be\ndeployed to real hardware after fewer episodes than typical\nMARL. This highlights how LLMs can utilise debate to\nact as a discrete planner for multi-robot systems. Whilst\nheuristic-based approaches could be useful for the tasks\noutlined in this work, the benefits of LLMs include more\ntransparency and incorporation of human preferences, and\nadaptations to dynamic and more complex environments.\nHowever, our system has a few limitations which could\nbe addressed by future work. Firstly, only discrete actions\nand environments are used in the MARL model. Preliminary\ntesting showed a decrease in LLM reasoning ability for con-\ntinuous environments, but future work can alter our method\nto maintain performance in continuous tasks. This work\nused the Llama 3.1 8B Instruct model due to its availability\nand low-cost hosting, but extensions to this work could\nexplore the deployment of local language models. Distilled\nsmall-scale models could run on the robots themselves to\nremove the requirement for an internet connection [29].\nAnother limitation of our system is the lack of environmental\ninformation being passed to the LLMs. It is possible that\nan extension in which robot sensor information is passed to\nthe LLMs could provide better performance. Future work\ncould also explore the performance implications of fine-\ntuning the models using human-written expert plans. Overall,\nour approach increased the speed of training with no loss in\nperformance by utilising LLMs’ inherent reasoning abilities.\nREFERENCES\n[1] S. V. Albrecht, F. Christianos, and L. Schäfer, Multi-Agent Reinforce-\nment Learning: Foundations and Modern Approaches.\nMIT Press,\n2024.\n[2] Y. Chen, J. Arkin, Y. Zhang, N. Roy, and C. Fan, “Scalable multi-robot\ncollaboration with large language models: Centralized or decentralized\nsystems?” in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2024, pp. 4311–4317.\n[3] Z. Mandi, S. Jain, and S. Song, “RoCo: Dialectic multi-robot col-\nlaboration with large language models,” in 2024 IEEE International\nConference on Robotics and Automation (ICRA), 2024, pp. 286–299.\n[4] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal Policy Optimization Algorithms,” Aug. 2017.\n[5] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and\nYI. WU, “The surprising effectiveness of PPO in cooperative multi-\nagent games,” in Advances in Neural Information Processing Systems,\nvol. 35, 2022, pp. 24 611–24 624.\n[6] C. Schroeder de Witt, J. Foerster, G. Farquhar, P. Torr, W. Boehmer,\nand S. Whiteson, “Multi-Agent Common Knowledge Reinforcement\nLearning,” in Advances in Neural Information Processing Systems,\nvol. 32.\nCurran Associates, Inc., 2019.\n[7] M. Dorigo, G. Theraulaz, and V. Trianni, “Swarm Robotics: Past,\nPresent, and Future [Point of View],” Proceedings of the IEEE, vol.\n109, no. 7, pp. 1152–1165, July 2021.\n[8] T. B. Brown, et al., “Language Models are Few-Shot Learners,” July\n2020.\n[9] A. Dubey, et al., “The Llama 3 Herd of Models,” Aug. 2024.\n[10] W. Hunt, S. D. Ramchurn, and M. D. Soorati, “A Survey of Language-\nBased Communication in Robotics,” June 2024.\n[11] W. Hunt, T. Godfrey, and M. D. Soorati, “Conversational language\nmodels for human-in-the-loop multi-robot coordination,” in Proceed-\nings of the 23rd International Conference on Autonomous Agents and\nMultiagent Systems, 2024.\n[12] A. Brohan, et al., “RT-2: Vision-Language-Action Models Transfer\nWeb Knowledge to Robotic Control,” July 2023.\n[13] D. Driess, et al., “PaLM-E: An embodied multimodal language\nmodel,” in Proceedings of the 40th International Conference on Ma-\nchine Learning, ser. ICML’23.\nHonolulu, Hawaii, USA: JMLR.org,\n2023.\n[14] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and\nM. S. Bernstein, “Generative Agents: Interactive Simulacra of Human\nBehavior,” in Proceedings of the 36th Annual ACM Symposium on\nUser Interface Software and Technology, Oct. 2023.\n[15] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch,\n“Improving Factuality and Reasoning in Language Models through\nMultiagent Debate,” May 2023.\n[16] C. Sun, S. Huang, and D. Pompili, “LLM-based Multi-Agent Rein-\nforcement Learning: Current and Future Directions,” May 2024.\n[17] Z. Zhang, “Advancing sample efficiency and explainability in multi-\nagent reinforcement learning,” in Proceedings of the 23rd International\nConference on Autonomous Agents and Multiagent Systems, 2024, pp.\n2791–2793.\n[18] Z. Wang, M. Fang, T. Tomilin, F. Fang, and Y. Du, “Safe Multi-\nagent Reinforcement Learning with Natural Language Constraints,”\nMay 2024.\n[19] S. Morad, A. Shankar, J. Blumenkamp, and A. Prorok, “Language-\nConditioned Offline RL for Multi-Robot Navigation,” July 2024.\n[20] W. Zu, W. Song, R. Chen, Z. Guo, F. Sun, Z. Tian, W. Pan,\nand J. Wang, “Language and sketching: An LLM-driven interactive\nmultimodal multitask robot navigation framework,” in 2024 IEEE\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2024, pp. 1019–1025.\n[21] A. Zhang, A. Parashar, and D. Saha, “A Simple Framework for\nIntrinsic Reward-Shaping for RL using LLM Feedback,” 2023.\n[22] I. Tarakli, S. Vinanzi, and A. D. Nuovo, “Interactive Reinforcement\nLearning from Natural Language Feedback,” in 2024 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems (IROS), Oct.\n2024, pp. 11 478–11 484.\n[23] Z. Liu, X. Yang, Z. Liu, Y. Xia, W. Jiang, Y. Zhang, L. Li, G. Fan,\nL. Song, and B. Jiang, “Knowing What Not to Do: Leverage Language\nModel Insights for Action Space Pruning in Multi-agent Reinforce-\nment Learning,” May 2024.\n[24] Y. Zhou, S. Liu, Y. Qing, K. Chen, T. Zheng, Y. Huang, J. Song,\nand M. Song, “Is Centralized Training with Decentralized Execution\nFramework Centralized Enough for MARL?” May 2023.\n[25] The Ray Team, “RLlib: Industry-Grade Reinforcement Learning —\nRay 2.35.0,” https://docs.ray.io/en/latest/rllib/index.html, 2024.\n[26] M. Bettini, A. Shankar, and A. Prorok, “Heterogeneous multi-robot\nreinforcement learning,” in Proceedings of the 2023 International\nConference on Autonomous Agents and Multiagent Systems, 2023, pp.\n1485–1494.\n[27] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc-\ntion.\nMIT press, 2018.\n[28] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem,\n“CAMEL: Communicative Agents for \"Mind\" Exploration of Large\nLanguage Model Society,” Nov. 2023.\n[29] J. Li, et al., “Large Language Model Inference Acceleration: A\nComprehensive Hardware Perspective,” Jan. 2025.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2024-10-18",
  "updated": "2025-03-04"
}