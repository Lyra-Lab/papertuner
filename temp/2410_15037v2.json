{
  "id": "http://arxiv.org/abs/2410.15037v2",
  "title": "mHumanEval -- A Multilingual Benchmark to Evaluate Large Language Models for Code Generation",
  "authors": [
    "Nishat Raihan",
    "Antonios Anastasopoulos",
    "Marcos Zampieri"
  ],
  "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced code generation from natural language prompts. The HumanEval\nBenchmark, developed by OpenAI, remains the most widely used code generation\nbenchmark. However, this and other Code LLM benchmarks face critical\nlimitations, particularly in task diversity, test coverage, and linguistic\nscope. Current evaluations primarily focus on English-to-Python conversion\ntasks with limited test cases, potentially overestimating model performance.\nWhile recent works have addressed test coverage and programming language (PL)\ndiversity, code generation from low-resource language prompts remains largely\nunexplored. To address this gap, we introduce mHumanEval, an extended benchmark\nsupporting prompts in over 200 natural languages. We employ established machine\ntranslation methods to compile the benchmark, coupled with a quality assurance\nprocess. Furthermore, we provide expert human translations for 15 diverse\nnatural languages (NLs). We conclude by analyzing the multilingual code\ngeneration capabilities of state-of-the-art (SOTA) Code LLMs, offering insights\ninto the current landscape of cross-lingual code generation.",
  "text": "mHumanEval - A Multilingual Benchmark to Evaluate Large Language\nModels for Code Generation\nNishat Raihan, Antonios Anastasopoulos, Marcos Zampieri\nGeorge Mason University\nFairfax, VA, USA\n{mraihan2, antonis, mzampier}@gmu.edu\nAbstract\nRecent advancements in large language mod-\nels (LLMs) have significantly enhanced code\ngeneration from natural language prompts. The\nHumanEval Benchmark, developed by OpenAI,\nremains the most widely used code genera-\ntion benchmark. However, this and other Code\nLLM benchmarks face critical limitations, par-\nticularly in task diversity, test coverage, and\nlinguistic scope. Current evaluations primarily\nfocus on English-to-Python conversion tasks\nwith limited test cases, potentially overestimat-\ning model performance. While recent works\nhave addressed test coverage and programming\nlanguage (PL) diversity, code generation from\nlow-resource language prompts remains largely\nunexplored. To address this gap, we introduce\nmHumanEval1, an extended benchmark support-\ning prompts in over 200 natural languages. We\nemploy established machine translation meth-\nods to compile the benchmark, coupled with\na quality assurance process. Furthermore, we\nprovide expert human translations for 15 di-\nverse natural languages (NLs). We conclude\nby analyzing the multilingual code generation\ncapabilities of state-of-the-art (SOTA) Code\nLLMs, offering insights into the current land-\nscape of cross-lingual code generation.\n1\nIntroduction\nLLMs have transformed software development\nwith their ability to generate programming code\nfrom simple natural language instructions. LLMs\nare trained on extensive datasets that include di-\nverse code samples, aiding programmers in code\ndevelopment and debugging. They also make pro-\ngramming more accessible to beginners. How-\never, assessing the performance of these models\nacross different coding tasks is still a major chal-\nlenge. Comprehensive testing is essential to verify\nthat these models are both effective and adaptable,\n1github.com/mraihan-gmu/mHumanEval-Benchmark\nrather than only performing well under specific\nconditions.\nThe most widely used benchmark for evaluating\nthese models is OpenAI’s HumanEval (Chen et al.,\n2021), which includes a collection of 164 tasks\ngenerated by human experts. Each task includes\nan English prompt, a canonical solution provided\nby the authors, and three test cases. Although this\nbenchmark is commonly used, it has significant lim-\nitations, such as limited test coverage and minimal\nsupport for non-English and non-Python prompts.\nWhile recent variations (Peng et al., 2024; Cassano\net al., 2023) of HumanEval address some of these\nissues, most do not include prompts in NLs other\nthan English and, in particular, in low-resource\nNLs. Consequently, current benchmarks fail to pro-\nvide key insights into the multilingual capabilities\nof LLMs in the context of code generation.\nFigure 1 demonstrates one such example.\nWhile the widely used GPT3.5 (Brown et al.,\n2020) model performs perfectly for the original\nprompt\n\"Write a Python code snippet that\ndetects whether a year is a leap year or\nnot.\", it fails when the same prompt is given in a\nlow-resource language (Nyanja, in this case).\n# Chaka chomwe tikufuna kuyang'ana\n# Yang'anani ngati chaka ndi chaka cha ziwalo\nif (year % 4 == 0 and year % 100 != 0):\n# Ngati chaka chimagawika ndi 4 ndipo\n# sichimagawika ndi 100\nsindikiza(f\"{year} ndi chaka cha ziwalo\")\nelse:\n# Ngati sichigwirizana ndi zofunikira\n# za chaka cha ziwalo\nsindikiza(f\"{year} si chaka cha ziwalo\")\nFigure 1:\nCode snippet generated by GPT3.5 when\nprompted to write a Python code to detect leap\nyears in Nyanja language. Some Python keywords\nare transformed into Nyanja words, resulting in compi-\nlation issues.\nMost LLMs, primarily pre-trained on large English\ncorpora like Common Crawl, perform poorly on\narXiv:2410.15037v2  [cs.CL]  26 Jan 2025\nmultilingual tasks, further propagating inequali-\nties in language technology access (Blasi et al.,\n2022). However, proprietary models like GPT-4\n(Achiam et al., 2023) and Claude 3 (Anthropic,\n2024), with undisclosed training data, show de-\ncent performance in multilingual scenarios. Peng\net al. (2024) for instance show that GPT-4 excels in\ncode generation even with mid-resource language\nprompts. The open-source community is also ad-\nvancing with multilingual models like Aya (Üstün\net al., 2024) and LLaMA 3. However, insights into\ntheir code generation performance in a massively\nmultilingual setting are lacking due to the absence\nof comprehensive benchmarks.\nIn this work, we introduce mHumanEval, a novel\nmultilingual code generation benchmark including\ncoding prompts in 204 NLs and expert human trans-\nlations for 15 NLs. mHumanEval further includes\ncanonical solutions in 25 PLs, including 4 new PLs\nthat are not covered by any prior benchmarks. The\nprimary contributions of this paper are as follows:\n1. The creation of mHumanEval, the first mas-\nsively multilingual benchmark for code gener-\nation.\n2. A translation quality evaluation for each\nprompt.\n3. A thorough evaluation of existing SOTA Code\nLLMs using mHumanEval.\nThe paper addresses two research questions (RQs):\n• RQ1: How do the code generation capabil-\nities of LLMs vary when prompts are pro-\nvided in English, or other high-, mid-, and\nlow-resource NLs?\n• RQ2: How does the performance of multilin-\ngual LLMs compare to specialized, fine-tuned\nCode LLMs in code generation tasks on the\nmHumanEval dataset?\nFinally, we also report secondary findings related to\nthe translation quality of machine translation (MT)\nmethods on coding prompts.\n2\nRelated Work\nThe most widely used benchmark dataset for evalu-\nating Code LLMs is the aformentioned HumanEval\n(Chen et al., 2021). Another key benchmark is\nDeepMind’s MBPP (Austin et al., 2021), which in-\ncludes 974 tasks with 3 test cases each. Despite\ntheir popularity, these benchmarks have signifi-\ncant limitations, such as inadequate test case cov-\nerage, limited number of PLs, and small task sets\nthat do not represent real-world scenarios. Other\nbenchmarks, like CONCODE (Iyer et al., 2018) (Java),\nAxiBench (Hao et al., 2022) (Java), CSEPrompts\n(Raihan et al., 2024) (Python) and CodeApex (Fu\net al., 2023) (C++) focus on a single PL.\nTo broaden PL coverage, Cassano et al. (2023)\ncombine both HumanEval and MBPP and add 17\nmore popular PLs besides Python, such as C++,\nJava, Ruby, and PHP. However, all prompts remain\nin English, with only 3 test cases per task. Sim-\nilarly, the authors of BabelCode (Orlanski et al.,\n2023) include 14 PLs and a more extensive test\nsuite. To address test case coverage, Liu et al.\n(2024) introduce two datasets, HumanEval+ and\nMBPP+, with significantly more test cases per task,\nensuring both node and edge coverage. Notably,\nCode LLM performance decreases with the addi-\ntional test cases, highlighting the initial bench-\nmarks’ limitations.\nNevertheless, these bench-\nmarks also use English prompts exclusively.\nFew studies explore non-English coding prompts\nand evaluate Code LLMs on them. The recent\nbenchmark, HumanEval-XL (Peng et al., 2024), ex-\ntends coverage for both NLs and PLs. This bench-\nmark includes coding prompts in 23 NLs and so-\nlutions in 12 PLs. The original prompts from Hu-\nmanEval (Chen et al., 2021) are translated into\n23 different NLs using GPT-4 (Achiam et al.,\n2023), with the quality of these translations as-\nsessed using a thresholded BERTScore (Zhang et al.,\n2019). While HumanEval-XL explores multilingual\nprompts for code generation (Table 1), its 23 pre-\ndominantly high-resource NLs limit insights into\nmid and low-resource NLs. The BERTScore (Zhang\net al., 2019) evaluation may be inadequate, with\nCometKiwi (Rei et al., 2023) and X-Comet (Guer-\nreiro et al., 2023) offering more robust alternatives.\nExperimenting with SOTA Code LLMs like Wiz-\nardCoder (Luo et al., 2023) or multilingual models\nlike Aya (Üstün et al., 2024) could yield valuable\ninsights. Also, they do not include any human\ntranslations.\nWe argue that NL coverage is more critical than\nPL coverage when compiling a code generation\nbenchmark. While prompts and tests can be reused\nacross PLs, different NLs require curating contex-\ntually and linguistically appropriate prompts. Thus,\nNL diversity introduces more complexity in bench-\nmark creation than PL diversity. To bridge the\nBenchmarks\nHumanEval\nMBPP\nBabel Code\nMultiPL-E\nHumanEval-XL\nmHumanEval\nNL-Covg (MT)\n1 (eng)\n1 (eng)\n1 (eng)\n1 (eng)\n23\n204\nNL-Covg (Human)\n✗\n✗\n✗\n✗\n✗\n15\nPL-Covg\n1 (py)\n1 (py)\n14\n18\n12\n25\nTable 1: Comparing popular benchmarks in terms of NL and PL coverage.\ngap, we present mHumanEval, offering comprehen-\nsive experiments with multilingual coding prompts\nacross 204 NLs and 25 PLs—the most extensive\ncoverage to date (see Appendix A for the full list)\nand the first one to include expert-human annota-\ntions (see Table 1). We describe mHumanEval in\ndetail in this paper and we evaluate SOTA models\non this dataset.\n3\nThe mHumanEval Benchmark\nThe mHumanEval benchmark is curated based on\nprompts from the original HumanEval (Chen et al.,\n2021) dataset. It includes a total of 33,456 prompts,\nsignificantly expanding from the original 164. The\ncuration process can be divided into several key\nsteps, as illustrated in Figure 2 and elaborated upon\nin the following subsections.\n3.1\nPrompt Extraction\nA typical prompt from the original dataset includes\noptional library imports, a function declaration, a\ndocstring, and optional examples (as illustrated\nin Figure 3).\nFor\ntranslation,\nwe\nonly\nconsider\nthe\ndocstrings (enclosed in triple quotes). These are\nmanually extracted from all 164 prompts to ensure\naccuracy.\n3.2\nPrompt Translation\nUpon extracting the prompts, we move on to trans-\nlating them into different languages. We use three\ndifferent machine translation strategies - leverag-\ning OpenAI’s GPT4-omni through API, MetaAI’s\nNLLB (Costa-jussà et al., 2022), which is the\nSOTA model for multiple NLs, and Google Trans-\nlate via API.\nOur target languages are all 204 languages from\nthe Flores 200 dataset-(Costa-jussà et al., 2022).\nWhile we employ GPT4-omni and NLLB for all\nthe target languages, it is important to note that we\nuse only Google Translator for the 108 languages\nit supports (available through the API). For each\nextracted prompt, we employ the three translation\nsystems for each target language, generating 5 can-\ndidate translation prompts (3 for GPT4o, due to\nbudget considerations). We then evaluate the qual-\nity of the translation and keep the best one (see\nFigure 2). The pseudocode is in Appendix B.\n3.3\nEvaluating Prompt Quality\nWe evaluate translation quality using BERTScore\n(Zhang et al., 2019), which focuses on similarity\nbased on contextual embeddings, and CometKiwi\n(Rei et al., 2023), which is trained on human judg-\nments of MT quality and incorporates linguistic fea-\ntures. While BERTScore uses BERT embeddings\nto measure candidate-reference translation similar-\nity (Appendix C), CometKiwi evaluates translations\nreference-free, using human judgments and com-\nbining linguistic features with contextual embed-\ndings (Appendix D). Using both ensures holistic\nevaluation, covering lexical similarity and human-\nassessed quality aspects.\nAs illustrated in Figure 2, we generate 13 can-\ndidate translations for each prompt. We also per-\nform round-trip translations back to the original\nlanguage (eng_Latn) to calculate the BERTScore.\nWhile CometKiwi is calculated as a reference-free\nmetric. Both metrics generate scores in the [0,1]\nrange. By computing the mean of the two metrics\nfor each prompt, we select the candidate with the\nhighest score. The mean scores for each language\nand system are provided in Appendices K, L and\nM. It is worth noting that the CometKiwi metrics\nare not available for all languages, as it relies on\nXLM-R models (Conneau et al., 2019; Goyal et al.,\n2021) supporting 100 languages (Rei et al., 2023).\nFor the remaining 104 Flores 200 languages, we\nuse round-trip translations to calculate BERTScore,\nsimilar to HumanEval-XL (Peng et al., 2024).\n3.4\nCategorization based on Language Classes\nTo better understand the performance of models on\nlanguages considered to be low- or high-resourced,\nwe group the languages in mHumanEval following\nthe methodology of Joshi et al. (2020), who iden-\ntify six classes of languages based on digital re-\nFigure 2: The workflow to generate prompts in a target language from the original HumanEval. Original prompts\nare first extracted manually. Then 3 Machine Translation models (GPT4o, NLLB, Google Translate) generate 13\ncandidates as well as roundtrip translations. Next, we evaluate each candidate’s quality using BERTScore using\nRoundTrip translations and CometKiwi as a reference-free metric (if the language is supported). We then select the\nbest candidate for each original prompt and compile the new benchmark for the target language.\nfrom typing import List\ndef all_prefixes:\n\"\"\" Return list of all prefixes\nfrom shortest to longest of the\ninput string. \"\"\"\n>>> all_prefixes('abc')\n['a', 'ab', 'abc']\nFigure 3: A sample prompt instance from the original\nHumanEval benchmark.\nsource availability. These classes range from 0 to 5,\nwith higher numbers indicating greater resource\navailability. Joshi et al. classify a total of 2,485\nlanguages, of which mHumanEval includes 204, in-\ncluding 15 with expert translations, as detailed in\nTable 2.\nWe present the class-wise evaluation scores for\nthe selected prompts in mHumanEval in Figure 4.\nThe language-specific scores are provided in Ap-\npendices K, L, and M. Generally, the quality of\nthe translation decreases as we address languages\nwith fewer resources. However, by implement-\ning Algorithm 1 and selecting from 13 candidate\ntranslations, the chosen candidates demonstrate im-\nproved quality compared to the model-specific re-\nsults (see Appendices N and F). The final prompts\nin mHumanEval exhibit significantly better quality.\nClass\nResource\nTotal\nmHumanEval\nExpert\n5\nHigh\n7\n7\n6\n4\nMid to High\n18\n18\n4\n3\nMid\n28\n27\n1\n2\nLow to Mid\n19\n16\n2\n1\nLow\n222\n98\n1\n0\nRare\n2191\n38\n1\nALL\n–\n2485\n204\n15\nTable 2: Class distribution of natural languages based\non resource availability. Expert denotes human transla-\ntions done by expert programmers.\n3.5\nPL coverage\nAs noted in Section 2, most benchmarks in this sub-\ndomain are limited to Python, including HumanEval\nand MBPP. While recent benchmarks such as\nMultiPL-E and HumanEval-XL offer broader cov-\nerage, they still omit several widely used program-\nming languages. With mHumanEval, we compile\na comprehensive set of programming languages\ncovered by existing multi-PL coding benchmarks\nand extend this set by incorporating four additional\nlanguages that have not been previously included:\nMATLAB, Visual Basic, Fortran, and COBOL (as\nshown in Table 7).\nWe provide canonical solutions for the newly\nincluded four languages in the same format as\nHumanEval . These solutions are handwritten by\nhuman experts and successfully pass all test cases.\nFigure 4: Evaluating the translated prompt qualities, chosen in mHumanEval. Our method results in better quality\nprompts compared to the model-specific translations (as depicted in Appendix F).\nPrompts\nNote\nmHumanEval-{NL}\n164 each\nEach NL\nmHumanEval-mini\n204\n204 NLs\nmHumanEval-T500\n500\nTop 500\nmHumanEval-R500\n500\nRandom 500\nmHumanEval-B500\n500\nBottom 500\nmHumanEval-Expert\n2460\nHuman Generated\nmHumanEval-{PL}\n4100 each\nEach PL\nmHumanEval\n33456\nOnly Python\nmHumanEval-Max\n836400\nAll Prompts\nTable 3: Subsets and Variants of mHumanEval. These\nenable practitioners to carry out both comprehensive\nand preliminary evaluations on the benchmark.\n3.6\nmHumanEval Subsets\nWe have a total of 33,456 prompts in mHumanEval\nspanning 204 NLs. Each prompt additionally sup-\nports 24 PLs, bringing the total number of prompts\nto 836,400. The entire dataset is publicly available\non GitHub.\nWe also provide multiple subsets of the dataset\nfor quick usability and interesting ablation stud-\nies (Table 3). Separate subsets are available for\neach NL and PL, in all possible combinations.\nAdditionally, we create several variants for test-\ning purposes- mHumanEval-T500: a subset consist-\ning of the 500 highest-quality prompts based on\nBERTScore and CometKiwi, mHumanEval-R500: a\nrandomly selected subset of 500 prompts, and\nmHumanEval-B500: a subset of the 500 lowest-\nquality prompts. Note that these prompts are drawn\nfrom the curated mHumanEval, which compiles the\nbest prompts from 13 candidates each. Finally,\nwe produce mHumanEval-mini which is a subset\ncontaining 204 prompts, with each prompt in a dif-\nferent language, where we select one prompt per\nlanguage.\n3.7\nmHumanEval - Expert\nThe mHumanEval-Expert benchmark encompasses\nhuman translations across 15 languages, represent-\ning all six language classes (Table 2). Native speak-\ners with computer science and engineering back-\ngrounds perform these translations, ensuring pre-\ncise interpretation of programming concepts and\nterminology. The curation process unfolds in three\nstages: (1) selection of 15 natural languages based\non native speaker availability, ensuring representa-\ntion from each language class; (2) translation by na-\ntive speakers; and (3) quality assessment by expert\nprogrammers to verify the integrity of the coding\nprompts. Figure 5 illustrates the whole curation\nprocess.\nA comparative analysis between human trans-\nlations\nand\nmHumanEval’s\nmachine-translated\nprompts yields comparable evaluation metrics, with\nBERTScore variations of ±0.02 and CometKiwi\nvariations of ±0.03 across the selected lan-\nguages.\nInterestingly,\nannotators report no\nsignificant terminology concerns when review-\ning machine translations.\nFurther examina-\ntion of the original HumanEval prompts reveals\nthat the docstrings—the primary translated con-\ntent—predominantly comprise general task descrip-\ntions, minimizing the use of specialized coding\nterminology. This observation emphasizes the neg-\nligible discrepancies between human and machine\ntranslations in this context.\nWe conclude that human and machine transla-\ntions of programming prompts across 15 languages\nshow similar quality, with minimal differences in\nevaluation metrics. This similarity is attributed to\nthe general nature of the content, which contains\nlimited specialized coding terminology.\nFigure 5: Curating mHumanEval-Expert via native human translation followed by expert programmer evaluation.\n4\nExperiments\nModel\nSelection\nWe\nexperiment\nwith\nmHumanEval using six models (Table 4), in-\ncluding both proprietary and open-source SOTA\nmodels for code generation.\nWe use a mix of\ngeneral-purpose and finetuned models to gather\nbroader insights.\nModel\nSize\nType\nRef.\nGPT4o\n–\nBase\n(Achiam et al., 2023)\nClaude-3.5-Opus\n–\nBase\n(Anthropic, 2024)\nGPT3.5\n175B\nBase\n(Brown et al., 2020)\nDeepSeek-Coder-V2\n236B\nFinetuned\n(Dai et al., 2024)\nWizardCoder\n33B\nFinetuned\n(Luo et al., 2023)\nAya\n33B\nFinetuned\n(Üstün et al., 2024)\nTable 4: LLMs evaluated on mHumanEval.\nPrompting\nWe use the proprietary models\nthrough their APIs.\nOur experiments include\nall 33,456 prompts from mHumanEval, with 164\nprompts for each language. We follow the standard\nprompt templates for each LLM. These templates\nare shown in Appendix G.\nCode Execution\nFollowing code generation, we\nmove to execution. The six models produce well-\nstructured code blocks, requiring minimal cleaning.\nWe use simple RegEx commands to extract these\nblocks, and evaluate them locally in batches using\nPython’s subprocess2 library, focusing exclusively\non the Pass@1 metric.\nResults\nFor each language, we present the\nPass@1 scores as percentages, categorizing them by\nthe six language classes as discussed in Section 3.4.\nAs illustrated in Figure 6, Claude3.5 and GPT4o\nexhibit the most consistent performance, main-\ntaining strong results even with coding prompts\n2docs.python.org/3/library/subprocess.html\nin low-resource languages. In contrast, GPT3.5\nand DeepSeek experience a significant decline in\nperformance for low-resource classes. Although\nAya shows the weakest results for higher resource\nclasses, it maintains relative consistency, even in ex-\ntremely low-resource languages. On the other hand,\nWizardCoder achieves excellent results in English\nand reasonable performance for Class 5, but its\nperformance deteriorates significantly in other lan-\nguages. The model and language-specific detailed\nresults are presented in Appendix O.\nOther PLs\nWe extend our evaluation to four\nadditional subsets of mHumanEval:\nmHumanEval-\nC++, mHumanEval-JAVA, mHumanEval-JavaScript,\nand mHumanEval-Ruby. The average Pass@1 scores\nacross all 204 NLs for the 5 PLs are shown in Table\n5.\nPython\nJava\nC++\nJavaScript\nRuby\nGPT4o\n0.738\n0.650\n0.652\n0.477\n0.480\nGPT3.5\n0.360\n0.270\n0.270\n0.099\n0.103\nClaude3.5\n0.739\n0.651\n0.649\n0.483\n0.477\nDeepSeek-Coder\n0.229\n0.139\n0.136\n0.000\n0.000\nWizardCoder\n0.098\n0.009\n0.007\n0.000\n0.000\nAya\n0.445\n0.355\n0.356\n0.186\n0.183\nTable 5: Mean performance of models across program-\nming languages.\nWe observe that GPT-4o and DeepSeek-Coder\nachieve strong results in Classes 4 and 5, with\nscores consistently exceeding 0.85 in Python, Java,\nand C++. Python shows top performance, with\nscores reaching above 0.88 in Class 5. For lower\nclasses (0-2), models like GPT-3.5, WizardCoder,\nand Aya underperform, often scoring below 0.70,\nparticularly in JavaScript and Ruby, where scores\nfrequently drop under 0.65. Even in higher classes,\nJavaScript and Ruby show challenges, with Class\n4 scores for most models not exceeding 0.75. This\nFigure 6: Comparing model performances (% in Pass@1) for the six models on mHumanEval-Python.\nhighlights the models’ limitations in handling non-\nPython languages, particularly for lower classes\nand specific scripting languages.\nWhile every\nmodel’s best scores are generated with English-\nPython pair, DeepSeek-Coder is the only exception\nwith Chinese-Python.\nA detailed analysis and discussion is provided in\nAppendix J.\n5\nInsights and Analysis\nUpon curating the mHumanEval benchmark and\ncompleting the model evaluations, we now present\nsome key analyses and gained insights based on\nthe obtained results.\n5.1\nLLMs’ Performance Analysis\nWe observe significant performance discrepancies\namong the models, as illustrated by Figure 6. While\nclosed-source models perform better, their reliance\non proprietary pretraining data complicates defini-\ntive conclusions. As suggested by the Chinchilla\nscaling hypothesis (Hoffmann et al., 2022), their su-\nperior performance may result from a larger param-\neter count and extensive training tokens, possibly\nincluding diverse and rare languages.\nAya, fine-tuned for multiple natural languages\nbut not specifically for code generation, has the\nlowest Pass@1 score in English. However, low\nvariability across language classes indicates that\nmultilingual pretraining and fine-tuning enhances\ncode generation across different NLs.\nWizardCoder’s poor performance in non-English\nlanguages is due to its fine-tuning on StarCoder (Li\net al., 2023), which is primarily pretrained on code\nand documentation with minimal non-English con-\ntent. In contrast, DeepSeek performs well for mid-\nresource languages but struggles with low-resource\nones. These results suggest that effective multilin-\ngual code generation requires multilingual pretrain-\ning and/or finetuning datasets.\n5.2\nPerformance based on Language Classes\nWhile there are significant discrepancies among\nthe models’ performances, a key trend observed\nis a somewhat consistent performance decline as\nwe move from high-resource to low-resource lan-\nguages.\nThis decline is not as pronounced for\nClaude and GPT-4o. However, it is quite substan-\ntial for others and exceptionally steep for Wizard-\nCoder and DeepSeek-Coder.\n5.3\nError Analysis\nIn our analysis of errors, we observe several unique\nissues. Notably, the models rarely fail to generate\nany code. Specifically, GPT4o and GPT3.5 gener-\nate code with almost no compilation issues. How-\never, a significant number of errors arise from mis-\nunderstandings of the problem, resulting in code\nthat addresses incorrect tasks. This issue primarily\noccurs because translated keywords (e.g., string,\nlist) do not always retain identical meanings in the\nGPT4o\nGPT3.5\nAya\nWizardCoder\nClaude3.5\nDeepSeek-Coder\nLLaMA 3\nCodeStral\nmHumanEval-mini\n.72\n.44\n.47\n.12\n.61\n.57\n.35\n.15\nmHumanEval-T500\n.87\n.76\n.6\n.63\n.86\n.73\n.56\n.36\nmHumanEval-R500\n.78\n.53\n.47\n.16\n.59\n.63\n.28\n.17\nmHumanEval-B500\n.48\n.21\n.42\n.00\n.31\n.22\n.11\n.10\nTable 6: Comparison of different LLMs’ based on % in Pass@1 metric on multiple subsets of mHumanEval.\ntarget language, as illustrated in Appendix H.1.\nFurthermore, the Aya model often uses identi-\nfiers or keywords from different languages, leading\nto compilation errors (Appendix H.2). A recurring\nproblem with DeepSeek-Coder and WizardCoder\nis the generation of nonsensical code, sometimes\nnot even in Python, especially when prompted in a\nnon-English language (Appendix H.3).\n5.4\nAblation Study\nWe present results from a limited ablation study\nconducted on various subsets of mHumanEval as\ndetailed in Table 3. This study incorporates two\nadditional models including MetaAI’s LLaMA 3\n(70B), and MistralAI’s code-finetuned CodeStral\n(22B) model.\nAs indicated by the results in Table 6,\nmHumanEval-mini serves as an effective prelimi-\nnary test for evaluating a model’s proficiency in\ncode generation following multilingual prompts.\nModels fine-tuned on code but lacking multilin-\ngual exposure perform poorly, whereas base mod-\nels with some multilingual exposure perform bet-\nter. The three subsets of mHumanEval are curated\nby prompt quality:\nmHumanEval-T500 includes\nprompts from language class 5, mHumanEval-B500\nfrom classes 0 or 1, and mHumanEval-R500 is ran-\ndomly selected. These results align with our find-\nings in Sections 5.1 and 5.2.\n6\nConclusion\nThis study introduces mHumanEval, a comprehen-\nsive multilingual code generation benchmark for\nassessing LLMs across 204 languages. We curated\nhigh-quality prompts for each language and eval-\nuated various models. Our analyses, including ab-\nlation studies, provided insights into LLMs’ mul-\ntilingual code-generation capabilities, addressing\nthe RQs posed in Section 1:\nRQ1:\nHow do the code generation\ncapabilities of LLMs vary when prompts\nare provided in English, or other high-,\nmid-, and low-resource NLs?\nLLMs generally demonstrate optimal perfor-\nmance when prompted in English. For prompts\nin other languages, performance varies based on\nthe language’s resource level. High-resource lan-\nguages tend to yield superior results compared\nto mid- and low-resource languages. The extent\nof performance variation is contingent upon the\nspecific language of the prompt and the model’s\nprior exposure and training in that language. This\nvariation is likely influenced by the model’s train-\ning data and the relative abundance of resources\navailable for each language.\nRQ2:\nHow does the performance of\nmultilingual LLMs compare to specialized,\nfine-tuned Code LLMs in code generation\ntasks on the mHumanEval dataset?\nWhile code-finetuned language models excel at\ngenerating code from English prompts, multi-\nlingual models demonstrate strong proficiency\nacross various NLs. Notably, even without spe-\ncific code fine-tuning for different NLs, they\nachieve decent results in code generation. This\nphenomenon suggests that multilingual models\ncan generalize coding capabilities across NLs,\nleveraging their understanding of multiple NLs\nto support diverse linguistic contexts in program-\nming.\nWhile we draw some insightful conclusions from\ncurating and evaluating mHumanEval, to facilitate\nfurther research, we are making it publicly avail-\nable. We plan to expand coverage to more NLs\nand PLs in future updates. Despite the high cost\nof human translation, we included human anno-\ntations for 15 NLs, including some low-resource\nand rare ones. Currently, our dataset includes 164\nprompts per language, following the HumanEval\nbenchmark, with plans to increase this number. We\nwill also explore strategies to enhance low-resource\nlanguage performance, such as transfer learning\nand diverse training datasets. Comparative studies\nbetween general-purpose multilingual LLMs and\nspecialized code LLMs will help optimize multilin-\ngual code generation.\nLimitations\nWe conducted primary evaluations on six LLMs,\nfocusing on key performance metrics. Given the\nbenchmark’s extensive 33,456 prompts, the evalua-\ntion process is exceedingly costly. This cost is the\nprimary reason why we adopted Pass@1 as our eval-\nuation metric, rather than more resource-intensive\nmetrics like Pass@10 or Pass@100. However, to\nensure a thorough analysis, we incorporated addi-\ntional models in our ablation study. In our next\niteration, we plan to comprehensively evaluate all\nmodels across the entire benchmark. This future\nwork aims to enhance the benchmark’s robustness\nand provide deeper insights into the performance\nof various LLMs in multilingual code generation.\nEthical Considerations\nThe benchmark introduced in this paper, which fo-\ncuses on analyzing code generation using large lan-\nguage models (LLMs), strictly adheres to the ACL\nEthics Policy. Each prompt in mHumanEval was\ntested multiple times by different models, and none\nproduced any malicious code. Although there can\noccasionally be garbage code snippets or similar\nissues, none have posed any threats to the system.\nTo ensure safety and reliability, we recommend\nexecuting code generated using prompts from\nmHumanEval in a contained virtual environment.\nThis precaution helps prevent potential issues re-\nlated to infinite execution loops and memory man-\nagement. Running code in a safe environment can\nalso stop problems like crashing the system or us-\ning too much memory. We believe and hope that\nresearchers and practitioners can maintain a secure\nand controlled testing environment while utilizing\nmHumanEval. This approach ensures that users can\nconfidently explore and innovate without risking\nsystem integrity.\nAcknowledgments\nWe would like to thank the human annotators and\nexperts for their valuable time and effort; also\nGeorge Mason’s Office of Research Computing\n(ORC) for providing the computing resources.\nAntonios Anastasopoulos is additionally sup-\nported by the National Science Foundation under\naward IIS-2327143 and benefited from resources\nprovided through the Microsoft Accelerate Foun-\ndation Models Research (AFMR) grant program.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, Red Avila, Igor Babuschkin, et al.\n2023. Gpt-4 technical report.\nAnthropic. 2024. Claude 3: A next-generation ai as-\nsistant. https://www.anthropic.com/news/claude-3-\nfamily.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nDamian Blasi, Antonios Anastasopoulos, and Gra-\nham Neubig. 2022. Systematic inequalities in lan-\nguage technology performance across the world’s\nlanguages. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5486–5505, Dublin,\nIreland. Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems.\nFederico Cassano, John Gouwar, Daniel Nguyen, Syd-\nney Nguyen, Luna Phipps-Costin, Donald Pinck-\nney, et al. 2023. Multipl-e: a scalable and polyglot\napproach to benchmarking neural code generation.\nIEEE Transactions on Software Engineering.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan, et al.\n2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nMarta R Costa-jussà, James Cross, Onur Çelebi,\net al. 2022.\nNo language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nDamai Dai, Chengqi Deng, Chenggang Zhao, et al.\n2024. Deepseekmoe: Towards ultimate expert spe-\ncialization in mixture-of-experts language models.\narXiv preprint arXiv:2401.06066.\nLingyue Fu, Huacan Chai, Shuang Luo, Kounianhua\nDu, Weiming Zhang, et al. 2023. Codeapex: A bilin-\ngual programming evaluation benchmark for large\nlanguage models. arXiv preprint arXiv:2309.01940.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-scale trans-\nformers for multilingual masked language modeling.\nIn Proceedings of the 6th Workshop on Representa-\ntion Learning for NLP (RepL4NLP-2021).\nNuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa\nCoheur, et al. 2023. xcomet: Transparent machine\ntranslation evaluation through fine-grained error de-\ntection. arXiv preprint arXiv:2310.10482.\nYiyang Hao, Ge Li, Yongqiang Liu, Xiaowei Miao,\net al. 2022. Aixbench: A code generation benchmark\ndataset. arXiv preprint arXiv:2206.13179.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018. Mapping language to code\nin programmatic context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the nlp\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics.\nR Li, LB Allal, Y Zi, N Muennighoff, D Kocetkov,\nC Mou, M Marone, C Akiki, J Li, J Chim, et al. 2023.\nStarcoder: May the source be with you! Transactions\non machine learning research.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2024. Is your code generated by chatgpt\nreally correct? rigorous evaluation of large language\nmodels for code generation. Advances in Neural\nInformation Processing Systems.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo\nGeng, et al. 2023. Wizardcoder: Empowering code\nlarge language models with evol-instruct. In The\nTwelfth International Conference on Learning Repre-\nsentations.\nGabriel Orlanski, Kefan Xiao, Xavier Garcia, et al. 2023.\nMeasuring the impact of programming language dis-\ntribution. In International Conference on Machine\nLearning. PMLR.\nQiwei Peng, Yekun Chai, and Xuhong Li. 2024.\nHumaneval-xl:\nA multilingual code generation\nbenchmark for cross-lingual natural language gen-\neralization. arXiv preprint arXiv:2402.16694.\nNishat\nRaihan,\nDhiman\nGoswami,\nSadiya\nSa-\nyara\nChowdhury\nPuspo,\nChristian\nNewman,\nTharindu Ranasinghe, and Marcos Zampieri. 2024.\nCseprompts: A benchmark of introductory computer\nscience prompts. arXiv preprint arXiv:2404.02540.\nRicardo Rei, Nuno M Guerreiro, Daan van Stigt, Marcos\nTreviso, et al. 2023. Scaling up cometkiwi: Unbabel-\nist 2023 submission for the quality estimation shared\ntask. In Proceedings of the Eighth Conference on\nMachine Translation.\nAhmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-\nYin Ko, Daniel D’souza, Gbemileke Onilude, et al.\n2024. Aya model: An instruction finetuned open-\naccess multilingual language model. arXiv preprint\narXiv:2402.07827.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\nA\nlist of NLs and PLs in mHumanEval\nmHumanEval supports 204 NLs and 25 PLs. The Expert subset contains human annotation for 15 NLs.\nA.1\nList of PLs\nComparing PL support provided by most widely used existing benchmarks -\nBenchmarks\nHumanEval\nMBPP\nBabel Code\nMultiPL-E\nHumanEval-XL\nmHumanEval\nPython\n✓\n✓\n✓\n✓\n✓\n✓\nBash\n✗\n✗\n✗\n✓\n✗\n✓\nC++\n✗\n✗\n✓\n✓\n✗\n✓\nC#\n✗\n✗\n✓\n✓\n✓\n✓\nD\n✗\n✗\n✗\n✓\n✗\n✓\nGo\n✗\n✗\n✓\n✓\n✓\n✓\nHaskell\n✗\n✗\n✓\n✗\n✗\n✓\nJava\n✗\n✗\n✓\n✓\n✓\n✓\nJavaScript\n✗\n✗\n✓\n✓\n✓\n✓\nJulia\n✗\n✗\n✗\n✓\n✗\n✓\nKotlin\n✗\n✗\n✓\n✗\n✓\n✓\nLua\n✗\n✗\n✗\n✓\n✗\n✓\nPerl\n✗\n✗\n✗\n✓\n✓\n✓\nPHP\n✗\n✗\n✓\n✓\n✓\n✓\nR\n✗\n✗\n✗\n✓\n✗\n✓\nRacket\n✗\n✗\n✗\n✓\n✗\n✓\nRuby\n✗\n✗\n✓\n✓\n✓\n✓\nRust\n✗\n✗\n✓\n✓\n✗\n✓\nScala\n✗\n✗\n✓\n✓\n✓\n✓\nSwift\n✗\n✗\n✓\n✓\n✓\n✓\nTypeScript\n✗\n✗\n✓\n✓\n✓\n✓\nMATLAB\n✗\n✗\n✗\n✗\n✗\n✓\nVisual Basic\n✗\n✗\n✗\n✗\n✗\n✓\nFortran\n✗\n✗\n✗\n✗\n✗\n✓\nCOBOL\n✗\n✗\n✗\n✗\n✗\n✓\nTable 7: Comparing popular benchmarks in terms of NL and PL coverage.\nA.2\nList of NLs: mHumanEval-Expert\nPrompts in these languages are generated using translations done by native speakers, followed by evalua-\ntions done by expert programmers.\nLanguage\nClass\nEnglish\n5\nSpanish\n5\nFrench\n5\nJapanese\n5\nArabic\n5\nChinese\n5\nPortuguese\n4\nItalian\n4\nKorean\n4\nHindi\n4\nBangla\n3\nSwahili\n2\nZulu\n2\nTelugu\n1\nSinhala\n0\nTable 8: NLs along with their classes in mHumanEval-Expert.\nA.3\nList of NLs: mHumanEval\nLanguage\nClass\nLanguage\nClass\nLanguage\nClass\nLanguage\nClass\narb_Arab\n5\nzsm_Latn\n3\ngla_Latn\n1\ntat_Cyrl\n1\ndeu_Latn\n5\namh_Ethi\n2\nguj_Gujr\n1\ntel_Telu\n1\neng_Latn\n5\ngle_Latn\n2\nhye_Armn\n1\ntgk_Cyrl\n1\nfra_Latn\n5\nhau_Latn\n2\nibo_Latn\n1\ntpi_Latn\n1\njpn_Jpan\n5\nisl_Latn\n2\nilo_Latn\n1\ntso_Latn\n1\nspa_Latn\n5\nlao_Laoo\n2\njav_Latn\n1\ntuk_Latn\n1\nzho_Hans\n5\nmar_Deva\n2\nkab_Latn\n1\ntum_Latn\n1\ncat_Latn\n4\nmlt_Latn\n2\nkan_Knda\n1\ntwi_Latn\n1\nces_Latn\n4\npan_Guru\n2\nkas_Arab\n1\nuig_Arab\n1\neus_Latn\n4\nsan_Deva\n2\nkas_Deva\n1\nvec_Latn\n1\nfin_Latn\n4\nswh_Latn\n2\nkhk_Cyrl\n1\nwar_Latn\n1\nhin_Deva\n4\ntir_Ethi\n2\nkhm_Khmr\n1\nydd_Hebr\n1\nhrv_Latn\n4\ntsn_Latn\n2\nkik_Latn\n1\nzho_Hant\n1\nhun_Latn\n4\nwol_Latn\n2\nkin_Latn\n1\nawa_Deva\n0\nita_Latn\n4\nxho_Latn\n2\nkir_Cyrl\n1\nbam_Latn\n0\nkor_Hang\n4\nyor_Latn\n2\nkmr_Latn\n1\nban_Latn\n0\nnld_Latn\n4\nzul_Latn\n2\nlij_Latn\n1\nbem_Latn\n0\npes_Arab\n4\nace_Arab\n1\nlim_Latn\n1\ncjk_Latn\n0\npol_Latn\n4\nace_Latn\n1\nlin_Latn\n1\ndyu_Latn\n0\npor_Latn\n4\nacm_Arab\n1\nlmo_Latn\n1\nfon_Latn\n0\nrus_Cyrl\n4\nacq_Arab\n1\nltg_Latn\n1\nfuv_Latn\n0\nsrp_Cyrl\n4\naeb_Arab\n1\nltz_Latn\n1\ngrn_Latn\n0\nswe_Latn\n4\najp_Arab\n1\nlug_Latn\n1\nhat_Latn\n0\ntur_Latn\n4\naka_Latn\n1\nmai_Deva\n1\nhne_Deva\n0\nvie_Latn\n4\nals_Latn\n1\nmal_Mlym\n1\nkac_Latn\n0\nafr_Latn\n3\napc_Arab\n1\nmin_Arab\n1\nkam_Latn\n0\narb_Latn\n3\nars_Arab\n1\nmin_Latn\n1\nkbp_Latn\n0\narz_Arab\n3\nary_Arab\n1\nmkd_Cyrl\n1\nkea_Latn\n0\nben_Beng\n3\nasm_Beng\n1\nmri_Latn\n1\nkmb_Latn\n0\nbos_Latn\n3\nast_Latn\n1\nmya_Mymr\n1\nknc_Arab\n0\nbul_Cyrl\n3\nayr_Latn\n1\nnno_Latn\n1\nknc_Latn\n0\nceb_Latn\n3\nazb_Arab\n1\nnob_Latn\n1\nkon_Latn\n0\ndan_Latn\n3\nazj_Latn\n1\nnpi_Deva\n1\nlua_Latn\n0\nell_Grek\n3\nbak_Cyrl\n1\noci_Latn\n1\nluo_Latn\n0\nest_Latn\n3\nbel_Cyrl\n1\nory_Orya\n1\nlus_Latn\n0\nglg_Latn\n3\nbho_Deva\n1\npag_Latn\n1\nmag_Deva\n0\nheb_Hebr\n3\nbjn_Arab\n1\npap_Latn\n1\nmni_Beng\n0\nind_Latn\n3\nbjn_Latn\n1\npbt_Arab\n1\nmos_Latn\n0\nkat_Geor\n3\nbod_Tibt\n1\nplt_Latn\n1\nnso_Latn\n0\nkaz_Cyrl\n3\nbug_Latn\n1\nquy_Latn\n1\nnus_Latn\n0\nlit_Latn\n3\nckb_Arab\n1\nsag_Latn\n1\nnya_Latn\n0\nlvs_Latn\n3\ncrh_Latn\n1\nsat_Olck\n1\nprs_Arab\n0\nron_Latn\n3\ncym_Latn\n1\nscn_Latn\n1\nrun_Latn\n0\nslk_Latn\n3\ndik_Latn\n1\nsmo_Latn\n1\nshn_Mymr\n0\nslv_Latn\n3\ndzo_Tibt\n1\nsna_Latn\n1\nsin_Sinh\n0\ntam_Taml\n3\nepo_Latn\n1\nsnd_Arab\n1\nsot_Latn\n0\ntgl_Latn\n3\newe_Latn\n1\nsom_Latn\n1\ntaq_Latn\n0\ntha_Thai\n3\nfao_Latn\n1\nsrd_Latn\n1\ntaq_Tfng\n0\nukr_Cyrl\n3\nfij_Latn\n1\nssw_Latn\n1\ntzm_Tfng\n0\nurd_Arab\n3\nfur_Latn\n1\nsun_Latn\n1\numb_Latn\n0\nuzn_Latn\n3\ngaz_Latn\n1\nszl_Latn\n1\nyue_Hant\n0\nTable 9: All NLs and their classes included in mHumanEval.\nB\nPrompt Translation and Evaluation\nAlgorithm\nThe pseudocode version of the workflow, presented\nin Figure 2.\nAlgorithm 1 Prompt Translation and Evaluation\n1: for each extracted prompt from HumanEval\ndo\n2:\nfor each translation system do\n3:\nfor each target language do\n4:\nif the language is supported then\n5:\ngenerate\n5\ntranslated\ncandidate\nprompts\n6:\ndo back translation\n7:\ncalculate\nBERT_Score\nand\nComet_Kiwi for each\n8:\ntake the average of the two\n9:\npick the best prompt\n10:\nelse\n11:\ndo back translation\n12:\ncalculate only BERT_Score\n13:\npick the best prompt\n14:\nend if\n15:\nend for\n16:\nend for\n17: end for\nIt describes how the originally extracted prompts\ngo through 13 candidate translations and evaluation\nvia BERTScore and CometKiwi to build the new sets\nof benchmarks in the target natural languages.\nC\nEvaluation Metric 1: BERTScore\nBERTScore uses pre-trained BERT embeddings to\nassess similarity between candidate and reference\ntranslations. For a candidate sentence C and a ref-\nerence sentence R, let EC and ER be the sets of\nBERT embeddings for tokens in C and R, respec-\ntively. The similarity S(i, j) between tokens i and\nj is the cosine similarity of their embeddings:\nS(i, j) =\neCi · eRj\n∥eCi∥∥eRj∥\nPrecision P, recall R, and F1-score F1 are then:\nP =\n1\n|EC|\nX\neCi∈EC\nmax\neRj ∈ER S(i, j)\nR =\n1\n|ER|\nX\neRj ∈ER\nmax\neCi∈EC S(j, i)\nF1 = 2 · P · R\nP + R\nHere, P and R denote precision and recall as\naverage maximum similarities from candidate to\nreference and vice versa. The F1 score is their\nharmonic mean.\nD\nEvaluation Metric 2: CometKiwi\nCometKiwi (Knowledge Integration via Weighted\nImportance) evaluates translations without refer-\nences, using human-judgment scores. Given source\nx and candidate y, it maps these inputs to a quality\nscore Q(x, y) using a neural network N trained on\nhuman scores Qhuman(x, y):\nQ(x, y) = f(Esrc(x), Ecand(y), L(x, y))\nwhere Esrc and Ecand are embeddings for x and\ny, and L represents linguistic features. The func-\ntion f is:\nf = N(Esrc, Ecand, L)\nThe network N minimizes the loss:\nL = 1\nN\nN\nX\ni=1\n(Q(xi, yi) −Qhuman(xi, yi))2\nwhere N is the sample size.\nE\nAnnotator Details\nAs mentioned in Section 3.7, mHumanEval-Expert\nutilizes native-speaking volunteer translators for 15\nNLs. Each translator was assigned 164 prompts,\nwith no monetary compensation involved. The ex-\nperts, also native speakers, possess backgrounds in\nComputer Science and/or Information Technology,\ncomplemented by substantial coding experience.\nBoth translators and experts were carefully selected\nthrough a rigorous process, ensuring a diverse de-\nmographic representation. This methodological\napproach enhances the dataset’s linguistic diversity\nand technical robustness across various cultural\ncontexts.\nF\nComparison of the Prompt Qualities by the 3 models vs mHumanEval\nFigure 7: Comparing the Machine Translation Quality for GPT4o, NLLB and Google Translator. The metrics used\nare BERTScore and CometKiwi. As shown in the figure, the prompts chosen for mHumanEval are better in quality\nupon choosing from 13 different candidates.\nG\nPrompt Templates\nGPT4o and GPT3.5\nprompt = \"Write a Python function for\nthe following: \" + mHumanEval[i] +\n\" Ensure your response includes a\nPython code block.\"\nmessages=[\n{\"role\": \"system\", \"content\":\n\"You are a helpful assistant\ntrained to generate Python code.\n\"},\n{\"role\": \"user\", \"content\":\nprompt}\n]\nFigure 8: Prompt template - GPT4o and GPT3.5.\nWizardCoder\nBelow is an instruction that describes\na task. Write a response that\nappropriately completes the request.\n### Instruction:\n\"mHumanEval[i]\"\n### Response:\nFigure 9: Prompt Template - WizardCoder\nAya\nmessages = [{\"role\": \"user\",\n\"content\": mHumanEval[i]}]\nFigure 10: Prompt Template - Aya.\nClaude3.5\nsystem=\"Write a Python Code snippet\nfor the following: \",\nprompt = mHumanEval[i] + \"Make sure\nyour response includes a code block.\"\nmessages=[\n{\"role\": \"user\",\n\"content\": prompt}\n]\nFigure 11: Prompt template - Claude3-Opus.\nLLaMA 3\nmessages = [\n{\"role\": \"system\", \"content\":\n\"You are a helpful AI assistant,\nwho writes Python Code.\"},\n{\"role\": \"user\", \"content\":\nmHumanEval[i]},\n]\nFigure 12: Prompt template - LLaMA 3.\nDeepSeek-Coder\nmessages =\n{\"role\": \"user\", \"content\":\n\"write a quick sort algorithm\nin python.\"}\nFigure 13: Prompt template - DeepSeek-Coder.\nCodeStral\nprompt = \"Below is a programming\nproblem, paired with a language in\nwhich the solution should be written.\nWrite a solution in the provided that\nappropriately solves the programming\nproblem.\n### Problem:\nmHumanEval[i]\n### Language: python\n### Solution: \"\nFigure 14: Prompt template - MagiCoder.\nH\nError Analysis - Examples\nH.1\nTask Misunderstanding\nGPT4o\ngenerates\na\ncode\nfor\nfinding\nsignificant digits,\nwhen\nit\nis\nprompted\nto generate a code for detecting prime numbers\nin Zulu. This is due to the reason of the translation\nof ’prime number’ in Zulu becomes ’Inombolo\nEyinhloko’ which means ’primary numbers’.\nThe model interprets it as ’significant number’\ninstead.\ndef find_significant_digits(number):\n# Convert the number to a string to handle it\nnum_str = str(number)\n# Remove any leading/trailing zeros and\n# the decimal point\nsignificant_digits = ''.join([char for char\nin num_str if char.isdigit()])\nreturn significant_digits\n# Example usage\nnumber = 123.456\nsignificant_digits = find_significant_digits(number)\nprint(f\"Significant digits in {number} are:\n{significant_digits}\")\nFigure 15:\nGP4o generated Python Code to find\nsignificant digits, when prompted to write code to\ndetect prime numbers in Zulu.\nIssues like this persist, even sometimes when\nprompted multiple times.\nH.2\nMultilingual Keyword Issues\nSome\ncode\nsnippets\ninclude\nkeywords\nand\nidentifiers in the target language as well, result-\ning in compilation errors. As shown in Figure 16,\nwhen prompted in Rundi, to find the largest\nnumber in a list, Aya generated a piece of code,\nwhere keywords like for and return turned into\nkuri and subiza.\ndef igitigiri_cane_kuruta(imibare):\nnamba not imibare:\nsubiza None\ncane = imibare[0]\nkuri imibare hagati imibare:\nnamba imibare > cane:\ncane = imibare\nsubiza cane\nFigure 16:\nWhen prompted in Rundi, Aya gener-\nated Python code to find the largest number in a\nlist, containing multiple keywords in Rundi, resulting\nin Compilation Error.\nThese kinds of errors are also persistent but the\noutputs differ with similar prompts. Still generate\nthe same issue again and again.\nH.3\nGarbage Results\n#include <stdio.h>\nint main() {\nint x = 10; x = x / (x - x);\nprintf(\"%d\\n\", x);\nx = x * \"Hello World!\";\n}\nFigure 17: When prompted in Sinhala, to reverse a\nlist, WizardCoder generated a garbage code in C.\nI\nExperimental Setup\nI.1\nMachine Translation\nGPT4o is accessed via API key, eliminating the\nneed for GPU hours. Hyperparameter tuning is\nnot conducted; instead, recommended values are\nutilized. The max_tokens parameter is set to 1000,\nand the temperature is maintained at 0.7. Addi-\ntionally, Google Translate is accessed through API\nkey, and the NLLB model is employed using a sin-\ngle NVIDIA A100 GPU with 40 GB of memory.\nI.2\nCode Generation\nGPT4o, GPT3.5, and Claude3-Opus are accessed\nthrough API keys, thereby negating the necessity\nfor GPU hours. We adhere to the recommended\nhyperparameters without conducting hyperparam-\neter searches. The max_tokens parameter is set to\n1000, and the temperature is maintained at 0.7.\nFor WizardCoder and Aya, we utilize the full\nprecision (FP32) models without employing any\nquantized versions. These models are run on four\nNVIDIA A100 GPUs, each with 40 GB of mem-\nory. Hyperparameter settings are maintained as per\nthe authors’ recommendations without additional\ntuning.\nFor MagiCoder, LLaMA 3, and Phi-3-mini, the\nfull precision (FP32) models are employed on a\nsingle NVIDIA A100 GPU with 40 GB of memory.\nHyperparameter configurations are again set to the\nrecommended values as specified by the authors.\nJ\nEvaluation Results: mHumanEval-PL\nWe evaluate the six LLMs from Table 4 for all 204 NLs in four different PLs. More specifically, we evaluate\nthem on four subsets of mHumanEval - mHumanEval-C++, mHumanEval-JAVA, mHumanEval-JavaScript, and\nmHumanEval-Ruby. The results with mHumanEval-Python are presented in Figure 6 and discussed in\nSection 4. The performance trend is similar to Python, as discussed in Section 4. However, the results are\nslightly worse than those of Python.\nJ.1\nmHumanEval-C++\nFigure 18: Comparing model performances (% in Pass@1) for the six models on mHumanEval-C++.\nJ.2\nmHumanEval-JAVA\nFigure 19: Comparing model performances (% in Pass@1) for the six models on mHumanEval-JAVA.\nJ.3\nmHumanEval-JavaScript\nFigure 20: Comparing model performances (% in Pass@1) for the six models on mHumanEval-JavaScript.\nJ.4\nmHumanEval-Ruby\nFigure 21: Comparing model performances (% in Pass@1) for the six models on mHumanEval-Ruby.\nJ.5\nAnalyzing PL-specific results\nPerformance Decline in Lower Classes (0-2)\nModels generally exhibit a noticeable performance\ndecline in lower language classes, particularly\nClasses 0-2. Across all programming languages,\nscores in these classes fall well below the perfor-\nmance seen in Classes 4 and 5. This decline is es-\npecially pronounced in JavaScript and Ruby, where\nscores frequently drop to or near 0.000, suggesting\nthese classes pose additional challenges.\nModel\nPython (C2)\nJava (C2)\nC++ (C2)\nJavaScript (Cl1)\nRuby (C1)\nGPT4o\n0.600\n0.590\n0.591\n0.000\n0.000\nGPT3.5\n0.200\n0.180\n0.181\n0.000\n0.000\nClaude3.5\n0.620\n0.600\n0.601\n0.478\n0.473\nDeepSeek-Coder\n0.350\n0.330\n0.331\n0.000\n0.000\nTable 10: Performance of models in lower classes (0-2)\nacross programming languages, with pronounced drops,\nparticularly in JavaScript and Ruby.\nGeneral Trends Across Language Classes\nIn\nClasses 4 and 5, GPT-4 and Claude3.5 achieve\nhigh scores, often exceeding 0.85 in Python and\nJava. Python consistently demonstrates the highest\nscores, especially in Class 5, where models like\nGPT-4 and DeepSeek-Coder surpass 0.88. How-\never, in Classes 0-3, performance drops across all\nmodels, particularly in JavaScript and Ruby, where\nscores frequently fall below 0.65.\nClass\nPython\nJava\nC++\nJavaScript\nRuby\nClass 5\n0.880\n0.850\n0.852\n0.650\n0.653\nClass 4\n0.860\n0.830\n0.832\n0.640\n0.643\nClass 3\n0.750\n0.720\n0.721\n0.530\n0.533\nClass 2\n0.620\n0.600\n0.601\n0.420\n0.423\nTable 11: General model performance across language\nclasses, highlighting high scores in Classes 4 and 5 and\nlower scores in Classes 0-3, particularly in JavaScript\nand Ruby.\nUnderperformance of WizardCoder and Aya in\nJavaScript and Ruby Across Classes\nWizard-\nCoder and Aya consistently struggle across all lan-\nguage classes in JavaScript and Ruby. In Classes 0-\n3, their scores frequently reach 0.000, underscoring\nlimitations in handling these scripting languages\nregardless of language class.\nMixed Adaptability of DeepSeek-Coder Across\nLanguage Classes\nDeepSeek-Coder shows mod-\nerate scores in Python for higher classes (Classes 4\nand 5) but drops to 0.000 in lower classes, partic-\nularly in JavaScript and Ruby, highlighting issues\nModel\nClass 5\nClass 4\nClass 3\nClass 2\nClass 1\nWizardCoder (JavaScript)\n0.000\n0.000\n0.000\n0.000\n0.000\nAya (JavaScript)\n0.186\n0.165\n0.143\n0.120\n0.100\nWizardCoder (Ruby)\n0.000\n0.000\n0.000\n0.000\n0.000\nAya (Ruby)\n0.183\n0.160\n0.138\n0.115\n0.090\nTable 12: Underperformance of WizardCoder and Aya\nin JavaScript and Ruby across language classes, with\nscores at 0.000 for WizardCoder across all classes.\nwith adaptability across classes.\nLanguage Class\nPython\nJava\nC++\nJavaScript\nRuby\nClass 5\n0.880\n0.850\n0.852\n0.000\n0.000\nClass 4\n0.860\n0.830\n0.832\n0.000\n0.000\nClass 3\n0.500\n0.480\n0.482\n0.000\n0.000\nTable 13: DeepSeek-Coder’s performance across lan-\nguage classes, illustrating high scores in Python and\nJava in Classes 4 and 5, but collapsing to 0.000 in\nJavaScript and Ruby.\nClaude3.5’s Stable Performance Across Lan-\nguage Classes\nClaude3.5 consistently scores\nabove 0.477 across all languages and classes, in-\ndicating versatility and robust adaptability across\ndifferent language classes and programming lan-\nguages.\nLanguage Class\nPython\nJava\nC++\nJavaScript\nRuby\nClass 5\n0.880\n0.850\n0.852\n0.483\n0.477\nClass 4\n0.860\n0.830\n0.832\n0.480\n0.475\nClass 3\n0.750\n0.720\n0.721\n0.480\n0.475\nClass 2\n0.620\n0.600\n0.601\n0.478\n0.473\nTable 14: Claude3.5’s consistent performance across\nlanguage classes and programming languages, with\nscores remaining stable above 0.477.\nImplications for Future Model Development\nThe significant underperformance in JavaScript and\nRuby across language classes indicates a need for\nenhanced training in scripting languages. Models\nlike GPT-4 and Claude3.5 excel in higher classes,\nparticularly in Python and Java, but gaps in lower\nclasses and scripting languages suggest a focus on\ndiversifying training data to boost adaptability.\nK\nEvaluating Prompt Translation by GPT4\nLanguage\nClass\nBERTScore\nCometKiwi\nLanguage\nClass\nBERTScore\nCometKiwi\narb_Arab\n5\n0.927\n0.807\ntha_Thai\n3\n0.874\n0.749\ndeu_Latn\n5\n0.948\n0.826\nukr_Cyrl\n3\n0.872\n0.722\neng_Latn\n5\n1.000\n0.930\nurd_Arab\n3\n0.841\n0.682\nfra_Latn\n5\n0.927\n0.807\nuzn_Latn\n3\n0.885\n0.740\njpn_Jpan\n5\n0.948\n0.807\nzsm_Latn\n3\n0.890\n0.711\nspa_Latn\n5\n0.927\n0.839\namh_Ethi\n2\n0.825\n0.690\nzho_Hans\n5\n0.921\n0.784\ngle_Latn\n2\n0.824\n0.666\ncat_Latn\n4\n0.911\n0.784\nhau_Latn\n2\n0.810\n0.666\nces_Latn\n4\n0.914\n0.799\nibo_Latn\n2\n0.842\n0.685\neus_Latn\n4\n0.920\n0.754\nkin_Latn\n2\n0.838\n0.665\nfin_Latn\n4\n0.870\n0.798\nlao_Laoo\n2\n0.844\n0.690\nhin_Deva\n4\n0.879\n0.795\nlug_Latn\n2\n0.824\n0.685\nhrv_Latn\n4\n0.921\n0.768\nlua_Latn\n2\n0.842\n0.690\nhun_Latn\n4\n0.879\n0.784\nluo_Latn\n2\n0.824\n0.685\nita_Latn\n4\n0.916\n0.768\nmar_Deva\n2\n0.811\n0.676\nkor_Hang\n4\n0.930\n0.768\nnpi_Deva\n2\n0.812\n0.666\nnld_Latn\n4\n0.887\n0.799\norm_Latn\n2\n0.842\n0.665\npes_Arab\n4\n0.929\n0.768\nprs_Arab\n2\n0.827\n0.685\npol_Latn\n4\n0.894\n0.754\nquc_Latn\n2\n0.842\n0.665\npor_Latn\n4\n0.879\n0.798\nsag_Latn\n2\n0.811\n0.676\nrus_Cyrl\n4\n0.929\n0.754\nsna_Latn\n2\n0.812\n0.666\nsrp_Cyrl\n4\n0.879\n0.795\nsrd_Latn\n2\n0.842\n0.665\nswe_Latn\n4\n0.914\n0.798\ntso_Latn\n2\n0.842\n0.665\ntur_Latn\n4\n0.920\n0.754\nuzb_Latn\n2\n0.827\n0.685\nvie_Latn\n4\n0.894\n0.795\nzdj_Arab\n2\n0.811\n0.676\narb_Latn\n3\n0.894\n0.731\nfuv_Latn\n1\n0.844\n0.666\nafr_Latn\n3\n0.890\n0.711\ngaz_Latn\n1\n0.839\n0.665\narz_Arab\n3\n0.891\n0.748\nhin_Latn\n1\n0.841\n0.682\nben_Beng\n3\n0.872\n0.749\njav_Latn\n1\n0.776\n0.508\nbos_Latn\n3\n0.900\n0.731\nkan_Knda\n1\n0.755\n0.489\nbul_Cyrl\n3\n0.886\n0.677\nkhm_Khmr\n1\n0.787\n0.529\nceb_Latn\n3\n0.841\n0.682\nkir_Cyrl\n1\n0.765\n0.578\ndan_Latn\n3\n0.827\n0.733\nkmr_Latn\n1\n0.784\n0.567\nell_Grek\n3\n0.887\n0.727\nmal_Mlym\n1\n0.785\n0.529\nest_Latn\n3\n0.885\n0.715\nmkd_Cyrl\n1\n0.737\n0.578\nglg_Latn\n3\n0.867\n0.701\nmya_Mymr\n1\n0.760\n0.579\nheb_Hebr\n3\n0.895\n0.673\nnob_Latn\n1\n0.750\n0.515\nind_Latn\n3\n0.874\n0.677\nory_Orya\n1\n0.776\n0.579\nkat_Geor\n3\n0.892\n0.741\nsnd_Arab\n1\n0.788\n0.432\nkaz_Cyrl\n3\n0.850\n0.745\nsom_Latn\n1\n0.750\n0.512\nlit_Latn\n3\n0.886\n0.740\nsun_Latn\n1\n0.778\n0.567\nlvs_Latn\n3\n0.827\n0.688\ntel_Telu\n1\n0.745\n0.560\nron_Latn\n3\n0.895\n0.722\nuig_Arab\n1\n0.741\n0.529\nslk_Latn\n3\n0.886\n0.731\nydd_Hebr\n1\n0.768\n0.508\nslv_Latn\n3\n0.890\n0.745\nzho_Hant\n1\n0.788\n0.529\ntam_Taml\n3\n0.887\n0.677\nsin_Sinh\n0\n0.690\n0.410\ntgl_Latn\n3\n0.827\n0.731\nTable 15: Evaluating the quality of machine translation by GPT4 using BERTScore and CometKiwi. The languages\nare given as Flores-200 codes.\nLanguage\nClass\nBERTScore\nLanguage\nClass\nBERTScore\nace_Arab\n1\n0.666\nquy_Latn\n1\n0.722\nace_Latn\n1\n0.719\nsag_Latn\n1\n0.735\nacm_Arab\n1\n0.680\nsat_Olck\n1\n0.717\nacq_Arab\n1\n0.714\nscn_Latn\n1\n0.730\naeb_Arab\n1\n0.664\nsmo_Latn\n1\n0.738\najp_Arab\n1\n0.675\nsna_Latn\n1\n0.679\naka_Latn\n1\n0.683\nsrd_Latn\n1\n0.705\nals_Latn\n1\n0.692\nssw_Latn\n1\n0.739\napc_Arab\n1\n0.697\nszl_Latn\n1\n0.716\nars_Arab\n1\n0.727\ntat_Cyrl\n1\n0.724\nary_Arab\n1\n0.700\ntgk_Cyrl\n1\n0.673\nast_Latn\n1\n0.716\ntpi_Latn\n1\n0.730\nayr_Latn\n1\n0.693\ntso_Latn\n1\n0.696\nazb_Arab\n1\n0.673\ntuk_Latn\n1\n0.674\nbak_Cyrl\n1\n0.702\ntum_Latn\n1\n0.724\nbho_Deva\n1\n0.708\ntwi_Latn\n1\n0.665\nbjn_Arab\n1\n0.686\nvec_Latn\n1\n0.698\nbjn_Latn\n1\n0.701\nwar_Latn\n1\n0.735\nbod_Tibt\n1\n0.735\nawa_Deva\n0\n0.600\nbug_Latn\n1\n0.678\nbam_Latn\n0\n0.691\nckb_Arab\n1\n0.662\nban_Latn\n0\n0.677\ncrh_Latn\n1\n0.682\nbem_Latn\n0\n0.656\ndik_Latn\n1\n0.727\ncjk_Latn\n0\n0.691\ndzo_Tibt\n1\n0.727\ndyu_Latn\n0\n0.648\newe_Latn\n1\n0.706\nfon_Latn\n0\n0.685\nfao_Latn\n1\n0.662\nfuv_Latn\n0\n0.613\nfij_Latn\n1\n0.730\ngrn_Latn\n0\n0.680\nfur_Latn\n1\n0.733\nhat_Latn\n0\n0.649\ngaz_Latn\n1\n0.669\nhne_Deva\n0\n0.633\nibo_Latn\n1\n0.680\nkac_Latn\n0\n0.665\nilo_Latn\n1\n0.723\nkam_Latn\n0\n0.623\nkab_Latn\n1\n0.690\nkbp_Latn\n0\n0.646\nkas_Arab\n1\n0.677\nkea_Latn\n0\n0.698\nkas_Deva\n1\n0.698\nkmb_Latn\n0\n0.680\nkhk_Cyrl\n1\n0.686\nknc_Arab\n0\n0.612\nkik_Latn\n1\n0.694\nknc_Latn\n0\n0.684\nkin_Latn\n1\n0.704\nkon_Latn\n0\n0.688\nlij_Latn\n1\n0.683\nlua_Latn\n0\n0.671\nlim_Latn\n1\n0.691\nluo_Latn\n0\n0.667\nlin_Latn\n1\n0.699\nlus_Latn\n0\n0.603\nlmo_Latn\n1\n0.662\nmag_Deva\n0\n0.665\nltg_Latn\n1\n0.720\nmni_Beng\n0\n0.629\nltz_Latn\n1\n0.688\nmos_Latn\n0\n0.610\nlug_Latn\n1\n0.685\nnso_Latn\n0\n0.601\nmai_Deva\n1\n0.664\nnus_Latn\n0\n0.614\nmin_Arab\n1\n0.669\nnya_Latn\n0\n0.649\nmin_Latn\n1\n0.695\nprs_Arab\n0\n0.698\nmri_Latn\n1\n0.706\nrun_Latn\n0\n0.608\nnno_Latn\n1\n0.703\nshn_Mymr\n0\n0.670\nnpi_Deva\n1\n0.671\nsot_Latn\n0\n0.624\noci_Latn\n1\n0.732\ntaq_Latn\n0\n0.658\npag_Latn\n1\n0.662\ntaq_Tfng\n0\n0.677\npap_Latn\n1\n0.728\ntzm_Tfng\n0\n0.622\npbt_Arab\n1\n0.693\numb_Latn\n0\n0.643\nplt_Latn\n1\n0.696\nyue_Hant\n0\n0.696\nTable 16: Evaluating the quality of machine translation by GPT4 using BERTScore. These languages are not\nsupported by CometKiwi. The languages are given as Flores-200 codes.\nL\nEvaluating Prompt Translation by NLLB\nLanguage\nClass\nBERTScore\nCometKiwi\nLanguage\nClass\nBERTScore\nCometKiwi\narb_Arab\n5\n0.941\n0.798\ntha_Thai\n3\n0.881\n0.704\ndeu_Latn\n5\n0.902\n0.809\nukr_Cyrl\n3\n0.883\n0.685\neng_Latn\n5\n1.000\n0.910\nurd_Arab\n3\n0.872\n0.697\nfra_Latn\n5\n0.917\n0.787\nuzn_Latn\n3\n0.875\n0.697\njpn_Jpan\n5\n0.935\n0.807\nzsm_Latn\n3\n0.864\n0.697\nspa_Latn\n5\n0.935\n0.809\namh_Ethi\n2\n0.817\n0.597\nzho_Hans\n5\n0.911\n0.831\ngle_Latn\n2\n0.816\n0.555\ncat_Latn\n4\n0.909\n0.777\nhau_Latn\n2\n0.827\n0.574\nces_Latn\n4\n0.899\n0.743\nibo_Latn\n2\n0.816\n0.579\neus_Latn\n4\n0.877\n0.719\nkin_Latn\n2\n0.817\n0.573\nfin_Latn\n4\n0.875\n0.704\nlao_Laoo\n2\n0.810\n0.585\nhin_Deva\n4\n0.875\n0.697\nlug_Latn\n2\n0.817\n0.573\nhrv_Latn\n4\n0.875\n0.697\nlua_Latn\n2\n0.816\n0.579\nhun_Latn\n4\n0.872\n0.685\nluo_Latn\n2\n0.818\n0.585\nita_Latn\n4\n0.872\n0.719\nmar_Deva\n2\n0.817\n0.573\nkor_Hang\n4\n0.872\n0.685\nnpi_Deva\n2\n0.816\n0.579\nnld_Latn\n4\n0.875\n0.704\norm_Latn\n2\n0.817\n0.573\npes_Arab\n4\n0.875\n0.697\nprs_Arab\n2\n0.816\n0.579\npol_Latn\n4\n0.872\n0.685\nquc_Latn\n2\n0.818\n0.585\npor_Latn\n4\n0.875\n0.704\nsag_Latn\n2\n0.817\n0.573\nrus_Cyrl\n4\n0.875\n0.697\nsna_Latn\n2\n0.816\n0.579\nsrp_Cyrl\n4\n0.872\n0.685\nsrd_Latn\n2\n0.817\n0.573\nswe_Latn\n4\n0.875\n0.704\ntso_Latn\n2\n0.818\n0.585\ntur_Latn\n4\n0.872\n0.685\nuzb_Latn\n2\n0.817\n0.573\nvie_Latn\n4\n0.875\n0.704\nzdj_Arab\n2\n0.816\n0.579\narb_Latn\n3\n0.875\n0.697\nfuv_Latn\n1\n0.818\n0.585\nafr_Latn\n3\n0.875\n0.704\ngaz_Latn\n1\n0.817\n0.573\narz_Arab\n3\n0.872\n0.685\nhin_Latn\n1\n0.816\n0.579\nben_Beng\n3\n0.872\n0.697\njav_Latn\n1\n0.758\n0.535\nbul_Cyrl\n3\n0.875\n0.697\nkan_Knda\n1\n0.740\n0.577\nceb_Latn\n3\n0.872\n0.716\nkhm_Khmr\n1\n0.754\n0.561\ndan_Latn\n3\n0.851\n0.666\nkir_Cyrl\n1\n0.757\n0.583\nell_Grek\n3\n0.883\n0.709\nkmr_Latn\n1\n0.770\n0.579\nest_Latn\n3\n0.877\n0.661\nmal_Mlym\n1\n0.736\n0.550\nglg_Latn\n3\n0.864\n0.697\nmkd_Cyrl\n1\n0.736\n0.559\nheb_Hebr\n3\n0.828\n0.701\nmya_Mymr\n1\n0.770\n0.582\nind_Latn\n3\n0.864\n0.697\nnob_Latn\n1\n0.766\n0.535\nkat_Geor\n3\n0.880\n0.709\nory_Orya\n1\n0.743\n0.582\nkaz_Cyrl\n3\n0.877\n0.719\nsnd_Arab\n1\n0.743\n0.582\nlit_Latn\n3\n0.872\n0.697\nsom_Latn\n1\n0.770\n0.520\nlvs_Latn\n3\n0.880\n0.661\nsun_Latn\n1\n0.743\n0.540\nron_Latn\n3\n0.864\n0.713\ntel_Telu\n1\n0.754\n0.540\nslk_Latn\n3\n0.828\n0.716\nuig_Arab\n1\n0.751\n0.579\nslv_Latn\n3\n0.879\n0.685\nydd_Hebr\n1\n0.757\n0.556\ntam_Taml\n3\n0.877\n0.716\nzho_Hant\n1\n0.736\n0.583\ntgl_Latn\n3\n0.851\n0.713\nsin_Sinh\n0\n0.645\n0.490\nTable 17: Evaluating the quality of machine translation by NLLB using BERTScore and CometKiwi. The languages\nare given as Flores-200 codes.\nLanguage\nClass\nBERTScore\nLanguage\nClass\nBERTScore\nace_Arab\n1\n0.672\nquy_Latn\n1\n0.700\nace_Latn\n1\n0.716\nsag_Latn\n1\n0.702\nacm_Arab\n1\n0.664\nsat_Olck\n1\n0.681\nacq_Arab\n1\n0.669\nscn_Latn\n1\n0.668\naeb_Arab\n1\n0.664\nsmo_Latn\n1\n0.739\najp_Arab\n1\n0.690\nsna_Latn\n1\n0.679\naka_Latn\n1\n0.720\nsrd_Latn\n1\n0.694\nals_Latn\n1\n0.716\nssw_Latn\n1\n0.735\napc_Arab\n1\n0.683\nszl_Latn\n1\n0.739\nars_Arab\n1\n0.669\ntat_Cyrl\n1\n0.716\nary_Arab\n1\n0.666\ntgk_Cyrl\n1\n0.672\nast_Latn\n1\n0.672\ntpi_Latn\n1\n0.670\nayr_Latn\n1\n0.684\ntso_Latn\n1\n0.706\nazb_Arab\n1\n0.688\ntuk_Latn\n1\n0.690\nbak_Cyrl\n1\n0.699\ntum_Latn\n1\n0.692\nbho_Deva\n1\n0.719\ntwi_Latn\n1\n0.705\nbjn_Arab\n1\n0.734\nvec_Latn\n1\n0.709\nbjn_Latn\n1\n0.668\nwar_Latn\n1\n0.684\nbod_Tibt\n1\n0.692\nawa_Deva\n0\n0.644\nbug_Latn\n1\n0.670\nbam_Latn\n0\n0.607\nckb_Arab\n1\n0.733\nban_Latn\n0\n0.645\ncrh_Latn\n1\n0.670\nbem_Latn\n0\n0.613\ndik_Latn\n1\n0.710\ncjk_Latn\n0\n0.658\ndzo_Tibt\n1\n0.726\ndyu_Latn\n0\n0.664\newe_Latn\n1\n0.737\nfon_Latn\n0\n0.694\nfao_Latn\n1\n0.710\nfuv_Latn\n0\n0.615\nfij_Latn\n1\n0.689\ngrn_Latn\n0\n0.677\nfur_Latn\n1\n0.739\nhat_Latn\n0\n0.666\ngaz_Latn\n1\n0.708\nhne_Deva\n0\n0.686\nibo_Latn\n1\n0.687\nkac_Latn\n0\n0.651\nilo_Latn\n1\n0.722\nkam_Latn\n0\n0.672\nkab_Latn\n1\n0.680\nkbp_Latn\n0\n0.600\nkas_Arab\n1\n0.684\nkea_Latn\n0\n0.672\nkas_Deva\n1\n0.716\nkmb_Latn\n0\n0.636\nkhk_Cyrl\n1\n0.725\nknc_Arab\n0\n0.615\nkik_Latn\n1\n0.668\nknc_Latn\n0\n0.611\nkin_Latn\n1\n0.705\nkon_Latn\n0\n0.637\nlij_Latn\n1\n0.719\nlua_Latn\n0\n0.606\nlim_Latn\n1\n0.706\nluo_Latn\n0\n0.679\nlin_Latn\n1\n0.723\nlus_Latn\n0\n0.632\nlmo_Latn\n1\n0.690\nmag_Deva\n0\n0.600\nltg_Latn\n1\n0.681\nmni_Beng\n0\n0.655\nltz_Latn\n1\n0.727\nmos_Latn\n0\n0.688\nlug_Latn\n1\n0.712\nnso_Latn\n0\n0.635\nmai_Deva\n1\n0.710\nnus_Latn\n0\n0.674\nmin_Arab\n1\n0.666\nnya_Latn\n0\n0.699\nmin_Latn\n1\n0.724\nprs_Arab\n0\n0.609\nmri_Latn\n1\n0.726\nrun_Latn\n0\n0.615\nnno_Latn\n1\n0.703\nshn_Mymr\n0\n0.657\nnpi_Deva\n1\n0.675\nsot_Latn\n0\n0.601\noci_Latn\n1\n0.735\ntaq_Latn\n0\n0.658\npag_Latn\n1\n0.709\ntaq_Tfng\n0\n0.677\npap_Latn\n1\n0.664\ntzm_Tfng\n0\n0.607\npbt_Arab\n1\n0.696\numb_Latn\n0\n0.643\nplt_Latn\n1\n0.683\nyue_Hant\n0\n0.677\nTable 18: Evaluating the quality of machine translation by NLLB using BERTScore. These languages are not\nsupported by CometKiwi. The languages are given as Flores-200 codes.\nM\nEvaluating Prompt Translation by Google Translate\nLanguage\nClass\nBERTScore\nCometKiwi\nLanguage\nClass\nBERTScore\nCometKiwi\narb_Arab\n5\n0.886\n0.802\nlua_Latn\n2\n0.825\n0.645\ndeu_Latn\n5\n0.910\n0.811\nibo_Latn\n2\n0.816\n0.637\neng_Latn\n5\n1.000\n0.950\nkin_Latn\n2\n0.829\n0.617\nfra_Latn\n5\n0.924\n0.802\nlao_Laoo\n2\n0.829\n0.645\njpn_Jpan\n5\n0.910\n0.812\nlug_Latn\n2\n0.835\n0.632\nspa_Latn\n5\n0.900\n0.802\nluo_Latn\n2\n0.832\n0.617\nzho_Hans\n5\n0.933\n0.807\nmar_Deva\n2\n0.836\n0.637\ncat_Latn\n4\n0.903\n0.689\nnpi_Deva\n2\n0.835\n0.632\nces_Latn\n4\n0.894\n0.722\norm_Latn\n2\n0.816\n0.645\neus_Latn\n4\n0.898\n0.725\nprs_Arab\n2\n0.832\n0.637\nfin_Latn\n4\n0.842\n0.655\nquc_Latn\n2\n0.832\n0.617\nhin_Deva\n4\n0.853\n0.630\nsag_Latn\n2\n0.835\n0.632\nhrv_Latn\n4\n0.875\n0.679\nsna_Latn\n2\n0.829\n0.645\nhun_Latn\n4\n0.871\n0.681\nsrd_Latn\n2\n0.836\n0.637\nita_Latn\n4\n0.899\n0.690\ntso_Latn\n2\n0.823\n0.604\nkor_Hang\n4\n0.887\n0.677\nuzb_Latn\n2\n0.829\n0.645\nnld_Latn\n4\n0.886\n0.676\nzdj_Arab\n2\n0.835\n0.632\npes_Arab\n4\n0.886\n0.681\nfuv_Latn\n1\n0.838\n0.637\npol_Latn\n4\n0.881\n0.666\ngaz_Latn\n1\n0.841\n0.637\npor_Latn\n4\n0.880\n0.689\nhin_Latn\n1\n0.841\n0.682\nrus_Cyrl\n4\n0.887\n0.667\njav_Latn\n1\n0.759\n0.554\nsrp_Cyrl\n4\n0.880\n0.669\nkan_Knda\n1\n0.759\n0.554\nswe_Latn\n4\n0.878\n0.676\nkhm_Khmr\n1\n0.767\n0.517\ntur_Latn\n4\n0.871\n0.689\nkir_Cyrl\n1\n0.766\n0.522\nvie_Latn\n4\n0.880\n0.677\nkmr_Latn\n1\n0.754\n0.559\narb_Latn\n3\n0.898\n0.726\nmal_Mlym\n1\n0.766\n0.573\nafr_Latn\n3\n0.871\n0.689\nmkd_Cyrl\n1\n0.759\n0.573\narz_Arab\n3\n0.880\n0.667\nmya_Mymr\n1\n0.767\n0.576\nben_Beng\n3\n0.880\n0.689\nory_Orya\n1\n0.766\n0.558\nbos_Latn\n3\n0.910\n0.668\nsnd_Arab\n1\n0.754\n0.576\nbul_Cyrl\n3\n0.878\n0.655\nsom_Latn\n1\n0.767\n0.576\nceb_Latn\n3\n0.904\n0.666\nsun_Latn\n1\n0.766\n0.554\ndan_Latn\n3\n0.906\n0.663\ntel_Telu\n1\n0.766\n0.574\nell_Grek\n3\n0.899\n0.667\nuig_Arab\n1\n0.766\n0.558\nest_Latn\n3\n0.893\n0.645\nydd_Hebr\n1\n0.727\n0.574\nglg_Latn\n3\n0.837\n0.635\nzho_Hant\n1\n0.766\n0.574\nheb_Hebr\n3\n0.884\n0.639\nals_Latn\n1\n0.665\n–\nind_Latn\n3\n0.880\n0.669\nazb_Arab\n1\n0.705\n–\nkat_Geor\n3\n0.871\n0.661\nckb_Arab\n1\n0.720\n–\nkaz_Cyrl\n3\n0.841\n0.654\nkhk_Cyrl\n1\n0.684\n–\nlat_Latn\n3\n0.910\n0.667\nmri_Latn\n1\n0.680\n–\nlit_Latn\n3\n0.897\n0.630\nnpi_Deva\n1\n0.662\n–\nlvs_Latn\n3\n0.878\n0.668\nplt_Latn\n1\n0.673\n–\nron_Latn\n3\n0.884\n0.635\nsna_Latn\n1\n0.713\n–\nslk_Latn\n3\n0.899\n0.645\ncos_Latn\n1\n0.714\n–\nslv_Latn\n3\n0.897\n0.667\nhaw_Latn\n1\n0.719\n–\ntam_Taml\n3\n0.884\n0.635\nibo_Latn\n1\n0.700\n–\ntgl_Latn\n3\n0.878\n0.667\nltz_Latn\n1\n0.711\n–\ntha_Thai\n3\n0.884\n0.635\nnno_Latn\n1\n0.721\n–\nukr_Cyrl\n3\n0.904\n0.668\npbt_Arab\n1\n0.686\n–\nurd_Arab\n3\n0.884\n0.655\nsmo_Latn\n1\n0.733\n–\nuzn_Latn\n3\n0.910\n0.630\ntgk_Cyrl\n1\n0.693\n–\nzsm_Latn\n3\n0.906\n0.645\nfry_Latn\n0\n0.683\n0.522\namh_Ethi\n2\n0.835\n0.623\nsin_Sinh\n0\n0.685\n0.410\ngle_Latn\n2\n0.835\n0.645\nhat_Latn\n0\n0.608\n–\nhau_Latn\n2\n0.823\n0.604\nhmn_Latn\n0\n0.624\n–\nibo_Latn\n2\n0.816\n0.637\nsot_Latn\n0\n0.623\n–\nkin_Latn\n2\n0.829\n0.617\nmni_Beng\n0\n0.650\n–\nlao_Laoo\n2\n0.829\n0.645\nnya_Latn\n0\n0.682\n–\nlug_Latn\n2\n0.835\n0.632\nTable 19: Evaluating the quality of machine translation by Google Translator using BERTScore and CometKiwi. The\nlanguages are given as Flores-200 codes.\nN\nEvaluating Prompt Quality in mHumanEval\nLanguage\nClass\nBERTScore\nCometKiwi\nLanguage\nClass\nBERTScore\nCometKiwi\neng_Latn\n5\n1.000\n0.961\nurd_Arab\n3\n0.911\n0.782\nspa_Latn\n5\n0.98\n0.919\nbul_Cyrl\n3\n0.956\n0.777\ndeu_Latn\n5\n0.99\n0.927\nind_Latn\n3\n0.944\n0.777\nfra_Latn\n5\n0.98\n0.896\ntam_Taml\n3\n0.957\n0.777\nzho_Hans\n5\n0.96\n0.89\nheb_Hebr\n3\n0.965\n0.773\njpn_Jpan\n5\n0.97\n0.88\namh_Ethi\n2\n0.895\n0.77\narb_Arab\n5\n0.96\n0.867\nmlt_Latn\n2\n0.904\n0.77\nces_Latn\n4\n0.964\n0.839\nisl_Latn\n2\n0.898\n0.765\nnld_Latn\n4\n0.937\n0.839\ntir_Ethi\n2\n0.913\n0.765\nfin_Latn\n4\n0.92\n0.838\nyor_Latn\n2\n0.913\n0.765\npor_Latn\n4\n0.929\n0.838\nzul_Latn\n2\n0.913\n0.765\nswe_Latn\n4\n0.964\n0.838\nlao_Laoo\n2\n0.887\n0.756\nhin_Deva\n4\n0.929\n0.835\nmar_Deva\n2\n0.881\n0.756\nsrp_Cyrl\n4\n0.929\n0.835\nxho_Latn\n2\n0.881\n0.756\nvie_Latn\n4\n0.944\n0.835\ngle_Latn\n2\n0.894\n0.746\ncat_Latn\n4\n0.961\n0.824\npan_Guru\n2\n0.916\n0.746\nhun_Latn\n4\n0.929\n0.824\nsan_Deva\n2\n0.925\n0.746\nhrv_Latn\n4\n0.971\n0.808\nwol_Latn\n2\n0.884\n0.746\nita_Latn\n4\n0.966\n0.808\nhau_Latn\n2\n0.884\n0.745\nkor_Hang\n4\n0.98\n0.808\nswh_Latn\n2\n0.913\n0.745\npes_Arab\n4\n0.979\n0.808\ntsn_Latn\n2\n0.925\n0.745\neus_Latn\n4\n0.97\n0.794\nguj_Gujr\n1\n0.828\n0.717\npol_Latn\n4\n0.944\n0.794\nepo_Latn\n1\n0.813\n0.709\nrus_Cyrl\n4\n0.979\n0.794\nmya_Mymr\n1\n0.828\n0.709\ntur_Latn\n4\n0.97\n0.794\nory_Orya\n1\n0.84\n0.709\nben_Beng\n3\n0.942\n0.849\nkir_Cyrl\n1\n0.819\n0.708\ntha_Thai\n3\n0.944\n0.849\nmkd_Cyrl\n1\n0.873\n0.708\narz_Arab\n3\n0.961\n0.848\ncym_Latn\n1\n0.865\n0.707\nkaz_Cyrl\n3\n0.92\n0.845\nkmr_Latn\n1\n0.86\n0.697\nslv_Latn\n3\n0.96\n0.845\nsun_Latn\n1\n0.854\n0.697\nkat_Geor\n3\n0.962\n0.841\ngla_Latn\n1\n0.846\n0.69\nlit_Latn\n3\n0.956\n0.84\ntel_Telu\n1\n0.823\n0.69\nuzn_Latn\n3\n0.955\n0.84\nkhm_Khmr\n1\n0.867\n0.659\nbos_Latn\n3\n0.97\n0.839\nmal_Mlym\n1\n0.871\n0.659\ndan_Latn\n3\n0.897\n0.833\nuig_Arab\n1\n0.809\n0.659\narb_Latn\n3\n0.964\n0.831\nzho_Hant\n1\n0.876\n0.659\nslk_Latn\n3\n0.956\n0.831\nasm_Beng\n1\n0.871\n0.645\ntgl_Latn\n3\n0.897\n0.831\nnob_Latn\n1\n0.823\n0.645\nell_Grek\n3\n0.957\n0.827\nhye_Armn\n1\n0.852\n0.642\nron_Latn\n3\n0.965\n0.822\nsom_Latn\n1\n0.812\n0.642\nukr_Cyrl\n3\n0.942\n0.822\njav_Latn\n1\n0.828\n0.638\nest_Latn\n3\n0.955\n0.815\nydd_Hebr\n1\n0.875\n0.638\nafr_Latn\n3\n0.96\n0.811\nkan_Knda\n1\n0.825\n0.619\nzsm_Latn\n3\n0.96\n0.811\nbel_Cyrl\n1\n0.809\n0.613\nglg_Latn\n3\n0.937\n0.801\nazj_Latn\n1\n0.829\n0.562\nlvs_Latn\n3\n0.897\n0.788\nsnd_Arab\n1\n0.851\n0.562\nceb_Latn\n3\n0.911\n0.782\nsin_Sinh\n0\n0.859\n0.56\nTable 20: Observing improved prompt quality in mHumanEval upon choosing the best ones from 13 candidates each,\nevaluated using BERTScore and CometKiwi. The languages are given as Flores-200 codes.\nLanguage\nClass\nBERTScore\nLanguage\nClass\nBERTScore\nace_Arab\n1\n0.806\nquy_Latn\n1\n0.862\nace_Latn\n1\n0.859\nsag_Latn\n1\n0.875\nacm_Arab\n1\n0.82\nsat_Olck\n1\n0.857\nacq_Arab\n1\n0.854\nscn_Latn\n1\n0.87\naeb_Arab\n1\n0.804\nsmo_Latn\n1\n0.878\najp_Arab\n1\n0.815\nsna_Latn\n1\n0.819\naka_Latn\n1\n0.823\nsrd_Latn\n1\n0.845\nals_Latn\n1\n0.832\nssw_Latn\n1\n0.879\napc_Arab\n1\n0.837\nszl_Latn\n1\n0.856\nars_Arab\n1\n0.867\ntat_Cyrl\n1\n0.864\nary_Arab\n1\n0.84\ntgk_Cyrl\n1\n0.813\nast_Latn\n1\n0.856\ntpi_Latn\n1\n0.87\nayr_Latn\n1\n0.833\ntso_Latn\n1\n0.836\nazb_Arab\n1\n0.813\ntuk_Latn\n1\n0.814\nbak_Cyrl\n1\n0.842\ntum_Latn\n1\n0.864\nbho_Deva\n1\n0.848\ntwi_Latn\n1\n0.805\nbjn_Arab\n1\n0.826\nvec_Latn\n1\n0.838\nbjn_Latn\n1\n0.841\nwar_Latn\n1\n0.875\nbod_Tibt\n1\n0.875\nawa_Deva\n0\n0.75\nbug_Latn\n1\n0.818\nbam_Latn\n0\n0.841\nckb_Arab\n1\n0.802\nban_Latn\n0\n0.827\ncrh_Latn\n1\n0.822\nbem_Latn\n0\n0.806\ndik_Latn\n1\n0.867\ncjk_Latn\n0\n0.841\ndzo_Tibt\n1\n0.867\ndyu_Latn\n0\n0.798\newe_Latn\n1\n0.846\nfon_Latn\n0\n0.835\nfao_Latn\n1\n0.802\nfuv_Latn\n0\n0.763\nfij_Latn\n1\n0.87\ngrn_Latn\n0\n0.83\nfur_Latn\n1\n0.873\nhat_Latn\n0\n0.799\ngaz_Latn\n1\n0.809\nhne_Deva\n0\n0.783\nibo_Latn\n1\n0.82\nkac_Latn\n0\n0.815\nilo_Latn\n1\n0.863\nkam_Latn\n0\n0.773\nkab_Latn\n1\n0.83\nkbp_Latn\n0\n0.796\nkas_Arab\n1\n0.817\nkea_Latn\n0\n0.848\nkas_Deva\n1\n0.838\nkmb_Latn\n0\n0.83\nkhk_Cyrl\n1\n0.826\nknc_Arab\n0\n0.762\nkik_Latn\n1\n0.834\nknc_Latn\n0\n0.834\nkin_Latn\n1\n0.844\nkon_Latn\n0\n0.838\nlij_Latn\n1\n0.823\nlua_Latn\n0\n0.821\nlim_Latn\n1\n0.831\nluo_Latn\n0\n0.817\nlin_Latn\n1\n0.839\nlus_Latn\n0\n0.753\nlmo_Latn\n1\n0.802\nmag_Deva\n0\n0.815\nltg_Latn\n1\n0.86\nmni_Beng\n0\n0.779\nltz_Latn\n1\n0.828\nmos_Latn\n0\n0.76\nlug_Latn\n1\n0.825\nnso_Latn\n0\n0.751\nmai_Deva\n1\n0.804\nnus_Latn\n0\n0.764\nmin_Arab\n1\n0.809\nnya_Latn\n0\n0.799\nmin_Latn\n1\n0.835\nprs_Arab\n0\n0.848\nmri_Latn\n1\n0.846\nrun_Latn\n0\n0.758\nnno_Latn\n1\n0.843\nshn_Mymr\n0\n0.82\nnpi_Deva\n1\n0.811\nsot_Latn\n0\n0.774\noci_Latn\n1\n0.872\ntaq_Latn\n0\n0.836\npag_Latn\n1\n0.802\ntaq_Tfng\n0\n0.774\npap_Latn\n1\n0.868\ntzm_Tfng\n0\n0.772\npbt_Arab\n1\n0.833\numb_Latn\n0\n0.795\nplt_Latn\n1\n0.836\nyue_Hant\n0\n0.846\nTable 21: Observing improved prompt quality in mHumanEval upon choosing the best ones from 13 candidates\neach, evaluated using BERTScore. These languages are not supported by CometKiwi. The languages are given as\nFlores-200 codes.\nO\nEvaluation Results on mHumanEval\nLanguage\nClass\nClaude3.5\nGPT4o\nGPT3.5\nDeepSeek-Coder\nWizardCoder\nAya\narb_Arab\n5\n0.831\n0.846\n0.719\n0.859\n0.650\n0.590\ndeu_Latn\n5\n0.846\n0.833\n0.730\n0.863\n0.670\n0.620\neng_Latn\n5\n0.938\n0.910\n0.770\n0.902\n0.800\n0.650\nfra_Latn\n5\n0.835\n0.850\n0.693\n0.849\n0.650\n0.608\njpn_Jpan\n5\n0.896\n0.868\n0.757\n0.849\n0.670\n0.609\nspa_Latn\n5\n0.880\n0.852\n0.759\n0.854\n0.610\n0.609\nzho_Hans\n5\n0.838\n0.810\n0.720\n0.933\n0.590\n0.570\nTable 22: Comparing LLMs’ performance (% in Pass@1) on mHumanEval - Class 5 languages. The languages are\ngiven as Flores-200 codes.\nLanguage\nClass\nClaude3.5\nGPT4o\nGPT3.5\nDeepSeek-Coder\nWizardCoder\nAya\ncat_Latn\n4\n0.764\n0.832\n0.613\n0.827\n0.420\n0.584\nces_Latn\n4\n0.908\n0.837\n0.649\n0.883\n0.390\n0.591\neus_Latn\n4\n0.880\n0.884\n0.617\n0.902\n0.480\n0.599\nfin_Latn\n4\n0.857\n0.882\n0.611\n0.882\n0.390\n0.565\nhin_Deva\n4\n0.854\n0.859\n0.600\n0.872\n0.480\n0.572\nhrv_Latn\n4\n0.831\n0.833\n0.608\n0.865\n0.450\n0.595\nhun_Latn\n4\n0.838\n0.860\n0.594\n0.824\n0.410\n0.568\nita_Latn\n4\n0.870\n0.860\n0.607\n0.796\n0.430\n0.563\nkor_Hang\n4\n0.814\n0.850\n0.605\n0.909\n0.390\n0.577\nnld_Latn\n4\n0.809\n0.849\n0.649\n0.843\n0.440\n0.546\npes_Arab\n4\n0.885\n0.859\n0.607\n0.902\n0.380\n0.586\npol_Latn\n4\n0.840\n0.850\n0.634\n0.821\n0.390\n0.569\npor_Latn\n4\n0.861\n0.862\n0.657\n0.835\n0.440\n0.576\nrus_Cyrl\n4\n0.814\n0.822\n0.615\n0.831\n0.470\n0.565\nsrp_Cyrl\n4\n0.815\n0.842\n0.591\n0.892\n0.400\n0.595\nswe_Latn\n4\n0.832\n0.840\n0.634\n0.867\n0.380\n0.551\ntur_Latn\n4\n0.867\n0.860\n0.618\n0.882\n0.480\n0.585\nvie_Latn\n4\n0.883\n0.833\n0.637\n0.833\n0.400\n0.591\nTable 23: Comparing LLMs’ performance (% in Pass@1) on mHumanEval - Class 4 languages. The languages are\ngiven as Flores-200 codes.\nLanguage\nClass\nClaude3.5\nGPT4o\nGPT3.5\nDeepSeek-Coder\nWizardCoder\nAya\nafr_Latn\n3\n0.886\n0.846\n0.542\n0.554\n0.180\n0.505\narb_Latn\n3\n0.792\n0.839\n0.548\n0.592\n0.110\n0.541\narz_Arab\n3\n0.807\n0.832\n0.495\n0.399\n0.130\n0.528\nben_Beng\n3\n0.797\n0.792\n0.541\n0.565\n0.090\n0.523\nbos_Latn\n3\n0.826\n0.812\n0.502\n0.746\n0.140\n0.546\nbul_Cyrl\n3\n0.848\n0.796\n0.491\n0.379\n0.120\n0.536\nceb_Latn\n3\n0.850\n0.827\n0.499\n0.473\n0.150\n0.479\ndan_Latn\n3\n0.825\n0.825\n0.504\n0.533\n0.090\n0.527\nell_Grek\n3\n0.742\n0.784\n0.484\n0.479\n0.180\n0.539\nest_Latn\n3\n0.821\n0.786\n0.529\n0.554\n0.090\n0.516\nglg_Latn\n3\n0.820\n0.805\n0.531\n0.407\n0.110\n0.492\nheb_Hebr\n3\n0.837\n0.847\n0.494\n0.449\n0.090\n0.518\nind_Latn\n3\n0.849\n0.809\n0.478\n0.511\n0.080\n0.482\nkat_Geor\n3\n0.836\n0.849\n0.548\n0.507\n0.110\n0.532\nkaz_Cyrl\n3\n0.814\n0.824\n0.522\n0.715\n0.110\n0.543\nlit_Latn\n3\n0.788\n0.812\n0.491\n0.413\n0.140\n0.476\nlvs_Latn\n3\n0.791\n0.798\n0.522\n0.555\n0.140\n0.520\nron_Latn\n3\n0.830\n0.829\n0.507\n0.491\n0.090\n0.488\nslk_Latn\n3\n0.772\n0.822\n0.501\n0.440\n0.120\n0.528\nslv_Latn\n3\n0.784\n0.784\n0.495\n0.619\n0.090\n0.545\ntam_Taml\n3\n0.837\n0.818\n0.532\n0.529\n0.160\n0.526\ntgl_Latn\n3\n0.794\n0.836\n0.485\n0.342\n0.140\n0.473\ntha_Thai\n3\n0.829\n0.823\n0.538\n0.642\n0.080\n0.488\nukr_Cyrl\n3\n0.846\n0.837\n0.546\n0.507\n0.060\n0.505\nurd_Arab\n3\n0.794\n0.823\n0.477\n0.513\n0.110\n0.537\nuzn_Latn\n3\n0.847\n0.838\n0.516\n0.591\n0.170\n0.540\nzsm_Latn\n3\n0.826\n0.804\n0.483\n0.543\n0.080\n0.514\nTable 24: Comparing LLMs’ performance (% in Pass@1) on mHumanEval - Class 3 languages. The languages are\ngiven as Flores-200 codes.\nLanguage\nClass\nClaude3.5\nGPT4o\nGPT3.5\nDeepSeek-Coder\nWizardCoder\nAya\namh_Ethi\n2\n0.765\n0.742\n0.373\n0.214\n0.020\n0.454\ngle_Latn\n2\n0.753\n0.748\n0.466\n0.425\n0.010\n0.449\nhau_Latn\n2\n0.670\n0.739\n0.431\n0.382\n0.070\n0.447\nisl_Latn\n2\n0.795\n0.770\n0.419\n0.606\n0.030\n0.439\nlao_Laoo\n2\n0.783\n0.745\n0.449\n0.440\n0.050\n0.516\nmar_Deva\n2\n0.764\n0.773\n0.464\n0.493\n0.050\n0.519\nmlt_Latn\n2\n0.826\n0.790\n0.348\n0.184\n0.020\n0.439\npan_Guru\n2\n0.730\n0.747\n0.363\n0.356\n0.060\n0.496\nsan_Deva\n2\n0.799\n0.799\n0.391\n0.407\n0.050\n0.496\nswh_Latn\n2\n0.801\n0.794\n0.363\n0.363\n0.030\n0.488\ntir_Ethi\n2\n0.802\n0.792\n0.343\n0.457\n0.020\n0.473\ntsn_Latn\n2\n0.786\n0.781\n0.396\n0.464\n0.040\n0.468\nwol_Latn\n2\n0.835\n0.799\n0.333\n0.435\n0.030\n0.430\nxho_Latn\n2\n0.805\n0.756\n0.486\n0.644\n0.050\n0.490\nyor_Latn\n2\n0.771\n0.773\n0.414\n0.364\n0.060\n0.490\nzul_Latn\n2\n0.847\n0.791\n0.364\n0.267\n0.050\n0.526\nTable 25: Comparing LLMs’ performance (% in Pass@1) on mHumanEval - Class 2 languages. The languages are\ngiven as Flores-200 codes.\nLanguage\nClass\nClaude3.5\nGPT4o\nGPT3.5\nDeepSeek-Coder\nWizardCoder\nAya\nace_Arab\n1\n0.812\n0.736\n0.281\n0.070\n0.050\n0.423\nace_Latn\n1\n0.712\n0.675\n0.338\n0.005\n0.040\n0.433\nacm_Arab\n1\n0.673\n0.671\n0.276\n0.019\n0.010\n0.437\nacq_Arab\n1\n0.786\n0.750\n0.284\n0.019\n0.030\n0.387\naeb_Arab\n1\n0.716\n0.739\n0.324\n0.036\n0.010\n0.413\najp_Arab\n1\n0.640\n0.686\n0.282\n0.046\n0.030\n0.376\naka_Latn\n1\n0.687\n0.708\n0.345\n0.124\n0.020\n0.371\nals_Latn\n1\n0.739\n0.720\n0.272\n0.081\n0.050\n0.414\napc_Arab\n1\n0.686\n0.704\n0.309\n0.055\n0.040\n0.372\nars_Arab\n1\n0.713\n0.699\n0.315\n0.028\n0.040\n0.374\nary_Arab\n1\n0.695\n0.707\n0.330\n0.016\n0.020\n0.436\nasm_Beng\n1\n0.652\n0.671\n0.338\n0.000\n0.040\n0.416\nast_Latn\n1\n0.690\n0.724\n0.298\n0.096\n0.040\n0.405\nayr_Latn\n1\n0.728\n0.733\n0.284\n0.015\n0.060\n0.447\nazb_Arab\n1\n0.684\n0.688\n0.290\n0.046\n0.050\n0.419\nazj_Latn\n1\n0.727\n0.726\n0.296\n0.046\n0.020\n0.435\nbak_Cyrl\n1\n0.743\n0.722\n0.326\n0.049\n0.030\n0.435\nbel_Cyrl\n1\n0.705\n0.705\n0.297\n0.067\n0.020\n0.378\nbho_Deva\n1\n0.705\n0.747\n0.338\n0.065\n0.050\n0.378\nbjn_Arab\n1\n0.709\n0.697\n0.272\n0.092\n0.030\n0.383\nbjn_Latn\n1\n0.733\n0.716\n0.276\n0.062\n0.050\n0.407\nbod_Tibt\n1\n0.769\n0.730\n0.291\n0.012\n0.010\n0.427\nbug_Latn\n1\n0.661\n0.686\n0.350\n0.016\n0.050\n0.447\nckb_Arab\n1\n0.650\n0.685\n0.288\n0.043\n0.030\n0.387\ncrh_Latn\n1\n0.703\n0.731\n0.285\n0.024\n0.020\n0.418\ncym_Latn\n1\n0.779\n0.747\n0.318\n0.000\n0.040\n0.424\ndik_Latn\n1\n0.713\n0.711\n0.335\n0.027\n0.030\n0.382\ndzo_Tibt\n1\n0.682\n0.701\n0.300\n0.014\n0.010\n0.419\nepo_Latn\n1\n0.714\n0.718\n0.313\n0.103\n0.040\n0.409\newe_Latn\n1\n0.653\n0.674\n0.309\n0.051\n0.020\n0.371\nfao_Latn\n1\n0.677\n0.729\n0.318\n0.073\n0.040\n0.400\nfij_Latn\n1\n0.657\n0.713\n0.326\n0.064\n0.050\n0.386\nfur_Latn\n1\n0.713\n0.690\n0.326\n0.042\n0.030\n0.397\ngaz_Latn\n1\n0.692\n0.741\n0.285\n0.000\n0.020\n0.421\ngla_Latn\n1\n0.722\n0.688\n0.319\n0.051\n0.030\n0.382\nguj_Gujr\n1\n0.762\n0.730\n0.273\n0.000\n0.020\n0.392\nhye_Armn\n1\n0.761\n0.735\n0.290\n0.069\n0.020\n0.400\nibo_Latn\n1\n0.732\n0.707\n0.285\n0.000\n0.010\n0.393\nilo_Latn\n1\n0.752\n0.706\n0.336\n0.096\n0.060\n0.381\njav_Latn\n1\n0.771\n0.747\n0.286\n0.004\n0.040\n0.386\nkab_Latn\n1\n0.777\n0.738\n0.337\n0.077\n0.050\n0.434\nkan_Knda\n1\n0.747\n0.745\n0.296\n0.007\n0.040\n0.436\nkas_Arab\n1\n0.707\n0.705\n0.294\n0.004\n0.010\n0.412\nkas_Deva\n1\n0.733\n0.698\n0.317\n0.000\n0.050\n0.398\nkhk_Cyrl\n1\n0.745\n0.730\n0.289\n0.043\n0.020\n0.413\nkhm_Khmr\n1\n0.682\n0.739\n0.310\n0.029\n0.030\n0.444\nkik_Latn\n1\n0.656\n0.719\n0.314\n0.080\n0.040\n0.413\nkin_Latn\n1\n0.676\n0.695\n0.328\n0.025\n0.020\n0.422\nkir_Cyrl\n1\n0.689\n0.693\n0.276\n0.056\n0.050\n0.412\nkmr_Latn\n1\n0.735\n0.723\n0.294\n0.105\n0.040\n0.378\nlij_Latn\n1\n0.725\n0.732\n0.294\n0.034\n0.050\n0.423\nlim_Latn\n1\n0.750\n0.727\n0.349\n0.032\n0.050\n0.384\nlin_Latn\n1\n0.722\n0.721\n0.295\n0.003\n0.050\n0.409\nlmo_Latn\n1\n0.781\n0.716\n0.331\n0.014\n0.020\n0.443\nltg_Latn\n1\n0.690\n0.698\n0.325\n0.083\n0.050\n0.418\nltz_Latn\n1\n0.688\n0.676\n0.312\n0.104\n0.060\n0.383\nlug_Latn\n1\n0.669\n0.673\n0.317\n0.000\n0.050\n0.449\nmai_Deva\n1\n0.721\n0.679\n0.292\n0.000\n0.050\n0.388\nmal_Mlym\n1\n0.748\n0.728\n0.293\n0.033\n0.050\n0.370\nmin_Arab\n1\n0.673\n0.698\n0.333\n0.000\n0.010\n0.424\nmin_Latn\n1\n0.757\n0.737\n0.291\n0.000\n0.030\n0.375\nmkd_Cyrl\n1\n0.739\n0.696\n0.322\n0.099\n0.050\n0.450\nmri_Latn\n1\n0.703\n0.708\n0.310\n0.050\n0.020\n0.439\nmya_Mymr\n1\n0.744\n0.710\n0.329\n0.009\n0.020\n0.441\nnno_Latn\n1\n0.642\n0.704\n0.340\n0.047\n0.030\n0.380\nnob_Latn\n1\n0.689\n0.733\n0.311\n0.010\n0.040\n0.425\nnpi_Deva\n1\n0.740\n0.715\n0.272\n0.015\n0.040\n0.385\nnno_Latn\n1\n0.642\n0.704\n0.340\n0.047\n0.030\n0.380\nnob_Latn\n1\n0.689\n0.733\n0.311\n0.010\n0.040\n0.425\nnpi_Deva\n1\n0.740\n0.715\n0.272\n0.015\n0.040\n0.385\noci_Latn\n1\n0.714\n0.701\n0.286\n0.020\n0.020\n0.417\nory_Orya\n1\n0.700\n0.714\n0.307\n0.064\n0.050\n0.438\npag_Latn\n1\n0.690\n0.723\n0.294\n0.051\n0.050\n0.393\npap_Latn\n1\n0.764\n0.729\n0.347\n0.095\n0.020\n0.393\npbt_Arab\n1\n0.706\n0.722\n0.281\n0.076\n0.030\n0.446\nplt_Latn\n1\n0.706\n0.717\n0.286\n0.000\n0.040\n0.371\nquy_Latn\n1\n0.685\n0.689\n0.334\n0.072\n0.050\n0.374\nsag_Latn\n1\n0.710\n0.740\n0.271\n0.103\n0.060\n0.438\nsat_Olck\n1\n0.702\n0.708\n0.320\n0.020\n0.040\n0.408\nscn_Latn\n1\n0.687\n0.703\n0.295\n0.039\n0.040\n0.422\nsmo_Latn\n1\n0.706\n0.699\n0.321\n0.049\n0.040\n0.377\nsna_Latn\n1\n0.676\n0.697\n0.320\n0.048\n0.050\n0.444\nsnd_Arab\n1\n0.714\n0.717\n0.344\n0.021\n0.040\n0.425\nsom_Latn\n1\n0.732\n0.718\n0.339\n0.000\n0.030\n0.392\nsrd_Latn\n1\n0.710\n0.745\n0.343\n0.000\n0.050\n0.441\nssw_Latn\n1\n0.708\n0.687\n0.310\n0.014\n0.030\n0.407\nsun_Latn\n1\n0.728\n0.718\n0.321\n0.060\n0.030\n0.427\nszl_Latn\n1\n0.752\n0.735\n0.311\n0.069\n0.060\n0.436\ntat_Cyrl\n1\n0.719\n0.709\n0.315\n0.056\n0.050\n0.420\ntel_Telu\n1\n0.708\n0.676\n0.347\n0.107\n0.060\n0.397\ntgk_Cyrl\n1\n0.669\n0.690\n0.328\n0.026\n0.050\n0.404\ntpi_Latn\n1\n0.699\n0.738\n0.327\n0.081\n0.060\n0.372\ntso_Latn\n1\n0.777\n0.728\n0.287\n0.042\n0.040\n0.394\ntuk_Latn\n1\n0.707\n0.711\n0.284\n0.042\n0.050\n0.417\ntum_Latn\n1\n0.669\n0.702\n0.286\n0.017\n0.020\n0.411\ntwi_Latn\n1\n0.749\n0.737\n0.302\n0.000\n0.020\n0.414\nuig_Arab\n1\n0.645\n0.694\n0.325\n0.021\n0.020\n0.429\nvec_Latn\n1\n0.744\n0.743\n0.336\n0.033\n0.020\n0.380\nwar_Latn\n1\n0.681\n0.717\n0.270\n0.041\n0.020\n0.402\nydd_Hebr\n1\n0.719\n0.722\n0.338\n0.007\n0.040\n0.390\nzho_Hant\n1\n0.636\n0.680\n0.300\n0.023\n0.020\n0.385\nTable 26: Comparing LLMs’ performance (% in Pass@1) on mHumanEval - Class 1 languages. The languages are\ngiven as Flores-200 codes.\nLanguage\nClass\nClaude3.5\nGPT4o\nGPT3.5\nDeepSeek-Coder\nWizardCoder\nAya\nawa_Deva\n0\n0.653\n0.628\n0.191\n0.033\n0.020\n0.353\nbam_Latn\n0\n0.645\n0.634\n0.268\n0.081\n0.010\n0.410\nban_Latn\n0\n0.639\n0.641\n0.285\n0.060\n0.010\n0.398\nbem_Latn\n0\n0.675\n0.654\n0.308\n0.000\n0.000\n0.415\ncjk_Latn\n0\n0.750\n0.720\n0.316\n0.000\n0.010\n0.366\ndyu_Latn\n0\n0.620\n0.636\n0.039\n0.000\n0.010\n0.367\nfon_Latn\n0\n0.719\n0.658\n0.072\n0.016\n0.000\n0.396\nfuv_Latn\n0\n0.657\n0.665\n0.212\n0.000\n0.010\n0.357\ngrn_Latn\n0\n0.698\n0.689\n0.021\n0.000\n0.010\n0.356\nhat_Latn\n0\n0.597\n0.621\n0.142\n0.012\n0.010\n0.363\nhne_Deva\n0\n0.670\n0.626\n0.215\n0.008\n0.000\n0.403\nkac_Latn\n0\n0.679\n0.670\n0.047\n0.051\n0.000\n0.332\nkam_Latn\n0\n0.637\n0.673\n0.140\n0.057\n0.020\n0.383\nkbp_Latn\n0\n0.694\n0.683\n0.107\n0.000\n0.000\n0.376\nkea_Latn\n0\n0.677\n0.720\n0.065\n0.000\n0.010\n0.346\nkmb_Latn\n0\n0.667\n0.661\n0.175\n0.000\n0.000\n0.381\nknc_Arab\n0\n0.664\n0.647\n0.218\n0.000\n0.020\n0.398\nknc_Latn\n0\n0.586\n0.621\n0.291\n0.061\n0.010\n0.348\nkon_Latn\n0\n0.745\n0.691\n0.093\n0.000\n0.010\n0.361\nlua_Latn\n0\n0.689\n0.660\n0.283\n0.000\n0.010\n0.411\nluo_Latn\n0\n0.692\n0.615\n0.228\n0.004\n0.020\n0.380\nlus_Latn\n0\n0.616\n0.640\n0.132\n0.018\n0.000\n0.383\nmag_Deva\n0\n0.657\n0.700\n0.128\n0.000\n0.010\n0.418\nmni_Beng\n0\n0.574\n0.628\n0.275\n0.033\n0.010\n0.368\nmos_Latn\n0\n0.659\n0.657\n0.232\n0.021\n0.010\n0.414\nnso_Latn\n0\n0.635\n0.647\n0.038\n0.000\n0.000\n0.408\nnus_Latn\n0\n0.636\n0.707\n0.227\n0.018\n0.000\n0.418\nnya_Latn\n0\n0.746\n0.667\n0.124\n0.000\n0.000\n0.387\nprs_Arab\n0\n0.633\n0.644\n0.283\n0.000\n0.010\n0.364\nrun_Latn\n0\n0.715\n0.707\n0.252\n0.005\n0.000\n0.382\nshn_Mymr\n0\n0.664\n0.637\n0.214\n0.044\n0.020\n0.377\nsin_Sinh\n0\n0.645\n0.633\n0.187\n0.000\n0.020\n0.391\nsot_Latn\n0\n0.723\n0.703\n0.194\n0.053\n0.010\n0.417\ntaq_Latn\n0\n0.655\n0.671\n0.042\n0.052\n0.020\n0.383\ntaq_Tfng\n0\n0.643\n0.639\n0.128\n0.000\n0.020\n0.351\ntzm_Tfng\n0\n0.654\n0.670\n0.114\n0.014\n0.020\n0.376\numb_Latn\n0\n0.647\n0.622\n0.176\n0.000\n0.000\n0.372\nyue_Hant\n0\n0.613\n0.666\n0.282\n0.021\n0.020\n0.419\nTable 27: Comparing LLMs’ performance (% in Pass@1) on mHumanEval - Class 0 languages. The languages are\ngiven as Flores-200 codes.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-10-19",
  "updated": "2025-01-26"
}