{
  "id": "http://arxiv.org/abs/2410.15061v1",
  "title": "Classifying extended, localized and critical states in quasiperiodic lattices via unsupervised learning",
  "authors": [
    "Bohan Zheng",
    "Siyu Zhu",
    "Xingping Zhou",
    "Tong Liu"
  ],
  "abstract": "Classification of quantum phases is one of the most important areas of\nresearch in condensed matter physics. In this work, we obtain the phase diagram\nof one-dimensional quasiperiodic models via unsupervised learning. Firstly, we\nchoose two advanced unsupervised learning algorithms, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) and Ordering Points To Identify\nthe Clustering Structure (OPTICS), to explore the distinct phases of\nAubry-Andr\\'{e}-Harper model and quasiperiodic p-wave model. The unsupervised\nlearning results match well with traditional numerical diagonalization.\nFinally, we compare the similarity of different algorithms and find that the\nhighest similarity between the results of unsupervised learning algorithms and\nthose of traditional algorithms has exceeded 98\\%. Our work sheds light on\napplications of unsupervised learning for phase classification.",
  "text": "Classifying extended, localized and critical states in quasiperiodic lattices via unsupervised learning\nBohan Zheng,1, ∗Siyu Zhu,2, ∗Xingping Zhou,2, † and Tong Liu3, ‡\n1School of Computer Science and Technology, School of Software,\nNanjing University of Posts and Telecommunications, Nanjing, 210023, China\n2Institute of Quantum Information and Technology, Nanjing University of Posts and Telecommunications, Nanjing 210003, China\n3Department of Applied Physics, School of Science, Nanjing University of Posts and Telecommunications, Nanjing 210003, China\nClassification of quantum phases is one of the most important areas of research in condensed matter physics.\nIn this work, we obtain the phase diagram of one-dimensional quasiperiodic models via unsupervised learning.\nFirstly, we choose two advanced unsupervised learning algorithms, Density-Based Spatial Clustering of Appli-\ncations with Noise (DBSCAN) and Ordering Points To Identify the Clustering Structure (OPTICS), to explore\nthe distinct phases of Aubry-André-Harper model and quasiperiodic p-wave model. The unsupervised learning\nresults match well with traditional numerical diagonalization. Finally, we compare the similarity of different\nalgorithms and find that the highest similarity between the results of unsupervised learning algorithms and those\nof traditional algorithms has exceeded 98%. Our work sheds light on applications of unsupervised learning for\nphase classification.\nKeywords: quantum phase, quasiperiodic, machine learning\nPACS: 71.23.Ft, 71.10.Fd, 71.23.An\nI.\nINTRODUCTION\nAnderson localization1, the suppression of wave diffusion\nin disordered media, is ubiquitous across many areas of clas-\nsical and quantum physics. Numerous experimental demon-\nstrations have been reported in various systems, such as pho-\ntonic systems2,3 and ultracold atoms4. Currently, the theoret-\nical framework of Anderson localization is well established.\nThe scaling theory predicts no delocalization in one- and two-\ndimensional systems, while three-dimensional (3D) systems\ncan exhibit a localization-delocalization transition5. A thresh-\nold energy that separates extended from localized eigenstates\nis referred to as the mobility edge (ME)6.\nQuasicrystals also display novel localized phenomena7,8.\nUnlike random systems, quasiperiodic systems exhibit local-\nization transitions even in one dimension.\nA paradigmatic\nmodel is provided by the well-known Aubry-André-Harper\n(AAH) model9,10, where the localization-delocalization tran-\nsition can be derived from a simple self-duality argument.\nRecently, significant interest has focused on finding low-\ndimensional quasicrystals that, analogous to the 3D Ander-\nson model, display MEs separating extended and localized\nstates11–27.\nTraditional methods for distinguishing extended, localized,\nand critical states across these various models involve calcu-\nlating a typical physical quantity, the inverse participation ra-\ntio (IPR)28–30. For a normalized wave function, the IPR of an\nextended state scales linearly as 1\nL, vanishing in the thermo-\ndynamic limit L →∞; the critical state approaches zero at\na slower (power-law) rate, while the IPR of a localized state\nremains finite. Another method involves calculating the Lya-\npunov exponent γ, which is defined as the divergence rate be-\ntween neighboring lattice points. For an extended state, the\namplitude between neighboring lattice points remains equal in\nthe thermodynamic limit, resulting in γ = 0; conversely, for\na localized state, the amplitude decays exponentially, leading\nto γ > 0. However, the single Lyapunov exponent cannot dis-\ntinguish the extended state from the critical state, hence the\nnumerical results of critical states are often ambiguous, and a\nexact definition of critical states is lacking.\nRemarkablely, a recent work31 proposes an explicit crite-\nrion for precisely characterizing critical states, asserting that\nthe Lyapunov exponents of critical states should simultane-\nously be zero in both position space and momentum space. In\nmore physically-based language, critical states should exhibit\nanomalous delocalization transport and multifractal structure\n(non-extended and non-localized) in both position space and\nmomentum space.\nAn interesting question arises: Are there alternative meth-\nods to distinguish different quantum states in disordered sys-\ntems? The rise of machine learning32,33 research seems to\nanswer this question, which has numerous applications in\nphysical contexts, such as photonic structure design34, quan-\ntum many-body physics35–37, quantum computing, and chem-\nical and material physics38, as well as topological phase\nclassification39–42.\nUnsupervised learning43, a significant\nbranch of machine learning, can facilitate the data-driven con-\nstruction of quantum states without prior knowledge.\nIn previous works, supervised learning methods44,45 have\nbeen employed to detect classical and quantum phase\ntransitions46,47, predict the phase diagrams of the long-range\nHarper model and the AAH model48, and address the edge\nlearning problem of single event migration49,50.\nHowever,\nmany questions remain unresolved. For example, prior works\nhave yet to derive eigenstates as markers through supervised\nlearning. Notably, unsupervised learning algorithms do not\nrequire a training set; rather, the algorithms independently\nsearch for features within the data. This implies that phase\ndiagrams can be obtained without traditional methods. Fur-\nthermore, previous algorithms44,45 have not effectively distin-\nguished critical states from extended and localized states. In\nthis work, we demonstrate the capacity of the unsupervised\narXiv:2410.15061v1  [cond-mat.dis-nn]  19 Oct 2024\n2\nInitial state \nCategorizing\nCompleted\nFigure 1: An example that demonstrates the process of the DBSCAN algorithm.\nclassification of three typical eigenstates in one-dimensional\nquasiperiodic models.\nII.\nDBSCAN AND OPTICS ALGORITHM\nFirst, let’s introduce the principle of unsupervised learning\nalgorithm. In practical calculations, we employ two widely-\nused algorithms, Density-Based Spatial Clustering of Appli-\ncations with Noise (DBSCAN)43,51 and Ordering Points To\nIdentify the Clustering Structure (OPTICS)43,52, due to their\nrobustness in handling complex data. DBSCAN, introduced\nby Martin Ester et al. in 1996, is capable of identifying clus-\nters of arbitrary shapes without requiring the pre-specified\nclusters, making it particularly suitable for distinguishing lo-\ncalized states (high density) and extended states (low density).\nOPTICS, developed by Ankerst et al. in 1999, orders points\nbased on their density reachability, allowing for a more de-\ntailed analysis of clustering structures, which is useful for de-\ntecting transitions between extended, localized, and critical\nstates.\nThe inputs for both algorithms are n-dimensional vectors,\nwhich are treated as coordinate points in an n-dimensional\nspace. The output consists of labels of these points. The al-\ngorithms rely on the distance between two points, commonly\ndefined by the Euclidean distance, which is given by the for-\nmula:\nd =\nv\nu\nu\nt\nn\nX\ni=1\n(x1,i −x2,i)2,\n(1)\nwhere x1,i and x2,i represent the coordinates of two points in\nthe i-th dimension, respectively.\nA.\nDBSCAN algorithm\nThe DBSCAN43,51 is effective for clustering data of arbi-\ntrary shapes, as its process is illustrated in Fig. 1. DBSCAN\ncategorizes data points into distinct clusters based on the dis-\ntances between them, which is calculated using the Euclidean\ndistance Eq. (1).\nThe core idea of the DBSCAN algorithm is to group to-\ngether points that are density-connected—that is, connected\nwith sufficient density—while marking those that are not part\nof any cluster as noise. The concept of \"density-connected\" is\ndefined by two parameters: EPS (the neighborhood radius)\nand MinPoints (the minimum number of points required to\nform a cluster). A point is classified as a core point if it has\nat least MinPoints neighbors within its EPS radius. If a\npoint q lies within the EPS radius of a core point p, then q is\nconsidered directly density-reachable. Subsequently a point r\nis said to be density-connected with p if there exists a chain of\ndirectly density-reachable points connecting them.\nAn intuitive example is illustrated in Fig. 1, the parameters\nare set to MinPoints = 3 and EPS = 1.8. In Fig. 1(b),\npoint P has points Q, A, and B within its EPS radius, while\npoint Q has points P, C, and D in its neighborhood, making\nboth P and Q core points. These core points and their neigh-\nbors form a cluster. Since P and Q are density-connected,\ntheir clusters merge in Fig. 1(c). Point E does not belong to\nany cluster and is classified as noise.\nB.\nOPTICS algorithm\nIn contrast to DBSCAN, which only classifies points that\nexceed a fixed density threshold into clusters, OPTICS43,52\nis capable of identifying clusters with varying density levels,\ngrouping points with similar densities into the same cluster.\nFig. 2 illustrates its process.\nThe core idea of the OPTICS is to generate a sequence that\nreflects the positional relationship between data points, and\ncluster the data points based on this sequence.\nEach data\npoint in the sequence has two attributes: the core distance\n(coreDist), defined as the distance to its MinPoints-th near-\nest neighbor, representing the density around the point, and\nthe reachability distance (reachDist), which reflects the dis-\ntance between the point and preceding points in the sequence.\n3\nFigure 2: An example that illustrates the OPTICS algorithm process.\nThe specific sequence generation process is as follows. Ini-\ntially, the algorithm computes the coreDist for each point and\nsets all reachDist values to infinity. A point is then randomly\nselected as the first core point [Fig. 2(a)]. In each iteration,\nthe new reachDist value is calculated as the larger between\nthe distance from point i to the current core point p and the\ncoreDist of p. If this new value is smaller than the previ-\nous reachDist, it is updated accordingly. The point with the\nsmallest reachDist is chosen as the next core point, while\nthe previous core point is added to the result sequence and\nexcluded from further calculations [Fig. 2(b)].\nAfter the iterations, clusters are determined based on the\nresult sequence and the threshold EPS [Fig. 2(c) and (d)]. If\na point’s reachDist is smaller than EPS, it is grouped with\nthe previous point in the same cluster. If its reachDist is\ngreater than EPS but its coreDist is smaller than EPS, it\nstarts a new cluster. Otherwise, it is classified as noise.\nFor\ninstance,\nFig.\n2\nillustrates\nseven\npoints\nwith\nMinPoints = 1 and EPS = 4. Initially, point A is cho-\nsen as the core point [Fig. 2(a)], and after the first round of\nreachDist calculations, point B is selected as the next core\npoint, with A being added to the result sequence [Fig. 2(b)].\nThe process continues until all points are classified. Point A\nforms a new cluster, and points B, D, G, and F are included\nin A’s cluster, while points C and E form a separate cluster\n[Fig. 2(c) and (d)].\nIII.\nTWO TYPICAL MODELS\nIn various quasiperiodic systems, the AAH model and the\nquasiperiodic p-wave model are two representative models\ndue to their display of rich quantum states: extended, local-\nized, and critical states.\nA.\nAubry-André-Harper model\nQuasiperiodic systems53–58 exhibit quasiperiodic structures\nrather than random distributions, such as the Fibonacci lat-\ntice model, the Thue-Morse lattice model, and the well-known\nAAH model. The lattice Hamiltonian for the AAH model is\ngiven by:\nˆH = −\nL−1\nX\ni=1\nt(ˆc†\ni ˆci+1 + H.c.)+\nL\nX\ni=1\nV cos(2παi + ϕ)ˆni. (2)\nHere, ˆc†\ni and ˆci+1 represent the fermion creation and annihila-\ntion operators, respectively; ˆni = ˆc†\ni ˆci is the particle number\noperator; and L denotes the total number of lattice points. The\nterm V cos(2παi + ϕ)ˆni represents the quasiperiodic poten-\ntial field. The parameter α =\n√\n5−1\n2\nis an irrational number.\nWithout loss of generality, we choose the phase factor ϕ = 0\nand set t = 1 as the energy unit for numerical calculations.\nIn quantum disordered systems, the inverse participation ra-\ntio (IPR) is a quantity typically used to characterize the local-\nized and extended properties of eigenstates. The variation of\nIPR in the AAH model with respect to the disorder potential\nstrength V within the range (0, 3) is shown in Fig. 3(a1). The\nbrightness of the color represents the value of the IPR, indicat-\ning that brighter colors correspond to larger IPR values. This\nanalysis clearly reveals a sudden change at V = 2t, where all\neigenstates of the Hamiltonian become critical states. More-\nover, all eigenstates of the Hamiltonian are extended when\nV < 2t, whereas they are localized when V > 2t.\n4\nFigure 3: Figures of the IPR for the AAH model (a1) and the quasiperiodic p-wave model (a2). Clustering results of the\neigenstates for (b1) the AAH model and (b2) the quasiperiodic p-wave model using the DBSCAN algorithm with\nparameters (b1) EPS = 0.2 and Minpoints = 1, and (b2) EPS = 0.5 and Minpoints = 1. Clustering results of the\neigenstates for (c1) the AAH model and (c2) the quasiperiodic p-wave model using the OPTICS algorithm with\nparameters (c1) EPS = 0.2 and Minpoints = 2, and (c2) EPS = 0.8 and Minpoints = 2.\nB.\nQuasiperiodic p-wave model\nThe Hamiltonian for a one-dimensional p-wave supercon-\nducting pairing model in a quasiperiodic lattice is given by:\nH =\nX h\n−tˆc†\ni ˆci+1 + ∆ˆciˆci+1 + H.c. + Viˆni\ni\n,\n(3)\nwhere Vi = V cos(2παi). When ∆= 0, the model in Eq. (3)\nreduces to the AAH model in Eq. (2). When ∆̸= 0 and V\nincreases, this model exhibits a transition from a topological\nsuperconducting phase to an Anderson localized phase at V =\n2|t + ∆|. Moreover, the model in Eq. (3) exhibits a large\nnumber of critical states.\nThe numerical phase diagram of IPR for this model59,60 is\nshown in Fig. 3(a2). In the region where V < 2|t −∆|, all\neigenstates of the system are extended states; in the region\n2|t −∆| < V < 2|t + ∆|, all eigenstates are critical states;\nand in the region V > 2|t + ∆|, all eigenstates are localized\nstates.\nC.\nSimulation results\nBy applying the DBSCAN and OPTICS algorithms, We\nperform numerical simulations on these two models. For the\nAAH model (Eq. 2), we set the total number of lattice points\nL = 500 to obtain the eigenvector of the Hamiltonian. Sim-\nilarly, for the quasiperiodic p-wave model (Eq. 3), we set\nL = 1000. These eigenvectors are put into the two algorithms\nseparately, where the parameters EPS and MinPoints are\nadjusted. The output is a list of clustering labels, indicating\nthe category to which each eigenvector belongs.\nThe clustering results for the AAH and quasiperiodic p-\nwave models are shown in Fig. 3(b1), (b2), (c1) and (c2), re-\nspectively. The horizontal axis of each figure represents the\ndisorder potential strength V , while the vertical axis repre-\nsents the index of eigenvalues for a given V . Each point cor-\nresponds to an eigenvector, and different colors represent the\ndistinct categories produced by the clustering.\nAs shown in Fig. 3, the classification of eigenstates is\nclearly visible. In Fig. 3(b1) and (c1), the region where V < 2\nis darker, representing the extended state; V > 2 corresponds\nto the localized state, and V = 2 marks the critical transition\npoint between two phases. In Fig. 3(b2) and (c2), V < 1\nindicates the extended state, V > 3 represents the localized\nstate, and the region 1 < V < 3 corresponds to the critical\nstate. Notably, the OPTICS algorithm distinctly identifies the\ncritical state of the AAH model at V = 2 in Fig. 3(c1).\nTo explain more specifically how DBSCAN and OPTICS\nalgorithms classify extended and localized states, we demon-\nstrate their frameworks for quantum disordered systems. DB-\nSCAN groups together eigenstates with high-density, while\nmarking those in lower-density regions as outliers.\nHence\nDBSCAN can identify extended states, which are more uni-\nformly spread across the lattice, by forming large and con-\ntinuous clusters. On the other hand, localized states, where\nthe wave function is confined to a smaller region, result in\nmore compact and isolated clusters. OPTICS, similar to DB-\nSCAN, focuses on ordering eigenstates based on the density\nreach-ability. This approach allows OPTICS to detect more\nsubtle transitions in quantum systems, such as critical states,\n5\nas the density structure captured by OPTICS reflects the grad-\nual transition from localized to extended states, and critical\nstates lie at the boundary between these phases.\nThe primary advantage of machine learning methods like\nDBSCAN and OPTICS over traditional methods lies in the\ndata-driven nature. Machine learning algorithms can auto-\nmatically detect patterns and classify states without requir-\ning prior knowledge of the system. This capability makes\nthem particularly effective for identifying complex phase tran-\nsitions, including critical states, which are challenging by the\nIPR simulation. Moreover, machine learning methods react\nwell for larger data-sets, allowing for more efficient classifi-\ncation in systems with a large number of eigenstates.\nIV.\nSIMILARITY\nIn this section, we employ the difference hash algo-\nrithm to calculate the similarity between traditional methods\n[Fig. 3(a)] and machine learning methods [Fig. 3(b) and (c)],\nnamely the similarity between the IPR results and the cluster-\ning results.\nThe difference hash algorithm treats a figure as a two-\ndimensional signal composed of various frequency compo-\nnents.\nHigh-frequency components correspond to regions\nwith significant brightness variations between adjacent pix-\nels, providing detailed information about the image. In con-\ntrast, low-frequency components represent areas with minor\nbrightness variations, capturing the general structure of the\nimage. The algorithm reduces the image size to filter out high-\nfrequency components and computes the hash values by fo-\ncusing on the low-frequency components. If a pixel is brighter\nthan the following pixel, the corresponding hash bit is set to\n1; otherwise, it is set to 0. The similarity between two im-\nages is then determined by comparing their hash values. The\nsimilarity Q is given by the formula:\nQ = 1 −\nN\nP\ni\n(h1,i + h2,i + 1) mod 2\nN\n,\n(4)\nwhere N is the number of pixels in the image, and h1,i and\nh2,i are the i-th bits of hash values, respectively.\nTable I: Similarity results for different algorithms.\nSimilarity\nDBSCAN\nOPTICS\nAAH\n98.40%\n90.63%\nquasiperiodic p-wave\n95.30%\n62.50%\nSimilarity results for different algorithms are illustrated in\nTable I, which indicate that the DBSCAN algorithm signifi-\ncantly surpasses the OPTICS algorithm. For both the AAH\nmodel and the quasiperiodic p-wave model, the DBSCAN al-\ngorithm achieves a similarity of over 95% compared to tradi-\ntional methods, with a peak value of 98.4%. In contrast, the\nOPTICS algorithm only attains approximately 90% and 62%\nsimilarity for the two models, respectively. Additionally, the\nunsupervised learning results for the AAH model are notably\nbetter than those for the quasiperiodic p-wave model. This\nis reasonable, as the AAH model deals with a single-particle\nproblem, while the quasiperiodic p-wave model is a mean-\nfield approximation of a strongly correlated system. The lat-\nter’s increased complexity and reduced robustness to machine\nlearning algorithms explain the lower performance.\nV.\nCONCLUSION\nIn this work, we investigate the capability of unsuper-\nvised learning algorithms to extract information about dis-\ntinct phases in various quasiperiodic systems. Specifically,\nwe employ two unsupervised learning algorithms, DBSCAN\nand OPTICS, to classify the extended, localized, and critical\nstates in the AAH model and the quasiperiodic p-wave model.\nWhile previous studies have focused on supervised learning\nalgorithms, we demonstrate that unsupervised learning al-\ngorithms can accurately reproduce phase diagrams in close\nagreement with traditional numerical methods. Furthermore,\nwe apply the difference hash algorithm to quantify the simi-\nlarity between the unsupervised learning phase diagrams and\nthe traditional numerical phase diagrams. Our results show\nthat the DBSCAN algorithm is particularly effective for ex-\nploring quasiperiodic systems. Additionally, DBSCAN is not\nonly applied to single-particle problems but also effectively\ndescribes the mean-field approximation of strongly correlated\nsystems. Thus, a potential extension of DBSCAN is to dis-\ntinguish many-body wave functions across various phases in\nmany-body interacting systems. This work provides a valu-\nable demonstration of unsupervised learning in classifying\ndifferent states of matter and highlights its potential for phase\nclassification.\nACKNOWLEDGMENTS\nThis work was supported by the Natural Science Founda-\ntion of Nanjing University of Posts and Telecommunications\n(Grant No. NY223109, NY220119, NY221055), China Post-\ndoctoral Science Foundation (Grant No. 2022M721693), and\nNational Natural Science Foundation of China under (Grant\n12404365).\n∗These authors contributed equally to this work.\n† zxp@njupt.edu.cn\n‡ t6tong@njupt.edu.cn\n1 Anderson P W 1958 Phys. Rev. 109 1492\n6\n2 Segev M, Silberberg Y and Christodoulides D N 2013 Nat. Pho-\nton. 7 197\n3 Lahini Y, Pugatch R, Pozzi F, Sorel M, Morandotti R, Davidson\nN and Silberberg Y 2009 Phys. Rev. Lett. 103 013901\n4 Roati G, D’Errico C, Fallani L, Fattori M, Fort C, Zaccanti M,\nModugno G, Modugno M and Inguscio M 2008 Nature 453 895\n5 Abrahams E, Anderson P W, Licciardello D C and Ramakrishnan\nT V 1979 Phys. Rev. Lett. 42 673\n6 Mott N 1987 J. Phys. C 20 3075\n7 Soukoulis C M and Economou E N 1982 Phys. Rev. Lett. 48 1043\n8 Buchler H P, Blatter G and Zwerger W 2003 Phys. Rev. Lett. 90\n130401\n9 Aubry S and André G 1980 Ann. Isr. Phys. Soc. 3 18\n10 Harper P G 1955 Proc. Phys. Soc. Sect. A 68 874\n11 Liu T, Guo H, Pu Y and Longhi S 2020 Phys. Rev. B 102 024205\n12 Xia X, Huang K, Wang S and Li X 2022 Phys. Rev. B 105 014207\n13 Cai X 2022 Phys. Rev. B 106 214207\n14 Zhou L and Han W 2021 Chin. Phys. B 30 100308\n15 Wang L, Wang Z B and Chen S 2024 Phys. Rev. B 110 L060201\n16 Liu Y, Jiang X P, Cao J and Chen S 2020 Phys. Rev. B 101 174205\n17 Xu Z, Xia X and Chen S 2021 Phys. Rev. B 104 224204\n18 Li S Z, Cheng E, Zhu S L and Li Z 2024 Phys. Rev. B 110 134203\n19 Li S Z and Li Z 2024 Phys. Rev. B 110 L041102\n20 Zhang D W, Chen Y L, Zhang G Q, Lang L J, Li Z and Zhu S L\n2020 Phys. Rev. B 101 235150\n21 Jiang S L, Liu Y and Lang L J 2023 Chin. Phys. B 32 097204\n22 Wang Y, Xia X, Zhang L, Yao H, Chen S, You J, Zhou Q and Liu\nX J 2020 Phys. Rev. Lett. 125 196604\n23 Zhou X C, Wang Y, Poon T F J, Zhou Q and Liu X J 2023 Phys.\nRev. Lett. 131 176401\n24 Biddle J and Das Sarma S 2010 Phys. Rev. Lett. 104 070601\n25 Longhi S 2019 Phys. Rev. B 100 125157\n26 Zhou L W 2024 Phys. Rev. B 109 024204\n27 Zeng Q B, Yang Y B and Xu Y 2020 Phys. Rev. B 101 020201\n28 Lin X, Chen X, Guo G C and Gong M 2023 Phys. Rev. B 108\n174206\n29 Lin X and Gong M 2024 Phys. Rev. A 109 033310\n30 Jiang H, Lang L, Yang C, Zhu S L and Chen S 2019 Phys. Rev. B\n100 054301\n31 Liu T and Xia X 2024 Chin. Phys. Lett. 41 017102\n32 Jordan M I and Mitchell T M 2015 Science 349 255\n33 Carleo G, Cirac I, Cranmer K, Daudet L, Schuld M, Tishby\nN, Vogt-Maranto L and Zdeborová L 2019 Rev. Mod. Phys. 91\n045002\n34 Ma W, Liu Z, Kudyshev Z A, Boltasseva A, Cai W and Liu Y\n2021 Nat. Photon. 15 77\n35 Carrasquilla J and Torlai G 2021 PRX Quantum 2 040201\n36 Lu S, Gao X and Duan L M 2019 Phys. Rev. B 99 155136\n37 Schmitt M and Heyl M 2020 Phys. Rev. Lett. 125 100503\n38 Gubernatis J E and Lookman T 2018 Phys. Rev. Materials 2\n120301\n39 Park S, Hwang Y and Yang B 2022 Phys. Rev. B 105 195115\n40 Zhang Y and Kim E A 2017 Phys. Rev. Lett. 118 216401\n41 Bai S C, Tang Y C and Ran S J 2022 Chin. Phys. Lett. 39 100701\n42 Zhang P, Shen H and Zhai H 2018 Phys. Rev. Lett. 120 066401\n43 Ellis K, Solar-Lezama A and Josh T 2015 Advances in Neural\nInformation Processing Systems (Montreal: Neural Information\nProcessing Systems Foundation) pp. 973-981\n44 Venderley J, Khemani V and Kim E A 2018 Phys. Rev. Lett. 120\n257204\n45 Hsu Y T, Li X, Deng D L and Das Sarma S 2018 Phys. Rev. Lett.\n121 245701\n46 Ch’Ng K, Carrasquilla J, Melko R G and Khatami E 2017 Phys.\nRev. X 7 031038\n47 Hu W, Singh R R and Scalettar R T 2017 Phys. Rev. E 95 062122\n48 Ahmed A, Nelson A, Raina A and Sharma A 2023 Phys. Rev. B\n108 155128\n49 Bai X D, Zhao J, Han Y Y, Zhao J C and Wang J G 2021 Phys.\nRev. B 103 134203\n50 Yao H, Khoudli A, Bresque L and Sanchez-Palencia L 2019 Phys.\nRev. Lett. 123 070405\n51 Ester M, Kriegel H P, Sander J and Xu X W 1996 Proceedings\nof the 2nd International Conference on Knowledge Discovery and\nData Mining (KDD-96) (Portland: AAAI Press) pp. 226-231\n52 Ankerst M, Breunig M M, Kriegel H P and Sander J 1999 Pro-\nceedings of the 1999 ACM SIGMOD International Conference on\nManagement of Data (New York: Association for Computing Ma-\nchinery) pp. 49-60\n53 Lu Z, Xu Z and Zhang Y B 2022 Ann. Phys. (Berlin) 534 2200203\n54 Wei X B, Wu L Q, Feng K W, Liu T and Zhang Y B 2024 Phys.\nRev. A 109 023314\n55 Lin X, Guo G C and Gong M 2023 arXiv:2311.08643 [cond-mat]\n56 Liu T, Cheng S, Zhang R, Ruan R and Jiang H 2022 Chin. Phys.\nB 31 027101\n57 Liu T, Xia X, Longhi S and Sanchez-Palencia L 2022 SciPost\nPhys. 12 027\n58 Jiang X P, Zeng W L, Hu Y Y and Pan L 2024 arXiv:2409.03591\n[cond-mat]\n59 Liu T, Cheng S, Guo H and Xianlong G 2021 Phys. Rev. B 103\n104203\n60 Wang J, Liu X J, Xianlong G, and Hu H 2016 Phys. Rev. B 93\n104504\n",
  "categories": [
    "cond-mat.dis-nn",
    "quant-ph"
  ],
  "published": "2024-10-19",
  "updated": "2024-10-19"
}