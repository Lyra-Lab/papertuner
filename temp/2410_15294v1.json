{
  "id": "http://arxiv.org/abs/2410.15294v1",
  "title": "Unsupervised feature selection algorithm framework based on neighborhood interval disturbance fusion",
  "authors": [
    "Xiaolin Lv",
    "Liang Du",
    "Peng Zhou",
    "Peng Wu"
  ],
  "abstract": "Feature selection technology is a key technology of data dimensionality\nreduction. Becauseof the lack of label information of collected data samples,\nunsupervised feature selection has attracted more attention. The universality\nand stability of many unsupervised feature selection algorithms are very low\nand greatly affected by the dataset structure. For this reason, many\nresearchers have been keen to improve the stability of the algorithm. This\npaper attempts to preprocess the data set and use an interval method to\napproximate the data set, experimentally verifying the advantages and\ndisadvantages of the new interval data set. This paper deals with these data\nsets from the global perspective and proposes a new algorithm-unsupervised\nfeature selection algorithm based on neighborhood interval disturbance\nfusion(NIDF). This method can realize the joint learning of the final score of\nthe feature and the approximate data interval. By comparing with the original\nunsupervised feature selection methods and several existing feature selection\nframeworks, the superiority of the proposed model is verified.",
  "text": "第45 卷第4 期\n2021 年8 月\n南京理工大学学报\nJournal of Nanjing University of Science and Technology\nVol．45 No．4\nAug 2021\n收稿日期: 2021－05－08\n修回日期: 2021－07－07\n基金项目: 国家自然科学基金( 61502289; 61806003) ; 山西省重点研发项目( 201803D31199) ; 山西省自然科学基\n金( 201801D221163; 201801D221173)\n作者简介: 吕晓林( 1995－) ，女，硕士生，主要研究方向: 机器学习、数据挖掘，E-mail: 969103861@ qq．com; 通讯作\n者: 杜亮( 1985 －) ，男，博士，副教授，主要研究方向: 数据挖掘、机器学习、大数据分析，E-mail:\n152009746@ qq．com。\n引文格式: 吕晓林，杜亮，周芃，等．基于邻域区间扰动融合的无监督特征选择算法框架［J］．南京理工大学学报，\n2021，45( 4) : 420－428．\n投稿网址: http: / /zrxuebao．njust．edu．cn\n基于邻域区间扰动融合的无监督特征选择算法框架\n吕晓林\n1，杜\n亮\n1，2，周\n芃\n3，吴\n鹏\n1，2\n( 山西大学1．计算机与信息技术学院; 2．大数据科学与产业研究院，山西太原030006;\n3．安徽大学计算机科学与技术学院，安徽合肥230601)\n摘\n要: 特征选择技术是数据降维的一种关键技术，由于采集到的数据样本标签信息缺失，无监\n督特征选择受到了更多人的关注。现有的无监督特征选择算法普适性及稳定性很低，受数据集\n结构的影响很大，因此很多研究者一直热衷于提高算法的稳定性。该文尝试从数据集的预处理\n出发，采用区间的方式来对数据集进行近似，得到与数据集相关联的几个数据集，通过实验验证\n新的区间数据集的优劣性，并思考从全局的角度对数据集进行处理，进一步提出了一种新的模\n型———基于邻域区间扰动融合的无监督特征选择算法框架( Unsupervised feature selection\nalgorithm framework based on neighborhood interval disturbance fusion，NIDF) 。该模型可实现对特\n征的最终得分和近似数据区间的联合学习，通过与原始无监督特征选择方法以及现有的几种特\n征选择框架的对比，体现出该文提出的模型的优越性。\n关键词: 区间扰动; 融合; 无监督特征选择; 特征选择; 特征选择算法框架\n中图分类号: TP181\n文章编号: 1005－9830( 2021) 04－0420－09\nDOI: 10．14177 /j．cnki．32－1397n．2021．45．04．005\nUnsupervised feature selection algorithm framework based on\nneighborhood interval disturbance fusion\nLv Xiaolin\n1，Du Liang\n1，2，Zhou Peng\n3，Wu Peng\n1，2\n( 1．College of Computer and Information Technology; 2．Institute of Big Data Science and Industry，\nShanxi University，Taiyuan 030006，China;\n3．College of Computer Science and Technology，Anhui University，Hefei 230601，China)\nAbstract: Feature selection technology is a key technology of data dimensionality reduction．Because\nof the lack of label information of collected data samples，unsupervised feature selection has attracted\nmore attention．The universality and stability of many unsupervised feature selection algorithms are\n总第239 期\n吕晓林\n杜\n亮\n周\n芃\n吴\n鹏\n基于邻域区间扰动融合的无监督特征选择算法框架\nvery low and greatly affected by the dataset structure．For this reason，many researchers have been\nkeen to improve the stability of the algorithm．This paper attempts to preprocess the data set and use\nan interval method to approximate the data set，experimentally verifying the advantages and disadvan-\ntages of the new interval data set．This paper deals with these data sets from the global perspective\nand proposes a new algorithm—unsupervised feature selection algorithm based on neighborhood\ninterval disturbance fusion( NIDF) ．This method can realize the joint learning of the final score of\nthe feature and the approximate data interval．By comparing with the original unsupervised feature\nselection methods and several existing feature selection frameworks，the superiority of the proposed\nmodel is verified．\nKey words: interval disturbance; fusion; unsupervised feature selection; feature selection; feature\nselection algorithm framework\n高维大数据在许多领域随处可见，科技的进\n步更是加快了大数据的产生，每天都会有数亿的\n数据产生，以文本、图像、音频、视频等形式存在，\n覆盖于各个领域。面对如此庞大的数据，从中选\n择出合适的信息对学习工作进行指导变得极为困\n难。在这种境况下，特征选择技术显得尤为重要。\n特征选择技术的主要目的是在一个特定的评估标\n准下，从原始的高维特征中选择出最重要的特征\n子集，然后利用选择出的特征子集结合一些有效\n的算法去完成数据聚类、分类等任务。\n根据数据样本是否含有标签信息，特征选择\n算法可分为有监督特征选择\n［1，2］、半监督特征选\n择\n［3－5］和无监督特征选择\n［6－8］3 类。有监督和半\n监督特征选择通常会用到样本的标签信息，通过\n特征和标签信息之间的相关性来评定特征的重要\n性。现实中采集到的数据很少有标签信息，并且\n标记标签信息的代价很昂贵，在大规模数据中更\n是难以实现，因此，研究无监督场景下的特征选择\n更具有实用价值。\n无监督特征选择方法可分为3 类: 过滤式方\n法\n［9］、包装式方法\n［10］和嵌入式方法\n［11－13］。过滤\n式方法的主要思路是对每一维的特征打分，即给\n每一维的特征赋予权重，权重代表该维特征的重\n要性，然后依据权重排序。过滤式方法是独立于\n学习算法的，它的计算量很低，它的性能也有所欠\n缺。包装式方法是将子集的选择看作是一个搜索\n寻优的问题，首先生成不同的组合，对组合进行评\n价，然后再与其他的组合进行比较。包装式方法\n是与特定的学习算法相联系的，虽然有较好的性\n能，但是它的计算量很大，不适用于对大规模数据\n的处理。嵌入式方法的思路是在模型既定的情况\n下得出对提高模型准确性最好的属性，即在确定\n模型的过程中，选出对模型有重要意义的属性。\n具体而言，嵌入式方法是将特征选择的学习过程\n嵌入进模型中，故嵌入式方法能获得一个较好的\n性能，但其计算量很大，不具备通用性。\n1\n相关工作\n近年来，研究者基于无监督特征选择方法已\n经做了大量的工作，其中最具有代表性的是拉普\n拉斯特征选择算法( Laplacian score for feature se-\nlection，LapScore)\n［14］和多类簇特征选择算法( Un-\nsupervised feature selection for multi-cluster data，\nMCFS)\n［15］。LapScore 是一种基于过滤式的无监\n督特征选择方法，是由何晓飞于2005 年提出的，\nLapScore 主要是基于拉普拉斯特征映射和局部保\n留投影，它的主要思想是距离近的两个样本点更\n可能属于同一类。因此，LapScore 认为，数据的局\n部结构比全局结构更重要。LapScore 是通过特征\n保留数据局部结构的能力来评估其重要性的。\nMCFS 是一种基于嵌入式的无监督特征选择方\n法，是由蔡登于2010 年提出的，其主要思想是通\n过谱分析展开数据流形，然后利用L1 谱分析回归\n来决定特征的重要性。这两种方法都是从改造模\n型的角度出发进行特征选择，并且都没有考虑特\n征的冗余性，所选出的特征具有很大的冗余性。\n对此，Wang 等\n［16］于2015 年提出了一种基于全局\n冗余最小化的特征选择框架( Feature selection via\nglobal redundancy minimization，GＲM) 。进一步\n地，Nie 等\n［17］于2019 年提出了一种基于全局冗余\n最小化的自动加权特征选择框架( General frame-\n1\n2\n4\n南京理工大学学报\n第45 卷第4 期\nwork for auto-weighted feature selection via global\nredun-dancy minimization，AGＲM) 。这两个框架\n都能够选择出不冗余的特征，既适用于有监督特\n征选择方法，也适用于无监督特征选择方法。\n另外，在以往的无监督特征选择工作中发现，\n对不同数据集进行降维，选择特征的能力是不同\n的，甚至有很大差异。数据的准确与否会对所选\n择的特征产生很大影响，提出的模型抗干扰能力\n差，性能极易受到个别异常点的破坏。为此，研究\n者们从不同的维度提出了一系列的解决办法，包\n括自步学习、鲁棒学习等。\n( 1) 自步学习。\n自步学习是近年来被提出的一种学习策\n略，它从简单到复杂逐步增加训练实例，可以典\n型地降低局部最优的风险。基于聚类任务现存\n的一些问题，如聚类结果很容易陷入局部最优、\n聚类结果容易受到少量异常值的影响、聚类结\n果对初始参数非常敏感等，很多研究者发现，通\n过加入自步学习框架可以大大缓解此类问题。\n具体地，Yu 等\n［18］于2020 年提出了一种基于自\n步学习的K-means聚类算法。其核心思想是通\n过自步学习方法来模拟人类学习知识的过程，\n即从易到难地学习知识。该方法首先使用一个\n自步正则化因子来选择样本的一个特定子集加\n入训练，然后自动调整自步学习的参数逐步地\n增加样本的数量和难度，从而逐渐提高聚类模\n型的性能和泛化能力。\n( 2) 鲁棒学习。\n为了克服异常点对模型的影响，增强算法的\n稳定性，研究者们针对聚类任务提出了许多鲁棒\n的算法\n［19，20］。这些方法大致可归为两类: 一类是\n基于惩罚正则化的方法，另一类是基于裁剪函数\n的方法。Georgogiannis 于2016 年通过借鉴回归\n中离群点检测的思想，提出了一个经典二次K－均\n值算法的变量鲁棒性和一致性的理论分析———鲁\n棒K-means\n［21］。在这项工作中，Georgogiannis 发\n现，一个数据集中的两个离群值足以分解这个聚\n类过程。然而，如果关注的是“结构良好的”数据\n集，那么尽管有离群值，鲁棒K-means 最终还是可\n以恢复底层的集群结构。\n可以看出，针对数据集中的离群点，为了降\n低其对模型性能的影响，研究者们都是努力地\n提升模型的稳定性，不可否认的是，这种做法确\n实能缓解离群点对模型整体性能的破坏，但是，\n正如Georgogiannis 在鲁棒K-means 的工作中所\n提出的，对于“结构良好的”数据集，鲁棒K-\nmeans 可以克服离群值的影响，恢复数据的底层\n集群结构，而对于某些数据集，这个数据集中的\n两个离群值足以破坏整个聚类过程，这足以说\n明对于一个“非结构良好的”数据集，离群值的\n破坏力很大。\n为此，本文从两方面来提高聚类的准确性。\n一方面，采用区间的方式对数据进行近似，相较于\n直接使用某个样本自身的信息，本文采用其邻近\n的几个样本来刻画这个样本，这样可以有效地降\n低离群点的影响。另一方面，基于上述产生的多\n种数据表示，提出了一种新的模型———基于邻域\n区间扰动融合的无监督特征选择算法框架\n( Unsupervised feature selection algorithm framework\nbased on neighborhood interval disturbance fusion，\nNIDF) 。此模型可实现对特征的最终得分和近似\n数据区间的联合学习，最终达到一个不错的聚类\n效果。\n2\n研究方法\n2．1\n方法的提出\n在这项工作中，本文首先对数据进行了区间\n近似，具体地，给定一个数据集X∈Ｒ\nn×d，本文从\n两个层面对这个数据集中的数据进行近似。一方\n面，从样本层面出发，采用K 近邻的方式找到每\n个样本的k 个邻居，然后采用方差浮动的方式将\n样本近似到一个区间上，得到近似数据集Xlow 和\nXup; 另一方面，从特征层面出发，同样采用K 近邻\n的方式找到每个特征的k 个邻居，将特征近似到\n一个区间上，得到近似数据集Flow和Fup，这样，就\n将一个原始数据集X 扩展成了4 个近似数据集，\n这里采用{ Xi}\n4\ni= 1来表示，{ Xi}\n4\ni= 1分别对应于Xlow、\nXup、Flow和Fup。其次，本文采用经典的无监督特\n征选择方法对新的区间近似数据集{ Xi}\n4\ni= 1 进行\n特征选择，并分别得到一个特征分数{ si}\n4\ni= 1。最\n后，基于此，本文给出了一种新的模型———基于邻\n域区间扰动融合的无监督特征选择算法框架\nNIDF。通过NIDF 模型联合学习特征选择和近似\n的数据区间，得到最终的特征分数。具体的算法\n过程示意图如图1 所示。\n2\n2\n4\n总第239 期\n吕晓林\n杜\n亮\n周\n芃\n吴\n鹏\n基于邻域区间扰动融合的无监督特征选择算法框架\n图1\n算法过程示意图\nNIDF 模型如下\nmin\nλ，z，wλ\n2 ∑\n4\ni = 1\nw\n2\ni z\nTAiz－λ ∑\n4\ni = 1\nwiz\nTsi\ns．t．\nz\nT1 = 1，z≥0\nw\nT1 = 1，w≥0\n( 1)\n式中: Ai∈Ｒ\nd×d是一个冗余矩阵，用来刻画第i 个近\n似数据集上所有特征间的冗余性; si∈Ｒ\nd×1代表的\n是对第i 个近似数据集进行无监督特征选择后得\n到的特征分数; wi 代表的是第i 个近似数据集的权\n重; λ 是一个自动加权的参数，用来平衡第一项和\n第二项; z∈Ｒ\nd×1是利用式( 1) 联合学习特征选择和\n近似的数据区间后得出的最终特征分数。\n2．2\n方法的优化\n式( 1) 可通过迭代地更新λ、z、w 来求解，详\n细过程如下。\n( 1) 固定z 和w，更新λ。则式( 1) 可等价于\n求解以下问题\nmin\nλ λ\n2z\nTAz－λz\nTs\n( 2)\n式中: A∈Ｒ\nd×d，A = ∑\n4\ni = 1\nw\n2\ni Ai，s = ∑\n4\ni = 1\nwisi 。\n对于式( 2) ，通过求其关于λ 的导数，并令其\n等于0，则可求出\nλ = z\nTs\n2z\nTAz\n( 3)\n( 2) 固定λ 和w，更新z。则式( 1) 可等价于\n求解以下问题\nmin\nzT1= 1，z≥0λz\nTAz－z\nTs\n( 4)\n式中: A∈Ｒ\nd×d，A = ∑\n4\ni = 1\nw\n2\ni Ai，s = ∑\n4\ni = 1\nwisi 。\n很明显，上述问题是一个带有线性约束的凸二\n次规划问题，该问题可用现有的优化工具轻松解决。\n( 3) 固定λ 和z，更新w。则式( 1) 可等价于\n求解以下问题\nmin\nwT1= 1，w≥0λ ∑\n4\ni = 1\nw\n2\ni z\nTAiz －∑\n4\ni = 1\nwiz\nTsi\n( 5)\n这里，引入矩阵H∈Ｒ\nd×d，f∈Ｒ\nd×1，其中H 是\n一个对角矩阵，其主对角线元素为Hii = z\nTAiz，矩\n阵f 中的元素为fi =z\nTsi。则式( 5) 可被重写为\nmin\nwT1= 1，w≥0λw\nTHw－f\nTw\n( 6)\n同样，这是一个带有线性约束的凸二次规划\n问题，可用现有的优化工具解决。\n总体来说，式( 1) 的解法如算法1 所示。另\n外，整个NIDF 模型的过程总结在算法2 中。\n算法1\n式( 1) 的优化解法\n输入: { Ai} 4\ni= 1，{ si} 4\ni= 1;\n初始化: z，w;\n重复:\n1．更新λ 通过式( 3) ;\n2．更新z 通过解决式( 4) ;\n3．更新w 通过解决式( 6) ;\n直到λ 收敛。\n输出: z\n算法2\nNIDF 模型的过程\n输入: 数据集X，样本标签Y，所选特征数m;\n步骤:\n1．对数据集X 进行区间近似，得到其近似数据集{ Xi} 4\ni= 1;\n2．采用经典的无监督算法对新的区间近似数据集{ Xi} 4\ni= 1\n进行特征选择，并分别得到一个特征分数{ si} 4\ni= 1;\n3．利用算法1 求解NIDF 模型，实现对特征选择和近似的\n数据区间联合学习，得出最终的特征分数z;\n4．对特征分数z 进行降序排序，并选出其前m 个特征。\n输出: 选出前m 个特征。\n3\n2\n4\n南京理工大学学报\n第45 卷第4 期\n3\n实验\n本节通过在8 个公开的数据集上进行实验，\n验证本文所提模型的有效性。\n3．1\n数据集\n在实验中，本文使用了多种数据集，包括2 个\n文本数据集，4 个图像数据集，1 个生物数据集，1\n个其他数据集，这些数据集都是进行特征选择的\n常用数据集，数据集的大小如表1 所示。\n表1\n数据集的大小\n数据集\n样本数\n特征数\n类数\nPIE\n1428\n1024\n68\n20NG\n3970\n1000\n4\nBBCNEWS\n737\n1000\n5\nAＲ\n840\n768\n120\nOＲL\n400\n4096\n40\nＲELATHE\n1427\n4322\n2\nCAＲCINOMAS\n174\n9182\n11\nUSPS\n9298\n256\n10\n3．2\n实验对比算法\n提出的模型作为一个后处理过程，可用于无\n监督特征选择方法中，这里使用了3 个现有的无\n监督特征选择方法: 拉普拉斯算法( LapScore) 、多\n类簇特征选择算法( MCFS) 和基于图结构的Kull-\nback-Leibler( KL) 散度最小化算法( Gragh-based\nKullback-Leibler divergence minimization for unsu-\npervised selection，KLMFS) 。\n另外，GＲM 和AGＲM 框架也可用于无监督\n特征选择中的后处理过程，此处将这两个框架应\n用于已有的无监督特征选择方法，可以得出以下\n几组对比算法。\n( 1) 第一组。\nLapScore\n［14］: 基于原始数据集，利用拉普拉斯\n特征选择算法进行特征选择。\nLapScore_GＲM\n［16］: 基于原始数据集，将GＲM\n框架应用于拉普拉斯特征选择算法中进行特征\n选择。\nLapScore _ AGＲM\n［17］: 基于原始数据集，将\nAGＲM 框架应用于拉普拉斯特征选择算法中进行\n特征选择。\nLapScore_NIDF: 新提出的方法，基于构建的\n区间近似数据集，将NIDF 框架应用于拉普拉斯\n特征选择算法中进行特征选择。\n( 2) 第二组。\nMCFS\n［15］: 基于原始数据集，利用多类簇特征\n选择算法进行特征选择。\nMCFS_GＲM: 基于原始数据集，将GＲM 框架\n应用于多类簇特征选择算法中进行特征选择。\nMCFS_AGＲM: 基于原始数据集，将AGＲM 框\n架应用于多类簇特征选择算法中进行特征选择。\nMCFS_NIDF: 新提出的方法，基于构建的区\n间近似数据集，将NIDF 框架应用于多类簇特征\n选择算法中进行特征选择。\n( 3) 第三组。\nKLMFS\n［22］: 基于原始数据集，利用KL 散度最\n小化算法进行特征选择。\nKLMFS_GＲM: 基于原始数据集，将GＲM 框\n架应用于KL 散度最小化算法中进行特征选择。\nKLMFS_AGＲM: 基于原始数据集，将AGＲM\n框架应用于KL 散度最小化算法中进行特征\n选择。\nKLMFS_NIDF: 新提出的方法，基于构建的区\n间近似数据集，将NIDF 框架应用于KL 散度最小\n化算法中进行特征选择。\n3．3\n评价指标\n在实验中，使用了聚类方法常用的两个评价\n指标来评估方法的性能，即聚类准确性\n( Accuracy，ACC) 和归一化互信息( Normalized\nmutual information，NMI) 。这两个指标的值越大，\n表示聚类性能越好。\n聚类准确性: 聚类准确性主要是用来刻画所\n聚的类和样本原始类之间的一对一关系。给定一\n个样本点xi，pi 和qi 分别用来表示聚类结果和样\n本的真实标签，则ACC 为\nACC = 1\nn ∑\nn\ni = 1 δ( qi，map( pi) )\n式中: n 是样本数，δ( x，y) 是一个这样的函数，若\nx =y，δ( x，y) 的值为1，否则为0。map(·) 是一个\n置换函数，将每一个簇索引映射到一个真实的类\n标签中。\n归一化互信息: 归一化互信息主要是用来刻\n画聚类的质量。记C 是真实类标签的集合，C'是\n通过聚类算法计算的类集合，则它们的互信息\nMI( C，C') 为\nMI( C，C') =\n∑\nci∈C，c'j∈C'\np( ci，c'j) log\np( ci，c'j)\np( ci) p( c'j)\n4\n2\n4\n总第239 期\n吕晓林\n杜\n亮\n周\n芃\n吴\n鹏\n基于邻域区间扰动融合的无监督特征选择算法框架\n式中: p( ci) 和p( c'j) 分别是从数据集中任意选定\n的一个样本点属于类ci 和c'j的概率，p( ci，c'j) 是这\n个数据点同时属于这两个类的概率。实验中，使\n用的归一化互信息NMI 为\nNMI( C，C') =\nMI( C，C')\nmax( H( C) ，H( C') )\n式中: H( C) 和H( C') 分别是C 和C'的熵。\n3．4\n实验结果分析\n本文进行了多组实验，以验证所提模型的有\n效性。\n首先，将数据集X 进行区间近似，得到其近\n似数据集{ Xi}\n4\ni= 1，接着，在原始数据集X 和近似\n数据集{ Xi}\n4\ni= 1 上进行无监督特征选择。这里以\nPIE 数据集中的第一个样本图像进行LapScore 特\n征选择为例。实验得出LapScore 在原始数据集X\n上的ACC 值为0．556 0，在近似数据集{ Xi}\n4\ni= 1 上\n的ACC 值分别为0．335 0、0．418 6、0．590 2 以及\n0．433 3，说明对数据集做区间近似在某些情况下\n可以增强特征选择方法的性能，进一步说明区间\n近似数据集对特征选择性能的积极作用，直观的\n特征选择效果如图2 所示。\n图2\n原始数据集和区间近似数据集上LapScore\n的特征选择效果\n其次，针对区间近似数据集对特征选择的不\n稳定影响，包括积极的和消极的，本文联合学习区\n间近似数据集和特征选择———NIDF 模型。这里\n同样以PIE 数据集中的第一个样本图像进行\nLapScore 特征选择为例。在这组实验中，本文以\n原始的LapScore 特征选择方法、经GＲM 和\nAGＲM 处理过的LapScore _ GＲM 和LapScore _\nAGＲM 方法作为对比，实验得出，经本文提出的\nNIDF 处理后的LapScore_NIDF 能一定程度上提高\n特征选择的效果。直观的特征选择效果如图3 所\n示，从图中可以看出，经NIDF 处理后的LapScore_\nNIDF 选择出的特征更集中，更具有代表性。\n图3\n3 种框架基于LapScore 的特征选择效果\n最后，通过在大批量的数据集上分别进行3\n组对比算法的特征选择能力测试，可以看出本文\n提出的模型能一定程度上提高无监督特征选择方\n法的性能，聚类结果分别如表2 和表3 所示，其中\n最好的结果加粗表示，次好的结果加下划线表示。\n另外，3 组算法在不同数据集上ACC 值的直观展\n示如图4 所示。\n表2\n不同数据集上的聚类ACC\n数据集\nLapScore\nLapScore_\nGＲM\nLapScore_\nAGＲM\nLapScore_\nNIDF\nMCFS\nMCFS_\nGＲM\nMCFS_\nAGＲM\nMCFS_\nNIDF\nKLMFS\nKLMFS_\nGＲM\nKLMFS_\nAGＲM\nKLMFS_\nNIDF\nPIE\n0．5349\n0．6823\n0．6915\n0．7501\n0．3653\n0．6221\n0．5720\n0．5685\n0．5857\n0．6501\n0．6314\n0．7043\n20NG\n0．2602\n0．2614\n0．2615\n0．3302\n0．2944\n0．2690\n0．2932\n0．4016\n0．2795\n0．2644\n0．2654\n0．3766\nBBCNEWS\n0．5131\n0．4556\n0．5097\n0．5246\n0．5157\n0．4835\n0．4982\n0．5497\n0．5516\n0．4810\n0．5129\n0．5550\nAＲ\n0．3015\n0．3381\n0．3529\n0．3582\n0．3228\n0．3442\n0．3399\n0．3493\n0．3556\n0．3727\n0．3654\n0．3610\nOＲL\n0．3887\n0．4831\n0．4911\n0．4748\n0．4630\n0．4727\n0．4735\n0．4716\n0．3542\n0．4625\n0．4548\n0．4520\nＲELATHE\n0．5395\n0．5423\n0．5377\n0．5548\n0．5446\n0．5455\n0．5451\n0．5451\n0．5461\n0．5459\n0．5460\n0．5514\nCAＲCINOMAS 0．5998\n0．5186\n0．6812\n0．6425\n0．5182\n0．4322\n0．5199\n0．5519\n0．6174\n0．5881\n0．6305\n0．6744\nUSPS\n0．4985\n0．5690\n0．5805\n0．6373\n0．4369\n0．5736\n0．5821\n0．5514\n0．5893\n0．5804\n0．5749\n0．6113\nAVEＲAGE\n0．4545\n0．4813\n0．5133\n0．5341\n0．4326\n0．4679\n0．4780\n0．4986\n0．4849\n0．4931\n0．4977\n0．5358\n5\n2\n4\n南京理工大学学报\n第45 卷第4 期\n表3\n不同数据集上的聚类NMI\n数据集\nLapScore\nLapScore_\nGＲM\nLapScore_\nAGＲM\nLapScore_\nNIDF\nMCFS\nMCFS_\nGＲM\nMCFS_\nAGＲM\nMCFS_\nNIDF\nKLMFS\nKLMFS_\nGＲM\nKLMFS_\nAGＲM\nKLMFS_\nNIDF\nPIE\n0．8157\n0．8833\n0．8896\n0．9251\n0．6875\n0．8480\n0．8218\n0．8215\n0．8294\n0．8675\n0．8576\n0．9015\n20NG\n0．0392\n0．0406\n0．0416\n0．1252\n0．0797\n0．0346\n0．0705\n0．2235\n0．0697\n0．0426\n0．0454\n0．1857\nBBCNEWS\n0．3731\n0．2410\n0．3295\n0．3794\n0．3741\n0．3386\n0．3468\n0．3924\n0．3705\n0．2824\n0．3175\n0．4205\nAＲ\n0．6651\n0．6777\n0．6873\n0．6934\n0．6709\n0．6788\n0．6743\n0．6808\n0．6936\n0．6999\n0．6980\n0．6939\nOＲL\n0．6222\n0．6967\n0．7056\n0．6936\n0．6852\n0．6884\n0．6896\n0．6888\n0．6082\n0．6862\n0．6809\n0．6793\nＲELATHE\n0．0082\n0．0212\n0．0225\n0．0284\n0．0077\n0．0022\n0．0058\n0．0076\n0．0043\n0．0042\n0．0042\n0．0135\nCAＲCINOMAS 0．6505\n0．5690\n0．7266\n0．6844\n0．5111\n0．3952\n0．5120\n0．5438\n0．6675\n0．6246\n0．6842\n0．7175\nUSPS\n0．4721\n0．5053\n0．5178\n0．5634\n0．3883\n0．5245\n0．5418\n0．4810\n0．5508\n0．5274\n0．5211\n0．5473\nAVEＲAGE\n0．4558\n0．4544\n0．4901\n0．5116\n0．4256\n0．4388\n0．4578\n0．4799\n0．4743\n0．4669\n0．4761\n0．5199\n图4\n不同数据集上ACC 值的直观展示\n6\n2\n4\n总第239 期\n吕晓林\n杜\n亮\n周\n芃\n吴\n鹏\n基于邻域区间扰动融合的无监督特征选择算法框架\n通过分析可以得出，经本文提出的NIDF 模\n型处理后，LapScore _ NIDF、MCFS _ NIDF 以及\nKLMFS_NIDF 相比较于原始的LapScore、MCFS 和\nKLMFS 方法，在准确性ACC 上分别能提高将近\n17．51%、15．26%和10．50%。然而，由图4 可以直\n观看出，本文提出的NIDF 模型并不能绝对优于\n现有的GＲM 和AGＲM 框架，但是从总体上看，经\n本文提出的NIDF 模型处理后的新方法，相比较\n于经GＲM 和AGＲM 处理后方法的平均值，在准\n确性ACC 上分别能提高将近7．40%、5．41% 和\n8．16%。\n总的来说，尽管本文提出的NIDF 模型不能\n绝对优于现有的GＲM 和AGＲM 框架，但是在大\n多数情况下可以达到比这两个框架更好的效果，\n少数情况下效果相差无几。另外，本文提出的\nNIDF 作为一个后处理框架，相比较于原始的无监\n督特征选择方法，可以达到对其性能的进一步提\n升，因此有一定的实用意义。\n3．5\n参数设置\n在求数据集的近似区间数据集时，涉及到邻域\nk 和参数alpha，这里设置的邻域数k 是15，alpha\n是3。由于K-means 聚类的结果受初始值影响较\n大，故本文在每一次评估算法性能时重复进行\nK-means聚类20 次，最终给出平均值。本文在利用\n所选特征评估算法性能时，所选择的特征数集合是\n［10: 10: 100］，最终取所有结果的平均数。\n4\n结束语\n本文首先采用区间的方式对数据集进行近\n似。将一个数据集表示成与其相关几个数据集的\n组合，然后基于上述得到的多种数据表示，本文通\n过实验验证了其优劣性，并思考从全局角度对这\n些数据集进行处理，进而提出了一种新的模\n型———基于邻域区间扰动融合的无监督特征选择\n算法框架NIDF。实验证明，通过对特征的最终得\n分和区间近似数据集的联合学习，该模型能一定\n程度上提高原始特征选择方法的性能，并且在大\n多数情况下优于现有的两个后处理框架GＲM 和\nAGＲM。\n参考文献:\n［1］Wolf L，Shashua A．Feature selection for unsupervised\nand supervised inference: The emergence of sparsity in\na weighted-based\napproach ［J］．\nThe\nJournal\nof\nMachine Learning Ｒesearch，2005，6: 1855－1887．\n［2］Yin Jun，Zeng Weiming，Wei Lai．Optimal feature\nextraction methods for classification methods and their\napplications to biometric recognition［J］．Knowledge-\nBased Systems，2016，99: 112－122．\n［3］肖宇，于剑．基于近邻传播算法的半监督聚类［J］．\n软件学报，2008，19( 11) : 2803－2813．\nXiao Yu，Yu Jian．Semi-supervised clustering based on\naffinity propagation algorithm［J］．Journal of Software，\n2008，19( 11) : 2803－2813．\n［4］Xu Zenglin，King I，Lyu M ＲT，et al．Discriminative\nsemi-supervised feature selection via manifold regulari-\nzation［J］．IEEE Transactions on Neural Networks，\n2010，21( 7) : 1033－1047．\n［5］李昆仑，曹铮，曹丽苹，等．半监督聚类的若干新进\n展［J］．模式识别与人工智能，2009，22 ( 5) :\n735－742．\nLi Kunlun，Cao Zheng，Cao Liping，et al．Some devel-\nopments on semi-supervised clustering［J］．Pattern\nＲecognition and Artificial Intelligence，2009，22( 5) :\n735－742．\n［6］张莉，孙钢，郭军．基于K－均值聚类的无监督的特\n征选择方法［J］．计算机应用研究，2005，22( 3) :\n23－24，42．\nZhang Li，Sun Gang，Guo Jun．Unsupervised feature\nselection method based on K-means clustering［J］．\nApplication Ｒesearch of Computers，2005，22( 3) : 23－\n24，42．\n［7］Zhou Wei，Wu Chengdong，Yi Yugen，et al．Structure\npreserving non-negative feature self-representation for\nunsupervised feature selection ［J］．IEEE Access，\n2017，5: 8792－8803．\n［8］Shang Ｒonghua，Xu Kaiming，Jiao Licheng．Subspace\nlearning for unsupervised feature selection via adaptive\nstructure learning and rank approximation［J］．Neuro-\ncomputing，2020，413: 72－84．\n［9］Gu Nannan，Fan Mingyu，Du Liang，et al．Efficient\nsequential\nfeature\nselection\nbased\non\nadaptive\neigenspace model［J］．Neurocomputing，2015，161:\n199－209．\n［10］Kohavi Ｒ，John G H．Wrappers for feature subset\nselection［J］．Artificial Intelligence，1997，97( 1－2) :\n273－324．\n［11］Du Liang，Shen Yidong．Unsupervised feature selection\nwith adaptive structure learning［C］/ /Proceedings of\nthe 21th ACM SIGKDD International Conference on\n7\n2\n4\n南京理工大学学报\n第45 卷第4 期\nKnowledge Discovery and Data Mining．New York，\nUSA: Association for Computing Machinery，2015:\n209－218．\n［12］Zhao Hua，Du Liang，Wei Jianglai，et al．\nLocal\nsensitive dual concept factorization for unsupervised\nfeature\nselection ［J］．\nIEEE\nAccess，2020，8:\n133128－133143．\n［13］Zhang Yan，Zhang Zhao，Li Sheng，et al．Unsupervised\nnonnegative adaptive feature extraction for data repre-\nsentation［J］．IEEE Transactions on Knowledge and\nData Engineering，2019，31( 12) : 2423－2440．\n［14］He Xiaofei，Cai Deng，Niyogi P．Laplacian score for fea-\nture selection［C］/ /Advances in Neural Information\nProcessing Systems 18．Cambridge，USA: Massachusetts\nInstitute of Technology Press，2005: 507－514．\n［15］Cai Deng，Zhang Chiyuan，He Xiaofei．Unsupervised\nfeature\nselection\nfor\nmulti-cluster\ndata ［C ］/ /\nProceedings of the 16th ACM SIGKDD international\nconference on Knowledge discovery and data mining-\nKDD’10．New York，USA: Association for Computing\nMachinery，2010: 333－342．\n［16］Wang De，Nie Feiping，Huang Heng．Feature selection\nvia global redundancy minimization［J］．IEEE Transac-\ntions on Knowledge and Data Engineering，2015，\n27( 10) : 2743－2755．\n［17］Nie Feiping，Yang Sheng，Zhang Ｒui，et al．A general\nframework for auto-weighted feature selection via global\nredundancy minimization［J］．IEEE Transactions on\nImage Processing，2019，28( 5) : 2428－2438．\n［18］Yu Hao，Wen Guoqiu，Gan Jiangzhang，et al．Self-\npaced learning for K-means clustering algorithm［J］．\nPattern Ｒecognition Letters，2020，132: 69－75．\n［19］张聪，顾晓清，王洪元．一种具有抗噪能力的贝叶斯\n可能性聚类方法［J］．南京理工大学学报，2020，\n44( 5) : 614－623．\nZhang Cong，Gu Xiaoqing，Wang Hongyuan．Bayesian\npossibility clustering method with anti-noise ability［J］．\nJournal\nof\nNanjing\nUniversity\nof\nScience\nand\nTechnology，2020，44( 5) : 614－623．\n［20］尹芳，宋垚，李骜．基于局部优化奇异值分解和K-\nmeans 聚类的协同过滤算法［J］．南京理工大学学\n报，2019，43( 6) : 720－726．\nYin Fang，Song Yao，Li Ao．Collaborative filtering\nalgorithm based on singular value decomposition of\nlocal optimization and K-means clustering［J］．Journal\nof Nanjing University of Science and Technology，\n2019，43( 6) : 720－726．\n［21］Georgogiannis A．Ｒobust K-means: A theoretical revisit\n［J］．\nAdvances in Neural Information Processing\nSystems，2016，29: 2891－2899．\n［22］Lv X，Du Liang．Graph-based Kullback-Leibler diver-\ngence minimization for unsupervised feature selection\n［C］/ /2021 The 5th International Conference on\nMachine Learning and Soft Computing．New York，\nUSA: Association for Computing Machinery，2021:\n109－114．\n8\n2\n4\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-20",
  "updated": "2024-10-20"
}