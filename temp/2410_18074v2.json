{
  "id": "http://arxiv.org/abs/2410.18074v2",
  "title": "UnCLe: Unsupervised Continual Learning of Depth Completion",
  "authors": [
    "Suchisrit Gangopadhyay",
    "Xien Chen",
    "Michael Chu",
    "Patrick Rim",
    "Hyoungseob Park",
    "Alex Wong"
  ],
  "abstract": "We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.",
  "text": "UnCLe: Unsupervised Continual Learning of Depth Completion\nSuchisrit Gangopadhyay1∗, Xien Chen1∗, Michael Chu1∗, Patrick Rim1, Hyoungseob Park1, Alex Wong1\nI. ABSTRACT\nWe propose UnCLe, a standardized benchmark for Unsuper-\nvised Continual Learning of a multimodal depth estimation\ntask: Depth completion aims to infer a dense depth map\nfrom a pair of synchronized RGB image and sparse depth\nmap. We benchmark depth completion models under the\npractical scenario of unsupervised learning over continuous\nstreams of data. Existing methods are typically trained on a\nstatic, or stationary, dataset. However, when adapting to novel\nnon-stationary distributions, they “catastrophically forget”\npreviously learned information. UnCLe simulates these non-\nstationary distributions by adapting depth completion models\nto sequences of datasets containing diverse scenes captured\nfrom distinct domains using different visual and range sensors.\nWe adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual\nlearning of depth completion. We benchmark these models for\nindoor and outdoor and investigate the degree of catastrophic\nforgetting through standard quantitative metrics. Furthermore,\nwe introduce model inversion quality as an additional measure\nof forgetting. We find that unsupervised continual learning\nof depth completion is an open problem, and we invite\nresearchers to leverage UnCLe as a development platform.\nII. INTRODUCTION\nThree-dimensional (3D) perception is fundamental for\nspatial tasks such as autonomous navigation, robotic manip-\nulation, and augmented reality. The systems to tackle these\ntasks are typically built upon a sensor suite including range\n(e.g. lidar, radar) and optics (e.g. camera). Depth completion\nis the task of recovering the dense depth map of a 3D scene\nfrom the measurements of these range sensors, which often\nyield sparse 3D coordinates, by leveraging the dense 2D\ntopology captured in RGB images.\nDespite these advantages, existing depth completion meth-\nods are predominately trained and tested on a single dataset\nunder the assumption that the distribution of 3D scenes is\nstationary. When deployed in the real world with evolving\nconditions and unseen domains where the encountered distri-\nbution is non-stationary, the model must be continually trained\nto adapt to new scenes. However, continual re-training can\nlead to the problem of where a model’s performance degrades\nsignificantly on previously learned tasks when trained on new\ntasks [1], [2], [3], [4]. This problem is commonly referred to\nas “catastrophic forgetting.”\n*Equal contribution\n1Authors are with the Department of Computer Science, Yale University,\nCT 06520, USA, {firstname.lastname}@yale.edu\nContinual (or lifelong) learning is the training paradigm\nthat addresses the challenge of catastrophic forgetting by\nenabling a single model to learn from a continual non-\nstationary stream of datasets, adapting to new tasks while\npreserving performance on previous tasks. Retraining models\nfor supervised 3D tasks on new domains is often impractical\ndue to the difficulty and high cost of acquiring accurate\nground-truth. This highlights the need for methods that can\nlearn new data distributions in an unsupervised manner.\nTherefore, our work investigates unsupervised continual\nlearning for depth completion, aiming to enable a multimodal\ndepth estimation model to learn new domains without ground-\ntruth while retaining performance on previously seen domains.\nThis paper presents a comprehensive benchmark for ex-\nisting unsupervised continual learning methods for depth\ncompletion to streamline further research in this area. We\nbegin with the motivating observation of how the performance\nof three unsupervised depth completion models [5], [6],\n[7] degrades after being finetuned on two sequences of\ndatasets. We adopt three canonical continual learning methods\n(i.e., regularization-based [8], [9] and rehearsal-based [10]\nmethods) for the 3D perception task of unsupervised depth\ncompletion. We evaluate on sequences of both indoor [11],\n[7], [12] and outdoor [13], [14], [15] datasets and detail our\nevaluation protocols for future methods.\nFurthermore, we present model inversion as a new quali-\ntative evaluation protocol to measure catastrophic forgetting\nof 3D vision models. Model inversion is the reconstruction\nof input data given the corresponding output and the frozen\nmodel weights. We can compare continual learning methods\nby evaluating the fidelity of reconstructed input data from\npreviously seen domains using the weights trained on new\ntasks. This is based on the assumption that a model that can\nreconstruct the input image has experienced less forgetting.\nOur contributions are as follows:\n• We are the first to adapt existing, canonical regularization-\nbased and rehearsal-based continual learning methods\nfor unsupervised depth completion. After implementing\nthese methods, we re-tune the existing best training\nsettings for the depth completion models that we use\nfor each continual learning method.\n• We provide extensive quantitative results on a sequence\nof indoor datasets, and on another sequence of outdoor\ndatasets. In this way, we demonstrate the effectiveness\nof continual learning methods for lifelong learning of\ntasks spanning from robotics to autonomous driving.\n• We introduce model inversion, a novel qualitative evalu-\nation protocol to measure catastrophic forgetting of 3D\nvision models.\narXiv:2410.18074v2  [cs.CV]  25 Oct 2024\nIII. RELATED WORKS\nA. Continual Learning\nRegularization-based One approach to mitigating catas-\ntrophic forgetting is through regularization, which introduces\nconstraints or penalties to the loss function to ensure that\nnew tasks are learned without significantly altering the\nweights from previous training. The first class of methods\nin this category focuses on weight regularization, selectively\nconstraining changes to network parameters that are important\nfor previous tasks. Elastic Weight Consolidation (EWC)\n[8] selectively decreases the plasticity of model weights\ndetermined by the Fisher information matrix. [16] estimates\nparameter importance based on their contribution to loss\nchanges during training. [17] assess importance by measuring\nthe sensitivity of model outputs to parameter variations. [18]\ncombines the regularization techniques of [8] and [16] to\nleverage their strengths in mitigating catastrophic forgetting.\nThe other class of regularization-based techniques focuses\non function regularization, which aims to preserve the model’s\noutput behavior on previous tasks by constraining changes\nin the intermediate features or final predictions through\nthe knowledge distillation (KD) framework [19]. Learning\nWithout Forgetting (LwF) [9] is a pioneering approach that\nmitigates catastrophic forgetting by using the new task data\nto approximate the responses of the old model. To further\nenhance retention, LwM [20] integrates attention maps into\nthe KD process to capture essential feature representations,\nwhile EBLL [21] preserves feature reconstructions by utilizing\ntask-specific autoencoders. Methods like GD [22] leverage\nexternal unlabeled data to extend regularization beyond the\ntraining set, increasing the model’s ability to generalize to\nunseen tasks. Generative approaches such as MeRGANs [23]\nand LifelongGAN [24] simulate old task data, thus reducing\nthe need for storing large datasets. Additionally, feature-based\nregularization techniques like PODNet [25] and LUCIR [26]\nwork to preserve feature similarity, addressing distribution\nshifts between tasks and improving overall stability. Bayesian\napproaches also offer robust function regularization. FRCL\n[27] and FROMP [28] use probabilistic models to regularize\nthe functional space, providing smooth task transitions by\nquantifying uncertainty. Broader frameworks like VCL [29]\nextend these ideas, using variational inference to maintain a\nbalance between task stability and plasticity. In this paper, we\nchoose to concentrate on EWC and LWF as we find them to\nbe milestone works of the two classes of regularization-based\napproaches.\nRehearsal-based Replay-based approaches in continual\nlearning mitigate catastrophic forgetting by reintroducing\npast examples during the training of new tasks. These\nmethods include Experience Replay (“Replay”) [10], which\nstores actual samples from previous tasks to train on again\nand reinforce performance on previously seen data. Within\nExperience Replay, there are different selection strategies for\nchoosing the replay buffer, such as randomized Reservoir\nSampling [30], [31], [32], class-based sampling [33], [34],\nand Generative replay [35] utilizes generative models like\nGANs or VAEs to produce synthetic data mimicking the\ndata distributions of previous tasks. Variational autoencoder\napproaches [36], [37], [38], [39], [40], [41] are able to control\nthe generated data labels, but suffer from blurry quality.\nGAN approaches [42], [41], [43] are able to improve the\nquality of the generated input data over the VAE methods.\nFeature replay stores data on the feature-level instead of the\nraw input data in order to be less resource-expensive. Some\napproaches [44], [45], [46] use feature distillation between\nnew and old models. [47] stores initial class statistics, like\nmean and covariance, to rectify biases in predictions. For\nour benchmark, we opt to use Experience Replay as our\nbaseline for rehearsal-based continual learning methods as it\nis the foundation for rehearsal-based methods. Its direct use of\nactual training data, despite being resource-intensive, makes\nit a more effective method, especially crucial for the high\nfidelity required in unsupervised depth completion tasks. We\nuse randomized sampling because many of the other discussed\nsampling methods are either specific to classification tasks\nor have similar performance.\nToken-based These methods aim to store information\nlearned from previous tasks in a small, fixed set of weights\ncalled “tokens,” which can be thought of as learnable biases.\nSome methods additionally utilize a small memory buffer,\nbut others are able to obtain similar performance without\none. They are often cheap and scalable, and recent methods\nhave achieved state-of-the-art results in continual learning for\nimage classification. However, these methods are currently\nfor 2D classification tasks [48], [49], [50] or can only be\napplied to transformer-based architectures [51]. Token-based\ncontinual learning methods for 3D regression tasks, such\nas depth completion, have not yet been explored. Thus, we\nare not able to obtain baselines for these methods for depth\ncompletion.\nContinual Learning for Depth Estimation The Mon-\noDepthCL [51] framework, employs a dual-memory rehearsal-\nbased method to address the challenges of catastrophic forget-\nting in unsupervised monocular depth estimation. CoDEPS\n[52] employs a unique domain-mixing strategy for pseudo-\nlabel generation with efficient experience replay. However,\nprevious works in this field typically focus on a single\nmodality and lack standardized benchmarks, limiting their\napplicability to real-world scenarios where the use of multi-\nmodal data is standard. Our paper contributes to this field by\nestablishing a benchmark for depth completion using both\nimages and sparse depth data, addressing these gaps and\nsetting a foundation for future research in continual depth\nestimation.\nB. Depth Completion\nDepth estimation is the task of reconstructing the 3D scene\nfrom visual inputs, e.g., a pair of calibrated stereo images [53],\n[54], [55], [56] or a monocular image [57], [57], [58], [59],\n[60], [61], [62], [63], [64]. Inferring depth from a single\nimage is an ill-posed task; recovering metric depth typically\nrequires an additional sensor, e.g., lidar [65], [66], radar [67].\nDepth completion [68], [69], [70], [71] is the task of\ninferring a dense metric depth from an RGB image and\nsynchronized sparse point cloud. Supervised methods [72],\n[73], [74], [75], [76], [77], [78], [79] rely on large datasets\nwith ground-truth dense depth maps, which are used to train\nmodels capable of mapping sparse depth points and RGB\nimages to dense depth maps. While effective, these approaches\nare limited by the availability and cost of obtaining annotated\ndata, making it less practical for continual learning scenarios\nwhere the collection of new data may not include dense depth\nmap annotations. Supervised depth completion methods are\nless practical than unsupervised approaches, as achieving\nprecise dense depth maps for every task in a continual learning\nscenario is challenging.\nUnsupervised depth completion methods [80], [81], [7],\n[6], [82], [83], [68], [70], in contrast, leverage the inherent\nstructure and patterns within the RGB image and sparse\ndepth data to infer missing depth values, making them more\nadaptable for continual learning as they do not require dense\nground-truth depth maps. [80] utilized Perspective-n-Point\n[84] and RANSAC [85] to align adjacent video frames. [68]\ntrains a depth prior constrained on the image. FusionNet\n[6] leverages synthetic data to learn a prior from a scene,\nwhile [86] converts synthetic data to real domain to use\nrendered depth. VOICED [7] approximated a scene with\nscaffolding. [82] introduced an adaptive scheme to reduce\npenalties incurred on occluded regions. KBNet [5] maps\nthe image onto the 3D scene frustum using calibrated back-\nprojection. [83] decouples scale and structure. [87] uses visual\nSLAM features and [69] proposed monitored distillation for\npositive congruent training.\nHowever, all of these methods are subject to catastrophic\nforgetting as they rely on updating model weights to fit onto\na new target dataset without the objective of retaining past\ninformation. This aspect makes it challenging for them to\nadapt to a continual learning setting where both new and\npreviously seen tasks need to be handled without performance\ndegradation on either. Therefore, it is critical to develop\nmethods for these models that enable continual learning,\nensuring that they can effectively adapt to new data while\npreserving previous examples.\nIV. METHOD FORMULATION\nProblem definition. The task of unsupervised depth\ncompletion can be formulated as follows: Let I : Ω⊂R2 →\nR3\n+ denote an RGB image at current timestamp t obtained\nfrom a calibrated camera, and z : Ωz ⊂Ω→R+ represent\nthe corresponding sparse depth map derived from a projected\n3D point cloud. Given the image I, the sparse depth map z,\nand the intrinsic calibration matrix K ∈R3×3, the goal is to\nlearn a function fθ(I, z) that estimates a dense depth map,\nwhich accurately recovers the distances between the camera\nand all visible points in the 3D scene, without relying on\nground-truth depth annotations. Similarly to [5], instead of\nusing ground truth we use a photometric reprojection error by\ncomparing the reconstructed image It from adjacent frames\nIτ for τ ∈{t −1, t + 1} with the estimated relative camera\npose matrix gτt ∈SE(3). To estimate gτt, we jointly train a\npose network pθ(I, Iτ):\nˆIτ(x, ˆd, gτt) = Iτ\n\u0000πgτtK−1¯x ˆd(x)\n\u0001\n.\n(1)\nIn this formula, K is the camera’s intrinsic matrix, π is\nthe canonical perspective projection, and ¯x represents the\nhomogeneous coordinates [x⊤, 1]⊤of x ∈Ω. We train\npθ(I, Iτ) during training-time, but it is not used for inference.\nIn the continual learning setting, our objective is to adapt\na pretrained model fθ and pθ, trained on an initial dataset\nT0 = {(I(i)\n0 , z(i)\n0 , K(i)\n0 )}n\ni=1, to a sequence of new datasets\nT1, T2, . . . , TN, where each Tk = {(I(i)\nk , z(i)\nk , K(i)\nk )}nk\ni=1 and\nnk denotes the number of training examples in Tk. The\nchallenge is to incrementally train both fθ and pθ on each new\ntarget dataset Tk without experiencing significant degradation\nin performance on the source dataset S and all previously\nseen target datasets Tj ∀j < k. This requires effectively\nmitigating catastrophic forgetting while enabling the model\nto generalize across different domains in the depth completion\ntask.\nUnsupervised Depth Completion Loss. We train fθ(I, z)\nto minimize\nL = wphℓph + wszℓsz + wsmℓsm\n(2)\nwhere each loss term can be weighted differently by their\nrespective w.\nThe photometric consistency loss,\nℓph = 1\n|Ω|\nX\nτ∈T\nX\nx∈Ω\nwco|ˆIτ(x) −I(x)|+\nwst\n\u00001 −SSIM(ˆIτ(x), I(x))\n\u0001\n,\n(3)\nencourages both structural and color similarity between the\nreconstructed and actual center images.\nThe sparse depth consistency loss,\nℓsz =\n1\n|Ωz|\nX\nx∈Ωz\n| ˆd(x) −z(x)|.\n(4)\nencourages consistency between the input sparse depth and\npredicted depth map over the sparse depth map’s domain\n(Ωz).\nThe local smoothness loss,\nℓsm = 1\n|Ω|\nX\nx∈Ω\nλX(x)|∂X ˆd(x)| + λY (x)|∂Y ˆd(x)|.\n(5)\nencourages local smoothness by penalizing depth map\ngradients in the x−(∂X) and y−(∂Y ) directions. These\ngradients are weighted by the corresponding image gradients\nλX = e−|∂XIt(x)| and λY = e−|∂Y It(x)| to account for object\nedges.\nA. Elastic Weight Consolidation\nFor the task of continual depth completion, we implement\nEWC as follows. When training on a new dataset Tk,\nwe first load the previously trained model fθ∗, which is\nparameterized by the weight matrices θ∗optimized for Tk−1.\nThese parameters are frozen and treated as a reference during\ntraining on Tk. The Fisher information matrix Fi quantifies\neach parameter’s θi importance to the previous model’s\nperformance on Tk−1. The final EWC loss is therefore defined\nas:\nLewc = λewc\nX\ni\n1\n2Fi(θi −θ∗\ni )2\n(6)\nEWC loss penalizes important weights from moving too far\naway from their values optimized for the previous task. We\nadd this to the total loss for unsupervised depth completion\nL in Eq. 2 with λewc = 1, which we found to be optimal\nafter extensive tuning.\nB. Learning Without Forgetting\nTo implement Learning Without Forgetting (LwF) for\nthe depth completion task, we aim to preserve output\nbehavior from previous tasks while training only on new\ndata. Specifically, we load the checkpoint of the old model\nfθ, which are kept frozen during training. This old model\nprocesses the new data to generate predictions, denoted as\nˆIτ(x, ˆd, gτt), where ˆd represents the predicted depth and gτt\nis the relative pose transformation between frames.\nWe compute a mean squared error (MSE) loss between the\noutputs of the frozen model and the new model f ′\nθ. This LwF\nloss encourages the new model’s predictions ˆI′τ(x, ˆd, gτt)\nto remain consistent with the frozen model’s predictions.\nSimultaneously, the joint loss function Llwf includes the loss\nbetween the new model’s output and the ground truth depth\ndata from the current task. The final loss function is defined\nas:\nLlwf = λo · Lold + Lnew\n(7)\nHere, Lold computes the loss between the frozen model’s\noutput ˆIτ and the new model’s output ˆI′τ, while Lnew is the\nloss between the new model’s output In and the ground truth\ndepth map zn. Both Lold and Lnew include all the terms in\nthe unsupervised loss algorithm L in Eq. 2. The parameter\nλo balances the contribution of the old and new task losses.\nAfter many experiments, we set the optimal λo to be 0.5.\nThis approach allows the model to learn from new depth\ndata without requiring access to previous task data, as the\nfrozen model’s predictions act as a proxy for past knowledge.\nC. Experience Replay\nTo perform experience replay with depth completion, we\nmaintain a replay buffer, which retains a fixed number of\nrepresentative samples from each dataset previously trained\non. The selections and the integration of the replay buffer into\nthe new task are designed to balance computational efficiency\nwith performance retention. Empirically, a buffer size of\n64 data points for each previous dataset was determined to\noffer an optimal balance. During each training iteration on\na new target dataset, a batch of data from the replay buffer\nis reintroduced into the training process. The data points\nfrom both the current and historical datasets are processed\nas follows:\nLet Dnew = Tk denote the set of new training data points\nfrom the current target dataset, where N is the number of new\ndata points. Similarly, let Dreplay = {(I(j)\nr , z(j)\nr , K(j)\nr )}M\nj=1\ndenote the set of replayed data points from the buffer, where\nM is the number of data points in the replay buffer. Each\ntraining batch is made up of a 50-50 new-replay data ratio,\nwhere the replay half is evenly split between previous datasets.\nThe total loss function Ltotal for a training iteration is then\nformulated as:\nLtotal = Lnew + Lreplay\n(8)\nwhere Lnew is the unsupervised loss L in Eq. 2 computed on\nthe new training data, and Lreplay is the unsupervised loss L\ncomputed on the replayed data.\nDuring training, the loss Lreplay computed from the data\npoints in the replay buffer is equal to total loss Ltotal from\nthe preceding continual training step, ensuring consistency\nand retention of previously acquired knowledge.\nD. Model Inversion\nWe propose a method of using model inversion to evaluate\na model’s forgetting. In other words, we can try to retrieve\nan image that a model fω(I, z) was trained on using just its\nweights and a supervised output complete depth map d. To\ninvert a depth completion model, we freeze the model, start\nwith a uniformly random set of pixels ˜I, and update ˜I to\nminimize ∥fθ(˜I, z) −d∥1, where z is the data point’s actual\nsparse depth map. To evaluate the model’s forgetting we use\nthe pretrained model fθ(I, z) to get d = fθ(I, z), where I\nand z are the actual input image and sparse depth for the data\npoint. Thus, we ultimately minimize ∥fω(˜I, z) −fθ(I, z)∥1\nto retrieve an old training image from fω(I, z), having been\ntrained on new data. Although we try to retrieve a trained\nimage, a similar method can be used to retrieve the input\nsparse depth map.\nV. EXPERIMENTS\nFor evaluation, we compute four key metrics: Mean\nAbsolute Error (MAE), Root Mean Squared Error (RMSE),\nInverse MAE (iMAE), and Inverse RMSE (iRMSE). These\nmetrics are described in detail in Table XIV. For qualitative\nresults, we use the proposed model inversion method to\nmeasure catastrophic forgetting on the previous task dataset.\nOur experiments consist of continual training on both indoor\nand outdoor datasets. For indoor sequences, we first train on\nNYUv2 (Table I) and evaluate the continual learning process\nover the sequences NYUv2 →ScanNet →VOID (Tables II,\nIII, and IV), and NYUv2 →VOID →ScanNet (Tables V,\nVI, and VII). For outdoor sequences, we train on KITTI and\nevaluate on KITTI →Virtual KITTI →Waymo (Tables VIII,\nIX, and X), and KITTI →Waymo →Virtual KITTI (Tables\nXI, XII, and XIII).\nA. Datasets\nWe use three datasets for indoor experiments and three\ndatasets for outdoor experiments. Indoor datasets: The NYU\nDepth V2 dataset [11] contains 464 indoor scenes captured\nTABLE I: Performance of the pretrained models on the NYUv2 (initial indoor) and KITTI (initial outdoor) datasets.\nNYUv2\nKITTI\nModel\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\n94.641\n177.134\n18.492\n39.309\n295.410\n1159.270\n1.200\n3.490\nFusionNet\n102.872\n192.23\n20.913\n42.821\n267.690\n1157.070\n1.080\n3.190\nKBNet\n85.842\n169.833\n17.065\n35.368\n256.961\n1122.597\n1.011\n3.155\nTABLE II: Evaluation on NYUv2. Starting from NYUv2 pretrained weights, models are trained on ScanNet (NYUv2 →ScanNet) and\nsubsequently on VOID (NYUv2 →ScanNet →VOID)\nNYUv2 →ScanNet\nNYUv2 →ScanNet →VOID\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n102.556\n193.830\n19.794\n44.300\n105.918\n193.336\n19.977\n42.623\nEWC\n99.426\n188.504\n19.866\n45.909\n111.247\n205.710\n19.425\n39.442\nLWF\n106.500\n198.029\n24.327\n64.434\n111.632\n200.986\n19.580\n40.796\nReplay\n101.879\n187.149\n20.214\n43.836\n104.386\n187.938\n21.597\n45.943\nFusionNet\nFinetuned\n106.999\n202.943\n20.984\n43.806\n117.709\n205.254\n21.647\n40.639\nEWC\n113.555\n217.060\n23.441\n50.584\n107.319\n201.506\n19.765\n39.049\nLwF\n100.577\n188.393\n18.645\n36.827\n103.157\n192.620\n18.413\n35.504\nReplay\n102.422\n193.054\n20.908\n43.278\n100.180\n187.368\n19.583\n39.493\nKBNet\nFinetuned\n94.357\n193.184\n17.321\n38.087\n108.699\n215.057\n19.139\n40.711\nEWC\n90.145\n186.402\n16.529\n36.180\n102.277\n200.576\n17.945\n37.814\nLwF\n97.987\n186.385\n18.598\n38.178\n91.784\n177.807\n17.628\n37.366\nReplay\n91.243\n175.683\n18.957\n39.680\n95.622\n184.347\n19.312\n40.620\nTABLE III: Evaluation on ScanNet. Starting from NYUv2 pretrained weights, models are trained on ScanNet (NYUv2 →ScanNet) and\nsubsequently on VOID (NYUv2 →ScanNet →VOID)\nNYUv2 →ScanNet\nNYUv2 →ScanNet →VOID\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n19.077\n47.153\n7.421\n18.953\n20.261\n47.069\n7.850\n18.953\nEWC\n18.496\n46.451\n7.155\n18.673\n19.553\n47.143\n7.375\n18.933\nLWF\n20.715\n47.719\n8.105\n19.566\n19.739\n47.770\n7.408\n19.101\nReplay\n21.478\n52.750\n7.681\n19.395\n21.711\n53.581\n7.544\n19.417\nFusionNet\nFinetuned\n18.021\n48.733\n6.520\n18.896\n25.064\n52.576\n9.898\n21.274\nEWC\n17.756\n47.644\n6.521\n18.874\n17.943\n47.301\n9.689\n19.085\nLwF\n16.231\n44.917\n5.986\n17.924\n17.349\n45.771\n6.517\n18.304\nReplay\n17.352\n48.241\n5.909\n17.841\n18.149\n49.309\n6.128\n17.923\nKBNet\nFinetuned\n14.290\n41.550\n5.289\n16.610\n15.961\n44.048\n5.866\n17.393\nEWC\n14.091\n41.344\n5.230\n16.578\n16.992\n44.559\n6.756\n17.856\nLwF\n16.060\n43.786\n5.675\n16.748\n17.358\n45.771\n6.028\n17.111\nReplay\n14.690\n42.702\n5.100\n16.192\n15.265\n43.755\n5.721\n16.447\nTABLE IV: Evaluation on VOID. Starting from NYUv2 pretrained weights, models are trained on ScanNet (NYUv2 →ScanNet) and\nsubsequently on VOID (NYUv2 →ScanNet →VOID)\nNYUv2 →ScanNet →VOID\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n37.656\n93.155\n19.229\n46.179\nEWC\n39.356\n95.292\n19.058\n43.938\nLwF\n37.585\n90.167\n19.413\n44.805\nReplay\n42.275\n100.427\n21.953\n49.866\nFusionNet\nFinetuned\n36.010\n87.008\n18.540\n43.159\nEWC\n35.195\n87.492\n17.600\n42.119\nLwF\n34.572\n85.400\n17.499\n41.203\nReplay\n35.363\n91.210\n18.582\n45.632\nKBNet\nFinetuned\n32.313\n83.962\n16.163\n40.282\nEWC\n35.139\n89.733\n18.923\n45.515\nLWF\n36.032\n90.976\n19.718\n48.217\nReplay\n34.583\n90.770\n18.192\n45.739\nTABLE V: Evaluation on NYUv2. Starting from NYUv2 pretrained weights, models are trained on VOID (NYUv2 →VOID) and\nsubsequently on ScanNet (NYUv2 →VOID →ScanNet)\nNYUv2 →VOID\nNYUv2 →VOID →ScanNet\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n99.042\n182.196\n18.819\n38.632\n99.281\n190.244\n19.317\n43.954\nEWC\n106.489\n193.701\n19.039\n38.042\n100.611\n192.186\n20.232\n47.167\nLwF\n104.732\n188.505\n20.060\n44.116\n100.974\n189.749\n20.457\n44.925\nReplay\n103.689\n190.762\n20.651\n43.913\n103.848\n189.240\n22.354\n48.550\nFusionNet\nFinetuned\n103.278\n193.581\n19.207\n38.495\n108.858\n208.497\n22.423\n48.256\nEWC\n103.478\n193.886\n18.983\n37.579\n107.600\n206.906\n22.223\n48.358\nLwF\n100.143\n186.084\n18.301\n35.863\n95.975\n180.125\n17.545\n34.595\nReplay\n102.832\n191.146\n20.288\n40.693\n99.377\n186.812\n19.513\n39.477\nKBNet\nFinetuned\n122.475\n241.367\n21.239\n45.710\n94.590\n194.518\n17.170\n37.546\nEWC\n107.250\n215.432\n19.176\n41.068\n91.339\n188.499\n16.640\n36.704\nLwF\n105.551\n186.658\n21.038\n44.658\n88.545\n174.485\n17.333\n36.729\nReplay\n88.359\n172.586\n17.765\n37.798\n90.553\n175.286\n18.522\n38.949\nTABLE VI: Evaluation on VOID. Starting from NYUv2 pretrained weights, models are trained on VOID (NYUv2 →VOID) and\nsubsequently on ScanNet (NYUv2 →VOID →ScanNet)\nNYUv2 →VOID\nNYUv2 →VOID →ScanNet\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n38.963\n93.236\n20.730\n47.662\n38.825\n90.594\n22.433\n50.817\nEWC\n38.771\n93.812\n20.367\n47.073\n38.763\n91.593\n22.709\n51.732\nLwF\n42.499\n97.798\n24.867\n56.016\n45.398\n100.679\n28.677\n60.894\nReplay\n48.855\n111.550\n26.400\n57.079\n50.553\n111.130\n33.331\n72.056\nFusionNet\nFinetuned\n35.221\n87.667\n18.149\n43.783\n34.371\n85.641\n17.435\n41.743\nEWC\n35.446\n87.978\n18.158\n42.460\n36.762\n93.557\n20.917\n54.146\nLwF\n34.572\n85.400\n17.499\n41.203\n32.425\n82.813\n15.834\n38.576\nReplay\n36.542\n91.689\n19.489\n46.890\n37.752\n94.848\n21.040\n51.004\nKBNet\nFinetuned\n32.746\n84.514\n16.277\n40.223\n34.129\n86.748\n18.148\n44.097\nEWC\n34.572\n86.971\n17.827\n43.114\n34.643\n89.025\n18.807\n46.172\nLwF\n36.321\n91.587\n20.385\n49.752\n38.944\n96.792\n22.951\n54.977\nReplay\n34.544\n90.054\n18.392\n45.751\n39.409\n99.833\n21.509\n52.007\nTABLE VII: Evaluation on ScanNet. Starting from NYUv2 pretrained weights, models are trained on VOID (NYUv2 →VOID) and\nsubsequently on ScanNet (NYUv2 →VOID →ScanNet)\nNYUv2 →VOID →ScanNet\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n18.921\n47.969\n7.260\n19.078\nEWC\n18.584\n47.835\n7.302\n19.397\nLwF\n20.229\n47.214\n7.468\n18.396\nReplay\n25.125\n55.480\n9.297\n20.723\nFusionNet\nFinetuned\n16.889\n47.318\n6.170\n18.574\nEWC\n16.681\n46.848\n6.185\n18.627\nLwF\n16.192\n44.808\n5.937\n17.789\nReplay\n17.392\n47.919\n6.017\n17.799\nKBNet\nFinetuned\n14.070\n41.372\n5.169\n16.390\nEWC\n14.710\n42.201\n5.422\n16.767\nLwF\n15.578\n42.373\n5.428\n16.112\nReplay\n15.122\n42.934\n5.356\n16.431\nusing RGB-D sensors, offering 1449 densely labeled pairs\nof aligned RGB and depth images. It is a benchmark dataset\nwidely used in indoor depth estimation tasks. NYUv2 is the\nprimary dataset on which we pretrain our depth completion\nmodels before outdoor continual learning. The VOID dataset\n[7] provides sparse depth maps and RGB frames for indoor\nenvironments, with approximately 58,000 frames. VOID\nemphasizes handling low-texture regions and scenes with\nsignificant camera motion, which are crucial for testing\nrobustness in indoor depth completion. ScanNet [12] is\na large-scale indoor dataset that includes more than 2.5\nmillion frames with corresponding RGB-D data. It provides\ndense depth ground truth and 3D reconstructions of indoor\nenvironments.\nFor the indoor datasets, we use the evaluation protocol in\n[7] and cap the depth values from 0.2 to 5 meters. During\nTABLE VIII: Evaluation on KITTI. Starting from KITTI pretrained weights, models are trained on VKITTI (KITTI →VKITTI) and\nsubsequently on Waymo (KITTI →VKITTI →Waymo)\nKITTI →VKITTI\nKITTI →VKITTI →Waymo\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n345.655\n1278.237\n1.431\n4.054\n2327.250\n3859.149\n6.375\n8.733\nEWC\n353.852\n1295.422\n1.487\n4.202\n2625.052\n4150.911\n7.281\n9.161\nLwF\n414.647\n1337.016\n1.656\n4.666\n1801.192\n3085.398\n4.909\n6.807\nReplay\n351.029\n1353.348\n1.379\n3.759\n326.585\n1344.943\n1.247\n3.715\nFusionNet\nFinetuned\n281.757\n1270.535\n1.136\n3.715\n317.543\n1281.393\n1.274\n3.697\nEWC\n285.263\n1285.849\n1.139\n3.715\n284.831\n1298.388\n1.117\n3.664\nLwF\n277.504\n1182.062\n1.147\n3.527\n312.728\n1221.368\n1.403\n3.945\nReplay\n290.409\n1310.964\n1.106\n3.537\n293.817\n1332.329\n1.102\n3.576\nKBNet\nFinetuned\n311.888\n1270.047\n1.353\n3.943\n297.905\n1252.206\n1.222\n3.655\nEWC\n286.406\n1266.132\n1.127\n3.667\n315.838\n1380.112\n1.275\n3.760\nLwF\n277.309\n1166.711\n1.117\n3.480\n304.141\n1184.546\n1.531\n4.204\nReplay\n299.883\n1368.686\n1.082\n3.473\n307.247\n1416.628\n1.080\n3.512\nTABLE IX: Evaluation on VKITTI. Starting from KITTI pretrained weights, models are trained on VKITTI (KITTI →VKITTI) and\nsubsequently on Waymo (KITTI →VKITTI →Waymo)\nKITTI →VKITTI\nKITTI →VKITTI →Waymo\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n966.268\n4031.375\n1.295\n3.449\n7896.476\n14200.599\n9.852\n13.761\nEWC\n858.869\n3881.263\n1.142\n3.242\n5801.778\n10616.265\n7.568\n10.538\nLwF\n881.591\n4055.195\n1.215\n3.403\n5001.189\n9463.238\n6.963\n10.024\nReplay\n934.656\n4153.693\n1.066\n2.866\n1050.157\n4211.335\n1.734\n3.228\nFusionNet\nFinetuned\n631.225\n3334.405\n0.723\n3.118\n790.000\n3685.247\n1.153\n3.782\nEWC\n635.105\n3365.572\n0.666\n2.917\n787.348\n3540.777\n1.309\n4.081\nLwF\n612.924\n3182.058\n0.699\n3.009\n517.690\n1667.856\n1.236\n2.685\nReplay\n633.858\n3348.971\n0.596\n2.692\n645.891\n3328.864\n0.678\n2.703\nKBNet\nFinetuned\n658.075\n3380.623\n1.194\n3.793\n804.278\n3759.132\n1.155\n3.856\nEWC\n673.121\n3552.306\n0.872\n3.373\n814.097\n3819.788\n1.392\n4.260\nLwF\n741.125\n3564.760\n1.146\n4.120\n875.343\n3681.855\n2.778\n6.438\nReplay\n677.493\n3567.431\n0.854\n3.452\n714.782\n3718.890\n0.930\n3.400\nTABLE X: Evaluation on Waymo. Starting from KITTI pretrained weights, models are trained on VKITTI (KITTI →VKITTI) and\nsubsequently on Waymo (KITTI →VKITTI →Waymo)\nKITTI →VKITTI →Waymo\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n532.430\n1731.000\n1.216\n2.656\nEWC\n570.878\n1770.593\n1.272\n2.724\nLwF\n565.197\n1758.846\n1.280\n2.719\nReplay\n546.847\n1756.500\n1.199\n2.586\nFusionNet\nFinetuned\n535.436\n1763.895\n1.189\n2.627\nEWC\n496.805\n1652.860\n1.179\n2.637\nLwF\n633.107\n3182.974\n0.976\n3.727\nReplay\n486.746\n1665.689\n1.172\n2.645\nKBNet\nFinetuned\n485.462\n1675.315\n1.176\n2.675\nEWC\n480.940\n1648.952\n1.166\n2.638\nLwF\n522.286\n1736.891\n1.270\n2.773\nReplay\n497.199\n1743.683\n1.178\n2.683\ntraining, we use a crop shape of 416 × 576 for NYUv2 and\na crop shape of 416 × 512 for VOID and Scannet.\nOutdoor datasets: The KITTI dataset [13] is a widely\nused benchmark for autonomous driving, consisting of over\n93,000 stereo image pairs and sparse LiDAR depth maps\nsynchronized with the images that were captured from a wider\nrange of urban and rural scenes. KITTI is the primary dataset\non which we pretrain our depth completion models before\noutdoor continual learning. The Waymo [14] Open Dataset\nincludes approximately 230,000 frames of high-resolution\nimages and dense LiDAR point clouds, covering a wide range\nof driving environments and conditions. Virtual KITTI [15]\nis a synthetic dataset designed to replicate KITTI scenes,\nproviding over 21,000 frames with dense ground truth depth.\nIt allows for evaluating domain adaptation, as we use multiple\ndata domains to simulate domain discrepancies which cause\nTABLE XI: Evaluation on KITTI. Starting from KITTI pretrained weights, models are trained on Waymo (KITTI →Waymo) and\nsubsequently on VKITTI (KITTI →Waymo →VKITTI)\nKITTI →Waymo\nKITTI →Waymo →VKITTI\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n1418.898\n2543.668\n4.608\n7.031\n388.247\n1355.323\n1.525\n4.340\nEWC\n1215.182\n2238.520\n4.056\n6.447\n442.459\n1468.315\n1.735\n4.799\nLwF\n2144.099\n3518.483\n5.767\n7.693\n415.244\n1346.477\n1.704\n4.616\nReplay\n337.384\n1216.277\n1.397\n3.670\n375.981\n1216.501\n1.512\n3.800\nFusionNet\nFinetuned\n322.472\n1300.640\n1.431\n4.086\n300.817\n1298.835\n1.288\n4.011\nEWC\n315.959\n1273.695\n1.292\n3.823\n379.437\n1326.112\n1.517\n4.052\nLwF\n296.444\n1206.908\n1.191\n3.414\n292.372\n1208.645\n1.214\n3.661\nReplay\n291.101\n1310.897\n1.094\n3.493\n302.014\n1369.085\n1.120\n3.542\nKBNet\nFinetuned\n351.044\n1292.620\n2.129\n5.190\n314.624\n1273.683\n1.480\n4.225\nEWC\n320.521\n1282.303\n1.499\n4.256\n305.615\n1140.672\n1.299\n3.635\nLwF\n291.704\n1131.483\n1.298\n3.419\n333.231\n1228.935\n1.874\n4.548\nReplay\n307.389\n1413.860\n1.082\n3.478\n353.075\n1634.394\n1.101\n3.485\nTABLE XII: Evaluation on Waymo. Starting from KITTI pretrained weights, models are trained on Waymo (KITTI →Waymo) and\nsubsequently on VKITTI (KITTI →Waymo →VKITTI)\nKITTI →Waymo\nKITTI →Waymo →VKITTI\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n525.367\n1706.168\n1.223\n2.672\n6236.390\n7681.729\n14.570\n16.039\nEWC\n525.354\n1684.696\n1.232\n2.672\n7389.857\n9277.607\n17.713\n19.281\nLwF\n565.813\n1777.017\n1.280\n2.750\n7508.272\n9683.822\n16.030\n17.836\nReplay\n583.101\n1731.286\n1.308\n2.756\n642.807\n1771.024\n1.407\n2.822\nFusionNet\nFinetuned\n512.323\n1760.833\n1.203\n2.704\n518.302\n1772.169\n1.210\n2.708\nEWC\n480.053\n1637.659\n1.166\n2.627\n495.613\n1748.918\n1.180\n2.699\nLwF\n469.060\n1613.890\n1.154\n2.622\n506.399\n1644.618\n1.210\n2.665\nReplay\n492.232\n1685.874\n1.149\n2.565\n508.490\n1715.932\n1.187\n2.652\nKBNet\nFinetuned\n496.915\n1701.736\n1.190\n2.654\n608.243\n2144.506\n1.213\n2.699\nEWC\n489.191\n1709.312\n1.180\n2.696\n620.688\n1878.749\n1.339\n2.881\nLwF\n493.037\n1744.968\n1.188\n2.714\n593.374\n1777.834\n1.387\n2.913\nReplay\n487.310\n1704.594\n1.170\n2.664\n581.065\n1983.698\n1.207\n2.694\nTABLE XIII: Evaluation on VKITTI. Starting from KITTI pretrained weights, models are trained on Waymo (KITTI →Waymo) and\nsubsequently on VKITTI (KITTI →Waymo →VKITTI)\nKITTI →Waymo →VKITTI\nModel\nMethod\nMAE ↓\nRMSE ↓\niMAE ↓\niRMSE ↓\nVOICED\nFinetuned\n858.263\n3986.613\n1.111\n3.289\nEWC\n909.536\n4247.935\n1.005\n3.423\nLwF\n914.993\n4191.131\n1.216\n3.501\nReplay\n910.002\n4161.021\n1.172\n3.613\nFusionNet\nFinetuned\n704.775\n3421.726\n0.947\n5.643\nEWC\n649.886\n3419.303\n0.631\n2.697\nLwF\n633.432\n3199.120\n0.935\n3.826\nReplay\n652.735\n3407.310\n0.631\n2.748\nKBNet\nFinetuned\n790.162\n4124.413\n1.004\n4.177\nEWC\n747.993\n3706.933\n0.999\n3.301\nLwF\n792.276\n3492.584\n1.416\n4.583\nReplay\n743.578\n3754.860\n1.038\n3.741\ncatastrophic forgetting.\nFor KITTI and VKITTI, we cap the depth output during\nevaluation from 0.001 to 100 meters and use a depth crop\nof 240 × 1216. During training, we use a crop of 320 × 768.\nFor Waymo, we cap the evaluation output from 0.001 to 80\nmeters and use a depth crop of 768 × 1920, and we use a\ntraining crop of 800 × 640.\nB. Results\nQuantitative. Observing the quantitative results, we see\nthat the continual learning methods we use perform similarly\nwell across datasets and depth completion models. According\nto Tables V and II, for indoor experiments, EWC outperforms\nthe finetuned baseline by 1.6% on MAE and 0.71% on RMSE.\nLwF outperforms the finetuned model by 4% on MAE and\n6.3% on RMSE. Replay outperforms the finetuned by 5.7% on\nTarget Depth\nFinetuned\nReplay\nEWC\nLwF\nTarget Image\nNYUv2\nVOID\nFig. 1: Model Inversion Qualitative Results (Top row) We have the target image I we retrieve from the NYUv2 dataset and the target\ndepth map predicted by inputting the image and its corresponding depth map into the KBnet NYUv2 pretrained model. We show an\nattempt at retrieving this image from the pretrained model and the VOID pretrained model for comparison. (Bottom) We also have the\nimage retrieval attempts for KBnet models trained from NYUv2 to VOID with all of our continual learning methods.\nMetric\nDefinition\nMAE\n1\n|Ω|\nP\nx∈Ω| ˆd(x) −dgt(x)|\nRMSE\n\u0000 1\n|Ω|\nP\nx∈Ω| ˆd(x) −dgt(x)|2\u00011/2\niMAE\n1\n|Ω|\nP\nx∈Ω|1/ ˆd(x) −1/dgt(x)|\niRMSE\n\u0000 1\n|Ω|\nP\nx∈Ω|1/ ˆd(x) −1/dgt(x)|2\u00011/2\nTABLE XIV: Error metrics. dgt denotes the ground-truth depth.\nMAE and 7.4% on RMSE. According to Tables VIII and XI,\nfor outdoor experiments, different from indoor experiments,\nregularization based continual learning methods harms the\nperformance, where EWC performs worse than finetuned by\n3.4% on MAE and 1% on RMSE. LwF performs worse than\nfinetuned by 3.7% on MAE and outperforms finetuned by\n2.3% on RMSE. However, Replay outperforms the finetuned\nby 12.9% on MAE and 3.6% on RMSE. Overall, Replay\nbased method performs better in both indoor and outdoor\nscenarios, while the regularization methods do not seem to\nconsistently outperform simple finetuning to a significant\ndegree. There is one specific case when training VOICED\nmodel on waymo dataset where EWC and LWF performs\nmuch worse than Replay (i.e. Replay outperforms EWC by\n72% and LWF by 84%).\nQualitative. For our model inversion method, we currently\nhave qualitative results that show the difference in retrieval\nquality between baselines and continual learning methods.\nFigure 1 shows inversion results from KBnet models that\nhave been pretrained on NYUv2 and finetuned on VOID,\nwhere the replay method outperforms the other continual\nlearning methods, improving the finetuned model by 11.57%.\nAs expected, the NYUv2 pretrained model is able to retrieve\nthe 2D structure of the scene with higher fidelity than all of\nthe models which have been trained on VOID afterwards;\nwhereas the VOID model is unable to retrieve the image at\nall because it has never been trained on it. In line with the\nquantitative results in Table V, we observe that the replay\nmethod can retrieve clearer object boundaries (e.g. the bottom\nof the vanity in the bottom right bounding box) and smoother\nsurfaces (e.g. the wall in the top left bounding box).\nVI. DISCUSSION\nTo the best of our knowledge, this is the first study to\napply continual learning methods to the problem of depth\ncompletion. By establishing a benchmark for continual depth\ncompletion across indoor and outdoor datasets, this paper sets\nthe foundation for future work in extending continual learning\nto more complex, multi-modal depth estimation problems. We\nhope that this contribution will motivate further research into\nmore effective strategies to combat catastrophic forgetting in\ndepth completion and related 3D tasks.\nREFERENCES\n[1] R. M. French, “Catastrophic forgetting in connectionist networks,”\nTrends in cognitive sciences, vol. 3, no. 4, pp. 128–135, 1999.\n[2] M. McCloskey and N. J. Cohen, “Catastrophic interference in connec-\ntionist networks: The sequential learning problem,” in Psychology of\nlearning and motivation.\nElsevier, 1989, vol. 24, pp. 109–165.\n[3] S. Thrun, “Is learning the n-th thing any easier than learning the first?”\nAdvances in neural information processing systems, vol. 8, 1995.\n[4] R. Ratcliff, “Connectionist models of recognition memory: constraints\nimposed by learning and forgetting functions.” Psychological review,\nvol. 97, no. 2, p. 285, 1990.\n[5] A. Wong and S. Soatto, “Unsupervised depth completion with calibrated\nbackprojection layers,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 12 747–12 756.\n[6] A. Wong, S. Cicek, and S. Soatto, “Learning topology from synthetic\ndata for unsupervised depth completion,” IEEE Robotics and Automa-\ntion Letters, vol. 6, no. 2, pp. 1495–1502, 2021.\n[7] A. Wong, X. Fei, S. Tsuei, and S. Soatto, “Unsupervised depth com-\npletion from visual inertial odometry,” IEEE Robotics and Automation\nLetters, vol. 5, no. 2, pp. 1899–1906, 2020.\n[8] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,\nA. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska,\net al., “Overcoming catastrophic forgetting in neural networks,” Pro-\nceedings of the national academy of sciences, vol. 114, no. 13, pp.\n3521–3526, 2017.\n[9] Z. Li and D. Hoiem, “Learning without forgetting,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition,\n2017, pp. 5077–5086.\n[10] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne, “Expe-\nrience replay for continual learning,” Advances in neural information\nprocessing systems, vol. 32, 2019.\n[11] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation\nand support inference from rgbd images,” in European conference on\ncomputer vision.\nSpringer, 2012, pp. 746–760.\n[12] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\nM. Nießner, “Scannet: Richly-annotated 3d reconstructions of indoor\nscenes,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017, pp. 5828–5839.\n[13] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving? the kitti vision benchmark suite,” in 2012 IEEE Conference\non Computer Vision and Pattern Recognition.\nIEEE, 2012, pp. 3354–\n3361.\n[14] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,\nJ. Guo, Y. Zhou, Y. Chai, B. Caine, et al., “Scalability in perception\nfor autonomous driving: Waymo open dataset,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 2446–2454.\n[15] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtual kitti: An annotated\nvirtual dataset for scene understanding,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition workshops,\n2016, pp. 28–37.\n[16] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through\nsynaptic intelligence,” in International conference on machine learning.\nPMLR, 2017, pp. 3987–3995.\n[17] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars,\n“Memory aware synapses: Learning what (not) to forget,” in Proceedings\nof the European conference on computer vision (ECCV), 2018, pp.\n139–154.\n[18] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, “Riemannian\nwalk for incremental learning: Understanding forgetting and intransi-\ngence,” in Proceedings of the European conference on computer vision\n(ECCV), 2018, pp. 532–547.\n[19] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” arXiv preprint arXiv:1503.02531, 2015.\n[20] P. Dhar, R. V. Singh, K.-C. Peng, Z. Wu, and R. Chellappa, “Learning\nwithout memorizing,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2019, pp. 5138–5146.\n[21] A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars, “Encoder\nbased lifelong learning,” in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 1320–1328.\n[22] K. Lee, K. Lee, J. Shin, and H. Lee, “Overcoming catastrophic\nforgetting with unlabeled data in the wild,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2019, pp.\n312–321.\n[23] A. Rios and L. Itti, “Closed-loop memory gan for continual learning,”\narXiv preprint arXiv:1811.01146, 2018.\n[24] S. Zhai, Y. Cheng, W. Zhang, and F. Lu, “Lifelong gan: Continual\nlearning for conditional image generation,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2019, pp.\n2759–2768.\n[25] A. Douillard, M. Cord, C. Ollion, T. Robert, and E. Valle, “Podnet:\nPooled outputs distillation for small-tasks incremental learning,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2020, pp. 1951–1960.\n[26] S. Hou, X. Pan, C. Change Loy, Z. Wang, and D. Lin, “Learning a\nunified classifier incrementally via rebalancing,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019, pp. 831–839.\n[27] M. K. Titsias, J. Schwarz, A. G. d. G. Matthews, R. Pascanu, and Y. W.\nTeh, “Functional regularisation for continual learning with gaussian\nprocesses,” arXiv preprint arXiv:1901.11356, 2019.\n[28] Y. Pan, F. Li, and W. K. Lee, “Continual deep learning by functional\nregularisation of memorable past,” arXiv preprint arXiv:2007.15302,\n2020.\n[29] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner, “Variational continual\nlearning,” in International Conference on Learning Representations,\n2018.\n[30] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania,\nP. H. Torr, and M. Ranzato, “On tiny episodic memories in continual\nlearning,” arXiv preprint arXiv:1902.10486, 2019.\n[31] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu, and G. Tesauro,\n“Learning to learn without forgetting by maximizing transfer and\nminimizing interference,” arXiv preprint arXiv:1810.11910, 2018.\n[32] J. S. Vitter, “Random sampling with a reservoir,” ACM Transactions\non Mathematical Software (TOMS), vol. 11, no. 1, pp. 37–57, 1985.\n[33] D. Lopez-Paz and M. Ranzato, “Gradient episodic memory for continual\nlearning,” Advances in neural information processing systems, vol. 30,\n2017.\n[34] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl:\nIncremental classifier and representation learning,” in Proceedings of\nthe IEEE conference on Computer Vision and Pattern Recognition,\n2017, pp. 2001–2010.\n[35] H. Shin, J. K. Lee, J. Kim, and J. Kim, “Continual learning with deep\ngenerative replay,” Advances in neural information processing systems,\nvol. 30, 2017.\n[36] M. Riemer, T. Klinger, D. Bouneffouf, and M. Franceschini, “Scalable\nrecollections for continual lifelong learning,” in Proceedings of the\nAAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp.\n1352–1359.\n[37] M. Rostami, S. Kolouri, and P. K. Pilly, “Complementary learning\nfor overcoming catastrophic forgetting using experience replay,” arXiv\npreprint arXiv:1903.04566, 2019.\n[38] B. Pfülb, A. Gepperth, and B. Bagus, “Continual learning with fully\nprobabilistic models,” arXiv preprint arXiv:2104.09240, 2021.\n[39] R. Kemker and C. Kanan, “Fearnet: Brain-inspired model for incre-\nmental learning,” arXiv preprint arXiv:1711.10563, 2017.\n[40] S. Gopalakrishnan, P. R. Singh, H. Fayek, S. Ramasamy, and A. Am-\nbikapathi, “Knowledge capture and replay for continual learning,” in\nProceedings of the IEEE/CVF winter conference on applications of\ncomputer vision, 2022, pp. 10–18.\n[41] A. Ayub and A. R. Wagner, “Eec: Learning to encode and regenerate\nimages for continual learning,” arXiv preprint arXiv:2101.04904, 2021.\n[42] O. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, and M. Nabi,\n“Learning to remember: A synaptic plasticity driven framework for\ncontinual learning,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2019, pp. 11 321–11 329.\n[43] Y. X. Wu, L. Herranz, X. Liu, J. van de Weijer, B. Raducanu,\nand T. Tuytelaars, “Memory replay gans: Learning to generate new\ncategories without forgetting,” in Proceedings of the 32nd International\nConference on Neural Information Processing Systems, 2018, pp. 5967–\n5977.\n[44] K. Zhu, W. Zhai, Y. Cao, J. Luo, and Z.-J. Zha, “Self-sustaining\nrepresentation expansion for non-exemplar class-incremental learning,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 9296–9305.\n[45] X. Liu, C. Wu, M. Menta, L. Herranz, B. Raducanu, A. D. Bagdanov,\nS. Jui, and J. v. de Weijer, “Generative feature replay for class-\nincremental learning,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops, 2020, pp.\n226–227.\n[46] A. Iscen, J. Zhang, S. Lazebnik, and C. Schmid, “Memory-efficient\nincremental learning through feature adaptation,” in Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XVI 16.\nSpringer, 2020, pp. 699–715.\n[47] E. Belouadah and A. Popescu, “Il2m: Class incremental learning\nwith dual memory,” in Proceedings of the IEEE/CVF international\nconference on computer vision, 2019, pp. 583–592.\n[48] A. Douillard, A. Rame, G. Couairon, and M. Cord, “Dytox: Trans-\nformers for continual learning with dynamic token expansion,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition workshops, 2022, pp. 9285–9295.\n[49] J. Wang, H. He, M. B. A. McDermott, and P. Szolovits, “Learning\nto prompt for continual learning,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2022,\npp. 139–149.\n[50] Z. Wang, Z. Zhan, L. Qi, J. Zhang, K. Chen, B. Liu, J. Peng,\nand T. Zhang, “Continual learning with lifelong vision transformer,”\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 10 893–10 903, 2022.\n[51] H. Chawla, A. Varma, E. Arani, and B. Zonooz, “Continual learning\nof unsupervised monocular depth from videos,” in Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision,\n2024, pp. 8419–8429.\n[52] N. Vödisch, K. Petek, W. Burgard, and A. Valada, “Codeps: Online\ncontinual learning for depth estimation and panoptic segmentation,”\narXiv preprint arXiv:2303.10147, 2023.\n[53] H. Xu and J. Zhang, “Aanet: Adaptive aggregation network for efficient\nstereo matching,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 1959–1968.\n[54] Z. Berger, P. Agrawal, T. Y. Liu, S. Soatto, and A. Wong, “Stereoscopic\nuniversal perturbations across different architectures and datasets,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 15 180–15 190.\n[55] A. Wong, M. Mundhra, and S. Soatto, “Stereopagnosia: Fooling stereo\nnetworks with adversarial perturbations,” in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 35, 2021, pp. 2879–2888.\n[56] F. Wang, S. Galliani, C. Vogel, P. Speciale, and M. Pollefeys, “Patch-\nmatchnet: Learned multi-view patchmatch stereo,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 14 194–14 203.\n[57] C. Godard, O. M. Aodha, and G. J. Brostow, “Unsupervised monocular\ndepth estimation with left-right consistency,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2017, pp. 270–279.\n[58] X. Fei, A. Wong, and S. Soatto, “Geo-supervised visual depth\nprediction,” IEEE Robotics and Automation Letters, vol. 4, no. 2,\npp. 1661–1668, 2019.\n[59] A. Wong and S. Soatto, “Bilateral cyclic constraint and adaptive regular-\nization for unsupervised monocular depth prediction,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 5644–5653.\n[60] R. Upadhyay, H. Zhang, Y. Ba, E. Yang, B. Gella, S. Jiang, A. Wong,\nand A. Kadambi, “Enhancing diffusion models with 3d perspective\ngeometry constraints,” ACM Transactions on Graphics (TOG), vol. 42,\nno. 6, pp. 1–15, 2023.\n[61] N. Zhang, F. Nex, G. Vosselman, and N. Kerle, “Lite-mono: A\nlightweight cnn and transformer architecture for self-supervised monoc-\nular depth estimation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 18 537–18 546.\n[62] Y. Zhang, M. Poggi, F. Tosi, X. Guo, Z. Zhu, G. Huang, and\nS. Mattoccia, “Monovit: Self-supervised monocular depth estimation\nwith a vision transformer,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2023, pp. 9708–9719.\n[63] Z. Zeng, Y. Wu, H. Park, D. Wang, F. Yang, S. Soatto, D. Lao, B.-W.\nHong, and A. Wong, “Rsa: Resolving scale ambiguities in monocular\ndepth estimators through language descriptions,” Advances in neural\ninformation processing systems, 2024.\n[64] Z. Zeng, D. Wang, F. Yang, H. Park, S. Soatto, D. Lao, and A. Wong,\n“Wordepth: Variational language prior for monocular depth estimation,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024, pp. 9708–9719.\n[65] M. Jaritz, R. De Charette, E. Wirbel, X. Perrotton, and F. Nashashibi,\n“Sparse and dense data with cnns: Depth completion and semantic\nsegmentation,” in 2018 International Conference on 3D Vision (3DV).\nIEEE, 2018, pp. 52–60.\n[66] V. Ezhov, H. Park, Z. Zhang, R. Upadhyay, H. Zhang, C. C. Chandrappa,\nA. Kadambi, Y. Ba, J. Dorsey, and A. Wong, “All-day depth completion,”\nin 2023 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2024.\n[67] A. D. Singh, Y. Ba, A. Sarker, H. Zhang, A. Kadambi, S. Soatto,\nM. Srivastava, and A. Wong, “Depth estimation from camera image\nand mmwave radar point cloud,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n9275–9285.\n[68] Y. Yang, A. Wong, and S. Soatto, “Dense depth posterior (ddp) from\nsingle image and sparse range,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n3353–3362.\n[69] T. Y. Liu, P. Agrawal, A. Chen, B.-W. Hong, and A. Wong, “Monitored\ndistillation for positive congruent depth completion,” in Computer\nVision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23–27, 2022, Proceedings, Part II.\nSpringer, 2022, pp.\n35–53.\n[70] Y. Wu, T. Y. Liu, H. Park, S. Soatto, D. Lao, and A. Wong,\n“Augundo: Scaling up augmentations for monocular depth completion\nand estimation,” in European Conference on Computer Vision. Springer,\n2024.\n[71] H. Park, A. Gupta, and A. Wong, “Test-time adaptation for depth\ncompletion,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2024, pp. 20 519–20 529.\n[72] M. Hu, S. Wang, B. Li, S. Ning, L. Fan, and X. Gong, “Penet: Towards\nprecise and efficient image guided depth completion,” in 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA).\nIEEE,\n2021, pp. 13 656–13 662.\n[73] A. Li, Z. Yuan, Y. Ling, W. Chi, C. Zhang, et al., “A multi-scale guided\ncascade hourglass network for depth completion,” in Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision,\n2020, pp. 32–40.\n[74] J. Park, K. Joo, Z. Hu, C. Liu, and I.-S. Kweon, “Non-local spatial\npropagation network for depth completion,” in European Conference\non Computer Vision (ECCV), 2020, pp. 120–136.\n[75] A. Eldesokey, M. Felsberg, K. Holmquist, and M. Persson, “Uncertainty-\naware cnns for depth completion: Uncertainty from beginning to end,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2020, pp. 12 014–12 023.\n[76] W. Van Gansbeke, D. Neven, B. De Brabandere, and L. Van Gool,\n“Sparse and noisy lidar completion with rgb guidance and uncertainty,”\nin 2019 16th International Conference on Machine Vision Applications\n(MVA).\nIEEE, 2019, pp. 1–6.\n[77] Y. Zhang and T. Funkhouser, “Deep depth completion of a single rgb-d\nimage,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2018, pp. 175–185.\n[78] Y. Zhang, X. Guo, M. Poggi, Z. Zhu, G. Huang, and S. Mattoccia,\n“Completionformer: Depth completion with convolutions and vision\ntransformers,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023, pp. 18 527–18 536.\n[79] Z. Yu, Z. Sheng, Z. Zhou, L. Luo, S. Cao, H. Gu, H. Zhang, and\nH. Shen, “Aggregating feature point cloud for depth completion,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 8732–8743.\n[80] F. Ma, G. Cavalheiro, and S. Karaman, “Self-supervised sparse-to-\ndense: Self-supervised depth completion from lidar and monocular\ncamera,” in 2019 International Conference on Robotics and Automation\n(ICRA).\nIEEE, 2019, pp. 3288–3295.\n[81] S. S. Shivakumar, T. Nguyen, I. D. Miller, S. W. Chen, V. Kumar,\nand C. J. Taylor, “Dfusenet: Deep fusion of rgb and sparse depth\ninformation for image guided dense depth completion,” in 2019 IEEE\nIntelligent Transportation Systems Conference (ITSC).\nIEEE, 2019,\npp. 13–20.\n[82] A. Wong, X. Fei, B.-W. Hong, and S. Soatto, “An adaptive framework\nfor learning unsupervised depth completion,” IEEE Robotics and\nAutomation Letters, vol. 6, no. 2, pp. 3120–3127, 2021.\n[83] Z. Yan, K. Wang, X. Li, Z. Zhang, J. Li, and J. Yang, “Desnet: Decom-\nposed scale-consistent network for unsupervised depth completion,” in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 37,\nno. 3, 2023, pp. 3109–3117.\n[84] V. Lepetit, F. Moreno-Noguer, and P. Fua, “Epnp: An accurate o (n)\nsolution to the pnp problem,” International journal of computer vision,\nvol. 81, no. 2, p. 155, 2009.\n[85] M. A. Fischler and R. C. Bolles, “Random sample consensus: a\nparadigm for model fitting with applications to image analysis and\nautomated cartography,” Communications of the ACM, vol. 24, no. 6,\npp. 381–395, 1981.\n[86] A. Lopez-Rodriguez, B. Busam, and K. Mikolajczyk, “Project to adapt:\nDomain adaptation for depth completion from noisy and sparse sensor\ndata,” in Proceedings of the Asian Conference on Computer Vision\n(ACCV), 2020.\n[87] J. Jeon, H. Lim, D. U. Seo, and H. Myung, “Struct-mdc: Mesh-refined\nunsupervised depth completion leveraging structural regularities from\nvisual slam,” in IEEE Robotics and Automation Letters (RA-L), vol. 7,\nno. 3, 2022, pp. 6391–6398.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2024-10-23",
  "updated": "2024-10-25"
}