{
  "id": "http://arxiv.org/abs/2410.18494v2",
  "title": "Assured Automatic Programming via Large Language Models",
  "authors": [
    "Martin Mirchev",
    "Andreea Costea",
    "Abhishek Kr Singh",
    "Abhik Roychoudhury"
  ],
  "abstract": "With the advent of AI-based coding engines, it is possible to convert natural\nlanguage requirements to executable code in standard programming languages.\nHowever, AI-generated code can be unreliable, and the natural language\nrequirements driving this code may be ambiguous. In other words, the intent may\nnot be accurately captured in the code generated from AI-coding engines like\nCopilot. The goal of our work is to discover the programmer intent, while\ngenerating code which conforms to the intent and a proof of this conformance.\nOur approach to intent discovery is powered by a novel repair engine called\nprogram-proof co-evolution, where the object of repair is a tuple (code,\nlogical specification, test) generated by an LLM from the same natural language\ndescription. The program and the specification capture the initial operational\nand declarative description of intent, while the test represents a concrete,\nalbeit partial, understanding of the intent. Our objective is to achieve\nconsistency between the program, the specification, and the test by\nincrementally refining our understanding of the user intent. Reaching\nconsistency through this repair process provides us with a formal, logical\ndescription of the intent, which is then translated back into natural language\nfor the developer's inspection. The resultant intent description is now\nunambiguous, though expressed in natural language. We demonstrate how the\nunambiguous intent discovered through our approach increases the percentage of\nverifiable auto-generated programs on a recently proposed dataset in the Dafny\nprogramming language.",
  "text": "Assured Automatic Programming via Large Language Models\nMARTIN MIRCHEV, National University of Singapore, Singapore\nANDREEA COSTEA, Delft University of Technology, The Netherlands\nABHISHEK KR SINGH, National University of Singapore, Singapore\nABHIK ROYCHOUDHURY, National University of Singapore, Singapore\nWith the advent of AI-based coding engines, it is possible to convert natural language requirements to\nexecutable code in standard programming languages. However, AI-generated code can be unreliable, and\nthe natural language requirements driving this code may be ambiguous. In other words, the intent may not\nbe accurately captured in the code generated from AI-coding engines like Copilot. The goal of our work is\nto discover the programmer intent, while generating code which conforms to the intent and a proof of this\nconformance. Our approach to intent discovery is powered by a novel repair engine called program-proof\nco-evolution, where the object of repair is a tuple (code, logical specification, test) generated by an LLM from\nthe same natural language description. The program and the specification capture the initial operational\nand declarative description of intent, while the test represents a concrete, albeit partial, understanding of\nthe intent. Our objective is to achieve consistency between the program, the specification, and the test by\nincrementally refining our understanding of the user intent. Reaching consistency through this repair process\nprovides us with a formal, logical description of the intent, which is then translated back into natural language\nfor the developer’s inspection. The resultant intent description is now unambiguous, though expressed in\nnatural language. We demonstrate how the unambiguous intent discovered through our approach increases the\npercentage of verifiable auto-generated programs on a recently proposed dataset in the Dafny programming\nlanguage.\nACM Reference Format:\nMartin Mirchev, Andreea Costea, Abhishek Kr Singh, and Abhik Roychoudhury. 2024. Assured Automatic\nProgramming via Large Language Models . In Proceedings of Make sure to enter the correct conference title\nfrom your rights confirmation email (Conference acronym ’XX). ACM, New York, NY, USA, 20 pages. https:\n//doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nIntroduction\nThe advent of automatically generated code, such as those produced by Generative AI/Large\nLanguage Models (LLMs), offers us new developer workflows. Tools such as GitHub’s Copilot [11]\nhave demonstrated the impact automatically generated code may have on productivity [32]. Pre-\ntrained language models perform a wide diversity of programming tasks remarkably well, assuming\nthe right instructions and setup [22]. A typical workflow involves developers describing the desired\nfunctionality in a natural language, followed by the AI assistant automatically generating the\ncorresponding code for them. Before the integration of the auto-generated code into production,\nAuthors’ Contact Information: Martin Mirchev, mmirchev@comp.nus.edu.sg, National University of Singapore, Singapore,\nSingapore; Andreea Costea, andre.costea@gmail.com, Delft University of Technology, Delft, The Netherlands; Abhishek Kr\nSingh, abhishek.uor@gmail.com, National University of Singapore, Singapore, Singapore; Abhik Roychoudhury, abhik@\ncomp.nus.edu.sg, National University of Singapore, Singapore, Singapore.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\narXiv:2410.18494v2  [cs.SE]  5 Nov 2024\ndevelopers are encouraged to check the freshly generated code for safety and correctness [19], and\npotentially ask the AI assistant to regenerate it in case of inconsistencies with the intended output.\nThis process turns out to be quite laborious in the case where the intent is not met [8]. Assuming\na fine-tuned model which performs relatively well, one explanation for the misalignment between\nintent and output is that natural language descriptions of intent are inherently ambiguous, while\nthe output code represents precise computations. In essence, the task of auto-generating code is\ncurrently a one-to-many translation from natural language to programming language, leading\nto non-determinism in the software production [24]. Our goal is to streamline this process by\nremoving the ambiguity from the intent – discover the intent – aiming for a one-to-one translation\nand reducing the non-determinism in the AI-assisted software development.\nOur approach to intent discovery is grounded in software verification, which involves checking\nthat code aligns with the intent as captured in the formal specification. We ask the AI assistant to\ngenerate both code and its formal specification from the same intent written in natural language. At\nthis point the AI assistant generates two transformations, we call them artifacts, in an unambiguous\nformat from the initial ambiguous intent description. Next, a verifier checks whether the code\nmeets the formal specification. In case the verification fails, that is, the code and its specification\ndo not align, we proceed by extracting the common facts about the intent which the code and its\nspecification both agree upon. We use this unambiguous partial intent to start a repair campaign,\nwhich we refer to as program-proof co-evolution, implemented in a tool we call ProofRover. The\nobjective of this campaign is to repair the code and its specification such that the verification of the\nrepaired program against the repaired specification succeeds. This results in alignment between the\nprogram and its specification, thus ensuring agreement upon the same intent. The final repaired\nspecification represents what we infer as the developer’s intent written in a formal, unambiguous\nmanner. We then translate this formal intent back into natural language for the developer’s inspec-\ntion. Although informal, this intent description is now unambiguous. The developer can modify\nthis refined intent and restart the code generation process, now with a more precise intent.\nThe repair campaign may result in multiple pairs of code and corresponding specifications, thus\nmultiple possible intents. To help with the selection of the intent closest to the developer’s intent,\nProofRover supports a third artifact, namely tests. Depending on their origin, tests may or may not\nbe trusted to express the developer’s true intent. If trusted (say, if the developer provides them), tests\nare used by ProofRover to further disambiguate the previously derived common intent, allowing\nit to filter out code-specification pairs. If not trusted (say, generated by LLM from the natural\nlanguage description), tests serve as another source of partial intent, potentially contributing to the\ncommon intent if there is any commonality between the test and the specification or the program.\nGenerative AI promises to alleviate human burden by automatically synthesizing both the code\nand its formal specification, given the developer’s intent written in natural language. As it turns\nout, this promise is not achievable with existing technology as demonstrated in a recent empirical\nstudy [5] showing that oftentimes, the auto-generated code and its corresponding auto-generated\nspecifications do not align, thus making the verification fail. We show how our approach to intent\ndiscovery may help the AI assistants produce more verifiable code.\nIn summary, we make the following contributions:\n• We introduce a mechanism for intent discovery grounded in formal reasoning. Our core idea\nis to generate a declarative and an operational description of the intent from LLMs - the\nspecification and the program, respectively. We then extract the common intent from each\nartifact and use it as the basis for a conformance repair campaign. The campaign repairs\nboth the declarative and operational descriptions to ensure their consistency, as confirmed\nby formal verification. Tests can be used to further clarify the developer’s intent.\n2\n• We introduce a novel algorithm named program-proof co-evolution to support our idea of\ndeveloper intent discovery and conformance repair. Specifically, this algorithm distinguishes\nbetween a partial, common intent (or hard intent) and an unconfirmed intent (or soft intent,\ncomprising the facts that are not common across the artifacts). The goal is to resolve the\nconformance problem by repairing the soft intent while preserving the hard intent.\n• We present ProofRover, an implementation of the program-proof co-evolution algorithm\nin the Boogie framework, and present experimental results for Dafny programs. We discuss\nhow the verifiability of code auto-generated from natural language descriptions increases\nwhen the intent gets disambiguated.\n2\nOverview\n2.1\nBackground\nOwning to its maturity as a programming language that supports software verification via design\nby contract, we will be using Dafny as our target language throughout this paper [7]. However,\nwe believe the ideas presented here can be extended to future verification systems developed to\ncertify auto-generated code. Dafny programs operate on methods that have declarative specification\nascribed to their bodies in the form or pre- and postconditions. A method is verified using Floyd-\nHoare-style verification, where reasoning begins by assuming the ascribed preconditions (specified\nwith the keyword requires). The method is then converted to a collection of verification conditions\nrepresenting the correctness of the method with regards to its specifications, which are sent to a\nSAT/SMT solver. A successful verification of the entire method implies the validity of the ascribed\npostconditions (specified with the keyword ensures). If the method contains loops, the verifier\nrequires loop annotations to summarize the loop’s behavior, even for unbounded executions.\n2.2\nSetup\nThe repair solution we are proposing for fixing intent conformance is applicable to a combination\nof human-written and auto-generated code, specifications, and tests. None of these three artifacts\nused for expressing intent are assumed correct. They may all\nrepresent incorrect or incomplete intent. For the simplicity of\nthis presentation, we are assuming that code and specification\nare LLM-generated, while tests are provided by an oracle.\nFurthermore, the framework is designed to work with either\njust two (any two) artifacts or with all three. We will walk through an example using two artifacts,\nnamely code, and specification, and introduce the third, unit tests, to refine the results.\n2.3\nRunning example\nWe ask an LLM to generate a simple Dafny method which finds the first odd value in a given array.\nThe LLM generates the imperative code in Fig. 1, which appears to be carrying out the expected\ncomputation by storing the index of the first odd value in the array in return variable odd. The\nLLM also produces a declarative description for the same intent that, while verbose, appears to be\ncorrect: assuming a non-null array (line 3), upon executing the code, odd stores the index of an odd\nelement (line 4), specifically the first odd element in the array (line 5). The loop invariant (lines\n11-17) summarises the effect of the loop1 using local variables found and i to specify the same\ncomputation as the method’s specification with one notable exception that will be discussed later.\nVerification Results. First, the verification fails with index out-of-range errors. As part of its no-\nruntime-errors safety guarantees, Dafny statically checks that all array accesses are within bounds, a\n1In Dafny, the variable i in for i := lo to hi takes the values from lo to, but not including, hi.\n3\ncondition that can be proven to hold for the method’s body given the loop invariant. For example, if\nwe were to add the following assertion at the end of FindFirstOdd, Dafny would prove its validity:\nassert found ==> 0 <= odd < arr.Length;\nAdditionally, to ensure the well-formedness of the specifications, a similar condition should also\nhold for them. Explicitly, specifications should not quantify over indices that fall outside the array\nbounds. A close inspection of the postconditions (lines 4-5) indicates that odd could refer to an\nindex beyond the array’s length.\nSecond, the verification fails to prove that arr[odd]%2!=0 holds on all paths, as required by the\nPrompt\nGenerate a method that finds the first odd value in an array.\nGenerate the formal specifications which captures this compu-\ntation and annotate the method with it.\n1\nmethod FindFirstOdd(arr: array<int>)\n2\nreturns(odd: int)\n3\nrequires arr != null\n4\nensures arr[odd] % 2 != 0\n5\nensures forall i ::0 <= i < odd ==> arr[i] % 2 == 0\n6\n{\n7\nvar found := false;\n8\nodd := -1;\n9\n10\nfor i := 0 to arr.Length\n11\ninvariant 0 <= i <= arr.Length\n12\ninvariant !found ==> odd == -1\n13\ninvariant found ==>\n14\n0 <= odd < i && arr[odd] % 2 != 0\n15\ninvariant forall j :: 0 <= j < i\n16\n==>((found ==> arr[j] % 2 != 0 || j == odd)\n17\n&& (!found ==> arr[j] % 2 == 0))\n18\n{\n19\nif arr[i] % 2 != 0 {\n20\nodd := i;\n21\nfound := true;\n22\nbreak;\n23\n}\n24\n}\n25\n}\nVerification Result\nline 4: Error 1: index out of range.\nline 5: Error 2: index out of range.\nline 6: Error 3: A postcondition might not hold on this path.\nline 4: This is the postcondition that might not hold.\nNo quick fixes available.\nFig. 1. A Dafny auto-generated method.\npostcondition at line 4. However, if added\nat the end of FindFirstOdd, the following\nassertion would be proven valid:\nassert found ==> arr[odd] %2 != 0;\nThe above assertion seems similar to the\ninvalid postcondition. What sets them apart\nis the presence of the found guard. While\nsimple, this example supports our observa-\ntion that a well-trained, finetuned LLM gen-\nerates almost correct artifacts. Despite this,\nturning these artifacts into ones that success-\nfully verify requires non-trivial intervention\n(suggested by the verification result too: “No\nquick fixes available”). We next provide an\nintuition of how ProofRover approaches\nthe repair of the conformance problem, in-\ndicating how the three errors get fixed.\nProgram-Specification Conformance.\nWe first target the program-specification\nconformance problem indicated by Error 3.\nA program conforms to a specification if the\nentire set of program behaviors is included\nin the set of behaviors described by the spec-\nification. With this interpretation of confor-\nmance, ProofRover identifies that both the\nprogram and specification agree that:\n(h1) all elements in the array up to the index\ndenoted by odd should be even numbers.\nIt also identifies that the program and\nspecification do not agree on the followings:\n(s1) when found is true, odd indexes into an\nodd element in arr.\n(s2) odd always indexes into an odd element\nin arr.\nThe common intent described in state-\nment (h1) is supported by the fact that Dafny\ncan prove that the postcondition at line 5 is implied by the program path on which the invariants\nat lines 11 and 15 hold. The intent described by statement (s1) and extracted from the program is\nsupported by the invariant at line 13. The intent described by statement (s2) and extracted from\nthe specification is supported by the postcondition at line 4. To simplify the explanation of this\n4\n// @trust\nmethod OddInArray()\n{\nvar x := new int[]{2,3,4};\nvar s := FindFirstOdd(x);\nassert s >= 0;\n}\n(a)\n// @trust\nmethod AllEven()\n{\nvar x := new int[]{2,2,4};\nvar s := FindFirstOdd(x);\nassert s == -1;\n}\n(b)\n// @trust\nmethod AllEvenLength()\n{\nvar x := new int[]{2,2,4};\nvar s := FindFirstOdd(x);\nassert s == -x.Length;\n}\n(c)\nFig. 2. Small test case for FindFirstOdd.\nexample, we described the intent expressed by statements (h1), (s1), and (s2) using natural language;\nhowever, ProofRover extracts logical formulas from Dafny code to represent these facts of the\nintent. The definition of facts and the detailed mechanics for deriving them are deferred to Sec. 4.\nAfter extracting all the facts, ProofRover then differentiates them into hard intent and soft intent\nbefore initiating the repair. The hard intent is a collection of facts which should still hold after\nthe repair. The common intent described by (h1) and the well-formedness statement (h2) all array\naccesses should be within bounds are examples of facts that represent hard intent. The soft intent is\na collection of facts which need not hold after the repair. Practically, the soft intent contains all the\nfacts that are not part of the hard intent. Conceptually, the soft intent represents facts on which the\nvarious artifacts do not reach full agreement. For instance, the formulas supporting statements (s1)\nand (s2) are examples of soft intent facts.\nThe goal of the repair is to modify the soft intent such that all artifacts respect their conformance\nrelation with each other while maintaining the validity of the hard intent. At the same time, it\nseeks to preserve as many soft constraints as possible, aiming for a less intrusive repair.\nFor Error 3, ProofRover identifies that it is the misalignment of statements (s1) and (s2) that\nlead to the conformance issue. It could then select statement (s1), statement (s2), or both statements\nas repair candidates. Choosing statement (s2) as the repair candidate seems to only involve the\nrefinement of the postcondition at line 4. Weakening it as indicated below makes Error 3 disappear\nwithout invalidating any of the hard constraints:\n4\n---\nensures arr[odd] %2 != 0;\n4\n+++\nensures\n0 <= odd < arr.Length ==> arr[odd] %2 != 0;\nLater in this section, we discuss how to automatically prioritize which soft constraints to repair.\nUpon examining the reasons for the proof failures behind Error 1 and Error 2, it becomes obvious\nthat the issues are related solely to the specification’s conformance to the well-formedness condition\nrather than to program-specification conformance. We ignore Error 1 since the fix for Error 3 is\nsufficient to also fix this failure. In fixing Error 2, ProofRover identifies the postcondition at line\n5 as a fact that is at odds with the well-formedness condition, which is part of the hard intent.\nAiming to align the two while maintaining the hard intent leads to the following fix:\n5\n---\nensures forall i::0 <= i < odd ==> arr[i] % 2 == 0;\n5\n+++\nensures\n0 <= odd < arr.Length ==> (forall i::0 <= i < odd ==> arr[i] % 2 == 0);\nDetails about how ProofRover manipulates the soft and hard intents to find a fix are presented\nin Sec. 4. There, we also dicuss their formal definition.\nSpecification-Test Alignment. One might argue that the specification fixes that ProofRover\nfound are not unique. Naïve fixes, such as replacing both postconditions from Fig. 1 with ensures true;\n– an obviously vacuous solution – would also fix the conformance problem. While a human would\nnot choose to repair the method this way, it is not clear how to instruct a repair engine to avoid\nsuch vacuous solutions if the goal of the repair were to simply make the verification succeed.\n5\nFig. 3. The manipulation of soft and hard intents for repairing the conformance issue in Fig. 1.\nLuckily, our repair framework lends itself well as a filter mechanism for vacuous specifications\nwhen unit tests are present. The reason for that is that the existence of a unit test, e.g., such as\nthe one in Fig. 2a, is still a verification problem for Dafny. The call to the function being tested\nwhich is described by its formal specification, must conform to the test constraints: given an input\narray int[]{2,3,4} the result returned by FindFirstOdd should be greater or equal to zero. In\nother words, the conformance of specification to the test implies that the behavior described by the\nspecification in the considered calling context is a subset of the behaviors captured by the unit test.\nFurthermore, if we could specify that this test is part of the hard intent2, then ProofRover could\nuse this information to restrict the search space of specification fixes accordingly and thus eliminate\nthe spurious fix. In the case of the spurious specification, ProofRover detects the verification\nfailure of assert s >= 0 in Fig. 2a and uses this failure information to restrict the conditions over\nthe return value odd in OddInArray. The spurious postcondition true fails to imply odd >= 0 for\nthe given input array, and it is thus considered a non-solution.\nLet us next consider a unit test with an input array that contains no odd element, like the one in\nFig. 2b, also marked as part of the hard intent. Although FindFirstOdd does return -1 in such a\ncalling context, its ascribed specification does not describe this scenario, deeming the assertion\ns == -1 as invalid. This failure triggers the repair engine which subsequently discovers that a new\npostcondition should be ascribed to FindFirstOdd, namely:\n6\n+++\nensures (forall i :: 0 <= i< arr.Length ==> arr[i] % 2 == 0) ==> odd == -1\nthus leading to a more “complete” specification.\nFinally, let us consider the unit test in Fig. 2c, which if marked as part of the hard intent imposes\nrepair obligations not only in the specification, but also, transitively, at the program level too:\n6\n+++\nensures (forall i :: 0 <= i< arr.Length ==> arr[i] % 2 == 0) ==> odd == -arr.Length\n8\n---\nodd := -1;\n8\n+++\nodd := -arr.Length;\nPrioritising the repair of the soft intent facts. Up to this point, we have not differentiated\nbetween the facts of a soft intent; ProofRover appears to have effectively selected the fact to be\n2ProofRover supports user annotations to indicate that an artifact is trusted, i.e., it is part of the hard intent.\n6\nresolved that resulted in the least intrusive fix for program-specification conformance. To come to\nthis result, we have actually devised a few heuristics for choosing a candidate fact to be fixed. For\nexample, when choosing to repair (s2) instead of (s1) earlier, the rationale behind that choice was\nthat (s2) seems to be at odds with (h2), the well-formedness constraint, while (s1) is not. Generally,\nwe choose the fact that is inconsistent with the most hard intent facts. If two facts break even, we\nnext choose the one which is inconsistent with the soft intent facts. If they further break even,\nwe then choose the strongest among the two if they are in an implication relation, i.e. (s2) ==>\n(s1), or pick randomly otherwise. For the unit test in Fig. 2b, even if the test were not trusted, that\nis, it would be considered a soft intent, ProofRover would still produce the same patch. That is\nbecause the postcondition would be inconsistent with both the program and the unit test, which\nboth indicate that the returned index should be -1 for an array containing only even elements.\nPutting it all together. Fig. 3 summarises our discussion so far. Given at least two of the\nfollowing artifacts: program (Pr), specification (𝜑), and unit test (𝑇), ProofRover aims to ensure\nthat any pair of these artifacts is in a conformance relation (sometimes transitively). To do this,\nProofRover extracts all the facts from the three artifacts and divides them into facts common\nacross all artifacts (the set of which forms the common intent) and facts which are inconsistent\nacross any two artifacts (some of which are responsible for breaking the conformance relation).\nThe facts in the common intent together with the well-formedness conditions (𝜑wf ) form the hard\nintent. The remaining facts form the soft intent. The purpose of the repair is to refine the soft\nintent, preserving the hard intent. The outcome consists of patches for the soft intent that, when\napplied to their corresponding artifacts, ensure a conformance relation holds between each pair of\nartifacts. Facts are logical abstractions that ProofRover extracts from each artifact using Boogie,\nDafny’s verification system. Additionally, Boogie’s capability to partition a program into slices of\nproof obligations (assumptions and assertions) allows ProofRover to identify which pairs of facts\nare expected to be in a conformance relation [30]. How ProofRover extracts some of these logical\nabstractions is described in Sec. 3, while the co-evolution process is introduced in Sec. 4\n3\nBackground: Dafny - A Verification Aware Programming Language\nDafny [7] is a strongly typed imperative programming language that supports Hoare-style code\nverification [14]. A Dafny program Pr consists of modules containing classes, methods, functions,\nand lemmas that can be annotated with specifications 𝜑, pairs of formulas in the first-order logic.\nWe denote these formulas as 𝜑pre and 𝜑post, respectively, representing the assumed precondition\nand ascribed postconditions to the method. To verify a program’s correctness, Dafny translates the\nannotated code into Boogie Intermediate Language methods, representing verification requirements,\nsuch as method correctness and signature well-formedness. Method correctness ensures that, given a\nprecondition 𝜑pre, the post-state satisfies the postcondition 𝜑post. It also verifies the well-formedness\nof statements 𝜑wf and intermediate assertions 𝜑im. Signature well-formedness checks that pre- and\npostconditions 𝜑pre,𝜑post do not allow ill-formed models, such as out-of-bounds errors (see Sec. 2).\nGenerating Verification Conditions. The Boogie tool constructs a control flow graph for each\nmethod, described by a small programming language [2] (see Fig. 4 ). The transformation of a Boogie\nmethod into this graph is called program passification. This passive program is tailored towards\nverification by focusing on logical relationships and eliminating side effects. The graph is then\nconverted into logical equations using weakest precondition calculus [2], forming the verification\nconditions. The process of converting a Dafny program Pr and specification 𝜑into verification\nconditions is covered by the function VCGen whose details are skipped for brevity [2]. As an example,\nfor the program var x:=1; var y:=2; and specification (assume true;, assert x + y >= 2) VCGen\nproduces the following formula true ⇒((𝑥= 1) ⇒((𝑦= 2) ⇒(𝑥+ 𝑦≥2))).\n7\n⟨program⟩::= ⟨block⟩+\n⟨block⟩::= ⟨ident⟩‘:’ ⟨stmt⟩‘;’ ‘goto’ ⟨blockId⟩*\n⟨stmt⟩::= ‘assert’ ⟨expr⟩| ‘assume’ ⟨expr⟩| ⟨stmt⟩‘;’ ⟨stmt⟩| ‘skip’\nFig. 4. The Verification Language\nmethod OddInArray()\n{\nvar x := new int[]{2,3,4};\n// 𝐼\nvar s := FindFirstOdd(x);\nassert s >= 0;\n// 𝑂\n}\nmethod OddInArray(int[] x) returns (s:int)\nrequires x == new int[]{2,3,4}\n𝜑𝐼𝑝𝑟𝑒\nensures s >= 0\n𝜑𝑂\n𝑝𝑜𝑠𝑡\n{\ns := FindFirstOdd(x);\n}\nFig. 5. Unit test translation from original (𝐼,𝑂) pair (left) to program Pr𝑇and specification 𝜑𝑇(right).\nVerification with Z3. After generating the verification conditions, Boogie interfaces with the\nZ3 theorem prover for validity check. We will use a function Valid to denote the validity check\non verification conditions. The results obtained from Z3 can fall into two main categories: proof\nof correctness or proof failure. Suppose Z3 proves that the verification conditions are valid. In that\ncase, it essentially means that, for all possible models the verification conditions are satisfied. This\ncan also be viewed as a conformance relation Pr ⊆𝜑between a program Pr and specification 𝜑:\nPr ⊆𝜑≜Valid(VCGen(Pr,𝜑))\nIf Z3 cannot prove the verification conditions to be valid, it indicates that at least one scenario\nexists in which the properties do not hold or the solver was unable to prove the target expression\ndue to incompleteness. Boogie collects the verification result by Z3. This information is crucial for\ndebugging and subsequently repairing the program. Using this proof failure we obtain a failing\nexecution trace𝑡fail = 𝑠1.𝑠2 . . .𝑠𝑓of blocks and statements from the control flow graph that highlights\na portion of the Boogie code where the failure occurred. This Boogie fragment can be mapped back\nto a fragment from the original Dafny program Pr.\nTests in Dafny. We define a test 𝑇as a pair (𝐼,𝑂) of an input variable 𝐼and a logical formula 𝑂.\nTests in Dafny can be static or dynamic. In this work, we will focus on static tests.\nTo check for conformance of a test 𝑇with a program Pr, we can construct a specification\n𝜑𝑇= (𝜑𝐼\n𝑝𝑟𝑒,𝜑𝑂\n𝑝𝑜𝑠𝑡) that acts as a specification for a program Pr, allowing for direct verification of\nthe implementation code. We define the relation Pr ⊆𝑇to check for such conformance.\nPr ⊆𝑇≜Pr ⊆𝜑𝑇\nTo check for conformance between a specification 𝜑, say, of method FindFirstOdd, and a test 𝑇,\nsay, OddInArray, we construct a program Pr𝑇whose specification constrains the parameters and\nreturn value of Pr𝑇according to (𝐼,𝑂). We present an example construction in Fig. 5 Using this\nconstruct, we can define conformance relation 𝜑⊆𝑇between a specification 𝜑and a test 𝑇:\n𝜑⊆𝑇≜Pr𝜑⊆𝜑𝑇\nThis approach to conformance checking allows us to treat conformance in a uniform manner\nacross all artifacts, regardless which pair of artifacts is considered for repair.\nAssertion Partitioning. To allow for multiple assertions along a path and ease the proving\nfor the underlying theorem prover, Boogie applies partitioning over the generated assertions [30].\nThis allows a user to examine multiple failing conditions instead of stopping the verification\nprocess upon encountering the first verification failure. For example, a simple assertion such as\nassert 𝜑1 ∧𝜑2 is decomposed into separate assertions assert 𝜑1; assert 𝜑2. Furthermore, Boogie\nconstructs unique control flow graph paths, allowing to separately validate these verification\nconditions. This partitioning, combined with the failing execution trace, enables a more granular\n8\nview of the conformance between a program Pr and its specification 𝜑. Instead of treating the entire\nprogram and specification as a monolithic pair, it decomposes the overall conformance problem\ninto a collection of smaller, more manageable pairs, each consisting of a sub-program Pr′ and a\ncorresponding sub-specification 𝜑′. This decomposition allows for targeted repair efforts, where\neach pair (Pr′,𝜑′) can be repaired independently. This partitioning facilitates the extraction of the\nsoft and hard intent, which will be discussed in Sec. 4.\n4\nProgram Proof Co-Evolution\nIn this section, we detail how ProofRover is designed to discover and formalize the programmer’s\ntrue intent with the overarching goal of facilitating formally verified auto-generated programs. Our\nworkflow, depicted in Fig. 6 and named program-proof co-evolution, involves three key entities: the\nprogrammer, a large language model (LLM), and a verification engine. The programmer initiates\nthe workflow by prompting the LLM with a natural language query 𝑄that conveys their intent.\nThe LLM generates a Dafny program Pr0 and corresponding specification 𝜑0. A program annotated\nthis way is sent to Dafny for verification (nodes 2 and 3 in Fig. 6). If the verification fails, that is,\nPr ⊈𝜑, then ProofRover first identifies the hard and soft intents (nodes 4) using the partitioning\ndescribed in Sec. 3, and then proceeds to generate patches for the soft intent (nodes 5). These\npatches are applied to (Pr,𝜑), the original pair of program and specification, before starting the\nverification process again (nodes 2-5) to check whether further refinement is required.\nAdditionally, ProofRover also considers unit tests represented as 𝑇(node 1). However, each\niteration of the co-evolution process (nodes 2-5) focuses on the conformance of exactly two\nartifacts, such as the program and specification. A third artifact, like the test 𝑇, is only integrated\nin a subsequent iteration of the co-evolution, along with the outcome of the previous iteration, for\ninstance, the repaired program-specification conformance. At a high level, for a triple (Pr,𝜑,𝑇) to\nbe accepted, our workflow aims to solve the following two problems in an interleaved way:\nP1 Conformance between a program Pr and specification 𝜑, i.e. Pr ⊆𝜑.\nP2 Refining the conforming solutions, as conveyed through feedback from tests.\nSolving the conformance problem P1 is achieved through a cyclic and automated interaction\nbetween the co-evolution driver and the verification engine (nodes 2-5). Through iterative cycles,\nthe co-evolution driver refines the annotated program and its specification based on feedback from\nthe verification engine. This automated process minimizes the manual interventions needed from\nthe programmer, allowing for a more efficient workflow.\nRefining the conforming solutions, as outlined in problem P2, is achieved via a carefully designed\ninteraction between the programmer and the co-evolution process (nodes 6-8). This interaction\ncrystalizes the programmer’s true intent, justifying the treatment of the test set 𝑇as hard intent in\nour running example in Sec. 2.3. We now elaborate on each of these essential components.\n4.1\nP1: Conformance of Program and Specification\nTo verify that Pr0 ⊆𝜑0 where 𝜑0 = (𝜑pre,𝜑post), the pair (Pr0,𝜑0) is converted by VCGen into a\nlogical expression, which is then checked for validity using an SMT solver (refer to node 3). For\nnow, we ignore the existence of tests. Suppose the outcome of the validity check is successful. In\nthat case, it confirms that Pr0 ⊆𝜑0, allowing us to proceed with a series of steps to clarify the\nprogrammer’s intent through a controlled interaction (detailed in Sec. 4.2).\nExtracting Hard and Soft Intent. Suppose the validity check is unsuccessful. The verifier\nreturns a failing trace of blocks of statements 𝑡fail = 𝑠1.𝑠2 . . .𝑠𝑓. The trace𝑠1.𝑠2 . . .𝑠𝑓−1 corresponds to\na subprogram 𝐼fail from Pr0. The final statement 𝑠𝑓corresponds to an unproven assertion 𝜑fail which\nmay come from the postcondition 𝜑post, intermediate assertions 𝜑𝑖𝑚or well-formedness checks\n9\nAnnotated\nProgram\nPr, 𝜑\n2.\nPr0,𝜑0,𝑇0\n1.\nNatural\nLanguage 𝑄\n(START)\n0.\nVerification\nEngine\n(Boogie)\n3.\nHard/Soft\nIntents\n𝜙𝐻, 𝜙𝑆\n4.\n𝑃𝑟⊈𝜑\nTest T\n6.\n𝑃𝑟⊆𝜑\nProgrammer’s\nIntent\n7.\nAccepted\n(STOP)\n9.\nPr ⊆𝜑⊆𝑇\nTest 𝑇′\n8.\nCandidate\nsynthesis\n5.\nLLM\nVCGen\nFail\nPass\nAgree\nRefine\nFeedback\nPr′′′,𝜑′′′\nFig. 6. Programmer’s intent discovery through Program Proof co-evolution, where the programmer bootstraps\nthe whole process at node 1. with a natural language query, and the co-evolution process finally terminates at\nnode 11. with the alignment of the programmer’s true intent and the actual behavior of the repaired Program.\n𝜑𝑤𝑓. A pair so formed, namely, (𝐼fail, (𝜑pre,𝜑fail)), represents a partition of the larger pair (Pr0,𝜑0),\nwhere 𝐼fail ⊈(𝜑pre,𝜑fail). To fix this nonconformance, we must first define the space of statements\nover which patches can be constructed. We need to discover both the commonality of intent,\nwhich is represented by all conforming partitions (Pr′,𝜑′) and the discrepencies in intent, which is\nrepresented by all nonconforming partitions (Pr′′,𝜑′′) . We denote the collection of verification\nconditions corresponding to all the pairs (Pr′,𝜑′) as the hard intent 𝜙𝐻and the collection of\nverification conditions corresponding to all pairs (Pr′′,𝜑′′) as the soft intent 𝜙𝑆. We detail the\nidentification of the hard intent 𝜙𝐻and the soft intent 𝜙𝑆in Algorithm 1, ExtractHSIntent.\nAlgorithm 1: ExtractHSIntent\nInput: Program Pr, Spec 𝜑\nOutput: Hard intent 𝜙𝐻, Soft intent 𝜙𝑆\n1 P ←retrieve all partitions of (Pr,𝜑);\n2 𝜙𝐻←{};\n𝜙𝑆←{};\n3 for (Pr𝑖,𝜑𝑖) ∈P do\n4\nif Pr𝑖⊆𝜑𝑖then\n5\n𝜙𝐻←𝜙𝐻∪VCGen(Pr𝑖,𝜑𝑖);\n6\nelse\n7\nfor 𝑠𝑡∈VCGen(Pr𝑖,𝜑𝑖) do\n8\nif IsTrusted(𝑠𝑡) then\n9\n𝜙𝐻←𝜙𝐻∪{𝑠𝑡};\n10\ncontinue;\n11\nif IsWF(𝑠𝑡) then\n12\n𝜙𝐻←𝜙𝐻∪TransformWF(𝑠𝑡);\n13\ncontinue;\n14\n𝜙𝑆←𝜙𝑆∪{𝑠𝑡};\n15 𝜙𝑆←remove facts from 𝜙𝑆that are also in 𝜙𝐻;\nAlgorithm 1 examines all the partitions\n(line 1) that come from (Pr,𝜑). All formu-\nlae of the conforming pairs are included in\nthe hard intent (lines 4 and 5). If the pair\nis nonconforming, we must examine each\nformula 𝑠𝑡of the failing pair (line 7). Sup-\npose that 𝑠𝑡originates from a statement that\nis annotated3 in the original program or\nspecification as a hard intent by the user\n(checked using a method called IsTrusted -\nline 8). In that case, we add it as a fact of the\nhard intent (lines 9). Otherwise, we check\nwhether it comes from a well-formedness\ncheck (checked using a method called IsWF).\nWell-formedness constraints 𝜑𝑤𝑓represent\nthe underlying safety conditions of the lan-\nguage. We must track them as a fact of the\nhard intent with a single caveat - we do not\ntake the original formula but a modified version that tracks whether the statement for which this\n3This annotation is represented by an attribute :trusted in the Dafny code, which can be attached to any statement.\n10\ncheck was created exists in the program. We represent this aforementioned statement transfor-\nmation by a method TransformWF. If the formula does not fall into these two cases, we include it\nas a fact of the soft intent (line 14). Finally, we ensure that both collections are disjoint, where a\nformula 𝑠𝑖∈𝜙𝑆∩𝜙𝐻will be kept in the hard intent (line 15).\nSynthesizing Patches. Having collected the soft intent 𝜙𝑆and hard intent 𝜙𝐻, we have to synthe-\nsize a patch which fixes the 𝐼fail ⊈(𝜑pre,𝜑fail) nonconformance. We achieve this using Algorithm 2,\nAlgorithm 2: SynthesizePatch\nInput: Hard intent 𝜙𝐻, Soft intent 𝜙𝑆, Failing trace\n𝑡fail, Number of patches 𝑘\nOutput: Patches 𝑃𝑠\n1 𝜙′\n𝑆←update 𝜙𝑆with priorities for each fact;\n2 𝜙′′\n𝑆←retrieve from 𝜙′\n𝑆the facts with highest priority;\n3 Pr𝑎,𝜑𝑎←translate 𝜙𝐻, 𝜙𝑆, 𝑡fail back to Dafny;\n4 Pr′′,𝜑′′ ←translate 𝜙′′\n𝑆back to Dafny;\n5 𝑄′ ←construct a prompt using Pr𝑎, 𝜑𝑎, Pr′′, 𝜑′′;\n6 𝑃𝑠←ask LLM for 𝑘patches using prompt 𝑄′;\nSynthesizePatch. Selecting to patch one\nor more facts from the soft intent can\nlead to a successful repair for the whole\nprogram. To prioritize the repair of the\nfacts that would produce the least intru-\nsive fix for program-specification confor-\nmance, we order all the facts lexicographi-\ncally over a triple of the following three cri-\nteria (lines 1): (1) number of non-conform-\ning facts from the hard intent; (2) number\nof non-conforming facts from the soft in-\ntent; (3) strength of formulae. We next select to fix the facts with the highest priority (line 2), that is,\nthe facts removing “most” non-conformance. For patch synthesis, we leverage the power of a Large\nLanguage Model synthesizer. This enables ProofRover to tackle multi-hunk repairs for a single\nfailure and to repair both the program 𝐼𝑓𝑎𝑖𝑙and specification (𝜑pre,𝜑fail) simultaneously, leading to\na real co-evolution towards conformance. Before prompting the model, we relate the intent and\ntrace back to the original Dafny program Pr𝑎and specification 𝜑𝑎to present the problem in a more\ntypical setup for an LLM (line 3). In the process of translating back to Dafny, ProofRover also\nannotates the statements and specifications corresponding to the hard intent with a :trusted\nattribute, instructing the model not to modify them. We also translate back the prioritized soft\nintent 𝜙′′\n𝑆as a hint for the model to focus on repairing the corresponding Pr′′ and 𝜑′′ fragments\n(line 4). We construct a prompt using the original but now annotated program and specification,\nPr𝑎and 𝜑𝑎, respectively–along with the highest-priority facts and use it to synthesize 𝑘patches\n(lines 5 and 6).\nCo-evolution. Algorithm 3 describes our conformance strategy. It iteratively identifies and fixes\nAlgorithm 3: CoEvolution\nInput: Program Pr, Specification 𝜑\nOutput: Verifiable solutions 𝑉𝑝\n1 𝐶←{(Pr,𝜑)};\n2 while 𝐶≠{} do\n3\n(Pr,𝜑) ←extract a candidate from 𝐶\n4\nif Pr ⊆𝜑then\n5\n𝑉𝑝←𝑉𝑝∪{(Pr,𝜑)};\n6\ncontinue;\n7\n𝑡fail ←get failing trace for (Pr,𝜑);\n8\n𝜙𝐻,𝜙𝑆←ExtractHSIntent(Pr,𝜑);\n9\n𝑃←SynthesizePatches(𝜙𝐻,𝜙𝑆,𝑡fail,𝑘);\n10\nfor (Pr′′′,𝜑′′′) ∈𝑃do\n11\n(Pr𝑟,𝜑𝑟) ←apply patch (Pr′′′,𝜑′′′) on (Pr,𝜑);\n12\n𝐶←add (Pr𝑟,𝜑𝑟) to 𝐶if it does not exhibit 𝑡fail ;\nall non-conforming partitions, each\nrepresented by a failing trace, until com-\nplete conformance is achieved, result-\ning in conforming program specifica-\ntion pairs collected in 𝑉𝑝. Starting with\n(Pr,𝜑) (line 1), the original pair of pro-\ngram and specification, the set 𝐶it-\neratively collects different versions of\nrefined pairs of program and specifi-\ncation until there are no more non-\nconforming pairs to repair (line 2) or\nuntil the upper bound of allowed iter-\nations is reached (not shown in the al-\ngorithm). If the candidate pair (line 3)\nis conforming, it is added to the set of\nverifiable pairs,𝑉𝑝(line 4-5). Otherwise,\nthat is, if Pr ⊈𝜑, ProofRover picks one failing trace 𝑡fail to be repaired (line 7). When selecting\n11\na failing trace, we prioritize the returned traces in a reverse trace-depth ordering to focus on the\nnon-conformance closer to the start of the failing program. Next, to prepare for the synthesis of\na patch for 𝑡fail, ProofRover extracts the hard and soft intent from Pr and 𝜑, respectively, using\nAlgorithm 1 and passes this information along with an integer 𝑘representing the number of patches\nto be synthesized to the synthesizer described by Algorithm 2. All the patches (Pr′′′,𝜑′′′) returned\nby the synthesizer are applied to the original pair, resulting in a refined program and specification\npair (Pr𝑟,𝜑𝑟) (line 11). If the refined pair no longer exhibits the failing trace 𝑡fail it is added to the\nset of refined pairs of program and specification (line 12) to be further refined in a subsequent\niteration of the refinement loop (line 2).\n4.2\nP2: Aligning Specification with Programmer’s Intent Through Tests\nAlgorithm 4: AutomatedAssurance\nInput: Program Pr0, Specification 𝜑0, Test suite 𝑇\nOutput: Conforming triples 𝑉𝑡\n1 𝑉𝑡←{}\n2 𝑉𝑝←CoEvolution(Pr0,𝜑0);\n3 𝜑𝑇←translate a test suite 𝑇to specification format;\n4 for (Pr,𝜑) ∈𝑉𝑝do\n5\nPr𝜑←translate 𝜑to a Dafny code format;\n6\n𝑉𝑟𝑝←CoEvolution(Pr𝜑,𝜑𝑇);\n7\nfor (Pr𝑟𝜑,𝜑𝑟\n𝑇) ∈𝑉𝑟𝑝do\n8\n𝜑𝑟←translate Pr𝑟𝜑to specification format\n9\n𝑇𝑟←translate 𝜑𝑟\n𝑇to test format\n10\nif Pr ⊆𝜑𝑟then\n11\n𝑉𝑡←𝑉𝑡∪{(Pr,𝜑𝑟,𝑇𝑟)}\n12\nelse\n13\n𝑉𝑡←𝑉𝑡∪AutomatedAssurance (Pr,𝜑𝑟,𝑇𝑟)\nAlgorithm 3 offers us a solution for\nproblem P1. In other words, we have\nan updated collection 𝑉𝑝= {(Pr,𝜑) :\n(Pr ⊆𝜑)} of repaired programs and\nspecifications. Although these pairs\nare conforming, they might not align\nwith the programmer’s intent. To\nachieve alignment with the program-\nmer’s intent, we support “interac-\ntion” through test cases. A user is\noffered a test that they can modify to\nexpress their intent. These tests can\noriginate from a trusted or untrusted\noracle, such as a user or an LLM.\nAlgorithm 4, AutomatedAssurance\npresents a methodology to achieve\nalignment between a conforming\npair and a test. We prioritize the alignment specifications with tests over the alignment of programs\nwith tests, as the latter aligns with more common verification use cases.\nOnce the program Pr0 and specification 𝜑0 co-evolution campaign has ended with a set of verified\npairs 𝑉𝑝(line 2), we prepare for another campaign that takes into consideration tests. The first\nstep in the preparation is to translate 𝑇into a specification 𝜑𝑇(line 3) as described in Sec. 3. While\niterating over all pairs (Pr,𝜑) in 𝑉𝑝(line 4), we translate the specification 𝜑into a program Pr𝜑(line\n5) before running a co-evolution campaign over the specification 𝜑and test 𝑇represented by the\npair (Pr𝜑,𝜑𝑇) (line 6). If a candidate (Pr𝑟\n𝜑,𝜑𝑟\n𝑇) resulting from the co-evolution campaign conforms\nwith program Pr, we store the fully-conforming triple (Pr,𝜑𝑟,𝑇𝑟) into the set 𝑉𝑇(lines 10 and 11).\nBefore doing this check, Pr𝑟\n𝜑is translated to a specification 𝜑𝑟, and 𝜑𝑟\n𝑇to a test 𝑇𝑟(lines 8 and 9).\nIf it does not conform, another cycle of the algorithm is initiated with the triple (Pr,𝜑𝑟,𝑇𝑟) with\nrefined specification and test (line 13). We discuss termination conditions in Sec. 5.\n5\nEvaluation\nThe goal of ProofRover is to construct valid artifacts that are verifiable programs while disam-\nbiguating the programmer’s intent. We examine the following research questions.\nRQ1: How effective is program-proof co-evolution in constructing conforming artifacts?\nRQ2: What is the quality of the repaired formal postconditions?\nRQ3: Does the refined, informal natural language intent reduce the ambiguity in the user intent?\n12\n5.1\nSetup\nWe implement our approach in a tool named ProofRover. The main driver is about 1000 lines of\nPython code that implements the assurance cycle and prompts the model. We also extend Boogie\nto generate the hard and soft intents and extract the underlying failing assertion. The invocation of\nBoogie is done through Dafny. The extension of these tools comes down to about 500 lines of C#.\nPrompt. ProofRover uses the following system prompt for interacting with the LLM-based\nsynthesizer (node 5 in Fig. 6):\nSystem Prompt for ProofRover\nYou are an expert Dafny programmer. You know Dafny’s verification process very well. You know how to explicitly\ndefine Dafny invariants in loops. You know how to write preconditions and postconditions for methods. You know how\nto write Dafny lemmas and functions.\nRefresher: <Dafny basics about its semantics and verification process>\nGoal: Your task will be to repair a given Dafny program that has one or more verification errors. You have to repair one\nof the verification errors such that the program no longer has this error. Use your knowledge to do that while following\nthe guidelines given to you below under the Guidelines header.\nInput prompt format: As input, you will be given seven parts in the following format: ...\nOutput prompt format: As output, write a patch for the verification error. Return the patch in the format below ...\nGuidelines: <Semantics of hard and soft intent annotations and additional goals for generating a repair>\nWe use the system prompt above to define the problem setup for the model and express the\nfull range of capabilities in Dafny. The Refresher section presents rules specific to Dafny, such as\ninformation about ghost data and lemmas. The Input and Output format sections describe the exact\nformat in which we will prompt the model and the output of the patches it must generate. The\nGuidelines section presents repair specific rules, such as the requirement that the synthesizer must\nmaintain the annotated hard intent and recommendations for what part of the soft intent to repair.\nFor the naive setup, we use the following system prompt:\nSystem Prompt for naive repair\nYou are a programming assistant in Dafny, with program repair expertise ...\nDataset. We use the recently proposed dataset MBPP−DFY by Misu et al. [21]. The dataset\nconsists of a subset of 178 problems from the MBPP dataset [1], which are used to assess the\ncapability of the Large Language Models to generate verifiable Dafny code. The dataset contains\n1,054 LLM-generated Dafny programs\ncorresponding to 178 problem state-\nments. Of these, 620 programs do not\ncompile and are therefore excluded\nfrom our evaluation. From the remain-\ning 434 programs that do compile, we\nextract 60 subjects whose program and specification do not conform and use them to evaluate\nProofRover’s program-specification conformance repair capability. We call this collection of 60\nsubjects datasetNV. Furthermore, 48 subjects of the 374 verifying programs have syntactically\nvalid tests that do not conform with the specifications and program, and 326 have syntactically\ninvalid tests. From the 326 subjects, we semi-automatically repaired the syntactic problems and\nextracted 205 non-conforming exemplars. We call this collection of 48 + 205 = 253 subjects that have\nprograms and specifications conforming but not tests, as datasetV. The remaining 121 subjects\nverify, hence we exclude them from our evaluation. We use datasetV to evaluate ProofRover’s\nprogram-specification-test conformance repair capability.\n13\nNaive Repair\nChain-of-Thought\nProofRover\nModel\nTemp. 𝑇\nAligned\nAvg. Time\nAligned\nAvg. Time\nAligned\nAvg. Time\nGPT-4o\n0.3\n11\n43s\n15\n55s\n23\n111s\n0.7\n13\n46s\n15\n55s\n23\n81s\n1.0\n13\n39s\n13\n58s\n22\n160s\nSonnet-3.5\n0.3\n12\n45s\n18\n74s\n27\n249s\n0.7\n12\n46s\n21\n73s\n31\n236s\n1.0\n14\n49s\n24\n87s\n30\n252s\nAverage\n12.8 (21.3%)\n45s\n17.3 (28.9%)\n67s\n22.2 (37%)\n182s\nTable 1. Reaching conformance between program and specification on datasetNV (60 subjects)\nConfiguration. To evaluate ProofRover and the baselines in different setups, we use two\nmodels – OpenAI’s GPT-4o (gpt-4o-2024-05-13) and Anthropic’s Sonnet-3.5 (claude-3-5-sonnet-\n20240620). We selected these models as they are state-of-the-art for code generation at the time of\nwriting. To explore the models’ capabilities for repairing Dafny code, a language less represented in\ntraining sets, we use three different temperatures,𝑇∈{0.3, 0.7, 1.0}. To ensure termination, we have\nset up multiple bounds for ProofRover: (1) time budget of 20 minutes per subject; (2) maximum of\n5 co-evolution campaigns; (3) stop if a verifying candidate is found within the above bounds. We\ngive the baseline five attempts to repair a non-verifiable program.\n5.2\nRQ1: Reaching Conformance\nIn this research question, we aim to evaluate the capability of ProofRover to repair Dafny programs.\nWe examine program-specification conformance using datasetNV and program-specification-test\nconformance repair using datasetV.\nExperimental Setup. We compare ProofRover with two baselines. The first one uses a simple\nprompt to ask the LLM to repair the conformance issue given a collection of verification failures as\ninput. We call this the “Naive Repair”. The second tool enhances the Naive Repair and uses the\nsame system prompt as the one ProofRover uses. We call this the “Chain-of-Thought”.\nResults. Tab. 1 summarizes our results for evaluating conformance between a program and a\nspecification. Across models, we observe that ProofRover aligns more programs with specifications\nthan both baselines. For example, using Sonnet-3.5 with𝑇= 1.0, ProofRover aligns 30 programs\nwhich is twice the amount of programs aligned by the Naive Repair which repairs only 14, and 25%\nmore than the Chain-of-Thought which repairs 24. Though the average time is significantly higher\nfor ProofRover, we attribute this to the larger amount of model calls resulting from the incremental\nrepair process and the additional calls to Dafny required for extracting intent. Furthermore, due to\nAnthropic’s APIs not providing multiple responses from a prompt, the average tool time is higher\nwhen using Sonnet-3.5. Lastly, a manual inspection of all outputs revealed that the difference in\neffectiveness between models comes from the model’s capability to follow the instructions in the\nprompts. In particular, GPT-4o dismisses some guidelines more often than Sonnet-3.5 does.\nRQ1.1: ProofRover aligns about 73% and 28% more programs with specifications than a Naive\nRepair baseline and a Chain-of-Thought repair, respectively.\nTab. 2 summarizes our results for evaluating conformance between a program, a specification,\nand a test. Across models, we observe that ProofRover aligns more programs with specifications\nand tests than both baselines. Compared to the results of program-specification alignment, we see\n14\nNaive Repair\nChain-of-Thought\nProofRover\nModel\nTemp. 𝑇\nAligned\nAvg. Time\nAligned\nAvg. Time\nAligned\nAvg. Time\nGPT-4o\n0.3\n66\n45s\n66\n58s\n69\n94s\n0.7\n69\n48s\n66\n64s\n71\n70s\n1.0\n77\n40s\n77\n58s\n75\n67s\nSonnet-3.5\n0.3\n67\n48s\n69\n60s\n73\n143s\n0.7\n68\n49s\n68\n62s\n79\n140s\n1.0\n69\n46s\n78\n63s\n88\n195s\nAverage\n69.3 (27.3%)\n46s\n70.6 (27.9%)\n61s\n75.83 (30%)\n118s\nTable 2. Reaching conformance between program, specification and tests on datasetV (253 subjects)\nthat the effect of the Chain-of-Thought system prompt is less significant, where ProofRover can\nincrease the alignment percentage by only two percentage points. A manual examination of the\nfailures revealed three main reasons for the inability of ProofRover and the baselines to repair\nmore subjects. The main reason is the models’ difficulty of inferring or strengthening specifications,\neven for the trivial programs in the dataset. Another weakness is the models’ inability to weaken\nthe preconditions when presenting a test that uses input data that does not satisfy the constraints.\nFurthermore, even though the Chain-of-Thought repair tool and ProofRover explicitly define\nthe concept of triggers for specifications with quantifiers, the models do not generate them often\nenough to support the verification of aligned subjects.\nAfter manually examining the results of Sonnet-3.5 with temperature 𝑇= 1.0 we observe that\nfor the problems no other instance solved, ProofRover has successfully aligned the artifacts by\nconstructing lemmas or using intermediate assertions to invoke quantifier triggers. We attribute\nthis capability to the high temperatures, allowing the model to try more uncommon patches. We\nbelieve that with an increased number of candidates sampled, the number of solutions can increase.\nRQ1.2: ProofRover on average performs similarly to a Naive Repair baseline and a Chain-of-\nThought repair for aligning program-specifications-tests.\n5.3\nRQ2: Behavior Coverage\nIn this research question, we aim to evaluate the capability of ProofRover to repair Dafny programs\nto reach a higher quality postcondition. We use the completeness evaluation metric proposed by\nLahiri [18]. The completeness score proposed by Lahiri quantifies the strength of a postcondition\n𝜑based on a set of tests 𝑇, representing consistent input-output pairs. The completeness score is\ndefined as the fraction of output mutations from 𝑇that are inconsistent with 𝜑. This metric is in-\nspired by the kill-set concept in mutation testing and intuitively reflects how well the postcondition\ndifferentiates the conforming code from the incorrect alternatives, the higher the score the better.\nExperimental setup. Using datasetNV, we examine the postcondition completeness of the pro-\ngram at three stages - before program-specification conformance, after program-specification\nconformance, and after program-specification-test conformance. Using datasetV, we examine the\npostcondition completeness score from a generated subject that is aligned and the score from the\nsame subject after being aligned with tests. We use a mutation set of size 20 in this experiment.\nResults. Tab. 3 summarizes our results for evaluating the capability of ProofRover to increase\nthe quality of the postcondition while reaching conformance. Column “Initial” denotes the initial\nvalue of the dataset before executing ProofRover. Columns “PS” and “PST” represent program-\nspecification conformance and program-specification-test conformance, respectively. We observe\n15\ndatasetNV\ndatasetV\nModel\nTemperature 𝑇\nInitial (None)\nPS\nInitial(PS)\nPST\nGPT-4o\n0.3\n0.38\n0.49\n0.28\n0.89\n0.7\n0.42\n0.41\n0.31\n0.88\n1.0\n0.51\n0.56\n0.29\n0.89\nSonnet-3.5\n0.3\n0.53\n0.45\n0.23\n0.84\n0.7\n0.50\n0.61\n0.22\n0.86\n1.0\n0.53\n0.54\n0.30\n0.89\nAverage\n0.48\n0.51\n0.27\n0.88\nTable 3. Evaluating completeness of the postconditions.\nthat for the program-specification con-\nformance, the completeness of the post-\nconditions stays the same on average or\nincreases by up to a maximum of 10%.\nThis shows that ProofRover maintains\nthe quality of the provided specifica-\ntions. With the inclusion of tests, the\ncompleteness of the postconditions in-\ncreases significantly since tests “puts a\nrequirement” on the specification. This\nexposes the problem of weak specifications, allowing for ProofRover to evolve the specifications.\nWe see this with the average increase of 61 percentage points across all setups.\nRQ2: ProofRover does not decrease average completeness when conforming a program with\nspecification and increases completeness when all three artifacts are provided by 61pp.\n5.4\nRQ3: Summary ambiguity\nAs the intent crystallizes into one that all artifacts (and possibly the user) agree on, we see the\nopportunity of translating it back into natural language for the purpose of using it as program\ndocumentation as software evolves or gets rewritten. Different from the original prompt, this\nshould be a clearer and unambiguous description of the desired computation. We, therefore, assess\nthe ambiguity of this description.\nGPT-4o\n T = 0.3\nGPT-4o\n T = 0.7\nGPT-4o\n T = 1.0\nSonnet-3.5\n T = 0.3\nSonnet-3.5\n T = 0.7\nSonnet-3.5\n T = 1.0\n0\n5\n10\n15\n20\n25\n30\nNon-Compiling\nCompiling\nVerified\nOriginal\nRefined\nFig. 7. Assessing ambiguity of intent.\nExperimental setup. We use the aligned arti-\nfacts created by ProofRover for datasetNV,\nspecific to the selected LLMs and their vary-\ning temperature settings, to extract the aligned\nsummary and assess its ambiguity. First, we\nconvert the identified formal intent into a nat-\nural language description. We then use this de-\nscription as a prompt for the LLM to generate\nverifiable code. If the generated code success-\nfully passes verification, it indicates that the\nsummary is clear and unambiguous, which re-\nduces nondeterminism in code generation and\nreflects a strong alignment with the desired in-\ntent.\nFor the translation from formal intent to nat-\nural language, we annotate each verifiable code produced by ProofRover with :trusted to indicate\nhard intent. We then prompt the model to summarize the annotated code, instructing it to focus on\nthe computation marked by the annotations. This approach guides the summarisation to focus on\nthe what instead of the how.\nResult. Fig. 7 provides a summary of the results from our experiment across different LLMs and\ntemperatures. The “Original” and “Refined” bars reflect the outcomes of using the original problem\nstatement and the aligned program summary, respectively, as prompts for producing verified\ncode. We observe a substantial increase in the number of programs that successfully compile\n(the Compiling and Verified sub-bars) and those that verify correctly (the Verified sub-bar) when\n16\nusing the aligned summary. This enhancement can be attributed to the aligned summaries offering\nessential information on preconditions, high-level algorithms, and invariants, thereby enabling the\nlanguage models to concentrate on accurately expressing the program’s behavior.\nRQ3: The summaries produced from an aligned program disambiguate the intent and allow\ngenerating the same program when provided to a different model.\n6\nDiscussion\nLimitations. Our approach currently tackles one failing trace at a time. We leave multipath repair\nby merging failing traces as future work, where one pass of the co-evolution process fixes the\nconformance problem entirely. We currently assume that any lemma required by the verification\nis already provided and is proven correct. Future work could look into leveraging LLMs to infer\nrequired lemmas or repair their body when not proven correct. The current limitations stem from\nour reliance on the Dafny ecosystem. While we are hopeful about discovering user intent in other\nprogramming languages through a co-evolution of artifacts and refinement of hard and soft intents,\nwe recognize that new programming environments may present their own set of challenges.\nThreats to Validity. Internal. An LLM-based synthesiser is not complete, hence it may not\nalways find a solution even when one exists. Furthermore, LLMs are prone to hallucinations, which\ncan result in patches that do not fully adhere to the constraints given to the synthesizer, such as\nmaintaining the hard intent. These solutions are filtered out through an additional validation step\nwhere Boogie verifies that the hard intent still holds for the patched Dafny program. A further\nimprovement would be to translate the hard intent and other constraints into unit tests, further\nfiltering out solutions that violate the synthesizer’s constraints. To handle quantifiers, Dafny\ninfers what are known as triggers [29], which serve as additional hints for instantiating quantified\nformulae. However, these triggers might not always be automatically inferred, leading to situations\nwhere a program is correctly patched but still not verifiable. In such cases, manual intervention is\nrequired from an oracle to specify the necessary triggers.\nExternal. One external threat is the reliance on the combination of the MBPP−DFY dataset and\nour selected models for the evaluation. At the time of writing this submission, this combination\nrepresents the state of the art in demonstrating the capability of LLMs to synthesize Dafny programs\nin the absence of program repair interventions. However, as the models become more powerful, the\neffectiveness gap between the baseline for achieving program-specification conformance and our\napproach may narrow and the risks of data leakage can increase. Nevertheless, intent disambiguation\nis likely to remain a challenge in the foreseeable future due to LLMs’ inherent dependence on natural\nlanguage prompts and the tendency of users to provide insufficiently precise intent descriptions.\n7\nRelated Work\nA significant body of work in automated program repair [10, 20, 23] focuses on synthesizing patches\nby gathering constraints mainly derived from the provided unit tests. Various methodologies\nlike static analysis [27], abstract interpretation [4], and dynamic analysis [6] have been used to\nautomatically infer specifications from code. Recent studies [5, 13, 17, 25] have also explored\nthe utilization of large language models (LLMs) for generating formal specifications in different\nformats. For instance, Kamath et al. [17] demonstrated that LLMs can generate and repair inductive\ninvariants, while Chakraborty et al. [3] introduced an approach to rerank the generated invariants\nto optimize the number of calls to the verifier. Endres et al. [5] proposed a method for creating\npostconditions from natural language definitions, which aids in identifying code errors for potential\nrepairs. Additionally, Hahn et al. [13] examined the generalization capabilities of language models\n17\nby fine-tuning the T5 model to generate competitive Linear Temporal Logic formulas, First-Order\nLogic, and regular expressions. Pei et al. [25] further fine-tuned a language model with invariants\nmined from the dynamic analysis tool Daikon, enabling the inference of new invariants in a single\nstep or across multiple execution points. Building on the insights from these studies, we developed\na co-evolution strategy that enables the simultaneous repair of code, invariants, preconditions, and\npostconditions in an integrated manner.\nThe work by First et al. [9] presents an approach in which an LLM is fine-tuned to repair proofs\nin conjunction with another LLM-based tool, Thor [16], for generating correct code in Isabelle/HOL.\nIn contrast, Gopinathan et al. [12] modify proofs to align with newly created code, assuming that\nthe program’s evolution is limited to non-functional changes. LeanDojo [31] focuses on extracting\ndata from Lean and enabling programmatic interaction with the proof environment, providing fine-\ngrained annotations of premises in proofs that are crucial for premise selection—a key challenge\nin theorem proving. Ringer’s work [26] centers on proof repair within an inductive reasoning\nsystem, aiming to maintain the proof while evolving system requirements without altering those\nrequirements. In contrast to these approaches, our workflow allows for the simultaneous evolution\nof code alongside the proofs and specifications.\nSimilar to our work, Misu et al. [21] choose the Dafny programming language and utilize 178\nprogramming problems from the MBPP dataset to prompt the models GPT-4 and PaLM-2 for\ngenerating methods in Dafny. They demonstrate that employing a Chain of Thought prompt can\neffectively produce verified and accurate Dafny methods with meaningful specifications. In another\nrecent study, Sun et al. [28] focus on iterating code generation without applying any repairs,\nusing artifacts like comments and existing annotations as feedback to produce correct outputs,\nemphasizing the importance of internal consistency. Huang et al. [15] also tackle the challenge of\naligning intent from specifications, code, and test cases, aiming to rerank solutions through intra-\nand inter-consistency. In our evaluation, we uniquely use tests as a hard constraint to represent the\nuser’s intent, guiding the entire evolution process. Furthermore, rather than merely ranking the\nsolutions provided by the language model, we adopt an iterative co-evolution approach to refine\nand develop all three artifacts—specifications, code, and tests—according to the user’s intent.\n8\nOutlook\nWith the advent of LLMs in automatic programming, the interest in trusted automatic programming\nvia LLMs increases. Unfortunately, it is difficult to give any guarantees about code generated from\nLLMs, partly also because a detailed specification of the intended behavior is usually not available. In\nthis paper we alleviate this lack of functionality specifications by aligning automatically generated\ncode via LLMs, automatically generated formal specifications (obtained from natural language using\nLLMs), as well as tests. The conformance between generated programs, generated specifications,\nand tests - does not provide absolute guarantees but enhances trust. Establishing such conformance\nalso helps us uncover the likely intended program behavior.\nIn this work, we developed this theme of program-proof co-evolution, with the goal of enhancing\ntrust in automatically generated code. There can be other use-cases of our program-proof co\nevolution technology, such as automatic documentation generation and specific scenarios of\nsoftware maintenance where a new software component needed by a larger software project\nmay undergo our program-proof co-evolution methodology.\nReferences\n[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models.\narXiv:2108.07732 [cs.PL] https://arxiv.org/abs/2108.07732\n18\n[2] Mike Barnett and Rustan Leino. 2005. Weakest-Precondition of Unstructured Programs. In PASTE ’05: The 6th\nACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering (paste ’05: the 6th acm\nsigplan-sigsoft workshop on program analysis for software tools and engineering ed.). ACM Press, 82–87.\nhttps:\n//www.microsoft.com/en-us/research/publication/weakest-precondition-of-unstructured-programs/\n[3] Saikat Chakraborty, Shuvendu Lahiri, Sarah Fakhoury, Madan Musuvathi, Akash Lal, Aseem Rastogi, Nikhil Swamy,\nand Rahul Sharma. 2023. Ranking LLM-Generated Loop Invariants for Program Verification. In 2023 Empirical Methods\nin Natural Language Processing. Singapore. EMNLP-Findings 2023.\n[4] Patrick M. Cousot, Radhia Cousot, Francesco Logozzo, and Michael Barnett. 2012. An abstract interpretation framework\nfor refactoring with application to extract methods with contracts. In Proceedings of the ACM International Conference\non Object Oriented Programming Systems Languages and Applications (Tucson, Arizona, USA) (OOPSLA ’12). Association\nfor Computing Machinery, New York, NY, USA, 213–232. https://doi.org/10.1145/2384616.2384633\n[5] Madeline Endres, Sarah Fakhoury, Saikat Chakraborty, and Shuvendu Lahiri. 2024.\nCan Large Lan-\nguage Models Transform Natural Language Intent into Formal Method Postconditions?. In The ACM In-\nternational Conference on the Foundations of Software Engineering (FSE). ACM, Porto de Galinhas, Brazil,\nBrazil. https://www.microsoft.com/en-us/research/publication/formalizing-natural-language-intent-into-program-\nspecifications-via-large-language-models/ https://2024.esec-fse.org/details/fse-2024-research-papers/51/Can-Large-\nLanguage-Models-Transform-Natural-Language-Intent-into-Formal-Method-Postco.\n[6] Michael D. Ernst, Jake Cockrell, William G. Griswold, and David Notkin. 1999. Dynamically discovering likely\nprogram invariants to support program evolution. In Proceedings of the 21st International Conference on Software\nEngineering (Los Angeles, California, USA) (ICSE ’99). Association for Computing Machinery, New York, NY, USA,\n213–224. https://doi.org/10.1145/302405.302467\n[7] Rustan Leino et al. 2024. The Dafny Programming and Verification Language. https://dafny.org/\n[8] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M. Zhang. 2023. Large Language Models for\nSoftware Engineering: Survey and Open Problems. In 2023 IEEE/ACM International Conference on Software Engineering:\nFuture of Software Engineering (ICSE-FoSE). IEEE Computer Society, Los Alamitos, CA, USA, 31–53. https://doi.org/10.\n1109/ICSE-FoSE59343.2023.00008\n[9] Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun. 2023. Baldur: Whole-Proof Generation and Repair with\nLarge Language Models. arXiv:2303.04910 [cs.LG]\n[10] Xiang Gao, Bo Wang, Gregory J. Duck, Ruyi Ji, Yingfei Xiong, and Abhik Roychoudhury. 2021. Beyond Tests: Program\nVulnerability Repair via Crash Constraint Extraction. ACM Trans. Softw. Eng. Methodol. 30, 2, Article 14 (feb 2021),\n27 pages. https://doi.org/10.1145/3418461\n[11] GitHub. 2021. GitHub Copilot. https://copilot.github.com/.\n[12] Kiran Gopinathan, Mayank Keoliya, and Ilya Sergey. 2023. Mostly Automated Proof Repair for Verified Libraries.\nProceedings of the ACM on Programming Languages 7, PLDI (2023), 25–49.\n[13] Christopher Hahn, Frederik Schmitt, Julia J. Tillman, Niklas Metzger, Julian Siber, and Bernd Finkbeiner. 2022. Formal\nSpecifications from Natural Language. arXiv:2206.01962 [cs.SE]\n[14] C. A. R. Hoare. 1969. An axiomatic basis for computer programming. Commun. ACM 12, 10 (oct 1969), 576–580.\nhttps://doi.org/10.1145/363235.363259\n[15] Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2024. Enhancing Large Language Models in\nCoding Through Multi-Perspective Self-Consistency. arXiv:2309.17272 [cs.CL] https://arxiv.org/abs/2309.17272\n[16] Albert Q. Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu,\nand Mateja Jamnik. 2022. Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers.\narXiv:2205.10893 [cs.AI]\n[17] Adharsh Kamath, Aditya Senthilnathan, Saikat Chakraborty, Pantazis Deligiannis, Shuvendu Lahiri, Akash Lal, Aseem\nRastogi, Subhajit Roy, and Rahul Sharma. 2024. Finding Inductive Loop Invariants using Large Language Models.\nTechnical Report 2311.07948. arXiv.\n[18] Shuvendu K. Lahiri. 2024. Evaluating LLM-driven User-Intent Formalization for Verification-Aware Languages.\narXiv:2406.09757 [cs.PL] https://arxiv.org/abs/2406.09757\n[19] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is Your Code Generated by Chatgpt\nReally Correct? Rigorous Evaluation of Large Language Models for Code Generation. Advances in Neural Information\nProcessing Systems 36 (2024).\n[20] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2015. DirectFix: Looking for Simple Program Repairs. In\n2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 1. Florence, Italy, 448–458. https:\n//doi.org/10.1109/ICSE.2015.63\n[21] Md Rakib Hossain Misu, Cristina V. Lopes, Iris Ma, and James Noble. 2024. Towards AI-Assisted Synthesis of Verified\nDafny Methods. Proceedings of the ACM on Software Engineering 1, FSE (July 2024), 812–835. https://doi.org/10.1145/\n3643763\n19\n[22] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-Based Prompt Selection for Code-Related Few-Shot\nLearning. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). Melbourne, Australia,\n2450–2462. https://doi.org/10.1109/ICSE48619.2023.00205\n[23] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. 2013. SemFix: Program repair\nvia semantic analysis. In 2013 35th International Conference on Software Engineering (ICSE). San Francisco, CA, USA,\n772–781. https://doi.org/10.1109/ICSE.2013.6606623\n[24] Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang. 2023. LLM is Like a Box of Chocolates: the Non-\ndeterminism of ChatGPT in Code Generation. arXiv:2308.02828 [cs.SE] https://arxiv.org/abs/2308.02828\n[25] Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023. Can Large Language Models Reason\nabout Program Invariants?. In Proceedings of the 40th International Conference on Machine Learning (Proceedings of\nMachine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan\nSabato, and Jonathan Scarlett (Eds.). PMLR, 27496–27520. https://proceedings.mlr.press/v202/pei23a.html\n[26] Talia Ringer. 2021. Proof Repair. University of Washington, University of Washington.\n[27] Sharon Shoham, Eran Yahav, Stephen Fink, and Marco Pistoia. 2007. Static specification mining using automata-based\nabstractions. In Proceedings of the 2007 International Symposium on Software Testing and Analysis (London, United\nKingdom) (ISSTA ’07). Association for Computing Machinery, New York, NY, USA, 174–184. https://doi.org/10.1145/\n1273463.1273487\n[28] Chuyue Sun, Ying Sheng, Oded Padon, and Clark Barrett. 2024. Clover: Closed-Loop Verifiable Code Generation.\narXiv:2310.17807 [cs.AI] https://arxiv.org/abs/2310.17807\n[29] The dafny-lang community. 2024. Dafny FAQ. https://github.com/dafny-lang/dafny/wiki/FAQ#how-does-dafny-\nhandle-quantifiers-ive-heard-about-triggers-what-are-those.\n[30] The dafny-lang community. 2024. Dafny Reference Manual. https://dafny.org/dafny/DafnyRef/DafnyRef.html.\n[31] Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger,\nand Anima Anandkumar. 2023.\nLeanDojo: Theorem Proving with Retrieval-Augmented Language Models.\narXiv:2306.15626 [cs.LG] https://arxiv.org/abs/2306.15626\n[32] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam,\nand Edward Aftandilian. 2024. Measuring GitHub Copilot’s Impact on Productivity. Commun. ACM 67, 3 (feb 2024),\n54–63. https://doi.org/10.1145/3633453\n20\n",
  "categories": [
    "cs.SE",
    "cs.LG",
    "cs.PL"
  ],
  "published": "2024-10-24",
  "updated": "2024-11-05"
}