{
  "id": "http://arxiv.org/abs/2410.19189v1",
  "title": "Reinforcement Learning the Chromatic Symmetric Function",
  "authors": [
    "Gergely Bérczi",
    "Jonas Klüver"
  ],
  "abstract": "We propose a conjectural counting formula for the coefficients of the\nchromatic symmetric function of unit interval graphs using reinforcement\nlearning. The formula counts specific disjoint cycle-tuples in the graphs,\nreferred to as Eschers, which satisfy certain concatenation conditions. These\nconditions are identified by a reinforcement learning model and are independent\nof the particular unit interval graph, resulting a universal counting\nexpression.",
  "text": "Reinforcement Learning the Chromatic Symmetric Function\nGergely B´erczi ∗\nJonas Kl¨uver †\nAbstract\nWe propose a conjectural counting formula for the coefficients of the chromatic\nsymmetric function of unit interval graphs using reinforcement learning. The formula\ncounts specific disjoint cycle-tuples in the graphs, referred to as Eschers, which satisfy\ncertain concatenation conditions. These conditions are identified by a reinforcement\nlearning model and are independent of the particular unit interval graph, resulting a\nuniversal counting expression.\n1\nIntroduction\nThe study of proper graph colorings is a fundamental area in graph theory and theoretical\ncomputer science.\nFor a given graph G, the number of ways to color its vertices with\nn colors, such that no two adjacent vertices share the same color, is given by the chro-\nmatic polynomial χG(n). The chromatic polynomial exhibits several intriguing properties,\nincluding log-concavity, a celebrated result established by Huh [4] The chromatic symmet-\nric function, a multi-variable generalization of the chromatic polynomial, was introduced\nby Richard Stanley [8], who formulated several deep conjectures in the 1990’s regarding\nits algebraic properties. This symmetric function is connected to various fields, including\ntopology, statistical mechanics, representation theory and algebraic geometry, and holds a\ncentral position in algebraic combinatorics and graph theory.\nThe goal of this paper is to study the chromatic symmetric function using machine\nlearning techniques. Motivated by the Stanley-Stembridge positivity conjecture [10, 8], our\nmain result is a conjectured counting formula for coefficients of the elementary symmetric\nfunction expansion of the chromatic symmetric function, suggested by deep reinforcement\nlearning models.\nLet G be a finite graph, with V (G) representing its vertices and E(G) representing its\nedges.\nDefinition 1.1. A proper coloring c of G is a function c : V →N, where no two adjacent\nvertices share the same color.\n∗Department of Mathematics, Aarhus University, email: gergely.berczi@math.au.dk\n†Department of Mathematics, Aarhus University, email: jonas.kluever@gmail.com\n1\narXiv:2410.19189v1  [math.CO]  24 Oct 2024\nFor a given coloring c, we can associate a monomial:\nxc =\nY\nv∈V\nxc(v),\nwhere x1, x2, . . . are commuting variables. Let Π(G) denote the set of all proper colorings\nof G, and let Λ ⊂Z[x1, x2, . . .] denote the ring of symmetric functions in the infinite set of\nvariables x1, x2, . . ..\nDefinition 1.2. The chromatic symmetric function XG ∈Λ of a graph G is defined\nas the sum of the monomials xc over all proper colorings c of G:\nXG =\nX\nc∈Π(G)\nxc.\nRemark 1.3. One could consider a finite but sufficiently large number of colors r, which\nwould lead to XG ∈Λr, the set of symmetric polynomials in r variables. Although this\nslightly complicates notation, it does not alter the results.\nDefinition 1.4. The m-th elementary symmetric function em is defined as:\nem =\nX\ni1<i2<···<im\nxi1xi2 . . . xim,\nwhere i1, . . . , im ∈N. Given a partition λ = (λ1 ≥λ2 ≥· · · ≥λk), we define the elementary\nsymmetric function eλ as eλ = Qk\ni=1 eλi. These functions form a basis of Λ.\nDefinition 1.5. A symmetric function X ∈Λ is e-positive if it can be written as a\nnon-negative linear combination of elementary symmetric functions.\nFor example, the chromatic symmetric function of Kn, the complete graph on n vertices,\nis XKn = n!en, which is e-positive.\nDefinition 1.6. The incomparability graph inc(P) of a poset P is the graph whose\nvertices are the elements of P, and two vertices are adjacent if they are incomparable in\nP.\nDefinition 1.7. A poset P is said to be (a+b)-free if it does not contain a chain of length\na and a chain of length b that are mutually incomparable.\nDefinition 1.8. A unit interval order (UIO) is a poset that is isomorphic to a finite\nsubset of U ⊂R, where u and w ∈U are incomparable if and only if |u −w| < 1.\nTheorem 1.9 (Scott-Suppes [6]). A finite poset P is a UIO if and only if it is 2 + 2-free\nand 3 + 1-free.\n2\nStanley and Stembridge [10, 8] proposed the following positivity conjecture.\nConjecture 1.10 (Stanley-Stembridge [10]). If P is a 3 + 1-free poset, then Xinc(P) is\ne-positive.\nThe Stanley-Stembridge suggests that for the incomparability graph of any 3 + 1-free\nposet P, the coefficients cλ in the expansion\nXG =\nX\nλ\ncλeλ,\nare non-negative for all partitions λ. It has been computationally verified for posets with\nup to 20 elements [1]. In [7] Sharesahian and Wachs introduced a refined version of the\nchromatic symmetric functions, and conjectured that these chromatic quasi-symmetric\nfunctions are also e-positive.\nIn 2013, Guay-Paquet [1] showed that it is enough to prove the Stanley-Stembridge\nconjecture for the case where the poset is both 3 + 1-free and 2 + 2-free, i.e., for unit\ninterval orders.\nTheorem 1.11 (Guay-Paquet [1]). Let P be a 3 + 1-free poset. Then, Xinc(P) is a convex\ncombination of the chromatic symmetric functions\n{Xinc(P ′) | P ′ is both 3 + 1-free and 2 + 2-free}.\nShortly before submitting our preprint, we became aware of Hikita’s work [3], which\npresents a probabilistic interpretation of the Stanley coefficients and establishes the e-\npositivity conjecture. In contrast, our results offer a fundamentally different interpretation\nof these coefficients, discovered through the application of machine learning techniques.\nAcknowledgments\nWe thank Andras Szenes for introducing us to this problem. We are greatly indebted to\n´Ad´am Zsolt Wagner, whose expertise in machine learning profoundly shaped this paper.\nThe first author was supported by DFF 40296 grant of the Danish Independent Research\nFund.\n2\nThe main results\nIn this paper, instead of using supervised learning models to study the Stanley coefficients\ndirectly, we follow a more sophisticated learning strategy, which was motivated by recent\nwork of Szenes and Rok [11]. This work suggests that the Stanley coefficients should count\nspecific cycle-tuples within the UIO. This method offers two key advantages:\n3\n1. It provides an exact counting interpretation of the coefficients.\n2. It yields a universal formula, as the cycles we count depend only on the partition λ,\nrather than on the specific UIO.\nThe strategy in a nutschell is the following: given a partition λ = (λ1, . . . , λs) the\nStanley coefficient cλ is given by the count of λ-Eschers in the UIO satisfying certain\nsplitting and insertion conditions. We use a reinforcement learning agent to identify the\ncorrect conditions. Hence the central objects are Eschers, which are cycles of a certain\ntype in the UIO graph. Given two unit intervals u1 and u2 on the real line, their relative\npositions can fall into one of three categories:\n1. u1 intersects u2,\n2. u1 ∩u2 = ∅and u1 is to the left of u2,\n3. u1 ∩u2 = ∅and u1 is to the right of u2.\nFollowing [11], we will use the notation u1 ≺u2 for case (2), u1 ≻u2 for case (3), and\nu1 →u2 if either case (1) or case (2) holds.\nDefinition 2.1. Let U be a unit interval order with intervals u0, . . . , un−1. The area\nsequence of U is the unique increasing sequence aU = (a0 ≤. . . ≤an−1) where\n1. 0 ≤ai ≤i is integer\n2. ui ∼uj if and only if i ≥aj.\nRemark 2.2. The number of UIOs of length n is equal to the number of increasing integer\nsequences satisfying condition (1), which is the Catalan number Cn =\n1\nn+1\n\u00122n\nn\n\u0013\n, and the\nnumber coefficients of the chromatic symmetric function XU is the number of partitions\nof n, which we denote by Pn. Here’s a table for small number of vertices:\n# Vertices\n# UIOs, Cn\n# Coeffs, |Pn|\n2\n2\n2\n3\n5\n3\n4\n14\n5\n5\n42\n7\n6\n132\n11\n7\n429\n15\n8\n1430\n22\n9\n4862\n30\n20\n6564120420\n627\n4\nFor small UIOs it is feasible to compute all coefficients and verify their non-negativity.\nComputation of the Stanley coefficients is based on a combinatorial formula of Theorem\n3.16, using the Stanley G-homomorphism. However, as the table suggests, the complexity\ngrows rapidly. Calculating these coefficients becomes both memory- and time-intensive.\nThe conjecture has been verified for UIOs of up to length 20 [1].\nDefinition 2.3. We call a sequence [v0, v1, . . . , vk−1] of distinct elements from U a k-\nEscher if the relation\nv0 →v1 →· · · →vk−1 →v0\nholds. We will denote by EU\nk the set of k-Eschers in U. For a pair of integers k, l we denote\nby EU\nk,l the set of disjoint (k, l) Escher pairs:\nEk,l = {(v, w) ∈EU\nk × EU\nl : v ∩w = ∅}\nWe adapt the same notation for longer Escher-tuples.\nThe operation of cyclic permutation on sequences is denoted by ζ:\nζ : [v0, v1, . . . , vk−1] 7→[v1, v2, . . . , vk−1, v0].\nBy applying powers of ζ to a k-Escher, we generate k −1 new k-Eschers. Since all the\nresulting sequences are equivalent, we refer to this group of sequences as a ”cyclic Escher”\nto represent the isomorphism class of sequences related by ζ. An Escher, then, is a cyclic\nEscher with a chosen starting point. For a given escher u we let uv denoted the escher\nequivalent to u, starting at v ∈u. Additionally, for a k-Escher, we treat the index set as\nintegers modulo k, so that, for instance, u0 = uk.\nOur starting point is the following observation of Szenes-Rok [11], following Stanley [8].\nThe general formula using G homomorphism will be given in the next section.\nProposition 2.4 (Stanley, Szenes-Rok). Let U be a UIO of length n ≥k + l. The Stanley\ncoefficient corresponding to λ = (k, l) is cU\n(k,l) = #EU\nk,l −#EU\nk+l.\nDefinition 2.5. Let U be a unit interval order (UIO) of length n, and let v = [v0, . . . , vk−1] ∈\nEU\nk be a k-Escher. Fix a positive integer l ≤k. If there exists an m ∈N such that\nvm+1 →vm+2 →· · · →vm+l →vm+1\nand\nv0 →· · · →vm →vm+l+1 →· · · →vk−1\nholds, then we call [vm+1, . . . , vm+l] a valid l-subescher of v. We call such m a splitting point\nor subescher starting point of v. We denote by S(v, l) the set of splitting points, and if\nthis is nonempty then we let SEStart(v, l) denote the first (smallest non-negative) splitting\npoint (SEStart for Sub-Escher Start), and SEEnd(v, l) = SEStart(v, l) + l.\n5\nv =\nu =\n0\n5\n3\n1\n0\n5\n3\n1\n0\n5\n3\n1\n. . .\n4\n2\n8\n6\n10\n9\n7\n4\n2\n8\n6\n10\n. . .\nFigure 1: Insertion points and sub-Eschers for the Escher pair u = [4, 2, 8, 6, 10, 9, 7], v =\n[0, 5, 3, 1] in the UIO U = (0, 0, 1, 1, 2, 3, 3, 4, 6, 7, 9). Green dots indicate insertion points\n(i.e 2 →3 and 5 →8 in U, and underbrace indicates length 4 sub-Eschers in u.\nIn short, a valid subescher of length l is a subescher such that the remaining part of v\nstill forms an Escher, illustrated as follows:\nwm\nwm+1\nwm+2\nwm+l\nwm+l+1\n. . .\n. . .\n. . .\nProposition 2.6. [11],Proposition 4.5. Any k+l-Escher has at least one valid k-subescher.\nDefinition 2.7. Let u = [u0, . . . , uk−1] ∈EU\nk and v = [v0, . . . , vl−1] ∈EU\nl\nbe disjoint\nsubeschers of length k, l respectively, that is, (u, v) ∈EU\nk,l is a (k, l) Escher-pair. We call\ni ∈Z an insertion point for (u, v) if ui →vi+1 and vi →ui+1 holds. In this case we can\nconcatenate u and v to get a length k + l escher u +i v as follows:\n1. for i = 0, ..., k −1\nu +i v = [u0, ..., ui, vi+1, ..., vi+l, ui+1, ..., uk−1]\n2. for i ≥k we define\nu +i v = [uq, ..., ui, vi+1, ..., vi+l, ui+1, ..., uq+k−1]\nwhere q ∈(i −k, i] is choosen s.t k|q.\nDenote the set of insertion points by I(u, v), and if this is nonempty, then the first (smallest\nnon-negative) insertion point by FirstIns(u, v).\nExample 2.8. Let U be the UIO with 11 intervals given by the area sequence U =\n(0, 0, 1, 1, 2, 3, 3, 4, 6, 7, 9) as in Figure 1. Let\nu = [4, 2, 8, 6, 10, 9, 7], v = [0, 5, 3, 1]\nforming a (7, 4) Escher-pair (u, v) ∈EU\n4,3. We put (u, v) periodically under each other with\na period lcm(7, 4) = 28 as in Figure 1. We indicated by green array the insertion points.\n6\nIn contrast to Proposition 2.6, there are Escher pairs that have no insertion points.\nHence Proposition 2.4 intuitively suggests that for a fixed pair (k, l) the Stanley coefficient\ncU\n(k,l) counts (k, l) Escher-pairs in U which cannot be concatenated to an (k + l)-Escher.\nThe technical difficulty lies in handling Eschers as linear, not cyclic objects.\nTo follow this intuition, to an Escher pair (u, v) ∈EU\nk,l we associate a 3-dimensional\nrational vector, which we call the core vector, defined as the function\nτ 2 : EU\nk,l →Q4\nτ 2(u, v) =\n\n\n\n\n\n(0, −1, −1, −0.5)\nif I(u, v) = ∅,\n(0, SEStart(u, l), SEEnd(u, l), FirstIns(u, v) + 0.5)\nif I(u, v) ̸= ∅& 0 < SEStart(u, l),\n(0, −1, −1, FirstIns(u, v) + 1.5)\nif I(u, v) ̸= ∅& 0 = SEStart(u, l).\nBy Proposition 2.6 all cases are covered.\nWe call τ 2 the core representation for\nEscher pairs.\nWe spent considerable time to come up with the right core coordinates,\nand generalise the core representation to Escher tuples of length 3 and higher. We see\nthat the second and third coordinates are integers, the fourth coordinate is always half-\ninteger, this is crucial in the RL model. We will denote the ith coordinate with τ 2\ni (u, v),\nand simply refer to them as 0, SEStart,SEEnd,FirstIns, but keeping in mind their more\nsophisticated definition.\nThe intuition is that the numerical relationship among the core coordinates character-\nizes those (k, l) Escher pairs which do not concatenate to a (k + l) Escher in Proposition\n2.4. Our first result supports this intuition.\nML Theorem 1 (Counting formula for pairs). For any UIO of length at least k + l\ncU\n(k,l) ≥#{(u, v) ∈EU\nk,l : τ 2\n3 (u, v) < τ 2\n4 (u, v)}\n(1)\nand equality holds for all UIOs when λ = (k, k). Moreover, equality holds for almost all\nUIOs even when k ̸= l: for a fixed (k, l) the mismatch ratio is less than 0.5%, see Remark\n2.9 for explanation.\nRemark 2.9.\n1. Table 1 illustrates Theorem 1.\nWe observe that for pairs (k, l) ̸=\n(5, 3), (5, 4) we get perfect matching and equality in (1). For λ = (5, 3), there is a\nmismatch in (1) for only 1 out of 429 UIOs. This exceptional UIO is\nU = (0, 0, 1, 2, 3, 4, 5, 6)\nwhich does not exhibit any obvious unusual properties, making it unclear why the\nmismatch occurs specifically with this UIO.\n2. More notably, from Table 1 the sum of the 429 Stanley coefficients is 52,500, while\nthe total absolute error is just 1.\n7\n3. This phenomenon highlights why proving a counting formula is so challenging: we\nencounter extremely rare exceptional UIOs with minimal mismatches, yet there is no\napparent explanation for their occurrence.\nWe can illustrate the conditions in (1) as a graph, whose vertices are the cores of the\nrepresentation, and an oriented edge from core1 to core2 means that core1 < core2, see\nFigure 2.\n0\nSEStart\nSEEnd\nFirstIns\nFigure 2: Condition graph for Escher pairs\nFor a triple λ = (k, l, m), the idea is that the Stanley coefficient cU\n(k,l,m) should count\nthe (k, l, m) Escher tuples where no two tuples concatenate at the deepest level—in other\nwords, any two tuples remain non-concatenated. For a triple (u, v, w) ∈EU\nk,l,m, we use the\ncore coordinates for the pairs (u, v), (u, w), and (v, w). However, if u and v do concatenate,\nthen we need to store the core coordinates for the pair (uv, w), (uw, v) and (vw, u) as well.\nThis results in a total of 19 core coordinates as follows:\nτ 3 : EU\nk,l,m →Q19\nτ 3(u, v, w) = (0, τ 2(u, v)[1 :], τ 2(v, w)[1 :], τ 2(u, w)[1 :], ˜τ 2(uv, w), ˜τ 2(uw, v), ˜τ 2(vw, u))\nwhere\n1. τ 2(u, v)[1 :] stands for the last 3 coordinates of τ 2(u, v), that is, we drop the 0 from\nthe beginning.\n2. the definition of ˜τ 2 is quite subtle due to the cases where u and v do not concatenate,\nmaking the cores involving uv undefined or irrelevant. We let\n˜τ 2(uv, w) =\n(\n(−1, −1, −0.5)\nif I(u, v) = ∅\nτ 2(u +i v, w)[1 :]\nif FirstIns(u, v) = i\n(2)\nFor a more concise and descriptive reference we use the following shorthand notation:\nfor a triple (u, v, w) ∈EU\n(k,l,m) we put\nSEStart(u, v) = SEStart(u, |v|) and SEEnd(u, v) = SEEnd(u, |v|).\n8\nSo the notations for the 19 core coordinates are\nτ3(u, v, w) =(0, SEStart(u, v), SEEnd(u, v), FirstIns(u, v),\nSEStart(v, w), SEEnd(v, w), FirstIns(v, w),\nSEStart(u, w), SEEnd(u, w), FirstIns(u, w),\nSEStart(uv, w), SEEnd(uv, w), FirstIns(uv, w),\nSEStart(uw, v), SEEnd(uw, v), FirstIns(uw, v),\nSEStart(vw, u), SEEnd(vw, u), FirstIns(vw, u))\nThe condition graph we are looking for has maximum 19 vertices, and our RL agent\nfound the following\nML Theorem 2 (Counting formula for triples).\n1. For a triple (k, l, m) with k ≥\nl ≥m\ncU\n(k,l,m) ≈#\n\n\n\n\n\n(u, v, w) ∈EU\nk,l,l :\nSEEnd(u, v) < FirstIns(u, v),\nSEEnd(u, w) < FirstIns(u, w),\nSEEnd(v, w) < FirstIns(v, w)\n\n\n\n\n\nwhere ≈means that the mismatch ratio is < 3% and total absolute error ratio less\nthan 1% (see Table 2).\n2. For λ = (k, l, l) with k ≥l we have equality above, and our counting formula gives\nthe Stanley coefficient (see Table 2)\n3. There is a modified canonical model which gives lower bound for all triples (k, l, m)\nwith k ≥l ≥m (see Table 3):\ncU\n(k,l,m) ≥#\n\n\n\n\n\n\n\n\n\n(u, v, w) ∈EU\nk,l,m :\n0 ≥SEStart(u, v), 0 ≥SEStart(u, w), 0 ≥SEStart(v, w)\nSEEnd(u, v) < FirstIns(u, v),\nSEEnd(u, w) < FirstIns(u, w),\nSEEnd(v, w) < FirstIns(v, w)\n\n\n\n\n\n\n\n\n\nThe corresponding condition graphs are shown in Figure 3.\nWe concluded similar results for quadruples, see Table 4,5 and we summarize our find-\nings as\nML Theorem 3 (General counting formula). Let λ = (λ1 ≥λ2 ≥. . . ≥λr) be a\npartition then\n1.\ncU\nλ ≈#\n\b\n(u1, . . . ur) ∈EU\nλ\n: SEEnd(ui, uj) < FirstIns(ui, uj) for all 1 ≤i < j ≤r\n\t\nwhere ≈means that the mismatch ratio is < 1% for most λ’s\n9\nSEEnd(u,v)\nFirstIns(u,v)\nSEEnd(u,w)\nFirstIns(u,w)\nSEEnd(v,w)\nFirstIns(v,w)\n(a) Best approximating graph and perfect match\nfor (k, l, l)\nSEEnd(u,v)\nFirstIns(u,v)\nSEEnd(u,w)\nFirstIns(u,w)\nSEEnd(v,w)\nFirstIns(v,w)\nSEStart(v,w)\nSEStart(u,v)\nSEStart(u,w)\n0\n(b) Lower bound for all triples\nFigure 3: The condition graphs for Escher triples\n2. For λ = (k, l, l, . . . l) with k ≥l we have equality above, and our counting formula\ngives the Stanley coefficient.\n3. Slight modification provides lower bound for the coefficients:\ncU\nλ ≥#\n(\n(u1, . . . ur) ∈EU\nλ\n: SEEnd(ui, uj) < FirstIns(ui, uj) for all 1 ≤i < j ≤r\n0 ≥SEStart(ui, uj) for all 1 ≤i < j ≤r\n)\n3\nMathematical background\n3.1\nStanley’s G-homomorphism\nLet Λ ⊂Z[x1, x2, . . .] denote the ring of symmetric functions in the variables x1, x2, . . .,\ngenerated as an algebra by the the elementary symmetric functions {ei : i = 0, 1, . . .}\ndefined in Definition 1.4, and as a Z-module by {eλ}. Given a finite graph G with vertices\nV = {v1, ..., vn}, let ΛG = Z[v1, ..., vn], be the polynomial ring on the vertices. Stanley’s\nG-homomorphism ρG : Λ →ΛG is defined as the unique ring-homomorphism s.t.\nρG(ei) =\nX\nS⊆V\nS is empty,|S|=i\nY\nv∈S\nv\nwhere the sum is taken over all subgraphs on i vertices without edges. For f ∈Λ we set\nfG = ρG(f).\n10\nExample 3.1. Let G be a finite graph on n vertices. Then eG\n1 = P\nv∈G v, if furthermore\nG has at least 1 edge, then eG\nn = 0. For a partition λ = (λ1 ≥λ2 ≥· · · ≥λr) we have\neG\nλ =\nr\nY\ni=1\neG\ni ,\nDefinition 3.2. Let α : V →N be a function assigning a nonnegative integer to each\nvertex of G.\n1. We use the shorthand notation vα = Q\nv∈V\nvα(v) ∈ΛG, and for f ∈Λ, let [fG, vα]\ndenote the coefficient of the monomial vα in fG.\n2. We denote by Gα, the graph where each vertex vi is replaced by the complete graph\nKα(i), and for i ̸= j a vertex from Kα(i) is connected to a vertex from Kα(j) if and\nonly if vi and vj are connected by an edge in G.\nNote that the function f 7→[fG, vα] is linear, but not a ring-homomorphism: given a\ngraph G with at least one edge we have [eG\ni , vα] = 0 for all i ∈N but [((e1)n)G, vα] = n!.\nDefinition 3.3. We have other canonical bases of symmetric functions:\n1. Denote by pm the m-th power sum symmetric function pm = P\ni∈N xm\ni . Given a\npartition λ = (λ1 ≥λ2 ≥· · · ≥λk), we define the power sum symmetric function\npλ =\nk\nY\ni=1\npλi.\n2. Given a partition λ = (λ1 ≥λ2 ≥· · · ≥λk), we define the monomial symmetric\nfunction\nmλ =\nX\ni1<i2<···<ik\nX\nσ∈Sk(λ)\nxλ1\ni1 xλ2\ni2 . . . xλk\nik ,\nwhere the inner sum is taken over the set of all permutations of the sequence λ,\ndenoted by Sk(λ).\n3. For a partition λ = (λ1 ≥λ2 ≥· · · ≥λk), define the Schur functions as\nsλ = det(eλ∗\ni +j−i)i,j,\nwhere λ∗is the conjugate partition to λ.\nThe sets {pλ},{mλ},{sλ} form three basis of the ring Λ of symmetric functions.\n11\nProposition 3.4 (Stanley [9]). Let x = (x1, x2, . . .) denote an infinite set of variables and\nv = (v1, . . . , vn) the variables given by the vertices of G. Let\nT(x, v) =\nX\nλ\neλ(x)mG\nλ (v)\nthen\n[T(x, v), vα]\nY\nv∈V\nα(v)! = XGα\nUsing Proposition 3.4, Stanley proved [9] that the simultaneous e-positivity of a large\nclass of graphs is equivalent to monomial positivity of mG\nλ for all partitions:\nTheorem 3.5. (Stanley) For every finite graph G we have\n1. XGα is s-positive for every α : V (G) →N ⇐⇒sG\nλ ∈N[V (G)] for every partition λ\n2. XGα is e-positive for every α : V (G) →N ⇐⇒mG\nλ ∈N[V (G)] for every partition λ\nProof. Let x, y denote 2 infinite sets of independent variables. The Cauchy product\nC(x, y) =\nY\ni,j\n(1 + xiyj)\nyields the identities [5, (4.2’), (4.3’)]\nX\nλ\nsλ(x)sλ∗(y) =\nX\nλ\nmλ(x)eλ(y) =\nX\nλ\neλ(x)mλ(y)\nwith the sums over all partitions of any length. By viewing only y as the indeterminates\nand x as constant we can apply the G-homomorphism and get\nX\nλ\nsλ(x)sG\nλ∗(v) =\nX\nλ\nmλ(x)eG\nλ (v) =\nX\nλ\neλ(x)mG\nλ (v)\nUsing Proposition 3.4 gives\nXGα = (\nY\nv∈V\nα(v)!)\nX\nλ\neλ(x)[mG\nλ (v), vα] = (\nY\nv∈V\nα(v)!)\nX\nλ\nsλ(x)[sG\nλ∗(v), vα]\n■\nIf the chromatic polynomial is XG = P\nλ\ncG\nλ eλ, then using Proposition 3.4 with α ≡1,\nwe get\ncG\nλ = [mG\nλ , v1 · v2 · ... · vn]\nfor any partition λ. In particular, the Stanley conjecture follows if\n[mU\nλ , v1 · v2 · ... · vn] ≥0\nfor all unit interval order graphs U.\n12\n3.2\nEschers\nLet U be a unit interval graph on N vertices. Recall the notation EU\nλ = {λ-Escher tuples in U},\nand XU = P\nλ cU\nλ eλ is the Stanley chromatic function. Let us work out the coefficient cU\nN\nfirst.\nDefinition 3.6. Let U be a UIO. We call a sequence w = [w1, . . . , wk] of distinct elements\nof U correct if\n• wi ⊁wi+1 for i = 1, 2, . . . , k −1 and\n• for each j = 2, . . . , k, there exists i < j such that wi ∼wj.\nWe denote by CU\nk the set of length-k correct sequences in U. For a partition λ = (λ1, . . . , λr)\nwe denote by CU\nλ the set of length λ1 + . . . + λr-sequences in U, which are concatenations\nof a length-λ1-correct sequence with a length-λ2-correct sequence with a ... with a length-\nλr-correct sequence.\nTheorem 3.7 (Szenes-Rok, [11]). For any k ≤N\npU\nk =\nX\nw∈CU\nk\nw1 · ... · wk ∈N[U]\nFurthermore for any partition λ = (λ1, . . . , λr) we have\npU\nλ =\nr\nY\ni=1\npU\nλi =\nr\nY\ni=1\nX\nw∈CU\nλi\nw1 · . . . · wλi =\nX\nw∈CU\nλ\nw1 · w1 · . . . · wN\n(3)\nThis leads to\nProposition 3.8. cU\nN = #CU\nN ≥0\nProof. Using the identity pN =\n∞\nP\ni=1\nxN\ni = mN we apply the G-homomorphism to get mU\nN =\npU\nN and thus cN = [mU\nN, v1 · v2 · ... · vN] = #CU\nN ≥0 by 3.7.\n■\nTheorem 3.9 (Szenes-Rok, [11]). Let U be a UIO on N intervals. Then #EU\nk = #CU\nk\nfor any k ≤N and hence by Proposition 3.8\ncU\nN = #EU\nN\n13\n3.3\nEscher-pairs\nLet λ = (n, k) be a fixed pair with n+k ≤N. Apply the G-homomorphism to the identity\nm(n,k) = pnpk −pn+k\nin Λ and we get\nmU\n(n,k) = pU\nn pU\nk −pU\nn+k\nUsing Theorem 3.9 we get\ncU\n(n,k) = #EU\nn · #EU\nk −#EU\nn+k\n(4)\nIn fact, it is not hard to see that we can work with disjoint Escher pairs, hence we arrive\nat\nTheorem 3.10 (Stanley coeff formula for pairs).\ncU\n(n,k) = #EU\nn,k −#EU\nn+k\nTo prove non-negativity of cU\n(n,k), Szenes and Rok [11] construct an injective function\nϕ : EU\nn+k →EU\n(n,k)\nWe go through the key steps of the proof to show the intuition behind our core represen-\ntations τk. The idea is to find two functions\nϕ : EU\nn+k →EU\n(n,k),\nψ : EU\nn,k →EU\nn+k such that ψ ◦ϕ = id\nLet w ∈EU\nn+k. By Proposition 2.6, w has a first splitting point L = SEStart(w, k), so\nthat v = [wL+1, ..., wL+k] and u = [wL+k+1, ..., wL+k+n] are k and n Eschers respectively.\nIt’s natural to define ϕ as w 7→(u, v), but the problem is that it is not always the case that\n(u, v) has an insertion point which we could use to concatenate u and v to define ψ. But it\nturns out that there are cyclic permutations u′ ∈[u], v′ ∈[v] s.t. (u′, v′) has an insertion\npoint. Namely, pick u′ ∈[u] s.t. u′\nL = un−1 and v′ ∈[v] s.t. v′\nL = vk−1, i.e.\nu′\ni = ui+n−L−1 and v′\ni = vi+k−L−1\n(5)\nDefine\nϕ : EU\nn+k →EU\nn,k,\nϕ(w) = (u′, v′)\n. Then we prove that\nLemma 3.11. For any w ∈EU\nn+k the pair (u′, v′) = ϕ(w) has an insertion point at\nL = SEStart(w, k). Furthermore u′ +L v′ ∈[w]\n14\nProof. u′ and v′ can be concatenated at L since\nu′\nL = un−1 = wL+k+n = wL →wL+1 = v0 = v′\nL+1,\nv′\nL = vk−1 = wL+k →wL+k+1 = u0 = u′\nL+1\nand u′ +L v′ is the same cycle-type as w since\nu′ +L v′ = [u′\nq, ..., u′\nL, v′\nL+1, ..., v′\nL+k, u′\nL+1, ..., u′\nq−1]\n= [uq−L−1, ..., un−1, v0, ..., vk−1, u0, ..., uq−L−2]\n∼[v0, ..., vk−1, u0, ..., un−1] = [wL+1, ..., wL+k, wL+k+1, ..., wL+k+n] ∈[w]\n■\nNow let’s try to construct ψ, we already know u′ +L v′ ∼w, so for them to be equal,\nwe only need their starting point to be the same. The first element of u′ +L v′ will by\ndefinition always be from u′, i.e. from u, but depending on where the k-subescher v starts\nin w, it could be that w0 ∈v. So if we want to recover w from u′ and v′ we need to change\nthe starting point of u′ +L v′ in one of the 2 cases.\nTo find the criterion to decide in what case we are, note that ui = wL+k+1+i for\n0 ≤i < n and vi = wL+1+i for 0 ≤i < k. So w0 ∈u can only happen if un−L−1 = w0,\nso the criterion is n −L −1 ∈[0, n −1]\n⇐⇒\n0 ≤L ≤n −1.\nSimilarly we get\nw0 ∈v ⇐⇒vn+k−L−1 = w0 ⇐⇒n ≤L < n + k.\nIf L < n we get by (5) that\n(u′ +L v′, L)0 = u′\n0 = uL−n−1 = w0 ⇒u′ +L v′ = w\nIf L ≥n we get by (5) that\n(u′ +L v′)v′\nn\n0 = v′\nn = vn+k−L−1 = w0 ⇒(u′ +L v′)v′\nn = w\nOur discussion can be summarized in\nLemma 3.12. Let w ∈En+k,(u, v) = ϕ(w) and L = FS(w) then\nw =\n(\nu +L v\nif L < n\n(u +L v)vn\nif L ≥n\nThis inspires our definition of ψ :\nDefinition 3.13. Fix some dummy w0 ∈En+k, define ψ : En,k →En+k as\nψ((u, v)) =\n\n\n\n\n\nw0\nif (u, v) has no insertion point\nu +FI(u,v) v\nif FI(u, v) < n\n(u +FI(u,v) v)vn\nif FI(u, v) ≥n\n15\nTheorem 3.14. ψ ◦ϕ is the identity\nWe have the following characterization of En,k \\ im(ϕ):\nCorollary 3.15. The complement of the image of ϕ can be seen as\nim(ϕ)C = {(u, v) ∈En,k | (u, v) has no insertion point or ϕ ◦ψ((u, v)) ̸= (u, v)}\nProof. Let (u, v) ∈im(ϕ), pick w ∈En+k s.t. ϕ(w) = (u, v) then ϕ ◦ψ(u, v) = ϕ(w) =\n(u, v). On the other hand ϕ ◦ψ((u, v)) = (u, v) ∈im(ϕ).\n■\n3.4\nEscher-tuples\nLet λ = (λ1, . . . , λr) a partition. Using Stanley’s G-homomorphism, we can come up with\na similar formula for cU\nλ using Escher-tuples as in to that in Theorem 3.10. Indeed, the\nsymmetric polynomial m(λ1,...,λr) ∈Λ can be uniquely written in the total sum basis as\nm(λ1,...,λr) =\nX\nτ\ndλ\nτ pτ\nwith integer coefficients dλ\nτ . Applying the G-homomorphism we obtain\nmU\n(λ1,...,λr) =\nX\nτ\ndλ\nτ pU\nτ\nand by Theorem 3.9 again, we get\nTheorem 3.16 (Stanley coeff formula).\ncU\nλ =\nX\nτ\ndλ\nτ · #EU\nτ .\nWe state this formula for triple and quadruple partitions as corollary, for reference.\nCorollary 3.17 (Stanley triple coeff formula).\ncU\n(n,k,l) = #EU\n(n,k,l) + 2 · #EU\n(n+k+l) −#EU\n(n+k,l) −#EU\n(n+l,k) −#EU\n(k+l,n)\nUsing the pair formula in Theorem 3.10 we can rewrite this as\ncU\n(n,k,l) = #EU\n(n,k,l) + #EU\n(n+k+l) −cU\n(n+k,l) −cU\n(n+l,k) −cU\n(k+l,n)\nCorollary 3.18 (Stanley quadruple coeff formula).\ncU\n(n,k,l,m) =#EU\n(n,k,l,m)−\n−#EU\n(n+k,l,m) −#EU\n(n+l,k,m) −#EU\n(n+m,k,l) −#EU\n(k+l,n,m) −#EU\n(k+m,n,l) −#EU\n(l+m,n,k)+\n+ #EU\n(n+k,l+m) + #EU\n(n+l,k+m) + #EU\n(n+m,k+l)+\n+ 2 ∗(#EU\n(n+k+l,m) + #EU\n(n+k+m,l) + #EU\n(n+l+m,k) + #EU\n(k+l+m,n)−\n−6 ∗#EU\n(n,k,l,m)\n16\n4\nMachine Learning\nThe message of Theorem 3.16 is that the Stanley coefficient cλ should count λ-Escher tuples\nsatisfying some specific splitting and concatenation properties. Following the conventions\nand notations of §2, for any partition λ = (λ1, . . . , λr) we define the core representation\nτλ : EU\nλ →Qξ(λ)\nwhich roughly sends a λ-Escher-tuple u = (u1, . . . ur) to its core vector of dimension ξ(λ),\ncollecting SEStart(v, w), SEEnd(v, w) and FirstIns(v, w) points for all possible v, w pairs\nwhich come from concatenation of Eschers in the tuple. Coordinates of the core vector are\ncalled cores, and the number ξ(λ) = ξ(|λ|) will depend only on the length of λ.\nAs we have seen in §2, the delicate issue is how to define those coordinates of the\nrepresentation where there is no concatenation point.\nFor example, how do we define\nSEStart(uiuj, uk), SEEnd(uiuj, uk), and FirstIns(uiuj, uk) when I(ui, uj) = ∅, and\ntherefore we can not concatenate ui with uj?\nτ r : EU\n(λ1,...,λr) →Qξ(λ)\nτ r(u1, . . . , ur) = (0, τ 2(ui, uj)[1 :], ˜τ 2(uiuj, uk))\nwhere 1 ≤i < j < k ≤r and we do not add further core coordinates, corresponding to\nlonger concatenations, because (according to our experiments) they do not result in better\ncondition graphs. For the definition of τ 2(ui, uj)[1 :] and ˜τ 2(uiuj, uk) see (2).\nWe call a permutation of the ξ(λ) entries of the core vector a sorting permutation, if it\nmoves the entries to descending order. The sorting permutation is not unique in case there\nare repeated entries. For a permutation τ ∈Sξ(λ) we denote by EU\nλ (τ) the set of λ-tuples\nwhose sorting permutation is τ.\nFollowing extensive experiments with Gurobi Software [2] involving Escher tuples,\ntriples and quadruples, we arrived to the following surprising conjecture, which says that\nthe Stanley coefficients are determined by the sorting orders, not the actual value of the\ncore coordinates.\nConjecture 4.1. For any partition λ there is a set of sorting orders\nTλ = {τ1, . . . , τN(λ))},\nindependent of U, such that\ncU\nλ = #(EU\nλ (τ1) ∪. . . ∪EU\nλ (τN)).\nThat is, the Stanley coefficient cλ counts the number of λ-Escher tuples with sorting core\norder in Tλ. We say that Tλ characterises the Stanley coefficient cλ\n17\nWhile attempting to describe the Stanley coefficients through the use of characterizing\nsets, several significant challenges arose, which we can summarise as follows.\n1. Exponential Growth of Characterizing Sets: The size N(λ) of the character-\nizing set increases drastically, even for small cases like triples. This rapid growth\nmakes the analysis computationally difficult and inefficient.\n2. Dependence on Partition Structure: The characterizing set is not solely de-\ntermined by the length |λ| of the partition but depends intricately on the specific\nstructure of the partition λ. This complicates generalization and creates additional\ncomplexity when dealing with different partitions of the same length.\n3. Subset-Sum Problem and Decision Tree Complexity: Identifying the appro-\npriate characterizing sets can be viewed as a specialized instance of the subset-sum\nproblem in combinatorics, which is known to be NP-hard. Attempts to represent\nthese sets using decision trees have resulted in extremely complicated and large struc-\ntures. These trees are not only cumbersome to compute but also impractical to use\nfor further theoretical analysis.\nDue to these challenges, we were unable to find a workable description of characterizing\nsets even for partitions of length 3, which highlights the difficulty in scaling this approach\nfor more general cases.\nOur strategy, instead, was to describe Tλ as a set of partitions τ ∈Sl(λ) satisfying a\nBoolean expressions of the form\nCondition1 OR Condition2 OR . . . OR Conditionr\nwhere each condition has the form\nCondition = τ(i1) < τ(j1) AND . . . AND τ(is) < τ(js)\nWe encoded Conditioni as a graph Gi\nλ on the vertices {1, . . . , ξ(λ)}, with a directed edge\nfrom it to jt for t = 1, . . . , r, and call\nGλ = G1\nλ ∪. . . ∪Gr\nλ\na condition graph. Due to the coding practice, we often refer to the number of Conditions\nas the number of rows of our condition graph.\n4.1\nThe RL algorithm\nOur policy-based reinforcement learning model is an adapted version of the cross-entropy\nmethod for graphs, as developed by Adam Zsolt Wagner [12].\nThe deep NN learns a\nprobability distribution that guides the agent’s decisions during graph construction.\n18\nThe RL agent learns to identify optimal condition graphs. Specifically, after fixing the\nnumber of rows r, we initialized r copies G1, . . . , Gr of the empty graph on ξ(λ) vertices.\nThe edge set of each graph Gi is ordered as ei\n1, . . . , ei\nm, where m =\n\u0000ξ(λ)\n2\n\u0001\n, and we defined\na sequential edge ordering across the r graphs as follows:\ne1\n1, e2\n1, . . . , er\n1, e1\n2, e2\n2, . . . , er\n2, . . . , er\nm.\nAt each step, the RL agent processes this ordered list of edges and adds the corresponding\nedge to the current graph Gλ based on a policy learned by a deep neural network (NN).\nEach run consists of r ·\n\u0000ξ(λ)\n2\n\u0001\nsteps, where the agent decides whether or not to add each\nedge.\nThe neural network takes two binary vectors as input:\n1. Game Turn Vector: A binary vector that tracks the number of moves made in the\ncurrent game, with all entries set to 0 except the one corresponding to the current\nturn set to 1.\n2. Graph State Vector: A binary vector representing the current state of the graph.\nThe i-th entry is set to 1 if the edge corresponding to that entry was added during\nthe i-th turn, and 0 otherwise.\nThe output of the NN is a probability distribution over {0, 1}. The agent samples from\nthis distribution to make its decision for the current step: if the sampled value is 1, the\ncorresponding edge is added to the graph.\nThe RL agent learns the game by running batches of agents through a series of complete\ngames. After each game, the agents are evaluated based on their performance, and the top-\nperforming individuals–typically the top 10%–are selected to update the neural network.\nIt is important to note that there is a single shared NN used by all agents during training.\nThe input-output pairs (game state at step i, decision at step i) from the top agents are\nused to adjust the NN parameters via the cross-entropy method. This selection mechanism\nencourages the model to replicate the successful behaviors of the best-performing agents.\nAdditionally, a small percentage of the all-time best agents are retained throughout\ntraining and used in future updates. This approach ensures that the NN continues to learn\nfrom historically strong strategies, thereby improving the likelihood of finding optimal\nsolutions.\nWe modified Wagner’s cross-entropy method at the following crucial points:\n1. Multi-Type Edges: While Wagner’s method uses binary edge choices, our graphs\nallow edges to take multiple types. Accordingly, we modify the probability distri-\nbution to output values in {0, . . . , K −1}, where K is the number of possible edge\ntypes. Furthermore we use categorical cross-entropy instead of binary cross-entropy\n19\nfor training and our Graph State Vector uses one-hot encoding for every K consec-\nutive bits to encode a value in {0, . . . , K −1}. Concretely we use K=3 to express\nthat either the source vertex is smaller than the target vertex, or greater equal to the\ntarget vertex or that the relation is irrelevant.\n2. Edge Selection: Not every two vertices have a meaningful relationship, so out of the\nfull r ·m edges we will only pick some and reduce the size of our ordered list of edges.\nOur selection was heavily guided by mathematical intuition and was necessary since\nthe training otherwise would be stuck in local minima when using all edges. Reducing\nthe edges also improves the running time of the algorithm linearly.\n3. State Memorization: To speed up computation, we cache the scores of previously\nencountered graph states.\nThis allows us to reuse scores instead of recomputing\nthem when the same graph configuration is encountered again, significantly reducing\ncomputational overhead in later stages of training.\n5\nImplementation\nThe complete python code implementation can be found at\nhttps://github.com/berczig/PositivityConjectures.\nTo generate the necessary training data for our model, we follow a multi-step process\ninvolving combinatorial generation and brute-force classification. The key steps are out-\nlined below:\nStep 1: Generating UIO graphs We first generate all UIOs of length N, where\nN ≤10. For each UIO, we construct the associated incomparability graph. This graph is\nrepresented as an N × N matrix, where for each pair (i, j), the matrix entry specifies the\nrelation between ui and uj for the UIO (u0, . . . , uN−1). The incomparability graph is used\nas a structural feature for downstream tasks.\nStep 2: Generating λ-Eschers Using a brute-force search approach, we generate all\npossible tuples of UIOs and identify which tuples correspond to Escher structures. The\nprocess systematically checks each tuple to determine whether or not it satisfies the Escher\ncondition. This classification serves as a binary label for the dataset.\nStep 3: Calculating Coefficients cU\nλ For each valid UIO U and partition λ, we\ncalculate the corresponding coefficient cU\nλ using Theorem 3.10 and Corollary 3.17, 3.18.\nThe generated dataset includes coefficients for all tuples (λ, U), where λ = (λ1, . . . , λr) and\nU = (u1, . . . , uN), subject to the constraints:\n2 ≤r ≤4,\nλ1 + · · · + λr ≤N ≤10.\n20\nWith a C++ implementation, we successfully generated a dataset of approximately 100,000\ntuples, enabling efficient training of our model.\nStep 4: Calculating Cores and Core-Types For each UIO U, we compute all\nits core vectors τλ(EU\nλ ), classifying all core vectors into emerging core-types. The dataset\nstores the distinct core-types along with their respective counts. Specifically, we iterate\nover all cores, and when a new core-type is encountered, we store it. If a core-type has\nbeen observed previously, we simply increment the counter associated with that core-type.\nThe number of core-types is significantly smaller than the total number of λ-Escher\ntuples, which enables optimization. When evaluating a condition graph, we iterate over\nthe distinct core-types, summing the contributions of all matching core-types with their\nrespective multiplicities:\ncU\nG =\nX\nw∈EU\nλ\nτ r(w) satisfies G\n1 =\nX\nw∈τ r(EU\nλ )\nw satisfies G\n#cores with core-type w.\n(6)\nBy reducing the evaluation to core-types, we can streamline computation, especially for\nlarge-scale experiments.\nIn our model we calculate all coefficients simultaneously, this is faster since two UIOs\ncan share the same core-type and by batching the computation we only have to check if\nthe core-type satisfies a condition graph once. We collect all distinct core-types\nB =\n[\nUIO U,|U|=n\nτ r(EU\nλ )\nand calculate the coefficient prediction vector (recall Cn stands for the nth Catalan\nnumber, which is the number of length n UIOs)\ncG =\nX\nw∈B\nw satisfies G\n(#cores with core-type w in U1, · · · , #cores with core-type w in UCn).\n(7)\n5.1\nScore Function\nGiven a partition λ we apply the same score function for all UIOs of the same size, i.e.\nhaving the same number of intervals. Let 0 < n denote the number of intervals, and Cn,\nthe n’th Catalan number, which is the number of UIOs of size n. We let x ∈NCn denote\nthe predicted coefficients vector and let y ∈NCn denote the true Stanley coefficients vector.\nThen we define the score function as\nscore1(x) = −||x −y||1 −1(has trivial row) · 10000 −num edges · EdgePenalty\n(8)\n21\nIn the modified version where we search for a lower bound of the Stanley coeffiecient,\nwe set the score to be a large negative number in case there is a UIO with where predicted\ncoeff > true coeff:\nscore2(x) =\n(\n−5000,\nif xi > yi, for any i ≥0\nscore1(x),\nelse\n(9)\nNotice, we have opted for a negative score function because our objective is to maximize\nthe score, with the optimal value set at 0.\n5.2\nHyperparameters\nWagner’s implementation includes several hyperparameters that govern the learning pro-\ncess, such as the learning rate, the number of graphs per batch, and the selection percentage\nof the top-performing individuals, among others. We retained most of these hyperparam-\neters in our experiments. The learning rate was primarily set to either 0.01 or 0.05. After\nconducting numerous experiments, we found that a batch size of 600 graphs yielded com-\nparable results to a larger batch size of 1500 graphs, making it a more efficient choice.\nMoreover, we employed one-hot encoding for graph representations.\nA significant speedup in our approach was achieved by focusing on a random subset\nof the UIOs. If the algorithm fails to find a condition graph for this subset, it is unlikely\nto succeed when evaluated against all UIOs. This strategic reduction not only enhances\ncomputational efficiency but also improves the likelihood of successful evaluations, and\nallowed us to work with longer UIOs.\n5.3\nResults\nThis section presents the results obtained using the RL agent, summarized in tables. Each\ntable provides an overview of the mismatches between the Stanley coefficients and the\ncounts associated with a given condition graph.\nThe mismatches are evaluated in two\ndifferent ways, in individual tables\n1. Tables with number of correctly predicted UIOs: The entry A/B indicates\nthat, out of a total of B UIOs, the condition graph correctly predicts the coefficient\nfor A UIOs.\n2. Tables with total absolute error: The entry A/B means that the total sum of\nthe Stanley coefficients for all UIOs is B, while\nA =\nX\nU\n|predicted coeff for U −real coeff for U|\n22\nEntries highlighted in blue indicate that the predicted coefficient is less than or equal to\nthe actual coefficient for all UIOs of the given length, meaning the condition graph provides\na lower bound for the coefficients. In contrast, entries highlighted in orange indicate the\nopposite: the predicted coefficient is greater or equal to the real for all UIOs. Magenta\nentries indicate mixed results, and non-higlighted entries indicate perfect match.\n23\nTotal absolute error/sum of real coefficients for all UIO\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 2)\n0/12\n0/192\n0/1876\n0/14496\n0/97436\n0/597056\n(3, 1)\n0/30\n0/440\n0/4044\n0/29852\n0/193626\n0/1152912\n(3, 2)\n0/75\n0/1446\n0/16517\n0/145828\n0/1100405\n(4, 1)\n0/259\n0/4590\n0/49193\n0/413056\n0/2992325\n(3, 3)\n0/414\n0/9630\n0/128796\n0/1302006\n(4, 2)\n0/746\n0/16532\n0/213152\n0/2093462\n(5, 1)\n0/2820\n0/58706\n0/719904\n0/6784258\n(4, 3)\n0/3903\n0/103326\n0/1550199\n(5, 2)\n0/9595\n0/240366\n0/3450237\n(6, 1)\n0/36639\n0/877158\n0/12124473\n(4, 4)\n0/32008\n0/971824\n(5, 3)\n1/52560\n28/1541010\n(6, 2)\n0/146100\n0/4093944\n(7, 1)\n0/550914\n0/14917146\n(5, 4)\n6/397195\n(6, 3)\n0/877977\n(7, 2)\n0/2534175\n(8, 1)\n0/9395415\nNumber of correctly predicted/number of all UIOs\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 2)\n14/14\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 1)\n14/14\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 2)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(4, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 3)\n132/132\n429/429\n1430/1430\n4862/4862\n(4, 2)\n132/132\n429/429\n1430/1430\n4862/4862\n(5, 1)\n132/132\n429/429\n1430/1430\n4862/4862\n(4, 3)\n429/429\n1430/1430\n4862/4862\n(5, 2)\n429/429\n1430/1430\n4862/4862\n(6, 1)\n429/429\n1430/1430\n4862/4862\n(4, 4)\n1430/1430\n4862/4862\n(5, 3)\n1429/1430\n4842/4862\n(6, 2)\n1430/1430\n4862/4862\n(7, 1)\n1430/1430\n4862/4862\n(5, 4)\n4858/4862\n(6, 3)\n4862/4862\n(7, 2)\n4862/4862\n(8, 1)\n4862/4862\nTable 1: Mismatch statistics for length 2 partitions for the model [SEEnd < FirstIns],\ncorresponding to the graph at bottom. Green entries indicate that this model gives lower\nbound for the Stanley coefficients even when mismatches occur.\n24\nTotal absolute error/sum of real coefficients for all UIO\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 1, 1)\n0/16\n0/256\n0/2552\n0/20304\n0/141088\n0/895072\n(2, 2, 1)\n0/42\n0/860\n0/10454\n0/98088\n0/784470\n(3, 1, 1)\n0/106\n0/2052\n0/23846\n0/215620\n0/1671830\n(2, 2, 2)\n0/108\n0/2712\n0/39216\n0/427572\n(3, 2, 1)\n5/286\n100/6872\n1184/95536\n10812/1006152\n(4, 1, 1)\n0/1024\n0/23148\n0/306112\n0/3091580\n(3, 2, 2)\n0/742\n0/21372\n0/347850\n(3, 3, 1)\n16/1824\n400/50940\n5736/805620\n(4, 2, 1)\n68/3034\n1503/82612\n19623/1279950\n(5, 1, 1)\n0/12472\n0/322284\n0/4778064\n(3, 3, 2)\n54/4776\n1560/156156\n(4, 2, 2)\n0/8344\n0/267792\n(4, 3, 1)\n724/18758\n20117/587564\n(5, 2, 1)\n1083/41706\n25703/1265520\n(6, 1, 1)\n0/180144\n0/5237856\n(3, 3, 3)\n0/28620\n(4, 3, 2)\n1173/51836\n(4, 4, 1)\n3236/177434\n(5, 2, 2)\n0/124470\n(5, 3, 1)\n12102/265838\n(6, 2, 1)\n17809/681792\n(7, 1, 1)\n0/2987556\nNumber of correctly predicted/number of all UIOs\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 1, 1)\n14/14\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 1, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 2)\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 2, 1)\n129/132\n391/429\n1157/1430\n3359/4862\n(4, 1, 1)\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 2, 2)\n429/429\n1430/1430\n4862/4862\n(3, 3, 1)\n425/429\n1369/1430\n4344/4862\n(4, 2, 1)\n404/429\n1202/1430\n3493/4862\n(5, 1, 1)\n429/429\n1430/1430\n4862/4862\n(3, 3, 2)\n1416/1430\n4651/4862\n(4, 2, 2)\n1430/1430\n4862/4862\n(4, 3, 1)\n1309/1430\n3824/4862\n(5, 2, 1)\n1268/1430\n3662/4862\n(6, 1, 1)\n1430/1430\n4862/4862\n(3, 3, 3)\n4862/4862\n(4, 3, 2)\n4593/4862\n(4, 4, 1)\n4625/4862\n(5, 2, 2)\n4862/4862\n(5, 3, 1)\n4144/4862\n(6, 2, 1)\n4068/4862\n(7, 1, 1)\n4862/4862\nTable 2: Triple partitions: performance of the model [SEEnd(u,v) < FirstIns(u,v)] AND\n[SEEnd(v,w) < FirstIns(v,w)] AND [SEEnd(u,w) < FirstIns(u,w)].\n25\nTotal absolute error/sum of real coefficients for all UIO\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 1, 1)\n4/16\n60/256\n564/2552\n4252/20304\n28116/141088\n170364/895072\n(2, 2, 1)\n10/42\n196/860\n2278/10454\n20456/98088\n156854/784470\n(3, 1, 1)\n52/106\n960/2052\n10676/23846\n92656/215620\n691364/1671830\n(2, 2, 2)\n0/108\n0/2712\n0/39216\n0/427572\n(3, 2, 1)\n178/286\n4100/6872\n54796/95536\n556260/1006152\n(4, 1, 1)\n664/1024\n14508/23148\n185808/306112\n1820580/3091580\n(3, 2, 2)\n454/742\n12468/21372\n194370/347850\n(3, 3, 1)\n834/1824\n22572/50940\n345780/805620\n(4, 2, 1)\n2026/3034\n53292/82612\n799206/1279950\n(5, 1, 1)\n9322/12472\n234984/322284\n3402364/4778064\n(3, 3, 2)\n2796/4776\n87360/156156\n(4, 2, 2)\n4792/8344\n147632/267792\n(4, 3, 1)\n15518/18758\n475244/587564\n(5, 2, 1)\n35406/41706\n1052460/1265520\n(6, 1, 1)\n146124/180144\n4170312/5237856\n(3, 3, 3)\n0/28620\n(4, 3, 2)\n42764/51836\n(4, 4, 1)\n106218/177434\n(5, 2, 2)\n107670/124470\n(5, 3, 1)\n237488/265838\n(6, 2, 1)\n584256/681792\n(7, 1, 1)\n2550966/2987556\nNumber of correctly predicted/number of all UIOs\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 1, 1)\n12/14\n27/42\n58/132\n121/429\n248/1430\n503/4862\n(2, 2, 1)\n39/42\n101/132\n242/429\n545/1430\n1174/4862\n(3, 1, 1)\n33/42\n71/132\n152/429\n321/1430\n673/4862\n(2, 2, 2)\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 2, 1)\n93/132\n192/429\n375/1430\n715/4862\n(4, 1, 1)\n96/132\n212/429\n473/1430\n1049/4862\n(3, 2, 2)\n398/429\n1135/1430\n3054/4862\n(3, 3, 1)\n381/429\n1022/1430\n2562/4862\n(4, 2, 1)\n261/429\n518/1430\n1028/4862\n(5, 1, 1)\n288/429\n671/1430\n1604/4862\n(3, 3, 2)\n1334/1430\n3902/4862\n(4, 2, 2)\n1276/1430\n3457/4862\n(4, 3, 1)\n956/1430\n2131/4862\n(5, 2, 1)\n794/1430\n1634/4862\n(6, 1, 1)\n895/1430\n2210/4862\n(3, 3, 3)\n4862/4862\n(4, 3, 2)\n3948/4862\n(4, 4, 1)\n4122/4862\n(5, 2, 2)\n4160/4862\n(5, 3, 1)\n2839/4862\n(6, 2, 1)\n2556/4862\n(7, 1, 1)\n2894/4862\nTable 3: Triple partitions with the model [0 ≥SEStart(u,v)] AND [0 ≥SEStart(u,w)] AND\n[0 ≥SEStart(v,w)] AND [SEEnd(u,v)<FirstIns(u,v)] AND [SEEnd(v,w)<FirstIns(v,w)]\nAND [SEEnd(u,w)<FirstIns(u,w)], giving lower bound for the Stanley coefficients\n26\nTotal absolute error/sum of real coefficients for all UIO\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(1, 1, 1, 1)\n0/24\n0/384\n0/3864\n0/31224\n0/221256\n0/1435056\n(2, 1, 1, 1)\n0/66\n0/1332\n0/16158\n0/152556\n0/1234590\n(2, 2, 1, 1)\n0/180\n0/4452\n0/63976\n0/698004\n(3, 1, 1, 1)\n0/456\n0/10800\n0/149676\n0/1583436\n(2, 2, 2, 1)\n0/486\n0/14316\n0/239166\n(3, 2, 1, 1)\n28/1272\n702/36148\n10150/585036\n(4, 1, 1, 1)\n0/4608\n0/124716\n0/1936776\n(2, 2, 2, 2)\n0/1296\n0/44352\n(3, 2, 2, 1)\n72/3476\n2120/115596\n(3, 3, 1, 1)\n144/8648\n4280/279956\n(4, 2, 1, 1)\n422/13804\n11560/439024\n(5, 1, 1, 1)\n0/58728\n0/1787232\n(3, 2, 2, 2)\n0/9306\n(3, 3, 2, 1)\n732/23864\n(4, 2, 2, 1)\n1058/39650\n(4, 3, 1, 1)\n5028/92456\n(5, 2, 1, 1)\n7176/193420\n(6, 1, 1, 1)\n0/886716\nNumber of correctly predicted/number of all UIOs\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(1, 1, 1, 1)\n14/14\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 1, 1, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 1, 1)\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 1, 1, 1)\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 2, 1)\n429/429\n1430/1430\n4862/4862\n(3, 2, 1, 1)\n420/429\n1306/1430\n3920/4862\n(4, 1, 1, 1)\n429/429\n1430/1430\n4862/4862\n(2, 2, 2, 2)\n1430/1430\n4862/4862\n(3, 2, 2, 1)\n1415/1430\n4620/4862\n(3, 3, 1, 1)\n1414/1430\n4610/4862\n(4, 2, 1, 1)\n1347/1430\n4054/4862\n(5, 1, 1, 1)\n1430/1430\n4862/4862\n(3, 2, 2, 2)\n4862/4862\n(3, 3, 2, 1)\n4762/4862\n(4, 2, 2, 1)\n4730/4862\n(4, 3, 1, 1)\n4388/4862\n(5, 2, 1, 1)\n4290/4862\n(6, 1, 1, 1)\n4862/4862\nTable 4: Quadruple partition with the canonical model\n27\nTotal absolute error/sum of real coefficients for all UIO\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(1, 1, 1, 1)\n0/24\n0/384\n0/3864\n0/31224\n0/221256\n0/1435056\n(2, 1, 1, 1)\n0/66\n0/1332\n0/16158\n0/152556\n0/1234590\n(2, 2, 1, 1)\n32/180\n768/4452\n10716/63976\n113612/698004\n(3, 1, 1, 1)\n0/456\n0/10800\n0/149676\n0/1583436\n(2, 2, 2, 1)\n84/486\n2424/14316\n39604/239166\n(3, 2, 1, 1)\n224/1272\n6196/36148\n97608/585036\n(4, 1, 1, 1)\n0/4608\n0/124716\n0/1936776\n(2, 2, 2, 2)\n0/1296\n0/44352\n(3, 2, 2, 1)\n600/3476\n19534/115596\n(3, 3, 1, 1)\n3086/8648\n97728/279956\n(4, 2, 1, 1)\n2480/13804\n76656/439024\n(5, 1, 1, 1)\n0/58728\n0/1787232\n(3, 2, 2, 2)\n0/9306\n(3, 3, 2, 1)\n11384/23864\n(4, 2, 2, 1)\n7074/39650\n(4, 3, 1, 1)\n32014/92456\n(5, 2, 1, 1)\n35868/193420\n(6, 1, 1, 1)\n0/886716\nNumber of correctly predicted/number of all UIOs\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(1, 1, 1, 1)\n14/14\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 1, 1, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 1, 1)\n123/132\n338/429\n882/1430\n2232/4862\n(3, 1, 1, 1)\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 2, 1)\n421/429\n1314/1430\n3934/4862\n(3, 2, 1, 1)\n393/429\n1100/1430\n2927/4862\n(4, 1, 1, 1)\n429/429\n1430/1430\n4862/4862\n(2, 2, 2, 2)\n1430/1430\n4862/4862\n(3, 2, 2, 1)\n1371/1430\n4172/4862\n(3, 3, 1, 1)\n1282/1430\n3497/4862\n(4, 2, 1, 1)\n1238/1430\n3308/4862\n(5, 1, 1, 1)\n1430/1430\n4862/4862\n(3, 2, 2, 2)\n4862/4862\n(3, 3, 2, 1)\n4287/4862\n(4, 2, 2, 1)\n4531/4862\n(4, 3, 1, 1)\n4150/4862\n(5, 2, 1, 1)\n3955/4862\n(6, 1, 1, 1)\n4862/4862\nTable 5: Quadruple partitions with the modified canonincal model, giving lower bound\nfor the Stanley coefficients. Also the best conditions when allowing up to 6 additional\nconditions while only using the uv,uw,uz,vw,vz,wz subescher for the [0 ≥subescher start]\nconditions\n28\nTotal absolute error/sum of real coefficients for all UIO\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 1, 1)\n0/16\n0/256\n0/2552\n0/20304\n0/141088\n0/895072\n(2, 2, 1)\n0/42\n0/860\n0/10454\n0/98088\n0/784470\n(3, 1, 1)\n0/106\n0/2052\n0/23846\n0/215620\n0/1671830\n(2, 2, 2)\n0/108\n0/2712\n0/39216\n0/427572\n(3, 2, 1)\n4/286\n80/6872\n948/95536\n8668/1006152\n(4, 1, 1)\n0/1024\n0/23148\n0/306112\n0/3091580\n(3, 2, 2)\n0/742\n0/21372\n0/347850\n(3, 3, 1)\n16/1824\n400/50940\n5736/805620\n(4, 2, 1)\n48/3034\n963/82612\n11736/1279950\n(5, 1, 1)\n0/12472\n0/322284\n0/4778064\n(3, 3, 2)\n90/4776\n2616/156156\n(4, 2, 2)\n0/8344\n0/267792\n(4, 3, 1)\n644/18758\n17877/587564\n(5, 2, 1)\n746/41706\n15840/1265520\n(6, 1, 1)\n0/180144\n0/5237856\n(3, 3, 3)\n0/28620\n(4, 3, 2)\n1380/51836\n(4, 4, 1)\n3236/177434\n(5, 2, 2)\n0/124470\n(5, 3, 1)\n9725/265838\n(6, 2, 1)\n13066/681792\n(7, 1, 1)\n0/2987556\nNumber of correctly predicted/number of all UIOs\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(2, 1, 1)\n14/14\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 1, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 2)\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 2, 1)\n130/132\n401/429\n1209/1430\n3566/4862\n(4, 1, 1)\n132/132\n429/429\n1430/1430\n4862/4862\n(3, 2, 2)\n429/429\n1430/1430\n4862/4862\n(3, 3, 1)\n425/429\n1369/1430\n4344/4862\n(4, 2, 1)\n411/429\n1248/1430\n3679/4862\n(5, 1, 1)\n429/429\n1430/1430\n4862/4862\n(3, 3, 2)\n1406/1430\n4526/4862\n(4, 2, 2)\n1430/1430\n4862/4862\n(4, 3, 1)\n1317/1430\n3886/4862\n(5, 2, 1)\n1306/1430\n3848/4862\n(6, 1, 1)\n1430/1430\n4862/4862\n(3, 3, 3)\n4862/4862\n(4, 3, 2)\n4588/4862\n(4, 4, 1)\n4625/4862\n(5, 2, 2)\n4862/4862\n(5, 3, 1)\n4193/4862\n(6, 2, 1)\n4164/4862\n(7, 1, 1)\n4862/4862\nTable 6: A model with 3 condition graphs meaning the Boolean expression [graph1 OR\ngraph2 OR graph3]. It is trained on (5, 2, 1) partition with 9 intervals, and shows that the\nRL agent often finds better conditions for individual partitions.\n29\nTotal absolute error/sum of real coefficients for all UIO\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(1, 1, 1, 1)\n0/24\n0/384\n0/3864\n0/31224\n0/221256\n0/1435056\n(2, 1, 1, 1)\n0/66\n0/1332\n0/16158\n0/152556\n0/1234590\n(2, 2, 1, 1)\n32/180\n768/4452\n10716/63976\n113612/698004\n(3, 1, 1, 1)\n0/456\n0/10800\n0/149676\n0/1583436\n(2, 2, 2, 1)\n84/486\n2424/14316\n39604/239166\n(3, 2, 1, 1)\n224/1272\n6196/36148\n97608/585036\n(4, 1, 1, 1)\n0/4608\n0/124716\n0/1936776\n(2, 2, 2, 2)\n0/1296\n0/44352\n(3, 2, 2, 1)\n600/3476\n19534/115596\n(3, 3, 1, 1)\n3086/8648\n97728/279956\n(4, 2, 1, 1)\n2480/13804\n76656/439024\n(5, 1, 1, 1)\n0/58728\n0/1787232\n(3, 2, 2, 2)\n0/9306\n(3, 3, 2, 1)\n11384/23864\n(4, 2, 2, 1)\n7074/39650\n(4, 3, 1, 1)\n32014/92456\n(5, 2, 1, 1)\n35868/193420\n(6, 1, 1, 1)\n0/886716\nNumber of correctly predicted/number of all UIOs\n``````````````\nPartition\nUIO length\n4\n5\n6\n7\n8\n9\n(1, 1, 1, 1)\n14/14\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 1, 1, 1)\n42/42\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 1, 1)\n123/132\n338/429\n882/1430\n2232/4862\n(3, 1, 1, 1)\n132/132\n429/429\n1430/1430\n4862/4862\n(2, 2, 2, 1)\n421/429\n1314/1430\n3934/4862\n(3, 2, 1, 1)\n393/429\n1100/1430\n2927/4862\n(4, 1, 1, 1)\n429/429\n1430/1430\n4862/4862\n(2, 2, 2, 2)\n1430/1430\n4862/4862\n(3, 2, 2, 1)\n1371/1430\n4172/4862\n(3, 3, 1, 1)\n1282/1430\n3497/4862\n(4, 2, 1, 1)\n1238/1430\n3308/4862\n(5, 1, 1, 1)\n1430/1430\n4862/4862\n(3, 2, 2, 2)\n4862/4862\n(3, 3, 2, 1)\n4287/4862\n(4, 2, 2, 1)\n4531/4862\n(4, 3, 1, 1)\n4150/4862\n(5, 2, 1, 1)\n3955/4862\n(6, 1, 1, 1)\n4862/4862\nTable 7: A model found by the RL agent for quadruple partitions with only negative errors,\nand with better performance than the model of Theorem 3. This shows that graphs which\ngive lower bound for the Stanley coefficients are not unique, but finding one with general\npattern is harder.\n30\n6\nAnalysis of the Machine Learning Method\nThe machine learning approach we employed to tackle this problem presented several no-\ntable challenges. Below, we outline key observations and strategies that emerged during\nthe process, along with the implications for optimizing the training process.\n6.1\nNon-Uniqueness of Solutions and Balancing Complexity\nFor most partition types, there is no single best solution. Even when a perfect solution (i.e.,\na solution that achieves a zero-error score) is found, the corresponding condition graph is\noften not unique. Table 2 and 6 illustrate this for the case of the partition (a, b, b). This\nobservation indicates the need to balance between obtaining optimal solutions with po-\ntentially highly complex condition graphs and accepting slightly suboptimal solutions that\ncorrespond to significantly simpler, more interpretable, and potentially more universal con-\ndition graphs. This trade-off between accuracy and simplicity is critical to the applicability\nand generalization of our model.\n6.2\nManaging the Comparison Space\nAs the size of the partition grows, the comparison space becomes exponentially large, lead-\ning to impractical memory requirements during training. For example, training the model\nfor partitions of length greater than five quickly exhausts available resources. To mitigate\nthis, we must reduce the comparison space, which necessitates leveraging mathematical\nintuition. By identifying symmetries and reducing redundant comparisons, we can make\nthe search space more tractable while still preserving solution quality.\n6.3\nTraining Pitfalls in Large Search Spaces\nOne of the common pitfalls in training with a large comparison space is the lack of satisfying\nsolutions for random Boolean expressions. In many cases, none of the Escher tuples meet\nthe given conditions after the initial random iteration, causing the model to become trapped\nin a suboptimal region where the best achievable score is merely the sum of the nonzero\nStanley coefficients across all UIOs. To avoid this, we must guide the training process\nto prevent such stagnation, ensuring that the model does not get trapped in these non-\nproductive domains.\n6.4\nReducing Cores and Comparisons\nAn essential aspect of optimizing the model involves reducing the number of cores and\ncomparisons. For example, in the case of n = 3, the optimal solution only utilizes specific\ncores (e.g., SEEnd and FirstIns), while others, such as 0 and SEStart, are not used.\nThis insight suggests that for n = 4, we can eliminate the SEStart cores and restrict\n31\ncomparisons to pairs of cores such as (0, SEStart) and (SEEnd, FirstIns). This reduction\nin action space helps prevent the model from being stuck in local minima during training,\nsignificantly improving efficiency.\n6.5\nCore Selection and Encoding Decision Trees\nThe process of selecting the appropriate cores is critical to model performance. Encod-\ning decision trees with graphs is a complex task, and it took several months to iden-\ntify the correct core vectors.\nSpecifically, interpreting the meaning of expressions like\nSEEnd(u, v) < FirstIns(u, v) requires a deep understanding of multiple aspects of the\ngraph’s structure. These core vectors play a crucial role in ensuring that the model cap-\ntures the right relationships between the nodes and edges of the decision tree.\n6.6\nCondition Graph Row Selection\nAnother critical factor in the model’s design is the number of rows used in the condition\ngraph. In theory, adding a second row expands the search space to include solutions that\nrequire only one row. However, in practice, increasing the number of rows dramatically\nincreases the state space, which, in turn, heightens the risk of the model becoming stuck\nin good-but-not-optimal solutions. Through experimentation, we found that solutions us-\ning 1-3 rows often outperformed those with more rows, even though a larger state space\ntheoretically allows for more complex solutions.\n6.7\nImportance of the Score Function\nThe choice of score function is pivotal in guiding the reinforcement learning (RL) agent\ntoward optimal solutions. In particular, the role of the edge penalty is crucial. We ob-\nserved that when EdgePenalty > 1, the RL agent tends to prioritize shorter solutions,\ni.e., smaller condition graphs, at the expense of better absolute error. Through extensive\nexperimentation, we determined that the optimal range for EdgePenalty lies between 0.1\nand 0.5. This range strikes a balance between solution length and accuracy, leading to\nmore efficient exploration of the solution space.\n6.8\nTraining charts\nWe added a ResultViewer folder to the GitHub repository, where the reinforcement training\nprocess is monitored and saved as charts. An example is shown in Figure 4.\n32\nFigure 4: Training charts for the partition (5, 2, 1) trained on all UIOs of length 8, with 3\nrows in the condition graph. The number of correctly predicted coefficients is 3848/4862.\nBecause EdgePenalty¡1, the model occasionally adds edges to the condition graph to achieve\na lower total score, resulting in jumps in the number of edges.\n7\nUsing different ML models\nA promising machine learning approach to determining Stanley coefficients is to apply a\nsupervised learning model. This idea is attractive for several reasons:\n1. The input data consists of a sequence of n unit intervals on a line, and the corre-\nsponding unit interval graph can be encoded as an increasing sequence of n integers\nbetween 0 and n −1, also called the area sequence.\n2. The output is a series of integer coefficients. With the right model, saliency analy-\nsis can be performed to identify which features of the Unit Interval Orders (UIOs)\ninfluence a particular coefficient.\nWe experimented with various feed-forward neural networks as well as small BERT-\ntype GPT models, utilizing encoder transformers with full attention. This method has two\nsignificant limitations: (1) The training set is limited. Computing Stanley coefficients for\n33\nUIOs is computationally expensive, requiring substantial memory, and we are constrained\nto sequences of length around 10. (2) While feature analysis is possible, it doesn’t provide\na true explanation for the coefficients.\nHowever, recent advances in the mathematical\ninterpretability of transformer models have prompted us to explore this avenue in our\nongoing research.\nReferences\n[1] M. Guay-Paquet. A modular law for the chromatic symmetric functions of (3 + 1)-free\nposets. arXiv:1306.2400, 2013.\n[2] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2024.\n[3] T. Hikita.\nA proof of the stanley-stembridge conjecture.\narXiv preprint\narXiv:2410.12758, 2024. https://arxiv.org/abs/2410.12758.\n[4] J. Huh. Milnor numbers of projective hypersurfaces and the chromatic polynomial of\ngraphs. Journal of the American Mathematical Society, 25:907–927, 2012.\n[5] I. Macdonald. Symmetric Functions and Hall Polynomials. Oxford University Press,\nOxford, 1979.\n[6] D. Scott and P. Suppes. Foundational aspects of theories of measurement. Journal of\nSymbolic Logic, 23:113–128, 1954.\n[7] J. Shareshian and M. L. Wachs. Chromatic quasisymmetric functions. Advances in\nMathematics, 295(4):497–551, June 2016.\n[8] R. P. Stanley. A symmetric function generalization of the chromatic polynomial of a\ngraph. Advances in Mathematics, 111(1):166–194, 1995.\n[9] R. P. Stanley. Graph colorings and related symmetric functions: ideas and applica-\ntions. Discrete Mathematics, 193(1):267–286, 1998.\n[10] R. P. Stanley and J. R. Stembridge.\nOn immanants of jacobi–trudi matrices and\npermutations with restricted position. Journal of Combinatorial Theory, Series A,\n62(2):261–279, March 1993.\n[11] A. Szenes and A. Rok.\nEschers and Stanley’s chromatic e-positivity conjecture in\nlength-2. arXiv:2305.00963, 2023.\n[12] A. Z. Wagner. Constructions in combinatorics via neural networks. arXiv:2104.14516,\n2021.\n34\n",
  "categories": [
    "math.CO",
    "cs.LG",
    "05C15, 05C31, 68T07,"
  ],
  "published": "2024-10-24",
  "updated": "2024-10-24"
}