{
  "id": "http://arxiv.org/abs/2410.20024v1",
  "title": "Beyond Fine-Tuning: Effective Strategies for Mitigating Hallucinations in Large Language Models for Data Analytics",
  "authors": [
    "Mikhail Rumiantsau",
    "Aliaksei Vertsel",
    "Ilya Hrytsuk",
    "Isaiah Ballah"
  ],
  "abstract": "Large Language Models (LLMs) have become increasingly important in natural\nlanguage processing, enabling advanced data analytics through natural language\nqueries. However, these models often generate \"hallucinations\"-inaccurate or\nfabricated information-that can undermine their reliability in critical\ndata-driven decision-making. Addressing the challenge of hallucinations is\nessential to improve the accuracy and trustworthiness of LLMs in processing\nnatural language queries. This research focuses on mitigating hallucinations in\nLLMs, specifically within the context of data analytics. We introduce and\nevaluate four targeted strategies: Structured Output Generation, Strict Rules\nEnforcement, System Prompt Enhancements, and Semantic Layer Integration. Our\nfindings show that these methods are more effective than traditional\nfine-tuning approaches in reducing hallucinations, offering a more reliable\nframework for deploying LLMs in natural language queries for data analytics.\nThis research demonstrates the potential of these strategies to enhance the\naccuracy of LLM-driven data queries, ensuring more dependable results in\ndata-driven environments.",
  "text": "Beyond Fine-Tuning: Effective Strategies for\nMitigating Hallucinations in Large Language\nModels for Data Analytics\nMikhail Rumiantsau, Aliaksei Vertsel, Ilya Hrytsuk, Isaiah Ballah\nteam@narrative.bi\nABSTRACT\nLarge Language Models (LLMs) have become increasingly important in natural\nlanguage processing, enabling advanced data analytics through natural language\nqueries. However, these models often generate \"hallucinations\"â€”inaccurate or\nfabricated informationâ€”that can undermine their reliability in critical data-driven\ndecision-making. Addressing the challenge of hallucinations is essential to\nimprove the accuracy and trustworthiness of LLMs in processing natural language\nqueries. This research focuses on mitigating hallucinations in LLMs, specifically\nwithin the context of data analytics. We introduce and evaluate four targeted\nstrategies: Structured Output Generation, Strict Rules Enforcement, System\nPrompt Enhancements, and Semantic Layer Integration. Our findings show that\nthese methods are more effective than traditional fine-tuning approaches in\nreducing hallucinations, offering a more reliable framework for deploying LLMs\nin natural language queries for data analytics. This research demonstrates the\npotential of these strategies to enhance the accuracy of LLM-driven data queries,\nensuring more dependable results in data-driven environments.\nKeywords:\nLLMs,\nHallucination,\nArtificial\nIntelligence,\nHallucination\nMitigation, Natural Language Query\n1.\nINTRODUCTION\n1.1. Background\nLarge Language Models (LLMs) have become pivotal in the field of natural language processing\n(NLP), offering powerful capabilities to understand and generate human-like text (Naveed et al.).\nTheir ability to process and interpret vast amounts of textual data has made them essential tools\nin a variety of applications, from conversational agents to complex data analysis (Kaddour et al.).\nAs organizations increasingly rely on LLMs for tasks such as data retrieval and interpretation,\nthe ability to interact with these models using natural language queries has significantly\nenhanced the accessibility and usability of data-driven insights (Zhang et al.).\nThe growing dependence on LLMs for natural language queries in data retrieval and analysis\nhighlights their importance in decision-making processes across industries. By transforming\nnatural language queries into meaningful data outputs, LLMs have the potential to democratize\ndata analytics, allowing users with varying levels of technical expertise to extract valuable\ninformation from large datasets. However, as the use of LLMs expands into critical domains, the\nreliability of these models becomes paramount, particularly in ensuring that the information they\ngenerate is accurate and trustworthy.\nA critical challenge that has emerged with the increased adoption of LLMs is their susceptibility\nto generating \"hallucinations\"â€”content that is inconsistent with real-world facts, user inputs, or\nthe broader context. The growing reliance on LLMs for data retrieval and analysis through\nnatural language queries has heightened the urgency to address this issue, as hallucinations can\nlead to the propagation of false or misleading information, undermining the reliability and\ntrustworthiness of LLM-powered data analytics.\n1.2. The Hallucination Problem in LLMs\nHallucinations in LLMs can manifest in various forms, such as generating factually incorrect\ninformation, contradicting previous context, or producing content that is entirely fabricated and\nungrounded in reality (Bruno et al.). These errors can have severe consequences in data-driven\ndecision-making, leading to suboptimal or even harmful outcomes.\nFor instance, an LLM might generate a detailed explanation of a marketing data trend that, upon\ncloser inspection, is based on fabricated data dimensions such as non-existing marketing and\nadvertising campaigns or increased traffic on non-existing web pages. In the context of data\nretrieval and analysis, such hallucinations can have serious consequences, particularly when the\ninformation is used to inform critical decisions. The reliability of LLMs is therefore\ncompromised when hallucinations occur, raising concerns about their deployment in high-stakes\nenvironments where accuracy is crucial.\n1.2. Research Objectives\nThis research paper aims to address the challenge of hallucinations in LLMs, with a specific\nfocus on their application in data analytics. We introduce and evaluate four targeted strategies to\nmitigate hallucinations:\n1. Structured Output Generation: Requiring the model to produce code or structured data\nbefore delivering natural language answers.\n2. Strict Rules Enforcement: Imposing clear guidelines for data retrieval and analysis to\navoid inaccuracies.\n3. System Prompt Enhancements: Augmenting system prompts with contextual metadata\nto better guide the model's responses.\n4. Semantic Layer Integration: Assigning synonyms and custom rules to inputs,\nimproving the model's understanding of data structures.\nBy evaluating these approaches, we aim to identify effective techniques that can enhance the\nreliability and trustworthiness of LLMs in natural language-driven data analytics, ultimately\npaving the way for more dependable and accurate data-driven decision-making.\n2.\nRELATED WORK\n2.1. Hallucination in Language Models\nHallucinations in LLMs have been the subject of growing research interest, as evidenced by\nseveral recent publications (Yadkori et al.). A comprehensive survey provides a thorough\noverview of the hallucination phenomenon, including a taxonomy of different types of\nhallucinations and an analysis of the contributing factors (Huang et al.).\nOne major cause is the model's tendency to overgeneralize from its training data, producing\nresponses that are statistically likely but contextually irrelevant. Another contributing factor is\nthe model's inability to access or retrieve relevant data during inference, leading it to \"fill in the\ngaps\" with fabricated content. Types of hallucinations identified in the literature include factual\nerrors, where the model incorrectly states information, and logical inconsistencies, where the\ngenerated output contradicts itself or the input data (Xu et al.).\n2.2. Traditional Mitigation Techniques\nExisting approaches to mitigate hallucinations in LLMs have primarily focused on fine-tuning\nand data augmentation strategies (Gekhman et al.). Fine-tuning involves adapting a pre-trained\nLLM to a specific task or dataset, and refining the model's parameters to better align with the\ndesired outputs (Fishman and Anadkat). This technique has been shown to improve the model's\nperformance by making it more sensitive to the nuances of the specific domain it is fine-tuned\non. However, while fine-tuning can reduce the frequency of hallucinations, it does not entirely\neliminate them. The technique may also introduce new biases if the fine-tuning dataset is not\nrepresentative or if the process overfits the model to the specific training data (Gekhman et al.).\nPrompt engineering, another common approach, involves carefully crafting the input prompts to\nguide the LLM toward generating more accurate and relevant outputs. By providing clear\ninstructions or specific context within the prompt, it is possible to reduce the likelihood of\nhallucinations. However, prompt engineering is often limited by the model's inherent capabilities\nand the ambiguity that may still arise in complex or open-ended queries (Anderson et al.).\nHybrid approaches that combine rule-based systems with LLMs have also been explored as a\nway to enhance the reliability of model outputs. For instance, rule-based systems can provide a\nstructured framework within which the LLM operates, thereby reducing the risk of hallucinations\nby enforcing strict adherence to predefined rules or logic. In the context of business intelligence,\nhybrid systems have shown promise by integrating the precision of rule-based methods with the\nflexibility and adaptability of LLMsâ€‹(Vertsel and Rumiantsau). However, these systems also face\nchallenges, such as the complexity of integrating different components and the potential for\nconflicts between rule-based logic and the generative capabilities of LLMs.\n3.\nMETHODOLOGY\n3.1. Structured Output Generation\nOne of the key strategies we explore to mitigate hallucinations in LLMs for data analytics is\nStructured Output Generation. Structured Output Generation involves compelling the LLM to\nproduce a structured output, such as code or formal data representations, before delivering\nanswers to natural language queries (Tam et al.). This method leverages the model's ability to\ngenerate syntactically correct code or structured data, which is then executed or interpreted to\nproduce a final answer. By requiring the model to generate structured outputs first, we enforce a\nlogical consistency in the responses, reducing the likelihood of hallucinations.\nTo implement structured output formats, specific templates or coding conventions are predefined\nwithin the model's prompt. For instance, when responding to a query about data trends, the\nmodel might be instructed to first generate SQL queries or Python code snippets that retrieve and\nprocess the relevant data. The structured output is then evaluated for correctness before the\nmodel is allowed to produce a natural language response. This two-step process ensures that the\nfinal answer is grounded in a verifiable data retrieval process, thereby minimizing errors.\nThe primary benefit of Structured Output Generation is its ability to reduce ambiguity in the\nmodel's responses. By forcing the model to articulate its reasoning in a structured format, we\nlimit the scope for speculative assertions that often lead to hallucinations. This approach also\nenhances the accuracy of the answers, as the model is guided by a clear logical framework that\nmust be adhered to before producing a response. Moreover, the structured output can be\nindependently verified, offering an additional layer of assurance.\n3.2. Strict Rules Enforcement\nAnother approach we investigate is Strict Rules Enforcement, where we impose clear guidelines\nand constraints on the model's behavior during data retrieval and analysis. A key aspect of this\napproach is the definition of clear criteria for when the model should abstain from answering\n(Tam et al.). For example, if the model detects that the available data is insufficient to answer a\nquery accurately, it is programmed to either seek clarification or decline to provide an answer.\nThe rules are typically formulated based on domain knowledge and the specific requirements of\nthe data being analyzed. For instance, rules might dictate that certain operations, such as\nstatistical analysis or data aggregation, can only be performed if the dataset meets predefined\ncriteria for completeness and relevance. By enforcing these rules, we reduce the likelihood that\nthe model will generate speculative or incorrect outputs.\nImplementing Strict Rules Enforcement within the LLM framework involves integrating these\nrules directly into the model's decision-making process. This can be achieved by embedding the\nrules in the model's prompt or by using external scripts that evaluate the model's outputs against\nthe rules. For example, if the model is tasked with analyzing sales data, the rules might require\nthat all conclusions be based on a minimum threshold of data points or specific time periods.\n3.3. System Prompt Enhancements\nWe also explore System Prompt Enhancements, where we augment the system prompt with\nadditional contextual information and metadata to better guide the model's responses.\nSystem Prompt Enhancements involve the inclusion of contextual metadata within the prompts\nprovided to the LLM (Sahoo et al.). This metadata provides additional context about the dataset,\nguiding the model's understanding and interpretation of the data. For example, metadata might\ninclude information about the data's source, the time period it covers, or specific variables that\nare relevant to the query. By enriching the prompt with this context, we help the model to better\ncomprehend the nuances of the data, which in turn reduces the likelihood of hallucinations.\nThe inclusion of contextual metadata can be implemented by appending detailed descriptions of\nthe dataset to the prompt or by embedding structured metadata tags within the input. By\nproviding the model with richer context, we enable it to generate more accurate and relevant\nresponses, as it can draw on the additional information provided to avoid misinterpretations.\nEnhanced prompts also contribute to more consistent outputs, as the model is less likely to\ndeviate from the intended context when generating its responses. This approach is particularly\neffective in complex data analysis tasks where understanding the context of the data is crucial for\naccurate interpretation.\n3.4. Semantic Layer Integration\nFinally, we investigate the integration of a semantic layer to improve the model's understanding\nof data structures and relationships, aiming to reduce hallucinations by providing more targeted\nand accurate responses.\nWe use the semantic layer to assign synonyms and custom rules to inputs, enhancing the model's\nunderstanding of the data structure and the relationships between different data elements. The\nsemantic layer acts as an intermediary between the natural language query and the data,\ntranslating the query into terms that the model can more accurately interpret. For example,\nsynonyms might be used to standardize the terminology used in queries, ensuring that the model\nrecognizes different expressions of the same concept.\nThe primary goal of Semantic Layer Integration is to enhance the model's understanding of the\ndata and its underlying semantics (Patel et al.). By providing a structured framework for\ninterpreting natural language inputs, the semantic layer helps the model to better grasp the\nrelationships between different data elements and the specific meaning of terms used in queries.\nThis improved understanding reduces the likelihood of hallucinations, as the model is less likely\nto misinterpret the input or generate outputs that are inconsistent with the data structure.\nTechniques for enhancing data understanding through semantic layers include the use of\nontology-based mappings, where terms in the query are mapped to specific concepts within an\nontology that defines the data structure. This approach ensures that the model's interpretation of\nthe query is grounded in a well-defined conceptual framework, leading to more accurate and\nreliable outputs. Additionally, by standardizing the language used in queries through synonyms\nand custom rules, the semantic layer helps to reduce variability in the model's responses, further\nminimizing the risk of hallucinations.\nCustom rules within the semantic layer can further refine the model's interpretation by defining\nhow certain terms or phrases should be understood in the context of the data. These rules might\nspecify, for instance, that certain terms are synonymous with specific data fields or that particular\nphrases imply a certain type of analysis. By integrating these semantic enhancements, the\nmodel's comprehension of the query is improved, leading to more accurate and relevant outputs.\n4.\nEXPERIMENTS AND RESULTS\n4.1 Datasets\nFor this study, we utilized a proprietary anonymized dataset, which is based on real-world data\ncollected from various data sources1 and custom datasets derived from real marketing campaigns.\nThe dataset was carefully anonymized to protect sensitive information while maintaining the\nintegrity and relevance of the data for our research.\nThe datasets encompass a wide range of data types, including structured data such as user\nengagement metrics, conversion rates, and ad performance data, as well as unstructured data like\n1 These sources include Google Analytics 4, Hubspot, Salesforce, Google Search Console,\nGoogle Ads, and Meta Ads\nsearch queries and user feedback. The diverse nature of the dataset allowed us to rigorously test\nthe effectiveness of our hallucination mitigation strategies in different contexts. The experiments\nwere conducted in a controlled computing environment using advanced LLMs and scripts to\nconvert data into structured formats, such as JSON and CSV, to facilitate seamless integration\nwith the LLMs.\nTo evaluate the combined approach, we utilize a large-scale, obfuscated Google Analytics 360\nDataset2 containing structured data, including marketing campaigns, website analytics, and\ntransactional records. We modified3 the timestamps and transaction values in the dataset to\npreserve the integrity and relevancy of the data.\n4.2 Evaluation Metrics\nTo evaluate the performance of our proposed methods in mitigating hallucinations, we used a\ncomprehensive set of metrics that focus on both the accuracy of the model's outputs and the\nreduction of hallucination occurrences.\nThe key metrics include:\nâ—\nHallucination Rate: The percentage of outputs containing fabricated or incorrect\ninformation (1).\nâ—\nAccuracy: The percentage of correct responses generated by the model, indicating\nadherence to the actual data (2).\nâ—\nPrecision: The percentage of true positive responses (correct and relevant) to the sum of\ntrue positives and false positives, reflecting the model's ability to avoid generating\nirrelevant information (3).\nâ—\nRecall: The ratio of true positive responses to the sum of true positives and false\nnegatives, indicating the model's ability to retrieve all relevant information (4).\nğ»ğ‘ğ‘™ğ‘™ğ‘¢ğ‘ğ‘–ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘…ğ‘ğ‘¡ğ‘’=\nğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘ \nğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ â„ğ‘ğ‘™ğ‘™ğ‘¢ğ‘ğ‘–ğ‘›ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘ Ã— 100% (1)\nğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦=\nğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ğ‘ \nğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’ğ‘ Ã— 100% (2)â€‹\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=\nğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ \nğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ +ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ Ã— 100% (3)â€‹\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=\nğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ â€‹\nğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ +ğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ Ã— 100% (4).\nThese metrics provided a robust framework for comparing the effectiveness of our methods\nagainst both fine-tuned models and baseline models.\n3 https://github.com/micrum/hallucination-vaccination\n2 https://console.cloud.google.com/marketplace/product/obfuscated-ga360-data/obfuscated-ga360-data\n4.3 Comparative Analysis\n4.3.1 Structured Output vs. Baseline Model\nIn this test, we evaluated the impact of Structured Output Generation on reducing hallucinations\ncompared to a baseline model4. The structured output method demonstrated a significant\nreduction in hallucination rates, particularly in scenarios involving complex data queries.\nTable 1. Hallucination rate comparison between Structured Output Generation and Baseline model\nHallucinations: Structured Output\nHallucinations: Baseline\nData aggregation\n3.1%\n13%\nCalculated metrics\n1.4%\n21%\nData comparison\n1.1%\n4.8%\nRelational operations5\n1.8%\n15.6%\nQuerying against large datasets6\n2.1%\n25.3%\nTable formatted output\n1.1%\n6.7%\nChart formatted output\n3.4%\n9.9%\nReasoning\n3.8%\n10.2%\n6 Datasets over 500MB\n5 e.g. joins\n4 GPT-4 Omni\n4.3.2 Strict Rules vs. Fine-Tuning vs. Baseline Model\nThe Strict Rules Enforcement approach was tested to assess its effectiveness in minimizing\nhallucinations. Compared to a fine-tuned and baseline model, this method showed a substantial\ndecrease in hallucination rates and an increase in accuracy. The use of strict criteria for when the\nmodel should abstain from answering significantly reduced the occurrence of speculative\noutputs, leading to more precise and reliable responses.\nTable 2. Accuracy rate comparison between Strict Rules Enforcement, Fine-Tuning, and Baseline model\nStrict Rules,\nAccuracy\nFine-Tuning,\nAccuracy\nBaseline model,\nAccuracy\nSecurity breach\n98.1%\n97.8%\n91.1%\nBroken links\n97.3%\n89.1%\n36.1%\nForbidden operations\n96.1%\n91.7%\n85.1%\nPrompt intrusion\n98.5%\n98.1%\n31.7%\n4.3.3 System Prompt Enhancements vs. Fine-Tuning vs. Baseline Model\nSystem Prompt Enhancements were evaluated by incorporating contextual metadata within the\nprompts and comparing the results against fine-tuned and baseline model7. The prompts were\nenriched with contextual metadata that included information about the data's source, the time\nperiod it covers, and specific variables that are relevant to the query.\nThis method resulted in a notable improvement in the modelâ€™s understanding of queries, reducing\nhallucination rates and enhancing recall. The enriched prompts guided the model more\neffectively, leading to more accurate and contextually relevant responses without the unnecessary\nintroduction of new information.\n7 Grok-2 Beta\nTable 3. Comparison of Hallucination rate between System Prompt Enhancements, Fine-Tuning, and\nBaseline models\nHallucination Rate:\nEnhanced Prompts\nHallucination Rate:\nFine-Tuning\nHallucination Rate:\nBaseline Model\nUnnecessary introductions\n1.2%\n1.5%\n33.9%\nGeneric answers\n2.9%\n8.9%\n20.3%\nRelative dates understanding\n1.9%\n10.3%\n18.4%\nData summarization\n4.8%\n5.0%\n6.5%\nVisualization type selection\n7.5%\n7.9%\n38.3%\n4.3.4 Semantic Layer vs. Fine-Tuning vs. Baseline Model\nThe Semantic Layer Integration approach was tested to determine its impact on the modelâ€™s\nability to process and understand natural language queries. By integrating synonyms and custom\nrules, this method improved the model's comprehension of the data structure, leading to a\nreduction in hallucinations and higher accuracy.\nThis method works by creating a semantic layer that standardizes language variations and\ndefines custom rules to handle context-specific terms within the data. By aligning various terms\nand phrases with their underlying data fields and relationships, the semantic layer enables the\nmodel to interpret user queries with greater precision. This structured mapping of language to\ndata concepts allows the model to generate responses that are consistent with the datasetâ€™s actual\ncontent, reducing the likelihood of misinterpretations or hallucinations in responses.\nThe comparative analysis showed that the semantic layer method outperformed the fine-tuning\nand baseline model.\nTable 4. Comparison of Hallucination rate between Semantic Layer Integration, Fine-Tuning, and\nBaseline models\nHallucination Rate:\nSemantic Layer\nHallucination Rate:\nFine-Tuning\nHallucination Rate:\nBaseline model\nAmbiguous metric names\n1.9%\n7.5%\n10.9%\nUse of metric synonyms\n2.7%\n4.9%\n11.3%\nIndirect questions\n1.9%\n10.3%\n18.4%\nCalculated metrics\n7.5%\n27.9%\n38.9%\nReasoning\n8.4%\n21.9%\n48.3%\n4.3.5 Baseline Models vs. Combined Strategies\nThe final experiment assessed the combined impact of the proposed hallucination mitigation\nstrategies, including Structured Output Generation, Strict Rules Enforcement, System Prompt\nEnhancements, and Semantic Layer Integration. By incorporating these methods simultaneously,\nmodel8 was guided through multiple layers of control and contextual understanding, each method\ncomplementing the others to ensure a more robust and accurate response generation.\nThis comprehensive approach demonstrated the most significant reduction in hallucination rates\nand the highest overall performance in terms of precision, recall, and accuracy, outperforming\nthe baseline models by a considerable margin.\nTogether, these combined strategies led to the highest overall performance in terms of precision,\nrecall, and accuracy, significantly outperforming the baseline models by creating a layered\nmitigation framework that minimized errors and improved the modelâ€™s reliability across diverse\nquery scenarios.\n8 NBI.AI-1\nTable 5. Performance Comparison: Baseline Models vs. Combined Strategy\nCombined Strategy\nGPT-4o\nGrok-2\nHallucination Rate\n1.52%\n16.67%\n13.64%\nPrecision\n89.39%\n46.97%\n42.42%\nRecall\n87.88%\n43.94%\n40.91%\n4.4. Experimental Setup\nTo evaluate the effectiveness of the proposed strategies, we conduct a series of experiments on a\nrange of data analytics tasks, including question answering, data summarization, and report\ngeneration.\n4.4.1 Data Aggregation Questions\nThese questions involve summarizing data across multiple records or metrics.\n4.4.2 Calculated Metrics Questions\nThese questions involve performing calculations based on available metrics.\n4.4.3 Data Comparison Questions\nThese questions require comparing different data sets or time periods.\n4.4.4 Relational Operations (e.g., Joins) Questions\nThese questions involve combining multiple datasets or data fields.\n4.4.5 Querying Against Large Datasets Questions\nThese questions involve performing operations on extensive datasets9.\n9 Datasets over 500MB\n4.4.6 Table Formatted Output Questions\nThese questions ask for results to be presented in a table format.\n4.4.7 Chart Formatted OutputQuestions\nThese questions ask for a chart as the output.\n4.4.8 Reasoning Questions\nThese questions involve logical reasoning or conclusions based on the data.\n4.5. Discussion\n4.5.1 Interpretation of Results\nThe results of our experiments demonstrate the effectiveness of the proposed methods in\nmitigating hallucinations in LLMs, particularly when compared to traditional fine-tuning and\nbaseline models.\nâ—\nStructured Output Generation consistently produced the most accurate and logically\nconsistent responses, particularly in complex data processing scenarios.\nâ—\nStrict Rules Enforcement effectively minimized speculative outputs, ensuring that the\nmodel only provided responses when there was sufficient data to support them.\nâ—\nSystem Prompt Enhancements significantly improved the model's contextual\nunderstanding, resulting in more relevant and accurate responses.\nâ—\nSemantic Layer Integration enhanced the modelâ€™s ability to interpret queries accurately,\nespecially in cases involving complex data semantics.\nâ—\nThe combination of these strategies outperformed the baseline models across all\nevaluation metrics, showcasing the synergistic benefits of a multi-pronged approach to\nhallucination mitigation.\n4.5.2 Comparison with Baseline Models\nOur methods outperformed baseline models across all evaluation metrics, particularly in\nreducing hallucination rates. The improvements were most pronounced in scenarios involving\ncomplex or ambiguous queries, where traditional fine-tuning approaches struggled to maintain\naccuracy and relevance.\n4.5.3 Potential Limitations and Areas for Improvement\nWhile the proposed methods demonstrated significant improvements, there are still areas for\nfurther research and development. The methods we used may increase computational\ncomplexity, particularly in the case of Structured Output Generation and Semantic Layer\nIntegration. Future research could focus on optimizing these methods to balance accuracy with\ncomputational efficiency.\n5.\nCASE STUDIES\n5.1 Real-World Application: AI Data Analyst\nIn this case study, we focus on how our hallucination mitigation strategies were applied in the AI\nData Analyst10, a tool designed to answer natural language queries about data. The tool is used in\ntwo key scenarios: datasets with a known structure11 and arbitrary datasets with unknown\nstructures12.\n5.1.1 Scenario 1: Known Structure (Google Analytics 4)\nFor datasets like Google Analytics 4, users often ask questions such as \"What are the key trends\nin website traffic over the past month?\" To ensure accurate answers, we implemented\nStructured Output Generation, which required the model to generate and execute Python code\nbefore providing a response. This step anchored the model's answers in concrete data,\nsignificantly reducing the incidence of hallucinations.\nAdditionally, we used Strict Rules Enforcement to prevent speculative answers. If the data was\nincomplete or did not meet predefined criteria, the model would abstain from answering or\nrequesting additional context. System Prompt Enhancements further improved accuracy by\nincluding metadata about the dataset, such as time ranges and relevant metrics, guiding the\nmodel to produce more contextually accurate responses.\n12 CSV files with marketing data\n11 Google Analytics 4 dataset\n10 https://nbi.ai/\nFigure 1. Question-answering scenario with known data structure\n5.1.3 Scenario 2: Unknown Structure (Arbitrary CSV Files)\nIn scenarios involving arbitrary datasets with unknown structures, such as CSV files from\nmarketing campaigns, users might ask, \"Which campaign had the highest ROI?\" Here, the\nSemantic Layer Integration played a crucial role. By mapping synonyms and applying custom\nrules, the model could better interpret the varied and potentially unfamiliar terminology used in\nthese datasets.\nFor these queries, Structured Output Generation helped by first generating code snippets that\nparsed and analyzed the CSV data before the model attempted to answer the query. This\napproach ensured that the answers were grounded in the actual data structure, even when the\ndataset was unfamiliar to the model.\nFigure 2. Question-answering scenario with an arbitrary dataset\n5.1.4 Results and Impact\nImplementing these strategies resulted in a significant reduction in hallucinations across both\nscenarios. The AI Data Analyst became more reliable in answering natural language queries,\nproviding users with accurate, actionable insights regardless of the dataset's structure (Vertsel).\nThis case study highlights the importance of tailored hallucination mitigation strategies in\nenhancing the performance of LLMs in real-world data query applications.\nCONCLUSION\nIn this research paper, we have presented a comprehensive investigation of effective strategies\nfor mitigating hallucinations in LLMs used for natural language queries in data analytics. By\nexploring\nStructured\nOutput\nGeneration,\nStrict\nRules\nEnforcement,\nSystem\nPrompt\nEnhancements, and Semantic Layer Integration, we have demonstrated that these proposed\nmethods significantly outperform fine-tuning in reducing hallucinations.\nThe superiority of these methods in mitigating hallucinations highlights their potential for\nbroader application in other data-driven tasks where accuracy is paramount. However, there\nremains room for further exploration. Future research could focus on optimizing these methods\nto balance computational efficiency with performance.\nREFERENCES\nAnderson, Ark, et al. â€œBeyond Fine Tuning: A Modular Approach to Learning on Small Data.â€ arXiv,\n2016, https://arxiv.org/abs/1611.01714.\nBruno, Alessandro, et al. â€œ[2311.08117] Insights into Classifying and Mitigating LLMs' Hallucinations.â€\narXiv, 14 November 2023, https://arxiv.org/abs/2311.08117. Accessed 24 October 2024.\nFishman, Simon, and Shyamal Anadkat. â€œHow to fine-tune chat models.â€ OpenAI Cookbook,\nhttps://cookbook.openai.com/examples/how_to_finetune_chat_models. Accessed 24 October\n2024.\nGekhman, Zorik, et al. â€œ[2405.05904] Does Fine-Tuning LLMs on New Knowledge Encourage\nHallucinations?â€ arXiv, 9 May 2024, https://arxiv.org/abs/2405.05904. Accessed 25 October\n2024.\nHuang, Lei, et al. â€œ[2311.05232] A Survey on Hallucination in Large Language Models: Principles,\nTaxonomy, Challenges, and Open Questions.â€ arXiv, 9 November 2023,\nhttps://arxiv.org/abs/2311.05232. Accessed 24 October 2024.\nKaddour, Jean, et al. â€œ[2307.10169] Challenges and Applications of Large Language Models.â€ arXiv, 19\nJuly 2023, https://arxiv.org/abs/2307.10169. Accessed 24 October 2024.\nNaveed, Humza, et al. â€œA Comprehensive Overview of Large Language Models.â€ arXiv, 2023,\nhttps://arxiv.org/pdf/2307.06435. Accessed 24 October 2024.\nPatel, Liana, et al. â€œLOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and\nStructured Data.â€ arXiv, 16 July 2024, https://arxiv.org/html/2407.11418v1. Accessed 24 October\n2024.\nSahoo, Pranab, et al. â€œ[2402.07927] A Systematic Survey of Prompt Engineering in Large Language\nModels: Techniques and Applications.â€ arXiv, 5 February 2024, https://arxiv.org/abs/2402.07927.\nAccessed 24 October 2024.\nTam, Zhi Rui, et al. â€œ[2408.02442] Let Me Speak Freely? A Study on the Impact of Format Restrictions\non Performance of Large Language Models.â€ arXiv, 5 August 2024,\nhttps://arxiv.org/abs/2408.02442. Accessed 24 October 2024.\nVertsel, Alexey. â€œIntroducing NBI.AI-1: a Specialized Generative BI Model.â€ Narrative BI, 28 May 2024,\nhttps://www.narrative.bi/company-news/hybrid-approach-nbi-ai-1. Accessed 24 October 2024.\nVertsel, Aliaksei, and Mikhail Rumiantsau. â€œ[2404.15604] Hybrid LLM/Rule-based Approaches to\nBusiness Insights Generation from Structured Data.â€ arXiv, 24 April 2024,\nhttps://arxiv.org/abs/2404.15604. Accessed 24 October 2024.\nXu, Ziwei, et al. â€œ[2401.11817] Hallucination is Inevitable: An Innate Limitation of Large Language\nModels.â€ arXiv, 22 January 2024, https://arxiv.org/abs/2401.11817. Accessed 24 October 2024.\nYadkori, Yasin Abbasi, et al. â€œ[2405.01563] Mitigating LLM Hallucinations via Conformal Abstention.â€\narXiv, 4 April 2024, https://arxiv.org/abs/2405.01563. Accessed 24 October 2024.\nZhang, Yue, et al. â€œ[2309.01219] Siren's Song in the AI Ocean: A Survey on Hallucination in Large\nLanguage Models.â€ arXiv, 3 September 2023, https://arxiv.org/abs/2309.01219. Accessed 24\nOctober 2024.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-10-26",
  "updated": "2024-10-26"
}