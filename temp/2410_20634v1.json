{
  "id": "http://arxiv.org/abs/2410.20634v1",
  "title": "Plastic Learning with Deep Fourier Features",
  "authors": [
    "Alex Lewandowski",
    "Dale Schuurmans",
    "Marlos C. Machado"
  ],
  "abstract": "Deep neural networks can struggle to learn continually in the face of\nnon-stationarity. This phenomenon is known as loss of plasticity. In this\npaper, we identify underlying principles that lead to plastic algorithms. In\nparticular, we provide theoretical results showing that linear function\napproximation, as well as a special case of deep linear networks, do not suffer\nfrom loss of plasticity. We then propose deep Fourier features, which are the\nconcatenation of a sine and cosine in every layer, and we show that this\ncombination provides a dynamic balance between the trainability obtained\nthrough linearity and the effectiveness obtained through the nonlinearity of\nneural networks. Deep networks composed entirely of deep Fourier features are\nhighly trainable and sustain their trainability over the course of learning.\nOur empirical results show that continual learning performance can be\ndrastically improved by replacing ReLU activations with deep Fourier features.\nThese results hold for different continual learning scenarios (e.g., label\nnoise, class incremental learning, pixel permutations) on all major supervised\nlearning datasets used for continual learning research, such as CIFAR10,\nCIFAR100, and tiny-ImageNet.",
  "text": "Preprint\nPLASTIC LEARNING WITH DEEP FOURIER FEATURES\nAlex Lewandowski†\nDale Schuurmans†‡⋆\nMarlos C. Machado†⋆\n†Department of Computing Science, University of Alberta, ‡Google DeepMind,\n⋆Canada CIFAR AI Chair\nABSTRACT\nDeep neural networks can struggle to learn continually in the face of non-\nstationarity. This phenomenon is known as loss of plasticity. In this paper, we\nidentify underlying principles that lead to plastic algorithms. In particular, we\nprovide theoretical results showing that linear function approximation, as well as a\nspecial case of deep linear networks, do not suffer from loss of plasticity. We then\npropose deep Fourier features, which are the concatenation of a sine and cosine in\nevery layer, and we show that this combination provides a dynamic balance between\nthe trainability obtained through linearity and the effectiveness obtained through the\nnonlinearity of neural networks. Deep networks composed entirely of deep Fourier\nfeatures are highly trainable and sustain their trainability over the course of learning.\nOur empirical results show that continual learning performance can be drastically\nimproved by replacing ReLU activations with deep Fourier features. These results\nhold for different continual learning scenarios (e.g., label noise, class incremental\nlearning, pixel permutations) on all major supervised learning datasets used for\ncontinual learning research, such as CIFAR10, CIFAR100, and tiny-ImageNet.\n1\nINTRODUCTION\nContinual learning is a problem setting that moves past some of the rigid assumptions found in\nsupervised, semi-supervised, and unsupervised learning (Ring, 1994; Thrun, 1998). In particular,\nthe continual learning setting involves learning from data sampled from a changing, non-stationary\ndistribution rather than from a fixed distribution. A performant continual learning algorithm faces\na trade-off due to its limited capacity: it should avoid forgetting what was previously learned while\nalso being able to adapt to new incoming data, an ability known as plasticity (Parisi et al., 2019).\nCurrent approaches that use neural networks for continual learning are not yet capable of making\nthis trade-off due to catastrophic forgetting (Kirkpatrick et al., 2017) and loss of plasticity (Dohare\net al., 2021; Lyle et al., 2023; Dohare et al., 2024). The training of neural networks is in fact an active\nresearch area in the theory literature for supervised learning (Jacot et al., 2018; Yang et al., 2023;\nKunin et al., 2024), which suggests there is much left to be understood in training neural networks\ncontinually. Compared to the relatively well-understood problem setting of supervised learning, even\nthe formalization of the continual learning problem is an active research area (Kumar et al., 2023a;\nAbel et al., 2024; Liu et al., 2023). With these uncertainties surrounding current practice, we take\na step back to better understand the inductive biases used to build algorithms for continual learning.\nOne fundamental capability expected from a continual learning algorithm is its sustained ability\nto update its predictions on new data. Recent work has identified the phenomenon of loss of plasticity\nin neural networks in which stochastic gradient-based training becomes less effective when faced\nwith data from a changing, non-stationary distribution (Dohare et al., 2024). Several methods have\nbeen proposed to address the loss of plasticity in neural networks, with their success demonstrated\nempirically across both supervised and reinforcement learning (Ash and Adams, 2020; Lyle et al.,\n2022; 2023; Lee et al., 2024). Empirically, works have identified that the plasticity of neural networks\nis sensitive to different components of the training process, such as the activation function (Abbas\net al., 2023). However, little is known about what is required for learning with sustained plasticity.\nThe goal of this paper is to identify a basic continual learning algorithm that does not lose plasticity\nin both theory and practice rather than mitigating the loss of plasticity in existing neural network\narchitectures. In particular, we investigate the effect of the nonlinearity of neural networks on the loss\nCorrespondence to: Alex Lewandowski <lewandowski@ualberta.ca>.\n1\narXiv:2410.20634v1  [cs.LG]  27 Oct 2024\nPreprint\nx1\nx2\nx3\nx4\nz1\nz2\nFigure 1: A neural network with deep Fourier features in every layer approximately embeds\na deep linear network. A single layer using deep Fourier features linearly combines the inputs,\nx, to compute the pre-activations, z, and each pre-activation is mapped to both a cos unit and a\nsin unit (Left). For each pre-activation, either the sin unit (Middle) or the cos unit (Right) is\nwell-approximated by a linear function.\nof plasticity. While loss of plasticity is a well-documented phenomenon in neural networks, previous\nempirical observations suggest that linear function approximation is capable of learning continually\nwithout suffering from loss of plasticity (Dohare et al., 2021; 2024). In this paper, we prove that\nlinear function approximation does not suffer from loss of plasticity and can sustain their learning\nability on a sequence of tasks. We then extend our analysis to a special case of deep linear networks,\nwhich provide an interesting intermediate case between deep nonlinear networks and linear function\napproximation. This is because deep linear networks are linear in representation but nonlinear in\ngradient dynamics (Saxe et al., 2014). We provide theoretical and empirical evidence that general\ndeep linear networks also do not suffer from loss of plasticity. The plasticity of deep linear networks\nis surprising because it suggests that, for sustaining plasticity, the nonlinear dynamics of deep linear\nnetworks are more similar to the linear dynamics of linear function approximation than they are to\nthe nonlinear dynamics of deep nonlinear networks.\nGiven this seemingly natural advantage of linearity for continual learning, as well as its inherent\nlimitation to learning only linear representations, we explore how nonlinear networks can better\nemulate the dynamics of deep linear networks to sustain plasticity. We hypothesize that, to effectively\nlearn continually, the neural network must balance between introducing too much linearity and\nsuffering from loss of deep representations and introducing too much nonlinearity and suffering from\nloss of plasticity. In fact, we show that previous work partially satisfies this hypothesis, such as the\nconcatenated ReLU (Shang et al., 2016), leaky-ReLU activations (Xu et al., 2015), and residual\nconnections (He et al., 2016), but they fail at striking this balance. Our results build on previous work\nthat identified issues of unit saturation (Abbas et al., 2023) and unit linearization (Lyle et al., 2024) as\nissues in continually training neural networks with common activation functions. In particular, we\ngeneralize these phenomena to unit sign entropy. We show that linear networks have high unit sign\nentropy, meaning that the sign of a hidden unit on different inputs is positive on approximately half\nthe inputs. In contrast, deep nonlinear networks with most activation functions tend to have low unit\nsign entropy, which indicates saturation or linearization.\nPeriodic activation functions (Parascandolo et al., 2017), like the sinusoid function (sin), are a\nnotable exception for having high unit sign entropy despite still suffering from loss of plasticity. Thus,\nin addition to unit sign entropy, we demonstrate that the network’s activation function should be well-\napproximated by a linear function. We propose deep Fourier features as a means of approximating\nlinearity dynamically, with every pre-activation being connected to two units, one of which will always\nbe well-approximated by a linear function. In particular, deep Fourier features concatenate a sine and\na cosine activation in each hidden layer. The resulting network is nonlinear while also approximately\nembedding a deep linear network using all of its parameters. Deep Fourier features differ from\nprevious approaches that use Fourier features only in the input layer (Tancik et al., 2020; Li and Pathak,\n2021; Yang et al., 2022) or that use fixed Fourier feature basis (Rahimi and Recht, 2007; Konidaris\net al., 2011). We demonstrate that networks using these shallow Fourier features still exhibit a loss\nof plasticity. Only by using deep Fourier features in every layer is the network capable of sustaining\nand improving trainability in a continual learning setting. Using tiny-ImageNet (Le and Yang, 2015),\nCIFAR10, and CIFAR100 (Krizhevsky, 2009), we show that deep Fourier features can be used as\n2\nPreprint\na drop-in replacement for improving trainability in commonly used neural network architectures.\nFurthermore, deep Fourier features achieve superior generalization performance when combined with\nregularization because their trainability allows for much higher regularization strengths.\n2\nPROBLEM SETTING\nWe define a deep network, fθ with a a parameter set, θ = {Wl, bl}L\nl=1, as a sequence of layers, in\nwhich each layer applies a linear transformation followed by an element-wise activation function,\nϕ in each hidden layer. The output of the network, fθ(x) := hL(x), is defined recursively by\nhl = [hl,1, . . . , hl,w] = [ϕ(zl,1), . . . , ϕ(zl,w)] = ϕ(zl), and, zl = Wlhl−1 + bl where w is the\nwidth of the network, and h0 = x. We refer to a particular element of the hidden layer’s output hl,i\nas a unit. The deep network is a deep linear network when the activation function is the identity,\nϕ(z) = z. Linear function approximation is equivalent to a linear network with L = 1.\nThe problem setting that we consider is continual supervised learning without task boundaries. At\neach iteration, a minibatch of observation-target pairs of size M, {xi, yi}M\ni=1, is used to update the\nparameters θ of a neural network fθ using a variant of stochastic gradient descent. The learning\nproblem is continual because the distribution from which the data is sampled, p(x, y), is changing.\nFor simplicity, we assume this non-stationarity changes the distribution over the input-target pairs\nevery T iterations. The data is sampled from a single distribution for T steps, and we refer to this\nparticular temporary stationary problem as a task, τ. The distribution over observations and targets\nthat defines a task τ is denoted by pτ.\nWe focus our theoretical analysis on the problem of loss of trainability, in which we evaluate the\nneural network at the end of each task using samples from the most recent task distribution, pτ, as\nis commonly done in previous work (Lyle et al., 2023). Loss of trainability refers to the problem\nwhere the neural network is unable to sustain its initial performance on the first task to later tasks.\nSpecifically, we denote the optimisation objective by Jτ(θ) = E(x,y)∼pτ\n\u0002\nℓ(fθ(x), y)\n\u0003\n, for some loss\nfunction ℓ, and task-specific data distribution pτ. We use t to denote the iteration count of the learning\nalgorithm, and thus the current task number can be written as τ(t) = ⌊t/T⌋.\n3\nTRAINABILITY AND LINEARITY\nIn this section, we show that, unlike nonlinear networks, linear networks do not suffer from loss of\ntrainability. That is, if the number of iterations in each task is sufficiently large, a linear network\nsustain trainability on every task in the sequence. We then show theoretically that a special case of\ndeep linear networks also does not suffer from loss of trainability, and we empirically validate the\ntheoretical findings in more general settings. These results provide a theoretical basis for previous\nwork that uses a linear baseline in loss of plasticity experiments.\n3.1\nTRAINABILITY OF LINEAR FUNCTION APPROXIMATION\nWe first prove that loss of trainability does not occur with linear function approximation,\nfθ(x) = Wlx + bl. We prove this by showing that any sequence of tasks can be learned with a\nlarge enough number of iterations per task. In particular, the suboptimality gap on the τ-th task can\nbe upper bounded on a quantity that is independent of the solution found on the first τ −1 tasks.\nLinear function approximation avoids loss of trainability because the optimisation problem on each\ntask is convex (Agrawal et al., 2021; Boyd and Vandenberghe, 2004), with a unique global optimum,\nθ⋆\nτ. We now state the theorem, which we prove in Appendix B\nTheorem 1. Let θ(τT ) denote the linear weights learned at the end of the τ-th task, with the\ncorresponding unique global minimum for task τ being denoted by θ⋆\nτ. Assuming the objective\nfunction is µ-strongly convex, the suboptimality gap for gradient descent on the τ-th task is\nJτ(θ(τT )) −Jτ(θ⋆\nτ) <\n2D(1 −αµ)T\nαT(1 −(1 −αµ)T ),\nwhere each task lasts for T iteration, D is the assumed bound on the parameters at the global\nminimum for every task, and α is the step-size.\nIn addition to convexity, we assume that the objective function is µ-strongly convex, ∇2\nθJτ(θ) ≻µI,\nwhere ∇2\nθJτ(θ) denotes the Hessian. This assumption is often satisfied in the continual learning\n3\nPreprint\nproblem outlined in Section 2 (see Appendix A.1 for more discussion). Lastly, we assume that the\nparameters at the global optimum for every task are bounded: ∥θτ∥2 < D. This is true for regression\nproblems if the observations and targets are bounded. In classification tasks, the global optimum\ncan be at infinity because activation functions such as the sigmoid and the softmax are maximized at\ninfinity. In this case, we constrain the parameter set, {θ : ∥θ∥2 < D}, and project the optimum onto\nthis set. Intuitively, this theorem states that if the problem is bounded and effectively strongly convex\ndue to a finite number of iterations, then the optimisation dynamics are well-behaved for every task\nin the bounded set. In particular, this means that the error on each task can be upper bounded by a\nquantity independent of the initialization found on previous tasks. Thus, given enough iterations,\nlinear function approximation can learn continually without loss of trainability.\n3.2\nTRAINABILITY OF DEEP LINEAR NETWORKS\nWe now provide evidence that, similar to linear function approximation, deep linear networks also\ndo not suffer from loss of trainability. Deep linear networks differ from deep nonlinear networks\nby not using nonlinear activation functions in their hidden layers (Bernacchia et al., 2018; Ziyin\net al., 2022). This means that a deep linear network can only represent linear functions. At the same\ntime, its gradient update dynamics are nonlinear and non-convex, similar to deep nonlinear neural\nnetworks (Saxe et al., 2014). Our central claim here is that deep linear networks under gradient\ndescent dynamics avoid parameter configurations that would lead to loss of trainability.\nTo simplify notation, without loss of generality, we combine the weights and biases into a single\nparameter for each layer in the deep linear network , θ = {θ1, . . . , θL}, and fθ(x) = θLθL−1 · · · θ1x.\nWe denote the product of weight matrices, or simply product matrix, as ¯θ = θLθL−1 · · · θ1, which\nallows us to write the deep linear network in terms of the product matrix: fθ(x) = ¯θx. The problem\nsetup we use for the deep linear analysis follows previous work (Huh, 2020), and we provide\nadditional technical details for optimisation dynamics of deep linear networks in Appendix A.3.\nThe gradient of the loss function with respect to the parameters of a deep linear network can be\nwritten in terms of the gradient with respect to the product matrix ¯θ (Bah et al., 2022):\n∇θjJ(θ) = θ⊤\nj+1θ⊤\nj+2 · · · θ⊤\nL ∇¯θJ(¯θ)θ⊤\n1 θ⊤\n2 · · · θ⊤\nj−1,\nwhere the term ∇¯θJ(¯θ) is the gradient of the loss with respect to the product matrix, treating it as if it\nwas linear function approximation. The gradient is nonlinear because of the coupling between the\ngradient of the parameter at one layer and the value of the parameters of the other layers. Nevertheless,\nthe gradient dynamics of the individual parameters can be combined to yield the dynamics of the\nproduct matrix (Arora et al., 2018),\n¯∇θJ(θ) = P¯θ∇¯θJ(¯θ).\nThe dynamics involve a preconditioner, P¯θ, that accelerates optimisation (Arora et al., 2018), which\nwe empirically demonstrate in Section 3.3. On the left-hand side of the equation, we use ¯∇θJ(θ) to\ndenote the combined dynamics of the gradients for each layer on the dynamics of the product matrix.1\nThis means that the effective gradient dynamics of the deep network is related to the dynamics of\nlinear function approximation with a precondition. While the dynamics are nonlinear and non-convex,\nthe overall dynamics are remarkably similar to that of linear function approximation, which is convex.\nWe now provide evidence to suggest that, despite deep linear networks being nonlinear in their gradient\ndynamics, they do not suffer from loss of trainability. We prove this for a special case of deep diagonal\nlinear networks, and provide empirical evidence to support this claim in general deep linear networks.\nTheorem 2. Let fθ(x) = θLθL−1 · · · θ1x be a deep diagonal linear network where θl =\nDiag(θl,1, . . . , θl,d). Then, a deep diagonal linear network converges on a sequence of tasks under\nthe same conditions for convergence in a single task (i.e., the conditions in Arora et al., 2019).\nTheorem 2 states that a deep diagonal linear network, a special case of general deep linear networks,\ncan converge to a solution on each task within a sequence of tasks. The proof, provided in Appendix\nB, shows that the minimum singular value of the product matrix stays greater than zero, σmin(¯θ) > 0.\nHence, deep diagonal linear networks do not suffer from loss of trainability. This result provides\nfurther evidence suggesting that linearity might be an effective inductive bias for learning continually.\n1Note we use ¯∇because ¯∇J(θ) is not a gradient for any function of ¯θ; see discussion by Arora et al. (2018).\n4\nPreprint\nWhile the analysis considers a special case of deep linear networks, namely deep diagonal networks,\nwe note that this is a common setting for the analysis of deep linear networks more generally (Nacson\net al., 2022; Even et al., 2023). In particular, the analysis is motivated by the fact that, under\ncertain conditions, the evolution of the deep linear network parameters can be analyzed through the\nindependent singular mode dynamics (Braun et al., 2022), which simplifies the analysis of deep linear\nnetworks to deep diagonal linear networks.\n3.3\nEMPIRICAL EVIDENCE FOR TRAINABILITY OF GENERAL DEEP LINEAR NETWORKS\nFigure 2: Trainability on a linearly separable\ntask. The higher opacity corresponds to deeper\nnetworks, ranging from {1, 2, 4, 8, 16}. Deep\nlinear networks sustain trainability on new tasks,\nwith some additional depth improving trainability.\nNonlinear networks, using ReLU, suffer from loss\nof trainability at any depth even on this simple\nsequence of linearly separable problems.\nIn the previous section, we proved that a special\ncase of deep linear networks do not suffer from\nloss of trainability. We now provide additional\nempirical evidence that general deep linear net-\nworks do not suffer from loss of trainability. To\ndo so, we use a linearly separable subset of the\nMNIST dataset (LeCun et al., 1998), in which\nthe labels of each image are randomized every\n100 epochs. For this experiment, the data is\nlinearly separable so that even a linear baseline\ncan fit the data if given enough iterations. While\nMNIST is a simple classification problem,\nmemorizing random labels highlights the dif-\nficulties associated with maintaining trainability\n(see Lyle et al., 2023; Kumar et al., 2023b).\nWe emphasize that the goal here is merely to\nvalidate that linear networks remain trainable in\ncontinual learning. We also provide results with\ntraditional nonlinear neural networks on the\nsame problem, showing that they suffer from\nloss of trainability in this simple problem. Later\nin Section 5, we extend our investigation of loss\nof trainability to larger-scale benchmarks.\nIn Figure 2, we see that deep linear networks ranging from a depth of 1 to 16 can sustain trainability.\nUsing a multi-layer perceptron with ReLU activations, deep nonlinear networks quickly reach a\nmuch higher accuracy on the first few tasks. However, due to loss of trainability, deep nonlinear\nnetworks of any depth eventually perform worse than the corresponding deep linear network. With\nadditional epochs, the linear networks could achieve perfect accuracy on this task because it is linear\nseparable. The number of epochs is comparatively low to showcase that, with some additional layers,\na deep linear network is able to improve its trainability as new tasks are encountered.\n4\nCOMBINING LINEARITY AND NONLINEARITY\nIn the previous section, we provided empirical and theoretical evidence that linearity provides an\neffective inductive bias for learning continually by avoiding loss of trainability. However, linear\nmethods are generally not as performant as deep nonlinear networks, meaning that their sustained\nperformance can be inadequate on complex tasks. Even deep linear networks have only linear\nrepresentational power, despite the fact that the gradient dynamics are nonlinear and can lead to\naccelerated learning. We now seek to answer the following question:\nHow can the sustained trainability of linear methods be combined with\nthe expressive power of learned nonlinear representations?\nTo answer this question, we first seek to better understand the effects of replacing linear activation\nfunctions with nonlinear ones in deep networks for continual learning. We observe that deep linear\nnetworks have diversity in their hidden units, which can be induced in nonlinear activation functions\nby adding linearity through a weighted linear component, an idea we refer to as α-linearization.\nTo dynamically balance linearity and nonlinearity, we propose to use deep Fourier features for every\nlayer in a network. We prove that such a network approximately embeds a deep linear network, a\nproperty we refer to as adaptive linearity. We demonstrate that this adaptively-linear network is\nplastic, maintaining trainability even on non-linearly-separable problems.\n5\nPreprint\n4.1\nADDING LINEARITY TO NONLINEAR ACTIVATION FUNCTIONS\nDeep nonlinear networks can learn expressive representations because of their nonlinear activation\nfunction, but these nonlinearities can also lead to issues with trainability.\nAlthough several\ncomponents of common network architectures incorporate linearity, the way in which linearity\nis used does not avoid loss of trainability. One example is the piecewise linearity of the ReLU\nactivation function (Shang et al., 2016), ReLU(x) = max(0, x), that can become saturated and\nprevent gradient propagation if ReLU(x) = 0 for most inputs x. While saturation is generally not\na problem for learning on a single distribution, it has been noted as problematic in learning from\nchanging distributions, for example, in reinforcement learning (Abbas et al., 2023).\nA potential solution to saturation is to use a non-saturating activation function. Two noteworthy\nexamples of non-saturating activation functions include a periodic activation like sin(x) (Paras-\ncandolo et al., 2017) and leaky-ReLUα(x) = αx + (1 −α)ReLU(x) (Xu et al., 2015), both of\nwhich are zero on a set of measure zero. Surprisingly, using leaky-ReLU leads to a related issue,\n“unit linearization” (Lyle et al., 2024), in which the activation is only positive (or negative) Unlike\nsaturated units, linearized units can provide non-zero gradients but render that unit effectively linear,\nlimiting the expressive power of the learned representation. While unit linearization seems to suggest\nthat loss of trainability can occur due to linearity, it is important to note that a “linearized unit” is\nnot the same as a linear unit. This is because a linearized unit provides mostly positive (or negative)\noutputs, whereas a linear unit can output both positive and negative values.\nWe generalize the idea behind unit saturation and unit linearization to unit sign entropy, which is\na metric applicable to activation functions beyond saturating and piecewise linear functions, such as\nperiodic activation functions. Intuitively, it measures the diversity of the activations of a hidden layer.\nDefinition 1 (Unit Sign Entropy). The entropy, H, of the unit’s sign, sgn(h(x)), on a distribution of\ninputs to the network, p(x), is given by H (sgn(h(x))) = Ep(x) [sgn(h(x))].\nThe maximum value of unit sign entropy is 1, which occurs when the unit is positive on half the\ninputs. Conversely, a low sign entropy is associated with the aforementioned issues of saturation\nand linearization. For example, a low sign entropy for a deep network using ReLU activations means\nthat the unit is almost always positive (P (sgn(h(x)) = 1) = 1, meaning it is linearized) or negative\n(P (sgn(h(x)) = 1) = 0, meaning it is saturated).\nWith unit sign entropy, we investigate how the leak parameter for the leaky-ReLU activation\nfunction influences training as pure linearity (α = 1) is traded-off for pure nonlinearity (α = 0). The\nidea of mixing a linearity and nonlinearity can also be generalized to an arbitrary activation function,\nwhich we refer to as the α-linearization of an activation function.\nDefinition 2 (α-linearization). The α-linearization of an activation function ϕ, is denoted by ϕα(x) =\nαx + (1 −α)ϕ(x).\nFigure 3:\nTrainability on a linearly\nseparable task with α-linearization\nDarker opacity lines correspond to\nhigher values of α. Unit sign entropy\nincreases as α increases (inset), leading\nto sustained trainability for α-relu.\nA natural hypothesis is that, as α increases from 0 to 1,\nand the network becomes more linear, loss of trainability\nis mitigated. We emphasize that the α-linearization is pri-\nmarily to gain insights from empirical investigation and it\nis not a solution to loss of trainability. This is because any\nbenefits of α-linearization depend on tuning α, and even\noptimal tuning can lead to overly linear representations\nand slow training compared to nonlinear networks.\nEmpirical Evidence for α-linear Plasticity\nTo under-\nstand the trainability issues introduced by nonlinearity, we\npresent a case-study using sin and ReLU with different\nvalues of the linearization parameter, α. The same exper-\niment setup is used from Section 3.3. Referring to the\nresults in Figure 3, we see that both ReLU and sin acti-\nvation functions are able to sustain trainability for larger\nvalues of α. This verifies the hypothesis: a larger α pro-\nvides more linearity to the network, allowing it to sustain trainability. For α-ReLU, we also verify\nthe hypothesis that the unit sign entropy increases for larger values of α (inset plot). The fact that the\n6\nPreprint\nperiodic sin activation function has a high unit sign entropy despite losing trainability is particularly\ninteresting, and we will return to this in Section 4.2. Note that, while trainability can be sustained, it\nis generally lower than the nonlinear networks for a large values of α.\n4.2\nADAPTIVE-LINEARITY BY CONCATENATING SINUSOID ACTIVATION FUNCTIONS\nUsing the insight that linearity promotes unit sign entropy, we explore an alternative approach\nto sustain trainability. In particular, we found that linearity can sustain trainability but requires\ntuning α, and even optimal tuning can lead to slow learning from overly linear representations. Our\napproach is motivated by concatenated ReLU activations (Shang et al., 2016; Abbas et al., 2023),\nCReLU(z) = [ReLU(z), ReLU(−z)], which avoids the problems from saturated units, but does not\navoid the problem of low unit sign entropy. In particular, we propose using a pair of activations\nfunctions such that one activation function is always approximately linear, with a bounded error.\nOne way to dynamically balance the linearities and nonlinearities of a network is using periodic\nactivation functions. This is because, due to their periodicity, the properties of the activation\nfunction can re-occur as the magnitude of the preactivations grows rather than staying constant,\nlinear, or saturating. But, as we saw in Figure 3, a single periodic activation function like sin\nis not enough. Instead, we propose to use deep Fourier features, meaning that every layer in the\nnetwork uses Fourier features. This is a notable departure from previous work which considers\nonly shallow Fourier features in the first layer (Rahimi and Recht, 2007; Tancik et al., 2020). In\nparticular, each unit is a concatenation of a sinusoid basis of two elements on the same pre-activation,\nFourier(z) = [sin(z), cos(z)]. The advantage of this approach is that a network with deep Fourier\nfeatures maintains approximate linearity in some of its units.\nProposition 1. For any z, there exists a linear function, Lz(x) = a(z)x + b(z), such that either:\n| sin(x) −Lz(x)| ≤c, or | cos(x) −Lz(x)| ≤c, for c =\n√\n2π2/28 and all x ∈[z −π/4, z + π/4].\nAn intuitive description of this is provided in Figure 1. The advantage of using two sinusoids over\njust a single sinusoid is that whenever cos(z) is near a critical point, d/dz cos(z) ≈0, we have that\nsin(z) ≈z, meaning that d/dz sin(z) ≈1 (and vice-versa). The argument follows from an analysis\nof the Taylor series remainder, showing that the Taylor series of half the units in a deep Fourier layer\ncan be approximated by a linear function, with a small error of c =\n√\n2π2/28 ≈0.05. While we found\nthat two sinusoids is sufficient, the approximation error can be further improved by concatenating\nadditional sinusoids, at the expense of reducing the effective width of the layer.\nBecause each pre-activation is connected to a unit that is approximately linear, we can conclude\nthat a deep network comprised of deep Fourier features approximately embeds a deep linear network.\nCorollary 1. A network parameterized by θ, with deep Fourier features, approximately embeds a\ndeep linear network parameterized by θ with a bounded error.\nFigure 4:\nTrainability on a non\nlinearly-separable task. Deep Fourier\nfeatures improve and sustain their train-\nability when other networks cannot.\nNotice that piecewise linear activations also embed a deep\nlinear network, but these embedded deep linear networks\ndo not use the same parameter set. For example, the deep\nlinear network embedded by a ReLU network does not\ndepend on any of the parameters used to compute a ReLU\nunit that is zero. Although the leaky-ReLU function\ninvolves every parameter, the deep linear network vanishes\nbecause the leak parameter is small, α < 1, and hence the\nembedded deep linear network is multiplied by a small\nconstant, α−L, where L is the depth of the network.\nEmpirical Evidence for Nonlinear Plasticity\nWe now\nconsider a similar experimental setup from Sections 3.3\nand 4.1, except we make the problem non linearly-\nseparable by considering random label assignments on\nthe entire dataset. Each task is more difficult because it\ninvolves memorizing more labels, and the effect of the\nnon-stationarity is also stronger due to randomization of more datapoints. As a result, the deep linear\nnetwork can no longer fit a single task well. Referring to Figure 4, the α-linear activation functions\n7\nPreprint\nFigure 5: Training a ResNet-18 continually with diminishing label noise. Deep Fourier features\nare particularly performant on complex tasks like tiny-ImageNet. Despite networks with deep Fourier\nfeatures having approximately half the number of parameters, they surpass the baselines in CIFAR100\nand are on-par with spectral regularization on CIFAR10.\ncan sustain and even improve their trainability, albeit very slowly. In contrast, using deep Fourier\nfeatures within the network enables the network to easily memorize all the labels for 100 tasks. Deep\nFourier features surpass the trainability of the other nonlinear baselines at initialization, CReLU and\nshallow Fourier features followed by ReLU. This is surprising, because deep nonlinear networks at\ninitialization are often a gold-standard for trainability.\n5\nEXPERIMENTS\nOur experiments demonstrate the benefits of the adaptive linearity provided by deep Fourier\nfeatures. While trainability was the primary focus behind our theoretical results and empirical\ncase studies, we show that these findings generalize to other problems in continual learning. In\nparticular, we demonstrate that networks composed of deep Fourier features are capable of learning\nfrom diminishing levels of label noise, and in class-incremental learning, in addition to sustaining\ntrainability on random labels. The main results we present are on all of the major continual supervised\nlearning settings considered in the plasticity literature. They build on the standard ResNet-18\narchitecture, widely used in practice (He et al., 2016).\nDatasets and Non-stationarities\nOur experiments use the common image classification datasets\nfor continual learning, namely tiny-ImageNet (Le and Yang, 2015), CIFAR10, and CIFAR100\n(Krizhevsky, 2009). We augment these datasets with commonly used non-stationarities to create\ncontinual learning problems, with the non-stationarity creating a sequence of tasks from the dataset.\nSpecifically, following recent work on continual learning (Lee et al., 2024), we consider diminishing\nlevels of label noise on each dataset: We start with half the data being corrupted by label noise and\nreduce the noise to clean labels over 10 tasks. Additionally, for the datasets with a larger number\nof classes, tiny-ImageNet and CIFAR100, we also consider the class-incremental setting: the first\ntask involves only five classes, and five new classes are added to the existing pool of classes at the\nbeginning of each task (Van de Ven et al., 2022). Other results and more details on datasets and\nnon-stationarities considered can be found in Appendix C.\nArchitecture and Baselines\nWe compare a ResNet-18 using only deep Fourier features against\na standard ResNet-18 with ReLU activations. The network with deep Fourier features has fewer\nparameters because it uses a concatenation of two different activation functions, halving the effective\nwidth compared to the network with ReLU activations. This provides an advantage to the nonlinear\nbaseline. We also include all prominent baselines that have previously been proposed to mitigate loss\nof plasticity in the field: L2 regularization towards zero, L2 regularization towards the initialization\n(Kumar et al., 2023b), spectral regularization (Lewandowski et al., 2024), Concatenated ReLU (Shang\net al., 2016; Abbas et al., 2023), Dormant Neuron Recycling (ReDO, Sokar et al., 2023), Shrink and\nPerturb (Ash and Adams, 2020), and Streaming Elastic Weight Consolidation (S-EWC, Kirkpatrick\net al., 2017; Elsayed and Mahmood, 2024).\n8\nPreprint\n5.1\nMAIN RESULTS\nOur main result demonstrates that adaptive-linearity is an effective inductive bias for continual\nlearning. In these set of experiments, we consider the problem of sustaining test accuracy on a\nsequence of tasks. In addition to requiring trainability, methods must also sustain their generalization.\nDiminishing Label Noise\nIn Figure 5, we can clearly see the benefits of deep Fourier features in\nthe diminishing label noise setting. At the end of training on ten tasks with diminishing levels of\nlabel noise, the network with deep Fourier features was always among the methods with the highest\ntest accuracy on the the uncorrupted test set. On the first of ten tasks, deep Fourier features could\noccasionally overfit to the corrupted labels leading to initially low test accuracy. However, as the\nlabel noise diminished on future tasks, the network with deep Fourier features was able to continue to\nlearn to correct its previous poorly-generalizing predictions. In contrast, the improvements achieved\nby the other methods that we considered was oftentimes marginal compared to the baseline ReLU\nnetwork. Two exceptions are: (i) networks with CReLU activations, which underperformed relative\nto the baseline network, and (ii) Shrink and Perturb, which was the best-performing baseline method\nfor diminishing label noise. Interestingly, the performance benefit of deep Fourier features is most\nprominent on more complex datasets, like tiny-ImageNet.\nFigure 6: Class incremental learning results on tiny-Imagenet\n(Left) and CIFAR-100 (Right). On both datasets, deep Fourier\nfeatures substantially improve over most baselines.\nClass-Incremental\nLearning\nDeep Fourier features are also\neffective in the class-incremental\nsetting, where later tasks involve\ntraining on a larger subset of\nthe classes.\nThe network is\nevaluated at the end of each\ntask on the entire test set. As\nthe network is trained on later\ntasks, its test set performance\nincreases because it has access\nto a larger subset of the training\ndata. In Figure 6, we see that\nDeep Fourier features largely\noutperform the baselines in this\nsetting, particularly on tiny-ImageNet in which the first forty tasks involve training on a growing\nsubset of the dataset and the last forty “tasks” involve training to convergence on the full dataset. 2\nNot only are deep Fourier features quicker to learn on earlier continual learning tasks, but they are\nalso able to improve their generalization performance by subsequently training on the full dataset.\nOn CIFAR100, the difference between methods is not as prominent, but we can see that deep Fourier\nfeatures are still among the top-performing methods.\n5.2\nSENSITIVITY ANALYSIS\nIn the previous sections, we used deep Fourier features in combination with spectral regularization to\nachieve high generalization. However, the theoretical analysis and case-studies that we presented\nearlier concerned trainability. We now present a sensitivity result to understand the relationship\nbetween trainability and generalization. Using a ResNet-18 with different activation functions, we\nvaried the regularization strength between no regularization and high degrees of regularization. In\nFigure 7, we can see that deep Fourier features indeed have a high degree of trainability, sustaining\ntrainability at different levels of regularization. However, without any regularization, deep Fourier\nfeatures have a tendency to overfit. Over-fitting is a known issue for shallow Fourier features (e.g.,\nwhen using Fourier features only for the input layer, Mavor-Parker et al., 2024). However, deep\nFourier features are able to use their high trainability to learn effectively even when highly regularized.\nThus, while trainability does not always lead to learning, the trainability provided by adaptive-learning\nstill provides a useful inductive bias for continual learning.\n2We use quotation marks to characterize the last forty tasks because they are, in fact, a single task, as the data\ndistribution stops changing after the first forty tasks. We call them “tasks” because of the number of iterations in\nwhich they are trained.\n9\nPreprint\nFigure 7: Sensitivity analysis on tiny-ImageNet, CIFAR10, and CIFAR100. Networks with deep\nFourier features are highly trainable, but have a tendency to overfit without regularization, leading to\nhigh training accuracy but low test accuracy. Due to deep Fourier features being highly trainable, they\nare able to train with much higher regularization strengths leading to ultimately better generalization.\n6\nCONCLUSION\nIn this paper, we proved that linear function approximation and a special case of deep linearity are\neffective inductive biases for learning continually without loss of trainability. We then investigated\nthe issues that arise from using nonlinear activation functions, namely the lack of unit sign entropy.\nMotivated by the effectiveness of linearity in sustaining trainability, we proposed deep Fourier\nfeatures to approximately embed a deep linear network inside a deep nonlinear network. We\nfound that deep Fourier features dynamically balance the trainability afforded by linearity and the\neffectiveness of nonlinearity, thus providing an effective inductive bias for learning continually.\nExperimentally, we demonstrate that networks with deep Fourier features provide benefits for\ncontinual learning across every dataset we consider. Importantly, networks with deep Fourier features\nare effective plastic learners because their trainability allows for higher regularization strengths that\nleads to improved and sustained generalization over the course of learning.\nREFERENCES\nAbbas, Z., Zhao, R., Modayil, J., White, A., and Machado, M. C. (2023). Loss of plasticity in\ncontinual deep reinforcement learning. In Conference on Lifelong Learning Agents.\nAbel, D., Barreto, A., Van Roy, B., Precup, D., van Hasselt, H. P., and Singh, S. (2024). A definition\nof continual reinforcement learning. Advances in Neural Information Processing Systems.\nAgrawal, A., Barratt, S., and Boyd, S. (2021). Learning convex optimization models. Journal of\nAutomatica Sinica, 8(8):1355–1364.\nArora, S., Cohen, N., Golowich, N., and Hu, W. (2019). A convergence analysis of gradient descent\nfor deep linear neural networks. In International Conference on Learning Representations.\nArora, S., Cohen, N., and Hazan, E. (2018). On the optimization of deep networks: Implicit\nacceleration by overparameterization. In International Conference on Machine Learning.\nAsh, J. T. and Adams, R. P. (2020). On Warm-Starting Neural Network Training. In Advances in\nNeural Information Processing Systems.\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. CoRR, abs/1607.06450v1.\n10\nPreprint\nBah, B., Rauhut, H., Terstiege, U., and Westdickenberg, M. (2022). Learning deep linear neural\nnetworks: Riemannian gradient flows and convergence to global minimizers. Information and\nInference: A Journal of the IMA.\nBernacchia, A., Lengyel, M., and Hennequin, G. (2018). Exact natural gradient in deep linear\nnetworks and its application to the nonlinear case. Advances in Neural Information Processing\nSystems.\nBoyd, S. P. and Vandenberghe, L. (2004). Convex optimization. Cambridge university press.\nBraun, L., Dominé, C., Fitzgerald, J., and Saxe, A. (2022). Exact learning dynamics of deep linear\nnetworks with prior knowledge. Advances in Neural Information Processing Systems.\nChou, H.-H., Gieshoff, C., Maly, J., and Rauhut, H. (2024). Gradient descent for deep matrix\nfactorization: Dynamics and implicit bias towards low rank. Applied and Computational Harmonic\nAnalysis.\nCohen, G., Afshar, S., Tapson, J., and Van Schaik, A. (2017). Emnist: Extending mnist to handwritten\nletters. In International Joint Conference on Neural Networks (IJCNN).\nDohare, S., Hernandez-Garcia, J. F., Lan, Q., Rahman, P., Mahmood, A. R., and Sutton, R. S. (2024).\nLoss of plasticity in deep continual learning. Nature, 632(8026):768–774.\nDohare, S., Sutton, R. S., and Mahmood, A. R. (2021). Continual backprop: Stochastic gradient\ndescent with persistent randomness. CoRR, abs/2108.06325v3.\nElsayed, M. and Mahmood, A. R. (2024). Addressing loss of plasticity and catastrophic forgetting in\ncontinual learning. In International Conference on Learning Representations.\nEven, M., Pesme, S., Gunasekar, S., and Flammarion, N. (2023). (s)GD over diagonal linear networks:\nImplicit bias, large stepsizes and edge of stability. In Advances in Neural Information Processing\nSystems.\nGarrigos, G. and Gower, R. M. (2023). Handbook of Convergence Theorems for (Stochastic) Gradient\nMethods. CoRR, abs/2301.11235v3.\nGlorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural\nnetworks. In International Conference on Artificial Intelligence and Statistics.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level\nperformance on imagenet classification. In International Conference on Computer Vision.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In\nConference on Computer Vision and Pattern Recognition.\nHuh, D. (2020). Curvature-corrected learning dynamics in deep neural networks. In International\nConference on Machine Learning.\nIoffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International Conference on Machine Learning.\nJacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization\nin neural networks. Advances in Neural Information Processing Systems.\nKingma, D. P. and Ba, J. (2015). Adam: A Method for Stochastic Optimization. In International\nConference on Learning Representations.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan,\nJ., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in\nneural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526.\nKleinman, M., Achille, A., and Soatto, S. (2024). Critical learning periods emerge even in deep linear\nnetworks. In International Conference on Learning Representations.\n11\nPreprint\nKonidaris, G., Osentoski, S., and Thomas, P. (2011). Value function approximation in reinforcement\nlearning using the fourier basis. In AAAI Conference on Artificial Intelligence.\nKrizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report,\nUniversity of Toronto.\nKumar, S., Marklund, H., Rao, A., Zhu, Y., Jeon, H. J., Liu, Y., and Van Roy, B. (2023a). Continual\nLearning as Computationally Constrained Reinforcement Learning. CoRR, abs/2307.04345.\nKumar, S., Marklund, H., and Roy, B. V. (2023b). Maintaining plasticity via regenerative regulariza-\ntion. CoRR, abs/2308.11958v1.\nKunin, D., Raventós, A., Dominé, C., Chen, F., Klindt, D., Saxe, A., and Ganguli, S. (2024). Get rich\nquick: exact solutions reveal how unbalanced initializations promote rapid feature learning. CoRR,\nabs/2406.06158v1.\nLe, Y. and Yang, X. (2015). Tiny imagenet visual recognition challenge.\nLeCun, Y., Cortes, C., and Burges, C. (1998). MNIST handwritten digit database. ATT Labs [Online].\nAvailable: http://yann.lecun.com/exdb/mnist.\nLee, H., Cho, H., Kim, H., Kim, D., Min, D., Choo, J., and Lyle, C. (2024). Slow and steady wins\nthe race: Maintaining plasticity with hare and tortoise networks. In International Conference on\nMachine Learning.\nLee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. (2019).\nWide neural networks of any depth evolve as linear models under gradient descent. Advances in\nNeural Information Processing Systems.\nLewandowski, A., Kumar, S., Schuurmans, D., György, A., and Machado, M. C. (2024). Learning\nContinually by Spectral Regularization. CoRR, abs/2406.06811v1.\nLi, A. C. and Pathak, D. (2021). Functional regularization for reinforcement learning via learned\nfourier features. In Advances in Neural Information Processing Systems.\nLiu, Y., Kuang, X., and Roy, B. V. (2023).\nA Definition of Non-Stationary Bandits.\nCoRR,\nabs/2302.12202v2.\nLyle, C., Rowland, M., and Dabney, W. (2022). Understanding and preventing capacity loss in\nreinforcement learning. In International Conference on Learning Representations.\nLyle, C., Zheng, Z., Khetarpal, K., van Hasselt, H., Pascanu, R., Martens, J., and Dabney, W. (2024).\nDisentangling the Causes of Plasticity Loss in Neural Networks. CoRR, abs/2402.18762v1.\nLyle, C., Zheng, Z., Nikishin, E., Avila Pires, B., Pascanu, R., and Dabney, W. (2023). Understanding\nplasticity in neural networks. In International Conference on Machine Learning.\nMavor-Parker, A. N., Sargent, M. J., Barry, C., Griffin, L., and Lyle, C. (2024). Frequency and Gener-\nalisation of Periodic Activation Functions in Reinforcement Learning. CoRR, abs/2407.06756v1.\nNacson, M. S., Ravichandran, K., Srebro, N., and Soudry, D. (2022). Implicit bias of the step size in\nlinear diagonal neural networks. In International Conference on Machine Learning.\nParascandolo, G., Huttunen, H., and Virtanen, T. (2017). Taming the waves: sine as activation\nfunction in deep neural networks.\nParisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter, S. (2019). Continual lifelong learning\nwith neural networks: A review. Neural networks, 113:54–71.\nRahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines. Advances in\nNeural Information Processing Systems.\nRing, M. B. (1994). Continual learning in reinforcement environments. The University of Texas at\nAustin.\n12\nPreprint\nSaxe, A., McClelland, J., and Ganguli, S. (2014). Exact solutions to the nonlinear dynamics of\nlearning in deep linear neural networks. In International Conference on Learning Represenatations.\nShang, W., Sohn, K., Almeida, D., and Lee, H. (2016). Understanding and improving convolutional\nneural networks via concatenated rectified linear units. In International Conference on Machine\nLearning.\nSokar, G., Agarwal, R., Castro, P. S., and Evci, U. (2023). The dormant neuron phenomenon in deep\nreinforcement learning. In International Conference on Machine Learning.\nTancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoor-\nthi, R., Barron, J., and Ng, R. (2020). Fourier features let networks learn high frequency functions\nin low dimensional domains. Advances in Neural Information Processing Systems.\nThrun, S. (1998). Lifelong learning algorithms. In Learning to Learn, pages 181–209. Springer.\nVan de Ven, G. M., Tuytelaars, T., and Tolias, A. S. (2022). Three types of incremental learning.\nNature Machine Intelligence, 4(12):1185–1197.\nXiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking\nmachine learning algorithms. CoRR, abs/1708.07747.\nXu, B., Wang, N., Chen, T., and Li, M. (2015). Empirical Evaluation of Rectified Activations in\nConvolutional Network. CoRR, abs/1505.00853v2.\nYang, G., Ajay, A., and Agrawal, P. (2022). Overcoming the spectral bias of neural value approxima-\ntion. In International Conference on Learning Representations.\nYang, G., Simon, J. B., and Bernstein, J. (2023). A Spectral Condition for Feature Learning. CoRR,\nabs/2310.17813v2.\nZiyin, L., Li, B., and Meng, X. (2022). Exact solutions of a deep linear network. Advances in Neural\nInformation Processing Systems.\n13\nPreprint\nA\nADDITIONAL DETAILS\nA.1\nASSUMPTIONS FOR TRAINABIILTY OF LINEAR FUNCTION APPROXIMATION\nNeither squared nor cross-entropy loss are µ-strongly convex in general. However, the assumption is\nsatisfied under reasonable conditions in practice and in the problem settings considered in this paper.\nFor regression, denote the features for task τ as Xτ ∈Rd×N where d is the feature dimension and N\nis the sample size. For linear function approximation, the Hessian is the outer products of the data\nmatrix, ∇2\nθJreg\nτ\n(θ) = XτX⊤\nτ ∈Rd×d. Thus, the squared loss is strongly-convex if the data is full\nrank. This is satisfied in high dimensional image classification problems, which is what we consider.\nFor binary classification, the Hessian involves an additional diagonal matrix of the predictions for\neach datapoint,\n∇2\nθJclass\nτ\n(θ) = XτDτX⊤\nτ ∈Rd×d,\nwhere Dτ = Diag(p1,τ, . . . , pN,τ), pi,τ = 2σ(fθ(xi,τ))(1 −σ(fθ(xi,τ))), and σ is the sigmoid\nfunction. If the prediction becomes sufficiently confident, σ(fθ(xi)) = 1, then there can be rank\ndeficiency in the Hessian. However, because each task is only budgeted a finite number of iterations\nthis bounds the predictions away from 1.\nA.2\nRELATED WORK REGARDING TRAINABILITY OF DEEP LINEAR NETWORKS\nSome authors have suggested deep linear networks suffer from a related issue, namely that critical\nlearning periods also occur for deep linear networks (Kleinman et al., 2024). Unlike the focus on loss\nof trainability in this work where the entire network is trained, these critical learning periods are due\nto winner-take-all dynamics due to manufactured defects in one half of the linear network, for which\nthe other half compensates.\nFinally, we note that some previous work have found that gradient dynamics have a low rank bias for\ndeep linear networks (Chou et al., 2024). One important assumption that these works make is that\nthe neural network weights are initialized identically across layers, θj = αθ1. Our analysis assumes\nthat the initialization uses small random values, such as those used in practice with common neural\nnetwork initialization schemes (Glorot and Bengio, 2010; He et al., 2015).\nA.3\nDETAILS FOR DEEP LINEAR SETUP\nTo simplify notation, without loss of generality, we consider a deep linear network without the bias\nterms, θ = {θ1, . . . , θL}, and fθ(x) = θLθL−1 · · · θ1x. We denote the product of weight matrices,\nor simply product matrix, as ¯θ = θLθL−1 · · · θ1, which allows us to write the deep linear network\nin terms of the product matrix: fθ(x) = ¯θx. The problem setup we use for the deep linear analysis\nfollows previous work (Huh, 2020), and we provide additional details in Appendix A.3. We consider\nthe squared error, Jτ(θ) = E(x,y)∼pτ\n\u0002\n∥y −¯θx∥\n\u00032\n2. and we assume that the observations are whitened\nto simplify the analysis,\nΣx = E\n\u0002\nxx⊤\u0003\n= I, focusing on the case where the targets y are changing during continual learning.\nThen we can write the squared error as\nJ(θ) = Tr\n\u0002\n∆τ∆⊤\nτ\n\u0003\n,\nwhere ∆τ = θ⋆\nτ −¯θ is the distance to the optimal linear predictor, θ⋆\nτ = Σyx,τ = Ex,y∼pτ [yx⊤]Σx.\nThe convergence of gradient descent for general deep linear networks requires an assumption on the\ndeficiency margin, which is used to ensure that the solution found by a deep linear network, in terms\nof the product matrix, is full rank (Arora et al., 2019). That is, the deep linear network converges if\nthe minimum singular value of the product matrix stays positive, σmin(¯θ) > 0.\nWe now show that a diagonal linear network maintains a positive minimum singular value under\ncontinual learning. This is a simplified setting for analysis, where we assume that the weight matrices\nare diagonal and thus the input, hidden, and output dimension are all equal. Let fθ(x) be a diagonal\nlinear network, defined by a set of diagonal weight matrices, θl = Diag(θl,1, . . . , θl,d). The output of\nthe diagonal linear network is the product of the diagonal matrices, fθ(x) = θLθL−1 . . . θ1x. Then\n14\nPreprint\nthe product matrix is also a diagonal matrix, whose diagonals are the products of the parameters\nof each layer, ¯θ = Diag(QL\nl=1 θl,1, . . . , QL\nl=1 θl,d) := Diag(¯θ1, . . . , ¯θd). The minimum singular\nvalue of a diagonal matrix is the minimum of its absolute values, σmin(¯θ) = mini |¯θi|. Thus, we\nmust show that the minimum absolute value of the product matrix is never zero.\nLemma 1. Consider a deep diagonal linear network, fθ(x) = θLθL−1 . . . θ1x and θl\n=\nDiag(θl,1, . . . , θl,d). Then, under gradient descent dynamics, θ(t)\nl,i = θ(t)\nl′,i iff θ(0)\nl,i = θ(0)\nl′,i for l′ ̸= l.\nThe proof of this proposition, and the next, can be found in Appendix B. This first proposition states\nthat two parameters that are initialized to different values, such as by a random initialization, will\nnever have the same value under gradient descent. Conversely, if the parameters are initialized\nidentically, then they will stay the same value under gradient descent. This means that, in particular,\ntwo parameters will never be simultaneously zero.\nLemma 2. Denote a deep diagonal linear network as fθ(x) = Diag(¯θ1, . . . , ¯θd)x where ¯θi =\nQL\nl=1 θl,i. Then, under gradient descent dynamics, ¯θ(t)\ni\n= ¯θ(t+1)\ni\n= 0 iff two (or more) components\nare zero, θ(t)\nl,i = θ(t)\nl′,i = 0, for l′ ̸= l.\nWhile the analysis considers a special case of deep linear networks, namely deep diagonal networks,\nwe note that this is a common setting for the analysis of deep linear networks more generally (Nacson\net al., 2022; Even et al., 2023). In particular, the analysis is motivated by the fact that, under certain\nconditions, the evolution of the deep linear network parameters can be analyzed through the indepen-\ndent singular mode dynamics (Saxe et al., 2014), which simplify the analysis of deep linear networks\nto deep diagonal linear networks. The target function being learned, y⋆(x) = θ⋆x, is represented in\nterms of the singular-value decomposition, θ⋆= U ⋆S⋆V ⋆= Pr\nj=1 siuiv⊤\ni . We also assume that the\nneural network has a fixed hidden dimension, so that θ1 ∈Rd×din, θL ∈Rdout×d, θ1<l<L ∈Rd×d;\nand we apply the singular value decomposition to the function approximator’s parameters, θl =\nUlSlVl ∈Rdout×dh. To simplify the product of weight matrices, we assume Vi+1 = Ui, V1 = V ⋆,\nand UL = U ⋆. The simplifying result is that the squared error loss can be expressed entirely in terms\nof the singular values, ∥y⋆x −Q1\ni=L θix∥2 ∝∥S⋆−Q1\ni=L Sl∥2, which is equivalent to our analysis\nof the deep diagonal network, as the matrix of singular values is a diagonal matrix. These decoupled\nlearning dynamics are closely approximated by networks with small random weights and they persist\nunder gradient flows (Huh, 2020).\n15\nPreprint\nB\nPROOFS\nProof of Theorem 1. We first present the result for two tasks and we then generalize it to an arbitary\nnumber of tasks. Let the linear weights learned on the first task be θ(T ), with the corresponding\nunique global minimum denoted by θ⋆\n1. The solution found on the first task is used as an initialization\non the second task, which will end at θ(2T ), with the corresponding unique global minimum denoted\nby θ⋆\n2. We start from the known suboptimality gap for gradient descent on the second task (Garrigos\nand Gower, 2023):\nJ2(θ(2T )) −J2(θ⋆\n2) < ∥θ⋆\n2 −θ(T )∥2\nαT\n.\n(1)\nWe upper bound the distance from the initialization on the second task, θ(T ), to the optimum, θ⋆\n2, by\n∥θ⋆\n2 −θ(T )∥2 < ∥θ⋆\n2 −θ⋆\n1∥2 + ∥θ⋆\n1 −θ(T )∥2 < ∥θ⋆\n2 −θ⋆\n1∥2 + (1 −αµ)T ∥θ⋆\n1 −θ0∥2.\n(2)\nWhere the last inequality uses the assumption that the objective function is µ-strongly convex. We\nupper bound the suboptimality gap on the second task by a quantity independent of θ(T ):\nJ2(θ(2T )) −J2(θ⋆\n2) < ∥θ⋆\n2 −θ(T )∥2\nαT\n< ∥θ⋆\n2 −θ⋆\n1∥2 + (1 −αµ)T ∥θ⋆\n1 −θ0∥2\nαT\n,\n(3)\nwhich implies that the parameter value learned on the previous task does not influence training on the\nnew task beyond a dependence on the initial distance. This is true for an arbitrary number of tasks:\nJτ(θ(τT )) −Jτ(θ⋆\nτ) <\nPτ\nk=1(1 −αµ)T (k−τ)∥θ⋆\nk −θ⋆\nk−1∥2\nαT\n<\n2D(1 −αµ)T\nαT(1 −(1 −αµ)T ),\n(4)\nwhere we denote θ⋆\n0 = θ0. The last inequality follows from our assumption that the distance between\nthe task solutions, ∥θ⋆\nk −θ⋆\nk−1∥2 < 2D, is bounded and using a geometric sum in (1 −αµ)T .\nProof of Lemma 1. ⇒We first prove the lemma in the forward direction:\nAssuming that θ(t)\nl,i = θ(t)\nl′,i for l′ ̸= l, we will show that θ(t−1)\nl,i\n= θ(t−1)\nl′,i\n.\nWriting the gradient update for θ(t)\nl,i with a fixed step-size α, we have that\nθ(t)\nl,i = θ(t−1)\nl,i\n−α∇θl,iJ(θ)\n(5)\n= θ(t−1)\nl,i\n−α∇fθℓ(fθ(x), y)∇θl,ifθ(x)\n(6)\n= θ(t−1) −α∇fθℓ(fθ(x), y)∇θl,i\nL\nY\nj=1\nθ(t−1)\nj,i\nx\n(7)\n= θ(t−1)\nl,i\n−α∇fθℓ(fθ(x), y)\nY\nj̸=l\nθ(t−1)\nj,i\nx.\n(8)\n(9)\nSimilarly, the gradient update for θl′,i is\nθ(t)\nl′,i = θ(t−1)\nl′,i\n−α∇fθℓ(fθ(x), y)\nY\nj̸=l′\nθ(t−1)\nj,i\nx\n(10)\n(11)\nUsing our assumption that θ(t)\nl,i = θ(t)\nl′,i, we set the two updates equal to eachother:\nθ(t−1)\nl,i\n−α∇fθℓ(fθ(x), y)\nY\nj̸=l\nθ(t−1)\nj,i\nx = θ(t−1)\nl′,i\n−α∇fθℓ(fθ(x), y)\nY\nj̸=l′\nθ(t−1)\nj,i\nx.\n(12)\n16\nPreprint\nWe can simplify both sides of the equations, where the LHS is\nθ(t−1)\nl,i\n−α∇fθℓ(fθ(x), y)\nY\nj\nθ(t−1)\nj,i\nx\nθ(t−1)\nl′,i\n(13)\n= θ(t−1)\nl,i\n\n1 −α∇fθℓ(fθ(x), y)\nY\nj\nθ(t−1)\nj,i\nx\nθ(t−1)\nl′,i\nθ(t−1)\nl,i\n\n.\n(14)\nSimilarly, the RHS of the equation is\nθ(t−1)\nl′,i\n\n1 −α∇fθℓ(fθ(x), y)\nY\nj\nθ(t−1)\nj,i\nx\nθ(t−1)\nl′,i\nθ(t−1)\nl,i\n\n.\n(15)\nNotice that both expressions in the parenthesis on the LHS and RHS are equal. Thus, θ(t−1)\nl′,i\n= θ(t−1)\nl,i\n⇐The reverse direction follows directly by following the above argument in reverse.\nProof of Lemma 2. ⇒We first prove the lemma in the forward direction:\nAssuming that ¯θ(t+1)\ni\n= ¯θ(t)\ni\n= 0, we will show that θ(t)\nl,i = θ(t)\nl′,i = 0.\nWe proceed by contradiction, and assume that only a single component is zero, that is θ(t)\nl′,i = 0 and\nθ(t)\nl,i ̸= 0 for l ̸= l′. We will show that the gradient update will ensure that θ(t+1)\ni\n̸= 0\nFirst, consider the update to θ(t)\nl′,i,\nθ(t+1)\nl′,i\n= θ(t)\nl′,i −α∇fθℓ(fθ(x), y)\nY\nj̸=l′\nθ(t−1)\nj,i\nx\n(16)\n= −α∇fθℓ(fθ(x), y)\nY\nj̸=l′\nθ(t−1)\nj,i\nx\n(17)\nBecause we assumed that θ(t)\nl,i ̸= 0 for l ̸= l′, we have that Q\nj̸=l θ(t−1)\nj,i\n̸= 0. Thus θ(t+1)\nl′,i\n̸= 0\nNext consider the update to θ(t)\nl,i ,\nθ(t+1)\nl,i\n= θ(t)\nl′,i −α∇fθℓ(fθ(x), y)\nY\nj̸=l\nθ(t−1)\nj,i\nx\n(18)\n= θ(t)\nl′,i\n(19)\nWhere the last line follows from the fact that Q\nj̸=l θ(t−1)\nj,i\n= 0 because θ(t)\nl′,i = 0.\nThus, we have shown that θ(t+1)\nl,i\n̸= 0 for all l, and hence, ¯θ(t+1) ̸= 0 which is a contradiction.\n⇐The reverse direction follows from the assumption directly. If two components are both equal\nto zero, θ(t)\nl,i = θ(t)\nl′,i = 0, then every sub-product is zero, Q\nj̸=l θ(t−1)\nj,i\nand so is the entire product,\nQL\nj=1 θ(t−1)\nj,i\n.\nProof of Theorem 2. We now show that a diagonal linear network maintains a positive minimum\nsingular value under continual learning. This is a simplified setting for analysis, where we assume\n17\nPreprint\nthat the weight matrices are diagonal and thus the input, hidden, and output dimension are all\nequal. Let fθ(x) be a diagonal linear network, defined by a set of diagonal weight matrices, θl =\nDiag(θl,1, . . . , θl,d). The output of the diagonal linear network is the product of the diagonal matrices,\nfθ(x) = θLθL−1 . . . θ1x. Then the product matrix is also a diagonal matrix, whose diagonals are the\nproducts of the parameters of each layer, ¯θ = Diag(QL\nl=1 θl,1, . . . , QL\nl=1 θl,d) := Diag(¯θ1, . . . , ¯θd).\nThe minimum singular value of a diagonal matrix is the minimum of its absolute values, σmin(¯θ) =\nmini |¯θi|. Thus, we must show that the minimum absolute value of the product matrix is never zero.\nThis follows immediately from Lemma 1 and Lemma 2. Taken together, these two lemmas state that\nwith a random initialization and under gradient dynamics, a diagonal linear network will not have\nmore than one parameter equal to zero. This means that the minimum singular value of the product\nmatrix will never be zero. Thus, we have shown that a diagonal linear network trained with gradient\ndescent, if initialized appropriately, will be able to converge on any given task in a sequence.\nProof of Proposition 1. We prove this by considering the remainder of a Taylor series on the given\ninterval. Due to periodicity of sin(z) and cos(z), we can consider z ∈[−π, π] without loss of\ngenerality. We can further consider two cases, either z ∈[−π, −3π/4] ∪[−π/4, π/4] ∪[3π/4, π] or\nh ∈[−3π/4, −π/4] ∪[π/4, 3π/4]. In the first case, z is near a critical point of cos(z) and in the second\ncase z is near a critical point of sin(z).\nWe focus on a particular subcase, where z ∈[−π/4, π/4], which is close to a critical point of cos(z),\nbut far from a critical point of sin(h) (the other cases follow a similar argument).\nBecause we know that z ∈[−π/4, π/4], by Taylor’s theorem it follows that sin(z) = z+R1,0(z), where\nR1,0(z) = sin(2)(c)\n2\nz2 is the 1st degree Taylor remainder centered at a = 0 for some c ∈[−π/4, π/4].\nIn the case of a sinusoid, this can be upperbounded, |R1,0(z)| = | −sin(c)\n2\nz2| <\n1\n8\n√\n2(π/4)2, using the\nfact that |z| < π/4 and sin(c) < 1/\n√\n2.\nThus, when cos(z) is close to a critical point, sin(z) is approximately linear. A similar argument\nholds for the other case, when sin(z) is close to a critical point, cos(z) is approximately linear. In\nthis other case, the error incurred is the same.\nProof of Corollary 1. We prove this claim using induction.\nBase case: We want to show that a single layer that outputs Fourier features embeds a deep linear\nnetwork. Using Proposition 1, there exists one unit for each pre-activation that is approximately linear.\nBecause each pre-activation is used in an approximately-linear unit, the single layer approximately\nembeds a deep linear network using all of its parameters.\nInduction step: Assume a deep Fourier network with depth L −1 embeds a deep linear network, we\nprove that adding an additional deep Fourier layer retains the embedded deep linear network. There\nare two cases to consider, corresponding to the units of the additional deep Fourier layer which are\napproximately-linear and the other units that are not approximately-linear\nCase 1 (approximately-linear units): For the additional deep Fourier layer, the set of approximately-\nlinear units already embeds a deep linear network. Because linearity is closed under composition,\nthe composition of the additional deep Fourier layer and the deep Fourier network with depth L −1\nsimply adds an additional linear layer to the embedded deep linear network, increasing its depth to L.\nCase 2 (other units): For the units that are not well-approximated by a linear function, we can treat\nthem as if they were separate inputs to the deep Fourier network with depth L −1. The network’s\nparameters associated with those inputs are, by the inductive hypothesis, already embedded in the\ndeep linear network.\nNote that case 1 embeds the parameters of the additional deep Fourier layer into the deep Fourier\nnetwork. Case 2 states that the parameters of the network associated with the nonlinear units of the\nadditional deep Fourier layer are already embedded in the deep Fourier network by construction.\nThus, a neural network composed of deep Fourier layers embeds a deep linear network.\n18\nPreprint\nC\nEMPIRICAL DETAILS\nAll of our experiments use 10 seeds and we report the standard error of the mean in the figures.\nThe optimiser used for all experiments was Adam, and after a sweep on each of the datasets over\n[0.005, 0.001, 0.0005], we found that α = 0.0005 was most performant.\nWe used the Adam optimizer (Kingma and Ba, 2015) for all experiments, settling on the default\nlearning rate of 0.001 after evaluating [0.005, 0.001, 0.0005]. Results are presented with standard\nerror of the mean, indicated by shaded regions, based on 10 random seeds.\nDataset specifications and non-stationarity conditions:\n• For MNIST, Fashion MNIST and EMNIST: we use a random sample of 25600 of the\nobservations and a batch size of 256 (unless otherwise indicated, such as the linearly\nseparable experiment).\n• For CIFAR10 and CIFAR1100: Full 50000 images for training, 1000 test images for valida-\ntion, rest for testing. The batch size used was 250. Random label non-stationarity: 20 epochs\nper task, 30 tasks total. Labelnoise non-stationarity: 80 epochs, 10 tasks. Class incremental\nlearning: 6000 iterations per task, 80 tasks. Note that the datasets on different tasks in the\nclass incremental setting can have different sizes, and so epochs are not comparable.\n• tiny-ImageNet: All 100000 images for training, 10000 for validation, 10000 for testing as\nper predetermined split. The batch size used was 250. Random label non-stationarity: 20\nepochs per task, 30 tasks total. Pixel permutation non-stationarity: 60 epochs, 100 tasks.\nClass incremental learning: 10000 iterations per task, 80 tasks. Note that the datasets on\ndifferent tasks in the class incremental setting can have different sizes, and so epochs are not\ncomparable.\nNeural Network Architectures\nFor tiny-ImageNet, CIFAR10, CIFAR100, and SVHN2: We\nutilized standard ResNet-18 with batch normalization and a standard tiny Vision Transformer. The\nsmaller datasets use an MLP with different widths and depths, as specified in the scaling section.\n19\nPreprint\nFigure 8: Trainability across different datasets and epochs per tasks. Nonlinear networks lose\ntheir trainability, whereas adaptively-linear networks improve and sustain their trainability\nD\nADDITIONAL EXPERIMENTS\nThese additional experiments validate the benefits of adaptive-linearity as a means of improving\ntrainability. The experiments use the following datasets for continual supervised learning: MNIST\n(LeCun et al., 1998), Fashion MNIST (Xiao et al., 2017), and EMNIST (Cohen et al., 2017). We focus\nprimarily on the problem of trainability, and thus consider random label non-stationarity, in which\nthe labels are randomly assigned to each observation and must be memorized on each task. This\ntype of non-stationarity is particular difficulty in sustaining trainability in continual learning (Lyle\net al., 2023; Kumar et al., 2023b). We compare our adaptively-linear network against a corresponding\nnonlinear feed-forward neural network with ReLU activations with the same depth. Because the\nadaptively-linear network uses a concatenation of two different activation functions, the adaptively-\nlinear network has half the width of the nonlinear network and less parameters, which provides an\nadvantage to the nonlinear baseline.\nD.1\nADAPTIVELY-LINEAR NETWORKS ARE HIGHLY TRAINABLE\nThe main result of this appendix is presented in Figure 8. Across different datasets, almost-linear\nnetworks are highly trainable, either achieving high accuracy and maintaining it on easier tasks, such\nas MNIST, or improving their trainability on new tasks, such as on Fashion MNIST. In contrast, the\nnonlinear network suffered from loss of trainability in each of the problems that we studied. This is\nnot surprising, as loss of trainability is a well-documented issue for nonlinear networks without some\nadditional method designed to mitigate it (Dohare et al., 2021; Lyle et al., 2022; Kumar et al., 2023b;\nElsayed and Mahmood, 2024).\n20\nPreprint\nFigure 9: Hyperparameter Sensitivity Analysis. Adaptively-linear networks seem to not benefit\nfrom regularization. While nonlinear networks are more trainable with regularization, their perfor-\nmance is still worse than the adaptively-linear network.\nFigure 10: Comparison of trainability with Layer Normalization. Nonlinear networks are more\ntrainable with Layer Normalization, but adaptively-linear networks learn faster and achieve better\naccuracy, particularly with linearized Layer Norm.\nD.2\nMETHODS FOR IMPROVING TRAINABILITY\nGiven that a nonlinear network is unable to maintain its trainability in isolation, we investigate\nwhether recently proposed methods for mitigating loss of trainability are able to make up for the dif-\nference in performance between an adaptively linear network and a nonlinear network. We investigate\ntwo categories of mitigators for loss of plasticity: (i) regularization and (ii) normalization layers.\nRegularization\nLoss of plasticity occurs in nonlinear networks when they are not regularized.\nThus, we compare the performance of the nonlinear network and the adaptively-linear network with\nvarying regularization strengths. In particular, we use the recently proposed L2 regularization towards\nthe initialization (Kumar et al., 2023b), because it addresses the issue of sensitivity towards zero\ncommon to L2 regularization towards zero. In Figure 9, we find that regularization does improve\nthe trainability of nonlinear networks, validating previous empirical findings. However, we found\nthat almost-linear networks do not benefit substantially from regularization. That is, almost-linear\nnetwork with a smaller regularization strength always outperformed the nonlinear network.\nLayer Normalization\nTraining deep neural networks typically involve normalization layers, either\nBatch Normalization (Ioffe and Szegedy, 2015) or Layer Normalization (Ba et al., 2016). Recently, it\nwas demonstrated that layer normalization is an effective mitigator for loss of trainability (Lyle et al.,\n2024). We investigate whether trainability can be improved with the addition of normalization layers,\nfor both the nonlinear and adaptively-linear network. In Figure 10, we found that layer normalization\nincreases performance but that loss of trainability can still occur with a nonlinear network. In addition\nto Layer Normalization, we also tried a linear version of LayerNorm which uses a stop-gradient\non the standard deviation to maintain linearity, which improved training speed in some instances.\n21\nPreprint\nFigure 11: Scaling Neural Network Width and Depth. (Top) Due to the concatenation used by the\nactivation function in adaptively-linear networks, they scale particularly well with width. (Bottom)\nDeeper adaptively-linear networks also lead to improved average end of task performance.\nD.3\nSCALING PROPERTIES OF ALMOST-LINEAR NETWORKS\nWidth Scaling\nAnother source of linearity recently proposed is an increasing width of the neural\nnetwork, causing their parameter dynamics evolves as linear models in the limit (Lee et al., 2019). We\ninvestigate whether an increase in width can close the gap between the trainability of the nonlinear\nnetwork and the almost-linear network. In Figure 11 (Top), we found that adaptively-linear networks\nscale particularly well with width, whereas width seems to have little effect on the trainability of\nnonlinear networks. Thus, our results suggest that increasing the width of a neural network does not\nnecessarily impact its trainability, at least not to the width values we considered.\nDepth Scaling\nNeural networks in supervised learning tend to scale with depth, allowing them to\nlearn more complex predictions. We investigate whether the depth scaling of almost-linear networks\nalso leads to similar improvements in continual learning. In Figure 11 (Bottom), we found that\nadaptively-linear networks do improve with additional depth, but the degree of improvement was not\nas pronounced as scaling the width.\n22\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-10-27",
  "updated": "2024-10-27"
}