{
  "id": "http://arxiv.org/abs/2410.20817v1",
  "title": "The Zeno's Paradox of `Low-Resource' Languages",
  "authors": [
    "Hellina Hailu Nigatu",
    "Atnafu Lambebo Tonja",
    "Benjamin Rosman",
    "Thamar Solorio",
    "Monojit Choudhury"
  ],
  "abstract": "The disparity in the languages commonly studied in Natural Language\nProcessing (NLP) is typically reflected by referring to languages as low vs\nhigh-resourced. However, there is limited consensus on what exactly qualifies\nas a `low-resource language.' To understand how NLP papers define and study\n`low resource' languages, we qualitatively analyzed 150 papers from the ACL\nAnthology and popular speech-processing conferences that mention the keyword\n`low-resource.' Based on our analysis, we show how several interacting axes\ncontribute to `low-resourcedness' of a language and why that makes it difficult\nto track progress for each individual language. We hope our work (1) elicits\nexplicit definitions of the terminology when it is used in papers and (2)\nprovides grounding for the different axes to consider when connoting a language\nas low-resource.",
  "text": "The Zeno’s Paradox of ‘Low-Resource’ Languages\nHellina Hailu Nigatu1, *\nAtnafu Lambebo Tonja2,3,\nBenjamin Rosman 3,4, †\nThamar Solorio 2,5, †\nMonojit Choudhury 2,†\nCorresponding author: hellina_nigatu@berkeley.edu\n1 UC Berkeley, USA, 2 MBZUAI, UAE, 3 Lelapa AI, South Africa\n4 RAIL Lab - University of the Witwatersrand, South Africa, 5 University of Houston, Houston, USA\nAbstract\nThe disparity in the languages commonly stud-\nied in Natural Language Processing (NLP) is\ntypically reflected by referring to languages\nas low vs high-resourced. However, there is\nlimited consensus on what exactly qualifies as\na ‘low-resource language.’ To understand how\nNLP papers define and study ‘low resource’ lan-\nguages, we qualitatively analyzed 150 papers\nfrom the ACL Anthology and popular speech-\nprocessing conferences that mention the key-\nword ‘low-resource.’ Based on our analysis, we\nshow how several interacting axes contribute to\n‘low-resourcedness’ of a language and why that\nmakes it difficult to track progress for each indi-\nvidual language. We hope our work (1) elicits\nexplicit definitions of the terminology when it\nis used in papers and (2) provides grounding for\nthe different axes to consider when connoting\na language as low-resource.\n1\nIntroduction\nIf the fleet-footed Achilles and a slow-\nmoving tortoise are in a race, Achilles\nwill never catch the tortoise if the tor-\ntoise has a head start. Regardless of how\nfast Achilles runs, he first has to reach\na point the tortoise already passed, by\nwhich point the tortoise will have moved\nahead. –Zeno’s Achilles Paradox 1\nThe majority of research in the NLP commu-\nnity has focused on only a handful of the world’s\nlanguages (Joshi et al., 2020; Bird, 2022). Partic-\nularly, languages spoken by communities in the\nGlobal South have largely been neglected (Nekoto\net al., 2020; Schwartz, 2022). Languages under-\nstudied by the NLP community are usually referred\nto as ‘low-resource’, while those well-studied are\n* Work done while this author was at MBZUAI.\n† These authors provided equal advice and supervision.\n1https://www.britannica.com/topic/Achilles-paradox\nreferred to as ‘high-resource.’ This framing of\nhigh vs low-resource languages resembles Zeno’s\nAchilles paradox: ‘high-resourced languages’ are\nthe tortoise, that have been given a head start in the\nresearch community and continue to receive much\nof the attention, and ‘low-resource languages’ are\nAchilles. In reality, Achilles can always outrun the\ntortoise2. However, the face value interpretation of\nthe paradox can serve as an analogy for how the\ncurrent trajectory of the NLP research community\nto include majority of the worlds languages in the\npath already forged for ‘high-resourced’ languages\nleaves ‘low-resource languages’ constantly trying\nto catch up to a goalpost that is always moving.\nThe disparity in research and performance of\nlanguage technologies across languages can be\na double-edged sword. On the one hand, under-\nstudied and underserved languages may be at a\nhigher risk of language loss and have speakers ex-\nposed to direct downstream harm due to failures\nof language technologies (Nigatu and Raji, 2024;\nChoudhury, 2023). On the other hand, the drive to\ninclude these languages in research without proper\nconsideration of community needs (1) may lead\nto aggressive–and at times exploitative–data col-\nlection and (2) result in technologies that do not\nmeet the needs of the communities who speak those\nlanguages (Diddee et al., 2022; Le Ferrand et al.,\n2022a; Dearden and Tucker, 2021).\nRecently, we have seen efforts to increase the\nrepresentation of ‘low-resource languages’ in NLP\nresearch (e.g. NLLB, 2024; Adelani et al., 2022).\nYet, the exact definition of the term ‘low-resource’\nremains elusive3. A common criterion to connote\nlanguages as ‘low’ vs ‘high’ resourced is data.\nHowever, using data as the only criterion oversim-\n2https://ibmathsresources.com/2018/11/30/zenos-\nparadox-achilles-and-the-tortoise-2/\n3‘Under-resource’ is a term used interchangeably–and per-\nhaps equally as ambiguously–with ‘low-resource.’ For brevity,\nwe mainly use the phrase ‘low-resource’ in this paper.\narXiv:2410.20817v1  [cs.CL]  28 Oct 2024\nplifies the context of the language itself. Languages\ndubbed as ‘low-resource’ may vary depending on\nfactors like their number of speakers, non-digital\narchives, or language experts (Kuhn, 2024).\nThe lack of consensus in what qualifies a lan-\nguage as ‘low-resource’ makes it challenging to\n(1) track progress in research and development for\n‘low-resource languages’ in general, (2) determine\nwhat interventions are effected towards a language,\n(3) pinpoint when a language stops being ‘low-\nresource’, and (4) discern if technologies built for\nthese languages truly address the needs of the com-\nmunities who speak them or if they are built simply\non the premise that the same technology exists for\na ‘higher resourced language.’\nIn this work, we survey papers that study lan-\nguages coined as ‘low-resource’. We qualitatively\nanalyzed 150 papers that include the keywords\n‘low-resource’ and ‘under-resource.’ We used qual-\nitative methods to unravel (1) how such papers\ndefine the term ‘low-resource’ or ‘under-resource’,\n(2) what languages are studied as ‘low-resource’,\nand (3) what criteria is used to classify a language\nas ‘low-resource.’\nOur analysis reveals four separate but interacting\naspects of ‘resourcedness’ that are used to connote\na language as ‘low-resource’ (see Section 3 & Sec-\ntion 4). In Section 5, we use real-world examples\nto demonstrate how each of the aspects interact\nand how those interactions impact what interven-\ntions are designed and implemented for a language.\nFinally, we use our analysis to ground recommen-\ndations for different stakeholders (see Section 6).\n2\nMethodology\nData\nWe collected data for papers published at\n*CL venues4 from the ACL Anthology5 and at the\nfollowing Speech Processing conferences: INTER-\nSPEECH and International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) us-\ning the Semantic Scholar (Kinney et al., 2023) API .\nWe used a keyword search to identify papers that in-\nclude the terms ‘low-resource’ or ‘under-resource’\nin their titles or abstracts. Our final corpus included\n868 unique papers.\nQualitative Analysis\nIn the initial stage of our\nanalysis, we found that the term ‘low-resource’ is\n4We focused on the top 6 venues based on Google\nScholar metrics for computational linguistics( https:\n//scholar.google.com/citations?view_op=top_\nvenues&hl=en&vq=eng_computationallinguistics)\n5https://github.com/acl-org/acl-anthology\nused to refer to three broad categories: (1) tasks and\ndomains where there is a lack of labeled data, (2)\n‘simulated low-resource’ settings via methods like\nunder-sampling , (3) ‘low-resource languages’ de-\nfined based on diverse criteria. Table 1 summarizes\nthis finding. For our qualitative analysis, we ex-\nclusively focused on the third category, i.e., papers\nthat study ‘low-resource languages’ as our inter-\nest is in understanding how a language is labeled\nas low-resource. We also found papers that tried\nboth sampling higher-resourced languages and us-\ning actual, low-resourced languages (e.g. Zevallos\nand Bel, 2023b). We include those in our analysis\nas they study a ‘low-resourced language’ in addi-\ntion to a simulated setting. We manually labeled\n541 papers to identify those that explicitly work\non non-simulated low-resource languages and ran-\ndomly sampled 150 papers for qualitative analysis.\nOur sampling strategy was independent of any pa-\nrameter such as publication year; the time span\nfor the 150 papers was 2017-2023. We conducted\nour analysis by reading each paper and annotating\nhow the term ‘low-resource’ or ‘under-resource’\nis defined, what languages are studied in the pa-\nper, and any additional challenges mentioned in\nthe paper in relation to the languages of study be-\ning ‘low-resource.’ We used inductive thematic\nanalysis (Braun and Clarke, 2006) and discussed\nthe themes that emerged from our analysis in fre-\nquent meetings to synthesize overarching themes.\nIn the following section, we present the results of\nour analysis along with illustrative quotes.\nCategory\nDescription\nExamples\n%\nTasks and\nDomains\ntasks\nand\ndo-\nmains\nwhere\nthere is limited\nlabeled data\nSun\net\nal.\n(2022a);\nBajaj et al.\n(2021)\n27.27\nSimulated\nusing\ntech-\nniques\nlike\nunder-sampling\nto\nsimulate\nlow-resource\nsettings\nZevallos and\nBel (2023b);\nDehouck\nand Gómez-\nRodríguez\n(2020)\n12.27\nLanguages\nlanguages cate-\ngorized based on\nfactors like data\nor\nnumber\nof\nspeakers\nCoto-Solano\n(2022);\nPonti et al.\n(2021)\n65.04\nTable 1: Three categories of papers returned for the\nkeyword search for ‘low-resource.’ Note that the per-\ncentages do not add up to 100 because some papers fall\ninto more than one category. For instance, Mager et al.\n(2020) study both simulated and actual low-resource\nlanguages.\n3\nWhat is a ‘low-resource’ language?\nIn this section, we present the overarching aspects\nwe found from our thematic analysis. It is first\nimportant to note the different styles papers use\nwhen defining the term ‘low-resource’:\n“Languages facing this lack of large\namount of data are called low-resourced,\nand all linguistic varieties in Mexico\nare struggling with this situation.”\n–\nSierra Martínez et al. (2020)\n“Under-resourced, under-studied and en-\ndangered or small languages yield prob-\nlems for automatic processing and ex-\nploiting because of the small amount of\navailable data as well as the missing or\nsparse description of the languages.” –\nFerger (2020)\n“It frames these as “low resource lan-\nguages,” lacking the text, speech and\nlexical resources that are needed for\ncreating speech and language technolo-\ngies (Krauwer, 2003)”. –Lane and Bird\n(2021a)\nIn the quotes shown above we see that\nSierra Martínez et al. (2020) explicitly define the\nterm, Ferger (2020) describes challenges of work-\ning with low-resource and Lane and Bird (2021a)\ndefine the term and provide citations from prior\nwork. If a paper uses prior work without explicitly\nstating its definition, we rely on the definition of\nthe cited work. In cases where there are no explicit\ndefinitions, we rely on the challenges mentioned\nby the paper to categorize how the paper decides if\na language is ‘low-resource.’\nWe found that definitions for the term ‘low-\nresource’ borrow from four aspects: (1) Socio-\npolitical aspects relating to financial and historical\nconstraints, (2) Resources, both human and digital,\n(3) Artifacts such as linguistic knowledge, data,\nand technological infrastructure, and (4) Agency\nof community members in what technology is built\nfor their languages. We summarize these four as-\npects in Figure 1 and dive into detail about each\naspect in the following subsections.\n3.1\nSocio-Political\nSome papers call out structural issues pertaining to\nsocietal, economic, and political forces. We found\nFigure 1: Four overarching aspects that contribute\nto a language being classified as low-resource. Socio-\npolitical aspects are at the top, influencing both the\navailability of resources and the creation of artifacts.\nCommunity agency is a common thread in all the other\nthree aspects.\npapers that reflect on low-resourcedness due to fi-\nnancial and economic constraints to curating data\n(e.g. Coto-Solano, 2022; Pathak et al., 2022) and\nlimited use of such languages in mainstream me-\ndia, government, and education (e.g. Mehta et al.,\n2020). For example:\n“In many of these communities, lan-\nguages like English and Spanish have\ndisplaced the Indigenous languages in\ndomains such as technology and chatting,\nand so the available data is curtailed.”–\nFeldman and Coto-Solano (2020)\n“However, these languages are not rep-\nresented in education, government, pub-\nlic services, and media, and therefore,\nthey show high levels of endangerment.”–\nSierra Martínez et al. (2020)\n3.2\nResource\nThe second aspect discussed by papers is the avail-\nability of and access to human and digital re-\nsources6.\nHuman Resources\nWe found three types of hu-\nman resources mentioned in papers in relation to\nlow-resource languages: (1) native speakers (e.g.\nFeldman and Coto-Solano, 2020; Leong et al.,\n2022), (2) linguistic experts (e.g. Pathak et al.,\n2022), and (3) NLP researchers (e.g. Yimam et al.,\n6Note that in the context of this work, data is an artifact\ncurated for NLP purposes and so is not referred to as a resource\nin this category.\n2020). With regards to native speakers, while some\nlow-resource languages are described as having a\nlimited number of native speakers, others are de-\nscribed as still being low-resourced despite a large\nnumber of native speakers. For instance:\n“Quechua, a low-resource language from\nSouth America, is a language spoken by\nmillions but, despite several efforts in\nthe past, still lacks the resources neces-\nsary to build high-performance computa-\ntional systems.”–Melgarejo et al. (2022)\n“However, low-resource languages such\nas Amharic have received less attention\ndue to several reasons such as lack of\nwell-annotated datasets, unavailability\nof computing resources, and fewer or no\nexpert researchers in the area.”–Yimam\net al. (2020)\nAccess to Digital Devices and Platforms\nLack\nof access to digital devices–and by extension, the\ndigital presence of communities–is another reason\nmentioned in relation to ‘low-resource’ languages\n(e.g. Bamutura et al., 2020; Nzeyimana and Niy-\nongabo Rubungo, 2022). Mainly, this reason is tied\nto the lack of available digital data for languages\nthat fit the mainstream way of training models. Pa-\npers state that ‘low-resource’ languages are not\navailable in formats suitable for crawls and scrap-\ning (e.g. Feldman and Coto-Solano, 2020).\n“The included low-resource languages\nare also very limited because they are\nmainly sourced from Wikipedia articles,\nwhere languages with few articles like\nKinyarwanda are often left behind.” –\nNzeyimana and Niyongabo Rubungo\n(2022)\n“In addition to this, many Indigenous\ncommunities have chronic digital in-\nequalities, which makes it difficult to\ngenerate crowd-sourcing campaigns for\nthose languages. Finally, in many cases,\nthe data that is most valuable to speakers\nof the language is collected from elders\nand knowledge keepers, but those elders\nmight be the people who have the least\naccess to technological means of commu-\nnication.” –Feldman and Coto-Solano\n(2020)\n3.3\nArtifacts\nThe third aspect of resourcedness is tied to the\nproduction and accessibility of artifacts: linguistic\nknowledge, data, and technology.\nLinguistic Features and Descriptions\nPapers\nstate how there are limited available linguistic de-\nscriptions for ‘low-resource’ languages (e.g. Fer-\nger, 2020; Sikasote and Anastasopoulos, 2022).\nOften, linguistic features–such as morphological\ncomplexity and typology–are used as reasons why\nit is difficult to blindly adopt methods that work\nfor high-resource languages, even in cases where\nthere is an equal number of training data (e.g.\nde Lhoneux et al., 2022). Standardization–or lack\nthereof–is another feature mentioned in relation to\n‘low-resourcedness’ of languages. Both linguistic\nfeatures and lack of standardization are mentioned\nas reasons for data sparsity. For example:\n“Due to differences in language typol-\nogy, it is not necessarily as simple as\nlooking only at number of lines of train-\ning data.[...] For example, Inuktitut is\nknown to be highly morphologically com-\nplex, resulting in many words (defined\nas space/punctuation separated) that ap-\npear just once or only a few times, even\nin such a large corpus.”–Knowles and\nLittell (2022)\n“Not only is data scarce, but it might\nlack standardization, making the dataset\nmore sparse than it would be for lan-\nguages with standardized orthographies\nand numerous speakers.” –Coto-Solano\n(2022)\nData\nWith regards to data, the classification of\na language as low-resource could be based on la-\nbeled or annotated data (e.g. Ponti et al., 2021),\nunlabeled data (e.g. ImaniGooghari et al., 2022),\nor benchmark data (e.g. Reid et al., 2021). Some\npapers focus their definitions on the quality of data\n(e.g. Maillard et al., 2023; Ramnath et al., 2021),\nstating that low-resource language data is usually\nnoisy. Other papers quantify the amount of data\n(e.g. Biswas et al., 2020). We also observed a sub-\nset of papers that use a predefined cutoff for the\namount of data: for instance, Ramachandran and\nde Melo (2020) state they “...picked six languages\nthat had around 10K or fewer verses available.”\nSome papers would quantify the amount of data in\nrelation to a popular trend in the field:\n“Only some of the 22 scheduled Indian\nlanguages, which are a subset of the nu-\nmerous languages spoken and written in\nIndia, have enough resources for train-\ning a deep learning model.” –Saurav\net al. (2020)\nTechnology\nExclusion from technological ad-\nvances for the languages of study is another aspect\nmentioned in relation to low-resource languages.\nThis ranges from the lack of basic computational\ntools–such as text pre-processing tools (e.g. Niy-\nongabo et al., 2020) –to exclusion from pre-trained\nlanguage models (e.g. Leong et al., 2022; Pfeiffer\net al., 2020). There were also mentions of lack of\ncompute resources (e.g. Yimam et al., 2020).\n“Handling\nutterances\nwith\nnon-\nKanien’kéha characters would have\nrequired grapheme-to-phoneme predic-\ntion capable of dealing with multilingual\ntext and code-switching, which we did\nnot have available.” –Pine et al. (2022a)\n“In total, we can discern four categories\nin our language set: 1) high-resource\nlanguages and 2) low-resource lan-\nguages covered by the pretrained SOTA\nmultilingual models (i.e., by mBERT and\nXLM-R); as well as 3) low-resource lan-\nguages and 4) truly low-resource lan-\nguages not covered by the multilingual\nmodels”–Pfeiffer et al. (2020)\n3.4\nAgency\nTranscending all the other aspects is community\nagency and the role it plays in what and by whom\nlanguage technologies are built.\nCoto-Solano\n(2022) state how even in cases where communities\nare willing to provide data, financial constraints pre-\nvent them from doing so. Le Ferrand et al. (2022a)\nemphasize building language tools detached from\ncommunity practices leads to technologies with\nminimal utility to the communities. This detach-\nment from community practices is also stated as a\nreason for minimal studies in these languages:\n“Although Assamese has a very old and\nrich literary history, technology develop-\nment in NLP is still in a nascent stage.”\n–Pathak et al. (2022)\nWhen communities are actively engaged, we\nobserve their values embedded in the production\nFigure 2: Number of languages included in the studies\nper language family.\nof technology, regardless of the outcome of the\nresearch project:\n“While a total of 24 hours of audio were\nrecorded, members of the Kanien’kéha-\nspeaking community told us it would be\ninappropriate to use the voices of speak-\ners who had passed away, leaving only\nrecordings of Satewas’s voice. [...] The\nresulting speech corpus comprised 3.46\nhours of speech.” –Pine et al. (2022b)\n4\nWhat Languages are Studied as\n‘Low-Resource’?\nLanguages may be studied in multilingual contexts,\ni.e. included alongside other languages (e.g. Ade-\nlani et al., 2022; Goyal et al., 2022b) or in mono-\nlingual contexts (e.g. Yimam et al., 2020; Pathak\net al., 2022). Papers had varying depths of descrip-\ntions for the languages they studied, with papers\nworking on fewer languages having more in-depth\ndescriptions. For instance, Mehta et al. (2020),\nwhich exclusively work on the Gondi language,\nhas a dedicated section on the historical, political,\nand linguistic context of the Gondi language and\nits community. On the other hand, Goyal et al.\n(2022b), which works on 101 languages, has one\ntable with all the languages, their ISO codes, lan-\nguage families, writing scripts, and the amount of\navailable data.\nIn Figure 2, we show the number of languages\nand language families studied in our samples,\nwhere papers explicitly mention them as low-\nresource. We observe a diverse set of language\nfamilies, with Indo-European languages having the\nhighest number of languages studied in our sam-\nples, followed by Niger-Congo and Austronesian.\nIn Appendix C, we detail the top 20 most frequently\nstudied languages in our sample.\nFigure 3: Criteria distribution used in the top-20 lan-\nguages to categorize languages.\nThe graph in Figure 3 shows the distributions of\nthe various criteria used for categorizing a language\nas ‘low-resource’ in the top 20 languages studied.\nWhile data is the most commonly used criterion\nacross many papers and languages, other factors,\nsuch as lack of computational tools, limited number\nof native speakers, etc, are also used (see Section\n3). Even with papers that use data as a criterion,\nwe observe different qualifications for what type\nof data a language may lack to qualify as a ‘low-\nresource’ language. In Figure 6, we further break\ndown the criterion of data. We observe that lack of\nlabeled data is the most commonly used criterion\nin our sample at 39.8%. We also observe the lack\nof digitized text (1.7%) and online-available data\n(6.9%) as criteria to connote a language as low-\nresource.\n5\nWhy does it matter?\nIn the previous section, we describe four overar-\nching aspects that determine if a language is ‘low-\nresource’: socio-political aspects, human and digi-\ntal resources, artifacts, and agency of community\nmembers. In Figure 4, we present language profiles\nfor 6 languages. We choose the six languages from\nthe bottom three classes in Joshi et al. (2020): ‘The\nLeft Behinds’ with limited labeled and unlabeled\ndata, ‘The Scraping-Bys’ with some amount of un-\nlabeled data, and ‘The Hopefuls’ with some labeled\ndata. We use literature about these languages and\ntheir communities to demonstrate why it matters\nthat we are specific in the terminology we use.\nLanguages in the same class of data availability\nmight differ in other aspects.\nFrom ‘The Left\nBehinds’, we present profiles for Numma-guhooni7\nand Warlpiri. Numma-guhooni is spoken in Kenya\nwhere the official Federal languages are Kiswahili8\nand English. Warlpiri is spoken by the Warlpiri\npeople of Australia, where the most dominant lan-\nguage is English. While both languages fall into the\nsame class, the number of speakers for Warlpiri is\n4 times that of Numma-guhooni. Ethnologue clas-\nsifies Warlpiri as a stable language, while Numma-\nguhooni is endangered. In terms of digital resource\navailability, Ethnologue classifies Numma-guhooni\nas still meaning, there is no sign of digital support\nfor the language, while Warlpiri is labeled emerg-\ning with some digital content available. Warlpiri\nalso has some NLP tools available, for instance,\nKirrKirr is a dictionary visualization tool for the\nWarlpiri language (Manning et al., 2001).\nFrom ‘The Scraping-Bys’, we look at Chero-\nkee and Kalaallisut. Cherokee, spoken by around\n2,000 out of the 300,000 Cherokee people of the\nCherokee Nation in the United States of America,\nis labeled as endangered by Ethnologue. On the\nother hand, Kalaallisut, which is spoken by about\n50,000 people and is the official Federal language\nof Greenland, is labeled as institutional by Eth-\nnoluge. However, Cherokee has a higher ranking\nfor digital language support, dubbed vital while\nKalaallisut is ascending.\nFor ‘The Hopefuls’, we look at isiZulu and\nKonkani. We observe the two languages are some-\nwhat similar in terms of human and digital re-\nsources, with both being institutional in vitality\nand vital in digital access. However, we see the\nlanguages vary by their number of speakers with\nisiZulu having about 6 times the number of speak-\ners as Konkani. Additionally, isiZulu is the most\ncommon language spoken as a first language in\nSouth Africa, while Konkani has shown a decline\nin number of speakers, with speakers outside of\nits primary province declaring other, dominant lan-\nguages as their native language (Rajan et al., 2020).\nBoth languages have NLP tools available for tasks\nlike machine translation and speech processing as\nwell as pre-processing tools.\nOverall, we observe that within a given class\nbased on data availability, there are drastic differ-\nences in what other resources are available for a\n7While this language is refereed to with another name in\nthe literature, there is evidence that the word is derogatory and\nso we exclusively use the name native speakers use (Stiles,\n1982).\n8also known as Swahili in English speaking contexts.\nFigure 4: Language profiles for six languages across three classes based on data availability. The first row in\neach profile deals with socio-political issues, the second row resources, and the last row with artifacts (see Figure 1).\nWe observe drastic differences between languages of the same class. See Appendix A for details on the labels.\nlanguage. We observe that the variance decreases\nas we move up the classes, which can partially be\nexplained by the stark 88.38% of the world’s lan-\nguages belonging to ‘The Left-Behinds’, compared\nto 5.49% in ‘The Scraping-Bys’ and 0.36% in ‘The\nHopefuls’ (Joshi et al., 2020). However, as we\ndemonstrate, the realities of each of the languages\nwithin each class are very different.\nThe different aspects that determine ‘low-\nresourcedness’ have causal links.\nThe four as-\npects we discuss in Section 3 interact with each\nother in constraining what is available.\nSocio-\npolitical issues constrain what Resources are avail-\nable for a given language, which in turn impact\nwhat Artifacts are produced for that language. For\ninstance, while there are no official languages in\nthe USA or Australia, federal policies in the US\nup to 1948 forced Indigenous children to assimi-\nlate into Western culture, punishing students for\nspeaking their languages (Wakeman, 2021). Simi-\nlarly, colonization destroyed several languages of\nIndigenous populations in Australia (Laura Stocker\nand Rooney, 2016). As a result, both Cherokee and\nWarlpiri, along with the numerous other Indigenous\nlanguages of the Americas, Australia, and Canada\nare endangered, i.e lack human resources.\nAssimilation is not limited to the languages of\nthe colonizer. Post-independence from colonial\nrule of Britain, Kenya adopted the educational\nand language policies of Britain, with English de-\nclared the official language in formal sectors and\nKiswahili the national language of the country. As\na result, the majority of data available in digital\nand electronic media as well as in public settings\nare in English or Kiswahili (Barasa, 2023). Hence,\nspeakers of languages like Numma-guhooni are\nlargely assimilated with larger ethnic groups and\nKiswahili is predominantly spoken and learned\nby the new generation (Tosco, 1992). While in\n2010, the Kenya constitution shifted towards cen-\ntering the preservation of native languages, there\nwere not enough funds allocated to carry this\nthrough (Barasa, 2023). Though at a different scale,\nthis is similar to the case of Konkani, which is in\n‘The Hopefuls’ class, losing native speakers to more\ndominant local languages (Rajan et al., 2020).\nConstraints of human and digital resources re-\nstrict the creation of artifacts for languages. As\ndiscussed in Section 3.2, the minimal digital pres-\nence results in limited available data, especially at\nthe scale needed for training SOTA models. Links\namong the different aspects are not necessarily lin-\near; socio-political issues also directly constrain\nwhat languages are taught in schools, impacting\nlinguistic knowledge produced for a language. Ad-\nAspect\nSub-Division\nTerminology\nDefinition\nSocio-political Economic\nlow-affluence (Hammarström,\n2009)\nbased on Gross Language Product (GLP) (product of the\nnumber of native speakers in any country and the country’s\nper capita Gross National Product.)\nPolitical\npolitically-disadvantaged\nlanguages not used in mainstream media and governmen-\ntal communications due to political forces\nResources\nNative\nSpeakers*\nextinct; critically endangered;\nseverely endangered;\ndefini-\ntively endangered; unsafe; safe\n(Brenzinger et al., 2003)\n6 point scale based on number of speakers of the language\nOnline\nPresence\nLow-Web Resource(Patil et al.,\n2022)\nlimited online corpus or web presence\nLanguage\nexperts\nexpert-constrained\nlimited number of linguistic experts or researchers\nArtifacts\nLinguistic\nKnowledge*\noral languages; non-native or-\nthography; native orthography\nbased on the availability and type of orthography a lan-\nguage has.\nundocumented;\ninadequate;\nfragmentory; fair; good; su-\nperlative (Brenzinger et al.,\n2003)\n6 point scale based on the amount and quality of documen-\ntation available for a language.\nData*\nClass 0; Class 1; Class 2; Class\n3; Class 4; Class 5 (Joshi et al.,\n2020)\n6 classes based on the availability of labeled and unlabeled\ndata\nTechnology*\nStill; Emerging; Ascending; Vi-\ntal; Thriving (Simons et al.,\n2022)\n5-level classification based on digital language support\navailable in a given language.\nTable 2: Suggestions for explicit terminology addressing three aspects we identified through our analysis. We\nprovide citations for terminology taken from prior work. (*) indicate the terminology are part of a scale and all\nlabels in the scale are listed.\nditionally, prior work demonstrates the Western-\ndominated researcher landscape in NLP and how\nit ties to coloniality (Held et al., 2023). With the\nlimited number of speakers for a given language,\nthe number of NLP researchers who are also na-\ntive speakers of the language is largely constrained,\nwhich is further confounded by the limited financial\nresources available to researchers from such com-\nmunities. As a result, having agency in what tools\nare designed for a language becomes challenging.\nKnowing which aspect a language is lacking\nin allows for targeted interventions.\nOne of\nthe main factors that determine the survival of a\nlanguage is inter-generational transmission (Bren-\nzinger et al., 2003). For instance, while Cherokee\nand Kalaallisut are both in the same class, Chero-\nkee is endangered while Kalaallisut is institutional.\nHence, interventions–both in socio-political and ar-\ntifact aspects–are best targeted toward reviving and\npreserving the Cherokee language. On the other\nhand, digital access for Kalaalisut is ascending,\nhence there might be more efforts towards increas-\ning the availability of digital data. Since Kalaallisut\nis institutional, financial resources for preserving\nand growing the language are available at a federal\nlevel. Additionally, it is used as the language of\ninstruction in the education system of the country,\naiding in the inter-generational transfer of the lan-\nguage. Across classes, we observe similarities in\nNumma-guhooni and Konkani, of native speakers\nassimilating to other dominant but local languages.\nHence, interventions for these languages may be\nmore effective in language learning apps that focus\non learning the less-dominant language and trans-\nlation systems between dominant local languages\nand the target language.\nCommunities are actively resisting exploita-\ntion and sustaining their languages; our tools\nshould support them. Despite the several layers of\nconstraints, it is important to note that communities\nare not in idle state of deficit. Across classes, we\nobserve a similarity between Warlpiri and Chero-\nkee, in that there are community-based initiatives to\npreserve and grow the languages (e.g. the Warlpiri\nEducation and Training Trust (WETT)9 and the\nCherokee Immersion School10). By centering com-\nmunity values in our designs and research, we can\ncollectively forge new paths for each language, con-\nditioned on its unique circumstances.\n9https://www.clc.org.au/wett/\n10https://www.cwyschools.org/\n6\nWhat can we do?\nUsing specific terminology or having explicit\ndefinitions allows us to measure progress more\nprecisely.\nThe specific resource a language is\ndeemed ‘low’ in directly impacts what interven-\ntions are effected towards it. For instance, pro-\ngrams aimed at increasing language representation\nin Human Language Technologies (HLTs) have sev-\neral selection criteria (Cieri et al., 2016). Such pro-\ngrams use different terminologies and definitions,\nwhere “each term encodes differences in traditions,\ngoals, and approaches” (Simpson et al., 2008). As\na result, what languages are included and served by\nsuch programs differ, even if languages have the\nsame amount of data.\nWhile Cherokee is tagged as having vital dig-\nital resources, it is also an endangered language.\nCollecting more data in the language from the lim-\nited number of speakers or including it in Large\nLanguage Models may not exactly alleviate its low-\nresourcedness. We argue for more explicit dec-\nlarations of which aspects of resources are being\nreferred to when the term low-resource is used. In\nTable 2, we give recommendations for terminolo-\ngies based on prior work and our findings. There\nare also several taxonomies and classes based on\ndata (e.g Joshi et al., 2020), language vitality (e.g\nBrenzinger et al., 2003), and digital support (e.g\nSimons et al., 2022).\nRecommendations for stakeholders:\nBased on\nour findings, we give recommendations for differ-\nent stakeholders involved in the effort to increase\nlanguage representation in NLP research. Individ-\nual researchers can (1) engage with community\nmembers and speakers of the languages they work\non, (2) articulate how their work is limited in re-\nlation to the characteristics of the languages they\nwork on, and (3) be explicit about what criteria\nthey use to denote a language as ‘low-resource.’\nCommunity members can also form grassroots or-\nganizations such as Masakhane11, which allow re-\nsearchers who speak diverse languages to build lan-\nguage technologies together and learn from each\nother’s experiences. Additionally, such organiza-\ntions can prioritize engaging with native speakers\nwho may not be in the NLP research field, allow-\ning for diverse perspectives when deciding what\ntools should be built for what language. Work-\n11https://www.masakhane.io/\nshops such as AmericasNLP12 and AfricaNLP13\ncontinue to serve as spaces for fostering research\nand collaboration for languages that are mostly ig-\nnored in mainstream NLP research. However, main\n(*)CL conferences can increase the representation\nof these languages by (1) offering alternative tracks\nfor papers, (2) easing the cost of attendance and\nregistration for researchers from these communi-\nties, and (3) diversifying conference venues. Aca-\ndemic institutions can aid researchers who speak\nthese languages by promoting interdisciplinary col-\nlaboration and partner with local and international\norganizations to document and preserve marginal-\nized languages. Industry players interested in lan-\nguage diversity of their products can play a role\nby offering financial and technical support; for\nexample, subsidizing resources for communities\nworking on low-resource languages. Companies\ncould also prioritize making their products acces-\nsible to the communities (e.g. Üstün et al., 2024).\nGovernment bodies can play a role in preserving\nlanguages through policies, funding, and digital\ninclusion. Funding agencies can support language\ndiversity and enforce building technologies that are\nrelevant to the specific linguistic community by\nsetting research priorities and prioritizing grants to\nunderrepresented researchers.\n7\nConclusion\nIn this paper, we present 4 aspects of ‘resourced-\nness’ used to classify a language as ‘low-resource’\nbased on a qualitative survey of 150 papers. Based\non our analysis, we give recommendations for ter-\nminology that explicitly calls out which resource\nwe are referring to when we say a language is ‘low-\nresource.’ A language may lack in several aspects,\nmaking the use of individual terminology difficult–\ne.g. in multilingual settings. However, the diffi-\nculty does not absolve us from the responsibility\nto provide detailed documentation. At the very\nleast, clear statements on what exactly is meant by\nlow-resource when referring to a language would\nallow us to more clearly articulate the problems\na particular technology resolves for a particular\nlanguage.\n8\nLimitations\nAs a qualitative study, our paper does not give the\ndefinitions of the term from all the papers in all the\n12https://github.com/AmericasNLP\n13https://africanlp.masakhane.io/\nvenues we searched. We also do not make quan-\ntitative claims. Instead, we focus on a nuanced\nanalysis of how our sample papers describe the\nphenomenon and provide direct quotes from pa-\npers we analyzed as evidence. While it was not\npractical for us to conduct qualitative analysis on\nmore than the papers in our sample, future work\ncould use automated methods and conduct a quan-\ntitative analysis. Similarly, our analysis of what\nlanguages are studied is limited to the papers in\nour sample. This could also be supplemented with\nautomated extraction at scale. Additionally, while\nwe could not perform a longitudinal analysis with\nour sample size of 150 papers, future work could\nexplore such a study to understand how the use of\nthe term ‘low-resource’ evolved over time.\nReferences\nIfe Adebara, Muhammad Abdul-Mageed, and Miikka\nSilfverberg. 2022. Linguistically-motivated Yorùbá-\nEnglish machine translation. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 5066–5075, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia\nKreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\nDietrich Klakow, Peter Nabende, Ernie Chang, Tajud-\ndeen Gwadabe, Freshia Sackey, Bonaventure F. P.\nDossou, Chris Emezue, Colin Leong, Michael Beuk-\nman, Shamsuddeen Muhammad, Guyo Jarso, Oreen\nYousuf, Andre Niyongabo Rubungo, Gilles Hacheme,\nEric Peter Wairagala, Muhammad Umair Nasir, Ben-\njamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade\nAbbott, Mohamed Ahmed, Millicent Ochieng, An-\nuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,\nFatoumata Ouoba Kabore, Godson Kalipe, Derguene\nMbaye, Allahsera Auguste Tapo, Victoire Memd-\njokam Koagne, Edwin Munkoh-Buabeng, Valen-\ncia Wagner, Idris Abdulmumin, Ayodele Awokoya,\nHappy Buzaaba, Blessing Sibanda, Andiswa Bukula,\nand Sam Manthalu. 2022. A few thousand transla-\ntions go a long way! leveraging pre-trained mod-\nels for African news translation. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3053–3070,\nSeattle, United States. Association for Computational\nLinguistics.\nWafia Adouane, Samia Touileb, and Jean-Philippe\nBernardy. 2020. Identifying sentiments in Algerian\ncode-switched user-generated comments.\nIn Pro-\nceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 2698–2705, Marseille,\nFrance. European Language Resources Association.\nKabir Ahuja, Sunayana Sitaram, Sandipan Dandapat,\nand Monojit Choudhury. 2022. On the calibration of\nmassively multilingual language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 4310–4323,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJesujoba Alabi, Kwabena Amponsah-Kaakyire, David\nAdelani, and Cristina España-Bonet. 2020. Massive\nvs. curated embeddings for low-resourced languages:\nthe case of Yorùbá and Twi. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 2754–2762, Marseille, France. European\nLanguage Resources Association.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive fine-tuning. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336–4349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nKarl Anderbeck. 2015. Portraits of language vitality in\nthe languages of Indonesia, pages 19–47.\nAndrei-Marius Avram,\nDarius Catrina,\nDumitru-\nClementin Cercel, Mihai Dascalu, Traian Rebedea,\nVasile Pais, and Dan Tufis. 2022.\nDistilling the\nknowledge of Romanian BERTs using multiple teach-\ners. In Proceedings of the Thirteenth Language Re-\nsources and Evaluation Conference, pages 374–384,\nMarseille, France. European Language Resources\nAssociation.\nAhsaas Bajaj, Pavitra Dangati, Kalpesh Krishna, Prad-\nhiksha Ashok Kumar, Rheeya Uppaal, Goldman\nSachs, Bradford Windsor, Eliot Brenner, Dominic\nDotterrer, Rajarshi Das, et al. 2021.\nLong docu-\nment summarization in a low resource setting us-\ning pretrained language models. ACL-IJCNLP 2021,\n120(4,268):71.\nDavid Bamutura, Peter Ljunglöf, and Peter Nebende.\n2020. Towards computational resource grammars\nfor Runyankore and rukiga. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 2846–2854, Marseille, France. European\nLanguage Resources Association.\nDavid Barasa. 2023.\nLanguage ideologies, policies\nand practices within the multilingual Kenyan context.\nJLLCS, 2(1):55–62.\nM Saiful Bari, Tasnim Mohiuddin, and Shafiq Joty.\n2021. UXLA: A robust unsupervised data augmenta-\ntion framework for zero-resource cross-lingual NLP.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1978–1992, Online. Association for Computational\nLinguistics.\nMartijn Bartelds, Nay San, Bradley McDonnell, Dan\nJurafsky, and Martijn Wieling. 2023. Making more of\nlittle data: Improving low-resource automatic speech\nrecognition using data augmentation. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 715–729, Toronto, Canada. Association for\nComputational Linguistics.\nMartijn Bartelds and Martijn Wieling. 2022. Quanti-\nfying language variation acoustically with few re-\nsources. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 3735–3741, Seattle, United States.\nAssociation for Computational Linguistics.\nVineet Bhat, Preethi Jyothi, and Pushpak Bhattacharyya.\n2023. Adversarial training for low-resource disflu-\nency correction. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 8112–\n8122, Toronto, Canada. Association for Computa-\ntional Linguistics.\nSteven Bird. 2022. Local Languages, Third Spaces,\nand other High-Resource Scenarios. ACL Anthology,\npages 7817–7829.\nAstik Biswas, Emre Yilmaz, Febe De Wet, Ewald\nVan der westhuizen, and Thomas Niesler. 2020.\nSemi-supervised development of ASR systems\nfor multilingual code-switched speech in under-\nresourced languages. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference,\npages 3468–3474, Marseille, France. European Lan-\nguage Resources Association.\nJan A. Botha, Zifei Shan, and Daniel Gillick. 2020. En-\ntity Linking in 100 Languages. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7833–7845,\nOnline. Association for Computational Linguistics.\nVirginia Braun and Victoria Clarke. 2006. Using the-\nmatic analysis in psychology. Qualitative Research\nin Psychology.\nMatthias Brenzinger, Arienne M. Dwyer, Tjeerd de\nGraaf, Colette Grinevald, Michael Krauss, Os-\nahito Miyaoka, Nicholas Ostler, Osamu Sakiyama,\nMaría E. Villalón, Akira Y. Yamamoto, and Ofelia\nZepeda. 2003. UNESCO Ad Hoc Expert Group on\nEndangered Languages.\nJacqueline Brixey, David Sides, Timothy Vizthum,\nDavid Traum, and Khalil Iskarous. 2020. Explor-\ning a Choctaw language corpus with word vectors\nand minimum distance length. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 2746–2753, Marseille, France. European\nLanguage Resources Association.\nGina Bustamante, Arturo Oncevay, and Roberto\nZariquiey. 2020. No data to crawl? monolingual\ncorpus creation from PDF files of truly low-resource\nlanguages in Peru. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n2914–2923, Marseille, France. European Language\nResources Association.\nJoan Byamugisha. 2022. Noun class disambiguation in\nRunyankore and related languages. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4350–4359, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nAbhisek Chakrabarty, Raj Dabre, Chenchen Ding,\nHideki Tanaka, Masao Utiyama, and Eiichiro Sumita.\n2022.\nFeatureBART: Feature based sequence-to-\nsequence pre-training for low-resource NMT.\nIn\nProceedings of the 29th International Conference\non Computational Linguistics, pages 5014–5020,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nAditi Chaudhary, Antonios Anastasopoulos, Zaid\nSheikh, and Graham Neubig. 2021. Reducing con-\nfusion in active learning for part-of-speech tagging.\nTransactions of the Association for Computational\nLinguistics, 9:1–16.\nNuo Chen, Linjun Shou, Ming Gong, Jian Pei, and\nDaxin Jiang. 2022. Bridging the gap between lan-\nguage models and cross-lingual sequence labeling.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1909–1923, Seattle, United States. Association\nfor Computational Linguistics.\nMonojit Choudhury. 2023. Generative AI has a lan-\nguage problem. Nat. Hum. Behav., 7:1802–1803.\nChiamaka Chukwuneke, Ignatius Ezeani, Paul Rayson,\nand Mahmoud El-Haj. 2022.\nIgboBERT models:\nBuilding and training transformer models for the\nIgbo language. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n5114–5122, Marseille, France. European Language\nResources Association.\nChristopher Cieri, Mike Maxwell, Stephanie Strassel,\nand Jennifer Tracey. 2016. Selection Criteria for\nLow Resource Language Programs. ACL Anthology,\npages 4543–4549.\nRolando Coto-Solano. 2022. Evaluating word embed-\ndings in extremely under-resourced languages: A\ncase study in Bribri. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 4455–4467, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nRolando Coto-Solano, Sally Akevai Nicholas, Samiha\nDatta, Victoria Quint, Piripi Wills, Emma Ngaku-\nravaru Powell, Liam Koka’ua, Syed Tanveer, and\nIsaac Feldman. 2022. Development of automatic\nspeech recognition for the documentation of Cook\nIslands M¯aori. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n3872–3882, Marseille, France. European Language\nResources Association.\nJeanne E. Daniel, Willie Brink, Ryan Eloff, and Charles\nCopley. 2019. Towards automating healthcare ques-\ntion answering in a noisy multilingual low-resource\nsetting. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n948–953, Florence, Italy. Association for Computa-\ntional Linguistics.\nMiryam de Lhoneux, Sheng Zhang, and Anders Søgaard.\n2022. Zero-shot dependency parsing with worst-case\naware automated curriculum learning. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 578–587, Dublin, Ireland. Association\nfor Computational Linguistics.\nWietse de Vries, Martijn Wieling, and Malvina Nissim.\n2022. Make the best of cross-lingual transfer: Ev-\nidence from POS tagging with over 100 languages.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 7676–7685, Dublin, Ireland.\nAssociation for Computational Linguistics.\nAndy Dearden and William D. Tucker. 2021. The ethi-\ncal limits of bungee research in ICTD. In 2015 IEEE\nInternational Symposium on Technology and Society\n(ISTAS), pages 1–6. IEEE Press.\nArnab Debnath, Navid Rajabi, Fardina Fathmiul Alam,\nand Antonios Anastasopoulos. 2021. Towards more\nequitable question answering systems: How much\nmore data do you need? In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 621–629, Online. Association\nfor Computational Linguistics.\nMathieu Dehouck and Carlos Gómez-Rodríguez. 2020.\nData Augmentation via Subtree Swapping for Depen-\ndency Parsing of Low-Resource Languages. ACL\nAnthology, pages 3818–3830.\nPrajit Dhar, Arianna Bisazza, and Gertjan van Noord.\n2022. Evaluating pre-training objectives for low-\nresource translation into morphologically rich lan-\nguages. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 4933–\n4943, Marseille, France. European Language Re-\nsources Association.\nHarshita Diddee, Kalika Bali, Monojit Choudhury, and\nNamrata Mukhija. 2022. The six conundrums of\nbuilding and deploying language technologies for\nsocial good. In Proceedings of the 5th ACM SIG-\nCAS/SIGCHI Conference on Computing and Sustain-\nable Societies, COMPASS ’22, page 12–19, New\nYork, NY, USA. Association for Computing Machin-\nery.\nBosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-\nengkrai, Thien Hai Nguyen, Shafiq Joty, Luo Si, and\nChunyan Miao. 2020. DAGA: Data augmentation\nwith a generation approach for low-resource tagging\ntasks. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6045–6057, Online. Association for\nComputational Linguistics.\nSaket Dingliwal, Shuyang Gao, Sanchit Agarwal, Chien-\nWei Lin, Tagyoung Chung, and Dilek Hakkani-Tur.\n2021. Few shot dialogue state tracking using meta-\nlearning. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 1730–1739,\nOnline. Association for Computational Linguistics.\nCheikh M. Bamba Dione, Alla Lo, Elhadji Mamadou\nNguer, and Sileye Ba. 2022. Low-resource neural\nmachine translation: Benchmarking state-of-the-art\ntransformer for Wolof<->French. In Proceedings of\nthe Thirteenth Language Resources and Evaluation\nConference, pages 6654–6661, Marseille, France. Eu-\nropean Language Resources Association.\nSuma Reddy Duggenpudi, Subba Reddy Oota, Mounika\nMarreddy, and Radhika Mamidi. 2022. TeluguNER:\nLeveraging multi-domain named entity recognition\nwith deep transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics: Student Research Workshop, pages 262–\n272, Dublin, Ireland. Association for Computational\nLinguistics.\nAbteen Ebrahimi, Arya D. McCarthy, Arturo Oncevay,\nJohn E. Ortega, Luis Chiruzzo, Gustavo Giménez-\nLugo, Rolando Coto-Solano, and Katharina Kann.\n2023. Meeting the needs of low-resource languages:\nThe value of automatic alignments via pretrained\nmodels. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 3912–3926, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nTobias Eder, Viktor Hangya, and Alexander Fraser.\n2021. Anchor-based bilingual word embeddings for\nlow-resource languages. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 227–232, Online. Association\nfor Computational Linguistics.\nRamy Eskander, Francesca Callejas, Elizabeth Nichols,\nJudith Klavans, and Smaranda Muresan. 2020a. Mor-\nphAGram, evaluation and framework for unsuper-\nvised morphological segmentation. In Proceedings\nof the Twelfth Language Resources and Evaluation\nConference, pages 7112–7122, Marseille, France. Eu-\nropean Language Resources Association.\nRamy Eskander, Smaranda Muresan, and Michael\nCollins. 2020b. Unsupervised cross-lingual part-of-\nspeech tagging for truly low-resource scenarios. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4820–4831, Online. Association for Computa-\ntional Linguistics.\nMeng Fang and Trevor Cohn. 2017. Model transfer\nfor tagging low-resource languages using a bilingual\ndictionary. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 587–593, Vancouver,\nCanada. Association for Computational Linguistics.\nHongliang Fei and Ping Li. 2020. Cross-lingual un-\nsupervised sentiment classification with multi-view\ntransfer learning. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5759–5771, Online. Association for\nComputational Linguistics.\nIsaac Feldman and Rolando Coto-Solano. 2020. Neu-\nral machine translation models with back-translation\nfor the extremely low-resource indigenous language\nBribri. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 3965–\n3976, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nAnne Ferger. 2020.\nProcessing language resources\nof under-resourced and endangered languages for\nthe generation of augmentative alternative commu-\nnication boards. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n2644–2648, Marseille, France. European Language\nResources Association.\nBesnik Fetahu, Anjie Fang, Oleg Rokhlenko, and\nShervin Malmasi. 2022.\nDynamic gazetteer inte-\ngration in multilingual models for cross-lingual and\ncross-domain named entity recognition. In Proceed-\nings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n2777–2790, Seattle, United States. Association for\nComputational Linguistics.\nMarina\nFomicheva,\nShuo\nSun,\nErick\nFonseca,\nChrysoula Zerva, Frédéric Blain, Vishrav Chaudhary,\nFrancisco Guzmán, Nina Lopatina, Lucia Specia, and\nAndré F. T. Martins. 2022. MLQE-PE: A multilin-\ngual quality estimation and post-editing dataset. In\nProceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 4963–4974, Mar-\nseille, France. European Language Resources Asso-\nciation.\nFitsum Gaim, Wonsuk Yang, Hancheol Park, and\nJong Park. 2023.\nQuestion-answering in a low-\nresourced language: Benchmark dataset and models\nfor Tigrinya. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 11857–11870,\nToronto, Canada. Association for Computational Lin-\nguistics.\nAndargachew Mekonnen Gezmu, Andreas Nürnberger,\nand Tesfaye Bayu Bati. 2022. Extended parallel cor-\npus for Amharic-English machine translation. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 6644–6653, Marseille,\nFrance. European Language Resources Association.\nSreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Ku-\nmar, S Ramaneswaran, S Sakshi, Utkarsh Tyagi, and\nDinesh Manocha. 2023. DALE: Generative data aug-\nmentation for low-resource legal NLP. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 8511–8565,\nSingapore. Association for Computational Linguis-\ntics.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022a. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022b. The flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nH Hammarström. 2009. . a survey of computational\nmorphological resources for low-density languages.\nNEALT.\nViktor Hangya, Hossain Shaikh Saadi, and Alexander\nFraser. 2022. Improving low-resource languages in\npre-trained multilingual language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 11993–\n12006, Abu Dhabi, United Arab Emirates. Associa-\ntion for Computational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Ma-\nsum Hasan, Madhusudan Basak, M. Sohel Rahman,\nand Rifat Shahriyar. 2020. Not low-resource any-\nmore: Aligner ensembling, batch filtering, and new\ndatasets for Bengali-English machine translation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2612–2623, Online. Association for Computa-\ntional Linguistics.\nWilliam Held, Camille Harris, Michael Best, and Diyi\nYang. 2023. A Material Lens on Coloniality in NLP.\narXiv.\nMarcelo Yuji Himoro and Antonio Pareja-Lora. 2022.\nPreliminary results on the evaluation of computa-\ntional tools for the analysis of Quechua and Aymara.\nIn Proceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 5450–5459, Mar-\nseille, France. European Language Resources Asso-\nciation.\nYilun Hua, Zhaoyuan Deng, and Kathleen McKeown.\n2023. Improving long dialogue summarization with\nsemantic graph representation. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 13851–13883, Toronto, Canada. Association\nfor Computational Linguistics.\nYichong Huang, Xiaocheng Feng, Xinwei Geng, and\nBing Qin. 2022. Unifying the convergences in multi-\nlingual neural machine translation. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 6822–6835, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nAyyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kar-\ngaran, Silvia Severini, Masoud Jalili Sabet, Nora\nKassner, Chunlan Ma, Helmut Schmid, André Mar-\ntins, François Yvon, and Hinrich Schütze. 2023.\nGlot500: Scaling multilingual corpora and language\nmodels to 500 languages. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1082–\n1117, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAyyoob ImaniGooghari, Silvia Severini, Masoud\nJalili Sabet, François Yvon, and Hinrich Schütze.\n2022. Graph-based multilingual label propagation\nfor low-resource part-of-speech tagging. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 1577–1589,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nHuiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya\nMcCarthy, and Katharina Kann. 2020. Unsupervised\nmorphological paradigm completion. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 6696–6707, Online.\nAssociation for Computational Linguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The State and\nFate of Linguistic Diversity and Inclusion in the NLP\nWorld. ACL Anthology, pages 6282–6293.\nIman Jundi, Neele Falk, Eva Maria Vecchi, and\nGabriella Lapesa. 2023. Node placement in argu-\nment maps: Modeling unidirectional relations in high\n& low-resource scenarios.\nIn Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5854–5876, Toronto, Canada. Association for Com-\nputational Linguistics.\nHuda Khayrallah, Brian Thompson, Matt Post, and\nPhilipp Koehn. 2020. Simulated multiple reference\ntraining improves low-resource machine translation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 82–89, Online. Association for Computational\nLinguistics.\nRodney Kinney, Chloe Anastasiades, Russell Authur,\nIz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\nski, Isabel Cachola, Stefan Candra, Yoganand Chan-\ndrasekhar, Arman Cohan, et al. 2023.\nThe se-\nmantic scholar open data platform. arXiv preprint\narXiv:2301.10140.\nRebecca Knowles and Patrick Littell. 2022. Transla-\ntion memories as baselines for low-resource machine\ntranslation. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n6759–6767, Marseille, France. European Language\nResources Association.\nWei-Jen Ko, Ahmed El-Kishky, Adithya Renduchin-\ntala, Vishrav Chaudhary, Naman Goyal, Francisco\nGuzmán, Pascale Fung, Philipp Koehn, and Mona\nDiab. 2021. Adapting high-resource NMT models to\ntranslate low-resource related languages without par-\nallel data. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 802–812, Online. Association for Computa-\ntional Linguistics.\nBoshko Koloski, Senja Pollak, Blaž Škrlj, and Matej\nMartinc. 2022. Out of thin air: Is zero-shot cross-\nlingual keyword detection better than unsupervised?\nIn Proceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 400–409, Mar-\nseille, France. European Language Resources Asso-\nciation.\nArjun Sai Krishnan and Seyoon Ragavan. 2021.\nMorphology-aware meta-embeddings for Tamil. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Student Research Workshop,\npages 94–111, Online. Association for Computa-\ntional Linguistics.\nRoland Kuhn. 2024. The Indigenous Languages Tech-\nnology (ILT) project at the National Research Coun-\ncil of Canada, and its context - NRC Publications\nArchive - Canada.ca.\nWilliam Lane and Steven Bird. 2020. Interactive word\ncompletion for morphologically complex languages.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 4600–4611,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nWilliam Lane and Steven Bird. 2021a. Local word dis-\ncovery for interactive transcription. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2058–2067, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nWilliam Lane and Steven Bird. 2021b. Local Word Dis-\ncovery for Interactive Transcription. ACL Anthology,\npages 2058–2067.\nAnna Langedijk, Verna Dankers, Phillip Lippe, Sander\nBos, Bryan Cardenas Guevara, Helen Yannakoudakis,\nand Ekaterina Shutova. 2022. Meta-learning for fast\ncross-lingual adaptation in dependency parsing. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 8503–8520, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nLeonard Collard Laura Stocker and Angela Rooney.\n2016. Aboriginal world views and colonisation: im-\nplications for coastal sustainability†. Local Environ-\nment, 21(7):844–865.\nÉric Le Ferrand, Steven Bird, and Laurent Besacier.\n2022a. Learning From Failure: Data Capture in an\nAustralian Aboriginal Community. ACL Anthology,\npages 4988–4998.\nEric Le Ferrand, Steven Bird, and Laurent Besacier.\n2022b. Learning from failure: Data capture in an\nAustralian aboriginal community. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4988–4998, Dublin, Ireland. Association for\nComputational Linguistics.\nColin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna\nFilighera, Abraham Owodunni, and Daniel White-\nnack. 2022. Bloom library: Multimodal datasets in\n300+ languages for a variety of downstream tasks.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8608–8621, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nMingqi Li, Fei Ding, Dan Zhang, Long Cheng, Hongxin\nHu, and Feng Luo. 2022a. Multi-level distillation\nof semantic knowledge for pre-training multilingual\nlanguage model. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3097–3106, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nShimin Li, Xiaotian Zhang, Yanjun Zheng, Linyang Li,\nand Xipeng Qiu. 2023a. Multijugate dual learning\nfor low-resource task-oriented dialogue system. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, pages 11037–11053, Toronto,\nCanada. Association for Computational Linguistics.\nXiangyang Li, Xiang Long, Yu Xia, and Sujian Li.\n2022b. Low resource style transfer via domain adap-\ntive meta learning. In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 3014–3026, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nYu Li, Baolin Peng, Pengcheng He, Michel Galley, Zhou\nYu, and Jianfeng Gao. 2023b. DIONYSUS: A pre-\ntrained model for low-resource dialogue summariza-\ntion. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1368–1386, Toronto,\nCanada. Association for Computational Linguistics.\nShining Liang, Linjun Shou, Jian Pei, Ming Gong,\nWanli Zuo, Xianglin Zuo, and Daxin Jiang. 2022.\nLabel-aware multi-level contrastive learning for\ncross-lingual spoken language understanding. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 9903–\n9918, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nJulian Linke, Philip N. Garner, Gernot Kubin, and Bar-\nbara Schuppler. 2022. Conversational speech recog-\nnition needs data? experiments with Austrian Ger-\nman. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 4684–\n4691, Marseille, France. European Language Re-\nsources Association.\nRobert Litschko, Ivan Vuli´c, Željko Agi´c, and Goran\nGlavaš. 2020. Towards instance-level parser selec-\ntion for cross-lingual transfer of dependency parsers.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 3886–3898,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nHuadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu,\nMaozong Zheng, Hong Chen, Jinzheng He, and Zhou\nZhao. 2023a. ViT-TTS: Visual text-to-speech with\nscalable diffusion transformer. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 15957–15969, Singa-\npore. Association for Computational Linguistics.\nLing Liu and Mans Hulden. 2020. Analogy models for\nneural word inflection. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 2861–2878, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nZoey Liu, Justin Spence, and Emily Prud’hommeaux.\n2023b. Investigating data partitioning strategies for\ncrosslinguistic low-resource ASR evaluation. In Pro-\nceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 123–131, Dubrovnik, Croatia. Associ-\nation for Computational Linguistics.\nFlorian Lux and Thang Vu. 2022. Language-agnostic\nmeta-learning for low-resource text-to-speech with\narticulatory features. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 6858–\n6868, Dublin, Ireland. Association for Computational\nLinguistics.\nManuel Mager, Özlem Çetino˘glu, and Katharina Kann.\n2020. Tackling the low-resource challenge for canon-\nical segmentation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5237–5250, Online. As-\nsociation for Computational Linguistics.\nJean\nMaillard,\nCynthia\nGao,\nElahe\nKalbassi,\nKaushik\nRam\nSadagopan,\nVedanuj\nGoswami,\nPhilipp Koehn, Angela Fan, and Francisco Guz-\nman. 2023.\nSmall data, big impact: Leveraging\nminimal data for effective machine translation. In\nProceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2740–2756, Toronto, Canada.\nAssociation for Computational Linguistics.\nChristopher D. Manning, Kevin Jansz, and Nitin In-\ndurkhya. 2001. Kirrkirr: Software for Browsing and\nVisual Exploration of a Structured Warlpiri Dictio-\nnary. Lit. Linguist. Computing, 16(2):135–151.\nKelly Marchisio, Ali Saad-Eldin, Kevin Duh, Carey\nPriebe, and Philipp Koehn. 2022. Bilingual lexicon\ninduction for low-resource languages using graph\nmatching via optimal transport. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2545–2561, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nDevansh Mehta, Sebastin Santy, Ramaravind Kommiya\nMothilal, Brij Mohan Lal Srivastava, Alok Sharma,\nAnurag Shukla, Vishnu Prasad, Venkanna U, Amit\nSharma, and Kalika Bali. 2020.\nLearnings from\ntechnological interventions in a low resource lan-\nguage: A case-study on Gondi. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 2832–2838, Marseille, France. European\nLanguage Resources Association.\nNelsi Melgarejo, Rodolfo Zevallos, Hector Gomez, and\nJohn E. Ortega. 2022. WordNet-QU: Development of\na lexical database for Quechua varieties. In Proceed-\nings of the 29th International Conference on Com-\nputational Linguistics, pages 4429–4433, Gyeongju,\nRepublic of Korea. International Committee on Com-\nputational Linguistics.\nNguyen Minh, Vu Hoang Tran, Vu Hoang, Huy Duc\nTa, Trung Huu Bui, and Steven Quoc Hung Truong.\n2022. ViHealthBERT: Pre-trained language models\nfor Vietnamese in health text mining. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 328–337, Marseille, France.\nEuropean Language Resources Association.\nBenjamin Minixhofer, Fabian Paischer, and Navid Rek-\nabsaz. 2022. WECHSEL: Effective initialization of\nsubword embeddings for cross-lingual transfer of\nmonolingual language models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3992–4006,\nSeattle, United States. Association for Computational\nLinguistics.\nSarah Moeller, Ling Liu, and Mans Hulden. 2021. To\nPOS tag or not to POS tag: The impact of POS tags\non morphological learning in low-resource settings.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 966–978,\nOnline. Association for Computational Linguistics.\nNikita Moghe, Evgeniia Razumovskaia, Liane Guillou,\nIvan Vuli´c, Anna Korhonen, and Alexandra Birch.\n2023. Multi3NLU++: A multilingual, multi-intent,\nmulti-domain dataset for natural language under-\nstanding in task-oriented dialogue. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 3732–3755, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nSyed Mostofa Monsur, Sakib Chowdhury, Md Shahrar\nFatemi, and Shafayat Ahmed. 2022. SHONGLAP: A\nlarge Bengali open-domain dialogue corpus. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 5797–5804, Marseille,\nFrance. European Language Resources Association.\nAaron Mueller, Garrett Nicolai, Arya D. McCarthy, Dy-\nlan Lewis, Winston Wu, and David Yarowsky. 2020.\nAn analysis of massively multilingual neural machine\ntranslation for low-resource languages. In Proceed-\nings of the Twelfth Language Resources and Evalua-\ntion Conference, pages 3710–3718, Marseille, France.\nEuropean Language Resources Association.\nJonathan Mukiibi, Andrew Katumba, Joyce Nakatumba-\nNabende, Ali Hussein, and Joshua Meyer. 2022. The\nmakerere radio speech corpus: A Luganda radio cor-\npus for automatic speech recognition. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 1945–1954, Marseille, France.\nEuropean Language Resources Association.\nSaliha Muradoglu and Mans Hulden. 2022.\nEeny,\nmeeny, miny, moe. how to choose data for morpho-\nlogical inflection. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7294–7303, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nWilhelmina Nekoto, Vukosi Marivate, Tshinondiwa\nMatsila,\nTimi\nFasubaa,\nTaiwo\nFagbohungbe,\nSolomon Oluwole Akinola, Shamsuddeen Muham-\nmad, Salomon Kabongo Kabenamualu, Salomey\nOsei, Freshia Sackey, Rubungo Andre Niyongabo,\nRicky Macharm, Perez Ogayo, Orevaoghene Ahia,\nMusie Meressa Berhe,\nMofetoluwa Adeyemi,\nMasabata Mokgesi-Selinga, Lawrence Okegbemi,\nLaura Martinus, Kolawole Tajudeen, Kevin Degila,\nKelechi Ogueji, Kathleen Siminyu, Julia Kreutzer,\nJason Webster, Jamiil Toure Ali, Jade Abbott,\nIroro Orife, Ignatius Ezeani, Idris Abdulkadir Dan-\ngana, Herman Kamper, Hady Elsahar, Goodness\nDuru, Ghollah Kioko, Murhabazi Espoir, Elan van\nBiljon, Daniel Whitenack, Christopher Onyefuluchi,\nChris Chinenye Emezue, Bonaventure F. P. Dossou,\nBlessing Sibanda, Blessing Bassey, Ayodele Olabiyi,\nArshath Ramkilowan, Alp Öktem, Adewale Akin-\nfaderin, and Abdallah Bashir. 2020. Participatory\nResearch for Low-resourced Machine Translation: A\nCase Study in African Languages. ACL Anthology,\npages 2144–2160.\nHellina Hailu Nigatu and Inioluwa Deborah Raji. 2024.\n“I Searched for a Religious Song in Amharic and\nGot Sexual Content Instead”: Investigating Online\nHarm in Low-Resourced Languages on YouTube. In\nProceedings of the 2024 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’24,\npage 141–160, New York, NY, USA. Association for\nComputing Machinery.\nRubungo Andre Niyongabo, Qu Hong, Julia Kreutzer,\nand Li Huang. 2020. KINNEWS and KIRNEWS:\nBenchmarking cross-lingual text classification for\nKinyarwanda and Kirundi. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 5507–5521, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nNLLB. 2024. Scaling neural machine translation to 200\nlanguages.\nAntoine Nzeyimana and Andre Niyongabo Rubungo.\n2022.\nKinyaBERT: a morphology-aware Kin-\nyarwanda language model. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5347–5363, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nBruce Oliver, Clarissa Forbes, Changbing Yang, Farhan\nSamir, Edith Coates, Garrett Nicolai, and Miikka Sil-\nfverberg. 2022. An inflectional database for gitksan.\nIn Proceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 6597–6606, Mar-\nseille, France. European Language Resources Asso-\nciation.\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2021.\nERNIE-M: Enhanced multilingual representation by\naligning cross-lingual semantics with monolingual\ncorpora. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 27–38, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMarinela Parovi´c, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2022. BAD-X: Bilingual adapters im-\nprove zero-shot cross-lingual transfer. In Proceed-\nings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1791–1799, Seattle, United States. Association for\nComputational Linguistics.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116, Online. Association for Computa-\ntional Linguistics.\nDhrubajyoti Pathak, Sukumar Nandi, and Priyankoo\nSarmah. 2022. AsNER - annotated dataset and base-\nline for Assamese named entity recognition. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 6571–6577, Marseille,\nFrance. European Language Resources Association.\nVaidehi Patil, Partha Talukdar, and Sunita Sarawagi.\n2022.\nOverlap-based vocabulary generation im-\nproves cross-lingual transfer among related lan-\nguages. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 219–233, Dublin,\nIreland. Association for Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021. UNKs everywhere: Adapting mul-\ntilingual language models to new scripts. In Proceed-\nings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10186–10203,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nAidan Pine, Dan Wells, Nathan Brinklow, Patrick Lit-\ntell, and Korin Richmond. 2022a. Requirements and\nmotivations of low-resource speech synthesis for lan-\nguage revitalization. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7346–\n7359, Dublin, Ireland. Association for Computational\nLinguistics.\nAidan Pine, Dan Wells, Nathan Brinklow, Patrick Lit-\ntell, and Korin Richmond. 2022b.\nRequirements\nand Motivations of Low-Resource Speech Synthesis\nfor Language Revitalization. ACL Anthology, pages\n7346–7359.\nEdoardo M. Ponti, Ivan Vuli´c, Ryan Cotterell, Marinela\nParovic, Roi Reichart, and Anna Korhonen. 2021.\nParameter space factorization for zero-shot learning\nacross tasks and languages. Transactions of the Asso-\nciation for Computational Linguistics, 9:410–428.\nNamoos Hayat Qasmi, Haris Bin Zia, Awais Athar, and\nAgha Ali Raza. 2020. SimplifyUR: Unsupervised\nlexical text simplification for Urdu. In Proceedings\nof the Twelfth Language Resources and Evaluation\nConference, pages 3484–3489, Marseille, France. Eu-\nropean Language Resources Association.\nKunxun Qi, Hai Wan, Jianfeng Du, and Haolan Chen.\n2022. Enhancing cross-lingual natural language in-\nference by prompt-learning from cross-lingual tem-\nplates. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1910–1923, Dublin,\nIreland. Association for Computational Linguistics.\nAnnie Rajan, Ambuja Salgaonkar, and Ramprasad Joshi.\n2020. A survey of konkani nlp resources. Computer\nScience Review, 38:100299.\nArun Ramachandran and Gerard de Melo. 2020. Cross-\nlingual emotion lexicon induction using representa-\ntion alignment in low-resource settings. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5879–5890, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nSahana Ramnath, Melvin Johnson, Abhirut Gupta, and\nAravindan Raghuveer. 2021. HintedBT: Augment-\ning Back-Translation with quality and transliteration\nhints. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1717–1733, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMachel Reid, Junjie Hu, Graham Neubig, and Yutaka\nMatsuo. 2021. AfroMT: Pretraining strategies and\nreproducible benchmarks for translation of 8 African\nlanguages. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1306–1320, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nShruti Rijhwani, Shuyan Zhou, Graham Neubig, and\nJaime Carbonell. 2020.\nSoft gazetteers for low-\nresource named entity recognition. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 8118–8123, On-\nline. Association for Computational Linguistics.\nRenato Rocha Souza, Amelie Dorn, Barbara Piringer,\nand Eveline Wandl-Vogt. 2020.\nIdentification of\nindigenous knowledge concepts through semantic\nnetworks, spelling tools and word embeddings. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 943–947, Marseille,\nFrance. European Language Resources Association.\nPaul Röttger, Debora Nozza, Federico Bianchi, and Dirk\nHovy. 2022. Data-efficient strategies for expanding\nhate speech detection into under-resourced languages.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5674–5691, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nEfsun Sarioglu Kayi, Linyong Nan, Bohan Qu, Mona\nDiab, and Kathleen McKeown. 2020. Detecting ur-\ngency status of crisis tweets: A transfer learning\napproach for low resource languages. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 4693–4703, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nKumar Saunack, Kumar Saurav, and Pushpak Bhat-\ntacharyya. 2021. How low is too low? a monolingual\ntake on lemmatisation in Indian languages. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4088–4094, Online. Association for Computational\nLinguistics.\nKumar Saurav, Kumar Saunack, and Pushpak Bhat-\ntacharyya. 2020. Analysing cross-lingual transfer\nin lemmatisation for Indian languages. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6070–6076, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nMichael Schlichtkrull and Anders Søgaard. 2017. Cross-\nlingual dependency parsing with late decoding for\ntruly low-resource languages. In Proceedings of the\n15th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Volume 1,\nLong Papers, pages 220–229, Valencia, Spain. Asso-\nciation for Computational Linguistics.\nFabian David Schmidt, Ivan Vuli´c, and Goran Glavaš.\n2022. Don’t stop fine-tuning: On training regimes\nfor few-shot cross-lingual transfer with multilingual\nlanguage models. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 10725–10742, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nLane Schwartz. 2022. Primum Non Nocere: Before\nworking with Indigenous data, the ACL must con-\nfront ongoing colonialism. ACL Anthology, pages\n724–731.\nHarshita Sharma, Pruthwik Mishra, and Dipti Sharma.\n2022. HAWP: a dataset for Hindi arithmetic word\nproblem solving. In Proceedings of the Thirteenth\nLanguage Resources and Evaluation Conference,\npages 3479–3490, Marseille, France. European Lan-\nguage Resources Association.\nAditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat,\nMia Chen, Sneha Kudugunta, Naveen Arivazhagan,\nand Yonghui Wu. 2020. Leveraging monolingual\ndata with self-supervision for multilingual neural ma-\nchine translation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2827–2835, Online. Association for\nComputational Linguistics.\nGerardo Sierra Martínez, Cynthia Montaño, Gemma\nBel-Enguix,\nDiego\nCórdova,\nand\nMargarita\nMota Montoya. 2020. CPLM, a parallel corpus for\nMexican languages: Development and interface. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 2947–2952, Marseille,\nFrance. European Language Resources Association.\nClaytone Sikasote and Antonios Anastasopoulos. 2022.\nBembaSpeech: A speech recognition corpus for the\nBemba language. In Proceedings of the Thirteenth\nLanguage Resources and Evaluation Conference,\npages 7277–7283, Marseille, France. European Lan-\nguage Resources Association.\nGary F. Simons, Abbey L. L. Thomas, and Chad K. K.\nWhite. 2022.\nAssessing digital language support\non a global scale. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 4299–4305, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nHeather Simpson, Christopher Cieri, Kazuaki Maeda,\nKathryn Baker, and Boyan Onyshkevych. 2008. Hu-\nman Language Technology Resources for Less Com-\nmonly Taught Languages: Lessons Learned Toward\nCreation of Basic Language Resources. ACL Anthol-\nogy.\nAlexey Sorokin. 2020.\nGetting more data for low-\nresource morphological inflection: Language mod-\nels and data augmentation. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 3978–3983, Marseille, France. European\nLanguage Resources Association.\nDaniel Stiles. 1982. A history of the hunting peoples of\nthe northern east africa coast: Ecological and socio-\neconomic considerations. Paideuma, 28:165–174.\nYixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta,\nDeng Cai, Yi-An Lai, and Yi Zhang. 2022. Multi-task\npre-training for plug-and-play task-oriented dialogue\nsystem. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4661–4676, Dublin,\nIreland. Association for Computational Linguistics.\nHaoran Sun and Deyi Xiong. 2022. Language branch\ngated multilingual neural machine translation. In\nProceedings of the 29th International Conference\non Computational Linguistics, pages 5046–5053,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nQingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yam-\ning Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo\nGeng, and Daxin Jiang. 2022a. Multimodal dialogue\nresponse generation. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2854–\n2866.\nQingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yam-\ning Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo\nGeng, and Daxin Jiang. 2022b. Multimodal dialogue\nresponse generation. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2854–\n2866, Dublin, Ireland. Association for Computational\nLinguistics.\nMauro Tosco. 1992. Dahalo: an Endangered Language.\nLanguage Death: Factual and Theoretical Explo-\nrations (“Contributions to the Sociology of Language\n64”).\nAhmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-\nYin Ko, Daniel D’souza, Gbemileke Onilude, Neel\nBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,\net al. 2024. Aya model: An instruction finetuned\nopen-access multilingual language model.\narXiv\npreprint arXiv:2402.07827.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, and Gert-\njan van Noord. 2020. UDapter: Language adaptation\nfor truly Universal Dependency parsing. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n2302–2315, Online. Association for Computational\nLinguistics.\nJessica Wakeman. 2021. Cherokee fight to save lan-\nguage from extinction.\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,\nFei Huang, and Kewei Tu. 2020. Structure-level\nknowledge distillation for multilingual sequence la-\nbeling. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n3317–3330, Online. Association for Computational\nLinguistics.\nMengzhou Xia, Xiang Kong, Antonios Anastasopou-\nlos, and Graham Neubig. 2019. Generalized data\naugmentation for low-resource translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5786–\n5796, Florence, Italy. Association for Computational\nLinguistics.\nMengzhou Xia, Guoqing Zheng, Subhabrata Mukherjee,\nMilad Shokouhi, Graham Neubig, and Ahmed Has-\nsan Awadallah. 2021.\nMetaXL: Meta representa-\ntion transformation for low-resource cross-lingual\nlearning. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 499–511, Online. Association\nfor Computational Linguistics.\nYi Xu, Shuqian Sheng, Jiexing Qi, Luoyi Fu, Zhouhan\nLin, Xinbing Wang, and Chenghu Zhou. 2023. Unsu-\npervised graph-text mutual conversion with a unified\npretrained language model. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5130–5144, Toronto, Canada. Association for Com-\nputational Linguistics.\nIsil Yakut Kilic and Shimei Pan. 2022. Incorporating\nLIWC in neural networks to improve human trait and\nbehavior analysis in low resource scenarios. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 4532–4539, Marseille,\nFrance. European Language Resources Association.\nSeid Muhie Yimam, Hizkiel Mitiku Alemayehu,\nAbinew Ayele, and Chris Biemann. 2020. Exploring\nAmharic sentiment analysis from social media texts:\nBuilding annotation tools and classification mod-\nels. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 1048–\n1060, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nMichelle Yuan, Mozhi Zhang, Benjamin Van Durme,\nLeah Findlater, and Jordan Boyd-Graber. 2020. Inter-\nactive refinement of cross-lingual word embeddings.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5984–5996, Online. Association for Computa-\ntional Linguistics.\nJiali Zeng, Yufan Jiang, Yongjing Yin, Yi Jing, Fandong\nMeng, Binghuai Lin, Yunbo Cao, and Jie Zhou. 2023.\nSoft language clustering for multilingual model pre-\ntraining. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 7021–7035, Toronto,\nCanada. Association for Computational Linguistics.\nRodolfo Zevallos and Nuria Bel. 2023a. Hints on the\ndata for language modeling of synthetic languages\nwith transformers. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 12508–\n12522, Toronto, Canada. Association for Computa-\ntional Linguistics.\nRodolfo Zevallos and Núria Bel. 2023b. Hints on the\ndata for language modeling of synthetic languages\nwith transformers. ACL Anthology, pages 12508–\n12522.\nRui Zhang, Caitlin Westerfield, Sungrok Shim, Gar-\nrett Bingham, Alexander Fabbri, William Hu, Neha\nVerma, and Dragomir Radev. 2019. Improving low-\nresource cross-lingual document retrieval by rerank-\ning with deep bilingual representations. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3173–3179, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nGuolin Zheng, Yubei Xiao, Ke Gong, Pan Zhou, Xiao-\ndan Liang, and Liang Lin. 2021. Wav-BERT: Coop-\nerative acoustic and linguistic representation learning\nfor low-resource speech recognition. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 2765–2777, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nShuyan Zhou, Shruti Rijhwani, John Wieting, Jaime\nCarbonell, and Graham Neubig. 2020. Improving\ncandidate generation for low-resource cross-lingual\nentity linking. Transactions of the Association for\nComputational Linguistics, 8:109–124.\nYicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, and\nQi Zhang. 2021. Low-resource dialogue summariza-\ntion with domain-agnostic multi-source pretraining.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n80–91, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nA\nLabels for Classifying Languages\nIn this section, we provide the descriptions for la-\nbels used for language vitality and digital access\nused in Figure 4.\nA.1\nVitality\nIn this work,\nwe refer to the scale from\nEthnologue14 which is derived from the Ex-\npanded Graded Intergenerational Disruption Scale\n(EGIDS) (Anderbeck, 2015).\nInstitutional\n— The language has been devel-\noped to the point that it is used and sustained by\ninstitutions beyond the home and community.\nStable\n— The language is not being sustained\nby formal institutions, but it is still the norm in the\nhome and community that all children learn and\nuse the language.\nEndangered\n— It is no longer the norm that chil-\ndren learn and use this language.\nExtinct\n- The language is no longer used, and\nno one retains a sense of ethnic identity associated\nwith the language.\nA.2\nDigital Access\nThis taxonomy is from Simons et al. (2022) and is\nalso used by Ethnologue.\nStill\n— this language shows no signs of digital\nsupport\nEmerging\n— the language has some content in\ndigital form and/or encoding tools\nAscending\n— the language has some spell check-\ning or localized tools or machine translation as well\nVital\n— the language is supported by multiple\ntools in all of the above categories and as well as\nsome speech processing\nThriving\n— the language has all of the above\nplus virtual assistants\nB\nCriteria used in Studying Languages\nFigure 5 shows the distributions of the various\ncriteria used for categorizing a language as ‘low-\nresource’ in the studied languages. Figure 6 depicts\ndifferent perspectives used to refer to the lack of a\ndataset for a language.\nC\nMost frequently studied languages\nFigure 7 shows the top 20 most frequently stud-\nied languages in our sample. We see that Swahili\nand Telugu take the lead with 14 papers working\non them. Geographically, we observe that Indian\n14https://www.ethnologue.com/\nFigure 5: Distribution of criteria stated by papers in our study to categorize languages as low-resource.\nFigure 6: Criteria used in the papers to show lack of\ndata.\nlanguages (n = 7) are the most represented in our\nsample, with an equal number of languages (n = 7)\nfrom the entire continent of Africa.\nFigure 7: Number of papers per language for the top-20\nmost studied languages.\nD\nCategories used to define low-resource\nHere, we grouped papers according to the criteria\nused in the paper to categorize a language as a\nlow-resource language.\nSocio-political\n[(Maillard et al., 2023; Coto-\nSolano, 2022; Pathak et al., 2022)]\nResources\nNative Speakers\n[(Pine et al., 2022a; Oliver\net al., 2022; Coto-Solano, 2022; Feldman and Coto-\nSolano, 2020; Leong et al., 2022)]\nOnline Presence\n[(Bamutura et al., 2020;\nSierra Martínez et al., 2020; Adelani et al., 2022;\nNzeyimana and Niyongabo Rubungo, 2022; Feld-\nman and Coto-Solano, 2020; Bustamante et al.,\n2020; Patil et al., 2022; Adelani et al., 2022)]\nLanguage experts\n[(Brixey et al., 2020; Yi-\nmam et al., 2020)]\nArtifacts\nLinguistic Knowledge\n[(Qasmi et al., 2020;\nCoto-Solano, 2022)]\nData\n[Ferger\n(2020);\nZevallos\nand\nBel\n(2023a); Pine et al. (2022a); Fei and Li (2020);\nEskander et al. (2020a); Xia et al. (2021); Goyal\net al. (2022a); Sorokin (2020); Pfeiffer et al. (2020);\nSierra Martínez et al. (2020); Ahuja et al. (2022);\nMehta et al. (2020); Le Ferrand et al. (2022b);\nMukiibi et al. (2022); Chaudhary et al. (2021);\nÜstün et al. (2020); Eskander et al. (2020b); Liang\net al. (2022); Pfeiffer et al. (2021); ImaniGooghari\net al. (2022); Dione et al. (2022); Chukwuneke\net al. (2022); Schmidt et al. (2022); Hasan et al.\n(2020); Muradoglu and Hulden (2022); Biswas\net al. (2020); Marchisio et al. (2022); Maillard\net al. (2023); Litschko et al. (2020); Coto-Solano\n(2022); Gaim et al. (2023); Adebara et al. (2022);\nKrishnan and Ragavan (2021); Alabi et al. (2020);\nYimam et al. (2020); Li et al. (2022a); Saunack\net al. (2021); Niyongabo et al. (2020); Ramnath\net al. (2021); Ponti et al. (2021); Adouane et al.\n(2020); Reid et al. (2021); Parovi´c et al. (2022);\nMinixhofer et al. (2022); Zeng et al. (2023); Pathak\net al. (2022); Botha et al. (2020); Chakrabarty et al.\n(2022); Debnath et al. (2021); Sarioglu Kayi et al.\n(2020); Alabi et al. (2022); Ko et al. (2021); Liu\nand Hulden (2020); Wang et al. (2020); Zhou et al.\n(2020); Sharma et al. (2022); Bari et al. (2021);\nImaniGooghari et al. (2023); Yuan et al. (2020);\nGezmu et al. (2022); Qi et al. (2022); Knowles\nand Littell (2022); Khayrallah et al. (2020); Mager\net al. (2020); Monsur et al. (2022); Ramachan-\ndran and de Melo (2020); Sun and Xiong (2022);\nHangya et al. (2022); Saurav et al. (2020); Ouyang\net al. (2021); Parvez and Chang (2021); Moeller\net al. (2021); Fomicheva et al. (2022); Mueller\net al. (2020); Siddhant et al. (2020); Bartelds et al.\n(2023); Daniel et al. (2019); Chen et al. (2022);\nFetahu et al. (2022); Li et al. (2022b,b); Bartelds\nand Wieling (2022); Minixhofer et al. (2022); Minh\net al. (2022); Koloski et al. (2022); Coto-Solano\net al. (2022); Yakut Kilic and Pan (2022); Linke\net al. (2022); Langedijk et al. (2022); Muradoglu\nand Hulden (2022); Huang et al. (2022); Jundi\net al. (2023); Xu et al. (2023); Li et al. (2023a);\nSu et al. (2022); Hua et al. (2023); Li et al. (2023b);\nSun et al. (2022b); Moghe et al. (2023); Bhat et al.\n(2023); de Vries et al. (2022); Eder et al. (2021);\nZhang et al. (2019); Fang and Cohn (2017); Xia\net al. (2019); Liu et al. (2023b); Schlichtkrull and\nSøgaard (2017); Dingliwal et al. (2021); Ebrahimi\net al. (2023); Röttger et al. (2022); Ghosh et al.\n(2023); Ding et al. (2020); Zou et al. (2021); Lux\nand Vu (2022); Zheng et al. (2021); Liu et al.\n(2023a)]\nTechnology\n[(Bamutura et al., 2020; Bya-\nmugisha, 2022; Melgarejo et al., 2022; Yimam\net al., 2020; Himoro and Pareja-Lora, 2022; Li\net al., 2022a; Niyongabo et al., 2020; Duggenpudi\net al., 2022; Avram et al., 2022; Lane and Bird,\n2021b; Eskander et al., 2020a; Rocha Souza et al.,\n2020; Lane and Bird, 2020; de Lhoneux et al., 2022;\nImaniGooghari et al., 2022; Brixey et al., 2020; Ri-\njhwani et al., 2020; Sikasote and Anastasopoulos,\n2022; Adouane et al., 2020; Botha et al., 2020;\nMoeller et al., 2021; Jin et al., 2020; Dhar et al.,\n2022; Pfeiffer et al., 2020; Leong et al., 2022)]\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-10-28",
  "updated": "2024-10-28"
}