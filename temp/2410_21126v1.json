{
  "id": "http://arxiv.org/abs/2410.21126v1",
  "title": "Current State-of-the-Art of Bias Detection and Mitigation in Machine Translation for African and European Languages: a Review",
  "authors": [
    "Catherine Ikae",
    "Mascha Kurpicz-Briki"
  ],
  "abstract": "Studying bias detection and mitigation methods in natural language processing\nand the particular case of machine translation is highly relevant, as societal\nstereotypes might be reflected or reinforced by these systems. In this paper,\nwe analyze the state-of-the-art with a particular focus on European and African\nlanguages. We show how the majority of the work in this field concentrates on\nfew languages, and that there is potential for future research to cover also\nthe less investigated languages to contribute to more diversity in the research\nfield.",
  "text": "Current State-of-the-Art of Bias Detection and Mitigation in Machine\nTranslation for African and European Languages: a Review\nCatherine Ikae\nApplied Machine Intelligence\nBern University of Applied Sciences\nBiel, Switzerland\ncatherine.ikae@bfh.ch\nMascha Kurpicz-Briki\nApplied Machine Intelligence\nBern University of Applied Sciences\nBiel, Switzerland\nmascha.kurpicz@bfh.ch\nAbstract\nStudying bias detection and mitigation meth-\nods in natural language processing and the par-\nticular case of machine translation is highly rel-\nevant, as societal stereotypes might be reflected\nor reinforced by these systems. In this paper,\nwe analyze the state-of-the-art with a particular\nfocus on European and African languages. We\nshow how the majority of the work in this field\nconcentrates on few languages, and that there\nis potential for future research to cover also\nthe less investigated languages to contribute to\nmore diversity in the research field.\n1\nIntroduction\nBias in Natural Language Processing (NLP) tech-\nnologies has over the last years become an impor-\ntant subject of research. It was been shown that\nsocietal stereotypes are included in NLP models\n(e.g., Bolukbasi et al. (2016) Caliskan et al. (2017)\nWilson and Caliskan (2024)). The bias can hap-\npen at different stages of the NLP pipeline Hovy\nand Prabhumoye (2021). Additionally, different\nforms of bias and in particular also intersectional\nbias can be present, making it challenging to detect\nand mitigate unwanted behavior caused by this in\nNLP models or applications. Furthermore, it has\nbeen shown that there is a disparity of language\nresources available for different languages (e.g.,\nJoshi et al. (2020)). One particular case where bias\ncan appear is machine translation (MT). For exam-\nple, when translating the non-gendered sentence\nI am a professor. from English to a language like\nGerman. In German, this sentence could translate\nto Ich bin ein Professor. or Ich bin eine Profes-\nsorin., depending on the gender of the speaker.\nThe machine translation application will typically\nselect one of them, and this choice might be im-\npacted by biases in the underlying model. A case\nstudy has shown that in Google Translate, even\nif not expecting a 50:50 gender distribution, the\nmachine translation engine yielded male defaults\nmuch more frequently than it would be expected\nfrom the corresponding demographic data (Prates\net al., 2020).\nWhereas different work has recently discussed\nthe matter of bias in machine translation from dif-\nferent perspectives (e.g., Savoldi et al. (2021a),\nGallegos et al. (2024)), there is a lack of overview\non the languages involved in the current state-of-\nthe-art research in this field. In this paper, we give\nan overview over the state-of-the-art of bias detec-\ntion and mitigation in machine translation, with a\nfocus on research involving European and African\nlanguages.\n2\nMethods\nWe have searched the literature in two steps. We\nfirst queried Web of Science using the query ma-\nchine translation AND bias, leading to 402 results.\nThose were downloaded as an excel file for fur-\nther processing. We then used a script to filter title\nand abstract, to identify papers where at least one\nlanguage from a list of African and European lan-\nguages (full list in Appendix A) appears in it. The\nlist of languages was generated using ChatGPT, in\na two step approach (see Appendix A for details).\nAfter this filtering, we had 11 papers for African\nlanguages and 39 papers for European languages.\nThose were then further analyzed by manual ab-\nstract screening by one of the authors. In unclear\ncases, the full-text of the paper was consulted and\nthe cases were discussed in the team.\nWhile doing this analysis, the authors realized\nthat relevant papers from the ACL Anthology were\nnot included, even though ACL was selected as a\nsource. We therefore added another search directly\non the ACL Anthology. The queries were com-\nposed to use already the filter for the languages.\nThe query is described in Appendix A. The first 5\npages of results sorted by relevance were manually\n1\narXiv:2410.21126v1  [cs.CL]  28 Oct 2024\nassessed. Duplicates were removed. 18 papers\nwere considered relevant for the purpose of our\nstudy. The excluded and included papers and ex-\nclusion/inclusion reasons are listed in Appendix\nB.\nAfter this search and selection process, we iden-\ntified 9 relevant papers for African languages, and\n25 papers for European languages, which will be\ndiscussed and compared in the next section.\n3\nResults\nIn reviewing the existing research on bias in ma-\nchine translation (MT) found in our study, it be-\ncomes evident that the majority of studies focuses\non a selection of few high-resource languages, of-\nten from Western Europe. Languages such as En-\nglish, German, French, and Spanish dominate the\nresearch landscape, appearing consistently across\nnumerous studies. From the languages in the query,\nGerman and Spanish are the most frequently stud-\nied, appearing in 14 papers and French in 7 pa-\npers. Italian, Hebrew, Arabic and Chinese also\nreceives notable attention, appearing in 5 papers\n(an overview of languages is shown in Table 1.\nThe other language from the query that appeared\n4 times in the reviewed papers include Russian,\nwhile those that appeared twice are Polish, Por-\ntuguese, Hungarian, Finnish, Estonian, Catalan,\nCzech, Korean, Hindi, Japanese and Hebrew. Pol-\nish and Russian are predominantly featured in stud-\nies addressing gender bias in machine translation,\nwhile Hungarian and Finnish are often discussed\nin the context of underrepresented European lan-\nguages, highlighting their unique linguistic chal-\nlenges. Yoruba and Hebrew are often linked to\nspecific translation or gender-related biases.\nSeveral languages from the query appear only\nonce in the reviewed literature, indicating a limited\nfocus on them. These include Mandarin, Turk-\nish, Ukrainian, Bengali, Punjabi, Gujarati, Tamil,\nIcelandic, Marathi, Latvian, Romanian, Yoruba,\nIndonesian, Mongolian, and one unspecified lan-\nguage. Arabic and Swahili, despite their signifi-\ncance in non-European contexts, receive limited\nattention, with Swahili being particularly under-\nrepresented in the research. Icelandic, though a\nunique case of a lesser-studied European language,\nalso appeared in only one paper.\nFrom the original query, many other languages,\nincluding Amharic, Tigrinya, Kabyle, Somali, and\nHausa, are entirely absent from the reviewed re-\nsearch.\nSome languages that were not part of the origi-\nnal query did appear in the reviewed studies. For\ninstance, Mongolian, Malay, and Japanese were\nmentioned in a few papers, indicating that there\nis some effort to explore bias in less commonly\nstudied languages.\nWe analyzed and compared the methodologies\nemployed across the studies focused on bias detec-\ntion and mitigation in machine translation, high-\nlighting key approaches such as corpus-based eval-\nuations, gendered translation benchmarks, and al-\ngorithmic bias quantification to assess how each\nmethod captures and mitigates biases across differ-\nent languages and systems. This led to the follow-\ning groups of papers.\nPapers that focus on bias detection using pre-\ndefined test suites, corpora, or experiments\nto measure gender bias in machine transla-\ntions.\nBias detection often focuses on how sys-\ntems handle gendered terms, such as pronouns\nand professions, or whether stereotypes are propa-\ngated in translations. For instance, Sólmundsdót-\ntir et al. (2022) focuses on Icelandic, where rich\ngender inflections in adjectives and nouns make\nit ideal for studying bias through inflectional er-\nrors. In contrast, Prates et al. (2019) evaluates\nEnglish, which uses gendered pronouns, Hungar-\nian, a gender-neutral language, Chinese, which has\ngender-ambiguous pronouns, and Yoruba, where\nnoun classes can affect gender interpretations. Pa-\npers like Kocmi et al. (2020) analyze Czech, Ger-\nman, Polish, and Russian—languages with strong\ngendered noun and pronoun systems, where incor-\nrect gender agreement can clearly highlight bias. In\nPolish, for example, grammatical gender is deeply\nembedded in professions, making it a focus of\nPułaczewska (2024). French, Italian, and Span-\nish are explored by Stewart and Mihalcea (2024)\nto assess bias in the context of same-gender re-\nlationships, as these Romance languages require\nconsistent gender agreement. Meanwhile, English\nand Catalan in Mash et al. (2024) reveal biases\nrelated to gender omission. Lastly, Costa-jussà\net al. (2020) uses a multilingual approach, examin-\ning diverse languages to reveal how bias manifests\nacross different grammatical systems.\nBias mitigation in machine translation, intro-\nducing innovative methods to reduce gender\nbias in translations.\nThese approaches generally\nfocus on two main strategies: adjusting the train-\n2\n#\nAuthor(s)\nLanguages\n1\nSólmundsdóttir et al. (2022)\nIcelandic\n2\nSavoldi et al. (2021b)\nEnglish, French, German, Spanish\n3\nLee et al. (2023)\nEnglish, French, Spanish\n4\nCosta-jussà and de Jorge (2020)\nEnglish, Spanish\n5\nTriboulet and Bouillon (2023)\nEnglish, French, Italian, Turkish\n6\nKocmi et al. (2020)\nCzech, German, Polish, Russian\n7\nVanmassenhove et al. (2021)\nEnglish, French, Spanish\n8\nStewart and Mihalcea (2024)\nFrench, Italian, Spanish\n9\nAttanasio et al. (2023)\nEnglish, German, Spanish\n10\nLevy et al. (2021)\nEnglish, Arabic, Czech, German, Spanish , Hebrew,\nItalian, Russian, Ukrainian\n11\nCabrera and Niehues (2023)\nEnglish, Hebrew\n12\nMash et al. (2024)\nEnglish, Catalan\n13\nVashishtha et al. (2023)\nHindi, Bengali, Marathi, Punjabi, Gujarati, Tamil,\nGerman, Japanese, Arabic, Spanish, Mandarin, Por-\ntuguese, Russian, Indonesian\n14\nHabash et al. (2019)\nArabic\n15\nStafanoviˇcs et al. (2020)\nEnglish, Latvian\n16\nSaunders and Byrne (2020)\nEnglish, Spanish\n17\nPrates et al. (2019)\nEnglish, Hungarian, Chinese, Yoruba\n18\nJi et al. (2020)\nKorean, Chinese, Mongolian\n19\nElaraby et al. (2018)\nEnglish, Arabic\n20\nMoryossef et al. (2019)\nEnglish, Hebrew\n21\nWisniewski et al. (2022)\nGerman, English\n22\nEscudé Font and Costa-jussà\n(2019)\nEnglish, Spanish\n23\nCho et al. (2021)\nEnglish, German, French\n24\nKrüger\nand\nHackenbuchner\n(2022)\nEnglish, German\n25\nCampolungo et al. (2022)\nEnglish, Chinese, Russian, Italian, Spanish, German\n26\nFreitag et al. (2019)\nEnglish, French, Romanian, German\n27\nCosta-jussà et al. (2020)\nEnglish, Spanish, Catalan\n28\nWang et al. (2023)\nChinese, Hungarian, Hindi, English, Japanese, Por-\ntuguese, Spanish, Arabic, Korean, Hebrew, Italian\n29\nShi et al. (2023)\nEnglish, German\n30\nPułaczewska (2024)\nEnglish, Polish\n31\nLu et al. (2020)\nEnglish, German, Chinese\n32\nLiu et al. (2023)\nEnglish, German\n33\nKaeser-Chen et al. (2020)\nUnspecified\n34\nIluz et al. (2023)\nEnglish, German, Spanish, Hebrew\nTable 1: Papers and Languages Studied in Machine Translation Bias.\ning data and modifying the model itself. For ex-\nample, dataset adjustments, such as balancing gen-\nder representation in English and Latvian, helped\nto address the rich gender agreement system in\nLatvian, which requires attention to grammatical\ngender inflections (Stafanoviˇcs et al., 2020). Con-\ntrastive learning, applied in French, Spanish, and\nEnglish, is effective for languages with consistent\ngrammatical gender rules, particularly in Spanish,\nwhere gendered nouns demand balanced training\n(Lee et al., 2023). Fine-tuning on gender-balanced\ndatasets has also proven successful for languages\n3\nlike Spanish (Costa-jussà and de Jorge, 2020). In\nmultilingual settings, debiasing word embeddings,\nas seen in English and German, helps manage\nGerman’s three-gender system, which requires a\nmore precise approach than binary-gendered lan-\nguages (Escudé Font and Costa-jussà, 2019). Fur-\nthermore, context injection, used in Hebrew and\nEnglish, adds external gender and number clues\nto clarify ambiguities in pronouns, particularly in\nHebrew, where gender-neutral pronouns are absent\n(Moryossef et al., 2019). These methods illustrate\nhow bias mitigation strategies are tailored to the\nspecific linguistic properties of each language.\nAddressing specific gender-related challenges\nsuch as pronoun translation, coreference resolu-\ntion, and the handling of relationships in trans-\nlations.\nThese works often introduce context-\nsensitive approaches or language-specific method-\nologies to improve gender accuracy. For instance,\nAttanasio et al. (2023) and Kocmi et al. (2020)\nfocus on German, Spanish, and Czech, which\nhave clear grammatical gender systems, using in-\nterpretability methods and evaluation metrics like\nWinoMT to enhance gender pronoun translation\nand coreference resolution. In Arabic, Habash et al.\n(2019) emphasizes the reinflection of verbs and\nnouns, utilizing neural models tailored to the com-\nplexities of Arabic grammar, where gender agree-\nment is required across pronouns, verbs, and adjec-\ntives. Additionally, Stewart and Mihalcea (2024)\nexamines the translation of same-gender relation-\nships in French, Italian, and Spanish, addressing\nthe challenges of aligning gendered terms with so-\ncial contexts in Romance languages. Finally, Iluz\net al. (2023) investigates how the distribution of\ngendered terms in Hebrew and German training\ndata affects translation accuracy, pointing out that\ntokenization can either exacerbate or reduce bias,\ndepending on how gendered terms are processed\nduring translation.\nEfforts to tackle exposure bias in neural ma-\nchine translation (NMT)\n. This involves im-\nproving translation quality through methods, that\nenhance translation quality by simulating real-\nworld inference conditions or augmenting data.\nThe goal is to make models more robust to noise\nand difficult translations, which can indirectly in-\nfluence bias detection and mitigation. For example,\nLu et al. (2020) introduces dynamic sampling tech-\nniques, which expose models to challenging or\nerroneous inputs during training. In languages like\nArabic and German, where morphology and gram-\nmatical gender are key factors, this approach may\nhelp address the noise created by gender inflections\nor verb agreements. Similarly, Liu et al. (2023)\nuses contextual augmentation and self-distillation\nto enhance sequence prediction, which is particu-\nlarly useful in languages with flexible word order,\nsuch as Czech or Russian, where sentence struc-\nture variations can introduce noise. Although not\nspecifically targeting gender bias, Shi et al. (2023)\napplies causal models to improve translation qual-\nity evaluation, which can indirectly mitigate bias by\nreducing translation errors in languages like Span-\nish and French, where accurate gender agreement\nis critical. These techniques show how exposure\nbias mitigation can vary in its effectiveness depend-\ning on the linguistic properties of each language.\nData collection plays a critical role in bias\nmitigation, with some researchers developing\ngender-balanced corpora for training and eval-\nuation, aiming to improve datasets for detecting\nand mitigating gender bias in machine transla-\ntion.\nThe languages included in these corpora\nsignificantly affect the outcomes of bias detec-\ntion. For instance, Costa-jussà et al. (2020) fo-\ncuses on constructing a gender-balanced corpus\nfrom Wikipedia biographies, covering English,\nFrench, Spanish, and other multilingual applica-\ntions. These languages, which feature strong gen-\nder marking in nouns, pronouns, and adjectives,\nrequire careful data balancing to ensure equal rep-\nresentation of male and female forms. Similarly,\nLevy et al. (2021) emphasizes the importance of\ngender-balanced datasets in both English corefer-\nence resolution and general machine translation\ntasks. In languages like English, where gender\nis primarily expressed through pronouns, ensur-\ning a fair representation of gendered terms helps\nimprove translation fairness. Additionally, when\napplied to more gendered languages like French\nand Spanish, these datasets play a crucial role in de-\ntecting bias in how gender agreements are handled\nin translations.\nVarious evaluation benchmarks and methods\nhave also been developed to detect bias in ma-\nchine translation.\nThese benchmarks address\nspecific linguistic challenges such as word-sense\ndisambiguation (WSD), pronoun translation, and\nthe impact of automatic post-editing (APE) in am-\nplifying biases. For example, Campolungo et al.\n(2022) evaluates semantic bias in WSD across lan-\n4\nguages like English and Chinese, where word am-\nbiguity is common, highlighting how biases can\nemerge from ambiguous word choices. WSD is\nparticularly challenging in Chinese, where many\nwords lack clear gender markers, making it eas-\nier for translation systems to misinterpret gender.\nIn Japanese and Chinese, Wang et al. (2023) ex-\nplores gender biases where pronouns are frequently\ndropped, requiring translators to infer gender from\ncontext, which can often lead to biased outputs\nwhen gender-neutral terms are incorrectly assigned.\nThis issue of dropped pronouns makes gender bias\nmore likely to occur, especially in languages where\nthe context must be carefully reconstructed to re-\ntain accuracy. Additionally, Freitag et al. (2019)\ninvestigates how APE affects French, Romanian,\nand German translations, where gender agreement\nplays a significant role. APE can introduce or ex-\nacerbate biases in these languages by amplifying\nsmall errors in gender agreement, which can lead\nto skewed or biased translations. These evaluations\nreveal how linguistic features, such as pronoun us-\nage and gender agreement, play a critical role in\nbias detection and mitigation across different lan-\nguages.\nPedagogical frameworks.\nTo raise awareness\nof bias in machine translation, some researchers\npropose pedagogical frameworks aimed at equip-\nping students and researchers with the tools to un-\nderstand and address these issues across various\nlanguages. For example, (Krüger and Hackenbuch-\nner, 2022) combines data literacy with machine\ntranslation literacy, teaching students how to detect\nand mitigate bias in translations of languages such\nas French, German, and Spanish, where grammat-\nical gender and gender agreement present unique\nchallenges. This framework also helps students\nnavigate gender-neutral languages, like Finnish or\nChinese, teaching them how to handle pronoun am-\nbiguity and cultural context. Meanwhile, Kaeser-\nChen et al. (2020) emphasizes the importance of\nunderstanding how developers’ backgrounds and\nperspectives might introduce bias into machine\ntranslation systems, particularly in gendered trans-\nlations and cultural contexts.\n4\nDiscussion\n4.1\nLimitations of the State-of-the-Art\nA general limitation we encountered in our study\nwas the limited or non-available research for many\nof the African and European languages from our\nqueries. This generally shows that there is room\nfor future research efforts for bias in machine trans-\nlation detection and mitigation, taking additional\nlanguages into consideration.\nThe authors of the reviewed papers acknowl-\nedge several challenges that limit the scope and\napplicability of their findings. Common themes in-\nclude the need to expand research beyond specific\nlinguistic features, incorporating more inclusive\ngender constructs, improving scalability, refining\nevaluation metrics, and empirically testing theoret-\nical frameworks.\nA key limitation is that many studies focus on\nspecific linguistic features or bias types, which can\nhinder the generalizability of their results across\ndifferent languages or grammatical categories. For\nexample, the work by Sólmundsdóttir et al. (2022)\nfocuses exclusively on adjectives and their gen-\ndered inflections in Icelandic, potentially overlook-\ning other forms of bias related to verbs or nouns.\nSimilarly, Attanasio et al. (2023) focuses exclu-\nsively on pronoun translation and gender accuracy,\nwhich overlooks broader aspects of gender bias\nthat might emerge in other word categories like\ngendered nouns. Other papers, like Stewart and Mi-\nhalcea (2024), focus specifically on same-gender\nrelationships, limiting their conclusions to rela-\ntional contexts and potentially missing biases in\nnon-relational settings. Papers such as Habash et al.\n(2019) and Habash et al. (2019) also focus on spe-\ncific grammatical phenomena (e.g., verb reinflec-\ntion), which might not generalize beyond the Ara-\nbic language or to more complex linguistic struc-\ntures.\nMoreover, many papers focus on binary gender\ncategories, which excludes non-binary and gender-\nfluid identities. For example, Stafanoviˇcs et al.\n(2020) relies on binary gender annotations that can-\nnot account for non-binary or more complex gen-\nder constructs. Similarly, Iluz et al. (2023), Costa-\njussà and de Jorge (2020) and Saunders and Byrne\n(2020) focus primarily on binary gender forms, lim-\niting their relevance in scenarios where gender is\nmore fluid. These binary-focused approaches also\nfail to address how gender bias manifests in lan-\nguages with non-binary pronouns or where gender\nis not rigidly categorized, creating an exclusionary\ngap in research regarding gender diversity.\nAdditionally, several theoretical frameworks for\naddressing bias lack empirical validation, which\nlimits their practical applicability. For example,\nKaeser-Chen et al. (2020) offers a framework to ac-\n5\ncount for developers’ positionality (i.e., how their\nbackground influences the machine learning sys-\ntems they design) but does not empirically test its\neffectiveness in real-world MT systems. Similarly,\nKrüger and Hackenbuchner (2022) and Krüger and\nHackenbuchner (2022) introduce an educational\nmodel for addressing gender bias in MT but do not\npresent empirical results on the effectiveness of\nthis teaching approach.\nAnother recurring challenge is the reliance on\nsmall, manually annotated datasets, which restricts\nthe scalability of the findings. Stafanoviˇcs et al.\n(2020) relies heavily on manually curated gender\nannotations, which becomes impractical for larger-\nscale datasets or for more complex contexts where\ngender forms may not be binary. Similarly, Levy\net al. (2021) offers a detailed dataset but does not\nexplore how well it scales or generalizes across\nlanguages and various social.\n4.2\nOutlook for Future Work\nFuture research should aim to generalize findings\nby investigating how gender bias manifests in\nbroader linguistic structures, such as nouns, verbs,\nand more complex syntactic forms. Papers like\nSólmundsdóttir et al. (2022) and Attanasio et al.\n(2023) could be extended by future research bring-\ning these analyses beyond adjectives and pronouns,\nrespectively. Similarly, studies that focus on spe-\ncific relational dynamics, like Stewart and Mihal-\ncea (2024), or occupational stereotypes, such as\nTriboulet and Bouillon (2023), can be used to ex-\nplore how these biases manifest in other linguistic\nand social contexts to gain a more comprehensive\nunderstanding of MT systems’ behavior.\nAnother key area for future research is the in-\nclusion of non-binary and gender-fluid identities.\nStudies, such as Stafanoviˇcs et al. (2020) and Saun-\nders and Byrne (2020), that primarily focus on\nbinary gender distinctions, can be expanded to ac-\ncommodate non-binary and fluid gender forms that\nwould ensure that bias detection and mitigation\nmethods are more representative of diverse gen-\nder identities. Studies on gender coreference and\nbias resolution, such as Kocmi et al. (2020), can\nbe adapted to incorporate more inclusive gender\ncategories, ensuring that MT systems are equipped\nto handle a wider range of gender expressions.\nExpanding the scope of research beyond specific\nlanguages or language pairs is another crucial area\nof development. Future research should aim to\ngeneralize findings of Prates et al. (2019) or Costa-\njussà et al. (2020) by testing these techniques\nacross more diverse linguistic contexts. This would\nnot only broaden the applicability of bias detection\ntools but also help ensure that solutions can scale\neffectively across different language pairs, espe-\ncially in low-resource and non-Indo-European lan-\nguages. Similarly, results from language-specific\nstudies like Habash et al. (2019) and Elaraby et al.\n(2018) can be used to extend the focus to other lan-\nguages, allowing researchers to evaluate whether\nthe same gender bias patterns are found across var-\nious linguistic systems.\nAnother priority for future research is improving\ndataset scalability and addressing the limitations\nof manual annotations. Papers like Stafanoviˇcs\net al. (2020) and Levy et al. (2021) would inspire\nways to automate the annotation process and build\nlarger, more diverse datasets that better reflect real-\nworld usage. Future research should could expand\ngender-balanced datasets like Costa-jussà et al.\n(2020) to cover more diverse text genres, such as\nnews articles, dialogue, and social media, to further\nimprove the robustness of MT systems in handling\ngender bias.\nImproving evaluation metrics and expanding\nbias analysis beyond specific translation stages\nis another important area for future work. The\nmethodologies employed in the studies, such as\nCampolungo et al. (2022) and Freitag et al. (2019),\ncould inspire future research to focus on develop-\ning more comprehensive evaluation frameworks\nthat capture gender bias at both the lexical and sen-\ntence levels, as well as biases introduced during\ndifferent stages of the translation process, from\ninput to post-editing. Extending causal models\nused in papers like Shi et al. (2023) to address\nbias directly could also lead to more evaluations of\ntranslation quality in relation to gender.\nFuture work should be expanded to address other\ntypes of bias, such as racial or cultural biases. For\ninstance, research could build on the foundational\nwork of studies like Escudé Font and Costa-jussà\n(2019) and Moryossef et al. (2019) by exploring\nhow similar debiasing techniques can be applied\nacross various biases and linguistic contexts. Ex-\npanding these techniques would contribute to more\ncomprehensive and effective bias mitigation strate-\ngies in machine translation systems.\nTesting of theoretical studies in real-world sys-\ntems such as Kaeser-Chen et al. (2020) that offers\na conceptual framework for addressing how devel-\nopers’ positionality influences MT systems, would\n6\nbe beneficial. Similarly, educational frameworks\nlike Krüger and Hackenbuchner (2022) should be\nevaluated in classroom settings to determine their\nimpact on students’ ability to detect and mitigate\nbias in machine translation. Empirical validation of\nthese theoretical frameworks would provide valu-\nable insights into their practical applications and\nhelp shape more effective bias mitigation strate-\ngies.\n5\nLimitations\nIn our approach, we concentrated on African and\nEuropean written languages, by using a fixed list.\nEven though we used a two-step approach, we can-\nnot be sure that all relevant languages have been in-\ncluded, as the lists were generated and not derived\nfrom a trusted source. For example, Afrikaans\nwas considered as a European language, due to\nits linguistic origin. However, it could be argued\nto be added to the list of African languages as\nthis is where it is primarily spoken. At the same\ntime, while focusing on written languages, we\nare missing out dialects and regional variations\nof languages. With our focus on European and\nAfrican languages, we are missing out languages\nfrom other regions, and encourage similar work to\nbe applied for those.\nReferences\nGiuseppe Attanasio, Flor Miriam Plaza del Arco, Deb-\nora Nozza, and Anne Lauscher. 2023. A tale of\npronouns: Interpretability informs gender bias mit-\nigation for fairer instruction-tuned machine trans-\nlation. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3996–4014. Association for Computational\nLinguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances in\nneural information processing systems, 29.\nLena Cabrera and Jan Niehues. 2023. Gender lost in\ntranslation: How bridging the gap between languages\naffects gender bias in zero-shot multilingual trans-\nlation.\nIn Proceedings of the First Workshop on\nGender-Inclusive Translation Technologies, pages\n25–35. European Association for Machine Transla-\ntion.\nAylin Caliskan,\nJoanna J Bryson,\nand Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nNiccolò Campolungo, Federico Martelli, Francesco\nSaina, and Roberto Navigli. 2022. DiBiMT: A novel\nbenchmark for measuring word sense disambigua-\ntion biases in machine translation. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4331–4352. Association for Computational\nLinguistics.\nWon Ik Cho, Jiwon Kim, Jaeyeong Yang, and Nam Soo\nKim. 2021. Towards cross-lingual generalization of\ntranslation gender bias. In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’21, pages 449–457. Associa-\ntion for Computing Machinery.\nMarta R. Costa-jussà and Adrià de Jorge. 2020.\nFine-tuning neural machine translation on gender-\nbalanced datasets. In Proceedings of the Second\nWorkshop on Gender Bias in Natural Language Pro-\ncessing, pages 26–34. Association for Computational\nLinguistics.\nMarta R. Costa-jussà, Pau Li Lin, and Cristina España-\nBonet. 2020. GeBioToolkit: Automatic extraction\nof gender-balanced multilingual corpus of wikipedia\nbiographies. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, pages 4081–\n4088. European Language Resources Association.\nMostafa Elaraby, Ahmed Y. Tawfik, Mahmoud Khaled,\nHany Hassan, and Aly Osama. 2018. Gender aware\nspoken language translation applied to english-arabic.\nPreprint, arxiv:1802.09287 [cs].\nJoel Escudé Font and Marta R. Costa-jussà. 2019.\nEqualizing gender bias in neural machine translation\nwith word embeddings techniques. In Proceedings\nof the First Workshop on Gender Bias in Natural\nLanguage Processing, pages 147–154. Association\nfor Computational Linguistics.\nMarkus Freitag, Isaac Caswell, and Scott Roy. 2019.\nAPE at scale and its implications on MT evaluation\nbiases. In Proceedings of the Fourth Conference\non Machine Translation (Volume 1: Research Pa-\npers), pages 34–44. Association for Computational\nLinguistics.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow,\nMd Mehrab Tanjim, Sungchul Kim, Franck Dernon-\ncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed.\n2024. Bias and fairness in large language models: A\nsurvey. Computational Linguistics, pages 1–79.\nNizar Habash, Houda Bouamor, and Christine Chung.\n2019. Automatic gender identification and reinflec-\ntion in arabic. In Proceedings of the First Workshop\non Gender Bias in Natural Language Processing,\npages 155–165. Association for Computational Lin-\nguistics.\nDirk Hovy and Shrimai Prabhumoye. 2021.\nFive\nsources of bias in natural language processing. Lan-\nguage and linguistics compass, 15(8):e12432.\n7\nBar Iluz, Tomasz Limisiewicz, Gabriel Stanovsky, and\nDavid Mareˇcek. 2023.\nExploring the impact of\ntraining data distribution and subword tokenization\non gender bias in machine translation.\nPreprint,\narxiv:2309.12491.\nYatu Ji, Nier Wu, and Hongxu Hou. 2020. A simple\nand general strategy for referential problem in low-\nresource neural machine translation.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the nlp\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293.\nChristine Kaeser-Chen, Elizabeth Dubois, Friederike\nSchüür, and Emanuel Moss. 2020.\nPositionality-\naware machine learning: translation tutorial. pages\n704–704. Conference Name: FAT* ’20: Conference\non Fairness, Accountability, and Transparency ISBN:\n9781450369367 Place: Barcelona Spain Publisher:\nACM.\nTom Kocmi,\nTomasz Limisiewicz,\nand Gabriel\nStanovsky. 2020. Gender coreference and bias eval-\nuation at WMT 2020. In Proceedings of the Fifth\nConference on Machine Translation, pages 357–364.\nAssociation for Computational Linguistics.\nRalph Krüger and Janiça Hackenbuchner. 2022. Outline\nof a didactic framework for combined data literacy\nand machine translation literacy teaching. pages 375–\n432.\nMinwoo Lee, Hyukhun Koh, Kang-il Lee, Dongdong\nZhang, Minsung Kim, and Kyomin Jung. 2023.\nTarget-agnostic gender-aware contrastive learning\nfor mitigating bias in multilingual machine trans-\nlation. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 16825–16839. Association for Computational\nLinguistics.\nShahar Levy, Koren Lazar, and Gabriel Stanovsky. 2021.\nCollecting a large-scale gender bias dataset for coref-\nerence resolution and machine translation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 2470–2480. Association\nfor Computational Linguistics.\nZhidong Liu, Junhui Li, and Muhua Zhu. 2023. Al-\nleviating exposure bias for neural machine transla-\ntion via contextual augmentation and self distilla-\ntion. 31:2079–2089. Conference Name: IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing.\nWenjie Lu, Jie Zhou, Leiying Zhou, Gongshen Liu,\nand Quanhai Zhang. 2020. Challenge training to\nsimulate inference in machine translation. In 2020\nInternational Joint Conference on Neural Networks\n(IJCNN), pages 1–8. ISSN: 2161-4407.\nAudrey Mash, Carlos Escolano, Aleix Sant, Maite\nMelero, and Francesca de Luca Fornaciari. 2024.\nUnmasking biases: Exploring gender bias in english-\ncatalan machine translation through tokenization\nanalysis and novel dataset. In Proceedings of the\n2024 Joint International Conference on Computa-\ntional Linguistics, Language Resources and Evalu-\nation (LREC-COLING 2024), pages 17144–17153.\nELRA and ICCL.\nAmit Moryossef, Roee Aharoni, and Yoav Goldberg.\n2019. Filling gender & number gaps in neural ma-\nchine translation with black-box context injection.\nIn Proceedings of the First Workshop on Gender\nBias in Natural Language Processing, pages 49–54.\nAssociation for Computational Linguistics.\nMarcelo O. R. Prates, Pedro H. C. Avelar, and Luis\nLamb. 2019. Assessing gender bias in machine trans-\nlation – a case study with google translate. Preprint,\narxiv:1809.02208 [cs].\nMarcelo OR Prates, Pedro H Avelar, and Luís C Lamb.\n2020. Assessing gender bias in machine translation:\na case study with google translate. Neural Comput-\ning and Applications, 32:6363–6381.\nHanna Pułaczewska. 2024. Cross-gender hijacking of\nhigh achievers by translation students: English to\npolish. pages 1–18.\nDanielle Saunders and Bill Byrne. 2020. Reducing gen-\nder bias in neural machine translation as a domain\nadaptation problem. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7724–7736. Association for Com-\nputational Linguistics.\nBeatrice Savoldi, Marco Gaido, Luisa Bentivogli, Mat-\nteo Negri, and Marco Turchi. 2021a. Gender bias in\nmachine translation. Transactions of the Association\nfor Computational Linguistics, 9:845–874.\nBeatrice Savoldi, Marco Gaido, Luisa Bentivogli, Mat-\nteo Negri, and Marco Turchi. 2021b. Gender bias in\nmachine translation. 9:845–874. Place: Cambridge,\nMA Publisher: MIT Press.\nXuewen Shi, Heyan Huang, Ping Jian, and Yi-Kun Tang.\n2023. Approximating to the real translation quality\nfor neural machine translation via causal motivated\nmethods. 22(5):126:1–126:26.\nArt¯urs Stafanoviˇcs, Toms Bergmanis, and M¯arcis Pinnis.\n2020. Mitigating gender bias in machine translation\nwith target gender annotations. In Proceedings of\nthe Fifth Conference on Machine Translation, pages\n629–638. Association for Computational Linguistics.\nIan Stewart and Rada Mihalcea. 2024. Whose wife\nis it anyway? assessing bias against same-gender\nrelationships in machine translation.\nPreprint,\narxiv:2401.04972 [cs].\n8\nAgnes Sólmundsdóttir,\nDagbjört Guðmundsdóttir,\nLilja Björk Stefánsdóttir, and Anton Ingason. 2022.\nMean machine translations: On gender bias in ice-\nlandic machine translations. In Proceedings of the\nThirteenth Language Resources and Evaluation Con-\nference, pages 3113–3121. European Language Re-\nsources Association.\nBertille Triboulet and Pierrette Bouillon. 2023. Evalu-\nating the impact of stereotypes and language combi-\nnations on gender bias occurrence in NMT generic\nsystems. In Proceedings of the Third Workshop on\nLanguage Technology for Equality, Diversity and\nInclusion, pages 62–70. INCOMA Ltd., Shoumen,\nBulgaria.\nEva Vanmassenhove, Dimitar Shterionov, and Matthew\nGwilliam. 2021. Machine translationese: Effects of\nalgorithmic bias on linguistic complexity in machine\ntranslation. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 2203–2213.\nAssociation for Computational Linguistics.\nAniket Vashishtha, Kabir Ahuja, and Sunayana Sitaram.\n2023. On evaluating and mitigating gender biases in\nmultilingual settings. In Findings of the Association\nfor Computational Linguistics: ACL 2023, pages\n307–318. Association for Computational Linguistics.\nLongyue Wang, Siyou Liu, Mingzhou Xu, Linfeng\nSong, Shuming Shi, and Zhaopeng Tu. 2023. A\nsurvey on zero pronoun translation. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 3325–3339. Association for Computational\nLinguistics.\nKyra Wilson and Aylin Caliskan. 2024. Gender, race,\nand intersectional bias in resume screening via\nlanguage model retrieval.\nIn Proceedings of the\nAAAI/ACM Conference on AI, Ethics, and Society,\nvolume 7, pages 1578–1590.\nGuillaume Wisniewski, Lichao Zhu, Nicolas Ballier,\nand François Yvon. 2022.\nBiais de genre dans\nun système de traduction automatique neuronale :\nune étude des mécanismes de transfert cross-langue\n[gender bias in a neural machine translation system:\na study of crosslingual transfer mechanisms]. In\nTraitement Automatique des Langues, Volume 63,\nNuméro 1 : Varia [Varia], pages 37–61. ATALA\n(Association pour le Traitement Automatique des\nLangues).\nA\nQueries and Lists of Languages\nVersion: ChatGPT4o\nInitial Query: List of all written African lan-\nguages?\nFollow-up Query: Same list in format of python\nlist, only language names.\nFollow-up Query: Same for European.\nInitial list of languages (African):\nArabic,\nAmharic, Tigrinya, Hebrew, Tamazight, Kabyle,\nShilha, Hausa, Somali, Oromo, Swahili, Zulu,\nXhosa, Shona, Kinyarwanda, Kirundi, Lingala,\nSetswana, Sesotho, Chichewa, Kikuyu, Bemba,\nLuganda, Bambara, Mandinka, Dyula, Soninke,\nAkan, Twi, Fante, Ewe, Yoruba, Igbo, Nubian,\nDinka, Nuer, Luo, Kanuri, Nama, Khoekhoe,\nAfrikaans, Cape Verdean Creole, Seychellois Cre-\nole, Mauritian Creole, Papiamento, Old Nubian,\nAncient Egyptian, Coptic\nInitial list of languages (European): German,\nDutch, Swedish, Danish, Norwegian, Icelandic,\nAfrikaans, Frisian, Spanish, French, Portuguese,\nItalian, Romanian, Catalan, Galician, Occitan, Rus-\nsian, Polish, Czech, Slovak, Ukrainian, Bulgarian,\nSerbian, Croatian, Bosnian, Slovenian, Belarusian,\nMacedonian, Montenegrin, Irish, Scottish Gaelic,\nWelsh, Breton, Cornish, Manx, Greek, Latvian,\nLithuanian, Finnish, Estonian, Hungarian, Alba-\nnian, Armenian, Basque\nValidation Query (new session): In the two lists\nabove, are there any African or European lan-\nguages missing?\nLanguages added after second query in a new\nsession:\nAfrican: Tshiluba, Sango, Malagasy, Wolof, Fon\nEuropean: Maltese, Luxembourgish, Sardinian,\nFaroese, Sorbian, Kashubian, Romansh, Asturian,\nAromanian, Aragonese\nThese combined lists were used to make the se-\nlection of the papers collected with Web of Science,\nwith a script.\nThis was also the basis for the queries on the\nACL Anthology, as shown in the following figures:\n9\nACL Query African Languages\nmachine learning AND bias AND (\"Ara-\nbic\" OR \"Amharic\" OR \"Tigrinya\" OR\n\"Hebrew\" OR \"Tamazight\" OR \"Kabyle\"\nOR \"Shilha\" OR \"Hausa\" OR \"Somali\"\nOR \"Oromo\" OR \"Swahili\" OR \"Zulu\"\nOR \"Xhosa\" OR \"Shona\" OR \"Kin-\nyarwanda\" OR \"Kirundi\" OR \"Lingala\" OR\n\"Setswana\" OR \"Sesotho\" OR \"Chichewa\"\nOR \"Kikuyu\" OR \"Bemba\" OR \"Lu-\nganda\" OR \"Bambara\" OR \"Mandinka\" OR\n\"Dyula\" OR \"Soninke\" OR \"Akan\" OR\n\"Twi\" OR \"Fante\" OR \"Ewe\" OR \"Yoruba\"\nOR \"Igbo\" OR \"Nubian\" OR \"Dinka\" OR\n\"Nuer\" OR \"Luo\" OR \"Kanuri\" OR \"Nama\"\nOR \"Khoekhoe\" OR \"Afrikaans\" OR \"Cape\nVerdean Creole\" OR \"Seychellois Creole\"\nOR \"Mauritian Creole\" OR \"Papiamento\"\nOR \"Old Nubian\" OR \"Ancient Egyptian\"\nOR \"Coptic\" OR “Tshiluba” OR “Sango”\nOR “Malagasy” OR “Wolof” OR “Fon”)\nFigure 1: ACL Query African Languages.\nACL Query European Languages:\nmachine learning AND bias AND (\"Ger-\nman\" OR \"Dutch\" OR \"Swedish\" OR \"Dan-\nish\" OR \"Norwegian\" OR \"Icelandic\" OR\n\"Afrikaans\" OR \"Frisian\" OR \"Spanish\"\nOR \"French\" OR \"Portuguese\" OR \"Ital-\nian\" OR \"Romanian\" OR \"Catalan\" OR\n\"Galician\" OR \"Occitan\" OR \"Russian\"\nOR \"Polish\" OR \"Czech\" OR \"Slovak\" OR\n\"Ukrainian\" OR \"Bulgarian\" OR \"Serbian\"\nOR \"Croatian\" OR \"Bosnian\" OR \"Slove-\nnian\" OR \"Belarusian\" OR \"Macedonian\"\nOR \"Montenegrin\" OR \"Irish\" OR \"Scottish\nGaelic\" OR \"Welsh\" OR \"Breton\" OR \"Cor-\nnish\" OR \"Manx\" OR \"Greek\" OR \"Lat-\nvian\" OR \"Lithuanian\" OR \"Finnish\" OR\n\"Estonian\" OR \"Hungarian\" OR \"Albanian\"\nOR \"Armenian\" OR \"Basque\" OR “Mal-\ntese” OR “Luxembourgish” OR “Sardinian”\nOR “Faroese” OR “Sorbian” OR “Kashu-\nbian” OR “Romansh” OR “Asturian” OR\n“Aromanian” OR “Aragonese”)\nFigure 2: ACL Query European Languages.\n10\nB\nInclusion and Exclusion Criteria\nThe exclusion criteria eliminate papers that are outside the core focus of gender bias in MT. Studies\nthat focus on other areas of NLP, such as speech recognition, grammatical error correction, or sentiment\nanalysis, without addressing MT-specific bias, are excluded. Health-related applications and technical\ninnovations in MT that do not focus on bias are also excluded.\nAuthor(s)\nPaper Title\nReason for Exclusion\nBircan,\nTuba;\nCeylan,\nDuha\nMachine Discriminating: Automated Speech Recog-\nnition Biases in Refugee Interviews\nFocus on ASR systems or\nspeech-based applications\nPaulik, M. et al.\nSpeech recognition in human mediated translation\nscenarios\nCasanova, Edresson et al.\nDeep Learning against COVID-19: Respiratory In-\nsufficiency Detection in Brazilian Portuguese Speech\nSolyman, Aiman et al.\nAutomatic Arabic Grammatical Error Correction\nbased on Expectation-Maximization routing and\ntarget-bidirectional agreement\nFocus on grammar\ncorrection systems\nMahmoud, Zeinab et al.\nSemi-supervised learning and bidirectional decod-\ning for effective grammar correction in low-resource\nscenarios\nHerrera-Espejel,\nPaula\nSofia et al.\nThe Use of Machine Translation for Outreach and\nHealth Communication in Epidemiology and Public\nHealth: Scoping Review\nFocus on public health\nand health-related\napplications\nPetrova-Antonova,\nDessislava et al.\nCogniSoft: A Platform for the Automation of Cogni-\ntive Assessment and Rehabilitation of Multiple Scle-\nrosis\nWambsganss, Thiemo et\nal.\nBias at a Second Glance: A Deep Dive into Bias for\nGerman Educational Peer-Review Data Modeling\nFocus on educational or\nclinical biases\nPedersen, Jannik et al.\nInvestigating anatomical bias in clinical machine\nlearning algorithms\nRynkiewicz, Agnieszka et\nal.\nAn investigation of the ’female camouflage effect’ in\nautism using a computerized ADOS-2 and a test of\nsex/gender differences\nCâmara, António et al.\nMapping the Multilingual Margins: Intersectional Bi-\nases of Sentiment Analysis Systems in English, Span-\nish, and Arabic\nFocus on bias in\nsentiment analysis or NLP\ntasks beyond machine\ntranslation\nChávez Mulsa, Rodrigo\nAlejandro et al.\nEvaluating Bias In Dutch Word Embeddings\nAdewumi, Tosin et al.\nBipol: Multi-Axes Evaluation of Bias with Explain-\nability in Benchmark Datasets\nLe, Dieu-thu et al.\nReducing cohort bias in natural language under-\nstanding systems with targeted self-training scheme\nZhou, Lei et al.\nInference Discrepancy Based Curriculum Learning\nfor Neural Machine Translation\nFocus on specific machine\ntranslation advancements\nor innovations\nSeedat, Nabeel et al.\nPEMS: Custom Neural Machine Translation System\nfor Portuguese TV subtitling\nYu, Heng et al.\nA2R2: Robust Unsupervised Neural Machine Trans-\nlation With Adversarial Attack and Regularization\nYan, Jianhao et al.\nMulti-Unit Transformers for Neural Machine Trans-\nlation\nZhang, Tianfu et al.\nEnlivening Redundant Heads in Multi-head Self-\nattention for Machine Translation\n11\nLiu, Yijin et al.\nConfidence-Aware Scheduled Sampling for Neural\nMachine Translation\nStahlberg, Felix et al.\nOn NMT Search Errors and Model Errors: Cat Got\nYour Tongue?\nAmrhein, Chantal et al.\nExploiting Biased Models to De-bias Text: A Gender-\nFair Rewriting Model\nFocus on debiasing or\nbias mitigation in NLP\nGoodman, Sebastian et al.\nTeaForN: Teacher-Forcing with N-grams\nHelal, Mohammed et al.\nThe CyberEquity Lab at FIGNEWS 2024 Shared\nTask: Annotating a Corpus of Facebook Posts to\nLabel Bias and Propaganda in Gaza-Israel War Cov-\nerage in Five Languages\nThe inclusion criteria for the manual selection process focus on papers that include at least one language\nfrom the query and specifically address gender bias in machine translation (MT). This includes studies that\nexplore bias detection and mitigation techniques in MT systems, particularly in multilingual settings or\nlanguages with grammatical gender. Papers proposing novel approaches to reducing bias, such as gender-\naware learning or the use of large-scale datasets for bias evaluation, are prioritized. We incorporated a\ntheoretical paper that addresses the issue of positionality in machine translation despite it not covering\nany of the languages in our query.\nAuthor(s)\nPaper Title\nReason for Inclusion\nSólmundsdóttir\net\nal.\n(2022)\nMean Machine Translations: On Gender Bias in\nIcelandic Machine Translations\nGeneral Approaches &\nEvaluations of Gender\nBias in MT\nSavoldi et al. (2021b)\nGender Bias in Machine Translation\nCosta-jussà and de Jorge\n(2020)\nFine-tuning Neural Machine Translation on Gender-\nBalanced Datasets\nVashishtha et al. (2023)\nOn Evaluating and Mitigating Gender Biases in Mul-\ntilingual Settings\nLee et al. (2023)\nTarget-Agnostic Gender-Aware Contrastive Learning\nfor Mitigating Bias in Multilingual Machine Transla-\ntion\nVanmassenhove\net\nal.\n(2021)\nMachine Translationese: Effects of Algorithmic Bias\non Linguistic Complexity in Machine Translation\nBias Amplification &\nLinguistic Impact in MT\nKocmi et al. (2020)\nGender Coreference and Bias Evaluation at WMT\n2020\nCabrera\nand\nNiehues\n(2023)\nGender Lost In Translation: How Bridging The Gap\nBetween Languages Affects Gender Bias in Zero-Shot\nMultilingual Translation\nStewart\nand\nMihalcea\n(2024)\nWhose wife is it anyway? Assessing bias against\nsame-gender relationships in machine translation\nBias Against\nSame-Gender\nRelationships\nAttanasio et al. (2023)\nA Tale of Pronouns: Interpretability Informs Gen-\nder Bias Mitigation for Fairer Instruction-Tuned Ma-\nchine Translation\nMash et al. (2024)\nUnmasking Biases:\nExploring Gender Bias in\nEnglish-Catalan Machine Translation through To-\nkenization Analysis and Novel Dataset\nTokenization &\nSubword Units Impact\non Gender Bias\nIluz et al. (2023)\nExploring the Impact of Training Data Distribution\nand Subword Tokenization on Gender Bias in Ma-\nchine Translation\nLevy et al. (2021)\nCollecting a Large-Scale Gender Bias Dataset for\nCoreference Resolution and Machine Translation\nLarge-Scale Datasets for\nEvaluating Gender Bias\nin MT\n12\nHabash et al. (2019)\nAutomatic Gender Identification and Reinflection in\nArabic\n13\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-10-28",
  "updated": "2024-10-28"
}