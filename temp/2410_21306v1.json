{
  "id": "http://arxiv.org/abs/2410.21306v1",
  "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges",
  "authors": [
    "Farid Ariai",
    "Gianluca Demartini"
  ],
  "abstract": "Natural Language Processing is revolutionizing the way legal professionals\nand laypersons operate in the legal field. The considerable potential for\nNatural Language Processing in the legal sector, especially in developing\ncomputational tools for various legal processes, has captured the interest of\nresearchers for years. This survey follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses framework, reviewing 148 studies, with a\nfinal selection of 127 after manual filtering. It explores foundational\nconcepts related to Natural Language Processing in the legal domain,\nillustrating the unique aspects and challenges of processing legal texts, such\nas extensive document length, complex language, and limited open legal\ndatasets. We provide an overview of Natural Language Processing tasks specific\nto legal text, such as Legal Document Summarization, legal Named Entity\nRecognition, Legal Question Answering, Legal Text Classification, and Legal\nJudgment Prediction. In the section on legal Language Models, we analyze both\ndeveloped Language Models and approaches for adapting general Language Models\nto the legal domain. Additionally, we identify 15 Open Research Challenges,\nincluding bias in Artificial Intelligence applications, the need for more\nrobust and interpretable models, and improving explainability to handle the\ncomplexities of legal language and reasoning.",
  "text": "Natural Language Processing for the Legal Domain: A Survey of Tasks,\nDatasets, Models, and Challenges\nFARID ARIAI and GIANLUCA DEMARTINI, The University of Queensland, Australia\nNatural Language Processing is revolutionizing the way legal professionals and laypersons operate in the legal field. The\nconsiderable potential for Natural Language Processing in the legal sector, especially in developing computational tools\nfor various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting\nItems for Systematic Reviews and Meta-Analyses framework [89], reviewing 148 studies, with a final selection of 127 after\nmanual filtering. It explores foundational concepts related to Natural Language Processing in the legal domain, illustrating\nthe unique aspects and challenges of processing legal texts, such as extensive document length, complex language, and\nlimited open legal datasets. We provide an overview of Natural Language Processing tasks specific to legal text, such as Legal\nDocument Summarization, legal Named Entity Recognition, Legal Question Answering, Legal Text Classification, and Legal\nJudgment Prediction. In the section on legal Language Models, we analyze both developed Language Models and approaches\nfor adapting general Language Models to the legal domain. Additionally, we identify 15 Open Research Challenges, including\nbias in Artificial Intelligence applications, the need for more robust and interpretable models, and improving explainability to\nhandle the complexities of legal language and reasoning.\nCCS Concepts: • General and reference →Surveys and overviews; • Applied computing →Law; • Computing\nmethodologies →Natural language processing; Natural language generation; Knowledge representation and reasoning;\nSupervised learning; Unsupervised learning; Reinforcement learning; Multi-task learning; Machine learning approaches;\nArtificial intelligence; • Information systems →Language models; Retrieval tasks and goals; Question answering;\nClustering and classification; Summarization.\nACM Reference Format:\nFarid Ariai and Gianluca Demartini. 2024. Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets,\nModels, and Challenges. ACM Comput. Surv. 1, 1 (October 2024), 35 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nIn recent years, advancements in Natural Language Processing (NLP) have significantly impacted the legal domain\nby simplifying complex tasks such as Legal Document Summarization (LDS), enhancing legal text comprehension\nfor laypersons, and improving Legal Question Answering (LQA) and Legal Judgment Prediction (LJP) [24, 42, 50,\n52, 63, 93, 98]. These improvements are primarily attributed to advancements in neural network architectures,\nsuch as transformer models [118]. NLP techniques now enable machines to generate text, answer legal questions,\ndrafting a regulation, and simulate legal reasoning, which can revolutionize legal practices [50]. Applications like\ncontract review [45, 76, 77, 117] and case prediction [85, 120] have been automated to a large extent, speeding\nup processes, reducing human error, and cutting operational costs. Additionally, the use of NLP allows lawyers\nand legal professionals to reduce their workload, enhance efficiency, and minimize errors in decision-making\nprocesses [98]. Despite the rapid development of NLP, challenges remain in processing lengthy documents,\nAuthors’ Contact Information: Farid Ariai, f.ariai@uq.edu.au; Gianluca Demartini, g.demartini@uq.edu.au, The University of Queensland,\nBrisbane, Queensland, Australia.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\nCopyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from\npermissions@acm.org.\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 1557-7341/2024/10-ART\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\narXiv:2410.21306v1  [cs.CL]  25 Oct 2024\n2\n•\nAriai and Demartini\nunderstanding complex language, and navigating complicated document structures [12, 39, 52, 84, 107, 120, 132],\nyet the promise of Large Language Models (LLMs) enhances the efficiency, accessibility, and precision of legal\nservices.\nDespite these advantages, the integration of NLP in the legal domain is not without challenges such as, biases\nand unfairness, and explainability issues. [28, 103, 113]. The use of Artificial Intelligence (AI) in legal applications\nmust follow strict standards of accuracy, fairness, and transparency, given the potential impact on clients’ lives\nand rights.\nThis survey article explores the current landscape of NLP applications within the legal domain. It discusses its\npotential benefits and the practical challenges it poses. NLP is a broad field covering a wide range of techniques\nfor processing, analyzing, and understanding human language. By examining the latest advancements and\napplications of NLP in law, this article provides a comprehensive overview of the field. Figure 1 summarizes the\nscope of the survey and categorise the research into several areas: LQA, LJP, Legal Text Classification (LTC),\nLDS, legal Named Entity Recognition (NER), legal corpora, and legal Language Models (LMs). Each category lists\nrelevant projects and papers and shows the work being done in each sub-field. Notably, there is comparatively\nless research in NER and legal corpora, whereas LDS and LQA have seen extensive research activity, with a\nsubstantial number of datasets and research contributions. This summary provides an overview of how NLP\ntechniques are applied to various challenges in the legal domain and offers insights into future directions of AI in\nlegal practice.\nTo provide a comprehensive understanding of the existing research on integrating AI within the legal domain,\nwe present an overview of recent literature reviews, as summarized in Table 1. Most survey papers on intelligent\nlegal systems focus either on traditional NLP technologies for specific tasks like LJP and LDS or take a broader\napproach but still miss certain aspects. As illustrated in Table 1, there seems to be a gap in survey papers that\nthoroughly examine all facets of this multidisciplinary field. Our current work aims to bridge this gap by offering\na comprehensive survey of all NLP tasks, existing datasets and corpora, and LMs in the legal domain. We use ✓\nto indicate papers that study the most of the existing research on each subject in legal NLP. Papers that do not\naddress a subject receive ✗, and those that partially cover specific subjects along with their datasets or LMs are\nmarked with –.\nThe main difference between this work and previous surveys is that this survey aims to provide a more general\ndescription of all aspects of NLP tasks in the legal domain, rather than focusing solely on specific applications.\nThe main contributions of this survey are summarized as follows: (1) This article extends beyond previous\nsurveys by examining a broad spectrum of studies, and applications of legal NLP. By discussing datasets and\nlarge corpora in 24 languages and exploring the popular legal LMs, this survey establishes itself as an important\nresource in the field of legal NLP; (2) The survey offers an in-depth look at the challenges of integrating NLP with\nlegal applications, with detailed discussions on technical solutions that tackle these issues, thereby enhancing\nunderstanding and encouraging further research in this evolving field. (3) This survey also highlights the existing\nresearch gaps in legal NLP, identifying areas that require further exploration and development, and providing a\nroad-map for future research efforts in the legal NLP domain.\nThis document is organized as follows. In Section 2, Background and Foundational Concepts, we provide a\ndetailed overview of legal language and the basic principles of NLP as they apply to the legal domain. In Section\n3, we briefly explain the research methodology of this work and how we extracted the resources. In section 4, we\nexplore various NLP tasks that are tailored for legal applications and show their unique requirements and the\nmethodologies employed to address them. These specialized tasks leverage advanced NLP techniques to process,\nanalyze, and extract meaningful information from legal texts, thereby facilitating more efficient and accurate legal\nresearch and decision-making. Additionally, we delve into the datasets available for training and evaluating legal\nNLP tasks, emphasizing their characteristics and the implications they have for model performance. Following this,\nin Section 5, we explore the development of LMs that has been specifically adapted for the legal field. Finally, in\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n3\nTable 1. Comparison of existing surveys with this work, which shows the covered topics of each survey.\nReferences\nCovered subjects\nPublished\nyear\nDescription\nLegal\nDataset\nNLP Tasks\nin Legal\nLegal LM\nLarge Legal\nCorpus\nDias\net\nal.\n[30]\n–\n–\n–\n✗\n2022\nIn this work, researchers did not only concentrate on the legal domain. Instead, they elaborated\non foundational concepts of AI and NLP. They briefly explored the applications of NLP within\nthe legal field but did not delve deeply into legal datasets, specific NLP tasks in the legal domain,\nor Legal LLMs. In contrast, our work provides a comprehensive analysis of all NLP tasks within\nthe legal domain, including LQA, LDS, LTC. Additionally, our study covers large legal corpora\nand thoroughly examines the datasets available for each legal NLP task, areas that were not\nfully addressed in this work.\nSun [112]\n✓\n–\n✓\n✗\n2023\nSun explored a limited number of research projects in the field of LLMs and the legal domain.\nIt focused on two key NLP tasks within this field: LJP and statutory reasoning. Additionally, it\nexamined three datasets and two LLMs relevant to these domain. The difference is that our\nwork provides a comprehensive overview of all NLP tasks in the legal domain, as well as large\nlegal corpora and datasets for each task, which were not fully covered in Sun’s study.\nCui et al. [26]\n✓\n–\n✓\n✗\n2023\nThis paper comprehensively reviewed on LJP task. Authors analyzed 43 LJP datasets in 9\ndifferent languages. They summarized 16 evaluation metrics to evaluate three NLP tasks (text\nclassification, text generation, and text regression) in LJP. For LMs, they explored existing\nPre-trained Language Models (PLMs). Unlike this paper, our work provides comprehensive\ncoverage of all NLP tasks in the legal domain and includes an exploration of large legal corpora.\nAnh et al. [4]\n✗\n✓\n✓\n✗\n2023\nThis survey give a short explanation regarding challenges in legal language and how LLMs try to\novercome the challenges. Then, the authors summarized six NLP tasks in the legal domain that\ncan be addressed by LLMs. In terms of ethical and legal considerations, they discussed ‘bias and\nfairness’, ‘privacy and confidentiality’, ‘intellectual property’, ‘explainability and transparency’,\nand ‘responsible use’. The difference is that our work focuses on existing methods for all Legal\nNLP tasks, including LJP and LDS, available datasets for each task, and large legal corpora for\npre-training and fine-tuning, whereas they concentrated on ethical and legal considerations\nand the impact of LLMs on NLP in legal texts.\nGanguly et al.\n[39]\n–\n–\n✓\n✗\n2023\nGanguly and his colleagues presented a comprehensive review at ECIR 2023 and discuss several\nkey areas including the processing challenges of legal text, such as NER and sentence boundary\ndetection. They traced the historical evolution of AI and law research from the 1980s, highlighted\nrecent developments in NLP and IR techniques with a focus on the architectures of PLMs, and\nconducted a detailed survey of current issues and advancements in legal IR and NLP tasks\nlike LDS and LJP. The review also included perspectives from the industry. In contrast to their\nsurvey, we also explore LQA and LTC tasks, which they did not cover, and examine large legal\ncorpora, an area they did not address.\nChen et al.\n[23]\n✓\n✓\n✓\n✗\n2024\nThis survey study focuses on LLMs in three distinct fields: finance, healthcare, and law. Although\nit attempts to cover all aspects of the legal domain and LLMs, the broad scope of addressing\nthree expansive topics has resulted in a less thorough examination of many specific research\ncases within the legal field. In addition, it did not cover large legal corpora which are used for\npre-training and fine-tuning purposes. Also, we explore methods for improving the efficiency\nof LLMs in legal domain.\nKrasadakis\net al. [58]\n–\n–\n–\n–\n2024\nThis survey study focuses on challenges and advances in some NLP tasks, such as NER and\nRelation Extraction. Unlike this survey, our study covers all legal NLP tasks, along with their\ncorresponding datasets and large legal corpora. Additionally, we examine existing LLMs tailored\nfor the legal domain.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n4\n•\nAriai and Demartini\nLegal\nNLP\nLQA\nHuang et al. [48], Khazaeli et al. [57], Sovrano et al. [109]\nAskari et al. [5], Zhong et al. [139]\nLouis et al. [72], Zhang et al. [134]\nAskari et al. [6], Sovrano et al. [108], Yuan et al. [131]\nBüttner and Habernal [14], Chen et al. [21], Sovrano et al. [107]\nLJP\nLuo et al. [73], Ye et al. [130], Zhong et al. [136]\nChalkidis et al. [15], Medvedeva et al. [79], Yang et al. [129]\nXu et al. [128], Zhong et al. [137]\nAlmuslim and Inkpen [2], Feng et al. [37], Ma et al. [74]\nLiu et al. [71], Tong et al. [116], Zhang et al. [133]\nNiklaus et al. [85], Strickson and De La Iglesia [111]\nSemo et al. [100]\nLTC\nElnaggar et al. [34], Lee and Lee [60], Wei et al. [122]\nBambroo and Awasthi [8], Song et al. [106]\nFragkogiannis et al. [38], Mamooler et al. [76], Wang et al. [121]\nChalkidis et al. [16, 17], Tuggener et al. [117]\nGraham et al. [45], Nguyen et al. [83], Papaloukas et al. [90]\nLDS\nFarzindar [36], Gelbart and Smith [40, 41], Moens et al. [80]\nPolsley et al. [92], Schraagen et al. [99], Zhong et al. [140]\nJain et al. [49], Moro et al. [82], Zhong and Litman [141]\nLiu et al. [68], Shen et al. [102]\nLegal NER\nDozier et al. [32], Păis et al. [94], Smădu et al. [105]\nAu et al. [7], Kalamkar et al. [54], Leitner et al. [61]\nLegal corpora\nChalkidis et al. [19, 20], Zheng et al. [135]\nHenderson et al. [46], Rabelo et al. [95], Xiao et al. [127]\nBarale et al. [9], Niklaus et al. [86], Park and James [91]\nGoebel et al. [44], Niklaus et al. [87], Östling et al. [142]\nLegal LMs\nModels\nChalkidis et al. [18], Xiao et al. [126]\nColombo et al. [25], Shi et al. [104]\nAl-qurishi et al. [1]\nMethods\nLi et al. [63], Mamakas et al. [75]\nFig. 1. An overview of the research areas in Legal NLP and the key publications in the survey.\nSection 6, we address the key challenges associated with deploying NLP technologies in legal settings, discussing\nboth current issues and potential solutions. Since this survey contains many acronyms, Table 2 provides the list\nof acronyms and their meanings to make it easier to follow.\n2\nBACKGROUND AND FOUNDATIONAL CONCEPTS\nIn this section, we will establish an essential understanding of how NLP intersects with the legal domain. Initially,\nwe explore the characteristics of legal documents, which are fundamental to this intersection. Legal texts are\nknown for their complex structure, specialized words. Recognizing these attributes is essential as they significantly\ninfluence the development and application of AI technologies in legal practices.\n2.1\nLegal Documents\nLegal documents are typically written in a descriptive language and presented in an unstructured text format. They\nhave unique features that set them apart from other fields. These documents cover a broad array of texts essential\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n5\nTable 2. List of Acronyms Used in the Survey.\nAcronyms\nMeaning\nAcronyms\nMeaning\nAMR\nAbstract Meaning Representation\nLJP\nLegal Judgment Prediction\nAL\nActive Learning\nLQA\nLegal Question Answering\nAI\nArtificial Intelligence\nLTC\nLegal Text Classification\nBERT\nBidirectional Encoder Representations from Transformers\nLSTM\nLong Short-Term Memory\nBi-GRU\nBidirectional Gated Recurrent Units\nML\nMachine Learning\nBi-LSTM\nBidirectional LSTM\nMLM\nMasked Language Modeling\nCanLII\nCanadian Legal Information Institute\nMGAT\nMulti-Graph Attention Network\nCA\nCase Analysis\nML-LJP\nMulti-Law aware LJP\nCRF\nConditional Random Field\nMLP\nMulti-Layer Perceptron\nCNN\nConvolutional Neural Networks\nMPBFN\nMulti-Perspective Bi-Feedback Network\nDL\nDeep Learning\nMTL\nMulti-task Learning\nDAG\nDirected Acyclic Graph\nNER\nNamed Entity Recognition\nEDUs\nElementary Discourse Units\nORC\nOpen Research Challenge\nECtHR\nEuropean Court of Human Rights\nPEFT\nParameter-Efficient Fine-Tuning\nFSCS\nFederal Supreme Court of Switzerland\nPLM\nPre-trained Language Model\nGRU\nGated Recurrent Unit\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\nGAT\nGraph Attention Networks\nQA\nQuestion Answering\nGESAN\nGraph-Based Evidence Retrieval and Aggregation Network\nRNN\nRecurrent Neural Networks\nIR\nInformation Retrieval\nRL\nReinforcement Learning\nJEC-QA\nJudicial Examination of Chinese Question Answering\nRLHF\nReinforcement Learning from Human Feedback\nKG\nKnowledge Graph\nRAG\nRetrieval-Augmented Generation\nKD\nKnowledge-Driven\nSLM\nSmall Language Model\nLLM\nLarge Language Model\nSVM\nSupport Vector Machine\nLM\nLanguage Model\nTF-IDF\nTerm Frequency Inverse Document Frequency\nLDS\nLegal Document Summarization\nto the functioning of the legal system. They include court filings, judgments, legislation, treaties, contracts,\nand legal correspondence, each serving a specific purpose and following to specific formatting and content\nstandards that reflect legal logic and hierarchy. Legal documents are Fundamental tools for lawyers, judges,\nand legal scholars, facilitating case analysis, legislative review, and contract drafting. They are also essential\nin legal education and practice and provide the basis for legal arguments and decisions. Common examples of\nlegal documents include case law repositories, statutory databases, and collections of legal agreements. These\ndocuments are utilized in various legal processes such as drafting legal arguments, performing legal analysis, and\nensuring regulatory compliance.\n2.1.1\nLegal language and its characteristics. Legal language is characterized by its unique features that set it\napart from everyday language, primarily due to its function in the legal system. One prominent feature is its\nformality, where legal language often employs a more formal vocabulary and syntax to ensure precision and\navoid ambiguity [43]. This formality is critical, as the precise meaning of terms can have significant legal effects.\nLegal texts also typically utilize passive constructions and complex sentence structures to provide detailed and\ncomprehensive descriptions [43]. These constructions help clarify responsibilities and outcomes without directly\nattributing actions or intentions to specific parties.\nAnother distinctive aspect of legal language is its reliance on specialized words and phrases. This includes\nterms that have specific meanings within legal contexts, archaic words that are not commonly used in everyday\nlanguage, and standardized phrases that have been historically embedded in legal tradition [43]. This can make\nlegal documents less accessible to non-specialists, requiring legal professionals to interpret the content accurately.\nFurthermore, legal language is heavily intertextual, meaning it frequently references other legal texts, such\nas statutes, regulations, and case law. This characteristic ensures that legal arguments are grounded in and\nsupported by existing legal frameworks and previous cases. The dense use of citations and references in legal\ndocuments not only supports the arguments made but also connects the document to a broader legal discourse.\nSuch intertextuality demands that legal professionals not only understand the texts themselves but also the\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n6\n•\nAriai and Demartini\nFig. 2. A sample page from the Code of Federal Regulations, illustrating the structured and referenced nature of legal\ndocuments.\nbroader legal context in which they operate. To illustrate the intertextuality of legal language, Figure 2 shows\na sample page from the Code of Federal Regulations of the United States, extracted from the WestLaw [124]\nwebsite. Figure 2 displays § 40.51, Labor Certification, from 22 C.F.R. § 40.51, which is part of the Code of Federal\nRegulations of the United States. This section falls under Title 22, governing regulations related to foreign\nrelations, specifically detailing the requirements and procedures for labor certification. Notably, the text includes\nhighlighted references to other legal sources, such as INA 212(a)(5). This citation refers to the Immigration and\nNationality Act, specifically section 212, which outlines various conditions for inadmissibility into the United\nStates, under subsection (a), paragraph (5). In the “Credits” part, you can see the reference “56 FR 30422, July\n2, 1991,” which points to a publication in the Federal Register. Here, “56 FR” indicates the volume number, and\n“30422” is the page number where the document begins. The date “July 2, 1991,” marks the publication date in\nthe Federal Register. Additionally, a sentence in subsection (b), paragraph (1) consists of 68 words, showing the\nlength and complexity typical of legal texts.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n7\nThe specific characteristics of legal language, such as its formal vocabulary, complex syntax, and extensive\nuse of references, present many challenges for NLP. Disambiguation titles and nested entities are other issues in\nlegal contexts [58]. Disambiguation titles, such as ‘The President of USA’ requires precise identification based\non contextual details like time and location. Nested entities , where titles of legislative articles refer to other\nlaws, introduce further complexity. On the other hand, legal documents are frequently provided in non-machine-\nreadable PDF formats, complicating data extraction and processing. These challenges highlight the need for\nadvanced and specialized NLP solutions tailored to the legal domain.\n2.2\nLegal NLP\n2.2.1\nIntroduction to Legal NLP. Legal NLP involves the application of NLP techniques to legal texts. This field\nis crucial as it helps automate and enhance the analysis of complex legal documents, improving efficiency and\naccuracy in legal research, compliance, and decision-making processes. The foundation of NLP is text, and the\nlegal domain primarily consists of textual data [2], including statutes, case law, contracts, and regulations. Given\nthe text-intensive nature of the legal field, NLP offers significant potential to transform how legal professionals\ninteract with and utilize this vast amount of information. By leveraging advanced algorithms and Machine\nLearning (ML) models, Legal NLP aims to make legal texts more accessible, interpretable, and actionable.\n2.2.2\nWhy NLP is a game-changer for the legal section. LLMs, as a part of NLP applications, such as ChatGPT [88],\nhave had a significant impact since their public debut in November 2022. The legal sector, however, has been\nexploring the potential of AI for a longer period, applying it practically. NLP applications in the legal field are\nextensive, ranging from drafting client briefs and producing complex analyses from large document sets to\nenabling smaller firms to compete with larger ones [78]. NLP is very important in doing thorough checks when\ncompanies merge and greatly helps in legal education and learning in fast-changing fields [78].\nA notable demonstration of the capabilities of NLP applications in the legal sector occurred when GPT-4\npassed the Uniform Bar Exam, underscoring the technology’s potential [56]. Lawyers and law students are keenly\naware of this potential, as evidenced by a LexisNexis survey released in August 2023 [62]. The survey revealed\nthat about half of all lawyers believe that LLMs will significantly transform legal practice, with nearly all (92\npercent) expecting some impact. Additionally, 77 percent of respondents believe LLMs will increase the efficiency\nof lawyers, paralegals, and law clerks, while 63% foresee changes in how law is taught and studied. Moving\nforward, we will introduce specific NLP tasks in the legal domain, exploring their applications and impacts they\nare expected to have on the legal profession.\n2.2.3\nBasic foundations of NLP. The integration of NLP in the legal domain relies on several foundational\ntechniques that enable the effective analysis and manipulation of legal texts. These fundamental methods provide\nthe building blocks for more complex applications, transforming unstructured legal documents into structured,\nactionable information. By leveraging these NLP techniques, legal professionals can enhance their efficiency and\naccuracy, making it easier to manage and interpret vast amounts of legal data. The following section provides\nessential definitions of terms related to the basic foundations of NLP:\n• Tokenization: Tokenization is the process of breaking down a sequence of text into smaller units, typically\nwords or sub-words, known as tokens. This is a fundamental step in NLP as it allows for the structured\nanalysis of text. In the legal domain, tokenization helps in processing and understanding lengthy documents\nby segmenting them into manageable pieces.\n• Word Embeddings: Word Embeddings are continuous vector representations that encode the semantic\nmeanings of words or tokens in a high-dimensional space. These representations allow models to convert\nindividual tokens into a format suitable for neural network processing. During training, LMs develop\nembeddings that capture the relationships between words, such as synonyms.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n8\n•\nAriai and Demartini\n• Transformers: Transformers [118] are a neural network architecture designed to convert input sequences\ninto output sequences by understanding the context and relationships among the elements of the sequence.\nFor instance, given the input “What is the color of the sky?”, a transformer model internally processes\nand identifies connections among the words ‘color’, ‘sky’, and ’blue’. This understanding enables it to\nproduce the output: “The sky is blue” [3]. Transformer models enhance this process through a self-attention\nmechanism. This mechanism allows the model to analyze different parts of the sequence simultaneously\ninstead of sequentially, helping it identify which parts are most significant.\n• PLMs: PLMs are trained on extensive corpora in a self-supervised manner, which involves tasks like recover-\ning incomplete input sentences or auto-regressive language modeling. These models, such as Bidirectional\nEncoder Representations from Transformers (BERT) [29] and RoBERTa [70], are initially trained on large-\nscale general text datasets. After pre-training, they can be fine-tuned for specific downstream tasks in the\nlegal domain, adapting them to comprehend and process legal language for applications like document\nclassification and information extraction.\n• Question Answering (QA): QA system is a type of NLP solution designed to answer questions posed in\nnatural language. These systems take a user’s query and, by extracting relevant information from a dataset\nprovide a relevant and informative response.\n• NER: NER is the task of identifying mentions of specific entities within a text that belong to predefined\ncategories such as persons, locations, organizations, and more. NER is a fundamental component for many\nNLP applications, including question answering, text Summarization, and machine translation [64].\n• Information Retrieval (IR): IR involves the process of obtaining relevant information from large\ncollections of unstructured legal texts, such as case laws, statutes, contracts, and regulations. The goal of IR\nis to provide users with the most relevant documents and data in response to specific queries.\n• Multi-task Learning (MTL): MTL is an approach in ML where a model is trained on multiple related\ntasks simultaneously or utilizes auxiliary tasks to enhance performance on a specific task. By learning\nfrom diverse tasks, MTL enables models to capture generalized and complementary knowledge, improving\nrobustness and addressing data scarcity, particularly for low-resource tasks. MTL’s ability to share implicit\nknowledge across tasks often leads to performance gains and more efficient models, making it a valuable\nstrategy for building robust and adaptable systems in NLP and other domains [22].\n• Parameter-Efficient Fine-Tuning (PEFT): PEFT is a method for adapting PLMs that involves freezing\nthe majority of the model’s parameters and only updating a small subset. This approach significantly\nreduces the computational resources and time required for fine-tuning, making it particularly effective in\nresource-limited scenarios, while still achieving competitive performance in tasks like text generation [65].\n• Retrieval-Augmented Generation (RAG): RAG is an advanced AI framework that enhances text creation\nby merging traditional information retrieval systems, with the generative power of LLMs. This integration\nallows the AI to access additional knowledge sources while utilizing its advanced language capabilities.\n2.2.4\nKey Publications and Conferences in Legal NLP. This section highlights the key journals, conferences, and\nworkshops that serve as platforms for sharing advancements and insights in the intersection of NLP and the\nlegal domain. These resources provide good opportunities for researchers to engage with cutting-edge research\nin Legal NLP.\nSeveral leading journals focus on the intersection of AI, NLP, and the legal domain. “Artificial Intelligence and\nLaw”, published by Springer, is a leading journal that features research articles on legal reasoning, legal IR, and\nlegal knowledge representation. Additionally, the “Journal of Law and Information Technology”, published by\nTaylor & Francis , focuses on the application of information technology in law, including research AI.\nConferences significantly advance research and promote collaboration in Legal NLP. The International Confer-\nence on Artificial Intelligence and Law (ICAIL) is a biennial event that showcases advances in AI applications in\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n9\nthe legal domain, including NLP and ML. The Conference on Legal Knowledge and Information Systems (JURIX)\nis an annual event that focuses on legal informatics and NLP technologies.\nIn Legal NLP, there are some good works which are presented in workshops. The Workshop on Automated\nSemantic Analysis of Information in Legal Texts delves into NLP and semantic analysis of legal texts. The Natural\nLegal Language Processing (NLLP) Workshop provides a platform for discussing NLP technologies tailored for\nlegal texts and is often part of major NLP conferences. The EXplainable AI in Law (XAILA) Workshop focuses on\nthe explainability of AI systems in legal contexts, aiming to improve transparency and trust in AI applications. The\nCompetition on Legal Information Extraction/Entailment (COLIEE) is an annual event that challenges participants\nto develop innovative solutions for legal information extraction and entailment tasks.\n3\nMETHODOLOGY\nThis survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) frame-\nwork [89]. It ensures a transparent and comprehensive assessment of research to NLP tasks within the legal\nsector.\n3.1\nSearch Strategy\nWe performed a systematic search across two academic databases to identify relevant studies, including: Google\nScholar and IEEE Xplore. Then, search queries were crafted to capture studies that focused on the application of\nNLP to legal tasks. The search was defined by the following queries:\n• Query 1: (“Natural Language Processing” OR “NLP”) AND (“Legal” OR “Law”)\n• Query 2: (“Legal” AND (“Named Entity Recognition” OR “NER” OR “Document Summarization” OR “Text\nClassification” OR “Document Classfication” OR “Judgment Prediction” OR “Question Answering” OR\n“Corpus” OR “Language Model”))\nOur search covered publications with the following date ranges for each specific NLP task: LQA from 2020-2024,\nLJP from 2017-2024, LTC from 2018-2023, LDS from 2016-2024, Legal NER from 2010-2022, legal corpora from\n2021-2024, and Legal LMs from 2020-2024. This ensured that we included recent advancements. Peer-reviewed\njournal articles and high-quality conference proceedings were prioritized, along with a secondary consideration\nof non-peer-reviewed sources, such as arXiv articles, where relevant.\n3.2\nStudy Selection\nA total of 148 studies were initially identified from the database search. To refine this list, we applied a manual\nreviewing and. This process involved:\n(1) Title and Abstract Screening: We reviewed the titles and abstracts of all retrieved studies to assess their\nrelevance to the predefined legal NLP tasks. Studies unrelated to the core legal NLP and its tasks were\nexcluded at this stage.\n(2) Full-Text Review: Articles that passed the initial screening went through a more detailed full-text review.\nThis was done to confirm their relevance, quality, and alignment with the inclusion criteria. During this\nphase, a careful study of the literature review sections of each included research and survey was conducted.\nThis helped ensure that the studies not only contributed original findings but also reflected a comprehensive\nunderstanding of the existing research landscape in legal NLP.\n(3) Final Selection: From the original 148 studies, 127 were included in the final list, selected based on their\ndirect relevance to key NLP tasks within the legal sector and their methodological quality, as well as their\nengagement with existing literature in the field.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n10\n•\nAriai and Demartini\n3.3\nEligibility Criteria\nTo determine which studies were included in the final synthesis, we established the following criteria:\n• Inclusion Criteria:\n– The study must focus on at least one of the following NLP tasks: Legal Question Answering, Legal Named\nEntity Recognition, Legal Judgment Prediction, Legal Document Summarization, Legal Text Classification,\nLegal Language Models, or legal corpora.\n– The study must present empirical research or significant methodological contributions to legal NLP.\n– Both peer-reviewed and non-peer-reviewed (e.g., arXiv) studies were considered if they provided valuable\ninsights.\n• Exclusion Criteria:\n– Studies focused exclusively on unrelated areas such as information retrieval methods, pattern mining,\ninformation extraction, or similarity detection without a clear application to the specific legal NLP tasks\nmentioned.\n– General NLP studies without a focus on legal applications.\n– Editorials, opinion pieces, or other non-research articles.\n– Papers that did not meet basic methodological standards were not included in the final analysis.\n4\nNLP TASKS, DATASETS, AND LARGE CORPORA IN LEGAL DOMAINS\nNLP tasks in the legal domains cover a range of specialized applications that address the unique challenges\nand requirements of legal texts. These tasks leverage advanced NLP techniques to process, analyze, and extract\nmeaningful information from legal documents. Legal NLP tasks help make legal research and decision-making\nmore efficient and accurate. In this section, we explore various NLP tasks that are tailored to the legal domain\nand show their impact on the legal section. Additionally, we will discuss existing works and research related to\neach task and provide an overview of the current state of the art in this field.\nFurthermore, the success of these NLP tasks heavily depends on the availability and quality of domain-specific\ndatasets. Therefore, we will also examine the datasets commonly used in legal NLP research, exploring their\ncharacteristics and the role they play in advancing the development of NLP models for the legal domain.\n4.1\nLegal Question Answering\n4.1.1\nTask. LQA involves responding to queries about law. This task is typically done by legal professionals skilled\nin the relevant legal field. This process requires a comprehensive review of existing laws, careful interpretation\nof statutes and regulations, and the application of legal principles and past cases to the relevant facts. LQA seeks\nto offer precise advice on legal matters. It helps people and businesses in navigating the legal landscape and\naddressing legal challenges. Recently, DL has been leveraged in LQA to employ neural network models that train\non extensive datasets to identify complex patterns and relationships. These models evaluate the questions posed,\nidentify relevant legal topics, and produce suitable answers based on the patterns they have learned.\nModern ML approaches for LQA, particularly DL, rely on neural networks to understand natural language.\nPopular architectures include Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Convo-\nlutional Neural Networks (CNN), which can be fine-tuned for specific tasks such as QA. These models generate\naccurate responses, adapt to new patterns, understand context, and provide relevant answers. Transformer-based\nmodels, such as BERT [29] and ChatGPT [88], have proven particularly effective in NLP tasks. These models\nuse the transformer architecture and self-attention mechanisms to learn the context of the text and understand\nthe meaning of words. This allows them to provide relevant answers by effectively weighing the importance of\ndifferent parts of the input. In the following, we will study the existing LQA works in legal domain.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n11\nHuang et al. [48] introduce the Artificial Intelligence Law Assistant, the first Chinese LQA system that integrates\na legal knowledge graph (KG) to enhance query comprehension and answer ranking. The system collects a\nlarge-scale QA corpus from an online legal forum and constructs a legal KG with over 42,000 legal concepts.\nIt employs a knowledge-enhanced interactive attention network using Bi-LSTM and co-attention mechanisms\nto enrich semantic representations of QA pairs with legal domain knowledge. Additionally, it provides visual\nexplanations for selected answers, offering users a clear understanding of the QA process.\nAn example of an answer retrieval system specific to Private International Law is proposed by Sovrano et al.\n[109]. This system integrates Term Frequency Inverse Document Frequency (TF-IDF) with deep LMs to retrieve\nrelevant answers from an automatically generated KG of contextualized grammatical sub-trees. The KG aligns\nwith a legal ontology based on Ontology Design Patterns, such as agent, role, event, temporal parameter, and\naction, to reflect the legal significance of the relationships within and between provisions.\nKhazaeli et al. [57] develop an IR-based QA system tailored to the legal domain, combining sparse vector search\n(BM25) and dense vector techniques (semantic embeddings) as input to a BERT-based [29] answer re-ranking\nsystem. The system utilizes Legal GloVe and Legal Siamese BERT embeddings to enhance retrieval effectiveness.\nAn answer finder component computes the probability of a passage answering the question using a BERT\nsequence binary classifier fine-tuned on question-answer pairs, improving the model’s ability to discriminate\ngood answers.\nLi et al. [66] introduce a retrieving-then-answering framework featuring the Graph-Based Evidence Retrieval\nand Aggregation Network (GESAN) to enhance QA on the Judicial Examination of Chinese Question Answering\n(JEC-QA) dataset [139]. The framework leverages relevant legal knowledge by predicting question topics and\nretrieving legal paragraphs using BM25. GESAN aggregates the evidence and processes it along with the question\nand options to make accurate predictions, demonstrating improved accuracy and reasoning abilities in LQA.\nAskari et al. [5] propose a method for generating query-dependent textual profiles for lawyers to improve legal\nexpert finding on QA platforms. Using data from the Avvo1 QA forum, they focus on aspects such as sentiment,\ncomments, and recency to create profiles. These profiles are fine-tuned using BERT models [29], and the final\naggregated score is calculated using a linear combination of profile-trained BERT models. It improves retrieval\nperformance in the legal expert finding task.\nZhang et al. [134] propose a generation-based method for LQA, modeling it as a generation task to produce\nnew, relevant answers tailored to each question. The system incorporates laws as external knowledge into the\nanswer generation process, using a retriever to fetch applicable law articles and a generator to create answers\nusing this knowledge. Both components are integrated into a single T5 [96] model using MTL. It can enhance the\nmodel’s understanding and generation abilities while ensuring answers are accurate and informative.\nLouis et al. [72] present an end-to-end methodology for generating long-form answers to statutory law\nquestions using a “retrieve-then-read” pipeline. The approach involves a retriever component that uses a bi-\nencoder model to fetch relevant legal provisions, followed by a generator that formulates comprehensive answers\nbased on these provisions. The generator, an autoregressive LLM based on the Transformer architecture, employs\nin-context learning and PEFT to generate detailed answers. The model’s interpretability is enhanced by an\nextractive rationale generation strategy, ensuring responses are accompanied by verifiable justifications.\nSovrano et al. [108] propose DiscoLQA, a discourse-based LQA system that focuses on important discourse\nelements like Elementary Discourse Units (EDUs) and Abstract Meaning Representations (AMRs). This approach\nhelps the answer retriever identify the most relevant parts of the discourse, enhancing retrieval accuracy. They\nintroduce the Q4EU dataset, containing over 70 questions and 200 answers on six European norms, demonstrating\nimproved performance in legal QA even without domain-specific training.\n1https://www.avvo.com\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n12\n•\nAriai and Demartini\nYuan et al. [131] present a three-step approach to bridge the legal knowledge gap by creating CLIC-pages—snippets\nthat explain technical legal concepts in layperson’s terms. They construct a legal question bank containing legal\nquestions answered by CLIC-pages, using large-scale PLMs like GPT-3 [13] to generate machine-generated\nquestions. The study demonstrates that machine-generated questions are more scalable and diversified, aiding in\nimproving accessibility to legal information for non-experts.\nAskari et al. [6] propose a cross-encoder re-ranker (𝐶𝐸𝐹𝑆) for legal answer retrieval, incorporating fine-grained\nstructured inputs from community QA data to enhance retrieval effectiveness. They introduce the LegalQA\ndataset containing 9,846 questions and 33,670 lawyer-curated answers. The approach involves a two-stage ranking\npipeline with a BM25 retriever followed by a re-ranker, showing that integrating question tags into the input\nstructure can bridge the knowledge gap and improve retrieval in the legal domain.\n4.1.2\nDatasets. The LQA datasets are a specialized resource designed to facilitate research in the domain of\nlegal NLP. they consist of a collection of legal questions and corresponding answers, drawn for various legal\ndocuments and case law. Most questions in the LQA datasets fall into two main categories: knowledge-driven\nquestions (KD-questions) and case-analysis questions (CA-questions) [139]. KD-questions are centered around\nthe understanding of specific legal concepts, whereas CA-questions involve the analysis of actual legal cases.\nBoth types demand advanced reasoning skills and a deep comprehension of the text, making LQA a particularly\nchallenging task in the field of NLP.\nZhong et al. [139] introduce JEC-QA, a dataset with 26,365 multiple-choice questions from the National Judicial\nExamination of China and related websites. Each question offers four possible answers and is labeled with the\ntype of reasoning required, such as word matching, concept understanding, numerical analysis, multi-paragraph\nreading, and multi-hop reasoning. This dataset poses significant challenges for QA models, highlighting the gap\nbetween machine performance and human expertise in complex legal reasoning.\nSovrano et al. [107] present a dataset designed to evaluate automated QA systems within the domain of Private\nInternational Law. It includes 17 carefully selected questions based on key EU regulations—Rome I, Rome II, and\nBrussels I bis—with answers derived directly from these regulations. The questions are classified based on their\nspecificity, allowing for nuanced analysis of context-dependency in legal reasoning. This dataset aids in assessing\nthe performance of QA systems intended for legal professionals navigating complex cross-border legal issues.\nEQUALS [21] is a large-scale annotated LQA dataset in Chinese law, containing 6,914 question-answer pairs\nwith answers based on specific law articles. Curated by senior law students, it covers 10 collections of Chinese\nlaws and includes annotations indicating the type of reasoning required for each question. The dataset ensures\nthat answers are precise excerpts from relevant law articles, making it valuable for developing advanced LQA\nsystems that can aid in legal research and decision-making.\nBüttner and Habernal [14] introduce GerLayQA, a dataset supporting LQA for laypersons in Germany, focusing\non the civil-law system. It contains 21,538 real-world questions posed by laypersons, paired with expert answers\nfrom lawyers grounded in specific paragraphs of German law books. The dataset are constructed through filtering\nand quality assurance to ensure accuracy and relevance, making it a valuable resource for developing LQA\nsystems that can interpret and apply German law to everyday legal inquiries.\n4.2\nLegal Judgment Prediction\n4.2.1\nTask. LJP is an important task within legal NLP, especially prevalent in civil law systems where judgments\nare determined based on case facts and statutory articles [138]. This task involves predicting legal outcomes from\nthe descriptions of cases and relevant legal statutes, and is essential in countries like France, Germany, Japan,\nand China [138]. LJP has garnered significant attention from AI researchers and legal professionals due to its\npotential to assist judges, lawyers, and legal scholars in predicting case outcomes based on historical data [23].\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n13\nDespite its promise, LJP is a highly demanding and challenging task. It requires careful handling of natural\nbiases in historical legal data, which can create feedback loops that amplify discrimination [58]. Therefore,\nensuring the impartiality of predicted rulings is crucial [58]. Currently, LJP is primarily performed by legal experts\nwho undergo extensive specialized training to manage the complex steps involved, such as identifying relevant\nlaw articles, defining charge ranges, and deciding penalty terms [26]. Nevertheless, LJP provides substantial\nbenefits, streamlining legal decision-making processes for both practitioners and ordinary citizens [26].\nLuo et al. [73] propose an attention-based neural network to enhance charge prediction by jointly modeling\ncharge prediction and relevant law article extraction. They used Bidirectional Gated Recurrent Units (Bi-GRUs)\nto encode fact descriptions and an article extractor to identify top relevant law articles. The model employs an\nattention mechanism guided by context vectors to combine embeddings for prediction. Evaluations on Chinese\njudgment documents showed improved accuracy in predicting charges and providing relevant legal articles.\nZhong et al. [136] introduce TopJudge, a topological MTL framework that models dependencies among subtasks\nin LJP, such as law article prediction, charge prediction, and penalty terms. Using a Directed Acyclic Graph\n(DAG), TopJudge processes subtasks in a topological order reflecting real-world legal decision-making. Evaluated\non large-scale Chinese criminal case datasets, it outperformed previous models in predicting legal outcomes.\nYe et al. [130] address the problem of Court View Generation from fact descriptions in criminal cases to\nenhance the interpretability of charge prediction systems and aid in automatic legal document generation.\nThey formulated this as a text-to-text Natural Language Generation (NLG) problem, using a label-conditioned\nSeq2Seq model with attention to decode court views based on encoded charge labels. Their model outperformed\nbasic Seq2Seq models in generating accurate and natural court views. This work contributes to automatic legal\ndocument generation by providing justifications for charge decisions.\nYang et al. [129] propose a Multi-Perspective Bi-Feedback Network (MPBFN) with a Word Collocation Attention\nmechanism to improve LJP. MPBFN addresses the challenges of multiple subtasks and their dependencies by\nusing a bi-feedback mechanism for forward prediction and backward verification among subtasks. The Word\nCollocation Attention integrates word collocation features and numerical semantics to better predict penalties.\nEvaluated on the CAIL-small and CAIL-big datasets [127], their model outperformed baselines in predicting law\narticles, charges, and penalty terms.\nChalkidis et al. [15] introduce an English LJP dataset containing approximately 11.5k cases from the European\nCourt of Human Rights (ECHR). They evaluated various neural models on this dataset, including a hierarchical\nversion of BERT [29] (HIER-BERT) to handle long legal documents. Their models outperformed previous feature-\nbased approaches in tasks like violation classification and case importance prediction. They also explored potential\nbiases in legal predictive models using data anonymization.\nMedvedeva et al. [79] investigate using NLP tools to predict judicial decisions of the ECHR based on court\nproceeding texts. They employed an SVM linear classifier to predict violations of articles, achieving an average\naccuracy of 75%. However, when predicting future decisions based on past cases, accuracy decreased to 58–68%.\nThe study also found that predicting outcomes based solely on judges’ surnames could achieve an average\naccuracy of 65%, highlighting potential biases.\nZhong et al. [137] introduce QAjudge, a reinforcement learning-based model designed to provide interpretable\nlegal judgments by visualizing the prediction process. QAjudge uses a Question Net to iteratively select relevant\nyes-no questions about case facts, an Answer Net to provide answers, and a Predict Net to generate the final\njudgment. The model aims to minimize the number of questions asked. It focuses on crucial elements to ensure\nfairness and interpretability. Evaluated on real-world datasets, QAjudge demonstrated potential in providing\nreliable and transparent legal judgments.\nXu et al. [128] propose the Law Article Distillation based Attention Network (LADAN), an end-to-end model\naddressing the issue of confusing charges in LJP by distinguishing similar law articles. The model uses a novel\ngraph neural network to learn differences between confusing law articles and an attention mechanism to extract\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n14\n•\nAriai and Demartini\ndiscriminative features from fact descriptions. Experiments on real-world datasets showed that LADAN improved\nperformance over previous methods in law article prediction, charge prediction, and penalty term prediction.\nStrickson and De La Iglesia [111] present the first LJP model for UK court cases, creating a labeled dataset of\nUK court judgments spanning 100 years. They evaluated various ML models and feature representations, with\ntheir best model achieving an accuracy of 69%. The study demonstrated the potential of LJP for UK courts, though\nchallenges remain due to the complexity of legal language and lack of structured public datasets.\nMa et al. [74] introduce MSJudge, a MTL framework designed to predict legal judgments by leveraging multi-\nstage judicial data, including pre-trial claims and court debates. MSJudge consists of components to encode\nmulti-stage context, model interactions among claims, facts, and debates, and predict judgments. Evaluated\non a large civil trial dataset2, MSJudge outperformed state-of-the-art baselines, enhancing trial efficiency and\njudgment quality.\nAlmuslim and Inkpen [2] focus on LJP for Canadian appeal court cases, employing various NLP and ML\nmethods to predict binary outcomes (‘Allow’ or ‘Dismiss’) based on case descriptions. Deep learning models\nusing custom Word2Vec embeddings achieved the highest accuracy of 93%, significantly outperforming classical\nML models. The study highlights the potential of predictive models to aid legal professionals and establishes a\nfoundation for future research in the Canadian legal system.\nFeng et al. [37] address limitations of state-of-the-art LJP models by proposing an event-based prediction\nmodel with constraints to improve performance. The model extracts fine-grained key events from case facts\nand predicts judgments based on these events rather than the entire fact statement. They manually annotated a\nlegal event dataset and introduced output constraints to guide learning. Their method effectively leverages event\ninformation and cross-task consistency constraints, enhancing LJP accuracy.\nTong et al. [116] introduce GJudge, a graph boosting framework incorporating constraints to address short-\ncomings of traditional LJP methods. GJudge features a multi-perspective interactive encoder and a Multi-Graph\nAttention Network (MGAT) consistency expert module. The encoder merges fact descriptions with label similarity\nconnections, while the expert module distinguishes similar labels and maintains task consistency. Testing on\ndatasets showed that GJudge outperformed other models, including the state-of-the-art RLJP [125] model, with\nhigher F1 scores.\nPrevious works mainly focus on creating accurate representations of a case’s fact description to enhance\njudgment prediction performance. However, these methods often overlook the practical judicial process, where\nhuman judges compare similar law articles or potential charges before making a decision. To address this gap,\nZhang et al. [133] propose CL4LJP, a supervised contrastive learning framework to improve LJP by capturing\nfine-grained differences between similar law articles and charges. The framework includes contrastive learning\ntasks at the article, charge, and label levels, enhancing the model’s ability to model relationships between fact\ndescriptions and labels. Experiments demonstrated that CL4LJP outperformed previous methods, proving its\neffectiveness and robustness.\nLiu et al. [71] propose ML-LJP, a Multi-Law aware LJP method that expands law article prediction into a\nmulti-label classification task incorporating both charge-related and term-related articles. The approach uses label-\nspecific representations and contrastive learning to distinguish similar definitions. A Graph Attention Network\n(GAT) is employed to learn interactions among multiple law articles for prison term prediction. Experiments\nshowed that ML-LJP outperformed state-of-the-art models, particularly in prison term prediction.\n4.2.2\nDatasets. The LJP datasets are specialized resources designed to advance research in predicting judicial\noutcomes within the domain of legal NLP. These datasets are categorized into four main types: court view\ngeneration datasets, law articles, charge prediction, and prison term prediction. Court view generation datasets\ninvolve court opinions and summaries. Law Articles datasets involve the prediction of legal outcomes based on\n2https://github.com/mly-nlp/LJP-MSJudge\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n15\nspecific statutes or regulations. Charge prediction datasets are concerned with predicting the charges that should\nbe brought against a defendant based on the case details, while prison term prediction datasets aim to estimate\nthe likely sentence duration given the nature of the crime and the legal context. Each type of dataset presents\nunique challenges, demanding not only text comprehension but also the ability to apply complex legal reasoning,\nmaking LJP a particularly complex task in the field of NLP.\nCourt View Gen [130] is an innovative dataset containing 171,981 Chinese legal cases, each involving a single\ndefendant and a corresponding charge, covering a total of 51 charge categories. This dataset is specifically curated\nto support the generation of court opinions based on charge labels. The data was sourced from publicly available\nlegal documents within the CJO3 repository.\nNiklaus et al. [85] introduce a multilingual LJP dataset from the Federal Supreme Court of Switzerland (FSCS),\ncontaining over 85,000 cases in German, French, and Italian. The dataset is annotated with publication years,\nlegal areas, and cantons of origin, making it suitable for NLP applications in judgment prediction.\nSemo et al. [100] introduce the first LJP dataset focused on class action cases in the United States. The dataset\ntargets predicting outcomes based on plaintiffs’ complaints rather than court-written fact summaries, involving a\nrule-based extraction system to identify relevant text spans from complaints.\nAlmuslim and Inkpen [2] construct a dataset for LJP within the Saskatchewan Court of Appeal. They collected\nand labeled 3,670 documents with case outcomes (‘allow’ or ‘dismiss’) using a two-step pattern matching and\nkeyword-based validation, ensuring label accuracy through manual annotation. This dataset supports research in\nthe Canadian legal system and aids in developing predictive models for legal judgments.\n4.3\nLegal Text Classification\n4.3.1\nTask. LTC is an important task within the domain of NLP that involves categorizing legal documents\nbased on their content, a foundational aspect of building intelligent legal systems. With the exponential growth\nof legal documents, it has become increasingly challenging for legal professionals to efficiently locate relevant\nrulings in similar cases for argumentation. LTC addresses this challenge by automatically associating legal texts\nwith predefined categories, such as criminal, civil, or administrative cases, thereby simplifying legal research and\ndecision-making processes.\nIn the legal field, this process is often referred to as predictive coding, where ML algorithms are trained through\nsupervised learning to classify documents into specific categories. The broader task of text classification in\nNLP involves assigning one or multiple categories to a document from a set of predefined options, and it can\ntake various forms, including binary classification, multi-class classification, and multi-label classification. Legal\ndocument classification often falls under large multi-label text classification, where the label space can consist of\nthousands of potential categories, adding significant complexity to the task [101]. This subsection explores the\nmethodologies and advancements in this area.\nDL typically requires extensive data to yield effective results, but MTL offers a potential solution to the data\nscarcity problem. Elnaggar et al. [34] leverage transfer learning and MTL to perform tasks like translation and\nmulti-label classification within legal document corpora. They utilize the MultiModel algorithm [53], which\nuses a fully convolutional sequence-to-sequence architecture integrating different modality nets. Their model\nprocesses legal texts through a unified embedding, enabling efficient task switching and promoting generalization\nacross different legal tasks, effectively tackling data scarcity in the legal field.\nWei et al. [122] investigate the application of CNNs for text classification in legal document review, comparing\ntheir performance with SVMs on four datasets from real legal cases. They found that CNNs perform better\nwith larger training datasets and offer a more stable growth trend compared to SVMs. The study highlighted\n3http://wenshu.court.gov.cn\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n16\n•\nAriai and Demartini\nchallenges such as sequence length limitations and the need for explainability in predictive analysis, suggesting\nimprovements like integrating annotated sentences with full text to enhance sentence relevance identification.\nLee and Lee [60] focuse on legal document classification in the Korean language and compare three different DL\napproaches: CNN with ASCII encoding, CNN with Word2Vec embeddings, and RNN with Word2Vec embeddings.\nThe classification models are used to classify case data into civil, criminal, and administrative. Using a dataset of\nnearly 60,000 past case documents, the study finds that the RNN model with Word2Vec embedding achieves the\nhighest classification accuracy.\nBambroo and Awasthi [8] introduce an architecture that integrates long attention mechanisms with a distilled\nBERT model pre-trained on legal domain-specific corpora. Their model employs a combination of local windowed\nattention and task-motivated global attention to handle inputs up to eight times longer than standard BERT\nmodels. The architecture, based on DistilBERT [97] and incorporating LongformerSelf-Attention, is optimized for\nlegal document classification, outperforming fine-tuned BERT and other transformer-based models in both speed\nand effectiveness.\nSong et al. [106] present a deep learning-based system built on top of RoBERTa [70] for multi-label legal\ndocument classification. They enhance the model with domain-specific pre-training, a label-attention mechanism,\nand MTL to improve classification accuracy, particularly for low-frequency classes. The label-attention mechanism\nuses label embeddings to bridge the semantic gap between samples and class labels, addressing class imbalance\nissues.\nFragkogiannis et al. [38] propose a method to improve classification of pages within lengthy documents\nby leveraging sequential context from preceding pages. They enhance the input to pre-trained models like\nBERT [29] by appending special tokens representing the predicted page type of the previous page, enabling\nmore context-aware classification without modifying the model architecture. Experiments on legal datasets\ndemonstrate improvements compared to non-recurrent setups.\nWang et al. [121] introduce a Document-to-Graph Classifier to classify legal documents based on facts and\nreasons rather than topics. They extract key entities and represented legal documents using four distinct relation\ngraphs capturing different aspects of entity relationships. A GATs [119] is used to learn document representations\nfrom the combined graph, improving classification by focusing on factual content.\nMamooler et al. [76] propose an active learning pipeline for fine-tuning PLMs for LTC. They address chal-\nlenges of specialized vocabulary and high annotation costs. Their method involves continued pre-training of\nRoBERTa [70] on legal texts, knowledge distillation using a pre-trained sentence transformer, and an efficient\ninitial sampling strategy by clustering unlabeled data. This approach reduces the number of labeling actions\nrequired and improves efficiency in adapting models to LTC tasks.\n4.3.2\nDatasets. LTC datasets are characterized by their domain-specific vocabulary and multi-label nature,\nrequiring models to interpret complex legal texts and categorize them into single or multiple legal themes.\nChalkidis et al. [16] release EURLEX57K, a dataset containing 57,000 EU legislative documents from the EUR-\nLEX portal4, annotated with EUROVOC5 concepts. This dataset facilitates research in LTC, including extreme\nmulti-label text classification, few-shot, and zero-shot learning, with documents tagged with an expansive set of\ndescriptors.\nTuggener et al. [117] introduce LEDGAR, a multi-label corpus of legal provisions from contracts scraped from\nthe U.S. Securities and Exchange Commission’s website. The dataset includes over 846,000 provisions across\n60,540 contracts, with an extensive label set suitable for text classification and legal studies, aiding in developing\nadvanced legal NLP models.\n4lex.europa.eu/\n5http://eurovoc.europa.eu/\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n17\nChalkidis et al. [17] present MULTI-EURLEX, a multilingual dataset containing 65,000 EU laws translated into\n23 official EU languages, annotated with EUROVOC labels. The dataset emphasizes temporal concept drift by\nadopting chronological splits, enhancing its utility for sophisticated LTC tasks requiring understanding nuanced\nlegal terms across different time periods.\nPapaloukas et al. [90] introduce the Greek Legal Code dataset, categorizing approximately 47,000 Greek\nlegislative documents into a detailed multi-level classification system. The dataset is structured into volumes,\nchapters, and subjects, each containing diverse legal documents from Greek legislation history, supporting LTC\nin the Greek legal domain.\nSong et al. [106] introduce POSTURE50K, a legal dataset containing 50,000 U.S. legal opinions annotated\nwith Legal Procedural Postures ranging from common to rare motions. The dataset includes an innovative split\nstrategy to support supervised and zero-shot learning evaluations, ensuring infrequent categories are adequately\nrepresented, enhancing model generalizability and testing accuracy.\nGraham et al. [45] develop a domain-specific dataset for LTC focusing on deontic modalities in contract\nsentences. They manually annotated contract sentences to train models for identifying deontic sentences like\npermissions, obligations, and prohibitions. The corpus, derived from the Contract Understanding Atticus Dataset\n(CUAD) [47], provides a resource for studying functional categories crucial for legal analysis.\n4.4\nLegal Document Sumarrization\n4.4.1\nTask. LDS is a specialized branch of automatic summarization that focuses on condensing legal texts, such\nas court judgments, into clear and informative summaries. Unlike general text summarization, which extracts key\ndetails without following specific formatting rules, LDS must account for the distinct structure and specialized\ncontent of legal documents. These documents often include complex details like article numbers, statutory\nlanguage, and citations that are critical for presenting the legal arguments and decisions accurately. The natural\ncomplexity of legal texts, characterized by their extensive length and detailed internal structures such as sections,\narticles, and paragraphs in statutes—demands tailored summarization techniques. This need is further emphasized\nby the hierarchical importance of documents based on their judicial origin, where the interpretation of texts can\nvary significantly between higher and lower court opinions [55].\nLDS can be approached through extractive and abstractive methods. Extractive summarization techniques in\nLDS focus on identifying and extracting the most critical sentences or phrases directly from the text, maintaining\nthe original wording and meaning. In contrast, abstractive summarization involves generating new sentences\nthat paraphrase the most important information, aiming for conciseness and coherence while ensuring that the\nessence of the legal text is preserved. This subsection will explore existing approaches to LDS and show the\nsmall differences that set it apart from more general summarization methods and discussing the challenges and\nsolutions specific to the legal domain.\nSeveral systems have been specifically designed to summarize legal documents. One of the first systems in this\nfield was the Fast Legal EXpert CONsultant (FLEXICON), created by Gelbart and Smith in 1991 [40]. FLEXICON\nutilizes a keyword-based approach [41], scanning a comprehensive database of terms to pinpoint crucial segments\nof text. Following this, Moens et al. [80] introduced the SALOMON system in 1999, which employs cosine\nsimilarity to cluster similar text regions, aiming to highlight relevant topics within the documents. This method\naligns with other abstraction-oriented techniques seen in the work of Erkan and Radev [35]. Another system,\nLetSum, devised by Farzindar and Lapalme in 2004 [36], also adopts a keyword-centric strategy but uses “cue\nphrases” to identify text related to specific themes such as ‘Introduction’, ‘Context’, and ‘Conclusion’. Although\nLetSum was fairly successful in mimicking human-generated summaries, it tended to produce summaries that\nwere excessively lengthy.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n18\n•\nAriai and Demartini\nBuilding on previous developments in LDS, Polsley et al. [92] introduce Casesummarizer, a tool designed for\nthe legal domain that pre-processes legal texts into sentences, scores them using a TF-IDF matrix from extensive\nlegal case reports, and enhances sentence scoring by identifying entities, dates, and section headings. The tool\nprovides a user-friendly interface with scalable summaries, lists of entities and abbreviations, and a significance\nheat map.\nZhong et al. [140] propose an automatic extractive summarization system for legal cases concerning Post-\ntraumatic Stress Disorder from the US Board of Veterans’ Appeals. It employs a train-attribute-mask pipeline\nusing a CNN classifier to iteratively select predictive sentences from case texts.\nNguyen et al. [83] propose an RL framework to enhance deep summarization models for the legal domain,\nutilizing Proximal Policy Optimization with a reward function that integrates both lexical and semantic criteria.\nThey fine-tune an extractive summarization backbone based on BERTSUM [69], employing a reward model\nthat includes lexical, sentence, and keyword-level semantics to produce better legal summaries. Schraagen et al.\n[99] apply an RL approach with a Bi-LSTM and a deep learning approach based on the BART transformer\nmodel to abstractive summarization of the Dutch case verdict database Rechtspraak.nl, combining extractive and\nabstractive summarization to retain core facts while creating concise summaries.\nZhong and Litman [141] focus on extractive summarization of legal case decisions, proposing an unsupervised\ngraph-based ranking model that leverages a reweighting algorithm to utilize document structure properties. They\nintroduce a reweighting algorithm to improve sentence selection in the HipoRank model [31]. It aims to reduce\nredundancy and enhance the selection of argumentative sentences from underrepresented sections.\nMoro et al. [82] introduce a transfer learning approach that combines extractive and abstractive summarization\ntechniques to address the lack of labeled legal summarization datasets, outperforming previous results on the\nAustralian Legal Case Reports dataset and establishing a new baseline for abstractive summarization.\nJain et al. [49] propose a sentence scoring approach, DCESumm, which combines supervised sentence-level\nsummary relevance prediction with unsupervised clustering-based document-level score enhancement. They\nutilize a Legal BERT-based Multi-Layer Perceptron (MLP) model to predict the summary relevance of each sentence,\nrefining scores through deep embedded sentence clustering to enhance the selection process by considering the\nglobal context of the document.\nLiu et al. [68] present Common Law Court Judgment Summarization (CLSum), a pioneering dataset for\nsummarizing multi-jurisdictional common law court judgments, leveraging large language models for data\naugmentation, summary generation, and evaluation. They employ a two-stage summarization process with\ntechniques like sparse attention mechanisms and efficient training methods to process lengthy legal documents\nwithin limited computational resources.\n4.4.2\nDatasets. LDS datasets are largely built from structured court proceedings and decisions, providing rich\nsources for both extractive and abstractive summarization methods. These datasets often emphasize abstractive\nsummarization to achieve concise, readable summaries that transform the original legal language into more\naccessible forms [102].\nZhong et al. [140] develop a dataset from 972,522 Board of Veterans’ Appeals decisions, focusing on single-issue\ncases related to Post-traumatic Stress Disorder. The dataset consists of 112 carefully sampled decisions, annotated\nby legal experts to capture key information such as ‘Issue’, ‘Procedural History’, ‘Service History’, ‘Outcome’,\n‘Reasoning’, and ‘Evidential Support’.\nShen et al. [102] introduce Multi-LexSum, an abstractive summarization dataset tailored for U.S. federal civil\nrights lawsuits, containing 40,000 source documents and 9,000 expert-written summaries of diverse lengths,\nproviding a rich resource for testing advanced summarization models.\nLiu et al. [68] publish CLSum, a dataset designed for summarizing multi-jurisdictional common law court\njudgments from Australia, Hong Kong, the United Kingdom, and Canada. This dataset leverages large language\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n19\nmodels for data augmentation and incorporates legal knowledge to enhance summary generation and evaluation.\nThis dataset addresses the challenge of sparse labeled data in legal domains. CLSum includes a comprehensive\ncollection of judgments and summaries from prominent court websites. It employs novel techniques to enrich\ntraining sets and improve model performance in few-shot and zero-shot learning scenarios.\n4.5\nLegal Named Entity Recognition\n4.5.1\nTask. NER is a fundamental task in NLP that involves identifying specific segments of text and classifying\nthem into predefined categories such as ‘organization’, ‘person’, and ‘location’ [59]. In the legal domain, NER\nextends to specialized recognition tasks that focus on extracting entities unique to legal texts, such as laws,\nlegal norms, and procedural terms. This specialized form of NER is crucial for structuring legal documents and\nenhancing legal IR systems. Unlike general NER systems that handle common entity types, legal NER must\nnavigate the complex language and structured format of legal documents, underscoring the need for systems and\nmethodologies specifically tailored to the legal context.\nDozier et al. [32] present pioneering work in NER within legal documents such as US case law and pleadings,\nemploying three methodologies: lookup, contextual rules, and statistical models to detect entities like judges,\nattorneys, and legal terms. Their system adapts these approaches to the specialized context of legal texts,\nprocessing various types of documents and extracting legal entities. This work highlights the challenges and\nnecessary adaptations for deploying NER in the legal domain, where the specialized language and high accuracy\nare required for successful legal analysis.\nPăis et al. [94] develop a NER model for the legal domain that integrates Bi-LSTM cells and a Conditional\nRandom Fields (CRFs) layer, utilizing multiple data sources and embedding types. Their system architecture\ncombines word embeddings from pre-trained models, character embeddings, gazetteer resources from the\nGeoNames6 database and JRC-Names [110], and known affixes to enrich the model’s understanding of legal\ntext. Their training process involved fine-tuning the word embeddings, while dynamically learning character\nembeddings with subsequent Bi-LSTM layers, enhancing the model’s capability to generalize across unseen\ntexts. They implemented the system using a modified version of NeuroNER [27], allowing for online model\nserving and incorporating features like dropout for regularization and gradient clipping to handle exploding\ngradients. They also explored ensemble methods to improve model accuracy by combining results from multiple\nmodel configurations, measuring the effectiveness through precision, recall, and F1 scores against a gold standard\ncorpus.\nSmădu et al. [105] explore domain adaptation in Legal NER, focusing on the Romanian and German languages.\nThey utilize a model architecture that integrates a pre-trained BERT [29] layer for feature extraction with Bi-LSTM\nnetworks to handle sequence dependencies and CRFs for sequence tagging. Their approach employs domain\nadaptation techniques through a gradient reversal layer connected to a domain discriminator, aimed at reducing\ndomain-specific biases and enhancing feature transferability across domains. This model trains on both legal and\ngeneral domains simultaneously, adapting to the peculiarities of each through adversarial learning that modifies\nthe learning process dynamically. This setup allows for improved generalization of the NER system across varied\nlinguistic and domain contexts, though the actual performance varied with minimal improvements noted for\nGerman and decreased performance for the Romanian legal dataset.\n4.5.2\nDatasets. Leitner et al. [61] present a dataset for NER focused on German federal court decisions, containing\napproximately 67,000 sentences and over two million tokens. This dataset features 54,000 manually annotated\nentities distributed across 19 fine-grained semantic classes, specifically tailored to the legal domain, such as\ncourt, judge, lawyer, law, person, and legal literature, along with over 35,000 TimeML-based time expressions.\nThe annotations cover both broad categories like location, person, and organization, and more specialized ones\n6https://www.geonames.org/\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n20\n•\nAriai and Demartini\nlike legal norms and case-by-case regulations, distinguishing between different types of legal acts and literature.\nThis dataset’s comprehensive annotation process involved multiple cycles to refine the tagging guidelines and\nenhance annotation quality.\nPăis et al. [94] introduce the LegalNERo corpus, a manually annotated resource for NER in the Romanian legal\ndomain, featuring 370 legal documents annotated with five general entity types: person, location, organization,\ntime expressions, and legal references. This corpus was developed to support both specific legal domain NER\ntasks and more general NER applications by enabling compatibility with existing general-purpose NER systems.\nThe corpus includes rich entity annotations, with legal references showing the highest token count per entity,\nindicating their complexity and length. The detailed annotation process, including inter-annotator agreement\nassessed by Cohen’s Kappa, and the subsequent mapping of entities to RDF format, highlights the corpus’s utility\nand precision for advancing NER research and applications within the legal domain.\nAu et al. [7] introduce the E-NER dataset, an annotated collection derived from the US SEC’s EDGAR filings,\ndesigned for legal NER. This dataset contains filings that are rich in text, such as quarterly reports (Form 10-Q)\nand significant event announcements (Form 8-K), from which sentences were extracted and annotated with seven\nnamed entity classes more tailored to legal content than those in the standard CoNLL dataset[115]. The entities\ninclude Person, Location, Organization, Government, Court, Business, and Legislation/Act, adjusting the CoNLL\nclasses to better suit legal documents. E-NER contains significantly longer sentences compared to CoNLL and\nincludes detailed annotations of financial entities from legal company filings.\nKalamkar et al. [54] present a comprehensive corpus aimed at enhancing legal NER, containing 46,545 entities\nacross 14 types identified in Indian High Court and Supreme Court judgments. This corpus, split into preamble and\njudgment sections, includes diverse entity types detailed in their legal NER taxonomy, such as court, petitioner,\nrespondent, and statute, among others. The training set, drawn from judgments between 1950 and 2017, features\n29,964 entities and the development and test sets, spanning 2018 to 2022. This dataset not only facilitates training\nand evaluation of NER models specific to the legal domain but also provides a structured framework for assessing\nthe performance of NER systems on legal texts. Their approach leverages a combination of manual annotation\nand ML techniques to ensure the precision of entity recognition in legal judgments.\n4.6\nLarge Legal Corpora\nThe foundational step in training an LLMs is the use of extensive corpora. For the development of a sophisticated\nLLMs that effectively addresses a wide range of legal NLP tasks, it is crucial to have access to large-scale legal\ncorpora. These corpora must meet several critical criteria to ensure their effectiveness and ethical utility. First,\nthey should be transparent in their sourcing and composition, allowing users to understand the origins and types\nof included data. Additionally, the privacy of individuals should be safeguarded, preventing any potential invasion\nof personal data. It is also imperative to minimize toxicity and bias within the corpora to promote fairness and\naccuracy in model outcomes. By following these principles, we can enhance the capabilities of LLMs in the legal\ndomain. This ensures they are both powerful and reliable tools for legal analysis. In this section, we will explore\nthe existing large legal corpora and online databases.\nZheng et al. [135] introduce the CaseHOLD dataset, a novel benchmark for evaluating NLP models in the\nlegal domain, designed to address the challenge of identifying the legal holdings from case texts. The dataset\ncontains over 53,000 multiple choice questions derived from U.S. case law citations, where each question requires\nthe identification of the correct holding from a set of potential answers. This task, simulating a fundamental\nlawyering skill taught in law school, involves contextual understanding and application of legal rules to factual\nsituations. CaseHOLD is aimed at enhancing model training by focusing on semantic matching and the ability\nto discern nuanced legal principles. The dataset is structured to provide a challenging yet accessible resource\nfor NLP researchers, with a clear focus on promoting deeper understanding and application of legal rules in\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n21\nautomated systems. It utilizes a format where a cited text serves as a prompt with five answer options—one\ncorrect holding and four closely related incorrect holdings—to refine the models’ abilities to accurately reflect\nlegal reasoning.\nChalkidis et al. [19] introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark,\na comprehensive suite of datasets aimed at assessing the capabilities of NLP models across various legal tasks.\nThe benchmark covers datasets such as ECtHR [15], SCOTUS7, EUR-LEX, LEDGAR [117], UNFAIR-ToS [67],\nand CaseHOLD [135], each chosen for its complexity, relevance, and need for legal expertise. These datasets\ncover a range of tasks from multi-label and multi-class classification to multiple-choice questions and are split\nchronologically into training, development, and test sets to provide standardized evaluation metrics. For instance,\nECtHR datasets focus on violations of European Convention of Human Rights provisions, SCOTUS database\nclassifies U.S. Supreme Court opinions by legal issues, EUR-LEX database involves labeling EU laws with EuroVoc\nconcepts, LEDGAR classifies provisions of U.S. contracts, UNFAIR-ToS identifies unfair terms in online service\nagreements, and CaseHOLD involves answering questions about legal rulings. This benchmark facilitates the\ntesting of NLP models, addressing the challenges of legal text comprehension and understanding required for\neffective application in the legal domain.\nChalkidis et al. [20] present FairLex, a benchmark suite consisting of four legal datasets—ECtHR [15], SCOTUS,\nFSCS, and CAIL [127]—that address the fairness of NLP applications across diverse legal jurisdictions and lan-\nguages, including English, German, French, Italian, and Chinese. These datasets, curated from European Council,\nUSA, Switzerland, and China, cover various legal tasks such as judgment prediction, issue area classification,\nand crime severity prediction, aiming to test the performance and fairness of LMs in recognizing and classifying\nlegal texts. FairLex focuses on ensuring demographic, regional, and legal topic fairness by analyzing attributes\nlike gender, age, region of origin, and legal areas within cases. Each dataset in FairLex provides a substantial\nnumber of cases, systematically divided into training, development, and test sets, and includes detailed attributes\nlike the defendant state in ECtHR, decision direction in SCOTUS, legal areas in FSCS, and demographic details\nin CAIL. The CAIL dataset from China contains over a million cases focusing on criminal law, annotated with\ndemographics and regional classifications, which are used to explore the crime severity prediction task.\nHenderson et al. [46] introduce the ‘Pile of Law’, the first and an important large corpus in the legal domain,\ncontaining a 256GB dataset of open-source English-language legal and administrative data. This dataset includes\ncontracts, court opinions, legislative records, and administrative rules, curated to explore data sanitization norms\nacross legal and administrative settings and serve as a tool for pre-training legal-domain LMs. They emphasize\nthe legal norms governing privacy and toxicity filtering, detailing how the dataset reflects these norms through\nbuilt-in filtering mechanisms in the collected data, which include court filings, legal analyses, and government\npublications. By analyzing how legal and administrative entities handle sensitive information and potentially\noffensive content, the paper provides actionable insights for researchers to improve content filtering practices\nbefore pre-training LLMs, thereby enhancing the ethical use of NLP in legal applications.\nRabelo et al. [95] summarize the 8th Competition on Legal Information Extraction and Entailment (COLIEE\n2021), which featured five tasks across case and statute law, engaging participants from various teams to apply\ndiverse NLP approaches. The competition tasks included case law retrieval and entailment, as well as statute law\nretrieval and entailment with and without prior retrieved data. Specifically, Task 1 focused on extracting relevant\nsupporting cases from a corpus, while Task 2 involved identifying paragraphs from cases that entail a given new\ncase fragment. For statute law, Tasks 3 and 4 entailed retrieving and answering questions based on civil code\nstatutes, with Task 5 challenging participants to answer without pre-retrieved statutes. The datasets used varied\nin complexity, from 4415 case files in Task 1 with a need to identify noticed cases without relying on citations,\n7https://www.supremecourt.gov\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n22\n•\nAriai and Demartini\nto the civil code-based Tasks 3, 4, and 5 which adapted to recent legal revisions in Japanese law and excluded\nuntranslated parts, reflecting the ongoing evolution and challenge in legal NLP applications.\nBarale et al. [9] present AsyLex, a pioneering dataset tailored for Refugee Law applications, featuring 59,112\ndocuments from Canadian refugee status determinations spanning from 1996 to 2022. This dataset is designed to\nenhance the capabilities of NLP models in legal research by providing 19,115 gold-standard human-annotated and\n30,944 inferred labels for entity extraction and LJP. Key contributions include anonymizing decision documents,\nemploying a robust annotation methodology, and creating datasets for specific NLP tasks like entity extraction\nand judgment prediction. This rich corpus, with detailed annotations across 22 categories, supports complex legal\nNLP tasks, thereby filling the gap in resources for the legal domain.\nNiklaus et al. [86] introduce LEXTREME, a multilingual benchmark specifically designed to evaluate LMs on\nlegal NLP tasks, a critical step given the unique challenges of legal language. Surveying legal NLP literature from\n2010 to 2022, they curate 11 datasets spanning 24 languages and cover a variety of legal domains, employing\ndatasets that only involve human-annotated texts or those with annotations derived through clear methodological\nframeworks. They introduce two aggregate scores to facilitate fair comparison across models: the dataset aggregate\nscore and the language aggregate score, revealing a performance correlation with model size on LEXTREME. The\nbenchmark consists of three task types: Single Label Text Classification, Multi Label Text Classification, and NER,\nusing existing splits for training, validation, and testing when available, or creating random splits otherwise. This\neffort marks a significant advancement in testing NLP capabilities across a diverse range of legal documents and\nlanguages.\nPark and James [91] explore the creation of a Natural Language Inference dataset within the legal domain,\nfocusing on criminal court verdicts in Korean. Their methodology includes the innovative use of adversarial\nhypothesis generation to challenge annotators and enhance the robustness of the dataset, supported by visual\ntools for hypothesis network construction. The data collection involves extracting context from verdicts and\naugmenting it using Easy Data Augmentation [123] techniques and round-trip translation to generate a dataset\nfor training and testing Natural Language Inference models. The study highlights issues such as annotators’\nlimited domain knowledge and challenges in handling long contexts but provides solutions like targeted data\ncollection and the use of gamification to boost annotator engagement and productivity.\nGoebel et al. [44] summarize the 10th Competition on Legal Information Extraction and Entailment (COLIEE\n2023), featuring four tasks across case and statute law with participation from ten different teams engaging in\nmultiple tasks. Task 1 involves legal case retrieval, requiring participants to extract supporting cases from a\ncorpus, and Task 2 focuses on legal case entailment, identifying paragraphs that entail aspects of a new case. Task\n3 and 4, based on Japanese Civil Code statutes from the bar exam, involve retrieving relevant articles and verifying\nstatements, respectively. The competition leverages a dataset of over 5,700 case law files and introduces new\nquery cases and test questions sourced from recent bar exams, testing the efficacy of different teams’ approaches\nin handling complex legal texts and hypotheses in a controlled competitive environment.\nÖstling et al. [142] introduce the Cambridge Law Corpus (CLC), a comprehensive legal dataset featuring\n258,146 cases from UK courts, dating from the 16th century to the present. The corpus includes raw text and\nmetadata across various court types, and is structured in XML format for ease of use and annotated for case\noutcomes in a subset of 638 cases. Additionally, the CLC is supported by a Python library for data manipulation\nand ML applications.\nNiklaus et al. [87] present the MultiLegalPile, the largest open-source multilingual legal corpus available,\ntotaling 689GB and spanning 17 jurisdictions across 24 languages. This extensive dataset is designed to facilitate\ntraining of LLMs within the legal domain, featuring diverse legal text types including case law, legislation, and\ncontracts, predominantly in English due to the integration of the ‘Pile of Law’ [46] dataset. Through careful\nregex-based filtering from the mC4 corpus and manual reviews, the team ensures high precision in legal content\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n23\nselection. The corpus, efficiently compressed using XZ and formatted in JSONL, supports comprehensive NLP\nresearch and modeling, emphasizing its broad applicability in advancing legal AI technologies.\n5\nLEGAL LANGUAGE MODELS AND METHODS FOR LEGAL DOMAIN ADAPTATION\nIn the fast-moving field of NLP, LLMs have become a key tool for processing and understanding large amounts of\nunstructured text data. These models, initially trained on broad datasets like Wikipedia, have shown great skill\nacross various language tasks. Building on this success, the legal technology community is increasingly interested\nin using these powerful models for Legal NLP applications. This involves adapting these general-domain models\nto legal texts and further training them on specialized legal documents. Such efforts aim to reduce the domain\ngap and customize the models to better understand the complex language used in legal documents. In this section,\nwe will explore how these models are being adapted and applied within the legal domain to enhance Legal NLP\napplications.\nIn this section, following the methodology of this survey, we studied all peer-reviewed LMs or methods.\nHowever, due to the significant challenges present in the legal domain, there are many legal LMs that have not\nundergone peer review. Given the scarcity of adequate peer-reviewed resources, our research has focused on\nthe investigation of, in order of priority, the peer-reviewed sources, then the most well-known and widely used\nnon-peer-reviewed legal LMs. Despite their lack of formal peer review, these models have gained considerable\nattention and usage in the field.\n5.1\nLanguage Models\nChalkidis et al. [18] present an in-depth analysis of applying BERT [29], a pre-trained language model, in the legal\ndomain, showcasing the need for domain-specific adaptation to enhance performance on legal NLP tasks. They\nexplore three strategies: using standard BERT directly, further pre-training on legal corpora, and pre-training\nfrom scratch with legal-specific data. Their study found that both further pre-training and pre-training from\nscratch generally outperform the use of BERT directly. They introduce legal-bert, a specialized family of\nmodels optimized for legal text, which includes versions for varied computational capacities and demonstrates\ncompetitive performance with a lower environmental impact.\nXiao et al. [126] introduce Lawformer, a Longformer-based [10] language model adapted for Chinese legal texts,\ndesigned to handle extensive document lengths common in legal data. Recognizing the limitation of standard\nPLMs with shorter token capacities, Lawformer employs a unique combination of sliding window, dilated sliding\nwindow, and global attention mechanisms to efficiently process long texts, making it suitable for legal AI tasks like\njudgment prediction and LQA. Pre-trained on a vast corpus of Chinese legal documents segmented into criminal\nand civil cases, Lawformer integrates complex sequential dependencies across tokens using these attention\ntechniques, enhancing model performance for legal-specific tasks.\nIn the development of specialized NLP tools for Arabic legal texts, a model specifically tailored to the unique\nlinguistic features of Arabic jurisprudence was designed, introducing AraLegal-BERT [1] midway through this\ninnovation. This model enhances NLP applications within the legal field by adapting BERT [29] technology to\nArabic’s specific content needs, involving pre-training BERT from scratch using a broad range of legal documents,\nincluding legislative materials and contracts.\nColombo et al. [25] introduce SaulLM-7B, a novel LLM specifically designed for legal text comprehension\nand generation, built on the 7 billion parameter Mistral [51] architecture. This model is trained on an extensive\nEnglish legal corpus, designed to meet the unique challenges of legal syntax and terms. SaulLM-7B uses a two-tier\ntraining approach: continued pre-training on a carefully curated 30 billion token legal dataset and an innovative\ninstruction fine-tuning method, incorporating both generic and legal-specific instructions to enhance the model’s\nperformance on legal tasks.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n24\n•\nAriai and Demartini\nShi et al. [104] develop Legal-LM, a specialized language model tailored for Chinese legal consulting, enhanced\nwith a KG to address domain-specific challenges such as data veracity and non-expert user interaction. The\nframework involves several steps: extensive pre-training on a rich corpus of legal texts integrated with a legal\nKG, keyword extraction and Direct Preference Optimization to refine responses, and the use of an external legal\nknowledge base for data retrieval and response validation. This multi-faceted approach ensures that Legal-LM\nnot only comprehends complex legal language but also generates precise and user-aligned legal advice.\n5.2\nMethods for Improving In-Domain Adaptability of Legal Language Models\nLi et al. [63] explore a novel adaptation of LMs for the legal domain by integrating domain-specific unsupervised\ndata from public legal forums to optimize prefix domain adaptation, a parameter-efficient learning approach that\ntrains only about 0.1% of the model’s parameters. They introduce a training methodology where a deep prompt\nis specifically tuned using a domain-adapted prefix from legal forums and then utilized in various legal tasks,\ndemonstrating improved few-shot performance compared to full model tuning methods like legal-bert [18].\nThis approach significantly reduces computational overhead while maintaining or exceeding performance metrics\nacross multiple legal tasks, suggesting an efficient and scalable model for legal NLP applications.\nMamakas et al. [75] explore strategies for adapting pre-trained transformers to cope with the challenges of long\nlegal texts within the LexGLUE benchmark, focusing on extending input capabilities and enhancing efficiency.\nThey modify Longformer [10], originally extending up to 4,096 sub-words, to process up to 8,192 sub-words by\nreducing local attention window size and incorporating a global token at the end of each paragraph to facilitate\ninformation flow across longer texts. Additionally, they adapt legal-bert [18] to employ TF-IDF representations\nto manage longer documents effectively, introducing variants like TF-IDF-SRT-LegalBERT, which deduplicates\nand sorts sub-words by TF-IDF scores, and TF-IDF-EMB-LegalBERT, which incorporates a TF-IDF embedding\nlayer. These adaptations aim to combine the robust capabilities of transformers with the practical requirements of\nhandling extensive legal documents, surpassing the performance of traditional linear classifiers while maintaining\ncomputational efficiency.\n6\nOPEN RESEARCH CHALLENGES\nDespite researchers’ efforts in the this interdisciplinary field and extensive advancements in AI techniques, Open\nResearch Challenges (ORCs) still exist. In this section, we identify the ORCs and provide advice to overcome\nthese challenges.\nORC1: Bias and Fairness. Bias and fairness are crucial concerns in the field of AI, especially at the intersection\nwith the legal domain where decisions can deeply impact individuals’ lives. The scarcity of unbiased data in\nlegal domains such as case law complicates the training of AI models, as these models often learn from historical\ndecisions that may reflect existing human biases [33, 114]. This reliance on biased datasets can lead to unfair and\nbiased outcomes in classification and prediction tasks. Addressing these issues is critical to ensure that AI-driven\nlegal decisions uphold the standards of impartiality and fairness required for justice.\nORC2: Interpretability and Explainability. Interpretability and explainability are crucial across various applications\nin legal NLP, yet these aspects remain underexplored in many studies. The ability to trace and comprehend\nthe decision-making process of AI systems is essential for identifying and mitigating biases. Transparent and\nunderstandable AI systems help build trust and ensure they are used responsibly, which is particularly important\nin legal contexts where decisions can significantly impact people’s lives. Improving these aspects of AI models is\nnecessary to their ethical use, ensuring they meet the high standards of fairness required in legal proceedings.\nORC3: Transparent Annotation. Transparently annotated datasets are rare in the field of legal NLP. Often, studies\nmention the involvement of expert annotators for tasks such as classification, question answering, or prediction,\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n25\nbut fail to provide details about the annotators’ backgrounds or the specific annotation processes used. To develop\nunbiased legal NLP systems, it is necessary to document the dataset creation process thoroughly. This includes\nproviding detailed descriptions of the annotation procedures and the qualifications of the annotators involved,\nwhich is essential for ensuring the reliability and fairness of the systems trained on these datasets. Researchers\nneed to prioritize transparency to build trust and allow for effective evaluation in the legal NLP field.\nORC4: Multilingual Capabilities. In legal NLP, enhancing multilingual capabilities remains an underdeveloped\narea. While efforts like MultiLegalPile [87] have begun to address this, there remains a gap in research for\nmany languages, including but not limited to Persian and Arabic. These limitations significantly restricts the\napplication of legal NLP across diverse legal systems worldwide, which is important for broader accessibility.\nMultilingual capabilities introduce unique challenges for legal NLP models, primarily due to the distinct linguistic\nstructures of each language, which often require extensive fine-tuning to ensure accuracy and relevancy in legal\ncontexts. Furthermore, each legal system possesses its own set of terms and document standards, which can vary\ndramatically from one language to another. Therefore, expanding research into these and other underserved\nlanguages is essential for making NLP tools universally applicable and effective.\nORC5: Ontology. The use of ontologies in the legal domain is relatively sparse, yet it holds considerable potential\nto enhance the robustness of AI methodologies. Ontology or KG can also enable the AI models to draw accurate\ninferences regarding the relationship between the terms and thereby better understand and process complicated\nlegal texts. This approach could advance the capability of AI systems to handle complex legal reasoning and\ndecision-making processes. However, utilizing ontology in legal NLP faces unique challenges. The complexity of\nlegal language and the concept of ‘open texture’, where the meaning of legal terms can evolve over time, complicate\nthe creation of static ontological models [81]. Legal ontologies must be dynamic, reflecting changes in law and its\ninterpretation over time. Additionally, the integration of real-world and legal concepts within ontologies presents\nfurther complexity, as it requires accommodating both legal terms and their relevant real-world contexts [81].\nORC6: Pre-processing Legal Text. Pre-processing legal texts is challenging due to the distinct nature of legal docu-\nments. Existing legal corpora are often contained of raw texts that require extensive cleaning and transformation\nto become suitable for ML models. Additionally, legal documents can include complex nested structures, like\nclauses within clauses, and cross-references to other legal cases, statutes, or provisions, making it difficult to\nbreak them into coherent units for analysis. These challenges of legal documents make it impractical to directly\nfine-tune LMs on these raw datasets without substantial pre-processing. This requirement complicates the use of\nlarge legal datasets, making them hard to convert into formats that NLP models can readily process and learn\nfrom. Without addressing these specific complexities, fine-tuning LMs on raw legal data becomes impractical,\nlimiting the effectiveness of legal NLP applications.\nORC7: Reinforcement Learning from Human Feedback (RLHF). The use of RLHF within the legal domain is notably\nscarce. Currently, there is only one peer-reviewed work [83] available that explores this approach. This indicates\na significant opportunity for research and development in this area, as RLHF could potentially enhance NLP’s\ncapability to learn and make decisions based on complex legal data under human guidance. Further exploration\ninto this method could lead to more responsive and adaptable legal NLP systems. However, due to the complex\nnature of legal reasoning and the need for accurate legal knowledge in human feedback phase, RLHF’s integration\ninto legal NLP poses some challenges. Therefore, on the human feedback side, legal experts such as lawyers and\njudges must provide guidance to ensure the AI accurately interprets and applies complex legal concepts.\nORC8: Expanding Legal Domain Coverage. There is a noticeable gap in the research across various areas of the legal\ndomain, including Intellectual Property, Criminal Law, Banking Law, Family Law, and Human Rights Law. These\nfields have seen limited exploration across all legal NLP tasks, such as LQA and other applications. Expanding\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n26\n•\nAriai and Demartini\nresearch into these areas is essential for developing comprehensive legal automated systems that can provide\ntailored solutions and insights highly relevant to these sectors of law.\nORC9: Small Language Models (SLMs). Research into SLMs specific to the legal domain is notably absent. Ad-\ndressing this gap could lead to more efficient, resource-conscious solutions that still maintain high performance\nin legal text processing and analysis. The development of SLMs tailored for legal applications could revolutionize\nthe accessibility and scalability of legal NLP tools.\nORC10: Domain-Specific Efficient Fine-Tuning. Domain-specific efficient fine-tuning within the legal field is an\nunderexplored area, with only two known studies [63, 72] addressing it. Legal texts consist of complex structures\nand specialized words that standard LMs may not capture without significant adaptation. Additionally, the legal\ndomain cover a vast array of document types, such as case law, statutes, and contracts, each requiring tailored\napproaches for effective model application. This diversity makes it imperative to develop fine-tuning strategies\nthat do not only adapt a model generally but rather tailor it to understand the differences between these document\ntypes. The majority of existing approaches involve fine-tuning the entire model, which can be resource-intensive.\nMore focused research could enable fine-tuning of legal LMs using fewer resources, enhancing the efficiency of\ndeploying these models in practice.\nORC11: Legal Logical Reasoning. Complex legal logical reasoning remains a significant challenge in LJP, particu-\nlarly in predicting prison terms. Current state-of-the-art methods struggle to achieve high accuracy in this area.\nThis highlights a clear need for enhanced approaches that can effectively handle the complex of legal reasoning.\nORC12: Legal Named Entity Recognition. Legal NER focuses on specific challenges such as disambiguating titles,\nresolving nested entities, addressing co-references, managing lengthy texts, and processing machine-inaccessible\nPDFs. Despite its critical role in understanding and structuring legal documents, there is limited research in this\narea, as observed from Fig. 1.\nORC13: Stochastic Parrots. The concept of “Stochastic Parrots” pertains particularly to LLMs. It shows the concern\nthat these models often do not truly understand language but merely mimic human patterns. This mimicry can\nlead to unreliable outcomes, especially in critical legal situations, if the models are not trained on high-quality,\nunbiased datasets. The risk is notably significant in LJP, where training on biased or unfair data could lead to\nirreversible outcomes, as discussed in Bender et al. [11]’s work on the limitations of LLMs. This underscores the\nimportance of ensuring that LLMs are trained responsibly to avoid perpetuating or amplifying existing biases in\nlegal decisions.\nORC14: Retrieval-Augmented Generation. In the legal domain, where documents are usually lengthy, often contain\ncross-references, and present a variety of complex linguistic structures, LLMs can sometimes generate hallucina-\ntory responses when faced with the task of generating accurate answers. RAG systems offer a promising solution\nto these challenges. RAG can mitigate issues such as the natural limitations of LLMs concerning maximum input\nlengths, where even extended limits may fall short due to the excessive length of many legal documents. This\napproach not only improves the model’s response quality but also its relevance and contextual appropriateness by\nincorporating more of the document’s content into the decision-making process. However, the integration of RAG\ninto the legal domain introduces unique challenges, such as managing documents from multiple jurisdictions,\nensuring temporal relevance, addressing multilingual issues, and overcoming biases in the retrieval phase. These\nchallenges must be addressed in future research when applying RAG in the legal domain.\nORC15: Automated Legal Assistance System. To decrease human error and the costs of legal services, there’s a\nneed for a comprehensive automated legal assistance system. This system should span all tasks within the legal\nNLP field, from question answering to judgment prediction, and cater to different legal specializations like civil\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n27\nTable 3. Summary of existing ORCs in each area.\nOpen Research Challenges\nLQA\nLJP\nLTC\nLDS\nLegal NER\nLLMs\nCorpora\nORC1: Bias and Fairness\n✓\n✓\n✓\n–\n–\n✓\n✓\nORC2: Interpretability and Explainability\n✓\n✓\n✓\n–\n–\n✓\n–\nORC3: Transparent Annotation\n✓\n✓\n✓\n✓\n✓\n–\n✓\nORC4: Multilingual Capabilities\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nORC5: Ontology\n✓\n✓\n✓\n✓\n✓\n✓\n–\nORC6: Pre-processing Legal Text\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nORC7: RLHF\n✓\n✓\n✓\n✓\n–\n✓\n–\nORC8: Expanding Legal Domain Coverage\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nORC9: SLMs\n–\n–\n–\n–\n–\n✓\n–\nORC10: Domain-Specific Efficient Fine-Tuning\n✓\n✓\n✓\n✓\n–\n✓\n–\nORC11: Legal Logical Reasoning\n✓\n✓\n✓\n–\n–\n✓\n–\nORC12: Legal NER\n–\n–\n–\n–\n✓\n–\n–\nORC13: Stochastic Parrots\n–\n–\n–\n–\n–\n✓\n–\nORC14: RAG\n✓\n✓\n–\n–\n–\n✓\n–\nORC15: Automated Legal Assistance System\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nand financial law, across multiple languages from English to Persian. Developing an accurate LLM pre-trained on\na vast, diverse dataset free from biases and unfairness is crucial. This ensures that the automated legal services\ncan reliably and equitably address a wide range of legal issues.\nSummary. Table 3 illustrates the connections between ORCs and discussed areas. A direct relationship is marked\nwith an ✓, and otherwise with a –. As shown, most ORCs are related to LJP, LQA, LTC, and LLMs, indicating\nmore extensive research fields in these areas.\n7\nCONCLUSION\nAdvances in AI and NLP have improved Legal NLP techniques and models. These improvements help better meet\nthe needs of laypersons in legal matters and ease the workload of legal professionals. This survey provides a\ncomprehensive overview of the advancements in NLP techniques used in the legal domain. Additionally, we\ndiscussed the unique characteristics of legal documents. We also reviewed existing datasets and LLMs tailored for\nthe legal domain. Legal NER research spans multiple languages and utilizes diverse methods, from rule-based\nto BERT-based models. LDS has largely focused on extractive and abstractive methods, including TF-IDF and\ntransformer-based models. In LTC, multi-class classification tasks dominate, with deep learning architectures like\nCNNs and Bi-LSTMs widely used. LJP primarily focuses on Chinese datasets with deep learning approaches like\nCNNs. LQA often leverages information retrieval techniques such as BM25, with a significant focus on statutory\nlaw. Finally, we explored key ORCs, such as the need for domain-specific fine-tuning strategies, addressing bias\nand fairness in legal datasets, and the importance of interpretability and explainability. Other challenges include\nthe development of more robust pre-processing techniques, handling multilingual capabilities, and integrating\nontology-based methods for more accurate legal reasoning.\nREFERENCES\n[1] Muhammad Al-qurishi, Sarah Alqaseemi, and Riad Souissi. 2022. AraLegal-BERT: A pretrained language model for Arabic Legal text.\nIn Proceedings of the Natural Legal Language Processing Workshop 2022. Association for Computational Linguistics, Abu Dhabi, United\nArab Emirates (Hybrid), 338–344. https://doi.org/10.18653/v1/2022.nllp-1.31\n[2] Intisar Almuslim and Diana Inkpen. 2022. Legal Judgment Prediction for Canadian Appeal Cases. In 2022 7th International Conference\non Data Science and Machine Learning Applications (CDMA). IEEE, Riyadh, Saudi Arabia, 163–168. https://doi.org/10.1109/CDMA54072.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n28\n•\nAriai and Demartini\n2022.00032\n[3] AWS Amazon. [n. d.]. What are Transformers in Artificial Intelligence? Retrieved July 24, 2024 from https://aws.amazon.com/what-\nis/transformers-in-artificial-intelligence\n[4] Dang Hoang Anh, Dinh-Truong Do, Vu Tran, and Nguyen Le Minh. 2023. The Impact of Large Language Modeling on Natural Language\nProcessing in Legal Texts: A Comprehensive Survey. In 2023 15th International Conference on Knowledge and Systems Engineering (KSE).\nIEEE, Hanoi, Vietnam, 1–7. https://doi.org/10.1109/KSE59128.2023.10299488\n[5] Arian Askari, Suzan Verberne, and Gabriella Pasi. 2022. Expert Finding in Legal Community Question Answering. In Advances in\nInformation Retrieval: 44th European Conference on IR Research (ECIR 2022), Matthias Hagen, Suzan Verberne, Craig Macdonald, Christin\nSeifert, Krisztian Balog, Kjetil Nørvåg, and Vinay Setty (Eds.). Springer International Publishing, Berlin, Heidelberg, 22–30.\n[6] Arian Askari, Zihui Yang, Zhaochun Ren, and Suzan Verberne. 2024. Answer Retrieval in Legal Community Question Answering. In\nAdvances in Information Retrieval: 46th European Conference on Information Retrieval (ECIR 2024), Nazli Goharian, Nicola Tonellotto,\nYulan He, Aldo Lipani, Graham McDonald, Craig Macdonald, and Iadh Ounis (Eds.). Springer Nature Switzerland, Berlin, Heidelberg,\n477–485.\n[7] Ting Wai Terence Au, Vasileios Lampos, and Ingemar Cox. 2022. E-NER — An Annotated Named Entity Recognition Corpus of Legal\nText. In Proceedings of the Natural Legal Language Processing Workshop 2022. Association for Computational Linguistics, Abu Dhabi,\nUnited Arab Emirates (Hybrid), 246–255. https://doi.org/10.18653/v1/2022.nllp-1.22\n[8] Purbid Bambroo and Aditi Awasthi. 2021. LegalDB: Long DistilBERT for Legal Document Classification. In 2021 International Conference\non Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT). IEEE, 1–4.\nhttps://doi.org/10.1109/\nICAECT49130.2021.9392558\n[9] Claire Barale, Mark Klaisoongnoen, Pasquale Minervini, Michael Rovatsos, and Nehal Bhuta. 2023. AsyLex: A Dataset for Legal\nLanguage Processing of Refugee Claims. In Proceedings of the Natural Legal Language Processing Workshop 2023, Daniel Preot,iuc-Pietro,\nCatalina Goanta, Ilias Chalkidis, Leslie Barrett, Gerasimos Spanakis, and Nikolaos Aletras (Eds.). Association for Computational\nLinguistics, Singapore, 244–257. https://doi.org/10.18653/v1/2023.nllp-1.24\n[10] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv:2004.05150 [cs.CL]\n[11] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can\nLanguage Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event,\nCanada) (FAccT ’21). Association for Computing Machinery, New York, NY, USA, 610–623. https://doi.org/10.1145/3442188.3445922\n[12] Paheli Bhattacharya, Kaustubh Hiware, Subham Rajgaria, Nilay Pochhi, Kripabandhu Ghosh, and Saptarshi Ghosh. 2019. A Comparative\nStudy of Summarization Algorithms Applied to Legal Case Judgments. Advances in Information Retrieval (2019), 413–428.\n[13] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are\nfew-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada)\n(NIPS ’20). Curran Associates Inc., Red Hook, NY, USA, Article 159, 25 pages.\n[14] Marius Büttner and Ivan Habernal. 2024. Answering legal questions from laymen in German civil law system. In Proceedings of the 18th\nConference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Yvette Graham and Matthew\nPurver (Eds.). Association for Computational Linguistics, St. Julian’s, Malta, 2015–2027. https://aclanthology.org/2024.eacl-long.122\n[15] Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural Legal Judgment Prediction in English. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics, Anna Korhonen, David Traum, and Lluís Màrquez (Eds.).\nAssociation for Computational Linguistics, Florence, Italy, 4317–4323. https://doi.org/10.18653/v1/P19-1424\n[16] Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2019. Extreme Multi-Label\nLegal Text Classification: A Case Study in EU Legislation. In Proceedings of the Natural Legal Language Processing Workshop 2019,\nNikolaos Aletras, Elliott Ash, Leslie Barrett, Daniel Chen, Adam Meyers, Daniel Preotiuc-Pietro, David Rosenberg, and Amanda Stent\n(Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 78–87. https://doi.org/10.18653/v1/W19-2209\n[17] Ilias Chalkidis, Manos Fergadiotis, and Ion Androutsopoulos. 2021. MultiEURLEX - A multi-lingual and multi-label legal document\nclassification dataset for zero-shot cross-lingual transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational\nLinguistics, Online and Punta Cana, Dominican Republic, 6974–6996. https://doi.org/10.18653/v1/2021.emnlp-main.559\n[18] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT: The\nMuppets straight out of Law School. In Findings of the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn, Yulan He,\nand Yang Liu (Eds.). Association for Computational Linguistics, Online, 2898–2904. https://doi.org/10.18653/v1/2020.findings-emnlp.261\n[19] Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Katz, and Nikolaos Aletras. 2022. LexGLUE:\nA Benchmark Dataset for Legal Language Understanding in English. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n29\nComputational Linguistics, Dublin, Ireland, 4310–4330. https://doi.org/10.18653/v1/2022.acl-long.297\n[20] Ilias Chalkidis, Tommaso Pasini, Sheng Zhang, Letizia Tomada, Sebastian Schwemer, and Anders Søgaard. 2022. FairLex: A Multi-\nlingual Benchmark for Evaluating Fairness in Legal Text Processing. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for\nComputational Linguistics, Dublin, Ireland, 4389–4406. https://doi.org/10.18653/v1/2022.acl-long.301\n[21] Andong Chen, Feng Yao, Xinyan Zhao, Yating Zhang, Changlong Sun, Yun Liu, and Weixing Shen. 2023. EQUALS: A Real-world\nDataset for Legal Question Answering via Reading Chinese Laws. In Proceedings of the Nineteenth International Conference on\nArtificial Intelligence and Law (Braga, Portugal) (ICAIL ’23). Association for Computing Machinery, New York, NY, USA, 71–80.\nhttps://doi.org/10.1145/3594536.3595159\n[22] Shijie Chen, Yu Zhang, and Qiang Yang. 2024. Multi-Task Learning in Natural Language Processing: An Overview. ACM Comput. Surv.\n56, 12, Article 295 (jul 2024), 32 pages. https://doi.org/10.1145/3663363\n[23] Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, Julian McAuley, Linda Petzold,\nand William Yang Wang. 2024. A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law.\narXiv:2405.01769 [cs.CL]\n[24] Odysseas S. Chlapanis, Ion Androutsopoulos, and Dimitrios Galanis. 2024. Archimedes-AUEB at SemEval-2024 Task 5: LLM explains\nCivil Procedure. arXiv:2405.08502 [cs.CL]\n[25] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera Lú-\ncia Raposo, Sofia Morgado, and Michael Desa. 2024. SaulLM-7B: A pioneering Large Language Model for Law. arXiv:2403.03883 [cs.CL]\n[26] Junyun Cui, Xiaoyu Shen, and Shaochun Wen. 2023. A Survey on Legal Judgment Prediction: Datasets, Metrics, Models and Challenges.\nIEEE Access 11 (2023), 102050–102071. https://doi.org/10.1109/ACCESS.2023.3317083\n[27] Franck Dernoncourt, Ji Young Lee, and Peter Szolovits. 2017. NeuroNER: an easy-to-use program for named-entity recognition based\non neural networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\nLucia Specia, Matt Post, and Michael Paul (Eds.). Association for Computational Linguistics, Copenhagen, Denmark, 97–102. https:\n//doi.org/10.18653/v1/D17-2017\n[28] Aniket Deroy and Subhankar Maity. 2023. Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?\narXiv:2312.00554 [cs.CL]\n[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.).\nAssociation for Computational Linguistics, Minneapolis, Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423\n[30] João Dias, Pedro A. Santos, Nuno Cordeiro, Ana Antunes, Bruno Martins, Jorge Baptista, and Carlos Gonçalves. 2022. State of the Art\nin Artificial Intelligence applied to the Legal Domain. arXiv:2204.07047 [cs.CL]\n[31] Yue Dong, Andrei Mircea, and Jackie Chi Kit Cheung. 2021. Discourse-Aware Unsupervised Summarization for Long Scientific\nDocuments. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main\nVolume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 1089–1102.\nhttps://doi.org/10.18653/v1/2021.eacl-main.93\n[32] Christopher Dozier, Ravikumar Kondadadi, Marc Light, Arun Vachher, Sriharsha Veeramachaneni, and Ramdev Wudali. 2010. Named\nEntity Recognition and Resolution in Legal Text. In Semantic Processing of Legal Texts: Where the Language of Law Meets the Law of\nLanguage, Enrico Francesconi, Simonetta Montemagni, Wim Peters, and Daniela Tiscornia (Eds.). Springer Berlin Heidelberg, Berlin,\nHeidelberg, 27–43. https://doi.org/10.1007/978-3-642-12837-0_2\n[33] Gary Edmond and Kristy A Martire. 2019. Just cognition: scientific research on bias and some implications for legal procedure and\ndecision-making. The modern law review 82, 4 (2019), 633–664.\n[34] Ahmed Elnaggar, Christoph Gebendorfer, Ingo Glaser, and Florian Matthes. 2018. Multi-Task Deep Learning for Legal Document\nTranslation, Summarization and Multi-Label Classification. In Proceedings of the 2018 Artificial Intelligence and Cloud Computing\nConference (Tokyo, Japan) (AICCC ’18). Association for Computing Machinery, New York, NY, USA, 9–15. https://doi.org/10.1145/\n3299819.3299844\n[35] Günes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of\nartificial intelligence research 22 (2004), 457–479.\n[36] Atefeh Farzindar. 2004. Atefeh Farzindar and Guy Lapalme,’LetSum, an automatic Legal Text Summarizing system’in T. Gordon (ed.),\nLegal Knowledge and Information Systems. Jurix 2004: The Seventeenth Annual Conference. Amsterdam: IOS Press, 2004, pp. 11-18..\nIn Legal knowledge and information systems: JURIX 2004, the seventeenth annual conference, Vol. 120. IOS Press, 11.\n[37] Yi Feng, Chuanyi Li, and Vincent Ng. 2022. Legal Judgment Prediction via Event Extraction with Constraints. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 648–664. https://doi.org/10.18653/v1/2022.acl-long.48\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n30\n•\nAriai and Demartini\n[38] Pavlos Fragkogiannis, Martina Forster, Grace E. Lee, and Dell Zhang. 2023. Context-Aware Classification of Legal Document Pages.\nIn Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan)\n(SIGIR ’23). Association for Computing Machinery, New York, NY, USA, 3285–3289. https://doi.org/10.1145/3539618.3591839\n[39] Debasis Ganguly, Jack G. Conrad, Kripabandhu Ghosh, Saptarshi Ghosh, Pawan Goyal, Paheli Bhattacharya, Shubham Kumar Nigam,\nand Shounak Paul. 2023. Legal IR and NLP: The History, Challenges, and State-of-the-Art. In European Conference on Information Retrieval\n(ECIR) (Advances in Information Retrieval). Springer-Verlag, Berlin, Heidelberg, 331–340. https://doi.org/10.1007/978-3-031-28241-6_34\n[40] Daphne Gelbart and JC Smith. 1991. Flexicon, a new legal information retrieval system. Can. L. Libr. 16 (1991), 9.\n[41] Dephne Gelbart and J. C. Smith. 1991. Beyond boolean search: FLEXICON, a legal tex-based intelligent system. In Proceedings of the 3rd\nInternational Conference on Artificial Intelligence and Law (Oxford, England) (ICAIL ’91). Association for Computing Machinery, New\nYork, NY, USA, 225–234. https://doi.org/10.1145/112646.112674\n[42] Joseph Gesnouin, Yannis Tannier, Christophe Gomes Da Silva, Hatim Tapory, Camille Brier, Hugo Simon, Raphael Rozenberg, Hermann\nWoehrel, Mehdi El Yakaabi, Thomas Binder, Guillaume Marie, Emilie Caron, Mathile Nogueira, Thomas Fontas, Laure Puydebois,\nMarie Theophile, Stephane Morandi, Mael Petit, David Creissac, Pauline Ennouchy, Elise Valetoux, Celine Visade, Severine Balloux,\nEmmanuel Cortes, Pierre-Etienne Devineau, Ulrich Tan, Esther Mac Namara, and Su Yang. 2024. LLaMandement: Large Language\nModels for Summarization of French Legislative Proposals. arXiv:2401.16182 [cs.CL]\n[43] John Gibbons and M. Teresa Turell. 2008. Dimensions of Forensic Linguistics (1 ed.). AILA Applied Linguistics Series, Vol. 5. John\nBenjamins Publishing Company, Netherlands. 1–317 pages.\n[44] Randy Goebel, Yoshinobu Kano, Mi-Young Kim, Juliano Rabelo, Ken Satoh, and Masaharu Yoshioka. 2024. Overview and Discussion of\nthe Competition on Legal Information, Extraction/Entailment (COLIEE) 2023. The Review of Socionetwork Strategies 18, 1 (2024), 27–47.\n[45] S. Georgette Graham, Hamidreza Soltani, and Olufemi Isiaq. 2023. Natural language processing for legal document review: categorising\ndeontic modalities in contracts. Artificial Intelligence and Law (2023). https://doi.org/10.1007/s10506-023-09379-2\n[46] Peter Henderson, Mark Krass, Lucia Zheng, Neel Guha, Christopher D Manning, Dan Jurafsky, and Daniel Ho. 2022. Pile of Law:\nLearning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. In Advances in Neural Information\nProcessing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., Red\nHook, NY, USA, 29217–29234. https://proceedings.neurips.cc/paper_files/paper/2022/file/bc218a0c656e49d4b086975a9c785f47-Paper-\nDatasets_and_Benchmarks.pdf\n[47] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review.\narXiv:2103.06268 [cs.CL]\n[48] Weiyi Huang, Jiahao Jiang, Qiang Qu, and Min Yang. 2020. AILA: A Question Answering System in the Legal Domain. In Proceedings\nof the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20 (Yokohama, Yokohama, Japan) (IJCAI’20), Christian\nBessiere (Ed.). International Joint Conferences on Artificial Intelligence Organization, Article 762, 3 pages. https://doi.org/10.24963/\nijcai.2020/762\n[49] Deepali Jain, Malaya Dutta Borah, and Anupam Biswas. 2024. A sentence is known by the company it keeps: Improving Legal Document\nSummarization Using Deep Clustering. Artificial Intelligence and Law 32, 1 (2024), 165–200.\n[50] Samyar Janatian, Hannes Westermann, Jinzhe Tan, Jaromir Savelka, and Karim Benyekhlef. 2023. From Text to Structure: Using Large\nLanguage Models to Support the Development of Legal Expert Systems. arXiv:2311.04911 [cs.CL]\n[51] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand,\nGianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]\n[52] Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex ’Sandy’ Pentland, Yoon Kim, Jad Kabbara, and\nDeb Roy. 2024. Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling. arXiv:2402.17019 [cs.CL]\n[53] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit. 2017. One model to\nlearn them all. arXiv:1706.05137 [cs.LG]\n[54] Prathamesh Kalamkar, Astha Agarwal, Aman Tiwari, Smita Gupta, Saurabh Karn, and Vivek Raghavan. 2022. Named Entity Recognition\nin Indian court judgments. In Proceedings of the Natural Legal Language Processing Workshop 2022. Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates (Hybrid), 184–193. https://doi.org/10.18653/v1/2022.nllp-1.15\n[55] Ambedkar Kanapala, Sukomal Pal, and Rajendra Pamula. 2019. Text summarization from legal documents: a survey. Artificial Intelligence\nReview 51 (2019), 371–402.\n[56] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2024. GPT-4 passes the bar exam. Philosophical\nTransactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 382, 2270 (2024), 20230254. https://doi.org/10.1098/\nrsta.2023.0254\n[57] Soha Khazaeli, Janardhana Punuru, Chad Morris, Sanjay Sharma, Bert Staub, Michael Cole, Sunny Chiu-Webster, and Dhruv Sakalley.\n2021. A Free Format Legal Question Answering System. In Proceedings of the Natural Legal Language Processing Workshop 2021,\nNikolaos Aletras, Ion Androutsopoulos, Leslie Barrett, Catalina Goanta, and Daniel Preotiuc-Pietro (Eds.). Association for Computational\nLinguistics, Punta Cana, Dominican Republic, 107–113. https://doi.org/10.18653/v1/2021.nllp-1.11\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n31\n[58] Panteleimon Krasadakis, Evangelos Sakkopoulos, and Vassilios S. Verykios. 2024. A Survey on Challenges and Advances in Natural\nLanguage Processing with a Focus on Legal Informatics and Low-Resource Languages. Electronics 13, 3 (2024). https://doi.org/10.3390/\nelectronics13030648\n[59] Amirhossein Layegh, Amir H. Payberah, Ahmet Soylu, Dumitru Roman, and Mihhail Matskin. 2023. ContrastNER: Contrastive-based\nPrompt Tuning for Few-shot NER. In 2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC). IEEE,\n241–249. https://doi.org/10.1109/COMPSAC57700.2023.00038\n[60] Jihoon Lee and Hyukjoon Lee. 2019. A Comparison Study on Legal Document Classification Using Deep Neural Networks. In\n2019 International Conference on Information and Communication Technology Convergence (ICTC). 926–928. https://doi.org/10.1109/\nICTC46691.2019.8939926\n[61] Elena Leitner, Georg Rehm, and Julian Moreno-Schneider. 2020. A Dataset of German Legal Documents for Named Entity Recognition.\nIn Proceedings of the Twelfth Language Resources and Evaluation Conference, Nicoletta Calzolari, Frédéric Béchet, Philippe Blache,\nKhalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo,\nAsuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 4478–4485.\nhttps://aclanthology.org/2020.lrec-1.551\n[62] LexisNexis [n. d.]. International Legal Generative AI Report. Retrieved July 22, 2024 from https://www.lexisnexis.com/community/\npressroom/b/news/posts/lexisnexis-international-legal-generative-ai-survey-shows-nearly-half-of-the-legal-profession-believe-\ngenerative-ai-will-transform-the-practice-of-law\n[63] Jonathan Li, Rohan Bhambhoria, and Xiaodan Zhu. 2022. Parameter-Efficient Legal Domain Adaptation. In Proceedings of the Natural\nLegal Language Processing Workshop 2022. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates (Hybrid),\n119–129. https://doi.org/10.18653/v1/2022.nllp-1.10\n[64] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2022. A Survey on Deep Learning for Named Entity Recognition. IEEE Transactions\non Knowledge and Data Engineering 34, 1 (Jan 2022), 50–70. https://doi.org/10.1109/TKDE.2020.2981314\n[65] Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. Pre-Trained Language Models for Text Generation: A\nSurvey. ACM Comput. Surv. 56, 9 (apr 2024), 1–39. https://doi.org/10.1145/3649449\n[66] Yanling Li, Jiaye Wu, and Xudong Luo. 2024. BERT-CNN based evidence retrieval and aggregation for Chinese legal multi-choice\nquestion answering. Neural Computing and Applications 36, 11 (2024), 5909–5925. https://doi.org/10.1007/s00521-023-09380-5\n[67] Marco Lippi, Przemysław Pałka, Giuseppe Contissa, Francesca Lagioia, Hans-Wolfgang Micklitz, Giovanni Sartor, and Paolo Torroni.\n2019. CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service. Artificial Intelligence and Law 27\n(2019), 117–139.\n[68] Shuaiqi Liu, Jiannong Cao, Yicong Li, Ruosong Yang, and Zhiyuan Wen. 2024. Low-resource court judgment summarization for\ncommon law systems. Information Processing and Management 61, 5 (2024), 103796. https://doi.org/10.1016/j.ipm.2024.103796\n[69] Yang Liu. 2019. Fine-tune BERT for extractive summarization. arXiv:1903.10318 [cs.CL]\n[70] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692 [cs.CL]\n[71] Yifei Liu, Yiquan Wu, Yating Zhang, Changlong Sun, Weiming Lu, Fei Wu, and Kun Kuang. 2023. ML-LJP: Multi-Law Aware Legal\nJudgment Prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\n(Taipei, Taiwan) (SIGIR ’23). Association for Computing Machinery, New York, NY, USA, 1023–1034. https://doi.org/10.1145/3539618.\n3591731\n[72] Antoine Louis, Gijs van Dijck, and Gerasimos Spanakis. 2024. Interpretable Long-Form Legal Question Answering with Retrieval-\nAugmented Large Language Models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. AAAI Press, 22266–22275.\nhttps://doi.org/10.1609/aaai.v38i20.30232\n[73] Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to Predict Charges for Criminal Cases with\nLegal Basis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Martha Palmer, Rebecca Hwa, and\nSebastian Riedel (Eds.). Association for Computational Linguistics, Copenhagen, Denmark, 2727–2736. https://doi.org/10.18653/v1/D17-\n1289\n[74] Luyao Ma, Yating Zhang, Tianyi Wang, Xiaozhong Liu, Wei Ye, Changlong Sun, and Shikun Zhang. 2021. Legal Judgment Prediction\nwith Multi-Stage Case Representation Learning in the Real Court Setting. In Proceedings of the 44th International ACM SIGIR Conference\non Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR ’21). Association for Computing Machinery, New\nYork, NY, USA, 993–1002. https://doi.org/10.1145/3404835.3462945\n[75] Dimitris Mamakas, Petros Tsotsi, Ion Androutsopoulos, and Ilias Chalkidis. 2022. Processing Long Legal Documents with Pre-trained\nTransformers: Modding LegalBERT and Longformer. In Proceedings of the Natural Legal Language Processing Workshop 2022, Nikolaos\nAletras, Ilias Chalkidis, Leslie Barrett, Cătălina Goant,ă, and Daniel Preot,iuc-Pietro (Eds.). Association for Computational Linguistics,\nAbu Dhabi, United Arab Emirates (Hybrid), 130–142. https://doi.org/10.18653/v1/2022.nllp-1.11\n[76] Sepideh Mamooler, Rémi Lebret, Stephane Massonnet, and Karl Aberer. 2022. An Efficient Active Learning Pipeline for Legal Text\nClassification. In Proceedings of the Natural Legal Language Processing Workshop 2022, Nikolaos Aletras, Ilias Chalkidis, Leslie Barrett,\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n32\n•\nAriai and Demartini\nCătălina Goant,ă, and Daniel Preot,iuc-Pietro (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates\n(Hybrid), 345–358. https://doi.org/10.18653/v1/2022.nllp-1.32\n[77] Stelios Maroudas, Sotiris Legkas, Prodromos Malakasiotis, and Ilias Chalkidis. 2022. Legal-Tech Open Diaries: Lesson learned on how\nto develop and deploy light-weight models in the era of humongous Language Models. In Proceedings of the Natural Legal Language\nProcessing Workshop 2022, Nikolaos Aletras, Ilias Chalkidis, Leslie Barrett, Cătălina Goant,ă, and Daniel Preot,iuc-Pietro (Eds.). Association\nfor Computational Linguistics, Abu Dhabi, United Arab Emirates (Hybrid), 88–110. https://doi.org/10.18653/v1/2022.nllp-1.8\n[78] Suzanne McGee. [n. d.]. Generative AI and the Law.\nRetrieved July 22, 2024 from https://www.lexisnexis.com/html/lexisnexis-\ngenerative-ai-story\n[79] Masha Medvedeva, Michel Vols, and Martijn Wieling. 2020. Using machine learning to predict decisions of the European Court of\nHuman Rights. Artificial Intelligence and Law 28, 2 (2020), 237–266. https://doi.org/10.1007/s10506-019-09255-y\n[80] Marie-Francine Moens, Caroline Uyttendaele, and Jos Dumortier. 1999. Abstracting of legal cases: the potential of clustering based on\nthe selection of representative objects. Journal of the American Society for Information Science 50, 2 (1999), 151–161.\n[81] Laurens Mommers. 2010. Ontologies in the Legal Domain. Springer Netherlands, Dordrecht, 265–276. https://doi.org/10.1007/978-90-\n481-8845-1_12\n[82] Gianluca Moro, Nicola Piscaglia, Luca Ragazzi, and Paolo Italiani. 2023. Multi-language transfer learning for low-resource legal case\nsummarization. Artificial Intelligence and Law (2023). https://doi.org/10.1007/s10506-023-09373-8\n[83] Duy-Hung Nguyen, Bao-Sinh Nguyen, Nguyen Viet Dung Nghiem, Dung Tien Le, Mim Amina Khatun, Minh-Tien Nguyen, and Hung\nLe. 2021. Robust Deep Reinforcement Learning for Extractive Legal Summarization. In Neural Information Processing, Teddy Mantoro,\nMinho Lee, Media Anugerah Ayu, Kok Wai Wong, and Achmad Nizar Hidayanto (Eds.). Springer International Publishing, Cham,\n597–604.\n[84] Ha-Thanh Nguyen, Manh-Kien Phi, Xuan-Bach Ngo, Vu Tran, Le-Minh Nguyen, and Minh-Phuong Tu. 2024. Attentive deep neural\nnetworks for legal document retrieval. Artificial Intelligence and Law 32, 1 (2024), 57–86. https://doi.org/10.1007/s10506-022-09341-8\n[85] Joel Niklaus, Ilias Chalkidis, and Matthias Stürmer. 2021. Swiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction\nBenchmark. In Proceedings of the Natural Legal Language Processing Workshop 2021, Nikolaos Aletras, Ion Androutsopoulos, Leslie\nBarrett, Catalina Goanta, and Daniel Preotiuc-Pietro (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic,\n19–35. https://doi.org/10.18653/v1/2021.nllp-1.3\n[86] Joel Niklaus, Veton Matoshi, Pooja Rani, Andrea Galassi, Matthias Stürmer, and Ilias Chalkidis. 2023. LEXTREME: A Multi-Lingual\nand Multi-Task Benchmark for the Legal Domain. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda\nBouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 3016–3054. https://doi.org/10.18653/\nv1/2023.findings-emnlp.200\n[87] Joel Niklaus, Veton Matoshi, Matthias Stürmer, Ilias Chalkidis, and Daniel E. Ho. 2024. MultiLegalPile: A 689GB Multilingual Legal\nCorpus. arXiv:2306.02069 [cs.CL]\n[88] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina\nSlama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F\nChristiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural\nInformation Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc.,\n27730–27744. https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf\n[89] Matthew J. Page, Joanne E. McKenzie, Patrick M. Bossuyt, Isabelle Boutron, Tammy C. Hoffmann, Cynthia D. Mulrow, Larissa Shamseer,\nJennifer M. Tetzlaff, Elie A. Akl, Sue E. Brennan, Roger Chou, Julie Glanville, Jeremy M. Grimshaw, Asbjørn Hróbjartsson, Manoj M.\nLalu, Tianjing Li, Elizabeth W. Loder, Evan Mayo-Wilson, Steve McDonald, Luke A. McGuinness, Lesley A. Stewart, James Thomas,\nAndrea C. Tricco, Vivian A. Welch, Penny Whiting, and David Moher. 2021. The PRISMA 2020 statement: an updated guideline for\nreporting systematic reviews. Systematic Reviews 10, 1 (2021), 89. https://doi.org/10.1186/s13643-021-01626-4\n[90] Christos Papaloukas, Ilias Chalkidis, Konstantinos Athinaios, Despina Pantazi, and Manolis Koubarakis. 2021. Multi-granular Legal\nTopic Classification on Greek Legislation. In Proceedings of the Natural Legal Language Processing Workshop 2021, Nikolaos Aletras, Ion\nAndroutsopoulos, Leslie Barrett, Catalina Goanta, and Daniel Preotiuc-Pietro (Eds.). Association for Computational Linguistics, Punta\nCana, Dominican Republic, 63–75. https://doi.org/10.18653/v1/2021.nllp-1.6\n[91] Sungmi Park and Joshua I. James. 2023. Lessons learned building a legal inference dataset. Artificial Intelligence and Law (2023).\nhttps://doi.org/10.1007/s10506-023-09370-x\n[92] Seth Polsley, Pooja Jhunjhunwala, and Ruihong Huang. 2016. CaseSummarizer: A System for Automated Summarization of Legal Texts.\nIn Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, Hideo Watanabe\n(Ed.). The COLING 2016 Organizing Committee, Osaka, Japan, 258–262. https://aclanthology.org/C16-2054\n[93] Thiago Dal Pont, Federico Galli, Andrea Loreggia, Giuseppe Pisano, Riccardo Rovatti, and Giovanni Sartor. 2023. Legal Summarisation\nthrough LLMs: The PRODIGIT Project. arXiv:2308.04416 [cs.CL]\n[94] Vasile Păis, Maria Mitrofan, Carol Luca Gasan, Vlad Coneschi, and Alexandru Ianov. 2021. Named Entity Recognition in the Romanian\nLegal Domain. In Proceedings of the Natural Legal Language Processing Workshop 2021, Nikolaos Aletras, Ion Androutsopoulos, Leslie\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n33\nBarrett, Catalina Goanta, and Daniel Preotiuc-Pietro (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic,\n9–18. https://doi.org/10.18653/v1/2021.nllp-1.2\n[95] Juliano Rabelo, Randy Goebel, Mi-Young Kim, Yoshinobu Kano, Masaharu Yoshioka, and Ken Satoh. 2022. Overview and discussion of\nthe competition on legal information extraction/entailment (COLIEE) 2021. The Review of Socionetwork Strategies 16, 1 (2022), 111–133.\n[96] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 1, Article 140 (jan 2020),\n67 pages.\n[97] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. DistilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv:1910.01108 [cs.CL]\n[98] Jaromir Savelka, Kevin D. Ashley, Morgan A. Gray, Hannes Westermann, and Huihui Xu. 2023. Explaining Legal Concepts with\nAugmented Large Language Models (GPT-4). arXiv:2306.09525 [cs.CL]\n[99] Marijn Schraagen, Floris Bex, Nick Van De Luijtgaarden, and Daniël Prijs. 2022. Abstractive Summarization of Dutch Court Verdicts\nUsing Sequence-to-sequence Models. In Proceedings of the Natural Legal Language Processing Workshop 2022, Nikolaos Aletras, Ilias\nChalkidis, Leslie Barrett, Cătălina Goant,ă, and Daniel Preot,iuc-Pietro (Eds.). Association for Computational Linguistics, Abu Dhabi,\nUnited Arab Emirates (Hybrid), 76–87. https://doi.org/10.18653/v1/2022.nllp-1.7\n[100] Gil Semo, Dor Bernsohn, Ben Hagag, Gila Hayat, and Joel Niklaus. 2022. ClassActionPrediction: A Challenging Benchmark for Legal\nJudgment Prediction of Class Action Cases in the US. In Proceedings of the Natural Legal Language Processing Workshop 2022, Nikolaos\nAletras, Ilias Chalkidis, Leslie Barrett, Cătălina Goant,ă, and Daniel Preot,iuc-Pietro (Eds.). Association for Computational Linguistics,\nAbu Dhabi, United Arab Emirates (Hybrid), 31–46. https://doi.org/10.18653/v1/2022.nllp-1.3\n[101] Zein Shaheen, Gerhard Wohlgenannt, and Erwin Filtz. 2020.\nLarge scale legal text classification using transformer models.\narXiv:2010.12871 [cs.CL]\n[102] Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey. 2022. Multi-LexSum: Real-world Summaries\nof Civil Rights Lawsuits at Multiple Granularities. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 13158–13173. https://proceedings.neurips.cc/paper_\nfiles/paper/2022/file/552ef803bef9368c29e53c167de34b55-Paper-Datasets_and_Benchmarks.pdf\n[103] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The Woman Worked as a Babysitter: On Biases in\nLanguage Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.).\nAssociation for Computational Linguistics, Hong Kong, China, 3407–3412. https://doi.org/10.18653/v1/D19-1339\n[104] Juanming Shi, Qinglang Guo, Yong Liao, Yuxing Wang, Shijia Chen, and Shenglin Liang. 2024. Legal-LM: Knowledge Graph Enhanced\nLarge Language Models for Law Consulting. In Advanced Intelligent Computing Technology and Applications, De-Shuang Huang,\nZhanjun Si, and Chuanlei Zhang (Eds.). Springer Nature Singapore, Singapore, 175–186.\n[105] Răzvan-Alexandru Smădu, Ion-Robert Dinică, Andrei-Marius Avram, Dumitru-Clementin Cercel, Florin Pop, and Mihaela-Claudia\nCercel. 2022. Legal Named Entity Recognition with Multi-Task Domain Adaptation. In Proceedings of the Natural Legal Language\nProcessing Workshop 2022, Nikolaos Aletras, Ilias Chalkidis, Leslie Barrett, Cătălina Goant,ă, and Daniel Preot,iuc-Pietro (Eds.). Association\nfor Computational Linguistics, Abu Dhabi, United Arab Emirates (Hybrid), 305–321. https://doi.org/10.18653/v1/2022.nllp-1.29\n[106] Dezhao Song, Andrew Vold, Kanika Madan, and Frank Schilder. 2022. Multi-label legal document classification: A deep learning-based\napproach with label-attention and domain-specific pre-training. Information Systems 106 (2022), 101718. https://doi.org/10.1016/j.is.\n2021.101718\n[107] Francesco Sovrano, Monica Palmirani, Biagio Distefano, Salvatore Sapienza, and Fabio Vitali. 2021. A dataset for evaluating legal question\nanswering on private international law. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law (São\nPaulo, Brazil) (ICAIL ’21). Association for Computing Machinery, New York, NY, USA, 230–234. https://doi.org/10.1145/3462757.3466094\n[108] Francesco Sovrano, Monica Palmirani, Salvatore Sapienza, and Vittoria Pistone. 2024. DiscoLQA: zero-shot discourse-based legal\nquestion answering on European Legislation. Artificial Intelligence and Law (2024). https://doi.org/10.1007/s10506-023-09387-2\n[109] Francesco Sovrano, Monica Palmirani, and Fabio Vitali. 2020. Legal knowledge extraction for knowledge graph based question-\nanswering. In Legal knowledge and information systems. IOS Press, 143–153. https://doi.org/10.3233/FAIA200858\n[110] Ralf Steinberger, Bruno Pouliquen, Mijail Kabadjov, Jenya Belyaeva, and Erik van der Goot. 2011. JRC-NAMES: A Freely Available,\nHighly Multilingual Named Entity Resource. In Proceedings of the International Conference Recent Advances in Natural Language\nProcessing 2011, Ruslan Mitkov and Galia Angelova (Eds.). Association for Computational Linguistics, Hissar, Bulgaria, 104–110.\nhttps://aclanthology.org/R11-1015\n[111] Benjamin Strickson and Beatriz De La Iglesia. 2020. Legal Judgement Prediction for UK Courts. In Proceedings of the 3rd International\nConference on Information Science and Systems (Cambridge, United Kingdom) (ICISS ’20). Association for Computing Machinery, New\nYork, NY, USA, 204–209. https://doi.org/10.1145/3388176.3388183\n[112] Zhongxiang Sun. 2023. A Short Survey of Viewing Large Language Models in Legal Aspect. arXiv:2303.09136 [cs.CL]\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n34\n•\nAriai and Demartini\n[113] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the Capabilities, Limitations, and Societal Impact of\nLarge Language Models. arXiv:2102.02503 [cs.CL]\n[114] Doron Teichman, Eyal Zamir, and Ilana Ritov. 2023. Biases in legal decision-making: Comparing prosecutors, defense attorneys, law\nstudents, and laypersons. Journal of empirical legal studies 20, 4 (2023), 852–894.\n[115] Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named\nEntity Recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. 142–147.\nhttps:\n//www.aclweb.org/anthology/W03-0419\n[116] Suxin Tong, Jingling Yuan, Peiliang Zhang, and Lin Li. 2024. Legal Judgment Prediction via graph boosting with constraints. Information\nProcessing & Management 61, 3 (2024), 103663. https://doi.org/10.1016/j.ipm.2024.103663\n[117] Don Tuggener, Pius von Däniken, Thomas Peetz, and Mark Cieliebak. 2020. LEDGAR: A Large-Scale Multi-label Corpus for Text\nClassification of Legal Provisions in Contracts. In Proceedings of the Twelfth Language Resources and Evaluation Conference, Nicoletta\nCalzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente\nMaegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources\nAssociation, Marseille, France, 1235–1241. https://aclanthology.org/2020.lrec-1.155\n[118] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.\n2017. Attention is All you Need. In Advances in Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17, Vol. 30),\nI. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., Red Hook,\nNY, USA, 6000–6010. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[119] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks.\narXiv:1710.10903 [stat.ML]\n[120] Daniela Vianna, Edleno Silva de Moura, and Altigran Soares da Silva. 2023. A topic discovery approach for unsupervised organization\nof legal document collections. Artificial Intelligence and Law (2023). https://doi.org/10.1007/s10506-023-09371-w\n[121] Qiqi Wang, Kaiqi Zhao, Robert Amor, Benjamin Liu, and Ruofan Wang. 2022. D2GCLF: Document-to-Graph Classifier for Legal\nDocument Classification. In Findings of the Association for Computational Linguistics: NAACL 2022, Marine Carpuat, Marie-Catherine\nde Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 2208–2221.\nhttps://doi.org/10.18653/v1/2022.findings-naacl.170\n[122] Fusheng Wei, Han Qin, Shi Ye, and Haozhen Zhao. 2018. Empirical Study of Deep Learning for Text Classification in Legal Document\nReview. In 2018 IEEE International Conference on Big Data (Big Data). IEEE, 3317–3320. https://doi.org/10.1109/BigData.2018.8622157\n[123] Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for\nComputational Linguistics, Hong Kong, China, 6382–6388. https://doi.org/10.18653/v1/D19-1670\n[124] Westlaw. [n. d.]. Westlaw. Retrieved May 23, 2024 from https://anzlaw.thomsonreuters.com/Browse/Home/Australia160?comp=wlau&\n__lrTS=20240523040153004&transitionType=Default&contextData=(sc.Default)\n[125] Yiquan Wu, Yifei Liu, Weiming Lu, Yating Zhang, Jun Feng, Changlong Sun, Fei Wu, and Kun Kuang. 2022. Towards Interactivity and\nInterpretability: A Rationale-based Legal Judgment Prediction Framework. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\nAbu Dhabi, United Arab Emirates, 4787–4799. https://doi.org/10.18653/v1/2022.emnlp-main.316\n[126] Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. 2021. Lawformer: A pre-trained language model for Chinese\nlegal long documents. AI Open 2 (2021), 79–84. https://doi.org/10.1016/j.aiopen.2021.06.003\n[127] Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng\nWang, and Jianfeng Xu. 2018. CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction. arXiv:1807.02478 [cs.CL] https:\n//arxiv.org/abs/1807.02478\n[128] Nuo Xu, Pinghui Wang, Long Chen, Li Pan, Xiaoyan Wang, and Junzhou Zhao. 2020. Distinguish Confusing Law Articles for Legal\nJudgment Prediction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce\nChai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 3086–3095. https://doi.org/10.\n18653/v1/2020.acl-main.280\n[129] Wenmian Yang, Weijia Jia, Xiaojie Zhou, and Yutao Luo. 2019. Legal judgment prediction via multi-perspective bi-feedback network.\nIn Proceedings of the 28th International Joint Conference on Artificial Intelligence (Macao, China) (IJCAI’19). AAAI Press, 4085–4091.\n[130] Hai Ye, Xin Jiang, Zhunchen Luo, and Wenhan Chao. 2018. Interpretable Charge Predictions for Criminal Cases: Learning to Generate\nCourt Views from Fact Descriptions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.).\nAssociation for Computational Linguistics, New Orleans, Louisiana, 1854–1864. https://doi.org/10.18653/v1/N18-1168\n[131] Mingruo Yuan, Ben Kao, Tien-Hsuan Wu, Michael M. K. Cheung, Henry W. H. Chan, Anne S. Y. Cheung, Felix W. H. Chan, and Yongxi\nChen. 2023. Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\nNatural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges\n•\n35\nArtificial Intelligence and Law (2023). https://doi.org/10.1007/s10506-023-09367-6\n[132] Kwan Yuen Iu and Vanessa Man-Yi Wong. 2023. ChatGPT by OpenAI: The End of Litigation Lawyers. https://doi.org/10.2139/ssrn.\n4339839\n[133] Han Zhang, Zhicheng Dou, Yutao Zhu, and Ji-Rong Wen. 2023. Contrastive Learning for Legal Judgment Prediction. ACM Transactions\non Information Systems 41, 4, Article 113 (apr 2023), 25 pages. https://doi.org/10.1145/3580489\n[134] Weiqi Zhang, Hechuan Shen, Tianyi Lei, Qian Wang, Dezhong Peng, and Xu Wang. 2023. GLQA: A Generation-based Method for Legal\nQuestion Answering. In 2023 International Joint Conference on Neural Networks (IJCNN). 1–8. https://doi.org/10.1109/IJCNN54540.2023.\n10191483\n[135] Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. 2021. When does pretraining help? assessing\nself-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings. In Proceedings of the Eighteenth International\nConference on Artificial Intelligence and Law (São Paulo, Brazil) (ICAIL ’21). Association for Computing Machinery, New York, NY, USA,\n159–168. https://doi.org/10.1145/3462757.3466088\n[136] Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. 2018. Legal Judgment Prediction via\nTopological Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David\nChiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 3540–3549.\nhttps://doi.org/10.18653/v1/D18-1390\n[137] Haoxi Zhong, Yuzhong Wang, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020. Iteratively Questioning and\nAnswering for Interpretable Legal Judgment Prediction. Proceedings of the AAAI Conference on Artificial Intelligence 34, 01 (Apr 2020),\n1250–1257. https://doi.org/10.1609/aaai.v34i01.5479\n[138] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020. How Does NLP Benefit Legal\nSystem: A Summary of Legal Artificial Intelligence. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online,\n5218–5230. https://doi.org/10.18653/v1/2020.acl-main.466\n[139] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020. JEC-QA: A Legal-Domain Question\nAnswering Dataset. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 9701–9708. https://doi.org/10.48550/arXiv.\n1911.12011\n[140] Linwu Zhong, Ziyi Zhong, Zinian Zhao, Siyuan Wang, Kevin D. Ashley, and Matthias Grabmair. 2019. Automatic Summarization\nof Legal Decisions using Iterative Masking of Predictive Sentences. In Proceedings of the Seventeenth International Conference on\nArtificial Intelligence and Law (Montreal, QC, Canada) (ICAIL ’19). Association for Computing Machinery, New York, NY, USA, 163–172.\nhttps://doi.org/10.1145/3322640.3326728\n[141] Yang Zhong and Diane Litman. 2022. Computing and Exploiting Document Structure to Improve Unsupervised Extractive Summarization\nof Legal Case Decisions. In Proceedings of the Natural Legal Language Processing Workshop 2022, Nikolaos Aletras, Ilias Chalkidis,\nLeslie Barrett, Cătălina Goant,ă, and Daniel Preot,iuc-Pietro (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab\nEmirates (Hybrid), 322–337.\n[142] Andreas Östling, Holli Sargeant, Huiyuan Xie, Ludwig Bull, Alexander Terenin, Leif Jonsson, Måns Magnusson, and Felix Steffek. 2024.\nThe cambridge law corpus: a dataset for legal AI research. In Proceedings of the 37th International Conference on Neural Information\nProcessing Systems (New Orleans, LA, USA) (NIPS ’23). Curran Associates Inc., Red Hook, NY, USA, Article 1793, 31 pages.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2024.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "A.1; I.2.7; J.1"
  ],
  "published": "2024-10-25",
  "updated": "2024-10-25"
}