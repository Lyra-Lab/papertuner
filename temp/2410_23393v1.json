{
  "id": "http://arxiv.org/abs/2410.23393v1",
  "title": "Resource Governance in Networked Systems via Integrated Variational Autoencoders and Reinforcement Learning",
  "authors": [
    "Qiliang Chen",
    "Babak Heydari"
  ],
  "abstract": "We introduce a framework that integrates variational autoencoders (VAE) with\nreinforcement learning (RL) to balance system performance and resource usage in\nmulti-agent systems by dynamically adjusting network structures over time. A\nkey innovation of this method is its capability to handle the vast action space\nof the network structure. This is achieved by combining Variational\nAuto-Encoder and Deep Reinforcement Learning to control the latent space\nencoded from the network structures. The proposed method, evaluated on the\nmodified OpenAI particle environment under various scenarios, not only\ndemonstrates superior performance compared to baselines but also reveals\ninteresting strategies and insights through the learned behaviors.",
  "text": "Resource Governance in Networked Systems via\nIntegrated Variational Autoencoders and\nReinforcement Learning\nQiliang Chen\nMAGICS lab\nCollege of Engineering\nNortheastern University\nBoston, MA 02115, USA\nchen.qil@northeastern.edu\nBabak Heydari*\nMAGICS lab\nCollege of Engineering and Network Science Institute\nNortheastern University\nBoston, MA 02115, USA\nb.heydari@northeastern.edu\nAbstract\nWe introduce a framework that integrates variational autoencoders (VAE) with\nreinforcement learning (RL) to balance system performance and resource usage\nin multi-agent systems by dynamically adjusting network structures over time. A\nkey innovation of this method is its capability to handle the vast action space of the\nnetwork structure. This is achieved by combining Variational Auto-Encoder and\nDeep Reinforcement Learning to control the latent space encoded from the network\nstructures. The proposed method, evaluated on the modified OpenAI particle\nenvironment under various scenarios, not only demonstrates superior performance\ncompared to baselines but also reveals interesting strategies and insights through\nthe learned behaviors.\nKeywords: Multi-agent system, Resource Allocation, Network Structure, Deep reinforcement\nlearning, Variational auto-encoder\n1\nIntroduction\nModern complex systems increasingly rely on the coordinated actions of multiple autonomous agents,\neach equipped with the ability to sense, decide, and act independently. These multi-agent systems\nhave transformed numerous domains including robotics [1, 2, 3, 4], supply chain management\n[5, 6, 7], communication networks [8, 9, 10], transportation systems [11, 12, 13], and most recently,\nsystems involving multiple Large Language Model agents [14, 15, 16]. A fundamental challenge in\nthese systems lies in their decentralized nature - while each agent operates based on its individual\nobjectives and local information, the overall system must achieve broader collective goals. System\nmanagers face the complex task of steering these autonomous agents toward system-level objectives\nthrough strategic allocation of limited resources, such as communication channels or energy.\nAt the core of these decentralized systems lies the crucial element of inter-agent communication and\ninteraction, which distinguishes multi-agent systems from independent single-agent scenarios. These\ninteractions can be modeled as networks, with resources represented by network links. Research\nhas shown that different network structures significantly impact system-level metrics including\nperformance, resource utilization, and cooperation levels (See the background section). This raises a\nPreprint. Under review.\narXiv:2410.23393v1  [cs.LG]  30 Oct 2024\nkey question: How can we achieve desired system-level outcomes by strategically modifying these\nnetwork structures?\nThe complexity of intervening at the network structure level is compounded by several factors\ninherent to decentralized multi-agent systems. The environment’s dynamic nature, combined with\nagents’ continuous learning and adaptation [17], makes developing effective coordination policies\nparticularly challenging. The heterogeneity of agents - variations in properties, policies, and learning\nrates - further increases system complexity [18]. Additionally, the decentralized nature of these\nsystems introduces partial observability, where both managers and agents must operate with limited\ninformation about the true environmental state [19].\nWhile Deep Reinforcement Learning [20, 21, 22, 23, 24, 25, 26] offers promising approaches for\nlearning adaptable policies in dynamic, partially observable environments [27], its application to\nnetwork structure control faces a significant challenge: the vast discrete action space of possible\nnetwork configurations. This space grows as O(2N×(N−1)/2) for a network with N nodes, quickly\nbecoming intractable for efficient policy learning as networks expand.\nTo address these challenges, we propose VAE-RL, a novel framework combining Variational Auto-\nEncoders (VAE) [28] with Deep Reinforcement Learning. VAEs, comprising an encoder and decoder,\ncan embed data distributions into latent spaces while maintaining the ability to reconstruct the\noriginal data. In our framework, we treat potential network configurations as a dataset, using a VAE\nto transform the vast discrete action space of network structures into a manageable continuous latent\nspace. This enables the application of continuous-action DRL algorithms like Deep Deterministic\nPolicy Gradient (DDPG), with the pre-trained VAE decoder reconstructing chosen latent actions into\nnetwork structures.\nOur work builds upon and extends our previous Two-tier framework for Systems of Systems (SoS)\n[27], which introduced the concept of SoS workers (Tier I) and SoS managers (Tier II). While the\nprevious framework was limited to homogeneous resource allocation, our current study enhances\nflexibility and efficiency by replacing the SoS manager with the VAE-RL framework. In this\nenhanced model, the manager controls communication network structures, with SoS workers sharing\ninformation through network connections. More connections indicate higher communication resource\nconsumption, requiring the VAE-RL-equipped manager to balance system performance with resource\nusage through dynamic network modifications.\nWe evaluate our framework in a modified OpenAI Gym particle environment [29], testing both\nhomogeneous and heterogeneous scenarios across various system scales. Our results demonstrate that\nVAE-RL outperforms baseline methods in optimizing the weighted sum of system-level performance\nand network-related resource usage, while revealing meaningful patterns for effective multi-agent\nsystem management.\n2\nBackground\nIn this section, we review literature relevant to our research across three key areas. First, we explore\nnetwork structure governance in various domains, which aligns with our paper’s primary focus.\nSecond, we examine generative models for network-based tasks, which relates to our approach of\ntransforming discrete action spaces into continuous latent spaces. Finally, we review our previous\nTwo-Tier framework, which serves as the foundation for evaluating our proposed VAE-RL method.\n2\n2.1\nNetwork Structure Governance\nNetwork structure governance - the ability to modify network configurations to influence agent\nbehavior and system performance - has emerged as a critical research area across multiple domains.\nWe examine its applications in three key areas: communication systems, public health policy, and\nsocio-technical systems. In communication systems, researchers have developed various approaches\nto network governance. Nakamura et al. [30] address telecommunication service cost reduction\nwhile maintaining network stability under varying electricity prices. Bijami et al. [31] introduce\na Distributed Networked Control Scheme (DNCS) for stabilizing large-scale interconnected sys-\ntems, specifically addressing communication challenges like random delays and packet dropouts.\nDeep Reinforcement Learning (DRL) applications in this domain include Chu et al.’s [32] scalable,\ndecentralized Multi-Agent Reinforcement Learning (MARL) algorithm for adaptive traffic signal\ncontrol, and Shibata et al.’s [33] exploration of MARL for multi-agent cooperative transport. Network\nstructures have also been studied in social and economic contexts, examining communication benefits,\ncosts, and decentralized strategic decisions [34, 35]. These studies have identified structures that\nbalance network stability and efficiency [36, 37]. Public health policy has increasingly embraced\nnetwork governance approaches. Dynamic complex networks have proven effective in modeling\nmicro-social interactions for public health [38], urban policies [39], and platform economies [40].\nDuring the COVID-19 pandemic, Mazzitello et al. [41] developed network-based pool testing strate-\ngies for low-resource settings, while Robins et al. [42] analyzed network interventions like social\ndistancing. Siciliano et al. [43] contributed a framework for network control in public administration\nto promote behavioral change. In socio-technical systems, network governance has facilitated various\nadvances. Ion et al. [44] developed a scalable algorithm for self-organizing control systems using\nnetworked multi-agent systems. Ellis et al. [45] studied network impacts on self-management support\ninterventions, while Joseph et al. [46] proposed a complex systems approach to model social media\nplatform interventions. Network structure has proven crucial in promoting prosocial behaviors such as\ncooperation [47, 48, 49], coordination [50], and fairness [51], particularly in social dilemma situations.\nWhile network structure control has garnered significant attention, most existing approaches rely on\ntraditional methods like hard-coded policies or heuristics, which struggle with complex, evolving,\nand partially observable environments. Deep Reinforcement Learning offers greater flexibility but\noften focuses on decentralized approaches, potentially compromising system-level optimization.\nOur approach addresses these limitations through a centralized method that tackles the curse of\ndimensionality.\n2.2\nGenerative Models for Networks\nGenerative models have revolutionized various fields, including images [52, 53], voice [54, 55, 56],\nand text [57, 58]. In network modeling, Kipf and Welling [59] introduced the Variational Graph\nAuto-Encoder (VGAE), enabling unsupervised learning on graph-structured data. Li et al. [60]\ndeveloped an approach using graph neural networks to express probabilistic dependencies among\nnodes and edges. Domain-specific applications of network generation include DeepLigBuilder by Li\net al. [61] for drug design, Zhao et al.’s [62] physics-aware crystal structure generation, and Singh et\nal.’s [63] TripletFit technique for network classification. Comprehensive surveys [64, 65, 66] provide\nextensive coverage of network generation methods. Our work takes a novel approach by integrating\ngenerative models with Deep Reinforcement Learning to enable centralized network structure control\nwhile addressing the curse of dimensionality.\n3\n2.3\nOverview of the Two-Tier RL Governance Framework\nOur previous work [27] introduced a Two-Tier framework integrating Reinforcement Learning to\noptimize resource distribution in Systems of Systems (SoS). The framework operates across two\ntraining levels. In Tier I, individual SoS components undergo decentralized-training-decentralized-\nexecution (DTDE) using the Deep Deterministic Policy Gradient (DDPG) algorithm [20]. This tier\ndevelops essential skills based on partial observations, after which policies become fixed. Tier II\nfocuses on the centralized resource manager’s allocation decisions, including resource type and timing,\nusing Deep Q-learning to maximize operational effectiveness. While our previous work limited\nthe system manager to choosing between empty or fully connected communication networks, our\ncurrent research expands these capabilities. The manager can now assign any possible communication\nnetwork configuration at each time step, enabling more adaptive and tailored strategies.\n3\nMethodology\nIn this section, we provide background knowledge and explain the method we are using. First, we\nbriefly introduce the Partially Observable Markov Decision Process (POMDP), the fundamental\nmodel underlying Deep Reinforcement Learning (DRL). Next, we discuss the Deep Deterministic\nPolicy Gradient (DDPG), the DRL algorithm we employ for managing continuous latent action spaces.\nWe also explain the Branching Structured-Based DQN, an important baseline method in our study.\nFinally, we comprehensively introduce our methodology for training Variational Auto-Encoders\n(VAE) on networks and the integration of VAE with DDPG to optimize our system’s performance.\n3.1\nPartial observable Markov decision process\nThe environmental dynamics in our study are aptly modeled using a Partial Observable Markov\nDecision Process (POMDP), as outlined in [67]. A POMDP is characteristically defined by a tuple\n< S, s0, A, O, T, R >: where S is a set of potential states; s0 represents the initial state of the\nenvironment, with s0 ∈S; A denotes the set of available actions; O comprises the observations\nderived from the actual state; R is the reward outputted by the environment; T is the transition\nfunction, defined as T : O × A →O; and U is the reward function, where U : O × A →R. The\nprimary objective within a POMDP framework is to develop an optimal policy, πθ: O × A →[0, 1].\nThis policy aims to maximize the expected return, calculated as Pn\nt=0 γtrt, where γ represents the\ndiscount factor.\n3.2\nDeep deterministic policy gradient\nPolicy gradient methods, as described in [68], represent a potent subclass of reinforcement learning\n(RL) algorithms. The fundamental concept involves directly modifying the policy parameters,\ndenoted as θ, to optimize the objective function J(θ) = Eπθ[R]. This optimization is achieved by\nprogressing in the direction of the gradient ∇θJ(θ). The policy gradient can be expressed as follows:\n∇θJ(θ) = Eo,a[∇θ log πθ(a|o)Qπ(o, a)]. Here, Qπ(o, a), representing the action-value function,\ncan be updated using techniques outlined in the Q learning section. This approach evolves into the\nactor-critic algorithm. Extending this framework to deterministic policies, denoted as µθ : O →A,\nleads to the formulation of the Deep Deterministic Policy Gradient (DDPG) algorithm. In DDPG, the\ngradient of the objective undergoes a transformation: ∇θJ(θ) = Eo,a[∇θµθ(a|o)∇aQµ(o, a)|a =\nµθ(o)]. In this context, µθ(a|s), which represents the deterministic policy, is modeled using a deep\n4\nneural network parameterized by θ. The DDPG algorithm is particularly suited for scenarios involving\ncontinuous action spaces, as it outputs a deterministic action value at each time step.\n3.3\nBranching Deep Q Network for Network-based system\nThe Branching Deep Q Network (BDQN) [69] presents a solution for implementing Deep Q learning\nin scenarios characterized by large discrete action spaces. When an action space encompasses several\ndimensions, denoted as N, with each dimension offering a multitude of options, symbolized as D,\nthe complexity of this space escalates exponentially, represented as O(DN). This exponential growth\nnot only results in impractically large model sizes but also significantly complicates the optimization\nprocess. To address this, BDQN leverages the Dueling Q Network architecture [70], which separately\nlearns Q functions for each dimension. It then integrates these functions using a shared state value,\nfacilitating coordination among them. Consequently, this approach effectively reduces the action\nspace complexity to O(N ∗D), rendering the learning process feasible for tasks with extensive\ndiscrete action spaces.\nTo elucidate how BDQN serves as a baseline in our context, our action space of network topology\ncan be converted into actions based on the number of link dimensions, where each dimension has two\noptions — to have a link or not. Therefore, in our case, N corresponds to the number of links, and\nD is 2, representing the binary choice for each link (either present or absent). Utilizing the BDQN\ntraining scheme, we can effectively learn policies for network structure assignment.\n3.4\nVariational Auto-encoder\nThe Variational Auto-encoder (VAE) [28], a renowned generative model, has garnered considerable\nacclaim in fields like image and network generation. Its primary objective is to encode a dataset\nof independently and identically distributed samples into an embedded distribution that represents\ncertain latent variables (encoder) and to subsequently generate new samples from this distribution\n(decoder). During the training phase, the encoder and decoder are trained jointly, facilitating a\ncohesive learning process. Upon establishing the embedded distribution, the decoder’s parameters\nare fixed, enabling it to generate samples that mirror the distribution of the original dataset. This\ncapability is the cornerstone of VAE’s proficiency in producing images or networks akin to those\nin the dataset. In our specific context, the dataset comprises network topologies, represented by\nadjacency matrices. The training regimen adheres closely to the standard VAE protocol, which\nunfolds as follows:\nIn our dataset, let’s denote the variables as x, with each data point following a distribution p(x).\nThe latent variables are characterized by the distribution p(z). The encoder in a VAE is effectively\nrepresented by the conditional probability p(z|x), while the decoder is represented by p(x|z). Direct\ncomputation of p(z|x) poses practical challenges, primarily because the underlying distribution p(x)\nis intractable, as evidenced when expanding p(z|x) using Bayes’ rule. To circumvent this, VAE\nintroduces an approximate function q(z|x), assuming its tractability. In traditional VAE models,\na standard multivariate Gaussian distribution is employed for this approximation. To refine our\napproximation of p(z|x), we focus on minimizing the Kullback–Leibler divergence between q(z|x)\nand p(z). This minimization process involves a series of mathematical operations, ultimately leading\nus to maximize the following objective: Eq(z|x) log p(x|z) −KL(q(z|x)||p(z)).\nThe objective function in VAEs comprises two key components. The first is the log-likelihood of the\nreconstructed variables, which measures the accuracy of the reconstruction. The second component\nis a regularization term, which ensures that the distribution of the latent variable z aligns closely with\n5\nOriginal graph\nSampling from \nGaussian \ndistribution\nGraph \nreconstruction\nReconstructed graph\nLatent \nvariables Z\nEncoder\nDecoder\nμ\nΣ\n……\nFigure 1: The diagram shows a Variational Autoencoder applied to network topology. The encoder\nprocesses the adjacency matrix, producing Gaussian distribution parameters. The decoder samples\nfrom this distribution to reconstruct the adjacency matrix. Both components use deep neural networks.\nthe prior distribution. As previously mentioned, this prior distribution is assumed to be a standard\nmultivariate Gaussian distribution. To model both the encoder q(z|x) and decoder p(x|z), deep neural\nnetworks are employed. Practically, the encoder outputs the mean and covariance of the Gaussian\ndistribution from which z is sampled. Furthermore, VAE employs reparameterization tricks to enable\nthe backpropagation of gradients through stochastic nodes, thereby facilitating effective training\nof the network. For a more comprehensive understanding of these mechanisms, further details are\navailable in [28].\n3.5\nDDPG with VAE for network topology governance\nIn controlling multi-agent systems at the system level, a centralized management approach is pivotal,\nprimarily to harmonize high-level performance with the allocation of various types of resources. This\napproach’s efficacy is substantiated in studies such as [27, 25]. Within these complex systems, the\ndynamics of information sharing and agent interaction are crucial. The topology of the network,\nin particular, plays a crucial role in influencing outcomes across different scenarios. Consequently,\nwhen controlling multi-agent systems, a frequent challenge encountered is the need for centralized\ncontrol over the network topology. This task is complicated by the sheer complexity and diversity\nof potential network topologies, resulting in an action space that is vast and often impractical for\ntraditional control algorithms with discrete action spaces, such as Deep Q-learning [71].\nPolicy gradient methods are notably proficient in handling complex and continuous action spaces [20].\nIn our cases, where the inherent action space comprises network topologies, it will be beneficial to\nencode this vast, discrete action space into a manageable, continuous latent action space. By applying\npolicy gradient methods to this transformed, latent continuous space, we can adeptly navigate and\noptimize within it. Subsequently, the optimized latent space can be decoded back into reconstructed\nnetwork topologies. This approach effectively circumvents the challenges associated with the original,\nlarge discrete action space, presenting a more efficient method for network topology construction and\noptimization.\nInspired by this idea, we introduce the VAE-RL framework as a solution. This approach begins\nby employing a Variational Auto-encoder (VAE) to transform the extensive, discrete action space\nassociated with network topologies into a continuous action space. Following this transformation,\npolicy gradient algorithms with continuous action spaces are utilized to learn effective control policies.\nA key advantage of this framework is its ability to leverage the generative capabilities of the VAE,\nallowing for the reversion of the controlled, continuous action space back to its original form — the\nnetwork topology. This process effectively surmounts the challenge posed by the vast discrete action\nspace, thereby enhancing the control mechanisms within multi-agent systems. The training process\n6\nfor the VAE-RL framework is separated into two distinct phases: the initial training of the VAE,\nfollowed by the DRL training, which is conducted using the pre-trained VAE.\n3.5.1\nVAE training process\nOur primary objective centers on managing network topologies within multi-agent systems. To\nachieve this, we concentrate on training a Variational Auto-encoder (VAE) specifically designed to\nencode adjacency matrices into a latent space, and then decode this latent space back into recon-\nstructed adjacency matrices. For both the encoder and decoder components of the VAE, we utilize\nfully connected neural networks. However, in scenarios where the system manager possesses compre-\nhensive control with higher authority, extending not only to network topology but also directly to\nagent properties, Graph Neural Networks (GNNs) [72] emerge as a viable alternative for representing\nboth the encoder and decoder. The training framework for the VAE, as applied to network topology,\nis depicted in Figure 1.\nInitially, we assemble a dataset, denoted as D, comprising samples of potential network topologies\ngiven the number of nodes. It’s crucial to acknowledge that encompassing every possible network\ntopology becomes more intractable as the number of nodes increases. Subsequently, this dataset\nis partitioned into a training set and a validation set, adhering to conventional supervised learning\nmethodologies. Network topologies are represented through a flattened adjacency matrix, symbolized\nas A.\nThe architecture of our model employs fully connected neural networks to instantiate the encoder and\ndecoder, denoted by fencoder(x; θ1) and fdecoder(x; θ2), respectively. During each training iteration,\na mini-batch of data a is sampled from the training set. The encoder processes this data to yield the\nmean and covariance of a multi-variate Gaussian distribution: µ, Σ = fencoder(a; θ1). Subsequently,\nthe latent variable values z are sampled from this distribution, where z ∈Rd and d, the latent variable\ndimension, is a pre-defined hyperparameter.\nThe decoder then takes z as input and produces the reconstructed adjacency matrix ˆa: ˆa =\nfdecoder(z; θ2). Loss is calculated using the standard VAE loss we mentioned earlier, and the\ngradient of the loss is backpropagated to update θ1 and θ2 in fencoder(x; θ1) and fdecoder(x; θ2).\nUltimately, the model is evaluated on the validation set, and the model with better performance\nthrough validation is saved.\n3.5.2\nDDPG training process with learned VAE\nIn the VAE training phase, we have two key outcomes: a learned encoder that effectively embeds\ninput network topologies into a latent space, and a learned decoder capable of reconstructing network\ntopologies from latent variables. During the DDPG training, the parameters of the pre-trained VAE\nare fixed, with an exclusive focus on utilizing the decoder. This process is illustrated in Figure 2.\nThe environment dynamics are defined as R, S′ = Transition(S, Aadj), where R represents the\nreward for the current step, S is the current state, and S′ is the subsequent state of the environment.\nThe current action, Aadj, corresponds to the network topology. For modeling the actor µ(o; ϕ) and\nthe critic Q(o, a; θ), fully connected networks are employed. In each step of the DDPG training,\nthe manager observes the current state of the environment, obtaining the observation ot at time step\nt. The actor then outputs latent variable values: zt = µ(ot; ϕ). As part of exploration, noise is\nadded to actions, diminishing over time, a strategy recommended by [20]. Subsequently, the decoder\ntransforms these latent variables into a reconstructed adjacency matrix: at\nadj = fdecoder(zt; θ2).\nThe environment takes this reconstructed matrix, updates its state, and generates corresponding\n7\nMulti-agent\nsystem \nenvironment\nGraph \nreconstruction\nReconstructed graph\nLatent \nvariables Z\nTrained Decoder\n……\nDDPG\nManager\nControl\nAction: \nnetwork topology\nObservation,\nrewards\nFigure 2: The diagram depicts the VAE-RL framework used for interaction within a multi-agent\nsystem environment. The DDPG manager directly manages the continuous latent variables, which\nare decoded by the decoder into the reconstructed network topology. This topology serves as the\nfinal action within a Partially Observable Markov Decision Process (POMDP). Following this, the\nmulti-agent system environment processes the action and updates its state accordingly. The DDPG\nManager then receives observations and rewards, which it uses to update its policy and execute\nsubsequent controls. During this process, the decoder’s parameters remain fixed from the pre-trained\nmodel.\nrewards: r, s(t+1) = Transition(st, at\nadj). This trajectory, denoted as st, zt, r, s(t+1), is stored in\nthe Replay-buffer D. During training, a mini-batch of trajectory data is sampled; the actor and critic\nare updated by backpropagating the gradient of the loss in the DDPG we mentioned earlier in Section\n3.2.\n4\nExperiment results and discussion\nIn this section, we first introduce the environment we are using and the related settings. Next, we\ncompare the performance of the proposed VAE-RL framework to baseline methods in different\nscenarios. We then analyze the learned behavior of the system manager using VAE-RL and discuss\nthe trends and insights derived from the results. Finally, we present snapshots of the network evolution\nover time, visualizing the system’s dynamics and justifying our findings.\n4.1\nExperiment design\nWe utilize the modified OpenAI Gym particle environment [29] to evaluate the effectiveness of our\nVAE-RL framework. In the original ’spread’ task within this particle environment, multiple agents are\ntasked with spreading themselves on landmarks while minimizing collisions. This scenario constitutes\na multi-agent system where effective coordination among agents is essential. In our previous work\n[27], we introduced modifications to this environment, making it partially observable. Furthermore,\nwe introduced different types of resources, such as additional vision and communication capabilities.\nThese resources are dynamically selected by the SoS manager to maintain a balance between system\nperformance and resource utilization for the SoS workers. However, the communication network\noptions were limited (either empty or complete) in the previous study. Additionally, the SoS agents\nwere homogeneous, and the proposed framework fell short in handling heterogeneous situations.\nIn this paper, we focus solely on the allocation of communication resources and adding flexibility to\nthe action space. The SoS manager can select different communication network topologies during\ntasks, providing opportunities to save communication resources and improve system performance in\nuncertain environments with heterogeneous agents. Therefore, we retain the essential components in\n8\nthe hierarchical framework for SoS that we previously proposed but mainly increase the flexibility of\nnetwork topologies within the action space in Tier II.\nIn Tier I, SoS workers are initialized in a manner consistent with the previous framework. SoS\nworkers and landmarks are randomly initialized at the beginning of each game within a 2 × 2 square.\nThey possess limited vision, allowing them to observe only landmarks or other agents within their\nvisual range. Furthermore, these workers act autonomously, making individual decisions based on the\ninformation available to them. While the SoS manager in Tier II lacks the ability to directly compel\nSoS workers to alter their actions, it can influence their behavior by manipulating the information they\nreceive. This manipulation is achieved through the assignment of different communication network\ntopologies. It’s important to note that the SoS manager does not possess an all-encompassing view of\nthe entire system but instead relies on aggregate information derived from the observations of SoS\nworkers.\nAt each time step, the SoS manager assigns a communication network to the SoS workers. Sub-\nsequently, these workers share their information with their neighbors within the communication\nnetwork. The allocation of communication networks involves the utilization of communication\nresources, with each link in the network incurring a cost of 0.1 in the current settings. The primary\nobjective of the SoS manager is to maximize a weighted combination of two key sub-objectives:\nmaximizing task performance and minimizing communication resource costs. The current weight is\n1, which means the scores of tasks and the communication resource cost are equally important. The\nspecific weights applied to these sub-objectives can be adjusted based on their relative importance,\nwhich is thoroughly explored in [27].\nRegarding the VAE-RL algorithms, we set several hyperparameters: the decoder and encoder in\nVAE, and the actors and critics in DDPG are represented using fully connected neural networks. For\nVAE, we use 512 × 256 to represent the encoder and 256 × 512 to represent the decoder, while the\ndimension of the latent space is 10 for the multi-agent system with 10 agents. The resource manager\nuses 1024 × 512 × 256 fully connected neural networks to represent critics and 1024 × 512 fully\nconnected neural networks to represent actors. We use the Adam optimizer with a 0.001 learning rate\nfor critic training and the Adam optimizer with a 0.0001 learning rate for actor training. The training\nfor resource managers includes 20,000 epochs. It is crucial to note that critics are only used during\nthe training phase, so only actors participate in the decision-making process during the execution\nphase. In our experiments, each game has 50 time steps, and the following results have been tested\non 1,000 new games.\n4.2\nPerformance of proposed method\nTo assess the generalization and robustness capabilities of our proposed VAE-RL method, we aim\nto compare its performance against the traditional DRL approach using a discrete action space for\nnetwork topology, hereafter referred to as Flat-RL, within a 10-agent system. Given that the action\nspace in this system is O(2N∗(N−1)), where N = 10, its enormity renders the training for Flat-RL\nimpractical. Therefore, we introduce another baseline, BDQN [69], which is scalable in larger\nnetworks. Initially, we apply both VAE-RL and the two baseline methods (Flat-RL and BDQN)\nto a smaller system comprising 4 agents. After establishing that BDQN demonstrates equivalent\nor superior performance compared to Flat-RL in this smaller setup, we then extend our evaluation\nto a larger 10-agent system. Here, BDQN’s performance serves as a proxy for the upper bound of\nFlat-RL’s capabilities. This approach allows us to indirectly gauge the performance of Flat-RL in\nlarger systems, thereby ensuring the coherence of our results.\n9\n0.6 vision\n0.8 vision\n1.0 vision\n1.2 vision\nheterogeneous vision\n100\n80\n60\n40\n20\n0\nScores\nComparison of different policies in 4-agents system\nrandom baseline\nResource cost VAE-RL\nWorker scores VAE-RL\nResource cost BDQN\nWorker scores BDQN\nResource cost Flat-RL\nWorker scores Flat-RL\nFigure 3: Results show performance and resource penalties for various methods with homogeneous\nagents (vision ranges 0.6-1.2) and heterogeneous agents in 4-agent systems. A star marker indicates\nthe random policy baseline’s overall performance.\nInitially, we established a homogeneous vision range for all SoS workers. We designed a series of\nfour experiments, varying in difficulty levels, with vision ranges of 0.6, 0.8, 1.0, and 1.2. It’s worth\nnoting that all entities are positioned within a 2 × 2 square configuration, meaning that even the\nsimplest task remains partially observable. Additionally, even tasks with a vision range of 1.2 are not\ntrivial because it is still possible that agents are unable to observe anything at the initial states. In\nnumerous real-world multi-agent systems, agents often exhibit heterogeneity in their properties or\ncapabilities. To test our framework in a more realistic environment, we have devised an environment\nwith heterogeneous agents, where agents have vision ranges of 2, 1.5, 1, and 0.5. These experiments\nserve as crucial and representative examples within the realm of heterogeneous scenarios.\nIn our experiments, we first assess the performance of our VAE-RL framework, followed by an\nanalysis of the learned behaviors of the Systems of Systems (SoS) manager. It is important to note that\nsince the VAE training employs a standard supervised scheme with distinct training and validation\nsets within a sampled dataset, both the VAE-RL framework and its training process can be scaled to\nlarger networks using the same training scheme by increasing the dataset and the neural networks to\nreasonable sizes.\n4.2.1\nResults on two environments\nWe first applied all methods on the small environment with 4 agents and got the results in Figure 3.\nFrom the results, our VAE-RL approach consistently outperforms other baseline methods across all\ntasks including heterogeneous cases, ranging from difficult to easy. This not only underscores the\nsuperior performance of our method but also underscores its robustness under different scenarios.\nBecause it is hard to compare the homogeneous scenarios and heterogeneous scenarios directly, for\nthe purpose of analyzing trends, our subsequent findings and discussions in this section will primarily\nfocus on homogeneous cases.\n10\n0.6 vision\n0.8 vision\n1.0 vision\n1.2 vision\nheterogeneous vision\n500\n400\n300\n200\n100\n0\nScores\nComparison of different policies in 10-agents system\nrandom baseline\nResource cost VAE-RL\nWorker scores VAE-RL\nResource cost BDQN\nWorker scores BDQN\nFigure 4: Results show performance and resource penalties for various methods with homogeneous\nagents (vision ranges 0.6-1.2) and heterogeneous agents in 10-agent systems. A star marker indicates\nthe random policy baseline’s overall performance.\nSecondly, it’s noteworthy that as the vision range increases, the performance of all methods and\ncommunication resource usage demonstrate improvement. This phenomenon can be attributed to the\ntasks becoming progressively easier. Even the simplest strategy, such as randomly selecting actions,\nbecomes more effective because SoS workers have access to more self-observed information with\nlarger vision ranges, leading to enhanced performance. Furthermore, with larger vision ranges, work-\ners have increased opportunities to independently observe landmarks and other agents. Consequently,\nthe reliance on the communication network diminishes, leading to reduced usage of communication\nresources.\nThirdly, as tasks become easier, the VAE-RL method exhibits substantial improvements compared to\nthe baseline methods. In environments where SoS workers have limited vision ranges, such as 0.6\nand 0.8, the tasks are exceedingly challenging for agents to accomplish with flawless performance,\neven when employing intelligent communication network assignment strategies. This difficulty arises\nfrom the fact that smaller vision ranges result in SoS workers having very limited opportunities for\nuseful observation at initial states. Despite the potential for communication through the network,\nthey have little to share due to the scarcity of information. Consequently, in these critical scenarios,\nwe observe only marginal enhancements in the VAE-RL method compared to the baseline methods.\nConversely, when tasks are moderately challenging, the VAE-RL method exhibits more substantial\nimprovements. This is because it leverages knowledge gleaned from the diverse positions of agents\nand landmarks to intelligently select communication networks that optimize system performance\nwhile conserving communication resources.\nFinally, it becomes evident that although BDQN does not surpass VAE-RL in total performance, it\nconsistently outperforms Flat-RL across a variety of scenarios, including those involving heteroge-\nneous cases. This consistent superiority of BDQN over Flat-RL serves as a compelling indicator\nof its effectiveness, suggesting that BDQN’s performance is on par with, or perhaps even superior\n11\n10\n20\n30\n40\n50\nTime step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio\nThe distribution of communication network through time\nsparse network\nmid dense \ndense network\nvery dense network\n(a) 0.6 vision range\n10\n20\n30\n40\n50\nTime step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio\nThe distribution of communication network through time\nsparse network\nmid dense \ndense network\nvery dense network\n(b) 0.8 vision range\n10\n20\n30\n40\n50\nTime step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio\nThe distribution of communication network through time\nsparse network\nmid dense \ndense network\nvery dense network\n(c) 1.0vision range\n10\n20\n30\n40\n50\nTime step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio\nThe distribution of communication network through time\nsparse network\nmid dense \ndense network\nvery dense network\n(d) 1.2 vision range\nFigure 5: Communication network distribution over time is analyzed for homogeneous agents with\nvision ranges of 0.6, 0.8, 1.0, and 1.2 (subgraphs a-d). Networks are categorized as sparse (≤9 links),\nmid-dense (9-17 links), dense (18-26 links), or very dense (≥27 links).\nto, Flat-RL. When considering larger and more complex environments that incorporate a greater\nnumber of agents, where the application of Flat-RL is hindered by the model’s size and optimization\ncomplexities, BDQN stands as a viable baseline. Its proven advantage over Flat-RL in smaller settings\nprovides a rationale for employing BDQN as a benchmark in these larger scenarios. This analysis\nallows for meaningful comparison and evaluation of the VAE-RL’s performance against a relevant\nand established baseline in more expansive and challenging environments.\nIn our expanded experiment involving a larger environment with 10 agents, we tested the VAE-RL\nmethod and the BDQN baseline. As previously noted, Flat-RL is not capable of handling this more\ncomplex task, leading us to focus solely on the performances of VAE-RL and BDQN, as illustrated in\nFigure 4. The results clearly show that VAE-RL continues to outperform the BDQN baseline across\nall environment settings, including those with heterogeneous conditions. This consistent pattern\nreinforces the trends observed in smaller environments, indicating that VAE-RL’s effectiveness\nscales well with increased complexity. On the other hand, the BDQN baseline struggles even in\nthe simplest case with agents having 1.2 vision ranges, where its performance is comparable to a\nrandom strategy. This outcome suggests that while BDQN remains applicable in this context, it\nfails to develop a meaningful policy for communication network assignment. In contrast, VAE-RL\nnot only demonstrates strong performance relative to the baseline but also retains its potential for\neffective implementation in even larger systems with more agents.\n4.3\nEvolution of Network Behavior\nAfter confirming that our VAE-RL method outperforms the baseline in both small and large systems,\nwe also aim to analyze the learned behaviors of SoS managers to enhance the explainability of our\n12\n10\n20\n30\n40\n50\nTime step\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\nAverage degrees\nThe degrees of agents through time\nagent0_2vision\nagent1-3_1vision\nagent4-6_0.5vision\nagent7-9_0vision\n(a) Evolution of node degrees\n10\n20\n30\n40\n50\nTime step\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\nAverage betweenness\nThe betweenness of agents through time\nagent0_2vision\nagent1-3_1vision\nagent4-6_0.5vision\nagent7-9_0vision\n(b) Evolution of node betweenness\nFigure 6: The results show average node degrees and betweenness centrality for agents with different\nvision ranges over time. The study includes one agent with vision 2.0, and three agents each with\nvisions 1.0, 0.5, and 0.\nmodels using deep learning techniques and generate valuable insights or heuristics. We illustrate the\ndistribution of communication networks with varying link densities during the task. We examine the\nbehaviors of VAE-RL’s policy in both homogeneous and heterogeneous cases.\n4.3.1\nScenarios with homogeneous agents\nWe have four environments for homogeneous cases, where agents have vision ranges of 0.6, 0.8,\n1.0, and 1.2. We categorize the communication networks based on the number of links they contain.\nSparse networks are defined as those with fewer than 9 links; mid-dense networks as those with more\nthan 9 but fewer than 18 links; dense networks as those with more than 18 but fewer than 27 links;\nand very dense networks as those with more than 27 links. We then plot the distribution of networks\nwith varying densities over time in Figure 5.\nFirstly, the graphs show that as tasks become less challenging, there is a notable increase in the\nfrequency of using less dense communication networks, while the frequency of employing costly\ndenser communication networks decreases. In the easiest task, with a vision range of 1.2, the use\nof sparse communication networks approaches approximately 100% at the end of the game. This\nobservation aligns with our earlier explanation that easier tasks correspond to reduced utilization of\ncommunication resources.\nSecondly, the behavior of the SoS manager exhibits two distinct phases: during the initial 10 time\nsteps, the manager leans towards employing more costly communication networks with a higher\nnumber of links, then it shifts towards utilizing cheaper communication networks. At the beginning of\neach task, due to the constraints posed by limited vision ranges, some SoS workers may not have the\ncapacity to observe any entities. Consequently, the manager’s preference is to encourage all workers to\naggregate their information through dense communication networks. However, denser communication\nnetworks entail higher communication resource costs. Therefore, as workers approach landmarks\nand can perceive landmarks within their own limited vision ranges, the manager is inclined to assign\nsparser communication networks to save communication resources. For instance, in the initial phase\nof sub-graph (a), where the vision range is 0.6, resulting in exceptionally challenging tasks, the\nmanager exhibits a frequency of assigning very dense communication networks of almost 60%.\nSubsequently, there is an immediate decrease in the frequency of employing costly communication\nnetworks, while the frequency of employing sparse communication networks exceeds 50% shortly\nafter a few time steps.\n13\n4.3.2\nExtending the model to heterogeneous agents\nIn numerous real-world multi-agent systems, agents often exhibit heterogeneity in their properties\nor capabilities. Regrettably, many studies in the domain of multi-agent system control or SoS\ncontrol [27] tend to exclusively focus on homogeneous scenarios, rendering them less adaptable\nfor deployment in heterogeneous environments. To address this limitation, we have designed an\nexperiment with 10 agents where agents possess heterogeneous vision ranges: (2, 1, 1, 1, 0.5, 0.5,\n0.5, 0, 0, 0). This experiment serves as a crucial and representative example within the realm of\nheterogeneous scenarios. Instead of analyzing the communication networks in general, we focus\nmore on the agents’ properties within networks. Thus, we examine the average node degrees of the\nagents and the average node betweenness centrality for those with the same vision range and plot\nthese through time. The calculation of node betweenness centrality includes both the start nodes and\nend nodes. Our analysis of the learned behaviors of the resource manager and the corresponding\nresults are depicted in Figure 6.\nUpon analysis, we can discern two distinct phases in the behavior patterns of the SoS manager,\nparalleling our previous findings. During the first phase (0-10 time steps), the node degrees of agents\nare relatively high, indicating a greater level of connectivity among agents. This is quickly followed\nby a shift to a state with fewer degrees. In the second phase (10-50 time steps), a notable stabilization\noccurs, with the agents maintaining generally low node degrees. The rationale behind this pattern\nmirrors our earlier analysis: At the beginning, agents require a higher degree of information exchange\nfor effective coordination and task completion, leading to an increased number of connections.\nHowever, maintaining a high node degree comes with substantial costs. Consequently, once the\nagents have acquired essential information and start converging toward their objectives, there’s a\nsignificant reduction in node degrees to minimize costs while maintaining efficiency.\nFor node betweenness centrality, we observe an increase during the 0-10 time steps, a decrease\nbetween 10-20 time steps, and stabilization afterward. Since betweenness centrality measures the\nimportance and effectiveness of each node in the information flow, we infer that initially, agents\ncannot effectively share their information due to their random positions and connections within the\nnetwork. With guidance from the VAE-RL manager, agents are strategically positioned within more\nreasonable network structures, enhancing their importance and effectiveness in the network. Finally,\nas agents approach their targets, the need for a complex communication network diminishes, reducing\nthe overall importance of all agents and resulting in stabilization.\nFurthermore, we sought to identify patterns among agents with differing vision ranges. It was\nobserved that agents with a vision range of 2 consistently exhibited the highest node degree and\nnode betweenness centrality over time. This suggests that agents with a vision range of 2 tend\nto remain central within the communication network, maintaining more connections and a higher\nlevel of importance. Contrary to expectations, a clear trend where agents with greater vision ranges\ndemonstrate larger node degrees and node betweenness centrality was not evident. Specifically,\nagents with a vision range of 1 generally had lower node degrees and node betweenness centrality\ncompared to agents with a vision range of 0.5.\nWe propose several explanations for our findings. First, agents with moderate abilities serve a dual\nrole: they require assistance from others to solve tasks in certain scenarios, yet they are also capable\nof providing valuable aid in different contexts. This bidirectional flow of information in a complex\nenvironment may contribute to the observed phenomenon. Second, while agents with moderate\nabilities lack the power to significantly assist others, they are able to solve tasks independently. This\nsuggests that in some cases, they should be isolated from the communication network, resulting in\n14\n2\n1\n0.5\n(a) Requirement of 0.5\nability\n2\n1\n0.5\n0.25\n1\n(b) Requirement of 1\nability\n2\n1\n0.5\n0.25\n1\n0.5\n1\n(c) Requirement of 1.5\nability\n2\n1\n0.5\n0.25\n1\n0.5\n1\n0.25\n0.5\n(d) Requirement of 2\nability\nFigure 7: The illustrative example justifying the \"flipping rank\" phenomenon. There are three agents\nwith different abilities: Agent 2 (red) with ability 2, Agent 1 (green) with ability 1, and Agent 0.5\n(blue) with ability 0.5. Agents can boost their total abilities by gaining half of their connected agents’\nabilities. The colored values represent the boosted abilities. In four different scenarios, agents must\nmeet the task requirements to succeed. Each successful task rewards 1, outweighing the connection\ncost of 0.1. Network structures shown are the most efficient, which maximizes performance minus\nconnection costs.\nlower node degrees and betweenness centrality compared to agents with lesser abilities. More details\ncan be found in Appendix B.\nTo elucidate these findings, we designed a simplified theoretical example to capture the core aspects of\nour environment and propose a couple of potential explanations for this \"flipping rank\" phenomenon,\nas illustrated in Figure 7. In this example, agents with varying capabilities, represented by different\nvision ranges in our experiment, assist one another via connections in a communication network.\nHowever, the assistance an agent can provide is typically less than its own capabilities, because\nthe information beneficial to one agent may not be as useful to another. The task difficulty in each\nepisode varies, resulting from the random initial positions of agents and landmarks in our experiment.\nThe system earns rewards when more agents successfully complete tasks, which occurs when their\ninnate abilities combined with the help received from others through the network meet the task’s\nrequirements. This scenario represents the combination of agents’ own observations and others’\ninformation from communication to facilitate solving the landmark spreading task.\nIn the example, we assume that the cost of establishing one link in the network (0.1) is less than the\nreward for an additional agent completing the task (1). To resolve a tie when two agents can help\nanother complete the task, the agent with higher ability will offer the help. This decision ensures\nthat our insights remain consistent and are not influenced by random methods such as flipping coins.\nHere, we have three agents with abilities quantified as 2, 1, and 0.5, which we refer to as agent\n2, agent 1, and agent 0.5 respectively. Each agent contributes half of their ability to assist others\nthrough undirected connections, facilitating mutual support among connected agents. We then explore\nthe most efficient communication network structures from the system manager’s perspective under\nvarious scenarios within this simplified framework.\n15\nIn scenarios where task requirements are minimal (0.5 ability), there is no need for additional links in\nthe communication network since all agents can independently meet these requirements, and adding\nlinks would only incur unnecessary costs. For tasks requiring 1 ability, only agent 0.5 cannot solve the\ntask alone. In this case, it is advantageous for agent 2 to connect to agent 0.5. This connection boosts\nthe latter’s total ability to 1.5, enabling it to accomplish the task. Although agent 2 also receives\nan additional 0.25 ability from agent 0.5, this surplus does not provide further benefit as it already\nexceeds the task requirement.\nWhen tasks demand 1.5 ability, both agents with abilities of 1 and 0.5 fall short on their own. Thus,\nagent 2 must establish connections with both to facilitate task completion. In the most challenging\nscenarios, where the requirement is 2 ability, agent 2 just manages to meet this threshold independently.\nHowever, agents with abilities of 1 and 0.5 are unable to complete the task on their own. In this case,\nagent 0.5 remains unable to fulfill the task requirements despite assistance from agent 2. Consequently,\nthe most effective network structure under these conditions is a fully connected network, where all\nagents are interconnected, maximizing the distribution of available abilities and support.\nAssuming that the tasks with varying difficulties are uniformly distributed across all episodes, the\naverage node degrees for agent 2, agent 1, and agent 0.5 are 1.25, 0.75, and 1, respectively; the average\nbetweenness centralities for agent 2, agent 1, and agent 0.5 are 0.67, 0.29, and 0.54, respectively. The\nrankings in this illustrative example reveal a similar pattern to our simulation results, showing that the\nrank of agents’ degrees and betweenness centrality may not align with the rank of their abilities. This\npattern holds even when the tasks are normally distributed in all episodes, with a higher probability\nfor tasks of middle difficulties, which more closely reflects the conditions of our original experiment.\nFirst, we clearly observe that agent 1 is in a dual role: it needs assistance from others to solve tasks in\nscenarios requiring abilities of 1.5 and 2, and it can also provide useful help in scenarios requiring an\nability of 2. While this factor does not directly result in the \"flipping rank\" phenomenon, it can become\nmore significant and contribute to this phenomenon in our more complex experiments. Second, the\nscenario with a requirement of 1 ability is the main reason for this phenomenon. Agent 1 is not\npowerful enough to assist others significantly, but it can independently solve the task, suggesting it\nshould be isolated from the communication network in some cases. In the scenario with a requirement\nof 1 ability, even if we relax the initial assumption for breaking ties and instead determine network\nstructures by flipping a coin, the results would not change.\nIt should be emphasized that this example significantly simplifies our original environment. In our\nexperiment, quantifying the abilities of agents and the difficulty of tasks is challenging; the process is\nmuch more complex, and the assistance offered by agents is heterogeneous, varying with different\nrecipients and evolving over time. However, this example captures the essential aspects and replicates\nsimilar patterns observed in our experimental results, potentially explaining the insights behind the\n\"flipping rank\" phenomenon as well.\n4.3.3\nEvolution of network structure\nThis visualization aims to elucidate the interactions between agents and the flow of information\nwithin communication networks, which is shown in Figure 8. For this demonstration, we standardize\nthe positions of landmarks while initializing the agents’ positions randomly, each with vision ranges\nof 0.6, 0.8, 1.0, and 1.2. We capture snapshots of the environment at various time steps—0, 5, 10, 15,\n20—to observe the progression of the environment and the behaviors of the agents. It should be noted\nthat the training and testing procedures described in our paper incorporate significant randomness to\nensure the robustness of our VAE-RL framework. The statistical analysis provided in the previous\n16\n(a) Vision range 0.6\n(b) Vision range 0.8\n(c) Vision range 1.0\n(d) Vision range 1.2\nFigure 8: The evolution of communication networks and agents’ behaviors using the VAE-RL policy\nunder different vision ranges. The graphs, from left to right, show the system’s status at time steps 0,\n5, 10, 15, and 20. Each row, from top to bottom, represents the evolution under vision ranges of 0.6,\n0.8, 1.0, and 1.2. For clarity, the initial positions of landmarks are set in a grid-like formation, while\nin other cases, they are randomly initialized.\nsection offers a more precise evaluation of VAE-RL. The examples presented in this section are\nintended to visually demonstrate the system’s evolution.\nWe begin by analyzing the evolution of the system over time. Initially, the agents are randomly\ndispersed across the environment for all scenarios at varying distances from the landmarks. At the\nearly stage, the communication network is notably dense, reflecting our earlier observation that denser\nnetworks facilitate information sharing among agents, aiding them in coordinating their efforts and\ncommencing their tasks. From time steps 5 to 15, a gradual thinning of the communication network\nis observed, with agents progressively moving closer to their landmarks. This trend corroborates\nour previous finding that communication networks become sparser over time, effectively balancing\nthe dual objectives of task completion and conservation of communication resources. From time\nsteps 15 to 20, the communication network is almost empty for all scenarios, indicating that most\nagents have either reached or are nearing their landmarks, with only a few agents continuing their\njourney towards the remaining landmarks. Finally, by time step 20, the snapshot reveals all agents\nsuccessfully occupying their landmarks, rendering the communication network unnecessary.\nWe also examined the system’s evolution across scenarios with varying vision ranges. A clear trend\nemerges: as the vision range increases, making the task progressively easier, the communication\nnetwork among agents becomes sparser at every time step. This trend corroborates our previous\n17\nstatistical analysis findings. Agents with wider vision ranges can more effectively gather information\nfrom their surroundings, enhancing their ability to coordinate with others and accomplish tasks\nindependently. Consequently, the reliance on communication decreases since rich communication\nbecomes unnecessary and incurs additional resource costs. For example, the system with a 0.6 vision\nrange maintains a relatively dense communication network up to time step 10, whereas systems with\n1.0 and 1.2 vision ranges exhibit sparse communication networks from the outset and completely\ndisable communication after time step 10.\n5\nConclusion\nWe introduce VAE-RL, a novel approach for optimizing multi-agent communication networks.\nTraditional Deep Reinforcement Learning (DRL) struggles with the exponentially large action space\nof network topologies. VAE-RL addresses this by using a Variational Autoencoder (VAE) to transform\nthe discrete action space into a manageable continuous latent space. DRL algorithms for continuous\naction spaces (DDPG) then learn policies in this latent space, which are decoded back into network\ntopologies. Tested on a modified OpenAI particle environment [27], VAE-RL outperforms baseline\nmethods like Flat-RL and BDQN in both small (4 agents) and large (10 agents) systems, across\nhomogeneous and heterogeneous scenarios. Our analysis reveals insightful trends in VAE-RL’s\nbehavior.\nWhile our environment is primarily modeled after the communication networks of multi-robotic\nsystems in the real world, it can also find parallels in other real-world situations. For instance,\nconsider a corporate management system where a department comprises multiple employees, includ-\ning at least one manager. These employees collaborate to address their respective tasks. However,\nvariations in talent and work experience among the employees lead to differences in task completion\nperformance. In such scenarios, the manager is tasked with making decisions on how to establish\nconnections among employees to facilitate collective task completion, especially for those employees\nwith comparatively weaker abilities. Optimizing departmental performance by strategically assigning\nconnection networks among employees is a critical responsibility of department managers. This sce-\nnario closely mirrors the communication network assignment strategy learned by VAE-RL. Notably,\nour method operates autonomously and is adaptable to systems featuring heterogeneous workers,\nmaking it versatile and applicable across various contexts.\nThe VAE-RL framework holds significant potential for application in numerous real-world multi-agent\nsystems, particularly those reliant on network-based information sharing and interaction processes.\nHowever, it has its limitations. A fundamental assumption underpinning VAE-RL is the strong\nauthority of the manager, responsible for assigning network structures to multi-agent systems. This\nframework presupposes that agents will comply with the manager’s commands, a scenario that may\nnot always hold true in real-life situations. Multi-agent systems can be broadly classified into three\ncategories: human-only systems, human-AI systems, and AI-only systems. In the last two categories,\nthe assumption of compliance is more likely to be valid, given the predominance of AI agents\nprogrammed to adhere to managerial decisions, except in cases of technical failures. However, in\nsystems with a higher proportion of human agents, this assumption may not always be correct. Even\nin scenarios where the manager represents high-authority entities such as governments, military, or\ncorporations, human agents may sometimes exhibit rebellious behavior, as evidenced by instances of\nnon-compliance with social distancing and masking policies during the Covid-19 pandemic [73, 74].\nTherefore, while our framework is highly effective in AI-dominated systems, its applicability may be\n18\nlimited in scenarios involving a majority of human agents due to the potential for reduced managerial\nauthority.\nAdditionally, the proposed VAE-RL framework exhibits a scalability limitation. While we have\ndemonstrated that the framework can effectively manage networks up to 10 nodes and potentially\nadapt to larger network scenarios, it may struggle with the computational demands posed by real-\nworld applications such as transportation, social media, and human interaction networks, which often\ncomprise thousands of nodes. Although our framework significantly reduces the complexity of the\naction space in networks, it still faces challenges related to computational expenses. Scaling up\nengineering management techniques for extensive networks remains an unresolved issue.\nHowever, our framework can accommodate larger networks through the concept of introducing a\nmulti-hierarchical structure [25]. The general process may involve dividing the network into several\nsub-networks using clustering algorithms. A first-layer manager then allocates communication\nresources, treating each sub-network as a node. Subsequent layers of management distribute these\nresources to agents within the sub-networks. As the network size increases, this hierarchical structure\ncan be expanded further. This approach introduces new challenges, including the efficiency of network\nclustering algorithms and the development of super networks that integrate these sub-networks as\nnodes. Despite these difficulties, this method holds promise for scaling our framework to handle\nmulti-agent systems in significantly larger networks.\nReferences\n[1] Michael Sony and Subhash Naik. Industry 4.0 integration with socio-technical systems theory:\nA systematic review and proposed theoretical model. Technology in society, 61:101248, 2020.\n[2] Hao-yu Liao, Yuhao Chen, Boyi Hu, and Sara Behdad. Optimization-based disassembly\nsequence planning under uncertainty for human–robot collaboration. Journal of Mechanical\nDesign, 145(2):022001, 2023.\n[3] Laxmi Poudel, Wenchao Zhou, and Zhenghui Sha. Resource-constrained scheduling for multi-\nrobot cooperative three-dimensional printing. Journal of Mechanical Design, 143(7):072002,\n2021.\n[4] Laxmi Poudel, Saivipulteja Elagandula, Wenchao Zhou, and Zhenghui Sha. Decentralized and\ncentralized planning for multi-robot additive manufacturing. Journal of Mechanical Design,\n145(1):012003, 2023.\n[5] Mihalis Giannakis and Michalis Louis. A multi-agent based framework for supply chain risk\nmanagement. Journal of Purchasing and Supply Management, 17(1):23–31, 2011.\n[6] Kannan Govindan, Mohammad Fattahi, and Esmaeil Keyvanshokooh. Supply chain network\ndesign under uncertainty: A comprehensive review and future research directions. European\njournal of operational research, 263(1):108–141, 2017.\n[7] Tong Zhou, Dunbing Tang, Haihua Zhu, and Zequn Zhang. Multi-agent reinforcement learning\nfor online scheduling in smart factories. Robotics and computer-integrated Manufacturing,\n72:102202, 2021.\n[8] Ruoyu Su, Dengyin Zhang, Ramachandran Venkatesan, Zijun Gong, Cheng Li, Fei Ding, Fan\nJiang, and Ziyang Zhu. Resource allocation for network slicing in 5g telecommunication\nnetworks: A survey of principles and models. IEEE Network, 33(6):172–179, 2019.\n19\n[9] Joshua T Gyory, Gary Stump, Hannah Nolte, and Jonathan Cagan. Adaptation through com-\nmunication: Assessing human–artificial intelligence partnership for the design of complex\nengineering systems. Journal of Mechanical Design, 146:081401–1, 2024.\n[10] Ashish M Chaudhari, Erica L Gralla, Zoe Szajnfarber, and Jitesh H Panchal. Co-evolution of\ncommunication and system performance in engineering systems design: a stochastic network-\nbehavior dynamics model. Journal of Mechanical Design, 144(4):041706, 2022.\n[11] Ibrahim M Chamseddine and Michael Kokkolaras. Bio-inspired heuristic network configuration\nin air transportation system-of-systems design optimization. Journal of Mechanical Design,\n139(8):081401, 2017.\n[12] Gautam Marwaha and Michael Kokkolaras. System-of-systems approach to air transporta-\ntion design using nested optimization and direct search. Structural and Multidisciplinary\nOptimization, 51:885–901, 2015.\n[13] Ke Zhang, Fang He, Zhengchao Zhang, Xi Lin, and Meng Li. Multi-vehicle routing problems\nwith soft time windows: A multi-agent reinforcement learning approach. Transportation\nResearch Part C: Emerging Technologies, 121:102861, 2020.\n[14] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\nmulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\n[15] Nunzio Lorè and Babak Heydari. Strategic behavior of large language models: Game structure\nvs. contextual framing. arXiv preprint arXiv:2309.05898, 2023.\n[16] Qiliang Chen, Nunzio Lore, Babak Heydari, et al. Instigating cooperation among llm agents\nusing adaptive information modulation. arXiv preprint arXiv:2409.10372, 2024.\n[17] Nicos Karcanias and Ali G Hessami. Complexity and the notion of system of systems: Part (i):\nGeneral systems and complexity. In 2010 World Automation Congress, pages 1–7. IEEE, 2010.\n[18] Philip Twu, Yasamin Mostofi, and Magnus Egerstedt. A measure of heterogeneity in multi-agent\nsystems. In 2014 American Control Conference, pages 3972–3977. IEEE, 2014.\n[19] Tommi Jaakkola, Satinder Singh, and Michael Jordan. Reinforcement learning algorithm for\npartially observable markov decision problems. Advances in neural information processing\nsystems, 7, 1994.\n[20] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\n[21] Qiliang Chen, Babak Heydari, and Mohsen Moghaddam. Leveraging task modularity in\nreinforcement learning for adaptable industry 4.0 automation. Journal of Mechanical Design,\n143(7), 2021.\n[22] Yitao Liang, Marlos C Machado, Erik Talvitie, and Michael Bowling. State of the art control of\natari games using shallow reinforcement learning. arXiv preprint arXiv:1512.01563, 2015.\n[23] Kun Shao, Zhentao Tang, Yuanheng Zhu, Nannan Li, and Dongbin Zhao. A survey of deep\nreinforcement learning in video games. arXiv preprint arXiv:1912.10944, 2019.\n[24] Charalampos P Andriotis and Konstantinos G Papakonstantinou. Managing engineering systems\nwith large state and action spaces through deep reinforcement learning. Reliability Engineering\n& System Safety, 191:106483, 2019.\n20\n[25] Qiliang Chen and Babak Heydari. The sos conductor: Orchestrating resources with iterative\nagent-based reinforcement learning. Systems Engineering, 2024.\n[26] Maximilian E Ororbia and Gordon P Warn. Discrete structural design synthesis: a hierarchical-\ninspired deep reinforcement learning approach considering topological and parametric actions.\nJournal of Mechanical Design, 146(9), 2024.\n[27] Qiliang Chen and Babak Heydari. Dynamic resource allocation in systems-of-systems using\na heuristic-based interpretable deep reinforcement learning. Journal of Mechanical Design,\n144(9):091711, 2022.\n[28] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n[29] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent\nactor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275,\n2017.\n[30] Ryota Nakamura, R. Kawahara, Takefumi Wakayama, and Shigeaki Harada. Virtual network\ncontrol for power bills reduction and network stability. IEEE Transactions on Network and\nService Management, 19:4338–4349, 2022.\n[31] E. Bijami and M. Farsangi. A distributed control framework and delay-dependent stability\nanalysis for large-scale networked control systems with non-ideal communication network.\nTransactions of the Institute of Measurement and Control, 41:768 – 779, 2019.\n[32] Tianshu Chu, Jie Wang, Lara Codecà, and Zhaojian Li. Multi-agent deep reinforcement learning\nfor large-scale traffic signal control. IEEE Transactions on Intelligent Transportation Systems,\n21:1086–1095, 2019.\n[33] Kazuki Shibata, Tomohiko Jimbo, and Takamitsu Matsubara. Deep reinforcement learning of\nevent-triggered communication and control for multi-agent cooperative transport. 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 8671–8677, 2021.\n[34] Matthew O Jackson and Asher Wolinsky. A strategic model of social and economic networks.\nNetworks and Groups: Models of Strategic Formation, pages 23–49, 2003.\n[35] Babak Heydari, Mohsen Mosleh, and Kia Dalili. Efficient network structures with separable\nheterogeneous connection costs. Economics Letters, 134:82–85, 2015.\n[36] Mohsen Mosleh, Peter Ludlow, and Babak Heydari. Resource allocation through network\narchitecture in systems of systems: A complex networks framework. In 2016 Annual IEEE\nSystems Conference (SysCon), pages 1–5. IEEE, 2016.\n[37] Mohsen Mosleh, Peter Ludlow, and Babak Heydari. Distributed resource management in\nsystems of systems: An architecture perspective. Systems Engineering, 19(4):362–374, 2016.\n[38] Qingtao Cao and Babak Heydari. Micro-level social structures and the success of covid-19\nnational policies. Nature Computational Science, 2(9):595–604, 2022.\n[39] Laiyang Ke, Daniel T. O’Brien, and Babak Heydari. Airbnb and neighborhood crime: The\nincursion of tourists or the erosion of local social dynamics? PLoS one, 16(7):e0253315, 2021.\n[40] Babak Heydari, Ozlem Ergun, Rashmi Dyal-Chand, and Yakov Bart. Reengineering the Sharing\nEconomy: Design, Policy, and Regulation. Cambridge University Press, 2023.\n[41] K. Mazzitello, Y. Jiang, and C. Arizmendi. Optimising sars-cov-2 pooled testing strategies on\nsocial networks for low-resource settings. Journal of Physics A: Mathematical and Theoretical,\n54, 2020.\n21\n[42] Garry Robins,\nDean Lusher,\nChiara Broccatelli,\nDavid Bright,\nColin Gallagher,\nMaedeh Aboutalebi Karkavandi, Petr Matous, James Coutinho, Peng Wang, Johan Koski-\nnen, Bopha Roden, and Giovanni Radhitio Putra Sadewo. Multilevel network interventions:\nGoals, actions, and outcomes. Social Networks, 72:108–120, 2023.\n[43] Michael D Siciliano and Travis A Whetsell. Strategies of network intervention: A pragmatic\napproach to policy implementation and public problem resolution through network science.\narXiv preprint arXiv:2109.08197, 2021.\n[44] Andreea Ion and M. P˘atras,cu. A scalable algorithm for self-organization in event-triggered\nnetworked control systems. 2019 18th European Control Conference (ECC), pages 2725–2730,\n2019.\n[45] Jaimie Ellis, Ivaylo Vassilev, Elizabeth James, and Anne Rogers. Implementing a social\nnetwork intervention: can the context for its workability be created? a quasi-ethnographic study.\nImplementation Science Communications, 1:1–11, 2020.\n[46] Kenneth Joseph, Huei-Yen Winnie Chen, Stefania Ionescu, Yuhao Du, Pranav Sankhe, Aniko\nHannak, and Atri Rudra. A qualitative, network-centric method for modeling socio-technical\nsystems, with applications to evaluating interventions on social media platforms to increase\nsocial equality. Applied Network Science, 7(1):49, 2022.\n[47] Martin A Nowak. Five rules for the evolution of cooperation. science, 314(5805):1560–1563,\n2006.\n[48] David A Gianetto and Babak Heydari.\nNetwork modularity is essential for evolution of\ncooperation under uncertainty. Scientific reports, 5(1):1–7, 2015.\n[49] David A Gianetto and Babak Heydari. Catalysts of cooperation in system of systems: The role\nof diversity and network structure. IEEE Systems Journal, 9(1):303–311, 2013.\n[50] David A Gianetto and Babak Heydari. Sparse cliques trump scale-free networks in coordination\nand competition. Scientific reports, 6(1):1–11, 2016.\n[51] Mohsen Mosleh and Babak Heydari. Fair topologies: community structures and network hubs\ndrive emergence of fairness norms. Scientific reports, 7(1):1–9, 2017.\n[52] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139–144, 2020.\n[53] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and\nAnil A Bharath. Generative adversarial networks: An overview. IEEE signal processing\nmagazine, 35(1):53–65, 2018.\n[54] G. Kumari and A. Sowjanya. An integrated single framework for text, image and voice for\nsentiment mining of social media posts. Revue d’Intelligence Artificielle, 2022.\n[55] Sharma Tripti, Neetu Anand, K. Gaurav, and Rohit Kapoor. Image captioning generator\ntext-to-speech. International Journal of Next-Generation Computing, 2022.\n[56] Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, A. Polyak, Benjamin Bolte,\nTu Nguyen, Jade Copet, Alexei Baevski, A. Mohamed, and Emmanuel Dupoux. On generative\nspoken language modeling from raw audio. Transactions of the Association for Computational\nLinguistics, 9:1336–1354, 2021.\n[57] Gustavo H de Rosa and Joao P Papa. A survey on text generation using generative adversarial\nnetworks. Pattern Recognition, 119:108098, 2021.\n22\n[58] Partha Pratim Ray.\nChatgpt: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical\nSystems, 2023.\n[59] Thomas N Kipf and Max Welling.\nVariational graph auto-encoders.\narXiv preprint\narXiv:1611.07308, 2016.\n[60] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep\ngenerative models of graphs. arXiv preprint arXiv:1803.03324, 2018.\n[61] Yibo Li, Jianfeng Pei, and Luhua Lai. Structure-based de novo drug design using 3d deep\ngenerative models. Chemical science, 12(41):13664–13675, 2021.\n[62] Yong Zhao, Edirisuriya M. D. Siriwardane, and Jianjun Hu. Physics guided deep learning\ngenerative models for crystal materials discovery. ArXiv, abs/2112.03528, 2021.\n[63] Kushal Veer Singh, Ajay Kumar Verma, and L. Vig. Deep learning based network similarity for\nmodel selection. Data Sci., 4:63–83, 2021.\n[64] Xiaojie Guo and Liang Zhao. A systematic survey on deep generative models for graph\ngeneration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):5370–5390,\n2022.\n[65] Yu Cheng, Yongshun Gong, Yuansheng Liu, Bosheng Song, and Quan Zou. Molecular design in\ndrug discovery: a comprehensive review of deep generative models. Briefings in bioinformatics,\n22(6):bbab344, 2021.\n[66] Hojjat Navidan, Parisa Fard Moshiri, Mohammad Nabati, Reza Shahbazian, Seyed Ali Gho-\nrashi, Vahid Shah-Mansouri, and David Windridge. Generative adversarial networks (gans) in\nnetworking: A comprehensive survey & evaluation. Computer Networks, 194:108149, 2021.\n[67] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[68] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Advances in neural\ninformation processing systems, pages 1057–1063, 2000.\n[69] Arash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep re-\ninforcement learning. In Proceedings of the aaai conference on artificial intelligence, volume 32,\n2018.\n[70] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.\nDueling network architectures for deep reinforcement learning. In International conference on\nmachine learning, pages 1995–2003. PMLR, 2016.\n[71] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n[72] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and\napplications. AI open, 1:57–81, 2020.\n[73] Lu He, Changyang He, Tera L Reynolds, Qiushi Bai, Yicong Huang, Chen Li, Kai Zheng, and\nYunan Chen. Why do people oppose mask wearing? a comprehensive analysis of us tweets\nduring the covid-19 pandemic. Journal of the American Medical Informatics Association,\n28(7):1564–1573, 2021.\n23\n[74] Koen van der Zwet, Ana I Barros, Tom M van Engers, and Peter MA Sloot. Emergence of\nprotests during the covid-19 pandemic: quantitative models to explore the contributions of\nsocietal conditions. Humanities and Social Sciences Communications, 9(1), 2022.\n24\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA"
  ],
  "published": "2024-10-30",
  "updated": "2024-10-30"
}