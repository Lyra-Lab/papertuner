{
  "id": "http://arxiv.org/abs/2411.02280v2",
  "title": "The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units",
  "authors": [
    "Badr AlKhamissi",
    "Greta Tuckute",
    "Antoine Bosselut",
    "Martin Schrimpf"
  ],
  "abstract": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.",
  "text": "The LLM Language Network:\nA Neuroscientific Approach for Identifying Causally Task-Relevant Units\nBadr AlKhamissi1\nGreta Tuckute2\nAntoine Bosselut*,1\nMartin Schrimpf∗,1\n1EPFL\n2MIT\nAbstract\nLarge language models (LLMs) exhibit re-\nmarkable capabilities on not just language\ntasks, but also various tasks that are not lin-\nguistic in nature, such as logical reasoning\nand social inference.\nIn the human brain,\nneuroscience has identified a core language\nsystem that selectively and causally supports\nlanguage processing.\nWe here ask whether\nsimilar specialization for language emerges in\nLLMs. We identify language-selective units\nwithin 18 popular LLMs, using the same local-\nization approach that is used in neuroscience.\nWe then establish the causal role of these units\nby demonstrating that ablating LLM language-\nselective units – but not random units – leads\nto drastic deficits in language tasks.\nCorre-\nspondingly, language-selective LLM units are\nmore aligned to brain recordings from the hu-\nman language system than random units. Fi-\nnally, we investigate whether our localization\nmethod extends to other cognitive domains:\nwhile we find specialized networks in some\nLLMs for reasoning and social capabilities,\nthere are substantial differences among mod-\nels.\nThese findings provide functional and\ncausal evidence for specialization in large lan-\nguage models, and highlight parallels with the\nfunctional organization in the brain.1\n1\nIntroduction\nRecent advancements in large language models\n(LLMs) have revealed their potential to perform\nfar more than language processing tasks, show-\ncasing abilities in reasoning (Sun et al., 2023),\nproblem-solving (Giadikiaroglou et al., 2024), and\neven mimicking aspects of human Theory of\nMind (Street et al., 2024). Despite these impres-\nsive feats, the internal workings of LLMs remain\npoorly understood, especially in relation to how\n*Equal Supervision\n1Code available via github.com/bkhmsi/llm-localization\nLesion Units\n2\nLanguage Unit\nRandom Unit\nAblated Unit\nTHE DOG CHASED THE CAT ALL DAY LONG\nLUT REE UMLY LOND E WAM GOVING HOM\nSentence:\nNon-Words:\nExtract Top-K \nLanguage Selective Activations\nSentence\nNon-Words\nContrast\nExtract Top-K \nLanguage Selective Activations\nSentence\nNon-Words\nContrast\nMethod: Fedorenko et al. (2010)\nOur Method\nLocalizing Language Selective Units from the Brain and Models \nLocalizing Language Units\n1\nPerformance Deficits\n3\nReasoning (MD)\nLanguage\nLanguage Units\nSocial Inferences (ToM)\nRandom Control\nn=3\nn=10\nn=4\n**\n*\n***\nThe\nquick \nbrown\nfox\njumps\nover the\nlazy dog\nfor =\nhad . \nhad ' of\nthe \nn=6\nn.s.\nn=7\nn.s.\nFigure 1: Identifying Specialized and Causally Task-\nRelevant Units in LLMs. (1) To identify language-\nselective units, we compare unit activations in response\nto language (sentences) versus a matched control con-\ndition (lists of non-words), and identify the units that\nexhibit the strongest selectivity to sentences over non-\nwords. The same method is used in neuroscience to\nlocalize the human brain’s language network (e.g., Fe-\ndorenko et al., 2010). (2) Testing the causal role of\nthe identified language-selective units, we ablate those\nunits as well as a set of random units, and (3) com-\npare the resulting performance drop. Ablating 1% of\nLLM language units leads to vast language deficits\n(p < 5−238) for all models tested. Beyond language,\nonly a few models exhibit specialization for reasoning\n(n=3, p < 5−2, Multiple Demand network) and social\ninferences (n=4, p < 5−5, Theory of Mind network).\nPlots averaged across n LLMs each; random control re-\npeated with 3 different seeds.\nspecific components of these models contribute to\narXiv:2411.02280v2  [cs.CL]  13 Feb 2025\nModel\nAblate Language Units\nAblate Random Units\nGemma-2B\n11 liquido\nsota(.)uggoon3\njumped over the lazy lamb.\nPhi-3.5-Mini-Instruct\nAME.AME and:ough.. MAR\njumps over the lazy dog.\nFalcon-7b\nSomeSReadWhenISearchSome\njumps over the lazy dog.\nMistral-7B-v0.3\nfoxfool foolfoolfoolfool\njumps over the lazy dog.\nLLaMA-3.1-8B-Instruct\nof An O of An O of\njumps over the lazy dog.\nTable 1: Disruption of Language Modeling Abilities Continuations of the prompt “The quick brown fox” across\nfive different models, following the ablation of the top 1% of language-selective units compared to the ablation\nof an equivalent number of randomly selected units. The baseline generation without ablation for all models was\n“jumps over the lazy dog.”\nmanifesting distinct cognitive functions.\nThe field of neuroscience has made significant\nstrides in mapping out the functional organiza-\ntion of the human brain, for instance by identi-\nfying specialized cognitive networks such as the\nlanguage network (Fedorenko et al., 2010, 2024),\nthe Multiple Demand network (Duncan, 2010; As-\nsem et al., 2020b), and the Theory of Mind net-\nwork (Saxe and Kanwisher, 2013), each under-\nlying distinct cognitive behaviors. In this paper,\nwe draw inspiration from neuroscience to investi-\ngate whether similar functional specialization ex-\nists within LLMs.\nSpecifically, we use the same localizer exper-\niments developed by neuroscientists to identify\nfunctional brain regions. These experiments con-\ntrast activations between target conditions of in-\nterest (e.g., sentences) and perceptually matched\ncontrol conditions (see Section 3). We discover\nthat, much like the human brain, there exists a\nset of units in LLMs that are critical for lan-\nguage processing, analogous to the human lan-\nguage network (Fedorenko et al., 2024, Fig. 2).\nWe find that these units show similar response pat-\nterns as those observed in the human language\nareas (Shain et al., 2024; Schrimpf et al., 2021),\nand, moreover, demonstrate selectivity for nat-\nural language compared to mathematical equa-\ntions and computer code, much like the human\nbrain (Ivanova et al., 2020; Fedorenko et al., 2011,\n2024).\nFurther, ablating even a small percentage of\nthese language-selective units results in a signif-\nicant decline in language performance, demon-\nstrated qualitatively in Table 1 and quantitatively\nin Figure 3 through benchmarks like SyntaxGym\n(Gauthier et al., 2020), BLiMP (Warstadt et al.,\n2019), and GLUE (Wang et al., 2018). Finally,\nthe language-selective units show stronger align-\nment with the brain’s language network compared\nto randomly sampled units, especially when se-\nlecting a small number of units to predict brain ac-\ntivity (Figs. 4, 5). Despite substantial evidence for\nthe existence of a language network in all LLMs\nwe tested, we only found evidence of units se-\nlective for social (Theory of Mind) and reason-\ning (Multiple Demand) tasks in a subset of models\n(Figure 6).\n2\nPreliminaries\nThe Human Language Network.\nThe human\nlanguage network comprises a set of brain re-\ngions that are functionally defined by their in-\ncreased activity to language inputs over percep-\ntually matched controls (e.g., lists of non-words)\n(Fedorenko et al., 2010; Lipkin et al., 2022, Sec-\ntion 3). These regions are predominantly localized\nin the left hemisphere, within frontal and temporal\nareas, and demonstrate a strong selectivity for lan-\nguage processing over various non-linguistic tasks\nsuch as music perception (Fedorenko et al., 2012;\nChen et al., 2023) and arithmetic computation (Fe-\ndorenko et al., 2011; Monti et al., 2012). Crucially,\nthese regions exhibit only weak activation in re-\nsponse to meaningless non-word stimuli, whether\nduring comprehension or production (Fedorenko\net al., 2010; Hu et al., 2023). This high degree of\nselectivity is well-established through neuroimag-\ning studies and is further supported by behav-\nioral data from aphasia studies: In individuals with\ndamage confined to these language areas, linguis-\ntic abilities are significantly impaired, while other\ncognitive functions—such as arithmetic computa-\ntions (Benn et al., 2013; Varley et al., 2005), gen-\neral reasoning (Varley and Siegal, 2000), and The-\nory of Mind (Siegal and Varley, 2006)—remain\nlargely intact.\nIn addition to language-specific\nsystems, the brain supports higher-level cognition\nthrough distinct networks that handle demanding\ntasks and social reasoning.\nThe Multiple Demand Network.\nThe Multi-\nple Demand Network (MD), encompassing bilat-\neral frontal, parietal, and temporal regions, is acti-\nvated during cognitively demanding tasks, show-\ning a consistent “hard > easy” response across\nvarious task types (e.g., spatial, verbal, mathemat-\nical; Duncan and Owen, 2000; Fedorenko et al.,\n2013; Shashidhara et al., 2020). This network un-\nderpins key cognitive functions such as working\nmemory, cognitive control, and attention, and is\nlinked to fluid intelligence (Woolgar et al., 2010;\nAssem et al., 2020a).\nThe Theory of Mind Network.\nThe Theory\nof Mind (ToM) network, primarily located in\nthe bilateral temporo-parietal junction and corti-\ncal midline, is involved in reasoning about men-\ntal states—whether one’s own or others’ (Saxe and\nKanwisher, 2003; Gallagher et al., 2000; Saxe and\nPowell, 2006). Functionally and anatomically dis-\ntinct from the language network, the ToM network\nis engaged across different content types (e.g., ver-\nbal, non-verbal) and is engaged in understand-\ning non-literal language such as sarcasm, and for\ndiscourse comprehension where multiple perspec-\ntives need to be inferred (Koster-Hale and Saxe,\n2013; Hauptman et al., 2023).\n3\nLocalizing the Language Network\nThe human language network is defined function-\nally rather than anatomically which means that\nunits are chosen according to a ‘localizer’ exper-\niment (Saxe et al., 2006). Specifically, the lan-\nguage network is the set of neural units (e.g.,\nvoxels/electrodes) that are more selective to sen-\ntences over a perceptually-matched control con-\ndition (e.g., lists of nonwords) (Fedorenko et al.,\n2010) as illustrated in Figure 1. In previous stud-\nies comparing artificial models to brain activity\nin the language network, units were selected by\nevaluating representations at different model lay-\ners and choosing the ones that maximized brain\nalignment (Schrimpf et al., 2021; Goldstein et al.,\n2022; Caucheteux and King, 2022; Tuckute et al.,\n2024b). However, LLMs learn diverse concepts\nand behaviors during their considerable pretrain-\ning, not all of which are necessarily related to lan-\nguage processing. Therefore, we here characterize\nthe language units in LLMs using functional lo-\ncalization as is already standard in neuroscience.\nThis approach comes with the advantage of com-\nparability across different models since we can\nchoose a fixed set of units which are localized in-\ndependently of the critical experiment or modality.\nSpecifically, we present each LLM with 240\nunique 12-word-long sentences and 240 unique\nstrings of 12 non-words used in neuroscience (Fe-\ndorenko et al., 2010), ensuring matched sequence\nlengths across conditions.\nHuman participants\nare typically exposed to 96 sentences/non-word\nstrings during an experimental fMRI session (Lip-\nkin et al., 2022). We then capture the activations\nfrom the units at the output of each Transformer\nblock for each stimulus. We define the model’s\nlanguage network as the top-k units that maxi-\nmize the difference in activation magnitude be-\ntween sentences and strings of non-words, mea-\nsured by positive t-values from a Welch’s t-test.\nThis localization method selects a targeted set of\nunits across the entire network, rather than restrict-\ning the representations to a single a priori stage as\ndone in prior work (Schrimpf et al., 2021; Gold-\nstein et al., 2022; Caucheteux and King, 2022;\nTuckute et al., 2024b). We examine unit activa-\ntions after each Transformer block. For instance,\nfor a model like LLAMA-3-8B (Dubey et al.,\n2024) which consists of 32 Transformer blocks\nand a hidden dimension of 4096, we consider\n32×4096 = 131, 072 units, from which we select\nthe most language selective units as the model’s\n“language network”.\n4\nExperimental Setup\nModels\nWe selected 10 autoregressive LLMs\nfrom a diverse range of model families to\ninvestigate\ntheir\nlanguage-selective\nunits:\nGPT2-{LARGE, XL} (Radford et al., 2019),\nLLAMA-2-{7B, 13B} (Touvron et al., 2023),\nLLAMA-3.1-8B-INSTRUCT\n(Dubey\net\nal.,\n2024), MISTRAL-7B-V0.3 (Jiang et al., 2023),\nFALCON-7B (Almazrouei et al., 2023), PHI-\n3.5-MINI-INSTRUCT (Abdin et al., 2024), and\nGEMMA-{2B, 7B} (Team et al., 2024).\nThe\nmodels were downloaded from the HuggingFace\nplatform (Wolf et al., 2019).\nLanguage Benchmarks\nTo assess the signifi-\ncance of the localized units on the models’ linguis-\ntic abilities, we utilize three widely used bench-\nmarks that measure formal linguistic competence\n(Mahowald et al., 2024). First, SyntaxGym (Gau-\nLLaMA-3.1-8B-Instruct\nMistral-7B-v0.3\nFaclon-7B\nPhi-3.5-Mini-Instruct\nGemma-2B\n(a)\n(b)\nFigure 2: Distribution of Language Units Across Layers. (a) The distribution of the top 1% most language-\nselective units across layers in a sample of five different models. The models are displayed from top to bottom,\nwith each layer labeled by the percentage of units identified as belonging to the top 1% language-selective units.\n(b) The language selectivity index for all models in the study (n=18) plotted against the relative depth of the layers.\nthier et al., 2020) offers 30 subtasks focused on\nevaluating syntactic knowledge. Second, BLiMP\n(Warstadt et al., 2019) contains 67 subtasks, each\nconsisting of 1,000 minimal pairs designed to\ntest contrasts in syntax, morphology, and seman-\ntics. Third, GLUE (Wang et al., 2018) includes 8\nsubtasks aimed at assessing the models’ broader\nlanguage understanding.\nThe models are eval-\nuated by calculating the negative log-likelihood\nof each candidate answer given the context, se-\nlecting the candidate that minimizes surprisal as\nthe model’s prediction. This method, commonly\nused in psycholinguistics, has been shown to cor-\nrelate with human behavioral measures (Smith and\nLevy, 2013). SyntaxGym and BLiMP are evalu-\nated in a zero-shot setting, while GLUE tasks are\ntested with 2-shot examples in context to achieve\nreasonable performance in the non-ablation set-\nting.\nBrain Alignment Benchmarks\nTo validate the\nmodel language units’ alignment to the human lan-\nguage network, we employ two approaches: i)\ninvestigating whether the model units can repli-\ncate landmark neuroscience studies that quali-\ntatively describe the response profiles observed\nin the human language regions, and ii) quanti-\ntatively testing the alignment of language units\nwith brain responses from the human language\nnetwork. For the first approach, we closely fol-\nlow the analyses in Fedorenko et al. (2010) and\nShain et al. (2024), using the same set of exper-\nimental conditions which are commonly used in\nneuroimaging studies examining lexical and syn-\ntactic processing. For the second approach, we\nmeasure how well the language units can predict\nbrain activity in the human language network. To\ndo so, we utilize the TUCKUTE2024 (Tuckute\net al., 2024b) and PEREIRA2018 (Pereira et al.,\n2018) benchmarks on the Brain-Score platform\n(Schrimpf et al., 2018, 2020).\nTUCKUTE2024\nconsists of brain responses from 5 participants\nwho each read 1,000 linguistically diverse sen-\ntences, while PEREIRA2018 consists of 15 sub-\njects reading short passages presented one sen-\ntence at a time spanning various different top-\nics. See Appendix F for more details about the\ndatasets.\n5\nA Specialized Language Network in\nLLMs\nFigure 2(a) shows the percentage of language units\nin each layer that belong to the top 1% of the\nmost selective units for five models analyzed in\nthis study (additional heatmaps for other models\ncan be found in Appendix B). Figure 2(b) demon-\nstrates a language selectivity index against the rel-\native depth of each layer across all models tested.\nThis index is calculated by summing 1 −p-values\nfor each unit where p < 0.05 after false discovery\ncorrection, and normalizing by the hidden dimen-\nsion size.\n6\nThe LLM Language Network is\nCausally Involved in Language\nProcessing\nTable 1 qualitatively illustrates the disruption in\nlanguage modeling abilities when 1% of language-\nselective units are ablated, in contrast to no dis-\nruption when an equivalent set of randomly sam-\npled units is ablated. To quantify this effect, Fig-\n(a)\n(b)\n(c)\n(d)\nSyntaxGym\nBLiMP\nGLUE\nGLUE Subtasks (1% Lesion)\nFigure 3: Lesion Studies. The average performance change after ablating the top x% of language-selective units,\ncompared to ablating three random sets of units for each model. Performance is evaluated across 10 models and\nthree language benchmarks: (a) SyntaxGym, (b) BLiMP, and (c) GLUE, with (d) presenting results for individual\nsubtasks within GLUE when ablating the top 1% of language units.\nure 3 shows the average change in performance\nacross the 10 LLMs after ablating the top-{0.125,\n0.25, 0.5, 1}% of language-selective units for a\nset of three language benchmarks which measure\nformal linguistic competencies (Mahowald et al.,\n2024). For comparison, we also measure perfor-\nmance changes after ablating an equivalent num-\nber of units randomly sampled from the remain-\ning units in the model (e.g., if 0.125% of the most\nlanguage-selective units are ablated, the random\nunits are sampled from the remaining 99.875%),\nsome of which may also have significant language\nselectivity.\nRandom sampling results are aver-\naged over three seeds for each model. The results\nunderscore the distinct role of language-selective\nunits: ablating just 0.125% of these units leads\nto a notable performance drop across all three\nbenchmarks (Cohen’s d = 0.8, large effect size;\np < 5−43). In contrast, ablating the same number\nof randomly sampled units has minimal impact on\nperformance (Cohen’s d = 0.1, small effect size;\np = 2−4), highlighting the unique contribution of\nlanguage-selective units to the model’s linguistic\ncapabilities. We found that not all tasks are im-\npacted equally (Figure 3(d)): within GLUE, lin-\nguistic acceptability (COLA) and sentiment anal-\nysis (SST2) experience much more drastic perfor-\nmance deficits compared to Question NLI (QNLI)\nand Winograd NLI (WNLI). This variation may be\nattributable to the reliance on other non-language-\nspecific units. We report the fine-grained results\nper model in Appendix D.\n7\nSimilarity Between the Language\nNetwork in LLMs and Brains\nQualitatively Similar Response Profiles Be-\ntween the Language Network in LLMs and\nBrains.\nIn this analysis, we record the responses\nof the localized units to the exact stimuli from\nfour experimental conditions used in neuroscien-\ntific studies (Fedorenko et al., 2010; Shain et al.,\n2024), along with a set of non-linguistic stimuli\nsuch as arithmetic equations and code. This al-\nlows us to assess how well the selectivity of lo-\ncalized language units generalizes to new stimuli\nfrom the same conditions (sentences and strings\nof non-words) and how well they map onto re-\nsults from neuroscience (Amalric and Dehaene,\n2019; Ivanova et al., 2020; Fedorenko et al.,\n2024).\nStimuli are presented in four condi-\ntions (examples in Figure 4a):\nSentences ,\ndenoted as S, are well-formed sentences con-\ntaining both lexical and syntactic information.\nUnconnected Words , W, are scrambled sen-\ntences containing lexical but not syntactic in-\nformation.\nJabberwocky Sentences , J,\nwhere content words are replaced by pronounce-\nable non-words (such as “pront”, or “blay”),\nthus containing syntactic but not lexical informa-\ntion.\nUnconnected Non-Words , N, which\nare scrambled Jabberwocky sentences containing\nneither lexical nor syntactic information.\nNote\nthat we use a disjoint set of Sentences and\nNon-Words for the original functional localiza-\ntion (Section 3). The brain’s language regions are\nhighly sensitive to linguistic structure: responses\nto S are numerically higher than all other condi-\ntions (Fedorenko et al., 2010; Shain et al., 2024;\nFedorenko et al., 2024).\nThe LLM language units exhibit a similar re-\nsponse pattern to that of the brain’s language net-\nwork (Figure 4c, first 4 bars). Consistent with hu-\nman neuroscience (Fedorenko et al., 2011; Amal-\nric and Dehaene, 2019; Ivanova et al., 2020), LLM\nlanguage units are particularly selective for nat-\nural language compared to arithmetic equations,\nC++ code, or random sequences of characters (all\nmatching the number of tokens and samples in\nFigure 4: Language-Selective Model Units Are Selective for Language and Exhibit Similar Response Profiles\nas the Language Network in the Brain. Brain (green) and model (blue) responses for Univariate Condition-Level\nResponses. (a) Examples of the four experimental conditions used in this analyses with the ‘+/-’ signs denoting\nwhether the condition contains lexical or syntactic information, respectively. (b) Human language network re-\nsponses to the four conditions; data from (Shain et al., 2024). Brain activity is strongest to S, followed by W and\nJ, and weakest to N. (c) Language-selective unit responses to the four conditions averaged across 10 models and\ncondition samples. (d) Control responses from random units averaged across condition samples and 10 models,\nwith 3 random seeds each.\n*\n**\n*\nn.s.\n**\nn.s.\n**\nn.s.\n*\nn.s.\nn.s.\nn.s.\nFigure 5: Language Units are Aligned to Brain Data. Raw Pearson correlation between predicted brain activity\nfrom the x% of model units and actual brain activity in the human language network across 10 models. The\nalignment of language-selective units shows significantly greater correlation compared to the average of three sets\nof randomly selected units when selecting a small subset of units. Error bars represent 95% confidence intervals\ncalculated across models. See Table 11 for the number of units corresponding to each percentage level per model.\nthe other conditions). In contrast, responses from\nthree sets of randomly sampled units show a dif-\nferent response profile (Figure 4d), demonstrating\nthat the language response profile is not ubiqui-\ntously present throughout the LLMs.\nQuantitative Alignment Between the Language\nNetwork in LLMs and Brains.\nBeyond quali-\ntative alignment between LLM language units and\nbrains, we investigate the quantitative alignment\nto brain data. Following standard practice in mea-\nsuring brain alignment, we train a ridge regression\nto predict brain activity from model representa-\ntions, using the same input stimuli presented to hu-\nman participants in neuroimaging studies (Nase-\nlaris et al., 2011; Schrimpf et al., 2021). We then\nmeasure the Pearson correlation between the pre-\ndicted brain activations and the actual brain acti-\nvations of human participants on a held-out set.\nThis process is repeated over 10 cross-validation\nsplits, and we report the average (mean) Pearson\ncorrelation as our final result which we here refer\nto as Brain-Score (Schrimpf et al., 2018, 2020).\nFigure 5 shows the average raw correlation when\nusing {0.03125, 0.0625, 0.125, 0.25, 0.5, 1}% of\nmodel units to predict brain activity for two neu-\nral datasets (Pereira et al., 2018; Tuckute et al.,\n2024b). This analysis is repeated for the most lan-\nguage selective units, and for three sets of ran-\ndomly sampled units for each of the 10 models.\nSee Appendix E for more statistical tests and Ap-\npendix F for more dataset details.\n8\nLocalizing the Multiple Demand &\nTheory of Mind Networks\nThe results so far suggest that the functional lo-\ncalization methods used in neuroscience to iden-\ntify the brain’s language network also applies ef-\nfectively to LLMs. This raises a natural question:\ncan we use the same localizers designed to iden-\ntify other brain networks, such as the Theory of\nFigure 6: Multiple Demand and Theory of Mind lesion study. Change in performance on the (top) MATH\nmultiple-choice benchmark as a function of the difficulty level, and the (bottom) TOMI multiple-choice benchmark,\ncategorized by whether the question involves a false-belief or true-belief scenario. Results are shown after ablating\nthe top 1% of MD-selective and ToM-selective units respectively as well as an equivalent number of random units\n(across 3 seeds). (a,d) Average performance change for MATH/TOMI, across all 10 models. (b,e) Models where\nablating MD/ToM units leads to a greater performance drop on difficult/false-belief problems compared to random\nunit ablation. (c,f) Sample of models showing no difference between ablating MD/ToM units and random units.\nMind network or the Multiple Demand network,\nto discover analogous networks in LLMs?\n8.1\nFunctional Localizers\nMultiple Demand Network\nTo localize the\nMultiple Demand (MD) network, neuroscientists\ntypically use either spatial or arithmetic tasks that\ncompare brain activations during a cognitively de-\nmanding problem (a “hard” task) with those dur-\ning an easier one (Fedorenko et al., 2013). In this\nwork, we adapted the arithmetic MD localizer into\na verbal format to explore whether a similar net-\nwork can be identified in LLMs. Instead of using\njust the representation of the final token (as was\ndone for localizing the language network), we av-\nerage the activations across all tokens in the con-\ntext before comparing the two stimulus sets. More\ndetails about the localizer stimuli can be found in\nAppendix A.2.\nTheory of Mind Network\nDodell-Feder et al.\n(2011) developed an efficient localizer to iden-\ntify brain regions involved in Theory of Mind\n(ToM) and social cognition in individual partici-\npants. This was achieved by contrasting brain ac-\ntivation during false-belief stories—where charac-\nters hold incorrect beliefs about the world—with\nactivation during false-photograph stories, where\na photograph, map, or sign depicts an outdated or\nmisleading world state. The false-photograph sto-\nries are verbalized to match the presentation style\nof the false-belief stories for consistency in the\nexperiment. Each stimuli set consists of only 10\nsamples, which are sufficient to robustly identify\nthe ToM network in the brain. Similar to the MD\nlocalizer, we average the activations across all to-\nkens in the context before comparing the two stim-\nulus sets. See Appendix A.3 for more details.\n8.2\nBenchmarks\nMATH.\nThe Multiple Demand (MD) network is\ninvolved during cognitively demanding tasks such\nas mathematical reasoning. Therefore, to evaluate\nthe effectiveness of the MD localizer, we use the\nmultiple choice version of the MATH benchmark\n(Hendrycks et al., 2021) introduced by Zhang et al.\n(2024). It consists of math questions encompass-\ning several topics ranging from “Counting & Prob-\nability” to “Geometry” and “Algebra”. There are\n4,914 questions categorized into 5 levels of diffi-\nculty, and each one contains 4 candidate answers\npresented to the model.\nToMi.\nTo evaluate the Theory of Mind (ToM)\nabilities of the model, we used the TOMI QA\ndataset preprocessed by Sap et al. (2022), focus-\ning only on questions that require first-order ToM\nreasoning. The dataset consists of 620 stories gen-\nerated through a stochastic rule-based algorithm,\nwhich involves selecting two participants, an ob-\nject of interest, and a set of locations or contain-\ners. These elements are combined into a narra-\ntive where the object is moved, and questions are\nasked about either the object’s original location or\nits final location (Le et al., 2019). The questions\ninclude both false-belief scenarios, where a par-\nticipant was absent when the object was moved,\nand true-belief scenarios, where the participant\nwas present. The task requires the model to in-\nfer the “mental states” of the characters and the\nrealities of the situation in the story. Each sam-\nple presents the model with an instruction, the\nstory, the question, and two possible answers. The\nmodel’s response is the answer that minimizes sur-\nprisal, measured by the negative log-likelihood.\n8.3\nModels\nGiven the complexity of the benchmarks used to\nevaluate higher-level cognitive networks, which\nrequire advanced reasoning abilities and mod-\nels that are capable of following instructions for\nzero-shot evaluation, we transitioned all mod-\nels to instruction-tuned versions.\nAdditionally,\nwe included QWEN2.5-{3B, 7B}-INSTRUCT and\nLLAMA-3.2-3B-INSTRUCT to maintain a con-\nsistent sample size of 10 models, matching those\nused in the language evaluations.\n8.4\nResults\nWe repeat the lesion analysis performed on the\nlanguage network for the Theory of Mind (ToM)\nand Multiple Demand (MD) selective units (top\n1%). After identifying units with the functional lo-\ncalizers discussed in Section 8.1, we measure the\nchange in performance following the ablation of\nthe most selective units.\nMultiple Demand.\nFigure 6(a-c) illustrates the\nchange in performance on the MATH multiple-\nchoice benchmark for a sample of models, broken\ndown by difficulty level. For a specialized LLM\nMultiple Demand network, we would expect a\ngreater performance drop as the question difficulty\nincreases, reflecting a more “cognitively demand-\ning” task.\nThis pattern is evident in LLAMA-\n2-13B-CHAT, GEMMA-1.1-7B-INSTRUCT, and\nPHI-3.5-MINI-INSTRUCT, but is less pronounced\nin other models. See Appendix D for results on all\nmodels.\nTheory of Mind.\nSimilar to the MD results,\nToM findings are incosistent across models. Fig-\nure 6(d-f) shows the results on a sample of mod-\nels on the TOMI benchmark when ablating the\nmost selective ToM units and three sets of random\nunits. We differentiate between results for ques-\ntions that involves false-belief scenarios and true-\nbelief ones. Our results indicate that we can lo-\ncalize specialized units for certain models, such as\nMISTRAL-7B-INSTRUCT, but not for others, like\nPHI-3.5-MINI-INSTRUCT. This differs from the\nfindings related to the language network, where\ntrends were consistent across all models (see Ap-\npendix D).\n9\nDiscussion\nSpecialized LLM Language Units.\nOur find-\nings provide compelling evidence that specialized\nlanguage units emerge within LLMs.\nIt is par-\nticularly surprising how effectively we can iden-\ntify these units with the same limited set of lo-\ncalization stimuli employed in neuroscience, and\nthat they prove to be causally relevant for lan-\nguage tasks.\nWhile we observed consistent re-\nsults across all 10 models we tested, it remains an\nopen question whether this specialization is uni-\nversal across all LLMs and under which condi-\ntions this specialization does or does not emerge.\nFor instance, does the nature of the training data or\nthe specific training objective influence the emer-\ngence of these specialized units? Moreover, the\nrole of non-language-selective units remains un-\nclear. It is possible they contribute to other spe-\ncialized systems. While our experiments with the\nMultiple Demand and Theory of Mind selective\nunits yielded some promising results, the variabil-\nity across models suggests that these systems may\neither emerge more sparsely or be more complex\nor challenging to identify.\nConsistency with the Brain’s Language Net-\nwork.\nOur paper adds to the growing body of\nresearch that uses neuroscience tools to interpret\nmachine learning models (Schrimpf et al., 2020,\n2021; Zador et al., 2023; Tuckute et al., 2024a).\nSpecifically, our work takes a step towards ana-\nlyzing LLM units that are causally useful within\na given system, providing a more stringent no-\ntion of functional correspondence (Cao, 2022; Cao\nand Yamins, 2024; Mahon, 2023; Prince et al.,\n2024). The consistency between the causally im-\nportant language units in LLMs and the human\nbrain may suggest that computations, beyond rep-\nresentations, could be shared between these two\nsystems. This prompts an intriguing question: do\nspecialized subsystems, such as the language net-\nwork, always emerge as a consequence of optimiz-\ning for next-word prediction, and is such a simple\nobjective the driver of specialization in the brain?\nExploring this connection further could shed light\non how cognitive processes evolve from such pre-\ndictive mechanisms.\nRelated Work\nPrevious work has identified a\ncore language system within LLMs (Zhao et al.,\n2023), but their approach requires finetuning the\nmodel on a next-token prediction task to locate\nparameters that exhibit minimal variation during\nfinetuning. In contrast, our method bypasses addi-\ntional training and leverages established research\nfrom language neuroscience. Concurrently, Sun\net al. (2024) have shown that LLMs exhibit brain-\nlike functional organization by using regressors\nto predict brain activity based on artificial neu-\nron responses, and thereby mapping LLM repre-\nsentations onto the brain. However, their method\nis computationally intensive and lacks the cog-\nnitive neuroscience grounding that underpins our\napproach. Other efforts have focused on identi-\nfying subnetworks that encode factual knowledge\n(Meng et al., 2022; Bayazit et al., 2023; Hernan-\ndez et al., 2023) and task-specific skill neurons\n(Panigrahi et al., 2023).\nFuture Work.\nExtending the analyses presented\nhere to multimodal models could shed light on\nwhether specialized Multiple Demand and Theory\nof Mind units are also responsive to non-linguistic\ninputs, regardless of the input modality (e.g., vi-\nsual or auditory stimuli). This investigation aligns\nwith the emergent modularity observed in the\nbrain, where these networks are robustly dissocia-\nble from language (Mahowald et al., 2024).\nIn\ncontrast, this dissociation is not evident in LLMs:\nablating the language units renders the model inca-\npable of understanding input sentences and, con-\nsequently, unable to perform any task presented\nverbally. This limitation applies to all tasks, as the\ninput to LLMs is solely language-based.\n10\nConclusion\nIn this paper, we explored whether functional spe-\ncialization observed in the human brain can be\nidentified in LLMs.\nDrawing inspiration from\nneuroscience, we applied the same localizers used\nin human neuroscience, to uncover language-\nselective units within LLMs, showing that a small\nsubset of these units are crucial for language mod-\neling.\nOur lesion studies revealed that ablating\neven a fraction of these units leads to signifi-\ncant drops in language performance across mul-\ntiple benchmarks, while randomly sampled non-\nlanguage units had no comparable effect.\nAl-\nthough we successfully identified a language net-\nwork analog in all models studied, we found\nmixed results when applying similar localization\ntechniques to Theory of Mind and Multiple De-\nmand networks, suggesting that not all cognitive\nfunctions neatly map onto current LLMs. These\nfindings provide new insights into the internal\nstructure of LLMs and open up avenues for further\nexploration of parallels between artificial systems\nand the human brain.\nLimitations\nOur analysis focused on models smaller than 13\nbillion parameters, which may not capture the spe-\ncialization that could emerge in larger models,\nsuch as those with 70 billion parameters.\nAd-\nditionally, we evaluated Theory of Mind (ToM)\nand Multiple Demand (MD) units using just one\nbenchmark for each: TOMI QA for ToM and a\nmathematical reasoning task (MATH) for MD.\nWhile these benchmarks provided initial insights,\nthey do not offer a comprehensive evaluation of\nthese cognitive systems since our main focus was\nanalyzing the language-selective units and their re-\nlationship to the human language network. Future\nwork will involve expanding our study to include\nmore models and a broader set of benchmarks to\nensure robustness and generalizability. We also\nplan to analyze varying numbers of selective units\nfor the MD and ToM networks, as this study fo-\ncused only on the top 1% which might not reflect\nthe total number of units specialized for cogni-\ntively demanding tasks.\nMoreover, the localizers we used to identify\nspecialized units were adapted from neuroscience.\nWhile these methods allowed us to draw mean-\ningful comparisons between LLMs and the brain,\nthey are constrained by the stimuli sets tradition-\nally used in neuroscience. Future work will con-\nsider developing more targeted and robust localiz-\ners that are not restricted by the same limitations,\nenabling deeper investigation into the specializa-\ntion of LLMs across different tasks and domains.\nEthical Statement\nThis research focuses on understanding the inter-\nnal mechanisms of existing large language models\n(LLMs) by drawing parallels to human cognitive\nsystems. Our work is aimed at advancing scientific\nknowledge in the field of AI and neuroscience and\ndoes not involve any human or animal subjects.\nReferences\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-\nrat Behl, et al. 2024.\nPhi-3 technical report: A\nhighly capable language model locally on your\nphone. arXiv preprint arXiv:2404.14219.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM´erouane Debbah, ´Etienne Goffinet, Daniel Hess-\nlow, Julien Launay, Quentin Malartic, et al. 2023.\nThe falcon series of open language models. arXiv\npreprint arXiv:2311.16867.\nMarie Amalric and Stanislas Dehaene. 2019. A distinct\ncortical network for mathematical knowledge in the\nhuman brain. NeuroImage, 189:19–31.\nMoataz Assem, Idan A. Blank, Zachary Mineroff, Ah-\nmet Ademo˘glu, and Evelina Fedorenko. 2020a. Ac-\ntivity in the fronto-parietal multiple-demand net-\nwork is robustly associated with individual differ-\nences in working memory and fluid intelligence.\nCortex, 131:1–16.\nMoataz Assem, Matthew F Glasser, David C Van Es-\nsen, and John Duncan. 2020b.\nA domain-general\ncognitive core defined in multimodally parcellated\nhuman cortex. Cerebral Cortex, 30(8):4361–4380.\nDeniz Bayazit, Negar Foroutan, Zeming Chen, Gail\nWeiss, and Antoine Bosselut. 2023.\nDiscovering\nknowledge-critical subnetworks in pretrained lan-\nguage models. ArXiv, abs/2310.03084.\nYael\nBenn,\nIain\nD.\nWilkinson,\nYing\nZheng,\nKathrin Cohen Kadosh, Charles A.J. Romanowski,\nMichael\nSiegal,\nand\nRosemary\nVarley.\n2013.\nDifferentiating core and co-opted mechanisms in\ncalculation:\nThe neuroimaging of calculation in\naphasia. Brain and Cognition, 82(3):254–264.\nRosa Cao. 2022. Putting representations to use. Syn-\nthese, 200(2):151.\nRosa Cao and Daniel Yamins. 2024. Explanatory mod-\nels in neuroscience, part 1: Taking mechanistic ab-\nstraction seriously.\nCognitive Systems Research,\npage 101244.\nCharlotte Caucheteux and Jean-R´emi King. 2022.\nBrains and algorithms partially converge in natu-\nral language processing. Communications biology,\n5(1):134.\nXuanyi\nChen,\nJosef\nAffourtit,\nRachel\nRyskin,\nTamar I Regev, Samuel Norman-Haignere, Olessia\nJouravlev, Saima Malik-Moraleda,\nHope Kean,\nRosemary Varley, and Evelina Fedorenko. 2023.\nThe human language system, including its in-\nferior frontal component in “broca’s area,” does\nnot support music perception.\nCerebral Cortex,\n33(12):7904–7929.\nDavid Dodell-Feder, Jorie Koster-Hale, Marina Bedny,\nand Rebecca Saxe. 2011.\nfmri item analysis in a\ntheory of mind task. NeuroImage, 55(2):705–712.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Let-\nman, and et al. 2024. The llama 3 herd of models.\nArXiv, abs/2407.21783.\nJohn Duncan. 2010. The multiple-demand (md) sys-\ntem of the primate brain: mental programs for in-\ntelligent behaviour.\nTrends in cognitive sciences,\n14(4):172–179.\nJohn Duncan and Adrian M Owen. 2000.\nCommon\nregions of the human frontal lobe recruited by di-\nverse cognitive demands. Trends in Neurosciences,\n23(10):475–483.\nEvelina Fedorenko, Michael K Behr, and Nancy Kan-\nwisher. 2011. Functional specificity for high-level\nlinguistic processing in the human brain.\nPro-\nceedings of the National Academy of Sciences,\n108(39):16428–16433.\nEvelina Fedorenko, John Duncan, and Nancy Kan-\nwisher. 2013.\nBroad domain generality in fo-\ncal regions of frontal and parietal cortex.\nPro-\nceedings of the National Academy of Sciences,\n110(41):16616–16621.\nEvelina Fedorenko, Po-Jang Hsieh, Alfonso Nieto-\nCastanon,\nSusan\nL.\nWhitfield-Gabrieli,\nand\nNancy G. Kanwisher. 2010.\nNew method for\nfmri investigations of language:\ndefining rois\nfunctionally in individual subjects.\nJournal of\nneurophysiology, 104 2:1177–94.\nEvelina Fedorenko, Anna A. Ivanova, and Tamar I.\nRegev. 2024.\nThe language network as a nat-\nural kind within the broader landscape of the\nhuman brain.\nNature Reviews Neuroscience,\n25(5):289–312.\nEvelina Fedorenko, Josh H. McDermott, Sam Norman-\nHaignere, and Nancy Kanwisher. 2012. Sensitivity\nto musical structure in the human brain. Journal of\nNeurophysiology, 108(12):3289–3300.\nH.L Gallagher, F Happ´e, N Brunswick, P.C Fletcher,\nU Frith, and C.D Frith. 2000.\nReading the mind\nin cartoons and stories: an fmri study of ‘theory\nof mind’ in verbal and nonverbal tasks. Neuropsy-\nchologia, 38(1):11–21.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. SyntaxGym: An online plat-\nform for targeted evaluation of language models.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 70–76, Online. Association\nfor Computational Linguistics.\nPanagiotis Giadikiaroglou, Maria Lymperaiou, Gior-\ngos Filandrianos, and Giorgos Stamou. 2024. Puz-\nzle solving using reasoning of large language mod-\nels: A survey. arXiv preprint arXiv:2402.11291.\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mari-\nano Schain, Amy Price, Bobbi Aubrey, Samuel A.\nNastase, Amir Feder, Dotan Emanuel, Alon Co-\nhen, Aren Jansen, Harshvardhan Gazula, Gina Choe,\nAditi Rao, Catherine Kim, Colton Casto, Lora\nFanda, Werner Doyle, Daniel Friedman, Patricia\nDugan, Lucia Melloni, Roi Reichart, Sasha De-\nvore, Adeen Flinker, Liat Hasenfratz, Omer Levy,\nAvinatan Hassidim, Michael Brenner, Yossi Ma-\ntias, Kenneth A. Norman, Orrin Devinsky, and Uri\nHasson. 2022. Shared computational principles for\nlanguage processing in humans and deep language\nmodels. Nature Neuroscience, 25(3):369–380.\nMiriam Hauptman, Idan Blank, and Evelina Fe-\ndorenko. 2023. Non-literal language processing is\njointly supported by the language and theory of mind\nnetworks: Evidence from a novel meta-analytic fmri\napproach. Cortex, 162:96–114.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021.\nMeasuring mathematical\nproblem solving with the math dataset. NeurIPS.\nEvan Hernandez, Belinda Z. Li, and Jacob Andreas.\n2023. Inspecting and editing knowledge represen-\ntations in language models.\nJennifer Hu, Hannah Small, Hope Kean, Atsushi Taka-\nhashi, Leo Zekelman, Daniel Kleinman, Elizabeth\nRyan, Alfonso Nieto-Casta˜n´on, Victor Ferreira, and\nEvelina Fedorenko. 2023.\nPrecision fmri reveals\nthat the language-selective network supports both\nphrase-structure building and lexical access during\nlanguage production. Cerebral Cortex, 33(8):4384–\n4404.\nAnna A Ivanova, Shashank Srikant, Yotaro Sueoka,\nHope H Kean, Riva Dhamala, Una-May O’Reilly,\nMarina U Bers, and Evelina Fedorenko. 2020. Com-\nprehension of computer code relies primarily on\ndomain-general executive brain regions.\neLife,\n9:e58906.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, et al. 2023.\nMistral 7b. arXiv preprint arXiv:2310.06825.\nJorie Koster-Hale and Rebecca Saxe. 2013.\nTheory\nof mind: A neural prediction problem.\nNeuron,\n79(5):836–848.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877, Hong Kong,\nChina. Association for Computational Linguistics.\nBenjamin Lipkin, Greta Tuckute, Josef Affourtit,\nHannah Small, Zachary Mineroff, Hope Kean,\nOlessia Jouravlev, Lara Rakocevic, Brianna Pritch-\nett, Matthew Siegelman, Caitlyn Hoeflin, Alvinc´e\nPongos, Idan A. Blank, Melissa Kline Struhl, Anna\nIvanova, Steven Shannon, Aalok Sathe, Malte Hoff-\nmann, Alfonso Nieto-Casta˜n´on, and Evelina Fe-\ndorenko. 2022. Probabilistic atlas for the language\nnetwork based on precision fmri data from¿800 in-\ndividuals. Scientific Data, 9(1).\nBradford Z Mahon. 2023. Higher order visual object\nrepresentations: A functional analysis of their role\nin perception and action.\nKyle Mahowald, Anna A Ivanova, Idan A Blank,\nNancy Kanwisher,\nJoshua B Tenenbaum,\nand\nEvelina Fedorenko. 2024.\nDissociating language\nand thought in large language models.\nTrends in\nCognitive Sciences.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 36. ArXiv:2202.05262.\nMartin M Monti, Lawrence M Parsons, and Daniel N\nOsherson. 2012. Thought beyond language: neural\ndissociation of algebra and natural language. Psy-\nchological science, 23(8):914–922.\nThomas Naselaris, Kendrick N Kay, Shinji Nishimoto,\nand Jack L Gallant. 2011. Encoding and decoding\nin fmri. Neuroimage, 56(2):400–410.\nAbhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and\nSanjeev Arora. 2023.\nTask-specific skill localiza-\ntion in fine-tuned language models. In International\nConference on Machine Learning.\nFrancisco Pereira, Bin Lou, Brianna Pritchett, Samuel\nRitter, Samuel J. Gershman, Nancy Kanwisher,\nMatthew Botvinick, and Evelina Fedorenko. 2018.\nToward a universal decoder of linguistic meaning\nfrom brain activation.\nNature Communications,\n9(1):963.\nJacob S Prince, George A Alvarez, and Talia Konkle.\n2024. Representation with a capital ’r’. In UniReps:\n2nd Edition of the Workshop on Unifying Represen-\ntations in Neural Models.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nMaarten Sap, Ronan LeBras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits\nof social intelligence in large lms.\nR Saxe and N Kanwisher. 2003.\nPeople think-\ning about thinking peoplethe role of the temporo-\nparietal junction in “theory of mind”. NeuroImage,\n19(4):1835–1842.\nRebecca Saxe, Matthew Brett, and Nancy Kanwisher.\n2006. Divide and conquer: a defense of functional\nlocalizers. Neuroimage, 30(4):1088–1096.\nRebecca Saxe and Nancy Kanwisher. 2013.\nPeo-\nple thinking about thinking people: the role of the\ntemporo-parietal junction in “theory of mind”. In\nSocial neuroscience, pages 171–182. Psychology\nPress.\nRebecca Saxe and Lindsey J. Powell. 2006.\nIt’s the\nthought that counts: Specific brain regions for one\ncomponent of theory of mind. Psychological Sci-\nence, 17(8):692–699.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute,\nCarina Kauf, Eghbal A. Hosseini, Nancy Kan-\nwisher, Joshua B. Tenenbaum, and Evelina Fe-\ndorenko. 2021. The neural architecture of language:\nIntegrative modeling converges on predictive pro-\ncessing. Proceedings of the National Academy of\nSciences, 118(45):e2105646118.\nMartin Schrimpf, Jonas Kubilius, Ha Hong, Najib J.\nMajaj, Rishi Rajalingham, Elias B. Issa, Kohitij Kar,\nPouya Bashivan, Jonathan Prescott-Roy, Franziska\nGeiger, Kailyn Schmidt, Daniel L. K. Yamins, and\nJames J. DiCarlo. 2018. Brain-Score: Which Artifi-\ncial Neural Network for Object Recognition is most\nBrain-Like? preprint, Neuroscience.\nMartin Schrimpf, Jonas Kubilius, Michael J. Lee,\nN. Apurva Ratan Murty, Robert Ajemian, and\nJames J. DiCarlo. 2020. Integrative benchmarking\nto advance neurally mechanistic models of human\nintelligence. Neuron, 108(3):413–423.\nCory Shain, Hope Kean, Colton Casto, Benjamin Lip-\nkin, Josef Affourtit, Matthew Siegelman, Francis\nMollica, and Evelina Fedorenko. 2024. Distributed\nSensitivity to Syntax and Semantics throughout the\nLanguage Network.\nJournal of Cognitive Neuro-\nscience, pages 1–43.\nSneha Shashidhara, Floortje S. Spronkers, and Yaara\nErez. 2020.\nIndividual-subject functional local-\nization increases univariate activation but not mul-\ntivariate pattern discriminability in the “multiple-\ndemand” frontoparietal network.\nJournal of Cog-\nnitive Neuroscience, 32(7):1348–1368.\nMichael Siegal and Rosemary Varley. 2006. Aphasia,\nlanguage, and theory of mind. Social Neuroscience,\n1(3–4):167–174.\nNathaniel J. Smith and Roger Levy. 2013. The effect of\nword predictability on reading time is logarithmic.\nCognition, 128(3):302–319.\nWinnie Street, John Oliver Siy, Geoff Keeling, Adrien\nBaranes, Benjamin Barnett, Michael McKibben,\nTatenda Kanyere, Alison Lentz, Robin IM Dunbar,\net al. 2024. Llms achieve adult human performance\non higher-order theory of mind tasks. arXiv preprint\narXiv:2405.18870.\nHaiyang Sun, Lin Zhao, Zihao Wu, Xiaohui Gao, Yutao\nHu, Mengfei Zuo, Wei Zhang, Jun-Feng Han, Tian-\nming Liu, and Xintao Hu. 2024. Brain-like func-\ntional organization within large language models.\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying\nLiu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu\nDing, Hongyang Li, Mengzhe Geng, et al. 2023. A\nsurvey of reasoning with foundation models. arXiv\npreprint arXiv:2312.11562.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale,\nJuliette Love, et al. 2024. Gemma: Open models\nbased on gemini research and technology.\narXiv\npreprint arXiv:2403.08295.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher,\nCristian Cant´on Ferrer, Moya Chen, Guillem Cu-\ncurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\nGoswami, Naman Goyal, Anthony S. Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin\nKardas, Viktor Kerkez, Madian Khabsa, Isabel M.\nKloumann, A. V. Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, R. Sub-\nramanian,\nXia Tan,\nBinh Tang,\nRoss Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu,\nZhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\nGreta Tuckute, Nancy Kanwisher, and Evelina Fe-\ndorenko. 2024a.\nLanguage in brains, minds, and\nmachines. Annual Review of Neuroscience, 47.\nGreta\nTuckute,\nAalok\nSathe,\nShashank\nSrikant,\nMaya Taliaferro, Mingye Wang, Martin Schrimpf,\nKendrick Kay, and Evelina Fedorenko. 2024b. Driv-\ning and suppressing the human language network\nusing large language models.\nNature Human Be-\nhaviour, pages 1–18.\nRosemary Varley and Michael Siegal. 2000. Evidence\nfor cognition without grammar from causal reason-\ning and ‘theory of mind’ in an agrammatic aphasic\npatient. Current Biology, 10(12):723–726.\nRosemary A. Varley, Nicolai J. C. Klessinger, Charles\nA. J. Romanowski, and Michael Siegal. 2005.\nAgrammatic but numerate. Proceedings of the Na-\ntional Academy of Sciences, 102(9):3519–3524.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding.\nIn Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2019. Blimp: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019.\nHuggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nAlexandra Woolgar, Alice Parr, Rhodri Cusack, Rus-\nsell Thompson, Ian Nimmo-Smith, Teresa Torralva,\nMaria Roca, Nagui Antoun, Facundo Manes, and\nJohn Duncan. 2010. Fluid intelligence loss linked to\nrestricted regions of damage within frontal and pari-\netal cortex. Proceedings of the National Academy of\nSciences, 107(33):14899–14902.\nAnthony Zador, Sean Escola, Blake Richards, Bence\n¨Olveczky,\nYoshua\nBengio,\nKwabena\nBoahen,\nMatthew\nBotvinick,\nDmitri\nChklovskii,\nAnne\nChurchland, Claudia Clopath, et al. 2023. Catalyz-\ning next-generation artificial intelligence through\nneuroai. Nature communications, 14(1):1597.\nZiyin Zhang, Lizhen Xu, Zhaokun Jiang, Hongkun\nHao,\nand Rui Wang. 2024.\nMultiple-choice\nquestions are efficient and robust llm evaluators.\nPreprint, arXiv:2405.11966.\nJun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui,\nLuhui Gao, and Xuanjing Huang. 2023. Unveiling\na core linguistic region in large language models.\nPreprint, arXiv:2310.14928.\nAppendix\nA\nFunctional Localizers\nFigure 7 shows a pair of examples for each net-\nwork localizer stimuli. We provide more details of\neach stimuli set below.\nA.1\nLanguage Localizer\nThe language localizer uses the same set of 240\nsentences and 240 lists of non-words2 as used\nby neuroscientists to localize the human language\nnetwork. Each sentence consists of 12 words, and\neach list of non-words consists of 12 non-words\nto control for length. Since we are using a trained\nBPE tokenizer that breaks down each word into\na number of tokens, we truncated the tokens to\nhave a maximum length of 12 to ensure similar\nsequence length.\nA.2\nMultiple Demand Localizer\nThe arithmetic multiple-demand localizer used in\nneuroscience includes a set of “hard” arithmetic\nquestions alongside a set of “easy” ones. These\nstimuli are usually generated by a MATLAB script\nthat displays a mathematical problem on a screen\nfor participants to solve, followed by two answer\nchoices, one of which is correct. “Hard” questions\nare defined as addition problems with results ex-\nceeding 20 (e.g., 18+5), while “easy” questions\nyield results below 10 (e.g., 4+2).\nWe adapted\nthis localizer by similarly generating hard and\neasy arithmetic questions but slightly increased the\ncomplexity. Specifically, for hard questions, we\nsampled two numbers between 100 and 200, with\neach problem randomly chosen to be either an ad-\ndition or subtraction with equal likelihood. For\neasy questions, we sampled two numbers in the\nrange of 1 to 20. We generated 100 questions for\neach stimuli set. Examples are shown in Figure 7.\nA.3\nTheory of Mind Localizer\nWe use the same set of stimuli employed in neu-\nroscience to localize the theory-of-mind network\nin the human brain (Dodell-Feder et al., 2011),\nwhich includes 10 false-belief stories and 10 false-\nphotograph stories3. The prompt was structured to\n2The language localizer stimuli were retrieved from:\nhttps://www.evlab.mit.edu/resources-all/\ndownload-localizer-tasks\n3The\nTheory\nof\nMind\nlocalizer\nstimuli\nwere\nre-\ntrieved\nfrom\nhttps://saxelab.mit.edu/\nuse-our-efficient-false-belief-localizer/\nmirror the instructions given to participants dur-\ning the neuroimaging study, followed by the story,\nthe question, two answer choices (True or False),\nand an answer. Example of the prompt given from\neach set are shown in Figure 7. When evaluating\nthe model on the test-set, we give it the following\ninstruction: “The following multiple choice ques-\ntions is based on the following story. The question\nis related to Theory-of-Mind. Read the story and\nthen answer the questions. Choose the best answer\nfrom the options provided by printing it as is with-\nout any modifications.”\nB\nLocalized Units Location\nB.1\nLanguage Units\nFigure 8 shows the distribution of the top 1% lan-\nguage selective units for all 18 models tested in\nthis work. An interesting observation is that the\ndistribution of language-selective units remains\nsimilar in models both before and after instruction\ntuning.\nB.2\nMultiple Demand Units\nFigure 9 shows the distribution of the top 1% Mul-\ntiple Demand (MD) selective units for the 10 mod-\nels tested for MD in this work.\nB.3\nTheory of Mind Units\nFigure 10 shows the distribution of the top 1%\nTheory of Mind (ToM) selective units for the 10\nmodels tested for ToM in this work. The ToM se-\nlective units are more distributed across the model\nlayers rather than being more clustered as in MD\nand the language-selective units. This might be\ndue to the small number of stimuli samples used\nfor the ToM localizer.\nC\nModels\nTable 2 lists all 18 models analyzed in this study\nand indicates which models were used in which\nexperiment. We kept 10 models for each experi-\nment as shown in the last row. Table 11 shows the\nnumber of units corresponding to each percentage\nlevel for all models.\nD\nFinegrained Results\nD.1\nLanguage Results\nTables 6 and 7 display results for the 10\nmodels tested on three language benchmarks—\nSyntaxGym, BLiMP, and GLUE—along with the\nModel\nLang\nMD/ToM\nGPT2-Large\n✓\n✗\nGPT2-XL\n✓\n✗\nGemma-2B\n✓\n✗\nGemma-7B\n✓\n✗\nGemma-1.1-7B-Instruct\n✗\n✓\nPhi-3.5-Mini-Instruct\n✓\n✓\nMistral-7B-v0.3\n✓\n✗\nMistral-7B-Instruct-v0.3\n✗\n✓\nLLaMA-2-7B\n✓\n✗\nLLaMA-2-7B-Chat\n✗\n✓\nLLaMA-2-13B\n✓\n✗\nLLaMA-2-13B-Chat\n✗\n✓\nLLaMA-3.1-8B-Instruct\n✓\n✓\nLLaMA-3.2-3B-Instruct\n✗\n✓\nFalcon-7B\n✓\n✗\nFalcon-7B-Instruct\n✗\n✓\nQwen2.5-3B-Instruct\n✗\n✓\nQwen2.5-7B-Instruct\n✗\n✓\n#\n18\n10\n10\nTable 2: Models Used in This Work Overview of\nthe 18 models analyzed, with an indication of the spe-\ncific experiments in which each model was used. Lang\nreferes to the language experiments, MD refers to the\nMultiple Demand experiments, and ToM refers to the\nTheory of Mind experiments.\naverage performance across these benchmarks.\nEach model’s performance is shown initially with-\nout ablation, followed by ablations of the top-\n0.125, 0.25, 0.5, 1% language-selective units, and\nthen with randomly sampled units at the same per-\ncentages. The performance changes in Figure 3\ncan be reproduced by subtracting post-ablation re-\nsults from the baseline (0%) for both language-\nselective and random unit ablations. Results with\nrandom units are averaged across three random\nseeds.\nD.2\nMultiple Demand Results\nTable 8 presents the results for the 10 models\ntested on the MATH benchmark, organized by dif-\nficulty level and including the overall macro aver-\nage across levels. Each model’s performance is\nshown under three conditions: without ablation,\nafter ablating the top 1% of Multiple Demand-\nselective units, and with an equivalent number of\nrandomly sampled units.\nD.3\nTheory of Mind Results\nTable 9 similarly shows the results for the 10 mod-\nels tested on the TOMI benchmark, organized by\nquestion type, whether it involves a false-belief\nscenario (n=231) or true-belief scenarios (n=389),\nand including the macro average across both types.\nEach model’s performance is shown under three\nconditions: without ablation, after ablating the top\n1% of theory-of-mind-selective units, and with an\nequivalent number of randomly sampled units. Ta-\nble 10 shows the same but when ablating the top-\n2% of units.\nE\nMore Brain Alignment Statistical Tests\nIn Section 7, we performed Welch’s t-test to\ndemonstrate that units from the language network\nare significantly more brain-aligned than three sets\nof randomly sampled units from the model, par-\nticularly when sampling a small number of units.\nHere, we conduct the Shapiro-Wilk test to verify\nthat each distribution follows a normal distribu-\ntion, as Welch’s t-test assumes normality in the\ncompared distributions.\nTables 3 and 4 present\nthe test statistics and p-values for the brain align-\nment results across models, comparing both lan-\nguage and random units at each percentage and\nfor each dataset. These results confirm that the\ndistributions are indeed normal. A p-value greater\nthan 0.05 indicates normality, while values be-\nlow this threshold suggest deviation from normal-\nity.\nNotably, the only cases where the p-value\nfalls below 0.05—indicating non-normal distribu-\ntions—are for the 0.5% and 1% conditions in the\nTuckute2024 dataset, where no significant differ-\nence was observed.\nPercentage\nLanguage Units\nRandom Units\n0.03125%\n(0.902, 0.233)\n(0.971, 0.565)\n0.0625%\n(0.957, 0.755)\n(0.939, 0.085)\n0.125%\n(0.955, 0.722)\n(0.981, 0.853)\n0.25%\n(0.933, 0.475)\n(0.962, 0.345)\n0.5%\n(0.945, 0.609)\n(0.974, 0.658)\n1%\n(0.945, 0.612)\n(0.970, 0.551)\nTable 3: Shapiro-Wilk test (statistics and p-values) for\nbrain alignment distributions across models. The test\nis conducted separately for language units and ran-\ndomly sampled units at each percentage level for the\nPEREIRA2018 dataset.\nWe also conducted a permutation test, a non-\nparametric statistical method that does not re-\nPercentage\nLanguage Units\nRandom Units\n0.03125%\n(0.948, 0.641)\n(0.9511, 0.181)\n0.0625%\n(0.962, 0.802)\n(0.9507, 0.176)\n0.125%\n(0.973, 0.915)\n(0.9810, 0.852)\n0.25%\n(0.914, 0.309)\n(0.9592, 0.296)\n0.5%\n(0.829, 0.032)\n(0.9693, 0.519)\n1%\n(0.825, 0.029)\n(0.9725, 0.608)\nTable 4: Shapiro-Wilk test (statistics and p-values) for\nbrain alignment distributions across models. The test\nis conducted separately for language units and ran-\ndomly sampled units at each percentage level for the\nTUCKUTE2024 dataset.\nquire the assumption of normality. This method\ninvolves randomly shuffling data labels across\n10,000 permutations to generate a null distribution\nof the test statistic. By comparing the observed\ntest statistic to this null distribution, we evaluated\nthe statistical significance of our results. The find-\nings from the permutation test confirmed the sig-\nnificance of our results, as shown in Table 5.\nPercentage\nPEREIRA2018\nTUCKUTE2024\n0.03125%\n0.001\n0.004\n0.0625%\n0.004\n0.013\n0.125%\n0.138\n0.000\n0.25%\n0.548\n0.013\n0.5%\n0.195\n0.169\n1%\n0.084\n0.458\nTable 5: Permutation test p-values assessing the sta-\ntistical significance of brain alignment differences on\nboth datasets.\nThe test was conducted by randomly\nshuffling data labels across 10,000 permutations to gen-\nerate a null distribution of the test statistic. The ob-\nserved test statistic was then compared to this null dis-\ntribution to compute the p-values. Lower p-values indi-\ncate stronger evidence against the null hypothesis, con-\nfirming the robustness of our findings.\nF\nBrain-Score Datasets\nTuckute2024\nThis dataset consists of 5 partic-\nipants reading 1000 6-word sentences presented\none sentence at a time for 2s. BOLD responses\nfrom voxels in the language network were av-\neraged within each participant and then across\nparticipants to yield an overall average language\nnetwork response to each sentence.\nThe stim-\nuli used span a large part of the linguistic space,\nenabling model-brain comparisons across a wide\nrange of single sentences.\nSentence presenta-\ntion order was randomized across participants.\nThe averaging of sentences across participants ef-\nfectively minimizes the effect of temporal au-\ntocorrelation/context in this dataset.\nIn combi-\nnation with the diversity in linguistic materials,\nthis dataset presents a particularly challenging\ndataset for model evaluation. The noise ceiling for\nTUCKUTE2024 is r = 0.56, see Tuckute et al.\n(2024b) for more details.\nPereira2018\nThis dataset consists of fMRI ac-\ntivations (blood-oxygen-level-dependent; BOLD\nresponses) recorded as participants read short pas-\nsages presented one sentence at a time for 4 s. The\ndataset is composed of two distinct experiments:\none with 9 subjects presented with 384 sentences,\nand another with 6 subjects presented with 243\nsentences each. The passages in each experiment\nspanned 24 different topics. The results reported\nfor this dataset are the average alignment across\nboth experiments (Pereira et al., 2018). The re-\nported results for this dataset use an unshuffled\ncross-validation scheme, ensuring that sentences\nfrom the same passage appear exclusively in either\nthe training or testing set.\nto the directors the problem appeared a\nmatter of intrigue or diplomacy\not momp vo detlerence frot mogs elibonce\npolved ro op ummosite comblision\nLanguage Localizer \nQuestion: Solve 151 + 192?\nAnswer: 343\nQuestion: Solve 7 + 15?\nAnswer: 22\nMultiple Demand Localizer\nIn this experiment, you will read a series of\nsentences and then answer True/False\nquestions about them. Press button 1 to\nanswer 'true' and button 2 to answer 'false'.\nStory: Expecting the game to be postponed\nbecause of the rain, the Garcia family took\nthe subway home. The score was tied, 3-3.\nDuring their commute the rain stopped and\nthe game soon ended with a score of 5-3.\nQuestion: The Garcia family arrives home\nbelieving the score is 5-3.\nOptions:\n- True\n- False\nAnswer: False\nTheory of Mind Localizer\nExamples of Localizers Stimuli\nIn this experiment, you will read a series of\nsentences and then answer True/False\nquestions about them. Press button 1 to\nanswer 'true' and button 2 to answer 'false'.\nStory: Accounts of the country's bustling\neconomic success were recorded in both\nfiction and non-fiction books from the early\n1900s. Soon after, a horrible plague hit the\ncountry and the country was sent into an\neconomic depression.\nQuestion: Early 1900s novels portray the\ncountry as experiencing economic wealth.\nOptions:\n- True\n- False\nAnswer: True\nFalse-Belief Story\nFalse-Photograph Story\nHard Arithmetic Question\nEasy Arithmetic Question\nSentence \nList of non-words\nFigure 7: Examples of Localizers Stimuli. Language stimuli consists of 240 sentences and 240 lists of non-words.\nMultiple Demand stimuli consists of 100 hard arithmetic problems and 100 easy ones. Theory of Mind consists of\n10 false-belief stories and 10 false-photograph stories. The instruction given in the Theory of Mind stimuli is the\nsame given to participants during the neuroimaging study. See Appendix A for more details about each localizer.\nFigure 8: Distribution of Language Units Across Layers. The distribution of the top 1% most language-selective\nunits across layers in all 18 models tested in this work. The models are displayed from top to bottom, with each\nlayer labeled by the percentage of units identified as belonging to the top 1%.\nFigure 9: Distribution of Multiple Demand Units Across Layers. The distribution of the top 1% most Multiple\nDemand (MD) selective units across layers in the 10 models tested for MD in this work. The models are displayed\nfrom top to bottom, with each layer labeled by the percentage of units identified as belonging to the top 1%.\nFigure 10: Distribution of Theory of Mind Units Across Layers. The distribution of the top 1% most theory of\nmind (ToM) selective units across layers in the 10 models tested for ToM in this work. The models are displayed\nfrom top to bottom, with each layer labeled by the percentage of units identified as belonging to the top 1%.\nModel\nAblation Units\nPercentage\nSyntaxGym\nBLiMP\nGLUE\nAverage\nGPT2-Large\n-\n0%\n78.50\n83.55\n45.42\n69.16\nLanguage\n0.125%\n61.87\n76.79\n44.23\n60.96\nLanguage\n0.25%\n50.03\n72.69\n43.13\n55.28\nLanguage\n0.5%\n46.99\n69.37\n38.55\n51.64\nLanguage\n1%\n41.07\n66.09\n39.96\n49.04\nRandom\n0.125%\n78.02\n83.50\n45.39\n68.97\nRandom\n0.25%\n78.28\n83.33\n45.49\n69.03\nRandom\n0.5%\n77.95\n82.89\n44.48\n68.44\nRandom\n1%\n76.87\n82.70\n43.97\n67.85\nGPT2-XL\n-\n0%\n82.70\n83.38\n46.85\n70.98\nLanguage\n0.125%\n80.20\n81.64\n44.78\n68.88\nLanguage\n0.25%\n70.73\n78.06\n46.24\n65.01\nLanguage\n0.5%\n66.54\n77.19\n44.75\n62.82\nLanguage\n1%\n56.02\n74.86\n43.12\n58.00\nRandom\n0.125%\n82.26\n83.16\n46.40\n70.61\nRandom\n0.25%\n80.76\n83.07\n46.38\n70.07\nRandom\n0.5%\n79.93\n82.68\n45.53\n69.38\nRandom\n1%\n79.44\n81.64\n45.19\n68.76\nGemma-2B\n-\n0%\n80.15\n81.14\n47.81\n69.70\nLanguage\n0.125%\n38.16\n56.34\n41.79\n45.43\nLanguage\n0.25%\n36.59\n54.52\n39.82\n43.64\nLanguage\n0.5%\n26.02\n52.54\n37.38\n38.64\nLanguage\n1%\n25.46\n51.60\n37.56\n38.21\nRandom\n0.125%\n80.18\n81.10\n47.35\n69.54\nRandom\n0.25%\n79.49\n80.88\n48.42\n69.60\nRandom\n0.5%\n79.51\n80.93\n46.25\n68.90\nRandom\n1%\n65.89\n72.20\n42.65\n60.25\nGemma-7B\n-\n0%\n80.37\n81.75\n62.29\n74.80\nLanguage\n0.125%\n54.99\n64.30\n43.34\n54.21\nLanguage\n0.25%\n52.91\n61.17\n44.38\n52.82\nLanguage\n0.5%\n25.67\n63.75\n41.15\n43.52\nLanguage\n1%\n29.61\n48.97\n45.90\n41.50\nRandom\n0.125%\n80.15\n80.48\n61.96\n74.20\nRandom\n0.25%\n80.44\n81.24\n60.59\n74.09\nRandom\n0.5%\n80.55\n81.25\n63.05\n74.95\nRandom\n1%\n79.65\n79.98\n58.36\n72.66\nPhi-3.5-Mini-Instruct\n-\n0%\n81.86\n80.63\n70.73\n77.74\nLanguage\n0.125%\n45.42\n58.62\n60.60\n54.88\nLanguage\n0.25%\n34.81\n55.72\n49.65\n46.72\nLanguage\n0.5%\n25.37\n53.56\n33.40\n37.44\nLanguage\n1%\n22.90\n53.79\n46.26\n40.98\nRandom\n0.125%\n80.16\n80.95\n70.80\n77.30\nRandom\n0.25%\n81.83\n81.64\n69.64\n77.70\nRandom\n0.5%\n78.79\n80.35\n68.61\n75.92\nRandom\n1%\n79.80\n79.05\n69.19\n76.01\nTable 6: Language Benchmarks Results 1 Results for the 5 models on the language benchmarks tested in this\nwork. Random for each percentage is averaged across 3 seeds. The results when ablating random units is almost\nthe same as ablating no units, while ablating language units lead to a sharp drop in performance. See Table 7 for\nthe results of the other models.\nModel\nAblation Units\nPercentage\nSyntaxGym\nBLiMP\nGLUE\nAverage\nLLaMA-2-7b\n-\n0%\n81.07\n85.63\n50.60\n72.43\nLanguage\n0.125%\n46.07\n66.85\n41.91\n51.61\nLanguage\n0.25%\n39.51\n64.24\n40.91\n48.22\nLanguage\n0.5%\n28.86\n57.07\n32.57\n39.50\nLanguage\n1%\n26.82\n56.01\n38.33\n40.39\nRandom\n0.125%\n81.09\n85.57\n50.74\n72.47\nRandom\n0.25%\n81.26\n85.03\n50.25\n72.18\nRandom\n0.5%\n80.23\n84.68\n51.15\n72.02\nRandom\n1%\n80.63\n84.53\n47.44\n70.87\nLLaMA-2-13b\n-\n0%\n82.91\n84.82\n59.53\n75.76\nLanguage\n0.125%\n78.57\n81.38\n48.05\n69.33\nLanguage\n0.25%\n62.12\n74.84\n42.47\n59.81\nLanguage\n0.5%\n23.85\n51.23\n29.16\n34.75\nLanguage\n1%\n29.13\n51.42\n30.03\n36.86\nRandom\n0.125%\n82.43\n84.79\n58.76\n75.33\nRandom\n0.25%\n82.13\n84.66\n55.18\n73.99\nRandom\n0.5%\n82.06\n83.77\n57.52\n74.45\nRandom\n1%\n81.21\n83.55\n53.94\n72.90\nLLaMA-3.1-8B-Instruct\n-\n0%\n80.05\n81.90\n69.20\n77.05\nLanguage\n0.125%\n80.25\n79.60\n66.44\n75.43\nLanguage\n0.25%\n78.22\n76.96\n61.43\n72.20\nLanguage\n0.5%\n73.12\n77.60\n55.77\n68.83\nLanguage\n1%\n54.12\n67.17\n46.36\n55.88\nRandom\n0.125%\n79.93\n81.89\n68.98\n76.93\nRandom\n0.25%\n79.99\n81.88\n68.71\n76.86\nRandom\n0.5%\n79.92\n81.10\n69.51\n76.85\nRandom\n1%\n79.14\n81.73\n67.41\n76.09\nMistral-7B\n-\n0%\n80.39\n83.44\n64.03\n75.95\nLanguage\n0.125%\n70.08\n75.38\n47.33\n64.26\nLanguage\n0.25%\n44.11\n66.73\n44.91\n51.91\nLanguage\n0.5%\n37.60\n66.39\n43.74\n49.24\nLanguage\n1%\n33.05\n61.85\n40.34\n45.08\nRandom\n0.125%\n80.28\n83.34\n63.54\n75.72\nRandom\n0.25%\n80.46\n83.13\n63.91\n75.84\nRandom\n0.5%\n80.34\n82.62\n62.75\n75.24\nRandom\n1%\n79.22\n82.51\n63.00\n74.91\nFalcon-7B\n-\n0%\n80.05\n80.35\n48.36\n69.59\nLanguage\n0.125%\n72.17\n75.83\n46.86\n64.95\nLanguage\n0.25%\n69.99\n71.67\n47.23\n62.97\nLanguage\n0.5%\n51.36\n60.23\n44.17\n51.92\nLanguage\n1%\n25.79\n55.42\n45.11\n42.10\nRandom\n0.125%\n79.59\n80.26\n48.44\n69.43\nRandom\n0.25%\n79.89\n80.35\n48.83\n69.69\nRandom\n0.5%\n78.62\n79.96\n48.40\n69.00\nRandom\n1%\n78.32\n79.99\n48.85\n69.05\nTable 7: Language Benchmarks Results 2 Results for 5 models on the language benchmarks tested in this work.\nRandom for each percentage is averaged across 3 seeds. The results when ablating random units is almost the same\nas ablating no units, while ablating language units lead to a sharp drop in performance. See Table 6 for the results\nof the other models.\nModel\nAblation\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nLevel 5\nAverage\nPhi-3.5-Mini-Instruct\n-\n52.33\n41.50\n40.45\n35.11\n31.91\n40.26\nMD\n38.37\n32.31\n33.90\n28.44\n27.33\n32.07\nRandom\n49.92\n43.50\n41.52\n37.31\n35.71\n41.59\nLLaMA-3.1-8B-Instruct\n-\n40.00\n37.41\n36.23\n36.36\n39.21\n37.84\nMD\n37.21\n34.13\n33.18\n33.61\n40.45\n35.72\nRandom\n36.20\n35.15\n33.96\n35.95\n38.59\n35.97\nMistral-7B-Instruct\n-\n39.07\n35.71\n37.31\n35.61\n34.01\n36.34\nMD\n36.28\n33.90\n32.65\n32.03\n30.51\n33.07\nRandom\n35.35\n33.07\n34.05\n34.03\n33.23\n33.95\nLLaMA-2-7b-Instruct\n-\n24.42\n28.57\n29.24\n28.69\n29.04\n27.99\nMD\n24.65\n29.71\n29.42\n29.44\n29.11\n28.47\nRandom\n23.41\n28.46\n27.50\n26.94\n28.39\n26.94\nLLaMA-2-13b-Instruct\n-\n25.35\n33.11\n29.78\n29.19\n29.35\n29.35\nMD\n28.14\n25.74\n24.48\n25.27\n23.84\n25.49\nRandom\n26.51\n31.18\n28.34\n28.86\n28.73\n28.72\nLLaMA-3.2-3B-Instruct\n-\n35.35\n32.77\n33.99\n33.69\n35.56\n34.27\nMD\n31.63\n32.20\n33.90\n33.11\n34.70\n33.11\nRandom\n34.19\n32.77\n32.56\n34.50\n35.74\n33.95\nGemma-1.1-7B-Instruct\n-\n40.00\n37.41\n35.96\n34.28\n35.87\n36.71\nMD\n35.58\n34.81\n30.49\n31.28\n32.53\n32.94\nRandom\n37.21\n36.28\n35.19\n33.75\n35.12\n35.51\nFalcon-7B-Instruct\n-\n23.49\n26.64\n23.86\n25.85\n23.91\n24.75\nMD\n27.91\n26.30\n25.20\n25.19\n24.15\n25.75\nRandom\n26.98\n25.21\n24.48\n25.05\n23.96\n25.14\nQwen2.5-7B-Instruct\n-\n59.53\n60.43\n59.37\n56.46\n56.91\n58.54\nMD\n59.07\n56.12\n56.59\n55.55\n54.66\n56.40\nRandom\n57.75\n57.60\n56.53\n54.05\n53.73\n55.93\nQwen2.5-3B-Instruct\n-\n53.49\n49.55\n49.33\n44.37\n47.28\n48.80\nMD\n43.72\n40.14\n40.54\n36.03\n38.66\n39.82\nRandom\n42.95\n40.36\n39.13\n36.86\n37.50\n39.36\nTable 8: MATH Benchmark Results Results for the 10 models tested on the MATH benchmark, showing per-\nformance in the following conditions for each model: without ablation, with ablation of the top 1% most MD-\nselective, and with randomly sampled. The results for Random is averaged across 3 seeds. MD stands for multiple\ndemand.\nModel\nAblation Units\nFalse Belief\nTrue Belief\nAverage\nPhi-3.5-Mini-Instruct\n-\n50.65\n86.38\n68.51\nToM\n17.75\n98.46\n58.10\nRandom\n17.75\n96.92\n57.33\nLLaMA-3.1-8B-Instruct\n-\n80.95\n75.32\n78.14\nToM\n64.50\n75.32\n69.91\nRandom\n76.62\n68.47\n72.54\nMistral-7B-Instruct\n-\n79.22\n65.81\n72.52\nToM\n64.07\n61.95\n63.01\nRandom\n69.55\n68.47\n69.01\nLLaMA-2-7b-Instruct\n-\n23.81\n79.69\n51.75\nToM\n20.78\n79.95\n50.36\nRandom\n32.47\n68.38\n50.42\nLLaMA-2-13b-Instruct\n-\n63.64\n68.38\n66.01\nToM\n49.35\n69.92\n59.64\nRandom\n59.88\n58.01\n58.95\nLLaMA-3.2-3B-Instruct\n-\n9.96\n92.80\n51.38\nToM\n9.52\n91.77\n50.65\nRandom\n16.88\n85.52\n51.20\nGemma-1.1-7B-Instruct\n-\n78.79\n65.55\n72.17\nToM\n62.77\n66.07\n64.42\nRandom\n72.87\n63.58\n68.23\nFalcon-7B-Instruct\n-\n50.22\n46.02\n48.12\nToM\n49.78\n45.50\n47.64\nRandom\n52.67\n48.41\n50.54\nQwen2.5-7B-Instruct\n-\n97.84\n41.65\n69.74\nToM\n93.51\n44.73\n69.12\nRandom\n92.35\n46.62\n69.48\nQwen2.5-3B-Instruct\n-\n81.82\n59.38\n70.60\nToM\n77.06\n56.56\n66.81\nRandom\n46.90\n60.07\n53.48\nTable 9: TOMi Benchmark Results (1% Lesion) Results for the 10 models tested on the TOMi benchmark,\nshowing performance in the following conditions for each model: without ablation, with ablation of the top 1%\nmost ToM-selective, and with randomly sampled. The results for Random is averaged across 3 seeds. ToM stands\nfor theory of mind.\nModel\nAblation Units\nFalse Belief\nTrue Belief\nAverage\nPhi-3.5-Mini-Instruct\n-\n50.65\n86.38\n68.51\nToM\n7.36\n97.69\n52.52\nRandom\n27.71\n92.03\n59.87\nLLaMA-3.1-8B-Instruct\n-\n80.95\n75.32\n78.14\nToM\n49.35\n64.52\n56.94\nRandom\n61.62\n56.56\n59.09\nMistral-7B-Instruct\n-\n79.22\n65.81\n72.52\nToM\n40.26\n71.98\n56.12\nRandom\n66.67\n66.50\n66.58\nLLaMA-2-7b-Instruct\n-\n23.81\n79.69\n51.75\nToM\n19.05\n77.63\n48.34\nRandom\n35.93\n63.75\n49.84\nLLaMA-2-13b-Instruct\n-\n63.64\n68.38\n66.01\nToM\n42.42\n60.15\n51.29\nRandom\n52.53\n54.07\n53.30\nLLaMA-3.2-3B-Instruct\n-\n9.96\n92.80\n51.38\nToM\n19.48\n82.01\n50.74\nRandom\n25.83\n76.86\n51.35\nGemma-1.1-7B-Instruct\n-\n78.79\n65.55\n72.17\nToM\n61.04\n67.87\n64.45\nRandom\n71.57\n64.27\n67.92\nFalcon-7B-Instruct\n-\n50.22\n46.02\n48.12\nToM\n52.81\n46.27\n49.54\nRandom\n50.07\n50.64\n50.36\nQwen2.5-7B-Instruct\n-\n97.84\n41.65\n69.74\nToM\n89.61\n50.13\n69.87\nRandom\n60.03\n59.13\n59.58\nQwen2.5-3B-Instruct\n-\n81.82\n59.38\n70.60\nToM\n87.45\n37.28\n62.36\nRandom\n64.07\n46.02\n55.04\nTable 10: TOMi Benchmark Results (2% Lesion) Results for the 10 models tested on the TOMi benchmark,\nshowing performance in the following conditions for each model: without ablation, with ablation of the top 2%\nmost ToM-selective, and with randomly sampled. The results for Random is averaged across 3 seeds. ToM stands\nfor theory of mind.\nModel\n0.03125%\n0.0625%\n0.125%\n0.25%\n0.5%\n1%\n2%\nFalcon-7B\n45\n90\n181\n363\n727\n1454\n2908\nFalcon-7B-Instruct\n45\n90\n181\n363\n727\n1454\n2908\nGPT2-Large\n14\n28\n57\n115\n230\n460\n921\nGPT2-XL\n24\n48\n96\n192\n384\n768\n1536\nGemma-1.1-7B-Instruct\n26\n53\n107\n215\n430\n860\n1720\nGemma-2B\n11\n23\n46\n92\n184\n368\n737\nGemma-7B\n26\n53\n107\n215\n430\n860\n1720\nLLaMA-2-13b\n64\n128\n256\n512\n1024\n2048\n4096\nLLaMA-2-13b-Instruct\n64\n128\n256\n512\n1024\n2048\n4096\nLLaMA-2-7b\n40\n81\n163\n327\n655\n1310\n2621\nLLaMA-2-7b-Instruct\n40\n81\n163\n327\n655\n1310\n2621\nLLaMA-3.1-8B-Instruct\n40\n81\n163\n327\n655\n1310\n2621\nLLaMA-3.2-3B-Instruct\n26\n53\n107\n215\n430\n860\n1720\nMistral-7B\n40\n81\n163\n327\n655\n1310\n2621\nMistral-7B-Instruct\n40\n81\n163\n327\n655\n1310\n2621\nPhi-3.5-Mini-Instruct\n30\n61\n122\n245\n491\n983\n1966\nQwen2.5-3B-Instruct\n23\n46\n92\n184\n368\n737\n1474\nQwen2.5-7B-Instruct\n31\n62\n125\n250\n501\n1003\n2007\nTable 11: Number of Units at Specified Percentage Levels for Each Model The table shows the number of\nunits corresponding to each percentage level (x%) for each model. These values are calculated by multiplying the\nnumber of layers, by the hidden dimension, and the specified percentage.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2024-11-04",
  "updated": "2025-02-13"
}