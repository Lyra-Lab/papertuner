{
  "id": "http://arxiv.org/abs/2411.02893v1",
  "title": "Generalization vs. Hallucination",
  "authors": [
    "Xuyu Zhang",
    "Haofan Huang",
    "Dawei Zhang",
    "Songlin Zhuang",
    "Shensheng Han",
    "Puxiang Lai",
    "Honglin Liu"
  ],
  "abstract": "With fast developments in computational power and algorithms, deep learning\nhas made breakthroughs and been applied in many fields. However, generalization\nremains to be a critical challenge, and the limited generalization capability\nseverely constrains its practical applications. Hallucination issue is another\nunresolved conundrum haunting deep learning and large models. By leveraging a\nphysical model of imaging through scattering media, we studied the lack of\ngeneralization to system response functions in deep learning, identified its\ncause, and proposed a universal solution. The research also elucidates the\ncreation process of a hallucination in image prediction and reveals its cause,\nand the common relationship between generalization and hallucination is\ndiscovered and clarified. Generally speaking, it enhances the interpretability\nof deep learning from a physics-based perspective, and builds a universal\nphysical framework for deep learning in various fields. It may pave a way for\ndirect interaction between deep learning and the real world, facilitating the\ntransition of deep learning from a demo model to a practical tool in diverse\napplications.",
  "text": "Generalization vs. Hallucination \nXuyu Zhang1,2, Haofan Huang3, Dawei Zhang2, Songlin Zhuang2, Shensheng Han1,5, Puxiang Lai3,4, *, and \nHonglin Liu1,5, * \n1Shanghai Institute of Optics and Fine Mechanics, Chinese Academy of Sciences, Shanghai 201800, China \n2School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai 200093, \nChina \n3Department of Biomedical Engineering, The Hong Kong Polytechnic University, Hong Kong SAR, China \n4Photonics Research Institute, The Hong Kong Polytechnic University, Hong Kong SAR, China \n5Center of Materials Science and Optoelectronics Engineering, University of Chinese Academy of Science, Beijing 100049, \nChina \n*puxiang.lai@polyu.edu.hk, and hlliu4@hotmail.com \n \nAbstract \nWith fast developments in computational power and algorithms, deep learning has made breakthroughs and been \napplied in many fields. However, generalization remains to be a critical challenge, and the limited generalization \ncapability severely constrains its practical applications. Hallucination issue is another unresolved conundrum haunting \ndeep learning and large models. By leveraging a physical model of imaging through scattering media, we studied the \nlack of generalization to system response functions in deep learning, identified its cause, and proposed a universal \nsolution. The research also elucidates the creation process of a hallucination in image prediction and reveals its cause, \nand the common relationship between generalization and hallucination is discovered and clarified. Generally speaking, \nit enhances the interpretability of deep learning from a physics-based perspective, and builds a universal physical \nframework for deep learning in various fields. It may pave a way for direct interaction between deep learning and the \nreal world, facilitating the transition of deep learning from a demo model to a practical tool in diverse applications.  \n \nIntroduction \nDeep learning, as an important branch of artificial intelligence (AI), has achieved remarkable breakthroughs and \ndevelopments over the past decade, being applied in various fields, such as computer vision, natural language \nprocessing, healthcare, financial technology, gaming and entertainment, and environmental science [1-6]. However, \ngeneralization remains to be a critical and common challenge in deep learning [7-9]. In the field of optical imaging, \nthe generalization of a neural network is related to various factors, e.g., different datasets, numerous optical systems, \nand alterations in incident angles, wavelengths and optical elements under the same optical system, posing huge \nhurdles for the adaptability of the network. \n    Taking imaging through scattering media [10-12] as an example, a neural network trained under a specific condition \ntypically only performs well under the same circumstance. Once the condition changes, the network often fails to \nreconstruct the image. The relationship between the input and output of an imaging system can generally be expressed \nas 𝑌= 𝑇𝑋, where 𝑋 is the input target, 𝑌 is the output, i.e., a speckle pattern, and 𝑇 is the scattering transmission \nmatrix of the system. It has been found that the key to address the generalization issue between different datasets is to \nlearn a relationship that closely approximates the mapping of the real physical system, and the solution to achieve this \ngoal is optimizing the training dataset so that each pixel on the input and output planes is sufficiently trained [13]. \nOther types of generalization challenges can be all attributed to 𝑇. The ability to identify multiple different matrices \n𝑇s of a neural network, in other words, the number of different inverse mappings 𝑇−1s a network can accommodate, \ndetermines its generalization capability.    \n    Under a specific condition, after training a network learns an approximate mapping relationship 𝑀 , which can \napproach 𝑇−1 infinitely in principle [13], to predict the image [14,15]. The network exhibits limited generalization \ncapability near 𝑇, for example, if 𝑐𝑜𝑟𝑟(𝑇, 𝑇′) ≥0.5, the network can reconstruct images for both 𝑇 and 𝑇′, though for \nthe latter case it typically shows degraded image quality.  However, when 𝑇′ is unrelated to 𝑇, the network fails to \npredict images for 𝑇′ [16]. To enhance the network's generalization across various 𝑇s, one potential approach is to \ntrain the network to simultaneously recognize multiple 𝑇s, and make them consecutively arranged by proper selections \nto enhance the generalization. The network′s compatibility to different 𝑇 s determines the upper threshold of its \ngeneralization capability. It should be noted that we are not talking about the generalization to predict from unseen \nscattering media based on residual ballistic light, whose 𝑇 is constant [16], in this study. \n    Compared to other systems, lensless imaging through scattering media offers an ideal physical model. In this model, \nthe system's configuration and parameters remain constant, and the transmission matrix 𝑇 can be altered simply by \nchanging the refractive index distribution within the illuminated region. There is no coupling between different \nphysical variables, while the situation changes for many other cases. For instance, once altering the distance between \nthe camera and the scattering medium, 𝑇 changes, and so does the field of view, speckle size, image resolution, et al., \nmaking it difficult to separate the influences of different factors. In contrast, by varying the refractive index distribution, \n𝑇 changes, while maintaining other factors constant. \n    By employing a typical CNN network in conjunction with a lensless system of imaging through scattering media, \nwe explored the neural network's ability to accommodate multiple transmission matrices 𝑇s. Our findings demonstrate \nthat training with multiple 𝑇s can enhance the network's generalization capability, and uncover the underlying causes \nof hallucinations, indicating a general solution to improve generalization and overcome hallucinations. This research \nalso enhances the interpretability of deep learning from the preceptive of physics, advances theoretical understanding, \nand promotes the applications of AI in various fields. \n \nMethods \nA solid-state laser (MGL-III-532-200mW, Changchun New Industries Optoelectronics Tech) with a wavelength 𝜆=\n532𝑛𝑚 was adopted as the illumination source, whose output was expanded to illuminate a digital micromirror device \n(DMD, V-7001 VIS, ViALUX). The reflected light from a target displayed on the DMD was scattered by a ground \nglass diffuser (DG10-120-MD, Thorlabs), and the corresponding speckle pattern was recorded by a CCD camera (Ace \nacA2440-75um, Basler). The object distance 𝑍1 = 16𝑐𝑚, and the image distance 𝑍2 = 10𝑐𝑚. The DMD has an array \nsize 1024×768 with a pixel size of 13.7 𝜇𝑚, and the CCD sensor is an array of 2448×2048 with a pixel pitch of 3.45 \n𝜇𝑚. Handwritten digit images from the MNIST database [17,18] were used as the targets, which were reshaped into \n64×64 arrays and displayed on the center of the DMD in subsequence. The diameter of the collimated beam on the \nDMD is 5 mm, sufficient to cover the whole region of the targets. \n \nFig.1 Schematics of the experiment. (a) The experimental setup, the iris has a diameter of 5 mm. (b) Illustration of the \narrangement of illumination areas to obtain training and testing data, the circles denote the illumination areas. The \ndash square is zoomed to see details clearly. 𝑇120 still has some overlap with 𝑇1. (c) The cross-correlation coefficient \ncurve for the diffuser at different displacements along the 𝑥 direction.   \n \n    The isoplanatic range of the 120-grit ground glass disk used in experiment is measured to be 40 𝜇𝑚 (Fig.1c). It \nmeans that a 40 𝜇𝑚  lateral shift of the diffuser results in a completely different transmission matrix, thus, \nfundamentally altering the mapping relationship of the experimental system. When the illumination region shifts \nwithin 20 𝜇𝑚 from the center, the new mapping relationship is similar to the original one, i.e., 𝑐𝑜𝑟𝑟(𝑇, 𝑇′) ≥0.5, and \nthe network retains a generalization ability. To test the network's compatibility with different mapping relationships, \nwe collected training data from different illumination regions, varying the number of independent mappings included \nin each training session to assess the network's compatibility. As shown in Fig.1b, the red circles represent the \nillumination regions. 𝑇2 is shifted 40 𝜇𝑚 to the right of 𝑇1, with each subsequent shift of 40 𝜇𝑚 until 𝑇120. This results \nin a total lateral shift of (120 −1) × 40𝜇𝑚= 4.76𝑚𝑚. For each 𝑇, the camera captures 5000 speckle images for \ntraining (a pair of an input target and the corresponding speckle pattern is a training sample) to ensure the learned \nrelationship stable. The illumination region of Test 1 coincides with 𝑇1, while Tests 2-4 are shifted upwards by 10, 20, \nand 50 𝜇𝑚 from Test 1, respectively.  \n    As the number of mapping relationships in the training set increases, the data volume, and consequently, the training \ntime, also rises rapidly. Two training strategies were adopted to a trial study. \nStrategy I: The network started with the same initial conditions, and 5000 ∗𝑛 pairs of data, where 𝑛 was the number \nof mapping relationships, were used for training at once. \nStrategy II: Each time a new transmission matrix 𝑇 was added, and the corresponding 5000 pairs of data were used \nto continuously train the network. This is to say, the network corresponding to 𝑛 𝑇s was trained by applying the new \n5000 pairs of training data on the network already trained for (𝑛−1) 𝑇s.  \nCompared to Strategy I, the computational resource consumption in the Strategy II was significantly reduced. In both \ncases, Test 1 was used for evaluation, and the Pearson Correlation Coefficient (PCC) [19, 20] between the predicted \nimages and the ground truth was used to measure the prediction accuracy. \n    The generalization range in illumination area of the network was demonstrated and quantified in a paired \ncomparison, where the training and testing areas were modified. \n    In the study, a CNN (U-Net) was adopted on Tensorflow2/Keras under the hardware environment of NVIDIA \nRTX3090. The batch size is 10, the time required for each epoch is about 49s, and when the number of training samples \ndoubles, the training time also doubles.  \n   Usually, there is a small amount of ballistic light residue when a light beam passes through a diffuser, and the \ntransmission matrix of the ballistic component does not change with refractive index variations in the illuminated \nregion. In experiment, it is challenging to eliminate ballistic residue or control its proportion. Therefore, in addition \nto experimental data, we also included simulation data for training and testing, where the proportion of ballistic \ncomponent could be precisely controlled. Based on wave optics, we simulated the corresponding speckle pattern for \neach digit image. In simulation, the ground glass was modeled by a random phase screen with a phase function \nexp [−𝑗𝐶𝜙(𝑥, 𝑦)] , where the phase 𝜙(𝑥, 𝑦)  had a Gaussian random distribution [16, 21], and the proportion 𝜂  of \nballistic residue was controlled by adjusting the coefficient 𝐶. \n \nResults \nIn experiment, a performance comparison between the two training strategies is shown in Fig.2. As the number of 𝑇 \nincreases, the quality of the predicted images gradually decreases and then stabilizes. The prediction accuracy of the \ntwo strategies is comparable, with negligible differences. Therefore, Strategy II was adopted for subsequent network \ntraining. Due to the presence of residual ballistic light, the PCC stabilizes at 0.65, enabling successful image prediction \neven when 𝑛= 40. Here, the ballistic residue is the key to such generalization capability [16]. \n \n                                                           Fig.2 Comparison of Strategies I and II. \n \nBased on the cross-correlation coefficient curve in Fig.1, whose tail is higher than 0, the proportion of residual ballistic \nlight after passing through the diffuser is estimated to be 𝜂≈0.01 . Fig.3 compares the networks trained with \nexperimental data and simulated data, where two cases of 𝜂= 0 and 𝜂= 0.01 are considered for the latter. When 𝜂=\n0.01, the simulation results match the experimental data well, confirming the reliability of the simulation and the \nproportion estimation of the ballistic residue. Without ballistic residue (𝜂= 0), as shown by the red squares and the \ncurve, the network's predictive ability diminishes, when the number of 𝑇 reaches 30, with prediction failures (see the \nfour inserted images for 𝑛= 31, 36, 41, 51). In contrast, the prediction at 𝑛= 41 for 𝜂= 0.01 is still recognizable, \nas shown in the inserted image above the yellow curve, where information is extracted from ballistic light instead of \nscattered light. From Fig.3, we can say that without ballistic light the network can still predict images well when 𝑛=\n20, that is to say, the network can accommodate 20 independent mappings 𝑇s. Therefore, the number of 𝑇s that the \nU-net network can effectively handle is selected as 20.   \n \n Fig.3 Compatibility results of the U-net network. Experimental results consist well with the simulation results at 𝜂=\n0.01. \n \n    To assess the network's compatibility with different 𝑇 s for scattered light and demonstrate the enhanced \ngeneralization capability, it is necessary to eliminate the influence of ballistic components. Fig.4 shows the predicted \nimaging results of the network in a paired comparison when it is trained to accommodate 20 different 𝑇s in the absence \nof ballistic residue with simulation data. In Group I, 𝑇𝑖+1 is shifted 40 𝜇𝑚 to the right of 𝑇𝑖, and five different regions \nwere tested. Test A coincides with the region of 𝑇1 , while Tests B, C, D and E are shifted to the right of 𝑇1  by \n0.5 × 40 𝜇𝑚 , 2.3 × 40 𝜇𝑚 , 9.6 × 40 𝜇𝑚  and 18.2 × 40 𝜇𝑚 , respectively, with the latter four positions randomly \nselected within the scanning range from 𝑇1  to 𝑇20 . The network successfully predicted images through all five \npositions, demonstrating generalization across the range of 𝑇1 to 𝑇20. In Group II, 𝑇𝑖+1 was shifted 30 𝜇𝑚 to the right \nof 𝑇𝑖 , and five different regions were tested. Test A coincides with the region of 𝑇1, while Tests B, C, D and E were \nshifted 0.5 × 30 𝜇𝑚, 2.3 × 30 𝜇𝑚, 9.6 × 30 𝜇𝑚, and 18.2 × 30 𝜇𝑚 to the right of 𝑇1, respectively. Similar to Group \nI, the network could successfully image through any position within the scan range. However, due to partial correlation \nbetween 𝑇𝑖 and 𝑇𝑖+1, the actually accommodated independent mapping relationships was less than 20, which results \nin better imaging quality compared to Group I. Obviously, by carefully selecting mapping relationships, the network's \ngeneralization can be improved, ensuring reliable predictions over a broader range, i.e., enhancing the generalization \nof the network. To further enhance generalization to various changes in the system, efforts should be focused on \nmodifying the network to increase its compatibility to different mapping relationships. \n \nFig.4 Generalization demonstration of the CNN in imaging through a scattering medium. (a) Schematic illustration of \ndifferent illumination areas to obtain the training and testing data. The insert in the up right corner is the ground truth. \n(b) Predicting results of digit target “5” in Tests A-E, the PCC value of each reconstructed image with the ground truth \nis added in its bottom.  \n \n    Fig.5 presents the results of network predictions under varying numbers of independent 𝑇 by shifting the diffuser \nalong 𝑥 axis (see Fig.1b). Without ballistic residue, the network can still predict images within the permissible number  \n \n \nFig.5 Comparison of image prediction under different numbers of 𝑇 in training between 𝜂= 0 and 𝜂= 0.01. (a) 𝜂=\n0, information is only extracted from scattered light, beyond the compatibility of the network, no image can be \nreconstructed but hallucinations appear. Within the compatibility, the image can be reconstructed, but the quality \ndeteriorates as ∆𝑦 increases. When ∆𝑦= 0,  the image quality for the case of training only with 𝑇1 is the best, since \nthere is no compromise between different 𝑇.  (b) 𝜂= 0.01. Within the compatibility, information can be reconstructed \nfrom scattered light, as shown in the first three rows of reconstructions at ∆𝑦= 0, 10, 20 𝜇𝑚. Information can also be \nextracted from ballistic photons, its influence on reconstruction is trivial due to its small portion, however, can be \nenhanced as more areas are seen in training. Beyond the compatibility, the network fails to predict the target from \nscattered photons, but its ability to extract information from ballistic light is enhanced, and images can be recognized \neven though there are hallucinations generated from scattered light. (c, d) Variations of the PCC value with ∆𝑦 in \ndifferent cases. \nof independent 𝑇, though the image qualities in Test 1 decline slightly. However, the network’s prediction capability \nrapidly deteriorates from Test 1 to 4. At Δ𝑦= 20 𝜇𝑚, near the edge of original generalization range, the prediction is \nmerely successful; at Δ𝑦= 50 𝜇𝑚 , the prediction fails (Effective relationships are learned, and there is target \ninformation in the speckles, but the network can’t decode it due to mismatching between encoding and decoding. If \nthere is no target information in speckles, the same phenomena will appear, too.). When the number of independent 𝑇 \nexceeds the network's compatibility, the network must compromise to all 𝑇s, leading to a failure to approximate any \nspecific 𝑇. This results in predictions completely unrelated to the ground truths, i.e., hallucinations [22-26], as shown \nin the fifth and sixth rows of Fig.5a. Clearly, these hallucinations arise due to the network’s failure to learn any effective \nmapping relationship, resulting in the prediction lacking of constraint. Despite this, the network captures a general \nfeature that handwritten digit images are centered, which is why the hallucinations have such characteristic. However, \nwhen a small amount of ballistic component is introduced (𝜂= 0.01), the target becomes recognizable, because the \nnetwork has learned the mapping relationship of ballistic component and allowed to extract information from the \nballistic residue based on that relationship. The hallucination from the scattering component (more specifically due to \nthe compromised relationship learned from scattered light) remains, creating artifacts in the image. Increasing the \nproportion of ballistic component can reduce and eventually eliminate these artifacts. As an effective mapping \nrelationship emerges, the hallucinations diminish even disappear. The network can simultaneously learn mapping \nrelationships for both the scattering and ballistic components, allowing it to extract information from both. With fewer \n𝑇s, the network primarily extracts information from the scattering component, but as the number of 𝑇 increases, the \nnetwork's ability to extract images from the ballistic component is strengthened. Since the ballistic component is \nalmost unaffected by position, the network’s sensitivity to positional shifts decreases in such prediction, i.e., with \nenhanced generalization to see through unseen scattering media [16].  \n \n  The creation process of a hallucination with increased number of 𝑇  is illustrated in Fig.6. As the number of 𝑇 \nincreases in training, the reconstructed images become increasingly distorted until they no longer reserve any feature \nof the target to recognize it, i.e., the output image is irrelevant to the input target. In a word, the distortion becomes a \nhallucination when reaching an extreme. The proportion of artifacts in the predicted image for different numbers of 𝑇 \nis added below each image. Without ballistic component (𝜂= 0), neoplasms dominate when the number of 𝑇 reaches \n40 or more, leading to hallucinations. However, introducing a small amount of ballistic component (𝜂= 0.01 ) \nsignificantly reduces artifacts, allowing the target to be discerned again. \n \nFig.6 Creation process of a hallucination in the prediction of a digit 5 with increased number of 𝑇. Reconstructed \nimages are displayed in the first row, the second row shows the overlay results of the digit in black on the reconstructed \nimages. As the ratio of the neoplasm area to the image area goes beyond 70%, a hallucination is created. \nDiscussions \nBased on above findings, training a network with data from multiple distinct 𝑇s allows the network to learn multiple \ninverse mapping relationships 𝑇−1s simultaneously. However, as the number of 𝑇 increases, the learned relationship \nincreasingly deviates from every mapping, eventually turns to be irrelevant to any specific 𝑇−1. In other words, there \nis no regulation in the trained network to constrain image recovery, and the prediction is random and aimless, that is, \ndisorder is the cause of hallucinations. Here, generalization and hallucination are inherently associated; improving \ngeneralization by accommodating more mapping relationships results in network predictions closer to hallucinations. \nTo enhance generalization while suppressing hallucinations, a straightforward strategy is to expand the network's \narchitecture, providing greater flexibility, which is the solution adopted in big models. However, while enhancing \ncompatibility and improving generalization, it introduces greater complexity in network and unpredictability in \nperformance. The future research of AI, particularly in deep learning and large models, could focus on designing \nappropriate network architectures to increase generalization capabilities while anticipating and mitigating potential \nunexpected outcomes. \n   There are different definitions of the hallucination, after the semantic shift in the late 2010s, the emphasis turns to \nbe the generation of factually incorrect or misleading output by AI systems. About its causes there are speculations on \ndata and modeling, such as, source-reference divergence and imperfect modeling.  By studying the generalization in \n𝑋 [13] and 𝑇, the causes of artifacts/incorrectness in predictions are differentiated. In this paper, hallucinations are \nincorrect generations due to lack of effective constraints in network prediction, to be more extreme, totally unrelated \nto the input. Someone might insist the artifacts/incorrectness caused by source-reference divergence as hallucinations, \nplease bear in mind that this type of hallucination can be suppressed by enhanced generalization, i.e., learning a \nrelationship approaching the real mapping as close as possible [13]. Neglecting the immeasurable complexities and \ninfinite source requirements, there is still no way to integrate all principles of nature and rules of humanity into an AI \nmodel, since many are unknowns, hence, hallucinations can be mitigated by not eliminated for an open model. It is \ndifferent for closed spaces, such as chess and Go, where all rules are accurately known, there is no hallucination in \nnetwork prediction normally. \n \n   A neural network is usually considered as a black box, it is difficult to explain how the internal changes affecting \nthe output. In order to enhance interpretability, traditional investigations focus on the conceptual representation of \nhidden layers and changes in weight parameters, such as hidden layer analysis methods, simulation/agent models and \nsensitivity analysis methods [27-31]. Different from above approaches, in our study the simulation capability of a \nCNN network to an imaging system is checked with reference to changes in the real physical system. The \ngeneralization capability of a network can be quantified and its performance is predictable. Moreover, it will be \npossible to locate problems and provide potential solutions. This paves a new way for network interpretability. In \nfuture, combing with key parameter change monitoring, sensitivity analysis, statistical analysis, etc., the \ninterpretability of deep learning could be greatly improved, thus build a solid foundation to enhance practical \napplications of AI in various fields. \n \nConclusions \nBy employing a physical model of imaging through scattering media, the issue of deep learning's lack of generalization \nto changes in system response functions is investigated. We identified the root causes of this limitation and proposed \na universal solution, explained the occurrence of hallucinations and discovered the relationship between hallucination \nand generalization. In combination with Ref. [13], a general physical framework for applications of deep learning in \nvarious fields is built. The interpretability of deep learning is also enhanced from a physical modeling perspective. It \nmay reshape the developments of deep learning, facilitate the transition of deep learning from a demo model to a \npractical tool in diverse applications, and enable direct interactions between AI and the real world someday. \n \nFunding  \nThe work was supported by National Natural Science Foundation of China (NSFC) (81930048), Guangdong Science \nand Technology Commission (2019BT02X105), Hong Kong Research Grant Council (15217721, R5029-19, C7074-\n21GF), Shenzhen Science and Technology Innovation Commission (JCYJ20220818100202005), and The Hong Kong \nPolytechnic University (P0045680, P0043485, P0045762, P0049101). \n \nAcknowledgements  \nH. L. conceived the idea and designed the study. X. Z. implemented the experiment and simulation. H. L. X. Z., H. \nH., and P. L. analyzed the data and wrote the manuscript. All contributed to revise the manuscript. \n \nCompeting Interests \nThe authors declare no conflict of interests. \n \nReferences \n[1] Voulodimos, A., Doulamis, N., Doulamis, A., and Protopapadakis, E. \"Deep Learning for Computer Vision: A \nBrief Review,\" Computational Intelligence and Neuroscience, 2018, 2018, 7068349. \n[2] Otter, D. W., Medina, J. R., and Kalita, J. K. \"A Survey of the Usages of Deep Learning for Natural Language \nProcessing,\" IEEE Transactions on Neural Networks and Learning Systems, 2021, 32(2), 604-624. \n[3] Shamshirband, S., Fathi, M., Dehzangi, A., Chronopoulos, A. T., and Alinejad-Rokny, H. \"A review on deep \nlearning approaches in healthcare systems: Taxonomies, challenges, and open issues,\" Journal of Biomedical \nInformatics, 2021, 113, 103627. \n[4] Ye, X. W., Jin, T., and Yun, C. B. \"A review on deep learning-based structural health monitoring of civil \ninfrastructures,\" Smart Structures and Systems, 2019, 24(5), 567-585. \n[5] Andersen, P. A., Goodwin, M., and Granmo, O. C. \"Towards a Deep Reinforcement Learning Approach for \nTower Line Wars,\" In M. Bramer & M. Petridis (Eds.), Artificial Intelligence XXXIV, AI 2017, Lecture Notes \nin Artificial Intelligence, 37th SGAI International Conference on Artificial Intelligence (AI), Cambridge, \nENGLAND, Dec 12-14, 2017, 10630, 101-114. \n[6] Feng, S. J., Ji, K. F., Wang, F. L., Zhang, L. B., Ma, X. J., and Kuang, G. Y. \"PAN: Part Attention Network \nIntegrating Electromagnetic Characteristics for Interpretable SAR Vehicle Target Recognition,\" IEEE \nTransactions on Geoscience and Remote Sensing, 2023, 61, 5204617. \n[7] Ganaie, M. A., Hu, M. H., Malik, A. K., Tanveer, M., and Suganthan, P. N. \"Ensemble deep learning: A review,\" \nEngineering Applications of Artificial Intelligence, 2022, 115, 105151. \n[8] Fan, Z. H., Xu, Q. F., Jiang, C. X., and Ding, S. X. \"Deep Mixed Domain Generalization Network for Intelligent \nFault Diagnosis Under Unseen Conditions,\" IEEE Transactions on Industrial Electronics, 2024, 71(1), 965-974. \n[9] Zhou, D. X. \"Universality of deep convolutional neural networks,\" Applied and Computational Harmonic \nAnalysis, 2020, 48(2), 787-794. \n[10] L Xu X, Liu HL, and Wang LV. Time-reversed ultrasonically encoded optical focusing into turbid media. Nature \nPhotonics. 2011; 5, 154-157. \n[11] Katz O, Small E, Silberberg Y. Looking around corners and through thin turbid layers in real time with scattered \nincoherent light. NATURE PHOTONICS. 2012;6(8):549-53. \n[12] Yu ZP, Li HH, Zhong TT, Park J, Cheng SF, Woo CM, Zhao Q, Yao J, Zhou YY, Huang XZ, Pang WR, Peng \nLF, Yoon H, Shen YC, Liu HL, Zheng YJ, Park YK, Wang LV, and Lai PX. Wavefront shaping: A versatile \ntool to conquer multiple scattering in multidisciplinary fields. THE INNOVATION. 2022;3(5):623-637. \n[13] X. Zhang, H. Huang, D. Zhang, S. Zhuang, S. Han, P. Lai, and H. Liu, \"Cross-Dataset Generalization in Deep \nLearning, \" arXiv:2410.11207[cs.LG]. \n[14]  Meng Lyu, Hao Wang, Guowei Li, and Guohai Situ. Exploit imaging through opaque wall via deep learning. \nComputer and information sciences. 2017. \n[15] Li, S., Deng, M., Lee, J., Sinha, A., and Barbastathis, G. \"Imaging through glass diffusers using densely \nconnected convolutional networks,\" Optica, 2018, 5(7), 803-813. \n[16] Zhang, X. Y., Cheng, S. F., Gao, J. J., Gan, Y., Song, C. Y., Zhang, D. W., Zhuang, S. L., Han, S. S., Lai, P. X., \nand Liu, H. L. \"Physical origin and boundary of scalable imaging through scattering media: a deep learning-\nbased exploration,\" Photonics Research, 2023, 11(6), 1038-1046. \n[17] Saini, A., Daniel, S., Saini, S., and Mittal, A. \"EffKannadaRes-NeXt: An efficient residual network for Kannada \nnumeral recognition,\" Multimedia Tools and Applications, 2021, 80(18), 28391-28417. \n[18] Ahlawat, S., and Choudhary, A. \"Hybrid CNN-SVM Classifier for Handwritten Digit Recognition,\" In V. Singh, \nV. K. Asari, & K. C. Li (Eds.), International Conference on Computational Intelligence and Data Science, \nProcedia Computer Science, NorthCap University, Gurugram, India, Sep 06-07, 2019, 167, 2554-2560. \n[19] Edelmann, D., Móri, T. F., and Székely, G. J. \"On relationships between the Pearson and the distance correlation \ncoefficients,\" Statistics and Probability Letters, 2021, 169, 108960. \n[20] Ly, A., Marsman, M., and Wagenmakers, E.-J. \"Analytic posteriors for Pearson's correlation coefficient,\" \nStatistica Neerlandica, 2018, 72(1), 4-13. \n[21] Wu, J. J. \"Simulation of rough surfaces with FFT,\" Tribology International, 2000, 33(1), 47-58. \n[22] Ji, Z. W., Lee, N., Frieske, R., Yu, T. Z., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. \"Survey \nof Hallucination in Natural Language Generation,\" ACM Computing Surveys, 2023, 55(12). \n[23] Jiang, K., Wang, Z. Y., Yi, P., Lu, T., Jiang, J. J., & Xiong, Z. X. \"Dual-Path Deep Fusion Network for Face \nImage Hallucination,\" IEEE Transactions on Neural Networks and Learning Systems, 2022, 33(1), 378-391. \n[24] Wicky, B. I. M., Milles, L. F., Courbet, A., Ragotte, R. J., Dauparas, J., Kinfu, E., Tipps, S., Kibler, R. D., Baek, \nM., DiMaio, F., Li, X., Carter, L., Kang, A., Nguyen, H., Bera, A. K., and Baker, D. \"Hallucinating symmetric \nprotein assemblies,\" Science, 2022, 378(6615), 56-61. \n[25] Anishchenko, I., Pellock, S. J., Chidyausiku, T. M., Ramelot, T. A., Ovchinnikov, S., Hao, J. Z., Bafna, K., Norn, \nC., Kang, A., Bera, A. K., DiMaio, F., Carter, L., Chow, C. M., Montelione, G. T., and Baker, D. \"De novo \nprotein design by deep network hallucination,\" Nature, 2021, 600(7889), 547+. \n[26] Chinmay Belthangady and Loic A. Royer, “Applications, promises, and pitfalls of deep learning for fluorescence \nimage reconstruction,” Nature Methods 16, 1215-1225 (2019). \n[27] Meissler, N., Wohlan, A., Hochgeschwender, N., and Schreiber, A. \"Explore Convolutional Neural Networks in \nVirtual Reality,\" In Proceedings of the 2019 IEEE International Conference on Artificial Intelligence and Virtual \nReality (AIVR), 2nd IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR), San \nDiego, CA, Dec 09-11, 2019, 249-250. \n[28] Zeiler, M.D., Fergus, R. \"Visualizing and Understanding Convolutional Networks, \" Computer Vision – ECCV \n2014, 2014, 818-833. \n[29] Bau, D., Zhou, B. L., Khosla, A., Oliva, A., and Torralba, A. \"Network Dissection: Quantifying Interpretability \nof Deep Visual Representations,\" In Proceedings of the 30th IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR 2017), Honolulu, HI, Jul 21-26, 2017, 3319-3327. \n[30] Zhang, Y., Tino, P., Leonardis, A., and Tang, K. \"A Survey on Neural Network Interpretability,\" IEEE \nTransactions on Emerging Topics in Computational Intelligence, 2021, 5(5), 726-742. \n[31] Li, X. H., Xiong, H. Y., Li, X. J., Wu, X. Y., Zhang, X., Liu, J., Bian, J., and Dou, D. J. \"Interpretable deep \nlearning: interpretation, interpretability, trustworthiness, and beyond,\" Knowledge and Information Systems, \n2022, 64(12), 3197-3234. \n \n",
  "categories": [
    "physics.optics"
  ],
  "published": "2024-11-05",
  "updated": "2024-11-05"
}