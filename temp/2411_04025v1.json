{
  "id": "http://arxiv.org/abs/2411.04025v1",
  "title": "Prompt Engineering Using GPT for Word-Level Code-Mixed Language Identification in Low-Resource Dravidian Languages",
  "authors": [
    "Aniket Deroy",
    "Subhankar Maity"
  ],
  "abstract": "Language Identification (LI) is crucial for various natural language\nprocessing tasks, serving as a foundational step in applications such as\nsentiment analysis, machine translation, and information retrieval. In\nmultilingual societies like India, particularly among the youth engaging on\nsocial media, text often exhibits code-mixing, blending local languages with\nEnglish at different linguistic levels. This phenomenon presents formidable\nchallenges for LI systems, especially when languages intermingle within single\nwords. Dravidian languages, prevalent in southern India, possess rich\nmorphological structures yet suffer from under-representation in digital\nplatforms, leading to the adoption of Roman or hybrid scripts for\ncommunication. This paper introduces a prompt based method for a shared task\naimed at addressing word-level LI challenges in Dravidian languages. In this\nwork, we leveraged GPT-3.5 Turbo to understand whether the large language\nmodels is able to correctly classify words into correct categories. Our\nfindings show that the Kannada model consistently outperformed the Tamil model\nacross most metrics, indicating a higher accuracy and reliability in\nidentifying and categorizing Kannada language instances. In contrast, the Tamil\nmodel showed moderate performance, particularly needing improvement in\nprecision and recall.",
  "text": "Prompt Engineering Using GPT for Word-Level\nCode-Mixed Language Identification in Low-Resource\nDravidian Languages\nAniket Deroy1,*,†, Subhankar Maity1\n1IIT Kharagpur, Kharagpur, India\nAbstract\nLanguage Identification (LI) is crucial for various natural language processing tasks, serving as a foundational step\nin applications such as sentiment analysis, machine translation, and information retrieval. In multilingual societies\nlike India, particularly among the youth engaging on social media, text often exhibits code-mixing, blending local\nlanguages with English at different linguistic levels. This phenomenon presents formidable challenges for LI\nsystems, especially when languages intermingle within single words. Dravidian languages, prevalent in southern\nIndia, possess rich morphological structures yet suffer from under-representation in digital platforms, leading to\nthe adoption of Roman or hybrid scripts for communication. This paper introduces a prompt based method for a\nshared task aimed at addressing word-level LI challenges in Dravidian languages. In this work, we leveraged\nGPT-3.5 Turbo to understand whether the large language models is able to correctly classify words into correct\ncategories. Our findings show that the Kannada model consistently outperformed the Tamil model across most\nmetrics, indicating a higher accuracy and reliability in identifying and categorizing Kannada language instances.\nIn contrast, the Tamil model showed moderate performance, particularly needing improvement in precision and\nrecall.\nKeywords\nGPT, Word level identification, Classification, Low-resource Languages, Prompt Engineering\n1. Introduction\nLanguage Identification (LI) [1] is a fundamental task in natural language processing (NLP) that involves\ndetermining the language(s) present in a given text. This task is pivotal for numerous applications such\nas sentiment analysis, machine translation, information retrieval, and natural language understanding.\nAccurate LI becomes particularly challenging in multilingual societies where texts often exhibit code-\nmixing, a phenomenon where multiple languages co-occur within the same discourse, ranging from\nphrases to individual words.\nIn the context of India, a country renowned for its linguistic diversity [2, 3, 4, 5], social media\nplatforms reflect a vibrant mix of languages. Among the youth, in particular, there is a prevalent\nuse of code-mixed text that blends local languages from the Dravidian language family with English.\nDravidian languages, spoken predominantly in southern India, including languages like Kannada, Tamil,\nMalayalam, and Tulu, are characterized by rich morphological structures and diverse linguistic features.\nHowever, despite their significance, these languages face technological challenges, such as inadequate\ndigital representation and script variations, which complicate language processing tasks like LI.\nThis paper focuses on addressing the specific challenges of word-level LI in Dravidian languages,\nleveraging the unique linguistic characteristics and code-mixed nature prevalent in social media and\ndigital communications. We introduce a prompt engineering based method aimed at advancing LI\ncapabilities in these languages by experimenting at different temperature values. By doing so, we aim\nto contribute to the broader goal of enhancing NLP tools for under-resourced languages, ultimately\nfacilitating more accurate and inclusive language processing technologies.\nForum for Information Retrieval Evaluation, December 12-15, 2024, India\n*Corresponding author.\n$ roydanik18@kgpian.iitkgp.ac.in (A. Deroy); subhankar.ai@kgpian.iitkgp.ac.in (S. Maity)\n\u001a 0000-0001-7190-5040 (A. Deroy); 0009-0001-1358-9534 (S. Maity)\n© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\narXiv:2411.04025v1  [cs.CL]  6 Nov 2024\nIn this work, we leveraged GPT-3.5 Turbo [6, 7, 8, 9, 10] to understand whether the large language\nmodels is able to correctly classify words into correct categories. We experiment with GPT at different\ntemperature values namely 0.7, 0.8, and 0.9.\nGPT models are trained on large corpora from the internet, but the availability of high-quality data\nin Dravidian languages is limited compared to more widely spoken languages like English, Spanish, or\nChinese. This means that GPT might not have been exposed to as much diverse or extensive data in\nthese languages. Dravidian languages use distinct scripts (e.g., Tamil script for Tamil, Kannada script\nfor Kannada). Moreover, code-mixing (where Dravidian languages are mixed with English or Hindi,\noften using the Roman script) is common on social media and informal communications. GPT’s ability\nto handle code-mixed text varies and may not be as robust as its handling of pure English text.\nWe observe that for Tamil and Kannada, GPT models have significant room for improvement.\n2. Related Work\nLanguage Identification (LI) [6, 11, 12, 13, 14, 15] has been a crucial area of research within Natural\nLanguage Processing (NLP) due to its foundational role in various applications such as sentiment analysis,\nmachine translation, and information retrieval. Traditional LI approaches [16, 17, 18, 19, 20, 21, 22] have\nprimarily focused on monolingual or bilingual contexts, where clear boundaries between languages\nare assumed. However, these methods often struggle in multilingual and code-mixed environments,\nespecially in regions like India, where linguistic diversity [23, 24, 25, 26, 27, 28, 29, 30] is high and social\nmedia usage reflects complex language practices.\nCode-mixing [31, 32, 33, 34, 35, 36] presents unique challenges for LI systems. Early research in\ncode-mixing focused on language pairs like English-Spanish or Hindi-English, where code-mixed texts\npredominantly used Roman scripts. Notable works explored the linguistic features of code-switched\ntexts and highlighted the difficulties in segmenting and identifying languages at the word level. Similarly,\nHindi-English code-mixed social media text, emphasizes the necessity for specialized LI models capable\nof handling intra-word language switches.\nDravidian languages [37, 38, 39, 40, 41] have been relatively underexplored in the context of LI,\nprimarily due to the scarcity of annotated datasets and the complex morphological characteristics\ninherent to these languages. Previous efforts have developed initial datasets and models for LI in\nDravidian languages; however, these models often fall short in handling code-mixed text, where Roman\nor hybrid scripts are employed. The Dravidian-CodeMix shared task aimed to address some of these gaps\nby introducing datasets for Tamil, Malayalam, and Kannada, which included code-mixed instances. Yet,\nthe performance of models on these datasets indicated significant room for improvement, particularly\nin distinguishing between closely related languages and dialects.\nLarge Language Models (LLMs) [42, 43, 44, 45, 46, 47, 48, 49] like GPT-3 have shown promise in various\nNLP tasks, including LI. Previous works have demonstrated the capability of GPT-3 in performing zero-\nshot and few-shot learning, making it a potentially powerful tool for LI in resource-constrained settings.\nHowever, the application of LLMs [50, 51, 52, 53, 54, 55, 56, 57] to code-mixed and morphologically rich\nlanguages remains underexplored. Recent studies, have started to explore the use of transformers and\npre-trained models for multilingual LI, but the effectiveness of these models in code-mixed Dravidian\nlanguages, particularly at the word level, requires further investigation.\nOur work builds upon these existing efforts by focusing on a prompt-based method using GPT-3.5\nTurbo to address word-level LI challenges in Dravidian languages. Unlike previous approaches, we\nleverage the linguistic diversity and code-mixed nature of the datasets to enhance the robustness of LI\nsystems in detecting and classifying under-resourced languages. This study contributes to the growing\nbody of research by providing a prompt engineering based method for Kannada, Tamil and evaluating\nthe performance of advanced LLMs in this complex linguistic landscape.\n3. Dataset\nThis shared task consists of four distinct datasets [58, 59, 60, 61, 62],:\n1. Tulu Dataset: The Tulu dataset consists of 7,171 code-mixed sentences sourced from YouTube\nvideos. These sentences have undergone preprocessing to eliminate non-textual characters and\nhave been transliterated into Roman script. The dataset comprises 36,002 words categorized into\nsix classes: ’Tulu’, ’Kannada’, ’English’, ’Mixed-language’, ’Name’, and ’Location’. The presence\nof mixed-language words poses significant challenges due to their dynamic and context-specific\nnature.\n2. Kannada Dataset: This dataset includes 14,847 tokens represented in Roman script. It is classified\ninto six categories: ’Kannada’, ’English’, ’Mixed-language’, ’Name’, ’Location’, and ’Other’. The\ndataset aims to enhance methods for language identification and categorization specifically\ntailored for Kannada-English code-mixed texts.\n3. Tamil Dataset: The Tamil dataset consists of 17,568 tokens following a methodology similar\nto that of the Tulu and Kannada datasets. It is categorized into six classes and supports various\nnatural language processing tasks specific to the Tamil language domain.\n4. Malayalam Dataset: The Malayalam dataset comprises 25,035 tokens categorized into seven\nclasses: ’Malayalam’, ’English’, ’Mixed’, ’Name’, ’Number’, ’Location’, and ’sym’ (for sentence\nboundaries). This dataset offers extensive coverage for natural language processing tasks and\nincludes additional categories such as ’Number’ for numerical values, similar to the other datasets\nprovided.\nWe participated in shared tasks based on two languages, namely, Kannada and Tamil. The test dataset\nsize for Kannada is 2502. The test dataset size for Tamil is 2024.\n4. Task Definition\nThe goal of this task is to classify individual words from a code-mixed text into predefined categories or\nclasses. The words should be classified into the following categories:\n• English: Words or phrases that are in the English language (e.g., hello, book, run).\n• Dravidian: Words or phrases that are in the Kannada language or Tamil language.\n• Mixed: Words or phrases that mix English, Kannada, or Tamil or combine elements from both\nlanguages.\n• Name: Proper nouns, including names of people, organizations, etc. (e.g., John, Infosys).\n• Location: Names of places, such as cities, countries, or landmarks (e.g., Bangalore, India, Taj\nMahal).\n• Symbol: Symbols or punctuation marks used in the text (e.g., *, =, #, ;).\n• Other: Words or elements that do not fit into the above categories or are ambiguous.\n5. Methodology\n5.1. Why Prompting?\nPrompting [63] to solve a word-level classification problem often arises from the need to accurately\nidentify and categorize individual words within texts that exhibit code-mixing or multilingual content.\nHere are some reasons why this problem is prompted:\n- Code-Mixing in Texts: In multilingual societies or digital platforms, texts frequently mix\nlanguages, such as local languages with English [64]. Understanding which language each\nword belongs to is essential for applications like sentiment analysis, machine translation, and\ninformation retrieval.\n- Accuracy in Language Processing: For effective natural language processing (NLP), identifying\nthe language of each word enhances the accuracy of subsequent tasks [65, 66]. It ensures that\nlanguage-specific models or algorithms are applied correctly.\n- Contextual Understanding: Words in code-mixed texts can change meaning based on the\nlanguage they are derived from [67]. Accurate language identification at the word level aids in\npreserving context and meaning during NLP tasks.\n- Challenges and Innovation: Word-level classification poses challenges due to the intricacies of\ncode-mixed languages, where words may seamlessly blend multiple languages or scripts [68].\nAddressing these challenges fosters innovation in NLP methodologies and technologies.\nIn summary, prompting to solve word-level classification problems stems from the practical need to\naccurately handle code-mixed languages and optimize language-specific processing in diverse linguistic\ncontexts.\n5.2. Prompt Engineering-Based Approach\nWe used the GPT-3.5 Turbo (Figure 1) model via prompting1 to solve the classification task. We\nused GPT-3.5 Turbo in Zero-Shot mode via prompting. After the prompt is provided to the LLM, the\nfollowing steps occur internally while generating the output. We list these steps, summarizing the\nprompting approach using GPT-3.5 Turbo:\nStep 1: Tokenization\n• Prompt: 𝑋= [𝑥1, 𝑥2, . . . , 𝑥𝑛]\n• The input text (prompt) is first tokenized into smaller units called tokens. These tokens are often\nsubwords or characters, depending on the model’s design.\n• Tokenized Input: 𝑇= [𝑡1, 𝑡2, . . . , 𝑡𝑚]\nStep 2: Embedding\n• Each token is converted into a high-dimensional vector (embedding) using an embedding matrix\n𝐸.\n• Embedding Matrix: 𝐸∈R|𝑉|×𝑑, where |𝑉| is the size of the vocabulary and 𝑑is the embedding\ndimension.\n• Embedded Tokens: 𝑇emb = [𝐸(𝑡1), 𝐸(𝑡2), . . . , 𝐸(𝑡𝑚)]\nStep 3: Positional Encoding\n• Since the model processes sequences, it adds positional information to the embeddings to capture\nthe order of tokens.\n• Positional Encoding: 𝑃(𝑡𝑖)\n• Input to the Model: 𝑍= 𝑇emb + 𝑃\nStep 4: Attention Mechanism (Transformer Architecture)\n• Attention Score Calculation: The model computes attention scores to determine the importance\nof each token relative to others in the sequence.\n• Attention Formula:\nAttention(𝑄, 𝐾, 𝑉) = softmax\n(︂𝑄𝐾𝑇\n√𝑑𝑘\n)︂\n𝑉\n(1)\n1https://platform.openai.com/docs/models/gpt-3-5-turbo\n• where 𝑄(query), 𝐾(key), and 𝑉(value) are linear transformations of the input 𝑍.\n• This attention mechanism is applied multiple times through multi-head attention, allowing the\nmodel to focus on different parts of the sequence simultaneously.\nStep 5: Feedforward Neural Networks\n• The output of the attention mechanism is passed through feedforward neural networks, which\napply non-linear transformations.\n• Feedforward Layer:\nFFN(𝑥) = max(0, 𝑥𝑊1 + 𝑏1)𝑊2 + 𝑏2\n(2)\n• where 𝑊1, 𝑊2 are weight matrices and 𝑏1, 𝑏2 are biases.\nStep 6: Stacking Layers\n• Multiple layers of attention and feedforward networks are stacked, each with its own set of\nparameters. This forms the \"deep\" in deep learning.\n• Layer Output:\n𝐻(𝑙) = LayerNorm(𝑍(𝑙) + Attention(𝑄(𝑙), 𝐾(𝑙), 𝑉(𝑙)))\n(3)\n𝑍(𝑙+1) = LayerNorm(𝐻(𝑙) + FFN(𝐻(𝑙)))\n(4)\nStep 7: Output Generation\n• The final output of the stacked layers is a sequence of vectors.\n• These vectors are projected back into the token space using a softmax layer to predict the next\ntoken or word in the sequence.\n• Softmax Function:\n𝑃(𝑦𝑖|𝑋) =\nexp(𝑍𝑖)\n∑︀|𝑉|\n𝑗=1 exp(𝑍𝑗)\n(5)\n• where 𝑍𝑖is the logit corresponding to token 𝑖in the vocabulary.\n• The model generates the next token in the sequence based on the probability distribution, and\nthe process repeats until the end of the output sequence is reached.\nStep 8: Decoding\n• The predicted tokens are then decoded back into text, forming the final output.\n• Output Text: 𝑌= [𝑦1, 𝑦2, . . . , 𝑦𝑘]\nWe used the following prompt for Kannada language for the purpose of classification: \"Please identify\nwhich category the word is in English, Kannada, Mixed, Name, Location, Symbol and Other. Please state en,\nkn, mixed, name, location, sym and other. The word is <Word>.\" The figure representing the methodology\nis shown in Figure 2.\nWe used the following prompt for Tamil language for the purpose of classification: \"Please identify\nwhich category the word is in English, Tamil, Mixed, Name, Location, Symbol and Other. Please state en,\ntm, tmen, name, Location, sym and Other. The word is <Word>.\" The figure representing the methodology\nis shown in Figure 3.\nFigure 1: An overview of the GPT-3.5 Turbo architecture.\nFigure 2: An overview of GPT-3.5 Turbo for Kannada code-mixed language classification.\nFigure 3: An overview of GPT-3.5 Turbo for Tamil code-mixed language classification.\n6. Results\nTable 1 presents metrics comparing the performance of two language identification models, one for Tamil\nand the other for Kannada. Here’s a detailed discussion of each metric. For Tamil language, the macro\nF1 score is 0.3312. This suggests that the model achieves a balanced performance in terms of precision\nand recall for Tamil language identification. However, it indicates there is room for improvement in\nMetric\nTamil\nRank (Tamil)\nKannada\nRank (Kannada)\nMacro F1\n0.3312\n10\n0.4493\n10\nMacro Precision\n0.3259\n10\n0.5474\n10\nMacro Recall\n0.3657\n10\n0.4241\n10\nWeighted F1\n0.7022\n10\n0.6725\n10\nWeighted Precision\n0.7559\n10\n0.7191\n10\nWeighted Recall\n0.6689\n10\n0.6994\n10\nAccuracy\n0.6689\n10\n0.6994\n10\nTable 1\nComparison of various metrics for Word level identification in code-mixed languages in two languages-Tamil,\nKannada. The team name is TextTitans and the username is roydanik18. The rank corresponding to a score is\nalso provided.\ncorrectly identifying both positive and negative instances. For Kannada, the macro F1 score is 0.4493.\nThis score is higher compared to Tamil, indicating a better overall balance between precision and recall\nfor Kannada language identification. The model for Kannada performs better in correctly classifying\ninstances across the dataset.\nFor Tamil, the macro precision score is 0.3259. For Kannada, the macro precision score is 0.5474. This\nscore indicates a higher accuracy in positive predictions for Kannada compared to Tamil, suggesting\nbetter precision in correctly identifying Kannada instances.\nFor Tamil, the macro recall score is 0.3657. The macro recall score is 0.4241 for Kannada. This score\nindicates a slightly higher ability to identify Kannada instances correctly compared to Tamil.\nFor Tamil, the weighted F1 score is 0.7022. This metric considers the F1 score weighted by the number\nof samples in each class, indicating a solid overall performance for Tamil language identification. For\nKannada, the weighted F1 score is 0.6725. This indicates a slightly lower weighted F1 score compared\nto Tamil, suggesting a nuanced performance when considering class distribution.\nFor Tamil, the weighted precision score is 0.7559. This metric reflects the precision of the model\nwhen adjusted for the distribution of samples across Tamil language classes. For Kannada, the weighted\nprecision score is 0.7191. This score indicates a slightly lower weighted precision compared to Tamil,\nreflecting the model’s ability to accurately predict positive instances in Kannada.\nFor Tamil, the weighted recall score is 0.6689. This metric demonstrates the model’s ability to\nidentify all positive instances within the Tamil language classes when considering class distribution. For\nKannada, the weighted recall score is 0.6994. This score indicates a slightly higher ability to correctly\nidentify positive instances within Kannada language classes compared to Tamil.\nFor Tamil, the accuracy score is 0.6689. This metric measures the overall correctness of the model’s\npredictions for Tamil language identification. For Kannada, the accuracy score is 0.6994. This indicates\na slightly higher overall correctness in predictions for Kannada compared to Tamil.\nThe metrics highlight differences in performance between the Tamil and Kannada language identifi-\ncation models across various evaluation criteria. These metrics provide insights into the strengths and\nareas for improvement in both models, guiding further optimizations and enhancements for accurate\nlanguage identification tasks in practical applications.\n7. Conclusion\nIn this study, we investigated the effectiveness of language identification models for Tamil and Kannada\nusing the advanced capabilities of GPT-3.5 Turbo via prompting. Language identification is a crucial\npreliminary step in various natural language processing applications, including sentiment analysis,\nmachine translation, and information retrieval. Our research focused on evaluating and comparing the\nperformance of these models across multiple metrics: macro F1 score, macro precision, macro recall,\nweighted F1, weighted precision, weighted recall, and accuracy. The results reveal notable distinctions\nbetween the Tamil and Kannada models. Kannada consistently demonstrated superior performance\nacross most metrics. This indicates that the GPT for Kannada effectively identifies and categorizes\nKannada language instances with greater accuracy and reliability. Conversely, while the Tamil model\nexhibited moderate performance, there remains room for improvement, particularly in precision and\nrecall metrics.\nThe methodology employed in this research leveraged GPT-3.5 Turbo via prompting, harnessing\nits natural language processing capabilities to handle code-mixed texts and diverse linguistic patterns\nprevalent in real-world applications. This approach allowed for comprehensive evaluation under varying\nlinguistic contexts, ensuring robustness and applicability in multilingual environments.\nMoving forward, further refinements in model training and dataset augmentation could enhance\nthe performance of language identification systems for both Tamil and Kannada. Future research\nefforts may focus on incorporating additional linguistic features, optimizing model architectures, and\nexpanding datasets to include more diverse linguistic variations and challenges. In conclusion, this\nstudy underscores the importance of tailored approaches in language identification, particularly in\nmultilingual settings like India where linguistic diversity is prominent. By advancing the capabilities of\nlanguage identification models through innovative methodologies such as GPT-3.5 Turbo via prompting,\nwe contribute to the broader goal of improving language processing technologies for diverse and\nunder-resourced languages, fostering more accurate and inclusive natural language understanding\nsystems.\nReferences\n[1] T. Jauhiainen, H. Jauhiainen, K. Linden, Automatic language identification using word embeddings\nand normalized log probabilities, in: Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, 2018, pp. 1146–1153.\n[2] A. Deroy, S. Maity, S. Sarkar, Mirror: A novel approach for the automated evaluation of open-ended\nquestion generation, arXiv preprint arXiv:2410.12893 (2024).\n[3] A. Mandal, et al., Multilingual language identification based on recurrent neural networks, in:\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 2019,\npp. 3345–3352.\n[4] A. Deroy, K. Ghosh, S. Ghosh, How ready are pre-trained abstractive models and llms for legal\ncase judgement summarization?, arXiv preprint arXiv:2306.01248 (2023).\n[5] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models in\neducational and assessment applications (2024).\n[6] S. K. Nigam, A. Deroy, S. Maity, A. Bhattacharya, Rethinking legal judgement prediction in a\nrealistic scenario in the era of large language models, arXiv preprint arXiv:2410.10542 (2024).\n[7] T. B. Brown, Language models are few-shot learners, arXiv preprint ArXiv:2005.14165 (2020).\n[8] A. Deroy, K. Ghosh, S. Ghosh, Ensemble methods for improving extractive summarization of legal\ncase judgements, Artificial Intelligence and Law 32 (2024) 231–289.\n[9] S. Maity, A. Deroy, Natural language correction with an emphasis on bangla (2023).\n[10] A. Deroy, Exploiting Machine Learning Techniques for Unsupervised Clustering of Speech Utter-\nances, Ph.D. thesis, Indian Institute of Technology Kharagpur, 2019.\n[11] T. Jauhiainen, H. Jauhiainen, K. Linden, A survey on automatic language identification in written\ntexts, in: Journal of Artificial Intelligence Research, volume 65, 2019, pp. 675–782.\n[12] Y. Muthusamy, R. A. Cole, B. T. Oshika, Automatic language identification: A review/tutorial, in:\nIEEE Signal Processing Magazine, volume 11, 1994, pp. 33–41.\n[13] J. Tiedemann,\nNews from opus-a collection of multilingual parallel corpora with tools and\ninterfaces, in: Recent advances in natural language processing (vol. 5), 2009, pp. 237–248.\n[14] K. S. Rao, et al., A novel approach to unsupervised pattern discovery in speech using convolutional\nneural network, Computer Speech & Language 71 (2022) 101259.\n[15] A. Deroy, P. Bhattacharya, K. Ghosh, S. Ghosh, An analytical study of algorithmic and expert\nsummaries of legal cases, in: Legal Knowledge and Information Systems, IOS Press, 2021, pp.\n90–99.\n[16] S. Maity, A. Deroy, The future of learning in the age of generative ai: Automated question\ngeneration and assessment with large language models, arXiv preprint arXiv:2410.09576 (2024).\n[17] M. Zampieri, B. Gebre, S. Malmasi, A system for tweet normalization and part-of-speech tagging\nof non-standard italian, Proceedings of the first workshop on noisy user-generated text (2014)\n61–70.\n[18] S. Malmasi, M. Dras, Discriminating between similar languages and dialects using crfs and svms,\nin: Proceedings of the International Conference Recent Advances in Natural Language Processing,\n2015, pp. 140–147.\n[19] B. King, S. Abney, Labeling the languages of words in mixed-language documents using weakly\nsupervised methods, in: Proceedings of the 2013 conference of the North American chapter of the\nassociation for computational linguistics: human language technologies, 1996, pp. 1110–1119.\n[20] V. Singh, J. Lal, A. Sharma, et al., Automatic language identification system using machine\nlearning techniques: A review, Journal of Ambient Intelligence and Humanized Computing 9\n(2018) 417–425.\n[21] S. Zwarts, P. McNamee, Proceedings of the vardial workshop series on variation of languages in\ndialects and varieties, in: Proceedings of the Fourth Workshop on NLP for Similar Languages,\nVarieties and Dialects (VarDial), 2017, pp. 9–18.\n[22] S. Maity, A. Deroy, S. Sarkar, A novel multi-stage prompting approach for language agnostic\nmcq generation using gpt, in: European Conference on Information Retrieval, Springer, 2024, pp.\n268–277.\n[23] S. Maity, A. Deroy, Generative ai and its impact on personalized intelligent tutoring systems,\narXiv preprint arXiv:2410.10650 (2024).\n[24] J. Tiedemann, Automatic identification of cognates and false friends in bilingual wordlists, in:\nProceedings of the tenth conference on European chapter of the Association for Computational\nLinguistics-Volume 1, 2003, pp. 116–119.\n[25] T. Jauhiainen, H. Jauhiainen, K. Linden, Automatic detection of compound words in multiple\nlanguages, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, 2017, pp. 2047–2052.\n[26] A. Mandal, A. Das, P. Pakray, Automatic language identification based on lexical and syntactic\nfeatures, in: Proceedings of the 6th International Conference on Computer Applications in\nBiotechnology, 2015, pp. 213–221.\n[27] R. Gamba, A. Das, Comparing the level of code-switching in corpora, in: Proceedings of the 10th\nedition of the Language Resources and Evaluation Conference, 2016, pp. 14–20.\n[28] B. King, S. Abney, Labeling the languages of words in mixed-language documents using weakly\nsupervised methods, in: Proceedings of the 2013 conference of the North American chapter of the\nassociation for computational linguistics: human language technologies, 2014, pp. 1110–1119.\n[29] P. Molaei, et al., Cross-language identification of dravidian languages using transformer models,\nProceedings of the Workshop on Computational Approaches to Linguistic Code-Switching (2020)\n45–52.\n[30] S. Maity, A. Deroy, S. Sarkar, Harnessing the power of prompt-based techniques for generating\nschool-level questions using large language models, in: Proceedings of the 15th Annual Meeting\nof the Forum for Information Retrieval Evaluation, 2023, pp. 30–39.\n[31] M. Zampieri, S. Malmasi, Y. Scherrer, Predicting the language of informal code-switched text, in:\nProceedings of the 6th Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial\n2019), 2019, pp. 135–144.\n[32] I. Bardaji, et al., Language identification using cross-lingual word embeddings, Natural Language\nEngineering 18 (2012) 515–531.\n[33] S. Maity, A. Deroy, S. Sarkar, How ready are generative pre-trained large language models for\nexplaining bengali grammatical errors?, in: B. PaaÃŸen, C. D. Epp (Eds.), Proceedings of the 17th\nInternational Conference on Educational Data Mining, International Educational Data Mining\nSociety, Atlanta, Georgia, USA, 2024, pp. 664–671. doi:10.5281/zenodo.12729912.\n[34] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models in\neducational and assessment applications, in: B. PaaÃŸen, C. D. Epp (Eds.), Proceedings of the 17th\nInternational Conference on Educational Data Mining, International Educational Data Mining\nSociety, Atlanta, Georgia, USA, 2024, pp. 961–968. doi:10.5281/zenodo.12730013.\n[35] A. Deroy, S. Maity, Questioning biases in case judgment summaries: Legal datasets or large\nlanguage models?, arXiv preprint arXiv:2312.00554 (2023).\n[36] S. K. Nigam, A. Deroy, N. Shallum, A. K. Mishra, A. Roy, S. K. Mishra, A. Bhattacharya, S. Ghosh,\nK. Ghosh, Nonet at semeval-2023 task 6: Methodologies for legal evaluation, arXiv preprint\narXiv:2310.11049 (2023).\n[37] A. Deroy, S. Maity, Code generation and algorithmic problem solving using llama 3.1 405b, arXiv\npreprint arXiv:2409.19027 (2024).\n[38] B. R. Chakravarthi, R. Priyadharshini, J. Jose, P. Kumaresan, S. Muralidaran, Findings of the shared\ntask on sentiment analysis for dravidian languages in code-mixed text, in: Proceedings of the first\nworkshop on speech and language technologies for Dravidian languages, 2021, pp. 133–139.\n[39] A. Deroy, K. Ghosh, S. Ghosh, Applicability of large language models and generative models for\nlegal case judgement summarization, Artificial Intelligence and Law (2024) 1–44.\n[40] S. K. Nigam, A. Deroy, Fact-based court judgment prediction, in: Proceedings of the 15th Annual\nMeeting of the Forum for Information Retrieval Evaluation, 2023, pp. 78–82.\n[41] S. Ghosh, K. Ghosh, D. Ganguly, A. Bhattacharya, P. P. Chakrabarti, S. Guha, A. Pal, K. Rudra,\nP. Majumder, D. Roy, et al., Report on the 2nd symposium on artificial intelligence and law (sail)\n2022, in: ACM SIGIR Forum, volume 56, ACM New York, NY, USA, 2023, pp. 1–7.\n[42] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised\nmultitask learners, in: OpenAI Blog, volume 1, 2019.\n[43] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Exploring\nthe limits of transfer learning with a unified text-to-text transformer, Journal of Machine Learning\nResearch 21 (2020) 1–67.\n[44] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers\nfor language understanding, in: Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 2019, pp. 4171–4186.\n[45] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov,\nRoberta: A robustly optimized bert pretraining approach, in: arXiv preprint arXiv:1907.11692,\n2019.\n[46] A. Deroy, S. Maity, S. Ghosh, Prompted zero-shot multi-label classification of factual incorrectness\nin machine-generated summaries., in: FIRE (Working Notes), 2023, pp. 734–746.\n[47] S. Maity, A. Deroy, S. Sarkar, How effective is gpt-4 turbo in generating school-level questions\nfrom textbooks based on bloom’s revised taxonomy?, 2024. URL: https://arxiv.org/abs/2406.15211.\narXiv:2406.15211.\n[48] A. Deroy, S. Maity, Multi-label classification of covid-tweets using large language models, arXiv\npreprint arXiv:2312.10748 (2023).\n[49] A. Deroy, S. Maity, Question generation: Past, present & future, Authorea Preprints (2024).\n[50] W. X. Zhao, K. Zhou, J. Li, X. Tang, J. J. Wang, J. Liu, T. Wang, Y. Bao, J.-R. Wen, A survey of large\nlanguage models, in: arXiv preprint arXiv:2303.18223, 2023.\n[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\nAttention is all you need, Advances in neural information processing systems 30 (2017) 5998–6008.\n[52] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Fine-tuning gpt-2 for human-like\ntext generation, in: arXiv preprint arXiv:1907.11692, 2019.\n[53] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, Y. Choi, Defending against\nneural fake news, in: Advances in Neural Information Processing Systems, volume 32, 2019, pp.\n9054–9065.\n[54] A. Deroy, N. K. Bailung, K. Ghosh, S. Ghosh, A. Chakraborty, Artificial intelligence (ai) in legal\ndata mining, arXiv preprint arXiv:2405.14707 (2024).\n[55] A. Deroy, S. Maity, Ai-powered answer assessment: A comprehensive overview, Authorea\nPreprints (2024).\n[56] A. Deroy, S. Maity, A short case study on understanding the capabilities of gpt for temporal\nreasoning tasks, Authorea Preprints (2024).\n[57] A. Deroy, S. Maity, Exploring the mathematical reasoning capabilities of gemini, Authorea\nPreprints (2024).\n[58] A. Hegde, M. D. Anusha, S. Coelho, H. L. Shashirekha, B. R. Chakravarthi, Corpus creation for\nsentiment analysis in code-mixed tulu text, in: Proceedings of the 1st Annual Meeting of the\nELRA/ISCA Special Interest Group on Under-Resourced Languages, 2022, pp. 33–40.\n[59] S. H. Lakshmaiah, F. Balouchzahi, M. D. Anusha, G. Sidorov, Coli-machine learning approaches for\ncode-mixed language identification at the word level in kannada-english texts, Acta Polytechnica\nHungarica 19 (2022).\n[60] A. Hegde, F. Balouchzahi, S. Coelho, S. HL, H. A. Nayel, S. Butt, Coli@ fire2023: Findings of\nword-level language identification in code-mixed tulu text, in: Proceedings of the 15th Annual\nMeeting of the Forum for Information Retrieval Evaluation, 2023, pp. 25–26.\n[61] A. Hegde, F. Balouchzahi, S. Coelho, H. Shashirekha, H. A. Nayel, S. Butt, Overview of coli-tunglish:\nWord-level language identification in code-mixed tulu text at fire 2023., in: FIRE (Working Notes),\n2023, pp. 179–190.\n[62] F. Balouchzahi, S. Butt, A. Hegde, N. Ashraf, H. Shashirekha, G. Sidorov, A. Gelbukh, Overview of\ncoli-kanglish: Word level language identification in code-mixed kannada-english texts at icon 2022,\nin: Proceedings of the 19th International Conference on Natural Language Processing (ICON):\nShared Task on Word Level Language Identification in Code-mixed Kannada-English Texts, 2022,\npp. 38–45.\n[63] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neubig, Pre-train, prompt, and predict: A systematic\nsurvey of prompting methods in natural language processing, ACM Computing Surveys 55 (2023)\n1–35.\n[64] C. Lee, Multilingualism online, Routledge, 2016.\n[65] K. Chowdhary, K. Chowdhary, Natural language processing, Fundamentals of artificial intelligence\n(2020) 603–649.\n[66] S. Maity, A. Deroy, Human-centric explainable ai in education, 2024. URL: https://arxiv.org/abs/\n2410.19822. arXiv:2410.19822.\n[67] G. Takawane, A. Phaltankar, V. Patwardhan, A. Patil, R. Joshi, M. S. Takalikar, Language augmen-\ntation approach for code-mixed text classification, Natural Language Processing Journal 5 (2023)\n100042.\n[68] A. Mangla, R. K. Bansal, S. Bansal, Code-mixing and code-switching on social media text: A brief\nsurvey, in: 2023 IEEE International Conference on Computer Vision and Machine Intelligence\n(CVMI), IEEE, 2023, pp. 1–5.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-11-06",
  "updated": "2024-11-06"
}