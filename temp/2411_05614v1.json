{
  "id": "http://arxiv.org/abs/2411.05614v1",
  "title": "Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey",
  "authors": [
    "Zhihong Liu",
    "Xin Xu",
    "Peng Qiao",
    "Dongsheng Li"
  ],
  "abstract": "Deep reinforcement learning has led to dramatic breakthroughs in the field of\nartificial intelligence for the past few years. As the amount of rollout\nexperience data and the size of neural networks for deep reinforcement learning\nhave grown continuously, handling the training process and reducing the time\nconsumption using parallel and distributed computing is becoming an urgent and\nessential desire. In this paper, we perform a broad and thorough investigation\non training acceleration methodologies for deep reinforcement learning based on\nparallel and distributed computing, providing a comprehensive survey in this\nfield with state-of-the-art methods and pointers to core references. In\nparticular, a taxonomy of literature is provided, along with a discussion of\nemerging topics and open issues. This incorporates learning system\narchitectures, simulation parallelism, computing parallelism, distributed\nsynchronization mechanisms, and deep evolutionary reinforcement learning.\nFurther, we compare 16 current open-source libraries and platforms with\ncriteria of facilitating rapid development. Finally, we extrapolate future\ndirections that deserve further research.",
  "text": "Acceleration for Deep Reinforcement Learning using Parallel and\nDistributed Computing: A Survey\nZHIHONG LIU, XIN XU, PENG QIAO, and DONGSHENG LI, National University of Defense\nTechnology, China\nDeep reinforcement learning has led to dramatic breakthroughs in the field of artificial intelligence for the past few years.\nAs the amount of rollout experience data and the size of neural networks for deep reinforcement learning have grown\ncontinuously, handling the training process and reducing the time consumption using parallel and distributed computing is\nbecoming an urgent and essential desire. In this paper, we perform a broad and thorough investigation on training acceleration\nmethodologies for deep reinforcement learning based on parallel and distributed computing, providing a comprehensive\nsurvey in this field with state-of-the-art methods and pointers to core references. In particular, a taxonomy of literature\nis provided, along with a discussion of emerging topics and open issues. This incorporates learning system architectures,\nsimulation parallelism, computing parallelism, distributed synchronization mechanisms, and deep evolutionary reinforcement\nlearning. Further, we compare 16 current open-source libraries and platforms with criteria of facilitating rapid development.\nFinally, we extrapolate future directions that deserve further research.\nCCS Concepts: • Computing methodologies →Concurrent algorithms; Reinforcement learning; Massively parallel\nalgorithms; Distributed artificial intelligence.\nAdditional Key Words and Phrases: deep reinforcement learning, acceleration, parallel and distributed computing, large-scale\nACM Reference Format:\nZhihong Liu, Xin Xu, Peng Qiao, and Dongsheng Li. 2023. Acceleration for Deep Reinforcement Learning using Parallel and\nDistributed Computing: A Survey. 1, 1 (November 2023), 34 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nSince the advent of Deep Q Network (DQN)[1] in 2015 which achieved human-level control on Atari video games,\ndeep reinforcement learning (DRL), a powerful machine learning paradigm, has been widely investigated by\nArtificial Intelligence (AI) researchers. DRL deals with training an agent to learn the optimal policy based on feed-\nback from interaction with the environment. Through leveraging the power of deep learning and reinforcement\nlearning, there are many merits in DRL. First, DRL is able to handle high-dimensional and large state spaces.\nSecond, DRL has the general self-learning ability without the requirements of labeled datasets. Third, problems\nthat need to be solved by online computation traditionally can be solved in offline training manner with DRL.\nWith these merits, DRL in recent years has led to dramatic breakthroughs in game playing[2–4], robotics[5–8],\nhealthcare[9–11], etc.\nHowever, with the increased complexity of the applications for DRL, more interaction iterations and larger\nscale of neural network models are required. As a result, the training process becomes very computation-intensive\nand thus time-consuming. For instance, it needs around 38 days of experience to train DQN on Atari 2600\nAuthors’ address: Zhihong Liu, zhliu@nudt.edu.cn; Xin Xu, xinxu@nudt.edu.cn; Peng Qiao, pengqiao@nudt.edu.cn; Dongsheng Li, dsli@\nnudt.edu.cn, National University of Defense Technology, 109 Deya Rd, Changsha, Hunan, China, 410073.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first\npage. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from\npermissions@acm.org.\n© 2023 Association for Computing Machinery.\nXXXX-XXXX/2023/11-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\n, Vol. 1, No. 1, Article . Publication date: November 2023.\narXiv:2411.05614v1  [cs.LG]  8 Nov 2024\n2\n•\nLiu et al.\nFig. 1. The structure of the survey.\ngame [1] with 50 million frames. Besides, tuning of hyper-parameters is tricky and further increases the time\nconsumption [12].\nThus, it is intuitive to leverage parallel and distributed computing in DRL to accelerate its training process.\nDuring the past few years, rapid progress has been witnessed in this field, resulting in a profusion of studies that\ntarget diversified aspects and use a variety of techniques. This causes significant difficulties in understanding the\ndevelopment of this field in a systematic manner.\nIn this survey, we collect, classify, and compare a huge body of work on DRL acceleration using parallel\nand distributed computing, providing a comprehensive survey in this field with state-of-the-art methods and\npointers to core references. In particular, we analyze the primary challenges to make DRL training distributed\nand demystify the details of the technologies that have been proposed by researchers to address these challenges.\nThis includes system architectures for distributed DRL, simulation parallelism, computing parallelism, distributed\nsynchronization mechanisms (backpropagation-based training), and deep evolutionary reinforcement learning\n(evolution-based training). Furthermore, we provide an overview and comparison of current open-source libraries\nand platforms that put parallel and distributed DRL into practice. Based on that, we extrapolate potential directions\nfor future work.\n1.1\nRelated surveys\nThere are a number of surveys in this field that are related to ours. Arulkumaran et al. [13] provide a brief survey\non DRL algorithms, applications, and challenges. Li [14] gives a wide coverage of core elements, important\nmechanisms, and applications in DRL. Wang et al. [15] present an overview of the theories, algorithms, and\nresearch topics of DRL. Moerland et al. [16] provide the taxonomy, key challenges, and benefits of model-based\nreinforcement learning. Meanwhile, many surveys targeting specific application domains have emerged in\nrecent years, e.g., autonomous driving[17], cyber security[18], networking[19], intelligence transportation[20],\nmulti-agent systems [21] and Internet of Things [22]. However, these surveys either focus on the fundamental\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n3\ntheories and general technologies of DRL, or investigate DRL methods in specific domains. Few of them studies\non learning acceleration by parallel and distributed computing.\nAdmittedly, there are several surveys have been proposed in distributed machine learning [23–25]. It is noted\nthat reinforcement learning is one of the sub-branches in machine learning. These surveys, however, summarize\ndistributed technologies from a holistic perspective of machine learning, without in-depth discussion regarding\ndistributed reinforcement learning. Besides, many works have emerged on distributed deep learning training\nacceleration. Mayer et al. [26] present a comprehensive investigation on challenges, techniques, and frameworks\nfor DL training on distributed infrastructures. Ben-Nun et al. [27] provide a concurrency analysis for parallel and\ndistributed DL. Tang et al. provide a comprehensive survey of distributed deep learning from the communication\nperspective. Chahal et al.[28] propose a hitchhiker’s guide on distributed training for DL. However, these surveys\ntarget at DL and have not analyzed any methods in DRL. Note that DL and DRL are different types of machine\nlearning paradigms, and the key difference exactly lays on the training scheme. With respect to DL, the methods\nlearn from supervised training datasets and seek to minimize the error between the neural network model and\nthe labeled data. In comparison, DRL methods require interaction with environments to generate experiences.\nBased on that, DRL methods learn what actions lead to the maximum outcome. This will unavoidably cause\ndifferences in training acceleration techniques between DL and DRL.\nIn particular, Samsami et al. [29] provide a brief overview of distributed DRL by introducing several key\nresearch works in this field. No taxonomy of existing methods is presented in their work. Besides, Yin et al.\n[30] propose a survey on distributed DRL for the specific field of multi-player multi-agent learning, where\nagents are either cooperating or competing within the environments. In contrast, our survey takes a much more\ncomprehensive and in-depth view on training acceleration by parallel and distributed computing. Along with the\noutlined taxonomy and literature overview, the details of key methodologies in DRL training are demystified.\nTo the best of our knowledge, this is the first comprehensive survey on acceleration for DRL using parallel and\ndistributed computing.\n1.2\nStructure of the survey\nThe structure of the survey is summarized in Fig. 1 and organized as follows.\n• Section 2 provides the foundations and challenges related to our study.\n• Section 3 classifies system architectures in general for parallel and distributed DRL.\n• Section 4 elaborates on different simulation parallelism strategies to expose simulation concurrency.\n• Section 5 analyzes computing parallelism patterns used in existing works.\n• Section 6 discusses distributed synchronization mechanisms for backpropagation-based distributed training\nmethods.\n• Section 7 explores technologies of deep evolutionary reinforcement learning for evolution-based distributed\ntraining methods.\n• Section 8 compares current open-source libraries and platforms that put parallel and distributed DRL into\npractice.\n• Section 9 gives concluding remarks and highlights future directions that deserve further research.\n2\nBACKGROUND\nDeep reinforcement learning (DRL) integrates deep learning (DL) and reinforcement learning (RL), where DL is\nleveraged to perform function approximation in RL. Thus, the basic knowledge of DRL and current distributed DL\ntraining development lay a foundation for parallel and distributed DRL. This section will discuss the foundations\nrelated to this work, and analyze its challenges to motivate this study.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n4\n•\nLiu et al.\nFig. 2. The semantics of deep reinforcement learning.\n2.1\nPreliminaries of deep reinforcement learning\nFormally, DRL can be modeled as a Markov Decision Process (MDP). More specifically, at each time step 𝑡, the\nDRL agent receives a state 𝑠𝑡∈S and makes its action decision 𝑎𝑡∈A according to a distribution called policy\n𝜋(𝜃|𝑠𝑡), where S and A are the state space and action space, respectively. The policy is basically modeled by a\nneural network with parameters 𝜃. After the action execution, the agent receives a reward 𝑟𝑡+1 and transitions\nto the next state 𝑠𝑡+1 according to the reward function 𝑅(𝑠𝑡,𝑎𝑡,𝑠𝑡+1) and transition distribution 𝑃(𝑠𝑡+1|𝑠𝑡,𝑎𝑡),\nrespectively. The return starting from time 𝑡to the end of the interaction is defined by 𝐺𝑡= Í∞\n𝑖=0 𝛾𝑖𝑟𝑡+𝑖, where\n𝛾∈[0, 1] is the discount factor. The semantics of DRL are illustrated in Fig. 2. DRL agents interact with the\nenvironment to collect experience data, which is usually denoted by (𝑠𝑡,𝑎𝑡,𝑠𝑡+1,𝑟𝑡+1). Based on experience data,\nthe agent is trained to learn an optimal policy for maximizing the return by updating the parameters 𝜃of the\nneural network.\nDRL can be classified to model-based and model-free. In model-based variants, the transition and reward\nmodels (also named MDP dynamics) are known or learned first. Then the policy optimization is based on the\nmodels. There are mainly two categories in model-based DRL [16]. First is model-based DRL with a learned model.\nThis category of methods learns both the model and the global policy. Examples are Dyna-style algorithms\nsuch as MB-MPO[31] and ME-TRPO[32]. These algorithms learn the model through data from interaction with\nenvironments. Then model-free algorithms are utilized to learn the global policy based on the data generated\nby the learned model. Second is model-based DRL with a known model. This category of methods plans over a\nknown model, and only performs learning for the global policy. Examples are dynamic programming[33] and\nMonte-Carlo tree search based algorithms (AlphaZero[3]).\nIn contrast, model-free DRL follows trail-and-error patterns and learns the policy directly. Basically, there are\nmainly three categories in model-free DRL. First is value-based methods. This category learns a value function that\nrepresents the expected return for being in a state or taking an action in the state, e.g., 𝑉𝜋(𝑠) and 𝑄𝜋(𝑠,𝑎). Then\nan optimal policy can be obtained based on the value function. Value-based methods are suited for discrete action\nspaces and deterministic policies. Second is Policy-based methods. This category learns the policy directly and\nextracts actions according to the policy. Since the policy is actually a distribution of possible actions, continuous\naction spaces, and stochastic policies can also be learned. Compared to value-based methods, policy-based\nmethods have better convergence properties. However, high variance and sample inefficient are common issues\nin policy-based methods. Third is Actor-critic based methods, which integrates the merits of value-based and\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n5\npolicy-based methods. Two separate neural networks are used in the architecture, named actor and critic. The\nactor learns the optimal policy and chooses the action by using the policy-based methods. The critic evaluates\nthe selected action through the value-based methods. Due to the superiority in performance, actor-critic based\nmethods have become more and more popular in recent years, e.g., PPO[34], SAC[35] and TD3[36].\nAlongside supervised learning and un-supervised learning, reinforcement learning performs optimization\nbased on the experiences collected from interacting with uncertain environments. Finding a balance between\nexploration (to discover new features about the world) and exploitation (to utilize what it knows) is fundamental\nto the efficiency of reinforcement learning methods. Meanwhile, according to the uniformity of the behavior\npolicy and the target policy, DRL algorithms can be divided into on-policy and off-policy. Here, the behavior\npolicy represents the policy used to generate the training data by interaction, while the target policy refers to the\npolicy that needs to be learned. More specifically, in off-policy learning, the behavior policy and the target policy\nare different. The goal of the off-policy learning is to learn the target policy by using the training data generated\nby the behavior policy. This has many benefits such as learning from demonstration, continuous exploration,\nand better sample utilization. However, it suffers from convergence and stability issues [37, 38]. In comparison,\nthe behavior policy and the target policy are the same in on-policy learning. As a result, better stability can be\nachieved in learning but sacrificing sample efficiency[35].\n2.2\nDistributed training for deep learning\nIn recent years, distributed training acceleration for deep learning has become a hot research topic and various\nmethods have flourished in this field. Due to the tight connection between DL and DRL, knowledge and progress\nof distributed DL methods play a foundational role in parallel and distributed DRL. Basically, there are mainly\nthree parallelism schemes of training acceleration for deep learning: data parallelism, model parallelism, and\npipeline parallelism.\nData parallelism [39–43] is the first proposed and the most common parallelism schemes. It partitions the\ntraining dataset (samples) into multiple subsets and dispatches them to multiple workers (e.g., compute nodes\nand devices). Each worker maintains a replica of the entire neural network model along with its local parameters.\nDuring training, each worker processes its subset of data independently and synchronizes parameters or gradients\nof the model across different workers either in a synchronous or asynchronous manner. This is the main parallelism\nscheme utilized in distributed DRL. Details can be found in Section 6.\nModel parallelism [44–47] slices the neural network model into disjoint partitions and assigns them to multiple\nworkers for computation. Compared to data parallelism, the computations between workers are no longer\nindependent. Neurons with connections across workers require data transfers before continuing the computation\nto the next layer. Due to the high computation dependency in neural networks, model parallelism is non-trivial\nfor accelerating the training [48]. The main advantage of model parallelism is making training a large model,\nwhich cannot fit into the memory of one node, become possible.\nPipeline parallelism [49–52] divides the neural network model into stages (consisting of consecutive layers)\nand assigns the stages to different workers. Each worker performs the computation of its stage for microbatches\nof samples one by one in a pipeline fashion. This scheme can significantly improve parallel training throughput\nover the vanilla model parallelism by fully saturating the pipeline. Basically, pipeline parallelism is a special form\nof model parallelism.\nCombining these parallelism schemes therefore giving full play to their respective advantages is a promising\ndirection[53–59]. Krizhevsky et al. [53] propose a hybrid scheme of using data parallelism for convolutional\nlayers and model parallelism for densely-connected layer. Wu et al.[54] propose to make use of data and model\nparallelism to accelerate the training of recurrent neural networks. Rasley et al. propose ZeRo [57], a parallelized\noptimizer based on model and data parallelism, which can train large models with over 100 billion parameters.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n6\n•\nLiu et al.\nPark et al. propose HeiPipe[58] that combines pipeline parallelism with data parallelism for training large deep\nnetworks.\n2.3\nChallenges of parallel and distributed training for DRL\nAlthough above distributed training methods can serve as good references for training acceleration in DRL,\ncopying these methods to DRL directly is infeasible. That is because there are differences in training schemes\nbetween DL and DRL. DL trains from supervised training datasets and seeks to minimize the error between the\nneural network model and the labeled data. In comparison, DRL does not have labeled data. It learns an optimal\npolicy that leads to the maximum outcome through interacting with the environment. This will unavoidably\ncause differences in training acceleration between DL and DRL and trigger requirements of new techniques and\nmethodologies. Overall, the challenges of parallel and distributed training for DRL can be summarized as follows.\n• Training DRL agents in parallel is a complex and dynamic problem that many components, such as actors,\nlearners, and parameter servers, need to operate cooperatively. How to organize the architecture of these\ncomponents and allocate the training tasks among them so as to maximize the system throughput is a\nchallenging problem[60, 61].\n• Requiring to interact with environments (mostly simulated) to generate training samples is a distinguished\nfeather of DRL. Training DRL agents for complex applications often need millions or even billions of\nexperience samples. How to parallel simulations so as to increase the sample efficiency is demanding\n[62, 63].\n• The workloads in distributed DRL training are heterogeneous, which may cause data movement across\ndevices even nodes frequently. This will degrade the overall throughput and computation efficiency. How to\nsaturate the computation resources by using different hardware architectures and corresponding computing\nparallelism techniques accordingly is challenging[64].\n• Since the parallel learning machines may process at different speeds in a heterogeneous environment,\nobsolete gradients exist and can have a large impact on stability and convergence in training. How to\naggregate the gradients and synchronize the model maintained in parallel workers is also non-trivial[65].\n• The essence of dominant training technology in distributed DRL is stochastic gradient descent based on\nbackpropagation. However, this technology still suffers from local optima and expensive computational\ncost [66]. How to exploit other optimization or search techniques in DRL to enable more sufficient training\nis desired.\nIn addressing these challenges, a profusion of studies have been proposed in the field of DRL acceleration using\nparallel and distributed. There is a necessity to collect, classify, and compare these studies in a structured manner,\nthereby facilitating researchers understanding the development of this field. In the following sections, we survey\nthe research for parallel and distributed DRL training and demystify the details of the key technologies. This\nincludes system architectures for distributed DRL, simulation parallelism, computing parallelism, distributed\nsynchronization mechanisms (backpropagation-based training), and deep evolutionary reinforcement learning\n(evolution-based training). In addition, we provide an overview and comparison of current open-source libraries\nand platforms that put parallel and distributed DRL into practice.\n3\nSYSTEM ARCHITECTURES FOR DISTRIBUTED DRL\nTraining DRL agents in parallel is a system engineering problem that many components need to operate\ncooperatively. Taking a panoramic view of the parallel and distributed DRL acceleration frameworks that\nbloomed in recent years, four components can be abstracted in the general case:\n• Actor: is in charge of interacting with the environment, including performing the actions obtained according\nto the policy, getting information such as observations and rewards, and producing experience data.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n7\n(a) Centralized architecture\n(b) Decentralized architecture\nFig. 3. Comparison of centralized and decentralized architectures for parallel and distributed training in DRL.\n• Learner: collects or samples the experience data, and computes the gradients for updating the model. Since\nthe gradient computation is usually based on a batch of experiences, high speed hardware like GPU is\ncommonly used in Learners.\n• Parameter Server: Maintains the up-to-date parameters of neural network models for Actors or Learners.\nNote that in some approaches (e.g., APE-X[67] and R2D2[68]), the job of the parameter server is concurrently\ntaken by the Learner.\n• Replay memory: Stores the experience data produced by Actors. This exists in cases of off-policy reinforce-\nment learning, e.g., Gorila[69], Ape-X[67], R2D2[68], etc.\nAccording to the organization of these components, we classify the parallel and distributed training architectures\nfor DRL into two classes: centralized and decentralized architectures.\n3.1\nCentralized architecture\nIn the centralized architecture, there is a center that maintains the global model of the neural network for learning.\nLearners optimize the global model by computing gradients based on the experiences from Actors. Both Learners\nand Actors synchronize their local model from the global model in some period or steps. Compared to Actors\nwho usually have a large scale, the number of Learners can be one or many. We abstract the general form of the\ncentralized architecture as shown in Fig. 3(a). The global model is either maintained by the Parameter Server or a\nLearner. More specifically, if there are multiple Learners in the architecture, the Parameter Sever serves as the\ncenter for updating the global model and distributing it to Actors and Learners. Therefore, the star communication\ntopology is constructed in the centralized architecture. In any event, since a large number of workers as well as\niterations could be involved in distributed DRL training, the bottleneck of the center node will have a strong\nimpact on the training scalability.\nSilver et al. [69] propose a massively distributed method for the DQN named Gorila (General Reinforcement\nLearning Architecture). To the best of our knowledge, this is one of the pioneering works in the area of distributed\ntraining for DRL. Gorila is based on the centralized architecture and consists of four components: Actors, Learners,\nthe Parameter Server, and the Replay Memory, as shown in Fig. 4(a). The center of Gorila is the Parameter Server,\nwhich is in charge of updating and distributing the network parameters. There can be many Actors and Learners\nin Gorila. Each Actor has a replica of the Q network and is responsible for generating experiences to the Replay\nMemory through interacting with the environment. Each learner also has a replica of the Q network and is in\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n8\n•\nLiu et al.\n(a) Gorila[69]\n(b) APE-X[67]\n(c) A3C[70]\n(d) IMPALA[61]\n(e) rlpyt[71]\n(f) DD-PPO[72]\nFig. 4. Architectures of current parallel and distributed DRL methods.\ncharge of producing the gradients to the parameter server based on the experiences from the Replay Memory.\nWith the help of parallel Actors and Learners, Gorila can significantly improve the training performance for\nDQN, reaching a reduction of ten times in training duration on most games of Atari.\nSome variants over Gorila have also been proposed to further improve the performance. Horgan et al. [67]\npropose a distributed method in centralized architecture for DQN with prioritized experience replay named APE-\nX. Rather than sampling uniformly in Gorila, APE-X focuses on learning the prioritized experiences with larger\nabsolute temporal difference (TD) errors, as shown in Fig. 4(b). Further, a distributed method named Recurrent\nReplay Distributed DQN (R2D2)[68] is proposed based on APE-X, which imports the recurrent neural network\n(LSTM) in training to achieve better performance in the partially observed environment. These two methods\nare based on a similar architecture to Gorila. However, only one Learner is used in these two methods, which\nis running on a GPU. By doing this, the Learner can exploit batch computing advantages of GPU for gradient\ncomputation compared to Gorila. Besides, they leverage the single Learner to maintain the latest parameters\ninstead of the Parameter Server.\nUnlike Gorila which is for off-policy learning, Minh et al. [70] propose a new type of distributed training\nmethod named A3C for on-policy learning. The architecture of A3C is shown in Fig. 4(c). We can see that A3C is\nbased on a centralized architecture. Different from the aforementioned methods, A3C does not utilize the Replay\nMemory. Instead, A3C leverages the parallelization of Actors to enrich the diversity of environmental interaction\nexperiences and accumulates updates over multiple steps to improve the stability of learning. Besides, the Actor\nand Learner are encapsulated together in an actor-learner thread. Further, although there is no Parameter Server,\na global network is also maintained on a worker, which is considered as the center of the architecture. Each\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n9\nactor-learner thread sends accumulated gradients to and obtains the latest network parameters from the global\nnetwork.\n3.2\nDecentralized architecture\nIn the decentralized architecture, there is no central node that maintains the global model. Basically, more than\none Leaner are deployed. Each Learner not only computes the gradients based on the experiences from the\ncorresponding Actors, but also obtains the gradients from other learners. Then, each Learner updates its model\nby aggregating all the gradients. Here, All-reduce [73, 74] distributed communication mechanism is utilized to\naggregate the gradients, which generally requires a fully connected communication topology for parallel workers.\nOther than interacting with environments and producing the experiences, Actors update the parameters from\nLearners. We abstract the general form of the decentralized architecture as shown in Fig. 3(b). Compared to\nthe centralized architecture, by using multiple Learners to aggregate the gradients and maintain the model, the\ndecentralized architecture removes the communication congestion on the Parameter Server [74].\nA decentralized architecture of multiple learners with importance weighted is proposed in IMPALA [61].\nIn IMPALA, actors utilize CPU cores to interact with the environment, while learners are deployed on GPUs\nto give play to the gradient computation. To maintain the latest model in the whole system, gradients are\naggregated across multiple learners synchronously and actors obtain the latest parameters from learners. Stooke\net al. [71, 75] propose an acceleration method of utilizing multiple GPUs for DRL named rlpyt, which is based\non the decentralized architecture. There are multiple Learners in the architecture. Each of them is running\non a GPU and is in charge of data sampling and model inference. Besides, All-reduce is performed in each\niteration to aggregate the gradients so as to keep the model in every Learner identical. This method has many\nvariants that are applied with different DRL algorithms including A3C, PPO, DQN, etc. A similar architecture\nis adopted in DD-PPO[72], which is also decentralized and leverages multiple GPUs. Other than the method\nin [75] that synchronizes the models of different GPUs in one machine, DD-PPO extends the architecture across\nmany machines and has better scalability. Similar to A3C[70], DD-PPO encapsulates the work of the Actor and\nLearner together in a worker. In this way, each worker alternates the workloads of experience collection, gradient\naggregation, and optimization to achieve better resource utilization.\n3.3\nDiscussion\nThese two classes of architectures are commonly used in recent studies. In the centralized architecture, it is\nknown that the central node may encounter communication congestion that limits the scalability of distributed\ntraining [23]. However, its merits on synchronization efficiency and realization simplicity are non-negligible.\nThis is because the global updated model is maintained in a central node, from which all workers can easily\nobtain and keep consistent. In the decentralized architecture, the scalability issue is relieved by eliminating the\nsingle-point issue. Nevertheless, the models optimize separately and need to be synchronized through assembling,\nwhich may incur convergence latency and communication overhead [74].\nOverall, there are many open issues and emerging topics from the perspective of the learning system architecture.\nFirstly, the question of whether Actors should conduct model inference to derive actions remains unresolved. In\nscenarios where Actors are tasked with model inference, Actors need to maintain the up-to-date copy of the\nmodel, obtain the actions through model inference, and transfer the experiences after environment interaction\n(e.g., IMPALA shown in Fig. 4(d)). Conversely, in other approaches, Actors focus on the interaction with the\nenvironment and give up the work of model inference (e.g., rlpyt shown in Fig. 4(e)). Instead, Learners obtain the\nactions through model inference and transfer them to Actors. Both paths of solutions have two sides. The former\npath takes full advantage of computational capacities in Actors to share the responsibilities of model inference.\nHowever, frequent model update requests from many Actors will significantly decrease the learning efficiency of\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n10\n•\nLiu et al.\nLearner [76]. The latter path reduces the overhead of model synchronization among Actors and Learners, but it\nmay incur latency on transferring actions and experiences [77].\nBesides, unlike the typical design of the system architecture where Actor and Leaner are the two types of\ndistributed workers, some methods innovatively extend the workers to fine-grained types. For example, there are\nActor worker, Policy worker, and Trainer in [78]. The Actor worker is responsible for environment interaction, the\nPolicy worker takes charge of policy inference, and the Trainer is in charge of model learning. The main purpose\nof this design is to increase computational efficiency by allocating different workers to different computing\nnodes that suit their tasks, e.g., CPU, GPU or TPU nodes. A similar design can be found in SampleFactory [60],\nwhere there are Rollout worker, Policy worker, and Leaner for the fine-grained types of parallel workers. Also, Li\net al. [79] refines the workers to Actor, V-learner, and P-learner for data collection, value learning, and policy\nlearning, respectively. Since there is a model learning job for Dyna-style model-based DRL, Zhang et al. [80]\nrefine the parallel workers to Data Collection Worker, Model Learning Worker, and Policy Improvement Worker in\ntheir asynchronous model-based training method.\nFurthermore, as previously discussed, the centralized architecture adheres to a star communication topology,\nwhereas the decentralized architecture employs a fully connected communication topology. Each of these solutions\ncarries its own drawbacks, such as the single point of failure in the centralized architecture and the increased\nsynchronization overhead in the decentralized one. To achieve a better trade-off, Assran et al. [81] propose\ngossip-based Actor-Learner architecture where parallel workers are organized in a peer-to-peer communication\ntopology. Gossip [81, 82] communication mechanism is used to exchange the model update in this method. Each\nparallel worker only communicates with its neighbors, and does not need to communicate with everyone else.\nThe theoretical proof is given that the parallel workers’ models are guaranteed to remain within 𝜖−𝑐𝑙𝑜𝑠𝑒during\ntraining. Experiments also show that this architecture achieves competitive performance compared to A3C[70]\nand IMPALA[61]. Similarly, Sha et al. [83] propose distributed asynchronous policy evaluation based on directed\npeer-to-peer networks, allowing each parallel worker to update its value function locally by using data transferred\nfrom its neighbors.\n4\nSIMULATION PARALLELISM\nRequiring to interact with the environment to collect training samples is one of the main differences between\nDRL and DL. Simulations that simulate realistic environments are widely used to reduce training cost and time,\nespecially for physics-based applications, e.g., robotics and autonomous driving. There are many simulation\nplatforms that are popular in DRL training, e.g., OpenAI Gym, MoJoCo, Surreal, Unity ML, Gazebo, and AirSim.\nBasically, the computations in simulations (e.g., physics calculation, rewards calculation, and 3D rendering) are\ninvolved in each step of training iteration. As the size of experiences increased for training complex applications,\nthe simulation efficiency becomes more and more important in the training acceleration. For instance, 2.5 billion\nframes of experience are needed for PointGoal navigation in 3D scanned environments[72]. How to parallel\nsimulations so as to increase the sample efficiency is challenging. To this end, many approaches have been\nproposed. According to existing reinforcement learning literature, there are mainly two simulation parallelism\nstrategies.\n4.1\nDistributed simulation based on CPU clusters\nMany simulation platforms in use today are run on CPUs for many existing DRL methods in training, e.g., Atari\nin [1, 70, 84, 85], MoJoCo in [36, 86–88] and OpenAI Gym in [73, 89, 90]. A straightforward strategy for training\nacceleration is employing a cluster of CPUs to execute a number of instances of the environment in a distributed\nmanner, where each instance normally runs on a process or a thread. Based on this, the experience data used\nfor training can be generated at a higher speed. Besides, the experience data can also be more diversified by\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n11\n(a) CPU simulation parallelism pipeline\n(b) GPU simulation parallelism pipeline\nFig. 5. Comparison of two simulation parallelism pipelines. (a) The intermediate data needs to be copied from CPUs to GPUs\nback and forth during training in CPU simulation parallelism pipeline. (b) GPU simulation of zero-copy enables directly\naccessing to simulation results in the GPU buffers and keeps all of the computations on the GPU. [62]\nimporting stochastic or using different policies when interacting with different instances of the environment.\nThis strategy is adopted by many existing parallel and distributed DRL training methods such as Gorila [69],\nA3C [70], APE-X [67] and IMPALA [61], and yields impressive results. For example, in APE-X, approximately\n50K environmental frames per second (FPS) can be generated by using 360 actor machines; each actor runs an\ninstance of the environment by a CPU core.\nNevertheless, the scaling of these distributed simulation methods is determined by the number of CPU cores in\nthe system. As the number of CPU cores and nodes in the cluster increases, potential overhead may be incurred\nin communication across nodes [91, 92], synchronization [72, 93] and resource allocation [94, 95]. Furthermore,\nas GPUs are commonly used in neural network computations, a combination of CPUs and GPUs is popular in\nmost of the DRL researches. In this case, additional context-switching overhead would be introduced, as shown\nin Fig. 5(a). That is because intermediate data needs to be copied from CPUs to GPUs back and forth during\ntraining in this case. It is noted that simulation is only one part of the training platform. After a simulation step\nin the environment, the next state and a reward need to be computed. When using GPUs for neural network\ncomputations, the experience data (e.g., the next state and rewards) needs to be transferred from system memory\nto GPU memory for neural network inference. Then, actions are produced and need to be copied again to system\nmemory for CPUs to perform the simulation step.\n4.2\nBatch simulation based on GPUs/TPUs\nIn view of the limitations of CPU simulations on a large-scale, researchers have been committed to developing\nbatch simulation methods based on specialized hardware architectures such as GPUs or TPUs. Liang et al. [96]\npropose a GPU-accelerated RL simulator to parallel the simulations, which can achieve thousands of humanoid\nrobots concurrently simulated and generate 60K environmental frames per second in a single machine with\none GPU and CPU. With the batch simulation framework, the humanoid robots can be trained to run in less\nthan 20 minutes while using 1000 × less CPU cores compared to previous works [97]. While relying on GPU to\nperform most of the simulation computations, this method still requires CPUs to perform tasks of getting state\nand applying controls. The performance of this method is also bounded by CPU-GPU communication bandwidth.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n12\n•\nLiu et al.\nTherefore, zero-copy batch simulation that eliminates CPU-GPU communication has become a hot research\ntopic in the field. Dalton et al. [94] propose a CUDA Learning Environment named CulE to support GPU emulation\nfor Atari. CuLE enables the Atari simulation to run directly within GPU memory, thereby eliminating off-chip\ncommunications that previous methods endured [96]. Up to 155K FPS in emulation only is achieved when emulated\nby CuLE with 4096 environments in parallel by 1 GPU. Further, other than targeting at 2D Atari simulation,\nShacklett et al. [98] propose a large batch simulation method for 3D environments (PointGoal navigation). This\nmethod bundles a large batch of requests for simulations and processes one entire batch at once. The key idea is\nto amortize the costs of simulation computation, synchronization, and communication across a batch of requests.\nSimilarly, Makoviychuk et al. [62] propose a high performance robotics simulation platform named Isaac Gym\nfor a variety of robotics environments. Isaac Gym offers a Tensor API that grants direct access to simulation\nresults stored in GPU buffers, ensuring that all computations remain within the GPU. In this way, the bottleneck\nof off-chip communication between CPUs and GPUs can be eliminated, as shown in Fig. 5(b). Isaac Gym enables\nthousands of environments simulated on a single GPU and achieves up to 300× improvement in the training time\nover previous work [99].\nBesides, Freeman et al. [100] from Google Research propose an open-source library named Brax for large-scale\nrigid body simulation by using TPUs. Brax enables simulation computation and RL optimizer performed together\non the same TPU. The experiment results show that Brax can achieve hundreds of millions of steps per second\nfor MujoCo-ant on a TPUv3 8×8 accelerator.\n4.3\nDiscussion\nCurrently, using a combination of CPUs and GPUs in DRL training is the mainstream in the field. CPUs are used\nto simulate the environments and perform environmental interaction, while GPUs are used to perform neural\nnetwork inference as well as weights update. Hence, CPU-GPU communication is an unavoidable performance\nbottleneck. To this end, zero-copy simulation that contains all the computation in GPUs or TPUs is one of the\nsignificant research directions for highly efficient simulations. This provides an alternative way of simulation\nparallelism, with no requirement of accessing to CPU clusters which most researchers find hard to get. Besides,\ndifferent from previous works that run each environment instance in the individual simulation, sharing scenes\nwith other robots in a simulation is an effective way to take advantage of batch parallelism and maximize the\nthroughput [62, 96, 98]. Since texture and geometry objects tend to be large in size, naively loading these objects\nfor every environment is unaffordable for GPU memory. For instance, Liang et al. [96] loads all agents (and\ntask-related objects) in the same simulation. Shacklett et al. [98] maintain 𝐾≪𝑁unique scenes in GPU memory,\nwhere 𝑁is the number of parallel environments in a batch.\nTransferring the learned policies from simulation to reality is an essential demand for many real-world\napplications. Simulation-to-reality (Sim-to-real) is a topic of much interest. One straightforward strategy is to\nbuild a realistic simulator that can perfectly replicate the reality. Hence, the learned policies can be directly\napplied in real-world environments. This is referred as the zero-shot transfer. In recent years, the zero-shot transfer\nhas been successfully demonstrated in several domains. Andrychowicz et al. [6] transfer learned policies for\ndexterous in-hand manipulation to physical robots by performing training entirely in simulation. Rudin et al. [101]\ndemonstrate sim-to-real transfer for quadrupedal robots by using massively parallel DRL. Loquercio et al. [102]\nachieve the zero-shot transfer for high-speed UAV navigation while some realistic scenarios that were never\nexperienced during training in simulation. The techniques for enabling seamless transfer in these approaches\ncan be summarized as follows. First, add realistic sensor noise to the observations. Second, randomize physical\nproperties in simulations such as friction coefficients and dynamics of objects. Third, learn with disturbances\nsuch as pushing the robot randomly and adding noise to rewards during training.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n13\n5\nCOMPUTING PARALLELISM\nOther than simulation, intensive computing workloads are unavoidable in DRL training e.g., network inference,\nbackpropagation, and evolutionary computation (see Section 7). As a result, computing parallelism techniques\nare widely utilized to speed-up computations in DRL training. There are mainly three ways in the literature:\ncluster computing, single machine parallelism, and specialized hardware architectures (i.e., GPUs, FPGA, TPUs)\nacceleration.\n5.1\nCluster computing\nCluster computing usually includes multiple machines working together to perform computation-intensive or\ndata-intensive tasks. Those machines are also called ‘nodes’ and inter-connected by a high-speed local network.\nSince the computing resources in individual nodes are relatively constrained, integrating the resources of multiple\nnodes is a straightforward and effective way for acceleration of large-scale processing. In the early days when\ndeep neural networks were not widely used in reinforcement learning, the classic cluster computing framework\n(i.e., MapReduce [103]) has been applied to reinforcement learning for acceleration [104]. This method distributes\nthe computation in the large matrix multiplication for Markov decision process (MDP) solutions such as policy\nevaluation and iteration. Considering the inadequacy of MapReduce for iterative computation involved in neural\nnetwork training, Dean et al. propose DistBelief[44] to utilize clusters of machines in training and inference for\nlarge-scale deep networks. However, this method targets deep network training, which is different from that\nof DRL. To this end, one of the earliest works on parallel and distributed DRL, Gorila [69], is proposed. Gorila\nutilizes a cluster of machines to achieve an order of magnitude of reduction in training time than single-machine\ntraining. To the best of our knowledge, this is the enlightenment work that many methods, such as Ape-X[67],\nIMPALA[61], and R2D2[68], spring up in this direction. From the evolution of related works, we can see that\ncluster computing is roughly a standard mode of accelerating training for DRL.\n5.2\nSingle machine parallelism\nOther than cluster computing that uses multiple machines, single machine parallelism utilizes multiprocessors or\nmulti-core CPUs in a single machine to increase the computing ability. The rationale for applying single machine\nparallelism is that distributed clusters may not be affordable for most researchers, unlike the widely available\ncommodity workstations. Besides, data transfer, model synchronization across machines during training will\nalso incur noticeable overhead. Multiprocessing and multithreading are the key technologies in single machine\nparallelism. More specifically, multiprocessing refers to operating different processes simultaneously by more\nthan one CPU processor, while multithreading refers to executing multiple independent threads in parallel by a\nsingle CPU, especially the multi-core CPU.\nThe representative method is A3C [70]. In A3C, many actor-learner threads are launched in parallel, and each\nthread performs a training procedure separately, including environment interaction, experience collection, and\nmodel update. In this way, the speed of data generation and the sample efficiency can be improved significantly.\nThe results in [70] showed that A3C using 16 CPU cores surpasses DQN variants using Nvidia K40 GPU in half\nof the training time and achieves comparative performance to Gorila which uses 100 machines.\nPetrenko et al. present a high-throughput training system named SampleFactory [60], which is designed based\non a single machine with a multi-core CPU and a GPU. Though optimizing the efficiency and resource utilization\nin a single machine setting, Sample Factory can achieve throughout as 130K FPS (Frame per Second) and 4 times\nspeedup over the state-of-the-art baseline SEED_RL [77] with a workstation-level PC. One of the significant\ncontributions of SampleFactory is that it allows large-scale DRL experiments accessible to a wider community by\nreducing the computational requirements.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n14\n•\nLiu et al.\nTable 1. Comparison of computing parallelism types in parallel and distributed DRL implementations.\nMethods\nComputing parallelism Types\nImplementation Details\nMajor Results\nCC\nMP/MT GPU\nFPGA\nTPU\nGorila[69]\n✓\n31 machines\n10× speedup over GPU implementation\nApe-X[67]\n✓\n✓\n360 CPU cores and 1 P100 GPU\n4× median scores over Gorila\nR2D2[68]\n✓\n✓\n256 actors and 1 GPU\n4× median scores over Ape-X\nIMPALA[61]\n✓\n✓\n✓\n500 CPU cores and 8 P100 GPUs\n250K FPS and multi-task setting\nRay RLlib [105]\n✓\n✓\n✓\n8,192 CPU cores on EC2\ncompletes training Mojoco in 3.7mins\nARS[106]\n✓\n48 CPU cores on EC2\n15× speedup over ES-based method[97]\nA3C[70]\n✓\n16 CPU cores\n2× speedup over K40 GPU implementa-\ntion\nReactor[85]\n✓\n✓\n20 CPU cores\n4× speedup over A3C\nDBA3C[107]\n✓\n✓\n64 nodes with 768 CPU cores\ncompletes training Atrai 2600 in 21 mins\nDPPO[108]\n✓\n64 actors\n>20× speedup over A3C\nD4PG[109]\n✓\n64 CPU cores\n4× higher return than PPO\nSampleFactory[60]\n✓\n✓\n36 CPU cores and a 2080Ti GPU\n4× speedup over SEED_RL\nGA3C[110]\n✓\n✓\n16 CPU cores and 1 Titan X GPU\n45× speedup over A3C\nPAAC[111]\n✓\n✓\n4 CPU cores and a GTX 980 Ti GPU\n>6× speedup over Gorila\nrlpyt[71][75]\n✓\n✓\n8 P100 GPUs and 40 CPU cores\n6× speedup using 8 GPUs relative to 1\nGPU\nDactyl[6]\n✓\n✓\n✓\n384 nodes (6144 cores and 8 GPUs)\n5.5× speedup over implementation with\n1 GPU and 768 CPU cores\nDD-PPO[72]\n✓\n✓\n256 V100 GPUs\n196× speed up over 1 V100 GPU\nMSRL[112]\n✓\n✓\n64 GPUs\n3× speedup over Ray RLlib\nSRL[78]\n✓\n✓\n15K CPU cores and 32 A100 GPUs\n5× speedup over OpenAI Rapid[113]\nSpeedyZero[114]\n✓\n✓\n192 CPU cores and 20 A100 GPUs\nmastering Atari benchmark within 35\nminutes using only 300k samples.\nNNQL[115]\n✓\nArria 10 AX066 FPGA\n346× speedup over GTX 760 GPU\nTRPO_FPGA[116]\n✓\nIntel Stratix-V FPGA\n19.29× speedup over i7 CPU\nDDPG_FPGA[117]\n✓\nIntel Stratix-V FPGA\n4.53× speedup over i7-6700 CPU core\nFA3C[118]\n✓\n✓\nXilinx VCU1525 VU9P FPGA\n27.9% better than Tesla P100\nPPO_FPGA[119]\n✓\n✓\nXilinx Alveo U200\n27.5× speedup against Titan Xp GPU\nOn-chip replay[120]\n✓\n✓\nXilinx Alveo U200 acceler\n4.3× higher IPS over GTX 3090 GPU\nAlphaZero[3]\n✓\n✓\n✓\n5000 TPUs v1 and 64 TPUs v2 cores\ndefeats world-champion program by\ntraining within 24 hours\nAlphaStar[4]\n✓\n✓\n✓\n3,072 TPU v3 and 50,400 CPU cores\nachieves above 99.8% of ranked human\nplayers by training in 44 days\nOpenAI Five[113]\n✓\n✓\n✓\n1,536 GPUs and 172,800 CPU cores\ndefeats Dota 2 world champion (Team\nOG) by training in 10 months\nGATO[121]\n✓\n✓\n✓\n256 TPU v3 cores\nhandles 604 distinct tasks with a single\nnetwork\nSEED_RL[77]\n✓\n✓\n✓\n520 CPU and 8 TPU v3 cores\n11× faster than the IMPALA with a P100\nGPU\nCC: Cluster Computing; MP/MT: Multiprocessing or Multithreading; Statistics are collected from the corresponding papers.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n15\nA combination of cluster computing and single machine parallelism is quite common in literature. Fiber [122]\nextends multiprocessing from one machine to distributed environments. AlphaGo [2] uses asynchronous multi-\nthreads to improve the performance in large-scale Monte Carlo tree search. IMPALA [61] provides single machine\nsettings as well as distributed settings to accelerate large-scale DRL training. The benefit of single machine\nparallelism over cluster computing is removing the communication overhead of sending data across machines by\nkeeping all the computing in a single machine. Note that the latency of synchronizing the model may have a\nnoticeable impact on the stability of learning, especially for on-policy learning.\n5.3\nSpecialized hardware architectures\nSince there are lots of batch processing operations (e.g. matrix multiplication) in neural network training and\ninference, the specialized computation hardware architectures with large throughputs such as GPUs (Graphics\nProcess Units), FPGAs (Field Programmable Gate Arrays), and TPUs (Tensor Process Units) are preferable. With\nmassive ALUs (arithmetic and logic units) and good programmability, GPUs are utilized to accelerate in a wide\nrange of applications that go well beyond traditional graphics processing. Comparably, FPGA supports customized\noperations for a specific application by user-programmable interconnects. Although FPGAs might not be superior\nto GPUs in computation efficiency, they are preponderant in reducing energy consumption. In addition, TPUs are\ncustom-developed application-specific integrated circuits (ASICs) for machine learning workloads and achieve\nhigh throughput and low energy consumption for matrix computations. Overall, all these hardware architectures\ncan exploit the inherent computing parallelism to achieve speedup on data-intensive computing tasks.\nIn recent years, Nvidia has been devoting itself to utilizing GPUs in distributed DRL. Mohammad et al. propose\nGA3C [110], an architecture based on A3C [70] with highlighted on GPU utilization to augment the training\nspeed. GA3C employs trainer threads to collect batches of data and submit them to a GPU for exploiting the\nGPU’s computational capabilities better. Rather than utilizing a single GPU, a multi-GPU RL framework has also\nbeen proposed [96], in which 32 GPUs in maximum are reported in the experiments. Based on that, hundreds to\nthousands of locomotion robots are parallelized to accelerate the training. Lasse et al. propose IMPALA [61] to\nsolve large-scale, multi-task RL problems with a multi-GPU system (NVIDIA DGX-1), which contains 8 NVIDIA\nTesla V100 GPUs.\nFPGAs are utilized in tabular reinforcement learning for acceleration in the early days [123, 124]. As the neural\nnetwork has been induced in reinforcement learning, more room for improvement by FPGAs is created. Many\nworks are proposed to use FPGAs in accelerating backpropagation in DRL. Su et al. [115] propose an FPGA\nacceleration system for DQN, which allows dynamically reconstructing the network to achieve better learning\nresults. Their experiment results show that the proposed system can achieve up to 346 times and 77 times speedup\ncompared to GPU implementation and CPU implementation, respectively. Cho et al. propose a FPGA-based A3C\nDeep RL platform named FA3C [118]. In FA3C, simple and generic processing elements are designed for all types\nof DNN layers. Compute unit pairs are also proposed for inference and training tasks separately. Experiment\nresults show that FA3C achieves 27.9% higher IPS values (the number of inferences processed per second) and\n1.62 times better energy efficiency than GPU implementation (Nvidia Tesla P100).\nTPUs are designed specifically for machine learning workloads and used frequently in the latest Google findings\nin the DRL domains such as AlphaZero [3], AlphaStar [4], GATO [121], etc. In terms of distributed training\nplatforms, Google research teams propose a scalable reinforcement learning framework called SEED_RL [77],\nin which multi-TPUs architecture is utilized and the learning speed is significantly improved compared to the\nbaseline based on multi-GPUs. For example, an 11 times speedup is achieved by SEED_RL over a strong baseline\nIMPALA on DeepMind tasks.\nAlthough these specialized hardware architectures have brought a huge speedup, CPUs also play an essential\nrole in DRL training (e.g., environment interaction, task scheduling, and parameter sharing). A heterogeneous\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n16\n•\nLiu et al.\narchitecture of CPUs and specialized hardware is a promising endeavor in this area. Combining CPUs and GPUs\nhas become the popular way of saturating the computation power of GPUs in recent years, and many works are\nproposed to utilize CPU-GPU heterogeneous architectures [61, 75, 110, 114]. Other than this, Meng et al. propose\na CPU-FPGA heterogeneous platform for accelerating Proximal Policy Optimization [119]. Rather than only\nfocusing on specific RL algorithms, Meng et al., further propose a more generic CPU-FPGA accelerator using\non-chip replay management for widely used RL algorithms including DQN and DDPG [120]. By allocating the\nworkloads to CPU and FPGA sophisticatedly, CPU-FPGA heterogeneous method achieves significant improvement\nin overall throughput.\n5.4\nDiscussion\nTable 1 compares the computing parallelism types utilized in current parallel and distributed DRL implementations.\nWe can see in the table that all of these parallelism types are widely used, and a combination of different computing\nparallelism techniques is popular. More specifically, all these three types of parallelism are reported in the latest\nresearch such as Ray RLlib[105], SEED_RL[77], Dactyl[6], AlphaZero[3], GATO[121], etc. As parallelism in DRL\nbecomes more and more ambitious, achievements in training efficiency in this area have been proven. The\nstate-of-the-art solution only took 3.6 mins [105] to train Atari 2600 games. In comparison, it took 15 days to train\na single Atari game several years ago, which at that time was a significant breakthrough in human history [1].\nBesides, with the parallelization of computing resources, computing task scheduling plays an important role\nin enhancing the efficiency of distributed DRL systems. The computing workloads involved in distributed DRL\ntraining are diverse and heterogeneous, including environment interaction, network interference, gradient\nbackpropagation, and model synchronization, which are handled by workers such as Actors, Learners, or the\nParameter Server. Hence, it is essential to effectively schedule these computing tasks across processors even\nmachines with varying resource configurations, thereby minimizing the overall training time. There are many\nscheduling strategies have been proposed in current distributed DRL systems.\n• Load balancing. This strategy enables load-aware scheduling that assigns computing tasks to distributed\nhardware, considering factors such as resource contention and input locality. Ray[125] utilizes fine-grained\nand coarse-grained load balancing methods for stateless and stateful computations, respectively. rlpyt[75]\nforms two alternating groups of simulation processes, and schedules GPU to serve each group in turn to\nkeep utilization high. Meng et al. [119] propose inter-load balancing method for two compute units (CUs)\ninvolved in training value and policy networks. Li et al. [79] propose to explicit control the frequencies for\nparallel workers to achieve load balance.\n• Resource dynamic adjusting. This strategy dynamically scales the resources for workers to accelerate DRL\ntraining with minimum costs. MINIONSRL [126] leverages a scheduler that dynamically adjusts the number\nof Actor workers to optimize the DRL training with minimal training time and costs. Similarly, Meng et al.\ndesign a scheduling mechanism that re-allocates CPU threads to processing a sub-batch of training for the\nLearner in heterogeneous computing environments.\n• Computation and communication overlapping. In distributed DRL training, computing tasks can be blocked\nby communication tasks due to their execution dependencies. For example, gradient aggregation has to\nwait for the completion of gradient transfer for parallel workers. This strategy schedules the computing\nand communication tasks to reduce the waiting time, thereby improving the computation utilization.\nPEARL [127], an open-source distributed DRL framework, designs a Learner module that enables scheduling\nto overlay the communication and computation. Mei et al [114] propose an optimized data transfer\nscheduling technique that overlaps computation in distributed DRL training.\n• Preemptive scheduling. This strategy proactively terminates lagging tasks, as each worker must wait\nfor all parallel workers to complete during synchronous training. Wijmans et al. propose a straggler\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n17\npreemption mechanism in DDPPO [72], where the slow-running worker is preempted once a certain\npercentage of the other parallel workers are completed collecting their experience data in one iteration.\nSimilarly, PyTorchRL [128] employs a comparable preemption mechanism within its distributed DRL\ntraining framework.\n6\nDISTRIBUTED SYNCHRONIZATION MECHANISMS\nThe dominant solution1 for training acceleration in distributed DRL is employing a number of workers to\ncooperatively train the model based on distributed stochastic gradient descent (DSGD) [27, 48]. This is based on the\ndata parallelism introduced in Section 2. During DSGD training, each parallel work holds a copy of the model,\nperforms training based on a subset of environmental experiences, and then aggregates the update to the target\nmodel cooperatively. It is noted that distributed synchronization among workers is vital to the training efficiency,\nespecially in the heterogeneous environment where the parallel workers may process at different speeds. Variants\nof synchronization mechanisms are systemically studied in deep learning area, i.e., Bulk Synchronous Parallel\n(BSP)[129], Asynchronous Parallel (ASP) [130] and Stale Synchronous Parallel (SSP) [131]. These mechanisms lay\na good technical foundation for distributed DRL.\n(a) Asynchronous Off-policy Training\n(b) Synchronous On-policy Training\nFig. 6. Comparison between different synchronization mechanisms in distributed DRL. The rectangular bar represents the\ncorresponding training step for the Actor and the Learner. Arrows represent data transmission for the model parameter\nupdate or the rollout data.\nHowever, since importing environment interaction and dividing the parallel workers into different roles such\nas Actors and Learners, DRL may face new difficulties while applying distributed synchronization approaches.\nThe model holding in Actors for the behavior policy may lag behind the model in Learners for the target\npolicy after several iterations in parallel settings. This makes the behavior policy different from the target\npolicy gradually. It is noted that there are off-policy and on-policy schemes for DRL algorithms (see Section 2).\nAccording to the training schemes and the way of synchronization among workers, there are mainly two types of\ndistributed synchronization mechanisms in current distributed DRL solutions: asynchronous off-policy training\nand synchronous on-policy training.\n6.1\nAsynchronous Off-policy Training\nAsynchronous off-policy training is the distributed synchronization mechanism that was initially applied in dis-\ntributed DRL, and it has been widely used later on, e.g., Gorila[69], APE-X[67], R2D2[68] etc. In the asynchronous\n1Another promising training solution based on evolutionary learning will be discussed in Section 7\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n18\n•\nLiu et al.\ntraining, the parallel workers operate independently. As shown in Fig.6, each worker performs model update at its\nown pace without waiting for other workers. This will naturally lead to differences between the behavior policy\nmodel for Actors and the target policy model for Learners. We can see in Fig.6(a), at time 𝑡1, the behavior policy\nwith parameter 𝑊0 for Actor 𝑗is different from the target policy with parameter 𝑊1 for the Learner. The same\nphenomenon can also be seen at time 𝑡2 , when the behavior policy with parameter 𝑊1 for Actor 𝑖is different\nfrom the target policy with parameter 𝑊2 for the Learner. Therefore, this type of synchronization mechanisms is\nmainly used for off-policy algorithms which do not require the consistency between the behavior policy and\nthe target model while learning. DQN is a classic off-policy algorithm, and many asynchronous distributed DRL\nmethods such as Gorila, APE-X, and R2D2 are based on DQN. More specifically, asynchronous off-policy training\nis normally based on a centralized architecture and there is a Parameter Sever maintaining the global model.\nEach Actor pushes its interaction training data to the Replay Memory and pulls the up-to-date parameters from\nthe Parameter Server asynchronously. The Learner updates the global model based on the training data in the\nReplay Memory. In this way, every worker tries to optimize the global model by contributing its efforts based on\nthe local copy of the model, thereby accelerating the model training.\nHowever, it is known that off-policy algorithms suffer from the stability issue [37]. That is mainly because the\ndistribution under the target policy shifts from that under the training data collected by the behavior policy, named\npolicy-lag [61]. To relieve this problem, there are many off-policy correction methods have been proposed in\ndistributed DRL. Espeholt et al. propose V-trace correction methods in IMPALA [61]. In this method, V-trace targets\nare computed based on trajectories collected by the behavior policy and the current target value function under\nthe target policy, where two truncated importance sampling (IS) weights are imported to improve convergence.\nBabaeizadeh et al. propose 𝜖-correction in GA3C [110], which adds a small constant 𝜖during gradient estimation.\nThis prevents the log value of the sampled action probability from becoming very small and leading to numerical\ninstabilities.\nFurther, the nature of the asynchronous update will also cause stale update, which occurs when a worker\nupdates the target model using training data generated by the obsolete model [129]. As shown in Fig. 6(a), the\nLearner, Actor 𝑖and Actor 𝑗at time 𝑡0 hold the model denoted as 𝑊0. Actor 𝑖at time 𝑡1 pushes the training data\nto create a new model denoted as 𝑊1. Actor 𝑗at time 𝑡2 pushes the training data to create a new model denoted\nas 𝑊2. However, at time 𝑡2, Worker 𝑗’s model used in training (i.e. 𝑊0) is stale compared to the target model (i.e.\n𝑊1). The stale update, shown as red arrows in Fig. 6(a), may slow down the convergence in training [70, 132].\nDistributed model-based DRL methods also suffer from this issue. For example, reanalyze staleness is incurred in\nSpeedyZero[114]. This is because the parallel trainers commonly receive batches that have been reanalyzed by\nan outdated version of the model, rather than the latest target model.\n6.2\nSynchronous On-policy training\nIn order to mitigate the stale update issue mentioned above, the synchronous on-policy training method lets\nthe parallel workers wait for the completion of gradient computation for all the workers, and then performs\nthe model update synchronously. In this way, the models contained in parallel workers are consistent. This\nconsistency among workers ensures the on-policy property of DRL algorithm that the behavior policy and the\ntarget policy are the same during training. Current synchronous on-policy training solutions in distributed DRL\nmainly use two ways to synchronize the model: centralized and decentralized manners. For the former one\nsuch as PAAC [133] and DBA3C [107], every worker transmits its gradients to the Parameter Server for model\noptimization. Then the updated model is copied to every worker. With respect to the decentralized manner such\nas DDPPO [72], all worker’s gradients are shuffled among them. Then, the model in each worker is updated based\non the aggregated gradients. In both ways, the parallel workers update the model synchronously and consistently.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n19\nIt is clear in Fig. 6(b) that there is no stale update that each worker holds the consistent model in each training\niteration.\nHowever, it may incur the synchronization barrier, where the fast-running worker keeps idle and waits for the\nslow-running one in each iteration, as shown in Fig. 6(b). At time 𝑡1, after Actor 𝑖completes its training iteration,\nit waits for Actor 𝑗to complete. This will deteriorate the utilization of computing resources in the cluster[72]. It\nis known that heterogeneity is the marked characteristic in the real cluster environment[134], where the case of\nprocessing at different speeds for different workers is quite common. Besides, the gradient aggregation induces\nburst traffic while transmitting the gradients for all workers in a short duration. This may cause congestion and\nthen reduce the utilization of communication resources.\n6.3\nDiscussion\nAlthough asynchronous off-policy training methods have been shown higher resource utilizations than synchro-\nnous counterparts, they usually suffer from stability issues and converge to poorer results[129, 135]. Besides,\nrecent works have demonstrated experimentally that synchronous on-policy training performs better than asyn-\nchronous off-policy training when using a large scale of distributed nodes. Adamski et al. [107] witnessed that\nsynchronous on-policy training outperformed asynchronous off-policy training in the experiments of training\nAtari games using 64 nodes.\nTo alleviate the synchronization barrier in synchronous training, stale-synchronous training introduces\nflexibility in the strict synchronization timing, allowing faster workers to proceed to the next iteration under\nthe condition of bounded staleness (i.e. a concept of the maximum obsolete age between the slowest and other\nworkers). Once the staleness bounded is reached, a synchronous model update is enforced. In this way, a better\ntrade-off between model consistency and resource utilization can be achieved. To the best of our knowledge,\nwhile stale-synchronous training achieves decent results in deep learning area[136, 137], it is not yet widely\nadopted in distributed deep reinforcement learning currently. We believe stale-synchronous training holds great\npotential value in distributed deep reinforcement learning. One possible way is to enforce a synchronization\naccording to the divergence between the target policy and the behavior policy.\nMoreover, beyond vanilla on-policy and off-policy training, combining on-policy and off-policy in distributed\nDRL has attracted much attention in recent years. Schmitt et al. [138] propose to mix the off-policy replay\nexperience with on-policy data and introduce a trust region algorithm that efficiently mitigates bias and enables\nefficient learning in distributed settings. Results show that this method outperforms IMPALA[61] based on V-trace\nimportance sampling. Other than this, Borges et al. [139] propose to combine the off-policy targets with the\non-policy targets in the distributed model-based DRL system named MuZero, improving the convergence speed\nand rewards.\n7\nDEEP EVOLUTIONARY REINFORCEMENT LEARNING\nAs mentioned above the dominant solution for training the neural network in distributed DRL is distributed\nstochastic gradient descent (DSGD) based on backpropagation of gradients. On the other hand, evolutionary\ncomputation, an approach inspired by the process of natural selection, finds its great potential in solving DRL\nproblems. Many evolution-based training methods for distributed DRL have been proposed in recent years\nand have demonstrated impressive performance[8, 66, 97, 140–142]. This field of research is also named Deep\nEvolutionary Reinforcement Learning [8] and neuroevolution [142] in the literature. In the interest of brevity,\nevolution-based training methods directly perform searches in the parameter space by evolving a population of\ncandidate solutions over many generations, without the need for backpropagating and aggregating gradients. As\na result, evolution-based training methods enable massive parallelization with a low bandwidth requirement.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n20\n•\nLiu et al.\n(a) Evolution-guided Policy Gradient Method[143]\n(b) Deep Evolutionary Reinforcement Learning[8]\nFig. 7. Architectures of current methods based on a combination of learning and evolution.\n7.1\nTraining acceleration based on evolution strategies\nEvolution Strategies (ES) is one of the major branches of evolutionary computation, which iteratively updates a\nsearch distribution by using an estimated gradient on the parameter spaces [144]. There are many proposals that\nutilize evolution strategies algorithms to solve DRL problem [97, 141, 145]. One representative research is [97]\nproposed by Salimans et al. In this method, a population of parameters (genotypes) and their objective function\nvalues (fitness) for the neural network are maintained for every iteration (generation). The parameters with the\ngreatest fitness are selected to form the population for the next generation. This iteration is ended while the\noptimization objective is achieved. Let 𝐹(𝜃) represent the objective function parameterized by 𝜃, and 𝑝𝜇denotes\nthe distribution (parameterized by 𝜇) that the population follows. The goal of the method is to maximize the\nexpectation value E𝜃∼𝑝𝜇𝐹(𝜃) over the population by searching for 𝜇with stochastic gradient ascent. In terms of\nsolving the DRL problem, the objective function 𝐹(𝜃) is the return after a sequence of actions a is taken, denoted\nby 𝑅(a(𝜃)), where the actions are determined by a policy a = 𝜋(𝑠|𝜃). The method supposes the distribution 𝑝𝜇as\nisotropic multivariate Gaussian with mean 𝜇and fixed covariance 𝜎2𝐼. Then, E𝜃∼𝑝𝜇𝐹(𝜃) can be written in the\nform of a Gaussian-blurred version of the original objective, that is,\nE𝜃∼𝑝𝜇= E𝜖∼N(0,𝐼)𝐹(𝜃+ 𝜎𝜖).\n(1)\nHence, the ES method performs the optimization by stochastic gradient ascent over 𝜃directly with the following\nestimator:\n∇𝜃𝐸𝜖∼N(0,𝐼)𝐹(𝜃+ 𝜎𝜖) = 1\n𝜎𝐸𝜖∼N(0,𝐼)𝐹(𝜃+ 𝜎𝜖)𝜖.\n(2)\nNote that the ES method does not calculate gradients analytically but estimates the gradients of the objection\nfunction over parameters accordingly. By exploring in the parameter space instead of action space compared to\npolicy gradient methods, the ES algorithms can be considered as a black box optimization that is invariant to\naction frequency and delayed rewards. This is thus better suited for problems with a long time horizon. Salimans\net al. [97] show that their algorithm achieves comparable results in continuous robotic control problems and\nvideo games.\nAnother surprising advantage of the method in [97] is the massive parallelization ability. This method incurs\nrelatively low communication overhead when parallelized across many workers for the following reasons: First,\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n21\nsince the operation in each iteration is conducted over the entire episode, the frequency of communication\nbetween workers is significantly reduced. Second, the information that needs to be transmitted between workers\nis limited to the return of an episode. This requires less bandwidth compared to distributed gradient-based\nmethods, where the transfer involves gradients of the parameter vector or the parameter vector itself. Third, the\nelimination of value function approximations helps to reduce communication overhead, as there is no requirement\nfor the additional gradient synchronization needed to learn the value function. Salimans et al. [97] have deployed\ntheir algorithm over 80 machines with 1, 440 CPU cores, which achieves training time reduction by two orders of\nmagnitude compared to single machine deployment.\n7.2\nTraining acceleration based on genetic algorithms\nGenetic Algorithms (GA) is another branch of evolutionary computation, which is based on the theory about the\ngenetic structure and behavior of chromosomes [146]. Different from the ES-based method [97] which is still\ngradient-based optimization, GA-based methods are gradient-free. Such et al. [66] propose a GA-based method\ncalled Deep GA for solving deep reinforcement-learning problem. Deep GA is based on a genetic algorithm\nwhich normally consists of three main operations: selection, mutation, crossover. In terms of selection, truncation\nselection is performed, which selects the top 𝑇individuals as the parents of the next generation. The mutation is\nperformed by adding Gaussian noise to the parameter vector shown as,\n𝜃𝑛= 𝜓(𝜃(𝑛−1),𝜏𝑛) = 𝜃(𝑛−1) + 𝜎𝜖(𝜏𝑛),\n(3)\nwhere 𝜃𝑛is the next generation of 𝜃(𝑛−1), 𝜓(𝜃(𝑛−1),𝜏𝑛) is a mutation function and 𝜏𝑛is a list of mutation seeds\nfor 𝜃𝑛. 𝜖(𝜏𝑛) follows Gaussian distribution N (0, 𝐼). With respect to crossover, Deep GA does not include it for\nsimplicity. Besides, in order to scale well in the distributed setting, Deep GA leverages a compact encoding\ntechnique that uses an initialization seed plus a list of random seeds to reconstruct the parameter vector. The\nexperiment results show that Deep GA can outperform ES[97], A3C, and DQN on average. The results also reveal\nthat Deep GA is superior to the gradient-based method in solving local optima problems by jumping across them\nin the parameter space.\nUnlike only evolving weights of the neural networks in [66], Stanley et al. propose a GA-based method called\nNEAT [147, 148], which evolves network topologies along with weights to enhance the learning efficiency.\nNEAT has been successfully applied to policy optimization and outperforms baseline with fixed-topology on a\nreinforcement learning benchmark. Although NEAT and HyperNEAT [149] represent early works for topology\nevolution of small networks, there are many recent works targeting deep (large-scale) neural networks [8, 150–\n153], including evolving the hyperparameters [154, 155]. It is known that the architecture and hyperparameters\nof neural networks highly impact the overall performance. Success in the applications of deep (reinforcement)\nlearning often requires fine-tuning of these building blocks of networks manually in practice. Fortunately, with\nthe help of these solutions, the issue of manually designing a neural network for every application can be relieved.\n7.3\nHybridization of evolutionary computation and backpropagation\nThe aforementioned methods mainly follow the path of performing a direct search in the parameter (or encodings)\nspace based on the evolutionary computation, thereby obtaining the optimized neural networks that can serve\nas the policy in DRL problem. While evolutionary computation has merits of diverse exploration and massive\nparallelism based on population-based scheme, it suffers from high sample complexity, especially for problems\nwith large numbers of parameters. Meanwhile, DRL methods leverage powerful backpropagation approaches\nto reinforce profitable actions into weight parameters. Hence, a hybrid scheme that combines the strengths of\nevolutionary computation and backpropagation has great potential value and triggers widely concerns.\nKhadka et al. [143] propose an evolution-guided policy gradient method in DRL. This method incorporates\nevolutionary algorithms to generate diverse experiences and utilizes backpropagation in DRL to learn from them.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n22\n•\nLiu et al.\nMore specifically, this method combines a genetic algorithm with DDPG algorithm, as shown in Fig. 7(a). A\npopulation of actor networks is maintained to interact with the environment and generate diverse experiences in\nparallel. The next generation of actors is created by selection, mutation, and crossover operators. Besides, the\ncritic network and the actor network are trained by using backpropagation based on samples from the reply\nbuffer. Then, the updated weights of the actor network are synchronized to the population of the actor networks.\nIn this way, this method injects the learned behaviors based on DRL into the evolving population. Experiments\nbased on continuous control benchmarks show that this hybrid method outperforms prior DRL and evolutionary\nalgorithms.\nGangwani et al. [156] propose a hybrid algorithm named Genetic Policy Optimization (GPO) for sample-efficient\ndeep policy optimization. Rather than perform crossover in the parameter space in existing methods [143, 147, 149],\nGPO does crossover in the state visitation space with the goal of cloning the behaviors of parents. Research\nshowed that the crossover way of exchanging the parameters straightforwardly often leads to hierarchical\nrelationship destruction and a loss of functionality [147]. Besides, GPO utilizes the DRL algorithm (PPO) based\non backpropagation instead of random permutation to mutate the weights of the actor networks. Experiments\non Mojoco benchmark locomotion tasks demonstrate that GPO outperforms PPO and A2C[157] algorithms and\nachieves comparable or higher sample efficiency.\n7.4\nDiscussion\nNeuroevolution enables useful capabilities that are basically unavailable for traditional DRL approaches, including\nevolving building blocks of neural networks such as weights, topologies, and hyperparameters, facilitating\ndiverse exploration and massive parallelization while maintaining the evolving population of networks. This\nalso opens the door to incorporating learning and evolution to create sophisticated intelligence for solving\nhigh-dimensional decision-making problems. Other than the approaches above, there are still many interesting\ntopics in this exciting area. Some examples incorporate quality diversity [158, 159], novelty search [160, 161]\nand adaptive noise [162, 163] to improve exploration. Many works focus on Neural Architecture Search (NAS)\nby utilizing indirect encoding [164–166], swarm intelligence algorithms [167–169], random search [170–172],\nsequential model-based optimization [173, 174], etc. While NAS is still in the initial research stage, breakthroughs\nhave been made in many field including image classification [175, 176], object detection [177, 178], machine\ntranslation [179, 180], multi-task learning [181, 182].\nBesides, with the increasing capacities of computing resources that are easy to obtain, neuroevolution has\nfurther demonstrated its great potential in solving complex problems. Li et al. utilize deep evolutionary reinforce-\nment learning to evolve diverse agent morphologies to handle locomotion and manipulation tasks in complex\nenvironments [8]. Its architecture is shown in Fig. 7(b). This method employs 1152 CPUs to evolve ten generations\nof populations and train four thousand agent morphologies, with five million environment interactions for each\nmorphology. Real et al. propose to automatically discover neural network models for CIFAR-10 and CIFAR-100\ndatasets by evolving 1000 population of models with 250 parallel workers [151]. Asseman et al. study to use FPGA\nacceleration on Deep GA [66] method with a distributed system of 432 Xilinx FPGAs [183]. In their experiment,\n832 instances in parallel can be run, and 1.2 million frames per second of the aggregation rate can be achieved.\nCompared to the baseline method [66], this method achieves twice speedup and high game scores in most of\ncases.\n8\nOPEN-SOURCE LIBRARIES AND PLATFORMS\nIn AI area that is flourishing, it is undoubted that developing and evaluating innovations in a rapid manner is\nessential in academia as well as the industrial community. This motivates researchers to develop tools, such as\nlibraries and platforms, to facilitate DRL algorithms development in the field. In recent years, many libraries\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n23\nand platforms have been proposed in DRL areas, e.g., OpenAI baselines2, Keras-RL3, MushroomRL4, etc. Some of\nthese are scalable and user-friendly that algorithmic components (e.g., functions, neural networks, environments,\netc.) can be flexibly reused to compose customized learning agents. However, many of them are designed to\nbe run on a single process or a single machine. Parallel and distributed execution at scale are not considered.\nParallel and distributed programming have high professional restrictions that require complex operations, which\nare not friendly to developers who major in learning algorithm design. Therefore, there is a crucial demand to\nencapsulate parallelism primitives in libraries and platforms.\nWe review and compare current open-source libraries or platforms that support parallel and distributed DRL\naccording to the following criteria.\n• Baseline Algorithms. They should provide rich choices of existing deep reinforcement learning algorithms,\nwhich facilitates rapid development and comparisons for users.\n• Environment Integration. Simulation interaction is a basic element in DRL training. Hence, integrating\nsimulation environments in training libraries and platforms plays an important role in supporting easy\nverification as well as applications.\n• Parallel and Distributed Features. The key parallel and distributed techniques and capabilities offered to\nenhance the DRL training efficiency are compared.\nIt is noted that we do not compare the performance of existing libraries and platforms experimentally, which\nis out of the scope of this survey article. Table 2 provides an overview of the open-source libraries or platforms\nthat support parallel and distributed DRL. Statistics are collected from the Github links, documents, and papers\nof corresponding codebases in Aug. 2024.\nMost of these libraries and platforms provide rich choices of baseline algorithms and supported environments.\nMore specifically, RLlib [105], rlpyt [71], ACME [187] and TorchRL [190] include value-based and policy gradient\nfamilies of DRL algorithms, and provide baselines of the SOTA parallel and distributed DRL methods such as\nIMPALA [61] and Ape-X [67]. In comparison, the open-source version of SampleFactory[60] and SRL[78] only\nimplement one algorithm (Proximal Policy Optimization, PPO). In terms of supported environments, all of the\nlibraries and platforms integrate OpenAI’s Gym and support Gym wrapper API. This provides a lot of convenience\nfor users to access to environments. There are 9 codebases that support more than three types of environments,\ne.g., SampleFactory[60] and MindSpore[112] provide 10 and 7 types of environments, respectively.\nBesides, we compare the parallel and distributed features such as parallel training methods and communication\nmechanisms for different libraries and platforms. RLlib [105] and rlpyt [71] offer training options of synchronous\nor asynchronous optimization. SEED_RL [77] encapsulates a fast communication layer based on streaming\nRPCs to mitigate the overhead of remote calls. Fiber [122] provides standard multiprocessing API and supports\nmigration from multiprocessing on one machine to cluster computing across multiple machines on the fly. Besides,\nsome of them [7, 105] provide demonstrations or even automation cluster setups on major cloud providers, e.g.,\nAWS, Azure, and Google Cloud. This is very useful to promote parallel and distributed DRL in practice since\nmost research teams cannot afford to build a cluster of resources.\n9\nCONCLUSION AND FUTURE DIRECTIONS\nExpanding DRL to a large-scale has become an inevitable trend nowadays. However, the rapid progress in which\nthe field is developing makes it difficult to understand in a systematic manner. In this survey, we have provided\na comprehensive literature review on training acceleration for DRL using parallel and distributed computing.\nIn particular, we have analyzed the primary challenges to make DRL training distributed, and demystified the\n2https://github.com/openai/baselines\n3https://github.com/keras-rl/keras-rl\n4https://github.com/MushroomRL/mushroom-rl\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n24\n•\nLiu et al.\nTable 2. Open-source Libraries/Platforms for Parallel and Distributed Deep Reinforcement Learning.\nLibraries/Platforms\nInstitutes\nYear\nBaseline Algorithms\nSupported Envs.\nParallel and Distributed Features\nTF-Agents[184]\nGoogle\n2017\nDQN, DDQN, DDPG, TD3, REIN-\nFORCE, PPO, SAC, etc\nBsuite, DeepMind Control,\nGym, MuJoCo, Pybullet\nBatch simulation and network forward\npass parallelism based on CPUs/TPUs.\nRay RLlib[105]\nBerkeley\n2017\nA2C, A3C, ARS, BC, DDPG, TD3,\nRainbow, ES, APE-X, IMPALA,\nPPO, APPO, DD-PPO, SAC, QMIX,\nVDN, IQN, MADDPG, MCTS, etc.\nGym, PettingZoo, Unity3D,\nGazebo\nSynchronous training with straggler mi-\ngrations, and various baselines including\nevolution-based and model-based algo-\nrithms\nReAgent[185]\nFacebook\n2018\nDQN, Double DQN, Dueling DQN,\nQR-DQN, DDPG, TD3, SAC, etc.\nGym\nWorkflows to perform training with\nlarge-scale data preprocessing, feature\ntransformation, distributed training, etc.\nSURREAL[7]\nStandford\n2018\nDDPG and PPO\nGym, Deepmind Control,\nSurreal Robotics, MuJoCo\nAsynchronous training and cluster au-\ntomation setup on major cloud providers,\ne.g., AWS and Azure.\nPARL[186]\nBaidu\n2019\nDQN, ES, DDPG, PPO, IMPALA,\nA2C, TD3, SAC, MADDPG, CQL,\nQMIX, etc.\nGym\nParallelization of training with thou-\nsands of CPUs and multi-GPUs based on\na centralized architecture.\nrlpyt[71]\nBerkeley\n2019\nA2C, PPO, DQN, DQN variants,\nRainbow, R2D2, Ape-X, DDPG,\nTD3, SAC, etc.\nAtari, Gym\nSynchronous and asynchronous sam-\npling and optimization.\nSEED_RL[77]\nGoogle\n2020\nIMPALA, R2D2, SAC, Vanilla PG,\nPPO, AWR, V-MPO, etc.\nAtari, DeepMind Control,\nGoogle Football, MuJoco\nCentralized inference and a fast commu-\nnication layer based on gRPC.\nACME[187]\nDeepMind\n2020\nD4PG, TD3, SAC, MPO, PPO,\nDMPO, MO-MPO, DQN, IMPALA,\nR2D2, MCTS etc.\nDeepMind Control, Gym\nAsynchronous training with a learner\nprocess and many distributed actor pro-\ncesses and low-level storage system Re-\nverb for experience replay.\nFiber[122]\nUber\n2020\nA3C, PPO, ES, etc.\nALE, Gym, MuJoCo\nStandard multiprocessing API and online\nmigration from multiprocessing on one\nmachine to cluster computing across mul-\ntiple machines.\nSampleFactory[60]\nIntel Lab\n2020\nPPO\nMuJoCo, Atari, VizDoom,\nDeepMind Lab, Megaverse,\nEnvpool, IsaacGym, Brax,\nQuad-Swarm-RL, Hugging\nFace Hub\nAsynchronous training with off-policy\ncorrections on a single-machine setting.\nChainerRL[188]\nPreferred\nNetworks\n2021\nDQN, IQN, Rainbow, REINFORCE,\nA2C, ACER, PCL, DDPG, TRPO,\nPPO, TD3, SAC, etc.\nGym, MuJoCo\nSynchronous and asynchronous training.\nTianshou[189]\nTsinghua\n2022\nREINFORCE, A2C, TRPO, PPO,\nDDPG, TD3, SAC, DQN, Rainbow,\nIQN, BCQ, CQL, CRR, PER, PSRL,\netc.\nGym, PettingZoo\nSynchronous and asynchronous training,\nand standardization support of the train-\ning process.\nTorchRL[190]\nUPF\n2023\nA2C, PPO, DDQN, SAC, REDQ,\nDreamer, Decision transformers,\nRLHF, APPO, DPPO, DDPPO, IM-\nPALA, Ape-X, etc.\nGym, DeepMind Control\nSynchronous and asynchronous training,\nand high modularity that allows flexible\ncomposability for DRL training.\nMindSpore[112]\nICL,\nHuawei\n2023\nDQN, PPO, A2C, A3C, DDPG,\nMAPPO, MADDPG TD3, SAC,\nDouble DQN, etc.\nGym, MuJoCo, MPE, SMAC,\nDMC, PettingZoo, D4RL\nSynchronous and asynchronous training,\nand flexible parallelization using frag-\nmented dataflow graph (FDG).\nSRL[78]\nTsinghua\n2024\nPPO\nGym, Atari, Google Foot-\nball, MuJoCo, Hide and Seek,\nSMAC\nSynchronous training and data batch pre-\nfetching.\nPEARL[127]\nMeta\nRe-\nsearch\n2024\nDQN, DDPG\nGym\nPortable implementations across hetero-\ngeneous platforms including FPGA and\nOptimization on DRL-specific runtime\nscheduling.\nStatistics are collected from the Github links, documents, and papers of corresponding codebases in Aug. 2024.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n25\ndetails of the technologies that have been proposed by researchers to address these challenges. This includes the\nsystem architectures for distributed DRL, the simulation parallelism to increase sample collection efficiency, the\ncomputing parallelism to improve the computational efficiency, the distributed synchronization mechanisms\nfor backpropagation-based training, the deep evolutionary reinforcement learning for evolution-based training.\nFurther, we have summarized the existing libraries and platforms in view of facilitating the research community\nin rapid development. This survey tries to enable readers to have a holistic view of distributed DRL and provides\na guideline for them to get started quickly in this area. Below, we extrapolate potential directions for future work\nin this field.\nSpecialized hardware accelerators. Designing specialized hardware accelerators that are computationally\nefficient with neural networks becomes an urgent and promising direction. It has been witnessed that many\ndomain-specific architectures have been proposed to accelerate DRL training such as NVIDIA Tensor Core[75],\nFPGAs[118], TPUs[77]. Despite the advancements in computation speed, several emerging topics remain in this\nhighly dynamic field of research. One such direction is in-memory computing, which is critical for reducing data\nmovement and minimizing latency for DRL training[120, 191]. Another direction is the exploitation of pipeline\nparallelism according to the DRL training workloads, which requires innovative hardware designs and algorithms\nto ensure all the arrays on the hardware are always active during training [192]. Additionally, how to reduce\nthe energy consumption while improving the computational efficiency is also very important. We believe that\nneuromorphic hardware design is a promising direction, such as spike-based neuromorphic chips [193]. Moreover,\nheterogeneous architectures that combine the strengths of different hardware platforms in the field of DRL can\nbe further investigated[127].\nIn-network distributed aggregation mechanisms. Network communication occupies a large part of the\nexecution time in distributed gradient aggregation. To alleviate the communication overhead, an emerging trend\nis to shift the gradient aggregation process from the worker nodes to the network infrastructure itself, such as at\nthe programmable switches. By doing so, the gradient aggregation is conducted on-the-fly at the granularity of\nnetwork packets rather than gradient vectors stored within the memory of the worker nodes. This approach\nhas the potential to substantially reduce the end-to-end network latency and the volume of data transferred\nacross the network during distributed training. Li et al. [74] were pioneers in leveraging in-switch computing\nto accelerate the distributed training of deep reinforcement learning, achieving significant results. We believe\ndesigning packet forwarding and computing mechanisms according to the characteristics of DRL workloads will\nhave great potential for training improvement.\nEfficient sample exploitation algorithms. A large amount of rollout data is required is one of the known\nissues of DRL training. One reason is counted for the low sample reuse rate. OpenAI researchers demonstrate\nthat the sample reuse rate is lower than 1 in the asynchronous training of the OpenAI Five[113]. Note that simply\ncollecting more data in parallel without effectively exploiting it does not necessarily translate into improved\ntraining outcomes. Therefore, the development of algorithms that can exploit rollout samples more effectively,\nwithout compromising the stability of the learning process, is a crucial direction for future research in distributed\nDRL. To address this challenge, several strategies can be pursued. First, more sophisticated replay mechanisms,\nincluding Priority-refresh[114], asynchronous curriculum experience replay[194], and regret minimization\nreplay[195], are promising techniques to better utilize the samples. Besides, driving DRL agents to perform\nsearches sophisticatedly rather than randomly is helping for generating more useful samples. Exploration\nstrategies such as curiosity-driven [196], diversity-driven [197], and novelty search [142] in large state-action\nspaces are potential directions of importance.\nLarge language model-enhanced DRL. Large language models (LLMs), equipped with extensive pre-\ntrained knowledge and powerful generalization abilities, are emerging as a promising direction to enhance the\nperformance of DRL, including accelerating the training process. Specifically, in the DRL training paradigm, LLMs\ncan assist DRL agents in reward function design, action selection, and policy evaluation, based on the modeling\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n26\n•\nLiu et al.\ncapability and common-sense pre-trained knowledge. In this way, LLMs not only enable a considerable level of\nability at the beginning of the training process, but also facilitate the decision-making during the optimization.\nCurrently, numerous pioneering works [198–200] have been proposed that I am confident will pave the way for\nrobust development in the coming future.\n10\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science Foundation of China under grant U21A20518, 62025208\nand 62421002.\nREFERENCES\n[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller,\nAndreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533,\n2015.\n[2] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis\nAntonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,\nTimothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep\nneural networks and tree search. Nature, 529(7587):484–489, 2016.\n[3] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre,\nDharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through\nself-play. Science, 362(6419):1140–1144, 2018.\n[4] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard\nPowell, Timo Ewalds, Petko Georgiev, and Others. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature,\n575(7782):350–354, 2019.\n[5] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Müller, Vladlen Koltun, and Davide Scaramuzza. Champion-level\ndrone racing using deep reinforcement learning. Nature, 620(7976):982–987, 2023.\n[6] Open AI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Józefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias\nPlappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba.\nLearning dexterous in-hand manipulation. International Journal of Robotics Research, 39(1):3–20, 2020.\n[7] Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan Creus-Costa, Silvio Savarese, and Li Fei-Fei. Surreal:\nOpen-source reinforcement learning framework and robot manipulation benchmark. In Conference on Robot Learning, pages 767–782.\nPMLR, 2018.\n[8] Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-fei. Embodied intelligence via learning and evolution. Nature Communications,\n(2021).\n[9] Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science advances,\n4(7):eaap7885, 2018.\n[10] Adam Yala, Peter G Mikhael, Constance Lehman, Gigin Lin, Fredrik Strand, Yung-Liang Wan, Kevin Hughes, Siddharth Satuluru,\nThomas Kim, Imon Banerjee, et al. Optimizing risk-based breast cancer screening policies with reinforcement learning. Nature Medicine,\n28(1):136–143, 2022.\n[11] Suchi Saria. Individualized sepsis treatment using reinforcement learning. Nature medicine, 24(11):1641–1642, 2018.\n[12] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that\nmatters. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n[13] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep reinforcement learning: A brief survey.\nIEEE Signal Processing Magazine, 34(6):26–38, 2017.\n[14] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.\n[15] Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao. Deep reinforcement learning: A\nsurvey. IEEE Transactions on Neural Networks and Learning Systems, pages 1–15, 2022.\n[16] Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based reinforcement learning: A survey. Foundations\nand Trends® in Machine Learning, 16(1):1–118, 2023.\n[17] B. Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A.Al Sallab, Senthil Yogamani, and Patrick Perez. Deep\nReinforcement Learning for Autonomous Driving: A Survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909–4926,\n2022.\n[18] Thanh Thi Nguyen and Vijay Janapa Reddi. Deep Reinforcement Learning for Cyber Security. IEEE Transactions on Neural Networks\nand Learning Systems, (Ml), 2021.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n27\n[19] Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying Chang Liang, and Dong In Kim. Applications\nof Deep Reinforcement Learning in Communications and Networking: A Survey. IEEE Communications Surveys and Tutorials,\n21(4):3133–3174, 2019.\n[20] Ammar Haydari and Yasin Yilmaz. Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey. IEEE Transactions\non Intelligent Transportation Systems, 23(1):11–32, 2022.\n[21] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep Reinforcement Learning for Multiagent Systems: A Review of\nChallenges, Solutions, and Applications. IEEE Transactions on Cybernetics, 50(9):3826–3839, 2020.\n[22] Wuhui Chen, Xiaoyu Qiu, Ting Cai, Hong-Ning Dai, Zibin Zheng, and Yan Zhang. Deep reinforcement learning for internet of things:\nA comprehensive survey. IEEE Communications Surveys & Tutorials, 23(3):1659–1692, 2021.\n[23] Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim Verbelen, and Jan S Rellermeyer. A survey on distributed\nmachine learning. Acm computing surveys (csur), 53(2):1–33, 2020.\n[24] Diego Peteiro-Barral and Bertha Guijarro-Berdiñas. A survey of methods for distributed machine learning. Progress in Artificial\nIntelligence, 2:1–11, 2013.\n[25] Meng Wang, Weijie Fu, Xiangnan He, Shijie Hao, and Xindong Wu. A survey on large-scale machine learning. IEEE Transactions on\nKnowledge and Data Engineering, 34(6):2574–2594, 2020.\n[26] Ruben Mayer and Hans Arno Jacobsen. Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools. ACM\nComputing Surveys, 53(1), 2020.\n[27] Tal Ben-Nun and Torsten Hoefler. Demystifying parallel and distributed deep learning: An in-depth concurrency analysis. ACM\nComputing Surveys, 52(4), 2019.\n[28] Karanbir Singh Chahal, Manraj Singh Grover, Kuntal Dey, and Rajiv Ratn Shah. A Hitchhiker’s Guide On Distributed Training Of\nDeep Neural Networks. Journal of Parallel and Distributed Computing, 137:65–76, 2020.\n[29] Mohammad Reza Samsami and Hossein Alimadad.\nDistributed Deep Reinforcement Learning: An Overview.\narXiv preprint\narXiv:2011.11012, pages 1–15, 2020.\n[30] Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Wancheng Ni, Kaiqi Huang, Bin Liang, and Liang Wang. Distributed\ndeep reinforcement learning: A survey and a multi-player multi-agent learning toolbox. Machine Intelligence Research, 21(3):411–430,\n2024.\n[31] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning\nvia meta-policy optimization. In Conference on Robot Learning, pages 617–629. PMLR, 2018.\n[32] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. arXiv\npreprint arXiv:1802.10592, 2018.\n[33] Richard S Sutton. Reinforcement learning: an introduction. A Bradford Book, 2018.\n[34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv\npreprint arXiv:1707.06347, 2017.\n[35] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement\nlearning with a stochastic actor. In International Conference on Machine Learning, pages 1861–1870. PMLR, 2018.\n[36] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International\nConference on Machine Learning, pages 1587–1596. PMLR, 2018.\n[37] Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep q-learning algorithms. In International\nConference on Machine Learning, pages 2021–2030. PMLR, 2019.\n[38] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error\nreduction. In NeurIPS, volume 32, 2019.\n[39] Cheng-Tao Chu, Sang Kim, Yi-An Lin, YuanYuan Yu, Gary Bradski, Kunle Olukotun, and Andrew Ng. Map-reduce for machine learning\non multicore. In NeurIPS, volume 19, 2006.\n[40] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient descent. In NeurIPS, volume 23, 2010.\n[41] Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed machine learning with the parameter\nserver. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, NeurIPS, volume 27. Curran Associates,\nInc., 2014.\n[42] Jiawei Jiang, Bin Cui, Ce Zhang, and Lele Yu. Heterogeneity-aware distributed parameter servers. In Proceedings of the ACM SIGMOD,\npage 463–478, New York, NY, USA, 2017. Association for Computing Machinery.\n[43] Hao Zhang, Yuan Li, Zhijie Deng, Xiaodan Liang, Lawrence Carin, and Eric Xing. Autosync: Learning to synchronize for data-parallel\ndistributed deep learning. In NeurIPS, volume 33, pages 906–917, 2020.\n[44] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker,\nKe Yang, et al. Large scale distributed deep networks. In NeurIPS, volume 25, 2012.\n[45] Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A Gibson, and Eric P Xing. On model parallelization and scheduling\nstrategies for distributed machine learning. In NeurIPS, volume 27, 2014.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n28\n•\nLiu et al.\n[46] Arpan Jain, Ammar Ahmad Awan, Asmaa M Aljuhani, Jahanzeb Maqbool Hashmi, Quentin G Anthony, Hari Subramoni, Dhableswar K\nPanda, Raghu Machiraju, and Anil Parwani. Gems: Gpu-enabled memory-aware model-parallelism system for distributed dnn training.\nIn SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–15. IEEE, 2020.\n[47] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand,\nPrethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using\nmegatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages\n1–15, 2021.\n[48] Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, and Bo Li. Communication-Efficient Distributed Deep Learning: A Compre-\nhensive Survey. arXiv preprint arXiv:2003.06307, (1):1–23, 2020.\n[49] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hyouk Joong Lee, Jiquan Ngiam, Quoc V. Le,\nYonghui Wu, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline parallelism. In NeurIPS, volume 32,\n2019.\n[50] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and\nMatei Zaharia. Pipedream: Generalized pipeline parallelism for DNN training. SOSP 2019 - Proceedings of the 27th ACM Symposium on\nOperating Systems Principles, pages 1–15, 2019.\n[51] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel dnn training. In\nInternational Conference on Machine Learning, pages 7937–7947. PMLR, 2021.\n[52] Ziyue Luo, Xiaodong Yi, Guoping Long, Shiqing Fan, Chuan Wu, Jun Yang, and Wei Lin. Efficient Pipeline Planning for Expedited\nDistributed DNN Training. (Ml):340–349, 2022.\n[53] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014.\n[54] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol\nVinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s Neural Machine Translation System: Bridging the Gap between\nHuman and Machine Translation. arXiv preprint arXiv:1609.08144, pages 1–23, 2016.\n[55] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee,\nMingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, NeurIPS, volume 31. Curran Associates, Inc., 2018.\n[56] Zhihao Jia, Matei Zaharia, and Alex Aiken. BEYOND DATA AND MODEL PARALLELISM FOR DEEP NEURAL NETWORKS. 2019.\n[57] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter\nmodels. International Conference for High Performance Computing, Networking, Storage and Analysis, SC, 2020-November:3505–3506,\n2020.\n[58] Jay H Park, Gyeongchan Yun, M Yi Chang, Nguyen T Nguyen, Seungmin Lee, Jaesik Choi, Sam H Noh, and Young-ri Choi. {HetPipe}:\nEnabling large {DNN} training on (whimpy) heterogeneous {GPU} clusters through integration of pipelined model parallelism and\ndata parallelism. In 2020 USENIX Annual Technical Conference (USENIX ATC 20), pages 307–321, 2020.\n[59] Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen Huang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang You. Colossal-AI: A\nUnified Deep Learning System For Large-Scale Parallel Training. arXiv preprint arXiv:2110.14883, 2021.\n[60] Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav Sukhatme, and Vladlen Koltun. Sample factory: Egocentric 3D control from\npixels at 100000 FPS with asynchronous reinforcement learning. In International Conference on Machine Learning, volume PartF16814,\npages 7608–7618. PMLR, 2020.\n[61] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Boron Yotam, Firoiu Vlad, Harley Tim, Iain\nDunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner\nArchitectures. In International Conference on Machine Learning, volume 4, pages 2263–2284. PMLR, 2018.\n[62] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur\nAllshire, Ankur Handa, and Gavriel State. Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning. arXiv\npreprint arXiv:2108.10470, 2021.\n[63] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from\nmulti-agent autocurricula. In International Conference on Learning Representations, 2020.\n[64] Raju Machupalli, Masum Hossain, and Mrinal Mandal. Review of asic accelerators for deep neural network. Microprocessors and\nMicrosystems, 89:104441, 2022.\n[65] Tianyi Chen, Kaiqing Zhang, Georgios B. Giannakis, and Tamer Basar. Communication-Efficient Policy Gradient Methods for\nDistributed Reinforcement Learning. IEEE Transactions on Control of Network Systems, pages 1–14, 2021.\n[66] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic\nalgorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567,\n2017.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n29\n[67] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed\nprioritized experience replay. In International Conference on Learning Representations, pages 1–19, 2018.\n[68] Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed\nreinforcement learning. In International Conference on Learning Representations, 2018.\n[69] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa\nSuleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, and David Silver. Massively Parallel\nMethods for Deep Reinforcement Learning. arXiv preprint arXiv:1507.04296, 2015.\n[70] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages\n1928–1937. PMLR, 2016.\n[71] Adam Stooke and Pieter Abbeel. rlpyt : A Research Code Base for Deep Reinforcement Learning in PyTorch. pages 1–12.\n[72] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. DD-PPO: Learning\nNear-Perfect PointGoal Navigators from 2.5 Billion Frames. In International Conference on Learning Representations, pages 1–21, 2020.\n[73] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica.\nRllib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning, pages 3053–3062. PMLR,\n2018.\n[74] Youjie Li, Iou Jen Liu, Yifan Yuan, Deming Chen, Alexander Schwing, and Jian Huang. Accelerating distributed reinforcement learning\nwith in-switch computing. Proceedings - International Symposium on Computer Architecture, pages 279–291, 2019.\n[75] Adam Stooke and Pieter Abbeel. Accelerated Methods for Deep Reinforcement Learning. arXiv preprint arXiv:1803.02811, 2018.\n[76] Amir Yazdanbakhsh, Junchao Chen, and Yu Zheng. Menger: Massively large-scale distributed reinforcement learning. 2020.\n[77] Lasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski. Seed rl: Scalable and efficient deep-rl with accelerated\ncentral inference. In International Conference on Learning Representations, 2020.\n[78] Zhiyu Mei, Wei Fu, Guangju Wang, Huanchen Zhang, and Yi Wu. Srl: Scaling distributed reinforcement learning to over ten thousand\ncores. In International Conference on Learning Representations, 2024.\n[79] Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, and Pulkit Agrawal. Parallel 𝑞-learning: Scaling off-policy reinforcement learning\nunder massively parallel simulation. In International Conference on Machine Learning, pages 19440–19459. PMLR, 2023.\n[80] Yunzhi Zhang, Ignasi Clavera, Boren Tsai, and Pieter Abbeel. Asynchronous methods for model-based reinforcement learning. In\nConference on Robot Learning, pages 1338–1347. PMLR, 2020.\n[81] Mahmoud Assran, Joshua Romoff, Nicolas Ballas, Joelle Pineau, and Michael Rabbat. Gossip-based actor-learner architectures for deep\nreinforcement learning. In NeurIPS, volume 32, 2019.\n[82] Adwaitvedant Mathkar and Vivek S. Borkar. Distributed Reinforcement Learning via Gossip. IEEE Transactions on Automatic Control,\n62(3):1465–1470, 2017.\n[83] Xingyu Sha, Jiaqi Zhang, Keyou You, Kaiqing Zhang, and Tamer Başar. Fully asynchronous policy evaluation in distributed reinforcement\nlearning over networks. Automatica, 136:110092, 2022.\n[84] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad\nAzar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. 32nd AAAI Conference on Artificial\nIntelligence, AAAI 2018, pages 3215–3222, 2018.\n[85] Audr¯unas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc G. Bellemare, and Rémi Munos. The reactor: A fast and\nsample-efficient actor-critic agent for reinforcement learning. In International Conference on Learning Representations, pages 1–18, 2018.\n[86] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International\nConference on Machine Learning, pages 1889–1897. PMLR, 2015.\n[87] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.\nContinuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n[88] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv\npreprint arXiv:1707.06347, pages 1–12, 2017.\n[89] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta,\nPieter Abbeel, and Sergey Levine. Soft Actor-Critic Algorithms and Applications. arXiv preprint arXiv:1812.05905, 2018.\n[90] Yuandong Tian, Qucheng Gong, Wenling Shang, Yuxin Wu, and C. Lawrence Zitnick. ELF: An extensive, lightweight and flexible\nresearch platform for real-time strategy games. In NeurIPS, number Nips, pages 2660–2670, 2017.\n[91] C Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax–a differentiable physics engine\nfor large scale rigid body simulation. arXiv preprint arXiv:2106.13281, 2021.\n[92] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel, Monia Ghobadi, Amin Vahdat, Yaogong Wang, David\nWetherall, and David Zats. Timely: Rtt-based congestion control for the datacenter. ACM SIGCOMM Computer Communication Review,\n45(4):537–550, 2015.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n30\n•\nLiu et al.\n[93] Zhihong Liu, Qi Zhang, Reaz Ahmed, Raouf Boutaba, Yaping Liu, and Zhenghu Gong. Dynamic resource allocation for mapreduce\nwith partitioning skew. IEEE Transactions on Computers, 65(11):3304–3317, 2016.\n[94] Steven Dalton and Iuri Frosio. Accelerating reinforcement learning through GPU Atari emulation. In NeurIPS, 2020.\n[95] Jing Guo, Zihao Chang, Sa Wang, Haiyang Ding, Yihui Feng, Liang Mao, and Yungang Bao. Who limits the resource efficiency of my\ndatacenter: An analysis of alibaba datacenter traces. In Proceedings of the International Symposium on Quality of Service, pages 1–10,\n2019.\n[96] Jacky Liang, Viktor Makoviychuk, Ankur Handa, Nuttapong Chentanez, Miles Macklin, and Dieter Fox. GPU-Accelerated Robotic\nSimulation for Distributed Reinforcement Learning. arXiv preprint arXiv:1810.05762, (CoRL):1–14, 2018.\n[97] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement\nlearning. arXiv preprint arXiv:1703.03864, 2017.\n[98] Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis Savva, Dhruv Batra, Vladlen Koltun, and Kayvon Fatahalian. Large batch\nsimulation for deep reinforcement learning. In International Conference on Learning Representations, 2021.\n[99] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based\ncharacter control. ACM Transactions on Graphics (TOG), 40(4):1–20, 2021.\n[100] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax – A Differentiable Physics\nEngine for Large Scale Rigid Body Simulation. arXiv preprint arXiv:2106.13281, 2021.\n[101] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement\nlearning. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, Proceedings of the 5th Conference on Robot Learning, volume\n164 of Proceedings of Machine Learning Research, pages 91–100. PMLR, 08–11 Nov 2022.\n[102] Antonio Loquercio, Elia Kaufmann, René Ranftl, Matthias Müller, Vladlen Koltun, and Davide Scaramuzza. Learning high-speed flight\nin the wild. Science Robotics, 6(59):eabg5810, 2021.\n[103] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107–113,\n2008.\n[104] Yuxi Li and Dale Schuurmans. Mapreduce for parallel reinforcement learning. In European Workshop on Reinforcement Learning, pages\n309–320. Springer, 2011.\n[105] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Joseph Gonzalez, Ken Goldberg, and Ion Stoica. Ray rllib: A\ncomposable and scalable reinforcement learning library. arXiv preprint arXiv:1712.09381, 85, 2017.\n[106] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive approach to reinforcement learning.\narXiv preprint arXiv:1803.07055, pages 1–22, 2018.\n[107] Igor Adamski, Robert Adamski, Tomasz Grel, Adam Jędrych, Kamil Kaczmarek, and Henryk Michalewski. Distributed deep reinforcement\nlearning: Learn how to play atari games in 21 minutes. In ISC High Performance 2018, Frankfurt, Germany, pages 370–388. Springer,\n2018.\n[108] Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami,\nMartin Riedmiller, and David Silver. Emergence of Locomotion Behaviours in Rich Environments. arXiv preprint arXiv:1707.02286, 2017.\n[109] P Olicy G Radients, Gabriel Barth-maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal,\nNicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. In International Conference on Learning\nRepresentations, pages 1–16.\n[110] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz. Reinforcement learning through asynchronous\nadvantage actor-critic on a GPU. In International Conference on Learning Representations, pages 1–12, 2017.\n[111] Alfredo V Clemente, Humberto N Castejón, and Arjun Chandra. Efficient parallel methods for deep reinforcement learning. arXiv\npreprint arXiv:1705.04862, 2017.\n[112] Huanzhou Zhu, Bo Zhao, Gang Chen, Weifeng Chen, Yijie Chen, Liang Shi, Yaodong Yang, Peter Pietzuch, and Lei Chen. {MSRL}:\nDistributed reinforcement learning with dataflow fragments. In 2023 USENIX Annual Technical Conference (USENIX ATC 23), pages\n977–993, 2023.\n[113] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer,\nShariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n[114] Yixuan Mei, Jiaxuan Gao, Weirui Ye, Shaohuai Liu, Yang Gao, and Yi Wu. Speedyzero: Mastering atari with limited data and time. In\nInternational Conference on Learning Representations, 2023.\n[115] Jiang Su, Jianxiong Liu, David B. Thomas, and Peter Y.K. Cheung. Neural Network Based Reinforcement Learning Acceleration on\nFPGA Platforms. ACM SIGARCH Computer Architecture News, 44(4):68–73, 2017.\n[116] Shengjia Shao and Wayne Luk. Customised pearlmutter propagation: A hardware architecture for trust region policy optimisation.\n2017 27th International Conference on Field Programmable Logic and Applications, FPL 2017, 2017.\n[117] Ce Guo, Wayne Luk, Stanley Qing Shui Loh, Alexander Warren, and Joshua Levine. Customisable control policy learning for robotics.\nProceedings of the International Conference on Application-Specific Systems, Architectures and Processors, 2019-July:91–98, 2019.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n31\n[118] Hyungmin Cho, Pyeongseok Oh, Jiyoung Park, Wookeun Jung, and Jaejin Lee. FA3C: FPGA-Accelerated Deep Reinforcement Learning.\nInternational Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS, pages 499–513, 2019.\n[119] Yuan Meng, Sanmukh Kuppannagari, and Viktor Prasanna. Accelerating Proximal Policy Optimization on CPU-FPGA Heterogeneous\nPlatforms. Proceedings - 28th IEEE International Symposium on Field-Programmable Custom Computing Machines, FCCM 2020, pages\n19–27, 2020.\n[120] Yuan Meng, Chi Zhang, and Viktor Prasanna. FPGA acceleration of deep reinforcement learning using on-chip replay management.\npages 40–48, 2022.\n[121] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury\nSulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia\nHadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A Generalist Agent. arXiv preprint arXiv:2205.06175, pages 1–40, 2022.\n[122] Jiale Zhi, Rui Wang, Jeff Clune, and Kenneth O. Stanley. Fiber: A Platform for Efficient Development and Distributed Training for\nReinforcement Learning and Population-Based Methods. arXiv preprint arXiv:2003.11164, (Oliphant 2006), 2020.\n[123] Lucileide MD Da Silva, Matheus F Torquato, and Marcelo AC Fernandes. Parallel implementation of reinforcement learning q-learning\ntechnique for fpga. IEEE Access, 7:2782–2798, 2018.\n[124] Rachit Rajat, Yuan Meng, Sanmukh Kuppannagari, Ajitesh Srivastava, Viktor Prasanna, and Rajgopal Kannan. Qtaccel: A generic fpga\nbased design for q-table based reinforcement learning accelerators. In Proceedings of the 2020 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays, pages 323–323, 2020.\n[125] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William\nPaul, Michael I. Jordan, and Ion Stoica. Ray: a distributed framework for emerging ai applications. In OSDI’18: Proceedings of the 13th\nUSENIX conference on Operating Systems Design and Implementation, 2018.\n[126] Hanfei Yu, Jian Li, Yang Hua, Xu Yuan, and Hao Wang. Cheaper and faster: Distributed deep reinforcement learning with serverless\ncomputing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 16539–16547, 2024.\n[127] Yuan Meng, Michael Kinsner, Deshanand Singh, Mahesh Iyer, and Viktor Prasanna. Pearl: Enabling portable, productive, and high-\nperformance deep reinforcement learning using heterogeneous platforms. In Proceedings of the 21st ACM International Conference on\nComputing Frontiers, pages 41–50, 2024.\n[128] Albert Bou and Gianni De Fabritiis. PyTorchRL: Modular and Distributed Reinforcement Learning in PyTorch. arXiv preprint\narXiv:2007.02622, 2020.\n[129] Xinghao Pan, Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting Distributed Synchronous SGD. arXiv preprint\narXiv:1702.05800, pages 1–10, 2017.\n[130] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In International\nConference on Machine Learning, volume 7, pages 4745–4767. PMLR, 2018.\n[131] Qirong Ho, James Cipar, Henggang Cui, Jin Kyu Kim, Seunghak Lee, Phillip B. Gibbons, Garth A. Gibson, Gregory R. Ganger, and\nEric P. Xing. More effective distributed ML via a stale synchronous parallel parameter server. In NeurIPS, number 1, pages 1–9, 2013.\n[132] Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam Sung Kim, and Alexander Schwing. PIPE-SGD: A decentralized pipelined\nSGD framework for distributed deep net training. In NeurIPS, volume 2018-Decem, pages 8045–8056, 2018.\n[133] Alfredo V. Clemente, Humberto N. Castejón, and Arjun Chandra. Efficient Parallel Methods for Deep Reinforcement Learning. arXiv\npreprint arXiv:1705.04862, pages 1–9, 2017.\n[134] Charles Reiss, Alexey Tumanov, Gregory R Ganger, Randy H Katz, and Michael A Kozuch. Heterogeneity and dynamicity of clouds at\nscale: Google trace analysis. In Proceedings of the third ACM symposium on cloud computing, pages 1–13, 2012.\n[135] Iou-Jen Liu, Raymond Yeh, and Alexander Schwing. High-throughput synchronous deep rl. Advances in Neural Information Processing\nSystems, 33:17070–17080, 2020.\n[136] Xing Zhao, Aijun An, Junfeng Liu, and Bao Xin Chen. Dynamic stale synchronous parallel distributed training for deep learning. In\n2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), pages 1507–1517. IEEE, 2019.\n[137] Haifeng Sun, Zhiyi Gui, Song Guo, Qi Qi, Jingyu Wang, and Jianxi Liao. GSSP: Eliminating Stragglers through Grouping Synchronous\nfor Distributed Deep Learning in Heterogeneous Cluster. IEEE Transactions on Cloud Computing, 14(8), 2021.\n[138] Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience replay. In International Conference\non Machine Learning, pages 8545–8554. PMLR, 2020.\n[139] Alexandre Borges and Arlindo Oliveira. Combining off and on-policy training in model-based reinforcement learning. arXiv preprint\narXiv:2102.12194, 2021.\n[140] Yunhao Tang. Guiding evolutionary strategies with off-policy actor-critic. In Proceedings of the 20th International Conference on\nAutonomous Agents and MultiAgent Systems, pages 1317–1325, 2021.\n[141] Edoardo Conti, Joel Lehman, and Kenneth O Stanley. Improving Exploration in Evolution Strategies for Deep Reinforcement Learning\nvia a Population of Novelty-Seeking Agents. (NeurIPS), 2018.\n[142] Kenneth O Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks through neuroevolution. Nature\nMachine Intelligence, 1(1):24–35, 2019.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n32\n•\nLiu et al.\n[143] Shauharda Khadka. Evolution-Guided Policy Gradient in Reinforcement Learning. (November), 2019.\n[144] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen Schmidhuber. Natural evolution strategies. The Journal\nof Machine Learning Research, 15(1):949–980, 2014.\n[145] Verena Heidrich-Meisner and Christian Igel. Uncertainty handling cma-es for reinforcement learning. In Proceedings of the 11th Annual\nconference on Genetic and evolutionary computation, pages 1211–1218, 2009.\n[146] Manoj Kumar, Dr Husain, Naveen Upreti, Deepti Gupta, et al. Genetic algorithm: Review and application. Available at SSRN 3529843,\n2010.\n[147] Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary computation,\n10(2):99–127, 2002.\n[148] Kenneth O Stanley and Risto Miikkulainen. Efficient reinforcement learning through evolving neural network topologies. In Proceedings\nof the 4th Annual Conference on genetic and evolutionary computation, pages 569–577, 2002.\n[149] Kenneth O Stanley, David B D’Ambrosio, and Jason Gauci. A hypercube-based encoding for evolving large-scale neural networks.\nArtificial life, 15(2):185–212, 2009.\n[150] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International Conference on Learning\nRepresentations, 2017.\n[151] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale\nevolution of image classifiers. In International Conference on Machine Learning, pages 2902–2911. PMLR, 2017.\n[152] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In\nProceedings of the aaai conference on artificial intelligence, volume 33, pages 4780–4789, 2019.\n[153] Gustavo-Adolfo Vargas-Hakim, Efren Mezura-Montes, and Hector-Gabriel Acosta-Mesa. A review on convolutional neural network\nencodings for neuroevolution. IEEE Transactions on Evolutionary Computation, 26(1):12–27, 2021.\n[154] Alejandro Baldominos, Yago Saez, and Pedro Isasi. Evolutionary convolutional neural networks: An application to handwriting\nrecognition. Neurocomputing, 283:38–52, 2018.\n[155] Miao Zhang, Huiqi Li, Shirui Pan, Juan Lyu, Steve Ling, and Steven Su. Convolutional neural networks-based lung nodule classification:\nA surrogate-assisted evolutionary algorithm for hyperparameter optimization. IEEE Transactions on Evolutionary Computation,\n25(5):869–882, 2021.\n[156] Tanmay Gangwani and Jian Peng. Policy optimization by genetic distillation. In International Conference on Learning Representations,\n2018.\n[157] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor,\nYuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.\n[158] Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like animals. Nature, 521(7553):503–507,\n2015.\n[159] Justin K Pugh, Lisa B Soros, and Kenneth O Stanley. Quality diversity: A new frontier for evolutionary computation. Frontiers in\nRobotics and AI, page 40, 2016.\n[160] Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, and Jeff Clune. Improving exploration in\nevolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In NeurIPS, volume 31, 2018.\n[161] Ethan C Jackson and Mark Daley. Novelty search for deep reinforcement learning policy network weights by action sequence edit\nmetric distance. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pages 173–174, 2019.\n[162] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and\nMarcin Andrychowicz. Parameter space noise for exploration. In International Conference on Learning Representations, 2018.\n[163] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis\nHassabis, Olivier Pietquin, et al. Noisy networks for exploration. In International Conference on Learning Representations, 2018.\n[164] Francisco E Fernandes Jr and Gary G Yen. Pruning deep convolutional neural networks architectures with evolution strategy. Information\nSciences, 552:29–47, 2021.\n[165] Gustavo Adolfo Vargas-Hákim, Efrén Mezura-Montes, and Héctor Gabriel Acosta-Mesa. A Review on Convolutional Neural Network\nEncodings for Neuroevolution. IEEE Transactions on Evolutionary Computation, 26(1):12–27, 2022.\n[166] Filipe Assunção, Nuno Lourenço, Bernardete Ribeiro, and Penousal Machado. Incremental evolution and development of deep artificial\nneural networks. In European Conference on Genetic Programming (Part of EvoStar), pages 35–51. Springer, 2020.\n[167] Francisco Erivaldo Fernandes Junior and Gary G Yen. Particle swarm optimization of deep neural networks architectures for image\nclassification. Swarm and Evolutionary Computation, 49:62–74, 2019.\n[168] Ye-Qun Wang, Chun-Hua Chen, Jun Zhang, and Zhi-Hui Zhan. Dropout topology-assisted bidirectional learning particle swarm\noptimization for neural architecture search. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pages\n93–96, 2022.\n[169] Ashraf Darwish, Aboul Ella Hassanien, and Swagatam Das. A survey of swarm and evolutionary computing approaches for deep\nlearning. Artificial intelligence review, 53(3):1767–1812, 2020.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\nAcceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey\n•\n33\n[170] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning\nRepresentations, 2018.\n[171] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In Uncertainty in artificial intelligence,\npages 367–377. PMLR, 2020.\n[172] Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and Steven Su. Overcoming multi-model forgetting in one-shot nas with diversity\nmaximization. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 7809–7818, 2020.\n[173] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian\noptimisation and optimal transport. In NeurIPS, volume 31, 2018.\n[174] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin\nMurphy. Progressive neural architecture search. In Proceedings of the European conference on computer vision (ECCV), pages 19–34,\n2018.\n[175] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International\nConference on Learning Representations, 2015.\n[176] Yongchun Zhu, Fuzhen Zhuang, Jindong Wang, Guolin Ke, Jingwu Chen, Jiang Bian, Hui Xiong, and Qing He. Deep subdomain\nadaptation network for image classification. IEEE Transactions on neural networks and learning systems, 32(4):1713–1722, 2020.\n[177] Kuan-Hung Shih, Ching-Te Chiu, Jiou-Ai Lin, and Yen-Yu Bu. Real-time object detection with reduced region proposal network via\nmulti-feature concatenation. IEEE Transactions on neural networks and learning systems, 31(6):2164–2173, 2019.\n[178] Dingwen Zhang, Junwei Han, Long Zhao, and Tao Zhao. From discriminant to complete: Reinforcement searching-agent learning for\nweakly supervised object detection. IEEE Transactions on Neural Networks and Learning Systems, 31(12):5549–5560, 2020.\n[179] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv\npreprint arXiv:1609.08144, 2016.\n[180] Biao Zhang, Deyi Xiong, Jun Xie, and Jinsong Su. Neural machine translation with gru-gated attention model. IEEE Transactions on\nneural networks and learning systems, 31(11):4688–4698, 2020.\n[181] Jason Liang, Elliot Meyerson, and Risto Miikkulainen. Evolutionary architecture search for deep multitask networks. In Proceedings of\nthe Genetic and Evolutionary Computation Conference, pages 466–473, 2018.\n[182] Eunwoo Kim, Chanho Ahn, and Songhwai Oh. Auto-virtualnet: Cost-adaptive dynamic architecture search for multi-task learning.\nNeurocomputing, 442:116–124, 2021.\n[183] Alexis Asseman, Nicolas Antoine, and Ahmet S. Ozcan. Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement\nLearning Problems. ACM Journal on Emerging Technologies in Computing Systems, 17(2), 2021.\n[184] Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina,\nNeal Wu, Efi Kokiopoulou, Luciano Sbaiz, Jamie Smith, Gábor Bartók, Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eugene\nBrevdo. TF-Agents: A library for reinforcement learning in tensorflow. https://github.com/tensorflow/agents, 2018. [Online; accessed\n25-June-2019].\n[185] Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary Kaden, Vivek Narayanan, Xiaohui Ye, Zhengxing\nChen, and Scott Fujimoto. Horizon : Facebook ’ s Open Source Applied Reinforcement Learning Platform. 2019.\n[186] Baidu. A flexible, distributed and object-oriented programming reinforcement learning framework. 2022.\n[187] Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin\nCassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le\nPaine, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A Research Framework for Distributed Reinforcement\nLearning. arXiv preprint arXiv:2006.00979, pages 1–33, 2020.\n[188] Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. Chainerrl: A deep reinforcement learning library. Journal\nof Machine Learning Research, 22(77):1–14, 2021.\n[189] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly\nmodularized deep reinforcement learning library. Journal of Machine Learning Research, 23(267):1–6, 2022.\n[190] Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis, and Vincent Moens.\nTorchrl: A data-driven decision-making library for pytorch, 2023.\n[191] Abu Sebastian, Manuel Le Gallo, Riduan Khaddam-Aljameh, and Evangelos Eleftheriou. Memory devices and applications for in-memory\ncomputing. Nature nanotechnology, 15(7):529–544, 2020.\n[192] Tianqi Wang, Tong Geng, Ang Li, Xi Jin, and Martin Herbordt. Fpdeep: Scalable acceleration of cnn training on deeply-pipelined fpga\nclusters. IEEE Transactions on Computers, 69(8):1143–1158, 2020.\n[193] Arjun Rao, Philipp Plank, Andreas Wild, and Wolfgang Maass. A long short-term memory for ai applications in spike-based\nneuromorphic hardware. Nature Machine Intelligence, 4(5):467–479, 2022.\n[194] Zijian Hu, Xiaoguang Gao, Kaifang Wan, Qianglong Wang, and Yiwei Zhai. Asynchronous curriculum experience replay: A deep\nreinforcement learning approach for uav autonomous motion control in unknown dynamic environments. IEEE Transactions on\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n34\n•\nLiu et al.\nVehicular Technology, 72(11):13985–14001, 2023.\n[195] Xu-Hui Liu, Zhenghai Xue, Jingcheng Pang, Shengyi Jiang, Feng Xu, and Yang Yu. Regret minimization experience replay in off-policy\nreinforcement learning. Advances in neural information processing systems, 34:17604–17615, 2021.\n[196] Clemens Schwarke, Victor Klemm, Matthijs van der Boon, Marko Bjelonic, and Marco Hutter. Curiosity-driven learning for joint\nlocomotion and manipulation tasks. In 7th Annual Conference on Robot Learning, 2023.\n[197] Shuang Wu, Jian Yao, Haobo Fu, Ye Tian, Chao Qian, Yaodong Yang, Qiang Fu, and Yang Wei. Quality-similar diversity via population\nbased reinforcement learning. In International Conference on Learning Representations, 2022.\n[198] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli,\nTom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022.\n[199] Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, and Stefan Wermter. Accelerating reinforcement learning of robotic manipulations\nvia feedback from large language models. arXiv preprint arXiv:2311.02379, 2023.\n[200] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu, Gaoqi Liang, Junhua Zhao, and Yun Li. Survey on large language\nmodel-enhanced reinforcement learning: Concept, taxonomy, and methods. arXiv preprint arXiv:2404.00282, 2024.\n, Vol. 1, No. 1, Article . Publication date: November 2023.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.DC"
  ],
  "published": "2024-11-08",
  "updated": "2024-11-08"
}