{
  "id": "http://arxiv.org/abs/2411.09160v1",
  "title": "Rationality based Innate-Values-driven Reinforcement Learning",
  "authors": [
    "Qin Yang"
  ],
  "abstract": "Innate values describe agents' intrinsic motivations, which reflect their\ninherent interests and preferences to pursue goals and drive them to develop\ndiverse skills satisfying their various needs. The essence of reinforcement\nlearning (RL) is learning from interaction based on reward-driven behaviors,\nmuch like natural agents. It is an excellent model to describe the\ninnate-values-driven (IV) behaviors of AI agents. Especially developing the\nawareness of the AI agent through balancing internal and external utilities\nbased on its needs in different tasks is a crucial problem for individuals\nlearning to support AI agents integrating human society with safety and harmony\nin the long term. This paper proposes a hierarchical compound intrinsic value\nreinforcement learning model -- innate-values-driven reinforcement learning\ntermed IVRL to describe the complex behaviors of AI agents' interaction. We\nformulated the IVRL model and proposed two IVRL models: DQN and A2C. By\ncomparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the\nRole-Playing Game (RPG) reinforcement learning test platform VIZDoom, we\ndemonstrated that rationally organizing various individual needs can\neffectively achieve better performance.",
  "text": "Rationality based Innate-Values-driven\nReinforcement Learning\nQin Yang\nIS3R Lab, Computer Science and Information Systems Department,\nBraldey University, Peoria, IL 61625, USA\nEmail: is3rlab@gmail.com\nAbstract\nInnate values describe agents’ intrinsic motivations, which reflect their inherent\ninterests and preferences to pursue goals and drive them to develop diverse skills sat-\nisfying their various needs. The essence of reinforcement learning (RL) is learning\nfrom interaction based on reward-driven behaviors, much like natural agents. It is\nan excellent model to describe the innate-values-driven (IV) behaviors of AI agents.\nEspecially developing the awareness of the AI agent through balancing internal\nand external utilities based on its needs in different tasks is a crucial problem for\nindividuals learning to support AI agents integrating human society with safety and\nharmony in the long term. This paper proposes a hierarchical compound intrinsic\nvalue reinforcement learning model – innate-values-driven reinforcement learning\ntermed IVRL to describe the complex behaviors of AI agents’ interaction. We\nformulated the IVRL model and proposed two IVRL models: DQN and A2C. By\ncomparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO\nin the Role-Playing Game (RPG) reinforcement learning test platform VIZDoom,\nwe demonstrated that rationally organizing various individual needs can effectively\nachieve better performance.\n1\nIntroduction\nIn natural systems, motivation is concerned explicitly with the activities of creatures that reflect the\npursuit of a particular goal and form a meaningful unit of behavior in this function [1]. Furthermore,\nintrinsic motivations describe incentives relating to an activity itself, and these incentives residing\nin pursuing an activity are intrinsic. Intrinsic motivations deriving from an activity may be driven\nprimarily by interest or activity-specific incentives, depending on whether the object of an activity or\nits performance provides the main incentive [2]. They also fall in the category of cognitive motivation\ntheories, which include theories of the mind that tend to be abstracted from the biological system of\nthe behaving organism [3].\nHowever, when we analyze natural agents, such as humans, they are usually combined motivation\nentities. They have biological motivations, including physiological, safety, and existence needs;\nsocial motivation, such as love and esteem needs; and cognitive motivation, like self-actualization or\nrelatedness and growth needs [4]. The combined motivation theories include Maslow’s Hierarchy of\nNeeds [5] and Alderfer’s Existence Relatedness Growth (ERG) theory [6]. Fig. 1 and 2 illustrate\nthe general innate values (intrinsic motivations) model and various models with three-level needs of\ndifferent amounts, respectively.\nMany researchers regard motivated behavior as behavior that involves the assessment of the con-\nsequences of behavior through learned expectations, which makes motivation theories tend to be\nintimately linked to theories of learning and decision-making [7]. In particular, intrinsic motivation\nleads organisms to engage in exploration, play, strategies, and skills driven by expected rewards. The\ncomputational theory of reinforcement learning (RL) addresses how predictive values can be learned\nand used to direct behavior, making RL naturally relevant to studying motivation.\narXiv:2411.09160v1  [cs.AI]  14 Nov 2024\nInnate Values\nInternal\nState\nCritic\nEnvirnonment\nPerception\nComponents\nActuators\nRL Brain\nAI Agent\nDecisions\nActions\nSignals\nExternal\nState\ne\n(S )\ni\n(S )\ne\n(S )\n(A)\nInnate\nRewards\n(R)\nFigure 1: The illustration of the proposed innate-\nvalues-driven reinforcement learning (IVRL)\nmodel.\nS-S-S\nS-S-M\nS-S-L\nS-L-S\nS-M-L\nS-M-M\nS-M-S\nS-L-M\nS-L-L\nM-M-S\nM-M-M\nM-S-S\nL-S-S\nL-L-S\nL-L-M\nL-L-L\nL-S-M\nL-S-L\nL-M-S\nL-M-M\nL-M-L\nHigh Level \nNeeds\nMiddle Level \nNeeds\nLow Level\nNeeds\nM-L-S\nM-S-M\nM-L-M\nM-L-L\nM-S-L\nM-M-L\nFigure 2: The illustration of innate values models\nwith three-level needs of different amounts. S:\nSmall; M: Medium; L: Large\nIn artificial intelligence, researchers propose various abstract computational structures to form the\nfundamental units of cognition and motivations, such as states, goals, actions, and strategies. For\nintrinsic motivation modeling, the approaches can be generally classified into three categories:\nprediction-based [8, 9], novelty-based [10, 4], and competence-based [11, 12]. Furthermore, the\nconcept of intrinsic motivation was introduced in machine learning and robotics to develop artificial\nsystems learning diverse skills autonomously [13]. The idea is that intelligent machines and robots\ncould autonomously acquire skills and knowledge under the guidance of intrinsic motivations and\nlater exploit such knowledge and skills to accomplish tasks more efficiently and faster than if they\nhad to acquire them from scratch [7].\nIn other words, by investigating intrinsically motivated learning systems, we would clearly improve\nthe utility and autonomy of intelligent artificial systems in dynamic, complex, and dangerous en-\nvironments [14, 15]. Specifically, compared with the traditional RL model, intrinsically motivated\nRL refines it by dividing the environment into an external environment and an internal environment,\nwhich clearly generates all reward signals within the organism1 [7]. However, although the extrinsic\nreward signals are triggered by the objects and events of the external environment, and activities of\nthe internal environment cause the intrinsic reward signals, it is hard to determine the complexity and\nvariability of the intrinsic rewards (innate values) generating mechanism.\nTo address those gaps, we introduce the innate-values-driven reinforcement learning (IVRL) model\nto describe the complex behaviors in AI agents’ interactions by integrating with combined motivation\ntheories. We formalize the idea and propose two IVRL models based on classic DQN and A2C\nalgorithms. Then, we compare them with benchmark RL algorithms such as DQN [16], DDQN [17],\nA2C [18], and PPO [19] in the RPG RL test platform VIZDoom [20, 21]. The results demonstrate\nthat the IVRL model can achieve convergence and adapt efficiently to complex and challenging tasks.\n2\nApproach Overview\nWe assume that all the AI agents (like robots) interact in the same working scenario, and their external\nenvironment includes all the other group members and mission setting. In contrast, the internal\nenvironment consists of individual perception components including various sensors (such as Lidar\nand camera), the critic module involving intrinsic motivation analysis and innate values generation,\nthe RL brain making the decision based on the feedback of rewards and description of the current state\n(including internal and external) from the critic module, and actuators relating to all the manipulators\nand operators executing the RL brain’s decisions as action sequence and strategies. Fig. 4 illustrates\nthe proposed IVRL model.\nCompared with the traditional RL model, our model generates the input state and rewards from\nthe critic module instead of directly from the environment. This means that the individual needs\nto calculate rewards based on the innate value and current utilities and then update its current\n1Here, the organism represents all the components of the internal environment in the AI agent.\n2\ns1\nw1\na1\nr1\ns2\nw2\na2\nr2\ns3\nw3\na3\nr3\n· · ·\nFigure 3: Illustration of the trajectory of state S, needs weight W, action A, and reward R in the\nIVRL model.\nState\n  St\nUtilities\n    Ut\nUt+1\nSt+1\nAgent\nEnvironment\nAction\n  At\nInnate-Values(IVt)\nNeeds Weights(Nt)\nExpected Utilities(E(Ut))\nReward(Rt)\nE(Ut)=∑Ui\nt×Ni\nt=IVt=Rt\nFigure 4: The illustration of the innate-values-driven reinforcement learning (IVRL) model.\ninternal status – the hierarchical needs model. For example, supposing two agents a1 and a2 have\ndifferent innate value models: Fig. 2 S-M-L and L-M-S. We use health points, energy levels, and\ntask achievement to represent their safety needs ns (low-level intrinsic motivations), basic needs\nnb (middle-level intrinsic motivations), and teaming needs nt (High-level intrinsic motivations)\n[22, 23, 24], respectively. Considering n1\ns > n2\ns, n1\nb = n2\nb, and n1\nt < n2\nt, if a1 and a2 receive the\nexternal repairing signal simultaneously, the innate rewards r1 will larger than r2 based on their\ninnate value model (low-level safety needs n1\ns > n2\ns). In contrast, if they get the order to collaborate\nwith other agents fulfilling a task (high-level teaming needs n1\nt < n2\nt) at the same time, the agent a2\nwill receive more credits from it (r1 < r2). In other words, due to different innate value models, their\nintrinsic motivations present significant differences, which will lead to different innate rewards for\nperforming the same task.\nSpecifically, we formalize the IVRL of an AI agent with an external environment using a Markov\ndecision process (MDP) [25]. The MDP is defined by the tuple ⟨S, A, R, T ,γ⟩where S represents\nthe finite sets of internal state Si2 and external states Se. A represents a finite set of actions. The\ntransition function T : S × A × S →[0, 1] determines the probability of a transition from any state\ns ∈S to any state s′ ∈S given any possible action a ∈A. Assuming the critic function is C, which\ndescribes the individual innate value model. The reward function R = C(Se) : S × A × S →R\ndefines the immediate and possibly stochastic innate reward C(Se) that an agent would receive given\nthat the agent executes action a which in state s and it is transitioned to state s′, γ ∈[0, 1) the\ndiscount factor that balances the trade-off between innate immediate and future rewards.\n2.1\nThe Source of Randomness\nIn our model, the randomness comes from three sources. The randomness in action is from the policy\nfunction: A ∼π(·|s); the needs weight function: W ∼ω(·|s) makes the randomness of innate\nvalues; the state-transition function: S′ ∼p(·|s, a) causes the randomness in state.\nSupposing at current state st an agent has a needs weight matrix Nt (Eq. (1)) in a mission, which\npresents its innate value weights for different levels of needs. Correspondingly, it has a utility matrix\nUt (Eq. (1)) for specific needs resulting from action at. Then, we can calculate its reward Rt for at\nthrough Eq. (2) at the state st.\n2The internal state Si describes an agent’s innate value distribution and presents the dominant intrinsic\nmotivation based on the external state Se.\n3\nNt =\n\n\nn11\nn12\n· · ·\nn1m\nn21\nn22\n· · ·\nn2m\n...\n...\n...\n...\nnn1\nnn2\n· · ·\nnnm\n\n;\nUt =\n\n\nu11\nu12\n· · ·\nu1m\nu21\nu22\n· · ·\nu2m\n...\n...\n...\n...\nun1\nun2\n· · ·\nunm\n\n\n(1)\nRt =\nm\nX\ni=1\nn\nX\nj=1\nNt × U T\nt\n(2)\nIn the process, the agent will first generate the needs weight and action based on the current state,\nthen, according to the feedback utilities and the needs weights, calculate the current reward and iterate\nthe process until the end of an episode. Fig. 3 illustrates the trajectory of state S, needs weight W,\naction A, and reward R in the IVRL model.\n2.2\nRandomness in Discounted Returns\nAccording to the above discussion, we define the discounted return G at t time as cumulative\ndiscounted rewards in the IVRL model (Eq. (3)) and γ is the discount factor.\nGt = Rt + γRt+1 + γ2Rt+2 + · · · + γn−tRn\n(3)\nAt time t, the randomness of the return Gt comes from the the rewards Rt, · · · , Rn. Since the reward\nRt depends on the state St, action At, and needs weight Wt, the return Gt also relies on them.\nFurthermore, we can describe their randomness as follows:\nState transition:\nP[A = a|S = s, A = a] = p(s′|s, a);\n(4)\nNeeds weights function:\nP[W = w|S = s] = ω(w|s);\n(5)\nPolicy function:\nP[A = a|S = s] = π(a|s).\n(6)\n2.3\nAction-Innate-Value Function\nBased on the discounted return Eq. (3) and its random factors – Eq. (4), (5), and (6), we can define\nthe Action-Innate-Value function as the expectation of the discounted return G at t time (Eq. (7)).\nQπ,ω(st, wt, at) = E[Gt|St = st, Wt = wt, At = at]\n(7)\nQπ,ω(st, wt, at) describes the quality of the action at taken by the agent in the state st, using the\nneeds weight wt generating from the needs weight function ω as the innate value judgment to execute\nthe policy π.\n2.4\nState-Innate-Value Function\nFurthermore, we can define the State-Innate-Value function as Eq. (8), which calculates the expec-\ntation of Qπ,ω(st, wt, at) for action A and reflects the situation in the state st with the innate value\njudgment wt.\nVπ(st, wt) = EA[Qπ,ω(st, wt, A)]\n(8)\n2.5\nApproximate the Action-Innate-Value Function\nThe agent’s goal is to interact with the environment by selecting actions to maximize future rewards\nbased on its innate value judgment. We make the standard assumption that a factor of γ per time-step\ndiscounts future rewards and define the future discounted return at time t as Eq. (3). Moreover, we\ncan define the optimal action-value function Q∗(s, a, w) as the maximum expected return achievable\nby following any strategy after seeing some sequence s, making corresponding innate value judgment\nw, and then taking action a, where ω is a needs weight function describing sequences about innate\nvalue weights and π is a policy mapping sequences to actions.\nQ∗(s, w, a) = max\nω,π E[Gt|St = st, Wt = wt, At = at, ω, π]\n(9)\n4\n[\n[\nNeeds Weights W\nQ(s, w, a; φ)\nEnvironment\nAction-Innate-\nValue Network\nNeeds-Behavior\nDistribution\nConv\nDense\nState S\nActions A\nMAX QA\nUtility U of Actions A\nGenerating Actions' \nUtilities of the State\nActions' Innate-Values \nScores: QA = W × U\nAgent's Action Utility\nFunction: V(A) = U\nw1\n=\n=\nw2\n...\nwj\n[\n[\na1\na2...\nai\n[\n[\nq1\nT\nq2...\nqi\nu11, u12, ..., u1j\n \nu21, u22, ..., u2j\n \nui1,  ui2,  ..., uij\n \n...\n...\n= QA\n=\n[\n[\n.   .  \n    .\nFigure 5: Illustration of the action-innate-value network generating Needs-Behavior distribution.\nSince the optimal action-innate-value function obeys the Bellman equation, we can estimate the\nfunction by using the Bellman equation as an iterative update. This is based on the following intuition:\nif the optimal innate-value Q∗(s′, w′, a′) of sequence s′ at the next time-step was known for all\npossible actions a′ and needs weights w′, then the optimal strategy is to select the reasonable action\na′ and rational innate value weight w′, maximising the expected innate value of r + γQ∗(s′, w′, a′),\nQ∗(s, w, a) = Es′∼ϵ\n\u0014\nr + γ max\nw′,a′ Q∗(s′, w′, a′)\n\f\f\f\fs, w, a\n\u0015\n(10)\nFurthermore, the same as the DQN [16], we use a function approximator (Eq. (11)) to estimate the\naction-innate-value function.\nQ(s, w, a; θ) ≈Q∗(s, w, a)\n(11)\nWe refer to a neural network function approximator with weights θ as a Q-network. It can be trained\nby minimising a sequence of loss function Li(θi) that changes at each iteration i,\nLi(θi) = Es,w,a∼σ(·)\n\u0002\n(yi −Q(s, w, a; θi))2\u0003\n(12)\nyi = Es′∼ϵ\n\u0014\nr + γ max\nw′,a′ Q(s′, w′, a′; θi−1)\n\f\f\f\fs, w, a\n\u0015\n(13)\nWhere Eq. 13 is the target for iteration i and σ(s, w, a) is a probability distribution over sequences s,\nneeds weights w, and action a that we refer to as the needs-behavior distribution. We can approximate\nthe gradient as follows:\n∇θiLi(θi) = Es,w,a∼σ(·);s′∼ϵ\n\u0014\u0012\nr + γ max\nw′,a′ Q(s′, w′, a′; θi−1) −Q(s, w, a; θi)\n\u0013\n∇θiQ(s, w, a; θi)\n\u0015\n(14)\nInstead of computing the full expectations in the Eq. (14), stochastic gradient descent is usually\ncomputationally expedient to optimize the loss function. Here, the weights θ are updated after every\ntime step, and single samples from the needs-behavior distribution σ and the emulator ϵ replace the\nexpectations, respectively.\nOur approach is a model-free and off-policy algorithm, which learns about the greedy strategy\na = maxw,a Q(s, w, a; θ) following a needs-behavior distribution to ensure adequate state space\nexploration. Moreover, the needs-behavior distribution selects action based on an ϵ-greedy strategy\nthat follows the greedy strategy with probability 1 −ϵ and selects a random action with probability ϵ.\nFig. 5 illustrates the action-innate-value network generating Needs-Behavior distribution.\nMoreover, we utilize the experience replay technique [26], which stores the agent’s experiences at\neach time-step, et = (st, wt, at, rt, st+1) in a data-set D = e1, . . . , eN, pooled over many episodes\ninto a replay memory. During the algorithm’s inner loop, we apply Q-learning updates, or minibatch\nupdates, to samples of experience, e ∼D, drawn at random from the pool of stored samples. After\nperforming experience replay, the agent selects and executes an action according to an ϵ-greedy\npolicy, as we discussed. Since implementing arbitrary length histories as inputs to a neural network\nis difficult, we use a function ϕ to produce our action-innate-value Q-function. Alg. 1 presents the\nalgorithm of the IVRL DQN.\n5\nAlgorithm 1: Innate-Values-driven RL (IVRL) DQN Version\n1 Initialize replay memory D to capacity N;\n2 Initialize the action-innate-value function Q with random neural network weights;\n3 for each episode do\n4\nfor each environment step t do\n5\nWith probability ϵ select a random action at and wt, otherwise select\nat = maxw,a Q(ϕ(s), w, a; θ);\n6\nExecute action at in emulator, calculate reward rt = wt × ut based on agent current needs weights\nwt and utilities ut, and image xt+1;\n7\nSet st+1 = st, at, wt, xt+1 and preprocess ϕt+1 = ϕ(st+1);\n8\nStore transition (ϕt, at, wt, rt, ϕt+1) in D;\n9\nSample random minibatch of transitions (ϕj, aj, wj, rj, ϕj+1) from D;\n10\nSet\nyj =\n\u001a\nrj,\nfor terminal ϕj+1;\nrj + γ maxw,a Q(ϕj+1, w′, a′; θ),\nelse.\n11\nPerform a gradient descent step on (yj −Q(ϕj, wj, aj; θ))2 according to Eq. (14).\nu(s, a; φ)\nEnvironment\nNeeds Weight Distribution\nConv\nDense\nState S\nAction\nπ( · | s; θ )\nω( · | s; δ )\nReward \nUtility Value Network\nCritic\nPolicy Network\nNeeds Network\nGenerating the Utilities \nof at in the State S\n=\n=\n=\n=\nat\nat\n[\n[\n[\n[\nw1\nWt\nUt\nrt\nT\n, w2,\n...\n, wj\nu1, u2, ..., uj\n \n=\nFigure 6: The architecture of the IVRL Actor-Critic Version.\n2.6\nIVRL Advantage Actor-Critic (A2C) Model\nFurthermore, we extend our IVRL method to the Advantage Actor-Critic (A2C) version. Specifically,\nour IVRL A2C maintains a policy network π(at|st; θ), a needs network ω(wt|st; δ), and a utility\nvalue network u(st, at; φ). Since the reward in each step is equal to the current utilities u(st, at)\nmultiplying the corresponding weight of needs, the state innated-values function can be approximated\nby presenting it as Eq. (15). Then, we can get the policy gradient Eq. (16) and needs gradient Eq.\n(17) of the Eq. (15) deriving V (s; θ, δ) according to the Multi-variable Chain Rule, respectively. We\ncan update the policy network θ and needs network δ by implementing policy gradient and needs\ngradient, and using the temporal difference (TD) to update the value network φ.\nVπ,ω(s) =\nX\nat,wt\nπ(at|st) · ω(wt|st) · u(st, at)\n≈\nX\na,w\nπ(at|st; θ) · ω(wt|st; δ) · u(st, at; φ) = V (s; θ, δ, φ)\n(15)\ngrad V (at, θt) = Vθ(s; θ, δ, φ) = ∂V (s; θ, δ, φ)\n∂θ\n= ∂π(at|st; θ)\n∂θ\n· ω(wt|st; δ) · u(st, at; φ)\n(16)\ngrad V (wt, δt) = Vδ(s; θ, δ, φ) = ∂V (s; θ, δ, φ)\n∂δ\n= π(at|st; θ) · ∂ω(wt|st; δ)\n∂δ\n· u(st, at; φ) (17)\nUsing an estimate of the utility u function as the baseline function, we subtract the V value term as\nthe advantage value. Intuitively, this means how much better it is to take a specific action and a needs\n6\nAlgorithm 2: Innate-Values-driven RL (IVRL) Advantage Actor-Critic (A2C) Version\n1 Procedure N-Step IVRL Advantage Actor-Critic;\n2 Start with policy model πθ, needs model ωδ, and utility value model Vφ;\n3 for each episode do\n4\nGenerate an episode s0, a0, w0, u0, · · · , sT −1, aT −1, wT −1, uT −1 following πθ(·) and ωδ(·)\n5\nfor each environment step t do\n6\nUend = 0;\n7\nif t + N ≥T else Uφ(st+N);\n8\nUt = γNUend + PN−1\nk=0 γk(ut+k if (t + k < T) else 0);\n9\nL(θ) = 1\nT\nPT −1\nt=0 (Ut −Vφ(st)) · wt · grad (πθ);\n10\nL(δ) = 1\nT\nPT −1\nt=0 (Ut −Vφ(st)) · grad (wt) · πθ;\n11\nL(φ) = 1\nT\nPT −1\nt=0 (Ut −Vφ(st))2;\n12\nOptimize πθ using ∇L(θ);\n13\nOptimize ωδ using ∇L(δ);\n14\nOptimize Uφ using ∇L(φ)\n(a) Defend the Center\n(b) Defend the Line\n(c) Deadly Corridor\n(d) Arena\nFigure 7: The four scenarios of experiments in the VIZDoom\nweight compared to the average, general action and the needs weights at the given state Eq. (18). Fig.\n6 illustrates the architecture of the IVRL actor-critic version and Alg. 2 presents the algorithm of the\nIVRL A2C.\nA(st, at) = U(st, at) −V (st)\n(18)\n3\nExperiments\nConsidering cross-platform support and the ease of creating custom scenarios, we selected the\nVIZDoom Role-Playing Game (RPG) reinforcement learning test platform [20, 21] to evaluate the\nperformance of the proposed innate-value-driven reinforcement learning model. We choose four\nscenarios: Defend the Center, Defend the Line, Deadly Corridor, and Arens (Fig. 7), and compare\nour models with several benchmark algorithms, such as DQN [16], DDQN [17], A2C [18], and PPO\n[19]. These models were trained on an NVIDIA GeForce RTX 3080Ti GPU with 16 GiB of RAM.\n3.1\nEnvironment Setting\nIn our experiments, we define four categories of utilities (health points, amount of ammo, environment\nrewards, and number of killed enemies), presenting three different levels of needs: low-level safety\nand basic needs, medium-level recognition needs, and high-level achievement needs. When the\nagent executes an action, it receives all the corresponding innate utilities, such as health points and\nammo costs, and external utilities, such as environment rewards (living time) and the number of\nkilled enemies. At each step, the agent can calculate the rewards for the action by multiplying the\ncurrent utilities and the needed weight for them. In our experiments, the initial needs weight for each\nutility category is 0.25, which has been fixed in the benchmark DRL algorithms’ training, such as\nDQN, DDQN, and PPO. For more details about the experiment code, please check the supplementary\nmaterials.\n7\na. Defend the Center – Fig. 7(a): For this scenario, the map is a large circle where the agent is in the\nmiddle, and monsters spawn along the wall. The agent’s basic actions are turn-left, turn-right, and\nattack, and the action space is 8. It needs to survive in the scenario as long as possible.\nb. Defend the Line. – Fig. 7(b): The agent is located on a rectangular map, and the monsters are on\nthe opposite side. Similar to the defend the center scenario, the agent needs to survive as long as\npossible. Its basic actions are move-left, move-right, turn-left, turn-right, and attack, and the action\nspace is 32.\nc. Deadly Corridor. – Fig. 7(c): In this scenario, the map is a corridor. The agent is spawned at one\nend of the corridor, and a green vest is placed at the other end. Three pairs of monsters are placed on\nboth sides of the corridor. The agent needs to pass the corridor and get the vest. Its basic actions are\nmove-left, move-right, move-forward, turn-left, turn-right, and attack, and the action space is 64.\nd. Arena. – Fig. 7(d): This scenario is the most challenging map compared with the other three. The\nagent’s start point is in the middle of the map, and it needs to eliminate various enemies to survive\nas long as possible. Its basic actions are move-left, move-right, move-forward, move-backward,\nturn-left, turn-right, and attack, and the action space is 128.\n3.2\nEvaluation\nThe performance of the proposed IVRL DQN and A2C models is shown in the Fig 8. Fig. 7(a),\n7(b), 7(c), and 7(d) demonstrate that IVRL models can achieve higher average scores than traditional\nRL benchmark methods (Fig. 8(a), 8(d), 8(g), and 8(j)). Especially for the IVRL A2C algorithm, it\npresents more robust, stable, and efficient properties than the IVRL DQN model. Although the IVRL\nDQN shows better performance in the Arena scenario (Fig. 8(j)), the small perturbation introduced\nby the innate-values utilities may have made the network weights in some topology difficult to reach\nconvergence.\nFurthermore, we also analyze their corresponding tendencies in different scenarios to compare the\nneeds weight differences between the IVRL DQN and A2C models. In the defend-the-center and\ndefend-the-line experiments, each category of the need weight in the IVRL DQN model does not\nsplit and converges to a specific range compared with its initial setting in our training (Fig. 8(b) and\n8(e)). In contrast, the weights of health depletion, ammo cost, and sub-goal (environment rewards)\nshrink to approximately zero, and the weight of the number of killed enemies converges to one in\nthe IVRL A2C model. This means that the top priority of the IVRL A2C agent is to eliminate all\nthe threats or adversaries in those scenarios so that it can survive, which is similar to the Arena task.\nAccording to the performance in those three scenarios (Fig. 8(c), 8(f), and 8(l)), the IVRL A2C agent\nrepresents the characteristics of bravery and fearlessness, much like the human hero in a real battle.\nHowever, in the deadly corridor mission, the needs weight of the task goal (getting the vest) becomes\nthe main priority, and the killing enemy weight switches to the second for the IVRL A2C agent (Fig.\n8(i)). They converge to around 0.6 and 0.4, respectively. In training, by adjusting its different needs\nweights to maximize rewards, the IVRL A2C agent develops various strategies and skills to kill the\nencounter adversaries and get the vast efficiently, much like a military spy.\nIn our experiments, we found that selecting the suitable utilities to consist of the agent innate-values\nsystem is critically important for building its reward mechanism, which decides the training speed\nand sample efficiency. Moreover, the difference in the selected utility might cause some irrelevant\nexperiences to disrupt the learning process, and this perturbation leads to high oscillations of both\ninnate-value rewards and needs weight.\nGenerally, the innate value system serves as a unique reward mechanism driving agents to develop\ndiverse actions or strategies satisfying their various needs in the systems. It also builds different\npersonalities and characteristics of agents in their interaction. From the environmental perspective,\ndue to the various properties of the tasks, agents need to adjust their innate value system (needs\nweights) to adapt to different tasks’ requirements. These experiences also shape their intrinsic values\nin the long term, similar to humans building value systems in their lives. Moreover, organizing agents\nwith similar interests and innate values in the mission can optimize the group utilities and reduce\ncosts effectively, just like “Birds of a feather flock together.\" in human society.\n8\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n(l)\nFigure 8: The performance comparison of IVRL DQN and A2C agents with DQN, DDQN, PPO, and\nA2C in the VIZDoom.\n4\nConclusion\nThis paper introduces the innate-values-driven reinforcement learning (IVRL) model mimicking the\ncomplex behaviors of agent interactions. By adjusting needs weights in its innate-values system,\nit can adapt to different tasks representing corresponding characteristics to maximize the rewards\nefficiently. For theoretical derivation, we formulated the IVRL model and proposed two types of\nIVRL models: DQN and A2C. Furthermore, we compared them with benchmark algorithms such as\nDQN, DDQN, A2C, and PPO in the RPG reinforcement learning test platform VIZDoom. The results\nprove that rationally organizing various individual needs can effectively achieve better performance.\n9\nFor future work, we want to improve the IVRL further and develop a more comprehensive system\nto personalize individual characteristics to achieve various tasks testing in several standard MAS\ntestbeds, such as StarCraft II, OpenAI Gym, Unity, etc. Especially in multi-object and multi-agent\ninteraction scenarios, building the awareness of AI agents to balance the group utilities and system\ncosts and satisfy group members’ needs in their cooperation is a crucial problem for individuals\nlearning to support their community and integrate human society in the long term. Moreover,\nintegrating efficient deep RL algorithms with the IVRL can help agents evolve diverse skills to adapt\nto complex environments in MAS cooperation. Furthermore, implementing the IVRL in real-world\nsystems, such as human-robot interaction, multi-robot systems, and self-driving cars, would be\nchallenging and exciting.\nReferences\n[1] Jutta Heckhausen and Heinz Heckhausen. Motivation and action. Springer, 2018.\n[2] Ulrich Schiefele. Motivation und Lernen mit Texten. Hogrefe Göttingen, 1996.\n[3] Kathryn E Merrick. Novelty and beyond: Towards combined motivation models and integrated\nlearning architectures. Intrinsically motivated learning in natural and artificial systems, pages\n209–233, 2013.\n[4] Kathryn E Merrick and Mary Lou Maher. Motivated reinforcement learning: curious characters\nfor multiuser games. Springer Science & Business Media, 2009.\n[5] Abraham Harold Maslow. A dynamic theory of human motivation. 1958.\n[6] Clayton P Alderfer. Existence, relatedness, and growth: Human needs in organizational settings.\n1972.\n[7] Gianluca Baldassarre and Marco Mirolli. Intrinsically motivated learning systems: an overview.\nIntrinsically motivated learning in natural and artificial systems, pages 1–14, 2013.\n[8] Jürgen Schmidhuber. Curious model-building control systems. In Proc. international joint\nconference on neural networks, pages 1458–1463, 1991.\n[9] Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990–2010).\nIEEE transactions on autonomous mental development, 2(3):230–247, 2010.\n[10] Stephen Marsland, Ulrich Nehmzow, and Jonathan Shapiro. A real-time novelty detector for\na mobile robot. EUREL European Advanced Robotics Systems Masterclass and Conference,\n2000.\n[11] Andrew G Barto, Satinder Singh, Nuttapong Chentanez, et al. Intrinsically motivated learning\nof hierarchical collections of skills. In Proceedings of the 3rd International Conference on\nDevelopment and Learning, volume 112, page 19. Citeseer, 2004.\n[12] Massimiliano Schembri, Marco Mirolli, and Gianluca Baldassarre. Evolution and learning\nin an intrinsically motivated reinforcement learning robot. In Advances in Artificial Life: 9th\nEuropean Conference, ECAL 2007, Lisbon, Portugal, September 10-14, 2007. Proceedings 9,\npages 294–303. Springer, 2007.\n[13] Qin Yang and Ramviyas Parasuraman. Bayesian strategy networks based soft actor-critic\nlearning. ACM Transactions on Intelligent Systems and Technology, 15(3):1–24, 2024.\n[14] Qin Yang and Ramviyas Parasuraman. Game-theoretic utility tree for multi-robot cooperative\npursuit strategy. In ISR Europe 2022; 54th International Symposium on Robotics, pages 1–7.\nVDE, 2022.\n[15] Qin Yang and Ramviyas Parasuraman. A hierarchical game-theoretic decision-making for\ncooperative multiagent systems under the presence of adversarial agents. In Proceedings of the\n38th ACM/SIGAPP Symposium on Applied Computing, pages 773–782, 2023.\n[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n[17] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.\nDueling network architectures for deep reinforcement learning. In International conference on\nmachine learning, pages 1995–2003. PMLR, 2016.\n10\n[18] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-\nment learning. In International conference on machine learning, pages 1928–1937. PMLR,\n2016.\n[19] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[20] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski.\nViZDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE\nConference on Computational Intelligence and Games, pages 341–348, Santorini, Greece, Sep\n2016. IEEE. The Best Paper Award.\n[21] Marek Wydmuch, Michał Kempka, and Wojciech Ja´skowski. ViZDoom Competitions: Playing\nDoom from Pixels. IEEE Transactions on Games, 11(3):248–259, 2019. The 2022 IEEE\nTransactions on Games Outstanding Paper Award.\n[22] Qin Yang and Ramviyas Parasuraman. Hierarchical needs based self-adaptive framework for\ncooperative multi-robot system. In 2020 IEEE International Conference on Systems, Man, and\nCybernetics (SMC), pages 2991–2998. IEEE, 2020.\n[23] Qin Yang and Ramviyas Parasuraman. Needs-driven heterogeneous multi-robot cooperation\nin rescue missions. In 2020 IEEE International Symposium on Safety, Security, and Rescue\nRobotics (SSRR), pages 252–259. IEEE, 2020.\n[24] Qin Yang and Ramviyas Parasuraman. How can robots trust each other for better cooperation?\na relative needs entropy based robot-robot trust assessment model. In 2021 IEEE International\nConference on Systems, Man, and Cybernetics (SMC), pages 2656–2663. IEEE, 2021.\n[25] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.\nJohn Wiley & Sons, 2014.\n[26] Long-Ji Lin. Reinforcement learning for robots using neural networks. Carnegie Mellon\nUniversity, 1992.\n11\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.RO"
  ],
  "published": "2024-11-14",
  "updated": "2024-11-14"
}