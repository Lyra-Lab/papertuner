{
  "id": "http://arxiv.org/abs/2411.09722v1",
  "title": "Iterative Batch Reinforcement Learning via Safe Diversified Model-based Policy Search",
  "authors": [
    "Amna Najib",
    "Stefan Depeweg",
    "Phillip Swazinna"
  ],
  "abstract": "Batch reinforcement learning enables policy learning without direct\ninteraction with the environment during training, relying exclusively on\npreviously collected sets of interactions. This approach is, therefore,\nwell-suited for high-risk and cost-intensive applications, such as industrial\ncontrol. Learned policies are commonly restricted to act in a similar fashion\nas observed in the batch. In a real-world scenario, learned policies are\ndeployed in the industrial system, inevitably leading to the collection of new\ndata that can subsequently be added to the existing recording. The process of\nlearning and deployment can thus take place multiple times throughout the\nlifespan of a system. In this work, we propose to exploit this iterative nature\nof applying offline reinforcement learning to guide learned policies towards\nefficient and informative data collection during deployment, leading to\ncontinuous improvement of learned policies while remaining within the support\nof collected data. We present an algorithmic methodology for iterative batch\nreinforcement learning based on ensemble-based model-based policy search,\naugmented with safety and, importantly, a diversity criterion.",
  "text": "Iterative Batch Reinforcement Learning via Safe\nDiversified Model-based Policy Search\nAmna Najib, Stefan Depeweg, Phillip Swazinna\nSiemens AG\n{amna.najib, stefan.depeweg, phillip.swazinna}@siemens.com\nAbstract: Batch reinforcement learning enables policy learning without direct\ninteraction with the environment during training, relying exclusively on previously\ncollected sets of interactions. This approach is, therefore, well-suited for high-risk\nand cost-intensive applications, such as industrial control. Learned policies are\ncommonly restricted to act in a similar fashion as observed in the batch. In a real-\nworld scenario, learned policies are deployed in the industrial system, inevitably\nleading to the collection of new data that can subsequently be added to the existing\nrecording. The process of learning and deployment can thus take place multiple\ntimes throughout the lifespan of a system. In this work, we propose to exploit this\niterative nature of applying offline reinforcement learning to guide learned policies\ntowards efficient and informative data collection during deployment, leading to\ncontinuous improvement of learned policies while remaining within the support of\ncollected data. We present an algorithmic methodology for iterative batch reinforce-\nment learning based on ensemble-based model-based policy search, augmented\nwith safety and, importantly, a diversity criterion.\n1\nIntroduction\nThe objective of batch (or offline) reinforcement learning (RL) is to extract the best possible behavior\nout of existing data, called a batch, without any learning during deployment. This implies that\nlearning is more successful if the initial data is diverse or collected through the deployment of an\nexpert agent. In a real setting, the initial batch is prone to limitations (low data coverage, low reward\nactions, etc.), forming a challenge in learning for real-world applications. This challenge of limited\ninformation calls for safety mechanisms, such as regularization, to ensure reliable performance of the\nagent [1, 2].\nIn many industrial setups, the application of offline reinforcement learning is not a one-time process\nbut iterative. After an RL agent is trained and deployed on the system, a new set of recordings\nbecomes available. The principal contribution of our work relies on the formulation of iterative\nbatch reinforcement learning (IBRL), a novel framework to iteratively refine the initial data batch\nand improve learned policies after each new batch collection, without dropping performance due\nto overly adventurous exploration. In every iteration, we seek to improve the data coverage by\ndeploying a set of policies, that we previously trained to be diverse, i.e. that act in a variety of ways to\nexplore, without compromising the rewards too much. The proposed IBRL algorithms adhere to safety\nconstraints by restricting the state or action space of the learned policies depending on the data support.\nThrough experiments in an illustrative 2D environment, as well as on the Industrial Benchmark [3], we\ndemonstrate the improved exploration capability and resulting improved performance of our approach,\nall while maintaining safety considerations and not underperforming the behavioral. Conceptually,\nour work is most closely related to [4, 5, 6], however these works do not address the combination\nof diversity and safety, or focus solely on the iterative process without incorporating an exploration\nincentive.\nWorkshop on Safe and Robust Robot Learning for Operation in the Real World (SAFE-ROL) at CoRL 2024.\narXiv:2411.09722v1  [cs.LG]  14 Nov 2024\nText\nLearn \nsimulation \nmodel\nInitial \nBehavior  \nPolicy\nAdd Batch to \nData \nmodel-based \npolicy \nsearch\nNew Batch RL Iteration\nLearn policies \n1\nEnvironment\nAction\nState, \nReward\nAgent\nFigure 1: Illustration of iterative model-based policy search.\n2\nRelated Work\nOffline RL: Early works in the so-called “batch RL” setting include [7, 8, 9, 10], as well as more\nrecently [11, 12, 13, 14]. While these works focused on reinforcement learning in the batch setting, a\nkey distinction to later proposed offline methods is that they mostly assume the initial batch to be\ncollected under uniform random actions, allowing a relatively well explored environment. While\nthe batch RL setting is certainly closer to the requirements imposed by real-world deployments of\nRL algorithms than commonly popular online algorithms such as [15, 16, 17], it is still not exactly\nwhat industry practitioners need most of the time. Algorithms such as [18, 19, 20, 21, 22, 23] employ\nvarious regularization techniques to keep the trained policies in regions of the environment where the\nmodels are sufficiently accurate. Most common techniques include behavior regularization [24, 19]\nand uncertainty-based regularization [25, 26]. In offline RL, model-based RL appears to have an\nedge over model-free methods, which enjoy better asymptotic performance but have generally been\nattributed lower data efficiency [27, 23, 28]. Methods such as [14, 1, 29, 30] have thus developed\nways to extend action divergence regularizations to the model-based setting. Offline-to-Online: Some\nof the early batch RL works mentioned above as well as [4, 31] introduced the idea of the growing\nbatch setting, a problem in between offline and online learning where one is constrained to only\ndeploy a limited number of times to the real system to collect new data. While the idea is appealing,\nour approach differs significantly since their almost unconstrained exploration can be an issue due to\nsafety constraints or requirements on the minimal performance. Other works that explored online\nfinetuning of offline pretrained policies without the deployment constraint exhibit similar issues due\nto their exploration strategies [32, 23, 33, 34, 35]. Diversity: Intrinsically motivated agents have been\nintroduced in [36]. Agents have since been found to perform effective exploration using intrinsic\nreward objectives like curiosity or diversity and have been studied e.g. in [37, 38, 39]. Previous work\nhas also explored the effect of collecting initial diverse data that is further used for offline learning in\nseveral downstream tasks [40, 41, 42], as well recently [43, 44].\n3\nMethod\nOffline reinforcement learning uses a fixed dataset D collected by one or more behavior policies πβ\nto learn a policy π. Once trained, the agent’s policy is fixed and no further learning occurs during\ndeployment. To ensure safety, the learned policy is often constrained to avoid significant deviations\nfrom the original behavior policies used to generate the data.\nOne approach for learning a policy is a model-based policy search. This is a two-step process. First,\na simulation model is learned from the available data. In the second step, one or a set of policies are\noptimized using virtual rollouts. For policy search, we seek to minimize the loss function\nL(θ) = −1\nN\n1\nK\n1\nH\nK\nX\nk=1\nX\nsk,1∼D\nH\nX\nt=1\nγte(sk,t, ak,t) ,\n(1)\n2\nwhere K is the number of policies, e(sk,t, ak,t) the reward received while taking action ak,t at state\nsk,t and θ = θ1, . . . , θK the parameters of the policies. We aim to maximize the accumulated reward\nof trajectories generated by following the K learned policies. Every trajectory is formalized as\nTk = {sk,1, ak,1, sk,2, ak,2..sk,H−1, ak,H−1, sk,H}\n= {sk,i | sk,i = f(sk,i−1, ak,i−1; η), ak,i−1 = π(sk,i−1; θk), 1 < i < H + 1, sk,1 ∼D}\n(2)\nwhere f(·; η) denotes the learned transition model.\nIn iterative offline reinforcement learning the offline RL setting is repeated at different times over\nthe lifetime of a system. The collected data in every iteration of deployment is added to the batch\nand further used to refine the training of the policy. We can apply the method of model-based policy\nsearch to the iterative offline setting: every time a new batch of data becomes available, we update the\ntransition model and then redo the policy training and deployment. This will serve as the backbone of\nour proposed algorithm, which we illustrate in Figure 1. We note that alternative approaches, such as\na model-free approach are viable as well. We choose model-based policy search, because it tends to\nwork well in practice, can be easily extended (e.g. via uncertainty modeling), and allows automatic\ndifferentiation in continuous state, action, and reward spaces.\nImportantly, we argue that two components are highly beneficial for model-based policy search in the\niterative batch scenario: safety and diversity. The former is ubiquitous in all real-world applications\nand should safeguard against exploiting model inaccuracies due to limited data. The latter should\nexploit the iterative nature and improve information gain over each iteration. We hypothesize that an\nideal algorithm for iterative batch RL is safe but diverse.\n3.1\nSafety\nWe formulate three approaches towards realizing safety in policy search. Safety may be used (i) as an\nadditional objective in the loss function, (ii) as a soft constraint, or finally, (iii) it may also be possible\nto constrain the policy directly as part of its architecture.\nSafety as an objective\nSafety is injected as an explicit objective in the loss function to balance\nbetween high rewards and not deviating from the behavior policy. It was used in previous research\nwork including [45, 46, 20]. At first, a behavior policy is learned in a supervised manner from the\nfixed initial dataset as denoted in Eq. (3). Afterwards, for every state, the behavioral policy action\nand the new policy actions are predicted and used to define the deviation between the behavior policy\nand the learned policy. The deviation in the simplest case is a mean-squared error between both\nactions. If a distribution of actions is learned for both policies, the Kullback-Leibler divergence (KL\ndivergence) could instead be used to assess the deviation. In the MSE case we have:\nL(ϕ) =\nX\ns,a∼D\n∥a −πβ(s; ϕ)∥2 .\n(3)\nThen, the loss function including the reward maximization and safety objectives is defined as\nL(θ) = 1\nK\n1\nH\nK\nX\nk=1\nX\nsk,1∼D\nH\nX\nt=1\n[−γtλe(sk,t, ak,t) + (1 −λ)p(ak,t) ,\np(ak,t) = ∥π(sk,t; θk) −πβ(sk,t; ϕ)∥2 .\n(4)\nWhile this is the standard approach for safe offline RL it has one key drawback: Weighing safety\nagainst performance (and possibly diversity) in the form of a trade-off (in this case via the parameter\nλ) appears counter-intuitive for safety-critical applications.\nSafety as a soft constraint\nAn alternative approach is to specify a loss term that is flat inside a safe\nregion and then provides a large loss value outside, thereby implementing a differentiable constraint.\nIn contrast to the aforementioned objective-based approach, here the safety term is not weighted\nagainst the objective but instead is effectively restricting the allowed solution space of the policy.\n3\nWe note that in iterative batch RL, the transitions in the batch may be generated by the execution of\ndifferent policies. To that end, we propose to approximate the behavior policy using a probabilistic\napproach. We assume that the learnt behavior policy is a Gaussian distribution with mean µβ and\ndiagonal covariance matrix Σβ, πβ(s; ϕ) ∼N(µβ, Σβ). Σβ reflects the aleatoric uncertainty in the\nmodel. A prediction has a high aleatoric uncertainty in the case of diverse actions present in the\nbehavior data. As a measure of safety we consider how likely an action is under the behavior policy.\nWe make 3 considerations for the proposed metric: to be normalized between 0 and 1, to be sensitive\nto low likelihoods in certain action dimensions and lastly to be more permissible in situations where\ndiverse actions were executed. To that end we use the negative unnormalized likelihood and compute\nthe geometric mean over the action space:\nG(S, A) = −[\nY\nd\nexp\n\u0012\n−1\n2(a −µβ(s; ϕ))T Σβ(s; ϕ)−1(a −µβ(s; ϕ))\n\u0013\n]\n1\nd ,\n(5)\nwhere d is the dimensionality of the actions. To enforce safety during training of policy π we can use\nthresholding:\nLS(θ) = αs\n1\nK\n1\nH\nK\nX\nk=1\nH\nX\nt=1\nmax(G(S, A) + δ, 0)\nwith\nS ∼f(·; η), A ∼π(S; θ) ,\n(6)\nwhere δ ∈[0, 1] represents the safety threshold and controls the permissibility of the training. Any\ndeviation of the actions represented by G(S, A) lower than −δ is not penalized in the learning. To\nconclude, the loss in Eq. (6) implements a likelihood-based safety zone.\nConstrained Policy\nThe direct approach to fulfill safety is arguably to directly constrain the\nexpressiveness of the policy itself. In this scenario, we consider safety with respect to the state space\nand not relative to a behavior policy. We assume that professionals may have prior knowledge about\nsafety ranges (bounds) of sensors. We start building on this assumption and limit all the states over all\nvirtual rollouts for the different ensembles to lie within the predefined bounds. We inject this bound\nconstraint in the policy specification.\nThis approach is only applicable if actions a affect a subset of system variables s′ in a known linear\nway. Let alower and aupper be the lower and upper bounds of actions. Further, let π(st; θ) be designed\nsuch that only actions in this bound can be computed. Let B1 and B2 be the lower and upper safety\nbound of the state. We then compute the valid action range amin, amax such that B1 < s′(t+1) < B2.\nWe then define the constrained policy such that:\nπconstr(st; θk) = amin + (amax −amin) ∗πconstr(st; θk) −alower\naupper −alower\n(7)\n3.2\nDiversified Policy Search\nWe define diversity as the ability to discover different or dissimilar state regions. This translates\nback to having a high entropy on the trajectory samples used for training and deployment. Given\nan ensemble of K policies, we draw K trajectory samples T1, . . . , TK, where Tk results from the\ninteraction between the learned transition model f(s, a; η), reward model f(s, a; ω) and policy πθi\nunder the same starting state s1. Let D(θ, η, ω) denote the distances between all trajectories. The\ndiversity-based exploration enforces the maximization of D(θ, η, ω), by adding the diversity loss as\nan intrinsic motivation for exploration.\nLd(η, ω, θ) = −D(T1, T2, .., TK) = −D(θ, ω, η)\nDiversity measures\nWe wish for the diversity-based objective to be differentiable w.r.t θk [47, 48].\nDifferent distance metrics between trajectories were used in different communities [49, 50, 51]. The\nsimplest distance metric is the pairwise distance between trajectories, also called lock-step Euclidean\n4\ndistance (LSED). LSED measures the spatial discrepancy between time-corresponding states over the\nrollout of different K ensemble policies. The distance between trajectory Ti and Tj is calculated as\nD(Ti, Tj) = 1\nH\nH\nX\nt=1\n∥si,t −sj,t∥2\nThe pairwise distance between all trajectories induced by the different K policies is\nD = 1\nK!\nK\nX\nk=1\nK\nX\nk′=1,k′̸=k\nD(Tk, Tk′) = 1\nH\n1\nK!\nK\nX\nk=1\nK\nX\nk′=1,k′̸=k\nH\nX\nt=1\n∥sk,t −sk′,t∥2\n(8)\nLSED diversity can suffer from outlier behavior. Due to the fact that diversity and reward maximiza-\ntion potentially act as conflicting objectives, LSED diversity can incentivize all policies but one to\nfocus on cost minimization and learns one outlier policy to bring the diversity to a high value, by that\nfulfilling the tradeoff to balance these competing objectives. Using the L1 norm instead of the L2\nnorm reduces the outlier behavior to an extent.\nAn alternative approach is to instead use the minimum pairwise distance between trajectories\n(MinLSED) to mitigate the outlier behavior of LSED. The MinLSED diversity represents the mini-\nmum mean distance over the horizon of the pairwise trajectories and is formalized as\nD = 1\nH\nmin\nk′̸=k,k∈K D(Tk, Tk′) .\n(9)\nIn this work we will utilize the MinLSED diversity specified in Eq. (9).\n3.3\nAlgorithm\nAlgorithm 1 Safe Diversified model-based\npolicy search (Soft Constraint)\nInput: Data D, behavior policy πβ(s; ϕ)\nTrain transition model f(st, at; η) on D\nTrain reward function f(s, a; ω) on D\nwhile not converged do\nsk,1 ∼D\nfor k=1,...,K do\nfor t=2,...,H do\nµβ(sk,t),\nΣβ(sk,t) = πβ(sk,t; ϕ)\nak,t = π(sk,t; θk)\nsk,t+1 = f(sk,t, ak,t; η)\nrk,t = f(sk,t, ak,t; ω)\nRk+ = γtrk,t\nsk,t = sk,t+1\nend for\nend for\nCompute Eq. (9) and Eq. (6)\nTrain π(.; θ1) . . . , π(.; θk) on Eq. (10)\nend while\nWe will show here one instantiation of a possible loss\nfunction using a soft-constrained policy and minLSD\ndiversity.\nLθ) = −\n1\nNKH\nK\nX\nk=1\nX\nsk,1∼D\nH\nX\nt=1\nγte(sk,t, ak,t)\n+ αs\n1\nKH\nK\nX\nk=1\nH\nX\nt=1\nmax(G(sk,t, π(sk,t; θk)) + δ, 0)\n−αd\n1\nH\nmin\nk′̸=k,k∈K D(Tk, Tk′)\n(10)\nwhere we note that in Eq. (10) by default we reduce the\nscope of the diversity term to exclude one (the first)\npolicy. The reasoning is to have always one policy\navailable that is only influenced by the reward and the\nsafety. All policies are used for data generation, while\nonly the first is used for evaluating the performance.\nThe aforementioned instantiation of the training algo-\nrithm for one batch iteration is shown in Algorithm 1.\nThis process is repeated whenever a new batch arrives after execution of the previous trained set of\npolicies, with the batch appended to the existing data set, following the schema illustrated in Figure 1.\n4\nExperiments\nWe investigate the advantage of the proposed iterative batch reinforcement learning framework in\nimproving learned policies. We seek to investigate the supplementary advantage of incorporating\ndiversity in the process. Finally, we investigate how different forms of safety impact the diversity\nobjective. We evaluate our methods on a 2D grid environment and the industrial benchmark [3].\n5\nFigure 2: 2D grid environment: The behavior policy guides agent towards the nearest behavior goal.\nThe best reward goal represents the state with the highest reward. Reward decreases according to\nGaussian distribution represented by circle lines.\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.0\n0\n0.5\n1.0\n1.5\n2.0\nangle to (1,0)\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.2\n0\n0.5\n1.0\n1.5\n2.0\nangle to (1,0)\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\n0\n0.5\n1.0\n1.5\n2.0\nangle to (1,0)\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.6\n0\n0.5\n1.0\n1.5\n2.0\nangle to (1,0)\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.8\n0\n0.5\n1.0\n1.5\n2.0\nangle to (1,0)\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 1.0\n0\n0.5\n1.0\n1.5\n2.0\nangle to (1,0)\nFigure 3: Policy maps for different λ values, colors illustrate action directions in every cell of the\ngrid. For λ = 0.0, the policy imitates behavior policy by navigating to the closest behavior goal.\nWith increasing λ values, the policy moves slowly towards the reward goal.\n4.1\n2D Grid Environment: Single Iteration\nThe 2D grid environment is a simplistic benchmark illustrating common navigation tasks. The state\n(x, y) represents the position of an agent in space. The agent is rewarded according to its position,\nfollowing a Gaussian distribution with mean µ = (3, 6)T and diagonal covariance matrix Σ with\nstandard deviation vector (1.5, 1.5)T . Concretely, the reward of an agent at state st is the likelihood\nof the reward Gaussian distribution: r(sk,t) =\n1\n(2π)\n3\n2 |Σ|\n1\n2 e−1\n2 (sk,t−µ)T Σ−1(sk,t−µ). See Figure 2 for\na visualization of the reward distribution. We gather the initial data by deploying a behavior policy\nthat navigates to one of the behavior goals denoted in Figure 2 and fixed to the positions (2.5, 2.5)T\nand (7.5, 7.5)T . The agent navigates to the goal closest to its current position. Additionally, the\nbehavior policy is augmented with 10% uniform random actions.\nFor reference, we first train a single policy without diversity for different values of λ for one\nsingle iteration. Figure 3 represents the policy maps of actions taken by policies learned with\nincreasing λ values in the 2D grid environment. With λ = 0.0, the objective is reduced to mimicking\nthe behavior policy. In the case of λ = 0.4, the policy predominantly adheres to the behavior\npolicy with slight adjustments in the direction of certain actions.\nNevertheless, convergence\ntowards both behavior poles is still observable. This validates our further assumption of fixing λ at 0.4.\nFigure 4 compares the results of a policy training without and with diversity. Here, we used the\n\"safety as an objective\" approach. We can see that diversity leads to a more heterogeneous set of\npolicies. Additionally, we also observe that the safety term still has a significant impact on the policy.\nThis result shows that diversity and safety can be combined to obtain safe, but different policies.\n4.2\nIndustrial Benchmark: Iterative Batch RL\nThe Industrial Benchmark serves as a reinforcement learning simulator specifically designed to\nreplicate challenges commonly encountered in industrial settings, such as high dimensionality,\n6\nEnsemble 1\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\nEnsemble 2\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\nEnsemble 3\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\nEnsemble 4\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\nEnsemble 1\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\nEnsemble 2\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\nEnsemble 3\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\nPolicymap - = 0.4\nEnsemble 4\nFigure 4: Policy maps of the ensemble policies using IBRL with safety as objective and parameter\nλ = 0.4. Colors represent the directions of the actions, where red corresponds to 2π and blue\ncorresponds to 0.Top row: without diversity αd = 0.0. Bottom row: with diversity αd = 0.15.\npartial observability, sparse rewards, and the consideration of Pareto optimality [13, 45, 1]. Every\nobservation st = (p, vt, gt, ht, ft, ct) consists of the variables (setpoint, velocity, gain, shift, fatigue,\nconsumption). Velocity, gain, and shift are bounded within [0, 100].\nIn order to see how the proposed algorithm from Section 3.3 performs in an iterative batch scenario\nand how different definitions of safety interact with the diversity term, we perform the following\ntwo experiments: First, we use a random bounded dataset collected by uniform sampling from the\naction space such that the states are restricted to lie within the safety bound [30, 70]. The samples are\ncollected by generating five rollouts of horizon 200 each. In the supplementary material we show the\nvelocity, gain, and shift observations of the random bounded batch. As a safety mechanism, we use a\nconstrained policy with the same bounds during training.\nFor the second experiment, we use a simple policy (referred to as medium) to generate the initial\nbatch, which tries to navigate to a fixed point in the state space, as also done in [45, 13]. The batch is\ncollected by randomly sampling a starting state in the bound [0, 100] and subsequently following:\nπβmedium(st) =\n\n\n\n50 −vt\n50 −gt\n50 −ht\n(11)\nAdditionally, the policy is augmented with 33% randomness. The velocity, gain, and shift observations\nof the medium policy batch are illustrated in the Appendix. The medium policy can be thought of as\nan example of a human technician who instructs the observable states to remain close to a fixed small\nregion. The small region is, in this case, an ϵ-surrouding of state [50 (velocity), 50 (gain), 50 (shift)],\nwhere ϵ reflects the additional randomness introduced in the behavior policy. In this experiment we\nuse the soft-constrain safety mechanism and thus the objective given by Eq. (10) for policy training.\nIn both experiments, we perform multiple iterations of batch RL. We start the iterative process by\nlearning a dynamics model based on the available batch. A new batch is generated by executing each\ntrained policy over 200 time steps. To account for partial observability, we incorporate fifteen past\nobservations to construct the utilized state. The learned transition and reward models are subsequently\nused to learn an ensemble of ten policies through an ensemble model-based policy search. We rollout\nthe policies for a horizon of 100 with a discount factor of 1. Simulation models, policy, and reward\nfunctions are two-layer MLPs with 50 hidden units each. We repeat each experiment three times and\nreport average results. For diversity, we use the MinLSED diversity criterion from Eq. (9).\nExperiment 1: Constrained policy\nWe summarize the main finding of this experiment in Table 1a. We observe that diversity accelerates\n7\nCost (-Reward) over Iterations\nPolicy\nαd = 0.0\nαd = 0.15\nInitial data\nIteration0\nx\nx\n216.5\nIteration1\n203.5 ± 2.6\n204.3 ± 2.4\nx\nIteration2\n194.0 ± 4.5\n190.6 ± 1.0\nx\nIteration3\n189.2 ± 2.0\n186.5 ± 1.5\nx\nIteration4\n188.9 ± 4.8\n182.7 ± 1.2\nx\n(a) Constrained Policy.\nCost (-Reward) over Iterations\nPolicy\nαd = 0.0\nαd = 0.15\nInitial data\nIteration0\nx\nx\n234.0\nIteration1\n199.8 ± 3.4\n198.7 ± 3.5\nx\nIteration2\n197.4 ± 3.8\n197.0 ± 1.6\nx\nIteration3\n196.8 ± 15.2\n194.0 ± 1.4\nx\n(b) Soft Constraint.\nTable 1: Costs over iterations of IBRL with (αd = 0.15) and without diversity (αd = 0.0) with\nstandard error over 6 repetitions.\nthe loss reduction over increasing iterations. While the cost is also decreasing without a diversity\nloss (left column) it does so much slower and converges to a higher cost value. Because the safety\nconstraint is integrated in the policy specification directly, this experiment shows that diversity enables\nbetter exploration in the iterative batch scenario. We also note that adding diversity leads to more\nstable results which can be seen from the lower variation over the experimental repetitions. Figure 5\nshows the policy costs for virtual rollouts (curves) as well as the true costs after evaluation (straight\nlines). In addition to the aforementioned improvement over the batch iterations, we can also see that\nthe predicted costs are much more realistic compared to the true cost when utilizing diversity. We\ntheorize that this is due to the improved quality of the simulation model because it is trained on more\ndiverse trajectory data. By that, model bias decreases.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTraining Iterations\n170\n180\n190\n200\n210\n216\n220\n230\n212\n190\n188\n186\nCost\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nInitial Cost\nTrue Cost\n(a) αd = 0.0\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nTraining Iterations\n170\n180\n190\n200\n210\n216\n220\n230\n200\n192\n184\n176\nCost\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nInitial Cost\nTrue Cost\n(b) αd = 0.15\nFigure 5: Constrained Policy.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTraining Iterations\n170\n180\n190\n210\n216\n220\n230\n201\n193\n190\nCost\nIteration 1\nIteration 2\nIteration 3\nInitial Cost\nTrue Cost\n(a) αd = 0.0\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTraining Iterations\n170.0\n180.0\n190.0\n210.0\n216.0\n220.0\n230.0\n201.5\n192.0\n188.0\nCost\nIteration 1\nIteration 2\nIteration 3\nInitial Cost\nTrue Cost\n(b) αd = 0.15\nFigure 6: Soft Constrain.\nFigure 7: Cost loss across iterations of IBRL with soft constraint for different iterations. The straight\nlines depict the true costs in deployment per batch iteration, while curves the costs based on the\nlearned transition model over training. Dashed line corresponds to the cost of initial batch.\nExperiment 2: Safety as soft constraint\nWe summarize the main finding of this experiment in Table 1b. We observe that diversity accelerates\nthe loss reduction over increasing iterations, however, the difference is more modest compared to\nthe previous experiment. Still, we notice that the diversified policies lead to a much more robust\nprocedure: the variation over the experiment repetitions is significantly lower than in the no-diversity\nscenario. The results in Figure 6 show that there is no significant difference in model bias in both\nscenarios. We believe the reason for this is that our proposed safety mechanism Eq. (6) implicitly\nencourages diversity. In the case of diverse policies, the unnormalized likelihood will expand, so the\nimpact of the safety criterion will naturally decrease over the batch iterations.\n5\nConclusion\nIn this paper, we presented an algorithm for iterative batch reinforcement learning based on model-\nbased policy search. We extended the aforementioned method via a safety mechanism using two\ndistinct approaches and incorporated diversity to exploit the iterative nature of the problem. Our\nexperiments demonstrate the effectiveness of using an iterative process in an offline reinforcement\nlearning setting to enhance policy learning. Moreover, incorporating diversity provides targeted\nimprovements to the policies with each iteration compared to setups without diversity.\n8\nReferences\n[1] P. Swazinna, S. Udluft, and T. Runkler.\nOvercoming model bias for robust offline deep\nreinforcement learning. Engineering Applications of Artificial Intelligence, 104:104366, 2021.\n[2] T. Yu, A. Kumar, R. Rafailov, A. Rajeswaran, S. Levine, and C. Finn. Combo: Conservative\noffline model-based policy optimization. Advances in neural information processing systems,\n34:28954–28967, 2021.\n[3] D. Hein, S. Depeweg, M. Tokic, S. Udluft, A. Hentschel, T. A. Runkler, and V. Sterzing. A\nbenchmark environment motivated by industrial control problems. In 2017 IEEE Symposium\nSeries on Computational Intelligence (SSCI), pages 1–8. IEEE, 2017.\n[4] T. Matsushima, H. Furuta, Y. Matsuo, O. Nachum, and S. Gu. Deployment-efficient rein-\nforcement learning via model-based offline optimization. arXiv preprint arXiv:2006.03647,\n2020.\n[5] X. Hu, Y. Ma, C. Xiao, Y. Zheng, and Z. Meng. In-sample policy iteration for offline reinforce-\nment learning. arXiv preprint arXiv:2306.05726, 2023.\n[6] L.\nZhang,\nL.\nTedesco,\nP.\nRajak,\nY.\nZemmouri,\nand\nH.\nBrunzell.\nAc-\ntive\nlearning\nfor\niterative\noffline\nreinforcement\nlearning.\nIn\nNeurIPS\n2023\nWorkshop\non\nAdaptive\nExperimental\nDesign\nand\nActive\nLearning\nin\nthe\nReal\nWorld,\n2023.\nURL\nhttps://www.amazon.science/publications/\nactive-learning-for-iterative-offline-reinforcement-learning.\n[7] D. Ernst, M. Glavic, P. Geurts, and L. Wehenkel. Approximate value iteration in the reinforce-\nment learning context. application to electrical power system control. International Journal of\nEmerging Electric Power Systems, 3(1), 2005.\n[8] M. Riedmiller. Neural fitted Q iteration–first experiences with a data efficient neural reinforce-\nment learning method. In European Conference on Machine Learning, pages 317–328. Springer,\n2005.\n[9] M. Riedmiller, T. Gabel, R. Hafner, and S. Lange. Reinforcement learning for robot soccer.\nAutonomous Robots, 27(1):55–73, 2009.\n[10] S. Lange, T. Gabel, and M. Riedmiller. Batch reinforcement learning. In Reinforcement\nLearning, pages 45–73. Springer, 2012.\n[11] D. Hein, A. Hentschel, T. A. Runkler, and S. Udluft. Reinforcement learning with particle\nswarm optimization policy (PSO-P) in continuous state and action spaces. International Journal\nof Swarm Intelligence Research (IJSIR), 7(3):23–42, 2016.\n[12] S. Depeweg, J. M. Hernández-Lobato, F. Doshi-Velez, and S. Udluft. Learning and policy search\nin stochastic dynamical systems with bayesian neural networks. International Conference on\nLearning Representations, 2017.\n[13] D. Hein, S. Udluft, and T. A. Runkler. Interpretable policies for reinforcement learning by\ngenetic programming. Engineering Applications of Artificial Intelligence, 76:158–169, 2018.\n[14] S. Depeweg, J.-M. Hernandez-Lobato, F. Doshi-Velez, and S. Udluft. Decomposition of\nuncertainty in bayesian deep learning for efficient and risk-sensitive learning. In International\nConference on Machine Learning, pages 1184–1193. PMLR, 2018.\n[15] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy\ngradient algorithms. In International conference on machine learning, pages 387–395. PMLR,\n2014.\n9\n[16] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[17] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.\n[18] S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without explo-\nration. In International conference on machine learning, pages 2052–2062. PMLR, 2019.\n[19] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine. Stabilizing off-policy Q-learning via\nbootstrapping error reduction. In Advances in Neural Information Processing Systems, pages\n11761–11771, 2019.\n[20] Y. Wu, G. Tucker, and O. Nachum. Behavior regularized offline reinforcement learning. arXiv\npreprint arXiv:1911.11361, 2019.\n[21] A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative Q-learning for offline reinforcement\nlearning. arXiv preprint arXiv:2006.04779, 2020.\n[22] S. Fujimoto and S. S. Gu. A minimalist approach to offline reinforcement learning. Advances\nin neural information processing systems, 34:20132–20145, 2021.\n[23] I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning.\narXiv preprint arXiv:2110.06169, 2021.\n[24] N. Y. Siegel, J. T. Springenberg, F. Berkenkamp, A. Abdolmaleki, M. Neunert, T. Lampe,\nR. Hafner, N. Heess, and M. Riedmiller. Keep doing what worked: Behavioral modelling priors\nfor offline reinforcement learning. arXiv preprint arXiv:2002.08396, 2020.\n[25] Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline rl? In International\nConference on Machine Learning, pages 5084–5096. PMLR, 2021.\n[26] M. Yin and Y.-X. Wang. Towards instance-optimal offline reinforcement learning with pes-\nsimism. Advances in neural information processing systems, 34:4065–4078, 2021.\n[27] M. Deisenroth and C. E. Rasmussen. PILCO: A model-based and data-efficient approach\nto policy search. In 28th International Conference on Machine Learning (ICML-11), pages\n465–472, 2011.\n[28] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-\nbased deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International\nConference on Robotics and Automation (ICRA), pages 7559–7566. IEEE, 2018.\n[29] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn, and T. Ma. MOPO: Model-\nbased offline policy optimization. In Advances in Neural Information Processing Systems,\nvolume 33, pages 14129–14142, 2020.\n[30] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims. MOReL: Model-based offline\nreinforcement learning. arXiv preprint arXiv:2005.05951, 2020.\n[31] J. Li, X. Hu, H. Xu, J. Liu, X. Zhan, and Y.-Q. Zhang. Proto: Iterative policy regularized\noffline-to-online reinforcement learning. arXiv preprint arXiv:2305.15669, 2023.\n[32] M. S. Mark, A. Ghadirzadeh, X. Chen, and C. Finn. Fine-tuning offline policies with optimistic\naction selection. In Deep Reinforcement Learning Workshop NeurIPS 2022, 2022. URL\nhttps://openreview.net/forum?id=ELmiPlCOSw.\n[33] A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning\nwith offline datasets. arXiv preprint arXiv:2006.09359, 2020.\n10\n[34] S. Lee, Y. Seo, K. Lee, P. Abbeel, and J. Shin. Offline-to-online reinforcement learning\nvia balanced replay and pessimistic q-ensemble. In Conference on Robot Learning, pages\n1702–1712. PMLR, 2022.\n[35] X. Hu, Y. Ma, C. Xiao, Y. Zheng, and J. Hao. Iteratively refined behavior regularization for\noffline reinforcement learning. 2023.\n[36] J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural\ncontrollers. In Proc. of the international conference on simulation of adaptive behavior: From\nanimals to animats, pages 222–227, 1991.\n[37] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-\nsupervised prediction. In International conference on machine learning, pages 2778–2787.\nPMLR, 2017.\n[38] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills\nwithout a reward function. arXiv preprint arXiv:1802.06070, 2018.\n[39] Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, and C.-Y. Lee. Diversity-driven\nexploration strategy for deep reinforcement learning. Advances in neural information processing\nsystems, 31, 2018.\n[40] D. Yarats, D. Brandfonbrener, H. Liu, M. Laskin, P. Abbeel, A. Lazaric, and L. Pinto. Don’t\nchange the algorithm, change the data: Exploratory data for offline reinforcement learning.\narXiv preprint arXiv:2201.13425, 2022.\n[41] N. Lambert, M. Wulfmeier, W. Whitney, A. Byravan, M. Bloesch, V. Dasagi, T. Hertweck, and\nM. Riedmiller. The challenges of exploration for offline reinforcement learning. arXiv preprint\narXiv:2201.11861, 2022.\n[42] Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, and C.-Y. Lee. Diversity-driven\nexploration strategy for deep reinforcement learning. Advances in neural information processing\nsystems, 31, 2018.\n[43] J. Parker-Holder, A. Pacchiano, K. M. Choromanski, and S. J. Roberts. Effective diversity in\npopulation based reinforcement learning. Advances in Neural Information Processing Systems,\n33:18050–18062, 2020.\n[44] S. Kumar, A. Kumar, S. Levine, and C. Finn. One solution is not all you need: Few-shot\nextrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33:\n8198–8210, 2020.\n[45] P. Swazinna, S. Udluft, and T. Runkler. User-interactive offline reinforcement learning. Interna-\ntional Conference on Learning Representations, 2023.\n[46] D. Brandfonbrener, W. Whitney, R. Ranganath, and J. Bruna. Offline rl without off-policy\nevaluation. Advances in neural information processing systems, 34:4933–4946, 2021.\n[47] N. K. Sinha and M. P. Griscik. A stochastic approximation method. IEEE Transactions on\nSystems, Man, and Cybernetics, SMC-1(4):338–344, 1971. doi:10.1109/TSMC.1971.4308316.\n[48] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning.\nSIAM review, 60(2):223–311, 2018.\n[49] K. Mangalam, H. Girase, S. Agarwal, K.-H. Lee, E. Adeli, J. Malik, and A. Gaidon. It is not\nthe journey but the destination: Endpoint conditioned trajectory prediction. In Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part\nII 16, pages 759–776. Springer, 2020.\n11\n[50] C. Choi, J. H. Choi, J. Li, and S. Malla. Shared cross-modal trajectory prediction for au-\ntonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 244–253, 2021.\n[51] H. Zhao, J. Gao, T. Lan, C. Sun, B. Sapp, B. Varadarajan, Y. Shen, Y. Shen, Y. Chai, C. Schmid,\net al. Tnt: Target-driven trajectory prediction. In Conference on Robot Learning, pages 895–904.\nPMLR, 2021.\n12\nA\nDataset visualization for industrial benchmark\nA.1\nRandom bounded dataset\nData collected by uniform sampling from the action space such that the states are restricted to lie\nwithin the safety bound [30, 70]. The samples are collected by generating five rollouts of horizon 200\neach.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nHorizon\n0\n20\n30\n40\n60\n70\n80\n100\nVelocities\nRandom Bounded Batch: Velocities' Rollouts\n0\n25\n50\n75\n100\n125\n150\n175\n200\nHorizon\n0\n20\n30\n40\n60\n70\n80\n100\nGains\nRandom Bounded Batch: Gains' Rollouts\n0\n25\n50\n75\n100\n125\n150\n175\n200\nHorizon\n0\n20\n30\n40\n60\n70\n80\n100\nShifts\nRandom Bounded Batch: Shifts' Rollouts\nFigure 8: Bounded dataset for the industrial benchmark. Shown are trajectories of Velocity (left),\nGain (middle) and Shift (right).\nA.2\nMedium Policy with Randomness\nWwe use a simple policy (referred to as medium) to generate the initial batch, which tries to navigate\nto a fixed point in the state space, as also done in [45, 13]. The batch is collected by randomly\nsampling a starting state in the bound [0, 100] and subsequently following:\nπβmedium(st) =\n\n\n\n50 −vt\n50 −gt\n50 −ht\n(12)\nAdditionally, the policy is augmented with 33% randomness.\n0\n50\n100\n150\n200\n250\nHorizon\n0\n20\n40\n60\n80\n100\nVelocities\nMedium Reward Batch: Velocities' Rollouts\n0\n50\n100\n150\n200\n250\nHorizon\n0\n20\n40\n60\n80\n100\nGains\nMedium Reward Batch: Gains' Rollouts\n0\n50\n100\n150\n200\n250\nHorizon\n0\n20\n40\n60\n80\n100\nShifts\nMedium Reward Batch: Shifts' Rollouts\nFigure 9: Velocity, gain and shift rollouts generated by following the medium behavior policy starting\nfrom a random state.\n13\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2024-11-14",
  "updated": "2024-11-14"
}