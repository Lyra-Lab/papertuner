{
  "id": "http://arxiv.org/abs/2411.13920v2",
  "title": "iHQGAN: A Lightweight Invertible Hybrid Quantum-Classical Generative Adversarial Network for Unsupervised Image-to-Image Translation",
  "authors": [
    "Xue Yang",
    "Rigui Zhou",
    "ShiZheng Jia",
    "YaoChong Li",
    "Jicheng Yan",
    "ZhengYu Long",
    "Wenyu Guo",
    "Fuhui Xiong",
    "Wenshan Xu"
  ],
  "abstract": "Leveraging quantum computing's intrinsic properties to enhance machine\nlearning has shown promise, with quantum generative adversarial networks\n(QGANs) demonstrating benefits in data generation. However, the application of\nQGANs to complex unsupervised image-to-image (I2I) translation remains\nunexplored. Moreover, classical neural networks often suffer from large\nparameter spaces, posing challenges for GAN-based I2I methods. Inspired by the\nfact that unsupervised I2I translation is essentially an approximate reversible\nproblem, we propose a lightweight invertible hybrid quantum-classical\nunsupervised I2I translation model - iHQGAN, by harnessing the invertibility of\nquantum computing. Specifically, iHQGAN employs two mutually approximately\nreversible quantum generators with shared parameters, effectively reducing the\nparameter scale. To ensure content consistency between generated and source\nimages, each quantum generator is paired with an assisted classical neural\nnetwork (ACNN), enforcing a unidirectional cycle consistency constraint between\nthem. Simulation experiments were conducted on 19 sub-datasets across three\ntasks. Qualitative and quantitative assessments indicate that iHQGAN\neffectively performs unsupervised I2I translation with excellent generalization\nand can outperform classical methods that use low-complexity CNN-based\ngenerators. Additionally, iHQGAN, as with classical reversible methods, reduces\nthe parameter scale of classical irreversible methods via a reversible\nmechanism. This study presents the first versatile quantum solution for\nunsupervised I2I translation, extending QGAN research to more complex image\ngeneration scenarios and offering a quantum approach to decrease the parameters\nof GAN-based unsupervised I2I translation methods.",
  "text": "iHQGAN:A Lightweight Invertible Hybrid\nQuantum-Classical Generative Adversarial Networks for\nUnsupervised Image-to-Image Translation\nXue Yanga,b, Rigui Zhou†a,b, ShiZheng Jiaa,b, YaoChong Lia,b, Jicheng\nYana,b, ZhengYu Longa,b, Wenyu Guoa,b, Fuhui Xionga,b, Wenshan Xua,b\naSchool of Information Engineering, Shanghai Maritime\nUniversity, Shanghai, 201306, China\nbResearch Center of Intelligent Information Processing and Quantum Intelligent\nComputing, Shanghai, 201306, China\nAbstract\nLeveraging quantum computing’s intrinsic properties to enhance machine\nlearning has shown promise, with quantum generative adversarial networks\n(QGANs) demonstrating benefits in data generation. However, the application\nof QGANs to complex unsupervised image-to-image (I2I) translation remains\nunexplored. Moreover,classical neural networks often suffer from large param-\neter spaces, posing challenges for GAN-based I2I methods. Inspired by the\nfact that unsupervised I2I translation is essentially an approximate reversible\nproblem, we propose a lightweight invertible hybrid quantum-classical unsu-\npervised I2I translation model — iHQGAN, by harnessing the invertibility\nof quantum computing.Specifically,iHQGAN employs two mutually approx-\nimately reversible quantum generators with shared parameters, effectively\nreducing the parameter scale. To ensure content consistency between gener-\nated and source images, each quantum generator is paired with an assisted\nclassical neural network (ACNN), enforcing a unidirectional cycle consistency\nconstraint between them. Simulation experiments were conducted on 19 sub-\ndatasets across three tasks. Qualitative and quantitative assessments indicate\nthat iHQGAN effectively performs unsupervised I2I translation with excellent\ngeneralization and can outperform classical methods that use low-complexity\nCNN-based generators. Additionally, iHQGAN, as with classical reversible\nmethods, reduces the parameter scale of classical irreversible methods via a\nreversible mechanism. This study presents the first versatile quantum solution\nfor unsupervised I2I translation, extending QGAN research to more complex\nPreprint submitted to Expert Systems with Applications\nNovember 26, 2024\narXiv:2411.13920v2  [quant-ph]  25 Nov 2024\nimage generation scenarios and offering a quantum approach to decrease the\nparameters of GAN-based unsupervised I2I translation methods.\nKeywords:\nQuantum machine learning, Unsupervised image-to-image\ntranslation, Quantum generative adversarial network, Quantum Computing\n1. Introduction\nQuantum computing Preskill (2012); Harrow and Montanaro (2017) is a\nnew computational paradigm that leverages the unique properties of quantum\nmechanics. Quantum machine learning (QML) combines quantum computing\nwith machine learning, aiming to enhance the performance of machine learn-\ning(ML) by leveraging quantum computing’s advantages. Currently, we are in\nthe Noisy Intermediate-Scale Quantum (NISQ) era Preskill (2018), character-\nized by a limited number of controllable qubits that are prone to noise. A key\ndirection of QML is the hybrid quantum-classical models Callison and Chan-\ncellor (2022) based on variational quantum algorithms(VQA) Cerezo et al.\n(2021), which are well-suited for NISQ devices and have shown remarkable\nperformance in domains like data generation Lloyd and Weedbrook (2018).\nRecently, QGANs have gained attention in quantum image generation and\nachieved significant milestonesHuang et al. (2021); Tsang et al. (2023); Zhou\net al. (2023); Chu et al. (2023); Silver et al. (2023). However, they are still\nin the early stages of development, and their application to more intricate\nimage generation scenarios remains an area for further exploration.\nOne particularly challenging task in the field of image generation is\nunsupervised I2I translation Zhu et al. (2017). Unsupervised I2I translation is\nan approximately reversible task that learns bidirectional mappings between\ntwo domains without paired training data. It enables image transfer between\nthe source and target domains while ensuring content consistency, such\nas structural and spatial consistency. Mathematically, the reversibility of\nI2I translation can be characterized using inverse functions. Specifically,\nthis involves identifying a function f that maps images from the source\ndomain to the target domain, along with its inverse function f −1, which maps\nimages from the target domain back to the source domain. Unsupervised I2I\ntranslation underpins the performance of various applications in many areas,\nsuch as super-resolution Yuan et al. (2018); Zhang et al. (2019), image style\ntransfer Tomei et al. (2019); Richardson et al. (2021), and image inpainting\nZhao et al. (2020); Huang et al. (2017b); Demir and Unal (2018). Classical\n2\nmethods often enhance the efficiency of image feature learning by incorporating\ncomputationally intensive modules such as attention mechanisms Emami\net al. (2020); Tang et al. (2021), resulting in a significant increase in model\nparameters. Moreover, for multi-target I2I translation Bhattacharjee et al.\n(2020); Lee et al. (2018), utilizing multiple generators to transfer images into\ndifferent target domains also enlarges the parameter scale.\nExisting methods for reducing parameters in unsupervised I2I translation\ncan be broadly categorized into irreversible-based and reversible-based ap-\nproaches. The former focuses on reducing the parameters of I2I translation\nmodels through techniques such as knowledge distillation Bhattacharjee et al.\n(2020); Lee et al. (2018), neural architecture search (NAS) Yang et al. (2021);\nvan Wyk and Bosman (2019), or improvements to heavyweight modules Deng\net al. (2023). However, despite efforts to minimize the number of parameters,\nthese models fail to leverage the approximate reversibility of unsupervised\nI2I translation. For reversible-based methods, most of the work incorporates\ninvertible neural networks (INNs) Dai and Tang (2021); van der Ouderaa and\nWorrall (2019) to construct reversible models. Due to the high computational\noverhead of INNs, they are commonly utilized as components within the\noverall framework. The reversible components typically handle data that has\nalready undergone dimensionality reduction in the initial phases. Additionally,\nthere is a study Shen et al. (2020) that proposes a self-inverse network for\nunpaired image-to-image translation. This approach involves augmenting the\ntraining data by swapping inputs and outputs during the training process,\nalong with implementing separated cycle consistency loss for each mapping\ndirection. Reversible methods can allow parameter sharing and thus reduce\nthe number of parameters. Inspired by this, we consider constructing a\nreversible model by utilizing the invertibility of quantum computing, offering\na novel quantum solution to reduce the parameter scale.\nIn light of the challenges mentioned above, our research presents an invert-\nible hybrid quantum-classical unsupervised I2I translation model—iHQGAN.\nBy combining the approximate reversibility of unsupervised I2I tasks with the\ninherent reversibility of quantum computing, iHQGAN provides a quantum\nsolution for unsupervised I2I translation, addressing the challenge of large\nparameter scales in GAN-based methods.\nIn iHQGAN, we designed two mutually approximately reversible quantum\ngenerators with shared parameters, effectively reducing the parameter scale.\nTo maintain content consistency between the input and output images, we\ndeployed an assisted classical neural network (ACNN) for each quantum\n3\ngenerator, ensuring the implementation of a unidirectional cycle consistency\nconstraint between them. Our research contributions can be summarized as\nfollows:\n1. iHQGAN is the first versatile quantum method for unsupervised I2I\ntranslation. It integrates the approximate reversibility of unsupervised\nI2I translation with the invertibility of quantum computing, extending\nthe research of QGANs to more complex scenarios and reducing the\nparameter scale of GAN-based unsupervised I2I translation methods.\n2. iHQGAN leverages the invertibility of quantum computing to design two\nmutually reversible quantum circuits with shared parameters, thereby\neffectively reducing the parameter scale.\n3. To preserve content consistency between the generated and source\nimages,iHQGAN deploys an ACNN for each quantum generator, intro-\nducing a unidirectional cycle consistency constraint between them.\n4. Simulation experiments were conducted on 19 sub-datasets across three\ntasks.\nBoth qualitative and quantitative assessments indicate that\niHQGAN effectively executes unsupervised I2I translation with robust\ngeneralization, and can outperform classical methods that use low-\ncomplexity CNN-based generators. Additionally, iHQGAN, similar to\nclassical reversible methods, reduces the parameter scale of classical\nirreversible methods by introducing the reversible mechanism.\n2. Preliminaries\n2.1. Fundamentals of Quantum Computing\nThe basic unit for computing and storing information in quantum comput-\ning is the quantum bit (qubit). The superposition state of a single quantum\nbit can be represented as |ψ⟩= α |0⟩+ β |1⟩,where α and β are complex\nnumbers called probability amplitudes, satisfying |α|2 + |β|2 = 1. When\nquantum measurements are performed on |ψ|, with probability |α|2 ,the state\ncollapses to |0⟩, and with probability |β|2, it collapses to |1⟩. The entangled\nstate means that the state of one qubit is inherently linked to the state of\nthe other, regardless of the distance between them. A typical example of\nan entangled state is (|00⟩+ |11⟩)/\n√\n2, where there is a special correlation\n4\nbetween two qubits such that no matter how far apart they are, measurements\non one immediately affect the state of the other.\nQuantum gates can manipulate quantum states. The most commonly used\nsingle-qubit gates are Rx(θ), Ry(θ), and Rz(θ). A two-qubit gate can create\nentanglement between two qubits. For example, the controlled NOT (CNOT)\ngate. The CZ gate acts on two qubits, the control qubit and the target qubit.\nThe ROT gate is defined as Rot(α, β, γ) = Rz(γ)Ry(β)Rz(α).In this paper,\nthe symbol R is used to denote the ROT gate.All quantum gates are unitary.\nThe unitary nature of quantum system evolution inherently guarantees\nthe invertibility of quantum computations. According to the fundamental\nprinciples of quantum mechanics, the time evolution of a quantum system is\ndescribed by the Schr¨odinger equation, which governs the evolution of the state\nvector|ψ(t)⟩from an initial time t0 to a later time t: ih ∂\n∂t|ψ(t)⟩= bH|ψ(t0)⟩.\nAs time evolves from t0 to t, the quantum state transitions from |ψ(t0)⟩to\n|ψ(t)⟩. This evolution can be expressed using a unitary operator U(t, t0),\nwhich is given by: |ψ(t)⟩= U (t, t0) |ψ (t0)⟩, where U(t, t0) = e−i ˆH(t−t0)/ℏis\na unitary operator satisfying U †U = UU † = I. Due to the unitarity of this\noperation, the initial state can also be reconstructed from the final state as\nfollows: |ψ(t0)⟩= U † (t, t0) |ψ (t)⟩.\nVariational quantum algorithms (VQA) solve specific problems by opti-\nmizing the parameters of parameterized quantum circuits (PQC) to minimize\na target function. A PQC is composed of quantum registers, parameterized\nquantum gates, and measurement operations. In a PQC, the first step is to\nprepare the initial quantum state |α⟩for input. Then, a series of unitary\noperations (quantum gates) is applied, and the final output quantum state is\nobtained through a measurement operator ˆO. A PQC can be represented as :\nf(θ) =\nD\nα\n\f\f\fU †(θ) ˆOU(θ)\n\f\f\f α\nE\n(1)\nWhere θ = (θ1, θ2, · · · ) is the set of parameters of the quantum gate, U repre-\nsents the unitary transformation of the quantum gates, ˆO is the measurement\noperator and f(θ) is the loss function defined according to the specific task.\n2.2. Quantum Generative Adversarial Networks(QGANs)\nGANs were first introduced by Goodfellow et al. in 2014 Goodfellow et al.\n(2020). The key idea of GANs is to achieve a Nash equilibrium between the\ngenerator and the critic by having them compete with each other, compelling\nthe generator to produce images that the critic cannot distinguish from\n5\nreal images. Primitive GANs often suffer from difficult convergence and\nmode collapse problems. To address these issues, researchers have proposed\nvarious variants. Notably, WGANs Arjovsky et al. (2017) introduced the\nWasserstein distance as a novel cost function. Building on this, WGANs-GP\nGulrajani et al. (2017) introduced the gradient penalty as a replacement\nfor weight clipping, enforcing the implementation of the Lipschitz constraint\nto further enhance training stability compared with WGANs.GANs have\nachieved impressive results in unsupervised I2I translation Yuan et al. (2018);\nZhang et al. (2019); Tomei et al. (2019); Huang et al. (2017a); Zhao et al.\n(2020); Huang et al. (2020); Demir and Unal (2018).\nInspired by GANs, Lloyd et al.Lloyd and Weedbrook (2018) introduced the\nconcept of QGANs in 2018, integrating quantum computing mechanisms into\nthe GAN framework. They demonstrated that QGANs exhibit an exponen-\ntial advantage over classical methods when using data consisting of samples\nfrom measurements made in high-dimensional spaces. Like classical GANs,\nQGANs also include generators and critics trained against each other. Lloyd et\nal.Lloyd and Weedbrook (2018) emphasized that the generator must be quan-\ntum, whereas the critic can be either classical or quantum. Dallaire-Demers\net al.Dallaire-Demers and Killoran (2018) successfully trained a quantum\nequivalent of the conditional GAN, named QuGAN, thereby validating the\ntheory proposed by Lloyd et al. The objective function of QGANs often\nleverages classical distance metrics, such as Wasserstein distance Tsang et al.\n(2023), and binary cross-entropy Huang et al. (2021); Zoufal et al. (2019);\nSitu et al. (2020). When both the generator and critic are quantum in QGAN,\nobjective functions can also use measures of distance between quantum states,\nincluding quantum state fidelity Stein et al. (2021), quantum Wasserstein\ndistance Chakrabarti et al. (2019), and quantum R´enyi distance Kieferova\net al. (2021). Recently, QGANs have demonstrated successful applications in\nproducing diverse data including target quantum states Benedetti et al. (2019),\nelectrocardiogram (ECG) data Qu et al. (2024) , chemical small molecules\nLi et al. (2021), and images generation Li et al. (2021); Huang et al. (2021);\nZhou et al. (2023). Currently, QGANs are still in the early exploratory stage\nin the field of image generation. Due to limitations in quantum resources, re-\nsearchers often employ dimensionality reduction techniques, such as principal\ncomponent analysis (PCA) Silver et al. (2023) or autoencoders Chang et al.\n(2024), during the preprocessing phase of image data. The low-dimensional\ndata generated by the quantum generator is used in the post-processing phase\nto reconstruct high-dimensional data. Recently, Huang et al. Huang et al.\n6\n(2021) introduced a quantum patch strategy that segments images into smaller\npatches, utilizing multiple quantum generators to independently generate the\ncorresponding patches, which are then combined to form a complete image.\nThis method effectively leverages the limited number of qubits, enabling\nthe generation of 8x8 images without the need for dimensionality reduction.\nBuilding on this, Tsang et al. Tsang et al. (2023) proposed PQWGAN, which\nincorporates Wasserstein distance to generate higher-resolution 28x28 images\nwithout dimensionality reduction. This advancement represents a significant\nbreakthrough in the field of quantum image generation and establishes a\nfoundation for the application of QGANs in more complex image generation\nscenarios.\n2.3. Unsupervised I2I translation\nUnsupervised I2I translation aims to translate images from the source\ndomain to the target domain without paired training data while maintaining\ncontent consistency, including structural consistency. Before CycleGAN Zhu\net al. (2017), several methods mainly relied on prior knowledge Isola et al.\n(2017) or shared weights Liu and Tuzel (2016) to relate the two domains\nfor addressing the challenges of the unpaired setting. CycleGAN provides\na generalized solution for achieving two-sided unsupervised I2I translation\nby utilizing bidirectional cycle consistency constraints, without depending\non task-specific prior knowledge. The underlying intuition behind the cycle\nconsistency constraints is that when an image is translated from one domain\nto the other and then back again, it should arrive where it started.\nHowever, relying solely on bidirectional cycle consistency constraints may\nlead to loose restrictions and content distortion. To address this problem, re-\nsearchers proposed feature-level losses Liu and Tuzel (2016); Tang et al. (2021),\nwhich extract feature maps using convolutional neural networks (CNNs) and\nevaluate the content distance in the deep feature space. QGAN C Chen et al.\n(2019)introduces image quality evaluation measures such as perceptual loss,\nallowing for direct comparison of the pixel-wise similarity between the original\nand reconstructed images rather than comparing feature similarity. Recent re-\nsearch has shown that one-sided unsupervised I2I translation can be achieved\nusing unidirectional cycle consistency constraints, thereby simplifying the\narchitecture and training process compared with two-sided unsupervised I2I\ntranslation. DistanceGAN Benaim and Wolf (2017) and GCGAN Fu et al.\n(2019) introduce an implicit distance in one-sided unsupervised I2I translation,\nshowcasing competitive translation performance across various applications.\n7\nInvertible neural networks (INNs) Dinh et al. (2014); Jacobsen et al. (2018);\nKingma and Dhariwal (2018); Dinh et al. (2016) have emerged as a crucial tool\nfor developing reversible methods in unsupervised image translation due to\ntheir reversible nature. However, INNs require high computational overhead\nand are often integrated as a component of the overall model framework,as\nexemplified by iFlow Dai and Tang (2021) and RevGAN van der Ouderaa and\nWorrall (2019).The reversible component specifically operates on data that\nhave already undergone dimensionality reduction during the initial phases of\nthe framework. There is another reversible approach Shen et al. (2019)where\na fully self-reversible generator can be achieved by augmenting the training\ndata through swapping inputs and outputs during the training process, while\nalso implementing separate cycle consistency loss for each mapping direction.\nThese reversible I2I methods can greatly decrease the number of parameters.\n3. iHQGAN\n3.1. Problem Formulation\nWe redefine the concept of I2I translation from a quantum perspective,\naiming to identify a forward quantum mapping F: X →Y and a correspond-\ning backward quantum mapping B: Y →X, such that B ◦F = F ◦B = I.\nGiven paired training samples x = {xi}N\ni=1, y = {yi}N\nj=1 ,where x ∼pdata (x),\ny ∼pdata (y). The distributions generated by G and F are denoted pG and\npF, respectively. The quantum states |ϕ⟩= {|ϕ⟩i}N\ni=1 and |ψ⟩= {|ψ⟩j}N\nj=1\ncorresponding to the training samples x and y , respectively.\n3.2. Overview\nLeveraging the inherent approximate reversibility of I2I translation, we\npropose a groundbreaking reversible hybrid quantum-classical model, iHQ-\nGAN, which capitalizes on the unique invertibility of quantum computing, as\nillustrated in Fig. 1. iHQGAN builds upon the PQWGAN Tsang et al. (2023)\nframework and introduces two key mechanisms: (1) by harnessing the invert-\nibility of quantum computing, we have designed two mutually approximately\nreversible quantum generators with shared parameters, thereby significantly\nreducing the parameter scale; (2) we incorporate ACNNs to implement a\nunidirectional cycle consistency constraint between each quantum generator\nand its corresponding ACNN, ensuring content consistency between input\nand output images. Ultimately, through the synergistic interplay of these\nmechanisms, iHQGAN effectively achieves unsupervised I2I translation.\n8\nQuantum generator\nG\n\n( )\ni\ndata\nx\nP\nX\n( )\ni\ndata\ny\nP\nY\niy \np\ni\n\n4i\n\n3\ni\n\n2\ni\n\n1\ni\n\np\ni\n\n4\ni\n\n3\ni\n\n2\ni\n\n1\ni\n\np\nj\n4\nj\n3j\n1j\np\nj\n4\nj\n3\nj\n2j\n1\nj\n2j\nadversarial loss\nAssisted classical neural \nnetworks\n Assisted classical neural \nnetworks\nQ\n\n\n\n\n\n\n\n\n\n\n2\nU\n3\nU\n4\nU\n1\nU\nR\nF\n\nconsistency loss\nconsistency loss\n'\nix\nQuantum generator\nCritic\nCritic\ny\nx\nx\nD\ny\nD\nadversarial loss\n†\n1\nU\n†\n2\nU\n†\n3\nU\n†\n4\nU\n†\np\nU\np\nU\n(\n)\nG\nG \n(\n)\nF\nF \nFigure 1: The overall architecture of iHQGAN. iHQGAN consists of two quantum generators,\ntwo assisted classical networks(ACNNs), and two classical critics. The quantum generators\nG and F comprise p sub-quantum generators. ACNNs are utilized to achieve consistency\nloss, while the critics implement adversarial loss. During the alternating training of the two\nquantum generators, their respective parameters θG and ωF are interchangeably assigned\nto facilitate parameter sharing.\n3.3. Quantum Generator\nLeveraging the inherent invertibility of quantum computing, we have\ndeveloped two quantum generators, denoted by G and F. The parameters\nassociated with these generators are represented by θG and ωF, respectively.\nEach quantum generator is implemented with a set of quantum circuits. For\neach quantum circuit in G,there is a corresponding quantum circuit in F.\nTo construct a reversible quantum circuit, we need to negate the parameter\nvalues of the quantum gates in the original circuit and subsequently arrange\nall quantum gates in reverse order.Consequently, two mutually reversible\nquantum circuits are able to share a common set of parameters.\niHQGAN is built upon the PQWGAN framework Tsang et al. (2023),\nwhich employs a patch-based strategy for image generation. Specifically,\nthe generators G and F are composed of p small circuits, denoted by uk,\nul (k, l = 1...p).The parameters of these circuits are represented as (θG)k\nand (ωF)l, respectively. When k = l, the circuits satisfy the relationship\n9\nukul = uluk = I, indicating they are mutual inverses, with ul being the\nconjugate transpose of uk, expressed as uk = u†\nl.\nTo derive ul, we first\nnegate the parameter values of the quantum gates in uk and then rearrange\nthese quantum gates in reverse order. Each quantum sub-circuit consists\nof N(N = 1, 2, ..n) qubits and includes an amplitude encoding module, a\nmeasurement module , and S(S = 1, 2, ..f) repeated parameterized blocks.\nEach block consists of a rotation layer and an entanglement layer. The\nrotation layer is formed by an assembly of single-qubit rotation gates R, and\nthe entanglement layer is formed by an assembly of CNOT gates. Only R\ngates have parameters, with three parameters per gate.\nWe continue to standardize and detail the notation for every parameter of\nthe gates within each block. The set of parameters for the k −th sub-circuit\nuk is denoted by θk\nG. For uk, the parameters of the quantum gate for the n−th\nqubit at f −th block include αf,n\nk , βf,n\nk , and σf,n\nk . Similarly, ωl\nG denotes the set\nof parameters for the l−th sub-circuit ul. For ul, the quantum gate parameters\nfor the n −th qubit at f −th block include αn,f\nl\n, β\nn,f\nl\n, and σn,f\nl\n.When\nk = l , we have ul=uk†. The correspondence of the parameters between\nthe two subcircuits can be expressed as αn,f\nk\n= αn,S−f+1\nl\n,β\nn,f\nk\n= β\nn,S−f+1\nl\n,and\nσn,f\nk\n= σn,S−f+1\nl\n. In this study, the quantum generators G and F each consist\nof p = 32 sub-circuits. Each quantum sub-circuit has N = 5 qubits and\nS = 12 parameterized blocks. Fig.2 illustrates the structures of the two\ntypes of quantum circuits uk and ul.More details of the quantum circuits\nare provided in Appendix A. Table.1 presents the common details of two\nquantum generators G and F.\nTable 1: The common details of two quantum generators G and F\nNumber of circuit\nQubits of per\ncircuit\nLayers of per\ncircuit\nGates of per\ncircuit\nParameter count\nof per circuit\n32\n5\n24\n60\n5760\nLast, we introduce the process of image translation using the quantum\ngenerators G and F. The source images xi and yj are segmented into patches\nx1\ni , . . . , xp\ni and y1\nj, . . . , yp\nj,respectively. These patches are then fed into the cor-\nresponding quantum circuits of the quantum generators G and F. The initial\nquantum states {|ϕ⟩1\ni , . . . , |ϕ⟩p\ni } and {|φ⟩1\nj, . . . , |φ⟩p\nj} are prepared by encoding\nthe pixel values of each image patch onto N qubits using the amplitude encod-\ning component of each quantum sub-circuit. Subsequently, the initial state\n{|ϕ⟩1\ni , · · · , |ϕ⟩p\ni } undergoes unitary evolution to yield a new quantum state\n10\n  repeated     times\nAmplitude \nCoding\n（a）\nrotational layer\nentanglement layer\nk\nu\n  repeated     times\nAmplitude \nCoding\n（b）\nrotational layer\nentanglement layer\n(\n)\n,5\n5,\n5,\n,\n,\nf\nf\nf\nk\nk\nk\nR \n\n\n(\n)\n5,\n5,\n5,\n,\n,\nf\nf\nf\nl\nl\nl\nR \n\n\n(\n)\n,\n,\n,\n1\n1\n1\n,\n,\nf\nf\nf\nk\nk\nk\nR \n\n\n(\n)\n,\n,\n2,\n2\n2\n,\n,\nf\nf\nf\nk\nk\nk\nR \n\n\n(\n)\n,\n,\n,\n3\n3\n3\n,\n,\nf\nf\nf\nk\nk\nk\nR \n\n\n(\n)\n,\n,\n,\n4\n4\n4\n,\n,\nf\nf\nf\nk\nk\nk\nR \n\n\n(\n)\n4,\n4,\n4,\n,\n,\nf\nf\nf\nl\nl\nl\nR \n\n\n(\n)\n3,\n3,\n3,\n,\n,\nf\nf\nf\nl\nl\nl\nR \n\n\n(\n)\n2,\n2,\n2,\n,\n,\nf\nf\nf\nl\nl\nl\nR \n\n\n(\n)\n1,\n1,\n1,\n,\n,\nf\nf\nf\nl\nl\nl\nR \n\n\n12\n12\nlu\nFigure 2: The structure of the two types of quantum circuits.\nSubfigure(a) depicts\nthe k −th quantum sub-circuit of quantum generator G.\nSubfigure(b) presents the\nl −th quantum sub-circuit of quantum generator F.\nEach quantum sub-circuit has\nN = 5 qubits and S = 12 blocks. When k = l, the relationship ul=uk† holds. The\ncorrespondence of the parameters between the two quantum circuits can be expressed as\nαn,f\nk\n= αn,S−f+1\nl\n,β\nn,f\nk\n= β\nn,S−f+1\nl\n,and σn,f\nk\n= σn,S−f+1\nl\n|φ⟩′i = {|φ⟩′1\ni , · · · , |φ⟩′p\ni }. Similarly, |φ⟩j =\n\b\n|φ⟩1\nj, · · · , |φ⟩p\nj\n\t\nevolves unitarily\nto produce the output quantum state |ϕ⟩′j =\n\b\n|ϕ⟩′1\nj, · · · , |ϕ⟩′5\nj\n\t\n. Following\nthe unitary evolution in each sub-quantum circuit, quantum measurements\nare performed on the output quantum states to extract classical pixel values\n{xi′1, ...xp\ni ′} and {yi′1, ...yi′p}. Ultimately, these pixel values are reassembled\nto construct the complete translated images xi′ and yj′.\n3.4. Unidirectional cycle consistency constraint\nIn the realm of classical unsupervised I2I translation, researchers pro-\nposed the strategy of unidirectional cycle consistency constraints between two\ngenerators, with one acting as the primary generator and the other as the\nauxiliary generator Fu et al. (2019); Benaim and Wolf (2017). Additionally,\nnumerous studies have demonstrated that classical components can enhance\nthe performance of quantum circuits Callison and Chancellor (2022); Chang\net al. (2024); Shu et al. (2024). Building on the above two points, we intro-\nduced the strategy of unidirectional cycle consistency constraint to iHQGAN,\nimplementing it between each quantum generator and its corresponding as-\nsisted classical neural networks (ACNN), where the ACNN helps optimize\nthe parameter space of the quantum generator. While bidirectional cycle\nconsistency constraints are commonly applied between two generators in the\n11\nX\n  \nG\nY^\nX^\nX\nY^\nX^\nY\nX^\nY^\n（b）\n（1）\n（1）\n（2）\nY\nX^\nY^\n（a）\n（2）\n  \nF\n  \nG\n  \nF\n  \nG\n  \nF\n  \nQ\n  \nR\n✱\n✱\n✱\n✱\nFigure 3: Two ways of implementing cycle-consistent constraints. Left: bidirectional\ncycle consistency constraints between two classical generators.Sub-figure(1) shows forward\ncycle-consistent constraints x →G∗(x) →F ∗(G∗(x)) ≈x. Sub-figure(2) shows backward\ncycle consistency constraints x →G∗(y) →F ∗(G∗(y)) ≈y Right: The unidirectional\nconstraints between each quantum generator and its corresponding ACNN. Sub-figure(1)\nshows forward cycle consistency constraints x →G(x) →Q(G(x)) ≈x. Sub-figure(2)\nshows forward cycle consistency constraints x →F(y) →R(F(y)) ≈y\nclassical domain, the application of such constraints between two quantum\ngenerators may hinder model performance due to the limited expressiveness\nof quantum generators. Fig.3 illustrates two different ways to implement\ncycle consistency constraints. The related discussion is detailed in Section\n4.4.2 of this paper.\nEach ACNN comprises six layers, including both an encoding block and a\ndecoding block. The four encoding layers and the first layer of the decoding\nblock utilize a LeakyReLU activation function with a slope of 0.05, while the\nlast layer of the decoding block employs a Tanh activation function.\n3.5. Loss Function\nAdversarial Loss The purpose of the adversarial loss function Goodfellow\net al. (2020) is to minimize the difference between the distribution of the\nlearned data and that of the real data . For the mapping function F: X →Y\nand its corresponding critic DY , the objective LDY\nGAN (G, DY , x, y) is formulated\nas:\nLDY\nGAN (G, DY , x, y) = Ey∼pdata (y)[DY (y)]−\nEy∼pdata (y)[DY (G(x))] −λEˆy∼Pdata\nˆ\n(y)\nh\u0000\r\r∇ˆyDY (ˆy)\n\r\r\n2 −1\n\u00012i\n(2)\nwhere ˆy is a distribution uniformly sampled between y ∼pdata (y) and\ny ∼pG and λ is the gradient penalty coefficient. A similar loss for the\n12\nbackward quantum mapping B: Y →X and its critic DX is denoted by\nLDY\nGAN (F, DX, x, y).\nLDX\nGAN (G, DXY , x, y) = Ex∼pdata (x)[DX(x)]−\nEx∼pdata (x)[DX(F(x))] −λEˆx∼Pdata\nˆ\n(x)\nh\n(∥∇ˆxDX(ˆx)∥2 −1)2i\n(3)\nFurther details on the critic can be found in Appendix B.\nCycle Consistency Loss Utilizing the cycle consistency loss Zhu et al.\n(2017), we implemented a unidirectional cycle consistency constraint between\neach quantum generator and its corresponding ACNN to maintain content\nconsistency.\nLG\ncyc (G, Q) = Ex∼pdata (x) [∥Q(G(x)) −x∥1]\n(4)\nLF\ncyc (G, F) = Ey∼pdata (y) [∥R(F(y)) −y∥1]\n(5)\nImage Quality-aware (IQA) Loss To generate higher fidelity images,\nChen et al. Chen et al. (2019) introduced a quality-aware loss that compares\nthe source and reconstructed images at the domain level. We chose the\nstructural similarity index (SSIM) metric for the image quality-aware loss\nto effectively maintain structural consistency between the input and output\nimages.\nLG\nIQA(G, Q) = 1 −SSIM(Q(G(x)), x)\n(6)\nLF\nIQA(F, R) = 1 −SSIM(R(F(y)), y)\n(7)\nThe complete objective function for each quantum generator is formulated as\nfollows:\nLG = ε ∗LG\nGAN + η ∗LG\ncyc + ρ ∗LG\nIQA\n(8)\nLF = ε ∗LF\nGAN + η ∗LF\ncyc + ρ ∗LF\nIQA\n(9)\nWhere ε, η, and ρ are the weight coefficients corresponding to the three\ncomponents.\n3.5.1. Training and optimization\nWe implemented an alternating training strategy that involves mutually\nassigning weights between the quantum generators G and F. During the\ntraining process, for each training batch, we first train G and update its\nparameters, then pass the updated parameters of G to F. Next, we train\nF and update its parameters, and then pass the updated parameters of F\n13\nback to G for further training. This process is repeated continuously until\nthe model reaches a Nash equilibrium. The complete training algorithm for\niHQGAN is detailed in Algorithm 1.\nAlgorithm 1 Pseudocode of training algorithm.\nInput: Gradient penalty coefficient λ, weight coefficients in the loss function\nper generator ϵ,η,ρ, critic iterations per generator iteration batch size nc,\nnumber of epochs nepochs, batch size m, Adam hyperparameters η1, η2,\nβ1, β2.random number ξ ∼U[0, 1]\n1: Initialize critic parameters κDy, κDx, generator parameters θG, ωF. θG\nand ωF contain parameters α, β, δ and α∗, β∗, δ∗respectively.\n2: α∗, β∗, δ∗:= α, β, δ\n3: batchnum := 0\n4: for epoch = 1, . . . , nepochs do\n5:\nfor i = 1, . . . , m do\n6:\nSample real data x ∼Pdata(x), y ∼Pdata(y)\n7:\nEncoding x into amplitude of |ϕ⟩\n8:\ny′ decoding\n←−\n|ψ⟩′ ←G(θG, |ϕ⟩)\n9:\nˆy ←ξy + (1 −ξ)y′\n10:\nLDY (i)\nGAN ←DY (y′) −DY (y) + λ\n\u0000∥∇ˆyDY (ˆy)∥2 −1\n\u00012\n11:\nκDy ←Adam\n\u0010\n1\nm\nPm\ni=1 LDY (i)\nGAN , κDy, η1, β1, β2\n\u0011\n12:\nEncoding y into amplitude of |ψ⟩\n13:\nx′ decoding\n←−\n|ψ⟩′ ←F(ωF, |ψ⟩)\n14:\nˆx ←ξx + (1 −ξ)x′\n15:\nLDX(i)\nGAN ←DX(x′) −DX(x) + λ (∥∇˜xDX(ˆx)∥2 −1)2\n16:\nκDx ←Adam\n\u0010\n1\nm\nPm\ni=1 LDX(i)\nGAN , κDx, η2, β1, β2\n\u0011\n17:\nbatchnum := batchnum + 1\n18:\nif batchnum/nc == 0 then\n19:\nEncoding x into amplitude of |ϕ⟩\n20:\ny′ decoding\n←−\n|ψ⟩′ ←G(θG, |ϕ⟩)\n21:\nˆy ←ξy + (1 −ξ)y′\n22:\nLG(i) ←ϵ(DY (y′)−DY (y)+λ\n\u0000∥∇ˆyDY (ˆy)∥2 −1\n\u00012)+η∥(Q(G(x))−\ny)∥1 + ρ(1 −SSIM(Q(G(x), x))\n23:\nθG ←Adam\n\u0000 1\nm\nPm\ni=1 LG(i), θG, η1, β1, β2\n\u0001\n24:\nα∗, β∗, δ∗:= α, β, δ\n25:\nEncoding y into amplitude of |ψ⟩\n14\n26:\nx′ decoding\n←−\n|ϕ⟩′ ←F(ωG, |ψ⟩)\n27:\nˆx ←ξx + (1 −ξ)x′\n28:\nLF(i) ←ϵ(DX(x′)−DX(x)+λ (∥∇ˆxDX(ˆx)∥2 −1)2)+η∥(R(F(Y ))−\nx)∥1 + ρ(1 −SSIM(R(F(y), y))\n29:\nθF ←Adam\n\u0000 1\nm\nPm\ni=1 LF(i), θF, η2, β1, β2\n\u0001\n30:\nα, β, δ:=α∗, β∗, δ∗\n31:\nend if\n32:\nend for\n33: end for\n3.5.2. Post-processing images generated by iHQGAN\nThe images generated by PQWGAN Tsang et al. (2023) often exhibit\nspecial discrete noise. This issue is also evident in iHQGAN, as illustrated in\nthe light blue dashed box of Fig.4(a). To improve image quality, we performed\nsimple post-processing on the generated images without significantly affecting\nthe features of the images directly output by iHQGAN. Specifically, we set the\npixel values to zero for rows i in the ranges [0, 7] and [26, 31]. The denoising\nresults are shown in Fig.4(b).\n（b）After post-processing \n（a）Before post-processing \nFigure 4: Post-processing of images generated by iHQGAN. (a) Images before post-\nprocessing; (b) Images after post-processing.\n4. Experiments\nThis section presents a rigorous experimental evaluation of the iHQGAN\nmodel, providing comprehensive empirical evidence to demonstrate its effec-\ntiveness, generalizability , and parameter-saving nature.\n15\n4.1. Datasets and Evaluation Metrics\nDatasets The current state-of-the-art QGANs are limited by quantum\nresources. They are suited for training on small grayscale image datasets, such\nas MNISTLeCun and Cortes (2010) and MNIST-CMu and Gilmer (2019).\nWe created three unpaired I2I translation tasks: Edge Detection, Font Style\nTransfer, and Image Denoising using MNIST datasets, resulting in a total of\n19 sub-datasets. Each sub-dataset comprises 1,250 images, with the training\nand test sets split in an 8:2 ratio. Fig.5 presents some examples from the\ndatasets. Table. 2 provides the basic details of datasets. Additional details\non the datasets can be found in Appendix C. To facilitate amplitude coding\nfor images using a 5-qubit quantum circuit, we preprocess the original 28×28\nimages by padding them to 32×32.\n Font  Style Transfer \nEdge Detection\n Image Denoise\nFigure 5: Examples of the datasets. The datasets consist of three I2I translation tasks\nwith 19 sub-datasets: 8 for Edge Detection(labels 0-7), 8 for Font Style Transfer (labels\n0-7), and 3 for Image Denoising (labels 0, 1, 7).\nTable 2: Datasets with basic setup used\nDataset\nNo.of training\nInstance\nNo.of test Instance\nSize of Instance\nNo.of Classes\nEdge Detection\n1000\n250\n28×28\n8 object categories\nFont Style Transfer\n1000\n250\n28×28\n8 object categories\nImage Denoising\n1000\n250\n28×28\n3 object categories\nEvaluation Metric For quantitative evaluation, we utilize Fr´echet incep-\ntion distance (FID), structural similarity index (SSIM), and peak signal-to-\nnoise ratio(PSNR) to assess the performance of the methods, as well as to\ninvestigate the number of parameters.\n(i) FID Silver et al. (2023) measures the distance between the distributions\nof the generated samples and the real samples. A lower score indicates\nthat the model can produce higher-quality images.The FID formula is:\nFID = ∥µr −µg∥2 + Tr\n\u0010\nΣr + Σg −2 (ΣrΣg)1/2\u0011\n(10)\n16\nwhere µr and µg represent the means of the features for real and\ngenerated images, respectively.Σr and Σg are the feature covariance\nmatrices.Tr denotes the matrix trace.\n(ii) SSIM Amirkolaee et al. (2022) evaluates the structural similarity between\ntwo images, with higher values reflecting greater similarity and aligning\nmore closely with human perception of image quality.The SSIM formula\nis:\nSSIM =\n(2µxµy + c1) (2σxy + c2)\n\u0000µ2\nx + µ2\ny + c1\n\u0001 \u0000σ2\nx + σ2\ny + c2\n\u0001\n(11)\nwhere,µx and µy denote the mean values of images x and y, σ2\nx and σ2\ny\nare the variances of images , and σxy represents their covariance.c1 =\n(0.01L)2 and c2 = (0.03L)2 are regularization constants.\n(iii) PSNR Kancharagunta and Dubey (2019) is a common metric for assess-\ning the difference between original and reconstructed images, focusing\nprimarily on pixel-level discrepancies. A higher PSNR value indicates\nless distortion and better quality of the generated samples.The PSNR\nformula is as follows:\nPSNR = 10 · log10\n\u0012 R2\nMSE\n\u0013\n(12)\nwhere R is the maximum pixel value in an image and MSE is the mean\nsquared error between two images.\n4.2. Implementation Details\nThe experiment was simulated using PyTorch, and PennyLane on a\n13th Gen Intel(R) Core(TM) i5-13490F CPU with 32.0GB of RAM. We\nutilized the Adam optimizer with initial learning rates of 0.0002 for classical\ncritics and ACNNs, and 0.01 for quantum generators. The two weight decay\nhyperparameters of the Adam optimizer were set to 0 and 0.9. The model\nunderwent training for 50 epochs with a batch size of 10. The hyperparameters\nfor the loss function of each discriminator, as defined in Equations 2 and 3\n, were set to λ = 10 and the hyperparameters of the loss function of each\nquantum generator, as defined in Equations 8 and 9, were set to ε = 10, η = 20,\nand ρ = 300.\n17\n4.3. Baselines\nEdge Detection and Font Style Transfer are utilized to evaluate iHQGAN’s\neffectiveness and demonstrate that iHQGAN can reduce the number of param-\neters in classical irreversible methods through a reversible mechanism, similar\nto classical reversible methods. We selected the classical irreversible method\nCycleGAN Zhu et al. (2017) and the reversible method One2One Shen et al.\n(2020) as baselines.Image Denoising is used to further assess the generaliz-\nability of iHQGAN, with the classical nonlinear,CNN-based CycleGAN and\nthe classical linear Gaussian filter serving as baselines.\nWe implement the CycleGAN and One2One using low-complexity CNN\ngenerators to preliminarily validate the aforementioned objective. Further\ndetails on the baselines are provided below. The generator configurations of\nthe baselines are summarized in Table 3. For the Gaussian filter, we utilized\nthe cv2.GaussianBlur function from the OpenCV libraryBradski (2000),\nwith a 5×5 Gaussian kernel size defining the neighborhood range, and the\nstandard deviation set to 0.\nTable 3: The generator configurations of baselines\nModel\nTask\nGenerator configurations (1Ck-2Dm)\nCycleGAN\nFont Style Transfer\nC12 −D1\nEdge Detection\nC70 −D1\nOne2One\nFont Style Transfer\nC12 −D1\nEdge Detection\nC70 −D1\n1 Ck denotes a 16 × 16 Convolution-LeakyReLU layer with k channels, stride 16, and no padding.\n2 Dm represents a 16 × 16 Deconvolution-LeakyReLU layer with m channels, stride 16, and no\npadding.\n4.4. Result\nWe conducted experiments on the Edge Detection, Font Style Transfer\ndatasets, and Image Denoising, yielding both quantitative and qualitative\nresults. To facilitate comparison, the methods involved are labeled as ’iHQ-\nGAN’, ’CycleGAN’, ’One2One’, and ’Gaussian filter’. The images from the\nsource domain are labeled as ’Source’, while the ground truth images manually\ncreated for comparison are labeled as ’GT’.\n18\n（a）Samples  results  for the  Edge →Digit direction.  \n   \n（b）Samples  results  for the Edge←Digit direction.  \niHQGAN\nCycleGAN\nSource\nGT\nOne2One\niHQGAN\nCycleGAN\nSource\nGT\nOne2One\nFigure 6: Qualitative comparison of outputs generated by various methods for the mapping\nEdge ↔Digit trained on the Edge Detection datasets. From top to bottom: Source,\nCycleGAN, One2One, iHQGAN, and GT(ground truth). The images generated by iHQGAN\neffectively achieve domain transfer while maintaining structural consistency with the source\nimages. They are smoother, with no noticeable pixel loss. The green dashed and solid\nlines highlight that CycleGAN and One2One generate only a few high-quality samples,\nrespectively, while others exhibit pixel loss and fail to preserve structural consistency with\nthe source images. The blue solid boxes highlight the key experiment.\n19\n（a）Samples results for the Blod digit →Thin digit  direction. \nCycleGAN\nSource\nGT\nOne2One\niHQGAN\nCycleGAN\nSource\nGT\nOne2One\niHQGAN\niHQGAN\nCycleGAN\nSource\nGT\nOne2One\n（c） Zoom in on and colorize images to emphasize different font styles: purple for bold \nfont, blue for thin font. Left: for the yellow-boxed area in sub-figure (a). Right: for the red-\nboxed area in sub-figure (b).\n（b） Samples  results  for the Blod digit     Thin digit direction.  \niHQGAN\nCycleGAN\nSource\nGT\nOne2One\n←\nFigure 7: Qualitative comparison of outputs generated by various methods trained on Font\nStyle Transfer datasets for the mapping Bold Digit ↔Thin Digit. From top to bottom:\nSource, CycleGAN, One2One, iHQGAN, and GT(ground truth). In contrast, the green\ndashed and solid lines highlight that CycleGAN and One2One respectively generate only a\nfew high-quality samples, while others exhibit pixel loss and fail to maintain structural\nconsistency with the source images. Subfigure (c) emphasizes the poor effectiveness of\ndomain transfer in CycleGAN and One2One, as the font styles of the generated images are\nnot easily distinguishable from those of the source images. The blue solid boxes highlight\nthe key experiment.\n4.4.1. Experiment on different unsupervised I2I translation tasks\nEdge Detection and Font Style Transfer For Edge Detection and Font\nStyle Transfer, iHQGAN effectively performs domain transfer and maintains\nstructural consistency between the source images and outputs. Moreover,\n20\niHQGAN is superior to CycleGAN and the One2One, as depicted in Fig.6 and\nFig.7. First, the images produced by iHQGAN exhibit more complete and\nsmoother content, while the other two methods suffer from pixel loss issues.\nThis phenomenon is particularly evident in Fig.6 when compared with Fig.7.\nAdditionally, iHQGAN maintains better structural consistency across all\nsamples, while CycleGAN and One2One preserve structural consistency only\nin a few cases, performing poorly overall. The few good results for CycleGAN\nand One2One are highlighted with green dashed and solid bounding boxes.\nFor Font Style Transfer, we zoom in on and colorize some examples of results\nto emphasize different font styles, as shown in subfigure (c) of Fig.7. Images\nproduced by iHQGAN exhibit more obvious changes in font style than those\nproduced by the other two classical methods.\nWe conducted quantitative assessments of image quality across different\nmethods using various quality metrics. In Table. 5, the optimal results for the\nEdge −→Digit direction primarily appear among One2One and iHQGAN,\nwhile in the Edge ←−Digit direction, iHQGAN encompasses nearly all\noptimal values. Table. 6 indicates that in both the Bold Digit −→Thin Digit\nand Bold Digit ←−Thin Digit directions, the optimal results are similarly\nconcentrated in iHQGAN. This suggests that although classical methods\nmay outperform iHQGAN in certain cases, overall, iHQGAN demonstrates\nexcellent performance in both the Edge Detection and Font Style Transfer\ntasks, showcasing the balanced performance of its two generators.\nThe effectiveness of the iHQGAN framework arises from the synergistic\neffects of mechanisms such as the suitable loss function, the unidirectional\ncycle consistency constraint, two mutually reversible generators with shared\nparameters, and the alternating training strategy.\nTable. 4 reports the\nparameter counts of various methods used in the Edge Detection and Font\nStyle Transfer. In both tasks, iHQGAN and One2One require only a single\ngenerator’s parameter set. In contrast, CycleGAN requires approximately\ntwice as many parameters as iHQGAN and One2One. This is because both\niHQGAN and One2One are reversible, whereas CycleGAN is irreversible.\nTable 4: The parameter count of different methods used for Edge Detection and Font Style\nTransfer\nTask\nCycleGAN\nOne2One\niHQGAN\nEdge Detection\n35911 × 2\n35911\n5760\nFont Style Transfer\n6157 × 2\n6157\n5760\n21\nTable 5: Quantitative evaluation of image quality generated by various methods used for\nEdge Detection.\nDataset\nEdge −→Digit\nEdge ←−Digit\nCycleGAN\nOne2One\niHQGAN\nCycleGAN\nOne2One\niHQGAN\n1FIDSSIMPSNR FID\n1SSIMPSNR FID SSIM1PSNRSSIMFIDPSNR FID SSIMPSNR FID SSIMPSNR\nlable0 31.36 0.38 11.43 220.77 0.41 10.62 22.46 0.59 12.53 43.230.31 9.15 31.62 0.26\n8.96 18.95 0.5 11.64\nlable1 5.48 0.65\n16.4\n8.65\n0.53 15.06 8.12 0.72 17.74 12.450.39 13.63 13.73 0.32 12.87 11.20 0.51 14.81\nlable2 27.55 0.43 12.06 19.46 0.44 11.93 22.48 0.47 11.88 33.330.25 9.89 26.20 0.29 10.24 19.44 0.33 11.0\nlable3 16.00 0.27 10.05 13.31 0.57 12.96 19.98 0.30 10.92 33.250.27 10.05 21.96 0.36 19.65 18.70 0.43 11.56\nlable4 36.91 0.25 12.22 25.60\n0.37 12.36 18.71 0.49 12.69 41.290.24 10.36 28.23 0.27 10.48 16.83 0.38 11.80\nlable5 24.79 0.43 12.31 17.72 0.44 12.49 21.01 0.32 11.03 34.130.27 9.99 25.18 0.3\n10.31 19.35 0.39 11.31\nlable6 21.02 0.52 12.46 20.32 0.44 12.09 20.60 0.48 12.01 30.950.28 10.3 25.20 0.31 10.42 17.85 0.42 11.66\nlable7 21.84 0.47 13.33\n8.95\n0.64 14.95 15.36 0.57 13.93 26.220.28 10.88 17.64 0.41 11.73 11.61 0.41 12.35\n1 A lower FID is better, while higher SSIM and PSNR values are more desirable.\n2 Bold text indicates the best-performing values within a set of experiments.\nTable 6: Quantitative evaluation of image quality generated by various methods used for\nFont Style Transfer.\nDataset\nBold Digit−→Thin Digit\nBold Digit ←−Thin Digit\nCycleGAN\nOne2One\niHQGAN\nCycleGAN\nOne2One\niHQGAN\n1FIDSSIMPSNR FID 1SSIMPSNR FID SSIM1PSNR FID SSIMPSNR FID SSIMPSNR FID SSIMPSNR\nlabel0 38.67 0.61 11.57 34.30 0.66 11.47 215.27 0.77 14.67 47.21 0.64 10.91 37.52 0.62 11.27 25.79 0.74 12.78\nlabel1\n7.92 0.82 19.53 10.00 0.74 17.08\n5.34 0.84 19.81 9.84 0.84 18.56 17.97 0.63 15.13 7.00 0.87 18.7\nlabel2 23.78 0.69\n13.2 21.11 0.73 13.44 15.47 0.71 14.23 43.04 0.61 11.16 30.91 0.64 12.4429.42 0.62 11.68\nlabel3 19.95 0.72 13.77 24.86 0.69 13.03 12.69 0.75 15.07 40.47 0.61 11.33 27.20 0.67 12.75 21.26 0.75 13.48\nlabel4 33.50 0.53 12.28 36.40 0.55 11.74 11.80 0.74 15.56 52.93 0.43 10.87 48.99 0.41 10.91 20.47 0.73 13.67\nlabel5 17.99 0.69 14.16 23.17 0.56 11.31 12.70 0.71 15.11 28.73 0.65 12.5130.91 0.55 12.26 21.92 0.37\n9.77\nlabel6 30.59 0.60 12.82 35.48 0.63 12.21 13.52 0.74 15.18 44.54 0.60 11.52 43.13 0.52 11.59 21.10 0.75 13.77\nlabel7 23.44 0.62 13.64 17.76 0.72 14.22 10.22 0.76 16.03 28.41 0.7\n13.02 23.70 0.64 13.41 18.06 0.74 14.22\n1 A lower FID is better, while higher SSIM and PSNR are more desirable.\n2 Bold text indicates the best-performing values within a set of experiments.\n22\nImage Denoising As shown in Fig.8, iHQGAN further demonstrates its\nstrong generalization by effectively addressing image denoising. It outperforms\nclassical methods such as CycleGAN and the Gaussian filter.\nAlthough\nCycleGAN can denoise and produce clean images, it often fails to maintain\nthe structural consistency that is crucial for image quality. On the other hand,\nthe Gaussian filter can cause image blurring, compromising the clarity and\nsharpness of the denoised results. Quantitative evaluations are reported in\nTable. 7. The optimal values are all concentrated in iHQGAN, demonstrating\nits superiority over the other two methods.\nSamples  results  for Denoise Gaussian noise.\niHQGAN\nCycleGAN\nSource\nGT\nGaussian filter\nFigure 8: Qualitative comparison of outputs generated by various methods trained on Image\nDenoising datasets. From top to bottom: Source, CycleGAN, Gaussian filter, iHQGAN, and\nGT (ground truth).iHQGAN achieves denoising while maintaining structural consistency\nbetween the generated and source images. CycleGAN also performs denoising but fails\nto preserve the structural consistency between the generated and source images. The\nGaussian filter’s denoising effect is poor, leading to blurry images.\nTable 7: Quantitative evaluation of image quality with baselines for Image Denoising.\nDataset\nCycleGAN\nOne2One\niHQGAN\n1 FID\n1 SSIM\n1 PSNR\nFID\nSSIM\nPSNR\nFID\nSSIM\nPSNR\nlable0\n16.94\n0.72\n15.44\n21.63\n0.62\n14.44\n211.04\n0.76\n15.91\nlable1\n9.35\n0.63\n18.21\n15.75\n0.29\n14.16\n6.37\n0.71\n18.94\nlable7\n18.03\n0.57\n15.53\n17.12\n0.55\n15.75\n9.50\n0.72\n16.79\n1 A lower FID is better, while higher SSIM and PSNR are more desirable.\n2 Bold text indicates the best-performing values within a set of experiments.\nThe details of the training are displayed in Appendix D. Additionally, we\nobserved that the images generated by iHQGAN in Fig.6, Fig.7, and Fig.8\n23\nexhibit lower brightness, which indicates that iHQGAN has a limited ability\nto learn brightness distributions. This issue arises because the quantum circuit\nstructure of iHQGAN is based on that of PQWGAN Tsang et al. (2023). As\npreviously noted in PQWGAN, an insufficient number of patches can result\nin darker output images. Although we used as many patches as possible, the\nissue remains unresolved.\n4.4.2. Analysis of the unidirectional cycle consistency constraint\niHQGAN employs a unidirectional cycle consistency constraint between\neach quantum generator and its corresponding ACNN. We compared iHQGAN\nwith two quantum schemes that utilize bidirectional cycle consistency con-\nstraints between two quantum generators: Q-wcycleGAN and iHQGAN w/o\nACNNs. The two quantum schemes are illustrated in Fig.??. Experiments\nwere conducted on the sub-dataset with label 0 from the Edge Detection\ndataset.\n(\n)\nG\nG \nQuantum generator\n(\n)\nF\nF \n( )\ni\ndata\nx\nP\nX\n( )\ni\ndata\ny\nP\nY\n5\ni\n\n\n4\ni\n\n\n3\ni\n\n\n2\ni\n\n\n1\ni\n\n\n5\ni\n\n4\ni\n\n3\ni\n\n2\ni\n\n1\ni\n\n5\nj\n4\nj\n3\nj\n1\nj\n5\nj\n\n4\nj\n\n3\nj\n2\nj\n\n1\nj\n\n2\nj\n\n\n\n\n\n\n\n\n\n\n2\nU\n3\nU\n4\nU\n1\nU\ncycle consistency\nQuantum generator\nCritic\nCritic\nx\nD\ny\nD\np\nU\ncycle consistency\n1\nU\n2\nU\n3\nU\n4\nU\n5\nU\n(a)Q-wcycleGAN. It features two quantum generators with the same structure and unshared\nparameters, implementing bidirectional cycle consistency constraints between the two\nquantum generators.\n(a) Two different quantum schemes utilize bidirectional cycle consistency constraints.\n(\n)\nG\nG \nQuantum generator\n(\n)\nF\nF \n( )\ni\ndata\nx\nP\nX\n( )\ni\ndata\ny\nP\nY\n5\ni\n\n\n4\ni\n\n\n3\ni\n\n\n2\ni\n\n\n1\ni\n\n\n5\ni\n\n4\ni\n\n3\ni\n\n2\ni\n\n1\ni\n\n5\nj\n4\nj\n3\nj\n1\nj\n5\nj\n\n4\nj\n\n3\nj\n2\nj\n\n1\nj\n\n2\nj\n\n\n\n\n\n\n\n\n\n\n2\nU\n3\nU\n4\nU\n1\nU\n†\n1\nU\n†\n2\nU\n†\n3\nU\n†\n4\nU\nixa\ncycle consistency\nQuantum generator\nCritic\nCritic\nx\nD\ny\nD\np\nU\n†\np\nU\ncycle consistency\nG\n\nF\n\n(b) iHQGAN w/o ACNNs. It includes two parameter-sharing and mutually reversible\nquantum generators, implementing bidirectional cycle consistency constraints between the\nquantum generators\n24\nThe qualitative and quantitative comparisons of the above two schemes\nwith iHQGAN are displayed in Fig.10 and Table. 8. iHQGAN produces\nclearer, higher-fidelity images in both directions. Compared with iHQGAN,\nQ-wcycleGAN and iHQGAN w/o ACNNs produce images with more noise.\nQ-wcycleGAN struggles to maintain structural consistency in both directions,\nwhile iHQGAN w/o ACNNs maintains it only in the Edge ←−Digit direction.\nTable. 8 shows that iHQGAN surpasses the other methods across all metrics\nfor both directions, highlighting its superior image data modeling, structural\nconsistency preservation, and image quality enhancement.\nThe superior\nperformance of iHQGAN can be attributed to the introduction of ACNNs\nthrough unidirectional cycle consistency constraints, which help the quantum\ngenerators find the optimal parameter space during training. In contrast,\nQ-wcycleGAN and iHQGAN w/o ACNNs encounter challenges in optimizing\nbidirectional cycle consistency due to the limited expressive capability of\nquantum generators.\nThese results indicate that the bidirectional cycle\nconsistency strategy fails to maintain structural consistency in both directions\nsimultaneously.\n（b）Edge        Digit\niHQGAN w/o ACNNs\niHQGAN\nSource\nGT\nQ-wcycleGAN\n（a）Edge        Digit\nFigure 10: Qualitative comparisons of outputs generated by various quantum schemes on\nthe sub-dataset with label 0 from the Edge Detection dataset.The blue solid boxes highlight\nthe key experiment.\nTable 8: Quantitative comparison of outputs generated by various quantum schemes.\nMethod\nEdge−→Digit\nEdge←−Digit\n1 FID\n1 SSIM\n1 PSNR\nFID\nSSIM\nPSNR\nQ-wcycleGAN\n23.94\n0.39\n10.5\n29.13\n0.3\n9.9\niHQGAN w/o ACNNs\n26.30\n0.35\n10.19\n31.17\n0.32\n10.0\niHQGAN\n2 22.46\n0.59\n12.53\n18.95\n0.5\n11.64\n1 A lower FID is better, while higher SSIM and PSNR are more desirable.\n2 Bold text indicates the best-performing parameters within a set of experiments.\n25\n4.4.3. An analysis of the hyperparameters in the loss function of the quantum\ngenerators\nThe loss function of the quantum generators ,as seen in Equation 8 and\nEquation 9 , draws on that of classical generators. In GANs, the learning rates\nof the classical generator and classical discriminator are similar, and the related\nwork sets the hyperparameters in the loss function of the classical generators\nto ε = 1, η = 10, ρ = 150, where ρ = 15η Chen et al. (2019). However, QGANs\noften feature a higher learning rate for the quantum generator compared to\nthe classical discriminator.Huang et al. (2021); Vieloszynski et al. (2024).\nTherefore, we need to find an appropriate combination of hyperparameters.\nExperiments were conducted on the sub-dataset with label 0 from the Edge\nDetection dataset We set the default values for the parameter pair as (η, ρ)\nare η = 10 and ρ = 150. Specifically, we explored ε = 10 in combination with\nvarious multiples of the parameter pair (η, ρ), and ε = 20 with 3 × (η, ρ).\n（a）Edge        Digit\nEdge        Digit\n（b）\n1,\n10,\n150\n\n\n\n\n\n\n10,\n20,\n300\n\n\n\n\n\n\n10,\n10,\n150\n\n\n\n\n\n\n10,\n30,\n450\n\n\n\n\n\n\n20,\n20,\n300\n\n\n\n\n\n\nGT\nSource\nFigure 11: Qualitative comparisons of outputs generated by different hyperparameter\ncombinations of iHQGAN on the sub-dataset with label 0 from the Edge Detection dataset.\nThe related work in classical fields sets ε = 1, η = 10, ρ = 150, and ρ = 15η. We set the\ndefault values for the parameter pair (η, ρ) as η = 10 and ρ = 150. For iHQGAN, we\nexplored ε = 10 with various multiples of the parameter pair (η, ρ),as well as ε = 20 with\n3 × (η, ρ). The blue solid boxes highlight the key experiment.\nFig.11 presents qualitative experimental results, while Fig.12 quantitatively\ncompares iHQGAN’s performance across various hyperparameter combina-\ntions on the test set, evaluated over 50 training sessions using the sub-dataset\nwith label 0 from the Edge Detection dataset. The images generated with\nthe combination ε = 10, η = 20, and ρ = 300 show clearer and more distinct\ncontours, yielding stable and excellent FID and SSIM scores in both directions.\nIn contrast, the combination ε = 1, η = 10, and ρ = 150 produces somewhat\nblurry contours in the generated images in the Edge ←−Digit direction, as\n26\nevidenced by the corresponding poor FID and SSIM scores. Additionally,\nimages produced with the combination ε = 10, η = 10, and ρ = 150 fail to\nmaintain structural consistency in the Edge ←−Digit direction, with the\nSSIM scores reflecting inadequate performance. The remaining three combi-\nnations produce visually similar images, and their values are close around the\n50th epoch. Our results indicate that when the weight ε of the adversarial\nlosses is too low relative to the weight ρ of the quality-aware loss, as in the\ncombination ε = 1, η = 10, and ρ = 150, the learning of the image data\ndistribution weakens, resulting in blurred contours. Conversely, when the\nweight ε of the adversarial losses is relatively high compared with that of the\nquality-aware loss, as seen in the combination ε = 10, η = 10, and ρ = 150, it\ncompromises the structural consistency between the generated images and\nthe source images in a certain translation direction.\n（b）\n（c）\n（a）\n（d）\nFigure 12: Quantitative comparison of iHQGAN’s performance across different hyperpa-\nrameter combinations on the test set, evaluated over 50 training sessions on the sub-dataset\nwith label 0 from the Edge Detection dataset. Subplots (a) and (b) above the dashed line\npresent the performance in the Edge −→Digit direction. Subplots (c) and (d) below the\ndashed line illustrate the performance in the Edge ←−Digit direction.The blue solid boxes\nhighlight the key experiment. A lower FID is better, while a higher SSIM is more desirable.\n27\n5. Discussion and Further Work\nThis paper combines the invertibility of quantum computing with the\napproximate reversible nature of unsupervised I2I translation and proposes\niHQGAN, a lightweight unsupervised I2I translation framework. iHQGAN is\nthe first versatile quantum solution for unsupervised I2I translation, which ex-\ntends QGAN research into more complex image generation scenarios and offers\na quantum approach that reduces the parameters of GAN-based unsupervised\nI2I translation methods. Compared with classical GAN-based methods for\nunsupervised I2I translation Isola et al. (2017); Chen et al. (2019); Choi\net al. (2017), this study introduces quantum technology to perform this task.\nFurthermore, similar to classical reversible methods (van der Ouderaa and\nWorrall (2019); Dai and Tang (2021), which reduce parameters using a classi-\ncal reversible mechanism, iHQGAN achieves parameter reduction through a\nquantum reversible mechanism. Currently, QGAN is limited to generating\nsingle-channel,low-resolution images, such as MNIST and MNIST-C. To assess\nthe model’s performance, this study processed the MNIST dataset to obtain\nthree unsupervised I2I translation tasks,with a total of 19 sub-datasets.\nIn Section 4.4.1, the experiments conducted on the Edge Detection and\nFont Style Transfer datasets demonstrated that iHQGAN can effectively\nachieve unsupervised I2I translation and reduce the parameter scale. As can\nbe seen from the qualitative evaluation in Fig.6 and Fig.7, the generated\nimages of iHQGAN achieve domain transfer while maintaining structural\nconsistency with the source images. In contrast, the classical irreversible\nmethod CycleGAN and the classical reversible method One2One using low-\ncomplexity CNN-based generators do not perform better than iHQGAN.Many\ngenerated images of the latter classical methods exhibit pixel loss and fail to\nmaintain structural consistency with the source images. Table. 5 and Table. 6\nquantitatively show that many best values are concentrated in iHQGAN.\nAdditionally, as shown in Table. 4, like classical reversible methods, iHQ-\nGAN leverages a reversible mechanism to reduce the parameters of classical\nirreversible methods. In fact, many classical reversible methods leverage the\napproximate reversible nature of unsupervised I2I translation to construct\nreversible models for reducing parameter counts.Shen et al. (2020); Dai and\nTang (2021); van der Ouderaa and Worrall (2019) Furthermore, we conducted\nexperiments on Image Denoising to validate the model’s generalization ability.\nAs shown in Fig.8 and Table. 7,the results indicate that, compared with the\nclassical nonlinear,low-complexity CNN-based CycleGAN and the classical\n28\nlinear Gaussian filter, the denoised images produced by iHQGAN demonstrate\nsignificant denoising effectiveness and higher fidelity. The effectiveness of the\niHQGAN framework can be attributed to the interplay of various mechanisms,\nincluding the unidirectional cycle consistency constraint, the appropriate loss\nfunction, two mutually reversible generators with shared parameters, and the\nalternating training strategy.\nIn Section 4.4.2, our work compared the effectiveness of the strategy of\nunidirectional cycle consistency constraints between each quantum generator\nand its corresponding ACNN in iHQGAN with bidirectional cycle consistency\nconstraints between two quantum generators. We designed two other quantum\nschemes adopting bidirectional cycle consistency constraints for unsupervised\nI2I translation, as illustrated in Fig.??. All quantum schemes were tested on\nthe sub-dataset with label 0 from the Edge Detection dataset. Fig.10 indicates\nthat, compared with iHQGAN, Q-wcycleGAN and iHQGAN w/o ACNNs fail\nto maintain structural consistency simultaneously in both directions. Neither\ngenerator in Q-wcycleGAN achieves this consistency, while iHQGAN w/o\nANNs has only one generator that does. As can be seen from Table. 8, all\nbest values are concentrated in iHQGAN. This is because unidirectional cycle\nconsistency constraints enable ANNs to optimize the parameter space of\nthe quantum generators. In the classical domain, the strategy of utilizing\nunidirectional cycle consistency constraints can achieve unsupervised image\ntranslation Fu et al. (2019); Benaim and Wolf (2017), and numerous studies\nhave demonstrated that classical components can enhance the performance\nof quantum circuits Callison and Chancellor (2022); Chang et al. (2024); Shu\net al. (2024).\nIn Section 4.4.3, we investigated the effect of different hyperparameters\ncombinations in the loss function of the quantum generators.The loss function\nof the quantum generators ,as defined in Equation 8 and Equation 9,draws\non that of classical generators. In GANs, the learning rates of the classical\ngenerator and classical discriminator are close.Chen et al. (2019). However, the\nlearning rate of the classical critic is relatively lower than that of the quantum\ngenerator in QGANs Huang et al. (2021); Tsang et al. (2023); Vieloszynski\net al. (2024) .Therefore,it is crucial to explore appropriate hyperparameters\ncombinations of the loss function for iHQGAN. Experiments were conducted\non the sub-dataset with label 0 from the Edge Detection dataset. Fig.11 and\nFig.12 present qualitative and quantitative comparisons of the results under\ndifferent weight combinations. When the weight of the adversarial losses is\ntoo low relative to that of the IQA loss in the loss function, the learning of the\n29\nimage data distribution weakens, resulting in blurred contours. Conversely, it\ncompromises the structural consistency between the generated images and\nthe source images in a certain translation direction.\nThis groundbreaking work paved the way for quantum technology to\naddress complex image translation tasks. However, the quantum generator’s\nstructure still requires improvement, as image quality depends on its expressive\ncapacity.\nThis study draws on the quantum generator’s structure from\nPQWGAN Tsang et al. (2023). However, the images generated by PQWGAN\nsuffer from many flaws, such as poorly fitting luminance distributions and\nexcessive noise. We plan to explore how to design quantum generators with\nbetter expressiveness to improve the quality of generated images from the\nperspective of the number of qubits, the depth of quantum circuits, and\nefficient ansatz designs, which stands as a significant future endeavor within\nour research trajectory.\n6. Conclusion\nThis paper presents iHQGAN, the first versatile quantum method for\nunsupervised I2I translation. iHQGAN combines the approximate reversibility\nof unsupervised I2I translation with the invertibility of quantum computing,\nextending the application of QGANs to complex image generation scenar-\nios and effectively addressing the issue of excessive parameters in classical\nGAN-based unsupervised I2I translation methods. Simulation experiments on\nextensive datasets demonstrate that iHQGAN effectively performs unsuper-\nvised I2I translation with strong generalization ,and can outperform classical\nmethods that use low-complexity CNN-based generators. Additionally, iHQ-\nGAN, like classical reversible methods, reduces the number of parameters\nin classical irreversible methods via a reversible mechanism. These results\nhighlight the effectiveness, broad applicability, and parameter-saving nature\nof the iHQGAN in unsupervised I2I translation. Future research will revolve\naround developing quantum generators with powerful expressiveness, aiming\nto improve the overall quality of images generated by iHQGAN.\nAuthors’ contributions\nXue Yang: Conceptualization, Methodology, Validation, and writing the\narticle. Ri-Gui Zhou: Project administration, Supervision. Yao-Chong Li:\nConceptualization. Shi-Zheng Jia: Conceptualization, Software. Zheng-Yu\n30\nLong: Reviewing. Cheng-Ji Yan Reviewing.Wen-Shan Xu:Reviewing.\nWen-Yu Guo: Formal analysis. Fu-Hui Xiong: Formal analysis.\nAvailability of supporting data\nAll data generated or analysed during this study are available and included\nin this published article. Code for our work has been open-sourced.\nAcknowledgements\nThis work is supported by the National Natural Science Foundation of\nChina under Grant No.\n62172268 and 62302289; Shanghai Science and\nTechnology Project under Grant No. 21JC1402800 and 23YF1416200.\nReferences\nAmirkolaee, H.A., Bokov, D.O., Sharma, H., 2022. Development of a gan architec-\nture based on integrating global and local information for paired and unpaired\nmedical image translation. Expert Systems with Applications 203, 117421.\nArjovsky, M., Chintala, S., Bottou, L., 2017.\nWasserstein generative adver-\nsarial networks, in: Precup, D., Teh, Y.W. (Eds.), Proceedings of the 34th\nInternational Conference on Machine Learning, PMLR. pp. 214–223. URL:\nhttps://proceedings.mlr.press/v70/arjovsky17a.html.\nBenaim, S., Wolf, L., 2017. One-sided unsupervised domain mapping. Advances in\nneural information processing systems 30.\nBenedetti, M., Grant, E., Wossnig, L., Severini, S., 2019. Adversarial quantum\ncircuit learning for pure state approximation. New Journal of Physics 21, 043023.\nBhattacharjee, D., Kim, S., Vizier, G., Salzmann, M., 2020. Dunit: Detection-based\nunsupervised image-to-image translation, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 4787–4796.\nBradski, G., 2000. The OpenCV Library. Dr. Dobb’s Journal of Software Tools .\nCallison, A., Chancellor, N., 2022. Hybrid quantum-classical algorithms in the noisy\nintermediate-scale quantum era and beyond. Physical Review A 106, 010101.\nCerezo, M., Arrasmith, A., Babbush, R., Benjamin, S.C., Endo, S., Fujii, K.,\nMcClean, J.R., Mitarai, K., Yuan, X., Cincio, L., et al., 2021. Variational\nquantum algorithms. Nature Reviews Physics 3, 625–644.\n31\nChakrabarti, S., Yiming, H., Li, T., Feizi, S., Wu, X., 2019. Quantum wasserstein\ngenerative adversarial networks. Advances in Neural Information Processing\nSystems 32.\nChang, S.Y., Thanasilp, S., Saux, B.L., Vallecorsa, S., Grossi, M., 2024. Latent\nstyle-based quantum gan for high-quality image generation. arXiv preprint\narXiv:2406.02668 .\nChen, L., Wu, L., Hu, Z., Wang, M., 2019. Quality-aware unpaired image-to-image\ntranslation. IEEE Transactions on Multimedia 21, 2664–2674.\nChoi, Y., Choi, M., Kim, M., Ha, J., Kim, S., Choo, J., 2017. Stargan: Uni-\nfied generative adversarial networks for multi-domain image-to-image trans-\nlation.\nCoRR abs/1711.09020.\nURL: http://arxiv.org/abs/1711.09020,\narXiv:1711.09020.\nChu, C., Skipper, G., Swany, M., Chen, F., 2023. Iqgan: Robust quantum generative\nadversarial network for image synthesis on nisq devices, in: ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), IEEE. pp. 1–5.\nDai, L., Tang, J., 2021. iflowgan: An invertible flow-based generative adversarial\nnetwork for unsupervised image-to-image translation. IEEE Transactions on\nPattern Analysis and Machine Intelligence 44, 4151–4162.\nDallaire-Demers, P.L., Killoran, N., 2018. Quantum generative adversarial networks.\nPhysical Review A 98, 012324.\nDemir, U., Unal, G., 2018. Patch-based image inpainting with generative adversarial\nnetworks. arXiv preprint arXiv:1803.07422 .\nDeng, H., Wu, Q., Huang, H., Yang, X., Wang, Z., 2023. Involutiongan: lightweight\ngan with involution for unsupervised image-to-image translation. Neural Com-\nputing and Applications 35, 16593–16605.\nDinh, L., Krueger, D., Bengio, Y., 2014. Nice: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516 .\nDinh, L., Sohl-Dickstein, J., Bengio, S., 2016. Density estimation using real nvp.\narXiv preprint arXiv:1605.08803 .\nEmami, H., Aliabadi, M.M., Dong, M., Chinnam, R.B., 2020. Spa-gan: Spatial\nattention gan for image-to-image translation. IEEE Transactions on Multimedia\n23, 391–401.\n32\nFu, H., Gong, M., Wang, C., Batmanghelich, K., Zhang, K., Tao, D., 2019.\nGeometry-consistent generative adversarial networks for one-sided unsupervised\ndomain mapping, in: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 2427–2436.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., Bengio, Y., 2020. Generative adversarial networks. Communica-\ntions of the ACM 63, 139–144.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C., 2017.\nImproved training of wasserstein gans. CoRR abs/1704.00028. URL: http:\n//arxiv.org/abs/1704.00028, arXiv:1704.00028.\nHarrow, A.W., Montanaro, A., 2017. Quantum computational supremacy. Nature\n549, 203–209.\nHuang, H.L., Du, Y., Gong, M., Zhao, Y., Wu, Y., Wang, C., Li, S., Liang, F., Lin,\nJ., Xu, Y., et al., 2021. Experimental quantum generative adversarial networks\nfor image generation. Physical Review Applied 16, 024051.\nHuang, X., Mei, G., Zhang, J., 2020. Feature-metric registration: A fast semi-\nsupervised approach for robust point cloud registration without correspondences,\nin: Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 11366–11374.\nHuang, X., Zhang, J., Fan, L., Wu, Q., Yuan, C., 2017a. A systematic approach for\ncross-source point cloud registration by preserving macro and micro structures.\nIEEE Transactions on Image Processing 26, 3261–3276.\nHuang, X., Zhang, J., Wu, Q., Fan, L., Yuan, C., 2017b. A coarse-to-fine algorithm\nfor matching and registration in 3d cross-source point clouds. IEEE Transactions\non Circuits and Systems for Video Technology 28, 2965–2977.\nIsola, P., Zhu, J.Y., Zhou, T., Efros, A.A., 2017. Image-to-image translation with\nconditional adversarial networks, in: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 1125–1134.\nJacobsen, J., Smeulders, A.W.M., Oyallon, E., 2018. i-revnet: Deep invertible\nnetworks. CoRR abs/1802.07088. URL: http://arxiv.org/abs/1802.07088,\narXiv:1802.07088.\nKancharagunta, K.B., Dubey, S.R., 2019.\nCsgan: Cyclic-synthesized genera-\ntive adversarial networks for image-to-image transformation. arXiv preprint\narXiv:1901.03554 .\n33\nKieferova, M., Carlos, O.M., Wiebe, N., 2021. Quantum generative training using\nr\\’enyi divergences. arXiv preprint arXiv:2106.09567 .\nKingma, D.P., Dhariwal, P., 2018. Glow: Generative flow with invertible 1x1\nconvolutions. Advances in neural information processing systems 31.\nLeCun, Y., Cortes, C., 2010. MNIST handwritten digit database URL: http:\n//yann.lecun.com/exdb/mnist/.\nLee, H.Y., Tseng, H.Y., Huang, J.B., Singh, M., Yang, M.H., 2018. Diverse image-\nto-image translation via disentangled representations, in: Proceedings of the\nEuropean conference on computer vision (ECCV), pp. 35–51.\nLi, J., Topaloglu, R.O., Ghosh, S., 2021. Quantum generative models for small\nmolecule drug discovery. IEEE transactions on quantum engineering 2, 1–8.\nLiu, M.Y., Tuzel, O., 2016. Coupled generative adversarial networks. Advances in\nneural information processing systems 29.\nLloyd, S., Weedbrook, C., 2018. Quantum generative adversarial learning. Physical\nreview letters 121, 040502.\nMu, N., Gilmer, J., 2019.\nMNIST-C: A robustness benchmark for computer\nvision.\nCoRR abs/1906.02337.\nURL: http://arxiv.org/abs/1906.02337,\narXiv:1906.02337.\nvan der Ouderaa, T.F., Worrall, D.E., 2019. Reversible gans for memory-efficient\nimage-to-image translation, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 4720–4728.\nPreskill, J., 2012. Quantum computing and the entanglement frontier. URL:\nhttps://arxiv.org/abs/1203.5813, arXiv:1203.5813.\nPreskill, J., 2018. Quantum computing in the nisq era and beyond. Quantum 2, 79.\nQu, Z., Chen, W., Tiwari, P., 2024. Hq-dcgan: Hybrid quantum deep convolutional\ngenerative adversarial network approach for ecg generation. Knowledge-Based\nSystems 301, 112260.\nRichardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S.,\nCohen-Or, D., 2021. Encoding in style: a stylegan encoder for image-to-image\ntranslation, in: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 2287–2296.\n34\nShen, Z., Chen, Y., Zhou, S.K., Georgescu, B., Liu, X., Huang, T.S., 2019. Towards\nlearning a self-inverse network for bidirectional image-to-image translation. arXiv\npreprint arXiv:1909.04104 .\nShen, Z., Zhou, S.K., Chen, Y., Georgescu, B., Liu, X., Huang, T., 2020. One-\nto-one mapping for unpaired image-to-image translation, in: Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision, pp.\n1170–1179.\nShu, R., Xu, X., Yung, M.H., Cui, W., 2024. Variational quantum circuits enhanced\ngenerative adversarial network. arXiv preprint arXiv:2402.01791 .\nSilver, D., Patel, T., Cutler, W., Ranjan, A., Gandhi, H., Tiwari, D., 2023. Mosaiq:\nQuantum generative adversarial networks for image generation on nisq computers,\nin: Proceedings of the IEEE/CVF International Conference on Computer Vision,\npp. 7030–7039.\nSitu, H., He, Z., Wang, Y., Li, L., Zheng, S., 2020. Quantum generative adversarial\nnetwork for generating discrete distribution. Information Sciences 538, 193–208.\nStein, S.A., Baheri, B., Chen, D., Mao, Y., Guan, Q., Li, A., Fang, B., Xu, S.,\n2021. Qugan: A quantum state fidelity based generative adversarial network, in:\n2021 IEEE International Conference on Quantum Computing and Engineering\n(QCE), IEEE. pp. 71–81.\nTang, H., Liu, H., Xu, D., Torr, P.H., Sebe, N., 2021. Attentiongan: Unpaired image-\nto-image translation using attention-guided generative adversarial networks.\nIEEE transactions on neural networks and learning systems 34, 1972–1987.\nTomei, M., Cornia, M., Baraldi, L., Cucchiara, R., 2019. Art2real: Unfolding\nthe reality of artworks via semantically-aware image-to-image translation, in:\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 5849–5859.\nTsang, S.L., West, M.T., Erfani, S.M., Usman, M., 2023. Hybrid quantum-classical\ngenerative adversarial network for high resolution image generation.\nIEEE\nTransactions on Quantum Engineering .\nVieloszynski, A., Cherkaoui, S., Laprade, J.F., Nahman-L´evesque, O., Aaraba,\nA., Wang, S., 2024. Latentqgan: A hybrid qgan with classical convolutional\nautoencoder. URL: https://arxiv.org/abs/2409.14622, arXiv:2409.14622.\n35\nvan Wyk, G.J., Bosman, A.S., 2019. Evolutionary neural architecture search for\nimage restoration, in: 2019 International Joint Conference on Neural Networks\n(IJCNN), IEEE. pp. 1–8.\nYang, T.J., Liao, Y.L., Sze, V., 2021.\nNetadaptv2: Efficient neural architec-\nture search with fast super-network training and architecture optimization, in:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 2402–2411.\nYuan, Y., Liu, S., Zhang, J., Zhang, Y., Dong, C., Lin, L., 2018. Unsupervised\nimage super-resolution using cycle-in-cycle generative adversarial networks, in:\nProceedings of the IEEE conference on computer vision and pattern recognition\nworkshops, pp. 701–710.\nZhang, Y., Liu, S., Dong, C., Zhang, X., Yuan, Y., 2019. Multiple cycle-in-cycle\ngenerative adversarial networks for unsupervised image super-resolution. IEEE\ntransactions on Image Processing 29, 1101–1112.\nZhao, L., Mo, Q., Lin, S., Wang, Z., Zuo, Z., Chen, H., Xing, W., Lu, D., 2020.\nUctgan: Diverse image inpainting based on unsupervised cross-space translation,\nin: Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 5741–5750.\nZhou, N.R., Zhang, T.F., Xie, X.W., Wu, J.Y., 2023. Hybrid quantum–classical\ngenerative adversarial networks for image generation via learning discrete distri-\nbution. Signal Processing: Image Communication 110, 116891.\nZhu, J.Y., Park, T., Isola, P., Efros, A.A., 2017. Unpaired image-to-image transla-\ntion using cycle-consistent adversarial networks, in: Proceedings of the IEEE\ninternational conference on computer vision, pp. 2223–2232.\nZoufal, C., Lucchi, A., Woerner, S., 2019. Quantum generative adversarial networks\nfor learning and loading random distributions. npj Quantum Information 5, 103.\n36\nAppendix A. Details of quantum circuits of iHQGAN\nFigure A.13: The quantum circuits Uk(k = 1...p) of the quantum generator G feature a\ndepth of 24 and consist of 48 CNOT operations.\n37\nFigure A.14: The quantum circuits Ul(l = 1...p) of the quantum generator F feature a\ndepth of 24 and consist of 48 CNOT operations.\nAppendix B. Architecture of the classical critic\nThe critics DX and DY are classical neural networks, with parameters κDx and\nκDY , respectively. Their structure is the same as that of the critic in PQWGAN,\ndesigned to assist in training the quantum generator. After training is complete, the\ncritic is discarded. One of the important principles of training an effective QGAN\n38\nmodel is to ensure that the generator and the critic have comparable capabilities.\nMode collapse may occur if the generator is overpowering and the critic is weak.\nConversely, it is difficult for the generator to produce realistic samples. There is no\nway to accurately determine the relationship between the learning ability of the\nclassical critic and the quantum generator. This paper draws on the PQWGAN\nexperience where the critic is a fully connected network with two hidden layers\nof 1024, 512, 256, and 1 neuron. Both hidden layers use a leakyReLU activation\nfunction with a slope of 0.2, which introduces more nonlinearities to increase the\nexpressive power of the neural network. The final hidden layer connects to the\noutput layer, which has only one neuron to produce a real value and determine\nwhether the input data is real or synthetic.\nAppendix C. How to build datasets\nThe MNIST-C dataset is a benchmark for robustness in computer vision. Image\nDenoising and Edge Detection correspond to the two corruptions - Gaussian Noise,\nand Canny Edges in MNIST-C code. For Font Style Transfer tasks, we applied\ndilation to the MNIST dataset using PyTorch and OpenCV libraries. First, the\nMNIST dataset is loaded using PyTorch’s datasets.MNIST\nfunction, followed by\nconverting the images into NumPy arrays. Subsequently, a 2x2 dilation kernel\nis defined, and the dilation operation is executed using OpenCV’s cv2.dilate\nfunction. Finally, the data are converted back to PyTorch tensors and stored in\nPNG format. We have published the dataset-making code in the GitHub repository.\nAppendix D. The display of more training details\nFig.D.15 shows the loss curves on three sub-datasets with label 0 from different\ntraining datasets. The generator loss initially decreases and then stabilizes dur-\ning training, whereas the critic loss first increases before stabilizing, ultimately\nreaching a Nash equilibrium. Fig.D.16,Fig.D.17 and Fig.D.18,show examples of\ntraining results for Edge Detection, and Font Style Transfer ,Image Denoising.\niHQGAN can effectively perform unsupervised I2I translation across the three\ntasks, demonstrating stable performance and impressive generalization.\n39\n(a) Edge Detection  task for label 0\n(b)  Font  Style Transfer task  for label 0 \n(c) Image Denoising  for label 0\nFigure D.15: Training loss curves on three sub-datasets with label 0 from different training\ndatasets.\n40\n（a）Edge →Digit\n（b）  Digit→Edge \nInput\nOutput\nInput\nOutput\nFigure D.16: The examples of training results of Edge Detection. Sub-figure(a) shows the\nexamples in the Edge →Digit direction. Sub-figure(b) the examples in the Digit →Edge\ndirection\n41\n（a）Blod digit →Thin Digit. \n（b）Thin digit →Bold Digit.  \nInput\nOutput\nInput\nOutput\nFigure D.17: The examples of training results of Font Style Transfer. Sub-figure(a) shows\nthe examples in the Bold Digit →Thin Digit direction. Sub-figure(b) shows the examples\nin the Thin Digit →Bold Digit direction\n42\nInput\nOutput\nFigure D.18: The examples of training results of Image Denoising\n43\n",
  "categories": [
    "quant-ph"
  ],
  "published": "2024-11-21",
  "updated": "2024-11-25"
}