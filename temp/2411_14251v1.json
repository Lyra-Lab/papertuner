{
  "id": "http://arxiv.org/abs/2411.14251v1",
  "title": "Natural Language Reinforcement Learning",
  "authors": [
    "Xidong Feng",
    "Ziyu Wan",
    "Haotian Fu",
    "Bo Liu",
    "Mengyue Yang",
    "Girish A. Koushik",
    "Zhiyuan Hu",
    "Ying Wen",
    "Jun Wang"
  ],
  "abstract": "Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases. Our code will be released at\nhttps://github.com/waterhorse1/Natural-language-RL.",
  "text": "Preprint. Work in progress.\n.NATURAL LANGUAGE REINFORCEMENT LEARNING\nXidong Feng1∗†, Ziyu Wan2∗†, Haotian Fu3†, Bo Liu4†, Mengyue Yang5, Girish A. Koushik6,\nZhiyuan Hu4, Ying Wen2, Jun Wang1\n1University College London, 2Shanghai Jiao Tong University, 3Brown University,\n4National University of Singapore, 5University of Bristol, 6University of Surrey\n \nYou are the player 'O' in tic-tac-toe, \ngiven current board state, \n                          to win the game, what's \n  your next move? \nLLMs as Language Policy\n1\nResponse:\n{\n    \"Thought\": The 'O' should block xxxxx,\n    \"Action\": 1 (top middle square)\n}\nQuery:\n \nYou are the player 'O' in tic-tac-toe, \ngiven current board state, \nYou next move is action 2, \nwhat's your assessment \nabout this move?\nLLMs as Language Value \nFunction Approximator\n2\nResponse:\n{\n    \"Thought\": The action 2 (top right) xxxxx,\n    \"Final Evaluation\": xxxxx\n}\nboard\nboard\nQuery:\n \nYou are the player 'O' in tic-tac-toe,  given \nvariations and subsequent evals, generate the\nevaluation for current state-action pair:\nResponse:\n{\n    \"Thought\": By reviewing and aggregating \nthese variations and subsequent evals, xxxx\n    \"Final Evaluation\": action 2 seems good xxxx\n}\n......\nMove\nOne-step Lookahead\n{\n\"Variation\": 'X' plays 7.\n\"Thought\": xxxxx,\n\"Final Evaluation\": \n    'O' can win in one step.\n}\n{\n\"Variation\": 'X' plays 0.\n\"Thought\": xxxxx,\n\"Final Evaluation\": \n    'O' needs to block 'x'.\n}\n3\n3\nLLMs as Language Monte-Carlo Or Temporal Difference operator\nQuery:\nQuery:\n \nYou are the player 'O' in tic-tac-toe,  given \ncurrent board state and move, evaluate the \nstate-action by aggregating the trajectories:\nResponse:\n......\nMove\nRollout to the End\n{\n    \"Thought\": By reviewing and aggregating \nthese trajectories after taking move 2, xxxx\n    \"Final Evaluation\": action 2 is good xxx\n}\nYou are the player 'O' in \ntic-tac-toe,  given current \nboard state, candidate \nmoves and their evaluations,\nwhat is the best move?\n5\nResponse:\nQuery:\n{\n\"Thought\": \nxxxxx,\n\"Final Evaluation\": \ngood\n}\n{\n\"Thought\": \nxxxxx,\n\"Final Evaluation\": \nbad\n}\n{\n\"Thought\": \nxxxxx,\n\"Final Evaluation\": \nbad\n}\nLLMs as Policy Improvement Operator\n{\n    \"Thought\": By comparing these action candidates, xxxx\n    \"Final action\": 2 (top right square)\n}\n4\nDistilling to Train Language\nValue function and Policy\n6\nLanguage Policy\nLanguage Value\nFunction\nResponse:\n{\n    \"Thought\": The 'O' should block 'X' xxxx\n    \"Action\": 2 (top right square)\n    \"Evaluation\": action 2 is a good move while \nin other cases 'X' wins\n}\n3\nBuffer D\n5\nFigure 1: Practical pipeline for implementing NLRL in the Tic-Tac-Toe game. LLMs can serve\nas the language policy ①, the language-based value function approximator ②, the language Monte\nCarlo or temporal-difference operator ③, and the language policy improvement operator ⑤. By\ndistilling (④, ⑥) the improved evaluations from ②and the enhanced actions from ⑤, the NLRL\nagent can iteratively refine its language policy and evaluation capabilities.\nABSTRACT\nReinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remark-\nable breakthroughs across various domains, including games, robotics, and lan-\nguage models. This paper seeks a new possibility, Natural Language Reinforce-\nment Learning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-\nlike policy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demon-\nstrate the effectiveness, efficiency, and interpretability of the NLRL framework\namong diverse use cases. Our code will be released at https://github.\ncom/waterhorse1/Natural-language-RL.\n*Equal contribution, correspondence to fengxidongwh@gmail.com\n†Core contributors.\n1\narXiv:2411.14251v1  [cs.LG]  21 Nov 2024\nPreprint. Work in progress.\n1\nINTRODUCTION\nReinforcement Learning (RL) (Sutton & Barto, 2018) provides a rigorous framework – Markov De-\ncision Process (MDP) for solving general decision-making problems. It transforms the policy learn-\ning problem into a mathematical optimization task. While RL has achieved breakthroughs across\nvarious domains, several challenges remain. For example, traditional RL algorithms generally lack\ntask-specific prior knowledge, requiring extensive sampling to approximate environment dynamics.\nRL policy also lacks interpretability. Even in superhuman-performing models like AlphaZero (Sil-\nver et al., 2017), strategic reasoning remains elusive, even to professional players. RL training is\nalso unstable (Zheng et al., 2023; Andrychowicz et al., 2020) due to its reliance on scalar rewards\nas the sole supervision signal. This one-dimensional feedback is particularly limiting in real-world\nscenarios where richer, multi-modal signals are naturally available, such as textual feedback (Bai\net al., 2022; Madaan et al., 2024), visual demonstrations (Bousmalis et al., 2023; Xu et al., 2024),\nor other sensory inputs (Qiao et al., 2024).\nTo tackle these challenges, we seek a new RL paradigm shift, inspired by language-centric decision-\nmaking. Unlike traditional RL, which relies heavily on formalized mathematical modeling, humans\ncan leverage natural language to interpret tasks, devise strategies, and communicate their reasoning.\nThis language-driven approach enables rapid generalization using text-based prior knowledge, en-\nhances interpretability through explicit reasoning, and provides access to rich, informative signals\nfrom linguistic data. Thus, natural language represents a largely untapped resource for improving the\nefficiency, stability, and interoperability of RL systems. The recent success of language-based trans-\nformers (Vaswani, 2017) further opens new avenues for integrating language into the RL framework.\nLarge language models (LLMs) have demonstrated their unprecedented proficiency in generating,\nunderstanding, and processing complex language-based information.\nBuilding upon language-centric decision-making and advancement of LLMs, we introduce Natural\nLanguage Reinforcement Learning (NLRL), a novel RL paradigm that combines RL’s mathe-\nmatical rigor with the representational richness of natural language. In NLRL, core RL compo-\nnents—such as task objectives, policies, value functions, and the Bellman equation—are reinter-\npreted as language-based constructs. The medium of natural language largely facilitates the inte-\ngration of prior knowledge stored in LLMs, and effectively translates decision-making processes\ninto a form that is both intuitive and interpretable. NLRL also provides a systematic solution for\nleveraging rich textual feedback in sequential decision-making tasks, enabling stable training.\nBuilding on this new paradigm, we can efficiently implement RL algorithms in language represen-\ntation space, resulting in various LLM applications. Leveraging the unsupervised nature of RL, all\ntraining and improvements can be achieved through environment interaction without the need for\nlabeled data from humans or advanced models. In particular, Sec 4.2 illustrates how NLRL algo-\nrithms can enhance an LLM agent’s critique and planning abilities using pure prompting. Sec 4.3\ndetails the training of a natural language value function to serve as an evaluator or validator, pro-\nviding reliable assessments for any given state. A comprehensive NLRL training pipeline, covering\niterative training of both language policies and critics, is outlined in Sec 4.4. Similar to the tradi-\ntional actor-critic framework (Barto et al., 1983; Sutton et al., 1999), this pipeline enables the LLM\nto learn purely from textual environmental feedback, generating language-based Chain-of-Thought\n(Wei et al., 2022b) enhanced policies and critiques. We empirically validate these use cases in en-\nvironments such as the Maze game, Breakthrough board game, and Tic-Tac-Toe, demonstrating the\neffectiveness and superiority of the NLRL framework.\n2\nPRELIMINARY OF REINFORCEMENT LEARNING\nReinforcement Learning models the decision-making problem as a Markov Decision Process\n(MDP), defined by the state space S, action space A, probabilistic transition function P\n:\nS × A × S\n→\n[0, 1], discount factor γ\n∈\n[0, 1) and reward function r\n:\nS × A\n→\n[−Rmax, Rmax].\nThe goal of RL aims to learn a policy π : S × A →[0, 1], which mea-\nsures the action a’s probability given the state s: π(a|s) = Pr (At = a | St = s). In decision-\nmaking tasks, the optimal policy tends to maximize the expected discounted cumulative re-\nward: π∗(a|s) = arg maxπ Eπ\n\u0002 P∞\nt=0 γtr (st, at)\n\u0003\n. The state-action and state value functions\nare two key concepts that evaluate states or state-action pairs by measuring the cumulative re-\n2\nPreprint. Work in progress.\nward starting from them: Qπ(st, at) = E(s,a)t+1:∞∼Pπ\n\u0002P∞\ni=t γi−tr (si, ai) | st, at\n\u0003\n, Vπ(st) =\nEat,(s,a)t+1:∞∼Pπ\n\u0002P∞\ni=t γi−tr (si, ai) | st\n\u0003\n, where Pπ is the trajectory distribution given the pol-\nicy π and dynamic transition P.\nGiven the definition of Vπ(st), the relationship between temporally adjacent state’s value (e.g.,\nV (st) and V (st+1)) can be derived as the Bellman expectation equation (Bellman et al., 1965).\nHere is an example of one-step Bellman expectation equation:\nVπ(st) = Eat∼πθ\nh\nr(st, at) + γEst+1∼p(st+1|st,at)[Vπ(st+1)]\ni\n, ∀st ∈S\n(1)\nA similar equation can also be derived for Qπ(s, a). Given these basic RL definitions and equations,\nmany RL algorithms fall in the scope of generalized policy iteration (GPI). GPI is an iterative process\nand each iteration consists of two procedures: policy evaluation and policy improvement.\nPolicy Evaluation. The target of the policy evaluation process is to estimate state value function\nVπ(s) or state-action value function Qπ(s, a) for a given policy π. For simplicity, we only utilize\nVπ(s) in the following illustration. Two common value function estimation methods are the Monte\nCarlo (MC) estimate and the Temporal-Difference (TD) estimate (Sutton, 1988). Starting from\nthe definition of Vπ(st), the Monte Carlo (MC) estimate uses sampling over complete trajectories to\ncalculate an unbiased estimate: Vπ(st) ≈1\nK\nPK\nn=1\n\u0002P∞\ni=t γi−tr(sn\ni , an\ni )\n\u0003\nwhere we average the cu-\nmulative rewards over multiple full paths starting from st. The Temporal Difference (TD) estimate,\non the other hand, builds on the temporal relationship between states without requiring complete\ntrajectories. It estimates Vπ(st) using the immediate reward and the estimated value of the next\nstate: Vπ(st) ≈1\nK\nPK\nn=1\n\u0002\nr(st, an\nt ) + γVπ(sn\nt+1)\n\u0003\n. This approach effectively uses a ”bootstrap” by\ndepending on the estimated value of the next state to approximate Vπ(st).\nPolicy Improvement. The policy improvement process aims to update and improve policy accord-\ning to the result from the policy evaluation process. Specifically, it replaces the old policy πold with\nthe new one πnew to increase the expected return: Vπnew (s0) ≥Vπold (s0). In the environment with\nsmall, discrete action spaces, such improvements can be achieved by greedily choosing the action\nthat maximizes Qπold(s, a) at each state:\nπnew(· | s) = arg max\n¯π(·|s)∈P(A)\nEa∼¯π [Qπold(s, a)] , ∀s ∈S,\n(2)\nAnother improvement method involves applying policy gradient ascent (Sutton et al., 1999). It\nparameterizes the policy πθ with θ and we can derive the analytical policy gradient:\n∇θVπθ (s0)|θ=θold = E(s,a)∼Pπθold\n\u0014\n∇θ log πθ(a|s)Qπθold (s, a)\n\f\f\f\nθ=θold\n\u0015\n,\n(3)\nWhere s0 denotes the initial state. By choosing a small step-size α > 0 to conduct gradient ascent:\nθnew = θ + α∇θVπθ(s0)|θ=θold , we can guarantee the policy improvement: Vπnew (s0) ≥Vπold (s0).\n3\nNATURAL LANGUAGE REINFORCEMENT LEARNING\nIn contrast to the precise statistical models used in traditional RL, NLRL frames all ele-\nments—including task objectives, value evaluations, and strategic policies—within the form of nat-\nural language. This section aims to navigate decision-making tasks using natural language, aligning\nit with traditional RL concepts, definitions, and equations. Due to the inherent ambiguity of nat-\nural language, the equations presented here are not strictly derived from mathematical definitions.\nInstead, they are analogical and based on empirical insights into original RL concepts. We leave\nrigorous theoretical definition and analysis for future work.\n3.1\nANALOGIES\nWe start with analogies in traditional RL to model NLRL. We provide Fig. 2 to illustrate most\nconcepts discussed in this section.\n3\nPreprint. Work in progress.\n0.5\nGet Reward Values\n1\nReinforcement Learning (RL)\nTraditional\nAgent\nWorld\nNatural Language\nReinforcement Learning (NLRL)\nValue function in RL\nThe Goal is one-step ahead\nGo straight\nYou reach the goal\nState\n[-1,0]\nThought: \"The goal is one-step ahead\";\nAction: \"Go straight\"\nGet Language Feedback\n\"You reach the goal\"\nLanguage State\n[\"You are at the cross\"]\nLanguage Value function in NLRL\nLanguage policy\nPolicy:\nBellman Equation\nLanguage Bellman\nEquation\nLanguage\nAgent\nFigure 2: RL and NLRL framework\nText-based MDP: To conduct RL in natural\nlanguage space, we convert traditional MDP to\nthe text-based one, which leverages text descrip-\ntions to represent MDP’s basic concepts, includ-\ning state s, action a, and environment feedback\n(state transitions P and reward r).\nLanguage Task instruction:\nFor decision-\nmaking tasks, NLRL defines a natural language\ntask instruction TL, like “reaching the goal” or\n“opening the door”.\nThen, we denote a met-\nric by F that measures the completeness of the\ntask instruction given the trajectory description\nDL(τπ), where DL is a language descriptor that can transform the trajectory distribution τπ into its\ncorresponding language description DL(τπ). The objective of NLRL is reformulated as\nmax\nπ\nF(DL(τπ), TL)\n(4)\nThat is, NLRL is trying to optimize the policy so that the language description of the trajectory\ndistribution τπ can show high completeness of the task instruction.\nLanguage Policy: Instead of directly modeling action probability, NLRL determines the action\nwith chain-of-thought process (Wei et al., 2022b), including strategic thoughts, logical reasoning,\nand planning. Thus, we represent the policy on language as πL(a, c|s) = πL(c|s)πL(a|c, s), which\nwill first generate such thought process πL(c|s), then output the final action probability π(a|c, s).\nLanguage Value Function: Similar to the definition of Q and V in traditional RL, NLRL leverages\nlanguage value function, relying on natural language evaluation to assess the policy effectiveness.\nThe language state value function V L\nπ and language state-action value function QL\nπ are defined as:\nQL\nπ(st, at) = D\n\u0000(s, a)t+1:∞∼Pπ | st, at, TL\n\u0001\n, V L\nπ (st) = D\n\u0000at, (s, a)t+1:∞∼Pπ | st, TL\n\u0001\n(5)\nGiven the current state st or state-action (st, at), QL\nπ and V L\nπ leverage language descriptions instead\nof scalar value to demonstrate the effectiveness of policy for achieving the task objective TL. The\nlanguage value functions are intuitively rich in the information of values and enhance interpretability\nrather than the traditional scalar-based value. It can represent the evaluation results from different\nperspectives, consisting of the underlying logic/thoughts, prediction/analysis of future outcomes,\ncomparison among different actions, etc.\nLanguage Bellman Equation: In the traditional Bellman equation (Equ. 1), the state evaluation\nvalue Vπ(st), can be decomposed into two parts: : (1) the intermediate transition, which include\nimmediate at, reward rt, and next state st+1. (2) the next state evaluation Vπ(st+1). Based on such\ndecomposition intuition, we argue that the language value function V L\nπ should analogically satisfy\nthe language Bellman equation Equ.6.\nV L\nπ (st) = Gat,st+1∼Pπ\n1\n\u0010\nG2\n\u0000d\n\u0000at, r (st, at) , st+1), V L\nπ (st+1\n\u0001\u0001 \u0011\n, ∀st ∈S,\n(6)\nwhere d (at, r (st, at) , st+1)) depicts the language description of intermediate transition, while G1\nand G2 serves as two information aggregation functions. By drawing an analogy to Equ. 1, G2 mim-\nics the add the summation operation ‘+’ in the original Bellman equation, aggregating information\nfrom intermediate transition’s description d and future evaluation V L\nπ (st+1). Meanwhile, G1 serves\nthe role of the expectation operator E, aggregating information accross different (at, st+1) pairs by\nsampling from the transition distribution Pπ.\n3.2\nLANGUAGE GENERALIZED POLICY ITERATION\nIn this part, we introduce how language GPI is conducted. Similar to traditional GPI, language GPI\nalso consists of two procedures–language policy evaluation and language policy improvement.\n3.2.1\nLANGUAGE POLICY EVALUATION\nLanguage policy evaluation aims to estimate language value function V L\nπ and QL\nπ for each state. We\npresent how two classical estimations: MC and TD estimate work in language policy evaluation.\n4\nPreprint. Work in progress.\nMonte-Carlo Estimate\nAggregating evaluation\nfrom multi trajectories\nAggregating all steps in\none trajectory\nIntermediate changes over\naction, reward and next state\nReward decay\nLanguage Monte-Carlo Estimate. Starting from the state st,\nMC estimate is conducted over text rollouts (i.e. K full trajec-\ntories {at, (s, a)t+1:∞}) given the policy π. Since we cannot\ntake the average operation in language space, we instead lever-\nage language aggregator G1 to aggregate information over fi-\nnite trajectories, approximating the expected evaluation:\nV L\nπ (st) ≈G1\n\u0010\b\nan\nt , (s, a)n\nt+1:∞\n\tK\nn=1\n\u0011\n,\n(7)\nTemporal Difference Estimate\nAggregating evaluation from\nmulti one-step samples\nAggregating intermediate\nchanges and future evaluation\nIntermediate changes over\naction, reward and next state\nFuture evaluation over \nnext state\nFuture evaluation decay\nLanguage Temporal-Difference Estimate.\nLan-\nguage TD estimate mainly relies on the one-step lan-\nguage Bellman equation illustrated in Equ. 6. Similar\nto the language MC estimate, we aggregate K one-\nstep samples to approximate the expected evaluation:\nV L\nπ (st) ≈G1\n\u0010\b\nG2\n\u0000d(st, an\nt , r(st, an\nt ), sn\nt+1), V L\nπ (sn\nt+1)\n\u0001\tK\nn=1\n\u0011\n, ∀st ∈S,\n(8)\nwhere d, G1 and G2 has the same meaning for that in Equ.6.\nCheck Appendix D for more about how language MC/TD corresponds to the traditional MC/TD.\nLanguage MC estimate is free from estimation “bias” * as it directly utilizes samples from complete\ntrajectories. However, the MC method is prone to high “variance” considering the significant number\nof variations (Large K) in the long-term future steps. Such variability poses a challenge for the\nlanguage aggregator G1 in Equ. 7 to effectively extract crucial information from diverse trajectories.\nOn the contrary, while the inaccuracy of the next state evaluation V L\nπ (st+1) can bring estimation\n“bias” to TD estimate, they effectively reduce “variance” by discarding future variations. G1 and\nG2 are only required to conduct simple one-step information aggregation with limited variations.\n3.2.2\nLANGUAGE POLICY IMPROVEMENT\nSimilar to traditional policy improvement, the motivation of language policy improvement also aims\nto select actions that maximize the task completeness function F:\nπnew(· | s) = arg max\n¯π(·|s)∈P(A)\nF(QL\nπold(s, a), TL), ∀s ∈S\n(9)\nThe task completeness F is difficult to quantify for general language-based tasks, as it largely relies\non human textual prior knowledge. Given this complexity, instead of mathematically optimizing F,\nwe also refer to the chain-of-thought process–NLRL leverages a language analysis process I that\ncan generate the thought process c to guide policy optimization and action selection.\nπnew(· | s), c = I(QL\nπold(s, a), TL), ¯π(· | s) ∈P(A), ∀s ∈S.\n(10)\nLanguage policy improvement conducts strategic analysis c to determine the most promising action\nfor task completion as the new policy πnew(· | s). Ideally, this analysis is mainly based on the\ncorrelation judgment between the language evaluation QL\nπold(s, a) and task objective TL.\n4\nPRACTICAL NLRL IMPLEMENTATION WITH LLMS\nSection 3 outlines the core philosophy of NLRL, which involves translating key concepts from RL\ninto their human natural language counterparts. To practically implement these key concepts, a\nmodel capable of understanding, processing, and generating language is essential. Large language\nmodels (LLMs), trained on vast corpora of human language and knowledge, emerge as a natural\nchoice to emulate human behaviour and implement language-based RL components. In the follow-\ning sections, we demonstrate how LLMs can function in various roles within the NLRL framework.\n*We use quotes for “bias” and “variance” to indicate that we draw on their conceptual essence, not their\nstrict statistical definitions, to clarify concepts in NLRL.\n5\nPreprint. Work in progress.\n4.1\nCOMPONENTS AND PROCEDURES\n①LLMs as language policy (πL). Many works adopted LLMs as the decision-making agent (Wang\net al., 2023a; Feng et al., 2023a; Christianos et al., 2023; Yao et al., 2022) with Chain-of-thought\nprocess (Wei et al., 2022b). By setting proper instructions, LLMs can leverage natural language to\ndescribe their underlying thought for determining the action, akin to human strategic thinking.\n②LLMs as language value function approximator (QL, V L). The original idea of value function\napproximation (Sutton et al., 1999) is to use a parameterized one-dimensional output function to\nreplace the big value table and serve as a value function approximator. Such design largely helps RL\nto handle high-dimensional and large-scale decision-making problems. Similarly, in NLRL, we can\nnaturally leverage (multi-modal) LLMs, to evaluate the state s or state-action pair (s, a), and serve\nas the language value function approximator for QL, V L. This exactly corresponds to what (multi-\nmodal) LLMs are capable of – they are designed to take in the features from the task state, such as\nlow-dimension statistics, text, or images, and output the corresponding language understanding. By\nfurther prompting or fine-tuning over evaluation dataset, LLMs can generate language assessment.\n③LLMs as language Monte-Carlo and TD operator (G1, G2). As mentioned in Sec 3.2.1, we can\nestimate the language value function by language MC or TD estimate. The key issue is implement-\ning language aggregator G1, G2. To achieve this, we can leverage the inherent strengths of LLMs in\naggregating and synthesizing diverse linguistic inputs. Specifically, LLMs can be powerful informa-\ntion summarizers (Zhang et al., 2023), extractors (Xu et al., 2023), and aggregators to help us fuse\nintermediate transitions and future language evaluations. LLMs can serve as G1, G2 by prompting\nthem to summarize and aggregate multiple rollout trajectories (language MC), or multiple few-step\ntransitions and future states’ evaluation (language TD).\n④Distilling language value estimation into language value function approximator. ③provides\nan unsupervised and scalable way to generate language evaluation data through environment in-\nteraction, which can be leveraged to train our language value function approximator in ②. This\ncorresponds to traditional critic training for value-based RL (e.g, DQN (Mnih et al., 2015)) or actor-\ncritic algorithms (e.g, PPO (Schulman et al., 2017)), but happens in natural language representation\nspace. The only difference is the training objective – the scalar-based value function regresses the\nvalue mean with L2 mean squared error while the language value function tries to imitate the lan-\nguage aggregation result with supervised-finetuning’s cross-entropy loss.\n⑤LLMs as policy improvement operator (I). With the chain-of-thought process and prior knowl-\nedge about the world, LLMs are better to determine the most promising action πnew(· | s) by taking\nlanguage analysis c over the correlation of language evaluation QL\nπold(s, a) and task objective TL.\nThe underlying idea also aligns with some recent works (Kwon et al., 2023a; Rocamonde et al.,\n2023) that leverage LLMs or Vision-language models as the reward–they can accurately model the\ncorrelation. Specifically, for a given state s, we prompt the LLM with several action candidates and\ntheir corresponding language evaluations QL, by which we can obtain the improved action with a\nchain-of-thought process analyzing different actions’ evaluation results.\n⑥Language policy training by distilling improved policy. Similar to ④, we can train our lan-\nguage policy LLM in ①by supervised-finetuning over the improved chain-of-thought based lan-\nguage policy data from ⑤. This corresponds to the traditional policy training in policy-based RL\n(e.g, REINFORCE (Williams, 1992)) or actor-critic algorithms (e.g, PPO (Schulman et al., 2017)).\nHowever, NLRL relies on the more stable supervised loss instead of the policy gradient loss.\nWe also provide example visualizations in Fig 1 to illustrate these components and procedures.\nBuilding upon these components and procedures, we can create various NLRL applications. Here\nwe illustrate three use cases, though many more possibilities exist beyond these examples.\n4.2\nLANGUAGE GPI BOOSTS LLM’S CRITIC AND POLICY BY PROMPTING (①, ②, ③, ⑤)\nOur first case utilizes language GPI to enhance LLM’s critic and policy solely through prompting,\nwhich can be particularly beneficial for improving proprietary models such as GPT-4 (OpenAI,\n2023) or Gemini (Team et al., 2023). Specifically, we first combine ①, ②, ③to build a language\npolicy evaluation pipeline. Take language TD shown in Equ. 8 as an example. We prompt LLMs to\n(1) evaluate the subsequent state’s value V L\nπ (st+1) (②) (2) serve as the TD operator G1, G2 (③). By\nperforming a one-step look-ahead with at ∼π (①) and leveraging G1, G2 to aggregate information\n6\nPreprint. Work in progress.\nfrom intermediate transition d alongside the subsequent evaluation V L\nπ (st+1), language TD can\ngenerate new and improved language evaluation, V L\nπ (st)new. Building upon this new evaluation,\nLLM-based policy improvement operator I (⑤) converts the evaluation into a better policy πnew. We\nrefer the reader for Algorithm 1 for detailed procedures.\n4.3\nTRAINING NATURAL LANGUAGE VALUE FUNCTION FOR A GIVEN POLICY (②, ③, ④)\nOur second case aims to train an LLM critic capable of evaluating any given state with natural\nlanguage explanations, similar to a chess annotator who provides insightful commentary on boards,\nmoves, and strategies. For example, we can build an iterative language TD pipeline by combining ②,\n③, and ④. First, we leverage a tunable LLM A and prompt it to become a language value function\napproximator ②. Combined with the look-ahead transitions (at, st+1) ∼Pπ by taking rollouts with\npolicy π, and the subsequent state evaluation V L\nπ (st+1) generated by A, we prompt LLM B for the\nlanguage TD estimate (③), similar to Sec 4.2. The model A is further finetuned by such language\nTD estimates (④) and will be plugged back for V L\nπ (st+1)new in a new iteration. Iteratively, we can\nobtain the final converged natural language value function. Check Algorithm 2 for more details.\n4.4\nNATURAL LANGUAGE ACTOR-CRITIC LEARNING (①, ②, ③, ④, ⑤, ⑥)\nLast, we combine all these procedures and build the full natural language actor-critic pipeline. Simi-\nlar to traditional actor-critic (Barto et al., 1983; Sutton et al., 1999), our natural language actor-critic\nframework simultaneously learns both a language policy and a language critic through unsupervised\nenvironment interactions. For each iteration, we first use language policy ①to take rollouts in the\nenvironment. With new trajectories, we update our language value model ②by language MC or TD\n③and train it with supervised-finetuning loss (④). For language policy improvement, we query our\nupdated language value to evaluate action candidates for states extracted from the rollout trajecto-\nries. Further LLM-based improvement operator ⑤brings us a stronger policy, which will be used to\ntrain our language policy (⑥). Check Algorithm 3 for more details.\n5\nEXPERIMENTS\n5.1\nLANGUAGE GPI BY PROMPTING (SEC 4.2)\nFigure 3: The “double T” maze and\nthe “medium” maze.\nAt the be-\nginning of each test, the agent ran-\ndomly respawns in white space.\nOur first experiments explore Sec 4.2 and leverage language\nGPI to improve LLM capability with pure prompting. Specif-\nically, we choose the maze games of LMRL (Abdulhai et al.,\n2023), aiming to validate that Language TD Estimate and\nLanguage Policy Improvement can benefit the evaluation\nand further improve policy. We use the original settings of\nLMRL Gym Maze, where the agent is required to navigate to\na goal position in a “Double T” or “Medium” maze. We con-\nsider the fully-observable setting, where the agent’s observa-\ntion (described by text) includes the agent’s current position in\nthe maze, the agent’s action history, the walls’ position around\nthe agent (if any), and the goal position. The action space is discrete, including moving up / down /\nright / left. We evaluate the performance on 30 different initial positions, each with 3 random seeds.\nRefer to Appendix B.1.2 for more experimental details and result visualizations.\nFor language TD estimate in Equ. 8, we prompt gpt-4o-mini-2024-07-18 as the language aggregator\nG1, language state-action value aggregator G2, and language state value function V L\nπ respectively.\nSpecifically, given an environment state, for each candidate action, we use a fully random policy\nto rollout N steps into the future and use the language state value function V L\nπ to evaluate the\nlook-ahead state. For each state we repeat this process for K times and use G2 to aggregate into a\nstate-action value estimation. For language policy improvement in Equ. 10, the same GPI-4o-mini\nis leveraged as the improvement operator I.\nWe compare Language GPI with a few different baselines, including prompt-based language policy\nπL (①), as well as prompt-based language value function QL(s, a) + language policy improvement\nI. As shown in Table 1, Language TD produces better performance than the prompt-based language\n7\nPreprint. Work in progress.\nTable 1: Language GPI results with ablations on look-ahead steps N and variations number K.\nAvg Reward\nDouble T Maze\nMedium Maze\nLanguage policy πL(s) (①)\n−27.29 ± 4.43\n−27.05 ± 5.27\nLanguage value function QL(s, a) + improvement I (②,⑤)\n−18.33 ± 6.11\n−33.57 ± 14.41\nLanguage GPI (1 variation, 3 look ahead steps) (②, ③, ⑤)\n−17.85 ± 3.68\n−20.85 ± 7.59\nLanguage GPI (4 variations, 1 look ahead steps) (②, ③, ⑤)\n−17.48 ± 4.53\n−12.65 ± 4.72\nLanguage GPI (4 variations, 3 look ahead steps) (②, ③, ⑤)\n−12.74 ± 4.47\n−15.09 ± 4.44\nLanguage GPI (6 variations, 3 look ahead steps) (②, ③, ⑤)\n−12.15 ± 2.96\n−15.44 ± 4.97\nLanguage GPI (8 variations, 3 look ahead steps) (②, ③, ⑤)\n−11.19 ± 2.86\n−12.23 ± 4.49\nvalue in both mazes, and more variations & look ahead steps can have even better results. Language\npolicy improvement can only benefit stably for language-TD-enhanced language value (②and ③).\n5.2\nTRAINING NATURAL LANGUAGE VALUE FUNCTION WITH LANGUAGE TD (SEC 4.3)\nFigure 4: 5x5 Breakthrough.\nOur second experiment is an implementation of Sec 4.3. Specifi-\ncally, we aim to train a language value function with language TD\nin the 5x5 breakthrough board game (Fig 4). Breakthrough is a turn-\nbased board game for two players, white and black. Each player has\na few lines of pawns. Black moves first and in each turn, the player\nchooses one piece to move one square forward (straight or diago-\nnal) onto empty squares or capture diagonally. The goal is to break\nthrough the opponent’s line and reach their home row.\nAs mentioned in Sec 4.3, this value function can serve as a reliable\nboard evaluator/annotator. However, evaluating board states in this\ngame is challenging for LLMs. First, the 5x5 breakthrough variant\nis niche, leading to sparse pre-training data on board evaluations.\nMost LLMs have minimal knowledge of it. Second, despite the\nsmall board size, its state space complexity can reach 108 (Saffidine et al., 2012), making natural\nlanguage assessments significantly more difficult. Refer to Appendix B.2 for more experimental\ndetails and result visualizations.\n5.2.1\nENVIRONMENT, DATASET, MODEL, AND EVALUATION\nText-Breakthrough. As mentioned in the text-based MDP, we textualize the game, including board\nrepresentation, pieces, moves, positions, and information about capturing or termination.\nPolicy π in V L\nπ (s). Our first task is to determine the π in V L\nπ (s) since V L\nπ (s) measures the value\nfor a specific policy, according to Equ 5. Given that our ultimate goal is to train a reliable board\nannotator, it makes sense to select a strong or near-optimal policy—analogous to the way we place\ngreater trust in a grandmaster’s annotations when evaluating a position. In our experiment, our π\nis the Monte-Carlo Tree-Search (MCTS) algorithm (Kocsis & Szepesv´ari, 2006) from OpenSpiel\n(Lanctot et al., 2019), with a high number of simulations and rollouts to ensure an extensive search.\nBuilding State Dataset s ∼P(s). Since V L\nπ (s) is over state s, our second step is to build a state\ndataset Ds–the distribution P(s) that our V L\nπ works on. To ensure that V L\nπ can assess positions\nacross different levels of gameplay, we build a mixed state dataset. Specifically, we maintain a pool\nof MCTs policies, where each policy variant is characterized by a distinct number of simulations\nand rollouts. The mixed state dataset is then created by pairwise combining policies from this pool,\nrunning rollouts for each policy pair, and merging the resulting data from all combinations. Then\nwe can easily split Ds to build training state set Dtrain\ns\nand test state set Dtest\ns\n.\nBuilding TD training dataset. The TD training dataset is built by conducting look-ahead expansion\nwith rollout policy π, from state training set Dtrain\ns\n. For each state st, we conduct a few times l-step\nlook-ahead rollout and deduplicate to K distinct variations: {(st+i, at+i, rt+i, st+i+1)l−1\ni=0}K.\nModels. We use two LLMs for the full language-TD pipelines. For the language TD operator\n(G1, G2), we prompt the large LLaMA-3.1-70B-Instruct (Dubey et al., 2024) model since it pre-\nserves stronger aggregation and reasoning ability. Because of the limitation of computing resources,\nthe language value function V L\nπ (s) is trained from the small LLaMA-3.1-8B-Instruct model.\n8\nPreprint. Work in progress.\n0\n10\n20\n30\n40\n50\nIteration\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nEvaluation Accuracy\n(a) Performance vs Baselines\nBest LVF\nGPT4o\nLLaMA3.1-70B\nLLaMA3.1-8B\n0\n10\n20\n30\n40\n50\nIteration\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nEvaluation Accuracy\n(b.1) Variation Number (Train set)\nVariation=1\nVariation=2\nVariation=4\n0\n10\n20\n30\n40\n50\nIteration\n0.60\n0.65\n0.70\n0.75\n0.80\nEvaluation Accuracy\n(b.2) Variation Number (Test set)\nVariation=1\nVariation=2\nVariation=4\n0\n10\n20\n30\n40\n50\nIteration\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nEvaluation Accuracy\n(c) State Scaling Law (Test set)\nSize=0.4k\nSize=2k\nSize=10k\nSize=45k\n0\n10\n20\n30\n40\n50\nIteration\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nEvaluation Accuracy\n(d.1) Look-ahead Step Number (Train set)\nStep=2\nStep=4\nStep=8\n0\n10\n20\n30\n40\n50\nIteration\n0.60\n0.65\n0.70\n0.75\n0.80\nEvaluation Accuracy\n(d.2) Look-ahead Step Number (Test set)\nStep=2\nStep=4\nStep=8\nFigure 5: Breakthrough experiment results. (a) Performance comparison with baselines. (b,d) Ab-\nlation study over look-ahead step number and variation number. (c) Results for state scaling law.\nEvaluation. The final step is to develop an automatic metric for evaluating the language value func-\ntion/board evaluation, which is challenging even for advanced LLMs like GPT-4o, given their limited\ndomain knowledge. Thus, we adopt a coarser metric—judging which side has the advantage—as\nour evaluation criterion. Specifically, for any given state, we estimate win rates using Monte Carlo\nsimulations by running n rollouts to game completion and computing the outcomes. The side with\nthe higher win rate serves as the ground-truth label. To reduce ambiguity, we only include states\nwhere one side’s win rate surpasses a predefined threshold in the evaluation set.\n5.2.2\nEXPERIMENTAL RESULTS\nPerformance vs Baselines. Our fig 5.a shows the evaluation accuracy comparison on Dtest\ns\n, be-\ntween our best language value function’s training curve and prompting-based LLMs (LLaMA-3.1-\n70B, LLaMA-3.1-8B and GPT-4o-08-06), which clearly demonstrates the necessity of language TD\ntraining. Due to the lack of domain knowledge, all prompting-based LLMs perform poorly. The\nbest accuracy (0.61) is only slightly higher than the random guess (0.5), while our trained language\nvalue function dominates the task and the accuracy reaches more than 0.85 on the test set Dtest\ns\n.\nLook-ahead Ablation. Fig 5.b and Fig 5 shows further ablation study over variation number (m)\nand look-ahead steps k. The test set performance is shown in Fig 5.b.2 and Fig 5.d.2, while Fig 5.b.1\nand Fig 5.d.1 present a subset of in-training state evaluations. The ablation study indicates that\ndetermining the appropriate m and k is crucial for optimal performance. Notably, in Fig 5.d.1 and\nFig 5.d.2, using 8 steps resulted in significantly higher in-training accuracy but slightly lower test\nset accuracy compared to 4 steps. This suggests that longer look-ahead steps may lead to overfitting,\npotentially hindering generalization to novel states.\nState Scaling Law. We also examine how scaling the training state size affects performance in\nFig 5.c, with datasets size 0.4k, 2k, 10k, and 45k. The results demonstrate robust scalability of our\nalgorithm, with consistent improvements in test set performance as the training set size increases.\n5.3\nNATURAL LANGUAGE ACTOR-CRITIC LEARNING (SEC 4.4)\nFor validating Sec 4.4, we implement the natural language actor-critic pipeline in a model-free\nsetting (Sutton & Barto, 2018) in the Tic-Tac-Toe game (Ju, 2024), where the system learns purely\nfrom sampled trajectories without access to environment dynamics. As a supplement to language\nTD in Sec. 5.1 and 5.2, we utilize the language MC (Equ. 7) for the language policy evaluation.\n5.3.1\nSTABLIZING LANGUAGE VALUE FUNCTION\nDespite the soundness of Sec 4.4, we identify training instability resulting from the language value\nfunction, mainly from two perspectives. First, we observed that the language value function V L\nπ can\n9\nPreprint. Work in progress.\n0\n2\n4\n6\n8\n10\nIteration\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nWin Rate\n+0.46\n(a) First-Move Opponent\nNatural Language Actor Critic\nNatural Language Actor Critic w/o Action Selection Mask\nPPO\nGPT-4o as Policy\nLLaMA-3.1-70B-Instruct as Policy\nLLaMA-3.1-8B-Instruct as Policy\n0\n5\n10\n15\n20\n25\n30\nIteration\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWin Rate\n+0.50\n(b) Random-Move Opponent\nNatural Language Actor Critic\nNatural Language Actor Critic w/o Action Selection Mask\nPPO\nGPT-4o as Policy\nLLaMA-3.1-70B-Instruct as Policy\nLLaMA-3.1-8B-Instruct as Policy\nFigure 6: Natural Language Actor Critic Pipeline training results against different opponents. (a)\nTraining results against the First-Move Opponent. (b) Training results against the Random-Move\nOpponent. These results demonstrate that our proposed Natural Language Actor Critic pipeline\ncan stably improve under deterministic and stochastic dynamics.\neasily hallucinate when evaluating unseen state-action pairs, leading to incorrect language policy\nimprovement. To address this, we add an action selection mask to restrict language policy improve-\nment only to moves that are likely to be chosen by the language policy πL during rollout. Instead of\nprompting LLM to select from all available moves, we sample the language policy Nsample times\nand build an action candidate list. The language policy improvement is conducted from the list’s\ntop-m moves. Such selection mask can help constrain policy updates within a limited range, akin to\nhow KL divergence is used in RLHF objective function (Ouyang et al., 2022).\nSecond, we encountered the challenge of catastrophic forgetting for value training. Specifically,\nLLM can rapidly overfit to the state-action pairs encountered in the current iteration while forgetting\npast experiences, e.g. state-action pairs encountered only a few iterations ago. To address this, we\nenlarge the language value function training set by merging the last Kbuffer value function buffers,\nwhich help stabilize the training of the language value function by preserving past knowledge.\n5.3.2\nEXPERIMENT SETUP AND METHODS\nText-TicTacToe. Similar to text-breakthrough, we textualize all information in the Tictactoe game.\nModel Architecture.\nOur implementation uses three language models: one LLaMA-3.1-70B-\nInstruct and two LLaMA-3.1-8B-Instruct models. The 70B model implements the language aggre-\ngator (G1) and policy improvement operator I - notably, we restrict this model to use only its general\nlanguage processing and information aggregation capabilities, rather than any game-specific knowl-\nedge it may have learned during pre-training. This restriction is enforced through carefully designed\nprompts (see Appendix C.3 for details). The two 8B models implement our trainable components:\none serves as the language policy πL, generating actions through chain-of-thought reasoning, while\nthe other implements the language value function QL\nπ, evaluating state-action pairs.\nDataset Construction. For each iteration, we collect 512 complete game trajectories with πL. For\nMonte Carlo value estimation, we collect KMC = 5 complete trajectories for each state-action pair.\nFor policy improvement, πL proposes Nsample = 10 candidate actions per state and we select the\ntop-m (m = 10) actions from the candidate action list.\nEvaluation Protocol. To thoroughly evaluate our approach, we test against two different types of\nopponents. The first is a deterministic opponent that always takes the first available action, while the\nsecond is a stochastic opponent that plays randomly, presenting a more challenging case for policy\nevaluation. We measure performance through metrics including win/loss/tie rates.\nBaselines. We compare our algorithm with prompting-based method using GPT-4o-08-06, LLaMA-\n3.1-8B/70B-Instruct with step-by-step reasoning. We also include a traditional RL baseline by fine-\ntuning LLaMA-3.1-8B-Instruct with standard PPO (Schulman et al., 2017).\n5.3.3\nEXPERIMENTAL RESULTS\nPerformance against baselines with different opponents The learning curves in Fig. 6 demon-\nstrate our system’s superiority against all other baselines in different settings. Our algorithm out-\n10\nPreprint. Work in progress.\n0\n5\n10\n15\n20\n25\n30\nIteration\n0.4\n0.5\n0.6\n0.7\n0.8\nWin Rate\n(a) Number of Training Epochs\nTrain Epoch = 1\nTrain Epoch = 2\nTrain Epoch = 3\n0\n5\n10\n15\n20\n25\n30\nIteration\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWin Rate\n(b) KMC\nKMC = 1\nKMC = 2\nKMC = 5\n0\n5\n10\n15\n20\n25\n30\nIteration\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWin Rate\n(c) Kbuffer\nKbuffer = 1\nKbuffer = 3\nKbuffer = 5\n0\n5\n10\n15\n20\n25\n30\nIteration\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWin Rate\n(d) Number of Rollout Trajectories\nRollout = 128\nRollout = 256\nRollout = 512\nFigure 7: Ablation studies on number of training epochs, Monte Carlo sample size KMC, experience\nbuffer size Kbuffer, and number of rollout trajectories.\nperforms all other baselines, including the PPO baseline, which helps verify the effectiveness and\nefficiency brought by NLRL. The improvement is substantial, especially in the random-move oppo-\nnent setting (Fig. 6(b)), demonstrating NLRL’s robustness despite the stochastic dynamics. Fig. 6\nalso presents the necessity of our proposed action selection mask in Sec 5.3.1.\nAblation Studies\nWe conduct comprehensive ablation studies under a reduced computational bud-\nget of 128 trajectories per iteration to investigate key hyperparameters. Our analysis of training\nepochs reveals that increasing from 1 to 3 epochs significantly improves stability and adaptation, as\nshown in Fig. 7(a). Second, our investigation of Monte Carlo sampling with KMC values of 1, 2,\nand 5 shows in Fig. 7(b) that increased sampling leads to more stable training, where KMC = 5\nachieves the best performance. Third, we examine the effect of experience buffer size Kbuffer on\nlearning in Fig. 7(c), demonstrating that when Kbuffer = 3 helps maintain consistent performance\nby preserving past experiences. Finally, Fig. 7(d) shows how the number of rollout trajectories af-\nfects training, with 512 trajectories per iteration yielding the most robust learning curve. Additional\nablation studies on action selection parameter top-m can be found in Appendix B.3.5, where we\ndemonstrate that larger m values enhance exploration while maintaining stable learning.\nRefer to Appendix B.3 for more experimental details and result visualizations.\n6\nRELATED WORK\nLanguage Model Based Autonomous Agent. Inspired by the strong emergent capabilities of LLMs\nin complex reasoning and planning scenarios (Brown et al., 2020; Wei et al., 2022a; Anil et al.,\n2023; OpenAI, 2023; Feng et al., 2023b). The field of language model-based autonomous agents\n(Feng et al., 2023a; Christianos et al., 2023; Zhang et al., 2024) has witnessed a growing trend\nof leveraging LLMs for high-level planning purposes. For text agents, ReAct(Yao et al., 2022)\nleverages chain-of-thought planning about the intermediate goals and task-specific generation via\nfew-shot prompting. Reflexion (Shinn et al., 2023) is built upon ReAct (Yao et al., 2022) with self-\nreflection, named verbal reinforcement learning, to generate tips given online feedback. Such tips\nstrengthen agent’s capability through in-context learning. (Zhong et al., 2024) proposes to distill\nsuch reflection ability from the large model to train a small policy feedback model that can guide\nthe policy. Their underlying ideas share similarities with NLRL’s language Monte-Carlo estimate,\nwhile NLRL formulates this process more formally and incorporates it into training–we extract\ncore information/concepts by sampling multiple trajectories and leverage them to train our natural\nlanguage value function. Another line of work like Zhang et al. (2023); Xu et al. (2023) leverages\nlarge language models to summarize and extract information to help fuse intermediate changes.\nThey are related to the ”aggregation” idea of language value function training.\nInterpretable Reinforcement Learning. One of the major purposes of interpretable RL is to au-\ntomatically seek explanations from non-AI experts. For instance, methods of concept-based expla-\nnations for sequential decision-making tasks. Ji et al. (2023) provide a concept-based explanation\nfor 3D action recognition CovNets by clustering learned human interpretable features. Sreedharan\net al. (2020) formulates concept-based explanations upon state preconditions and action costs, rep-\nresenting any factual statement a user associates with a given state. Similarly, Hayes & Shah (2017)\nuses logical formulas to summarize policies. Additionally, Das et al. (2023) trains a joint embedding\nmodel for state-action pairs and concept-based explanations.\n11\nPreprint. Work in progress.\nLearning from Language Feedback. Our work is also related to research on learning from lan-\nguage feedback. While Cheng et al. (2023) focuses on benchmarking algorithms, we aim to propose\na new algorithmic framework. Studies such as (Yang et al., 2023; Yuksekgonul et al., 2024; Cheng\net al., 2024) introduce an LLM-based optimization paradigm that leverages natural language to rep-\nresent optimization operators like gradients and backpropagation, achieving end-to-end generative\noptimization. NLRL, on the other hand, represents a parallel approach, offering a generative frame-\nwork specifically designed for RL optimization problems.\nLLMs as evaluation function. Our language value function aligns with recent efforts in NLP that\nleverage LLMs as generative evaluators or verifiers, as seen in (Wang et al., 2023b; Li et al., 2023;\nJiang et al., 2023; Gao et al., 2024; Zhang et al., 2024; Mahan et al., 2024), or that utilize LLMs’ self-\nevaluation capabilities to enhance task-solving performance (Putta et al., 2024; Yao et al., 2023; Hao\net al., 2023). These approaches often rely on the LLM’s self-assessment (which can be unreliable),\nor distill on stable supervision signals like human annotations or guidance from stronger LLMs. In\ncontrast, NLRL’s training of a language value function offers a novel unsupervised approach: we can\ntrain a generative evaluator solely by aggregating language feedback from the environment, without\ndistilling any strong model–experimental results in Sec. 5.2 show that our language value function’s\nperformance is much better than any LLMs we utilize during the training.\nSuccessor Features. Successor features (Dayan, 1993; Barreto et al., 2017) aim to learn a high-\ndimensional value function representation, where each component captures the expected future oc-\ncurrence of all state representations under a fixed policy. As shown in (Barreto et al., 2017), succes-\nsor features help decouple the environment dynamics from the rewards, facilitating transfer learning\nin RL. The language value function in NLRL is conceptually similar to successor features since it\nalso represents state values in a high-dimensional space and satisfies the language Bellman equation.\n7\nCONCLUSION, LIMITATIONS, AND FUTURE WORK\nIn this work, we propose a new paradigm NLRL, which reformulates the RL process with natural\nlanguage representation. We believe NLRL opens a new door for understanding and implement-\ning RL algorithms in natural language space, improving agent’s effectiveness, efficiency and in-\nterpretability. NLRL is also compatible with the current language model and LLM-based agent.\nIt serves a potential way for generating high-quality language synthetic data for policy and critic,\nwhich could be crucial in training more advanced language models and agents.\nLimitations.\nCurrently, NLRL still faces several limitations. First, our experiments demonstrate\nits effectiveness primarily in environments with discrete action spaces and low-dimensional (tex-\ntualisable) states. Its performance in environments involving continuous action spaces and high-\ndimensional states (e.g., low-level robotic control tasks) remains largely unexplored. Second, our\nexperiments are primarily conducted in small-scale settings due to the substantial computational cost\nassociated with invoking large language models. The time efficiency of the current NLRL approach\nis significantly lower than that of traditional RL methods using smaller networks.\nFuture work.\nGiven NLRL’s potential, there are several promising directions for future research.\nFirst, a theoretical foundation for the entire framework in the language space is needed to move\nbeyond analogical reasoning towards a more robust, principled basis. This would help formalise the\nframework and enhance its generalisability. Another important direction is exploring how NLRL can\nbe integrated more organically with existing research on self-evaluation, self-improvement, LLM\nagent framework, and LLM’s planning. Such integration could unlock new capabilities and effi-\nciency improvements. Additionally, in terms of application domains, there is significant potential\nto extend NLRL beyond the current setups–we are actively exploring extending NLRL to general\ndomains beyond games, such as reasoning tasks (Cobbe et al., 2021) and code (Chen et al., 2021).\n8\nACKNOWLEDGEMENT\nWe thank Ziyan Wang and Yali Du for their efforts in the early-stage research of this paper.\n12\nPreprint. Work in progress.\nREFERENCES\nMarwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu,\nand Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language\nmodels. arXiv preprint arXiv:2311.18232, 2023.\nMarcin Andrychowicz, Anton Raichuk, Piotr Sta´nczyk, Manu Orsini, Sertan Girgin, Raphael\nMarinier, L´eonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What\nmatters in on-policy reinforcement learning?\na large-scale empirical study.\narXiv preprint\narXiv:2006.05990, 2020.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-\nlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\nAndr´e Barreto, Will Dabney, R´emi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt,\nand David Silver. Successor features for transfer in reinforcement learning. Advances in neural\ninformation processing systems, 30, 2017.\nAndrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can\nsolve difficult learning control problems. IEEE transactions on systems, man, and cybernetics,\n(5):834–846, 1983.\nRichard Bellman, Robert E Kalaba, et al. Dynamic programming and modern control theory, vol-\nume 81. Citeseer, 1965.\nKonstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Manon Devin, Alex X Lee,\nMaria Bauza Villalonga, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat:\nA self-improving generalist agent for robotic manipulation. Transactions on Machine Learning\nResearch, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nChing-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. Llf-bench:\nBenchmark for interactive learning from language feedback. arXiv preprint arXiv:2312.06853,\n2023.\nChing-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the next autodiff: Generative opti-\nmization with rich feedback, execution traces, and llms. arXiv preprint arXiv:2406.16218, 2024.\nFilippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan\nChen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, et al. Pangu-agent: A\nfine-tunable generalist agent with structured reasoning. arXiv preprint arXiv:2312.14878, 2023.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nDevleena Das, Sonia Chernova, and Been Kim. State2explanation: Concept-based explanations to\nbenefit agent learning and user understanding. arXiv preprint arXiv:2309.12482, 2023.\nPeter Dayan. Improving generalization for temporal difference learning: The successor representa-\ntion. Neural computation, 5(4):613–624, 1993.\n13\nPreprint. Work in progress.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nXidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni,\nYali Du, and Jun Wang. Chessgpt: Bridging policy learning and language modeling. arXiv\npreprint arXiv:2306.09200, 2023a.\nXidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-\nlike tree-search can guide large language model decoding and training.\narXiv preprint\narXiv:2309.17179, 2023b.\nMingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan. Llm-based nlg evaluation: Current\nstatus and challenges. arXiv preprint arXiv:2402.01383, 2024.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\nReasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992,\n2023.\nBradley Hayes and Julie A Shah. Improving robot controller transparency through autonomous\npolicy explanation. In Proceedings of the 2017 ACM/IEEE international conference on human-\nrobot interaction, pp. 303–312, 2017.\nYing Ji, Yu Wang, and Jien Kato.\nSpatial-temporal concept based explanation of 3d convnets.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n15444–15453, 2023.\nDongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. Tiger-\nscore:\nTowards building explainable metric for all text generation tasks.\narXiv preprint\narXiv:2310.00752, 2023.\nKim Jeong Ju. gym-tictactoe: A tic tac toe environment for openai gym, 2024. URL https:\n//github.com/haje01/gym-tictactoe. GitHub repository.\nLevente Kocsis and Csaba Szepesv´ari. Bandit based monte-carlo planning. In European conference\non machine learning, pp. 282–293. Springer, 2006.\nMinae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language\nmodels. arXiv preprint arXiv:2303.00001, 2023a.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Prin-\nciples, pp. 611–626, 2023b.\nMarc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay,\nJulien P´erolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel\nHennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, J´anos Kram´ar, Bart De\nVylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian\nSchrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis. Open-\nSpiel: A framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019. URL\nhttp://arxiv.org/abs/1908.09453.\nJunlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge\nfor evaluating alignment. arXiv preprint arXiv:2310.05470, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems, 36, 2024.\nDakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato,\nJan-Philipp Fr¨anken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint\narXiv:2410.12832, 2024.\n14\nPreprint. Work in progress.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level\ncontrol through deep reinforcement learning. nature, 518(7540):529–533, 2015.\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730–27744, 2022.\nPranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and\nRafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv\npreprint arXiv:2408.07199, 2024.\nShuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang,\nChengfei Lv, and Huajun Chen.\nAutoact: Automatic agent learning from scratch via self-\nplanning. arXiv preprint arXiv:2401.05268, 2024.\nJuan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. Vision-\nlanguage models are zero-shot reward models for reinforcement learning.\narXiv preprint\narXiv:2310.12921, 2023.\nAbdallah Saffidine, Nicolas Jouandeau, and Tristan Cazenave. Solving breakthrough with race pat-\nterns and job-level proof number search. In Advances in Computer Games: 13th International\nConference, ACG 2011, Tilburg, The Netherlands, November 20-22, 2011, Revised Selected Pa-\npers 13, pp. 196–207. Springer, 2012.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go\nwithout human knowledge. nature, 550(7676):354–359, 2017.\nSarath Sreedharan, Utkarsh Soni, Mudit Verma, Siddharth Srivastava, and Subbarao Kambham-\npati. Bridging the gap: Providing post-hoc symbolic explanations for sequential decision-making\nproblems with inscrutable representations. arXiv preprint arXiv:2002.01080, 2020.\nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning,\n3:9–44, 1988.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-\nods for reinforcement learning with function approximation. Advances in neural information\nprocessing systems, 12, 1999.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nA Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,\nRui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm\ninstruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023b.\n15\nPreprint. Work in progress.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\nmodels. arXiv preprint arXiv:2206.07682, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824–24837, 2022b.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8:229–256, 1992.\nDerong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng\nZheng, and Enhong Chen. Large language models for generative information extraction: A sur-\nvey. arXiv preprint arXiv:2312.17617, 2023.\nJie Xu, Hanbo Zhang, Xinghang Li, Huaping Liu, Xuguang Lan, and Tao Kong. Sinvig: A self-\nevolving interactive visual agent for human-robot interaction. arXiv preprint arXiv:2402.11792,\n2024.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\nChen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\nMert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and\nJames Zou. Textgrad: Automatic” differentiation” via text. arXiv preprint arXiv:2406.07496,\n2024.\nLunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh\nAgarwal.\nGenerative verifiers: Reward modeling as next-token prediction.\narXiv preprint\narXiv:2408.15240, 2024.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B\nHashimoto.\nBenchmarking large language models for news summarization.\narXiv preprint\narXiv:2301.13848, 2023.\nRui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin,\nQin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint\narXiv:2307.04964, 2023.\nVictor Zhong, Dipendra Misra, Xingdi Yuan, and Marc-Alexandre Cˆot´e. Policy improvement using\nlanguage feedback models. arXiv preprint arXiv:2402.07876, 2024.\n16\nPreprint. Work in progress.\nCONTENTS\nA Algorithms\n18\nA.1\nLanguage GPI by prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.2\nTrain natural language language value function with language TD\n. . . . . . . . .\n18\nA.3\nNatural language actor critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB\nExperimental details\n20\nB.1\nMaze Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.1.1\nExperiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.1.2\nLanguage GPI Visualization . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.1.3\nThe effect of Language TD on the Value function . . . . . . . . . . . . . .\n23\nB.2\nBreakthrough Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nB.2.1\nExperiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nB.2.2\nLanguage TD Visualization\n. . . . . . . . . . . . . . . . . . . . . . . . .\n25\nB.2.3\nComparison of language value function . . . . . . . . . . . . . . . . . . .\n29\nB.3\nTicTacToe Experiment\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nB.3.1\nTraining Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nB.3.2\nAlgorithm Implementation Details . . . . . . . . . . . . . . . . . . . . . .\n31\nB.3.3\nVisualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nB.3.4\nBaseline Implementation Details . . . . . . . . . . . . . . . . . . . . . . .\n42\nB.3.5\nAdditional Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nC Prompts\n43\nC.1\nMaze experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nC.2\nBreakthrough Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nC.3\nTicTacToe Experiment\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nD Analogy\n49\nD.1\nLanguage MC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nD.2\nLanguage TD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n17\nPreprint. Work in progress.\nA\nALGORITHMS\nIn this section, we provide the full pseudo-code for three cases mentioned in Sec. 4.2, 4.3, and 4.4.\nA.1\nLANGUAGE GPI BY PROMPTING\nAlgorithm 1 Language GPI by Prompting\nRequire: Initial language policy π, language value function V L\nπ , LLM implementing operators G1,\nG2, policy improvement operator I, number of lookahead steps N, number of variations K\n1: for each state st do\n2:\n// Language TD estimation\n3:\nfor i = 1 to K do\n4:\nSample action a(i)\nt\n(①)\n5:\nSimulate N steps ahead to get trajectory τ (i)\n6:\nPrompt LLM to evaluate V L\nπ (s(i)\nt+N)\n(②)\n7:\nGenerate intermediate description d(i) = d(a(i)\nt , r(i)\nt , s(i)\nt+1)\n8:\nCompute value estimate V (i) = G2(d(i), V L\nπ (s(i)\nt+N))\n(③)\n9:\nend for\n10:\nAggregate value estimates V L\nπ (st) = G1({V (i)}K\ni=1)\n(③)\n11:\n// Policy improvement\n12:\nUse V L\nπ (st) and prompt LLM as operator I to select the optimal action at\n(⑤)\n13: end for\nA.2\nTRAIN NATURAL LANGUAGE LANGUAGE VALUE FUNCTION WITH LANGUAGE TD\nAlgorithm 2 Train natural language language value function with language TD\nRequire: Pre-defined policy π, LLM-based language value model V L\nπ , language descriptor d, lan-\nguage aggregator G1, G2, state buffer Ds, number of iterations T, number of trajectories N,\nnumber of MC trajectories K\n1: // Collect few-step rollout data using fixed policy for TD data buffer\n2: Initialize TD data buffer B ←{}\n3: for state st in state buffer Ds do\n4:\nStarting from st, run policy π for a few l-step look-ahead rollouts and collect K distinct\nvariations: Vst = {(st+i, at+i, rt+i, st+i+1)l−1\ni=0}K\n5:\nB ←B ∪Vst\n6: end for\n7: // Language TD learning\n8: for iteration t = 1 to T do\n9:\n// Language TD estimate ②, ③\n10:\nDV ←{} {Value training data}\n11:\nfor each look-ahead data (st, Vst) in B do\n12:\nFor each variation n, generate the final state evaluation V L\nπ (sn\nt+l) with V L\nπ , and utilize d to\ndescribe intermediate transitions dn = d(st, an\nt , rn\nt , sn\nt+1, ..., sn\nt+l−1, an\nt+l−1, rn\nt+l−1)\n13:\nV L\nπ (st)new ←G1\n\u0010\b\nG2\n\u0000dn, V L\nπ (sn\nt+1)\n\u0001\tK\nn=1\n\u0011\n{l-step, k-variation Language TD}\n14:\nDV ←DV ∪V L\nπ (st)new\n15:\nend for\n16:\n// Update value function ④\n17:\nTrain V L\nπ using language TD estimate dataset DV with supervised loss\n18: end for\n18\nPreprint. Work in progress.\nA.3\nNATURAL LANGUAGE ACTOR CRITIC\nAlgorithm 3 Natural Language Actor-Critic Learning\nRequire: Initial language policy model πL, language value model V L\nπ , large language model for\nevaluation G1/G1,G2, improvement operator I\nRequire: Number of iterations T, number of trajectories N, number of MC trajectories K\n1: Initialize replay buffer history B ←{}\n2: for iteration t = 1 to T do\n3:\n// Collect trajectories using language policy ①\n4:\nτ ←{} {Initialize trajectory buffer}\n5:\nfor i = 1 to N do\n6:\nRun policy πL to collect trajectory: τi ←{(sj, aj, rj, sj+1)Hi\nj=0}\n7:\nτ ←τ ∪{τi}\n8:\nend for\n9:\n// Language value estimation ②, ③\n10:\nDV ←{} {Value training data}\n11:\nfor each state-action pair (s, a) in τ do\n12:\nif using MC estimate then\n13:\nSample KMC trajectories starting from (s, a): {τk}KMC\nk=1\n14:\nv ←G1({τk}KMC\nk=1 ) {Language MC estimate}\n15:\nelse if using TD estimate then\n16:\nSample KT D transitions starting from (s, a): {(sk, ak, rk, s′\nk)}KT D\nk=1\n17:\nv ←G1({G2(d(sk, ak, rk, s′\nk), V L\nπ (s′\nk))}KT D\nk=1 ) {Language TD estimate}\n18:\nend if\n19:\nDV ←DV ∪{(s, a, v)}\n20:\nend for\n21:\n// Update value function ④\n22:\nB ←B ∪{DV } {Add to buffer history}\n23:\nTrain V L\nπ on merged data from last Kbuffer buffers in B with supervised loss\n24:\n// Language policy improvement ⑤\n25:\nDπ ←{} {Policy training data}\n26:\nfor each state s in τ do\n27:\nSample actions {ai}Nsample\ni=1\nfrom πL(s)\n28:\nSelect top-m actions Ak based on sampling frequency\n29:\nQuery value estimates: QL ←{V L\nπ (s, a)|a ∈Ak}\n30:\nπnew, c ←I(QL, TL) {Language improvement operator}\n31:\nDπ ←Dπ ∪{(s, πnew, c)}\n32:\nend for\n33:\n// Update policy ⑥\n34:\nTrain πL on Dπ with supervised loss\n35: end for\n19\nPreprint. Work in progress.\nB\nEXPERIMENTAL DETAILS\nB.1\nMAZE EXPERIMENT\nB.1.1\nEXPERIMENT DETAILS\nOur Maze experiment is conducted using LMRL Gym (Abdulhai et al., 2023). We use the original\ncodebase for the environment. We consider the fully-observable setting, where the agent’s observa-\ntion (described by text) includes the agent’s current position in the maze, the agent’s action history,\nthe walls’ position around the agent (if any), and the goal position. The action space is discrete, in-\ncluding moving up / down / right / left. We evaluate the performance on 30 different initial positions,\neach with 3 random seeds.\nWe prompt gpt-4o-mini-2024-07-18 (temperature = 0.7) as the language aggregator G1, language\nstate-action value aggregator G2, and language state value function V L\nπ respectively.\nB.1.2\nLANGUAGE GPI VISUALIZATION\nWe present the language GPI’s prompt and response visualization for language state value function\nV L\nπ and language state-action value aggregator G2 in the LMRL Maze experiment.\nMaze prompts\nRole\nContent\nSYSTEM\nYou are an expert maze solver.\nYou only respond in\njson.\nUSER (V L\nπ )\nYou are playing a simple maze game.\nAn agent is moving\nin the maze and the objective is to reach the goal in\nas few steps as possible.\nThe possible actions are \"move up\", \"move down\", \"move\nleft\", \"move right\".\nYou are a highly skilled evaluator in this game and is\nexpected to function similar to state value function in\nreinforcement learning.\nAt each step I will provide\nyou with the move history of the agent (from old\nto new), including where the goal is, your current\nposition, the walls that surround the agent.\nYour\noutput evaluation should be a json array that includes\nthe following concepts:\n- \"thoughts\":\nLet’s think step by step.\nGenerate your\ndetailed thought process and evaluation of the agent’s\nposition and the distance towards the goal.\n- \"final evaluation\":\nConcisely judge how good the\nagent’s current position is compared to all the\npossible positions in the maze, in terms of reaching\nthe goal.\nLet’s start a new game.\nNow, please give your\nevaluation of the current state given the move history\nof the agent:\nThe goal is at position 8, 6.\nYour current position\nis at position 5, 7.\nThere are walls above you, below\nyou.\nmove up\nThe goal is at position 8, 6.\nYour current position\nis at position 5, 7.\nThere are walls above you, below\nyou.\nmove down\nThe goal is at position 8, 6.\nYour current position\nis at position 5, 7.\nThere are walls above you, below\nyou.\nmove right\n20\nPreprint. Work in progress.\nThe goal is at position 8, 6.\nYour current position\nis at position 5, 8.\nThere are walls above you, below\nyou.\nmove right\nThe goal is at position 8, 6.\nYour current position is\nat position 5, 9.\nThere are walls to your right, below\nyou.\nASSISTANT\n’thoughts’:\n[\"The goal is located at (8, 6) and the\nagent’s current position is (5, 9).\", ’To evaluate the\ndistance to the goal, I will calculate the Manhattan\ndistance, which is the sum of the absolute differences\nof the coordinates:\n|8 - 5| + |6 - 9| = 3 + 3 =\n6.’, \"The agent’s current position (5, 9) has walls\ndirectly below (6, 9) and to the right (5, 10).\nThis\nlimits movement options.\", ’The possible actions for\nthe agent are to move left to (5, 8) or move up to\n(4, 9).\nMoving down or right is not possible due to\nwalls.’, ’Moving left to (5, 8) would bring the agent\nback to a previous position, while moving up to (4,\n9) opens potential new pathways.’, ’Overall, the agent\nis relatively far from the goal, with limited movement\noptions due to surrounding walls.’],\n’final evaluation’:\n\"The agent’s current position (5,\n9) is not ideal as it is 6 steps away from the goal and\nhas limited movement options.\nMoving up to (4, 9) is\nthe best immediate option to explore new paths.\"\n21\nPreprint. Work in progress.\nRole\nContent\nSYSTEM\nYou are an expert maze solver.\nYou only respond in\njson.\nUSER (G2)\nYou are playing a simple maze game.\nAn agent is moving\nin the maze and the objective is to reach the goal in\nas few steps as possible.\nThe possible actions are \"move up\", \"move down\", \"move\nleft\", \"move right\".\nYou are a highly skilled evaluator in this game,\nparticularly adept at making accurate assessments\nthrough look-ahead of the current maze position after\ntaking the given action.\nAt each step I will provide\nyou with the move history of the agent (from old\nto new), including where the goal is, your current\nposition, the walls that surround the agent, the\naction that the agent is going to take, *along with\nseveral key variations of trajectory pieces after\ntaking this action (and the corresponding natural\nlanguage evaluations of the trajectory pieces)*.\nYour\ntask is to understand these look-ahead information\nand summarize, derive non-trivial analysis and\nunderstanding the *the agent’s position after taking\nthe action*.\nYour output evaluation should be a json\narray with the following *two* concepts:\n\"thoughts\":\nLet’s think step by step.\nSummarize the\nlook-ahead information of the variations after taking\naction move down.\n\"final evaluation\":\nNow Concisely judge how good the\nchosen action is, in terms of reaching the goal.\nNow, please give your evaluation given action move\ndown, the *current environment state*:\nThe goal is at position 8, 6.\nYour current position is\nat position 5, 7.\nThere are walls above you, below you\nand the look-ahead information of different variations\nafter taking action move down:\nAGGREGATION OF VALUE ESTIMATES FOR ALL VARIATIONS\nASSISTANT\n’thoughts’:\n[\"The goal is located at (8, 6), which\nis three rows down and one column to the left of the\ncurrent position (5, 7).\", \"The agent attempts to\nmove up from (5, 7) but cannot because there is a wall\nabove.\", \"Each variation shows the agent getting stuck\nin a loop of ineffective moves after trying to move up,\nconsistently returning to (5, 7) or moving to (5, 9),\nwhich is still not closer to the goal.\", \"The walls\nsurrounding the agent restrict movement, making it\nimpossible to progress toward the goal efficiently.\",\n\"In all variations, the agent finds itself either\nreturning to previous positions or moving to areas\nblocked by walls.\", \"The agent´s progression towards\nthe goal becomes increasingly limited as it cannot move\nup, down, or directly toward the goal from its current\nposition.\"],\n’final evaluation’:\n\"{ \"current position\":\n\"(5, 7)\",\n\"goal position\":\n\"(8, 6)\", \"manhattan distance\":\n6,\n\"possible moves\":\n[\"move down\", \"move left\"], \"blocked\nmoves\":\n[\"move up\", \"move right\"], \"evaluation\":\n\"The\nchosen action ´move up´ıs ineffective as it does not\nchange the agent´s position, and the surrounding walls\ncreate a dead end, making it impossible to reach the\ngoal efficiently.\"}\"\n22\nPreprint. Work in progress.\nB.1.3\nTHE EFFECT OF LANGUAGE TD ON THE VALUE FUNCTION\nBelow we show a comparison of the Language state-action values with/without Language TD. Com-\nparing the two Language State-action Value, we can see that by analyzing various potential out-\ncomes, Language TD identifies both immediate and future benefits or drawbacks of actions, which\nis reflected in its more detailed thoughts and final evaluation. The inclusion of varied trajectory\ninformation allows Language TD to provide a more balanced and informed assessment, mitigating\nover-reliance on immediate next-step evaluations.\nLanguage State-action Value (Language TD)\nFor action \"move down\", {\"thoughts\": [\"The agent’s current position\nis (5, 7) with the goal at (8, 6).\", \"After attempting to move\ndown, the agent remains at (5, 7) due to walls above and below.\",\n\"In Variation 1, the agent eventually moves to (6, 6), which is\none step away from the goal at (8, 6).\", \"Variation 2 shows that\nmoving down leads to blocked paths, necessitating lateral movement,\nbut ultimately keeps the agent away from the goal.\", \"Variation 3\nallows the agent to successfully reach (7, 6), positioning it just\none step away from the goal.\", \"Variation 4 also confirms that moving\ndown to (6, 6) is effective, placing the agent two steps away from\nthe goal.\", \"Overall, the evaluations indicate that while the agent\nfaces wall constraints, moving down leads to favorable outcomes that\nprogress toward the goal.\"], \"final_evaluation\": {\"current_position\":\n\"(5, 7)\", \"distance_to_goal\": 3, \"proposed_action\": \"move down\",\n\"evaluation\": \"Although the move down is initially blocked,\nsubsequent evaluations indicate that this action can lead to optimal\npositions in the following moves. The agent can reach the goal within\ntwo more steps if it navigates correctly.\"}}\nLanguage State-action Value (without Language TD)\nFor action \"move down\", {\n\"thoughts\": [\n\"The agent’s current position is (5, 7) and the goal is at (8,\n6).\",\n\"The possible action is to move down to (6, 7).\",\n\"There are walls above (4, 7) and below (6, 7), which means the\nagent can only move down if it is not blocked.\",\n\"Moving down to (6, 7) is valid since there is no wall at that\nposition.\",\n\"After moving down, the new position will be (6, 7).\",\n\"The distance to the goal can be calculated as follows: the\nhorizontal distance is |8 - 6| = 2 and the vertical distance is |6\n- 7| = 1.\",\n\"The total distance to the goal after moving down would be 2 + 1\n= 3.\"\n], \"final_evaluation\": {\n\"current_position\": \"(6, 7)\",\n\"distance_to_goal\": 3,\n\"evaluation\": \"The agent’s position after moving down is better\nthan the previous one, as it moves closer to the goal. There are no\nwalls blocking further movement towards the goal from this position.\"\n}\n}\n23\nPreprint. Work in progress.\nB.2\nBREAKTHROUGH EXPERIMENT\nB.2.1\nEXPERIMENT DETAILS\nOur breakthrough experiment is conducted with Openspiel (Lanctot et al., 2019) 5x5 breakthrough\ngame and MCTS policy. We slightly modified Openspiel’s codebase so the 5x5 breakthrough game\nsupports two lines of pieces for each side.\nFor the policy π, as mentioned in the paper, we choose an MCTS policy with uctc = 1, 1000\nsimulations for search, and 100 rollouts for the random rollout evaluator. For the state dataset, we\ndefine the MCTS policies using a grid of configurations based on the number of simulations and\nrollouts. Specifically, we use the following values for these parameters:\n• Simulation numbers: 2, 10, 100, 1000\n• Rollout numbers: 1, 10, 100, 1000\nThis results in 4 × 4 = 16 unique MCTS policies, each characterized by a specific combination\nof simulation and rollout numbers. To generate the mixed state dataset, we pair every possible\ncombination of these 16 policies (including self-pairing), leading to 16 × 16 = 256 policy pairs.\nFor each pair of policies, we perform the same number of rollouts where both policies interact within\nthe environment. The states encountered during these rollouts are recorded, ensuring that the dataset\ncaptures a diverse distribution of positions. By merging the data from all policy pairs, we create\nthe final mixed state dataset Ds, which is designed to represent states arising from a wide range of\ngameplay strategies and skill levels.\nWe use vLLM (Kwon et al., 2023b) for any LLM inference used in the experiment, including lan-\nguage value function inference and aggregation inference.\nWe sample 3000 states from the state dataset for evaluation to serve as the Ds\ntest. When computing\nprediction accuracy, we only count state evaluation which can pass our rule-based parser to extract\nthe advantageous side judgment.\nHere we present the hyperparameters used in our experiment:\nRollout\nParameter\nValue\nParallel Environments\n192\nLookahead Step\n4\nLookahead Rollout Number\n4\nDeduplicate State\nTrue\nTable 4: Rollout Parameters\nPrompting\nLLM Sampling Parameter\nValue\nTemperature\n1.0\nTop K\n50\nTop P\n0.95\nMax Tokens\n512\nTable 5: LLM sampling parameters for prompting.\nTraining\n24\nPreprint. Work in progress.\nParameter\nValue\nMax Sequence Length\n1024\nWarmup Ratio\n0.03\nLearning Rate\n2e-5\nLearning Rate Scheduler\nConstant\nDtype\nbfloat16\nPer Device Train Batch Size\n4\nGradient Accumulation Step\n8\nTraining Epoch\n2\nNumber of GPUs\n4\nDistributed Framework\nFSDP\nTable 6: Data Collection Parameters\nEvaluation. We use the same LLM sampling parameters as the prompting process.\nB.2.2\nLANGUAGE TD VISUALIZATION\nHere we present an example of how language TD works for query and response.\nLanguage State-action Value (Before)\nRole\nContent\nSYSTEM\nHere is the rule for the Breakthrough board game:\nThe game is played on an 5x5 board for 2 players (white\nand black), with each player starting with 10 pawns.\nwhite pawns are on the first two rows and black pawns\nare on the last two rows.\nBlack moves first.\nIn each turn, players can move one\nof their pieces one square forward, diagonally forward\nif the target square is empty.\nOr it can capture an\nopponent’s piece if that square is one step diagonally\nforward.\nThe game ends when one player successfully break\nthrough the opponent lines -- either move a piece to\nthe opposite last row of the board or captures all of\nthe opponent’s pieces.\nFor board representation, we use b for black pieces,\nw for white pieces, and .\nfor empty squares.\n(1-5)\nand (a-e) are used to show the rows and columns index\nrespectively.\nYou are a highly skilled evaluator in this game,\nparticularly adept at making accurate assessments\nthrough look-ahead analysis of the current board\nposition.\nI will provide you with current board representation,\n*along with several key variations starting from this\nposition (and their corresponding natural language\nevaluations of the subsequent positions)*.\nYour task is to aggregate and compare these look-ahead\ninformation, to summarize, derive non-trivial analysis\nabout the *current board*.\nYour output should include\nthe following concepts:\n1.\n*Tactical Considerations*:\nAny immediate threats,\npotential tactics, or vulnerabilities in the position.\n25\nPreprint. Work in progress.\n2.\n*Positional Evaluation*:\nConsideration of pawn\nstructure, piece activity, control of key squares, and\ngame safety.\n3.\n*Suggested Moves*:\nOne or two strong candidate\nmoves for the side to move, along with a brief\nrationale for comparing different moves.\n4.\n*Advantage*:\nBased on all previous rationale,\ndetermine if white or black takes advantage.\nUse\n<white> or <black> to represent the winning side.\nYour response should be informative and concise.\nUSER\n\"*The board you need to evaluate:*\n5bb.b.\n4b..b.\n3..bw.\n2w.w..\n1wwwww\nabcde\nIt is White’s turn.\nWhite pieces are at:\nd3, a2, c2, a1, b1, c1, d1, e1.\nBlack pieces are at:\na5, b5, d5, a4, d4, c3.\nHere are the look-ahead variations from the current\nboard position:\n*Key Variations and Subsequent Evaluation:*:\n*Variation 1:*\nDescription of variation’s move sequence:\nThe action sequence is:\nd3e4,d5e4*,a2b3,a4b3*.\nMove 1:White moves piece from d3 (Column d, Row 3) to\ne4 (Column e, Row 4).\nMove 2:Black moves piece from d5 (Column d, Row 5) to\ne4 (Column e, Row 4), capturing White piece.\nMove 3:White moves piece from a2 (Column a, Row 2) to\nb3 (Column b, Row 3).\nMove 4:Black moves piece from a4 (Column a, Row 4) to\nb3 (Column b, Row 3), capturing White piece.\nSubsequent position evaluation:\nThe subsequent board is:\n5bb...\n4...bb\n3.bb..\n2..w..\n1wwwww\nabcde\nIt is White’s turn.\nWhite pieces are at:\nc2, a1, b1, c1, d1, e1.\nBlack pieces are at:\na5, b5, d4, e4, b3, c3.\nThe evaluation of this subsequent board is:\n**Current Board Analysis:**\n**Tactical Considerations:**\n26\nPreprint. Work in progress.\nWhite’s most significant tactical consideration is\nthe potential to capture Black’s pieces on d4 and e4,\nwhich would gain a significant advantage.\nWhite’s\npieces are well-positioned to launch a decisive attack.\nBlack’s pieces on a5 and b5 are somewhat isolated and\nvulnerable to potential attacks.\n**Positional Evaluation:**\nThe current position slightly favors White due to their\npiece activity, control of key squares, and potential\nto create a strong pawn center.\nWhite’s pawns on a1\nand d1 provide potential support for the central pawns.\nBlack’s pawns are somewhat isolated, but they still\nhave a chance to reorganize.\n**Suggested Moves:**\nBased on the look-ahead analysis, two strong candidate\nmoves for White are:\n1.\n**e1-d2**:\nThis move captures Black’s potential\npiece on d2 (in case Black plays d4-d2) and creates a\nstrong initiative.\n2.\n**c2-b3**:\nThis move attacks Black’s position and\nprepares to break through to the opposite side, gaining\na strong initiative.\n**Advantage:**\n<white>\nOverall, White has a slight advantage in this position,\nwith multiple ways to break through Black’s lines\nand gain a significant advantage.\nHowever, Black’s\nposition is not yet critical, and they can still\nreorganize their pieces to defend.\nWhite’s advantage\nis mainly due to their piece activity, control of key\nsquares, and potential to create a strong pawn center.\n*Variation 2:*\nDescription of variation’s move sequence:\nThe action sequence is:\nc1b2,c3d2,e1d2*,d4e3.\nMove 1:White moves piece from c1 (Column c, Row 1) to\nb2 (Column b, Row 2).\nMove 2:Black moves piece from c3 (Column c, Row 3) to\nd2 (Column d, Row 2).\nMove 3:White moves piece from e1 (Column e, Row 1) to\nd2 (Column d, Row 2), capturing Black piece.\nMove 4:Black moves piece from d4 (Column d, Row 4) to\ne3 (Column e, Row 3).\nSubsequent position evaluation:\nThe subsequent board is:\n5bb.b.\n4b....\n3...wb\n2wwww.\n1ww.w.\nabcde\nIt is White’s turn.\nWhite pieces are at:\nd3, a2, b2, c2, d2, a1, b1, d1.\nBlack pieces are at:\na5, b5, d5, a4, e3.\n27\nPreprint. Work in progress.\nThe evaluation of this subsequent board is:\n**Tactical Considerations:**\nThe current position presents a strong opportunity\nfor White to create a breakthrough on the ’d’ file,\nas demonstrated by the two variations.\nBlack’s pawn\nstructure is somewhat disjointed, making it difficult\nto respond to White’s advances on the ’d’ and ’e’\nfiles.\n**Positional Evaluation:**\nWhite’s pawn structure is more compact and cohesive,\ncontrolling key central squares.\nThe pawns on ’a’ and\n’b’ files provide additional support for the potential\nbreakthrough on ’d’ and ’e’.\nBlack’s pieces on ’a5’\nand ’d5’ are somewhat isolated, making it difficult to\ncoordinate a defense.\n**Suggested Moves:**\nConsidering the look-ahead variations, the strongest\ncandidate move for White is **d3d4**.\nThis move puts\npressure on Black’s position and sets up the potential\nbreakthrough on the ’d’ and ’e’ files.\nThe rationale\nfor this move is that it creates an immediate threat\nand forces Black to respond, potentially weakening\ntheir position.\nAnother possible move could be **d2d3**, but it is less\naggressive and does not put as much pressure on Black.\n**Advantage:**\nBased on the analysis, the advantage is **<white>**.\nThe two variations demonstrate a clear path to victory\nfor White, and Black’s position appears to be under\nsignificant pressure.\nPlease provide your analysis and understanding of the\ncurrent board position based on the provided look-ahead\ninformation.\nYour response should be informative and concise.\nASSISTANT\n\"**Current Board Analysis:**\n**Tactical Considerations:**\nThe primary tactical consideration for White is\nto utilize the pawns on the ’d’ file to create a\nbreakthrough, potentially capturing Black’s pieces\non d4 and e4.\nAdditionally, White should focus on\ncontrolling the key squares and putting pressure on\nBlack’s somewhat isolated pieces on a5 and b5.\nBlack\nneeds to reorganize their pawns and pieces to defend\nagainst White’s central pawn advance.\n**Positional Evaluation:**\nThe position slightly favors White due to their more\ncompact and cohesive pawn structure, piece activity,\nand control of key central squares.\nWhite’s pawns\non the ’a’ and ’b’ files provide additional support\nfor the potential breakthrough on the ’d’ and ’e’\nfiles.\nBlack’s pawns are somewhat isolated, making\nit challenging to coordinate a defense.\n**Suggested Moves:**\n28\nPreprint. Work in progress.\nBased on the look-ahead analysis, two strong candidate\nmoves for White are:\n1.\n**d3d4**:\nThis move puts pressure on Black’s\nposition, sets up a potential breakthrough on the ’d’\nand ’e’ files, and forces Black to respond.\n2.\n**e1-d2**:\nThis move captures Black’s potential\npiece on d2 and creates a strong initiative.\n**Advantage:**\n<white>\nOverall, White has a slight advantage in this position,\nwith multiple ways to break through Black’s lines and\ngain a significant advantage.\nThe two variations\ndemonstrate a clear path to victory for White, and\nBlack’s position appears to be under significant\npressure.\nWhite’s advantage is mainly due to their\npiece activity, control of key squares, and potential\nto create a strong pawn center.\nThe provided look-ahead information suggests that White\nshould prioritize controlling the central squares,\nputting pressure on Black’s position, and utilizing\nthe pawns on the ’d’ file to create a breakthrough.\"\nB.2.3\nCOMPARISON OF LANGUAGE VALUE FUNCTION\nHere we present another visualization comparison between the initial LLaMA-3.1-8B-Instruct\nModel and our trained model. The evaluation is conducted on the board 5..b.b 4.bb.. 3.w... 2.w...\n1ww..w abcde, where white takes large advantage in this place. By our language TD training,\nthe language value function can accurately evaluate this board and generate insightful annotation,\nlargely decreasing the hallucination.\nLanguage State Value (Before)\nBoard: 5..b.b 4.bb.. 3.w... 2.w... 1ww..w abcde, white turn\nBased on the current board position, here’s the evaluation for\nWhite’s turn:\n*Tactical Considerations:*\n- Black has an opportunity to attack White’s pawn at e1 with a pawn\nfrom e5 or c5, which might put White in a difficult position.\n- White’s pawn at b3 and b2 is a target for Black’s pawns at b4 and\nc4.\n- White needs to focus on safely advancing their pawns and creating a\nbarrier for Black’s pieces.\n*Positional Evaluation:*\n- White has a relatively weak pawn structure, with some pawns on the\nfirst row.\n- The center is somewhat controlled by White, but Black has a solid\npresence on the fifth row.\n- Black has a strategic advantage in pawn structure, but White has\nthe opportunity to influence the game with proper planning.\n*Suggested Moves:*\n1. Move the pawn from b2 to b3. This move has the goal of preparing\nto defend and possibly create a barrier against Black’s pieces.\n29\nPreprint. Work in progress.\n2. Move the pawn from a1 to a2. This move aims to create more space\nand put pressure on Black’s pieces, which will make it harder for\nthem to maneuver.\n*Advantage:*\nBased on the current board position, <black> is favored.\nLanguage State Value (After training)\nBoard: 5..b.b 4.bb.. 3.w... 2.w... 1ww..w abcde, white turn\n**Analysis and Evaluation of the Current Board Position**\n**Tactical Considerations:**\nThe key variation shows an immediate threat of White breaking through\nBlack’s lines. White’s piece on b3 is strategically positioned to\ncapture Black’s piece on c4, which creates an opportunity for a\ndecisive push.\n**Positional Evaluation:**\nWhite has a significant advantage in terms of piece activity and\ncontrol of key squares. With pieces on the third and second rows,\nWhite is poised to push forward and break through Black’s lines.\nBlack’s pieces are concentrated on the fifth and fourth rows, which\nlimits their mobility and ability to respond to White’s attack.\n**Suggested Moves:**\nTwo strong candidate moves for White are:\n1. b3-c4*: This move captures Black’s piece and creates a weakness in\nBlack’s position, allowing White to potentially break through.\n2. b2-c3: This move develops another White piece and prepares to\nsupport the potential breakthrough on the c-file.\nHowever, given the look-ahead variations, the most promising move is\nb3-c4*, as it leads to a terminal state where White wins.\n**Advantage:**\nBased on the analysis, White has a decisive advantage. The ability\nto capture Black’s piece on c4 and break through Black’s lines gives\nWhite a strong initiative and a clear path to victory. Therefore,\n<white> is the winning side.\nB.3\nTICTACTOE EXPERIMENT\nOur experiments are conducted on the Tic-tac-toe environment (Ju, 2024), a standard two-\nplayer game environment that provides a clear testbed for evaluating sequential decision-\nmaking. The implementation uses several key components: https://github.com/haje01/\ngym-tictactoe for the Tic-tac-toe environment, FSDP (Fully Sharded Data Parallel) for dis-\ntributed model training, vLLM (Kwon et al., 2023b) for efficient language model inference, and a\ncustom parallel framework for trajectory collection. The environment is wrapped with a parallel\nexecution layer to enable efficient batch processing of multiple games simultaneously.\nFor policy evaluation, we employ Monte Carlo sampling to estimate state-action values, requiring\ncomplete game trajectories until terminal states. Policy improvement utilizes a structured sampling\napproach with temperature-controlled exploration and frequency-based action selection.\nTo manage computational resources efficiently, we employ gradient checkpointing and use the FSDP\nstrategy across our GPU cluster. This configuration allows us to train larger language models while\nmaintaining reasonable memory requirements and training speed.\n30\nPreprint. Work in progress.\nThe parallel data collection framework is designed to maximize GPU utilization during training. It\nmaintains a queue of active environments and processes state-action transitions in batches, signifi-\ncantly reducing the overall training time compared to sequential execution.\nB.3.1\nTRAINING INFRASTRUCTURE\nData Collection Pipeline\nFor each training iteration, we collect data using the following configuration:\nParameter\nValue\nParallel Environments\n64\nTrajectories per Iteration\n512\nMonte Carlo Samples (KMC)\n5\nPolicy Samples per State (Nsample)\n10\nTop-k Actions\n10\nTable 8: Data Collection Parameters\nModel Architecture and Training\nBoth policy and value networks are trained from LLaMA-3.1-8B-Instruct using the following pa-\nrameters:\nHyperparameter\nValue\nLearning Rate\n1e-5\nLearning Rate Schedule\nConstant\nTraining Epochs per Iteration\n2\nFSDP Configuration\nFull Sharding\nGradient Checkpointing\nEnabled\nBatch Size\n8\nMax Sequence Length\n1024\nTraining Hardware\n4 × H100 GPUs\nTable 9: Model Training Hyperparameters\nValue Function Buffer Management\nTo prevent catastrophic forgetting, we maintain experience buffers with the following configuration:\nParameter\nValue\nBuffer History Length (Kbuffer)\n3 iterations\nMerging Strategy\nEqual sampling\nBuffer Content\nState-action pairs with MC estimates\nTable 10: Buffer Management Configuration\nB.3.2\nALGORITHM IMPLEMENTATION DETAILS\nPolicy Evaluation Details\nFor Monte Carlo evaluation, we use the following configuration:\n31\nPreprint. Work in progress.\nParameter\nValue\nMC Trajectories per State-Action\n5\nTrajectory Completion\nFull game\nValue Aggregation\nAverage over returns\nSampling Temperature\n0.7\nAction Space Size\n9 positions (0-8)\nTable 11: Policy Evaluation Configuration\nB.3.3\nVISUALIZATION\nBelow we show a comparison of the Language state-action values before and after Language MC\nestimation. The evaluation is conducted on the following board:\nO\nX\n3\n4\nO\nO\n7\nX\n9\nThrough analyzing multiple complete game trajectories, Language MC helps the value function\ndevelop a comprehensive understanding of long-term consequences of actions, rather than just im-\nmediate rewards. This is particularly evident in the enhanced evaluation quality after MC estimation,\nwhere the model provides more strategic insights and nuanced assessment of game positions.\nLanguage State-action Value (Before)\nRole\nContent\nSYSTEM\nYou are a player of the game of Tic Tac Toe.\nThe game\ngoal is to get multiple of one’s own symbols in a row,\neither horizontally, vertically, or diagonally, before\nthe opponent does.\nIf all nine squares are filled and\nno player has three in a row, the game is a draw.\nThe board consists of \"O\", \"X\" and grid numbers.\nThe\ngrid number indicates an empty grid.\nYou are learning\nhow to evaluate a (board, action) pair in the Tic Tac\nToe game by playing the game given the (board, action)\npair and reflecting on the playing history.\nThe playing history depicts a game in progress with a\nfinal result.\nYour answer consists of two elements:\n- \"thought\":\nLet’s think step by step.\nGenerate your\ndetailed evaluation over the (board, action) pair\nby merely reflecting the playing history after this\npair from different perspectives.\nYou should only\nrely on the playing history as context and shouldn’t\nevaluate the game with your own judgment.\nYour\nevaluation should contain the following elements:\nWin\nprobability, Threat, and Potential strategies.\n- \"final evaluation\":\nAfter all of your thoughts,\njudge which side takes advantage.\n1 means you think\n’O’ takes advantage, -1 means you think ’X’ takes\nadvantage, and 0 means you think the game will be a\ndraw.\nYou should output your answer in the JSON format.\nEXAMPLE:\nuser:\nThe (board, action) to evaluate is O’s turn:\nBoard:\nO | O | X\n---------\n4 | X | 6\n---------\n32\nPreprint. Work in progress.\n7 | 8 | 9\nAction:\nThe O’s move is 4.\nBelow is the rollout sequence after this (board,\naction):\nAfter O’s move 4, the board position is:\nO | O | X\n---------\nO | X | 6\n---------\n7 | 8 | 9\nAfter X’s move 7, the board position is:\nO | O | X\n---------\nO | X | 6\n---------\nX | 8 | 9\nThe game is over.\nX wins.\nX wins by occupying the\npositions 3, 5, 7.\nassistant:\n{\n\"thought\":\n{\n\"Reflection\":\n\"It appears that the initial board\nposition\nO | O | X\n---------\n4 | X | 6\n---------\n7 | 8 | 9\nand action 4 were not favorable for O, as X was able to\nblock on O’s moves and ultimately win the game.\",\n\"Win probability\":\n\"The win probability for X is\nlarge, while the win probability for O is low.\",\n\"Threat\":\n\"X has played center 5 and corner 3.\nX can\nwin by playing corner 7.\nO was able to occupy 1, 4, 7\nand create a potential winning opportunity.\",\n\"Potential strategies\":\n\"Potential strategies for O\ninclude playing the corner 7 to block X as opposite\ncorner and win by occupying 1, 4, 7.\nX could have\noccupied 3, 5, 7 to win the game.\nX has already\noccupied 3, 5, and there is 1 step to complete the\nplan.\"\n},\n\"final evaluation\":\n-0.9\n}\nUSER\nThe board to evaluate is O’s turn:\nBoard:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nAction:\nThe O’s move is 3.\nBelow is the rollout sequence 1 after this (board,\naction):\nAfter O taking action 3, the board position is:\nO | X | O\n---------\n4 | O | 6\n---------\n7 | X | 9.\nAfter X taking action 4, the board position is:\nO | X | O\n---------\n33\nPreprint. Work in progress.\nX | O | 6\n---------\n7 | X | 9.\nAfter O taking action 7, the board position is:\nO | X | O\n---------\nX | O | 6\n---------\nO | X | 9.\nThe game is over.\nO wins.\nO wins by occupying the\npositions [3, 5, 7].\nNow generate your evaluation for the (board, action)\npair.\nASSISTANT\n{\n\"thought\":\n{\n\"Reflection\":\n\"The initial board position\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nand action 3 were favorable for O, as O was able to\noccupy a strategic position and ultimately win the\ngame.\",\n\"Win probability\":\n\"The win probability for O is high,\nwhile the win probability for X is low.\",\n\"Threat\":\n\"O has played center 5 and corner 3.\nO\ncan win by playing corner 7.\nX has played center\n5’s opposite corner 9 but failed to block O’s winning\nline.\",\n\"Potential strategies\":\n\"Potential strategies for O\ninclude playing the corner 7 to win by occupying 3, 5,\n7.\nX could have occupied 4, 6, 9 to block O’s winning\nline but failed to do so.\"\n},\n\"final evaluation\":\n0.9\n}\nLanguage State-action Value (After)\nRole\nContent\nSYSTEM\nYou are a player of the game of Tic Tac Toe.\nThe game\ngoal is to get multiple of one’s own symbols in a row,\neither horizontally, vertically, or diagonally, before\nthe opponent does.\nIf all nine squares are filled and\nno player has three in a row, the game is a draw.\nThe board consists of \"O\", \"X\" and grid numbers.\nThe\ngrid number indicates an empty grid.\nYou are learning\nhow to evaluate a (board, action) pair in the Tic Tac\nToe game by playing the game given the (board, action)\npair and reflecting on the playing history.\nThe playing history depicts a game in progress with a\nfinal result.\nYour answer consists of two elements:\n- \"thought\":\nlet’s think step by step.\nGenerate your\ndetailed evaluation over the (board, action) pair by\nmerely reflecting the playing history after this pair\nfrom different perspectives.\nYou should only rely on\nthe playing history as context and don’t evaluate the\ngame with your own judgment.\nYour evaluation should\ncontain the following elements:\nWin probability,\nThreat, and Potential strategies.\n34\nPreprint. Work in progress.\n- \"final evaluation\":\nAfter all of your thoughts,\njudge which side takes advantage.\n1 means you think\n’O’ takes advantage, -1 means you think ’X’ takes\nadvantage, and 0 means you think the game will be a\ndraw.\nYou should output your answer in the JSON format.\nEXAMPLE:\nuser:\nThe (board, action) to evaluate is O’s turn:\nBoard:\nO | O | X\n---------\n4 | X | 6\n---------\n7 | 8 | 9\nAction:\nThe O’s move is 4.\nBelow is the rollout sequence after this (board,\naction):\nAfter O’s move 4, the board position is:\nO | O | X\n---------\nO | X | 6\n---------\n7 | 8 | 9\nAfter X’s move 7, the board position is:\nO | O | X\n---------\nO | X | 6\n---------\nX | 8 | 9\nThe game is over.\nX wins.\nX wins by occupying the\npositions 3, 5, 7.\nassistant:\n{\n\"thought\":\n{\n\"Reflection\":\n\"It appears that the initial board\nposition\nO | O | X\n---------\n4 | X | 6\n---------\n7 | 8 | 9\nand action 4 were not favorable for O, as X was able to\nblock on O’s moves and ultimately win the game.\",\n\"Win probability\":\n\"The win probability for X is\nlarge, while the win probability for O is low.\",\n\"Threat\":\n\"X has played center 5 and corner 3.\nX can\nwin by playing corner 7.\nO was able to occupy 1, 4, 7\nand create a potential winning opportunity.\",\n\"Potential strategies\":\n\"Potential strategies for O\ninclude playing the corner 7 to block X as opposite\ncorner and win by occupying 1, 4, 7.\nX could have\noccupied 3, 5, 7 to win the game.\nX has already\noccupied 3, 5, and there is 1 step to complete the\nplan.\"\n},\n\"final evaluation\":\n-0.9\n}\nUSER\nThe board to evaluate is O’s turn:\nBoard:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\n35\nPreprint. Work in progress.\nAction:\nThe O’s move is 9.\nBelow is the rollout sequence 1 after this (board,\naction):\nAfter O taking action 9, the board position is:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | O.\nThe game is over.\nO wins.\nO wins by occupying the\npositions [1, 5, 9].\nBelow is the rollout sequence 2 after this (board,\naction):\nAfter O taking action 9, the board position is:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | O.\nThe game is over.\nO wins.\nO wins by occupying the\npositions [1, 5, 9].\nBelow is the rollout sequence 3 after this (board,\naction):\nAfter O taking action 9, the board position is:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | O.\nThe game is over.\nO wins.\nO wins by occupying the\npositions [1, 5, 9].\nBelow is the rollout sequence 4 after this (board,\naction):\nAfter O taking action 9, the board position is:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | O.\nThe game is over.\nO wins.\nO wins by occupying the\npositions [1, 5, 9].\nBelow is the rollout sequence 5 after this (board,\naction):\nAfter O taking action 9, the board position is:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | O.\nThe game is over.\nO wins.\nO wins by occupying the\npositions [1, 5, 9].\nNow generate your evaluation for the (board, action)\npair.\nASSISTANT\n{\n\"thought\":\n{\n\"Reflection\":\n\"It appears that the initial board\nposition\n36\nPreprint. Work in progress.\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nand action 9 were highly favorable for O, as O was able\nto win the game immediately after taking action 9 in\nall rollout sequences.\",\n\"Win probability\":\n\"The win probability for O is\nextremely high, while the win probability for X is\nextremely low.\",\n\"Threat\":\n\"There were no threats from X in any of\nthe rollout sequences, as O was able to win the game\nimmediately after taking action 9.\",\n\"Potential strategies\":\n\"The potential strategy for O\nwas to occupy the corner 9 to win the game by occupying\nthe positions [1, 5, 9].\nThis strategy was successful\nin all rollout sequences.\"\n},\n\"final evaluation\":\n1\n}\nPolicy Improvement Details\nBelow we show a comparison of the Language Policy Improvement procedure at the beginning of\ntraining and at the end. The evaluation is conducted on the following board:\nO\nX\n3\n4\nO\n6\n7\nX\n9\nThe evolution in policy quality demonstrates how our framework effectively leverages value function\nassessments to enhance decision-making. The improved policy shows more sophisticated strategic\nreasoning, considering not just immediate moves but also their implications for future game states.\nThis transformation is achieved while maintaining our restriction against using the model’s pre-\ntrained game knowledge, instead relying purely on learned evaluation capabilities.\nLanguage Policy Improvement (Beginning)\nRole\nContent\nSYSTEM\nYou are playing the game tic-tac-toe on a 3*3 board.\nTic Tac Toe is a two-player game played on a grid.\nPlayers take turns marking a space with their\nrespective symbol.\nThe goal is to get multiple of\none’s own symbols in a row, either horizontally,\nvertically, or diagonally, before the opponent does.\nIf all nine squares are filled and no player has three\nin a row, the game is a draw.\nThe board consists of \"O\", \"X\" and grid number.\nThe\ngrid number indicates an empty grid.\nYour task is to determine the best move for the next\nplayer based on the given board position and the next\nplayer.\nThe evaluations of (board, action) pairs after possible\nmoves are given.\nDO NOT judge the board based on your knowledge, only\nuse the evaluations to determine the best move.\nThe evaluation for the next board is in the format of a\nJSON, consisting of two elements:\n- \"thought\":\nEvaluation of the board and action pair.\n37\nPreprint. Work in progress.\n- \"final evaluation\":\nJudge which side takes advantage.\n1 means ’O’ takes advantage, -1 means ’X’ takes\nadvantage, and 0 means the game will be a draw.\nUSER\nHere is the board position and the next player is O:\nBoard:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9.\nThe possible moves are [3, 7, 4, 9].\nThe following are the boards after each possible move:\n### Evaluation for taking action 3:\n{\n\"thought\":\n{\n\"Reflection\":\n\"The initial board position\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nand action 3 were favorable for O, as O was able to win\nthe game in all rollout sequences.\",\n\"Win probability\":\n\"The win probability for O is high,\nwhile the win probability for X is low.\",\n\"Threat\":\n\"O has played corner 1 and center 5.\nO\ncan win by playing corner 3, 4, 7 or 9.\nX has played\ncorner 2 and 8 but failed to block O’s winning lines.\",\n\"Potential strategies\":\n\"Potential strategies for O\ninclude playing the corner 3 to create a potential\nwinning opportunity.\nO could have occupied 1, 4, 7\nor 1, 5, 9 or 3, 5, 7 to win the game.\nX could have\nblocked O’s winning lines by playing 4 or 7 but failed\nto do so.\"\n},\n\"final evaluation\":\n0.9\n}\n### Evaluation for taking action 7:\n{\n\"thought\":\n{\n\"Reflection\":\n\"The initial board position\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nand action 7 were favorable for O, as O was able to win\nthe game in all rollout sequences.\",\n\"Win probability\":\n\"The win probability for O is high,\nwhile the win probability for X is low.\",\n\"Threat\":\n\"O has played corner 1 and center 5.\nO can\nwin by playing corner 7 and then 9, or 4 and then 7.\nX has played corner 3 and 8 but failed to block O’s\nwinning lines.\",\n\"Potential strategies\":\n\"Potential strategies for O\ninclude playing the corner 7 to create a potential\nwinning opportunity and then playing 9 or 4 to win\nthe game.\nX could have occupied 3, 6, 9 to block O’s\nwinning lines but failed to do so.\"\n},\n\"final evaluation\":\n0.9\n}\n### Evaluation for taking action 4:\n38\nPreprint. Work in progress.\n{\n\"thought\":\n{\n\"Reflection\":\n\"The initial board position\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nand action 4 were favorable for O, as O was able to win\nthe game in all rollout sequences.\",\n\"Win probability\":\n\"The win probability for O is high,\nwhile the win probability for X is low.\",\n\"Threat\":\n\"O has played center 5 and corner 1.\nO can\nwin by playing corner 7 or 9.\nX has played center 6\nand corner 3 but failed to block O’s winning lines.\",\n\"Potential strategies\":\n\"Potential strategies for O\ninclude playing the corner 7 or 9 to win the game.\nX\ncould have occupied 3, 6, 9 to block O’s winning lines\nbut failed to do so.\"\n},\n\"final evaluation\":\n0.9\n}\n### Evaluation for taking action 9:\n{\n\"thought\":\n{\n\"Reflection\":\n\"It appears that the initial board\nposition\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nand action 9 were favorable for O, as O was able to win\nthe game in both rollout sequences.\",\n\"Win probability\":\n\"The win probability for O is high,\nwhile the win probability for X is low.\",\n\"Threat\":\n\"O has played center 5 and corner 1.\nO\ncan win by playing corner 9.\nX has played corner 3\nand center 6 is empty, but X was not able to block O’s\nmoves and ultimately lost the game.\",\n\"Potential strategies\":\n\"Potential strategies for O\ninclude playing the corner 9 to create a potential\nwinning opportunity and then playing 3 or 7 to win\nthe game.\nX could have occupied 3, 6, 7 to block O’s\nwinning opportunity, but X was not able to do so.\"\n},\n\"final evaluation\":\n0.9\n}\nNow, please give your evaluation and the best move for\nO based on the given board position:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9.\nYou should output your answer in the JSON format.\nYour\nanswer consists of two elements:\n- \"thought\":\nLet’s think step by step.\nGenerate\nyour detailed reflection by analyzing the next board\npositions and their evaluations.\n39\nPreprint. Work in progress.\n- \"best move\":\nThe best move for the next player.\nThe\nmove should be in the format of a number from 1 to 9,\nindicating the position on the board.\nAnd the move\nshould be in the available positions [3, 7, 4, 9].\nDon’t output extra information except for the JSON\nformat.\nASSISTANT\n{\n\"thought\":\n\"Analyzing the given board position\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9 and the evaluations of the possible moves, we\ncan see that all four moves (3, 7, 4, 9) have the same\nfinal evaluation of 0.9, indicating that O has a high\nchance of winning with any of these moves.\nHowever,\nwe should consider the potential strategies and threats\nfor each move.\nMove 3 and 7 allow O to create a potential winning\nopportunity by playing the corner, while move 4 allows\nO to play the center and create a potential winning\nopportunity by playing the corner 7 or 9.\nMove 9\nallows O to play the corner and create a potential\nwinning opportunity by playing 3 or 7.\nConsidering the potential strategies, all four moves\nseem to have similar advantages.\nHowever, move 7 seems\nto have a slight advantage as it allows O to create a\npotential winning opportunity and then play 9 to win\nthe game, while also putting pressure on X to block the\nwinning line.\nTherefore, the best move for O is 7.\",\n\"best move\":\n7\n}\nLanguage Policy Improvement (End)\nRole\nContent\nSYSTEM\nYou are playing the game tic-tac-toe on a 3*3 board.\nTic Tac Toe is a two-player game played on a grid.\nPlayers take turns marking a space with their\nrespective symbol.\nThe goal is to get multiple of\none’s own symbols in a row, either horizontally,\nvertically, or diagonally, before the opponent does.\nIf all nine squares are filled and no player has three\nin a row, the game is a draw.\nThe board consists of \"O\", \"X\" and grid numbers.\nThe\ngrid number indicates an empty grid.\nYour task is to determine the best move for the next\nplayer based on the given board position and the next\nplayer.\nThe evaluations of (board, action) pairs after possible\nmoves are given.\nDO NOT judge the board based on your knowledge, only\nuse the evaluations to determine the best move.\nThe evaluation for the next board is in the format of a\nJSON, consisting of two elements:\n- \"thought\":\nEvaluation of the board and action pair.\n- \"final evaluation\":\nJudge which side takes advantage.\n1 means ’O’ takes advantage, -1 means ’X’ takes\nadvantage, and 0 means the game will be a draw.\nUSER\nHere is the board position and the next player is O:\nBoard:\n40\nPreprint. Work in progress.\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9.\nThe possible moves are [9].\nThe following are the boards after each possible move:\n### Evaluation for taking action 9:\n{\n\"thought\":\n{\n\"Reflection\":\n\"It appears that the initial board\nposition\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9\nand action 9 were highly favorable for O, as O was able\nto win the game immediately after taking action 9 in\nall rollout sequences.\",\n\"Win probability\":\n\"The win probability for O is\nextremely high, while the win probability for X is\nextremely low.\",\n\"Threat\":\n\"There were no threats from X in any of\nthe rollout sequences, as O was able to win the game\nimmediately after taking action 9.\",\n\"Potential strategies\":\n\"The potential strategy for O\nwas to occupy the corner 9 to win the game by occupying\nthe positions [1, 5, 9].\nThis strategy was successful\nin all rollout sequences.\"\n},\n\"final evaluation\":\n1\n}\nNow, please give your evaluation and the best move for\nO based on the given board position:\nO | X | 3\n---------\n4 | O | 6\n---------\n7 | X | 9.\nYou should output your answer in the JSON format.\nYour\nanswer consists of two elements:\n- \"thought\":\nLet’s think step by step.\nGenerate\nyour detailed reflection by analyzing the next board\npositions and their evaluations.\n- \"best move\":\nThe best move for the next player.\nThe\nmove should be in the format of a number from 1 to 9,\nindicating the position on the board.\nAnd the move\nshould be in the available positions [9].\nDon’t output extra information except for the JSON\nformat.\nASSISTANT\n{\n\"thought\":\n\"Based on the evaluation of the board\nposition after taking action 9, it appears that this\nmove is highly favorable for O. The win probability\nfor O is extremely high, while the win probability\nfor X is extremely low.\nThere were no threats from\nX in any of the rollout sequences, as O was able to\nwin the game immediately after taking action 9.\nThe\npotential strategy for O was to occupy the corner 9\nto win the game by occupying the positions [1, 5, 9].\nThis strategy was successful in all rollout sequences.\nTherefore, taking action 9 is the best move for O.\",\n41\nPreprint. Work in progress.\n\"best move\":\n9\n}\nB.3.4\nBASELINE IMPLEMENTATION DETAILS\nFor our PPO baseline, we implement a custom PPO agent adapted for language models. The agent\nconsists of two key components: an actor network based on LLaMA-3.1-8B-Instruct for action\nselection, and a critic network for value estimation.\nArchitecture Details\nThe actor network uses LLaMA-3.1-8B-Instruct’s architecture with added\nspecial tokens for moves 1-9. The critic network consists of a two-layer MLP (Multi-Layer Percep-\ntron) that takes the last hidden state from the actor as input:\n• Input dimension: LLaMA-3.1-8B hidden size\n• Hidden layer: ReLU activation\n• Output layer: Single value with Tanh activation\nTraining Configuration\nThe PPO training uses the following hyperparameters:\nParameter\nValue\nLearning Rate (Actor)\n1e-5\nLearning Rate (Critic)\n1e-5\nPPO Epochs\n1\nBatch Size\n16\nGradient Accumulation Steps\n2\nValue Loss Coefficient\n0.5\nEntropy Coefficient\n0.01\nPPO Clip Range\n0.2\nDiscount Factor (γ)\n0.99\nGAE Parameter (λ)\n0.95\nTable 16: PPO Training Hyperparameters\nImplementation Details\nFor each training iteration:\n• We collect 512 complete game trajectories using 8 parallel environments\n• Actions are selected using a temperature-controlled sampling (T = 0.7) over the logits\n• The policy proposes Nsample = 1 candidate actions per state and selects top-k (k = 1)\nbased on sampling frequency\n• We use 1 A100 GPU for training\n• Gradient checkpointing is enabled to optimize memory usage\nBoth networks are trained using AdamW optimizer with weight decay of 0.01. We maintain experi-\nence buffers with a history length of 1 iterations for on-policy optimization.\nB.3.5\nADDITIONAL ABLATION STUDIES\nAction Selection Parameter Study To complement the main ablation studies presented in Sec-\ntion 5.3.3, we conduct additional experiments on the action selection parameter top-m. Our results\ndemonstrate that increasing m from 2 to 10 enhances exploration capabilities while maintaining\ntraining stability. Specifically, larger m value (10) show more stable training compared to smaller\nvalues (2-5).\n42\nPreprint. Work in progress.\n0\n5\n10\n15\n20\n25\n30\nIteration\n0.5\n0.6\n0.7\n0.8\nWin Rate\n(a) top-m\ntop-m=2\ntop-m=5\ntop-m=10\nFigure 8: Ablation studies on action selection parameter top-m.\nC\nPROMPTS\nHere we provide all the prompt templates in three experiments.\nC.1\nMAZE EXPERIMENT\nLanguage Value/Evaluation Prompt\nEVAL_USER_PROMPT_S_V = f\"\"\"\\\nYou are playing a simple maze game. An agent is moving in the maze\nand the objective is to reach the goal in as few steps as possible.\nThe possible actions are \"move up\\n\", \"move down\\n\", \"move left\\n\",\n\"move right\\n\".\nYou are a highly skilled evaluator in this game and is expected to\nfunction similar to state value function in reinforcement learning.\nAt each step I will provide you with the move history of the\nagent (from old to new), including where the goal is, your current\nposition, the walls that surround the agent. Your output evaluation\nshould be a json array that includes the following concepts:\n- \"thoughts\": Let’s think step by step. Generate your detailed\nthought process and evaluation of the agent’s position and the\ndistance towards the goal.\n- \"final_evaluation\": Concisely judge how good the agent’s current\nposition is compared to all the possible positions in the maze, in\nterms of reaching the goal.\nLet’s start a new game. Now, please give your evaluation of the\ncurrent state given the move history of the agent:\n‘‘‘\n{{game_content}}\n‘‘‘\n\"\"\"\n43\nPreprint. Work in progress.\nLanguage TD Prompt\nEVAL_USER_PROMPT_S_TD_G2_new = f\"\"\"\\\nYou are playing a simple maze game. An agent is moving in the maze\nand the objective is to reach the goal in as few steps as possible.\nThe possible actions are \"move up\\n\", \"move down\\n\", \"move left\\n\",\n\"move right\\n\".\nYou are a highly skilled evaluator in this game, particularly adept\nat making accurate assessments through look-ahead of the current maze\nposition after taking the given action. At each step I will provide\nyou with the move history of the agent (from old to new), including\nwhere the goal is, your current position, the walls that surround\nthe agent, the action that the agent is going to take, *along with\nseveral key variations of trajectory pieces after taking this action\n(and the corresponding natural language evaluations of the trajectory\npieces)*.\nYour task is to understand these look-ahead information and\nsummarize, derive non-trivial analysis and understanding the *the\nagent’s position after taking the action*. Your output evaluation\nshould be a json array with the following *two* concepts:\n- \"thoughts\": Let’s think step by step. Summarize the look-ahead\ninformation of the variations after taking action {{chosen_action}}.\n- \"final_evaluation\": Now Concisely judge how good the chosen action\nis, in terms of reaching the goal.\nNow, please give your evaluation given action {{chosen_action}}, the\n*current environment state*:\n‘‘‘\n{{game_content}}\n‘‘‘\nand the look-ahead information of different variations after taking\naction {{chosen_action}}:\n\"\"\"\nPolicy Improvement Prompt\nPOLICY_IMPROVEMENT_PROMPT_TD = f\"\"\"\\\nYou are playing a simple maze game. An agent is moving in the maze\nand the objective is to reach the goal in as few steps as possible.\nYour task is to determine the best action for the next time step\ngiven the current state (the move history of the agent (from old to\nnew), including where the goal is, your current position, the walls\nthat surround the agent).\nYour possible actions are \"move up\\n\", \"move down\\n\", \"move left\\n\",\n\"move right\\n\".\nThe evaluations of the agent after possible actions are given. Each\nof them consists of two elements:\n- \"thoughts\": Summarization of the look-ahead information of the\nvariations after taking the chosen action.\n- \"final_evaluation\": Judge how good the chosen action is, in terms\nof reaching the goal.\nDO NOT judge the action based on your exterior knowledge, only use\nthe given evaluations to determine the best move.\nHere are the evaluations of each possible action:\n44\nPreprint. Work in progress.\nFor action \"move up\", {{evaluations_up}}\n,\nFor action \"move down\", {{evaluations_down}}\n,\nFor action \"move left\", {{evaluations_left}}\n,\nFor action \"move right\", {{evaluations_right}}\nReturn the best action (choose only one from the possible actions)\ngiven the evaluations in a json array with a key \"action\".\n\"\"\"\nC.2\nBREAKTHROUGH EXPERIMENT\nLanguage Value/Evaluation Prompt\nGAME_RULE_PROMPT = \"\"\"Here is the rule for the Breakthrough board\ngame:\nThe game is played on an 5x5 board for 2 players (white and black),\nwith each player starting with 10 pawns. white pawns are on the first\ntwo rows and black pawns are on the last two rows.\nBlack moves first. In each turn, players can move one of their pieces\none square forward, diagonally forward if the target square is empty.\nOr it can capture an opponent’s piece if that square is one step\ndiagonally forward.\nThe game ends when one player successfully break through the opponent\nlines -- either move a piece to the opposite last row of the board or\ncaptures all of the opponent’s pieces.\nFor board representation, we use b for black pieces, w for white\npieces, and . for empty squares. (1-5) and (a-e) are used to show\nthe rows and columns index respectively.\"\"\"\nEVAL_SYSTEM_PROMPT = f\"\"\"{GAME_RULE_PROMPT}\nYou are a highly skilled evaluator in this game. I will provide you\nwith specific board information representing the current board. Your\noutput should include the following concepts:\n1. *Tactical Considerations*: Any immediate threats, potential\ntactics, or vulnerabilities in the position.\n2. *Positional Evaluation*: Consideration of pawn structure, piece\nactivity, control of key squares, and game safety.\n3. *Suggested Moves*: One or two strong candidate moves for the side\nto move, along with a brief rationale for comparing different moves.\n4. *Advantage*: Based on all previous rationale, determine if white\nor black takes advantage. Use <white> or <black> to represent the\nwinning side.\nYour response should be informative and concise.\"\"\"\nEVAL_USER_PROMPT = \"\"\"*The board you need to evaluate:*\n{board}\"\"\"\nLanguage TD Prompt\nTD_SYSTEM_PROMPT = f\"\"\"{GAME_RULE_PROMPT}\nYou are a highly skilled evaluator in this game, particularly adept\nat making accurate assessments through look-ahead analysis of the\ncurrent board position.\nI will provide you with current board representation, *along with\nseveral key variations starting from this position (and their\n45\nPreprint. Work in progress.\ncorresponding natural language evaluations of the subsequent\npositions)*.\nYour task is to aggregate and compare these look-ahead information,\nto summarize, derive non-trivial analysis about the *current board*.\nYour output should include the following concepts:\n1. *Tactical Considerations*: Any immediate threats, potential\ntactics, or vulnerabilities in the position.\n2. *Positional Evaluation*: Consideration of pawn structure, piece\nactivity, control of key squares, and game safety.\n3. *Suggested Moves*: One or two strong candidate moves for the side\nto move, along with a brief rationale for comparing different moves.\n4. *Advantage*: Based on all previous rationale, determine if white\nor black takes advantage. Use <white> or <black> to represent the\nwinning side.\nYour response should be informative and concise.\"\"\"\nTD_USER_PROMPT = \"\"\"*The board you need to evaluate:*\n{board}\nHere are the look-ahead variations from the current board position:\n*Key Variations and Subsequent Evaluation:*:\n{variations}\nPlease provide your analysis and understanding of the current board\nposition based on the provided look-ahead information.\nYour response should be informative and concise.\"\"\"\nVARIATION_PROMPT = \"\"\"*Variation {i}:*\nDescription of variation’s move sequence:\n{move_desc}\nSubsequent position evaluation:\n{subsequent_eval}\"\"\"\nSUBSEQUENT_PROMPT = \"\"\"The subsequent board is:\n{sub_board}\nThe evaluation of this subsequent board is:\n{sub_eval}\"\"\"\nC.3\nTICTACTOE EXPERIMENT\nPolicy Inference Prompt\nPOLICY_INFERENCE_SYSTEM_PROMPT = \"\"\"\\\nYou are an expert agent at playing the game tic-tac-toe on a 3*3\nboard. Tic Tac Toe is a two-player game played on a grid.\nPlayers take turns marking a space with their respective symbol.\nThe goal is to get multiple of one’s own symbols in a row, either\nhorizontally, vertically, or diagonally, before the opponent does.\nIf all nine squares are filled and no player has three in a row, the\ngame is a draw.\nYour task is to choose the best move given board position.\nThe board consists of \"O\", \"X\" and grid number. The grid number\nindicates empty grid.\nYou should output your answer in the json format. Your answer\nconsists of two elements:\n46\nPreprint. Work in progress.\n- \"thought\": let’s think step by step. Generate your detailed thought\nprocess and plan for the next move.\n- \"best_move\": the best move for the next player. The move should be\nin the format of a number from 1 to 9, indicating the position on the\nboard. And the move should be in the available positions.\"\"\"\nPOLICY_INFERENCE_EXAMPLE_USER_PROMPT = \"\"\"\\\nHere is the board position and the next player is O:\nO | O | X\n---------\n4 | X | 6\n---------\n7 | 8 | 9\nThe available move positions are 4, 6, 7, 8, 9.\n\"\"\"\nPOLICY_INFERENCE_EXAMPLE_ASSISTANT_PROMPT = \"\"\"\\\n{\"thought\": \"It appears that the initial board position\nO | O | X\n---------\n4 | X | 6\n---------\n7 | 8 | 9 was favorable for X, as X has occupied the positions 3,\n5 and X can win by occupying the position 7. O has occupied the\npositions 1, 2. Therefore, the best move for O is to occupy the\nposition 7 to block X and create a potential winning opportunity by\noccupying the positions 1, 4, 7.\", \"best_move\": 7}\n\"\"\"\nPOLICY_INFERENCE_USER_PROMPT = \"\"\"\\\nHere is the board position and the next player is {next_player}:\n{state}. The available move positions are {available_positions}.\n\"\"\"\nPolicy Improvement Prompt\nPOLICY_IMPROVEMENT_SYSTEM_PROMPT = \"\"\"\\\nYou are playing the game tic-tac-toe on a 3*3 board. Tic Tac Toe is a\ntwo-player game played on a grid.\nPlayers take turns marking a space with their respective symbol.\nThe goal is to get multiple of one’s own symbols in a row, either\nhorizontally, vertically, or diagonally, before the opponent does.\nIf all nine squares are filled and no player has three in a row, the\ngame is a draw.\nThe board consists of \"O\", \"X\" and grid number. The grid number\nindicates empty grid.\nYour task is to determine the best move for the next player based on\nthe given board position and the next player.\nThe evaluations of (board, action) pairs after possible moves are\ngiven.\nDO NOT judge the board based on your knowledge, only use the\nevaluations to determine the best move.\nThe evaluation for the next board is in the format of a json format,\nconsisting of two elements:\n- \"thought\": evaluation of the the board and action pair.\n- \"final_evaluation\": Judge which side takes advantage. 1 means ’O’\ntakes advantage, -1 means ’X’ takes advantage, and 0 means the game\nwill be a draw.\n\"\"\"\nPOLICY_IMPROVEMENT_USER_PROMPT = \"\"\"\\\nHere is the board position and the next player is {next_player}:\n{state}. The possible moves are {available_positions}.\n47\nPreprint. Work in progress.\nThe following are the boards after each possible move:\n{next_states}\nNow, please give your evaluation and the best move for {next_player}\nbased on the given board position {state}.\nYou should output your answer in the json format. Your answer\nconsists of two elements:\n- \"thought\": let’s think step by step. Generate your detailed\nreflection by analyzing the next board positions and their\nevaluations.\n- \"best_move\": the best move for the next player. The move should\nbe in the format of a number from 1 to 9, indicating the position\non the board. And the move should be in the available positions\n{available_positions}.\nDon’t output extra information except for the json format.\n\"\"\"\nPolicy Evaluation Prompt\nPOLICY_EVALUATION_SYSTEM_PROMPT = \"\"\"You are a player of the game of\nTic Tac Toe. \\nThe game goal is to get multiple of one’s own symbols\nin a row, either horizontally, vertically, or diagonally, before the\nopponent does. If all nine squares are filled and no player has three\nin a row, the game is a draw. \\nThe board consists of \\\"O\\\", \\\"X\\\"\nand grid number. The grid number indicates empty grid. \\nYou are\nlearning how to evaluate a (board, action) pair in the tic tac toe\nby playing the game given the (board, action) pair and reflect the\nplaying history. \\nThe playing history depicts a game in progress\nwith a final result. Your answer consists of two elements:\n- \"thought\": let’s think step by step. Generate your detailed\nevaluation over the (board, action) pair by merely reflecting the\nplaying history after this pair from different perspectives. You\nshould only rely on the playing history as context and don’t evaluate\ngame with your own judgement. Your evaluation should contain the\nfollowing elements: Win probability, Threat, Potential strategies.\n- \"final_evaluation\": After all of your thoughts, judge which side\ntakes advantage. 1 means you think ’O’ takes advantage, -1 means you\nthink ’X’ takes advantage, and 0 means you think the game will be a\ndraw.\nYou should output your answer in the json format.\"\"\"\nPOLICY_EVALUATION_EXAMPLE_USER_PROMPT = \"\"\"The (board, action) to\nevaluate is O’s turn:\nBoard:\nO | O | X\n---------\n4 | X | 6\n---------\n7 | 8 | 9\nAction:\nThe O’s move is 4.\nBelow is the rollout sequence after this (board, action):\nAfter O’s move 4, the board position is:\nO | O | X\n---------\nO | X | 6\n---------\n7 | 8 | 9\nAfter X’s move 7, the board position is:\nO | O | X\n---------\nO | X | 6\n---------\n48\nPreprint. Work in progress.\nX | 8 | 9\nThe game is over. X wins. X wins by occupying the positions 3, 5, 7.\n\"\"\"\nPOLICY_EVALUATION_EXAMPLE_ASSISTANT_PROMPT = \"\"\"\n{\"thought\": {\"Reflection\": \"It appears that the initial board\nposition\nO | O | X\n---------\n4 | X | 6\n---------\n7 | 8 | 9\nand O’s move 4 were not favorable for O, as X was able to block on\nO’s move at 7 and ultimately win the game.\", \"Win probability\": \"The\nwin probability for X is large, while the win probability for O is\nlow.\", \"Threat\": \"X has played at 5 and 3. X can win by move 7. O\ncan occupy 1, 4, 7, and create a potential winning opportunity. X\noccupies 5, which is a key position to win the game.\", \"Potential\nstrategies\": \"Potential strategies for O include playing at 7 to\nblock X and create a potential win by occupying 1, 4, 7. X could have\noccupied 3, 5, 7 to win the game. X has already occupied 3, 5, and\nneeds only 1 move to complete the win.\"}\n\"final_evaluation\": -0.8}\n\"\"\"\nPOLICY_EVALUATION_USER_PROMPT = \"\"\"The board to evaluate is\n{player}’s turn:\nBoard:\n{board}\nAction: The {player}’s move is {action}.\nBelow is the rollout sequence 1 after this (board, action):\nAfter {player} taking action {action}, the board position is:\n{rollout_board_1}\nThe game is over. {winner_1} wins. {winner_1} wins by occupying the\npositions {positions_1}.\n...\nBelow is the rollout sequence N after this (board, action):\nAfter {player} taking action {action}, the board position is:\n{rollout_board_N}\nThe game is over. {winner_N} wins. {winner_N} wins by occupying the\npositions {positions_N}.\nNow generate your evaluation for the (board, action) pair.\"\"\"\nD\nANALOGY\nD.1\nLANGUAGE MC\nAs mentioned in the main paper and figure, language MC is an analogy of traditional MC. Specif-\nically, the mean and sum operation in traditional MC estimate can be replaced by the new lew\nlanguage aggregator G1. r can correspond to the intermediate transition for action, reward, and next\nstate. The discount factor γ can also have an interesting analogy in G1 if G1 acts as a lossy infor-\nmation compressor. Specifically, just as the discount factor reduces the weight of future rewards,\nG1 can perform a lossy compression of future information, selectively retaining only the recent and\nmost relevant aspects.\n49\nPreprint. Work in progress.\nD.2\nLANGUAGE TD\nMost Analogies between language TD and traditional TD share similarities with that in the language\nMC setting. G1 is a language aggregator over multiple lookahead variations, corresponding to the\ntrajectory mean operation, while G2 combines immediate change/intermediate transition and future\nvalue evaluation, akin to the sum operation.\n50\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ],
  "published": "2024-11-21",
  "updated": "2024-11-21"
}