{
  "id": "http://arxiv.org/abs/2411.14393v1",
  "title": "POS-tagging to highlight the skeletal structure of sentences",
  "authors": [
    "Grigorii Churakov"
  ],
  "abstract": "This study presents the development of a part-of-speech (POS) tagging model\nto extract the skeletal structure of sentences using transfer learning with the\nBERT architecture for token classification. The model, fine-tuned on Russian\ntext, demonstrating its effectiveness. The approach offers potential\napplications in enhancing natural language processing tasks, such as improving\nmachine translation.\n  Keywords: part of speech tagging, morphological analysis, natural language\nprocessing, BERT.",
  "text": "УДК 378.147: 004.42 \n \nГ. А. Чураков \nЧАСТЕРЕЧНАЯ РАЗМЕТКА ДЛЯ ВЫДЕЛЕНИЯ \nСКЕЛЕТНОЙ СТРУКТУРЫ ПРЕДЛОЖЕНИЙ \n \n \nВ статье описан процесс разработки модели для нанесения частеречной аннотации на текст \nс использованием переноса обучения BERT. Процесс подготовки данных и оценка полученных \nрезультатов. Выявлено, что предложенный способ позволяет достичь хороших результатов в \nнанесении разметки на текст. \nКлючевые \nслова: \nчастеречная \nразметка, \nморфологический \nанализ, \nобработка естественного языка. \n \nG. A. Churakov \nPOS-TAGGING TO HIGHLIGHTTHE SKELETAL STRUCTURE OF \nSENTENCES \n \nThe article describes the process of developing a model for applying partial annotation to text \nusing BERT learning transfer. Process of data preparation and evaluation of obtained results. It has been \nfound that the proposed method makes it possible to achieve good results in marking text. \nKeywords: part of speech tagging, morphological analysis, natural language processing. \nВведение \nОдним из подвидов задачи по извлечению структурированной информации \nиз текста является задача по частеречной разметке (Part-of-Speech Tagging), процесс \nаннотирования слова в тексте (корпусе) определенной части речи в зависимости от \nее определения и контекста.  \nРазметка частей речи представляет собой более сложную задачу, чем просто \nсоставление списка слов с соответствующими частями речи. Это связано с тем, что \nнекоторые слова могут включать в себя несколько частей речи в зависимости от \nконтекста, а некоторые части речи сложны по своей природе. Это нередкое явление \nв естественных языках, где значительная часть словоформ, в отличие от многих \nискусственных языков, неоднозначна. \nДанная задача решалась различными методами, первые подходы к решению \nданного вопроса основывались на составлении правил вручную. Это требовало \nобширных знаний о грамматике конкретного языка, что делало подобную систему \nаннотирования трудно переносимой на другие языки. В данной статье будет \nрассмотрен подход с использованием нейросетевой модели использующей \nархитектуру BERT в качестве основы. \nОбъектом исследования служат синтаксические и грамматические \nпризнаки, по которым данные лексемы можно распознать в тексте.  \nПредметом исследования является возможность применения алгоритма \nавтоматического аннотирования текстовых данных на русском языке частеречной \nразметкой.  \nЦель – разработать модель, способную с хорошей точностью выделять из \nтекста на русском языке скелетную частеречевую структуру.  \nМатериалом исследования является корпус, из 100 предложений на русском \nязыке, размеченных экспертом.  \nМетоды исследования: интуитивное выделение необходимых признаков для \nопределения \nотнесенности \nслова \nчасти \nречи \n– \nпредставление \nслов \nв \nзакодированном виде, моделирование синтаксического уровня при представлении \nпредложения в виде закодированной последовательности, аугментация текстовых \nданных. \nРезультатом работы является модель, позволяющая распознавать в тексте \nчасти речи. Практическая значимость заключается в том, что тегирование текста \nчастями речи является важным компонентом на этапе подготовки данных. \nОтсутствие этой лингвистической аннотации затрудняет дальнейший анализ \nтекста, в первую очередь из-за неоднозначности значений слов. Конкретно эта \nмодель предназначения для аннотирования текста базовыми тегами для улучшения \nкачества машинного перевода с применением скелетных структур [10,13,14,16,18]. \nМетрики оценки качества разметки \nСформулировав задачу POS-тэггинга, как многоклассовую классификацию, \nиспользовались соответствующие метрики. Учитывая также природу языка следует \nзаметить дисбаланс классов в выборке, в этих условиях в качестве основной метрики была \nвыбрана взвешенная F-1 мера. Данная метрика в задаче многоклассовой классификации \nвычисляется с точки зрения «один-против всех», когда к True Positive - относятся все \nклассифицированные токены, а объекты других классов относятся к False Positive. \nФормула ниже описывает вычисление F1 меры для отдельно взятого класса. \nF-1 Score(class=a) =\n2 ∗TP (class=a)\n2 ∗TP(class=a) + FP (class=a) + FN (class=a) \nФормула 1 – Вычисление F-1 меры. \nВ задаче классификации с несколькими классами используется взвешеная сумма \nоценок для каждого из классов. \nОбработка данных \nКорпус размеченного текста представлял собой 100 предложений, он был \nпредварительно предобработан, данные были трансформированы в последовательности \nтипа: предложение – набор тэгов для каждого слова в предложении. Такого объема \nданных недостаточно для обучения модели, поэтому были предприняты шаги по \nувеличению выборки[7]. Предложения тестовой выборки были нарезаны на фрагменты \nпри помощи скользящего окна размером [1; количество слов в предложении]. Таким \nобразом, количество наблюдений увеличилось тренировочную выборку в до 20000 \nнаблюдений. \nАрхитектура модели \nДля построения модели данные недостоточно разнообразны, поэтотму было \nрешение произвести перенос обучения. \nВ качестве базовой модели решения выбрана RuBERT-base[9] содержащая в себе \nенкодер блоки сети трансформер[3,4,5]. Среди особенностей модели хочется выделить ее \nустройство: \n- предобученый Токенизатор – BPE[1]. \n- Модель обучена на задаче маскированного языкового моделирования, в процессе \nобучения модель извлекает из последовательности фиксированной длинны признаки \nнеобходимые для предсказания скрытого токена, расположенного на случайной позиции \nв последовательности. \n \n \nРисунок 1– Картинка из оригинальной статьи[17]: дообучение BERT для задачи \nToken Classification. \n \nОбучение модели \n \nКак описано в оригинальной статье, BERT можно легко дообучить под решение \nузкой задачи, для этого добавим к предобученой русскоязычной модели полносвязный \nслой и определим функцию активации Softmax, на выходе модели получим \nзакодированные значения классов токенов, которые затем раскодируем во время \nпостобработки.  \nДалее обучим модель на задачу Token Classification с учителем, будем оценивать \nкачество после каждой эпохи и при помощи оптимизатора Adam корректировать веса \nмодели методом обратного распространения ошибки. \nПроцесс обучения модели можно наблюдать на рисунке ниже. \n \n \n \nНа валидации были получены следующие метрики для модели: \n- F1: 0.8642 \n- Accuracy: 0.8822 \n \nИтоги \nЦелью данной работы разработка модели, способной с хорошей точностью \nвыделять из текста на русском языке скелетную частеречевую структуру. Важно признать, \nчто обучение и оценка модели проводились на относительно ограниченном наборе \nданных, поэтому результаты могут отличаться в зависимости от набора данных. \nДанную работу можно продолжать за счёт увеличения объёма тренировочных \nданных (мультиязычность, более сбалансированная выборка) и подбора гиперпараметров \nнейронной сети. В ходе анализа результатов работы модели, также было выявлено, что \nмодель показывает способна к разметке на другом языке, корректно выделяются части \nречи, которых встретилось сравнительно много в наборе данных для обучения.  \nС результатами работы можно ознакомиться на следующих ресурсах: \n- Github: https://github.com/disk0Dancer/rubert-finetuned-pos.git; \n- Hugging Face Demo: https://huggingface.co/disk0dancer/ruBert-base-finetuned-pos. \n \nБиблиография \n1. Sennrich R., Haddow B., Birch A. Neural Machine Translation of Rare Words with \nSubword Units // 2016. \n2. Bojanowski P. [и др.]. Enriching Word Vectors with Subword Information // 2017. \n3. Vaswani A. [и др.]. Attention Is All You Need // 2023. \n4. Devlin J. [и др.]. BERT: Pre-training of Deep Bidirectional Transformers for Language \nUnderstanding // 2019. \n5. Tenney I., Das D., Pavlick E. BERT Rediscovers the Classical NLP Pipeline // 2019. \n6. Wang C., Cho K., Gu J. Neural Machine Translation with Byte-Level Subwords // 2019. \n7. Thakur N. [и др.]. Augmented SBERT: Data Augmentation Method for Improving Bi-\nEncoders for Pairwise Sentence Scoring Tasks // 2021. \n8. Hui W. Performance of Transfer Learning Model vs. Traditional Neural Network in \nLow System Resource Environment // 2020. \n9. Zmitrovich D. [и др.]. A Family of Pretrained Transformer Language Models for \nRussian // 2023. \n10. Ailamazyan Program Systems Institute of RAS, Pereslavl-Zalessky, 152020, Russian \nFederation, Trofimov I. V. Automatic Morphological Analysis for Russian: Application-\nOriented Survey // PROGRAMMNAYA INGENERIA. 2019. № 9–10 (10). C. 391–399. \n11. Hiroki Nakayama Seqeval: A Python framework for sequence labeling evaluation \n[Электронный ресурс]. URL: https://github.com/chakki-works/seqeval. \n12. Liao W., Veeramachaneni S. A simple semi-supervised algorithm for named entity \nrecognition Boulder, Colorado: Association for Computational Linguistics, 2009.C. 58–65. \n13. Mylnikova A. V., Trusov V. A., Mylnikov L. A. Use of Text Skeleton Structures for \nthe Development of Semantic Search Methods // Automatic Documentation and Mathematical \nLinguistics. 2023. № 5 (57). C. 301–307. \n14. Novikova A. Direct Machine Translation and Formalization Issues of Language \nStructures and Their Matches by Automated Machine Translation for the Russian-English \nLanguage Pair 2018. \n15. Pennington J., Socher R., Manning C. Glove: Global Vectors for Word Representation \nDoha, Qatar: Association for Computational Linguistics, 2014.C. 1532–1543. \n16. Sapin A. S. Building neural network models for morphological and morpheme \nanalysis of texts // Proceedings of the Institute for System Programming of the RAS. 2021. № 4 \n(33). C. 117–130. \n17. Wei Q. [и др.]. Relation Extraction from Clinical Narratives Using Pre-trained \nLanguage Models // AMIA ... Annual Symposium proceedings. AMIA Symposium. 2019. \n(2019). C. 1236–1245. \n18. Zhang Y., Jin R., Zhou Z.-H. Understanding bag-of-words model: a statistical \nframework // International Journal of Machine Learning and Cybernetics. 2010. № 1–4 (1). C. \n43–52. \n19. INTELLECTUAL ANALYSIS OF DATA ON THE BASIS OF STANFORD \nCoreNLP FOR POS TAGGING OF TEXTS IN THE RUSSIAN LANGUAGE // Systems and \nMeans of Informatics. 2018. \n20. \nClassical \nML \nEquations \nin \nLaTeX \n[Электронный ресурс]. \nURL: https://blmoistawinde.github.io/ml_equations_latex/. \n21. \nMLM \n[Электронный \nресурс]. \nURL: https://www.sbert.net/examples/unsupervised_learning/MLM/README.html. \n22. \nSummary \nof \nthe \ntokenizers \n[Электронный ресурс]. \nURL: https://huggingface.co/docs/transformers/tokenizer_summary. \n23. \nwhat-is-word2vec-and-how-does-it-work \n[Электронный \nресурс]. \nURL: https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work. \n \nОб авторе \nГригорий \nАлександрович \nЧураков \n– \nстудент \nбакалавриата \nобразовательной программы «Программная инженерия», факультета социально-\nэкономических и компьютерных наук, Пермский филиал Национального \nисследовательского университета «Высшая школа экономики», Россия, Пермь, \nemail: gachurakov@edu.hse.ru \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-11-21",
  "updated": "2024-11-21"
}