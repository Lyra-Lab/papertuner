{
  "id": "http://arxiv.org/abs/2411.15197v1",
  "title": "K-means Derived Unsupervised Feature Selection using Improved ADMM",
  "authors": [
    "Ziheng Sun",
    "Chris Ding",
    "Jicong Fan"
  ],
  "abstract": "Feature selection is important for high-dimensional data analysis and is\nnon-trivial in unsupervised learning problems such as dimensionality reduction\nand clustering. The goal of unsupervised feature selection is finding a subset\nof features such that the data points from different clusters are well\nseparated. This paper presents a novel method called K-means Derived\nUnsupervised Feature Selection (K-means UFS). Unlike most existing spectral\nanalysis based unsupervised feature selection methods, we select features using\nthe objective of K-means. We develop an alternating direction method of\nmultipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS\nmodel. Extensive experiments on real datasets show that our K-means UFS is more\neffective than the baselines in selecting features for clustering.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n1\nK-means Derived Unsupervised Feature Selection\nusing Improved ADMM\nZiheng Sun, Chris Ding, and Jicong Fan\nAbstract—Feature selection is important for high-dimensional\ndata analysis and is non-trivial in unsupervised learning problems\nsuch as dimensionality reduction and clustering. The goal of\nunsupervised feature selection is finding a subset of features such\nthat the data points from different clusters are well separated.\nThis paper presents a novel method called K-means Derived\nUnsupervised Feature Selection (K-means UFS). Unlike most\nexisting spectral analysis based unsupervised feature selection\nmethods, we select features using the objective of K-means. We\ndevelop an alternating direction method of multipliers (ADMM)\nto solve the NP-hard optimization problem of our K-means UFS\nmodel. Extensive experiments on real datasets show that our\nK-means UFS is more effective than the baselines in selecting\nfeatures for clustering.\nIndex Terms—Feature selection, K-means, ADMM\nI. INTRODUCTION\nF\nEATURE selection aims to select a subset among a\nlarge number of features and is particularly useful in\ndealing with high-dimensional data such as gene data in\nbioinformatics. The selected features should preserve the most\nimportant information of the data for downstream tasks such\nas classification and clustering. Many unsupervised feature\nselection methods have been proposed in the past decades.\nThey can be organized into three categories [1], [2]: filter\nmethods [3], wrapper methods [4], and hybrid methods. Filter\nmethods evaluate the score of each feature according to certain\ncriteria, such as Laplacian score (LS) [5], [6] and scatter\nseparability criterion [7]. Wrapper methods utilize algorithms\nto evaluate the quality of selected features. They repeat se-\nlecting a subset of features and evaluating the performance of\nan algorithm on these features until the performance of the\nalgorithm is desired. Correlation-based feature selection [8]\nand Gaussian mixture models [9] are representative ones of\nwrapper methods. Hybrid methods [10], [11] utilize filtering\ncriteria to select feature subsets and evaluate the feature\nsubsets by algorithm performance.\nWithout labels to evaluate the feature relevance, many\ncriteria have been proposed for unsupervised feature selection\nin recent years. The most widely used one is to select features\nthat can preserve the data similarity using Laplacian matrix.\nFor instance, Multi-Cluster Feature Selection (MCFS) [12]\nselects features using spectral analysis and a regression model\nwith ℓ1-norm regularization. Non-negative Discriminative Fea-\nture Selection (NDFS) [13] selects features using non-negative\nZiheng Sun, Chris Ding, and Jicong Fan are with the School of\nData Science, The Chinese University of Hong Kong (Shenzhen), China.\nZiheng Sun and Jicong Fan are also with Shenzhen Research Insti-\ntute of Big Data, China. Email: zihengsun@link.cuhk.edu.cn,\n{chrisding,fanjicong}@cuhk.edu.cn\nspectral analysis and a regression model with ℓ2,1-norm reg-\nularization. Robust Unsupervised Feature Selection (RUFS)\n[14] selects features using label learning, non-negative spectral\nanalysis and a regression model with ℓ2,1-norm regularization.\nJoint Embedding Learning and Sparse Regression (JELSR)\n[15] selects features using embedding learning and sparse\nregression jointly. Li et al. [16] developed a sampling scheme\ncalled feature generating machines (FGMs) to select informa-\ntive features on extremely high-dimensional problems. Non-\nnegative Spectral Learning and Sparse Regression-based Dual-\ngraph regularized feature selection (NSSRD) [17] extends the\nframework of joint embedding learning and sparse regression\nby incorporating a feature graph. Wang et al. [18] proposed\nto select features using an autoweighted framework based on\na similarity graph. Embedded Unsupervised Feature Selection\n(EUFS) [19] used sparse regression and spectral analysis. Li\net al. [20] proposed an unsupervised feature selection method\nbased on sparse PCA with ℓ2,p-norm. Sparse and Flexible\nProjection for Unsupervised Feature Selection with Optimal\nGraph (SF2SOG) [21] selects features using the optimal flex-\nible projections and orthogonal sparse projection with ℓ2,0-\nnorm constraint. All these methods select features based on\ndata similarity using spectral analysis, but they don’t focus on\nthe separability of data points under the selected feature space.\nIn this work, we present a new method called K-means\nDerived Unsupervised Feature Selection (K-means UFS). Un-\nlike those spectral analysis based methods, we select features\nto minimize the K-means objective proposed by [22], [23].\nThe goal of our method is to select the most discriminative\nfeatures such that the data points are well separated, that is,\nhave small within-cluster differences and large between-cluster\ndifferences. We focus on the separability of data points and\nderive this new unsupervised feature selection method from\nK-means. The contributions of this work are as follows.\n• A novel unsupervised feature selection method is pro-\nposed to select the most discriminative features based on\nthe objective of K-means.\n• An\nAlternating\nDirection\nMethod\nof\nMultipliers\n(ADMM) [24] algorithm is developed for the NP-hard\nproblem of K-means UFS model.\n• We compare K-means UFS with other state-of-the-art un-\nsupervised feature selection methods and conduct exper-\niments on real datasets to demonstrate the effectiveness\nof our method.\nThe rest of this paper is organized as follows. In Section II,\nwe derive the K-means UFS model from K-means objective.\nIn Section III, we develop an ADMM algorithm to solve\narXiv:2411.15197v1  [cs.LG]  19 Nov 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n2\nTABLE I: Notations\nsymbol\ndescription\nx\ncolumn vector\nxi\ncolumn vector with index i\nX\nmatrix\nTr(·)\ntrace of matrix\n∥· ∥2\nℓ2 or Euclidean norm of vector\n∥· ∥F\nFrobenius norm of matrix\n1(·)\n1(x) = 1 if x ̸= 0; 1(x) = 0 if x = 0\n∥· ∥2,1\nℓ2,1 norm of matrix, defined as:\n∥M∥2,1 = Pq\nj=1\nqPp\ni=1 M2\nij = Pq\nj=1 ∥mj∥2\n∥· ∥2,0\nℓ2,0 norm of matrix, defined as:\n∥M∥2,0 = Pq\nj=1 1\n\u0010qPp\ni=1 M2\nij\n\u0011\n= Pq\nj=1 1(∥mj∥2)\nthe optimization problem of our K-means UFS model. In\nSection IV, we compare K-means UFS with other state-of-\nart unsupervised feature selection methods in detail. Section\nV presents the experiments and Section VI draws conclusions\nfor this paper. Table I shows the main notations used in this\npaper.\nII. K-MEANS DERIVED UNSUPERVISED FEATURE\nSELECTION\nIn unsupervised feature selection, there is no unique cri-\nteria to evaluate the quality of selected features. We choose\nto select features by minimizing the objective of K-means\nclustering proposed by [22], [23]. Given a data matrix X =\n(x1, x2, ..., xn) ∈Rp×n, where p denotes the number of\nfeatures and n denotes the number of samples. Each feature\n(row) of X is standardized to have zero mean and unit vari-\nance. In K-means clustering, the k centroids are determined\nby minimizing the sum of squared errors,\nJk =\nk\nX\nj=1\nX\ni∈Cj\n∥xi −mj∥2\n2 ,\n(1)\nwhere mj = P\ni∈Cj xi/nj is the centroid of cluster Cj\nconsisting of nj points, j = 1, . . . , k. According to [22], Jk\ncan be reformulated as\nJk = Tr(X⊤X) −Tr(G⊤X⊤XG),\n(2)\nwhere G = (g1, . . . , gk) ∈Rn×k is a normalized indicator\nmatrix denoting whether a data point is in a cluster or not,\nnamely,\ngij =\n\u001a 1/√nj,\nif xi ∈Cj\n0,\notherwise.\n(3)\nLet Ψ be the feasible set of all possible indicator matrices with\nk clusters. Since Tr(X⊤X) is a constant, [22] pointed out that\nK-means clustering is equivalent to\nmin\nG∈Ψ −Tr(G⊤X⊤XG).\n(4)\nOur K-means derived unsupervised feature selection (K-means\nUFS) seeks to select the most discriminative features. The\ninput data X contains p rows of features. Let Xh ∈Rh×n be\nthe selected h rows of X. Our feature selection goals is the\nfollowing: Among all possible choice of Xh, we selection\nthe Xh which minimizes the K-means objective. Therefore,\nK-means UFS solves the following problem\nmin\nXh\n[min\nG∈Ψ −Tr(G⊤X⊤\nh XhG)]\n(5)\nIn order to show the intuition of model (5), we generate a\ntoy data matrix X ∈R4×30, shown in Figure 1. The left plot\nshows Xh1 consisting of the first two rows of X, while the\nright one shows Xh2 consisting of the last two rows of X. We\nprefer Xh2 because the within-cluster differences are much\nsmaller than those in Xh1. It is expected that solving problem\n(5) can select the last two features. We want to select the most\ndiscriminative features in an unsupervised manner.\nFig. 1: Visualization of Xh1 and Xh2 of a toy example\n(Description: The x-axis and y-axis of the left figure represent the\nfirst two features (rows) of X, while the axis of the right one\nrepresent the last two features of X.)\nA. K-means UFS model\nIn this section, we show that problem (5) can be formulated\ninto a discrete quadratic optimization problem. Let S ∈Rp×h\nbe a selection matrix defined as:\nXh = S⊤X,\ns.t. S⊤S = I,\nSij ∈{0, 1}\n(6)\nLet Φ be the feasible set of all the selection matrix S, we\nrewrite problem (5) as:\nmin\nG∈Ψ,S∈Φ −Tr(S⊤XGG⊤X⊤S)\n(7)\nTherefore (7) solves both the K-means clustering problem\n(the optimal G) and the feature selection problem (optimal\nS) simultaneously.\nProblem (7) is difficult to optimize. Fortunately, the ap-\nproximate solution to K-means clustering was obtained in\n[22], [23]. The approximate solution of K-means indicator G∗\ncan be constructed as the following. Let the singular value\ndecomposition (SVD) of X be X = PΣQ⊤, where P and\nQ denote the left and right singular vectors and the singular\nvalues in Σ are sorted decreasingly. Let Pk be the first k\ncolumns of P, Qk be the first k columns of Q and Σk be\nthe first k singular values of Σ. [22], [23] showed that Qk is\na good approximation of G∗.\nNow, with this approximate optimal solution for G, we\nrewrite the objective of problem (7) as\nmin\nS∈Φ −Tr(S⊤PΣQ⊤QkQ⊤\nk QΣP ⊤S)\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n3\nthen, the feature selection optimization problem (7) is simpli-\nfied to\nmin\nS∈Φ −Tr(S⊤PkΣ2\nkP ⊤\nk S) ≡−Tr(S⊤AS)\n(8)\nwhere\nA = PkΣ2\nkPk\nis in fact the largest k rank of the square of data covariance\nmatrix. Problem (8) is our model for K-means UFS.\nB. Relaxation\nProblem (8) is a discrete optimization problem which is\ntypically NP-hard. To solve this problem we use numerical\nrelaxation. From Eq.(6), we infer that S has h nonzero rows.\nThis can be seen as the following. The p rows of X can be\nreordered such that the selected rows are reshuffled to the top\nh rows of X,\nX =\n\u0014 Xh\nX−h\n\u0015\n, S =\n\u0014\nIh×h\n0(p−h)×h\n\u0015\nwhere X−h represents the rest of rows un-selected, Ih×h is\nan identity matrix and 0(p−h)×h is a zero matrix. Therefore,\nS has three constraints:\nS⊤S = I, ∥S⊤∥2,0 = h,\nSij ∈{0, 1}\n(9)\nNow, from the K-means UFS model (8), if S∗is an optimal\nsolution, V = S∗R (where R ∈Rh×h is a rotation matrix,\nthat is, RR⊤= I.) is also an optimal solution, because\nTr(S⊤AS)\n=\nTr(R⊤S⊤ASR). Thus the binary discrete\nconstraint in Eq.(9) is not necessary. Therefore, the relaxed\nV has only two constraints:\nV ⊤V = I, ∥V ⊤∥2,0 = h\n(10)\nFinally, we solve the K-means UFS model by the following\nrelaxed optimization problem.\nmin\nV\n−Tr(V ⊤AV )\ns.t.\nV ⊤V = I, ∥V ⊤∥2,0 = h\n(11)\nNote that once the optimal solution V ∗is obtained, the feature\nselection matrix S∗is determined uniquely by the index of the\nh nonzero rows of V ∗. The value of rotation matrix R has no\ncontribution to feature selection.\nIII. OPTIMIZATION: AN IMPROVED ADMM\nIn this section, we elaborate how to solve optimization\nproblem (11) using ADMM.\nA. Vanilla ADMM and its limitation\nWe consider an equivalent form of problem (11):\nmin\nV\n−Tr(V ⊤AV )\ns.t.\nV = U,\nU ⊤U = I\nV = W,\n∥W ⊤∥2,0 = h.\n(12)\nThen the augmented Lagrangian function of (12) is\nLµ(V, U, W) = −Tr(V ⊤AV ) + µ\n2 ∥V −U + Ω/µ∥2\nF\n+ µ\n2 ∥V −W + Γ/µ∥2\nF + const\n(13)\nwhere Ω, Γ ∈Rp×h are Lagrange multiplier matrices, µ > 0\nis a penalty parameter, U is an orthogonal matrix and W is\na row-sparse matrix. Then we update the variables alternately\n[24]:\nV t+1 = arg min\nV\nLµ(V, U t, W t)\n(14)\nU t+1 = arg min\nU\nLµ(V t+1, U, W t)\n(15)\nW t+1 = arg min\nW\nLµ(V t+1, U t+1, W)\n(16)\nΩt+1 = Ωt + µ⊤(V t+1 −U t+1)\n(17)\nΓt+1 = Γt + µ⊤(V t+1 −W t+1)\n(18)\nµt+1 = µ⊤× ρ,\nρ = 1.05.\n(19)\n(20)\nStep 1: Update V\nLet B = U ⊤−Ω⊤\nµ⊤, C = W ⊤−Γ⊤\nµ⊤, after algebra, the update\nV step (14) is solving the following problem:\nmin\nV\n−Tr(V ⊤AV )+ µ⊤\n2 ∥V −B∥2\nF + µ⊤\n2 ∥V −C∥2\nF (21)\nTake the derivative of this objective function to be zero, we\ncan update V in t iteration by:\nV t+1 = 1\n2µ⊤(µ⊤I −A)−1(B + C)\n(22)\nTo guarantee the minimal solution exists, (µ⊤I−A) should be\na positive definite matrix. Suppose λ1 is the max eigenvalue\nof A, we select the initial value of µ as µ0 = λ1 + 0.1. Step\n2: Update U\nThe update U step (15) is solving the following problem:\nmin\nU\nµ⊤\n2 ∥V t+1 −U + Ω⊤/µ⊤∥2\nF\ns.t.\nU ⊤U = I\nLet D = V t+1 + Ω⊤/µ⊤, it is equivalent to:\nmax\nU\nTr(D⊤U)\ns.t.\nU ⊤U = I\n(23)\nLet Ph be the first h column of P from D = PΣQ⊤(SVD).\nThe solution is:\nU t+1 = PhQ⊤\n(24)\nStep 3: Update W\nLet F = V t+1 + Γ⊤/µ⊤, the update W step (16) is solving\nthe following problem:\nmin\nW\n∥F −W∥2\nF\ns.t.\n∥W ⊤∥2,0 = h\n(25)\nNow we split W and F by rows:\nW ⊤= [w1T , w2T , . . . , wpT ]\nF ⊤= [f 1T , f 2T , . . . , f pT ]\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n4\nSelect l = {l1, l2, ..., lh} as the subset of h row indices in F\nsatisfy:\n∥f l1∥2 ≥∥f l2∥2 ≥· · · ≥∥f lh∥2 ≥∥f j∥2,\n∀j /∈l\nThus, we update each row of W in t iteration by:\nw(t+1)j = f j,\n∀j ∈l\nw(t+1)j = ⃗0,\n∀j /∈l\n(26)\nFig. 2: log10(∥V ∥F ) of Quadratic ADMM Eq. (12) and Bi-\nlinear ADMM Eq. (27).\n(Description: The y-axis represents the value of log10(∥V ∥F ) and\nthe x-axis represents the iteration of ADMM. The ∥V ∥F of\nQuadratic ADMM Eq. (12) increases into 1012.5 in 80 iterations.)\nIn Figure 2, we apply Quadratic ADMM Eq.(12) on StatLog\nDNA data[25] (3186 samples, 180 features and 3 classes) and\nobsverve the log10(∥V ∥F ) (blue) increases into 12.5 before\nconvergence, that means the scale of matrix V is blow-up. The\nvery large values in V will dominate the ADMM process. It’s\ncaused by the subproblem (14) in update V step. The (µI−A)\nmatrix is an ill-conditioned matrix for some data and µ, thus\nwe should NOT compute the the inversion of it in Eq. (22). In\nnext subsection, two useful tricks are presented to avoid the\nscale blow-up problem.\nB. Bi-Linear ADMM\nThe first trick to avoid scale blow-up is to force ∥V ∥F to be\na constant in each iteration of ADMM. Considering V is an\northogonal matrix in Eq. (11), the norm of V should satisfy\nthe constraint:\n∥V ∥2\nF = Tr(V ⊤V ) = Tr(Ih×h) = h\nThe second trick is to change the quadratic objective function\n(−Tr(V ⊤AV )) into a bi-linear one (−Tr(V ⊤AU)). This trick\ncan avoid the matrix inversion in Eq.(22). Using these two\ntricks, we obtain an equivalent bi-linear form of Quadratic\nADMM Eq.(12) as following:\nmin\nV\n−Tr(V ⊤AU)\ns.t.\n∥V ∥2\nF = h,\nV = U,\nU ⊤U = I\nV = W,\n∥W ⊤∥2,0 = h\n(27)\nThe ∥V ∥2\nF = h is necessary for Bi-linear ADMM, though it\nis redundant in math. We can see it in the update V step.\nStep 1: Update V\nLet B = U ⊤−Ω⊤\nµ⊤and C = W t −Γt/µ⊤, the update V step\nof problem (27) is solving the following problem:\nmin\nV\n−Tr[(U tT A + µ⊤B⊤+ µ⊤C⊤)V ]\ns.t.\n∥V ∥2\nF = h\n(28)\nLet D = U tT A + µ⊤B⊤+ µ⊤C⊤, the solution is:\nV t+1 =\n√\nh\n∥D∥F\nD\n(29)\nThe ∥V ∥2\nF\n= h constraint is necessary to guarantee the\nminimal solution for problem (28) exits. In each iteration,\nupdating V by Eq. (29) will force ∥V ∥2\nF to be a constant\nh. In Figure 2, the log10(∥V ∥F ) (red) of Bi-linear ADMM is\nalways a constant.\nStep 2: Update U\nLet E = V t+1 + Ω⊤/µ⊤, after algebra, the update U step of\nBi-linear ADMM is solving the following problem:\nmin\nU\n−Tr(V (t+1)T AU) + µ⊤\n2 ∥U −E∥2\nF\ns.t.\nU ⊤U = I\n(30)\nLet H = AV t+1 + µE, Ph be the first h column of P from\nH = PΣQ⊤(SVD). The solution is:\nU t+1 = PhQ⊤\n(31)\nThe update W step is exactly the same as Eq. (26), so we\nomit it here. With these update rules, our K-means UFS using\nBi-linear ADMM is summarized in Algorithm 1.\nAlgorithm 1 K-means UFS via Bi-linear ADMM\nRequire: Data X, number of clusters k, number of selected\nfeatures h.\n1: Initialize µ = 0.1, ρ = 1.05. V, U, W is initialized using\nEq. (32). A is initialized using Eq. (8).\n2: Set t = 0\n3: repeat\n4:\nUpdate V using Eq.(29).\n5:\nUpdate U using Eq.(31).\n6:\nUpdate W using Eq.(26).\n7:\nUpdate Ω, Γ and µ using Eq.(17), Eq.(18), Eq.(19).\n8: until Convergence\nSelect the h features determined by the h nonzero rows\nin W.\nC. Discussion on The Initialization\nV, U, W should be initialized into the same value because\nof the V = U and V = W constraint. We initialize V by\nremoving the discrete ℓ2,0-norm constraint in problem (11):\nmin\nV\n−Tr(V ⊤AV )\ns.t.\nV ⊤V = I\n(32)\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n5\nLet Ph be the first h column of P from A = PΣP ⊤(SVD),\nthen we initialize V = Ph. Lagrange multiplier matrices Ω=\nΓ = 0. µ is empirically set in the range from 10−4 to 10−1\ndepending on the datasets and is updated by µt+1 = µ⊤× ρ\nin each iteration. If µ is larger than µmax = 107, we stop\nupdating µ. ρ is empirically set to 1.05 in our algorithm.\nD. Discussion on The Convergence\nThe convergence proof of ADMM can be found in [24].\nIn ADMM process, W is always a row-sparse matrix with h\nnonzero rows (Eq. (26)) which represents the h selected fea-\ntures. In t iteration, we can recover a unique feature selection\nmatrix S⊤determined by the h nonzero row indices of W ⊤\nbecause of W ⊤= V ⊤and V ⊤= S⊤R. Our termination\ncriteria is: If S⊤doesn’t change in 30 iterations, we stop\nthe ADMM and output the selected features. In practice,\nwe set the maximum iteration value as 3000. In experiment,\nour algorithm converges within 300 iterations for all datasets.\nE. Discussion on The Time Complexity\nThe time complexity of update V step (29) involves the\ncomputation of D and its Frobenius norm, which is both\nO(np). The time complexity of update U (31) involves com-\nputation of E and its SVD, which is O(np) and O(np2). The\ntime complexity of update W Eq.(26) involves computation\nof F and sort the norm of rows in F, which is O(np) and\nO(p log(p)). Since n ≫p, the time complexity of each\niteration is O(np2).\nF. Discussion on The Reproducibility\nThere are no adjustable parameters in the relaxed K-means\nUFS model (10). The two parameters µ and ρ come from\nthe ADMM. If we set ρ = 1.05 and using the initialization\nsettings above. Our algorithm is a deterministic algorithm with\nreproducibility for all the datasets we used.\nIV. OTHER UNSUPERVISED FEATURE SELECTION\nMETHODS\nIn this section, we compare K-means UFS with other state-\nof-the-art unsupervised feature selection (UFS) methods. Most\nother UFS methods select features using spectral analysis [5],\n[6] and sparse regression. They tends to select features that can\npreserve the structure of similarity matrix. Our K-means UFS\n(5) select features in a totally different way. We select the most\ndiscriminative features using the K-means objective which\nhave smaller within-cluster difference and larger between-\ncluster difference.\nFor convenience, we denote original data X ∈Rp×n,\nsimilarity matrix1 ˜S ∈Rn×n, 1 = [1, ..., 1]⊤, degree matrix\n1Different methods use different definition of ˜S and details can be found\nin their papers. Laplacian score use locality projection[26]. In NDFS and\nRUFS, they use the k-nearest similarity matrix ˜S. Denote Nk(i) as the set\nof k-nearest nodes of xi, then\n˜Sij =\n(\nexp ∥xi −xj∥2/σ,\nxi ∈Nk(j) or xj ∈Nk(i)\n0,\notherwise\nD = diag( ˜S1), Laplacian matrix L = D −˜S, normalized\nLaplacian matrix ˆL = D−1\n2 (D−˜S)D−1\n2 , and k is the number\nof clusters. Let Y ∈Rn×k be the cluster indicator matrix,\nW\n∈Rp×k be the regression coefficient weights matrix,\nV ∈Rp×k be the latent feature matrix.\nNonnegative Discriminative Feature Selection\nNDFS [13] selects features using non-negative spectral analy-\nsis and sparse regression with ℓ2,1-norm regularization. They\npropose the NDFS model as following:\nmin\nY,W Tr(Y ⊤ˆLY ) + α∥W ⊤X −Y ⊤∥2\nF + β∥W ⊤∥2,1\ns.t. Y ⊤Y = I,\nY ≥0\n(33)\nwhere α and β are parameters.\nRobust Unsupervised Feature Selection\nRUFS[14] selects features using non-negative matrix factoriza-\ntion of X, non-negative spectral analysis and sparse regression\nwith ℓ2,1-norm. They propose the RUFS model as following:\nmin\nY,C,W\n∥X −CY ⊤∥2,1 + νTr(Y ⊤ˆLY )\n+ α∥W ⊤X −Y ⊤∥2,1 + β∥W ⊤∥2,1,\ns.t.\nY ⊤Y = I,\nY ≥0,\nC ≥0\n(34)\nwhere C ∈Rp×k are cluster centers and ν, α β are parameters.\nEmbedded Unsupervised Feature Selection\nEUFS[19] also selects features using non-negative matrix\nfactorization of X, non-negative spectral analysis and sparse\nregression with ℓ2,1-norm. They propose the EUFS model as\nfollowing:\nmin\nY,W\n∥X −V Y ⊤∥2,1 + νTr(Y ⊤ˆLY ) + β∥V ⊤∥2,1\ns.t.\nY ⊤Y = I,\nY ≥0\n(35)\nSparse and Flexible Projection for Unsupervised Feature\nSelection with Optimal Graph\nSF2SOG [21] applies spectral analysis on the optimal graph\nG ∈Rn×n, not similarity matrix S. Denote LG ∈Rn×n\nas the normalized Laplacian matrix of G. They propose the\nSF2SOG model as follows:\nmin\nY,W,G Tr(Y ⊤ˆLGY ) + λ∥W ⊤X −Y ⊤∥2\nF + γ∥G −S∥2\nF\ns.t. W ⊤W = I, ∥W ⊤∥2,0 = h, 0 ≤Gij ≤1,\nn\nX\nj=1\nGij = 1\n(36)\nA. Discussion and Comparison\nNDFS (33), RUFS (34), EUFS (35) and SF2SOG (36)\nare all based on the spectral analysis and sparse regression\nframework. Our K-means UFS are totally different from these\nmethods. First, K-means UFS selects the most discriminative\nfeatures based on the K-means objective and focuses on\nthe separability of data points. NDFS, RUFS, EUFS and\nSF2SOG consider the local structure of data distribution using\nspectral analysis and sparse regression. Second, the ℓ2,0-norm\nconstraint used in K-means UFS is derived from the selection\nmatrix S in Eq. (6) directly. The ℓ2,1-norm regularization used\nin NDFS, RUFS and EUFS can be regarded as a relaxation\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n6\nof ℓ2,0-norm constraints[27].Third, K-means UFS reveals the\nrelationship between data separability and the largest k rank of\nthe square of data covariance matrix A in Eq. (8). Researchers\ncan design more advanced methods base on our K-means\nobjective criterion to select features that can increase data\nseparability.\nV. EXPERIMENTS\nIn this section, we conduct experiments to evaluate the\nperformance of K-means UFS. Following previous unsuper-\nvised feature selection work[6], [13], [12], we only evaluate\nthe performance of K-means UFS in terms of clustering. We\nevaluate the performance of clustering using accuracy(ACC)\nand Normalized Mutual Information(NMI).\nA. Datasets\nThe experiments are conducted on six real datasets. Some\ndatasets are too large so we choose a small subset of them\nto reduce the time costs of Laplacian matrix computation\n(O(n2)) in Laplacian Score, NDFS, RUFS and EUFS. The\nstatistics of the datasets are summarized in Table II.\n• MicroMass2 is a dataset for the identification of microor-\nganisms from mass-spectrometry data.\n• Human\nActivity\nRecognition\nusing\nSmartphones\n(HARS)3 is a dataset for the recognition of human\nactivities\nusing\nwaist-mounted\nsmartphone\nwith\nembedded inertial sensors. The original dataset consists\nof 10299 samples so we select the first 500 samples in\neach classes.\n• Fashion-MNIST (Fashion)4 is a dataset of Zalando’s\narticle images consisting of 70000 samples including T-\nshirts, Trousers and so on. We select the first 300 samples\nin each classes.\n• Gina5 is a benchmark dataset for handwritten digit recog-\nnition in the agnostic learning.\n• Fabert and Dilbert6 are benchmark datasets for multiclass\ntasks in AutoML challenge[28]. Fabert contains 8237\nsamepls so we select the first 500 samples in each classes.\nDilbert contains 10000 samepls so we select the first 700\nsamples in each classes.\nTABLE II: Statistics of the Datasets\nDataset\n# of samples\n# of features\n# of classes\nMicroMass\n1300\n360\n10\nHARS\n3000\n561\n6\nFashion\n3000\n784\n10\nGina\n3468\n784\n10\nFabert\n3500\n800\n7\nDilbert\n3500\n2000\n5\n2MicroMass is in UCI Machine Learning Repository: https://archive.ics.uci.\nedu/ml/datasets/MicroMass\n3HARS is in UCI Machine Learning Repository: https://archive.ics.uci.edu/\nml/datasets/human+activity+recognition+using+smartphones\n4Fashion-MNIST is a dataset for Kaggle competitions : https://www.kaggle.\ncom/datasets/zalando-research/fashionmnist\n5Gina is a benchmark dataset in IJCNN 2007 Workshop on Agnostic\nLearning vs. Prior Knowledge: http://www.agnostic.inf.ethz.ch/datasets.php\n6https://automl.chalearn.org/data\nB. Experimental Settings\nIn experiments, the numbers of selected features are set\nas {50, 100, 150, 200, 250, 300} for all datasets. There are\nno adjustable parameters in K-means UFS model need to\nbe tuned. In experiments, we set ρ = 1.05 and use the\ninitialization settings above, the ADMM (Algorithm 1) is a de-\nterministic algorithm with reproducibility for all the datasets.\nWe compare K-means UFS with the following unsupervised\nfeature selection methods:\n1) All Features: All original features are adropted.\n2) Laplacian Score [5]\n3) NDFS: Non-negative Discriminative Feature Selection\n(33) [13]\n4) RUFS: Robust Unsupervised Feature Selection (34) [14]\n5) EUFS: Embedded Unsupervised Feature Selection (35)\n[19]\n6) SF2SOG: Sparse and Flexible Projection for Unsuper-\nvised Feature Selection with Optimal Graph (36) [21]\nThere are some parameters to be set for baseline methods.\nFollowing [14], [19], we set the neighborhood size to be 5\nfor the similarity matrix of all datasets. To fairly compare\ndifferent unsupervised feature selection methods, the α, β, γ, λ\nparameters in RUFS (34), EUFS (35), SF2SOG (36) are tuned\nby ”grid-search” strategy from {10−6, 10−4, ..., 104, 106}. We\nreport the best clustering results from the optimal parameters.\nFollowing [13], [14], [19], we use K-means to cluster samples\nbased on the selected features. Since K-means depends on\ninitialization, we repeat the clustering 20 times with random\ninitialization and report the average results with standard\ndeviation.\nC. Experimental Results\nWe list the experimental results of different methods in\nTable III and Table IV. From these two tables, we have the\nfollowing observations. First, feature selection is necessary\nand effective. It can reduce the number of features significantly\nand improve the clustering performance. Second, K-means\nobjective criterion can select the most discriminative features\nsuch that data points are well separated. This UFS critierion\nis effective and is different from other spectral analysis based\ncriteria such as NDFS and RUFS. Third, K-means UFS\nachieves the best performance for all the datasets we used.\nThis can be mainly explained by the following reasons. First,\nK-means UFS mainly focus on the discriminative information\ninformation which results in more accurate clustering. Second,\n[22], [23] pointed out that the approximate solution G∗(8) to\nK-means clustering can preserve the structure information of\ndata X. Third, the ℓ2,0-norm constraint in k-means UFS can\nreduce the redundant and noisy features.\nWe also study the sensitiveness of parameters and the\nconvergence of ADMM. There are no adjustable parame-\nters need to be tuned in our K-means UFS model Eq. (8).\nWe tune the initial value of penalty parameter µ0 from\n{10−6, 10−−3, ..., 106} and its update coefficient ρ from\n{1.0001, 1.01, 1.1, 10, 100} in ADMM (Algorithm 1). Due to\nthe space limit, we only report experiments over MicroMass.\nFig. 3 show that when µ0 ≤1 and ρ ≤1.1, our method is not\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n7\nTABLE III: Clustering Results(ACC % ± std) of Different Feature Selection Algorithms\nDatasets\nAll Features\nLaplacian Score\nNDFS\nRUFS\nEUFS\nSF2SOG\nK-means UFS\nMicroMass\n48.1 ± 4.47\n49.2 ± 4.58\n51.1 ± 1.75\n50.6 ± 3.61\n50.4 ± 2.89\n51.6 ± 2.73\n52.3 ± 5.65\nHARS\n44.6 ± 0.70\n46.1 ± 0.29\n46.2 ± 1.15\n47.2 ± 0.21\n46.7 ± 0.16\n47.1 ± 1.32\n47.4 ± 0.17\nFashion\n50.0 ± 1.46\n52.1 ± 1.20\n53.2 ± 3.34\n53.3 ± 1.88\n53.8 ± 2.42\n53.0 ± 1.24\n54.1 ± 1.00\nGina\n48.0 ± 1.58\n46.0 ± 2.38\n48.1 ± 0.14\n49.5 ± 2.42\n46.9 ± 0.67\n49.1 ± 1.25\n50.1 ± 0.42\nFabert\n17.3 ± 0.51\n17.1 ± 0.90\n17.8 ± 0.61\n17.3 ± 0.72\n17.3 ± 0.75\n17.5 ± 1.32\n18.0 ± 0.49\nDilbert\n35.1 ± 0.28\n37.2 ± 0.39\n37.9 ± 0.28\n37.6 ± 0.19\n37.3 ± 0.09\n37.1 ± 2.10\n38.0 ± 0.57\nTABLE IV: Clustering Results(NMI % ± std) of Different Feature Selection Algorithms\nDatasets\nAll Features\nLaplacian Score\nNDFS\nRUFS\nEUFS\nSF2SOG\nK-means UFS\nMicroMass\n58.7 ± 3.53\n59.3 ± 3.84\n61.6 ± 1.05\n61.2 ± 2.75\n61.3 ± 1.92\n61.7 ± 1.11\n62.3 ± 5.59\nHARS\n46.6 ± 0.40\n47.3 ± 0.55\n47.7 ± 0.82\n48.1 ± 0.12\n48.2 ± 0.06\n47.1 ± 1.22\n48.4 ± 0.12\nFashion\n49.7 ± 0.62\n50.3 ± 0.56\n51.2 ± 1.90\n52.1 ± 0.66\n51.9 ± 0.72\n52.2 ± 1.26\n52.5 ± 0.34\nGina\n43.4 ± 1.14\n40.0 ± 1.22\n42.2 ± 0.17\n44.3 ± 1.25\n41.9 ± 0.42\n43.1 ± 1.14\n44.8 ± 0.34\nFabert\n2.9 ± 1.13\n2.7 ± 1.12\n3.4 ± 0.55\n3.0 ± 0.60\n3.3 ± 0.98\n3.5 ± 1.28\n3.6 ± 0.77\nDilbert\n13.6 ± 0.30\n14.6 ± 0.12\n16.9 ± 0.24\n16.4 ± 0.18\n15.7 ± 0.02\n16.1 ± 1.22\n17.5 ± 1.08\n(a) ACC for MicroMass\n(ρ = 1.05)\n(b) NMI for MicroMass\n(ρ = 1.05)\n(c) ACC for MicroMass\n(µ0 = 0.1)\n(d) NMI for MicroMass\n(µ0 = 0.1)\nFig. 3: ACC and NMI of K-means UFS with different initial\nµ0 and ρ on MicroMass data\n(Description: In figure(a)(b), we fix ρ then tune µ0. In figure(c)(d),\nwe fix µ0 then tune ρ.)\nsensitive to µ0 and ρ. If µ0 ≥103 or ρ ≥10, the ADMM will\nconverge within few iterations, which damages the clustering\nperformance. Therefore, we choose µ0 = 0.1 and ρ = 1.05\nin experiments. Fig. 4 is the convergence curves of K-means\nUFS over MicroMass and HARS. Our method will converges\nvery quickly and selects a subset of features with low K-means\nobjective value.\nVI. CONCLUSION\nIn this paper, we propose a novel unsupervised feature\nselection method called K-means UFS. Our method selects the\n(a) MicroMass\n(b) HARS\nFig. 4: Convergence curve of K-means UFS on MicroMass\nand HARS data (µ0 = 0.1, ρ = 1.05, h = 50)\n(Description: The x-axis represents iterations and the y-axis\nrepresents objective value of K-means UFS in ADMM.)\nmost discriminative features by minimizing the objective value\nof K-means. We derive a solvable K-means UFS model with\nℓ2,0-norm using the approximate indicators of K-means and\nnumerical relaxed trick. We also develop an ADMM algorithm\nfor K-means UFS. The K-means objective criterion for UFS\nis totally different from the most widely used spectral analysis\ncriterion. Experiments on real data validate the effectiveness\nof our method.\nREFERENCES\n[1] S. Alelyani, J. Tang, and H. Liu, “Feature selection for clustering: a\nreview.” Data clustering: algorithms and applications, vol. 29, no. 110-\n121, p. 144, 2013.\n[2] S. Khalid, T. Khalil, and S. Nasreen, “A survey of feature selection and\nfeature extraction techniques in machine learning,” in 2014 science and\ninformation conference.\nIEEE, 2014, pp. 372–378.\n[3] P. Langley et al., “Selection of relevant features in machine learning,” in\nProceedings of the AAAI Fall symposium on relevance, vol. 184, 1994,\npp. 245–271.\n[4] R. Kohavi and G. H. John, “Wrappers for feature subset selection,”\nArtificial intelligence, vol. 97, no. 1-2, pp. 273–324, 1997.\n[5] X. He, D. Cai, and P. Niyogi, “Laplacian score for feature selection,”\nAdvances in neural information processing systems, vol. 18, pp. 507–\n514, 2005.\n[6] Z. Zhao and H. Liu, “Spectral feature selection for supervised and unsu-\npervised learning,” in Proceedings of the 24th international conference\non Machine learning, 2007, pp. 1151–1157.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020\n8\n[7] J. G. Dy and C. E. Brodley, “Feature selection for unsupervised\nlearning,” Journal of machine learning research, vol. 5, no. Aug, pp.\n845–889, 2004.\n[8] M. A. Hall and L. A. Smith, “Feature selection for machine learning:\nComparing a correlation-based filter approach to the wrapper.” in\nFLAIRS conference, vol. 1999, 1999, pp. 235–239.\n[9] C. Constantinopoulos, M. K. Titsias, and A. Likas, “Bayesian feature\nand model selection for gaussian mixture models,” IEEE Transactions\non Pattern Analysis and Machine Intelligence, vol. 28, no. 6, pp. 1013–\n1018, 2006.\n[10] S. Das, “Filters, wrappers and a boosting-based hybrid for feature\nselection,” in Icml, vol. 1.\nCiteseer, 2001, pp. 74–81.\n[11] H. Liu and L. Yu, “Toward integrating feature selection algorithms for\nclassification and clustering,” IEEE Transactions on knowledge and data\nengineering, vol. 17, no. 4, pp. 491–502, 2005.\n[12] D. Cai, C. Zhang, and X. He, “Unsupervised feature selection for multi-\ncluster data,” in Proceedings of the 16th ACM SIGKDD international\nconference on Knowledge discovery and data mining, 2010, pp. 333–\n342.\n[13] Z. Li, Y. Yang, J. Liu, X. Zhou, and H. Lu, “Unsupervised feature\nselection using nonnegative spectral analysis,” in Proceedings of the\nAAAI conference on artificial intelligence, vol. 26, no. 1, 2012, pp. 1026–\n1032.\n[14] M. Qian and C. Zhai, “Robust unsupervised feature selection,” in\nTwenty-third international joint conference on artificial intelligence.\nCiteseer, 2013.\n[15] C. Hou, F. Nie, X. Li, D. Yi, and Y. Wu, “Joint embedding learning\nand sparse regression: A framework for unsupervised feature selection,”\nIEEE Transactions on Cybernetics, vol. 44, no. 6, pp. 793–804, 2013.\n[16] S. Li and D. Wei, “Extremely high-dimensional feature selection via fea-\nture generating samplings,” IEEE Transactions on Cybernetics, vol. 44,\nno. 6, pp. 737–747, 2013.\n[17] R. Shang, W. Wang, R. Stolkin, and L. Jiao, “Non-negative spectral\nlearning and sparse regression-based dual-graph regularized feature\nselection,” IEEE transactions on cybernetics, vol. 48, no. 2, pp. 793–\n806, 2017.\n[18] Q. Wang, X. Jiang, M. Chen, and X. Li, “Autoweighted multiview\nfeature selection with graph optimization,” IEEE Transactions on Cy-\nbernetics, 2021.\n[19] S. Wang, J. Tang, and H. Liu, “Embedded unsupervised feature selec-\ntion,” in Proceedings of the AAAI Conference on Artificial Intelligence,\nvol. 29, no. 1, 2015.\n[20] Z. Li, F. Nie, J. Bian, D. Wu, and X. Li, “Sparse pca via l2, p-norm\nregularization for unsupervised feature selection,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2021.\n[21] R. Wang, C. Zhang, J. Bian, Z. Wang, F. Nie, and X. Li, “Sparse and\nflexible projections for unsupervised feature selection,” IEEE Transac-\ntions on Knowledge and Data Engineering, 2022.\n[22] C. Ding and X. He, “K-means clustering via principal component\nanalysis,” in Proceedings of the twenty-first international conference on\nMachine learning, 2004, p. 29.\n[23] H. Zha, X. He, C. Ding, M. Gu, and H. D. Simon, “Spectral relaxation\nfor k-means clustering,” in Advances in neural information processing\nsystems, 2001, pp. 1057–1064.\n[24] S. Boyd, N. Parikh, and E. Chu, Distributed optimization and statistical\nlearning via the alternating direction method of multipliers.\nNow\nPublishers Inc, 2011.\n[25] R. D. King, C. Feng, and A. Sutherland, “Statlog: comparison of clas-\nsification algorithms on large real-world problems,” Applied Artificial\nIntelligence an International Journal, vol. 9, no. 3, pp. 289–333, 1995.\n[26] X. He and P. Niyogi, “Locality preserving projections,” Advances in\nneural information processing systems, vol. 16, 2003.\n[27] M. Zhang, C. Ding, Y. Zhang, and F. Nie, “Feature selection at the\ndiscrete limit,” in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 28, no. 1, 2014.\n[28] I. Guyon, L. Sun-Hosoya, M. Boull´e, H. J. Escalante, S. Escalera, Z. Liu,\nD. Jajetic, B. Ray, M. Saeed, M. Sebag et al., “Analysis of the automl\nchallenge series,” Automated Machine Learning, p. 177, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2024-11-19",
  "updated": "2024-11-19"
}