{
  "id": "http://arxiv.org/abs/2411.15674v1",
  "title": "Quantile deep learning models for multi-step ahead time series prediction",
  "authors": [
    "Jimmy Cheung",
    "Smruthi Rangarajan",
    "Amelia Maddocks",
    "Xizhe Chen",
    "Rohitash Chandra"
  ],
  "abstract": "Uncertainty quantification is crucial in time series prediction, and quantile\nregression offers a valuable mechanism for uncertainty quantification which is\nuseful for extreme value forecasting. Although deep learning models have been\nprominent in multi-step ahead prediction, the development and evaluation of\nquantile deep learning models have been limited. We present a novel quantile\nregression deep learning framework for multi-step time series prediction. In\nthis way, we elevate the capabilities of deep learning models by incorporating\nquantile regression, thus providing a more nuanced understanding of predictive\nvalues. We provide an implementation of prominent deep learning models for\nmulti-step ahead time series prediction and evaluate their performance under\nhigh volatility and extreme conditions. We include multivariate and univariate\nmodelling, strategies and provide a comparison with conventional deep learning\nmodels from the literature. Our models are tested on two cryptocurrencies:\nBitcoin and Ethereum, using daily close-price data and selected benchmark time\nseries datasets. The results show that integrating a quantile loss function\nwith deep learning provides additional predictions for selected quantiles\nwithout a loss in the prediction accuracy when compared to the literature. Our\nquantile model has the ability to handle volatility more effectively and\nprovides additional information for decision-making and uncertainty\nquantification through the use of quantiles when compared to conventional deep\nlearning models.",
  "text": "Quantile deep learning models for multi-step ahead time series prediction\nJimmy Cheunga,∗∗, Smruthi Rangarajana,∗∗, Amelia Maddocksa,∗∗, Xizhe Chena, Rohtiash Chandraa,∗\naTransitional Artificial Intelligence Research Group, School of Mathematics and Statistics, University of New South Wales, Sydney, Australia\nAbstract\nUncertainty quantification is crucial in time series prediction, and quantile regression offers a valuable mechanism for uncertainty\nquantification which is useful for extreme value forecasting. Although deep learning models have been prominent in multi-step\nahead prediction, the development and evaluation of quantile deep learning models have been limited. We present a novel quantile\nregression deep learning framework for multi-step time series prediction. In this way, we elevate the capabilities of deep learning\nmodels by incorporating quantile regression, thus providing a more nuanced understanding of predictive values. We provide an\nimplementation of prominent deep learning models for multi-step ahead time series prediction and evaluate their performance under\nhigh volatility and extreme conditions. We include multivariate and univariate modelling, strategies and provide a comparison with\nconventional deep learning models from the literature. Our models are tested on two cryptocurrencies: Bitcoin and Ethereum,\nusing daily close-price data and selected benchmark time series datasets. The results show that integrating a quantile loss function\nwith deep learning provides additional predictions for selected quantiles without a loss in the prediction accuracy when compared\nto the literature. Our quantile model has the ability to handle volatility more effectively and provides additional information for\ndecision-making and uncertainty quantification through the use of quantiles when compared to conventional deep learning models.\nKeywords: Deep learning,, time series prediction, quantile regression, multivariate modelling, multi-step ahead prediction.\n1. Introduction\nIn the realm of time series forecasting, uncertainty quan-\ntification is a critical component that allows for more in-\nformed decision-making, particularly in fields characterised by\nhigh volatility such as financial markets, energy demand, and\nweather forecasting. Conventional deep learning models, whilst\npowerful in multi-step ahead forecasting, often fall short in\nproviding comprehensive measures of uncertainty. This gap\ncan be addressed by integrating quantile regression, a statis-\ntical technique that offers a mechanism for extreme forecast-\ning by predicting the conditional quantiles of a response vari-\nable. Koenker and Bassett [1] introduced the quantile regres-\nsion model in the mid-1970s to estimate conditional quantiles,\noffering a measure of uncertainty rather than single-point pre-\ndictions as in conventional linear regression models. Quantile\nregression has been widely used in statistical analysis [2] and\nfinds applications in various fields, including epidemiology [3],\neconomics [4, 5], ecology [6], and finance [7]. For instance,\nin the field of economics, it has been employed to study salary\ndistributions influenced by returns to education and student ex-\nperience [8]. In medicine, quantile regression has been used to\nanalyse the effects of different local anesthetics on the duration\nof nerve blocks [9]. Unlike traditional linear models such as\nleast squares regression [10, 11], quantile regression provides\nmore comprehensive information about the conditional distri-\nbution, revealing data characteristics across different quantiles\n∗Corresponding author\n∗∗Joint first authors with equal contribution.\nEmail address: rohitash.chandra@unsw.edu.au (Rohtiash Chandra)\nas well as the average of the data. Hence, this approach offers a\nmethodology for projecting uncertainties in prediction [12, 13].\nExtreme value prediction [14] focuses on forecasting rare\nand significant events, these are often outliers or extreme val-\nues in a dataset and have a low probability of occurrence but\ncan cause major consequences [15]. In a meteorology context,\nan example is the rapid intensification of cyclones [16]. Given\nthe distribution of a dataset, this statistical modeling approach\ntargets the tail of the distribution where extreme events reside,\nallowing for the estimation of their probability. It finds applica-\ntions in various fields, including natural disasters [17, 18, 19],\nfinancial crises [20], and system failures [21]. Extreme value\nprediction is crucial for enhancing risk management as it pro-\nvides a foundation for developing emergency plans [22] and\npreventive measures [23]. For instance, it can be used to assess\npotential casualties in earthquake disasters [19], helping to min-\nimise losses. Additionally, whilst extreme value prediction tar-\ngets the rare, extreme events that in the tails of the distribution,\nquantile regression provides a more generalised approach to es-\ntimate various quantiles, such as the median and 90th percentile\nof the response variable’s conditional distribution [24]. How-\never, classic quantile regression can perform poorly for extreme\nvalues.\nWhen integrated with extreme value theory (EVT),\nextreme quantile regression can estimate conditional quantiles\nthat extend beyond the observed data range [25]. Further lit-\nerature on extreme value prediction using a quantile function\nmodel are detailed by Cai et al. [24].\nDeep learning models can handle complex, high-dimensional\ndata and extract hidden patterns and features, making them par-\nticularly effective for time series prediction, especially with\nPreprint submitted to .\nNovember 26, 2024\narXiv:2411.15674v1  [cs.LG]  24 Nov 2024\nnonlinear and multivariate data [26]. These models have been\nextensively used for time series forecasting, including univari-\nate, multivariate, single-step, and multi-step predictions [27].\nIn the meteorological field, deep learning can be used to predict\nextreme weather events [28] such as smog [29], heavy rainfall\n[30], and declining groundwater levels [31]. By analysing his-\ntorical meteorological data and satellite images, deep learning\nmodels can also identify early signals of extreme weather, en-\nabling advanced preparation to mitigate potential damage [28].\nThe combination of quantile regression with deep learning\nis gaining traction [32, 33]. Deep learning-based quantile re-\ngression has been applied to right-censored survival data [34],\nutilising the Huber check function and inverse probability cen-\nsoring weights (IPCW) function to more accurately adjust for\ncensoring. This has been validated through simulation studies\nand applications to breast cancer gene datasets [34].\nRecent studies have utilised the quantile regression forests\n(QRF) model to predict road traffic volume, showing signifi-\ncant implications for regional development [35]. Furthermore,\nintegrating quantile regression with deep learning models, such\nas the long short-term memory (LSTM) network has signifi-\ncantly improved the accuracy and reliability of river runoff pre-\ndictions [36]. The monotone quantile regression neural net-\nwork (MQRNN) was employed by Hu et al. [37] to address the\nquantile crossing problem in time series prediction by taking\nthe monotonicity of quantile into consideration. An improved\nquantile regression neural network (iQRNN) [38] was used for\nprobabilistic load forecasting that utilised deep learning strate-\ngies such as batch training, early stopping, and dropout regu-\nlarisation that significantly improved the training efficiency and\nprediction stability of the model. Recent advancements includ-\ning MQRNN [39] and the deep partially linear quantile regres-\nsion neural network (DPLQR) model [40] have addressed quan-\ntile crossover, where different quantile estimation lines (e.g.\n10% quantile, 50% quantile) may cross or stagger during the\nprediction process in the quantile regression, leading to in-\nconsistency or irrationality in the prediction results, and con-\nstructing confidence intervals for time series predictions. These\nmodels highlight the potential of combining deep learning with\nquantile regression for enhanced uncertainty quantification.\nThe integration of deep learning and extreme value predic-\ntion has demonstrated significant application potential across\nvarious fields. Deep learning has been leveraged to predict ex-\ntreme market fluctuations, aiding investors anticipate the risk\nof financial crises or market crashes [41]. The combination of\nextreme value theory (EVT) with neural networks has signif-\nicantly improved the accuracy of predicting extreme events in\nfinancial markets [42]. A hybrid model framework that com-\nbines EVT and machine learning [43] can more accurately es-\ntimate stock market risks by processing multivariate and high-\nfrequency data, thereby enhancing risk management and invest-\nment decision-making accuracy. Furthermore, there is limited\nwork in the area of quantile regression for multi-step ahead\nforecasting.\nIn this study, we present a novel quantile regression deep\nlearning framework for multi-step time series prediction. In this\nway, we elevate the capabilities of deep learning models by in-\ncorporating quantile regression, thus providing a more nuanced\nunderstanding of predictive values. We evaluate the framework\nusing univariate and multivariate benchmark datasets and focus\non multi-step ahead time series predictions under conditions of\nhigh volatility and extremes that include cryptocurrency mar-\nket, specifically Bitcoin and Ethereum datasets used by Wu et\nal. [44]. We evaluate the framework with two novel deep learn-\ning models that include LSTM networks [45] and convolutional\nneural networks [46] which have been very promising for multi-\nstep ahead forecasting [47]. We provide open-source Python\ncode and data so that our framework can be extended and ap-\nplied to various fields that feature extreme values and require\nuncertainty quantification.\nThe rest of the paper is organised as follows. In Section 2,\nwe provide background and related work, and in Section 3, we\npresent the methodology. Section 4 presents the results and\nSection 5 and 6 provide the discussion and conclusions, respec-\ntively.\n2. Background and Related Work\n2.1. Quantile regression\nThe quantile regression model is an extension of linear re-\ngression that estimates the conditional median (other quantiles)\nof the response variable using the conditional quantile function.\nFor the τ-th quantile (0 < τ < 1), the quantile model is:\nQy(τ|X) = Xβ(τ)\n(1)\nwhere: Qy(τ|X) represents the τ-th quantile of the dependent\nvariable y given the independent variables X. X is the vector\nof independent variables, containing n observations. β(τ) is the\nvector of coefficients associated with the τ-th quantile. Quan-\ntile regression estimates the conditional distribution of the de-\npendent variables under the different quantiles. It is different\nfrom ordinary least squares regression (OLS) which is based\non the conditional mean of the estimated dependent variable.\nTherefore, quantile regression can provide a more comprehen-\nsive understanding of the data and provide a means of uncer-\ntainty quantification in predictions. Through quantile regres-\nsion, we can obtain a more detailed description of the entire\ndistribution of a given dataset by estimating different quantiles\n(such as the 10th, 50th, and 90th quantiles).\nQuantile regression is less sensitive to outliers because its es-\ntimation is based on minimising the absolute error with a spe-\ncialised loss function, rather than the conventional squared er-\nror loss in linear models. The quantile loss function is given as\nfollows:\nρτ(u) = u(τ −1u<0))\n(2)\nwhere: u = y −Xβ(τ) represents the residuals. ρτ(u) is the\nasymmetric absolute loss function, also known as the ”check\nloss function.” I(·) is the indicator function, which takes the\nvalue 1 if the condition inside is true, otherwise 0.\nA simple linear regression example can be used to visualise\nthe concept of quantiles, where regression lines for different\n2\nFigure 1: The uncertainty range provided in the demand forecast using quantile\nregression includes the forecast mean and the trend of the upper and lower\nquantiles (25% and 75%).\nquantiles are displayed on the same graph as shown in Figure\n1.\nQuantile regression is also know to be applicable for data that\nis heteroskedastic, providing for a more precise estimate. More-\nover, quantile regression can capture the potential non-linear\nand heterogeneous effects of explanatory variables on the de-\npendent variable, allowing for enhanced insights on the given\ndataset [48, 49].\nThe application of quantile regression in time series analysis\nhas garnered widespread attention [50] by expanding modelling\noptions through allowing for modelling of local and quantile-\nspecific dynamics.\nFor example, the weighted Nadaraya-\nWatson (WNW) regression is a novel method to implement\nquantile regression, it effectively estimates conditional quan-\ntiles in time series data [50]. Additionally, quantile regression\ncan be applied to interval forecasting, structural change detec-\ntion, and portfolio construction [51]. Koenker [52] provided a\ncomprehensive review on the development and applications of\nquantile regression over the past forty years which was further\nextended by Tyralis et al. [33].\n2.2. Extreme value theory\nExtreme value theory (EVT) [53, 54] forms the basis of ex-\ntreme value prediction which focuses on modelling the prob-\nability distribution of the tail of the data. It provides a set of\nmethods and distribution models for modeling and analysing\nextreme events.\nThe generalised extreme value distribution\n(GEV) [55] and the generalised Pareto distribution (GPD) [56]\nare two important tools to define and estimate models in EVT.\nThe block maxima (BM) [57] method and the Peak Over\nThreshold (POT) [53] method are two common model parame-\nter estimation methods for extreme value analysis.\nThe application of the two main theorems of extreme value\ntheory includes (i) the main limit theorem of EVT (proved by\nFr´echet [58] in 1927 for the Pareto-type limit distribution and\nby Fisher and Tippet [59] in 1928 for the Weibull and Gumbel\nlimit distributions) leading to the GEV distribution, and (ii) the\nGnedenko–Pickands–Balkema–de Haan theorem [60] leading\nto the GPD distribution. In the case when the extreme value\ndistribution of a random variable meets certain conditions, ex-\nceedance over a high threshold can be approximated by the\nGPD, while the GEV distribution can approximate the block\nmaxima. The GEV distribution is used in the block BM method\nfor analysing extremes [57], while the GPD distribution is used\nin the POT method for threshold exceedances analysis [53].\nComparing the BM method with the POT method [61], POT\nis more suitable for quantile estimation as it can better utilise\nextreme observations with a larger sample size, whilst BM is\nmore suitable for estimating the return level, which refers to the\nthreshold value that is expected to be exceeded once within a\nparticular time period.\nThere are various methods for extreme value prediction,\nincluding parametric [62], non-parametric [63], and semi-\nparametric approaches [64]. Parametric methods assume that\nthe data follows a specified extreme value distribution and\nmakes predictions by estimating parameters. Non-parametric\nmethods use the data directly for prediction and do not rely\non a specific distribution structure. Semi-parametric methods\ncombine the advantages of both approaches, utilising data char-\nacteristics whilst also considering specific distributions.\n2.3. Multi-step time series prediction\nSingle-step prediction refers to a model prediction one step\nahead in time, while multi-step time series is a task that aims to\npredict multiple time steps into the future [65]. This becomes\nincreasingly complex with the number of forecast steps, par-\nticularly with time series data. The strengths and weaknesses\nof different neural network architectures vary significantly for\ntime series prediction [47]. Since time series prediction de-\npends on temporal patterns, it’s essential to carefully select the\noptimal neural network architecture and training method. The\nprediction errors can accumulate over time, especially when\ndealing with chaotic time series datasets. This implies that in\norder to produce precise results, the predictive capabilities and\nhyperparameters of different models need to be considered and\ncustomised to the given dataset. Chandra et al. [47] evalu-\nated a variety of deep learning models, including simple recur-\nrent neural networks (RNN), long short-term memory networks\n(LSTM), bidirectional LSTM networks (BD-LSTM), encoder-\ndecoder LSTM networks (ED-LSTM), and convolutional neu-\nral networks (CNN). These models have then been compared\non their performance on univariate time series datasets. They\nreported that bidirectional LSTM and encoder-decoder LSTM\nnetworks performed the best in terms of their prediction accu-\nracy which highlights the advantages of utilising deep learning\nmodels in handling multi-step ahead time series prediction.\nChang et al. [66] used real-time recurrent learning for train-\ning RNNs for flood forecasting, using an iterative approach for\ntwo-step-ahead forecasts. Additionally, Khedkar et al. [67] in-\ncorporated EVT into deep learning models to address extreme\nflooding issues across Australia’s major catchments.\nThey\nutilised multivariate and multi-step time series prediction and\nreported that quantile-LSTM outperformed the baseline deep\nlearning models while providing uncertainty estimates in hy-\ndrological forecasting.\nIn recent years, deep learning models have shown consid-\nerable potential in predicting cryptocurrency prices, which is\nan area characterised by high volatility and unpredictability.\n3\nBayesian neural networks (BNNs) have been used to deal with\nvolatility [68] and uncertainty quantification in predictions by\ntreating model parameters as probability distributions rather\nthan fixed values [69].\nIn cryptocurrency price prediction,\nBNNs provide a way to quantify the uncertainty of predictions\n[70] which is particularly important in financial applications\nthat require risk assessment. Chandra and He [71] employed\nBNNs to investigate the performance of related multi-step-\nahead forecasting models for stock prices, during the COVID-\n19 pandemic and reported that accurate forecasting was chal-\nlenging due to the high volatility of the stock market. These\nareas are precisely where BNNs may perform well in volatile\nmarkets by providing more reliable uncertainty estimates.\nIn addition to the use of Bayesian neural networks, Wang et\nal. [72] applied machine learning techniques to forecast cryp-\ntocurrency volatility utilising intrinsic features (internal and ex-\nternal determinants). Their findings revealed that LSTM net-\nworks significantly outperformed traditional volatility models\nsuch as Generalised Autoregressive Conditional Heteroskedas-\nticity (GARCH). Wu et al.\n[44] evaluated selected deep\nlearning models, including CNNs, Transformer models, and\nLSTM variants, using datasets from both before and during\nthe COVID-19 pandemic for cryptocurrency price prediction.\nThey emphasised the importance of evaluating models in dif-\nferent scenarios, and identified the convolutional LSTM with a\nmultivariate approach as the most accurate model.\n3. Methodology\n3.1. Deep learning models\nRNNs are neural networks designed to handle sequential data\n[73] which feature recurrent connections in the hidden layer\nto represent temporal data [74]. RNNs have been extensively\nutilised for time series forecasting [47].\nLSTM network [75] is a variant of an RNN that addresses\nthe problem of learning of long-term dependencies by conven-\ntional RNNs such as the Elman RNN [74]. LSTMs are par-\nticularly effective for handling temporal data, since they can\nretain information over longer periods, outperforming conven-\ntional RNNs. LSTM models enhance traditional RNNs by in-\ncorporating memory cells that feature multiple gates to manage\ninformation flow. There are 4 components in a LSTM memory\ncell (unit): the input gate, the forget gate, the output gate and\nthe cell state. The interaction of these gates is the crucial part, in\nupdating the cell state which aids in combating issues related to\nvanishing and exploding gradients [75] faced by conventional\nRNNs [73].\nThe bidirectional long short-term memory (BD-LSTM) is an\nadvanced LSTM model that handles information in both for-\nward and backward directions through two independent hidden\nlayers, as shown in Figure 2. Unlike canonical LSTM models\nthat process information in a single direction, each input se-\nquence is passed through the RNN twice; once in the forward\ndirection and once in reverse [76]. This made them promi-\nnent for language modelling and natural language processing\nFigure 2: Bidirectional-LSTM network showing the flow of information.\n(NLP) tasks [77], and also for multi-step ahead time series fore-\ncasting [47, 44]. The encoder-decoder long short-term mem-\nory (ED-LSTM) model was designed to handle language mod-\nelling tasks [78] which is also effective for time series predic-\ntion due to its ability to capture complex temporal patterns and\ndependencies over long sequences. Although the convolutional\nLSTM (Conv-LSTM) network was initially used for weather\nforecasting problems [79], it is also capable of handling a wide\nrange of time series-related data. Conv-LSTM can effectively\nharness both spatial and temporal dependencies in data by com-\nbining the strengths of CNNs [46] and LSTM networks. This\ncapability makes Conv-LSTM particularly suited for tasks in-\nvolving multivariate time series forecasting, such as predicting\ncryptocurrency prices. Therefore, we used these models for our\nquantile deep learning framework.\n3.2. Quantile deep learning model\nThe key and unique feature of these model implementations\nis the use of the quantile loss function. For each defined model,\nthere will be a ‘classic’ version with a standard loss function\nand another version utilising the quantile loss function. This\napproach allows us to evaluate which set of models performs\nbetter, offering more comprehensive predictions that account\nfor the inherent volatility in cryptocurrency markets.\nThe quantile loss function helps in making predictions that\nare more tailored to specific sections of the dataset. Instead of\npredicting only the average outcome, it allows for the predic-\ntion of a set of defined quantiles. Although the quantile loss\nfunction does not predict an exact value, we assume the median\nvalues for each time step (prediction horizon)to be the predicted\nvalues.\nℓq(y, ˆy) =\n\nq · (y −ˆy)\nif y ≥ˆy\n(q −1) · (y −ˆy)\nif y < ˆy\n(3)\nwhere, y is the true value, ˆy is the predicted value and q is the\nspecific quantile (e.g. q = 0.95). We can interpret that if y ≥ˆy,\nthe actual value is greater than or equal to the predicted value.\nThe loss is given as q times the difference between the true and\npredicted values. Therefore, for higher quantiles, the error is\n4\nhigher when the prediction is less than the actual value. In the\ncase that y < ˆy, the actual value is then less than the predicted\nvalue. In this case, the loss is then found to be (q −1) times the\ndifference between the actual and predicted values.\nApplying the quantile loss function to time series data allows\nfor a broader range of predicted values and enables an overview\nof uncertainties. Instead of predicting a single close price, our\nimplementation will use the quantile loss function that consid-\ners a set of quantiles, of a prediction horizon (step). Figure 3\npresents quantile recurrent neural network (RNN) for one-step\nahead and multi-step ahead prediction using two strategies, i.)\ngrouped percentiles (Panel b) and ii.) vector-based quantiles\n(Panel c).\nx represents the time series data index by time t\nthat is windowed by size d for m step-ahead prediction. Note\nthat the vector-based quantiles have further connections to the\nhidden neurons, which are not explicitly shown. Furthermore,\nthe time-based input and recurrent connections are also not ex-\nplicitly shown in the RNN. Figure 3 highlights the interaction\nbetween quantile loss function at the output layer of a simple\nRNN, which is also applicable to other deep learning models\n(CNN and LSTM models). We use the quantile loss function\ninstead of the mean squared error loss for the output layer. We\nare interested to capture the uncertainty in predictions at the 5th,\n25th, 50th, 75th and 95th percentile, hence use the quantile val-\nues of 0.05, 0.25, 0.5, 0.75 and 0.95. Note that other quantile\nvalues can be defined, as long as it ranges from 0 to 1. After we\nhave defined our quantile values, data is fed into the input layer\nof the respective neural network model and propagated through\nthe hidden layers, and finally to the output neurons. This will\nresult in different output and hidden neuron values during back\npropagations [80]. Depending on the quantile value τ, we as-\nsign a weight to the quantile loss function and the further τ\ndeviates from 0.5, the more bias the loss function producing a\nlower (τ < 0.5) or upper (τ > 0.5) value than the median pre-\ndiction. The number of quantile values determines the length\nof each output neuron. In Figure 3, each output neuron features\nthe predicted values from all the defined quantiles.\nIn the case of the multivariate features, additional neurons in\nthe input layer can be added for each feature based on Figure 3.\n3.3. Framework\nThe framework presented in Figure 4 outlines the key com-\nponents that include data processing and predictions using deep\nlearning models. In Stage 1, we begin by extracting and pro-\ncessing the selected datasets and applying exploratory data\nanalysis. We need to transform the original time series data\ninto sequences that can be used for prediction.\nStage 2 involves preparing the data for model training. In\nthe case of deep learning models, we need to process the data\ndepending on their nature, i.e. univariate and multivariate data\nfor associated models as shown in our framework. This sliding\nwindow technique ensures that the model learns from a variety\nof overlapping sequences, capturing the temporal dependencies\nin the data. These sequences are then normalised and split into\ntraining and testing datasets, as done in previous work in the lit-\nerature [44]. The univariate time series is divided into overlap-\nping windows, each window contains an input sequence vector\n(a) One-step ahead prediction.\n(b) Multi-step ahead prediction using grouped quantiles.\n(c) Multi-step ahead prediction using vector-based quantiles for each\nperdiction horizon.\nFigure 3: Quantile recurrent neural network for one-step ahead and multi-step\nahead prediction using two strategies, i.) grouped percentiles (Panel b) and\nii.) vector-based quantiles (Panel c). Note that the vector-based quantiles have\nfurther connections to the hidden neurons, which is not explicitly shown. In the\ncase of the multivariate features, additional neurons in the input layer can be\nadded for each feature. The time-based input and recurrent connections are also\nnot explicitly suing in the recurrent neural network. x represents the time series\ndata index by time t that is windowed by size d for m step-ahead prediction.\n5\nFigure 4: Framework diagram showing the key stages that include data processing model training and evaluation. We present quartile-based implementation for a\na set of deep learning models including BD-LSTM, Conv-LSTM, and ED-LSTM.\nof a fixed number of consecutive time points (size d = 6) and\nan output sequence vector (size m = 5) for the future predicted\ntime points. In the case of the multivariate strategy, the model\ninput features include (high, low, open, close price and volume)\nto predict the close price for five days (steps). The input fea-\ntures are crucial factors that affect the future close price of the\ngiven cryptocurrency, and the previous high and low prices also\nsupport estimating the quantiles. In the case of univariate mod-\nels, we selected close price, as this was determined to be the\nmost important feature in earlier work [44].\nIn Stage 3, we reviewed the literature to find the most ap-\npropriate deep learning models and selected BD-LSTM, Conv-\nLSTM, and ED-LSTM and defined their hyperparameters from\nprior literature [44], to ensure the efficiency of our model. We\ndefine the deep learning model architectures, such as the input\nsize and output size, as shown in Figure 3.\nIn Stage 4, using the three models, we developed a quantile\nloss function as shown in Figure 3. The most complex part in\nour framework is setting up and training the multivariate multi-\nstep ahead quantile-based deep learning models. In both cases,\nthe multi-step ahead predictions are handled by defining mul-\ntiple output neurons, with each output neuron representing a\ndistinct step-ahead prediction along with each output neuron\npresenting a quantile as shown in Figure 3-Panel (b) employ-\ning the quantile loss function. We create both a standard and\nquantile deep learning model and use the Adam optimiser for\ntraining them.\nIn Stage 5, we provide analysis of the predictions and review\nstrengths and weaknesses of the respective models and train-\ning strategies. We can facilitate a comprehensive comparison\nof model performance using different metrics including root-\nmean-square error (RMSE), mean absolute error (MAE), and\nmean absolute percentage error (MAPE). In our evaluation, we\nspecifically use the RMSE as given below.\nRMSE =\nv\nt\n1\nn\nn\nX\ni=1\n(yi −ˆyi)2\n(4)\nwhere, n is the number of data points (samples), y and ˆy are\nthe actual and predicted values, respectively. In the case of\nmulti-step ahead prediction, we take the mean of the RMSE of\nthe m steps. We report RMSE of the different quantiles, where\nthe ˆy is of a specific quantile (e.g. quantile value of 0.95).\n3.4. Data\nWe demonstrate the effectiveness of quantile deep learn-\ning models using Multivariate and Univariate time series\ndatasets such as cryptocurrency using processed data taken\nfrom [44] (Bitcoin and Ethereum) and Sunspot, Mackey-Glass\nand Lorenz time series from Chandra et al. [47].\n1. Bitcoin is a Multivariate dataset that contains daily entries\nof Bitcoin prices - high, price low, open and close prices -\nalong with trade volume and market capitalisation. There\nare 2991 daily observations, dating from April 2013 to\nJuly 2021.\n2. Ethereum is a Multivariate dataset that contains daily en-\ntries of Ethereum prices - high, price low, open and close\n6\n(a) Bitcoin\n(b) Ethereum\nFigure 5: Cryptocurrency time series reporting daily close prices\nprice - along with trade volume and market capitalisa-\ntion. There are 2160 daily observations dating from Au-\ngust 2015 to July 2021. Both cryptocurrency datasets con-\ntains columns such as serial number, name, symbol and\ndate but we will omit them as we won’t be needing those\nin our models.\n3. Sunspot is a univariate dataset that records monthly ob-\nservations of the sun’s surface dating from 1749 to 2021\nwhere the number of sunspots fluctuates and follows an\napproximate 11-year cycle.\n4. The Mackey-Glass is a univariate dataset that features a\ncontinuous chaotic time series, computed with the follow-\ning delayed differential equation [81]. In this study, we use\nMackey-Glass [82] parameters where a = 0.2, b = 0.1 and\nτ = 10. We have generated 3000 time steps and have set a\nseed for data reproducibility.\n5. The Lorenz equations [83] three-dimensional chaotic time\nseries composed of ordinary differential equations, inher-\nently unpredictable over long periods. We have used the\ndefault values of the Lorenz system, where ρ = 28, σ = 10\nand β = 2.667. We generated 10000 time steps and have\npartitioned the dataset into three univariate time series.\n3.5. Experiment setup\nAfter developing the initial models, we considered several\nfactors for selecting the appropriate hyperparameters for each\nmodel type. Since Bitcoin and Ethereum are highly volatile,\ntraining on continuous data would not adequately prepare the\nmodel for handling such fluctuations. Therefore, we created\nthe training dataset using a split that was randomly selected,\ni.e. 80:20 ratio. The reason for the random train test split is\nto ensure the models account for data across all time periods.\nFor instance, cryptocurrency data is especially volatile during\nthe COVID-19 pandemic period, which falls only in the test\ndataset if the train test split wasn’t implemented. Our goal is\nto ensure that the respective models have the ability to manage\nvolatile data effectively.\nWe kept the models consistent with previous work ([47]) and\nhence used the hyperparameters presented in Table 1. In the re-\nspective deep learning models, we use adaptive moment estima-\ntion (Adam) [84] optimiser for training with a learning rate of\n0.0001. In the case of the cryptocurrency datasets (Bitcoin and\nEthereum), we use 6 as the input window size with 5 outputs (5\nprediction horizons) as done by Wu et al. [44]. In other real-\nworld and simulated time series datasets, the input and output\nwindow sizes were adjusted to allow comparison with related\nwork by Chandra et al. [47], where the input window is fixed at\n5, and the output window at 10 (10 prediction horizons). Fur-\nthermore, the following needs to be taken into account along\nwith information in Table 1.\n• The BD-LSTM model includes both the forward and back-\nward LSTM layer.\n• ED-LSTM includes two LSTM networks with a time dis-\ntributed layer, in the Encoder and Decoder submodels.\n• The Conv-LSTM includes a 1D convolutional layer for the\nunivariate time series and the 2D layer for the multivariate\ntime series. In the convolutional layer, we use 64 filters\nwith a kernel size of 2. It also utilises LSTM network and\na dense layer.\nwe use 64 filters with a kernel size of 2\nWe report the RMSE mean and 95% confidence intervals\nfrom the test dataset based on 30 independent experimental\nruns. We note that a lower RMSE indicates better model per-\nformance and high uncertainty is indicated by a high confidence\ninterval. In the case of our quantile-based deep learning mod-\nels, we calculate the average RMSE across the number of time\nsteps at each quantile (0.05, 0.25, 0.5, 0.75, and 0.95), with the\nmean representing the median value (0.5).\n7\nModel\nStrategy\nInput\nHidden Layers\nOutput\nBD-LSTM\nUnivariate\n[f = 1, d = 6]\n[h2 = 50, h1 = 50]\n5\nMultivariate\n[ f = 6, d = 6]\n[h1 = 50, h2 = 50]\n5\nED-LSTM\nUnivariate\n[f = 1, d = 6]\n[h1 = 100, h2 = 100]\n5\nMultivariate\n[ f = 6, d = 6]\n[h1 = 100, h2 = 100]\n5\nConv-LSTM\nUnivariate\n[f = 1, d = 6]\n[h2 = 20, h1 = 20]\n5\nMultivariate\n[ f = 6, d = 6]\n[h1 = 20, h2 = 20]\n5\nTable 1: Model architecture for univariate/multivariate strategy for the respective deep learning models for the cryptocurrency datasets. We present number of\nneurons in input layer (number of features f and window size d), hidden layers h1, h2, and output layer.\nNote that number of neurons in input layer and output layer varies in the rest of the datasets.\n4. Results\nAs outlined earlier, we develop quantile deep learning mod-\nels for time series prediction including BD-LSTM, ED-LSTM,\nand Conv-LSTM as the base models and their corresponding\nquantile versions (e.g. Quantile BD-LSTM).\n4.1. Cryptocurrency datasets\nTable 2 presents the performance (RMSE) of univariate and\nmultivariate linear regression and deep learning models (BD-\nLSTM, ED-LSTM, Conv-LSTM) for the Bitcoin test dataset.\nWe highlight in bold the best performance for the respective\nprediction horizons. We observe that the quantile linear regres-\nsion accuracy (RMSE) is similar to linear regression (mean and\nprediction horizons given by the steps). This implies that quan-\ntile regression can effectively handle the volatility of cryptocur-\nrency data while providing predictions of the respective quan-\ntiles, which accounts for uncertainty quantification. An inter-\nesting observation can be seen for multivariate strategy, where\nthere is a higher mean RMSE but a more condensed confidence\ninterval for multivariate quantile linear model.\nGreaves et al. [85] demonstrated that neural networks are\nsuperior model classification than linear regression for Bitcoin\nprice prediction; hence, we move on to deep learning models.\nEarlier, Wu et al. [44] showed that the BD-LSTM, ED-LSTM,\nand Conv-LSTM networks provided the best accuracy ranks in\nthe univariate and multivariate strategies for a wider range of\ndeep learning models.\nAcross both univariate and multivariate strategies in Table\n2, the ED-LSTM and quantile ED-LSTM models provide the\nhighest prediction accuracy and consistently the ED-LSTM\nmodels outperform BD-LSTM and Conv-LSTM. Specifically,\nthe Quantile-ED-LSTM model provides the best accuracy for\nall prediction horizons, except step one in the univariate and\nmultivariate strategies. Additionally, in Figure 6 (b), (d), we\ncan observe that ED-LSTM and Quantile-ED–LSTM both pro-\nvide consistent accuracy as the prediction horizon changes, thus\nbeing the most robust and stable model. We can also note that\nBD-LSTM and Conv-LSTM provide similar performance, but\nthe Quantile-BD-LSTM model consistently provides higher ac-\ncuracy than its counterpart (BD-LSTM) across both univariate\nand multivariate strategies. In Table 3, we can see the accuracy\n(mean RMSE) for each quantile, not only do quantile models\noften provide similar predictions, but they also provide further\ninformation (quantiles) for uncertainty quantification.\nIn the Ethereum dataset, Table 5 presents the performance\n(RMSE) of linear regression and deep learning models for the\ntest datasets, with the best performance highlighted in bold. We\nobserve that the Univariate quantile models provide the best ac-\ncuracy (RMSE) and the Multivariate quantile models provides\nthe most robustness, as indicated by consistently low confi-\ndence intervals. Figure A.10 presents a visualisation of predic-\ntions for the respective models and quantiles for the Ethereum\ntime series, where we observe that the quantiles well capture\nthe actual data points. In Figures 7 (c) and (d), the Multivariate\nstrategy shows that consistently the ED-LSTM models outper-\nform BD-LSTM and Conv-LSTM. In contrast to Bitcoin (Fig-\nure 6, the classic ED-LSTM model provides the most accurate\npredictions overall for all time horizons, with the lowest RMSE\nvalue.\nMoreover, in Figure 7 (a) and (b), the univariate strategy\nshows that the ED-LSTM models also consistently outperform\nBD-LSTM and Conv-LSTM in prediction accuracy with the ex-\nception of step one, and Quantile-BD-LSTM provides the best\nprediction accuracy. Furthermore, the Quantile-ED-LSTM is\nthe most robust univariate model for predicting Bitcoin as the\nprediction horizon increases. We note that although BD-LSTM\nand Conv-LSTM present consistent results, as the number of\nprediction days increases, the forecast accuracy gradually de-\ncreases. The Quantile-BD-LSTM and Quantile-Conv-LSTM\nmodels also consistently provide higher accuracy than their\ncounterparts for the Univariate strategy. Finally, Conv-LSTM\nexhibits better performance than the Quantile-Conv-LSTM for\nmultivariate strategies.\nTable 4 outlines multivariate and univariate strategies for the\nEthereum dataset (test dataset mean RMSE across 5 time steps\nat different quantiles) for 30 independent model training runs.\nSince the median quantile is our prediction, it has the lowest\nRMSE compared to any other quantiles. This is logically con-\nsistent, as other quantiles cover more extreme prediction values.\nAdditionally, it has a much smaller confidence interval which\nhighlights that quantile models excel in reducing percentage-\nbased errors, making them particularly effective in dealing with\n8\n(a) Univariate strategy\n(b) Univariate strategy\n(c) Multivariate strategy\n(d) Multivariate strategy\nFigure 6: Bitcoin time series: prediction plots of respective univariate and multivariate strategies (RMSE mean with 95% confidence interval given as error bar).\n9\nLinear regression\nModel\nMean\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nUnivariate\n0.0062 ± 0.0058\n0.0068 ± 0.0105\n0.0076 ± 0.0185\n0.0076 ± 0.0149\n0.0062 ± 0.0110\n0.0029 ± 0.0040\nQuantile Univariate\n0.0059 ± 0.0055*\n0.0067 ± 0.0103\n0.0070 ± 0.0172\n0.0075 ± 0.0152\n0.0058 ± 0.0099\n0.0027 ± 0.0044\nMultivariate\n0.0061 ± 0.0056\n0.0066 ± 0.0101\n0.0079 ± 0.0185\n0.0069 ± 0.0126\n0.0064 ± 0.0123\n0.0027 ± 0.0042\nQuantile Multivariate\n0.0158 ± 0.0021*\n0.0092 ± 0.0011\n0.0129 ± 0.0013\n0.0163 ± 0.0017\n0.0193 ± 0.0017\n0.0215 ± 0.0022\nUnivariate deep learning models\nModel\nMean\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nBD-LSTM\n0.0155 ± 0.0018\n0.0098 ± 0.0009\n0.0130 ± 0.0012\n0.0159 ± 0.0017\n0.0185 ± 0.0018\n0.0204 ± 0.0014\nQuantile BD-LSTM\n0.0153 ± 0.0018*\n0.0096 ± 0.0011\n0.0127 ± 0.0013\n0.0157 ± 0.0018\n0.0182 ± 0.0018\n0.0203 ± 0.0015\nConv-LSTM\n0.0152 ± 0.0018\n0.0093 ± 0.0009\n0.0127 ± 0.0011\n0.0156 ± 0.0012\n0.0182 ± 0.0016\n0.0202 ± 0.0016\nQuantile Conv-LSTM\n0.0153 ± 0.0019*\n0.0092 ± 0.0009\n0.0127 ± 0.0011\n0.0157 ± 0.0012\n0.0183 ± 0.0017\n0.0204 ± 0.0018\nED-LSTM\n0.0108 ± 0.0006\n0.0106 ± 0.0013\n0.0107 ± 0.0012\n0.0110 ± 0.0011\n0.0110 ± 0.0012\n0.0109 ± 0.0016\nQuantile ED-LSTM\n0.0107 ± 0.0006*\n0.0103 ± 0.0012\n0.0105 ± 0.0011\n0.0109 ± 0.0012\n0.0110 ± 0.0012\n0.0107 ± 0.0014\nMultivariate deep learning models\nModel\nMean\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nBD-LSTM\n0.0163 ± 0.0017\n0.0109 ± 0.0013\n0.0139 ± 0.0015\n0.0167 ± 0.0014\n0.0191 ± 0.0016\n0.0209 ± 0.0015\nQuantile BD-LSTM\n0.0159 ± 0.0018*\n0.0102 ± 0.0013\n0.0137 ± 0.0014\n0.0164 ± 0.0016\n0.0187 ± 0.0015\n0.0205 ± 0.0016\nConv-LSTM\n0.0160 ± 0.0017\n0.0107 ± 0.0014\n0.0137 ± 0.0015\n0.0164 ± 0.0015\n0.0186 ± 0.0016\n0.0204 ± 0.0015\nQuantile Conv-LSTM\n0.0165 ± 0.0017*\n0.0113 ± 0.0015\n0.0144 ± 0.0017\n0.0170 ± 0.0014\n0.0191 ± 0.0015\n0.0209 ± 0.0015\nED-LSTM\n0.0113 ± 0.0007\n0.0101 ± 0.0013\n0.0112 ± 0.0012\n0.0118 ± 0.0013\n0.0119 ± 0.0014\n0.0117 ± 0.0018\nQuantile ED-LSTM\n0.0112 ± 0.0005*\n0.0110 ± 0.0009\n0.0110 ± 0.0011\n0.0112 ± 0.0012\n0.0114 ± 0.0012\n0.0113 ± 0.0013\nTable 2: Prediction accuracy on the Bitcoin dataset, reporting accuracy (mean RMSE and± 95% confidence interval) for 30 independent model training runs. *\nmedian across quantile (0.5).\nStrategy\nModel\nQuantile\n0.05\n0.25\n0.5\n0.75\n0.95\nQuantile BD-LSTM\n0.0339 ± 0.0032\n0.0195 ± 0.0022\n0.0158 ± 0.0013\n0.0201 ± 0.0021\n0.0374 ± 0.0039\nUnivariate\nQuantile Conv-LSTM\n0.0307 ± 0.0022\n0.0189 ± 0.0014\n0.0158 ± 0.0011\n0.0190 ± 0.0015\n0.0296 ± 0.0022\nQuantile ED-LSTM\n0.0272 ± 0.0032\n0.0153 ± 0.0019\n0.0134 ± 0.0018\n0.0161 ± 0.0026\n0.0258 ± 0.0037\nQuantile BD-LSTM\n0.0313 ± 0.0033\n0.0198 ± 0.0025\n0.0163 ± 0.0012\n0.0197 ± 0.0019\n0.0314 ± 0.0030\nMultivariate\nQuantile Conv-LSTM\n0.0322 ± 0.0038\n0.0203 ± 0.0029\n0.0169 ± 0.0012\n0.0201 ± 0.0021\n0.0318 ± 0.0033\nQuantile ED-LSTM\n0.0226 ± 0.0028\n0.0133 ± 0.0019\n0.0112 ± 0.0008\n0.0131 ± 0.0014\n0.0206 ± 0.0023\nTable 3: Bitcoin prediction accuracy (mean RMSE across 5 time steps) at different quantiles.\n10\n(a) Univariate strategy\n(b) Univariate strategy\n(c) Multivariate strategy\n(d) Multivariate strategy\nFigure 7: Ethereum performance evaluation of respective Univariate and Multivariate deep learning models for 5 prediction horizons (mean RMSE with 95%\nconfidence interval as error bar).\n11\nLinear regression\nModel\nMean\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nUnivariate\n0.0079 ± 0.0060\n0.0071 ± 0.0093\n0.0070 ± 0.0111\n0.0092 ± 0.0126\n0.0105 ± 0.0187\n0.0055 ± 0.0124\nQuantile Univariate\n0.0075 ± 0.0061*\n0.0068 ± 0.0097\n0.0067 ± 0.0111\n0.0089 ± 0.0130\n0.0103 ± 0.0195\n0.0050 ± 0.0126\nMultivariate\n0.0090 ± 0.0077\n0.0071 ± 0.0098\n0.0090 ± 0.0149\n0.0102 ± 0.0126\n0.0112 ± 0.0214\n0.0074 ± 0.0228\nQuantile Multivariate\n0.0201 ± 0.0030*\n0.0115 ± 0.0025\n0.0162 ± 0.0027\n0.0213 ± 0.0042\n0.0242 ± 0.0039\n0.0272 ± 0.0052\nUnivariate deep learning models\nModel\nMean\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nBD-LSTM\n0.0178 ± 0.0022\n0.0115 ± 0.0018\n0.0149 ± 0.0021\n0.0184 ± 0.0029\n0.0206 ± 0.0028\n0.0235 ± 0.0037\nQuantile BD-LSTM\n0.0176 ± 0.0023*\n0.0112 ± 0.0018\n0.0146 ± 0.0021\n0.0182 ± 0.0030\n0.0205 ± 0.0029\n0.0233 ± 0.0038\nConv-LSTM\n0.0187 ± 0.0023\n0.0123 ± 0.0024\n0.0154 ± 0.0022\n0.0193 ± 0.0029\n0.0221 ± 0.0030\n0.0244 ± 0.0033\nQuantile Conv-LSTM\n0.0186 ± 0.0022*\n0.0124 ± 0.0024\n0.0154 ± 0.0023\n0.0192 ± 0.0029\n0.0218 ± 0.0027\n0.0241 ± 0.0031\nED-LSTM\n0.0136 ± 0.0010\n0.0135 ± 0.0024\n0.0136 ± 0.0019\n0.0136 ± 0.0026\n0.0136 ± 0.0018\n0.0138 ± 0.0022\nQuantile ED-LSTM\n0.0133 ± 0.0011*\n0.0133 ± 0.0025\n0.0133 ± 0.0021\n0.0134 ± 0.0027\n0.0132 ± 0.0021\n0.0134 ± 0.0027\nMultivariate deep learning models\nModel\nMean\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nBD-LSTM\n0.0200 ± 0.0024\n0.0134 ± 0.0024\n0.0173 ± 0.0024\n0.0206 ± 0.0039\n0.0229 ± 0.0034\n0.0256 ± 0.0044\nQuantile BD-LSTM\n0.0197 ± 0.0025*\n0.0128 ± 0.0025\n0.0171 ± 0.0024\n0.0204 ± 0.0038\n0.0228 ± 0.0035\n0.0252 ± 0.0042\nConv-LSTM\n0.0199 ± 0.0024\n0.0134 ± 0.0023\n0.0174 ± 0.0025\n0.0207 ± 0.0036\n0.0228 ± 0.0035\n0.0253 ± 0.0041\nQuantile Conv-LSTM\n0.0208 ± 0.0024*\n0.0145 ± 0.0024\n0.0188 ± 0.0031\n0.0214 ± 0.0040\n0.0236 ± 0.0036\n0.0259 ± 0.0041\nED-LSTM\n0.0113 ± 0.0007\n0.0101 ± 0.0013\n0.0112 ± 0.0012\n0.0118 ± 0.0013\n0.0119 ± 0.0014\n0.0117 ± 0.0018\nQuantile ED-LSTM\n0.0126 ± 0.0008*\n0.0121 ± 0.0020\n0.0121 ± 0.0020\n0.0119 ± 0.0014\n0.0132 ± 0.0016\n0.0135 ± 0.0018\nTable 4: Results for the Ethereum dataset reporting accuracy (mean RMSE and ± 95% confidence interval) for 30 experimental runs for each model. * represents\nthe median across quantiles (0.5).\nStrategy\nModel\nQuantile\n0.05\n0.25\n0.5\n0.75\n0.95\nQuantile BD-LSTM\n0.0372 ± 0.0039\n0.0219 ± 0.0025\n0.0181 ± 0.0024\n0.0221 ± 0.0035\n0.0390 ± 0.0045\nUnivariate\nQuantile Conv-LSTM\n0.0368 ± 0.0052\n0.0226 ± 0.0034\n0.0191 ± 0.0023\n0.0222 ± 0.0028\n0.0353 ± 0.0040\nQuantile ED-LSTM\n0.0272 ± 0.0032\n0.0153 ± 0.0019\n0.0134 ± 0.0018\n0.0161 ± 0.0026\n0.0258 ± 0.0037\nQuantile BD-LSTM\n0.0385 ± 0.0042\n0.0228 ± 0.0025\n0.0202 ± 0.0030\n0.0249 ± 0.0053\n0.0383 ± 0.0056\nMultivariate\nQuantile Conv-LSTM\n0.0392 ± 0.0041\n0.0241 ± 0.0025\n0.0212 ± 0.0031\n0.0267 ± 0.0057\n0.0405 ± 0.0063\nQuantile ED-LSTM\n0.0265 ± 0.0031\n0.0155 ± 0.0020\n0.0137 ± 0.0017\n0.0168 ± 0.0029\n0.0270 ± 0.0039\nTable 5: Performance evaluation of Multivariate and Univariate strategies for the Ethereum dataset (test dataset mean RMSE across 5-time steps at different\nquantiles) for 30 independent model training runs.\n12\nprice fluctuations and the inherent volatility of cryptocurrency\nmarkets. In Figure A.10, we present selected predictions for the\ngiven quantiles where we observe that in Figure A.10 (c), the\nQuantile-Conv-LSTM model performs accurately.\nAlthough\nConv-LSTM model failed to capture the actual values, its quan-\ntile counterpart improved the model’s ability to capture the true\nvalues.\n4.2. Benchmark datasets\nNext, we evaluate our framework for the benchmark datasets\nthat include Sunspots, Mackey-Glass and Lorenz. Chandra et\nal. [47] demonstrated that BD-LSTM and ED-LSTM provided\nthe best accuracy ranks in the evaluation of selected univari-\nate deep learning models. Figure 8 presents 10-step prediction\nhorizons of the BD-LSTM, and ED-LSTM models along with\ntheir quantile variants. In Figures 8 (a)-(c), we observe that\nthe BD-LSTM is the least robust and has the largest error mar-\ngins (95% confidence intervals). Additionally, the ED-LSTM\nand Quantile-ED-LSTM provide a consistent level of accuracy\nacross the prediction horizon, whereas BD-LSTM models are\nless accurate as time steps increase.\nTable 6 presents the prediction accuracy for the ED-LSTM\nand Quantile-ED-LSTM models, both models provided very\nsimilar results. In the Sunspot and Lorenz datasets, ED-LSTM\nmodels provide the best mean RMSE accuracy and quan-\ntile ED-LSTM models exhibit the best performance for the\nMackey-Glass time series. We note that there is a distinct dif-\nference between ED-LSTM models and the rest of the field,\nclearly demonstrating that it is the favourable model across var-\nious time series datasets. Note that our goal for the Quantile-\nED-LSTM is to achieve a similar level of performance to ED-\nLSTM while providing predictions for the different quantiles as\nprovided.\nWe provide prediction visualisations for all datasets in Figure\nA.9, A.10 and A.11. All models and their prediction prowess\nwere showcased, often times falling into the quantile range.\nNote that models tend to perform well for large consistent\nprices. For low values outputs such as sunspots, mackey-glass\nand lorenz dataset, we noticed that quantile prediction often de-\nviates from actual values.\n5. Discussion\nThis study explored quantile deep learning models for mul-\ntivariate and multi-step ahead time series prediction with uni-\nvariate and multivariate models for selected cryptocurrency\nand time series prediction datasets. Table 7 presents a sum-\nmary of the results, highlighting that both the conventional ED-\nLSTM and Quantile-ED-LSTM models consistently deliver the\nstrongest predictive performance across all datasets. We can\nalso observe that for the cryptocurrency datasets, neither the\nBD-LSTM nor the Conv-LSTM (including their quantile vari-\nants) show a clear performance hierarchy. This aligns with the\nRMSE results outlined in Tables 2 and 4, where all four models\nexhibit competitive accuracy (close performance). Addition-\nally, the primary goal of this study is to enhance the represen-\ntation of uncertainty with the quantile loss function, rather than\nto improve the forecast accuracy of the conventional models.\nTherefore, we expected a similar performance of quantile mod-\nels relative to conventional deep learning models, and has been\ndemonstrated across all datasets in Figures 6, 7 and 8.\nIn both univariate and multivariate cases, the quantile mod-\nels performed similar to the conventional deep learning mod-\nels. In the case of Ethereum, the performance for the quan-\ntile models are slightly poorer than conventional models (Table\n4). However, in the Bitcoin dataset (Table 2), the predictions\nremained similar to the univariate datasets, demonstrating that\ndeep learning models have different predictive abilities depend-\ning on the dataset. We can gather that the model with the most\nconsistent and accurate predictions is the Quantile-ED-LSTM\nmodel. Furthermore, we can review the results of the other\ndatasets. The best models in the cryptocurrency datasets do not\nautomatically imply they are the best across all other datasets.\nWe ran similar experiments on three other volatile datasets (i.e.\nsunspots, mackey-glass and lorenz). The BD-LSTM standard\nmodel proved to be particularly unreliable, with very large con-\nfidence intervals and RMSE values (Figure 8). The BD-LSTM\nquantile model grew in RMSE value across the time steps but\nhas a small confidence interval, demonstrating the model’s ro-\nbustness. Consistent with the cryptocurrency results, the ED-\nLSTM models outperformed the BD-LSTM models as seen in\nTable 6, where the ED-LSTM models have a higher mean rank\nthan the BD-LSTM models across all datasets.\nWe next review the multivariate results for the cryptocur-\nrency datasets (Tables 2 and 4) where the distinguishing fea-\ntures between each model and their quantile counterparts have\nmore clarity and definition. The multivariate results remained\nconsistent with the univariate parts, where the ED-LSTM out-\nperforms both the Conv-LSTM and BD-LSTM models, we can\nsee the contrast between the three models in Figures 6 and\n7. Since ED-LSTM models process the entirety of the given\nhistorical data through the encoder and then make predictions\nthrough the decoder, they are able to take into account the en-\ntire history of the dataset and are hence able to make more valid\npredictions. BD-LSTM model also performs poorly in compar-\nison to the ED-LSTM models as seen in Table 7, as they are\nnot as capable in handling long term dependencies in the data,\nwhich is a key feature in volatile datasets.\nOur findings have better results than those reported in the\nrelated study [44], as their test mean for the ED-LSTM multi-\nvariate Bitcoin data was 0.0373 compared to our quantile ED-\nLSTM result of 0.0112 as seen in Table 2. However, do note\nthat the train test split ratio are different across the two papers\nwhere we used 80:20 in comparison to 70:30 by Wu et al [44].\nThe lower mean RMSE from our ED-LSTM quantile regression\nanalysis particularly emphasises that the median (0.5 quantile)\nresults yield superior predictive accuracy compared to tradi-\ntional methods. This enhancement in performance underscores\nthe robustness of quantile regression for time series forecast-\ning. Furthermore, quantile regression not only improves pre-\ndiction accuracy but also offers a probabilistic interpretation by\nproviding a spectrum of potential outcomes, thereby enriching\nthe decision-making process with a more comprehensive risk\nassessment. In the literature, conventional deep learning mod-\n13\n(a) Sunspot\n(b) Mackey-Glass 10 step-ahead prediction\n(c) Lorenz 10 step-ahead prediction\nFigure 8: Performance evaluation of respective Univariate deep learning models, showing 10 step-ahead prediction horizons for benchmark datasets for 30 indepen-\ndent model training runs (mean RMSE with 95% confidence interval as error bar).\nSunspot\nModel\nMean\nStep 2\nStep 5\nStep 8\nStep 10\nBD-LSTM\n0.0712 ± 0.0139\n0.0858 ± 0.0499\n0.0736 ± 0.0389\n0.0664 ± 0.0319\n0.0640 ± 0.0405\nQuantile BD-LSTM\n0.0892 ± 0.0045*\n0.0735 ± 0.0029\n0.0871 ± 0.0027\n0.1003 ± 0.0030\n0.1111 ± 0.0045\nED-LSTM\n0.0630 ± 0.0008\n0.0628 ± 0.0027\n0.0629 ± 0.0025\n0.0627 ± 0.0026\n0.0633 ± 0.0029\nQuantile ED-LSTM\n0.0630 ± 0.0024*\n0.0629 ± 0.0028\n0.0630 ± 0.0026\n0.0626 ± 0.0026\n0.0633 ± 0.0029\nMackey-Glass\nModel\nMean\nStep 2\nStep 5\nStep 8\nStep 10\nBD-LSTM\n0.0725 ± 0.0145\n0.0591 ± 0.0362\n0.0592 ± 0.0385\n0.0732 ± 0.0492\n0.0701 ± 0.0495\nQuantile BD-LSTM\n0.0807 ± 0.0119*\n0.0311 ± 0.0024\n0.0800 ± 0.0037\n0.1161 ± 0.0046\n0.1269 ± 0.0055\nED-LSTM\n0.0071 ± 0.0011\n0.0101 ± 0.0023\n0.0060 ± 0.0011\n0.0048 ± 0.0014\n0.0049 ± 0.0012\nQuantile ED-LSTM\n0.0071 ± 0.0010*\n0.0100 ± 0.0021\n0.0059 ± 0.0008\n0.0049 ± 0.0012\n0.0050 ± 0.0012\nLorenz\nModel\nMean\nStep 2\nStep 5\nStep 8\nStep 10\nBD-LSTM\n0.0097 ± 0.0017\n0.0072 ± 0.0053\n0.0111 ± 0.0097\n0.0097 ± 0.0086\n0.0076 ± 0.0055\nQuantile BD-LSTM\n0.0140 ± 0.0013*\n0.0044 ± 0.009\n0.0104 ± 0.0013\n0.0177 ± 0.0018\n0.0239 ± 0.0022\nED-LSTM\n0.0015 ± 0.0004\n0.0022 ± 0.0007\n0.0014 ± 0.0006\n0.0009 ± 0.0004\n0.0010 ± 0.0004\nQuantile ED-LSTM\n0.0021 ± 0.0004*\n0.0028 ± 0.0005\n0.0020 ± 0.0007\n0.0013 ± 0.0004\n0.0013 ± 0.0004\nTable 6: Benchmark time series datasets reporting model test accuracy (mean RMSE and ± 95% confidence interval for 30 experimental runs) for univariate deep\nlearning models. * represents the median prediction quantile (0.5).\n14\nData\nStrategy\nBD-LSTM\nQuantile\nBD-LSTM\nConv-LSTM\nQuantile\nConv-LSTM\nED-LSTM\nQuantile\nED-LSTM\nBitcoin\nUnivariate\n6\n5\n3\n4\n1\n2\nEthereum\n4\n3\n6\n5\n2\n1\nMean Rank\n5\n4\n4.5\n4.5\n1.5\n1.5\nBitcoin\nMultivariate\n5\n3\n4\n6\n2\n1\nEthereum\n5\n3\n4\n6\n2\n1\nMean Rank\n5\n3\n4\n6\n2\n1\nSunspot\nUnivariate\n3\n4\n-\n-\n1\n2\nMackey-Glass\n3\n4\n-\n-\n2\n1\nLorenz\n3\n4\n-\n-\n1\n2\nMean Rank\n3\n4\n-\n-\n1.33\n1.67\nTable 7: Performance (rank) of different models for respective time-series problems. Note lower rank denotes better performance.\nels have been evaluated for multi-step ahead time series pre-\ndiction on Sunspots, Mackey-Glass and Lorenz system [47];\nwhere, BD-LSTM had the best accuracy for univariate time se-\nries data. Furthermore, Chandra et. al [47]; reported a common\ntrend where the predictive accuracy decreases across higher\nsteps-ahead prediction. Note that in their results, in the case\nof Mackey-Glass, ED-LSTM was the best-performing model\nand for the Lorenz system, both ED-LSTM and LSTM outper-\nformed BD-LSTM. This is in line with the observations in our\nstudy, where ED-LSTM is the best performing model.\nWe recall that the process of extreme value forecasting (EVF)\n[86, 87] does not directly calculate extreme values [88], and\ninstead calculates uncertainty bounds [89], which in turn, im-\nplicitly addresses extreme values. Our quantile deep learning\nmodels for multi-step ahead prediction serve as an example of\nEVF in action, as the quantile nature of the model indicates un-\ncertainty.\nIn terms of the limitations, there is room for improving\nthe hyperparameter tuning [90], including adjusting the model\ntopology given by the number of hidden layers, and neurons.\nWe had no indication on how accurate our quantile predictions\nare. In our paper, we could only mention that the quantile pre-\ndictions have a higher RMSE value than its median, suggest-\ning them covering the upper and lower range of the prediction.\nHowever, there is no set values for us to compare our quantile\nprediction. Furthermore, to enhance model accuracy, regulari-\nsation techniques such as Lasso [91] and ridge regression [92]\ncan be incorporated into our framework. Dropout-based reg-\nularisation [93] has been prominent in deep learning, and this\ncan also be incorporated in the respective model architectures.\nExtensive evaluation using different training-test split ratios,\ncross-validation, and different input dimension window can be\nconsidered.\nFinally, model outputs and feature variable as-\nsumptions need to be clearly defined and constrained to ensure\nquantile predictions adequately cover the expected range. Many\nvariables such as price and volume cannot be negative. The out-\nput values of other datasets such as sunspots and mackey-glass\nalso has to be positive. Therefore, we should have specified our\nmodel predictions to always be greater than 0.\nThere are several strategies that can be taken to enhance our\nquantile deep learning framework further. The quantile deep\nlearning model for predicting cryptocurrency can be further en-\nhanced using multimodal [94] framework that considers text\ndata such as data from news media and twitter about cryptocur-\nrency markets. Sentiment analysis using natural language pro-\ncessing and large language models [95] can be useful in pro-\nviding further information for the deep learning models. Senti-\nment analysis in combination with a quantile deep learning can\nbe very useful in improving future predictions that features the\nquantiles for robust uncertainty quantification.\nThere are other approaches to uncertainty quantification,\nsuch as Bayesian inference [96], where posterior distribution\nis obtained by prior distribution and likelihood function [97].\nIn particular, uncertainty bound can be calculated by sam-\npling from the posterior distribution using Markov Chain Monte\nCarlo (MCMC) [98]. There have been efforts in developing\nBayesian neural networks and Bayesian deep learning mod-\nels with MCMC [99] and variational-Bayes sampling strate-\ngies [100]. In the field of finance, there exists literature which\nperforms multi-step ahead price forecasting using the Bayesian\napproach [71]. Although our study provides a frequentest ap-\nproach [101] to uncertainty quantification using quantile regres-\nsion in deep learning, our framework can be extended using\nBayesian deep learning. In doing so, we would be sampling\nfrom the posterior distribution (model weights and biases) us-\ning MCMC or variational Bayes and computing additional un-\ncertainties projected. Although this would be an overkill in con-\nventional problems, it would be useful in problems where risk\nanalysis is vital, such as medical diagnosis. Furthermore, the\nquantile deep learning model can be utilised for time series data\nimputation tasks [102], where uncertainties obtained from the\ndifferent quantiles can be useful in producing different versions\nof impute datasets.\n6. Conclusion\nIn this study, we investigated the combination of quantile re-\ngression in selected deep learning models for multi-step ahead\ntime series prediction. Our results demonstrated that the com-\nbining quantile regression with deep learning models has been\nvery effective, even in volatile environments such as the cryp-\ntocurrency markets. In the case of the cryptocurrency datasets,\n15\nthe quantile ED-LSTM model has consistently outperformed\ntraditional methods, highlighting their ability to effectively han-\ndle forecast uncertainty and volatility. Although our current\nmodels have shown strong predictive capabilities, it is still pos-\nsible to improve them further through further hyperparameter\ntuning and incorporating novel architectural and training strate-\ngies.\nThe overall objective was to not really outperform the exist-\ning versions of the models, but rather to see if we can provide\nmore clarity and information with the quantile loss function.\nNot only do our quantile models provide exceptional accuracy,\nbut also demonstrate remarkable stability and robustness across\nvarious prediction horizons. However, even though our results\ndo show that combining the quantile loss function with deep\nlearning does occasionally provide slightly more accurate pre-\ndictions. The quantile models offer a more reliable prediction\nby embracing the inherent uncertainties, providing more infor-\nmation as well as being less sensitive to outliers. This allows\nus to be more certain within our predictions, making them in-\nvaluable tools for navigating datasets that are unpredictable and\nvolatile in nature.\nOur study shows that quantile deep learning models not only\nimprove forecast accuracy but also provide an indication of\nuncertainty, which is useful in risk assessment and decision-\nmaking processes. This process is critical in dealing with high\nvolatility and extreme data in risk-sensitive environments and\nhas potential for modelling climate extreme events. Future re-\nsearch can explore model optimisation and applications to other\nforecasting problems and regression tasks.\nData and Code Availability\nWe provide Python code and the data used in our models\nusing the GitHub repository 1.\nAcknowledgements\nWe thank Arpit Kapoor from UNSW Sydney for earlier dis-\ncussions.\nReferences\n[1] R. Koenker, G. Bassett Jr, Regression quantiles, Econometrica: journal\nof the Econometric Society (1978) 33–50.\nURL https://www.jstor.org/stable/1913643\n[2] E. Waldmann, Quantile regression: A short story on how and why, Sta-\ntistical Modelling 18 (3-4) (2018) 203–218.\nURL https://doi.org/10.1177/1471082X18759142\n[3] Y. Wei, R. D. Kehm, M. e. a. Goldberg, Applications for quantile regres-\nsion in epidemiology, Current Epidemiology Reports 6 (2019) 191–199.\n[4] B. Fitzenberger, R. Koenker, J. A. Machado, Economic applications of\nquantile regression, Springer Science & Business Media, 2013.\n[5] B. e. a. Tillaguango, Impact of oil price, economic globalization, and in-\nflation on economic output: Evidence from latin american oil-producing\ncountries using the quantile-on-quantile approach, Energy (2024).\nURL\nhttps://www.sciencedirect.com/science/article/\npii/S0360544224015597\n1https://github.com/sydney-machine-learning/\nquantiledeeplearning\n[6] L. Briollais, G. Durrieu, Application of quantile regression to recent ge-\nnetic and-omic studies, Human genetics 133 (8) (2014) 951–966.\n[7] D. E. Allen, P. Gerrans, R. e. a. Powell, Quantile regression: its applica-\ntion in investment analysis, Jassa 4 (2009) 7–12.\n[8] M. Buchinsky, Changes in the us wage structure 1963-1987: Applica-\ntion of quantile regression, Econometrica: Journal of the Econometric\nSociety (1994) 405–458.\n[9] S. J. Staffa, D. S. Kohane, D. Zurakowski, Quantile regression and its\napplications: a primer for anesthesiologists, Anesthesia & Analgesia\n128 (4) (2019) 820–830.\n[10] G. S. Watson, Linear least squares regression, The Annals of Mathemat-\nical Statistics (1967) 1679–1699.\n[11] P. Geladi, B. R. Kowalski, Partial least-squares regression: a tutorial,\nAnalytica chimica acta 185 (1986) 1–17.\n[12] K. Vaysse, P. Lagacherie, Using quantile regression forest to estimate\nuncertainty of digital soil mapping products, Geoderma 291 (2017) 55–\n64.\n[13] N. Dogulu, P. L´opez L´opez, D. P. Solomatine, A. H. Weerts, D. L.\nShrestha, Estimation of predictive hydrologic uncertainty using the\nquantile regression and uneec methods and their comparison on con-\ntrasting catchments, Hydrology and Earth System Sciences 19 (7) (2015)\n3181–3201.\n[14] M. K. Ochi, On prediction of extreme values, Journal of Ship Research\n17 (01) (1973) 29–37.\n[15] R. P. Ribeiro, N. Moniz, Imbalanced regression and extreme value pre-\ndiction, Machine Learning 109 (2020) 1803–1835.\n[16] B. Wang, X. Zhou, Climate variation and prediction of rapid intensifica-\ntion in tropical cyclones in the western north pacific, Meteorology and\nAtmospheric Physics 99 (1) (2008) 1–16.\n[17] L. Makkonen, Problems in the extreme value analysis, Structural safety\n30 (5) (2008) 405–419.\n[18] J. H. Lee, H. Kim, H. J. e. a. Park, Temporal prediction modeling for\nrainfall-induced shallow landslide hazards using extreme value distribu-\ntion, Landslides 18 (2021) 321–338.\n[19] H. Xing, S. Junyi, H. Jin, The casualty prediction of earthquake disaster\nbased on extreme learning machine method, Natural Hazards 102 (3)\n(2020) 873–886.\n[20] X. Zhao, C. Scarrott, L. e. a. Oxley, Extreme value modelling for fore-\ncasting market crisis impacts, Routledge, 2014.\n[21] C. Wang, L. Zhang, G. Tao, Quantifying the influence of corrosion de-\nfects on the failure prediction of natural gas pipelines using generalized\nextreme value distribution (gevd) model and copula function with a case\nstudy, Emergency Management Science and Technology 4 (1) (2024).\n[22] P. Cumperayot, R. Kouwenberg, Early warning systems for currency\ncrises: A multivariate extreme value approach, Journal of international\nmoney and finance 36 (2013) 151–171.\n[23] T. B. Messervey, D. M. Frangopol, S. Casciati, Application of the statis-\ntics of extremes to the reliability assessment and performance prediction\nof monitored highway bridges, in: Structures and Infrastructure Sys-\ntems, Routledge, 2019, pp. 287–299.\n[24] Y. Cai, D. Reeve, Extreme value prediction via a quantile function\nmodel, Coastal Engineering 77 (2013).\n[25] J. Velthoen, C. Dombry, J. e. a. Cai, Gradient boosting for extreme quan-\ntile regression, Extremes 77 (2023) 639–667.\n[26] B. Lim, S. Zohren, Time-series forecasting with deep learning: a survey,\nPhilosophical Transactions of the Royal Society A 379 (2194) (2021)\n20200209.\n[27] J. F. Torres, D. Hadjout, A. Sebaa, F. Mart´ınez- ´Alvarez, A. Troncoso,\nDeep learning for time series forecasting: a survey, Big Data 9 (1) (2021)\n3–21.\n[28] W. Fang, Q. Xue, L. e. a. Shen, Survey on the application of deep learn-\ning in extreme weather prediction, Atmosphere 12 (6) (2021) 661.\n[29] Z. Chen, H. Yu, Y. e. a. Geng, Evanet: An extreme value attention net-\nwork for long-term air quality prediction, in: 2020 IEEE International\nConference on Big Data (Big Data), IEEE, 2020, pp. 4545–4552.\n[30] S. Gope, S. Sarkar, P. e. a. Mitra, Early prediction of extreme rainfall\nevents: a deep learning approach, in: Advances in Data Mining. Ap-\nplications and Theoretical Aspects: 16th Industrial Conference, ICDM\n2016, New York, NY, USA, July 13-17, 2016. Proceedings 16, Springer\nInternational Publishing, 2016, pp. 154–167.\n[31] E. Chen, M. S. Andersen, R. Chandra, Deep learning framework with\n16\nbayesian data imputation for modelling and forecasting groundwater lev-\nels, Environmental Modelling & Software 178 (2024) 106072.\n[32] G. e. a. Papacharalampous, Uncertainty estimation in spatial interpola-\ntion of satellite precipitation with ensemble learning, https://arxiv.\norg/abs/2403.10567 (2024).\n[33] H. Tyralis, G. Papacharalampous, A review of predictive uncertainty\nestimation with machine learning, Artificial Intelligence Review 57 (4)\n(2024) 94.\nURL\nhttps://link.springer.com/article/10.1007/\ns10462-023-10698-8#Abs1\n[34] Y. Jia, J. H. Jeong, Deep learning for quantile regression under right\ncensoring: Deepquantreg, Computational Statistics & Data Analysis 165\n(2022) 107323.\n[35] M. J. Van Strien, A. Grˆet-Regamey, A global time series of traffic\nvolumes on extra-urban roads, Scientific data 11 (1) (2024) 470.\nURL\nhttps://www.nature.com/articles/\ns41597-024-03287-z\n[36] S. Zhu, M. Zhang, C. e. a. Wang, A probabilistic runoff prediction model\nbased on improved long short-term memory and interval correction,\nJournal of Hydrologic Engineering 29 (4) (2024) 04024018.\n[37] J. Hu, J. Tang, Z. Liu, A novel time series probabilistic prediction\napproach based on the monotone quantile regression neural network,\nInformation Sciences 654 (2024) 119844.\nURL\nhttps://www.sciencedirect.com/science/article/\npii/S0020025523014299\n[38] W. Zhang, H. Quan, D. Srinivasan, An improved quantile regression\nneural network for probabilistic load forecasting, IEEE Transactions on\nSmart Grid 10 (4) (2018) 4425–4434.\n[39] J. Hu, J. Tang, Z. Liu, A novel time series probabilistic prediction ap-\nproach based on the monotone quantile regression neural network, In-\nformation Sciences 119844 (2023).\n[40] J. Tang, et al., Neural networks for partially linear quantile regression,\nJournal of Business & Economic Statistics (2023).\nURL\nhttps://www.tandfonline.com/doi/abs/10.1080/\n07350015.2023.2208183\n[41] M. et al., A conceptual model of investment-risk prediction in the stock\nmarket using extreme value theory with machine learning: a semisys-\ntematic literature review, Risks 11 (3) (2023) 60.\nURL https://www.mdpi.com/2227-9091/11/3/60\n[42] A.-A. e. a. Ibn Musah, The asymptotic decision scenarios of an emerg-\ning stock exchange market: Extreme value theory and artificial neural\nnetwork, Risks 6 (4) (2018) 132.\nURL https://www.mdpi.com/2227-9091/6/4/132\n[43] M. e. a. Melina, Modeling of machine learning-based extreme value\ntheory in stock investment risk prediction:\nA systematic literature\nreview, Big Data (2024).\nURL\nhttps://www.liebertpub.com/doi/abs/10.1089/big.\n2023.0004\n[44] J. Wu, X. Zhang, F. Huang, H. Zhou, R. Chandra, Review of deep learn-\ning models for crypto price prediction: implementation and evaluation,\narXiv (2024).\nURL https://arxiv.org/abs/2405.11431\n[45] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural com-\nputation 9 (8) (1997) 1735–1780.\n[46] L. Alzubaidi, J. Zhang, A. J. Humaidi, et al., Review of deep learning:\nconcepts, cnn architectures, challenges, applications, future directions,\nJournal of Big Data 8 (2021) 1–74.\n[47] R. Chandra, S. Goyal, R. Gupta, Evaluation of deep learning models for\nmulti-step ahead time series prediction, IEEE Access 9 (2021) 83105–\n83123.\n[48] H. Zhu, H. Xia, Y. Guo, C. Peng, The heterogeneous effects of urban-\nization and income inequality on co2 emissions in brics economies: ev-\nidence from panel quantile regression, Environmental Science and Pol-\nlution Research 25 (2018) 17176–17193.\n[49] R. Koenker, Quantile regression for longitudinal data, Journal of multi-\nvariate analysis 91 (1) (2004) 74–89.\n[50] Z. Cai, Regression quantiles for time series, Econometric theory 18 (1)\n(2002) 169–192.\nURL\nhttps://www.cambridge.org/core/\njournals/econometric-theory/article/abs/\nregression-quantiles-for-time-series/\n485A99DE06615C32AA41D737D8E29A77\n[51] Z. Xiao, Time series quantile regressions, in: Handbook of statistics,\nVol. 30, Elsevier, 2012, pp. 213–257.\nURL\nhttps://www.sciencedirect.com/science/article/\nabs/pii/B9780444538581000090\n[52] R. Koenker, Quantile regression:\n40 years on, Annual review of\neconomics 9 (2017) 155–176.\nURL\nhttps://www.annualreviews.org/content/journals/\n10.1146/annurev-economics-063016-103651#right-ref-B44\n[53] A. J. McNeil, Extreme value theory for risk managers, Departement\nMathematik ETH Zentrum 12 (5) (1999) 217–237.\n[54] J. Pickands III, Statistical inference using extreme order statistics, The\nAnnals of Statistics (1975) 119–131.\n[55] T. G. Bali, The generalized extreme value distribution, Economics letters\n79 (3) (2003) 423–427.\n[56] E. Castillo, A. S. Hadi, Fitting the generalized Pareto distribution to data,\nJournal of the American Statistical Association 92 (440) (1997) 1609–\n1620.\n[57] A. Ferreira, L. De Haan, On the block maxima method in extreme value\ntheory: Pwm estimators, The Annals of statistics (2015) 276–298.\nURL http://www.jstor.org/stable/43556515\n[58] M. Fr´echet, Sur la loi de probabilit´e de l’´ecart maximum, Ann. de la Soc.\nPolonaise de Math. (1927).\n[59] R. A. Fisher, L. H. C. Tippett, Limiting forms of the frequency distribu-\ntion of the largest and smallest member of a sample, Proc. Cambridge\nPhilosophical Society 24 (1928) 180–190.\n[60] V. F. Pisarenko, A. Sornette, D. Sornette, et al., Characterization of the\ntail of the distribution of earthquake magnitudes by combining the gev\nand gpd descriptions of extreme value theory, Pure and Applied Geo-\nphysics 171 (2014) 1599–1624.\n[61] A. B¨ucher, C. Zhou, A horse race between the block maxima method\nand the peak–over–threshold approach, Statistical Science 36 (3) (2021)\n360–378.\nURL\nhttps://projecteuclid.org/journals/\nstatistical-science/volume-36/issue-3/\nA-Horse-Race-between-the-Block-Maxima-Method-and-the/\n10.1214/20-STS795.short\n[62] T. H. Soukissian, C. Tsalis, The effect of the generalized extreme value\ndistribution parameter estimation methods in extreme wind speed pre-\ndiction, Natural Hazards 78 (2015) 1777–1809.\n[63] J. Schaumburg, Predicting extreme value at risk: Nonparametric quantile\nregression with refinements from extreme value theory, Computational\nStatistics & Data Analysis 56 (12) (2012) 4081–4096.\n[64] F. F. do Nascimento, D. Gamerman, H. F. Lopes, A semiparametric\nbayesian approach to extreme value estimation, Statistics and Comput-\ning 22 (2012) 661–675.\n[65] H. B. Sandya, P. H. Kumar, S. B. Patil, Feature extraction, classification\nand forecasting of time series signal using fuzzy and garch techniques,\nin: Proceedings of the National Conference on Challenges in Research\nand Technology in the Coming Decades (CRT), 2013, pp. 1–7.\n[66] L. C. Chang, P. A. Chen, F. J. Chang, Reinforced two-step-ahead weight\nadjustment technique for online training of recurrent neural networks,\nIEEE Transactions on Neural Networks and Learning Systems 23 (8)\n(2012) 1269–1278. doi:10.1109/TNNLS.2012.2200695.\n[67] R. W. V. S. Khedkar, R. Chandra, Evaluation of deep learning models for\naustralian climate extremes: prediction of streamflow and floods (2024).\narXiv:2407.15882.\nURL https://arxiv.org/abs/2407.15882\n[68] J.\nLampinen,\nA.\nVehtari,\nBayesian\napproach\nfor\nneural\nnet-\nworks—review and case studies, Neural Networks 14 (3) (2001) 257–\n274.\n[69] I. Kononenko, Bayesian neural networks, Biological Cybernetics 61 (5)\n(1989) 361–370.\n[70] H. Jang, J. Lee, An empirical study on modeling and prediction of bit-\ncoin prices with bayesian neural networks based on blockchain informa-\ntion, IEEE Access 6 (2017) 5427–5437.\n[71] R. Chandra, Y. He, Bayesian neural networks for stock price forecast-\ning before and during covid-19 pandemic, PLoS ONE 16 (7) (2021)\ne0253217. doi:10.1371/journal.pone.0253217.\n[72] A. G. Wang, Y., B. Martin-Barragan, Machine learning approaches to\nforecasting cryptocurrency volatility: Considering internal and exter-\n17\nnal determinants, International Review of Financial Analysis 90 (2023)\n102914.\n[73] G. Van Houdt, C. Mosquera, G. N´apoles, A review on the long short-\nterm memory model, Artificial Intelligence Review 53 (12 2020). doi:\n10.1007/s10462-020-09838-1.\n[74] J. L. Elman, Finding structure in time, Cognitive Science 14 (2) (1990)\n179–211.\n[75] S. Hochreiter, The vanishing gradient problem during learning recurrent\nneural nets and problem solutions, International Journal of Uncertainty,\nFuzziness and Knowledge-Based Systems 6 (02) (1998) 107–116.\n[76] A. Graves, J. Schmidhuber, Framewise phoneme classification with bidi-\nrectional lstm and other neural network architectures, Neural Networks\n18 (5-6) (2005) 602–610.\n[77] Y. Yu, X. Si, C. Hu, J. Zhang, A review of recurrent neural net-\nworks: LSTM cells and network architectures, Neural computation\n31 (7) (2019) 1235–1270.\n[78] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learning with\nneural networks, in: Advances in Neural Information Processing Sys-\ntems, Vol. 27, 2014.\n[79] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, W.-c. Woo, Con-\nvolutional lstm network: A machine learning approach for precipitation\nnowcasting, in: Advances in Neural Information Processing Systems,\nVol. 28, 2015.\n[80] R. Hecht-Nielsen, Theory of the backpropagation neural network, in:\nNeural Networks for Perception, Academic Press, 1992, pp. 65–93.\n[81] X.\nHinaut,\nN.\nTrouvain,\nMackey-glass\ntimeseries\ndataset,\nhttps://reservoirpy.readthedocs.io/en/latest/api/\ngenerated/reservoirpy.datasets.mackey_glass.html (2021).\n[82] M. C. Mackey, L. Glass, Oscillation and chaos in physiological con-\ntrol systems, Science 197 (4300) (1977) 287–289.\ndoi:10.1126/\nscience.267326.\n[83] E. N. Lorenz, The statistical prediction of solutions of dynamic\nequations, Proceedings of the International Symposium on Numerical\nWeather Prediction (1960) 628–635.\nURL\nhttps://web.archive.org/web/20190523190103/http:\n//eaps4.mit.edu/research/Lorenz/The_Statistical_\nPrediction_of_Solutions_1962.pdf\n[84] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization\n(2017). arXiv:1412.6980.\nURL https://arxiv.org/abs/1412.6980\n[85] A. Greaves, B. Au, Using the bitcoin transaction graph to predict the\nprice of bitcoin, 2015.\nURL https://api.semanticscholar.org/CorpusID:18038866\n[86] R. Y. Chou, Forecasting financial volatilities with extreme values: the\nconditional autoregressive range (carr) model, Journal of Money, Credit\nand Banking (2005) 561–582.\n[87] X. Zhao, C. Scarrott, L. Oxley, M. Reale, Extreme value modelling for\nforecasting market crisis impacts, in: The Global Financial Crisis, Rout-\nledge, 2014, pp. 61–70.\n[88] W. J. Dixon, Analysis of extreme values, The Annals of Mathematical\nStatistics 21 (4) (1950) 488–506.\n[89] B. Merz, A. H. Thieken, Flood risk curves and uncertainty bounds, Nat-\nural Hazards 51 (2009) 437–458.\n[90] L. Yang, A. Shami, On hyperparameter optimization of machine learn-\ning algorithms: Theory and practice, Neurocomputing 415 (2020) 295–\n316.\n[91] R. Muthukrishnan, R. Rohini, Lasso: A feature selection technique in\npredictive modeling for machine learning, in: 2016 IEEE International\nConference on Advances in Computer Applications (ICACA), IEEE,\n2016, pp. 18–20.\n[92] W. N. van Wieringen, Lecture notes on ridge regression, arXiv preprint\narXiv:1509.09169 (Sep 2015).\n[93] S. Wager, S. Wang, P. S. Liang, Dropout training as adaptive regulariza-\ntion, Advances in Neural Information Processing Systems 26 (2013).\n[94] I. S. Al-Tameemi, M. R. Feizi-Derakhshi, S. Pashazadeh, M. Asadpour,\nMulti-model fusion framework using deep learning for visual-textual\nsentiment classification, Computers, Materials & Continua 76 (2) (2023)\n2145–2177.\n[95] W. Zhang, Y. Deng, B. Liu, S. J. Pan, L. Bing, Sentiment analysis\nin the era of large language models: A reality check, arXiv preprint\narXiv:2305.15005 (May 2023).\n[96] R. M. Neal, Bayesian Learning for Neural Networks, Vol. 118, Springer\nNew York, 1996.\n[97] Y. Yoon, G. J. S, T. M. Margavio, A comparison of discriminant analysis\nversus artificial neural networks, Journal of the Operational Research\nSociety 44 (1) (1993) 51–60. doi:10.1057/jors.1993.6.\n[98] D. J. C. MacKay, A practical bayesian framework for backpropagation\nnetworks, Neural Computation 4 (3) (1992) 448–472. doi:10.1162/\nneco.1992.4.3.448.\n[99] R. Zhang, C. Li, J. Zhang, C. Chen, A. G. Wilson, Cyclical\nstochastic gradient mcmc for bayesian deep learning, arXiv preprint\narXiv:1902.03932 (Feb 2019).\n[100] H. Attias, Inferring parameters and structure of latent variable models\nby variational bayes, arXiv preprint arXiv:1301.6676 (Jan 2013).\n[101] B. Chaput, J. C. Girard, M. Henry, Frequentist approach: Modelling and\nsimulation in statistics and probability teaching, in: Teaching Statistics\nin School Mathematics—Challenges for Teaching and Teacher Educa-\ntion: A Joint ICMI/IASE Study: The 18th ICMI Study, 2011, pp. 85–95.\n[102] E. Afrifa-Yamoah, U. A. Mueller, S. M. Taylor, A. J. Fisher, Missing\ndata imputation of high-resolution temporal climate time series data,\nMeteorological Applications 27 (1) (2020) e1873.\nAppendix A. Prediction quantiles\n18\n(a) Univariate Quantile BD-LSTM\n(b) Multivariate Quantile BD-LSTM\n(c) Univariate Quantile Conv-LSTM\n(d) Multivariate Quantile Conv-LSTM\n(e) Univariate Quantile ED-LSTM\n(f) Multivariate Quantile ED-LSTM\nFigure A.9: Prediction for the Bitcoin time series, showing quantiles for Univariate and Multivariate strategies for the Quantile-ED-LSTM (e.g. Quantiles 0.05-0.25)\nand ED-LSTM (Predicted).\n19\n(a) Univariate Quantile BD-LSTM\n(b) Multivariate Quantile BD-LSTM\n(c) Univariate Quantile Conv-LSTM\n(d) Multivariate Quantile Conv-LSTM\n(e) Univariate Quantile ED-LSTM\n(f) Multivariate Quantile ED-LSTM\nFigure A.10: Prediction for the Ethereum time series, showing quantiles for Univariate and Multivariate strategies for the Quantile-ED-LSTM (e.g. Quantile\n0.05-0.25) and ED-LSTM (Predicted).\n20\n(a) Sunspot Quantile BD-LSTM\n(b) Sunspot Quantile ED-LSTM\n(c) Mackey-Glass Quantile BD-LSTM\n(d) Mackey-Glass Quantile ED-LSTM\n(e) Lorenz Quantile BD-LSTM\n(f) Lorenz Quantile ED-LSTM\nFigure A.11: Predictions for univariate Quantile-ED-LSTM models (e.g. Quantile 0.05-0.25) and ED-LSTM (Predicted) for Mackey-Glass, Sunspot and Lorenz\ntime series.\n21\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "q-fin.ST",
    "stat.ME"
  ],
  "published": "2024-11-24",
  "updated": "2024-11-24"
}