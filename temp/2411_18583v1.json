{
  "id": "http://arxiv.org/abs/2411.18583v1",
  "title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
  "authors": [
    "Nurshat Fateh Ali",
    "Md. Mahdi Mohtasim",
    "Shakil Mosharrof",
    "T. Gopi Krishna"
  ],
  "abstract": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model.",
  "text": "Automated Literature Review Using NLP \nTechniques and LLM-Based Retrieval-Augmented \nGeneration \n \nNurshat Fateh Ali \nDepartment of Computer Science and Engineering \nMilitary Institute of Science and Technology \nDhaka, Bangladesh \nnurshatfateh@gmail.com \n \nShakil Mosharrof \nDepartment of Computer Science and Engineering \nMilitary Institute of Science and Technology \nDhaka, Bangladesh \nshakilmrf8@gmail.com \nMd. Mahdi Mohtasim \nDepartment of Computer Science and Engineering \nMilitary Institute of Science and Technology \nDhaka, Bangladesh \nmahdimohtasim@gmail.com \n \nT. Gopi Krishna \nDepartment of Computer Science and Engineering \nMilitary Institute of Science and Technology \nDhaka, Bangladesh \ngopi.mistbd@gmail.com \n \n \n \nAbstract—This research presents and compares multiple ap- \nproaches to automate the generation of literature reviews using \nseveral Natural Language Processing (NLP) techniques and \nretrieval-augmented generation (RAG) with a Large Language \nModel (LLM). The ever-increasing number of research articles \nprovides a huge challenge for manual literature review. It has \nresulted in an increased demand for automation. Developing a \nsystem capable of automatically generating the literature reviews \nfrom only the PDF files as input is the primary objective of this \nresearch work. The effectiveness of several Natural Language \nProcessing (NLP) strategies, such as the frequency-based method \n(spaCy), the transformer model (Simple T5), and retrieval- \naugmented generation (RAG) with Large Language Model (GPT- \n3.5-turbo), is evaluated to meet the primary objective. The \nSciTLDR dataset is chosen for this research experiment and \nthree distinct techniques are utilized to implement three different \nsystems for auto-generating the literature reviews. The ROUGE \nscores are used for the evaluation of all three systems. Based \non the evaluation, the Large Language Model GPT-3.5-turbo \nachieved the highest ROUGE-1 score, 0.364. The transformer \nmodel comes in second place and spaCy is at the last position. \nFinally, a graphical user interface is created for the best system \nbased on the large language model. \nIndex Terms—T5, SpaCy, Large Language Model, GPT, \nROUGE, Literature Review, Natural Language Processing, \nRetrieval-augmented generation. \n \nI. INTRODUCTION \nLiterature reviews have gained considerable importance \nfor scholars. It provides researchers with a comprehensive \noverview of previous findings in a specific field and assists \nscholars in identifying gaps in past understandings. It helps to \nconduct future research and informs researchers of areas where \nthey can provide significant input. However, conducting liter- \nature reviews can be incredibly cumbersome because there’s \nso much to read. Due to the vast volume of research articles \nbeing released, reviewing all related studies and extracting \nrelevant information can be a time-consuming, tedious, and \nerror-prone task. Due to these difficulties, there has been \nan increasing interest in automating the process of literature \nreviews [1]. Automated systems can use natural language \nprocessing techniques and machine learning algorithms to \nanalyze extensive amounts of text, extract relevant details, and \ncreate structured summaries [2]. \nThe primary objective of this research is to develop a system \nthat can automatically generate the literature review segment \nof a research paper by using only the PDF files of the related \npapers as input. Several Natural Language Processing tech- \nniques such as the Frequency-based approach, Transformer- \nbased approach, and Large Language Model-based approach \nare implemented and compared to find the best procedure. The \nSciTLDR dataset [3] is selected for this research work. The \nfirst procedure uses the frequency-based approach. The library \nnamed spaCy [4] is utilized here. The second procedure uses \nthe transformer-based model. The Simple T5 model is utilized \nhere. The last procedure is based on using the Large Language \nModel. The GPT-3.5-TURBO-0125 model is utilized here. \nThe evaluation and comparison are performed using ROUGE \nscores [5]. Then the best approach is identified and a Graphical \nUser Interface-based tool is created. \nAutomating aspects of the literature review process allows \nacademicians to save time and concentrate on the most perti- \nnent articles for their research. It can also reduce the chance \nof errors or prejudice in the review process. The highlights of \nthis article are: \n• All three considered NLP approaches such as spaCy, T5, \nand GPT-3.5-TURBO-0125 model can produce satisfac- \ntory results in automating the literature review generation. \n• The LLM-based model outperforms T5 and spaCy in \ngenerating literature reviews. \nII. LITERATURE REVIEW \nA framework was proposed by Silva et al. [6] for auto- \nmatically producing systematic literature reviews. They have \nfocused on four technical steps: Searching, Screening, Map- \nping, and Synthesizing. In response to a specific inquiry, \nextensive searches are conducted to find as much relevant \nresearch as feasible, involving looking through reference lists, \nscouring internet databases, and reviewing published materials. \nScreening reduces the search scope by limiting the collection \nto only the papers pertinent to a particular review, aiming \nto highlight important findings and facts that could influence \npolicy. Mapping is used to comprehend research activity in \na particular area, involve stakeholders, and define priorities \nconcerning the review emphasis. Synthesizing integrates data \nfrom numerous sources and provides an overview of the \noutcomes. The formulation of research questions, reporting \nphase, and peer review are some steps that are also discussed \nfor the composition of systematic literature reviews. \nPeer-reviewed publications are growing exponentially with \nthe rapid development of science. Therefore, Yuan et al. \n[7] have explored the use of machine learning techniques, \nnatural language generation, multi-document summarization, \nand multi-objective optimization for automating scientific re- \nviewing. They have discussed the generation of comprehensive \nreviews and noted the limitations of constructive feedback \ncompared to human-written reviews. The models used in this \nresearch are not yet fully capable of automating Literature \nReviews and they require human reviewers. \nA comprehensive analysis of existing tools for systematic \nliterature reviews was done by Karakan et al. [8]. They have \nexplored the potential for automation in various phases of the \nreview process, highlighting the need for a holistic tool de- \nsign to address researchers’ challenges effectively. They have \ndiscussed two methodologies to accomplish their research: \nRapid Review and Semi-Structured Interviews. Rapid Review \nemphasizes decision-making procedures for resolving issues, \ndifficulties, and challenges that software engineers encounter \nin their daily work. Semi-structured interviews are used \nto explore researchers’ experiences, challenges, strategies, \nstrengths, weaknesses of Systematic Literature Review tools, \nand requirements for effective support in software engineering. \nJaspers et al. [9] focused on the use of machine learning \ntechniques for automation of literature reviews and systematic \nreviews. They have outlined the pros and cons of different \nmachine-learning techniques. The process of automating the \nliterature review was elaborately discussed. The paper lacks \npractical validation across diverse domains and detailed in- \nsights. \nA concise overview of automated literature reviews was \npresented by Tauchert et. al. [10] They have emphasized the \npotential for automation in various stages of the systematic \nreview process. The paper discusses the importance of in- \ntegrating computational techniques to streamline tasks such \nas searching, screening, extraction, and synthesis. It also ac- \nknowledges the need for further research to address challenges \nand enhance the effectiveness of automated approaches. \nA brief overview on the topic of automatic literature review \ntools was given by Tsai et. al. [11] They discussed the \nexisting research in the field, the challenges faced in conduct- \ning literature reviews manually, and the potential benefits of \nautomating the process. The main focus of their contributions \nis the evaluation of Mistral LLM’s effectiveness in the field \nof Academic Research. \nThe gaps in the intersection of systematic literature reviews \n(SLRs) and LLMs are discussed by Susnjak et. al. [12]. They \nalso emphasized the need to address challenges in the synthesis \nphase of research and highlighted the potential of fine-tuning \nLLMs with datasets to enhance knowledge synthesis accuracy. \nThe study aims to bridge this gap by proposing a Systematic \nLiterature Review automation framework. \nMost of the related works that have been discussed are \nmainly focused on discussing the potential and challenges of \nusing NLP techniques and LLMs to automate the literature \nreview process. None of them proposes a complete system \npipeline where users can directly generate the literature re- \nview only using the PDF and DOI. In contrast, this article \nproposes and implements three unique end-to-end pipelines \nand procedures for a literature review automation system. This \nresearch endeavor has also resulted in the implementation \nof a UI tool where users can directly upload PDFs and get \na literature review segment generated automatically without \nany additional effort. Moreover, this paper also includes a \ncomparative analysis of different approaches such as the \nfrequency-based approach, transformer-based approach, and \nrag-based approach using ROUGE scores which contributes \ntowards finding the effectiveness of these approaches for this \ntask. \nIII. SYSTEM DESIGN \nThe research is carried out in four stages: 1. Defining \nresearch objectives. 2. Proposing multiple procedures for au- \ntomated literature review generation. 3. Evaluating multiple \nprocedures to find the best approach. 4. The final system \ndevelopment. \nA. Dataset Selection \nThe SciTLDR dataset from the Hugging Face is selected \nfor this research work [13]. It contains the summarization \nof scientific documents. It is a dataset with 5,400 TLDRs \nderived from over 3,200 papers. It contains both author-written \nand expert-derived TLDRs of scientific documents. Curated \nresearch articles’ abstract, introduction, and conclusion (AIC) \nor full text of the paper are given as ”source” and the \nsummaries of the corresponding articles are given as ”target”. \nOnly these two attributes are utilized in all three proposed \nprocedures. There is no training for the spaCy approach, but \nthe dataset is utilized for testing purposes. The T5 model is \ntrained using the SciTLDR dataset for the transformer-based \napproach and later evaluated on the test dataset. For the LLM- \nbased approach, this dataset is used as the knowledge base for \nthe model. \nB. The Procedure Utilizing the Frequency-Based Approach \nusing spaCy \nThe first procedure utilizes the frequency-based approach by \nusing spaCy. The first task is to build the model pipeline. The \nmodel pipeline takes text as input and converts the text into \nNLP tokens using the spaCy library. Then preprocessing step is \ndone by removing stop words and punctuation. Afterward, the \nword frequency is calculated for each word which later helps \nto calculate individual sentence weights. This sentence weight \nrepresents the importance of that sentence. Then the top 10 \npercent of sentences are selected as the final output. The model \nis later evaluated using ROUGE scores to get an overview of \nthe performance. The overview of the spaCy Model is given \nin Figure 1. \n \nFigure 1: Building spaCy Model \n \nThe next step is to implement a system pipeline by using \nthe spaCy model to generate a literature review segment \nautomatically. The system takes the DOI and PDF files of \nmultiple papers as input. It uses the Requests library to collect \nthe paper titles and first author names from the DOI. Then it \nuses PYPDF2 and Regular Expression (RE) libraries to collect \nonly the conclusion of each PDF. Then it uses the previously \nimplemented spaCy model to get a summary of each paper. \nLater it performs post-processing and merges all summaries \nto produce a coherent literature review segment. The system \npipeline overview of the spaCy Model is given in Figure 2. \n \nFigure 2: Pipeline using spaCy \n \nC. The Procedure Utilizing the Transformer-Based T5 Model \nThe second approach utilizes the transformer-based Simple \nT5 model. The first task is to train the model and prepare the \nmodel for the final pipeline. The SciTLDR dataset is collected \nto train the model. Then the dataset is prepared to use as the \ntraining data for the selected model. A task-specific prefix is \nadded to summarize individual papers. Then the model is fine- \ntuned as per the requirements. Then the model is trained with \nthe training data and the result is predicted. The result is the \nsummarization of individual papers. Then the evaluation is \nperformed using ROUGE scores and the model is saved for \nfurther utilization later in the system pipeline. The training \noverview of the Transformer Model is given in Figure 3. \n \n \nFigure 3: Training of Transformer Model \n \n \nThe next step is to implement a system pipeline by using \nthe transformer-based model to generate a literature review \nsegment automatically. The system takes the DOI and PDF of \nmultiple papers as input. It uses the Requests library to collect \nthe paper titles and first author names from DOIs. Then it uses \nPYPDF2 and Regular Expression (RE) libraries to collect each \nPDF’s abstract, introduction, and conclusion. Then it merges 3 \nof these sections to get the final model input. Later it uses the \npreviously trained and saved T5 model to get a summary of \neach paper. In the next step, it performs post-processing and \nmerges all summaries to produce a coherent literature review \nsegment. The system pipeline overview of the Transformer \nModel is given in Figure 4. \n \n \nFigure 4: Pipeline using Transformer Model \nD. The Procedure Utilizing the Large Language Model: GPT- \n3.5-TURBO-0125 \nThe third procedure utilizes the RAG-based approach by \nusing the Large Language Model: GPT-3.5-TURBO-0125. The \nfirst task is to create a custom OpenAI Assistant. Firstly, the \nSciTLDR dataset is collected, and then the GPT-3.5-TURBO- \n0125 model is selected for the OpenAI assistant. The retrieval \nis turned on and the dataset is added for the knowledge of the \nLLM. Now some prompt engineering is performed to produce \nthe required output. Then the LLM results are evaluated using \nROUGE SCORE. The overview of the creation of the OpenAI \nassistant is given in figure 5. \n \nFigure 5: Creation of Custom OpenAI Assistant \n \nThe used prompt: “The user will give you a pdf file as input, \nsimilar to the “input” field of the given “data.json” file in your \nknowledge base. You have to produce a summarized “output” \nfor the given pdf based on the file given to your knowledge. \nThe output will be of max 80 words. Note: You must write \nin a way that can be considered a literature review of a new \nresearch paper. The user in the future might add more PDFs \nso try to make the literature review coherent and as per IEEE \nstandards. Please mention the first author’s name and paper \ntitle. Don’t write like this “Literature Review of. . . ”.” \n \nFigure 6: Pipeline using LLM \n \nThe next step is to implement a system pipeline by using \nthe LLM to generate a literature review segment automatically. \nThe system takes PDFs of multiple papers as input. It uses the \nPYPDF2 library to extract the entire text of each PDF. Then it \ncreates a new thread with the extracted text as a message and \nsubmits the thread to the assistant with the extracted text as \na query. Then the response from the assistant is retrieved and \nthe outputs of each paper are merged for the final literature \nreview segment. The system pipeline overview of the LLM is \ngiven in Figure 6. \nE. The Final System Tool \nThe final system is implemented using the Large Language \nModel: GPT-3.5-TURBO-0125 as the backend. An aesthetic \nand simple user interface is created where the user can easily \nupload multiple research articles or PDF files. The user has \nto press the ”Browse files” button and then select the files \nto upload. Then the system loads the research papers and \nwithin a few seconds, it produces the literature review segment \nautomatically. It individually processes each paper and pro- \nduces output. The loading screen and processing file numbers \nindicate the progress level and the number of processed papers. \nAt the end of the literature review, the UI shows ”Done” text \nto indicate the completion of the task. The user interface of \nthe system is given in Figure 7 \n \nFigure 7: The Preview of the System UI \n \nIV. SYSTEM EVALUATION \nThe ROUGE scores are used for the evaluation in this \nresearch. The evaluation is done based on the test data of \nthe selected dataset. ROUGE (Recall-Oriented Understudy for \nGisting Evaluation) is a set of metrics used for evaluating the \nquality of machine-generated summaries by comparing them \nto reference summaries. The used ROUGE metrics are: \n• ROUGE-N (precision, recall, and F1 score for n-gram \noverlaps), \n• ROUGE-L (measuring longest common subsequence) \n• ROUGE-Lsum (ROUGE-Longest for summary level \nevaluation) \nA. Evaluation of Frequency-Based spaCy \nThe spaCy-based model was evaluated on the test data \nutilizing the ROUGE scores. The results are stated in Table I. \nTable I: ROUGE Scores for spaCy \n \nROUGE-1 \n0.257 \nROUGE-2 \n0.055 \nROUGE-L \n0.144 \nROUGE-L SUM \n0.146 \n \nB. Evaluation of Transformer \nT5 The transformer-based model was evaluated on the test \ndata utilizing the ROUGE scores. The results are stated in \nTable II. \nTable II: ROUGE Scores for T5 \n \nROUGE-1 \n0.268 \nROUGE-2 \n0.115 \nROUGE-L \n0.204 \nROUGE-L SUM \n0.204 \n \nC. Evaluation of Large Language Model: GPT-3.5-TURBO- \n0125 \nThe LLM-based model was evaluated on the test data \nutilizing the ROUGE scores. The results are stated in Table \nIII. \nTable III: ROUGE Scores for LLM \n \nROUGE-1 \n0.364 \nROUGE-2 \n0.123 \nROUGE-L \n0.181 \nROUGE-L SUM \n0.182 \n \nD. Comparison of Multiple Approaches \nThe comparison between all three approaches is provided \nin Table IV. \nTable IV: Comparison of Rouge Scores \n \nT5 \nspaCy \nGPT-3.5-TURBO-0125 \nROUGE-1 \n0.268 \n0.257 \n0.364 \nROUGE-2 \n0.115 \n0.055 \n0.123 \nROUGE-L \n0.204 \n0.144 \n0.181 \nROUGE-L SUM \n0.204 \n0.146 \n0.182 \n \nFrom the ROUGE scores, it is clear that the LLM-based \nmodel outperformed both T5 and spaCy. The Transformer- \nbased model is in the second spot based on the ROUGE-1 \nscore. The evaluation proves that large language models are \nsuperior to older NLP approaches. \nE. User Evaluation on UI \nThe user evaluation was performed on the LLM-based UI \nsystem. The user chose 4 research articles as input. These \nwere: \n• AI-IoT based Healthcare Prognosis Interactive System \n• Applying Internet of Things and Machine-Learning for \nPersonalized Healthcare: Issues and Challenges \n• Improving patients health-care using IoT \n• IoT Based AI and its Implementations in Industries \nThe received output from the system: ”The paper by Joshua \nErnest Pedi Reddy et al. presents an AI-IoT based Healthcare \nPrognosis Interactive System (HPIS) to provide real-time \nmedical diagnosis and support in the absence of healthcare \nworkers. The system includes an AI-based chatbot and Appli- \ncation Interface to gather information, answer medical queries, \nassist users, and ensure medication adherence. With a focus \non improving healthcare accessibility and quality, the system \nintegrated smart pill dispensers for monitoring and enhancing \nmedication adherence, offering great potential in revolution- \nizing healthcare services. Ahamed explores the application \nof IoT and Machine Learning in Personalized Healthcare \n(PH) to enhance disease management and intervention. IoT \nsensor devices and wearables collect patient data, which is \nanalyzed using AI and ML techniques for disease prediction \nand patient self-management. Challenges include biased data \ncollection, outdated training datasets, and privacy concerns. \nThe integration of IoT and ML in PH faces issues such as data \ntransmission reliability and biased training datasets, impacting \ndiagnostic accuracy. Addressing these challenges is crucial for \nimproving personalized healthcare systems. Khurana imple- \nments a Smart Healthcare System using IoT sensors to enhance \npatient care in hospitals. The system utilizes Ultrasonic and IR \nProximity Sensors connected to an Arduino Uno for automated \nIV fluid level monitoring and patient alarm system. The liter- \nature review highlights the importance of automated Hospital \nManagement Systems for efficient healthcare administration. \nThe proposed system reduces manpower, costs, and human \nerrors while improving patient care. Future research aims to \nenhance system scalability and functionality for widespread \nhospital use. Sherif El-Gendy explores the integration of IoT \nand AI in industries in the paper ”IoT Based AI and its \nImplementations in Industries.” The paper delves into Industry \n4.0, IIoT, IAIoT, and IoRT, showcasing the impact on au- \ntomation and robotics. It discusses IoT challenges, benefits \nof AI in data analysis, and presents case studies like oil field \nproduction optimization and smart robotics by companies like \nABB and Boeing. The future of IoT/AI integration promises \ntransformative advancements in various sectors.” \nV. RESULT AND DISCUSSION \nThe study introduced three procedures for automated lit- \nerature review generation. The research work also illustrates \nthe performance comparison between various NLP approaches \nsuch as the frequency-based method (spaCy), transformer \nmodel (Simple T5), and retrieval-augmented generation (RAG) \nwith LLM (GPT-3.5-turbo). All three procedures are im- \nplemented and the ROUGE-1, ROUGE-2, ROUGE-L, and \nROUGE-Lsum scores are calculated based on the Test dataset. \nFor all three approaches, the ROUGE-1 and ROUGE-2 scores \nare found above the acceptable mark. \nFrom the evaluation, it is seen that the GPT-3.5-turbo model \nproduced results with higher ROUGE-1 and ROUGE-2 scores \nthan the SpaCy and T5. The overall ROUGE-1 score for the \nLLM is 0.364 while the score for T5 is 0.268 and spaCy is \n0.257. It shows that the LLM-generated summaries have better \nunigram and bigram overlapping with human summaries. The \ntransformer T5 is also an advanced model which comes in \nsecond place. The last position is occupied by the frequency- \nbased spaCy model. \nFrom the scores, it is clear that the most advanced models \nare LLMs which outperformed all other NLP techniques. But \nother approaches such as transformer models and frequency- \nbased approaches are also capable of producing satisfactory \nROUGE scores and a coherent literature review segment. \nVI. CONCLUSION AND FUTURE SCOPES \nThe research focused on implementing and comparing vari- \nous NLP techniques for automated literature review. All three \nimplemented systems are successful in generating the coherent \nLiterature Review segment of a research paper. The results \nof various Natural Language Processing techniques such as \nthe Frequency-based approach, Transformer model, and Large \nLanguage Model are also successfully obtained and compared. \nBased on the comparisons, the LLM-based approach is proven \nto be the best-performing one based on ROUGE-N scores. \nThus, based on the LLM, a final system tool is also success- \nfully developed where the user can upload multiple PDF files \nto automatically generate a coherent literature review segment. \nFuture work of this research work can be focused on \nenhancing the effectiveness and applicability of the developed \nsystem tool. More functionality can be added to the Graphical \nUser Interface such as model options, output size, etc. More \nmodels such as Bert, Gemini, and LLaMA can be utilized to \nfind better results. \n[6] da Silva Ju´nior EM, Dutra ML. A roadmap toward the automatic \ncomposition of systematic literature reviews. Iberoamerican Journal of \nScience Measurement and Communication. 2021 Jul 27. \n[7] Yuan W, Liu P, Neubig G. Can we automate scientific reviewing?. \nJournal of Artificial Intelligence Research. 2022 Sep 29;75:171-212. \n[8] Karakan B, Wagner S, Bogner J. Tool support for systematic literature \nreviews: Analyzing existing solutions and the potential for automation \n(Doctoral dissertation, University of Stuttgart). \n[9] Jaspers S, De Troyer E, Aerts M. Machine learning techniques for the \nautomation of literature reviews and systematic reviews in EFSA. EFSA \nSupporting Publications. 2018 Jun;15(6):1427E. \n[10] Tauchert C, Bender M, Mesbah N, Buxmann P. Towards an integrative \napproach for automated literature reviews using machine learning. \n[11] Tsai HC, Huang YF, Kuo CW. Comparative analysis of automatic liter- \nature review using mistral large language model and human reviewers. \n[12] Susnjak T, Hwang P, Reyes NH, Barczak AL, McIntosh TR, Ranathunga \nS. Automating research synthesis with domain-specific large language \nmodel fine-tuning. arXiv preprint arXiv:2404.08680. 2024 Apr 8. \n[13] AllenAI. SCITL-DR Dataset. [Dataset]. Hugging Face. [Online]. Avail- \nable: https://huggingface.co/datasets/allenai/scitldr. [Accessed: Sep. 8, \n2024]. \n \nREFERENCES \n[1] Felizardo KR, Carver JC. Automating systematic literature review. \nContemporary empirical methods in software engineering. 2020:327-55. \n[2] Adhikari S. Nlp based machine learning approaches for text summariza- \ntion. In2020 Fourth International Conference on Computing Methodolo- \ngies and Communication (ICCMC) 2020 Mar 11 (pp. 535-538). IEEE. \n[3] Cachola I, Lo K, Cohan A, Weld DS. TLDR: Extreme summarization \nof scientific documents. arXiv preprint arXiv:2004.15011. 2020 Apr 30. \n[4] Jugran S, Kumar A, Tyagi BS, Anand V. Extractive automatic text \nsummarization using SpaCy in Python & NLP. In2021 International \nconference on advance computing and innovative technologies in en- \ngineering (ICACITE) 2021 Mar 4 (pp. 582-585). IEEE. \n[5] Ali NF, Tanvin JU, Islam MR, Ahmed J, Akhtaruzzaman M. ROUGE \nScore Analysis and Performance Evaluation Between Google T5 and \nSpaCy for YouTube News Video Summarization. In2023 26th Interna- \ntional Conference on Computer and Information Technology (ICCIT) \n2023 Dec 13 (pp. 1-6). IEEE. \n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.IR",
    "cs.LG"
  ],
  "published": "2024-11-27",
  "updated": "2024-11-27"
}