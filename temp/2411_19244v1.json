{
  "id": "http://arxiv.org/abs/2411.19244v1",
  "title": "Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks",
  "authors": [
    "Jinu Nyachhyon",
    "Mridul Sharma",
    "Prajwal Thapa",
    "Bal Krishna Bal"
  ],
  "abstract": "The Nepali language has distinct linguistic features, especially its complex\nscript (Devanagari script), morphology, and various dialects, which pose a\nunique challenge for natural language processing (NLP) evaluation. While the\nNepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a\nfoundation for evaluating models, it remains limited in scope, covering four\ntasks. This restricts their utility for comprehensive assessments of NLP\nmodels. To address this limitation, we introduce eight new datasets, creating a\nnew benchmark, the Nepali Language Understanding Evaluation (NLUE) benchmark,\nwhich covers a total of 12 tasks for evaluating the performance of models\nacross a diverse set of Natural Language Understanding (NLU) tasks. The added\ntasks include single-sentence classification, similarity and paraphrase tasks,\nand Natural Language Inference (NLI) tasks. On evaluating the models using\nadded tasks, we observe that the existing models fall short in handling complex\nNLU tasks effectively. This expanded benchmark sets a new standard for\nevaluating, comparing, and advancing models, contributing significantly to the\nbroader goal of advancing NLP research for low-resource languages.",
  "text": "arXiv:2411.19244v1  [cs.CL]  28 Nov 2024\nCONSOLIDATING AND DEVELOPING BENCHMARKING\nDATASETS FOR THE NEPALI NATURAL LANGUAGE\nUNDERSTANDING TASKS\n∗Jinu Nyachhyon,\n∗Mridul Sharma,\n∗Prajwal Thapa, Bal Krishna Bal\nInformation and Language Processing Research Lab (ILPRL), Kathmandu University\nDecember 2, 2024\nABSTRACT\nThe Nepali language has distinct linguistic features, especially its complex script (Devanagari script),\nmorphology, and various dialects, which pose a unique challenge for natural language processing\n(NLP) evaluation. While the Nepali Language Understanding Evaluation (Nep-gLUE) benchmark\nprovides a foundation for evaluating models, it remains limited in scope, covering four tasks. This\nrestricts their utility for comprehensive assessments of NLP models. To address this limitation,\nwe introduce eight new datasets, creating a new benchmark, the Nepali Language Understanding\nEvaluation (NLUE) benchmark, which covers a total of 12 tasks for evaluating the performance\nof models across a diverse set of Natural Language Understanding (NLU) tasks. The added tasks\ninclude single-sentence classiﬁcation, similarity and paraphrase tasks, and Natural Language Infer-\nence (NLI) tasks. On evaluating the models using added tasks, we observe that the existing models\nfall short in handling complex NLU tasks effectively. This expanded benchmark sets a new standard\nfor evaluating, comparing, and advancing models, contributing signiﬁcantly to the broader goal of\nadvancing NLP research for low-resource languages.\n1\nIntroduction\nNepali is written in the Devanagari script and is a highly inﬂected language. The Nepali language incorporates a\ncomplex system of noun, adjective, and verb in-ﬂections. Nouns have a system of gender, case, and number [1]. It\nhas a rich vocabulary with many homonyms and is spoken in different dialects across various regions, and there are\nvariations in vocabulary, grammar, and pronunciation. In order to develop and establish robust models for Nepali, it is\ncrucial to have reliable mechanisms for evaluating their quality and effectiveness. Tools that enable us to assess how\nwell models handle the unique linguistic challenges while identifying their limitations are really important to drive the\nprogress.\nDespite Nepali’s importance as a primary or secondary language for millions of speakers, research efforts and re-\nsources dedicated to its computational processing and evaluation remain relatively sparse. Existing benchmarks, such\nas Nep-gLUE [2], have made signiﬁcant progress in this direction, providing a foundation for evaluating models on\nfundamental tasks. However, these benchmarks are limited in scope, primarily addressing a few tasks and overlook-\ning critical aspects of linguistic understanding such as pronoun resolution, paraphrase interpretation, and advanced\ninference capabilities. To address this, we present an expanded suite of Natural Language Understanding (NLU)\ndatasets designed to evaluate models on a broader range of linguistic tasks. Building on the Nep-gLUE benchmark\n[2], which evaluates models on four tasks, we introduce eight additional datasets that cover diverse aspects of NLU.\nThe new tasks include Sentiment Analysis (SA), Corpus of Linguistic Acceptability (CoLA), Paraphrase Detection,\nwith datasets such as Quora Question Pairs (QQP) and the Microsoft Research Paraphrase Corpus (MRPC), NLI, with\ndatasets such as Multi-Genre NLI (MNLI), Question-Answer NLI (QA/NLI), Recognizing Textual Entailment (RTE),\nand Coreference Resolution (CR). These tasks collectively offer a more comprehensive evaluation of NLU capabilities\nfor Nepali language models.\nPREPRINT - DECEMBER 2, 2024\nThe table below provides an overview of the tasks, the number of data points, the evaluation metrics employed, and the\ndomains from which the datasets were collected. All statistics presented are derived from the translated benchmark.\nCorpus\nTrain\nTest\nTask\nMetric\nDomain\nNep-gLUE (Existing)\nNER\n68.8k\n17.2k\nNamed Entity Recognition\nF1\nmisc.\nPOS\n89.1k\n22.2k\nPart-of-Speech Tagging\nF1\nmisc.\nCPS\n36k\n9k\nCategorical Pair Similarity\nF1\nmisc.\nCC\n35.5k\n8.8k\nContent Classiﬁcation\nF1\nmisc.\nSingle sentence tasks (Added)\nSA\n65.1k\n16.3k\nsentiment analysis\nF1\nMovie Reviews\nCoLA\n8.4k\n1k\nacceptability judgements\nF1\nmisc.\nSimilarity and Paraphrase Tasks (Added)\nQQP\n20k\n4.29k\nparaphrase\nF1\nsocial QA\nMPRC\n3.2k\n815\nparaphrase\nF1\nNews\nNatural Language Inference (Added)\nMNLI\n19.2k\n4.8k\nNLI\nF1\nmisc.\nQA/NLI\n12k\n3k\nQA/NLI\nF1\nWikipedia\nRTE\n2.2k\n554\nNLI\nF1\nNews, Wikipedia\nCR\n635\n71\ncoreference resolution\nF1\nFiction\nTable 1: Task descriptions and statistics showing the existing Nep-gLUE dataset and newly added datasets.\nThe expanded NLU dataset is inspired by the General Language Understanding Evaluation (GLUE) benchmark [3]\nand was primarily created through a combination of automated and manual processes to ensure high-quality, task-\nspeciﬁc datasets. Our contributions involves translating datasets with large language models (LLMs), particularly\ngpt-4o-mini[4]. We ensured the accuracy and contextual relevance of these translations. We also conducted a thor-\nough review of the availability of existing Nepali datasets for each task and if such datasets existed, we integrated them\nwith the translated data, carefully removing duplicates to create a uniﬁed, comprehensive dataset. For tasks like Ac-\nceptability Judgments and WNLI (Coreference), where suitable datasets or high-quality translations were unavailable,\nwe performed manual translations to ensure linguistic accuracy and consistency. These efforts collectively ensure that\nthe ﬁnal dataset is both robust and reﬂective of the linguistic diversity in the Nepali language.\nTo assess the effectiveness of the expanded benchmark and performance of models, we conducted experiments by ﬁne-\ntuning both monolingual models trained exclusively on Nepali-language data and multilingual models that include\nNepali as one of their supported languages. Each model was ﬁne-tuned on the newly introduced tasks and evaluated\nusing metrics provided in Table 1, providing a comprehensive understanding of their performance on various aspects\nof Natural Language Understanding (NLU).\n2\nRelated Works\nBenchmarks such as GLUE [3] and its successor Super General Language Understanding Evaluation (SuperGLUE)\nbenchmark [5] have been instrumental in advancing research in Natural Language Understanding (NLU). GLUE\n[3] introduced a multitask framework for evaluating various NLU capabilities, such as single-sentence classiﬁcation,\nsentence-pair similarity, and inference tasks. SuperGLUE [5] extended this with more challenging tasks, includ-\ning causal reasoning and co-reference resolution, addressing the limitations of GLUE [3] for state-of-the-art models.\nThese benchmarks set a standard for evaluating linguistic and semantic understanding in high-resource languages like\nEnglish, inspiring adaptations in other languages and low-resource settings. Efforts such as XGLUE [6] and XTREME\n[7] expanded these concepts to multilingual contexts, allowing learning of cross-lingual transfer.\nNep-gLUE [2] is the ﬁrst comprehensive benchmark for Natural Language Understanding (NLU) tasks in Nepali. It in-\ncludes four core tasks: Named Entity Recognition (NER), Part-of-Speech Tagging (POS), Content Classiﬁcation (CC),\nand Categorical Pair Similarity (CPS). Although Nep-gLUE offers a robust foundation with its multitask dataset, it\nfalls short in addressing more advanced NLP tasks necessary for comprehensive evaluations of models at the linguistic\nlevel. Advanced and complex tasks are crucial for further progress in low-resource languages such as Nepali.\n2\nPREPRINT - DECEMBER 2, 2024\nNepali Sentiment Analysis (NepSA) [8] is a targeted aspect-based sentiment analysis dataset, comprising 3,068 com-\nments extracted from 37 YouTube videos across 9 channels. The dataset is annotated using a binary sentiment polarity\nschema across six aspects: General, Profanity, Violence, Feedback, Sarcasm, and Out-of-scope. Another dataset,\nAspect-Based Sentiment Analysis [9], contains 1,576 sentences, equally divided between positive and negative senti-\nments. Additionally, sentiment analysis datasets like Nepali Language Sentiment Analysis - Movie Reviews [10], 602\ndata points, and Nepali Sentiment Analysis [11], 2,161 data points found on Kaggle, are limited in size and domain\nspeciﬁc. For our benchmark, we used the NepCOV19Tweets dataset [12], which includes 33.5k sentiments labeled as\npositive, negative, or neutral. We selected 14.9k positive and 13.5k negative data points for sentiment analysis. A more\nrecent dataset, Sentiment of Election-Based Nepali Tweets [13], contains 17.8k tweets but includes English charac-\nters and numbers, making it less suitable for our benchmark dataset. And there are no publicly available datasets for\nco-reference resolution, acceptability judgment, or paraphrase detection in the Nepali language. While some studies\nhave explored aspects of Nepali grammar, the absence of datasets for advanced tasks represents a signiﬁcant gap of\nresources.\n3\nTasks\nNLUE is a benchmark designed to evaluate the performance of language understanding models across a variety of\ntasks, building on the foundation established by its predecessor, Nep-gLUE. The objective of NLUE is to provide\na robust evaluation metric applicable to a broad range of language understanding challenges. We describe the tasks\nbelow and in Table 1.\n3.1\nSingle-Sentence Tasks\nSingle-sentence tasks in the NLUE benchmark focus on assessing a model’s ability to understand and analyze in-\ndividual sentences. These tasks evaluate a model’s ability to understand and interpret the meaning, sentiment, and\ngrammatical structure of individual sentences.\n3.1.1\nSA\nA sentiment analysis dataset has been added to evaluate models’ ability to classify the emotional tone (positive, neg-\native) of Nepali text. We created the dataset for sentiment analysis by translating Stanford Sentiment Treebank [14]\nwhich consists of sentences from movie reviews and human annotations of their sentiment from the GLUE Benchmark\nusing using GPT-4o-mini [4], and manually translating instances that could not be accurately translated. It has 51k\ndata points and is approximately equally divided between two classes, positive and negative sentiment, and uses only\nsentence-level labels. We incorporated this dataset with pre-existing sentiment analysis of Nepali COVID-19-related\ntweets [12], with 15k data points for each positive and negative sentiment. In total, the dataset has 81k data points,\nsplit equally between both classes. The evaluation metrics are Accuracy and F1 score.\n3.1.2\nCoLA\nThis dataset tests the model’s ability to distinguish grammatically correct and incorrect sentences in Nepali. The task\ninvolves determining whether a given sentence follows the linguistic rules of Nepali, ensuring the model can assess\ngrammaticality. To create the acceptability judgments dataset, we translated the Corpus of Linguistic Acceptability\n(CoLA)[15] into Nepali which consists of judgments drawn from books and journal articles on linguistic theory from\nthe GLUE Benchmark using using gpt-4o-mini [4]. For sections that were not translated correctly, we relied on manual\ntranslation. It has 9.5k data points, with both (correct/incorrect) classes.\n3.2\nSimilarity and Paraphrase Task\nSimilarity and Paraphrase Task in the NLUE benchmark evaluates a model’s ability to determine whether two sen-\ntences convey the same meaning or are paraphrases of each other. By focusing on this aspect of language compre-\nhension, these tasks provide valuable insights into a model’s proﬁciency in handling diverse expressions of similar\nideas.\n3.2.1\nQQP\nWe have introduced a paraphrase detection dataset to assess whether models can correctly determine whether two\nNepali sentences convey the same meaning. The Quora Question Pairs [16] dataset is a collection of question pairs\nfrom the community question-answering website Quora. Using GPT-4o-mini,[16] we translated the Quora Question\n3\nPREPRINT - DECEMBER 2, 2024\nPairs from the GLUE Benchmark into Nepali to create a paraphrase detection dataset. The class distribution of para-\nphrase detection is almost balanced, and we report F1 score.\n3.2.2\nMRPC\nWe have introduced another paraphrase detection dataset based on the Microsoft Research Paraphrase Corpus [17].\nThe MRPC dataset, originally developed by Microsoft, consists of sentence pairs extracted from news articles, is used\nto identify whether the sentences are paraphrases of each other. Using GPT-4o-mini, we translated the MRPC dataset\ninto Nepali to create a paraphrase detection dataset for evaluation. The class distribution of this dataset is 70-30, with\na higher proportion of paraphrase pairs, and we report an F1 score to evaluate model performance.\n3.3\nInference Tasks\nThe NLI tasks in this benchmark, assess a model’s ability to understand relationships between sentences, such as en-\ntailment, contradiction, and neutral alignment. These tasks are crucial because they evaluate a model’s comprehension\nof contextual meaning, logical inference, and its ability to handle complex linguistic structures, making them essential\nfor advancing robust language understanding.\n3.3.1\nCR\nThis dataset tests the model’s ability to resolve coreference relationships within a Nepali text. We developed the\nconference resolution dataset by manually translating the Winograd Schema Challenge [18], which is a reading com-\nprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from\na list of choices. To convert the problem into sentence pair classiﬁcation, sentence pairs are constructed by replacing\nthe ambiguous pronoun with each possible referent. The task is to predict if the sentence with the pronoun substituted\nis entailed by the original sentence. The training set has 635 data points and the test set has 71 data points, balanced\nbetween two classes, evaluated with F1 score.\n3.3.2\nMNLI\nThe dataset is translated into Nepali from the Stanford Natural Language Inference Corpus [19] using GPT-4o-mini.\nThis corpus is a crowd-sourced collection of sentence pairs annotated with textual entailment labels. Each pair consists\nof a premise and a hypothesis, and the task is to predict the relationship between them—whether the premise entails\nthe hypothesis (entailment), contradicts it (contradiction), or is unrelated (neutral).\n3.3.3\nQA/NLI\nThe QA/NLI (Question-answering Natural Language Inference) dataset has been adapted for Nepali from GLUE\nbenchmark by translating the original English dataset using GPT-4o-mini which originates from the Stanford Question\nAnswering Dataset [20] that contains question-paragraph pairs sourced from Wikipedia. This dataset evaluates a\nmodel’s ability to determine whether a context sentence contains the answer to a given question, framed as a sentence-\npair classiﬁcation task. The dataset has an equal division between entailment and non-entailment pairs, ensuring\nbalanced class distribution.\n3.3.4\nRTE\nThe Recognizing Textual Entailment (RTE) dataset for this benchmark is converted to Nepali by translating the GLUE\nbenchmark’s RTE dataset using GPT-4o-mini. The dataset evaluates a model’s ability to predict whether a hypothesis\nlogically follows from a given premise, framed as a two-class classiﬁcation task. For consistency, examples are\ncategorized into entailment and not entailment, collapsing the neutral and contradiction classes into the latter where\napplicable, securing balanced class distribution.\n4\nResult\nWe conducted ﬁne-tuning and evaluation across all tasks using a range of hyperparameter combinations to ensure com-\nprehensive analysis. Speciﬁcally, we experimented with learning rates between 1e-5 and 5e-5, freezing and unfreezing\nﬁnal four layers, and trained for 4 to 10 epochs. For each task, we selected the best-performing model on the test set\nbased on its evaluation metrics. Scores for each model across each task is provided in Table 2.\n4\nPREPRINT - DECEMBER 2, 2024\nOur results demonstrate that the models generally perform well on simpler tasks, such as Named Entity Recognition\n(NER) and Part-of-Speech (POS) tagging, where patterns in the data are easier to learn and do not require deep con-\ntextual understanding. However, as the complexity of the tasks increases, such as in tasks requiring reasoning or\ncontextual inference (e.g., Natural Language Inference), model performance declines signiﬁcantly. This gap is partic-\nularly pronounced in tasks where the available ﬁne-tuning data is limited, further emphasizing the models’ inability to\ngeneralize effectively when trained on smaller datasets.\nThis trend highlights a critical limitation in the current Nepali language models. While they can excel in tasks with\nabundant data and straightforward structures, their performance struggles to scale for tasks demanding complex rea-\nsoning or where training data is sparse.\nModel\nPARAMS\nNER\nPOS\nCC\nCPS\nSA\nCoLA\nQQP\nMPRC\nMNLI\nQA/NLI\nRTE\nCR\nmultilingual BERT [21]\n172M\n85.45\n94.65\n91.08\n93.60\n86.711\n80.803\n78.16\n70.14\n74.45\n78.56\n63.90\n45.77\nXLM-Rbase [22]\n270M\n87.59\n94.88\n92.33\n93.65\n88.732\n81.776\n77.73\n69.22\n76.42\n81.22\n56.11\n47.513\nNepBERT [23]\n110M\n79.12\n90.63\n90.98\n91.05\n87.565\n81.175\n72.01\n71.55\n71.28\n79.37\n55.81\n54.921\nNepaliBERT [24]\n110M\n82.45\n91.67\n90.10\n89.46\n83.421\n80.974\n66.46\n69.31\n71.59\n79.28\n52.4\n49.2\nNepBERTa [2]\n110M\n91.09\n95.56\n93.13\n94.42\n84.438\n80.656\n74.42\n71.29\n72.80\n80.3\n57.72\n52.198\nBERT Nepali [25]\n110M\n93.57\n96.94\n94.47\n95.72\n87.901\n81.646\n75.28\n70.38\n74.66\n80.29\n52.43\n58.816\nRoBERTa Nepali [25]\n125M\n93.74\n97.52\n94.68\n96.49\n88.33\n21.56\n78.43\n69.87\n75.78\n80.86\n54.64\n47.21\nTable 2: Scores of each model across twelve evaluation tasks\n5\nAnalysis\nBoth monolingual and multilingual models show impressive performance on simpler tasks like named entity recogni-\ntion (NER), part-of-speech tagging (POS), and content classiﬁcation, which primarily depend on token-level or lexical\npatterns. However, their effectiveness drops when given more complex challenges, such as natural language inference\n(NLI) and paraphrase detection, which requires contextual understanding. This suggests that although these models\nare good at identifying linguistic features, they frequently encounter difﬁculties in tasks that require understanding,\nlogical inference or adaptation to speciﬁc domains. The result shows us the signiﬁcant limitations in generalization\ncapabilities of all models. Tasks such as RTE and WNLI, which span diverse domains like Wikipedia and ﬁction, and\ncontain fewer examples to ﬁnetune, reveal notable performance gaps, highlighting the critical need for more diverse\nand representative training corpora.\n6\nConclusion\nIn this paper, we created a new benchmark (NLUE), extending the Nep-gLUE benchmark by adding eight new tasks,\nresulting in a total of twelve tasks. Through our experiments, we found that while there have been a few models devel-\noped for Nepali, their ability to generalize in complex linguistic scenarios is still limited. Speciﬁcally, when ﬁne-tuned\non datasets of limited size, these models often underperform, highlighting challenges in their robustness across diverse\nlanguage use cases. A key factor contributing to this limitation can be the heavy reliance on news data for training,\nwhich does not fully capture the variety and richness of the Nepali language. As a result, models trained primarily\non news data struggle to generalize effectively when evaluated on tasks or domains they have not encountered before.\nFuture work should focus on diversifying the training data and exploring novel methods to improve generalization\nacross various tasks and domains. We believe that these benchmarks will serve as foundation for future research in\nLanguage Models.\nReferences\n[1] Bal Krishna Bal. Structure of Nepali Grammar. 2004.\n[2] Sulav Timilsina, Milan Gautam, and Binod Bhattarai. Nepberta: Nepali language model trained in a large corpus.\nIn Proceedings of the 2nd Conference of the Asia-Paciﬁc Chapter of the Association for Computational Linguis-\ntics and the 12th International Joint Conference on Natural Language Processing. Association for Computational\nLinguistics (ACL), 2022.\n[3] Alex Wang. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\n[4] OpenAI. Gpt-4o mini: Advancing cost-efﬁcient intelligence., 2024.\n5\nPREPRINT - DECEMBER 2, 2024\n[5] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems,\n2020.\n[6] Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti,\nYing Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and\nMing Zhou. XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation. In\nBonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 6008–6018, Online, November 2020. Association for\nComputational Linguistics.\n[7] Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. Xtreme: A\nmassively multilingual multi-task benchmark for evaluating cross-lingual generalization, 2020.\n[8] Oyesh Mann Singh, Sandesh Timilsina, Bal Krishna Bal, and Anupam Joshi. Aspect based abusive sentiment\ndetection in nepali social media texts. In 2020 IEEE/ACM International Conference on Advances in Social\nNetworks Analysis and Mining (ASONAM), pages 301–308, 2020.\n[9] Sujan Tamrakar, Bal Krishna Bal, and Rajendra Thapa. Aspect Based Sentiment Analysis of Nepali Text Using\nSupport Vector Machine and Naive Bayes. PhD thesis, 10 2020.\n[10] Shikhar Ghimire. Nepali language sentiment analysis - movie reviews.\n[11] Mahesh Acharya. Nepali language sentiment analysis.\n[12] Chiranjibi Sitaula, Anish Basnet, Ashish Mainali, and Tej Shahi. Deep learning-based methods for sentiment\nanalysis on nepali covid-19-related tweets. Computational Intelligence and Neuroscience, 2021, 11 2021.\n[13] Durga Pokharel. Sentiment of election based nepali tweets.\n[14] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher\nPotts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Tim-\nothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference\non Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, October\n2013. Association for Computational Linguistics.\n[15] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Transac-\ntions of the Association for Computational Linguistics, 7:625–641, 2019.\n[16] Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. First quora dataset release: Question pairs. QuoraData,\n2017.\n[17] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Pro-\nceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.\n[18] Hector J. Levesque, Ernest Davis, and L. Morgenstern. The winograd schema challenge.\nIn AAAI Spring\nSymposium: Logical Formalizations of Commonsense Reasoning, 2011.\n[19] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus\nfor learning natural language inference, 2015.\n[20] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\ncomprehension of text, 2016.\n[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186. Association for Computational Linguistics, 2019.\n[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual repre-\nsentation learning at scale. arXiv preprint arXiv:1911.02116v2, 2020.\n[23] Shushanta Pudasaini, Subarna Shakya, Aakash Tamang, Sajjan Adhikari, Sunil Thapa, and Sagar Lamichhane.\nNepalibert: Pre-training of masked language model in nepali corpus. In 7th International Conference on IoT in\nSocial, Mobile, Analytics and Cloud, 2023.\n[24] Rajan. Nepalibert, 2021.\n[25] Prajwal Thapa, Jinu Nyachhyon, Mridul Sharma, and Bal Krishna Bal. Development of pre-trained transformer-\nbased models for the nepali language, 2024.\n6\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-11-28",
  "updated": "2024-11-28"
}