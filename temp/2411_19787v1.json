{
  "id": "http://arxiv.org/abs/2411.19787v1",
  "title": "CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives",
  "authors": [
    "Armin Saghafian",
    "Amirmohammad Izadi",
    "Negin Hashemi Dijujin",
    "Mahdieh Soleymani Baghshah"
  ],
  "abstract": "Grounding the instruction in the environment is a key step in solving\nlanguage-guided goal-reaching reinforcement learning problems. In automated\nreinforcement learning, a key concern is to enhance the model's ability to\ngeneralize across various tasks and environments. In goal-reaching scenarios,\nthe agent must comprehend the different parts of the instructions within the\nenvironmental context in order to complete the overall task successfully. In\nthis work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a\nnew framework to solve this problem using auxiliary loss functions inspired by\nvideo-text retrieval literature and a novel method called instruction tracking,\nwhich automatically keeps track of progress in an environment. The results of\nour experiments suggest superior sample efficiency and systematic\ngeneralization for this framework in multi-modal reinforcement learning\nproblems. Our code base is available here.",
  "text": "CAREL: Instruction-guided reinforcement learning with\ncross-modal auxiliary objectives\nArmin Saghafian1Y, Amirmohammad Izadi1Y, Negin Hashemi Dijujin1Y,\nMahdieh Soleymani Baghshah1*,\n1 Department of Computer Engineering, Sharif University of Technology, Tehran, Iran\nYThese authors contributed equally to this work.\n* correspondingauthor: soleymani@sharif.edu\nAbstract\nGrounding the instruction in the environment is a key step in solving language-guided\ngoal-reaching reinforcement learning problems. In automated reinforcement learning, a\nkey concern is to enhance the model’s ability to generalize across various tasks and\nenvironments. In goal-reaching scenarios, the agent must comprehend the different parts\nof the instructions within the environmental context in order to complete the overall\ntask successfully. In this work, we propose CAREL (Cross-modal Auxiliary\nREinforcement Learning) as a new framework to solve this problem using auxiliary loss\nfunctions inspired by video-text retrieval literature and a novel method called\ninstruction tracking, which automatically keeps track of progress in an environment.\nThe results of our experiments suggest superior sample efficiency and systematic\ngeneralization for this framework in multi-modal reinforcement learning problems. Our\ncode base is available here.\nIntroduction\nNumerous studies have examined the use of language goals or instructions within the\ncontext of reinforcement learning (RL) [1–3]. Language goals typically provide a\nhigher-level and more abstract representation than goals derived from the state\nspace [4]. While state-based goals often specify the agent’s final expected goal\nrepresentation [5,6], language goals offer more information about the desired sequence\nof actions and the necessary subtasks [5]. Therefore, it is important to develop\napproaches that can extract concise information from states or observations and\neffectively align it with textual information, a process referred to as grounding [1].\nPrevious research has attempted to ground instructions in observations or states\nusing methods such as reward shaping [7,8] or goal-conditioned policy/value\nfunctions [9–12], with the latter being a key focus of many studies. Their approaches\nincorporate various architectural or algorithmic inductive biases, such as\ncross-attention [13], hierarchical policies [14,15], and feature-wise modulation [16,17].\nTypically, these works involve feeding instructions and observations into policy or value\nnetworks, extracting internal representations of tokens and observations at each time\nstep, and propagating them through the network. Previous studies have explored\nauxiliary loss functions to improve these internal representations in RL [18–20], and\nhave emphasized the importance of self-supervised/unsupervised learning objectives [21]\nin RL. However, these loss functions lack the alignment property between different\nPLOS ONE PrePrint\n1/10\narXiv:2411.19787v1  [cs.LG]  29 Nov 2024\ninput modalities, such as visual/symbolic states and textual commands/descriptions.\nRecent studies have suggested contrastive loss functions to align text and vision\nmodalities in an unsupervised manner [22–26]. Most of these studies fall under the\nvideo-text retrieval literature [22,27], where the language tokens and video frames align\nat different granularities. Since these methods require a corresponding textual input\nalong with the video, the idea has not yet been employed in language-informed\nreinforcement learning, where the sequence of observation might not always match the\ntextual modality (due to action failures or inefficacy of trials). One can leverage the\nsuccess signal or reward to detect the successful episodes and consider them aligned to\nthe textual modality containing instructions or environment descriptions. Doing so, the\napplication of the abovementioned auxiliary loss functions makes sense.\nIn this study, we propose a new framework, called CAREL (Cross-modal Auxiliary\nREinforcement Learning), for the adoption of auxiliary grounding objectives from the\nvideo-text retrieval literature [27], particularly X-CLIP [22], to enhance the learned\nrepresentations within these networks and improve cross-modal grounding at different\ngranularities. By leveraging this grounding objective, we aim to improve the grounding\nbetween language instructions and observed states by transferring the multi-grained\nalignment property of video-text retrieval methods to instruction-following agents. We\nalso propose a novel method to mask the accomplished parts of the instruction via the\nauxiliary score signals calculated for the cross-modal loss while the episode progresses.\nThis helps the agent to focus on the remaining parts of the task without repeating\npreviously done sub-tasks or being distracted by past goal-relevant entities in the\ninstruction. Our experiments on the BabyAI environment [17] showcase the idea’s\neffectiveness in improving the systematic generalization and sample efficiency of\ninstruction-following agents. The primary contributions of our work are outlined as\nfollows:\n• We designed an auxiliary loss function to improve cross-modal grounding between\nlanguage instructions and environmental observations.\n• We introduced a novel instruction tracking mechanism to help the agent focus on\nthe remaining tasks by preventing the repetition of completed sub-tasks.\n• We enhanced overall performance and sample efficiency in two benchmarks.\nMethods\nIn this study, we incorporate an auxiliary loss inspired by the X-CLIP model [22] to\nenhance the grounding between instruction and observations in instruction-following RL\nagents. This auxiliary loss serves as a supplementary objective, augmenting the primary\nRL task with a multi-grained alignment property which introduces an additional\nlearning signal to guide the model’s learning process. This design choice was motivated\nby the need to improve the model’s ability to extract meaningful information from its\nobservations and align it more effectively with the intended instruction, ultimately\nenhancing the overall performance of the RL system. We also leverage the alignment\nscores calculated within the X-CLIP loss to track the accomplished sub-tasks and mask\ntheir information from the instruction. This masking aims to filter out the distractor\nparts of the instruction and focus on the remaining parts, hopefully improving the\noverall sample efficiency of the agents. We call this technique instruction tracking. In\nthe remainder of this section, we explain the auxiliary loss and the instruction tracking\nseparately.\nPLOS ONE PrePrint\n2/10\nPut the red ball next to the purple box.\nMemory\nObs. Global Aggregator\nLang. Global Aggregator\nPolicy/Value\nHeads\nObs.\nEncoder\nE-I Score\nE-W Value Vec.\nO-W Value Vec.\nO-I Value Vec.\nAOSM\nE-W Score\nO-I Score\nO-W Score\nAuxiliary Loss\nInstruction Masking\nInstruction\nTracking via\nFig 1. Overall view of CAREL. In this figure, we showcase CAREL over a candidate baseline model from [17]. (Left) The\nblue box handles the (masked) instruction and its local/global representations, while the pink box contains the components\nrelated to observation. (Right) The purple box shows the calculation steps for the X-CLIP loss and tracks scores for\ninstruction masking.\nAlgorithm 1 CAREL framework\n1: Encode instruction (v1, v2, ..., vm) = TextEncoder(I1, I2, ..., Im)\n2: Split instruction into sub-tasks by rule-based heuristics C = {ci}\n3: for t = 1 to max step do\n4:\nCompute each sub-task’s (ci) similarity with this step’s observation (Ot)\nSt\nci = avg(XtV T\nci )\n5:\nMaintain average scores over time { ˜Sci}C\ni=1\n6:\nDetect sudden score increases compared to the average, Sj\nci > ˜Sci × k\n7:\nOmit detected sub-task (ci) from the instruction with a probability p\n8: end for\n9: Collect Trajectories B\n10: Gather Instruction and Observation Embeddings (v1, ..., vm) and (x1, ...xn)\n11: Calculate CAREL loss based on Equations 1 to 12\n12: Add the auxiliary loss to the RL loss\nAuxiliary Loss\nWe calculate the proposed loss function over the successful episodes generated by an\narbitrary instruction-conditioned RL model within a batch of online trials. To avoid the\nmodel being influenced by goal-unrelated behavioral patterns in unsuccessful\ntrajectories, we exclude those trajectories from consideration and leverage reward values\nto organize only successful ones into a separate batch for the auxiliary loss. This\nseparation is done only for the auxiliary loss, and the overall RL loop is run over all\ninteractions, whether successful or unsuccessful. Hence, it differs from offline RL in\nwhich only certain episodes are selected for the whole training process [28].\nEach successful episode contains a sequence of observation-action pairs\nep = ([O1, a1]..., [On, an]) meeting the instructed criteria and an accompanying\ninstruction instr = (I1, ..., Im) with m tokens. Since the X-CLIP loss requires local and\nglobal encoders for each modality, we must choose such representations from the model\nor incorporate additional modules to extract them. To explore the exclusive impact of\nthe auxiliary loss and minimize any changes to the architecture, we use the model’s\nPLOS ONE PrePrint\n3/10\nexisting observation and instruction encoders, which are crucial components of the\nmodel itself. We utilize these encoders to extract local representations for each\nobservation-action [Ot, at] denoted as xt ∈Rd×1, t = 1, ..., n in which each action is\nembedded similar to positional embedding in Transformers [29] and is added to the\nobservation representation. Each instruction token Ii is encoded as vi ∈Rd×1,\ni = 1, ..., m. The global representations can be chosen from the model itself or added to\nthe model by aggregation techniques such as mean-pooling or attention. We denote the\nglobal representations for observations and the instruction by ˜x and ˜v, respectively. The\nauxiliary loss function is then calculated according to [22] as below. We restate the\nformulas in our context to make this paper self-contained.\nTo utilize contrastive loss, we first need to calculate the similarity score for each\nepisode (ep), a sequence of observations, and an instruction (instr) pair denoted as\ns(ep, instr). To do this, we calculate four separate values; Episode-Instruction (SE−I),\nas well as Episode-Word (SE−W ), Observation-Instruction (SO−I) and\nObservation-Word (SO−W ) similarity values. Episode-Instruction score can be\ncalculated using this formula:\nSE−I = ˜xT ˜v,\n(1)\nwith ˜x, ˜v ∈Rd×1, SE−I ∈R. Other values are calculated similarly:\nSE−W = (V ˜x)T ,\n(2)\nSO−I = X˜v,\n(3)\nSO−W = XV T ,\n(4)\nwhere X = (xT\n1 ; ...; xT\nn) ∈Rn×d is local representation for observations,\nV = (vT\n1 ; ...; vT\nm) ∈Rm×d in local representations for instruction tokens, and\nSE−W ∈R1×m, SO−I ∈Rn×1 and SO−W ∈Rn×m provide fine-granular similarities\nbetween the language instruction and the episode of observation. These values are then\naggregated with appropriate attention weights via a technique called Attention Over\nSimilarity Matrix (AOSM). Episode-Word (S′\nE−W ) and Observation-Instruction\n(S′\nO−I) scores are calculated from the values as follows:\nS′\nO−I = Softmax(SO−I[., 1])T SO−I[., 1],\n(5)\nS′\nE−W = Softmax(SE−W [1, .])T SE−W [1, .],\n(6)\nwhere:\nSoftmax(x[.]) =\nexp(x[.]/τ)\nP\nj exp(x[j]/τ),\n(7)\nin which, τ controls the softmax temperature. For the Observation-Word score, bi-level\nattention is performed, resulting in two fine-grained similarity vectors. These vectors\nare then converted to scores similar to the previous part:\nS′\ninstr[i, 1] = Softmax(SO−W [i, .])T SO−W [i, .],\ni ∈{1, ..., n},\n(8)\nS′\nep[1, i] = Softmax(SO−W [., i])T SO−W [., i]\ni ∈{1, ..., m},\n(9)\nwhere S′\ninstr ∈Rn×1 show the similarity value between the instruction and n\nobservations in the episode and S′\nep ∈R1×m represents the similarity value between the\nepisode and m words in the instruction.\nThe second attention operation is performed on these vectors to calculate the\nObservation-Word similarity score (S′\nO−W ):\nS′\nO−W = (Softmax(S′\nep[1, .])T S′\nep[1, .] + Softmax(S′\ninstr[., 1])T S′\ninstr[., 1])/2.\n(10)\nPLOS ONE PrePrint\n4/10\nThe final similarity score between an episode and an instruction is computed using\nthe previously calculated scores:\ns(ep, instr) = (SE−I + S′\nE−W + S′\nO−I + S′\nO−W )/4.\n(11)\nThis method takes into consideration both fine-grained and coarse-grained contrasts.\nConsidering N episode-instruction pairs in a batch of successful trials, the auxiliary loss\nis calculated as below:\nLaux = −1\nn\nN\nX\ni=1\n(log\nexp(s(epi, instri))\nPN\nj=1 exp(s(epi, instrj))\n+ log\nexp(s(epi, instri))\nPN\nj=1 exp(s(epj, instri))\n)\n(12)\nThe total objective is calculated by adding this loss to the primary RL loss, LRL,\nwith a coefficient of λC.\nLtotal = LRL + λC.Laux\n(13)\nThe overall architecture of a base model [17] and the calculation of the auxiliary loss\nis depicted in Figure 1. If the shape of the output representations from the observation\nand instruction encoders does not align, we employ linear transformation layers to bring\nthem into the same feature space. This transformation is crucial as it facilitates the\ncalculation of similarity between these representations within our loss function.\nInstruction Tracking\nWe can consider the similarities from eqs. 1 to 4 as a measure of matching between the\ninstruction and the episode at different granularities. Once calculated at each time step\nof the episode, this matching can signal the agent about the status of the sub-task\naccomplishments. The agent then can be guided toward the residual goal by masking\nthose sub-tasks from the instruction. More precisely, at time step t of the current\nepisode, the agent has seen a partial episode ep(t) = ([O1, a1], ..., [Ot, at]) that in a fairly\ntrained model should align with initial stages of the instruction. The instruction itself\ncan be parsed into a set of related sub-tasks C = {ci} via rule-based heuristics, and\nthere can be constraints on their interrelations. For example, an instruction of the form\n”Do X, then do Y, then do Z” includes three sub-tasks X, Y, and Z which have a\nsequential order constraint (X →Y →Z). Other examples could involve different forms\nof directed graphs where a specific sub-task is acceptable only if its parents have been\nsatisfied before during the episode. The set of acceptable sub-tasks at time step t is\ndenoted by Ct, which contains the root nodes in the dependency graphs at the start of\nthe episode.\nIn order to track the accomplished sub-task, we assess the similarity between C\nmembers and the partial episode. This can be done by tracking SE−W or SO−W , which\nprovides fine-grained similarities across the language modality. In the case of SE−W ,\nthe similarity per token in ci is averaged to get a final scalar similarity. For SO−W , the\nmaximum similarity between the observations and each word is considered for averaging\nacross ci tokens. Another option is to calculate a learned representation for the whole ci\ninstead of averaging and to track the instructions based on its similarity with the partial\nepisode, aiming at preserving the contextual information in the representation of the\nsub-task. The final calculated similarity of each acceptable sub-task ci, denoted by St\nci,\nis tracked at each time step. Once this similarity rises significantly, the matching is\ndetected, and ci is removed from the instruction participating in the\nlanguage-conditioned model. More precisely, we remove ci from the instruction when\nthe following condition is satisfied:\nPLOS ONE PrePrint\n5/10\n(ci ∈Ct)\n∧\n(St\nci ≥k ×\n1\nt −1\nt−1\nX\nj=1\nSj\nci).\n(14)\nHere, k > 1 is a hyperparameter that specifies the significance of the matching\nscore’s spike. While the auxiliary loss described in the previous subsection is applied on\nthe episode level, instruction tracking happens at every time step of the episode over the\npartial episode and the masked instruction.\nThis overall process is represented in Figure 1. These two techniques can be applied\njointly, as the auxiliary loss improves the similarity scores through time, and the\nimproved similarities enhance the instruction tracking. To prevent false positives during\ntracking at the initial epochs of training, one can constrain the probability of masking\nand relax this constraint gradually as the learning progresses.\nExperiments\nIn our experiments, we conducted a comparative analysis to assess the impact of\nX-CLIP [22] auxiliary loss on generalization and sample efficiency of\ninstruction-following agents. We showcase the success of CAREL along with the\ninstruction tracking technique in our experiments1. For this purpose, we employ two\nbaselines called BabyAI [17] (the proposed model along with the BabyAI benchmark)\nand SHELM [30] for which we explain the experimental setup and results in the\nfollowing paragraphs.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nFrames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate (SR)\nGoToSeqS5R2\nBabyai + CAREL (attention)\nBabyai + CAREL (mean)\nBabyai\n(a)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate (SR)\nOpenDoorsOrderN4\nBabyai + CAREL (attention)\nBabyai + CAREL (mean)\nBabyai\n(b)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nFrames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate (SR)\nPickupLoc\nBabyai + CAREL (attention)\nBabyai + CAREL (mean)\nBabyai\n(c)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nFrames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate (SR)\nPutNextLocalS6N4\nBabyai + CAREL (attention)\nBabyai + CAREL (mean)\nBabyai\n(d)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nFrames\n1e7\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nSuccess Rate (SR)\nSynthS5R2\nBabyai + CAREL (attention)\nBabyai + CAREL (mean)\nBabyai\n(e)\nFig 2. Test SRs indicating the overall effect of Vanilla CAREL on BabyAI.\n1For the experiments reported in this paper, we have used one NVIDIA 3090 GPU and one TITAN\nRTX GPU over two weeks.\nPLOS ONE PrePrint\n6/10\nVanilla CAREL Results\nWe employ the BabyAI environment [17], a lightweight but logically complex\nbenchmark with procedurally generated difficulty levels, which enables in-depth\nexploration of grounded language learning in the goal-conditioned RL context. We use\nBabyAI’s baseline model as the base model and minimally modify its current structure.\nWord-level representations are calculated using a simple token embedding layer. Then, a\nGRU encoder calculates the global instruction representation. Similarly, we use the\nmodel’s default observation encoder, a convolutional neural network with three\ntwo-dimensional convolution layers. All observations pass through this encoder to\ncalculate local representations. Mean-pooling/Attention over these local representations\nis applied as the aggregation method to calculate the global observation representation.\nThe RL agent is trained using the PPO algorithm [31] and Adam optimizer with\nparameters β1 = 0.9 and β2 = 0.999. The learning rate is 7e −4, and the batch size is\n256. We set λC = 0.01 and the temperature τ = 1 as CAREL-specific hyperparameters.\nTo minimize the changes to the baseline model updates, we backpropagate the gradients\nin an outer loop of PPO loss to be able to capture episode-level similarities. This\ngradient update with different frequencies has been tried in the literature before [16].\nThe evaluation framework for this work is based on systematic generalization to\nassess the language grounding property of the model. We report the agent’s success rate\n(SR) over a set of unseen tasks at each BabyAI level, separated by pairs of color and\ntype of target objects or specific orders of objects in the instruction. This metric is\nrecorded during validation checkpoints throughout training.\nFigure 2 illustrates the improved sample efficiency brought about by CAREL\nauxiliary loss (without instruction tracking and action embedding to minimize the\nmodifications to the baseline model, hence called Vanilla CAREL). All results are\nreported over two random seeds. The results indicate improved sample efficiency of\nCAREL methods across all levels, especially those with step-by-step solutions that\nrequire the alignment between the instruction parts and episode interactions more\nexplicitly, namely GoToSeq and OpenDoorsOrder which contain a sequence of\nOpen/GoTo subtasks described in the instruction. The generalization is significantly\nimproved in more complex tasks, i.e., Synth.\nInstruction Tracking Results\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nFrames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate (SR)\nGoToSeqS5R2\nBabyai + CAREL\nBabyai + CAREL + IT\nBabyai\n(a)\n0\n1\n2\n3\n4\n5\n6\n7\nFrames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate (SR)\nOpenDoorsOrderN4\nBabyai + CAREL\nBabyai + CAREL + IT\nBabyai\n(b)\n0\n1\n2\n3\n4\n5\nFrames\n1e6\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nMean Reward\nMiniGrid-PutNear-6x6-N2-v0\nSHELM + CAREL + IT\nSHELM\n(c)\nFig 3. Test SRs indicating the effect of CAREL + instruction tracking on the baseline models. (a) and (b) are based on\nBabyAI baseline while (c) compares SHELM to CAREL + instruction tracking.\nFor instruction tracking, We use only the SE−W vector and average over tokens of\neach sub-task to track the score over time. To detect sub-task matching from the score\nsignal, we set k = 2 in Equation 14. All the other settings are kept the same as in\nvanilla CAREL, except that we also add action embeddings to local observation\nPLOS ONE PrePrint\n7/10\nrepresentations, as described in the Auxiliary Loss section. We mask acceptable\nsub-tasks with a certain possibility which follows a hyperbolic tangent function in terms\nof training steps (p = 0.8 × tanh (2 × step/maxsteps) + 0.01) where maxsteps is the\ntotal number of training frames. This is meant to minimize the amount of masking at\nthe start of the learning process when the model has not yet learned a good embedding\nfor instructions and observations and increase it over time.\nTo evaluate the capability of our framework on RGB environments, we apply and\ntest it on SHELM [30]. SHELM leverages the knowledge hidden in pre-trained models\nsuch as CLIP and Transformer-XL. It also uses CLIP to extract textual tokens related\nto every observation. Then these tokens are passed through the frozen Transformer-XL\nnetwork to form a memory of tokens throughout the episode. This hidden memory is\nthen concatenated to a CNN representation of observation and passed to actor/critic\nheads. We must modify SHELM’s structure as it doesn’t use the environment’s\ninstructions, which are crucial to success in a multi-goal setting. To do so, we utilize\nBERT’s tokenizer to embed the instructions and pass them through a\nMultihead-Attention layer with four heads. The resulting embedding is concatenated to\nthe hidden layer alongside the outputs of the CNN model and Transformer-XL, which\nare then passed to the actor-critic head.\nThe results of the full CAREL method (with instruction tracking and action\nembedding) are reported on the PutNear environment. We break down the instructions\nin this environment with a rule-based parsing to increase the level of detail in the\ninstruction. The instruction, stated initially as ”put the [obj1] near the [obj2]”,\nis converted to ”go to the [obj1], then pick up the [obj1], then go to\n[obj2]” and so on. This introduces the challenge of sequential sub-tasks into SHELM\ntasks. We consider the CLIP output for observations as the local representations and\nadd another Multi-head Attention layer followed by a mean-pooling over them to\ncalculate the corresponding global representations. We train the learnable parts of the\nmodel using the PPO algorithm and Adam optimizer with the same hyperparameters.\nThe learning rate is 1e −4, and the batch size is set to 16. The results in Figure 3\nindicate that instruction tracking improves CAREL, especially in the case of RGB\ninputs coming from more complex tasks.\nConclusion\nThis paper proposes the CAREL framework which adopts auxiliary cross-modal\ncontrastive loss functions to the multi-modal RL setting, especially instruction-following\nagents. The aim is to improve the multi-grained alignment between different modalities,\nleading to superior grounding in the context of learning agents. We apply this method\nto existing instruction-following agents. The results indicate the sample efficiency and\ngeneralization boost from the proposed framework. As for the future directions of this\nstudy, we suggest further experiments on more complex environments and other\nmulti-modal sequential decision-making agents. Also, the instruction tracking idea\nseems to be a promising direction for further investigation.\nReferences\n1. R¨oder F, ¨Ozdemir O, Nguyen PD, Wermter S, Eppe M. The embodied\ncrossmodal self forms language and interaction: a computational cognitive review.\nFrontiers in psychology. 2021;12:716671.\nPLOS ONE PrePrint\n8/10\n2. Geffner H. Target languages (vs. inductive biases) for learning to act and plan.\nIn: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36; 2022.\np. 12326–12333.\n3. Luketina J, Nardelli N, Farquhar G, Foerster J, Andreas J, Grefenstette E, et al.\nA survey of reinforcement learning informed by natural language. arXiv preprint\narXiv:190603926. 2019;.\n4. Rolf M, Asada M. Where do goals come from? A generic approach to\nautonomous goal-system development. arXiv preprint arXiv:14105557. 2014;.\n5. Liu M, Zhu M, Zhang W. Goal-conditioned reinforcement learning: Problems and\nsolutions. arXiv preprint arXiv:220108299. 2022;.\n6. Eysenbach B, Zhang T, Levine S, Salakhutdinov RR. Contrastive learning as\ngoal-conditioned reinforcement learning. Advances in Neural Information\nProcessing Systems. 2022;35:35603–35620.\n7. Goyal P, Niekum S, Mooney RJ. Using natural language for reward shaping in\nreinforcement learning. arXiv preprint arXiv:190302020. 2019;.\n8. Mirchandani S, Karamcheti S, Sadigh D. Ella: Exploration through learned\nlanguage abstraction. Advances in Neural Information Processing Systems.\n2021;34:29529–29540.\n9. Zhong V, Rockt¨aschel T, Grefenstette E. Rtfm: Generalising to novel\nenvironment dynamics via reading. arXiv preprint arXiv:191008210. 2019;.\n10. Hejna III DJ, Abbeel P, Pinto L. Improving Long-Horizon Imitation Through\nLanguage Prediction. 2021;.\n11. Akakzia A, Colas C, Oudeyer PY, Chetouani M, Sigaud O. Grounding language\nto autonomously-acquired skills via goal generation. arXiv preprint\narXiv:200607185. 2020;.\n12. Deng Z, Narasimhan K, Russakovsky O. Evolving graphical planner: Contextual\nglobal planning for vision-and-language navigation. Advances in Neural\nInformation Processing Systems. 2020;33:20660–20672.\n13. Hanjie AW, Zhong VY, Narasimhan K. Grounding language to entities and\ndynamics for generalization in reinforcement learning. In: International\nConference on Machine Learning. PMLR; 2021. p. 4051–4062.\n14. Jiang Y, Gu SS, Murphy KP, Finn C. Language as an abstraction for hierarchical\ndeep reinforcement learning. Advances in Neural Information Processing Systems.\n2019;32.\n15. Andreas J, Klein D, Levine S. Modular multitask reinforcement learning with\npolicy sketches. In: International conference on machine learning. PMLR; 2017. p.\n166–175.\n16. Madan K, Ke NR, Goyal A, Sch¨olkopf B, Bengio Y. Fast and slow learning of\nrecurrent independent mechanisms. arXiv preprint arXiv:210508710. 2021;.\n17. Chevalier-Boisvert M, Bahdanau D, Lahlou S, Willems L, Saharia C, Nguyen TH,\net al. Babyai: A platform to study the sample efficiency of grounded language\nlearning. arXiv preprint arXiv:181008272. 2018;.\nPLOS ONE PrePrint\n9/10\n18. Stooke A, Lee K, Abbeel P, Laskin M. Decoupling representation learning from\nreinforcement learning. In: International Conference on Machine Learning.\nPMLR; 2021. p. 9870–9879.\n19. Wang H, Yang X, Wang Y, Xuguang L. Constrained Contrastive Reinforcement\nLearning. In: Asian Conference on Machine Learning. PMLR; 2023. p. 1070–1084.\n20. Zheng R, Wang X, Sun Y, Ma S, Zhao J, Xu H, et al. TACO: Temporal Latent\nAction-Driven Contrastive Loss for Visual Reinforcement Learning. arXiv\npreprint arXiv:230613229. 2023;.\n21. Levine S. Understanding the world through action. In: Conference on Robot\nLearning. PMLR; 2022. p. 1752–1757.\n22. Ma Y, Xu G, Sun X, Yan M, Zhang J, Ji R. X-clip: End-to-end multi-grained\ncontrastive learning for video-text retrieval. In: Proceedings of the 30th ACM\nInternational Conference on Multimedia; 2022. p. 638–647.\n23. Yao L, Huang R, Hou L, Lu G, Niu M, Xu H, et al. Filip: Fine-grained\ninteractive language-image pre-training. arXiv preprint arXiv:211107783. 2021;.\n24. Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, et al. Learning\ntransferable visual models from natural language supervision. In: International\nconference on machine learning. PMLR; 2021. p. 8748–8763.\n25. Yu J, Wang Z, Vasudevan V, Yeung L, Seyedhosseini M, Wu Y. Coca:\nContrastive captioners are image-text foundation models. arXiv preprint\narXiv:220501917. 2022;.\n26. Li J, He X, Wei L, Qian L, Zhu L, Xie L, et al. Fine-grained semantically aligned\nvision-language pre-training. Advances in neural information processing systems.\n2022;35:7290–7303.\n27. Zhu C, Jia Q, Chen W, Guo Y, Liu Y. Deep learning for video-text retrieval: a\nreview. International Journal of Multimedia Information Retrieval. 2023;12(1):3.\n28. Levine S, Kumar A, Tucker G, Fu J. Offline reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv preprint arXiv:200501643.\n2020;.\n29. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.\nAttention is all you need. Advances in neural information processing systems.\n2017;30.\n30. Paischer F, Adler T, Hofmarcher M, Hochreiter S. Semantic HELM: An\nInterpretable Memory for Reinforcement Learning. arXiv preprint\narXiv:230609312. 2023;.\n31. Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal policy\noptimization algorithms. arXiv preprint arXiv:170706347. 2017;.\nPLOS ONE PrePrint\n10/10\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-11-29",
  "updated": "2024-11-29"
}