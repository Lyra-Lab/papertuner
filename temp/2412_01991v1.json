{
  "id": "http://arxiv.org/abs/2412.01991v1",
  "title": "Real-Time Multilingual Sign Language Processing",
  "authors": [
    "Amit Moryossef"
  ],
  "abstract": "Sign Language Processing (SLP) is an interdisciplinary field comprised of\nNatural Language Processing (NLP) and Computer Vision. It is focused on the\ncomputational understanding, translation, and production of signed languages.\nTraditional approaches have often been constrained by the use of gloss-based\nsystems that are both language-specific and inadequate for capturing the\nmultidimensional nature of sign language. These limitations have hindered the\ndevelopment of technology capable of processing signed languages effectively.\n  This thesis aims to revolutionize the field of SLP by proposing a simple\nparadigm that can bridge this existing technological gap. We propose the use of\nSignWiring, a universal sign language transcription notation system, to serve\nas an intermediary link between the visual-gestural modality of signed\nlanguages and text-based linguistic representations.\n  We contribute foundational libraries and resources to the SLP community,\nthereby setting the stage for a more in-depth exploration of the tasks of sign\nlanguage translation and production. These tasks encompass the translation of\nsign language from video to spoken language text and vice versa. Through\nempirical evaluations, we establish the efficacy of our transcription method as\na pivot for enabling faster, more targeted research, that can lead to more\nnatural and accurate translations across a range of languages.\n  The universal nature of our transcription-based paradigm also paves the way\nfor real-time, multilingual applications in SLP, thereby offering a more\ninclusive and accessible approach to language technology. This is a significant\nstep toward universal accessibility, enabling a wider reach of AI-driven\nlanguage technologies to include the deaf and hard-of-hearing community.",
  "text": "Real-Time Multilingual\nSign Language Processing\nAmit Moryossef\nDepartment of Computer Science\nPh.D. Thesis\nSubmitted to the Senate of Bar-Ilan University\nRamat Gan, Israel\nDecember 2023\nThis work was carried out under the supervision of Prof. Yoav Goldberg,\nDepartment of Computer Science, Bar-Ilan University.\nTo those who always believed in me,\nand to the ones I have loved and lost.\nAcknowledgements\nFirst and foremost, my sincerest appreciation to my supervisor, Yoav Goldberg,\nfor his guidance, support, and encouragement. The autonomy he granted and\nhis candid feedback significantly contributed to my growth as a scientist.\nThis thesis owes its very existence to Maayan Gazuli (Hazut), who first in-\ntroduced me to the world of sign language. It was a casual conversation on a\ncar ride that sparked my interest and set me on this journey. I am immensely\nthankful for Maayan’s unwitting role as the catalyst of this entire endeavor.\nMy deepest gratitude goes to Valerie Sutton, the inventor of Sutton Sign-\nWriting, for her pioneering work in creating a linguistic transcription system\nfor signed languages. Her system served as a vital tool in the research and anal-\nysis underpinning this thesis and profoundly influenced its outcome. Without\nher contributions, this thesis would not have been feasible. I am honored to\nbuild upon her groundbreaking work and grateful for her commitment to mak-\ning signed languages accessible to all.\nI also extend my thanks to Ioannis Tsochantaridis for hosting me as an intern\nat Google during the COVID-19 pandemic, and to Sarah Ebling, Annette Rios,\nand Mathias M¨uller for their unwavering support and for hosting me at the\nUniversity of Zurich. Their kindness and hospitality played a crucial role in\nmaintaining my mental well-being, for which I am deeply grateful.\nTo my Bar-Ilan University labmates, particularly Yanai Elazar, Valentina Py-\natkin, Hila Gonen, Shauli Ravfogel, and Amir David Nissan Cohen, I thank you\nfor being my academic social circle during this challenging period.\nTo the Computational Linguistics Ph.D. students at the University of Zurich,\nespecially No¨emi Aepli, Tannon Kew, and Anastassia Shaitarova, your warm\nwelcome and inclusive atmosphere made my time there not only productive\nbut also enjoyable. You’ve made a meaningful impact on both my professional\ndevelopment and personal well-being, and for that, I am genuinely grateful.\nSpecial recognition goes to the team at the University of Hamburg Institute\nof German Sign Language and Communication of the Deaf for their openness\nand outreach. Thanks to Thomas Hanke for granting me access to the DGS cor-\npus and providing continual support; Maria Kopf for teaching me how to read\nand write HamNoSys; and Annika Herrmann for filling gaps in my knowledge\nwith interesting linguistic information. Your collective wisdom has widely con-\ntributed to this thesis.\nA big shoutout to the students I supervised during this thesis — Zifan Jiang\nfrom the University of Zurich and Rotem Shalev Arkushin from Reichman Uni-\nversity. Your willingness to heed my advice, coupled with the courage to chal-\nlenge my ideas when they were off the mark, made this journey intellectually\nenriching for all parties involved. Thank you for keeping me on my toes.\nI would also like to acknowledge Rebecca Norton and Hannah Neuser for\ntheir encouragement and support, which gave me the motivation and confi-\ndence I needed to persevere through difficult times and complete this work.\nLastly, I wish to express my heartfelt gratitude to my family for their love,\nsupport, and encouragement throughout my journey.\nFunding\nThis work was financially supported by Bar-Ilan University (the European Re-\nsearch Council (ERC) under the European Union’s Horizon 2020 research and\ninnovation programme, grant agreement No. 802774 (iEXTRACT)); by the Uni-\nversity of Zurich (the European Union’s Horizon 2020 research and innova-\ntion programme (grant number 101016982, EASIER), and the Swiss Innovation\nAgency (Innosuisse) flagship IICT (PFFS-21-47)); and by Google.\nPreface\nPublications\nPortions of this thesis are joint work and have been published elsewhere.\n• Chapter 2, “Background” includes material published as “Sign Language\nProcessing” in https://sign-language-processing.github.io,\n(Moryossef and Goldberg, 2021).\n• Section 3.1, “pose-format: Library for Viewing, Augmenting, and Han-\ndling .pose Files” appeared in https://github.com/sign-language\n-processing/pose, (Moryossef et al., 2021a).\n• Section 3.2, “Sign Language Datasets” appeared in https://github\n.com/sign-language-processing/datasets, (Moryossef and\nM¨uller, 2021).\n• Section 3.3, “3D Hand Pose Benchmark” appeared in https://gith\nub.com/sign-language-processing/3d-hands-benchmark,\n(Moryossef, 2022).\n• Section 5.1, “Activity Detection” includes material that appeared in SLRTP\n2020: The Sign Language Recognition, Translation & Production Work-\nshop, in “Real-Time Sign-Language Detection using Human Pose Estima-\ntion” (Moryossef et al., 2020).\n• Section 5.2, “Isolated Recognition” includes material that appeared in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, in “Evaluating the immediate applicability of pose estima-\ntion for sign language recognition” (Moryossef et al., 2021b).\n• Section 5.3, “Gloss Translation” includes material that appeared in Pro-\nceedings of the 1st International Workshop on Automatic Translation for\nSigned and Spoken Languages (AT4SSL 2021), in “Data Augmentation for\nSign Language Gloss Translation” (Moryossef et al., 2021c).\n• Section 6.1, “Segmentation” includes material that appeared in Proceed-\nings of the Findings of the 2023 Conference on Empirical Methods in Nat-\nural Language Processing, in “Linguistically Motivated Sign Language\nSegmentation” (Moryossef et al., 2023a).\n• Section 6.3, “Translation” includes material under submission to Proceed-\nings of the 2024 Joint International Conference on Computational Linguis-\ntics, Language Resources and Evaluation, in “SignBank+: Multilingual\nSign Language Translation Dataset” (Moryossef and Jiang, 2023).\n• Section 7.1, “Baseline” includes material that appeared in Proceedings of\nthe 2nd International Workshop on Automatic Translation for Signed and\nSpoken Languages (AT4SSL 2023), in “An Open-Source Gloss-Based Base-\nline for Spoken to Signed Language Translation” (Moryossef et al., 2023b).\nThis work was further presented at the Meeting of Computational Lin-\nguistics in The Netherlands (CLIN 2023).\n• Chapter 8, “Sign Language Translation Application” includes material\nthat appeared in “sign.mt: Real-Time Multilingual Sign Language Trans-\nlation Application” (Moryossef, 2023c). ⋆Outstanding demo paper award ⋆\n• Chapter 9, “Implications for Spoken Languages” includes material that\nappeared in “Addressing the Blind Spots in Spoken Language Processing”\n(Moryossef, 2023b).\nHighlighted Collaborations\n• “Including Signed Languages in Natural Language Processing” appeared\nin Proceedings of the 59th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL 2021, Yin et al. (2021)). ⋆Best paper award ⋆\n• “Machine Translation between Spoken Languages and Signed Languages\nRepresented in SignWriting” appeared in Proceedings of the 17th Confer-\nence of the European Chapter of the Association for Computational Lin-\nguistics (EACL 2023, Jiang et al. (2023a)).\n• “Ham2Pose: Animating Sign Language Notation into Pose Sequences”\nappeared in Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition 2023 (CVPR 2023, Arkushin et al. (2023)).\n• “Considerations for meaningful sign language machine translation based\non glosses” appeared in Proceedings of the 61th Annual Meeting of the\nAssociation for Computational Linguistics (ACL 2023, M¨uller et al. (2023)).\n⋆Outstanding paper award ⋆\nContents\nAbstract\ni\nI\nIntroduction to Sign Language Processing\n1\n1\nIntroduction\n3\n2\nBackground (Moryossef and Goldberg, 2021)\n8\n3\nPreliminary Work (Libraries)\n24\n3.1\npose-format: Library for Viewing, Augmenting, and Handling\n.pose Files (Moryossef et al., 2021a) . . . . . . . . . . . . . . . . . .\n25\n3.2\nSign Language Datasets (Moryossef and M¨uller, 2021) . . . . . . .\n34\n3.3\n3D Hand Pose Benchmark (Moryossef, 2022) . . . . . . . . . . . .\n39\nII\nSign Language Transcription\n48\n4\nIntroduction\n50\n5\nPreliminary Work\n52\n5.1\nActivity Detection (Moryossef et al., 2020) . . . . . . . . . . . . . .\n52\n5.2\nIsolated Recognition (Moryossef et al., 2021b) . . . . . . . . . . . .\n66\n5.3\nGloss Translation (Moryossef et al., 2021c) . . . . . . . . . . . . . .\n81\n6\nSign Language Translation\n93\n6.1\nSegmentation (Moryossef et al., 2023a) . . . . . . . . . . . . . . . .\n93\n6.2\nTranscription . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n6.3\nTranslation (Moryossef and Jiang, 2023) . . . . . . . . . . . . . . . 120\n7\nSign Language Production\n144\n7.1\nBaseline (Moryossef et al., 2023b) . . . . . . . . . . . . . . . . . . . 144\n7.2\nTranslation (Jiang et al., 2023a)\n. . . . . . . . . . . . . . . . . . . . 166\n7.3\nProduction (Arkushin et al., 2023) . . . . . . . . . . . . . . . . . . . 167\n7.4\nAnimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\nIII\nDiscussion and Implications\n171\n8\nSign Language Translation Application (Moryossef, 2023c)\n173\n9\nImplications for Spoken Languages (Moryossef, 2023b)\n183\n10 Conclusions\n193\n11 Bibliography\n195\nℵ\nHebrew Abstract\nAbstract\nSigned languages serve as a vital means of communication for millions of deaf\nand hard-of-hearing individuals worldwide. Utilizing a visual-gestural modal-\nity, they convey complex linguistic structures through manual articulations com-\nbined with non-manual elements like facial expressions and body movement.\nDespite their linguistic richness and cultural importance, signed languages have\noften been marginalized by the latest advances in text-centric artificial intelli-\ngence technologies, such as Machine Translation and Large Language Models.\nThis marginalization restricts access to these technologies for a significant pop-\nulation, leaving them behind in the rapid advancements in language-based AI.\nSign Language Processing (SLP) is an interdisciplinary field comprised of\nNatural Language Processing (NLP) and Computer Vision. It is focused on the\ncomputational understanding, translation, and production of signed languages.\nTraditional approaches have often been constrained by the use of gloss-based\nsystems that are both language-specific and inadequate for capturing the mul-\ntidimensional nature of sign language. These limitations have hindered the de-\nvelopment of technology capable of processing signed languages effectively.\nThis thesis aims to revolutionize the field of SLP by proposing a simple\nparadigm that can bridge this existing technological gap. We propose the use of\nSignWiring, a universal sign language transcription notation system, to serve as\nan intermediary link between the visual-gestural modality of signed languages\nand text-based linguistic representations.\nUnlike gloss-based approaches, our paradigm using SignWriting is designed\nto accurately capture the multidimensional and language-independent aspects\ni\nof signed languages. This allows for the creation of a unified and scalable frame-\nwork that can accommodate the rich linguistic diversity found in various signed\nlanguages across the globe.\nWe contribute foundational libraries and resources to the SLP community,\nthereby setting the stage for a more in-depth exploration of the tasks of sign\nlanguage translation and production. These tasks encompass the translation\nof sign language from video to spoken language text and vice versa. Through\nempirical evaluations, we establish the efficacy of our transcription method as\na pivot for enabling faster, more targeted research, that can lead to more natural\nand accurate translations across a range of languages.\nOur paradigm establishes a clear boundary between NLP and Computer\nVision within the broader context of SLP. This division mirrors the existing sep-\naration between NLP and Signal Processing in the realm of spoken language\ntechnologies. By doing so, we open the door for more specialized research ef-\nforts in each sub-discipline, thereby enriching the ecosystem of technologies\nand methodologies available for SLP.\nThe universal nature of our transcription-based paradigm also paves the\nway for real-time, multilingual applications in SLP, thereby offering a more in-\nclusive and accessible approach to language technology. This is a significant\nstep toward universal accessibility, enabling a wider reach of AI-driven lan-\nguage technologies to include the deaf and hard-of-hearing community.\nIn summary, this thesis presents a new approach to Sign Language Process-\ning, one that aims to set a new standard for inclusive, real-time, and multilin-\ngual language technologies. By bridging the existing gap between text-centric\nAI and the visual-gestural world of signed languages, we substantially con-\ntribute toward making language-based AI universally accessible.\nii\nPart I\nIntroduction to Sign Language\nProcessing\n1\n2\n“we do these things not because they are easy,\nbut because we thought they were going to be.”\n— Pinboard, The Programmers’ Credo\nChapter 1\nIntroduction\nSigned languages (also known as sign languages) are languages that use the\nvisual-gestural modality to convey meaning through manual articulations in\ncombination with non-manual elements like the face and body. They serve as\nthe primary means of communication for numerous deaf and hard-of-hearing\nindividuals. Similar to spoken languages, signed languages are natural lan-\nguages governed by a set of linguistic rules (Sandler and Lillo-Martin, 2006),\nboth emerging through an abstract, protracted aging process and evolving with-\nout deliberate meticulous planning. Signed languages are not universal or mu-\ntually intelligible, despite often having striking similarities among them. They\nare also distinct from spoken languages—i.e., American Sign Language (ASL)\nis not a visual form of English but its own unique language.\nSign Language Processing (Bragg et al., 2019; Yin et al., 2021) is an emerging\nfield of artificial intelligence concerned with the automatic processing and anal-\nysis of sign language content. While research has focused more on the visual\naspects of signed languages, it is a subfield of both Natural Language Process-\ning (NLP) and Computer Vision (CV). Challenges in sign language processing\noften include machine translation of sign language videos into spoken language\ntext (sign language translation), from spoken language text (sign language pro-\nduction), or sign language recognition for sign language understanding.\nUnfortunately, the latest advances in language-based artificial intelligence,\n3\nCHAPTER 1. INTRODUCTION\n4\nlike machine translation and personal assistants, expect a spoken language in-\nput (text or transcribed speech), excluding around 200 to 300 different signed\nlanguages (United Nations, 2022) and up to 70 million deaf people (World Health\nOrganization, 2021; World Federation of the Deaf, 2022).\nThroughout history, Deaf communities fought for the right to learn and use\nsigned languages and for the public recognition of signed languages as legiti-\nmate ones. Indeed, signed languages are sophisticated communication modal-\nities, at least as capable as spoken languages in all aspects, both linguistic and\nsocial. However, in a predominantly oral society, deaf people are constantly en-\ncouraged to use spoken languages through lip-reading or text-based communi-\ncation. The exclusion of signed languages from modern language technologies\nfurther suppresses signing in favor of spoken languages. This exclusion disre-\ngards the preferences of the Deaf communities who strongly prefer to communi-\ncate in signed languages both online and for in-person day-to-day interactions,\namong themselves and when interacting with spoken language communities\n(Padden and Humphries, 1988; Glickman and Hall, 2018). Thus, it is essential\nto make signed languages accessible.\nTo date, a large amount of research on Sign Language Processing (SLP) has\nbeen focused on the visual aspect of signed languages, led by the Computer Vi-\nsion (CV) community, with little NLP involvement. This focus is not unreason-\nable, given that a decade ago, we lacked adequate CV tools to process videos for\nfurther linguistic analyses. However, similar to spoken languages, signed lan-\nguages are fully-fledged systems exhibiting all the fundamental characteristics\nof natural languages, and existing SLP techniques do not adequately address\nor leverage the linguistic structure of signed languages. Signed languages in-\ntroduce novel challenges for NLP due to their visual-gestural modality, simul-\ntaneity, spatial coherence, and lack of written form. The lack of a written form\nmakes the spoken language processing pipelines - which often start with audio\ntranscription before processing - incompatible with signed languages, forcing\nresearchers to work directly on the raw video signal.\nFurthermore, SLP is not only intellectually appealing but also an important\nresearch area with significant potential to benefit signing communities. Bene-\nCHAPTER 1. INTRODUCTION\n5\nficial applications enabled by signed language technologies include improved\ndocumentation of endangered sign languages; educational tools for sign lan-\nguage learners; tools for query and retrieval of information from signed lan-\nguage videos; personal assistants that react to signed languages; real-time au-\ntomatic sign language interpretations; and more. Needless to say, in addressing\nthis research area, researchers should work alongside and under the direction of\ndeaf communities, and to benefit the signing communities’ interest above all\n(Harris et al., 2009).\n1.1\n(Brief) History of Signed Languages and Deaf\nCulture\nThroughout modern history, spoken languages were dominant, so much so\nthat signed languages struggled to be recognized as languages in their own\nright, and educators developed misconceptions that signed language acqui-\nsition might hinder the development of speech skills. For example, in 1880,\na large international conference of deaf educators called the “Second Interna-\ntional Congress on Education of the Deaf” banned teaching signed languages,\nfavoring speech therapy instead. It was not until the seminal work on American\nSign Language (ASL) by Stokoe Jr (1960) that signed languages started gain-\ning recognition as natural, independent, and well-defined languages, which in-\nspired other researchers to further explore signed languages as a research area.\nNevertheless, antiquated attitudes that placed less importance on signed lan-\nguages continue to inflict harm and subject many to linguistic neglect (Humphries\net al., 2016). Several studies have shown that deaf children raised solely with\nspoken languages do not gain enough access to a first language during their\ncritical period of language acquisition (Murray et al., 2020). This language de-\nprivation can lead to life-long consequences on the cognitive, linguistic, socio-\nemotional, and academic development of the deaf (Hall et al., 2017).\nSigned languages are the primary languages of communication for the Deaf1\n1When capitalized, “Deaf” refers to a community of deaf people who share a language and\nCHAPTER 1. INTRODUCTION\n6\nand are at the heart of Deaf communities. In the past, the failure to recognize\nsigned languages as fully-fledged natural language systems in their own right\nhas had detrimental effects, and in an increasingly digitized world, NLP re-\nsearch should strive to enable a world in which all people, including the Deaf,\nhave access to languages that fit their lived experience.\n1.2\nThesis Overview\nThe dissertation asserts that for progress to be made in the area of sign language\nprocessing, it is vital to adopt a written phonetic lexical representation for sign\nlanguage, as an intermediary stage for any subsequent tasks.\nThis section provides an overview of the thesis structure and content, as well\nas the contributions made to the field of Sign Language Processing.\nPart I\nIntroduces the field of Sign Language Processing to the reader.\n1. Chapter 1 situates this field within the broader context of artificial in-\ntelligence and machine learning research, outlines the main problem ad-\ndressed, and introduces signed languages and Deaf culture.\n2. Chapter 2 introduces the linguistic aspects of signed languages as natu-\nral languages, explains and demonstrates their representation, overviews\nthe existing types of available resources, and covers the various tasks in-\nvolved. Some of these tasks are further described in background sections\nwithin relevant chapters.\n3. Chapter 3 discusses some of the preliminary work carried out in prepara-\ntion for this thesis, with a focus on libraries designed to be widely used in\nsign language processing research.\na culture, whereas the lowercase “deaf” refers to the audiological condition of not hearing. We\nfollow the more recent convention of abandoning a distinction between “Deaf” and “deaf”,\nusing the latter term also to refer to (deaf) members of the sign language community (Napier\nand Leeson, 2016; Kusters et al., 2017).\nCHAPTER 1. INTRODUCTION\n7\nPart II\nExplores the use of universal phonetic sign language written forms as\nan intermediate representation for downstream sign language processing tasks,\nsuch as translation and production.\n1. Chapter 4 introduces the idea of utilizing written sign language represen-\ntations as an intermediate stage.\n2. Chapter 5 discusses some of the preliminary work carried out at the be-\nginning of this thesis, with a focus on relevant work published in sign\nlanguage research venues.\n3. Chapter 6 explores the application of lexical written representations as an\nintermediate phase for translation signed-to-spoken language translation.\n4. Chapter 7 compliments Chapter 6 by examining the usability of such an\nintermediate representation in the production process and assesses its ef-\nfectiveness in comparison to the use of semantic forms.\nPart III\nWraps up the thesis by integrating the different components into a\nworking demonstration, and discussing key insights and contributions.\n1. Chapter 8 presents a sign language translation application that can trans-\nlate from and to multiple signed languages in real-time and offline. It\ndelves into the engineering, design, and development of this application.\n2. Chapter 9 discusses the implications of the findings in this thesis as they\nrelate to the field of Spoken Language Processing, and proposes a way for\nthe two fields to benefit from each other.\n3. Chapter 10 concludes the thesis by summarizing the main findings and\ncontributions, outlining potential avenues for future research directions\nin Sign Language Processing, and highlighting the potential impact of this\nwork on the deaf community.\nChapter 2\nBackground (Moryossef and\nGoldberg, 2021)\n2.1\n(Brief) Sign Language Linguistics Overview\nSigned languages consist of phonological, morphological, syntactic, and seman-\ntic levels of structure that fulfill the same social, cognitive, and communicative\npurposes as other natural languages. While spoken languages primarily chan-\nnel the oral-auditory modality, signed languages use the visual-gestural modal-\nity, relying on the signer’s face, hands, body, and space around them to create\ndistinctions in meaning. We present the linguistic features of signed languages1\nthat researchers must consider during their modeling.\nPhonology\nSigns are composed of minimal units that combine manual fea-\ntures such as hand configuration, palm orientation, placement, contact, path\nmovement, local movement, as well as non-manual features including eye aper-\nture, head movement, and torso positioning (Liddell and Johnson, 1989; John-\nson and Liddell, 2011; Brentari, 2011; Sandler, 2012). Not all possible phonemes\nare realized in both signed and spoken languages, and inventories of two lan-\nguages’ phonemes/features may not overlap completely. Different languages\n1We mainly refer to ASL, where most research has been conducted, but not exclusively.\n8\nCHAPTER 2. BACKGROUND\n9\nare also subject to rules for the allowed combinations of features.\nSimultaneity\nThough an ASL sign takes about twice as long to produce than\nan English word, the rates of transmission of information between the two lan-\nguages are similar (Bellugi and Fischer, 1972). One way signed languages com-\npensate for the slower production rate of signs is through simultaneity: Signed\nlanguages use multiple visual cues to convey different information simultane-\nously (Sandler, 2012). For example, the signer may produce the sign for “cup”\non one hand while simultaneously pointing to the actual cup with the other to\nexpress “that cup.” Similarly to tone in spoken languages, the face and torso\ncan convey additional affective information (Liddell et al., 2003; Johnston and\nSchembri, 2007). Facial expressions can modify adjectives, adverbs, and verbs;\na head shake can negate a phrase or sentence; gaze can help indicate referents.\nReferencing\nThe signer can introduce referents in discourse either by pointing\nto their actual locations in space or by assigning a region in the signing space\nto a non-present referent and by pointing to this region to refer to it (Rathmann\nand Mathur, 2011; Schembri et al., 2018). Signers can also establish relations\nbetween referents grounded in signing space by using directional signs or em-\nbodying the referents using body shift or eye gaze (Dudis, 2004; Liddell and\nMetzger, 1998). Spatial referencing also impact morphology when the direc-\ntionality of a verb depends on the location of the reference to its subject and/or\nobject (de Beuzeville, 2008; Fenlon et al., 2018): For example, a directional verb\ncan move from its subject’s location and end at its object’s location. While the\nrelation between referents and verbs in spoken language is more arbitrary, ref-\nerent relations are usually grounded in signed languages. The visual space is\nheavily exploited to make referencing clear.\nAnother way anaphoric entities are referenced in sign language is by using\nclassifiers or depicting signs that help describe the characteristics of the referent\n(Supalla, 1986; Wilcox and Hafer, 2004; Roy, 2011). Classifiers are typically one-\nhanded signs that do not have a particular location or movement assigned to\nthem, or derive features from meaningful discourse (Liddell et al., 2003), so\nCHAPTER 2. BACKGROUND\n10\nthey can be used to convey how the referent relates to other entities, describe\nits movement, and give more details. For example, to tell about a car swerving\nand crashing, one might use the hand classifier for a vehicle, move it to indicate\nswerving, and crash it with another entity in space.\nTo quote someone other than oneself, signers perform role shift (Cormier\net al., 2015), where they may physically shift in space to mark the distinction\nand take on some characteristics of the people they represent. For example, to\nrecount a dialogue between a taller and a shorter person, the signer may shift\nto one side and look up when taking the shorter person’s role, shift to the other\nside and look down when taking the taller person’s role.\nFingerspelling\nresults from language contact between a signed language and\na surrounding spoken language (Battison, 1978; Wilcox, 1992; Brentari and Pad-\nden, 2001; Patrie and Johnson, 2011). A set of manual gestures corresponds\nwith a written orthography or phonetic system. This phenomenon, found in\nmost signed languages, is often used to indicate names, places, or new concepts\nfrom the spoken language, but has often become integrated into the language\nas another linguistic strategy (Padden, 1998; Montemurro and Brentari, 2018).\n2.2\nRepresentation\nRepresentation is a significant challenge for SLP. Unlike spoken languages, signed\nlanguages have no widely adopted written form.\nAs signed languages are\nconveyed through the visual-gestural modality, video recording is the most\nstraightforward way to capture them. However, as videos include more in-\nformation than needed for modeling and are expensive to record, store, and\ntransmit, a lower-dimensional representation has been sought after.\nFigure 2.1 illustrates various signed language representations. In this demon-\nstration, we deconstruct the video into its individual frames to exemplify the\nalignment of the annotations between the video and representations.\nYOUR\n\nHamNoSys\numbrella\nSignWriting\nASL Gloss\nNAME\n\nWHAT\n\nPose Stream\nVideo Stream\n107 FRAMES\nFigure 2.1: Representations of an American Sign Language phrase with video frames, pose estimations, SignWriting, HamNoSys and glosses.\nEnglish translation: “What is your name?” (Yin et al., 2021)\nCHAPTER 2. BACKGROUND\n12\nVideos\nare the most straightforward representation of a signed language\nand can amply incorporate the information conveyed through signing. One\nmajor drawback of using videos is their high dimensionality: They usually in-\nclude more information than needed for modeling and are expensive to store,\ntransmit, and encode. As facial features are essential in sign, anonymizing raw\nvideos remains an open problem, limiting the possibility of making these videos\npublicly available (Isard, 2020).\nSkeletal Poses\nreduce the visual cues in videos to skeleton-like wireframes\nor meshes, representing the location of joints. This representation has been ex-\ntensively used in computer vision, estimating human pose from video data,\nand determining the spatial configuration of the body at each point in time.\nAlthough high-quality pose estimation can be achieved using motion capture\nequipment, such methods are often expensive and intrusive. As a result, es-\ntimating pose from videos has become the preferred method in recent years\n(Pishchulin et al., 2012; Chen et al., 2017; Cao et al., 2019; G¨uler et al., 2018).\nCompared to video representations, accurate skeletal poses have a lower com-\nplexity and provide a semi-anonymized representation of the human body, while\nobserving relatively low information loss. However, they remain a continuous,\nmultidimensional representation that is not adapted to most NLP models.\nWritten notation systems\nrepresent signs as discrete visual features. Some\nsystems are written linearly, and others use graphemes in two dimensions.\nWhile various universal (Sutton, 1990; Prillwitz and Zienert, 1990) and language-\nspecific notation systems (Stokoe Jr, 1960; Kakumasu, 1968; Bergman, 1977)\nhave been proposed, no writing system has been adopted widely by any sign\nlanguage community, and the lack of standards hinders the exchange and uni-\nfication of resources and applications between projects. Figure 2.1 depicts two\nuniversal notation systems: SignWriting (Sutton, 1990), a two-dimensional pic-\ntographic system, and HamNoSys (Prillwitz and Zienert, 1990), a linear stream\nof graphemes designed to be machine-readable.\nCHAPTER 2. BACKGROUND\n13\nGlosses\nare the transcription of signed languages sign-by-sign, with each\nsign having a unique semantic identifier. While various sign language corpus\nprojects have provided guidelines for gloss annotation (Mesch and Wallin, 2015;\nJohnston and De Beuzeville, 2016; Konrad et al., 2018), a standardized gloss an-\nnotation protocol has yet to be established. Gloss IDs offer a more precise ap-\nproach by assigning unique identifiers to each form of a sign, not just its mean-\ning, capturing unspecified phonetic variations. Linear gloss annotations have\nbeen criticized for their imprecise representation of signed language. These an-\nnotations fail to capture all the information expressed simultaneously through\ndifferent cues, such as body posture, eye gaze, or spatial relations, leading to\na loss of information that can significantly affect downstream performance on\nSLP tasks (Yin and Read, 2020a; M¨uller et al., 2023).\nTable 2.1 additionally exemplifies the various representations for more iso-\nlated signs. For this example, we use SignWriting as the notation system. Note\nthat the same sign might have two unrelated glosses, and the same gloss might\nhave multiple valid spoken language translations.\nVideo\nPose\nEstimation\nSignWriting\nGloss\nEnglish Translation\nHOUSE\nHouse\nWRONG-WHAT\nWhat’s the matter?\nWhat’s wrong?\nDIFFERENT\nBUT\nDifferent\nBut\nTable 2.1: Example of isolated signs represented in multiple ways.\nCHAPTER 2. BACKGROUND\n14\n2.3\nAnnotation Tools\nELAN - EUDICO Linguistic Annotator (Wittenburg et al., 2006)\nis an an-\nnotation tool for audio and video recordings. With ELAN, a user can add an\nunlimited number of textual annotations to audio and/or video recordings. An\nannotation can be a sentence, word, gloss, comment, translation, or description\nof any feature observed in the media. Annotations can be created on multiple\nlayers, called tiers, which can be hierarchically interconnected. An annotation\ncan either be time-aligned to the media or refer to other existing annotations.\nThe content of annotations consists of Unicode text, and annotation documents\nare stored in an XML format (EAF). ELAN is open source (GPLv3), and installa-\ntion is available for Windows, macOS, and Linux. PyMPI (Lubbers and Torreira,\n2013) allows for simple python interaction with Elan files.\niLex (Hanke, 2002)\nis a tool for sign language lexicography and corpus analy-\nsis, that combines features found in empirical sign language lexicography and\nsign language discourse transcription. It supports the user in integrated lexi-\ncon building while working on the transcription of a corpus and offers several\nunique features considered essential due to the specific nature of signed lan-\nguages. iLex binaries are available for macOS.\nSignStream (Neidle et al., 2001)\nis a tool for linguistic annotations and com-\nputer vision research on visual-gestural language data SignStream installation\nis available for macOS and is distributed under an MIT license.\nAnvil - The Video Annotation Research Tool (Kipp, 2001)\nis a free video an-\nnotation tool, offering multi-layered annotation based on a user-defined coding\nscheme. In Anvil, the annotator can see color-coded elements on multiple tracks\nin time alignment. Some special features are cross-level links, non-temporal ob-\njects, timepoint tracks, coding agreement analysis, 3D viewing of motion cap-\nture data and a project tool for managing whole corpora of annotation files.\nAnvil installation is available for Windows, macOS, and Linux.\nCHAPTER 2. BACKGROUND\n15\n2.4\nResources\nSigned language resources come in multiple forms, such as bilingual dictionar-\nies, fingerspelling and isolated sign corpora, and continuous sign corpora. Each\nhas its own limitations, but they are all essential for translation and production\nin signed languages.\nBilingual dictionaries\nfor signed language (Mesch and Wallin, 2012; Fen-\nlon et al., 2015; Crasborn et al., 2016; Gutierrez-Sigut et al., 2016) map a spoken\nlanguage word or short phrase to a signed language video. One notable dictio-\nnary, SpreadTheSign2 is a parallel dictionary containing around 25,000 words\nwith 42 different spoken-signed language pairs and more than 600,000 videos\nin total. Unfortunately, while dictionaries lexically map between languages,\nthey do not demonstrate the grammar or the usage of signs in context.\nFingerspelling corpora\nusually consist of videos of words borrowed from\nspoken languages that are signed letter-by-letter. They can be synthetically cre-\nated (Dreuw et al., 2006) or mined from online resources (Shi et al., 2018, Shi\net al. (2019)). However, they only capture one aspect of signed languages.\nIsolated sign corpora\nare collections of a limited vocabulary (20-1000 signs)\nof annotated single signs. They are synthesized (Ebling et al., 2018; Huang et al.,\n2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources\n(Vaezi Joze and Koller, 2019; Li et al., 2020), and can be used for isolated sign\nlanguage recognition or contrastive analysis of minimal signing pairs (Imashev\net al., 2020). However, like dictionaries, they do not describe relations between\nsigns, nor do they capture coarticulation during the signing.\nContinuous sign corpora\ncontain parallel sequences of signs and spoken\nlanguage. Available continuous sign corpora are extremely limited, containing\n2https://www.spreadthesign.com/\nCHAPTER 2. BACKGROUND\n16\n4-6 orders of magnitude fewer sentence pairs than similar corpora for spoken\nlanguage machine translation (Arivazhagan et al., 2019). Moreover, while auto-\nmatic speech recognition (ASR) datasets contain up to 50,000 hours of record-\nings (Pratap et al., 2020), the most extensive continuous sign language corpus\ncontains only 1,150 hours, and only 50 of them are publicly available (Hanke\net al., 2020). These datasets are usually synthesized (Databases, 2007; Crasborn\nand Zwitserlood, 2008; Ko et al., 2019; Hanke et al., 2020) or recorded in studio\nconditions (Forster et al., 2014, Camg¨oz et al. (2018)), which does not account\nfor noise in real-life conditions. Moreover, some contain signed interpretations\nof spoken language rather than naturally-produced signs, which may not accu-\nrately represent native signing since translation is now a part of the discourse.\nAvailability\nUnlike the vast amount and diversity of available spoken lan-\nguage resources that allow various applications, sign language resources are\nscarce and, currently only support translation and production. Unfortunately,\nmost of the sign language corpora discussed in the literature are either not avail-\nable for use or available under heavy restrictions and licensing terms. Further-\nmore, sign language data is especially challenging to anonymize due to the im-\nportance of facial and other physical features in signing videos, limiting its open\ndistribution. Developing anonymization with minimal information loss or ac-\ncurate anonymous representations is a promising research direction.\n2.4.1\nReal-World Data Collection\nData is essential to develop any data-driven technology, and current efforts in\nSLP are often limited by the lack of adequate data. We discuss the considera-\ntions to keep in mind when building datasets, the challenges of collecting such\ndata, and directions to facilitate data collection.\nWhat is Good Sign Language Data?\nFor SLP models to be deployable,\nthey must be developed using data representing the real world accurately. What\nconstitutes an ideal sign language dataset is an open question; we propose the\nCHAPTER 2. BACKGROUND\n17\nfollowing requirements: (1) a broad domain; (2) sufficient data and vocabulary\nsize; (3) real-world conditions; (4) naturally produced signs; (5) a diverse signer\ndemographic; (6) native signers; and when applicable, (7) dense annotations.\nTo illustrate the importance of data quality during modeling, Yin et al. (2021)\nfirst take as an example a current benchmark for SLP, the RWTH-PHOENIX-\nWeather 2014T dataset (Camg¨oz et al., 2018) of German Sign Language, that\ndoes not meet most of the above criteria: it is restricted to the weather domain\n(1); contains only around 8K segments with 1K unique signs (2); filmed in stu-\ndio conditions (3); interpreted from German utterances (4); and signed by nine\nCaucasian interpreters (5,6). Although this dataset successfully addressed data\nscarcity issues at the time and successfully rendered results and fueled com-\npetitive research, it does not accurately represent signed languages in the real\nworld. On the other hand, the Public DGS Corpus (Hanke et al., 2020) is an\nopen-domain (1) dataset consisting of 50 hours of natural signing (4) by 330 na-\ntive signers from various regions in Germany (5,6), annotated with glosses, and\nGerman translations (7), meeting all but two requirements we suggest.\nThey train a gloss-to-text sign language translation transformer (Yin and\nRead, 2020a) on both datasets. On RWTH-PHOENIX-Weather 2014T, they ob-\ntain 22.17 BLEU on testing; on the Public DGS Corpus, they obtain a mere 3.2\nBLEU. Although Transformers achieve encouraging results on RWTH-PHOENIX-\nWeather 2014T (Saunders et al., 2020c; Camg¨oz et al., 2020a), they fail on more\nrealistic, open-domain data. These results reveal that, for real-world applica-\ntions, we need more data to train such models. At the same time, available data\nis severely limited in size; less data-hungry and more linguistically-informed\napproaches may be more suitable. This experiment reveals how it is crucial to\nuse data that accurately represent the complexity and diversity of signed lan-\nguages to precisely assess what types of methods are suitable and how well our\nmodels would deploy to the real world.\nChallenges of Data Collection\nCollecting and annotating signed data in\nline with the ideal requires more resources than speech or text data, taking up\nto 600 minutes per minute of an annotated signed language video (Hanke et al.,\nCHAPTER 2. BACKGROUND\n18\n2020). Moreover, annotation usually requires specific knowledge and skills,\nwhich makes recruiting or training qualified annotators challenging. Addition-\nally, there is little existing sign language data in the wild openly licensed for use,\nespecially from native signers that are not interpretations of speech. Therefore,\ndata collection often requires significant efforts and costs of on-site recording.\nAutomating Annotation\nOne helpful research direction for collecting more\ndata that enables the development of deployable SLP models is creating tools\nthat can simplify or automate parts of the collection and annotation process.\nOne of the most significant bottlenecks in obtaining more adequate sign lan-\nguage data is the time and scarcity of experts required to perform annotation.\nTherefore, tools that perform automatic parsing, detection of sign boundaries,\nextraction of articulatory features, suggestions for lexical annotations, and al-\nlow parts of the annotation process to be crowdsourced to non-experts, to name\na few, have a high potential to accelerate the availability of good data.\n2.4.2\nPracticing Deaf Collaboration\nFinally, when working with signed languages, it is vital to keep in mind who\nthis technology should benefit and what they need. Researchers in SLP should\nacknowledge that signed languages belong to the Deaf community and avoid\nexploiting their language as a commodity (Bird, 2020).\nSolving Real Needs\nMany efforts in SLP have developed intrusive meth-\nods (e.g., requiring signers to wear special gloves), which are often rejected by\nsigning communities and therefore have limited real-world value. Such efforts\nare often marketed to perform “sign language translation” when they, in fact,\nonly identify fingerspelling or recognize a minimal set of isolated signs at best.\nThese approaches oversimplify the rich grammar of signed languages, promote\nthe misconception that signs are solely expressed through the hands, and are\nconsidered by the Deaf community as a manifestation of audism, where it is\nthe signers who must make the extra effort to wear additional sensors to be un-\nCHAPTER 2. BACKGROUND\n19\nderstood by non-signers (Erard, 2017). To avoid such mistakes, we encourage\nclose Deaf involvement throughout the research process to ensure that we direct\nour efforts toward applications that will be adopted by signers and do not make\nfalse assumptions about signed languages or the needs of signing communities.\nBuilding Collaboration\nDeaf collaborations and leadership are essential\nfor developing sign language technologies to ensure they address the commu-\nnity’s needs and will be adopted, not relying on misconceptions or inaccuracies\nabout signed language (Harris et al., 2009; Kusters et al., 2017). Hearing re-\nsearchers cannot relate to the deaf experience or fully understand the context\nin which the tools being developed would be used, nor can they speak for the\ndeaf. Therefore, we encourage creating a long-term collaborative environment\nbetween sign language researchers and users so that deaf users can identify\nmeaningful challenges and provide insights on the considerations to take while\nresearchers cater to the signers’ needs as the field evolves. We also recommend\nreaching out to signing communities for reviewing papers on signed languages\nto ensure an adequate evaluation of this type of research results published at\nacademic venues. There are several ways to connect with Deaf communities for\ncollaboration: one can seek deaf students in their local community, reach out\nto schools for the deaf, contact deaf linguists, join a network of researchers of\nsign-related technologies3, or participate in deaf-led projects.\n2.5\nTasks\nSo far, the computer vision community has primarily led the SLP research to\nfocus on processing the visual features in sign language videos. As a result,\ncurrent SLP methods do not fully address the linguistic complexity of signed\nlanguages. We survey common SLP tasks and current methods’ limitations,\ndrawing on signed languages’ linguistic theories.\nSign Language Translation (SLT) commonly refers to the translation of signed\n3https://www.crest-network.com/\nCHAPTER 2. BACKGROUND\n20\nlanguage to spoken language (De Coster et al., 2022; M¨uller et al., 2022). Sign\nLanguage Production is the reverse process of producing a sign language video\nfrom spoken language text. Sign Language Recognition (SLR) (Adaloglou et al.,\n2020) detects and labels signs from isolated (Imashev et al., 2020; Sincan and\nKeles, 2020) or continuous (Cui et al., 2017; Camg¨oz et al., 2018, 2020b) sign\nlanguage videos.\nLanguage Agnostic Tasks\nLanguage Specific Tasks\nVideo\nText\nPose\nGlosses\nNotation\nFigure 2.2: The various sign language processing tasks, visualized as a fully\nconnected graph.\nFigure 2.2 presents a fully connected graph where each node is a single data\nrepresentation, and each directed edge represents the task of converting one\ndata representation to another. We split the graph into two parts - language\nagnostic, and language specific.\n• Every edge to the left, on the orange background, represents a task in\ncomputer vision. These tasks are inherently language-agnostic; thus, they\ngeneralize between signed languages.\n• Every edge to the right, on the blue background, represents a task in natu-\nral language processing. These tasks are sign language-specific, requiring\na specific sign language lexicon or spoken language tokens.\nCHAPTER 2. BACKGROUND\n21\n• Every edge crossing between backgrounds represents a task requiring a\ncombination of computer vision and natural language processing.\nThis graph conceptually defines 20 different tasks, with varying amounts of\nprevious research. Every path between two nodes that goes from left-to-right\nor right-to-left can be a valid pipeline of tasks. However, in this thesis, we make\nthe case that the most valid paths are paths with tasks that do not cross between\nthe modalities, and instead go through the ‘Notation’ representation.\nThe necessary background information for various tasks is disseminated\nthroughout this thesis, specifically in Part II. Following is a guide to direct you\nto where you can find an overview of each task’s background.\nFor insights into Sign Language Activity Detection, refer to Section 5.1. An\noverview of Pose Estimation (Video-to-Pose), as well as Isolated Sign Recognition\n(Video-to-Gloss and Pose-to-Gloss), is available in Section 5.2. Section 5.3 delves\ninto previous work related to Gloss-to-Text translation. To understand Sign Lan-\nguage Sign and Phrase Segmentation, please consult Section 6.1. Section 6.2\nprovides the background for Pose-to-Notation sign language transcription. For\na comprehensive background on both Text-to-Notation and Notation-to-Text sign\nlanguage translation, visit Section 6.3. To explore the Text-to-Gloss and Gloss-to-\nPose tasks, refer to Section 7.1. Section 7.3 offers insights into Text-to-Pose and\nNotation-to-Pose tasks. Lastly, background information on the Pose-to-Video ani-\nmation task can be found in Section 7.4.\n2.5.1\nFingerspelling\nFingerspelling is spelling a word letter-by-letter, borrowing from the spoken\nlanguage alphabet (Battison, 1978; Wilcox, 1992; Brentari and Padden, 2001; Pa-\ntrie and Johnson, 2011). This phenomenon, found in most signed languages,\noften occurs when there is no previously agreed-upon sign for a concept, like\nin technical language, colloquial conversations involving names, conversations\ninvolving current events, emphatic forms, and the context of code-switching\nbetween the signed language and the corresponding spoken language (Padden,\nCHAPTER 2. BACKGROUND\n22\n1998; Montemurro and Brentari, 2018). The relative amount of fingerspelling\nvaries between signed languages, and for American Sign Language (ASL), ac-\ncounts for 12-35% of the signed content (Padden and Gunsauls, 2003).\nPatrie and Johnson (2011) described the following terminology to describe\nthree different forms of fingerspelling:\n• Careful—slower spelling where each letter pose is clearly formed.\n• Rapid—quick spelling where letters are often not completed and contain\nremnants of other letters in the word.\n• Lexicalized—a sign produced by often using no more than two letter-\nhand-shapes (Battison, 1978). For example, lexicalized ALL uses A and\nL, lexicalized BUZZ uses B and Z, etc...\nRecognition\nFingerspelling recognition, a sub-task of sign language recognition, is the task\nof recognizing fingerspelled words from a sign language video.\nShi et al. (2018) introduced a large dataset available for American Sign Lan-\nguage fingerspelling recognition. This dataset includes both the “careful” and\n“rapid” forms of fingerspelling collected from naturally occurring videos “in\nthe wild”, which are more challenging than studio conditions. They trained a\nbaseline model to take a sequence of images cropped around the signing hand.\nThey found that using CTC outperformed autoregressive decoding, but that\nboth achieved poor recognition rates (35-41% character level accuracy) com-\npared to human performance (around 82%).\nIn follow-up work, Shi et al. (2019) collected nearly an order-of-magnitude\nlarger dataset and designed a new recognition model. Instead of detecting the\nsigning hand, they detected the face and cropped a large area around it. Then,\nthey performed an iterative process of zooming in to the hand using visual at-\ntention to retain sufficient information in high resolution of the hand. Finally,\nlike their previous work, they encoded the image hand crops sequence and used\nCHAPTER 2. BACKGROUND\n23\na CTC to obtain the frame labels. They showed that this method outperformed\ntheir original “hand crop” method by 4% and that they could achieve up to\n62.3% character-level accuracy using the additional data collected.\nLooking\nthrough this dataset, we note that the videos in the dataset were taken from\nlonger videos, and as they were cut, they did not retain the signing before the\nfingerspelling. This context relates to language modeling, where at first, one fin-\ngerspells a word carefully, and when repeating it, might fingerspell it rapidly,\nbut the interlocutors can infer they are fingerspelling the same word.\nProduction\nFingerspelling production, a sub-task of sign language production, is the task\nof producing a fingerspelling video for words.\nIn its basic form, “careful” fingerspelling production can be trivially solved\nusing pre-defined letter handshapes interpolation. Adeline (2013) demonstrated\nthis approach for American Sign Language and English fingerspelling. They\nrigged a hand armature for each letter in the English alphabet (N = 26) and\ngenerated all (N2 = 676) transitions between every two letters using interpola-\ntion or manual animation. Then, to fingerspell entire words, they chain pairs of\nletter transitions. For example, for the word “CHLOE”, they would chain the\nfollowing transitions sequentially: #C CH HL LO OE E#.\nHowever, to produce life-like animations, one must also consider the rhythm\nand speed of holding letters, and transitioning between letters, as those can af-\nfect how intelligible fingerspelling motions are to an interlocutor (Wilcox (1992)).\nWheatland et al. (2016) analyzed both “careful” and “rapid” fingerspelling videos\nfor these features. They found that for both forms of fingerspelling, on average,\nthe longer the word, the shorter the transition and hold time. Furthermore, they\nfound that less time is spent on middle letters on average, and the last letter is\nheld on average for longer than the other letters in the word. Finally, they used\nthis information to construct an animation system using letter pose interpola-\ntion and controlled the timing using a data-driven statistical model.\nChapter 3\nPreliminary Work (Libraries)\nIn the emerging field of sign language processing, standardized approaches\nfor dataset distribution, loading, visualization, and augmentation remain un-\ndeveloped. Addressing this gap, we have prioritized two pivotal open-source\nprojects. First, pose-format (§3.1) serves as a comprehensive library tailored to\nsign language processing, enabling users to easily read, write, visualize, and\naugment pose sequences. The second, sign-language-datasets (§3.2), facilitates\nseamless integration of datasets regardless of their distribution or format, by\nproviding swift disk-mapped storing and loading of datasets. In tandem, pose-\nformat and sign-language-datasets have become the foundational tools for con-\ntemporary sign language processing research. We further introduce 3d-hands-\nbenchmark (§3.3), a tool to evaluate the consistency and usefulness of hand pose\nestimation models in the context of sign language hand shapes.\n24\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n25\n3.1\npose-format: Library for Viewing, Augmenting,\nand Handling .pose Files (Moryossef et al., 2021a)\nManaging and analyzing pose data is a complex task, with challenges ranging\nfrom handling diverse file structures and data types to facilitating effective data\nmanipulations such as normalization and augmentation. This paper presents\npose-format, a comprehensive toolkit designed to address these challenges\nby providing a unified, flexible, and easy-to-use interface. The library includes\na specialized file format that encapsulates various types of pose data, accom-\nmodating multiple individuals and an indefinite number of time frames, thus\nproving its utility for both image and video data. Furthermore, it offers seam-\nless integration with popular numerical libraries such as NumPy, PyTorch, and\nTensorFlow, thereby enabling robust machine-learning applications. Through\nbenchmarking, we demonstrate that our .pose file format offers vastly su-\nperior performance against prevalent formats like OpenPose, with added ad-\nvantages like self-contained pose specification.\nAdditionally, the library in-\ncludes features for data normalization, augmentation, and easy-to-use visual-\nization capabilities, both in Python and Browser environments. pose-format\nemerges as a one-stop solution, streamlining the complexities of pose data man-\nagement and analysis.\n3.1.1\nIntroduction\nWorking with pose data introduces many complexities, from the diversity in file\nstructures to the variety of data types that need to be accommodated. Develop-\ners and researchers often find themselves juggling numerous data manipula-\ntion tasks such as normalization, augmentation, and visualization. In addition\nto these challenges, pose data itself can be inherently multidimensional, fre-\nquently encompassing multiple individuals and varying time frames. This cre-\nates an intricate ecosystem of variables that can be challenging to manage and\nanalyze effectively, which is important in fields like Sign Language Processing.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n26\nFigure 3.1: Examples of human skeletal poses extracted from a sign language\nvideo sequence.\nTo overcome these complexities, we designed pose-format, a comprehen-\nsive toolkit to alleviate these challenges by offering a unified, flexible, and easy-\nto-use interface for managing and analyzing pose data. Designed with versatil-\nity in mind, the library includes a specialized file format that accommodates an\narray of pose types, multiple people, and an indefinite number of time frames,\nmaking it highly adaptable for both video and single-frame data. Users can ef-\nfortlessly import .pose files and perform a range of manipulations such as data\nnormalization and augmentation. The library also integrates seamlessly with\npopular numerical libraries like NumPy (Harris et al., 2020), PyTorch (Paszke\net al., 2019b), and TensorFlow (Abadi et al., 2015), allowing for additional com-\nputational flexibility for machine learning. With features for easy visualiza-\ntion and compatibility with other popular pose data formats like OpenPose\n(Cao et al., 2019) and MediaPipe Holistic (Grishchenko and Bazarevsky, 2020),\npose-format emerges as a one-stop solution for working with poses.\n3.1.2\nBackground\nIn the context of our library, a pose consists of keypoints, which are 2D or 3D\ncoordinates marking points of interest usually on a human body in image or\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n27\nvideo frames (Figure 3.1). Systems like OpenPose and MediaPipe Holistic are\nprominent for pose estimation but have differing methodologies and keypoint\nconfigurations. OpenPose, for instance, uses a classification objective and out-\nputs 135 or 137 keypoints with 2D coordinates. MediaPipe Holistic employs a\nregression objective, estimating 543 keypoints with 3D coordinates.\nKeypoints are hierarchically organized, often attached to larger body com-\nponents like LEFT HAND or FACE. Moreover, models implicitly define which\nkeypoints are connected, forming an underlying graph structure. Confidence\nmetrics vary across systems. OpenPose assigns a confidence score to each clas-\nsification, while MediaPipe Holistic only predicts the likelihood of each BODY\nkeypoint’s presence in the original image.\nThe utility of human pose estimation (Zheng et al., 2023) spans various\nfields such as human-computer interaction, motion capture, motion analysis,\nand mixed reality, with specialized applications like automatic sign language\nprocessing (Moryossef et al., 2021b; M¨uller et al., 2022).\n3.1.3\nJustification\npose-format addresses a void in the ecosystem by delivering a uniform layer\nof abstraction over disparate pose estimation system outputs, such as OpenPose\nand MediaPipe Holistic. The necessity for this unified interface arises from three\nprimary factors: inconsistent standards, inadequacy of existing libraries, and\nperformance bottlenecks.\nInconsistent Standards\nAs delineated in §3.1.2, there are competing pose es-\ntimation systems, each adhering to its own data storage and representation\nscheme. This inconsistency impedes interoperability between systems and makes\nthe data hard to share or transition across different platforms. We remedy this\nby standardizing how pose data is managed, making it simpler to operate with\nmultiple systems, switch between them, or even disseminate pose data.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n28\nLimitations of Existing Libraries\nCurrent libraries focus extensively on low-\nlevel operations, lacking the higher-level abstractions that can expedite routine\ntasks. For instance, in the absence of our toolkit, users have to micromanage ar-\nray values, discerning between coordinates and confidence scores or handling\nmissing keypoints. Such intricacies detract from productivity and introduce\nunnecessary complexity. Our library fills this gap by offering user-friendly op-\nerations, many of which are indispensable for machine learning research, such\nas frame rate interpolation, rotation, scaling, frame dropout, or converting the\nunderlying data into tensors of a specific machine learning library.\nEfficiency\nAs demonstrated in §3.1.7, prevailing methods for pose data man-\nagement suffer from performance limitations in both speed and storage. These\ninefficiencies create bottlenecks for data-intensive tasks, especially those preva-\nlent in machine learning pipelines. pose-format offers optimized data stor-\nage and retrieval, mitigating these inefficiencies.\n3.1.4\nFormat Specification\nThe core of the pose-format library is its specialized file format that accom-\nmodates a wide range of scenarios. This unique format enables the storing of\nmultidimensional data capturing various pose types, multiple individuals, and\nan indefinite number of time frames. Currently, at version 0.1, the file format is\nbifurcated into two components: the Header and the Body.\nHeader (PoseHeader)\nThe header contains meta information that defines the overall structure of the\npose data. This information is useful for visualization and code readability.\nSpecifically, it includes:\n• (float32) Version - The version of the file format.\n• (uint16[3]) Dimensions - Width, height, and depth specifications.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n29\n• (uint16) Number of Components - The number of pose components.\nComponent Details\nEach component includes its (string) name, (string)\nformat, and the (uint16) number of points, (uint16) limbs, and (uint16)\ncolors it contains.\n• (string[]) Names of points.\n• (uint16[2][]) Start and end indices of limbs.\n• (uint16[3][]) Points color RGB values.\nBody (PoseBody)\nThe body of the file comprises the actual pose data and includes the following:\n• (uint16) FPS - The frame rate of the pose.\n• (uint16) Number of frames - deprecated due to challenges for longer\npose sequences.\n• (uint16) Number of People - The number of people included in every\nframe.\n• (float[][][][]) Data - The coordinate of every point for every person\nin every frame.\n• (float[][][]) Confidence - The confidence for every point for every\nperson in every frame.\nThis format’s granularity and modularity make it aptly suited for a wide\nrange of applications, from simple image-based pose representation to more\ncomplex video analysis tasks. By leveraging this detailed yet flexible format,\nthe pose-format library ensures ease of use without sacrificing the intricacies\nthat pose data often necessitates.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n30\nv0.1 Limitations\nWhile the pose-format library has been designed to cater to a wide array of\nneeds, there are some limitations and criticisms in the current file format that\nusers should be aware of:\n• FPS Representation: The FPS is stored as uint16, which does not allow\nfor floating-point values.\n• Number of Frames: The number of frames is also restricted to uint16,\nwhich limits the frame count to 65,535. The current workaround calculates\nthe number of frames based on the file size, which introduces computa-\ntional overhead.\n• Pose Data Precision: The pose data utilizes 32-bit floating-point values\nfor storage. However, 16-bit numbers could be sufficient for many appli-\ncations. Support for both types would improve memory efficiency.\n• Confidence Precision: Similar to the pose data, the confidence metrics are\nstored as 32-bit floating-point numbers. A 16-bit representation would be\nmore than sufficient for most practical purposes.\n3.1.5\nData Manipulations\nOne of the key advantages of this toolkit is its robust support for various data\nmanipulation tasks, which are crucial for the preprocessing and augmentation\nof pose data. This section elaborates on how the library facilitates operations\nsuch as normalization and augmentation.\nNormalization\nNormalization is a crucial step to make pose data scale and\ntranslation invariant, thereby improving the effectiveness of downstream tasks\nlike training machine learning models. Our toolkit offers a simple yet powerful\ninterface to normalize pose data. For example, when dealing with human body\nposes, we can specify the names of the left and right shoulders, and the skeleton\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n31\nwill be scaled such that the mean distance between the shoulders is equal to 1,\nand the center point lies on (0, 0). If we deal with 3D poses, we can also specify a\nplane by naming three points, to make sure they always fall on the same plane.\nThese normalizations try to remove the effect of camera angles and distance\nfrom the subject on the resulting video.\nAugmentation\nData augmentation is a technique to artificially increase the\nsize and diversity of your training dataset by applying various transformations.\nIn the context of pose data, these can include affine transformations such as\ntranslation, scaling, reflection, rotation, and shear, interpolation of frames at\nvariable speeds, noise, and dropout, to name a few. The pose-format toolkit\nprovides built-in functions to perform these augmentations effortlessly. You can\neither apply these transformations individually or chain them together to create\na complex augmentation pipeline, thereby enhancing the library’s adaptability\nto various project needs.\nIntegration with Numerical Libraries\nData manipulations are seamlessly in-\ntegrated with popular numerical libraries like NumPy, PyTorch, and Tensor-\nFlow. This facilitates easy data flow between data manipulation and machine\nlearning models, reducing the friction in the data science pipeline. It allows\nloading and augmenting the data in a framework of your choosing, minimizing\nthe number of memory copy operations.\n3.1.6\nVisualization\nThe ability to visualize pose data is crucial for understanding its characteristics,\ndebugging algorithms, and even for presentation purposes.\nPython\nIn Python, users can use the PoseVisualizer for different visual-\nization tasks, such as visualizing the pose by itself as a sequence of still images,\na video, a GIF, with the background being either a fixed color or overlayed on\nanother video. An example of visualizing the pose as a video would be:\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n32\nfrom pose format import Pose\nfrom pose format.pose visualizer import PoseVisualizer\nwith open(\"example.pose\", \"rb\") as f:\npose = Pose.read(f.read())\nv = PoseVisualizer(pose)\nv.save video(\"example.mp4\", v.draw())\nBrowser\nAdditionally, for web-based applications or quick interactive view-\ning, poses can be visualized in the browser. Unlike the rasterized Python visu-\nalization, the web-based visualization is vectorized and thus more suitable for\nclient-facing applications.\n<script type=\"module\"\nsrc=\"https://unpkg.com/pose-viewer@latest\n/dist/pose-viewer/pose-viewer.esm.js\" />\n<pose-format src=\"example.pose\" />\n3.1.7\nBenchmarking\nTo evaluate our custom file format, we benchmarked it against OpenPose, a\nprevalent standard. Metrics of interest were read speed and file size. We ob-\ntained OpenPose data from a single video in the Public DGS Corpus (Hanke\net al., 2020, DOI: /10.25592/dgs.corpus-3.0-text-1413451-11105600-11163240). Their format\nemploys a monolithic JSON file to store frames, diverging from the common\none-file-per-frame approach.\nTo gauge reading performance, we measured OpenPose’s JSON load time in\nisolation, sidestepping tensor conversion. For our format, we include both full-\nfile reads and body-only tensor reads where we skip loading the pose header,\nand only load the tensor of coordinates and confidences.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n33\n# Frames\nOpenPose\npose-format\nSize\nSpeed\nSize\nSpeed\nSpeed (Body)\n1\n3.9 KB\n37.4 µs ± 600 ns\n3.6 KB\n535 µs ± 66.1 µs\n61.7 µs ± 6.94 µs\n10\n38 KB\n364 µs ± 6.9 µs\n18 KB\n490 µs ± 63.8 µs\n57.9 µs ± 2.56 µs\n100\n388 KB\n3.75 ms ± 113 µs\n163 KB\n415 µs ± 49.7 µs\n72.4 µs ± 4.87 µs\n1,000\n3.9 MB\n43.1 ms ± 704 µs\n1.6 MB\n658 µs ± 110 µs\n228 µs ± 9.09 µs\n10,000\n39 MB\n439 ms ± 29.5 ms\n16 MB\n2.72 ms ± 110 µs\n2.71 ms ± 245 µs\nTable 3.1: Benchmarking pose-format against OpenPose from the Public\nDGS Corpus. We compare both the resulting file size, and file read speed.\nSpeed (Body) measures loading the .pose files data only, without metadata.\nQuantitative Edge\nTable 3.1 reveals we achieve up to a 60% file size reduction\nand outpace OpenPose in read speed by a staggering 162×, thereby obliterating\nany machine learning bottlenecks.\nQualitative Edge\nOur pose-format packs all pose data into a singular, ro-\nbust file, avoiding the file fragmentation issues seen in OpenPose. Moreover,\nour header encodes pose structure, obviating the need for hard-coded interpre-\ntation logic and boosting both portability and usability.\nIn summation, pose-format offers superior performance across key met-\nrics, making it a compelling alternative for pose data management.\n3.1.8\nCommunity Contributions\nOur library is fully open-source, and released under an MIT License. We wel-\ncome contributions from the community of any kind, and we encourage collab-\noration. Source code and bug reporting are available at https://github.c\nom/sign-language-processing/pose.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n34\n3.2\nSign Language Datasets (Moryossef and M¨uller,\n2021)\nFollowing our discussion on the variety of resources essential for sign language\nprocessing in Section 2, and underscoring the importance of collecting real-\nworld data in collaboration with the deaf community, we now shift our focus to\nthe acquisition and processing of these datasets.\nFirst, we compile an inventory of currently available datasets, detailing the\ntypes of annotations each dataset contains when that information is available.\nThis list is presented in Table 3.2. In addition to its inclusion in this thesis,\nthe inventory is maintained as a living document available at https://re\nsearch.sign.mt/#resources. This document is updated periodically to\nincorporate newly released datasets, serving as a continually evolving resource\nfor researchers in the field.\nAt present, there exists no standardized approach or consensus for down-\nloading and loading sign language datasets, and as such, evaluation of these\ndatasets is scarce. To address this gap, we have streamlined the process of\ndataset loading through the use of Tensorflow Datasets (authors, 2019). This\nutility enables researchers to effortlessly load datasets — regardless of their size\n— with a single command, thereby facilitating comparability across different\nstudies. We offer access to these datasets through our custom-built library, Sign\nLanguage Datasets (Moryossef and M¨uller, 2021).\nTo demonstrate the use of our library, we show how a dataset can be easily\nloaded in one line of Python. In this example, we load the ASLG-PC12 dataset\nwith its default configuration:\nimport tensorflow datasets as tfds\nimport sign language datasets.datasets\naslg pc12 = tfds.load(\"aslg pc12\")\nWe also support loading datasets with custom configuration. For example,\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n35\nwe want to load the videos resized to 256x256 as tensors at 12 frames-per-\nsecond, and also load MediaPipe Holistic poses for each of the videoss.\n# Loading a dataset with custom configuration\nfrom sign language datasets.datasets.config\nimport SignDatasetConfig\nconfig = SignDatasetConfig(\nname=\"videos and poses256x256:12\",\n# Specific version\nversion=\"3.0.0\",\n# Download, and load dataset videos\ninclude video=True,\n# Load videos at constant, 12 fps\nfps=12,\n# Convert videos to a constant resolution, 256x256\nresolution=(256, 256),\n# Download and load Holistic pose estimation\ninclude pose=\"holistic\")\nrwth phoenix2014 t = tfds.load(\nname='rwth phoenix2014 t',\nbuilder kwargs=dict(config=config))\nWe follow a unified interface when possible, making attributes the same and\ncomparable between datasets:\n{\n\"id\": tfds.features.Text(),\n\"signer\": tfds.features.Text() | tf.int32,\n\"video\": tfds.features.Video(\nshape=(None, HEIGHT, WIDTH, 3)),\n\"depth video\": tfds.features.Video(\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n36\nshape=(None, HEIGHT, WIDTH, 1)),\n\"fps\": tf.int32,\n\"pose\": {\n\"data\": tfds.features.Tensor(\nshape=(None, 1, POINTS, CHANNELS),\ndtype=tf.float32),\n\"conf\": tfds.features.Tensor(\nshape=(None, 1, POINTS),\ndtype=tf.float32)\n},\n\"gloss\": tfds.features.Text(),\n\"text\": tfds.features.Text()\n}\nVideo |\nPose |\nMouthing |\nNotation |\nGloss |\nText |\nSpeech |\nSave using sign-language-datasets\nDataset\nPublication\nLanguage\nFeatures\n#Signs\n#Samples\n#Signers\nLicense\nASL-100-RGBD\nHassan et al. (2020)\nAmerican\n100\n4,150 Tokens\n22\nAuthorized\nAcademics\nASL-Homework-\nRGBD\nHassan et al. (2022)\nAmerican\n935\n45\nAuthorized\nAcademics\nASLG-PC12\nOthman and Jemni (2012)\nAmerican\n(Synthetic)\n> 100,000,000\nSentences\nN/A\nSample Available (1, 2)\nASLLVD\nAthitsos et al. (2008)\nAmerican\n-\n3,000\n12,000 Samples\n4\nAttribution\nATIS\nBungeroth et al. (2008)\nMultilingual\n-\n292\n595 Sentences\nAUSLAN\nJohnston (2010)\nAustralian\n-\n1,100 Videos\n100\nAUTSL\nSincan and Keles (2020)\nTurkish\n226\n36,302 Samples\n43\nCodalab\nBosphorusSign\nCamg¨oz et al. (2016)\nTurkish\n-\n636\n24,161 Samples\n6\nNot Published\nBSL Corpus\nSchembri et al. (2013)\nBritish\n-\n40,000 Lexical Items\n249\nPartially Restricted\nCDPSL\nŁacheta and Rutkowski (2014)\nPolish\n300 hours\nChicagoFSWild\nShi et al. (2018)\nAmerican\n26\n7,304 Sequences\n160\nPublic\nChicagoFSWild+\nShi et al. (2019)\nAmerican\n26\n55,232 Sequences\n260\nPublic\nCopyCat\nZafrulla et al. (2010)\nAmerican\n-\n22\n420 Phrases\n5\nCorpus NGT\nCrasborn and Zwitserlood\n(2008)\nNetherlands\n-\n15 Hours\n92\nCC BY-NC-SA 3.0 NL\nDEVISIGN\nChai et al. (2014)\nChinese\n-\n2,000\n24,000 Samples\n8\nResearch purpose\nDicta-Sign\nMatthes et al. (2012)\nMultilingual\n-\n6-8 Hours\n(/Participant)\n16-18\n/Lan-\nguage\nHow2Sign\nDuarte et al. (2020)\nAmerican\n16,000\n79 hours (35,000\nsentences)\n11\nCC BY-NC 4.0\nK-RSL\nImashev et al. (2020)\nKazakh-\nRussian\n600\n28,250 Videos\n10\nAttribution\nKETI\nKo et al. (2019)\nKorean\n524\n14,672 Videos\n14\nDataset\nPublication\nLanguage\nFeatures\n#Signs\n#Samples\n#Signers\nLicense\nKRSL-\nOnlineSchool\nMukushev et al. (2022)\nKazakh-\nRussian\n890 Hours (1M\nsentences)\n7\nLSE-SIGN\nGutierrez-Sigut et al. (2016)\nSpanish\n-\n2,400\n2,400 Samples\n2\nCustom\nMS-ASL\nVaezi Joze and Koller (2019)\nAmerican\n-\n1,000\n25,000 (25 hours)\n200\nPublic\nNCSLGR\nDatabases (2007)\nAmerican\n1,875 sentences\n4\n-\nPublic DGS\nCorpus\nPrillwitz et al. (2008)\nGerman\n50 Hours\n330\nCustom\nRVL-SLLL ASL\nMart´ınez et al. (2002)\nAmerican\n-\n104\n2,576 Videos\n14\nResearch Attribution\nRWTH\nFingerspelling\nDreuw et al. (2006)\nGerman\n35\n1,400 single-char\nvideos\n20\nRWTH-BOSTON-\n104\nDreuw et al. (2008)\nAmerican\n104\n201 Sentences\n3\nRWTH-\nPHOENIX-\nWeather T\nForster et al. (2014);Camg¨oz\net al. (2018)\nGerman\n1,231\n8,257 Sentences\n9\nCC BY-NC-SA 3.0\nS-pot\nViitaniemi et al. (2014)\nFinnish\n-\n1,211\n5,539 Videos\n5\nPermission\nSign2MINT\n2021\nGerman\n740\n1135\nCC BY-NC-SA 3.0 DE\nSignBank\nMultilingual\n222148\nSIGNOR\nVintar et al. (2012)\nSlovene\n80\nSIGNUM\nVon Agris and Kraiss (2007)\nGerman\n-\n450\n15,600 Sequences\n20\nSMILE\nEbling et al. (2018)\nSwiss-\nGerman\n-\n100\n9,000 Samples\n30\nCustom\nSSL Corpus\n¨Oqvist et al. (2020)\nSwedish\nSSL Lexicon\nMesch and Wallin (2012)\nSwedish\n20,000\nCC BY-NC-SA 2.5 SE\nVideo-Based CSL\nHuang et al. (2018)\nChinese\n-\n500\n125,000 Videos\n50\nResearch Attribution\nWLASL\nLi et al. (2020)\nAmerican\n2,000\n100\nC-UDA 1.0\nTable 3.2: Curated list of some of the existing datasets, showcasing the mostly restrictive licensing, and small sizes.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n39\n3.3\n3D Hand Pose Benchmark (Moryossef, 2022)\n3.3.1\nIntroduction to Hand Shapes in Sign Language\nThe most prominent feature of signed languages is their use of the hands. In\nfact, the hands play an integral role in the phonetics of signs, and a slight varia-\ntion in hand shape can convey differences in meaning (Stokoe Jr, 1960). In sign\nlanguages such as American Sign Language (ASL) and British Sign Language\n(BSL), different hand shapes contribute to the vocabulary of the language, sim-\nilar to how different sounds contribute to the vocabulary of spoken languages.\nASL is estimated to use between 30 to 80 hand shapes1, while BSL is limited\nto approximately 40 hand shapes2. SignWriting (Sutton, 1990), a system of no-\ntation used for sign languages, specifies a superset of 261 distinct hand shapes\n(Frost and Sutton, 2022). Each sign language uses a subset of these hand shapes.\nDespite the fundamental role of hand shapes in sign languages, accurately\nrecognizing and classifying them is a challenging task. In this section, we ex-\nplore rule-based hand shape analysis in sign languages using 3D hand normal-\nization. By performing 3D hand normalization, we can transform any given\nhand shape to a fixed orientation, making it easier for a model to extract the\nhand shape, and hence improving the recognition and classification of hand\nshapes in sign languages.\n3.3.2\nCharacteristics of the Human Hand\nThe human hand consists of 27 bones and can be divided into three main sec-\ntions: the wrist (carpals), the palm (metacarpals), and the fingers (phalanges).\nEach finger consists of three bones, except for the thumb, which has two. The\nbones are connected by joints, which allow for the complex movements and\nshapes that the hand can form.\nUnderstanding the different characteristics of hands and their implications\n1https://aslfont.github.io/Symbol-Font-For-ASL/asl/handshapes.html\n2https://bsl.surrey.ac.uk/principles/i-hand-shapes\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n40\nFigure 3.2: Anatomy of a human hand.\n©American Society for Surgery of the Hand\nin signed languages is crucial for the extraction and classification of hand shapes.\nThese characteristics are based on the SignWriting definitions of the five major\naxes of hand variation: handedness, plane, rotation, view, and shape.\nHandedness\nis the distinction between the right and left hands. Signed lan-\nguages make a distinction between the dominant hand and the non-dominant\nhand. For right-handed individuals, the right hand is considered dominant, and\nvice-versa. The dominant hand is used for fingerspelling and all one-handed\nsigns, while the non-dominant hand is used for support and two-handed signs.\nUsing 3D pose estimation, the handedness analysis is trivial, as the pose esti-\nmation platform predicts which hand is which.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n41\nPlane\nrefers to whether the hand is parallel to the wall or the floor. The vari-\nation in the plane can, but does not have to, create a distinction between two\nsigns. For example, in ASL the signs for “date” and “dessert” exhibit the same\nhand shape, view, rotation, and movement, but differ by plane. The plane of\na hand can be estimated by comparing the positions of the wrist and middle\nfinger metacarpal bone (M MCP).\nAlgorithm 1 Hand Plane Estimation\ny ←|M MCP.y −WRIST.y| × 1.5 // add bias to y\nz ←|M MCP.z −WRIST.z|\nreturn y > z ? ‘wall’ : ‘floor’\nRotation\nrefers to the angle of the hand in relation to the body. SignWriting\ngroups the hand rotation into eight equal categories, each spanning 45 degrees.\nThe rotation of a hand can be calculated by finding the angle of the line created\nby the wrist and the middle finger metacarpal bone.\nView\nrefers to the side of the hand as observed by the signer, and is grouped\ninto four categories: front, back, sideways, and other-sideways. The view of\na hand can be estimated by analyzing the normal of the plane created by the\npalm of the hand (between the wrist, index finger metacarpal bone, and pinky\nmetacarpal bone).\nAlgorithm 2 Hand View Estimation\nnormal ←math.normal(WRIST, I MCP, P MCP)\nplane ←get plane(WRIST, M MCP)\nif plane = ‘wall’ then\nangle ↚ (normal.z, normal.x)\nreturn angle > 210 ? ‘front’ : (angle > 150 ? ‘sideways’ : ‘back’)\nelse\nangle ↚ (normal.y, normal.x)\nreturn angle > 0 ? ‘front’ : (angle > −60 ? ‘sideways’ : ‘back’)\nend if\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n42\nShape\nrefers to the configuration of the fingers and thumb. This characteristic\nof the hand is the most complex to analyze due to the vast array of possible\nshapes the human hand can form. The shape of a hand is determined by the\nstate of each finger and thumb, specifically whether they are straight, curved,\nor bent, and their position relative to each other. Shape analysis can be accom-\nplished by examining the bend and rotation of each finger joint. More advanced\nmodels may also take into consideration the spread between the fingers and\nother nuanced characteristics. 3D pose estimation can be used to extract these\nfeatures for a machine learning model, which can then classify the hand shape.\n3.3.3\n3D Hand Normalization\n3D hand normalization is an attempt at standardizing the orientation and po-\nsition of the hand, thereby enabling models to effectively classify various hand\nshapes. The normalization process involves several steps, as illustrated below:\n1. Pose Estimation Initially, the 3D pose of the hand is estimated from the\nhand image crop (Figure 3.3).\nFigure 3.3: Pictures of six hands all performing the same hand shape (v-shape)\ntaken from six different orientations. MediaPipe fails at estimating the pose of\nthe bottom-middle image.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n43\n2. 3D Rotation The pose is then rotated in 3D space such that the normal of\nthe back of the hand aligns with the Z-axis. As a result, the palm plane\nnow resides within the XY plane (Figure 3.4).\nFigure 3.4: Hand poses after 3D rotation. The scale difference between the\nhands demonstrates a limitation of the 3D pose estimation system used.\n3. 2D Orientation Subsequently, the pose is rotated in 2D such that the metacarpal\nbone of the middle finger aligns with the Y-axis (Figure 3.5).\nFigure 3.5: Hand poses after being rotated.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n44\n4. Scale The hand is scaled such that the metacarpal bone of the middle fin-\nger attains a constant length (which we typically set to 200, Figure 3.6).\nFigure 3.6: Hand poses after being scaled.\n5. Translation Lastly, the wrist joint is translated to the origin of the coor-\ndinate system (0, 0, 0). Figure 3.7 demonstrates how when overlayed, we\ncan see all hands producing the same shape, except for one outlier.\nFigure 3.7: Normalized hand poses overlayed after being translated to the\nsame position. The positions of the wrist and the metacarpal bone of the\nmiddle finger are fixed.\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n45\nBy conducting these normalization steps, a hand model can be standard-\nized, reducing the complexity of subsequent steps such as feature extraction\nand hand shape classification. This standardization simplifies the recognition\nprocess and can contribute to improving the overall accuracy of the system.\n3.3.4\n3D Hand Pose Evaluation\nIn order to assess the performance of our 3D hand pose estimation and nor-\nmalization, we introduce two metrics that gauge the consistency of the pose\nestimation across orientations and crops.\nOur dataset is extracted from the SignWriting Hand Symbols Manual Frost\nand Sutton (2022), and includes images of 261 different hand shapes, from 6\ndifferent angles. All images are of the same hand, of an adult white man.\nMulti Angle Consistency Error (MACE)\nevaluates the consistency of the pose\nestimation system across the different orientations. We perform 3D hand nor-\nmalization, and overlay the hands. The MACE score is the average standard\ndeviation of all pose landmarks, between all views. A high MACE score indi-\ncates a problem in the pose estimation system’s ability to maintain consistency\nacross different orientations. This could adversely affect the model’s perfor-\nmance when analyzing hand shapes in sign languages, as signs can significantly\nvary with hand rotation.\nFigure 3.8: Visualizations of 10 hand shapes, each with 6 orientations 3D\nnormalized and overlayed.\nFigure 3.8 shows that our 3D normalization does work to some extent using\nMediaPipe. We can identify differences across hand shapes, but still note high\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n46\nvariance within each hand shape.\nCrop Consistency Error (CCE)\ngauges the pose estimation system’s consis-\ntency across different crop sizes. We do not perform 3D normalization, but still\noverlay all the estimated hands, shifting the wrist point of each estimated hand\nto the origin (0, 0, 0). The CCE score is the average standard deviation of all\npose landmarks across crops. A high CCE score indicates that the pose esti-\nmation system is sensitive to the size of the input crop, which is a significant\ndrawback as the system should be invariant to the size of the input image.\nFigure 3.9: Visualizations of 10 hand shapes, each with 48 crops overlayed.\nFigure 3.9 shows that for some poses, MediaPipe is very resilient to crop size\ndifferences (e.g. the first and last hand shapes). However, it is concerning that\nfor some hand shapes, it exhibits very high variance, and possibly even wrong\npredictions.\n3.3.5\nConclusion\nOur normalization process appears to work reasonably well when applied to\ndifferent views within the same crop size. It succeeds in simplifying the hand\nshape, which in turn, can aid in improving the accuracy of hand shape classifi-\ncation systems.\nHowever, it is crucial to note that while this method may seem to perform\nwell on a static image, its consistency and reliability in a dynamic context, such\nas a video, may be quite different. In a video, the crop size can change between\nframes, introducing additional complexity and variance. This dynamic nature\nCHAPTER 3. PRELIMINARY WORK (LIBRARIES)\n47\ncoupled with the inherently noisy nature of the estimation process can pose\nchallenges for a model that aims to consistently estimate hand shapes.\nIn light of these findings, it is clear that there is a need for the developers\nof 3D pose estimation systems to consider these evaluation methods and strive\nto make their systems more robust to changes in hand crops. The Multi An-\ngle Consistency Error (MACE) and the Crop Consistency Error (CCE) can be\nvaluable tools in this regard.\nMACE could potentially be incorporated as a loss function for 3D pose es-\ntimation, thereby driving the model to maintain consistency across different\norientations. Alternatively, MACE could be used as an indicator to identify\nhand shapes that require more training data. It is apparent from our study that\nthe performance varies greatly across hand shapes and orientations, and this\napproach could help in prioritizing the allocation of training resources.\nUltimately, the goal of improving 3D hand pose estimation is to enhance\nthe ability to encode signed languages accurately. The insights gathered from\nthis study can guide future research and development efforts in this direction,\npaving the way for more robust and reliable sign language technology.\nThe benchmark, metrics, and visualizations are available at https://gith\nub.com/sign-language-processing/3d-hands-benchmark.\nPart II\nSign Language Transcription\n48\n49\n“we learn from history that we do not learn from history.”\n— Georg Wilhelm Friedrich Hegel\nChapter 4\nIntroduction\nThis chapter introduces the methodology adopted in this thesis to advance the\nfield of Sign Language Processing, recognizing the unique challenges posed by\nthe different modalities of signed and spoken languages. We embarked on a\njourney to adapt and extend the fundamental NLP theories to signed languages.\nTo achieve this, a critical part of the puzzle was the introduction of an interme-\ndiary transcription system, able to map continuous signed language videos to a\ndiscrete, accurate representation without linguistic information loss.\nAt the core of this research was the acknowledgment of the shortcomings of\nexisting SLP systems and datasets, which often rely on glosses for discretiza-\ntion. We identified three key challenges with this approach: the inability of\nlinear, single-dimensional glosses to capture the multidimensional spatial na-\nture of signed languages; the language-specificity of glosses requiring imprac-\ntical glossing models for each language; and the lack of standardization across\ncorpora, which inhibits data sharing and complicates modeling.\nOur proposed solution was to adopt a universal and standardized represen-\ntation for the transcription and tokenization of signed languages, considering\nkey linguistic factors such as the degree to which phonological units of signed\nlanguages can be mapped to lexical units, the cross-linguistic phonological dif-\nferences, and the extent to which automatic transcription methods could be ap-\nplied to signed languages.\n50\nCHAPTER 4. INTRODUCTION\n51\nFor our purposes, we relied on SignWriting, an existing universal notation\nsystem for signed languages. Although its 2D construction poses challenges\nfor token representation, the advantages of human readability and comprehen-\nsion of the transcription were deemed critical. SignWriting provided us with a\nframework to pivot from language-specific models to universal ones.\nWith the foundation of our transcription-based approach in place, we then\nexplored the dual aspects of sign language translation, namely the translation\nfrom a sign language video to spoken language text (translation), and from spo-\nken language text to a sign language video (production). The use of lexical tran-\nscription as an intermediary step was examined in both directions.\nIn the following chapters, we will delve into the detailed procedures and\ntechniques utilized in our research, including preliminary work in the field\n(Chapter 5), and the application and evaluation of SignWriting in sign language\ntranslation (Chapter 6) and production (Chapter 7).\nChapter 5\nPreliminary Work\n5.1\nActivity Detection (Moryossef et al., 2020)\nWe propose a lightweight real-time sign language detection model, as we iden-\ntify the need for such a case in videoconferencing. We extract optical flow fea-\ntures based on human pose estimation and, using a linear classifier, show these\nfeatures are meaningful with an accuracy of 80%, evaluated on the Public DGS\nCorpus. Using a recurrent model directly on the input, we see improvements\nof up to 91% accuracy, while still working under 4ms. We describe a demo ap-\nplication to sign language detection in the browser in order to demonstrate its\nusage possibility in videoconferencing applications.\n5.1.1\nIntroduction\nSign language detection (Borg and Camilleri, 2019) is defined as the binary-\nclassification task for any given frame of a video if a person is using sign-\nlanguage or not. Unlike sign language recognition (Camg¨oz et al., 2018; Cui\net al., 2017), where the task is to recognize the form and meaning of signs in a\nvideo, or sign language identification, where the task is to identify which sign\nlanguage is used, the task of sign language detection is to detect when something\nis being signed.\n52\nCHAPTER 5. PRELIMINARY WORK\n53\nWith the recent rise of videoconferencing platforms, we identify the problem\nof signers not “getting the floor” when communicating, which either leads to\nthem being ignored or to a cognitive load on other participants, always checking\nto see if someone starts signing. Hence, we focus on the real-time sign language\ndetection task with uni-directional context to allow for videoconferencing sign\nlanguage prominence.\nWe propose a simple human optical-flow representation for videos based on\npose estimation (§5.1.3), which is fed to a temporally sensitive neural network\n(§5.1.3) to perform a binary classification per frame — is the person signing or\nnot. We compare various possible inputs, such as full-body pose estimation,\npartial pose estimation, and bounding boxes (§5.1.4), and contrast their acqui-\nsition time in light of our targeted real-time application.\nWe demonstrate our approach on the Public DGS Corpus (German Sign\nLanguage) (Hanke et al., 2020), using full-body pose estimation (Schulder and\nHanke, 2019) collected through OpenPose (Cao et al., 2019; Simon et al., 2017).\nWe show results of 87%-91% prediction accuracy depending on the input, with\nper-frame inference time of 350 −3500µs (§5.1.5), and release our training code\nand models1.\n5.1.2\nBackground\nSign Language Detection\nSign language detection (Borg and Camilleri, 2019; Pal et al., 2023) is the bi-\nnary classification task of determining whether signing activity is present in\na given video frame. A similar task in spoken languages is voice activity de-\ntection (VAD) (Sohn et al., 1999; Ramırez et al., 2004), the detection of when a\nhuman voice is used in an audio signal. As VAD methods often rely on speech-\nspecific representations such as spectrograms, they are not necessarily applica-\nble to videos.\n1https://github.com/google-research/google-research/tree/master/sig\nn language detection\nCHAPTER 5. PRELIMINARY WORK\n54\nBorg and Camilleri (2019) introduced the classification of frames taken from\nYouTube videos as either signing or not signing. They took a spatial and tem-\nporal approach based on VGG-16 (Simonyan and Zisserman, 2015) CNN to en-\ncode each frame and used a Gated Recurrent Unit (GRU) (Cho et al., 2014) to\nencode the sequence of frames in a window of 20 frames at 5fps. In addition\nto the raw frame, they either encoded optical-flow history, aggregated motion\nhistory, or frame difference. We improve upon their method by performing sign\nlanguage detection in real time. They identified that sign language use involves\nmovement of the body and, as such, designed a model that works on top of es-\ntimated human poses rather than directly on the video signal. They calculated\nthe optical flow norm of every joint detected on the body and applied a shal-\nlow yet effective contextualized model to predict for every frame whether the\nperson is signing or not.\nWhile these recent detection models achieve high performance, we need\nwell-annotated data that include interference and distractions with non-signing\ninstances for proper real-world evaluation. Pal et al. (2023) conducted a detailed\nanalysis of the impact of signer overlap between the training and test sets on\ntwo sign detection benchmark datasets (Signing in the Wild (Borg and Camil-\nleri, 2019) and the DGS Corpus (Hanke et al., 2020)) used by Borg and Camilleri\n(2019) and by us. By comparing the accuracy with and without overlap, they\nnoticed a relative decrease in performance for signers not present during train-\ning. As a result, they suggested new dataset partitions that eliminate overlap\nbetween train and test sets and facilitate a more accurate evaluation.\nSign Language Identification\nSign language identification (Gebre et al., 2013; Monteiro et al., 2016) classifies\nwhich signed language is used in a given video.\nGebre et al. (2013) found that a simple random-forest classifier utilizing the\ndistribution of phonemes can distinguish between British Sign Language (BSL)\nand Greek Sign Language (ENN) with a 95% F1 score. This finding is further\nsupported by Monteiro et al. (2016), which, based on activity maps in signing\nCHAPTER 5. PRELIMINARY WORK\n55\nspace, manages to differentiate between British Sign Language and French Sign\nLanguage (Langue des Signes Franc¸aise, LSF) with a 98% F1 score in videos\nwith static backgrounds, and between American Sign Language and British\nSign Language, with a 70% F1 score for videos mined from popular video-\nsharing sites. The authors attribute their success mainly to the different fin-\ngerspelling systems, which are two-handed in the case of BSL and one-handed\nin the case of ASL and LSF.\nAlthough these pairwise classification results seem promising, better mod-\nels would be needed for classifying from a large set of signed languages. These\nmethods only rely on low-level visual features, while signed languages have\nseveral distinctive features on a linguistic level, such as lexical or structural dif-\nferences (McKee and Kennedy, 2000; Kimmelman, 2014; Ferreira-Brito, 1984;\nShroyer and Shroyer, 1984) which have not been explored for this task.\n5.1.3\nModel\nFor a video, for every frame given, we would like to predict whether the person\nin the video is signing or not.\nInput Representation\nAs evident by previous work (Borg and Camilleri, 2019), using the raw frames\nas input is computationally expensive, and noisy. Alternatively, in computer\nvision, optical flow is one way to calculate the movement of every object in a\nscene. However, because signing is inherently a human action, we do not care\nabout the flow of every object, but rather only the flow of the human. Optimally,\nwe would like to track the movement of every pixel on the human body from\none frame to another, to gauge its movement vector. As a proxy to such data,\nwe opt for full-body human pose estimation, defining a set of points detected\nin every video frame that marks informative landmarks, like joints and other\nmoving parts (mouth, eyes, eyebrows, and others).\nGetting the optical flow F for these predefined points P at time t is then\nCHAPTER 5. PRELIMINARY WORK\n56\nwell defined as the L2 norm of the vector resulting from subtracting every two\nconsecutive frames. We normalize the flow by the frame-rate in which the video\nwas captured for the representation to be frame-rate invariant (Equation 5.1).\nF(P)t = ||Pt −Pt−1||2 ∗f ps\n(5.1)\nWe note that if a point p was not identified in a given frame t, the value of\nF(p)t and F(p)t+1 automatically equals to 0. This is done to avoid introducing\nfake movements from a poor pose estimation system or unknown movement\nfrom landmarks going out-of-frame.\nAn additional benefit of using full-body pose estimation is that we can nor-\nmalize the size of all people, regardless of whether they use a high-/low-resolution\ncamera and the distance at which they are from the camera.\nTemporal Model\nFigure 5.1 demonstrates our input representation for an example video. It shows,\nto the naked eye, that this representation is meaningful. The movement, indi-\ncated by the bright colors, is well aligned with the gold spans annotation. Thus,\nwe opt to use a shallow sequence tagging model on top of it.\nbody\nface\nleft\nright\nbody\nface\nleft\nright\nFigure 5.1: Optical-flow norm representation of a conversation between two\nsigners. The x-axis is the progression of time, 1,500 frames over 30 seconds in\ntotal. The yellow marks are the gold labels for spans when a signer is signing.\nWe use a uni-directional LSTM (Hochreiter and Schmidhuber, 1997) with\none layer and 64 hidden units directly on this input, normalized for frame\nrate, and project the output to a 2-dimensional vector. For training, we use\nCHAPTER 5. PRELIMINARY WORK\n57\nthe negative-log-likelihood loss on the predicted classes for every frame. For\ninference, we take the arg-max of the output vector (Equation 5.2).\nsigning(P) = arg max LSTM(F(P)) ∗W\n(5.2)\nNote that this model allows us to process each frame as we get it, in real-\ntime, by performing a single step of the LSTM and project its output. Unlike\nautoregressive models, we do not feed the last-frame classification as input for\nthe next frame, as just classifying the new frame with the same tag would al-\nmost get 100% accuracy on this task, depending on gold labels to be available.\nInstead, we rely on the hidden state of the LSTM to hold such information.\n5.1.4\nExperiments\nThe Public DGS Corpus (Hanke et al., 2020) includes 301 videos with an aver-\nage duration of 9 minutes, of two signers in conversation2, at 50fps. Each video\nincludes gloss annotations and spoken language translations (German and En-\nglish). Using this information, we mark each frame as either “signing” (50.9% of\nthe data) or “not-signing” (49.1% of the data) depending on whether it belongs\nto a gloss segment. Furthermore, this corpus is enriched with OpenPose (Cao\net al., 2019) full-body pose estimations (Schulder and Hanke, 2019) including\n137 points per frame (70 for the face, 25 for the body, and 21 for each hand). In\norder to disregard video resolution and distance from the camera, we normalize\neach of these poses such that the mean distance between the shoulders of each\nperson equals 1. We split this dataset into 50:25:25 for training, validation, and\ntest, respectively. For every “part” (face, body, left and right hands), we also\ncalculate its bounding box based on the minimum and maximum value of all of\nthe landmarks.\nWe experiment with three linear baselines with a fixed context (Linear-1,\nLinear-25, Linear-50) and four experimental recurrent models with different\ncounts of input features:\n2There are also monologue story-telling, but both signers are always shown.\nCHAPTER 5. PRELIMINARY WORK\n58\n(a) Pose-All\n(b) Pose-Body\n(c) Pose-Hands\n(d) BBOX\nFigure 5.2: Visualization of our different experiments inputs.\n1. Pose-All—all of the landmarks from the poses. (f. 5.2a)\n2. Pose-Body—only the body landmarks. (f. 5.2b)\n3. Pose-Hands—only the left- and right-hand landmarks. (f. 5.2c)\n4. BBOX—the bounding boxes of the face, body, and hands. (f. 5.2d)\nFinally, we measure the execution time of each model on CPU, using an In-\ntel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz. We measure the execution time per\nframe given a single frame at a time, using multiple frameworks: Scikit-Learn\n(sk) (Pedregosa et al., 2011), TensorFlow (tf) (Abadi et al., 2015) and PyTorch (pt)\n(Paszke et al., 2019b).\n5.1.5\nResults\nTable 5.1 includes the accuracy and inference times for each of our scenarios.\nOur baseline systems show that using a linear classifier with a fixed number of\ncontext frames achieves between 79.9% to 84.3% accuracy on the test set. How-\never, all of the baselines perform worse than our recurrent models, for which\nwe achieve between 87.7% to 91.5% accuracy on the test set. Generally, we see\nthat using more diverse sets of landmarks performs better. Although the hand\nCHAPTER 5. PRELIMINARY WORK\n59\nlandmarks are very indicative, using just the hand BBOX almost matches in ac-\ncuracy, and using the entire body pose, with a single point per hand, performs\nmuch better. Furthermore, we see that regardless of the number of landmarks\nused, our models generally perform faster the fewer landmarks are used. We\nnote that the prediction time varies between the different frameworks, but does\nnot vary much within a particular framework. It is clear, however, that the\nspeed of these models’ is sufficient, as even the slowest model, using the slow-\nest framework, runs at 285 frames-per-second on CPU.\nWe note from manually observing the gold data that sometimes a gloss seg-\nment starts before the person actually begins signing, or moving at all. This\nmeans that our accuracy ceiling is not 100%. We did not perform a rigorous\nre-annotation of the dataset to quantify how extensive this problem is.\nModel\nPoints\nParams\nDev Acc\nTest Acc\n∂t (sk)\n∂t (tf)\n∂t (pt)\nLinear-1\n25\n25\n79.99%\n79.93%\n6.49µs\n823µs\n2.75µs\nLinear-25\n25\n625\n84.13%\n83.79%\n6.78µs\n824µs\n5.10µs\nLinear-50\n25\n1, 250\n85.06%\n83.39%\n6.90µs\n821µs\n7.41µs\nBBOX\n8\n18, 818\n87.49%\n87.78%\n—\n3519µs\n367µs\nPose-Hands\n42\n27, 522\n87.65%\n88.05%\n—\n3427µs\n486µs\nPose-Body\n25\n23, 170\n92.17%\n90.35%\n—\n3437µs\n443µs\nPose-All\n137\n51, 842\n92.31%\n91.53%\n—\n3537µs\n588µs\nTable 5.1: Accuracy and inference-time (∂t) results for the various experiments.\n5.1.6\nAnalysis\nAs we know that different pose landmarks have varying importance to the clas-\nsification, we use the Linear-1 model’s coefficients magnitude to visualize how\nthe different landmarks contribute. Figure 5.3 visualizes the average human\npose in the dataset, with the opacity of every landmark being the absolute value\nof the coefficient.\nFirst, we note that the model attributes no importance to any landmark be-\nlow the waist. This makes sense as they both do not appear in all videos, and\nbare no meaning in sign language. The eyes and nose seem to carry little weight,\nCHAPTER 5. PRELIMINARY WORK\n60\nwhile the ears carry more. We do not attribute this to any signing phenomenon.\nAdditionally, we note hands asymmetry. While both wrists have a high\nweight, the elbow and shoulder for the right hand carry more weights than\ntheir corresponding left counterparts. This could be attributed to the fact that\nmost people are right handed, and that in some sign languages the signer must\ndecide which hand is dominant in a consistent manner. We see this asymme-\ntry as a feature of our model, and note that apps using our models could also\ninclude a “dominant hand” selection.\nTo further understand what situations our models capture, we check mul-\ntiple properties of them on the test set. We start by generally noting that our\ndata is conversational. 84.87% of the time, only one participant is signing, while\n8.5% of the time both participants are signing, and in the remaining 6.63% of\nthe time no one is signing, primarily when the participants are being instructed\non the task.\nOur test set includes 4, 138 signing sequences with an average length of 11.35\nseconds, and a standard deviation of 29.82 seconds. It also includes 4, 091 not-\nsigning sequences with an average length of 9.95 seconds, and a standard devi-\nation of 24.18 seconds.\nFor each of our models, we compare the following error types (Figure 5.4):\n• Bridged—Cases where the model bridged between two signing sections,\nFigure 5.3: The average pose in the dataset. The opacity of every landmark is\ndetermined by its coeffient in the Linear-1 model.\nCHAPTER 5. PRELIMINARY WORK\n61\nFigure 5.4: Visualization of the different types of errors. The first row contains\nthe gold annotations, and the second row contains a model’s prediction.\nstill predicting the person to be signing while the annotation says they are\nnot.\n• Signing Detected Incorrectly—Cases where the model predicted a signing\nspan fully contained within a not-signing annotation.\n• Signing Overflow—Cases where signing was still predicted after a signing\nsection ended.\n• Started Pre-Signing—Cases where signing was predicted before a signing\nsection started.\n• Skipped—Cases where the model did not detect entire signing sections.\n• Signing Undetected Incorrectly—Cases where the model predicted a not-\nsigning span fully contained within a signing annotation.\n• Started Post-Signing—Cases where the signing section started before it\nwas predicted to start.\n• Signing Underflow—Cases where the signing section was predicted to\nend prematurely.\nTable 5.2 includes the number of sequences, including average length and\nstandard deviation in seconds, for each of the error types. Most notably, we see\nthat the less context the model has, the more sporadic its predictions and thus it\nwill generally completely bridge or skip less sequences. The same locality how-\never introduces many signing detected / undetected incorrectly errors, albeit of\nshort lengths.\nCHAPTER 5. PRELIMINARY WORK\n62\nlinear-1\nlinear-25\nlinear-50\nBridged\n107 (0.10 ± 0.15)\n308 (0.34 ± 0.40)\n426 (0.45 ± 0.46)\nSigning Detected Incorrectly\n132151 (0.04 ± 0.07)\n8773 (0.30 ± 0.81)\n6594 (0.34 ± 1.06)\nSigning Overflow\n4094 (0.09 ± 0.15)\n3893 (0.32 ± 0.43)\n3775 (0.46 ± 1.17)\nStarted Pre-Signing\n873 (0.09 ± 0.13)\n345 (0.45 ± 0.68)\n257 (0.88 ± 4.27)\nSkipped\n50 (1.41 ± 1.95)\n298 (1.38 ± 1.43)\n446 (1.49 ± 1.60)\nSigning undetected incorrectly\n219531 (0.05 ± 0.10)\n26185 (0.27 ± 0.50)\n18037 (0.32 ± 0.66)\nStarted Post-Signing\n4199 (0.17 ± 0.23)\n3951 (0.48 ± 0.57)\n3803 (0.60 ± 0.77)\nSigning Underflow\n1677 (0.15 ± 0.26)\n1092 (0.58 ± 0.91)\n827 (0.71 ± 0.96)\nBBOX\nPose-Hands\nPose-Body\nPose-All\nBridged\n754 (0.97 ± 1.94)\n861 (1.26 ± 2.63)\n747 (1.12 ± 2.35)\n573 (0.75 ± 1.08)\nSigning Detected Incorrectly\n5697 (0.64 ± 1.93)\n12919 (0.33 ± 1.33)\n6286 (0.38 ± 1.29)\n11384 (0.25 ± 1.14)\nSigning Overflow\n3337 (0.95 ± 2.10)\n3230 (1.01 ± 2.46)\n3344 (0.67 ± 1.29)\n3518 (0.48 ± 0.87)\nStarted Pre-Signing\n402 (1.33 ± 2.73)\n558 (1.59 ± 5.15)\n298 (1.48 ± 3.87)\n408 (0.70 ± 1.97)\nSkipped\n199 (1.31 ± 1.40)\n115 (1.45 ± 1.54)\n243 (1.31 ± 1.30)\n146 (1.41 ± 1.42)\nSigning undetected incorrectly\n4089 (0.48 ± 0.76)\n3526 (0.26 ± 0.51)\n4786 (0.32 ± 0.60)\n5526 (0.23 ± 0.44)\nStarted Post-Signing\n3939 (0.34 ± 0.44)\n4023 (0.24 ± 0.34)\n3895 (0.37 ± 0.49)\n3992 (0.29 ± 0.36)\nSigning Underflow\n370 (0.82 ± 1.08)\n297 (0.55 ± 0.68)\n506 (0.63 ± 0.97)\n666 (0.44 ± 0.66)\nTable 5.2: We evaluate every model on the different error types, and show\nnumber of sequences with that error, including average sequence length in\nseconds and standard deviation.\nIn the sequential models, we generally see a lower number of sequences\nas they can incorporate global features in the classification. As indicated by\nthe accuracy scores, we see fewer errors of most types the more diverse the in-\nput points are, with one notable exception for the Pose-All model which under-\nperforms Pose-Body on all errors except for Bridged and Skipped.\n5.1.7\nDemo Application\nWith this publication, we release a demo application working in the browser\nfor computers and mobile devices. Pragmatically, we choose to use the “Pose-\nBody” model variant, as it performs almost on par with our best model, “Pose-\nAll”, and we find it is feasible to acquire the human body poses in real-time\nwith currently available tools.\nWe use PoseNet (Papandreou et al., 2017, 2018) running in the browser using\nTensorFlow.js (Smilkov et al., 2019). PoseNet includes two main image encod-\ning variants: MobileNet (Howard et al., 2017), which is a lightweight model\naimed at mobile devices, and ResNet (He et al., 2016), which is a larger model\nCHAPTER 5. PRELIMINARY WORK\n63\nthat requires a dedicated GPU. Each model includes many sub-variants with\ndifferent image resolution and convolutional strides, to further allow for tailor-\ning the network to the user’s needs. In our demo, we first tailor a network to the\ncurrent device to run at least at 25fps. While using a more lightweight network\nmight be faster, it might also introduce pose estimation errors.\nThe pose estimation we use only returns 17 points compared to the 25 of\nOpenPose; hence, we map the 17 points to the corresponding indexes for Open-\nPose. We then normalize the body pose vector by the mean shoulder width the\nperson had in the past 50 frames in order to disregard camera resolution and\ndistance of the signer from the camera.\nOnward, there are two options: either send the pose vector to the video-\nconferencing server where inference could be done or perform the inference\nlocally. As our method is faster than real-time, we chose the latter and perform\ninference on the device using TensorFlow.js. For every frame, we get a signing\nprobability, which we then show on the screen.\nIn a production videoconferencing application, this signing probability should\nbe streamed to the call server, where further processing could be done to show\nthe correct people on screen. We suggest using the signing probability as a\nnormalized “volume”, such that further processing is comparable to videocon-\nferencing users using speech.\nWhile this is the recommended way to add sign language detection to a\nvideoconferencing app, as the goal of this work is to empower signers, our\ndemo application can trigger the speaker detection by transmitting audio when\nthe user is signing. Transmitting ultrasonic audio at 20KHz, which is inaudible\nfor humans, manages to fool Google Meet, Zoom and Slack into thinking the\nuser is speaking, while still being inaudible. One limitation of this method is\nthat videoconferencing app developers can crop the audio to be in the audible\nhuman range and thus render this application useless. Another limitation is\nthat using high-frequency audio can sound crackly when compressed, depend-\ning on the signer’s internet connection strength.\nOur model and demo, in their current forms, only allow for the detection of\nCHAPTER 5. PRELIMINARY WORK\n64\na single signer per video stream. However, if we can detect more than a single\nperson, and track which poses belong to which person in every frame, there is\nno limitation to run our model independently on each signer.\n5.1.8\nDiscussion\nLimitations\nWe note several limitations to our approach. The first is that it relies on the\npose estimation system to run in real-time on any user’s device. This proves\nto be challenging, as even performing state-of-the-art pose estimation on a sin-\ngle frame on a GPU with OpenPose (Cao et al., 2019; Cao et al., 2017) can take\nupwards of 300ms, which introduces two issues: (1) If in order to get the optical-\nflow, we need to pose two frames, we create a delay from when a person starts\nsigning to when they could be accurately detected as signing, equal to at least\ntwo times the pose processing time. (2) Running this on mobile devices or de-\nvices without hardware acceleration like a GPU may be too slow.\nAs we only look at the input’s optical flow norm, our model might not be\nable to pick up on times when a person is just gesturing rather than signing.\nHowever, as this approach is targeted directly at sign language users rather\nthan the general non-signing public, erring on the side of caution and detecting\nany meaningful movements is preferred.\nDemographic Biases\nThe data we use for training was collected from various regions of Germany,\nwith equal number of males and females, as well as an equal number of partic-\nipants from different age groups (Schulder et al., 2020). Although most of the\npeople in the dataset are European white, we do not attribute any significance\nbetween the color of their skin to the performance of the system, as long as the\npose estimation system is not biased.\nRegardless of age, gender, and race, we do not address general ethnic biases\nCHAPTER 5. PRELIMINARY WORK\n65\nsuch as different communities of signers outside of Germany signing differently\n- whether it is the size, volume, speed, or other properties.\nSigner Independence\nFollowing the publication of this work, Pal et al. (2023) conducted a detailed\nanalysis of the impact of signer overlap between our training and test sets. By\ncomparing the accuracy with and without overlap, they noticed a relative de-\ncrease in performance for signers not present during training. As a result, they\nsuggested new dataset partitions that eliminate overlap between train and test\nsets and facilitate a more accurate evaluation of performance.\n5.1.9\nConclusions\nWe propose a simple human optical-flow representation for videos based on\npose estimation to perform a binary classification per frame — is the person\nsigning or not. We compare various possible inputs, such as full-body pose es-\ntimation, partial pose estimation, and bounding boxes and contrast their acqui-\nsition time in light of our targeted real-time videoconferencing sign language\ndetection application.\nWe demonstrate our approach on the Public DGS Corpus (German Sign Lan-\nguage), and show results of 87%-91% prediction accuracy depending on the in-\nput, with per-frame inference time of 350 −3500µs.\nCHAPTER 5. PRELIMINARY WORK\n66\n5.2\nIsolated Recognition (Moryossef et al., 2021b)\nIn this section, we explore whether or not estimated skeletal poses are viable for\nuse in sign language recognition. A large part of this section was independently\npublished as “Evaluating the Immediate Applicability of Pose Estimation for\nSign Language Recognition”.\nSign languages are visual languages produced by the movement of the hands,\nface, and body. In this paper, we evaluate representations based on skeleton\nposes, as these are explainable, person-independent, privacy-preserving, low-\ndimensional representations. Basically, skeletal representations generalize over\nan individual’s appearance and background, allowing us to focus on the recog-\nnition of motion. But how much information is lost by the skeletal representa-\ntion? We perform two independent studies using two state-of-the-art pose es-\ntimation systems. We analyze the applicability of the pose estimation systems\nto sign language recognition by evaluating the failure cases of the recognition\nmodels. Importantly, this allows us to characterize the current limitations of\nskeletal pose estimation approaches in sign language recognition.\n5.2.1\nIntroduction\nSign languages are visual languages produced by the movement of the hands,\nface, and body. As languages that rely on visual communication, recordings are\nin video form. Current state-of-the-art sign language processing systems rely\non the video to model tasks such as sign language recognition (SLR) and sign\nlanguage translation (SLT). However, using the raw video signal is computa-\ntionally expensive and can lead to overfitting and person dependence.\nIn an attempt to abstract over the video information, skeleton poses have\nbeen suggested as an explainable, person-independent, privacy-preserving, and\nlow-dimensional representation that provides the signer body pose and infor-\nmation on how it changes over time. Theoretically, skeletal poses contain all the\nrelevant information required to understand signs produced in videos, except\nfor interactions with elements in space (for example, a mug or a table).\nCHAPTER 5. PRELIMINARY WORK\n67\nThe recording of accurate human skeleton poses is difficult and often intru-\nsive, requiring signers to wear specialized and expensive motion capture hard-\nware. Fortunately, advances in computer vision now allow the estimation of\nhuman skeleton poses directly from videos. However, as these estimation sys-\ntems were not specifically designed with sign language in mind, we currently\ndo not understand their suitability for use in processing sign languages both in\nrecognition or production.\nIn this study, we evaluate two pose estimation systems and demonstrate\ntheir suitability (and limitations) for SLR by conducting two independent stud-\nies on the CVPR21 ChaLearn challenge Sincan et al. (2021). Because we perform\nno pretraining of the skeletal model, the final results are considerably lower\nthan potential end-to-end approaches (§5.2.3). The results demonstrate that the\nskeletal representation loses considerable information. To better understand\nwhy, we evaluate our approaches (§5.2.4), categorize their failure cases (§5.2.5),\nand conclude by characterizing the attributes a pose estimation system should\nhave to be applicable for SLR (§5.2.6).\n5.2.2\nBackground\nPose Estimation\nVideo-to-Pose—commonly known as pose estimation—is the task of detecting\nhuman figures in images and videos, so that one could determine, for example,\nwhere someone’s elbow shows up in an image. It was shown that the face\npose correlates with facial non-manual features like head direction (Vogler and\nGoldenstein, 2005).\nThis area has been thoroughly researched (Pishchulin et al., 2012; Chen et al.,\n2017; Cao et al., 2019; G¨uler et al., 2018) with objectives varying from predicting\n2D / 3D poses to a selection of a small specific set of landmarks or a dense mesh\nof a person.\nOpenPose (Cao et al., 2019; Simon et al., 2017; Cao et al., 2017; Wei et al.,\n2016) is the first multi-person system to jointly detect human body, hand, facial,\nCHAPTER 5. PRELIMINARY WORK\n68\nand foot keypoints (in total 135 keypoints) in 2D on single images. While their\nmodel can estimate the full pose directly from an image in a single inference,\nthey also suggest a pipeline approach where they first estimate the body pose\nand then independently estimate the hands and face pose by acquiring higher\nresolution crops around those areas. Building on the slow pipeline approach, a\nsingle network whole body OpenPose model has been proposed (Hidalgo et al.,\n2019), which is faster and more accurate for the case of obtaining all keypoints.\nWith multiple recording angles, OpenPose also offers keypoint triangulation to\nreconstruct the pose in 3D.\nDensePose (G¨uler et al., 2018) takes a different approach. Instead of classify-\ning for every keypoint which pixel is most likely, they suggest a method similar\nto semantic segmentation, for each pixel to classify which body part it belongs\nto. Then, for each pixel, knowing the body part, they predict where that pixel\nis on the body part relative to a 2D projection of a representative body model.\nThis approach results in the reconstruction of the full-body mesh and allows\nsampling to find specific keypoints similar to OpenPose.\nHowever, 2D human poses might not be sufficient to fully understand the\nposition and orientation of landmarks in space, and applying pose estimation\nper frame disregards video temporal movement information into account, es-\npecially in cases of rapid movement, which contain motion blur.\nPavllo et al. (2019) developed two methods to convert between 2D poses\nto 3D poses. The first, a supervised method, was trained to use the temporal\ninformation between frames to predict the missing Z-axis. The second is an\nunsupervised method, leveraging the fact that the 2D poses are merely a pro-\njection of an unknown 3D pose and training a model to estimate the 3D pose\nand back-project to the input 2D poses. This back projection is a deterministic\nprocess, applying constraints on the 3D pose encoder. Zelinka and Kanis (2020)\nfollowed a similar process and added a constraint for bones to stay of a fixed\nlength between frames.\nPanteleris et al. (2018) suggest converting the 2D poses to 3D using inverse\nkinematics (IK), a process taken from computer animation and robotics to calcu-\nlate the variable joint parameters needed to place the end of a kinematic chain,\nCHAPTER 5. PRELIMINARY WORK\n69\nsuch as a robot manipulator or animation character’s skeleton, in a given po-\nsition and orientation relative to the start of the chain. Demonstrating their\napproach to hand pose estimation, they manually explicitly encode the con-\nstraints and limits of each joint, resulting in 26 degrees of freedom. Then, non-\nlinear least-squares minimization fits a 3D model of the hand to the estimated\n2D joint positions, recovering the 3D hand pose. This process is similar to the\nback-projection used by Pavllo et al. (2019), except here, no temporal informa-\ntion is being used.\nMediaPipe Holistic (Grishchenko and Bazarevsky, 2020) attempts to solve\n3D pose estimation by taking a similar approach to OpenPose, having a pipeline\nsystem to estimate the body, then the face and hands. Unlike OpenPose, the\nestimated poses are in 3D, and the pose estimator runs in real-time on CPU,\nallowing for pose-based sign language models on low-powered mobile devices.\nThis pose estimation tool is widely available and built for Android, iOS, C++,\nPython, and the Web using JavaScript.\nSign Language Recognition\nSign language recognition (SLR) (Adaloglou et al., 2020) detects and labels signs\nfrom a video, either on isolated (Imashev et al., 2020; Sincan and Keles, 2020) or\ncontinuous Cui et al. (2017); Camg¨oz et al. (2018, 2020b) signs.\nThis task has been attempted both with computer vision models, assuming\nthe input is the raw video, and with poses, assuming the video has been pro-\ncessed with a pose estimation tool.\nVideo to Sign\nVideo-to-Gloss, also known as sign language recognition, is the\ntask of recognizing a sequence of signs from a video.\nFor this recognition, Cui et al. (2017) constructs a three-step optimization\nmodel. First, they train a video-to-gloss end-to-end model, where they encode\nthe video using a spatio-temporal CNN encoder and predict the gloss using a\nConnectionist Temporal Classification (CTC) (Graves et al., 2006). Then, from\nthe CTC alignment and category proposal, they encode each gloss-level seg-\nCHAPTER 5. PRELIMINARY WORK\n70\nment independently, trained to predict the gloss category, and use this gloss\nvideo segments encoding to optimize the sequence learning model.\nCamg¨oz et al. (2018) fundamentally differ from that approach and formu-\nlate this problem as if it is a natural-language translation problem. They en-\ncode each video frame using AlexNet (Krizhevsky et al., 2012), initialized us-\ning weights trained on ImageNet (Deng et al., 2009). Then they apply a GRU\nencoder-decoder architecture with Luong Attention (Luong et al., 2015) to gen-\nerate the gloss. In follow-up work, Camg¨oz et al. (2020b) use a transformer\nencoder (Vaswani et al., 2017) to replace the GRU and use a CTC to decode the\ngloss. They show a slight improvement with this approach on the video-to-\ngloss task.\nAdaloglou et al. (2020) perform a comparative experimental assessment of\ncomputer vision-based methods for the video-to-gloss task. They implement\nvarious approaches from previous research (Camg¨oz et al., 2017; Cui et al., 2019;\nVaezi Joze and Koller, 2019) and test them on multiple datasets (Huang et al.,\n2018; Camg¨oz et al., 2018; Von Agris and Kraiss, 2007; Vaezi Joze and Koller,\n2019) either for isolated sign recognition or continuous sign recognition. They\nconclude that 3D convolutional models outperform models using only recur-\nrent networks to capture the temporal information, and that these models are\nmore scalable given the restricted receptive field, which results from the CNN\n“sliding window” technique.\nMomeni et al. (2022) developed a comprehensive pipeline that combines var-\nious models to densely annotate sign language videos. By leveraging the use\nof synonyms and subtitle-signing alignment, their approach demonstrates the\nvalue of pseudo-labeling from a sign recognition model for sign spotting. They\npropose a novel method to increase annotations for both known and unknown\nclasses, relying on in-domain exemplars. As a result, their framework signifi-\ncantly expands the number of confident automatic annotations on the BOBSL\nBSL sign language corpus (Albanie et al., 2021) from 670K to 5M, and they gen-\nerously make these annotations publicly available.\nCHAPTER 5. PRELIMINARY WORK\n71\nPose to Sign\nPose-to-Gloss, also known as sign language recognition, is the\ntask of recognizing a sequence of signs from a sequence of poses. Though some\nprevious works have referred to this as “sign language translation,” recogni-\ntion merely determines the associated label of each sign, without handling the\nsyntax and morphology of the signed language (Padden, 1988) to create a spo-\nken language output. Instead, SLR has often been used as an intermediate step\nduring translation to produce glosses from signed language videos.\nJiang et al. (2021) proposed a novel Skeleton Aware Multi-modal Framework\nwith a Global Ensemble Model (GEM) for isolated SLR (SAM-SLR-v2) to learn\nand fuse multimodal feature representations. Specifically, they use a Sign Lan-\nguage Graph Convolution Network (SL-GCN) to model the embedded dynam-\nics of skeleton keypoints and a Separable Spatial-Temporal Convolution Net-\nwork (SSTCN) to exploit skeleton features. The proposed late-fusion GEM fuses\nthe skeleton-based predictions with other RGB and depth-based modalities to\nprovide global information and make an accurate SLR prediction.\nDafnis et al. (2022) work on the same modified WLASL dataset as Jiang et al.\n(2021), but do not require multimodal data input. Instead, they propose a bidi-\nrectional skeleton-based graph convolutional network framework with linguis-\ntically motivated parameters and attention to the start and end frames of signs.\nThey cooperatively use forward and backward data streams, including various\nsub-streams, as input. They also use pre-training to leverage transfer learning.\nSelvaraj et al. (2022) introduced an open-source OpenHands library, which\nconsists of standardized pose datasets for different existing sign language datasets\nand trained checkpoints of four pose-based isolated sign language recognition\nmodels across six languages (American, Argentinian, Chinese, Greek, Indian,\nand Turkish). To address the lack of labeled data, they propose self-supervised\npretraining on unlabeled data and curate the largest pose-based pretraining\ndataset on Indian Sign Language (Indian-SL). They established that pretrain-\ning is effective for sign language recognition by demonstrating improved fine-\ntuning performance especially in low-resource settings and high crosslingual\ntransfer from Indian-SL to a few other sign languages.\nThe work of Kezar et al. (2023), based on the OpenHands library, explicitly\nCHAPTER 5. PRELIMINARY WORK\n72\nrecognizes the role of phonology to achieve more accurate isolated sign lan-\nguage recognition (ISLR). To allow additional predictions on phonological char-\nacteristics (such as handshape), they combine the phonological annotations in\nASL-LEX 2.0 (Sehyr et al., 2021) with signs in the WLASL 2000 ISLR benchmark\n(Li et al., 2020). Interestingly, Tavella et al. (2022) construct a similar dataset\naiming just for phonological property recognition in American Sign Language.\n5.2.3\nExperiments\nTo evaluate whether pose estimation models are applicable for SLR, we par-\nticipated in the CVPR21 ChaLearn challenge for person-independent isolated\nSLR on the Ankara University Turkish Sign Language (AUTSL) Sincan and Ke-\nles (2020) dataset. Even though the dataset includes Kinect pose estimations,\nKinect poses have not been made available for the challenge. We processed the\ndataset using two pose estimation tools: 1. OpenPose Single-Network Whole-\nBody Pose Estimation Hidalgo et al. (2019); and 2.\nMediaPipe Holistic Gr-\nishchenko and Bazarevsky (2020); and made the data available via an open-\nsource sign language datasets repository Moryossef and M¨uller (2021).\nWe approach the recognition task with two independent experiments per-\nformed by different teams unaware of the other team’s work throughout the\nvalidation stage. In the validation stage, each team focussed on one pose es-\ntimation approach, and in the test stage, both teams got access to both pose\nestimation outputs. We eventually submitted three systems: 1. based on Open-\nPose poses; 2. based on Holistic poses; 3. based on both OpenPose and Holistic\nposes combined (concatenated).\nTeam 1\nTeam 1 worked with OpenPose Hidalgo et al. (2019) pose estimation output\nand used the SLR transformer architecture from Camg¨oz et al. Camg¨oz et al.\n(2020b). The model takes as input a series of feature vectors, in this case, hu-\nman upper body skeletal coordinates extracted from the video frames. These\nCHAPTER 5. PRELIMINARY WORK\n73\nConnectionist Temporal Classification\nLinear\nLinear\nLinear\nLinear\nLinear\nLinear\nKE\nKE\nKE\nSelf-Attention\nFF\nFF\nFF\nFF\nFF\nFF\nAdd & Normalize\nAdd & Normalize\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nPE\nPE\nLinear\nLinear\nLinear\nLinear\nLinear\nLinear\nPE\nFigure 5.5: Diagram of Team 1’s model with one subnetwork (in green). (KE:\nKeypoint extraction, PE: Positional encoding, FF: feed forward)\nCHAPTER 5. PRELIMINARY WORK\n74\nare each projected to a lower dimension hidden state vector. The size of the\nhidden state remains constant throughout the subsequent operations. A sinu-\nsoidal positional encoding is added to provide temporal information. This is\nthen passed to a subnetwork consisting of a multiheaded self-attention layer,\nfollowed by a feedforward layer. After each of these layers, the output is added\nto the input and normalized. This subnetwork can be repeated any number of\ntimes. Finally, the output is fed to a linear layer and softmax to give probabili-\nties for each class (Figure 5.5).\nThe model is trained using CTC loss. This is designed to allow the output to\nbe invariant to alignment; however, this is not a significant concern when there\nshould only be one output symbol. The final prediction is obtained via CTC\nbeam search decoding, collapsing multiple same class outputs into one. As the\nmodel is trained to predict a single class per video, it does not predict different\nclasses within a sequence.\nThe number of layers, heads, hidden size, and dropout rate affect the model\ncomplexity.\nThere is, therefore, a tradeoff between sufficient complexity to\nmodel the data and overfitting.\nAdditionally, as a baseline, the pose estimation keypoints were replaced\nwith the output of three off-the-shelf image-based frame feature extractors, giv-\ning us small dense representations for each frame. Three extractors were used:\n1. EfficientNet-B7 Tan and Le (2019); 2. I3D trained on Kinetics Carreira and\nZisserman (2017); and 3. I3D trained on BSL1K Albanie et al. (2020).\nTeam 2\nTeam 2 worked with the MediaPipe Holistic Grishchenko and Bazarevsky (2020)\npose estimation system output. From the 543 landmarks, the face mesh was re-\nmoved which consists of 468 landmarks and the remaining 75 landmarks were\nused for the body and hands.\nA standard sequence classification architecture was used. The model takes\nas input a series of feature vectors, constructed from a flat vector representa-\ntion of the pose concatenated with the 2D angle and length of every limb, using\nCHAPTER 5. PRELIMINARY WORK\n75\nLinear\nKE\nKE\nKE\nLSTM\nLSTM\nLSTM\nSoftmax\nLinear\nLinear\nLinear\nLinear\nLinear\nLinear\nFeature Extractors: Angles, Length, Raw Keypoints\nBatch Norm\nBatch Norm\nBatch Norm\n \nLSTM\nLSTM\nLSTM\nMax Pooling\n \n \nFigure 5.6: Diagram of Team 2’s model. (KE: Keypoint extraction)\nCHAPTER 5. PRELIMINARY WORK\n76\nthe pose-format3 library. These representations are subjected to a 20% dropout,\nnormalized using 1D batch normalization, and are projected to a lower dimen-\nsion hidden state vector (512 dimensions). This is then passed to a two-layer\nBiLSTM with hidden dimension 256, followed by a max-pooling operation to\nobtain a single representation vector per video. Finally, the output is fed to a\nlinear layer and softmax to give probabilities for each class (Figure 5.6).\nThe model is trained using cross-entropy loss with the Adam optimizer\n(with default parameters) and a batch size of 512 on a single GPU. No data aug-\nmentation or frame dropout is applied at training time, except for horizontal\nframe flip to account for left-handed signers in the dataset.\n5.2.4\nResults\nTable 5.3 shows our teams’ results on the validation set. We note that both\nteams’ approaches using pose estimation performed similarly, with validation\naccuracy ranging between 80% and 85%. It rules out trivial errors and imple-\nmentation issues that, despite working independently, and with two separate\npose estimation tools, both teams achieve similar evaluation scores. Further-\nmore, from a comparison between the pose estimation based systems (80-85%)\nand the pretrained image feature extractors (38-68%), we can see that pose es-\ntimation features do indeed generalize better to the nature of the challenge, in-\ncluding unseen signers and backgrounds.\nWe submitted Team 2’s test set predictions to the official challenge evalu-\nation. On the test set, both OpenPose and Holistic performed equally well de-\nspite making different predictions, each with 78.35% test set accuracy. However,\nour combined system, which was trained using both pose estimations, achieves\n81.93% test set accuracy.\n3https://github.com/AmitMY/pose-format\nCHAPTER 5. PRELIMINARY WORK\n77\nTeam 1\nTeam 2\nEfficientNet-B7\n38.80%\n—\nI3D (Kinetics)\n47.46%\n—\nI3D (BSL1K)\n68.65%\n—\nOpenPose\n83.25%\n79.99%\nHolistic\n85.63%\n82.14%\nOpenPose+Holistic\n84.16%\n82.89%\nTable 5.3: Results evaluated on the validation set with various frame-level\nfeatures.\n5.2.5\nAnalysis\nThe interpretability of skeletal poses allows us to assess them qualitatively us-\ning visualisation. We manually review our model’s failure cases and categorize\nthem into two main categories: hands interaction and hand-face interaction.\nFigure 5.7: Example of hands interaction, where the pose estimation fails for\none of the hands (Holistic).\nHands Interaction\nWhen there exists an interaction between both hands, or\none hand occludes the other from the camera’s view, we often fail to estimate\nthe pose of one of the hands (Figure 5.7) or estimate it incorrectly such that the\ninteraction is not clearly shown (Figure 5.8).\nCHAPTER 5. PRELIMINARY WORK\n78\nFigure 5.8: Example of hands interaction, where the pose estimation does not\nreflect the existing interaction (Holistic).\nHand-Face Interaction\nWhen there exists an interaction between a hand and\nthe face, or one hand overlaps with the face from the camera’s angle, we often\nfail to estimate the pose of the interacting hand (Figure 5.9).\nFigure 5.9: Example of hand-face interaction, where the pose estimation fails\nfor the interacting hand (Holistic).\nThese cases of missed interactions between the different body parts often\nlose the essence of the sign, where the interaction and the hand shape are the\nmain distinguishing features for those signs, and thus hinder the model’s ability\nto extract meaningful information from the pose that is relevant to the sign.\nPresence or absence of hand pose\nWe describe a number of failure cases of\nHolistic pose estimation above. Many of them mean that keypoints for the\nCHAPTER 5. PRELIMINARY WORK\n79\nhands are not available at all, since Holistic can omit them if it fails to detect\nthe hand. As a complementary quantitative analysis, we correlate prediction\noutcomes with the average number of frames where hand pose was present\n(Figure 5.10).\nFigure 5.10: Distribution of percent of frames containing the Holistic pose\nestimation of the dominant hand in each validation sample, grouped by\nwhether the final prediction of our model was correct.\nWe find that on average, for all correct predictions the percentage of frames\nthat do contain hand keypoints (85.13%) is significantly higher4 than for all in-\ncorrect predictions (79.78%). This is in line with our qualitative analysis.\n5.2.6\nConclusions\nAlthough many teams outperformed our models that use only off-the-shelf\nskeletal representations, with the best submission reaching 98.4% test set ac-\ncuracy, it is unclear how well such approaches will generalise to other datasets.\nOur initial questions related to how good skeletal representations are for recog-\nnition, given their natural ability to generalise. However, performance in the\n4We tested for a significant difference of the mean values with a Wilcoxon rank-sum test\nWilcoxon (1992), p < 0.0001.\nCHAPTER 5. PRELIMINARY WORK\n80\nChaLearn challenge suggests that despite their benefits, considerable informa-\ntion is lost in the skeletal representation that must be represented in the image\ndomain. A qualitative analysis of our models’ failure cases shows that pose es-\ntimation tools suffer from shortcomings when body parts interact. We conclude\nthat pose estimation tools are not immediately applicable for the use in sign lan-\nguage recognition – the current representations are not sufficiently expressive,\nand that further improvements with regard to interacting body parts is crucial\nfor their applicability.\n5.2.7\nDiscussion\nAs shown in Figure 5.10, improved pose estimation strongly correlates with bet-\nter prediction accuracy. Since this work’s publication in 2021, more advanced\nmodels have emerged, surpassing MediaPipe Holistic and OpenPose, such as\nMeta’s Sapiens model (Khirodkar et al., 2024). These new models significantly\nenhance pose estimation and may offer substantial improvements over the re-\nsults presented here. By 2024, these advancements could make pose estimation\nmore applicable to sign language recognition.\nCHAPTER 5. PRELIMINARY WORK\n81\n5.3\nGloss Translation (Moryossef et al., 2021c)\nIn this section, we explore the maximum potential of translating from signed to\nspoken language using glosses, based on the assumption of flawless sign lan-\nguage recognition. Among other things, the findings reveal that glosses alone\ncannot fully encapsulate the nuances of sign language.\nSign language translation (SLT) is often decomposed into video-to-gloss recog-\nnition and gloss-to-text translation, where a gloss is a sequence of transcribed\nspoken-language words in the order in which they are signed. We focus here\non gloss-to-text translation, which we treat as a low-resource neural machine\ntranslation (NMT) problem. However, unlike traditional low-resource NMT,\ngloss-to-text translation differs because gloss-text pairs often have a higher lex-\nical overlap and lower syntactic overlap than pairs of spoken languages. We\nexploit this lexical overlap and handle syntactic divergence by proposing two\nrule-based heuristics that generate pseudo-parallel gloss-text pairs from mono-\nlingual spoken language text. By pre-training on this synthetic data, we im-\nprove translation from American Sign Language (ASL) to English and German\nSign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.\n5.3.1\nIntroduction\nSign language is the most natural mode of communication for the Deaf. How-\never, in a predominantly hearing society, they often resort to lip-reading, text-\nbased communication, or closed-captioning to interact with others. Sign lan-\nguage translation (SLT) is an important research area that aims to improve com-\nmunication between signers and non-signers while allowing each party to use\ntheir preferred language. SLT consists of translating a sign language (SL) video\ninto a spoken language (SpL) text, and current approaches often decompose\nthis task into two steps: (1) video-to-gloss, or continuous sign language recog-\nnition (CSLR) (Cui et al., 2017; Camg¨oz et al., 2018); (2) gloss-to-text, which is a\ntext-to-text machine translation (MT) task (Camg¨oz et al., 2018; Yin and Read,\n2020a).\nCHAPTER 5. PRELIMINARY WORK\n82\nfs-JOHN FUTURE FINISH READ BOOK WHEN HOLD\nWhen will John ﬁnish reading the book?\nASL Video:\nASL Gloss:\nEnglish:\nGLOSSING\nTRANSLATION\n(a) ASL video with gloss annotation and English translation\nFORWARD LOOK TOMORROW CHILD SEE\nI'm looking forward to seeing the children tomorrow.\nSynthetic Gloss:\nEnglish:\nI look forward to seeing the child tomorrow.\nModel Output:\nGENERATE\nTRAIN\n(b) Data augmentation and training\nFigure 5.11: Real and synthetic gloss-spoken pairs.\nIn this paper, we focus on gloss-to-text translation. SL data and resources\nare often scarce, or nonexistent (§5.3.2; Bragg et al. (2019)). Gloss-to-text trans-\nlation is, therefore, an example of an extremely low-resource MT task. However,\nwhile there is extensive literature on low-resource MT between spoken lan-\nguages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019),\nthe dissimilarity between sign and spoken languages calls for novel methods.\nSpecifically, as SL glosses borrow the lexical elements from their ambient spo-\nken language, handling syntax and morphology poses greater challenges than\nlexeme translation (§5.3.4).\nIn this work, we (1) discuss the scarcity of SL data and quantify how the\nrelationship between a sign and spoken language pair is different from a pair\nof two spoken languages; (2) show that the de facto method for data augmenta-\ntion using back-translation is not viable in extremely low-resource SLT; (3) pro-\npose two rule-based heuristics that exploit the lexical overlap and handles the\nsyntactic divergence between sign and spoken language, to synthesize pseudo-\nparallel gloss/text examples (Figure 5.11b); (4) demonstrate the effectiveness of\nour methods on two sign-to-spoken language pairs.\nCHAPTER 5. PRELIMINARY WORK\n83\n5.3.2\nBackground\nSign Language Glossing\nSLs are often transcribed word-for-word using a\nspoken language through glossing to aid in language learning, or automatic sign\nlanguage processing (Ormel et al., 2010). While many SL glosses are words from\nthe ambient spoken language, glossing preserves SL’s original syntactic struc-\nture and therefore differs from translation (Figure 5.11a).\nData Scarcity\nWhile standard machine translation architectures such as the\nTransformer (Vaswani et al., 2017) achieve reasonable performance on gloss-to-\ntext datasets (Yin and Read, 2020b; Camg¨oz et al., 2020b), parallel SL and spoken\nlanguage corpora, especially those with gloss annotations, are usually far more\nscarce when compared with parallel corpora that exist between many spoken\nlanguages (Table 5.4).\nLanguage Pair\n# Parallel Gloss-Text Pairs\nVocabulary Size (Gloss / Spoken)\nSignum (Von Agris and Kraiss, 2007)\nDGS-German\n780\n565 / 1,051\nNCSLGR (SignStream, 2007)\nASL-English\n1,875\n2,484 / 3,104\nRWTH-PHOENIX-Weather 2014T (Camg¨oz et al., 2018)\nDGS-German\n7,096 + 519 + 642\n1,066 / 2,887 + 393 / 951 + 411 / 1,001\nDicta-Sign-LSF-v2 (Limsi, 2019)\nFrench SL-French\n2,904\n2,266 / 5,028\nThe Public DGS Corpus (Hanke et al., 2020)\nDGS-German\n63,912\n4,694 / 23,404\nTable 5.4: Some publicly available SL corpora with gloss annotations and\nspoken language translations.\n5.3.3\nPrevious Work\nGloss-to-Text, also known as sign language translation, is the natural language\nprocessing task of translating between gloss text representing sign language\nsigns and spoken language text. These texts commonly differ in terminology,\ncapitalization, and sentence structure.\nCamg¨oz et al. (2018) experimented with various machine-translation archi-\ntectures and compared using an LSTM (Hochreiter and Schmidhuber, 1997)\nvs. GRU for the recurrent model, as well as Luong attention (Luong et al.,\n2015) vs. Bahdanau attention (Bahdanau et al., 2015) and various batch sizes.\nThey concluded that on the RWTH-PHOENIX-Weather-2014T dataset, which\nCHAPTER 5. PRELIMINARY WORK\n84\nwas also presented in this work, using GRUs, Luong attention, and a batch size\nof 1 outperforms all other configurations.\nIn parallel with the advancements in spoken language machine translation,\nYin and Read (2020a) proposed replacing the RNN with a Transformer (Vaswani\net al., 2017) encoder-decoder model, showing improvements on both RWTH-\nPHOENIX-Weather-2014T (DGS) and ASLG-PC12 (ASL) datasets both using a\nsingle model and ensemble of models. Interestingly, in gloss-to-text, they show\nthat using the sign language recognition (video-to-gloss) system output outper-\nforms using the gold annotated glosses.\n5.3.4\nSigned vs. Spoken Language\nDue to the paucity of parallel data for gloss-to-text translation, we can treat it as\na low-resource translation problem and apply existing techniques for improv-\ning accuracy in such settings. However, we argue that the relationship between\nglossed SLs and their spoken counterparts is different from the usual relation-\nship between two spoken languages. Specifically, glossed SLs are lexically simi-\nlar but syntactically different from their spoken counterparts. This contrasts heav-\nily with the relationship among spoken language pairs where lexically similar\nlanguages tend also to be syntactically similar the great majority of the time.\nTo demonstrate this empirically, we adopt measures from (Lin et al., 2019)\nto measure the lexical and syntactic similarity between languages, two fea-\ntures also shown to be positively correlated with the effectiveness of performing\ncross-lingual transfer in MT.\nLexical similarity\nbetween two languages is measured using word overlap:\now = |T1 ∩T2|\n|T1| + |T2|\nwhere T1 and T2 are the sets of types in a corpus for each language. The word\noverlap between spoken language pairs is calculated using the TED talks dataset\n(Qi et al., 2018). The overlap between sign-spoken language pairs is calculated\nCHAPTER 5. PRELIMINARY WORK\n85\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nLexical Similarity\n0.3\n0.4\n0.5\n0.6\n0.7\nSyntactic Similarity\naze-eng\nbel-eng\nspa-por\nfra-por\nglg-eng\nheb-por\nita-por\npor-eng\nrus-eng\nrus-por\ntur-eng\nase-eng\nbfi-eng\ngsg-deu\nfsl-fra\nSpL-SpL\nSL-SpL\nFigure 5.12: Lexical and syntactic similarity between different language pairs\ndenoted by their ISO639-2 codes.\nfrom the corresponding corpora in Table 5.4.\nSyntactic similarity\nbetween two languages is measured by 1 −dsyn where\ndsyn is the syntactic distance from (Littell et al., 2017) calculated by taking the\ncosine distance between syntactic features adapted from the World Atlas of Lan-\nguage Structures (Dryer and Haspelmath, 2013).\nFigure 5.12 shows that sign-spoken language pairs are indeed outliers with\nlower syntactic similarity and higher lexical similarity. We seek to leverage this\nfact and the high availability of monolingual spoken language data to compen-\nsate for the scarcity of SL resources. In the following section, we propose data\naugmentation techniques using word order modifications to create synthetic\nsign gloss data from spoken language corpora.\n5.3.5\nData Augmentation\nThis section discusses methods to improve gloss-to-text translation through\ndata augmentation, specifically those that take monolingual corpora of stan-\ndard spoken languages and generate pseudo-parallel “gloss” text. We first dis-\ncuss back-translation, point out its potential failings in the SL setting, and then\nCHAPTER 5. PRELIMINARY WORK\n86\npropose a novel rule-based data augmentation algorithm.\nBack-translation\nBack-translation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016a) auto-\nmatically creates pseudo-parallel sentence pairs from monolingual text to im-\nprove MT in low-resource settings. However, back-translation is only effective\nwith sufficient parallel data to train a functional MT model, which is not always\nthe case in extremely low-resource settings (Currey et al., 2017), and particu-\nlarly when the domain of the parallel training data and monolingual data to be\ntranslated are mismatched (Dou et al., 2020).\nProposed Rule-based Augmentation Strategies\nGiven the limitations of standard back-translation techniques, we next move to\nthe proposed method of using rule-based heuristics to generate SL glosses from\nspoken language text.\nGeneral rules\nThe differences in SL glosses from spoken language can be sum-\nmarized by (1) A lack of word inflection, (2) An omission of punctuation and\nindividual words, and (3) Syntactic diversity.\nWe, therefore, propose the corresponding three heuristics to generate pseudo-\nglosses from spoken language: (1) Lemmatization of spoken words; (2) POS-\ndependent and random word deletion; (3) Random word permutation.\nWe use spaCy (Honnibal and Montani, 2017) for (1) lemmatization and (2)\nPOS tagging to only keep nouns, verbs, adjectives, adverbs, and numerals. We\nalso drop the remaining tokens with probability p = 0.2, and (3) randomly\nreorder tokens with maximum distance d = 4.\nFor a given sentence S:\n1. Discard all tokens t ∈S if POS(t) ̸∈{noun, verb, adjective, adverb, nu-\nmeral}\nCHAPTER 5. PRELIMINARY WORK\n87\n2. Discard remaining tokens t ∈S with probability p = 0.2\n3. Lemmatize all tokens t ∈S\n4. Apply a random permutation σ to S verifying ∀i ∈{1, n}, |σ(i) −i| ≤4\nwhere n is the number of tokens in S at step 4 and POS is a part-of-speech\ntagger.\nLanguage-specific rules\nWhile random permutation allows some degree of\nrobustness to word order, it cannot capture all aspects of syntactic divergence\nbetween signed and spoken language. Therefore, inspired by previous work on\nrule-based syntactic transformations for reordering in MT (Collins et al., 2005;\nIsozaki et al., 2010; Zhou et al., 2019), we manually devise a shortlist of syntax\ntransformation rules based on the grammar of DGS and German.\nWe perform lemmatization and POS filtering as before. In addition, we ap-\nply compound splitting (Tuggener, 2016) on nouns and only keep the first noun,\nreorder German SVO sentences to SOV, move adverbs and location words to the\nstart of the sentence, and move negation words to the end.\nFor a given sentence S:\n1. For each subject-verb-object triplet (s, v, o) ∈S, swap the positions of v\nand o in S\n2. Discard all tokens t ∈S if POS(t) ̸∈{noun, verb, adjective, adverb, nu-\nmeral}\n3. For t ∈S, if POS(t) = adverb, then move t to the start of s\n4. For t ∈S, if NER(t) = location, then move t to the start of s\n5. For t ∈S, if DEP(t) = negation, then move t to the end of s\n6. For t ∈S, if t is a compound noun c1c2...cn, replace t by c1\n7. Lemmatize all tokens t ∈S\nCHAPTER 5. PRELIMINARY WORK\n88\nwhere POS is a part-of-speech tagger, NER is a named entity recognizer and\nDEP is a dependency parser.\n5.3.6\nExperimental Setting\nDatasets\nDGS & German\nRWTH-PHOENIX-Weather 2014T (Camg¨oz et al., 2018) is\na parallel corpus of 8,257 DGS interpreted videos from the Phoenix5 weather\nnews channel, with corresponding SL glosses and German translations.\nTo obtain monolingual German data, we crawled tagesschau6 and extracted\nnews caption files containing the word “wetter” (German for “weather”). We\nsplit the 1,506 caption files into 341,023 German sentences using the spaCy sen-\ntence splitter and generate synthetic glosses using our methods described in\n§5.3.5.\nASL & English\nThe NCSLGR dataset (SignStream, 2007) is a small, general\ndomain dataset containing 889 ASL videos with 1,875 SL glosses and English\ntranslations.\nWe use ASLG-PC12 (Othman and Jemni, 2012), a large synthetic ASL gloss\ndataset created from English text using rule-based methods with 87,710 publicly\navailable examples, for our experiments on ASL-English with language-specific\nrules. We also create another synthetic variation of this dataset using our pro-\nposed general rule-based augmentation.\nBaseline Setup\nWe first train a Baseline system on the small manually annotated SL dataset\nwe have available in each language pair. The model architecture and training\nmethod are based on Yin and Read (2020a)’s Transformer gloss-to-text trans-\n5www.phoenix.de\n6www.tagesschau.de\nCHAPTER 5. PRELIMINARY WORK\n89\nlation model. While previous work (Yin and Read Reimpl.) used word-level\ntokenization, for Baseline and all other models described below, we instead use\nBPE tokenization (Sennrich et al. (2016b); with 2,000 BPE codes) for efficiency\nand simple handling of unknown words. For all tested methods, we repeat\nevery experiment 3 times to account for variance in training.\nModel Reproduction\nFor reproduction purposes, here we lay the exact commands for training a sin-\ngle model using OpenNMT 1.2.0 (Klein et al., 2017). These commands are taken\nfrom Yin and Read (2020a).\nGiven 6 files—train.gloss / train.txt, dev.gloss / dev.txt, test.gloss / test.txt—we\nstart by preprocessing the data using the following command:\nonmt preprocess −dynamic dict −save data\nprocessed data \\\n−t r a i n s r c\nt r a i n . gloss −t r a i n t g t\nt r a i n . t x t\n\\\n−v a l i d s r c dev . gloss −v a l i d t g t dev . t x t\nThen, we train a translation system using the train command:\nonmt train −data\nprocessed data −save model model −layers 2 \\\n−rnn size\n512 −word vec size 512 −heads 8 −encoder type\ntransformer \\\n−decoder type\ntransformer −position encoding −transformer ff\n2048 \\\n−dropout\n0.1 −early stopping 3 −e a r l y s t o p p i n g c r i t e r i a\naccuracy ppl \\\n−batch size\n2048 −accum count 3 −batch type\ntokens \\\n−max generator batches 2 −normalization\ntokens \\\n−optim adam −adam beta2 0.998 −decay method noam −warmup steps 3000 \\\n−learning rate\n0.5 −max grad norm 0 −param init 0 −param init glorot \\\n−label smoothing\n0.1 −valid steps\n100 −save checkpoint steps\n100 \\\n−world size 1 −gpu ranks 0\nAt the end of the training procedure, it prints to console “Best model found\nat step X”. Locate it, and use it for translating the data:\nonmt translate −model model step X . pt −src\nt e s t . gloss \\\n−output hyp . t x t −gpu 0 −replace unk −beam size 4\nCHAPTER 5. PRELIMINARY WORK\n90\nFinally, evaluate the output using SacreBLEU:\ncat hyp . t x t\n|\nsacrebleu\nt e s t . t x t\nPre-training on Augmented Data\nFor General-pre and Specific-pre, we pre-train a tokenizer and translation model\non pseudo-parallel data obtained using general and language-specific rules re-\nspectively, until the accuracy on the synthetic validation set drops. We test both\nmodels on the parallel SL dataset in a zero-shot setting.\nFor BT-tuned, General-tuned and Specific-tuned, we take models pre-trained\non pseudo-parallel data obtained with either back-translation, general rules,\nor language-specific rules, and continue training with half of the training data\ntaken from the synthetic pseudo-parallel data and the other half taken from the\nreal SL data. Then, we fine-tune these models on the real SL data and evaluate\nthem on the test set.\n5.3.7\nResults\nWe evaluate our models across all datasets and sizes using SacreBLEU (v1.4.14)\n(Post, 2018) and COMET (wmt-large-da-estimator-1719) (Rei et al., 2020). We also\ncompare our results to previous work on PHOENIX in Table 5.5.\nFirst, we note results on General-pre and Specific-pre. Interestingly, the scores\nare non-negligible, demonstrating that the model can learn with only augmented\ndata.7 Moreover, on PHOENIX Specific-pre achieves significantly better perfor-\nmance than General-pre, which suggests our hand-crafted syntax transforma-\ntions effectively expose the model to the divergence between DGS and German\nduring pre-training.\nNext, turning to the tuned models, we see that Specific and General outper-\nform both the baseline and BT by large margins, demonstrating the effective-\n7In contrast, merely outputting the source sentence results in 1.36 BLEU, -90.28 COMET on\nPHOENIX and 1.5 BLEU, -119.45 COMET on NCSLGR.\nCHAPTER 5. PRELIMINARY WORK\n91\n102\n103\n# of fine­tuning sentences\n0\n5\n10\n15\n20\nBLEU\nBaseline\nBT­tuned\nGeneral­tuned\nSpecific­tuned\n(a) NCSLGR (ASL)\n102\n103\n# of fine­tuning sentences\n5\n10\n15\n20\n(b) PHOENIX (DGS)\nFigure 5.13: Translation results using various amounts of annotated parallel\ndata.\nness of our proposed methods. Interestingly, General-tuned performs slightly\nbetter, in contrast to the previous result. We posit that, similarly to previously\nreported results on sampling-based back translation (Edunov et al., 2018), Gen-\neral is benefiting from the diversity provided by sampling multiple reordering\ncandidates, even if each candidate is of lower quality.\nLooking at Figure 5.13, we see that the superior performance of our methods\nholds for all data sizes, but it is particularly pronounced when the parallel-\ndata-only baseline achieves moderate BLEU scores in the range of 5-20. This\nconfirms that BT is not a viable data augmentation method when parallel data\nis not plentiful enough to train a robust back-translation system.\nTable 5.6 includes the evaluation scores for all of our experiments, ran three\ntimes.\n8The original work achieves 23.32 BLEU; correspondence with the authors has led us to\nbelieve that the discrepancy is due to different versions of the underlying software.\nCHAPTER 5. PRELIMINARY WORK\n92\nPHOENIX\nNCSLGR\nBLEU↑\nCOMET↑\nBLEU↑\nCOMET↑\nYin and Read Reimpl.8\n22.17\n-2.93\n-\n-\nBaseline\n21.15\n-5.74\n15.95\n-61.00\nGeneral-pre (0-shot)\n3.95\n-69.09\n0.97\n-135.99\nSpecific-pre (0-shot)\n7.26\n-53.14\n0.95\n-134.13\nBT-tuned\n22.02\n6.84\n16.67\n-51.86\nGeneral-tuned\n23.35\n13.65\n19.09\n-34.50\nSpecific-tuned\n23.17\n11.70\n18.58\n-39.96\nTable 5.5: Results of our models on PHOENIX and NCSLGR. We bold scores\nstatistically significantly higher than baseline at the 95% confidence level.\n% of available annotated data used\n1%\n5%\n25%\n100%\nBLEU\nCOMET\nBLEU\nCOMET\nBLEU\nCOMET\nBLEU\nCOMET\nPHOENIX\nBaseline\n6.37 ± 0.89\n-89.21 ± 12.82\n10.18 ± 0.40\n-71.37 ± 2.86\n16.20 ± 0.27\n-33.88 ± 4.35\n21.15 ± 0.58\n-5.74 ± 2.35\nBT-tuned\n4.12 ± 1.55\n-91.87 ± 16.35\n9.91 ± 0.54\n-53.38 ± 4.04\n17.10 ± 0.56\n-16.46 ± 2.52\n22.02 ± 0.50\n6.84 ± 0.34\nGeneral-tuned\n9.49 ± 1.01\n-52.23 ± 6.31\n14.78 ± 0.51\n-27.13 ± 2.29\n19.86 ± 0.64\n-0.72 ± 2.44\n23.35 ± 0.22\n13.65 ± 1.68\nSpecific-tuned\n9.70 ± 0.75\n-55.94 ± 2.08\n14.65 ± 0.29\n-30.85 ± 1.45\n19.66 ± 0.08\n-5.62 ± 0.51\n23.17 ± 0.30\n11.70 ± 1.20\nNCSLGR\nBaseline\n0.47 ± 0.60\n-153.90 ± 11.89\n2.07 ± 0.32\n-145.14 ± 1.15\n8.07 ± 0.43\n-101.24 ± 5.14\n15.95 ± 1.11\n-61.00 ± 6.86\nBT-tuned\n1.07 ± 0.47\n-139.80 ± 3.78\n3.71 ± 0.55\n-117.33 ± 3.03\n9.11 ± 0.05\n-82.41 ± 2.29\n16.67 ± 0.32\n-51.86 ± 0.66\nGeneral-tuned\n1.58 ± 0.60\n-134.22 ± 1.73\n5.13 ± 0.15\n-106.59 ± 1.56\n11.04 ± 0.04\n-66.35 ± 2.00\n19.09 ± 0.20\n-34.50 ± 1.19\nSpecific-tuned\n1.30 ± 0.52\n-128.14 ± 1.58\n4.94 ± 0.45\n-107.60 ± 4.01\n10.99 ± 0.12\n-71.37 ± 1.01\n18.58 ± 0.84\n-39.96 ± 1.91\nTable 5.6: Mean and standard deviation of BLEU and COMET over different\nexperimental settings. We bold scores statistically significantly higher than\nbaseline at the 95% confidence level.\n5.3.8\nImplications and Future Work\nConsistent improvements over the baseline across two language pairs by our\nproposed rule-based augmentation strategies demonstrate that data augmenta-\ntion using monolingual spoken language data is a promising approach for sign\nlanguage translation.\nGiven the efficiency of our general rules compared to language-specific rules,\nfuture work may also include a more focused approach on specifically pre-\ntraining the target-side decoder with spoken language sentences so that by\nlearning the syntax of the target spoken language, it can generate fluent sen-\ntences from sign language glosses having little to no parallel examples during\ntraining (both few-shot and zero-shot settings).\nChapter 6\nSign Language Translation\n6.1\nSegmentation (Moryossef et al., 2023a)\nSign language segmentation is a crucial task in sign language processing sys-\ntems. It enables downstream tasks such as sign recognition, transcription, and\nmachine translation. In this work, we consider two kinds of segmentation: into\nindividual signs and into phrases, larger units comprising several signs. We pro-\npose a novel approach to jointly model these two tasks.\nOur method is motivated by linguistic cues observed in sign language cor-\npora. We replace the predominant IO tagging scheme with BIO tagging to ac-\ncount for continuous signing. Given that prosody plays a significant role in\nphrase boundaries, we explore the use of optical flow features. We also provide\nan extensive analysis of hand shapes and 3D hand normalization.\nWe find that introducing BIO tagging is necessary to model sign bound-\naries. Explicitly encoding prosody by optical flow improves segmentation in\nshallow models, but its contribution is negligible in deeper models. Careful\ntuning of the decoding algorithm atop the models further improves the seg-\nmentation quality.\nWe demonstrate that our final models generalize to out-of-domain video\ncontent in a different signed language, even under a zero-shot setting.\nWe\n93\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n94\nobserve that including optical flow and 3D hand normalization enhances the\nrobustness of the model in this context.\n6.1.1\nIntroduction\nSigned languages are natural languages used by deaf and hard-of-hearing in-\ndividuals to communicate through a combination of manual and non-manual\nelements (Sandler and Lillo-Martin, 2006). Like spoken languages, signed lan-\nguages have their own distinctive grammar, and vocabulary, that have evolved\nthrough natural processes of language development (Sandler, 2010).\nSign language transcription and translation systems rely on the accurate\ntemporal segmentation of sign language videos into meaningful units such as\nsigns (Santemiz et al., 2009; Renz et al., 2021a) or signing sequences correspond-\ning to subtitle units1 (Bull et al., 2020b). However, sign language segmentation\nremains a challenging task due to the difficulties in defining meaningful units\nin signed languages (De Sisto et al., 2021). Our approach is the first to consider\ntwo kinds of units in one model. We simultaneously segment single signs and\nphrases (larger units) in a unified framework.\nPrevious work typically approached segmentation as a binary classification\ntask (including segmentation tasks in audio signal processing and computer vi-\nsion), where each frame/pixel is predicted to be either part of a segment or not.\nHowever, this approach neglects the intricate nuances of continuous signing,\nwhere segment boundaries are not strictly binary and often blend in reality. One\nsign or phrase can immediately follow another, transitioning smoothly, without\na frame between them being distinctly outside (Figure 6.1 and §6.1.3).\nWe propose incorporating linguistically motivated cues to address these chal-\nlenges and improve sign language segmentation. To cope with continuous sign-\ning, we adopt a BIO-tagging approach (Ramshaw and Marcus, 1995), where in\naddition to predicting a frame to be in or out of a segment, we also classify the\nbeginning of the segment as shown in Figure 6.2. Since phrase segmentation is\n1Subtitles may not always correspond directly to sentences. They frequently split within a\nsentence and could be temporally offset from the corresponding signing segments.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n95\nO O B\nI\nI\nB\nI\nI\nI\nO B\nI\nI\nI\nO O\nsign 1\nsign 2\nsign 3\nphrase\nO O B\nI\nI\nI\nI\nI\nI\nI\nI\nI\nI\nI\nO O\nFigure 6.1: Per-frame classification of a sign language utterance following a\nBIO tagging scheme. Each box represents a single frame of a video. We\npropose a joint model to segment signs (top) and phrases (bottom) at the same\ntime. B=beginning, I=inside, O=outside. The figure illustrates continuous\nsigning where signs often follow each other without an O frame between them.\nprimarily marked with prosodic cues (i.e., pauses, extended sign duration, fa-\ncial expressions) (Sandler, 2010; Ormel and Crasborn, 2012), we explore using\noptical flow to explicitly model them. Since signs employ a limited number of\nhand shapes, we additionally perform 3D hand normalization (§6.1.3).\nFigure 6.2: The annotation of the first phrase in a video from the test set\n(dgskorpus goe 02), in yellow, signing: “Why do you smoke?” through the use\nof three signs: WHY (+mouthed), TO-SMOKE, and a gesture (+mouthed)\ntowards the other signer. At the top, our phrase segmentation model predicts a\nsingle phrase that initiates with a B tag (in green) above the B-threshold (green\ndashed line), followed by an I (in light blue), and continues until falling below\na certain threshold. At the bottom, our sign segmentation model accurately\nsegments the three signs.\nEvaluating on the Public DGS Corpus (Prillwitz et al., 2008; Hanke et al.,\n2020) (DGS stands for German Sign Language), we report enhancements in\nmodel performance following specific modifications.\nWe compare our final\nmodels after hyperparameter optimization, including parameters for the de-\ncoding algorithm, and find that our best architecture using only the poses is\ncomparable to the one that uses optical flow and hand normalization.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n96\nReassuringly, we find that our model generalizes when evaluated on addi-\ntional data from different signed languages in a zero-shot approach. We obtain\nsegmentation scores that are competitive with previous work and observe that\nincorporating optical flow and hand normalization makes the model more ro-\nbust for out-of-domain data. Our code and models are available at https:\n//github.com/sign-language-processing/transcription.\n6.1.2\nRelated Work\nSign Language Detection\nSegmentation consists of detecting the frame boundaries for signs or phrases\nin videos to divide them into meaningful units. While the most canonical way\nof dividing a spoken language text is into a linear sequence of words, due to\nthe simultaneity of sign language, the notion of a sign language “word” is ill-\ndefined, and sign language cannot be fully linearly modeled.\nCurrent methods resort to segmenting units loosely mapped to signed lan-\nguage units (Santemiz et al., 2009; Farag and Brock, 2019; Bull et al., 2020b; Renz\net al., 2021a,b; Bull et al., 2021) and do not explicitly leverage reliable linguis-\ntic predictors of sentence boundaries such as prosody in signed languages (i.e.,\npauses, extended sign duration, facial expressions) (Sandler, 2010; Ormel and\nCrasborn, 2012). De Sisto et al. (2021) call for a better understanding of sign lan-\nguage structure, which they believe is the necessary ground for the design and\ndevelopment of sign language recognition and segmentation methodologies.\nSantemiz et al. (2009) automatically extracted isolated signs from continuous\nsigning by aligning the sequences obtained via speech recognition, modeled by\nDynamic Time Warping (DTW) and Hidden Markov Models (HMMs).\nFarag and Brock (2019) used a random forest classifier to distinguish frames\ncontaining signs in Japanese Sign Language based on the composition of spatio-\ntemporal angular and distance features between specific pairs of joint segments.\nBull et al. (2020b) segmented French Sign Language into segments corre-\nsponding to subtitle units by relying on the alignment between subtitles and\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n97\nsign language videos, leveraging a spatio-temporal graph convolutional net-\nwork (STGCN; Yu et al. (2017)) with a BiLSTM on 2D skeleton data.\nRenz et al. (2021a) located temporal boundaries between signs in continu-\nous sign language videos by employing 3D convolutional neural network rep-\nresentations with iterative temporal segment refinement to resolve ambigui-\nties between sign boundary cues.\nRenz et al. (2021b) further proposed the\nChangepoint-Modulated Pseudo-Labelling (CMPL) algorithm to solve the prob-\nlem of source-free domain adaptation.\nBull et al. (2021) presented a Transformer-based approach to segment sign\nlanguage videos and align them with subtitles simultaneously, encoding subti-\ntles by BERT (Devlin et al., 2019) and videos by CNN video representations.\n6.1.3\nMotivating Observations\nTo motivate our proposed approach, we make a series of observations regarding\nthe intrinsic nature of sign language expressions. Specifically, we highlight the\nunique challenges posed by the continuous flow of sign language expressions,\nthe role of prosody in determining phrase boundaries, and the influence of hand\nshape changes in indicating sign boundaries.\nBoundary Modeling\nWhen examining the nature of sign language expressions, we note that the ut-\nterances are typically signed in a continuous flow, with minimal to no pauses\nbetween individual signs. This continuity is particularly evident when dealing\nwith lower frame rates. This continuous nature presents a significant differ-\nence from text where specific punctuation marks serve as indicators of phrase\nboundaries, and a semi-closed set of tokens represent the words.\nGiven these characteristics, directly applying conventional segmentation or\nsign language detection models—that is, utilizing IO tagging in a manner sim-\nilar to image or audio segmentation models—may not yield the optimal solu-\ntion, particularly at the sign level. Such models often fail to precisely identify\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n98\nthe boundaries between signs.\nA promising alternative is Beginning-Inside-Outside (BIO) tagging (Ramshaw\nand Marcus, 1995). BIO tagging was originally used for named entity recogni-\ntion, but its application extends to any sequence chunking task beyond the text\nmodality. In the context of sign language, BIO tagging provides a more refined\nmodel for discerning boundaries between signs and phrases, thus significantly\nimproving segmentation performance (Figure 6.1).\nTo test the viability of the BIO tagging approach in comparison with the\nIO tagging, we conducted an experiment on the Public DGS Corpus. The en-\ntire corpus was transformed to various frame rates and the sign segments were\nconverted to frames using either BIO or IO tagging, then decoded back into\nsign segments. Figure 6.3 illustrates the results of this comparison. Note that\nthe IO tagging was unable to reproduce the same number of segments as the\nBIO tagging on the gold data. This underscores the importance of BIO tagging\nin identifying sign and phrase boundaries.\n10\n20\n30\n40\n50\n92%\n94%\n96%\n98%\n100%\nBIO\nIO\nFigure 6.3: Reproduced sign segments in the Public DGS Corpus by BIO and\nIO tagging at various frame rates. 99.7% of segments reproduced at 25fps by\nBIO tagging.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n99\nPhrase Boundaries\nLinguistic research has shown that prosody is a reliable predictor of phrase\nboundaries in signed languages (Sandler, 2010; Ormel and Crasborn, 2012). We\nobserve that this is also the case in the Public DGS Corpus dataset used in our\nexperiments. To illustrate this, we model pauses and movement using optical\nflow directly on the poses as proposed by Moryossef et al. (2020). Figure 5.1\ndemonstrates that a change in motion signifies the presence of a pause, which,\nin turn, indicates a phrase boundary.\nSign Boundaries\nWe observe that signs generally utilize a limited number of hand shapes, with\nthe majority of signs utilizing a maximum of two hand shapes.\nFor exam-\nple, linguistically annotated datasets, such as ASL-LEX (Sehyr et al., 2021) and\nASLLVD (Neidle et al., 2012), only record one initial hand shape per hand and\none final hand shape. Mandel (1981, p. 87) argued that there can only be one set\nof selected fingers per sign, constraining the number of handshapes in signs.\nThis limitation is referred to as the Selected Fingers Constraint. And indeed, San-\ndler et al. (2008) find that the optimal form of a sign is monosyllabic, and that\nhandshape change is organized by the syllable unit.\nTo illustrate this constraint empirically, we show a histogram of hand shapes\nper sign in SignBank2 for 705, 151 signs in Figure 6.4.\nAdditionally, we found that a change in the dominant hand shape often\nsignals the presence of a sign boundary. Specifically, out of 27, 658 sentences,\ncomprising 354, 955 pairs of consecutive signs, only 17.38% of consecutive signs\nshare the same base hand shape3. Based on these observations, we propose\nusing 3D hand normalization as an indicative cue for hand shapes to assist in\ndetecting sign boundaries. We hypothesize that performing 3D hand normal-\nization makes it easier for the model to extract the hand shape.\n2https://signbank.org/signpuddle2.0/\n3It is important to note that this percentage is inflated, as it may encompass overlaps across\nthe dominant and non-dominant hands, which were not separated for this analysis.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n100\n0\n1\n2\n3\n4\n0%\n20%\n40%\n60%\nFigure 6.4: Number of hand shapes per sign in SignBank.\n6.1.4\nExperimental Setup\nIn this section, we describe the experimental setup used to evaluate our lin-\nguistically motivated approach for sign language segmentation. This includes\na description of the Public DGS Corpus dataset used in the study, the method-\nology employed to perform sign and phrase segmentation, and the evaluation\nmetrics used to measure the performance of the proposed approach.\nDataset\nThe Public DGS Corpus (Prillwitz et al., 2008; Hanke et al., 2020) is a sign lan-\nguage dataset that includes both accurate sign-level annotation from continu-\nous signing, and well-aligned phrase-level translation in spoken language.\nThe corpus comprises 404 documents / 714 videos4 with an average dura-\ntion of 7.55 minutes, featuring either one signer or two signers, at 50 fps. Most\nof these videos feature gloss transcriptions and spoken language translations\n(German and English), except for the ones in the “Joke” category, which are not\nannotated and thus excluded from our model. We also exclude documents with\nmissing annotations. id ∈{1289910, 1245887, 1289868, 1246064, 1584617}. The\ntranslations are comprised of full spoken language paragraphs, sentences, or\n4The number of videos is nearly double the number of documents because each document\ntypically includes two signers, each of whom produces one video for segmentation.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n101\nphrases (i.e., independent/main clauses).\nEach gloss span is considered a gold sign segment, following a tight annota-\ntion scheme (Hanke et al., 2012). Phrase segments are identified by examining\nevery translation, with the segment assumed to span from the start of its first\nsign to the end of its last sign, correcting imprecise annotation.\nThis corpus is enriched with full-body pose estimations from OpenPose (Cao\net al., 2019; Schulder and Hanke, 2019) and Mediapipe Holistic (Grishchenko\nand Bazarevsky, 2020). We use the 3.0.0-uzh-document split from Zhang et al.\n(2023). After filtering the data, we are left with 296 documents / 583 videos for\ntraining, 6 / 12 for validation, and 9 / 17 for testing. The mean number of signs\nand phrases in a video from the training set is 613 and 111, respectively.\nMethodology\nOur proposed approach for sign language segmentation is based on the follow-\ning steps:\n1. Pose Estimation Given a video, we first adjust it to 25 fps and estimate\nbody poses using the MediaPipe Holistic pose estimation system.\nWe\ndo not use OpenPose because it lacks a Z-axis, which prevents 3D rota-\ntion used for hand normalization. The shape of a pose is represented as\n(frames × keypoints × axes).\n2. Pose Normalization To generalize over video resolution and distance from\nthe camera, we normalize each of these poses such that the mean distance\nbetween the shoulders of each person equals 1, and the mid-point is at\n(0, 0) (Celebi et al., 2013). We also remove the legs since they are less rele-\nvant to signing.\n3. Optical Flow We follow Moryossef et al. (2020, Equation 1).\n4. 3D Hand Normalization Following §3.3, we rotate and scale each hand\nto ensure that the same hand shape is represented in a consistent manner\nacross different frames. We rotate the 21 XYZ keypoints of the hand so\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n102\nthat the back of the hand lies on the XY plane, we then rotate the hand\nso that the metacarpal bone of the middle finger lies on the Y-axis, and\nfinally, we scale the hand such that the bone is of constant length.\n5. Sequence Encoder For every frame, the pose is first flattened and pro-\njected into a standard dimension (256), then fed through an LSTM encoder\n(Hochreiter and Schmidhuber, 1997).\n6. BIO Tagging On top of the encoder, we place two BIO classification heads\nfor sign and phrase independently. B denotes the beginning of a sign or\nphrase, I denotes the middle of a sign or phrase, and O denotes being out-\nside a sign or phrase. Our cross-entropy loss is proportionally weighted\nin favor of B as it is a rare label5 compared to I and O.\n7. Greedy Segment Decoding To decode the frame-level BIO predictions\ninto sign/phrase segments, we define a segment to start with the first\nframe possessing a B probability greater than a predetermined thresh-\nold (defaulted at 0.5). The segment concludes with the first frame among\nthe subsequent frames, having either a B or O probability exceeding the\nthreshold. We provide our exact decoding algorithm in Algorithm 3. We\nopt to employ adjustable thresholds rather than argmax prediction, as our\nempirical findings demonstrate superior performance (§6.1.5).\n5B:I:O is about 1:5:18 for signs and 1:58:77 for phrases.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n103\nAlgorithm 3 Probabilities to Segments Conversion.\nRequire: probs, a list of probabilities from 0 to 100\nthresholdb ←50.0\nthresholdo ←50.0\nstart ←None\ndid pass start ←False\nfor i = 0 to len(probs) do\nb, i, o ←probs[i]\nif start = None then\nif b > thresholdb then\nstart ←i\nend if\nelse\nif did pass start then\nif b > thresholdb or o > thresholdo then\nyield (start, i −1))\nstart ←None\ndid pass start ←False\nend if\nelse\nif b < thresholdb then\ndid pass start ←True\nend if\nend if\nend if\nend for\nif start ̸= None then\nyield (start, len(probs)))\nend if\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n104\nExperiments\nOur approach is evaluated through a series of six sets of experiments. Each set\nis repeated three times with varying random seeds. Preliminary experiments\nwere conducted to inform the selection of hyperparameters and features, the\ndetails of which can be found in Table 6.2 in §6.1.5. Model selection is based on\nvalidation metrics.\n1. E0: IO Tagger We re-implemented and reproduced6 the sign language\ndetection model proposed by Moryossef et al. (2020), in PyTorch (Paszke\net al., 2019a) as a naive baseline. This model processes optical flow as\ninput and outputs I (is signing) and O (not signing) tags.\n2. E1: Bidirectional BIO Tagger We replace the IO tagging heads in E0 with\nBIO heads to form our baseline. Our preliminary experiments indicate\nthat inputting only the 75 hand and body keypoints and making the LSTM\nlayer bidirectional yields optimal results.\n3. E2: Adding Reduced Face Keypoints Although the 75 hand and body\nkeypoints serve as an efficient minimal set for sign language detection/seg-\nmentation models, we investigate the impact of other nonmanual sign lan-\nguage articulators, namely, the face. We introduce a reduced set of 128 face\nkeypoints that signify the signer’s face contour7.\n4. E3: Adding Optical Flow At every time step t we append the optical flow\nbetween t and t −1 to the current pose frame as an additional dimension\nafter the XYZ axes.\n5. E4: Adding 3D Hand Normalization At every time step, we normalize\nthe hand poses and concatenate them to the current pose.\n6. E5: Autoregressive Encoder We replace the encoder with the one pro-\nposed by Jiang et al. (2023b) for the detection and classification of great\n6The initial implementation uses OpenPose (Cao et al., 2019), at 50 fps. Preliminary experi-\nments reveal that these differences do not significantly influence the results.\n7We reduce the dense FACE LANDMARKS in Mediapipe Holistic to the contour keypoints\naccording to the variable mediapipe.solutions.holistic.FACEMESH CONTOURS.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n105\nape calls from raw audio signals. Specifically, we add autoregressive con-\nnections between time steps to encourage consistent output labels. The\nlogits at time step t are concatenated to the input of the next time step,\nt + 1. This modification is implemented bidirectionally by stacking two\nautoregressive encoders and adding their output up before the Softmax\noperation. This approach is slow, as we have to wait for the previous time\nstep predictions before we can feed them to the next time step.\nEvaluation Metrics\nWe evaluate the performance of our proposed approach for sign and phrase\nsegmentation using the following metrics:\n• Frame-level F1 Score For each frame, we apply the argmax operation to\nmake a local prediction of the BIO class and calculate the macro-averaged\nper-class F1 score against the ground truth label. We use this frame-level\nmetric during validation as the primary metric for model selection and\nearly stopping, due to its independence from a potentially variable seg-\nment decoding algorithm.\n• Intersection over Union (IoU) We compute the IoU between the ground\ntruth segments and the predicted segments to measure the degree of over-\nlap. Note that we do not perform a one-to-one mapping between the two\nusing techniques like DTW. Instead, we calculate the total IoU based on\nall segments in a video.\n• Percentage of Segments (%) To complement IoU, we introduce the per-\ncentage of segments to compare the number of segments predicted by\nthe model with the ground truth annotations. It is computed as follows:\n#predicted segments\n#ground truth segments. The optimal value is 1.\n• Efficiency We measure the efficiency of each model by the number of pa-\nrameters and the training time of the model on a Tesla V100-SXM2-32GB\nGPU for 100 epochs8.\n8Exceptionally the autoregressive models in E5 were trained on an NVIDIA A100-SXM4-\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n106\n6.1.5\nResults and Discussion\nWe report the mean test evaluation metrics for our experiments in Table 6.1.\nSign\nPhrase\nEfficiency\nExperiment\nF1\nIoU\n%\nF1\nIoU\n%\n#Params\nTime\nE0\nMoryossef et al. (2020)\n—\n0.46\n1.09\n—\n0.70\n1.00\n102K\n0:50:17\nE1\nBaseline\n0.56\n0.66\n0.91\n0.59\n0.80\n2.50\n454K\n1:01:50\nE2\nE1 + Face\n0.53\n0.58\n0.64\n0.57\n0.76\n1.87\n552K\n1:50:31\nE3\nE1 + Optical Flow\n0.58\n0.62\n1.12\n0.60\n0.82\n3.19\n473K\n1:20:17\nE4\nE3 + Hand Norm\n0.56\n0.61\n1.07\n0.60\n0.80\n3.24\n516K\n1:30:59\nE1s\nE1 + Depth=4\n0.63\n0.69\n1.11\n0.65\n0.82\n1.63\n1.6M\n4:08:48\nE2s\nE2 + Depth=4\n0.62\n0.69\n1.07\n0.63\n0.84\n2.68\n1.7M\n3:14:03\nE3s\nE3 + Depth=4\n0.60\n0.63\n1.13\n0.64\n0.80\n1.53\n1.7M\n4:08:30\nE4s\nE4 + Depth=4\n0.59\n0.63\n1.13\n0.62\n0.79\n1.43\n1.7M\n4:35:29\nE1s*\nE1s + Tuned Decoding\n—\n0.69\n1.03\n—\n0.85\n1.02\n—\n—\nE4s*\nE4s + Tuned Decoding\n—\n0.63\n1.06\n—\n0.79\n1.12\n—\n—\nE5\nE4s + Autoregressive\n0.45\n0.47\n0.88\n0.52\n0.63\n2.72\n1.3M\n˜3 days\nTable 6.1: Mean test evaluation metrics for our experiments. The best score of\neach column is in bold and a star (*) denotes further optimization of the\ndecoding algorithm without changing the model (only affects IoU and %).\nTable 6.3 contains a complete report including validation metrics and standard\ndeviation of all experiments.\nWe do not report F1 Score for E0 since it has a different number of classes and\nis thus incomparable. Comparing E1 to E0, we note that the model’s bidirection-\nality, the use of poses, and BIO tagging indeed help outperform the model from\nprevious work where only optical flow and IO tagging are used. While E1 pre-\ndicts an excessive number of phrase segments, the IoUs for signs and phrases\nare both higher.\nAdding face keypoints (E2) makes the model worse, while including optical\nflow (E3) improves the F1 scores. For phrase segmentation, including optical\nflow increases IoU, but over-segments phrases by more than 300%, which fur-\nther exaggerates the issue in E1. Including hand normalization (E4) on top of\nE3 slightly deteriorates the quality of both sign and phrase segmentation.\n80GB GPUA100 which doubles the training speed of V100, still the training is slow.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n107\nFrom the non-exhaustive hyperparameter search in the preliminary exper-\niments (Table 6.2), we examined different hidden state sizes (64, 128, 256, 512,\n1024) and a range of 1 to 8 LSTM layers, and conclude that a hidden size of\n256 and 4 layers with 1e −3 learning rate are optimal for E1, which lead to E1s.\nWe repeat the setup of E2, E3, and E4 with these refined hyper-parameters, and\nshow that all of them outperform their counterparts, notably in that they ease\nthe phrase over-segmentation problem.\nIn E2s, we reaffirmed that adding face keypoints does not yield beneficial\nresults, so we exclude face in future experiments. Although the face is an essen-\ntial component to understanding sign language expressions and does play some\nrole in sign and phrase level segmentation, we believe that the 128 face contour\npoints are too dense for the model to learn useful information compared to the\n75 body points, and may instead confuse the model.\nIn addition, the benefits of explicitly including optical flow (E3s) fade away\nwith the increased model depth and we speculate that now the model might\nbe able to learn the optical flow features by itself. Surprisingly, while adding\nhand normalization (E4s) still slightly worsens the overall results, it has the best\nphrase percentage.\nFrom E4s we proceeded with the training of E5, an autoregressive model.\nUnexpectedly, counter to our intuition and previous work, E5 underachieves\nour baseline across all evaluation metrics9.\nExtended Experimental Results\nWe conducted some preliminary experiments (starting with P0) on training a\nsign language segmentation model to gain insights into hyperparameters and\nfeature choices. The results are shown in Table 6.210. We found in P1.3.2 the\n9E5 should have more parameters than E4s, but because of an implementation bug, each\nLSTM layer has half the parameters. Based on the current results, we assume that autoregressive\nconnections (even with more parameters) will not improve our models.\n10Note that due to an implementation issue on edge cases (which we fixed later), the IoU\nand % values in Table 6.2 are lower than the ones in Table 6.1 and Table 6.3 thus not compa-\nrable across tables. The comparison inside of Table 6.2 between different experiments remains\nmeaningful. In addition, the results in Table 6.2 are based on only one run instead of three.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n108\noptimal hyperparameters and repeated them with different feature choices.\nSign\nPhrase\nExperiment\nF1\nIoU\n%\nF1\nIoU\n%\nP0\nMoryossef et al. (2020)\ntest\n—\n0.4\n1.45\n—\n0.65\n0.82\ndev\n—\n0.35\n1.36\n—\n0.6\n0.77\nP0.1\nP0 + Holistic 25fps\ntest\n—\n0.39\n0.86\n—\n0.64\n0.5\ndev\n—\n0.32\n0.81\n—\n0.58\n0.52\nP1\nP1 baseline\ntest\n0.55\n0.49\n0.83\n0.6\n0.67\n2.63\ndev\n0.56\n0.43\n0.75\n0.58\n0.62\n2.61\nP1.1\nP1 - encoder bidirectional\ntest\n0.48\n0.45\n0.68\n0.5\n0.64\n2.68\ndev\n0.46\n0.41\n0.64\n0.51\n0.61\n2.56\nP1.2.1\nP1 + hidden dim=512\ntest\n0.47\n0.42\n0.44\n0.52\n0.63\n1.7\ndev\n0.46\n0.4\n0.43\n0.52\n0.61\n1.69\nP1.2.2\nP1 + hidden dim=1024\ntest\n0.48\n0.45\n0.42\n0.58\n0.65\n1.53\ndev\n0.46\n0.41\n0.36\n0.53\n0.61\n1.49\nP1.3.1\nP1 + encoder depth=2\ntest\n0.55\n0.48\n0.76\n0.58\n0.67\n2.56\ndev\n0.56\n0.43\n0.69\n0.58\n0.62\n2.52\nP1.3.2\nP1 + encoder depth=4\ntest\n0.63\n0.51\n0.91\n0.66\n0.67\n1.41\ndev\n0.61\n0.47\n0.84\n0.64\n0.6\n1.39\nP1.4.1\nP1 + hidden dim=128 + encoder depth=2\ntest\n0.58\n0.48\n0.8\n0.6\n0.67\n2.0\ndev\n0.55\n0.43\n0.75\n0.54\n0.62\n2.03\nP1.4.2\nP1 + hidden dim=128 + encoder depth=4\ntest\n0.62\n0.51\n0.91\n0.64\n0.68\n2.43\ndev\n0.6\n0.47\n0.83\n0.6\n0.62\n2.57\nP1.4.3\nP1 + hidden dim=128 + encoder depth=8\ntest\n0.59\n0.52\n0.91\n0.63\n0.68\n3.04\ndev\n0.6\n0.47\n0.84\n0.6\n0.62\n3.02\nP1.5.1\nP1 + hidden dim=64 + encoder depth=4\ntest\n0.57\n0.5\n0.8\n0.6\n0.68\n2.41\ndev\n0.58\n0.45\n0.75\n0.59\n0.62\n2.39\nP1.5.2\nP1 + hidden dim=64 + encoder depth=8\ntest\n0.62\n0.51\n0.85\n0.64\n0.68\n2.53\ndev\n0.6\n0.46\n0.79\n0.6\n0.62\n2.53\nP2\nP1 + optical flow\ntest\n0.58\n0.5\n0.95\n0.63\n0.68\n3.17\ndev\n0.59\n0.45\n0.84\n0.59\n0.61\n3.08\nP2.1\nP1.3.2 + optical flow\ntest\n0.63\n0.51\n0.92\n0.66\n0.67\n1.51\ndev\n0.62\n0.46\n0.81\n0.62\n0.6\n1.53\nP3\nP1 + hand normalization\ntest\n0.55\n0.48\n0.77\n0.58\n0.67\n2.79\ndev\n0.55\n0.42\n0.71\n0.57\n0.62\n2.73\nP3.1\nP1.3.2 + hand normalization\ntest\n0.63\n0.51\n0.91\n0.66\n0.66\n1.43\ndev\n0.61\n0.46\n0.82\n0.64\n0.61\n1.46\nP4\nP2.1 + P3.1\ntest\n0.56\n0.51\n0.92\n0.61\n0.66\n1.45\ndev\n0.61\n0.46\n0.81\n0.63\n0.6\n1.41\nP4.1\nP4 + encoder depth=8\ntest\n0.6\n0.51\n0.95\n0.62\n0.67\n1.08\ndev\n0.61\n0.47\n0.86\n0.62\n0.6\n1.12\nP5\nP1.3.2 + reduced face\ntest\n0.63\n0.51\n0.94\n0.64\n0.66\n1.16\ndev\n0.61\n0.47\n0.86\n0.64\n0.58\n1.14\nP5.1\nP1.3.2 + full face\ntest\n0.54\n0.49\n0.8\n0.6\n0.68\n2.29\ndev\n0.57\n0.45\n0.7\n0.59\n0.62\n2.29\nTable 6.2: Results of the preliminary experiments.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n109\nWe selected some promising models from our preliminary experiments and\nreran them three times using different random seeds to make the final conclu-\nsion reliable and robust. Table 6.3 includes the standard deviation and the vali-\ndation results for readers to scrutinize.\nSign\nPhrase\nEfficiency\nExperiment\nF1\nIoU\n%\nF1\nIoU\n%\n#Params\nTime\nE0\nMoryossef et al. (2020)\ntest\n—\n0.46 ± 0.03\n1.09 ± 0.41\n—\n0.70 ± 0.01\n1.00 ± 0.06\n102K\n0:50:17\ndev\n—\n0.42 ± 0.05\n1.21 ± 0.59\n—\n0.61 ± 0.06\n2.47 ± 0.85\n102K\n0:50:17\nE1\nBaseline\ntest\n0.56 ± 0.03\n0.66 ± 0.01\n0.91 ± 0.05\n0.59 ± 0.02\n0.80 ± 0.03\n2.50 ± 0.13\n454K\n1:01:50\ndev\n0.55 ± 0.01\n0.59 ± 0.00\n1.12 ± 0.11\n0.56 ± 0.02\n0.75 ± 0.05\n2.94 ± 0.08\n454K\n1:01:50\nE2\nE1 + Face\ntest\n0.53 ± 0.05\n0.58 ± 0.07\n0.64 ± 0.30\n0.57 ± 0.02\n0.76 ± 0.03\n1.87 ± 0.83\n552K\n1:50:31\ndev\n0.50 ± 0.07\n0.53 ± 0.11\n0.90 ± 0.19\n0.53 ± 0.05\n0.71 ± 0.07\n2.43 ± 1.02\n552K\n1:50:31\nE3\nE1 + Optical Flow\ntest\n0.58 ± 0.01\n0.62 ± 0.00\n1.12 ± 0.05\n0.60 ± 0.03\n0.82 ± 0.03\n3.19 ± 0.11\n473K\n1:20:17\ndev\n0.58 ± 0.00\n0.62 ± 0.00\n1.50 ± 0.19\n0.59 ± 0.01\n0.79 ± 0.00\n3.94 ± 0.14\n473K\n1:20:17\nE4\nE3 + Hand Norm\ntest\n0.56 ± 0.02\n0.61 ± 0.00\n1.07 ± 0.05\n0.60 ± 0.00\n0.80 ± 0.00\n3.24 ± 0.17\n516K\n1:30:59\ndev\n0.57 ± 0.01\n0.61 ± 0.01\n1.50 ± 0.07\n0.58 ± 0.00\n0.79 ± 0.00\n4.04 ± 0.31\n516K\n1:30:59\nE1s\nE1 + Depth=4\ntest\n0.63 ± 0.01\n0.69 ± 0.00\n1.11 ± 0.01\n0.65 ± 0.02\n0.82 ± 0.04\n1.63 ± 0.10\n1.6M\n4:08:48\ndev\n0.61 ± 0.00\n0.63 ± 0.00\n1.27 ± 0.01\n0.63 ± 0.01\n0.77 ± 0.01\n2.17 ± 0.18\n1.6M\n4:08:48\nE2s\nE2 + Depth=4\ntest\n0.62 ± 0.02\n0.69 ± 0.00\n1.07 ± 0.03\n0.63 ± 0.01\n0.84 ± 0.03\n2.68 ± 0.53\n1.7M\n3:14:03\ndev\n0.60 ± 0.01\n0.63 ± 0.01\n1.20 ± 0.12\n0.59 ± 0.02\n0.76 ± 0.05\n3.30 ± 0.62\n1.7M\n3:14:03\nE3s\nE3 + Depth=4\ntest\n0.60 ± 0.01\n0.63 ± 0.00\n1.13 ± 0.01\n0.64 ± 0.03\n0.80 ± 0.03\n1.53 ± 0.18\n1.7M\n4:08:30\ndev\n0.62 ± 0.00\n0.63 ± 0.00\n1.63 ± 0.05\n0.63 ± 0.00\n0.76 ± 0.00\n2.14 ± 0.09\n1.7M\n4:08:30\nE4s\nE4 + Depth=4\ntest\n0.59 ± 0.00\n0.63 ± 0.00\n1.13 ± 0.03\n0.62 ± 0.00\n0.79 ± 0.00\n1.43 ± 0.10\n1.7M\n4:35:29\ndev\n0.61 ± 0.00\n0.63 ± 0.00\n1.56 ± 0.04\n0.63 ± 0.00\n0.77 ± 0.01\n1.89 ± 0.07\n1.7M\n4:35:29\nE4ba\nE4s + Autoregressive\ntest\n0.45 ± 0.03\n0.47 ± 0.05\n0.88 ± 0.08\n0.52 ± 0.02\n0.63 ± 0.10\n2.72 ± 1.33\n1.3M\n2 days, 21:28:42\ndev\n0.40 ± 0.01\n0.40 ± 0.01\n2.02 ± 0.73\n0.47 ± 0.00\n0.57 ± 0.04\n4.26 ± 1.26\n1.3M\n2 days, 21:28:42\nTable 6.3: Mean evaluation metrics for our main experiments. A complete\nversion of Table 6.1.\nChallenges with 3D Hand Normalization\nWhile the use of 3D hand normalization is well-justified in §6.1.3, we believe it\ndoes not help the model due to poor depth estimation quality, as further corrob-\norated by recent research from De Coster et al. (2023). Therefore, we consider\nit a negative result, showing the deficiencies in the 3D pose estimation system.\nThe evaluation metrics we propose in §3.3 could help identify better pose esti-\nmation models for this use case.\nTuning the Segment Decoding Algorithm\nWe selected E1s and E4s to further explore the segment decoding algorithm. As\ndetailed in §6.1.4, the decoding algorithm has two tunable parameters, thresholdb\nand thresholdo. We conducted a grid search with these parameters, using values\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n110\nfrom 10 to 90 in increments of 10. We additionally experimented with a varia-\ntion of the algorithm that conditions on the most likely class by argmax instead\nof fixed threshold values, which turned out similar to the default version.\nWe only measured the results using IoU and the percentage of segments at\nvalidation time since the F1 scores remain consistent in this case. For sign seg-\nmentation, we found using thresholdb = 60 and thresholdo = 40/50/60 yields\nslightly better results than the default setting (50 for both). For phrase segmen-\ntation, we identified that higher threshold values (thresholdb = 90, thresholdo =\n90 for E1s and thresholdb = 80, thresholdo = 80/90 for E4s) improve on the de-\nfault significantly, especially on the percentage metric. We report the test results\nunder E1s* and E4s*, respectively.\nDespite formulating a single model, we underline a separate sign/phrase\nmodel selection process to archive the best segmentation results. Figure 6.5 il-\nlustrates how higher threshold values reduce the number of predicted segments\nand skew the distribution of predicted phrase segments towards longer ones in\nE1s/E1s*. As Bull et al. (2020b) suggest, advanced priors could also be inte-\ngrated into the decoding algorithm.\n0\n5\n10\n15\nLength in seconds\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nProbability density\nE1s segments\nE1s* segments\nGold segments\nFigure 6.5: Probability density of phrase segment lengths.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n111\nComparison to Previous Work\nWe re-implemented and re-purposed the sign language detection model intro-\nduced in Moryossef et al. (2020) for our segmentation task as a baseline since\ntheir work is the state-of-the-art and the only comparable model designed for\nthe Public DGS Corpus dataset. As a result, we show the need of replacing IO\ntagging with BIO tagging to tackle the subtle differences between the two tasks.\nFor phrase segmentation, we compare to Bull et al. (2020b). We note that\nour definition of sign language phrases (spanning from the start of its first sign\nto the end of its last sign) is tighter than the subtitle units used in their paper\nand that we use different training datasets of different languages and domains.\nNevertheless, we implemented some of their frame-level metrics and show the\nresults in Table 6.4 on both the Public DGS Corpus and the MEDIAPI-SKEL\ndataset (Bull et al., 2020a) in French Sign Language (LSF). We report both zero-\nshot out-of-domain results11 and the results of our models trained specifically\non their dataset without the spatio-temporal graph convolutional network (ST-\nGCN; Yan et al. (2018)) used in their work for pose encoding.\nFor sign segmentation, we do not compare to Renz et al. (2021a,b) due to dif-\nferent datasets and the difficulty in reproducing their segment-level evaluation\nmetrics. The latter depends on the decoding algorithm and a way to match the\ngold and predicted segments, both of which are variable.\n6.1.6\nConclusions\nThis work focuses on the automatic segmentation of signed languages. We\nare the first to formulate the segmentation of individual signs and larger sign\nphrases as a joint problem.\nWe propose a series of improvements over previous work, linguistically mo-\ntivated by careful analyses of sign language corpora. Recognizing that sign lan-\nguage utterances are typically continuous with minimal pauses, we opted for\n11The zero-shot results are not directly comparable to theirs due to different datasets and\nlabeling approaches.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n112\nData\nModel\nROC-AUC\nF1-M\nLSF\nfull (theirs)\n0.87\n—\nbody (theirs)\n0.87\n—\nE1s (ours, zero-shot)\n0.71\n0.41\nE4s (ours, zero-shot)\n0.76\n0.44\nE1s (ours, trained)\n0.87\n0.49\nE4s (ours, trained)\n0.87\n0.51\nDGS\nE1s (ours)\n0.91\n0.65\nE4s (ours)\n0.90\n0.62\nTable 6.4: Evaluation metrics used in Bull et al. (2020b). ROC-AUC is applied\nexclusively on the O-tag. For comparison F1-M denotes the macro-averaged\nper-class F1 used in this work across all tags. The first two rows are the best\nresults taken from Table 1 in their paper. The next four rows represent how our\nmodels perform on their data in a zero-shot setting, and in a supervised\nsetting, and the last two rows represent how our models perform on our data.\na BIO tagging scheme over IO tagging. Furthermore, leveraging the fact that\nphrase boundaries are marked by prosodic cues, we introduce optical flow fea-\ntures as a proxy for prosodic processes. Finally, since signs typically employ a\nlimited number of hand shapes, to make it easier for the model to understand\nhandshapes, we attempt 3D hand normalization.\nOur experiments conducted on the Public DGS Corpus confirmed the effi-\ncacy of these modifications for segmentation quality. By comparing to previous\nwork in a zero-shot setting, we demonstrate that our models generalize across\nsigned languages and domains and that including linguistically motivated cues\nleads to a more robust model in this context.\nFinally, we envision that the proposed model has applications in real-world\ndata collection for signed languages. Furthermore, a similar segmentation ap-\nproach could be leveraged in various other fields such as co-speech gesture\nrecognition (Moryossef, 2023a) and action segmentation (Tang et al., 2019).\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n113\nLimitations\nPose Estimation\nIn this work, we employ the MediaPipe Holistic pose estimation system (Gr-\nishchenko and Bazarevsky, 2020). There is a possibility that this system exhibits\nbias towards certain protected classes (such as gender or race), underperform-\ning in instances with specific skin tones or lower video quality. Thus, we cannot\nattest to how our system would perform under real-world conditions, given\nthat the videos utilized in our research are generated in a controlled studio en-\nvironment, primarily featuring white participants.\nEncoding of Long Sequences\nIn this study, we encode sequences of frames that are significantly longer than\nthe typical 512 frames often seen in models employing Transformers (Vaswani\net al., 2017).\nNumerous techniques, ranging from basic temporal pooling/-\ndownsampling to more advanced methods such as a video/pose encoder that\naggregates local frames into higher-level ‘tokens’ (Renz et al., 2021a), graph\nconvolutional networks (Bull et al., 2020b), and self-supervised representations\n(Baevski et al., 2020), can alleviate length constraints, facilitate the use of Trans-\nformers, and potentially improve the outcomes. Moreover, a hierarchical method\nlike the Swin Transformer (Liu et al., 2021) could be applicable.\nLimitations of Autoregressive LSTMs\nIn this paper, we replicated the autoregressive LSTM implementation originally\nproposed by Jiang et al. (2023b). Our experiments revealed that this implemen-\ntation exhibits significant slowness, which prevented us from performing fur-\nther experimentation. In contrast, other LSTM implementations employed in\nthis project have undergone extensive optimization (Appleyard, 2016), includ-\ning techniques like combining general matrix multiplication operations (GEMMs),\nparallelizing independent operations, fusing kernels, rearranging matrices, and\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n114\nimplementing various optimizations for models with multiple layers (which are\nnot necessarily applicable here).\nA comparison of CPU-based performance demonstrates that our implemen-\ntation is x6.4 times slower. Theoretically, the number of operations performed\nby the autoregressive LSTM is equivalent to that of a regular LSTM. However,\nwhile the normal LSTM benefits from concurrency based on the number of lay-\ners, we do not have that luxury. The optimization of recurrent neural networks\n(RNNs) (Que et al., 2020, 2021, 2022) remains an ongoing area of research. If\nproven effective in other domains, we strongly advocate for efforts to optimize\nthe performance of this type of network.\nInterference Between Sign and Phrase Models\nIn our model, we share the encoder for both the sign and phrase segmenta-\ntion models, with a shallow linear layer for the BIO tag prediction associated\nwith each task. It remains uncertain whether these two tasks interfere with or\nenhance each other. An ablation study (not presented in this work) involving\nseparate modeling is necessary to obtain greater insight into this matter.\nNoisy Training Objective\nAlthough the annotations utilized in this study are of expert level, the deter-\nmination of precise sign (Hanke et al., 2012) and phrase boundaries remains\na challenging task, even for experts. Training the model on these annotated\nboundaries might introduce excessive noise. A similar issue was observed in\nclassification-based pose estimation (Cao et al., 2019). The task of annotating\nthe exact anatomical centers of joints proves to be nearly impossible, leading to\na high degree of noise when predicting joint position as a 1-hot classification\ntask. The solution proposed in this previous work was to distribute a Gaussian\naround the annotated location of each joint. This approach allows the joint’s\ncenter to overlap with some probability mass, thereby reducing the noise for\nthe model. A similar solution could be applied in our context. Instead of pre-\ndicting a strict 0 or 1 class probability, we could distribute a Gaussian around\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n115\nthe boundary.\nNaive Segment Decoding\nWe recognize that the frame-level greedy decoding strategy implemented in our\nstudy may not be optimal. Previous research in audio segmentation (Venkatesh\net al., 2022) employed a You Only Look Once (YOLO; Redmon et al. (2015)) de-\ncoding scheme to predict segment boundaries and classes. We propose using\na similar prediction atop a given representation, such as the LSTM output or\nclassification logits of an already trained network. Differing from traditional\nobject detection tasks, this process is simplified due to the absence of a Y axis\nand non-overlapping segments. In this scenario, the network predicts the seg-\nment boundaries using regression, thereby avoiding the class imbalance issue\nof the BIO tagging. We anticipate this to yield more accurate sign language\nsegmentation.\nLack of Transcription\nSpeech segmentation is a close task to our sign language segmentation task on\nvideos. In addition to relying on prosodic cues from audio, the former could\nbenefit from automatic speech transcription systems, either in terms of surro-\ngating the task to text-level segmentation and punctuation (Cho et al., 2015), or\ngaining additional training data from automatic speech recognition / spoken\nlanguage translation (Tsiamas et al., 2022).\nHowever, for signed languages, there is neither a standardized and widely\nused written form nor a reliable transcription procedure into some potential\nwriting systems like SignWriting (Sutton, 1990), HamNoSys (Prillwitz and Zienert,\n1990), and glosses (Johnston, 2008). Transcription/recognition and segmenta-\ntion tasks need to be solved simultaneously, so we envision that a multi-task\nsetting helps. Sign spotting, the localization of a specific sign in continuous\nsigning, is a simplification of the segmentation and recognition problem in a\nclosed-vocabulary setting (Wong et al., 2022; Varol et al., 2022). It can be used to\nfind candidate boundaries for some signs, but not all.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n116\n6.2\nTranscription\nSign Language Transcription involves converting visual sign language, either\nsegmented into single sign units or presented as continuous signing, into a cor-\nresponding phonetic form. It serves as an essential intermediary step, bridging\nthe gap between skeletal poses and their translation into spoken languages.\n6.2.1\nPreface\nIdeally, this thesis would include a model for automatic transcription, thus com-\npleting the envisioned translation pipeline. However, due to the already com-\nprehensive nature of this research and the practical constraints, this compo-\nnent remains a topic for future exploration. This section aims to provide some\nbackground and conceptual understanding of the transcription task, setting the\nstage for future research endeavors in this domain.\n6.2.2\nIntroduction\nUnlike spoken languages, signed languages have traditionally lacked a stan-\ndardized written form, presenting significant challenges in transcription and\ndocumentation. This has far-reaching implications for accessibility, education,\nresearch, and broader communication within and beyond the deaf commu-\nnity. While SignWriting (Sutton, 1990) provides a comprehensive transcription\nmethod by capturing the complex movements, facial expressions, and body po-\nsitions that characterize signed languages, its manual transcription process can\nbe time-consuming and requires expertise.\nTo address these issues, we urge an automatic transcription system for signed\nlanguages using the SignWriting notation. The development of sign-level tran-\nscription will offer numerous potential applications and opportunities for fu-\nture research. Once established, this transcription method can be combined\nwith sign language segmentation models to transcribe full videos, sign by sign.\nMoreover, a sign language transcription model based on SignWriting could po-\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n117\ntentially be employed in the context of our pipeline to translate these transcrip-\ntions into spoken language text (Jiang et al., 2023a; Moryossef and Jiang, 2023).\nThis opens up exciting possibilities for further integration of signed languages\ninto the digital realm, particularly in areas such as automatic subtitling and\nreal-time translation. Furthermore, this approach can enrich existing parallel\nresources with aligned signed and spoken language data, potentially enhanc-\ning the performance of machine translation systems.\n6.2.3\nBackground\nWriting Signed Languages\nWritten notation systems represent signs as discrete visual features. Some sys-\ntems are written linearly, and others use graphemes in two dimensions. While\nvarious universal (Sutton, 1990; Prillwitz and Zienert, 1990) and language-specific\nnotation systems (Stokoe Jr, 1960; Kakumasu, 1968; Bergman, 1977) have been\nproposed, no writing system has been adopted widely by any sign language\ncommunity, and the lack of standards hinders the exchange and unification of\nresources and applications between projects.\nSemantic Transcription\nThis is the first automatic transcription model of signed languages into a pho-\nnetic transcription system. To put our work in context, we include related work\non semantic transcription.\nPose-to-Gloss, also known as sign language recognition, is the task of rec-\nognizing a sequence of signs from a sequence of poses. Though some previous\nworks have referred to this as “sign language translation,” recognition merely\ndetermines the associated label of each sign, without handling the syntax and\nmorphology of the signed language (Padden, 1988) to create a spoken language\noutput. Instead, SLR has often been used as an intermediate step during trans-\nlation to produce glosses from signed language videos.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n118\nJiang et al. (2021) proposed a novel Skeleton Aware Multi-modal Framework\nwith a Global Ensemble Model (GEM) for isolated SLR (SAM-SLR-v2) to learn\nand fuse multimodal feature representations. Specifically, they use a Sign Lan-\nguage Graph Convolution Network (SL-GCN) to model the embedded dynam-\nics of skeleton keypoints and a Separable Spatial-Temporal Convolution Net-\nwork (SSTCN) to exploit skeleton features. The proposed late-fusion GEM fuses\nthe skeleton-based predictions with other RGB and depth-based modalities to\nprovide global information and make an accurate SLR prediction.\nDafnis et al. (2022) work on the same modified WLASL dataset as Jiang et al.\n(2021), but do not require multimodal data input. Instead, they propose a bidi-\nrectional skeleton-based graph convolutional network framework with linguis-\ntically motivated parameters and attention to the start and end frames of signs.\nThey cooperatively use forward and backward data streams, including various\nsub-streams, as input. They also use pre-training to leverage transfer learning.\nSelvaraj et al. (2022) introduced an open-source OpenHands library, which\nconsists of standardized pose datasets for different existing sign language datasets\nand trained checkpoints of four pose-based isolated sign language recognition\nmodels across six languages (American, Argentinian, Chinese, Greek, Indian,\nand Turkish). To address the lack of labeled data, they propose self-supervised\npretraining on unlabeled data and curate the largest pose-based pretraining\ndataset on Indian Sign Language (Indian-SL). They established that pretrain-\ning is effective for sign language recognition by demonstrating improved fine-\ntuning performance especially in low-resource settings and high crosslingual\ntransfer from Indian-SL to a few other sign languages.\nThe work of Kezar et al. (2023), based on the OpenHands library, explicitly\nrecognizes the role of phonology to achieve more accurate isolated sign lan-\nguage recognition (ISLR). To allow additional predictions on phonological char-\nacteristics (such as handshape), they combine the phonological annotations in\nASL-LEX 2.0 (Sehyr et al., 2021) with signs in the WLASL 2000 ISLR benchmark\n(Li et al., 2020). Interestingly, Tavella et al. (2022) construct a similar dataset\naiming just for phonological property recognition in American Sign Language.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n119\n6.2.4\nDatasets\nFor this study, there are two notable lexicons, containing isolated sign language\nvideos with SignWriting transcriptions, which can be used to train automatic\ntranscription systems.\nSign2MINT (Barth et al., 2021)\nis a lexicon of German Signed Language (DGS)\nfocusing on natural science subjects. It features 5, 263 videos with SignWriting\ntranscriptions.\nSignSuisse (Schweizerischer Geh¨orlosenbund SGB-FSS, 2023)\nis a Swiss Signed\nLanguages Lexicon that covers Swiss-German Sign Language (DSGS), French\nSign Language (LSF), and Italian Sign Language (LIS). The lexicon includes ap-\nproximately 4, 500 LSF videos with SignWriting transcriptions in SignBank12.\n6.2.5\nOutlook\nAutomatic sign language transcription would not only contribute to more ro-\nbust machine translation but also pave the way for more seamless integration\nof sign languages into various digital platforms and services, and allow for the\nanonymized distribution of sign language data.\nChapter 9 expands on how automatic sign language transcription can be\nused in spoken language processing pipelines, including the critical but often\noverlooked non-verbal cues such as co-speech gestures and facial expressions.\n12https://www.signbank.org/signpuddle2.0/index.php?ui=4&sgn=49\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n120\n6.3\nTranslation (Moryossef and Jiang, 2023)\nDuring this thesis, I had the privilege of supervising the research undertaken\nby Jiang et al. (2023a). Our work established a foundational framework for\ntranslating between spoken languages rendered in textual format and signed\nlanguages depicted through SignWriting. The ensuing section delves deeper\ninto the initiatives undertaken to improve upon our work, refine data quality,\nand develop a model tailored for client-side deployment.\nIn this thesis, we introduce SignBank+, a clean version of the SignBank\ndataset, optimized for machine translation. Contrary to previous work that\nemploys complex factorization techniques for translation, we advocate for a\ntraditional text-to-text translation approach. Our naive evaluation shows that\nmodels trained on SignBank+ surpass those on the original dataset, establishing\na new benchmark and providing an open resource for future research.\n6.3.1\nIntroduction\nSign Language serves as an indispensable mode of communication for the deaf.\nUnfortunately, the available methods for translating between signed and spo-\nken languages, have been limited in scope and effectiveness. The main objective\nof this research is to explore technological advancements that can enhance the\ntranslation process, focusing on the cleaning and enrichment of an existing sign\nlanguage dataset, SignBank13, a multilingual collection of puddles, covering a\nrange of domains.\nThe pioneering work of Jiang et al. (2023a) set the stage for this task. They\npresented an approach to translating SignWriting through specialized parsing\nand factorized machine translation techniques. Motivated by their efforts, this\nresearch aims to build upon their foundation by:\n1. Undertaking a rigorous data cleaning and expansion process.\n2. Reverting to a simple translation mechanism, omitting any factorization.\n13https://www.signbank.org/signpuddle/\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n121\nThe hypothesis driving this study is twofold: First, a meticulously curated\ndataset will enhance the accuracy and reliability of translation models. Second,\nby simplifying the translation process, it becomes feasible to train a diverse ar-\nray of models and streamline their deployment.\nTo validate our claims, we compare the translation quality of signed-to-\nspoken translation using the original and cleaned data. We show that with our\nnew, cleaner data, we can train standard machine translation models with im-\nproved quality over the original data. We share our data openly under CC-BY-\n4.0 (available at https://github.com/sign-language-processing/si\ngnbank-plus) to be used in future machine translation research.\n6.3.2\nBackground\nThis work only concerns machine translation between signed and spoken lan-\nguages where both the input and the output are represented as text.\nSigned-to-Spoken\nJiang et al. (2023a) explore text-to-text sign to spoken language translation, with\nSignWriting as the chosen sign language notation system. Despite SignWrit-\ning usually represented in 2D, they use the 1D Formal SignWriting specifica-\ntion and propose a neural factored machine translation approach to encode se-\nquences of the SignWriting graphemes as well as their position in the 2D space.\nThey verify the proposed approach on the SignBank dataset in both a bilin-\ngual setup (American Sign Language to English) and two multilingual setups\n(4 and 21 signed-to-spoken language pairs, respectively). They apply several\nlow-resource machine translation techniques used to improve spoken language\ntranslation to similarly improve the performance of sign language translation.\nTheir findings validate the use of an intermediate text representation for signed\nlanguage translation, and pave the way for including sign language translation\nin natural language processing research.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n122\nSpoken-to-Signed\nJiang et al. (2023a) also explore the reverse translation direction, i.e., text to Sign-\nWriting translation. They conduct experiments under a same condition of their\nmultilingual SignWriting to text (4 language pairs) experiment, and again pro-\npose a neural factored machine translation approach to decode the graphemes\nand their position separately. They borrow BLEU from spoken language trans-\nlation to evaluate the predicted graphemes and mean absolute error to evaluate\nthe positional numbers.\nWalsh et al. (2022) explore Text to HamNoSys (T2H) translation, with Ham-\nNoSys as the target sign language notation system. They experiment with direct\nT2H and Text to Gloss to HamNoSys (T2G2H) on a subset of the data from the\nMEINE DGS dataset (Hanke et al., 2020), where all glosses are mapped to Ham-\nNoSys by a dictionary look up. They find that direct T2H translation results in\nhigher BLEU (it still needs to be clarified how well BLEU represents the quality\nof HamNoSys translations, though). They encode HamNoSys with BPE (Sen-\nnrich et al., 2016b), and it outperforms character-level and word-level tokeniza-\ntion. They also leverage BERT to create better sentence-level embeddings and\nuse HamNoSys to extract the hand shape of a sign as additional supervision\nduring training.\nMachine Translation Frameworks\nMachine translation has witnessed substantial advancements in recent years,\nboth in terms of model architectures and frameworks that facilitate their train-\ning and deployment. When it comes to text-to-text translation, several open-\nsource platforms have emerged, leading to the democratization of machine trans-\nlation technology.\nProminent machine translation frameworks\ninclude OpenNMT (Klein et al.,\n2017), Sockeye (Hieber et al., 2017, 2020), Joey NMT (Kreutzer et al., 2019), and\nFairseq (Ott et al., 2019). They are all widely renowned for simplicity, efficiency,\nand emphasis on performance, promoting rapid prototyping and thus becom-\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n123\ning a popular choice among machine translation researchers.\nBergamot (2022)\naims to bring machine translation to local clients. Leverag-\ning advancements in Marian NMT (Junczys-Dowmunt et al., 2018), Bergamot\nprovides recipes for fast, local, multilingual machine translation models. It pro-\nvides an opinionated pipeline and assumes both the source and the target come\nfrom spoken languages. It only supports text-to-text translation, and expects a\nshared source-target vocabulary and a huge amount of data, uncommon in sign\nlanguage resources. Despite the project’s disadvantages, it is the only one that\nincludes a realistic training pipeline for machine translation deployment.\n6.3.3\nData\nIn our efforts to improve sign language translation through a text-to-text ap-\nproach, data quality and quantity are of paramount importance. This section\noutlines our data curation strategy, encompassing both the data we generate\nourselves and the data we clean and expand.\nFingerspelling Data\nFingerspelling is a significant component of signed languages, often used for\nspelling out names, places, or other words that might not have a designated\nsign. Given its importance, we collected and annotated fingerspelling for let-\nters and numbers across 22 different signed languages14 to be used in future\nmachine translation systems. These annotations are largely derived manually\nfrom the fingerspelling keyboard15.\n14American, Brazilian, British, Chinese, Danish, Flemish, French, French Belgian, German,\nHonduran, Irish, Israeli, Italian, Japanese, Mexican, Nicaraguan, Norwegian, Portuguese, Span-\nish, Swedish, Swiss German, and Thai.\n15https://www.signwriting.org/forums/software/fingkeys/fkey001.html\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n124\nSignBank Cleaning and Expansion\nThe SignBank dataset, while invaluable, includes numerous inconsistencies and\nimperfections. Multiple non-parallel textual entries were associated with singu-\nlar signing sequences. For instance, while some entries indicated chapter and\npage numbers from a book, the actual text was missing. In others, definitions\nwere jumbled with the intended word. In light of these challenges, we initiated\nmeticulous data-cleaning and expansion processes detailed below:\nDataset Cleaning\nInitially, we manually corrected at least five entries for each\npuddle. Given the formulaic nature of certain puddles (e.g., the bible), rule-\nbased corrections enabled immediate annotation of multiple entries. Compre-\nhensive rules used in this phase are detailed in §6.3.4.\nUsing ChatGPT (OpenAI, 2022), we defined a pseudo function that gets the\nnumber of signs, language code, and existing terms to return a cleaned, parallel\nversion of the terms:\nclean(number of signs, language code, terms).\nAn illustration would be the function call:\nclean(1, \"sl\", [\"Koreja (mednarodno)\", \"Korea\", \"S125-P1\"])\nreturning [\"Koreja\", \"Korea\"]. More examples are available in §6.3.5.\nTo ascertain the efficacy of this cleaning method, we employed the gpt-3.5-\nturbo-0613 model on the manually cleaned samples. By comparing these results\nto the cleaned dataset, we assessed the quality via the Intersection over Union\n(IoU)16 metric between the predicted terms and the annotated terms. We com-\npared multiple settings, with various approaches to cleaning the data:\n1. E0: No changes.\n2. E1: Rule-based cleaning (§6.3.4).\n3. E2: E1 + ChatGPT with four fixed, manually selected few-shot examples.\n16Note: The maximum IoU is not 1. We can not ignore possible annotation errors/variations,\nespecially when dealing with non-English data.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n125\n4. E3: E1 + ChatGPT with five few-shot examples from the same puddle.\n5. E4: E1 + ChatGPT with four fixed examples and five examples from the\nsame puddle.\n6. E5: E4 + using gpt-4-0613.\nDoing nothing (E0) leads to a base IoU of 0.50. The rule-based approach\n(E1), which conservatively eliminated undesired text entries, provided a slight\nboost, resulting in an IoU of 0.53. Incorporating general few-shot examples into\nthe cleaning process (E2) significantly increased the IoU to 0.63. A more targeted\napproach using five few-shot examples from the same puddle (E3) further im-\nproved this to 0.71 IoU. When combining the general few-shot examples with\npuddle-specific examples (E4), we achieved an IoU of 0.74. Our best results,\nhowever, came from GPT-4 (E5), which achieved an IoU of 0.80.\nFor cost considerations, the following pricing was assumed: $0.0015/1K to-\nkens for gpt-3.5-turbo and $0.03/1K tokens for gpt-4, indicating a 20× price dis-\nparity. Given the average of 714 tokens for E4 and E5 and around 200K annota-\ntions, the projected costs for gpt-3.5-turbo and gpt-4 are approximately $200 and\n$4000, respectively. For financial reasons, we use gpt-3.5-turbo. The final cost\nended up being $230.18, paid to OpenAI.\nDataset Expansion\nOur next objective is to further enrich the dataset by in-\ntroducing variations for each cleaned term. Variability in language representa-\ntion can significantly benefit the robustness of machine translation models by\nproviding multiple ways of expressing the same idea. For this, we designed\na function, expand(language code, terms), producing expanded terms\nand proper capitalization. As some terms were in English, outputs for both the\nspecific language and English were generated separately. Prompt in §6.3.5.\nFor an illustration, consider a term in Swedish such as ‘tre’. When passed\nto our function like so: expand(\"sv\", [\"tre\"]), the returned output could\nbe {\"sv\":\n[\"Tre\", \"3\"], \"en\":\n[\"Three\", \"3\"]}. This means that\nfor the Swedish language (‘sv’), the term ‘tre’ can be represented as ‘Tre’ or\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n126\nthe numeral ‘3’. The corresponding English translation for the term would be\n‘Three’. Another example would be the German term for ‘father’. The function\ncall expand(\"de\", [\"Vater\", \"father\"]) yields\n{\"de\":\n[\"Vater\", \"Vati\", \"Papa\", \"Erzeuger\"],\n\"en\":\n[\"Father\", \"Dad\", \"Daddy\"]}.\nHere, the term expands to multiple terms in both German and English.\nThis expansion approach (using gpt-3.5-turbo with 9 fixed few-shot exam-\nples), although seemingly straightforward with a similar cost to the cleaning\nprocess, introduces vast richness to our dataset. Each term is now associated\nwith multiple representations, thereby enhancing the potential of our model to\nunderstand the nuances and variability of language. However, this expansion\ncan also introduce errors, either when expanding terms that were not properly\ncleaned, or when the expansion itself is wrong. The expansion cost ended up\nbeing $299.72, paid to OpenAI.\nEvaluating the efficacy of this expansion step is non-trivial, due to the inher-\nent subjectivity involved in determining which expansions are valid or more\nuseful than others. Interested readers are referred to §6.3.6 for more outputs.\n6.3.4\nCleaning Rules (Appendix)\nAutomatic Annotation Rules\nQuestion Marks\nIt is rare, but sometimes, this movement symbol is used as\na question mark, because of visual resemblance. We remove all entries that\ncontain only a question mark (M510x517S29f0c491x484).\nKorean (puddle 78)\nThis large puddle (25k entries) is quite standardized. Most\nentries include four terms, in a predictable fashion. For all 22k entries that match\nthis fashion, we annotate them with the second term, excluding the number that\nfollows (i.e., in English, hello3 becomes hello).\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n127\nSlovene (puddle 52)\nOut of 6k entries, about 3k seem to fit a specific pattern.\nA single term, with possibly a single uppercase letter (variation) and the source\nin parenthesis. For example, {zdarma B (UPOL)} is annotated by removing\nthe variation and source, to result in zdarma.\nThe Bible (puddles 151 and 152)\nThese puddles include translation of the\nBible into SignWriting in Signed Exact English (SEE) and not American Sign\nLanguage (ASL). Almost every entry includes a book, chapter, and verse identi-\nfier, for example 1Corinthians01v03 means The First Epistle to the Corinthi-\nans, Chapter 1, Verse 3. We only address entries that we can extract the book,\nchapter, and verse from, and that are of a single verse, not split apart (some en-\ntries contain parts of verses, and others contain multiple verses). Based on the\nmatch, we extract the verse from the bible-corpus17, and disregard any other text\nin the entry. In some entries, the SignWriting starts with indicating ‘Verse’ and\na number. We attempt to recognize when this happens based on simple string\nmatching, and when it does, we add Verse {number}: to the beginning of\nthe verse.\nData Filtering Rules\n• We remove all terms that include a URL in them. These usually link to an\nimage, a video, or a source.\n• For Slovene entries in puddle 52 that did not match our criteria for auto-\nmatic annotation, we strip the suffix as mentioned above from all terms.\n• For Swiss-French in puddle 49, we remove entries that indicate the source\nbased on the following regex: (lexique SGBFSS|lexique SGB-FSS|\nˆliste:|jeu SignEcriture|JEU-COULEURS|CCSS|Ap´eroSignes).\nWe also remove entries that start with FMS or EMM, or include a n°\n• For Honduran in puddle 16, we remove entries that include ‘SWS-TAG’.\n17https://github.com/christos-c/bible-corpus\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n128\n• For German in puddle 53, we remove all entries that include vgl, KK or\ndelegs, or that match Variante \\d, Geschichte \".*?\", [Ss][\\d.\n]* or rwth\\d*.\n• For American in puddle 4, we remove all entries equal to English sign.\n• For Argentinian in puddle 41, we remove all entries that start with .LSC.\n• For Quebecois in puddle 47, we remove all entries that start with either\nListe:\nor Alice. We also filter out the last term if it matches a part of\nspeech (nom, verbe, adjectif, adverbe, pronom, pr´eposition, conjonction,\ninterjection, d´eterminant, phrase, g´eographie).\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n129\n6.3.5\nPrompts (Appendix)\nData Cleaning\nSystem\nYou are a proficient assistant, responsible for data sanitization \\\nfor a machine translation model. Your main task involves \\\noperating the ‘clean‘ function:\nThe ‘clean‘ function is specifically designed to return a list of \\\naccurate translations that correspond to the provided SignWriting text.\nThe text output must be valid spoken language text, \\\nfixed in case of errors.\nThis aids in the training of machine translation models.\nSince SignWriting can also write fingerspelling, \\\na single character can also be a valid output.\nWhen confronted with an unfamiliar SignWriting string, \\\nthe function employs heuristic methods, including:\n1. A similarity in the number of words to the number of signs. \\\nFor example, a string with 20 signs and 3 words is unlikely \\\nto have a parallel translation.\n2. Potential multiple entries are considered when several terms \\\nin the text have the same meaning. For instance, \"one (1)\" \\\nor \"one-also-1\" or \"one/1\" will result in \"one\" and \"1\".\n3. In cases of uncertainty, the function returns an empty array.\nParameters:\nSignWriting (int): Number of signs present.\nLanguage (string): language code (Optional)\ntexts (list): A list of raw, unfiltered candidate texts.\nReturns:\ntranslations (list): A list of strings that accurately \\\nmatch the SignWriting, intended for machine translation.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n130\nIn the course of the session, users will invoke the ‘clean‘ \\\nfunction and you will respond with the function’s output.\nManually Curated Few Shots\nUser\nAssistant\nclean(1, \"sl\", [\"Koreja\n(mednarodno)\", \"Korea\"])\n[\"Koreja\", \"Korea\"]\nclean(1, \"sl\", [\"Bosna in\nHercegovina 2\", \"Bosnia and\nHerzegovina\"])\n[\"Bosna in Hercegovina\",\n\"Bosnia and Herzegovina\"]\nclean(18, \"en\", [\"Acts\n04 27-31c\", \"James Orlow\"])\n[]\nclean(8, \"es\", [\"Juan el\nBautista predica\", \"1:1 El\nprincipio de la buena noticia\nde Jesucristo, el Hijo de\nDios.\"])\n[\"El principio de la buena\nnoticia de Jesucristo, el\nHijo de Dios.\"]\nTable 6.5: Manually curated few-shot user and assistant interactions.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n131\nData Expansion\nSystem\nYou are a proficient assistant, responsible for data augmentation \\\nfor a machine translation model. \\\nYour main task involves executing the ‘expand‘ function:\nThe expand function is specifically crafted to return a list \\\nof equivalent expressions that match the provided text \\\nin a certain language. The text output must be accurately \\\nspelled and grammatically correct spoken language text. \\\nThis is beneficial for improving the robustness of \\\nmachine translation models.\nThe function uses a series of methods to ensure a variety \\\nof equivalent terms, including:\n1. Paraphrases in the same language. For example, the text \"hello\" \\\ncan be paraphrased as \"Hi\" or \"Hey\"\n2. Capitalization corrections: For instance, the text \"one\" may \\\nresult in [\"One\"]. \"donald duck\" may result in [\"Donald Duck\"] \\\n3. Numerical translation: When the text represents a number, \\\nits numeric\nequivalent is added. For example, \\\n\"one\" would return [\"One\", \"1\"].\n4. Language translation: If the text is in a non-English language, \\\nand it’s feasible to translate it to English, the English \\\ntranslation is included. For instance, \\\n\"domingo\" would return [\"Domingo\", \"Sunday\"].\n5. If the text is not spoken language text, but instead a random \\\nidentifier like \"rom-ale-10-44r\", ignore it completely.\nParameters:\nlanguage (string): language code for the terms in the list.\ntexts (list): A list of terms requiring expansion.\nReturns:\nexpansions (obj):\nlanguage (list): A list of unique strings that are equivalent \\\nto the provided terms, intended for machine translation training.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n132\nen (list): A list of unique translations to English if \\\nthe language is not english and translation is feasible.\nDuring the session, users will call the ‘expand‘ function and you \\\nwill respond with the function’s output.\nManually Curated Few Shots\nUser\nAssistant\nexpand(\"sl\", [\"2\"])\n{\"sl\":\n[\"2\", \"Dva\"], \"en\":\n[\"2\", \"Two\"]}\nexpand(\"fr\", [\"Lac\",\n\"LEMAN\"])\n{\"fr\":\n[\"Lac Leman\"], \"en\":\n[\"Lake Geneva\"]}\nexpand(\"no\", [\"fire\",\n\"tall\", \"4\"])\n{\"no\":\n[\"fire\", \"4\"], \"en\":\n[\"four\", \"4\"]}\nexpand(\"pt\", [\"MAGIA\",\n\"MAGICO\"])\n{\"pt\":\n[\"MAGIA\", \"M´AGICO\"],\n\"en\":\n[\"Magic\", \"Magical\",\n\"Magician\"]}\nexpand(\"de\", [\"Vater\",\n\"father\"])\n{\"de\":\n[\"Vater\", \"Papa\", \"Papi\",\n\"Vati\", \"Erzeuger\"], \"en\":\n[\"Father\", \"Dad\", \"Daddy\",\n\"Papa\"]}\nexpand(\"en\", [\"Hello\",\n\"hi\"])\n{\"en\":\n[\"Hello\", \"Hi\", \"Hey\",\n\"Greetings\", \"Howdy\", \"Hiya\",\n\"Aloha\", \"Bonjour\", \"Hola\",\n\"Salutations\", \"Hello there\",\n\"Hi there\"]}\nexpand(\"pt\",\n[\"Eu-tentar\"])\n{\"pt\":\n[\"Eu tentar\"], \"en\":\n[\"I\ntry\"]}\nexpand(\"de\", [\"zwaiund\nzwanzig\", \"S3-07163-V\"])\n{\"de\":\n[\"Zwei und Zwanzig\",\n\"22\", \"Zweiundzwanzig\"], \"en\":\n[\"Twenty-two\", \"22\"]}\nTable 6.6: Manually curated few-shot user and assistant interactions.\n6.3.6\nDataset Preview\nHere we include a preview of the dataset, including the original spoken language terms, the automatically cleaned terms based on the original\ntexts, the human annotation based on the original text, and the expanded texts based on the human annotation. We include one sample per\npuddle (as long as the puddle includes a sample that is less than 200 characters).\nDue to incompatibility in displaying non-latin scripts, we exclude Korean, Chinese, Arabic, Urdu, Hebrew, Russian, Ukrainian, Pashto,\nJapanese, Hindi, Amharic, Thai, Polish, Greek, Maltese, Bulgarian, Nepali, and Vietnamese.\npid\neid\nLang\nOriginal\nCleaned (from Original)\nAnnotation\nExpanded (from Annotation)\n4\n101\nen\n[glasses]\n[glasses]\n[Glasses]\n[Glasses, Eyeglasses,\nSpectacles, Optical\nframes]\n5\n296\nen\n[gesture 1, heck with it]\n[heck with it, gesture 1]\n[heck with it]\n[Heck with it, Forget\nit, Never mind, Screw it,\nDrop it, Let it go]\n11\n92\nen\n[cookie, biscuit,\nhttps://www.youtube.com/watch?v=jyOh9Ss7Dzs]\n[cookie, biscuit]\n[cookie, biscuit]\n[Cookie, Biscuit,\nCracker, Snack, Treat,\nSweet, Baked good]\n13\n2\nzh-CN\n[epigrafe]\n[]\n[]\n[]\n16\n2829\nes\n[so˜nar,\nSWS-TAG[SignLists:Dict\nSept15\nHonduras,SignLists:New\nDict,Dibujos:Dibujos\nGrupo1]]\n[so˜nar]\n[so˜nar]\n[so˜nar, so˜nando]\n17\n2\nen\n[CBS 1, Deaf Hands Story\nCBS 1 (as retold by\nStuart)]\n[CBS 1, Deaf Hands Story\nCBS 1]\n[]\n[]\n19\n1947\npl\n[sobie, siebie, to\noneself; oneself]\n[to oneself, oneself,\nsiebie, sobie]\n[sobie, siebie, to\noneself, oneself]\n[sobie, siebie]\n20\n49\nfr\n[amygdalectomie]\n[amygdalectomie]\n[amygdalectomie]\n[Amygdalectomie]\n21\n624\nen\n[VetDr04]\n[]\n[]\n[]\n22\n8\nfr\n[ADN, ADN-d´efinition]\n[ADN]\n[]\n[]\n23\n17\nno\n[Gullh˚ar 12, Vi tre g˚ar\nen tur i skogen.]\n[Vi tre g˚ar en tur i\nskogen.]\n[Vi tre g˚ar en tur i\nskogen.]\n[Vi tre g˚ar en tur i\nskogen.]\n24\n4\nno\n[f˚a (ikke mange)]\n[f˚a]\n[f˚a, ikke mange]\n[f˚a, ikke mange]\n25\n1268\nen\n[wrong, accidental, by\nmistake]\n[by mistake, accidental,\nwrong]\n[wrong, accidental, by\nmistake]\n[Wrong, Incorrect,\nInaccurate,\nMistaken, Accidental,\nUnintentional, By\nmistake, By accident,\nIn error]\n26\n261\nde\n[St¨adte Teil 2, Spiel:\n\"Stadt, Land, Fluss\"]\n[Spiel:\nStadt, Land,\nFluss, St¨adte Teil 2]\n[]\n[]\n27\n14\nde\n[Beispielsatz index,\nThomas kauft ein Auto.\nEs ist billig.]\n[Thomas kauft ein Auto.\nEs ist billig.]\n[Thomas kauft ein Auto.\nEs ist billig.]\n[Thomas kauft ein Auto.\nEs ist billig.]\n28\n11584\nen\n[455]\n[455]\n[455]\n[455, Four hundred\nfifty-five]\n29\n4\nde\n[Vater, father]\n[Vater, father]\n[Vater, father]\n[Vater, Papa, Papi,\nErzeuger]\n30\n30\nda\n[R, fingerspelling]\n[R]\n[R]\n[R]\n31\n173\nmt\n[Kugin, Cousin,\nDizzjunarju ta’\nAffarijiet ta’\nKuljum\\n\\nVolum:\nFAMILJA]\n[Cousin, Kugin]\n[Kugin, Cousin]\n[Kugin, Kuzin]\n32\n9\nen\n[God is with us, The\nbiblical meaning of\nEmmanuel]\n[God is with us,\nEmmanuel]\n[God is with us]\n[God is with us]\n33\n409\npt\n[express˜ao-facial,\nexpress˜ao-facial]\n[express˜ao-facial]\n[express˜ao-facial,\nexpress˜ao-facial]\n[Express˜ao facial, Rosto,\nExpress˜ao no rosto]\n35\n6\nen\n[Arkansas, US State, (n)\na state in the United\nStates.]\n[Arkansas]\n[Arkansas]\n[Arkansas]\n36\n156\ncs\n[O pern´ıkov´e chaloupce 2]\n[O pern´ıkov´e chaloupce 2]\n[]\n[]\n37\n9\ncs\n[podtˇr´ıda]\n[]\n[]\n[]\n41\n1765\nes\n[Grace, .LSC vocab\nPersonas]\n[Grace]\n[Grace]\n[Grace]\n42\n23\nen\n[brother]\n[brother]\n[brother]\n[Brother, Sibling,\nBro, Buddy, Mate, Pal,\nComrade, Fellow]\n43\n1396\nfr\n[cin´ema]\n[cin´ema]\n[cin´ema]\n[cin´ema]\n44\n5617\nnl\n[Jelle, jelle]\n[jelle, Jelle]\n[Jelle, jelle]\n[Jelle]\n45\n3\nes\n[walk]\n[caminar]\n[walk]\n[caminar, andar, pasear]\n46\n11173\npt\n[alfabeto]\n[alfabeto]\n[]\n[]\n47\n10094\nfr\n[tr´esorier, tr´esori`ere,\nnom]\n[tr´esorier, tr´esori`ere]\n[tr´esorier, tr´esori`ere]\n[tr´esorier, tr´esori`ere]\n48\n3891\nde\n[glcklich-2]\n[glcklich]\n[Gl¨ucklich, Happy]\n[Gl¨ucklich, Froh,\nFr¨ohlich, Zufrieden]\n49\n1267\nfr\n[jeu SignEcriture,\n3-11-4]\n[]\n[]\n[]\n50\n2\nit\n[VAUD, canton Suisse]\n[VAUD]\n[VAUD]\n[VAUD]\n51\n801\nes\n[pensamiento]\n[pensamiento]\n[pensamiento]\n[pensamiento, reflexi´on,\nidea, concepci´on,\ncogitaci´on]\n52\n1007\nsk\n[displej (IMoTeSP)]\n[displej]\n[displej]\n[displej, obrazovka]\n53\n12013\nde\n[f¨ur, hier:\nf¨ur 2010\n(obewohl kein Bonativ?)]\n[f¨ur, hier:\nf¨ur 2010]\n[f¨ur]\n[f¨ur]\n54\n1222\neo\n[ist, G@17]\n[ist]\n[ist]\n[estas, estis, estos]\n55\n34\nes\n[tocar]\n[tocar]\n[tocar]\n[tocar, reproducir,\ninterpretar, ejecutar]\n56\n1605\nca\n[geograf´ıa]\n[geograf´ıa, geografia,\ngeography]\n[geograf´ıa]\n[geografia, geografies]\n57\n173\nfi\n[TAVATA]\n[TAVATA]\n[TAVATA]\n[TAVATA, Tavata]\n58\n1375\nfr\n[surnom]\n[surnom]\n[surnom]\n[surnom, sobriquet,\npseudo, surnommer]\n59\n274\nen\n[boy, Theme:\nfamily\ndetails, son, theme:\ncards]\n[boy, son]\n[boy, son]\n[Boy, Son, Child,\nYoungster, Lad, Kid,\nOffspring, Male child]\n60\n12\nen\n[seven]\n[seven]\n[seven]\n[Seven, 7]\n62\n35\nen\n[A, fingerspelling]\n[A]\n[A]\n[A, One]\n63\n677\nit\n[(passato) incontrare]\n[incontrare]\n[incontrare]\n[incontrare, incontrarsi]\n65\n65\nes\n[Uno, Uno (one)]\n[Uno, one]\n[Uno, one]\n[Uno, 1]\n66\n3\nms\n[Malaysia]\n[Malaysia]\n[Malaysia]\n[Malaysia]\n67\n1598\nes\n[banarse, bathe]\n[ba˜narse]\n[banarse, bathe]\n[ba˜narse, ducharse]\n68\n68\nnl\n[AANGIFTE / AANGEVEN ()]\n[AANGIFTE, AANGEVEN]\n[AANGIFTE, AANGEVEN]\n[AANGIFTE, AANGEVEN,\nMelding, Melden, Aangifte\ndoen]\n69\n679\nno\n[4, fire, tall]\n[fire, tall, 4]\n[4]\n[4, Fire]\n70\n33\nen\n[I]\n[I]\n[I]\n[I, Me, Myself]\n71\n67\nes\n[flor, flower]\n[flor, flower]\n[flor, flower]\n[flor, flora]\n72\n325\nfil\n[mountain]\n[mountain]\n[mountain]\n[bundok]\n73\n72\nsv\n[¨A, fingerspelling]\n[¨A, ¨a]\n[¨A]\n[¨A, A med ring ¨over]\n74\n2477\nsl\n[pojutriˇsnjem, day after\ntomorrow]\n[pojutriˇsnjem, day after\ntomorrow]\n[pojutriˇsnjem, the day\nafter tomorrow]\n[pojutriˇsnjem,\npojutriˇsnjem dnevu]\n76\n39\nes\n[y]\n[y]\n[y]\n[y, e]\n77\n45\nen\n[Science]\n[Science]\n[Science]\n[Science, Scientific,\nSciences]\n79\n10\nsw\n[see]\n[see]\n[see]\n[see]\n80\n883\npt\n[barriga]\n[barriga]\n[Barriga, Stomach]\n[Barriga, Estˆomago]\n81\n36\nfr\n[Les cinq fr`eres chinois\n02]\n[]\n[]\n[]\n82\n115\nsq\n[Itali, italy]\n[Itali, Italy]\n[Itali, italy]\n[Itali, Italia]\n89\n260\nsk\n[chlapec]\n[chlapec]\n[chlapec]\n[chlapec, chalan, kluk,\nchlapˇcensk´y]\n90\n34\ntr\n[o, he/she/it; er/sie/es]\n[o, he/she/it, er/sie/es]\n[o, he, she, it, er, sie,\nes]\n[o, he, she, it, er, sie,\nes]\n91\n28\nar\n[Goldilocks Page 1]\n[]\n[]\n[]\n93\n527\nes\n[SignoEscritura Reading\nLessons pg.\n20]\n[]\n[]\n[]\n94\n25\nca\n[patinatge art´ıstic]\n[patinatge art´ıstic]\n[patinatge art´ıstic]\n[Patinatge art´ıstic,\nPatinatge art´ıstic sobre\ngel]\n96\n13\nde\n[Noah 15]\n[]\n[]\n[]\n98\n35\nnl\n[android]\n[android]\n[android]\n[Android]\n99\n16\nja\n[texto11]\n[]\n[]\n[]\n100\n1\nam\n[texto11]\n[texto11]\n[]\n[]\n104\n1181\nar\n[Bonjour]\n[Bonjour]\n[]\n[]\n105\n338\nen\n[exit, leave, out]\n[exit, out, leave]\n[exit, leave, out]\n[Exit, Leave, Out,\nQuit, Depart, Go away,\nWithdraw, Egress, Vacate]\n111\n192\nen\n[why]\n[why]\n[why]\n[Why, For what reason,\nWhat is the reason, What\nis the purpose, What is\nthe cause]\n113\n1\nht\n[Zebra]\n[Zebra]\n[Zebra]\n[Zebra]\n114\n2190\npt\n[prova]\n[prova]\n[prova]\n[prova, teste, exame]\n115\n24\npt\n[bebe12]\n[]\n[]\n[]\n116\n1348\npt\n[Dentes superiores\ntocando a l´ıngua]\n[Dentes superiores\ntocando a l´ıngua]\n[]\n[]\n117\n8\npt\n[Isabel Morais, Nome\nGesto]\n[]\n[]\n[]\n119\n404\nes\n[World Explorers Part 1\npg.\n06]\n[]\n[]\n[]\n120\n17\nes\n[Tapa abriendo]\n[Tapa abriendo]\n[Tapa abriendo]\n[Tapa abriendo]\n122\n3\nhu\n[Orsz´ag, Country, Land]\n[Orsz´ag, Country, Land]\n[Orsz´ag, Country, Land]\n[Orsz´ag, Orsz´agok]\n123\n1\nhu\n[raus, im ¨argerlichen\nBefehlston]\n[raus]\n[raus]\n[raus]\n124\n15\nfr\n[corbeau]\n[corbeau]\n[corbeau]\n[corbeau, corneille,\ncorbin]\n125\n16\nen\n[The farmer is in his\nden, part 6, Theme:\npractice makes perfect\nDVD, The dog wants a\nbone, the dog wants a\nbone, E I A D O, the dog\nwants a bone.]\n[The farmer is in his\nden, part 6, The dog\nwants a bone, the dog\nwants a bone, E I A D O,\nthe dog wants a bone.]\n[The dog wants a bone,\nthe dog wants a bone, E\nI A D O, the dog wants a\nbone.]\n[The dog wants a bone,\nthe dog wants a bone, E\nI A D O, the dog wants a\nbone.]\n126\n319\nar\n[Luc4:23 (LSF)]\n[]\n[]\n[]\n128\n41\nmw\n[teacher]\n[teacher]\n[teacher]\n[teacher]\n129\n5284\ngn\n[Mejorar 2]\n[Mejorar, 2]\n[Mejorar]\n[Mejorar]\n131\n36\nis\n[st´or (pf.1)]\n[st´or]\n[st´or]\n[st´or, st´ort]\n132\n12\nro\n[luni, Transilvania\nSemne, Monday, Montag]\n[Monday, Montag, luni]\n[luni, Monday, Montag]\n[luni]\n135\n89\nes\n[h, Letra (consonante)]\n[h]\n[h]\n[h]\n137\n14\nes\n[San Salvador, san\nsalvador, Capital (El\nSalvador), Geografia.]\n[San Salvador]\n[San Salvador, san\nsalvador]\n[San Salvador]\n143\n84\nes\n[abierto]\n[abierto]\n[abierto]\n[abierto, desbloqueado,\nlibre, sin restricciones]\n147\n1154\nmt\n[Taken from...Ghaqda,\nMehud mit-test tal-Ghaqda\nBibblika ..etc]\n[Mehud mit-test\ntal-Ghaqda Bibblika\n..etc]\n[]\n[]\n148\n17\nsl\n[Sveti Filip, prosi za\nnas!]\n[Sveti Filip, prosi za\nnas!]\n[Sveti Filip, prosi za\nnas!]\n[Sveti Filip, prosi za\nnas!]\n151\n10009\nen\n[Matthew15v07 NLT, You\nhypocrites!\nIsaiah was\nright when he prophesied\nabout you, for he\nwrote,\\n\\nMatthew15v7\nNLT]\n[Verse 7:\nYou\nhypocrites!\nWell did\nIsaiah prophesy of you,\nsaying:, Verse 7:\nYe\nhypocrites, well did\nEsaias prophesy of you,\nsaying,]\n[Verse 7:\nYou\nhypocrites!\nWell did\nIsaiah prophesy of you,\nsaying,, Verse 7:\nYe\nhypocrites, well did\nEsaias prophesy of you,\nsaying,]\n[]\n152\n10173\nen\n[Proverbs 24:28, Pr\n24:28 ¶ Be not a witness\nagainst thy neighbour\nwithout cause; and\ndeceive [not] with thy\nlips.\\n\\nDo not witness\nagainst neighbor for no\nreason and do not deceive\npeople.]\n[Be not a witness against\nyour neighbor without\ncause; and deceive not\nwith your lips., Do\nnot witness against\nneighbor without cause;\nand deceive not with your\nlips.]\n[Don’t be a witness\nagainst your neighbor\nwithout cause.\nDon’t\ndeceive with your lips.,\nBe not a witness against\nthy neighbour without\ncause; and deceive not\nwith thy lips.]\n[]\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n140\n6.3.7\nData Quality Experiments\nSockeye\nFairseq\nOpenNMT\nKeras (mT5)\nDataset\nTraining Pairs\nVocab\nBLEU\nchrF\nBLEU\nchrF\nBLEU\nchrF\nBLEU\nchrF\nOriginal\n521, 390\n6, 016\n0.2\n8.4\n0.18\n4.74\n0.69\n9.21\n0.07\n6.39\nCleaned\n357, 574\n5, 200\n22.32\n28.63\n1.1\n7.59\n30.6\n22.46\n6.02\n12.35\nExpanded\n1, 027, 418\n5, 976\n0.55\n7.22\n1.26\n6.52\n13.38\n13.0\n2.99\n12.49\nTable 6.8: Evaluation of the usability of our data for machine translation.\nTo evaluate the quality of our cleaning and expansion, we test its effect on\nmachine translation. We train machine translation models on the original data,\non the cleaned data, and on the expanded data, in an imbalanced multilingual\nsetting that contains all of the puddles on SignBank. For this comparison, we\nfocus on the signed-to-spoken direction, since automatic evaluation of spoken\nlanguage text is well established. For a development set, in each data scenario,\nwe consider the first 3000 entries. For our test set, we use our manually anno-\ntated data from §6.3.3. In the source text, we include tags to indicate the source\nand target language for the translation. We use sacreBLEU 2.3.1 (Post, 2018), to\nevaluate BLEU18 (Papineni et al., 2002) and chrF19 (Popovi´c, 2016b).\nThis comparison is only made to evaluate the quality of the different datasets.\nThus, for every framework, we use the default training settings and avoid at-\ntempting to optimize with smaller models or different architecture. We posit\nthat better test-set performance in a given framework indicates higher data\nquality. While we believe that this effect should be highly potent for the spoken-\nto-signed translation direction, it is not evaluated in this work since there are no\nhuman-validated automatic metrics to evaluate SignWriting output.\nSockeye / Fairseq / OpenNMT\nIn pre-processing, the SignWriting text is tok-\nenized using §6.3.8, and the spoken language text is tokenized using BPE (Sen-\nnrich et al., 2016b) with 3000 merges. For the cleaned dataset, this results in\na smaller vocabulary than for the original dataset since some unigrams are fil-\n18BLEU = case:mixed|eff:no|tok:13a|smooth:exp\n19chrF = case:mixed|eff:yes|nc:6|nw:0|space:no\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n141\ntered out. Model training is early-stopped on validation chrF score (Sockeye),\nBLEU (Fairseq), and accuracy (OpenNMT) with a patience of 10 epochs.\nKeras (Chollet et al., 2015)\nTo address the effect of clean data on pre-trained\nlanguage models, we fine-tune mT5-small (Xue et al., 2021) using Keras and\nHuggingFace Transformers (Wolf et al., 2020). In this setting, both the source\nand target texts are tokenized using the mT5 tokenizer. Since our source data\nis extremely out-of-domain to the original language model training, we do not\nexpect to see improvements from the pre-trained language model. The model\nis fine-tuned for up to 20 epochs, early stopped on validation loss.\n6.3.8\nTokenization\nWe tokenize the FSW sequences into discrete tokens. For example, the Ameri-\ncan Sign Language sign for “Hello” is represented as:\nM518x529S14c20481x471S27106503x489\nThis representation is a sequence of graphemes, each with a structure of a\nsymbol , modifiers , and an <x,y> position (Table 6.9).\nsymbol\nmodifiers\nx\ny\n1.\nM\n-\n518\n529\n2.\nS14c\n2 0\n481\n471\n3.\nS271\n0 6\n503\n489\nTable 6.9: Tokenized structure for the ASL sign for ‘Hello’.\nFrom this structure, we treat each component as a separate token. We further\nremove predictable and redundant symbols, such as M, x and the size of the box,\nresulting in the following sequence:\nM p518 p529 S14c c2 r0 p481 p471 S271 c0 r6 p503 p489\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n142\nThis tokenization process simplifies the complex FSW strings, creating a\nsmall vocabulary of 1182 tokens for our NMT framework (4 boxes, 656 sym-\nbols, 6 plane modifiers, 16 rotation modifiers, and 500 positions).\n6.3.9\nResults\nTable 6.8 shows that despite the different frameworks, pre-trained models, un-\noptimized modeling, and imbalanced multilingual translation scenarios, per-\nformance on the cleaned data is consistently better compared to the original\ndata. This establishes our cleaned data as more useful for signed-to-spoken ma-\nchine translation.\nIn the signed-to-spoken translation direction, the use of our expanded data is\ndubious. If our cleaned data is of perfectly good quality, our expansion can only\nadd noise by introducing multiple targets for the same source. However, since\nwe know that our cleaned data is not perfect, we hypothesize that the additional\nnoise from the data expansion smooths out the noise in the imperfect data, by\nintroducing more overlaps between identical translations, thus drowning the\nnoise. This is very difficult to evaluate. As we vary the target texts in many\ndimensions (gender, formality, capitalization, script, and form), uncontrolled\ntranslation of the test set into the original distribution of these dimensions is im-\nprobable, even when disregarding noise coming from wrong expansions. This\nis reflected in the results. Using the expanded data for pre-training our Sockeye\nmodel, then fine-tuning on the cleaned data gets the model back to the target\ndistribution, with state-of-the-art results of 31.39 BLEU and 31.97 chrF.\nWe compare these results to the previous work. Specifically, we query the\nAPI endpoint made available by Jiang et al. (2023a) to translate our test set.\nTo some extent, this is an unfair comparison, since they likely saw these exact\ntranslation sources in training and since we are evaluating more languages than\ntheir model was trained on. And yet, their method achieves 5.03 BLEU and 18.92\nchrF on our test set. Despite their optimization in modeling, our optimization\nin data quality makes up for simple modeling.\nCHAPTER 6. SIGN LANGUAGE TRANSLATION\n143\n6.3.10\nConclusions\nThis work introduces a methodology for data cleaning and expansion for low-\nresource settings such as sign language translation. Its main contribution is the\nintroduction of SignBank+, a cleaner and more expansive sign language trans-\nlation dataset than SignBank. The data and the code for the baseline models are\npublically available on https://github.com/sign-language-process\ning/signbank-plus.\n6.3.11\nFuture Work\nWe encourage future work to expand on our efforts and create SignBank++. The\nclean and expand steps can be executed with more, and better language models.\nQuality estimation filtering methods can be created to filter out text pairs likely\nto not be parallel. Additionally, optimizing the input representation, by encod-\ning SignWriting as images (Dosovitskiy et al., 2021), reducing the token count,\nor standardizing phoneme order, all of which could improve translation per-\nformance. Finally, robust evaluation metrics for spoken-to-signed translation\nshould be created and validated with human judgments.\nChapter 7\nSign Language Production\n7.1\nBaseline (Moryossef et al., 2023b)\nWe explore wether or not estimated skeletal poses are viable for use in sign\nlan- guage translation. We introduce a sign language production baseline that\nsmartly stitches dictionary entries. This baseline is very limited, and the limi-\ntations are throughly discussed. A large part of this section was independently\npublished as “An Open-Source Gloss-Based Baseline for Spoken to Signed Lan-\nguage Translation”.\nSign language translation systems are complex and require many compo-\nnents.\nAs a result, it is very hard to compare methods across publications.\nWe present an open-source implementation of a text-to-gloss-to-pose-to-video\npipeline approach, demonstrating conversion from German to Swiss German\nSign Language, French to French Sign Language of Switzerland, and Italian to\nItalian Sign Language of Switzerland. We propose three different components\nfor the text-to-gloss translation: a lemmatizer, a rule-based word reordering\nand dropping component, and a neural machine translation system. Gloss-to-\npose conversion occurs using data from a lexicon for three different signed lan-\nguages, with skeletal poses extracted from videos. To generate a sentence, the\ntext-to-gloss system is first run, and the pose representations of the resulting\nsigns are stitched together.\n144\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n145\n7.1.1\nIntroduction\nSign language plays a crucial role in communication for many deafindividu-\nals worldwide. However, producing sign language content is often a challeng-\ning, laborious, and time-consuming process, requiring skilled translators/inter-\npreters for effective communication. Recent technological advancements have\nled to the development of automatic sign language translation systems, which\nhave the potential to increase accessibility for the deaf community.\nOne of the critical issues in this field is the lack of a reproducible and re-\nliable baseline. Without a baseline, it is challenging to measure the progress\nand effectiveness of new methods and systems. Additionally, the absence of\nsuch a baseline makes it difficult for new researchers to enter the field, hampers\ncomparative evaluation, and discourages innovation.\nAddressing this gap, this paper presents an open-source implementation of\na text-to-gloss-to-pose-to-video pipeline approach for sign language translation,\nextending the work of Stoll et al. (2018, 2020). Our main contribution is the\ndevelopment of an open-source, reproducible baseline that can aid in making\nsign language translation systems more available and accessible, particularly\nin resource-limited settings. This open-source approach allows the community\nto identify issues, work together on improving these systems, and facilitates\nresearch into novel techniques and strategies for sign language translation.\nOur approach involves three alternatives for text-to-gloss translation, in-\ncluding a lemmatizer, a rule-based word reordering and dropping component,\nand a neural machine translation (NMT) system. For gloss-to-pose conversion,\nwe use lexicon-acquired data for three signed languages, including Swiss Ger-\nman Sign Language, Swiss French Sign Language, and Swiss Italian Sign Lan-\nguage. We extract skeletal poses using a state-of-the-art pose estimation frame-\nwork, and apply a series of improvements to the poses, including cropping,\nconcatenation, and smoothing, before applying a smoothing filter.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n146\nSuchen\nSie\neine\n¨Arztin\nauf,\nwenn\nSie\nAusk¨unfte\noder\nHilfe\nben¨otigen.\nSie-ix\nAusk¨unft+ oder\nHilfe\nben¨otigen\nsie-IX\n¨Arztin\naufsuchen\nSIE\nAUSKUNFT+ ODER\nHILFE\n(missing)\nSIE\n(missing)\nSUCHEN\nText-to-Gloss\nGloss Lookup\nGloss-to-Pose\nPose-to-Video\nFigure 7.1: The figure depicts the entire pipeline of the proposed\ntext-to-gloss-to-pose-to-video approach for sign language translation. Starting\nwith a German sentence, the system applies text-to-gloss translation, for\nexample, using a rule-based word reordering and dropping component. The\nresulting gloss sequence is used to search for relevant videos from a lexicon of\nSwiss German Sign Language (DSGS). The poses of each relevant video are\nthen extracted and concatenated in the gloss-to-pose step to create a pose\nsequence for the sentence, which is then transformed back to a (synthesized)\nvideo using the pose-to-video model. The figure demonstrates the\ntransformation of the sentence “Suchen Sie eine ¨Arztin auf, wenn Sie\nAusk¨unfte oder Hilfe ben¨otigen.” (‘Seek out a doctor if you need information\nor assistance.’) to a sequence of glosses, the search for relevant videos for each\ngloss, the concatenation of pose videos, and the final video output.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n147\n7.1.2\nBackground\nSign language translation can be accomplished in various ways. In this section,\nwe focus on the pipeline approach that involves text-to-gloss, gloss-to-pose,\nand, optionally, pose-to-video techniques. The text-to-gloss technique trans-\nlates spoken language text into sign language glosses, which are then converted\ninto a sequence of poses by gloss-to-pose techniques, and into a photorealistic\nvideo using pose-to-video techniques.\nThis pipeline offers the benefit of preserving the content of the sentence,\nwhile exhibiting a tendency for verbosity and a lower degree of fluency. In\nthis section, we explore each of the pipeline components comprehensively and\nexamine recent progress in sign language translation utilizing these methods.\nText-to-Gloss\nText-to-gloss, an instantiation of sign language translation, is the task of trans-\nlating between a spoken language text and sign language glosses. It is an ap-\npealing area of research because of its simplicity for integrating into existing\nNMT pipelines, despite recent works (Yin and Read, 2020a; M¨uller et al., 2023)\nclaiming that glosses are an inefficient representation of sign language, and that\nglosses are not a complete representation of signs (Pizzuto et al., 2006). Zhao\net al. (2000) used a Tree Adjoining Grammar (TAG)-based system to translate\nEnglish sentences to American Sign Language (ASL) gloss sequences. They\nparsed the English text and simultaneously assembled an ASL gloss tree, using\nSynchronous TAGs (Shieber and Schabes, 1990; Shieber, 1994), by associating\nthe ASL elementary trees with the English elementary trees and associating the\nnodes at which subsequent substitutions or adjunctions can occur. Synchronous\nTAGs have been used for translation between spoken languages (Abeill´e et al.,\n1991), but this was the first application to a signed language.\nFor the automatic translation of gloss-to-text, Othman and Jemni (2012) iden-\ntified the need for a large parallel sign language gloss and spoken language\ntext corpus. They developed a part-of-speech-based grammar to transform En-\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n148\nglish sentences from the Gutenberg Project ebooks collection (Lebert, 2008) into\nAmerican Sign Language gloss. Their final corpus contains over 100 million\nsynthetic sentences with 800 million words and is the most extensive English-\nASL gloss corpus we know of. Unfortunately, it is hard to attest to the quality\nof the corpus, as the authors did not evaluate their method on real English-ASL\ngloss pairs.\nEgea G´omez et al. (2021) presented a syntax-aware transformer for this task,\nby injecting word dependency tags to augment the embeddings inputted to\nthe encoder. This involves minor modifications in the neural architecture lead-\ning to negligible impact on computational complexity of the model. Testing\ntheir model on the RWTH-PHOENIX-Weather-2014T (Camg¨oz et al., 2018), they\ndemonstrated that injecting this additional information results in better transla-\ntion quality.\nGloss-to-Pose\nGloss-to-Pose, subsumed under the task of sign language production, is the task\nof producing a sequence of poses that adequately represent a sequence of signs\nwritten as gloss.\nTo produce a sign language video, Stoll et al. (2018) constructed a lookup ta-\nble between glosses and sequences of 2D poses. They aligned all pose sequences\nat the neck joint of a reference skeleton and grouped all sequences belonging to\nthe same gloss. Then, for each group, they applied dynamic time warping and\naveraged out all sequences in the group to construct the mean pose sequence.\nThis approach suffers from not having an accurate set of poses aligned to the\ngloss and from unnatural motion transitions between glosses.\nTo alleviate the downsides of the previous work, Stoll et al. (2020) con-\nstructed a lookup table of gloss to a group of sequences of poses rather than\ncreating a mean pose sequence. They built a Motion Graph (Min and Chai,\n2012), which is a Markov process used to generate new motion sequences that\nare representative of natural motion, and selected the motion primitives (se-\nquence of poses) per gloss with the highest transition probability. To smooth\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n149\nthat sequence and reduce unnatural motion, they used a Savitzky–Golay mo-\ntion transition smoothing filter (Savitzky and Golay, 1964). Moryossef et al.\n(2023b) re-implemented their approach and made it open-source.\nHuang et al. (2021) used a new non-autoregressive model to generate a se-\nquence of poses for a sequence of glosses. They argued that existing models like\nSaunders et al. (2020a) are prone to error accumulation and high inference la-\ntency due to their autoregressive nature. Their model performs gradual upsam-\npling of the poses, by starting with a pose including only two joints in the first\nlayer, and gradually introducing more keypoints. They evaluated their model\non the Phoenix-14T dataset (Forster et al., 2014) using Dynamic Time Warping\n(DTW) (Berndt and Clifford, 1994) to align the poses before computing Mean\nJoint Error (DTW-MJE). They demonstrated that their model outperforms exist-\ning methods in terms of accuracy and speed, making it a promising approach\nfor fast and high-quality sign language production.\nPose-to-Video\nPose-to-Video, also known as motion transfer or skeletal animation in the field\nof robotics and animation, is the conversion of a sequence of poses to a video.\nThis task is the final “rendering” of sign language in a visual modality.\nChan et al. (2019) demonstrated a semi-supervised approach where they\ntook a set of videos, ran pose estimation with OpenPose (Cao et al., 2019), and\nlearned an image-to-image translation (Isola et al., 2017) between the rendered\nskeleton and the original video. They demonstrated their approach on human\ndancing, extracting poses from a choreography and rendering any person as if\nthey were dancing. They predicted two consecutive frames for temporally co-\nherent video results and introduced a separate pipeline for a more realistic face\nsynthesis, although still flawed.\nWang et al. (2018) suggested a similar method using DensePose (G¨uler et al.,\n2018) representations in addition to the OpenPose (Cao et al., 2019) ones. They\nformalized a different model, with various objectives to optimize for, such as\nbackground-foreground separation and temporal coherence by using the previ-\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n150\nous two timestamps in the input.\nUsing the method of Chan et al. (2019) on “Everybody Dance Now”, Gir´o-i\nNieto (2020) asked, “Can Everybody Sign Now?” and investigated if people\ncould understand sign language from automatically generated videos. They\nconducted a study in which participants watched three types of videos: the\noriginal signing videos, videos showing only poses (skeletons), and reconstructed\nvideos with realistic signing. The researchers evaluated the participants’ under-\nstanding after watching each type of video. Results revealed a preference for re-\nconstructed videos over skeleton videos. However, the standard video synthe-\nsis methods used in the study were not effective enough for clear sign language\ntranslation. Participants had trouble understanding the reconstructed videos,\nsuggesting that improvements are needed for better sign language translation\nin the future.\nAs a direct response, Saunders et al. (2020b) showed that like in Chan et al.\n(2019), where an adversarial loss was added to specifically generate the face,\nadding a similar loss to the hand generation process yielded high-resolution,\nmore photo-realistic continuous sign language videos. To further improve the\nhand image synthesis quality, they introduced a keypoint-based loss function\nto avoid issues caused by motion blur.\nIn a follow-up paper, Saunders et al. (2021) introduced the task of Sign Lan-\nguage Video Anonymisation (SLVA) as an automatic method to anonymize the\nvisual appearance of a sign language video while retaining the original sign lan-\nguage content. Using a conditional variational autoencoder framework, they\nfirst extracted pose information from the source video to remove the original\nsigner appearance, then generated a photo-realistic sign language video of a\nnovel appearance from the pose sequence. The authors proposed a novel style\nloss that ensures style consistency in the anonymized sign language videos.\nSign Language Avatars\nJASigning\nis a virtual signing system that generates sign language perfor-\nmances using virtual human characters. This system evolved from the earlier\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n151\nSiGMLSigning system, which was developed during the ViSiCAST (Bangham\net al., 2000; Elliott et al., 2000) and eSIGN (Zwitserlood et al., 2004) projects, and\nlater underwent further development as part of the Dicta-Sign project (Matthes\net al., 2012; Efthimiou et al., 2012).\nOriginally, JASigning relied on Java JNLP apps for standalone use and in-\ntegration into web pages. However, this approach became outdated due to\nthe lack of support for Java in modern browsers. Consequently, the more re-\ncent CWA Signing Avatars (CWASA) system was developed, which is based on\nHTML5, utilizing JavaScript and WebGL technologies.\nSiGML (Signing Gesture Markup Language) (Elliott et al., 2004) is an XML\napplication that enables the transcription of sign language gestures. SiGML\nbuilds on HamNoSys, and indeed, one variant of SiGML is essentially an en-\ncoding of HamNoSys manual features, accompanied by a representation of\nnon-manual aspects. SiGML is the input notation used by the JASigning ap-\nplications and web applets. A number of editing tools for SiGML are available,\nmostly produced by the University of Hamburg.\nThe system parses the English text into SiGML before mapping it onto a 3D\nsigning avatar that can produce signing. CWASA then uses a large database of\npre-defined 3D signing avatar animations, which can be combined to form new\nsentences. The system includes a 3D editor, allowing users to create custom\nsigning avatars and animations.\nPAULA (Davidson, 2006)\nis a computer-based sign language avatar, ini-\ntially developed for teaching sign language to hearing adults. The avatar is a\n3D model of a person with a sign vocabulary that is manually animated. It\ntakes an ASL utterance as a stream of glosses, performs syntactic and morpho-\nlogical modifications, decides on the appropriate phonemes and timings, and\ncombines the results into a 3D animation of the avatar. Over the years, several\ntechniques were used to make the avatar look more realistic.\nOver the years, several advancements have been made to enhance the re-\nalism and expressiveness of the PAULA avatar, such as refining the eyebrow\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n152\nmotion to appear more natural (Wolfe et al., 2011), combining emotion and\nco-occurring facial nonmanual signals (Schnepp et al., 2012, 2013), improving\nsmoothness while avoiding robotic movements (McDonald et al., 2016), and\nfacilitating simultaneity (McDonald et al., 2017). Other developments include\ninterfacing with sign language notation systems like AZee (Filhol et al., 2017),\nenhancing mouthing animation (Johnson et al., 2018; Wolfe et al., 2022), multi-\nlayering facial textures and makeup (Wolfe et al., 2019), and applying adverbial\nmodifiers (Moncrief, 2020, 2021).\nAdditional improvements to PAULA focus on making the avatar more life-\nlike by relaxing wrist orientations and other extreme “mathematical” angles\n(Filhol and McDonald, 2020), refining hand shape transition, relaxation, and\ncollision (Baowidan, 2021), implementing hierarchical transitions (McDonald\nand Filhol, 2021), creating more realistic facial muscle control (McDonald et al.,\n2022), and supporting geometric relocations (Filhol and McDonald, 2022).\nSiMAX (Sign Time GmbH, 2020)\nis a software application developed to\ntransform textual input into 3D animated sign language representations. Utiliz-\ning a comprehensive database and the expertise of deaf sign language profes-\nsionals, SiMAX ensures accurate translations of both written and spoken con-\ntent. The process begins with the generation of a translation suggestion, which\nis subsequently reviewed and, if necessary, modified by deaf translators to en-\nsure accuracy and cultural appropriateness. These translations are carried out\nby a customizable digital avatar that can be adapted to reflect the corporate\nidentity or target audience of the user. This approach offers a cost-effective al-\nternative to traditional sign language video production, as it eliminates the need\nfor expensive film studios and complex video technology typically associated\nwith such productions.\nImage and Video Generation Models\nMost recently in the field of image\nand video generation, there have been notable advances in methods such as\nStyle-Based Generator Architecture for Generative Adversarial Networks (Kar-\nras et al., 2018, Karras et al. (2020),Karras et al. (2021)), Variational Diffusion\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n153\nModels (Kingma et al., 2021), High-Resolution Image Synthesis with Latent Dif-\nfusion Models (Rombach et al., 2021), High Definition Video Generation with\nDiffusion Models (Ho et al., 2022), and High-Resolution Video Synthesis with\nLatent Diffusion Models (Blattmann et al., 2023). These methods have signifi-\ncantly improved image and video synthesis quality, providing stunningly real-\nistic and visually appealing results.\nHowever, despite their remarkable progress in generating high-quality im-\nages and videos, these models trade-off computational efficiency. The complex-\nity of these algorithms often results in slower inference times, making real-time\napplications challenging. On-device deployment of these models provides ben-\nefits such as lower server costs, offline functionality, and improved user privacy.\nWhile compute-aware optimizations, specifically targeting hardware capabili-\nties of different devices, could improve the inference latency of these models,\nChen et al. (2023) found that optimizing such models on top-of-the-line mobile\ndevices such as the Samsung S23 Ultra or iPhone 14 Pro Max can decrease per-\nframe inference latency from around 23 seconds to around 12.\nControlNet (Zhang and Agrawala, 2023) recently presented a neural net-\nwork structure for controlling pretrained large diffusion models with additional\ninput conditions. This approach enables end-to-end learning of task-specific\nconditions, even with a small training dataset. Training a ControlNet is as fast\nas fine-tuning a diffusion model and can be executed on personal devices or\nscaled to large amounts of data using powerful computation clusters. Con-\ntrolNet has been demonstrated to augment large diffusion models like Stable\nDiffusion with conditional inputs such as edge maps, segmentation maps, and\nkeypoints. One of the applications of ControlNet is pose-to-image translation\ncontrol, which allows the generation of images based on pose information. Al-\nthough this method has shown promising results, it still requires retraining the\nmodel and does not inherently support temporal coherency, which is important\nfor tasks like sign language translation.\nIn the near future, we can expect many works on controlling video diffu-\nsion models directly from text for sign language translation. These models will\nlikely generate visually appealing and realistic videos. However, they may still\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n154\nmake mistakes and be limited to scenarios with more training data available.\nDeveloping models that can accurately generate sign language videos from text\nor pose information while maintaining visual quality and temporal coherency\nwill be essential for advancing the field of sign language production.\n7.1.3\nMethod\nIn this section, we provide an overview of our text-to-gloss-to-pose-to-video\npipeline, detailing the components and how they work together to convert in-\nput spoken language text into a sign language video. The pipeline consists of\nthree main components: text-to-gloss translation, gloss-to-pose conversion, and\npose-to-video animation. For text-to-gloss translation, we provide three dif-\nferent alternatives: a lemmatizer, a rule-based word reordering and dropping\ncomponent, and a neural machine translation system. Figure 7.1 illustrates the\nentire pipeline and its components.\nPipeline\nBelow, we describe the structure of our pipeline, including the text-to-gloss\ntranslation, gloss-to-pose conversion, and pose-to-video animation components:\n1. Text-to-Gloss Translation: The input (spoken language) text is first pro-\ncessed by the text-to-gloss translation component, which converts it into\na sequence of glosses.\n2. Gloss-to-Pose Conversion: The sequence of glosses generated from the\nprevious step is then used to search for relevant videos from a lexicon of\nsigned languages (e.g., DSGS, LSF-CH, LIS-CH). We extract the skeletal\nposes from the relevant videos using a state-of-the-art pre-trained pose\nestimation framework. These poses are then cropped, concatenated, and\nsmoothed, creating a pose representation for the input sentence.\n3. Pose-to-Video Generation: The processed pose video is transformed back\ninto a synthesized video using an image translation model, based on a\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n155\ncustom training of Pix2Pix.\nImplementation Details\nOur system accepts spoken language text as input and outputs an .mp4 video\nfile, or a binary .pose file, which can be handled by the pose-format library Moryossef\net al. (2021a) in Python and JavaScript. The .pose file represents the sign lan-\nguage pose sequence generated from the input text. To make our system easy\nto use, we deploy it as an HTTP endpoint that receives text as input and out-\nputs the .pose file. We provide a demonstration of our system using https:\n//sign.mt, with support for the three signed languages of Switzerland.\nWe implement our pipeline using Python and package it using Flask, a lightweight\nweb framework. This allows us to create an HTTP endpoint for our application,\nmaking it easy to integrate with other systems and web applications. Our sys-\ntem is deployed on a Google Cloud Platform (GCP) server, providing scalability\nand easy access. Furthermore, we release the source code of our implementa-\ntion as open-source software, allowing others to build upon our work and con-\ntribute to improving the accessibility of sign language translation systems.\nBy implementing our system as an open-source Python application and de-\nploying it as an HTTP endpoint, we aim to facilitate collaboration and improve-\nments to sign language translation systems.\n7.1.4\nText-to-Gloss\nWe explore three different components as part of text-to-gloss translation, in-\ncluding a lemmatizer, a rule-based word reordering and dropping component,\nand a neural machine translation (NMT) system.\nLemmatizer\nWe use the Simplemma simple multilingual lemmatizer for Python (Barbaresi,\n2023) to reduce words to their base form (i.e., lemma), which is useful for our\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n156\ncase, as it helps to preserve meaning while reducing the complexity of the input.\nThis approach is limited by the use of the simplistic context-free lemmatizer,\nsince no sense information is captured in the lemma, which causes ambiguity.\nWord Reordering and Dropping\nWe generate near-glosses for sign language from spoken language text using a\nrule-based approach. The process from converting spoken language sentences\ninto sign language gloss sequences can be naively summarized by a removal\nof word inflection, an omission of punctuation and specific words, and word\nreordering. To address these differences, we adopt the rule-based approach\nfrom Moryossef et al. (2021c) to generate near-glosses from spoken language:\nlemmatization of spoken words, PoS-dependent word deletion, and word order\npermutation. With their permission, we re-share these rules:\nSpecifically, we use spaCy (Montani et al., 2023) for lemmatization, PoS tag-\nging, and dependency parsing. Unlike Simplelemma, the spaCy lemmatizer is\nlanguage-specific and context-based. We drop words that are not content words\n(e.g., articles, prepositions), as they are largely unused in signed languages, but\nkeep possessive and personal pronouns as well as nouns, verbs, adjectives, ad-\nverbs, and numerals. We devise a short list of syntax transformation rules based\non the grammar of the sign language and the corresponding spoken language.\nWe identify the subject, verb, and object in the input text, and reorder them\nto match the order used in the signed language. For example, for German-to-\nGerman Sign Language (Deutsche Geb¨ardensprache, DGS), we reorder SVO sen-\ntences to SOV, move verb-modifying adverbs and location words to the start of\nthe sentence (a form of topicalization), and move negation words to the end.\nThe specific rules we use for German to DGS/DSGS are:\n1. For each subject-verb-object triplet (s, v, o) ∈S, swap the positions of v\nand o in S\n2. Keep all tokens t ∈S if PoS(t) ∈{noun, verb, adjective, adverb, numeral,\npronoun}\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n157\n3. If PoS(t) = adverb and HEAD(t) = verb, move t to the start of S\n4. If NER(t) = location, move t to the start of S\n5. If DEP(t) = negation, move t to the end of S\n6. Lemmatize all tokens t ∈S\nWe first split each sentence into separate clauses and reorder them before\nwe apply these rules to each clause. Reordering the clauses may be needed for\nconditional sentences where the conditional subordinate clause should precede\nthe main clause, as in “if...then...”. These rules allow us to transform spo-\nken language text into near-glosses that more closely match the word order and\nstructure of sign language. Overall, our rule-based approach provides a flexible\nand effective way to generate near-glosses for sign language from spoken lan-\nguage text, with the ability to incorporate language-specific rules to capture the\nnuances of different sign languages. This approach employs a more accurate\nlemmatizer, however, it still suffers from word sense ambiguity.\nNeural Machine Translation\nAs an alternative to rule-based transformations of text to glosses, we train a\nneural machine translation (NMT) system.\nData\nWe use the Public DGS Corpus, a publicly available corpus of German\nSign Language videos with annotated glosses (Hanke et al., 2020). We hold out\na random sample of 1k training examples each for development and testing\npurposes. Table 7.1 overviews the number of sentence pairs in all splits.\nWe download and process release 3.0 of the corpus. To DGS glosses we\napply the following modifications derived from the DGS Corpus transcription\nconventions (Konrad et al., 2022):\n• Removing entirely two specific gloss types that cannot possibly help the\ntranslation: $GEST-OFF and $$EXTRA-LING-MAN.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n158\nPartition\nAvailable Languages\nEN\nDGS·DE\nDGS·EN\nDE\nTrain\n61912\n61912\n61912\n61912\nDev\n1000\n1000\n1000\n1000\nTest\n1000\n1000\n1000\n1000\nTotal\n63912\n63912\n63912\n63912\nTable 7.1: Number of sentence pairs used for gloss models.\nDGS·DE=original gloss transcriptions,\nDGS·EN=DGS glosses translated to English.\n• Removing ad-hoc deviations from citation forms, marked by *. Example:\nANDERS1* →ANDERS1.\n• Removing the distinction between type glosses and subtype glosses, marked\nby ˆ. Example: WISSEN2Bˆ →WISSEN2B.\n• Collapsing phonological variations of the same type that are meaning-\nequivalent. Such variants are marked with uppercase letter suffixes. Ex-\nample: WISSEN2B →WISSEN2.\n• Deliberately keep numerals ($NUM), list glosses ($LIST) and finger alpha-\nbet ($ALPHA) intact, except for removing handshape variants.\nSee Table 7.2 for examples for this preprocessing step. Overall these sim-\nplifications should reduce the number of observed forms while not affecting\nthe machine translation task. For other purposes such as linguistic analysis our\npreprocessing would of course be detrimental.\nPreprocessing\nOur preprocessing and model settings are inspired by OPUS-\nMT (Tiedemann and Thottingal, 2020). The only preprocessing step that we\napply to all data is Sentencepiece segmentation (Kudo, 2018). We learn a shared\nvocabulary with a desired total size of 1k pieces.\nWe additionally preprocess DGS glosses in a corpus-specific way, informed\nby the DGS Corpus glossing conventions (Konrad et al., 2022). See Table 7.2 for\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n159\nBefore\n$INDEX1 ENDE1ˆ ANDERS1* SEHEN1 M¨UNCHEN1B* BEREICH1A*\nAfter\n$INDEX1 ENDE1 ANDERS1 SEHEN1 M¨UNCHEN1 BEREICH1\nBefore\nICH1 ETWAS-PLANEN-UND-UMSETZEN1 SELBST1A* KLAPPT1*\n$GEST-OFFˆ BIS-JETZT1 GEWOHNHEIT1* $GEST-OFFˆ*\nAfter\nICH1 ETWAS-PLANEN-UND-UMSETZEN1 SELBST1 KLAPPT1\nBIS-JETZT1 GEWOHNHEIT1\nTable 7.2: Examples for preprocessing of DGS glosses.\nexamples for this preprocessing step. Overall the desired effect is to reduce the\nnumber of observed forms while not altering the meaning itself.\nCore model settings\nWe train NMT models with Sockeye 3 (Hieber et al.,\n2022). The models are standard Transformer models (Vaswani et al., 2017), ex-\ncept with some hyperparameters modified for a low-resource scenario. E.g.,\ndropout rate is set to a high value of 0.5 for all dropout layers of the model\n(Sennrich and Zhang, 2019).\nThe NMT system itself is trained with three-way weight tying between the\nsource embeddings, target embeddings matrix and softmax output (Press and\nWolf, 2017).\nWe train a multilingual model, following the methodology described in John-\nson et al. (2017) which inserts special tokens into all source sentences to indicate\nthe desired target language. For comparison, we also train bilingual systems\nthat can translate in only one direction each. Our automatic evaluation confirms\nthat one multilingual system leads to higher translation quality than individual\nbilingual systems.\nWe perform an automatic evaluation of translation quality.\nWe measure\ntranslation quality with BLEU (Papineni et al., 2002) and CHRF (Popovi´c, 2016a),\ncomputed with the tool SacreBLEU (Post, 2018). See Table 7.3 for all SacreBLEU\nsignatures.\nWhenever gloss output is evaluated we disable BLEU’s internal tokeniza-\ntion, as advocated by M¨uller et al. (2023). Earlier works did not consider this\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n160\nBLEU with internal tokenization\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.14\nBLEU without internal tokenization\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.none+version.1.4.14\nCHRF\nchrF2+numchars.6+space.false+version.1.4.14\nTable 7.3: SacreBLEU signatures for evaluation metrics.\nDGS→DE\nDE→DGS\nBilingual\n28.610\n-\nBilingual\n-\n32.920\nMultilingual: all DE and DGS directions\n28.210\n34.760\nTable 7.4: CHRF scores of the multilingual translation system compared to\nbilingual systems.\ndetail and therefore our BLEU scores may appear low in comparison.\nFinally, because DGS glosses are preprocessed in a corpus-specific way (see\nabove), they are evaluated against a preprocessed reference as well, since this\nprocess cannot be reversed after translation. This means that corpus-specific\npreprocessing for DGS glosses simplifies the translation task overall, compared\nto a system that predicts glosses in their original forms.\nTable 7.4 reports the translation quality of our machine translation systems,\nas measured by CHRF. The table shows that one multilingual system that can\ntranslate between DGS and German leads to higher translation quality than two\nbilingual systems.\nLanguage Dependent Implementation\nIn this paper, we study three sign languages: LIS-CH, LSF-CH and DSGS. For\nLIS-CH and LSF-CH we always apply our simple lemmatizer for the text-to-\ngloss step. The lemmatizer-only component is universally applicable to many\nmore languages. However, it is worth noting that this approach does not cap-\nture the full spectrum of syntactic and morphological changes necessary in go-\ning from a spoken to a signed language, which leads to suboptimal translations.\nFor DSGS, we explored different options for text-to-gloss, comparing the\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n161\nlemmatizer, rule-based system and NMT system. We observed that the glosses\noutput by the NMT system are less accurate than rule-based reordering. A po-\ntential explanation for this is that the system is trained on German Sign Lan-\nguage (DGS) data. Due to the inherent differences between DGS and DSGS,\nusing the NMT system could result in inaccurate translations or out-of-lexicon\nglosses. Furthermore, we found that the NMT system is not robust to out-of-\ndomain text or capitalization differences, which further limits its applicability\nin these scenarios.\nIn the end, for DSGS we opted to employ our rule-based system (§7.1.4),\nwhich has been tailored to accommodate the unique linguistic characteristics of\nDSGS, and produces the best results.\n7.1.5\nGloss-to-Pose\nGloss-to-pose translation involves converting sign language glosses into a se-\nquence of poses that adequately represent a sequence of signs.\nWe use the SignSuisse dataset (Schweizerischer Geh¨orlosenbund SGB-FSS,\n2023), which consists of sign language videos in three different languages. We\nextract skeletal poses from these videos using Mediapipe Holistic (Grishchenko\nand Bazarevsky, 2020), a state-of-the-art pose estimation framework that es-\ntimates 3D coordinates of various landmarks on the human body, including\nthe face, hands, and body. We preprocess the poses by ensuring that the body\nwrists are in the same location as the hand wrists, removing the legs, hands,\nand face from the body pose, and cropping the videos in the beginning and end\nto avoid returning to a neutral body position.\nWe concatenate the poses for each gloss by finding the best ‘stitching’ point\nthat minimizes L2 distance. We then concatenate these poses, adding 0.2 sec-\nonds of ‘padding’ in between, before applying cubic smoothing on each joint\nto ensure smooth transitions between signs, and filling in missing keypoints.\nFinally, we apply a Savitzky-Golay motion transition smoothing filter (Savitzky\nand Golay, 1964), similar to Stoll et al. (2020), to reduce unnatural motion.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n162\n7.1.6\nPose-to-Video\nWe use a semi-realistic human-like avatar system to animate the poses gener-\nated by our approach. The avatar system is a Pix2Pix model (Isola et al., 2017)\nadjusted to operate on pose sequences, not individual images. With her permis-\nsion, we use the likeness of Maayan Gazuli1. We use OpenCV (Bradski, 2000)\nto render the poses as images and feed them into the Pix2Pix model to generate\nrealistic-looking video frames. The avatar system can run in real-time on sup-\nported devices and is integrated into https://sign.mt (Moryossef, 2023c).\nThis system is far from the state of the art, however, we believe that the open-\nsource nature of it will bring rapid improvements, like faster inference speed,\nand higher animation quality.\n7.1.7\nFuture Work\nHere we include several future work directions that we believe have the po-\ntential to further enhance the performance and user experience of our system\nfor text-to-gloss-to-pose-to-video generation, and we look forward to exploring\nthese possibilities in the future, together with the open-source community.\n7.1.8\nQualitative Evaluation\nTo evaluate the effectiveness of our approach, we will conduct a study to gather\nfirst impressions from deaf users. We already recruited a group of deaf individ-\nuals and will ask them to use our system to translate text into sign language.\nEach participant will be asked to provide feedback on the system after using\nit to translate five different sentences from German into DSGS. We will provide\nthe sentences to the participants, and they will be asked to sign the translations\ngenerated by our system. After each sentence, the participant will be asked to\nprovide feedback on the accuracy of the translation, the quality of the poses\nand/or synthesized video, and the overall usability of the system.\n1https://nlp.biu.ac.il/˜amit/datasets/GreenScreen/\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n163\nGloss Sense Disambiguation\nThe current approach to text-to-gloss translation relies on a simple lemmatizer\nand a rule-based word reordering and dropping component, which can lead to\nambiguity in the glosses produced. In the future, we can enhance our system by\nincorporating gloss sense disambiguation to better capture the intended mean-\ning of the input text. Our NMT approach responds with gloss IDs from the\nMeineDGS corpus, which already are sense-disambiguated. Annotation of our\nsign language lexicon with senses will allow us to retrieve the relevant sense.\nHandling Unknown Glosses\nWhere we encounter a gloss that does not exist in our lexicon, we propose ex-\nploring alternative methods to generate a video for it. One possible solution is\nto leverage another lexicon that includes a written representation of the gloss\nin question (e.g., SignWriting Sutton (1990) or HamNoSys Prillwitz and Zienert\n(1990)), or to employ a neural machine translation system to translate the indi-\nvidual concept to a writing system. Utilizing the capabilities of machine trans-\nlation to embed words, we can perform a fuzzy match, addressing issues such\nas synonyms.\nAdditionally, for named entities such as proper nouns and place names that\nare not covered by our current gloss-to-pose conversion system, we could revert\nto fingerspelling them.\nOnce we have the written representation, we can use a system like Ham2Pose\nArkushin et al. (2023) to generate a single sign video from the writing. When\ncombined with fingerspelling for named entities, this approach should enable\ngreater coverage of the language.\nHandling Unknown Gloss Variations\nIn situations where the required gloss variation is not present in the lexicon\nbut a related gloss exists, we propose developing a system that can modify the\nknown gloss to generate the desired variation. This would allow for better han-\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n164\ndling of unknown gloss variations and increase the accuracy of the information\nconveyed by the signing.\nNumber Forms\nFor words like KINDER (children), we may encounter glosses\nsuch as KIND+, which represent “child” in plural form. Assuming that we have\nKIND in our lexicon but not KINDER, a system could be developed to modify\nsigns to plural forms, such as by repeating movements or incorporating spe-\ncific handshapes or locations that indicate plurality in the target sign language.\nConversely, if we only have the plural form of a gloss in our lexicon, the system\ncould be designed to generate the singular form by removing or modifying the\nelements that indicate plurality.\nPart of Speech Conversion\nAnother challenge arises when nouns or verbs ex-\nist in the lexicon, but their counterparts do not. For instance, if HELFEN (to\nhelp) is present in the dictionary as a verb, but HILFE (help) does not exist as\na noun, a system could be designed to modify signs from one part of speech\nto another, such as from verb to noun or noun to verb. This system could po-\ntentially involve morphological or movement modifications, depending on the\nlinguistic rules of the target sign language.\nPost-editing Pose Sequences\nThe current approach generates a sequence of poses that represent a sign lan-\nguage sentence. We believe that there is also room for improvement in terms\nof the fluency and naturalness of the generated sequence. Exploring the use of\nautomatic post-editing techniques is necessary. One such approach could iden-\ntify datasets that include sentences and gloss sequences, such as the Public DGS\nCorpus, then, using our gloss-to-pose approach generate a pose sequence with\nposes from the lexicon, and could learn a diffusion model between the synthetic\nand real pose sequences.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n165\n7.1.9\nConclusions\nWe presented an implementation of a text-to-gloss-to-pose-to-video pipeline for\nsign language translation, focusing on Swiss German Sign Language, Swiss\nFrench Sign Language, and Swiss Italian Sign Language. Our approach com-\nprises three main components: text-to-gloss translation, gloss-to-pose conver-\nsion, and pose-to-video animation.\nWe explained the structure of our system and discussed its limitations, as\nwell as future work directions to address them. These directions have the po-\ntential to improve our system, and we look forward to exploring them in col-\nlaboration with the open-source community.\nThe main contribution of this paper is the creation of a reproducible baseline\nfor spoken to signed language translation. The system should serve as a base-\nline for comparison with more sophisticated sign language translation systems\nand can be improved upon by the community. You can try our system for the\nthree signed languages of Switzerland on https://sign.mt.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n166\n7.2\nTranslation (Jiang et al., 2023a)\nOur sign language production pipeline relies critically on the translation from\nspoken language text to SignWriting. Further technical insights into our bi-\ndirectional translation models are covered in Jiang et al. (2023a) and additional\nbackground can be found in Section 6.3.\nWhile we delve into the analysis and comparison of models for translat-\ning from SignWriting to spoken language in Section 6.3, we abstain from such\nscrutiny in the opposite direction. The primary constraint here is the absence\nof established metrics to assess the quality of SignWriting translations, which\ncomplicates and raises the cost of performance evaluation.\nWe encourage researchers in the sign language domain to develop suitable\nevaluation frameworks for this issue. The ability to automatically gauge the\nsimilarity between a reference and a hypothesis in SignWriting is a precondition\nfor furthering research in this field.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n167\n7.3\nProduction (Arkushin et al., 2023)\nIn the course of this thesis, I had the honor of overseeing the research detailed in\nArkushin et al. (2023). Together, we developed a language-agnostic framework\nfor sign language production, leveraging written phonetic representations of\nvarious signed languages. This section serves as a brief overview of the exist-\ning landscape of sign language production. It aims not only to contextualize\nthe current state of the field but also to highlight the innovative impact of our\ncontributions within this larger framework.\n7.3.1\nSigned Text to Pose\nArkushin et al. (2023) proposed Ham2Pose, a model to animate HamNoSys into\na sequence of poses. They first encode the HamNoSys into a meaningful “con-\ntext” representation using a transform encoder, and use it to predict the length\nof the pose sequence to be generated. Then, starting from a still frame they\nused an iterative non-autoregressive decoder to gradually refine the sign over\nT steps, In each time step t from T to 1, the model predicts the required change\nfrom step t to step t −1. After T steps, the pose generator outputs the final pose\nsequence. Their model outperformed previous methods like Saunders et al.\n(2020c), generating more realistic sign language sequences.\n7.3.2\nSpoken Text to Pose\nText-to-Pose, also known as sign language production, is the task of producing a\nsequence of poses that adequately represent a spoken language text in sign lan-\nguage, as an intermediate representation to overcome challenges in animation.\nMost efforts use poses as an intermediate representation to overcome the chal-\nlenges in generating videos directly, with the goal of using computer animation\nor pose-to-video models to perform video production.\nSaunders et al. (2020c) proposed Progressive Transformers, a model to trans-\nlate from discrete spoken language sentences to continuous 3D sign pose se-\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n168\nquences in an autoregressive manner. Unlike symbolic transformers (Vaswani\net al., 2017), which use a discrete vocabulary and thus can predict an end-\nof-sequence (EOS) token in every step, the progressive transformer predicts a\ncounter ∈[0, 1] in addition to the pose. In inference time, counter = 1 is con-\nsidered the end of the sequence. They tested their approach on the RWTH-\nPHOENIX-Weather-2014T dataset using OpenPose 2D pose estimation, uplifted\nto 3D (Zelinka and Kanis, 2020), and showed favorable results when evaluat-\ning using back-translation from the generated poses to spoken language. They\nfurther showed (Saunders et al., 2020a) that using an adversarial discriminator\nbetween the ground truth poses and the generated poses, conditioned on the in-\nput spoken language text, improves the production quality as measured using\nback-translation.\nTo overcome the issues of under-articulation seen in the above works, Saun-\nders et al. (2020b) expanded on the progressive transformer model using a Mix-\nture Density Network (MDN) (Bishop, 1994) to model the variation found in\nsign language. While this model underperformed on the validation set, com-\npared to previous work, it outperformed on the test set.\nZelinka and Kanis (2020) presented a similar autoregressive decoder ap-\nproach, with added dynamic-time-warping (DTW) and soft attention. They\ntested their approach on Czech Sign Language weather data extracted from the\nnews, which is not manually annotated, or aligned to the spoken language cap-\ntions, and showed their DTW is advantageous for this kind of task.\nXiao et al. (2020) closed the loop by proposing a text-to-pose-to-text model\nfor the case of isolated sign language recognition. They first trained a classifier\nto take a sequence of poses encoded by a BiLSTM and classify the relevant sign,\nthen proposed a production system to take a single sign and sample a constant\nlength sequence of 50 poses from a Gaussian Mixture Model. These compo-\nnents are combined such that given a sign class y, a pose sequence is generated,\nthen classified back into a sign class ˆy, and the loss is applied between y and\nˆy, and not directly on the generated pose sequence. They evaluate their ap-\nproach on the CSL dataset (Huang et al., 2018) and show that their generated\npose sequences almost reach the same classification performance as the refer-\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n169\nence sequences.\nDue to the need for more suitable automatic evaluation methods for gener-\nated signs, existing works resort to measuring back-translation quality, which\ncannot accurately capture the quality of the produced signs nor their usability\nin real-world settings. Understanding how distinctions in meaning are created\nin signed language may help develop a better evaluation method.\nCHAPTER 7. SIGN LANGUAGE PRODUCTION\n170\n7.4\nAnimation\nI consider the task of animating/rendering poses into 3D avatars or photore-\nalistic videos out-of-scope for this thesis for brevity. Section 7.1 covers a sim-\nple approach for the task, as well as the background for this step (§7.1.2). I\nurge computer vision researchers to improve on this baseline method, develop\nstronger on-device animation models, and release them publicly.\nPart III\nDiscussion and Implications\n171\n172\n“if I have seen further it is by standing on the shoulders of giants.”\n— Isaac Newton\nChapter 8\nSign Language Translation\nApplication (Moryossef, 2023c)\nThis chapter presents sign.mt, an open-source application pioneering real-time\nmultilingual bi-directional translation between spoken and signed languages.\nHarnessing state-of-the-art open-source models, this tool aims to address the\ncommunication divide between the hearing and the deaf, facilitating seamless\ntranslation in both spoken-to-signed and signed-to-spoken directions.\nPromising reliable and unrestricted communication, sign.mt offers offline\nfunctionality, crucial in areas with limited internet connectivity. It further en-\nhances user engagement by offering customizable photo-realistic avatars, thereby\nencouraging a more personalized and authentic user experience.\nLicensed under CC BY-NC-SA 4.0, sign.mt signifies an important stride to-\nwards open, inclusive communication. The app can be used, and modified for\npersonal and academic uses. It features a translation API, fostering integration\ninto a range of applications. However, it is by no means a finished product.\nWe invite the NLP community to contribute towards the evolution of sign.mt.\nWhether it be the integration of more refined models, the development of in-\nnovative pipelines, or user experience improvements. Available at https:\n//sign.mt, it stands as a testament to what we can achieve together, as we\nstrive to make communication accessible to all.\n173\nCHAPTER 8. Sign Language Translation Application\n174\n8.1\nMotivation\nSign language translation applications are crucial tools for enabling commu-\nnication between individuals who are deaf or hard of hearing and those who\ncommunicate through spoken language. However, the complexity of devel-\noping sign language translation applications goes beyond handling mere text.\nThese applications must be able to process and generate videos, demanding ad-\nditional considerations like compute capabilities, accessibility, usability, work-\ning with large files, and platform support.\nsign.mt, standing for Sign Language Machine Translation, was conceived\nas a response to these challenges. Current research in the field of sign lan-\nguage translation is fragmented and somewhat nebulous, with different re-\nsearch groups focusing on various aspects of the translation pipeline or on spe-\ncific languages. Moreover, the high costs associated with server-side deploy-\nment and the complexity of client-side implementations often deter the devel-\nopment of interactive demonstrations for newly proposed models.\nBy providing a comprehensive application infrastructure that integrates the\nessential features around the translation process, sign.mt serves as a dynamic\nproof-of-concept. It aims to streamline the integration of new research find-\nings into the application, sidestepping the overhead typically associated with\nimplementing a full-stack application. When a research group develops a new\nmodel or improves a pipeline, they can integrate their advancements into the\napp swiftly, focusing only on their model. This approach allows researchers to\ndeploy the app in a branch, testing their models in a practical environment. If\nthe license allows and the models show an improvement, they can contribute\ntheir models to the main codebase. This is the first tool of its kind, diverging\nsignificantly from closed-source commercial applications.\nFurther, sign.mt serves as a multilingual platform, thus unifying the frag-\nmented research landscape. It enables the concurrent running of models from\ndifferent research groups for the supported languages, providing users with\nstate-of-the-art translation capabilities for each language. Through this, sign.mt\nnot only enhances accessibility and communication but also fuels continuous\nCHAPTER 8. Sign Language Translation Application\n175\ninnovation in sign language translation research.\n8.2\nImplementation\nSign language translation presents unique challenges that set it apart from text-\nbased translation.\nWhile text-based translation operates entirely within the\ntextual domain for both input and output, sign language translation involves\ncross-modal transformation – from text to video and vice versa. This demands\ndistinct implementations not only in functionality but also in the user interface.\nIt is essential to emphasize that the specific models utilized within various\npipelines are deliberately modular and interchangeable. Our current choice of\nmodels for each module or task is primarily opportunistic, driven by availabil-\nity rather than performance metrics or user evaluations. The app serves as a dy-\nnamic orchestrator, seamlessly coordinating among these models to deliver an\nintegrated user experience. The platform’s design accommodates the likelihood\nthat researchers may wish to experiment with different models or fine-tune ex-\nisting pipelines, without being constrained by rigid implementation details.\n8.2.1\nSpoken-to-Signed Translation\nFor spoken-to-signed translation, the process begins with an input of spoken\nlanguage text. Optionally, we allow audio input, which is first transcribed into\nspoken language text using on-device Speech-to-Text (STT) technology.\nWhen the input language is unknown, this textual input undergoes Spo-\nken Language Identification (using cld3 (Salcianu et al., 2016)), which detects\nthe language of the provided text. This is crucial for choosing the appropriate\nmodel for subsequent translation steps. Simultaneously, the text is optionally\nnormalized (using ChatGPT (OpenAI, 2022)) - this includes fixing capitaliza-\ntion, punctuation, grammatical errors, or misspellings, which we have found to\nenhance the performance of subsequent translation stages.\nThe language-identified and potentially normalized text is then translated\nCHAPTER 8. Sign Language Translation Application\n176\ninto SignWriting (Sutton, 1990). Here, our system leverages real-time client-\nside machine translation (Bogoychev et al., 2021) to translate the grammatical\nstructures and lexicon of spoken languages into the visual-gestural modality of\nsign languages (Jiang et al., 2023a; Moryossef and Jiang, 2023).\nThe SignWriting output is then converted into a pose sequence (Inspired\nby Arkushin et al. (2023)), an ordered set of human poses that represent the\nsigned sentence. This pose sequence is the input for the rendering engine, with\nthree options: Skeleton Viewer (Minimalistic visualization of the skeletal pose\n(Moryossef et al., 2021a)) Human GAN (Pix2Pix (Isola et al., 2017; Shi et al.,\n2016) image-to-image model, generating a realistic human avatar video), and a\n3D Avatar (Neural model to translate between pose positions and rigged rota-\ntions, generating a stylized 3D character performing the signs).\nThese different outputs provide users with a choice on how they prefer to\nview the translation, catering to a broad range of preferences and use cases.\nThe skeleton viewer is useful for developers to see the raw output, as well as\nfor low-compute users. The 3D Avatar is useful in mixed reality applications,\nwhere it can be integrated in the environment, and the Human GAN is useful\nfor high-compute users, facilitating a natural interaction.\nThrough this pipeline (Figure 8.1), sign.mt is capable of real-time translation\nfrom spoken language audio (or text) into sign language video, further democ-\nratizing communication across modalities.\nCurrently, while we don’t have a fully functional SignWriting to pose anima-\ntion model, we have created a baseline model as an interim solution (Moryossef\net al., 2023b). This model performs dictionary-based translation from the spo-\nken language text directly to poses, bypassing the SignWriting stage. However,\nit’s important to note that there are numerous common cases in sign languages\nthat this baseline model cannot handle adequately yet. We have made the base-\nline model open-source, and it is available for further improvements and con-\ntributions from the community at https://github.com/ZurichNLP/spo\nken-to-signed-translation. We hope that this approach will stimulate\nfurther research and development in this area, allowing for the integration of\nmore sophisticated and accurate models in future iterations of the application.\nCHAPTER 8. Sign Language Translation Application\n177\nFigure 8.1: The Spoken-to-Signed translation pipeline.\n8.2.2\nSigned-to-Spoken Translation\nFor signed-to-spoken translation, the source is a video (either by the user up-\nloading a pre-existing sign language video or using the camera to record a live\nsign language video). Our current pipeline takes the video, and using Medi-\napipe Holistic (Grishchenko and Bazarevsky, 2020) pose estimation extracts the\nfull body pose from each frame.\nThis pose information is then fed into a Segmentation module (Moryossef\net al., 2023a), which segments distinct signs within the continuous signing flow,\nas well as phrase boundaries. The segmented signs are subsequently lexically\ntranscribed using SignWriting (Sutton, 1990), a comprehensive system for tran-\nscribing sign languages visually.\nThis SignWriting transcription serves as the textual input for the translation\nmodel, which translates it into corresponding spoken language text (Jiang et al.,\nCHAPTER 8. Sign Language Translation Application\n178\n2023a; Moryossef and Jiang, 2023). This text is then optionally converted into\nspoken language audio using on-device Text-to-Speech (TTS), providing an au-\nditory output for the user.\nFigure 8.2: The Signed-to-Spoken translation pipeline.\nThrough this pipeline (Figure 8.2), sign.mt can take a sign language video\nand output corresponding spoken language text or audio in real-time. The of-\nfline functionality of the app ensures that this feature remains accessible even in\nareas with limited connectivity, provided that the models were pre-loaded on\nthe device.\nCHAPTER 8. Sign Language Translation Application\n179\n8.3\nUser Engagement\nThe impact of sign.mt can be measured by its widespread and consistent usage,\nhighlighting the tremendous growth potential as the app continues to slowly\nimprove.\nFigure 8.3: Distribution of sign.mt users across the world, over the last year.\nFigure 8.3 depicts the global adoption of sign.mt, with users distributed across\nmultiple countries. None of these top user countries are home to the core de-\nveloper of the app.\nFigure 8.4: Growth of sign.mt users over the last year.\nCHAPTER 8. Sign Language Translation Application\n180\nAs shown in Figure 8.4, sign.mt demonstrates slow but consistent user growth\n(by Google Analytics), indicative of its reliability and sustained relevance.\nFigure 8.5: Number of stars for the repository over time.\nFurther validation of the community interest in sign.mt is evidenced by the\nincreasing number of stars for its repository, reaching 151 stars as of August 1st,\n2023 (Figure 8.5).\nPublic interest in sign.mt is further supported by Google Search Console met-\nrics (Figure 8.6), showing a significant increase in impressions and clicks over\nthe past six months: 3.75K clicks (up from 1.56K), and 106K impressions (up\nfrom 24.4K). Despite the absence of a marketing team and a single maintainer,\nsign.mt has managed to carve a niche for itself in the realm of NLP tools, reiter-\nating its significance and impact.\n8.4\nDistribution\nThe code for sign.mt is openly accessible and available for contribution on GitHub\nat https://github.com/sign/translate, under CC BY-NC-SA 4.0.\nOpen sourcing with a permissive license encourages the continuous refinement\nCHAPTER 8. Sign Language Translation Application\n181\nFigure 8.6: Google Search Console metrics showing increasing interest in\nsign.mt. (Clicks in blue)\nand enhancement of the app through contributions from the wider developer\nand research communities.\nThe web application is freely accessible at https://sign.mt, designed\nwith a responsive layout to cater to both desktop and mobile devices. Adhering\nto the design principles native to each platform, the application ensures an in-\ntuitive and user-friendly experience across all devices. With localization being\na critical aspect of accessibility, the app interface supports 104 languages. Con-\ntributors can add their language or enhance the support for existing languages.\nIn addition to the web application, native builds for iOS and Android de-\nvices are also provided through the GitHub repository. While these are cur-\nrently in development, the plan is to make them available to download on\nthe respective app stores as they reach stability, thereby extending the reach\nof sign.mt to a wider audience.\nLimitations\nAs an evolving open-source project, sign.mt still faces several challenges and\nlimitations. At present, the app does not provide complete support for every\ncomponent of the translation pipeline. Notably, the SignWriting-to-pose ani-\nCHAPTER 8. Sign Language Translation Application\n182\nmation model does not currently exist, and instead, we use a simple dictionary\nlookup approach (Moryossef et al., 2023b). Although it serves as an interim so-\nlution, it is insufficient for handling signed languages. We eagerly anticipate\nand encourage contributions from the research community to fill this gap with\nmore advanced models.\nAlthough the app aspires to be a multilingual platform, the availability of\nmodels for different languages is currently fragmented. We rely on the research\ncommunity to develop and contribute models for different languages.\nThe\nsupport for each language, therefore, depends on the respective models avail-\nable, leading to varying degrees of effectiveness across languages. For example,\nthe SignWriting translation module works reasonably well for English/Amer-\nican Sign Language, German/German Sign Language and Portuguese/Brazil-\nian Sign Language translations, and much worse for all other language pairs.\nAnother example is the dictionary-based baseline only working on languages\nwhere dictionaries are available.\nDue to the client-side deployment, we are restricted to using relatively smaller\nmodels. This inevitably leads to trade-offs in terms of translation accuracy and\nquality. While the offline functionality ensures accessibility in low connectivity\nareas, the constraint on model size is challenging.\nThe video processing components, including pose estimation and video ren-\ndering, are computationally intensive. This demands significant computational\npower, limiting the app’s performance on devices with lesser computing capa-\nbilities. Optimizing these components further to ensure a smoother user ex-\nperience across a wider range of devices is a challenge, often met with using\nlower-end models to achieve smoothness at the cost of accuracy.\nDespite these limitations, sign.mt serves as a robust foundation upon which\nfuture advancements can be built. It continues to evolve in response to the\nfeedback of the wider community, consistently striving towards the goal of fa-\ncilitating accessible, inclusive communication.\nChapter 9\nImplications for Spoken Languages\n(Moryossef, 2023b)\nThis chapter explores the critical but often overlooked role of non-verbal cues,\nincluding co-speech gestures and facial expressions, in human communication\nand their implications for Natural Language Processing (NLP). We argue that\nunderstanding human communication requires a more holistic approach that\ngoes beyond textual or spoken words to include non-verbal elements. Borrow-\ning from advances in sign language processing, we propose the development\nof universal automatic gesture segmentation and transcription models to tran-\nscribe these non-verbal cues into textual form. Such a methodology aims to\nbridge the blind spots in spoken language understanding, enhancing the scope\nand applicability of NLP models. Through motivating examples, we demon-\nstrate the limitations of relying solely on text-based models. We propose a com-\nputationally efficient and flexible approach for incorporating non-verbal cues,\nwhich can seamlessly integrate with existing NLP pipelines. We conclude by\ncalling upon the research community to contribute to the development of uni-\nversal transcription methods and to validate their effectiveness in capturing the\ncomplexities of real-world, multi-modal interactions.\n183\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n184\n9.1\nIntroduction\nHuman speech is typically accompanied by a dynamic combination of co-speech\ngestures and facial expressions, together forming an integral part of human\ncommunication. These non-verbal cues, far from being random or merely acces-\nsory, provide additional layers of meaning, clarify intention, emphasize points,\nregulate conversation flow, and facilitate emotional connection. They enrich\nour interactions and help convey complex or nuanced information that words\nalone might not capture.\nCo-speech gestures refer to the hand and body movements accompanying\nspoken discourse; they supplement verbal communication by offering addi-\ntional information, such as object size or shape; they emphasize and make\nabstract concepts tangible, like gesturing upwards to signify an increase; they\ncontrol the conversation flow, signaling a speaker’s intent, inviting listener in-\nteraction, or showing that the speaker is in thought or pause; and lastly, they\ncompensate for the limitations of spoken language, especially in high-stakes or\nnoisy environments, by providing an alternative mode of conveying complex\nor nuanced information.\nFacial expressions during speech significantly contribute to communication\nby indicating the speaker’s emotions, and providing insight into their feelings\nabout the topic; they can emphasize certain aspects of the discourse, with ac-\ntions like raised eyebrows signifying surprise or importance; they offer social\ncues, with expressions like a smile suggesting friendliness or a serious look in-\ndicating sincerity; they help clarify verbal meaning, especially in ambiguous\nsituations, for example, a confused expression might denote misunderstand-\ning; finally, they enhance interpersonal connection by helping to build rapport,\nexpressing empathy, and conveying cues of understanding and engagement; al-\ntogether, facial expressions, like gestures, add complexity and depth to verbal\ncommunications.\nThe field of Natural Language Processing (NLP) has become highly effective\nin understanding language directly from text. However, understanding speech,\nwith its imperfect and noisy signals, remains a more complex challenge. Text-\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n185\nbased language models have proven highly scalable, thanks largely to the com-\npressible nature of text and its abundant availability in semi-anonymous forms.\nYet, these models fundamentally ignore the rich layers of meaning added by\nnon-verbal cues, a significant aspect of human communication. This means\nthat while we have become adept at parsing text, we are missing out on the\nnuanced interplay of speech and gesture that characterizes in-person commu-\nnication. Despite some promising work in generating co-speech gestures from\naudio (Ginosar et al., 2019; Bhattacharya et al., 2021; Liu et al., 2022), these ges-\ntures are often treated as accessory to speech rather than integral components,\nand thus, they do not always contribute the correct or intended information. As\nsuch, an understanding and integration of non-verbal cues remain an important\nfrontier for further exploration in NLP.\nSpoken language understanding, we propose, can benefit immensely from\nthe advances in sign language processing. We advocate for the implementa-\ntion of universal automatic gesture segmentation and transcription models that\ncan transcribe co-speech gestures into textual input. This could be a pioneering\nstep towards integrating the richness of non-verbal cues directly into the NLP\nmodels. By including transcribed gestures, the models would bridge the blind\nspots in spoken language understanding. This is a bidirectional process; Just\nas spoken language models can learn from sign language processing, the in-\nsights from the transcription of spoken language gestures can also inform and\nenhance sign language processing, due to iconicity, and metaphors. Ultimately,\nthis holistic approach would result in a more nuanced and comprehensive un-\nderstanding of human communication, bringing us closer to the complexities\nand richness of real-world, multi-modal interactions.\n9.2\nStereotypical Language Variation\nNon-verbal forms of communication are subject to significant cultural variabil-\nity, shaped by a complex interplay of historical, societal, and cultural factors.\nIn Mediterranean cultures, non-verbal communication is prevalent and vi-\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n186\nbrant. People in this region often use expressive gestures and maintain close\npersonal space when communicating. Italian, for instance, is renowned for its\nextensive use of gestures. Italians often use their hands and bodies expressively\nto illustrate their points or emotions, and there is a broad range of specific ges-\ntures that carry particular meanings, often comprehensible even without ac-\ncompanying speech.\nIn contrast, Japanese communication tends to incorporate fewer and more\nsubtle non-verbal cues. A bow, a nod, or a slight tilt of the head can convey a\nmyriad of meanings depending on the context, demonstrating respect, agree-\nment, or understanding. Meanwhile, in Nordic cultures, such as Swedish or\nFinnish, non-verbal cues are typically used sparingly. The communication style\ntends to be direct and understated, with less emphasis on gestures and more\nfocus on verbal content.\nOverall, these stereotypical examples highlight the diverse ways in which\nlanguages around the world incorporate non-verbal cues into communication.\nThis diversity emphasizes the importance of cultural understanding and sen-\nsitivity in interpreting and engaging in cross-cultural communication research,\nand data collection and annotation.\n9.3\nMotivating Examples\nNon-verbal cues can act to affirm and reinforce the spoken words, thereby strength-\nening the communicated message. They can also undermine the verbal mes-\nsage, creating a contradiction between what is being said and the speaker’s true\nintent or feelings. For NLP research to understand speech, it can not rely solely\non audio (or textual transcription) to understand the intent of the speaker.\nFor example, saying ‘Perfect’ while making a circle with the thumb and in-\ndex finger often emphasizes approval and satisfaction. Similarly, nodding while\nsaying ‘Yes’ reinforces affirmation, underscoring the speaker’s understanding\nor agreement. On the other hand, saying ‘OK’ while rolling one’s eyes, can\nsuggest that the speaker doesn’t find the situation truly satisfactory, despite the\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n187\nverbal agreement. Similarly, stating “I’m not mad” while frowning or clenching\nfists suggests that the speaker is upset, contradicting their verbal assertion.\n9.3.1\nSentiment Analysis\nTo demonstrate the limitations of text-based sentiment analysis, consider the\nfollowing hypothetical dialogue between a couple, where the man is utilizing\npassive-aggressive communication. In each turn, we also present the sentiment\nscore as predicted by the Google Cloud Natural Language API Demo1, where\nscores range between [−1, 1].\nWoman: How is it going? (0)\nMan:\nI am fine. (0.74)\n[crosses his arms]\nWoman: Did you enjoy dinner? (0.55)\nMan:\nIt was fine. (0.92)\n[avoids eye contact,\nlips pressed tightly]\nWoman: Is something wrong?\nYou seem distant. (-0.78)\nMan:\nNo, nothing’s wrong. (0.53)\n[shakes his head slightly,\nexhales loudly]\nWoman: Are you sure? (0)\nMan:\nI said I’m fine. (0)\n[rolls eyes, turns away]\nWhile all the man’s responses register as neutral to positive, his body lan-\nguage—avoiding eye contact, pressing his lips tightly together, shaking his head,\n1https://cloud.google.com/natural-language\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n188\nexhaling loudly, rolling his eyes, and turning away—signals that he may actu-\nally be upset, frustrated, or disengaged. By neglecting body language and other\ncontextual clues, current models miss out on a significant layer of human com-\nmunication, particularly in emotionally charged or complex dialogues. Such a\nholistic approach could provide a more nuanced and accurate understanding of\nthe emotional context, thus enriching machine-human interactions.\n9.3.2\nMachine Translation\nWhile existing in many other languages, Italian stereotypically gives us many\nexamples of gestures conveying meaning, where the verbal part is often dropped\naltogether, making it even more similar to signed languages.\nTable 9.1 showcases a toy example of a conversation between two Italians\nusing only gestures, without speech. It is transcribed using SignWriting (Sutton,\n1990) to demonstrate that anonymous non-verbal transcription can be done in\na low-bandwidth manner and that it can be reproduced and understood by\npeople trained at reading SignWriting.\n9.4\nMethodology\nMachine learning techniques that focus solely on text have gained predomi-\nnance due to several key factors: the abundance of readily available text data,\nthe potential for semi-anonymous data collection and processing, the very high\nbandwidth-to-overhead ratio as a word consumes only a few bytes compared\nto kilobytes or more for a second of speech or video, and the ease with which\ntext can be viewed, edited, and corrected.\nPrevious efforts have attempted to include other modalities like images (Razavi\net al., 2019), videos (Yan et al., 2021), or audio through the use of techniques like\nVQ-VAEs (van den Oord et al., 2017). However, these approaches often signif-\nicantly increase the context size, are not transferable across different systems,\nand generally require the original signal (like a video) to be sent for processing.\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n189\nwth\nwith\nthese\ntwo??\nI know! What the\nhell?\nDid\nyou\nhear\nthey’re\nto-\ngether??\nOh yeah. They’re\ndefinitely\nto-\ngether!\nI\nshouldn’t\nbe\nsaying this but...\nshe’s cheating on\nhim\nShe’s cheating on\nhim?? Woahhh\nYeah!\nBut it’s\nnone of my busi-\nness\nI can’t believe it\nTable 9.1: “How to gossip in Italian” by the Pasinis, transcribed in Sutton\nSignWriting by Sutthikhun Phaengphongsai\nhttps://www.youtube.com/watch?v=7V-GniCQFkE, demonstrating a\nconversation between two Italians using only gestures, without speech.\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n190\nIn contrast, our proposal offers a more flexible, universal, and efficient way to\nincorporate non-verbal cues directly as text.\n9.4.1\nProposal\nWe propose adopting a universal transcription system for body language, much\nlike the written system used for spoken languages. This system would tran-\nscribe gestures, facial expressions, and other non-verbal cues into textual form.\nThe advantages of this approach are numerous:\nFlexibility in Transcription\nDifferent programs can decide on their own tran-\nscription methods, taking into account local variations and context.\nComputational Efficiency\nText-based methods require significantly lower com-\nputational resources compared to image or video processing.\n(Notoriously,\nGPT-4 was released without image upload support, since inference on a single\nimage takes upwards of 20 seconds)\nCompatibility with Existing Models\nAs the body language would be tran-\nscribed into discrete tokens, it can fit seamlessly into existing large language\nmodels without any modification.\nAnonymity\nTranscription acts as a form of biometric anonymization, remov-\ning the need to share actual video or images.\nExplainability\nThe textual transcription provides a more transparent input,\nmaking the language modeling process more understandable.\nSeamless Integration\nThe proposed methodology does not require any sig-\nnificant changes to existing NLP pipelines. It simply acts as an additional layer\nof data for better understanding and disambiguation. You can include it, or not.\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n191\n9.4.2\nImplementation\nTo successfully integrate non-verbal cues when processing spoken language,\nwe advocate the following steps:\n1. Capture both video and audio during speech.\n2. Use sign language segmentation models to identify boundaries of indi-\nvidual gestures.\n3. Transcribe these gestures into a textual notation system like SignWriting.\n4. Use speech-to-text models to transcribe the spoken language, identifying\nthe boundaries where each word is expressed.\n5. If word boundaries are not directly accessible, a re-alignment model can\nbe used to approximate these boundaries.\n6. Combine both speech and gesture transcriptions into a single text string,\nwhere gestures provide additional context to the spoken words.\nThis approach can be thought of as analogous to incorporating additional\ncontext, such as gender, into machine translation (Moryossef et al., 2019). By\ntraining on a large dataset that includes unmarked sentences, the model may\ndevelop certain biases. Introducing a smaller dataset with contextual informa-\ntion can help the model learn correlations between language and specific con-\ntexts. During inference, one has the option to either provide just the text for a\nmore generalized output or include additional contextual tags for a more accu-\nrate and targeted output.\n9.5\nConclusions\nThis chapter underscores the fundamental role of non-verbal cues, such as co-\nspeech gestures and facial expressions, in human communication. While strides\nhave been made in the realm of Natural Language Processing for understanding\nCHAPTER 9. IMPLICATIONS FOR SPOKEN LANGUAGES\n192\ntextual content, a holistic approach that integrates the rich layers of non-verbal\ninformation is significantly lacking. This shortfall not only hampers the com-\nprehension of spoken language but also limits our ability to construct nuanced,\ncontext-aware NLP models.\nThe key to advancing in this frontier may lie in borrowing techniques and\ninsights from sign language processing. We advocate for the adaptation and\nimplementation of universal automatic gesture segmentation and transcription\nmodels that can transcribe co-speech gestures into textual input. Such an ap-\nproach would be a pivotal step in bridging the gap between text-based and\nreal-world, spoken interactions, thereby enriching both the scope and applica-\nbility of NLP models.\nWhen processing spoken language content, researchers should adopt a more\nholistic lens, one that goes beyond words and phrases to also consider non-\nverbal cues. Existing universal segmentation and transcription models used in\nsign language can serve as invaluable resources for this purpose, as they offer\nthe ability to transcribe gestures directly as text.\nWe call upon researchers in spoken language processing to contribute to the\ndevelopment of universal gesture transcription methods. Furthermore, we en-\ncourage the academic community to construct challenge sets specifically tai-\nlored to validate the utility of these transcription methods in capturing the\ncomplexities of non-verbal communication. These steps are not merely sup-\nplementary but are central to achieving a more comprehensive understanding\nof human communication in its full richness and complexity.\nChapter 10\nConclusions\nThis thesis introduces a novel pipeline to including signed languages in Natu-\nral Language Processing. Preliminary work showed that the currently predom-\ninant gloss-based pipeline is problematic, and contemporary research shows\nthat we are far from solving this problem in an end-to-end manner. Therefore,\nthis thesis proposes the introduction of lexical sign language notations as the\npivot between the video and text modalities. It addresses both directions -\nsigned-to-spoken translation (Chapter 6) and spoken-to-signed translation (Chap-\nter 7), and addresses the various sub-tasks of each pipeline to some extent. We\nbelieve that solving each one of these sub-tasks requires deeper studies and en-\ntire theses can be written on each.\nWhile this thesis does not, by all means, solve the task of sign language\ntranslation, we believe that it introduces the first viable pipeline to address both\ndirections, not only in a real-time multilingual setting. This pipeline relies on\nlittle, high-quality sign language transcription data in SignWriting, which since\nit is a universal (multilingual) representation of signed languages, weakens the\nnecessity of large annotated datasets for every signed language. We believe\nthat the implications of this thesis go beyond signed languages, into the realm\nof spoken language processing and even action recognition. Exhaustive move-\nment transcription opens new and exciting avenues in all of these fields, the\nsame way normalized spoken language writing systems have revolutionized\n193\nCHAPTER 10. CONCLUSIONS\n194\nNatural Language Processing.\nWe call for a complete separation of sign language processing between the\nfields of Natural Language Processing and Computer Vision. In Natural Lan-\nguage Processing, researchers should explore the translation from SignWriting\nto spoken language text and vice-versa, dealing directly with the language con-\ntent as written. They should be aware that transcription of real signing data\nis noisy and does not necessarily map into well-formed dictionary forms. In\nComputer Vision, researchers should explore the transcription of sign language\nvideos into SignWriting, without understanding the meaning, in a language-\nagnostic setting. Further, they should explore the animation of SignWriting se-\nquences into videos, with the use of 3D avatars or photo-realistic models.\nCaution for Future Researchers\nIn this thesis, we propose using SignWriting as a pivot between spoken lan-\nguage text and sign language video, in both directions.\nWe take a specific\npipeline approach that utilizes more intermediate steps in order to explore this\npipeline as it is possible in our times. However, we believe that as this pipeline\ngets used, it will make more data available (e.g., transcribing existing sign lan-\nguage videos will create more parallel data between spoken language text and\nSignWriting), and allow for end-to-end approaches in SLP. We encourage these\nimprovements and the reduction of steps in the pipeline.\nIdeally, we believe that end-to-end sentence-level video-to-SignWriting tran-\nscription systems, and SignWriting-to-video animation systems will prevail over\nthe inclusion of pose estimation and sign segmentation, but, we caution against\ntreating the SignWriting pivot the same way, and attempting end-to-end video-\nto-text and text-to-video systems. These systems will be fundamentally hard to\nevaluate, challenging to modify, and language-dependent. Breaking away from\nthe SignWriting pivot would also break the separation between Natural Lan-\nguage Processing and Computer Vision, and could lead to sub-par research.\nChapter 11\nBibliography\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\nCraig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael\nIsard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\nLevenberg, Dan Man´e, Rajat Monga, Sherry Moore, Derek Murray, Chris\nOlah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\nTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi´egas,\nOriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and\nXiaoqiang Zheng. TensorFlow: Large-scale machine learning on heteroge-\nneous systems, 2015. URL http://tensorflow.org/. Software available\nfrom tensorflow.org.\nAnne Abeill´e, Yves Schabes, and Aravind K Joshi. Using lexicalized tags for\nmachine translation. 1991.\nNikolas Adaloglou, Theocharis Chatzis, Ilias Papastratis, Andreas Stergioulas,\nGeorgios Th Papadopoulos, Vassia Zacharopoulou, George J Xydopou-\nlos, Klimnis Atzakas, Dimitris Papazachariou, and Petros Daras.\nA com-\nprehensive study on sign language recognition methods.\narXiv preprint\narXiv:2007.12530, 2020.\nChloe Adeline. Fingerspell.net, 2013. URL http://fingerspell.net/.\nSamuel Albanie, G¨ul Varol, Liliane Momeni, Triantafyllos Afouras, Joon Son\nChung, Neil Fox, and Andrew Zisserman. BSL-1K: Scaling up co-articulated\nsign language recognition using mouthing cues. In ECCV, 2020.\nSamuel Albanie, G¨ul Varol, Liliane Momeni, Hannah Bull, Triantafyllos\nAfouras, Himel Chowdhury, Neil Fox, Bencie Woll, Rob Cooper, Andrew Mc-\n195\nCHAPTER 11. BIBLIOGRAPHY\n196\nParland, and Andrew Zisserman. BOBSL: BBC-Oxford British Sign Language\nDataset. 2021.\nJeremy Appleyard. Optimizing recurrent neural networks in cuDNN 5, 4 2016.\nURL https://developer.nvidia.com/blog/optimizing-recurre\nnt-neural-networks-cudnn-5/. Accessed: 2023-06-09.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin\nJohnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, et al. Massively multilingual neural machine translation in the wild:\nFindings and challenges. arXiv preprint arXiv:1907.05019, 2019.\nRotem Shalev Arkushin, Amit Moryossef, and Ohad Fried. Ham2Pose: An-\nimating Sign Language Notation Into Pose Sequences. pages 21046–21056,\nJune 2023.\nVassilis Athitsos, Carol Neidle, Stan Sclaroff, Joan Nash, Alexandra Stefan,\nQuan Yuan, and Ashwin Thangali. The american sign language lexicon video\ndataset. In 2008 IEEE Computer Society Conference on Computer Vision and Pat-\ntern Recognition Workshops, pages 1–8. IEEE, 2008.\nTensorFlow authors. TensorFlow Datasets, a collection of ready-to-use datasets.\nhttps://github.com/tensorflow/datasets, 2019.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.\nwav2vec 2.0: A framework for self-supervised learning of speech representa-\ntions. Advances in neural information processing systems, 33:12449–12460, 2020.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine\ntranslation by jointly learning to align and translate.\n2015.\nURL http:\n//arxiv.org/abs/1409.0473.\nJ Andrew Bangham, SJ Cox, Ralph Elliott, John RW Glauert, Ian Marshall, Sanja\nRankov, and Mark Wells. Virtual signing: Capture, animation, storage and\ntransmission-an overview of the visicast project. In IEE Seminar on speech and\nlanguage processing for disabled and elderly people (Ref. No. 2000/025), pages 6–1.\nIET, 2000.\nSouad Baowidan. Improving realism in automated fingerspelling of american\nsign language. Machine Translation, 35(3):387–404, 2021.\nAdrien Barbaresi. Simplemma, January 2023. URL https://doi.org/10.5\n281/zenodo.7555188.\nCHAPTER 11. BIBLIOGRAPHY\n197\nIngo Barth, Jung-Woo Kim, et al. Sign2mint. https://sign2mint.de/, 2021.\nAccessed: 2023-12-15.\nRobbin Battison. Lexical borrowing in american sign language. 1978.\nUrsula Bellugi and Susan Fischer. A comparison of sign language and spoken\nlanguage. Cognition, 1(2-3):173–200, 1972.\nThe Bergamot Project Consortium Bergamot. Bergamot: Machine translation\ndone locally in your browser. Web page, 2022. URL https://browser.mt\n/.\nBrita Bergman.\nTecknad svenska:[Signed Swedish].\nLiberL¨aromedel/Utbild-\nningsf¨orl.:, 1977.\nDonald J. Berndt and James Clifford. Using dynamic time warping to find pat-\nterns in time series. In KDD Workshop, 1994.\nUttaran Bhattacharya, Elizabeth Childs, Nicholas Rewkowski, and Dinesh\nManocha. Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with\nGenerative Adversarial Affective Expression Learning. Proceedings of the 29th\nACM International Conference on Multimedia, 2021.\nSteven Bird.\nDecolonising speech and language technology.\nIn Proceedings\nof the 28th International Conference on Computational Linguistics, pages 3504–\n3519, Barcelona, Spain (Online), December 2020. International Committee on\nComputational Linguistics. doi: 10.18653/v1/2020.coling-main.313. URL\nhttps://aclanthology.org/2020.coling-main.313.\nChristopher M Bishop. Mixture density networks. 1994.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook\nKim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution\nvideo synthesis with latent diffusion models. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2023.\nNikolay Bogoychev, Jelmer Van der Linde, and Kenneth Heafield. Translate-\nLocally: Blazing-fast translation running on the local CPU.\nIn Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 168–174, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.emnlp-demo.20. URL https://aclanthology.org/2\n021.emnlp-demo.20.\nCHAPTER 11. BIBLIOGRAPHY\n198\nMark Borg and Kenneth P Camilleri. Sign language detection ”in the wild” with\nrecurrent neural networks. In ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages 1637–1641. IEEE,\n2019.\nG. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.\nDanielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudreault,\nAnnelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa\nVerhoef, et al. Sign language recognition, generation, and translation: An\ninterdisciplinary perspective. In The 21st International ACM SIGACCESS Con-\nference on Computers and Accessibility, pages 16–31, 2019.\nDiane Brentari. Sign language phonology. The handbook of phonological theory,\npages 691–721, 2011.\nDiane Brentari and Carol Padden. A language with multiple origins: Native\nand foreign vocabulary in american sign language. Foreign vocabulary in sign\nlanguage: A cross-linguistic investigation of word formation, pages 87–119, 2001.\nHannah Bull, Annelies Braffort, and Mich`ele Gouiff`es. MEDIAPI-SKEL - a 2D-\nskeleton video database of French Sign Language with aligned French sub-\ntitles. In Proceedings of the Twelfth Language Resources and Evaluation Confer-\nence, pages 6063–6068, Marseille, France, May 2020a. European Language Re-\nsources Association. ISBN 979-10-95546-34-4. URL https://aclantholo\ngy.org/2020.lrec-1.743.\nHannah Bull, Mich`ele Gouiff`es, and Annelies Braffort. Automatic segmentation\nof sign language into subtitle-units. In European Conference on Computer Vision,\npages 186–198. Springer, 2020b.\nHannah Bull, Triantafyllos Afouras, G¨ul Varol, Samuel Albanie, Liliane Mo-\nmeni, and Andrew Zisserman. Aligning subtitles in sign language videos. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages\n11552–11561, 2021.\nJan Bungeroth, Daniel Stein, Philippe Dreuw, Hermann Ney, Sara Morrissey,\nAndy Way, and Lynette van Zijl. The ATIS sign language corpus. In Proceed-\nings of the Sixth International Conference on Language Resources and Evaluation\n(LREC’08), Marrakech, Morocco, May 2008. European Language Resources\nAssociation (ELRA). URL http://www.lrec-conf.org/proceedings/\nlrec2008/pdf/748 paper.pdf.\nCHAPTER 11. BIBLIOGRAPHY\n199\nNecati Cihan Camg¨oz, Ahmet Alp Kındıro˘g lu, Serpil Karab¨ukl¨u, Meltem\nKelepir, Ays¸ e Sumru ¨Ozsoy, and Lale Akarun. BosphorusSign: A Turkish\nSign Language recognition corpus in health and finance domains. In Proceed-\nings of the Tenth International Conference on Language Resources and Evaluation\n(LREC’16), pages 1383–1388, Portoroˇz, Slovenia, May 2016. European Lan-\nguage Resources Association (ELRA). URL https://www.aclweb.org/a\nnthology/L16-1220.\nNecati Cihan Camg¨oz, Simon Hadfield, Oscar Koller, and Richard Bowden.\nSubunets: End-to-end hand shape and continuous sign language recognition.\nIn 2017 IEEE International Conference on Computer Vision (ICCV), pages 3075–\n3084. IEEE, 2017.\nNecati Cihan Camg¨oz, Simon Hadfield, Oscar Koller, Hermann Ney, and\nRichard Bowden. Neural sign language translation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 7784–7793, 2018.\nNecati Cihan Camg¨oz, Oscar Koller, Simon Hadfield, and Richard Bowden.\nMulti-channel transformers for multi-articulatory sign language translation.\nIn European Conference on Computer Vision, pages 301–319, 2020a.\nNecati Cihan Camg¨oz, Oscar Koller, Simon Hadfield, and Richard Bowden.\nSign language transformers: Joint end-to-end sign language recognition and\ntranslation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10023–10033, 2020b.\nZ. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. OpenPose:\nRealtime multi-person 2D pose estimation using part affinity fields.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2019.\nZhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person\n2d pose estimation using part affinity fields. In CVPR, 2017.\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition. A new\nmodel and the kinetics dataset. CoRR, abs/1705.07750, 2(3):1, 2017.\nSait Celebi, Ali Selman Aydin, Talha Tarik Temiz, and Tarik Arici. Gesture recog-\nnition using skeleton data with weighted dynamic time warping. In Interna-\ntional Conference on Computer Vision Theory and Applications, 2013.\nXiujuan Chai, Hanjie Wang, and Xilin Chen. The devisign large vocabulary\nof chinese sign language database and baseline evaluations.\nTechnical re-\nport VIPL-TR-14-SLR-001. Key Lab of Intelligent Information Processing of Chinese\nAcademy of Sciences (CAS), Institute of Computing Technology, CAS, 2014.\nCHAPTER 11. BIBLIOGRAPHY\n200\nCaroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody\ndance now.\nIn Proceedings of the IEEE International Conference on Computer\nVision, pages 5933–5942, 2019.\nYu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and Jian Yang. Adver-\nsarial posenet: A structure-aware convolutional network for human pose es-\ntimation. In Proceedings of the IEEE International Conference on Computer Vision,\npages 1212–1221, 2017.\nYu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang,\nAndrei Kulik, and Matthias Grundmann. Speed is all you need: On-device\nacceleration of large diffusion models via gpu-aware optimizations. 2023.\nEunah Cho, Jan Niehues, Kevin Kilgour, and Alex Waibel. Punctuation inser-\ntion for real-time spoken language translation. In Proceedings of the 12th Inter-\nnational Workshop on Spoken Language Translation: Papers, pages 173–179, 2015.\nKyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase rep-\nresentations using RNN encoder–decoder for statistical machine translation.\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1724–1734, Doha, Qatar, October 2014. Associ-\nation for Computational Linguistics.\ndoi: 10.3115/v1/D14-1179.\nURL\nhttps://www.aclweb.org/anthology/D14-1179.\nFranc¸ois Chollet et al. Keras. https://keras.io, 2015.\nMichael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. Clause restructuring for\nstatistical machine translation. In Proceedings of the 43rd Annual Meeting of the\nAssociation for Computational Linguistics (ACL’05), pages 531–540, Ann Arbor,\nMichigan, June 2005. Association for Computational Linguistics. doi: 10.311\n5/1219840.1219906. URL https://aclanthology.org/P05-1066.\nKearsy Cormier, Sandra Smith, and Zed Sevcikova-Sehyr.\nRethinking con-\nstructed action. Sign Language & Linguistics, 18(2):167–204, 2015.\nOnno Crasborn and Inge Zwitserlood. The Corpus NGT: An online corpus for\nprofessionals and laymen. 2008.\nOnno Crasborn, Richard Bank, Inge Zwitserlood, Els van der Kooij, Anique\nSch¨uller, Ellen Ormel, Ellen Nauta, Merel van Zuilen, Frouke van Winsum,\nand Johan Ros. NGT signbank. Nijmegen: Radboud University, Centre for Lan-\nguage Studies, 2016.\nCHAPTER 11. BIBLIOGRAPHY\n201\nRunpeng Cui, Hu Liu, and Changshui Zhang. Recurrent convolutional neural\nnetworks for continuous sign language recognition by staged optimization.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 7361–7369, 2017.\nRunpeng Cui, Hu Liu, and Changshui Zhang. A deep neural framework for\ncontinuous sign language recognition by iterative training. IEEE Transactions\non Multimedia, 21(7):1880–1891, 2019.\nAnna Currey, Antonio Valerio Miceli Barone, and Kenneth Heafield. Copied\nmonolingual data improves low-resource neural machine translation. In Pro-\nceedings of the Second Conference on Machine Translation, pages 148–156, Copen-\nhagen, Denmark, September 2017. Association for Computational Linguis-\ntics. doi: 10.18653/v1/W17-4715. URL https://aclanthology.org/W\n17-4715.\nKonstantinos M Dafnis, Evgenia Chroni, Carol Neidle, and Dimitris N Metaxas.\nBidirectional skeleton-based isolated sign recognition using graph convolu-\ntion networks. In Proceedings of the 13th Conference on Language Resources and\nEvaluation (LREC 2022), Marseille, 20-25 June 2022., 2022.\nNCSLGR Databases. Volumes 2–7, 2007.\nMary Jo Davidson. Paula: A computer-based sign language tutor for hearing\nadults. In Intelligent Tutoring Systems 2006 Workshop on Teaching with Robots,\nAgents, and Natural Language Processing, pages 66–72. Citeseer, 2006.\nLouise de Beuzeville. Pointing and verb modification: the expression of seman-\ntic roles in the auslan corpus. In Workshop Programme, page 13. Citeseer, 2008.\nMathieu De Coster, Dimitar Shterionov, Mieke Van Herreweghe, and Joni\nDambre. Machine translation from signed to spoken languages: State of the\nart and challenges. arXiv preprint arXiv:2202.03086, 2022.\nMathieu De Coster, Ellen Rushe, Ruth Holmes, Anthony Ventresque, and Joni\nDambre. Towards the extraction of robust sign embeddings for low resource\nsign language recognition. arXiv preprint arXiv:2306.17558, 2023.\nMirella De Sisto, Dimitar Shterionov, Irene Murtagh, Myriam Vermeerbergen,\nand Lorraine Leeson. Defining meaningful units. challenges in sign segmen-\ntation and segment-meaning mapping (short paper). In Proceedings of the 1st\nInternational Workshop on Automatic Translation for Signed and Spoken Languages\nCHAPTER 11. BIBLIOGRAPHY\n202\n(AT4SSL), pages 98–103, Virtual, August 2021. Association for Machine Trans-\nlation in the Americas. URL https://aclanthology.org/2021.mtsu\nmmit-at4ssl.11.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In 2009 IEEE conference on computer\nvision and pattern recognition, pages 248–255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of deep bidirectional transformers for language understanding.\nIn Proceedings of the 2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://aclanthology.org/N19-1423.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words:\nTransformers for image recognition at scale.\nIn International Conference on Learning Representations, 2021.\nURL https:\n//openreview.net/forum?id=YicbFdNTTy.\nZi-Yi Dou, Antonios Anastasopoulos, and Graham Neubig.\nDynamic data\nselection and weighting for iterative back-translation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\npages 5894–5904, Online, November 2020. Association for Computational\nLinguistics.\ndoi: 10.18653/v1/2020.emnlp-main.475.\nURL https:\n//aclanthology.org/2020.emnlp-main.475.\nPhilippe Dreuw, Thomas Deselaers, Daniel Keysers, and Hermann Ney. Mod-\neling image variability in appearance-based gesture recognition. In ECCV\nworkshop on statistical methods in multi-image and video processing, pages 7–18,\n2006.\nPhilippe Dreuw, Carol Neidle, Vassilis Athitsos, Stan Sclaroff, and Hermann\nNey. Benchmark databases for video-based automatic sign language recogni-\ntion. In LREC, 2008.\nMatthew S. Dryer and Martin Haspelmath, editors. WALS Online. Max Planck\nInstitute for Evolutionary Anthropology, Leipzig, 2013. URL https://wa\nls.info/.\nCHAPTER 11. BIBLIOGRAPHY\n203\nAmanda Duarte, Shruti Palaskar, Deepti Ghadiyaram, Kenneth DeHaan, Flo-\nrian Metze, Jordi Torres, and Xavier Giro-i Nieto. How2sign: A large-scale\nmultimodal dataset for continuous american sign language. arXiv preprint\narXiv:2008.08143, 2020.\nPaul G Dudis. Body partitioning and real-space blends. Cognitive linguistics, 15\n(2):223–238, 2004.\nSarah Ebling, Necati Cihan Camg¨oz, Penny Boyes Braem, Katja Tissi, Sandra\nSidler-Miserez, Stephanie Stoll, Simon Hadfield, Tobias Haug, Richard Bow-\nden, Sandrine Tornay, Marzieh Razavi, and Mathew Magimai-Doss. SMILE\nSwiss German sign language dataset. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Evaluation (LREC 2018), Miyazaki,\nJapan, May 2018. European Language Resources Association (ELRA). URL\nhttps://www.aclweb.org/anthology/L18-1666.\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding\nback-translation at scale.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 489–500, Brussels, Belgium,\nOctober-November 2018. Association for Computational Linguistics.\ndoi:\n10.18653/v1/D18-1045. URL https://aclanthology.org/D18-1045.\nEleni Efthimiou, Stavroula-Evita Fotinea, Thomas Hanke, John Glauert, Richard\nBowden, Annelies Braffort, Christophe Collet, Petros Maragos, and Franc¸ois\nLefebvre-Albaret. Sign language technologies and resources of the dicta-sign\nproject. In 5th Workshop on the Representation and Processing of Sign Languages:\nInteractions between Corpus and Lexicon. Satellite Workshop to the eighth Interna-\ntional Conference on Language Resources and Evaluation (LREC-2012), 2012.\nSantiago Egea G´omez, Euan McGill, and Horacio Saggion. Syntax-aware trans-\nformers for neural machine translation: The case of text to sign gloss trans-\nlation. In Proceedings of the 14th Workshop on Building and Using Comparable\nCorpora (BUCC 2021), pages 18–27, Online (Virtual Mode), September 2021.\nINCOMA Ltd. URL https://aclanthology.org/2021.bucc-1.4.\nRalph Elliott, John RW Glauert, JR Kennaway, and Ian Marshall. The develop-\nment of language processing support for the visicast project. In Proceedings of\nthe fourth international ACM conference on Assistive technologies, pages 101–108,\n2000.\nRalph Elliott, John Glauert, Vince Jennings, and Richard Kennaway.\nAn\noverview of the sigml notation and sigmlsigning software system. sign-lang\nLREC 2004, pages 98–104, 2004.\nCHAPTER 11. BIBLIOGRAPHY\n204\nMichael Erard. Why sign-language gloves don’t help deaf people. The Atlantic,\n9, 2017.\nIva Farag and Heike Brock. Learning motion disfluencies for automatic sign\nlanguage segmentation. In ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 7360–7364. IEEE, 2019.\nJordan Fenlon, Kearsy Cormier, and Adam Schembri. Building BSL signbank:\nThe lemma dilemma revisited. International Journal of Lexicography, 28(2):169–\n206, 2015.\nJordan Fenlon, Adam Schembri, and Kearsy Cormier. Modification of indicat-\ning verbs in british sign language: A corpus-based study. Language, 94(1):\n84–118, 2018.\nLucinda Ferreira-Brito.\nSimilarities & differences in two brazilian sign lan-\nguages. Sign language studies, 42:45–56, 1984.\nMichael Filhol and John C McDonald. The synthesis of complex shape deploy-\nments in sign language. In Proceedings of the 9th workshop on the Representation\nand Processing of Sign Languages, 2020.\nMichael Filhol and John C McDonald. Representation and synthesis of geomet-\nric relocations. In Workshop on the representation and processing Sign Language,\n2022.\nMichael Filhol, John C McDonald, and Rosalee J Wolfe. Synthesizing sign lan-\nguage by connecting linguistically structured descriptions to a multi-track\nanimation system. In Universal Access in Human–Computer Interaction. Design-\ning Novel Interactions: 11th International Conference, UAHCI 2017, Held as Part\nof HCI International 2017, Vancouver, BC, Canada, July 9–14, 2017, Proceedings,\nPart II 11, pages 27–40. Springer, 2017.\nJens Forster, Christoph Schmidt, Oscar Koller, Martin Bellgardt, and Hermann\nNey.\nExtensions of the sign language recognition and translation corpus\nRWTH-PHOENIX-Weather. In LREC, pages 1911–1916, 2014.\nAdam Frost and Valerie Sutton. SignWriting Hand Symbols. SignWriting Press,\n2022.\nBinyam Gebrekidan Gebre, Peter Wittenburg, and Tom Heskes. Automatic sign\nlanguage identification. In 2013 IEEE International Conference on Image Process-\ning, pages 2626–2630. IEEE, 2013.\nCHAPTER 11. BIBLIOGRAPHY\n205\nShiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and\nJitendra Malik. Learning individual styles of conversational gesture. 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n3492–3501, 2019.\nXavier Gir´o-i Nieto. Can everybody sign now? exploring sign language video\ngeneration from 2d poses. SLRTP 2020: The Sign Language Recognition, Trans-\nlation & Production Workshop, 2020.\nNeil S Glickman and Wyatte C Hall. Language deprivation and deaf mental health.\nRoutledge, 2018.\nAlex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber.\nConnectionist temporal classification: labelling unsegmented sequence data\nwith recurrent neural networks. In Proceedings of the 23rd international confer-\nence on Machine learning, pages 369–376, 2006.\nIvan Grishchenko and Valentin Bazarevsky. Mediapipe holistic, 2020. URL ht\ntps://google.github.io/mediapipe/solutions/holistic.html.\nRıza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense\nhuman pose estimation in the wild. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 7297–7306, 2018.\nEva Gutierrez-Sigut, Brendan Costello, Cristina Baus, and Manuel Carreiras.\nLSE-sign: A lexical database for spanish sign language.\nBehavior Research\nMethods, 48(1):123–137, 2016.\nWyatte C Hall, Leonard L Levin, and Melissa L Anderson. Language depriva-\ntion syndrome: A possible neurodevelopmental disorder with sociocultural\norigins. Social psychiatry and psychiatric epidemiology, 52(6):761–776, 2017.\nThomas Hanke. iLex-a tool for sign language lexicography and corpus analysis.\nIn LREC, 2002.\nThomas Hanke, Silke Matthes, Anja Regen, and Satu Worseck. Where does a\nsign start and end? segmentation of continuous signing. In Language Resources\nand Evaluation Conference, 2012.\nThomas Hanke, Marc Schulder, Reiner Konrad, and Elena Jahn. Extending the\nPublic DGS Corpus in size and depth.\nIn Proceedings of the LREC2020 9th\nWorkshop on the Representation and Processing of Sign Languages: Sign Language\nResources in the Service of the Language Community, Technological Challenges and\nApplication Perspectives, pages 75–82, Marseille, France, May 2020. European\nCHAPTER 11. BIBLIOGRAPHY\n206\nLanguage Resources Association (ELRA). ISBN 979-10-95546-54-2. URL ht\ntps://www.aclweb.org/anthology/2020.signlang-1.12.\nCharles R. Harris, K. Jarrod Millman, St´efan van der Walt, Ralf Gommers,\nPauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg,\nNathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten Hen-\nric van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern’andez del R’io,\nMarcy Wiebe, Pearu Peterson, Pierre G’erard-Marchant, Kevin Sheppard,\nTyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and\nTravis E. Oliphant.\nArray programming with numpy.\nNature, 585:357 –\n362, 2020. URL https://api.semanticscholar.org/CorpusID:\n219792763.\nRaychelle Harris, Heidi M Holmes, and Donna M Mertens. Research ethics in\nsign language communities. Sign Language Studies, 9(2):104–131, 2009.\nSaad Hassan, Larwan Berke, Elahe Vahdani, Longlong Jing, Yingli Tian, and\nMatt Huenerfauth.\nAn isolated-signing RGBD dataset of 100 American\nSign Language signs produced by fluent ASL signers. In Proceedings of the\nLREC2020 9th Workshop on the Representation and Processing of Sign Languages:\nSign Language Resources in the Service of the Language Community, Technologi-\ncal Challenges and Application Perspectives, pages 89–94, Marseille, France, May\n2020. European Language Resources Association (ELRA). ISBN 979-10-95546-\n54-2. URL https://www.aclweb.org/anthology/2020.signlang-1\n.14.\nSaad Hassan, Matthew Seita, Larwan Berke, Yingli Tian, Elaine Gale, Sooyeon\nLee, and Matt Huenerfauth. ASL-Homework-RGBD dataset: An annotated\ndataset of 45 fluent and non-fluent signers performing American Sign Lan-\nguage homeworks.\nIn Eleni Efthimiou, Stavroula-Evita Fotinea, Thomas\nHanke, Julie A. Hochgesang, Jette Kristoffersen, Johanna Mesch, and Marc\nSchulder, editors, Proceedings of the LREC2022 10th Workshop on the Represen-\ntation and Processing of Sign Languages: Multilingual Sign Language Resources,\npages 67–72, Marseille, France, June 2022. European Language Resources As-\nsociation (ELRA). ISBN 979-10-95546-86-3. URL https://www.sign-lan\ng.uni-hamburg.de/lrec/pub/22008.pdf.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\nGines Hidalgo, Yaadhav Raaj, Haroon Idrees, Donglai Xiang, Hanbyul Joo,\nCHAPTER 11. BIBLIOGRAPHY\n207\nTomas Simon, and Yaser Sheikh. Single-network whole-body pose estima-\ntion. In ICCV, 2019.\nFelix Hieber, Tobias Domhan, Michael J. Denkowski, David Vilar, Artem\nSokolov, Ann Clifton, and Matt Post. Sockeye: A toolkit for neural machine\ntranslation. ArXiv, abs/1712.05690, 2017. URL https://api.semanticsc\nholar.org/CorpusID:24218611.\nFelix Hieber, Tobias Domhan, and David Vilar. Sockeye 2: A toolkit for neural\nmachine translation. In European Association for Machine Translation Confer-\nences/Workshops, 2020. URL https://api.semanticscholar.org/Corp\nusID:219682429.\nFelix Hieber, Michael Denkowski, Tobias Domhan, Barbara Darques Barros,\nCelina Dong Ye, Xing Niu, Cuong Hoang, Ke Tran, Benjamin Hsu, Maria\nNadejde, Surafel Lakew, Prashant Mathur, Anna Currey, and Marcello Fed-\nerico. Sockeye 3: Fast neural machine translation with pytorch, 2022. URL\nhttps://arxiv.org/abs/2207.05851.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A.\nGritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J.\nFleet, and Tim Salimans. Imagen video: High definition video generation\nwith diffusion models. ArXiv, abs/2210.02303, 2022.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735–1780, 1997.\nMatthew Honnibal and Ines Montani. spaCy 2: Natural language understand-\ning with Bloom embeddings, convolutional neural networks and incremental\nparsing. To appear, 2017.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets:\nEfficient convolutional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017.\nJie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li, and Weiping Li. Video-\nbased sign language recognition without temporal segmentation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\nWencan Huang, Wenwen Pan, Zhou Zhao, and Qi Tian. Towards fast and high-\nquality sign language production. In Proceedings of the 29th ACM International\nConference on Multimedia, pages 3172–3181, 2021.\nCHAPTER 11. BIBLIOGRAPHY\n208\nTom Humphries, Poorna Kushalnagar, Gaurav Mathur, Donna Jo Napoli, Carol\nPadden, Christian Rathmann, and Scott Smith. Avoiding linguistic neglect of\ndeaf children. Social Service Review, 90(4):589–619, 2016.\nAlfarabi\nImashev,\nMedet\nMukushev,\nVadim\nKimmelman,\nand\nAnara\nSandygulova. A dataset for linguistic understanding, visual evaluation, and\nrecognition of sign languages: The k-rsl. In Proceedings of the 24th Conference\non Computational Natural Language Learning, pages 631–640, 2020.\nAnn Irvine and Chris Callison-Burch. Combining bilingual and comparable\ncorpora for low resource machine translation.\nIn Proceedings of the Eighth\nWorkshop on Statistical Machine Translation, pages 262–270, Sofia, Bulgaria,\nAugust 2013. Association for Computational Linguistics.\nURL https:\n//aclanthology.org/W13-2233.\nAmy Isard. Approaches to the anonymisation of sign language corpora. In Pro-\nceedings of the LREC2020 9th Workshop on the Representation and Processing of\nSign Languages: Sign Language Resources in the Service of the Language Commu-\nnity, Technological Challenges and Application Perspectives, pages 95–100, 2020.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image\ntranslation with conditional adversarial networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1125–1134, 2017.\nHideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. Head fi-\nnalization: A simple reordering rule for sov languages. In Proceedings of the\nJoint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages\n244–251, 2010.\nSongyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu.\nSign language recognition via skeleton-aware multi-model ensemble. arXiv\npreprint arXiv:2110.06161, 2021.\nZifan Jiang, Amit Moryossef, Mathias M¨uller, and Sarah Ebling. Machine trans-\nlation between spoken languages and signed languages represented in Sign-\nWriting. In Findings of the Association for Computational Linguistics: EACL 2023,\npages 1661–1679, Dubrovnik, Croatia, May 2023a. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/2023.findings\n-eacl.127.\nZifan Jiang, Adrian Soldati, Isaac Schamberg, Adriano R Lameira, and Steven\nMoran. Automatic sound event detection and classification of great ape calls\nusing neural networks. arXiv preprint arXiv:2301.02214, 2023b.\nCHAPTER 11. BIBLIOGRAPHY\n209\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu,\nZhifeng Chen, Nikhil Thorat, Fernanda Vi´egas, Martin Wattenberg, Greg\nCorrado, Macduff Hughes, and Jeffrey Dean.\nGoogle’s multilingual neu-\nral machine translation system: Enabling zero-shot translation.\nTransac-\ntions of the Association for Computational Linguistics, 5:339–351, 2017.\ndoi:\n10.1162/tacl a 00065. URL https://aclanthology.org/Q17-1024.\nRobert E Johnson and Scott K Liddell. Toward a phonetic representation of\nsigns: Sequentiality and contrast. Sign Language Studies, 11(2):241–274, 2011.\nRonan Johnson, Maren Brumm, and Rosalee J Wolfe. An improved avatar for\nautomatic mouth gesture recognition. In Language Resources and Evaluation\nConference, pages 107–108, 2018.\nTrevor Johnston. From archive to corpus: Transcription and annotation in the\ncreation of signed language corpora. International journal of corpus linguistics,\n15(1):106–131, 2010.\nTrevor Johnston and Louise De Beuzeville. Auslan corpus annotation guide-\nlines. Auslan Corpus, 2016.\nTrevor Johnston and Adam Schembri. Australian Sign Language (Auslan): An\nintroduction to sign language linguistics. Cambridge University Press, 2007.\nTrevor Alexander Johnston. From archive to corpus: transcription and annota-\ntion in the creation of signed language corpora. In Pacific Asia Conference on\nLanguage, Information and Computation, 2008.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu\nHoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, Andr´e F. T. Martins, and Alexandra\nBirch. Marian: Fast neural machine translation in C++. In Proceedings of ACL\n2018, System Demonstrations, pages 116–121, Melbourne, Australia, July 2018.\nAssociation for Computational Linguistics. URL http://www.aclweb.o\nrg/anthology/P18-4020.\nJim Kakumasu. Urubu sign language. International journal of American linguis-\ntics, 34(4):275–281, 1968.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture\nfor generative adversarial networks. 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4396–4405, 2018.\nCHAPTER 11. BIBLIOGRAPHY\n210\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and\nTimo Aila. Analyzing and improving the image quality of StyleGAN. In Proc.\nCVPR, 2020.\nTero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen, Janne Hellsten, Jaakko\nLehtinen, and Timo Aila. Alias-free generative adversarial networks. In Proc.\nNeurIPS, 2021.\nLee Kezar, Jesse Thomason, and Zed Sehyr. Improving sign recognition with\nphonology. In Proceedings of the 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics, pages 2724–2729, Dubrovnik, Croa-\ntia, May 2023. Association for Computational Linguistics.\nURL https:\n//aclanthology.org/2023.eacl-main.200.\nRawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin\nJames, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foun-\ndation for human vision models. arXiv preprint arXiv:2408.12569, 2024.\nVadim Kimmelman. Information structure in russian sign language and sign\nlanguage of the netherlands. Sign Language & Linguistics, 18(1):142–150, 2014.\nDiederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational\ndiffusion models. ArXiv, abs/2107.00630, 2021.\nMichael Kipp. Anvil-a generic annotation tool for multimodal dialogue. In\nSeventh European Conference on Speech Communication and Technology, 2001.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush.\nOpenNMT: Open-source toolkit for neural machine translation. In Proceedings\nof ACL 2017, System Demonstrations, pages 67–72, Vancouver, Canada, July\n2017. Association for Computational Linguistics. URL https://aclantho\nlogy.org/P17-4012.\nSang-Ki Ko, Chang Jo Kim, Hyedong Jung, and Choongsang Cho. Neural sign\nlanguage translation based on human keypoint estimation. Applied Sciences,\n9(13):2683, 2019.\nReiner Konrad, Thomas Hanke, Gabriele Langer, Susanne K¨onig, Lutz K¨onig,\nRie Nishio, and Anja Regen. Public DGS corpus: Annotation conventions.\nTechnical report, Project Note AP03-2018-01, DGS-Korpus project, IDGS,\nHamburg University, 2018.\nReiner Konrad, Thomas Hanke, Gabriele Langer, Susanne K¨onig, Lutz K¨onig,\nRie Nishio, and Anja Regen. Public DGS Corpus: Annotation Conventions /\nCHAPTER 11. BIBLIOGRAPHY\n211\n¨Offentliches DGS-Korpus: Annotationskonventionen, June 2022. URL http\ns://doi.org/10.25592/uhhfdm.10251.\nJulia Kreutzer, Jasmijn Bastings, and Stefan Riezler.\nJoey NMT: A minimal-\nist NMT toolkit for novices. In Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and the 9th International Joint Con-\nference on Natural Language Processing (EMNLP-IJCNLP): System Demonstra-\ntions, pages 109–114, Hong Kong, China, November 2019. Association for\nComputational Linguistics.\ndoi: 10.18653/v1/D19-3019.\nURL https:\n//www.aclweb.org/anthology/D19-3019.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification\nwith deep convolutional neural networks. In Advances in neural information\nprocessing systems, pages 1097–1105, 2012.\nTaku Kudo.\nSubword regularization: Improving neural network translation\nmodels with multiple subword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 66–75, Melbourne, Australia, July 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/P18-1007. URL https://www.aclweb.org\n/anthology/P18-1007.\nAnnelies Maria Jozef Kusters, Dai O’Brien, and Maartje De Meulder. Innovations\nin Deaf Studies: Critically Mapping the Field, pages 1–53. Oxford University\nPress, United Kingdom, 2017. ISBN 9780190612184.\nJoanna Łacheta and PawełRutkowski. A corpus-based dictionary of polish sign\nlanguage (pjm). 2014.\nMarie Lebert. Project gutenberg (1971-2008), 2008.\nDongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li. Word-level deep\nsign language recognition from video: A new large-scale dataset and methods\ncomparison. In The IEEE Winter Conference on Applications of Computer Vision,\npages 1459–1469, 2020.\nScott K Liddell and Robert E Johnson. American sign language: The phonolog-\nical base. Sign language studies, 64(1):195–277, 1989.\nScott K Liddell and Melanie Metzger. Gesture in sign language discourse. Jour-\nnal of pragmatics, 30(6):657–697, 1998.\nScott K Liddell et al. Grammar, gesture, and meaning in American Sign Language.\nCambridge University Press, 2003.\nCHAPTER 11. BIBLIOGRAPHY\n212\nLimsi. Dicta-sign-lsf-v2, 2019. URL https://hdl.handle.net/11403\n/dicta-sign-lsf-v2/v1. ORTOLANG (Open Resources and TOols for\nLANGuage) –www.ortolang.fr.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou\nXia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, Antonios\nAnastasopoulos, Patrick Littell, and Graham Neubig. Choosing transfer lan-\nguages for cross-lingual learning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages 3125–3135, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-\n1301. URL https://aclanthology.org/P19-1301.\nPatrick Littell, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner,\nand Lori Levin. URIEL and lang2vec: Representing languages as typological,\ngeographical, and phylogenetic vectors. In Proceedings of the 15th Conference\nof the European Chapter of the Association for Computational Linguistics: Volume\n2, Short Papers, pages 8–14, Valencia, Spain, April 2017. Association for Com-\nputational Linguistics. URL https://aclanthology.org/E17-2002.\nXian Liu, Qianyi Wu, Hang Zhou, Yuanqi Du, Wayne Wu, Dahua Lin, and Ziwei\nLiu. Audio-Driven Co-Speech Gesture Video Generation. Advances in Neural\nInformation Processing Systems, 2022.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. Swin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012–10022, 2021.\nMart Lubbers and Francisco Torreira. pympi-ling: a Python module for pro-\ncessing ELANs EAF and Praats TextGrid annotation files. https://pypi.p\nython.org/pypi/pympi-ling, 2013. Version 1.69.\nThang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches\nto attention-based neural machine translation. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language Processing, pages 1412–1421,\nLisbon, Portugal, September 2015. Association for Computational Linguis-\ntics. doi: 10.18653/v1/D15-1166. URL https://www.aclweb.org/ant\nhology/D15-1166.\nMark Alan Mandel. Phonotactics and morphophonology in American Sign Language.\nUniversity of California, Berkeley, 1981.\nAleix M Mart´ınez, Ronnie B Wilbur, Robin Shay, and Avinash C Kak.\nPur-\ndue rvl-slll asl database for automatic recognition of american sign language.\nCHAPTER 11. BIBLIOGRAPHY\n213\nIn Proceedings. Fourth IEEE International Conference on Multimodal Interfaces,\npages 167–172. IEEE, 2002.\nSilke Matthes, Thomas Hanke, Anja Regen, Jakob Storz, Satu Worseck, Eleni\nEfthimiou, Athanasia-Lida Dimou, Annelies Braffort, John Glauert, and Eva\nSafar. Dicta-sign–building a multilingual sign language corpus. In Proceed-\nings of the 5th Workshop on the Representation and Processing of Sign Languages:\nInteractions between Corpus and Lexicon (LREC 2012), 2012.\nJohn C McDonald and Michael Filhol. Natural synthesis of productive forms\nfrom structured descriptions of sign language. Machine Translation, 35(3):363–\n386, 2021.\nJohn C McDonald, Rosalee J Wolfe, Jerry C Schnepp, Julie Hochgesang, Di-\nana Gorman Jamrozik, Marie Stumbo, Larwan Berke, Melissa Bialek, and\nFarah Thomas. An automated technique for real-time production of lifelike\nanimations of american sign language. Universal Access in the Information So-\nciety, 15:551–566, 2016.\nJohn C McDonald, Rosalee J Wolfe, Sarah Johnson, Souad Baowidan, Robyn\nMoncrief, and Ningshan Guo.\nAn improved framework for layering lin-\nguistic processes in sign language generation: Why there should never be\na “brows” tier.\nIn Universal Access in Human–Computer Interaction. Design-\ning Novel Interactions: 11th International Conference, UAHCI 2017, Held as Part\nof HCI International 2017, Vancouver, BC, Canada, July 9–14, 2017, Proceedings,\nPart II 11, pages 41–54. Springer, 2017.\nJohn C McDonald, Ronan Johnson, and Rosalee J Wolfe. A novel approach to\nmanaging lower face complexity in signing avatars. In Proceedings of the 7th\nInternational Workshop on Sign Language Translation and Avatar Technology: The\nJunction of the Visual and the Textual: Challenges and Perspectives, pages 67–72,\n2022.\nDavid McKee and Graeme Kennedy. Lexical comparison of signs from ameri-\ncan, australian, british and new zealand sign languages. The signs of language\nrevisited: An anthology to honor Ursula Bellugi and Edward Klima, pages 49–76,\n2000.\nJohanna Mesch and Lars Wallin. From meaning to signs and back: Lexicogra-\nphy and the swedish sign language corpus. In Proceedings of the 5th Workshop\non the Representation and Processing of Sign Languages: Interactions between Cor-\npus and Lexicon [Language Resources and Evaluation Conference (LREC)], pages\n123–126, 2012.\nCHAPTER 11. BIBLIOGRAPHY\n214\nJohanna Mesch and Lars Wallin. Gloss annotations in the swedish sign language\ncorpus. International Journal of Corpus Linguistics, 20(1):102–120, 2015.\nJianyuan Min and Jinxiang Chai. Motion graphs++ a compact generative model\nfor semantic motion analysis and synthesis. ACM Transactions on Graphics\n(TOG), 31(6):1–12, 2012.\nLiliane Momeni, Hannah Bull, KR Prajwal, Samuel Albanie, G¨ul Varol, and\nAndrew Zisserman. Automatic dense annotation of large-vocabulary sign\nlanguage videos. In Computer Vision–ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV, pages 671–690.\nSpringer, 2022.\nRobyn Moncrief. Extending a model for animating adverbs of manner in amer-\nican sign language. In Proceedings of the LREC2020 9th Workshop on the Repre-\nsentation and Processing of Sign Languages: Sign Language Resources in the Service\nof the Language Community, Technological Challenges and Application Perspectives,\npages 151–156, 2020.\nRobyn Moncrief. Generalizing a model for animating adverbs of manner in\namerican sign language. Machine Translation, 35(3):345–362, 2021.\nInes Montani, Matthew Honnibal, Matthew Honnibal, Sofie Van Landeghem,\nAdriane Boyd, Henning Peters, Paul O’Leary McCann, jim geovedi, Jim\nO’Regan, Maxim Samsonov, Gy¨orgy Orosz, Dani¨el de Kok, Duygu Altinok,\nSøren Lind Kristiansen, Madeesh Kannan, Rapha¨el Bournhonesque, Lj Mi-\nranda, Peter Baumgartner, Edward, Explosion Bot, Richard Hudson, Raphael\nMitsch, Roman, Leander Fiedler, Ryn Daniels, Wannaphong Phatthiyaphai-\nbun, Gr´egory Howard, Yohei Tamura, and Sam Bozek.\nexplosion/spaCy:\nv3.5.0: New CLI commands, language updates, bug fixes and much more,\nJanuary 2023. URL https://doi.org/10.5281/zenodo.7553910.\nCaio DD Monteiro, Christy Maria Mathew, Ricardo Gutierrez-Osuna, and Frank\nShipman. Detecting and identifying sign languages through visual features.\nIn 2016 IEEE International Symposium on Multimedia (ISM), pages 287–290.\nIEEE, 2016.\nKathryn Montemurro and Diane Brentari.\nEmphatic fingerspelling as code-\nmixing in american sign language. Proceedings of the Linguistic Society of Amer-\nica, 3(1):61–1, 2018.\nAmit Moryossef. 3d hand pose benchmark. https://github.com/sign-\nlanguage-processing/3d-hands-benchmark, 2022.\nCHAPTER 11. BIBLIOGRAPHY\n215\nAmit Moryossef. Addressing the blind spots in spoken language processing.\narXiv preprint arXiv:2309.06572, 2023a.\nAmit Moryossef. Addressing the blind spots in spoken language processing,\nAugust 2023b.\nAmit Moryossef. sign.mt: A web-based application for real-time multilingual\nsign language translation. https://sign.mt/, September 2023c.\nAmit Moryossef and Yoav Goldberg. Sign Language Processing. https://si\ngn-language-processing.github.io/, 2021.\nAmit Moryossef and Zifan Jiang. Signbank+: Multilingual sign language trans-\nlation dataset, 2023.\nAmit Moryossef and Mathias M¨uller. Sign language datasets. https://gith\nub.com/sign-language-processing/datasets, 2021.\nAmit Moryossef, Roee Aharoni, and Yoav Goldberg. Filling gender & num-\nber gaps in neural machine translation with black-box context injection. In\nProceedings of the First Workshop on Gender Bias in Natural Language Processing,\npages 49–54, Florence, Italy, August 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W19-3807. URL https://aclanthology.org\n/W19-3807.\nAmit Moryossef, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling, and Srini\nNarayanan. Real-time sign-language detection using human pose estimation.\nIn Computer Vision–ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020,\nProceedings, Part II 16, SLRTP 2020: The Sign Language Recognition, Translation\nand Production Workshop, pages 237–248, 2020.\nAmit Moryossef, Mathias M¨uller, and Rebecka Fahrni. pose-format: Library for\nviewing, augmenting, and handling .pose files. https://github.com/s\nign-language-processing/pose, 2021a.\nAmit Moryossef, Ioannis Tsochantaridis, Joe Dinn, Necati Cihan Camg¨oz,\nRichard Bowden, Tao Jiang, Annette Rios, Mathias M¨uller, and Sarah Ebling.\nEvaluating the immediate applicability of pose estimation for sign language\nrecognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3434–3440, 2021b.\nAmit Moryossef, Kayo Yin, Graham Neubig, and Yoav Goldberg. Data augmen-\ntation for sign language gloss translation. In Proceedings of the 1st International\nWorkshop on Automatic Translation for Signed and Spoken Languages (AT4SSL),\nCHAPTER 11. BIBLIOGRAPHY\n216\npages 1–11, Virtual, August 2021c. Association for Machine Translation in the\nAmericas. URL https://aclanthology.org/2021.mtsummit-at4ss\nl.1.\nAmit Moryossef, Zifan Jiang, Mathias M¨uller, Sarah Ebling, and Yoav Gold-\nberg. Linguistically motivated sign language segmentation. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023. Association for Com-\nputational Linguistics, December 2023a. URL https://aclanthology.o\nrg/volumes/2023.findings-emnlp/.\nAmit Moryossef, Mathias M¨uller, Anne G¨ohring, Zifan Jiang, Yoav Goldberg,\nand Sarah Ebling. An open-source gloss-based baseline for spoken to signed\nlanguage translation. In 2nd International Workshop on Automatic Translation\nfor Signed and Spoken Languages (AT4SSL), June 2023b. URL https://gith\nub.com/ZurichNLP/spoken-to-signed-translation. Available at:\nhttps://arxiv.org/abs/2305.17714.\nMedet Mukushev, Aigerim Kydyrbekova, Vadim Kimmelman, and Anara\nSandygulova.\nTowards large vocabulary Kazakh-Russian Sign Language\ndataset: KRSL-OnlineSchool.\nIn Eleni Efthimiou, Stavroula-Evita Fotinea,\nThomas Hanke, Julie A. Hochgesang, Jette Kristoffersen, Johanna Mesch,\nand Marc Schulder, editors, Proceedings of the LREC2022 10th Workshop on\nthe Representation and Processing of Sign Languages: Multilingual Sign Lan-\nguage Resources, pages 154–158, Marseille, France, June 2022. European Lan-\nguage Resources Association (ELRA). ISBN 979-10-95546-86-3. URL https:\n//www.sign-lang.uni-hamburg.de/lrec/pub/22031.pdf.\nMathias M¨uller, Sarah Ebling, Eleftherios Avramidis, Alessia Battisti, Mich`ele\nBerger, Richard Bowden, Annelies Braffort, Necati Cihan Camg¨oz, Cristina\nEspa˜na-bonet, Roman Grundkiewicz, Zifan Jiang, Oscar Koller, Amit\nMoryossef, Regula Perrollaz, Sabine Reinhard, Annette Rios, Dimitar Shte-\nrionov, Sandra Sidler-miserez, and Katja Tissi.\nFindings of the first WMT\nshared task on sign language translation (WMT-SLT22). In Proceedings of the\nSeventh Conference on Machine Translation (WMT), pages 744–772, Abu Dhabi,\nUnited Arab Emirates (Hybrid), December 2022. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/2022.wmt-1.71.\nMathias M¨uller, Zifan Jiang, Amit Moryossef, Annette Rios, and Sarah Ebling.\nConsiderations for meaningful sign language machine translation based on\nglosses. In Proceedings of the 61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages 682–693, Toronto, Canada,\nJuly 2023. Association for Computational Linguistics. URL https://acla\nnthology.org/2023.acl-short.60.\nCHAPTER 11. BIBLIOGRAPHY\n217\nJoseph J Murray, Wyatte C Hall, and Kristin Snoddon. The importance of signed\nlanguages for deaf children and their families. The Hearing Journal, 73(3):30–\n32, 2020.\nJemina Napier and Lorraine Leeson. Sign Language in Action. Palgrave Macmil-\nlan, London, 2016.\nCarol Neidle, Stan Sclaroff, and Vassilis Athitsos. Signstream: A tool for linguis-\ntic and computer vision research on visual-gestural language data. Behavior\nResearch Methods, Instruments, & Computers, 33(3):311–320, 2001.\nCarol Neidle, Ashwin Thangali, and Stan Sclaroff. Challenges in development\nof the American Sign Language lexicon video dataset (ASSLVD) corpus. In\n5th workshop on the representation and processing of sign languages: interactions\nbetween corpus and Lexicon, LREC. Citeseer, 2012.\nOpenAI. Chatgpt: Optimizing language models for dialogue, 2022. URL http\ns://openai.com/blog/chatgpt.\nZrajm ¨Oqvist, Nikolaus Riemer Kankkonen, and Johanna Mesch. STS-korpus:\nA sign language web corpus tool for teaching and public use. In Proceed-\nings of the LREC2020 9th Workshop on the Representation and Processing of Sign\nLanguages: Sign Language Resources in the Service of the Language Community,\nTechnological Challenges and Application Perspectives, pages 177–180, Marseille,\nFrance, May 2020. European Language Resources Association (ELRA). ISBN\n979-10-95546-54-2. URL https://www.aclweb.org/anthology/2020.\nsignlang-1.29.\nEllen Ormel and Onno Crasborn. Prosodic correlates of sentences in signed\nlanguages: A literature review and suggestions for new types of studies. Sign\nLanguage Studies, 12(2):279–315, 2012.\nEllen Ormel, Onno Crasborn, Els van der Kooij, Lianne van Dijken, E Nauta,\nJens Forster, and Daniel Stein. Glossing a multi-purpose sign language cor-\npus. In Proceedings of the 4th Workshop on the Representation and Processing of\nSign Languages: Corpora and sign language technologies, pages 186–191, 2010.\nAchraf Othman and Mohamed Jemni. English-asl gloss parallel corpus 2012:\nAslg-pc12. In 5th Workshop on the Representation and Processing of Sign Lan-\nguages: Interactions between Corpus and Lexicon LREC, 2012.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,\nDavid Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for se-\nquence modeling. In North American Chapter of the Association for Computa-\nCHAPTER 11. BIBLIOGRAPHY\n218\ntional Linguistics, 2019. URL https://api.semanticscholar.org/Co\nrpusID:91184134.\nC. Padden. Interaction of Morphology and Syntax in American Sign Language. Out-\nstanding Disc Linguistics Series. Garland, 1988. ISBN 9780824051945. URL\nhttps://books.google.com/books?id=Mea7AAAAIAAJ.\nCarol A Padden. The ASL lexicon. Sign language & linguistics, 1(1):39–60, 1998.\nCarol A Padden and Darline Clark Gunsauls. How the alphabet came to be\nused in a sign language. Sign Language Studies, pages 10–33, 2003.\nCarol A Padden and Tom Humphries. Deaf in America. Harvard University\nPress, 1988.\nAbhilash Pal, Stephan Huber, Cyrine Chaabani, Alessandro Manzotti, and Os-\ncar Koller. On the importance of signer overlap for sign language detection.\narXiv preprint arXiv:2303.10782, 2023.\nPaschalis Panteleris, Iason Oikonomidis, and Antonis Argyros. Using a single\nrgb frame for real time 3d hand pose estimation in the wild. In 2018 IEEE\nWinter Conference on Applications of Computer Vision (WACV), pages 436–445.\nIEEE, 2018.\nGeorge Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan\nTompson, Chris Bregler, and Kevin Murphy. Towards accurate multi-person\npose estimation in the wild. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4903–4911, 2017.\nGeorge Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan\nTompson, and Kevin Murphy. Personlab: Person pose estimation and in-\nstance segmentation with a bottom-up, part-based, geometric embedding\nmodel. In Proceedings of the European Conference on Computer Vision (ECCV),\npages 269–286, 2018.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method\nfor automatic evaluation of machine translation.\nIn Proceedings of the 40th\nAnnual Meeting of the Association for Computational Linguistics, pages 311–318,\nPhiladelphia, Pennsylvania, USA, July 2002. Association for Computational\nLinguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology\n.org/P02-1040.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nCHAPTER 11. BIBLIOGRAPHY\n219\nAlban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. PyTorch: An imperative style, high-performance\ndeep learning library. In Advances in Neural Information Processing Systems 32,\npages 8024–8035. Curran Associates, Inc., 2019a. URL http://papers.n\neurips.cc/paper/9015-pytorch-an-imperative-style-high-pe\nrformance-deep-learning-library.pdf.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d ´Alch´e-\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019b.\nURL\nhttp://papers.neurips.cc/paper/9015-pytorch-an-impera\ntive-style-high-performance-deep-learning-library.pdf.\nCarol J Patrie and Robert E Johnson. Fingerspelled word recognition through rapid\nserial visual presentation: RSVP. DawnSignPress, 2011.\nDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.\n3d human pose estimation in video with temporal convolutions and semi-\nsupervised training. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 7753–7762, 2019.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,\nM. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-\nchine learning in Python. Journal of Machine Learning Research, 12:2825–2830,\n2011.\nLeonid Pishchulin, Arjun Jain, Mykhaylo Andriluka, Thorsten Thorm ¨a hlen,\nand Bernt Schiele.\nArticulated people detection and pose estimation: Re-\nshaping the future. In 2012 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3178–3185. IEEE, 2012.\nElena Antinoro Pizzuto, Paolo Rossini, and Tommaso Russo.\nRepresenting\nsigned languages in written form: Questions that need to be posed. In Chiara\nVettori, editor, Proceedings of the LREC2006 2nd Workshop on the Representation\nand Processing of Sign Languages: Lexicographic Matters and Didactic Scenarios,\nCHAPTER 11. BIBLIOGRAPHY\n220\npages 1–6, Genoa, Italy, May 2006. European Language Resources Associa-\ntion (ELRA). URL https://www.sign-lang.uni-hamburg.de/lrec/\npub/06001.pdf.\nMaja Popovi´c. chrF deconstructed: beta parameters and n-gram weights. In\nProceedings of the First Conference on Machine Translation: Volume 2, Shared\nTask Papers, pages 499–504, Berlin, Germany, August 2016a. Association for\nComputational Linguistics. doi: 10.18653/v1/W16-2341. URL https:\n//www.aclweb.org/anthology/W16-2341.\nMaja Popovi´c. chrF deconstructed: beta parameters and n-gram weights. In\nProceedings of the First Conference on Machine Translation: Volume 2, Shared Task\nPapers, pages 499–504, 2016b.\nMatt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third\nConference on Machine Translation: Research Papers, pages 186–191, Brussels,\nBelgium, October 2018. Association for Computational Linguistics. doi: 10.1\n8653/v1/W18-6319. URL https://aclanthology.org/W18-6319.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan\nCollobert. MLS: A Large-Scale Multilingual Dataset for Speech Research. In\nProc. Interspeech 2020, pages 2757–2761, 2020. doi: 10.21437/Interspeech.2020-\n2826. URL http://dx.doi.org/10.21437/Interspeech.2020-2826.\nOfir Press and Lior Wolf. Using the output embedding to improve language\nmodels. In Proceedings of the 15th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Volume 2, Short Papers, pages 157–163,\nValencia, Spain, April 2017. Association for Computational Linguistics. URL\nhttps://aclanthology.org/E17-2025.\nSiegmund Prillwitz and Heiko Zienert. Hamburg notation system for sign lan-\nguage: Development of a sign writing with computer application. In Cur-\nrent trends in European Sign Language Research. Proceedings of the 3rd European\nCongress on Sign Language Research, pages 355–379, 1990.\nSiegmund Prillwitz, Thomas Hanke, Susanne K¨onig, Reiner Konrad, Gabriele\nLanger, and Arvid Schwarz.\nDGS corpus project–development of a cor-\npus based electronic dictionary german sign language/german. In sign-lang\nat LREC 2008, pages 159–164. European Language Resources Association\n(ELRA), 2008.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Gra-\nham Neubig. When and why are pre-trained word embeddings useful for\nCHAPTER 11. BIBLIOGRAPHY\n221\nneural machine translation?\nIn Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Papers), pages 529–535, New Or-\nleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n10.18653/v1/N18-2084. URL https://aclanthology.org/N18-2084.\nZhiqiang Que, Hiroki Nakahara, Eriko Nurvitadhi, Hongxiang Fan, Chenglong\nZeng, Jiuxi Meng, Xinyu Niu, and Wayne W. C. Luk. Optimizing reconfig-\nurable recurrent neural networks. 2020 IEEE 28th Annual International Sympo-\nsium on Field-Programmable Custom Computing Machines (FCCM), pages 10–18,\n2020.\nZhiqiang Que, Erwei Wang, Umar Marikar, Eric Moreno, Jennifer Ngadiuba,\nHamza Javed, Bartłomiej Borzyszkowski, Thea Klaeboe Aarrestad, Vladimir\nLoncar, Sioni Paris Summers, Maurizio Pierini, Peter Y. K. Cheung, and\nWayne W. C. Luk. Accelerating recurrent neural networks for gravitational\nwave experiments.\n2021 IEEE 32nd International Conference on Application-\nspecific Systems, Architectures and Processors (ASAP), pages 117–124, 2021.\nZhiqiang Que,\nHiroki Nakahara,\nHongxiang Fan,\nHe Li,\nJiuxi Meng,\nKuen Hung Tsoi, Xinyu Niu, Eriko Nurvitadhi, and Wayne W. C. Luk. Re-\nmarn: A reconfigurable multi-threaded multi-core accelerator for recurrent\nneural networks. ACM Transactions on Reconfigurable Technology and Systems,\n16:1 – 26, 2022.\nJavier Ramırez, Jos´e C Segura, Carmen Benıtez, Angel De La Torre, and Antonio\nRubio. Efficient voice activity detection algorithms using long-term speech\ninformation. Speech communication, 42(3-4):271–287, 2004.\nLance Ramshaw and Mitch Marcus. Text chunking using transformation-based\nlearning. In Third Workshop on Very Large Corpora, 1995. URL https://acla\nnthology.org/W95-0107.\nChristian Rathmann and Gaurav Mathur. A featural approach to verb agree-\nment in signed languages. Theoretical Linguistics, 37(3-4):197–208, 2011.\nAli Razavi, A¨aron van den Oord, and Oriol Vinyals. Generating diverse high-\nfidelity images with vq-vae-2. In Neural Information Processing Systems, 2019.\nURL https://api.semanticscholar.org/CorpusID:173990382.\nJoseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi.\nYou only look once: Unified, real-time object detection. 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages 779–788, 2015.\nCHAPTER 11. BIBLIOGRAPHY\n222\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. COMET: A neu-\nral framework for MT evaluation.\nIn Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 2685–2702,\nOnline, November 2020. Association for Computational Linguistics.\ndoi:\n10.18653/v1/2020.emnlp-main.213. URL https://aclanthology.o\nrg/2020.emnlp-main.213.\nKatrin Renz, Nicolaj C Stache, Samuel Albanie, and G¨ul Varol.\nSign lan-\nguage segmentation with temporal convolutional networks. In ICASSP 2021-\n2021 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 2135–2139. IEEE, 2021a.\nKatrin Renz, Nicolaj C Stache, Neil Fox, Gul Varol, and Samuel Albanie. Sign\nsegmentation with changepoint-modulated pseudo-labelling. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n3403–3412, 2021b.\nRobin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Om-\nmer.\nHigh-resolution image synthesis with latent diffusion models.\n2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n10674–10685, 2021.\nCynthia B Roy. Discourse in signed languages. Gallaudet University Press, 2011.\nAlex Salcianu, Andy Golding, Anton Bakalov, Chris Alberti, Daniel Andor,\nDavid Weiss, Emily Pitler, Greg Coppola, Jason Riesa, Kuzman Ganchev,\nMichael Ringgaard, Nan Hua, Ryan McDonald, Slav Petrov, Stefan Istrate,\nand Terry Koo. Compact language detector v3 (cld3). https://github.c\nom/google/cld3, 2016. Accessed: 2023-08-01.\nWendy Sandler. Prosody and syntax in sign languages. Transactions of the philo-\nlogical society, 108(3):298–328, 2010.\nWendy Sandler. The phonological organization of sign languages. Language and\nlinguistics compass, 6(3):162–182, 2012.\nWendy Sandler and Diane Lillo-Martin. Sign language and linguistic universals.\nCambridge University Press, 2006.\nWendy Sandler, Peter Macneilage, Barbara Davis, Kristine Zajdo, New York,\nand Taylor Francis.\nThe syllable in sign language: Considering the other\nnatural language modality. 2008. URL http://sandlersignlab.haifa\n.ac.il/pdf/The syllable in sign language.pdf.\nCHAPTER 11. BIBLIOGRAPHY\n223\nPinar Santemiz, Oya Aran, Murat Saraclar, and Lale Akarun. Automatic sign\nsegmentation from continuous signing via multiple sequence alignment. In\n2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV\nWorkshops, pages 2001–2008. IEEE, 2009.\nBen Saunders, Necati Cihan Camg¨oz, and Richard Bowden. Adversarial train-\ning for multi-channel sign language production. In The 31st British Machine\nVision Virtual Conference. British Machine Vision Association, 2020a.\nBen Saunders, Necati Cihan Camg¨oz, and Richard Bowden. Everybody sign\nnow: Translating spoken language to photo realistic sign language video.\narXiv preprint arXiv:2011.09846, 2020b.\nBen Saunders, Necati Cihan Camg¨oz, and Richard Bowden. Progressive trans-\nformers for end-to-end sign language production. In European Conference on\nComputer Vision, pages 687–705, 2020c.\nBen Saunders, Necati Cihan Camg¨oz, and Richard Bowden. Anonysign: Novel\nhuman appearance synthesis for sign language video anonymisation. In 2021\n16th IEEE International Conference on Automatic Face and Gesture Recognition\n(FG 2021), pages 1–8, 2021. doi: 10.1109/FG52635.2021.9666984.\nAbraham Savitzky and Marcel JE Golay. Smoothing and differentiation of data\nby simplified least squares procedures. Analytical chemistry, 36(8):1627–1639,\n1964.\nAdam Schembri, Jordan Fenlon, Ramas Rentelis, Sally Reynolds, and Kearsy\nCormier. Building the british sign language corpus. Language Documentation\n& Conservation, 7:136–154, 2013.\nAdam Schembri, Kearsy Cormier, and Jordan Fenlon. Indicating verbs as ty-\npologically unique constructions: Reconsidering verb ‘agreement’in sign lan-\nguages. Glossa: a journal of general linguistics, 3(1), 2018.\nJerry C Schnepp, Rosalee J Wolfe, John C McDonald, and Jorge A Toro. Com-\nbining emotion and facial nonmanual signals in synthesized american sign\nlanguage. In Proceedings of the 14th international ACM SIGACCESS conference\non Computers and accessibility, pages 249–250, 2012.\nJerry C. Schnepp, Rosalee J. Wolfe, John C. McDonald, and Jorge A. Toro. Gen-\nerating co-occurring facial nonmanual signals in synthesized american sign\nlanguage. In GRAPP/IVAPP, 2013.\nCHAPTER 11. BIBLIOGRAPHY\n224\nMarc Schulder and Thomas Hanke.\nOpenPose in the Public DGS Corpus.\nProject Note AP06-2019-01, DGS-Korpus project, IDGS, Hamburg University,\nHamburg, Germany, 2019.\nMarc Schulder, Dolly Blanck, Thomas Hanke, Ilona Hofmann, Sung-Eun Hong,\nOlga Jeziorski, Lutz K¨onig, Susanne K¨onig, Reiner Konrad, Gabriele Langer,\nRie Nishio, and Christian Rathmann. Data statement for the Public DGS Cor-\npus. Project Note AP06-2020-01, DGS-Korpus project, IDGS, Hamburg Uni-\nversity, Hamburg, Germany, 2020.\nSchweizerischer\nGeh¨orlosenbund\nSGB-FSS.\nGeh¨orlosenbund\nGeb¨ardensprache-Lexikon.\nhttps://signsuisse.sgb-fss.ch/,\n2023. Accessed on: Monday 2nd December, 2024.\nZed Sevcikova Sehyr, Naomi Caselli, Ariel M Cohen-Goldberg, and Karen Em-\nmorey. The ASL-LEX 2.0 project: A database of lexical and phonological prop-\nerties for 2,723 signs in american sign language. The Journal of Deaf Studies and\nDeaf Education, 26(2):263–277, 2021.\nPrem Selvaraj, Gokul Nc, Pratyush Kumar, and Mitesh Khapra. OpenHands:\nMaking sign language recognition accessible with pose-based pretrained\nmodels across languages. In Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers), pages 2114–2133,\nDublin, Ireland, May 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.acl-long.150. URL https://aclanthology.org/202\n2.acl-long.150.\nRico Sennrich and Biao Zhang. Revisiting low-resource neural machine trans-\nlation: A case study. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 211–221, Florence, Italy, July 2019. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/P19-1021. URL\nhttps://aclanthology.org/P19-1021.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine\ntranslation models with monolingual data. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 86–96, Berlin, Germany, August 2016a. Association for Computational\nLinguistics. doi: 10.18653/v1/P16-1009. URL https://aclanthology.o\nrg/P16-1009.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine trans-\nlation of rare words with subword units.\nIn Proceedings of the 54th An-\nnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nCHAPTER 11. BIBLIOGRAPHY\n225\nPapers), pages 1715–1725, Berlin, Germany, August 2016b. Association for\nComputational Linguistics.\ndoi: 10.18653/v1/P16-1162.\nURL https:\n//aclanthology.org/P16-1162.\nB. Shi, A. Martinez Del Rio, J. Keane, J. Michaux, G. Shakhnarovich D. Brentari,\nand K. Livescu. American sign language fingerspelling recognition in the\nwild. SLT, December 2018.\nB. Shi, A. Martinez Del Rio, J. Keane, D. Brentari, G. Shakhnarovich, and\nK. Livescu. Fingerspelling recognition in the wild with iterative visual at-\ntention. ICCV, October 2019.\nWenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz, Andrew P. Aitken,\nRob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and\nvideo super-resolution using an efficient sub-pixel convolutional neural net-\nwork. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 1874–1883, 2016.\nStuart Shieber and Yves Schabes. Synchronous tree-adjoining grammars. In\nProceedings of the 13th international conference on computational linguistics. Asso-\nciation for Computational Linguistics, 1990.\nStuart M Shieber. Restricting the weak-generative capacity of synchronous tree-\nadjoining grammars. Computational Intelligence, 10(4):371–385, 1994.\nEdgar H Shroyer and Susan P Shroyer. Signs across America: A look at regional\ndifferences in American Sign Language. Gallaudet University Press, 1984.\nSign Time GmbH, Oct 2020. URL https://simax.media/.\nNCSLGR SignStream. Volumes 2–7, 2007.\nTomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh. Hand keypoint\ndetection in single images using multiview bootstrapping. In CVPR, 2017.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks\nfor large-scale image recognition. CoRR, 2015.\nOzge Mercanoglu Sincan and Hacer Yalim Keles. Autsl: A large scale multi-\nmodal turkish sign language dataset and baseline methods. IEEE Access, 8:\n181340–181355, 2020.\nOzge Mercanoglu Sincan, Julio C. S. Jacques Junior, Sergio Escalera, and\nHacer Yalim Keles. Chalearn LAP large scale signer independent isolated\nCHAPTER 11. BIBLIOGRAPHY\n226\nsign language recognition challenge: Design, results and future research. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) Workshops, 2021.\nDaniel Smilkov, Nikhil Thorat, Yannick Assogba, Ann Yuan, Nick Kreeger,\nPing Yu, Kangyi Zhang, Shanqing Cai, Eric Nielsen, David Soergel, et al.\nTensorflow. js: Machine learning for the web and beyond.\narXiv preprint\narXiv:1901.05350, 2019.\nJongseo Sohn, Nam Soo Kim, and Wonyong Sung. A statistical model-based\nvoice activity detection. IEEE signal processing letters, 6(1):1–3, 1999.\nWilliam C Stokoe Jr. Sign Language Structure: An Outline of the Visual Com-\nmunication Systems of the American Deaf. The Journal of Deaf Studies and Deaf\nEducation, 10(1):3–37, 01 1960. ISSN 1081-4159. doi: 10.1093/deafed/eni001.\nURL https://doi.org/10.1093/deafed/eni001.\nStephanie Stoll, Necati Cihan Camg¨oz, Simon Hadfield, and Richard Bowden.\nSign language production using neural machine translation and generative\nadversarial networks. In Proceedings of the 29th British Machine Vision Confer-\nence (BMVC 2018). British Machine Vision Association, 2018.\nStephanie Stoll, Necati Cihan Camg¨oz, Simon Hadfield, and Richard Bowden.\nText2sign: towards sign language production using neural machine transla-\ntion and generative adversarial networks. International Journal of Computer\nVision, pages 1–18, 2020.\nTed Supalla. The classifier system in american sign language. Noun classes and\ncategorization, 7:181–214, 1986.\nValerie Sutton. Lessons in sign writing. SignWriting, 1990.\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolu-\ntional neural networks. In International Conference on Machine Learning, pages\n6105–6114. PMLR, 2019.\nYansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili\nZhao, Jiwen Lu, and Jie Zhou. COIN: A large-scale dataset for comprehensive\ninstructional video analysis. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\nFederico Tavella, Viktor Schlegel, Marta Romeo, Aphrodite Galata, and Angelo\nCangelosi. WLASL-LEX: a dataset for recognising phonological properties\nin American Sign Language. In Proceedings of the 60th Annual Meeting of the\nCHAPTER 11. BIBLIOGRAPHY\n227\nAssociation for Computational Linguistics (Volume 2: Short Papers), pages 453–\n463, Dublin, Ireland, May 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-short.49. URL https://aclanthology.org/2\n022.acl-short.49.\nJ¨org Tiedemann and Santhosh Thottingal. OPUS-MT — Building open transla-\ntion services for the World. In Proceedings of the 22nd Annual Conferenec of the\nEuropean Association for Machine Translation (EAMT), Lisbon, Portugal, 2020.\nIoannis Tsiamas, Gerard I. G´allego, Jos´e A. R. Fonollosa, and Marta Ruiz Costa-\njuss`a.\nSHAS: Approaching optimal segmentation for end-to-end speech\ntranslation. In Proc. Interspeech 2022, pages 106–110, 2022. doi: 10.21437/I\nnterspeech.2022-59.\nDon Tuggener. Incremental coreference resolution for German. PhD thesis, Univer-\nsity of Zurich, 2016.\nUnited Nations. International day of sign languages. https://www.un.org\n/en/observances/sign-languages-day, 2022.\nHamid Vaezi Joze and Oscar Koller. MS-ASL: A large-scale data set and bench-\nmark for understanding american sign language. In The British Machine Vision\nConference (BMVC), September 2019. URL https://www.microsoft.co\nm/en-us/research/publication/ms-asl-a-large-scale-data\n-set-and-benchmark-for-understanding-american-sign-lan\nguage/.\nA¨aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete\nrepresentation learning. ArXiv, abs/1711.00937, 2017. URL https://api.\nsemanticscholar.org/CorpusID:20282961.\nG¨ul Varol, Liliane Momeni, Samuel Albanie, Triantafyllos Afouras, and Andrew\nZisserman. Scaling up sign spotting through sign language dictionaries. In-\nternational Journal of Computer Vision, 130:1416 – 1439, 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing systems, pages 5998–6008,\n2017.\nSatvik Venkatesh, David Moffat, and Eduardo Reck Miranda. You only hear\nonce: a YOLO-like algorithm for audio segmentation and sound event detec-\ntion. Applied Sciences, 12(7):3293, 2022.\nCHAPTER 11. BIBLIOGRAPHY\n228\nVille Viitaniemi, Tommi Jantunen, Leena Savolainen, Matti Karppa, and Jorma\nLaaksonen. S-pot - a benchmark in spotting signs within continuous signing.\nIn Proceedings of the Ninth International Conference on Language Resources and\nEvaluation (LREC’14), pages 1892–1897, Reykjavik, Iceland, May 2014. Euro-\npean Language Resources Association (ELRA). URL http://www.lrec-c\nonf.org/proceedings/lrec2014/pdf/440 Paper.pdf.\nˇSpela Vintar, Boˇstjan Jerko, and Marjetka Kulovec.\nCompiling the slovene\nsign language corpus.\nIn 5th Workshop on the Representation and Processing\nof Sign Languages: Interactions between Corpus and Lexicon. Language Resources\nand Evaluation Conference (LREC), volume 5, pages 159–162, 2012.\nChristian Vogler and Siome Goldenstein. Analysis of facial expressions in amer-\nican sign language. In Proc, of the 3rd Int. Conf. on Universal Access in Human-\nComputer Interaction, Springer, 2005.\nUlrich Von Agris and Karl-Friedrich Kraiss.\nTowards a video corpus for\nsigner-independent continuous sign language recognition. Gesture in Human-\nComputer Interaction and Simulation, Lisbon, Portugal, May, 11, 2007.\nHarry Thomas Walsh, Ben Saunders, and Richard Bowden. Changing the repre-\nsentation: Examining language representation for neural sign language pro-\nduction. In LREC 2022, 2022.\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan\nKautz, and Bryan Catanzaro. Video-to-video synthesis. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2018.\nShih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolu-\ntional pose machines. In CVPR, 2016.\nNkenge Wheatland, Ahsan Abdullah, Michael Neff, Sophie J¨org, and Victor\nZordan. Analysis in support of realistic timing in animated fingerspelling. In\n2016 IEEE Virtual Reality (VR), pages 309–310. IEEE, 2016.\nSherman Wilcox. The phonetics of fingerspelling, volume 4. John Benjamins Pub-\nlishing, 1992.\nSherman\nWilcox\nand\nSarah\nHafer.\nRethinking\nclassifiers.\nemmorey,\nk.(ed.).(2003). perspectives on classifier constructions in sign languages. mah-\nwah, nj: Lawrence erlbaum associates. 332 pages. hardcover., 2004.\nFrank Wilcoxon. Individual comparisons by ranking methods. In Breakthroughs\nin statistics, pages 196–202. Springer, 1992.\nCHAPTER 11. BIBLIOGRAPHY\n229\nPeter Wittenburg, Hennie Brugman, Albert Russel, Alex Klassmann, and Han\nSloetjes. ELAN: a professional framework for multimodality research. In\n5th International Conference on Language Resources and Evaluation (LREC 2006),\npages 1556–1559, 2006.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,\nJulien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art nat-\nural language processing.\nIn Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing: System Demonstrations, pages 38–\n45, Online, October 2020. Association for Computational Linguistics. URL\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.6.\nRosalee Wolfe, Peter Cook, John C McDonald, and Jerry C Schnepp. Linguis-\ntics as structure in computer animation: Toward a more effective synthesis of\nbrow motion in american sign language. Sign Language & Linguistics, 14(1):\n179–199, 2011.\nRosalee J Wolfe, Elena Jahn, Ronan Johnson, and John C McDonald. The case\nfor avatar makeup. 2019.\nRosalee J Wolfe, John C McDonald, Ronan Johnson, Ben Sturr, Syd Klinghof-\nfer, Anthony Bonzani, Andrew Alexander, and Nicole Barnekow. Supporting\nmouthing in signed languages: New innovations and a proposal for future\ncorpus building. In Proceedings of the 7th International Workshop on Sign Lan-\nguage Translation and Avatar Technology: The Junction of the Visual and the Tex-\ntual: Challenges and Perspectives, pages 125–130, 2022.\nRyan Wong, Necati Cihan Camgoz, and R. Bowden. Hierarchical I3D for sign\nspotting. In ECCV Workshops, 2022.\nWorld Federation of the Deaf. World federation of the deaf - our work. https:\n//wfdeaf.org/our-work/, 2022.\nWorld Health Organization. Deafness and hearing loss. https://www.who.\nint/news-room/fact-sheets/detail/deafness-and-hearing-lo\nss, 2021.\nMengzhou Xia, Xiang Kong, Antonios Anastasopoulos, and Graham Neubig.\nGeneralized data augmentation for low-resource translation. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages\nCHAPTER 11. BIBLIOGRAPHY\n230\n5786–5796, Florence, Italy, July 2019. Association for Computational Linguis-\ntics. doi: 10.18653/v1/P19-1579. URL https://www.aclweb.org/antho\nlogy/P19-1579.\nQinkun Xiao, Minying Qin, and Yuting Yin. Skeleton-based chinese sign lan-\nguage recognition and generation for bidirectional communication between\ndeaf and hearing people. Neural Networks, 125:41–55, 2020.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya\nSiddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 483–498, Online, June 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL\nhttps://aclanthology.org/2021.naacl-main.41.\nSijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional\nnetworks for skeleton-based action recognition. In Proceedings of the AAAI\nconference on artificial intelligence, volume 32, 2018.\nWilson Yan, Yunzhi Zhang, P. Abbeel, and A. Srinivas. Videogpt: Video gen-\neration using vq-vae and transformers. ArXiv, abs/2104.10157, 2021. URL\nhttps://api.semanticscholar.org/CorpusID:233307257.\nKayo Yin and Jesse Read.\nBetter sign language translation with STMC-\ntransformer. In Proceedings of the 28th International Conference on Computational\nLinguistics, pages 5975–5989, Barcelona, Spain (Online), December 2020a. In-\nternational Committee on Computational Linguistics. doi: 10.18653/v1/20\n20.coling-main.525. URL https://aclanthology.org/2020.coling\n-main.525.\nKayo Yin and Jesse Read. Attention is all you sign: Sign language translation\nwith transformers. In Sign Language Recognition, Translation and Production\n(SLRTP) Workshop - Extended Abstracts, 2020b.\nKayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe\nAlikhani.\nIncluding signed languages in natural language processing.\nIn\nProceedings of the 59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7347–7360, Online, August 2021. Association\nfor Computational Linguistics. URL https://aclanthology.org/202\n1.acl-long.570.\nCHAPTER 11. BIBLIOGRAPHY\n231\nTing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional\nnetworks: A deep learning framework for traffic forecasting. In International\nJoint Conference on Artificial Intelligence, 2017.\nZahoor Zafrulla, Helene Brashear, Harley Hamilton, and Thad Starner. A novel\napproach to american sign language (ASL) phrase verification using reversed\nsigning. In 2010 IEEE Computer Society Conference on Computer Vision and Pat-\ntern Recognition-Workshops, pages 48–55. IEEE, 2010.\nJan Zelinka and Jakub Kanis. Neural sign language synthesis: Words are our\nglosses. In The IEEE Winter Conference on Applications of Computer Vision, pages\n3395–3403, 2020.\nBiao Zhang, Mathias M¨uller, and Rico Sennrich. SLTUNET: A simple unified\nmodel for sign language translation. In The Eleventh International Conference\non Learning Representations, Kigali, Rwanda, 2023. URL https://openrevi\new.net/forum?id=EBS4C77p 5S.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-\nimage diffusion models, 2023.\nLiwei Zhao, Karin Kipper, William Schuler, Christian Vogler, Norman Badler,\nand Martha Palmer. A machine translation system from english to american\nsign language. In Conference of the Association for Machine Translation in the\nAmericas, pages 54–67. Springer, 2000.\nCe Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Sijie Zhu, Ju Shen, Nasser\nKehtarnavaz, and Mubarak Shah. Deep learning-based human pose estima-\ntion: A survey. ACM Comput. Surv., 56(1), aug 2023. ISSN 0360-0300. doi:\n10.1145/3603618. URL https://doi.org/10.1145/3603618.\nChunting Zhou, Xuezhe Ma, Junjie Hu, and Graham Neubig. Handling syn-\ntactic divergence in low-resource machine translation.\nIn Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 1388–1394, Hong Kong, China, November 2019. Associa-\ntion for Computational Linguistics.\ndoi: 10.18653/v1/D19-1143.\nURL\nhttps://aclanthology.org/D19-1143.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning\nfor low-resource neural machine translation. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language Processing, pages 1568–1575,\nAustin, Texas, November 2016. Association for Computational Linguistics.\nCHAPTER 11. BIBLIOGRAPHY\n232\ndoi: 10.18653/v1/D16-1163. URL https://www.aclweb.org/antholo\ngy/D16-1163.\nInge Zwitserlood, Margriet Verlinden, Johan Ros, Sanny Van Der Schoot, and\nT Netherlands. Synthetic signing for the deaf: Esign. In Proceedings of the\nconference and workshop on assistive technologies for vision and hearing impairment,\nCVHI, 2004.\nתקציר\nשפות סימנים משמשות כאמצעי תקשורת חיוני עבור מיליוני אנשים חירשים וכבדי\nתוך שימוש במודאליות חזותית־תנועתית, הם מעבירים\nשמיעה ברחבי העולם.\nמבנים לשוניים מורכבים באמצעות ניסוחים ידניים בשילוב עם אלמנטים לא\nידניים כמו הבעות פנים ותנועת גוף. למרות העושר הלשוני והחשיבות התרבותית\nשלהן, שפות סימנים נדחקו לעתים קרובות לשוליים על ידי ההתקדמות האחרונה\nבטכנולוגיות של בינה מלאכותית ממוקדת טקסט, כגון תרגום מכונה ומודלים של\nדחיקה זו מגבילה את הגישה לטכנולוגיות אלו עבור אוכלוסייה\nשפות גדולות.\nמשמעותית, ומשאירה אותם מאחור בהתקדמות בבינה מלאכותית מבוססת שפה.\nעיבוד שפת סימנים הוא תחום בינתחומי המורכב מעיבוד שפה טבעית וראייה\nממוחשבת. הוא מתמקד בהבנה חישובית, תרגום והפקה של שפות סימנים. גישות\nמסורתיות הוגבלו לעתים קרובות על ידי שימוש במערכות מבוססות גלוס שהן גם\nספציפיות לשפה וגם לא מתאימות ללכידת הטבע הרב־ממדי של שפת הסימנים.\nמגבלות אלו הפריעו להתפתחות טכנולוגיה המסוגלת לעבד שפות סימנים ביעילות.\nתזה זו שואפת לחולל מהפכה בתחום עיבוד שפת הסימנים על ידי הצעת\nאנו מציעים להשתמש ב־\nפרדיגמה פשוטה שיכולה לגשר על פער טכנולוגי זה.\n, מערכת תמלול אוניברסלית של שפות סימנים, כדי לשמש כקישורSIGNWIRING\nמתווך בין המודאליות החזותית־מחוותית של שפות הסימנים לבין ייצוגים לשוניים\nמבוססי טקסט.\nנועדהSIGNWIRINGבניגוד לגישות מבוססות גלוס, הפרדיגמה שלנו באמצעות\nללכוד במדויק את ההיבטים הרב־ממדיים והבלתי תלויים בשפה של שפות סימנים.\nזה מאפשר יצירת מסגרת מאוחדת וניתנת להרחבה שיכולה להכיל את המגוון\nהלשוני העשיר שנמצא בשפות סימנים שונות ברחבי העולם.\nאנו תורמים ספריות ומשאבים בסיסיים לקהילת עיבוד שפת הסימנים, ובכך\nא\nתקציר\nב\n.מאפשרים חקירה מעמיקה יותר של משימות התרגום וההפקה של שפת הסימנים\nמשימות אלו מקיפות את התרגום של שפת הסימנים מוידאו לטקסט בשפה\nדבורה ולהיפך. באמצעות הערכות אמפיריות, אנו מבססים את היעילות של שיטת\nהתמלול שלנו כציר לאפשר מחקר מהיר וממוקד יותר, שיכול להוביל לתרגומים\nטבעיים ומדויקים יותר במגוון שפות.\nהפרדיגמה שלנו קובעת גבול ברור בין עיבוד שפה טבעית לראייה ממוחשבת\nחלוקה זו משקפת את ההפרדה\nבהקשר הרחב יותר של עיבוד שפת הסימנים.\nהקיימת בין עיבוד שפה טבעית ועיבוד אותות בתחום טכנולוגיות השפה הדבורה.\nעל ידי כך, אנו פותחים את הדלת למאמצי מחקר מדוייקטים יותר בכל תת־\nדיסציפלינה, ובכך מעשירים את המערכת האקולוגית של טכנולוגיות ומתודולוגיות\nהזמינות עבור עיבוד שפת הסימנים.\nהאופי האוניברסלי של הפרדיגמה המבוססת על התמלול שלנו גם סולל את\nהדרך ליישומים רב־לשוניים בזמן אמת בעיבוד שפת הסימנים, ובכך מציע גישה\nכוללת ונגישה יותר לטכנולוגיית שפה. זהו צעד משמעותי לקראת נגישות אוניברסלית,\nהמאפשר טווח רחב יותר של טכנולוגיות שפה מונעות בינה מלאכותית לכלול את\nקהילת החירשים וכבדי השמיעה.\nלסיכום, עבודת גמר זו מציגה גישה חדשה לעיבוד שפת הסימנים, כזו שמטרתה\nעל\nלקבוע סטנדרט חדש לטכנולוגיות שפה כוללניות, בזמן אמת ורב־לשוניות.\nידי גישור על הפער הקיים בין בינה מלאכותית ממוקדת טקסט לעולם החזותי־\nתנועתי של שפות הסימנים, אנו תורמים באופן משמעותי להפיכת בינה מלאכותית\nמבוססת שפה לנגישה אוניברסלית.\nתוכן עניינים\ni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .תקציר\n1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .הקדמה לעיבוד שפת הסימניםI\n3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .הקדמה1\n8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .רקע2\n24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (עבודה מקדימה )ספריות3\n25 . . . . . . . . . . . . . . . . . . . . . . . . . . .ספריה לצפיה, שינוי, ועבודה עם קבצי שלד3.1\n34 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .מאגרי נתונים לשפות הסימנים3.2\n39 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .מדד איכות של תנוחת יד תלת מימדית3.3\n48 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .תמלול של שפת הסימניםII\n50 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .הקדמה4\n52 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .עבודה מקדימה5\n52 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .זיהוי פעילות5.1\n66 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .זיהוי סימנים בודדים5.2\n81 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .תרגום באמצעות גלוס5.3\n93 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .תרגום שפת הסימנים6\n93 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .סגמנטציה6.1\n116 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .תמלול6.2\n120. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .תרגום6.3\n144. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .יצירת שפת הסימנים7\n144. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .מודל בסיס7.1\n166. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .תרגום7.2\n167 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .יצירה7.3\n170 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .אנימציה7.4\n171 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .דיון והשלכותIII\n173 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .אפליקציה לתרגום שפת הסימנים8\n183 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .השלכות לשפות דבורות9\n193 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .מסקנות10\n195 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .רשימת מקורות\nתקציר בעברית . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . א\nתודות\nבראש ובראשונה, הערכתי הכנה למנחה שלי, יואב גולדברג, על ההנחיה, התמיכה\nהאוטונומיה שהוא העניק והמשוב הישיר שלו תרמו באופן משמעותי\nוהעידוד.\nלצמיחתי כמדען.\nהתזה הזאת חייבת את עצם קיומה למעיין גזולי )חזות(, שהכירה לי לראשונה\nזו הייתה שיחה סתמית בנסיעה במכונית שעוררה בי\nאת עולם שפת הסימנים.\nאני אסיר תודה על תפקידה הבלתי מודע של\nעניין והובילה אותי למסע הזה.\nמעיין כזרז של כל המאמץ הזה.\nתודתי העמוקה מגיעה לואלרי סאטון, הממציאה של מערכת כתיבת הסימנים\n, על עבודתה החלוצית ביצירת מערכת תמלול לשונית לשפותSUTTON SIGNWRITING\nסימנים. המערכת שלה שימשה כלי חיוני במחקר שביסוד תזה זו והשפיעה עמוקות\nעל תוצאותיה. ללא תרומותיה, התזה הזו לא הייתה אפשרית. אני מתכבד לבנות\nעל עבודתה פורצת הדרך ואסיר תודה על מחויבותה להנגיש שפות סימנים לכל.\nאני מודה גם ליואניס צוכנטארידיס על שאירח אותי כמתמחה בגוגל במהלך\nמגיפת הקורונה, ולשרה אבלינג, אנט ריוס ומתיאס מולר על תמיכתם הבלתי\nמעורערת ועל האירוח שלי באוניברסיטת ציריך. האדיבות והאירוח שלהם מילאו\nתפקיד מכריע בשמירה על הרווחה הנפשית שלי, ועל כך אני אסיר תודה עמוקה.\nלחבריי למעבדה באוניברסיטת בר־אילן, ובמיוחד ינאי אלעזר, ולנטינה פיאטקין,\nהילה גונן, שאולי רבפוגל ואמיר דוד ניסן כהן, אני מודה לכם על היותכם המעגל\nהחברתי האקדמי שלי בתקופה מאתגרת זו.\nלדוקטורנטים בבלשנות חישובית באוניברסיטת ציריך, במיוחד נעמי אפלי, טנון\nקיו ואנסטסיה שיטארובה, קבלת הפנים החמה והאווירה המכילה שלכם הפכו\nהשפעתם לחיוב על\nאת הזמן שלי שם לא רק לפרודוקטיבי אלא גם למהנה.\nההתפתחות המקצועית ועל הרווחה האישית שלי, ועל כך אני באמת אסיר תודה.\nהוקרה מיוחדת מגיעה לצוות במכון לשפת הסימנים הגרמנית ותקשורת החירשים\nתודה לתומס האנקה שהעניק לי גישה\nשל אוניברסיטת המבורג על פתיחותם.\nונתן לי תמיכה מתמשכת; מריה קופף שלימדה אותי איך לקרואDGSלקורפוס\n; ואניקה הרמן על השלמת פערים בידע שלי במידע לשוניHAMNOSYSולכתוב\nמעניין. החוכמה הקולקטיבית שלכם תרמה רבות לתזה זו.\nהוקרה גדולה לסטודנטים שהנחיתי במהלך עבודת הגמר: זיפן ג׳יאנג מאוניברסיטת\nציריך ורותם שלו ארקושין מאוניברסיטת רייכמן. הנכונות שלכם להיענות לעצתי,\nיחד עם האומץ לאתגר את הרעיונות שלי כשהם לא היו מדויקים, הפכו את המסע\nהזה למעשיר אינטלקטואלית עבור כל הצדדים המעורבים. תודה שהשארתם אותי\nעל קצות האצבעות.\nאני רוצה גם להודות לרבקה נורטון והאנה נויזר על העידוד והתמיכה שהעניקו\nלי את המוטיבציה והביטחון שהייתי צריך כדי להתמיד בזמנים קשים ולהשלים\nאת העבודה הזו.\nלבסוף, אני רוצה להביע את תודתי העמוקה למשפחתי על האהבה, התמיכה\nוהעידוד שלהם לאורך כל המסע שלי.\n,לאלו שתמיד האמינו בי\nולאלה שאהבתי ואיבדתי.\n,עבודה זו בוצעה בהנחייתו של פרופ׳ יואב גולדברג, המחלקה למדעי המחשב\nאוניברסיטת בר־אילן.\nעיבוד שפת הסימנים\nמרובה שפות בזמן אמת\nחיבור לשם קבלת התואר ׳׳דוקטור לפילוסופיה׳׳\nעמית מוריוסף\nהמחלקה למדעי המחשב\nהוגש לסנט של אוניברסיטת בר־אילן\nטבת תשפ׳׳ד\nרמת גן\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-12-02",
  "updated": "2024-12-02"
}