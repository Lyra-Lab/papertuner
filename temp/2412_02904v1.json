{
  "id": "http://arxiv.org/abs/2412.02904v1",
  "title": "Enhancing Trust in Large Language Models with Uncertainty-Aware Fine-Tuning",
  "authors": [
    "Ranganath Krishnan",
    "Piyush Khanna",
    "Omesh Tickoo"
  ],
  "abstract": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing with their impressive reasoning and question-answering\ncapabilities. However, these models are sometimes prone to generating\ncredible-sounding but incorrect information, a phenomenon known as LLM\nhallucinations. Reliable uncertainty estimation in LLMs is essential for\nfostering trust in their generated responses and serves as a critical tool for\nthe detection and prevention of erroneous or hallucinated outputs. To achieve\nreliable and well-calibrated uncertainty quantification in open-ended and\nfree-form natural language generation, we propose an uncertainty-aware\nfine-tuning approach for LLMs. This approach enhances the model's ability to\nprovide reliable uncertainty estimates without compromising accuracy, thereby\nguiding them to produce more trustworthy responses. We introduce a novel\nuncertainty-aware causal language modeling loss function, grounded in the\nprinciples of decision theory. Through rigorous evaluation on multiple\nfree-form question-answering datasets and models, we demonstrate that our\nuncertainty-aware fine-tuning approach yields better calibrated uncertainty\nestimates in natural language generation tasks than fine-tuning with the\nstandard causal language modeling loss. Furthermore, the experimental results\nshow that the proposed method significantly improves the model's ability to\ndetect hallucinations and identify out-of-domain prompts.",
  "text": "ENHANCING TRUST IN LARGE LANGUAGE MODELS\nWITH UNCERTAINTY-AWARE FINE-TUNING\nRanganath Krishnan ∗\nIntel Labs\nPiyush Khanna †\nCarnegie Mellon University\nOmesh Tickoo\nIntel Labs\nABSTRACT\nLarge language models (LLMs) have revolutionized the field of natural language\nprocessing with their impressive reasoning and question-answering capabilities.\nHowever, these models are sometimes prone to generating credible-sounding but\nincorrect information, a phenomenon known as LLM hallucinations. Reliable\nuncertainty estimation in LLMs is essential for fostering trust in their generated\nresponses and serves as a critical tool for the detection and prevention of erro-\nneous or hallucinated outputs. To achieve reliable and well-calibrated uncertainty\nquantification in open-ended and free-form natural language generation, we pro-\npose an uncertainty-aware fine-tuning approach for LLMs. This approach en-\nhances the model’s ability to provide reliable uncertainty estimates without com-\npromising accuracy, thereby guiding them to produce more trustworthy responses.\nWe introduce a novel uncertainty-aware causal language modeling loss function,\ngrounded in the principles of decision theory. Through rigorous evaluation on\nmultiple free-form question-answering datasets and models, we demonstrate that\nour uncertainty-aware fine-tuning approach yields better calibrated uncertainty es-\ntimates in natural language generation tasks than fine-tuning with the standard\ncausal language modeling loss. Furthermore, the experimental results show that\nthe proposed method significantly improves the model’s ability to detect halluci-\nnations and identify out-of-domain prompts.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have shown remarkable success in various natural language pro-\ncessing tasks (Touvron et al., 2023; Gemma et al., 2024; Achiam et al., 2023) and are increas-\ningly becoming ubiquitous in a variety of domains for their decision-making and reasoning abili-\nties (Eigner & H¨andler, 2024). However, their real-world deployment, particularly in high-stakes\nand safety-critical applications, is hindered by challenges such as hallucinations and out-of-domain\nprompts, which can lead to the generation of erroneous or nonsensical outputs. Hallucinations, of-\nten described as plausible-sounding but incorrect or unfaithful model generations (Ji et al., 2023),\npresent a crucial challenge in developing trustworthy systems especially in critical domains such as\nmedical (Ahmad et al., 2023) and legal (Magesh et al., 2024). The ability to recognize out-of-domain\nprompts and to acknowledge the limits of a model’s knowledge base paves the way for building safe\nAI systems (Amodei et al., 2016).\nUncertainty quantification (UQ) in LLMs plays a pivotal role in understanding what the model\nknows and does not know, which is an active area of research for free-form natural language gener-\nation (NLG) (Kadavath et al., 2022; Kuhn et al., 2023; Lin et al., 2024). UQ methods has emerged\nas a step towards determining the trustworthiness of responses generated by LLMs (Fadeeva et al.,\n2023; Plaut et al., 2024; Kadavath et al., 2022). Uncertainty estimation techniques such as semantic\nentropy (Kuhn et al., 2023) have shown to be effective indicators in detecting ‘confabulations’ (Far-\nquhar et al., 2024), a subcategory of hallucinations characterized by the generation of arbitrary and\nincorrect responses.\nThe calibration of uncertainty estimates is crucial for the reliability of LLMs; a well-calibrated\nmodel should correlate low uncertainty with accurate responses and high uncertainty with likely\nincorrect responses. However, recent studies (Xiong et al., 2024; Yang et al., 2024) have revealed\nthat LLM predictions are often poorly calibrated, leading to overconfidence in incorrect outputs.\n∗Correspondence to ranganath.krishnan@intel.com.\n†Work done during internship at Intel.\nPreprint. Under review.\n1\narXiv:2412.02904v1  [cs.CL]  3 Dec 2024\nThis problem is more pronounced in fine-tuned language models (Kong et al., 2020; Liu et al.,\n2024b). Unlike the pre-training phase, where models are exposed to vast amounts of unlabeled\ndata, fine-tuning involves limited labeled data. Consequently, the immense capacity of LLMs can\nlead to overfitting on this limited data, producing overconfident predictions (Kong et al., 2020).\nThis presents a substantial challenge, as the model may produce unreliable uncertainty metrics that\nare influenced by the model’s miscalibrated token confidence. Moreover, evaluating calibration in\nNLG is particularly challenging due to the variable lengths of generated text compared to reference\nsentences. Traditional calibration error metrics (Naeini et al., 2015; Nixon et al., 2019) assume a\nfixed number of outcomes or classes, which aligns well with classification tasks but not with the\nopen-ended nature of NLG, where the number of tokens in generated text and reference sentences\ncan differ. To address this, we exploit the inverse correlation between uncertainty quantification and\nthe quality of generated text, allowing us to perform calibration analysis in NLG settings.\nIn this work, we introduce an uncertainty-aware fine-tuning approach for LLMs, which is grounded\nin decision theory and tailored for free-form natural language generation. Our approach is orthog-\nonal to existing uncertainty quantification methods and is driven by the goal of enhancing the reli-\nability of uncertainty metrics through uncertainty-aware fine-tuning. Specifically, we achieve this\nwith an optimization objective that encourages the model to learn to associate high uncertainty with\nincorrectly generated tokens and low uncertainty with correctly generated tokens, while maximiz-\ning accuracy within the framework of causal language modeling. We show that fine-tuning with\nour calibration objective enhances the reliability of uncertainty quantification in LLMs. The cali-\nbrated uncertainty estimates serve as a crucial tool for enhancing the trustworthiness of generated\nresponses, represents a substantial step towards identifying hallucinations, and improving decision-\nmaking capabilities through selective generation (Ren et al., 2022).\nOur contributions are as follows:\n• We propose uncertainty-aware causal language modeling (UA-CLM) loss function, de-\nsigned for fine-tuning LLMs to produce well-calibrated uncertainty estimates in free-form\nnatural language generation.\n• We conduct uncertainty calibration analysis in free-form NLG settings, employing an in-\nnovative methodology that leverages the inverse correlation between uncertainty quantifi-\ncation and the quality of the generated text.\n• We perform a comprehensive empirical evaluation across four key aspects: hallucination\ndetection; selective generation; out-of-prompt detection; and calibration analysis, demon-\nstrating that UA-CLM significantly enhances the quality of uncertainty estimates in LLMs\nin open-ended and free-form question-answering tasks. Notably, these enhancements in\nuncertainty calibration are achieved without compromising the accuracy when compared\nto standard CLM.\n• In addition, we have also applied our proposed UA-CLM methodology to a large vision\nlanguage model (LVLM), demonstrating its efficacy in the open-ended visual question-\nanswering task. This extension of our work shows the versatility of UA-CLM in handling\nmultimodal inputs and complex tasks beyond text-based question-answering, showcasing\nit’s utilty to wider range of applications.\n2\nBACKGROUND AND RELATED WORKS\n2.1\nUNCERTAINTY ESTIMATION IN NATURAL LANGUAGE GENERATION\nWe refer to Abdar et al. (2021); Gawlikowski et al. (2023) for surveys on uncertainty quantification\n(UQ) in deep learning. In machine learning models, predictive uncertainty is composed of two\nprimary sources: epistemic uncertainty, associated with model’s lack of knowledge and aleatoric\nuncertainty - the inherent noise in the data or observation. As LLMs continue to evolve rapidly,\nthere is an increasing interest in enhancing our understanding of the uncertainty associated with\nLLM responses for developing trustworthy and reliable systems (Fadeeva et al., 2023). UQ methods\nfrom deep learning can be effectively applied to structured natural language processing tasks, such\nas text classification (Xiao & Wang, 2019) and multiple-choice question answering (Kumar et al.,\n2023). However, the application of these methods to free-form natural language generation presents\ndistinct challenges.\n2\nThe landscape of UQ in free-form NLG is a dynamic field of research, where methodologies are\ngenerally bifurcated into white-box (Fomicheva et al., 2020; Kuhn et al., 2023) and black-box ap-\nproaches (Lin et al., 2024; Xiong et al., 2024). White-box methods necessitate access to the model’s\nlogits, or likelihood scores, or other internals of LLMs. Black-box methods rely solely on the analy-\nsis of the text sequences generated by the LLMs. Recent work (Tian et al., 2023) explores prompting\ntechniques in reinforcement learning from human feedback (RLHF) language models to explicitly\nelicit verbalized response confidence. In contrast, another line of research has explored unsupervised\nmethods for UQ from the models by utilizing confidence (Plaut et al., 2024), perplexity (Fomicheva\net al., 2020), token entropy (Malinin & Gales, 2021) and semantic entropy (Kuhn et al., 2023) to\nquantify uncertainty in LLM responses. Previous research (Minderer et al., 2021) has shown that\nconfidence or entropy measures can be susceptible to poor calibration and may not fully reflect\na model’s underlying uncertainties. Our work focuses on improving these uncertainty metrics in\nwhite-box settings by better calibrating the language models with uncertainty-aware finetuning.\n2.2\nMODEL CALIBRATION\nCalibration is important in applications where decision-making relies on not just the model’s pre-\ndictions, but also on the trustworthiness of its uncertainty scores. It is important to capture well-\ncalibrated uncertainty estimates of a model for creating reliable and trustworthy systems. Model\ncalibration is a well-explored area of research in deep learning, with a variety of strategies proposed\nto enhance the calibration of deep neural networks for classification and regression tasks. These\nstrategies include post-hoc rescaling techniques (Guo et al., 2017; Kull et al., 2017), which adjust\nthe model’s predictions to better align with true event likelihoods; data augmentation, which en-\nriches the training dataset to promote generalization (Thulasidasan et al., 2019; Hendrycks et al.,\n2020); and probabilistic modeling approaches that integrate uncertainty directly into the model’s\narchitecture (Blundell et al., 2015; Lakshminarayanan et al., 2017). Other line of works utilize ex-\nplicit calibration loss functions during training to directly optimize for calibration (Kumar et al.,\n2018; Krishnan & Tickoo, 2020; Mukhoti et al., 2020; Karandikar et al., 2021), that has resulted in\nbetter calibrated models.\nCalibration of LLMs for natural language processing tasks is an ongoing area of research. Prior\nworks have largely focused on refining LLMs for structured tasks like text classification (Kong\net al., 2020) or multiple-choice question answering (Desai & Durrett, 2020; Jiang et al., 2021). Stud-\nies like the one by Xiong et al. (2024) have highlighted that despite the impressive performance of\nfoundational LLMs on a wide array of tasks, these models often exhibit poor calibration, particularly\nexhibiting overconfidence in their predictions. As LLMs are increasingly used in natural language\ngeneration tasks, new calibration techniques (Geng et al., 2024) are emerging to enhance the relia-\nbility of the generated text. More recently, Liu et al. (2024b) introduced a calibration technique for\nLLMs that trains a single linear layer over the model’s last hidden layer representations to predict\na bias term, which is then used to adjust the model’s logits and alter the generation confidence for\nshort-form and long-form responses. Band et al. (2024) propose a training objective for linguistic\ncalibration, utilizing reinforcement learning to optimize and calibrate long-form text generations.\nKapoor et al. (2024) proposed a calibration tuning method designed for LLMs in multiple-choice\nquestion-answering settings. Prior work by Liu et al. (2024b) has shown that standard fine-tuning\nof LLMs can lead to poorer calibration. The calibration of LLMs for free-form text generation, as\nwell as uncertainty-aware fine-tuning, represents a significant open area of research. Our work ad-\ndresses this gap by developing an uncertainty-aware fine-tuning method for LLMs, fine-tuning less\nthan 1% of the model parameters, to achieve well-calibrated models for free-form natural language\ngeneration.\n2.3\nFINE-TUNING LARGE LANGUAGE MODELS\nWith the emergence of foundation models, fine-tuning have become a common practice in the field\nof natural language processing, enabling the adaptation of general-purpose pre-trained models to\nspecialized tasks and domains. As fine-tuning a LLM with billions of parameters can be resource-\nintensive, parameter-efficient fine-tuning (Mangrulkar et al., 2022) strategies have been proposed.\nThese parameter-efficient fine-tuning techniques also mitigate catastrophic forgetting more effec-\ntively in comparison to full fine-tuning (Wang et al., 2022). One approach is to update only a subset\nof the model’s parameters, such as adapter modules (Houlsby et al., 2019) or Low-Rank Adaptation\n3\n(LoRA) (Hu et al., 2022), where only a small set of parameters are updated while the pre-trained\nweights are frozen. Another strategy is prompt-based fine-tuning (Liu et al., 2023), where models\nare conditioned on task-specific prompts to guide the text generation without model parameter up-\ndates. We leverage LoRA strategy to illustrate our proposed uncertainty-aware finetuning in this\nwork.\n3\nUNCERTAINTY-AWARE CAUSAL LANGUAGE MODELING\nMotivated by the need to overcome the challenges of uncertainty miscalibration (Xiong et al., 2024)\nin Large Language Models (LLMs) and the increasing trend of fine-tuning pre-trained foundational\nmodels for domain-specific adaptation — where fine-tuned LLMs often exhibit overconfidence in\ntheir predictions (Kong et al., 2020) — we propose a novel uncertainty calibration fine-tuning ap-\nproach for natural language generation settings. We introduce a novel uncertainty-aware causal\nlanguage modeling loss based on the principles of decision theory (Murphy, 2012). Our fine-tuning\napproach emphasizes increasing the uncertainty for wrong token predictions, while optimizing for\naccuracy and certainty for correct token predictions. Decision theory offers a mathematical and\ntheoretical framework that guides to achieve optimal predictions by employing a task-specific util-\nity function. Within the decision theory framework, our task is to generate natural language text\naccompanied by reliable uncertainty estimates. The utility function in this scenario is represented\nby the uncertainty-aware optimization objective function that is aimed at producing well-calibrated\nuncertainty estimates for causal language modeling. We design a differentiable loss function that\nincentivizes the model to yield low uncertainty when it generates correct tokens, and encourages the\nmodel to exhibit high uncertainty when it is at risk of predicting the next token incorrectly.\nIn causal language modeling, the goal is to predict the next token in a sequence given the previous\ntokens. Given a sequence of tokens [w1, w2, . . . , wT ], where T is the length of the sequence and each\ntoken wi is an element from a fixed vocabulary of size V, the model aims to learn the conditional\nprobability distribution Pθ(wi|w0:i-1) for each token wi given the preceding set of tokens w0:i-1;\nwhere, θ represents the parameters of the LLM. The loss function for standard causal language\nmodeling (CLM) is typically the negative log-likelihood as defined in Equation 1 below.\nLθ\nCLM := −1\nT\nT\nX\ni=0\nlog Pθ(wi|w0:i−1)\n(1)\nDesideratum:\nThe desired and ideal outcome in causal language modeling is to achieve a state\nwhere every correctly generated token is assigned low predictive uncertainty, and high predictive\nprobability, reflecting the model’s high confidence in its accuracy. Conversely, for every token that\nis generated incorrectly, the model should assign high uncertainty, and low predictive probability,\ndenoting low confidence in these instances. This ensures that the model’s confidence levels and\nuncertainty estimates are perfectly calibrated with the actual correctness of its predictions.\nWe define the uncertainty-aware causal language modeling (UA-CLM) loss in Equation 2 based on\nthe above desideratum. The loss function captures the trade-off between predictive accuracy and\nuncertainty calibration, and is composed of two terms: one that deals with incorrectly generated\ntokens and one that deals with correctly generated tokens.\nLθ\nUA-CLM := −1\n| eC|\nX\ni ∈e\nC\nPθ(wi|w0:i-1) log (tanh (Hi))\n|\n{z\n}\nUtility function for incorrect tokens\n−1\n|C|\nX\ni ∈C\n(1 −Pθ(wi|w0:i-1)) log (1 −tanh (Hi))\n|\n{z\n}\nUtility function for correct tokens\n(2)\nwhere,\nHi := −\nV\nX\nj=1\nPθ(wj\ni |w0:i-1) log Pθ(wj\ni |w0:i-1)\n(3)\n4\nHere, Hi is the entropy of the probability distribution of the ith token wi given the previous tokens\nw0:i−1 in the sequence, wi is the ground-truth reference token, C := {i | wi = wi} is a set of indices\ncorresponding to correctly predicted tokens, eC := {i | wi ̸= wi} is a set of indices corresponding\nto incorrectly predicted tokens, V is the size of the vocabulary, wj\ni is the jth token in the vocabulary,\nand Pθ(wj\ni |w0:i−1) is the predicted probability of the jth token in the vocabulary. The hyperbolic\ntangent function is employed to scale the token entropy values due to its smooth gradient properties\n(Krishnan & Tickoo, 2020), ensuring that tanh(Hi) lies in the interval [0, 1].\nThe loss function in Equation 2 offers theoretical guarantees as an optimization objective and sat-\nisfies the desideratum, converging to a perfect value of zero when all correctly generated tokens\nhave a predictive probability of 1 (indicating high confidence) and scaled predictive entropy of 0\n(implying low uncertainty), while all incorrectly generated tokens have a predictive probability of 0\n(representing low confidence) and scaled predictive entropy of 1 (implying high uncertainty). The\ndifferentiable utility functions in the UA-CLM loss as shown in Equation 2 steers the predictive\nprobabilities and uncertainty estimates to align with the accuracy of subsequent token predictions in\nautoregressive models. When the uncertainty estimates in the predicted tokens are misaligned, the\nloss increases, thereby directing the stochastic gradient computations to drive the loss towards min-\nimization. This loss reduces when the uncertainty estimates conform to the desideratum, enabling\nthe model to produce well-calibrated uncertainties while maximizing accuracy.\nAlgorithm 1 Uncertainty-aware fine-tuning in LLMs\n1: Input: Pre-trained LLM M with parameters ϕ, LoRA parameters θ, learning rate η, epochs E,\ntraining data D\n2: Output: Uncertainty calibrated fine-tuned LLM M′\n3: Initialize LoRA parameters θ, and freeze ϕ\n4: for epoch = 1 to E do\n5:\nfor each batch B ⊆D do\n6:\nCompute forward pass to get Pϕ+θ(wi|w0:i-1) and token uncertainty estimate Hi\n7:\nCompute UA-CLM loss Lϕ+θ\nUA-CLM\n▷Equation 2\n8:\nCompute gradients of loss function w.r.t. θ, ∇θLϕ+θ\nUA-CLM\n9:\nUpdate LoRA parameters: θ ←θ −η · ∇θLϕ+θ\nUA-CLM\n10:\nend for\n11: end for\n12: θ∗←θ\n13: M′ ←M with updated LoRA parameters θ∗\n14: return M′\nOur proposed UA-CLM loss is designed to be agnostic to various parameter-efficient fine-tuning\nmethods in LLMs; however, in this paper, we leverage and illustrate its application through Low-\nRank Adaptation (LoRA) (Hu et al., 2022) as described in Algorithm 1. By incorporating uncer-\ntainty directly into the loss function, the model can not only learn to improve accuracy but also to\nunderstand a meaningful representation of uncertainty in its predictions. This dual emphasis on ac-\ncuracy and uncertainty in the optimization objective ensures that the model’s uncertainty estimates\nare closely aligned with the actual predictive accuracy of the generated tokens, leading to improved\nuncertainty calibration in natural language generation.\n4\nEXPERIMENTS AND RESULTS\nWe perform extensive empirical evaluation to compare our proposed uncertainty-aware causal lan-\nguage modeling (UA-CLM) fine-tuning method to the standard causal language modeling (CLM)\nfine-tuning, pre-trained baseline, unlikelihood training (ULT) (Welleck et al., 2020), and calibration\ntuning (CT) (Kapoor et al., 2024) methods. We evaluate on open-ended, and free-form natural lan-\nguage generation tasks. Our comprehensive evaluation rigorously assesses the quality of uncertainty\nestimates and the quality of the generated text. This includes an analysis of broadly four aspects:\nhallucination detection, uncertainty-guided selective generation, out-of-domain prompt detection,\nand calibration analysis based on the inverse correlation between the uncertainty estimates and the\nquality of generated text.\n5\n(a) Hallucination detection\n(b) Selective generation\nFigure 1: The proposed Uncertainty-aware Causal Language Modeling (UA-CLM) outperforms\nstandard Causal Language Modeling (CLM) in all four UQ metrics across various models. The\nperformance is evaluated using AUROC for hallucination detection and AUARC for selective gen-\neration based on four distinct UQ metrics.\n4.1\nEXPERIMENTAL SETTINGS\nDatasets\nWe utilize free-form question-answering (QA) datasets to evaluate the proposed methods\non LLMs: CoQA (Reddy et al., 2019), an open-book conversational QA dataset; TriviaQA (Joshi\net al., 2017), a reading comprehension QA dataset. These datasets are frequently utilized bench-\nmarks for evaluating uncertainty quantification (UQ) in LLMs for natural language generation, as\nevidenced by prior works from Kuhn et al. (2023), Lin et al. (2024), and Farquhar et al. (2024).\nWe employ the OK-VQA dataset (Marino et al., 2019), an open-ended visual question-answering\n(VQA) dataset to extend our evaluation to large vision language models (LVLMs), thereby pro-\nviding a comprehensive analysis of our approach across diverse open-ended free-form QA tasks.\nAdditionally, we use BioASQ (Krithara et al., 2023), a biomedical question-answering dataset for\nthe evaluation of out-of-domain prompt detection. In our experiments, we utilize the development\nsplit of the CoQA dataset, which contains approximately 8,000 question-answer pairs, the validation\nsplit of TriviaQA with around 10,000 question-answer pairs, and the validation split of OK-VQA,\ncomprising roughly 5,000 question-answer pairs along with their corresponding images. For each\ndataset, we allocate 20% of the data for fine-tuning purposes, while the remaining 80% serve as the\ntest sets for evaluation. We use a standard prompt across all datasets, more details on the datasets\nand the prompt are provided in Appendix A.1.1 and A.1.2, respectively.\nModels\nWe use the Llama-2 models with 7B and 13B parameters (Touvron et al., 2023) and the\nGemma model with 2B parameters (Gemma et al., 2024) for the free-form QA experiments. Addi-\ntionally, we utilize the LLaVA-1.5 model with 7B parameters (Liu et al., 2024a) for the open-ended\nvisual question-answering task.\nFine-tuning\nWe perform parameter-efficient fine-tuning using the Low-rank Adaptation (LoRA)\nframework (Hu et al., 2022), where less than 1% of model parameters are trainable. The models\nundergo fine-tuning as described in Section 3 for the uncertainty-aware causal language modeling\nmethod. For comparative purposes, we also fine-tune models using the standard cross-entropy loss to\nevaluate against the standard causal language modeling fine-tuning method. For all our experiments,\nthe models are fine-tuned for a concise duration of 3 epochs, utilizing only 20% of the data split.\nThe optimization is carried out using the AdamW optimizer (Loshchilov & Hutter, 2019), with an\ninitial learning rate of 1e-4, a weight decay of 0.001, and a warm-up ratio of 0.03. We follow the\nsame setup for both CLM and UA-CLM methods for a fair comparison. During the fine-tuning\nprocess, only the LoRA parameters are updated, while all other model parameters remain frozen.\nWe provide more details on the hyperparameters and implementation in Appendix A.1.3, to facilitate\nthe reproducibility of the results.\n6\nTable 1: Evaluation of uncertainty quantification: Comparative analysis of the proposed Uncertainty-aware\nCausal Language Modeling (UA-CLM) with standard Causal Language Modeling (CLM), pre-trained base-\nline, UnLikelihood Training (ULT) (Welleck et al., 2020), and Calibration Tuning (CT) (Kapoor et al., 2024)\nmethods (the best values are in bold). The comparison spans different datasets and models, with quality of\nuncertainty quantification evaluated using the Area Under the Receiver Operating Characteristic (AUROC) and\nthe Area Under the Accuracy-Rejection Curve (AUARC) based on four different uncertainty metrics.\nDataset\nModel\nFinetuning\nMethod\nAUROC ↑(Hallucination detection)\nAUARC ↑(Area under accuracy-rejection curve)\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nCoQA\nLlama-2-7B\nPre-trained\n0.5813\n0.6324\n0.6686\n0.7467\n0.8361\n0.8606\n0.9348\n0.9411\nCLM\n0.6252\n0.6320\n0.6635\n0.6889\n0.9435\n0.9444\n0.9508\n0.9530\nULT\n0.5790\n0.5915\n0.6793\n0.6495\n0.9212\n0.9219\n0.9315\n0.9326\nCT\n0.6175\n0.6571\n0.6706\n0.7292\n0.8603\n0.8780\n0.9029\n0.9075\nUA-CLM\n0.6955\n0.7398\n0.7413\n0.7741\n0.9603\n0.9657\n0.9699\n0.9716\nLlama-2-13B\nPre-trained\n0.6027\n0.6404\n0.6679\n0.7111\n0.8672\n0.8940\n0.9137\n0.9209\nCLM\n0.6302\n0.6348\n0.6815\n0.6910\n0.9579\n0.9584\n0.9659\n0.9661\nULT\n0.6323\n0.6523\n0.6883\n0.7158\n0.9510\n0.9534\n0.9592\n0.9612\nCT\n0.5299\n0.5599\n0.6072\n0.6958\n0.8497\n0.8647\n0.8944\n0.9200\nUA-CLM\n0.6701\n0.7255\n0.7363\n0.7694\n0.9645\n0.9700\n0.9784\n0.9792\nGemma-2B\nPre-trained\n0.7073\n0.7089\n0.6962\n0.7635\n0.9271\n0.9339\n0.9235\n0.9452\nCLM\n0.7723\n0.7606\n0.7295\n0.7618\n0.9468\n0.9454\n0.9619\n0.9655\nULT\n0.7097\n0.6921\n0.6540\n0.7162\n0.9172\n0.9152\n0.9093\n0.9168\nUA-CLM\n0.7780\n0.7837\n0.7358\n0.7871\n0.9652\n0.9668\n0.9671\n0.9672\nTriviaQA\nLlama-2-7B\nPre-trained\n0.7687\n0.8220\n0.8191\n0.8315\n0.8050\n0.8259\n0.8251\n0.8369\nCLM\n0.8135\n0.8192\n0.8108\n0.8371\n0.8617\n0.8615\n0.8558\n0.8630\nULT\n0.7676\n0.8003\n0.8004\n0.8276\n0.8273\n0.8418\n0.8448\n0.8519\nCT\n0.7714\n0.8211\n0.8037\n0.8233\n0.8571\n0.8812\n0.8769\n0.8834\nUA-CLM\n0.8293\n0.8393\n0.8197\n0.8423\n0.8879\n0.8927\n0.8780\n0.8934\nLlama-2-13B\nPre-trained\n0.7984\n0.8123\n0.7801\n0.8365\n0.8441\n0.8574\n0.8584\n0.8587\nCLM\n0.8264\n0.8333\n0.7971\n0.8407\n0.8798\n0.8807\n0.8708\n0.8829\nULT\n0.8240\n0.8485\n0.8245\n0.8456\n0.8949\n0.9055\n0.9013\n0.9063\nCT\n0.7338\n0.7897\n0.7991\n0.8222\n0.8460\n0.8914\n0.9007\n0.9019\nUA-CLM\n0.8297\n0.8352\n0.8033\n0.8447\n0.9200\n0.9254\n0.9155\n0.9252\nGemma-2B\nPre-trained\n0.7633\n0.7719\n0.7920\n0.8127\n0.6912\n0.7225\n0.7127\n0.7279\nCLM\n0.8030\n0.8138\n0.7989\n0.8018\n0.7256\n0.7251\n0.7162\n0.7198\nULT\n0.7935\n0.8134\n0.7912\n0.8035\n0.7212\n0.7429\n0.7246\n0.7413\nUA-CLM\n0.8085\n0.8211\n0.7960\n0.8228\n0.7373\n0.7436\n0.7258\n0.7453\nOK-VQA\nLLaVA-1.5-7B\nCLM\n0.5504\n0.5419\n0.5455\n0.5370\n0.5809\n0.5781\n0.5790\n0.5747\nUA-CLM\n0.6001\n0.5984\n0.6106\n0.6638\n0.5989\n0.5965\n0.6012\n0.6265\nTable 2: Generated text quality and calibration evaluation: Comparative analysis of Uncertainty-aware Causal\nLanguage Modeling (UA-CLM) fine-tuning method with standard Causal Language Modeling (CLM) fine-\ntuning, pre-trained baseline, UnLikelihood training (ULT) (Welleck et al., 2020) and Calibration Tuning (CT)\n(Kapoor et al., 2024) methods. The results in the table indicate that UA-CLM achieves higher ROUGE-L and\naccuracy, and lower expected calibration error (ECE) as compared to other methods.\nFinetuning\nMethod\nLlama-2-7B (CoQA)\nLlama-2-7B (TriviaQA)\nLlama-2-13B (CoQA)\nLlama-2-13B (TriviaQA)\nROUGE-L ↑\nAccuracy ↑\nECE ↓\nROUGE-L ↑\nAccuracy ↑\nECE ↓\nROUGE-L ↑\nAccuracy ↑\nECE ↓\nROUGE-L ↑\nAccuracy ↑\nECE ↓\nPret-trained\n0.7449\n0.8350\n0.0561\n0.6654\n0.7048\n0.2304\n0.7832\n0.8550\n0.0559\n0.7160\n0.7610\n0.2133\nCLM\n0.8886\n0.9253\n0.0343\n0.6037\n0.6529\n0.2407\n0.9106\n0.9406\n0.0323\n0.6588\n0.6967\n0.2241\nULT\n0.8409\n0.8950\n0.0588\n0.6121\n0.6586\n0.3111\n0.8771\n0.9250\n0.0595\n0.6875\n0.7309\n0.1517\nCT\n0.7437\n0.8125\n0.0410\n0.6600\n0.6987\n0.2276\n0.8022\n0.8725\n0.0992\n0.7018\n0.7429\n0.1937\nUA-CLM\n0.8882\n0.9264\n0.0094\n0.6679\n0.7108\n0.2090\n0.9118\n0.9461\n0.0084\n0.7277\n0.7710\n0.1365\nUQ metrics\nTo assess the uncertainty quantification from CLM and UA-CLM in the context\nof free-form text generation, we employ four widely used metrics as the baselines: mean token\nentropy (Fomicheva et al., 2020), perplexity (Fadeeva et al., 2023), predictive entropy (Malinin\n& Gales, 2021) and semantic entropy (Kuhn et al., 2023). The predictive entropy and semantic\nentropy are estimated by generating 5 stochastic sequences from the model, each obtained through\ntemperature sampling with a temperature setting of T=0.3.\n4.2\nEVALUATION AND RESULTS\nHallucination detection\nWe evaluate the performance of detecting confabulations (hallucina-\ntions) in the generated text using uncertainty estimates. Confabulations (Farquhar et al., 2024;\nBerrios, 1998) are a subset of hallucinations, characterized by LLMs making fluent claims that are\n7\nFigure 2: Uncertainty calibration analysis: Spearman’s rank correlation coefficient and Pearson\ncorrelation coefficient between uncertainty estimates and generated text quality (ROUGE-L) scores\nfor free-form open-ended question answering. Stronger negative correlation is desired for well-\ncalibrated uncertainty quantification.\nboth wrong and arbitrary. Hallucination detection is a binary classification task to distinguish be-\ntween correct and hallucinated (incorrect) responses based on the uncertainty estimate. We use Area\nUnder the Receiver Operating Characteristic (AUROC) (Davis & Goadrich, 2006) to evaluate the\nquality of uncertainty quantification in terms of the model’s capability to detect hallucinations, fol-\nlowing the methodology used in (Farquhar et al., 2024). A higher AUROC indicates that the model\nis more effective at identifying correct responses and flagging hallucinations. The bar plot depicted\nin Figure 1(a) provides a visual representation of the AUROC performance for both the CLM and\nUA-CLM methods. It shows the improvement in various uncertainty quantification metrics across\nthree distinct LLMs, underscoring the enhanced reliability of uncertainty estimates in the generated\ntext achieved by UA-CLM. This plot consolidates the AUROC performance metrics from the CoQA\nand TriviaQA datasets, with specific numbers for each dataset and each LLM provided in Table 1.\nThis table also includes the results for the OK-VQA dataset when evaluated with LVLM, offer-\ning a comprehensive study across different datasets, models, and uncertainty quantification metrics.\nWe observe that the UA-CLM method exhibits a significant improvement in hallucination detection\nperformance of up to 17.1% on QA tasks, and upto 23.6% on VQA task, over the standard CLM\nmethod.\nNotably, these enhancements with UA-CLM are achieved without compromising quality of the gen-\nerated text or the overall accuracy as presented in Table 2. We refer to Appendix A.2 for details of\ntext quality metrics, and additional evaluation metrics can be found in Appendix A.4.\nUncertainty-guided selective generation\nThe ability of a large language model to decide when\nto generate a response and when to abstain from providing one, based on its uncertainty estimates is\ncrucial for building trustworthy and reliable generative AI models. This capability enables models to\nrecognize and communicate their limitations. Selective generation (Ren et al., 2022) plays an impor-\ntant role in scenarios where providing an incorrect response could have negative consequences, such\nas in medical diagnosis, legal advice, or safety-critical information systems. We adopt the method-\nology proposed by Farquhar et al. (2024) and utilize the Area Under the Accuracy-Rejection Curve\n(AUARC) (Nadeem et al., 2009) to evaluate both the performance of the model and the quality of\nuncertainty estimates in the context of selective generation. AUARC serves as a valuable metric for\nevaluating the quality of a model’s uncertainty estimates and its decision-making ability regarding\nwhen to make predictions and when to abstain due to high uncertainty. The bar plots presented in\nFigure 1(b), along with the numerical data provided in Table 1 shows a significant improvement in\nthe AUARC scores achieved by the UA-CLM. This notable enhancement in AUARC indicates the\nuncertainty estimates with UA-CLM’s can lead to better informed downstream decision-making.\nCorrelation between uncertainty estimates and generated text quality\nCalibration serves as a\nmechanism for ensuring the quality of uncertainty estimates. It is not feasible to assess the gen-\nerated tokens calibration due to the potential mismatch in the number of tokens between the gen-\n8\nFigure 3: Accuracy versus Expected Calibration Error (ECE) comparison between UA-CLM, CLM,\nand pre-trained baseline across different LLM architectures on CoQA and TriviaQA datasets. The\nideal model should have high accuracy and low ECE, indicating accurate predictions with well-\ncalibrated uncertainty quantification (upper-left of the plot). The ECE of models fine-tuned with UA-\nCLM shows significant improvement compared to the pre-trained baseline and CLM fine-tuning.\nTable 3: Out-of-domain detection: Evaluation with Biomedical question answering (BioASQ) as\nout-of-domain dataset on Llama-2-7B finetuned with CoQA dataset. The table shows the com-\nparison of CLM and UA-CLM with AUROC and AUPR scores for out-of-domain detection using\ndifferent uncertainty metrics.\nMethod\nAUROC ↑(Out-of-domain detection)\nAUPR ↑(Out-of-domain detection)\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nCLM\n0.7828\n0.7661\n0.7500\n0.7902\n0.8124\n0.7928\n0.8570\n0.8422\nUA-CLM\n0.9025\n0.8931\n0.7763\n0.9061\n0.9235\n0.9117\n0.8958\n0.9250\nerated text and the ground truth reference. Hence we employ Spearman’s rank correlation coeffi-\ncient (Zwillinger & Kokoska, 1999) and Pearson correlation coefficient (Benesty et al., 2009) to\nevaluate the reliability of uncertainty estimates and examine how well these estimates align with\nthe quality of the generated text, this is a novel methodology for calibration analysis to circumvent\nthe challenges posed by varying token lengths in the generative outputs. We analyze the negative\ncorrelation between uncertainty estimates and ROUGE-L (Lin & Och, 2004) scores, a widely recog-\nnized metric for gauging generated text quality. A model with well-calibrated uncertainty estimates\nshould demonstrate a strong negative correlation between uncertainty estimates and the generated\ntext quality. As depicted in Figure 2, our findings from the CoQA, TriviaQA and OK-VQA datasets\nreveal a consistent negative correlation: the uncertainty estimates increase as the ROUGE-L scores\ntend to decrease, and vice-versa. The results indicate that fine-tuning with UA-CLM show a more\npronounced inverse correlation between uncertainty and text quality compared to standard fine-\ntuning methods. We also estimate the sentence-level calibration with Expected Calibration Error\n(ECE) (Naeini et al., 2015) based on the correctness of generated response. The results in Ta-\nble 2 shows that UA-CLM fine-tuning yields lower ECE as compared to other methods. Figure 3\nshows the accuracy versus ECE plots for CoQA and TriviaQA datasets across different models for\npre-trained baseline, CLM and UA-CLM fine-tuning methods. These results from the calibration\nanalysis demonstrates the effectiveness of uncertainty-aware fine-tuning to obtain better calibrated\nuncertainty in free-form text generation tasks.\nTable 4: Evaluating generalization of fine-tuning methods on QA task and biography generation task.\nCoQA −→TriviaQA\nTriviaQA −→CoQA\nCoQA −→BioGen\nMethod\nAUROC ↑(Hallucination detection)\nAUROC ↑(Hallucination detection)\nBERT F1 ↑\nECE ↓\nAUROC ↑(Hallucination detection)\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nToken\nEntropy\nPerplexity\nSemantic\nEntropy\nCLM\n0.7201\n0.7847\n0.7532\n0.7874\n0.5952\n0.6349\n0.6665\n0.7146\n0.7394\n0.2511\n0.5653\n0.5793\n0.5281\nUA-CLM\n0.8271\n0.8261\n0.7880\n0.8146\n0.6456\n0.6824\n0.7154\n0.7528\n0.7405\n0.1713\n0.6135\n0.6123\n0.5354\n9\nOut-of-domain detection\nIn our experiments, we assess the model’s capability to identify whether\na given prompt is out-of-domain, referring to a question or input that falls outside the scope of\nmodel’s trained knowledge base. To quantify this ability, we employ two widely recognized met-\nrics: the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the\nPrecision-Recall curve (AUPR) (Saito & Rehmsmeier, 2015). The Biomedical question-answering\n(BioASQ) (Krithara et al., 2023) dataset serves as our out-of-domain dataset, while the Conver-\nsational Question Answering (CoQA) dataset is used to represent in-domain data. We leverage\nuncertainty metrics as a means to detect out-of-domain prompts effectively. The results, as detailed\nin Table 3, demonstrate that our UA-CLM significantly outperforms the standard CLM in out-of-\ndomain detection tasks. This performance is consistently observed across all four uncertainty met-\nrics employed in the study, with up to 16.5% improvement in AUROC and up to 15% improvement\nin AUPR scores.\nGeneralization and long-form text generation\nWe conduct experiments to evaluate the general-\nization of the proposed uncertainty-aware CLM fine-tuning method. Since we fine-tune the model\nin a causal language modeling setup that involves next-token prediction, the learning should be\ntransferable, thereby encouraging generalizability. We evaluated the model fine-tuned with CoQA\ndataset on TriviaQA dataset, and vice-versa. Additionally, to assess the generalization beyond QA\ntasks, we performed experiments for biography generation, a long-form paragraph-level generation\ntask, following the recent works by Liu et al. (2024b) and Band et al. (2024). The Llama-2-7B\nmodels fine-tuned with CLM and UA-CLM on CoQA were given prompts to write biographies of\npopular figures, with names sourced from BioGen (Min et al., 2023). The generated responses were\ncompared against those obtained from GPT-4 (Achiam et al., 2023) using the same prompts, which\nserve as the ground truth for evaluation. The results provided in the Table 4 show that the generated\nresponse quality of both UA-CLM and CLM is similar for biography generation. However, there is\na significant improvement in calibration error and uncertainty quality, as quantified by hallucination\ndetection AUROC for the model fine-tuned with UA-CLM.\n5\nDISCUSSION\nWe proposed a novel fine-tuning approach to improve uncertainty calibration in Large Language\nModels (LLMs) devised for natural language generation. Our method incorporates a differentiable\nuncertainty-aware causal language modeling loss, which is grounded in the principles of decision\ntheory. This loss function is designed to enhance the model’s ability to provide well-calibrated uncer-\ntainty estimates without compromising the quality of text generation, a crucial aspect of trustworthy\nAI models.\nOur extensive empirical evaluations on open-ended and free-form question-answering tasks has\nshown that the uncertainty-aware causal language modeling approach yield better-calibrated uncer-\ntainty quantification, which in turn significantly enhances the model’s ability to detect hallucinations,\nidentify out-of-domain prompts, and selective generation decisions. We demonstrated the general-\nizability of the proposed fine-tuning method to different text generation tasks. We also introduced a\nnovel application of correlation analysis to the evaluation of sentence-level uncertainty calibration in\nfree-form text generation, accounting for the varying sentence lengths between generated responses\nand ground-truth references.\nLimitations and Future work: Currently, the proposed method is tailored to white-box model\nsettings, where the model internals are accessible for calibration fine-tuning. However, there is a\npotential to extend uncertainty-aware fine-tuning to black-box models by calibrating an auxiliary\nmodel, or prompt tuning for calibrated uncertainty quantification. Additionally, the focus of this\nwork has been on calibrating token-level uncertainty, which sets the stage for the exploration of\ncalibrating sentence-level uncertainty. We plan to explore these two avenues in our future work. We\nhope this work opens new avenues for the research community to enhance the uncertainty calibration\nin LLMs for free-form natural language generation.\nIn conclusion, this work contributes towards the broader goal of developing trustworthy LLMs. The\nability to recognize out-of-domain prompts and to acknowledge the limits of a model’s knowledge\nbase through reliable uncertainty quantification paves the way for reducing hallucinations and en-\nhancing decision-making in AI systems.\n10\nREFERENCES\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad\nGhavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A\nreview of uncertainty quantification in deep learning: Techniques, applications and challenges.\nInformation fusion, 76:243–297, 2021.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. arXiv preprint arXiv:2303.08774, 2023.\nMuhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy. Creating trustworthy llms:\nDealing with hallucinations in healthcare ai. arXiv preprint arXiv:2311.01463, 2023.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-\ncrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\nNeil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto.\nLinguistic calibration of long-\nform generations. In Forty-first International Conference on Machine Learning, 2024. URL\nhttps://openreview.net/forum?id=rJVjQSQ8ye.\nJacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. In\nNoise reduction in speech processing, pp. 37–40. Springer, 2009.\nGerman E Berrios. Confabulations: a conceptual history. Journal of the History of the Neuro-\nsciences, 7(3):225–241, 1998.\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in\nneural network. In International conference on machine learning, pp. 1613–1622. PMLR, 2015.\nJesse Davis and Mark Goadrich.\nThe relationship between precision-recall and roc curves.\nIn\nProceedings of the 23rd international conference on Machine learning, pp. 233–240, 2006.\nShrey Desai and Greg Durrett.\nCalibration of pre-trained transformers.\nIn Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 295–\n302, 2020.\nEva Eigner and Thorsten H¨andler. Determinants of llm-assisted decision-making. arXiv preprint\narXiv:2402.17385, 2024.\nEkaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill\nFedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, et al. Lm-\npolygraph: Uncertainty estimation for language models. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations, pp. 446–461,\n2023.\nSebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large\nlanguage models using semantic entropy. Nature, 630(8017):625–630, 2024.\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr´ed´eric Blain, Francisco Guzm´an, Mark Fishel,\nNikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for\nneural machine translation. Transactions of the Association for Computational Linguistics, 8:\n539–555, 2020.\nJakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt,\nJianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey\nof uncertainty in deep neural networks. Artificial Intelligence Review, 56(Suppl 1):1513–1589,\n2023.\nGemma, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models\nbased on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.\n11\nJiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. A survey\nof confidence estimation and calibration in large language models. In Proceedings of the 2024\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), pp. 6577–6595, 2024.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.\nOn calibration of modern neural\nnetworks. In International conference on machine learning, pp. 1321–1330. PMLR, 2017.\nDan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-\nnarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In\nInternational Conference on Learning Representations, 2020.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.\nIn International conference on machine learning, pp. 2790–2799. PMLR, 2019.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2022.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\nComputing Surveys, 55(12):1–38, 2023.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language\nmodels know? on the calibration of language models for question answering. Transactions of the\nAssociation for Computational Linguistics, 9:962–977, 2021.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611,\n2017.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,\nNicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language mod-\nels (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.\nSanyam Kapoor, Nate Gruver, Manley Roberts, Arka Pal, Samuel Dooley, Micah Goldblum, and\nAndrew Wilson. Calibration-tuning: Teaching large language models to know what they don’t\nknow. In Proceedings of the 1st Workshop on Uncertainty-Aware NLP (UncertaiNLP 2024), pp.\n1–14, 2024.\nArchit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens,\nMichael C Mozer, and Becca Roelofs. Soft calibration objectives for neural networks. Advances\nin Neural Information Processing Systems, 34:29768–29779, 2021.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, and Chao Zhang. Calibrated\nlanguage model fine-tuning for in-and out-of-distribution data. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1326–1340, 2020.\nRanganath Krishnan and Omesh Tickoo. Improving model calibration with accuracy versus un-\ncertainty optimization. Advances in Neural Information Processing Systems, 33:18237–18248,\n2020.\nAnastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras.\nBioasq-qa: A manually curated corpus for biomedical question answering. Scientific Data, 10\n(1):170, 2023.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for\nuncertainty estimation in natural language generation. In The Eleventh International Conference\non Learning Representations, 2023.\n12\nMeelis Kull, Telmo Silva Filho, and Peter Flach. Beta calibration: a well-founded and easily im-\nplemented improvement on logistic calibration for binary classifiers. In Artificial intelligence and\nstatistics, pp. 623–631. PMLR, 2017.\nAviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks\nfrom kernel mean embeddings. In International Conference on Machine Learning, pp. 2805–\n2814. PMLR, 2018.\nBhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and An-\ndrew Beam. Conformal prediction with large language models for multi-choice question answer-\ning. arXiv preprint arXiv:2305.18404, 2023.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive\nuncertainty estimation using deep ensembles. Advances in neural information processing systems,\n30, 2017.\nChin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using\nlongest common subsequence and skip-bigram statistics.\nIn Proceedings of the 42nd annual\nmeeting of the association for computational linguistics (ACL-04), pp. 605–612, 2004.\nZhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantifi-\ncation for black-box large language models. Transactions on Machine Learning Research, 2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\nin neural information processing systems, 36, 2024a.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\ncessing. ACM Computing Surveys, 55(9):1–35, 2023.\nXin Liu, Muhammad Khalifa, and Lu Wang. Litcab: Lightweight language model calibration over\nshort-and long-form responses. In The Twelfth International Conference on Learning Represen-\ntations, 2024b.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019.\nVarun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D Manning, and Daniel E\nHo. Hallucination-free? assessing the reliability of leading ai legal research tools. arXiv preprint\narXiv:2405.20362, 2024.\nAndrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. In\nInternational Conference on Learning Representations, 2021.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin\nBossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.\ncom/huggingface/peft, 2022.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf\nconference on computer vision and pattern recognition, pp. 3195–3204, 2019.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. In The 2023 Conference on Empirical Methods in Natural\nLanguage Processing, 2023.\nMatthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby,\nDustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. Advances\nin Neural Information Processing Systems, 34:15682–15694, 2021.\nJishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Doka-\nnia. Calibrating deep neural networks using focal loss. Advances in Neural Information Process-\ning Systems, 33:15288–15299, 2020.\n13\nKevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.\nMalik Sajjad Ahmed Nadeem, Jean-Daniel Zucker, and Blaise Hanczar. Accuracy-rejection curves\n(arcs) for comparing classification methods with a reject option. In Machine Learning in Systems\nBiology, pp. 65–81. PMLR, 2009.\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated proba-\nbilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 29, 2015.\nJeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measur-\ning calibration in deep learning. In CVPR workshops, 2019.\nBenjamin Plaut, Khanh Nguyen, and Tu Trinh. Softmax probabilities (mostly) predict large language\nmodel correctness on multiple-choice q&a. arXiv preprint arXiv:2402.13213, 2024.\nSiva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019.\nJie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan,\nand Peter J Liu. Out-of-distribution detection and selective generation for conditional language\nmodels. In The Eleventh International Conference on Learning Representations, 2022.\nTakaya Saito and Marc Rehmsmeier. The precision-recall plot is more informative than the roc plot\nwhen evaluating binary classifiers on imbalanced datasets. PloS one, 10(3):e0118432, 2015.\nSunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Micha-\nlak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks.\nAdvances in neural information processing systems, 32, 2019.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea\nFinn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated con-\nfidence scores from language models fine-tuned with human feedback. In The 2023 Conference\non Empirical Methods in Natural Language Processing, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan,\nand Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.\n5744–5760, 2022.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\nNeural text generation with unlikelihood training. In International Conference on Learning Rep-\nresentations, 2020.\nYijun Xiao and William Yang Wang. Quantifying uncertainties in natural language processing tasks.\nIn Proceedings of the AAAI conference on artificial intelligence, pp. 7322–7329, 2019.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. Can llms\nexpress their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth\nInternational Conference on Learning Representations, 2024.\nHaoyan Yang, Yixuan Wang, Xingyin Xu, Hanyuan Zhang, and Yirong Bian.\nCan we trust\nllms?\nmitigate overconfidence bias in llms through knowledge transfer.\narXiv preprint\narXiv:2405.16856, 2024.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-\ning text generation with bert. In International Conference on Learning Representations, 2020.\nDaniel Zwillinger and Stephen Kokoska. CRC standard probability and statistics tables and formu-\nlae. Crc Press, 1999.\n14\nENHANCING TRUST IN LARGE LANGUAGE MODELS WITH\nUNCERTAINTY-AWARE FINE-TUNING\nA\nAPPENDIX\nA.1\nEXPERIMENTAL DETAILS\nA.1.1\nDATASETS\nCoQA\nConversational Question Answering (CoQA) (Reddy et al., 2019) dataset was developed to\nevaluate models’ ability to respond to natural, dialogue-based questions, with free-form text answers\nsupported by highlighted evidence from the passage. The full dataset comprises of 127k question-\nanswer pairs derived from 8k conversations based on text passages across 7 distinct domains. For all\nour experiments, we utilize the development subset of CoQA, which consists of 8k question-answer\npairs. Figure 4 shows the color-coded co-reference chains in CoQA as illustrated in the (Reddy\net al., 2019).\nTriviaQA\nTriviaQA (Joshi et al., 2017) is a reading comprehension dataset consisting of over\n650k question-answer-evidence triplets. It includes 95,000 question-answer pairs authored by trivia\nenthusiasts, along with an average of six independently gathered evidence documents per question,\nproviding high-quality distant supervision for answering the questions. In our experiment, we used\nthe validation split of the dataset with around 10,000 question-answer pairs. Table 5 shows some of\nthe samples from the dataset.\nOK-VQA\nOutside Knowledge-Visual Question Answering benchmarks (Marino et al., 2019) con-\nsists of visual queries where the image content alone is not sufficient to answer the questions. Thus,\nit requires models to incorporate external knowledge to generate accurate answers. The dataset con-\nsists of 14k questions across 10 knowledge categories. In our experiment, we used the validation\nsplit of the dataset with around 5k question-answer pairs. Figure 5 shows a few samples from the\ndataset across different knowledge categories.\nFigure 4: Sample from CoQA (Reddy et al., 2019) illustrating the co-reference chain of conversa-\ntional questions.\n15\nQuestion\nAnswer\nMiami Beach in Florida borders which ocean?\nAtlantic\nWhat was the occupation of Lovely Rita according to the song by the Beatles\nTraffic Warden\nWho was Poopdeck Pappys most famous son?\nPopeye\nThe Nazi regime was Germany’s Third Reich; which was the first Reich?\nHOLY ROMAN EMPIRE\nTable 5: Data samples from TriviaQA (Joshi et al., 2017)\nFigure 5: Data samples from OK-VQA (Marino et al., 2019) across different knowledge categories.\nBioASQ\nThe BioASQ (Krithara et al., 2023) challenge, conducted every year, focuses on tech-\nniques in large-scale biomedical semantic indexing and question answering (QA). For our exper-\niments, we utilize Task B (Table 6) from the eleventh edition of the BioASQ challenge (BioASQ\n2023), which includes biomedical questions in English and their corresponding gold standard an-\nswers. We consider exact answers as gold answers where available; otherwise, we refer to the ideal\nanswers field in the dataset.\nQuestion\nAnswer\nWhich amino acid in implicated in the Blue diaper syndrome?\ntryptophan\nWhat are the outcomes of ubiquitination?\nProtein degradation, Degradation of proteins\nWhat causes Serpentine Supravenous Hyperpigmentation?\n5-fluorouracil, docetaxel\nWhat are positive cell-cycle regulators that can cause cancer when mutated called?\nProto-oncogenes\nTable 6: Data samples from BioASQ (Krithara et al., 2023)\nA.1.2\nPROMPT TEMPLATE\nOpen-book QA Prompt:\nAnswer the following question as briefly as possible.\nContext: [Provided context paragraph]\nQuestion: [Associated Question]\nAnswer:\nBiography generation Prompt:\nYou are an AI assistant. You use a tone that is technical and scientific.\nUSER: Write a paragraph for [name]’s biography.\nASSISTANT:\nA.1.3\nFINETUNING HYPERPARAMETERS AND IMPLEMENTATION\nWe fine-tune the models for generic causal language modeling (CAUSAL LM) task in autore-\ngressive manner that predict the next token in a sequence based on the preceding tokens. In the\nCAUSAL LM task, labels are created directly from the prompt itself by using the subsequent to-\nkens in the sequence as the target labels for prediction. For each position in the prompt sequence,\n16\nthe model takes the preceding tokens as input and the subsequent token as the label by progres-\nsively shifting the window of context during the fine-tuning process. We fine-tune our models for\nall experiments for 3 epochs using LoRA (Hu et al., 2022) with AdamW optmizer (Loshchilov &\nHutter, 2019). We use an initial learning rate of 1e-4, weight decay of 0.001 and a warm up ratio of\n0.03. In our experiments we used Low-Rank Adaptation (LoRA) to efficiently fine-tune pre-trained\nLLMs and LVLMs for the causal language modeling task. For LLMs, we set the LoRA rank as\n32, alpha parameter as 64 and a dropout of 0.1. LoRA was applied specifically to the following\nmodules: q proj, k proj, v proj, up proj, and down proj, resulting in less than 1% of trainable model\nparameters. In addition to LoRA, we applied 4-bit normalized float (nf4) quantization to the model’s\nparameters and utilized FP16 precision during fine-tuning to reduce the computational overhead.\nFor inference, we utilized FP16 precision and the default greedy decoding provided by Hugging\nFace with temperature value T=0.3. The predictive entropy and semantic entropy are estimated by\ngenerating 5 stochastic sequences from the model, each obtained through temperature sampling with\na temperature setting of T=0.3. This temperature was chosen to obtain optimal uncertainty estimates\nbalanced with high quality generated text, based on the ablation study shown in Figure 6. Our source\ncode was implemented using Pytorch1 framework and the models from Hugging Face2 library. For\nthe UnLikelihood Training (ULT), we implemented the loss function as described in (Welleck et al.,\n2020) and fine-tuned it using LoRA, similar to the CLM and UA-CLM methods. For Calibration\nTuning (CT) (Kapoor et al., 2024) method, we utilized the calibration-tuned models provided in\ntheir repository3.\nFor the LVLM model, LLaVA-1.5 (Liu et al., 2024a), we configured LoRA with a rank of 8, an\nalpha value of 8, and applied a 0.1 dropout rate to mitigate overfitting on the small OK-VQA training\nsubset. In addition to the proposed UA-CLM loss, we experimented with a combined loss function\nthat anneals the CLM loss with our UA-CLM loss. This approach allows the model to learn to answer\nOK-VQA queries using the context provided in the early stages of training, without uncertainty\ncalibration. As training progresses, we shift our focus toward calibrating the model’s uncertainty.\nBy this stage, the model has already learned to answer visual question-answering prompts, allowing\nus to refine its performance on questions it is likely to answer correctly or incorrectly, based on\ninsights gained during the initial training phases. Specifically, we assign a higher weight to the\nCLM loss in the early stages of training, gradually increasing the weight of the UA-CLM loss after\n20% of the training is completed as shown in Equation 4. Our ablation results for this study are\npresented in Table 9.\nL = LCLM + β · LUA-CLM\nwhere β =\n\u001a0.2\nif steps ≤0.2 · total steps\n0.8\nif steps > 0.2 · total steps\n(4)\nA.2\nTEXT GENERATION QUALITY METRICS\n• ROUGE-L (Lin & Och, 2004):\nRecall-Oriented Understudy for Gisting Evaluation\n(ROUGE) is a widely-used evaluation metric for assessing the quality of text generated\nbased on n-gram matching. We use the Rouge-L variant which uses the longest common\nsubsequence between the generated answer and the ground truth answer.\n• Exact Match (EM): Exact Match (EM) metric is a stringent evaluation criterion used\nto assess the performance of models on tasks such as question answering (QA), where a\ngenerated response is compared to a reference answer. It is a widely used metric for open-\nbook QA, this metric evaluates a model’s ability to extract the precise text span from the\ncontext to answer a question.\n• Accuracy: The generated answer is considered as accurate if it achieves Rouge-L(y, ˆy) >\n0.3, for a given reference answer y and a model generation ˆy. We follow this criterion for\nquantifying accuracy in free-form text generation based on the findings from (Kuhn et al.,\n2023) that demonstrated this criterion closely matches the human evaluation accuracy on\nCOQA and TriviaQA datasets, both of which are utilized in our experiments.\n• BERTScore (Zhang et al., 2020):\nBERTScore utilizes word embeddings to compute a\nsimilarity score between the tokens in the prediction and ground truth and has shown to\nwell correlate with human judgement. We report Precision, Recall and F1 BERTScores for\nall our experiments.\n1 https://pytorch.org/\n2 https://huggingface.co/\n3 https://github.com/activatedgeek/calibration-tuning\n17\nFigure 6: Ablation study: Effect of temperature value on the quality of generated text and the quality of\nuncertainty estimates evaluated with AUROC for hallucination detection. The study was performed on pre-\ntrained Llama-2-7B model with CoQA dataset. Based on this study, we selected temperature T=0.3 as it results\nin optimal AUROC and ROUGE-L scores.\nA.3\nUNCERTAINTY ESTIMATION METRICS\nWe assess uncertainty in natural language predictions by utilizing the Area Under the Receiver Op-\nerating Characteristic (AUROC) scores, calculated between correct and incorrect predictions across\nthe following metrics:\n• Predictive Entropy (Fomicheva et al., 2020): This is a widely used measure for uncer-\ntainty estimation and is defined as the entropy of the model’s output probability distribu-\ntion from stochastic generated responses. Formally, for a specific instance x, the predic-\ntive entropy, denoted as PE(x), is defined as the conditional entropy of the output ran-\ndom variable Y , with realization y, given x (Kuhn et al., 2023): PE(x) = H(Y |x) =\n−\nR\np(y|x) ln p(y|x)dy\n• Semantic Entropy (Kuhn et al., 2023): Defined as entropy of output distributions in se-\nmantic event-space rather than traditional token event-space and has been shown to be a\ngood indicator in detecting confabulation in language models.\n• Perplexity (Fomicheva et al., 2020):\nA standard metric to assess the quality of\nmodel and is defined as the inverse probability of the generated text: Perplexity =\nexp\n\u0010\n−1\nN\nPN\ni=1 log p(wi|w1, . . . , wi−1)\n\u0011\nA.4\nADDITIONAL RESULTS\nFigure 7 and Figure 9 shows the number of incorrect tokens and correct tokens during the fine-\ntuning process, along with the associated uncertainty estimates. These plots highlight the issue of\noverconfidence and miscalibration with CLM, while UA-CLM improves the reliability of uncer-\ntainty estimates.\nThe results in the Table 7 presents a detailed quantitative evaluation of various text generation qual-\nity metrics across various models, datasets, and uncertainty quantification (UQ) metrics. It compares\nstandard Causal Language Modeling (CLM) with our Uncertainty-Aware Causal Language Model-\ning (UA-CLM).\nThe results in Table 8 presents quantitative data with the values of Spearman’s rank correlation co-\nefficient and Pearson correlation coefficient across different models, datasets, and uncertainty quan-\ntification (UQ) metrics, with a specific focus on comparing standard Causal Language Modeling\n(CLM) and our Uncertainty-Aware Causal Language Modeling (UA-CLM). The data reveals that\nUA-CLM exhibits a stronger inverse correlation between UQ metrics and ROUGE-L scores, indi-\ncating better reliability of uncertainty estimates. This enhanced inverse relationship suggests that\nUA-CLM is more adept at associating higher uncertainty with low quality text generation quality\nand vice versa, which is a key indicator of better uncertainty calibration.\n18\n(a) CLM\n(b) UA-CLM\nFigure 7: Analysis of Correct and Incorrect Token Counts in mini-batch during fine-tuning with CLM and\nUA-CLM. Both CLM and UA-CLM show increase in correct tokens and a decrease in incorrect tokens as fine-\ntuning progresses.\n(a) CLM\n(b) UA-CLM\nFigure 8: Analysis of Token Uncertainty associated with Correct and Incorrect tokens in the mini-batch dur-\ning fine-tuning with CLM and UA-CLM. A well-calibrated model should provide low uncertainty for correct\ntokens and higher uncertainty for incorrect tokens. With standard CLM Loss, uncertainty for both correct and\nincorrect tokens decreases, indicating overconfidence even on incorrect tokens. In contract, with UA-CLM, the\nuncertainty for incorrect tokens increases and the decreasing uncertainty on correct tokens, supporting that the\nfine-tuning with UA-CLM improves the reliability of uncertainty estimates.\n(a) CLM\n(b) UA-CLM\nFigure 9: Analysis of Token Softmax Probability associated with Correct and Incorrect tokens during fine-\ntuning with CLM and UA-CLM. A well-calibrated model should assign high probability to correct tokens and\nlower probability to incorrect tokens. With standard CLM loss, probabilities for both correct and incorrect\ntokens increase as fine-tuning progress, indicating overconfidence. In contrast, UA-CLM fine-tuning results in\nhigher probabilities for correct tokens and lower probabilities for incorrect tokens, enhancing the reliability of\ntoken probability scores\n19\n(i) CLM\n(ii) UA-CLM\nFigure 10: Llama-2-7B: Loss convergence and uncertainty values associated with correct and incorrect tokens.\n(i) CLM\n(ii) UA-CLM\nFigure 11: Llama-2-13B: Loss convergence and uncertainty values for correct and incorrect tokens.\nFigure 12: Llava-1.5: Loss convergence and uncertainty values associated with correct and incorrect tokens.\n20\nTable 7: Evaluation of generated text quality metrics: Comparative analysis of Causal Language Modeling\n(CLM) and Uncertainty-aware Causal Language Modeling (UA-CLM) fine-tuning methods. The results in the\ntable indicate that UA-CLM achievies similar or better generated text quality metrics than standard CLM across\na range of models and datasets.\nDataset\nModel\nFinetuning\nMethod\nRouge-L\nExact Match\nAccuracy\nBERT Score\n(Precision)\nBERT Score\n(Recall)\nBERT Score\n(F1)\nCoQA\nLlama-2-7b\nCLM\n0.8886\n0.8071\n0.9253\n0.9633\n0.9598\n0.9604\nUA-CLM\n0.8882\n0.8027\n0.9264\n0.9671\n0.9644\n0.9648\nLlama-2-13b\nCLM\n0.9106\n0.8434\n0.9406\n0.9678\n0.9639\n0.9650\nUA-CLM\n0.9118\n0.8204\n0.9461\n0.9732\n0.9698\n0.9705\nGemma-2b\nCLM\n0.8654\n0.7606\n0.9143\n0.962\n0.9548\n0.9570\nUA-CLM\n0.8632\n0.7632\n0.9088\n0.9627\n0.9554\n0.9578\nTriviaQA\nLlama-2-7b\nCLM\n0.5867\n0.4939\n0.6385\n0.8743\n0.8785\n0.8754\nUA-CLM\n0.6342\n0.5627\n0.6754\n0.8951\n0.8883\n0.8910\nLlama-2-13b\nCLM\n0.6588\n0.5883\n0.6967\n0.9026\n0.8989\n0.9001\nUA-CLM\n0.7277\n0.6445\n0.7710\n0.9204\n0.9164\n0.9177\nGemma-2b\nCLM\n0.4349\n0.3674\n0.4759\n0.8375\n0.8349\n0.8355\nUA-CLM\n0.4563\n0.3915\n0.4959\n0.8404\n0.8382\n0.8387\nOK-VQA\nLlava-1.5-7b\nCLM\n0.5569\n0.5099\n0.5891\n0.8897\n0.8864\n0.8877\nUA-CLM\n0.5354\n0.4950\n0.5643\n0.8841\n0.8820\n0.8827\nTable 8: Uncertainty calibration analysis: The results show UA-CLM have more pronounced negative cor-\nrelation between the uncertainty estimates and the generated text quality (ROUGE-L) than standard Causal\nLanguage Modeling CLM, indicating enhanced reliability in uncertainty quantification with UA-CLM.\nDataset\nModel\nFinetuning\nMethod\nSpearman’s rank correlation coefficient ↓\nPearson correlation coefficient ↓\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nCoQA\nLlama-2-7b\nCLM\n-0.2130\n-0.2379\n-0.3398\n-0.2898\n-0.2029\n-0.2109\n-0.2710\n-0.2881\nUA-CLM\n-0.2479\n-0.3401\n-0.4334\n-0.3742\n-0.3414\n-0.3414\n-0.3414\n-0.3414\nLlama-2-13b\nCLM\n-0.2325\n-0.2523\n-0.3253\n-0.3004\n-0.2302\n-0.2495\n-0.3001\n-0.2636\nUA-CLM\n-0.2398\n-0.3280\n-0.4170\n-0.3717\n-0.2335\n-0.3244\n-0.3269\n-0.3481\nGemma-2b\nCLM\n-0.3639\n-0.3629\n-0.4335\n-0.3756\n-0.3860\n-0.3713\n-0.3483\n-0.3399\nUA-CLM\n-0.3676\n-0.4063\n-0.4476\n-0.4127\n-0.4033\n-0.4019\n-0.3517\n-0.3530\nTriviaQA\nLlama-2-7b\nCLM\n-0.5627\n-0.5863\n-0.5765\n-0.5994\n-0.5047\n-0.4854\n-0.2864\n-0.5020\nUA-CLM\n-0.5713\n-0.6011\n-0.5822\n-0.5980\n-0.5385\n-0.5326\n-0.3382\n-0.4916\nLlama-2-13b\nCLM\n-0.5711\n-0.5845\n-0.5522\n-0.5959\n-0.5155\n-0.4915\n-0.4548\n-0.4612\nUA-CLM\n-0.5725\n-0.5862\n-0.5607\n-0.5854\n-0.5362\n-0.5407\n-0.4786\n-0.4479\nGemma-2b\nCLM\n-0.5636\n-0.5772\n-0.5609\n-0.5537\n-0.5020\n-0.4534\n-0.4494\n-0.4514\nUA-CLM\n-0.5623\n-0.5913\n-0.5457\n-0.5928\n-0.5164\n-0.5010\n-0.4534\n-0.4947\nOK-VQA\nLlava-1.5-7b\nCLM\n-0.1253\n-0.1132\n-0.1320\n-0.1062\n-0.0862\n-0.0861\n-0.1256\n-0.1340\nUA-CLM\n-0.1606\n-0.1619\n-0.2050\n-0.2660\n-0.0748\n-0.1214\n-0.2100\n-0.3020\nTable 9: Ablation study: Effect of different loss functions during fine-tuning. Exact match is used\nas accuracy metric in computing AUARC.\nDataset\nModel\nFine-tuning Loss\nAUROC (Hallucination/Confabulation detection)\nAUARC (Area under rejection accuracy curve)\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nToken\nEntropy\nPerplexity\nPredictive\nEntropy\nSemantic\nEntropy\nOKVQA\nLlava-1.5-7b\nLCLM\n0.5504\n0.5419\n0.5455\n0.537\n0.5809\n0.5781\n0.579\n0.5747\nLUA-CLM\n0.5839\n0.6032\n0.5701\n0.6727\n0.5657\n0.5771\n0.5601\n0.6028\nLCLM + β ∗LUA-CLM\n0.6001\n0.5984\n0.6106\n0.6638\n0.5989\n0.5965\n0.6012\n0.6265\nCoQA\nLlama-2-7b\nLCLM\n0.6252\n0.632\n0.6635\n0.6889\n0.823\n0.829\n0.8516\n0.8405\nLUA-CLM\n0.6955\n0.7398\n0.7413\n0.7741\n0.8246\n0.8477\n0.8743\n0.8571\nLCLM + β ∗LUA-CLM\n0.6101\n0.6183\n0.6978\n0.7252\n0.8153\n0.8153\n0.8614\n0.8455\nTriviaQA\nLlama-2-13b\nLCLM\n0.8264\n0.8333\n0.7971\n0.8407\n0.7464\n0.7526\n0.7532\n0.7556\nLUA-CLM\n0.8297\n0.8352\n0.8033\n0.8447\n0.7960\n0.8059\n0.804\n0.8069\nLCLM + β ∗LUA-CLM\n0.8340\n0.8263\n0.8049\n0.8307\n0.7666\n0.7692\n0.7673\n0.7693\n21\nFigure 13: Accuracy versus Expected Calibration Error (ECE) comparison between UA-CLM, CLM, and\npre-trained baseline across different LLM architectures on CoQA dataset. The ideal model should have high\naccuracy and low expected calibration error, indicating accurate predictions with well-calibrated uncertainty\nquantification (top-left of the Accuracy vs ECE plot). When evaluating three different model architectures, we\nobserve that the accuracy of models with CLM and UA-CLM remains within a similar range and better than\nthe pre-trained baseline. While, the ECE of models fine-tuned with UA-CLM shows significant improvement\ncompared to both the pre-trained baseline and CLM fine-tuning.\nFigure 14: Accuracy versus Expected Calibration Error (ECE) comparison between UA-CLM, CLM, and\npre-trained baseline across different LLM architectures on TriviaQA dataset. The ideal model should have high\naccuracy and low expected calibration error, indicating accurate predictions with well-calibrated uncertainty\nquantification (top-left of the Accuracy vs ECE plot). When evaluating three different model architectures, we\nobserve that the both accuracy and ECE of the models fine-tuned with UA-CLM shows significant improvement\ncompared to both the pre-trained baseline and CLM fine-tuning.\n22\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-12-03",
  "updated": "2024-12-03"
}