{
  "id": "http://arxiv.org/abs/2412.04682v2",
  "title": "Two stages domain invariant representation learners solve the large co-variate shift in unsupervised domain adaptation with two dimensional data domains",
  "authors": [
    "Hisashi Oshima",
    "Tsuyoshi Ishizone",
    "Tomoyuki Higuchi"
  ],
  "abstract": "Recent developments in the unsupervised domain adaptation (UDA) enable the\nunsupervised machine learning (ML) prediction for target data, thus this will\naccelerate real world applications with ML models such as image recognition\ntasks in self-driving. Researchers have reported the UDA techniques are not\nworking well under large co-variate shift problems where e.g. supervised source\ndata consists of handwritten digits data in monotone color and unsupervised\ntarget data colored digits data from the street view. Thus there is a need for\na method to resolve co-variate shift and transfer source labelling rules under\nthis dynamics. We perform two stages domain invariant representation learning\nto bridge the gap between source and target with semantic intermediate data\n(unsupervised). The proposed method can learn domain invariant features\nsimultaneously between source and intermediate also intermediate and target.\nFinally this achieves good domain invariant representation between source and\ntarget plus task discriminability owing to source labels. This induction for\nthe gradient descent search greatly eases learning convergence in terms of\nclassification performance for target data even when large co-variate shift. We\nalso derive a theorem for measuring the gap between trained models and\nunsupervised target labelling rules, which is necessary for the free parameters\noptimization. Finally we demonstrate that proposing method is superiority to\nprevious UDA methods using 4 representative ML classification datasets\nincluding 38 UDA tasks. Our experiment will be a basis for challenging UDA\nproblems with large co-variate shift.",
  "text": "TWO STAGES DOMAIN INVARIANT REPRESENTATION\nLEARNERS SOLVE THE LARGE CO-VARIATE SHIFT IN\nUNSUPERVISED DOMAIN ADAPTATION WITH TWO DI-\nMENSIONAL DATA DOMAINS\nHisashi Oshima\nGraduate School of Science and Engineering\nChuo University\nTokyo, Japan\na16.kk5e@g.chuo-u.ac.jp\nTsuyoshi Ishizone\nGraduate School of Advanced Mathematical Sciences\nMeiji University\nTokyo, Japan\ntsuyoshi.ishizone@gmail.com\nTomoyuki Higuchi\nFaculty of Science and Engineering\nChuo University\nTokyo, Japan\nhiguchi@kc.chuo-u.ac.jp\nABSTRACT\nRecent developments in the unsupervised domain adaptation (UDA) enable the\nunsupervised machine learning (ML) prediction for target data, thus this will ac-\ncelerate real world applications with ML models such as image recognition tasks\nin self-driving. Researchers have reported the UDA techniques are not working\nwell under large co-variate shift problems where e.g. supervised source data con-\nsists of handwritten digits data in monotone color and unsupervised target data\ncolored digits data from the street view. Thus there is a need for a method to\nresolve co-variate shift and transfer source labelling rules under this dynamics.\nWe perform two stages domain invariant representation learning to bridge the\ngap between source and target with semantic intermediate data (unsupervised).\nThe proposed method can learn domain invariant features simultaneously between\nsource and intermediate also intermediate and target. Finally this achieves good\ndomain invariant representation between source and target plus task discriminabil-\nity owing to source labels. This induction for the gradient descent search greatly\neases learning convergence in terms of classification performance for target data\neven when large co-variate shift. We also derive a theorem for measuring the gap\nbetween trained models and unsupervised target labelling rules, which is neces-\nsary for the free parameters optimization. Finally we demonstrate that proposing\nmethod is superiority to previous UDA methods using 4 representative ML clas-\nsification datasets including 38 UDA tasks. Our experiment will be a basis for\nchallenging UDA problems with large co-variate shift.\n1\nINTRODUCTION\nThese days UDA is attracting attentions from researchers and engineers in ML projects, since it can\nautomatically correct difference in the marginal distributions of the training source data (supervised)\nand test target data (unsupervised) and learn the better model based on labeling rule from source\ndata. Especially domain invariant representation learning methods including the domain adversarial\ntraining of neural networks (DANNs) (Ganin et al., 2017), the correlation alignment for deep domain\nadaptation (Deep CoRALs) (Sun & Saenko, 2016), and the deep adaptation networks (DANs) (Long\net al., 2015) have achieved performance improvements in a variety of ML tasks for instance digits\nimage recognition and the human activity recognition (HAR) with accelerometer and gyroscope\n(Wilson et al., 2020). There are emerging projects in a fairly business-like setting with UDA, and\n1\narXiv:2412.04682v2  [cs.LG]  17 Feb 2025\ndemonstrated a certain level of success. For instance image semantic segmentation task in self-\ndriving where different whether conditions data undermines ML models performance due to that\ndistribution gap between training and testing (Liu et al., 2020).\nOn the other hand, researchers have reported the UDA techniques are not working well under large\nco-variate shift. The techniques could cope with a small amount of distribution gap, e.g. between\nmonotone color digit images and their colored version without changing background and digit itself\nin other means, or between simulated binary toy data and their 30 degrees rotated version (Ganin\net al., 2017). In contrast to that, reported discrimination performance was not over 25 % and had\nvery high variance using UDA with the convolutional neural networks (CNNs) backbones between\nmonotone color digit data and colored digit data from street view (Ganin et al., 2017). Same problem\nwas observed between binary toy data and 50-70 degrees rotated version (See Figure 2 that we\nexplain later. We can create much better model for 30 degrees target data using same shallow neural\nnetworks backbone (Ganin et al., 2017).). Such UDA problems with large co-variate shift are often\nin real-world ML tasks. We limit scope of large co-variate shift problem to two dimensional data\ndomains causing co-variate shifts. In this setting two attributes related to data are causing co-variate\nshifts e.g. from monotone to color and from not on the street to on the street, the details of which\nare dealt with in 2.2.\nIn this study we propose two stages domain invariant representation learning to fill this gap. Use in-\ntermediate data (unsupervised) between source and target to ensure simultaneous domain invariance\nbetween source and intermediate data and invariance between intermediate and final target data, this\ngreatly enhances learning convergence in terms of classification performance for target. We can usu-\nally get access to intermediate unsupervised data compared to huge burden for labelling processes\ninvolving human labour and expensive measuring equipment. We also demonstrate a theorem that\nallows unsupervised hyper-parameters optimisation based on the reverse validation (RV) (Zhong\net al., 2010). This measures the difference between the target labelling rules and the labelling rules\nof the model after UDA training without any access to the target supervised labels. The UDA tends\nto negative transfer with inappropriate free parameters (Wang et al., 2019b), therefore theoretical\nsupported indicator is important.\nExperimental results with four datasets confirmed the superiority of the proposed method to previous\nstudies. Datasets for comparison tests with high demand for social implementations include image\nrecognition and accelerometer based HAR, and occupancy detection with energy consumption data\nmeasured by smarter maters in general households. This paper contributes to (1) Proposition of UDA\nstrategy as a solution to large co-variate shift, (2) Derivation of free parameters tuning indicator,\nenables validation for conditional distribution difference without any access for target ground truth\nlabels, (3) Demonstration of experimental superiority after comparison tests with benchmarks using\nfour representative datasets.\n2\nPRELIMINARY\n2.1\nUNSUPERVISED DOMAIN ADAPTATION\nSets of data are comprised of DS = {(xS\ni , yS\ni )}NS\ni=1, DT = {xT\ni }NT\ni=1, DT ′ = {xT ′\ni }NT ′\ni=1 (Source\ndomain, intermediate domain, target domain respectively and we denote NS, NT , NT ′ as the sample\nsizes for source, intermediate and target.), in our setting DT is e.g. from (UserA, Summer) when\nDS is from (UserA, Winter) and DT ′ is from (UserB, Summer). Then let PS(y|x), PS(x) denote the\nmarginal and conditional distribution of source, defined for intermediate and target similarly. They\nare in the homogeneous domain adaptation assumption, namely they share same sample space but\ndifferent distribution (Wilson & Cook, 2020). Also this research is in co-variate shift problem, that\nis generally sharing conditional distribution but different marginal distribution of co-variate (Zhao\net al., 2019). The objective is learning ϕ(·) = F ◦C to predict ground truth labels for DT ′ using\nthree sets of data without access to target ground truth labels, F corresponds to feature extractor\nwith arbitrary neural networks parameter θf and C task classifier with θc to be optimized by gradient\ndescent.\n2\nFigure 1: Forward path and backward process of Normal(DANNs), Step-by-step, Ours.\nThe\nLtask, −Ldomain as Lc, Ld for short.\n2.2\nWHAT IS TWO DIMENSIONAL CO-VARIATE SHIFTS\nLet’s dive into what is intermediate domain DT , we assume data domain is not one dimensional but\nis two dimensional e.g. (UserA or UserB, Accelerometer Model A and Accelerometer Model B)\nin HAR, (UserA or UserB, Winter or Summer) in occupancy detection problem and (Monotone or\nColor, In the street or not) in image recognition. In this paradigm, DS, DT , DT ′ may be (UserA,\nAccelerometer Model A), (UserB, Accelerometer Model A), (UserB, Accelerometer Model B) for\ninstance. These two dimensional factors are influencing data distribution (we call this as two di-\nmensional co-variate shifts), energy consumption data differs between users and also seasons for\ninstance. Basically under this two dimensional co-variate shifts problem correcting difference be-\ntween domains in UDA is quite difficult but more natural case closer to businesses or real world\nprojects. Additionally we assume gathering unsupervised data DT is quite easy, so UDA problem\nwith three sets DS, DT , DT ′ is natural and there is demand for solving this problem. Two dimen-\nsional domains assumption is novel in this field, although uses of an intermediate domain to resolve\nlarge co-variate shift are explored in (Lin et al., 2021; Zhang et al., 2019; Oshima et al., 2024) as\nwell. Their experiments showed a synthetic intermediate domain can improve UDA (Lin et al.,\n2021; Zhang et al., 2019), but limited to computer vision tasks technically or experimentally. Our\nproposition is not limited to a specific data type. We investigated whether or not each dataset follows\nthis assumption in the Appendix E.\n3\nUDA WHEN TWO DIMENSIONAL CO-VARIATE SHIFTS\n3.1\nLARGE CO-VARIATE SHIFT SOLVER: TWO STAGES DOMAIN INVARIANT LEARNERS\nTo begin with previous domain invariant learning abstraction, their objective functions and optimiza-\ntion are below (we call this as Normal). Domain invariant learning is parallel learning including the\nfeature extractor and task classifier’s learning labelling rule based on DS by minimizing Ltask and\nthe feature extractor’s learning domain invariance between DS, DT ′ by Ldomain (measurement of\ndistribution gap between source and target, we elaborate later). The constant value λ is coefficient\nof weight to adjust the balance between task classification performance and domain invariant perfor-\nmance. We define Ltask = −Pbatch size\ni=1\nPnum class\nj=1\nyS\ni,j log(ˆyS\ni,j) as the cross entropy loss with a\n3\npredicted probability ˆyS\ni,j for source input.\nL = Ltask + λLdomain\n(1)\nargminθf ,θcLtask, argminθf ,(θc)Ldomain\n(2)\nThis will achieve the feature extractor and task classifier’s generalization performance for target\ndata theoretically, since the UDA goal(expectation risk for target discrimination performance) is\nbounded by marginal distribution difference between source and target (corresponds to Ldomain),\nempirical risk for source discrimination performance (corresponds to Ltask), conditional distribution\ngap (under co-variate shift problem this should be small), and so fourth. Let H be a hypothesis\nspace of VC-dimension d, ˆDS, ˆDT ′ be empirical sample sets with size N drawn from source joint\ndistribution (features and label) and target marginal distribution (features), di be the domain label\n(binary label identifying source or target, di ∈{0, 1}), ET ′(h), ES(h) be expectations for hypothesis\nh (task classification), ˆES(h) be the empirical expectation, I[·] be the function outputting 1 when\ntrue inside the square brackets otherwise outputting 0, and then w.p. at least 1 −δ (δ ∈(0, 1)),\n∀h ∈H, the following inequality holds (Zhao et al., 2019; Ben-David et al., 2006).\nET ′(h) ≤ˆES(h) + 1\n2dH( ˆDS, ˆDT ′) + λ∗+ O(\ns\ndlogN + log( 1\nδ )\nN\n)\n(3)\nh∗= argminh∈HES(h) + ET ′(h), λ∗= ES(h∗) + ET ′(h∗)\n(4)\ndH( ˆDS, ˆDT ′) = 2(1 −min\nh∈H( 1\nNS\nNS\nX\ni=1\nI[h(xS\ni ) ̸= dS\ni ] +\n1\nNT ′\nNT ′\nX\nj=1\nI[h(xT ′\nj ) ̸= dT ′\nj ]))\n(5)\nPrevious studies such as DANNs, CoRALs and DANs have been published as embodiments of\nequation 1 and 2. The construction of Ldomain differs across methods. DANNs define it as a loss\nof domain classification problem, while CoRALs use the distance between covariance matrices of\nC. DANs build it using the multiple kernel variant of maximum mean discrepancy (MK-MMD\n(Gretton et al., 2012)) calculated on F and C, and Deep Joint Distribution Optimal Transportation\n(DeepJDOT) define it by wasserstein distance based on the optimal transport problem (Damodaran\net al., 2018). There are many variants, e.g. Convolutional deep Domain Adaptation model for Time\nSeries data (CoDATS) is signal processing layers and multi sources version of DANNs (Wilson\net al., 2020) and CoRALs variants are using different distant metrics for correlation alignment in-\ncluding a euclidean distance (Zhang et al., 2018a), geodesic distance (Zhang et al., 2018b), and log\neuclidean distance (Wang et al., 2017). For DANNs, optimizing Ldomain in a adversarial way with\nthe domain discriminator D with arbitrary neural networks’ parameters θd, we omitted description\nof arg maxθd Ldomain. Please note that arg minθc Ldomain is only worked for e.g. CoRALs and\nDANs.\nWe extend equation 1 to use intermediate data, as a decomposition of distribution differences be-\ntween source and intermediate data and between intermediate and terminal target data, and same\noptimization can be used in this formula as well.\nLpropose = Ldomain(DS, DT ) + Ldomain(DT , DT ′)\n(6)\nargminθf ,θcLtask, argminθf ,(θc)Lpropose\n(7)\nThe equation 6 says if we have intermediate data we can minimize Ldomain substituted by\nLdomain(DS, DT ) and Ldomain(DT , DT ′). Concurrent training of Lpropose, Ltask is tantamount\nto (1) aquisition of domain invariance between source and intermediate domain, discrim-\ninability for intermediate domain, (2) aquition of domain invariance between intermediate\ndomain and target, discriminability for target, we can do each quite easier compared with nor-\nmal domain invariant learning between source and target with same goal due to data domain’s se-\nmantic reason, so as a whole our strategy will facilitate learning target discriminability. Based\non assumption of two dimensional data domains divergence between source and target is larger\nthan divergence between source and intermediate, or divergence between intermediate and tar-\nget, we can say Ldomain ≥\nLpropose\n2\n.\nThis term is the optimisation target of domain invari-\nant representation learning, but it can be inferred that the larger it is, the more likely it is to be\naddicted to stale solutions (e.g. local minima) when paralleled with loss minimisation for task\nclassification. Pseudo codes for UDA with Lpropose are in Algorithm 1 and Algorithm 3 (Ap-\npendix A), though of course this can be used in other domain invariant representation learning\n4\ntechniques in the almost same way. We implemented two stages domain invariant learners with\nCoRALs as Lpropose = Ldomain(DS, DT ) + Ldomain(DS, DT ′), since we can say same thing as\nLpropose = Ldomain(DS, DT ) + Ldomain(DT , DT ′) and to avoid noisy correlation alignment be-\ntween intermediate and target data in the early stages of epochs. We denote di as domain labels, CE\nas the cross entropy loss, BCE as the binary cross entropy loss, MSE as the mean squared error\nand Cov as the covariance matrix. The schematic diagram for DANNs version two stages domain\ninvariant learners is in Figure 1 (CoRALs version in Appendix A Figure 7).\nAlgorithm 1 2stages-DANNs\nRequire: source,intermediate domain,target DS, DT , DT ′\nEnsure: neural network parameters {θf, θc, θd, θd′}\n1: θf, θc, θd, θd′ ←init()\n2: while epoch training() do\n3:\nwhile batch training() do\n4:\nˆE(Ldomain(DS, DT )) ←\n1\nbatch\nPbatch\ni=1\nBCE(D(F(xS\ni )), dS\ni ) +\n1\nbatch\nPbatch\ni=1\nBCE(D(F(xT\ni )), dT\ni )\n5:\nˆE(Ldomain(DT , DT ′)) ←\n1\nbatch\nPbatch\ni=1\nBCE(D′(F(xT\ni )), dT\ni ) +\n1\nbatch\nPbatch\ni=1\nBCE(D′(F(xT ′\ni )), dT ′\ni )\n6:\nˆE(Ltask) ←\n1\nbatch\nPbatch\ni=1\nCE(C(F(xS\ni )), yS\ni )\n7:\nθc ←θc −∂ˆE(Ltask)\n∂θc\n8:\nθf ←θf −∂(ˆE(Ltask)−ˆE(Ldomain(DS,DT ))−ˆE(Ldomain(DT ,DT ′)))\n∂θf\n9:\nθd ←θd −∂ˆE(Ldomain(DS,DT ))\n∂θd\n10:\nθd′ ←θd′ −∂ˆE(Ldomain(DT ,DT ′))\n∂θd′\n11:\nend while\n12: end while\nPrevious paper (Oshima et al., 2024) is intuitively step-by-step version of this paper (center of Figure\n1), this executes DANNs learning between source and intermediate data then second time DANNs\nbetween intermediate data (pseudo labeling by that learned task classifier) and target. We call this as\nStep-by-step. Very limited evaluation was conducted and achieved higher classification performance\ncompared to normal DANNs and without adaptation model using occupancy detection data(Dataset\nD, explain later). There are five main differences between the method of (Oshima et al., 2024) and\nthis paper, (1) (Oshima et al., 2024) generates noise due to erroneous answers in the pseudo-labelling\nstep ({(xT\ni , ˆyT\ni )}NT\ni=1 in Figure 1), which may hinder learning convergence, whereas our end-to-end\nmethod does not, (2) The two domain invariant representation learnings have partially independent\nstructure, and there is no guarantee that the first learning will necessarily be good for the second\nlearning, but our method can perform the two learnings in end-to-end, which may make it easier to\nkeep the overall balance, (3) They introduced confidence threshold technique which has large impact\non learning convergence, but these engineering tricks are not easy to tune in unsupervised settings\nin practical use cases, (4) Comparative experiments on four sets of data show that our method has a\nperformance advantage on most of the settings (5/8 settings) and (5) The learning algorithm encom-\npasses the entire domain invariant representation learning techniques including CoRALs, DANNs\nand DANs.\n3.2\nFREE PARAMETERS TUNING\nFree parameters(e.g. learning rate for gradient descent optimizer, layer-wise configurations) selec-\ntion has been regarded as a crucial role because deep learning methods generally are susceptible\nto, additionally UDA require us to do this in a agnostic way for target ground truth labels at all.\nWe propose free parameter tuning method specialized in this two stages domain invariant learners\nby extending RV (same as Normal (Ganin et al., 2017)). RV applies pseudo-labeling to the target\ndata and uses the pseudo-labeled data and the source unsupervised data in an inverse relationship\n(Zhong et al., 2010), authors proved this can measure conditional distribution difference between\ntarget data and learned model. We apply the RV idea to two stages domain invariant representation\n5\nAlgorithm 2 two stages domain invariant learners free parameter indicator\nRequire: DS, DT , DT ′, a neural network ϕ\n1: Split DS into DStrain and DSval\n2: Execute domain invariant learning with ϕ, DStrain, DT , DT ′, validate by DSval do early stop-\nping\n3: Pseudo labeling for DT ′ get {(xT ′\ni , ˆyT ′\ni )}NT ′\ni=1 using ϕ\n4: With pseudo-supervised {(xT ′\ni , ˆyT ′\ni )}NT ′\ni=1 as source and DT and unsupervised {xS\ni }\nNStrain\ni=1\n, do\ndomain invariant learning build ϕr\n5: calculate loss between predictions for DSval by ϕr and its ground truth labels\nlearners, replacing the internal learning steps from regular supervised machine learning with two\nstages domain invariant representation learners and calculating classification loss between predic-\ntions for source data by reverse model and its ground truth labels. We can find better configurations\nby arg minθ |ϕr(xS) −yS|. Algorithm is denoted in Algorithm 2, and the Theorem 3.1 states that\nthis method can measure the gap in labeling rules between the trained model and the final target\ndata, the proof of the theorem is given in Appendix B.\nTheorem 3.1. When executing Algorithm 2, conditional distribution gap between ϕ(·) and DT ′’s\nground truth is calculated by (C1, C2 as constant values)\n|ϕr(xS) −yS| ∝|C1{P(y|x, ϕ) −PT ′(y|x)} + C2{PT (y|x) −PT ′(y|x)}|\n(8)\n4\nEXPERIMENTAL VALIDATION\n4.1\nSETUP AND DATASETS\nThe evaluation follows the hold-out method. The evaluation score is the percentage of correct labels\npredicted by the task classifier of the two stages domain invariant learners for the target data i.e.\naccuracy(xT ′) = 1\nn\nP\nxT ′ I[C(F(xT ′)) = yT ′] (n as the sample size of target data for testing). The\ntarget data is divided into training data and test data in 50%,the training data is input to the training\nas unsupervised data, and the test data is not input to the training but used only when calculating\nthe evaluation scores. In order to take into account the variations in the evaluation scores caused by\nthe initial values of each layer of deep learning, 10 evaluations are carried out for each evaluation\npattern and the average value is used as the final evaluation score. In addition, hyperparameters\noptimisation with 3.2 is performed on the learning rate using the training data. The all codes we\nused in this paper are available on GitHub 1.\nWe validate our method with 4 datasets and 38 tasks including simulated toy data, image digits\nrecognition, HAR, occupancy detection. We adopt six benchmark models (conventional Train on\nTarget model, Ste-by-step with DANNs or CoRALs, Normal with DANNs or CoRALs, conventional\nWithout Adapt model) as a comparison test to ours in Table 14 (Appendix I). If our hypothesis is\ntrue, our method should be close to its Upper bound and better than previous studies and Lower\nbound models therefore ideal state ”Upper bound > Ours > Max(Step-by-step, Normal, Lower\nbound)” should be expected.\nA. sklearn.datasets.make moons\nSource data is two interleaving half circles with binary class. Intermediate data is ro-\ntated a degrees (semi-clockwise from the centre), target data is rotated 2a degrees (a ∈\n{15, 20, 25, 30, 35}). These rotations correspond to toy versions of two dimensional co-\nvariate shifts. We set noise=0.1.\nB. MNIST, MNIST-M, SVHN(Lecun et al., 1998; Ganin et al., 2017; Netzer et al., 2011)\nSource data is modified national institute of standards and technology database (MNIST),\n1please check v1.0.0 compatible to this paper https://github.com/oh-yu/domain-invariant-learning, also we\nelaborated experimental configurations in the Appendix D\n6\nintermediate data is MNIST-M which is the randomly colored version of MNIST, target\ndata is the street view house numbers (SVHN). Case with source and target reversed was\ndemonstrated to be coped with by Normal (Ganin et al., 2017). Two dimensional co-variate\nshifts should be (Monotone, Not on the Street)→(Color, Not on the Street)→(Color, On the\nStreet).\nC. HHAR(Stisen et al., 2015)\nHeterogeneity human activity recognition dataset (HHAR). Signal processing problem\nto determine human behaviour ({bike, sit, stand, walk, stairsup, stairsdown}) by ac-\ncelerometer data (sliding window with size 128, sampling rate is 100-150Hz). Subjects\nwere taking actions in the pre-programmed way, so there are no chances of distribution\nshift by different or strange movements. The two dimensional co-variate shifts should\nbe (UserA, SensorA)→(UserB, SeonsorA)→(UserB, SensorB), same actions but different\nuser and sensor differ in co-variate. We extract 16 patterns randomly from 9 users and 4\nmodels.\nD. ECO data set(Beckel et al., 2014)\nElectricity consumption & occupancy (ECO) data set. Signal processing problem same as\nHHAR with different time window and activity classes(only {occupied, unoccupied}).\nThe\ntwo\ndimensional\nco-variate\nshifts\nshould\nbe\n(HouseA,\nWinter)→(HouseB,\nWinter)→(HouseB, Summer). Subjects were not pre-programmed in any means so should\ninclude distribution shift in multiple ways, though basically the sharing of the rule of being\nat home when energy consumption is high and absent when it is not is indicated by (Oshima\net al., 2024). 16 Patterns as total.\n4.2\nQUANTITATIVE RESULTS\nWe confirmed the Ours’ obvious performance advantage compared to Step-by-step and Normal in\nDataset A with DANNs when larger co-variate shift exists i.e. target data with 60 and 70 degrees\nrotated (highlighted in red in left of Figure 2). The difference was 0.174 compared to Normal when\n60 degrees rotated target, 0.074 compared to Step-by-step and 0.091 compared to Normal when 70\ndegrees rotated target, and variance was much smaller. In Dataset A with CoRALs, increasing the\nangle of rotation significantly reduces the evaluation scores of Normal, but Step-by-step and Ours\nwere somewhat able to cope with this (highlighted in red in right).\nThe Figure 3 shows our methods’ superiority to previous studies in Dataset A-D(with DANNs),\nD(with CoRALs), namely the ideal state ”Upper bound > Ours > Max(Step-by-step, Normal, Lower\nbound)” was observed in the UDA experiment. In cases Dataset A and C, a clear difference in\naccuracy was identified compared to Step-by-step or Normal, the difference in accuracy was 0.074\nfor Ours and Step-by-step in Dataset C, 0.061 for Ours and Normal, 0.053 for Ours and Normal in\nDataset A. In cases other than the above, the degree of deviation from the ideal state is case-by-case.\nThe superiority to Step-by-step i.e. ”Ours > Step-by-step” is confirmed 5 out of 8 settings and\nthis demonstrated effectiveness of proposing method i.e. end-to-end domain invariant learning with\nthree sets of data. The superiority to Normal i.e. ”Ours > Normal” is confirmed 8 out of 8 settings,\nhighlighting the positive impact of two times domain invariant learnings itself.\nCounting the cases where evaluation score is at least higher than the Lower bound, which is impor-\ntant in the UDA setting (”Ours > Lower bound”), 8/8 settings. This result means that when UDA\nis performed in business and real-world settings, a better discriminant model can be built than when\nit is not performed, highlighting its practical usefulness. Evaluation patterns and results for each\nDataset, before aggregation, are provided in the Appendix C.\n7\nFigure 3: Quantitative result overview covering 8 methods and 4 datasets.\nFigure 2: Comparison of the evaluation values between methods at each rotation angle in Dataset\nA (the left is with DANNs, right with CoRALs). Detailed results are in Appendix C Table 1 and 2\nincluding the standard deviations.\n4.3\nQUALITATIVE RESULTS\nTo begin, we investigate how our method performs learning at each epoch in terms of domain in-\nvariance and task classifiability. We simultaneously visualise the learning loss of domain invariance\nper epoch (i.e. −Ldomain = CrossEntropy(·)), the learning loss of task classifiability and the\nsynchronised evaluation of task classification on test target data. Figure 4 shows that in any case,\ndomain invariance between the source and the intermediate domain and invariance between the in-\ntermediate domain and the target are learnt in an adversarial manner and eventually a solution with\nhigh invariance is reached. Also we found that the task classifiability to the source is simultaneously\noptimised and eventually asymptotically approaches zero or small value. Correspondingly to the\nabove three learnings, the evaluation scores are improving and we can recognise that our proposed\noptimisation algorithm is effective in the point of task classification performance for target data.\nTo get more insights into our method, we investigated neural networks’ learned representation at\nboth of feature level and classifier level. The Figure 5 includes feature extractor’s learned repre-\n8\nFigure 4:\nThe Ltask, Ldomain(DS, DT ), Ldomain(DT , DT ′) and evaluation per epoch.\nThe\nLdomain(DS, DT )\nas\nLdomain(S, T)\nfor\nshort.\nColumn\nis\none\ntrial\nfor\nDataset\nA\n(source→30rotated→60rotated), C ((d,s3mini)→(e,s3)), D ((3, s)→(1, w)) with DANNs.\nFigure 5: Learned representation at different levels in the Dataset A (source→30rotated→60rotated)\nexperiment with DANNs. The first column corresponds to feature representation with domain labels\ncolor, second feature representation with task labels and third one is predictive probability for grid\nspace. Rows express methods(Ours, Step-by-step, Normal in order). Representations were gone\nthrough t-distributed stochastic neighbor embedding (t-SNE)(van der Maaten & Hinton, 2008).\n9\nFigure 6: Learned representation at feature level in one trial from Dataset C ((d,s3mini)→(e,s3))\nwith DANNs. The first row corresponds to representation with domain labels, second feature repre-\nsentation with task labels. Columns represent methods(Ours, Step-by-step, Normal in order).\nsentation, task classifier’s sigmoid probability for grid space and representation at feature level is\ncolored in two ways i.e. domain labels and task labels. We can qualitatively recognize ours could\nlearn domain invariant feature between source and target and task discriminability for target data\nwhile keeping balance between both. Even though we have not accessed any labels for the target\ndata in black in the figure, we can see a probability boundary that discriminates the data approx-\nimately perfectly. On the other hand Normal and Step-by-step could not. Normal could not get\ndomain invariance, task variance, and appropriate boundary. Step-by-step could not learn domain\ninvariance and appropriate boundary.\nThe Figure 6 shows Ours’ success of domain invariant and task variant features acquirement. Other\nmethods are struggling to get task variant features e.g. the space around the yellow-green scatter\npoints is very dense, including the other three classes. It is difficult for them to classify these human’s\nbehaviour classes (this four classes are {bike, walk, stairsup, stairsdown} far from {stand, sit},\nOurs could overcome this apparently). The Step-by-step could not learn domain invariant features\nsince we can very easily identify yellow and brown scatter points at a glance.\n5\nCONCLUSION AND FUTURE RESEARCH DIRECTION\nWe proposed novel UDA strategy whose domain invariance and task variant nature could overcome\nUDA problem with two dimensional co-variate shifts. Also our proposing free parameters tuning\nmethod is useful since it can validate UDA model automatically without access to target ground\ntruth labels.\nIn terms of research directions for the method, further improvements in accuracy can be expected\nwhen the method is used in combination with other UDA methods e.g. (Yang et al., 2024; French\net al., 2018; Sun et al., 2022; Yang et al., 2021; Singha et al., 2023). Our method is attractive because\nit is a broad abstraction that encompasses layers of deep learning and domain invariant representation\nlearning methods internally, and can be used in combination with many other methods (not limited\nto specific data type e.g. table data, image, and signal). The hyper-parameter optimisation method of\n(Yang et al., 2024) uses not the conditional distribution gap that can be measured by this method, but\nwith the hopkins statistics (Banerjee & Dave, 2004) and mutual information, transferability at clas-\nsifier level and transferability and discriminability at feature level can be measured in a combined\nmanner. Also (French et al., 2018) is based on that consistency regularisation ensures that the neural\nnetworks’ outputs are close each other for stochastic perturbations. It is well-matched with tons\nof image augmentation methods of image processing (Shorten & Khoshgoftaar, 2019) and is likely\nto improve experiments with Dataset B in particular.Safe Self-Refinement for Transformer-based\n10\ndomain adaptation uses vision transformer backbone and consistency regularization as well (Sun\net al., 2022), might improve the performance (vision transformer based UDA was also analyzed in\nanother paper (Yang et al., 2021) and improved the performance). Also large vision-language mod-\nels’ prompting was used for UDA. They format domain invariant and task variant information as the\nprompting and showed impressive performance improvements in image recognition datasets(Singha\net al., 2023).\nAnother research question is how to obtain an intermediate domain. For time series data, databases\nnormally hold huge amounts of unsupervised data(He et al., 2015; Wang et al., 2019a), so two\ndimensional co-variate shifts assumption is quite natural. For other data types such as image data,\nyour database can’t always hold a suitable intermediate. One way is to use the Web with huge\nunlabeled data. Alto this should be a future research direction e.g. generative models possibly will\ncreate suitable one based on prompting or papers (Lin et al., 2021; Zhang et al., 2019) methods\nmight help create intermediate synthetically.\nREFERENCES\nA. Banerjee and R.N. Dave. Validating clusters using the hopkins statistic. In 2004 IEEE Interna-\ntional Conference on Fuzzy Systems (IEEE Cat. No.04CH37542), volume 1, pp. 149–153 vol.1,\n2004. doi: 10.1109/FUZZY.2004.1375706.\nChristian Beckel, Wilhelm Kleiminger, Romano Cicchetti, Thorsten Staake, and Silvia Santini. The\neco data set and the performance of non-intrusive load monitoring algorithms. In Proceedings\nof the 1st ACM International Conference on Embedded Systems for Energy-Efficient Buildings\n(BuildSys 2014)., pp. 80–89. ACM, 2014.\nShai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations\nfor domain adaptation. In B. Sch¨olkopf, J. Platt, and T. Hoffman (eds.), Advances in Neural\nInformation Processing Systems, volume 19. MIT Press, 2006.\nBharath Bhushan Damodaran, Benjamin Kellenberger, R´emi Flamary, Devis Tuia, and Nicolas\nCourty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation.\nIn Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September\n8-14, 2018, Proceedings, Part IV, pp. 467–483, Berlin, Heidelberg, 2018. Springer-Verlag. ISBN\n978-3-030-01224-3. doi: 10.1007/978-3-030-01225-0 28. URL https://doi.org/10.\n1007/978-3-030-01225-0_28.\nGeoff French, Michal Mackiewicz, and Mark Fisher.\nSelf-ensembling for visual domain adap-\ntation.\nIn International Conference on Learning Representations, 2018.\nURL https://\nopenreview.net/forum?id=rkpoTaxA-.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois\nLaviolette, Mario Marchand, and Victor Lempitsky.\nDomain-Adversarial Training of Neural\nNetworks, pp. 189–209. Springer International Publishing, 2017.\nArthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano\nPontil, Kenji Fukumizu, and Bharath K. Sriperumbudur.\nOptimal kernel choice for large-\nscale two-sample tests.\nIn F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.),\nAdvances in Neural Information Processing Systems, volume 25. Curran Associates, Inc.,\n2012.\nURL https://proceedings.neurips.cc/paper_files/paper/2012/\nfile/dbe272bab69f8e13f14b405e038deb64-Paper.pdf.\nGuoliang He, Yong Duan, Yifei Li, Tieyun Qian, Jinrong He, and Xiangyang Jia. Active learn-\ning for multivariate time series classification with positive unlabeled data. In 2015 IEEE 27th\nInternational Conference on Tools with Artificial Intelligence (ICTAI), pp. 178–185, 2015. doi:\n10.1109/ICTAI.2015.38.\nNicholas D. Lane, Ye Xu, Hong Lu, Shaohan Hu, Tanzeem Choudhury, Andrew T. Campbell, and\nFeng Zhao.\nEnabling large-scale human activity inference on smartphones using community\nsimilarity networks (csn). In Proceedings of the 13th International Conference on Ubiquitous\nComputing, UbiComp ’11, pp. 355–364, New York, NY, USA, 2011. Association for Computing\n11\nMachinery. ISBN 9781450306300. doi: 10.1145/2030112.2030160. URL https://doi.\norg/10.1145/2030112.2030160.\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.\nLuojun Lin, Han Xie, Zhishu Sun, Weijie Chen, Wenxi Liu, Yuanlong Yu, and Lei Zhang. Semi-\nsupervised domain generalization with evolving intermediate domain. Pattern Recognit., 149:\n110280, 2021. URL https://api.semanticscholar.org/CorpusID:257952697.\nZiwei Liu, Zhongqi Miao, Xingang Pan, Xiaohang Zhan, Dahua Lin, Stella X. Yu, and Boqing\nGong. Open compound domain adaptation. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020.\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features\nwith deep adaptation networks. In Francis Bach and David Blei (eds.), Proceedings of the 32nd\nInternational Conference on Machine Learning, volume 37 of Proceedings of Machine Learning\nResearch, pp. 97–105, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.\nmlr.press/v37/long15.html.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading\ndigits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning\nand Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford.edu/\nhousenumbers/nips2011_housenumbers.pdf.\nHisashi Oshima, Tsuyoshi Ishizone, and Tomoyuki Higuchi. Inter-seasons and inter-households\ndomain adaptation based on danns and pseudo labeling for non-intrusive occupancy detection.\nThe Japanese Society for Artificial Intelligence, 39(5):E–O41 1–13, 2024.\nURL https://\nwww.jstage.jst.go.jp/article/tjsai/39/5/39_39-5_E-O41/_article.\nSanjay Purushotham, Wilka Carvalho, Tanachat Nilanon, and Yan Liu. Variational recurrent adver-\nsarial deep domain adaptation. In International Conference on Learning Representations, 2017.\nConnor Shorten and Taghi Khoshgoftaar. A survey on image data augmentation for deep learning.\nJournal of Big Data, 6, 07 2019. doi: 10.1186/s40537-019-0197-0.\nMainak Singha, Harsh Pal, Ankit Jha, and Biplab Banerjee. Ad-clip: Adapting domains in prompt\nspace using clip. In 2023 IEEE/CVF International Conference on Computer Vision Workshops\n(ICCVW), pp. 4357–4366, 2023. doi: 10.1109/ICCVW60793.2023.00470.\nAllan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard,\nAnind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are different: Assessing and\nmitigatingmobile sensing heterogeneities for activity recognition.\nIn Proceedings of the 13th\nACM Conference on Embedded Networked Sensor Systems, SenSys ’15, pp. 127–140, New York,\nNY, USA, 2015. Association for Computing Machinery. ISBN 9781450336314. doi: 10.1145/\n2809695.2809718. URL https://doi.org/10.1145/2809695.2809718.\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.\nIn Gang Hua and Herv´e J´egou (eds.), Computer Vision – ECCV 2016 Workshops, pp. 443–450,\nCham, 2016. Springer International Publishing. ISBN 978-3-319-49409-8.\nTao Sun, Cheng Lu, Tianshuo Zhang, and Haibin Ling. Safe self-refinement for transformer-based\ndomain adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 7181–7190, 2022. doi: 10.1109/CVPR52688.2022.00705.\nLaurens van der Maaten and Geoffrey Hinton.\nVisualizing data using t-sne.\nJournal of Ma-\nchine Learning Research, 9(86):2579–2605, 2008. URL http://jmlr.org/papers/v9/\nvandermaaten08a.html.\nHaishuai Wang, Qin Zhang, Jia Wu, Shirui Pan, and Yixin Chen. Time series feature learning\nwith labeled and unlabeled data. Pattern Recognition, 89:55–66, 2019a. ISSN 0031-3203. doi:\nhttps://doi.org/10.1016/j.patcog.2018.12.026. URL https://www.sciencedirect.com/\nscience/article/pii/S0031320318304473.\n12\nYifei Wang, Wen Li, Dengxin Dai, and Luc Van Gool. Deep domain adaptation by geodesic distance\nminimization. 2017 IEEE International Conference on Computer Vision Workshops (ICCVW), pp.\n2651–2657, 2017. URL https://api.semanticscholar.org/CorpusID:4552286.\nZirui Wang, Zihang Dai, Barnab´as P´oczos, and Jaime Carbonell.\nCharacterizing and avoiding\nnegative transfer. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 11285–11294, 2019b. doi: 10.1109/CVPR.2019.01155.\nGary Weiss and Jeffrey Lockhart. The impact of personalization on smartphone-based activity recog-\nnition. AAAI Workshop - Technical Report, 01 2012.\nGarrett Wilson and Diane J. Cook. A survey of unsupervised deep domain adaptation. ACM Trans.\nIntell. Syst. Technol., 11(5):1–46, July 2020.\nGarrett Wilson, Janardhan Rao Doppa, and Diane J. Cook. Multi-source deep domain adaptation\nwith weak supervision for time-series sensor data. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, pp. 1768–1778. ACM, 2020.\nJianfei Yang, Hanjie Qian, Yuecong Xu, Kai Wang, and Lihua Xie. Can we evaluate domain adapta-\ntion models without target-domain labels? In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=fszrlQ2DuP.\nJinyu Yang, Jingjing Liu, Ning Xu, and Junzhou Huang. Tvt: Transferable vision transformer for\nunsupervised domain adaptation. 2023 IEEE/CVF Winter Conference on Applications of Com-\nputer Vision (WACV), pp. 520–530, 2021. URL https://api.semanticscholar.org/\nCorpusID:237048276.\nLei Zhang, Shanshan Wang, Guang-Bin Huang, Wangmeng Zuo, Jian Yang, and David Zhang. Man-\nifold criterion guided transfer learning via intermediate domain generation. IEEE Transactions on\nNeural Networks and Learning Systems, 30(12):3759–3773, 2019. doi: 10.1109/TNNLS.2019.\n2899037.\nYun Zhang, Nianbin Wang, Shaobin Cai, and Lei Song.\nUnsupervised domain adaptation by\nmapped correlation alignment. IEEE Access, 6:44698–44706, 2018a. doi: 10.1109/ACCESS.\n2018.2865249.\nZhen Zhang, Mianzhi Wang, Yan Huang, and Arye Nehorai. Aligning infinite-dimensional covari-\nance matrices in reproducing kernel hilbert spaces for domain adaptation. In 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 3437–3445, 2018b. doi: 10.1109/\nCVPR.2018.00362.\nH. Zhao, R´emi Tachet des Combes, Kun Zhang, and Geoffrey J. Gordon. On learning invariant\nrepresentations for domain adaptation. In International Conference on Machine Learning(2019),\n2019.\nErheng Zhong, Wei Fan, Qiang Yang, Olivier Verscheure, and Jiangtao Ren.\nCross validation\nframework to choose amongst models and datasets for transfer learning. In Jos´e Luis Balc´azar,\nFrancesco Bonchi, Aristides Gionis, and Mich`ele Sebag (eds.), Machine Learning and Knowl-\nedge Discovery in Databases, pp. 547–562, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg.\nISBN 978-3-642-15939-8.\nA\nCORALS VERSION TWO STAGES DOMAIN INVARIANT LEARNERS\nThe pseudo code for Ours with CoRALs is in Algorithm 3 and the schematic diagram is in Figure\n7.\n13\nAlgorithm 3 2stages-CoRALs\nRequire: source,intermediate domain,target DS, DT , DT ′\nEnsure: neural network parameters {θf, θc}\n1: θf, θc ←init()\n2: while epoch training() do\n3:\nwhile batch training() do\n4:\nˆE(Ldomain(DS, DT )) ←\n1\nbatch\nPbatch\ni=1\nMSE(Cov(C(F(xS\ni ))), Cov(C(F(xT\ni ))))\n5:\nˆE(Ldomain(DS, DT ′)) ←\n1\nbatch\nPbatch\ni=1\nMSE(Cov(C(F(xS\ni ))), Cov(C(F(xT ′\ni ))))\n6:\nˆE(Ltask) ←\n1\nbatch\nPbatch\ni=1\nCE(C(F(xS\ni )), yS\ni )\n7:\nθc ←θc −∂(ˆE(Ltask)+ˆE(Ldomain(DS,DT ))+ˆE(Ldomain(DS,DT ′)))\n∂θc\n8:\nθf ←θf −∂(ˆE(Ltask)+ˆE(Ldomain(DS,DT ))+ˆE(Ldomain(DS,DT ′)))\n∂θf\n9:\nend while\n10: end while\nFigure 7: Forward path and backward process of Normal(CoRALs), Step-by-step, Ours.\nThe\nLtask, Ldomain as Lc, Ld for short.\n14\nTable 1: Dataset A with DANNs. The best method in each PAT except for Train on Target was\nspecified in bold. The value inside of parentheses is the standard deviation of 10 times evaluations,\nwe omitted the depictions for Dataset B-D.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(DANNs)\nWithout Adapt\n15rotated→30rotated\n1(0)\n0.868(.07)\n0.919(.06)\n0.875(.07)\n0.775(.02)\n20rotated→40rotated\n1(0)\n0.869(.08)\n0.879(.09)\n0.873(.06)\n0.619(.03)\n25rotated→50rotated\n1(0)\n0.805(.06)\n0.830(.1)\n0.793(.1)\n0.533(.02)\n30rotated→60rotated\n1(0)\n0.834(.04)\n0.813(.07)\n0.660(.2)\n0.439(.05)\n35rotated→70rotated\n1(0)\n0.774(.1)\n0.700(.1)\n0.683(.2)\n0.339(.01)\nAverage\n1(0)\n0.830(.07)\n0.828(.09)\n0.777(.1)\n0.541(.03)\nTable 2: Dataset A with CoRALs.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(CoRALs)\nWithout Adapt\n15rotated→30rotated\n1(0)\n0.923(.06)\n0.936(.07)\n0.855(.1)\n0.784(.04)\n20rotated→40rotated\n1(0)\n0.853(.09)\n0.925(.07)\n0.731(.09)\n0.631(.03)\n25rotated→50rotated\n1(0)\n0.633(.1)\n0.780(.1)\n0.560(.07)\n0.529(.03)\n30rotated→60rotated\n1(0)\n0.553(.2)\n0.656(.2)\n0.512(.06)\n0.420(.02)\n35rotated→70rotated\n1(0)\n0.475(.1)\n0.547(.1)\n0.437(.2)\n0.349(.01)\nAverage\n1(0)\n0.687(.1)\n0.769(.1)\n0.619(.08)\n0.543(.03)\nB\nPROOF OF REVERSE VALIDATION BASED FREE PARAMETERS TUNING\nLine\n2 in\nAlgorithm\n2\nis saying\nlearned\nϕ\nshould\nbe\napproximation\nof\nmixing\nof\nPS(y|x), PT (y|x), PT ′(y|x) and line 4 can be seen in the same way. We omitted the descriptions of\napproximation errors.\nP(y|x, ϕ) = (1 −γ −δ)PS(y|x) + γPT (y|x) + δPT ′(y|x)\nP(y|x, ϕr) = (1 −α −β)P(y|x, ϕ) + αPS(y|x) + βPT (y|x)\nWhere α, β, γ, δ are the nuisance parameters related to the ratio between the sizes of distributions.\n|ϕr(xS) −yS|\n=\n|P(y|x, ϕr) −PS(y|x)|\n=\n|(1 −α −β)P(y|x, ϕ) + αPS(y|x) + βPT (y|x) −PS(y|x)|\n=\n|(1 −α −β)P(y|x, ϕ) + βPT (y|x) −\n1 −α\n1 −γ −δ {P(y|x, ϕ) −\nγPT (y|x) −δPT ′(y|x)}|\n|ϕr(xS) −yS|(1 −γ −δ)\n=\n|{(1 −α −β)P(y|x, ϕ) + βPT (y|x) −\n1 −α\n1 −γ −δ {P(y|x, ϕ) −\nγPT (y|x) −δPT ′(y|x)}}(1 −γ −δ)|\n=\n|C1P(y|x, ϕ) + C2PT (y|x) + C3PT ′(y|x)|\n=\n|C1P(y|x, ϕ) + {C2 + C3}PT ′(y|x) + C2{PT (y|x) −PT ′(y|x)}|\n|ϕr(xS) −yS|\n=\n|C1{P(y|x, ϕ) −PT ′(y|x)} + C2{PT (y|x) −PT ′(y|x)}|C4\nWe denoted fixed values as C1, C2, C3, C4 respectively.C1 = (1 −α −β)(1 −γ −δ) −(1 −α),\nC2 = β(1 −γ −δ) + (1 −α)γ, C3 = (1 −α)δ, C4 =\n1\n1−γ−δ\nC\nQUANTITATIVE RESULTS IN DETAIL\nDetailed results including the pattern of each data, each method and each domain adaptation task(as\nPAT for short in these tables) before calculating the average for each dataset and method are in Table\n2-13.\nD\nIMPLEMENTATION CONFIGURATION IN DETAIL\nData load and pre-processing steps are same as UDA previous studies (Ganin et al., 2017; Wilson\net al., 2020; Oshima et al., 2024) (experiment with Dataset A and B from (Ganin et al., 2017),\n15\nTable 3: Dataset A with JDOT.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(JDOT)\nWithout Adapt\n15rotated→30rotated\n1(0)\n0.934(.02)\n0.895(.07)\n0.8882(.04)\n0.782(.04)\n20rotated→40rotated\n1(0)\n0.839(.08)\n0.764(.05)\n0.787(.04)\n0.617(.03)\n25rotated→50rotated\n1(0)\n0.720(.1)\n0.677(.04)\n0.707(.09)\n0.530(.04)\n30rotated→60rotated\n1(0)\n0.620(.1)\n0.522(.03)\n0.554(.1)\n0.427(.02)\n35rotated→70rotated\n1(0)\n0.492(.1)\n0.440(.08)\n0.434(.05)\n0.329(.01)\nAverage\n1(0)\n0.721(.09)\n0.660(.05)\n0.673(.06)\n0.537(.03)\nTable 4: Dataset B with DANNs.\nTrial index\nTrain on Target\nOurs\nStep-by-step\nNormal(DANNs)\nWithout Adapt\n0\n0.8142286539077759\n0.2569529712200165\n0.31246158480644226\n0.2764674127101898\n0.2764674127101898\n1\n0.8248693943023682\n0.29966962337493896\n0.29859402775764465\n0.29452213644981384\n0.288836807012558\n2\n0.8270590305328369\n0.365281194448471\n0.29548248648643494\n0.2744314670562744\n0.3106561303138733\nAverage\n0.8220523596\n0.307301263014475\n0.302179366350173\n0.28019360701243\n0.29198678334554\nTable 5: Dataset B with CoRALs.\nTrial index\nTrain on Target\nOurs\nStep-by-step\nNormal(CoRALs)\nWithout Adapt\n0\n0.824062705039978\n0.28626304864883423\n0.2970958948135376\n0.1885755956172943\n0.2821911573410034\n1\n0.8216425776481628\n0.27185770869255066\n0.267324835062027\n0.2312154322862625\n0.23025506734848022\n2\n0.7872233986854553\n0.231292262673378\n0.2524969279766083\n0.20732176303863525\n0.2641364336013794\nAverage\n0.810976227124532\n0.263137673338254\n0.272305885950724\n0.20903759698073\n0.258860886096954\nTable 6: Dataset B with JDOT.\nTrial index\nTrain on Target\nOurs\nStep-by-step\nNormal(JDOT)\nWithout Adapt\n0\n0.766364455223083\n0.275046110153198\n0.252228021621704\n0.252727419137954\n0.274469882249832\n1\n0.829706487655639\n0.281576514244079\n0.303741544485092\n0.273547947406768\n0.28322833776474\n2\n0.763560235500335\n0.288875222206115\n0.23782268166542\n0.213429629802703\n0.258681625127792\nAverage\n0.786543726126352\n0.281832615534464\n0.264597415924072\n0.246568332115808\n0.272126615047455\nTable 7: Dataset C with DANNs.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(DANNs)\nWithout Adapt\n(d,nexus4)→(f,samsungold)\n0.979234308\n0.333990708\n0.3791183114\n0.2445475519\n0.3040603146\n(f,s3mini)→(g,s3)\n0.9537375212\n0.6039866924\n0.3563122801\n0.4890365332\n0.4035714209\n(d,s3mini)→(e,s3)\n0.9632268667\n0.8621621609\n0.6398853391\n0.6870597839\n0.8511875629\n(b,s3)→(f,s3mini)\n0.9091445386\n0.7703539729\n0.6380531043\n0.6666666627\n0.7166666567\n(a,nexus4)→(d,s3)\n0.9471365869\n0.5560352564\n0.4770925194\n0.5336563945\n0.3418502301\n(d,s3)→(e,samsungold)\n0.9668659866\n0.3383971155\n0.3572966397\n0.3135167331\n0.2260765433\n(e,s3mini)→(i,nexus4)\n0.9696132898\n0.633001852\n0.5512707353\n0.5638305902\n0.5797790289\n(e,samsungold)→(f,s3)\n0.9335558951\n0.4998330414\n0.4212019861\n0.4297161847\n0.3644407243\n(f,samsungold)→(h,s3)\n0.9565408528\n0.230904308\n0.2175592646\n0.2302897334\n0.2808604091\n(f,s3mini)→(g,nexus4)\n0.9656078279\n0.5516470492\n0.3412548937\n0.4018039137\n0.3747843146\n(b,samsungold)→(h,s3mini)\n0.8693650961\n0.2890476286\n0.2026984192\n0.2434920691\n0.2863492191\n(c,s3)→(i,nexus4)\n0.9692449689\n0.5139595062\n0.5257458717\n0.5067403525\n0.2813628078\n(a,nexus4)→(e,s3mini)\n0.8998363554\n0.5181669414\n0.512438634\n0.5076923162\n0.3450081885\n(h,s3)→(i,nexus4)\n0.9661510408\n0.5531123698\n0.5488398015\n0.558121568\n0.476427266\n(b,nexus4)→(e,s3mini)\n0.9135843039\n0.588052386\n0.5351882339\n0.508019653\n0.4073649794\n(a,s3)→(b,samsungold)\n0.9768714905\n0.2908379823\n0.2344133988\n0.2617877066\n0.2089385405\nAverage\n0.9462323081\n0.5083430607\n0.4336480896\n0.4466236092\n0.4030455129\nTable 8: Dataset C with CoRALs.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(CoRALs)\nWithout Adapt\n(d,nexus4)→(f,samsungold)\n0.979234308\n0.2712296873\n0.3948955804\n0.2451276004\n0.3040603146\n(f,s3mini)→(g,s3)\n0.9537375212\n0.5656976521\n0.5803986609\n0.5835548103\n0.4035714209\n(d,s3mini)→(e,s3)\n0.9632268667\n0.8188370228\n0.7868959963\n0.8158067286\n0.8511875629\n(b,s3)→(f,s3mini)\n0.9091445386\n0.7690265477\n0.7792035401\n0.7799409986\n0.7166666567\n(a,nexus4)→(d,s3)\n0.9471365869\n0.5544493496\n0.5429075032\n0.529603532\n0.3418502301\n(d,s3)→(e,samsungold)\n0.9668659866\n0.3571770191\n0.2886363477\n0.3734449655\n0.2260765433\n(e,s3mini)→(i,nexus4)\n0.9696132898\n0.6311602473\n0.7156169653\n0.5859300375\n0.5797790289\n(e,samsungold)→(f,s3)\n0.9335558951\n0.5492487252\n0.5727879584\n0.52420699\n0.3644407243\n(f,samsungold)→(h,s3)\n0.9565408528\n0.2807726175\n0.2603160739\n0.274363485\n0.2808604091\n(f,s3mini)→(g,nexus4)\n0.9656078279\n0.5189019471\n0.4879607767\n0.567921567\n0.3747843146\n(b,samsungold)→(h,s3mini)\n0.8693650961\n0.2895238206\n0.2641269937\n0.2858730286\n0.2863492191\n(c,s3)→(i,nexus4)\n0.9692449689\n0.6111602366\n0.6216574728\n0.5978268981\n0.2813628078\n(a,nexus4)→(e,s3mini)\n0.8998363554\n0.5220949322\n0.5574468166\n0.639607209\n0.3450081885\n(h,s3)→(i,nexus4)\n0.9661510408\n0.5809944987\n0.5965377748\n0.543167603\n0.476427266\n(b,nexus4)→(e,s3mini)\n0.9135843039\n0.7222586095\n0.7522095025\n0.5345335573\n0.4073649794\n(a,s3)→(b,samsungold)\n0.9768714905\n0.2730726153\n0.2655865803\n0.3111731768\n0.2089385405\nAverage\n0.9462323081\n0.5197253455\n0.529199034\n0.5120051367\n0.4030455129\n16\nTable 9: Dataset C with JDOT.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(JDOT)\nWithout Adapt\n(d,nexus4)→(f,samsungold)\n0.979234308\n0.355220401287078\n0.317865419387817\n0.321693730354309\n0.3040603146\n(f,s3mini)→(g,s3)\n0.9537375212\n0.503405305743217\n0.438787361979484\n0.503405302762985\n0.4035714209\n(d,s3mini)→(e,s3)\n0.9632268667\n0.851760858297348\n0.866830480098724\n0.864209669828415\n0.8511875629\n(b,s3)→(f,s3mini)\n0.9091445386\n0.74144542813301\n0.73274335861206\n0.754129791259765\n0.7166666567\n(a,nexus4)→(d,s3)\n0.9471365869\n0.389691638946533\n0.348017627000808\n0.392158597707748\n0.3418502301\n(d,s3)→(e,samsungold)\n0.9668659866\n0.339234438538551\n0.336124384403228\n0.33349280655384\n0.2260765433\n(e,s3mini)→(i,nexus4)\n0.9696132898\n0.58184163570404\n0.605451226234436\n0.588397806882858\n0.5797790289\n(e,samsungold)→(f,s3)\n0.9335558951\n0.534557574987411\n0.497829702496528\n0.527045065164566\n0.3644407243\n(f,samsungold)→(h,s3)\n0.9565408528\n0.310798954963684\n0.146356455236673\n0.268832318484783\n0.2808604091\n(f,s3mini)→(g,nexus4)\n0.9656078279\n0.446235281229019\n0.332862740755081\n0.438745093345642\n0.3747843146\n(b,samsungold)→(h,s3mini)\n0.8693650961\n0.287936517596244\n0.298888900876045\n0.278095249831676\n0.2863492191\n(c,s3)→(i,nexus4)\n0.9692449689\n0.376316770911216\n0.30895028412342\n0.308692456781864\n0.2813628078\n(a,nexus4)→(e,s3mini)\n0.8998363554\n0.32962357699871\n0.376268419623374\n0.335024553537368\n0.3450081885\n(h,s3)→(i,nexus4)\n0.9661510408\n0.539668530225753\n0.493407014012336\n0.518563541769981\n0.476427266\n(b,nexus4)→(e,s3mini)\n0.9135843039\n0.377741411328315\n0.53698855638504\n0.38936171233654\n0.4073649794\n(a,s3)→(b,samsungold)\n0.9768714905\n0.264245799183845\n0.202234633266925\n0.258994407951831\n0.2089385405\nAverage\n0.9462323081\n0.451857757754623\n0.427475410280749\n0.442552631534636\n0.4030455129\nTable 10: Dataset D with DANNs.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(DANNs)\nWithout Adapt\n(1, w)→(2, s)\n0.8957642913\n0.7409972489\n0.7875346422\n0.7360110939\n0.6867036104\n(1, w)→(3, s)\n0.861866653\n0.7052208841\n0.7192771018\n0.6852744341\n0.6829986632\n(2, w)→(1, s)\n0.8012861729\n0.693053323\n0.7054927468\n0.6919224679\n0.6621971011\n(2, w)→(3, s)\n0.8601333261\n0.7986613035\n0.7954484522\n0.8053547502\n0.8078982592\n(3, w)→(1, s)\n0.807395494\n0.6822294176\n0.7321486413\n0.6969305456\n0.6933764279\n(3, w)→(2, s)\n0.8927256107\n0.7096952975\n0.7237303913\n0.696583581\n0.6832871735\n(4, w)→(5, s)\n0.8287172318\n0.8516837239\n0.853587091\n0.8380673289\n0.81859442\n(5, w)→(4, s)\n0.8698020101\n0.876616919\n0.8845771074\n0.8573797703\n0.8353233814\n(1, s)→(2, w)\n0.8868200541\n0.8035789073\n0.8214736462\n0.8545262694\n0.8671578526\n(1, s)→(3, w)\n0.780769217\n0.7954063714\n0.66077739\n0.8070671439\n0.7749116659\n(2, s)→(1, w)\n0.6769754767\n0.8073871732\n0.7716826499\n0.8065663874\n0.7937072694\n(2, s)→(3, w)\n0.7828671217\n0.7583038926\n0.7551236808\n0.760777396\n0.7763250887\n(3, s)→(1, w)\n0.724523145\n0.8347469509\n0.7835841656\n0.8384405255\n0.8228454411\n(3, s)→(2, w)\n0.8864016414\n0.8627368033\n0.8244210184\n0.8679999769\n0.8871578455\n(4, s)→(5, w)\n0.7400809884\n0.8338086188\n0.7645621732\n0.8256619632\n0.84969455\n(5, s)→(4, w)\n0.7781984448\n0.9292267561\n0.9292267561\n0.9102228284\n0.7644823313\nAverage\n0.8171454299\n0.7927095994\n0.7820404784\n0.7924241539\n0.7754163176\nTable 11: Dataset D with CoRALs.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(CoRALs)\nWithout Adapt\n(1, w)→(2, s)\n0.8957642913\n0.7522622466\n0.7524469137\n0.7475531042\n0.6867036104\n(1, w)→(3, s)\n0.861866653\n0.7228915513\n0.7121820569\n0.7263721406\n0.6829986632\n(2, w)→(1, s)\n0.8012861729\n0.6936995268\n0.6898223042\n0.6877221525\n0.6621971011\n(2, w)→(3, s)\n0.8601333261\n0.8285140514\n0.8327978611\n0.8334672034\n0.8078982592\n(3, w)→(1, s)\n0.807395494\n0.7008077681\n0.7260097086\n0.6957996964\n0.6933764279\n(3, w)→(2, s)\n0.8927256107\n0.7213296473\n0.7214219868\n0.7216989934\n0.6832871735\n(4, w)→(5, s)\n0.8287172318\n0.853587091\n0.853587091\n0.853587091\n0.81859442\n(5, w)→(4, s)\n0.8698020101\n0.8825870633\n0.8850746214\n0.8812603652\n0.8353233814\n(1, s)→(2, w)\n0.8868200541\n0.831789434\n0.8061052263\n0.7663157582\n0.8671578526\n(1, s)→(3, w)\n0.780769217\n0.7992932916\n0.7176678598\n0.7975265026\n0.7749116659\n(2, s)→(1, w)\n0.6769754767\n0.7957592607\n0.7577291667\n0.7726402462\n0.7937072694\n(2, s)→(3, w)\n0.7828671217\n0.7575971723\n0.6526501805\n0.7731448889\n0.7763250887\n(3, s)→(1, w)\n0.724523145\n0.8332421601\n0.8025992155\n0.8411765158\n0.8228454411\n(3, s)→(2, w)\n0.8864016414\n0.8532631218\n0.8418946981\n0.8774736524\n0.8871578455\n(4, s)→(5, w)\n0.7400809884\n0.8350306153\n0.8350306153\n0.8350306153\n0.84969455\n(5, s)→(4, w)\n0.7781984448\n0.9263434052\n0.9292267561\n0.9259502232\n0.7644823313\nAverage\n0.8171454299\n0.7992498379\n0.7822653914\n0.7960449468\n0.7754163176\n17\nTable 12: Dataset D with JDOT.\nPAT\nTrain on Target\nOurs\nStep-by-step\nNormal(JDOT)\nWithout Adapt\n(1, w)→(2, s)\n0.8957642913\n0.73711912035942\n0.7653739690780\n0.717543876171112\n0.6867036104\n(1, w)→(3, s)\n0.861866653\n0.70575635433197\n0.7164658486843\n0.696385538578033\n0.6829986632\n(2, w)→(1, s)\n0.8012861729\n0.7024232804723\n0.691599369049072\n0.686106634140014\n0.6621971011\n(2, w)→(3, s)\n0.8601333261\n0.8397590339188\n0.829183393716812\n0.810575628280639\n0.8078982592\n(3, w)→(1, s)\n0.807395494\n0.689499223232269\n0.7218093872031\n0.690468513965606\n0.6933764279\n(3, w)→(2, s)\n0.8927256107\n0.705632507801055\n0.7588181078499\n0.710249316692352\n0.6832871735\n(4, w)→(5, s)\n0.8287172318\n0.853294265270233\n0.8535870909690\n0.851098072528839\n0.81859442\n(5, w)→(4, s)\n0.8698020101\n0.866003310680389\n0.791708122938871\n0.8747927069664\n0.8353233814\n(1, s)→(2, w)\n0.8868200541\n0.82652627825737\n0.815578907728195\n0.853052592277526\n0.8671578526\n(1, s)→(3, w)\n0.780769217\n0.7865724503998\n0.74770318865776\n0.763250893354415\n0.7749116659\n(2, s)→(1, w)\n0.6769754767\n0.8117647409408\n0.786183339357376\n0.810807144641876\n0.7937072694\n(2, s)→(3, w)\n0.7828671217\n0.768551242351531\n0.7830388784407\n0.764664322137832\n0.7763250887\n(3, s)→(1, w)\n0.724523145\n0.8399453103542\n0.777428218722343\n0.808344769477844\n0.8228454411\n(3, s)→(2, w)\n0.8864016414\n0.867789441347122\n0.855368375778198\n0.863999962806701\n0.887157845\n(4, s)→(5, w)\n0.7400809884\n0.838900262117385\n0.835030615329742\n0.832179284095764\n0.84969455\n(5, s)→(4, w)\n0.7781984448\n0.832503297179937\n0.91310617923\n0.871428591012954\n0.7644823313\nAverage\n0.8171454299\n0.7920025074388\n0.790123937046155\n0.787809240445494\n0.7754163176\nDataset C from (Wilson et al., 2020), Dataset D from (Oshima et al., 2024)). For standardisation pre-\nprocessing, the statistics of the target test data are not accessed, only the statistics of the training data\nare accessed and executed. Internal layers are same between methods, Normal and Step-by-step and\nOurs have same layers and same shape of F, C, D respectively though the number of components is\nnot the same since e.g. Normal does not have two domain discriminators. The number of F, C when\ninference is same. Internal layers are from previous studies, Dataset A with shallow neural networks\nis from (Ganin et al., 2017), Dataset B with CNN based back bone from (Ganin et al., 2017), Dataset\nC with one dimensional CNN based from Figure 3 in (Wilson et al., 2020) and Dataset D from Figure\n4 and section 4.3. in (Oshima et al., 2024). In the settings during learning, a fixed learning rate is\nadopted for Dataset A and B. For the other Datasets, the learning rate is determined by optimising\nfrom 0.001-0.00001 using the Theorem 3.1 method. In Step-by-step and Normal, the (Ganin et al.,\n2017) method is used to perform the optimisation. Terminal Evaluation step for target data is exactly\nsame for any method, the feature extractor and task classifier are applied to the 50% of target data\nnot used for training in any sense and their predictions are compared with the ground truth labels\nto calculate the accuracy. The number of repetitions is three only for Dataset B. The training set of\nDT ′ and test set are identical in Dataset A experiment.\nE\nTWO DIMENSIONAL CO-VARIATE SHIFTS OBSERVATION\nIn this section, we confirm whether or not four datasets are following that two dimensional co-\nvariate shifts assumption we introduced in the 2.2. We need to check dataset-wise, (1) existence\nof marginal distributions shifts between DS, DT , DT ′ (2) mostly sharing of conditional distribution\nP(y|x) between any domains. Apparently Dataset A follows that since the three data sets retain co-\nvariate misalignment based on the semi-clockwise rotation action, and without this misalignment the\nlabelling rules are perfectly consistent (Figure 8), likewise DS, DT in Dataset B can be understood\non the action of coloring digit part and background part (Figure 9). About the DT ′ in Dataset B,\nalthough the explicit action does not return to DS, DT , and includes shifts about labelling rules as\nwell as simple two dimensional co-variate shifts, we speculate a certain sharing of rule based on the\nfacts that we can identify numbers with the human eyes.\nPrevious work with Dataset C showed their co-variate shift existence between users, so they did\nthe UDA experiment (Wilson et al., 2020) (e.g. see differences between user f and user g in Figure\n10). It has been suggested in similar studies (Lane et al., 2011; Weiss & Lockhart, 2012) as well\nthat the characteristics of the data measured will change depending on the age and user of the same\nbehaviour, e.g. ML models trained on data from one age group will not generalise to different age\ngroups. We additionally assume that different models of accelerometer equipment generate further\nshift based on the analysis by (Stisen et al., 2015) (e.g. see differences between model s3mini and\nmodel s3 in Figure 10). The measurement of the same phenomenon under static conditions with\ndifferent models demonstrated that the distribution of measured values can be very different (please\ncheck Figure 1 in (Stisen et al., 2015)). Because all the data for a given model and a given user\nare taking a predetermined pattern of action classes, we can speculate that P(y|x) is also sharing\nbasically.\n18\nFigure 8: Data examples from Dataset A. Left is the source→15rotated→30rotated pattern, middle\nis 50 rotated target, right is 70 rotated target.\nFigure 9: Data examples from Dataset B.\n19\nFigure 10: Data examples from Dataset C with (f,s3mini)→(g,s3). We omitted the plots of gyro-\nscope data, blue plots correspond to the average of x-axis accelerometer data, red plots to the average\nof y-axis and green plots to the average of z-axis.\nIt is described by qualitative analysis of the previous study as co-variate shifts that satisfies a certain\ndegree of shared labelling rules in the Dataset D (Oshima et al., 2024). The dynamics include\ndifferent frequency of electricity peaks for certain time intervals between households (differences in\nthe number of family members) or between seasons, while the previously mentioned rule of being at\nhome if the electricity consumption is large and absent if it is small is shared (please check Figure 2\nin (Oshima et al., 2024)). In particular, the Dataset D subjects live in Switzerland, where electricity\nconsumption tends to vary significantly between summer and winter, and is generally higher in\nwinter.\nF\nAPPLIED RESEARCHES’ FUTURE RESEARCH DIRECTION\nA promising direction for applied research is to evaluate the method with data and tasks that are in\nhigh demand for other social implementations, and to promote the application of UDA in business\nand the real world. For example, the problem of determining whether a patient with acute hypoxemic\nrespiratory failure died during hospitalisation from medical data (e.g. blood pH and arterial blood\noxygen partial pressure) and the problem of classifying the name of the disease,(Purushotham et al.,\n2017) may result in distribution shifts due to two dimensional data domains between different ages\nand different sexes. Semantic image segmentation in self-driving also may put a need for UDA\nbetween whether conditions and between a.m. or p.m. e.g. (Noon, Sunny)→(Noon, Rainy)→(Night,\nRainy) (Liu et al., 2020).\nG\nJDOT VERSION TWO STAGES DOMAIN INVARIANT LEARNERS AND\nEXPERIMENTAL VALIDATION\nAlgorithm and schematic diagram are in Algorithm 4 and Figure 11. In the pseudo code, we denote\noptimal transport solution for sample i and sample j as OTi,j for short. Figure 12 shows two-stages\nJDOT’s superiority to previous studies, namely we found that ”Upper bound > Ours > Max(Step-\nby-step, Normal, Lower bound)” for 4 out of 4 datasets. Figure 13 also says that evaluation for ours\nis always better than Normal and Step-by-step when any rotated target data in Dataset A.\n20\nFigure 11: Forward path and backward process of Normal(JDOT), Step-by-step, Ours.\nThe\nLtask, Ldomain as Lc, Ld for short. The OptimalTransporti,j in this figure is optimal transport\nsolution for source samplei and target samplej.\nAlgorithm 4 2stages-JDOT\nRequire: source,intermediate domain,target DS, DT , DT ′\nEnsure: neural network parameters {θf, θc}\n1: θf, θc ←init()\n2: while epoch training() do\n3:\nwhile batch training() do\n4:\nˆE(Ldomain(DS, DT )) ←\n1\nbatch×batch\nPbatch\ni=1\nPbatch\nj=1 {||F(xS\ni ) −F(xT\nj )||2 + CE(C(F(xT\nj )), yS\ni )} × OTi,j\n5:\nˆE(Ldomain(DT , DT ′)) ←\n1\nbatch×batch\nPbatch\ni=1\nPbatch\nj=1 {||F(xT\ni ) −F(xT ′\nj )||2 + CE(C(F(xT ′\nj )), yS\ni )} × OTi,j\n6:\nˆE(Ltask) ←\n1\nbatch\nPbatch\ni=1\nCE(C(F(xS\ni )), yS\ni )\n7:\nθc ←θc −∂(ˆE(Ltask)+ˆE(Ldomain(DS,DT ))+ˆE(Ldomain(DT ,DT ′)))\n∂θc\n8:\nθf ←θf −∂(ˆE(Ltask)+ˆE(Ldomain(DS,DT ))+ˆE(Ldomain(DT ,DT ′)))\n∂θf\n9:\nend while\n10: end while\nH\nEMPIRICAL STUDY: EFFECTS OF TWO STAGES DOMAIN INVARIANT\nLEARNERS FREE PARAMETER INDICATOR\nWe analyzed whether there is a positive correlation between RV-based indicators (left of Theorem\n3.1) and actual losses (1st term, right of Theorem 3.1). Using dataset A, learning rate was varied\nfrom 0.00000001 −0.1, and the RV-based score (cross entropy loss) was calculated for the model\nas the result of UDA learning. At the same time, actual losses (cross entropy loss) were calculated\nusing target data for testing. Finally, pearson correlation coefficient was calculated to determine\nif there was a positive correlation between the two scores. Table 13 shows that there is a high\npositive correlation for all patterns. This result supports that two stages domain invariant learners\nfree parameter indicator is useful in UDA experiments.\n21\nFigure 12: Quantitative result overview with JDOT\nFigure 13: Quantitative result with JDOT in Dataset A\n22\nTable 13: Correlation measurements between Theorem 3.1 indicators and actual target ground truth losses using Dataset A and DANNs module.\nPAT\n15rotated→30rotated\n20rotated→40rotated\n25rotated→50rotated\n30rotated→60rotated\n35rotated→70rotated\nMethod\nRV based loss\nGround truth loss\nRV based loss\nGround truth loss\nRV based loss\nGround truth loss\nRV based loss\nGround truth loss\nRV based loss\nGround truth loss\n0.00000001\n0.705\n0.709\n0.716\n0.694\n0.700\n0.700\n0.712\n0.696\n0.687\n0.694\n0.0000001\n0.694\n0.706\n0.691\n0.701\n0.681\n0.698\n0.693\n0.698\n0.690\n0.698\n0.000001\n0.685\n0.698\n0.730\n0.699\n0.709\n0.700\n0.690\n0.657\n0.690\n0.686\n0.00001\n0.692\n0.683\n0.706\n0.612\n0.687\n0.664\n0.713\n0.950\n0.693\n0.689\n0.0001\n0.713\n0.547\n0.707\n0.785\n0.720\n1.47\n0.713\n0.950\n0.704\n1.72\n0.001\n0.683\n0.121\n0.729\n0.683\n0.691\n0.550\n0.686\n0.333\n0.696\n1.19\n0.01\n6.83\n0.953\n3.87\n1.64\n6.87\n0.97\n6.49\n5.54\n3.91\n2.61\n0.1(learning rate)\n14.7\n11.7\n16.1\n12.6\n22.3\n33.5\n3.65\n13.4\n5.05\n11.8\nCorr\n0.92\n0.992\n0.960\n0.671\n0.859\n23\nI\nBENCHMARKS DESCRIPTION FOR 4 EXPERIMENTAL VALIDATION\nWe adopted six benchmark models for the comparison test and described input when training, input\nwhen inference and what the meaning is when compared to Ours.\nTable 14: Benchmarks list.\nMethod\nInput\nWhen Training\nInput\nWhen Inference\nRole and Note\nTrain on Target\nTraining data of DT ′ with its labels\nTest data of DT ′\nCall as Upper bound.\nTraining F, C with target ground truth labels\n(other methods cannot access to),\nvalidation on its test data.\nNormal(DANNs,CoRALs)\nDS, training set of DT ′\nsame as above\nNormal domain invariant learners.\nOur methods should be better than these.\nInternal layers are from\n(Ganin et al., 2017; Wilson et al., 2020)\n(Oshima et al., 2024)\n.\nStep-by-step\nidentical to ours\nDS, DT and training data of DT ′\nsame as above\nStep by step domain invariant learners.\nOur methods should be better than these.\nWe can implement this with CoRALs easily.\n(Oshima et al., 2024) did not do that.\nWithout Adapt\nDS\nsame as above\nCall as Lower bound.\nOrdinary supervised learning with source\nthen validated on target test data.\n24\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-12-06",
  "updated": "2025-02-17"
}