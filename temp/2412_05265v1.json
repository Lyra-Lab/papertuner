{
  "id": "http://arxiv.org/abs/2412.05265v1",
  "title": "Reinforcement Learning: An Overview",
  "authors": [
    "Kevin Murphy"
  ],
  "abstract": "This manuscript gives a big-picture, up-to-date overview of the field of\n(deep) reinforcement learning and sequential decision making, covering\nvalue-based RL, policy-gradient methods, model-based methods, and various other\ntopics (including a very brief discussion of RL+LLMs).",
  "text": "Reinforcement Learning: An Overview1\nKevin P. Murphy\nDecember 9, 2024\n1Parts of this monograph are borrowed from chapters 34 and 35 of my textbook [Mur23]. However, I have added a\nlot of new material, so this text supercedes those chapters. Thanks to Lihong Li, who wrote Section 5.4 and parts of\nSection 1.4, and Pablo Samuel Castro, who proof-read a draft of this manuscript.\narXiv:2412.05265v1  [cs.AI]  6 Dec 2024\n2\nContents\n1\nIntroduction\n9\n1.1\nSequential decision making\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.1.1\nProblem definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.1.2\nUniversal model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.1.3\nEpisodic vs continuing tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n1.1.4\nRegret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n1.1.5\nFurther reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1.2\nCanonical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1.2.1\nPartially observed MDPs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1.2.2\nMarkov decision process (MDPs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1.2.3\nContextual MDPs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n1.2.4\nContextual bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n1.2.5\nBelief state MDPs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n1.2.6\nOptimization problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n1.2.6.1\nBest-arm identification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n1.2.6.2\nBayesian optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n1.2.6.3\nActive learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n1.2.6.4\nStochastic Gradient Descent (SGD) . . . . . . . . . . . . . . . . . . . . . . .\n17\n1.3\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n1.3.1\nValue-based RL (Approximate Dynamic Programming)\n. . . . . . . . . . . . . . . . .\n18\n1.3.2\nPolicy-based RL\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.3.3\nModel-based RL\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1.3.4\nDealing with partial observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n1.3.4.1\nOptimal solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n1.3.4.2\nFinite observation history . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n1.3.4.3\nStateful (recurrent) policies . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n1.3.5\nSoftware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n1.4\nExploration-exploitation tradeoff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n1.4.1\nSimple heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n1.4.2\nMethods based on the belief state MDP . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n1.4.2.1\nBandit case (Gittins indices) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n1.4.2.2\nMDP case (Bayes Adaptive MDPs)\n. . . . . . . . . . . . . . . . . . . . . . .\n22\n1.4.3\nUpper confidence bounds (UCBs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n1.4.3.1\nBasic idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n1.4.3.2\nBandit case: Frequentist approach . . . . . . . . . . . . . . . . . . . . . . . .\n23\n1.4.3.3\nBandit case: Bayesian approach\n. . . . . . . . . . . . . . . . . . . . . . . . .\n23\n1.4.3.4\nMDP case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n1.4.4\nThompson sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n1.4.4.1\nBandit case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n1.4.4.2\nMDP case (posterior sampling RL)\n. . . . . . . . . . . . . . . . . . . . . . .\n25\n3\n1.5\nRL as a posterior inference problem\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n1.5.1\nModeling assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n1.5.2\nSoft value functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n1.5.3\nMaximum entropy RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n1.5.4\nActive inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n2\nValue-based RL\n31\n2.1\nBasic concepts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n2.1.1\nValue functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n2.1.2\nBellman’s equations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n2.1.3\nExample: 1d grid world . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n2.2\nComputing the value function and policy given a known world model . . . . . . . . . . . . . .\n33\n2.2.1\nValue iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n2.2.2\nReal-time dynamic programming (RTDP) . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n2.2.3\nPolicy iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n2.3\nComputing the value function without knowing the world model\n. . . . . . . . . . . . . . . .\n35\n2.3.1\nMonte Carlo estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n2.3.2\nTemporal difference (TD) learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n2.3.3\nCombining TD and MC learning using TD(λ) . . . . . . . . . . . . . . . . . . . . . . .\n36\n2.3.4\nEligibility traces\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n2.4\nSARSA: on-policy TD control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n2.5\nQ-learning: off-policy TD control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n2.5.1\nTabular Q learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n2.5.2\nQ learning with function approximation . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n2.5.2.1\nNeural fitted Q . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n2.5.2.2\nDQN\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n2.5.2.3\nExperience replay\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n2.5.2.4\nThe deadly triad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n2.5.2.5\nTarget networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n2.5.2.6\nTwo time-scale methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n2.5.2.7\nLayer norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n2.5.3\nMaximization bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n2.5.3.1\nDouble Q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n2.5.3.2\nDouble DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n2.5.3.3\nRandomized ensemble DQN\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n2.5.4\nDQN extensions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n2.5.4.1\nQ learning for continuous actions . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n2.5.4.2\nDueling DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n2.5.4.3\nNoisy nets and exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n2.5.4.4\nMulti-step DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n2.5.4.5\nRainbow\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n2.5.4.6\nBigger, Better, Faster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n2.5.4.7\nOther methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n3\nPolicy-based RL\n49\n3.1\nThe policy gradient theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n3.2\nREINFORCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n3.3\nActor-critic methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n3.3.1\nAdvantage actor critic (A2C) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n3.3.2\nGeneralized advantage estimation (GAE)\n. . . . . . . . . . . . . . . . . . . . . . . . .\n53\n3.3.3\nTwo-time scale actor critic algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n3.3.4\nNatural policy gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n4\n3.3.4.1\nNatural gradient descent\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n3.3.4.2\nNatural actor critic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.4\nPolicy improvement methods\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.4.1\nPolicy improvement lower bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.4.2\nTrust region policy optimization (TRPO) . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n3.4.3\nProximal Policy Optimization (PPO) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n3.4.4\nVMPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n3.5\nOff-policy methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n3.5.1\nPolicy evaluation using importance sampling\n. . . . . . . . . . . . . . . . . . . . . . .\n59\n3.5.2\nOff-policy actor critic methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n3.5.2.1\nLearning the critic using V-trace . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n3.5.2.2\nLearning the actor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.5.2.3\nIMPALA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.5.3\nOff-policy policy improvement methods\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.5.3.1\nOff-policy PPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.5.3.2\nOff-policy VMPO\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.5.3.3\nOff-policy TRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.5.4\nSoft actor-critic (SAC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3.5.4.1\nPolicy evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3.5.4.2\nPolicy improvement: Gaussian policy\n. . . . . . . . . . . . . . . . . . . . . .\n64\n3.5.4.3\nPolicy improvement: softmax policy . . . . . . . . . . . . . . . . . . . . . . .\n64\n3.5.4.4\nAdjusting the temperature\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n3.6\nDeterministic policy gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n3.6.1\nDDPG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n3.6.2\nTwin Delayed DDPG (TD3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n4\nModel-based RL\n69\n4.1\nDecision-time planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n4.1.1\nModel predictive control (MPC)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n4.1.2\nHeuristic search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n4.1.3\nMonte Carlo tree search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n4.1.3.1\nAlphaGo and AlphaZero\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n4.1.3.2\nMuZero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n4.1.3.3\nEfficientZero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.1.4\nTrajectory optimization for continuous actions\n. . . . . . . . . . . . . . . . . . . . . .\n73\n4.1.4.1\nRandom shooting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.1.4.2\nLQG\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.1.4.3\nCEM\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.1.4.4\nMPPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n4.1.4.5\nGP-MPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n4.1.5\nSMC for MPC\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n4.2\nBackground planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n4.2.1\nA game-theoretic perspective on MBRL . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n4.2.2\nDyna\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n4.2.2.1\nTabular Dyna\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n4.2.2.2\nDyna with function approximation . . . . . . . . . . . . . . . . . . . . . . . .\n78\n4.2.3\nDealing with model errors and uncertainty . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n4.2.3.1\nAvoiding compounding errors in rollouts . . . . . . . . . . . . . . . . . . . . .\n79\n4.2.3.2\nEnd-to-end differentiable learning of model and planner . . . . . . . . . . . .\n80\n4.2.3.3\nUnified model and planning variational lower bound . . . . . . . . . . . . . .\n80\n4.2.3.4\nDynamically switching between MFRL and MBRL . . . . . . . . . . . . . . .\n80\n4.3\nWorld models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n5\n4.3.1\nGenerative world models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n4.3.1.1\nObservation-space world models . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n4.3.1.2\nFactored models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n4.3.1.3\nLatent-space world models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n4.3.1.4\nDreamer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n4.3.1.5\nIris\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n4.3.2\nNon-generative world models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n4.3.2.1\nValue prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n4.3.2.2\nSelf prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n4.3.2.3\nPolicy prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n4.3.2.4\nObservation prediction\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n4.3.2.5\nPartial observation prediction\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n4.3.2.6\nBYOL-Explore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n4.4\nBeyond one-step models: predictive representations . . . . . . . . . . . . . . . . . . . . . . . .\n88\n4.4.1\nGeneral value functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n4.4.2\nSuccessor representations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n4.4.3\nSuccessor models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n4.4.3.1\nLearning SMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n4.4.3.2\nJumpy models using geometric policy composition . . . . . . . . . . . . . . .\n92\n4.4.4\nSuccessor features\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n4.4.4.1\nGeneralized policy improvement . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n4.4.4.2\nOption keyboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n4.4.4.3\nLearning SFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n4.4.4.4\nChoosing the tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n5\nOther topics in RL\n97\n5.1\nDistributional RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n5.1.1\nQuantile regression methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n5.1.2\nReplacing regression with classification . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n5.2\nReward functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n5.2.1\nReward hacking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n5.2.2\nSparse reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n5.2.3\nReward shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n5.2.4\nIntrinsic reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n5.2.4.1\nKnowledge-based intrinsic motivation . . . . . . . . . . . . . . . . . . . . . .\n99\n5.2.4.2\nGoal-based intrinsic motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n5.3\nHierarchical RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n5.3.1\nFeudal (goal-conditioned) HRL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n5.3.1.1\nHindsight Experience Relabeling (HER) . . . . . . . . . . . . . . . . . . . . . 101\n5.3.1.2\nHierarchical HER\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.3.1.3\nLearning the subgoal space . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n5.3.2\nOptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n5.3.2.1\nDefinitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n5.3.2.2\nLearning options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n5.4\nImitation learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n5.4.1\nImitation learning by behavior cloning . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n5.4.2\nImitation learning by inverse reinforcement learning\n. . . . . . . . . . . . . . . . . . . 104\n5.4.3\nImitation learning by divergence minimization\n. . . . . . . . . . . . . . . . . . . . . . 105\n5.5\nOffline RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n5.5.1\nOffline model-free RL\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n5.5.1.1\nPolicy constraint methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n5.5.1.2\nBehavior-constrained policy gradient methods\n. . . . . . . . . . . . . . . . . 107\n6\n5.5.1.3\nUncertainty penalties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n5.5.1.4\nConservative Q-learning and pessimistic value functions . . . . . . . . . . . . 107\n5.5.2\nOffline model-based RL\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n5.5.3\nOffline RL using reward-conditioned sequence modeling . . . . . . . . . . . . . . . . . 108\n5.5.4\nHybrid offline/online methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n5.6\nLLMs and RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n5.6.1\nRL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n5.6.1.1\nRLHF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n5.6.1.2\nAssistance game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n5.6.1.3\nRun-time inference as MPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n5.6.2\nLLMs for RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n5.6.2.1\nLLMs for pre-processing the input . . . . . . . . . . . . . . . . . . . . . . . . 111\n5.6.2.2\nLLMs for rewards\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n5.6.2.3\nLLMs for world models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n5.6.2.4\nLLMs for policies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n5.7\nGeneral RL, AIXI and universal AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n7\n8\nChapter 1\nIntroduction\n1.1\nSequential decision making\nReinforcement learning or RL is a class of methods for solving various kinds of sequential decision making\ntasks. In such tasks, we want to design an agent that interacts with an external environment. The agent\nmaintains an internal state st, which it passes to its policy π to choose an action at = π(st). The environment\nresponds by sending back an observation ot+1, which the agent uses to update its internal state using the\nstate-update function st+1 = U(st, at, ot+1). See Figure 1.1 for an illustration.\n1.1.1\nProblem definition\nThe goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\nVπ(s0) = Ep(a0,s1,a1,...,aT ,sT |s0,π)\n\" T\nX\nt=0\nR(st, at)|s0\n#\n(1.1)\nwhere s0 is the agent’s initial state, R(st, at) is the reward function that the agent uses to measure the\nvalue of performing an action in a given state, Vπ(s0) is the value function for policy π evaluated at s0, and\nthe expectation is wrt\np(a0, s1, a1, . . . , aT , sT |s0, π) = π(a0|s0)penv(o1|a0)δ(s1 = U(s0, a0, o1))\n(1.2)\n× π(a1|s1)penv(o2|a1, o1)δ(s2 = U(s1, a1, o2))\n(1.3)\n× π(a2|s2)penv(o3|a1:2, o1:2)δ(s3 = U(s2, a2, o3)) . . .\n(1.4)\nwhere penv is the environment’s distribution over observations (which is usually unknown). We define the\noptimal policy as\nπ∗= arg max\nπ\nEp0(s0) [Vπ(s0)]\n(1.5)\nNote that picking a policy to maximize the sum of expected rewards is an instance of the maximum\nexpected utility principle. There are various ways to design or learn an optimal policy, depending on the\nassumptions we make about the environment, and the form of the agent. We will discuss some of these\noptions below.\n1.1.2\nUniversal model\nA generic representation for sequential decision making problems (which is an extended version of the\n“universal modeling framework” proposed in [Pow22]) is shown in Figure 1.2. Here we have assumed the\n9\nFigure 1.1: A small agent interacting with a big external world.\nFigure 1.2: Diagram illustrating the interaction of the agent and environment. The agent has internal state st, and\nchooses action at based on its policy πt. It then predicts its next internal states, st+1|t, via the predict function P,\nand optionally predicts the resulting observation, ˆot+1, via the observation decoder D. The environment has (hidden)\ninternal state zt, which gets updated by the world model W to give the new state zt+1 = W(zt, at) in response to the\nagent’s action. The environment also emits an observation ot+1 via the observation model O. This gets encoded to et+1\nby the agent’s observation encoder E, which the agent uses to update its internal state using st+1 = U(st, at, et+1).\nThe policy is parameterized by θt, and these parameters may be updated (at a slower time scale) by the RL policy πRL.\nSquare nodes are functions, circles are variables (either random or deterministic). Dashed square nodes are stochastic\nfunctions that take an extra source of randomness (not shown).\n10\nenvironment can be modeled by a controlled Markov process1 with hidden state zt, which gets updated\nat each step in response to the agent’s action at. To allow for non-deterministic dynamics, we write this as\nzt+1 = W(zt, at, ϵz\nt ), where W is the environment’s state transition function (which is usually not known\nto the agent) and ϵz\nt is random system noise.2, The agent does not see the world state zt, but instead\nsees a potentially noisy and/or partial observation ot+1 = O(zt+1, ϵo\nt+1) at each step, where ϵo\nt+1 is random\nobservation noise. For example, when navigating a maze, the agent may only see what is in front of it, rather\nthan seeing everything in the world all at once; furthermore, even the current view may be corrupted by\nsensor noise. Any given image, such as one containing a door, could correspond to many different locations in\nthe world (this is called perceptual aliasing), each of which may require a different action. Thus the agent\nneeds use these observations to incrementally update its own internal belief state about the world, using\nthe state update function st+1 = SU(st, at, ot+1); this represents the agent’s beliefs about the underlying\nworld state zt, as well as the unknown world model W itself (or some proxy thereof). In the simplest setting,\nthe internal st can just store all the past observations, ht = (o1:t, a1:t−1), but such non-parametric models\ncan take a lot of time and space to work with, so we will usually consider parametric approximations. The\nagent can then pass its state to its policy to pick actions, using at+1 = πt(st+1).\nWe can further elaborate the behavior of the agent by breaking the state-update function into two\nparts. First the agent predicts its own next state, st+1|t = P(st, at), using a prediction function P,\nand then it updates this prediction given the observation using update function U, to give st+1 =\nU(st+1|t, ot+1). Thus the SU function is defined as the composition of the predict and update functions:\nst+1 = SU(st, at, ot+1) = U(P(st, at), ot+1). If the observations are high dimensional (e.g., images), the\nagent may choose to encode its observations into a low-dimensional embedding et+1 using an encoder,\net+1 = E(ot+1); this can encourage the agent to focus on the relevant parts of the sensory signal. (The state\nupdate then becomes st+1 = U(st+1|t, et+1).) Optionally the agent can also learn to invert this encoder by\ntraining a decoder to predict the next observation using ˆot+1 = D(st+1|t); this can be a useful training signal,\nas we will discuss in Chapter 4. Finally, the agent needs to learn the action policy πt. We parameterize this\nby θt, so πt(st) = π(st; θt). These parameters themselves may need to be learned; we use the notation πRL\nto denote the RL policy which specifies how to update the policy parameters at each step. See Figure 1.2 for\nan illustration.\nWe see that, in general, there are three interacting stochastic processes we need to deal with: the\nenvironment’s states zt (which are usually affected by the agents actions); the agent’s internal states st (which\nreflect its beliefs about the environment based on the observed data); and the the agent’s policy parameters\nθt (which are updated based on the information stored in the belief state). The reason there are so many\nRL algorithms is that this framework is very general. In the rest of this manuscript we will study special\ncases, where we make different assumptions about the environment’s state zt and dynamics, the agent’s\nstate st and dynamics, the form of the action policy π(st|θt), and the form of the policy learning method\nθt+1 = πRL(θt, st, at, ot+1).\n1.1.3\nEpisodic vs continuing tasks\nIf the agent can potentially interact with the environment forever, we call it a continuing task. Alternatively,\nthe agent is in an episodic task, if its interaction terminates once the system enters a terminal state or\nabsorbing state, which is a state which transitions to itself with 0 reward. After entering a terminal state,\nwe may start a new epsiode from a new initial world state z0 ∼p0. (The agent will typically also reinitialize\nits own internal state s0.) The episode length is in general random. For example, the amount of time a robot\ntakes to reach its goal may be quite variable, depending on the decisions it makes, and the randomness in the\nenvironment. Finally, if the trajectory length T in an episodic task is fixed and known, it is called a finite\nhorizon problem.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards,\n1The Markovian assumption is without loss of generality, since we can always condition on the entire past sequence of states\nby suitably expanding the Markovian state space.\n2Representing a stochastic function as a deterministic function with some noisy inputs is known as a functional causal model,\nor structural equation model. This is standard practice in the control theory and causality communities.\n11\nwhere each reward is multiplied by a discount factor γ ∈[0, 1]:\nGt ≜rt + γrt+1 + γ2rt+2 + · · · + γT −t−1rT −1\n(1.6)\n=\nT −t−1\nX\nk=0\nγkrt+k =\nT −1\nX\nj=t\nγj−trj\n(1.7)\nwhere rt = R(st, at) is the reward, and Gt is the reward-to-go. For episodic tasks that terminate at time T,\nwe define Gt = 0 for t ≥T. Clearly, the return satisfies the following recursive relationship:\nGt = rt + γ(rt+1 + γrt+2 + · · · ) = rt + γGt+1\n(1.8)\nFurthermore, we define the value function to be the expected reward-to-go:\nVπ(st) = E [Gt|π]\n(1.9)\nThe discount factor γ plays two roles. First, it ensures the return is finite even if T = ∞(i.e., infinite\nhorizon), provided we use γ < 1 and the rewards rt are bounded. Second, it puts more weight on short-term\nrewards, which generally has the effect of encouraging the agent to achieve its goals more quickly. (For\nexample, if γ = 0.99, then an agent that reaches a terminal reward of 1.0 in 15 steps will receive an expected\ndiscounted reward of 0.9915 = 0.86, whereas if it takes 17 steps it will only get 0.9917 = 0.84.) However, if γ is\ntoo small, the agent will become too greedy. In the extreme case where γ = 0, the agent is completely myopic,\nand only tries to maximize its immediate reward. In general, the discount factor reflects the assumption that\nthere is a probability of 1 −γ that the interaction will end at the next step. For finite horizon problems,\nwhere T is known, we can set γ = 1, since we know the life time of the agent a priori.3\n1.1.4\nRegret\nSo far we have been discussing maximizing the reward. However, the upper bound on this is usually unknown,\nso it can be hard to know how well a given agent is doing. An alternative approach is to work in terms of\nthe regret, which is defined as the difference between the expected reward under the agent’s policy and the\noracle policy π∗, which knows the true MDP. Specifically, let πt be the agent’s policy at time t. Then the\nper-step regret at t is defined as\nlt ≜Es1:t\n\u0002\nR(st, π∗(st)) −Eπ(at|st) [R(st, at)]\n\u0003\n(1.10)\nHere the expectation is with respect to randomness in choosing actions using the policy π, as well as earlier\nstates, actions and rewards, as well as other potential sources of randomness.\nIf we only care about the final performance of the agent, as in most optimization problems, it is enough\nto look at the simple regret at the last step, namely lT . Optimizing simple regret results in a problem\nknown as pure exploration [BMS11], where the agent needs to interact with the environment to learn\nthe underlying MDP; at the end, it can then solve for the resulting policy using planning methods (see\nSection 2.2). However, in RL, it is more common to focus on the cumulative regret, also called the total\nregret or just the regret, which is defined as\nLT ≜E\n\" T\nX\nt=1\nlt\n#\n(1.11)\nThus the agent will accumulate reward (and regret) while it learns a model and policy. This is called earning\nwhile learning, and requires performing exploratory actions, to learn the model (and hence optimize\nlong-term reward), while also performing actions that maximize the reward at each step. This requires solving\nthe exploration-exploitation tradeoff, as we discussed in Section 1.4.\n3We may also use γ = 1 for continuing tasks, targeting the (undiscounted) average reward criterion [Put94].\n12\n1.1.5\nFurther reading\nIn later chapters, we will describe methods for learning the best policy to maximize Vπ(s0) = E [G0|s0, π]).\nMore details on RL can be found in textbooks such as [Sze10; SB18; Aga+22a; Pla22; ID19; RJ22; Li23;\nMMT24], and reviews such as [Aru+17; FL+18; Li18; Wen18a]. For details on how RL relates to control\ntheory, see e.g., [Son98; Rec19; Ber19; Mey22], and for connections to operations research, see [Pow22].\n1.2\nCanonical examples\nIn this section, we describe different forms of model for the environment and the agent that have been studied\nin the literature.\n1.2.1\nPartially observed MDPs\nThe model shown in Figure 1.2 is called a partially observable Markov decision process or POMDP\n(pronounced “pom-dee-pee”) [KLC98]. Typically the environment’s dynamics model is represented by a\nstochastic transition function, rather than a deterministic function with noise as an input. We can derive this\ntransition function as follows:\np(zt+1|zt, at) = Eϵz\nt [I (zt+1 = W(zt, at, ϵz\nt ))]\n(1.12)\nSimilarly the stochastic observation function is given by\np(ot+1|zt+1) = Eϵo\nt+1\n\u0002\nI\n\u0000ot+1 = O(zt+1, ϵo\nt+1)\n\u0001\u0003\n(1.13)\nNote that we can combine these two distributions to derive the joint world model pW O(zt+1, ot+1|zt, at).\nAlso, we can use these distributions to derive the environment’s non-Markovian observation distribution,\npenv(ot+1|o1:t, a1:t), used in Equation (1.4), as follows:\npenv(ot+1|o1:t, a1:t) =\nX\nzt+1\np(ot+1|zt+1)p(zt+1|a1:t)\n(1.14)\np(zt+1|a1:t) =\nX\nz1\n· · ·\nX\nzt\np(z1|a1)p(z2|z1, a1) . . . p(zt+1|zt, at)\n(1.15)\nIf the world model (both p(o|z) and p(z′|z, a)) is known, then we can — in principle — solve for the optimal\npolicy. The method requires that the agent’s internal state correspond to the belief state st = bt = p(zt|ht),\nwhere ht = (o1:t, a1:t−1) is the observation history. The belief state can be updated recursively using Bayes rule.\nSee Section 1.2.5 for details. The belief state forms a sufficient statistic for the optimal policy. Unfortunately,\ncomputing the belief state and the resulting optimal policy is wildly intractable [PT87; KLC98]. We discuss\nsome approximate methods in Section 1.3.4.\n1.2.2\nMarkov decision process (MDPs)\nA Markov decision process [Put94] is a special case of a POMDP in which the environment states are\nobserved, so zt = ot = st.4 We usually define an MDP in terms of the state transition matrix induced by the\nworld model:\npS(st+1|st, at) = Eϵs\nt [I (st+1 = W(st, at, ϵs\nt))]\n(1.16)\n4The field of control theory uses slightly different terminology and notation. In particular, the environment is called the\nplant, and the agent is called the controller. States are denoted by xt ∈X ⊆RD, actions are denoted by ut ∈U ⊆RK, and\nrewards are replaced by costs ct ∈R.\n13\nFigure 1.3: Illustration of an MDP as a finite state machine (FSM). The MDP has three discrete states (green\ncirlces), two discrete actions (orange circles), and two non-zero rewards (orange arrows). The numbers on the\nblack edges represent state transition probabilities, e.g., p(s′ = s0|a = a0, s′ = s0) = 0.7; most state transitions\nare impossible (probability 0), so the graph is sparse. The numbers on the yellow wiggly edges represent expected\nrewards, e.g., R(s = s1, a = a0, s′ = s0) = +5; state transitions with zero reward are not annotated.\nFrom\nhttps: // en. wikipedia. org/ wiki/ Markov_ decision_ process . Used with kind permission of Wikipedia author\nwaldoalvarez.\nIn lieu of an observation model, we assume the environment (as opposed to the agent) sends out a reward\nsignal, sampled from pR(rt|st, at, st+1). The expected reward is then given by\nR(st, at, st+1) =\nX\nr\nr pR(r|st, at, st+1)\n(1.17)\nR(st, at) =\nX\nst+1\npS(st+1|st, at)R(st, at, st+1)\n(1.18)\nGiven a stochastic policy π(at|st), the agent can interact with the environment over many steps. Each\nstep is called a transition, and consists of the tuple (st, at, rt, st+1), where at ∼π(·|st), st+1 ∼pS(st, at),\nand rt ∼pR(st, at, st+1). Hence, under policy π, the probability of generating a trajectory length T,\nτ = (s0, a0, r0, s1, a1, r1, s2, . . . , sT ), can be written explicitly as\np(τ) = p0(s0)\nT −1\nY\nt=0\nπ(at|st)pS(st+1|st, at)pR(rt|st, at, st+1)\n(1.19)\nIn general, the state and action sets of an MDP can be discrete or continuous. When both sets are finite,\nwe can represent these functions as lookup tables; this is known as a tabular representation. In this case,\nwe can represent the MDP as a finite state machine, which is a graph where nodes correspond to states,\nand edges correspond to actions and the resulting rewards and next states. Figure 1.3 gives a simple example\nof an MDP with 3 states and 2 actions.\nIf we know the world model pS and pR, and if the state and action space is tabular, then we can solve for\nthe optimal policy using dynamic programming techniques, as we discuss in Section 2.2. However, typically\nthe world model is unknown, and the states and actions may need complex nonlinear models to represent\ntheir transitions. In such cases, we will have to use RL methods to learn a good policy.\n1.2.3\nContextual MDPs\nA Contextual MDP [HDCM15] is an MDP where the dynamics and rewards of the environment depend\non a hidden static parameter referred to as the context. (This is different to a contextual bandit, discussed\nin Section 1.2.4, where the context is observed at each step.) A simple example of a contextual MDP is a\nvideo game, where each level of the game is procedurally generated, that is, it is randomly generated\neach time the agent starts a new episode. Thus the agent must solve a sequence of related MDPs, which are\n14\ndrawn from a common distribution. This requires the agent to generalize across multiple MDPs, rather than\noverfitting to a specific environment [Cob+19; Kir+21; Tom+22]. (This form of generalization is different\nfrom generalization within an MDP, which requires generalizing across states, rather than across environments;\nboth are important.)\nA contextual MDP is a special kind of POMDP where the hidden variable corresponds to the unknown\nparameters of the model. In [Gho+21], they call this an epistemic POMDP, which is closely related to the\nconcept of belief state MDP which we discuss in Section 1.2.5.\n1.2.4\nContextual bandits\nA contextual bandit is a special case of a POMDP where the world state transition function is independent\nof the action of the agent and the previous state, i.e., p(zt|zt−1, at) = p(zt). In this case, we call the world\nstates “contexts”; these are observable by the agent, i.e., ot = zt. Since the world state distribution is\nindependent of the agents actions, the agent has no effect on the external environment. However, its actions\ndo affect the rewards that it receives. Thus the agent’s internal belief state — about the underlying reward\nfunction R(o, a) — does change over time, as the agent learns a model of the world (see Section 1.2.5).\nA special case of a contextual bandit is a regular bandit, in which there is no context, or equivalently, st is\nsome fixed constant that never changes. When there are a finite number of possible actions, A = {a1, . . . , aK},\nthis is called a multi-armed bandit.5 In this case the reward model has the form R(a) = f(wa), where wa\nare the parameters for arm a.\nContextual bandits have many applications. For example, consider an online advertising system. In\nthis case, the state st represents features of the web page that the user is currently looking at, and the action\nat represents the identity of the ad which the system chooses to show. Since the relevance of the ad depends\non the page, the reward function has the form R(st, at), and hence the problem is contextual. The goal is to\nmaximize the expected reward, which is equivalent to the expected number of times people click on ads; this\nis known as the click through rate or CTR. (See e.g., [Gra+10; Li+10; McM+13; Aga+14; Du+21; YZ22]\nfor more information about this application.) Another application of contextual bandits arises in clinical\ntrials [VBW15]. In this case, the state st are features of the current patient we are treating, and the action\nat is the treatment the doctor chooses to give them (e.g., a new drug or a placebo).\nFor more details on bandits, see e.g., [LS19; Sli19].\n1.2.5\nBelief state MDPs\nIn this section, we describe a kind of MDP where the state represents a probability distribution, known as a\nbelief state or information state, which is updated by the agent (“in its head”) as it receives information\nfrom the environment.6 More precisely, consider a contextual bandit problem, where the agent approximates\nthe unknown reward by a function R(o, a) = f(o, a; w). Let us denote the posterior over the unknown\nparameters by bt = p(w|ht), where ht = {o1:t, a1:t, r1:t} is the history of past observations, actions and\nrewards. This belief state can be updated deterministically using Bayes’ rule; we denote this operation by\nbt+1 = BayesRule(bt, ot+1, at+1, rt+1). (This corresponds to the state update SU defined earlier.) Using this,\nwe can define the following belief state MDP, with deterministic dynamics given by\np(bt+1|bt, ot+1, at+1, rt+1) = I (bt+1 = BayesRule(bt, ot+1, at+1, rt+1))\n(1.20)\nand reward function given by\np(rt|ot, at, bt) =\nZ\npR(rt|ot, at; w)p(w|bt)dw\n(1.21)\n5The terminology arises by analogy to a slot machine (sometimes called a “bandit”) in a casino. If there are K slot machines,\neach with different rewards (payout rates), then the agent (player) must explore the different machines until they have discovered\nwhich one is best, and can then stick to exploiting it.\n6Technically speaking, this is a POMDP, where we assume the states are observed, and the parameters are the unknown\nhidden random variables. This is in contrast to Section 1.2.1, where the states were not observed, and the parameters were\nassumed to be known.\n15\n,\n,\nAction 2\nSuccess\nSuccess\nFailure\nFailure\n,\n,\n,\nAction 1\nSuccess\nFailure\n,\n,\nAction 1\nFigure 1.4: Illustration of sequential belief updating for a two-armed beta-Bernoulli bandit. The prior for the reward\nfor action 1 is the (blue) uniform distribution Beta(1, 1); the prior for the reward for action 2 is the (orange) unimodal\ndistribution Beta(2, 2). We update the parameters of the belief state based on the chosen action, and based on whether\nthe observed reward is success (1) or failure (0).\nIf we can solve this (PO)MDP, we have the optimal solution to the exploration-exploitation problem.\nAs a simple example, consider a context-free Bernoulli bandit, where pR(r|a) = Ber(r|µa), and\nµa = pR(r = 1|a) = R(a) is the expected reward for taking action a. The only unknown parameters are\nw = µ1:A. Suppose we use a factored beta prior\np0(w) =\nY\na\nBeta(µa|αa\n0, βa\n0)\n(1.22)\nwhere w = (µ1, . . . , µK). We can compute the posterior in closed form to get\np(w|Dt) =\nY\na\nBeta(µa| αa\n0 + N 0\nt (a)\n|\n{z\n}\nαa\nt\n, βa\n0 + N 1\nt (a)\n|\n{z\n}\nβa\nt\n)\n(1.23)\nwhere\nN r\nt (a) =\nt−1\nX\ni=1\nI (ai = a, ri = r)\n(1.24)\nThis is illustrated in Figure 1.4 for a two-armed Bernoulli bandit. We can use a similar method for a\nGaussian bandit, where pR(r|a) = N(r|µa, σ2\na).\nIn the case of contextual bandits, the problem is conceptually the same, but becomes more complicated\ncomputationally. If we assume a linear regression bandit, pR(r|s, a; w) = N(r|ϕ(s, a)Tw, σ2), we can use\nBayesian linear regression to compute p(w|Dt) exactly in closed form. If we assume a logistic regression\nbandit, pR(r|s, a; w) = Ber(r|σ(ϕ(s, a)Tw)), we have to use approximate methods for approximate Bayesian\nlogistic regression to compute p(w|Dt). If we have a neural bandit of the form pR(r|s, a; w) = N(r|f(s, a; w))\nfor some nonlinear function f, then posterior inference is even more challenging (this is equivalent to the\nproblem of inference in Bayesian neural networks, see e.g., [Arb+23] for a review paper for the offline case,\nand [DMKM22; JCM24] for some recent online methods).\nWe can generalize the above methods to compute the belief state for the parameters of an MDP in the\nobvious way, but modeling both the reward function and state transition function.\nOnce we have computed the belief state, we can derive a policy with optimal regret using the methods\nlike UCB (Section 1.4.3) or Thompson sampling (Section 1.4.4).\n1.2.6\nOptimization problems\nThe bandit problem is an example of a problem where the agent must interact with the world in order to\ncollect information, but it does not otherwise affect the environment. Thus the agents internal belief state\n16\nchanges over time, but the environment state does not.7 Such problems commomly arise when we are trying\nto optimize a fixed but unknown function R. We can “query” the function by evaluating it at different points\n(parameter values), and in some cases, the resulting observation may also include gradient information. The\nagent’s goal is to find the optimum of the function in as few steps as possible. We give some examples of this\nproblem setting below.\n1.2.6.1\nBest-arm identification\nIn the standard multi-armed bandit problem our goal is to maximize the sum of expected rewards. However,\nin some cases, the goal is to determine the best arm given a fixed budget of T trials; this variant is known as\nbest-arm identification [ABM10]. Formally, this corresponds to optimizing the final reward criterion:\nVπ,πT = Ep(a1:T ,r1:T |s0,π) [R(ˆa)]\n(1.25)\nwhere ˆa = πT (a1:T , r1:T ) is the estimated optimal arm as computed by the terminal policy πT applied to\nthe sequence of observations obtained by the exploration policy π. This can be solved by a simple adaptation\nof the methods used for standard bandits.\n1.2.6.2\nBayesian optimization\nBayesian optimization is a gradient-free approach to optimizing expensive blackbox functions. That is, we\nwant to find\nw∗= argmax\nw\nR(w)\n(1.26)\nfor some unknown function R, where w ∈RN, using as few actions (function evaluations of R) as possible.\nThis is essentially an “infinite arm” version of the best-arm identification problem [Tou14], where we replace\nthe discrete choice of arms a ∈{1, . . . , K} with the parameter vector w ∈RN. In this case, the optimal\npolicy can be computed if the agent’s state st is a belief state over the unknown function, i.e., st = p(R|ht).\nA common way to represent this distribution is to use Gaussian processes. We can then use heuristics like\nexpected improvement, knowledge gradient or Thompson sampling to implement the corresponding policy,\nwt = π(st). For details, see e.g., [Gar23].\n1.2.6.3\nActive learning\nActive learning is similar to BayesOpt, but instead of trying to find the point at which the function is largest\n(i.e., w∗), we are trying to learn the whole function R, again by querying it at different points wt. Once\nagain, the optimal strategy again requires maintaining a belief state over the unknown function, but now the\nbest policy takes a different form, such as choosing query points to reduce the entropy of the belief state. See\ne.g., [Smi+23].\n1.2.6.4\nStochastic Gradient Descent (SGD)\nFinally we discuss how to interpret SGD as a sequential decision making process, following [Pow22]. The action\nspace consists of querying the unknown function R at locations at = wt, and observing the function value\nrt = R(wt); however, unlike BayesOpt, now we also observe the corresponding gradient gt = ∇wR(w)|wt,\nwhich gives non-local information about the function. The environment state contains the true function R\nwhich is used to generate the observations given the agent’s actions. The agent state contains the current\nparameter estimate wt, and may contain other information such as first and second moments mt and vt,\nneeded by methods such as Adam. The update rule (for vanilla SGD) takes the form wt+1 = wt + αtgt,\nwhere the stepsize αt is chosen by the policy, αt = π(st). The terminal policy has the form π(sT ) = wT .\n7In the contextual bandit problem, the environment state (context) does change, but not in response to the agent’s actions.\nThus p(ot) is usually assumed to be a static distribution.\n17\nApproach\nMethod\nFunctions learned\nOn/Off\nSection\nValue-based\nSARSA\nQ(s, a)\nOn\nSection 2.4\nValue-based\nQ-learning\nQ(s, a)\nOff\nSection 2.5\nPolicy-based\nREINFORCE\nπ(a|s)\nOn\nSection 3.2\nPolicy-based\nA2C\nπ(a|s), V (s)\nOn\nSection 3.3.1\nPolicy-based\nTRPO/PPO\nπ(a|s), A(s, a)\nOn\nSection 3.4.3\nPolicy-based\nDDPG\na = π(s), Q(s, a)\nOff\nSection 3.6.1\nPolicy-based\nSoft actor-critic\nπ(a|s), Q(s, a)\nOff\nSection 3.5.4\nModel-based\nMBRL\np(s′|s, a)\nOff\nChapter 4\nTable 1.1: Summary of some popular methods for RL. On/off refers to on-policy vs off-policy methods.\nAlthough in principle it is possible to learn the learning rate (stepsize) policy using RL (see e.g., [Xu+17]),\nthe policy is usually chosen by hand, either using a learning rate schedule or some kind of manually\ndesigned adaptive learning rate policy (e.g., based on second order curvature information).\n1.3\nReinforcement Learning\nIn this section, we give a brief overview of how to compute optimal policies when the model of the environment\nis unknown; this is the core problem tackled by RL. We mostly focus on the MDP case, but discuss the\nPOMDP case in Section 1.3.4.\nWe may categorize RL methods along two main dimensions: (1) by what the agent represents and learns:\nthe value function, and/or the policy, and/or the model; (2) and by how actions are selected: on-policy\n(actions must be selected by the agent’s current policy), and off-policy (actions can be select by any kind of\npolicy, including human demonstrations). Table 1.1 lists a few representative examples. More details are\ngiven in the subsequent sections.\n1.3.1\nValue-based RL (Approximate Dynamic Programming)\nIn this section, we give a brief introduction to value-based RL, also called Approximate Dynamic\nProgramming or ADP; see Chapter 2 for more details.\nWe introduced the value function Vπ(s) in Equation (1.1), which we repeat here for convenience:\nVπ(s) ≜Eπ [G0|s0 = s] = Eπ\n\" ∞\nX\nt=0\nγtrt|s0 = s\n#\n(1.27)\nThe value function for the optimal policy π∗is known to satisfy the following recursive condition, known as\nBellman’s equation:\nV∗(s) = max\na\nR(s, a) + γEpS(s′|s,a) [V∗(s′)]\n(1.28)\nThis follows from the principle of dynamic programming, which computes the optimal solution to a\nproblem (here the value of state s by combining the optimal solution of various subproblems (here the values\nof the next states s′). This can be used to derive the following learning rule:\nV (s) ←V (s) + η[r + γV (s′) −V (s)]\n(1.29)\nwhere s′ ∼pS(·|s, a) is the next state sampled from the environment, and r = R(s, a) is the observed reward.\nThis is called Temporal Difference or TD learning (see Section 2.3.2 for details). Unfortunately, it is not\nclear how to derive a policy if all we know is the value function. We now describe a solution to this problem.\n18\nWe first generalize the notion of value function to assigning a value to a state and action pair, by defining\nthe Q function as follows:\nQπ(s, a) ≜Eπ [G0|s0 = s, a0 = a] = Eπ\n\" ∞\nX\nt=0\nγtrt|s0 = s, a0 = a\n#\n(1.30)\nThis quantity represents the expected return obtained if we start by taking action a in state s, and then\nfollow π to choose actions thereafter. The Q function for the optimal policy satisfies a modified Bellman\nequation\nQ∗(s, a) = R(s, a) + γEpS(s′|s,a)\nh\nmax\na′ Q∗(s′, a′)\ni\n(1.31)\nThis gives rise to the following TD update rule:\nQ(s, a) ←r + γ max\na′ Q(s′, a′) −Q(s, a)\n(1.32)\nwhere we sample s′ ∼pS(·|s, a) from the environment. The action is chosen at each step from the implicit\npolicy\na = argmax\na′\nQ(s, a′)\n(1.33)\nThis is called Q learning (see Section 2.5 for details),\n1.3.2\nPolicy-based RL\nIn this section we give a brief introductin to Policy-based RL; for details see Chapter 3.\nIn policy-based methods, we try to directly maximize J(πθ) = Ep(s0) [Vπ(s0)] wrt the parameter’s θ; this\nis called policy search. If J(πθ) is differentiable wrt θ, we can use stochastic gradient ascent to optimize θ,\nwhich is known as policy gradient (see Section 3.1).\nPolicy gradient methods have the advantage that they provably converge to a local optimum for many\ncommon policy classes, whereas Q-learning may diverge when approximation is used (Section 2.5.2.4). In\naddition, policy gradient methods can easily be applied to continuous action spaces, since they do not need\nto compute argmaxa Q(s, a). Unfortunately, the score function estimator for ∇θJ(πθ) can have a very high\nvariance, so the resulting method can converge slowly.\nOne way to reduce the variance is to learn an approximate value function, Vw(s), and to use it as a\nbaseline in the score function estimator. We can learn Vw(s) using using TD learning. Alternatively, we can\nlearn an advantage function, Aw(s, a), and use it as a baseline. These policy gradient variants are called actor\ncritic methods, where the actor refers to the policy πθ and the critic refers to Vw or Aw. See Section 3.3 for\ndetails.\n1.3.3\nModel-based RL\nIn this section, we give a brief introduction to model-based RL; for more details, see Chapter 4.\nValue-based methods, such as Q-learning, and policy search methods, such as policy gradient, can be very\nsample inefficient, which means they may need to interact with the environment many times before finding\na good policy, which can be problematic when real-world interactions are expensive. In model-based RL, we\nfirst learn the MDP, including the pS(s′|s, a) and R(s, a) functions, and then compute the policy, either using\napproximate dynamic programming on the learned model, or doing lookahead search. In practice, we often\ninterleave the model learning and planning phases, so we can use the partially learned policy to decide what\ndata to collect, to help learn a better model.\n19\n1.3.4\nDealing with partial observability\nIn an MDP, we assume that the state of the environment st is the same as the observation ot obtained by the\nagent. But in many problems, the observation only gives partial information about the underlying state of the\nworld (e.g., a rodent or robot navigating in a maze). This is called partial observability. In this case, using\na policy of the form at = π(ot) is suboptimal, since ot does not give us complete state information. Instead\nwe need to use a policy of the form at = π(ht), where ht = (a1, o1, . . . , at−1, ot) is the entire past history of\nobservations and actions, plus the current observation. Since depending on the entire past is not tractable for\na long-lived agent, various approximate solution methods have been developed, as we summarize below.\n1.3.4.1\nOptimal solution\nIf we know the true latent structure of the world (i.e., both p(o|z) and p(z′|z, a), to use the notation of\nSection 1.1.2), then we can use solution methods designed for POMDPs, discussed in Section 1.2.1. This\nrequires using Bayesian inference to compute a belief state, bt = p(zt|ht) (see Section 1.2.5), and then using\nthis belief state to guide our decisions.\nHowever, learning the parameters of a POMDP (i.e., the generative\nlatent world model) is very difficult, as is recursively computing and updating the belief state, as is computing\nthe policy given the belief state. Indeed, optimally solving POMDPs is known to be computationally very\ndifficult for any method [PT87; KLC98]. So in practice simpler approximations are used. We discuss some of\nthese below. (For more details, see [Mur00].)\nNote that it is possible to marginalize out the POMDP latent state zt, to derive a prediction over the\nnext observable state, p(ot+1|ht, at). This can then become a learning target for a model, that is trained to\ndirectly predict future observations, without explicitly invoking the concept of latent state. This is called a\npredictive state representation or PSR [LS01]. This is related to the idea of observable operator\nmodels [Jae00], and to the concept of successor representations which we discuss in Section 4.4.2.\n1.3.4.2\nFinite observation history\nThe simplest solution to the partial observability problem is to define the state to be a finite history of the\nlast k observations, st = ht−k:t; when the observations ot are images, this is often called frame stacking.\nWe can then use standard MDP methods. Unfortunately, this cannot capture long-range dependencies in the\ndata.\n1.3.4.3\nStateful (recurrent) policies\nA more powerful approach is to use a stateful policy, that can remember the entire past, and not just respond\nto the current input or last k frames. For example, we can represent the policy by an RNN (recurrent neural\nnetwork), as proposed in the R2D2 paper [Kap+18], and used in many other papers. Now the hidden state\nzt of the RNN will implicitly summarize the past observations, ht, and can be used in lieu of the state st in\nany standard RL algorithm.\nRNNs policies are widely used, and this method is often effective in solving partially observed problems.\nHowever, they typically will not plan to perform information-gathering actions, since there is no explicit\nnotion of belief state or uncertainty. However, such behavior can arise via meta-learning [Mik+20].\n1.3.5\nSoftware\nImplementing RL algorithms is much trickier than methods for supervised learning, or generative methods\nsuch as language modeling and diffusion, all of which have stable (easy-to-optimize) loss functions. Therefore\nit is often wise to build on existing software rather than starting from scratch. We list some useful libraries\nin Section 1.3.5.\nIn addition, RL experiments can be very high variance, making it hard to draw valid conclusions. See\n[Aga+21b; Pat+24; Jor+24] for some recommended experimental practices. For example, when reporting\nperformance across different environments, with different intrinsic difficulties (e.g., different kinds of Atari\n20\nURL\nLanguage\nComments\nStoix\nJax\nMini-library with many methods (including MBRL)\nPureJaxRL\nJax\nSingle files with DQN; PPO, DPO\nJaxRL\nJax\nSingle files with AWAC, DDPG, SAC, SAC+REDQ\nStable Baselines Jax\nJax\nLibrary with DQN, CrossQ, TQC; PPO, DDPG, TD3, SAC\nJax Baselines\nJax\nLibrary with many methods\nRejax\nJax\nLibrary with DDQN, PPO, (discrete) SAC, DDPG\nDopamine\nJax/TF\nLibrary with many methods\nRlax\nJax\nLibrary of RL utility functions (used by Acme)\nAcme\nJax/TF\nLibrary with many methods (uses rlax)\nCleanRL\nPyTorch\nSingle files with many methods\nStable Baselines 3\nPyTorch\nLibrary with DQN; A2C, PPO, DDPG, TD3, SAC, HER\nTianShou\nPyTorch\nLibrary with many methods (including offline RL)\nTable 1.2: Some open source RL software.\ngames), [Aga+21b] recommend reporting the interquartile mean (IQM) of the performance metric, which\nis the mean of the samples between the 0.25 and 0.75 percentiles, (this is a special case of a trimmed mean).\nLet this estimate be denoted by ˆµ(Di), where D is the empirical data (e.g., reward vs time) from the i’th\nrun. We can estimate the uncertainty in this estimate using a nonparametric method, such as bootstrap\nresampling, or a parametric approximation, such as a Gaussian approximation. (This requires computing the\nstandard error of the mean,\nˆσ\n√n, where n is the number of trials, and ˆσ is the estimated standard deviation of\nthe (trimmed) data.)\n1.4\nExploration-exploitation tradeoff\nA fundamental problem in RL with unknown transition and reward models is to decide between choosing\nactions that the agent knows will yield high reward, or choosing actions whose reward is uncertain, but which\nmay yield information that helps the agent get to parts of state-action space with even higher reward. This is\ncalled the exploration-exploitation tradeoff. In this section, we discuss various solutions.\n1.4.1\nSimple heuristics\nWe start with a policy based on pure exploitation. This is known as the greedy policy, at = argmaxa Q(s, a).\nWe can add exploration to this by sometimes picking some other, non-greedy action.\nOne approach is to use an ϵ-greedy policy πϵ, parameterized by ϵ ∈[0, 1]. In this case, we pick the\ngreedy action wrt the current model, at = argmaxa ˆRt(st, a) with probability 1 −ϵ, and a random action\nwith probability ϵ. This rule ensures the agent’s continual exploration of all state-action combinations.\nUnfortunately, this heuristic can be shown to be suboptimal, since it explores every action with at least a\nconstant probability ϵ/|A|, although this can be solved by annealing ϵ to 0 over time.\nAnother problem with ϵ-greedy is that it can result in “dithering”, in which the agent continually changes\nits mind about what to do. In [DOB21] they propose a simple solution to this problem, known as ϵz-greedy,\nthat often works well. The idea is that with probability 1 −ϵ the agent exploits, but with with probability ϵ\nthe agent explores by repeating the sampled action for n ∼z() steps in a row, where z(n) is a distribution\nover the repeat duration. This can help the agent escape from local minima.\nAnother approach is to use Boltzmann exploration, which assigns higher probabilities to explore more\npromising actions, taking itno account the reward function. That is, we use a polocy of the form\nπτ(a|s) =\nexp( ˆRt(st, a)/τ)\nP\na′ exp( ˆRt(st, a′)/τ)\n(1.34)\nwhere τ > 0 is a temperature parameter that controls how entropic the distribution is. As τ gets close to 0,\nπτ becomes close to a greedy policy. On the other hand, higher values of τ will make π(a|s) more uniform,\n21\nˆR(s, a1)\nˆR(s, a2)\nπϵ(a|s1)\nπϵ(a|s2)\nπτ(a|s1)\nπτ(a|s2)\n1.00\n9.00\n0.05\n0.95\n0.00\n1.00\n4.00\n6.00\n0.05\n0.95\n0.12\n0.88\n4.90\n5.10\n0.05\n0.95\n0.45\n0.55\n5.05\n4.95\n0.95\n0.05\n0.53\n0.48\n7.00\n3.00\n0.95\n0.05\n0.98\n0.02\n8.00\n2.00\n0.95\n0.05\n1.00\n0.00\nTable 1.3: Comparison of ϵ-greedy policy (with ϵ = 0.1) and Boltzmann policy (with τ = 1) for a simple MDP with 6\nstates and 2 actions. Adapted from Table 4.1 of [GK19].\nand encourage more exploration. Its action selection probabilities can be much “smoother” with respect to\nchanges in the reward estimates than ϵ-greedy, as illustrated in Table 1.3.\nThe Boltzmann policy explores equally widely in all states. An alternative approach is to try to explore\n(state,action) combinations where the consequences of the outcome might be uncertain. This can be achived\nusing an exploration bonus Rb\nt(s, a), which is large if the number of times we have tried actioon a in state\ns is small. We can then add Rb to the regular reward, to bias the behavior in a way that will hopefully\ncause the agent to learn useful information about the world. This is called an intrinsic reward function\n(Section 5.2.4).\n1.4.2\nMethods based on the belief state MDP\nWe can compute an optimal solution to the exploration-exploitation tradeoff by adopting a Bayesian approach\nto the problem. We start by computing the belief state MDP, as discussed in Section 1.2.5. We then compute\nthe optimal policy, as we explain below.\n1.4.2.1\nBandit case (Gittins indices)\nSuppose we have a way to compute the recursively compute the belief state over model parameters, p(θt|D1:t).\nHow do we use this to solve for the policy in the resulting belief state MDP?\nIn the special case of context-free bandits with a finite number of arms, the optimal policy of this belief\nstate MDP can be computed using dynamic programming. The result can be represented as a table of action\nprobabilities, πt(a1, . . . , aK), for each step; this are known as Gittins indices [Git89] (see [PR12; Pow22] for\na detailed explanation). However, computing the optimal policy for general contextual bandits is intractable\n[PT87].\n1.4.2.2\nMDP case (Bayes Adaptive MDPs)\nWe can extend the above techniques to the MDP case by constructing a BAMDP, which stands for “Bayes-\nAdaptive MDP” [Duf02]. However, this is computationally intractable to solve, so various approximations are\nmade (see e.g., [Zin+21; AS22; Mik+20]).\n1.4.3\nUpper confidence bounds (UCBs)\nThe optimal solution to explore-exploit is intractable. However, an intuitively sensible approach is based\non the principle known as “optimism in the face of uncertainty” (OFU). The principle selects actions\ngreedily, but based on optimistic estimates of their rewards. The optimality of this approach is proved in the\nR-Max paper of [Ten02], which builds on the earlier E3 paper of [KS02].\nThe most common implementation of this principle is based on the notion of an upper confidence\nbound or UCB. We will initially explain this for the bandit case, then extend to the MDP case.\n22\n1.4.3.1\nBasic idea\nTo use a UCB strategy, the agent maintains an optimistic reward function estimate ˜Rt, so that ˜Rt(st, a) ≥\nR(st, a) for all a with high probability, and then chooses the greedy action accordingly:\nat = argmax\na\n˜Rt(st, a)\n(1.35)\nUCB can be viewed a form of exploration bonus, where the optimistic estimate encourages exploration.\nTypically, the amount of optimism, ˜Rt−R, decreases over time so that the agent gradually reduces exploration.\nWith properly constructed optimistic reward estimates, the UCB strategy has been shown to achieve near-\noptimal regret in many variants of bandits [LS19]. (We discuss regret in Section 1.1.4.)\nThe optimistic function ˜R can be obtained in different ways, sometimes in closed forms, as we discuss\nbelow.\n1.4.3.2\nBandit case: Frequentist approach\nA frequentist approach to computing a confidence bound can be based on a concentration inequal-\nity [BLM16] to derive a high-probability upper bound of the estimation error: | ˆRt(s, a) −Rt(s, a)| ≤δt(s, a),\nwhere ˆRt is a usual estimate of R (often the MLE), and δt is a properly selected function. An optimistic\nreward is then obtained by setting ˜Rt(s, a) = ˆRt(s, a) + δt(s, a).\nAs an example, consider again the context-free Bernoulli bandit, R(a) ∼Ber(µ(a)). The MLE ˆRt(a) =\nˆµt(a) is given by the empirical average of observed rewards whenever action a was taken:\nˆµt(a) = N 1\nt (a)\nNt(a) =\nN 1\nt (a)\nN 0\nt (a) + N 1\nt (a)\n(1.36)\nwhere N r\nt (a) is the number of times (up to step t −1) that action a has been tried and the observed reward\nwas r, and Nt(a) is the total number of times action a has been tried:\nNt(a) =\nt−1\nX\ns=1\nI (at = a)\n(1.37)\nThen the Chernoff-Hoeffding inequality [BLM16] leads to δt(a) = c/\np\nNt(a) for some constant c, so\n˜Rt(a) = ˆµt(a) +\nc\np\nNt(a)\n(1.38)\n1.4.3.3\nBandit case: Bayesian approach\nWe can also derive an upper confidence about using Bayesian inference. If we use a beta prior, we can compute\nthe posterior in closed form, as shown in Equation (1.23). The posterior mean is ˆµt(a) = E [µ(a)|ht] =\nαa\nt\nαa\nt +βa\nt ,\nand the posterior standard deviation is approximately\nˆσt(a) =\np\nV [µ(a)|ht] ≈\ns\nˆµt(a)(1 −ˆµt(a))\nNt(a)\n(1.39)\nWe can use similar techniques for a Gaussian bandit, where pR(R|a, θ) = N(R|µa, σ2\na), µa is the expected\nreward, and σ2\na the variance. If we use a conjugate prior, we can compute p(µa, σa|Dt) in closed form.\nUsing an uninformative version of the conjugate prior, we find E [µa|ht] = ˆµt(a), which is just the empirical\nmean of rewards for action a. The uncertainty in this estimate is the standard error of the mean, i.e.,\np\nV [µa|ht] = ˆσt(a)/\np\nNt(a), where ˆσt(a) is the empirical standard deviation of the rewards for action a.\nOnce we have computed the mean and posterior standard deviation, we define the optimistic reward\nestimate as\n˜Rt(a) = ˆµt(a) + cˆσt(a)\n(1.40)\nfor some constant c that controls how greedy the policy is. See Figure 1.5 for an illustration. We see that\nthis is similar to the frequentist method based on concentration inequalities, but is more general.\n23\nFigure 1.5: Illustration of the reward distribution Q(a) for a Gaussian bandit with 3 different actions, and the\ncorresponding lower and upper confidence bounds. We show the posterior means Q(a) = µ(a) with a vertical dotted line,\nand the scaled posterior standard deviations cσ(a) as a horizontal solid line. From [Sil18]. Used with kind permission\nof David Silver.\n1.4.3.4\nMDP case\nThe UCB idea (especially in its frequentist form) has been extended to the MDP case in several works. (The\nBayesian version is discussed in Section 1.4.4.) For example, [ACBF02] proposes to combine UCB with Q\nlearning, by defining the policy as\nπ(a|s) = I\n\u0012\na = argmax\na′\nQ(s, a′) + c\np\nlog(t)/Nt(s, a′)\n\u0013\n(1.41)\n[AJO08] presents the more sophisticated UCRL2 algorithm, which computes confidence intervals on all the\nMDP model parameters at the start of each episode; it then computes the resulting optimistic MDP and\nsolves for the optimal policy, which it uses to collect more data.\n1.4.4\nThompson sampling\nA common alternative to UCB is to use Thompson sampling [Tho33], also called probability matching\n[Sco10]. We start by describing this in the bandit case, then extend to the MDP case. For more details, see\n[Rus+18]. (See also [Ger18] for some evidence that humans use Thompson-sampling like mechanisms.)\n1.4.4.1\nBandit case\nIn Thompson sampling, we define the policy at step t to be πt(a|st, ht) = pa, where pa is the probability that\na is the optimal action. This can be computed using\npa = Pr(a = a∗|st, ht) =\nZ\nI\n\u0012\na = argmax\na′\nR(st, a′; θ)\n\u0013\np(θ|ht)dθ\n(1.42)\nIf the posterior is uncertain, the agent will sample many different actions, automatically resulting in exploration.\nAs the uncertainty decreases, it will start to exploit its knowledge.\nTo see how we can implement this method, note that we can compute the expression in Equation (1.42)\nby using a single Monte Carlo sample ˜θt ∼p(θ|ht). We then plug in this parameter into our reward model,\nand greedily pick the best action:\nat = argmax\na′\nR(st, a′; ˜θt)\n(1.43)\nThis sample-then-exploit approach will choose actions with exactly the desired probability, since\npa =\nZ\nI\n\u0012\na = argmax\na′\nR(st, a′; ˜θt)\n\u0013\np(˜θt|ht) =\nPr\n˜θt∼p(θ|ht)\n(a = argmax\na′\nR(st, a′; ˜θt))\n(1.44)\n24\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n−20\n−15\n−10\n−5\n0\n5\n10\narm0\narm1\narm2\n(a)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\ntime\n0\n200\n400\n600\n800\n1000\n1200\n1400\ncumulative reward\narm0\narm1\narm2\n(b)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\ntime\n0\n5\n10\n15\n20\n25\n30\n35\n40\nLT\nCumulative regret\nobserved\nc √t\n(c)\nFigure 1.6: Illustration of Thompson sampling applied to a linear-Gaussian contextual bandit. The context has the\nform st = (1, t, t2). (a) True reward for each arm vs time. (b) Cumulative reward per arm vs time. (c) Cumulative\nregret vs time. Generated by thompson_sampling_linear_gaussian.ipynb.\nDespite its simplicity, this approach can be shown to achieve optimal regret (see e.g., [Rus+18] for a\nsurvey). In addition, it is very easy to implement, and hence is widely used in practice [Gra+10; Sco10;\nCL11].\nIn Figure 1.6, we give a simple example of Thompson sampling applied to a linear regression bandit. The\ncontext has the form st = (1, t, t2). The true reward function has the form R(st, a) = wT\nast. The weights per\narm are chosen as follows: w0 = (−5, 2, 0.5), w1 = (0, 0, 0), w2 = (5, −1.5, −1). Thus we see that arm 0 is\ninitially worse (large negative bias) but gets better over time (positive slope), arm 1 is useless, and arm 2 is\ninitially better (large positive bias) but gets worse over time. The observation noise is the same for all arms,\nσ2 = 1. See Figure 1.6(a) for a plot of the reward function. We use a conjugate Gaussian-gamma prior and\nperform exact Bayesian updating. Thompson sampling quickly discovers that arm 1 is useless. Initially it\npulls arm 2 more, but it adapts to the non-stationary nature of the problem and switches over to arm 0, as\nshown in Figure 1.6(b). In Figure 1.6(c), we show that the empirical cumulative regret in blue is close to the\noptimal lower bound in red.\n1.4.4.2\nMDP case (posterior sampling RL)\nWe can generalize Thompson sampling to the (episodic) MDP case by maintaining a posterior over all the\nmodel parameters (reward function and transition model), sampling an MDP from this belief state at the start\nof each episode, solving for the optimal policy corresponding to the sampled MDP, using the resulting policy\nto collect new data, and then updating the belief state at the end of the episode. This is called posterior\nsampling RL [Str00; ORVR13; RR14; OVR17; WCM24].\nAs a more computationally efficient alternative, it is also possible to maintain a posterior over policies\nor Q functions instead of over world models; see e.g., [Osb+23a] for an implementation of this idea based\non epistemic neural networks [Osb+23b]. Another approach is to use successor features (Section 4.4.4),\nwhere the Q function is assumed to have the form Qπ(s, a) = ψπ(s, a)Tw. In particular, [Jan+19b] proposes\nSucessor Uncertainties, in which they model the uncertainty over w as a Gaussian, p(w) = N(µw, Σw).\nFrom this they can derive the posterior distribution over Q values as\np(Q(s, a)) = N(Ψπµw, ΨπΣw(Ψπ)T)\n(1.45)\nwhere Ψπ = [ψπ(s, a)]T is a matrix of features, one per state-action pair.\n1.5\nRL as a posterior inference problem\nIn this section, we discuss an approach to policy optimization that reduces it to probabilistic inference. This\nis called control as inference or RL as inference, and has been discussed in numerous works (see e.g.,\n[Att03; TS06; Tou09; ZABD10; RTV12; BT12; KGO12; HR17; Lev18; Wat+21]). The resulting framework\n25\nFigure 1.7: A graphical model for optimal control.\nalso forms the foundation of the SAC method discussed in Section 3.5.4, the MPO discussed in Section 3.4.4,\nand the MPC method discussed in Section 4.1.5.\n1.5.1\nModeling assumptions\nFigure 1.7 gives a probabilistic model, which not only captures state transitions as in a standard MDP, but\nalso introduces a new variable, Ot. This variable is binary, indicating whether the action at time t is optimal\nor not, and has the following probability distribution:\np(Ot = 1|st, at) = exp(R(st, at))\n(1.46)\nIn the above, we have assumed that R(s, a) < 0, so that Equation (1.46) gives a valid probability. However,\nthis is not required, since we can simply replace the likelihood term p(Ot = 1|st, at) with an unnormalized\npotential, ϕt(st, at); this will not affect the results of inference. For brevity, we will just write p(Ot) rather\nthan p(Ot = 1), since 1 is just a dummy value.\nTo simplify notation, we assume a uniform action prior, p(at|st) = 1/|A|; this is without loss of generality,\nsince we can always push an informative action prior p(at|st) into the potential function ϕt(st, at). (We call\nthis an “action prior” rather than a policy, since we are going to derive the policy using posterior inference, as\nwe explain below.) Under these assumptions, the posterior probability of observing a length-T trajectory τ,\nwhen optimality achieved in every step, is\np(τ|O1:T ) ∝p(τ, O1:T ) ∝\n\"\np(s1)\nT −1\nY\nt=1\npS(st+1|st, at)\n# \" T\nY\nt=1\np(Ot|st, at)\n#\n= p(s1)\nT −1\nY\nt=1\npS(st+1|st, at) exp\n T\nX\nt=1\nR(st, at)\n!\n(1.47)\n(Typically p(s1) is a delta function at the observed initial state s1.) The intuition of Equation (1.47) is\nclearest when the state transitions are deterministic. In this case, pS(st+1|st, at) is either 1 or 0, depending\non whether the transition is dynamically feasible or not. Hence we have\np(τ|O1:T ) ∝I (p(τ) ̸= 0) exp(\nT\nX\nt=1\nR(st, at))\n(1.48)\nwhere the first term determines if τ is feasible or not. In this case, find the action sequence that maximizes\nthe sum of rewards is equivalent to inferring the MAP sequence of actions, which we denote by ˆa1:T (s1).\n(The case of stochastic transitions is more complicated, and will be discussed later.)\n26\nFor deterministic environments, the optimal policy is open loop, and corresponds to following the optimal\naction sequence ˆa1:T (s1). (This is like a shortest path planning problem.) However, in the stochastic case,\nwe need to compute a closed loop policy, π(at|st), that conditions on the observed state. To compute this,\nlet us define the following quantities:\nβt(st, at) ≜p(Ot:T |st, at)\n(1.49)\nβt(st) ≜p(Ot:T |st)\n(1.50)\n(These terms are analogous to the backwards messages in the forwards-backwards algorithm for HMMs\n[Rab89].) Using this notation, we can write the optimal policy using\np(at|st, Ot:T ) = p(st, at|Ot:T )\np(st|Ot:T )\n= p(Ot:T |st, at)p(at|st)p(st)\np(Ot:T |st)p(st)\n∝βt(st, at)\nβt(s)\n(1.51)\nWe can compute the backwards messages as follows:\nβt(st, at) =\nZ\nS\nβt+1(st+1)pS(st+1|st, at)p(Ot|st, at)dst+1\n(1.52)\nβs(st) =\nZ\nA\nβt(st, at)p(at|st)dat ∝\nZ\nA\nβt(st, at)dat\n(1.53)\nwhere we have assumed the action prior p(at|st) = 1/|A| for notational simplicty. (Recall that the action\nprior is distinct from the optimal policy, which is given by p(at|st, Ot:T ).)\n1.5.2\nSoft value functions\nWe can gain more insight into what is going on by working in log space. Let us define\nQ(st, at) = log βt(st, at)\n(1.54)\nV (st) = log βt(st)\n(1.55)\nThe update for V becomes\nV (st) = log\nX\nat\nexp(Q(st, at))\n(1.56)\nThis is a standard log-sum-exp computation, and is similar to the softmax operation. Thus we call it a soft\nvalue function. When the values of Q(st, at) are large (which can be ensure by scaling up all the rewards),\nthis approximates the standard hard max operation:\nV (st) = log\nX\nat\nexp(Q(st, at)) ≈max\nat Q(st, at)\n(1.57)\nFor the deterministic case, the backup for Q becomes the usual\nQ(st, at) = log p(Ot|st, at) + log βt+1(st+1) = r(st, at) + V (st+1)\n(1.58)\nwhere st+1 = f(st, at) is the next state. However, for the stochastic case, we get\nQ(st, at) = r(st, at) + log EpS(st+1|st,at) [exp(V (st+1))]\n(1.59)\nThis replaces the standard expectation over the next state with a softmax. This can result in Q functions that\nare optimistic, since if there is one next state with particularly high reward (e.g., you win the lottery), it will\ndominate the backup, even if on average it is unlikely. This can result in risk seeking behavior, and is known\nas the optimism bias (see e.g., [Mad+17; Cha+21] for discussion). We will discuss a solution to this below.\n27\n1.5.3\nMaximum entropy RL\nRecall that the true posterior is given by\np(τ|O1:T ) ≜p∗(τ) ∝p(s1)\nT −1\nY\nt=1\npS(st+1|st, at) exp\n T\nX\nt=1\nR(st, at)\n!\n(1.60)\nIn the sections above, we derived the exact posterior over states and actions conditioned on the optimality\nvariables. However, in general we will have to approximate it.\nLet us denote the approximate posterior by q(τ). Variational inference corresponds to the minimizing\n(wrt q) the following objective:\nDKL (q(τ) ∥p∗(τ)) = −Eq(τ) [log p∗(τ) −log q(τ)]\n(1.61)\nWe can drive this loss to its minimum value of 0 by performing exact inference, which sets q(τ) = p∗(τ),\nwhich is given by\np∗(τ) = p(s1|O1:T )\nT −1\nY\nt=1\npS(st+1|st, at, O1:T )p(at|st, O1:T ))\n(1.62)\nUnfortunately, this uses an optimistic form of the dynamics, pS(st+1|st, at, O1:T ), in which the agent plans\nassuming it directly controls the state distribution itself, rather than just the action distribution. We can\nsolve this optimism bias problem by instead using a “causal” variational posterior of the following form:8\nq(τ) = p(s1)\nT −1\nY\nt=1\npS(st+1|st, at)p(at|st, O1:T ) = p(s1)\nT −1\nY\nt=1\npS(st+1|st, at)π(at|st)\n(1.63)\nwhere π(at|st) is the policy we wish to learn. In the case of deterministic transitions, where pS(st+1|st, at) =\nδ(st+1 −f(st, at)), we do not need this simplification, since pS(st+1|st, at, O1:T ) = pS(st+1|st, at). (And in\nboth cases p(s1|O1:T ) = p(s1), which is assumed to be a delta function.) We can now write the (negative of)\nthe objective as follows:\n−DKL (q(τ) ∥p∗(τ)) = Eq(τ)\n\"\nlog p(s1) +\nT\nX\nt=1\n(log pS(st+1|st, at) + R(st, at)) −\n(1.64)\n−log p(s1) −\nT\nX\nt=1\n(log pS(st+1|st, at) + log π(at|st))\n#\n(1.65)\n= Eq(τ)\n\" T\nX\nt=1\nR(st, at) −log π(at|st)\n#\n(1.66)\n=\nT\nX\nt=1\nEq(st,at)[R(st, at)] + Eq(st) H(π(·|st))\n(1.67)\nThis is known as the maximum entropy RL objective [ZABD10].We can optimize this using the soft actor\ncritic algorithm which we discuss in Section 3.5.4.\nNote that we can tune the magnitude of the entropy regularizer by defining the optimality variable using\np(Ot = 1|st, at) = exp( 1\nαR(st, at)). This gives the objective\nJ(π) =\nT\nX\nt=1\nEq(st,at)[R(st, at)] + αEq(st) H(π(·|st))\n(1.68)\nAs α →0 (equivalent to scaling up the rewards), this approaches the standard (unregularized) RL objective.\n8Unfortunately, this trick is specific to variational inference, which means that other posterior inference methods, such as\nsequential Monte Carlo [Pic+19; Lio+22], will still suffer from the optimism bias in the stochastic case (see e.g., [Mad+17] for\ndiscussion).\n28\n1.5.4\nActive inference\nControl as inference is closely related to a technique known as active inference, as we explain below. For\nmore details on the connection, see [Mil+20; WIP20; LÖW21; Saj+21; Tsc+20].\nThe active inference technique was developed in the neuroscience community, that has its own vocabulary\nfor standard ML concepts. We start with the free energy principle [Fri09; Buc+17; SKM18; Ger19;\nMaz+22]. The FEP is equivalent to using variational inference to perform state estimation (perception) and\nparameter estimation (learning) in a latent variable model. In particular, consider an LVM p(z, o|θ) with\nhidden states z, observations o, and parameters θ. We define the variational free energy to be\nF(o|θ) = DKL (q(z|o, θ) ∥p(z|o, θ)) −log p(o|θ) = Eq(z|o,θ) [log q(z|o, θ) −log p(o, z|θ)] ≥−log p(o|θ)\n(1.69)\nwhich is the KL between the approximate variational posterior q and the true posterior p, minus a normalization\nconstant, log p(o|θ), which is known as the free energy. State estimation (perception) corresponds to solving\nminq(z|o,θ) F(o|θ), and parameter estimation (model fitting) corresponds to solving minθ F(o|θ), just as in\nthe EM (expectation maximization) algorithm. (We can also be Bayesian about θ, as in variational Bayes\nEM, instead of just computing a point estimate.) This EM procedure will minimize the VFE, which is an\nupper bound on the negative log marginal likelihood of the data. In other words, it adjusts the model (belief\nstate and parameters) so that it better predicts the observations, so the agent is less surprised (minimizes\nprediction errors).\nTo extend the above FEP to decision making problems, we define the expected free energy as follows\nG(a) = Eq(o|a) [F(o)] = Eq(o,z|a) [log q(z|o) −log p(o, z)]\n(1.70)\nwhere q(o|a) is the posterior predictive distribution over future observations given action sequence a. (We\ncan also condition on any observed history or agent state h, but we omit this (and the model parameters θ)\nfrom the notation for brevity.) We can decompose the EFE (which the agent wants to minimize) into two\nterms. First there is the intrinsic value, known as the epistemic drive:\nGepistemic(a) = Eq(o,z|a) [log q(z|o) −log q(z)]\n(1.71)\nMinimizing this will encourage the agent to choose actions which maximize the mutual information between\nthe observations o and the hidden states z, thus reducing uncertainty about the hidden states. (This is called\nepistemic foraging.) The extrinsic value, known as the exploitation term, is given by\nGextrinsic(a) = −Eq(o|a) [log p(o)]\n(1.72)\nMinimizing this will encourage the agent to choose actions that result in observations that match its prior.\nFor example, if the agent predicts that the world will look brighter when it flips a light switch, it can take\nthe action of flipping the switch to fulfill this prediction. This prior can be related to a reward function by\ndefining as p(o) ∝eR(o), encouraging goal directed behavior, exactly as in control-as-inference. However, the\nactive inference approach provides a way of choosing actions without needing to specify a reward. Since\nsolving to the optimal action at each step can be slow, it is possible to amortize this cost by training a\npolicy network to compute π(a|h) = argmina G(a|h), where h is the observation history (or current state),\nas shown in [Mil20; HL20]; this is called “deep active inference”.\nOverall, we see that this framework provides a unified theory of both perception and action, both of which\ntry to minimize some form of free energy. In particular, minimizing the expected free energy will cause the\nagent to pick actions to reduce its uncertainty about its hidden states, which can then be used to improve\nits predictive model pθ of observations; this in turn will help minimize the VFE of future observations, by\nupdating the internal belief state q(z|o, θ) to explain the observations. In other words, the agent acts so it\ncan learn so it becomes less surprised by what it sees. This ensures the agent is in homeostasis with its\nenvironment.\nNote that active inference is often discussed in the context of predictive coding. This is equivalent to a\nspecial case of FEP where two assumptions are made: (1) the generative model p(z, o|θ) is a a nonlinear\n29\nhierarchical Gaussian model (similar to a VAE decoder), and (2) the variational posterior approximation uses\na diagonal Laplace approximation, q(z|o, θ) = N(z|ˆz, H) with the mode ˆz being computed using gradient\ndescent, and H being the Hessian at the mode. This can be considered a non-amortized version of a VAE,\nwhere inference (E step) is done with iterated gradient descent, and parameter estimation (M step) is also\ndone with gradient descent. (A more efficient incremental EM version of predictive coding, which updates\n{ˆzn : n = 1 : N} and θ in parallel, was recently presented in [Sal+24], and an amortized version in [Tsc+23].)\nFor more details on predictive coding, see [RB99; Fri03; Spr17; HM20; MSB21; Mar21; OK22; Sal+23;\nSal+24].\n30\nChapter 2\nValue-based RL\n2.1\nBasic concepts\nIn this section we introduce some definitions and basic concepts.\n2.1.1\nValue functions\nLet π be a given policy. We define the state-value function, or value function for short, as follows (with\nEπ [·] indicating that actions are selected by π):\nVπ(s) ≜Eπ [G0|s0 = s] = Eπ\n\" ∞\nX\nt=0\nγtrt|s0 = s\n#\n(2.1)\nThis is the expected return obtained if we start in state s and follow π to choose actions in a continuing task\n(i.e., T = ∞).\nSimilarly, we define the state-action value function, also known as the Q-function, as follows:\nQπ(s, a) ≜Eπ [G0|s0 = s, a0 = a] = Eπ\n\" ∞\nX\nt=0\nγtrt|s0 = s, a0 = a\n#\n(2.2)\nThis quantity represents the expected return obtained if we start by taking action a in state s, and then\nfollow π to choose actions thereafter.\nFinally, we define the advantage function as follows:\nAπ(s, a) ≜Qπ(s, a) −Vπ(s)\n(2.3)\nThis tells us the benefit of picking action a in state s then switching to policy π, relative to the baseline\nreturn of always following π. Note that Aπ(s, a) can be both positive and negative, and Eπ(a|s) [Aπ(s, a)] = 0\ndue to a useful equality: Vπ(s) = Eπ(a|s) [Qπ(s, a)].\n2.1.2\nBellman’s equations\nSuppose π∗is a policy such that Vπ∗≥Vπ for all s ∈S and all policy π, then it is an optimal policy. There\ncan be multiple optimal policies for the same MDP, but by definition their value functions must be the same,\nand are denoted by V∗and Q∗, respectively. We call V∗the optimal state-value function, and Q∗the\noptimal action-value function. Furthermore, any finite MDP must have at least one deterministic optimal\npolicy [Put94].\n31\nA fundamental result about the optimal value function is Bellman’s optimality equations:\nV∗(s) = max\na\nR(s, a) + γEpS(s′|s,a) [V∗(s′)]\n(2.4)\nQ∗(s, a) = R(s, a) + γEpS(s′|s,a)\nh\nmax\na′ Q∗(s′, a′)\ni\n(2.5)\nConversely, the optimal value functions are the only solutions that satisfy the equations. In other words,\nalthough the value function is defined as the expectation of a sum of infinitely many rewards, it can be\ncharacterized by a recursive equation that involves only one-step transition and reward models of the MDP.\nSuch a recursion play a central role in many RL algorithms we will see later.\nGiven a value function (V or Q), the discrepancy between the right- and left-hand sides of Equations (2.4)\nand (2.5) are called Bellman error or Bellman residual. We can define the Bellman operator B given\nan MDP M = (R, T) and policy π as a function that takes a value function V and derives a few value function\nV ′ that satisfies\nV ′(s) = Bπ\nMV (s) ≜Eπ(a|s)\n\u0002\nR(s, a) + γET (s′|s,a) [V (s′)]\n\u0003\n(2.6)\nThis reduces the Bellman error. Applying the Bellman operator to a state is called a Bellman backup. If\nwe iterate this process, we will converge to the optimal value function V∗, as we discuss in Section 2.2.1.\nGiven the optimal value function, we can derive an optimal policy using\nπ∗(s) = argmax\na\nQ∗(s, a)\n(2.7)\n= argmax\na\n\u0002\nR(s, a) + γEpS(s′|s,a) [V∗(s′)]\n\u0003\n(2.8)\nFollowing such an optimal policy ensures the agent achieves maximum expected return starting from any\nstate.\nThe problem of solving for V∗, Q∗or π∗is called policy optimization. In contrast, solving for Vπ or Qπ\nfor a given policy π is called policy evaluation, which constitutes an important subclass of RL problems as\nwill be discussed in later sections. For policy evaluation, we have similar Bellman equations, which simply\nreplace maxa{·} in Equations (2.4) and (2.5) with Eπ(a|s) [·].\nIn Equations (2.7) and (2.8), as in the Bellman optimality equations, we must take a maximum over all\nactions in A, and the maximizing action is called the greedy action with respect to the value functions,\nQ∗or V∗. Finding greedy actions is computationally easy if A is a small finite set. For high dimensional\ncontinuous spaces, see Section 2.5.4.1.\n2.1.3\nExample: 1d grid world\nIn this section, we show a simple example, to make some of the above concepts more concrete. Consider the\n1d grid world shown in Figure 2.1(a). There are 5 possible states, among them ST 1 and ST 2 are absorbing\nstates, since the interaction ends once the agent enters them. There are 2 actions, ↑and ↓. The reward\nfunction is zero everywhere except at the goal state, ST 2, which gives a reward of 1 upon entering. Thus the\noptimal action in every state is to move down.\nFigure 2.1(b) shows the Q∗function for γ = 0. Note that we only show the function for non-absorbing\nstates, as the optimal Q-values are 0 in absorbing states by definition. We see that Q∗(s3, ↓) = 1.0, since the\nagent will get a reward of 1.0 on the next step if it moves down from s3; however, Q∗(s, a) = 0 for all other\nstate-action pairs, since they do not provide nonzero immediate reward. This optimal Q-function reflects the\nfact that using γ = 0 is completely myopic, and ignores the future.\nFigure 2.1(c) shows Q∗when γ = 1. In this case, we care about all future rewards equally. Thus\nQ∗(s, a) = 1 for all state-action pairs, since the agent can always reach the goal eventually. This is infinitely\nfar-sighted. However, it does not give the agent any short-term guidance on how to behave. For example, in\ns2, it is not clear if it is should go up or down, since both actions will eventually reach the goal with identical\nQ∗-values.\nFigure 2.1(d) shows Q∗when γ = 0.9. This reflects a preference for near-term rewards, while also taking\nfuture reward into account. This encourages the agent to seek the shortest path to the goal, which is usually\n32\nST1\nS1\nS2\nS3\nST2\nST1\nS1\nS2\nS3\nST2\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1.0\n0\n1.0\n1.0\n1.0\n1.0\n1.0\n0\n0.81\n0.73\n0.9\n0.81\n1.0\nUp \t \t\n Down\nUp \t \t\n Down\nUp \t \t\n Down\nR(s)\nQ*(s, a)\nUp\na1\n\nDown\na2\n(a)\n(b)\n(c)\n(d)\n𝛄 = 1\n𝛄 = 0\n𝛄 = 0.9\nFigure 2.1: Left: illustration of a simple MDP corresponding to a 1d grid world of 3 non-absorbing states and 2\nactions. Right: optimal Q-functions for different values of γ. Adapted from Figures 3.1, 3.2, 3.4 of [GK19].\nwhat we desire. A proper choice of γ is up to the agent designer, just like the design of the reward function,\nand has to reflect the desired behavior of the agent.\n2.2\nComputing the value function and policy given a known world\nmodel\nIn this section, we discuss how to compute the optimal value function (the prediction problem) and the\noptimal policy (the control problem) when the MDP model is known. (Sometimes the term planning is\nused to refer to computing the optimal policy, given a known model, but planning can also refer to computing\na sequence of actions, rather than a policy.) The algorithms we discuss are based on dynamic programming\n(DP) and linear programming (LP).\nFor simplicity, in this section, we assume discrete state and action sets with γ < 1. However, exact\ncalculation of optimal policies often depends polynomially on the sizes of S and A, and is intractable, for\nexample, when the state space is a Cartesian product of several finite sets. This challenge is known as\nthe curse of dimensionality. Therefore, approximations are typically needed, such as using parametric\nor nonparametric representations of the value function or policy, both for computational tractability and\nfor extending the methods to handle MDPs with general state and action sets. This requires the use of\napproximate dynamic programming (ADP) and approximate linear programming (ALP) algorithms\n(see e.g., [Ber19]).\n2.2.1\nValue iteration\nA popular and effective DP method for solving an MDP is value iteration (VI). Starting from an initial\nvalue function estimate V0, the algorithm iteratively updates the estimate by\nVk+1(s) = max\na\n\"\nR(s, a) + γ\nX\ns′\np(s′|s, a)Vk(s′)\n#\n(2.9)\nNote that the update rule, sometimes called a Bellman backup, is exactly the right-hand side of the\nBellman optimality equation Equation (2.4), with the unknown V∗replaced by the current estimate Vk. A\n33\nfundamental property of Equation (2.9) is that the update is a contraction: it can be verified that\nmax\ns\n|Vk+1(s) −V∗(s)| ≤γ max\ns\n|Vk(s) −V∗(s)|\n(2.10)\nIn other words, every iteration will reduce the maximum value function error by a constant factor.\nVk will converge to V∗, after which an optimal policy can be extracted using Equation (2.8). In practice,\nwe can often terminate VI when Vk is close enough to V∗, since the resulting greedy policy wrt Vk will be\nnear optimal. Value iteration can be adapted to learn the optimal action-value function Q∗.\n2.2.2\nReal-time dynamic programming (RTDP)\nIn value iteration, we compute V∗(s) and π∗(s) for all possible states s, averaging over all possible next states\ns′ at each iteration, as illustrated in Figure 2.2(right). However, for some problems, we may only be interested\nin the value (and policy) for certain special starting states. This is the case, for example, in shortest path\nproblems on graphs, where we are trying to find the shortest route from the current state to a goal state.\nThis can be modeled as an episodic MDP by defining a transition matrix pS(s′|s, a) where taking edge a from\nnode s leads to the neighboring node s′ with probability 1. The reward function is defined as R(s, a) = −1\nfor all states s except the goal states, which are modeled as absorbing states.\nIn problems such as this, we can use a method known as real-time dynamic programming or RTDP\n[BBS95], to efficiently compute an optimal partial policy, which only specifies what to do for the reachable\nstates. RTDP maintains a value function estimate V . At each step, it performs a Bellman backup for\nthe current state s by V (s) ←maxa EpS(s′|s,a) [R(s, a) + γV (s′)]. It picks an action a (often with some\nexploration), reaches a next state s′, and repeats the process. This can be seen as a form of the more general\nasynchronous value iteration, that focuses its computational effort on parts of the state space that are\nmore likely to be reachable from the current state, rather than synchronously updating all states at each\niteration.\n2.2.3\nPolicy iteration\nAnother effective DP method for computing π∗is policy iteration. It is an iterative algorithm that searches\nin the space of deterministic policies until converging to an optimal policy. Each iteration consists of two\nsteps, policy evaluation and policy improvement.\nThe policy evaluation step, as mentioned earlier, computes the value function for the current policy. Let π\nrepresent the current policy, v(s) = Vπ(s) represent the value function encoded as a vector indexed by states,\nr(s) = P\na π(a|s)R(s, a) represent the reward vector, and T(s′|s) = P\na π(a|s)p(s′|s, a) represent the state\ntransition matrix. Bellman’s equation for policy evaluation can be written in the matrix-vector form as\nv = r + γTv\n(2.11)\nThis is a linear system of equations in |S| unknowns. We can solve it using matrix inversion: v = (I−γT)−1r.\nAlternatively, we can use value iteration by computing vt+1 = r + γTvt until near convergence, or some form\nof asynchronous variant that is computationally more efficient.\nOnce we have evaluated Vπ for the current policy π, we can use it to derive a better policy π′, thus the\nname policy improvement. To do this, we simply compute a deterministic policy π′ that acts greedily with\nrespect to Vπ in every state, using\nπ′(s) = argmax\na\n{R(s, a) + γE [Vπ(s′)]}\n(2.12)\nWe can guarantee that Vπ′ ≥Vπ. This is called the policy improvement theorem. To see this, define r′,\nT′ and v′ as before, but for the new policy π′. The definition of π′ implies r′ + γT′v ≥r + γTv = v, where\nthe equality is due to Bellman’s equation. Repeating the same equality, we have\nv ≤r′ + γT′v ≤r′ + γT′(r′ + γT′v) ≤r′ + γT′(r′ + γT′(r′ + γT′v)) ≤· · ·\n(2.13)\n= (I + γT′ + γ2T′2 + · · · )r′ = (I −γT′)−1r′ = v′\n(2.14)\n34\nFigure 2.2: Policy iteration vs value iteration represented as backup diagrams. Empty circles represent states, solid\n(filled) circles represent states and actions. Adapted from Figure 8.6 of [SB18].\nStarting from an initial policy π0, policy iteration alternates between policy evaluation (E) and improvement\n(I) steps, as illustrated below:\nπ0\nE→Vπ0\nI→π1\nE→Vπ1 · · ·\nI→π∗\nE→V∗\n(2.15)\nThe algorithm stops at iteration k, if the policy πk is greedy with respect to its own value function Vπk. In\nthis case, the policy is optimal. Since there are at most |A||S| deterministic policies, and every iteration\nstrictly improves the policy, the algorithm must converge after finite iterations.\nIn PI, we alternate between policy evaluation (which involves multiple iterations, until convergence of\nVπ), and policy improvement. In VI, we alternate between one iteration of policy evaluation followed by one\niteration of policy improvement (the “max” operator in the update rule). We are in fact free to intermix any\nnumber of these steps in any order. The process will converge once the policy is greedy wrt its own value\nfunction.\nNote that policy evaluation computes Vπ whereas value iteration computes V∗. This difference is illustrated\nin Figure 2.2, using a backup diagram. Here the root node represents any state s, nodes at the next level\nrepresent state-action combinations (solid circles), and nodes at the leaves representing the set of possible\nresulting next state s′ for each possible action. In PE, we average over all actions according to the policy,\nwhereas in VI, we take the maximum over all actions.\n2.3\nComputing the value function without knowing the world model\nIn the rest of this chapter, we assume the agent only has access to samples from the environment, (s′, r) ∼\np(s′, r|s, a). We will show how to use these samples to learn optimal value function and Q-function, even\nwithout knowing the MDP dynamics.\n2.3.1\nMonte Carlo estimation\nRecall that Vπ(s) = E [Gt|st = s] is the sum of expected (discounted) returns from state s if we follow policy\nπ. A simple way to estimate this is to rollout the policy, and then compute the average sum of discounted\nrewards. The trajectory ends when we reach a terminal state, if the task is episodic, or when the discount\nfactor γt becomes negligibly small, whichever occurs first. This is called Monte Carlo estimation. We can\nuse this to update our estimate of the value function as follows:\nV (st) ←V (st) + η [Gt −V (st)]\n(2.16)\nwhere η is the learning rate, and the term in brackets is an error term. We can use a similar technique to\nestimate Qπ(s, a) = E [Gt|st = s, at = a] by simply starting the rollout with action a.\nWe can use MC estimation of Q, together with policy iteration (Section 2.2.3), to learn an optimal policy.\nSpecifically, at iteration k, we compute a new, improved policy using πk+1(s) = argmaxa Qk(s, a), where Qk\n35\nis approximated using MC estimation. This update can be applied to all the states visited on the sampled\ntrajectory. This overall technique is called Monte Carlo control.\nTo ensure this method converges to the optimal policy, we need to collect data for every (state, action)\npair, at least in the tabular case, since there is no generalization across different values of Q(s, a). One way\nto achieve this is to use an ϵ-greedy policy (see Section 1.4.1). Since this is an on-policy algorithm, the\nresulting method will converge to the optimal ϵ-soft policy, as opposed to the optimal policy. It is possible to\nuse importance sampling to estimate the value function for the optimal policy, even if actions are chosen\naccording to the ϵ-greedy policy. However, it is simpler to just gradually reduce ϵ.\n2.3.2\nTemporal difference (TD) learning\nThe Monte Carlo (MC) method in Section 2.3.1 results in an estimator for V (s) with very high variance, since\nit has to unroll many trajectories, whose returns are a sum of many random rewards generated by stochastic\nstate transitions. In addition, it is limited to episodic tasks (or finite horizon truncation of continuing tasks),\nsince it must unroll to the end of the episode before each update step, to ensure it reliably estimates the long\nterm return.\nIn this section, we discuss a more efficient technique called temporal difference or TD learning [Sut88].\nThe basic idea is to incrementally reduce the Bellman error for sampled states or state-actions, based on\ntransitions instead of a long trajectory. More precisely, suppose we are to learn the value function Vπ for a\nfixed policy π. Given a state transition (st, at, rt, st+1), where at ∼π(st), we change the estimate V (st) so\nthat it moves towards the target value qt = rt + γV (st+1) ≈Gt:t+1:\nV (st) ←V (st) + η\n\nrt + γV (st+1) −V (st)\n|\n{z\n}\nδt\n\n\n(2.17)\nwhere η is the learning rate. (See [RFP15] for ways to adaptively set the learning rate.) The δt = yt −V (st)\nterm is known as the TD error.\nA more general form of TD update for parametric value function\nrepresentations is\nw ←w + η [rt + γVw(st+1) −Vw(st)] ∇wVw(st)\n(2.18)\nwe see that Equation (2.16) is a special case. The TD update rule for evaluating Qπ is similar, except we\nreplace states with states and actions.\nIt can be shown that TD learning in the tabular case, Equation (2.16), converges to the correct value func-\ntion, under proper conditions [Ber19]. However, it may diverge when using nonlinear function approximators,\nas we discuss in Section 2.5.2.4. The reason is that this update is a “semi-gradient”, which refers to the fact\nthat we only take the gradient wrt the value function, ∇wV (st, wt), treating the target Ut as constant.\nThe potential divergence of TD is also consistent with the fact that Equation (2.18) does not correspond\nto a gradient update on any objective function, despite having a very similar form to SGD (stochastic gradient\ndescent). Instead, it is an example of bootstrapping, in which the estimate, Vw(st), is updated to approach\na target, rt + γVw(st+1), which is defined by the value function estimate itself. This idea is shared by DP\nmethods like value iteration, although they rely on the complete MDP model to compute an exact Bellman\nbackup. In contrast, TD learning can be viewed as using sampled transitions to approximate such backups.\nAn example of a non-bootstrapping approach is the Monte Carlo estimation in the previous section. It\nsamples a complete trajectory, rather than individual transitions, to perform an update; this avoids the\ndivergence issue, but is often much less efficient. Figure 2.3 illustrates the difference between MC, TD, and\nDP.\n2.3.3\nCombining TD and MC learning using TD(λ)\nA key difference between TD and MC is the way they estimate returns. Given a trajectory τ = (s0, a0, r0, s1, . . . , sT ),\nTD estimates the return from state st by one-step lookahead, Gt:t+1 = rt + γV (st+1), where the return from\n36\nFigure 2.3: Backup diagrams of V (st) for Monte Carlo, temporal difference, and dynamic programming updates of the\nstate-value function. Used with kind permission of Andy Barto.\ntime t + 1 is replaced by its value function estimate. In contrast, MC waits until the end of the episode\nor until T is large enough, then uses the estimate Gt:T = rt + γrt+1 + · · · + γT −t−1rT −1. It is possible to\ninterpolate between these by performing an n-step rollout, and then using the value function to approximate\nthe return for the rest of the trajectory, similar to heuristic search (Section 4.1.2). That is, we can use the\nn-step return\nGt:t+n = rt + γrt+1 + · · · + γn−1rt+n−1 + γnV (st+n)\n(2.19)\nFor example, the 1-step and 2-step returns are given by\nGt:t+1 = rt + γvt+1\n(2.20)\nGt:t+1 = rt + γrt+1 + γ2vt+2\n(2.21)\nThe corresponding n-step version of the TD update becomes\nw ←w + η [Gt:t+n −Vw(st)] ∇wVw(st)\n(2.22)\nRather than picking a specific lookahead value, n, we can take a weighted average of all possible values,\nwith a single parameter λ ∈[0, 1], by using\nGλ\nt ≜(1 −λ)\n∞\nX\nn=1\nλn−1Gt:t+n\n(2.23)\nThis is called the lambda return. Note that these coefficients sum to one (since P∞\nt=0(1 −λ)λt = 1−λ\n1−λ = 1,\nfor λ < 1), so the return is a convex combination of n-step returns. See Figure 2.4 for an illustration. We can\nnow use Gλ\nt inside the TD update instead of Gt:t+n; this is called TD(λ).\nNote that, if a terminal state is entered at step T (as happens with episodic tasks), then all subsequent\nn-step returns are equal to the conventional return, Gt. Hence we can write\nGλ\nt = (1 −λ)\nT −t−1\nX\nn=1\nλn−1Gt:t+n + λT −t−1Gt\n(2.24)\nFrom this we can see that if λ = 1, the λ-return becomes equal to the regular MC return Gt. If λ = 0, the\nλ-return becomes equal to the one-step return Gt:t+1 (since 0n−1 = 1 iff n = 1), so standard TD learning is\noften called TD(0) learning. This episodic form also gives us the following recursive equation\nGλ\nt = rt + γ[(1 −λ)vt+1 + λGλ\nt+1]\n(2.25)\nwhich we initialize with GT = vt.\n37\nFigure 2.4: The backup diagram for TD(λ). Standard TD learning corresponds to λ = 0, and standard MC learning\ncorresponds to λ = 1. From Figure 12.1 of [SB18]. Used with kind permission of Richard Sutton.\n2.3.4\nEligibility traces\nAn important benefit of using the geometric weighting in Equation (2.23), as opposed to the n-step update,\nis that the corresponding TD learning update can be efficiently implemented through the use of eligibility\ntraces, even though Gλ\nt is a sum of infinitely many terms. The eligibility term is a weighted sum of the\ngradients of the value function:\nzt = γλzt−1 + ∇wVw(st)\n(2.26)\n(This trace term gets reset to 0 at the start of each episode.) We replace the TD(0) update of wt+1 =\nwt + ηδt∇wVw(st) with the TD(λ) version to get\nwt+1 = wt + ηδtzt\n(2.27)\nSee [Sei+16] for more details.\n2.4\nSARSA: on-policy TD control\nTD learning is for policy evaluation, as it estimates the value function for a fixed policy. In order to find an\noptimal policy, we may use the algorithm as a building block inside generalized policy iteration (Section 2.2.3).\nIn this case, it is more convenient to work with the action-value function, Q, and a policy π that is greedy\nwith respect to Q. The agent follows π in every step to choose actions, and upon a transition (s, a, r, s′) the\nTD update rule is\nQ(s, a) ←Q(s, a) + η [r + γQ(s′, a′) −Q(s, a)]\n(2.28)\nwhere a′ ∼π(s′) is the action the agent will take in state s′. After Q is updated (for policy evaluation), π\nalso changes accordingly as it is greedy with respect to Q (for policy improvement). This algorithm, first\nproposed by [RN94], was further studied and renamed to SARSA by [Sut96]; the name comes from its\nupdate rule that involves an augmented transition (s, a, r, s′, a′).\nIn order for SARSA to converge to Q∗, every state-action pair must be visited infinitely often, at least in\nthe tabular case, since the algorithm only updates Q(s, a) for (s, a) that it visits. One way to ensure this\ncondition is to use a “greedy in the limit with infinite exploration” (GLIE) policy. An example is the ϵ-greedy\npolicy, with ϵ vanishing to 0 gradually. It can be shown that SARSA with a GLIE policy will converge to Q∗\nand π∗[Sin+00].\n38\n2.5\nQ-learning: off-policy TD control\nSARSA is an on-policy algorithm, which means it learns the Q-function for the policy it is currently using,\nwhich is typically not the optimal policy, because of the need to perform exploration. However, with a\nsimple modification, we can convert this to an off-policy algorithm that learns Q∗, even if a suboptimal or\nexploratory policy is used to choose actions.\n2.5.1\nTabular Q learning\nSuppose we modify SARSA by replacing the sampled next action a′ ∼π(s′) in Equation (2.28) with a greedy\naction: a′ = argmaxb Q(s′, b). This results in the following update when a transition (s, a, r, s′) happens\nQ(s, a) ←Q(s, a) + η\nh\nr + γ max\na′ Q(s′, a′) −Q(s, a)\ni\n(2.29)\nThis is the update rule of Q-learning for the tabular case [WD92].\nSince it is off-policy, the method can use (s, a, r, s′) triples coming from any data source, such as older\nversions of the policy, or log data from an existing (non-RL) system. If every state-action pair is visited\ninfinitely often, the algorithm provably converges to Q∗in the tabular case, with properly decayed learning\nrates [Ber19]. Algorithm 1 gives a vanilla implementation of Q-learning with ϵ-greedy exploration.\nAlgorithm 1: Tabular Q-learning with ϵ-greedy exploration\n1 Initialize value function Q\n2 repeat\n3\nSample starting state s of new episode\n4\nrepeat\n5\nSample action a =\n(\nargmaxb Q(s, b),\nwith probability 1 −ϵ\nrandom action,\nwith probability ϵ\n6\n(s′, r) = env.step(a)\n7\nCompute the TD error: δ = r + γ maxa′ Q(s′, a′) −Q(s, a)\n8\nQ(s, a) ←Q(s, a) + ηδ\n9\ns ←s′\n10\nuntil state s is terminal\n11 until converged\nFor terminal states, s ∈S+, we know that Q(s, a) = 0 for all actions a. Consequently, for the optimal\nvalue function, we have V ∗(s) = maxa′ Q∗(s, a) = 0 for all terminal states. When performing online learning,\nwe don’t usually know which states are terminal. Therefore we assume that, whenever we take a step in the\nenvironment, we get the next state s′ and reward r, but also a binary indicator done(s′) that tells us if s′ is\nterminal. In this case, we set the target value in Q-learning to V ∗(s′) = 0 yielding the modified update rule:\nQ(s, a) ←Q(s, a) + η\nh\nr + (1 −done(s′))γ max\na′ Q(s′, a′) −Q(s, a)\ni\n(2.30)\nFor brevity, we will usually ignore this factor in the subsequent equations, but it needs to be implemented in\nthe code.\nFigure 2.5 gives an example of Q-learning applied to the simple 1d grid world from Figure 2.1, using\nγ = 0.9. We show the Q-functon at the start and end of each episode, after performing actions chosen by an\nϵ-greedy policy. We initialize Q(s, a) = 0 for all entries, and use a step size of η = 1. At convergence, we have\nQ∗(s, a) = r + γQ∗(s′, a∗), where a∗=↓for all states.\n39\nQ-function \nepisode start\nEpisode Time Step Action\n(s,α,r , s')\nr + γ Q*(s' , α)\nUP     DOWN\n1\n1\n(S1 , D,0,S2)\n0 + 0.9 X 0 = 0\n1\n2\n(S2 ,U,0,S1)\n0 + 0.9 X 0 = 0\n1\n3\n(S1 , D,0,S2)\n0 + 0.9 X 0 = 0\n1\n4\n(S2 , U,0,S1)\n0 + 0.9 X 0 = 0\n1\n5\n(S3 , D,1,ST2)\n1\n2\n1\n(S1 , D,0,S2)\n0 + 0.9 x 0 = 0\n2\n2\n(S2 , D,0,S3)\n0 + 0.9 x 1 = 0.9\n2\n3\n(S3 , D,0,ST2)\n1\n3\n1\n(S1 , D,0,S2)\n0 + 0.9 x 0.9 = 0.81\n3\n2\n(S2 , D,0,S3)\n0 + 0.9 x 1 = 0.9\n3\n3\n(S3 , D,0,S2)\n0 + 0.9 x 0.9 = 0.81\n3\n4\n(S2 , D,0,S3)\n0 + 0.9 x 1 = 0.9\n3\n5\n(S3 , D,0,ST2)\n1\n4\n1\n(S1 , D,0,S2)\n0 + 0.9 x 0.9 = 0.81\n4\n2\n(S2 , U,0,S1)\n0 + 0.9 x 0.81 = 0.73\n4\n3\n(S1 , D,0,S2)\n0 + 0.9 x 0.9 = 0.81\n4\n4\n(S2 , U,0,S3)\n0 + 0.9 x 0.81 = 0.73\n4\n5\n(S1 , D,0,S3)\n0 + 0.9 x 0.9 = 0.81\n4\n6\n(S2 , D,0,S3)\n0 + 0.9 x 1 = 0.9\n4\n7\n(S2 , D,0,S3)\n1\n5\n1\n(S1 , U, 0,ST1)\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0.9\n0\n1\n0\n0.81\n0\n0.9\n0.81\n1\n0\n0\n0\n0.9\n0\n1\n0\n0.81\n0\n0.9\n0.81\n1\n0\n0.81\n0.73\n0.9\n0.81\n1\n0\n0.81\n0.73\n0.9\n0.81\n1\n0\n0.81\n0.73\n0.9\n0.81\n1\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nS1\nS2\nS3\nQ-function       \nepisode end\nUP     DOWN\nQ1\nQ2\nQ3\nQ4\nQ5\nFigure 2.5: Illustration of Q learning for one random trajectory in the 1d grid world in Figure 2.1 using ϵ-greedy\nexploration. At the end of episode 1, we make a transition from S3 to ST 2 and get a reward of r = 1, so we estimate\nQ(S3, ↓) = 1. In episode 2, we make a transition from S2 to S3, so S2 gets incremented by γQ(S3, ↓) = 0.9. Adapted\nfrom Figure 3.3 of [GK19].\n40\n2.5.2\nQ learning with function approximation\nTo make Q learning work with high-dimensional state spaces, we have to replace the tabular (non-parametric)\nrepresentation with a parametric approximation, denoted Qw(s, a). We can update this function using one or\nmore steps of SGD on the following loss function\nL(w|s, a, r, s′) =\n\u0000(r + γ max\na′ Qw(s′, a′)) −Qw(s, a)\n\u00012\n(2.31)\nSince nonlinear functions need to be trained on minibatches of data, we compute the average loss over multiple\nrandomly sampled experience tuples (see Section 2.5.2.3 for discussion) to get\nL(w) = E(s,a,r,s′)∼U(D) [L(w|s, a, r, s′)]\n(2.32)\nSee Algorithm 2 for the pseudocode.\nAlgorithm 2: Q learning with function approximation and replay buffers\n1 Initialize environment state s, network parameters w0, replay buffer D = ∅, discount factor γ, step\nsize η, policy π0(a|s) = ϵUnif(a) + (1 −ϵ)δ(a = argmaxa Qw0(s, a))\n2 for iteration k = 0, 1, 2, . . . do\n3\nfor environment step s = 0, 1, . . . , S −1 do\n4\nSample action: a ∼πk(a|s)\n5\nInteract with environment: (s′, r) = env.step(a)\n6\nUpdate buffer: D ←D ∪{(s, a, s′, r)}\n7\nwk,0 ←wk\n8\nfor gradient step g = 0, 1, . . . , G −1 do\n9\nSample batch: B ⊂D\n10\nCompute error: L(B, wk,g) =\n1\n|B|\nP\n(s,a,r,s′)∈B\n\u0002\nQwk,g(s, a) −(r + γ maxa′ Qwk(s′, a′))\n\u00032\n11\nUpdate parameters: wk,g ←wk,g −η∇wk,gL(B, wk,g)\n12\nwk+1 ←wk,G\n2.5.2.1\nNeural fitted Q\nThe first approach of this kind is known as neural fitted Q iteration [Rie05], which corresponds to fully\noptimizing L(w) at each iteration (equivalent to using G = ∞gradient steps).\n2.5.2.2\nDQN\nThe influential deep Q-network or DQN paper of [Mni+15] also used neural nets to represent the Q function,\nbut performed a smaller number of gradient updates per iteration. Furthermore, they proposed to modify the\ntarget value when fitting the Q function in order to avoid instabilities during training (see Section 2.5.2.4 for\ndetails).\nThe DQN method became famous since it was able to train agents that can outperform humans when\nplaying various Atari games from the ALE (Atari Learning Environment) benchmark [Bel+13]. Here the\ninput is a small color image, and the action space corresponds to moving left, right, up or down, plus an\noptional shoot action.1\nSince 2015, many more extensions to DQN have been proposed, with the goal of improving performance\nin various ways, either in terms of peak reward obtained, or sample efficiency (e.g., reward obtained after only\n1For more discussion of ALE, see [Mac+18a], and for a recent extension to continuous actions (representing joystick control),\nsee the CALE benchmark of [FC24]. Note that DQN was not the first deep RL method to train an agent from pixel input; that\nhonor goes to [LR10], who trained an autoencoder to embed images into low-dimensional latents, and then used neural fitted Q\nlearning (Section 2.5.2.1) to fit the Q function.\n41\n(a)\n(b)\nFigure 2.6: (a) A simple MDP. (b) Parameters of the policy diverge over time. From Figures 11.1 and 11.2 of [SB18].\nUsed with kind permission of Richard Sutton.\n100k steps in the environment, as proposed in the Atari-100k benchmark [Kai+19]), or training stability, or\nall of the above. We discuss some of these extensions in Section 2.5.4.\n2.5.2.3\nExperience replay\nSince Q learning is an off-policy method, we can update the Q function using any data source. This is\nparticularly important when we use nonlinear function approximation (see Section 2.5.2), which often needs a\nlot of data for model fitting. A natural source of data is data collected earlier in the trajectori of the agent;\nthis is called an experience replay buffer, which stores (s, a, r, s′) transition tuples into a buffer. This can\nimprove the stability and sample efficiency of learning, and was originally proposed in [Lin92].\nThis modification has two advantages. First, it improves data efficiency as every transition can be used\nmultiple times. Second, it improves stability in training, by reducing the correlation of the data samples\nthat the network is trained on, since the training tuples do not have to come from adjacent moments in time.\n(Note that experience replay requires the use of off-policy learning methods, such as Q learning, since the\ntraining data is sampled from older versions of the policy, not the current policy.)\nIt is possible to replace the uniform sampling from the buffer with one that favors more important\ntransition tuples that may be more informative about Q. This idea is formalized in [Sch+16a], who develop a\ntechnique known as prioritized experience replay.\n2.5.2.4\nThe deadly triad\nThe problem with the naive Q learning objective in Equation (2.31) is that it can lead to instability, since\nthe target we are regressing towards uses the same parameters w as the function we are updating. So the\nnetwork is “chasing its own tail”. Although this is fine for tabular models, it can fail for nonlinear models, as\nwe discuss below.\nIn general, an RL algorithm can become unstable when it has these three components: function approxi-\nmation (such as neural networks), bootstrapped value function estimation (i.e., using TD-like methods instead\nof MC), and off-policy learning (where the actions are sampled from some distribution other than the policy\nthat is being optimized). This combination is known as the deadly triad [Sut15; van+18]).\nA classic example of this is the simple MDP depicted in Figure 2.6a, due to [Bai95]. (This is known as\nBaird’s counter example.) It has 7 states and 2 actions. Taking the dashed action takes the environment\nto the 6 upper states uniformly at random, while the solid action takes it to the bottom state. The reward is\n0 in all transitions, and γ = 0.99. The value function Vw uses a linear parameterization indicated by the\nexpressions shown inside the states, with w ∈R8. The target policies π always chooses the solid action in\nevery state. Clearly, the true value function, Vπ(s) = 0, can be exactly represented by setting w = 0.\n42\nSuppose we use a behavior policy b to generate a trajectory, which chooses the dashed and solid actions\nwith probabilities 6/7 and 1/7, respectively, in every state. If we apply TD(0) on this trajectory, the\nparameters diverge to ∞(Figure 2.6b), even though the problem appears simple. In contrast, with on-policy\ndata (that is, when b is the same as π), TD(0) with linear approximation can be guaranteed to converge to\na good value function approximate [TR97]. The difference is that with on-policy learning, as we improve\nthe value function, we also improve the policy, so the two become self-consistent, whereas with off-policy\nlearning, the behavior policy may not match the optimal value function that is being learned, leading to\ninconsistencies.\nThe divergence behavior is demonstrated in many value-based bootstrapping methods, including TD,\nQ-learning, and related approximate dynamic programming algorithms, where the value function is represented\neither linearly (like the example above) or nonlinearly [Gor95; TVR97; OCD21]. The root cause of these\ndivergence phenomena is that bootstrapping methods typically are not minimizing a fixed objective function.\nRather, they create a learning target using their own estimates, thus potentially creating a self-reinforcing\nloop to push the estimates to infinity. More formally, the problem is that the contraction property in the\ntabular case (Equation (2.10)) may no longer hold when V is approximated by Vw.\nWe discuss some solutions to the deadly triad problem below.\n2.5.2.5\nTarget networks\nOne heuristic solution to the deadly triad, proposed in the DQN paper, is to use a “frozen” target network\ncomputed at an earlier iteration to define the target value for the DQN updates, rather than trying to chase\na constantly moving target. Specifically, we maintain an extra copy the Q-network, Qw−, with the same\nstructure as Qw. This new Q-network is used to compute bootstrapping targets\nq(r, s′; w−) = r + γ max\na′ Qw−(s′, a′)\n(2.33)\nfor training Qw. We can periodically set w−←sg(w), usually after a few episodes, where the stop gradient\noperator is used to prevent autodiff propagating gradients back to w. Alternatively, we can use an exponential\nmoving average (EMA) of the weights, i.e., we use w = ρw + (1 −ρ)sg(w), where ρ ≪1 ensures that Qw\nslowly catches up with Qw. (If ρ = 0, we say that this is a detached target, since it is just a frozen copy of\nthe current weights.) The final loss has the form\nL(w) = E(s,a,r,s′)∼U(D) [L(w|s, a, r, s′)]\n(2.34)\nL(w|s, a, r, s′) = (q(r, s′; w) −Qw(s, a))2\n(2.35)\nTheoretical work justifying this technique is given in [FSW23; Che+24a].\n2.5.2.6\nTwo time-scale methods\nA general way to ensure convergence in off-policy learning is to construct an objective function, the minimiza-\ntion of which leads to a good value function approximation. This is the basis of the gradient TD method of\n[SSM08; Mae+09; Ghi+20]. In practice, this can be achieved by updating the target value in the TD update\nmore quickly than the value function itself; this is known as a two timescale optimization (see e.g., [Yu17;\nZha+19; Hon+23]). It is also possible to use a standard single timescale method provided the target value is\ncomputed using a frozen target network, as discussed in Section 2.5.2.5. See [FSW23; Che+24a] for details.\n2.5.2.7\nLayer norm\nMore recently, [Gal+24] proved that just adding LayerNorm [BKH16] to the penultimate layer of the critic net-\nwork, just before the linear head, is sufficient to provably yield convergence of TD learning even in the off-policy\nsetting. In particular, suppose the network has the form Q(s, a|w, θ) = wT ReLU(LayerNorm(f(s, a; θ))).\nSince ||LayerNorm(f(s, a; θ))|| ≤1, we have ||Q(s, a|w, θ) ≤||w||, which means the magnitude of the output\nis always bounded, as shown in Figure 2.7. In [Gal+24], they prove this (plus ℓ2 regularization on w, and a\nsufficiently wide penultimate layer) is sufficient to ensure convergence of the value function estimate.\n43\n−1.00−0.75−0.50−0.250.00 0.25 0.50 0.75 1.00 −1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nData\n−1.00−0.75−0.50−0.250.00 0.25 0.50 0.75 1.00 −1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNo LayerNorm\n−1.00−0.75−0.50−0.250.00 0.25 0.50 0.75 1.00 −1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWith LayerNorm\nFigure 2.7: We generate a dataset (left) with inputs x distributed in a circle with radius 0.5 and labels y = ||x||. We\nthen fit a two-layer MLP without LayerNorm (center) and with LayerNorm (right). LayerNorm bounds the values and\nprevents catastrophic overestimation when extrapolating. From Figure 3 of [Bal+23]. Used with kind permission of\nPhilip Ball.\nFigure 2.8: Comparison of Q-learning and double Q-learning on a simple episodic MDP using ϵ-greedy action selection\nwith ϵ = 0.1. The initial state is A, and squares denote absorbing states. The data are averaged over 10,000 runs.\nFrom Figure 6.5 of [SB18]. Used with kind permission of Richard Sutton.\n2.5.3\nMaximization bias\nStandard Q-learning suffers from a problem known as the optimizer’s curse [SW06], or the maximization\nbias. The problem refers to the simple statistical inequality: E [maxa Xa] ≥maxa E [Xa], for a set of random\nvariables {Xa}. Thus, if we pick actions greedily according to their random scores {Xa}, we might pick a\nwrong action just because random noise makes it appealing.\nFigure 2.8 gives a simple example of how this can happen in an MDP. The start state is A. The right\naction gives a reward 0 and terminates the episode. The left action also gives a reward of 0, but then enters\nstate B, from which there are many possible actions, with rewards drawn from N(−0.1, 1.0). Thus the\nexpected return for any trajectory starting with the left action is −0.1, making it suboptimal. Nevertheless,\nthe RL algorithm may pick the left action due to the maximization bias making B appear to have a positive\nvalue.\n2.5.3.1\nDouble Q-learning\nOne solution to avoid the maximization bias is to use two separate Q-functions, Q1 and Q2, one for selecting\nthe greedy action, and the other for estimating the corresponding Q-value. In particular, upon seeing a\ntransition (s, a, r, s′), we perform the following update for i = 1 : 2:\nQi(s, a) ←Qi(s, a) + η(qi(s, a) −Qi(s, a))\n(2.36)\nqi(s, a) = r + γQi(s′, argmax\na′\nQ−i(s′, a′))\n(2.37)\nSo we see that Q1 uses Q2 to choose the best action but uses Q1 to evaluate it, and vice versa. This technique\nis called double Q-learning [Has10]. Figure 2.8 shows the benefits of the algorithm over standard Q-learning\n44\nin a toy problem.\n2.5.3.2\nDouble DQN\nIn [HGS16], they combine double Q learning with deep Q networks (Section 2.5.2.2) to get double DQN.\nThis modifies Equation (2.37) to its gradient form, and then the current network for action proposals, but\nthe target network for action evaluation. Thus the training target becomes\nq(r, s′; w, w) = r + γQw(s′, argmax\na′\nQw(s′, a′))\n(2.38)\nIn Section 3.6.2 we discuss an extension called clipped double DQN which uses two Q networks and\ntheir frozen copies to define the following target:\nq(r, s′; w1:2, w1:2) = r + γ min\ni=1,2 Qwi(s′, argmax\na′\nQwi(s′, a′))\n(2.39)\nwhere Qwi is the target network for Qwi.\n2.5.3.3\nRandomized ensemble DQN\nThe double DQN method is extended in the REDQ (randomized ensembled double Q learning) method\nof [Che+20], which uses an ensemble of N > 2 Q-networks. Furthermore, at each step, it draws a random\nsample of M ≤N networks, and takes the minimum over them when computing the target value. That is, it\nuses the following update (see Algorithm 2 in appendix of [Che+20]):\nq(r, s′; w1:N, w1:N) = r + γ max\na′\nmin\ni∈M Qwi(s′, a′)\n(2.40)\nwhere M is a random subset from the N value functions. The ensemble reduces the variance, and the\nminimum reduces the overestimation bias.2 If we set N = M = 2, we get a method similar to clipped double\nQ learning. (Note that REDQ is very similiar to the Random Ensemble Mixture method of [ASN20],\nwhich was designed for offline RL.)\n2.5.4\nDQN extensions\nIn this section, we discuss various extensions of DQN.\n2.5.4.1\nQ learning for continuous actions\nQ learning is not directly applicable to continuous actions due to the need to compute the argmax over\nactions. An early solution to this problem, based on neural fitted Q learning (see Section 2.5.2.1), is proposed\nin [HR11]. This became the basis of the DDPG algorithm of Section 3.6.1, which learns a policy to predict\nthe argmax.\nAn alternative approach is to use gradient-free optimizers such as the cross-entropy method to approximate\nthe argmax. The QT-Opt method of [Kal+18] treats the action vector a as a sequence of actions, and\noptimizes one dimension at a time [Met+17]. The CAQL (continuous action Q-learning) method of [Ryu+20])\nuses mixed integer programming to solve the argmax problem, leveraging the ReLU structure of the Q-network.\nThe method of [Sey+22] quantizes each action dimension separately, and then solves the argmax problem\nusing methods inspired by multi-agent RL.\n2In addition, REDQ performs G ≫1 updates of the value functions for each environment step; this high Update-To-Data\n(UTD) ratio (also called Replay Ratio) is critical for sample efficiency, and is commonly used in model-based RL.\n45\n2.5.4.2\nDueling DQN\nThe dueling DQN method of [Wan+16], learns a value function and an advantage function, and derives the\nQ function, rather than learning it directly. This is helpful when there are many actions with similar Q-values,\nsince the advantage A(s, a) = Q(s, a) −V (s) focuses on the differences in value relative to a shared baseline.\nIn more detail, we define a network with |A| + 1 output heads, which computes Aw(s, a) for a = 1 : A\nand Vw(s). We can then derive\nQw(s, a) = Vw(s) + Aw(s, a)\n(2.41)\nHowever, this naive approach ignores the following constraint that holds for any policy π:\nEπ(a|s) [Aπ(s, a)] = Eπ(a|s) [Qπ(s, a) −V π(s)]\n(2.42)\n= V π(s) −V π(s) = 0\n(2.43)\nFortunately, for the optimal policy π∗(s) = argmaxa′ Q∗(s, a′) we have\n0 = Eπ∗(a|s) [Q∗(s, a)] −V ∗(s)\n(2.44)\n= Q∗(s, argmax\na′\nQ∗(s, a′)) −V ∗(s)\n(2.45)\n= max\na′ Q∗(s, a′) −V ∗(s)\n(2.46)\n= max\na′ A∗(s, a′)\n(2.47)\nThus we can satisfy the constraint for the optimal policy by subtracting off maxa A(s, a) from the advantage\nhead. Equivalently we can compute the Q function using\nQw(s, a) = Vw(s) + Aw(s, a) −max\na′ Aw(s, a′)\n(2.48)\nIn practice, the max is replaced by an average, which seems to work better empirically.\n2.5.4.3\nNoisy nets and exploration\nStandard DQN relies on the epsilon-greedy strategy to perform exploration. However, this will explore equally\nin all states, whereas we would like to the amount of exploration to be state dependent, to reflect the amount\nof uncertainty in the outcomes of trying each action in that state due to lack of knowledge (i.e., epistemic\nuncertainty rather than aleatoric or irreducile uncertainty). An early approach to this, known as noisy\nnets [For+18], added random noise to the network weights to encourage exploration which is temporally\nconsistent within episodes. More recent methods for exploration are discussed in Section 1.4.\n2.5.4.4\nMulti-step DQN\nAs we discussed in Section 2.3.3, we can reduce the bias introduced by bootstrapping by replacing TD(1)\nupdates with TD(n) updates, where we unroll the value computation for n MC steps, and then plug in the\nvalue function at the end. We can apply this to the DQN context by defining the target\nq(s0, a0) =\nn\nX\nt=1\nγt−1rt + γn max\nan Qw(sn, an)\n(2.49)\nThis can be implemented for episodic environments by storing experience tuples of the form\nτ = (s, a,\nn\nX\nk=1\nγk−1rk, sn, done)\n(2.50)\nwhere done = 1 if the trajectory ended at any point during the n-step rollout.\n46\nFigure 2.9: Plot of median human-normalized score over all 57 Atari games for various DQN agents. The yellow,\nred and green curves are distributional RL methods (Section 5.1), namely categorical DQN (C51) (Section 5.1.2)\nQuantile Regression DQN (Section 5.1.1), and Implicit Quantile Networks [Dab+18]. Figure from https: // github.\ncom/ google-deepmind/ dqn_ zoo .\nTheoretically this method is only valid if all the intermediate actions, a2:n−1, are sampled from the current\noptimal policy derived from Qw, as opposed to some behavior policy, such as epsilon greedy or some samples\nfrom the replay buffer from an old policy. In practice, we can just restrict sampling to recent samples from\nthe replay buffer, making the resulting method approximately on-policy.\nInstead of using a fixed n, it is possible to use a weighted combination of returns; this is known as the\nQ(λ) algorithm [PW94; Koz+21].\n2.5.4.5\nRainbow\nThe Rainbow method of [Hes+18] combined 6 improvements to the vanilla DQN method, as listed below.\n(The paper is called “Rainbow” due to the color coding of their results plot, a modified version of which is\nshown in Figure 2.9.) At the time it was published (2018), this produced SOTA results on the Atari-200M\nbenchmark. The 6 improvements are as follows:\n• Use double DQN, as in Section 2.5.3.2.\n• Use prioritized experience replay, as in Section 2.5.2.3.\n• Use the categorical DQN (C51) (Section 5.1.2) distributional RL method.\n• Use n-step returns (with n = 3), as in Section 2.5.4.4.\n• Use dueling DQN, as in Section 2.5.4.2.\n• Use noisy nets, as in Section 2.5.4.3.\nEach improvement gives diminishing returns, as can be see in Figure 2.9.\nRecently the “Beyond the Rainbow” paper [Unk24] proposed several more extensions:\n• Use a larger CNN with residual connections, namely the Impala network from [Esp+18] with the\nmodifications (including the use of spectral normalization) proposed in [SS21].\n• Replace C51 with Implicit Quantile Networks [Dab+18].\n• Use Munchausen RL [VPG20], which modifies the Q learning update rule by adding an entropy-like\npenalty.\n• Collect 1 environment step from 64 parallel workers for each minibatch update (rather than taking\nmany steps from a smaller number of workers).\n47\n2.5.4.6\nBigger, Better, Faster\nAt the time of writing this document (2024), the SOTA on the 100k sample-efficient Atari benchmark [Kai+19]\nis obtained by the BBF algorithm of [Sch+23b]. (BBF stands for “Bigger, Better, Faster”.) It uses the\nfollowing tricks, in order of decreasing importance:\n• Use a larger CNN with residual connections, namely a modified version of the Impala network from\n[Esp+18].\n• Increase the update-to-data (UTD) ratio (number of times we update the Q function for every\nobservation that is observed), in order to increase sample efficiency [HHA19].\n• Use a periodic soft reset of (some of) the network weights to avoid loss of elasticity due to increased\nnetwork updates, following the SR-SPR method of [D’O+22].\n• Use n-step returns, as in Section 2.5.4.4, and then gradually decrease (anneal) the n-step return from\nn = 10 to n = 3, to reduce the bias over time.\n• Add weight decay.\n• Add a self-predictive representation loss (Section 4.3.2.2) to increase sample efficiency.\n• Gradually increase the discount factor from γ = 0.97 to γ = 0.997, to encourage longer term planning\nonce the model starts to be trained.3\n• Drop noisy nets (which requires multiple network copies and thus slows down training due to increased\nmemory use), since it does not help.\n• Use dueling DQN (see Section 2.5.4.2).\n• Use distributional DQN (see Section 5.1).\n2.5.4.7\nOther methods\nMany other methods have been proposed to reduce the sample complexity of value-based RL while maintaining\nperformance, see e.g., the MEME paper of [Kap+22].\n3The Agent 57 method of [Bad+20] automatically learns the exploration rate and discount factor using a multi-armed\nbandit stratey, which lets it be more exploratory or more exploitative, depending on the game. This resulted in super human\nperformance on all 57 Atari games in ALE. However, it required 80 billion frames (environment steps)! This was subsequently\nreduced to the “standard” 200M frames in the MEME method of [Kap+22].\n48\nChapter 3\nPolicy-based RL\nIn the previous section, we considered methods that estimate the action-value function, Q(s, a), from which\nwe derive a policy. However, these methods have several disadvantages: (1) they can be difficult to apply to\ncontinuous action spaces; (2) they may diverge if function approximation is used (see Section 2.5.2.4); (3)\nthe training of Q, often based on TD-style updates, is not directly related to the expected return garnered\nby the learned policy; (4) they learn deterministic policies, whereas in stochastic and partially observed\nenvironments, stochastic policies are provably better [JSJ94].\nIn this section, we discuss policy search methods, which directly optimize the parameters of the policy\nso as to maximize its expected return. We mostly focus on policy gradient methods, that use the gradient\nof the loss to guide the search. As we will see, these policy methods often benefit from estimating a value or\nadvantage function to reduce the variance in the policy search process, so we will also use techniques from\nChapter 2. The parametric policy will be denoted by πθ(a|s). For discrete actions, this can be a DNN with a\nsoftmax output. For continuous actions, we can use a Gaussian output layer, or a diffusion policy [Ren+24].\nFor more details on policy gradient methods, see [Wen18b; Leh24].\n3.1\nThe policy gradient theorem\nWe start by defining the objective function for policy learning, and then derive its gradient. The objective,\nwhich we aim to maximize, is defined as\nJ(π) ≜Eπ\n\" ∞\nX\nt=0\nγtRt+1\n#\n(3.1)\n=\n∞\nX\nt=0\nγt X\ns\n X\ns0\np0(s0)pπ(s0 →s, t)\n! X\na\nπ(a|s)R(s, a)\n(3.2)\n=\nX\ns\n X\ns0\n∞\nX\nt=0\nγtp0(s0)pπ(s0 →s, t)\n! X\na\nπ(a|s)R(s, a)\n(3.3)\n=\nX\ns\nρπ(s)\nX\na\nπ(a|s)R(s, a)\n(3.4)\nwhere we have defined the discounted state visitation measure\nργ\nπ(s) ≜\n∞\nX\nt=0\nγt X\ns0\np0(s0)pπ(s0 →s, t)\n|\n{z\n}\npπ\nt (s)\n(3.5)\n49\nwhere pπ(s0 →s, t) is the probability of going from s0 to s in t steps, and pπ\nt (s) is the marginal probability of\nbeing in state s at time t (after each episodic reset). Note that ργ\nπ is a measure of time spent in non-terminal\nstates, but it is not a probability measure, since it is not normalized, i.e., P\ns ργ\nπ(s) ̸= 1. However, we may\nabuse notation and still treat it like a probability, so we can write things like\nEργ\nπ(s) [f(s)] =\nX\ns\nργ\nπ(s)f(s)\n(3.6)\nUsing this notation, we can define the objective as\nJ(π) = Eργ\nπ(s),π(a|s) [R(s, a)]\n(3.7)\nWe can also define a normalized version of the measure ρ by noting that P∞\nt=0 γt =\n1\n1−γ for γ < 1. Hence the\nnormalized discounted state visitation distribution is given by\npγ\nπ(s) = (1 −γ)ργ\nπ(s) = (1 −γ)\n∞\nX\nt=0\nγtpt(s)\n(3.8)\n(Note the change from ρ to p.)\nNote that in [SB18, Sec 13.2], they use slightly different notation. In particular, they assume γ = 1, and\ndefine the non-discounted state visitation measure as η(s) and the corresponding normalized version by µ(s).\nThis is equivalent to ignoring the discount factor γt when defining ρπ(s). This is standard practice in many\nimplementations, since we can just average over (unweighted) trajectories when estimating the objective and\nits gradient, even though it results in a biased estimate [NT20; CVRM23].\nIt can be shown that the gradient of the above objective is given by\n∇θJ(θ) =\nX\ns\nργ\nπ(s)\nX\na\nQπ(s, a)∇θπθ(a|s)\n(3.9)\n=\nX\ns\nργ\nπ(s)\nX\na\nQπθ(s, a)πθ(a|s)∇θ log πθ(a|s)\n(3.10)\n= Eργ\nπ(s)πθ(a|s) [Qπθ(s, a)∇θ log πθ(a|s)]\n(3.11)\nThis is known as the policy gradient theorem [Sut+99]. In statistics, the term ∇θ log πθ(a|s) is called the\n(Fisher) score function1, so sometimes Equation (3.11) is called the score function estimator or SFE\n[Fu15; Moh+20].\n3.2\nREINFORCE\nOne way to apply the policy gradient theorem to optimize a policy is to use stochastic gradient ascent.\nTheoretical results concerning the convergence and sample complexity of such methods can be found in\n[Aga+21a].\nTo implement such a method, let τ = (s0, a0, r0, s1, . . . , sT ) be a trajectory created by sampling from\ns0 ∼p0 and then following πθ. Then we have\n∇θJ(πθ) =\n∞\nX\nt=0\nγtEpt(s)πθ(at|st) [∇θ log πθ(at|st)Qπθ(st, at)]\n(3.12)\n≈\nT −1\nX\nt=0\nγtGt∇θ log πθ(at|st)\n(3.13)\n1This is distinct from the Stein score, which is the gradient wrt the argument of the log probability, ∇a log πθ(a|s), as used\nin diffusion.\n50\nwhere the return is defined as follows\nGt ≜rt + γrt+1 + γ2rt+2 + · · · + γT −t−1rT −1 =\nT −t−1\nX\nk=0\nγkrt+k =\nT −1\nX\nj=t\nγj−trj\n(3.14)\nSee Algorithm 3 for the pseudocode.\nAlgorithm 3: REINFORCE (episodic version)\n1 Initialize policy parameters θ\n2 repeat\n3\nSample an episode τ = (s0, a0, r0, s1, . . . , sT ) using πθ\n4\nfor t = 0, 1, . . . , T −1 do\n5\nGt = PT\nk=t+1 γk−t−1Rk\n6\nθ ←θ + ηθγtGt∇θ log πθ(at|st)\n7 until converged\nIn practice, estimating the policy gradient using Equation (3.11) can have a high variance. A baseline\nfunction b(s) can be used for variance reduction to get\n∇θJ(πθ) = Eρθ(s)πθ(a|s) [∇θ log πθ(a|s)(Qπθ(s, a) −b(s))]\n(3.15)\nAny function that satisfies E [∇θb(s)] = 0 is a valid baseline. This follows since\nX\na\n∇θπθ(a|s)(Q(s, a) −b(s)) = ∇θ\nX\na\nπθ(a|s)Q(s, a) −∇θ[\nX\na\nπθ(a|s)]b(s) = ∇θ\nX\na\nπθ(a|s)Q(s, a) −0\n(3.16)\nA common choice for the baseline is b(s) = Vπθ(s). This is a good choice since Vπθ(s) and Q(s, a) are\ncorrelated and have similar magnitudes, so the scaling factor in front of the gradient term will be small.\nUsing this we get an update of the following form\nθ ←θ + η\nT −1\nX\nt=0\nγt(Gt −b(st))∇θ log πθ(at|st)\n(3.17)\nThis is is called the REINFORCE estimator [Wil92].2 The update equation can be interpreted as follows:\nwe compute the sum of discounted future rewards induced by a trajectory, compared to a baseline, and if\nthis is positive, we increase θ so as to make this trajectory more likely, otherwise we decrease θ. Thus, we\nreinforce good behaviors, and reduce the chances of generating bad ones.\n3.3\nActor-critic methods\nAn actor-critic method [BSA83] uses the policy gradient method, but where the expected return Gt is\nestimated using temporal difference learning of a value function instead of MC rollouts. (The term “actor”\nrefers to the policy, and the term “critic” refers to the value function.) The use of bootstrapping in TD\nupdates allows more efficient learning of the value function compared to MC, and further reduces the variance.\nIn addition, it allows us to develop a fully online, incremental algorithm, that does not need to wait until the\nend of the trajectory before updating the parameters.\n2The term “REINFORCE” is an acronym for “REward Increment = nonnegative Factor x Offset Reinforcement x Characteristic\nEligibility”. The phrase “characteristic eligibility” refers to the ∇log πθ(at|st) term; the phrase “offset reinforcement” refers to\nthe Gt −b(st) term; and the phrase “nonnegative factor” refers to the learning rate η of SGD.\n51\n3.3.1\nAdvantage actor critic (A2C)\nConcretely, consider the use of the one-step TD method to estimate the return in the episodic case, i.e.,\nwe replace Gt with Gt:t+1 = rt + γVw(st+1). If we use Vw(st) as a baseline, the REINFORCE update in\nEquation (3.17) becomes\nθ ←θ + η\nT −1\nX\nt=0\nγt (Gt:t+1 −Vw(st)) ∇θ log πθ(at|st)\n(3.18)\n= θ + η\nT −1\nX\nt=0\nγt\u0000rt + γVw(st+1) −Vw(st)\n\u0001\n∇θ log πθ(at|st)\n(3.19)\nNote that δt = rt+1 + γVw(st+1) −Vw(st) is a single sample approximation to the advantage function\nA(st, at) = Q(st, at) −V (st).\nThis method is therefore called advantage actor critic or A2C. See\nAlgorithm 4 for the pseudo-code.3 (Note that Vw(st+1) = 0 if st is a done state, representing the end of an\nepisode.) Note that this is an on-policy algorithm, where we update the value function V π\nw to reflect the value\nof the current policy π. See Section 3.3.3 for further discussion of this point.\nAlgorithm 4: Advantage actor critic (A2C) algorithm (episodic)\n1 Initialize actor parameters θ, critic parameters w\n2 repeat\n3\nSample starting state s0 of a new episode\n4\nfor t = 0, 1, 2, . . . do\n5\nSample action at ∼πθ(·|st)\n6\n(st+1, rt, donet) = env.step(st, at)\n7\nqt = rt + γ(1 −donet)Vw(st+1) // Target\n8\nδt = qt −Vw(st) // Advantage\n9\nw ←w + ηwδt∇wVw(st) // Critic\n10\nθ ←θ + ηθγtδt∇θ log πθ(at|st) // Actor\n11\nif donet = 1 then\n12\nbreak\n13 until converged\nIn practice, we should use a stop-gradient operator on the target value for the TD update, for reasons\nexplained in Section 2.5.2.4. Furthermore, it is common to add an entropy term to the policy, to act as a\nregularizer (to ensure the policy remains stochastic, which smoothens the loss function — see Section 3.5.4).\nIf we use a shared network with separate value and policy heads, we need to use a single loss function for\ntraining all the parameters ϕ. Thus we get the following loss, for each trajectory, where we want to minimize\nTD loss, maximize the policy gradient (expected reward) term, and maximize the entropy term.\nL(ϕ; τ) = 1\nT\nT\nX\nt=1\n[λT DLT D(st, at, rt, st+1) −λP GJP G(st, at, rt, st+1) −λentJent(st)]\n(3.20)\nqt = rt + γ(1 −done(st))Vϕ(st+1)\n(3.21)\nLT D(st, at, rt, st+1) = (sg(qt) −Vϕ(s))2\n(3.22)\nJP G(st, at, rt, st+1) = (sg(qt −Vϕ(st)) log πϕ(at|st)\n(3.23)\nJent(st) = −\nX\na\nπϕ(a|st) log πϕ(a|st)\n(3.24)\n3In [Mni+16], they proposed a distributed version of A2C known as A3C which stands for “asynchrononous advantage actor\ncritic”.\n52\nTo handle the dynamically varying scales of the different loss functions, we can use the PopArt method of\n[Has+16; Hes+19] to allow for a fixed set of hyper-parameter values for λi. (PopArt stands for “Preserving\nOutputs Precisely, while Adaptively Rescaling Targets”.)\n3.3.2\nGeneralized advantage estimation (GAE)\nIn A2C, we replaced the high variance, but unbiased, MC return Gt with the low variance, but biased,\none-step bootstrap return Gt:t+1 = rt + γVw(st+1). More generally, we can compute the n-step estimate\nGt:t+n = rt + γrt+1 + γ2rt+2 + · · · + γn−1rt+n−1 + γnVw(st+n)\n(3.25)\nand thus obtain the (truncated) n-step advantage estimate as follows:\nA(n)\nw (st, at) = Gt:t+n −Vw(st)\n(3.26)\nUnrolling to infinity, we get\nA(1)\nt\n= rt + γvt+1 −vt\n(3.27)\nA(2)\nt\n= rt + γrt+1 + γ2vt+2 −vt\n(3.28)\n...\n(3.29)\nA(∞)\nt\n= rt + γrt+1 + γ2rt+2 + · · · −vt\n(3.30)\nA(1)\nt\nis high bias but low variance, and A(∞)\nt\nis unbiased but high variance.\nInstead of using a single value of n, we can take a weighted average. That is, we define\nAt =\nPT\nn=1 wnA(n)\nt\nPT\nn=1 wn\n(3.31)\nIf we set wn = λn−1 we get the following simple recursive calculation:\nδt = rt + γvt+1 −vt\n(3.32)\nAt = δt + γλδt+1 + · · · + (γλ)T −t+1δT −1 = δt + γλAt+1\n(3.33)\nHere λ ∈[0, 1] is a parameter that controls the bias-variance tradeoff: larger values decrease the bias but\nincrease the variance. This is called generalized advantage estimation (GAE) [Sch+16b]. See Algorithm 5\nfor some pseudocode. Using this, we can define a general actor-critic method, as shown in Algorithm 6.\nAlgorithm 5: Generalized Advantage Estimation\n1 def GAE(r1:T , v1:T , γ, λ)\n2 A′ = 0\n3 for t = T : 1 do\n4\nδt = rt + γvt+1 −vt\n5\nA′ = δt + γλA′\n6\nAt = A′ // advantage\n7\nqt = At + vt // TD target\n8 Return (A1:T ), q1:T )\nWe can generalize this approach even further, by using gradient estimators of the form\n∇J(θ) = E\n\" ∞\nX\nt=0\nΨt∇log πθ(at|st)\n#\n(3.34)\n53\nAlgorithm 6: Actor critic with GAE\n1 Initialize parameters ϕ, environment state s\n2 repeat\n3\n(s1, a1, r1, . . . , sT ) = rollout(s, πϕ)\n4\nv1:T = Vϕ(s1:T )\n5\n(A1:T , q1:T ) = sg(GAE(r1:T , v1:T , γ, λ))\n6\nL(ϕ) = 1\nT\nPT\nt=1\n\u0002\nλT D(Vϕ(st) −qt)2 −λP GAt log πϕ(at|st) −λent H(πϕ(·|st))\n\u0003\n7\nϕ := ϕ −η∇L(ϕ)\n8 until converged\nwhere Ψt may be any of the following:\nΨt =\n∞\nX\ni=t\nγiri\nMonte Carlo target\n(3.35)\nΨt =\n∞\nX\ni=t\nγiri −Vw(st)\nMC with baseline\n(3.36)\nΨt = Aw(st, at)\nadvantage function\n(3.37)\nΨt = Qw(st, at)\nQ function\n(3.38)\nΨt = rt + Vw(st+1) −Vw(st)\nTD residual\n(3.39)\nSee [Sch+16b] for details.\n3.3.3\nTwo-time scale actor critic algorithms\nIn standard AC, we update the actor and critic in parallel. However, it is better to let critic Vw learn using a\nfaster learning rate (or more updates), so that it reflects the value of the current policy πθ more accurately,\nin order to get better gradient estimates for the policy update. This is known as two timescale learning or\nbilevel optimization [Yu17; Zha+19; Hon+23; Zhe+22; Lor24]. (See also Section 4.2.1, where we discuss\nRL from a game theoretic perspective.)\n3.3.4\nNatural policy gradient methods\nIn this section, we discuss an improvement to policy gradient methods that uses preconditioning to speedup\nconvergence. In particular, we replace gradient descent with natural gradient descent (NGD) [Ama98;\nMar20], which we explain below. We then show how to combine it with actor-critic.\n3.3.4.1\nNatural gradient descent\nNGD is a second order method for optimizing the parameters of (conditional) probability distributions, such\nas policies, πθ(a|s). It typically converges faster and more robustly than SGD, but is computationally more\nexpensive.\nBefore we explain NGD, let us review standard SGD, which is an update of the following form\nθk+1 = θk −ηkgk\n(3.40)\nwhere gk = ∇θL(θk) is the gradient of the loss at the previous parameter values, and ηk is the learning rate.\nIt can be shown that the above update is equivalent to minimizing a locally linear approximation to the loss,\nˆLk, subject to the constraint that the new parameters do not move too far (in Euclidean distance) from the\n54\n(a)\n(b)\nFigure 3.1: Changing the mean of a Gaussian by a fixed amount (from solid to dotted curve) can have more impact\nwhen the (shared) variance is small (as in a) compared to when the variance is large (as in b). Hence the impact (in\nterms of prediction accuracy) of a change to µ depends on where the optimizer is in (µ, σ) space. From Figure 3 of\n[Hon+10], reproduced from [Val00]. Used with kind permission of Antti Honkela.\nprevious parameters:\nθk+1 = argmin\nθ\nˆLk(θ) s.t. ||θ −θk||2\n2 ≤ϵ\n(3.41)\nˆLk(θ) = L(θk) + gT\nk(θ −θk)\n(3.42)\nwhere the step size ηk is proportional to ϵ. This is called a proximal update [PB+14].\nOne problem with the SGD update is that Euclidean distance in parameter space does not make sense for\nprobabilistic models. For example, consider comparing two Gaussians, pθ = p(y|µ, σ) and pθ′ = p(y|µ′, σ′).\nThe (squared) Euclidean distance between the parameter vectors decomposes as ||θ−θ′||2\n2 = (µ−µ′)2+(σ−σ′)2.\nHowever, the predictive distribution has the form exp(−1\n2σ2 (y −µ)2), so changes in µ need to be measured\nrelative to σ. This is illustrated in Figure 3.1(a-b), which shows two univariate Gaussian distributions (dotted\nand solid lines) whose means differ by ϵ. In Figure 3.1(a), they share the same small variance σ2, whereas in\nFigure 3.1(b), they share the same large variance. It is clear that the difference in µ matters much more (in\nterms of the effect on the distribution) when the variance is small. Thus we see that the two parameters\ninteract with each other, which the Euclidean distance cannot capture.\nThe key to NGD is to measure the notion of distance between two probability distributions in terms\nof the KL divergence. This can be approximated in terms of the Fisher information matrix (FIM). In\nparticular, for any given input x, we have\nDKL (pθ(y|x) ∥pθ+δ(y|x)) ≈1\n2δTFxδ\n(3.43)\nwhere Fx is the FIM\nFx(θ) = −Epθ(y|x)\n\u0002\n∇2 log pθ(y|x)\n\u0003\n= Epθ(y|x)\n\u0002\n(∇log pθ(y|x))(∇log pθ(y|x))T\u0003\n(3.44)\nWe now replace the Euclidean distance between the parameters, d(θk, θk+1) = ||δ||2\n2, with\nd(θk, θk+1) = δTFkδk\n(3.45)\nwhere δ = θk+1 −θk and Fk = Fx(θk) for a randomly chosen input x. This gives rise to the following\nconstrained optimization problem:\nδk = argmin\nδ\nˆLk(θk + δ) s.t. δTFkδ ≤ϵ\n(3.46)\nIf we replace the constraint with a Lagrange multiplier, we get the unconstrained objective:\nJk(δ) = L(θk) + gT\nkδ + ηkδTFkδ\n(3.47)\n55\nSolving Jk(δ) = 0 gives the update\nδ = −ηkF−1\nk gk\n(3.48)\nThe term F−1g is called the natural gradient. This is equivalent to a preconditioned gradient update,\nwhere we use the inverse FIM as a preconditioning matrix. We can compute the (adaptive) learning rate\nusing\nηk =\nr\nϵ\ngT\nkF−1\nk gk\n(3.49)\nComputing the FIM can be hard. A simple approximation is to replace the model’s distribution with the\nempirical distribution. In particular, define pD(x, y) = 1\nN\nPN\nn=1 δxn(x)δyn(y), pD(x) = 1\nN\nPN\nn=1 δxn(x) and\npθ(x, y) = pD(x)p(y|x, θ). Then we can compute the empirical Fisher [Mar16] as follows:\nF(θ) = Epθ(x,y)\n\u0002\n∇log p(y|x, θ)∇log p(y|x, θ)T\u0003\n(3.50)\n≈EpD(x,y)\n\u0002\n∇log p(y|x, θ)∇log p(y|x, θ)T\u0003\n(3.51)\n=\n1\n|D|\nX\n(x,y)∈D\n∇log p(y|x, θ)∇log p(y|x, θ)T\n(3.52)\n3.3.4.2\nNatural actor critic\nTo apply NGD to RL, we can adapt the A2C algorithm in Algorithm 6. In particular, define\ngkt = ∇θkAt log πθ(at|st)\n(3.53)\nwhere At is the advantage function at step t of the random trajectory generated by the policy at iteration k.\nNow we compute\ngk = 1\nT\nT\nX\nt=1\ngkt, Fk = 1\nT\nT\nX\nt=1\ngktgT\nkt\n(3.54)\nand compute δk+1 = −ηkF−1\nk gk. This approach is called natural policy gradient [Kak01; Raj+17].\nWe can compute F−1\nk gk without having to invert Fk by using the conjugate gradient method, where\neach CG step uses efficient methods for Hessian-vector products [Pea94]. This is called Hessian free\noptimization [Mar10]. Similarly, we can efficiently compute gT\nk(F−1\nk gk).\nAs a more accurate alternative to the empirical Fisher, [MG15] propose the KFAC method, which stands\nfor “Kronecker factored approximate curvature”; this approximates the FIM of a DNN as a block diagonal\nmatrix, where each block is a Kronecker product of two small matrices. This was applied to policy gradient\nlearning in [Wu+17].\n3.4\nPolicy improvement methods\nIn this section, we discuss methods that try to monotonically improve performance of the policy at each step,\nrather than just following the gradient, which can result in a high variance estimate where performance can\nincrease or decrease at each step. These are called policy improvement methods. Our presentation is\nbased on [QPC24].\n3.4.1\nPolicy improvement lower bound\nWe start by stating a useful result from [Ach+17]. Let πk be the current policy at step k, and let π be any\nother policy (e.g., a candidate new one). Let pγ\nπk be the normalized discounted state visitation distribution\nfor πk, defined in Equation (3.8). Let Aπk(s, a) = Qπk(s, a) −V πk(s) be the advantage function. Finally, let\nthe total variation distance between two distributions be given by\nTV(p, q) ≜1\n2||p −q||1 = 1\n2\nX\ns\n|p(s) −q(s)|\n(3.55)\n56\nThen one can show [Ach+17] that\nJ(π) −J(πk) ≥\n1\n1 −γ Epγ\nπk (s)πk(a|s)\n\u0014 π(a|s)\nπk(a|s)Aπk(s, a)\n\u0015\n|\n{z\n}\nL(π,πk)\n−2γCπ,πk\n(1 −γ)2 Epγ\nπk (s) [TV(π(·|s), πk(·|s))]\n(3.56)\nwhere Cπ,πk = maxs |Eπ(a|s) [Aπk(s, a)] |. In the above, L(π, πk) is a surrogate objective, and the second term\nis a penalty term.\nIf we can optimize this lower bound (or a stochastic approximation, based on samples from the current\npolicy πk), we can guarantee monotonic policy improvement (in expectation) at each step. We will replace\nthis objective with a trust-region update that is easier to optimize:\nπk+1 = argmax\nπ\nL(π, πk) s.t. Epγ\nπk (s) [TV(π, πk)(s)] ≤ϵ\n(3.57)\nThe constraint bounds the worst-case performance decline at each update. The overall procedure becomes\nan approximate policy improvement method. There are various ways of implementing the above method in\npractice, some of which we discuss below. (See also [GDWF22], who propose a framework called mirror\nlearning, that justifies these “approximations” as in fact being the optimal thing to do for a different kind of\nobjective.)\n3.4.2\nTrust region policy optimization (TRPO)\nIn this section, we describe the trust region policy optimization (TRPO) method of [Sch+15b]. This\nimplements an approximation to Equation (3.57). First, it leverages the fact that if\nEpγ\nπk (s) [DKL (πk ∥π) (s)] ≤δ\n(3.58)\nthen π also satisfies the TV constraint with δ = ϵ2\n2 . Next it considers a first-order expansion of the surrogate\nobjective to get\nL(π, πk) = Epγ\nπk (s)πk(a|s)\n\u0014 π(a|s)\nπk(a|s)Aπk(s, a)\n\u0015\n≈gT\nk(θ −θk)\n(3.59)\nwhere gk = ∇θL(πθ, πk)|θk.\nFinally it considers a second-order expansion of the KL term to get the\napproximate constraint\nEpγ\nπk (s) [DKL (πk ∥π) (s)] ≈1\n2(θ −θk)TFk(θ −θk)\n(3.60)\nwhere Fk = gkgT\nk is an approximation to the Fisher information matrix (see Equation (3.54)). We then use\nthe update\nθk+1 = θk + ηkvk\n(3.61)\nwhere vk = F−1\nk gk is the natural gradient, and the step size is initialized to ηk =\nq\n2δ\nvT\nkFkvk . (In practice we\ncompute vk by approximately solving the linear system Fkv = gk using conjugate gradient methods, which\njust require matrix vector multiplies.) We then use a backtracking line search procedure to ensure the trust\nregion is satisfied.\n3.4.3\nProximal Policy Optimization (PPO)\nIn this section, we describe the the proximal policy optimization or PPO method of [Sch+17], which is a\nsimplification of TRPO.\nWe start by noting the following result:\nEpγ\nπk (s) [TV(π, πk)(s)] = 1\n2E(s,a)∼pγ\nπk\n\u0014\n| π(a|s)\nπk(a|s) −1|\n\u0015\n(3.62)\n57\nThis holds provided the support of π is contained in the support of πk at every state. We then use the\nfollowing update:\nπk+1 = argmax\nπ\nE(s,a)∼pγ\nπk [min (ρk(s, a)Aπk(s, a), ˜ρk(s, a)Aπk(s, a))]\n(3.63)\nwhere ρk(s, a) =\nπ(a|s)\nπk(a|s) is the likelihood ratio, and ˜ρk(s, a) = clip(ρk(s, a), 1 −ϵ, 1 + ϵ), where clip(x, l, u) =\nmin(max(x, l), u).\nSee [GDWF22] for a theoretical justification for these simplifications. Furthermore,\nthis can be modified to ensure monotonic improvement as discussed in [WHT19], making it a true bound\noptimization method.\nSome pseudocode for PPO (with GAE) is given in Algorithm 7. It is basically identical to the AC code in\nAlgorithm 6, except the policy loss has the form min(ρtAt, ˜ρtAt) instead of At log πϕ(at|st), and we perform\nmultiple policy updates per rollout, for increased sample efficiency. For all the implementation details, see\nhttps://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/.\nAlgorithm 7: PPO with GAE\n1 Initialize parameters ϕ, environment state s\n2 for iteration k = 1, 2, . . . do\n3\n(τ, s) = rollout(s, πϕ)\n4\n(s1, a1, r1, . . . , sT ) = τ\n5\nvt = Vϕ(st) for t = 1 : T\n6\n(A1:T , q1:T ) = GAE(r1:T , v1:T , γ, λ)\n7\nϕold ←ϕ\n8\nfor m = 1 : M do\n9\nρt =\nπϕ(at|st)\nπϕold(at|st) for t = 1 : T\n10\n˜ρt = clip(ρt) for t = 1 : T\n11\nL(ϕ) = 1\nT\nPT\nt=1\n\u0002\nλT D(Vϕ(st) −qt)2 −λP G min(ρtAt, ˜ρtAt) −λent H(πϕ(·|st))\n\u0003\n12\nϕ := ϕ −η∇ϕL(ϕ)\n3.4.4\nVMPO\nIn this section, we discuss the VMPO algorithm of [FS+19], which is an on-policy extenson of the earlier\non-policy MPO algorithm (MAP policy optimization) from [Abd+18]. It was originally explained in terms of\n“control as inference” (see Section 1.5), but we can also view it as a contrained policy improvement method,\nbased on Equation (3.57). In particular, VMPO leverages the fact that if\nEpγ\nπk (s) [DKL (π ∥πk) (s)] ≤δ\n(3.64)\nthen π also satisfies the TV constraint with δ = ϵ2\n2 .\nNote that here the KL is reversed compared to TRPO in Section 3.4.2. This new version will encourage\nπ to be mode-covering, so it will naturally have high entropy, which can result in improved robustness.\nUnfortunately, this kind of KL is harder to compute, since we are taking expectations wrt the unknown\ndistribution π.\nTo solve this problem, VMPO adopts an EM-type approach. In the E step, we compute a non-parametric\nversion of the state-action distribution given by the unknown new policy:\nψ(s, a) = π(a|s)pγ\nπk(s)\n(3.65)\nThe optimal new distribution is given by\nψk+1 = argmax\nψ\nEψ(s,a) [Aπk(s, a)]\ns.t. DKL (ψ ∥ψk) ≤δ\n(3.66)\n58\nwhere ψk(s, a) = πk(a|s)pγ\nπk(s). The solution to this is\nψk+1(s, a) = pγ\nπk(s)πk(a|s)w(s, a)\n(3.67)\nw(s, a) = exp(Aπk(s, a)/λ∗)\nZ(λ∗)\n(3.68)\nZ(λ) = E(s,a)∼pγ\nπk [exp(Aπk(s, a)/λ)]\n(3.69)\nλ∗= argmin\nλ≥0\nλδ + λ log Z(λ)\n(3.70)\nIn the M step, we project this target distribution back onto the space of parametric policies, while satisfying\nthe KL trust region constraint:\nπk+1 = argmax\nπ\nE(s,a)∼pγ\nπk [w(s, a) log π(a|s)]\ns.t. Epγ\nπk [DKL (ψk ∥ψ) (s)] ≤δ\n(3.71)\n3.5\nOff-policy methods\nIn many cases, it is useful to train a policy using data collected from a distinct behavior policy πb(a|s) that\nis not the same as the target policy π(a|s) that is being learned. For example, this could be data collected\nfrom earlier trials or parallel workers (with different parameters θ′) and stored in a replay buffer, or it\ncould be demonstration data from human experts. This is known as off-policy RL, and can be much\nmore sample efficient than the on-policy methods we have discussed so far, since these methods can use data\nfrom multiple sources. However, off-policy methods are more complicated, as we will explain below.\nThe basic difficulty is that the target policy that we want to learn may want to try an action in a\nstate that has not been experienced before in the existing data, so there is no way to predict the outcome\nof this new (s, a) pair. In this section, we tackle this problem by assuming that the target policy is not\ntoo different from the behavior policy, so that the ratio π(a|s)/πb(a|s) is bounded, which allows us to use\nmethods based on importance sampling. In the online learning setting, we can ensure this property by using\nconservative incremental updates to the policy. Alternatively we can use policy gradient methods with various\nregularization methods, as we discuss below.\nIn Section 5.5, we discuss offline RL, which is an extreme instance of off-policy RL where we have a fixed\nbehavioral dataset, possibly generated from an unknown behavior policy, and can never collect any new data.\n3.5.1\nPolicy evaluation using importance sampling\nAssume we have a dataset of the form D = {τ (i)}1≤i≤n, where each trajectory is a sequence τ (i) =\n(s(i)\n0 , a(i)\n0 , r(i)\n0 , s(i)\n1 . . .), where the actions are sampled according to a behavior policy πb, and the reward and\nnext states are sampled according to the reward and transition models. We want to use this offline dataset to\nevaluate the performance of some target policy π; this is called off-policy policy evaluation or OPE. If\nthe trajectories τ (i) were sampled from π. we could use the standard Monte Carlo estimate:\nˆJ(π) ≜1\nn\nn\nX\ni=1\nT −1\nX\nt=0\nγtr(i)\nt\n(3.72)\nHowever, since the trajectories are sampled from πb, we use importance sampling (IS) to correct for the\ndistributional mismatch, as first proposed in [PSS00]. This gives\nˆJIS(π) ≜1\nn\nn\nX\ni=1\np(τ (i)|π)\np(τ (i)|πb)\nT −1\nX\nt=0\nγtr(i)\nt\n(3.73)\nIt can be verified that Eπb\nh\nˆJIS(π)\ni\n= J(π), that is, ˆJIS(π) is unbiased, provided that p(τ|πb) > 0 whenever\np(τ|π) > 0. The importance ratio,\np(τ (i)|π)\np(τ (i)|πb), is used to compensate for the fact that the data is sampled\n59\nfrom πb and not π. It can be simplified as follows:\np(τ|π)\np(τ|πb) = p(s0) QT −1\nt=0 π(at|st)pS(st+1|st, at)pR(rt|st, at, st+1)\np(s0) QT −1\nt=0 πb(at|st)pS(st+1|st, at)pR(rt|st, at, st+1)\n=\nT −1\nY\nt=0\nπ(at|st)\nπb(at|st)\n(3.74)\nThis simplification makes it easy to apply IS, as long as the target and behavior policies are known. (If the\nbehavior policy is unknown, we can estimate it from D, and replace πb by its estimate ˆπb. For convenience,\ndefine the per-step importance ratio at time t by\nρt(τ) ≜π(at|st)/πb(at|st)\n(3.75)\nWe can reduce the variance of the estimator by noting that the reward rt is independent of the trajectory\nbeyond time t. This leads to a per-decision importance sampling variant:\nˆJPDIS(π) ≜1\nn\nn\nX\ni=1\nT −1\nX\nt=0\nY\nt′≤t\nρt′(τ (i))γtr(i)\nt\n(3.76)\n3.5.2\nOff-policy actor critic methods\nIn this section, we discuss how to extend actor-critic methods to work with off-policy data.\n3.5.2.1\nLearning the critic using V-trace\nIn this section we build on Section 3.5.1 to develop a practical method, known as V-trace [Esp+18], to\nestimate the value function for a target policy using off-policy data. (This is an extension of the earlier\nRetrace algorithm [Mun+16], which estimates the Q function using off-policy data.)\nFirst consider the n-step target value for V (si) in the on-policy case:\nVi = V (si) +\ni+n−1\nX\nt=i\nγt−irt + γnV (si+n)\n(3.77)\n= V (si) +\ni+n−1\nX\nt=i\nγt−i (rt + γV (st+1) −V (st))\n|\n{z\n}\nδt\n(3.78)\nwhere we define δt = (rt + γV (st+1) −V (st)) as the TD error at time t. To extend this to the off-policy case,\nwe use the per-step importance ratio trick. However, to bound the variance of the estimator, we truncate the\nIS weights. In particular, we define\nct = min\n\u0012\nc, π(at|st)\nπb(at|st)\n\u0013\n, ρt = min\n\u0012\nρ, π(at|st)\nπb(at|st)\n\u0013\n(3.79)\nwhere c and ρ are hyperparameters. We then define the V-trace target value for V (si) as\nvi = V (si) +\ni+n−1\nX\nt=i\nγt−i\n t−1\nY\nt′=i\nct′\n!\nρtδt\n(3.80)\nNote that we can compute these targets recursively using\nvi = V (si) + ρiδi + γci(vi+1 −V (si+1))\n(3.81)\nThe product of the weights ci . . . ct−1 (known as the “trace”) measures how much a temporal difference δt\nat time t impacts the update of the value function at earlier time i. If the policies are very different, the\n60\nvariance of this product will be large. So the truncation parameter c is used to reduce the variance. In\n[Esp+18], they find c = 1 works best.\nThe use of the target ρtδt rather than δt means we are evaluating the value function for a policy that is\nsomewhere between πb and π. For ρ = ∞(i.e., no truncation), we converge to the value function V π, and for\nρ →0, we converge to the value function V πb. In [Esp+18], they find ρ = 1 works best.\nNote that if c = ρ, then ci = ρi. This gives rise to the simplified form\nvt = V (st) +\nn−1\nX\nj=0\nγj\n \njY\nm=0\nct+m\n!\nδt+j\n(3.82)\nWe can use the above V-trace targets to learn an approximate value function by minimizing the usual ℓ2\nloss:\nL(w) = Et∼D\n\u0002\n(vt −Vw(st))2\u0003\n(3.83)\nthe gradient of which has the form\n∇L(w) = 2Et∼D [(vt −Vw(st))∇wVw(st)]\n(3.84)\n3.5.2.2\nLearning the actor\nWe now discuss how to update the actor using an off-policy estimate of the policy gradient. We start by\ndefining the objective to be the expected value of the new policy, where the states are drawn from the\nbehavior policy’s state distribution, but the actions are drawn from the target policy:\nJπb(πθ) =\nX\ns\npγ\nπb(s)Vπ(s) =\nX\ns\npγ\nπb(s)\nX\na\nπθ(a|s)Qπ(s, a)\n(3.85)\nDifferentiating this and ignoring the term ∇θQπ(s, a), as suggested by [DWS12], gives a way to (approximately)\nestimate the off-policy policy-gradient using a one-step IS correction ratio:\n∇θJπb(πθ) ≈\nX\ns\nX\na\npγ\nπb(s)∇θπθ(a|s)Qπ(s, a)\n(3.86)\n= Epγ\nπb(s),πb(a|s)\n\u0014πθ(a|s)\nπb(a|s) ∇θ log πθ(a|s)Qπ(s, a)\n\u0015\n(3.87)\nIn practice, we can approximate Qπ(st, at) by qt = rt + γvt+1, where vt+1 is the V-trace estimate for state\nst+1. If we use V (st) as a baseline, to reduce the variance, we get the following gradient estimate for the\npolicy:\n∇J(θ) = Et∼D [ρt∇θ log πθ(at|st)(rt + γvt −Vw(st))]\n(3.88)\nWe can also replace the 1-step IS-weighted TD error ρt(rt + γvt −Vw(st)) with an IS-weighted GAE value\nby modifying the generalized advantage estimation method in Section 3.3.2. In particular, we just need to\ndefine λt = λ min(1, ρt). We denote the IS-weighted GAE estimate as Aρ\nt .4\n3.5.2.3\nIMPALA\nAs an example of an off-policy AC method, we consider IMPALA, which stands for “Importance Weighted\nActor-Learning Architecture”. [Esp+18]. This uses shared parameters for the policy and value function (with\ndifferent output heads), and adds an entropy bonus to ensure the policy remains stochastic. Thus we end up\nwith the following objective, which is very similar to on-policy actor-critic shown in Algorithm 6:\nL(ϕ) = Et∼D\n\u0002\nλT D(Vϕ(st) −vt)2 −λP GAρ\nt log πϕ(at|st) −λent H(πϕ(·|st))\n\u0003\n(3.89)\n4For an implementation, see https://github.com/google-deepmind/rlax/blob/master/rlax/_src/multistep.py#L39\n61\nThe only difference from standard A2C is that we need to store the probabilities of each action, πb(at|st), in\naddition to (st, at, rt, st+1) in the dataset D, which can be used to compute ρt. [Esp+18] was able to use this\nmethod to train a single agent (using a shared CNN and LSTM for both value and policy) to play all 57\ngames at a high level. Furthermore, they showed that their method — thanks to its off-policy corrections —\noutperformed the A3C method (a parallel version of A2C) in Section 3.3.1.\n3.5.3\nOff-policy policy improvement methods\nSo far we have focused on actor-critic methods. However, policy improvement methods, such as PPO, are\noften preferred to AC methods, since they monotonically improve the objective. In [QPC21] they propose\none way to extend PPO to the off-policy case. This method was generalized in [QPC24] to cover a variety of\npolicy improvement algorithms, including TRPO and VMPO. We give a brief summary below.\nThe key insight is to realize that we can generalize the lower bound in Equation (3.56) to any reference\npolicy\nJ(π) −J(πk) ≥\n1\n1 −γ Epγ\nπref(s)πk(a|s)\n\u0014 π(a|s)\nπref(a|s)Aπk(s, a)\n\u0015\n−2γCπ,πk\n(1 −γ)2 Epγ\nπref(s) [TV(π(·|s), πref(·|s))]\n(3.90)\nThe reference policy can be any previous policy, or a convex combination of them. In particular, if πk is the\ncurrent policy, we can consider the reference policy to be πref = PM\ni=1 νiπk−i, where 0 ≤νi ≤1 and P\ni νi = 1\nare mixture weights. We can approximate the expectation by sampling from the replay buffer, which contains\nsamples from older policies. That is, (s, a) ∼pγ\nπref can be implemented by i ∼ν and (s, a) ∼pγ\nπk−i.\nTo compute the advantage function Aπk from off policy data, we can adapt the V-trace method of\nEquation (3.82) to get\nAπk\ntrace(st, at) = δt +\nn−1\nX\nj=0\nγj\n \njY\nm=1\nct+m\n!\nδt+j\n(3.91)\nwhere δt = rt + γV (st+1) −V (st), and ct = min\n\u0010\nc,\nπk(at|st)\nπk−i(at|st)\n\u0011\nis the truncated importance sampling ratio.\nTo compute the TV penalty term from off policy data, we need to choose between the PPO (Section 3.4.3),\nVMPO (Section 3.4.4) and TRPO (Section 3.4.2) approach. We discuss each of these cases below.\n3.5.3.1\nOff-policy PPO\nThe simplest is to use off-policy PPO, which gives an update of the following form (known as Generalized\nPPO):\nπk+1 = argmax\nπ\nEi∼ν\nh\nE(s,a)∼pγ\nπk−i [min(ρk−i(s, a)Aπk(s, a), ˜ρk−i(s, a)Aπk(s, a))]\ni\n(3.92)\nwhere ρk−i(s, a) =\nπ(a|s)\nπk−i(a|s) and ˜ρk−i(s, a) = clip(\nπ(a|s)\nπk−i(a|s), l, u), where l =\nπk(a|s)\nπk−i(a|s) −ϵ and u =\nπk(a|s)\nπk−i(a|s) + ϵ.\n(For other off-policy variants of PPO, see e.g., [Men+23; LMW24].)\n3.5.3.2\nOff-policy VMPO\nFor an off-policy version of VMPO, see the original MPO method of [Abd+18]; this is derived using an EM\nframework, but EM is just another bound optimization algorithm [HL04], and the result is equivalent to the\nversion presented in [QPC24].\n3.5.3.3\nOff-policy TRPO\nFor details on the off-policy version of TRPO, see [QPC24].\n62\n3.5.4\nSoft actor-critic (SAC)\nThe soft actor-critic (SAC) algorithm [Haa+18a; Haa+18b] is an off-policy actor-critic method based on a\nframework known as maximum entropy RL, which we introduced in Section 1.5.3. Crucially, even though\nSAC is off-policy and utilizes a replay buffer to sample past experiences, the policy update is done using\nthe actor’s own probability distribution, eliminating the need to use importance sampling to correct for\ndiscrepancies between the behavior policy (used to collect data) and the target policy (used for updating), as\nwe will see below.\nWe start by slightly rewriting the maxent RL objective from Equation (1.67) using modified notation:\nJSAC(θ) ≜Epγ\nπθ (s)πθ(a|s) [R(s, a) + α H(πθ(·|s))]\n(3.93)\nNote that the entropy term makes the objective easier to optimize, and encourages exploration.\nTo optimize this, we can perform a soft policy evaluation step, and then a soft policy improvement step.\nIn the policy evaluation step, we can repeatedly apply a modified Bellman backup operator T π defined as\nT πQ(st, at) = r(st, at) + γEst+1∼p [V (st+1)]\n(3.94)\nwhere\nV (st) = Eat∼π [Q(st, at) −α log π(at|st)]\n(3.95)\nis the soft value function. If we iterate Qk+1 = T πQk„ this will converge to the soft Q function for π.\nIn the policy improvement step, we derive the new policy based on the soft Q function by softmaxing over\nthe possible actions for each state. We then project the update back on to the policy class Π:\nπnew = arg min\nπ′∈Π DKL\n\u0012\nπ′(·|st) ∥exp( 1\nαQπold(st, ·))\nZπold(st)\n\u0013\n(3.96)\n(The partition function Zπold(st) may be intractable to compute for a continuous action space, but it cancels\nout when we take the derivative of the objective, so this is not a problem, as we show below.) After solving\nthe above optimization problem, we are guaranteed to satisfy the soft policy improvement theorem, i.e.,\nQπnew(st, at) ≥Qπold(st, at) for all st and at.\nThe above equations are intractable in the non-tabular case, so we now extend to the setting where we\nuse function approximation.\n3.5.4.1\nPolicy evaluation\nFor policy evaluation, we hold the policy parameters π fixed and optimize the parameters w of the Q function\nby minimizing the soft Bellman residual\nJQ(w) = E(st,at,rt+1,st+1)∼D\n\u00141\n2 (Qw(st, at) −q(rt+1, st+1))2\n\u0015\n(3.97)\nwhere D is a replay buffer,\nq(rt+1, st+1) = rt+1 + γVw(st+1)\n(3.98)\nis the frozen target value, and and Vw(s) is a frozen version of the soft value function from Equation (3.95):\nVw(st) = Eπ(at|st) [Qw(st, at) −α log π(at|st)]\n(3.99)\nwhere w is the EMA version of w. (The use of a frozen target is to avoid bootstrapping instablilities discussed\nin Section 2.5.2.4.)\nTo avoid the positive overestimation bias that can occur with actor-critic methods, [Haa+18a], suggest\nfitting two soft Q functions, by optimizing JQ(wi), for i = 1, 2, independently. Inspired by clipped double Q\nlearning, used in TD3 (Section 3.6.2), the targets are defined as\nq(rt+1, st+1; w1:2, θ) = rt+1 + γ\n\u0014\nmin\ni=1,2 Qwi(st+1, ˜at+1) −α log πθ(˜at+1|st+1)\n\u0015\n(3.100)\n63\nwhere ˜at+1 ∼πθ(st+1) is a sampled next action. In [Che+20], they propose the REDQ method (Section 2.5.3.3)\nwhich uses a random ensemble of N ≥2 networks instead of just 2.\n3.5.4.2\nPolicy improvement: Gaussian policy\nFor policy improvement, we hold the value function parameters w fixed and optimize the parameters θ of\nthe policy by minimizing the objective below, which is derived from the KL term by multiplying by α and\ndropping the constant Z term:\nJπ(θ) = Est∼D [Eat∼πθ [α log πθ(at|st) −Qw(st, at)]]\n(3.101)\nSince we are taking gradients wrt θ, which affects the inner expectation term, we need to either use the\nREINFORCE estimator from Equation (3.15) or the reparameterization trick (see e.g., [Moh+20]). The\nlatter is much lower variance, so is preferable.\nTo explain this in more detail, let us assume the policy distribution has the form πθ(at|st) = N(µθ(st), σ2I).\nWe can write the random action as at = fθ(st, ϵt), where f is a deterministic function of the state and a\nnoise variable ϵt, since at = µ(st) + σ2ϵt, where ϵt ∼N(0, I). The objective now becomes\nJπ(θ) = Est∼D,ϵt∼N [α log πθ(fθ(st, ϵt)|st) −Qw(st, fθ(st, ϵt))]\n(3.102)\nwhere we have replaced the expectation of at wrt πθ with an expectation of ϵt wrt its noise distribution\nN. Hence we can now safely take stochastic gradients. See Algorithm 8 for the pseudocode. (Note that,\nfor discrete actions, we can avoid the need for the reparameterization trick by computing the expectations\nexplicitly, as discussed in Section 3.5.4.3.)\n3.5.4.3\nPolicy improvement: softmax policy\nFor discrete actions, we can replace the Gaussian reparameterization with the gumbel-softmax reparameter-\nization [JGP16; MMT17]. Alternatively, we can eschew sampling and compute the expectations over the\nactions explicitly, to derive lower variance versions of the equations; this is known as SAC-Discrete [Chr19].\nThe Jπ objective can now be computed as\nJ′\nπ(θ) = Est∼D\n\"X\na\nπθ(a|st)[α log πθ(a|st) −Qw(st, a)]\n#\n(3.103)\nwhich avoids the need for reparameterization. (In [Zho+22], they propose to augment J′\nπ with an entropy\npenalty, adding a term of the form 1\n2(Hold −Hπ)2, to prevent drastic changes in the policy, where the entropy\nof the policy can be computed analytically per sampled state.) The JQ term is similar to before\nJ′\nQ(w) = E(st,at,rt+1,st+1)∼D\n\u00141\n2 (Qw(st, at) −q′(rt+1, st+1))2)\n\u0015\n(3.104)\nwhere now the frozen target function is given by\nq′(rt+1, st+1) = rt+1 + γ\n\nX\nat+1\nπθ(at+1|st+1)[min\ni=1,2 Qwi(st+1, at+1) −α log πθ(at+1|st+1)]\n\n\n(3.105)\n3.5.4.4\nAdjusting the temperature\nIn [Haa+18b] they propose to automatically adjust the temperature parameter α by optimizing\nJ(α) = Est∼D,at∼πθ\n\u0002\n−α(log πθ(at|st) + H)\n\u0003\nwhere H is the target entropy (a hyper-parameters). This objective is approximated by sampling actions\nfrom the replay buffer.\n64\nAlgorithm 8: SAC\n1 Initialize environment state s, policy parameters θ, N critic parameters wi, target parameters\nwi = wi, replay buffer D = ∅, discount factor γ, EMA rate ρ, step size ηw, ηπ.\n2 repeat\n3\nTake action a ∼πθ(·|s)\n4\n(s′, r) = step(a, s)\n5\nD := D ∪{(s, a, r, s′)}\n6\ns ←s′\n7\nfor G updates do\n8\nSample a minibatch B = {(sj, aj, rj, s′\nj)} from D\n9\nw = update-critics(θ, w, B)\n10\nSample a minibatch B = {(sj, aj, rj, s′\nj)} from D\n11\nθ = update-policy(θ, w, B)\n12 until converged\n13 .\n14 def update-critics(θ, w, B):\n15 Let (sj, aj, rj, s′\nj)B\nj=1 = B\n16 qj = q(rj, s′\nj; w1:N, θ) for j = 1 : B\n17 for i = 1 : N do\n18\nL(wi) =\n1\n|B|\nP\n(s,a,r,s′)j∈B(Qwi(sj, aj) −sg(qj))2\n19\nwi ←wi −ηw∇L(wi) // Descent\n20\nwi := ρwi + (1 −ρ)wi //Update target networks\n21 Return w1:N, w1:N\n22 .\n23 def update-actor(θ, w, B):\n24 ˆQ(s, a) ≜1\nN\nPN\ni=1 Qwi(s, a) // Average critic\n25 J(θ) =\n1\n|B|\nP\ns∈B\n\u0010\nˆQ(s, ˜aθ(s)) −α log πθ(˜a(s)|s)\n\u0011\n, ˜aθ(s) ∼πθ(·|s)\n26 θ ←θ + ηθ∇J(θ) // Ascent\n27 Return θ\n65\nFor discrete actions, temperature objective is given by\nJ′(α) = Est∼D\n\"X\na\nπt(a|st)[−α(log πt(at|st) + H)]\n#\n(3.106)\n3.6\nDeterministic policy gradient methods\nIn this section, we consider the case of a deterministic policy, that predicts a unique action for each state, so\nat = µθ(st), rather than at ∼πθ(st). (We require that the actions are continuous, because we will take the\nJacobian of the Q function wrt the actions; if the actions are discrete, we can just use DQN.) The advantage\nof using a deterministic policy is that we can modify the policy gradient method so that it can work off policy\nwithout needing importance sampling, as we will see.\nFollowing Equation (3.7), we define the value of a policy as the expected discounted reward per state:\nJ(µθ) ≜Eρµθ (s) [R(s, µθ(s))]\n(3.107)\nThe deterministic policy gradient theorem [Sil+14] tells us that the gradient of this expression is given\nby\n∇θJ(µθ) = Eρµθ (s) [∇θQµθ(s, µθ(s))]\n(3.108)\n= Eρµθ (s)\n\u0002\n∇θµθ(s)∇aQµθ(s, a)|a=µθ(s)\n\u0003\n(3.109)\nwhere ∇θµθ(s) is the M × N Jacobian matrix, and M and N are the dimensions of A and θ, respectively.\nFor stochastic policies of the form πθ(a|s) = µθ(s) + noise, the standard policy gradient theorem reduces to\nthe above form as the noise level goes to zero.\nNote that the gradient estimate in Equation (3.109) integrates over the states but not over the actions,\nwhich helps reduce the variance in gradient estimation from sampled trajectories.\nHowever, since the\ndeterministic policy does not do any exploration, we need to use an off-policy method for training. This\ncollects data from a stochastic behavior policy πb, whose stationary state distribution is pγ\nπb. The original\nobjective, J(µθ), is approximated by the following:\nJb(µθ) ≜Epγ\nπb(s) [Vµθ(s)] = Epγ\nπb(s) [Qµθ(s, µθ(s))]\n(3.110)\nwith the off-policy deterministic policy gradient from [DWS12] is approximated by\n∇θJb(µθ) ≈Epγ\nπb(s) [∇θ [Qµθ(s, µθ(s))]] = Epγ\nπb(s)\n\u0002\n∇θµθ(s)∇aQµθ(s, a)|a=µθ(s)\n\u0003\n(3.111)\nwhere we have a dropped a term that depends on ∇θQµθ(s, a) and is hard to estimate [Sil+14].\nTo apply Equation (3.111), we may learn Qw ≈Qµθ with TD, giving rise to the following updates:\nδ = rt + γQw(st+1, µθ(st+1)) −Qw(st, at)\n(3.112)\nwt+1 ←wt + ηwδ∇wQw(st, at)\n(3.113)\nθt+1 ←θt + ηθ∇θµθ(st)∇aQw(st, a)|a=µθ(st)\n(3.114)\nSo we learn both a state-action critic Qw and an actor µθ. This method avoids importance sampling in the\nactor update because of the deterministic policy gradient, and we avoids it in the critic update because of the\nuse of Q-learning.\nIf Qw is linear in w, and uses features of the form ϕ(s, a) = aT∇θµθ(s), then we say the function\napproximator for the critic is compatible with the actor; in this case, one can show that the above\napproximation does not bias the overall gradient.\nThe basic off-policy DPG method has been extended in various ways, some of which we describe below.\n66\n3.6.1\nDDPG\nThe DDPG algorithm of [Lil+16], which stands for “deep deterministic policy gradient”, uses the DQN\nmethod (Section 2.5.2.2) to update Q that is represented by deep neural networks. In more detail, the actor\ntries to minimize the output of the critic by optimize\nLθ(s) = Qw(s, µθ(s))\n(3.115)\naveraged over states s drawn from the replay buffer. The critic tries to minimize the 1-step TD loss\nLw(s, a, r, s′) = [Qw(s, a) −(r + γQw(s′, µθ(s′)))]2\n(3.116)\nwhere Qw is the target critic network, and the samples (s, a, r, a′) are drawn from a replay buffer. (See\nSection 2.5.2.5 for a discussion of target networks.)\nThe D4PG algorithm [BM+18], which stands for “distributed distributional DDPG”, extends DDPG to\nhandle distributed training, and to handle distributional RL (see Section 5.1).\n3.6.2\nTwin Delayed DDPG (TD3)\nThe TD3 (“twin delayed deep deterministic”) method of [FHM18] extends DDPG in 3 main ways. First, it\nuses target policy smoothing, in which noise is added to the action, to encourage generalization:\n˜a = µθ(s) + noise = πθ(s)\n(3.117)\nSecond it uses clipped double Q learning, which is an extension of the double Q-learning discussed in\nSection 2.5.3.1 to avoid over-estimation bias. In particular, the target values for TD learning are defined using\nq(r, s′; w1:2, θ) = r + γ min\ni=1,2 Qwi(s′, πθ(s′))\n(3.118)\nThird, it uses delayed policy updates, in which it only updates the policy after the value function has\nstabilized. (See also Section 3.3.3.) See Algorithm 9 for the pseudcode.\n67\nAlgorithm 9: TD3\n1 Initialize environment state s, policy parameters θ, target policy parameters θ, critic parameters wi,\ntarget critic parameters wi = wi, replay buffer D = ∅, discount factor γ, EMA rate ρ, step size ηw,\nηθ.\n2 repeat\n3\na = µθ(s) + noise\n4\n(s′, r) = step(a, s)\n5\nD := D ∪{(s, a, r, s′)}\n6\ns ←s′\n7\nfor G updates do\n8\nSample a minibatch B = {(sj, aj, rj, s′\nj)} from D\n9\nw = update-critics(θ, w, B)\n10\nSample a minibatch B = {(sj, aj, rj, s′\nj)} from D\n11\nθ = update-policy(θ, w, B)\n12 until converged\n13 .\n14 def update-critics(θ, w, B):\n15 Let (sj, aj, rj, s′\nj)B\nj=1 = B\n16 for j = 1 : B do\n17\n˜aj = µθ(s′\nj) + clip(noise, −c, c)\n18\nqj = rj + γ mini=1,2 Qwi(s′\nj, ˜aj)\n19 for i = 1 : 2 do\n20\nL(wi) =\n1\n|B|\nP\n(s,a,r,s′)j∈B(Qwi(sj, aj) −sg(qj))2\n21\nwi ←wi −ηw∇L(wi) // Descent\n22\nwi := ρwi + (1 −ρ)wi //Update target networks with EMA\n23 Return w1:N, w1:N\n24 .\n25 def update-actor(θ, w, B):\n26 J(θ) =\n1\n|B|\nP\ns∈B (Qw1(s, µθ(s)))2\n27 θ ←θ + ηθ∇J(θ) // Ascent\n28 θ := ρθ + (1 −ρ)θ //Update target policy network with EMA\n29 Return θ, θ\n68\nChapter 4\nModel-based RL\nModel-free approaches to RL typically need a lot of interactions with the environment to achieve good\nperformance. For example, state of the art methods for the Atari benchmark, such as rainbow (Section 2.5.2.2),\nuse millions of frames, equivalent to many days of playing at the standard frame rate. By contrast, humans\ncan achieve the same performance in minutes [Tsi+17]. Similarly, OpenAI’s robot hand controller [And+20]\nneeds 100 years of simulated data to learn to manipulate a rubiks cube.\nOne promising approach to greater sample efficiency is model-based RL (MBRL). In the simplest\napproach to MBRL, we first learn the state transition or dynamics model pS(s′|s, a) — also called a world\nmodel — and the reward function R(s, a), using some offline trajectory data, and then we use these models\nto compute a policy (e.g., using dynamic programming, as discussed in Section 2.2, or using some model-free\npolicy learning method on simulated data, as discussed in Chapter 3). It can be shown that the sample\ncomplexity of learning the dynamics is less than the sample complexity of learning the policy [ZHR24].\nHowever, the above two-stage approach — where we first learn the model, and then plan with it — can\nsuffer from the usual problems encountered in offline RL (Section 5.5), i.e., the policy may query the model\nat a state for which no data has been collected, so predictions can be unreliable, causing the policy to learn\nthe wrong thing. To get better results, we have to interleave the model learning and policy learning, so that\none helps the other (since the policy determines what data is collected).\nThere are two main ways to perform MBRL. In the first approach, known as decision-time planning or\nmodel predictive control, we use the model to choose the next action by searching over possible future\ntrajectories. We then score each trajectory, pick the action corresponding to the best one, take a step in the\nenvironment, and repeat. (We can also optionally update the model based on the rollouts.) This is discussed\nin Section 4.1.\nThe second approach is to use the current model and policy to rollout imaginary trajectories, and to use\nthis data (optionally in addition to empirical data) to improve the policy using model-free RL; this is called\nbackground planning, and is discussed in Section 4.2.\nThe advantage of decision-time planning is that it allows us to train a world model on reward-free data,\nand then use that model to optimize any reward function. This can be particularly useful if the reward\ncontains changing constraints, or if it is an intrinsic reward (Section 5.2.4) that frequently changes based on\nthe knowledge state of the agent. The downside of decision-time planning is that it is much slower. However,\nit is possible to combine the two methods, as we discuss below. For an empirical comparison of background\nplanning and decision-time planning, see [AP24].\nSome generic pseudo-code for an MBRL agent is given in Algorithm 10. (The rollout function is defined\nin Algorithm 11; some simple code for model learning is shown in Algorithm 12, although we discuss other\nloss functions in Section 4.3; finally, the code for the policy learning is given in other parts of this manuscript.)\nFor more details on general MBRL, see e.g., [Wan+19; Moe+23; PKP21].\n69\nAlgorithm 10: MBRL agent\n1 def MBRL-agent(Menv; T, H, N):\n2 Initialize state s ∼Menv\n3 Initialize data buffer D = ∅, model ˆ\nM\n4 Initialize value function V , policy proposal π\n5 repeat\n6\n// Collect data from environment\n7\nτenv = rollout(s, π, T, Menv),\n8\ns = τenv[−1],\n9\nD = D ∪τenv\n10\n// Update model\n11\nif Update model online then\n12\nˆ\nM = update-model( ˆ\nM, τenv)\n13\nif Update model using replay then\n14\nτ n\nreplay = sample-trajectory(D), n = 1 : N\n15\nˆ\nM = update-model( ˆ\nM, τ 1:N\nreplay)\n16\n// Update policy\n17\nif Update on-policy with real then\n18\n(π, V ) = update-on-policy(π, V, τenv)\n19\nif Update on-policy with imagination then\n20\nτ n\nimag = rollout(sample-init-state(D), π, T, ˆ\nM), n = 1 : N\n21\n(π, V ) = update-on-policy(π, V, τ 1:N\nimag)\n22\nif Update off-policy with real then\n23\nτ n\nreplay = sample-trajectory(D), n = 1 : N\n24\n(π, V ) = update-off-policy(π, V, τ 1:N\nreplay)\n25\nif Update off-policy with imagination then\n26\nτ n\nimag = rollout(sample-state(D), π, T, ˆ\nM), n = 1 : N\n27\n(π, V ) = update-off-policy(π, V, τ 1:N\nimag)\n28 until until converged\nAlgorithm 11: Rollout\n1 def rollout(s1, π, T, M)\n2 τ = [s1]\n3 for t = 1 : T do\n4\nat = π(st)\n5\n(st+1, rt+1) ∼M(st, at)\n6\nτ+ = [at, rt+1, st+1]\n7 Return τ\nAlgorithm 12: Model learning\n1 def update-model(M, τ 1:N) :\n2 ℓ(M) = −\n1\nNT\nPN\nn=1\nP\n(st,at,rt+1,st+1)∈τ n log M(st+1, rt+1|st, at) // NLL\n3 M = M −ηM∇Mℓ(M)\n4 Return M\n70\n4.1\nDecision-time planning\nIf the model is known, and the state and action space is discrete and low dimensional, we can use exact\ntechniques based on dynamic programming to compute the policy, as discussed in Section 2.2. However, for\nthe general case, approximate methods must be used for planning, whether the model is known (e.g., for\nboard games like Chess and Go) or learned.\nOne approach to approximate planning is to be lazy, and just wait until we know what state we are in,\ncall it st, and then decide what to do, rather than trying to learn a policy that maps any state to the best\naction. This is called decision time planning or “planning in the now” [KLP11]. We discuss some\nvariants of this approach below.\n4.1.1\nModel predictive control (MPC)\nWe now describe a method known as receeding horizon control or model predictive control (MPC)\n[MM90; CA13; RMD22]: We use the world model to predict future states and rewards that might follow\nfrom the current state for each possible sequence of future actions we might pursue, and we then take the\naction from the sequence that looks most promising. More precisely, at each step, we compute\na∗\nt:t+H−1 = planning(st, M, R, ˆV, H)\n(4.1)\n= argmax\nat:t+H−1\nEst+1:t+H∼M(·|st,at:t+H−1)\n\"H−1\nX\nh=0\nR(st+h, at+h) + ˆV (st+H)\n#\n(4.2)\nπMPC(st) = a∗\nt\n(4.3)\nHere, H is called the planning horizon, and ˆV (st+H) is an estimate of the reward-to-go at the end of this\nH-step look-ahead process. We can often speed up the optimization process by using a pre-trained proposal\npolicy at = π(st), which can be used to guide the search process, as we discuss below.\nNote that MPC computes a fixed sequence of actions, at:t+H−1, also called a plan, given the current state\nst; since the future actions at′ for t′ > t are independent of the future states st′, this is called an open loop\ncontroller. Such a controller can work well in deterministic environments (where st′ can be computed from\nst and the action sequence), but in general, we will need to replan at each step, as the actual next state is\nobserved. Thus MPC is a way of creating a closed loop controller.\nWe can combine MPC with model and policy/proposal learning using the pseudocode in Algorithm 10,\nwhere the decision policy at = πMPC(st) is implemented by Equation (4.2). If we want to learn the propsoal\npolicy at = π(st), we should use off-policy methods, since the training data (even if imaginary) will be\ncollected by πMPC rather than by π. When learning the world model, we only need it to be locally accurate,\naround the current state, which means we can often use simpler models in MPC than in background planning\napproaches.\nIn the sections below, we discuss particular kinds of MPC methods. Further connections between MPC\nand RL are discussed in [Ber24].\n4.1.2\nHeuristic search\nIf the state and action spaces are finite, we can solve Equation (4.2) exactly, although the time complexity\nwill typically be exponential in H. However, in many situations, we can prune off unpromising trajectories,\nthus making the approach feasible in large scale problems.\nIn particular, consider a discrete, deterministic MDP where reward maximization corresponds to finding a\nshortest path to a goal state. We can expand the successors of the current state according to all possible\nactions, trying to find the goal state. Since the search tree grows exponentially with depth, we can use a\nheuristic function to prioritize which nodes to expand; this is called best-first search, as illustrated in\nFigure 4.1.\nIf the heuristic function is an optimistic lower bound on the true distance to the goal, it is called\nadmissible. If we aim to maximize total rewards, admissibility means the heuristic function is an upper\n71\nFigure 4.1: Illustration of heuristic search. In this figure, the subtrees are ordered according to a depth-first search\nprocedure. From Figure 8.9 of [SB18]. Used with kind permission of Richard Sutton.\nbound of the true value function. Admissibility ensures we will never incorrectly prune off parts of the search\nspace. In this case, the resulting algorithm is known as A∗search, and is optimal. For more details on\nclassical AI heuristic search methods, see [Pea84; RN19].\n4.1.3\nMonte Carlo tree search\nMonte Carlo tree search or MCTS is similar to heuristic search, but learns a value function for each\nencountered state, rather than relying on a manually designed heuristic (see e.g., [Mun14] for details). MCTS\nis inspired by the upper confidence bound (UCB) method for bandits, but works for general MDPs [KS06].\n4.1.3.1\nAlphaGo and AlphaZero\nThe famous AlphaGo system [Sil+16], which was the first AI system to beat a human grandmaster at\nthe board game Go, used the MCTS method, combined with a value function learned using RL, and a\npolicy that was initialized using supervised learning from human demonstrations. This was followed up by\nAlphaGoZero [Sil+17a], which had a much simpler design, and did not train on any human data, i.e., it\nwas trained entirely using RL and self play. It significantly outperformed the original AlphaGo. This was\ngeneralized to AlphaZero [Sil+18], which can play expert-level Go, chess, and shogi (Japanese chess), using\na known model of the environment.\n4.1.3.2\nMuZero\nAlphaZero assumes the world model is known. The MuZero method of [Sch+20] learns a world model, by\ntraining a latent representation of the observations, zt = ϕ(ot), and a corresponding latent dynamics model\nzt = M(zt, at). The world model is trained to predict the immediate reward, the future reward (i..e, the\nvalue), and the optimal policy, where the optimal policy is computed using MCTS.\nIn more detail, to learn the model, MuZero uses a sum of 3 loss terms applied to each (zt−1, at, zt, rt) tuple\nin the replay buffer. The first loss is L(rt, ˆrt), where rt is the observed reward and ˆrt = R(zt) is the predicted\nreward. The second loss is L(πMCTS\nt\n, πt), where πMCTS\nt\nis the target policy from MCTS search (see below)\nand πt = f(zt) is the predicted policy. The third loss is L(GMCTS\nt\n, vt), where GMCTS\nt\n= Pn−1\ni=0 γirt+i +γkvt+n\nis the n-step bootstrap target value derived from MCTS search (see below), and vt = V (zt) is the predicted\nvalue from the current model.\nTo pick an action, MuZero does not use the policy directly. Instead it uses MCTS to rollout a search tree\nusing the dynamics model, starting from the current state zt. It uses the predicted policy πt and value vt as\nheuristics to limit the breadth and depth of the search. Each time it expands a node in the tree, it assigns it\na unique integer id (since we are assuming the dynamics are deterministic), thus lazily creating a discrete\nMDP. It then partially solves for the tabular Q function for this MDP using Monte Carlo rollouts, similar to\nreal-time dynamic programming (Section 2.2.2).\n72\nIn more detail, the MCTS process is as follows. Let sk = zt be the root node, for k = 0. We initialize\nQ(sk, a) = 0 and P(sk, a) = πt(a|sk), where the latter is the prior for each action. To select the action ak\nto perform next (in the rollout), we use the UCB heuristic (Section 1.4.3) based on the empirical counts\nN(s, a) combined with the prior policy, P(s, a), which act as pseudocounts. After expanding this node, we\ncreate the child node sk+1 = M(sk, ak); we initialize Q(sk+1, a) = 0 and P(sk+1, a) = πt(a|sk+1), and repeat\nthe process until we reach a maximum depth, where we apply the value function to the corresponding leaf\nnode. We then compute the empirical sum of discounted rewards along each of the explored paths, and\nuse this to update the Q(s, a) and N(s, a) values for all visited nodes. After performing 50 such rollouts,\nwe compute the empirical distribution over actions at the root node to get the MCTS visit count policy,\nπMCTS\nt\n(a) = [N(s0, a)/(P\nb N(s0, b))]1/τ, where τ is a temperature. Finally we sample an action at from\nπMCTS\nt\n, take a step, add (ot, at, rt, πMCTS\nt\n, GMCTS\nt\n) to the replay buffer, compute the losses, update the model\nand policy parameters, and repeat.\nThe Stochastic MuZero method of [Ant+22] extends MuZero to allow for stochastic environments. The\nSampled MuZero method of [Hub+21] extends MuZero to allow for large action spaces.\n4.1.3.3\nEfficientZero\nThe Efficient Zero paper [Ye+21] extends MuZero by adding an additional self-prediction loss to help train\nthe world model. (See Section 4.3.2.2 for a discussion of such losses.) It also makes several other changes. In\nparticular, it replaces the empirical sum of instantaneous rewards, Pn−1\ni=0 γirt+i, used in computing GMCTS\nt\n,\nwith an LSTM model that predicts the sum of rewards for a trajectory starting at zt; they call this the value\nprefix. In addition, it replaces the stored value at the leaf nodes of trajectories in the replay buffer with\nnew values, by rerunning MCTS using the current model applied to the leaves. They show that all three\nchanges help, but the biggest gain is from the self-prediction loss. The recent Efficient Zero V2 [Wan+24b]\nextends this to also work with continuous actions, by replacing tree search with sampling-based Gumbel\nsearch, amongst other changes.\n4.1.4\nTrajectory optimization for continuous actions\nFor continuous actions, we cannot enumerate all possible branches in the search tree. Instead, we can view\nEquation (4.2) as a standard optimization problem over the real valued sequence of vectors at:t+H−1.\n4.1.4.1\nRandom shooting\nFor general nonlinear models (such as neural networks), a simple approach is to pick a sequence of random\nactions to try, evaluate the reward for each trajectory, and pick the best. This is called random shooting\n[Die+07; Rao10].\n4.1.4.2\nLQG\nIf the system dynamics are linear and the reward function corresponds to negative quadratic cost, the optimal\naction sequence can be solved mathematically, as in the linear-quadratic-Gaussian (LQG) controller (see\ne.g., [AM89; HR17]).\nIf the model is nonlinear, we can use differential dynamic programming (DDP) [JM70; TL05]. In\neach iteration, DDP starts with a reference trajectory, and linearizes the system dynamics around states\non the trajectory to form a locally quadratic approximation of the reward function. This system can be\nsolved using LQG, whose optimal solution results in a new trajectory. The algorithm then moves to the next\niteration, with the new trajectory as the reference trajectory.\n4.1.4.3\nCEM\nIt common to use black-box (gradient-free) optimization methods like the cross-entropy method or CEM\nin order to find the best action sequence. The CEM method is a simple derivative-free optimization method for\n73\ncontinuous black-box functions f : RD →R. We start with a multivariate Gaussian, N(µ0, Σ0), representing\na distribution over possible action a. We sample from this, evaluate all the proposals, pick the top K, then\nrefit the Gaussian to these top K, and repeat, until we find a sample with sufficiently good score (or we\nperform moment matching on the top K scores). For details, see [Rub97; RK04; Boe+05].\nIn Section 4.1.4.4, we discuss the MPPI method, which is a common instantiation of CEM method.\nAnother example is in the TD-MPC paper [HSW22a]. They learn the world model (dynamics model) in a\nlatent space so as to predict future value and reward using temporal difference learning, and then use CEM\nto implement MPC for this world model. In [BXS20] they discuss how to combine CEM with gradient-based\nplanning.\n4.1.4.4\nMPPI\nThe model predictive path integral or MPPI approach [WAT17] is a version of CEM. Originally MPPI\nwas limited to models with linear dynamics, but it was extended to general nonlinear models in [Wil+17].\nThe basic idea is that the initial mean of the Gaussian at step t, namely µt = at:t+H, is computed based on\nshifting ˆµt−1 forward by one step. (Here µt is known as a reference trajectory.)\nIn [Wag+19], they apply this method for robot control.\nThey consider a state vector of the form\nst = (qt, ˙qt), where qt is the configuration of the robot. The deterministic dynamics has the form\nst+1 = F(st, at) =\n\u0012\nqt + ˙qt∆t\n˙qt + f(st, at)∆t\n\u0013\n(4.4)\nwhere f is a 2 layer MLP. This is trained using the Dagger method of [RGB11], which alternates between\nfitting the model (using supervised learning) on the current replay buffer (initialized with expert data), and\nthen deploying the model inside the MPPI framework to collect new data.\n4.1.4.5\nGP-MPC\n[KD18] proposed GP-MPC, which combines a Gaussian process dynamics model with model predictive\ncontrol. They compute a Gaussian approximation to the future state trajectory given a candidate action\ntrajectory, p(st+1:t+H|at:t+H−1, st), by moment matching, and use this to deterministically compute the\nexpected reward and its gradient wrt at:t+H−1. Using this, they can solve Equation (4.2) to find a∗\nt:t+H−1;\nfinally, they execute the first step of this plan, a∗\nt , and repeat the whole process.\nThe key observation is that moment matching is a deterministic operator that maps p(st|a1:t−1) to\np(st+1|a1:t), so the problem becomes one of deterministic optimal control, for which many solution methods\nexist. Indeed the whole approach can be seen as a generalization of the LQG method from classical control,\nwhich assumes a (locally) linear dynamics model, a quadratic cost function, and a Gaussian distribution over\nstates [Rec19]. In GP-MPC, the moment matching plays the role of local linearization.\nThe advantage of GP-MPC over the earlier method known as PILCO (“probabilistic inference for learning\ncontrol”), which learns a policy by maximizing the expected reward from rollouts (see [DR11; DFR15] for\ndetails), is that GP-MPC can handle constraints more easily, and it can be more data efficient, since it\ncontinually updates the GP model after every step (instead of at the end of an trajectory).\n4.1.5\nSMC for MPC\nA general way to tackle MPC — which supports discrete and continuous actions, as well as discrete and\ncontinuous states and linear and nonlinear world models — is to formulate it as the problem of posterior\ninference over state-action sequences with high reward. That is, following the control as inference framework\ndiscussed in Section 1.5, we define the goal as computing the following posterior:\np(x1:T |s1, O1:T ) ∝p(x1:T , O1:T |s1) =\nT −1\nY\nt=1\np(st+1|at, st) exp\n T\nX\nt=1\nR(st, at) + log p(at)\n!\n(4.5)\n74\nwhere xt = (st, at), and Ot is the “optimality variable” which is clamped to the value 1, with distribution\np(Ot = 1|st, at) = exp(R(st, at)). (Henceforth we will assume a uniform prior over actions, so p(at) ∝1.) If\nwe can sample from this distribution, we can find state-action sequences with high expected reward, and then\nwe can just extract the first action from one of these sampled trajectories.1\nIn practice we only compute the posterior for h steps into the future, although we still condition on\noptimality out to the full horizon T. Thus we define our goal as computing\np(x1:h|O1:T ) ∝p(x1:h|O1:h)\n|\n{z\n}\nαh(x1:h)\np(Oh+1:T |xh)\n|\n{z\n}\nβh(xh)\n(4.6)\nwhere p(Ot = 1|st, at) = exp(R(st, at)) is the probability that the “optimality variable” obtains its observed\n(clamped) value of 1. We have decomposed the posterior as a forwards filtering term, αh(x1:h), and a\nbackwards likelihood or smoothing term, βh(xh), as is standard in the literature on inference in state-space\nmodels (see e.g., [Mur23, Ch.8-9]). Note that if we define the value function as V (sh) = log p(Oh:T |sh), then\nthe backwards message can be rewritten as follows [Pic+19]:\np(Oh+1:T |xh) = Ep(sh+1|xh) [exp(V (sh+1))]\n(4.7)\nA standard way to perform posterior inference in models such as these is to use Sequential Monte Carlo\nor SMC, which is an extension of particle filtering (i.e., sequential importance sampling with resampling) to a\ngeneral sequence of distributions over a growing state space (see e.g., [Mur23, Ch 13.]). When combined with\nan approximation to the backwards message, the approach is called twisted SMC [BDM10; WL14; AL+16;\nLaw+22; Zha+24]. This was applied to MPC in [Pic+19]. In particular, they suggest using SAC to learn a\nvalue function V , analogous to the backwards twist function, and policy π, which can be used to create the\nforwards proposal. More precisely, the policy can be combined with the world model M(st|st−1, at−1) to\ngive a (Markovian) proposal disribution over the next state and action:\nq(xt|x1:t−1) = M(st|st−1, at−1)π(at|st)\n(4.8)\nThis can then be used inside of an SMC algorithm to sample trajectories from the posterior in Equation (4.6).\nIn particular, at each step, we sample from the proposal to extend each previous particle (sampled trajectory)\nby one step, and then reweight the corresponding particle using\nwt = p(x1:T |O1:T )\nq(x1:t)\n= p(x1:t−1|O1:T )p(xt|x1:t−1, O1:T )\nq(x1:t−1)q(xt|x1:t−1)\n(4.9)\n= wt−1\np(xt|x1:t−1, O1:T )\nq(xt|x1:t−1)\n∝wt−1\n1\nq(xt|x1:t−1)\np(x1:t|O1:T )\np(x1:t−1|O1:T )\n(4.10)\nNow plugging in the forward-backward equation from Equation (4.6), and doing some algebra, we get the\nfollowing (see [Pic+19, App. A.4] for the detailed derivation):\nwt ∝wt−1\n1\nq(xt|x1:t−1)\np(x1:t|O1:t)p(Ot+1:T |xt)\np(x1:t−1|O1:t−1)p(Ot:T |xt−1)\n(4.11)\n∝wt−1Ep(st+1|st,at) [exp(A(st, at, st+1))]\n(4.12)\nwhere\nA(st, at, st+1) = rt −log π(at|st) + V (st+1) −Ep(st|st−1,at−1) [exp(V (st))]\n(4.13)\nis a maximum entropy version of an advantage function. We show the overall pseudocode in Algorithm 13.\nAn improved version of the above method, called Critic SMC, is presented in [Lio+22]. The main\ndifference is that they first extend each of the N particles (sampled trajectories) by K possible “putative\nactions” ank\ni , then score them using a learned heuristic function Q(sn\ni , ank\ni ), then resample N winners an\ni from\n1We should really marginalize over the state sequences, and then find the maximum marginal probability action sequence, as\nin Equation (4.2), but we approximate this by joint sampling, for simplicity. For more discussion on this point, see [LG+24].\n75\nAlgorithm 13: SMC for MPC\n1 def SMC-MPC(st, M, π, V, H)\n2 Initialize particles: {sn\nt = st}N\nn=1\n3 Initialize weights: {wn\nt = 1}N\nn=1\n4 for i = t : t + H do\n5\n// Propose one-step extension\n6\n{an\ni ∼π(·|sn\ni )}\n7\n{(sn\ni+1, rn\ni ) ∼M(·|sn\ni , an\ni )}\n8\n// Update weights\n9\n{wn\ni ∝wn\ni−1 exp(A(sn\ni , an\ni , sn\ni+1))}\n10\n// Resampling\n11\n{xn\n1:i} ∼Multinom(n; w1\ni , . . . , wN\ni )\n12\n{wn\ni = 1}\n13 Sample n ∼Unif(1 : N) // Pick one of the top samples\n14 Return an\nt\nthis set of N ×K particles, and then push these winners through the dynamics model to get sn\ni+1 ∼M(·|sn\ni , an\ni ).\nFinally, they reweight the N particles by the advantage and resample, as before. This can be advantageous if\nthe dynamics model is slow to evaluate, since we can evaluate K possible extensions just using the heuristic\nfunction. We can think of this as a form of stochastic beam search, where the beam has N candidates, and\nyou expand each one using K possible actions, and then reduce the population (beam) back to N\n4.2\nBackground planning\nIn Section 4.1, we discussed how to use models to perform decision time planning. However, this can be\nslow. Fortunately, we can amortize the planning process into a reactive policy. To do this, we can use the\nmodel to generate synthetic trajectories “in the background” (while executing the current policy), and use\nthis imaginary data to train the policy; this is called “background planning”. We discuss a game theoretic\nformulation of this setup in Section 4.2.1. Then in Section 4.2.2, we discuss ways to combine model-based\nand model-free learning. Finally, in Section 4.2.3, we discuss ways to deal with model errors, that might lead\nthe policy astray.\n4.2.1\nA game-theoretic perspective on MBRL\nIn this section, we discuss a game-theoretic framework for MBRL, as proposed in [RMK20]. This provides a\ntheoretical foundation for many of the more heuristic methods in the literature.\nWe denote the true world model by Menv. To simplify the notation, we assume an MDP setup with a\nknown reward function, so all that needs to be learned is the world model, ˆ\nM, representing p(s′|s, a). (It is\ntrivial to also learn the reward function.) We define the value of a policy π when rolled out in some model\nM ′ as the (discounted) sum of expected rewards:\nJ(π, M ′) = EM ′,π\n\" ∞\nX\nt=0\nγtR(st)\n#\nWe define the loss of a model ˆ\nM given a distribution µ(s, a) over states and actions as\nℓ( ˆ\nM, µ) = E(s,a)∼µ\nh\nDKL\n\u0010\nMenv(·|s, a) ∥ˆ\nM(·|s, a)\n\u0011i\n76\nWe now define MBRL as a two-player general-sum game:\npolicy player\nz\n}|\n{\nmax\nπ\nJ(π, ˆ\nM),\nmodel player\nz\n}|\n{\nmin\nˆ\nM\nℓ( ˆ\nM, µπ\nMenv)\nwhere µπ\nMenv = 1\nT\nPT\nt=0 Menv(st = s, at = a) as the induced state visitation distribution when policy π is\napplied in the real world Menv, so that minimizing ℓ( ˆ\nM, µπ\nMenv) gives the maximum likelhood estimate\nfor ˆ\nM.\nNow consider a Nash equilibrium of this game, that is a pair (π, ˆ\nM) that satisfies ℓ( ˆ\nM, µπ\nMenv) ≤ϵMenv\nand J(π, ˆ\nM) ≥J(π′, ˆ\nM) −ϵπ for all π′. (That is, the model is accurate when predicting the rollouts from π,\nand π cannot be improved when evaluated in ˆ\nM). In [RMK20] they prove that the Nash equilibirum policy\nπ is near optimal wrt the real world, in the sense that J(π∗, Menv) −J(π, Menv) is bounded by a constant,\nwhere π∗is an optimal policy for the real world Menv. (The constant depends on the ϵ parameters, and the\nTV distance between µπ∗\nMenv and µπ∗\nˆ\nM .)\nA natural approach to trying to find such a Nash equilibrium is to use gradient descent ascent or\nGDA, in which each player updates its parameters simultaneously, using\nπk+1 = πk + ηπ∇πJ(πk, ˆ\nMk)\nˆ\nMk+1 = ˆ\nMk −ηM∇ˆ\nMℓ( ˆ\nMk, µπk\nMenv)\nUnfortunately, GDA is often an unstable algorithm, and often needs very small learning rates η. In addition,\nto increase sample efficiency in the real world, it is better to make multiple policy improvement steps using\nsynthetic data from the model ˆ\nMk at each step.\nRather than taking small steps in parallel, the best response strategy fully optimizes each player given\nthe previous value of the other player, in parallel:\nπk+1 = argmax\nπ\nJ(π, ˆ\nMk)\nˆ\nMk+1 = argmin\nˆ\nM\nℓ( ˆ\nM, µπk\nMenv)\nUnfortunately, making such large updates in parallel can often result in a very unstable algorithm.\nTo avoid the above problems, [RMK20] propose to replace the min-max game with a Stackelberg game,\nwhich is a generalization of min-max games where we impose a specific player ordering. In particular, let the\nplayers be A and B, let their parameters be θA and θB, and let their losses be LA(θA, θB) and LB(θA, θB).\nIf player A is the leader, the Stackelberg game corresponds to the following nested optimization problem,\nalso called a bilevel optimization problem:\nmin\nθA LA(θA, θ∗\nB(θA))\ns.t.\nθ∗\nB(θA) = argmin\nθ\nLB(θA, θ)\nSince the follower B chooses the best response to the leader A, the follower’s parameters are a function of the\nleader’s. The leader is aware of this, and can utilize this when updating its own parameters.\nThe main advantage of the Stackelberg approach is that one can derive gradient-based algorithms that\nwill provably converge to a local optimum [CMS07; ZS22]. In particular, suppose we choose the policy as\nleader (PAL). We then just have to solve the following optimization problem:\nˆ\nMk+1 = argmin\nˆ\nM\nℓ( ˆ\nM, µπk\nMenv)\nπk+1 = πk + ηπ∇πJ(πk, ˆ\nMk+1)\nWe can solve the first step by executing πk in the environment to collect data Dk and then fitting a local\n(policy-specific) dynamics model by solving ˆ\nMk+1 = argmin ℓ( ˆ\nM, Dk). (For example, this could be a locally\n77\nlinear model, such as those used in trajectory optimization methods discussed in Section 4.1.4.4.) We then\n(slightly) improve the policy to get πk+1 using a conservative update algorithm, such as natural actor-critic\n(Section 3.3.4) or TRPO (Section 3.4.2), on “imaginary” model rollouts from ˆ\nMk+1.\nAlternatively, suppose we choose the model as leader (MAL). We now have to solve\nπk+1 = argmax\nπ\nJ(π, ˆ\nMk)\nˆ\nMk+1 = ˆ\nMk −ηM∇ˆ\nMℓ( ˆ\nM, µπk+1\nMenv)\nWe can solve the first step by using any RL algorithm on “imaginary” model rollouts from ˆ\nMk to get πk+1. We\nthen apply this in the real world to get data Dk+1, which we use to slightly improve the model to get ˆ\nMk+1\nby using a conservative model update applied to Dk+1. (In practice we can implement a conservative model\nupdate by mixing Dk+1 with data generated from earlier models, an approach known as data aggregation\n[RB12].) Compared to PAL, the resulting model will be a more global model, since it is trained on data from\na mixture of policies (including very suboptimal ones at the beginning of learning).\n4.2.2\nDyna\nThe Dyna paper [Sut90] proposed an approach to MBRL that is related to the approach discussed in\nSection 4.2.1, in the sense that it trains a policy and world model in parallel, but it differs in one crucial way: the\npolicy is also trained on real data, not just imaginary data. That is, we define πk+1 = πk+ηπ∇πJ(πk, ˆDk∪Dk),\nwhere Dk is data from the real environment and ˆDk = rollout(πk, ˆ\nMk) is imaginary data from the model.\nThis makes Dyna a hybrid model-free and model-based RL method, rather than a “pure” MBRL method.\nIn more detail, at each step of Dyna, the agent collects new data from the environment and adds it to a\nreal replay buffer. This is then used to do an off-policy update. It also updates its world model given the real\ndata. Then it simulates imaginary data, starting from a previously visited state (see sample-init-state\nfunction in Algorithm 10), and rolling out the current policy in the learned model. The imaginary data is\nthen added to the imaginary replay buffer and used by an on-policy learning algorithm. This process continue\nuntil the agent runs out of time and must take the next step in the environment.\n4.2.2.1\nTabular Dyna\nThe original Dyna paper was developed under the assumption that the world model s′ = M(s, a) is\ndeterministic and tabular, and the Q function is also tabular. See Algorithm 14 for the simplified pseudocode\nfor this case. Since we assume a deterministic world model of the form s′ = M(s, a), then sampling a single\nstep from this starting at a previously visited state is equivalent to experience replay (Section 2.5.2.3). Thus\nwe can think of ER as a kind of non-parametric world model [HHA19].\n4.2.2.2\nDyna with function approximation\nIt is easy to extend Dyna to work with function approximation and policy gradient methods. The code is\nidentical to the MBRL code in Algorithm 10, where now we train the policy on real as well as imaginary data.\n([Lai+21] argues that we should gradually increase the fraction of real data that is used to train the policy, to\navoid suboptimal performance due to model limitations.) If we use real data from the replay buffer, we have\nto use an off-policy learner, since the replay buffer contains trajectories that may have been generated from\nold policies. (The most recent real trajectory, and all imaginary trajectories, are always from the current\npolicy.)\nWe now mention some examples of this “generalized Dyna” framework. In [Sut+08] they extended Dyna to\nthe case where the Q function is linear, and in [HTB18] they extended it to the DQN case. In [Jan+19a], they\npresent the MBPO (model based policy optimization) algorithm, which uses Dyna with the off-policy SAC\nmethod. Their world model is an ensemble of DNNs, which generates diverse predictions (an approach\nwhich was originally proposed in the PETS (probabilistic ensembles with trajectory sampling) paper of\n[Chu+18]). In [Kur+19], they combine Dyna with TRPO (Section 3.4.2) and ensemble world models, and\n78\nAlgorithm 14: Tabular Dyna-Q\n1 def dyna-Q-agent(s, Menv; ϵ, η, γ):\n2 Initialize data buffer D = ∅, Q(s, a) = 0 and ˆ\nM(s, a) = 0\n3 repeat\n4\n// Collect real data from environment\n5\na = eps-greedy(Q, ϵ)\n6\n(s′, r) = env.step(s, a)\n7\nD = D ∪{(s, a, r, s′)}\n8\n// Update policy on real data\n9\nQ(s, a) := Q(s, a) + η[r + γ maxa′ Q(s′, a′) −Q(s, a)]\n10\n// Update model on real data\n11\nˆ\nM(s, a) = (s′, r)\n12\ns := s′\n13\n// Update policy on imaginary data\n14\nfor n=1:N do\n15\nSelect (s, a) from D\n16\n(s′, r) = ˆ\nM(s, a)\n17\nQ(s, a) := Q(s, a) + η[r + γ maxa′ Q(s′, a′) −Q(s, a)]\n18 until until converged\nin [Wu+23] they combine Dyna with PPO and GP world models. (Technically speaking, these on-policy\napproaches are not valid with Dyna, but they can work if the replay buffer used for policy training is not too\nstale.)\n4.2.3\nDealing with model errors and uncertainty\nThe theory in Section 4.2.1 tells us that the model-as-leader approach, which trains a new policy in imagination\nat each inner iteration while gradually improving the model in the outer loop, will converge to the optimal\npolicy, provided the model converges to the true model (or one that is value equivalent to it, see Section 4.3.2.1).\nThis can be assured provided the model is sufficiently powerful, and the policy explores sufficiently widely to\ncollect enough diverse but task-relevant data. Nevertheless, models will inevitably have errors, and it can be\nuseful for the policy learning to be aware of this. We discuss some approaches to this below.\n4.2.3.1\nAvoiding compounding errors in rollouts\nIn MBRL, we have to rollout imaginary trajectories to use for training the policy. It makes intuitive sense\nto start from a previously visited real-world state, since the model will likely be reliable there. We should\nstart rollouts from different points along each real trajectory, to ensure good state coverage, rather than just\nexpanding around the initial state [Raj+17]. However, if we roll out too far from a previously seen state, the\ntrajectories are likely to become less realistic, due to compounding errors from the model [LPC22].\nIn [Jan+19a], they present the MBPO method, which uses short rollouts (inside Dyna) to prevent\ncompounding error (an approach which is justified in [Jia+15]). [Fra+24] is a recent extension of MBPO\nwhich dynamically decides how much to roll out, based on model uncertainty.\nAnother approach to mitigating compounding errors is to learn a trajectory-level dynamics model, instead\nof a single-step model, see e.g., [Zho+24] which uses diffusion to train p(st+1:t+H|st, at:t+H−1), and uses this\ninside an MPC loop.\nIf the model is able to predict a reliable distribution over future states, then we can leverage this\nuncertainty estimate to compute an estimate of the expected reward. For example, PILCO [DR11; DFR15]\nuses Gaussian processes as the world model, and uses this to analytically derive the expected reward over\ntrajectories as a function of policy parameters, which are then optimized using a deterministic second-order\n79\ngradient-based solver. In [Man+19], they combine the MPO algorithm (Section 3.4.4) for continuous control\nwith uncertainty sets on the dynamics to learn a policy that optimizes for a worst case expected return\nobjective.\n4.2.3.2\nEnd-to-end differentiable learning of model and planner\nOne solution to the mismatch problem between model fitting and policy learning is to use differentiable\nplanning, in which we learn the model so as to minimize the planning loss. This bilevel optimization problem\nwas first proposed in the Value Iteration Network paper of [Tam+16] and extended in the TreeQN paper\nof [Far+18]. In [AY20] they proposed a version of this for continuous actions based on the differentiable cross\nentropy method. In [Nik+22; Ban+23] they propose to use implicit differentation to avoid explicitly unrolling\nthe inner optimization.\n4.2.3.3\nUnified model and planning variational lower bound\nIn [Eys+22], they propose a method called Mismatched No More (MNM) to solve the objective mismatch\nproblem. They define an optimality variable (see Section 1.5) based on the entire trajectory, p(O = 1|τ) =\nR(τ) = P∞\nt=1 γtR(st, at). This gives rise to the following variational lower bound on the log probability of\noptimality:\nlog p(O = 1) = log\nZ\nτ\nP(O = 1, τ) = log EP (τ) [P(O = 1|τ)] ≥EQ(τ) [log R(τ) + log P(τ) −log Q(τ)]\nwhere P(τ) is the distribution over trajectories induced by policy applied to the true world model, P(τ) =\nµ(s0) Q∞\nt=0 M(st+1|st, at)π(at|st), and Q(τ) is the distribution over trajectories using the estimated world\nmodel, Q(τ) = µ(s0) Q∞\nt=0 ˆ\nM(st+1|st, at)π(at|st). They then maximize this bound wrt π and ˆ\nM.\nIn [Ghu+22] they extend MNM to work with images (and other high dimensional states) by learning a\nlatent encoder ˆE(zt|ot) as well as latent dynamics ˆ\nM(zt+1|zt, at), similar to other self-predictive methods\n(Section 4.3.2.2). They call their method Aligned Latent Models.\n4.2.3.4\nDynamically switching between MFRL and MBRL\nOne problem with the above methods is that, if the model is of limited capacity, or if it learns to model\n“irrelevant” aspects of the environment, then any MBRL method may be dominated by a MFRL method that\ndirectly optimizes the true expected reward. A safer approach is to use a model-based policy only when the\nagent is confident it is better, but otherwise to fall back to a model-free policy. This is the strategy proposed\nin the Unified RL method of [Fre+24].\n4.3\nWorld models\nIn this section, we discuss various kinds of world models that have been proposed in the literature. These can\nbe used for decision-time planning or for background planning\n4.3.1\nGenerative world models\nIn this section, we discuss different kinds of world model M(s′|s, a). We can use this to generate imaginary\ntrajectories by sampling from the following joint distribution:\np(st+1:T , rt+1:T , at:T −1|st) =\nT −1\nY\ni=t\nπ(ai|si)M(si+1|si, ai)R(ri+1|si, ai)\n(4.14)\n80\n4.3.1.1\nObservation-space world models\nThe simplest approach is to define M(s′|s, a) as a conditional generative model over states. If the state space\nis high dimensional (e.g., images), we can use standard techniques for image generation such as diffusion\n(see e.g., the Diamond method of [Alo+24]). If the observed states are low-dimensional vectors, such as\nproprioceptive states, we can use transformers (see e.g., the Transformer Dynamics Model of [Sch+23a]).\n4.3.1.2\nFactored models\nIn some cases, the dimensions of the state vector s represent distinct variables, and the joint Markov transition\nmatrix p(s′|s, a) has conditional independence properties which can be represented as a sparse graphical\nmodel, This is called a factored MDP [BDG00].\n4.3.1.3\nLatent-space world models\nIn this section, we describe some methods that use latent variables as part of their world model. We let\nzt denote the latent (or hidden) state at time t; this can be a discrete or continuous variable (or vector of\nvariables). The generative model has the form of a controlled HMM:\np(ot+1:T , zt+1:T , rt+1:T , at:T −1|zt) =\nT −1\nY\ni=t\n[π(ai|zi)M(zi+1|zi, ai)R(ri|zi+1, ai)D(oi|zi+1)]\n(4.15)\nwhere p(ot|zt) = D(ot|zt) is the decoder, or likelihood function, and π(at|zt) is the policy.\nThe world model is usually trained by maximizing the marginal likelihood of the observed outputs given\nan action sequence. (We discuss non-likelihood based loss functions in Section 4.3.2.) Computing the marginal\nlikelihood requires marginalizing over the hidden variables zt+1:T . To make this computationally tractable,\nit is common to use amortized variational inference, in which we train an encoder network, p(zt|ot), to\napproximate the posterior over the latents. Many papers have followed this basic approach, such as the\n“world models” paper [HS18], and the methods we discuss below.\n4.3.1.4\nDreamer\nIn this section, we summarize the approach used in Dreamer paper [Haf+20] and its recent extensions,\nsuch as DreamerV2 [Haf+21] and DreamerV3 [Haf+23]. These are all based on the background planning\napproach, in which the policy is trained on imaginary trajectories generated by a latent variable world model.\n(Note that Dreamer is based on an earlier approach called PlaNet [Haf+19], which used MPC instead of\nbackground planning.)\nIn Dreamer, the stochastic dynamic latent variables in Equation (4.15) are replaced by deterministic\ndynamic latent variables ht, since this makes the model easier to train. (We will see that ht acts like the\nposterior over the hidden state at time t −1; this is also the prior predictive belief state before we see ot.) A\n“static” stochastic variable ϵt is now generated for each time step, and acts like a “random effect” in order\nto help generate the observations, without relying on ht to store all of the necessary information. (This\nsimplifies the recurrent latent state.) In more detail, Dreamer defines the following functions:2\n• A hidden dynamics (sequence) model: ht+1 = U(ht, at, ϵt)\n• A latent state prior: ˆϵt ∼P(ˆϵt|ht)\n• A latent state decoder (observation predictor): ˆot ∼D(ˆot|ht, ˆϵt).\n• A reward predictor: ˆrt ∼R(ˆrt|ht, ˆϵt)\n• A latent state encoder: ϵt ∼E(ϵt|ht, ot).\n2To map from our notation to the notation in the paper, see the following key: ot →xt, U →fϕ (sequence model),\nP →pϕ(ˆzt|ht) (dynamics predictor),ion model), D →pϕ(ˆxt|ht, ˆzt) (decoder), E →qϕ(ϵt|ht, xt) (encoder).\n81\not\nLop\nˆot\nD\nE\nˆǫt\nǫt\nP\nht\nU\nht+1\nLkl\nπ\nat\nFigure 4.2: Illustration of Dreamer world model as a factor graph (so squares are functions, circles are variables). We\nhave unrolled the forwards prediction for only 1 step. Also, we have omitted the reward prediction loss.\n82\n• A policy function: at ∼π(at|ht)\nSee Figure 4.2 for an illustration of the system.\nWe now give a simplified explanation of how the world model is trained. The loss has the form\nLWM = Eq(ϵ1:T )\n\" T\nX\nt=1\nβoLo(ot, ˆot) + βzLz(ϵt, ˆϵt)\n#\n(4.16)\nwhere the β terms are different weights for each loss, and q is the posterior over the latents, given by\nq(ϵ1:T |h0, o1:T , a1:T ) =\nT\nY\nt=1\nE(ϵt|ht, ot)δ(ht −U(ht−1, at−1, ϵt−1))\n(4.17)\nThe loss terms are defined as follows:\nLo = −ln D(ot|ϵt, ht)\n(4.18)\nLz = DKL (E(ϵt|ht, ot)) ∥P(ϵt|ht)))\n(4.19)\nwhere we abuse notation somewhat, since Lz is a function of two distributions, not of the variables ϵt and ˆϵt.\nIn addition to the world model loss, we have the following actor-critic losses\nLcritic =\nT\nX\nt=1\n(V (ht) −sg(Gλ\nt ))2\n(4.20)\nLactor = −\nT\nX\nt=1\nsg((Gλ\nt −V (ht))) log π(at|ht)\n(4.21)\nwhere Gλ\nt is the GAE estimate of the reward to go:\nGλ\nt = rt + γ\n\u0000(1 −λ)V (ht) + λGλ\nt+1\n\u0001\n(4.22)\nThere have been several extensions to the original Dreamer paper. DreamerV2 [Haf+21] adds categorical\n(discrete) latents and KL balancing between prior and posterior estimates. This was the first imagination-\nbased agent to outperform humans in Atari games. DayDreamer [Wu+22] applies DreamerV2 to real robots.\nDreamerV3 [Haf+23] builds upon DreamerV2 using various tricks — such as symlog encodings3 for the reward,\ncritic, and decoder — to enable more stable optimization and domain independent choice of hyper-parameters.\nIt was the first method to create diamonds in the Minecraft game without requiring human demonstration\ndata. (However, reaching this goal took 17 days of training.) [Lin+24a] extends DreamerV3 to also model\nlanguage observations.\nVariants of Dreamer such as TransDreamer [Che+21a] and STORM [Zha+23b] have also been explored,\nwhere transformers replace the recurrent network. The DreamingV2 paper of [OT22] replaces the generative\nloss with a non-generative self-prediction loss (see Section 4.3.2.2).\n4.3.1.5\nIris\nThe Iris method of [MAF22] follows the MBRL paradigm, in which it alternates beween (1) learning a world\nmodel using real data Dr and then generate imaginary rollouts Di using the WM, and (2) learning the policy\ngiven Di and collecting new data D′\nr for learning. In the model learning stage, Iris learns a discrete latent\nencoding using the VQ-VAE method, and then fits a transformer dynamics model to the latent codes. In\nthe policy learning stage, it uses actor critic methods. The Delta-Iris method of [MAF24] extends this by\ntraining the model to only predict the delta between neighboring frames. Note that, in both cases, the policy\nhas the form at = π(ot), where ot is an image, so the the rollouts need to ground to pixel space, and cannot\nonly be done in latent space.\n3The symlog function is defined as symlog(x) = sign(x) ln(|x| + 1), and its inverse is symexp(x) = sign(x)(exp(|x|) −1). The\nsymlog function squashes large positive and negative values, while preserving small values.\n83\nLoss\nPolicy\nUsage\nExamples\nOP\nObservables\nDyna\nDiamond [Alo+24], Delta-Iris [MAF24]\nOP\nObservables\nMCTS\nTDM [Sch+23a]\nOP\nLatents\nDyna\nDreamer [Haf+23]\nRP, VP, PP\nLatents\nMCTS\nMuZero [Sch+20]\nRP, VP, PP, ZP\nLatents\nMCTS\nEfficientZero [Ye+21]\nRP, VP, ZP\nLatents\nMPC-CEM\nTD-MPC [HSW22b]\nVP, ZP\nLatents\nAux.\nMinimalist [Ni+24]\nVP, ZP\nLatents\nDyna\nDreamingV2 [OT22]\nVP, ZP, OP\nLatents\nDyna\nAIS [Sub+22]\nPOP\nLatents\nDyna\nDenoised MDP [Wan+22]\nTable 4.1: Summary of some world-modeling methods. The “loss” column refers to the loss used to train the latent\nencoder (if present) and the dynamics model (OP = observation prediction, ZP = latent state prediction, RP = reward\nprediction, VP = value prediction, PP = policy prediction, POP = partial observation prediction). The “policy” column\nrefers to the input that is passed to the policy. (For MCTS methods, the policy is just used as a proposal over action\nsequences to initialize the search/ optimization process.) The “usage” column refers to how to the world model is used:\nfor background planning (which we call “Dyna”), or for decision-time planning (which we call “MCTS”), or just as\nan auxiliary loss on top of standard policy/value learning (which we call “Aux”). Thus Aux methods are single-stage\n(“end-to-end”), whereas the other methods alternate are two-phase, and alternate between improving the world model\nand then using it for improving the policy (or searching for the optimal action).\n4.3.2\nNon-generative world models\nIn Section 4.2.1, we argued that, if we can learn a sufficiently accurate world model, then solving for the\noptimal policy in simulation will give a policy that is close to optimal in the real world. However, a simple\nagent may not be able to capture the full complexity of the true environment; this is called the “small agent,\nbig world” problem [DVRZ22; Lu+23; Aru+24a; Kum+24].\nConsider what happens when the agent’s model is misspecified (i.e., it cannot represent the true world\nmodel), which is nearly always the case. The agent will train its model to reduce state (or observation)\nprediction error, by minimizing ℓ( ˆ\nM, µπ\nM). However, not all features of the state are useful for planning. For\nexample, if the states are images, a dynamics model with limited representational capacity may choose to focus\non predicting the background pixels rather than more control-relevant features, like small moving objects,\nsince predicting the background reliably reduces the MSE more. This is due to “objective mismatch”\n[Lam+20; Wei+24], which refers to the discrepancy between the way a model is usually trained (to predict\nthe observations) vs the way its representation is used for control. To tackle this problem, in this section we\ndiscuss methods for learning representations and models that don’t rely on predicting all the observations.\nOur presentation is based in part on [Ni+24] (which in turn builds on [Sub+22]). See Table 4.1 for a summary\nof some of the methods we will discuss.\n4.3.2.1\nValue prediction\nLet Dt = (Dt−1, at−1, rt−1, ot) be the observed history at time t, and let zt = ϕ(Dt) be a latent representation\n(compressed encoding) of this history, where ϕ is called an encoder or a state abstraction function. We will\ntrain the policy at = π(zt) in the usual way, so our focus will be on how to learn good latent representations.\nAn optimal representation zt = ϕ(Dt) is a sufficient statistic for the optimal action-value function Q∗.\nThus it satifies the value equivalence principle [LWL06; Cas11; Gri+20; GBS22; AP23; ARKP24], which\nsays that two states s1 and s2 are value equivalent (given a policy) if V π(s1) = V π(s2). In particular, if the\nrepresentation is optimal, it will satisfy value equivalence wrt the optimal policy, i.e., if ϕ(Di) = ϕ(Dj) then\nQ∗(Di, a) = Q∗(Dj, a). We can train such a representation function by using its output z = ϕ(D) as input to\nthe Q function or to the policy. (We call such a loss VP, for value prediction.) This will cause the model to\nfocus its representational power on the relevant parts of the observation history.\nNote that there is a stronger property than value equivalence called bisimulation [GDG03]. This says\n84\nFigure 4.3: Illustration of an encoder zt = E(ot), which is passed to a value estimator vt = V (zt), and a world model,\nwhich predicts the next latent state ˆzt+1 = M(zt, at), the reward rt = R(zt, at), and the termination (done) flag,\ndt = done(zt). From Figure C.2 of [AP23]. Used with kind permission of Doina Precup.\nthat two states s1 and s2 are bisimiliar if P(s′|s1, a) ≈P(s′|s2, a) and R(s1, a) = R(s2, a). From this, we can\nderive a continuous measure called the bisimulation metric [FPP04]. This has the advantage (compared\nto value equivalence) of being policy independent, but the disadvantage that it can be harder to compute\n[Cas20; Zha+21], although there has been recent progress on computaitonally efficient methods such as MICo\n[Cas+21] and KSMe [Cas+23].\n4.3.2.2\nSelf prediction\nUnfortunately, in problems with sparse reward, predicting the value may not provide enough of a feedback\nsignal to learn quickly. Consequently it is common to augment the training with a self-prediction loss\nwhere we train ϕ to ensure the following condition hold:\n∃M s.t. EM ∗[z′|D, a] = EM [z′|ϕ(D), a)] ∀D, a\n(4.23)\nwhere the LHS is the predicted mean of the next latent state under the true model, and the RHS is the\npredicted mean under the learned dynamics model. We call this the EZP, which stands for expected z\nprediction.4\nA trivial way to minimize the (E)ZP loss is for the embedding to map everything to a constant vector,\nsay E(D) = 0, in which case zt+1 will be trivial for the dynamics model M to predict. However this is not a\nuseful representation. This problem is representational collapse [Jin+22]. Fortunately, we can provably\nprevent collapse (at least for linear encoders) by using a frozen target network [Tan+23; Ni+24]. That is, we\nuse the following auxiliary loss\nLEZP(ϕ, θ; D, a, D′) = ||Mθ(Eϕ(D, a)) −Eϕ(D′)||2\n2\n(4.24)\nwhere\nϕ = ρϕ + (1 −ρ)sg(ϕ)\n(4.25)\nis the (stop-gradient version of) the EMA of the encoder weights. (If we set ρ = 0, this is called a detached\nnetwork.)\n4In [Ni+24], they also describe the ZP loss, which requires predicting the full distribution over z′ using a stochastic transition\nmodel. This is strictly more powerful, but somewhat more complicated, so we omit it for simplicity.\n85\nWe can also train the latent encoder to predict the reward. Formally, we want to ensure we can satisfy\nthe following condition, which we call RP for “reward prediction”:\n∃R s.t. ER∗[r|D, a] = ER [r|ϕ(D), a)] ∀D, a\n(4.26)\nSee Figure 4.3 for an illustration. In [Ni+24], they prove that a representation that satisfies ZP and RP is\nenough to satisfy value equivalence (sufficiency for Q∗).\nMethods that optimize ZP and VP loss have been used in many papers, such as Predictron [Sil+17b],\nValue Prediction Networks [OSL17], Self Predictive Representations (SPR) [Sch+21], Efficient\nZero (Section 4.1.3.3), BYOL-Explore (Section 4.3.2.6), etc.\n4.3.2.3\nPolicy prediction\nThe value function and reward losses may be too sparse to learn efficiently. Although self-prediction loss can\nhelp somewhat, it does not use any extra information from the environment as feedback. Consequently it is\nnatural to consider other kinds of prediction targets for learning the latent encoder (and dynamics). When\nusing MCTS, it is possible compute what the policy should be for a given state, and this can be used as a\nprediction target for the reactive policy at = π(zt), which in turn can be used as a feedback signal for the\nlatent state. This method is used by MuZero (Section 4.1.3.2) and EfficientZero (Section 4.1.3.3).\n4.3.2.4\nObservation prediction\nAnother natural target to use for learning the encoder and dynamics is the next observation, using a one-step\nversion of Equation (4.14). Indeed, [Ni+24] say that a representation ϕ satsifies the OP (observation\nprediction) criterion if it satisfies the following condition:\n∃D s.t. p∗(o′|D, a) = D(o′|ϕ(D), a) ∀D, a\n(4.27)\nwhere D is the decoder. In order to repeatedly apply this, we need to be able to update the encoding z = ϕ(D)\nin a recursive or online way. Thus we must also satisfify the following recurrent encoder condition, which\n[Ni+24] call Rec:\n∃U s.t. ϕ(D′) = U(ϕ(D), a, o′) ∀D, a, o′\n(4.28)\nwhere U is the update operator. Note that belief state updates (as in a POMDP) satisfy this property.\nFurthermore, belief states are a sufficient statistic to satisfy the OP condition. See Section 4.3.1.3 for a\ndiscussion of generative models of this form. However, there are other approaches to partial observability\nwhich work directly in prediction space (see Section 4.4.2).\n4.3.2.5\nPartial observation prediction\nWe have argued that predicting all the observations is problematic, but not predicting them is also problematic.\nA natural compromise is to predict some of the observations, or at least sone function of them. This is known\nas a partial world model (see e.g., [AP23]).\nThe best way to do this is an open research problem. A simple approach would be to predict all the\nobservations, but put a penalty on the resulting OP loss term. A more sophisticated approach would be\nto structure the latent space so that we distinguish latent variables that are useful for learning Q∗(i.e.,\nwhich affect the reward and which are affected by the agent’s actions) from other latent variables that are\nneeded to explain parts of the observation but otherwise are not useful. We can then impose an information\nbottleneck penalty on the latter, to prevent the agent focusing on irrelevant observational details. (See e.g.,\nthe denoised MDP method of [Wan+22].)\n86\not\nˆzt\nD\nˆot\nLop\nP\nE\nzt−1\nU\nzt\nV\nπ\nat\nV\nVt−1\nLvp\nGt\nTD\nV t\nrt\nLzp\nFigure 4.4: Illustration of (a simplified version of) the BYOL-Explore architecture, represented as a factor graph (so\nsquares are functions, circles are variables). The dotted lines represent an optional observation prediction loss. The\nmap from notation in this figure to the paper is as follows: U →hc (closed-loop RNN update), P →ho (open-loop\nRNN update), D →g (decoder), E →f (encoder). We have unrolled the forwards prediction for only 1 step. Also, we\nhave omitted the reward prediction loss. The V node is the EMA version of the value function. The TD node is the\nTD operator.\n87\n4.3.2.6\nBYOL-Explore\nAs an example of the above framework, consider the BYOL-Explore paper [Guo+22a], which uses a\nnon-generative world model trained with ZP and VP loss. (BYOL stands for “build your own latent”.) See\nFigure 4.4 for the computation graph, which we see is slightly simpler than the Dreamer computation graph\nin Figure 4.2 due to the lack of stochastic latents. In addition to using self-prediction loss to help train the\nlatent representation, the error in this loss can be used to define an intrinsic reward, to encourage the agent\nto explore states where the model is uncertain. See Section 5.2.4 for further discussion of this topic.\n4.4\nBeyond one-step models: predictive representations\nThe “world models” we described in Section 4.3 are one-step models of the form p(s′|s, a), or p(z′|z, a)\nfor z = ϕ(s), where ϕ is a state-abstraction function. However, such models are problematic when it comes\nto predicting many kinds of future events, such as “will a car pull in front of me?” or “when will it start\nraining?”, since it is hard to predict exactly when these events will occur, and these events may correspond to\nmany different “ground states”. In principle we can roll out many possible long term futures, and apply some\nabstraction function to the resulting generated trajectories to extract features of interest, and thus derive a\npredictive model of the form p(t′, ϕ(st+1:t′)|st, π), where t′ is the random duration of the sampled trajectory.\nHowever, it would be more efficient if we could directly predict this distribution without having to know the\nvalue of t′, and without having to predict all the details of all the intermediate future states, many of which\nwill be irrelevant given the abstraction function ϕ. This motivates the study of multi-step world models,\nthat predict multiple steps into the future, either at the state level, or at the feature level. These are called\npredictive representations, and are a compromise between standard model-based RL and model-free RL,\nas we will see. Our presentation on this topic is based on [Car+24]. (See also Section 5.3, where we discuss\nthe related topic of temporal abstraction from a model-free perspective.)\n4.4.1\nGeneral value functions\nThe value function is based on predicting the sum of expected discounted future rewards. But the reward is\njust one possible signal of interest we can extract from the environment. We can generalize this by considering\na cumulant Ct ∈R, which is some scalar of interest derived from the state or observation (e.g., did a loud\nbang just occur? is there a tree visible in the image?). We then define the general value function or GVF\nas follows [Sut95]:\nV π,C,γ(s) = E\n\" ∞\nX\nt=0\nγtC(st+1)|s0 = s, a0:∞∼π\n#\n(4.29)\nIf C(st+1) = Rt+1, this reduces to the value function.5 However, we can also define the GVF to predict\ncomponents of the observation vector; this is called nexting [MWS14], since it refers to next state prediction\nat different timescales.\n4.4.2\nSuccessor representations\nIn this section we consider a variant of GVF where the cumulant corresponds to a state occupancy vector\nC(st+1) = I (st+1 = ˜s), which provides a dense feedback signal. This give us the successor representation\nor SR [Day93]:\nM π(s, ˜s) = E\n\" ∞\nX\nt=0\nγtI (st+1 = ˜s) |S0 = s\n#\n(4.30)\n5This follows the convention of [SB18], where we write (st, at, rt+1, st+1) to represent the transitions, since rt+1 and st+1\nare both generated by applying at in state st.\n88\nIf we define the policy-dependent state-transition matrix by\nT π(s, s′) =\nX\na\nπ(a|s)T(s′|s, a)\n(4.31)\nthen the SR matrix can be rewritten as\nMπ =\n∞\nX\nt=0\nγt[Tπ]t+1 = Tπ(I −γTπ)−1\n(4.32)\nThus we see that the SR replaces information about individual transitions with their cumulants, just as the\nvalue function replaces individual rewards with the reward-to-go.\nLike the value function, the SR obeys a Bellman equation\nM π(s, ˜s) =\nX\na\nπ(a|s)\nX\ns′\nT(s′|s, a) (I (s′ = ˜s) + γM π(s′, ˜s))\n(4.33)\n= E [I (s′ = ˜s) + γM π(s′, ˜s)]\n(4.34)\nHence we can learn an SR using a TD update of the form\nM π(s, ˜s) ←M π(s, ˜s) + η (I (s′ = ˜s) + γM π(s′, ˜s) −M π(s, ˜s))\n|\n{z\n}\nδ\n(4.35)\nwhere s′ is the next state sampled from T(s′|s, a).\nCompare this to the value-function TD update in\nEquation (2.16):\nV π(s) ←V π(s) + η (R(s′) + γV π(s′) −V π(s))\n|\n{z\n}\nδ\n(4.36)\nHowever, with an SR, we can easily compute the value function for any reward function (as approximated by\na given policy) as follows:\nV R,π =\nX\n˜s\nM π(s, ˜s)R(˜s)\n(4.37)\nSee Figure 4.5 for an example.\nWe can also make a version of SR that depends on the action as well as the state to get\nM π(s, a, ˜s) = E\n\" ∞\nX\nt=0\nγtI (st+1 = ˜s) |s0 = s, a0 = a, a1:∞∼π\n#\n(4.38)\n= E [I (s′ = ˜s) + γM π(s′, a, ˜s)|s0 = s, a0 = a, a1:∞∼π]\n(4.39)\nThis gives rise to a TD update of the form\nM π(s, a, ˜s) ←M π(s, a, ˜s) + η (I (s′ = ˜s) + γM π(s′, a′, ˜s) −M π(s, a, ˜s))\n|\n{z\n}\nδ\n(4.40)\nwhere s′ is the next state sampled from T(s′|s, a) and a′ is the next action sampled from π(s′). Compare this\nto the (on-policy) SARSA update from Equation (2.28):\nQπ(s, a) ←Qπ(s, a) + η (R(s′) + γQπ(s′, a′) −Qπ(s, a))\n|\n{z\n}\nδ\n(4.41)\nHowever, from an SR, we can compute the state-action value function for any reward function:\nQR,π(s, a) =\nX\n˜s\nM π(s, a, ˜s)R(˜s)\n(4.42)\n89\nGoal\nAgent\n˜s\ns\ns\nFigure 4.5: Illustration of successor representation for the 2d maze environment shown in (a) with reward shown in (d),\nwhich assings all states a reward of -0.1 except for the goal state which has a reward of 1.0. In (b-c) we show the SRs\nfor a random policy and the optimal policy. In (e-f) we show the corresponding value functons. In (b), we see that the\nSR under the random policy assigns high state occupancy values to states which are close (in Manhattan distance) to the\ncurrent state s13 (e.g., M π(s13, s14) = 5.97) and low values to states that are further away (e.g., M π(s13, s12) = 0.16).\nIn (c), we see that the SR under the optimal policy assigns high state occupancy values to states which are close to the\noptimal path to the goal (e.g., M π(s13, s14) = 1.0) and which fade with distance from the current state along that path\n(e.g., M π(s13, s12) = 0.66). From Figure 3 of [Car+24]. Used with kind permission of Wilka Carvalho. Generated by\nhttps: // github. com/ wcarvalho/ jaxneurorl/ blob/ main/ successor_ representation. ipynb .\n90\nThis can be used to improve the policy as we discuss in Section 4.4.4.1.\nWe see that the SR representation has the computational advantages of model-free RL (no need to do\nexplicit planning or rollouts in order to compute the optimal action), but also the flexibility of model-based\nRL (we can easily change the reward function without having to learn a new value function). This latter\nproperty makes SR particularly well suited to problems that use intrinsic reward (see Section 5.2.4), which\noften changes depending on the information state of the agent.\nUnfortunately, the SR is limited in several ways: (1) it assumes a finite, discrete state space; (2) it\ndepends on a given policy. We discuss ways to overcome limitation 1 in Section 4.4.3, and limitation 2 in\nSection 4.4.4.1.\n4.4.3\nSuccessor models\nIn this section, we discuss the successor model (also called a γ-model), which is a probabilistic extension\nof SR [JML20; Eys+21]. This allows us to generalize SR to work with continuous states and actions, and\nto simulate future state trajectories. The approach is to define the cumulant as the k-step conditional\ndistribution C(sk+1) = P(sk+1 = ˜s|s0 = s, π), which is the probability of being in state ˜s after following π\nfor k steps starting from state s. (Compare this to the SR cumulant, which is C(sk+1) = I (sk+1 = ˜s).) The\nSM is then defined as\nµπ(˜s|s) = (1 −γ)\n∞\nX\nt=0\nγtP(st+1 = ˜s|s0 = s)\n(4.43)\nwhere the 1 −γ term ensures that µπ integrates to 1. (Recall that P∞\nt=0 γt =\n1\n1−γ for γ < 1.) In the tabular\nsetting, the SM is just the normalized SR, since\nµπ(˜s|s) = (1 −γ)M π(s, ˜s)\n(4.44)\n= (1 −γ)E\n\" ∞\nX\nt=0\nγtI (st+1 = ˜s) |s0 = s, a0:∞∼π\n#\n(4.45)\n= (1 −γ)\n∞\nX\nt=0\nγtP(st+1 = ˜s|s0 = s, π)\n(4.46)\nThus µπ(˜s|s) tells us the probability that ˜s can be reached from s within a horizon determined by γ when\nfollowing π, even though we don’t know exactly when we will reach ˜s.\nSMs obey a Bellman-like recursion\nµπ(˜s|s) = E [(1 −γ)T(˜s|s, a) + γµπ(˜s|s′)]\n(4.47)\nWe can use this to perform policy evaluation by computing\nV π(s) =\n1\n1 −γ Eµπ(˜s|s) [R(˜s)]\n(4.48)\nWe can also define an action-conditioned SM\nµπ(˜s|s, a) = (1 −γ)\n∞\nX\nt=0\nγtP(st+1 = ˜s|s0 = s, a0 = a)\n(4.49)\n= (1 −γ)T(˜s|s, a) + γE [µπ(˜s|s′, a′, π)]\n(4.50)\nHence we can learn an SM using a TD update of the form\nµπ(˜s|s, a) ←µπ(˜s|s, a) + η ((1 −γ)T(s′|s, a) + γµπ(˜s|s′, a′) −µπ(˜s|s, a))\n|\n{z\n}\nδ\n(4.51)\n91\nwhere s′ is the next state sampled from T(s′|s, a) and a′ is the next action sampled from π(s′).\nWith an\nSM, we can compute the state-action value for any reward:\nQR,π(s, a) =\n1\n1 −γ Eµπ(˜s|s,a) [R(˜s)]\n(4.52)\nThis can be used to improve the policy as we discuss in Section 4.4.4.1.\n4.4.3.1\nLearning SMs\nAlthough we can learn SMs using the TD update in Equation (4.51), this requires evaluating T(s′|s, a) to\ncompute the target update δ, and this one-step transition model is typically unknown. Instead, since µπ is a\nconditional density model, we will optimize the cross-entropy TD loss [JML20], defined as follows\nLµ = E(s,a)∼p(s,a),˜s∼(T πµπ)(·|s,a) [log µθ(˜s|s, a)]\n(4.53)\nwhere (T πµπ)(·|s, a) is the Bellman operator applied to µπ and then evaluated at (s, a), i.e.,\n(T πµπ)(˜s|s, a) = (1 −γ)T(s′|s, a) + γ\nX\ns′\nT(s′|s, a)\nX\na′\nπ(a′|s′(µπ(˜s|s′, a′)\n(4.54)\nWe can sample from this as follows: first sample s′ ∼T(s′|s, a) from the environment and then with probability\n1 −γ set ˜s = s′ and terminate. Otherwise sample a′ ∼π(a′|s′) and then create a bootstrap sample from the\nmodel using ˜s ∼µπ(˜s|s′, a′).\nThere are many possible density models we can use for µπ. In [Tha+22], they use a VAE. In [Tom+24],\nthey use an autoregressive transformer applied to a set of discrete latent tokens, which are learned using\nVQ-VAE or a non-reconstructive self-supervised loss. They call their method Video Occcupancy Models.\nAn alternative approach to learning SMs, that avoids fitting a normalized density model over states, is to\nuse contrastive learning to estimate how likely ˜s is to occur after some number of steps, given (s, a), compared\nto some randomly sampled negative state [ESL21; ZSE24]. Although we can’t sample from the resulting\nlearned model (we can only use it for evaluation), we can use it to improve a policy that achieves a target\nstate (an approach known as goal-conditioned policy learning, discussed in Section 5.3.1).\n4.4.3.2\nJumpy models using geometric policy composition\nIn [Tha+22], they propose geometric policy composition or GPC as a way to learn a new policy by\nsequencing together a set of N policies, as opposed to taking N primitive actions in a row. This can be\nthought of as a jumpy model, since it predicts multiple steps into the future, instead of one step at a time\n(c.f., [Zha+23a]).\nIn more detail, in GPC, the agent picks a sequence of n policies πi for i = 1 : n, and then samples states\naccording to their corresponding SMs: starting with (s0, a0), we sample s1 ∼µπ1\nγ (·|s0, a0), then a1 ∼π1(·|s1),\nthen s2 ∼µπ2\nγ (·|s1, a1), etc. This continues for n −1 steps. Finally we sample sn ∼µπn\nγ′ (·|sn−1, an−1), where\nγ′ > γ represents a longer horizon SM. The reward estimates computed along this sampled path can then be\ncombined to compute the value of each candidate policy sequence.\n4.4.4\nSuccessor features\nBoth SRs and SMs require defining expectations or distributions over the entire future state vector, which can\nbe problematic in high dimensional spaces. In [Bar+17] they introduced successor features, that generalize\nSRs by working with features ϕ(s) instead of primitive states. In particular, if we define the cumulant to be\nC(st+1) = ϕ(st+1), we get the following definition of SF:\nψπ,ϕ(s) = E\n\" ∞\nX\nt=0\nγtϕ(st+1)|s0 = s, a0:∞∼π\n#\n(4.55)\n92\nWe will henceforth drop the ϕ superscript from the notation, for brevity. SFs obey a Bellman equation\nψ(s) = E [ϕ(s′) + γψ(s′)]\n(4.56)\nIf we assume the reward function can be written as\nR(s, w) = ϕ(s)Tw\n(4.57)\nthen we can derive the value function for any reward as follows:\nV π,w(s) = E [R(s1) + γR(s2) + · · · |s0 = s]\n(4.58)\n= E\n\u0002\nϕ(s1)Tw + γϕ(s2)Tw + · · · |s0 = s\n\u0003\n(4.59)\n= E\n\" ∞\nX\nt=0\nγtϕ(st+1)|s0 = s\n#T\nw = ψπ(s)Tw\n(4.60)\nSimilarly we can define an action-conditioned version of SF as\nψπ,ϕ(s, a) = E\n\" ∞\nX\nt=0\nϕ(st+1)|s0 = s, a0 = a, a1:∞∼π\n#\n(4.61)\n= E [ϕ(s′) + γψ(s′, a′)]\n(4.62)\nWe can learn this using a TD rule\nψπ(s, a) ←ψπ(s, a) + η (ϕ(s′) + γψπ(s′, a′) −ψπ(s, a))\n|\n{z\n}\nδ\n(4.63)\nAnd we can use it to derive a state-action value function:\nQπ,w(s) = ψπ(s, a)Tw\n(4.64)\nThis allows us to define multiple Q functions (and hence policies) just by changing the weight vector w, as\nwe discuss in Section 4.4.4.1.\n4.4.4.1\nGeneralized policy improvement\nSo far, we have discussed how to compute the value function for a new reward function but using the SFs\nfrom an existing known policy. In this section we discuss how to create a new policy that is better than an\nexisting set of policies, by using Generalized Policy Improvement or GPI [Bar+17; Bar+20].\nSuppose we have learned a set of N (potentially optimal) policies πi and their corresponding SFs ψπi for\nmaximizing rewards defined by wi. When presented with a new task wnew, we can compute a new policy\nusing GPI as follows:\na∗(s; wnew) = argmax\na\nmax\ni\nQπi(s, a, wnew) = argmax\na\nmax\ni\nψπi(s, a)Twnew\n(4.65)\nIf wnew is in the span of the training tasks (i.e., there exist weights αi such that wnew\nP\ni αiwi), then the GPI\ntheorem states that π(a|s) = I (a = a∗(s, wnew)) will perform at least as well as any of the existing policies,\ni.e., Qπ(s, a) ≥maxi Qπi(s, a) (c.f., policy improvement in Section 3.4). See Figure 4.6 for an illustration.\nNote that GPI is a model-free approach to computing a new policy, based on an existing library of policies.\nIn [Ale+23], they propose an extension that can also leverage a (possibly approximate) world model to learn\nbetter policies that can outperform the library of existing policies by performing more decision-time search.\n93\nLegend\nApple\nMilk\nFork\nKnife\nState features φ\n. . .\nt\nφ1\nφT\nState features agent experiences over time\n⇡drawer = Open Drawer\nTask Policies\nOpen Drawer\nw\nTask\nC\nmEwm7dDJg5kbpYQs3Pgrblwo4taPcOfOG2z0NYDFw7n3Mu9/ip4Aos69tYWl5ZXVuvbFQ3t7Z3ds29/Y5KMklZmyYikT2fKCZ4zNrAQbBeKhmJfMG6/uh64nfvmVQ8ie9gnDI3IoOYh5wS0Jn1oiXQ4EdxSPspNzLHQAcSPLAZOGZdathTYEXiV2SOirR8swvJ0\nhoFrEYqCBK9W0rBTcnEjgVrKg6mWIpoSMyYH1NYxIx5ebTJwp8pJUAh4nUFQOeqr8nchIpNY583RkRGKp5byL+5/UzC/dnMdpBiyms0VhJjAkeJIDrhkFMRYE0Il17diOiSUNC5VXUI9vzLi6Rz0rDPG2e3p/XmVRlHBdXQITpGNrpATXSDWqiNKHpEz+gVvRlPx\novxbnzMWpeMcuYA/YHx+QPo/JhJ</latexit>at ⇠⇡drawer\n ⇡drawer\n. . .\n ⇡fridge\n⇡fridge = Open Fridge\nOpen Fridge\nat ⇠⇡fridge\n(a) Successor Features\n ⇡= E⇡\n⇥\nφ1 + γφ2 + γ2φ3 + . . .\n⇤\n=\n⊙\nb\npQSsvFX3LhQxK2f4c6/cdpmoa0HLhzOuZd7/FTwRVY1rextLyurZe2ahubm3v7Jp7+x2VZJKyNk1EIns+UzwmLWBg2C9VDIS+YJ1/dH1xO8+MKl4Et/BOGVuRAYxDzkloCXPHRSxe9zJ+Ve7gDgQJHJovCM2tW3ZoCLxK7JDVUouWZX06Q0CxiMVBlOrbVgpuTiRwKlhRdTLFUkJHZMD6msYkYs\nrNpw8U+EQrAQ4TqSsGPFV/T+QkUmoc+bozIjBU895E/M/rZxBeujmP0wxYTGeLwkxgSPAkDRxwySiIsSaESq5vxXRIJKGgM6vqEOz5lxdJ56xun9cbt41a86qMo4KO0DE6RTa6QE10g1qojSgq0DN6RW/Gk/FivBsfs9Ylo5w5QH9gfP4Ak0iXDQ=</latexit> ⇡drawer\nwmilk\n⊙\nwmilk\n ⇡fridge\nQ⇡drawer\nwmilk\nQ⇡fridge\nwmilk\n...\n=\n(b) Generalized Policy Improvement\nMax\n⇡milk\n⇡(a|s) / max\n⇡i { ⇡i(s, a)>wnew}\nNew task: Get milk\nFigure 4.6: Illustration of successor features representation. (a) Here ϕt = ϕ(st) is the vector of features for the state\nat time t, and ψπ is the corresponding SF representation, which depends on the policy π. (b) Given a set of existing\npolicies and their SFs, we can create a new one by specifying a desired weight vector wnew and taking a weighted\ncombination of the existing SFs. From Figure 5 of [Car+24]. Used with kind permission of Wilka Carvalho.\n4.4.4.2\nOption keyboard\nOne limitation of GPI is that it requires that the reward function, and the resulting policy, be defined in\nterms of a fixed weight vector wnew, where the preference over features is constant over time. However, for\nsome tasks we might want to initially avoid a feature or state and then later move towards it. To solve this,\n[Bar+19; Bar+20] introduced the option keyboard, in which the weight vector for a task can be computed\ndynamically in a state-dependent way, using ws = g(s, wnew). (Options are discussed in Section 5.3.2.)\nActions can then be chosen as follows:\na∗(s; wnew) = argmax\na\nmax\ni\nψπi(s, a)Tws\n(4.66)\nThus the policy πi that is chosen depends in the current state. Thus ws induces a set of policies that are\nactive for a period of time, similar to playing a chord on a piano.\n4.4.4.3\nLearning SFs\nA key question when using SFs is how to learn the cumulants or state-features ϕ(s). Various approaches\nhave been suggested, including leveraging meta-gradients [Vee+19], image reconstruction [Mac+18b], and\nmaximizing the mutual information between task encodings and the cumulants that an agent experiences\nwhen pursuing that task [Han+19]. The cumulants are encouraged to satisfies the linear reward constraint by\nminimizing\nLr = ||r −ϕθ(s)Tw||2\n2\n(4.67)\nOnce the cumulant function is known, we have to learn the corresponding SF. The standard approach\nlearns a different SF for every policy, which is limiting. In [Bor+19] they introduced Universal Successor\nFeature Approximators which takes an input a policy encoding zw, representing a policy πw (typically\nwe set zw = w). We then define\nψπw(s, a) = ψθ(s, a, zw)\n(4.68)\n94\nThe GPI update then becomes\na∗(s; wnew) = argmax\na\nmax\nzw ψθ(s, a, zw)Twnew\n(4.69)\nso we replace the discrete max over a finite number of policies with a continuous optimization problem (to be\nsolved per state).\nIf we want to learn the policies and SFs at the same time, we can optimize the following losses in parallel:\nLQ = ||ψθ(s, a, zw)Tw −yQ||, yQ = R(s′; w) + γψθ(s′, a∗, zw)Tw\n(4.70)\nLψ = ||ψθ(s, a, zw) −yψ||, yψ = ϕ(s′) + γψθ(s′, a∗, zw)\n(4.71)\nwhere a∗= argmaxa′ ψθ(s′, a′, zw)Tw. The first equation is standard Q learning loss, and the second is\nthe TD update rule in Equation (4.63) for the SF. In [Car+23], they present the Successor Features\nKeyboard, that can learn the policy, the SFs and the task encoding zw, all simultaneously. They also\nsuggest replacing the squared error regression loss in Equation (4.70) with a cross-entropy loss, where each\ndimension of the SF is now a discrete probability distribution over M possible values of the corresponding\nfeature. (c.f. Section 5.1.2).\n4.4.4.4\nChoosing the tasks\nA key advantage of SFs is that they provide a way to compute a value function and policy for any given\nreward, as specified by a task-specific weight vector w. But how do we choose these tasks? In [Han+19] they\nsample w from a distribution at the start of each task, to encourage the agent to learn to explore different\nparts of the state space (as specified by the feature function ϕ). In [LA21] they extend this by adding an\nintrinsic reward that favors exploring parts of the state space that are surprising (i.e., which induce high\nentropy), c.f., Section 5.2.4. In [Far+23], they introduce proto-value networks, which is a way to define\nauxiliary tasks based on successor measures.\n95\n96\nChapter 5\nOther topics in RL\nIn this section, we briefly mention some other important topics in RL.\n5.1\nDistributional RL\nThe distributional RL approach of [BDM17; BDR23], predicts the distribution of (discounted) returns, not\njust the expected return. More precisely, let Zπ = PT\nt=0 γtrt be a random variable representing the reward-to-\ngo. The standard value function is defined to compute the expectation of this variable: V π(s) = E [Zπ|s0 = s].\nIn DRL, we instead attempt to learn the full distribution, p(Zπ|s0 = s). For a general review of distributional\nregression, see [KSS23]. Below we briefly mention a few algorithms in this class that have been explored in\nthe context of RL.\n5.1.1\nQuantile regression methods\nAn alternative to predicting a full distribution is to predict a fixed set of quantiles. This is called quantile\nregression, and has been used with DQN in [Dab+17] to get QR-DQN, and with SAC in [Wur+22] to get\nQR-SAC. (The latter was used in Sony’s GTSophy Gran Turismo AI racing agent.)\n5.1.2\nReplacing regression with classification\nAn alternative to quantile regression is to approximate the distribution over returns using a histogram, and\nthen fit it using cross entropy loss (see Figure 5.1). This approach was first suggested in [BDM17], who called\nit categorical DQN. (In their paper, they use 51 discrete categories (atoms), giving rise to the name C51.)\nCategorical Distributional RL\nTwo-Hot\nHLGauss\nFigure 5.1: Illustration of how to encode a scalar target y or distributional target Z using a categorical distribution.\nFrom Figure 1 of [Far+24]. Used with kind permission of Jesse Farebrother.\n97\nAn even simpler approach is to replace the distributional target with the standard scalar target (representing\nthe mean), and then discretize this target and use cross entropy loss instead of squared error.1 Unfortunately,\nthis encoding is lossy. In [Sch+20], they proposed the two-hot transform, that is a lossless encoding of the\ntarget based on putting appropriate weight on the nearest two bins (see Figure 5.1). In [IW18], they proposed\nthe HL-Gauss histogram loss, that convolves the target value y with a Gaussian, and then discretizes the\nresulting continuous distribution. This is more symetric than two-hot encoding, as shown in Figure 5.1.\nRegardless of how the discrete target is chosen, predictions are made using ˆy(s; θ) = P\nk pk(s)bk, where pk(s)\nis the probability of bin k, and bk is the bin center.\nIn [Far+24], they show that the HL-Gauss trick works much better than MSE, two-hot and C51 across a\nvariety of problems (both offline and online), especially when they scale to large networks. They conjecture\nthat the reason it beats MSE is that cross entropy is more robust to noisy targets (e.g., due to stochasticity)\nand nonstationary targets. They also conjecture that the reason HL works better than two-hot is that HL is\ncloser to ordinal regression, and reduces overfitting by having a softer (more entropic) target distribution\n(similiar to label smoothing in classification problems).\n5.2\nReward functions\nSequential decision making relies on the user to define the reward function in order to encourage the agent to\nexhibit some desired behavior. In this section, we discuss this crucial aspect of the problem.\n5.2.1\nReward hacking\nIn some cases, the reward function may be misspecified, so even though the agent may maximize the reward, this\nmight turn out not to be what the user desired. For example, suppose the user rewards the agent for making as\nmany paper clips as possible. An optimal agent may convert the whole world into a paper clip factory, because\nthe user forgot to specify various constraints, such as not killing people or not destroying the environment.\nIn the AI alignment community, this example is known as the paperclip maximizer problem, and is\ndue to Nick Bostrom [Bos16]. (See e.g., https://openai.com/index/faulty-reward-functions/ for some\nexamples that have occurred in practice.) This is an example of a more general problem known as reward\nhacking [Ska+22]. For a potential solution, based on the assistance game paradigm, see Section 5.6.1.2.\n5.2.2\nSparse reward\nEven if the reward function is correct, optimizing it is not always easy. In particular, many problems suffer\nfrom sparse reward, in which R(s, a) = 0 for almost all states and actions, so the agent only every gets\nfeedback (either positive or negative) on the rare occasions when it achieves some unknown goal. This\nrequires deep exploration [Osb+19] to find the rewarding states. One approach to this is use to use PSRL\n(Section 1.4.4.2). However, various other heuristics have been developed, some of which we discuss below.\n5.2.3\nReward shaping\nIn reward shaping, we add prior knowledge about what we believe good states should look like, as a way to\ncombat the difficulties of learning from sparse reward. That is, we define a new reward function r′ = r + F,\nwhere F is called the shaping function. In general, this can affect the optimal policy. For example, if a\nsoccer playing agent is “artificially” rewarded for making contact with the ball, it might learn to repeatedly\ntouch and untouch the ball (toggling between s and s′), rather than trying to win the original game. But in\n[NHR99], the prove that if the shaping function has the form\nF(s, a, s′) = γΦ(s′) −Φ(s)\n(5.1)\n1Technically speaking, this is no longer a distributional RL method, since the prediction target is the mean, but the mechanism\nfor predicting the mean leverages a distribution, for robustness and ease of optimization.\n98\nwhere Φ : S →R is a potential function, then we can guarantee that the sum of shaped rewards will match\nthe sum of original rewards plus a constant. This is called Potential-Based Reward Shaping.\nIn [Wie03], they prove that (in the tabular case) this approach is equivalent to initializing the value\nfunction to V (s) = Φ(s). In [TMM19], they propose an extension called potential-based advice, where they\nshow that a potential of the form F(s, a, s′, a′) = γΦ(s′, a′) −Φ(s, a) is also valid (and more expressive). In\n[Hu+20], they introduce a reward shaping function z which can be used to down-weight or up-weight the\nshaping function:\nr′(s, a) = r(s, a) + zϕ(s, a)F(s, a)\n(5.2)\nThey use bilevel optimization to optimize ϕ wrt the original task performance.\n5.2.4\nIntrinsic reward\nWhen the extrinsic reward is sparse, it can be useful to (also) reward the agent for solving “generally useful”\ntasks, such as learning about the world. This is called intrinsically motivated RL [AMH23; Lin+19;\nAmi+21; Yua22; Yua+24; Col+22]. It can be thought of as a special case of reward shaping, where the\nshaping function is dynamically computed.\nWe can classify these methods into two main types: knowledge-based intrinsic motivation, or\nartificial curiosity, where the agent is rewarded for learning about its environment; and competence-\nbased intrinsic motivation, where the agent is rewarded for achieving novel goals or mastering new\nskills.\n5.2.4.1\nKnowledge-based intrinsic motivation\nOne simple approach to knowledge-based intrinsic motivation is to add to the extrinsic reward an intrinsic\nexploration bonus Ri\nt(st), which is high when the agent visits novel states. For tabular environments, we\ncan just count the number of visits to each state, Nt(s), and define Ri\nt(s) = 1/Nt(s) or Ri\nt(s) = 1/\np\nNt(s),\nwhich is similar to the UCB heuristic used in bandits (see Section 1.4.3). We can extend exploration bonuses\nto high dimensional states (e.g. images) using density models [Bel+16]. Alternatively, [MBB20] propose to\nuse the ℓ1 norm of the successor feature (Section 4.4.4) representation as an alternative to the visitation\ncount, giving rise to an intrinsic reward of the form Ri(s) = 1/||ψπ(s)||1. Recently [Yu+23] extended this to\ncombine SFs with predecessor representations, which encode retrospective information about the previous\nstate (c.f., inverse dynamics models, mentioned below). This encourages exploration towards bottleneck\nstates.\nAnother approach is the Random Network Distillation or RND method of [Bur+18]. This uses a\nfixed random neural network feature extractor zt = f(st; θ∗) to define a target, and then trains a predictor\nˆzt = f(st; ˆθt) to predict these targets. If st is similar to previously seen states, then the trained model\nwill have low prediction error. We can thus define the intrinsic reward as proportional to the squared\nerror ||ˆzt −zt||2\n2. The BYOL-Explore method of [Guo+22b] goes beyond RND by learning the target\nrepresentation (for the next state), rather than using a fixed random projection, but is still based on prediction\nerror.\nWe can also define an intrinsic reward in terms of the information theoretic surprise of the next state\ngiven the current one:\nR(s, a, s′) = −log q(s′|s, a)\n(5.3)\nThis is the same as methods based on rewarding states for prediction error. Unfortunately such methods can\nsuffer from the noisy TV problem (also called a stochastic trap), in which an agent is attracted to states\nwhich are intrinsically to predict. To see this, note that by averaging over future states we see that the above\nreward reduces to\nR(s, a) = −Ep∗(s′|s,a) [log q(s′|s, a)] = Hce(p∗, q)\n(5.4)\nwhere p∗is the true model and q is the learned dynamics model, and Hce is the cros -entropy. As we learn\nthe optimal model, q = p∗, this reduces to the conditional entropy of the predictive distribution, which can\nbe non-zero for inherently unpredictable states.\n99\nTo help filter out such random noise, [Pat+17] proposes an Intrinsic Curiosity Module. This first\nlearns an inverse dynamics model of the form a = f(s, s′), which tries to predict which action was used,\ngiven that the agent was in s and is now in s′. The classifier has the form softmax(g(ϕ(s), ϕ(s′), a)), where\nz = ϕ(s) is a representation function that focuses on parts of the state that the agent can control. Then the\nagent learns a forwards dynamics model in z-space. Finally it defines the intrinsic reward as\nR(s, a, s′) = −log q(ϕ(s′)|ϕ(s), a)\n(5.5)\nThus the agent is rewarded for visiting states that lead to unpredictable consequences, where the difference\nin outcomes is measured in a (hopefully more meaningful) latent space.\nAnother solution is to replace the cross entropy with the KL divergence, R(s, a) = DKL(p||q) = Hce(p, q)−\nH(p), which goes to zero once the learned model matches the true model, even for unpredictable states.\nThis has the desired effect of encouraging exploration towards states which have epistemic uncertainty\n(reducible noise) but not aleatoric uncertainty (irreducible noise) [MP+22]. The BYOL-Hindsight method\nof [Jar+23] is one recent approach that attempts to use the R(s, a) = DKL(p||q) objective. Unfortunately,\ncomputing the DKL(p||q) term is much harder than the usual variational objective of DKL(q||p). A related\nidea, proposed in the RL context by [Sch10], is to use the information gain as a reward. This is defined\nas Rt(st, at) = DKL(q(st|ht, at, θt)||q(st|ht, at, θt−1), where ht is the history of past observations, and\nθt = update(θt−1, ht, at, st) are the new model parameters. This is closely related to the BALD (Bayesian\nActive Learning by Disagreement) criterion [Hou+11; KAG19], and has the advantage of being easier to\ncompute, since it is does not reference the true distribution p.\n5.2.4.2\nGoal-based intrinsic motivation\nWe will discuss goal-conditioned RL in Section 5.3.1. If the agent creates its own goals, then it provides\na way to explore the environment. The question of when and how an agent to switch to pursuing a new\ngoal is studied in [Pis+22] (see also [BS23]). Some other key work in this space includes the scheduled\nauxiliary control method of [Rie+18], and the Go Explore algorithm in [Eco+19; Eco+21] and its recent\nLLM extension [LHC24].\n5.3\nHierarchical RL\nSo far we have focused on MDPs that work at a single time scale. However, this is very limiting. For example,\nimagine planning a trip from San Francisco to New York: we need to choose high level actions first, such as\nwhich airline to fly, and then medium level actions, such as how to get to the airport, followed by low level\nactions, such as motor commands. Thus we need to consider actions that operate multiple levels of temporal\nabstraction. This is called hierarchical RL or HRL. This is a big and important topic, and we only brief\nmention a few key ideas and methods. Our summary is based in part on [Pat+22]. (See also Section 4.4\nwhere we discuss multi-step predictive models; by contrast, in this section we focus on model-free methods.)\n5.3.1\nFeudal (goal-conditioned) HRL\nIn this section, we discuss an approach to HRL known as feudal RL [DH92]. Here the action space of the\nhigher level policy consists of subgoals that are passed down to the lower level policy. See Figure 5.2 for an\nillustration. The lower level policy learns a universal policy π(a|s, g), where g is the goal passed into it\n[Sch+15a]. This policy optimizes an MDP in which the reward is define as R(s, a|g) = 1 iff the goal state is\nachieved, i.e., R(s, a|s) = I (s = g). (We can also define a dense reward signal using some state abstraction\nfunction ϕ, by definining R(s, a|g) = sim(ϕ(s), ϕ(g)) for some similarity metric.) This approach to RL is\nknown as goal-conditioned RL [LZZ22].\n100\nState\n𝜋0(s,g)\nPrimitive Action\nGoal\nState\n𝜋1(s,g)\nSubgoal\nGoal\nState\n𝜋2(s,g)\nSubgoal\nGoal\nFigure 5.2: Illustration of a 3 level hierarchical goal-conditioned controller. From http: // bigai. cs. brown. edu/\n2019/ 09/ 03/ hac. html . Used with kind permission of Andrew Levy.\n5.3.1.1\nHindsight Experience Relabeling (HER)\nIn this section, we discuss an approach to efficiently learning goal-conditioned policies, in the special case\nwhere the set of goal states G is the same as the set of original states S. We will extend this to the hierarchical\ncase below.\nThe basic idea is as follows. We collect various trajectores in the environment, from a starting state s0 to\nsome terminal state sT , and then define the goal of each trajectory as being g = sT ; this trajectory then\nserves as a demonstration of how to achieve this goal. This is called hindsight experience relabeling\nor HER [And+17]. This can be used to relabel the trajectories stored in the replay buffer. That is, if we\nhave (s, a, R(s|g), s′, g) tuples, we replace them with (s, a, R(s|g′), g′) where g′ = sT . We can then use any\noff-policy RL method to learn π(a|s, g). In [Eys+20], they show that HER can be viewed as a special case of\nmaximum-entropy inverse RL, since it is estimating the reward for which the corresponding trajectory was\noptimal.\n5.3.1.2\nHierarchical HER\nWe can leverage HER to learn a hierarchical controller in several ways. In [Nac+18] they propose HIRO\n(Hierarchical Reinforcement Learning with Off-policy Correction) as a way to train a two-level controller.\n(For a two-level controller, the top level is often called the manager, and the low level the worker.) The\ndata for the manager are transition tuples of the form (st, gt, P rt:t+c, st+c), where c is the time taken for\nthe worker to reach the goal (or some maximum time), and rt is the main task reward function at step t.\nThe data for the worker are transition tuples of the form (st+i, gt, at+i, rgt\nt+i, st+i+1) for i = 0 : c, where rg\nt\nis the reward wrt reaching goal g. This data can be used to train the two policies. However, if the worker\nfails to achieve the goal in the given time limit, all the rewards will be 0, and no learning will take place. To\ncombat this, if the worker does not achieve gt after c timesteps, the subgoal is relabeled in the transition\ndata with another subgoal g′\nt which is sampled from p(g|τ), where τ is the observed trajectory. Thus both\npolicies treat g′\nt as the goal in hindsight, so they can use the actually collected data for training\nThe hierarchical actor critic (HAC) method of [Lev+18] is a simpler version of HIRO that can be\nextended to multiple levels of hierarchy, where the lowest level corresponds to primitive actions (see Figure 5.2).\nIn the HAC approach, the output subgoal in the higher level data, and the input subgoal in the lower-level\ndata, are replaced with the actual state that was achieved in hindsight. This allows the training of each level of\nthe hierarchy independently of the lower levels, by assuming the lower level policies are already optimal (since\nthey achieved the specified goal). As a result, the distribution of (s, a, s′) tuples experienced by a higher level\n101\nwill be stable, providing a stationary learning target. By contrast, if all policies are learned simultaneously,\nthe distribution becomes non-stationary, which makes learning harder. For more details, see the paper, or\nthe corresponding blog post (with animations) at http://bigai.cs.brown.edu/2019/09/03/hac.html.\n5.3.1.3\nLearning the subgoal space\nIn the previous approaches, the subgoals are defined in terms of the states that were achieved at the end of\neach trajectory, g′ = sT . This can be generalized by using a state abstraction function to get g′ = ϕ(sT ). The\nmethods in Section 5.3.1.2) assumed that ϕ was manually specified. We now mention some ways to learn ϕ.\nIn [Vez+17], they present Feudal Networks for learning a two level hierarchy. The manager samples\nsubgoals in a learned latent subgoal space. The worker uses distance to this subgoal as a reward, and is\ntrained in the usual way. The manager uses the “transition gradient” as a reward, which is derived from the\ntask reward as well as the distance between the subgoal and the actual state transition made by the worker.\nThis reward signal is used to learn the manager policy and the latent subgoal space.\nFeudal networks do not guarantee that the learned subgoal space will result in optimal behavior. In\n[Nac+19], they present a method to optimize the policy and ϕ function so as to minimize a bound on the\nsuboptimality of the hierarchical policy. This approach is combined with HIRO (Section 5.3.1.2) to tackle the\nnon-stationarity issue.\n5.3.2\nOptions\nThe feudal approach to HRL is somewhat limited, since not all subroutines or skills can be defined in terms\nof reaching a goal state (even if it is a partially specified one, such as being in a desired location but without\nspecifying the velocity). For example, consider the skill of “driving in a circle”, or “finding food”. The options\nframework is a more general framework for HRL first proposed in [SPS99]. We discuss this below.\n5.3.2.1\nDefinitions\nAn option ω = (I, π, β) is a tuple consisting of: the initiation set Iω ⊂S, which is a subset of states that this\noption can start from (also called the affordances of each state [Khe+20]); the subpolicy πω(a|s) ∈[0, 1];\nand the termination condition βω(s) ∈[0, 1], which gives the probability of finishing in state s. (This\ninduces a geometric distribution over option durations, which we denote by τ ∼βω.) The set of all options is\ndenoted Ω.\nTo execute an option at step t entails choosing an action using at = πω(st) and then deciding whether to\nterminate at step t + 1 with probability 1 −βω(st+1) or to continue following the option at step t + 1. (This\nis an example of a semi-Markov decision process [Put94].) If we define πω(s) = a and βω(s) = 0 for all\ns, then this option corresponds to primitive action a that terminates in one step. But with options we can\nexpand the repertoire of actions to include those that take many steps to finish.\nTo create an MDP with options, we need to define the reward function and dynamics model. The reward\nis defined as follows:\nR(s, ω) = E\n\u0002\nR1 + γR2 + · · · + γτ−1Rτ|S0 = s, A0:τ−1 ∼πω, τ ∼βω\n\u0003\n(5.6)\nThe dynamics model is defined as follows:\npγ(s′|s, ω) =\n∞\nX\nk=1\nγk Pr (Sk = s′, τ = k|S0 = s, A0:k−1 ∼πω, τ ∼βω)\n(5.7)\nNote that pγ(s′|s, ω) is not a conditional probability distribution, because of the γk term, but we can usually\ntreat it like one. Note also that a dynamics model that can predict multiple steps ahead is sometimes called\na jumpy model (see also Section 4.4.3.2).\n102\nWe can use these definitions to define the value function for a hierarchical policy using a generalized\nBellman equation, as follows:\nVπ(s) =\nX\nω∈Ω(s)\nπ(ω|s)\n\"\nR(s, ω) +\nX\ns′\npγ(s′|s, ω)Vπ(s′)\n#\n(5.8)\nWe can compute this using value iteration. We can then learn a policy using policy iteration, or a policy\ngradient method. In other words, once we have defined the options, we can use all the standard RL machinery.\nNote that GCRL can be considered a special case of options where each option corresponds to a different\ngoal. Thus the reward function has the form R(s, ω) = I (s = ω), the termination function is βω(s) = I (s = ω),\nand the initiation set is the entire state space.\n5.3.2.2\nLearning options\nThe early work on options, including the MAXQ approach of [Die00], assumed that the set of options was\nmanually specified. Since then, many methods for learning options have been proposed. We mention a few of\nthese below.\nThe first set of methods for option learning rely on two stage training. In the first stage, exploration\nmethods are used to collect trajectories. Then this data is analysed, either by inferring hidden segments using\nEM applied to a latent variable model [Dan+16], or by using the skill chaining method of [KB09], which\nuses classifiers to segment the trajectories. The labeled data can then be used to define a set of options,\nwhich can be trained using standard methods.\nThe second set of methods for option learning use end-to-end training, i.e., the options and their policies\nare jointly learned online. For example, [BHP17] propose the option-critic architecture. The number\nof options is manually specified, and all policies are randomly initialized. Then they are jointly trained\nusing policy gradient methods designed for semi-MDPs. (See also [RLT18] for a hierarchical extension of\noption-critic to support options calling options.) However, since the learning signal is just the main task\nreward, the method can work poorly in problems with sparse reward compared to subgoal methods (see\ndiscussion in [Vez+17; Nac+19]).\nAnother problem with option-critic is that it requires specialized methods that are designed for optimizing\nsemi-MDPs. In [ZW19], they propose double actor critic, which allows the use of standard policy gradient\nmethods. This works by defining two parallel augmented MDPs, where the state space of each MDP is the\ncross-product of the original state space and the set of options. The manager learns a policy over options, and\nthe worker learns a policy over states for each option. Both MDPs just use task rewards, without subgoals or\nsubtask rewards.\nIt has been observed that option learning using option-critic or double actor-critic can fail, in the sense\nthat the top level controller may learn to switch from one option to the next at almost every time step [ZW19;\nHar+18]. The reason is that the optimal policy does not require the use of temporally extended options, but\ninstead can be defined in terms of primitive actions (as in standard RL). Therefore in [Har+18] they propose\nto add a regularizer called the deliberation cost, in which the higher level policy is penalized whenever it\nswitches options. This can speed up learning, at the cost of a potentially suboptimal policy.\nAnother possible failure mode in option learning is if the higher level policy selects a single option for\nthe entire task duration. To combat this, [KP19] propose the Interest Option Critic, which learns the\ninitiation condition Iω so that the option is selected only in certain states of interest, rather than the entire\nstate space.\nIn [Mac+23], they discuss how the successor representation (discussed in Section 4.4) can be used to\ndefine options, using a method they call the Representation-driven Option Discovery (ROD) cycle.\nIn [Lin+24b] they propose to represent options as programs, which are learned using LLMs.\n103\n5.4\nImitation learning\nIn previous sections, an RL agent is to learn an optimal sequential decision making policy so that the total\nreward is maximized. Imitation learning (IL), also known as apprenticeship learning and learning\nfrom demonstration (LfD), is a different setting, in which the agent does not observe rewards, but has access\nto a collection Dexp of trajectories generated by an expert policy πexp; that is, τ = (s0, a0, s1, a1, . . . , sT )\nand at ∼πexp(st) for τ ∈Dexp. The goal is to learn a good policy by imitating the expert, in the absence\nof reward signals. IL finds many applications in scenarios where we have demonstrations of experts (often\nhumans) but designing a good reward function is not easy, such as car driving and conversational systems.\n(See also Section 5.5, where we discuss the closely related topic of offline RL, where we also learn from a\ncollection of trajectories, but no longer assume they are generated by an optimal policy.)\n5.4.1\nImitation learning by behavior cloning\nA natural method is behavior cloning, which reduces IL to supervised learning; see [Pom89] for an early\napplication to autonomous driving. It interprets a policy as a classifier that maps states (inputs) to actions\n(labels), and finds a policy by minimizing the imitation error, such as\nmin\nπ Epγ\nπexp(s) [DKL (πexp(s) ∥π(s))]\n(5.9)\nwhere the expectation wrt pγ\nπexp may be approximated by averaging over states in Dexp. A challenge with\nthis method is that the loss does not consider the sequential nature of IL: future state distribution is not\nfixed but instead depends on earlier actions. Therefore, if we learn a policy ˆπ that has a low imitation error\nunder distribution pγ\nπexp, as defined in Equation (5.9), it may still incur a large error under distribution pγ\nˆπ\n(when the policy ˆπ is actually run). This problem has been tackled by the offline RL literature, which we\ndiscuss in Section 5.5.\n5.4.2\nImitation learning by inverse reinforcement learning\nAn effective approach to IL is inverse reinforcement learning (IRL) or inverse optimal control (IOC).\nHere, we first infer a reward function that “explains” the observed expert trajectories, and then compute a\n(near-)optimal policy against this learned reward using any standard RL algorithms studied in earlier sections.\nThe key step of reward learning (from expert trajectories) is the opposite of standard RL, thus called inverse\nRL [NR00].\nIt is clear that there are infinitely many reward functions for which the expert policy is optimal, for\nexample by several optimality-preserving transformations [NHR99]. To address this challenge, we can follow\nthe maximum entropy principle, and use an energy-based probability model to capture how expert trajectories\nare generated [Zie+08]:\np(τ) ∝exp\n\u0000 T −1\nX\nt=0\nRθ(st, at)\n\u0001\n(5.10)\nwhere Rθ is an unknown reward function with parameter θ.\nAbusing notation slightly, we denote by\nRθ(τ) = PT −1\nt=0 Rθ(st, at)) the cumulative reward along the trajectory τ. This model assigns exponentially\nsmall probabilities to trajectories with lower cumulative rewards. The partition function, Zθ ≜\nR\nτ exp(Rθ(τ)),\nis in general intractable to compute, and must be approximated. Here, we can take a sample-based approach.\nLet Dexp and D be the sets of trajectories generated by an expert, and by some known distribution q,\nrespectively. We may infer θ by maximizing the likelihood, p(Dexp|θ), or equivalently, minimizing the negative\nlog-likelihood loss\nL(θ) = −\n1\n|Dexp|\nX\nτ∈Dexp\nRθ(τ) + log 1\n|D|\nX\nτ∈D\nexp(Rθ(τ))\nq(τ)\n(5.11)\n104\n(a) online reinforcement learning\nrollout(s)\nupdate\nrollout data\n(b) off-policy reinforcement learning\nrollout(s)\nupdate\nrollout data\nbuffer\n(c) offline reinforcement learning\nrollout(s)\nlearn\nbuffer\ndata collected once \nwith any policy\ndeployment\ntraining phase\nFigure 5.3: Comparison of online on-policy RL, online off-policy RL, and offline RL. From Figure 1 of [Lev+20a].\nUsed with kind permission of Sergey Levine.\nThe term inside the log of the loss is an importance sampling estimate of Z that is unbiased as long as\nq(τ) > 0 for all τ. However, in order to reduce the variance, we can choose q adaptively as θ is being updated.\nThe optimal sampling distribution, q∗(τ) ∝exp(Rθ(τ)), is hard to obtain. Instead, we may find a policy ˆπ\nwhich induces a distribution that is close to q∗, for instance, using methods of maximum entropy RL discussed\nin Section 1.5.3. Interestingly, the process above produces the inferred reward Rθ as well as an approximate\noptimal policy ˆπ. This approach is used by guided cost learning [FLA16], and found effective in robotics\napplications.\n5.4.3\nImitation learning by divergence minimization\nWe now discuss a different, but related, approach to IL. Recall that the reward function depends only on\nthe state and action in an MDP. It implies that if we can find a policy π, so that pγ\nπ(s, a) and pγ\nπexp(s, a) are\nclose, then π receives similar long-term reward as πexp, and is a good imitation of πexp in this regard. A\nnumber of IL algorithms find π by minimizing the divergence between pγ\nπ and pγ\nπexp. We will largely follow\nthe exposition of [GZG19]; see [Ke+19] for a similar derivation.\nLet f be a convex function, and Df be the corresponding f-divergence [Mor63; AS66; Csi67; LV06; CS04].\nFrom the above intuition, we want to minimize Df\n\u0010\npγ\nπexp\n\r\r\rpγ\nπ\n\u0011\n. Then, using a variational approximation of\nDf [NWJ10], we can solve the following optimization problem for π:\nmin\nπ max\nw Epγ\nπexp(s,a) [Tw(s, a)] −Epγ\nπ(s,a) [f ∗(Tw(s, a))]\n(5.12)\nwhere f ∗is the convex conjugate of f, and Tw : S × A →R is some function parameterized by w. We can\nthink of π as a generator (of actions) and Tw as an adversarial critic that is used to compare the generated\n(s, a) pairs to the real ones. Thus the first expectation can be estimated using Dexp, as in behavior cloning,\nand the second can be estimated using trajectories generated by policy π. Furthermore, to implement this\nalgorithm, we often use a parametric policy representation πθ, and then perform stochastic gradient updates\nto find a saddle-point to Equation (5.12). With different choices of the convex function f, we can obtain\nmany existing IL algorithms, such as generative adversarial imitation learning (GAIL) [HE16] and\nadversarial inverse RL (AIRL) [FLL18], etc.\n5.5\nOffline RL\nOffline reinforcement learning (also called batch reinforcement learning [LGR12]) is concerned with\nlearning a reward maximizing policy from a fixed, static dataset, collected by some existing policy, known as\nthe behavior policy. Thus no interaction with the environment is allowed (see Figure 5.3). This makes\npolicy learning harder than the online case, since we do not know the consequences of actions that were not\ntaken in a given state, and cannot test any such “counterfactual” predictions by trying them. (This is the\nsame problem as in off-policy RL, which we discussed in Section 3.5.) In addition, the policy will be deployed\n105\non new states that it may not have seen, requiring that the policy generalize out-of-distribution, which is the\nmain bottleneck for current offline RL methods [Par+24b].\nA very simple and widely used offline RL method is known as behavior cloning or BC. This amounts to\ntraining a policy to predict the observed output action at associated with each observed state st, so we aim\nto ensure π(st) ≈at, as in supervised learning. This assumes the offline dataset was created by an expert,\nand so falls under the umbrella of imitation learning (see Section 5.4.1 for details). By contrast, offline RL\nmethods can leverage suboptimal data. We give a brief summary of some of these methods below. For more\ndetails, see e.g., [Lev+20b; Che+24b; Cet+24]. For some offline RL benchmarks, see DR4L [Fu+20], RL\nUnplugged [Gul+20], OGBench (Offline Goal-Conditioned benchmark) [Par+24a], and D5RL [Raf+24].\n5.5.1\nOffline model-free RL\nIn principle, we can tackle offline RL using the off-policy methods that we discussed in Section 3.5. These\nuse some form of importance sampling, based on π(a|s)/πb(a|s), to reweight the data in the replay buffer D,\nwhich was collected by the behavior policy, towards the current policy (the one being evaluated/ learned).\nUnfortunately, such methods only work well if the behavior policy is is close to the new policy. In the online\nRL case, this can be ensured by gradually updating the new policy away from the behavior policy, and then\nsampling new data from the updated policy (which becomes the new behavior policy). Unfortunately, this is\nnot an option in the offline case. Thus we need to use other strategies to control the discrepancy between\nthe behavior policy and learned policy, as we discuss below. (Besides the algorithmic techniques we discuss,\nanother reliable way to get better offline RL performance is to train on larger, more diverse datasets, as\nshown in [Kum+23].)\n5.5.1.1\nPolicy constraint methods\nIn the policy constraint method, we use a modified form of actor-critic, which, at iteration k, uses an\nupdate of the form\nQπ\nk+1 ←argmin\nQ\nE(s,a,s′)∼D\nh\u0000Q(s, a) −(R(s, a) + γEπk(a′|s′) [Qπ\nk(s′, a′)])\n\u00012i\n(5.13)\nπk+1 ←argmax\nπ\nEs∼D\n\u0002\nEπ(a|s)\n\u0002\nQπ\nk+1(s, a)\n\u0003\u0003\ns.t. D(π, πb) ≤ϵ\n(5.14)\nwhere D(π(·|s), πb(·|s)) is a divergence measure on distributions, such as KL divergence or another f-\ndivergence. This ensures that we do not try to evaluate the Q function on actions a′ that are too dissimilar\nfrom those seen in the data buffer (for each sampled state s), which might otherwise result in artefacts similar\nan adversarial attack.\nAs an alternative to adding a constraint, we can add a penalty of αD(π(·|s), πb(·|s)) to the target Q value\nand the actor objective, resulting in the following update:\nQπ\nk+1 ←argmin\nQ\nE(s,a,s′)∼D\nh\u0000Q(s, a) −(R(s, a) + γEπk(a′|s′) [Qπ\nk(s′, a′) −αγD(πk(·|s′), πb(·|s′))])\n\u00012i\n(5.15)\nπk+1 ←argmax\nπ\nEs∼D\n\u0002\nEπ(a|s)\n\u0002\nQπ\nk+1(s, a)\n\u0003\n−αD(π(·|s′), πb(·|s′))\n\u0003\n(5.16)\nOne problem with the above method is that we have to fit a parametric model to πb(a|s) in order to\nevaluate the divergence term. Fortunately, in the case of KL, the divergence can be enforced implicitly, as in\nthe advantage weighted regression or AWR method of [Pen+19], the reward weighted regression\nmethod of [PS07], the advantage weighted actor critic or AWAC method of [Nai+20], the advantage\nweighted behavior model or ABM method of [Sie+20], In this approach, we first solve (nonparametrically)\nfor the new policy under the KL divergence constraint to get πk+1, and then we project this into the required\n106\npolicy function class via supervised regression, as follows:\nπk+1(a|s) ←1\nZ πb(a|s) exp\n\u0012 1\nαQπ\nk(s, a)\n\u0013\n(5.17)\nπk+1 ←argmin\nπ\nDKL (πk+1 ∥π)\n(5.18)\nIn practice the first step can be implemented by weighting samples from πb(a|s) (i.e., from the data buffer)\nusing importance weights given by exp\n\u0000 1\nαQπ\nk(s, a)\n\u0001\n, and the second step can be implemented via supervised\nlearning (i.e., maximum likelihood estimation) using these weights.\nIt is also possible to replace the KL divergence with an integral probability metric (IPM), such as the\nmaximum mean discrepancy (MMD) distance, which can be computed from samples, without needing to fit\na distribution πb(a|s). This approach is used in [Kum+19]. This has the advantage that it can constrain\nthe support of the learned policy to be a subset of the behavior policy, rather than just remaining close to\nit. To see why this can be advantageous, consider the case where the behavior policy is uniform. In this\ncase, constraining the learned policy to remain close (in KL divergence) to this distribution could result in\nsuboptimal behavior, since the optimal policy may just want to put all its mass on a single action (for each\nstate).\n5.5.1.2\nBehavior-constrained policy gradient methods\nRecently a class of methods has been developed that is simple and effective: we first learn a baseline policy\nπ(a|s) (using BC) and a Q function (using Bellman minimization) on the offline data, and then update the\npolicy parameters to pick actions that have high expected value according to Q and which are also likely\nunder the BC prior. An early example of this is the Q† algorithm of [Fuj+19]. In [FG21], they present the\nDDPG+BC method, which optimizes\nmax\nπ\nJ(π) = E(s,a)∼D [Q(s, µπ(s)) + α log π(a|s)]\n(5.19)\nwhere µπ(s) = Eπ(a|s) [a] is the mean of the predicted action, and α is a hyper-parameter. As another example,\nthe DQL method of [WHZ23] optimizes a diffusion policy using\nmin\nπ L(π) = Ldiffusion(π) + Lq(π) = Ldiffusion(π) −αEs∼D,a∼π(·|s) [Q(s, a)]\n(5.20)\nFinally, [Aga+22b] discusses how to transfer the policy from a previous agent to a new agent by combining\nBC with Q learning.\n5.5.1.3\nUncertainty penalties\nAn alternative way to avoid picking out-of-distribution actions, where the Q function might be unreliable, is\nto add a penalty term to the Q function based on the estimated epistemic uncertainty, given the dataset\nD, which we denote by Unc(PD(Qπ)), where PD(Qπ) is the distribution over Q functions, and Unc is some\nmetric on distributions. For example, we can use a deep ensemble to represent the distribution, and use the\nvariance of Q(s, a) across ensemble members as a measure of uncertainty. This gives rise to the following\npolicy improvement update:\nπk+1 ←argmax\nπ\nEs∼D\nh\nEπ(a|s)\nh\nEPD(Qπ\nk+1)\n\u0002\nQπ\nk+1(s, a)\n\u0003i\n−αUnc(PD(Qπ\nk+1))\ni\n(5.21)\nFor examples of this approach, see e.g., [An+21; Wu+21; GGN22].\n5.5.1.4\nConservative Q-learning and pessimistic value functions\nAn alternative to explicitly estimating uncertainty is to add a conservative penalty directly to the Q-learning\nerror term. That is, we minimize the following wrt w using each batch of data B:\nE(B, w) = αC(B, w) + E(B, w)\n(5.22)\n107\nwhere E(B, w) = E(s,a,s′)∈B\n\u0002\n(Qw(s, a) −(r + γ maxa′ Qw(s′, a′)))2\u0003\nis the usual loss for Q-learning, and\nC(B, w) is some conservative penalty. In the conservative Q learning or CQL method of [Kum+20], we\nuse the following penalty term:\nC(B, w) = Es∼B,a∼π(·|s) [Qw(s, a)] −E(s,a)∼B [Qw(s, a)]\n(5.23)\nIf π is the behavior policy, this penalty becomes 0.\n5.5.2\nOffline model-based RL\nIn Chapter 4, we discussed model-based RL, which can train a dynamics model given a fixed dataset, and\nthen use this to generate synthetic data to evaluate and then optimize different possible policies. However,\nif the model is wrong, the method may learn a suboptimal policy, as we discussed in Section 4.2.3. This\nproblem is particularly severe in the offline RL case, since we cannot recover from any errors by collecting\nmore data. Therefore various conservative MBRL algorithms have been developed, to avoid exploiting model\nerrors. For example, [Kid+20] present the MOREL algorithm, and [Yu+20] present the MOPO algorithm.\nUnlike the value function uncertainty method of Section 5.5.1.3, or the conservative value function method of\nSection 5.5.1.4, these model-based methods add a penalty for visiting states where the model is likely to be\nincorrect.\nIn more detail, let u(s, a) be an estimate of the uncertainty of the model’s predictions given input (s, a).\nIn MOPO, they define a conservative reward using R(s, a) = R(s, a) −λu(s, a), and in MOREL, they modify\nthe MDP so that the agent enters an absorbing state with a low reward when u(s, a) is sufficiently large.\nIn both cases, it is possible to prove that the model-based estimate of the policy’s performance under\nthe modified reward or dynamics is a lower bound of the performance of the policy’s true performance in\nthe real MDP, provided that the uncertainty function u is an error oracle, which means that is satisfies\nD(Mθ(s′|s, a), M ∗(s′|s, a)) ≤u(s, a), where M ∗is the true dynamics, and Mθ is the estimated dynamics.\nFor more information on offline MBRL methods, see [Che+24c].\n5.5.3\nOffline RL using reward-conditioned sequence modeling\nRecently an approach to offline RL based on sequence modeling has become very popular. The basic idea\n— known as upside down RL [Sch19] or RvS (RL via Supervised learning) [KPL19; Emm+21] — is to\ntrain a generative model over future states and/or actions conditioned on the observed reward, rather than\npredicting the reward given a state-action trajectory. At test time, the conditioning is changed to represent\nthe desired reward, and futures are sampled from the model. The implementation of this idea then depends\non what kind of generative model to use, as we discuss below.\nThe trajectory transformer method of [JLL21] learns a joint model of the form p(s1:T , a1:T , r1:T ) using\na transformer, and then samples from this using beam search, selecting the ones with high reward (similar to\nMPC, Section 4.1.1). The decision transformer [Che+21b] is related, but just generates action sequences,\nand conditions on the past observations and the future reward-to-go. That is, it fits\nargmax\nθ\nEpD [log πθ(at|s0:t, a0:t−1, RTG0:t)]\n(5.24)\nwhere RTGt = PT\nk=t rt is the return to go. (For a comparison of decision transformers to other offline RL\nmethods, see [Bha+24].)\nThe diffuser method of [Jan+22] is a diffusion version of trajectory transformer, so it fits p(s1:T , a1:T , r1:T )\nusing diffusion, where the action space is assumed to be continuous. They also replace beam search with\nclassifier guidance. The decision diffuser method of [Aja+23] extends diffuser by using classifer-free\nguidance, where the conditioning signal is the reward-to-go, simlar to decision transformer. However, unlike\ndiffuser, the decision diffuser just models the future state trajectories (rather than learning a joint distribution\nover states and actions), and infers the actions using an inverse dynamics model at = π(st, st+1), which is\ntrained using supervised learning.\n108\nOne problem with the above approaches is that conditioning on a desired return and taking the predicted\naction can fail dramatically in stochastic environments, since trajectories that result in a return may have\nonly achieved that return due to chance [PMB22; Yan+23; Bra+22; Vil+22]. (This is related to the optimism\nbias in the control-as-inference approach discussed in Section 1.5.)\n5.5.4\nHybrid offline/online methods\nDespite the progress in offline RL, it is fundamentally more limited in what it can learn compared to online\nRL [OCD21]. Therefore, various hybrids of offline and online RL have been proposed, such as [Bal+23] and\n[Nak+23].\nFor example, [Nak+23] suggest pre-training with offline RL (specifically CQL) followed by online finetuning.\nNaively this does not work that well, because CQL can be too conservative, requiring the online learning to\nwaste some time at the beginning fixing the pessimism. So they propose a small modification to CQL, known\nas calibrated Q learning. This simply prevents CQL from being too conservative, by replacing the CQL\nregularizer with\nmin\nQ max\nπ\nJ(Q, π) + αEs∼D,a∼π(a|s)\n\u0002\nmax(Q(s, a), V πβ(s)) −αE(s,a)∼D [Q(s, a)]\n\u0003\n(5.25)\nwhere the Q(s, a) term inside the max ensures conservatism (so Q lower bounds the value of the learned\npolicy), and the V πβ(s) term ensures “calibration” (so Q upper bounds the value of the behavior policy).\nThen online finetuning is performed in the usual way.\n5.6\nLLMs and RL\nIn this section, we discuss some connections between RL and “foundation models” (see e.g., [Cen21]).\nThese are large pretrained generative models of text and/or images such as large language models\n(LLMs) and their multimodal extension, sometimes called vision language models (VLMs). Note that\nthis is a very fast growing field, so we only briefly mention a few highlights. For more details, see e.g.\nhttps://github.com/WindyLab/LLM-RL-Papers.\n5.6.1\nRL for LLMs\nWe can think of LLMs as agents, where the state st is the entire sequence of previous words, st = (w1, . . . , wt−1),\nthe action is the next word wt2, the stochastic policy π(at|st) is the LLM, and the transition model is the\ndetermistic function p(st+1|st, at) = δ(st = concat(st, at)). We see that the size of the state grows linearly\nover time, which is a standard way to capture non-local dependencies in a Markov model.\nWe discuss how to train these models below. Once trained, they are used in a semi-MDP fashion, in\nwhich at round t the agent generates an answer at = (at,1, . . . , at,Nt), which is a sequence of Nt tokens,\nin response to a prompt from the user, pt = (pt,1, . . . , pt,Mt), and the previous context (dialog history),\nct = (p1,1:M1, a1,1:N1, p2,1:M2, . . . , at−1,1:Nt−1).\nWe can now define the state as the sequence of tokens\nst = (ct, pt). Similarly, the action sequence at can be flattened into a single atomic (string-valued) action,\nsince there is no intermediate feedback from the environment after each token is produced.3 Note that, if\nthere is a single round of prompting and answering (as is often assumed during training), then this is a\ncontextual bandit problem rather than a full MDP. In particular, the context is the string pt and the action\nis the string at. However, in multi-turn dialog situations, the agent’s actions will affect the environment (i.e.,\nthe user’s mental state, and hence subsequent prompt pt+1), turning it into a full MDP.\n2When using VLMs, the “words” are a tokenized representation of the visual input and/or output. Even when using language,\nthe elementary components wt are sub-words (which allows for generalization), not words. So a more precise term would be\n“tokens” instead of “words”.\n3The fact that the action (token) sequence is generated by an autoregressive policy inside the agent’s head is an implementation\ndetail, and not part of the problem specification; for example, the agent could instead use discrete diffusion to generate\nat = (at,1, . . . , at,Nt).\n109\n5.6.1.1\nRLHF\nLLMs are usually trained with behavior cloning, i.e., MLE on a fixed dataset, such as a large text (and\ntokenized image) corpus scraped from the web. This is called “pre-training”. We can then improve their\nperformance using RL, as we describe below; this is called “post-training”.\nA common way to perform post-training is to use reinforcement learning from human feedback or\nRLHF. This technique, which was first introduced in the InstructGPT paper [Ouy+22], works as follows.\nFirst a large number of (context, answer0, answer1) tuples are generated, either by a human or an LLM.\nThen human raters are asked if they prefer answer 0 or answer 1. Let y = 0 denote the event that they prefer\nanswer 0, and y = 1 the event that they prefer answer 1. We can then fit a model of the form\np(y = 0|a0, a1, c) =\nexp(ϕ(c, a0))\nexp(ϕ(c, a0)) + exp(ϕ(c, a1))\n(5.26)\nusing binary cross entropy loss, where ϕ(c, a) is some function that maps text to a scalar (interpreted as\nlogits). Typically ϕ(c, a) is a shallow MLP on top of the last layer of a pretrained LLM. Finally, we define\nthe reward function as R(s, a) = ϕ(s, a), where s is the context (e.g., a prompt or previous dialog state), and\na is the action (answer generated by LLM). We then use this reward to fine-tune the LLM using a policy\ngradient method such as PPO (Section 3.4.3), or a simpler method such as RLOO [Ahm+24], which is based\non REINFORCE (Section 3.2).\nNote that this form of training assumes the agent just interacts with a single action (answer) in response\nto a single prompt, so is learning the reward for a bandit problem, rather than the full MDP. Also, the\nlearned reward function is a known parametric model (since it is fit to the human feedback data), whereas in\nRL, the reward is an unknown non-differentiable blackbox function. When viewed in this light, it becomes\nclear that one can also use non-RL algorithms to improve performance of LLMs, such as DPO [Raf+23] or\nthe density estimation methods of [Dum+24]. For more details on RL for LLMs, see e.g., [Kau+23].\n5.6.1.2\nAssistance game\nIn general, any objective-maximizing agent may suffer from reward hacking (Section 5.2.1), even if the reward\nhas been learned using lots of RLHF data. In [Rus19], Stuart Russell proposed a clever solution to this\nproblem. Specifically, the human and machine are both treated as agents in a two-player cooperative game,\ncalled an assistance game, where the machine’s goal is to maximize the user’s utility (reward) function,\nwhich is inferred based on the human’s behavior using inverse RL. That is, instead of trying to learn a point\nestimate of the reward function using RLHF, and then optimizing that, we treat the reward function as an\nunknown part of the environment. If we adopt a Bayesian perspective on this, we can maintain a posterior\nbelief over the model parameters. This will incentivize the agent to perform information gathering actions.\nFor example, if the machine is uncertain about whether something is a good idea or not, it will proceed\ncautiously (e.g., by asking the user for their preference), rather than blindly solving the wrong problem. For\nmore details on this framework, see [Sha+20].\n5.6.1.3\nRun-time inference as MPC\nRecently the LLM community has investigated ways to improve the “reasoning” performance of LLMs by\nusing MCTS-like methods (see Section 4.1.3). The basic idea is to perform Monte Carlo rollouts of many\npossible action sequences (by generating different “chains of thought” in response to the context so far), and\nthen applying a value function to the leaves of this search tree to decide on which trajectory to return as the\nfinal “decision”. The value function is usually learned using policy gradient methods, such as REINFORCE\n(see e.g., [Zel+24]). It is believed that OpenAI’s recently released o1 (aka Strawberry) model4uses similar\ntechniques, most likely pre-training on large numbers of human reasoning traces.\nNote that the resulting policy is an instance of MPC (Section 4.1.1) or decision time planning. This\nmeans that, as in MPC, the agent must replan after every new state observation (which incorporates the\n4See https://openai.com/index/learning-to-reason-with-llms/.\n110\nresponse from the user), making the method much slower than “reactive” LLM policies, that does not use\nlook-ahead search (but still conditions on the entire past context). However, once trained, it may be possible\nto distill this slower “system 2” policy into a faster reactive “system 1” policy.\n5.6.2\nLLMs for RL\nThere are many ways that (pretrained) LLMs can be used for RL, by leveraging their prior knowledge, their\nability to generate code, their “reasoning” ability, and their ability to perform in-context learning (which\ncan be viewed as a form of Bayesian inference [PAG24], which is a “gradient-free” way of optimally learning\nthat is well suited to rapid learning from limited data). The survey in [Cao+24] groups the literature into\nfour main categories: LLMs for pre-processing the inputs, LLMs for rewards, LLMs for world models, and\nLLMs for decision making or policies. In our brief presentation below, we follow this categorization. (See also\n[Spi+24] for a similar grouping.)\n5.6.2.1\nLLMs for pre-processing the input\nIf the input observations ot sent to the agent are in natural language (or some other textual representation, such\nas JSON), it is natural to use an LLM to process them, in order to compute a more compact representation,\nst = ϕ(ot), where ϕ can the hidden state of the last layer of an LLM. This encoder can either be frozen, or\nfine-tuned with the policy network. Note that we can also pass in the entire past observation history, o1:t, as\nwell as static “side information”, such as instruction manuals or human hints; these can all be concatenated\nto form the LLM prompt.\nFor example, the AlphaProof system5 uses an LLM (called the “formalizer network”) to translate an\ninformal specification of a math problem into the formal Lean representation, which is then passed to an\nagent (called the “solver network”) which is trained, using the AlphaZero method (see Section 4.1.3.1), to\ngenerate proofs inside the Lean theorem proving environment. In this environment, the reward is 0 or 1 (proof\nis correct or not), the state space is a structured set of previously proved facts and the current goal, and the\naction space is a set of proof tactics. The agent itself is a separate transformer policy network (distincy from\nthe formalizer network) that is trained from scratch in an incremental way, based on the AlphaZero method.\nIf the observations are images, it it is traditional to use a CNN to proccess the input, so st ∈RN would\nbe an embedding vector. However, we could alternatively use a VLM to compute a structured representation,\nwhere st might be a set of tokens describing the scene at a high level. We then proceed as in the text case.\nNote that the information that is extracted will heavily depend on the prompt that is used. Thus we\nshould think of an LLM/VLM as an active sensor that we can control via prompts. Choosing how to control\nthis sensor requires expanding the action space of the agent to include computational actions [Che+24d].\nNote also that these kinds of “sensors” are very expensive to invoke, so an agent with some limits on its time\nand compute (which is all practical agents) will need to reason about the value of information and the cost of\ncomputation. This is called metareasoning [RW91]. Devising good ways to train agents to perform both\ncomputational actions (e.g., invoking an LLM or VLM) and environment actions (e.g., taking a step in the\nenvironment or calling a tool) is an open research problem.\n5.6.2.2\nLLMs for rewards\nIt is difficult to design a reward function to cause an agent to exhibit some desired behavior, as we discussed\nin Section 5.2. Fortunately LLMs can often help with this task. We discuss a few approaches below.\nIn [Kli+24], they present the Motif system, that uses an LLM in lieu of a human to provide preference\njudgements to an RLHF system. In more detail, a pre-trained policy is used to collect trajectories, from\nwhich pairs of states, (o, o′), are selected at random. The LLM is then asked which state is preferable, thus\ngenerating (o, o′, y) tuples, which can be used to train a binary classifier from which a reward model is\nextracted, as in Section 5.6.1.1. In [Kli+24], the observations o are text captions generated by the NetHack\ngame, but the same method could be applied to images if we used a VLM instead of an LLM for learning the\n5See https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.\n111\nreward. The learned reward model is then used as a shaping function (Section 5.2.3) when training an agent\nin the NetHack environment, which has very sparse reward.\nIn [Ma+24], they present the Eureka system, that learns the reward using bilevel optimization, with\nRL on the inner loop and LLM-powered evolutionary search on the outer loop. In particular, in the inner\nloop, given a candidate reward function Ri, we use PPO to train a policy, and then return a scalar quality\nscore Si = S(Ri). In the outer loop, we ask an LLM to generate a new set of reward functions, R′\ni, given\na population of old reward functions and their scores, (Ri, Si), which have been trained and evaluated in\nparallel on a fleet of GPUs. The prompt also includes the source code of the environment simulator. Each\ngenerated reward function Ri is represented as a Python function, that has access to the ground truth state\nof the underlying robot simulator. The resulting system is able to learn a complex reward function that is\nsufficient to train a policy (using PPO) that can control a simulated robot hand to perform various dexterous\nmanipulation tasks, including spinning a pen with its finger tips. In [Li+24], they present a somewhat related\napproach and apply it to Minecraft.\nIn [Ven+24], they propose code as reward, in which they prompt a VLM with an initial and goal image,\nand ask it to describe the corresponding sequence of tasks needed to reach the goal. They then ask the LLM\nto synthesize code that checks for completion of each subtask (based on processing of object properties, such\nas relative location, derived from the image). These reward functions are then “verified” by applying them to\nan offline set of expert and random trajectories; a good reward function should allocate high reward to the\nexpert trajectories and low reward to the random ones. Finally, the reward functions are used as auxiliary\nrewards inside an RL agent.\nThere are of course many other ways an LLM could be used to help learn reward functions, and this\nremains an active area of research.\n5.6.2.3\nLLMs for world models\nThere are many papers that use transformers or diffusion models to represent the world model p(s′|s, a), and\nlearn them from data collected by the agent, as we discussed in Section 4.3. Here we focus our attention on\nways to use pre-trained foundation models as world models (WM).\n[Yan+24] presents UniSim, which is an action-conditioned video diffusion model trained on large amounts\nof robotics and visual navigation data. Combined with a VLM reward model, this can be used for decision-time\nplanning as follows: sample candidate action trajectories from a proposal, generate the corresponding images,\nfeed them to the reward model, score the rollouts, and then pick the best action from this set. (Note that\nthis is just standard MPC in image space with a diffusion WM and a random shooting planning algorithm.)\n[TKE24] presents WorldCoder, which takes a very different approach. It prompts a frozen LLM to\ngenerate code to represent the WM p(s′|s, a), which it then uses inside of a planning algorithm. The agent\nthen executes this in the environment, and passes back failed predictions to the LLM, asking it to improve\nthe WM. (This is related to the Eureka reward-learning system mentioned in Section 5.6.2.2.)\nThere are of course many other ways an LLM could be used to help learn world models, and this remains\nan active area of research.\n5.6.2.4\nLLMs for policies\nFinally we turn to LLMs as policies.\nOne approach is to pre-train a special purpose foundation model on state-action sequences (using behavior\ncloning), then sample the next action from it using at ∼p(at|ot, ht−1), where ot is the latest observation\nand ht−1 = (o1:t−1, a1:t−1) is the history. See e.g., Gato model [Ree+22] RT-2 [Zit+23], and RoboCat\n[Bou+23].\nMore recently it has become popular to leverage pre-trained LLMs that are trained on web data, and\nthen to repurpose them as “agents” using in-context learning. We can then sample an action from the policy\nπ(at|pt, ot, ht−1), where pt is a manually chosen prompt. This approach is used by the ReAct paper [Yao+22]\nwhich works by prompting the LLM to “think step-by-step” (“reasoning”) and then to predict an action\n(“acting”). This approach can be extended by prompting the LLM to first retrieve relevant past examples\n112\nFigure 5.4: Illustration of how to use a pretrained LLM (combined with RAG) as a policy. From Figure 5 of [Par+23].\nUsed with kind permission of Joon Park.\nfrom an external “memory”, rather than explicitly storing the entire history ht in the context (this is called\nretrieval augmented generation or RAG); see Figure 5.4 for an illustration. Note that no explicit\nlearning (in the form of parametric updates) is performed in these systems; instead they rely entirely on\nin-context learning (and prompt engineering).\nAn alternative approach is to enumerate all possible discrete actions, and use the LLM to score them in\nterms of their likelihoods given the goal, and their suitability given a learned value function applied to the\ncurrent state, i.e. π(at = k|g, pt, ot, ht) ∝LLM(wk|gt, pt, ht)Vk(ot), where gt is the current goal, wk is a text\ndescription of action k, and Vk is the value function for action k. This is the approach used in the robotics\nSayCan approach [Ich+23], where the primitive actions ak are separately trained goal-conditioned policies.\nCalling the LLM at every step is very slow, so an alternative is to use the LLM to generate code that\nrepresents (parts of) the policy. For example, the Voyager system in [Wan+24a] builds up a reusable skill\nlibrary (represented as Python functions), by alternating between environment exploration and prompting\nthe (frozen) LLM to generate new tasks and skills, given the feedback collected so far.\nThere are of course many other ways an LLM could be used to help learn policies, and this remains an\nactive area of research.\n5.7\nGeneral RL, AIXI and universal AGI\nThe term “general RL” (see e.g., [Hut05; LHS13; HQC24; Maj21]) refers to the setup in which an agent\nreceives a stream of observations o1, o2, . . . and rewards r1, r2, . . ., and performs a sequence of actions in\nresponse, a1, a2, . . ., but where we do not make any Markovian (or even stationarity) assumptions about the\nenvironment that generates the observation stream. Instead, we assume that the environment is a computable\nfunction or program p∗, which generated the observations o1:t and r1:t seen so far in response to the actions\ntaken, a1:t−1. We denote this by U(p∗, a1:t) = (o1r1 · · · otrt), where U is a universal Turing machine. If we\nuse the receeding horizon control strategy (see Section 4.1.1), the optimal action at each step is the one that\nmaximizes the posterior expected reward-to-go (out to some horizon m steps into the future). If we assume\nthe agent represents the unknown environment as a program p ∈M, then the optimal action is given by the\nfollowing expectimax formula:\nat = argmax\nat\nX\not,rt\n· · · max\nam\nX\nom,rm\n[rt + · · · + rm]\nX\np:U(p,a1:m)=(o1r1···omrm)\nPr(p)\n(5.27)\nwhere Pr(p) is the prior probability of p, and we assume the likelihood is 1 if p can generate the observations\ngiven the actions, and is 0 otherwise.\nThe key question is: what is a reasonable prior over programs? In [Hut05], Marcus Hutter proposed\nto apply the idea of Solomonoff induction [Sol64] to the case of an online decision making agent. This\n113\namounts to using the prior Pr(p) = 2−ℓ(p), where ℓ(p) is the length of program p. This prior favors shorter\nprograms, and the likelihood filters out programs that cannot explain the data.\nThe resulting agent is known as AIXI, where “AI” stands for “Artificial Intelligence” and “XI” referring\nto the Greek letter ξ used in Solomonoff induction. The AIXI agent has been called the “most intelligent\ngeneral-purpose agent possible” [HQC24], and can be viewed as the theoretical foundation of (universal)\nartificial general intelligence or AGI.\nUnfortunately, the AIXI agent is intractable to compute, since it relies on Solomonoff induction and\nKolmogorov complexity, both of which are intractable, but various approximations can be devised. For\nexample, we can approximate the expectimax with MCTS (see Section 4.1.3). Alternatively, [GM+24] showed\nthat it is possible to use meta learning to train a generic sequence predictor, such as a transformer or LSTM,\non data generated by random Turing machines, so that the transformer learns to approximate a universal\npredictor. Another approach is to learn a policy (to avoid searching over action sequences) using TD-learning\n(Section 2.3.2); the weighting term in the policy mixture requires that the agent predict its own future actions,\nso this approach is known as self-AIXI [Cat+23].\nNote that AIXI is a normative theory for optimal agents, but is not very practical, since it does not take\ncomputational limitations into account. In [Aru+24a; Aru+24b], they describe an approach which extends\nthe above Bayesian framework, while also taking into account the data budget (due to limited environment\ninteractions) that real agents must contend with (which prohibits modeling the entire environment or\nfinding the optimal action). This approach, known as Capacity-Limited Bayesian RL (CBRL), combines\nBayesian inference, RL, and rate distortion theory, and can be seen as a normative theoretical foundation for\ncomputationally bounded rational agents.\n114\nBibliography\n[Abd+18]\nA. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. “Maximum\na Posteriori Policy Optimisation”. In: International Conference on Learning Representations.\nFeb. 2018. url: https://openreview.net/pdf?id=S1ANxQW0b.\n[ABM10]\nJ.-Y. Audibert, S. Bubeck, and R. Munos. “Best Arm Identification in Multi-Armed Bandits”.\nIn: COLT. 2010, pp. 41–53.\n[ACBF02]\nP. Auer, N. Cesa-Bianchi, and P. Fischer. “Finite-time Analysis of the Multiarmed Bandit\nProblem”. In: MLJ 47.2 (May 2002), pp. 235–256. url: http://mercurio.srv.di.unimi.it/\n~cesabian/Pubblicazioni/ml-02.pdf.\n[Ach+17]\nJ. Achiam, D. Held, A. Tamar, and P. Abbeel. “Constrained Policy Optimization”. In: ICML.\n2017. url: http://arxiv.org/abs/1705.10528.\n[Aga+14]\nD. Agarwal, B. Long, J. Traupman, D. Xin, and L. Zhang. “LASER: a scalable response\nprediction platform for online advertising”. In: WSDM. 2014.\n[Aga+21a]\nA. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. “On the Theory of Policy Gradient\nMethods: Optimality, Approximation, and Distribution Shift”. In: JMLR 22.98 (2021), pp. 1–76.\nurl: http://jmlr.org/papers/v22/19-736.html.\n[Aga+21b]\nR. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, and M. G. Bellemare. “Deep Reinforcement\nLearning at the Edge of the Statistical Precipice”. In: NIPS. Aug. 2021. url: http://arxiv.\norg/abs/2108.13264.\n[Aga+22a]\nA. Agarwal, N. Jiang, S. Kakade, and W. Sun. Reinforcement Learning: Theory and Algorithms.\n2022. url: https://rltheorybook.github.io/rltheorybook_AJKS.pdf.\n[Aga+22b]\nR. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare. “Reincarnating\nReinforcement Learning: Reusing Prior Computation to Accelerate Progress”. In: NIPS. Vol. 35.\n2022, pp. 28955–28971. url: https://proceedings.neurips.cc/paper_files/paper/2022/\nhash/ba1c5356d9164bb64c446a4b690226b0-Abstract-Conference.html.\n[Ahm+24]\nA. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, A. Üstün, and S. Hooker. “Back\nto basics: Revisiting REINFORCE style optimization for learning from Human Feedback in\nLLMs”. In: arXiv [cs.LG] (Feb. 2024). url: http://arxiv.org/abs/2402.14740.\n[Aja+23]\nA. Ajay, Y. Du, A. Gupta, J. B. Tenenbaum, T. S. Jaakkola, and P. Agrawal. “Is Conditional\nGenerative Modeling all you need for Decision Making?” In: ICLR. 2023. url: https://\nopenreview.net/forum?id=sP1fo2K9DFG.\n[AJO08]\nP. Auer, T. Jaksch, and R. Ortner. “Near-optimal Regret Bounds for Reinforcement Learning”.\nIn: NIPS. Vol. 21. 2008. url: https://proceedings.neurips.cc/paper_files/paper/2008/\nfile/e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf.\n[AL+16]\nJ. Ala-Luhtala, N. Whiteley, K. Heine, and R. Piche. “An Introduction to Twisted Particle\nFilters and Parameter Estimation in Non-linear State-space Models”. In: IEEE Trans. Signal\nProcess. 64.18 (2016), pp. 4875–4890. url: http://arxiv.org/abs/1509.09175.\n115\n[Ale+23]\nL. N. Alegre, A. L. C. Bazzan, A. Nowé, and B. C. da Silva. “Multi-step generalized policy\nimprovement by leveraging approximate models”. In: NIPS. Vol. 36. Curran Associates, Inc.,\n2023, pp. 38181–38205. url: https://proceedings.neurips.cc/paper_files/paper/2023/\nhash/77c7faab15002432ba1151e8d5cc389a-Abstract-Conference.html.\n[Alo+24]\nE. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. Storkey, T. Pearce, and F. Fleuret. “Diffusion\nfor world modeling: Visual details matter in Atari”. In: arXiv [cs.LG] (May 2024). url: http:\n//arxiv.org/abs/2405.12399.\n[AM89]\nB. D. Anderson and J. B. Moore. Optimal Control: Linear Quadratic Methods. Prentice-Hall\nInternational, Inc., 1989.\n[Ama98]\nS Amari. “Natural Gradient Works Efficiently in Learning”. In: Neural Comput. 10.2 (1998),\npp. 251–276. url: http://dx.doi.org/10.1162/089976698300017746.\n[AMH23]\nA. Aubret, L. Matignon, and S. Hassas. “An information-theoretic perspective on intrinsic\nmotivation in reinforcement learning: A survey”. en. In: Entropy 25.2 (Feb. 2023), p. 327. url:\nhttps://www.mdpi.com/1099-4300/25/2/327.\n[Ami+21]\nS. Amin, M. Gomrokchi, H. Satija, H. van Hoof, and D. Precup. “A survey of exploration\nmethods in reinforcement learning”. In: arXiv [cs.LG] (Aug. 2021). url: http://arxiv.org/\nabs/2109.00157.\n[An+21]\nG. An, S. Moon, J.-H. Kim, and H. O. Song. “Uncertainty-Based Offline Reinforcement Learning\nwith Diversified Q-Ensemble”. In: NIPS. Vol. 34. Dec. 2021, pp. 7436–7447. url: https://\nproceedings.neurips.cc/paper_files/paper/2021/file/3d3d286a8d153a4a58156d0e02d8570c-\nPaper.pdf.\n[And+17]\nM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,\nP. Abbeel, and W. Zaremba. “Hindsight Experience Replay”. In: arXiv [cs.LG] (July 2017).\nurl: http://arxiv.org/abs/1707.01495.\n[And+20]\nO. M. Andrychowicz et al. “Learning dexterous in-hand manipulation”. In: Int. J. Rob. Res.\n39.1 (2020), pp. 3–20. url: https://doi.org/10.1177/0278364919887447.\n[Ant+22]\nI. Antonoglou, J. Schrittwieser, S. Ozair, T. K. Hubert, and D. Silver. “Planning in Stochastic\nEnvironments with a Learned Model”. In: ICLR. 2022. url: https://openreview.net/forum?\nid=X6D9bAHhBQ1.\n[AP23]\nS. Alver and D. Precup. “Minimal Value-Equivalent Partial Models for Scalable and Robust\nPlanning in Lifelong Reinforcement Learning”. en. In: Conference on Lifelong Learning Agents.\nPMLR, Nov. 2023, pp. 548–567. url: https://proceedings.mlr.press/v232/alver23a.\nhtml.\n[AP24]\nS. Alver and D. Precup. “A Look at Value-Based Decision-Time vs. Background Planning\nMethods Across Different Settings”. In: Seventeenth European Workshop on Reinforcement\nLearning. Oct. 2024. url: https://openreview.net/pdf?id=Vx2ETvHId8.\n[Arb+23]\nJ. Arbel, K. Pitas, M. Vladimirova, and V. Fortuin. “A Primer on Bayesian Neural Networks:\nReview and Debates”. In: arXiv [stat.ML] (Sept. 2023). url: http://arxiv.org/abs/2309.\n16314.\n[ARKP24]\nS. Alver, A. Rahimi-Kalahroudi, and D. Precup. “Partial models for building adaptive model-\nbased reinforcement learning agents”. In: COLLAS. May 2024. url: https://arxiv.org/abs/\n2405.16899.\n[Aru+17]\nK. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath. “A Brief Survey of Deep\nReinforcement Learning”. In: IEEE Signal Processing Magazine, Special Issue on Deep Learning\nfor Image Understanding (2017). url: http://arxiv.org/abs/1708.05866.\n116\n[Aru+24a]\nD. Arumugam, M. K. Ho, N. D. Goodman, and B. Van Roy. “Bayesian Reinforcement Learning\nWith Limited Cognitive Load”. en. In: Open Mind 8 (Apr. 2024), pp. 395–438. url: https:\n//direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_\n00132.pdf.\n[Aru+24b]\nD. Arumugam, S. Kumar, R. Gummadi, and B. Van Roy. “Satisficing exploration for deep\nreinforcement learning”. In: Finding the Frame Workshop at RLC. July 2024. url: https:\n//openreview.net/forum?id=tHCpsrzehb.\n[AS22]\nD. Arumugam and S. Singh. “Planning to the information horizon of BAMDPs via epistemic\nstate abstraction”. In: NIPS. Oct. 2022.\n[AS66]\nS. M. Ali and S. D. Silvey. “A General Class of Coefficients of Divergence of One Distribution\nfrom Another”. In: J. R. Stat. Soc. Series B Stat. Methodol. 28.1 (1966), pp. 131–142. url:\nhttp://www.jstor.org/stable/2984279.\n[ASN20]\nR. Agarwal, D. Schuurmans, and M. Norouzi. “An Optimistic Perspective on Offline Reinforce-\nment Learning”. en. In: ICML. PMLR, Nov. 2020, pp. 104–114. url: https://proceedings.\nmlr.press/v119/agarwal20c.html.\n[Att03]\nH. Attias. “Planning by Probabilistic Inference”. In: AI-Stats. 2003. url: http://research.\ngoldenmetallic.com/aistats03.pdf.\n[AY20]\nB. Amos and D. Yarats. “The Differentiable Cross-Entropy Method”. In: ICML. 2020. url:\nhttp://arxiv.org/abs/1909.12830.\n[Bad+20]\nA. P. Badia, B. Piot, S. Kapturowski, P Sprechmann, A. Vitvitskyi, D. Guo, and C Blundell.\n“Agent57: Outperforming the Atari Human Benchmark”. In: ICML 119 (Mar. 2020), pp. 507–517.\nurl: https://proceedings.mlr.press/v119/badia20a/badia20a.pdf.\n[Bai95]\nL. C. Baird. “Residual Algorithms: Reinforcement Learning with Function Approximation”. In:\nICML. 1995, pp. 30–37.\n[Bal+23]\nP. J. Ball, L. Smith, I. Kostrikov, and S. Levine. “Efficient Online Reinforcement Learning with\nOffline Data”. en. In: ICML. PMLR, July 2023, pp. 1577–1594. url: https://proceedings.\nmlr.press/v202/ball23a.html.\n[Ban+23]\nD. Bansal, R. T. Q. Chen, M. Mukadam, and B. Amos. “TaskMet: Task-driven metric learning for\nmodel learning”. In: NIPS. Ed. by A Oh, T Naumann, A Globerson, K Saenko, M Hardt, and S\nLevine. Vol. abs/2312.05250. Dec. 2023, pp. 46505–46519. url: https://proceedings.neurips.\ncc / paper _ files / paper / 2023 / hash / 91a5742235f70ae846436d9780e9f1d4 - Abstract -\nConference.html.\n[Bar+17]\nA. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver. “Suc-\ncessor Features for Transfer in Reinforcement Learning”. In: NIPS. Vol. 30. 2017. url: https://\nproceedings.neurips.cc/paper_files/paper/2017/file/350db081a661525235354dd3e19b8c05-\nPaper.pdf.\n[Bar+19]\nA. Barreto et al. “The Option Keyboard: Combining Skills in Reinforcement Learning”. In:\nNIPS. Vol. 32. 2019. url: https://proceedings.neurips.cc/paper_files/paper/2019/\nfile/251c5ffd6b62cc21c446c963c76cf214-Paper.pdf.\n[Bar+20]\nA. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup. “Fast reinforcement learning with\ngeneralized policy updates”. en. In: PNAS 117.48 (Dec. 2020), pp. 30079–30087. url: https:\n//www.pnas.org/doi/abs/10.1073/pnas.1907370117.\n[BBS95]\nA. G. Barto, S. J. Bradtke, and S. P. Singh. “Learning to act using real-time dynamic pro-\ngramming”. In: AIJ 72.1 (1995), pp. 81–138. url: http://www.sciencedirect.com/science/\narticle/pii/000437029400011O.\n[BDG00]\nC. Boutilier, R. Dearden, and M. Goldszmidt. “Stochastic dynamic programming with factored\nrepresentations”. en. In: Artif. Intell. 121.1-2 (Aug. 2000), pp. 49–107. url: http://dx.doi.\norg/10.1016/S0004-3702(00)00033-3.\n117\n[BDM10]\nM. Briers, A. Doucet, and S. Maskel. “Smoothing algorithms for state-space models”. In: Annals\nof the Institute of Statistical Mathematics 62.1 (2010), pp. 61–89.\n[BDM17]\nM. G. Bellemare, W. Dabney, and R. Munos. “A Distributional Perspective on Reinforcement\nLearning”. In: ICML. 2017. url: http://arxiv.org/abs/1707.06887.\n[BDR23]\nM. G. Bellemare, W. Dabney, and M. Rowland. Distributional Reinforcement Learning. http:\n//www.distributional-rl.org. MIT Press, 2023.\n[Bel+13]\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. “The Arcade Learning Environment:\nAn Evaluation Platform for General Agents”. In: JAIR 47 (2013), pp. 253–279.\n[Bel+16]\nM. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. “Unifying\nCount-Based Exploration and Intrinsic Motivation”. In: NIPS. 2016. url: http://arxiv.org/\nabs/1606.01868.\n[Ber19]\nD. Bertsekas. Reinforcement learning and optimal control. Athena Scientific, 2019. url: http:\n//www.mit.edu/~dimitrib/RLbook.html.\n[Ber24]\nD. P. Bertsekas. “Model Predictive Control and Reinforcement Learning: A unified framework\nbased on Dynamic Programming”. In: arXiv [eess.SY] (June 2024). url: http://arxiv.org/\nabs/2406.00592.\n[Bha+24]\nP. Bhargava, R. Chitnis, A. Geramifard, S. Sodhani, and A. Zhang. “When should we prefer\nDecision Transformers for Offline Reinforcement Learning?” In: ICLR. 2024. url: https:\n//arxiv.org/abs/2305.14550.\n[BHP17]\nP.-L. Bacon, J. Harb, and D. Precup. “The Option-Critic Architecture”. In: AAAI. 2017.\n[BKH16]\nJ. L. Ba, J. R. Kiros, and G. E. Hinton. “Layer Normalization”. In: (2016). arXiv: 1607.06450\n[stat.ML]. url: http://arxiv.org/abs/1607.06450.\n[BLM16]\nS. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory\nof Independence. Oxford University Press, 2016.\n[BM+18]\nG. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, T. B. Dhruva, A. Muldal,\nN. Heess, and T. Lillicrap. “Distributed Distributional Deterministic Policy Gradients”. In:\nICLR. 2018. url: https://openreview.net/forum?id=SyZipzbCb&noteId=SyZipzbCb.\n[BMS11]\nS. Bubeck, R. Munos, and G. Stoltz. “Pure Exploration in Finitely-armed and Continuous-armed\nBandits”. In: Theoretical Computer Science 412.19 (2011), pp. 1832–1852.\n[Boe+05]\nP.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein. “A Tutorial on the Cross-Entropy\nMethod”. en. In: Ann. Oper. Res. 134.1 (2005), pp. 19–67. url: https://link.springer.com/\narticle/10.1007/s10479-005-5724-z.\n[Bor+19]\nD. Borsa, A. Barreto, J. Quan, D. J. Mankowitz, H. van Hasselt, R. Munos, D. Silver, and\nT. Schaul. “Universal Successor Features Approximators”. In: ICLR. 2019. url: https://\nopenreview.net/pdf?id=S1VWjiRcKX.\n[Bos16]\nN. Bostrom. Superintelligence: Paths, Dangers, Strategies. en. London, England: Oxford Uni-\nversity Press, Mar. 2016. url: https://www.amazon.com/Superintelligence-Dangers-\nStrategies-Nick-Bostrom/dp/0198739834.\n[Bou+23]\nK. Bousmalis et al. “RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation”.\nIn: TMLR (June 2023). url: http://arxiv.org/abs/2306.11706.\n[Bra+22]\nD. Brandfonbrener, A. Bietti, J. Buckman, R. Laroche, and J. Bruna. “When does return-\nconditioned supervised learning work for offline reinforcement learning?” In: NIPS. June 2022.\nurl: http://arxiv.org/abs/2206.01079.\n[BS23]\nA. Bagaria and T. Schaul. “Scaling goal-based exploration via pruning proto-goals”. en. In: IJCAI.\nAug. 2023, pp. 3451–3460. url: https://dl.acm.org/doi/10.24963/ijcai.2023/384.\n118\n[BSA83]\nA. G. Barto, R. S. Sutton, and C. W. Anderson. “Neuronlike adaptive elements that can\nsolve difficult learning control problems”. In: SMC 13.5 (1983), pp. 834–846. url: http :\n//dx.doi.org/10.1109/TSMC.1983.6313077.\n[BT12]\nM. Botvinick and M. Toussaint. “Planning as inference”. en. In: Trends Cogn. Sci. 16.10 (2012),\npp. 485–488. url: https://pdfs.semanticscholar.org/2ba7/88647916f6206f7fcc137fe7866c58e6211e.\npdf.\n[Buc+17]\nC. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth. “The free energy principle for action\nand perception: A mathematical review”. In: J. Math. Psychol. 81 (2017), pp. 55–79. url:\nhttps://www.sciencedirect.com/science/article/pii/S0022249617300962.\n[Bur+18]\nY. Burda, H. Edwards, A Storkey, and O. Klimov. “Exploration by random network distillation”.\nIn: ICLR. Vol. abs/1810.12894. Sept. 2018.\n[BXS20]\nH. Bharadhwaj, K. Xie, and F. Shkurti. “Model-Predictive Control via Cross-Entropy and\nGradient-Based Optimization”. en. In: Learning for Dynamics and Control. PMLR, July 2020,\npp. 277–286. url: https://proceedings.mlr.press/v120/bharadhwaj20a.html.\n[CA13]\nE. F. Camacho and C. B. Alba. Model predictive control. Springer, 2013.\n[Cao+24]\nY. Cao, H. Zhao, Y. Cheng, T. Shu, G. Liu, G. Liang, J. Zhao, and Y. Li. “Survey on large\nlanguage model-enhanced reinforcement learning: Concept, taxonomy, and methods”. In: arXiv\n[cs.LG] (Mar. 2024). url: http://arxiv.org/abs/2404.00282.\n[Car+23]\nW. C. Carvalho, A. Saraiva, A. Filos, A. Lampinen, L. Matthey, R. L. Lewis, H. Lee, S. Singh, D.\nJimenez Rezende, and D. Zoran. “Combining Behaviors with the Successor Features Keyboard”.\nIn: NIPS. Vol. 36. 2023, pp. 9956–9983. url: https://proceedings.neurips.cc/paper_\nfiles/paper/2023/hash/1f69928210578f4cf5b538a8c8806798- Abstract- Conference.\nhtml.\n[Car+24]\nW. Carvalho, M. S. Tomov, W. de Cothi, C. Barry, and S. J. Gershman. “Predictive rep-\nresentations: building blocks of intelligence”. In: Neural Comput. (Feb. 2024). url: https:\n//gershmanlab.com/pubs/Carvalho24.pdf.\n[Cas11]\nP. S. Castro. “On planning, prediction and knowledge transfer in Fully and Partially Observable\nMarkov Decision Processes”. en. PhD thesis. McGill, 2011. url: https://www.proquest.com/\nopenview/d35984acba38c072359f8a8d5102c777/1?pq-origsite=gscholar&cbl=18750.\n[Cas20]\nP. S. Castro. “Scalable methods for computing state similarity in deterministic Markov Decision\nProcesses”. In: AAAI. 2020.\n[Cas+21]\nP. S. Castro, T. Kastner, P. Panangaden, and M. Rowland. “MICo: Improved representations\nvia sampling-based state similarity for Markov decision processes”. In: NIPS. Nov. 2021. url:\nhttps://openreview.net/pdf?id=wFp6kmQELgu.\n[Cas+23]\nP. S. Castro, T. Kastner, P. Panangaden, and M. Rowland. “A kernel perspective on behavioural\nmetrics for Markov decision processes”. In: TMLR abs/2310.19804 (Oct. 2023). url: https:\n//openreview.net/pdf?id=nHfPXl1ly7.\n[Cat+23]\nE. Catt, J. Grau-Moya, M. Hutter, M. Aitchison, T. Genewein, G. Delétang, K. Li, and J.\nVeness. “Self-Predictive Universal AI”. In: NIPS. Vol. 36. 2023, pp. 27181–27198. url: https://\nproceedings.neurips.cc/paper_files/paper/2023/hash/56a225639da77e8f7c0409f6d5ba996b-\nAbstract-Conference.html.\n[Cen21]\nCenter for Research on Foundation Models (CRFM). “On the Opportunities and Risks of\nFoundation Models”. In: (2021). arXiv: 2108.07258 [cs.LG]. url: http://arxiv.org/abs/\n2108.07258.\n[Cet+24]\nE. Cetin, A. Tirinzoni, M. Pirotta, A. Lazaric, Y. Ollivier, and A. Touati. “Simple ingredients\nfor offline reinforcement learning”. In: arXiv [cs.LG] (Mar. 2024). url: http://arxiv.org/\nabs/2403.13097.\n119\n[Cha+21]\nA. Chan, H. Silva, S. Lim, T. Kozuno, A Mahmood, and M. White. “Greedification opera-\ntors for policy optimization: Investigating forward and reverse KL divergences”. In: JMLR\nabs/2107.08285.253 (July 2021), pp. 1–79. url: http://jmlr.org/papers/v23/21-054.html.\n[Che+20]\nX. Chen, C. Wang, Z. Zhou, and K. W. Ross. “Randomized Ensembled Double Q-Learning:\nLearning Fast Without a Model”. In: ICLR. Oct. 2020. url: https://openreview.net/pdf?\nid=AY8zfZm0tDd.\n[Che+21a]\nC. Chen, Y.-F. Wu, J. Yoon, and S. Ahn. “TransDreamer: Reinforcement Learning with\nTransformer World Models”. In: Deep RL Workshop NeurIPS. 2021. url: http://arxiv.org/\nabs/2202.09481.\n[Che+21b]\nL. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and\nI. Mordatch. “Decision Transformer: Reinforcement Learning via Sequence Modeling”. In: arXiv\n[cs.LG] (June 2021). url: http://arxiv.org/abs/2106.01345.\n[Che+24a]\nF. Che, C. Xiao, J. Mei, B. Dai, R. Gummadi, O. A. Ramirez, C. K. Harris, A. R. Mahmood, and\nD. Schuurmans. “Target networks and over-parameterization stabilize off-policy bootstrapping\nwith function approximation”. In: ICML. May 2024.\n[Che+24b]\nJ. Chen, B. Ganguly, Y. Xu, Y. Mei, T. Lan, and V. Aggarwal. “Deep Generative Models for\nOffline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions”. In: TMLR\n(Feb. 2024). url: https://openreview.net/forum?id=Mm2cMDl9r5.\n[Che+24c]\nJ. Chen, B. Ganguly, Y. Xu, Y. Mei, T. Lan, and V. Aggarwal. “Deep Generative Models for\nOffline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions”. In: TMLR\n(Feb. 2024). url: https://openreview.net/forum?id=Mm2cMDl9r5.\n[Che+24d]\nW. Chen, O. Mees, A. Kumar, and S. Levine. “Vision-language models provide promptable\nrepresentations for reinforcement learning”. In: arXiv [cs.LG] (Feb. 2024). url: http://arxiv.\norg/abs/2402.02651.\n[Chr19]\nP. Christodoulou. “Soft Actor-Critic for discrete action settings”. In: arXiv [cs.LG] (Oct. 2019).\nurl: http://arxiv.org/abs/1910.07207.\n[Chu+18]\nK. Chua, R. Calandra, R. McAllister, and S. Levine. “Deep Reinforcement Learning in a Handful\nof Trials using Probabilistic Dynamics Models”. In: NIPS. 2018. url: http://arxiv.org/abs/\n1805.12114.\n[CL11]\nO. Chapelle and L. Li. “An empirical evaluation of Thompson sampling”. In: NIPS. 2011.\n[CMS07]\nB. Colson, P. Marcotte, and G. Savard. “An overview of bilevel optimization”. en. In: Ann. Oper.\nRes. 153.1 (Sept. 2007), pp. 235–256. url: https://link.springer.com/article/10.1007/\ns10479-007-0176-2.\n[Cob+19]\nK. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. “Quantifying Generalization in\nReinforcement Learning”. en. In: ICML. May 2019, pp. 1282–1289. url: https://proceedings.\nmlr.press/v97/cobbe19a.html.\n[Col+22]\nC. Colas, T. Karch, O. Sigaud, and P.-Y. Oudeyer. “Autotelic agents with intrinsically motivated\ngoal-conditioned reinforcement learning: A short survey”. en. In: JAIR 74 (July 2022), pp. 1159–\n1199. url: https://www.jair.org/index.php/jair/article/view/13554.\n[CS04]\nI. Csiszár and P. C. Shields. “Information theory and statistics: A tutorial”. In: (2004).\n[Csi67]\nI. Csiszar. “Information-Type Measures of Difference of Probability Distributions and Indirect\nObservations”. In: Studia Scientiarum Mathematicarum Hungarica 2 (1967), pp. 299–318.\n[CVRM23]\nF. Che, G. Vasan, and A Rupam Mahmood. “Correcting discount-factor mismatch in on-\npolicy policy gradient methods”. en. In: ICML. PMLR, July 2023, pp. 4218–4240. url: https:\n//proceedings.mlr.press/v202/che23a.html.\n120\n[Dab+17]\nW. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. “Distributional reinforcement learning\nwith quantile regression”. In: arXiv [cs.AI] (Oct. 2017). url: http://arxiv.org/abs/1710.\n10044.\n[Dab+18]\nW. Dabney, G. Ostrovski, D. Silver, and R. Munos. “Implicit quantile networks for distributional\nreinforcement learning”. In: arXiv [cs.LG] (June 2018). url: http://arxiv.org/abs/1806.\n06923.\n[Dan+16]\nC. Daniel, H. van Hoof, J. Peters, and G. Neumann. “Probabilistic inference for determining\noptions in reinforcement learning”. en. In: Mach. Learn. 104.2-3 (Sept. 2016), pp. 337–357. url:\nhttps://link.springer.com/article/10.1007/s10994-016-5580-x.\n[Day93]\nP. Dayan. “Improving generalization for temporal difference learning: The successor representa-\ntion”. en. In: Neural Comput. 5.4 (July 1993), pp. 613–624. url: https://ieeexplore.ieee.\norg/abstract/document/6795455.\n[DFR15]\nM. P. Deisenroth, D. Fox, and C. E. Rasmussen. “Gaussian Processes for Data-Efficient Learning\nin Robotics and Control”. en. In: IEEE PAMI 37.2 (2015), pp. 408–423. url: http://dx.doi.\norg/10.1109/TPAMI.2013.218.\n[DH92]\nP. Dayan and G. E. Hinton. “Feudal Reinforcement Learning”. In: NIPS 5 (1992). url: https://\nproceedings.neurips.cc/paper_files/paper/1992/file/d14220ee66aeec73c49038385428ec4c-\nPaper.pdf.\n[Die00]\nT. G. Dietterich. “Hierarchical reinforcement learning with the MAXQ value function decompo-\nsition”. en. In: JAIR 13 (Nov. 2000), pp. 227–303. url: https://www.jair.org/index.php/\njair/article/view/10266.\n[Die+07]\nM. Diehl, H. G. Bock, H. Diedam, and P.-B. Wieber. “Fast Direct Multiple Shooting Algo-\nrithms for Optimal Robot Control”. In: Lecture Notes in Control and Inform. Sci. 340 (2007).\nurl: https://www.researchgate.net/publication/29603798_Fast_Direct_Multiple_\nShooting_Algorithms_for_Optimal_Robot_Control.\n[DMKM22]\nG. Duran-Martin, A. Kara, and K. Murphy. “Efficient Online Bayesian Inference for Neural\nBandits”. In: AISTATS. 2022. url: http://arxiv.org/abs/2112.00195.\n[D’O+22]\nP. D’Oro, M. Schwarzer, E. Nikishin, P.-L. Bacon, M. G. Bellemare, and A. Courville. “Sample-\nEfficient Reinforcement Learning by Breaking the Replay Ratio Barrier”. In: Deep Reinforcement\nLearning Workshop NeurIPS 2022. Dec. 2022. url: https://openreview.net/pdf?id=\n4GBGwVIEYJ.\n[DOB21]\nW. Dabney, G. Ostrovski, and A. Barreto. “Temporally-Extended epsilon-Greedy Exploration”.\nIn: ICLR. 2021. url: https://openreview.net/pdf?id=ONBPHFZ7zG4.\n[DR11]\nM. P. Deisenroth and C. E. Rasmussen. “PILCO: A Model-Based and Data-Efficient Approach\nto Policy Search”. In: ICML. 2011. url: http://www.icml-2011.org/papers/323_icmlpaper.\npdf.\n[Du+21]\nC. Du, Z. Gao, S. Yuan, L. Gao, Z. Li, Y. Zeng, X. Zhu, J. Xu, K. Gai, and K.-C. Lee.\n“Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning”. In: KDD.\nKDD ’21. Association for Computing Machinery, 2021, pp. 2792–2801. url: https://doi.org/\n10.1145/3447548.3467089.\n[Duf02]\nM. Duff. “Optimal Learning: Computational procedures for Bayes-adaptive Markov decision\nprocesses”. PhD thesis. U. Mass. Dept. Comp. Sci., 2002. url: http://envy.cs.umass.edu/\nPeople/duff/diss.html.\n[Dum+24]\nV. Dumoulin, D. D. Johnson, P. S. Castro, H. Larochelle, and Y. Dauphin. “A density estimation\nperspective on learning from pairwise human preferences”. In: Trans. on Machine Learning\nResearch 2024 (2024). url: https://openreview.net/pdf?id=YH3oERVYjF.\n121\n[DVRZ22]\nS. Dong, B. Van Roy, and Z. Zhou. “Simple Agent, Complex Environment: Efficient Reinforce-\nment Learning with Agent States”. In: J. Mach. Learn. Res. (2022). url: https://www.jmlr.\norg/papers/v23/21-0773.html.\n[DWS12]\nT. Degris, M. White, and R. S. Sutton. “Off-Policy Actor-Critic”. In: ICML. 2012. url: http:\n//arxiv.org/abs/1205.4839.\n[Eco+19]\nA. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. “Go-Explore: a New Approach\nfor Hard-Exploration Problems”. In: (2019). arXiv: 1901.10995 [cs.LG]. url: http://arxiv.\norg/abs/1901.10995.\n[Eco+21]\nA. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. “First return, then explore”.\nen. In: Nature 590.7847 (Feb. 2021), pp. 580–586. url: https://www.nature.com/articles/\ns41586-020-03157-9.\n[Emm+21]\nS. Emmons, B. Eysenbach, I. Kostrikov, and S. Levine. “RvS: What is essential for offline RL\nvia Supervised Learning?” In: arXiv [cs.LG] (Dec. 2021). url: http://arxiv.org/abs/2112.\n10751.\n[ESL21]\nB. Eysenbach, R. Salakhutdinov, and S. Levine. “C-Learning: Learning to Achieve Goals via\nRecursive Classification”. In: ICLR. 2021. url: https://openreview.net/pdf?id=tc5qisoB-\nC.\n[Esp+18]\nL. Espeholt et al. “IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-\nLearner Architectures”. en. In: ICML. PMLR, July 2018, pp. 1407–1416. url: https://\nproceedings.mlr.press/v80/espeholt18a.html.\n[Eys+20]\nB. Eysenbach, X. Geng, S. Levine, and R. Salakhutdinov. “Rewriting History with Inverse RL:\nHindsight Inference for Policy Improvement”. In: NIPS. Feb. 2020.\n[Eys+21]\nB. Eysenbach, A. Khazatsky, S. Levine, and R. Salakhutdinov. “Mismatched No More: Joint\nModel-Policy Optimization for Model-Based RL”. In: (2021). arXiv: 2110.02758 [cs.LG]. url:\nhttp://arxiv.org/abs/2110.02758.\n[Eys+22]\nB. Eysenbach, A. Khazatsky, S. Levine, and R. Salakhutdinov. “Mismatched No More: Joint\nModel-Policy Optimization for Model-Based RL”. In: NIPS. 2022.\n[Far+18]\nG. Farquhar, T. Rocktäschel, M. Igl, and S. Whiteson. “TreeQN and ATreeC: Differentiable\nTree-Structured Models for Deep Reinforcement Learning”. In: ICLR. Feb. 2018. url: https:\n//openreview.net/pdf?id=H1dh6Ax0Z.\n[Far+23]\nJ. Farebrother, J. Greaves, R. Agarwal, C. Le Lan, R. Goroshin, P. S. Castro, and M. G.\nBellemare. “Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks”. In:\nICLR. 2023. url: https://openreview.net/pdf?id=oGDKSt9JrZi.\n[Far+24]\nJ. Farebrother et al. “Stop regressing: Training value functions via classification for scalable\ndeep RL”. In: arXiv [cs.LG] (Mar. 2024). url: http://arxiv.org/abs/2403.03950.\n[FC24]\nJ. Farebrother and P. S. Castro. “CALE: Continuous Arcade Learning Environment”. In: NIPS.\nOct. 2024. url: https://arxiv.org/abs/2410.23810.\n[FG21]\nS. Fujimoto and S. s. Gu. “A Minimalist Approach to Offline Reinforcement Learning”. In: NIPS.\nVol. 34. Dec. 2021, pp. 20132–20145. url: https://proceedings.neurips.cc/paper_files/\npaper/2021/file/a8166da05c5a094f7dc03724b41886e5-Paper.pdf.\n[FHM18]\nS. Fujimoto, H. van Hoof, and D. Meger. “Addressing Function Approximation Error in Actor-\nCritic Methods”. In: ICLR. 2018. url: http://arxiv.org/abs/1802.09477.\n[FL+18]\nV. François-Lavet, P. Henderson, R. Islam, M. G. Bellemare, and J. Pineau. “An Introduction\nto Deep Reinforcement Learning”. In: Foundations and Trends in Machine Learning 11.3 (2018).\nurl: http://arxiv.org/abs/1811.12560.\n[FLA16]\nC. Finn, S. Levine, and P. Abbeel. “Guided Cost Learning: Deep Inverse Optimal Control via\nPolicy Optimization”. In: ICML. 2016, pp. 49–58.\n122\n[FLL18]\nJ. Fu, K. Luo, and S. Levine. “Learning Robust Rewards with Adverserial Inverse Reinforcement\nLearning”. In: ICLR. 2018.\n[For+18]\nM. Fortunato et al. “Noisy Networks for Exploration”. In: ICLR. 2018. url: http://arxiv.\norg/abs/1706.10295.\n[FPP04]\nN. Ferns, P. Panangaden, and D. Precup. “Metrics for finite Markov decision processes”. en. In:\nUAI. 2004. url: https://dl.acm.org/doi/10.5555/1036843.1036863.\n[Fra+24]\nB. Frauenknecht, A. Eisele, D. Subhasish, F. Solowjow, and S. Trimpe. “Trust the Model Where\nIt Trusts Itself - Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption”. In:\nICML. June 2024. url: https://openreview.net/pdf?id=N0ntTjTfHb.\n[Fre+24]\nB. Freed, T. Wei, R. Calandra, J. Schneider, and H. Choset. “Unifying Model-Based and\nModel-Free Reinforcement Learning with Equivalent Policy Sets”. In: RL Conference. 2024. url:\nhttps://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_37.pdf.\n[Fri03]\nK. Friston. “Learning and inference in the brain”. en. In: Neural Netw. 16.9 (2003), pp. 1325–1352.\nurl: http://dx.doi.org/10.1016/j.neunet.2003.06.005.\n[Fri09]\nK. Friston. “The free-energy principle: a rough guide to the brain?” en. In: Trends Cogn. Sci.\n13.7 (2009), pp. 293–301. url: http://dx.doi.org/10.1016/j.tics.2009.04.005.\n[FS+19]\nH Francis Song et al. “V-MPO: On-Policy Maximum a Posteriori Policy Optimization for\nDiscrete and Continuous Control”. In: arXiv [cs.AI] (Sept. 2019). url: http://arxiv.org/\nabs/1909.12238.\n[FSW23]\nM. Fellows, M. J. A. Smith, and S. Whiteson. “Why Target Networks Stabilise Temporal Differ-\nence Methods”. en. In: ICML. PMLR, July 2023, pp. 9886–9909. url: https://proceedings.\nmlr.press/v202/fellows23a.html.\n[Fu15]\nM. Fu, ed. Handbook of Simulation Optimization. 1st ed. Springer-Verlag New York, 2015. url:\nhttp://www.springer.com/us/book/9781493913831.\n[Fu+20]\nJ. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4RL: Datasets for Deep Data-Driven\nReinforcement Learning. arXiv:2004.07219. 2020.\n[Fuj+19]\nS. Fujimoto, E. Conti, M. Ghavamzadeh, and J. Pineau. “Benchmarking batch deep reinforcement\nlearning algorithms”. In: Deep RL Workshop NeurIPS. Oct. 2019. url: https://arxiv.org/\nabs/1910.01708.\n[Gal+24]\nM. Gallici, M. Fellows, B. Ellis, B. Pou, I. Masmitja, J. N. Foerster, and M. Martin. “Simplifying\ndeep temporal difference learning”. In: ICML. July 2024.\n[Gar23]\nR. Garnett. Bayesian Optimization. Cambridge University Press, 2023. url: https://bayesoptbook.\ncom/.\n[GBS22]\nC. Grimm, A. Barreto, and S. Singh. “Approximate Value Equivalence”. In: NIPS. Oct. 2022.\nurl: https://openreview.net/pdf?id=S2Awu3Zn04v.\n[GDG03]\nR. Givan, T. Dean, and M. Greig. “Equivalence notions and model minimization in Markov\ndecision processes”. en. In: Artif. Intell. 147.1-2 (July 2003), pp. 163–223. url: https://www.\nsciencedirect.com/science/article/pii/S0004370202003764.\n[GDWF22]\nJ. Grudzien, C. A. S. De Witt, and J. Foerster. “Mirror Learning: A Unifying Framework of Policy\nOptimisation”. In: ICML. Vol. 162. Proceedings of Machine Learning Research. PMLR, 2022,\npp. 7825–7844. url: https://proceedings.mlr.press/v162/grudzien22a/grudzien22a.\npdf.\n[Ger18]\nS. J. Gershman. “Deconstructing the human algorithms for exploration”. en. In: Cognition 173\n(Apr. 2018), pp. 34–42. url: https://www.sciencedirect.com/science/article/abs/pii/\nS0010027717303359.\n[Ger19]\nS. J. Gershman. “What does the free energy principle tell us about the brain?” In: Neurons,\nBehavior, Data Analysis, and Theory (2019). url: http://arxiv.org/abs/1901.07945.\n123\n[GGN22]\nS. K. S. Ghasemipour, S. S. Gu, and O. Nachum. “Why So Pessimistic? Estimating Uncertainties\nfor Offline RL through Ensembles, and Why Their Independence Matters”. In: NIPS. Oct. 2022.\nurl: https://openreview.net/pdf?id=z64kN1h1-rR.\n[Ghi+20]\nS. Ghiassian, A. Patterson, S. Garg, D. Gupta, A. White, and M. White. “Gradient temporal-\ndifference learning with Regularized Corrections”. In: ICML. July 2020.\n[Gho+21]\nD. Ghosh, J. Rahme, A. Kumar, A. Zhang, R. P. Adams, and S. Levine. “Why Generalization\nin RL is Difficult: Epistemic POMDPs and Implicit Partial Observability”. In: NIPS. Vol. 34.\nDec. 2021, pp. 25502–25515. url: https://proceedings.neurips.cc/paper_files/paper/\n2021/file/d5ff135377d39f1de7372c95c74dd962-Paper.pdf.\n[Ghu+22]\nR. Ghugare, H. Bharadhwaj, B. Eysenbach, S. Levine, and R. Salakhutdinov. “Simplifying Model-\nbased RL: Learning Representations, Latent-space Models, and Policies with One Objective”.\nIn: ICLR. Sept. 2022. url: https://openreview.net/forum?id=MQcmfgRxf7a.\n[Git89]\nJ. Gittins. Multi-armed Bandit Allocation Indices. Wiley, 1989.\n[GK19]\nL. Graesser and W. L. Keng. Foundations of Deep Reinforcement Learning: Theory and Practice\nin Python. en. 1 edition. Addison-Wesley Professional, 2019. url: https://www.amazon.com/\nDeep-Reinforcement-Learning-Python-Hands/dp/0135172381.\n[GM+24]\nJ. Grau-Moya et al. “Learning Universal Predictors”. In: arXiv [cs.LG] (Jan. 2024). url:\nhttps://arxiv.org/abs/2401.14953.\n[Gor95]\nG. J. Gordon. “Stable Function Approximation in Dynamic Programming”. In: ICML. 1995,\npp. 261–268.\n[Gra+10]\nT. Graepel, J. Quinonero-Candela, T. Borchert, and R. Herbrich. “Web-Scale Bayesian Click-\nThrough Rate Prediction for Sponsored Search Advertising in Microsoft’s Bing Search Engine”.\nIn: ICML. 2010.\n[Gri+20]\nC. Grimm, A. Barreto, S. Singh, and D. Silver. “The Value Equivalence Principle for Model-Based\nReinforcement Learning”. In: NIPS 33 (2020), pp. 5541–5552. url: https://proceedings.\nneurips.cc/paper_files/paper/2020/file/3bb585ea00014b0e3ebe4c6dd165a358-Paper.\npdf.\n[Gul+20]\nC. Gulcehre et al. RL Unplugged: Benchmarks for Offline Reinforcement Learning. arXiv:2006.13888.\n2020.\n[Guo+22a]\nZ. D. Guo et al. “BYOL-Explore: Exploration by Bootstrapped Prediction”. In: Advances in\nNeural Information Processing Systems. Oct. 2022. url: https://openreview.net/pdf?id=\nqHGCH75usg.\n[Guo+22b]\nZ. D. Guo et al. “BYOL-Explore: Exploration by bootstrapped prediction”. In: NIPS. June 2022.\nurl: https://proceedings.neurips.cc/paper_files/paper/2022/hash/ced0d3b92bb83b15c43ee32c7f57d\nAbstract-Conference.html.\n[GZG19]\nS. K. S. Ghasemipour, R. S. Zemel, and S. Gu. “A Divergence Minimization Perspective on\nImitation Learning Methods”. In: CORL. 2019, pp. 1259–1277.\n[Haa+18a]\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. “Soft Actor-Critic: Off-Policy Maximum\nEntropy Deep Reinforcement Learning with a Stochastic Actor”. In: ICML. 2018. url: http:\n//arxiv.org/abs/1801.01290.\n[Haa+18b]\nT. Haarnoja et al. “Soft Actor-Critic Algorithms and Applications”. In: (2018). arXiv: 1812.\n05905 [cs.LG]. url: http://arxiv.org/abs/1812.05905.\n[Haf+19]\nD. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. “Learning Latent\nDynamics for Planning from Pixels”. In: ICML. 2019. url: http://arxiv.org/abs/1811.\n04551.\n[Haf+20]\nD. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. “Dream to Control: Learning Behaviors by\nLatent Imagination”. In: ICLR. 2020. url: https://openreview.net/forum?id=S1lOTC4tDS.\n124\n[Haf+21]\nD. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. “Mastering Atari with discrete world models”.\nIn: ICLR. 2021.\n[Haf+23]\nD. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. “Mastering Diverse Domains through World\nModels”. In: arXiv [cs.AI] (Jan. 2023). url: http://arxiv.org/abs/2301.04104.\n[Han+19]\nS. Hansen, W. Dabney, A. Barreto, D. Warde-Farley, T. Van de Wiele, and V. Mnih. “Fast\nTask Inference with Variational Intrinsic Successor Features”. In: ICLR. Sept. 2019. url:\nhttps://openreview.net/pdf?id=BJeAHkrYDS.\n[Har+18]\nJ. Harb, P.-L. Bacon, M. Klissarov, and D. Precup. “When waiting is not an option: Learning\noptions with a deliberation cost”. en. In: AAAI 32.1 (Apr. 2018). url: https://ojs.aaai.\norg/index.php/AAAI/article/view/11831.\n[Has10]\nH. van Hasselt. “Double Q-learning”. In: NIPS. Ed. by J. D. Lafferty, C. K. I. Williams, J\nShawe-Taylor, R. S. Zemel, and A Culotta. Curran Associates, Inc., 2010, pp. 2613–2621. url:\nhttp://papers.nips.cc/paper/3964-double-q-learning.pdf.\n[Has+16]\nH. van Hasselt, A. Guez, M. Hessel, V. Mnih, and D. Silver. “Learning values across many\norders of magnitude”. In: NIPS. Feb. 2016.\n[HDCM15]\nA. Hallak, D. Di Castro, and S. Mannor. “Contextual Markov decision processes”. In: arXiv\n[stat.ML] (Feb. 2015). url: http://arxiv.org/abs/1502.02259.\n[HE16]\nJ. Ho and S. Ermon. “Generative Adversarial Imitation Learning”. In: NIPS. 2016, pp. 4565–\n4573.\n[Hes+18]\nM. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,\nM. Azar, and D. Silver. “Rainbow: Combining Improvements in Deep Reinforcement Learning”.\nIn: AAAI. 2018. url: http://arxiv.org/abs/1710.02298.\n[Hes+19]\nM. Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt, and H. van Hasselt. “Multi-task\ndeep reinforcement learning with PopArt”. In: AAAI. 2019.\n[HGS16]\nH. van Hasselt, A. Guez, and D. Silver. “Deep Reinforcement Learning with Double Q-Learning”.\nIn: AAAI. AAAI’16. AAAI Press, 2016, pp. 2094–2100. url: http://dl.acm.org/citation.\ncfm?id=3016100.3016191.\n[HHA19]\nH. van Hasselt, M. Hessel, and J. Aslanides. “When to use parametric models in reinforcement\nlearning?” In: NIPS. 2019. url: http://arxiv.org/abs/1906.05243.\n[HL04]\nD. R. Hunter and K. Lange. “A Tutorial on MM Algorithms”. In: The American Statistician 58\n(2004), pp. 30–37.\n[HL20]\nO. van der Himst and P. Lanillos. “Deep active inference for partially observable MDPs”. In:\nECML workshop on active inference. Sept. 2020. url: https://arxiv.org/abs/2009.03622.\n[HM20]\nM. Hosseini and A. Maida. “Hierarchical Predictive Coding Models in a Deep-Learning Frame-\nwork”. In: (2020). arXiv: 2005.03230 [cs.CV]. url: http://arxiv.org/abs/2005.03230.\n[Hon+10]\nA. Honkela, T. Raiko, M. Kuusela, M. Tornio, and J. Karhunen. “Approximate Riemannian\nConjugate Gradient Learning for Fixed-Form Variational Bayes”. In: JMLR 11.Nov (2010),\npp. 3235–3268. url: http://www.jmlr.org/papers/volume11/honkela10a/honkela10a.pdf.\n[Hon+23]\nM. Hong, H.-T. Wai, Z. Wang, and Z. Yang. “A two-timescale stochastic algorithm framework for\nbilevel optimization: Complexity analysis and application to actor-critic”. en. In: SIAM J. Optim.\n33.1 (Mar. 2023), pp. 147–180. url: https://epubs.siam.org/doi/10.1137/20M1387341.\n[Hou+11]\nN. Houlsby, F. Huszár, Z. Ghahramani, and M. Lengyel. “Bayesian active learning for classifica-\ntion and preference learning”. In: arXiv [stat.ML] (Dec. 2011). url: http://arxiv.org/abs/\n1112.5745.\n[HQC24]\nM. Hutter, D. Quarel, and E. Catt. An introduction to universal artificial intelligence. Chapman\nand Hall, 2024. url: http://www.hutter1.net/ai/uaibook2.htm.\n125\n[HR11]\nR. Hafner and M. Riedmiller. “Reinforcement learning in feedback control: Challenges and\nbenchmarks from technical process control”. en. In: Mach. Learn. 84.1-2 (July 2011), pp. 137–169.\nurl: https://link.springer.com/article/10.1007/s10994-011-5235-x.\n[HR17]\nC. Hoffmann and P. Rostalski. “Linear Optimal Control on Factor Graphs — A Message\nPassing Perspective”. In: Intl. Federation of Automatic Control 50.1 (2017), pp. 6314–6319. url:\nhttps://www.sciencedirect.com/science/article/pii/S2405896317313800.\n[HS18]\nD. Ha and J. Schmidhuber. “World Models”. In: NIPS. 2018. url: http://arxiv.org/abs/\n1803.10122.\n[HSW22a]\nN. A. Hansen, H. Su, and X. Wang. “Temporal Difference Learning for Model Predictive Control”.\nen. In: ICML. PMLR, June 2022, pp. 8387–8406. url: https://proceedings.mlr.press/\nv162/hansen22a.html.\n[HSW22b]\nN. A. Hansen, H. Su, and X. Wang. “Temporal Difference Learning for Model Predictive Control”.\nen. In: ICML. PMLR, June 2022, pp. 8387–8406. url: https://proceedings.mlr.press/\nv162/hansen22a.html.\n[HTB18]\nG. Z. Holland, E. J. Talvitie, and M. Bowling. “The effect of planning shape on Dyna-style\nplanning in high-dimensional state spaces”. In: arXiv [cs.AI] (June 2018). url: http://arxiv.\norg/abs/1806.01825.\n[Hu+20]\nY. Hu, W. Wang, H. Jia, Y. Wang, Y. Chen, J. Hao, F. Wu, and C. Fan. “Learning to Utilize Shap-\ning Rewards: A New Approach of Reward Shaping”. In: NIPS 33 (2020), pp. 15931–15941. url:\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/b710915795b9e9c02cf10d6d2bdb688c-\nPaper.pdf.\n[Hub+21]\nT. Hubert, J. Schrittwieser, I. Antonoglou, M. Barekatain, S. Schmitt, and D. Silver. “Learning\nand planning in complex action spaces”. In: arXiv [cs.LG] (Apr. 2021). url: http://arxiv.\norg/abs/2104.06303.\n[Hut05]\nM. Hutter. Universal Artificial Intelligence: Sequential Decisions Based On Algorithmic Proba-\nbility. en. 2005th ed. Springer, 2005. url: http://www.hutter1.net/ai/uaibook.htm.\n[Ich+23]\nB. Ichter et al. “Do As I Can, Not As I Say: Grounding Language in Robotic Affordances”. en. In:\nConference on Robot Learning. PMLR, Mar. 2023, pp. 287–318. url: https://proceedings.\nmlr.press/v205/ichter23a.html.\n[ID19]\nS. Ivanov and A. D’yakonov. “Modern Deep Reinforcement Learning algorithms”. In: arXiv\n[cs.LG] (June 2019). url: http://arxiv.org/abs/1906.10025.\n[IW18]\nE. Imani and M. White. “Improving Regression Performance with Distributional Losses”. en.\nIn: ICML. PMLR, July 2018, pp. 2157–2166. url: https://proceedings.mlr.press/v80/\nimani18a.html.\n[Jae00]\nH Jaeger. “Observable operator models for discrete stochastic time series”. en. In: Neural\nComput. 12.6 (June 2000), pp. 1371–1398. url: https://direct.mit.edu/neco/article-\npdf/12/6/1371/814514/089976600300015411.pdf.\n[Jan+19a]\nM. Janner, J. Fu, M. Zhang, and S. Levine. “When to Trust Your Model: Model-Based Policy\nOptimization”. In: NIPS. 2019. url: http://arxiv.org/abs/1906.08253.\n[Jan+19b]\nD. Janz, J. Hron, P. Mazur, K. Hofmann, J. M. Hernández-Lobato, and S. Tschiatschek.\n“Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning”. In:\nNIPS. Vol. 32. 2019. url: https://proceedings.neurips.cc/paper_files/paper/2019/\nfile/1b113258af3968aaf3969ca67e744ff8-Paper.pdf.\n[Jan+22]\nM. Janner, Y. Du, J. B. Tenenbaum, and S. Levine. “Planning with Diffusion for Flexible\nBehavior Synthesis”. In: ICML. May 2022. url: http://arxiv.org/abs/2205.09991.\n[Jar+23]\nD. Jarrett, C. Tallec, F. Altché, T. Mesnard, R. Munos, and M. Valko. “Curiosity in Hindsight:\nIntrinsic Exploration in Stochastic Environments”. In: ICML. June 2023. url: https://\nopenreview.net/pdf?id=fIH2G4fnSy.\n126\n[JCM24]\nM. Jones, P. Chang, and K. Murphy. “Bayesian online natural gradient (BONG)”. In: May 2024.\nurl: http://arxiv.org/abs/2405.19681.\n[JGP16]\nE. Jang, S. Gu, and B. Poole. “Categorical Reparameterization with Gumbel-Softmax”. In:\n(2016). arXiv: 1611.01144 [stat.ML]. url: http://arxiv.org/abs/1611.01144.\n[Jia+15]\nN. Jiang, A. Kulesza, S. Singh, and R. Lewis. “The Dependence of Effective Planning Horizon\non Model Accuracy”. en. In: Proceedings of the 2015 International Conference on Autonomous\nAgents and Multiagent Systems. AAMAS ’15. Richland, SC: International Foundation for\nAutonomous Agents and Multiagent Systems, May 2015, pp. 1181–1189. url: https://dl.acm.\norg/doi/10.5555/2772879.2773300.\n[Jin+22]\nL. Jing, P. Vincent, Y. LeCun, and Y. Tian. “Understanding Dimensional Collapse in Contrastive\nSelf-supervised Learning”. In: ICLR. 2022. url: https : / / openreview . net / forum ? id =\nYevsQ05DEN7.\n[JLL21]\nM. Janner, Q. Li, and S. Levine. “Offline Reinforcement Learning as One Big Sequence Modeling\nProblem”. In: NIPS. June 2021.\n[JM70]\nD. H. Jacobson and D. Q. Mayne. Differential Dynamic Programming. Elsevier Press, 1970.\n[JML20]\nM. Janner, I. Mordatch, and S. Levine. “Gamma-Models: Generative Temporal Difference\nLearning for Infinite-Horizon Prediction”. In: NIPS. Vol. 33. 2020, pp. 1724–1735. url: https://\nproceedings.neurips.cc/paper_files/paper/2020/file/12ffb0968f2f56e51a59a6beb37b2859-\nPaper.pdf.\n[Jor+24]\nS. M. Jordan, A. White, B. C. da Silva, M. White, and P. S. Thomas. “Position: Benchmarking\nis Limited in Reinforcement Learning Research”. In: ICML. June 2024. url: https://arxiv.\norg/abs/2406.16241.\n[JSJ94]\nT. Jaakkola, S. Singh, and M. Jordan. “Reinforcement Learning Algorithm for Partially Observ-\nable Markov Decision Problems”. In: NIPS. 1994.\n[KAG19]\nA. Kirsch, J. van Amersfoort, and Y. Gal. “BatchBALD: Efficient and Diverse Batch Acquisition\nfor Deep Bayesian Active Learning”. In: NIPS. 2019. url: http://arxiv.org/abs/1906.08158.\n[Kai+19]\nL. Kaiser et al. “Model-based reinforcement learning for Atari”. In: arXiv [cs.LG] (Mar. 2019).\nurl: http://arxiv.org/abs/1903.00374.\n[Kak01]\nS. M. Kakade. “A Natural Policy Gradient”. In: NIPS. Vol. 14. 2001. url: https://proceedings.\nneurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.\npdf.\n[Kal+18]\nD. Kalashnikov et al. “QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic\nManipulation”. In: CORL. 2018. url: http://arxiv.org/abs/1806.10293.\n[Kap+18]\nS. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. “Recurrent Experience Replay\nin Distributed Reinforcement Learning”. In: ICLR. Sept. 2018. url: https://openreview.\nnet/pdf?id=r1lyTjAqYX.\n[Kap+22]\nS. Kapturowski, V. Campos, R. Jiang, N. Rakicevic, H. van Hasselt, C. Blundell, and A. P.\nBadia. “Human-level Atari 200x faster”. In: ICLR. Sept. 2022. url: https://openreview.net/\npdf?id=JtC6yOHRoJJ.\n[Kau+23]\nT. Kaufmann, P. Weng, V. Bengs, and E. Hüllermeier. “A survey of reinforcement learning from\nhuman feedback”. In: arXiv [cs.LG] (Dec. 2023). url: http://arxiv.org/abs/2312.14925.\n[KB09]\nG. Konidaris and A. Barto. “Skill Discovery in Continuous Reinforcement Learning Domains\nusing Skill Chaining”. In: Advances in Neural Information Processing Systems 22 (2009). url:\nhttps://proceedings.neurips.cc/paper_files/paper/2009/file/e0cf1f47118daebc5b16269099ad7347-\nPaper.pdf.\n127\n[KD18]\nS. Kamthe and M. P. Deisenroth. “Data-Efficient Reinforcement Learning with Probabilistic\nModel Predictive Control”. In: AISTATS. 2018. url: http://proceedings.mlr.press/v84/\nkamthe18a/kamthe18a.pdf.\n[Ke+19]\nL. Ke, S. Choudhury, M. Barnes, W. Sun, G. Lee, and S. Srinivasa. Imitation Learning as\nf-Divergence Minimization. arXiv:1905.12888. 2019.\n[KGO12]\nH. J. Kappen, V. Gómez, and M. Opper. “Optimal control as a graphical model inference\nproblem”. In: Mach. Learn. 87.2 (2012), pp. 159–182. url: https://doi.org/10.1007/s10994-\n012-5278-7.\n[Khe+20]\nK. Khetarpal, Z. Ahmed, G. Comanici, D. Abel, and D. Precup. “What can I do here? A\nTheory of Affordances in Reinforcement Learning”. In: Proceedings of the 37th International\nConference on Machine Learning. Ed. by H. D. Iii and A. Singh. Vol. 119. Proceedings of\nMachine Learning Research. PMLR, 2020, pp. 5243–5253. url: https://proceedings.mlr.\npress/v119/khetarpal20a.html.\n[Kid+20]\nR. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims. “MOReL: Model-Based Offline\nReinforcement Learning”. In: NIPS. Vol. 33. 2020, pp. 21810–21823. url: https://proceedings.\nneurips.cc/paper_files/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.\npdf.\n[Kir+21]\nR. Kirk, A. Zhang, E. Grefenstette, and T. Rocktäschel. “A survey of zero-shot generalisation\nin deep Reinforcement Learning”. In: JAIR (Nov. 2021). url: http://jair.org/index.php/\njair/article/view/14174.\n[KLC98]\nL. P. Kaelbling, M. Littman, and A. Cassandra. “Planning and acting in Partially Observable\nStochastic Domains”. In: AIJ 101 (1998).\n[Kli+24]\nM. Klissarov, P. D’Oro, S. Sodhani, R. Raileanu, P.-L. Bacon, P. Vincent, A. Zhang, and M.\nHenaff. “Motif: Intrinsic motivation from artificial intelligence feedback”. In: ICLR. 2024.\n[KLP11]\nL. P. Kaelbling and T Lozano-Pérez. “Hierarchical task and motion planning in the now”. In:\nICRA. 2011, pp. 1470–1477. url: http://dx.doi.org/10.1109/ICRA.2011.5980391.\n[Koz+21]\nT. Kozuno, Y. Tang, M. Rowland, R Munos, S. Kapturowski, W. Dabney, M. Valko, and\nD. Abel. “Revisiting Peng’s Q-lambda for modern reinforcement learning”. In: ICML 139 (Feb.\n2021). Ed. by M. Meila and T. Zhang, pp. 5794–5804. url: https://proceedings.mlr.press/\nv139/kozuno21a/kozuno21a.pdf.\n[KP19]\nK. Khetarpal and D. Precup. “Learning options with interest functions”. en. In: AAAI 33.01 (July\n2019), pp. 9955–9956. url: https://ojs.aaai.org/index.php/AAAI/article/view/5114.\n[KPL19]\nA. Kumar, X. B. Peng, and S. Levine. “Reward-Conditioned Policies”. In: arXiv [cs.LG] (Dec.\n2019). url: http://arxiv.org/abs/1912.13465.\n[KS02]\nM. Kearns and S. Singh. “Near-Optimal Reinforcement Learning in Polynomial Time”. en. In:\nMLJ 49.2/3 (Nov. 2002), pp. 209–232. url: https://link.springer.com/article/10.1023/\nA:1017984413808.\n[KS06]\nL. Kocsis and C. Szepesvári. “Bandit Based Monte-Carlo Planning”. In: ECML. 2006, pp. 282–\n293.\n[KSS23]\nT. Kneib, A. Silbersdorff, and B. Säfken. “Rage Against the Mean – A Review of Distributional\nRegression Approaches”. In: Econometrics and Statistics 26 (Apr. 2023), pp. 99–123. url:\nhttps://www.sciencedirect.com/science/article/pii/S2452306221000824.\n[Kum+19]\nA. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine. “Stabilizing Off-Policy Q-Learning via\nBootstrapping Error Reduction”. In: NIPS. Vol. 32. 2019. url: https://proceedings.neurips.\ncc/paper_files/paper/2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf.\n[Kum+20]\nA. Kumar, A. Zhou, G. Tucker, and S. Levine. “Conservative Q-Learning for Offline Reinforce-\nment Learning”. In: NIPS. June 2020.\n128\n[Kum+23]\nA. Kumar, R. Agarwal, X. Geng, G. Tucker, and S. Levine. “Offline Q-Learning on Diverse\nMulti-Task Data Both Scales And Generalizes”. In: ICLR. 2023. url: http://arxiv.org/abs/\n2211.15144.\n[Kum+24]\nS. Kumar, H. J. Jeon, A. Lewandowski, and B. Van Roy. “The Need for a Big World Simulator:\nA Scientific Challenge for Continual Learning”. In: Finding the Frame: An RLC Workshop\nfor Examining Conceptual Frameworks. July 2024. url: https://openreview.net/pdf?id=\n10XMwt1nMJ.\n[Kur+19]\nT. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. “Model-Ensemble Trust-Region\nPolicy Optimization”. In: ICLR. 2019. url: http://arxiv.org/abs/1802.10592.\n[LA21]\nH. Liu and P. Abbeel. “APS: Active Pretraining with Successor Features”. en. In: ICML. PMLR,\nJuly 2021, pp. 6736–6747. url: https://proceedings.mlr.press/v139/liu21b.html.\n[Lai+21]\nH. Lai, J. Shen, W. Zhang, Y. Huang, X. Zhang, R. Tang, Y. Yu, and Z. Li. “On effective\nscheduling of model-based reinforcement learning”. In: NIPS 34 (Nov. 2021). Ed. by M Ran-\nzato, A Beygelzimer, Y Dauphin, P. S. Liang, and J. W. Vaughan, pp. 3694–3705. url: https://\nproceedings.neurips.cc/paper_files/paper/2021/hash/1e4d36177d71bbb3558e43af9577d70e-\nAbstract.html.\n[Lam+20]\nN. Lambert, B. Amos, O. Yadan, and R. Calandra. “Objective Mismatch in Model-based\nReinforcement Learning”. In: Conf. on Learning for Dynamics and Control (L4DC). Feb. 2020.\n[Law+22]\nD. Lawson, A. Raventós, A. Warrington, and S. Linderman. “SIXO: Smoothing Inference with\nTwisted Objectives”. In: NIPS. June 2022.\n[Leh24]\nM. Lehmann. “The definitive guide to policy gradients in deep reinforcement learning: Theory,\nalgorithms and implementations”. In: arXiv [cs.LG] (Jan. 2024). url: http://arxiv.org/abs/\n2401.13662.\n[Lev18]\nS. Levine. “Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review”.\nIn: (2018). arXiv: 1805.00909 [cs.LG]. url: http://arxiv.org/abs/1805.00909.\n[Lev+18]\nA. Levy, G. Konidaris, R. Platt, and K. Saenko. “Learning Multi-Level Hierarchies with\nHindsight”. In: ICLR. Sept. 2018. url: https://openreview.net/pdf?id=ryzECoAcY7.\n[Lev+20a]\nS. Levine, A. Kumar, G. Tucker, and J. Fu. “Offline Reinforcement Learning: Tutorial, Review,\nand Perspectives on Open Problems”. In: (2020). arXiv: 2005.01643 [cs.LG]. url: http:\n//arxiv.org/abs/2005.01643.\n[Lev+20b]\nS. Levine, A. Kumar, G. Tucker, and J. Fu. Offline Reinforcement Learning: Tutorial, Review,\nand Perspectives on Open Problems. arXiv:2005.01643. 2020.\n[LG+24]\nM. Lázaro-Gredilla, L. Y. Ku, K. P. Murphy, and D. George. “What type of inference is\nplanning?” In: NIPS. June 2024.\n[LGR12]\nS. Lange, T. Gabel, and M. Riedmiller. “Batch reinforcement learning”. en. In: Adaptation,\nLearning, and Optimization. Adaptation, learning, and optimization. Berlin, Heidelberg: Springer\nBerlin Heidelberg, 2012, pp. 45–73. url: https://link.springer.com/chapter/10.1007/978-\n3-642-27645-3_2.\n[LHC24]\nC. Lu, S. Hu, and J. Clune. “Intelligent Go-Explore: Standing on the shoulders of giant foundation\nmodels”. In: arXiv [cs.LG] (May 2024). url: http://arxiv.org/abs/2405.15143.\n[LHS13]\nT. Lattimore, M. Hutter, and P. Sunehag. “The Sample-Complexity of General Reinforcement\nLearning”. en. In: ICML. PMLR, May 2013, pp. 28–36. url: https://proceedings.mlr.\npress/v28/lattimore13.html.\n[Li+10]\nL. Li, W. Chu, J. Langford, and R. E. Schapire. “A contextual-bandit approach to personalized\nnews article recommendation”. In: WWW. 2010.\n[Li18]\nY. Li. “Deep Reinforcement Learning”. In: (2018). arXiv: 1810.06339 [cs.LG]. url: http:\n//arxiv.org/abs/1810.06339.\n129\n[Li23]\nS. E. Li. Reinforcement learning for sequential decision and optimal control. en. Singapore:\nSpringer Nature Singapore, 2023. url: https://link.springer.com/book/10.1007/978-\n981-19-7784-8.\n[Li+24]\nH. Li, X. Yang, Z. Wang, X. Zhu, J. Zhou, Y. Qiao, X. Wang, H. Li, L. Lu, and J. Dai. “Auto\nMC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft”. In:\nCVPR. 2024, pp. 16426–16435. url: https://openaccess.thecvf.com/content/CVPR2024/\npapers/Li_Auto_MC-Reward_Automated_Dense_Reward_Design_with_Large_Language_\nModels_CVPR_2024_paper.pdf.\n[Lil+16]\nT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.\n“Continuous control with deep reinforcement learning”. In: ICLR. 2016. url: http://arxiv.\norg/abs/1509.02971.\n[Lin+19]\nC. Linke, N. M. Ady, M. White, T. Degris, and A. White. “Adapting behaviour via intrinsic\nreward: A survey and empirical study”. In: J. Artif. Intell. Res. (June 2019). url: http:\n//arxiv.org/abs/1906.07865.\n[Lin+24a]\nJ. Lin, Y. Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein, and A. Dragan. “Learning to model\nthe world with language”. In: ICML. 2024.\n[Lin+24b]\nY.-A. Lin, C.-T. Lee, C.-H. Yang, G.-T. Liu, and S.-H. Sun. “Hierarchical Programmatic Option\nFramework”. In: NIPS. Nov. 2024. url: https://openreview.net/pdf?id=FeCWZviCeP.\n[Lin92]\nL.-J. Lin. “Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and\nTeaching”. In: Mach. Learn. 8.3-4 (1992), pp. 293–321. url: https://doi.org/10.1007/\nBF00992699.\n[Lio+22]\nV. Lioutas, J. W. Lavington, J. Sefas, M. Niedoba, Y. Liu, B. Zwartsenberg, S. Dabiri, F.\nWood, and A. Scibior. “Critic Sequential Monte Carlo”. In: ICLR. Sept. 2022. url: https:\n//openreview.net/pdf?id=ObtGcyKmwna.\n[LMW24]\nB. Li, N. Ma, and Z. Wang. “Rewarded Region Replay (R3) for policy learning with discrete\naction space”. In: arXiv [cs.LG] (May 2024). url: http://arxiv.org/abs/2405.16383.\n[Lor24]\nJ. Lorraine. “Scalable nested optimization for deep learning”. In: arXiv [cs.LG] (July 2024).\nurl: http://arxiv.org/abs/2407.01526.\n[LÖW21]\nT. van de Laar, A. Özçelikkale, and H. Wymeersch. “Application of the Free Energy Principle\nto Estimation and Control”. In: IEEE Trans. Signal Process. 69 (2021), pp. 4234–4244. url:\nhttp://dx.doi.org/10.1109/TSP.2021.3095711.\n[LPC22]\nN. Lambert, K. Pister, and R. Calandra. “Investigating Compounding Prediction Errors in\nLearned Dynamics Models”. In: arXiv [cs.LG] (Mar. 2022). url: http://arxiv.org/abs/\n2203.09637.\n[LR10]\nS. Lange and M. Riedmiller. “Deep auto-encoder neural networks in reinforcement learning”.\nen. In: IJCNN. IEEE, July 2010, pp. 1–8. url: https://ieeexplore.ieee.org/abstract/\ndocument/5596468.\n[LS01]\nM. Littman and R. S. Sutton. “Predictive Representations of State”. In: Advances in Neural\nInformation Processing Systems 14 (2001). url: https://proceedings.neurips.cc/paper_\nfiles/paper/2001/file/1e4d36177d71bbb3558e43af9577d70e-Paper.pdf.\n[LS19]\nT. Lattimore and C. Szepesvari. Bandit Algorithms. Cambridge, 2019.\n[Lu+23]\nX. Lu, B. Van Roy, V. Dwaracherla, M. Ibrahimi, I. Osband, and Z. Wen. “Reinforcement Learn-\ning, Bit by Bit”. In: Found. Trends® Mach. Learn. (2023). url: https://www.nowpublishers.\ncom/article/Details/MAL-097.\n[LV06]\nF. Liese and I. Vajda. “On divergences and informations in statistics and information theory”.\nIn: IEEE Transactions on Information Theory 52.10 (2006), pp. 4394–4412.\n130\n[LWL06]\nL. Li, T. J. Walsh, and M. L. Littman. “Towards a Unified Theory of State Abstraction for\nMDPs”. In: (2006). url: https://thomasjwalsh.net/pub/aima06Towards.pdf.\n[LZZ22]\nM. Liu, M. Zhu, and W. Zhang. “Goal-conditioned reinforcement learning: Problems and\nsolutions”. In: IJCAI. Jan. 2022.\n[Ma+24]\nY. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and\nA. Anandkumar. “Eureka: Human-Level Reward Design via Coding Large Language Models”.\nIn: ICLR. 2024.\n[Mac+18a]\nM. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling.\n“Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for\nGeneral Agents”. In: J. Artif. Intell. Res. (2018). url: http://arxiv.org/abs/1709.06009.\n[Mac+18b]\nM. C. Machado, C. Rosenbaum, X. Guo, M. Liu, G. Tesauro, and M. Campbell. “Eigenoption\nDiscovery through the Deep Successor Representation”. In: ICLR. Feb. 2018. url: https:\n//openreview.net/pdf?id=Bk8ZcAxR-.\n[Mac+23]\nM. C. Machado, A. Barreto, D. Precup, and M. Bowling. “Temporal Abstraction in Reinforcement\nLearning with the Successor Representation”. In: JMLR 24.80 (2023), pp. 1–69. url: http:\n//jmlr.org/papers/v24/21-1213.html.\n[Mad+17]\nC. J. Maddison, D. Lawson, G. Tucker, N. Heess, A. Doucet, A. Mnih, and Y. W. Teh. “Particle\nValue Functions”. In: ICLR Workshop on RL. Mar. 2017.\n[Mae+09]\nH. Maei, C. Szepesvári, S. Bhatnagar, D. Precup, D. Silver, and R. S. Sutton. “Convergent\nTemporal-Difference Learning with Arbitrary Smooth Function Approximation”. In: NIPS.\nVol. 22. 2009. url: https://proceedings.neurips.cc/paper_files/paper/2009/file/\n3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf.\n[MAF22]\nV. Micheli, E. Alonso, and F. Fleuret. “Transformers are Sample-Efficient World Models”. In:\nICLR. Sept. 2022.\n[MAF24]\nV. Micheli, E. Alonso, and F. Fleuret. “Efficient world models with context-aware tokenization”.\nIn: ICML. June 2024.\n[Maj21]\nS. J. Majeed. “Abstractions of general reinforcement learning: An inquiry into the scalability\nof generally intelligent agents”. PhD thesis. ANU, Dec. 2021. url: https://arxiv.org/abs/\n2112.13404.\n[Man+19]\nD. J. Mankowitz, N. Levine, R. Jeong, Y. Shi, J. Kay, A. Abdolmaleki, J. T. Springenberg,\nT. Mann, T. Hester, and M. Riedmiller. “Robust Reinforcement Learning for Continuous\nControl with Model Misspecification”. In: (2019). arXiv: 1906.07516 [cs.LG]. url: http:\n//arxiv.org/abs/1906.07516.\n[Mar10]\nJ Martens. “Deep learning via Hessian-free optimization”. In: ICML. 2010. url: http://www.\ncs.toronto.edu/~asamir/cifar/HFO_James.pdf.\n[Mar16]\nJ. Martens. “Second-order optimization for neural networks”. PhD thesis. Toronto, 2016. url:\nhttp://www.cs.toronto.edu/~jmartens/docs/thesis_phd_martens.pdf.\n[Mar20]\nJ. Martens. “New insights and perspectives on the natural gradient method”. In: JMLR (2020).\nurl: http://arxiv.org/abs/1412.1193.\n[Mar21]\nJ. Marino. “Predictive Coding, Variational Autoencoders, and Biological Connections”. en. In:\nNeural Comput. 34.1 (2021), pp. 1–44. url: http://dx.doi.org/10.1162/neco_a_01458.\n[Maz+22]\nP. Mazzaglia, T. Verbelen, O. Çatal, and B. Dhoedt. “The Free Energy Principle for Perception\nand Action: A Deep Learning Perspective”. en. In: Entropy 24.2 (2022). url: http://dx.doi.\norg/10.3390/e24020301.\n[MBB20]\nM. C. Machado, M. G. Bellemare, and M. Bowling. “Count-based exploration with the successor\nrepresentation”. en. In: AAAI 34.04 (Apr. 2020), pp. 5125–5133. url: https://ojs.aaai.org/\nindex.php/AAAI/article/view/5955.\n131\n[McM+13]\nH. B. McMahan, G. Holt, D Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips, E.\nDavydov, D. Golovin, et al. “Ad click prediction: a view from the trenches”. In: KDD. 2013,\npp. 1222–1230.\n[Men+23]\nW. Meng, Q. Zheng, G. Pan, and Y. Yin. “Off-Policy Proximal Policy Optimization”. en. In:\nAAAI 37.8 (June 2023), pp. 9162–9170. url: https://ojs.aaai.org/index.php/AAAI/\narticle/view/26099.\n[Met+17]\nL. Metz, J. Ibarz, N. Jaitly, and J. Davidson. “Discrete Sequential Prediction of Continuous\nActions for Deep RL”. In: (2017). arXiv: 1705.05035 [cs.LG]. url: http://arxiv.org/abs/\n1705.05035.\n[Mey22]\nS. Meyn. Control Systems and Reinforcement Learning. Cambridge, 2022. url: https://meyn.\nece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/.\n[MG15]\nJ. Martens and R. Grosse. “Optimizing Neural Networks with Kronecker-factored Approximate\nCurvature”. In: ICML. 2015. url: http://arxiv.org/abs/1503.05671.\n[Mik+20]\nV. Mikulik, G. Delétang, T. McGrath, T. Genewein, M. Martic, S. Legg, and P. Ortega. “Meta-\ntrained agents implement Bayes-optimal agents”. In: NIPS 33 (2020), pp. 18691–18703. url:\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/d902c3ce47124c66ce615d5ad9ba304f-\nPaper.pdf.\n[Mil20]\nB. Millidge. “Deep Active Inference as Variational Policy Gradients”. In: J. Mathematical\nPsychology (2020). url: http://arxiv.org/abs/1907.03876.\n[Mil+20]\nB. Millidge, A. Tschantz, A. K. Seth, and C. L. Buckley. “On the Relationship Between Active\nInference and Control as Inference”. In: International Workshop on Active Inference. 2020. url:\nhttp://arxiv.org/abs/2006.12964.\n[MM90]\nD. Q. Mayne and H Michalska. “Receding horizon control of nonlinear systems”. In: IEEE Trans.\nAutomat. Contr. 35.7 (1990), pp. 814–824.\n[MMT17]\nC. J. Maddison, A. Mnih, and Y. W. Teh. “The Concrete Distribution: A Continuous Relaxation\nof Discrete Random Variables”. In: ICLR. 2017. url: http://arxiv.org/abs/1611.00712.\n[MMT24]\nS. Mannor, Y. Mansour, and A. Tamar. Reinforcement Learning: Foundations. 2024. url:\nhttps://sites.google.com/corp/view/rlfoundations/home.\n[Mni+15]\nV. Mnih et al. “Human-level control through deep reinforcement learning”. In: Nature 518.7540\n(2015), pp. 529–533.\n[Mni+16]\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K.\nKavukcuoglu. “Asynchronous Methods for Deep Reinforcement Learning”. In: ICML. 2016. url:\nhttp://arxiv.org/abs/1602.01783.\n[Moe+23]\nT. M. Moerland, J. Broekens, A. Plaat, and C. M. Jonker. “Model-based Reinforcement\nLearning: A Survey”. In: Foundations and Trends in Machine Learning 16.1 (2023), pp. 1–118.\nurl: https://arxiv.org/abs/2006.16712.\n[Moh+20]\nS. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. “Monte Carlo Gradient Estimation in\nMachine Learning”. In: JMLR 21.132 (2020), pp. 1–62. url: http://jmlr.org/papers/v21/19-\n346.html.\n[Mor63]\nT. Morimoto. “Markov Processes and the H-Theorem”. In: J. Phys. Soc. Jpn. 18.3 (1963),\npp. 328–331. url: https://doi.org/10.1143/JPSJ.18.328.\n[MP+22]\nA. Mavor-Parker, K. Young, C. Barry, and L. Griffin. “How to Stay Curious while avoiding Noisy\nTVs using Aleatoric Uncertainty Estimation”. en. In: ICML. PMLR, June 2022, pp. 15220–15240.\nurl: https://proceedings.mlr.press/v162/mavor-parker22a.html.\n[MSB21]\nB. Millidge, A. Seth, and C. L. Buckley. “Predictive Coding: a Theoretical and Experimental\nReview”. In: (2021). arXiv: 2107.12979 [cs.AI]. url: http://arxiv.org/abs/2107.12979.\n132\n[Mun14]\nR. Munos. “From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to\nOptimization and Planning”. In: Foundations and Trends in Machine Learning 7.1 (2014),\npp. 1–129. url: http://dx.doi.org/10.1561/2200000038.\n[Mun+16]\nR. Munos, T. Stepleton, A. Harutyunyan, and M. G. Bellemare. “Safe and Efficient Off-Policy\nReinforcement Learning”. In: NIPS. 2016, pp. 1046–1054.\n[Mur00]\nK. Murphy. A Survey of POMDP Solution Techniques. Tech. rep. Comp. Sci. Div., UC Berkeley,\n2000. url: https://www.cs.ubc.ca/~murphyk/Papers/pomdp.pdf.\n[Mur23]\nK. P. Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023.\n[MWS14]\nJ. Modayil, A. White, and R. S. Sutton. “Multi-timescale nexting in a reinforcement learning\nrobot”. en. In: Adapt. Behav. 22.2 (Apr. 2014), pp. 146–160. url: https://sites.ualberta.\nca/~amw8/nexting.pdf.\n[Nac+18]\nO. Nachum, S. Gu, H. Lee, and S. Levine. “Data-Efficient Hierarchical Reinforcement Learn-\ning”. In: NIPS. May 2018. url: https://proceedings.neurips.cc/paper/2018/hash/\ne6384711491713d29bc63fc5eeb5ba4f-Abstract.html.\n[Nac+19]\nO. Nachum, S. Gu, H. Lee, and S. Levine. “Near-Optimal Representation Learning for Hier-\narchical Reinforcement Learning”. In: ICLR. 2019. url: https://openreview.net/pdf?id=\nH1emus0qF7.\n[Nai+20]\nA. Nair, A. Gupta, M. Dalal, and S. Levine. “AWAC: Accelerating Online Reinforcement\nLearning with Offline Datasets”. In: arXiv [cs.LG] (June 2020). url: http://arxiv.org/abs/\n2006.09359.\n[Nak+23]\nM. Nakamoto, Y. Zhai, A. Singh, M. S. Mark, Y. Ma, C. Finn, A. Kumar, and S. Levine.\n“Cal-QL: Calibrated offline RL pre-training for efficient online fine-tuning”. In: arXiv [cs.LG]\n(Mar. 2023). url: http://arxiv.org/abs/2303.05479.\n[NHR99]\nA. Ng, D. Harada, and S. Russell. “Policy invariance under reward transformations: Theory and\napplication to reward shaping”. In: ICML. 1999.\n[Ni+24]\nT. Ni, B. Eysenbach, E. Seyedsalehi, M. Ma, C. Gehring, A. Mahajan, and P.-L. Bacon. “Bridging\nState and History Representations: Understanding Self-Predictive RL”. In: ICLR. Jan. 2024.\nurl: http://arxiv.org/abs/2401.08898.\n[Nik+22]\nE. Nikishin, R. Abachi, R. Agarwal, and P.-L. Bacon. “Control-oriented model-based reinforce-\nment learning with implicit differentiation”. en. In: AAAI 36.7 (June 2022), pp. 7886–7894. url:\nhttps://ojs.aaai.org/index.php/AAAI/article/view/20758.\n[NR00]\nA. Ng and S. Russell. “Algorithms for inverse reinforcement learning”. In: ICML. 2000.\n[NT20]\nC. Nota and P. S. Thomas. “Is the policy gradient a gradient?” In: Proc. of the 19th International\nConference on Autonomous Agents and MultiAgent Systems. 2020.\n[NWJ10]\nX Nguyen, M. J. Wainwright, and M. I. Jordan. “Estimating Divergence Functionals and the\nLikelihood Ratio by Convex Risk Minimization”. In: IEEE Trans. Inf. Theory 56.11 (2010),\npp. 5847–5861. url: http://dx.doi.org/10.1109/TIT.2010.2068870.\n[OCD21]\nG. Ostrovski, P. S. Castro, and W. Dabney. “The Difficulty of Passive Learning in Deep Reinforce-\nment Learning”. In: NIPS. Vol. 34. Dec. 2021, pp. 23283–23295. url: https://proceedings.\nneurips.cc/paper_files/paper/2021/file/c3e0c62ee91db8dc7382bde7419bb573-Paper.\npdf.\n[OK22]\nA. Ororbia and D. Kifer. “The neural coding framework for learning generative models”. en. In:\nNat. Commun. 13.1 (Apr. 2022), p. 2064. url: https://www.nature.com/articles/s41467-\n022-29632-7.\n[ORVR13]\nI. Osband, D. Russo, and B. Van Roy. “(More) Efficient Reinforcement Learning via Posterior\nSampling”. In: NIPS. 2013. url: http://arxiv.org/abs/1306.0940.\n133\n[Osb+19]\nI. Osband, B. Van Roy, D. J. Russo, and Z. Wen. “Deep exploration via randomized value\nfunctions”. In: JMLR 20.124 (2019), pp. 1–62. url: http://jmlr.org/papers/v20/18-\n339.html.\n[Osb+23a]\nI. Osband, Z. Wen, S. M. Asghari, V. Dwaracherla, M. Ibrahimi, X. Lu, and B. Van Roy.\n“Approximate Thompson Sampling via Epistemic Neural Networks”. en. In: UAI. PMLR, July\n2023, pp. 1586–1595. url: https://proceedings.mlr.press/v216/osband23a.html.\n[Osb+23b]\nI. Osband, Z. Wen, S. M. Asghari, V. Dwaracherla, M. Ibrahimi, X. Lu, and B. Van Roy.\n“Epistemic Neural Networks”. In: NIPS. 2023. url: https://proceedings.neurips.cc/paper_\nfiles/paper/2023/file/07fbde96bee50f4e09303fd4f877c2f3-Paper-Conference.pdf.\n[OSL17]\nJ. Oh, S. Singh, and H. Lee. “Value Prediction Network”. In: NIPS. July 2017.\n[OT22]\nM. Okada and T. Taniguchi. “DreamingV2: Reinforcement learning with discrete world models\nwithout reconstruction”. en. In: 2022 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS). IEEE, Oct. 2022, pp. 985–991. url: https://ieeexplore.ieee.org/\nabstract/document/9981405.\n[Ouy+22]\nL. Ouyang et al. “Training language models to follow instructions with human feedback”. In:\n(Mar. 2022). arXiv: 2203.02155 [cs.CL]. url: http://arxiv.org/abs/2203.02155.\n[OVR17]\nI. Osband and B. Van Roy. “Why is posterior sampling better than optimism for reinforcement\nlearning?” In: ICML. 2017, pp. 2701–2710.\n[PAG24]\nM. Panwar, K. Ahuja, and N. Goyal. “In-context learning through the Bayesian prism”. In:\nICLR. 2024. url: https://arxiv.org/abs/2306.04891.\n[Par+23]\nJ. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. “Generative\nagents: Interactive simulacra of human behavior”. en. In: Proceedings of the 36th Annual ACM\nSymposium on User Interface Software and Technology. New York, NY, USA: ACM, Oct. 2023.\nurl: https://dl.acm.org/doi/10.1145/3586183.3606763.\n[Par+24a]\nS. Park, K. Frans, B. Eysenbach, and S. Levine. “OGBench: Benchmarking Offline Goal-\nConditioned RL”. In: arXiv [cs.LG] (Oct. 2024). url: http://arxiv.org/abs/2410.20092.\n[Par+24b]\nS. Park, K. Frans, S. Levine, and A. Kumar. “Is value learning really the main bottleneck in\noffline RL?” In: NIPS. June 2024. url: https://arxiv.org/abs/2406.09329.\n[Pat+17]\nD. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. “Curiosity-driven Exploration by Self-\nsupervised Prediction”. In: ICML. 2017. url: http://arxiv.org/abs/1705.05363.\n[Pat+22]\nS. Pateria, B. Subagdja, A.-H. Tan, and C. Quek. “Hierarchical Reinforcement Learning: A\ncomprehensive survey”. en. In: ACM Comput. Surv. 54.5 (June 2022), pp. 1–35. url: https:\n//dl.acm.org/doi/10.1145/3453160.\n[Pat+24]\nA. Patterson, S. Neumann, M. White, and A. White. “Empirical design in reinforcement learning”.\nIn: JMLR (2024). url: http://arxiv.org/abs/2304.01315.\n[PB+14]\nN. Parikh, S. Boyd, et al. “Proximal algorithms”. In: Foundations and Trends in Optimization\n1.3 (2014), pp. 127–239.\n[Pea84]\nJ. Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-Wesley\nLongman Publishing Co., Inc., 1984. url: https://dl.acm.org/citation.cfm?id=525.\n[Pea94]\nB. A. Pearlmutter. “Fast Exact Multiplication by the Hessian”. In: Neural Comput. 6.1 (1994),\npp. 147–160. url: https://doi.org/10.1162/neco.1994.6.1.147.\n[Pen+19]\nX. B. Peng, A. Kumar, G. Zhang, and S. Levine. “Advantage-weighted regression: Simple\nand scalable off-policy reinforcement learning”. In: arXiv [cs.LG] (Sept. 2019). url: http:\n//arxiv.org/abs/1910.00177.\n[Pic+19]\nA. Piche, V. Thomas, C. Ibrahim, Y. Bengio, and C. Pal. “Probabilistic Planning with Sequential\nMonte Carlo methods”. In: ICLR. 2019. url: https://openreview.net/pdf?id=ByetGn0cYX.\n134\n[Pis+22]\nM. Pislar, D. Szepesvari, G. Ostrovski, D. L. Borsa, and T. Schaul. “When should agents\nexplore?” In: ICLR. 2022. url: https://openreview.net/pdf?id=dEwfxt14bca.\n[PKP21]\nA. Plaat, W. Kosters, and M. Preuss. “High-Accuracy Model-Based Reinforcement Learning, a\nSurvey”. In: (2021). arXiv: 2107.08241 [cs.LG]. url: http://arxiv.org/abs/2107.08241.\n[Pla22]\nA. Plaat. Deep reinforcement learning, a textbook. Berlin, Germany: Springer, Jan. 2022. url:\nhttps://link.springer.com/10.1007/978-981-19-0638-1.\n[PMB22]\nK. Paster, S. McIlraith, and J. Ba. “You can’t count on luck: Why decision transformers and\nRvS fail in stochastic environments”. In: arXiv [cs.LG] (May 2022). url: http://arxiv.org/\nabs/2205.15967.\n[Pom89]\nD. Pomerleau. “ALVINN: An Autonomous Land Vehicle in a Neural Network”. In: NIPS. 1989,\npp. 305–313.\n[Pow22]\nW. B. Powell. Reinforcement Learning and Stochastic Optimization: A Unified Framework\nfor Sequential Decisions. en. 1st ed. Wiley, Mar. 2022. url: https://www.amazon.com/\nReinforcement-Learning-Stochastic-Optimization-Sequential/dp/1119815037.\n[PR12]\nW. B. Powell and I. O. Ryzhov. Optimal Learning. Wiley Series in Probability and Statistics.\nhttp://optimallearning.princeton.edu/. Hoboken, NJ: Wiley-Blackwell, Mar. 2012. url: https:\n//castle.princeton.edu/wp-content/uploads/2019/02/Powell-OptimalLearningWileyMarch112018.\npdf.\n[PS07]\nJ. Peters and S. Schaal. “Reinforcement Learning by Reward-Weighted Regression for Operational\nSpace Control”. In: ICML. 2007, pp. 745–750.\n[PSS00]\nD. Precup, R. S. Sutton, and S. P. Singh. “Eligibility Traces for Off-Policy Policy Evaluation”.\nIn: ICML. ICML ’00. Morgan Kaufmann Publishers Inc., 2000, pp. 759–766. url: http:\n//dl.acm.org/citation.cfm?id=645529.658134.\n[PT87]\nC. Papadimitriou and J. Tsitsiklis. “The complexity of Markov decision processes”. In: Mathe-\nmatics of Operations Research 12.3 (1987), pp. 441–450.\n[Put94]\nM. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley,\n1994.\n[PW94]\nJ. Peng and R. J. Williams. “Incremental Multi-Step Q-Learning”. In: Machine Learning\nProceedings. Elsevier, Jan. 1994, pp. 226–232. url: http://dx.doi.org/10.1016/B978-1-\n55860-335-6.50035-0.\n[QPC21]\nJ. Queeney, I. C. Paschalidis, and C. G. Cassandras. “Generalized Proximal Policy Optimization\nwith Sample Reuse”. In: NIPS. Oct. 2021.\n[QPC24]\nJ. Queeney, I. C. Paschalidis, and C. G. Cassandras. “Generalized Policy Improvement algorithms\nwith theoretically supported sample reuse”. In: IEEE Trans. Automat. Contr. (2024). url:\nhttp://arxiv.org/abs/2206.13714.\n[Rab89]\nL. R. Rabiner. “A Tutorial on Hidden Markov Models and Selected Applications in Speech\nRecognition”. In: Proc. of the IEEE 77.2 (1989), pp. 257–286.\n[Raf+23]\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. “Direct Preference\nOptimization: Your language model is secretly a reward model”. In: arXiv [cs.LG] (May 2023).\nurl: http://arxiv.org/abs/2305.18290.\n[Raf+24]\nR. Rafailov et al. “D5RL: Diverse datasets for data-driven deep reinforcement learning”. In:\nRLC. Aug. 2024. url: https://arxiv.org/abs/2408.08441.\n[Raj+17]\nA. Rajeswaran, K. Lowrey, E. Todorov, and S. Kakade. “Towards generalization and simplicity\nin continuous control”. In: NIPS. Mar. 2017.\n[Rao10]\nA. V. Rao. “A Survey of Numerical Methods for Optimal Control”. In: Adv. Astronaut. Sci.\n135.1 (2010). url: http://dx.doi.org/.\n135\n[RB12]\nS. Ross and J. A. Bagnell. “Agnostic system identification for model-based reinforcement\nlearning”. In: ICML. Mar. 2012.\n[RB99]\nR. P. Rao and D. H. Ballard. “Predictive coding in the visual cortex: a functional interpretation\nof some extra-classical receptive-field effects”. en. In: Nat. Neurosci. 2.1 (1999), pp. 79–87. url:\nhttp://dx.doi.org/10.1038/4580.\n[Rec19]\nB. Recht. “A Tour of Reinforcement Learning: The View from Continuous Control”. In: Annual\nReview of Control, Robotics, and Autonomous Systems 2 (2019), pp. 253–279. url: http:\n//arxiv.org/abs/1806.09460.\n[Ree+22]\nS. Reed et al. “A Generalist Agent”. In: TMLR (May 2022). url: https://arxiv.org/abs/\n2205.06175.\n[Ren+24]\nA. Z. Ren, J. Lidard, L. L. Ankile, A. Simeonov, P. Agrawal, A. Majumdar, B. Burchfiel, H. Dai,\nand M. Simchowitz. “Diffusion Policy Policy Optimization”. In: arXiv [cs.RO] (Aug. 2024). url:\nhttp://arxiv.org/abs/2409.00588.\n[RFP15]\nI. O. Ryzhov, P. I. Frazier, and W. B. Powell. “A new optimal stepsize for approximate dynamic\nprogramming”. en. In: IEEE Trans. Automat. Contr. 60.3 (Mar. 2015), pp. 743–758. url:\nhttps://castle.princeton.edu/Papers/Ryzhov-OptimalStepsizeforADPFeb242015.pdf.\n[RGB11]\nS. Ross, G. J. Gordon, and J. A. Bagnell. “A reduction of imitation learning and structured\nprediction to no-regret online learning”. In: AISTATS. 2011.\n[Rie05]\nM. Riedmiller. “Neural fitted Q iteration – first experiences with a data efficient neural reinforce-\nment learning method”. en. In: ECML. Lecture notes in computer science. 2005, pp. 317–328.\nurl: https://link.springer.com/chapter/10.1007/11564096_32.\n[Rie+18]\nM. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess,\nand J. T. Springenberg. “Learning by Playing Solving Sparse Reward Tasks from Scratch”. en.\nIn: ICML. PMLR, July 2018, pp. 4344–4353. url: https://proceedings.mlr.press/v80/\nriedmiller18a.html.\n[RJ22]\nA. Rao and T. Jelvis. Foundations of Reinforcement Learning with Applications in Finance.\nChapman and Hall/ CRC, 2022. url: https://github.com/TikhonJelvis/RL-book.\n[RK04]\nR. Rubinstein and D. Kroese. The Cross-Entropy Method: A Unified Approach to Combinatorial\nOptimization, Monte-Carlo Simulation, and Machine Learning. Springer-Verlag, 2004.\n[RLT18]\nM. Riemer, M. Liu, and G. Tesauro. “Learning Abstract Options”. In: NIPS 31 (2018). url:\nhttps://proceedings.neurips.cc/paper_files/paper/2018/file/cdf28f8b7d14ab02d12a2329d71e4079-\nPaper.pdf.\n[RMD22]\nJ. B. Rawlings, D. Q. Mayne, and M. M. Diehl. Model Predictive Control: Theory, Computa-\ntion, and Design (2nd ed). en. Nob Hill Publishing, LLC, Sept. 2022. url: https://sites.\nengineering.ucsb.edu/~jbraw/mpc/MPC-book-2nd-edition-1st-printing.pdf.\n[RMK20]\nA. Rajeswaran, I. Mordatch, and V. Kumar. “A game theoretic framework for model based\nreinforcement learning”. In: ICML. 2020.\n[RN19]\nS. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. 4th edition. Prentice Hall,\n2019.\n[RN94]\nG. A. Rummery and M Niranjan. On-Line Q-Learning Using Connectionist Systems. Tech. rep.\nCambridge Univ. Engineering Dept., 1994. url: http://dx.doi.org/.\n[RR14]\nD. Russo and B. V. Roy. “Learning to Optimize via Posterior Sampling”. In: Math. Oper. Res.\n39.4 (2014), pp. 1221–1243.\n[RTV12]\nK. Rawlik, M. Toussaint, and S. Vijayakumar. “On stochastic optimal control and reinforcement\nlearning by approximate inference”. In: Robotics: Science and Systems VIII. Robotics: Science and\nSystems Foundation, 2012. url: https://blogs.cuit.columbia.edu/zp2130/files/2019/\n03/On_Stochasitc_Optimal_Control_and_Reinforcement_Learning_by_Approximate_\nInference.pdf.\n136\n[Rub97]\nR. Y. Rubinstein. “Optimization of computer simulation models with rare events”. In: Eur. J.\nOper. Res. 99.1 (1997), pp. 89–112. url: http://www.sciencedirect.com/science/article/\npii/S0377221796003852.\n[Rus+18]\nD. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen. “A Tutorial on Thompson\nSampling”. In: Foundations and Trends in Machine Learning 11.1 (2018), pp. 1–96. url:\nhttp://dx.doi.org/10.1561/2200000070.\n[Rus19]\nS. Russell. Human Compatible: Artificial Intelligence and the Problem of Control. en. Kin-\ndle. Viking, 2019. url: https : / / www . amazon . com / Human - Compatible - Artificial -\nIntelligence- Problem- ebook/dp/B07N5J5FTS/ref=zg_bs_3887_4?_encoding=UTF8&\npsc=1&refRID=0JE0ST011W4K15PTFZAT.\n[RW91]\nS. Russell and E. Wefald. “Principles of metareasoning”. en. In: Artif. Intell. 49.1-3 (May 1991),\npp. 361–395. url: http://dx.doi.org/10.1016/0004-3702(91)90015-C.\n[Ryu+20]\nM. Ryu, Y. Chow, R. Anderson, C. Tjandraatmadja, and C. Boutilier. “CAQL: Continuous\nAction Q-Learning”. In: ICLR. 2020. url: https://openreview.net/forum?id=BkxXe0Etwr.\n[Saj+21]\nN. Sajid, P. J. Ball, T. Parr, and K. J. Friston. “Active Inference: Demystified and Compared”.\nen. In: Neural Comput. 33.3 (Mar. 2021), pp. 674–712. url: https://web.archive.org/web/\n20210628163715id_/https://discovery.ucl.ac.uk/id/eprint/10119277/1/Friston_\nneco_a_01357.pdf.\n[Sal+23]\nT. Salvatori, A. Mali, C. L. Buckley, T. Lukasiewicz, R. P. N. Rao, K. Friston, and A. Ororbia.\n“Brain-inspired computational intelligence via predictive coding”. In: arXiv [cs.AI] (Aug. 2023).\nurl: http://arxiv.org/abs/2308.07870.\n[Sal+24]\nT. Salvatori, Y. Song, Y. Yordanov, B. Millidge, L. Sha, C. Emde, Z. Xu, R. Bogacz, and\nT. Lukasiewicz. “A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding\nNetworks”. In: ICLR. Oct. 2024. url: https://openreview.net/pdf?id=RyUvzda8GH.\n[SB18]\nR. Sutton and A. Barto. Reinforcement learning: an introduction (2nd edn). MIT Press, 2018.\n[Sch10]\nJ. Schmidhuber. “Formal Theory of Creativity, Fun, and Intrinsic Motivation”. In: IEEE Trans.\nAutonomous Mental Development 2 (2010). url: http://people.idsia.ch/~juergen/\nieeecreative.pdf.\n[Sch+15a]\nT. Schaul, D. Horgan, K. Gregor, and D. Silver. “Universal Value Function Approximators”. en.\nIn: ICML. PMLR, June 2015, pp. 1312–1320. url: https://proceedings.mlr.press/v37/\nschaul15.html.\n[Sch+15b]\nJ. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. “Trust Region Policy Optimiza-\ntion”. In: ICML. 2015. url: http://arxiv.org/abs/1502.05477.\n[Sch+16a]\nT. Schaul, J. Quan, I. Antonoglou, and D. Silver. “Prioritized Experience Replay”. In: ICLR.\n2016. url: http://arxiv.org/abs/1511.05952.\n[Sch+16b]\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. “High-Dimensional Continuous\nControl Using Generalized Advantage Estimation”. In: ICLR. 2016. url: http://arxiv.org/\nabs/1506.02438.\n[Sch+17]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. “Proximal Policy Optimization\nAlgorithms”. In: (2017). arXiv: 1707.06347 [cs.LG]. url: http://arxiv.org/abs/1707.\n06347.\n[Sch19]\nJ. Schmidhuber. “Reinforcement learning Upside Down: Don’t predict rewards – just map them\nto actions”. In: arXiv [cs.AI] (Dec. 2019). url: http://arxiv.org/abs/1912.02875.\n[Sch+20]\nJ. Schrittwieser et al. “Mastering Atari, Go, Chess and Shogi by Planning with a Learned\nModel”. In: Nature (2020). url: http://arxiv.org/abs/1911.08265.\n137\n[Sch+21]\nM. Schwarzer, A. Anand, R. Goel, R Devon Hjelm, A. Courville, and P. Bachman. “Data-\nEfficient Reinforcement Learning with Self-Predictive Representations”. In: ICLR. 2021. url:\nhttps://openreview.net/pdf?id=uCQfPZwRaUu.\n[Sch+23a]\nI. Schubert, J. Zhang, J. Bruce, S. Bechtle, E. Parisotto, M. Riedmiller, J. T. Springenberg,\nA. Byravan, L. Hasenclever, and N. Heess. “A Generalist Dynamics Model for Control”. In:\narXiv [cs.AI] (May 2023). url: http://arxiv.org/abs/2305.10912.\n[Sch+23b]\nM. Schwarzer, J. Obando-Ceron, A. Courville, M. Bellemare, R. Agarwal, and P. S. Castro.\n“Bigger, Better, Faster: Human-level Atari with human-level efficiency”. In: ICML. May 2023.\nurl: http://arxiv.org/abs/2305.19452.\n[Sco10]\nS. Scott. “A modern Bayesian look at the multi-armed bandit”. In: Applied Stochastic Models in\nBusiness and Industry 26 (2010), pp. 639–658.\n[Sei+16]\nH. van Seijen, A Rupam Mahmood, P. M. Pilarski, M. C. Machado, and R. S. Sutton. “True\nOnline Temporal-Difference Learning”. In: JMLR (2016). url: http://jmlr.org/papers/\nvolume17/15-599/15-599.pdf.\n[Sey+22]\nT. Seyde, P. Werner, W. Schwarting, I. Gilitschenski, M. Riedmiller, D. Rus, and M. Wulfmeier.\n“Solving Continuous Control via Q-learning”. In: ICLR. Sept. 2022. url: https://openreview.\nnet/pdf?id=U5XOGxAgccS.\n[Sha+20]\nR. Shah, P. Freire, N. Alex, R. Freedman, D. Krasheninnikov, L. Chan, M. D. Dennis, P.\nAbbeel, A. Dragan, and S. Russell. “Benefits of Assistance over Reward Learning”. In: NIPS\nWorkshop. 2020. url: https://aima.cs.berkeley.edu/~russell/papers/neurips20ws-\nassistance.pdf.\n[Sie+20]\nN. Siegel, J. T. Springenberg, F. Berkenkamp, A. Abdolmaleki, M. Neunert, T. Lampe, R.\nHafner, N. Heess, and M. Riedmiller. “Keep Doing What Worked: Behavior Modelling Priors\nfor Offline Reinforcement Learning”. In: ICLR. 2020. url: https://openreview.net/pdf?id=\nrke7geHtwH.\n[Sil+14]\nD. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. “Deterministic\nPolicy Gradient Algorithms”. In: ICML. ICML’14. JMLR.org, 2014, pp. I–387–I–395. url:\nhttp://dl.acm.org/citation.cfm?id=3044805.3044850.\n[Sil+16]\nD. Silver et al. “Mastering the game of Go with deep neural networks and tree search”. en. In:\nNature 529.7587 (2016), pp. 484–489. url: http://dx.doi.org/10.1038/nature16961.\n[Sil+17a]\nD. Silver et al. “Mastering the game of Go without human knowledge”. en. In: Nature 550.7676\n(2017), pp. 354–359. url: http://dx.doi.org/10.1038/nature24270.\n[Sil+17b]\nD. Silver et al. “The predictron: end-to-end learning and planning”. In: ICML. 2017. url:\nhttps://openreview.net/pdf?id=BkJsCIcgl.\n[Sil18]\nD. Silver. Lecture 9L Exploration and Exploitation. 2018. url: http://www0.cs.ucl.ac.uk/\nstaff/d.silver/web/Teaching_files/XX.pdf.\n[Sil+18]\nD. Silver et al. “A general reinforcement learning algorithm that masters chess, shogi, and Go\nthrough self-play”. en. In: Science 362.6419 (2018), pp. 1140–1144. url: http://dx.doi.org/\n10.1126/science.aar6404.\n[Sin+00]\nS. Singh, T. Jaakkola, M. L. Littman, and C. Szepesvári. “Convergence Results for Single-\nStep On-PolicyReinforcement-Learning Algorithms”. In: MLJ 38.3 (2000), pp. 287–308. url:\nhttps://doi.org/10.1023/A:1007678930559.\n[Ska+22]\nJ. Skalse, N. H. R. Howe, D. Krasheninnikov, and D. Krueger. “Defining and characterizing\nreward hacking”. In: NIPS. Sept. 2022.\n[SKM18]\nS. Schwöbel, S. Kiebel, and D. Marković. “Active Inference, Belief Propagation, and the Bethe\nApproximation”. en. In: Neural Comput. 30.9 (2018), pp. 2530–2567. url: http://dx.doi.org/\n10.1162/neco_a_01108.\n138\n[Sli19]\nA. Slivkins. “Introduction to Multi-Armed Bandits”. In: Foundations and Trends in Machine\nLearning (2019). url: http://arxiv.org/abs/1904.07272.\n[Smi+23]\nF. B. Smith, A. Kirsch, S. Farquhar, Y. Gal, A. Foster, and T. Rainforth. “Prediction-Oriented\nBayesian Active Learning”. In: AISTATS. Apr. 2023. url: http://arxiv.org/abs/2304.08151.\n[Sol64]\nR. J. Solomonoff. “A formal theory of inductive inference. Part I”. In: Information and Control\n7.1 (Mar. 1964), pp. 1–22. url: https://www.sciencedirect.com/science/article/pii/\nS0019995864902232.\n[Son98]\nE. D. Sontag. Mathematical Control Theory: Deterministic Finite Dimensional Systems. 2nd.\nVol. 6. Texts in Applied Mathematics. Springer, 1998.\n[Spi+24]\nB. A. Spiegel, Z. Yang, W. Jurayj, B. Bachmann, S. Tellex, and G. Konidaris. “Informing\nReinforcement Learning Agents by Grounding Language to Markov Decision Processes”. In:\nWorkshop on Training Agents with Foundation Models at RLC 2024. Aug. 2024. url: https:\n//openreview.net/pdf?id=uFm9e4Ly26.\n[Spr17]\nM. W. Spratling. “A review of predictive coding algorithms”. en. In: Brain Cogn. 112 (2017),\npp. 92–97. url: http://dx.doi.org/10.1016/j.bandc.2015.11.003.\n[SPS99]\nR. S. Sutton, D. Precup, and S. Singh. “Between MDPs and semi-MDPs: A framework for\ntemporal abstraction in reinforcement learning”. In: Artif. Intell. 112.1 (Aug. 1999), pp. 181–211.\nurl: http://www.sciencedirect.com/science/article/pii/S0004370299000521.\n[SS21]\nD. Schmidt and T. Schmied. “Fast and Data-Efficient Training of Rainbow: an Experimental\nStudy on Atari”. In: Deep RL Workshop NeurIPS 2021. Dec. 2021. url: https://openreview.\nnet/pdf?id=GvM7A3cv63M.\n[SSM08]\nR. S. Sutton, C. Szepesvári, and H. R. Maei. “A convergent O(n) algorithm for off-policy temporal-\ndifference learning with linear function approximation”. en. In: NIPS. NIPS’08. Red Hook, NY,\nUSA: Curran Associates Inc., Dec. 2008, pp. 1609–1616. url: https://proceedings.neurips.\ncc/paper_files/paper/2008/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf.\n[Str00]\nM. Strens. “A Bayesian Framework for Reinforcement Learning”. In: ICML. 2000.\n[Sub+22]\nJ. Subramanian, A. Sinha, R. Seraj, and A. Mahajan. “Approximate information state for\napproximate planning and reinforcement learning in partially observed systems”. In: JMLR\n23.12 (2022), pp. 1–83. url: http://jmlr.org/papers/v23/20-1165.html.\n[Sut+08]\nR. S. Sutton, C. Szepesvari, A. Geramifard, and M. P. Bowling. “Dyna-style planning with\nlinear function approximation and prioritized sweeping”. In: UAI. 2008.\n[Sut15]\nR. Sutton. Introduction to RL with function approximation. NIPS Tutorial. 2015. url: http:\n/ / media . nips . cc / Conferences / 2015 / tutorialslides / SuttonIntroRL - nips - 2015 -\ntutorial.pdf.\n[Sut88]\nR. Sutton. “Learning to predict by the methods of temporal differences”. In: Machine Learning\n3.1 (1988), pp. 9–44.\n[Sut90]\nR. S. Sutton. “Integrated Architectures for Learning, Planning, and Reacting Based on Ap-\nproximating Dynamic Programming”. In: ICML. Ed. by B. Porter and R. Mooney. Morgan\nKaufmann, 1990, pp. 216–224. url: http://www.sciencedirect.com/science/article/\npii/B9781558601413500304.\n[Sut95]\nR. S. Sutton. “TD models: Modeling the world at a mixture of time scales”. en. In: ICML.\nJan. 1995, pp. 531–539. url: https://www.sciencedirect.com/science/article/abs/pii/\nB9781558603776500724.\n[Sut96]\nR. S. Sutton. “Generalization in Reinforcement Learning: Successful Examples Using Sparse\nCoarse Coding”. In: NIPS. Ed. by D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo. MIT\nPress, 1996, pp. 1038–1044. url: http://papers.nips.cc/paper/1109-generalization-in-\nreinforcement-learning-successful-examples-using-sparse-coarse-coding.pdf.\n139\n[Sut+99]\nR. Sutton, D. McAllester, S. Singh, and Y. Mansour. “Policy Gradient Methods for Reinforcement\nLearning with Function Approximation”. In: NIPS. 1999.\n[SW06]\nJ. E. Smith and R. L. Winkler. “The Optimizer’s Curse: Skepticism and Postdecision Surprise\nin Decision Analysis”. In: Manage. Sci. 52.3 (2006), pp. 311–322.\n[Sze10]\nC. Szepesvari. Algorithms for Reinforcement Learning. Morgan Claypool, 2010.\n[Tam+16]\nA. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel. “Value Iteration Networks”. In: NIPS.\n2016. url: http://arxiv.org/abs/1602.02867.\n[Tan+23]\nY. Tang et al. “Understanding Self-Predictive Learning for Reinforcement Learning”. In: ICML.\n2023. url: https://proceedings.mlr.press/v202/tang23d/tang23d.pdf.\n[Ten02]\nR. B. A. Tennenholtz. “R-max – A General Polynomial Time Algorithm for Near-Optimal\nReinforcement Learning”. In: JMLR 3 (2002), pp. 213–231. url: http://www.ai.mit.edu/\nprojects/jmlr/papers/volume3/brafman02a/source/brafman02a.pdf.\n[Tha+22]\nS. Thakoor, M. Rowland, D. Borsa, W. Dabney, R. Munos, and A. Barreto. “Generalised\nPolicy Improvement with Geometric Policy Composition”. en. In: ICML. PMLR, June 2022,\npp. 21272–21307. url: https://proceedings.mlr.press/v162/thakoor22a.html.\n[Tho33]\nW. R. Thompson. “On the Likelihood that One Unknown Probability Exceeds Another in View\nof the Evidence of Two Samples”. In: Biometrika 25.3/4 (1933), pp. 285–294.\n[TKE24]\nH. Tang, D. Key, and K. Ellis. “WorldCoder, a model-based LLM agent: Building world models\nby writing code and interacting with the environment”. In: arXiv [cs.AI] (Feb. 2024). url:\nhttp://arxiv.org/abs/2402.12275.\n[TL05]\nE. Todorov and W. Li. “A Generalized Iterative LQG Method for Locally-optimal Feedback\nControl of Constrained Nonlinear Stochastic Systems”. In: ACC. 2005, pp. 300–306.\n[TMM19]\nC. Tessler, D. J. Mankowitz, and S. Mannor. “Reward Constrained Policy Optimization”. In:\nICLR. 2019. url: https://openreview.net/pdf?id=SkfrvsA9FX.\n[Tom+22]\nT. Tomilin, T. Dai, M. Fang, and M. Pechenizkiy. “LevDoom: A benchmark for generalization\non level difficulty in reinforcement learning”. In: 2022 IEEE Conference on Games (CoG). IEEE,\nAug. 2022. url: https://ieee-cog.org/2022/assets/papers/paper_30.pdf.\n[Tom+24]\nM. Tomar, P. Hansen-Estruch, P. Bachman, A. Lamb, J. Langford, M. E. Taylor, and S. Levine.\n“Video Occupancy Models”. In: arXiv [cs.CV] (June 2024). url: http://arxiv.org/abs/2407.\n09533.\n[Tou09]\nM. Toussaint. “Robot Rrajectory Optimization using Approximate Inference”. In: ICML. 2009,\npp. 1049–1056.\n[Tou14]\nM. Toussaint. Bandits, Global Optimization, Active Learning, and Bayesian RL – understanding\nthe common ground. Autonomous Learning Summer School. 2014. url: https://www.user.\ntu-berlin.de/mtoussai/teaching/14-BanditsOptimizationActiveLearningBayesianRL.\npdf.\n[TR97]\nJ. Tsitsiklis and B. V. Roy. “An analysis of temporal-difference learning with function approxi-\nmation”. In: IEEE Trans. on Automatic Control 42.5 (1997), pp. 674–690.\n[TS06]\nM. Toussaint and A. Storkey. “Probabilistic inference for solving discrete and continuous state\nMarkov Decision Processes”. In: ICML. 2006, pp. 945–952.\n[Tsc+20]\nA. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley. “Reinforcement learning through active\ninference”. In: ICLR workshop on “Bridging AI and Cognitive Science“. Feb. 2020.\n[Tsc+23]\nA. Tscshantz, B. Millidge, A. K. Seth, and C. L. Buckley. “Hybrid predictive coding: Inferring,\nfast and slow”. en. In: PLoS Comput. Biol. 19.8 (Aug. 2023), e1011280. url: https : / /\njournals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1011280&\ntype=printable.\n140\n[Tsi+17]\nP. A. Tsividis, T. Pouncy, J. L. Xu, J. B. Tenenbaum, and S. J. Gershman. “Human Learning\nin Atari”. en. In: AAAI Spring Symposium Series. 2017. url: https://www.aaai.org/ocs/\nindex.php/SSS/SSS17/paper/viewPaper/15280.\n[TVR97]\nJ. N. Tsitsiklis and B Van Roy. “An analysis of temporal-difference learning with function\napproximation”. en. In: IEEE Trans. Automat. Contr. 42.5 (May 1997), pp. 674–690. url:\nhttps://ieeexplore.ieee.org/abstract/document/580874.\n[Unk24]\nUnknown. “Beyond The Rainbow: High Performance Deep Reinforcement Learning On A\nDesktop PC”. In: (Oct. 2024). url: https://openreview.net/pdf?id=0ydseYDKRi.\n[Val00]\nH. Valpola. “Bayesian Ensemble Learning for Nonlinear Factor Analysis”. PhD thesis. Helsinki\nUniversity of Technology, 2000. url: https://users.ics.aalto.fi/harri/thesis/valpola_\nthesis.ps.gz.\n[van+18]\nH. van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, and J. Modayil. Deep Reinforcement\nLearning and the Deadly Triad. arXiv:1812.02648. 2018.\n[VBW15]\nS. S. Villar, J. Bowden, and J. Wason. “Multi-armed Bandit Models for the Optimal Design\nof Clinical Trials: Benefits and Challenges”. en. In: Stat. Sci. 30.2 (2015), pp. 199–215. url:\nhttp://dx.doi.org/10.1214/14-STS504.\n[Vee+19]\nV. Veeriah, M. Hessel, Z. Xu, J. Rajendran, R. L. Lewis, J. Oh, H. P. van Hasselt, D. Silver, and S.\nSingh. “Discovery of Useful Questions as Auxiliary Tasks”. In: NIPS. Vol. 32. 2019. url: https://\nproceedings.neurips.cc/paper_files/paper/2019/file/10ff0b5e85e5b85cc3095d431d8c08b4-\nPaper.pdf.\n[Ven+24]\nD. Venuto, S. N. Islam, M. Klissarov, D. Precup, S. Yang, and A. Anand. “Code as re-\nward: Empowering reinforcement learning with VLMs”. In: ICML. Feb. 2024. url: https:\n//openreview.net/forum?id=6P88DMUDvH.\n[Vez+17]\nA. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu.\n“FeUdal Networks for Hierarchical Reinforcement Learning”. en. In: ICML. PMLR, July 2017,\npp. 3540–3549. url: https://proceedings.mlr.press/v70/vezhnevets17a.html.\n[Vil+22]\nA. R. Villaflor, Z. Huang, S. Pande, J. M. Dolan, and J. Schneider. “Addressing Optimism\nBias in Sequence Modeling for Reinforcement Learning”. en. In: ICML. PMLR, June 2022,\npp. 22270–22283. url: https://proceedings.mlr.press/v162/villaflor22a.html.\n[VPG20]\nN. Vieillard, O. Pietquin, and M. Geist. “Munchausen Reinforcement Learning”. In: NIPS.\nVol. 33. 2020, pp. 4235–4246. url: https://proceedings.neurips.cc/paper_files/paper/\n2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf.\n[Wag+19]\nN. Wagener, C.-A. Cheng, J. Sacks, and B. Boots. “An online learning approach to model\npredictive control”. In: Robotics: Science and Systems. Feb. 2019. url: https://arxiv.org/\nabs/1902.08967.\n[Wan+16]\nZ. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and N. de Freitas. “Dueling Network\nArchitectures for Deep Reinforcement Learning”. In: ICML. 2016. url: http://proceedings.\nmlr.press/v48/wangf16.pdf.\n[Wan+19]\nT. Wang, X. Bao, I. Clavera, J. Hoang, Y. Wen, E. Langlois, S. Zhang, G. Zhang, P. Abbeel,\nand J. Ba. “Benchmarking Model-Based Reinforcement Learning”. In: arXiv [cs.LG] (July 2019).\nurl: http://arxiv.org/abs/1907.02057.\n[Wan+22]\nT. Wang, S. S. Du, A. Torralba, P. Isola, A. Zhang, and Y. Tian. “Denoised MDPs: Learning\nWorld Models Better Than the World Itself”. In: ICML. June 2022. url: http://arxiv.org/\nabs/2206.15477.\n[Wan+24a]\nG. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar.\n“Voyager: An Open-Ended Embodied Agent with Large Language Models”. In: TMLR (2024).\nurl: https://openreview.net/forum?id=ehfRiF0R3a.\n141\n[Wan+24b]\nS. Wang, S. Liu, W. Ye, J. You, and Y. Gao. “EfficientZero V2: Mastering discrete and continuous\ncontrol with limited data”. In: arXiv [cs.LG] (Mar. 2024). url: http://arxiv.org/abs/2403.\n00564.\n[WAT17]\nG. Williams, A. Aldrich, and E. A. Theodorou. “Model Predictive Path Integral Control: From\nTheory to Parallel Computation”. In: J. Guid. Control Dyn. 40.2 (Feb. 2017), pp. 344–357. url:\nhttps://doi.org/10.2514/1.G001921.\n[Wat+21]\nJ. Watson, H. Abdulsamad, R. Findeisen, and J. Peters. “Stochastic Control through Approxi-\nmate Bayesian Input Inference”. In: arxiv (2021). url: http://arxiv.org/abs/2105.07693.\n[WCM24]\nC. Wang, Y. Chen, and K. Murphy. “Model-based Policy Optimization under Approximate\nBayesian Inference”. en. In: AISTATS. PMLR, Apr. 2024, pp. 3250–3258. url: https://\nproceedings.mlr.press/v238/wang24g.html.\n[WD92]\nC. Watkins and P. Dayan. “Q-learning”. In: Machine Learning 8.3 (1992), pp. 279–292.\n[Wei+24]\nR. Wei, N. Lambert, A. McDonald, A. Garcia, and R. Calandra. “A unified view on solving\nobjective mismatch in model-based Reinforcement Learning”. In: Trans. on Machine Learning\nResearch (2024). url: https://openreview.net/forum?id=tQVZgvXhZb.\n[Wen18a]\nL. Weng. “A (Long) Peek into Reinforcement Learning”. In: lilianweng.github.io (2018). url:\nhttps://lilianweng.github.io/posts/2018-02-19-rl-overview/.\n[Wen18b]\nL. Weng. “Policy Gradient Algorithms”. In: lilianweng.github.io (2018). url: https://lilianweng.\ngithub.io/posts/2018-04-08-policy-gradient/.\n[WHT19]\nY. Wang, H. He, and X. Tan. “Truly Proximal Policy Optimization”. In: UAI. 2019. url:\nhttp://auai.org/uai2019/proceedings/papers/21.pdf.\n[WHZ23]\nZ. Wang, J. J. Hunt, and M. Zhou. “Diffusion Policies as an Expressive Policy Class for Offline\nReinforcement Learning”. In: ICLR. 2023. url: https://openreview.net/pdf?id=AHvFDPi-\nFA.\n[Wie03]\nE Wiewiora. “Potential-Based Shaping and Q-Value Initialization are Equivalent”. In: JAIR.\n2003. url: https://jair.org/index.php/jair/article/view/10338.\n[Wil+17]\nG. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou.\n“Information theoretic MPC for model-based reinforcement learning”. In: ICRA. IEEE, May\n2017, pp. 1714–1721. url: https://ieeexplore.ieee.org/document/7989202.\n[Wil92]\nR. J. Williams. “Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning”. In: MLJ 8.3-4 (1992), pp. 229–256.\n[WIP20]\nJ. Watson, A. Imohiosen, and J. Peters. “Active Inference or Control as Inference? A Unifying\nView”. In: International Workshop on Active Inference. 2020. url: http://arxiv.org/abs/\n2010.00262.\n[WL14]\nN. Whiteley and A. Lee. “Twisted particle filters”. en. In: Annals of Statistics 42.1 (Feb. 2014),\npp. 115–141. url: https://projecteuclid.org/journals/annals-of-statistics/volume-\n42/issue-1/Twisted-particle-filters/10.1214/13-AOS1167.full.\n[Wu+17]\nY. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. “Scalable trust-region method for deep\nreinforcement learning using Kronecker-factored approximation”. In: NIPS. 2017. url: https:\n//arxiv.org/abs/1708.05144.\n[Wu+21]\nY. Wu, S. Zhai, N. Srivastava, J. Susskind, J. Zhang, R. Salakhutdinov, and H. Goh. “Uncertainty\nWeighted Actor-critic for offline Reinforcement Learning”. In: ICML. May 2021. url: https:\n//arxiv.org/abs/2105.08140.\n[Wu+22]\nP. Wu, A. Escontrela, D. Hafner, K. Goldberg, and P. Abbeel. “DayDreamer: World Models\nfor Physical Robot Learning”. In: (June 2022). arXiv: 2206 . 14176 [cs.RO]. url: http :\n//arxiv.org/abs/2206.14176.\n142\n[Wu+23]\nG. Wu, W. Fang, J. Wang, P. Ge, J. Cao, Y. Ping, and P. Gou. “Dyna-PPO reinforcement\nlearning with Gaussian process for the continuous action decision-making in autonomous driving”.\nen. In: Appl. Intell. 53.13 (July 2023), pp. 16893–16907. url: https://link.springer.com/\narticle/10.1007/s10489-022-04354-x.\n[Wur+22]\nP. R. Wurman et al. “Outracing champion Gran Turismo drivers with deep reinforcement\nlearning”. en. In: Nature 602.7896 (Feb. 2022), pp. 223–228. url: https://www.researchgate.\nnet/publication/358484368_Outracing_champion_Gran_Turismo_drivers_with_deep_\nreinforcement_learning.\n[Xu+17]\nC. Xu, T. Qin, G. Wang, and T.-Y. Liu. “Reinforcement learning for learning rate control”. In:\narXiv [cs.LG] (May 2017). url: http://arxiv.org/abs/1705.11159.\n[Yan+23]\nM. Yang, D Schuurmans, P Abbeel, and O. Nachum. “Dichotomy of control: Separating what\nyou can control from what you cannot”. In: ICLR. Vol. abs/2210.13435. 2023. url: https:\n//github.com/google-research/google-research/tree/.\n[Yan+24]\nS. Yang, Y. Du, S. K. S. Ghasemipour, J. Tompson, L. P. Kaelbling, D. Schuurmans, and\nP. Abbeel. “Learning Interactive Real-World Simulators”. In: ICLR. 2024. url: https://\nopenreview.net/pdf?id=sFyTZEqmUY.\n[Yao+22]\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. “ReAct: Synergizing\nReasoning and Acting in Language Models”. In: ICLR. Sept. 2022. url: https://openreview.\nnet/pdf?id=WE_vluYUL-X.\n[Ye+21]\nW. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao. “Mastering Atari games with limited data”.\nIn: NIPS. Oct. 2021.\n[Yu17]\nH. Yu. “On convergence of some gradient-based temporal-differences algorithms for off-policy\nlearning”. In: arXiv [cs.LG] (Dec. 2017). url: http://arxiv.org/abs/1712.09652.\n[Yu+20]\nT. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn, and T. Ma. “MOPO: Model-\nbased Offline Policy Optimization”. In: NIPS. Vol. 33. 2020, pp. 14129–14142. url: https://\nproceedings.neurips.cc/paper_files/paper/2020/hash/a322852ce0df73e204b7e67cbbef0d0a-\nAbstract.html.\n[Yu+23]\nC. Yu, N Burgess, M Sahani, and S Gershman. “Successor-Predecessor Intrinsic Exploration”. In:\nNIPS. Vol. abs/2305.15277. Curran Associates, Inc., May 2023, pp. 73021–73038. url: https://\nproceedings.neurips.cc/paper_files/paper/2023/hash/e6f2b968c4ee8ba260cd7077e39590dd-\nAbstract-Conference.html.\n[Yua22]\nM. Yuan. “Intrinsically-motivated reinforcement learning: A brief introduction”. In: arXiv [cs.LG]\n(Mar. 2022). url: http://arxiv.org/abs/2203.02298.\n[Yua+24]\nM. Yuan, R. C. Castanyer, B. Li, X. Jin, G. Berseth, and W. Zeng. “RLeXplore: Accelerating\nresearch in intrinsically-motivated reinforcement learning”. In: arXiv [cs.LG] (May 2024). url:\nhttp://arxiv.org/abs/2405.19548.\n[YZ22]\nY. Yang and P. Zhai. “Click-through rate prediction in online advertising: A literature review”.\nIn: Inf. Process. Manag. 59.2 (2022), p. 102853. url: https://www.sciencedirect.com/\nscience/article/pii/S0306457321003241.\n[ZABD10]\nB. D. Ziebart, J Andrew Bagnell, and A. K. Dey. “Modeling Interaction via the Principle of\nMaximum Causal Entropy”. In: ICML. 2010. url: https://www.cs.uic.edu/pub/Ziebart/\nPublications/maximum-causal-entropy.pdf.\n[Zel+24]\nE. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D. Goodman. “Quiet-STaR:\nLanguage Models Can Teach Themselves to Think Before Speaking”. In: arXiv [cs.CL] (Mar.\n2024). url: http://arxiv.org/abs/2403.09629.\n[Zha+19]\nS. Zhang, B. Liu, H. Yao, and S. Whiteson. “Provably convergent two-timescale off-policy actor-\ncritic with function approximation”. In: ICML 119 (Nov. 2019). Ed. by H. D. Iii and A. Singh,\npp. 11204–11213. url: https://proceedings.mlr.press/v119/zhang20s/zhang20s.pdf.\n143\n[Zha+21]\nA. Zhang, R. T. McAllister, R. Calandra, Y. Gal, and S. Levine. “Learning Invariant Repre-\nsentations for Reinforcement Learning without Reconstruction”. In: ICLR. 2021. url: https:\n//openreview.net/pdf?id=-2FCwDKRREu.\n[Zha+23a]\nJ. Zhang, J. T. Springenberg, A. Byravan, L. Hasenclever, A. Abdolmaleki, D. Rao, N. Heess,\nand M. Riedmiller. “Leveraging Jumpy Models for Planning and Fast Learning in Robotic\nDomains”. In: arXiv [cs.RO] (Feb. 2023). url: http://arxiv.org/abs/2302.12617.\n[Zha+23b]\nW. Zhang, G. Wang, J. Sun, Y. Yuan, and G. Huang. “STORM: Efficient Stochastic Transformer\nbased world models for reinforcement learning”. In: arXiv [cs.LG] (Oct. 2023). url: http:\n//arxiv.org/abs/2310.09615.\n[Zha+24]\nS. Zhao, R. Brekelmans, A. Makhzani, and R. B. Grosse. “Probabilistic Inference in Language\nModels via Twisted Sequential Monte Carlo”. In: ICML. June 2024. url: https://openreview.\nnet/pdf?id=frA0NNBS1n.\n[Zhe+22]\nL. Zheng, T. Fiez, Z. Alumbaugh, B. Chasnov, and L. J. Ratliff. “Stackelberg actor-critic: Game-\ntheoretic reinforcement learning algorithms”. en. In: AAAI 36.8 (June 2022), pp. 9217–9224.\nurl: https://ojs.aaai.org/index.php/AAAI/article/view/20908.\n[Zho+22]\nH. Zhou, Z. Lin, J. Li, Q. Fu, W. Yang, and D. Ye. “Revisiting discrete soft actor-critic”. In:\narXiv [cs.LG] (Sept. 2022). url: http://arxiv.org/abs/2209.10081.\n[Zho+24]\nG. Zhou, S. Swaminathan, R. V. Raju, J. S. Guntupalli, W. Lehrach, J. Ortiz, A. Dedieu,\nM. Lázaro-Gredilla, and K. Murphy. “Diffusion Model Predictive Control”. In: arXiv [cs.LG]\n(Oct. 2024). url: http://arxiv.org/abs/2410.05364.\n[ZHR24]\nH. Zhu, B. Huang, and S. Russell. “On representation complexity of model-based and model-free\nreinforcement learning”. In: ICLR. 2024.\n[Zie+08]\nB. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. “Maximum Entropy Inverse Reinforce-\nment Learning”. In: AAAI. 2008, pp. 1433–1438.\n[Zin+21]\nL Zintgraf, S. Schulze, C. Lu, L. Feng, M. Igl, K Shiarlis, Y Gal, K. Hofmann, and S. Whiteson.\n“VariBAD: Variational Bayes-Adaptive Deep RL via meta-learning”. In: J. Mach. Learn. Res.\n22.289 (2021), 289:1–289:39. url: https://www.jmlr.org/papers/volume22/21-0657/21-\n0657.pdf.\n[Zit+23]\nB. Zitkovich et al. “RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic\nControl”. en. In: Conference on Robot Learning. PMLR, Dec. 2023, pp. 2165–2183. url: https:\n//proceedings.mlr.press/v229/zitkovich23a.html.\n[ZS22]\nN. Zucchet and J. Sacramento. “Beyond backpropagation: Bilevel optimization through implicit\ndifferentiation and equilibrium propagation”. en. In: Neural Comput. 34.12 (Nov. 2022), pp. 2309–\n2346. url: https://direct.mit.edu/neco/article-pdf/34/12/2309/2057431/neco_a_\n01547.pdf.\n[ZSE24]\nC. Zheng, R. Salakhutdinov, and B. Eysenbach. “Contrastive Difference Predictive Coding”.\nIn: The Twelfth International Conference on Learning Representations. 2024. url: https:\n//openreview.net/pdf?id=0akLDTFR9x.\n[ZW19]\nS. Zhang and S. Whiteson. “DAC: The Double Actor-Critic Architecture for Learning Options”.\nIn: NIPS 32 (2019). url: https://proceedings.neurips.cc/paper_files/paper/2019/\nfile/4f284803bd0966cc24fa8683a34afc6e-Paper.pdf.\n144\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-12-06",
  "updated": "2024-12-06"
}