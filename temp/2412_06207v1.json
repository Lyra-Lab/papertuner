{
  "id": "http://arxiv.org/abs/2412.06207v1",
  "title": "Skill-Enhanced Reinforcement Learning Acceleration from Demonstrations",
  "authors": [
    "Hanping Zhang",
    "Yuhong Guo"
  ],
  "abstract": "Learning from Demonstration (LfD) aims to facilitate rapid Reinforcement\nLearning (RL) by leveraging expert demonstrations to pre-train the RL agent.\nHowever, the limited availability of expert demonstration data often hinders\nits ability to effectively aid downstream RL learning. To address this problem,\nwe propose a novel two-stage method dubbed as Skill-enhanced Reinforcement\nLearning Acceleration (SeRLA). SeRLA introduces a skill-level adversarial\nPositive-Unlabeled (PU) learning model to extract useful skill prior knowledge\nby enabling learning from both limited expert data and general low-cost\ndemonstration data in the offline prior learning stage. Subsequently, it\ndeploys a skill-based soft actor-critic algorithm to leverage this acquired\nprior knowledge in the downstream online RL stage for efficient training of a\nskill policy network. Moreover, we develop a simple skill-level data\nenhancement technique to further alleviate data sparsity and improve both skill\nprior learning and downstream skill policy training. Our experimental results\non multiple standard RL environments show the proposed SeRLA method achieves\nstate-of-the-art performance on accelerating reinforcement learning on\ndownstream tasks, especially in the early learning phase.",
  "text": "Skill-Enhanced Reinforcement Learning Acceleration\nfrom Demonstrations\nHanping Zhang1,\nYuhong Guo1,2\n1School of Computer Science, Carleton University, Ottawa, Canada\n2Canada CIFAR AI Chair, Amii, Canada\njagzhang@cmail.carleton.ca, yuhong.guo@carleton.ca\nAbstract\nLearning from Demonstration (LfD) aims to facilitate rapid Reinforcement Learn-\ning (RL) by leveraging expert demonstrations to pre-train the RL agent. However,\nthe limited availability of expert demonstration data often hinders its ability to ef-\nfectively aid downstream RL learning. To address this problem, we propose a novel\ntwo-stage method dubbed as Skill-enhanced Reinforcement Learning Accelera-\ntion (SeRLA). SeRLA introduces a skill-level adversarial Positive-Unlabeled (PU)\nlearning model to extract useful skill prior knowledge by enabling learning from\nboth limited expert data and general low-cost demonstration data in the offline prior\nlearning stage. Subsequently, it deploys a skill-based soft actor-critic algorithm\nto leverage this acquired prior knowledge in the downstream online RL stage for\nefficient training of a skill policy network. Moreover, we develop a simple skill-\nlevel data enhancement technique to further alleviate data sparsity and improve\nboth skill prior learning and downstream skill policy training. Our experimental\nresults on multiple standard RL environments show the proposed SeRLA method\nachieves state-of-the-art performance on accelerating reinforcement learning on\ndownstream tasks, especially in the early learning phase.\n1\nIntroduction\nDespite the wide applicability of reinforcement Learning (RL) across robotics [1], video games [2, 3],\nand large language models [4, 5], a conventional deep RL agent often requires numerous iterative\ninteractions with the environment to learn a useful policy by maximizing the expected discounted\ncumulative reward [6], resulting in prolonged training periods and limited computational efficiency\nthat can be particularly problematic for large input scales. To overcome this problem, Learning from\nDemonstration (LfD), also known as imitation learning (IL), has been investigated to accelerate\nRL. In LfD, the agent is pre-trained on a small offline demonstration dataset provided by human\nexperts [7, 8] to acquire knowledge and learn behaviors that can be executed in the environment,\nwhich can then be leveraged to accelerate the online learning process of the downstream RL task\nwith fewer interactions with the environment. Due to the limited availability of expert demonstration\ndata, some recent studies seek to supplement the expert data with a large task-agnostic demonstration\ndataset collected inexpensively using methods such as autonomous exploration [9, 10] or human-tele\noperation [11, 12].\nAs an RL technique that learns reusable skills from a given expert behavior [13, 14] or interacting\nwith online environments [15, 16, 14, 9] to guide the RL training, skill-based RL shows great potential\nfor LfD. Recently, researchers have introduced skill-based RL to LfD by learning reusable skill\nbehaviors from the demonstration data and deploying them for the downstream tasks [11, 17–20].\nHowever, these previous studies either focus solely on learning from the expert dataset or treat the\ngeneral demonstration data as negative examples, impeding the effective utilization of the low-cost\ndemonstration data that are readily available and can still contain valuable fragmented skills.\narXiv:2412.06207v1  [cs.LG]  9 Dec 2024\nIn this paper, we propose a novel SeRLA method, which stands for Skill-enhanced Reinforcement\nLearning Acceleration from demonstrations, to address the problem of learning from heterogeneous\ndemonstration data and accelerating the downstream RL with the learned knowledge. SeRLA\naccelerates RL by pursuing skill-level learning in two stages with three coherent components: a skill-\nlevel adversarial Positive-Unlabeled (PU) learning model, a skill-based policy learning algorithm,\nand a skill-level data enhancement technique. In the offline skill prior training stage, the skill-level\nadversarial PU learning model induces useful skill prior knowledge by exploiting the general and\ntask-agnostic demonstration data as unlabeled examples in addition to the positive expert data, instead\nof simply differentiating them. This strategy facilitates improved utilization of the extensive low-cost\ndemonstration data and help alleviate the scarcity of the expert data. In the online downstream RL\npolicy training stage, a skill-based soft actor-critic algorithm is deployed to integrate skills learned\nin the offline stage and accelerate skill policy learning. Moreover, a simple but novel Skill-level\nData Enhancement (SDE) technique is introduced to improve the robustness of skill learning and\nadaptation at both stages. We conduct experiments on four standard RL environments by comparing\nthe proposed SeRLA with the state-of-the-art skill-based imitation learning methods: SPiRL [17] and\nmodel-based SkiMo [18].\nThe main contributions of this paper can be summarized as follows:\n• This is the first work that conducts skill-level Positive-Unlabeled Learning for LfD/IL. The\nproposed SeRLA takes the low-cost general demonstration data as unlabeled examples\nto statistically support skill learning from the limited positive examples (i.e., the expert\ndemonstration data) through skill-level adversarial PU learning.\n• We propose a simple but novel skill-level data enhancement (SDE) technique, which auto-\nmatically augments the skill-level representations for both the skill prior learning and the\ndownstream policy learning processes to improve the robustness of the learned skill prior\nand accelerate the skill-policy function training.\n• The proposed SeRLA produces effective empirical results for accelerating downstream\nRL tasks. It largely outperforms the standard skill prior learning method, SPiRL, while\nproducing notable improvements over the state-of-the-art model-based skill-level method,\nSkiMo, in the early downstream training stage.\n2\nRelated Works\nRL from Demonstrations\nLearning from Demonstration (LfD) or imitation learning (IL), unlike\noffline RL which learns optimal policies solely from offline data, focuses on accelerating downstream\nRL tasks by pre-training the RL agent on a small set of expert demonstrations without reward\nsignals [7, 8]. In addition to the limited expert demonstrations, large task-agnostic demonstration\ndatasets can also be collected inexpensively [9–12] from the environment for extracting potential\nlearnable behaviors through LfD. There are three major methods to solve LfD/IL problems: Behavior\nCloning (BC) [7], Inverse Reinforcement Learning (IRL) [21], and Generative Adversarial Imitation\nLearning (GAIL) [22]. BC learns a direct mapping from observations to actions through supervised\nlearning but struggles with generalization and distribution shifts [23, 24]. IRL infers reward functions\nfrom demonstrations and trains the agent using standard RL algorithms, but it is computationally\nexpensive and relies on the reward model’s effectiveness [25]. GAIL uses a generative adversarial\nnetwork where a discriminator distinguishes between agent and expert behaviors, achieving strong\nperformance despite needing many interactions with the environment [26].\nSkill-Based RL\nSkill-based RL methods extract reusable skills as abstracted long-horizon behavior\nsequences of actions [13, 15, 17, 27, 14, 9, 11, 16], where the skills are predefined by experts\nor extracted from datasets. In recent works, skill-Prior RL (SPiRL) accelerates downstream RL\ntasks using learned skill priors from offline demonstration data [17]. Skill-based Learning with\nDemonstration (SkiLD) regularizes policy learning in downstream tasks using a skill posterior [28].\nFew-shot Imitation with Skill Transition Models (FIST) learns skills from few-shot demonstration\ndata and generalizes to unseen tasks [19]. Adaptive Skill Prior for RL (ASPiRe) adaptively learns\ndistinct skill priors from different datasets [20]. Skill-based Model-based RL (SkiMo) applies\nplanning on downstream tasks using a skill dynamics model [18].\n2\nPositive-Unlabeled Learning\nUnlike standard binary classifiers that use positive and negative\nexamples, PU learning utilizes only positive and unlabeled data [29], where unlabeled examples can\nbelong to either class. PU learning has proven valuable in applications like medical diagnosis [30]\nand knowledge base construction [31, 32]. Prior work has focused on loss functions and optimizers\n[33, 34]. Kiryo et al. [35] extended PU learning to deep neural networks, and Xu and Denil [36]\napplied it to imitation reward learning by replacing the adversarial loss in GAIL [22] with PU loss,\ncreating a PU-based reward function for RL agents. By contrast, our work delivers a skill-level\ntwo-stage training framework that learns skill priors simultaneously from both limited expert data and\nlow-cost general demonstration data via a PU learning model to accelerate the downstream RL tasks.\n3\nProblem Setting\nReinforcement Learning from Demonstrations (LfD) aims to accelerate the online downstream RL\nprocedure by leveraging offline demonstration datasets. Specifically, we assume LfD has access to\ntwo demonstration datasets: a limited expert dataset Dπe and a low-cost general demonstration dataset\nDπ. The expert dataset is a task-specific small offline dataset that contains expert demonstration\ntrajectories (state-action sequences) Dπe = {s0, a0, · · · , st, at, · · · }, which are generated by human\nexperts or fully trained RL agents. The general demonstration dataset is a much larger task-agnostic\noffline dataset that consists of randomly collected trajectories Dπ = {s0, a0, · · · , st, at, · · · }. The\naction sequences contained in Dπe and Dπ can be denoted as Aπe and Aπ, respectively. While the\ngeneral demonstration dataset does not provide as much useful information as the expert dataset, it\nmay still contain short-horizon behaviors that, if properly extracted, can guide the RL agent to behave\nwith feasible actions and propel the policy learning.\nThe downstream RL task is a standard reinforcement learning problem that can be represented as\na Markov Decision Process (MDP) M = (S, A, T , R, γ), as described in [6]. In this MDP, S is\nthe state space, A is the action space, T : S × A →S is the transition dynamics p(st+1|st, at),\nR : S × A →R is the reward function, and γ ∈(0, 1) is the discount factor. The goal is to learn\nan optimal policy π⋆: S →A that maximizes the expected discounted cumulative reward (return):\nπ⋆= arg maxπ Jr(π) = Eπ[PT\nt=0 γtrt]. The goal of this work is to learn useful skill prior from\nthe offline demonstration datasets and then deploys such skill-level knowledge to facilitate fast policy\ntraining for the downstream RL task.\n4\nMethod\nThe proposed SeRLA method has two stages: the offline skill prior training stage with skill-level\nadversarial PU learning and the online skill-based downstream policy training. The skill prior training\ninduces useful high-level skill knowledge in form of skill prior from the given demonstration datasets\n(Dπe and Dπ), which is then used to accelerate the downstream skill-based policy training through a\nSkill-based Soft Actor-Critic (SSAC) algorithm. Moreover, a simple skill-level data enhancement\ntechnique is further devised for the two training stages to improve the overall performance. Below,\nwe elaborate on these approach components.\n4.1\nSkill Prior Training with PU Learning\nIn the skill prior training stage, we build a regularized deep autoencoder model with skill-level\nadversarial PU learning to learn a conditional skill prior distribution function qψ(zt|st) from the\ntrajectories provided in the two demonstration datasets, where latent variables {zt} are used to capture\nthe high level representations of skills, each of which can be interpreted as an action sequence. The\nmodel includes a skill encoder network qµ(·), a skill decoder network pν(·), a skill prior network\nqψ(·), and a discriminator Dζ. The first three components can be learned from the expert data Dπe\nwithin a conventional autoencoder framework, while the discriminator is innovatively deployed to\nincorporate the general demonstration data Dπ into the skill prior training process and alleviate skill\ndata sparsity via adversarial PU learning.\nConventional Skill Learning Framework\nUnder a deep autoencoder framework [17], the skill\nencoder qµ(zt|at) takes an action sequence at = {at, ..., at+H−1} with length H as input and maps\nit to a latent skill embedding zt. Conversely, the skill decoder pν(ˆat|zt) reconstructs an action\n3\nsequence ˆat from the given skill zt. The autoencoder can be learned by minimizing a reconstruction\nloss on the expert action sequences Aπe:\nLrec(ν, µ) = Eat∼Aπe ℓls\n\u0000ˆat ∼pν\n\u0000· |zt ∼qµ(·|at)\n\u0001\n, at\n\u0001\n,\n(1)\nwhere ℓls(·, ·) is the standard least squares loss. The skill prior network qψ(zt|st) generates a skill zt\nfrom a given starting state st. It is designed to support policy network training for the downstream\ntasks by encoding the expert behavior in a given state in terms of skills from the expert dataset.\nIdeally, given a pair of observed state and action sequence (st, at), the skills produced by the encoder\nqµ(zt|at) and the prior network qψ(zt|st) should be consistent. Hence, the skill prior network can be\nlearned together with the encoder by minimizing the following prior training loss:\nLprior(ψ, µ) = E(st,at)∼Dπe LKL(qµ(zt|at), qψ(zt|st)),\n(2)\nwhere LKL(·, ·) denotes the Kullback Leibler (KL) divergence function. Moreover, a standard\nGaussian distribution prior p(zt) = N(0, 1) can be deployed to regularize the skill embedding space\nwith the following regularization loss:\nLreg(µ) = Eat∼Aπe LKL(qµ(zt|at), p(zt))\n(3)\n4.1.1\nSkill-level Adversarial PU Learning\nDifferent from the expert data, the randomly collected large demonstration data Dπ can present a\ngreat number of short-horizon behaviors, some of which can be meaningful while many others can be\nspontaneous or arbitrary. Hence it is not suitable to directly deploy Dπ in the autoencoder model\nabove in the same way as the expert data. To effectively filter out the noisy behaviors and exploit\nthe useful ones from Dπ, we deploy a PU learning scheme to perform skill learning simultaneously\nfrom both the small expert data Dπe and the large general demonstration data Dπ. Specifically, we\ntreat the skills (capturing the behaviors) from the expert data, Ze = qµ(Aπe), as positive examples\n(i.e., useful skills), and treat skills from the general demonstration data, Z = qµ(Aπ), as unlabeled\nexamples that can be either positive or negative. Then we propose to learn a binary probabilistic\ndiscriminator Dζ from the positive and unlabeled skill examples by adapting a standard non-negative\nPU risk function derived in the literature [35] into the skill-level learning:\nLpu\nDζ(qµ(Aπe), qµ(Aπ)) = λL1\nDζ(qµ(Aπe)) + max(−ξ, L0\nDζ(qµ(Aπ)) −λL0\nDζ(qµ(Aπe)))\n(4)\nwhere λ > 0 and ξ ≥0 are hyperparameters. Here the true positive risk L1\nDζ(qµ(Aπe)) is calculated\non positive skill examples Ze, while the true negative risk is calculated on both positive and unlabeled\ndata (Ze and Z) using two terms, L0\nDζ(qµ(Aπe)) and L0\nDζ(qµ(Aπ)). These risk terms are defined in\nterms of the discriminator Dζ as follows:\nL1\nDζ(qµ(Aπe)) =\nE\nat∼Aπe\n[log(1 −Dζ(zt ∼qµ(·|at)))]\n(5)\nL0\nDζ(qµ(Aπ)) =\nE\nat∼Aπ\n[log(Dζ(zt ∼qµ(·|at)))]\n(6)\nL0\nDζ(qµ(Aπe)) =\nE\nat∼Aπe\n[log(Dζ(zt ∼qµ(·|at)))]\n(7)\nwhere Dζ(zt) predicts the probability of the given skill vector zt being a positive example and\n(1 −Dζ(zt)) denotes the probability of the given skill vector zt being a negative example.\nThis PU loss Lpu\nDζ can be integrated into the deep skill learning model in an adversarial manner to\nenable the exploitation of the large demonstration data Dπ: the discriminator Dζ will be learned to\nminimize the PU loss in Eq.(4) given the skill examples extracted, while the skill encoder qµ(·) will\nbe learned to maximize the PU loss, aiming to alleviate the scarcity of expert data and generalize the\nskill learning to the large general demonstration data Dπ.\nSkill Prior Training Algorithm\nThe total loss function for adversarial PU-learning based skill\nprior training can be expressed as the sum of four terms:\nL(µ, ν, ψ) = Lrec(ν, µ) + Lprior(ψ, µ) + βLreg(µ) −ρ minζ Lpu\nDζ(qµ(Aπe), qµ(Aπ))\n(8)\nwhere the reconstruction loss Lrec enforces consistency between the skill embedding zt and the\naction sequence at; the prior training loss Lprior ensures that the generated skill is consistent with the\n4\nAlgorithm 1 Skill Prior Training via PU Learning\nInput: Expert data Dπe, general demonstration data Dπ\nInitialize: Encoder qµ(·), decoder pν(·), skill prior qψ(·), and discriminator Dζ(·)\nOutput: Trained skill prior network qψ(zt|st), and decoder pν(at:t+H−1|zt)\nProcedure:\n1: for each iteration do\n2:\nfor every H environment steps do\n3:\nSample st and sequence at:t+H−1 from Dπe\n4:\nSample action sequence a′\nt′:t′+H−1 from Dπ\n5:\nUpdate µ, ν, ψ by minimizing Eq.(8)\n6:\nUpdate ζ by maximizing Eq.(8)\n7:\nend for\n8: end for\ncurrent state and action sequence; Lreg regularizes the skill embedding space; and the PU loss Lpu\nDζ\nis used to effectively incorporate large random demonstration data into skill learning in an adversarial\nmanner. The joint training of all these components is expected to effectively and proficiently learn\nvaluable skill knowledge from the unified heterogeneous offline demonstration datasets.\nAlgorithm 1 outlines the main steps of stochastic skill prior training. At each timestep t, we collect\nthe current state st and an action sequence at:t+H−1 from the expert dataset Dπe in an H-step\nrollout. Similarly we collect a sequence of actions at′:t′+H−1 from the general demonstration\ndataset. We jointly learn the parameters µ, ν, and ψ for the skill prior network qψ(zt|st), skill\nencoder qµ(zt|at:t+H−1), and skill decoder pν(at:t+H−1|zt) by minimizing Eq.(8). Adversarially,\nthe discriminator Dζ is updated by minimizing Eq.(4), which is equivalent to maximizing Eq.(8).\n4.2\nDownstream Policy Training\nIn the downstream policy training stage, we aim to exploit the skill knowledge learned from the\ndemonstration data, encoded by the skill prior network qψ(·) and decoder network pν(·|zt), to\naccelerate the online RL process. To this end, we train a skill-based policy network πθ(zt|st) for the\ndownstream online RL task with skill-level behavior cloning.\nSpecifically, following the work [17], when interacting with the environment, we sample a skill zt\nfrom the skill policy network πθ(·|st) given the current state st. The skill zt is then decoded to an\naction sequence at:t+H−1 using the skill decoder pν(·|zt) to guide the RL agent to reach state st+H\nin H steps. The cumulative reward over the H steps, i.e., the H-step reward, can be collected from the\nenvironment as ˜rt = Pt+H\nt\nrt. With the online skill-based transition data D = {(st, zt, ˜rt, st+H)},\nwe deploy a Skill-based soft actor-critic (SSAC) algorithm utilized in [17] to conduct skill-based\npolicy learning with skill-level behavior cloning. SSAC extends soft actor-critic (SAC) [37] to\nlearn the skill policy function network πθ(zt|st) with the support of a skill-based soft Q-function\nnetwork Qϕ(st, zt). It utilizes a KL-divergence regularization term to enforce the skill policy function\nπθ(zt|st) to clone the behavior of the pre-trained skill prior network qψ(zt|st). In particular, SSAC\nlearns the skill policy function network by maximizing the following regularized expected skill-based\nQ-value:\nJπ(θ) =\nE\nst∼D,\nzt∼πθ\n\u0002\nQϕ(st, zt) −κLKL(πθ(zt|st), qψ(zt|st))\n\u0003\n.\n(9)\n4.3\nSkill-Level Data Enhancement\nCollecting expert demonstrations can be challenging and expensive, due to the involvement of human\nexperts [8]. The scarcity of the limited expert data however hinders the robust and effective learning\nof skills. To alleviate the problem, in addition to incorporating general random demonstration data\nduring the prior training stage, we further propose a Skill-level Data Enhancement (SDE) technique\nto augment the skill-level data in both the skill prior training stage and the downstream policy training\nstage, improving the robustness of learning at the skill-level.\n5\nAlgorithm 2 Online Skill-Policy Training with SDE\nInput: Skill prior network qψ(zt|st), and decoder pν(at:t+H−1|zt), η ∈(0, 1)\nInitialize: Replay buffer D, skill policy πθ(zt|st), and skill-based Q value functions\nOutput: Trained skill policy network πθ(zt|st)\nProcedure:\n1: for each iteration do\n2:\nfor every H environment steps do\n3:\nSample skill zt from policy: zt ∼πθ(zt|st)\n4:\nSample at = at:t+H−1 from decoder pν(·|zt)\n5:\nSample state st+H and cumulative reward ˜rt by interacting with environment using at\n6:\nUpdate buffer: D ←D ∪{st, zt, ˜rt, st+H}\n7:\nSample ϵ ∼N m(0, 1), and let ˆzt = zt + ηϵ\n8:\nAugment buffer: D ←D ∪{st, ˆzt, ˜rt, st+H}\n9:\nend for\n10:\nfor each gradient step do\n11:\nUpdate skill-policy function and Q-function\n12:\nend for\n13: end for\nConventional data augmentation is typically applied to the input data, e.g., state observations. It is not\napplicable to the action space since actions (continuous or discrete) cannot be easily re-represented\nor rescaled. By using a latent variable model to learn skills as latent representations for behaviours\ncaptured by action sequences, our proposed approach provides a new augmentation space at the\nskill level, without interfering with the real action space. Specifically, we propose to augment our\nskill level data with Gaussian noise altered versions as follows. For each skill embedding zt, we\ncan add a Gaussian noise altered version ˆzt = zt + ηϵ into learning, where ϵ ∼N m(0, 1) is a\nGaussian noise vector sampled from a m-dimensional independent standard Gaussian distribution,\nm is the dimension of the skill embedding, and η ∈(0, 1) is a very small scaling factor. We then\nenforce zt and ˆzt correspond to the same action sequence, aiming to achieve stable and differentiable\nrepresentations for different behaviors in the skill embedding space, and enhance the robustness of\nskill learning.\nIn the skill prior training stage, SDE is realized by adding the following auxiliary reconstruction loss\ninto the learning objective in Eq.(8):\nˆLrec(ν, µ) = α Eat∼Aπe ℓls\n\u0000ˆat ∼pν\n\u0000· |ˆzt ∼ηϵ+qµ(·|at)\n\u0001\n, at\n\u0001\n(10)\nwhere α is a trade-off parameter. This augmenting loss adds Gaussian noise to the encoded skill\nvector, aiming to enforce the robustness of the skill encoder and decoder functions, and make them\nresistant to minor variations in the skill embedding vectors.\nIn the downstream skill policy training stage, SDE is realized efficiently by augmenting the skill-based\ntransition data. For each observed skill-based transition {st, zt, ˜rt, st+H}, we produce an altered skill\nvector and add an augmenting transition {st, ˆzt, ˜rt, st+H} to buffer D, without any extra interaction\nwith the environment. The goal is to make the skill-policy network more robust to small variations in\nthe skill embedding space, accelerating the learning process. The online skill policy training process\naugmented with SDE is outlined in Algorithm 2.\n5\nExperiment\n5.1\nExperimental Setting\nEnvironments\nWe conduct experiments with four demonstration-guided tasks with long-horizon\nand sparse rewards in four different environments that are commonly used for skill learning: Maze,\nKitchen, Mis-aligned Kitchen from the D4RL datasets [38] and CALVIN [18] from CALVIN\nchallenge [39]. Maze is a navigation environment, in which a point mass agent is required to find\na path between a randomly initialized starting point and a goal point [28]. Kitchen is a robotic\nmanipulation environment in which a robotic arm completes a sequence of four sub-tasks (Microwave-\nKettle-Bottom Burner-Light) [11]. Mis-aligned Kitchen is modified from the Kitchen environment\n6\nFigure 1: The comparison results on the four long-horizon sparse-reward tasks (Maze, Kitchen,\nMis-aligned Kitchen, and CALVIN) are presented in the figure. Each plot presents the average\nper-trajectory return (i.e., reward) v.s. environment steps for each environment during the downstream\ntraining process.\nwith a different task sequence (Microwave-Light-Slide Cabinet-Hinge Cabinet) [28, 18]. CALVIN is\na Language-Conditioned Policy Learning challenge[39], which has been adapted for skill learning in\nSkiMo [18], requiring the RL agent to complete a sequence of four sub-tasks in order: Open Drawer,\nTurn on Lightbulb, Move Slider Left, and Turn on LED.\nComparison Methods\nWe compared the proposed SeRLA with two state-of-the-art skill-based\nmethods, SPiRL [17] and SkiMo [18], which train skill priors for the downstream tasks.\n5.2\nExperimental Results\nWe conducted experiments on the four environments to compare the proposed full method, SeRLA,\nand its variant without SDE, SeRLA-w/o-SDE, with the other two skill-based methods, SPiRL and\nSkiMo. The skills were learned on the demonstration data prior to the downstream task and the\nreward was evaluated in the downstream RL learning process over 107 environment steps. The\nmaximum trajectory reward for Maze environment is 1, while for Kitchen, Mis-aligned Kitchen, and\nCALVIN environments, it is 4. The experimental results are presented in Figure 1. The four plots\non the left side report results (return v.s. environment steps) on the four environments separately.\nWe can see that the proposed SeRLA-w/o-SDE largely outperforms the baseline SPiRL across all\nthe four environments, especially on Maze, Mis-aligned Kitchen and CALVIN, which validates\nthe effectiveness and contribution of the proposed PU Learning component and SSAC algorithm\nin extracting and deploying skill knowledge. The proposed full approach SeRLA further boosts\nthe performance over SeRLA-w/o-SDE with notable gains across the four plots, which verifies the\neffectiveness of the proposed skill-level data enhancement technique. SeRLA also outperforms the\nmodel-based state-of-the-art SkiMo and produces best results on Kitchen and Mis-aligned Kitchen.\nOn the other two environments, Maze and CALVIN, SeRLA yields comparable overall performance\nto SkiMo throughout the downstream RL training, while producing the best results in the early\ntraining stage.\n6\nConclusion\nIn this paper, we proposed a novel two-stage skill-level learning method SeRLA to exploit offline\ndemonstration data and accelerate downstream RL tasks. SeRLA deploys skill-level adversarial\nPU learning to learn reusable skills from both limited expert demonstration data and large low-cost\ndemonstration data. Then a skill-based soft actor-critic algorithm is deployed to utilize the learned\nskill prior knowledge and accelerate the online downstream RL through skill-based behavior cloning.\nThe proposed approach conveniently provides a new augmentation space at the skill level without\ninterfering with the real action space, which enables novel skill-level data enhancement (SDE) in\nboth training stages. Our experimental results on four benchmark environments demonstrate that\nSeRLA outperforms two state-of-the-art skill learning methods, SPiRL and SkiMo, especially in the\nearly downstream training stage. Given the promise and effectiveness of our simple but innovative\nskill-level data enhancement technique, we plan to further investigate various data augmentation\nstrategies in the high level latent skill space in the future.\n7\nReferences\n[1] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in robotics: A survey,” The\nInternational Journal of Robotics Research, 2013.\n[2] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani,\nH. Küttler, J. Agapiou, J. Schrittwieser et al., “Starcraft ii: A new challenge for reinforcement\nlearning,” arXiv preprint arXiv:1708.04782, 2017.\n[3] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer,\nS. Hashme, C. Hesse et al., “Dota 2 with large scale deep reinforcement learning,” arXiv\npreprint arXiv:1912.06680, 2019.\n[4] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F.\nChristiano, “Learning to summarize with human feedback,” in NeurIPS, 2020.\n[5] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray et al., “Training language models to follow instructions with human feedback,”\nin NeurIPS, 2022.\n[6] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[7] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of robot learning from\ndemonstration,” Robotics and autonomous systems, 2009.\n[8] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and A. Nowé, “Reinforcement\nlearning from demonstration through shaping,” in IJCAI, 2015.\n[9] K. Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. Riedmiller, “Learning an embed-\nding space for transferable robot skills,” in ICLR, 2018.\n[10] A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman, “Dynamics-aware unsupervised\ndiscovery of skills,” in ICLR, 2020.\n[11] A. Gupta, V. Kumar, C. Lynch, S. Levine, and K. Hausman, “Relay policy learning: Solving\nlong-horizon tasks via imitation and reinforcement learning,” in CoRL, 2019.\n[12] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta,\nE. Orbay et al., “Roboturk: A crowdsourcing platform for robotic skill learning through\nimitation,” in CoRL, 2018.\n[13] Y. Lee, S.-H. Sun, S. Somasundaram, E. S. Hu, and J. J. Lim, “Composing complex skills by\nlearning transition policies,” in ICLR, 2019.\n[14] M. Dalal, D. Pathak, and R. R. Salakhutdinov, “Accelerating robotic reinforcement learning via\nparameterized action primitives,” in NeurIPS, 2021.\n[15] Y. Lee, J. Yang, and J. J. Lim, “Learning to coordinate manipulation skills via skill behavior\ndiversification,” in ICLR, 2020.\n[16] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine, “Diversity is all you need: Learning skills\nwithout a reward function,” in ICLR, 2019.\n[17] K. Pertsch, Y. Lee, and J. Lim, “Accelerating reinforcement learning with learned skill priors,”\nin CoRL, 2021.\n[18] L. X. Shi, J. J. Lim, and Y. Lee, “Skill-based model-based reinforcement learning,” in CoRL,\n2022.\n[19] K. Hakhamaneshi, R. Zhao, A. Zhan, P. Abbeel, and M. Laskin, “Hierarchical few-shot imitation\nwith skill transition models,” in ICLR, 2022.\n[20] M. Xu, M. Veloso, and S. Song, “ASPire: Adaptive skill priors for reinforcement learning,” in\nNeurIPS, 2022.\n8\n[21] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement learning,” in ICML,\n2004.\n[22] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in NeurIPS, 2016.\n[23] S. Ross and D. Bagnell, “Efficient reductions for imitation learning,” in AISTATS, 2010.\n[24] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning and structured prediction\nto no-regret online learning,” in AISTATS, 2011.\n[25] Q. Li, J. Zhang, D. Ghosh, A. Zhang, and S. Levine, “Accelerating exploration with unlabeled\nprior data,” in NeurIPS, 2023.\n[26] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and\nY. Bengio, “Generative adversarial networks,” in NeurIPS, 2014.\n[27] Y. Lee, J. J. Lim, A. Anandkumar, and Y. Zhu, “Adversarial skill chaining for long-horizon\nrobot manipulation via terminal state regularization,” in CoRL, 2021.\n[28] K. Pertsch, Y. Lee, Y. Wu, and J. J. Lim, “Demonstration-guided reinforcement learning with\nlearned skills,” in CoRL, 2021.\n[29] J. Bekker and J. Davis, “Learning from positive and unlabeled data: A survey,” Machine\nLearning, 2020.\n[30] M. Claesen, F. De Smet, P. Gillard, C. Mathieu, and B. De Moor, “Building classifiers to predict\nthe start of glucose-lowering pharmacotherapy using belgian health expenditure data,” arXiv\npreprint arXiv:1504.07389, 2015.\n[31] L. Galárraga, C. Teflioudi, K. Hose, and F. M. Suchanek, “Fast rule mining in ontological\nknowledge bases with amie+,” The VLDB Journal, 2015.\n[32] K. Zupanc and J. Davis, “Estimating rule quality for knowledge base completion with the\nrelationship between coverage assumption,” in WWW, 2018.\n[33] M. C. Du Plessis, G. Niu, and M. Sugiyama, “Analysis of learning from positive and unlabeled\ndata,” in NeurIPS, 2014.\n[34] G. Patrini, F. Nielsen, R. Nock, and M. Carioni, “Loss factorization, weakly supervised learning\nand label noise robustness,” in ICML, 2016.\n[35] R. Kiryo, G. Niu, M. C. Du Plessis, and M. Sugiyama, “Positive-unlabeled learning with\nnon-negative risk estimator,” in NeurIPS, 2017.\n[36] D. Xu and M. Denil, “Positive-unlabeled reward learning,” in CoRL, 2021.\n[37] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor,” in ICML, 2018.\n[38] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl: Datasets for deep data-driven\nreinforcement learning,” arXiv preprint arXiv:2004.07219, 2020.\n[39] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, “Calvin: A benchmark for language-\nconditioned policy learning for long-horizon robot manipulation tasks,” IEEE Robotics and\nAutomation Letters, 2022.\n9\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-12-09",
  "updated": "2024-12-09"
}