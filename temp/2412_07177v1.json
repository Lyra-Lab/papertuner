{
  "id": "http://arxiv.org/abs/2412.07177v1",
  "title": "Effective Reward Specification in Deep Reinforcement Learning",
  "authors": [
    "Julien Roy"
  ],
  "abstract": "In the last decade, Deep Reinforcement Learning has evolved into a powerful\ntool for complex sequential decision-making problems. It combines deep\nlearning's proficiency in processing rich input signals with reinforcement\nlearning's adaptability across diverse control tasks. At its core, an RL agent\nseeks to maximize its cumulative reward, enabling AI algorithms to uncover\nnovel solutions previously unknown to experts. However, this focus on reward\nmaximization also introduces a significant difficulty: improper reward\nspecification can result in unexpected, misaligned agent behavior and\ninefficient learning. The complexity of accurately specifying the reward\nfunction is further amplified by the sequential nature of the task, the\nsparsity of learning signals, and the multifaceted aspects of the desired\nbehavior.\n  In this thesis, we survey the literature on effective reward specification\nstrategies, identify core challenges relating to each of these approaches, and\npropose original contributions addressing the issue of sample efficiency and\nalignment in deep reinforcement learning. Reward specification represents one\nof the most challenging aspects of applying reinforcement learning in\nreal-world domains. Our work underscores the absence of a universal solution to\nthis complex and nuanced challenge; solving it requires selecting the most\nappropriate tools for the specific requirements of each unique application.",
  "text": "POLYTECHNIQUE MONTRÉAL\naffiliée à l’Université de Montréal\nEffective Reward Specification in Deep Reinforcement Learning\nJULIEN ROY\nDépartement de génie informatique et génie logiciel\nThèse présentée en vue de l’obtention du diplôme de Philosophiæ Doctor\nGénie informatique\nAvril 2024\n© Julien Roy, 2024.\narXiv:2412.07177v1  [cs.LG]  10 Dec 2024\nPOLYTECHNIQUE MONTRÉAL\naffiliée à l’Université de Montréal\nCette thèse intitulée :\nEffective Reward Specification in Deep Reinforcement Learning\nprésentée par Julien ROY\nen vue de l’obtention du diplôme de Philosophiæ Doctor\na été dûment acceptée par le jury d’examen constitué de :\nDaniel ALOISE, président\nChristopher J. PAL, membre et directeur de recherche\nPierre-Luc BACON, membre et codirecteur de recherche\nHanane DAGDOUGUI, membre\nPeter VAMPLEW, membre externe\niii\nACKNOWLEDGEMENTS\nFirst, I wish to express my gratitude to my advisors, Christopher Pal and Pierre-Luc Bacon,\nfor taking a chance on me, introducing me to Mila, this beautiful ecosystem, and granting\nme the freedom to explore the subjects that caught my interest. I also wish to thank Farida\nCheriet, Fantin Girard, Kipton Barros and Nicholas Lubbers who gave me my first oppor-\ntunities back in undergrad. Thank you Olivier Delalleau, Joshua Romoff, Pedro Pinheiro,\nJoseph Viviano, and Emmanuel Bengio for being such great mentors and for inspiring me\nto be an ever more investigative, efficient, and confident researcher. Thank you to all my\ncollaborators, Derek Nowrouzezahrai, Wonseok Jeon, Joelle Pineau, and Roger Girgis. It has\nbeen a great pleasure to work with each and everyone of you.\nThank you, Félix G. Harvey, for first sparking my fascination with these mysterious neural\nnetworks. You have been like a big brother to me on this journey, and I might never have\nembarked on this career path without you. Thank you, Paul Barde, for being such a great\ncompanion during our first years in the program, and such a great friend afterwards. I have\nlearned a lot working with you. Thank you, Samuel Lavoie, Maude Lizaire, and David Kanaa,\nmy close friends from the lab. I look forward to many more coffee breaks with you, chatting\nabout movies, chess, and the meaning of life.\nThank you to my family for always encouraging me in my studies. In particular, to my dad,\nAlain Roy, for making me a curious child, and my mom, Martine Ducharme, for making me\na meticulous one! Thanks to my sisters, Laurence and Flavie, for your presence and your\ncolors; you both make me proud. Thank you, Noémie, for your patience, your kindness, and\nyour unwavering support.\nFinally, a deep thanks to my dear friends outside the lab.\nI am extremely lucky to be\nsurrounded by such wonderful human beings. Friends are the family we choose, and I am\ngrateful for every one of you.\niv\nRÉSUMÉ\nAu cours de la dernière décennie, les progrès dans le domaine de l’apprentissage par ren-\nforcement profond en ont fait l’un des outils les plus efficaces pour résoudre les problèmes de\nprise de décision séquentiels. Cette approche combine l’excellence de l’apprentissage profond\nà traiter des signaux complexes avec l’adaptabilité de l’apprentissage par renforcement (RL)\npour s’attaquer à une panoplie de problèmes de contrôle. Lorsqu’il effectue une tâche, un\nagent de RL reçoît des récompenses ou des pénalités en fonction de ses actions. Cet agent\ncherche à maximiser la somme de ses récompenses, permettant ainsi aux algorithmes d’IA de\ndécouvrir des solutions novatrices dans plusieurs domaines. Cependant, cette focalisation sur\nla maximisation de la récompense introduit également une difficulté importante: une spéci-\nfication inappropriée de la fonction de récompense peut considérablement affecter l’efficacité\ndu processus d’apprentissage et entraîner un comportement indésirable de la part de l’agent.\nDans cette thèse, nous présentons des contributions au domaine de la spécification de ré-\ncompense en apprentissage par renforcement profond sous forme de quatre articles. Nous\ncommençons par explorer l’apprentissage par renforcement inverse, qui modélise la fonction\nde récompense à partir d’un ensemble de démonstrations d’experts, et proposons un algo-\nrithme permettant une implémentation et un un processus d’optimisation efficaces. Ensuite,\nnous nous penchons sur le domaine de la composition de récompense, visant à construire des\nfonctions de récompense efficaces à partir de plusieurs composantes. Nous prenons le cas de\nla coordination multi-agent, et proposons des tâches auxiliaires qui ajoutent des signaux de\nrécompense sous forme de biais inductifs qui permettent de découvrir des politiques perfor-\nmantes dans des environnements coopératifs. Nous investiguons également l’utilisation de\nl’optimisation sous contrainte et proposons un cadre pour une spécification plus directe et\nintuitive de la fonction de récompense. Finalement, nous nous tournons vers le problème de\nl’apprentissage par renforcement pour la découverte de nouveaux médicaments et présentons\nune approche multi-objectif conditionnée permettant d’explorer tout l’espace des objectifs.\nCi-après, nous commençons par présenter une revue la littérature sur les stratégies de spécifi-\ncation, identifions les limitations de chacune de ces approches et proposons des contributions\noriginales abordant le problème de l’efficacité et de l’alignement en apprentissage par renforce-\nment profond. La spécification de récompense représente l’un des aspects les plus difficiles de\nl’application de l’apprentissage par renforcement dans les domaines réels. Pour le moment,\nil n’existe pas de solution universelle à ce défi complexe et nuancé; sa résolution nécessite la\nsélection des outils les plus appropriés pour les exigences spécifiques de chaque application.\nv\nABSTRACT\nIn the last decade, Deep Reinforcement Learning has evolved into a powerful tool for complex\nsequential decision-making problems. It combines deep learning’s proficiency in processing\nrich input signals with reinforcement learning’s adaptability across diverse control tasks. At\nits core, an RL agent seeks to maximize its cumulative reward, enabling AI algorithms to\nuncover novel solutions previously unknown to experts. However, this focus on reward max-\nimization also introduces a significant difficulty: improper reward specification can result in\nunexpected, misaligned agent behavior and inefficient learning. The complexity of accurately\nspecifying the reward function is further amplified by the sequential nature of the task, the\nsparsity of learning signals, and the multifaceted aspects of the desired behavior.\nIn this thesis, we present contributions to the field of reward specification in deep rein-\nforcement learning in the form of four articles. We start by exploring inverse reinforcement\nlearning, which models the reward function from a set of expert demonstrations, and intro-\nduce an algorithm allowing for an efficient implementation and training procedure. Then, we\ndelve into the realm of reward composition, aiming to construct effective reward functions\nfrom various components. We take the case of multi-agent coordination and propose auxil-\niary tasks that augment the reward signal with inductive biases leading to high-performing\npolicies in cooperative multi-agent environments. We also investigate the use of constrained\noptimization and propose a framework for direct reward specification when using a specific\nconstraint family. Lastly, we turn our attention to the problem of RL for drug discovery and\npresent a goal-conditioned, multi-objective approach to explore the entire objective space of\nmolecular candidates.\nThroughout this document, we survey the literature on effective reward specification strate-\ngies, identify core challenges relating to each of these approaches, and propose original con-\ntributions addressing the issue of sample efficiency and alignment in deep reinforcement\nlearning. Reward specification represents one of the most challenging aspects of applying\nreinforcement learning in real-world domains. Our work underscores the absence of a uni-\nversal solution to this complex and nuanced challenge; solving it requires selecting the most\nappropriate tools for the specific requirements of each unique application.\nvi\nTABLE OF CONTENTS\nACKNOWLEDGEMENTS\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niii\nRÉSUMÉ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niv\nABSTRACT\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nv\nTABLE OF CONTENTS\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nvi\nLIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nx\nLIST OF FIGURES\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxi\nLIST OF ACRONYMS\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxiii\nLIST OF APPENDICES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxv\nCHAPTER 1\nINTRODUCTION\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1\nThe Three Paradigms of Machine Learning . . . . . . . . . . . . . . . . . . .\n2\n1.2\nCharacteristic Challenges of Reinforcement Learning\n. . . . . . . . . . . . .\n3\n1.3\nOutline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\nCHAPTER 2\nTECHNICAL BACKGROUND\n. . . . . . . . . . . . . . . . . . . . .\n5\n2.1\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.1.1\nNeural Networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.1.2\nTraining Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nReinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2.1\nMarkov Decision Processes and the RL objective . . . . . . . . . . . .\n9\n2.2.2\nThe Bellman Equations\n. . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.3\nTabular RL with known environment dynamics\n. . . . . . . . . . . .\n14\n2.2.4\nModel-free RL from experience\n. . . . . . . . . . . . . . . . . . . . .\n16\n2.3\nDeep Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.3.1\nDeep Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2.3.2\nDeep Deterministic Policy Gradients\n. . . . . . . . . . . . . . . . . .\n23\n2.3.3\nMaximum Entropy Reinforcement Learning\n. . . . . . . . . . . . . .\n24\n2.3.4\nGenerative Flow Networks . . . . . . . . . . . . . . . . . . . . . . . .\n26\nvii\nCHAPTER 3\nLITERATURE REVIEW . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.1\nReward Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.1.1\nPotential-Based Reward Shaping\n. . . . . . . . . . . . . . . . . . . .\n30\n3.1.2\nAuxiliary Tasks in RL\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3.1.3\nMulti-Objective RL . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.2\nReward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n3.2.1\nInverse RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.2.2\nPreference-based RL . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n3.2.3\nLanguage-guided RL . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nCHAPTER 4\nPREAMBLE TO TECHNICAL CONTRIBUTIONS . . . . . . . . . .\n44\nCHAPTER 5\nARTICLE 1: ADVERSARIAL SOFT ADVANTAGE FITTING:\nIMITATION LEARNING WITHOUT POLICY OPTIMISATION . . . . . . . . .\n46\n5.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n5.2\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.3\nImitation Learning without Policy Optimization . . . . . . . . . . . . . . . .\n50\n5.3.1\nAdversarial Soft Advantage Fitting – Theoretical setting\n. . . . . . .\n50\n5.3.2\nA Specific Policy Class . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n5.3.3\nAdversarial Soft Advantage Fitting (ASAF) – practical algorithm\n. .\n53\n5.4\nRelated works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5.5\nResults and discussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n5.5.1\nExperimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n5.5.2\nExperiments on classic control and Box2D tasks (discrete and continuous) 57\n5.5.3\nExperiments on MuJoCo (continuous control) . . . . . . . . . . . . .\n57\n5.5.4\nExperiments on Pommerman (discrete control) . . . . . . . . . . . . .\n58\n5.6\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\nCHAPTER 6\nARTICLE 2: PROMOTING COORDINATION THROUGH POLICY\nREGULARIZATION IN MULTI-AGENT DEEP REINFORCEMENT LEARNING\n61\n6.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n6.2\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n6.2.1\nMarkov Games\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n6.2.2\nMulti-Agent Deep Deterministic Policy Gradient . . . . . . . . . . . .\n63\n6.3\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n6.4\nCoordination and Policy regularization . . . . . . . . . . . . . . . . . . . . .\n65\n6.4.1\nTeam regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nviii\n6.4.2\nCoach regularization . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.5\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n6.6\nTraining environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.7\nResults and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.7.1\nAsymptotic Performance . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n6.7.2\nEffects of enforcing predictable behavior\n. . . . . . . . . . . . . . . .\n72\n6.7.3\nAnalysis of synchronous sub-policy selection . . . . . . . . . . . . . .\n73\n6.7.4\nExperiments on discrete action spaces . . . . . . . . . . . . . . . . . .\n74\n6.8\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\nCHAPTER 7\nARTICLE 3: DIRECT BEHAVIOR SPECIFICATION VIA\nCONSTRAINED REINFORCEMENT LEARNING . . . . . . . . . . . . . . . . .\n77\n7.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n7.2\nThe problem with reward engineering . . . . . . . . . . . . . . . . . . . . . .\n79\n7.3\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n7.4\nProposed Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n7.4.1\nIndicator cost functions . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n7.4.2\nMultiplier normalisation . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n7.4.3\nBootstrap Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n7.5\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n7.6\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n7.6.1\nExperiments in the Arena environment . . . . . . . . . . . . . . . . .\n88\n7.6.2\nExperiment in the OpenWorld environment . . . . . . . . . . . . . . .\n90\n7.7\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\nCHAPTER 8\nARTICLE 4: GOAL-CONDITIONED GFLOWNETS FOR\nCONTROLLABLE MULTI-OBJECTIVE MOLECULAR DESIGN . . . . . . . .\n93\n8.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n8.2\nBackground & Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n8.3\nMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n8.3.1\nGoal-conditioned GFlowNets . . . . . . . . . . . . . . . . . . . . . . .\n95\n8.3.2\nLearned Goal Distribution . . . . . . . . . . . . . . . . . . . . . . . .\n96\n8.3.3\nEvaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n8.4\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n8.4.1\nEvaluation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n8.4.2\nComparisons in Difficult Objective Landscapes . . . . . . . . . . . . .\n98\n8.4.3\nComparisons for Increasing Number of Objectives . . . . . . . . . . .\n100\nix\n8.5\nFuture Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\nCHAPTER 9\nGENERAL DISCUSSION . . . . . . . . . . . . . . . . . . . . . . . .\n102\n9.1\nSucesses and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\n9.2\nAdditional considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n9.3\nReward Specification: A persistent challenge . . . . . . . . . . . . . . . . . .\n108\nCHAPTER 10 CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n110\nREFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n110\nAPPENDICES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n141\nx\nLIST OF TABLES\nTable 6.1\nFinal performance in coordination environments . . . . . . . . . . . .\n72\nTable 8.1\nEvaluation of conditioning methods with 2 objectives . . . . . . . . .\n98\nTable 8.2\nEvaluation of conditioning methods with 3 and 4 objectives . . . . . .\n100\nTable A.1\nFixed Hyperparameters for classic control tasks\n. . . . . . . . . . . .\n151\nTable A.2\nBest found hyperparameters for Cartpole . . . . . . . . . . . . . . . .\n152\nTable A.3\nBest found hyperparameters for Mountaincar . . . . . . . . . . . . . .\n152\nTable A.4\nBest found hyperparameters for Lunarlander . . . . . . . . . . . . . .\n152\nTable A.5\nBest found hyperparameters for Pendulum . . . . . . . . . . . . . . .\n153\nTable A.6\nBest found hyperparameters for Mountaincar-c . . . . . . . . . . . . .\n153\nTable A.7\nBest found hyperparameters for Lunarlander-c . . . . . . . . . . . . .\n153\nTable A.8\nFixed hyperparameters for MuJoCo environments. . . . . . . . . . . .\n154\nTable A.9\nBest found hyperparameters for the Hopper-v2 environment\n. . . . .\n154\nTable A.10\nBest found hyperparameters for the HalfCheetah-v2 environment\n. .\n155\nTable A.11\nBest found hyperparameters for the Walker2d-v2 environment . . . .\n155\nTable A.12\nBest found hyperparameters for the Ant-v2 environment\n. . . . . . .\n155\nTable A.13\nFixed Hyperparameters for Pommerman environment . . . . . . . . .\n156\nTable A.14\nBest found hyperparameters for the Pommerman environment . . . .\n157\nTable A.15\nExpert demonstrations used for Imitation Learning . . . . . . . . . .\n158\nTable B.1\nHyperparameter ranges for multi-agent coordination environments . .\n162\nTable B.2\nBest found hyperparameters for SPREAD\n. . . . . . . . . . . . . . .\n163\nTable B.3\nBest found hyperparameters for BOUNCE . . . . . . . . . . . . . . .\n163\nTable B.4\nBest found hyperparameters for CHASE . . . . . . . . . . . . . . . .\n163\nTable B.5\nBest found hyperparameters for COMPROMISE . . . . . . . . . . . .\n163\nTable B.6\nBest found hyperparameters for the football environment . . . . . . .\n164\nTable B.7\nAblations: best found hyperparameters for SPREAD . . . . . . . . .\n164\nTable B.8\nAblations: best found hyperparameters for BOUNCE . . . . . . . . .\n164\nTable B.9\nAblations: best found hyperparameters for CHASE . . . . . . . . . .\n164\nTable B.10\nAblations: best found hyperparameters for COMPROMISE\n. . . . .\n165\nTable C.1\nHyperparameters for experiments in the Arena Environment. . . . . .\n176\nTable C.2\nHyperparameters for experiments in the OpenWorld Environment. . .\n178\nTable D.1\nHyperparameters used in our conditional-GFN training pipeline . . .\n182\nxi\nLIST OF FIGURES\nFigure 2.1\nMarkov Chain over state-action pairs . . . . . . . . . . . . . . . . . .\n10\nFigure 3.1\nOverview of reward composition strategies . . . . . . . . . . . . . . .\n30\nFigure 3.2\nOverview of reward modeling strategies . . . . . . . . . . . . . . . . .\n37\nFigure 5.1\nResults on classic control and Box2D tasks . . . . . . . . . . . . . . .\n57\nFigure 5.2\nResults on MuJoCo tasks\n. . . . . . . . . . . . . . . . . . . . . . . .\n58\nFigure 5.3\nResults on Pommerman Random-Tag . . . . . . . . . . . . . . . . . .\n59\nFigure 6.1\nSimple coordination MDP . . . . . . . . . . . . . . . . . . . . . . . .\n65\nFigure 6.2\nIllustration of TeamReg with two agents . . . . . . . . . . . . . . . .\n68\nFigure 6.3\nIllustration of CoachReg with two agents . . . . . . . . . . . . . . . .\n68\nFigure 6.4\nDepiction of multi-agent coordination environments . . . . . . . . . .\n71\nFigure 6.5\nLearning curves on all four coordination environments . . . . . . . . .\n72\nFigure 6.6\nTeamReg ablation study . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nFigure 6.7\nCoachReg synchronicity analysis . . . . . . . . . . . . . . . . . . . . .\n73\nFigure 6.8\nSnapshot of the google research football environment . . . . . . . . .\n74\nFigure 7.1\nEnvironments for direct behavior specification using constrained RL .\n79\nFigure 7.2\nEnforcing behavioral constraints using reward engineering\n. . . . . .\n80\nFigure 7.3\nEffect of the multiplier normalization . . . . . . . . . . . . . . . . . .\n89\nFigure 7.4\nResults with single constraints in the Arena environment . . . . . . .\n89\nFigure 7.5\nResults with multiple constraints in the Arena environment . . . . . .\n90\nFigure 7.6\nResults in the OpenWorld environment . . . . . . . . . . . . . . . . .\n91\nFigure 8.1\nGoal-Conditioned GFlowNets for molecular design . . . . . . . . . . .\n96\nFigure 8.2\nControl comparison of conditioning methods with 2 objectives . . . .\n99\nFigure 8.3\nDensity comparison of conditioning methods with 2 objectives . . . .\n99\nFigure 8.4\nDepiction of a GFlowNet Goal Sampler . . . . . . . . . . . . . . . . .\n101\nFigure A.1\nComparison between ASAF-1 and ASQF . . . . . . . . . . . . . . . .\n148\nFigure A.2\nEffect of gradient penalty on GAIL algorithm\n. . . . . . . . . . . . .\n149\nFigure A.3\nASAF-1 on Ant-v2 for different expert levels of performance . . . . .\n150\nFigure A.4\nTraining times on MuJoCo tasks\n. . . . . . . . . . . . . . . . . . . .\n150\nFigure B.1\nHyperparameter tuning results for coordination algorithms . . . . . .\n165\nFigure B.2\nAverage performance difference between the two agents . . . . . . . .\n166\nFigure B.3\nLearning curves for TeamReg and the baselines on COMPROMISE .\n167\nFigure B.4\nAgent’s policy mask distributions . . . . . . . . . . . . . . . . . . . .\n168\nFigure B.5\nVisualization of two different BOUNCE evaluation episodes\n. . . . .\n169\nxii\nFigure B.6\nVisualization of sequences on two different environments\n. . . . . . .\n169\nFigure B.7\nAnalysis of the policy mask distributions . . . . . . . . . . . . . . . .\n170\nFigure B.8\nLearning curves for varying number of agents\n. . . . . . . . . . . . .\n172\nFigure C.1\nReward engineering for 3 behavioral requirements . . . . . . . . . . .\n179\nFigure C.2\nTD3-Lagrangian agent in the Arena environment\n. . . . . . . . . . .\n180\nFigure D.1\nLearned conditional-distributions for different focus regions . . . . . .\n183\nFigure D.2\nEffect of the replay buffer on goal-conditioned GFNs\n. . . . . . . . .\n183\nFigure D.3\nEffect of the hyperparameter mg on the reward profile . . . . . . . . .\n184\nFigure D.4\nLearning curves with and without a tabular goal sampler . . . . . . .\n186\nFigure D.5\nComparison between conditioning methods for 2 objectives . . . . . .\n187\nFigure D.6\nComparison between conditioning methods for 3 objectives . . . . . .\n188\nFigure D.7\nComparison between conditioning methods for 4 objectives . . . . . .\n188\nxiii\nLIST OF ACRONYMS\nAI\nArtificial Intelligence\nAIL\nAdversarial Imitation Learning\nAIRL\nAdversarial Inverse Reinforcement Learning\nASAF\nAdversarial Soft-Advantage Fitting\nBCE\nBinary Cross-Entropy\nCMDP\nConstrained Markov Decision Processes\nCNN\nConvolutional Neural Network\nCRL\nConstrained Reinforcement Learning\nCTDE\nCentralized Training and a Decentralized Execution\nCUD\nCategorical Uniform Distributions\nDAG\nDirected Acyclic Graph\nDDPG\nDeep Deterministic Policy Gradient\nDPG\nDeterministic Policy Gradient\nDQN\nDeep Q-Networks\nGAN\nGenerative Adversarial Network\nGFN\nGenerative Flow Network\nGNN\nGraph Neural Networks\nGP\nGradient Penalty\nGVF\nGeneral Value Function\nHiL\nHuman-in-the-Loop\nIGD\nInverted Generational Distance\nIL\nImitation Learning\nIRL\nInverse Reinforcement Learning\nLLM\nLarge Language Model\nMADDPG\nMulti-Agent Deep Deterministic Policy Gradient\nMARL\nMulti-Agent Reinforcement Learning\nMC\nMonte Carlo\nMCMC\nMonte Carlo Markov Chain\nMDP\nMarkov Decision Processes\nML\nMachine Learning\nMLE\nMaximum Likelihood Estimation\nMOMDP\nMulti-Objective MDP\nMOO\nMulti-Objective Optimization\nxiv\nMORL\nMulti-Objective Reinforcement Learning\nMSE\nMean Squared Error\nNLP\nNatural Language Processing\nNPC\nNon-Player Characters\nPBRS\nPotential-Based Reward Shaping\nPC-ent\nPareto-Clusters Entropy\nPCC\nPearson Correlation Coefficient\nPPO\nProximal Policy Optimization\nPbRL\nPreference-based Reinforcement Learning\nQED\nQuantitative Estimate of Drug-likeness\nRL\nReinforcement Learning\nRLHF\nReinforcement Learning from Human Feedback\nSAC\nSoft Actor-Critic\nSE\nStandard Error\nSGD\nStochastic Gradient Descent\nSQIL\nSoft-Q Imitation Learning\nTD\nTemporal Difference\nTRPO\nTrust Region Policy Optimization\nTab-GS\nTabular Goal-Sampler\nxv\nLIST OF APPENDICES\nAppendix A\nSupplementary Material for Chapter 5\n. . . . . . . . . . . . . . . . .\n142\nAppendix B\nSupplementary Material for Chapter 6\n. . . . . . . . . . . . . . . . .\n159\nAppendix C\nSupplementary Material for Chapter 7\n. . . . . . . . . . . . . . . . .\n173\nAppendix D\nSupplementary Material for Chapter 8\n. . . . . . . . . . . . . . . . .\n181\n1\nCHAPTER 1\nINTRODUCTION\nFrom the implementation of the first perceptron to modern neural network architectures,\nMachine Learning (ML) has been at the center of great leaps in artificial intelligence and\nkeeps pushing its boundaries today like never before. Starting with notable achievements\nin digit recognition and learning to play Backgammon (LeCun et al., 1998; Tesauro, 1994),\nML has since asserted its dominance in fields requiring high-dimensional data processing\nsuch as computer vision, speech synthesis and natural language processing (Chai et al., 2021;\nOord et al., 2016; Otter et al., 2020). These methods are now showing great capability in\ncomplex control problems, setting new benchmarks in playing strategic board games, flying\nstratospheric balloons, and advancing robotics (Silver et al., 2017; Bellemare et al., 2020;\nAkkaya et al., 2019), each time pushing the boundaries of what computers can do.\nThe ML revolution in information processing has been made possible by stepping away from\ntraditional programming to embrace a radically different paradigm. Instead of requiring a\nperson to directly encode a precise sequence of instructions into a computer program, ML\ncombines powerful optimization methods and flexible models to distill vast amounts of data\ninto complex functional behaviors. The “program” now consists of a set of connection weights\nthat form an artificial neural network, converting inputs into outputs to achieve our goals.\nCrucially, these weights are not explicitly designed, but rather discovered by an optimization\nalgorithm, and the designers of such systems are now responsible for defining objective func-\ntions, curating datasets, and crafting algorithms that guide the learning process (Karpathy,\n2017). Not only are such systems easily scalable and can be optimized on dedicated hardware,\nbut more importantly, they allow to address problems of a complexity beyond the reach of\nconventional logic-based programming.\nThis new paradigm provides a powerful framework for problem solving, but also presents\nunique challenges in the realm of Reinforcement Learning (RL), particularly in how agents\nare instructed to achieve desired outcomes. We use the term reward specification to describe\nthe act of providing an agent with a reward function (Taylor, 2023; Bowling et al., 2023;\nAbel et al., 2021; Singh et al., 2009; Icarte et al., 2022). Although central to the field, reward\nspecification is not a problem that all RL researchers have faced.\nMost research efforts\nreport results on established benchmarks using pre-defined reward functions. However, when\ndeveloping an RL solution for a real-life application, how the reward is specified becomes a\nmajor factor pertaining to the success or failure of the project (Knox et al., 2023).\n2\nThis thesis aims to address the fundamental question: How can we specify a reward function\nthat effectively captures human intentions, ensuring that an agent can learn efficiently and\nthat its behavior aligns with our objectives?\nWe start by motivating how reinforcement\nlearning is uniquely positioned to help us solve difficult problems in artificial intelligence and\nhighlight two of its own challenges.\n1.1\nThe Three Paradigms of Machine Learning\nThe field of machine learning encompasses a wide range of approaches, each with distinct\nmethods for directing algorithmic learning. These methods are often categorized into three\nlearning paradigms. Supervised learning, perhaps the most widely used, operates under a\nstrict framework where algorithms learn from labeled data, mapping inputs to known outputs\nthrough a clear supervisory signal. This guidance clearly establishes the task that the model\nshould perform. However, its dependency on large volumes of labeled data can be a limitation,\nas obtaining such data is often costly and labor intensive (Mathewson and Pilarski, 2022).\nUnsupervised learning, on the other hand, does not require labels, leaving algorithms to\ndiscern patterns and structures within the data autonomously. This approach thus operates\nunder a much lower supervisory burden. However, the absence of explicit guidance in unsu-\npervised learning is both its strength and weakness; it takes advantage of readily available\nunlabeled data but lacks a definitive direction for problem solving. For this reason, it is often\nused as a pre-training procedure for other downstream tasks (Erhan et al., 2010).\nReinforcement Learning (RL) strikes a unique balance between the structured guidance of\nsupervised learning and the autonomous nature of unsupervised learning. In RL, an agent\nlearns to perform a task by interacting with its environment, guided by a reward function\nrather than explicit data labels. This reward function provides a scalar value as feedback for\nthe actions taken, similar to receiving a score in a game, rather than a precise road map to\nsolve the task. The agent’s objective is to maximize its cumulative reward, which indirectly\nshapes its behavior. This form of learning is thus less prescriptive than supervised learning,\navoiding the need for exhaustive labeling, yet it is more directed than unsupervised learning\nsince the reward function encodes the task objectives.\nBy specifying the task using a reward function rather than collecting data, the RL paradigm\nopens up new possibilities. First, to implement a reward function which captures the desired\nbehavior, a system designer does not need to know the solution to the problem but simply to\nbe able to rate solutions, significantly reducing the amount of expert knowledge that must be\nheld to tackle a particular problem. Second, precisely because one does not need to be able to\n3\nsolve the problem to design its reward function, the performance of the model is unbounded.\nThis, for example, allowed a computer to surpass the best human player at Go for the first\ntime in 2016 (Silver et al., 2016). In certain AI applications, such as autonomous driving or\nobject classification, we simply seek to have computers emulate human behavior to free up\ntime and resources to other needs. However, the world is full of challenges such as energy\ngrid management, traffic light optimization, and molecular design, for which the ability to go\nbeyond human performance and reach the best possible solution would be extremely valuable.\nIts lower supervision requirements and superhuman potential make reinforcement learning\nuniquely positioned to tackle some of the most important problems that machine learning\ncan face (Leike et al., 2018).\n1.2\nCharacteristic Challenges of Reinforcement Learning\nThe unique freedom of strategy given to the agent to maximize its rewards comes with\nimportant challenges. First, there is the problem of exploration. The space of behaviors\nthat can be produced in an environment is often exponential in trajectory length, making\nit intractable to evaluate by performing an exhaustive sweep.\nHow to efficiently explore\nthis environment is a fundamental question in RL (Amin et al., 2021). To tackle this issue,\nmost algorithms alternate between taking decisions that are already known to lead to good\noutcomes, to avoid wasting time on completely unfit strategies, and taking random actions\nto discover whether the current strategy can be improved. This is known as the exploitation-\nexploration dilemma and no single solution has been found to offer the perfect balance in\nevery environment, leaving practitioners to experiment with different hyperparameters for\ntheir specific application.\nSecondly, there is the problem of alignment (Amodei et al., 2016; Leike et al., 2018). Specify-\ning the task with a reward function is advantageous in reducing the burden of expert knowl-\nedge. However, this reward function typically fails to capture all of the aspects of behavior\nthat its designer values. This is because when designing a reward function, a practitioner\nmight envision specific scenarios in which the current function would incentivize the desired\nbehavior, but when exploring vast and complex environments, the agent will inevitably find\nitself in circumstances that were unforeseen and where this same reward function can become\ncounterproductive (Hadfield-Menell et al., 2017). Trying to correct this mistake is often com-\nplicated since the consequences of modifying the reward function or adding new elements to\nit can be very difficult to foresee due to the unintuitive nature of the reward accumulation\nthrough time, the effect of discounting, and of the competition between reward components\n(Septon and Amir, 2022; Booth et al., 2023).\n4\nTo address exploration and alignment, the reward function must allow the agent to both learn\nefficiently and converge to a behavior that is consistent with the intentions of its designers\n(Sorg, 2011). However, these two objectives are often conflictual. In their seminal textbook,\nSutton and Barto (2018) underscore the indirect nature of behavior specification in RL:\n“The agent always learns to maximize its reward. If we want it to do something\nfor us, we must provide rewards to it in such a way that in maximizing them the\nagent will also achieve our goals.”\nThis convoluted mapping from reward to behavior makes the design of an effective reward\nfunction a surprisingly arduous task (Singh et al., 2010). The propensity of RL to develop\nstrategies that unexpectedly exploit its reward function has been reported in several works\n(Randløv and Alstrøm, 1998; Clark and Amodei, 2016; Knox et al., 2023; Pan et al., 2022),\nand this challenge may be even more familiar to practitioners outside of academic circles\n(Gupta et al., 2022). Designing effective reward functions thus represents one of the most\nchallenging aspects of RL (Leike et al., 2018; Daniel et al., 2014; Christiano et al., 2017;\nHu et al., 2020; Vamplew et al., 2022), and this difficulty appears to increase as RL algo-\nrithms become more capable (Dewey, 2014; Pan et al., 2022), making the question of reward\nspecification ever more critical.\n1.3\nOutline\nIn this thesis, we present contributions to different algorithmic families that aim to incorpo-\nrate human intuition in the task specification process of RL agents in the form of demonstra-\ntions, auxiliary tasks, behavioral constraints and goal-conditioning. The next sections are\norganized as follows. Chapter 2 presents the technical background that lays the foundations\nof deep learning and reinforcement learning. Chapter 3 presents a review of the relevant\nliterature surrounding the problem of reward specification in RL. Chapters 4 to 8 present\nfour original contributions in the form of peer-reviewed articles. Finally, Chapter 9 presents\na discussion of the limitation of our methods, the additional opportunities offered by environ-\nment design, agent monitoring, and human interventions, and some fundamental obstacles\nto effective objective specification.\n5\nCHAPTER 2\nTECHNICAL BACKGROUND\nThe contributions presented in this thesis focus on various approaches for reward specifica-\ntion in deep reinforcement learning. In this chapter, we present essential elements of deep\nlearning (Section 2.1) and dive into more details on the foundations of reinforcement learning\n(Section 2.2) to establish how these two fields can be brought together to tackle challenging,\nhigh-dimensional sequential decision making problems.\n2.1\nDeep Learning\nDeep learning (Goodfellow et al., 2016) is a subfield of machine learning (Murphy, 2012)\nwhich focuses on the design of architectures and training algorithms for neural networks.\n2.1.1\nNeural Networks\nNeural networks are a powerful family of parametric models for function approximation.\nWhile a plethora of architectures have been developed for different applications, all neural\nnetworks at their core are composed of artificial neurons, which simply consist of a weighted\nlinear combination a(·) of their input x followed by a nonlinearity z(·). These units, sometimes\ncalled perceptron (Rosenblatt, 1958), can be assembled into a layer of da such neurons, giving\nthe model its width. Layers can also be composed in sequence, giving the model its depth.\nAll but the final layer are called hidden layers, and allow to build increasingly more abstract\nrepresentations of the data (LeCun et al., 2015). For example, a simple model could be\nbuilt from one hidden layer followed by a linear output: f(x) := bo + wo⊤z\n\u0010\na(x)\n\u0011\n. Here,\nthe variables x and a represent vectors, and nonlinearity functions denoted by z are applied\nelement-wise:\nf(x) := bo +\nda\nX\ni=1\nwo\ni z(ai)\n,\naj := ba\nj +\ndx\nX\ni=1\nW a\nijxi\n(2.1)\nwhere wo (the output weight vector), W a (the input weight matrix) and b (the bias terms)\nare the parameters of the model. The output can be augmented with any differentiable func-\ntion to satisfy the task at hand. For example, binary classification tasks typically suggest\nthe use of the sigmoid output function, while regression tasks often use the network’s output\nas-is. This basic framework has led to the development of several specialized architectures to\naccommodate different data types. For example, Convolutional Neural Networks (CNNs) are\n6\nspecialized for grid-like data, such as images, and reuse artificial neurons with local connec-\ntivity across the entire grid to enforce spatial equivariance of the function it learns (LeCun\net al., 1998). Graph Neural Networks (GNNs) generalize this idea to node permutations in\ngraph structures (Xu et al., 2018).\nThese building blocks give all neural networks two characteristics crucial to their success.\nFirst, neural networks are easily differentiable, allowing the use of the backpropagation al-\ngorithm (Rumelhart et al., 1986) to compute parameter updates. This procedure not only\nallows an efficient and parallelizable way of training large models, but it has become simple\nto implement with the advent of modern automatic differentiation software (Paszke et al.,\n2019; Abadi et al., 2016), contributing to its popularity.\nSecond, the capacity of neural\nnetworks can be adjusted by increasing the number of parameters (typically the width and\ndepth of the hidden layers). The universal approximation theorem (Cybenko, 1989) states\nthat, given certain assumptions, even a single-layered neural network can, in principle, rep-\nresent any continuous function arbitrarily precisely given enough capacity. Although this\nresult does not guarantee that such functions can be easily found using first-order optimiza-\ntion processes, it speaks of the scalability of this model family and its ability to represent\nvery complex functions (Kaplan et al., 2020). Together, these properties have allowed neural\nnetworks to take advantage of the increasing availability of data and computational power\nto achieve remarkable performance across a wide array of tasks and disciplines.\n2.1.2\nTraining Objectives\nTo learn from data, a machine learning model needs a training objective. The foundation\nof training objectives for deep learning models often begins with the concept of Maximum\nLikelihood Estimation (MLE) (Goodfellow et al., 2016). Given a dataset D of N samples\nD := {x(i)}N\ni=1 from some true data distribution pd, this statistical approach aims to find the\nparameters that maximize the likelihood of the data under the model distribution pθ. Under\nthe assumption that the samples x(i) are independently and identically distributed (i.i.d.),\nthe probability over the entire dataset decomposes into a product which can be expressed as\nthe average log-likelihood (the log function is monotonous and\n1\nN does not depend on θ):\nθ∗\nMLE := arg max\nθ\nP\n\u0010\n{x(i)}N\ni=1; θ\n\u0011\n= arg max\nθ\nN\nY\ni=1\npθ(x(i)) = arg max 1\nN\nN\nX\ni=1\nlog pθ(x)\n(2.2)\nwhere θ represents the parameters of the model (e.g. the weights of a neural network). By\nthe law of large numbers, this sum recovers the expectation of the true data distribution as\n7\nthe number of samples increases:\nlim\nN→∞\n1\nN\nN\nX\ni=1\nlog pθ(x(i)) = Ex∼pd\nh\nlog pθ(x)\ni\n(2.3)\nIn the context of supervised learning, for each datapoint x(i), a label y(i) is provided, and\nthe goal of the model is to predict the correct output for each sample. In a classification\ntask, maximizing the likelihood of the data is equivalent to minimising the well-known cross-\nentropy loss:\nθ∗\nMLE := arg max\nθ\nE(x,y)∼pd\nh\nlog pθ(y|x)\ni\n= arg min\nθ\n−E(x,y)∼pd\nh\nlog pθ(y|x)\ni\n(2.4)\nFor a regression task, it is common to assume a gaussian distribution of the label with fixed\nstandard deviation, and where the model predicts the mean of the distribution. Maximizing\nthe likelihood then uncovers the popular mean-squared error:\nθ∗\nMLE := arg max\nθ\nE(x,y)∼pd\nh\nlog pθ(y|x)\ni\n,\nwith\npθ(y|x) := N\n\u0010\ny; fθ(x), σ2\u0011\n(2.5)\n= arg max\nθ\nE(x,y)∼pd\n\"\nlog\n\u0012\n1\nσ\n√\n2π\n\u0013\n−\n1\n2σ2||y −fθ(x)||2\n#\n(2.6)\n= arg min\nθ\nE(x,y)∼pd\n\u0014 1\n2||y −fθ(x)||2\n2\n\u0015\n(2.7)\nwhere N(y) denotes the gaussian probability density function and || · ||2\n2 is the L2-norm.\nIn unsupervised learning, we generally seek to learn a model of the data distribution which\ncan be used for a variety of purposes including compression, anomaly detection, or generating\nnew datapoints. Interestingly, the maximum likelihood objective can be seen as minimizing\nthe KL-divergence between our model’s distribution pθ and the target distribution pd:\narg min DKL(pd||pθ) := arg min −Ex∼pd\n\"\nlog pθ(x)\npd(x)\n#\n= arg max Ex∼pd\nh\nlog pθ(x)\ni\n(2.8)\nThe expectation over log pd(x) is independent from θ and can be ignored, leaving us with\nthe likelihood term alone. Although KL-divergence is not an exact distance measure (it is\nnot reversible), it is still informative of the progress that pθ is making towards pd as its\noptimum DKL(pd||pθ) = 0 is reached only when pθ = pd. The maximum likelihood objective\nin Equation 2.2 can thus be optimized directly to model pd.\nA possible approach is to parameterize pθ as an energy-based model, which can capture\narbitrarily complex distributions.\nHowever, this family of models does not scale well to\n8\nhigh-dimensional data as it requires computing a normalization constant Z that involves\nan intractable marginalization step.\nSeveral methods have thus been developed to avoid\ncomputing the partition function. For example, autoregressive models use the chain rule\nof probability to decompose the joint (Van den Oord et al., 2016), flow networks leverage\nthe change of variable to model the likelihood as an invertible transformation of a simpler\ndistribution (Rezende and Mohamed, 2015), and variational autoencoders introduce latent\nvariables to optimize a lower bound on the likelihood (Kingma and Welling, 2013). Another\napproach, called implicit modeling, avoids modeling the likelihood function entirely and\nhas led to great success in generating high-quality samples.\nIt consists of learning from\na likelihood ratio between the true data distribution and the model’s distribution, leading to\na min-max game where the loss function Dϕ is learned from binary classification along with\na generative model trained to generate plausible samples (Goodfellow et al., 2014; Mohamed\nand Lakshminarayanan, 2016):\nmin\nθ\nmax\nϕ\nEx∼pd\nh\nlog Dϕ(x)\ni\n+ Ex∼pθ\nh\nlog(1 −Dϕ(x))\ni\n(2.9)\nThrough various applications, deep learning has proven to be an effective and versatile tool\nin the realm of machine learning, offering a wealth of architectures and training paradigms\nto suit both supervised and unsupervised settings. Its strength lies in its ability to learn\nhierarchical representations from data, enabling the extraction of complex patterns and re-\nlationships that may not be readily apparent. In the next section, we lay the foundations\nof reinforcement learning, a task specification paradigm that departs from using data itself\nas the main objective and instead captures the goal of an agent as a reward function to\nmaximize. We then discuss how deep learning can be leveraged to provide powerful function\napproximators for reinforcement learning in vast and complex environments.\n2.2\nReinforcement Learning\nReinforcement Learning (RL) is a very distinct paradigm, naturally suited for sequential de-\ncision making problems. From a supervision perspective, it finds itself in between supervised\nand unsupervised learning. As opposed to unsupervised learning, it has a precisely defined\nobjective through the use of a user-specified reward function. However, contrary to super-\nvised learning, it does not require labeling every datapoint with the correct output, making\nthis approach to task specification much less reliant on expert knowledge. In this section, we\ndescribe Markov Decision Processes (Section 2.2.1), the Bellman equations (Section 2.2.2)\nand a variety of foundational approaches to learn optimal policies (Sections 2.2.3 and 2.2.4).\n9\n2.2.1\nMarkov Decision Processes and the RL objective\nMarkov Decision Processes (MDPs) are a mathematical framework for sequential decision\nmaking (Puterman, 1990). An MDP is defined by the tuple (P, P0, S, A, R, γ). The ensemble\nS denotes the state space and A(s) denotes the action space, both of which can be discrete\nor continuous. The process initiates by sampling an initial state s0 from the initial state\ndistribution P0 : S →[0, 1], P\ns∈S P0(s) = 1. At time-step t, the agent finds itself in state\nst ∈S and must choose an action at ∈A(st). The environment then returns the next state\nst+1 ∈S sampled from the transition distribution P(·|st, at), with P : S ×S ×A →[0, 1] and\nP\ns′∈S P(s′|s, a) = 1. This transition distribution, also called the “environment dynamics”,\nis a central component, as it defines how the agent can influence its environment through\nthe selected action a. This interaction between agent and environment induces a Markov\nchain over state and action pairs that unfolds over time, as depicted in Figure 2.1, where\nS0, A0, ..., ST are random variables representing the state and action at each time-step. Note\nthat, for simplicity, we often omit the detailed reference to the random variable itself, such\nas St, and instead directly refer to its realization, represented by st.\nImportantly, in an MDP, we assume the Markov property, namely, that the environment\ndynamics P only depends on the current state and action (st, at). This implies that we assume\nthe conditional independences st+1 ⊥s<t, a<t | st which allow us to safely use a stationary\npolicy π(at|st), i.e. a policy that depends only on st. Thus, a stationary policy represents\na mapping from a state to a probability distribution over the possible actions in that state,\nπ : S × A →[0, 1] and P\na∈A(s) π(a|s) = 1.\nA given realization of this chain is called a trajectory or rollout, denoted τ = (s0, a0, ..., sT).\nThe probability of a trajectory is given by the joint probability of the entire chain under the\ntrajectory distribution pπ induced by the policy π:\npπ(τ) = pπ(s0, a0, ..., sT−1, aT−1, sT) = P0(s0)\nT−1\nY\nt=0\nP(st+1|st, at)π(at|st)\n(2.10)\nIt is also convenient to define the marginals pπ(st) and pπ(st, at):\npπ,t(st) :=\nX\ns0:T \\{st}\nX\na0:T\npπ(τ) ,\npπ,t(st, at) :=\nX\ns0:T \\{st}\nX\na0:T \\{at}\npπ(τ)\n(2.11)\nwhich can be interpreted as the probability of the union over all trajectories containing St = st\n(or both St = st and At = at).\n10\n. . .\nFigure 2.1 Markov Chain over state-action pairs.\nThey can also be written as functions of one another using the product rule:\npπ,t(st, at) = P(St =st, At =at|π) = P(At =at|St =st, π)P(St =st|π) = π(at|st)pπ,t(st) (2.12)\nSupervised learning in MDPs\nDescribing MDPs sets the table for reasoning about\nsequential decision making. Before diving into reinforcement learning, it is important to\nnote that RL is not the only method for handling sequential decision making problems.\nFor example, assuming access to a dataset of N trajectories D = {τi}N\ni=1 solving the task,\nsupervised learning can be applied to learn a policy that captures this behavior. In particular,\nbehavioral cloning is the simplest such method. It consists of searching for the policy π that\nmaximizes the likelihood of D under the induced trajectory distribution pπ, reducing the\nproblem of learning a policy to a classification problem (for discrete control) or a regression\nproblem (for continuous control). From Equations 2.2 and 2.10, we have:\narg max\nπ\n1\nN\nN\nX\ni=1\nlog pπ(τi)\n(2.13)\n= arg max\nπ\n1\nN\nN\nX\ni=1\n\u0012\nlog P0(s0,i) +\nT−1\nX\nt=0\nlog P(st+1,i|st,i, at,i) +\nT−1\nX\nt=0\nlog π(at,i|st,i)\n\u0013\n(2.14)\n= arg max\nπ\n1\nN\nN\nX\ni=1\nT−1\nX\nt=0\nlog π(at,i|st,i)\n(2.15)\nThe terms associated with the environment dynamics are independent from π and can be\nremoved. We thus seek to maximize the probability of selecting the action ai associated\nwith the state si in the dataset. However, this method requires having access to a set of\ndemonstrations D, which limits its applicability to problems where the target behavior has\nalready been observed. Furthermore, due to its lack of exploration, the method famously\nsuffers from the problem of compounding error (Ross et al., 2011; Laskey et al., 2017). It\nwould be much more flexible to be able to define the task without having to demonstrate\n11\nit, in a way that lets the agent interact with the environment and find the best strategy\nautonomously.\nThe RL objective\nThe reinforcement learning paradigm is based on a drastically different\napproach for specifying the desired behavior. Instead of using labels ai for each state si, we\nassume that the environment also emits a bounded scalar reward rt = R(st, at) at each\ntime-step, using the reward function R : S × A →[rmin, rmax]. The reward can also be\nstochastic or depend on St+1, but for simplicity, we assume here the common case in which it\nis deterministic and depends only on (st, at). The sum of rewards r1 + r2 + · · · + rT is called\nthe return and the goal in reinforcement learning is to learn a policy that maximizes this\nsum in expectation. This objective is called the expected total reward (or expected return)\nand can be expressed both under the trajectory distribution or the state-action marginal:\nJR(π) := Eτ∼pπ\n\" T\nX\nt=0\nR(st, at)\n#\n=\nT\nX\nt=0\nE(st,at)∼pπ,t\nh\nR(st, at)\ni\n(2.16)\nAn optimal policy is then defined as the policy π∗that maximizes the expected total reward:\nπ∗= arg max\nπ\nJR(π)\n(2.17)\nThis objective is appropriate in the episodic case, where a trajectory is guaranteed to ter-\nminate after T time-steps. However, some problems involve an infinite-horizon for which\nT →∞. In such cases, to keep this infinite sum bounded, we usually introduce a discount\nfactor γ ∈[0, 1), and the RL objective becomes the expected total discounted reward:\nJR(π) := Eτ∼pπ\n\" ∞\nX\nt=0\nγtR(st, at)\n#\n=\n∞\nX\nt=0\nγtE(st,at)∼pπ,t\nh\nR(st, at)\ni\n(2.18)\nSince R(·) is bounded by rmax, we can see that JR(π) is bounded by a geometric series which\nevaluates to P∞\nt=0 γtrmax = rmax\n1−γ ∀γ ∈[0, 1). Note that in practice, a discount factor is often\nused even in the episodic case. In addition to keeping the sum bounded, it can be interpreted\nas a way to trade-off instantaneous and future rewards, with γ = 0 putting weight on the next\nreward only and γ →1 considering all rewards equally, or as the probability of transitioning\nto an absorbing state, after which the reward is null forever after. To gain some intuition\nabout the impact of a given value on the effective horizon of the agent, one can use the rule\nof thumb that the number of time-steps considered to compute the return is of the order of\n1\n1−γ, after which the remaining discounted rewards become very small (Tallec et al., 2019).\nFor example γ = 0.99 represents an effective horizon of about 100 time-steps.\n12\nVisitation distributions\nFinally, another useful distribution is called the state (or state-\naction) visitation distribution (also called the normalized occupancy measure), defined as:\ndπ(s) :=\n1\nZ(γ, T)\nT\nX\nt=0\nγtpπ,t(St = s) ,\ndπ(s, a) :=\n1\nZ(γ, T)\nT\nX\nt=0\nγtpπ,t(St = s, At = a)\n(2.19)\nwhere Z(γ, T) = PT\nt=0 γt is a normalizing constant e.g. Z(1, T) = T, Z(γ, ∞) =\n1\n1−γ. Much\nlike for the state and state-action marginals, these two distributions can also be written in\nterms of one another:\ndπ(s)π(a|s) = π(a|s)\nZ(γ, T)\nT\nX\nt=0\nγtpπ,t(St = s) =\n1\nZ(γ, T)\nT\nX\nt=0\nγtπ(a|s)pπ,t(St = s) = dπ(s, a)\n(2.20)\nThe expected total discounted reward can also be written as an expectation over the state-\naction visitation distribution. Starting from Equation 2.18, we can use the fact that the state\nand action space are the same along the trajectory to fully remove the dependency over t,\nyielding:\nJR(π) =\nT\nX\nt=0\nγt X\ns,a\npπ,t(s, a)R(s, a)\n=\nX\ns,a\nR(s, a)\nT\nX\nt=0\nγtpπ,t(s, a)\n|\n{z\n}\nZ(γ,T)dπ(s,a)\n= Z(γ, T)E(s,a)∼dπ[R(s, a)]\n(2.21)\n2.2.2\nThe Bellman Equations\nMany reinforcement learning algorithms estimate value functions to evaluate an agent’s policy\nand to improve it. Two types of value function are often used: the state value function vπ(s)\nand the state-action value function qπ(s, a). vπ evaluates how desirable it is for an agent to\nfind itself in state s whereas qπ evaluates how desirable it is to take action a when finding\nitself in state s (Sutton and Barto, 2018). More formally, for any state s ∈S, vπ(s) is defined\nas the expected total discounted reward assuming that we start in state St = s and then\nfollow π for the rest of the interaction. Similarly, for any state-action pair s, a ∈S × A,\nqπ(s, a) is defined as the expected total discounted reward assuming that we start in state\nSt = s, take action At = a, and then continue on by following π:\nvπ(s) := Epπ\n\" ∞\nX\nk=0\nγkrt+k\n\f\f\f\fst = s\n#\n,\nqπ(s, a) := Epπ\n\" ∞\nX\nk=0\nγkrt+k\n\f\f\f\fst = s, at = a\n#\n(2.22)\n13\nThe objective we seek to maximize, the expected return, can be expressed in terms of vπ and\nthe initial state distribution P0:\nJR(π) = Es∼P0\nh\nvπ(s)\ni\n(2.23)\nA very important property of value functions is that they can be written recursively. By\nexpanding the sum over the rewards P∞\nk=0 γkrt+k = rt + γ P∞\nk=0 γkrt+k+1 and pushing the\nexpectation to the right, we get:\nvπ(s) = Epπ\nh\nR(s, at) + γvπ(st+1)\ni\n,\nqπ(s, a) = R(s, a) + γEpπ\nh\nqπ(st+1, at+1)\ni\n(2.24)\nThe Equations 2.24 for vπ and qπ are the famous Bellman equations. A useful identity is to\nwrite them in terms of each other. Starting from their definition, one can again push the\nexpectations to uncover their mixed forms (vπ from qπ and vice-versa):\nvπ(s) = Epπ\nh\nqπ(st, at)|st = s\ni\n,\nqπ(s, a) = R(s, a) + γEpπ\nh\nvπ(st+1)\ni\n(2.25)\nThe advantage function is another useful quantity defined as the expected gain from taking\naction a in state s instead of following the policy:\naπ(s, a) = qπ(s, a) −vπ(s)\n(2.26)\n= R(s, a) + γEpπ\nh\nvπ(st+1)\ni\n−vπ(s)\n(2.27)\nWe can use value functions to define an ordering on policies (Sutton and Barto, 2018). We\nsay that a policy π′ is better than a policy π if vπ′(s) ≥vπ(s) ∀s ∈S and strictly superior\nfor at least one state s. An optimal policy π∗is a policy that is better than or equal to all\npolicies. The state and state-action value functions of an optimal policy are called optimal\nvalue functions v∗and q∗\nv∗(s) := max\nπ\nvπ(s) ,\nq∗(s, a) := max\nπ\nqπ(s, a)\n(2.28)\nNote that the optimal value functions can be written independently of any policy, only as a\nfunction of the optimal value function at the next state\nv∗(s) = max\na∈A(s) q∗(s, a)\n(2.29)\n= max\na∈A(s) Epπ\nh\nR(s, a) + γv∗(st+1)\ni\n(2.30)\n14\nq∗(s, a) = R(s, a) + γEpπ\nh\nv∗(st+1)\ni\n(2.31)\n= R(s, a) + γEpπ\nh\nmax\na′∈A(st+1) q∗(St+1, a′)\ni\n(2.32)\nThis relationship shows that an optimal policy can be very simply expressed as acting greedily\non the optimal value function. Since q∗(s, a) by definition already accounts for the long-term\neffect of taking action a in state s, a one-step look-ahead on q∗yields optimal behavior. While\ndefining an optimal policy is important, in most problems, computing the exact optimal\npolicy is prohibitively expensive. Luckily, the Policy Improvement Theorem tells us how the\ndefinition of optimal policy can be used to at least improve the policy that we currently\nhave (Sutton and Barto, 2018). Given an accurate estimate of the value function qπ, one can\nobtain an improved policy π′ by increasing in all states the probability of selecting the action\na∗that yields the highest value, that is, a∗(s) = arg maxa∈A(s) qπ(s, a) ∀s ∈S. If there is\nequality (multiple optimal actions), any partitioning of the probability mass among them will\nyield the same performance, as long as no probability mass is added to suboptimal actions.\nThe problem of estimating value functions is referred to as policy evaluation while the step\ninvolving modifying the policy is referred to as policy improvement. These two procedures\nare, in one form or another, at the core of most reinforcement learning algorithms.\n2.2.3\nTabular RL with known environment dynamics\nIn small environments for which state values vπ(s) can be enumerated in a table, several\nmethods have been developed to build accurate value estimate which cover the entire state\nspace by leveraging known environment dynamics.\nSolving the System of Equations\nGiven perfect knowledge of the dynamics of the environment P and for sufficiently small\nMDPs, one can directly solve the system of |S| equations and |S| unknowns (one for each\ns ∈S) given by the Bellman equation for vπ (Equation 2.24). This can be better seen in the\nvector-matrix form. By defining some arbitrary ordering 1, 2, ... , |S| over the states s ∈S,\nwe can define the vector of unknowns vπ where vπ\ni = vπ(si). We can also express the reward\nfunction in vector form and the environment-policy dynamics in matrix form by marginalizing\nout the effect of actions in the reward function and transition distribution, respectively:\nrπ(s) := Ea∼π(·|s)\nh\nR(s, a)\ni\n,\nP π(s′|s) := Ea∼π(·|s)\nh\nP(s′|s, a)\ni\n(2.33)\n15\nBy expanding the sum from the definition of the value function we can then obtain our\nsystem of Bellman equations:\nvπ :=\n∞\nX\nt=0\n(γP π)t rπ = rπ +\n∞\nX\nt=1\n(γP π)t rπ = rπ + γP π\n∞\nX\nt=0\n(γP π)t rπ = rπ + γP πvπ (2.34)\nFrom there, we can solve for vπ to get the unique solution of the policy evaluation problem:\nvπ = (I −γP π)−1rπ\n(2.35)\nLinear Programming\nThe problem of finding the optimal value function v∗can also be formulated as a Linear\nProgram (LP). Let a γ-superharmonic vector be any vector v satisfying\nv ≥rπ + γP πv\n(2.36)\nOne can show that the optimal value function v∗is the smallest such γ-superharmonic vector\n(Kallenberg, 2011). This result is at the root of the LP formulation. We want to minimize\nour value function candidate v as much as possible but such that it remains γ-superharmonic.\nUsing this constraint, v∗can be defined as the solution to the Linear Program:\nminimize\np⊤\n0 v\nsubject to\nv(s) −γ\nP\ns′∈S P(s′|s, a)v(s′)\n≥\nr(s, a),\n∀s, a ∈S × A\n(2.37)\nwhere p0 is the vectorized initial state distribution. Any LP solver can thus be used in order\nto recover the optimal value function by solving this constrained optimization problem. The\nLP formulation also highlights an interesting relationship between the value function vπ(s)\nand the (unormalized) state-action occupancy measure, since dπ(s, a) is in fact the solution\nto the dual problem of this Linear Program.\nDynamic Programming\nA very popular family of methods that scales better than the Linear Programming approach\n(but which is, however, still computationally expensive) are Dynamic Programming algo-\nrithms (Bellman, 1966; Rust, 2008). The main mechanism behind such methods is to use\nthe Bellman equation as an iterative update rule and evaluate it exactly using the known\n16\ntransition distribution P:\nvk+1(s) :=\nX\na∈A(s)\nπ(a|s)\n\nR(s, a) + γ\nX\ns′∈S\nP(s′|s, a)vk(s′)\n\n\n(2.38)\nOne can show that starting from any arbitrary estimate v0, the sequence {vk}k=1,2, ... converges\nto the true value function of the policy vπ as k →∞. The Policy Iteration algorithm uses\nsuch updates. Specifically, it alternates between policy evaluation and policy improvement\nsteps until the optimal value function v∗is recovered. During the policy evaluation step,\nwe keep updating the value of each state s ∈S until they converge to the true vπ. Then,\nthe policy improvement step (greedification) consists of greedily improving the deterministic\npolicy by selecting the best-performing action according to the expectation over the next\nstates, i.e. π(s) = arg maxa∈A(s) R(s, a) + Es′∈S[vπ(s′)].\nThe Value Iteration algorithm instead fuses these two steps together by using the Bellman\noptimality equation as an update rule:\nvk+1(s) := max\na∈A\n\nR(s, a) + γ\nX\ns′∈S\nP(s′|s, a)vk(s′)\n\n\n(2.39)\nwhich is equivalent to performing Policy Iteration but interrupting the policy evaluation\nstep after a single sweep over S instead of waiting until convergence to the true vπ at each\nstep. While a more accurate estimate of the value function means better knowledge of how to\nimprove the policy, when considering deterministic policies, only the ordinality of state-action\nvalues really matters, and we can speed up the algorithm by improving the policy based on\nthe correct action ordering without necessarily having the exact values of each action in hand.\nHence, the policy iteration and value iteration algorithms represent two extreme answers to\nan interesting question: since a more accurate estimate of the policy value does not always\nlead to a revised policy improvement step, how long should we keep updating the value\nestimates before modifying the policy? The former optimizes the value estimate as much as\npossible, while the latter performs a single update before moving on. For most problems,\nthe best trade-off is somewhere in the middle. Generalized Policy Iteration refers the whole\nspectrum of algorithms that put different levels of emphasis on the value function accuracy\nbefore switching to policy improvement.\n2.2.4\nModel-free RL from experience\nThe methods presented in the previous section are very restrictive in practice, as they require\ncomplete knowledge of the environment’s transition distribution. They also assume that we\n17\ncan query the reward function directly with any given state and action. While these quantities\nare known in some simple cases, such as board games, many setups allow only for sampling\nR(s, a) and P(s′|s, a) by interacting with the environment. One approach is to attempt to\nmodel these distributions using samples, an often challenging task that leads to a whole family\nnamed model-based RL algorithms. In this thesis, we focus on model-free RL algorithms, the\nalternative approach that seeks to directly solve the control problem from interaction samples\n(experience), without learning its dynamics.\nMonte Carlo methods\nA foundational family of algorithms is called Monte Carlo (MC) methods. They seek to\nestimate qπ(s, a) or vπ(s) directly from Equation 2.22 by collecting i.i.d.\nsample episodes\nand averaging complete returns (Sutton and Barto, 2018). By the law of large numbers, these\nunbiased estimates converge to the true values as the number of samples tends to infinity.\nAlthough restricted to the episodic case (T ∈N), MC methods can be particularly useful\nwhen trying to estimate the value of a subset of states only, as they allow each state value\nestimate to be completely independent (unlike bootstrap methods).\nAn important consideration that arises for control without a model is the need to maintain\nsufficient exploration throughout learning. Indeed, to perform policy improvement without\na state-transition model, one needs to evaluate qπ rather than vπ. However, when evaluating\nqπ(s, a) from interaction samples collected by a deterministic policy π, only one action will\nbe chosen for each state s ∈S, making it impossible to know whether a different action\nwould have led to higher returns. Two options are available to allow for exploration. The\nfirst option is to learn on-policy the value function of a stochastic policy, often carried out\nby implementing an ϵ-greedy policy which attributes every action a probability of\nϵ\n|A(s)| to\nbe selected, and the rest of the probability mass to the greedy action. The same principle\nof generalized policy iteration can be proven to lead to the optimal ϵ-greedy policy (Sutton\nand Barto, 2018) (which is, we hope, very close in performance to the optimal deterministic\npolicy). The second option is to learn off-policy the value function of a deterministic target\npolicy π using samples collected by a different behavior policy β. The use of importance\nsampling estimators allows us to correct for the fact that the trajectories are now sampled\nfrom pβ rather than from pπ. Indeed, to estimate the value vπ(s) under the target policy π,\nwe have:\nvπ(s) := Eτ∼pπ\n\"T−t\nX\nk=0\nγkrt+k\n\f\f\f\fst = s\n#\n= Eτ∼pβ\n\"pπ(τ)\npβ(τ)\nT−t\nX\nk=0\nγkrt+k\n\f\f\f\fst = s\n#\n(2.40)\n18\nTo compute the importance sampling weights\npπ(τ)\npβ(τ), we assume that the behavior policy\nβ(a|s) can be evaluated and is positive in all state-action pairs where the target policy is\npositive (assumption of coverage). Off-policy methods tend to converge more slowly as the\nimportance weights increase the variance of the value estimates, but are more general as they\ninclude the on-policy methods as a special case where β = π.\nTemporal Difference methods\nA second family of experience-based algorithms is called Temporal Difference (TD) methods.\nInstead of learning directly from the expected return definition of Equation 2.22 like in the\ncase of Monte Carlo methods, they use the recursive property of the expected return by\nusing an approximation of the Bellman equations of Equation 2.24 as their update rule. In\nparticular, TD(0) takes a one-sample estimate of the expectation in the Bellman equation\nfor vπ, and SARSA does the same using the Bellman equation for qπ. Both MC and TD\nmethods admit online implementations with the following update rules respectively:\nvπ\nk+1(s) = vπ\nk(s) + α\n\u0010\nˆqπ(s, a) −vπ\nk(s)\n\u0011\n(MC online update)\nvπ\nk+1(s) = vπ\nk(s) + α\n\u0010\nR(s, a) + γvπ\nk(s′) −vπ\nk(s)\n\u0011\n(TD(0) online update)\nwhere α is a step-size hyperparameter, a and s′ are the sampled action and next-state and\nˆqπ(s, a) is the discounted return collected from (s, a) until the end of the trajectory. It is\nclear from these equations that while MC methods use the true sampled return as a target\nto update their value estimate of a given state s, TD methods instead use a single sampled\nreward and then use the current estimate of the return at the next state vπ\nk(s′). The term in\nparentheses δ := R(s, a) + γvπ\nk(s′) −vπ\nk(s) is often referred to as the TD error and quantifies\nhow much the current estimate should change to respect the Bellman equation. Learning an\nestimate of the value at the current state from an estimate of the value at the next state is\nreferred to as bootstrapping, and while it might appear ambitious, TD(0) and SARSA have\nbeen shown to converge to the true value function assuming that all states (and actions) are\nvisited infinitely many times.\nThe most popular TD method for control is the Q-learning algorithm (Watkins and Dayan,\n1992). Like SARSA, it estimates the state-action values, however it instead implements the\nBellman optimality equation as its learning rule:\nqπ\nk+1(s, a) = qπ\nk(s, a) + α\n\u0010\nR(s, a) + γ max\na′∈A(s′) qπ\nk(s′, a′) −qπ\nk(s, a)\n\u0011\n(2.41)\nTherefore, instead of estimating the values of any given policy π, the Q-learning algorithm\n19\nlearns the state-action values of the greedy-policy defined over the current value estimates,\nand has been shown to converge to the optimal value function q∗assuming that all pairs\ncontinue to be updated. Because the Bellman optimality equation does not depend on any\nparticular policy, the Q-learning algorithm can be used for off-policy control as is, without the\nneed to use importance sampling weights, making it one of the most widely used reinforcement\nlearning algorithms to this day.\nTwo important points explain why experience-based algorithms scale so much better than dy-\nnamic programming algorithms. The first one is statistical. Dynamic programming methods\ncompute full expectations in their update rules (expected updates) which involves considering\nall successor states, whereas experience-based methods need only a single sample obtained\nthrough interaction (sampled updates).\nThe second is computational.\nExperience-based\nmethods inherently focus on improving their value estimates of regions of the state space\nthat are more visited, therefore avoiding lost computation on improving estimation of states\nthat the agent will never encounter.\nMethods based on finding a solution to the Bellman equations, called value-based methods,\nonly need to implicitly represent the policy through the learned value function. While these\ntechniques are useful for discrete control – problems for which the size of action space |A(s)|\nis finite for all states s ∈S – they are not well suited for continuous control tasks, in which\nthe action space is a continuous domain and consequently the number of possible actions is\ninfinite. One possibility for such cases is to discretize the action space by partitioning it into\nquantiles. However, this poses a precision vs. complexity trade-off; larger bins prohibit fine\ncontrol whereas a large number of bins becomes intractable as the number of possible actions\ngrows exponentially with the number of action dimensions.\nPolicy Gradient methods\nA better suited alternative to continuous control than value-based methods is to learn the\npolicy directly by parameterizing it separately from the value function, a family of algorithms\ncalled policy-based methods. As long as this policy πθ is differentiable w.r.t. its parameters θ,\nit can be trained to directly maximize the expected total discounted reward using a likelihood\nratio estimator of its gradient. This result is known as the Policy Gradient Theorem (Sut-\nton et al., 2000) and algorithms that take this approach are called policy gradient methods.\nStarting with JR(πθ) from Equation 2.18, we have:\n∇θ JR(πθ) := ∇θ Eτ∼pπθ\n\" T\nX\nt=0\nγtrt\n#\n(2.42)\n20\n=\nZ\nτ ∇θ pπθ(τ)\n\u0012 T\nX\nt=0\nγtrt\n\u0013\ndτ\n(2.43)\n=\nZ\nτ pπθ(τ) ∇θ log pπθ(τ)\n\u0012 T\nX\nt=0\nγtrt\n\u0013\ndτ\n(2.44)\n= Eτ∼pπθ\n\"\u0012\n∇θ log P0(s0) + ∇θ\nT\nX\nt=0\nlog P(st+1|st, at) + ∇θ\nT\nX\nt=0\nlog πθ(at|st)\n\u0013\u0012 T\nX\nt=0\nγtrt\n\u0013#\n(2.45)\n= Eτ∼pπθ\n\"\u0012 T\nX\nt=0\n∇θ log πθ(at|st)\n\u0013\u0012 T\nX\nt=0\nγtrt\n\u0013#\n(2.46)\nThis last expectation can be approximated using a Monte Carlo estimator to yield our policy\ngradient estimate using collected trajectories. We can then use it to update the parameters\nof our policy through gradient ascent, i.e. θ ←θ + α ∇θ JR(πθ) where α is the learning\nrate. The algorithm that uses this particular gradient estimate is known as REINFORCE\n(Williams, 1992). It uses the actual expected total discounted reward PT\nt=0 γtrt to weigh the\ngradient ∇θ log πθ(at|st) at each time-step, making it a Monte Carlo policy gradient method.\nOne advantage of using the actual discounted return is that this policy gradient estimate\ndoes not make use of the Markov assumption; we could use it even in the case of partial\nobservability where our policy is conditioned on some observation ot rather than the true\nstate of the environment st. However, it also means that this Monte Carlo estimate is only\nwell defined in the episodic case (an episode must terminate before learning can start) and\nthat the estimate generally has a high variance.\nTwo techniques are commonly used to reduce the variance of this gradient estimator. First,\nto weigh the gradient term ∇θ log πθ(at|st), we can omit all rewards that occurred before\ntime-step t and instead use the discounted return ˆqπ = PT\nt′=t γt′−trt′ collected from (st, at).\nThis comes from the fact that, by temporal causality, we have Rt ⊥A<t|(St, At), yielding a\nsimpler version of Equation 2.46:\n∇θ JR(πθ) = Eτ∼pπθ\n\" T\nX\nt=0\n∇θ log πθ(at|st)ˆqπ(st, at)\n#\n(2.47)\nSecond, we can also subtract a state-dependent baseline b(s) from this weighting term, yield-\ning:\n∇θ JR(πθ) = Eτ∼pπθ\n\" T\nX\nt=0\n∇θ log πθ(at|st)\n\u0010\nˆqπ(st, at) −b(st)\n\u0011#\n(2.48)\nA typical choice is to use an estimate of the state value function vπθ as baseline. By developing\nthe term with b(st), one can show that the use of a baseline does not bias the gradient in\n21\nexpectation:\nT\nX\nt=0\nE(st,at)∼pπθ\n\u0014\n∇θ log πθ(at|st)b(st)\n\u0015\n=\nT\nX\nt=0\nEst∼pπθ\n\nb(st) ∇θ\nX\na∈A(st)\nπθ(a|st)\n\n= 0\n(2.49)\nFinally, most policy gradient algorithms do not use the actual return ˆqπθ to weigh the gradient,\nbut instead use a parameterized estimate qϕ of the true state-action value function qπθ. While\nthe collected return ˆqπθ is already a one-sample estimate of the expected state-action value, it\nis computed online for each episode. At the price of introducing a bias in the gradient estimate\n(Sutton et al., 2000), using a parameterized estimate qϕ allows to evaluate the return at any\ntime-step without requiring us to wait for the episode to terminate before updating. When\ncombined with vϕ(st) = Eat∼πθ[qϕ(st, at)] as baseline, we get the policy gradient computed by\nthe Advantage Actor-Critic (A2C) algorithm, which weighs the gradient with the advantage\nfunction aϕ(st, at)\n∇θ JR(πθ) = Eτ∼pπθ\n\" T\nX\nt=0\n∇θ log πθ(at|st)aϕ(st, at)\n#\n(2.50)\nFinally, note that the policy gradient is on-policy because the expectation is taken w.r.t. the\ntrajectory distribution pπθ induced by the current policy πθ. Computing the gradient using\ntrajectories collected by a different (or past) policy would yield a biased gradient estimate.\nIn such cases, an unbiased but higher variance estimate can be derived using importance\nsampling (Degris et al., 2012).\n2.3\nDeep Reinforcement Learning\nIn the last section, we reviewed foundational RL algorithms which are designed to operate on\nMDPs with discrete state space S that contain a small enough number of states to be stored\nin a table. Such tasks are useful for testing algorithms and developing the theory. However,\nmost real-life problems involve a number of possible states that is so large that they would\nnot fit in any computer’s memory. For such cases, function approximators must be used to\nrepresent policies and value functions over these gigantic spaces using a compact number of\nparameters. Today, neural networks are the most commonly used function approximators\nin RL due to their representation power and their ability to be trained efficiently using\ngradient-based methods (see Section 2.1). Such a combination of deep learning and RL is\noften referred to as deep Reinforcement Learning (deep RL) algorithms.\n22\n2.3.1\nDeep Q-Learning\nNeural networks had already been used in the 1990s in successful applications of reinforcement\nlearning with function approximation (Tesauro, 1994; Lin, 1993). In the last decade however,\nthe DQN1 algorithm (Mnih et al., 2013, 2015) stood out by tackling the challenging task of\nlearning to play Atari arcade games directly from raw pixel images (Bellemare et al., 2013),\nthus successfully applying reinforcement learning to a much higher-dimensional input space.\nAt its core, the DQN algorithm essentially consists of training a deep neural network param-\neterized by ϕ that takes a state s ∈S as input and maps it to the q-value estimate Qϕ(s, a) of\neach action a ∈A(s). This model is trained using the Q-Learning algorithm presented in Sec-\ntion 2.2.4 by backpropagating through each layer to correct for the TD error. However, the\ncombined use of bootstrapping, off-policy learning, and function approximators (sometimes\ncalled the deadly triad (Sutton, 2015)) is known to destabilize RL algorithms. The authors\nof DQN alleviate this issue by making two main changes to the Q-learning algorithm.\nThe first and most important is the use of experience replay. For each interaction with the\nenvironment, a transition (s, a, r, s′) is collected and stored in a replay buffer D. After a\nfixed number of interactions, a minibatch of transitions is uniformly sampled from the buffer\nand used to compute the stochastic gradient update. Experience replay allows to improve\ndata efficiency as the collected data can be re-used several times and because sampling\nacross the entire buffer effectively decorrelates the samples used for computing the updates\nas opposed to using consecutive transitions. It also allows the model to maintain its accuracy\nin estimating the value of long-past states and actions and to avoid oscillations due to drastic\nchanges in the collected data distribution after a parameter update (Mnih et al., 2013). The\nsecond modification is the use of target networks in the Q-learning update (Mnih et al.,\n2015). Denoted Q¯ϕ, the target network is a copy of the main deep Q-network Qϕ that is\nused to compute the value of the next state and actions (s′, a′) of the target y = R(s, a) +\nγ maxa′∈A(s′) Q¯ϕ(s′, a′). The interest lies in the fact that Q¯ϕ always lags behind Qϕ, making\nthe target relatively constant for a few updates of the parameters ϕ. This is achieved either\nby hard updates ¯ϕ ←ϕ after a fixed number of parameter steps, or more frequently using\nsoft updates of the form ¯ϕ ←ϵϕ + (1 −ϵ)¯ϕ with ϵ ∈[0, 1].\nFinally, several follow-up works have provided improvements to the original DQN algorithm.\nHessel et al. (2018) provides a detailed evaluation of some of these. Among them, Double-\nDQN (DDQN) (Van Hasselt et al., 2016) proposes to reduce the overestimation bias in Q-\nlearning by decoupling the action selection and the state-action evaluation when computing\n1Although the algorithm is called Deep Q-Learning and DQN only stands for Deep Q-Networks, members\nof the community often use the DQN acronym to refer to the algorithm as a whole.\n23\nthe TD target, yielding y = R(s, a) + γQ¯ϕ\n\u0012\ns′, arg maxa′∈A(s′) Qϕ(s′, a′)\n\u0013\nwhere Q¯ϕ is the\ntarget network presented above. Prioritized Experience Replay (Schaul et al., 2015b) aims\nto sample with higher probability transitions from which the agent can learn the most by\nassigning a sampling probability pi to each transition (s, a, r, s′)i proportionally to its TD\nerror (pi ∝|δi| + ϵ) and uses weighted importance sampling (Mahmood et al., 2014) to\ncorrect for the introduced bias. Dueling networks (Wang et al., 2016) propose a different\nnetwork architecture that represents the state value estimate and the advantage estimate\nseparately before recombining them into Q-values to allow all action values to quickly benefit\nfrom an updated state estimate. Distributional approaches to deep RL (Bellemare et al.,\n2017; Dabney et al., 2018) aim to learn the approximate value distributions of states and\nactions rather than their expected value alone, allowing more stable learning and an explicit\nspecification of risk aversion within agents.\n2.3.2\nDeep Deterministic Policy Gradients\nValue-based methods are well suited for discrete control because a policy is implicitly defined\nby taking the arg max on the Q-values. In continuous action spaces, this operation comes\ndown to an optimization over A(s) at every time-step, which is generally computationally\nprohibitive. We must use policy gradient methods instead.\nOne option is to represent the policy using an analytic continuous distribution (e.g., Gaussian)\nand to learn a mapping from the input state to the parameters of that distribution (e.g., mean\nand variance) using the (stochastic) policy gradient presented in Section 2.2.4.\nAnother\napproach is to use the deterministic policy gradient (DPG) formulation that is specifically\nderived for continuous control (Silver et al., 2014).\nIntuitively, the DPG moves the policy parameters in the direction that maximizes the action\nvalue function Qϕ when averaged over all states. The key element on which the deterministic\npolicy gradient is derived lies in the fact that because the action space is continuous, and\nassuming that the Q-function is parameterized using a differentiable function approximator\nϕ, we can backpropagate the signal from Qϕ through the selected action a to compute the\nderivative of JR w.r.t. the parameters θ of the continuous policy µθ:\n∇θ JR(µθ) ≈Epβ[∇θ Qϕ(s, a)|a=µθ(s)]\n(2.51)\n= Epβ[∇θ µθ(s) ∇a Qϕ(s, a)|a=µθ(s)]\n(2.52)\nThe critic Qϕ is learned using the Q-Learning algorithm described above. But rather than\ncomputing the explicit arg max over actions for the action selection of the TD target, the\n24\nnext action a′ is computed using the target policy µ¯θ, which is trained to maximize the\ncritic’s action value. In this sense, the DPG algorithm can be seen as an approximate Q-\nlearning algorithm that uses a parameterized approximate action maximizer to handle large\nor continuous action spaces. Importantly, because it eliminates the integral over actions, the\ndeterministic policy gradient does not require importance sampling correction ratios when\nevaluating it using a distinct (stochastic) behavioral policy β(a|s) (Silver et al., 2014), making\nit an off-policy policy gradient algorithm.\nLillicrap et al. (2015) essentially extend DPG with the same techniques used by the DQN\nalgorithm (Mnih et al., 2015) (replay buffer, target networks, gradient clipping) to improve\nstability when used with nonlinear function approximators. The original work also uses batch\nnormalization (Ioffe and Szegedy, 2015) to eliminate scale differences between state variables\nand consequently reduce the need to adjust the hyperparameter for every environment. They\ncall this algorithm Deep Deterministic Policy Gradient (DDPG).\nTD3 (Twin Delayed DDPG) (Fujimoto et al., 2018) further improves on DDPG by introducing\nthree modifications that greatly improve performance. First, inspired by the success of Double\nDQN (DDQN), the authors empirically show that the overestimation bias also affects actor-\ncritic methods. However, contrary to DDQN, their results suggest that in this framework, the\ntarget Q-network is too dependent on the main critic to correct for overestimation. Instead,\nthey propose to train two instances of the critic Qϕ1 and Qϕ2 (both of which also have a\ncorresponding target network Q¯ϕ1 and Q¯ϕ2) and to use the smallest Q-value for their target\ny = R(s, a) + γ mini=1,2 Q¯ϕi(s′, a′). Second, they recommend updating the policy µθ at a\nlower frequency than value networks to allow to obtain better value estimates before taking\na policy improvement step. Third, they propose to perturb the target action using random\nnoise a′ = µ¯θ(s′) + ϵ , ϵ ∼clip(N\n\u0010\n0, σ), −c, c\n\u0011\nto smooth out the value function along the\naction dimensions and allow bootstrapping from similar state-action value estimates.\n2.3.3\nMaximum Entropy Reinforcement Learning\nAlthough the optimal value function for a finite MDP is unique, there might exist several\noptimal deterministic policies. In principle, these optimal deterministic policies could be\ncombined into a single stochastic policy that captures many different modes of optimal be-\nhavior.\nIn general, there are several reasons why one would prefer learning a stochastic\npolicy. For example, problems with partial observability might only allow for a stochastic\noptimal policy. Stochastic policies might also be more robust to adapt to a sudden change\nin the environment. Finally, they allow for a smooth exploration mechanism, as opposed\nto ϵ-greedy policies. The Maximum Entropy framework for reinforcement learning (MaxEnt\n25\nRL) (Ziebart, 2010) aims at learning a policy that maximizes both the expected discounted\nreturn and the expected discounted entropy of the policy:\nJMaxEnt(π) :=\n∞\nX\nt=0\nγtE(st,at)∼pπ\n\u0014\nR(st, at) + αH\n\u0010\nπ(·|st)\n\u0011\u0015\n(2.53)\nwith α ≥0 and where α →0 recovers the original RL objective (Equation 2.18). A similar\napproach which uses entropy maximization to prevent an early collapse of the policy (i.e.,\nmaintain exploration) is sometimes used with policy gradient approaches (O’Donoghue et al.,\n2016). Crucially, in the case of MaxEnt RL (Equation 2.53), the entropy term is found inside\nthe expectation in the main objective rather than as a regularization term on the policy\nupdates, which will push the policy not only to maximize its entropy in any given state,\nbut to seek and navigate to states in which high entropy is aligned with high return, thus\nmaximizing the entropy of the entire trajectory.\nThe MaxEnt RL objective allows for a smooth-equivalent of the Bellman Equations often\nreferred to as the soft-Bellman Equations (Haarnoja et al., 2017):\nv∗\nsoft(s) = α log\nX\na∈A(s)\nexp\n\u0012 1\nαq∗\nsoft(s, a)\n\u0013\n(2.54)\nq∗\nsoft(s, a) = R(s, a) + γEpπ[v∗\nsoft(st+1)]\n(2.55)\nwhere the max operator over actions for v∗has essentially been replaced by a smooth-max\noperator (log-sum-exp) which approaches a hard-max as α →0. The optimal policy w.r.t.\nthe MaxEnt RL objective is proportional to the exponential of the soft q-values and vsoft can\nbe seen as the partition function:\nπ∗\nMaxEnt(a|s) = exp\n\u0012 1\nα\n\u0010\nq∗\nsoft(s, a) −v∗\nsoft(s)\n\u0011\u0013\n=\nexp\n\u0010\n1\nαq∗\nsoft(s, a)\n\u0011\nP\na′∈A(s) exp\n\u0010\n1\nαq∗\nsoft(s, a′)\n\u0011\n(2.56)\nwhere q∗\nsoft(s, a) −v∗\nsoft(s) is also referred to as the soft-advantage function. This optimal\npolicy puts equal probability mass on two actions yielding the same expected return and\nexponentially less mass as the advantage decreases.\nIn the discrete control setting, we see from Equation 2.56 that the optimal policy can simply\nbe represented by a softmax over the optimal soft q-function. Thus, we can learn a policy\nby parameterizing the soft q-function only using a function approximator Qsoft\nϕ (s, a) and\nevaluating V soft\nϕ\n(s) exactly using Equation 2.54 to produce a one-sample estimate of the target\nfrom Equation 2.55. This model can then be learned by minimizing the error in a DQN-like\n26\nfashion, an approach that can be referred to as Soft Q-Learning (Haarnoja et al., 2017). In\nthe continuous control case, the summation of actions in Equations 2.54 and 2.56 turn into\nintegrals and one cannot simply represent the policy using a q-function only. In this case, the\nSoft Actor-Critic (SAC) algorithm (Haarnoja et al., 2018) can be used. Using this approach,\nthe policy πθ, the soft q-function Qsoft\nϕ\nand the soft value function V soft\nψ\nare parameterized\nseparately. Qsoft\nϕ\nand V soft\nψ\nare trained by minimizing Bellman residuals, whereas πθ is trained\nto maximize a DDPG-like policy gradient using the reparameterization trick. Note that since\nthe trade-off between maximizing entropy and return depends on the scale of the reward\nfunction, Haarnoja et al. (2018) also propose an entropy temperature adjustment method for\nlearning α automatically in order to avoid having to tune this hyperparameter when applying\nthe algorithm to a different task.\n2.3.4\nGenerative Flow Networks\nGenerative Flow Networks (GFlowNets, GFNs) are a class of generative models originally\ndesigned for compositional object generation (Bengio et al., 2021). In this setting, the gen-\nerated objects are assembled step by step by taking actions corresponding to adding new\nelements to the current state. Although closely linked to energy-based models and Monte\nCarlo Markov Chain sampling methods (Bengio et al., 2023), the framework learns to model\na distribution from a reward function, which also positions it as a suitable approach for some\nreinforcement learning problems.\nGFlowNets generally operate on a particular subclass of MDP defined by some key charac-\nteristics. First, the action space is discrete, allowing for a finite set of actions in each state.\nSecond, the state-space forms a Directed Acyclic Graph (DAG), meaning that all states must\nalways eventually lead to a terminal state, without the possibility for any loop, and the tran-\nsition function is deterministic (i.e., each action at leads to only one successor state st+1).\nFinally, the reward function is positive in terminal states R(sT) > 0 and null on all other\nstates R(s) = 0. Note that several tasks of interest such as molecular generation (Bengio\net al., 2021), causal discovery (Deleu et al., 2022) and sequence generation (Jain et al., 2022a)\npresent this type of MDP.\nThe central property of GFlowNets lies in the terminal state distribution that characterises\nthe target policy. Here, the model seeks to learn a policy π such that the probability that a\ntrajectory ends in a particular terminal state sT is proportional to its reward:\nπ∗\nGFN\n:\npπ∗\nGFN(sT) ∝R(sT)\n(2.57)\n27\nSimilarly to MaxEnt RL, this behavior is desirable as it allows to capture all of the modes\ndefined by the reward function rather than uncovering a single high-performing behavior\nusing traditional RL methods.\nHowever GFlowNets operate from a different framework.\nHere, we model the problem as learning a flow of probability particles that operates in the\nstate space S. The flow starts from a unique initial state s0 and spreads itself across the\ntransitions s →s′ until reaching a terminal state sT. It is constrained by border conditions\nwhich state that the amount of flow to a terminal state must equal its reward R(sT), and\nthat the total flow Z in the network (departing from s0) must equal the sum of rewards\nZ := P\nsT R(sT). Enforcing these constraints alongside the principle of conservation of flow\nthroughout the network allows to recover a policy that samples terminal states proportionally\nto their reward.\nSeveral objectives have been shown to be sufficient to meet these conditions (Madan et al.,\n2023). The most common is the Trajectory-Balance objective (Malkin et al., 2022a), which\nstates that any given trajectory τ should yield the same probability when going forward as\nwhen going backward in the MDP which introduces the corresponding forward and backward\npolicies πF and πB. Recalling the MDP is deterministic, from Equation 2.10 we have:\nP0(s0)pπF (τ|s0) = P(sT)pπB(τ|sT)\n(2.58)\n⇔\nT−1\nY\nt=0\nπF(st+1|st) = R(sT)\nZ\nT−1\nY\nt=0\nπB(st|st+1)\n(2.59)\nwith P0(s0) = 1. Concretely, both πF and πB can be parameterized by neural networks while\nZθ can be kept as a free parameter, and this equation can be turned into a loss function LTB\nusing a squared log-ratio:\nLTB(θ) :=\n \nlog\nZθ\nQT−1\nt=0 πF\nθ (st|st+1)\nR(sT)\nQT−1\nt=0 πB\nθ (st+1|st)\n!2\n(2.60)\nAs in MaxEnt RL, the result is a stochastic policy which seeks to capture all modes of the\nreward function. However, both methods lead to different behaviors as the likelihood of sam-\npling each mode is not the same. MaxEnt RL converges to a policy which samples trajectories\nproportionally to their exponential return along the entire path, whereas GFlowNets samples\ntrajectories proportionally to their final reward. These behaviors are equivalent when the\nstate space is represented as a tree (i.e., there is only one path leading to any terminal state)\nbut differ in general DAGs where multiple paths may lead to the same terminal state. In\nthis case, the MaxEnt RL objective of maximizing the diversity of trajectories will favor final\nstates sT that can be reached from many paths, while a GFlowNet will sample terminal states\n28\nstrictly based on their terminal rewards, thus enforcing a diversity of outcomes. Whether\ndiversity of trajectories or diversity of outcomes is preferable will depend on the context.\nFor example, a policy for character control in a video game might benefit from trajectory\ndiversity to embody all possible styles of locomotion. In contrast, in a drug discovery ap-\nplication where only the quality of the finished molecule matters, the diversity of outcomes\nwould be preferred. All in all, GFlowNets add another tool to a growing collection of deep\nreinforcement learning methods for complex sequential decision making in the real world.\n29\nCHAPTER 3\nLITERATURE REVIEW\nThe reward function is a central component of reinforcement learning algorithms. It defines\nthe task to be solved in a given MDP. Designing a reward function is traditionally a manual\nand iterative process, guided by the user’s intuition and refined through trial and error (Knox\net al., 2023; Booth et al., 2023; Hayes et al., 2022). Since each iteration requires training\nan RL agent to convergence, this process is slow and costly, posing a major challenge for\nRL deployment in real-world applications.\nNumerous studies have focused on the issue\nof reward specification, creating a range of paradigms aimed at guiding exploration and\nensuring alignment through advanced design techniques for the reward function.\nIn this\nchapter, we review the most established areas of reinforcement learning that address this\nchallenge. We group these methods into two distinct categories. First, in Section 3.1, we\npresent Reward Composition approaches, which take account of the multi-faceted nature of\nthe reward function and provide it as a set of reward components to the learning algorithm.\nThen, in Section 3.2, we cover Reward Modeling methods, which leverage various supervision\nsignals to learn the reward function instead of explicitly specifying it.\n3.1\nReward Composition\nThe Reward Hypothesis, posited by Sutton and Barto (2018), states that any goals that we\nwish an agent to accomplish can be thought of as the maximization of the expected total\nreward. In other words, a sufficiently complex scalar reward function could, in principle,\nbe used to specify any conceivable task. While the exactitude of this hypothesis relies on\ncareful theoretical assumptions (Bowling et al., 2023), the large number of RL benchmark\nenvironments developed for research in the last decade (Todorov et al., 2012; Bellemare et al.,\n2013; Brockman et al., 2016; Juliani et al., 2018; Dosovitskiy et al., 2017; Vinyals et al., 2017;\nWydmuch et al., 2018) and the numerous applications of RL in the real world (Evans and\nGao, 2016; Yu et al., 2019; Bellemare et al., 2020; Shahidinejad and Ghobaei-Arani, 2020)\nrepresent strong evidence that rewards can indeed capture a very wide variety of tasks.\nNevertheless, despite their scalar nature, most reward functions usually encompass a number\nof distinct elements that are simply weighted against each other before being aggregated into\na single number. For example, a self-driving car should aim to reach its destination while\nrespecting traffic laws and avoiding obstacles. A grasping robot should learn to move objects\naround while minimizing its energy consumption and without damaging its environment.\n30\nMulti-Objective RL\nAuxiliary Tasks\nPareto front\nReward Shaping\nshared layers\nFigure 3.1 Overview of reward composition strategies. Reward composition consists in inte-\ngrating different components into a reward function. (Left) Potential-based reward shaping\naims at hinting the agent toward its goal by augmenting its sparse reward function with a\ndenser signal defined from a potential function Φ on the state space. (Middle) Auxiliary\ntasks (here L1 and L2) are optimised simultaneously with the main objective JR to help the\nagent learn a useful representation of the input x. (Right) Multi-objective RL treats each\nreward component as a distinct objective and uses various techniques to select for different\nsolutions on the Pareto front.\nReward composition here refers to the idea of assembling a reward function from multiple\ndistinct components. In addition to making the agent’s behavior more interpretable (Ander-\nson et al., 2019), a component-centric treatment of the reward function allows the utilization\nof these components for various purposes, such as aiding exploration, enhancing alignment,\nor balancing diverse goals.\nIn the next sections, we survey prior work that frame these\ncomponents either as shaping rewards, auxiliary tasks, or competing objectives.\n3.1.1\nPotential-Based Reward Shaping\nThe challenge of specifying good reward functions is as old as reinforcement learning. One of\nits central dilemmas is that of sparse vs. dense rewards. Sparse rewards are often natural to\nspecify but difficult to learn from. For example, we could reward an agent only when reaching\nits final destination in a navigation task. However, when learning from a sparse reward only,\nan agent needs to explore its environment in the total absence of feedback until it happens\nto stumble on the solution. Only then can it learn from this experience and reinforce the\nsuccessful behavior. A denser reward signal could potentially help guide the agent towards\nthe solution. For example, we could give a smaller reward when the agent navigates closer\nto its destination. While this approach can dramatically improve the sample efficiency of the\nalgorithm, a naive implementation of such breadcrumbs can also lead to degenerate solutions\nand reward hacking by exploiting cycles in the reward function (Amodei et al., 2016). For\n31\nexample, the agent could learn to run in circles to repeatedly get rewarded for “making\nprogress” without ever actually reaching the goal (Randløv and Alstrøm, 1998).\nPotential-Based Reward Shaping (PBRS) (Ng et al., 1999) is a reward composition strategy\nallowing to augment a sparse reward function R with a denser signal Rshaping without changing\nthe set of optimal policies. It eliminates the risk of reward cycles by restricting the form of\nthe shaping reward to the discounted difference between the potential of the next state Φ(s′)\nand that of the current state Φ(s):\nR′ := R + Rshaping ,\nRshaping := γΦ(s′) −Φ(s) ,\narg max\nπ∈Π\nJR′(π) = arg max\nπ∈Π\nJR(π) (3.1)\nThe potential function Φ can be thought of as a topographic map guiding the agent towards\nhigher peaks of the reward function (see Figure 3.1). Since it does not depend on actions, the\ncumulative discounted return for Rshaping is independent of π. Consequently, Rshaping does\nnot affect the ordering of the Q-values for R and the set of optimal policies under JR, but\nallows the agent to reach the critical regions of the MDP with fewer exploration steps (Laud\nand DeJong, 2003). Empirically, this approach has been shown to greatly accelerate learning\nand is still being used in challenging high-dimensional environments (Berner et al., 2019).\nIn subsequent work, Wiewiora et al. (2003) extend potential-based shaping to potential func-\ntions of both states and actions Φ(s, a) and shows that using it for shaping reward is equiva-\nlent to initializing the Q-values of the main reward R to this potential. Devlin and Kudenko\n(2012) instead augment potential functions with a time dependence Φ(s, t) to allow dynamic\nadaptation of the shaping reward. Harutyunyan et al. (2015) proposes to use the value func-\ntion of any arbitrary shaping signal as a potential over states to be used for potential-based\nreward shaping, allowing the PBRS framework to be extended to a larger family of shaping\nfunctions. Finally, Hu et al. (2020) cast dynamic reward shaping as a bilevel optimization\nproblem in which they automatically learn a weighting coefficient to reduce the impact of\nharmful reward shaping once detected.\n3.1.2\nAuxiliary Tasks in RL\nReinforcement learning is a very general approach to artificial intelligence. Silver et al. (2021)\neven argue that the act of maximizing a reward signal by trial and error in a rich environment\ncould be sufficient to develop any attribute of intelligence that is required to solve a particular\ntask. However, this process can be highly inefficient due to the sparsity of the reward function\nand the informational complexity of the environment (Yu, 2018). Although potential-based\nreward shaping offers a principled solution to the problem of reward sparsity, it is limited to\n32\nspecific functional forms (Harutyunyan et al., 2015). A common alternative consists of using\nauxiliary tasks to leverage additional learning signal from the environment. An auxiliary\ntask often involves predicting quantities that are useful to complete the main control task.\nOne or even several such tasks can be optimized simultaneously with the RL objective JR by\nscalarizing them using fixed weighting coefficients λk for each of the K auxiliary losses Lk:\nJtot(π) := JR(π) −\nK\nX\nk=1\nλkLk(π)\n(3.2)\nThis is typically implemented by training a single neural network model sharing the first layers\nacross all tasks while being equipped with different heads for the policy and the auxiliary\ntask predictions (see Figure 3.1). The shared encoding layers thus benefit from the learning\nsignal of both objective types, allowing to perform informative updates even in the absence of\nexternal reward. Contrary to model-based approaches (Moerland et al., 2023), which seek to\nlearn the transition distribution of the environment to perform planning, auxiliary tasks are\nonly applicable with deep parameterisations and seek to improve the representations learned\nby an agent (Vincent et al., 2008), or to accelerate the optimization process by helping avoid\nlarge portions of the policy space (Gupta et al., 2022).\nOne of the first uses of auxiliary tasks is presented in the work of Suddarth and Kergosien\n(1990), which used so-called hints to accelerate the learning of neural networks trained to\nsolve simple logical problems.\nIn reinforcement learning, Sutton et al. (2011) introduces\ngeneralized value functions (GVFs), which extend the concept of state-action value function\nto measuring different properties of a policy π beyond its return JR(π) on the main task.\nThe idea has since been applied more generally to deep reinforcement learning in a variety of\ncontexts, often achieving important improvements in learning speed and performance. For\nexample, in addition to its main task, Jaderberg et al. (2016) train an agent to correctly\npredict incoming rewards and maximize perceptual changes in its environment. Shelhamer\net al. (2016) and Laskin et al. (2020) use self-supervised successive-states prediction and\ncontrastive losses to learn useful representations for the policy and value function. Mirowski\net al. (2016) use depth and loop-closure prediction as auxiliary objectives for navigation tasks.\nLample and Chaplot (2017) task the agent to predict the presence of enemies or weapons\nwhen learning to play a video game. Kartal et al. (2019) train the agent to predict whether\nit is close to the end of the episode. Fedus et al. (2019) predict the return for multiple time\nhorizons. Hernandez-Leal et al. (2019a) use action prediction as an auxiliary task in multi-\nagent settings. Song et al. (2021) perform velocity estimation to improve the representations\nlearned by a policy controlling mobile indoor robots.\n33\nInterestingly, auxiliary tasks seem to provide benefits that exceed the information contained\nin the target of such task. For example, Mirowski et al. (2016) compare predicting depth as\nan auxiliary task with simply providing the agent with a depth-map as additional input, and\nreport significantly improved performance with the former approach, supporting the wider\neffect of representation learning provided by the act of solving these tasks, as opposed to being\nprovided with their answer. A growing body of work focuses on uncovering the mechanisms\nby which auxiliary tasks are so effective in improving sample efficiency in RL, investigating\ntheir role in preventing overfitting (Dabney et al., 2021) and representation collapse (Lyle\net al., 2021).\nFinally, while scalarization and parameter sharing are often used to integrate auxiliary tasks\nin the training pipeline, alternative approaches have been developed to address the challenges\nof competing gradient updates and learning instabilities (Teh et al., 2017; Yu et al., 2020;\nRosenbaum et al., 2017). In particular, some works investigate the automatic adaptation of\ntask coefficients to automatically detect and tune down the influence of auxiliary tasks that\nwould become harmful to the main objective (Du et al., 2018; Lin et al., 2019a).\n3.1.3\nMulti-Objective RL\nAs seen previously, some problems such as playing chess can be broken down into a main\nsparse objective, e.g. winning the game, and denser rewards which are often correlated with\nthe main objective and meant to guide the agent towards successful policies, e.g. capturing\nopponent pieces. However, for other tasks, the target behavior encapsulates truly distinct or\neven conflicting objectives which are equally important and should not be freely traded-off by\nthe agent (Vamplew et al., 2022). For example, avoiding obstacles in a navigation task is not\nenforced merely to guide the agent towards reaching its goal; both preserving the integrity\nof its surroundings and reaching its target location are nonnegotiable criteria for a successful\npolicy. The question of how to best handle multiple objectives has been identified as one of\nthe main limitations preventing RL from being applied in more real world domains (Dulac-\nArnold et al., 2021), and multi-objective RL (MORL) algorithms are designed specifically to\naddress this question.\nAt their core, multi-objective approaches treat each reward component as a distinct criterion.\nThey are formalized as Multi-Objective MDPs (MOMDPs) (Roijers et al., 2013), defined by\nthe tuple (P, P0, S, A, {R}K\nk=1, γ). For the most part, they are identical to their regular MDP\ncounterpart, defined in Section 2.2.1. However, instead of a single reward function, we now\nhave a set of K reward functions {Rk}K\nk=1 that map a state-action pair to a vector of rewards.\nMOMDPs thus generalize regular MDPs since K = 1 brings us back to the single-objective\n34\ncase. Each component is defined as Rk : S × A →[rk-min, rk-max] and JRk(π) represents the\nexpected total discounted reward for the reward component k as defined in Equation 2.18.\nThe problem now becomes:\narg max\nπ∈Π\n(JR1, . . . , JRK)\n(3.3)\nImportantly, with K > 1, the notion of optimality is not directly transferable to the multi-\nobjective case. In single-reward MDPs, the RL objective induces a total order over policies.\nHowever, with multiple objectives, a policy π′ may perform better than π on JR1 but worse\non JR2, thus requiring a different criterion to compare potential solutions. A policy π′ is said\nto dominate π if it is superior on at least one objective and at least equal on the others:\nπ′ ≻π\n⇔\n\n\n\n\n\n∀k :\nJRk(π′) ≥JRk(π)\n∃k∗:\nJRk∗(π′) > JRk∗(π)\n(3.4)\nA solution π is said to be Pareto-optimal if there are no other solution that dominates it, and\nthe set of Pareto-optimal solutions forms the Pareto front in objective space (see Figure 3.1).\nIn most problems, some of the objectives will conflict and an optimal solution π∗which\ndominates all other policies will not exist; we will have to settle for a solution that strikes an\nacceptable trade-off between the objectives. MORL algorithms can generally be categorized\ninto single-policy and multi-policy methods (Vamplew et al., 2011) depending on whether\nthey seek to generate a single policy or an approximation of the entire Pareto front.\nSingle-policy approaches seek to find the policy that best captures the desired trade-off.\nThey typically assume that the utility function u of the user is known and use it to com-\nbine the objectives and recover a total order over the policies in Π (Hayes et al., 2022). A\nsimple and popular approach to utility-based MORL is to extend existing RL algorithms\nby learning a set of value functions {Qπ\nk}K\nk=1 and adapting the action selection process by\ntaking the action that maximizes utility. Aissani et al. (2008) employ this technique with the\nSARSA algorithm and a linear utility function that leads to an action selection of the form:\narg maxa\nP\nk Qπ\nk(s, a). To capture preferences on some of the objectives, linear utility func-\ntions can employ different weighting coefficients {wk}K\nk=1with\nP\nk wk = 1 (Castelletti et al.,\n2002). However, the effective coverage of this approach is limited to only the convex parts of\nthe Pareto fronts (Das and Dennis, 1997; Vamplew et al., 2008). To overcome this limitation,\nother methods employ nonlinear utility functions (Van Moffaert et al., 2013a,b), but nonlin-\near scalarization is difficult to combine with value-based RL algorithms, as they break the\nadditivity property required for Q-decomposition (Russell and Zimdars, 2003). Therefore,\nthese approaches have been extended to policy-gradient actor-critic architectures (Siddique\n35\net al., 2020; Reymond et al., 2023).\nIn the absence of a known utility function, other methods recover a total order over the\npolicies in Π by assuming a hierarchy of priority among the objectives to optimize (Gábor\net al., 1998). A lexicographic ordering implies that the ordinality of the objectives now reflects\ntheir rank in importance. In the same spirit as Asimov’s famous “laws of robotics”, this means\nthat an optimal policy π∗should maximize JR1 in priority, then JR2, then JR3, and so on. Let\nus define Π∗\nk the set of optimal policies w.r.t. JR1 through JRk. A lexicographically optimal\npolicy is defined as a policy that maximizes each objective in lexicographic order without\nworsening the previous ones:\nπ∗:= arg max\nπ∈Π∗\nK−1\nJRK(π)\n,\nΠ∗\nk :=\n(\nπ : JRk(π) = max\nπ′∈Π∗\nk−1\nJRk(π′)\n)\n,\nΠ∗\n0 := Π\n(3.5)\nwhere the recursion occurs in the constraint established by the optimal policy-sets Π∗\nk. Since\nthis hierarchy severely restricts the set of remaining optimal policies at each step, a strong\nlexicographic ordering would not scale to a large number of conflicting objectives. Instead,\nit is common to relax this ordering by introducing slack variables to specify how much we\ncan deviate from J∗\nRk to improve JRk+1 (Wray et al., 2015; Skalse et al., 2022; Pineda et al.,\n2015). This method has been applied in various contexts, including autonomous driving (Li\nand Czarnecki, 2018) and multi-agent RL (Hayes et al., 2020).\nAnother related single-policy approach called Constrained Reinforcement Learning (CRL)\npicks one of the objectives as the main reward to optimize and defines a set of constraints\nusing the other reward functions. While several types of constraints have been used (proba-\nbilistic, instantaneous) (Liu et al., 2021), the most common approach is to define a cumulative\nconstraint on each objective JRk for k > 1 by specifying thresholds dk. Formally, the problem\nbecomes:\nπ∗:= arg max\nπ∈Π\nJR1(π)\ns.t.\nJRk(π) ≥dk , k = 2, . . . , K\n(3.6)\nA policy that satisfies the constraint set is said to be a feasible policy, and we seek to\nfind the best-performing feasible policy π ∈ΠC over JR1.\nNote that, while equivalent,\nthe constraints are typically re-labeled as cost functions Ck and their threshold revsersed\ni.e. JCk(π) ≤dk (Altman, 1999). A popular approach to solving such problems is to use a\nLagrangian relaxation (Bertsekas, 1997) to fold the constraints and JR1 into a single objective\naddressed by a bilevel optimization process (Tessler et al., 2018; Chow et al., 2017; Liang\net al., 2018; Stooke et al., 2020; Bohez et al., 2019). Other alternatives based on trust-regions\n(Achiam et al., 2017; Yang et al., 2020), Lyapunov functions (Chow et al., 2018, 2019) and\n36\nthe interior-point method (Liu et al., 2020) have also been proposed for Safe RL applications\nin which respecting the constraints is important throughout the entire search.\nIn general, the choice of approach will depend on which formulation best captures the true\npreferences of the user. Some problems are naturally formulated with a hierarchy of im-\nportance over objectives, while for others, a distinction between objectives to maximize and\nconstraints to satisfy makes the most sense. Finally, when the user is unsure about their own\npreferences, multi-policy methods can be employed. These approaches focus on generating a\nvariety of solutions striking different trade-offs on the Pareto front to allow the user to select\none of these options later on (Hayes et al., 2022). Inner-loop methods aim at maintaining a\nset of nondominated policies to learn about all Pareto-optimal solutions simultaneously (Bar-\nrett and Narayanan, 2008; Iima and Kuroe, 2014; Van Moffaert and Nowé, 2014; Reymond\nand Nowé, 2019; Li et al., 2020). In contrast, outer-loop methods generate multiple solutions\nby simply running single-policy algorithms in sequence while varying the parameters of the\nutility function, the ordering of the objectives, or the thresholds of the constraints (Parisi\net al., 2014; Mossalam et al., 2016; Xu et al., 2020). Some methods also condition the policy\nwith these preferences to have access to all the learned policies and share their representa-\ntion in a single model (Abels et al., 2019). Ultimately, multi-policy approaches remain more\ncomputationally costly but offer additional flexibility in terms of the proposed solutions.\n3.2\nReward Modeling\nDespite the variety of tools and approaches presented in Section 3.1 that help practitioners\ndefine safe and efficient reward functions for RL, many take the position that, for complex\ntasks and environments, manually specified reward functions are doomed to be incomplete\nand underspecified (Ibarz et al., 2018; Hadfield-Menell et al., 2017; Leike et al., 2018). Instead,\nthey advocate for learning the reward function from human supervision, a radically different\nstrategy which can be referred to as reward modeling.\nThe main benefit of such approaches is that they enable a definition of the task that is not\nsubject to arbitrary scaling choices for the numerical value of the reward, the shaping signal,\nand the other components describing the desired behavior. Instead, reward modeling seeks\nto leverage different forms of human input to model a parameterized reward function rψ,\nwhich will then lead a downstream RL algorithm to learn the intended behavior (Jeon et al.,\n2020). In the next sections, we review the literature on reward modeling from three types\nof supervision signal: expert demonstrations, human preferences, and natural language (see\nFigure 3.2).\n37\nExample\nA\nB\nInverse RL\nPreference-based RL\nLanguage-guided RL\nFigure 3.2 Overview of reward modeling strategies. Reward modeling consists in learning\na model of the reward function from human supervision in an attempt to capture the true\nintentions of the task designer.\n(Left) In Inverse RL, a model of the reward function is\nlearned from expert demonstrations. (Middle) In preference-based RL, the reward model is\nlearned from human preferences over pairs of examples. (Right) In language-guided RL, a\nreward model can be trained to predict whether an agent’s behavior is in accordance with a\nlanguage command.\n3.2.1\nInverse RL\nReinforcement learning (RL) aims to learn a behavior policy from a reward function. Con-\nversely, inverse reinforcement learning (IRL) aims to learn a reward function from a demon-\nstrated behavior. More specifically, starting from a finite set of N expert trajectories DE :=\n{τ (i)}N\ni=1, the goal is to learn a parameterized reward function rψ for which the expert is\nuniquely optimal, and then to use it to train an agent to behave like the expert by running\na reinforcement learning algorithm on this learned reward function.\nThe IRL problem was first introduced by Russell (1998). An algorithm for this problem\ngenerally follows this general procedure: starting from a first estimate of the true reward\nfunction, we need to iteratively (1) solve for a policy π which is optimal under Jrψ and (2)\nmodify our reward estimate rψ to minimize the distance between the learned policy’s behav-\nior and expert behavior inferred from the demonstration set DE. This paradigm presents\ntwo important challenges. First, to obtain π from our current estimate rψ, one needs to solve\na complete reinforcement learning problem which must be repeated at each step of the IRL\nalgorithm. Second, there are generally a large number of reward functions that could ex-\nplain the demonstrated behavior (including degenerate solutions such as rψ(s, a) = 0 ∀s, a).\nFoundational works in IRL handle the ambiguity of the solution set in different ways, and\nmost can be categorized either as maximum margin methods, maximum entropy methods,\nand Bayesian approaches (Adams et al., 2022; Arora and Doshi, 2021).\n38\nMaximum-margin methods aim at solving the ambiguity problem by maximizing a margin\nwhich describes how well the learned reward rψ explains the demonstrated behavior compared\nto any other policy (Ng et al., 2000; Ratliff et al., 2009; Silver et al., 2008; Ratliff et al., 2006).\nIn other words, it seeks to make the expert as uniquely optimal as possible. An example of\nsuch margin could be, for each state, the difference between the value of the action from the\ndemonstration set a∗and any other action a:\narg max\nrψ\nX\ns∈S\n\u0012\nˆQ(s, a∗) −\nmax\na∈A(s)\\{a∗}\nˆQ(s, a)\n\u0013\n(3.7)\nApprenticeship learning (Abbeel and Ng, 2004, 2005) is a particularly influential method for\nmaximum margin optimisation which seeks to match the feature expectation of the demon-\nstrated behavior.\nAnother approach to tackle the ambiguity of the rewards is the maximum entropy IRL\nframework (Ziebart et al., 2008), which states that the probability of a trajectory should be\nproportional to its cumulative rewards:\nP(τ|rψ) ∝exp\n\nX\n(s,a)∈τ\nrψ(s, a)\n\n\n(3.8)\nIn other words, trajectories generating the same return under rψ should be equally likely\nunder π, whereas a trajectory with higher return should be exponentially more likely, with\nthe trajectories from the demonstration set DE be the most probable of all. The goal here is\nto obtain a policy that acts as randomly as possible, while maximizing the reward estimate\nrψ so that the solution commits as little as possible to any one possible reward function. To\nachieve this property, the learned reward function is often extended with an entropy term\nwhen training the policy π. Entropy over trajectories is often used, but extensions of this\nframework include the use of causal entropy (Ziebart, 2010) or relative entropy with a baseline\npolicy (Boularias et al., 2011).\nFinally, bayesian IRL (Ramachandran and Amir, 2007) consists in capturing the distribution\nover all possible candidate reward functions which explain the expert behavior:\nP(rψ|τ) ∝P(τ|rψ)P(rψ)\n,\nτ ∼DE\n(3.9)\nWhile allowing to handle the reward ambiguity issue in a principled way, the prior distribu-\ntion P(rψ) must be carefully selected based on the expected properties of the true reward\nfunction (Arora and Doshi, 2021). Different parameterisations have been explored for like-\nlihood P(τ|rψ), such as Boltzmann distributions (Choi and Kim, 2011), which require com-\n39\nputationally expensive MCMC sampling methods to estimate the normalization constant or\nGaussian processes (Levine et al., 2011) that limit the expressivity of the model by depending\non engineered features.\nThe reliance of early methods on linear combinations of predefined reward features or on\nsimple nonlinear parameterisations prevented their applicability to higher-dimensional prob-\nlems. More recent approaches (Wulfmeier et al., 2015; Finn et al., 2016b) leverage deep\nfunction approximators to model complex nonlinear relations between the state features and\nthe reward model. They also circumvent the need to fully optimize the policy in the inner\nloop of the reward optimization by swapping the two optimisation loops or by considering\npartial updates of the policy for each reward iteration. In particular, Adversarial Inverse\nReinforcement Learning (AIRL) (Fu et al., 2017) take inspiration from both the adversarial\nimitation learning framework from Ho and Ermon (2016) and the potential-based reward\nshaping from Ng et al. (1999) to learn robust reward functions from expert demonstrations.\nThe idea is to train a discriminator Dψ to classify whether a given transition has been gen-\nerated by the agent policy πθ or the expert policy πE, while parameterizing it so that the\nlearned reward implements a potential function hω over states:\nmin\nθ\nmax\nψ\nEdπE\nh\nlog Dψ(s, a, s′)\ni\n+ Edπθ\nh\nlog(1 −Dψ(s, a, s′))\ni\n(3.10)\nwith\nDψ(s, a, s′) :=\nexp\n\u0010\nfψ(s, a, s′)\n\u0011\nexp\n\u0010\nfψ(s, a, s′)\n\u0011\n+ πθ(a|s)\n(3.11)\nand\nfψ(s, a, s′) := rψ(s, a)\n|\n{z\n}\nmain reward\n+ γhω(s′) −hω(s)\n|\n{z\n}\nshaping reward\n(3.12)\nThese approaches seek to allow for more expressive reward modeling capabilities and im-\nprove the generalisation properties of these algorithms by providing negative examples to the\nlearned policy. Ghasemipour et al. (2020) provides a divergence-based classification of recent\nprogress in this field.\n3.2.2\nPreference-based RL\nPreference-based RL (PbRL) is a paradigm for learning a policy from non-numerical feedback\nusing reinforcement learning (Wirth et al., 2017). The idea of learning a reward function from\nqualitative feedback stems from the observation that it is often easier to judge the quality of\na solution than to produce it ourselves; a concept that has been fundamental to the field of\ncomputational complexity (Fortnow, 2009). Indeed, since expert demonstrations are expected\nto be optimal, providing them may be expensive and requires the human supervisor to have\n40\ngreat proficiency at that task. Instead, preference-based RL takes a different approach by\nasking a human to simply compare different outcomes and using this ranking as a supervision\nsignal, thus providing feedback to the agent without having to fully specify a reward function\n(regular RL) or to produce a complete set of demonstrations (inverse RL).\nThe simplest approach for collecting human preferences consists of presenting the examiner\nwith a set (or even a single pair) of example trajectories and asking them to provide a ranking\nfrom the least adequate to the best example. Despite its simplicity, qualitative feedback in\nthe form of rankings has been found to be sufficient to specify a variety of control problems.\nTheoretical analysis of task specification in RL even pinpoint the very generic idea of “goals”\nas a “a binary preference relation expressing preference over one outcome over another”\n(Bowling et al., 2023), suggesting that enumerating preferences between pairs of candidates\nin the set of all possible solutions may be sufficient to specify any task (Abel et al., 2021).\nThe relationship τi ≻τj indicates that the trajectory τi is strictly preferred over τj. In PbRL,\nthe goal is to find a policy π∗that maximizes the difference in likelihood for trajectories τi, τj\nfor all the preferences i, j available:\nτi ≻τj\n⇒\nπ∗= arg max\nπ∈Π\npπ(τi) −pπ(τj)\n(3.13)\nThe use of rankings as a learning signal in reinforcement learning dates back to Cheng et al.\n(2011) and Akrour et al. (2011), who, respectively, propose algorithms that exploit the signal\nfrom user preferences over actions or trajectories to derive improved policies. This approach\nhas been the subject of growing interest with several follow-up works (Wirth et al., 2017) and\nhas since been scaled up in the deep RL framework by Christiano et al. (2017). They propose\nto train a reward model rψ parameterized by a neural network ψ from pair comparisons of\npartial trajectories and to use it to learn a policy πθ. Crucially, reward modeling, policy\noptimization and preference rating all run in parallel in an asynchronous fashion. The policy\nis randomly initialized and starts collecting trajectories τ.\nThese trajectories are broken\ninto trajectory fragments σ and sent to a human evaluator in pairs (σi, σj). The evaluator\nindicates whether a fragment is better, that they are equal, or refuses to include them in the\ndatabase. The preference label y := I(σi ≻σj) is recorded (or y = 1\n2 if they are equal), and\nthis relationship is leveraged to train rψ by assuming that the preference grows exponentially\nwith the sum of rewards to form a probabilistic model of human preferences:\nPψ(σi ≻σj) :=\nexp\n\u0010P\n(st,at)∈σi rψ(st, at)\n\u0011\nexp\n\u0010P\n(st,at)∈σi rψ(st, at)\n\u0011\n+ exp\n\u0010P\n(st,at)∈σj rψ(st, at)\n\u0011\n(3.14)\n41\nIt can then be optimized by minimizing the Binary Cross-Entropy loss (BCE) between the\npredicted likelihood of the preference ordering and the true preference label (David, 1963):\nBCE(Pψ, y) = −y log Pψ(σi ≻σj) −(1 −y) log Pψ(σi ≺σj)\n∀\n(i, j, y) ∈D\n(3.15)\nThe policy πθ can then be trained to maximize the return over rψ using any deep RL al-\ngorithm. To prioritize the order in which trajectories should be queried for evaluation, the\nauthors train an ensemble of reward models and query the trajectory-fragment pair which\nshows the highest variance between to maximally reduce the uncertainty across the ensemble.\nThe results show that preference-based RL can scale to complex problems without having\naccess to the true reward function, and that it can even outperform learning from the true re-\nward function in some cases where human evaluations lead to a better shaped reward model\nrψ than the true reward function R. Several extensions of this work have been explored,\nfor example, to enable off-policy learning by relabeling past experiences when the reward\nmodel is updated (Lee et al., 2021) or to assess the benefits of modeling the dynamics of the\nenvironment alongside preference-based policy iteration (Liu et al., 2023).\nPreference-based learning can also be used on a fixed set of demonstrations. Brown et al.\n(2019a) uses human rankings in the context of imitation learning where the demonstrations\nmay be suboptimal. They require the dataset to provide a total order over the trajectories,\nwhich implies a number of constraints that grows quadratically in the number of samples\nand allows learning the reward function entirely off-line. In cases where rankings are not\nprovided but the demonstrations are deemed optimal, Reddy et al. (2019) use a method\nwhich considers all the trajectories in the demonstration set as being of equal value and\npreferred to any other generated trajectory, which can be interpreted either as inferring\npreferences from a set of demonstrations or as performing IRL with a discretized reward. Jain\net al. (2013) experiment with a more subtle way of combining the information from human\ndemonstrations and preferences by offering the evaluator the option to provide a trajectory\nthat merely improves upon the last demonstration rather than providing a near-optimal\nexample. Finally, demonstrations and preferences can be used in combination to improve the\nsample efficiency of preference-based approaches while outperforming the demonstrations\n(Ibarz et al., 2018; Palan et al., 2019).\nMore recently, with the advent of Large Language Models (LLMs), preference-based rein-\nforcement learning (also known as RL from Human Feedback, RLHF) has experienced a\nsurge in popularity in an effort to improve the behavior of large pre-trained models (Fer-\nnandes et al., 2023; Ziegler et al., 2019; Stiennon et al., 2020; Jaques et al., 2019a; Kreutzer\net al., 2018; Bai et al., 2022; Askell et al., 2021). In particular, Ouyang et al. (2022) exper-\n42\niment with fine-tuning LLMs using RLHF and show that incorporating human preferences\nallows to obtain better aligned models even with a fraction of the original capacity. Glaese\net al. (2022) break down human preferences into several distinct aspects of desirable behav-\nior to make better use of the provided feedback. These methods showcase the remarkable\nflexibility of preference-based reward specification, which allows to capture the essence of\nvery nuanced and personalized objectives such as insuring that personal assistants provide\nharmless, accurate and useful advice.\n3.2.3\nLanguage-guided RL\nFinally, natural language could represent a source of supervision which is less demanding\nthan complete demonstrations, but more sample efficient than preference rankings. Humans\nuse natural language extensively to give feedback to each other, and recent advances in\nnatural language processing (Treviso et al., 2023) open up the opportunity for leveraging\nthe flexibility of language instructions for task specification in RL. This approach to reward\nspecification is particularly appealing because language naturally captures object relations\nand elements of compositionality of the agent’s environment. Moreover, it provides a user\ninterface which does not require a technical background to specify objectives to or modify the\nbehavior of an agent. This source of supervision could be particularly useful in areas where\ndata efficiency is required or where human priors can be helpful (Luketina et al., 2019).\nMost approaches for language-guided RL aim to learn a dense reward function RNLP indicat-\ning the partial completion of a task and use it to augment the external reward function in a\nway similar to reward shaping (Section 3.1.1) and auxiliary tasks (Section 3.1.2):\nR′ = R + λRNLP\n(3.16)\nEarly attempts to use language for task specification used an object-oriented definition of the\nenvironment to specify an agent’s reward function (MacGlashan et al., 2017; Arumugam et al.,\n2017; Williams et al., 2018; Chevalier-Boisvert et al., 2018). These methods are constrained\nto a certain set of pre-defined concepts and object relations, limiting their scalability to the\ncomplexity of the target behavior and the training environment. Goyal et al. (2019, 2021)\npropose a more flexible language-to-reward model which takes as input the embeddings of\nboth a language command and a trajectory in the environment and estimates whether they\nare related. Others instead learn a mapping from language commands to state embeddings\nto specify goals through natural language (Kaplan et al., 2017; Waytowich et al., 2019). Fu\net al. (2019) learn a language-conditional reward function from demonstrations in the aim\nto be reused for different tasks. Sumers et al. (2021) propose to use sentiment inference on\n43\nthe provided commands to avoid the need to collect an explicit dataset. Bahdanau et al.\n(2018) take inspiration from IRL and use an adversarial loss to learn a reward function that\nconnects commands and goals.\nMore recently, advances in Large Language Models (LLMs) (Zhao et al., 2023) have enabled\nimportant steps towards scalable language-based reward specification. Kwon et al. (2023)\nexplore the idea of prompting a language model with the desired behavior and then using it\nto provide a numerical reward to the agent in a text-based negotiation game. Klissarov et al.\n(2023) use LLMs to provide preferences over pairs of events in a captioned environment to\nguide the agent towards interesting states. Another approach is to use LLMs to automatically\nbreak down a language description of a high-level task into a set of low-level subgoals which\ncan be executed by an underlying language-conditioned control algorithm (Huang et al.,\n2022; Ahn et al., 2022). Finally, a more end-to-end perspective consists in using LLMs and\niterative prompting to translate a language description of the desired behavior into a code\nimplementation of the reward function, which can then be optimized by any RL algorithm (Yu\net al., 2023; Ma et al., 2023). These successes demonstrate that the vast amount of knowledge\nabout the world distilled in LLMs has the potential to help bridge the gap between human\nintentions and effective reward specifications.\n44\nCHAPTER 4\nPREAMBLE TO TECHNICAL CONTRIBUTIONS\nChapters 5 to 8 present four original contributions related to the field of reward specifica-\ntion in deep reinforcement learning.\nRespectively, we present methods that make use of\ninverse reinforcement learning (article 1), auxiliary tasks (article 2), and multi-objective RL\n(articles 3 and 4) which are intended to support policy learning both in terms of efficiency\nand alignment. In all of these works, I (Julien Roy) made significant contributions and was\ndeeply involved in designing the algorithms, surveying the relevant literature, carrying out\nand analyzing experiments, and writing the manuscript. Here is a brief overview of each\narticle.\nArticle 1\nis titled Adversarial Soft Advantage Fitting: Imitation Learning without Policy\nOptimization. It presents improvements in adversarial imitation learning. Imitation learning\ntakes a supervised learning approach to sequential decision making. The agent is provided\nwith a set of demonstrations that are deemed optimal and seeks to learn a policy that\nemulates this behavior. We propose a novel architecture for adversarial IL which allows to\nsignificantly simplify the implementation and accelerate the training of these methods. In\ncombination with RL, efficient IL algorithms could be used to aid exploration and further\nground the behavior using a small set of demonstrations.\nArticle 2\nis titled Promoting Coordination through Policy Regularization in Multi-Agent\nDeep Reinforcement Learning. It focuses on the use of auxiliary tasks in RL. We investigate\nthe case of multi-agent cooperative tasks and propose different auxiliary objectives which are\noptimized simultaneously with the main task to allow the agents to discover effective coop-\nerative behaviors faster. Our approach is an example of how intuitions about the properties\nof effective strategies can be incorporated in the optimization process and serve as useful\ninductive biases for RL agents.\nArticle 3\nis titled Direct Behavior Specification via Constrained Reinforcement Learning.\nIt presents a general framework to easily incorporate hard constraints on the agent behavior.\nThis framework separates the main task that the agent is asked to perform from additional\nrequirements that it should abide to, and allows the system designer to specify all of these\nconstraints in a foreseeable and intuitive way. The resulting paradigm allows to efficiently\n45\ndesign new tasks, maintain a clear monitoring on the ability of the agent to meet these\nconstraints, and insure that the final behavior is aligned with the designer’s intentions.\nArticle 4\nis titled Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular\nDesign. It presents an application of goal-conditioned reinforcement learning to the problem\nof multi-objective molecular design for computer-based drug discovery. We formulate the\ngoals as subregions of the objective space and train a discrete generative model to target\nspecific trade-offs on the Pareto front in order to widen its solution coverage. Our method\nallows for a controllable generative process that can then be leveraged to explore the molecular\nspace in an intentional manner.\n46\nCHAPTER 5\nARTICLE 1: ADVERSARIAL SOFT ADVANTAGE FITTING:\nIMITATION LEARNING WITHOUT POLICY OPTIMISATION\nCo-authors\nPaul Barde, Wonseok Jeon, Joelle Pineau, Christopher Pal & Derek Nowrouzezahrai\nPublished in\nAdvances in Neural Information Processing Systems, December 12, 2020\nAbstract\nAdversarial Imitation Learning alternates between learning a discriminator – which\ntells apart expert’s demonstrations from generated ones – and a generator’s policy to\nproduce trajectories that can fool this discriminator. This alternated optimization\nis known to be delicate in practice since it compounds unstable adversarial training\nwith brittle and sample-inefficient reinforcement learning. We propose to remove the\nburden of the policy optimization steps by leveraging a novel discriminator formu-\nlation. Specifically, our discriminator is explicitly conditioned on two policies: the\none from the previous generator’s iteration and a learnable policy. When optimized,\nthis discriminator directly learns the optimal generator’s policy. Consequently, our\ndiscriminator’s update solves the generator’s optimization problem for free: learning\na policy that imitates the expert does not require an additional optimization loop.\nThis formulation effectively cuts by half the implementation and computational bur-\nden of Adversarial Imitation Learning algorithms by removing the Reinforcement\nLearning phase altogether. We show on a variety of tasks that our simpler approach\nis competitive to prevalent Imitation Learning methods.\n5.1\nIntroduction\nImitation Learning (IL) treats the task of learning a policy from a set of expert demonstra-\ntions. IL is effective on control problems that are challenging for traditional Reinforcement\nLearning (RL) methods, either due to reward function design challenges or the inherent dif-\nficulty of the task itself (Abbeel and Ng, 2004; Ross et al., 2011). Most IL work can be\ndivided into two branches: Behavioral Cloning and Inverse Reinforcement Learning. Behav-\nioral Cloning casts IL as a supervised learning objective and seeks to imitate the expert’s\nactions using the provided demonstrations as a fixed dataset (Pomerleau, 1991). Thus, Be-\nhavioral Cloning usually requires a lot of expert data and results in agents that struggle to\n47\ngeneralize. As an agent deviates from the demonstrated behaviors – straying outside the\nstate distribution on which it was trained – the risks of making additional errors increase, a\nproblem known as compounding error (Ross et al., 2011).\nInverse Reinforcement Learning aims to reduce compounding error by learning a reward\nfunction under which the expert policy is optimal (Abbeel and Ng, 2004). Once learned,\nan agent can be trained (with any RL algorithm) to learn how to act at any given state of\nthe environment. Early methods were prohibitively expensive on large environments because\nthey required training the RL agent to convergence at each learning step of the reward\nfunction (Ziebart et al., 2008; Abbeel and Ng, 2004).\nRecent approaches instead apply\nan adversarial formulation (Adversarial Imitation Learning, AIL) in which a discriminator\nlearns to distinguish between expert and agent behaviors to learn the reward optimized by\nthe expert. AIL methods allow for the use of function approximators and can in practice\nbe used with only a few policy improvement steps for each discriminator update (Ho and\nErmon, 2016; Fu et al., 2017; Finn et al., 2016a).\nWhile these advances have allowed Imitation Learning to tackle bigger and more complex\nenvironments (Kuefler et al., 2017; Ding et al., 2019), they have also significantly complexified\nthe implementation and learning dynamics of Imitation Learning algorithms. It is worth\nasking how much of this complexity is actually mandated. For example, in recent work,\nReddy et al. (2019) have shown that competitive performance can be obtained by hard-\ncoding a very simple reward function to incentivize expert-like behaviors and manage to\nimitate it through off-policy direct RL. Reddy et al. (2019) therefore remove the reward\nlearning component of AIL and focus on the RL loop, yielding a regularized version of\nBehavioral Cloning. Motivated by these results, we also seek to simplify the AIL framework\nbut following the opposite direction: keeping the reward learning module and removing the\npolicy improvement loop.\nWe propose a simpler yet competitive AIL framework. Motivated by Finn et al. (2016a) who\nuse the optimal discriminator form, we propose a structured discriminator that estimates the\nprobability of demonstrated and generated behavior using a single parameterized maximum\nentropy policy. Discriminator learning and policy learning therefore occur simultaneously,\nrendering seamless generator updates: once the discriminator has been trained for a few\nepochs, we simply use its policy model to generate new rollouts.\nWe call this approach\nAdversarial Soft Advantage Fitting (ASAF).\n48\nWe make the following contributions:\n• Algorithmic: we present a novel algorithm (ASAF) designed to imitate expert demon-\nstrations without any Reinforcement Learning step.\n• Theoretical: we show that our method retrieves the expert policy when trained to\noptimality.\n• Empirical: we show that ASAF outperforms prevalent IL algorithms on a variety of\ndiscrete and continuous control tasks. We also show that, in practice, ASAF can be\neasily modified to account for different trajectory lengths.\n5.2\nBackground\nMarkov Decision Processes (MDPs)\nWe use Hazan et al. (2018)’s notation and con-\nsider the classic T-horizon γ-discounted MDP M = ⟨S, A, P, P0, γ, r, T⟩. For simplicity,\nwe assume that S and A are finite.\nSuccessor states are given by the transition distri-\nbution P(s′|s, a) ∈[0, 1], and the initial state s0 is drawn from P0(s) ∈[0, 1].\nTransi-\ntions are rewarded with r(s, a) ∈R with r being bounded. The discount factor and the\nepisode horizon are γ ∈[0, 1] and T ∈N ∪{∞}, where T < ∞for γ = 1. Finally, we\nconsider stationary stochastic policies π ∈Π : S × A →]0, 1[ that produce trajectories\nτ = (s0, a0, s1, a1, ..., sT−1, aT−1, sT) when executed on M.\nThe probability of trajectory τ under policy π is Pπ(τ) ≜P0(s0) QT−1\nt=0 π(at|st)P(st+1|st, at)\nand the corresponding marginals are defined as dt,π(s) ≜P\nτ:st=s Pπ(τ) and dt,π(s, a) ≜\nP\nτ:st=s,at=a Pπ(τ) = dt,π(s)π(a|s), respectively. With these marginals, we define the normal-\nized discounted state and state-action occupancy measures as dπ(s) ≜\n1\nZ(γ,T)\nPT−1\nt=0 γtdt,π(s)\nand dπ(s, a) ≜\n1\nZ(γ,T)\nPT−1\nt=0 γtdt,π(s, a) = dπ(s)π(a|s) where the partition function Z(γ, T) is\nequal to PT−1\nt=0 γt. Intuitively, the state (or state-action) occupancy measure can be inter-\npreted as the discounted visitation distribution of the states (or state-action pairs) that the\nagent encounters when navigating with policy π. The expected sum of discounted rewards\ncan be expressed in term of the occupancy measures as follows:\nJπ[r(s, a)] ≜Eτ∼Pπ\nhPT−1\nt=0 γt r(st, at)\ni\n= Z(γ, T) E(s,a)∼dπ[r(s, a)]\nIn the entropy-regularized Reinforcement Learning framework (Haarnoja et al., 2018), the\noptimal policy maximizes its entropy at each visited state in addition to the standard RL\nobjective:\n49\nπ∗≜arg max\nπ\nJπ[r(s, a) + αH(π(·|s))] ,\nH(π(·|s)) = Ea∼π(·|s)[−log(π(a|s))]\nAs shown in (Ziebart, 2010; Haarnoja et al., 2017) the corresponding optimal policy is:\nπ∗\nsoft(a|s) = exp\n\u0010\nα−1 A∗\nsoft(s, a)\n\u0011\nwith\nA∗\nsoft(s, a) ≜Q∗\nsoft(s, a) −V ∗\nsoft(s)\n(5.1)\nV ∗\nsoft(s) = α log\nX\na∈A\nexp\n\u0010\nα−1 Q∗\nsoft(s, a)\n\u0011\n, Q∗\nsoft(s, a) = r(s, a) + γEs′∼P(·|s,a) [V ∗\nsoft(s′)]\n(5.2)\nMaximum Causal Entropy Inverse Reinforcement Learning\nIn the problem of In-\nverse Reinforcement Learning (IRL), it is assumed that the MDP’s reward function is un-\nknown but that demonstrations from using expert’s policy πE are provided. Maximum causal\nentropy IRL (Ziebart et al., 2008) proposes to fit a reward function r from a set R of reward\nfunctions and retrieve the corresponding optimal policy by solving the optimization problem:\nmin\nr∈R\n\u0012\nmax\nπ\nJπ[r(s, a) + H(π(·|s))]\n\u0013\n−JπE [r(s, a)]\n(5.3)\nIn brief, the problem reduces to finding a reward function r for which the expert policy is\noptimal. In order to do so, the optimization procedure searches high entropy policies that are\noptimal with respect to r and minimizes the difference between their returns and the return\nof the expert policy, eventually reaching a policy π that approaches πE. Most of the proposed\nsolutions (Abbeel and Ng, 2004; Ziebart, 2010; Ho and Ermon, 2016) transpose IRL to the\nproblem of distribution matching; Abbeel and Ng (2004) and Ziebart et al. (2008) used linear\nfunction approximation and proposed to match the feature expectation; Ho and Ermon (2016)\nproposed to cast Equation 5.3 with a convex reward function regularizer into the problem of\nminimizing the Jensen-Shannon divergence between the state-action occupancy measures:\nmin\nπ DJS(dπ, dπE ) −Jπ[H(π(·|s))]\n(5.4)\nConnections between Generative Adversarial Networks (GANs) and IRL\nFor the\ndata distribution pE and the generator distribution pG defined on the domain X, the GAN\nobjective (Goodfellow et al., 2014) is\nmin\npG max\nD\nL(D, pG) ,\nL(D, pG) ≜Ex∼pE [log D(x)] + Ex∼pG[log(1 −D(x))]\n(5.5)\nIn Goodfellow et al. (2014), the maximizer of the inner problem in Equation 5.5 is shown to\nbe:\n50\nD∗\npG ≜arg max\nD\nL(D, pG) =\npE\npE + pG\n(5.6)\nand the optimizer for Equation 5.5 is arg minpG maxD L(D, pG) = arg minpG L(D∗\npG, pG) = pE.\nLater, Finn et al. (2016a) and Ho and Ermon (2016) concurrently proposed connections\nbetween GANs and IRL. The Generative Adversarial Imitation Learning (GAIL) formulation\nin Ho and Ermon (2016) is based on matching state-action occupancy measures, while Finn\net al. (2016a) considered matching trajectory distributions.\nOur work is inspired by the\ndiscriminator proposed and used by Finn et al. (2016a):\nDθ(τ) ≜\npθ(τ)\npθ(τ) + q(τ)\n(5.7)\nwhere pθ(τ) ∝exp rθ(τ) with reward approximator rθ motivated by maximum causal entropy\nIRL. Note that Equation 5.7 matches the form of the optimal discriminator in Equation 5.6.\nAlthough Finn et al. (2016a) do not empirically support the effectiveness of their method, the\nAdversarial IRL approach of Fu et al. (2017) (AIRL) successfully used a similar discriminator\nfor state-action occupancy measure matching.\n5.3\nImitation Learning without Policy Optimization\nIn this section, we derive Adversarial Soft Advantage Fitting (ASAF), our novel Adversar-\nial Imitation Learning approach. Specifically, in Section 5.3.1, we present the theoretical\nfoundations for ASAF to perform Imitation Learning on full-length trajectories. Intuitively,\nour method is based on the use of such structured discriminators – that match the optimal\ndiscriminator form – to fit the trajectory distribution induced by the expert policy. This\napproach requires being able to evaluate and sample from the learned policy and allows us to\nlearn that policy and train the discriminator simultaneously, thus drastically simplifying the\ntraining procedure. We present in Section 5.3.2 parametrization options that satisfy these\nrequirements. Finally, in Section 5.3.3, we explain how to implement a practical algorithm\nthat can be used for arbitrary trajectory-lengths, including the transition-wise case.\n5.3.1\nAdversarial Soft Advantage Fitting – Theoretical setting\nBefore introducing our method, we derive GAN training with a structured discriminator.\n51\nGAN with structured discriminator\nSuppose that we have a generator distribution\npG and some arbitrary distribution ˜p and that both can be evaluated efficiently, e.g., cate-\ngorical distribution or probability density with normalizing flows (Rezende and Mohamed,\n2015).\nWe call a structured discriminator a function D˜p,pG : X →[0, 1] of the form\nD˜p,pG(x) = ˜p(x)\n.\n(˜p(x) + pG(x)) which matches the optimal discriminator form for Equa-\ntion 5.6. Considering our new GAN objective, we get:\nmin\npG max\n˜p\nL(˜p, pG) ,\nL(˜p, pG) ≜Ex∼pE [log D˜p,pG(x)] + Ex∼pG[log(1 −D˜p,pG(x))]\n(5.8)\nWhile the unstructured discriminator D from Equation 5.5 learns a mapping from x to a\nBernoulli distribution, we now learn a mapping from x to an arbitrary distribution ˜p from\nwhich we can analytically compute D˜p,pG(x). One can therefore say that D˜p,pG is parameter-\nized by ˜p. For the optimization problem of Equation 5.8, we have the following optima:\nLemma 1. The optimal discriminator parameter for any generator pG in Equation 5.8 is\nequal to the expert’s distribution, ˜p∗≜arg max˜p L(˜p, pG) = pE , and the optimal discriminator\nparameter is also the optimal generator, i.e.,\np∗\nG ≜arg min\npG\nmax\n˜p\nL(˜p, pG) = arg min\npG\nL(pE, pG) = pE = ˜p∗\nProof. See Appendix A.1.1\nIntuitively, Lemma 1 shows that the optimal discriminator parameter is also the target data\ndistribution of our optimization problem (i.e., the optimal generator). In other words, solving\nthe inner optimization yields the solution of the outer optimization. In practice, we update\n˜p to minimize the discriminator objective and use it directly as pG to sample new data.\nMatching trajectory distributions with structured discriminator\nMotivated by the\nGAN with structured discriminator, we consider the trajectory distribution matching problem\nin IL. Here, we optimise Equation 5.8 with x = τ, X = T , pE = PπE , pG = PπG, which yields\nthe following objective:\nmin\nπG max\n˜π\nL(˜π, πG) ,\nL(˜π, πG) ≜Eτ∼PπE [log D˜π,πG(τ)] + Eτ∼PπG [log(1 −D˜π,πG(τ))],\n(5.9)\nwith the structured discriminator:\nD˜π,πG(τ) =\nP˜π(τ)\nP˜π(τ) + PπG(τ) =\nq˜π(τ)\nq˜π(τ) + qπG(τ)\n(5.10)\n52\nHere we used the fact that Pπ(τ) decomposes into two distinct products: qπ(τ) ≜QT−1\nt=0 π(at|st)\nwhich depends on the stationary policy π and ξ(τ) ≜P0(s0)\nQT−1\nt=0 P(st+1|st, at) which ac-\ncounts for the environment dynamics.\nCrucially, ξ(τ) cancels out in the numerator and\ndenominator leaving ˜π as the sole parameter of this structured discriminator. In this way,\nD˜π,πG(τ) can evaluate the probability of a trajectory being generated by the expert policy\nsimply by evaluating products of stationary policy distributions ˜π and πG. With this form,\nwe can get the following result:\nTheorem 1. The optimal discriminator parameter for any generator policy πG in Equa-\ntion 5.9 ˜π∗≜arg max˜π L(˜π, πG) is such that q˜π∗= qπE , and using generator policy ˜π∗mini-\nmizes L(˜π∗, πG), i.e.,\n˜π∗∈arg min\nπG\nmax\n˜π\nL(˜π, πG) = arg min\nπG\nL(˜π∗, πG)\nProof. See Appendix A.1.2\nTheorem 1’s benefits are similar to the ones from Lemma 1: we can use a discriminator of\nthe form of Equation 5.10 to fit to the expert demonstrations a policy ˜π∗that simultaneously\nyields the optimal generator’s policy and produces the same trajectory distribution as the\nexpert policy.\n5.3.2\nA Specific Policy Class\nThe derivations of Section 5.3.1 rely on the use of a learnable policy that can both be\nevaluated and sampled from in order to fit the expert policy. A number of parameterization\noptions that satisfy these conditions are available.\nFirst of all, we observe that since πE is independent of r and π, we can add the entropy of\nthe expert policy H(πE(·|s)) to the MaxEnt IRL objective of Eq. (5.3) without modifying the\nsolution to the optimization problem:\nmin\nr∈R\n\u0012\nmax\nπ∈Π Jπ[r(s, a) + H(π(·|s))]\n\u0013\n−JπE [r(s, a) + H(πE(·|s))]\n(5.11)\nThe max over policies implies that when optimising r, π has already been made optimal with\nrespect to the causal entropy augmented reward function r′(s, a|π) = r(s, a) + H(π(·|s)) and\ntherefore it must be of the form presented in Eq. (5.1). Moreover, since π is optimal w.r.t.\nr′ the difference in performance Jπ[r′(s, a|π)] −JπE [r′(s, a|πE)] is always non-negative and its\nminimum of 0 is only reached when πE is also optimal w.r.t. r′, in which case πE must also\nbe of the form of Eq. (5.1).\n53\nWith discrete action spaces we propose to parameterize the MaxEnt policy defined in Equa-\ntion 5.1 with the following categorical distribution:\n˜π(a|s) = exp\n \nQθ(s, a) −log\nX\na′\nexp Qθ(s, a′)\n!\n(5.12)\nwhere Qθ is a model parameterized by θ that approximates 1\nαQ∗\nsoft.\nWith continuous action spaces, the soft value function involves an intractable integral over\nthe action domain. Therefore, we approximate the MaxEnt distribution with a Normal distri-\nbution with diagonal covariance matrix like it is commonly done in the literature (Haarnoja\net al., 2018; Nachum et al., 2018).\nBy parameterizing the mean and variance we get a\nlearnable density function that can be easily evaluated and sampled from.\n5.3.3\nAdversarial Soft Advantage Fitting (ASAF) – practical algorithm\nSection 5.3.1 shows that assuming ˜π can be evaluated and sampled from, we can use the\nstructured discriminator of Equation 5.10 to learn a policy ˜π that matches the expert’s tra-\njectory distribution. Section 5.3.2 proposes parameterizations for discrete and continuous\naction spaces that satisfy those assumptions. In practice, as with GANs (Goodfellow et al.,\n2014), we do not train the discriminator to convergence as gradient-based optimisation can-\nnot be expected to find the global optimum of non-convex problems. Instead, Adversarial\nSoft Advantage Fitting (ASAF) alternates between two simple steps: (1) training D˜π,πG by\nminimizing the binary cross-entropy loss,\nLBCE(DE, DG, ˜π) ≈−1\nnE\nnE\nX\ni=1\nlog D˜π,πG(τ (E)\ni\n) −1\nnG\nnG\nX\ni=1\nlog\n\u0010\n1 −D˜π,πG(τ (G)\ni\n)\n\u0011\nwhere\nτ (E)\ni\n∼DE , τ (G)\ni\n∼DG and D˜π,πG(τ) =\nQT−1\nt=0 ˜π(at|st)\nQT−1\nt=0 ˜π(at|st) + QT−1\nt=0 πG(at|st)\n(5.13)\nwith minibatch sizes nE = nG, and (2) updating the generator’s policy as πG ←˜π to minimize\nEquation 5.9 (see Algorithm 1).\nWe derived ASAF considering full trajectories, yet it might be preferable in practice to\nsplit full trajectories into smaller chunks. This is particularly true in environments where\ntrajectory length varies a lot or tends to infinity.\nTo investigate whether the practical benefits of using partial trajectories hurt ASAF’s per-\nformance, we also consider a variation, ASAF-w, where we treat trajectory-windows of size\nw as if they were full trajectories. Note that considering windows as full trajectories results\n54\nin approximating that the initial state of these sub-trajectories have equal probability under\nthe expert’s and the generator’s policy (this is easily seen when deriving Equation 5.10).\nIn the limit, ASAF-1 (window-size of\n1) becomes a transition-wise algorithm\nwhich can be desirable if one wants to\ncollect rollouts asynchronously or has\nonly access to unsequential expert data.\nWhile ASAF-1 may work well in practice\nit essentially assumes that the expert’s\nand the generator’s policies have the\nsame state occupancy measure, which\nis incorrect until actually recovering the\ntrue expert policy.\nAlgorithm 1: ASAF\nRequire: expert trajectories DE = {τi}NE\ni=1\nRandomly initialize ˜π and set πG ←˜π\nfor steps m = 0 to M do\nCollect trajectories DG = {τi}NG\ni=1 using πG\nUpdate ˜π by minimizing Equation 5.13\nSet πG ←˜π\nend for\nFinally, to offer a complete family of algorithms based on the structured discriminator ap-\nproach, we show in Appendix A.2 that this assumption is not mandatory and derive a\ntransition-wise algorithm based on Soft Q-function Fitting (rather than soft advantages)\nthat also gets rid of the RL loop. We call this algorithm ASQF. While theoretically sound,\nwe found that in practice, ASQF is outperformed by ASAF-1 in more complex environments\n(see Section 5.5.1).\n5.4\nRelated works\nZiebart et al. (2008) first proposed MaxEnt IRL, the foundation of modern IL. Ziebart (2010)\nfurther elaborated MaxEnt IRL as well as deriving the optimal form of the MaxEnt policy\nat the core of our methods. Finn et al. (2016a) proposed a GAN formulation to IRL that\nleveraged the energy based models of Ziebart (2010). Finn et al. (2016b)’s implementation of\nthis method, however, relied on processing full trajectories with Linear Quadratic Regulator\nand on optimizing with guided policy search, to manage the high variance of trajectory costs.\nTo retrieve robust rewards, Fu et al. (2017) proposed a straightforward transposition of (Finn\net al., 2016a) to state-action transitions. In doing so, they had to however do away with a\nGAN objective during policy optimization, consequently minimizing the Kullback–Leibler\ndivergence from the expert occupancy measure to the policy occupancy measure (instead of\nthe Jensen-Shannon divergence) (Ghasemipour et al., 2019).\nLater works (Sasaki et al., 2018; Kostrikov et al., 2020) move away from the Generative\nAdversarial formulation. To do so, Sasaki et al. (2018) directly express the expectation of\n55\nthe Jensen-Shannon divergence between the occupancy measures in term of the agent’s Q-\nfunction, which can then be used to optimize the agent’s policy with off-policy Actor-Critic\n(Degris et al., 2012). Similarly, Kostrikov et al. (2020) use Dual Stationary Distribution\nCorrection Estimation (Nachum et al., 2019) to approximate the Q-function on the expert’s\ndemonstrations before optimizing the agent’s policy under the initial state distribution using\nthe reparametrization trick (Haarnoja et al., 2018). While (Sasaki et al., 2018; Kostrikov\net al., 2020) are related to our methods in their interests in learning directly the value\nfunction, they differ in their goal and thus in the resulting algorithmic complexity. Indeed,\nthey aim at improving the sample efficiency in terms of environment interaction and therefore\nmove away from the algorithmically simple Generative Adversarial formulation towards more\ncomplicated divergence minimization methods.\nIn doing so, they further complicate the\nImitation Learning methods while still requiring to explicitly learn a policy. Yet, simply using\nthe Generative Adversarial formulation with an Experience Replay Buffer can significantly\nimprove the sample efficiency (Kostrikov et al., 2019). For these reasons, and since our aim is\nto propose efficient yet simple methods, we focus on the Generative Adversarial formulation.\nWhile Reddy et al. (2019) share our interest for simpler IL methods, they pursue an opposite\napproach to ours. They propose to eliminate the reward learning steps of IRL by simply\nhard-coding a reward of 1 for expert’s transitions and of 0 for agent’s transitions. They then\nuse Soft Q-learning (Haarnoja et al., 2017) to learn a value function by sampling transitions\nin equal proportion from the expert’s and agent’s buffers. Unfortunately, once the learner\naccurately mimics the expert, it collects expert-like transitions that are labeled with a reward\nof 0 since they are generated and not coming from the demonstrations. This effectively causes\nthe reward of expert-like behavior to decay as the agent improves and can severely destabilize\nlearning to a point where early-stopping becomes required (Reddy et al., 2019).\nOur work builds on (Finn et al., 2016a), yet its novelty is to explicitly express the probability\nof a trajectory in terms of the policy in order to directly learn this latter when training the\ndiscriminator. In contrast, (Fu et al., 2017) considers a transition-wise discriminator with\nun-normalized probabilities which makes it closer to ASQF (Appendix A.2) than to ASAF-1.\nAdditionally, AIRL (Fu et al., 2017) minimizes the Kullback-Leiber Divergence (Ghasemipour\net al., 2019) between occupancy measures whereas ASAF minimizes the Jensen-Shanon Di-\nvergence between trajectory distributions.\nFinally, Behavioral Cloning uses the loss function from supervised learning (classification or\nregression) to match expert’s actions given expert’s states and suffers from compounding error\ndue to co-variate shift (Ross and Bagnell, 2010) since its data is limited to the demonstrated\nstate-action pairs without environment interaction. Contrarily, ASAF-1 uses the binary cross\n56\nentropy loss in Equation 5.13 and does not suffer from compounding error as it learns on\nboth generated and expert’s trajectories.\n5.5\nResults and discussion\nWe evaluate our methods on a variety of discrete and continuous control tasks. Our results\nshow that, in addition to drastically simplifying the adversarial IRL framework, our methods\nperform on par or better than previous approaches on all but one environment.\nWhen\ntrajectory length is really long or drastically varies across episodes (see MuJoCo experiments\nSection 5.5.3), we find that using sub-trajectories with fixed window-size (ASAF-w or ASAF-\n1) significantly outperforms its full trajectory counterpart ASAF.\n5.5.1\nExperimental setup\nWe compare our algorithms ASAF, ASAF-w and ASAF-1 against GAIL (Ho and Ermon,\n2016), the predominant Adversarial Imitation Learning algorithm in the literature, and AIRL\n(Fu et al., 2017), one of its variations that also leverages the access to the generator’s pol-\nicy distribution.\nAdditionally, we compare against SQIL (Reddy et al., 2019), a recent\nReinforcement Learning-only approach to Imitation Learning that proved successful on high-\ndimensional tasks.\nOur implementations of GAIL and AIRL use PPO (Schulman et al.,\n2017) instead of TRPO (Schulman et al., 2015) as it has been shown to improve performance\n(Kostrikov et al., 2019). Finally, to be consistent with (Ho and Ermon, 2016), we do not use\ncausal entropy regularization.\nFor all tasks except MuJoCo, we selected the best performing hyperparameters through a\nrandom search of equal budget for each algorithm-environment pair (see Appendix A.4) and\nthe best configuration is retrained on ten random seeds. For the MuJoCo experiments, GAIL\nrequired extensive tuning (through random searches) of both its RL and IRL components\nto achieve satisfactory performances. Our methods, ASAF-w and ASAF-1, on the other\nhand showed much more stable and robust to hyperparameterization, which is likely due\nto their simplicity. SQIL used the same SAC(Haarnoja et al., 2018) implementation and\nhyperparameters that were used to generate the expert demonstrations.\nFinally for each task, all algorithms use the same neural network architectures for their policy\nand/or discriminator (see full description in Appendix A.4).\nExpert demonstrations are\neither generated by hand (mountaincar), using open-source bots (Pommerman) or from our\nimplementations of SAC and PPO (all remaining). More details are given in Appendix A.5.\n57\n0\n200\n400\n600\n800\n1000\nEpisodes\n0\n50\n100\n150\n200\nEvaluation return\nCARTPOLE\n0\n200\n400\n600\n800\n1000\nEpisodes\n200\n175\n150\n125\n100\nMOUNTAINCAR\n0\n200\n400\n600\n800\n1000\nEpisodes\n1200\n800\n400\n0\nLUNARLANDER\n0\n100\n200\n300\nEnvironment steps x 1e-3\n1600\n1200\n800\n400\n0\nEvaluation return\nPENDULUM\n0\n100\n200\n300\nEnvironment steps x 1e-3\n40\n0\n40\n80\nMOUNTAINCAR-C\n0\n100\n200\n300\nEnvironment steps x 1e-3\n1200\n800\n400\n0\nLUNARLANDER-C\nASAF (ours)\nASAF-w (ours)\nASAF-1 (ours)\nSQIL\nGAIL + PPO\nAIRL + PPO\nExpert\nFigure 5.1 Results on classic control and Box2D tasks for 10 expert demonstrations. First\nrow contains discrete actions environments, second row corresponds to continuous control.\n5.5.2\nExperiments on classic control and Box2D tasks (discrete and continuous)\nFigure 5.1 shows that ASAF and its approximate variations ASAF-1 and ASAF-w quickly\nconverge to expert’s performance (here w was tuned to values between 32 to 200, see Ap-\npendix A.4 for selected window-sizes). This indicates that the practical benefits of using\nshorter trajectories or even just transitions does not hinder performance on these simple\ntasks. Note that for Box2D and classic control environments, we retrain the best configura-\ntion of each algorithm for twice as long than was done in the hyperparameter search, which\nallows to uncover unstable learning behaviors. Figure 5.1 shows that our methods display\nmuch more stable learning: their performance rises until they match the expert’s and does\nnot decrease once it is reached. This is a highly desirable property for an Imitation Learning\nalgorithm since in practice one does not have access to a reward function and thus cannot\nmonitor the performance of the learning algorithm to trigger early-stopping. The baselines\non the other hand experience occasional performance drops. For GAIL and AIRL, this is\nlikely due to the concurrent RL and IRL loops, whereas for SQIL, it has been noted that an\neffective reward decay can occur when accurately mimicking the expert (Reddy et al., 2019).\nThis instability is particularly severe in the continuous control case. In practice, all three\nbaselines use early stopping to avoid performance decay (Reddy et al., 2019).\n5.5.3\nExperiments on MuJoCo (continuous control)\nTo scale up our evaluations in continuous control we use the popular MuJoCo benchmarks. In\nthis domain, the trajectory length is either fixed at a large value (1000 steps on HalfCheetah)\n58\n0.0\n0.6\n1.2\n1.8\nEnvironment Steps (M)\n0\n1500\n3000\nEvaluation Return\nHOPPER\n0.0\n1.5\n3.0\n4.5\nEnvironment Steps (M)\n0\n2000\n4000\n6000\nWALKER2D\n0.0\n0.6\n1.2\n1.8\nEnvironment Steps (M)\n0\n3000\n6000\nHALFCHEETAH\n0.0\n0.6\n1.2\n1.8\nEnvironment Steps (M)\n0\n2000\n4000\n6000\nANT\nSQIL\nGAIL + PPO (w/ GP)\nASAF-w (ours)\nASAF (ours)\nASAF-1 (ours)\nExpert\nFigure 5.2 Results on MuJoCo tasks for 25 expert demonstrations.\nor varies a lot across episodes due to termination when the character falls down (Hopper,\nWalker2d and Ant). Figure 5.2 shows that these trajectory characteristics hinder ASAF’s\nlearning as ASAF requires collecting multiple episodes for every update, while ASAF-1 and\nASAF-w perform well and are more sample-efficient than ASAF in these scenarios.\nWe\nfocus on GAIL since (Fu et al., 2017) claim that AIRL performs on par with it on MuJoCo\nenvironments. In Figure A.2 in Appendix A.3 we evaluate GAIL both with and without\ngradient penalty (GP) on discriminator updates (Gulrajani et al., 2017; Kostrikov et al., 2019)\nand while GAIL was originally proposed without GP (Ho and Ermon, 2016), we empirically\nfound that GP prevents the discriminator to overfit and enables RL to exploit dense rewards,\nwhich highly improves its sample efficiency. Despite these ameliorations, GAIL proved to be\nquite inconsistent across environments despite substantial efforts on hyperparameter tuning.\nOn the other hand, ASAF-1 performs well across all environments. Finally, we see that\nSQIL’s instability is exacerbated on MuJoCo.\n5.5.4\nExperiments on Pommerman (discrete control)\nFinally, to scale up our evaluations in discrete control environments, we consider the domain\nof Pommerman (Resnick et al., 2018), a challenging and very dynamic discrete control en-\nvironment that uses rich and high-dimensional observation spaces (see Appendix A.5). We\nperform evaluations of all of our methods and baselines on a 1 vs 1 task where a learning\nagent plays against a random agent, the opponent. The goal for the learning agent is to navi-\ngate to the opponent and eliminate it using expert demonstrations provided by the champion\nalgorithm of the FFA 2018 competition (Zhou et al., 2018). We removed the ability of the\nopponent to lay bombs so that it doesn’t accidentally eliminate itself. Since it can still move\naround, it is however surprisingly tricky to eliminate: the expert has to navigate across the\nwhole map, lay a bomb next to the opponent and retreat to avoid eliminating itself. This en-\ntire routine has then to be repeated several times until finally succeeding since the opponent\nwill often avoid the hit by chance. We refer to this task as Pommerman Random-Tag. Note\n59\n0\n15000\n30000\n45000\nEpisodes\n1.0\n0.5\n0.0\n0.5\n1.0\nEvaluation Return\nPOMMERMAN RANDOM-TAG\nASAF (ours)\nASAF-w (ours)\nASAF-1 (ours)\nSQIL\nAIRL + PPO\nGAIL + PPO\nExpert\nBC\n1\n75\n150\n300\nNumber of expert demonstrations\n1.0\n0.5\n0.0\n0.5\n1.0\nFigure 5.3 Results on Pommerman Random-Tag: (Left) Snapshot of the environment. (Cen-\nter) Learning measured as evaluation return over episodes for 150 expert trajectories (Right)\nAverage return on last 20% of training for decreasing number of expert trajectories [300, 150,\n75, 15, 5, 1].\nthat since we measure success of the imitation task with the win-tie-lose outcome (sparse\nperformance metric), a learning agent has to truly reproduce the expert behavior until the\nvery end of trajectories to achieve higher scores. Figure 5.3 shows that all three variations of\nASAF as well as Behavioral Cloning (BC) outperform the baselines.\n5.6\nConclusion\nWe propose an important simplification to the Adversarial Imitation Learning framework\nby removing the Reinforcement Learning optimisation loop altogether. We show that, by\nusing a particular form for the discriminator, our method recovers a policy that matches\nthe expert’s trajectory distribution. We evaluate our approach against prior works on many\ndifferent benchmarking tasks and show that our method (ASAF) compares favorably to the\npredominant Imitation Learning algorithms. The approximate versions, ASAF-w and ASAF-\n1, that use sub-trajectories yield a flexible algorithms that work well both on short and long\ntime horizons.\nFinally, our approach still involves a reward learning module through its\ndiscriminator, and it would be interesting in future work to explore how ASAF can be used\nto learn robust rewards, along the lines of Fu et al. (2017).\nBroader Impact\nOur contributions are mainly theoretical and aim at simplifying current Imitation Learning\nmethods. We do not propose new applications nor use sensitive data or simulator. Yet our\nmethod can ease and promote the use, design and development of Imitation Learning algo-\nrithms and may eventually lead to applications outside of simple and controlled simulators.\nWe do not pretend to discuss the ethical implications of the general use of autonomous agents\n60\nbut we rather try to investigate what are some of the differences in using Imitation Learning\nrather than reward oriented methods in the design of such agents.\nUsing only a scalar reward function to specify the desired behavior of an autonomous agent\nis a challenging task as one must weight different desiderata and account for unsuspected\nbehaviors and situations. Indeed, it is well known in practice that Reinforcement Learning\nagents tend to find bizarre ways of exploiting the reward signal without solving the desired\ntask. The fact that it is difficult to specify and control the behavior of an RL agents is a major\nflaw that prevent current methods to be applied to risk sensitive situations. On the other\nhand, Imitation Learning proposes a more natural way of specifying nuanced preferences by\ndemonstrating desirable ways of solving a task. Yet, IL also has its drawbacks. First of all one\nneeds to be able to demonstrate the desired behavior and current methods tend to be only\nas good as the demonstrator. Second, it is a challenging problem to ensure that the agent\nwill be able to adapt to new situations that do not resemble the demonstrations. For these\nreasons, it is clear for us that additional safeguards are required in order to apply Imitation\nLearning (and Reinforcement Learning) methods to any application that could effectively\nhave a real world impact.\nAcknowledgments\nWe thank Eloi Alonso, Olivier Delalleau, Félix G. Harvey, Maxim Peter and the entire re-\nsearch team at Ubisoft Montreal’s La Forge R&D laboratory. Their feedback and comments\ncontributed significantly to this work. Christopher Pal and Derek Nowrouzezahrai acknowl-\nedge funding from the Fonds de Recherche Nature et Technologies (FRQNT), Ubisoft Mon-\ntreal and Mitacs’ Accelerate Program in support of our work, as well as Compute Canada for\nproviding computing resources. Derek and Paul also acknowledge support from the NSERC\nIndustrial Research Chair program.\n61\nCHAPTER 6\nARTICLE 2: PROMOTING COORDINATION\nTHROUGH POLICY REGULARIZATION IN MULTI-AGENT\nDEEP REINFORCEMENT LEARNING\nCo-authors\nPaul Barde, Félix Harvey, Derek Nowrouzezahrai & Christopher Pal\nPublished in\nAdvances in Neural Information Processing Systems, December 12, 2020\nAbstract\nIn multi-agent reinforcement learning, discovering successful collective behaviors is\nchallenging as it requires exploring a joint action space that grows exponentially with\nthe number of agents. While the tractability of independent agent-wise exploration\nis appealing, this approach fails on tasks that require elaborate group strategies.\nWe argue that coordinating the agents’ policies can guide their exploration and we\ninvestigate techniques to promote such an inductive bias. We propose two policy\nregularization methods: TeamReg, which is based on inter-agent action predictabil-\nity and CoachReg that relies on synchronized behavior selection. We evaluate each\napproach on four challenging continuous control tasks with sparse rewards that re-\nquire varying levels of coordination as well as on the discrete action Google Research\nFootball environment. Our experiments show improved performance across many\ncooperative multi-agent problems. Finally, we analyze the effects of our proposed\nmethods on the policies that our agents learn and show that our methods success-\nfully enforce the qualities that we propose as proxies for coordinated behaviors.\n6.1\nIntroduction\nMulti-Agent Reinforcement Learning (MARL) refers to the task of training an agent to\nmaximize its expected return by interacting with an environment that contains other learning\nagents. It represents a challenging branch of Reinforcement Learning (RL) with interesting\ndevelopments in recent years (Hernandez-Leal et al., 2018). A popular framework for MARL\nis the use of a Centralized Training and a Decentralized Execution (CTDE) procedure (Lowe\net al., 2017; Foerster et al., 2018; Iqbal and Sha, 2019; Foerster et al., 2019; Rashid et al.,\n2018). Typically, one leverages centralized critics to approximate the value function of the\naggregated observations-actions pairs and train actors restricted to the observation of a single\n62\nagent. Such critics, if exposed to coordinated joint actions leading to high returns, can steer\nthe agents’ policies toward these highly rewarding behaviors. However, these approaches\ndepend on the agents luckily stumbling on these collective actions in order to grasp their\nbenefit.\nThus, it might fail in scenarios where such behaviors are unlikely to occur by\nchance. We hypothesize that in such scenarios, coordination-promoting inductive biases on\nthe policy search could help discover successful behaviors more efficiently and supersede task-\nspecific reward shaping and curriculum learning. To motivate this proposition we present\na simple Markov Game in which agents forced to coordinate their actions learn remarkably\nfaster. For more realistic tasks in which coordinated strategies cannot be easily engineered\nand must be learned, we propose to transpose this insight by relying on two coordination\nproxies to bias the policy search. The first avenue, TeamReg, assumes that an agent must be\nable to predict the behavior of its teammates in order to coordinate with them. The second,\nCoachReg, supposes that coordinated agents collectively recognize different situations and\nsynchronously switch to different sub-policies to react to them.1.\nOur contributions are threefold. First, we show that coordination can crucially accelerate\nmulti-agent learning for cooperative tasks. Second, we propose two novel approaches that aim\nat promoting such coordination by augmenting CTDE MARL algorithms through additional\nmulti-agent objectives that act as policy regularizers and are optimized jointly with the main\nreturn-maximization objective. Third, we design two new sparse-reward cooperative tasks in\nthe multi-agent particle environment (Mordatch and Abbeel, 2018). We use them along with\ntwo standard multi-agent tasks to present a detailed evaluation of our approaches’ benefits\nwhen they extend the reference CTDE MARL algorithm MADDPG (Lowe et al., 2017).\nWe validate our methods’ key components by performing an ablation study and a detailed\nanalysis of their effect on agents’ behaviors. Finally, we verify that these benefits hold on the\nmore complex, discrete action, Google Research Football environment (Kurach et al., 2019).\nOur experiments suggest that our TeamReg objective provides a dense learning signal that\ncan help guiding the policy towards coordination in the absence of external reward, eventually\nleading it to the discovery of higher performing team strategies in a number of cooperative\ntasks. However we also find that TeamReg does not lead to improvements in every single case\nand can even be harmful in environments with an adversarial component. For CoachReg, we\nfind that enforcing synchronous sub-policy selection enables the agents to concurrently learn\nto react to different agreed upon situations and consistently yields significant improvements\non the overall performance.\n1Source code for the algorithms and environments will be made public upon publication of this work.\nVisualisations of CoachReg are available here: https://sites.google.com/view/marl-coordination/\n63\n6.2\nBackground\n6.2.1\nMarkov Games\nWe consider the framework of Markov Games (Littman, 1994), a multi-agent extension of\nMarkov Decision Processes (MDPs). A Markov Game of N agents is defined by the tuple\n⟨S, T , P, {Oi, Ai, Ri}N\ni=1⟩where S, T , and P are respectively the set of all possible states, the\ntransition function and the initial state distribution. While these are global properties of the\nenvironment, Oi, Ai and Ri are individually defined for each agent i. They are respectively\nthe observation functions, the sets of all possible actions and the reward functions. At each\ntime-step t, the global state of the environment is given by st ∈S and every agent’s individual\naction vector is denoted by ai\nt ∈Ai. To select their action, each agent i only has access to its\nown observation vector oi\nt which is extracted by the observation function Oi from the global\nstate st. The initial state s0 is sampled from the initial state distribution P : S →[0, 1]\nand the next state st+1 is sampled from the probability distribution over the possible next\nstates given by the transition function T : S × S × A1 × ... × AN →[0, 1]. Finally, at\neach time-step, each agent receives an individual scalar reward ri\nt from its reward function\nRi : S ×S ×A1 ×...×AN →R. Agents aim at maximizing their expected discounted return\nE\nhPT\nt=0 γtri\nt\ni\nover the time horizon T, where γ ∈[0, 1] is a discount factor.\n6.2.2\nMulti-Agent Deep Deterministic Policy Gradient\nMADDPG (Lowe et al., 2017) is an adaptation of the Deep Deterministic Policy Gradient al-\ngorithm (Lillicrap et al., 2015) to the multi-agent setting. It allows the training of cooperating\nand competing decentralized policies through the use of a centralized training procedure. In\nthis framework, each agent i possesses its own deterministic policy µi for action selection and\ncritic Qi for state-action value estimation, which are respectively parametrized by θi and ϕi.\nAll parametric models are trained off-policy from previous transitions ζt := (ot, at, rt, ot+1)\nuniformly sampled from a replay buffer D. Note that ot := [o1\nt, ..., oN\nt ] is the joint observa-\ntion vector and at := [a1\nt, ..., aN\nt ] is the joint action vector, obtained by concatenating the\nindividual observation vectors oi\nt and action vectors ai\nt of all N agents. Each centralized\ncritic is trained to estimate the expected return for a particular agent i from the Q-learning\nloss (Watkins and Dayan, 1992):\nLi(ϕi) = Eζt∼D\n\u00141\n2\n\u0010\nQi(ot, at; ϕi) −yi\nt\n\u00112\u0015\nyi\nt = ri\nt + γQi(ot+1, at+1; ¯ϕi)\n\f\f\f\faj\nt+1=µj(oj\nt+1;¯θj) ∀j\n(6.1)\n64\nFor a given set of weights w, we define its target counterpart ¯w, updated from ¯w ←τw +\n(1 −τ) ¯w where τ is a hyperparameter. Each policy is updated to maximize the expected\ndiscounted return of the corresponding agent i :\nJi\nPG(θi) = Eot∼D\n\nQi(ot, at)\n\f\f\f\f\fai\nt=µi(oi\nt; θi),\naj\nt=µj(oj\nt; ¯θj) ∀j̸=i\n\n\n(6.2)\nBy taking into account all agents’ observation-action pairs when guiding an agent’s policy,\nthe value-functions are trained in a centralized, stationary environment, despite taking place\nin a multi-agent setting.\nThis mechanism can allow to learn coordinated strategies that\ncan then be deployed in a decentralized way. However, this procedure does not encourage\nthe discovery of coordinated strategies since high-return behaviors have to be randomly\nexperienced through unguided exploration.\n6.3\nMotivation\nIn this section, we aim to answer the following question: can coordination help the discovery of\neffective policies in cooperative tasks? Intuitively, coordination can be defined as an agent’s\nbehavior being informed by the behavior of another agent, i.e.\nstructure in the agents’\ninteractions. Namely, a team where agents behave independently of one another would not\nbe coordinated.\nConsider the simple Markov Game consisting of a chain of length L leading to a termination\nstate as depicted in Figure 6.1.\nAt each time-step, both agents receive rt = −1.\nThe\njoint action of these two agents in this environment is given by a ∈A = A1 × A2, where\nA1 = A2 = {0, 1}. Agent i tries to go right when selecting ai = 0 and left when selecting\nai = 1. However, to transition to a different state both agents need to perform the same\naction at the same time (two lefts or two rights).\nNow consider a slight variant of this\nenvironment with a different joint action structure a′ ∈A′. The former action structure is\naugmented with a hard-coded coordination module which maps the joint primitive ai to ai′\nlike so:\na′ =\n\na1′ = a1\na2′ = a1a2 + (1 −a1)(1 −a2)\n\n,\n\na1\na2\n\n∈A\nWhile the second agent still learns a state-action value function Q2(s, a2) with a2 ∈A2, the\ncoordination module builds a2′ from (a1, a2) so that a2′ effectively determines whether the\nsecond agent acts in agreement or in disagreement with the first agent. In other words, if\na2 = 1, then a2′ = a1 (agreement) and if a2 = 0, then a2′ = 1 −a1 (disagreement).\n65\nWhile it is true that this additional struc-\nture does not modify the action space nor\nthe independence of the action selection, it\nreduces the stochasticity of the transition dy-\nnamics as seen by agent 2. In the first setup,\nthe outcome of an agent’s action is condi-\ntioned on the action of the other agent. In\nthe second setup, if agent 2 decides to dis-\nagree, the transition becomes deterministic\nas the outcome is independent of agent 1.\nThis suggests that by reducing the entropy\nof the transition distribution, this mapping\nreduces the variance of the Q-updates and\nthus makes online tabular Q-learning agents\nlearn much faster (Figure 6.1).\nThis example uses a handcrafted mapping\nin order to demonstrate the effectiveness of\nexploring in the space of coordinated poli-\ncies rather than in the unconstrained pol-\nicy space. Now, the following question re-\nmains: how can one softly learn the same\ntype of constraint throughout training for\nany multi-agent cooperative tasks?\nIn the\nfollowing sections, we present two algorithms\nthat tackle this problem.\n50\n25\nReturn\n′, L = 5\n, L = 5\n100\n50\nReturn\n′, L = 10\n, L = 10\n0\n20\n40\n60\n80\n100\nEpisodes\n200\n100\nReturn\n′, L = 20\n, L = 20\n(1, 1)\n0\n1\n2\nh\nh−1\n...\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 0)\n(0, 1)\n(1, 0)\n(0, 1)\n(1, 0)\n(0, 1)\n(1, 0)\n(0, 1)\na = (1,1)\nh\n0\nFigure 6.1 (Top) The tabular Q-learning\nagents learn much more efficiently when con-\nstrained to the space of coordinated policies\n(solid lines) than in the original action space\n(dashed lines).\n(Bottom) Simple Markov\nGame consisting of a chain of length L lead-\ning to a terminal state (in grey). Agents can\nbe seen as the two wheels of a vehicle so that\ntheir actions need to be in agreement for the\nvehicle to move. The detailed experimental\nsetup is reported in Appendix B.1.\n6.4\nCoordination and Policy regularization\nPseudocodes of our implementations are provided in Appendix B.4 (see Algorithms 3 and 4).\n6.4.1\nTeam regularization\nThis first approach aims at exploiting the structure present in the joint action space of\ncoordinated policies to attain a certain degree of predictability of one agent’s behavior with\nrespect to its teammate(s).\nIt is based on the hypothesis that the reciprocal also holds\ni.e. that promoting agents’ predictability could foster such team structure and lead to more\ncoordinated behaviors. This assumption is cast into the decentralized framework by training\n66\nagents to predict their teammates’ actions given only their own observation. For continuous\ncontrol, the loss is the mean squared error (MSE) between the predicted and true actions\nof the teammates, yielding a teammate-modelling secondary objective. For discrete action\nspaces, we use the KL-divergence (DKL) between the predicted and real action distributions\nof an agent pair.\nWhile estimating teammates’ policies can be used to enrich the learned representations, we\nextend this objective to also drive the teammates’ behaviors towards the predictions by\nleveraging a differentiable action selection mechanism. We call team-spirit this objective pair\nJi,j\nTS and Jj,i\nTS between agents i and j:\nJi,j\nTS-continuous(θi, θj) = −Eot∼D\nh\nMSE(µj(oj\nt; θj), ˆµi,j(oi\nt; θi))\ni\n(6.3)\nJi,j\nTS-discrete(θi, θj) = −Eot∼D\nh\nDKL\n\u0010\nπj(·|oj\nt; θj)||ˆπi,j(·|oi\nt; θi)\n\u0011i\n(6.4)\nwhere ˆµi,j (or ˆπi,j in the discrete case) is the policy head of agent i trying to predict the\naction of agent j. The total objective for a given agent i becomes:\nJi\ntotal(θi) = Ji\nPG(θi) + λ1\nX\nj\nJi,j\nTS(θi, θj) + λ2\nX\nj\nJj,i\nTS(θj, θi)\n(6.5)\nwhere λ1 and λ2 are hyperparameters that respectively weigh how well an agent should\npredict its teammates’ actions, and how predictable an agent should be for its teammates.\nWe call TeamReg this dual regularization from team-spirit objectives. Figure 6.2 summarizes\nthese interactions.\n6.4.2\nCoach regularization\nIn order to foster coordinated interactions, this method aims at teaching the agents to rec-\nognize different situations and synchronously select corresponding sub-behaviors.\nSub-policy selection\nFirstly, to enable explicit sub-behavior selection, we propose the use\nof policy masks as a means to modulate the agents’ policies. A policy mask uj is a one-hot\nvector of size K (a fixed hyperparameter) with its jth component set to one. In practice,\nwe use these masks to perform dropout (Srivastava et al., 2014) in a structured manner on\n˜h1 ∈RM, the pre-activations of the first hidden layer h1 of the policy network π. To do so,\nwe construct the vector uj, which is the concatenation of C copies of uj, in order to reach\nthe dimensionality M = C ∗K. The element-wise product uj ⊙˜h1 is performed and only\nthe units of ˜h1 at indices m modulo K = j are kept for m = 0, . . . , M −1. Each agent i\n67\ngenerates ei\nt, its own policy mask from its observation oi\nt, to modulate its policy network.\nHere, a simple linear layer li is used to produce a categorical probability distribution pi(ei\nt|oi\nt)\nfrom which the one-hot vector is sampled:\npi(ei\nt = uj|oi\nt) =\nexp (li(oi\nt; θi)j)\nPK−1\nk=0 exp (li(oi\nt; θi)k)\n(6.6)\nSynchronous sub-policy selection\nAlthough the policy masking mechanism enables\nthe agent to swiftly switch between sub-policies it does not encourage the agents to syn-\nchronously modulate their behavior. To promote synchronicity we introduce the coach en-\ntity, parametrized by ψ, which learns to produce policy-masks ec\nt from the joint observations,\ni.e. pc(ec\nt|ot; ψ). The coach is used at training time only and drives the agents toward syn-\nchronously selecting the same behavior mask. Specifically, the coach is trained to output\nmasks that (1) yield high returns when used by the agents and (2) are predictable by the\nagents. Similarly, each agent is regularized so that (1) its private mask matches the coach’s\nmask and (2) it derives efficient behavior when using the coach’s mask. At evaluation time,\nthe coach is removed and the agents only rely on their own policy masks. The policy gradient\nobjective when agent i is provided with the coach’s mask is given by:\nJi\nEPG(θi, ψ) = Eot,at∼D\n\nQi(ot, at)\n\f\f\f\f\f\fai\nt=µ(oi\nt,ec\nt;θi)\nec\nt∼pc(·|ot;ψ)\n\n\n(6.7)\nThe difference between the mask distribution of agent i and the coach’s is measured from\nthe Kullback–Leibler divergence:\nJi\nE(θi, ψ) = −Eot∼D\nh\nDKL\n\u0010\npc(·|ot; ψ)| |pi(·|oi\nt; θi)\n\u0011i\n(6.8)\nThe total objective for agent i is:\nJi\ntotal(θi) = Ji\nPG(θi) + λ1Ji\nE(θi, ψ) + λ2Ji\nEPG(θi, ψ)\n(6.9)\nwith λ1 and λ2 the regularization coefficients. Similarly, the coach is trained with the following\ndual objective, weighted by the λ3 coefficient:\nJc\ntotal(ψ) = 1\nN\nN\nX\ni=1\n\u0010\nJi\nEPG(θi, ψ) + λ3Ji\nE(θi, ψ)\n\u0011\n(6.10)\nIn order to propagate gradients through the sampled policy mask we reparameterized the cat-\negorical distribution using the Gumbel-softmax (Jang et al., 2017). We call this coordinated\nsub-policy selection regularization CoachReg and illustrate it in Figure 6.3.\n68\n        𝝁1\n  𝝁2\nFigure 6.2 Illustration of TeamReg with two\nagents. Each agent’s policy is equipped with\nadditional heads that are trained to predict\nother agents’ actions and every agent is regu-\nlarized to produce actions that its teammates\ncorrectly predict. The method is depicted for\nagent 1 only to avoid cluttering\n𝝁1\nCoach\n𝝁2\nFigure 6.3 Illustration of CoachReg with two\nagents. A central model, the coach, takes all\nagents’ observations as input and outputs the\ncurrent mode (policy mask). Agents are regu-\nlarized to predict the same mask from their lo-\ncal observations and optimize the correspond-\ning sub-policy.\n6.5\nRelated Work\nSeveral works in MARL consider explicit communication channels between the agents and\ndistinguish between communicative actions (e.g. broadcasting a given message) and physical\nactions (e.g.\nmoving in a given direction) (Foerster et al., 2016; Mordatch and Abbeel,\n2018; Lazaridou et al., 2016). Consequently, they often focus on the emergence of language,\nconsidering tasks where the agents must discover a common communication protocol to\nsucceed. Deriving a successful communication protocol can already be seen as coordination\nin the communicative action space and can enable, to some extent, successful coordination\nin the physical action space (Ahilan and Dayan, 2019). Yet, explicit communication is not a\nnecessary condition for coordination as agents can rely on physical communication (Mordatch\nand Abbeel, 2018; Gupta et al., 2017).\nTeamReg falls in the line of work that explores how to shape agents’ behaviors with respect\nto other agents through auxiliary tasks. Strouse et al. (2018) use the mutual information\nbetween the agent’s policy and a goal-independent policy to shape the agent’s behavior\ntowards hiding or spelling out its current goal. However, this approach is only applicable for\ntasks with an explicit goal representation and is not specifically intended for coordination.\n69\nJaques et al. (2019b) approximate the direct causal effect between agent’s actions and use it\nas an intrinsic reward to encourage social empowerment. This approximation relies on each\nagent learning a model of other agents’ policies to predict its effect on them. In general, this\ntype of behavior prediction can be referred to as agent modelling (or opponent modelling)\nand has been used in previous work to enrich representations (Hernandez-Leal et al., 2019b;\nHong et al., 2017), to stabilise the learning dynamics (He et al., 2016) or to classify the\nopponent’s play style (Schadd et al., 2007).\nWith CoachReg, agents learn to unitedly recognize different modes in the environment and\nadapt by jointly switching their policy. This echoes with the hierarchical RL literature and\nin particular with the single agent options framework (Bacon et al., 2017) where the agent\nswitches between different sub-policies, the options, depending on the current state.\nTo\nencourage cooperation in the multi-agent setting, Ahilan and Dayan (2019) proposed that\nan agent, the “manager”, is extended with the possibility of setting other agents’ rewards\nin order to guide collaboration. CoachReg stems from a similar idea: reaching a consensus\nis easier with a central entity that can asymmetrically influence the group.\nYet, Ahilan\nand Dayan (2019) guides the group in terms of “ends” (influences through the rewards)\nwhereas CoachReg constrains it in terms of “means” (the group must synchronously switch\nbetween different strategies). Hence, the interest of CoachReg does not just lie in training\nsub-policies (which are obtained here through a simple and novel masking procedure) but\nrather in co-evolving synchronized sub-policies across multiple agents. Mahajan et al. (2019)\nalso looks at sub-policies co-evolution to tackle the problem of joint exploration, however\ntheir selection mechanism occurs only on the first timestep and requires duplicating random\nseeds across agents at test time. On the other hand, with CoachReg the sub-policy selection\nis explicitly decided by the agents themselves at each timestep without requiring a common\nsampling procedure since the mode recognition has been learned and grounded on the state\nthroughout training.\nFinally, Barton et al. (2018) propose convergent cross mapping (CCM) to measure the degree\nof effective coordination between two agents. Although this represents an interesting avenue\nfor behavior analysis, it fails to provide a tool for effectively enforcing coordination as CCM\nmust be computed over long time series making it an impractical learning signal for single-\nstep temporal difference methods.\nTo our knowledge, this work is the first to extend agent modelling to derive an inductive bias\ntowards team-predictable policies or to introduce a collective, agent induced, modulation\nof the policies without an explicit communication channel.\nImportantly, these coordina-\ntion proxies are enforced throughout training only, which allows to maintain decentralised\n70\nexecution at test time.\n6.6\nTraining environments\nOur continuous control tasks are built on OpenAI’s multi-agent particle environment (Mor-\ndatch and Abbeel, 2018). SPREAD and CHASE were introduced by (Lowe et al., 2017).\nWe use SPREAD as is but with sparse rewards. CHASE is modified with a prey controlled\nby repulsion forces so that only the predators are learnable, as we wish to focus on coor-\ndination in cooperative tasks. Finally we introduce COMPROMISE and BOUNCE where\nagents are physically tied together. While positive return can be achieved in these tasks by\nselfish agents, they all benefit from coordinated strategies and maximal return can only be\nachieved by agents working closely together. Figure 6.4 presents a visualization and a brief\ndescription. In all tasks, agents receive as observation their own global position and velocity\nas well as the relative position of other entities. A more detailed description is provided\nin Appendix B.2. Note that work showcasing experiments on these environments often use\ndiscrete action spaces and dense rewards (e.g. the proximity with the objective) (Iqbal and\nSha, 2019; Lowe et al., 2017; Jiang and Lu, 2018). In our experiments, agents learn with\ncontinuous action spaces and from sparse rewards which is a far more challenging setting.\n6.7\nResults and Discussion\nThe proposed methods offer a way to incorporate new inductive biases in CTDE multi-agent\npolicy search algorithms. We evaluate them by extending MADDPG, one of the most widely\nused algorithm in the MARL literature. We compare against vanilla MADDPG as well as\ntwo of its variants in the four cooperative multi-agent tasks described in Section 6.6. The first\nvariant (DDPG) is the single-agent counterpart of MADDPG (decentralized training). The\nsecond (MADDPG + sharing) shares the policy and value-function models across agents.\nAdditionally to the two proposed algorithms and the three baselines, we present results for\ntwo ablated versions of our methods. The first ablation (MADDPG + agent modelling) is\nsimilar to TeamReg but with λ2 = 0, which results in only enforcing agent modelling and\nnot encouraging agent predictability. The second ablation (MADDPG + policy mask) uses\nthe same policy architecture as CoachReg, but with λ1,2,3 = 0, which means that agents still\npredict and apply a mask to their own policy, but synchronicity is not encouraged.\nTo offer a fair comparison between all methods, the hyperparameter search routine is the\nsame for each algorithm and environment (see Appendix B.5.1). For each search-experiment\n(one per algorithm per environment), 50 randomly sampled hyperparameter configurations\n71\nFigure 6.4 Multi-agent tasks we employ. (a) SPREAD: Agents must spread out and cover a\nset of landmarks. (b) BOUNCE: Two agents are linked together by a spring and must position\nthemselves so that the falling black ball bounces towards a target. (c) COMPROMISE: Two\nlinked agents must compete or cooperate to reach their own assigned landmark. (d) CHASE:\nTwo agents chase a (non-learning) prey (turquoise) that moves w.r.t repulsion forces from\npredators and walls.\neach using 3 random seeds are used to train the models for 15, 000 episodes.\nFor each\nalgorithm-environment pair, we then select the best hyperparameter configuration for the\nfinal comparison and retrain them on 10 seeds for twice as long. This thorough evaluation\nprocedure represents around 3 CPU-year. We give all details about the training setup and\nmodel selection in Appendix B.3 and B.5.2. The results of the hyperparameter searches are\ngiven in Appendix B.5.5. Interestingly, Figure B.1 shows that our proposed coordination\nregularizers improve robustness to hyperparameters despite having more hyperparameters to\ntune.\n6.7.1\nAsymptotic Performance\nFigure 6.5 reports the average learning curves and Table 6.1 presents the final performance.\nCoachReg is the best performing algorithm considering performance across all tasks. Team-\nReg also significantly improves performance on two tasks (SPREAD and BOUNCE) but\nshows unstable behavior on COMPROMISE, the only task with an adversarial component.\nThis result reveals one limitation of this approach and is dicussed in details in Appendix B.6.\nNote that all algorithms perform similarly well on CHASE, with a slight advantage to the\none using parameter sharing; yet this superiority is restricted to this task where the optimal\nstrategy is to move symmetrically and squeeze the prey into a corner. Contrary to popular\nbelief, we find that MADDPG almost never significantly outperforms DDPG in these sparse\nreward environments, supporting the hypothesis that while CTDE algorithms can in princi-\nple identify and reinforce highly rewarding coordinated behavior, they are likely to fail to do\nso if not incentivized to coordinate.\n72\nTable 6.1 Final performance reported as mean return over agents averaged across 10 episodes\nand 10 seeds (± SE).\nenv\nalg\nDDPG\nMADDPG\nMADDPG\n+sharing\nMADDPG\n+agent modelling\nMADDPG\n+policy mask\nMADDPG\n+TeamReg (ours)\nMADDPG\n+CoachReg (ours)\nSPREAD\n133 ± 12\n159 ± 6\n47 ± 8\n183 ± 10\n221 ± 11\n216 ± 12\n210 ± 12\nBOUNCE\n3.6 ± 1.4\n4.0 ± 1.6\n0.0 ± 0.0\n3.8 ± 1.5\n3.7 ± 1.1\n5.8 ± 1.3\n7.4 ± 1.2\nCOMPROMISE\n19.1 ± 1.2\n18.1 ± 1.1\n19.6 ± 1.5\n12.9 ± 0.9\n18.4 ± 1.3\n8.8 ± 0.9\n31.1 ± 1.1\nCHASE\n727 ± 87\n834 ± 80\n980 ± 64\n946 ± 69\n722 ± 82\n917 ± 90\n949 ± 54\n0\n15000\n30000\nEpisodes\n0\n100\n200\nReturn\nSPREAD\n0\n15000\n30000\nEpisodes\n0\n3\n6\n9\nBOUNCE\n0\n15000\n30000\nEpisodes\n0\n10\n20\n30\nCOMPROMISE\n0\n15000\n30000\nEpisodes\n0\n400\n800\n1200\nCHASE\nMADDPG + policy mask\nMADDPG + CoachReg (ours)\nMADDPG + agent modelling\nMADDPG + TeamReg (ours)\nMADDPG + sharing\nMADDPG\nDDPG\nFigure 6.5 Learning curves (mean return over agents) for our two proposed algorithms, two\nablations and three baselines on all four environments. Solid lines are the mean and envelopes\nare the Standard Error (SE) across the 10 training seeds.\nRegarding the ablated versions of our methods, the use of unsynchronized policy masks might\nresult in swift and unpredictable behavioral changes and make it difficult for agents to perform\ntogether and coordinate. Experimentally, “MADDPG + policy mask” performs similarly or\nworse than MADDPG on all but one environment, and never outperforms the full CoachReg\napproach. However, policy masks alone seem sufficient to succeed on SPREAD, which is\nabout selecting a landmark from a set. Finally “MADDPG + agent modelling” does not\ndrastically improve on MADDPG apart from one environment, and is always outperformed\nby the full TeamReg (except on COMPROMISE, see Appendix B.6) which supports the\nimportance of enforcing predictability alongside agent modeling.\n6.7.2\nEffects of enforcing predictable behavior\nHere we validate that enforcing predictability makes the agent-modelling task more success-\nful. To this end, we compare, on the SPREAD environment, the team-spirit losses between\nTeamReg and its ablated versions. Figure 6.6 shows that initially, due to the weight ini-\ntialization, the predicted and actual actions both have relatively small norms yielding small\nvalues of team-spirit loss. As training goes on (∼1000 episodes), the norms of the action-\nvector increase and the regularization loss becomes more important. As expected, MADDPG\n73\nleads to the worst team-spirit loss as it is not trained to predict the actions of other agents.\nWhen using only the agent-modelling objective (λ1 > 0), the agents significantly decrease the\nteam-spirit loss, but it never reaches values as low as when using the full TeamReg objective\n(λ1 > 0 and λ2 > 0). Note that the team-spirit loss increases when performance starts to\nimprove i.e. when agents start to master the task (∼8000 episodes). Indeed, once the return\nmaximisation signal becomes stronger, the relative importance of the auxiliary objective is\nreduced. Being predictable with respect to one-another may push agents to explore in a\nmore structured and informed manner in the absence of reward signal, as similarly pursued\nby intrinsic motivation approaches (Chentanez et al., 2005).\n6.7.3\nAnalysis of synchronous sub-policy selection\nIn this section we confirm that CoachReg yields the desired behavior: agents synchronously\nalternating between varied sub-policies.\nFigure 6.7 shows the average entropy of the mask distributions for each environment com-\npared to the entropy of Categorical Uniform Distributions of size k (k-CUD). On all the\nenvironments, agents use several masks and tend to alternate between masks with more va-\nriety (close to uniformly switching between 3 masks) on SPREAD (where there are 3 agents\nand 3 goals) than on the other environments (comprised of 2 agents). Moreover, the Hamming\nproximity between the agents’ mask sequences, 1 −Dh where Dh is the Hamming distance\n(i.e. the ratio of timesteps for which the two sequences are different) shows that agents are\nsynchronously selecting the same policy mask at test time (without a coach). Finally, we\nobserve that some settings result in the agents coming up with interpretable strategies, like\n0\n10000\n20000\n30000\nEpisodes\n0.0\n0.3\n0.6\n0.9\nTeam Spirit Loss\nMADDPG\nMADDPG + agent modelling\nMADDPG + TeamReg (ours)\nFigure 6.6 Effect of enabling and\ndisabling the coefficients λ1 and\nλ2 on the ability of agents to\npredict their teammates behav-\nior. Solid lines and envelope are\naverage and SE on 10 seeds on\nSPREAD.\nSPREAD\nBOUNCE\nCOMPROMISE\nCHASE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nH/Hmax, k = 4\nHmax, k = 4\nHmax, k = 3\nHmax, k = 2\nSPREAD\nBOUNCE\nCOMPROMISE\nCHASE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\nDh\nrandk = 2\nrandk = 3\nrandk = 4\nFigure 6.7 (Left) Average entropy of the policy mask\ndistributions for each task. Hmax,k is the entropy of a\nk-CUD. (Right) Average Hamming Proximity between\nthe policy mask sequence of agent pairs. randk stands\nfor agents independently sampling their masks from k-\nCUD. Error bars are the SE on 10 seeds.\n74\nthe one depicted in Figure B.5 in Appendix B.7.2 where the agents alternate between two\nsub-policies depending on the position of the target2.\n6.7.4\nExperiments on discrete action spaces\nWe evaluate our techniques on the more chal-\nlenging task of 3vs2 Google Research foot-\nball environment (Kurach et al., 2019). In\nthis environment, each agent controls an of-\nfensive player and tries to score against a\ndefensive player and a goalkeeper controlled\nby the engine’s rule-based bots. Here agents\nhave discrete action spaces of size 21, with\nactions like moving direction, dribble, sprint,\nshort pass, high pass, etc. We use as observa-\ntions 37-dimensional vectors containing play-\ners’ and ball’s coordinates, directions, etc.\nTable 2: Average Returns for 3v2 football\nMADDPG\n0.004 ± 0.002\nMADDPG + sharing\n0.005 ± 0.003\nMADDPG + TeamReg (ours)\n0.006 ± 0.003\nMADDPG + CoachReg (ours)\n0.088 ± 0.017\nFigure 6.8 Snapshot of the google research\nfootball environment 3vs1-with-keeper.\nThe algorithms presented in Table 2 were trained using 25 randomly sampled hyperparameter\nconfigurations. The best configuration was retrained using 10 seeds for 80,000 episodes of\n100 steps. Table 2 shows the mean return (± standard error across seeds) on the last 10,000\nepisodes. All algorithms but MADDPG + CoachReg fail to reliably learn policies that achieve\npositive return (i.e. scoring goals).\n6.8\nConclusion\nIn this work we motivate the use of coordinated policies to ease the discovery of successful\nstrategies in cooperative multi-agent tasks and propose two distinct approaches to promote\ncoordination for CTDE multi-agent RL algorithms. While the benefits of TeamReg appear\ntask-dependent – we show for example that it can be detrimental on tasks with a com-\npetitive component – CoachReg significantly improves performance on almost all presented\nenvironments. Motivated by the success of this single-step coordination technique, a promis-\ning direction is to explore model-based planning approaches to promote coordination over\nlong-term multi-agent interactions.\n2See animations at https://sites.google.com/view/marl-coordination/\n75\nBroader Impact\nIn this work, we present and study methods to enforce coordination in MARL algorithms.\nIt goes without saying that multi-agent systems can be employed for positive and negative\napplications alike. We do not propose methods aimed at making new applications possible or\nimproving a particular set of applications. We instead propose methods that allow to better\nunderstand and improve multi-agent RL algorithms in general. Therefore, we do not aim\nin this section at discussing the impact of Multi-Agent Reinforcement Learning applications\nthemselves but focus on the impact of our contribution: promoting multi-agent behaviors\nthat are coordinated.\nWe first observe that current Multi-Agent Reinforcement Learning (MARL) algorithms may\nfail to train agents that leverage information about the behavior of their teammates and that\neven when explicitly given their teammates observations, action and current policy during\nthe training phase. We believe that this is an important observation worth raising some\nconcern among the community since there is a widespread belief that centralized training\n(like MADDPG) should always outperform decentralize training (DDPG). Not only is this\nbelief unsupported by empirical evidence (at least in our experiments) but it also prevents\nthe community from investigating and tackling this flaw that is an important limitation for\nlearning safer and more effective multi-agent behavior. By not accounting for the behavior\nof its teammates, an agent could not adapt to a new teammate or even a change in the\nteammates behavior. This prevents current methods to be applied in the real world where\nthere is external perturbations and uncertainties and where an artificial agent may need to\ninteract with various different individuals.\nWe propose to focus on coordination and sketch a definition of coordination: an agent behav-\nior should be predictable given its teammate behavior. While this definition is restrictive, we\nbelieve that it is a good starting point to consider. Indeed, enforcing that criterion should\nmake learning agents more aware of their teammates in order to coordinate with them. Yet,\ncoordination alone does not ensure success, as agents could be coordinated in an unproduc-\ntive manner. More so, coordination could have detrimental effects if it enables an attacker\nto influence an agent through taking control of a teammate or using a mock-up teammate.\nFor these reasons, when using multi-agent RL algorithms (or even single-agent RL for that\nmatter) for real world applications, additional safeguards are absolutely required to prevent\nthe system from misbehaving, which is highly probable if out-of-distribution states are to be\nencountered.\n76\nAcknowledgements\nWe thank Olivier Delalleau for his insightful feedback and comments.\nWe also acknowl-\nedge funding in support of this work from the Fonds de Recherche Nature et Technologies\n(FRQNT) and Mitacs, as well as Compute Canada for supplying computing resources.\n77\nCHAPTER 7\nARTICLE 3: DIRECT BEHAVIOR SPECIFICATION VIA\nCONSTRAINED REINFORCEMENT LEARNING\nCo-authors\nRoger Girgis, Joshua Romoff, Pierre-Luc Bacon & Christopher Pal\nPublished in\nProceedings of Machine Learning Research, July 28, 2023\nAbstract\nThe standard formulation of Reinforcement Learning lacks a practical way of specify-\ning what are admissible and forbidden behaviors. Most often, practitioners go about\nthe task of behavior specification by manually engineering the reward function, a\ncounter-intuitive process that requires several iterations and is prone to reward hack-\ning by the agent. In this work, we argue that constrained RL, which has almost\nexclusively been used for safe RL, also has the potential to significantly reduce the\namount of work spent for reward specification in applied RL projects. To this end,\nwe propose to specify behavioral preferences in the CMDP framework and to use\nLagrangian methods to automatically weigh each of these behavioral constraints.\nSpecifically, we investigate how CMDPs can be adapted to solve goal-based tasks\nwhile adhering to several constraints simultaneously. We evaluate this framework\non a set of continuous control tasks relevant to the application of Reinforcement\nLearning for NPC design in video games.\n7.1\nIntroduction\nReinforcement Learning (RL) has shown rapid progress and lead to many successful applica-\ntions over the past few years (Mnih et al., 2013; Silver et al., 2017; Andrychowicz et al., 2020).\nThe RL framework is predicated on the simple idea that all tasks could be defined as a single\nscalar function to maximise, an idea generally referred to as the reward hypothesis (Sutton\nand Barto, 2018; Silver et al., 2021; Abel et al., 2021). This idea has proven very useful to\ndevelop the theory and concentrate research on a single theoretical framework. However, it\ncan be significantly limiting when translating a real-life problem into an RL problem, since\nthe question of where the reward function comes from is completely ignored (Singh et al.,\n2009). In practice, human-designed reward functions often lead to unforeseen behaviors and\n78\nrepresent a serious obstacle to the reliable application of RL in the industry (Amodei et al.,\n2016).\nConcretely, for an engineer working on applying RL methods to an industrial problem, the\ntask of reward specification implies to: (1) characterise the desired behavior that the system\nshould exhibit, (2) write in a computer program a reward function for which the optimal\npolicy corresponds to that desired behavior, (3) train an RL agent on that task using one\nof the methods available in the literature and (4) evaluate whether the agent exhibits the\nexpected behavior. Multiple design iterations of that reward function are generally required,\neach time accompanied by costly trainings of the policy (Hadfield-Menell et al., 2017; Dulac-\nArnold et al., 2019). This inefficient design loop is exacerbated by the fact that current\nDeep RL algorithms cannot be guaranteed to find the optimal policy (Sutton and Barto,\n2018), meaning that the reward function could be correctly specified but still fail to lead to\nthe desired behavior. The design problem thus becomes “What reward function would lead\nSAC (Haarnoja et al., 2018) or PPO (Schulman et al., 2017) to give me a policy that I find\nsatisfactory?”, a difficult puzzle that every RL practitioner has had to deal with.\nMost published work on Reinforcement Learning focuses on point (3) i.e. improving the\nreliability and efficiency with which these algorithms can yield a near-optimal policy for a\ngiven reward function. This line of work is crucial to allow RL to tackle difficult problems.\nHowever, as agents become more and more capable of solving the tasks we present them\nwith, our ability to (2) correctly specify these reward functions will only become more critical\n(Dewey, 2014).\nConstrained Markov Decision Processes (Altman, 1999) offer an alternative framework for\nsequential decision making.\nThe agent still seeks to maximise a single reward function,\nbut must do so while respecting a set of constraints defined by additional cost functions.\nWhile it is generally recognised that this formulation has the potential to allow for an easier\ntask definition from the end user (Ray et al., 2019), most work on CMDPs focuses on the\nsafety aspect of this framework i.e. that the constraint-satisfying behavior be maintained\nthroughout the entire exploration process (Achiam et al., 2017; Zhang et al., 2020; Turchetta\net al., 2020; Marchesini et al., 2022).\nIn this paper we specifically focus on the benefits\nof CMDPs relating to behavior specification. We make the following contributions: (1) we\nshow experimentally that reward engineering poorly scales with the complexity of the target\nbehavior, (2) we propose a solution where a designer can directly specify the desired frequency\nof occurrence of some events, (3) we develop a novel algorithmic approach capable of jointly\nsatisfying many more constraints and (4) we evaluate this framework on a set of constrained\ntasks illustrative of the development cycle required for deploying RL in video games.\n79\nFigure 7.1 Depictions of our setup to evaluate direct behavior specification using con-\nstrained RL; Arena environment (left); OpenWorld environment (right).\nFor videos see:\nhttps://sites.google.com/view/behaviorspecificationviacrl/home.\n7.2\nThe problem with reward engineering\nIn this section, we motivate the impracticality of using reward engineering to shape behavior.\nWe consider a navigation task in which the agent has to reach a goal location while being\nsubject to additional behavioral constraints. These constraints are (1) looking at a visible\nmarker 90% of the time, (2) avoiding forbidden terrain 99% of the time and (3) avoiding to\nrun out of energy also 99% of the time. The environment is depicted in Figure 7.1 (left) and\nthe details are presented in Appendix C.2. The reward function for this task is of the form:\nR′(s, a) = R(s, a) −1 ∗wnot-looking −1 ∗win-lava −1 ∗wno-energy\n(7.1)\nwhere R(s, a) gives a small shaping reward for progressing towards the goal and a terminal\nreward for reaching the goal, and the 1’s are indicator functions which are only active if their\ncorresponding behavior is exhibited.\nThe main challenge for an RL practitioner is to determine the correct values of the weights\nwnot-looking, win-lava and wno-energy such that the agent maximises its performance on the main\ntask while respecting the behavioral requirements, a problem often referred to as reward\nengineering. Setting these weights too low results in an agent that ignores these requirements\nwhile setting them too high distracts the agent from completing the main task. In general,\nknowing how to scale these components relatively to one another is not intuitive and is often\nperformed by trial and error across the space of reward coefficients wk. To illustrate where\nthe desired solutions can be found for this particular problem, we perform 3 grid searches\non 7 different values for each of these weights, ranging from 0.1 to 10 times the scale of the\nmain reward function, for the cases of 1, 2 and 3 behavioral constraints. The searches thus\nrespectively must go through 7, 49 and 343 training runs. Figure 7.2 (and Figure C.1 in\n80\na)\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\nAverage Episodic Return\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\nNot Looking at Marker < 0.10\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\nAverage Episodic Return\nfor feasible policies\nb)\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\nAverage Episodic Return\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nin Lava < 0.01\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nNot Looking at Marker < 0.10\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nAverage Episodic Return\nfor feasible policies\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 7.2 Enforcing behavioral constraints using reward engineering. Each grid represents\na different metric. Within each grid, each square represents the final performance (according\nto that metric) of an agent trained for 3M steps using the reward function in Equation 7.1\nparameterised as given by the grid coordinates. Performance is obtained by evaluating the\nagent on 1000 episodes. The leftmost column indicates the episodic return of the trained\npolicies, the middle columns indicates whether or not the agent respects the behavioral\nconstraint(s) and the rightmost column indicates the average return for these feasible policies\nonly. a) The “looking-at marker\" behavior does not affect too much the main task and,\nconsequently, all chosen weights allow to satisfy the constraint (looking at marker 90% of the\ntime) and many of them also lead to good performance on the main navigation task (−0.1 ≥\nw ≥−2). b) When also enforcing the “Not in Lava\" behavior, which is much more in the\nway of the main task, most of the resulting policies do not respect the constraint or perform\npoorly on the navigation task, highlighting the difficulty of choosing the correct penalty\nweights ahead of time. On 49 experiments, only two yielded good performing feasible policies:\n(−0.10, −2.0) and (−0.25, −1.0). On the largest search with 3 behavioral constraints, none of\nthe 343 experiments found a good performing feasible policy (see Figure C.1 in Appendix C.4).\nAppendix C.4) show the results of these experiments. We can see that a smaller and smaller\nproportion of these trials lead to successful policies as the number of behavioral constraints\ngrows. For an engineer searching to find the right trade-off, they find themselves cornered\nbetween two undesirable solutions: an ad-hoc manual approach guided by intuition or to run\na computationally demanding grid-search. While expert knowledge or other search strategies\ncan partially alleviate this burden, the approach of reward engineering clearly does not scale\nas the control problem grows in complexity.\nIt is important to note that whether or not it is the case that all tasks can in principle be\ndefined as a single scalar function to maximise i.e. the reward hypothesis (Sutton and Barto,\n2018), this notion should not be seen as a restrictive design principle when translating a\nreal-life problem into an RL problem. That is because it does not guarantee that this reward\n81\nfunction admits a simple form. Rich and multi-faceted behaviors may only be specifiable\nthrough a complex reward function (Abel et al., 2021) beyond the reach of human intuition.\nIn the next sections we present a practical framework in which CMDPs can be used to provide\na more intuitive and human-centric interface for behavioral specification.\n7.3\nBackground\nMarkov Decision Processes (MDPs)\n(Sutton and Barto, 2018) are formally defined\nthrough the following four components: (S, A, P, R). At timestep t, an agent finds itself in\nstate st ∈S and picks an action at ∈A(st). The transition probability function P encodes the\nconditional probability P(st+1|st, at) of transitioning to the next state st+1. Upon entering\nthe next state, an immediate reward is generated through a reward function R : S × A →\nR. In this paper, we restrict our attention to stationary randomized policies of the form\nπ(a|s) – which are sufficient for optimality in both MDPs and CMDPs (Altman, 1999). The\ninteraction of a policy within an MDP gives rise to trajectories (s0, a0, r0, . . . , sT, aT, RT)\nover which can be computed the sum of rewards which we call the return. Under the Markov\nassumption, the probability distribution over trajectories is of the form:\npπ(τ) := P0(s0)\nT\nY\nt=0\nP(st+1|st, at)π(at|st)\n(7.2)\nwhere P0 is some initial state distribution. Furthermore, any such policy induces a marginal\ndistribution over state-action pairs referred to as the visitation distribution or state-action\noccupation measure:\nxπ(s, a) :=\n1\nZ(γ, T)\nT\nX\nt=0\nγtpπ,t(St = s, At = a) ,\n(7.3)\nwhere Z(γ, T) = PT\nt=0 γt is a normalising constant.\nIn this paper, it is useful to extend the notion of return to any function f : S × A →R\nover states and actions other than the reward function of the MDP itself. The expected\ndiscounted sum of f then becomes:\nJf(π) := Eτ∼pπ\n\" T\nX\nt=0\nγtf(st, at)\n#\n(7.4)\nwhere γ ∈[0, 1] is a discount factor. While this idea is the basis for much of the work on\nGeneral Value Functions (GVFs) (White, 2015) for predictive state representation (Sutton\n82\net al., 2011), our focus here is on problem of behavior specification and not that of prediction.\nFinally, in the MDP setting, a policy is said to be optimal under the expected discounted\nreturn criterion if π∗= arg maxπ∈Π JR(π), where Π is the set of possible policies.\nConstrained MDPs (CMDPs)\n(Altman, 1999) is a framework that extends the notion\nof optimality in MDPs to a scenario where multiple cost constraints need to be satisfied in\naddition to the main objective. We write Ck : S × A →R to denote such a cost function\nwhose expectation must remain bounded below a specified threshold dk ∈R. The set of\nfeasible policies is then:\nΠC = {π ∈Π : JCk(π) ≤dk, k = 1, . . . , K}.\n(7.5)\nOptimal policies in the CMDP framework are those of maximal expected return among the\nset of feasible policies:\nπ∗= arg max\nπ∈Π\nJR(π), s.t. JCk(π) ≤dk , k = 1, . . . , K\n(7.6)\nWhile it is sufficient to consider the space of stationary deterministic policies in searching for\noptimal policies in the MDP setting, this is no longer true in general with CMDPs (Altman,\n1999) and we must consider the larger space of stationary randomized policies.\nLagrangian methods for CMPDs.\nSeveral recent works have found that the class of\nLagrangian methods for solving CMDPs is capable of finding good feasible solutions at con-\nvergence (Achiam et al., 2017; Ray et al., 2019; Stooke et al., 2020; Zhang et al., 2020). The\nbasis for this line of work stems from the saddle-point characterisation of the optimal solu-\ntions in nonlinear programs with inequality constraints (Uzawa et al., 1958; Polyak, 1970;\nKorpelevich, 1976). Intuitively, these methods combine the main objective JR and the con-\nstraints into a single function L called the Lagrangian. The relative weight of the constraints\nare determined by additional variables λk called the Lagrange multipliers. Applied in our\ncontext, this idea leads to the following min-max formulation:\nmax\nπ\nmin\nλ≥0 L(π, λ)\nL(π, λ) = JR(π) −\nK\nX\nk=1\nλk(JCk(π) −dk)\n(7.7)\nwhere we denoted λ := {λk}K\nk=1 for conciseness. Uzawa et al. (1958) proposed to find a\nsolution to this problem iteratively by taking gradient ascent steps of the Lagrangian L in\n83\nthe variable π and descent ones in λ. This is also the same gradient ascent-descent (Lin\net al., 2020) procedure underpinning many learning algorithms for Generative Adversarial\nNetworks (Goodfellow et al., 2014).\nThe maximization of the Lagrangian over the policy variables can be carried out by applying\nany existing unconstrained policy optimization methods to the new reward function L :\nS × A →R where:\nL(s, a) = R(s, a) −\nK\nX\nk=1\nλkCk(s, a).\n(7.8)\nFor the gradient w.r.t. the Lagrange multipliers λ, the term depending on π cancels out and\nwe are left with ∇λkL(π, λ) = −(JCk(π) −dk). The update is followed by a projection onto\nλk ≥0 using the max-clipping operator. If the constraint is violated (JCk(π) > dk), taking\na step in the opposite direction of the gradient will increase the corresponding multiplier λk,\nthus increasing the relative importance of this constraint in JL(π). Inversely, if the constraint\nis respected (JCk(π) < dk), the update will decrease λk, allowing the optimisation process to\nfocus on the other constraints and the main reward function R.\n7.4\nProposed Framework\nIn Reinforcement Learning, the reward function is often assumed to be provided apriori. For\nexample, in most RL benchmarking environments this is indeed the case and researchers can\nfocus on improving current algorithms at finding better policies, faster and more reliably. In\nindustrial applications however, several desiderata are often required for the agent’s behavior,\nand balancing these components into a single reward function is highly non-trivial. In the\nnext sections, we describe a framework in which CMDPs can be used for efficient behavior\nspecification.\n7.4.1\nIndicator cost functions\nThe difficulty of specifying the desired behavior of an agent using a single reward function\nstems from the need to tune the relative scale of each reward component. Moreover, finding\nthe most appropriate ratio becomes more challenging as the number of reward components\nincreases (see Section 7.2). While the prioritisation and saturation characteristics of CMDPs\nhelp factoring the behavioral specification problem (Ray et al., 2019), there remains im-\nportant design challenges. First, the CMDP framework allows for arbitrary forms of cost\nfunctions, again potentially leading to unforeseen behaviors. Second, specifying the appro-\npriate thresholds dk can be difficult to do solely based on intuition. For example, in the\n84\nmujoco experiments performed by Zhang et al. (2020), the authors had to run an uncon-\nstrained version of PPO (Schulman et al., 2017) to first estimate the typical range of values\nfor the cost infringements and then run their constrained solver over the appropriately chosen\nthresholds.\nWe show here that this separate phase of threshold estimation can be avoided completely if\nwe consider a subclass of CMDPs that allows for a more intuitive connection between the\nchosen cost functions Ck and their expected returns JCk. More specifically, we restrict our\nattention to CMDPs where the cost functions are defined as indicators of the form:\nCk(s, a) = I(behavior k is met in (s, a))\n(7.9)\nwhich simply expresses whether an agent showcases some particular behavior k when selecting\naction a in state s. An interesting property of this design choice is that, by rewriting the\nexpected discounted sum of these indicator cost functions as an expectation over the visitation\ndistribution of the agent, we can interpret this quantity as a re-scaled probability that the\nagent exhibits behavior k at any given time during its interactions with the environment:\nJCk(π) = Eτ∼pπ\n\" T\nX\nt=0\nγtCk(st, at)\n#\n(7.10)\n= Z(γ, T)E(s,a)∼xπ(s,a)[Ck(s, a)]\n(7.11)\n= Z(γ, T)E(s,a)∼xπ(s,a)[I(behavior k met in (s, a))]\n(7.12)\n= Z(γ, T)Pr\n\u0010\nbehavior k met in (s, a)\n\u0011\n, (s, a) ∼xπ\n(7.13)\nDividing each side of JCk(π) ≤dk by Z(γ, T), we are left with ˜dk, a normalized constraint\nthreshold for the constraint k which represents the desired rate of encountering the behavior\ndesignated by the indicator cost function Ck. In practice, we simply compute the average\ncost function across the batch to give equal weighting to all state-action pairs regardless of\ntheir position t in the trajectory:\n˜JCk(π) := 1\nN\nN\nX\ni=1\nCk(si, ai)\n(7.14)\nwhere i is the sample index from the batch. We also train the corresponding critic Q(k) using\na discount factor γk < 1 for numerical stability.\nWhile the class of cost functions defined in Equation 7.9 still allows for modelling a large\nvariety of behavioral preferences, it has the benefit of informing the user on the range of\nappropriate thresholds – a probability ˜dk ∈[0, 1] – and the semantics is clear regarding its\n85\neffect on the agent’s behavior (assuming that the constraint is binding and that a feasible\npolicy is found). This effectively allows for minimal to no tuning behavior specification (or\n“zero-shot” behavior specification).\nFinally, indicator cost functions also have the practical advantage of allowing to capture\nboth desired and undesired behaviors without affecting the termination tendencies of the\nagent. Indeed, when using an arbitrary cost function, it could be tempting to simply flip\nits sign to enforce the opposite behavior. However, as noted in previous work (Kostrikov\net al., 2018), the choice of whether to enforce behaviors through bonuses or penalties should\ninstead be thought about with the termination conditions in mind. A positive bonus could\ncause the agent to delay termination in order to accumulate more bonuses while negative\npenalties could shape the agent behavior such that it seeks to trigger the termination of the\nepisode as soon as possible. Indicator cost functions are thus very handy in that they offer\na straightforward way to enforce the opposite behavior by simply inverting the indicator\nfunction Not\n\u0010\nI(s, a)\n\u0011\n= 1−I(s, a) without affecting the sign of the constraint (penalties v.s.\nbonuses).\n7.4.2\nMultiplier normalisation\nWhen the constraint k is violated, the multiplier λk associated with that constraint increases\nto put more emphasis on that aspect of the overall behavior. While it is essential for the\nmultipliers to be able to grow sufficiently compared to the main objective, a constraint that\nenforces a behavior which is long to discover can end up reaching very large multiplier values.\nIt then leads to very large policy updates and destabilizes the learning dynamics.\nTo maintain the ability of one constraint to dominate the policy updates when necessary\nwhile keeping the scale of the updates bounded, we propose to normalize the multipliers.\nThis can be readily implemented by using a softmax layer:\nλk =\nexp(zk)\nexp(a0) + PK\nk′=1 exp(zk′) ,\nk = 1, . . . , K\n(7.15)\nwhere zk are the base parameters for each one of the multipliers and a0 is a dummy variable\nused to obtain a normalized weight λ0 := 1 −PK\nk=1 λk for the main objective JR(π). The\ncorresponding min-max problem becomes:\nmax\nπ\nmin\nz1:K≥0 L(π, λ)\nL(π, λ) = λ0JR(π) −\nK\nX\nk=1\nλk(JCk(π) −dk)\n(7.16)\n86\n7.4.3\nBootstrap Constraint\nIn the presence of many constraints, one difficulty that emerges with the above multiplier\nnormalisation is that the coefficient of the Lagrangian function that weighs the main objective\nis constrained to be λ0 = 1−PK\nk=1 λk, which leaves very little to no traction to improve on the\nmain task while the process is looking for a feasible policy. Furthermore, as more constraints\nare added, the optimisation path becomes discontinuous between regions of feasible policies,\npreventing learning progress on the main task objective.\nA possible solution is to grant the main objective the same powers as the behavioral con-\nstraints that we are trying to enforce. This can be done by defining an additional function\nSK+1(s, a) which captures some measure of success on the main task. Indeed, many RL\ntasks are defined in terms of such sparse, clearly defined success conditions, and then often\nonly augmented with a dense reward function to guide the agent toward these conditions\n(Ng et al., 1999). A so-called success constraint of the form JSK+1(π) ≥˜dK+1 can thus be\nimplemented using an indicator cost function as presented above and added to the existing\nconstraint set {JCk(π) ≤˜dk}K\nk=1. While the use of a success constraint alone can be expected\nto aid learning of the main task, it is only a sparse signal and could be very difficult to\ndiscover if the main task is itself challenging. Since the success function SK+1 is meant to be\nhighly correlated with the reward function R, by going a step further and using the success\nconstraint multiplier λK+1 in place of the reward multiplier λ0, we can take full advantage\nof the density of the main reward function when enforcing that constraint.\nHowever, to\nmaintain a true maximisation objective over the main reward function, we still need to keep\nusing λ0 when other constraints are satisfied, so that the most progress can be made on\nJR(π). We thus take the largest of these two coefficients for weighing the main objective\n˜λ0 := max\n\u0010\nλ0, λK+1\n\u0011\nand replace λ0 with ˜λ0 in Equation 7.16. Here we say that constraint\nK + 1 is used as a bootstrap constraint.\nOur method of encoding a success criterion in the constraint set can be seen as a way of\nrelaxing the behavioral constraints during the optimisation process without affecting the\nconvergence requirements.\nFor example, in previous work, Calian et al. (2020) tune the\nlearning rate of the Lagrange multipliers to automatically turn some constraints into soft-\nconstraints when the agent is not able to satisfy them after a given period of time. Instead,\nthe bootstrap constraint allows to start making some progress on the main task without\nturning our hard constraints into soft constraints.\n87\n7.5\nRelated Work\nConstrained Reinforcement Learning.\nCMDPs (Altman, 1999) have been the focus\nof several previous work in Reinforcement Learning. Lagrangian methods (Borkar, 2005;\nTessler et al., 2018; Stooke et al., 2020) combine the constraints and the main objective into\na single function and seek to find a saddle point corresponding to feasible solutions to the\nmaximisation problem. Projection-based methods (Achiam et al., 2017; Chow et al., 2019;\nYang et al., 2020; Zhang et al., 2020) instead use a projection step to try to map the policy\nback into a feasible region after the reward maximisation step. While most of these works\nfocus on the single-constraint case (Zhang et al., 2020; Dalal et al., 2018; Calian et al., 2020;\nStooke et al., 2020) and seek to minimize the total regret over the cost functions throughout\ntraining (Ray et al., 2019), we focus on the potential of CMDPs for precise and intuitive\nbehavior specification and work on satisfying many constraints simultaneously.\nReward Specification.\nImitation Learning (Zheng et al., 2021) is largely motivated by\nthe difficulty of designing reward functions and instead seeks to use expert data to define\nthe task. Other approaches introduce a human in the loop to either guide the agent towards\nthe desired behavior (Christiano et al., 2017) or to prevent it from making catastrophic\nerrors while exploring the environment (Saunders et al., 2017). While our approach of using\nCMDPs for behavior specification also seeks to make better use of human knowledge, we\nfocus on the idea of providing this knowledge by simply specifying thresholds and indicator\nfunctions rather than requiring expert demonstrations or constant human feedback. Another\nline of work studies whether natural language can be used as a more convenient interface\nto specify the agent’s desired behavior (Goyal et al., 2019; MacGlashan et al., 2015). While\nthis idea presents interesting perspectives, natural language is inherently ambiguous and\nprone to reward hacking by the agent. Moreover such approaches generally come with the\nadded complexity of having to learn a language-to-reward model. Finally, others seek to\nsolve reward mis-specification through Inverse Reward Design (Hadfield-Menell et al., 2017;\nMindermann et al., 2018; Ratner et al., 2018) which treats the provided reward function as a\nsingle observation of the true intent of the designer and seeks to learn a probabilistic model\nthat explains it. While this approach is interesting for adapting to environmental changes,\nwe focus on behavior specification in fixed-distribution environments.\nRL in video games.\nVideo games have been used as a benchmark for Deep RL for several\nyears (Shao et al., 2019; Berner et al., 2019; Vinyals et al., 2019). However, examples of\nRL being used in a video game production are limited due to a variety of factors which\n88\ninclude the difficulty of shaping behavior, interpretability, and compute limitations at run-\ntime (Jacob et al., 2020; Alonso et al., 2020). Still, there has been a recent push in the video\ngame industry to build NPCs (Non Player Characters) using RL, for applications including\nnavigation (Alonso et al., 2020; Devlin et al., 2021), automated testing (Bergdahl et al., 2020;\nGordillo et al., 2021), play-style modeling (de Woillemont et al., 2021) and content generation\n(Gisslén et al., 2021).\n7.6\nExperiments\nTo evaluate the proposed framework, we train SAC agents (Haarnoja et al., 2018) to solve\nnavigation tasks with up to 5 constraints imposed on their behavior. Many of these con-\nstraints interact with the main task and with one another which significantly restricts the\nspace of admissible policies. We conduct most of our experiments in the Arena environment\n(see Figure 7.1, left)1 where we seek to verify the capacity of the proposed framework to allow\nfor easy specification of the desired behavior and the ability of the algorithm to deal with\na large number of constraints simultaneously. We also perform an experiment in the Open-\nWorld environment (see Figure 7.1, right), a much larger and richer map generated using the\nGameRLand map generator (Beeching et al., 2021), where we seek to verify the scalability\nof that approach and whether it fits the needs of agent behavior specification for the video\ngame industry. See Appendices C.2 and C.3 for a detailed description of both experimental\nsetups.\n7.6.1\nExperiments in the Arena environment\nMultiplier Normalization\nOur first set of experiments showcases the effect of normalizing\nthe Lagrange multipliers. For illustrative purposes, we designed a simple scenario where one\nof the constraints is not satisfied for a long period of time. Specifically, the agent is attempting\nto satisfy an impossible constraint of never touching the ground. Figure 7.3 (in red) shows\nthat the multiplier on the unsatisfied constraint endlessly increases in magnitude, eventually\nharming the entire learning system; the loss on the critic diverges and the performance\ncollapses.\nWhen using our normalization technique, Figure 7.3 (in blue) shows that the\nmultiplier and critic losses remain bounded, avoiding such instabilities.\n1The algorithm is presented in Appendix C.1.\nThe code for the Arena environment experiments is\navailable at:\nhttps://github.com/ubisoft/DirectBehaviorSpecification\n89\n0.00\n0.25\n0.50\n0.75\n1.00\nEnvironment steps\n1e7\n0.00\n0.25\n0.50\n0.75\n1.00\non Ground\nbehavior rate\nconstraint threshold\n0.00\n0.25\n0.50\n0.75\n1.00\nEnvironment steps\n1e7\n0.0\n0.8\n1.6\n2.4\nMultipliers\nunormalized multipliers\nnormalized multipliers\n0.00\n0.25\n0.50\n0.75\n1.00\nEnvironment steps\n1e7\n0\n2500\n5000\n7500\nCritic Loss\n0.00\n0.25\n0.50\n0.75\n1.00\nEnvironment steps\n1e7\n1\n0\n1\n2\nAverage Return\nFigure 7.3 The multiplier normalisation keeps the learning dynamics stable when discovering\na constraint-satisfying behavior takes a large amount of time. To simulate such a case, an\nimpossible constraint is set for 7.5M steps and then replaced by a feasible one for the last\n2.5M steps. The method using unormalized multipliers (red) keeps taking larger and larger\nsteps in policy space leading to the divergence of its learning dynamics and complete collapse\nof its performance.\n0\n1\n2\n3\n0.0\n0.5\n1.0\n1.5\n2.0\nAverage\nReturn\nNot Looking at Marker\n0\n1\n2\n3\n0.0\n0.5\n1.0\n1.5\n2.0\nNot on Ground\n0\n1\n2\n3\n0.0\n0.5\n1.0\n1.5\n2.0\nin Lava\n0\n1\n2\n3\n0.0\n0.5\n1.0\n1.5\n2.0\nabove Speed Limit\n0\n1\n2\n3\n0.0\n0.5\n1.0\n1.5\n2.0\nunder Energy Limit\n0\n1\n2\n3\nEnvironment Steps (M)\n0.0\n0.3\n0.6\n0.9\nAverage\nBehavior Rate\n0\n1\n2\n3\nEnvironment Steps (M)\n0.0\n0.2\n0.4\n0.6\n0\n1\n2\n3\nEnvironment Steps (M)\n0.00\n0.04\n0.08\n0.12\n0\n1\n2\n3\nEnvironment Steps (M)\n0.00\n0.15\n0.30\n0.45\n0\n1\n2\n3\nEnvironment Steps (M)\n0.00\n0.06\n0.12\n0.18\nFigure 7.4 Each column presents the results for an experiment in which the agent is trained\nfor 3M steps with a single constraint enforced on its behavior. Training is halted after every\n20, 000 environment steps and the agent is evaluated for 10 episodes. All curves show the\naverage over 5 seeds and envelopes show the standard error around that mean. The top row\nshows the average return, the bottom row shows the average behavior rate on which the\nconstraint is enforced. The black doted lines mark the constraint thresholds.\nSingle Constraint satisfaction\nWe use our framework to encode the different behavioral\npreferences into indicator functions and specify their respective thresholds. Figure 7.4 shows\nthat our SAC-Lagrangian with multiplier normalisation can solve the task while respecting\nthe behavioral requirements when imposed with constraints individually. We note that the\ndifferent constraints do not affect the main task to the same extent; while some still allow to\nquickly solve the navigation task, like the behavioral requirement to avoid jumping, others\nmake the navigation task significantly more difficult to solve, like the requirement to avoid\ncertain types of terrain (lava).\nMultiple Constraints Satisfaction\nIn Figure 7.5 we see that when imposed with all of\nthe constraints simultaneously, the agent learns a feasible policy but fails at solving the main\ntask entirely. The agent effectively settles on a trivial behavior in which it only focuses on\n90\nFigure 7.5 Each row presents the results of an experiment in which an agent is trained for\n10M steps. Training is halted after every 20, 000 environment steps and the agent is evaluated\nfor 10 episodes. All curves show the average over 5 seeds and envelopes show the standard\nerror around that mean. (a) Unconstrained SAC agent; none of the behavioral preferences\nare enforced and consequently improvement on performance is very fast but none of the\nconstraints are satisfied. (b) SAC-Lagrangian with the 5 behavioral constraints enforced.\nWhile each constraint was successfully dealt with when imposed one by one (see Figure 7.4),\nmaximising the main objective when subject to all the constraints simultaneously proves to\nbe much harder. The agent does not find a policy that improves on the main task while\nkeeping the constraints in check. (c) By using an additional success constraint (that the\nagent should reach its goal in 99% of episodes), the agent can cut through infeasible policy\nspace to start improving on the main task and optimise the remaining constraints later on.\n(d) By using the success constraint as a bootstrap constraint (bound to the main reward\nfunction) improvement on the main task is much faster as the agent benefits from the dense\nreward function to improve on the goal-reaching task.\nsatisfying the constraints, but from which it is very hard to move away without breaking the\nconstraints. By introducing a success constraint, the agent at convergence is able to satisfy\nall of the constraints as well as succeeding in the navigation task. This additional incentive\nto traverse infeasible regions of the policy space allows to find feasible but better performing\nsolutions. Our best results are obtained when using the success constraint as a bootstrap\nconstraint, effectively lending λK+1 to the main reward while the agent is still looking for a\nfeasible policy.\n7.6.2\nExperiment in the OpenWorld environment\nIn the OpenWorld environment, we seek to verify that the proposed solution scales well to\nmore challenging and realistic tasks. Contrarily to the Arena environment, the OpenWorld\n91\n0\n10\n20\n30\n40\n50\nEnvironment steps (M)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nNot Looking At Marker\n Behavior Rate\n0\n10\n20\n30\n40\n50\nEnvironment steps (M)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nNot on Ground\n Behavior Rate\n0\n10\n20\n30\n40\n50\nEnvironment steps (M)\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nIn Lava\n Behavior Rate\n0\n10\n20\n30\n40\n50\nEnvironment steps (M)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nUnder Energy Limit\n Behavior Rate\n0\n10\n20\n30\n40\n50\nEnvironment steps (M)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHas Reached Goal\n Behavior Rate\n0\n10\n20\n30\n40\n50\nEnvironment steps (M)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMultipliers\n0\n10\n20\n30\n40\n50\nEnvironment steps (M)\n1\n0\n1\n2\n3\nAverage Return\nFigure 7.6 A SAC-Lagrangian agent trained to solve the navigation problem in the Open-\nWorld environment while respecting four constraints and imposing the bootstrap constraint.\nResults suggest that our SAC-Lagrangian method using indicator cost functions, normalised\nmultipliers and bootstrap constraint scales well to larger and more complex environments.\ncontains uneven terrain, buildings, and interactable objects like jump-pads, which brings this\nevaluation setting much closer to an actual RL application in the video game industry. For\nthis experiment, we trained a SAC-Lagrangian agent to solve the navigation problem with\nfour constraints on its behavior: On-Ground, Not-In-Lava, Looking-At-Marker and Above-\nEnergy-Limit.\nThe SAC component uses the same hyperparameters as in Alonso et al.\n(2020). The results are shown in Figure 7.6. While training the agent in this larger and more\ncomplex environment now requires up to 50M environment steps, the agent still succeeds at\ncompleting the task and respecting the constraints, favourably supporting the scalability of\nthe proposed framework for direct behavior specification.\n7.7\nDiscussion\nOur work showed that CMDPs offer compelling properties when it comes to task specification\nin RL. More specifically, we developed an approach where the agent’s desired behavior is\ndefined by the frequency of occurrence for given indicator events, which we view as constraints\nin a CMDP formulation. We showed through experiments that this methodology is preferable\nover the reward engineering alternative where we have to do an extensive hyperparameter\nsearch over possible reward functions. We evaluated this framework on the many constraints\ncase in two different environments. Our experiments showed that simultaneously satisfying\na large number of constraints is difficult and can systematically prevent the agent from\nimproving on the main task.\nWe addressed this problem by normalizing the constraint\nmultipliers, which resulted in improved stability during training and proposed to bootstrap\nthe learning on the main objective to avoid getting trapped by the composing constraint set.\nThis bootstrap constraint becomes a way for practitioners to incorporate prior knowledge\nabout the task and desired result – if the threshold is strenuous, a high success is prioritized\n– if the threshold is lax, it will simply be used to exit the initialisation point and the other\n92\nconstraints will quickly takeover. Our overall method is easy to implement over existing\npolicy gradient code bases and can scale across domains easily.\nWe hope that these insights can contribute to a wider use of Constrained RL methods in\nindustrial application projects, and that such adoption can be mutually beneficial to the\nindustrial and research RL communities.\nAcknowledgments\nWe wish to thank Philippe Marcotte, Maxim Peter, Rémi Labory, Pierre Le Pelletier De\nWoillemont, Julien Varnier, Pierre Falticska, Gabriel Robert, Vincent Martineau, Olivier\nPomarez, Tristan Deleu and Paul Barde as well as the entire research team at Ubisoft La Forge\nfor providing technical support and insightful comments on this work. We also acknowledge\nfunding in support of this work from Fonds de Recherche Nature et Technologies (FRQNT),\nMitacs Accelerate Program, Institut de valorisation des données (IVADO) and Ubisoft La\nForge.\n93\nCHAPTER 8\nARTICLE 4: GOAL-CONDITIONED GFLOWNETS FOR\nCONTROLLABLE MULTI-OBJECTIVE\nMOLECULAR DESIGN\nCo-authors\nPierre-Luc Bacon, Christopher Pal & Emmanuel Bengio\nPresented at\nWorkshop on Challenges in Deployable Generative AI\nat the International Conference on Machine Learning, June 23, 2023\nAbstract\nIn recent years, in-silico molecular design has received much attention from the\nmachine learning community. When designing a new compound for pharmaceutical\napplications, there are usually multiple properties of such molecules that need to\nbe optimized: binding energy to the target, synthesizability, toxicity, EC50, and so\non. While previous approaches have employed a scalarization scheme to turn the\nmulti-objective problem into a preference-conditioned single objective, it has been\nestablished that this kind of reduction may produce solutions that tend to slide\ntowards the extreme points of the objective space when presented with a problem\nthat exhibits a concave Pareto front. In this work we experiment with an alternative\nformulation of goal-conditioned molecular generation to obtain a more controllable\nconditional model that can uniformly explore solutions along the entire Pareto front.\n8.1\nIntroduction\nModern Multi-Objective optimization (MOO) is comprised of a large number of paradigms\n(Keeney et al., 1993; Miettinen, 2012) intended to solve the problem of trading off between\ndifferent objectives; a setting particularly relevant to molecular design (Jin et al., 2020;\nJain et al., 2022b). One particular paradigm that integrates well with recent discrete deep-\nlearning based MOO is scalarization (Ehrgott, 2005; Pardalos et al., 2017), which transforms\nthe problem of discovering the Pareto front of a problem into a family of problems, each\ndefined by a set of coefficients over the objectives. One notable issue with such approaches\nis that the solution they give tends to depend on the shape of the Pareto front in objective\nspace (Emmerich and Deutz, 2018).\n94\nTo tackle this problem, we propose to train models which explicitly target specific regions\nin objective space. Taking inspiration from goal-conditional reinforcement learning (Schaul\net al., 2015a), we condition GFlowNet (Bengio et al., 2021, 2023) models on a description\nof such goal regions. Through the choice of distribution over these goals, we enable users of\nthese models to have more fine-grained control over trade-offs. We also find that assuming\nproper coverage of the goal distribution, goal-conditioned models discover a more complete\nand higher entropy approximation of the Pareto front than the scalarization approach.\n8.2\nBackground & Related Work\nThe Multi-Objective optimisation problem can be broadly described as the desire to\nmaximize a set of K objectives over X, R(x) ∈RK. In typical MOO problems, there is\nno single optimal solution x such that Rk(x) > Rk(x′) ∀k, x′. Instead, the solution set is\ngenerally composed of Pareto optimal points, which are points x that are not dominated by\nany other point, i.e. ∄x′ s.t. Rk(x) ≥Rk(x′) ∀k. In other words, a point is Pareto optimal if\nit cannot be locally improved. The projection in objective space of the set of Pareto optimal\npoints forms the so-called Pareto front.\nAs graph-based models improve (Rampášek et al., 2022) and more molecular data become\navailable (Wu et al., 2018), molecular design has become an active field of research within\nthe deep learning community (Brown et al., 2019b; Huang et al., 2021), and core to this\nresearch is the fact that molecular design is a fundamentally multi-objective search problem\n(Papadopoulos and Linke, 2006; Brown et al., 2006). The advent of such tools has led to\nvarious important work at the intersection of these two fields (Zhou et al., 2019; Ståhl et al.,\n2019; Jin et al., 2020; Jain et al., 2022b).\nThe Generative Flow Network (GFlowNet, GFN) framework is a recently introduced\nmethod to train energy-based generative models (i.e. models that learn pθ(x) ∝R(x); Bengio\net al., 2021).\nThey have now been successfully applied to a variety of settings such as\nbiological sequences (Jain et al., 2022a), causal discovery (Deleu et al., 2022; Atanackovic\net al., 2023), discrete latent variable modeling (Hu et al., 2023), and computational graph\nscheduling (Zhang et al., 2023). The framework itself has also received theoretical attention\n(Bengio et al., 2023), for example, highlighting its connections to variational methods (Zhang\net al., 2022; Malkin et al., 2022b), and several objectives to train GFNs have been proposed\n(Malkin et al., 2022a; Madan et al., 2022; Pan et al., 2023) including extensions to continuous\ndomains (Lahlou et al., 2023).\nIn the context of molecular design, GFlowNets have several important properties that make\n95\nthem an interesting method for this task. Notably, they are naturally well-suited for discrete\ncompositional object generation, and their multi-modal modeling capabilities allow them to\ninduce greater state space diversity in the solutions they find than previous methods. A\nrecent GFN-based approach to multi-objective molecular design, which we call preference-\nconditioning (Jain et al., 2022b), amounts to scalarizing the objective function by using a set\nof weights (or preferences) w:\nRw(x) =\nX\nk\nwkrk\n,\nX\nk\nwk = 1\n,\nwk ≥0\n(8.1)\nand then passing this preference vector w as input to the model. By sampling various w’s\nfrom a distribution such as Dirichlet’s during training, one can obtain a model that can\nbe conditioned to emphasize some preferred dimensions of the reward function. Jain et al.\n(2022b) also find that such a method finds diverse candidates in both state and objective\nspaces.\n8.3\nMethods\n8.3.1\nGoal-conditioned GFlowNets\nBuilding on the method of Jain et al. (2022b), our approach also formulates the problem\nas a conditional generative task but now imposes a hard constraint on the model: the goal\nis to generate samples for which the image in objective space falls into the specified goal\nregion. While many different goal-design strategies could be employed, we take inspiration\nfrom Lin et al. (2019b) and state that a sample x meets the specified goal g if the cosine\nsimilarity between its reward vector r and the goal direction dg is above the threshold cg:\ng := {r ∈RK :\nr·dg\n||r||·||dg|| ≥cg}.\nWe call such a goal a focus region, which represents a\nparticular choice of trade-off in the objective space (see Figure 8.1). The method can be\nconsidered a form of goal-conditional reinforcement learning (Schaul et al., 2015a), where the\nreward function Rg depends on the current goal g. In our case we have:\nRg(x) =\n\n\n\n\n\nP\nk rk,\nif r ∈g\n0,\notherwise\n(8.2)\nTo alleviate the effects of the now increased sparsity of the reward function Rg, we use a replay\nbuffer which proved to stabilise the learning dynamics of our models (see Appendix D.3.1).\nNotably, by explicitly formulating a goal, we can measure the goal-reaching accuracy of our\nmodel, which refers to the proportion of samples that successfully landed in their prescribed\n96\nFigure 8.1 The diagram on the left depicts the state space of a GFlowNet molecule generator\nwhich learns a forward policy that sequentially builds diverse molecules. a) The sampling dis-\ntribution learned by such a model on a two-objective problem (seh, qed). Each dot represents\na molecule’s image in the objective space. The focus region (see Section 8.3.1) is depicted\nas a light blue cone, and the colors represent the density of the distribution. The model\nlearns to produce molecules that mostly belong within the focus region. b) By training a\ngoal-conditioned GFlowNet and sampling from several focus regions (here showing 4 distinct\nregions), we can cover a wider section of the objective space and increase the diversity of\nproposed candidates.\nregion. This measurement enables us to employ hindsight experience replay (Andrychowicz\net al., 2017), which lets the model learn from the sampled trajectories that didn’t meet\ntheir goal. Finally, to further increase the goal-reaching accuracy we sharpen the reward\nfunction’s profile to help the model generate samples closer to the center of the focus region\n(see Appendix D.3.2).\n8.3.2\nLearned Goal Distribution\nPreference conditioning uses soft constraints to steer the model in some regions of the ob-\njective space. While hard constraints provide a more explicit way of incorporating the user’s\nintentions in the model (Amodei et al., 2016; Roy et al., 2021), they come with the unique\nchallenge that not every goal may be feasible. In such cases, the model will only observe\nsamples with a reward of 0 and thus return molecules of little interest drawn uniformly across\nthe state space. These “bad samples” are not harmful in themselves and can easily be filtered\nout. Still, their prominence will affect the sampling efficiency of goal-conditioned approaches\ncompared to their soft-constrained counterpart. Moreover, the number of infeasible regions\nwill likely multiply as the number of objectives grows, further aggravating this disparity. To\ncope with this challenge, we propose to use a simple tabular goal-sampler (Tab-GS) which\nmaintains a belief about whether any particular goal direction dg is feasible. Once learned,\nwe can start drawing new goals from it with a much lower likelihood on the goals that are\nbelieved to be infeasible, thus restoring most of the lost sample efficiency. We give more\n97\ndetails on this approach in Appendix D.3.3 and use it in our experiments in Section 8.4.3.\n8.3.3\nEvaluation Metrics\nWhile there exists many multi-objective scoring functions to choose from, any single metric\nonly partially captures the desirable properties of the learned generative distribution (Audet\net al., 2021). In this work, we focus on sampling high-performing molecules across the entire\nPareto front in a controllable manner at test time. With that in mind, we propose combining\nthree metrics to evaluate our solution. The first one, the Inverted Generational Distance\n(IGD) (Coello and Cortés, 2005), uses a set of reference points P (the true Pareto front) and\ntakes the average of the distance to the closest generated sample for each of these points:\nIGD(S, P) :=\n1\n|P|\nP\np∈P mins∈S ||s −p||2 where S = {si}N\ni=1 is the image in objective space\nof a set of N generated molecules si. When the true Pareto front is unknown, we use a\ndiscretization of the extreme faces of the objective space hypercube as reference points. IGD\nthus captures the width and depth at which our Pareto front approximation reaches out in\nthe objective space. The second metric, which we call the Pareto-Clusters Entropy (PC-\nent), measures how uniformly distributed the samples are along the true Pareto front. To\naccomplish this, we use the same reference points P as for IGD, and cluster together in\nthe subset Sj all of the samples si located closer to the reference point pj than any other\nreference point. PC-ent computes the entropy of the histogram of each counts |Sj|, reaching\nits maximum value of −log\n1\n|P| when all the samples are uniformly distributed relative to the\ntrue Pareto front: PC-ent(S, P) := −P\nj\n|Sj|\n|P| log |Sj|\n|P| . Finally, to report on the controllability\nof the compared methods, we measure the Pearson correlation coefficient (PCC) between the\nconditional vector c (goal or preference) and the resulting reward vector s, averaged across\nobjectives k: Avg-PCC(S, C) := 1\nK\nPK\nk=1 PCC(s·,k, c·,k).\n8.4\nResults\n8.4.1\nEvaluation Tasks\nWe primarily experiment on a two-objective task, the well-known drug-likeness heuristic\nQED (Bickerton et al., 2012), which is already between 0 and 1, and the sEH binding energy\nprediction of a pre-trained publicly available model (Bengio et al., 2021); we divide the output\nof this model by 8 to ensure it will likely fall between 0 and 1 (some training data goes past\nvalues of 8). For 3 and 4 objective tasks, we use a standard heuristic of synthetic accessibility\n(Ertl and Schuffenhauer, 2009) and a penalty for compounds exceeding a molecular weight\nof 300. See Appendix D.1 for all task and training details.\n98\n8.4.2\nComparisons in Difficult Objective Landscapes\nTo simulate the effect of complexifying the objective landscape while keeping every other\nparameter of the evaluation fixed, we incorporate unreachable regions, depicted in dark in\nFigures 8.2 & 8.3, by simply setting to null the reward function of any molecule whose image\nin the objective space would fall into these dark regions. We can see that the preference-\nconditioned approach can effectively solve problems exhibiting a convex pareto-front (Fig-\nure 8.2 & 8.3, columns 1-2). However, it is far less effective on problems exhibiting more\ncomplex objective landscapes. When faced with a concave Pareto front, the algorithm favours\nsolutions towards the extreme ends (Figure 8.2 & 8.3, columns 3-7). In contrast, by explicitly\nforcing the algorithm to sample from each trade-off direction in the objective space, our goal-\nconditioned method learns a sampling distribution that spans the entire space diagonally, no\nmatter how complex we make the objective landscape. Table 8.1 reports the performance of\nboth methods on these objective landscapes in terms of IGD, Avg-PCC and PC-ent (mean\n± sem, over 3 seeds).\nWe see in Table 8.1 that according to IGD, preference-conditioning and goal-conditioning per-\nform similarly in terms of pushing the empirical Pareto front forward. While the two learned\ndistributions are in many cases very different (Figure 8.2, columns 3-7), the preference-\nconditioning method still manages to produce a few samples in the middle areas of the\nPareto front, which satisfies IGD as it only looks for the single closest sample to each refer-\nence point. However, the two algorithms differ drastically in terms of controllability of the\ndistribution (color-coded in Figure 8.2) and uniformity of the distribution along the Pareto\nfront (color-coded in Figure 8.3), which are highlighted by the Avg-PCC and PC-ent criteria\nin Table 8.1.\nTable 8.1 Comparisons according to IGD, Avg-PCC and PC-ent between preference-\nconditioned and goal-conditioned GFNs on a set of increasingly difficult objective landscapes,\nmetrics reported on 3 seeds (mean ± sem).\nalgorithm\nunrestrained\nrestrained-convex\nconcave\nconcave-sharp\nmulti-concave\n4-dots\n16-dots\nIGD (↓)\npref-cond\n0.087 ± 0.001\n0.316 ± 0.002\n0.272 ± 0.001\n0.180 ± 0.002\n0.152 ± 0.006\n0.130 ± 0.011\n0.109 ± 0.009\ngoal-cond\n0.095 ± 0.002\n0.310 ± 0.001\n0.266 ± 0.001\n0.197 ± 0.002\n0.173 ± 0.004\n0.134 ± 0.002\n0.115 ± 0.004\nAvg-PCC (↑)\npref-cond\n0.905 ± 0.001\n0.673 ± 0.009\n0.830 ± 0.002\n0.855 ± 0.004\n0.700 ± 0.009\n0.768 ± 0.038\n0.770 ± 0.011\ngoal-cond\n0.967 ± 0.002\n0.953 ± 0.001\n0.926 ± 0.002\n0.915 ± 0.001\n0.946 ± 0.004\n0.928 ± 0.002\n0.948 ± 0.001\nPC-ent (↑)\npref-cond\n2.170 ± 0.004\n1.913 ± 0.019\n1.563 ± 0.009\n1.629 ± 0.002\n1.867 ± 0.015\n1.521 ± 0.022\n1.610 ± 0.019\ngoal-cond\n2.472 ± 0.006\n2.242 ± 0.013\n1.997 ± 0.002\n1.918 ± 0.001\n2.380 ± 0.020\n2.270 ± 0.025\n2.262 ± 0.014\n99\nFigure 8.2 Comparisons between a preference-conditioned GFN (top row) and a goal-\nconditioned GFN (bottom row) on a set of increasingly complex modifications of a two-\nobjective (seh, qed) fragment-based molecule generation task (Jain et al., 2022b). The BRG\ncolors represent the angle between the vector [1, 0] and either the preference-vector w (top)\nor the goal direction dg (bottom), respectively.\nFor example, in the case of preference-\nconditioning, a green dot means that such samples were produced with a strong preference\nfor the qed-objective, while in the goal-conditioning case, a green dot means that the model\nintended to produce a sample alongside the qed-axis. We see that goal-conditioning allows\nto span the entire objective space even in very challenging landscapes (columns 3-7) and in\na more controllable way.\nFigure 8.3 Comparisons of the same sampling distributions depicted in Figure 8.2. Now the\ncolors indicate how densely populated a particular area of the objective space is (brighter\nis more populated). We can see that by explicitly targeting different trade-off regions in\nobjective space, our goal-conditioning approach (bottom row) produces far more evenly dis-\ntributed samples along the Pareto front than with preference-conditioning (top row).\n100\n8.4.3\nComparisons for Increasing Number of Objectives\nUsing the same metrics, we also evaluate the performance of both methods when the number\nof objectives increases. As described in Section 8.3.2, to maintain the sample efficiency of\nour goal-conditioned approach we sample the goal directions dg from a learned tabular goal-\nsampler (Tab-GS) rather than uniformly across the objective space (Uniform-GS). We can\nsee in Table 8.2 (and in the ablation in Appendix D.3.3) that with this adaptation, our goal-\nconditioned approach maintains its advantages in terms of controllability and uniformity of\nthe learned distribution as the number of objectives increases, proving to be an effective\nmethod for probing large, high-dimensional objective spaces for diverse solutions.\nTable 8.2 Comparisons according to IGD, Avg-PCC and PC-ent between preference- and\ngoal-conditioned GFNs faced with increasing objectives (3 seeds, mean ± sem).\nalgorithm\n2 objectives\n3 objectives\n4 objectives\nIGD (↓)\npref-cond\n0.088 ± 0.001\n0.218 ± 0.003\n0.370 ± 0.000\ngoal-cond\n0.094 ± 0.004\n0.199 ± 0.002\n0.303 ± 0.001\nAvg-PCC (↑)\npref-cond\n0.904 ± 0.002\n0.775 ± 0.004\n0.612 ± 0.002\ngoal-cond\n0.961 ± 0.001\n0.909 ± 0.001\n0.893 ± 0.002\nPC-ent (↑)\npref-cond\n2.166 ± 0.007\n3.775 ± 0.016\n4.734 ± 0.004\ngoal-cond\n2.471 ± 0.001\n4.571 ± 0.008\n6.320 ± 0.009\n8.5\nFuture Work\nIn this work, we proposed goal-conditioned GFlowNets for multi-objective molecular design.\nWe showed that they are an effective solution to give practitioners more control over their\ngenerative models, allowing them to obtain a large set of more widely and more uniformly\ndistributed molecules across the objective space. An important limitation of the proposed\napproach was the reduced sample efficiency of the method due to the existence of a priori\nunknown infeasible goals. We proposed a tabular approach to gradually discredit these fruit-\nless goal regions as we explore the objective space. However, this set of parameters, one\nfor every goal direction, grows exponentially with the number of objectives K, eventually\nleading to statistical and memory limitations. As future steps, we plan to experiment with\na GFlowNet-based Goal Sampler (GFN-GS) which would learn to sample feasible goal di-\nrections dimension by dimension, thus benefiting from parameter sharing and the improved\nstatistical efficiency of its hierarchical structure.\n101\nFigure 8.4 Depiction of a GFlowNet Goal Sampler (GFN-GS) gradually building goal direc-\ntions dg one coordinate at a time, as a sequence of K steps.\nAcknowledgements\nWe wish to thank Berton Earnshaw, Paul Barde and Tristan Deleu as well as the entire\nresearch team at Recursion’s Emerging ML Lab for providing insightful comments on this\nwork. We also acknowledge funding in support of this work from Fonds de Recherche Nature\net Technologies (FRQNT), Institut de valorisation des données (IVADO) and Recursion\nPharmaceuticals.\n102\nCHAPTER 9\nGENERAL DISCUSSION\nReinforcement learning has shown significant potential in tackling complex sequential decision-\nmaking challenges in various real-world domains. More than a decade ago, the potential\napplications of this technology were already being demonstrated for production scheduling\n(Wang and Usher, 2005), aerobatic helicopter flight (Abbeel et al., 2006, 2010) and patient-\nprosthetic interfaces (Pilarski et al., 2011). Today, Deep RL algorithms are actively being\nused to improve energy efficiency (Luo et al., 2022), are investigated as a novel approach for\nplasma control in nuclear fusion reactors (Degrave et al., 2022) and hold great potential in\nmolecular design (Popova et al., 2018), addressing critical issues such as climate change and\ndisease treatment. However, the core principle of reward maximization, crucial to uncovering\ninnovative solutions, also poses a major challenge in its effective deployment. Designing pre-\ndictable and efficient reward functions is a complex task, and attempts at reward engineering\noften result in misaligned solutions or inefficient learning due to incomplete reward specifi-\ncation. In this chapter, we summarize the contributions of this thesis to the field of reward\nspecification in reinforcement learning and discuss their successes and limitations. We then\ncover additional considerations to alleviate this challenge and conclude by touching on some\nof the fundamental difficulties that make reward specification so persistent.\n9.1\nSucesses and Limitations\nOver the last decades, several families of strategies have been developed to guide and assist\nthe task of reward specification in reinforcement learning. In Chapter 3, we divide them\ninto two distinct categories: reward composition and reward modeling. Our contributions,\nsummarized in Chapter 4, span both of these paradigms.\nReward modeling aims to bypass the challenge of reward design by learning a model of\nthe reward function using human supervision. In Chapter 5, we present Adversarial Soft-\nAdvantage Fitting (ASAF), a method which takes advantage of the analytical solution of\nthe adversarial imitation learning problem to parameterize the discriminator in a way that\nallows to learn a near-optimal policy without performing any policy improvement step. This\napproach allows to accelerate the learning process and drastically simplify the implementation\nof adversarial imitation learning algorithms, making it easier to use and deploy. However,\nreward modeling has important limitations. First, supervision sources need to be available\n103\nfor the task at hand. In the case of imitation learning and inverse reinforcement learning, the\nalgorithm is provided with expert demonstrations. Such demonstrations can be expensive to\nobtain. For example, in the case of robotics, data collection may require the design of virtual\nreality simulations, haptic interfaces, or direct robot-manipulation by a human (Calinon et al.,\n2009; Zhang et al., 2018). In other cases, such as in molecular design, existing compounds\nmay represent interesting examples to serve as a starting point but may not be deemed\noptimal, thus restricting the ability of the model to discover new solutions with optimal\nproperties. Secondly, even when available, these demonstrations generally cannot cover all\nthe scenarios of interest. Overcoming this limitation leads to the challenges of generalizing\nbeyond the sampling distribution. The limited availability of human supervision and brittle\ngeneralization affect all reward modeling families, including those relying on cheaper input\nsources such as preference-based methods. Therefore, while demonstration data represent an\nimportant asset in accelerating the early phases of learning, in many applications, defining\nexplicity a numerical reward function remains the preferred solution.\nThe second paradigm, reward composition, regroups reward design strategies which aim at\nbuilding a reward function from multiple components. Some of these approaches are now\nwidely spread and well understood. For example, sparse rewards can often be augmented\nwith dense potential-based shaping, and this approach can drastically accelerate learning\nwhile theoretically preserving the set of optimal policies. However, this type of composition\nis limited in the type of reward functions that can be captured, and does not explicitly use all\nof the available information from the environment to shape the learned representation of the\npolicy. The use of auxiliary tasks aims to bridge that gap. Although not guaranteeing the\npreservation of the set of optimal policies, reasonable assumptions on the necessary properties\nof the solution set can be made to accelerate learning. For example, a navigation robot\noperating in a dynamic environment should be able to estimate its own velocity. Auxiliary\nobjectives are an attempt to make use of such additional learning signal to enrich the learned\nrepresentation of the policy. In Chapter 6, we present two auxiliary task approaches named\nTeam regularization and Coach regularization, which promote coordination between agents\nin cooperative multi-agent scenarios.\nWhen properly designed and well calibrated, such\nauxiliary tasks can yield significant gains in performance and sample efficiency by guiding\nthe exploration process towards more promising regions of the policy space. The limitations of\nauxiliary tasks are two-fold. First, if the proposed task is not sufficiently correlated with the\nmain objective, these additional learning signals can prove detrimental to the performance of\nthe agent. We demonstrate a case of such conflicting signals in the Compromise environment\nin Appendix B.6 where an agent trained to behave in a predictable way in an adversarial\nenvironment becomes subservient to its teammate and neglects to pursue its own goals. While\n104\nsome families of auxiliary tasks are less at risk of being detrimental, for example, when simply\nenforcing object detection in a vision-based model, such cases of conflicting auxiliary tasks\nare important reminders of the necessary prior knowledge required to craft effective inductive\nbiases. Moreover, approaches based on auxiliary tasks generally focus on learning efficiency\nand do not address the problem of alignment. We confront this problem more directly in our\ntwo last contributions using multi-objective paradigms.\nIn Chapter 7, we present a framework leveraging constrained RL for reward composition.\nConstrained reinforcement learning defines some components of a task as constraints to\nsatisfy. By restricting cost functions in the constrained MDP framework to identity func-\ntions, the expected discounted cost becomes probabilities of events on the agent’s visitation\ndistribution, forming a natural interface between the designer’s intentions and the agent’s\nbehavior. With this framework, for each aspect of behavior that we seek to enforce, a task\ndesigner simply needs to write a detection function and specify a target threshold. It may\nbe the case that not every aspect of behavior can be conveniently captured in the form of\nan indicator function, and this framework comes at the cost of longer training time since the\nweighting coefficient of each constraint need to be adapted in a bi-level optimization proce-\ndure. However, in practice, we find this particular family of cost functions to still retain a\nlot of expressivity in capturing very diverse disederata, and the additional training time is\ncompensated by saving several design iterations of the reward function to a task designer,\nthus coming out as a much more effective solution in the overall project development. By\nenforcing a more thorough and intentional specification procedure, this framework reduces\nthe risk of emergence of exploitative behaviors. These results highlight the benefits of limited\noptimization focusing on goal-satisfaction, rather than unbounded goal-maximization (Vam-\nplew et al., 2022). We have found the use of constraints to be much more intuitive than\nmanually weighting the reward components, and because the CMDP framework naturally\nreduces to the unconstrained case when only one objective is pursued, we suggest employing\na constrained approach as the default paradigm for reward specification over the traditional\nscalarized MDP approach.\nFinally, in Chapter 8, we turn our attention to a fundamentally multi-objective problem:\nmolecular design. We present Goal-Conditioned GFlowNets which leverage goal-conditioning\nas a way to specify the task in a controllable and flexible set of policies that can be adapted\nat test-time. While requiring additional computation to explore the set of Pareto-optimal\npolicies, conditional approaches allow to defer the final decision on user preferences to de-\nployment time, effectively postponing the problem of reward specification.\nThis method\nleverages the compression capabilities of neural parameterizations for solving the dilemma\nof reward design. Moreover, since rewards are strictly terminal, enforcing hard-constraints\n105\ncan be done efficiently by leveraging the proportional sampling property of GFlowNets using\nreward manipulation, without requiring a bi-level optimization procedure as in the case on\nreturn-based constraints. These constraints allow to cover the most complex of Pareto fronts,\nthus retaining a broad coverage of solutions independently of the properties of the objective\nspace.\nIn summary, this thesis explores the landscape of reward specification in reinforcement learn-\ning, delving into the realms of reward modeling and reward composition. Our contributions\nnavigate through the challenges and opportunities inherent in these approaches. From hu-\nman demonstrations to the use of auxiliary tasks, we have demonstrated the potential to\naccelerate learning using prior knowledge about the task to perform.\nSubsequently, our\nexploration of constrained reinforcement learning and goal-conditioned molecular design un-\nderlines the importance of intentional design and explicit goals. All of these methods present\nunique strengths and limitations, and the use of demonstrations, auxiliary tasks and multi-\nobjectivization should be combined to tackle complex real-world challenges most efficiently.\n9.2\nAdditional considerations\nThis thesis focuses on reward specification. However, there are other considerations that\nshould be taken into account to ensure efficient exploration and aligned behavior. In this\nsection, we briefly discuss the role of environment design, monitoring strategies, and human-\nin-the-loop to guide policy learning and address misaligned behavior.\nEnvironment design\nThe reward function in great part captures the task to be accomplished – for a fixed MDP,\ndifferent reward functions can lead to completely different behaviors. However, in real-world\nRL applications, engineers generally have agency over the design of the entire MDP, not just\nits reward function (Taylor, 2023).\nOne of the most fundamental decisions is the representation of the state space S. A correct\nstate specification must include all the information relevant to making the proper action\nselection and respect the Markov property. As a design decision, it can be difficult to choose\nbetween a preprocessed state vector which may omit some environment details but allow for\nefficient policy learning, and a rich higher-dimensional state representation such as images\nwhich makes all of the information available to the agent but requires significantly more\ntraining samples. The action space A also has a great influence on the difficulty of learning the\ntask. Reframing the problem to remove potentially harmful actions from the agent’s control\n106\nand narrowing the learned policy on a low-dimensional control vector can both accelerate\npolicy learning and reduce the risk of misalignment. In some cases, fixed subroutines can be\nused for finer control while the agent would be in charge of a more distant decision making\nprocedure. Finally, the definition of time-step t has great implications for both controllability\nand sample efficiency. An agent that acts more frequently will have more precise control over\nthe environment at the cost of more difficult credit assignment.\nAny given real-world task can typically be formulated as several different MDP instantiations,\nand the design choices behind these MDP components greatly impact the nature of the\nproblem to solve from the RL agent’s perspective. For example, starting from a well-defined\nobjective such as teaching a robot arm to play table tennis, D’Ambrosio et al. (2023) make\na myriad of design decisions to define the MDP, essentially transforming the engineering\nchallenge from hard-coding the robot behavior to designing an environment that can be\nsolved by reinforcement learning.\nThe challenge of environment design intertwines with different research areas. Hierarchical\nreinforcement learning aims at learning a hierarchy of agents where higher-level decision-\nmakers receive the true reward function and take temporally-extended action delegating\nfiner control to lower-level effectors (Dayan and Hinton, 1992). Higher-level managers pro-\nvide learned rewards to lower-level effectors, essentially bringing together the concepts of\nenvironment design and reward modeling by both specifying multiple MDPs at different lev-\nels while learning a model of the reward function to cascade the true signal down. Curriculum\nlearning in RL instead seeks to build a sequence of MDPs terminating with the real task to\nsolve (Chevalier-Boisvert et al., 2018). The first MDPs are meant to be easier to solve and\ntheir optimal policies are used as starting point for the next problem, allowing to gradually\nbuild up the control problem in its full complexity. This approach brings together notions of\nenvironment design and auxiliary tasks, decomposing the true problem into several MDPs of\nincreasing complexity.\nMonitoring\nTo find the best configuration for the algorithm, the environment and the reward function,\nRL projects often involve running hundreds of experiments in parallel. Due to this necessary\npractice, identifying misaligned behaviors becomes a complex task. Manually inspecting each\nagent’s trajectory is not only labor-intensive but also impractical for large-scale searches.\nTherefore, automated methods to pinpoint deviations from expected behaviors are essential\nfor the effective deployment of RL systems.\nA notable approach by Pan et al. (2022) suggests detecting significant shifts in agent behavior\n107\nby evaluating the divergence between the agent’s policy and a pre-examined, trusted policy.\nThis method illustrates the utility of expert policies or demonstrations not just in learning but\nalso in monitoring the progression of an agent’s behavior. Subsequently, when misalignment\nis observed, dissecting the reward components individually can shed light on the influences\ndriving the agent’s behavior. As discussed by Vamplew et al. (2018), this is particularly\npertinent in multi-objective frameworks.\nSome works have even explored decomposing a\nblack-boxed scalar rewards into interpretable components to elucidate the agent’s behavior\nin terms of trade-offs (Juozapaitis et al., 2019). Anderson et al. (2019) further experimented\nwith graphical interfaces that display the anticipated returns for each reward component to\nallow human observers to analyze surprising decisions from the agent.\nOverall, monitoring is a fundamental aspect in detecting, understanding, and rectifying issues\nrelated to reward misspecification. It presents significant research opportunities to develop\nmore interpretable agents to inform and guide the design of the reward function. With the\nincreasing adoption of RL in industry settings, this aspect of development will likely take a\ncentral place in more standardized development practices.\nHuman-in-the-loop\nPerfectly specifying a reward function on the first attempt for a complex task is highly\nunlikely. On the other hand, an iterative process is very costly and time consuming, as the\nagent needs to be trained to convergence between each attempt. To address this dilemma,\none idea consists in intervening during the agent training, which can be seen as a way to edit\nthe reward function on the fly, to re-specify it as the agent’s behavior starts taking form.\nThis paradigm can be referred to as Human-in-the-Loop (HiL) (Mosqueira-Rey et al., 2023).\nHiL can take the form of active learning (Settles, 2009), where the agent models its own\nuncertainty about the reward function and is responsible for querying human annotators for\nmore labels when its predictions are too ambiguous. Many reward modeling approaches allow\nfor such a feedback loop. In particular, both learning from demonstrations and learning from\npreferences have been combined with active learning frameworks to optimize the number\nof queries to human experts (Cui and Niekum, 2018; Sadigh et al., 2017). Further along\nthat spectrum, we find more interactive methods in which humans remain in control of\nwhen additional feedback should be provided. For example, Knox and Stone (2009, 2012)\npropose a framework where a human supervisor directly provides signals of approval and\ndisapproval, allowing the agent to learn to a policy without being responsible for credit\nassignment. Saunders et al. (2017) intervene to prevent the agent from making catastrophic\nerrors while learning to interact with its environment. Bajcsy et al. (2017) use human physical\n108\ninterventions on a robot arm to learn the parameters of its objective function.\nThese approaches operationalize the idea of widening the modelization of our relationship\nwith learning agents. In a similar line of thought, Jeon et al. (2020) propose to use the\nintervention itself as a source of feedback; the agent should take note that the fact that a\nhuman intervention was necessary is unacceptable. Taylor (2023) zooms back even more and\nargues that RL as a whole should be seen as a human-in-the-loop procedure, from the MDP\ndesign to the deployment of the solution. A general HiL perspective and concrete methods\nto implement it mitigate the challenges of reward specification by allowing the designers to\ncorrect course, both accelerating and re-aligning the agent’s learning in a live system.\n9.3\nReward Specification: A persistent challenge\nReward specification presents itself as a fundamental challenge in the realm of control algo-\nrithms. RL agents have a pervasive tendency to yield exploitative solutions, a characteristic\nthat seems to be inherent to automated learning systems, as this phenomenon has also been\nfrequently observed in digital evolution studies (Lehman et al., 2020). Both the progress\nin this field and the remaining difficulties invite for additional work. However, despite its\ntechnical nature, reward specification also has an ethical and psychological component, and\nthe disconcerting fragility of any particular reward design can in part be attributed to the\ndeep connections between the act of defining an objective and some core issues relating to\npolicy making, system design, and uncertainty.\nFirst, among a set of desiderata, objectives not only differ in their respective weights and\nlevels of priority but they also vary in nature. Certain rules exhibit flexibility, allowing a\ndegree of tolerance (e.g. be on time), while others hold axiomatic significance (e.g. do not\nkill). The balancing of these rules often manifests in societal decisions, where, for example, it\nmight be deemed tolerable to infringe on an individual’s rights to own land to facilitate the\nconstruction of a bridge or electrical infrastructure that can benefit the life of millions. For\nan agent to understand in which scenarios such trade-offs are acceptable requires significant\nknowledge about human cultures and values.\nSecondly, our capacity to measure aspects of a system is often limited. For instance, the\nobjective of training an agent to “play a game in an entertaining manner” is conceptually\nsimple yet presents considerable challenges in execution.\nCertain attributes, such as the\nconcept of entertainment, cannot be directly quantified. Moreover, there is often a lack of\nconsensus among individuals regarding what they consider entertaining. System designers\nthus have to make significant efforts to distill this overarching goal into a set of quantifiable\n109\ntargets. Despite our best efforts, the agent tends to develop behaviors that diverge from\nthe intended path, in a manner akin to how individuals and corporations invariably find\nloopholes to maximize profits, regardless how complex tax regulations might become. It is\nthis gap from a conceptual goal to a set of enforceable heuristics that creates the opportunity\nfor exploitative behavior and misalignment.\nFinally, users themselves are often uncertain about their own preferences. Humans regularly\nchange their minds, and external circumstances evolve, making yesterday’s targets unfit to\nmeet today’s needs. Consequently, a system that was effective under one set of preferences\nmay become less effective or even counterproductive as preferences evolve, necessitating con-\ntinuous adaptation and reevaluation of the algorithm and its objectives.\nThese challenges suggest that the problem of creating robust and adaptable reward functions\nwill persist as a continual and evolving aspect of the field, and mandate for multi-displinary\napproaches to designing such systems, favouring methods that allow non-technical stakehold-\ners to take part in the reward specification process of RL agents.\n110\nCHAPTER 10\nCONCLUSION\nReward specification is the process of providing a reinforcement learning agent with a reward\nfunction. It is designed such that, in maximizing its return, the agent is accomplishing a task\nfor us. This reward function is often engineered by hand through trial-and-error, iteratively\nrefined after each training cycle to speed up the agent’s learning and better align its behavior\nwith our objectives.\nHowever, due to the accumulation of the reward through time, the\ndifficulty to capture human intentions and the need to balance conflicting signals, many\nattempts lead to ineffective training and the emergence of undesireable behaviors. Crucially,\nthere is no one-size-fits-all solution to these challenges, and the choice of appropriate methods\ndepends on the availability of data and on the specific task at hand.\nIn this thesis, we have presented contributions to several important paradigms address-\ning these issues, starting from the use of auxiliary tasks and demonstrations to accelerate\nagent learning, to multi-objective formulations that incorporate several requirements in an\nagent’s behavior. We also surveyed complementary approaches such as preference-based and\nlanguage-guided reward modeling and discussed important considerations regarding environ-\nment design, monitoring and human-in-the-loop. All of these tools can work together. Deep\nreinforcement learning has the potential to tackle significant real-world problems and advance\nhuman knowledge. To deliver these benefits and function effectively within larger systems,\nRL frameworks must employ controllable and transparent strategies for reward specification,\nenable interpretable analysis and monitoring, and allow for rapid adaptations in the face of\nevolving circumstances and shifting goals.\n111\nREFERENCES\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S.,\nIrving, G., Isard, M., et al. (2016). {TensorFlow}: a system for {Large-Scale} machine\nlearning. In 12th USENIX symposium on operating systems design and implementation\n(OSDI 16), pages 265–283.\nAbbeel, P., Coates, A., and Ng, A. Y. (2010). Autonomous helicopter aerobatics through\napprenticeship learning. The International Journal of Robotics Research, 29(13):1608–1639.\nAbbeel, P., Coates, A., Quigley, M., and Ng, A. (2006). An application of reinforcement\nlearning to aerobatic helicopter flight. Advances in neural information processing systems,\n19.\nAbbeel, P. and Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning.\nIn Proceedings of the twenty-first international conference on Machine learning, page 1.\nAbbeel, P. and Ng, A. Y. (2005). Exploration and apprenticeship learning in reinforcement\nlearning. In Proceedings of the 22nd international conference on Machine learning, pages\n1–8.\nAbel, D., Dabney, W., Harutyunyan, A., Ho, M. K., Littman, M., Precup, D., and Singh, S.\n(2021). On the expressivity of markov reward. Advances in Neural Information Processing\nSystems, 34:7799–7812.\nAbels, A., Roijers, D., Lenaerts, T., Nowé, A., and Steckelmacher, D. (2019).\nDynamic\nweights in multi-objective deep reinforcement learning.\nIn International conference on\nmachine learning, pages 11–20. PMLR.\nAchiam, J., Held, D., Tamar, A., and Abbeel, P. (2017). Constrained policy optimization.\nIn International conference on machine learning, pages 22–31. PMLR.\nAdams, S., Cody, T., and Beling, P. A. (2022). A survey of inverse reinforcement learning.\nArtificial Intelligence Review, 55(6):4307–4346.\nAhilan, S. and Dayan, P. (2019). Feudal multi-agent hierarchies for cooperative reinforcement\nlearning. arXiv preprint arXiv:1901.08492.\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C.,\nGopalakrishnan, K., Hausman, K., et al. (2022). Do as i can, not as i say: Grounding\nlanguage in robotic affordances. arXiv preprint arXiv:2204.01691.\n112\nAissani, N., Beldjilali, B., and Trentesaux, D. (2008). Efficient and effective reactive schedul-\ning of manufacturing system using sarsa-multi-objective agents. In MOSIM’08: 7th Con-\nference Internationale de Modelisation et Simulation, pages 698–707.\nAkkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A.,\nPlappert, M., Powell, G., Ribas, R., et al. (2019). Solving rubik’s cube with a robot hand.\narXiv preprint arXiv:1910.07113.\nAkrour, R., Schoenauer, M., and Sebag, M. (2011). Preference-based policy learning. In\nMachine Learning and Knowledge Discovery in Databases: European Conference, ECML\nPKDD 2011, Athens, Greece, September 5-9, 2011. Proceedings, Part I 11, pages 12–27.\nSpringer.\nAlonso, E., Peter, M., Goumard, D., and Romoff, J. (2020). Deep reinforcement learning for\nnavigation in aaa video games. arXiv preprint arXiv:2011.04764.\nAltman, E. (1999). Constrained Markov Decision Processes. Stochastic Modeling Series.\nTaylor & Francis.\nAmin, S., Gomrokchi, M., Satija, H., van Hoof, H., and Precup, D. (2021). A survey of\nexploration methods in reinforcement learning. arXiv preprint arXiv:2109.00157.\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. (2016).\nConcrete problems in ai safety. arXiv preprint arXiv:1606.06565.\nAnderson, A., Dodge, J., Sadarangani, A., Juozapaitis, Z., Newman, E., Irvine, J., Chat-\ntopadhyay, S., Fern, A., and Burnett, M. (2019). Explaining reinforcement learning to\nmere mortals: An empirical study. arXiv preprint arXiv:1903.09708.\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew,\nB., Tobin, J., Abbeel, O. P., and Zaremba, W. (2017). Hindsight experience replay. In\nAdvances in neural information processing systems, pages 5048–5058.\nAndrychowicz, O. M., Baker, B., Chociej, M., Jozefowicz, R., McGrew, B., Pachocki, J.,\nPetron, A., Plappert, M., Powell, G., Ray, A., et al. (2020). Learning dexterous in-hand\nmanipulation. The International Journal of Robotics Research, 39(1):3–20.\nArora, S. and Doshi, P. (2021).\nA survey of inverse reinforcement learning: Challenges,\nmethods and progress. Artificial Intelligence, 297:103500.\n113\nArumugam, D., Karamcheti, S., Gopalan, N., Wong, L. L., and Tellex, S. (2017). Accu-\nrately and efficiently interpreting human-robot instructions of varying granularities. arXiv\npreprint arXiv:1704.06616.\nAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N.,\nMann, B., DasSarma, N., et al. (2021). A general language assistant as a laboratory for\nalignment. arXiv preprint arXiv:2112.00861.\nAtanackovic, L., Tong, A., Hartford, J., Lee, L. J., Wang, B., and Bengio, Y. (2023). Dyn-\ngfn: Bayesian dynamic causal discovery using generative flow networks. arXiv preprint\narXiv:2302.04178.\nAudet, C., Bigeon, J., Cartier, D., Le Digabel, S., and Salomon, L. (2021).\nPerfor-\nmance indicators in multiobjective optimization. European journal of operational research,\n292(2):397–422.\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016).\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nBacon, P.-L., Harb, J., and Precup, D. (2017). The option-critic architecture. In Thirty-First\nAAAI Conference on Artificial Intelligence.\nBahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and Grefenstette, E.\n(2018). Learning to understand goal specifications by modelling reward. arXiv preprint\narXiv:1806.01946.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S.,\nGanguli, D., Henighan, T., et al. (2022). Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.\nBajcsy, A., Losey, D. P., O’malley, M. K., and Dragan, A. D. (2017). Learning robot objectives\nfrom physical human interaction. In Conference on Robot Learning, pages 217–226. PMLR.\nBarrett, L. and Narayanan, S. (2008). Learning all optimal policies with multiple criteria. In\nProceedings of the 25th international conference on Machine learning, pages 41–47.\nBarton, S. L., Waytowich, N. R., Zaroukian, E., and Asher, D. E. (2018). Measuring collab-\norative emergent behavior in multi-agent reinforcement learning. In International Confer-\nence on Human Systems Engineering and Design: Future Trends and Applications, pages\n422–427. Springer.\n114\nBeeching, E., Peter, M., Marcotte, P., Debangoye, J., Simonin, O., Romoff, J., and Wolf, C.\n(2021). Graph augmented deep reinforcement learning in the gamerland3d environment.\narXiv preprint arXiv:2112.11731.\nBellemare, M. G., Candido, S., Castro, P. S., Gong, J., Machado, M. C., Moitra, S., Ponda,\nS. S., and Wang, Z. (2020). Autonomous navigation of stratospheric balloons using rein-\nforcement learning. Nature, 588(7836):77–82.\nBellemare, M. G., Dabney, W., and Munos, R. (2017).\nA distributional perspective on\nreinforcement learning. arXiv preprint arXiv:1707.06887.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artificial Intelligence\nResearch, 47:253–279.\nBellman, R. (1966). Dynamic programming. science, 153(3731):34–37.\nBengio, E., Jain, M., Korablyov, M., Precup, D., and Bengio, Y. (2021).\nFlow network\nbased generative models for non-iterative diverse candidate generation. Advances in Neural\nInformation Processing Systems, 34:27381–27394.\nBengio, Y., Lahlou, S., Deleu, T., Hu, E. J., Tiwari, M., and Bengio, E. (2023). Gflownet\nfoundations. Journal of Machine Learning Research, 24(210):1–55.\nBergdahl, J., Gordillo, C., Tollmar, K., and Gisslén, L. (2020). Augmenting automated game\ntesting with deep reinforcement learning. In 2020 IEEE Conference on Games (CoG),\npages 600–603. IEEE.\nBerner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D.,\nFischer, Q., Hashme, S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv:1912.06680.\nBertsekas, D. P. (1997). Nonlinear programming. Journal of the Operational Research Society,\n48(3):334–334.\nBickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S., and Hopkins, A. L. (2012).\nQuantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98.\nBohez, S., Abdolmaleki, A., Neunert, M., Buchli, J., Heess, N., and Hadsell, R. (2019). Value\nconstrained model-free continuous control. arXiv preprint arXiv:1902.04623.\n115\nBooth, S., Knox, W. B., Shah, J., Niekum, S., Stone, P., and Allievi, A. (2023). The perils of\ntrial-and-error reward design: misdesign through overfitting and invalid task specifications.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5920–\n5929.\nBorkar, V. S. (2005). An actor-critic algorithm for constrained markov decision processes.\nSystems & control letters, 54(3):207–213.\nBoularias, A., Kober, J., and Peters, J. (2011). Relative entropy inverse reinforcement learn-\ning. In Proceedings of the fourteenth international conference on artificial intelligence and\nstatistics, pages 182–189. JMLR Workshop and Conference Proceedings.\nBowling, M., Martin, J. D., Abel, D., and Dabney, W. (2023). Settling the reward hypothesis.\nIn International Conference on Machine Learning, pages 3003–3020. PMLR.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and\nZaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.\nBrown, D. S., Goo, W., Nagarajan, P., and Niekum, S. (2019a).\nExtrapolating beyond\nsuboptimal demonstrations via inverse reinforcement learning from observations. arXiv\npreprint arXiv:1904.06387.\nBrown, N., Fiscato, M., Segler, M. H., and Vaucher, A. C. (2019b). Guacamol: benchmarking\nmodels for de novo molecular design.\nJournal of chemical information and modeling,\n59(3):1096–1108.\nBrown, N., McKay, B., and Gasteiger, J. (2006).\nA novel workflow for the inverse qspr\nproblem using multiobjective optimization. Journal of computer-aided molecular design,\n20:333–341.\nCalian, D. A., Mankowitz, D. J., Zahavy, T., Xu, Z., Oh, J., Levine, N., and Mann, T.\n(2020).\nBalancing constraints and rewards with meta-gradient d4pg.\narXiv preprint\narXiv:2010.06324.\nCalinon, S., Evrard, P., Gribovskaya, E., Billard, A., and Kheddar, A. (2009). Learning\ncollaborative manipulation tasks by demonstration using a haptic interface. In 2009 In-\nternational Conference on Advanced Robotics, pages 1–6. IEEE.\nCastelletti, A., Corani, G., Rizzolli, A., Soncinie-Sessa, R., and Weber, E. (2002). Reinforce-\nment learning in the operational management of a water system. In IFAC workshop on\nmodeling and control in environmental issues, pages 325–330. Keio University Yokohama.\n116\nChai, J., Zeng, H., Li, A., and Ngai, E. W. (2021). Deep learning in computer vision: A\ncritical review of emerging techniques and application scenarios. Machine Learning with\nApplications, 6:100134.\nCheng, W., Fürnkranz, J., Hüllermeier, E., and Park, S.-H. (2011). Preference-based policy\niteration: Leveraging preference learning for reinforcement learning. In Machine Learn-\ning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011,\nAthens, Greece, September 5-9, 2011. Proceedings, Part I 11, pages 312–327. Springer.\nChentanez, N., Barto, A. G., and Singh, S. P. (2005). Intrinsically motivated reinforcement\nlearning. In Advances in neural information processing systems, pages 1281–1288.\nChevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H.,\nand Bengio, Y. (2018). Babyai: A platform to study the sample efficiency of grounded\nlanguage learning. arXiv preprint arXiv:1810.08272.\nChoi, J. and Kim, K.-E. (2011). Map inference for bayesian inverse reinforcement learning.\nAdvances in neural information processing systems, 24.\nChow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. (2017). Risk-constrained reinforce-\nment learning with percentile risk criteria. The Journal of Machine Learning Research,\n18(1):6070–6120.\nChow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M. (2018). A lyapunov-\nbased approach to safe reinforcement learning. Advances in neural information processing\nsystems, 31.\nChow, Y., Nachum, O., Faust, A., Duenez-Guzman, E., and Ghavamzadeh, M. (2019).\nLyapunov-based safe policy optimization for continuous control.\narXiv preprint\narXiv:1901.10031.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep\nreinforcement learning from human preferences. In Advances in Neural Information Pro-\ncessing Systems, pages 4299–4307.\nClark, J. and Amodei, D. (2016). Faulty reward functions in the wild. Open AI.\nCoello, C. A. C. and Cortés, N. C. (2005). Solving multiobjective optimization problems using\nan artificial immune system. Genetic programming and evolvable machines, 6:163–190.\nCui, Y. and Niekum, S. (2018). Active reward learning from critiques. In 2018 IEEE inter-\nnational conference on robotics and automation (ICRA), pages 6907–6914. IEEE.\n117\nCybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics\nof control, signals and systems, 2(4):303–314.\nDabney, W., Barreto, A., Rowland, M., Dadashi, R., Quan, J., Bellemare, M. G., and Silver,\nD. (2021). The value-improvement path: Towards better representations for reinforcement\nlearning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npages 7160–7168.\nDabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2018). Distributional reinforce-\nment learning with quantile regression. In Thirty-Second AAAI Conference on Artificial\nIntelligence.\nDalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y. (2018). Safe\nexploration in continuous action spaces. arXiv preprint arXiv:1801.08757.\nD’Ambrosio, D. B., Abelian, J., Abeyruwan, S., Ahn, M., Bewley, A., Boyd, J., Choromanski,\nK., Cortes, O., Coumans, E., Ding, T., et al. (2023). Robotic table tennis: A case study\ninto a high speed learning system. arXiv preprint arXiv:2309.03315.\nDaniel, C., Viering, M., Metz, J., Kroemer, O., and Peters, J. (2014). Active reward learning.\nIn Robotics: Science and systems, volume 98.\nDas, I. and Dennis, J. E. (1997). A closer look at drawbacks of minimizing weighted sums\nof objectives for pareto set generation in multicriteria optimization problems. Structural\noptimization, 14:63–69.\nDavid, H. A. (1963). The method of paired comparisons, volume 12. London.\nDayan, P. and Hinton, G. E. (1992). Feudal reinforcement learning. Advances in neural\ninformation processing systems, 5.\nde Woillemont, P. L. P., Labory, R., and Corruble, V. (2021). Configurable agent with reward\nas input: A play-style continuum generation. In 2021 IEEE Conference on Games (CoG),\npages 1–8. IEEE.\nDegrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner,\nR., Abdolmaleki, A., de Las Casas, D., et al. (2022). Magnetic control of tokamak plasmas\nthrough deep reinforcement learning. Nature, 602(7897):414–419.\nDegris, T., White, M., and Sutton, R. S. (2012). Off-policy actor-critic. arXiv preprint\narXiv:1205.4839.\n118\nDeleu, T., Góis, A., Emezue, C., Rankawat, M., Lacoste-Julien, S., Bauer, S., and Bengio,\nY. (2022). Bayesian structure learning with generative flow networks. In Uncertainty in\nArtificial Intelligence, pages 518–528. PMLR.\nDevlin, S., Georgescu, R., Momennejad, I., Rzepecki, J., Zuniga, E., Costello, G., Leroy, G.,\nShaw, A., and Hofmann, K. (2021). Navigation turing test (ntt): Learning to evaluate\nhuman-like navigation. arXiv preprint arXiv:2105.09637.\nDevlin, S. M. and Kudenko, D. (2012). Dynamic potential-based reward shaping. In Proceed-\nings of the 11th international conference on autonomous agents and multiagent systems,\npages 433–440. IFAAMAS.\nDewey, D. (2014). Reinforcement learning and the reward engineering principle. In 2014\nAAAI Spring Symposium Series.\nDing, Y., Florensa, C., Abbeel, P., and Phielipp, M. (2019). Goal-conditioned imitation\nlearning. In Advances in Neural Information Processing Systems (NeurIPS), pages 15298–\n15309.\nDosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and Koltun, V. (2017). Carla: An open\nurban driving simulator. In Conference on robot learning, pages 1–16. PMLR.\nDu, Y., Czarnecki, W. M., Jayakumar, S. M., Farajtabar, M., Pascanu, R., and Lakshmi-\nnarayanan, B. (2018). Adapting auxiliary losses using gradient similarity. arXiv preprint\narXiv:1812.02224.\nDulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Paduraru, C., Gowal, S., and Hester,\nT. (2021). Challenges of real-world reinforcement learning: definitions, benchmarks and\nanalysis. Machine Learning, 110(9):2419–2468.\nDulac-Arnold, G., Mankowitz, D., and Hester, T. (2019). Challenges of real-world reinforce-\nment learning. arXiv preprint arXiv:1904.12901.\nEhrgott, M. (2005). Multicriteria optimization, volume 491. Springer Science & Business\nMedia.\nEmmerich, M. T. and Deutz, A. H. (2018). A tutorial on multiobjective optimization: fun-\ndamentals and evolutionary methods. Natural computing, 17:585–609.\nErhan, D., Courville, A., Bengio, Y., and Vincent, P. (2010). Why does unsupervised pre-\ntraining help deep learning?\nIn Proceedings of the thirteenth international conference\n119\non artificial intelligence and statistics, pages 201–208. JMLR Workshop and Conference\nProceedings.\nErtl, P. and Schuffenhauer, A. (2009). Estimation of synthetic accessibility score of drug-\nlike molecules based on molecular complexity and fragment contributions.\nJournal of\ncheminformatics, 1:1–11.\nEvans, R. and Gao, J. (2016). Deepmind ai reduces google data centre cooling bill by 40\nDeepMind.\nFedus, W., Gelada, C., Bengio, Y., Bellemare, M. G., and Larochelle, H. (2019). Hyperbolic\ndiscounting and learning over multiple horizons. arXiv preprint arXiv:1902.06865.\nFernandes, P., Madaan, A., Liu, E., Farinhas, A., Martins, P. H., Bertsch, A., de Souza, J. G.,\nZhou, S., Wu, T., Neubig, G., et al. (2023). Bridging the gap: A survey on integrating\n(human) feedback for natural language generation. arXiv preprint arXiv:2305.00955.\nFinn, C., Christiano, P., Abbeel, P., and Levine, S. (2016a). A connection between genera-\ntive adversarial networks, inverse reinforcement learning, and energy-based models. arXiv\npreprint arXiv:1611.03852.\nFinn, C., Levine, S., and Abbeel, P. (2016b). Guided cost learning: Deep inverse optimal\ncontrol via policy optimization. In Proceedings of the 33rd International Conference on\nMachine Learning (ICML), pages 49–58.\nFoerster, J., Assael, I. A., de Freitas, N., and Whiteson, S. (2016). Learning to commu-\nnicate with deep multi-agent reinforcement learning. In Advances in Neural Information\nProcessing Systems, pages 2137–2145.\nFoerster, J., Song, F., Hughes, E., Burch, N., Dunning, I., Whiteson, S., Botvinick, M., and\nBowling, M. (2019). Bayesian action decoder for deep multi-agent reinforcement learning.\nInternational Conference on Machine Learning.\nFoerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. (2018). Coun-\nterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial\nIntelligence.\nFortnow, L. (2009). The status of the p versus np problem. Communications of the ACM,\n52(9):78–86.\n120\nFu, J., Korattikara, A., Levine, S., and Guadarrama, S. (2019). From language to goals:\nInverse reinforcement learning for vision-based instruction following.\narXiv preprint\narXiv:1902.07742.\nFu, J., Luo, K., and Levine, S. (2017). Learning robust rewards with adversarial inverse\nreinforcement learning. arXiv preprint arXiv:1710.11248.\nFujimoto, S., Van Hoof, H., and Meger, D. (2018). Addressing function approximation error\nin actor-critic methods. arXiv preprint arXiv:1802.09477.\nGábor, Z., Kalmár, Z., and Szepesvári, C. (1998). Multi-criteria reinforcement learning. In\nICML, volume 98, pages 197–205.\nGhasemipour, S. K. S., Zemel, R., and Gu, S. (2019). A divergence minimization perspective\non imitation learning methods. In Proceedings of the 3rd Conference on Robot Learning\n(CoRL).\nGhasemipour, S. K. S., Zemel, R., and Gu, S. (2020). A divergence minimization perspective\non imitation learning methods. In Conference on Robot Learning, pages 1259–1277. PMLR.\nGisslén, L., Eakins, A., Gordillo, C., Bergdahl, J., and Tollmar, K. (2021).\nAdversarial\nreinforcement learning for procedural content generation. arXiv preprint arXiv:2103.04847.\nGlaese, A., McAleese, N., Trębacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M.,\nWeidinger, L., Chadwick, M., Thacker, P., et al. (2022). Improving alignment of dialogue\nagents via targeted human judgements. arXiv preprint arXiv:2209.14375.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. MIT press.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,\nA., and Bengio, Y. (2014). Generative adversarial nets. Advances in neural information\nprocessing systems, 27.\nGordillo, C., Bergdahl, J., Tollmar, K., and Gisslén, L. (2021). Improving playtesting cover-\nage via curiosity driven reinforcement learning agents. In 2021 IEEE Conference on Games\n(CoG), pages 1–8. IEEE.\nGoyal, P., Niekum, S., and Mooney, R. (2021). Pixl2r: Guiding reinforcement learning using\nnatural language by mapping pixels to rewards. In Conference on Robot Learning, pages\n485–497. PMLR.\n121\nGoyal, P., Niekum, S., and Mooney, R. J. (2019). Using natural language for reward shaping\nin reinforcement learning. arXiv preprint arXiv:1903.02020.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved\ntraining of Wasserstein GANs. In Advances in Neural Information Processing Systems\n(NeurIPS), pages 5767–5777.\nGupta, A., Pacchiano, A., Zhai, Y., Kakade, S., and Levine, S. (2022). Unpacking reward\nshaping: Understanding the benefits of reward engineering on sample complexity. Advances\nin Neural Information Processing Systems, 35:15281–15295.\nGupta, J. K., Egorov, M., and Kochenderfer, M. J. (2017). Cooperative multi-agent control\nusing deep reinforcement learning. In AAMAS Workshops.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep\nenergy-based policies. arXiv preprint arXiv:1702.08165.\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H.,\nGupta, A., Abbeel, P., et al. (2018). Soft actor-critic algorithms and applications. arXiv\npreprint arXiv:1812.05905.\nHadfield-Menell, D., Milli, S., Abbeel, P., Russell, S. J., and Dragan, A. (2017). Inverse\nreward design. Advances in neural information processing systems, 30.\nHarutyunyan, A., Devlin, S., Vrancx, P., and Nowé, A. (2015). Expressing arbitrary reward\nfunctions as potential-based advice. In Proceedings of the AAAI conference on artificial\nintelligence, volume 29.\nHayes, C. F., Howley, E., and Mannion, P. (2020). Dynamic thresholded lexicograpic ordering.\nIn Adaptive and Learning Agents Workshop (AAMAS 2020).\nHayes, C. F., Rădulescu, R., Bargiacchi, E., Källström, J., Macfarlane, M., Reymond, M.,\nVerstraeten, T., Zintgraf, L. M., Dazeley, R., Heintz, F., et al. (2022). A practical guide to\nmulti-objective reinforcement learning and planning. Autonomous Agents and Multi-Agent\nSystems, 36(1):26.\nHazan, E., Kakade, S. M., Singh, K., and Van Soest, A. (2018). Provably efficient maximum\nentropy exploration. arXiv preprint arXiv:1812.02690.\nHe, H., Boyd-Graber, J., Kwok, K., and Daumé III, H. (2016). Opponent modeling in deep\nreinforcement learning. In International Conference on Machine Learning, pages 1804–\n1813.\n122\nHernandez-Leal, P., Kartal, B., and Taylor, M. E. (2018). Is multiagent deep reinforcement\nlearning the answer or the question? a brief survey. arXiv preprint arXiv:1810.05587.\nHernandez-Leal, P., Kartal, B., and Taylor, M. E. (2019a). Agent modeling as auxiliary\ntask for deep reinforcement learning. In Proceedings of the AAAI conference on artificial\nintelligence and interactive digital entertainment, volume 15, pages 31–37.\nHernandez-Leal, P., Kartal, B., and Taylor, M. E. (2019b). Agent Modeling as Auxiliary\nTask for Deep Reinforcement Learning. In AAAI Conference on Artificial Intelligence and\nInteractive Digital Entertainment.\nHessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan,\nD., Piot, B., Azar, M., and Silver, D. (2018). Rainbow: Combining improvements in deep\nreinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence.\nHo, J. and Ermon, S. (2016). Generative adversarial imitation learning. Advances in neural\ninformation processing systems, 29:4565–4573.\nHong, Z.-W., Su, S.-Y., Shann, T.-Y., Chang, Y.-H., and Lee, C.-Y. (2017). A deep policy\ninference q-network for multi-agent systems. arXiv preprint arXiv:1712.07893.\nHu, E., Malkin, N., Jain, M., Everett, K., Graikos, A., and Bengio, Y. (2023). Gflownet-em\nfor learning compositional latent variable models. arXiv preprint arXiv:2302.06576.\nHu, Y., Wang, W., Jia, H., Wang, Y., Chen, Y., Hao, J., Wu, F., and Fan, C. (2020).\nLearning to utilize shaping rewards: A new approach of reward shaping. Advances in\nNeural Information Processing Systems, 33:15931–15941.\nHuang, K., Fu, T., Gao, W., Zhao, Y., Roohani, Y., Leskovec, J., Coley, C. W., Xiao, C.,\nSun, J., and Zitnik, M. (2021). Therapeutics data commons: Machine learning datasets\nand tasks for drug discovery and development. arXiv preprint arXiv:2102.09548.\nHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. (2022). Language models as zero-\nshot planners: Extracting actionable knowledge for embodied agents.\nIn International\nConference on Machine Learning, pages 9118–9147. PMLR.\nIbarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. (2018). Reward learning\nfrom human preferences and demonstrations in atari.\nAdvances in neural information\nprocessing systems, 31.\n123\nIcarte, R. T., Klassen, T. Q., Valenzano, R., and McIlraith, S. A. (2022). Reward machines:\nExploiting reward function structure in reinforcement learning. Journal of Artificial Intel-\nligence Research, 73:173–208.\nIima, H. and Kuroe, Y. (2014). Multi-objective reinforcement learning for acquiring all pareto\noptimal policies simultaneously-method of determining scalarization weights. In 2014 IEEE\nInternational Conference on Systems, Man, and Cybernetics (SMC), pages 876–881. IEEE.\nIoffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. arXiv preprint arXiv:1502.03167.\nIqbal, S. and Sha, F. (2019). Actor-attention-critic for multi-agent reinforcement learning.\nIn International Conference on Machine Learning, pages 2961–2970.\nJacob, M., Devlin, S., and Hofmann, K. (2020).\n“it’s unwieldy and it takes a lot of\ntime”—challenges and opportunities for creating agents in commercial games.\nIn Pro-\nceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Enter-\ntainment, volume 16, pages 88–94.\nJaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., and\nKavukcuoglu, K. (2016). Reinforcement learning with unsupervised auxiliary tasks. arXiv\npreprint arXiv:1611.05397.\nJain, A., Wojcik, B., Joachims, T., and Saxena, A. (2013). Learning trajectory preferences\nfor manipulators via iterative improvement. Advances in neural information processing\nsystems, 26.\nJain, M., Bengio, E., Hernandez-Garcia, A., Rector-Brooks, J., Dossou, B. F., Ekbote, C. A.,\nFu, J., Zhang, T., Kilgour, M., Zhang, D., et al. (2022a). Biological sequence design with\ngflownets. In International Conference on Machine Learning, pages 9786–9801. PMLR.\nJain, M., Raparthy, S. C., Hernandez-Garcia, A., Rector-Brooks, J., Bengio, Y., Miret, S.,\nand Bengio, E. (2022b). Multi-objective gflownets. arXiv preprint arXiv:2210.12765.\nJang, E., Gu, S., and Poole, B. (2017). Categorical reparametrization with gumble-softmax.\nIn International Conference on Learning Representations (ICLR 2017). OpenReview. net.\nJaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N., Gu,\nS., and Picard, R. (2019a). Way off-policy batch deep reinforcement learning of implicit\nhuman preferences in dialog. arXiv preprint arXiv:1907.00456.\n124\nJaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Ortega, P., Strouse, D., Leibo, J. Z.,\nand De Freitas, N. (2019b). Social influence as intrinsic motivation for multi-agent deep\nreinforcement learning. In International Conference on Machine Learning, pages 3040–\n3049.\nJeon, H. J., Milli, S., and Dragan, A. (2020). Reward-rational (implicit) choice: A unify-\ning formalism for reward learning. Advances in Neural Information Processing Systems,\n33:4415–4426.\nJiang, J. and Lu, Z. (2018). Learning attentional communication for multi-agent cooperation.\nIn Advances in Neural Information Processing Systems, pages 7254–7264.\nJin, W., Barzilay, R., and Jaakkola, T. (2020). Multi-objective molecule generation using\ninterpretable substructures. In International conference on machine learning, pages 4849–\n4859. PMLR.\nJuliani, A., Berges, V.-P., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y.,\nHenry, H., Mattar, M., et al. (2018). Unity: A general platform for intelligent agents.\narXiv preprint arXiv:1809.02627.\nJuozapaitis, Z., Koul, A., Fern, A., Erwig, M., and Doshi-Velez, F. (2019).\nExplainable\nreinforcement learning via reward decomposition. In IJCAI/ECAI Workshop on explainable\nartificial intelligence.\nKaelbling, L. P., Littman, M. L., and Moore, A. W. (1996). Reinforcement learning: A\nsurvey. Journal of artificial intelligence research, 4:237–285.\nKallenberg, L. (2011). Markov decision processes. Lecture Notes. University of Leiden, pages\n65–66.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S.,\nRadford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nKaplan, R., Sauer, C., and Sosa, A. (2017). Beating atari with natural language guided\nreinforcement learning. arXiv preprint arXiv:1704.05539.\nKarpathy, A. (2017). Software 2.0. Data Set.\nKartal, B., Hernandez-Leal, P., and Taylor, M. E. (2019). Terminal prediction as an auxiliary\ntask for deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial\nIntelligence and Interactive Digital Entertainment, volume 15, pages 38–44.\n125\nKeeney, R., Raiffa, H., L, K., and Meyer, R. (1993). Decisions with Multiple Objectives:\nPreferences and Value Trade-Offs. Wiley series in probability and mathematical statistics.\nApplied probability and statistics. Cambridge University Press.\nKingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nKingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114.\nKlissarov, M., D’Oro, P., Sodhani, S., Raileanu, R., Bacon, P.-L., Vincent, P., Zhang, A.,\nand Henaff, M. (2023). Motif: Intrinsic motivation from artificial intelligence feedback.\narXiv preprint arXiv:2310.00166.\nKnox, W. B., Allievi, A., Banzhaf, H., Schmitt, F., and Stone, P. (2023). Reward (mis)\ndesign for autonomous driving. Artificial Intelligence, 316:103829.\nKnox, W. B. and Stone, P. (2009). Interactively shaping agents via human reinforcement:\nThe tamer framework. In Proceedings of the fifth international conference on Knowledge\ncapture, pages 9–16.\nKnox, W. B. and Stone, P. (2012). Reinforcement learning from simultaneous human and\nmdp reward. In AAMAS, volume 1004, pages 475–482. Valencia.\nKorpelevich, G. M. (1976). The extragradient method for finding saddle points and other\nproblems. Matecon, 12:747–756.\nKostrikov, I., Agrawal, K. K., Dwibedi, D., Levine, S., and Tompson, J. (2018).\nDiscriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial\nimitation learning. arXiv preprint arXiv:1809.02925.\nKostrikov, I., Agrawal, K. K., Dwibedi, D., Levine, S., and Tompson, J. (2019).\nDiscriminator-actor-critic: Addressing sample inefficiency and reward bias in adversar-\nial imitation learning.\nIn Proceedings of the 7th International Conference on Learning\nRepresentations (ICLR).\nKostrikov, I., Nachum, O., and Tompson, J. (2020). Imitation learning via off-policy distri-\nbution matching. In Proceedings of the 8th International Conference on Learning Repre-\nsentations (ICLR).\nKreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. (2018). Can neural machine translation\nbe improved with user feedback? arXiv preprint arXiv:1804.05958.\n126\nKuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017). Imitating driver behavior\nwith generative adversarial networks. In Proceedings of 2017 IEEE Intelligent Vehicles\nSymposium (IV), pages 204–211.\nKumar, A., Voet, A., and Zhang, K. Y. (2012). Fragment based drug design: from experi-\nmental to computational approaches. Current medicinal chemistry, 19(30):5128–5147.\nKurach, K., Raichuk, A., Stańczyk, P., Zajac, M., Bachem, O., Espeholt, L., Riquelme, C.,\nVincent, D., Michalski, M., Bousquet, O., et al. (2019). Google research football: A novel\nreinforcement learning environment. arXiv preprint arXiv:1907.11180.\nKwon, M., Xie, S. M., Bullard, K., and Sadigh, D. (2023). Reward design with language\nmodels. arXiv preprint arXiv:2303.00001.\nLahlou, S., Deleu, T., Lemos, P., Zhang, D., Volokhova, A., Hernández-García, A., Ezzine,\nL. N., Bengio, Y., and Malkin, N. (2023). A theory of continuous generative flow networks.\narXiv preprint arXiv:2301.12594.\nLample, G. and Chaplot, D. S. (2017). Playing fps games with deep reinforcement learning.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.\nLaskey, M., Lee, J., Fox, R., Dragan, A., and Goldberg, K. (2017). Dart: Noise injection for\nrobust imitation learning. In Conference on robot learning, pages 143–156. PMLR.\nLaskin, M., Srinivas, A., and Abbeel, P. (2020). Curl: Contrastive unsupervised representa-\ntions for reinforcement learning. In International Conference on Machine Learning, pages\n5639–5650. PMLR.\nLaud, A. and DeJong, G. (2003). The influence of reward on the speed of reinforcement\nlearning: An analysis of shaping. In Proceedings of the 20th International Conference on\nMachine Learning (ICML-03), pages 440–447.\nLazaridou, A., Peysakhovich, A., and Baroni, M. (2016). Multi-agent cooperation and the\nemergence of (natural) language. arXiv preprint arXiv:1612.07182.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. nature, 521(7553):436–444.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11):2278–2324.\nLee, K., Smith, L., and Abbeel, P. (2021). Pebble: Feedback-efficient interactive reinforce-\nment learning via relabeling experience and unsupervised pre-training.\narXiv preprint\narXiv:2106.05091.\n127\nLehman, J., Clune, J., Misevic, D., Adami, C., Altenberg, L., Beaulieu, J., Bentley, P. J.,\nBernard, S., Beslon, G., Bryson, D. M., et al. (2020). The surprising creativity of digital\nevolution: A collection of anecdotes from the evolutionary computation and artificial life\nresearch communities. Artificial life, 26(2):274–306.\nLeike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. (2018). Scalable agent\nalignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871.\nLevine, S., Popovic, Z., and Koltun, V. (2011). Nonlinear inverse reinforcement learning with\ngaussian processes. Advances in neural information processing systems, 24.\nLi, C. and Czarnecki, K. (2018).\nUrban driving with multi-objective deep reinforcement\nlearning. arXiv preprint arXiv:1811.08586.\nLi, K., Zhang, T., and Wang, R. (2020). Deep reinforcement learning for multiobjective\noptimization. IEEE transactions on cybernetics, 51(6):3103–3114.\nLiang, Q., Que, F., and Modiano, E. (2018). Accelerated primal-dual policy optimization for\nsafe reinforcement learning. arXiv preprint arXiv:1802.06480.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and\nWierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint\narXiv:1509.02971.\nLin, L.-J. (1993). Reinforcement learning for robots using neural networks. Technical report,\nCarnegie-Mellon Univ Pittsburgh PA School of Computer Science.\nLin, T., Jin, C., and Jordan, M. (2020). On gradient descent ascent for nonconvex-concave\nminimax problems. In International Conference on Machine Learning, pages 6083–6093.\nPMLR.\nLin, X., Baweja, H., Kantor, G., and Held, D. (2019a). Adaptive auxiliary task weighting for\nreinforcement learning. Advances in neural information processing systems, 32.\nLin, X., Zhen, H.-L., Li, Z., Zhang, Q.-F., and Kwong, S. (2019b). Pareto multi-task learning.\nAdvances in neural information processing systems, 32.\nLittman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning.\nIn Machine learning proceedings 1994, pages 157–163. Elsevier.\nLiu, Y., Datta, G., Novoseller, E., and Brown, D. S. (2023).\nEfficient preference-based\nreinforcement learning using learned dynamics models. arXiv preprint arXiv:2301.04741.\n128\nLiu, Y., Ding, J., and Liu, X. (2020). Ipo: Interior-point policy optimization under con-\nstraints. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages\n4940–4947.\nLiu, Y., Halev, A., and Liu, X. (2021). Policy learning with constraints in model-free re-\ninforcement learning: A survey. In The 30th International Joint Conference on Artificial\nIntelligence (IJCAI).\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. (2017). Multi-\nagent actor-critic for mixed cooperative-competitive environments. In Advances in Neural\nInformation Processing Systems, pages 6379–6390.\nLuketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas, J., Grefenstette, E., Whiteson,\nS., and Rocktäschel, T. (2019). A survey of reinforcement learning informed by natural\nlanguage. arXiv preprint arXiv:1906.03926.\nLuo, J., Paduraru, C., Voicu, O., Chervonyi, Y., Munns, S., Li, J., Qian, C., Dutta, P., Davis,\nJ. Q., Wu, N., et al. (2022). Controlling commercial cooling systems using reinforcement\nlearning. arXiv preprint arXiv:2211.07357.\nLyle, C., Rowland, M., Ostrovski, G., and Dabney, W. (2021). On the effect of auxiliary\ntasks on representation dynamics. In International Conference on Artificial Intelligence\nand Statistics, pages 1–9. PMLR.\nMa, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan,\nL., and Anandkumar, A. (2023). Eureka: Human-level reward design via coding large\nlanguage models.\nMacGlashan, J., Babes-Vroman, M., desJardins, M., Littman, M. L., Muresan, S., Squire, S.,\nTellex, S., Arumugam, D., and Yang, L. (2015). Grounding english commands to reward\nfunctions. In Robotics: Science and Systems.\nMacGlashan, J., Ho, M. K., Loftin, R., Peng, B., Wang, G., Roberts, D. L., Taylor, M. E.,\nand Littman, M. L. (2017). Interactive learning from policy-dependent human feedback.\nIn International conference on machine learning, pages 2285–2294. PMLR.\nMadan, K., Rector-Brooks, J., Korablyov, M., Bengio, E., Jain, M., Nica, A., Bosc, T.,\nBengio, Y., and Malkin, N. (2022). Learning gflownets from partial episodes for improved\nconvergence and stability. arXiv preprint arXiv:2209.12782.\n129\nMadan, K., Rector-Brooks, J., Korablyov, M., Bengio, E., Jain, M., Nica, A. C., Bosc, T.,\nBengio, Y., and Malkin, N. (2023). Learning gflownets from partial episodes for improved\nconvergence and stability. In International Conference on Machine Learning, pages 23467–\n23483. PMLR.\nMahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S. (2019). Maven: Multi-agent\nvariational exploration.\nIn Advances in Neural Information Processing Systems, pages\n7613–7624.\nMahmood, A. R., van Hasselt, H. P., and Sutton, R. S. (2014). Weighted importance sam-\npling for off-policy learning with linear function approximation. In Advances in Neural\nInformation Processing Systems, pages 3014–3022.\nMalkin, N., Jain, M., Bengio, E., Sun, C., and Bengio, Y. (2022a). Trajectory balance: Im-\nproved credit assignment in gflownets. Advances in Neural Information Processing Systems,\n35:5955–5967.\nMalkin, N., Lahlou, S., Deleu, T., Ji, X., Hu, E., Everett, K., Zhang, D., and Bengio, Y.\n(2022b). Gflownets and variational inference. arXiv preprint arXiv:2210.00580.\nMarchesini, E., Corsi, D., and Farinelli, A. (2022). Exploring safer behaviors for deep re-\ninforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pages 7701–7709.\nMathewson, K. W. and Pilarski, P. M. (2022). A brief guide to designing and evaluating\nhuman-centered interactive machine learning. arXiv preprint arXiv:2204.09622.\nMiettinen, K. (2012). Nonlinear multiobjective optimization, volume 12. Springer Science &\nBusiness Media.\nMindermann, S., Shah, R., Gleave, A., and Hadfield-Menell, D. (2018). Active inverse reward\ndesign. arXiv preprint arXiv:1809.03060.\nMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., Denil, M.,\nGoroshin, R., Sifre, L., Kavukcuoglu, K., et al. (2016). Learning to navigate in complex\nenvironments. arXiv preprint arXiv:1611.03673.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and\nRiedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602.\n130\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,\nA., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control\nthrough deep reinforcement learning. nature, 518(7540):529–533.\nMoerland, T. M., Broekens, J., Plaat, A., Jonker, C. M., et al. (2023). Model-based reinforce-\nment learning: A survey. Foundations and Trends® in Machine Learning, 16(1):1–118.\nMohamed, S. and Lakshminarayanan, B. (2016). Learning in implicit generative models.\narXiv preprint arXiv:1610.03483.\nMordatch, I. and Abbeel, P. (2018).\nEmergence of grounded compositional language in\nmulti-agent populations. In Thirty-Second AAAI Conference on Artificial Intelligence.\nMosqueira-Rey, E., Hernández-Pereira, E., Alonso-Ríos, D., Bobes-Bascarán, J., and\nFernández-Leal, Á. (2023). Human-in-the-loop machine learning: A state of the art. Arti-\nficial Intelligence Review, 56(4):3005–3054.\nMossalam, H., Assael, Y. M., Roijers, D. M., and Whiteson, S. (2016). Multi-objective deep\nreinforcement learning. arXiv preprint arXiv:1610.02707.\nMurphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT press.\nNachum, O., Chow, Y., Dai, B., and Li, L. (2019). DualDICE: Behavior-agnostic estima-\ntion of discounted stationary distribution corrections. In Advances in Neural Information\nProcessing Systems (NeurIPS), pages 2318–2328.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2018). Trust-PCL: An off-policy trust\nregion method for continuous control. In Proceedings of the 6th International Conference\non Learning Representations (ICLR).\nNair, V. and Hinton, G. E. (2010).\nRectified linear units improve restricted boltzmann\nmachines. In Proceedings of the 27th international conference on machine learning (ICML-\n10), pages 807–814.\nNg, A. Y., Harada, D., and Russell, S. (1999). Policy invariance under reward transforma-\ntions: Theory and application to reward shaping. In ICML, volume 99, pages 278–287.\nNg, A. Y., Russell, S., et al. (2000). Algorithms for inverse reinforcement learning. In Icml,\nvolume 1, page 2.\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016).\nCombining policy\ngradient and q-learning. arXiv preprint arXiv:1611.01626.\n131\nOord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner,\nN., Senior, A., and Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio.\narXiv preprint arXiv:1609.03499.\nOtter, D. W., Medina, J. R., and Kalita, J. K. (2020). A survey of the usages of deep learn-\ning for natural language processing. IEEE transactions on neural networks and learning\nsystems, 32(2):604–624.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal,\nS., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730–27744.\nPalan, M., Landolfi, N. C., Shevchuk, G., and Sadigh, D. (2019). Learning reward functions\nby integrating human demonstrations and preferences. arXiv preprint arXiv:1906.08928.\nPan, A., Bhatia, K., and Steinhardt, J. (2022).\nThe effects of reward misspecification:\nMapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544.\nPan, L., Malkin, N., Zhang, D., and Bengio, Y. (2023). Better training of gflownets with\nlocal credit and incomplete trajectories. arXiv preprint arXiv:2302.01687.\nPapadopoulos, A. I. and Linke, P. (2006). Multiobjective molecular design for integrated\nprocess-solvent systems synthesis. AIChE Journal, 52(3):1057–1070.\nPardalos, P. M., Žilinskas, A., Žilinskas, J., et al. (2017). Non-convex multi-objective opti-\nmization. Springer.\nParisi, S., Pirotta, M., Smacchia, N., Bascetta, L., and Restelli, M. (2014). Policy gradient\napproaches for multi-objective sequential decision making. In 2014 International Joint\nConference on Neural Networks (IJCNN), pages 2323–2330. IEEE.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,\nGimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information processing systems, 32.\nPilarski, P. M., Dawson, M. R., Degris, T., Fahimi, F., Carey, J. P., and Sutton, R. S. (2011).\nOnline human training of a myoelectric prosthesis controller via actor-critic reinforcement\nlearning.\nIn 2011 IEEE international conference on rehabilitation robotics, pages 1–7.\nIEEE.\nPineda, L. E., Wray, K. H., and Zilberstein, S. (2015). Revisiting multi-objective mdps with\nrelaxed lexicographic preferences. In 2015 AAAI Fall Symposium Series.\n132\nPolyak, B. (1970). Iterative methods using lagrange multipliers for solving extremal problems\nwith constraints of the equation type. USSR Computational Mathematics and Mathemat-\nical Physics, 10(5):42–52.\nPomerleau, D. A. (1991).\nEfficient training of artificial neural networks for autonomous\nnavigation. Neural computation, 3(1):88–97.\nPopova, M., Isayev, O., and Tropsha, A. (2018). Deep reinforcement learning for de novo\ndrug design. Science advances, 4(7):eaap7885.\nPuterman, M. L. (1990). Markov decision processes. Handbooks in operations research and\nmanagement science, 2:331–434.\nRamachandran, D. and Amir, E. (2007). Bayesian inverse reinforcement learning. In IJCAI,\nvolume 7, pages 2586–2591.\nRampášek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf, G., and Beaini, D. (2022).\nRecipe for a general, powerful, scalable graph transformer. Advances in Neural Information\nProcessing Systems, 35:14501–14515.\nRandløv, J. and Alstrøm, P. (1998). Learning to drive a bicycle using reinforcement learning\nand shaping. In ICML, volume 98, pages 463–471.\nRashid, T., Samvelyan, M., Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. (2018).\nQmix: Monotonic value function factorisation for deep multi-agent reinforcement learning.\nIn International Conference on Machine Learning, pages 4292–4301.\nRatliff, N. D., Bagnell, J. A., and Zinkevich, M. A. (2006). Maximum margin planning. In\nProceedings of the 23rd international conference on Machine learning, pages 729–736.\nRatliff, N. D., Silver, D., and Bagnell, J. A. (2009). Learning to search: Functional gradient\ntechniques for imitation learning. Autonomous Robots, 27:25–53.\nRatner, E., Hadfield-Menell, D., and Dragan, A. D. (2018). Simplifying reward design through\ndivide-and-conquer. arXiv preprint arXiv:1806.02501.\nRay, A., Achiam, J., and Amodei, D. (2019). Benchmarking safe exploration in deep rein-\nforcement learning. arXiv preprint arXiv:1910.01708, 7.\nReddy, S., Dragan, A. D., and Levine, S. (2019). SQIL: Imitation learning via reinforcement\nlearning with sparse rewards.\n133\nResnick, C., Eldridge, W., Ha, D., Britz, D., Foerster, J., Togelius, J., Cho, K., and Bruna,\nJ. (2018). Pommerman: A multi-agent playground. arXiv preprint arXiv:1809.07124.\nReymond, M., Hayes, C. F., Steckelmacher, D., Roijers, D. M., and Nowé, A. (2023). Actor-\ncritic multi-objective reinforcement learning for non-linear utility functions. Autonomous\nAgents and Multi-Agent Systems, 37(2):23.\nReymond, M. and Nowé, A. (2019). Pareto-dqn: Approximating the pareto front in com-\nplex multi-objective decision problems. In Proceedings of the adaptive and learning agents\nworkshop (ALA-19) at AAMAS.\nRezende, D. and Mohamed, S. (2015).\nVariational inference with normalizing flows.\nIn\nInternational conference on machine learning, pages 1530–1538. PMLR.\nRoijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. (2013).\nA survey of multi-\nobjective sequential decision-making. Journal of Artificial Intelligence Research, 48:67–\n113.\nRosenbaum, C., Klinger, T., and Riemer, M. (2017). Routing networks: Adaptive selection\nof non-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239.\nRosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and\norganization in the brain. Psychological review, 65(6):386.\nRoss, S. and Bagnell, D. (2010). Efficient reductions for imitation learning. In Proceedings\nof the 13th International Conference on Artificial Intelligence and Statistics (AISTATS),\npages 661–668.\nRoss, S., Gordon, G., and Bagnell, D. (2011). A reduction of imitation learning and struc-\ntured prediction to no-regret online learning. In Proceedings of the fourteenth international\nconference on artificial intelligence and statistics, pages 627–635. JMLR Workshop and\nConference Proceedings.\nRoy, J., Girgis, R., Romoff, J., Bacon, P.-L., and Pal, C. (2021). Direct behavior specification\nvia constrained reinforcement learning. arXiv preprint arXiv:2112.12228.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by\nback-propagating errors. nature, 323(6088):533–536.\nRussell, S. (1998). Learning agents for uncertain environments. In Proceedings of the eleventh\nannual conference on Computational learning theory, pages 101–103.\n134\nRussell, S. J. and Zimdars, A. (2003). Q-decomposition for reinforcement learning agents. In\nProceedings of the 20th International Conference on Machine Learning (ICML-03), pages\n656–663.\nRust, J. (2008). Dynamic programming. The new Palgrave dictionary of economics, 1:8.\nSadigh, D., Dragan, A. D., Sastry, S., and Seshia, S. A. (2017). Active preference-based\nlearning of reward functions.\nSasaki, F., Yohira, T., and Kawaguchi, A. (2018). Sample efficient imitation learning for\ncontinuous control. In Proceedings of the 6th International Conference on Learning Rep-\nresentations (ICLR).\nSaunders, W., Sastry, G., Stuhlmueller, A., and Evans, O. (2017).\nTrial without er-\nror:\nTowards safe reinforcement learning via human intervention.\narXiv preprint\narXiv:1707.05173.\nSchadd, F., Bakkes, S., and Spronck, P. (2007). Opponent modeling in real-time strategy\ngames. In GAMEON, pages 61–70.\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015a). Universal value function approx-\nimators. In International conference on machine learning, pages 1312–1320.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015b). Prioritized experience replay.\narXiv preprint arXiv:1511.05952.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy\noptimization. In Proceedings of the 32nd International Conference on Machine Learning\n(ICML), pages 1889–1897.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347.\nSepton, Y. and Amir, O. (2022). Integrating policy summaries with reward decomposition\nexplanations. In ICAPS 2022 Workshop on Explainable AI Planning.\nSettles, B. (2009). Active learning literature survey.\nShahidinejad, A. and Ghobaei-Arani, M. (2020). Joint computation offloading and resource\nprovisioning for e dge-cloud computing environment: A machine learning-based approach.\nSoftware: Practice and Experience, 50(12):2212–2230.\n135\nShao, K., Tang, Z., Zhu, Y., Li, N., and Zhao, D. (2019). A survey of deep reinforcement\nlearning in video games. arXiv preprint arXiv:1912.10944.\nShelhamer, E., Mahmoudieh, P., Argus, M., and Darrell, T. (2016). Loss is its own reward:\nSelf-supervision for reinforcement learning. arXiv preprint arXiv:1612.07307.\nSiddique, U., Weng, P., and Zimmer, M. (2020). Learning fair policies in multi-objective\n(deep) reinforcement learning with average and discounted rewards. In International Con-\nference on Machine Learning, pages 8905–8915. PMLR.\nSilver, D., Bagnell, J., and Stentz, A. (2008). High performance outdoor navigation from over-\nhead data using imitation learning. Robotics: Science and Systems IV, Zurich, Switzerland,\n1.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrit-\ntwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the\ngame of go with deep neural networks and tree search. nature, 529(7587):484–489.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). De-\nterministic policy gradient algorithms. In International conference on machine learning,\npages 387–395. Pmlr.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,\nBaker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human\nknowledge. nature, 550(7676):354–359.\nSilver, D., Singh, S., Precup, D., and Sutton, R. S. (2021). Reward is enough. Artificial\nIntelligence, 299:103535.\nSingh, S., Lewis, R. L., and Barto, A. G. (2009). Where do rewards come from. In Proceedings\nof the annual conference of the cognitive science society, pages 2601–2606. Cognitive Science\nSociety.\nSingh, S., Lewis, R. L., Sorg, J., Barto, A. G., and Helou, A. (2010). On separating agent\ndesigner goals from agent goals: Breaking the preferences–parameters confound.\nSkalse, J., Hammond, L., Griffin, C., and Abate, A. (2022). Lexicographic multi-objective\nreinforcement learning. arXiv preprint arXiv:2212.13769.\nSong, H., Li, A., Wang, T., and Wang, M. (2021). Multimodal deep reinforcement learning\nwith auxiliary task for obstacle avoidance of indoor mobile robot. Sensors, 21(4):1363.\n136\nSorg, J. D. (2011). The optimal reward problem: Designing effective reward for bounded\nagents. PhD thesis, University of Michigan.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine\nlearning research, 15(1):1929–1958.\nStåhl, N., Falkman, G., Karlsson, A., Mathiason, G., and Bostrom, J. (2019). Deep rein-\nforcement learning for multiparameter optimization in de novo drug design. Journal of\nchemical information and modeling, 59(7):3166–3176.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D.,\nand Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008–3021.\nStooke, A., Achiam, J., and Abbeel, P. (2020). Responsive safety in reinforcement learning\nby pid lagrangian methods. In International Conference on Machine Learning, pages 9133–\n9143. PMLR.\nStrouse, D., Kleiman-Weiner, M., Tenenbaum, J., Botvinick, M., and Schwab, D. J. (2018).\nLearning to share and hide intentions using information regularization. In Advances in\nNeural Information Processing Systems, pages 10270–10281.\nSuddarth, S. C. and Kergosien, Y. (1990). Rule-injection hints as a means of improving\nnetwork performance and learning time.\nIn European association for signal processing\nworkshop, pages 120–129. Springer.\nSumers, T. R., Ho, M. K., Hawkins, R. D., Narasimhan, K., and Griffiths, T. L. (2021).\nLearning rewards from linguistic feedback. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 6002–6010.\nSutton, R. S. (2015). Introduction to reinforcement learning with function approximation.\nIn Tutorial at the conference on neural information processing systems, volume 33.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\nSutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient\nmethods for reinforcement learning with function approximation. In Advances in neural\ninformation processing systems, pages 1057–1063.\n137\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, D.\n(2011). Horde: A scalable real-time architecture for learning knowledge from unsupervised\nsensorimotor interaction. In The 10th International Conference on Autonomous Agents\nand Multiagent Systems-Volume 2, pages 761–768.\nTallec, C., Blier, L., and Ollivier, Y. (2019). Making deep q-learning methods robust to\ntime discretization. In International Conference on Machine Learning, pages 6096–6104.\nPMLR.\nTaylor, M. E. (2023). Reinforcement learning requires human-in-the-loop framing and ap-\nproaches. In HHAI 2023: Augmenting Human Intellect, pages 351–360. IOS Press.\nTeh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and\nPascanu, R. (2017). Distral: Robust multitask reinforcement learning. Advances in neural\ninformation processing systems, 30.\nTesauro, G. (1994). Td-gammon, a self-teaching backgammon program, achieves master-level\nplay. Neural computation, 6(2):215–219.\nTessler, C., Mankowitz, D. J., and Mannor, S. (2018). Reward constrained policy optimiza-\ntion. arXiv preprint arXiv:1805.11074.\nTodorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ international conference on intelligent robots and systems,\npages 5026–5033. IEEE.\nTreviso, M., Lee, J.-U., Ji, T., Aken, B. v., Cao, Q., Ciosici, M. R., Hassid, M., Heafield, K.,\nHooker, S., Raffel, C., et al. (2023). Efficient methods for natural language processing: A\nsurvey. Transactions of the Association for Computational Linguistics, 11:826–860.\nTurchetta, M., Kolobov, A., Shah, S., Krause, A., and Agarwal, A. (2020). Safe reinforcement\nlearning via curriculum induction. arXiv preprint arXiv:2006.12136.\nUhlenbeck, G. E. and Ornstein, L. S. (1930). On the theory of the brownian motion. Physical\nreview, 36(5):823.\nUzawa, H., Anow, K., and Hurwicz, L. (1958). Studies in linear and nonlinear programming.\nVamplew, P., Dazeley, R., Berry, A., Issabekov, R., and Dekker, E. (2011). Empirical eval-\nuation methods for multiobjective reinforcement learning algorithms. Machine learning,\n84:51–80.\n138\nVamplew, P., Dazeley, R., Foale, C., Firmin, S., and Mummery, J. (2018). Human-aligned\nartificial intelligence is a multiobjective problem.\nEthics and Information Technology,\n20:27–40.\nVamplew, P., Smith, B. J., Källström, J., Ramos, G., Rădulescu, R., Roijers, D. M., Hayes,\nC. F., Heintz, F., Mannion, P., Libin, P. J., et al. (2022). Scalar reward is not enough: A\nresponse to silver, singh, precup and sutton (2021). Autonomous Agents and Multi-Agent\nSystems, 36(2):41.\nVamplew, P., Yearwood, J., Dazeley, R., and Berry, A. (2008). On the limitations of scalari-\nsation for multi-objective reinforcement learning of pareto fronts. In AI 2008: Advances in\nArtificial Intelligence: 21st Australasian Joint Conference on Artificial Intelligence Auck-\nland, New Zealand, December 1-5, 2008. Proceedings 21, pages 372–378. Springer.\nVan den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al. (2016).\nConditional image generation with pixelcnn decoders.\nAdvances in neural information\nprocessing systems, 29.\nVan Hasselt, H., Guez, A., and Silver, D. (2016). Deep reinforcement learning with double\nq-learning. In Thirtieth AAAI conference on artificial intelligence.\nVan Moffaert, K., Drugan, M. M., and Nowé, A. (2013a). Hypervolume-based multi-objective\nreinforcement learning. In Evolutionary Multi-Criterion Optimization: 7th International\nConference, EMO 2013, Sheffield, UK, March 19-22, 2013. Proceedings 7, pages 352–366.\nSpringer.\nVan Moffaert, K., Drugan, M. M., and Nowé, A. (2013b). Scalarized multi-objective reinforce-\nment learning: Novel design techniques. In 2013 IEEE symposium on adaptive dynamic\nprogramming and reinforcement learning (ADPRL), pages 191–199. IEEE.\nVan Moffaert, K. and Nowé, A. (2014). Multi-objective reinforcement learning using sets of\npareto dominating policies. The Journal of Machine Learning Research, 15(1):3483–3512.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and compos-\ning robust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning, pages 1096–1103.\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi,\nD. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning. Nature, 575(7782):350–354.\n139\nVinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., Makhzani,\nA., Küttler, H., Agapiou, J., Schrittwieser, J., et al. (2017). Starcraft ii: A new challenge\nfor reinforcement learning. arXiv preprint arXiv:1708.04782.\nWang, Y.-C. and Usher, J. M. (2005). Application of reinforcement learning for agent-based\nproduction scheduling. Engineering applications of artificial intelligence, 18(1):73–82.\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N. (2016). Duel-\ning network architectures for deep reinforcement learning. In International conference on\nmachine learning, pages 1995–2003.\nWatkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279–292.\nWaytowich, N., Barton, S. L., Lawhern, V., and Warnell, G. (2019).\nA narration-based\nreward shaping approach using grounded natural language commands.\narXiv preprint\narXiv:1911.00497.\nWhite, A. (2015). Developing a predictive approach to knowledge. PhD thesis, University of\nAlberta.\nWiewiora, E., Cottrell, G. W., and Elkan, C. (2003). Principled methods for advising rein-\nforcement learning agents. In Proceedings of the 20th international conference on machine\nlearning (ICML-03), pages 792–799.\nWilliams, E. C., Gopalan, N., Rhee, M., and Tellex, S. (2018). Learning to parse natural\nlanguage to grounded reward functions with weak supervision. In 2018 ieee international\nconference on robotics and automation (icra), pages 4430–4436. IEEE.\nWilliams, R. J. (1992).\nSimple statistical gradient-following algorithms for connectionist\nreinforcement learning. Machine learning, 8(3-4):229–256.\nWirth, C., Akrour, R., Neumann, G., Fürnkranz, J., et al. (2017). A survey of preference-\nbased reinforcement learning methods. Journal of Machine Learning Research, 18(136):1–\n46.\nWray, K., Zilberstein, S., and Mouaddib, A.-I. (2015). Multi-objective mdps with conditional\nlexicographic reward preferences.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 29.\nWu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing,\nK., and Pande, V. (2018). Moleculenet: a benchmark for molecular machine learning.\nChemical science, 9(2):513–530.\n140\nWulfmeier, M., Ondruska, P., and Posner, I. (2015). Maximum entropy deep inverse rein-\nforcement learning. arXiv preprint arXiv:1507.04888.\nWydmuch, M., Kempka, M., and Jaśkowski, W. (2018). Vizdoom competitions: Playing\ndoom from pixels. IEEE Transactions on Games, 11(3):248–259.\nXu, J., Tian, Y., Ma, P., Rus, D., Sueda, S., and Matusik, W. (2020). Prediction-guided\nmulti-objective reinforcement learning for continuous robot control. In International con-\nference on machine learning, pages 10607–10616. PMLR.\nXu, K., Hu, W., Leskovec, J., and Jegelka, S. (2018).\nHow powerful are graph neural\nnetworks? arXiv preprint arXiv:1810.00826.\nYang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P. J. (2020). Projection-based con-\nstrained policy optimization. arXiv preprint arXiv:2010.03152.\nYu, L., Xie, W., Xie, D., Zou, Y., Zhang, D., Sun, Z., Zhang, L., Zhang, Y., and Jiang, T.\n(2019). Deep reinforcement learning for smart home energy management. IEEE Internet\nof Things Journal, 7(4):2751–2762.\nYu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. (2020).\nGradi-\nent surgery for multi-task learning. Advances in Neural Information Processing Systems,\n33:5824–5836.\nYu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.-H., Arenas, M. G., Chiang, H.-T. L.,\nErez, T., Hasenclever, L., Humplik, J., et al. (2023). Language to rewards for robotic skill\nsynthesis. arXiv preprint arXiv:2306.08647.\nYu, Y. (2018). Towards sample efficient reinforcement learning. In IJCAI, pages 5739–5743.\nYun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J. (2019). Graph transformer networks.\nAdvances in neural information processing systems, 32.\nZhang, D., Chen, R. T., Malkin, N., and Bengio, Y. (2022). Unifying generative models with\ngflownets. arXiv preprint arXiv:2209.02606.\nZhang, D. W., Rainone, C., Peschl, M., and Bondesan, R. (2023). Robust scheduling with\ngflownets. arXiv preprint arXiv:2302.05446.\nZhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg, K., and Abbeel, P. (2018).\nDeep imitation learning for complex manipulation tasks from virtual reality teleoperation.\nIn 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 5628–\n5635. IEEE.\n141\nZhang, Y., Vuong, Q., and Ross, K. W. (2020). First order constrained optimization in policy\nspace. arXiv preprint arXiv:2002.06506.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,\nZhang, J., Dong, Z., et al. (2023). A survey of large language models. arXiv preprint\narXiv:2303.18223.\nZheng, B., Verma, S., Zhou, J., Tsang, I., and Chen, F. (2021). Imitation learning: Progress,\ntaxonomies and opportunities. arXiv preprint arXiv:2106.12177.\nZhou, H., Gong, Y., Mugrai, L., Khalifa, A., Nealen, A., and Togelius, J. (2018). A hybrid\nsearch agent in pommerman. In Proceedings of the 13th International Conference on the\nFoundations of Digital Games (FDG), pages 1–4.\nZhou, Z., Kearnes, S., Li, L., Zare, R. N., and Riley, P. (2019). Optimization of molecules\nvia deep reinforcement learning. Scientific reports, 9(1):10752.\nZiebart, B. D. (2010). Modeling purposeful adaptive behavior with the principle of maximum\ncausal entropy. Carnegie Mellon University.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K. (2008).\nMaximum entropy\ninverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial\nIntelligence, pages 1433–1438.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano,\nP., and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593.\n142\nAPPENDIX A\nSUPPLEMENTARY MATERIAL FOR CHAPTER 5\nA.1\nProofs\nA.1.1\nProof of Lemma 1\nProof. Lemma 1 states that given L(˜p, pG) defined in Equation 5.8:\n(a) ˜p∗≜arg max\n˜p\nL(˜p, pG) = pE\n(b) arg min\npG\nL(pE, pG) = pE\nStarting with (a), we have:\narg max\n˜p\nL(˜p, pG) = arg max\n˜p\nX\nxi\npE(xi) log D˜p,pG(xi) + pG(xi) log(1 −D˜p,pG(xi))\n≜arg max\n˜p\nX\nxi\nLi\nAssuming infinite discriminator’s capacity, Li can be made independent for all xi ∈X and we\ncan construct our optimal discriminator D∗\n˜p,pG as a look-up table D∗\n˜p,pG : X →]0, 1[ ; xi 7→D∗\ni\nwith D∗\ni the optimal discriminator for each xi defined as:\nD∗\ni = arg max\nDi\nLi = arg max\nDi\npE,i log Di + pG,i log(1 −Di),\n(A.1)\nwith pG,i ≜pG(xi), pE,i ≜pE(xi) and Di ≜D(xi).\nRecall that Di ∈]0, 1[ and that pG,i ∈]0, 1[. Therefore the function ˜pi 7→Di =\n˜pi\n˜pi + pG,i\nis\ndefined for ˜pi ∈]0, +∞[. Since it is strictly monotonic over that domain we have that:\nD∗\ni = arg max\nDi\nLi ⇔˜p∗\ni = arg max\n˜pi\nLi\n(A.2)\nTaking the derivative and setting to zero, we get:\ndLi\nd˜pi\n\f\f\f\f\f\n˜pi\n= 0 ⇔˜pi = pE,i\n(A.3)\nThe second derivative test confirms that we have a maximum, i.e. d2Li\nd˜p2\ni\n\f\f\f\f\f\n˜p∗\ni\n< 0. The values\n143\nof Li at the boundaries of the domain of definition of ˜pi tend to −∞, therefore Li(˜p∗\ni = pE,i)\nis the global maximum of Li w.r.t. ˜pi. Finally, the optimal global discriminator is given by:\nD∗\n˜p,pG(x) =\npE(x)\npE(x) + pG(x)\n∀x ∈X\n(A.4)\nThis concludes the proof for (a).\nThe proof for (b) can be found in the work of Goodfellow et al. (2014). We reproduce it here\nfor completion. Since from (a) we know that ˜p∗(x) = pE(x) ∀x ∈X, we can write the GAN\nobjective for the optimal discriminator as:\narg min\npG\nL(˜p∗, pG) = arg min\npG\nL(pE, pG)\n(A.5)\n= arg min\npG\nEx∼pE\n\"\nlog\npE(x)\npE(x) + pG(x)\n#\n+ Ex∼pG\n\"\nlog\npG(x)\npE(x) + pG(x)\n#\n(A.6)\nNote that:\nlog 4 = Ex∼pE [log 2] + Ex∼pG [log 2]\n(A.7)\nAdding Equation A.7 to Equation A.6 and subtracting log 4 on both sides:\narg min\npG\nL(pE, pG) = −log 4 + Ex∼pE\n\"\nlog\n2pE(x)\npE(x) + pG(x)\n#\n+ Ex∼pG\n\"\nlog\n2pG(x)\npE(x) + pG(x)\n#\n(A.8)\n= −log 4 + DKL\n\u0012\npE\n\r\r\r\r\npE + pG\n2\n\u0013\n+ DKL\n\u0012\npE\n\r\r\r\r\npE + pG\n2\n\u0013\n(A.9)\n= −log 4 + 2DJS (pE ∥pG )\n(A.10)\nWhere DKL and DJS are respectively the Kullback-Leibler and the Jensen-Shannon di-\nvergences. Since the Jensen-Shannon divergence between two distributions is always non-\nnegative and zero if and only if the two distributions are equal, we have that:\narg min\npG\nL(pE, pG) = pE\n(A.11)\nThis concludes the proof for (b).\nA.1.2\nProof of Theorem 1\nProof. Theorem 1 states that given L(˜π, πG) defined in Equation 5.9:\n(a) ˜π∗≜arg max\n˜π\nL(˜π, πG) satisfies q˜π∗= qπE\n144\n(b) π∗\nG = ˜π∗∈arg min\nπG\nL(˜π∗, πG)\nThe proof of (a) is very similar to the one from Lemma 1. Starting from Equation 5.9 we\nhave:\narg max\n˜π\nL(˜π, πG) = arg max\n˜π\nX\nτi\nPπE (τi) log D˜π,πG(τi) + PπG(τi) log(1 −D˜π,πG(τi))\n(A.12)\n= arg max\n˜π\nX\nτi\nξ(τi)\n\u0010\nqπE (τi) log D˜π,πG(τi) + qπG(τi) log(1 −D˜π,πG(τi))\n\u0011\n(A.13)\n= arg max\n˜π\nX\nτi\nLi\n(A.14)\nLike for Lemma 1, we can optimise for each Li individually. When doing so, ξ(τi) can be\nomitted as it is constant w.r.t ˜π. The rest of the proof is identical to the one of but Lemma 1\nwith pE = qπE and pG = qπG. It follows that the max of L(˜π, πG) is reached for q∗\n˜π = qπE . From\nthat we obtain that the policy ˜π∗that makes the discriminator D˜π∗,πG optimal w.r.t L(˜π, πG)\nis such that q˜π∗= q∗\n˜π = qπE i.e. QT−1\nt=0 ˜π∗(at|st) = QT−1\nt=0 πE(at|st) ∀τ.\nThe proof for (b) stems from the observation that choosing πG = ˜π∗(the policy recovered by\nthe optimal discriminator D˜π∗,πG) minimizes L(˜π∗, πG):\nπG(a|s) = ˜π∗(a|s) ∀(s, a) ∈S × A\n⇒\nT−1\nY\nt=0\nπG(at|st) =\nT−1\nY\nt=0\n˜π∗(at|st) ∀τ ∈T\n(A.15)\n⇒\nqπG(τ) = qπE (τ) ∀τ ∈T\n(A.16)\n⇒\nD˜π∗,˜π∗= 1\n2 ∀τ ∈T\n(A.17)\n⇒\nL(˜π∗, ˜π∗) = −log 4\n(A.18)\nBy multiplying the numerator and denominator of D˜π∗,˜π∗by ξ(τ) it can be shown in exactly\nthe same way as in Appendix A.1.1 that −log 4 is the global minimum of L(˜π∗, πG).\nA.2\nAdversarial Soft Q-Fitting: transition-wise Imitation Learn-\ning without Policy Optimization\nIn this section we present Adversarial Soft Q-Fitting (ASQF), a principled approach to Imi-\ntation Learning without Reinforcement Learning that relies exclusively on transitions. Using\ntransitions rather than trajectories presents several practical benefits such as the possibility\n145\nto deal with asynchronously collected data or non-sequential experts demonstrations. We\nfirst present the theoretical setting for ASQF and then test it on a variety of discrete control\ntasks. We show that while it is theoretically sound, ASQF is often outperformed by ASAF-1,\nan approximation to ASAF that also allows to rely on transitions instead of trajectories.\nTheoretical Setting\nWe consider the GAN objective of Equation 5.5 with x = (s, a),\nX = S × A, pE = dπE ,\npG = dπG and a discriminator D ˜f,πG of the form of Fu et al. (2017):\nmin\nπG max\n˜f\nL( ˜f, πG) ,\nL( ˜f, πG) ≜EdπE [log D ˜f,πG(s, a)] + EdπG [log(1 −D ˜f,πG(s, a))],\nwith\nD ˜f,πG =\nexp ˜f(s, a)\nexp ˜f(s, a) + πG(a|s),\n(A.19)\nfor which we present the following theorem.\nTheorem 2. For any generator policy πG, the optimal discriminator parameter for Equa-\ntion A.19 is\n˜f ∗≜arg max\n˜f\nL( ˜f, πG) = log\n \nπE(a|s)dπE (s)\ndπG(s)\n!\n∀(s, a) ∈S × A\nUsing ˜f ∗, the optimal generator policy π∗\nG is\narg min\nπG\nmax\n˜f\nL( ˜f, πG) = arg min\nπG\nL( ˜f ∗, πG) = πE(a|s) =\nexp ˜f ∗(s, a)\nP\na′ exp ˜f ∗(s, a′) ∀(s, a) ∈S × A.\nProof. The beginning of the proof closely follows the proof of Appendix A.1.1.\narg max\n˜f\nL( ˜f, πG) =\narg max\n˜f\nX\nsi,ai\ndπE (si, ai) log D ˜f,πG(si, ai) + dπG(si, ai) log(1 −D ˜f,πG(si, ai))\n(A.20)\nWe solve for each individual (si, ai) pair and note that ˜fi 7→Di =\nexp ˜fi\nexp ˜fi + πG,i\nis strictly\nmonotonic on ˜fi ∈R ∀πG,i ∈]0, 1[ so,\nD∗\ni = arg max\nDi\nLi ⇔˜f ∗\ni = arg max\n˜f\nLi\n(A.21)\n146\nTaking the derivative and setting it to 0, we find that\ndLi\nd ˜fi\n\f\f\f\f\f ˜fi\n= 0\n⇔\n˜fi = log\n \nπG,i\ndπE ,i\ndπG,i\n!\n(A.22)\nWe confirm that we have a global maximum with the second derivative test and the values\nat the border of the domain i.e.\nd2Li\nd ˜f 2\ni\n\f\f\f\f\f ˜f∗\ni\n< 0 and Li goes to −∞for ˜fi →+∞and for\n˜fi →−∞.\nIt follows that\n˜f ∗(s, a) = log\n \nπG(a|s)dπE (s, a)\ndπG(s, a)\n!\n∀(s, a) ∈S × A\n(A.23)\n=⇒˜f ∗(s, a) = log\n \n\u0018\u0018\u0018\u0018\nπG(a|s)dπE (s)πE(a|s)\ndπG(s)\u0018\u0018\u0018\u0018\nπG(a|s)\n!\n∀(s, a) ∈S × A\n(A.24)\n=⇒˜f ∗(s, a) = log\n \nπE(a|s)dπE (s)\ndπG(s)\n!\n∀(s, a) ∈S × A\n(A.25)\nThis proves the first part of Theorem 2.\nTo prove the second part notice that\nD ˜f∗,πG(s, a) =\nπE(a|s)dπE (s)\ndπG(s)\nπE(a|s)dπE (s)\ndπG(s) + πG(a|s)\n=\nπE(a|s)dπE (s)\nπE(a|s)dπE (s) + πG(a|s)dπG(s)\n=\ndπE (s, a)\ndπE (s, a) + dπG(s, a)\n(A.26)\nThis is equal to the optimal discriminator of the GAN objective Equation A.4 when x = (s, a).\nFor this discriminator we showed in Section A.1.1 that the optimal generator π∗\nG is such that\ndπ∗\nG(s, a) = dπE (s, a) ∀(s, a) ∈S × A, which is satisfied for π∗\nG(a|s) = πE(a|s) ∀(s, a) ∈S × A.\nUsing the fact that\nX\na′\nexp ˜f ∗(s, a′) =\nX\na′\nπE(a′|s)dπE (s)\ndπG(s) = dπE (s)\ndπG(s)\nX\na′\nπE(a′|s) = dπE (s)\ndπG(s).\n(A.27)\nwe can combine Equation A.25 and Equation A.27 to write the expert’s policy πE as a function\n147\nof the optimal discriminator parameter ˜f ∗:\nπE(a|s) =\nexp ˜f ∗(s, a)\nP\na′ exp ˜f ∗(s, a′) ∀(s, a) ∈S × A.\n(A.28)\nThis concludes the second part of the proof.\nAdversarial Soft-Q Fitting (ASQF) - practical algorithm\nIn a nutshell, Theorem 2\ntells us that training the discriminator in Equation A.19 to distinguish between transitions\nfrom the expert and transitions from a generator policy can be seen as retrieving ˜f ∗which\nplays the role of the expert’s soft Q-function (i.e. which matches Equation 5.1 for ˜f ∗=\n1\nαQ∗\nsoft,E):\nπE(a|s) =\nexp ˜f ∗(s, a)\nP\na′ exp ˜f ∗(s, a′) = exp\n \n˜f ∗(s, a) −log\nX\na′\nexp ˜f ∗(s, a′)\n!\n,\n(A.29)\nTherefore, by training the discriminator, one simultaneously retrieves the optimal generator\npolicy.\nThere is one caveat though: the summation over actions that is required in Equation A.29\nto go from ˜f ∗to the policy is intractable in continuous action spaces and would require\nan additional step such as a projection to a proper distribution (Haarnoja et al. (2018) use\na Gaussian) in order to draw samples and evaluate likelihoods. Updating in this way the\ngenerator policy to match a softmax over our learned state-action preferences ( ˜f ∗) becomes\nvery similar in requirements and computational load to a policy optimization step, thus\ndefeating the purpose of this work which is to get rid of the policy optimization step. For\nthis reason we only consider ASQF for discrete action spaces.\nAs explained in Section 5.3.3, in practice we optimize D ˜f,πG only for a few steps before\nupdating πG by normalizing exp ˜f(s, a) over the action dimension. See Algorithm 2 for the\npseudo-code.\nAlgorithm 2: Adversarial Soft-Q Fitting (ASQF)\nRequire: expert transitions DE = {(si, ai)}NE\ni=1\nRandomly initialize ˜f and get πG from Equation A.29\nfor steps m = 0 to M do\nCollect transitions DG = {(si, ai)}NG\ni=1 by executing πG\nTrain D ˜f,πG using binary cross-entropy on minibatches of transitions from DE and DG\nGet πG from Equation A.29\nend for\n148\nExperimental results\nFigure A.1 shows that ASQF performs well on small scale environ-\nments but struggles and eventually fails on more complicated environments. Specifically, it\nseems that ASQF does not scale well with the observation space size. Indeed mountaincar,\ncartpole, lunarlander and pommerman have respectively an observation space dimensionality\nof 2, 4, 8 and 960. This may be due to the fact that the partition function Equation A.27\nbecomes more difficult to learn. Indeed, for each state, several transitions with different\nactions are required in order to learn it. Poorly approximating this partition function could\nlead to assigning too low a probability to expert-like actions and eventually failing to behave\nappropriately. ASAF on the other hand explicitly learns the probability of an action given\nthe state – in other word it explicitly learns the partition function – and is therefore immune\nto that problem.\n0\n500\n1000\nEpisodes\n200\n175\n150\n125\n100\nEvaluation return\nMOUNTAINCAR\n0\n500\n1000\nEpisodes\n40\n80\n120\n160\n200\nCARTPOLE\n0\n500\n1000\nEpisodes\n400\n200\n0\n200\nLUNARLANDER\n0\n20000\n40000\nEpisodes\n1.0\n0.5\n0.0\n0.5\n1.0\nEvaluation return\nPommerman Random-Tag\nASAF-1 (ours)\nASQF (ours)\nExpert\nFigure A.1 Comparison between ASAF-1 and ASQF, our two transition-wise methods, on\nenvironments with increasing observation space dimensionality\n149\nA.3\nAdditional Experiments\nA.3.1\nGAIL - Importance of Gradient Penalty\n0\n2\n4\n6\n8\nEnvironment steps x 1e-5\n0\n800\n1600\n2400\n3200\nEvaluation return\nHOPPER\n0\n1\n2\n3\n4\n5\nEnvironment steps x 1e-6\n0\n1500\n3000\n4500\nWALKER2D\nGAIL + PPO (w/ GP)\nGAIL + PPO (w/o GP)\nEXPERT\n0\n1\n2\n3\n4\n5\nEnvironment steps x 1e-6\n0\n2000\n4000\n6000\n8000\nHALFCHEETAH\nFigure A.2 Comparison between original GAIL (Ho and Ermon, 2016) and GAIL with gra-\ndient penalty (GP) (Gulrajani et al., 2017; Kostrikov et al., 2019)\n150\nA.3.2\nMimicking the expert\nTo ensure that our method actually mimics the ex-\npert and doesn’t just learn a policy that collects\nhigh rewards when trained with expert demonstra-\ntions, we ran ASAF-1 on the Ant-v2 MuJoCo environ-\nment using various sets of 25 demonstrations. These\ndemonstrations were generated from a Soft Actor-\nCritic agent at various levels of performance during\nits training.\nSince at low-levels of performance the\nvariance of episode’s return is high, we filtered col-\nlected demonstrations to lie in the targeted range of\nperformance (e.g. return in [800, 1200] for the 1K set).\n0.0\n0.6\n1.2\n1.8\nSteps(M)\n0.0\n1.5\n3.0\n4.5\nEvaluation Return(K)\nASAF-1 ON ANT\n5K-expert\n4K-expert\n3K-expert\n2K-expert\n1K-expert\nFigure A.3 ASAF-1 on Ant-v2. Col-\nors are 1K, 2K, 3K, 4K, 5K expert’s\nperformance.\nResults in Figure A.3 show that our algorithm succeeds at learning a policy that closely\nemulates various demonstrators (even when non-optimal).\nA.3.3\nWall Clock Time\nWe report training times in Figure A.4 and observe that ASAF-1 is always fastest to learn.\nNote however that reports of performance w.r.t wall-clock time should always be taken with\na grain of salt as they are greatly influenced by hyperparameters and implementation details.\n0\n4\n8\n12\n16\nWall Clock Time (H)\n0\n1500\n3000\nEvaluation Return\nHOPPER\n0\n5\n10\n15\n20\nWall Clock Time (H)\n0\n2000\n4000\n6000\nWALKER2D\nSQIL\nGAIL + PPO (w/ GP)\nASAF-w (ours)\nASAF (ours)\nASAF-1 (ours)\nEXPERT\n0\n2\n4\n6\n8\nWall Clock Time (H)\n0\n3000\n6000\nHALFCHEETAH\n0\n2\n4\n6\n8\nWall Clock Time (H)\n0\n2000\n4000\n6000\nANT\nFigure A.4 Training times on MuJoCo tasks for 25 expert demonstrations.\n151\nA.4\nHyperparameter tuning and best configurations\nA.4.1\nClassic Control\nFor this first set of experiments, we use the fixed hyperparameters presented in Table A.1.\nTable A.1 Fixed Hyperparameters for classic control tasks\nRL component\nHyperparameter\nDiscrete Control\nContinuous Control\nSAC\nBatch size (in transitions)\n256\n256\nReplay Buffer length |B|\n106\n106\nWarmup (in transitions)\n1280\n10240\nInitial entropy weight α\n0.4\n0.4\nGradient norm clipping threshold\n0.2\n1\nTransitions between update\n40\n1\nTarget network weight τ\n0.01\n0.01\nPPO\nBatch size (in transitions)\n256\n256\nGAE parameter λ\n0.95\n0.95\nTransitions between update\n-\n2000\nEpisodes between updates\n10\n-\nEpochs per update\n10\n10\nUpdate clipping parameter\n0.2\n0.2\nReward Learning component\nHyperparameter\nDiscrete Control\nContinuous Control\nAIRL, GAIL, ASAF-1\nBatch size (in transitions)\n256\n256\nTransitions between update\n-\n2000\nEpisodes between updates\n10\n-\nEpochs per update\n50\n50\nGradient value clipping threshold\n-\n1\n(ASAF-1)\nASAF, ASAF-w\nBatch size (in trajectories)\n10\n10\nEpisodes between updates\n10\n20\nEpochs per update\n50\n50\nWindow size w\n(searched)\n200\nGradient value clipping threshold\n-\n1\nFor the most sensitive hyperparameters, the learning rates for the reinforcement learning and\n152\ndiscriminator updates (ϵRL and ϵD), we perform a random search over 50 configurations and\n3 seeds each (for each algorithm on each task) for 500 episodes. We consider logarithmic\nranges, i.e. ϵ = 10u with u ∼Uniform(−6, −1) for ϵD and u ∼Uniform(−4, −1) for ϵRL.\nWe also include in this search the critic learning rate coefficient κ for PPO also sampled\naccording to a logarithmic scale with u ∼Uniform(−2, 2) so that the effective learning rate\nfor PPO’s critic network is κ · ϵRL. For discrete action tasks, the window-size w for ASAF-\nw is sampled uniformly within {32, 64, 128}. The best configuration for each algorithm is\npresented in Tables A.2 to A.7. Figure 5.1 uses these configurations retrained on 10 seeds\nand twice as long.\nFinally for all neural networks (policies and discriminators) for these experiments we use a\nfully-connected MLP with two hidden layers and ReLU activation (except for the last layer).\nWe used hidden sizes of 64 for the discrete tasks and of 256 for the continuous tasks.\nTable A.2 Best found hyperparameters for Cartpole\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nAIRL + PPO\nGAIL + PPO\nDiscriminator update lr ϵD\n0.028\n0.039\n0.00046\n-\n2.5*10−6\n0.00036\nRL update lr ϵRL\n-\n-\n-\n0.0067\n0.0052\n0.012\nCritic lr coefficient κ\n-\n-\n-\n-\n0.25\n0.29\nwindow size w\n-\n64\n1\n-\n-\n-\nwindow stride\n-\n64\n1\n-\n-\n-\nTable A.3 Best found hyperparameters for Mountaincar\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nAIRL + PPO\nGAIL + PPO\nDiscriminator update lr ϵD\n0.059\n0.059\n0.0088\n-\n0.0042\n0.00016\nRL update lr ϵRL\n-\n-\n-\n0.062\n0.016\n0.0022\nCritic lr coefficient κ\n-\n-\n-\n-\n4.6\n0.018\nwindow size w\n-\n32\n1\n-\n-\n-\nwindow stride\n-\n32\n1\n-\n-\n-\nTable A.4 Best found hyperparameters for Lunarlander\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nAIRL + PPO\nGAIL + PPO\nDiscriminator update lr ϵD\n0.0055\n0.0015\n0.00045\n-\n0.0002\n0.00019\nRL update lr ϵRL\n-\n-\n-\n0.0036\n0.0012\n0.0016\nCritic lr coefficient κ\n-\n-\n-\n-\n0.48\n8.5\nwindow size w\n-\n32\n1\n-\n-\n-\nwindow stride\n-\n32\n1\n-\n-\n-\nA.4.2\nMuJoCo\nFor MuJoCo experiments (Hopper-v2, Walker2d-v2, HalfCheetah-v2, Ant-v2), the fixed hy-\n153\nTable A.5 Best found hyperparameters for Pendulum\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nAIRL + PPO\nGAIL + PPO\nDiscriminator update lr ϵD\n0.00069\n0.00082\n0.00046\n-\n4.3*10−6\n1.6*10−5\nRL update lr ϵRL\n-\n-\n-\n0.0001\n0.00038\n0.00028\nCritic lr coefficient κ\n-\n-\n-\n-\n0.028\n84\nwindow size w\n-\n200\n1\n-\n-\n-\nwindow stride\n-\n200\n1\n-\n-\n-\nTable A.6 Best found hyperparameters for Mountaincar-c\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nAIRL + PPO\nGAIL + PPO\nDiscriminator update lr ϵD\n0.00021\n3.8*10−5\n6.2*10−6\n-\n1.7*10−5\n1.5*10−5\nRL update lr ϵRL\n-\n-\n-\n0.0079\n0.0012\n0.0052\nCritic lr coefficient κ\n-\n-\n-\n-\n10\n12\nwindow size w\n-\n200\n1\n-\n-\n-\nwindow stride\n-\n200\n1\n-\n-\n-\nTable A.7 Best found hyperparameters for Lunarlander-c\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nAIRL + PPO\nGAIL + PPO\nDiscriminator update lr ϵD\n0.0051\n0.0022\n0.0003\n-\n0.0045\n0.00014\nRL update lr ϵRL\n-\n-\n-\n0.0027\n0.00031\n0.00049\nCritic lr coefficient κ\n-\n-\n-\n-\n14\n0.01\nwindow size w\n-\n200\n-\n-\n-\n-\nwindow stride\n-\n200\n-\n-\n-\n-\nperparameters are presented in Table A.8. For all exeperiments, fully-connected MLPs with\ntwo hidden layers and ReLU activation (except for the last layer) were used, where the\nnumber of hidden units is equal to 256.\nFor SQIL we used SAC with the same hyperparameters that were used to generate the expert\ndemonstrations. For ASAF, ASAF-1 and ASAF-w, we set the learning rate for the discrim-\ninator at 0.001 and ran random searches over 25 randomly sampled configurations and 2\nseeds for each task to select the other hyperparameters for the discriminator training. These\nhyperparameters included the discriminator batch size sampled from a uniform distribution\nover {10, 20, 30} for ASAF and ASAF-w (in trajectories) and over {100, 500, 1000, 2000} for\nASAF-1 (in transitions), the number of epochs per update sampled from a uniform dis-\ntribution over {10, 20, 50}, the gradient norm clipping threshold sampled form a uniform\ndistribution over {1, 10}, the window-size (for ASAF-w) sampled from a uniform distribu-\ntion over {100, 200, 500, 1000} and the window stride (for ASAF-w) sampled from a uniform\ndistribution over {1, 50, w}. For GAIL, we obtained poor results using the original hyper-\nparameters from (Ho and Ermon, 2016) for a number of tasks so we ran random searches\nover 100 randomly sampled configurations for each task and 2 seeds to select for the follow-\n154\nTable A.8 Fixed hyperparameters for MuJoCo environments.\nRL component\nHyperparameter\nHopper, Walker2d, HalfCheetah, Ant\nPPO (for GAIL)\nGAE parameter λ\n0.98\nTransitions between updates\n2000\nEpochs per update\n5\nUpdate clipping parameter\n0.2\nCritic lr coefficient κ\n0.25\nDiscount factor γ\n0.99\nReward Learning component\nHyperparameter\nHopper, Walker2d, HalfCheetah, Ant\nGAIL\nTransitions between updates\n2000\nASAF\nEpisodes between updates\n25\nASAF-1 and ASAF-w\nTransitions between updates\n2000\ning hyperparameters: the log learning rate of the RL update and the discriminator update\nseparately sampled from uniform distributions over [−7, −1], the gradient norm clipping for\nthe RL update and the discriminator update separately sampled from uniform distributions\nover {None, 1, 10}, the number of epochs per update sampled from a uniform distribution\nover {5, 10, 30, 50}, the gradient penalty coefficient sampled from a uniform distribution over\n{1, 10} and the batch size for the RL update and discriminator update separately sampled\nfrom uniform distributions over {100, 200, 500, 1000, 2000}.\nTable A.9 Best found hyperparameters for the Hopper-v2 environment\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nGAIL + PPO\nRL batch size (in transitions)\n-\n-\n-\n256\n200\nDiscriminator batch size (in transitions)\n-\n-\n100\n-\n2000\nDiscriminator batch size (in trajectories)\n10\n10\n-\n-\n-\nGradient clipping (RL update)\n-\n-\n-\n-\n1.\nGradient clipping (discriminator update)\n10.\n10.\n1.\n-\n1.\nEpochs per update\n50\n50\n30\n-\n5\nGradient penalty (discriminator update)\n-\n-\n-\n-\n1.\nRL update lr ϵRL\n-\n-\n-\n3 ∗10−4\n1.8 ∗10−5\nDiscriminator update lr ϵD\n0.001\n0.001\n0.001\n-\n0.011\nwindow size w\n-\n200\n1\n-\n-\nwindow stride\n-\n1\n1\n-\n-\n155\nTable A.10 Best found hyperparameters for the HalfCheetah-v2 environment\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nGAIL + PPO\nRL batch size (in transitions)\n-\n-\n-\n256\n1000\nDiscriminator batch size (in transitions)\n-\n-\n100\n-\n100\nDiscriminator batch size (in trajectories)\n10\n10\n-\n-\n-\nGradient clipping (RL update)\n-\n-\n-\n-\n-\nGradient clipping (discriminator update)\n10.\n1\n1\n-\n10\nEpochs per update\n50\n10\n30\n-\n30\nGradient penalty (discriminator update)\n-\n-\n-\n-\n1.\nRL update lr ϵRL\n-\n-\n-\n3 ∗10−4\n0.0006\nDiscriminator update lr ϵD\n0.001\n0.001\n0.001\n-\n0.023\nwindow size w\n-\n200\n1\n-\n-\nwindow stride\n-\n1\n1\n-\n-\nTable A.11 Best found hyperparameters for the Walker2d-v2 environment\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nGAIL + PPO\nRL batch size (in transitions)\n-\n-\n-\n256\n200\nDiscriminator batch size (in transitions)\n-\n-\n500\n-\n2000\nDiscriminator batch size (in trajectories)\n20\n20\n-\n-\n-\nGradient clipping (RL update)\n-\n-\n-\n-\n-\nGradient clipping (discriminator update)\n10.\n1.\n10.\n-\n-\nEpochs per update\n30\n10\n50\n-\n30\nGradient penalty (discriminator update)\n-\n-\n-\n-\n1.\nRL update lr ϵRL\n-\n-\n-\n3 ∗10−4\n0.00039\nDiscriminator update lr ϵD\n0.001\n0.001\n0.001\n-\n0.00066\nwindow size w\n-\n100\n1\n-\n-\nwindow stride\n-\n1\n1\n-\n-\nTable A.12 Best found hyperparameters for the Ant-v2 environment\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nGAIL + PPO\nRL batch size (in transitions)\n-\n-\n-\n256\n500\nDiscriminator batch size (in transitions)\n-\n-\n100\n-\n100\nDiscriminator batch size (in trajectories)\n20\n20\n-\n-\n-\nGradient clipping (RL update)\n-\n-\n-\n-\n-\nGradient clipping (discriminator update)\n10.\n1.\n1.\n-\n10.\nEpochs per update\n50\n50\n10\n-\n50\nGradient penalty (discriminator update)\n-\n-\n-\n-\n10\nRL update lr ϵRL\n-\n-\n-\n3 ∗10−4\n8.5 ∗10−5\nDiscriminator update lr ϵD\n0.001\n0.001\n0.001\n-\n0.0016\nwindow size w\n-\n200\n1\n-\n-\nwindow stride\n-\n50\n1\n-\n-\n156\nA.4.3\nPommerman\nFor this set of experiments, we use a number of fixed hyperparameters for all algorithms\neither inspired from their original papers for the baselines or selected through preliminary\nsearches. These fixed hyperparameters are presented in Table A.13.\nTable A.13 Fixed Hyperparameters for Pommerman Random-Tag environment.\nRL component\nHyperparameter\nPommerman Random-Tag\nSAC\nBatch size (in transitions)\n256\nReplay Buffer length |B|\n105\nWarmup (in transitions)\n1280\nInitial entropy weight α\n0.4\nGradient norm clipping threshold\n0.2\nTransitions between update\n10\nTarget network weight τ\n0.05\nPPO\nBatch size (in transitions)\n256\nGAE parameter λ\n0.95\nEpisodes between updates\n10\nEpochs per update\n10\nUpdate clipping parameter\n0.2\nCritic lr coefficient κ\n0.5\nReward Learning component\nHyperparameter\nPommerman Random-Tag\nAIRL, GAIL, ASAF-1\nBatch size (in transitions)\n256\nEpisodes between updates\n10\nEpochs per update\n10\nASAF, ASAF-w\nBatch size (in trajectories)\n5\nEpisodes between updates\n10\nEpochs per update\n10\nFor the most sensitive hyperparameters, the learning rates for the reinforcement learning\nand discriminator updates (ϵRL and ϵD), we perform a random search over 25 configurations\nand 2 seeds each for all algorithms.\nWe consider logarithmic ranges, i.e.\nϵ = 10u with\nu ∼Uniform(−7, −3) for ϵD and u ∼Uniform(−4, −1) for ϵRL. We also include in this\nsearch the window-size w for ASAF-w, sampled uniformly within {32, 64, 128}. The best con-\nfiguration for each algorithm is presented in Table A.14. Figure 5.3 uses these configurations\nretrained on 10 seeds.\nFinally for all neural networks (policies and discriminators) we use the same architecture.\n157\nTable A.14 Best found hyperparameters for the Pommerman Random-Tag environment\nHyperparameter\nASAF\nASAF-w\nASAF-1\nSQIL\nAIRL + PPO\nGAIL + PPO\nBC\nDiscriminator update lr ϵD\n0.0007\n0.0002\n0.0001\n-\n3.1*10−7\n9.3*10−7\n0.00022\nRL update lr ϵRL\n-\n-\n-\n0.00019\n0.00017\n0.00015\n-\nwindow size w\n-\n32\n1\n-\n-\n-\n-\nwindow stride\n-\n32\n1\n-\n-\n-\n-\nSpecifically, we first process the feature maps (see Section A.5.3) using a 3-layers convolutional\nnetwork with number of hidden feature maps of 16, 32 and 64 respectivelly. Each one of\nthese layers use a kernel size of 3x3 with stride of 1, no padding and a ReLU activation. This\nmodule ends with a fully connected layer of hidden size 64 followed by a ReLU activation.\nThe output vector is then concatenated to the unprocessed additional information vector (see\nSection A.5.3) and passed through a final MLP with two hidden layers of size 64 and ReLU\nactivations (except for the last layer).\nA.5\nEnvironments and expert data\nA.5.1\nClassic Control\nThe environments used here are the reference Gym implementations for classic control1 and\nfor Box2D2. We generated the expert trajectories for mountaincar (both discrete and con-\ntinuous version) by hand using keyboard inputs. For the other tasks, we trained our SAC\nimplementation to get experts on the discrete action tasks and our PPO implementation to\nget experts on the continuous action tasks.\nA.5.2\nMuJoCo\nThe experts were trained using our implementation of SAC (Haarnoja et al., 2018) the state-\nof-the-art RL algorithm in MuJoCo continuous control tasks. Our implementation basically\nrefactors the SAC implementation from Rlpyt3. We trained SAC agent for 1,000,000 steps for\nHopper-v2 and 3,000,000 steps for Walker2d-v2 and HalfCheetah-v2 and Ant-v2. We used\nthe default hyperparameters from Rlpyt.\nA.5.3\nPommerman\nThe observation space that we use for Pommerman domain (Resnick et al., 2018) is composed\nof a set of 15 feature maps as well as an additional information vector. The feature maps\n1See: http://gym.openai.com/envs/#classic_control\n2See: http://gym.openai.com/envs/#box2d\n3See: https://github.com/astooke/rlpyt\n158\nwhose dimensions are given by the size of the board (8x8 in the case of 1vs1 tasks) are\none-hot across the third dimension and represent which element is present at which location.\nSpecifically, these feature maps identify whether a given location is the current player, an\nally, an ennemy, a passage, a wall, a wood, a bomb, a flame, fog, a power-up. Other feature\nmaps contain integers indicating bomb blast stength, bomb life, bomb moving direction and\nflame life for each location. Finally, the additional information vecor contains the time-step,\nnumber of ammunition, whether the player can kick and blast strengh for the current player.\nThe agent has an action space composed of six actions: do-nothing, up, down, left, right and\nlay bomb.\nFor these experiments, we generate the expert demonstrations using Agent47Agent, the open-\nsource champion algorithm of the FFA 2018 competition (Zhou et al., 2018) which uses\nhardcoded heuristics and Monte-Carlo Tree-Search4. While this agent occasionally elimi-\nnates itself during a match, we only select trajectories leading to a win as being expert\ndemonstrations.\nA.5.4\nDemonstrations summary\nTable A.15 provides a summary of the expert data used.\nTable A.15 Expert demonstrations used for Imitation Learning\nTask-Name\nExpert mean return\nNumber of expert trajectories\nCartpole\n200.0\n10\nMountaincar\n-108.0\n10\nLunarlander\n277.5\n10\nPendulum\n-158.6\n10\nMountaincar-c\n93.92\n10\nLunarlander-c\n266.1\n10\nHopper\n3537\n25\nWalker2D\n5434\n25\nHalfcheetah\n7841\n25\nAnt\n5776\n25\nPommerman random-tag\n1\n300, 150, 75, 15, 5, 1\n4See: https://github.com/YichenGong/Agent47Agent/tree/master/pommerman\n159\nAPPENDIX B\nSUPPLEMENTARY MATERIAL FOR CHAPTER 6\nB.1\nAdditional details on Motivation section\nWe trained each agent i with online Q-learning (Watkins and Dayan, 1992) on the Qi(ai, s)\ntable using Boltzmann exploration (Kaelbling et al., 1996). The Boltzmann temperature is\nfixed to 1 and we set the learning rate to 0.05 and the discount factor to 0.99. After each\nlearning episode we evaluate the current greedy policy on 10 episodes and report the mean\nreturn. Curves are averaged over 20 seeds and the shaded area represents the standard error.\nB.2\nTasks descriptions\nSPREAD (Figure 6.4a): In this environment, there are 3 agents (small orange circles) and\n3 landmarks (bigger gray circles). At every timestep, agents receive a team-reward rt = n−c\nwhere n is the number of landmarks occupied by at least one agent and c the number\nof collisions occurring at that timestep. To maximize their return, agents must therefore\nspread out and cover all landmarks. Initial agents’ and landmarks’ positions are random.\nTermination is triggered when the maximum number of timesteps is reached.\nBOUNCE (Figure 6.4b): In this environment, two agents (small orange circles) are linked\ntogether with a spring that pulls them toward each other when stretched above its relaxation\nlength. At episode’s mid-time a ball (smaller black circle) falls from the top of the environ-\nment. Agents must position correctly so as to have the ball bounce on the spring towards\nthe target (bigger beige circle), which turns yellow if the ball’s bouncing trajectory passes\nthrough it. They receive a team-reward of rt = 0.1 if the ball reflects towards the side walls,\nrt = 0.2 if the ball reflects towards the top of the environment, and rt = 10 if the ball reflects\ntowards the target. At initialisation, the target’s and ball’s vertical position is fixed, their\nhorizontal positions are random. Agents’ initial positions are also random. Termination is\ntriggered when the ball is bounced by the agents or when the maximum number of timesteps\nis reached.\nCOMPROMISE (Figure 6.4c): In this environment, two agents (small orange circles) are\nlinked together with a spring that pulls them toward each other when stretched above its\nrelaxation length. They both have a distinct assigned landmark (light gray circle for light\norange agent, dark gray circle for dark orange agent), and receive a reward of rt = 10\nwhen they reach it. Once a landmark is reached by its corresponding agent, the landmark is\n160\nrandomly relocated in the environment. Initial positions of agents and landmark are random.\nTermination is triggered when the maximum number of timesteps is reached.\nCHASE (Figure 6.4d): In this environment, two predators (orange circles) are chasing a prey\n(turquoise circle). The prey moves with respect to a scripted policy consisting of repulsion\nforces from the walls and predators. At each timestep, the learning agents (predators) receive\na team-reward of rt = n where n is the number of predators touching the prey. The prey\nhas a greater max speed and acceleration than the predators. Therefore, to maximize their\nreturn, the two agents must coordinate in order to squeeze the prey into a corner or a wall\nand effectively trap it there. Termination is triggered when the maximum number of time\nsteps is reached.\nB.3\nTraining details\nIn all of our experiments, we use the Adam optimizer (Kingma and Ba, 2014) to perform\nparameter updates. All models (actors, critics and coach) are parametrized by feedforward\nnetworks containing two hidden layers of 128 units. We use the Rectified Linear Unit (ReLU)\n(Nair and Hinton, 2010) as activation function and layer normalization (Ba et al., 2016) on\nthe pre-activations unit to stabilize the learning. We use a buffer-size of 106 entries and a\nbatch-size of 1024. We collect 100 transitions by interacting with the environment for each\nlearning update. For all tasks in our hyperparameter searches, we train the agents for 15, 000\nepisodes of 100 steps and then re-train the best configuration for each algorithm-environment\npair for twice as long (30, 000 episodes) to ensure full convergence for the final evaluation.\nThe scale of the exploration noise is kept constant for the first half of the training time and\nthen decreases linearly to 0 until the end of training. We use a discount factor γ of 0.95 and\na gradient clipping threshold of 0.5 in all experiments. Finally for CoachReg, we fixed K to\n4 meaning that agents could choose between 4 sub-policies. Since policies’ hidden layers are\nof size 128 the corresponding value for C is 32. All experiments were run on Intel E5-2683\nv4 Broadwell (2.1GHz) CPUs in less than 12 hours.\n161\nB.4\nAlgorithms\nAlgorithm 3: Team\nRandomly initialize N critic networks Qi and actor networks µi\nInitialize the target weights\nInitialize one replay buffer D\nfor episode from 0 to number of episodes do\nInitialize random processes N i for action exploration\nReceive initial joint observation o0\nfor timestep t from 0 to episode length do\nSelect action ai = µi(oi\nt) + N i\nt for each agent\nExecute joint action at and observe joint reward rt and new observation ot+1\nStore transition (ot, at, rt, ot+1) in D\nend for\nSample a random minibatch of M transitions from D\nfor each agent i do\nEvaluate Li and Ji\nP G from Equations (6.1) and (6.2)\nfor each other agent (j ̸= i) do\nEvaluate Ji,j\nT S from Equations (6.3, 6.4)\nUpdate actor j with θj ←θj + αθ∇θj λ2Ji,j\nT S\nend for\nUpdate critic with ϕi ←ϕi −αϕ∇ϕiLi\nUpdate actor i with θi ←θi + αθ∇θi\n\u0010\nJi\nP G + λ1\nPN\nj=1 Ji,j\nT S\n\u0011\nend for\nUpdate all target weights\nend for\nAlgorithm 4: Coach\nRandomly initialize N critic networks Qi, actor networks µi and one coach network pc\nInitialize N target networks Qi′ and µi′\nInitialize one replay buffer D\nfor episode from 0 to number of episodes do\nInitialize random processes N i for action exploration\nReceive initial joint observation o0\nfor timestep t from 0 to episode length do\nSelect action ai = µi(oi\nt) + N i\nt for each agent\nExecute joint action at and observe joint reward rt and new observation ot+1\nStore transition (ot, at, rt, ot+1) in D\nend for\nSample a random minibatch of M transitions from D\nfor each agent i do\nEvaluate Li and Ji\nP G from Equations (6.1) and (6.2)\nUpdate critic with ϕi ←ϕi −αϕ∇ϕiLi\nUpdate actor with θi ←θi + αθ∇θiJi\nP G\nend for\nfor each agent i do\nEvaluate Ji\nE and Ji\nEP G from Equations (6.8) and (6.7)\nUpdate actor with θi ←θi + αθ∇θi\n\u0000λ1Ji\nE + λ2Ji\nEP G\n\u0001\nend for\nUpdate coach with ψ ←ψ + αψ∇ψ 1\nN\nPN\ni=1\n\u0000Ji\nEP G + λ3Ji\nE\n\u0001\nUpdate all target weights\nend for\n162\nB.5\nHyperparameter search\nB.5.1\nHyperparameter search ranges\nWe perform searches over the following hyperparameters: the learning rate of the actor αθ,\nthe learning rate of the critic ωϕ relative to the actor (αϕ = ωϕ ∗αθ), the target-network\nsoft-update parameter τ and the initial scale of the exploration noise ηnoise for the Ornstein-\nUhlenbeck noise generating process (Uhlenbeck and Ornstein, 1930) as used by Lillicrap et al.\n(2015). When using TeamReg and CoachReg, we additionally search over the regularization\nweights λ1, λ2 and λ3. The learning rate of the coach is always equal to the actor’s learning\nrate (i.e. αθ = αψ), motivated by their similar architectures and learning signals and in\norder to reduce the search space. Table B.1 shows the ranges from which values for the\nhyperparameters are drawn uniformly during the searches.\nTable B.1 Ranges for hyperparameter search, the log base is 10\nHyperparameter\nRange\nlog(αθ)\n[−8, −3]\nlog(ωϕ)\n[−2,\n2]\nlog(τ)\n[−3, −1]\nlog(λ1)\n[−3 ,\n0]\nlog(λ2)\n[−3 ,\n0]\nlog(λ3)\n[−1 ,\n1]\nηnoise\n[0.3, 1.8]\nB.5.2\nModel selection\nDuring training, a policy is evaluated on a set of 10 different episodes every 100 learning\nsteps. At the end of the training, the model at the best evaluation iteration is saved as the\nbest version of the policy for this training, and is re-evaluated on 100 different episodes to\nhave a better assessment of its final performance. The performance of a hyperparameter\nconfiguration is defined as the average performance (across seeds) of the best policies learned\nusing this set of hyperparameter values.\n163\nB.5.3\nSelected hyperparameters\nTables B.2, B.3, B.4, and B.5 shows the best hyperparameters found by the random searches\nfor each of the environments and each of the algorithms.\nTable B.2 Best found hyperparameters for the SPREAD environment\nHyperparameter\nDDPG\nMADDPG\nMADDPG+Sharing\nMADDPG+TeamReg\nMADDPG+CoachReg\nαθ\n5.3 ∗10−5\n2.1 ∗10−5\n9.0 ∗10−4\n2.5 ∗10−5\n1.2 ∗10−5\nωϕ\n53\n79\n0.71\n42\n82\nτ\n0.05\n0.083\n0.076\n0.098\n0.0077\nλ1\n-\n-\n-\n0.054\n0.13\nλ2\n-\n-\n-\n0.29\n0.24\nλ3\n-\n-\n-\n-\n8.4\nηnoise\n1.0\n0.5\n0.7\n1.2\n1.6\nTable B.3 Best found hyperparameters for the BOUNCE environment\nHyperparameter\nDDPG\nMADDPG\nMADDPG+Sharing\nMADDPG+TeamReg\nMADDPG+CoachReg\nαθ\n8.1 ∗10−4\n3.8 ∗10−5\n1.2 ∗10−4\n1.3 ∗10−5\n6.8 ∗10−5\nωϕ\n2.4\n87\n0.47\n85\n9.4\nτ\n0.089\n0.016\n0.06\n0.055\n0.02\nλ1\n-\n-\n-\n0.06\n0.0066\nλ2\n-\n-\n-\n0.0026\n0.23\nλ3\n-\n-\n-\n-\n0.34\nηnoise\n1.2\n0.9\n1.2\n1.0\n1.1\nTable B.4 Best found hyperparameters for the CHASE environment\nHyperparameter\nDDPG\nMADDPG\nMADDPG+Sharing\nMADDPG+TeamReg\nMADDPG+CoachReg\nαθ\n4.5 ∗10−4\n2.0 ∗10−4\n9.7 ∗10−4\n1.3 ∗10−5\n1.8 ∗10−4\nωϕ\n32\n64\n0.79\n85\n90\nτ\n0.031\n0.021\n0.032\n0.055\n0.011\nλ1\n-\n-\n-\n0.06\n0.0069\nλ2\n-\n-\n-\n0.0026\n0.86\nλ3\n-\n-\n-\n-\n0.76\nηnoise\n0.6\n1.0\n1.5\n1.0\n1.1\nTable B.5 Best found hyperparameters for the COMPROMISE environment\nHyperparameter\nDDPG\nMADDPG\nMADDPG+Sharing\nMADDPG+TeamReg\nMADDPG+CoachReg\nαθ\n6.1 ∗10−5\n3.1 ∗10−4\n6.2 ∗10−4\n1.5 ∗10−5\n3.4 ∗10−4\nωϕ\n1.7\n0.94\n0.58\n90\n29\nτ\n0.065\n0.045\n0.007\n0.02\n0.0037\nλ1\n-\n-\n-\n0.0013\n0.65\nλ2\n-\n-\n-\n0.56\n0.5\nλ3\n-\n-\n-\n-\n1.3\nηnoise\n1.1\n0.7\n1.3\n1.6\n1.6\n164\nTable B.6 Best found hyperparameters for the 3-vs-1-with-keeper Google Football environ-\nment\nHyperparameter\nMADDPG\nMADDPG+Sharing\nMADDPG+TeamReg\nMADDPG+CoachReg\nαθ\n1.6 ∗10−6\n3.4 ∗10−5\n3.5 ∗10−6\n9.4 ∗10−5\nωϕ\n3.1\n13\n0.96\n2.9\nτ\n0.004\n0.0014\n0.0066\n0.018\nλ1\n-\n-\n0.1\n0.027\nλ2\n-\n-\n0.02\n0.027\nλ3\n-\n-\n-\n2.4\nB.5.4\nSelected hyperparameters (ablations)\nTables B.7, B.8, B.9, and B.10 shows the best hyperparameters found by the random searches\nfor each of the environments and each of the ablated algorithms.\nTable B.7 Best found hyperparameters for the SPREAD environment\nHyperparameter\nMADDPG+Agent Modelling\nMADDPG+Policy Mask\nαθ\n1.3 ∗10−5\n6.8 ∗10−5\nωϕ\n85\n9.4\nτ\n0.055\n0.02\nλ1\n0.06\n0\nλ2\n0\n0\nλ3\n-\n0\nηnoise\n1.0\n1.1\nTable B.8 Best found hyperparameters for the BOUNCE environment\nHyperparameter\nMADDPG+Agent Modelling\nMADDPG+Policy Mask\nαθ\n1.3 ∗10−5\n2.5 ∗10−4\nωϕ\n85\n0.52\nτ\n0.055\n0.0077\nλ1\n0.06\n0\nλ2\n0\n0\nλ3\n-\n0\nηnoise\n1.0\n1.3\nTable B.9 Best found hyperparameters for the CHASE environment\nHyperparameter\nMADDPG+Agent Modelling\nMADDPG+Policy Mask\nαθ\n2.5 ∗10−5\n6.8 ∗10−5\nωϕ\n42\n9.4\nτ\n0.098\n0.02\nλ1\n0.054\n0\nλ2\n0\n0\nλ3\n-\n0\nηnoise\n1.2\n1.1\n165\nTable B.10 Best found hyperparameters for the COMPROMISE environment\nHyperparameter\nMADDPG+Agent Modelling\nMADDPG+Policy Mask\nαθ\n1.2 ∗10−4\n2.5 ∗10−4\nωϕ\n0.71\n0.52\nτ\n0.0051\n0.0077\nλ1\n0.0075\n0\nλ2\n0\n0\nλ3\n-\n0\nηnoise\n1.8\n1.3\nB.5.5\nHyperparameter search results\nThe performance distributions across hyperparameters configurations for each algorithm on\neach task are depicted in Figure B.1 using box-and-whisker plot.\nIt can be seen that,\nwhile most algorithms can perform reasonably well with the correct configuration, Team-\nReg, CoachReg as well as their ablated versions boost the performance of the third quartile,\nsuggesting an increase in the robustness across hyperparameter compared to the baselines.\n0\n50\n100\n150\n200\naverage return\nSPREAD\n0\n2\n4\n6\n8\nBOUNCE\nMADDPG \n+  CoachReg (ours)\nMADDPG \n+  policy mask\nMADDPG \n+  TeamReg (ours)\nMADDPG \n+  agent modelling\nMADDPG \n+  sharing\nMADDPG\nDDPG\n0\n5\n10\n15\n20\n25\naverage return\nCOMPROMISE\nMADDPG \n+  CoachReg (ours)\nMADDPG \n+  policy mask\nMADDPG \n+  TeamReg (ours)\nMADDPG \n+  agent modelling\nMADDPG \n+  sharing\nMADDPG\nDDPG\n0\n200\n400\n600\n800\n1000\nCHASE\nFigure B.1 Hyperparameter tuning results for all algorithms. There is one distribution per\n(algorithm, environment) pair, each one formed of 50 data-points (hyperparameter config-\nuration samples).\nEach point represents the best model performance averaged over 100\nevaluation episodes and averaged over the 3 training seeds for one sampled hyperparameters\nconfiguration. The box-plots divide in quartiles the 49 lower-performing configurations for\neach distribution while the score of the best-performing configuration is highlighted above\nthe box-plots by a single dot.\n166\nB.6\nThe effects of enforcing predictability (additional results)\nThe results presented in Figure 6.5 show that MADDPG + TeamReg is outperformed by all\nother algorithms when considering average return across agents. In this section we seek to\nfurther investigate this failure mode.\nImportantly, COMPROMISE is the only task with a competitive component (i.e. the only\none in which agents do not share their rewards). The two agents being strapped together, a\ngood policy has both agents reach their landmark successively (e.g. by having both agents\nnavigate towards the closest landmark). However, if one agent never reaches for its landmark,\nthe optimal strategy for the other one becomes to drag it around and always go for its own,\nleading to a strong imbalance in the return cumulated by both agents. While such scenario\ndoesn’t occur for the other algorithms, we found TeamReg to often lead to cases of domination\nsuch as depicted in Figure B.3.\nFigure B.2 depicts the performance difference between the two agents for every 150 runs of\nthe hyperparameter search for TeamReg and the baselines, and shows that (1) TeamReg is\nthe only algorithm that leads to large imbalances in performance between the two agents and\n(2) that these cases where one agent becomes dominant are all associated with high values\nof λ2, which drives the agents to behave in a predictable fashion to one another.\nLooking back at Figure B.3, while these domination dynamics tend to occur at the beginning\nof training, the dominated agent eventually gets exposed more and more to sparse reward\ngathered by being dragged (by chance) onto its own landmark, picks up the goal of the\ntask and starts pulling in its own direction, which causes the average return over agents\n0\n5\n10\n15\n20\n25\n|\nperf|\nDDPG\nMADDPG\nMADDPG + sharing\nMADDPG + TeamReg (ours)\n10\n3\n10\n2\n10\n1\n100\n2\nFigure B.2 Average performance difference (∆perf) between the two agents in COMPROMISE\nfor each 150 runs of the hyperparameter searches (left). All occurrences of abnormally high\nperformance difference are associated with high values of λ2 (right).\n167\nto drop as we see happening midway during training in Figure 6.5. These results suggest\nthat using a predictability-based team-regularization in a competitive task can be harmful;\nquite understandably, you might not want to optimize an objective that aims at making your\nbehavior predictable to your opponent.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n0\n15\n30\n45\nAverage Return\nDDPG\nagent 0\nagent 1\n0\n5000\n10000\n15000\n20000\n25000\n30000\n0\n15\n30\n45\nMADDPG\nagent 0\nagent 1\n0\n5000\n10000\n15000\n20000\n25000\n30000\nEpisodes\n0\n15\n30\n45\nAverage Return\nMADDPG + sharing\nagent 0\nagent 1\n0\n5000\n10000\n15000\n20000\n25000\n30000\nEpisodes\n0\n15\n30\n45\nMADDPG + TeamReg\nagent 0\nagent 1\nFigure B.3 Learning curves for TeamReg and the three baselines on COMPROMISE. We\nsee that while both agents remain equally performant as they improve at the task for the\nbaseline algorithms, TeamReg tends to make one agent much stronger than the other one.\nThis domination is optimal as long as the other agent remains docile, as the dominant agent\ncan gather much more reward than if it had to compromise. However, when the dominated\nagent finally picks up the task, the dominant agent that has learned a policy that does\nnot compromise see its return dramatically go down and the mean over agents overall then\nremains lower than for the baselines.\nB.7\nAnalysis of sub-policy selection (additional results)\nB.7.1\nMask densities\nWe depict on Figure B.4 the mask distribution of each agent for each (seed, environment)\nexperiment when collected on a 100 different episodes. Firstly, in most of the experiments,\nagents use at least 2 different masks. Secondly, for a given experiments, agents’ distributions\nare very similar, suggesting that they are using the same masks in the same situations and\nthat they are therefore synchronized. Finally, agents collapse more to using only one mask on\nCHASE, where they also display more dissimilarity between one another. This may explain\nwhy CHASE is the only task where CoachReg does not improve performance. Indeed, on\nCHASE, agents do not seem synchronized nor leveraging multiple sub-policies which are the\npriors to coordination behind CoachReg. In brief, we observe that CoachReg is less effective\nin enforcing those priors to coordination of CHASE, an environment where it does not boost\nnor harm performance.\n168\nFigure B.4 Agent’s policy mask distributions. For each (seed, environment) we collected the\nmasks of each agents on 100 episodes.\nB.7.2\nEpisodes rollouts with synchronous sub-policy selection\nWe display here and on https://sites.google.com/view/marl-coordination/ some in-\nteresting sub-policy selection strategy evolved by CoachReg agents.\nOn Figure B.5, the\nagents identified two different scenarios depending on the target-ball location and use the\ncorresponding policy mask for the whole episode. Whereas on Figure B.5, the agents syn-\nchronously switch between policy masks during an episode. In both cases, the whole group\n169\nselects the same mask as the one that would have been suggested by the coach.\n(a) BOUNCE: The ball is on the left side of the target, agents both select the purple policy mask\nt = 0, C = \nt = 5, C = \nt = 10, C =  \nt = 15, C = \nt = 50, C =\nt = 59, C =\nt = 60, C =\nt = 65, C =\n(b) BOUNCE: The ball is on the right side of the target, agents both select the green policy mask\nt = 0, C = \nt = 5, C = \nt = 10, C = \nt = 15, C = \nt = 50, C =\nt = 58, C =\nt = 59, C =\nt = 65, C =\nFigure B.5 Visualization of two different BOUNCE evaluation episodes.\nNote that here,\nthe agents’ colors represent their chosen policy mask. Agents have learned to synchronously\nidentify two distinct situations and act accordingly. The coach’s masks (not used at evaluation\ntime) are displayed with the timestep at the bottom of each frame.\n(a) SPREAD\nt = 0, C = \nt = 5, C = \nt = 20, C =\nt = 10, C =  \nt = 15, C = \nt = 25, C =\nt = 30, C = \nt = 35, C = \n(b) COMPROMISE\nt = 0, C = \nt = 3, C = \nt = 17, C =\nt = 6, C =  \nt = 7, C = \nt = 22, C =\nt = 32, C = \nt = 33, C = \nt = 34, C = \nt = 37, C = \nt = 40, C =  \nt = 42, C = \nt = 43, C =\nt = 48, C =\nt = 51, C =\nt = 52, C =\nt = 53, C = \nt = 54, C = \nt = 64, C = \nt = 65, C = \nt = 66, C =\nt = 67, C =\nt = 68, C =\nt = 73, C =\nFigure B.6 Visualization of sequences on two different environments. An agent’s color repre-\nsent its current policy mask. The coach’s masks (not used at evaluation time) are displayed\nwith the timestep at the bottom of each frame. Agents synchronously switch between the\navailable policy masks.\n170\nB.7.3\nMask diversity and synchronicity (ablation)\nAs in Subsection 6.7.3 we report the mean entropy of the mask distribution and the mean\nHamming proximity for the ablated “MADDPG + policy mask” and compare it to the full\nCoachReg. With “MADDPG + policy mask” agents are not incentivized to use the same\nmasks. Therefore, in order to assess if they synchronously change policy masks, we computed,\nfor each agent pair, seed and environment, the Hamming proximity for every possible masks\nequivalence (mask 3 of agent 1 corresponds to mask 0 of agent 2, etc.) and selected the\nequivalence that maximised the Hamming proximity between the two sequences.\nWe can observe that while “MADDPG + policy mask” agents display a more diverse mask\nusage, their selection is less synchronized than with CoachReg. This is easily understandable\nas the coach will tend to reduce diversity in order to have all the agents agree on a common\nmask, on the other hand this agreement enables the agents to synchronize their mask selec-\ntion. To this regard, it should be noted that “MADDPG + policy mask” agents are more\nsynchronized that agents independently sampling their masks from k-CUD, suggesting that,\neven in the absence of the coach, agents tend to synchronize their mask selection.\nFigure B.7 (Left) Entropy of the policy mask distributions for each task and method, aver-\naged over agents and training seeds. Hmax,k is the entropy of a k-CUD. (Right) Hamming\nProximity between the policy mask sequence of each agent averaged across agent pairs and\nseeds. randk stands for agents independently sampling their masks from k-CUD. Error bars\nare SE across seeds.\n171\nB.8\nScalability with the number of agents\nB.8.1\nComplexity\nIn this section we discuss the increases in model complexity that our methods entail. In prac-\ntice, this complexity is negligible compared to the overall complexity of the CTDE framework.\nTo that respect, note that (1) the critics are not affected by the regularizations, so our ap-\nproaches only increase complexity for the forward and backward propagation of the actor,\nwhich consists of roughly half of an agent’s computational load at training time. Moreover,\n(2) efficient design choices significantly impact real-world scalability and performance: we\nimplement TeamReg by adding only additional heads to the pre-existing actor model (ef-\nfectively sharing most parameters for the teammates’ action predictions with the agent’s\naction selection model). CoachReg consists only of an additional linear layer per agent and\na unique Coach entity for the whole team (which scales better than a critic since it only\ntakes observations as inputs). As such, only a small number of additional parameters need\nto be learned relatively to the underlying base CTDE algorithm. For a TeamReg agent, the\nnumber of parameters of the actor increases linearly with the number of agents (additional\nheads) whereas the critic model grows quadratically (since the observation size themselves\nusually depend on the number of agents). In the limit of increasing the number of agents,\nthe proportion of added parameters by TeamReg compared to the increase in parameters of\nthe centralised critic vanishes to zero. On the SPREAD task for example, training 3 agents\nwith TeamReg increases the number of parameters by about 1.25% (with similar computa-\ntional complexity increase). With 100 agents, this increase is only of 0.48%. For CoachReg,\nthe increase in an agent’s parameter is independent of the number of agent. Finally, any\nadditional heads in TeamReg or the Coach in CoachReg are only used during training and\ncan be safely removed at execution time, reducing the systems computational complexity to\nthat of the base algorithm.\nB.8.2\nRobustness\nTo assess how the proposed methods scale to greater number of agents, we increase the\nnumber of agents in the SPREAD task from three to six agents. The results presented in\nFigure B.8 show that the performance benefits provided by our methods hold when the num-\nber of agents is increased. Unsurprisingly, we also note how quickly learning becomes more\nchallenging when the number of agents rises. Indeed, with each new agent, the coordina-\ntion problem becomes more and more difficult, and that might explain why our methods\nthat promote coordination maintain a higher degree of performance. Nonetheless, in the\nsparse reward setting, the complexity of the task soon becomes too difficult and none of the\n172\nalgorithms is able to solve it with six agents.\nWhile these results show that our methods do not contribute to a quicker downfall when\nthe number of agents is increased, they are not however aimed at tackling the problem of\nmassively-multi-agent RL. Other approaches that use attention heads (Iqbal and Sha, 2019)\nor restrict one agent perceptual field to its n-closest teammates are better suited to these\nparticular challenges and our proposed regularisation schemes could readily be adapted to\nthese settings as well.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n0\n80\n160\nReturn\nSPREAD - 3 AGENTS\n0\n5000\n10000\n15000\n20000\n25000\n30000\n100\n0\n100\n200\nSPREAD - 4 AGENTS\n0\n5000\n10000\n15000\n20000\n25000\n30000\nEpisodes\n120\n60\n0\n60\nReturn\nSPREAD - 5 AGENTS\nDDPG\nMADDPG\nMADDPG + sharing\nMADDPG + TeamReg (ours)\nMADDPG + CoachReg (ours)\n0\n5000\n10000\n15000\n20000\n25000\n30000\nEpisodes\n120\n60\n0\nSPREAD - 6 AGENTS\nFigure B.8 Learning curves (mean return over agents) for all algorithms on the SPREAD\nenvironment for varying number of agents. Solid lines are the mean and envelopes are the\nStandard Error (SE) across the 10 training seeds.\n173\nAPPENDIX C\nSUPPLEMENTARY MATERIAL FOR CHAPTER 7\nC.1\nAlgorithm\nOur implementation of the SAC-Lagrangian algorithm is presented below. The exact values\nof each hyperparameter for all of our experiments are listed in Tables C.1 and C.2. One\nnotable difference between an unconstrained Soft-Actor Critic (Haarnoja et al., 2018) and our\nconstrained version is that SAC is typically updated after every environment step to maximise\nthe sample efficiency of the algorithm. In the constrained case however, since the constraints\nare optimized on-policy, updating the SAC agent at every environment step would only allow\nfor one-sample estimates of the multiplier’s objective. On the other hand, freezing the SAC-\nagent for as many environment steps as the Lagrange multiplier batch-size Nλ makes the\noverall algorithm significantly less sample efficient. One could disregard the “on-policyness”\nof the multiplier’s objective but in preliminary experiments we found that, unsurprisingly,\nupdating the Lagrange multipliers very frequently while using a large set of samples (many of\nwhich were collected using previous versions of the policy) lead to significant overshoot and\nharms the ability of the multipliers to converge to a stable behavior. There is thus a trade-\noff to make between the variance of the multiplier’s objective estimate, the degree to which\nthe multipliers are updated on-policy and the sample efficiency of the overall algorithm. In\npractice we found that the values for Mθ and Mλ presented in Tables C.1 and C.2 represented\ngood compromises between these different characteristics. Another important detail is that\nwe use K +1 separate critics to model the discounted expected sum of reward and costs. Q(0)\nis the critic that models the main objective and Q(k), k = 1, . . . , K + 1 are the critics that\nmodel the constraint components of the Lagrangian. Using separate critics allows to avoid\nfast changes in the scale of the objective, as seen by the critics, when the multipliers λk get\nadjusted; they can solely focus on modeling the agent’s changing behavior with respect to\ntheir respective function (reward or costs).\n174\nAlgorithm 5: SAC-Lagrangian with Bootstrap Constraint\nRequire: learning rate β, replay buffer B, entropy coefficient α and minibatch sizes Nθ and Nλ\nRequire: Initialise the policy πθ and value-functions Q(k)\nϕ\nrandomly, k = 0, . . . , K + 1\nRequire: Initialise the Lagrange multiplier parameters zk\nRequire: Collect enough transitions to fill B with max(Nθ, Nλ) samples\nfor updates u = 1, ... (until convergence) do\n# Data collection\nSample from the current policy: a ∼πθ(·|s)\nQuery next state, reward and indicators (s′, r, {e}K+1\nk=1 ) by interacting with the environment\nAppend transition (s, a, r, s′, {e}K\nk=1 + 1) to the replay buffer B\n# Policy Gradient update\nif u % Mθ == 0 then\nSample a minibatch of Nθ transitions uniformly from the replay buffer\nSample next actions:\na′\ni ∼πθ(·|s′\ni)\ni = 1, ..., Nθ\nfor k = 0, . . . , K + 1 do\nSet the “rewards” to their corresponding values:\nr(0)\ni\n= ri\nand\nr(k)\ni\n= e(k)\ni\nCompute the Q-targets:\ny(k)\ni\n= −α log πθ(a′\ni|s′\ni) + minj∈{1,2} Q(k)\nϕj (s′\ni, a′\ni)\nAdam descent on Q-nets with:\n∇ϕj\n1\nNθ\nPNθ\ni=1 ||Q(k)\nϕj (si, ai) −\n\u0000r(k)\ni\n+ (1 −done)γy(k)\ni\n\u0001||2\nend for\nRe-sample the current actions:\nai ∼πθ(·|si)\ni = 1, ..., Nθ\nAdam ascent on policy with:\n∇θ\n1\nNθ\nNθ\nX\ni=1\n−α log πθ(ai|si) + max(λ0, λK+1) min\nj\nQ(0)\nϕj (si, ai)\n+ λK+1 min\nj\nQ(K+1)\nϕj\n(si, ai) −\nK\nX\nk=1\nλk min\nj\nQ(k)\nϕj (si, ai)\nend if\n# Multipliers update\nif u % Mλ == 0 then\nDraw from the replay buffer a minibatch composed of the last Nλ transitions\nfor k = 0, . . . , K + 1 do\nCompute average costs:\n˜JCk(π) =\n1\nNλ\nPNλ\ni=1 e(k)\ni\nAdam descent on multipliers with:\n∇zkλk( ˜JCk(π) −˜dk) if k = K + 1 else\n∇zkλk( ˜dk −˜JCk(π))\nend for\nend if\nend for\n175\nC.2\nDetails for experiments in the Arena environment\nC.2.1\nEnvironment details\nIn the Arena Environment, the agent’s main goal is to navigate to the green tile (see Fig-\nure 7.1, left). The constraints that we explore in this environment are {On-Ground, Not-\nin-Lava, Looking-At-Marker, Under-Speed-Limit and Above-Energy-Limit}.\nIt receives as\nobservations its XYZ position, direction and velocity, the relative XZ position of the goal,\nits distance to the goal, as well as an indicator for whether it is on the ground. For the\nlooking-at constraint, it also receives the XZ vector for the direction it is looking at, its\nY-angular velocity, the marker’s relative XZ position and distance, the normalised angle be-\ntween the agent’s looking direction and the marker as well as an indicator for whether the\nmarker is within its field of view (a fixed-angle cone in front of the agent). For the energy\nconstraint, the agent receives the normalised value of its energy bar and an indicator for\nwhether it is currently recharging. Finally for the lava constraint, the agent receives an indi-\ncator of whether it currently stands in lava as well as an indicator for 25 vertical raycast of\nits surrounding (0 indicating safe ground and 1 indicating lava). We also add to the agent’s\nobservations the per-episode rates of indicator cost functions to the agent observation for\neach of the constraint as well a normalised representation of the remaining time-steps before\nreaching the time limit condition, leading to a total dimensionality of 53 for the observation\nvector. The action space is composed of 5 continuous actions (clamped between -1 and 1)\nwhich represent its XZ velocity and Y-angular velocity, a jump action (jump is triggered when\nthe agent outputs a value above 0 for that dimensionality) and a recharge action (also with\nthreshold of 0). The reward function is simply 1 when the agent reaches the goal (causing\ntermination), 0 otherwise, and augmented with a small shaping reward function (Ng et al.,\n1999) based on whether the agent got closer or further away from the goal location.\nC.2.2\nHyperparameters\nMost of the hyperparameters are the same as in the original unconstrained Soft Actor-Critic\n(SAC) (Haarnoja et al., 2018). Some additional hyperparameters emerge from the constraint\nenforcement aspect of our version of SAC-Lagrangian and are described in the Algorithm\nsection above. We use the Adam optimizer (Kingma and Ba, 2014) for all parameter updates\n(policy, critics and Lagrange multipliers). For all experiments taking place in the Arena\nEnvironment, the policy is parameterized as a a two layer neural networks that outputs the\nparameters of a Gaussian distribution with a diagonal covariance matrix. The hidden layers\nare composed of 256 units and followed by a tanh activation function. The first hidden layer\n176\nalso uses layer-normalisation before the application of the tanh function. We use K + 1 fully\nindependent critic models to estimate the expected discount sum of each of the constraint and\nof the main reward function. The critic models are also parameterized with two-hidden-layers\nneural networks with the same size for the hidden layers as the policy but instead followed\nby relu activation functions. Table C.1 shows the hyperparameters used in our experiments\nconducted in the Arena environment.\nTable C.1 Hyperparameters for experiments in the Arena Environment.\nGeneral\nDiscount factor γ\n0.9\nNumber of random exploration steps\n10000\nNumber of buffer warmup steps\n2560\nSAC Agent\nLearning rate β\n0.0003\nTransitions between updates Mθ\n200\nBatch size Nθ\n256\nReplay buffer size\n1,000,000\nInitial entropy coefficient α\n0.02\nTarget networks soft-update coefficient τ\n0.005\nLagrange Multipliers\nLearning rate β\n0.03\nInitial multiplier parameters value zk\n0.02\nTransitions between updates Mλ\n2000\nBatch size Nλ\n2000\nConstraint Thresholds\nHas reached goal (lower-bound)\n0.99\nNOT looking at marker\n0.10\nNOT on ground\n0.40\nIn lava\n0.01\nAbove speed limit\n0.01\nIs under the minimum energy level\n0.01\n177\nC.3\nDetails for experiments in the OpenWorld environment\nC.3.1\nEnvironment details\nThe OpenWorld environment is a large environment (approximately 30, 000 times larger than\nthe agent) that includes multiple multi-storey buildings with staircases, mountains, tunnels,\nnatural bridges and lava. In addition, the environment includes 50 jump-pads that propel\nthe agent into the air when it steps on one of them. The agent is tasked with navigating\ntowards a goal randomly placed in the environment at the beginning of every episode. The\nagent controls include translation in the XY frame (2 inputs), a jumping action (1 input), a\nrotation action controlling where the agent is looking independent of its direction of travel (1\ninput), and a recharging action which allows the agent to recharge its energy level (1 input).\nThe recharging action immobilizes the agent, i.e., it does not allow the agent to progress\ntowards its goal. The environment also includes a look-at marker which we would like the\nagent to look at while it accomplishes its main navigation task.\nAt every timestep, the agent receives as observations its XYZ position relative to the goal as\nwell as its normalized velocity and acceleration in the environment. In addition, it receives its\nrelative position to the nearest jump-pad in the environment. For looking at the marker, as\nin the Arena environment, the agent receives the marker’s relative XZ position and distance,\nthe normalised angle between the agent’s looking direction and the marker, as well as an\nindicator for whether the marker is within its field of view (a fixed-angle cone in front of\nthe agent). For the energy-limit constraint, the agent obtains the value of its energy level, a\nboolean describing if it is currently recharging and a Boolean indicating if it was recharging\nin the previous timestep. The agent also receives a series of indicators denoting whether it is\ncurrently standing in lava, if it is touching the ground, and if the agent is currently below the\nminimum energy level. In order for the agent to observe lava and other elements it can collide\nwith in the environment (e.g., buildings, doors, mountains), the agent receives 2 channels of\n8 × 8 raycasts around the agent.\nC.3.2\nHyperparameters\nThe SAC agent in the OpenWorld environment uses the same architecture and similar hy-\nperparameters as in (Alonso et al., 2020). The raycasts and raw state described above are\nprocessed using two separate embedding models. For the raycasts, we employ a CNN with 3\nconvolutional layers, each with a corresponding ReLU layer. The raw state is processed using\na separate 3-layer MLP with 1024 hidden units at each layer. The two representations are\nconcatenated into a single vector representing the current state. The policy is parameterized\n178\nby a 3-layer MLP that receives as input the concatenated representation and outputs the\nparameters of a Gaussian distribution with a diagonal covariance matrix. Each hidden layer\nis composed of 1024 hidden units and is followed by a ReLU activation function. The critic\nmodels are also parameterized by 3-layer MLP, are composed of 1024 hidden units and use\nReLU activation functions. Table C.2 shows some of these hyperparameters with a focus on\nthe constrained enforcement aspect of our version of SAC-Lagrangian.\nTable C.2 Hyperparameters for experiments in the OpenWorld Environment.\nGeneral\nDiscount factor γ\n0.99\nNumber of random exploration steps β\n200\nNumber of buffer warmup steps β\n2560\nSAC Agent\nLearning rate β\n0.0001\nBatch size Nθ\n2560\nReplay buffer size\n4,000,000\nInitial entropy coefficient α\n0.005\nTarget networks soft-update coefficient τ\n0.005\nLagrange Multipliers\nLearning rate β\n0.00005\nInitial multiplier parameters value zk\n0.02\nTransitions between updates\nevery timestep\nBatch size Nλ\n5000\nConstraint Thresholds\nHas reached goal (lower-bound)\n0.80\nNOT looking at marker\n0.10\nNOT on ground\n0.40\nIn lava\n0.001\nIs under the minimum energy level\n0.01\n179\nC.4\nAdditional experiments on reward engineering\nSee Section 7.2 for the description of our experiments motivating against the use of reward\nengineering for behavior specification. Figure C.1 below shows the results for the biggest of\nthe 3 grid searches performed to showcase the difficulty of finding a reward function that fits\nthe behavioral requirements when the number of requirements grows.\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\n-0.1\nwno\nenergy\nAverage Episodic Return\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nNot above Energy Limit < 0.01\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nin Lava < 0.01\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nNot Looking at Marker < 0.10\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nAverage Episodic Return\nfor feasible policies\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\n-0.25\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\n-0.5\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\n-1.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\n-2.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\n-4.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\nwin\nlava\n-10.0\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n-0.1 -0.25 -0.5\n-1.0\n-2.0\n-4.0 -10.0\nwnot\nlooking\n-10.0\n-4.0\n-2.0\n-1.0\n-0.5\n-0.25\n-0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure C.1 Also see Figure 7.2.\nWhen enforcing 3 behavioral requirements with reward\nengineering, an ever larger proportion of the experiments are wasted finding either low-\nperforming policies or policies that do not satisfy the behavioral constraints. In this case,\nnone of the 343 experiments yielded a feasible policy that also solves the task (success rate\nnear 1.0), showcasing that reward engineering scales poorly with the number of constraints\ndue to the curse of dimensionality and to the composing effect of the multiple constraints in\nnarrowing the space of feasible policies.\n180\nC.5\nAdditional experiments on TD3\nWe validate that our framework can be combined with any policy optimisation algorithm by\napplying it to the TD3 algorithm (Fujimoto et al., 2018). This leads to a TD3-Lagrangian for-\nmulation using our indicator cost functions, normalized multipliers and bootstrap constraint.\nAs for our experiments with SAC (Figure 7.5-d), our TD3-Lagrangian agent performs well\nand all constraints are satisfied. The results are presented in Figure C.2.\nFigure C.2 TD3-Lagrangian agent in the Arena environment using normalised multipliers,\nindicator cost functions and using the success constraint as a bootstrap constraint. Training\nis halted after every 20, 000 environment steps and the agent is evaluated for 10 episodes.\nAll curves show the average over 5 seeds and envelopes show the standard error around that\nmean.\n181\nAPPENDIX D\nSUPPLEMENTARY MATERIAL FOR CHAPTER 8\nD.1\nTask and Training Details\nWe use the GFlowNet framework (Bengio et al., 2021, 2023) to train discrete distribution\nsamplers over the space of molecules that can be assembled from a set of pre-defined molec-\nular fragments (Kumar et al., 2012). A state is represented as a graph in which each node\nrepresents a fragment from the fragment library and where each edge has two attributes\nrepresenting the attachment point of each connected fragment to its neighbor. The state\nrepresentation is augmented with a fully-connected virtual node, whose features are an em-\nbedding of the conditioning information computed from the conditioning vector that rep-\nresents the preferences w and/or the goal direction dg. To produce the state-conditional\ndistribution over actions, the model processes the state using a graph transformer architec-\nture (Yun et al., 2019) for a predefined number of message-passing steps (number of layers).\nOur GFlowNet sampler thus starts from the initial state s0 representing an empty graph. It\niteratively constructs a molecule by either adding a node or an edge to the current state st\nuntil it eventually selects the ‘STOP’ action.\nTo maintain some amount of exploration throughout training, at each construction step\nt, the model samples a random action with probability ϵ and otherwise samples from its\nforward transition distribution. The model is trained using the trajectory balance criterion\n(Malkin et al., 2022a) and thus is parameterised by a forward action distribution PF and an\nestimation of the partition function Z := P\nx R(x). Forbidden actions are masked out from\nthe forward transition distribution (for example, the action of adding an edge to the empty\nstate). We use a uniform distribution for the backward policy PB. To prevent the sampling\ndistribution from changing too abruptly, we collect new trajectories from a sampling model\nPF( · |θsampling) which uses a soft update with hyperparameter τ to track the learned GFN at\nupdate k: θ(k)\nsampling ←τ · θ(k−1)\nsampling + (1 −τ) · θ(k). This is akin to the target Q-functions and\ntarget policies used in actor-critic frameworks (Mnih et al., 2015; Fujimoto et al., 2018).\nThe hyperparameters used for training both methods are listed in Table D.1.\nD.2\nFailure Modes and Filtering\nWhile using goal regions as hard constraints offers a more precise tool for controllable gen-\neration, it faces the additional challenge that not all goals may be feasible (or that reaching\n182\nTable D.1 Hyperparameters used in our conditional-GFN training pipeline\nHyperparameters\nValues\nGoal-conditioned GFN\nPreference-conditioned GFN\nBatch size\n64\n64\nGFN temperature parameter β\n60\n60\nNumber of training steps\n40,000\n40,000\nNumber of GNN layers\n2\n2\nGNN node embedding size\n256\n256\nLearning rate for GFN’s PF\n10−4\n10−4\nLearning rate for GFN’s Z-estimator\n10−3\n10−3\nSampling moving average τ\n0.95\n0.95\nRandom action probability ϵ\n0.01\n0.01\nFocus region cosine similarity threshold cg\n0.98\n-\nLimit reward coefficient mg\n0.20\n-\nReplay buffer length\n100,000\n-\nNumber of replay buffer trajectory warmups\n1,000\n-\nHindsight ratio\n0.30\n-\nConditioning-vector sampling distribution\ndg ∼\n\u001a\nUniform-GS\n(Sec 8.4.2)\nTab-GS\n(Sec 8.4.3)\nw ∼Dirichlet(1)\nsome goals may be much easier to learn than others). When a model is conditioned with an\ninfeasible goal, all the samples that it will observe will have a reward R(x) = 0. The proper\nbehavior, in that case, is to sample any possible molecule with equal weight, thus sampling\nuniformly across the entire molecular state space. Such molecules generally won’t be of any\ninterest and can be discarded. Thus, in our experiments, we filter out such out-of-focus sam-\nples (molecules falling outside the focus region) and evaluate the candidates that were inside\ntheir prescribed focus region. Figure D.1 shows the conditional distributions learned by a\nsingle model trained on the 2-objective task. The picture on the last row, second column\nshowcases such an occurrence of difficult focus region which results in many samples simply\nbelonging to the uniform distribution over the state space.\nD.3\nAblations\nD.3.1\nReplay Buffer\nWhile both the un-conditional and the preference-conditioned GFN models are learning sta-\nbly even in a purely on-policy setting, we found that the goal-conditioned models were more\nprone to instabilities and mode-collapse when employed purely on-policy (see Figure D.2).\nThis could be because imposing these hard constraints on the generative behavior of the\nmodel drastically changes the reward landscape from one set of goals to another. While\nlarger batches could potentially alleviate this problem, sampling uniformly from a replay\nbuffer of the last trajectories proved effective, as observed in many works stemming from\nMnih et al. (2015). As described in Section 8.3.1, we also use hindsight experience replay\n(Andrychowicz et al., 2017). Specifically, for every batch of data, we randomly select a subset\n183\nFigure D.1 Learned conditional-distributions for different focus regions passed as input to the\nsame model. Each dot marks the image of a generated molecule in the objective space. The\ncolors indicate how densely populated a particular area of the objective space is (brighter is\ndenser). The focus regions (goal regions) are depicted in light blue. The distribution on the\nlast row, second column, showcases a focus region which seems difficult to reach and may\nnot contain as large a population of molecules in the state space. In such cases, the model\ncannot learn to consistently produce samples from that goal region when conditioned on\nthis goal direction dg and will instead produce several samples very similar to the sampling\ndistribution of an untrained model (uniform across the state space).\nof trajectories (hindsight-ratio * batch-size), among which we re-label both the goal direction\ndg and the corresponding reward for the examples that didn’t reach their goal.\nFigure D.2 Learning curves for goal-conditioned models either trained purely on-policy (in\nblue) or using a replay buffer of past trajectories (in orange) on the 2-objective (seh, qed)\ntask.\n184\nD.3.2\nLimit Reward Coefficient\nWhile the GFN model is given the goal direction dg as input, the width of the goal region,\nwhich depends on the cosine-similarity threshold cg is fixed, and the model adapts to pro-\nducing samples within the region over time by trial-and-error. One can trade off the level\nof controllability of the goal-conditioned model with the difficulty of reaching those goals\nby increasing or reducing cg. Another way to increase the controllability and goal-reaching\naccuracy without drastically affecting the difficulty of reaching such goals is to make the\nmodel preferentially generate samples near the center of the focus region, thus reducing the\nrisk of producing an out-of-focus sample due to epistemic uncertainty. To do so, we modify\nEquation 8.2 and add a reward-coefficient αg, which further modulates the magnitude of the\nscalar reward based on how close to the center of the focus region the sample was generated.\nWhile many shaping functions could be devised, we choose the following form:\nRg(x) =\n\n\n\n\n\nαg\nP\nk rk,\nif r ∈g\n0,\notherwise\n,\nαg =\n \nr · dg\n||r|| · ||dg||\n! log mg\nlog cg\n(D.1)\nFigure D.3 Effect of the hyperparameter mg on the profile of the reward coefficient αg and\nthe learned sampling distribution (top row).\nIn words, the reward coefficient αg is equal to the cosine similarity between the reward vector\nr and the goal direction dg exponentiated in such a way that αg = mg at the limit of the\nfocus region. So for example, setting mg = 0.2 means that the reward is maximal at the\ncenter of the focus region, is at 20% of that magnitude at the limit of the focus region, and\nfollows a sharp sigmoid-like profile in between. Figure D.3 showcases the reward coefficient\n185\nas a function of the angle between r and dg for different values of mg and the corresponding\ndistributions learned by the model. We can see that a smaller value of mg encourages the\nmodel to produce samples in a more focused way towards the center of the goal region.\nImportantly, with a large enough value of mg, this design preserves the notion of a well-\ndefined goal region (positive reward inside the region and zero reward outside) and thus\nalso preserves our ability to reason about goal-reaching accuracy, a beneficial concept for\nmonitoring the model, filtering out-of-focus samples, etc.\nD.3.3\nTabular Goal-Sampler\nTo cope with the problem of infeasible goal regions described in Section 8.3.2, we explore\nthe idea of sampling the goal directions dg from a learned goal distribution rather than\nsampling all directions uniformly. The idea is that, as the model learns about which goal\ndirections point towards infeasible regions of the objective space, we can attribute a much\nlower sampling likelihood to these regions in order to focus on more fruitful goals.\nWe implement a first version of this idea as a tabular goal-sampler (Tab-GS). We first build\na dataset of goal directions DG. This could be done in many different ways such as sampling\na large number of positive vectors at the surface of the unit hypersphere in objective space.\nIn our case, we discretise the extreme faces of the unit hypercube and normalize them. At\ntraining time, for each direction vector dg ∈DG, we keep a count of the number of samples\nwhich have landed closest to it (closer than any other direction d′\ng) and follow this very simple\nscheme: from the beginning up to 25% of the training iterations, we sample batches of goal\ndirections {dg}N\ni=1 uniformly over DG. Then starting at 25% of the training iterations, while\nwe keep updating each direction’s count, we sample batches of goal directions according to\nthe following (unnormalized) likelihoods:\nf(dg) =\n\n\n\n\n\n\n\n\n\n\n\n1\nif dg has never been sampled\n1\nif there has been a sample r closer to dg than any other goal direction in DG\n0.1\notherwise\n(D.2)\nFinally, at 75% of the training, we stop updating the goal direction counts to allow the\nmodel to fine-tune itself to a now stationary goal-distribution Tab-GS(f). At test time we\nalso sample from that same stationary distribution.\nFigure D.4 shows the effect of our learned tabular goal-sampler (Tab-GS) on the model’s\n186\nperformance and learning dynamics. While the 2-objective problem does not contain a lot of\ninfeasible goal directions, resulting in very similar behaviors for both methods, we can see that\nin the case of 3 and 4 objectives, the model experiences an important immediate improvement\nin goal-reaching accuracy at 25% of training when we start sampling dg’s according to our\nlearned goal-sampler and that this improved focus helps the model further improves on these\nmore fruitful goal directions, resulting in an increase IGD and PC-ent scores.\nFigure D.4 Learning curves for our goal-conditioned model trained by sampling goal directions\ndg either uniformly on the positive quadrant of a K-dimensional hypersphere (Uniform-GS) in\nblue or according to our learned tabular goal-sampler (Tab-GS) in purple on a) 2 objectives,\nb) 3 objectives and c) 4 objectives. Vertical dotted lines indicate 25% and 75% of training\nwhen we start sampling goal directions according to Equation D.2 and when we stop updating\nthe learned goal-sampler, respectively.\n187\nD.4\nAdditional Results\nIn this section, we present additional plots for experiments on 2, 3 and 4 objectives (Fig-\nures D.5, D.6 & D.7).\nFigure D.5 Comparison between a) preference-conditioned and b) goal-conditioned models\ntrained on the 2-objective problem (seh, qed).\nEach panel is an assemblage of K × K\nplots where K is the number of objectives.\nOn the diagonal, each plot focuses on a\nsingle objective.\nThey each show a histogram (dark) of the samples’ scores r·,k for that\nobjective, overlayed with a scatter plot (orange) in which each point is a distinct sample\ni with coordinates (x, y) = (ri,k, ci,k), where ri,k is the reward attributed to sample i for\nobjective k and ci,k is the corresponding value of the conditioning vector that was used\nto generate that sample. The histogram thus showcases the distribution and span of our\nset of samples for a given dimension in objective space while the scatter plot allows us to\nvisualise the correlation between the conditioning vectors and the resulting rewards for that\ndimension. Above the diagonal, each plot shows the density of the learned distribution\non the plane corresponding to a pair of objectives. Brighter colors indicate that a region is\nmore densely populated. Below the diagonal, each plot shows the controllability of the\nlearned distribution where BRG colors represent the angle between the vector [1, 0] and a)\nthe preference-vector w or b) the goal direction dg (this is the same as in Figure 8.2). Overall,\nwe can see that on the density plot and on the histograms that the goal-conditioned approach\nproduces a more uniformly distributed set of samples while the orange scatter plot and the\nBRG-colored plots show that they also provide a finer control over the generated samples.\n188\nFigure D.6 Idem to Figure D.6 but with 3 objectives: seh, qed, sa.\nFigure D.7 Idem to Figure D.5 but with 4 objectives: seh, qed, sa, mw.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-12-10",
  "updated": "2024-12-10"
}