{
  "id": "http://arxiv.org/abs/2412.08392v1",
  "title": "The Roles of English in Evaluating Multilingual Language Models",
  "authors": [
    "Wessel Poelman",
    "Miryam de Lhoneux"
  ],
  "abstract": "Multilingual natural language processing is getting increased attention, with\nnumerous models, benchmarks, and methods being released for many languages.\nEnglish is often used in multilingual evaluation to prompt language models\n(LMs), mainly to overcome the lack of instruction tuning data in other\nlanguages. In this position paper, we lay out two roles of English in\nmultilingual LM evaluations: as an interface and as a natural language. We\nargue that these roles have different goals: task performance versus language\nunderstanding. This discrepancy is highlighted with examples from datasets and\nevaluation setups. Numerous works explicitly use English as an interface to\nboost task performance. We recommend to move away from this imprecise method\nand instead focus on furthering language understanding.",
  "text": "The Roles of English in Evaluating Multilingual Language Models\nWessel Poelman and Miryam de Lhoneux\nDepartment of Computer Science\nKU Leuven, Belgium\n{wessel.poelman, miryam.delhoneux}@kuleuven.be\nAbstract\nMultilingual natural language processing\nis getting increased attention, with numer-\nous models, benchmarks, and methods be-\ning released for many languages. English\nis often used in multilingual evaluation to\nprompt language models (LMs), mainly\nto overcome the lack of instruction tuning\ndata in other languages. In this position\npaper, we lay out two roles of English in\nmultilingual LM evaluations: as an inter-\nface and as a natural language. We argue\nthat these roles have different goals: task\nperformance versus language understand-\ning. This discrepancy is highlighted with\nexamples from datasets and evaluation se-\ntups. Numerous works explicitly use En-\nglish as an interface to boost task perfor-\nmance.\nWe recommend to move away\nfrom this imprecise method and instead fo-\ncus on furthering language understanding.\n1\nIntroduction\nWith the increase of in-context, prompt-based\nevaluation of auto-regressive languages models\n(LMs, Brown et al., 2020), choices have to be\nmade on how prompts are created. Specifically\nin multilingual evaluation, a crucial choice is in\nwhich language(s) prompts are written. In prac-\ntice, English tends to be mixed with a target lan-\nguage with the explicit goal of increasing task per-\nformance. We argue this goal is different from fur-\nthering language understanding. In this position\npaper, we outline two roles of English at the core\nof this discrepancy and their implications.\nSeveral works have highlighted methodological\nissues in multilingual evaluation setups (Artetxe\net al., 2020; Ploeger et al., 2024). The dominance\nof English in natural language processing (NLP)\nhas also been discussed repeatedly (Joshi et al.,\n2020; Ruder et al., 2022). With the increase of\nprompt-based evaluations of models, a new issue\nhas appeared: English being used as an interface,\nrather than a natural language.\nIn recent work, Zhang et al. (2023) propose a\ntaxonomy of prompt-based multilingual LM eval-\nuations. They conclude that “[the model] achieves\nhigher performance when the task is presented in\nEnglish.” This finding is consistent among a large\nnumber of papers (Shi et al., 2022; Huang et al.,\n2022; Fu et al., 2022; Lin et al., 2022; Asai et al.,\n2024; Etxaniz et al., 2024, inter alia).\nResort-\ning to using English like this is hardly surprising\ngiven that instruction tuning datasets are expen-\nsive to create and not readily available for most\nlanguages. Less surprising still is the finding that\nEnglish performs well, as it is included in virtually\nall LMs. It does bring into question: what is being\nevaluated and what do we learn from this?\nTo illustrate: MaLa-500 (Lin et al., 2024) is a\nLlama 2-based model (Touvron et al., 2023) that\nunderwent continued pre-training in over 500 lan-\nguages. It is partially evaluated on a news topic\nclassification task using SIB-200 (Adelani et al.,\n2024a), a dataset of (sentence, topic) pairs in 205\nlanguages. The model is prompted as follows:\nThe topic of the news {sentence} is {topic}\nUsing the prompt with a Turkish1 example gives:\nThe topic of the news Bu oteller g¨un¨un zenginlerinin\nve ¨unl¨ulerinin kalaca˘gı yerlerdi ve c¸o˘gu zaman kaliteli\nyemeklere ve gece hayatına sahipti. is entertainment\nThis format is used across all 205 languages in\nfew-shot setups from one to ten. This mixture of\nEnglish and a target language is, arguably, not very\n‘natural’. We refer to this role of English as an in-\nterface, rather than a natural language. In the next\nsections, we outline these roles and why they are\nimportant to consider in multilingual evaluation.\n1English translations of examples are in Appendix A.\narXiv:2412.08392v1  [cs.CL]  11 Dec 2024\nNatural\nLanguage\nInterface\nRole\nMonolingual or\nCode-switched\nMultilingual\nLM\nMixed-prompt\nLanguage\nUnderstanding\nTask\nPerformance\nFormat\nGoal\nFigure 1 – Schematic overview of the different roles of English in multilingual LM evaluation.\n2\nEvaluation Goals\nLanguage understanding.\nWe take the com-\nmon perspective that evaluation concerns a task\nwhich is used as a proxy for understanding. This is\nexemplified by the natural language understand-\ning (NLU) label many datasets and models adhere\nto (including SIB-200). A news topic classifica-\ntion task shows that the model (arguably) ‘under-\nstands’ some of the differences between news cat-\negories. A model that rewrites, translates or sum-\nmarizes ‘understands’ both task instructions and\ntarget passages. In a multilingual setting, the un-\nderstanding of interest is generalizability across\nlanguages; a model performing a task in a tar-\nget language supposedly understands something\nabout that language. This is then applied to mul-\ntiple languages. We refer to this as ‘multilingual\nnatural language understanding’ (MLU). Specifi-\ncally, we use MLU to mean ‘understanding a tar-\nget language is part of multilingual natural lan-\nguage understanding.’2\nUnderstanding English by itself and under-\nstanding a natural mix of English and another lan-\nguage are both part of MLU. The latter enters\nthe domain of code-switching: the phenomenon\nwhere a speaker fluently switches between multi-\nple different languages during the same conversa-\ntional turn (Milroy and Muysken, 1995).3\nThe MaLa-500 prompt mixes English and a tar-\nget language. However, it is hard to classify this as\ncode-switching, as the switch is hardly natural, es-\n2We are aware this (ab)use of terminology is not standard.\n3Some differentiate between code-switching and code-\nmixing, we do not make a distinction. For an overview of\ncode-switching in NLP, we refer to Winata et al. (2023).\npecially in a few-shot setup. Rather than a natural\nlanguage that tells something about language un-\nderstanding, English is used as an interface to the\nLM with the goal of increasing task performance.\nWe refer to this mixing as a mixed-prompt.\nTask performance.\nAnother widespread per-\nspective on evaluation in (multilingual) NLP con-\nsiders performance on a task as an end in itself.4\nIf we want to classify news topics in a practi-\ncal application operating in a multilingual setting,\nwhat a model supposedly understands or how well\nit models a particular language is of little value.\nWhat matters is the system performing its task ad-\nequately across languages. Without using English,\nthe system might not even work at all. This is\na common justification; mixing in English is ar-\nguably better than not having a system at all.\nWhile practical, this perspective is seemingly at\nodds with the many tasks and datasets that present\nthemselves under the aforementioned label of lan-\nguage understanding. Additionally, task perfor-\nmance as the sole goal introduces a usability issue.\nAuto-regressive LMs are increasingly meant to be\ndirectly interacted with (a natural language inter-\nface). If we have to resort to a mixed-prompt for\nthe system to even function, it means the user has\nto be able to write English and get familiar with\nthis unnatural mixing of languages.\nFigure 1 summarizes our argument and termi-\nnology. Next, we provide more details regarding\nthe discrepancies between using English as an in-\nterface versus using it as a natural language.\n4We thank two reviewers for suggesting to put more em-\nphasis on this perspective.\n3\nEvaluation Methods\nAs mentioned in §1, a large body of contemporary\nresearch in multilingual NLP focuses on prompt-\ning methods.\nCommon evaluation setups range\nfrom (i) prompts fully in a target language, to (ii)\nEnglish instructions with task-specific passages in\nthe target language, to (iii) translating all text into\nEnglish before presenting it to a model.5 None of\nthese works refer to this mixture as being code-\nswitched text. All conclude that a mixture of En-\nglish and a target language (a mixed-prompt) gen-\nerally results in the best task performance. In this\nsection we show why a mixed-prompt is an inher-\nently imprecise method to use in evaluation, even\nif maximizing task performance is the goal.\nIf we use a prompt fully in a target language,\nwe are clearly evaluating part of MLU. A mixed-\nprompt introduces additional factors that are eval-\nuated that are neither the task nor MLU. We illus-\ntrate this from two angles: the representation of\nthe prompt and fortuitous issues from unnaturally\nmixing English and a target language.\nConsider how to evaluate a multilingual masked\nlanguage model on the news classification task. A\nclassification layer is added to a pre-trained model\nto predict the topic labels; it sees label indices\nthat are consistent across languages. The labels\nare language-agnostic for the model (i.e., detached\nfrom natural language). The evaluation method\nand goal are clear: mapping a target language se-\nquence to one of these indices. There are no addi-\ntional signals influencing this process.\nIn a prompting setup, the representation of the\nlabels can either be language-agnostic (numbers,\nletters, symbols, etc.), or not (English words, tar-\nget language words, etc.). These options result in\nany number of tokens, which will have different\nrepresentations within the model, unless specifi-\ncally accounted for. In many multilingual eval-\nuation prompts, the classification labels are En-\nglish words (such as in the MaLa-500 example).\nWithout target language words or (to an extent)\nlanguage-agnostic labels, the evaluation method\nand goal will be inherently imprecise.\nIn addition to the different representation, more\nthan just the task is evaluated with a mixed-prompt\nsetup.\nTo illustrate this, consider the following\nsetup from the AfriMMLU subtask of IrokoBench\n(Adelani et al., 2024b):\n5We do not further discuss ‘translate everything’ as this\nresembles evaluating English as a natural language.\nYou are a highly knowledgeable and intelligent\nartificial intelligence model answers multiple-choice\nquestions about {subject}\nQuestion: {question}\nChoices:\nA: {choice1}\nB: {choice2}\nC: {choice3}\nD: {choice4}\nAnswer:\nThe prompt and subject are always in English,\nthe question and choices in the target lan-\nguage. With this setup, more is tested than just a\ntask in a target language:\n• Code-switching, if this is considered natural,\nor unnatural ‘mixed-prompt’ switching.\n• Script-switching, if the target language uses\na non-Latin script (which applies to Amharic\nin IrokoBench, using the Ge‘ez script).\n• Instruction following in English.\n• Grammatical error correction in English.6\n• Answering high-school level exam questions\nin the target language.\nWith these mixed-prompts, we arguably do not\ntest MLU, as that would entail a native target lan-\nguage prompt. At the same time, we test more than\njust the task, even though that is the explicit goal\nof using English in this way.\nWhile we only discussed classification tasks un-\ntil now, our argument also applies to other types of\ntasks. Consider the following zero-shot machine\ntranslation prompt from Hendy et al. (2023):\nTranslate this sentence from {source} to {target}\nSource: {source sentence}\nTarget:\nThe prompt is always in English, the source and\ntarget are English words referring to the lan-\nguages, and the source sentence is in the tar-\nget language. Filled in, it looks like this:\n# DE →NL\nTranslate this sentence from German to Dutch\nSource: Du gehst mir auf den Keks\nTarget:\n# NL →DE\nTranslate this sentence from Dutch to German\nSource: tijd voor een bakje koffie\nTarget:\n6We have notified the AfriMMLU authors about this. The\ntypo is in the prompt in the paper and in the lm-evaluation-\nharness (Biderman et al., 2024), which is used to obtain their\nresults:\nhttps://github.com/EleutherAI/lm-evaluation-harness/\nblob/7882043b4ee1ef9577b829809c2f4970b0bdba91/lm_eval/tasks/\nafrimmlu/direct/utils.py.\nThe authors mention they “explore prompt selec-\ntion strategies along two dimensions: quality and\nrelevance”, but do not mention target language\nprompts. To underline the interface role of En-\nglish: it is neither the translation source nor tar-\nget here. Hendy et al. (2023) mention that “keep-\ning the prompt format the same allows us to po-\ntentially leverage the benefits of the underlying\ninstruction finetuning protocol to the full extent.”\nThis makes explicit the goal of task performance.\nPrompting a model to translate a sentence is easily\ndone in a manner that more closely aligns with the\ngoal of MLU, does not use English, and is closer\nto natural code-switching:\n# DE →NL (Dutch speaker)\nWat betekent “Du gehst mir auf den Keks” in het\nNederlands?\n# NL →DE (Dutch speaker)\nHoe zeg je “tijd voor een bakje koffie” in het Duits?\n4\nWhy does this matter?\nInteracting with computers in a natural manner\nis arguably the ultimate goal of numerous sub-\nfields of computer science.\nWork on natural\nlanguage interfaces to information systems dates\nback decades (Winograd, 1972; Waltz, 1978).\nLMs bring us ever closer to this goal. However, in\na multilingual setting, it is important to consider\nwhat natural language is, what is being evaluated,\nand what promises are sold. Next, we outline the\nimplications of the interface versus natural lan-\nguage roles on evaluation practices.\nInterface.\nLet us start with the role in which En-\nglish is akin to a programming language.7\nWe\nneed an interface to communicate with a system,\nin a way the system can understand.\nWe have\nseen that mixed-prompts are used to get the sys-\ntem to perform better on a given task. Given the\nscarcity of instruction tuning datasets and the costs\ninvolved in creating these, it is understandable that\nthis is a common (albeit sometimes implicit) per-\nspective. English becomes the ‘programming’ lan-\nguage that glues target language passages together\nand makes the system perform a task. Program-\nming languages also predominantly use English\nlabels for their keywords. However, if the key-\nword for a while loop happens to be mientras\nor kjsdfk is irrelevant for its function. These\n7Also reflected in this famous post: https://x.com/\nkarpathy/status/1617979122625712128\nare natural language-agnostic as the meaning (as\ninterpreted by a compiler or interpreter) does not\nchange. Variable names and keywords can be cho-\nsen arbitrarily.8 This is not the case with prompt-\ning, which is sensitive to slight changes, both in\nEnglish (Sclar et al., 2023) and multilingual setups\n(Zhang et al., 2023; Asai et al., 2024).\nAdditionally, evaluation setups that use English\nas an interface introduce knowledge leakage from\nEnglish to the target language.\nThis is, again,\nwith the explicit goal of improving task perfor-\nmance.9 Being able to understand English instruc-\ntions is not the same as being able to understand\ntarget language instructions. If English truly was\na programming language, this would not matter,\nas the meaning of the instructions would be sepa-\nrate from the meaning of the target language pas-\nsages. Given that English is a natural language,\nthis de facto means more is evaluated than just the\ntask. Consequently, such evaluations are impre-\ncise at best, as shown in §3.\nPrompt-based evaluations should extend MLU\nto the instruction domain. A mixed-prompt setup\nclaiming to test “multilingual understanding”\nmight more accurately be described as “under-\nstanding English instructions interleaved with\npassages from target language(s), albeit not in a\nnatural code-switching setup.”\nNatural language.\nWhen we consider the other\nrole of English in multilingual prompt-based eval-\nuation, we should treat it the same as any other lan-\nguage. The ‘Multilingual Exemplars’ setup from\nShi et al. (2022) is a creative interpretation of this\nperspective. In this few-shot setup, the model sees\nvarious examples, all in different languages. The\nfinal question is asked in the target language. A\nsetup like this extends the definition of ‘multilin-\ngual language understanding’ to the extreme. It\nbecomes harder to interpret what a multilingual\nmodel knows about any individual language in this\ncontext, but English is certainly not an interface, it\nis a natural language like all others.\nA less extreme setup would simply use native,\ntarget language prompts or natural code-switched\nprompts. This is costly, but it aligns much bet-\n8Within the specifications of the programming language.\n9Knowledge leakage also explicitly happens in parameter\nsharing (Zeman and Resnik, 2008) or cross-lingual transfer\n(Philippy et al., 2023). However, these methods are funda-\nmentally different from mixed-prompts as they (i) treat En-\nglish as a natural language, and (ii) target knowledge sharing\nat the training or finetuning phase, not the evaluation phase.\nter with the goal of multilingual natural language\nunderstanding. Indeed, several works specifically\nexplore this direction (K¨opf et al., 2023; Singh\net al., 2024). This approach clearly tests multi-\nlingual language understanding, including the in-\nstruction domain. If performance on a particular\ntask in a particular language is lagging behind, or\nnot working at all, it means focus should be put on\naddressing the core of these issues (e.g., data or\nmodeling). Ideally, we should not resort to impre-\ncise methods to boost task performance.\n5\nConclusion\nIn this position paper we outline two roles of En-\nglish in multilingual language model evaluation:\nas an interface, with the goal of task performance,\nand as a natural language, with the goal of lan-\nguage understanding. We (i) list works that incor-\nporate English with the explicit goal of boosting\ntask performance, even in tasks such as transla-\ntion where it is neither the source nor target, un-\nderlining the interface role, (ii) show that mix-\ning English with a target language in a mixed-\nprompt is unnatural (i.e., not code-switching), and\n(iii) outline why the interface role is an imprecise\nchoice when evaluating multilingual language un-\nderstanding of language models.\nAdditionally, we argue that using a mixed-\nprompt tests more than just performance on a cer-\ntain task. Because English is a natural language\nand not a programming language, using it in a\nmixed prompt will inherently lead to fortuitous\nfactors such as (un)natural switching between lan-\nguages or scripts, grammatical error correction,\nand more. This all results in imprecise or mislead-\ning evaluations, even if the ultimate goal was to\nevaluate and improve task performance.\nWe finally contrast the implications of the two\nroles on evaluation practices. We recommend to\nmove away from using English as an interface in\nmultilingual evaluations and ultimately advocate\nfor the goal of language understanding.\nAcknowledgments\nWP is funded by a KU Leuven Bijzonder Onder-\nzoeksfonds C1 project with reference C14/23/096.\nWe thank the LAGoM-NLP group at KU Leuven\nfor valuable paper recommendations and Mahdi\nDhaini for reviewing an early draft of this paper.\nWe also thank the reviewers for their constructive\ncomments.\nReferences\nDavid Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vass-\nilyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, and\nEn-Shiun Lee. 2024a. SIB-200: A Simple, Inclu-\nsive, and Big Evaluation Dataset for Topic Classifi-\ncation in 200+ Languages and Dialects. In Proceed-\nings of the 18th Conference of the European Chap-\nter of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 226–245.\nDavid Ifeoluwa Adelani, Jessica Ojo, Israel Abebe\nAzime, Jian Yun Zhuang, Jesujoba O. Alabi, Xu-\nanli He, Millicent Ochieng, Sara Hooker, Andiswa\nBukula, En-Shiun Annie Lee, Chiamaka Chuk-\nwuneke, Happy Buzaaba, Blessing Sibanda, God-\nson Kalipe, Jonathan Mukiibi, Salomon Kabongo,\nFoutse Yuehgoh,\nMmasibidi Setaka,\nLolwethu\nNdolela, Nkiruka Odu, Rooweither Mabuya, Sham-\nsuddeen Hassan Muhammad, Salomey Osei, Sokhar\nSamb, Tadesse Kebede Guge, and Pontus Stenetorp.\n2024b. IrokoBench: A New Benchmark for African\nLanguages in the Age of Large Language Models.\narXiv preprint, arXiv:2406.03368v1.\nMikel Artetxe, Sebastian Ruder, Dani Yogatama,\nGorka Labaka, and Eneko Agirre. 2020. A Call for\nMore Rigor in Unsupervised Cross-lingual Learn-\ning.\nIn Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 7375–7388.\nAkari Asai, Sneha Kudugunta, Xinyan Yu, Terra\nBlevins, Hila Gonen, Machel Reid, Yulia Tsvetkov,\nSebastian Ruder, and Hannaneh Hajishirzi. 2024.\nBUFFET: Benchmarking Large Language Models\nfor Few-shot Cross-lingual Transfer.\nIn Proceed-\nings of the 2024 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers), pages 1771–1800.\nStella\nBiderman,\nHailey\nSchoelkopf,\nLintang\nSutawika, Leo Gao, Jonathan Tow, Baber Abbasi,\nAlham Fikri Aji, Pawan Sasanka Ammanamanchi,\nSidney Black, Jordan Clive, Anthony DiPofi, Julen\nEtxaniz, Benjamin Fattori, Jessica Zosa Forde,\nCharles Foster, Jeffrey Hsu, Mimansa Jaiswal,\nWilson Y. Lee, Haonan Li, Charles Lovering, Niklas\nMuennighoff, Ellie Pavlick, Jason Phang, Aviya\nSkowron, Samson Tan, Xiangru Tang, Kevin A.\nWang, Genta Indra Winata, Franc¸ois Yvon, and\nAndy Zou. 2024.\nLessons from the Trenches on\nReproducible Evaluation of Language Models.\narXiv preprint, arXiv.2405.14782v2.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,\nJared D Kaplan,\nPrafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey\nWu, Clemens Winter, Chris Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020.\nLanguage Models are Few-Shot\nLearners. In Advances in Neural Information Pro-\ncessing Systems, volume 33, pages 1877–1901.\nJulen Etxaniz, Gorka Azkune, Aitor Soroa, Oier La-\ncalle, and Mikel Artetxe. 2024.\nDo Multilingual\nLanguage Models Think Better in English? In Pro-\nceedings of the 2024 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Vol-\nume 2: Short Papers), pages 550–564.\nJinlan Fu, See-Kiong Ng, and Pengfei Liu. 2022. Poly-\nglot Prompt: Multilingual Multitask Prompt Train-\ning. In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing,\npages 9919–9935.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas\nRaunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Has-\nsan Awadalla. 2023. How Good Are GPT Models\nat Machine Translation? A Comprehensive Evalua-\ntion. arXiv preprint, arXiv:2302.09210v1.\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu\nWei, and Houfeng Wang. 2022. Zero-shot Cross-\nlingual Transfer of Prompt-based Tuning with a Uni-\nfied Multilingual Prompt.\nIn Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 11488–11497.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The State and\nFate of Linguistic Diversity and Inclusion in the\nNLP World.\nIn Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 6282–6293.\nAndreas K¨opf, Yannic Kilcher, Dimitri von R¨utte,\nSotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,\nAbdullah Barhoum, Duc Minh Nguyen, Oliver\nStanley, Rich´ard Nagyfi, Shahul Es, Sameer Suri,\nDavid Alexandrovich\nGlushkov,\nArnav\nVarma\nDantuluri,\nAndrew Maguire,\nChristoph Schuh-\nmann, Huu Nguyen, and Alexander Julian Mattick.\n2023. OpenAssistant Conversations - Democratiz-\ning Large Language Model Alignment. In Thirty-\nSeventh Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track.\nPeiqin Lin, Shaoxiong Ji, J¨org Tiedemann, Andr´e F. T.\nMartins, and Hinrich Sch¨utze. 2024.\nMaLA-500:\nMassive Language Adaptation of Large Language\nModels. arXiv preprint, arXiv:2401.13303v2.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin\nStoyanov, and Xian Li. 2022.\nFew-shot Learn-\ning with Multilingual Generative Language Models.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n9019–9052.\nLesley Milroy and Pieter Muysken, editors. 1995. One\nSpeaker, Two Languages: Cross-Disciplinary Per-\nspectives on Code-Switching. Cambridge University\nPress.\nFred Philippy, Siwen Guo, and Shohreh Haddadan.\n2023. Towards a Common Understanding of Con-\ntributing Factors for Cross-Lingual Transfer in Mul-\ntilingual Language Models: A Review. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 5877–5891.\nEsther Ploeger, Wessel Poelman, Miryam de Lhoneux,\nand Johannes Bjerva. 2024. What is “Typological\nDiversity” in NLP? In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 5681–5700.\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard.\n2022. Square One Bias in NLP: Towards a Multi-\nDimensional Exploration of the Research Manifold.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 2340–2354.\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane\nSuhr. 2023.\nQuantifying Language Models’ Sen-\nsitivity to Spurious Features in Prompt Design or:\nHow I learned to start worrying about prompt for-\nmatting.\nIn The Twelfth International Conference\non Learning Representations.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi\nWang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Di-\npanjan Das, and Jason Wei. 2022. Language mod-\nels are multilingual chain-of-thought reasoners. In\nThe Eleventh International Conference on Learning\nRepresentations.\nShivalika Singh, Freddie Vargus, Daniel D’souza,\nB¨orje Karlsson, Abinaya Mahendiran, Wei-Yin Ko,\nHerumb Shandilya, Jay Patel, Deividas Mataciu-\nnas, Laura O’Mahony, Mike Zhang, Ramith Het-\ntiarachchi, Joseph Wilson, Marina Machado, Luisa\nMoura, Dominik Krzemi´nski, Hakimeh Fadaei,\nIrem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan\nMudannayake, Zaid Alyafeai, Vu Chien, Sebastian\nRuder, Surya Guthikonda, Emad Alghamdi, Sebas-\ntian Gehrmann, Niklas Muennighoff, Max Bartolo,\nJulia Kreutzer, Ahmet ¨Ust¨un, Marzieh Fadaee, and\nSara Hooker. 2024. Aya Dataset: An Open-Access\nCollection for Multilingual Instruction Tuning. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 11521–11567.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan,\nBinh Tang, Ross Taylor, Adina Williams, Jian Xi-\nang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur,\nSharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. 2023. Llama\n2: Open Foundation and Fine-Tuned Chat Models.\narXiv preprint, arXiv:2307.09288v2.\nDavid L. Waltz. 1978. An English language question\nanswering system for a large relational database.\nCommunications of the ACM, 21(7):526–539.\nGenta Winata, Alham Fikri Aji, Zheng Xin Yong, and\nThamar Solorio. 2023.\nThe Decades Progress on\nCode-Switching Research in NLP: A Systematic\nSurvey on Trends and Challenges. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 2936–2978.\nTerry Winograd. 1972.\nUnderstanding natural lan-\nguage. Cognitive Psychology, 3(1):1–191.\nDaniel Zeman and Philip Resnik. 2008.\nCross-\nLanguage Parser Adaptation between Related Lan-\nguages. In Proceedings of the IJCNLP-08 Workshop\non NLP for Less Privileged Languages.\nXiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and\nGrzegorz Kondrak. 2023.\nDon’t Trust ChatGPT\nwhen your Question is not in English: A Study of\nMultilingual Abilities and Types of LLMs. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, pages 7915–\n7927.\nA\nExamples\nThe examples containing Turkish, Dutch or Ger-\nman are repeated here with English translations.\nSIB-200 (sample 755):\nThe topic of the news Bu oteller g¨un¨un zenginlerinin\nve ¨unl¨ulerinin kalaca˘gı yerlerdi ve c¸o˘gu zaman kaliteli\nyemeklere ve gece hayatına sahipti. is entertainment\nThe topic of the news These hotels were where the rich\nand the famous of the day would stay, and often had fine\ndining and nightlife. is entertainment\nInterface translation examples:\n# DE →NL\nTranslate this sentence from German to Dutch\nSource: Du gehst mir auf den Keks\nTarget:\n# DE →NL\nTranslate this sentence from German to Dutch\nSource: You’re getting on my nerves\nTarget:\n# NL →DE\nTranslate this sentence from Dutch to German\nSource: tijd voor een bakje koffie\nTarget:\n# NL →DE\nTranslate this sentence from Dutch to German\nSource: time for a cup of coffee\nTarget:\nNatural translation examples:\n# DE →NL (Dutch speaker)\nWat betekent “Du gehst mir auf den Keks” in het\nNederlands?\n# DE →NL (Dutch speaker)\nWhat does “Du gehst mir auf den Keks” mean in Dutch?\n# NL →DE (Dutch speaker)\nHoe zeg je “tijd voor een bakje koffie” in het Duits?\n# NL →DE (Dutch speaker)\nHow would one say “tijd voor een bakje koffie” in Ger-\nman?\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-12-11",
  "updated": "2024-12-11"
}