{
  "id": "http://arxiv.org/abs/2412.08933v3",
  "title": "Deep clustering using adversarial net based clustering loss",
  "authors": [
    "Kart-Leong Lim"
  ],
  "abstract": "Deep clustering is a recent deep learning technique which combines deep\nlearning with traditional unsupervised clustering. At the heart of deep\nclustering is a loss function which penalizes samples for being an outlier from\ntheir ground truth cluster centers in the latent space. The probabilistic\nvariant of deep clustering reformulates the loss using KL divergence. Often,\nthe main constraint of deep clustering is the necessity of a closed form loss\nfunction to make backpropagation tractable. Inspired by deep clustering and\nadversarial net, we reformulate deep clustering as an adversarial net over\ntraditional closed form KL divergence. Training deep clustering becomes a task\nof minimizing the encoder and maximizing the discriminator. At optimality, this\nmethod theoretically approaches the JS divergence between the distribution\nassumption of the encoder and the discriminator. We demonstrated the\nperformance of our proposed method on several well cited datasets such as\nMNIST, REUTERS10K and CIFAR10, achieving on-par or better performance with some\nof the state-of-the-art deep clustering methods.",
  "text": "DEEP CLUSTERING USING ADVERSARIAL\nNET BASED CLUSTERING LOSS\nKart-Leong Lim *\nInstitute of Microelectronics\n2 Fusionopolis Way\nSingapore, 138634\n{limkl}@ime.a-star.edu.sg\nFebruary 6, 2025\nAbstract\nDeep clustering is a recent deep learning technique which combines deep learn-\ning with traditional unsupervised clustering. At the heart of deep clustering is a loss\nfunction which penalizes samples for being an outlier from their ground truth clus-\nter centers in the latent space. The probabilistic variant of deep clustering reformu-\nlates the loss using KL divergence. Often, the main constraint of deep clustering is\nthe necessity of a closed form loss function to make backpropagation tractable. In-\nspired by deep clustering and adversarial net, we reformulate deep clustering as an\nadversarial net over traditional closed form KL divergence. Training deep cluster-\ning becomes a task of minimizing the encoder and maximizing the discriminator.\nAt optimality, this method theoretically approaches the JS divergence between\nthe distribution assumption of the encoder and the discriminator. We demonstrated\nthe performance of our proposed method on several well cited datasets such as\nMNIST, REUTERS and CIFAR10, achieving on-par or better performance with\nsome of the state-of-the-art deep clustering methods.\n1\nIntroduction\nDeep neural network such as the autoencoder is applied to many signal processing do-\nmains such as speech and audio processing [1, 2], image clustering [3, 4] and medical\ndata processing [5, 6]. While the latent space of autoencoder is well suited for dimen-\nsion reduction through reconstruction loss [7], the latter is not optimized for clustering/\nclassification since class labels cannot be used in the reconstruction loss [8]. A recent\nautoencoder technique known as the Euclidean distance based clustering (ABC) [7],\nutilize class label information in neural network by introducing a loss function known\n*Use footnote for providing further information about author (webpage, alternative address)—not for\nacknowledging funding agencies. Funding acknowledgements go at the end of the paper.\n1\narXiv:2412.08933v3  [cs.CV]  5 Feb 2025\nas the deep clustering loss. The goal is to minimizes the Euclidean distance in the latent\nspace between samples and partitioning learnt by clustering algorithms e.g. Kmeans\nor Gaussian mixture model (GMM). In other words, the latent space of an autoencoder\nlearnt using reconstruction loss will look different when we further perform K-means\non the latent space. A recent probabilistic approach to ABC uses KL divergence (KLD)\n[4, 9] and assumes that both the variational autoencoder (VAE) [10] latent space and\nclustering approach are Gaussian distributed. We can compare the similarities and dif-\nferences between deep clustering and VAE to better understand the former. The neural\nnetwork of VAE and deep clustering are optimized by backpropagating samples in the\nlatent space to update the encoder weights. Also, both deep clustering and VAE share\na similar goal of modeling each input image as a sample draw of the Gaussian distri-\nbution in the latent space. However, their Gaussian distributions are different. VAE\nenforce samples in the latent space to be closely distributed by a Gaussian distribution\nwith continuous mean µ and variance σ i.e. z ∼µ + σ · N(0, 1). Whereas the la-\ntent space of deep clustering approaches a Gaussian mixture model (GMM) and each\nGaussian has a set of fixed parameters i.e. z∼N(ηk, τk), ∀k ∈K. The goal of VAE\nis to enforce all class samples in the latent space to be distributed by continuous mean\nand variance. The goal of deep clustering is to enforce class samples in the latent space\nto be distributed by a discrete set of mean and variance i.e. N(η∗, τ ∗). Thus, only\ndeep clustering is associated with clustering, making it more suitable for unsupervised\nclassification. Deep clustering is not without problem. Mainly, when backpropagat-\ning from a neural network, we require a closed form loss function for tractability [11].\nMost deep learning methods including VAE restrict to the assumption of a KLD be-\ntween two Gaussians to obtain a closed form loss function for backpropagating. When\ngeneralizing to f-divergence between two Gaussians, closed form solution does not\nexist. A well known workaround to this problem is to approximate the JS divergence\n(JSD) between two Gaussians using adversarial net. In fact, adversarial net is quite ver-\nsatile as shown in Table 1. For different applications, the discriminator can be seen as\nperforming a specific task different from the discriminators in other adversarial net. We\ndistinguish the proposed work from other “adversarial net based X” in Table I where\nX can be any discriminator task. In GAN, X refers to image generation. Other instances\nof “adversarial net based X” includes AAE, DASC and etc. In our approach, “Deep\nclustering using adversarial net based clustering loss” (DCAN), X refers to clustering.\nMore specifically, in DCAN the JSD between latent space and clustering is approxi-\nmated by adversarial net. More details are discussed in the next section.\n1.1\nAdversarial net based X\nThe optimization of adversarial net [11] in general centers around minimizing the en-\ncoder/generator and maximizing the discriminator. However, as mentioned “adversarial\nnet based X” are versatile and we highlight their similarities and differences as follows:\nIn GAN, the generator is responsible for generating an image when given a sample\nfrom the latent space. The discriminator of GAN checks whether the generated image\nbelongs to the distribution of the original dataset. Similarly, in DCAN we minimize\nthe encoder and maximize the discriminator but for different purpose. In DCAN, the\nencoder receives an image and outputs a sample in the latent space. GAN trains the\n2\nAdversarial net based X\nDiscriminator\nGAN [11]\n\u001a if x ∼p(data), T = 1\nelse, T = 0\nx is from the dataset?\nDASC [12]\n\u001a if z ∼p(ς∗), T = 1\nelse, T = 0\nz is from an assigned subspace?\nAAE [13]\n\u001a if z ∼µ + σ · N(0, 1), T = 1\nelse, T = 0\nz is from a Gaussian prior?\nDCAN (proposed)\n\u001a if z ∼p(z | θ∗), T = 1\nelse, T = 0\nz is from an assigned cluster?\nTable 1: Different strategies of using adversarial net based X\ngenerator to be sensitive to the variance of the image class i.e. small displacement of\nthe sample in the latent space can result in huge image variance. DCAN trains the\nencoder to be insensitive to the class variance i.e. different images from the same class\npopulate tightly close to the cluster center in the latent space. Both DCAN and AAE\n[13] utilize the adversarial net in totally different ways despite having similar archi-\ntecture (encoder acting as generator). The goal of AAE is identical to VAE i.e. the\nAAE encoder models a sample in the latent space as a sum and product of N(0, 1). On\nthe other hand, the goal of DCAN is not identical to VAE. DCAN models a sample in\nthe latent space, as the sample draw from a cluster (e.g. Kmeans or Gaussian mixture\nmodel), specifically the sample draw of a 1−of −K cluster. The goal of DASC [12] is\nsubspace clustering, and it is different from deep clustering. Like principal component\nanalysis, DASC perform unsupervised learning by decomposing the dataset into differ-\nent individual eigenvectors, each eigenvector as orthogonal to each other as possible.\nSamples from each classes should ideally reside on their respective Kth subspaces, just\nas each sample should ideally fall within their respective Kth clusters in deep cluster-\ning. In DASC, the latent space of DASC is trained to behave like subspace clustering.\nThus the adversarial net of all four methods, GAN (models the dataset), AAE (models\nVAE loss), DASC (models subspace clustering) and DCAN (models deep clustering)\nare optimized in totally different ways as summarized in Table I.\n2\nProposed method\nOne motivation for using a JSD approach is that it does not suffer from asymme-\ntry unlike KLD [14, 11]. However, there is no closed form solution available for\nthe JSD between two data distributions. GAN [11] overcome the need for a closed\nform solution by employing an adversarial training procedure. GAN approximates\nJSD or DJS (pdata ∥pg) = Ex∼pdata [log DG(x)] + Ex∼pg [log (1 −DG {x})] at op-\ntimality, where pdata is the real data distribution and pg is the generated data dis-\ntribution.\nIn the JSD between two Gaussian distributions for deep clustering, we\nseek DJS ( p(z | θ∗) ∥q(z | x) ) = Ez∼p(z|θ∗) [ln D(z)]+Ez∼q(z|x) [ln (1 −D {z})],\nwhereby z ∼p(z | θ∗) refers to the sample distributed from clustering and z ∼q(z | x)\nrefers to the sample distributed from the encoder. Unlike the discriminator of GAN in\n3\nFigure 1: Proposed deep clustering loss using adversarial net. It uses the discriminator\nand encoder architecture inspired by AAE.\nthe original space, the discriminator of DCAN works in the latent space. We can vi-\nsualize DJS ( p(z | θ∗) ∥q(z | x) ) using Fig. 1. The discriminator tries to distinguish\nsamples from clustering, z ∼p(z | θ∗) versus samples generated from the encoder,\nz ∼q(z | x).\n2.1\nTraining DCAN\nWe train DCAN by feeding it with 1...N positive and negative samples. We refer to\neach nth sample of x as x(n) with dimension I. In the forward pass in Fig 1, samples\nenter the discriminator through the encoder and clustering. Samples generated from\nclustering1 are positive i.e. T = 1 and samples generated from encoder are negative\ni.e. T = 0. We define the neural network output and target label as y = {0, 1} and\nT = {0, 1} respectively. Both y and T have the same dimension d. We update the\ndiscriminator weight using DCAN loss in eqn (1). Since the samples in the latent\nspace displaces each time the encoder weight changes, clustering parameters θk =\n{ηk, τk, π(n)∀k} has to be re-computed each time the discriminator weight is updated.\nThe encoder q(z | x) maps x to z in the latent space. However, we desire to minimize\nbetween q(z | x) and p(z | θ∗). Meaning that we desire to produce a sample z from\nq(z | x) that is as close as possible to the sampling from p(z | θ∗). Similarly, DCAN\nloss in eqn (1) is used to update the encoder weight.\nLDCAN =\n\u001a\n1\nN\nPN\nn=1\nE\nx(n)∼pdata\n\u0002\nln\n\u00001 −D\n\u0000G(x(n))\n\u0001\u0001\u0003\u001b\nT =0\n+\n\u001a\n1\nN\nPN\nn=1\nE\nz(n)∼p(z|θ∗)\n\u0002\nln D\n\u0000z(n)\u0001\u0003\u001b\nT =1\n(1)\n3\nExperiments\nWe selected some benchmarked datasets used by deep clustering for our experiments\nin Tables 2. There are several factors affecting deep clustering. i) The clustering algo-\nrithm used in the latent space e.g. Kmeans or GMM. ii) The number of hidden layers\nand nodes of the neural network. iii) The gradient ascent method used for training\nthe loss functions. iv) The activation functions used. For DCAN, we use tanh for\n4\nthe hidden layers and sigmoid for the output. We set the clustering iteration to one\nin Table 2 and a sufficient statistics of at least 600 points in the raw latent space to\nestimate the cluster parameters. The number of cluster is the same as the number of\nclasses. We use a minibatch size of N = 16 samples, 500 iterations and we use gra-\ndient ascent with momentum with a learning rate between 10−3 and 10−5. We use\naccuracy (ACC) [15] to evaluate the performance of our clustering. MNIST: A well\ncited 10 digit classes dataset with no background but it has more samples than USPS.\nThe train and test sample size is at 50K and 10K. Our settings are 196 −384 −256\nfor the encoder and 256 −16 −1 for the discriminator. The input dimension is orig-\ninally at 28 × 28 but we have downsampled it to 14 × 14 and flattened to 196. In\nTable 2, DCAN using raw pixel information obtained 0.8565 for ACC which is on\npar or better than deep clustering methods including DEC, ABC and DC-GMM. How-\never on MNIST, most deep clustering methods could not obtain the performance of\nrecent deep learning methods such as KINGDRA-LADDER-1M (a.k.a. KINGDRA)\nand IMSAT as their goals are much more ACC result oriented. For KINGDRA, it com-\nbines several ideas together including ladder network [16] and psuedo-label learning\n[17] for the neural network and using ensemble clustering. In DAC, their approach is\nbased on pairwise classification. In IMSAT-RPT, the neural network was trained by\naugmenting the training dataset. In our case, we simplified our focused on deep clus-\ntering using adversarial net. DCAN penalizes samples for being an outlier from their\ncluster centers and does so uniquely by using the discriminator to penalize the encoder.\nThe original objective of deep clustering is specifically aimed at clustering. In some\ndatasets, the latent space of this objective do not necessarily co-serve as the most im-\nportant factor for raw pixel feature extraction. Thus, we also used ResNet18 pretrained\nmodel as a feature extractor for MNIST, prior to deep clustering. This simple step\nallows the ACC for DCAN to improved to 0.995 (not shown in Table 2) and in fact\noutperforms KINGDRA. Reuters-10k: The Reuters-10k dataset according to [18, 19]\ncontains 10K samples with 4 classes and the feature dimension is 2000. End-to-end\nlearning is performed on this dataset. We compared DCAN to other unsupervised deep\nlearning approaches. DCAN uses 2000−100−500 and 500−100−1 respectively for\nthe encoder and discriminator. In Table 2, we observed that Kmeans alone can achieve\nan ACC of 0.6018 on the raw feature dimension. Most deep learning methods can\nachieve between 0.69 to 0.73 on this dataset. DCAN was able to obtain the best ACC\nat 0.7867. CIFAR10: A 10 classes object categorization dataset with 50K training and\n10K testing of real images. On CIFAR10, most methods have difficulty performing\nend-to-end learning. As compared to deep ConvNet which utilizes convolutional neu-\nral network and supervised learning for a specific task (i.e. feature extraction), deep\nclustering typically rely on fewer hidden layer in the encoder for end-to-end learn-\ning (i.e. both feature extraction and clustering). Thus, we followed the experiment\nsetup in KINGDRA [20] by using the “avg pool” layer of ResNet50 [21] (ImageNet\npretrained) as a feature extractor with a dimension of 2048. In Table 2, DCAN uses\n2048−1024−512−128 and 128−16−1 respectively for the encoder and discrimina-\ntor. Despite the stronger performance of DAC, IMSAT and KINGDRA on MNIST, we\noutperformed these comparisons on CIFAR10 at 0.5844. This is despite the fact that\nboth KINGDRA and IMSAT also use ResNet50 (ImageNet pretrained) for CIFAR10.\n5\nApproach\nMNIST\nReuters10k\nCIFAR10\nABC [7]\n0.760\n0.7019\n0.435\nDEC [19]\n0.843\n0.7217\n0.469\nDC-GMM [18]\n0.8555\n0.6906\n-\nAAE [13]\n0.8348\n0.6982\n-\nIMSAT-RPT [22]\n0.896\n0.719\n0.455\nKINGDRA [20]\n0.985\n0.705\n0.546\nDCAN (proposed)\n0.8565\n0.7867\n0.5844\nTable 2: ACC Benchmark\n4\nConclusion\nThe objective of deep clustering is to optimize the autoencoder latent space with clus-\ntering information. The key to this technique lies in a loss function that minimizes both\nclustering and encoder in the latent space. Despite the recent success of deep clustering,\nthere are some concerns: i) How to use probabilistic approach for the loss function?\nii) Can we backpropagate the neural network when there is no closed form solution for\nthe probabilistic function? A JS divergence approach does not suffer from asymmetry\nlike KL divergence loss. Unlike the latter, there is no closed form solution to the JS\ndivergence when backpropagating the network. This paper addresses the above two\nconcerns by proposing a novel deep clustering approach based on adversarial net to\nestimate JS divergence for deep clustering.\nReferences\n[1] E. Karamatli, A. T. Cemgil, S. Kirbiz, Audio source separation using variational\nautoencoders and weak class supervision, IEEE Signal Processing Letters (2019)\n1–1doi:10.1109/LSP.2019.2929440. 1\n[2] P. Last, H. A. Engelbrecht, H. Kamper, Unsupervised feature learning for speech\nusing correspondence and siamese networks, IEEE Signal Processing Letters 27\n(2020) 421–425. doi:10.1109/LSP.2020.2973798. 1\n[3] W. Huang, M. Yin, J. Li, S. Xie, Deep clustering via weighted k-subspace net-\nwork, IEEE Signal Processing Letters 26 (11) (2019) 1628–1632. 1\n[4] K.-L. Lim, X. Jiang, C. Yi, Deep clustering with variational autoencoder, IEEE\nSignal Processing Letters 27 (2020) 231–235. 1, 5.2\n[5] A. M. Abdelhameed, M. Bayoumi, Semi-supervised eeg signals classification\nsystem for epileptic seizure detection, IEEE Signal Processing Letters 26 (12)\n(2019) 1922–1926. 1\n[6] M. Han, O. Ozdenizci, Y. Wang, T. Koike-Akino, D. Erdogmus, Disentangled ad-\nversarial autoencoder for subject-invariant physiological feature extraction, IEEE\n6\nSignal Processing Letters 27 (2020) 1565–1569. doi:10.1109/LSP.2020.\n3020215. 1\n[7] C. Song, F. Liu, Y. Huang, L. Wang, T. Tan, Auto-encoder based data clustering,\nin: Iberoamerican Congress on Pattern Recognition, Springer, 2013, pp. 117–124.\n1, 3, 5.1, 5.2\n[8] E. Min, X. Guo, Q. Liu, G. Zhang, J. Cui, J. Long, A survey of clustering with\ndeep learning: From the perspective of network architecture, IEEE Access 6\n(2018) 39501–39514. 1\n[9] Z. Jiang, Y. Zheng, H. Tan, B. Tang, H. Zhou, Variational deep embedding: an\nunsupervised and generative approach to clustering, in: Proceedings of the 26th\nInternational Joint Conference on Artificial Intelligence, AAAI Press, 2017, pp.\n1965–1972. 1, 5.2\n[10] D. P. Kingma, M. Welling, Stochastic gradient vb and the variational auto-\nencoder, in:\nSecond International Conference on Learning Representations,\nICLR, 2014. 1\n[11] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, Y. Bengio, Generative adversarial nets, in: Advances in neural in-\nformation processing systems, 2014, pp. 2672–2680. 1, 1.1, 2, 5.3\n[12] P. Zhou, Y. Hou, J. Feng, Deep adversarial subspace clustering, in: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.\n1596–1604. 1, 1.1\n[13] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, Adversarial autoencoders, in:\nInternational Conference on Learning Representations, 2016. 1, 1.1, 3\n[14] F. Nielsen, On the jensen–shannon symmetrization of distances relying on ab-\nstract means, Entropy 21 (5) (2019) 485. 2\n[15] D. Cai, X. He, J. Han, Document clustering using locality preserving indexing,\nIEEE Transactions on Knowledge and Data Engineering 17 (12) (2005) 1624–\n1637. 3\n[16] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, T. Raiko, Semi-supervised\nlearning with ladder networks, in: Advances in neural information processing\nsystems, 2015, pp. 3546–3554. 3\n[17] D.-H. Lee, Pseudo-label: The simple and efficient semi-supervised learning\nmethod for deep neural networks, in: Workshop on challenges in representation\nlearning, ICML, Vol. 3, 2013. 3\n[18] K. Tian, S. Zhou, J. Guan, Deepcluster: A general clustering framework based on\ndeep learning, in: Joint European Conference on Machine Learning and Knowl-\nedge Discovery in Databases, Springer, 2017, pp. 809–825. 3\n7\n[19] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clustering anal-\nysis, in: International conference on machine learning, 2016, pp. 478–487. 3\n[20] D. Gupta, R. Ramjee, N. Kwatra, M. Sivathanu, Unsupervised clustering using\npseudo-semi-supervised learning, in: International Conference on Learning Rep-\nresentations, 2020. 3\n[21] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,\nin: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition, 2016, pp. 770–778. 3\n[22] W. Hu, T. Miyato, S. Tokui, E. Matsumoto, M. Sugiyama, Learning discrete\nrepresentations via information maximizing self-augmented training, in: Pro-\nceedings of the 34th International Conference on Machine Learning-Volume 70,\nJMLR. org, 2017, pp. 1558–1567. 3\n[23] B. Yang, X. Fu, N. D. Sidiropoulos, M. Hong, Towards k-means-friendly spaces:\nSimultaneous deep learning and clustering, in: Proceedings of the 34th Interna-\ntional Conference on Machine Learning-Volume 70, JMLR. org, 2017, pp. 3861–\n3870. 5.2\n5\nAppendix\n5.1\nDeep clustering\nThe simplest form of deep clustering uses the Euclidean distance loss in the Autoen-\ncoder based clustering (ABC) [7] approach. In ABC, the deep clustering objective\noptimizes the neural network to be affected by Kmeans clustering or L2 loss function\nin eqn (1). On the contrary, traditional reconstruction loss or L1 does not utilize any\ninformation from Kmeans.\nmin\nw,b {L1 + λ · L2}\n= max\nw,b\n−1\n2 (T −y)2 −λ\nn\n−1\n2 (hz −η∗)2o\n(2)\n5.2\nProbabilistic variant of deep clustering\nTraining a deep clustering algorithm such as ABC in eqn (2) requires a closed-form\nequation for the clustering loss function. In higher dimensional latent space, the Eu-\nclidean function will become less robust due to the curse of dimensionality. The prob-\nabilistic approach of ABC is based on the “KLD between two Gaussians” in [4, 9].\nWhen the probabilistic approach is no longer a “KLD between two Gaussians”, the\nmain problem is that:\na) the loss function can become non-trival to differentiate for backproga-\ntion.\n8\nInstead, we propose a new way to train deep clustering without facing such constrains\ni.e. we discard the use of probabilistic approach in deep clustering. Instead, we ap-\nproximate deep clustering using adversarial net. There exist a relationship between\ndeep clustering and adversarial net in two simple steps:\na) From ABC to KLD to JSD: We show that under certain condition (i.e.\nunit variance assumption), ABC is identical to KLD and thus related to\nJSD. Both KLD and JSD are also under the family of the f-divergence.\nb) Adversarial net as JSD: We approximate JSD using adversarial net.\nThis is possible since adversarial net approaches JSD when the training\nbecomes optimal.\nOur main goal is to find a relationship between adversarial net and deep clustering.\nFirst, we establish the relationship between KLD and ABC as seen in [4]. KLD mea-\nsures the probabilistic distance between two distributions. The distributions of the\nlatent space and deep clustering space are defined as q(z | x) and p(z | θ) respectively.\nDeep clustering space is the latent space partitioned using GMM or Kmeans. Whereby\nin GMM, z =\n\b\nz(n)\tN\nn=1 ∈RZ and θ = {η, τ, ζ}, which are the mean η, precision\nτ and assignment parameter ζ for K number of clusters and N number of samples.\nSpecifically, η = {ηk}K\nk=1 ∈RZ, τ = {τk}K\nk=1 ∈RZ, ζ(n)\nk\n∈{0, 1} and ζ(n) is a\n1 −of −K vector. Thus, the KLD based clustering loss in [4] as follows:\nDKL ( p(z | θ) ∥q(z | x) ) ,\np(z | θ) = QK\nk=1 N(z | ηk, (τk)−1)ζk,\nq(z | x) = µ + σ · N(0, 1)\n(3)\nThe problem is how do we define KLD in terms of two Gaussians, instead of a\nGaussian and a GMM in eqn (3), which is intractable. To overcome this, we re-express\nthe GMM term in p(z | θ) as follows\np(z | θ∗) = arg max\nk\np(z | θk)\n= N(z | η∗, (τ ∗) −1)\n(4)\nWe define p(z | θ∗) as the kth Gaussian component of Kmeans or GMM that\ngenerates sample z in the latent space. i.e. we use ζ ( cluster assignment) to compute\nη∗(mean) and τ ∗(precision). Thus, the KLD of GMM and latent space in eqn (5)\ncan now become a KLD between two Gaussian distributions. Also, a closed formed\nequation is available for the latter.\nDKL\n\u0000N(z | η∗, (τ ∗) −1) ∥N(z | µ, ϱ2)\n\u0001\n(5)\nThe relationship between KLD and ABC can be explained in Lemma 1: If we\ndiscard the second order terms i.e. ϱ2 and τ in eqn (7), the KLD reverts back to the\noriginal ABC loss [7, 23] in eqn (2).\nDKL (N(z | η∗) ∥N(z | µ)) = 1\n2 (η∗−µ)2\n(6)\n9\nTo relate KLD to JSD, we simply recall JSD as the averaging between two KLDs\nin eqn (8). We next discuss how to relate adversarial net to JSD.\nDJS ( p(z | θ∗) ∥q(z | x) )\n= 1\n2DKL\n\u0000p ∥p+q\n2\n\u0001\n+ 1\n2DKL\n\u0000q ∥p+q\n2\n\u0001\n(7)\nLemma 1 Relating deep clustering in eqn (2) to probabilistic deep clustering in eqn\n(5): We show that under assumption of “unit variance”, the KLD between the encoder\nlatent space and GMM reverts back to the ABC loss.\nProof:\nFor a DKL between two Gaussian distributions, there is a unique closed-form\nexpression available. When we assume unit variance i.e. τ = σ = 1, the DKL reverts\nback to the original Euclidean distance based clustering loss in eqn (1) as follows\nDKL (p(z | θ) ∥q(z | x)) s.t. {τ = σ = 1}\n= DKL\n\u0000N(zn | η∗, (τ ∗) −1) ∥N(zn | µ, σ)\n\u0001\n= ln τ ∗+ ln σ + (τ ∗)−1+(η∗−µ)2\n2σ2\n−1\n2\n= 1\n2 (η∗−µ)2\n(8)\n□\n5.3\nDeep clustering using Adversarial net approaches JSD\nThe problem with using JSD for deep clustering is that there is rarely a closed form\nsolution available for a “JSD between two Gaussian distributions”. GAN overcome this\nby employing an adversarial training procedure that approximates DJS (pdata ∥pg)\nat optimality, where pdata is the real data distribution and pg is the generated data\ndistribution as follows [11]:\nDJS (pdata ∥pg)\n=\nE\nx∼pdata [log DG(x)] +\nE\nz∼p(z) [log (1 −DG {G(z)})]\n=\nE\nx∼pdata [log DG(x)] + E\nx∼pg [log (1 −DG {x})]\n(9)\nUnlike the discriminator of GAN in the original space, the discriminator of DCAN\nworks in the latent space. Despite that, we can easily reformulate GAN into DCAN as\nfollows:\nDJS (p(z | θ∗) ∥q(z | x))\n=\nE\nz∼p(z|θ∗) [ln D(z)] +\nE\nx∼pdata [ln (1 −D {G(x)})]\n=\nE\nz∼p(z|θ∗) [ln D(z)] +\nE\nz∼q(z|x) [ln (1 −D {z})]\n(10)\nWhereby Ez∼p(z|θ∗) [·] refers to the expectation function where the sample is dis-\ntributed from deep clustering space and Ez∼q(z|x) [·] refers to the expectation function\nwhere the sample is distributed from the latent space. We refer to Lemma 2 for the\nclaim on eqn (10).\nLemma 2 The adversarial net based deep clustering loss by DCAN can be shown to\napproach JSD at optimum:\n10\nProof:\nFor the sake of brevity, we refer to p(z | θ∗) and q(z | x) as p and q respec-\ntively. Optimal discriminator occurs when G is fixed, i.e. D =\np\np+q. Substituting D,\nwe define the LHS and RHS below. Lastly, if we subject p = q = 1 on both sides, we\ncan validate the claim on eqn (10).\nLHS : Ez∼p [ln D(z)] + Ex∼pdata [ln (1 −D {G(x)})] s.t. {D =\np\np+q}\n= Ez∼p [log D(z)] + Ez∼q [log (1 −D(z))] s.t. {D =\np\np+q}\n= Ez∼p\nh\nln\np\np+q\ni\n+ Ez∼q\nh\nln\nq\np+q\ni\n=\nR\nz p ln\np\np+q + q ln\nq\np+q dz\nRHS : DJS (p ∥q) = 1\n2\nR\np ln 2·p\np+q + q ln 2·p\np+q dz\n= 1\n2\nR\nz p\nn\nln\np\np+q + ln 2\no\n+ q\nn\nln\nq\np+q + ln 2\no\ndz\n(11)\nThus, LHS ≤2RHS −2 log 2.\n□\n11\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-12-12",
  "updated": "2025-02-05"
}