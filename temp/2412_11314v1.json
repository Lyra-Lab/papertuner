{
  "id": "http://arxiv.org/abs/2412.11314v1",
  "title": "Reliable, Reproducible, and Really Fast Leaderboards with Evalica",
  "authors": [
    "Dmitry Ustalov"
  ],
  "abstract": "The rapid advancement of natural language processing (NLP) technologies, such\nas instruction-tuned large language models (LLMs), urges the development of\nmodern evaluation protocols with human and machine feedback. We introduce\nEvalica, an open-source toolkit that facilitates the creation of reliable and\nreproducible model leaderboards. This paper presents its design, evaluates its\nperformance, and demonstrates its usability through its Web interface,\ncommand-line interface, and Python API.",
  "text": "Reliable, Reproducible, and Really Fast Leaderboards with Evalica\nDmitry Ustalov\nJetBrains / Belgrade, Serbia\ndmitry.ustalov@jetbrains.com\nAbstract\nThe\nrapid\nadvancement\nof\nnatural\nlan-\nguage processing (NLP) technologies, such\nas instruction-tuned large language models\n(LLMs), urges the development of modern eval-\nuation protocols with human and machine feed-\nback. We introduce Evalica, an open-source\ntoolkit that facilitates the creation of reliable\nand reproducible model leaderboards. This\npaper presents its design, evaluates its perfor-\nmance, and demonstrates its usability through\nits Web interface, command-line interface, and\nPython API.\n1\nIntroduction\nThe emergent abilities, as exhibited by highly ca-\npable natural language processing (NLP) methods,\nsuch as instruction-tuned large language models\n(LLMs), urge the development of sound and reli-\nable evaluation protocols. While the earlier meth-\nods could be reasonably evaluated on static datasets\nor individual benchmarks, modern methods require\nup-to-date benchmarks with live feedback from hu-\nmans and machines (Faggioli et al., 2024). These\nbenchmarks are often represented as pairwise com-\nparison leaderboards (Figure 1), as popularized by\nLMSYS Arena (Chiang et al., 2024) and Alpaca-\nEval (Dubois et al., 2024) projects.\nAs the NLP methodology evolves rapidly, to-\nday’s evaluation methods are often implemented in\ncomputational notebooks and ad-hoc programs as\nan afterthought, which introduces errors, incompat-\nibilities, and harms reproducibility and adoption.\nTo improve the engineering aspect of benchmark-\ning by reducing the number of methodological er-\nFigure 1: Evalica facilitates the highlighted aspects of\nleaderboard-making that involve aggregation of judge-\nments, scoring the models with bootstrapped confidence\nintervals (CIs), and getting the final model ranks.\nrors and simplifying the exchange and interpreta-\ntion of the results, we present Evalica, an open-\nsource evaluation toolkit that facilitates and speeds\nup the creation of reliable and reproducible NLP\nmodel benchmarks,1 currently focused on the pref-\nerence data. Based on our four-year experience in\nthe development of production-grade tooling for\nquality control in crowdsourcing (Ustalov et al.,\n2024), we built Evalica with three practical goals\nin mind:\n• make the popular evaluation practices avail-\nable for a wide audience of users\n• ensure the performance and correctness of the\noffered implementations\n• provide the best developer experience possible\nThe remainder of this paper is organized as fol-\nlows. Section 2 reviews the related work and its\nrelationship with the declared goals. Section 3\n1https://github.com/dustalov/evalica\narXiv:2412.11314v1  [cs.CL]  15 Dec 2024\nshows Evalica’s design and how it satisfies these\ngoals. Section 4 describes the technical details of\nthe Evalica implementation, including the means\nto ensure its correctness. Section 5 reports perfor-\nmance benchmarks against alternative implementa-\ntions. Finally, Appendix A demonstrates a Web, a\ncommand-line , and a Python application program-\nming interfaces (API) of Evalica.\n2\nRelated Work\nThe research community has been developing vari-\nous toolkits for ranking systems, such as Elo (1978)\nand TrueSkill (Herbrich et al., 2006). In our analy-\nsis, we distinguish several classes of them.\nFirst, dedicated leaderboard building tools, such\nas IFEval (Zhou et al., 2023), LMSYS Arena (Chi-\nang et al., 2024), Arena-Hard (Li et al., 2024), and\nAlpacaEval (Dubois et al., 2024). These toolkits\nwere created by teams of researchers to implement\na specific novel evaluation methodology. The code\nwas generally written strictly tailored to the particu-\nlar benchmark, requiring extra effort from the user\nto apply it to their own dataset and domain. Due\nto the high pace of today’s scientific research, cer-\ntain software engineering best practices were often\nomitted, such as test coverage, code documentation,\ncontinuous integration, and data format compati-\nbility. At the same time, some implementations\nsuffer from suboptimal computational performance\non larger realistic datasets, which were out of scope\nof the original benchmarks.\nSecond, ranking system implementations, includ-\ning Rust packages Propagon2 and skillrating,3 a\nPython package OpenSkill.py (Joshi, 2024), and\nothers. As these packages are often written by\nskilled programmers in the best effort to bring cor-\nrect implementations, these methods do not always\nmatch the ones used in current best practices in\nNLP evaluation. Also, the non-Python packages\nrequire an additional non-trivial effort to integrate\nwith the existing Python code and notebooks.\nFinally, application-specific toolkits like Elova-\ntion,4 ArtistAssistApp,5 and Crowd-Kit (Ustalov\net al., 2024). These toolkits were built to accommo-\ndate user-generated content, usually in the form of\ncrowdsourcing annotation, and often do not follow\nthe methodology used in NLP evaluation.\n2https://github.com/Refefer/propagon\n3https://github.com/atomflunder/skillratings\n4https://github.com/elovation/elovation\n5https://github.com/eugene-khyst/\npairwise-comparison\nCore Test Suite\n(Python)\nAPI\n(Python)\nNaïve\nImplementations\n(Python)\nOther\nLanguages\nCore Implementations\n(Rust)\nFigure 2: Evalica has a core in Rust that is covered by\na comprehensive suite of tests in Python. We simplify\nprototyping and increase test reliability by keeping an\nindependent implementation of each method in Python.\n3\nDesign of Evalica\nEvalica facilitates three tasks shown in Figure 1:\nit provides optimized single-threaded implementa-\ntions of rating systems, simplifies the computation\nof confidence intervals for model scores, and offers\nconvenient routines to prepare visualizations.\nFigure 2 outlines the architecture of Evalica. In\nits core, there are performance-critical routines in\nRust that process the raw data. These core routines\nare wrapped in convenient APIs for application\ndevelopers in other languages. These APIs were\nresponsible for transforming the representation into\nthe indexed format as used by the core routines.6\nExamples of core routines are all the ranking al-\ngorithm implementations and helper routines for\nconstructing win matrices.\nWe currently only support Python due to its pop-\nularity in machine learning. For the sake of re-\nliability and ease of prototyping, we naïvely and\nimplemented all the methods additionally in Python\nand built a comprehensive test suite that compares\nthe Python implementations with the Rust ones.\nOther languages can be supported relatively easily\n(as long as there exists a bridge between Rust and\nthat language), and improvements to the core im-\nplementations and tests will improve the state of\nall the derivative code.\nWe believe that these measures allowed satis-\nfying the three goals mentioned in Section 1 ad-\n6Models\nusually\nhave\nnames\nlike\nllama-3.1-405b-instruct\nand\nclaude-3-5-sonnet-20240620.\nComputers do not op-\nerate with strings per se, so we need to transform such names\ninto the corresponding indices, e.g., 0 and 1.\nequately. Evalica accelerates popular evaluation\npractices by shipping the corresponding implemen-\ntations in a high-performing compiled program-\nming language, building on lessons learned from\npreviously developed software to increase the de-\nveloper’s productivity.\n4\nImplementation Details\nEvalica implements scoring approaches from popu-\nlar benchmarks, such as Chatbot Arena and Arena-\nHard: Elo (1978) and Bradley and Terry (1952),\nand average win rate. We ensured that they pro-\nvided the same results as in these benchmarks.\nThe package also contains implementations of the\neigenvalue method (Bonacich, 1987), PageRank\n(Brin and Page, 1998), tie-aware method of New-\nman (2023), and trivial vote counting.\nTo invoke the implementation in Evalica (List-\ning 1), one needs to supply a vector of left objects\n(xs), a vector of right objects (ys), a vector of win-\nner labels (winners), and, optionally, a vector of\nexample weights (weights) for style control, as\nproposed in Li et al. (2024). Possible values of the\nwinner labels are “X won,” “Y won,” and “tie.” All\nmethods are available in a lightweight and uniform\nfunctional API. We intentionally decided to avoid\nmaking assumptions about the tabular form of data\nas our experience in running Crowd-Kit (Ustalov\net al., 2024) in production showed that it required\nan error-prone data transformation step that could\nhave been avoided.\nInternally, Evalica does not operate with model\nnames, and core implementations require an index\nto compare the model name to the unique numeri-\ncal identifier (as described in Section 3). Since this\noperation takes short yet non-negligible time, we\nprovided the possibility to pass the already built\nindex to save time during bootstrapping the con-\nfidence intervals and other routines that require\nresampling and recomputing the scores (Listing 2).\nBesides the API, Evalica offers a built-in Web\ninterface and a command-line interface, see Ap-\npendix A for illustrative examples. More specif-\nically, the built-in Web interface follows a well-\nknown input-output separation paradigm from\nAbid et al. (2019) and was created using the Gradio\ntoolkit (Figure 4).7 The command-line interface\nwas developed using the pandas library for data\nmanipulation (McKinney, 2010) and the tools avail-\nable from the Python standard library (Figure 5).\n7https://www.gradio.app/\nAfter computing the scores and ranks, it is often\nuseful to visualize the pairwise win rates for the\ncompared models. Following Chiang et al. (2024),\nwe applied the Bradley and Terry (1952) definition\nof such a quantity for all pairs of models i and j:\npij =\nsi\nsi + sj\n,\nwhere pij is the probability of model i winning\nagainst the model j, si is the score of model i, and\nsj is the score of model j.\n4.1\nCorrectness and Reliability\nWe applied a set of reasonable means to ensure\ncorrectness and reliability of the method implemen-\ntations in Evalica. First, we implemented all the\nmethods independently in two different program-\nming languages, Rust and Python. We ensured\nthat the outputs for the same inputs are the same\nbetween these implementations. Second, we em-\nployed property-based tests with the Hypothesis\nlibrary (MacIver et al., 2019) for Python, which\nenumerated corner cases including empty or illegal\ninputs to break the program.8 We covered all such\ncases and provide reasonable numerical fallbacks,\nwhere possible. Third, we compared the outputs\nagainst the canonical scores from external bench-\nmarks. Fourth, we ensured that the test coverage is\nno less than 100%, and the test suite was executed\non every revision in the repository.\n4.2\nGovernance and Availability\nWe built Evalica using the trusted open-source\necosystem. The source code of Evalica was avail-\nable under the Apache License 2.0 on GitHub.9\nFeature requests and code contributions were pro-\ncessed using the Issues and Pull Requests features\non GitHub, correspondingly. We used continu-\nous integration on GitHub Actions to invoke per-\nrevision checks, including unit tests, linting, type\nchecking, test coverage measurement, and com-\nputational performance testing. Public dashboards\nwith test coverage and performance tests were avail-\nable on Codecov10 and Codspeed,11 correspond-\ningly. We used the trusted publishing approach\nto release Python packages to PyPI for the Linux,\nWindows, and macOS platforms.12 Our compiled\n8https://github.com/HypothesisWorks/hypothesis\n9https://github.com/dustalov/evalica\n10https://codecov.io/gh/dustalov/evalica\n11https://codspeed.io/dustalov/evalica\n12https://pypi.python.org/pypi/evalica\nSetup\nTime ↑\nBT in Evalica\n1.174 ± 0.009\nElo in Evalica\n1.256 ± 0.019\nElo from Arena-Hard\n3.778 ± 0.322\nBT from Chatbot Arena\n51.949 ± 1.797\nTable 1: Performance of Evalica, Chatbot Arena, and\nArena-Hard on the Chatbot Arena dataset. Time is in\nseconds; a 95% confidence interval is shown for ten\nruns. Smaller is better. BT means Bradley and Terry\n(1952), Elo means Elo (1978).\npackages were forward compatible with any ver-\nsion of Python newer than 3.8 due to the use of the\nstable CPython ABI. We also released Evalica on\nconda-forge for the users of Anaconda, a popular\ndistribution of scientific computing tools.13 Last\nbut not least, we published the developer documen-\ntation on Read the Docs.14\n5\nPerformance Tests\nWe performed two series of computational exper-\niments to study the running time of algorithm im-\nplementations in Evalica after ensuring their cor-\nrectness. First, we evaluated the difference in com-\nputational performance between the current imple-\nmentations in a popular benchmark and the ones\nprovided by Evalica. Second, we compared the\nperformance of core and naïve implementations of\nall the methods inside Evalica. All the experiments\nwere run using CPython 3.13.1, NumPy 2.2.0, and\nEvalica 0.3.2 on macOS 15.2 (Intel® Core™i5-\n8500 CPU, 32 GB RAM). All confidence intervals\nwere built using bootstrap with 10K samples and\n95% significance level.\n5.1\nChatbot Arena Experiment\nWe evaluated the performance of four setups in pro-\ncessing the August 14, 2024 version of the Chatbot\nArena dataset (Chiang et al., 2024) that contained\n1.7M pairwise comparisons of 129 models, ties\nwere not excluded.15 We compared four differ-\nent setups: an implementation of the Elo (1978)\nranking system in pure Python, as used in Chat-\nbot Arena, an implementation of Bradley and Terry\n(1952) in Python with scikit-learn (Pedregosa et al.,\n13https://anaconda.org/conda-forge/evalica\n14https://evalica.readthedocs.io/\n15https://storage.googleapis.com/arena_\nexternal_data/public/clean_battle_20240814_\npublic.json\nAlgorithm\nRust\nPython\nAverage Win Rate\n0.005 ± 0.000\n0.006 ± 0.000\nBradley–Terry\n0.005 ± 0.000\n0.012 ± 0.000\nCounting\n0.005 ± 0.000\n0.009 ± 0.000\nEigenvalue\n0.005 ± 0.000\n0.006 ± 0.000\nElo\n0.005 ± 0.000\n0.484 ± 0.004\nNewman\n0.006 ± 0.000\n0.010 ± 0.000\nPageRank\n0.005 ± 0.000\n0.006 ± 0.000\nTable 2: Running time comparison of core Rust and\nnaïve Python implementations of methods in Evalica\non the LLMFAO dataset. Time is in seconds; a 95%\nconfidence interval for ten runs is shown for each imple-\nmentation. Smaller is better.\n2011), as used in Arena-Hard, and Rust implemen-\ntations of these two methods in Evalica. For that,\nwe ran every setup ten times to simulate the realis-\ntic problem of confidence interval estimation that\ndoes often appear in model leaderboards. As the\nresults in Table 1 indicate, Evalica’s implementa-\ntions of ranking methods outperformed the current\nones as used in the benchmarks by up to 46 times\nwithout any involvement of multi-threading pro-\ncessing. Although this was expected since Python\nis an interpreted language and Rust is a compiled\nlanguage, we believe that the Evalica’s combina-\ntion of performance and ergonomics would allow\nrunning more experiments within the same time\nbudget. At the same time, performing computation\nin multiple threads, e.g., processing one sampling\nround per thread, would allow one to better use of\nthe modern multi-core CPUs and reduce the com-\nputation time by multiple times.\n5.2\nRust vs. Python in Evalica Experiment\nWe evaluated the performance of all the methods\nimplemented in Evalica’s core in Rust against their\nnaïve implementations in Python.\nDespite the\nname, these Python implementations were writ-\nten using NumPy (Harris et al., 2020), a highly\noptimized library built on decades of successful\nperformance engineering work for numerical com-\nputation in C and Fortran. We used the dataset from\na smaller benchmark called LLMFAO (Ustalov,\n2023), which had 9K pairwise comparisons for 59\nLLMs, gathered in October 2023 using crowdsourc-\ning. As the results in Table 2 show, the differences\nbetween core and naïve implementation were sta-\ntistically significant, according to the permutation\ntest (p < 0.01), but the effect size was not no-\nticeable on that scale due to the efficient NumPy\n101\n102\n103\n104\n105\n106\n107\nSize, pairs\n10 4\n10 3\n10 2\n10 1\n100\n101\nTime, seconds\nNewman\nCounting\nEigenvalue\nPageRank\nAvg. Win Rate\nElo\nBradley Terry\nFigure 3: Performance scaling analysis of the Rust implementations in Evalica on the synthetic version of the\nChatbot Arena dataset. Both scales are logarithmic. Time is in seconds, dataset size is the number of pairs; a 95%\nconfidence interval is shown for ten runs. Lower is better.\nroutines used in the pure Python implementations.\nOne important exception was Elo, whose equiva-\nlent implementation in Rust appeared to be more\nthan 96 times faster than in Python due to the ef-\nficient compiler optimizations. At the same time,\nthe Rust implementations had a smaller runtime\nvariance and more predictable performance, which\nshould be useful on larger-scale datasets.\n5.3\nScaling on Synthetic Data Experiment\nWe analyzed the relationship between dataset size\nand computation time using Evalica on a synthetic\ndataset derived from Chatbot Arena as the origi-\nnal dataset was already larger than most existing\npreference-based NLP datasets. We selected seven\ndataset sizes, ranging from 101 to 107 pairs, with\neach size increasing by a factor of ten. For each\nsize, we sampled the required number of pairs with\nreplacement from Chatbot Arena ten times to study\nthe time variance. Computation times were mea-\nsured using Rust implementations of the methods\navailable in Evalica, and we constructed 95% con-\nfidence intervals using bootstrapping. Figure 3\nshows that the relationship between dataset size\nand computation time scales linearly for all meth-\nods, indicating good scalability. However, there are\nclear performance differences for small input sizes,\nwith methods like Newman (2023) being slower\ninitially but converging to similar trends as input\nsize increases. Note that our analysis was limited\nby the number of models in the version of Chatbot\nArena used in our experiments.\n6\nConclusion\nWe believe that Evalica will foster the creation of\nreliable and reproducible benchmarks for future\nNLP systems. We define several potential direc-\ntions of further work: (1) implementing a larger set\nof use cases widely used in practice, including con-\nfidence interval construction out of the box and ad-\nditional ranking algorithms, (2) bringing additional\nperformance and memory optimizations, and (3)\nsupporting other popular programming languages\nwith good interoperability with Rust, including\nJavaScript and Ruby. To the best of our knowl-\nedge, Evalica is the first attempt to offer drop-in\naccelerated preference-based benchmarks, which\naffects their computational performance and numer-\nical reliability. We expect that a broader adoption\nof Evalica will result in faster iteration times, more\nuseful experiments, and fewer model-selection mis-\ntakes.\nAcknowledgements\nWe would like to thank three anonymous review-\ners for their useful feedback.\nWe are grateful\nto the early adopters of Evalica from the Vikhr\nteam (Nikolich et al., 2024) and JetBrains AI team,\nwhose kind feedback allowed improving the library.\nNote that the correct pronunciation of the library\nname is eh-vah-lee-tsah, which resembles a typical\nvillage name in the Balkans.\nReferences\nAbubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,\nAbdulrahman Alfozan, and James Y. Zou. 2019. Gra-\ndio: Hassle-Free Sharing and Testing of ML Models\nin the Wild. Preprint, arXiv:1906.02569.\nPhillip Bonacich. 1987. Power and Centrality: A Fam-\nily of Measures. American Journal of Sociology,\n92(5):1170–1182.\nRalph Allan Bradley and Milton E. Terry. 1952.\nRank Analysis of Incomplete Block Designs: I.\nThe Method of Paired Comparisons. Biometrika,\n39(3/4):324–345.\nSergey Brin and Lawrence Page. 1998. The anatomy of\na large-scale hypertextual Web search engine. Com-\nputer Networks and ISDN Systems, 30(1):107–117.\nProceedings of the Seventh International World Wide\nWeb Conference.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anasta-\nsios Nikolas Angelopoulos, Tianle Li, Dacheng Li,\nBanghua Zhu, Hao Zhang, Michael Jordan, Joseph E.\nGonzalez, and Ion Stoica. 2024. Chatbot Arena: An\nOpen Platform for Evaluating LLMs by Human Pref-\nerence.\nIn Proceedings of the 41st International\nConference on Machine Learning, volume 235 of\nProceedings of Machine Learning Research, pages\n8359–8388. PMLR.\nYann Dubois, Percy Liang, and Tatsunori Hashimoto.\n2024. Length-Controlled AlpacaEval: A Simple De-\nbiasing of Automatic Evaluators. In First Conference\non Language Modeling.\nArpad E. Elo. 1978. The Rating Of Chess Players, Past\n& Present. Arco Publishing Inc., New York.\nGuglielmo Faggioli, Laura Dietz, Charles L. A. Clarke,\nGianluca Demartini, Matthias Hagen, Claudia Hauff,\nNoriko Kando, Evangelos Kanoulas, Martin Potthast,\nBenno Stein, and Henning Wachsmuth. 2024. Who\nDetermines What Is Relevant? Humans or AI? Why\nNot Both? Communications of the ACM, 67(4):31–\n34.\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der\nWalt, Ralf Gommers, et al. 2020. Array program-\nming with NumPy. Nature, 585(7825):357–362.\nRalf Herbrich, Tom Minka, and Thore Graepel. 2006.\nTrueSkill™: A Bayesian Skill Rating System. In Ad-\nvances in Neural Information Processing Systems 19,\npages 569–576. MIT Press.\nVivek Joshi. 2024. OpenSkill: A faster asymmetric\nmulti-team, multiplayer rating system. Journal of\nOpen Source Software, 9(93):5901.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap,\nBanghua Zhu, Joseph E. Gonzalez, and Ion Stoica.\n2024. From Live Data to High-Quality Benchmarks:\nThe Arena-Hard Pipeline.\nDavid R. MacIver, Zac Hatfield-Dodds, et al. 2019. Hy-\npothesis: A new approach to property-based testing.\nJournal of Open Source Software, 4(43):1891.\nWes McKinney. 2010. Data Structures for Statistical\nComputing in Python. In Proceedings of the 9th\nPython in Science Conference, SciPy 2010, pages\n56–61.\nMark E. J. Newman. 2023. Efficient Computation of\nRankings from Pairwise Comparisons. Journal of\nMachine Learning Research, 24(238):1–25.\nAleksandr Nikolich, Konstantin Korolev, Artem Shel-\nmanov, and Igor Kiselev. 2024. Vikhr: The Family\nof Open-Source Instruction-Tuned Large Language\nModels for Russian. Preprint, arXiv:2405.13929.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, Jake Vanderplas, Alexandre Passos,\nDavid Cournapeau, Matthieu Brucher, Matthieu Per-\nrot, and Édouard Duchesnay. 2011. Scikit-learn: Ma-\nchine Learning in Python. Journal of Machine Learn-\ning Research, 12(85):2825–2830.\nDmitry Ustalov. 2023.\nLarge Language Model\nFeedback Analysis and Optimization (LLMFAO).\nDataset.\nDmitry Ustalov, Nikita Pavlichenko, and Boris Tseitlin.\n2024. Learning from Crowds with Crowd-Kit. Jour-\nnal of Open Source Software, 9(96):6227.\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt\nHaberland, Tyler Reddy, et al. 2020. SciPy 1.0: Fun-\ndamental Algorithms for Scientific Computing in\nPython. Nature Methods, 17:261–272.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-\ndhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. 2023.\nInstruction-Following\nEvaluation for Large Language Models. Preprint,\narXiv:2311.07911.\nA\nUsage Examples\n>>> from evalica import elo, pairwise_frame, Winner\n>>> result = elo(\n...\nxs=[\"pizza\", \"burger\", \"pizza\"],\n...\nys=[\"burger\", \"sushi\", \"sushi\"],\n...\nwinners=[Winner.X, Winner.Y, Winner.Draw],\n... )\n>>> result.scores\npizza\n1014.972058\nburger\n970.647200\nsushi\n1014.380742\nName: elo, dtype: float64\n>>> df_scores = pairwise_frame(result.scores)\n>>> df_scores\n# can be used for plotting the pairwise win rate\npizza\nsushi\nburger\npizza\n0.500000\n0.500003\n0.501499\nsushi\n0.499997\n0.500000\n0.501496\nburger\n0.498501\n0.498504\n0.500000\nListing 1: An example of computing Elo ranking and the corresponding pairwise win rates with Evalica. Other\nmethods can be applied similarly with a trivial modification: bradley_terry, average_win_rate, etc. See\nhttps://github.com/dustalov/evalica/blob/master/Tutorial.ipynb for an executable example.\n# index the compared models to save time by not re-indexing them at each round\n*_, index = evalica.indexing(\nxs=df[\"model_a\"],\n# series with model A identifiers\nys=df[\"model_b\"],\n# series with model B identifiers\n)\nbootstrap: list[\"pd.Series[str]\"] = []\n# assuming model names are strings\nfor r in range(BOOTSTRAP_ROUNDS):\n# for reproducibility, set the random seed equal to the number\n# of the bootstrapping round\ndf_sample = df_arena.sample(frac=1.0, replace=True, random_state=r)\n# estimate the Bradley-Terry scores for the given sample\nresult_sample = evalica.bradley_terry(\nxs=df_sample[\"model_a\"],\nys=df_sample[\"model_b\"],\nwinners=df_sample[\"winner\"],\nindex=index\n# use the index built above to speed up\n)\nbootstrap.append(result_sample.scores)\n# this is a data frame with BOOTSTRAP_ROUNDS rows,\n# each row represents the score of each model at the r-th round\ndf_bootstrap = pd.DataFrame(bootstrap)\n# this is a data frame with confidence intervals of scores\n# for each compared model\ndf_bootstrap_ci = pd.DataFrame({\n\"lower\": df_bootstrap.quantile(.025),\n\"rating\": df_bootstrap.quantile(.5),\n\"upper\": df_bootstrap.quantile(.975),\n}).reset_index(names=\"model\").sort_values(\"rating\", ascending=False)\nListing 2: An example of bootstrapping a 95% confidence interval of Bradley and Terry (1952) scores with\nEvalica and pandas (McKinney, 2010). Any other supported model can be applied after a trivial modification. For\nsimplicity, we do not show an example with scipy.stats.bootstrap (Virtanen et al., 2020), yet it is possible. See\nhttps://github.com/dustalov/evalica/blob/master/Chatbot-Arena.ipynb for an executable example.\nFigure 4: A screenshot of the Evalica’s Web interface with the LLMFAO benchmark (Ustalov, 2023). On the\nleft, there are the input file, algorithm choice, and additional parameters. On the right, there is a table with the\nranking results and a win rate plot. For the sake of brevity, we showed only a truncated output, with no columns\ncorresponding to the number of compared pairs and the current rank of the model. A live example can be accessed\nat https://huggingface.co/spaces/dustalov/pair2rank.\n$ head -n6 food.csv | column -ts,\nleft\nright\nwinner\nPizza\nSushi\nleft\nBurger\nPasta\nright\nTacos\nPizza\nleft\nSushi\nTacos\nright\nBurger\nPizza\nleft\n$ evalica -i food.csv bradley-terry | column -ts,\nitem\nscore\nrank\nTacos\n2.509025136024378\n1\nSushi\n1.1011561298265815\n2\nBurger\n0.8549063627182466\n3\nPasta\n0.7403814336665869\n4\nPizza\n0.5718366915548537\n5\nFigure 5: An example of using a command-line interface of Evalica to process a file in the comma-separated values\nformat and print the item ranks and estimated scores.\n",
  "categories": [
    "cs.CL",
    "62-04",
    "D.2.3"
  ],
  "published": "2024-12-15",
  "updated": "2024-12-15"
}