{
  "id": "http://arxiv.org/abs/2412.16365v1",
  "title": "Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)",
  "authors": [
    "Hansi Hettiarachchi",
    "Tharindu Ranasinghe",
    "Paul Rayson",
    "Ruslan Mitkov",
    "Mohamed Gaber",
    "Damith Premasiri",
    "Fiona Anting Tan",
    "Lasitha Uyangodage"
  ],
  "abstract": "The first Workshop on Language Models for Low-Resource Languages (LoResLM\n2025) was held in conjunction with the 31st International Conference on\nComputational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates.\nThis workshop mainly aimed to provide a forum for researchers to share and\ndiscuss their ongoing work on language models (LMs) focusing on low-resource\nlanguages, following the recent advancements in neural language models and\ntheir linguistic biases towards high-resource languages. LoResLM 2025 attracted\nnotable interest from the natural language processing (NLP) community,\nresulting in 35 accepted papers from 52 submissions. These contributions cover\na broad range of low-resource languages from eight language families and 13\ndiverse research areas, paving the way for future possibilities and promoting\nlinguistic inclusivity in NLP.",
  "text": "Overview of the First Workshop on Language Models for\nLow-Resource Languages (LoResLM 2025)\nHansi Hettiarachchi1, Tharindu Ranasinghe1, Paul Rayson1, Ruslan Mitkov1\nMohamed Gaber2, Damith Premasiri1, Fiona Anting Tan3, Lasitha Uyangodage4\n1Lancaster University, UK 2Birmingham City University, UK\n3National University of Singapore, Singapore 4University of Münster, Germany\nloreslm2025@gmail.com\nAbstract\nThe first Workshop on Language Models for\nLow-Resource Languages (LoResLM 2025)\nwas held in conjunction with the 31st Interna-\ntional Conference on Computational Linguis-\ntics (COLING 2025) in Abu Dhabi, United\nArab Emirates. This workshop mainly aimed\nto provide a forum for researchers to share and\ndiscuss their ongoing work on language mod-\nels (LMs) focusing on low-resource languages,\nfollowing the recent advancements in neural\nlanguage models and their linguistic biases\ntowards high-resource languages. LoResLM\n2025 attracted notable interest from the natural\nlanguage processing (NLP) community, result-\ning in 35 accepted papers from 52 submissions.\nThese contributions cover a broad range of low-\nresource languages from eight language fami-\nlies and 13 diverse research areas, paving the\nway for future possibilities and promoting lin-\nguistic inclusivity in NLP.\n1\nIntroduction\nLanguage models (LMs) have been a long-standing\nresearch topic, originating with simple n-gram\nmodels in the 1950s (Shannon, 1951). They are\ncomputational models that use the generative like-\nlihood of word sequences to perform natural lan-\nguage processing (NLP) tasks (Zhao et al., 2023).\nRecent advancements in LMs have significantly\nshifted towards neural language models due to their\nmore robust capabilities (Zhao et al., 2023; Minaee\net al., 2024). Developing pre-trained neural lan-\nguage models/transformers is a key milestone in\nLM research that notably enhanced NLP perfor-\nmance (Vaswani et al., 2017; Devlin et al., 2019).\nThis breakthrough has also prompted the devel-\nopment of more advanced large language models\n(LLMs), such as GPT, which consist of vast num-\nbers of parameters pre-trained on extensive text cor-\npora, resulting in state-of-the-art natural language\nunderstanding and generation across various appli-\ncations (Touvron et al., 2023; Jiang et al., 2023).\nThere are approximately 7,000 spoken languages\nworldwide (van Esch et al., 2022). However, most\nNLP research focuses on about 20 languages with\nhigh resources (Magueresse et al., 2020). For ex-\nample, 63% of the papers published at ACL 2008\nfocused on English (Bender, 2011), and even a\ndecade later, 70% of the papers at ACL 2021 were\nevaluated only in English (Ruder et al., 2022). The\nremaining numerous languages that receive little\nresearch attention are commonly referred to as low-\nresource languages. These languages generally\nlack sufficient digital data and resources to support\nNLP tasks. They are also known as resource-scarce,\nresource-poor, less computerised, low-data, or low-\ndensity languages (Ranathunga et al., 2023).\nSince the capabilities of LMs are primarily de-\ntermined by the characteristics of their pre-trained\nlanguage corpora, disparities in language resources\nare also evident within the models. For instance,\nmany widely used transformer models (e.g., BERT\n(Devlin et al., 2019), RoBERTa (Liu et al., 2019),\nBART (Lewis et al., 2020), and T5 (Raffel et al.,\n2020)) only support English. However, the cross-\nlingual capabilities of transformers have paved the\nway for multilingual models (e.g., mBERT (Devlin\net al., 2019), XLM-R (Conneau et al., 2020), mT5\n(Xue et al., 2021), and BLOOM (Scao et al., 2022)),\nallowing low-resource languages to benefit from\nother languages through joint learning approaches.\nDespite this progress, these models are typically\nlimited to up to 100 languages due to the curse of\nmultilingualism (Conneau et al., 2020). In light\nof this challenge, developing monolingual mod-\nels (e.g., SinBERT for Sinhala (Dhananjaya et al.,\n2022), and PhoBERT for Vietnamese (Nguyen and\nTuan Nguyen, 2020)) is another growing trend\nrecently established to promote research in low-\nresource languages.\nThere are several common factors which impede\nlow-resource language research. One major issue\nis limited data availability, as the performance of\narXiv:2412.16365v1  [cs.CL]  20 Dec 2024\nmost models depends heavily on the amount of\ntraining data (Hettiarachchi et al., 2024). Even\nrecent neural LMs with multilingual capabilities\ntend to perform poorly when pre-training data for\na particular language is limited or unseen (Ahuja\net al., 2022; Hettiarachchi et al., 2023). Data qual-\nity also plays a pivotal role in research outcomes,\nyet the absence of recommended guidelines hinders\nthe quality of low-resource language data (Lignos\net al., 2022). Additionally, the scarcity of bench-\nmark datasets tailored for low-resource languages\ntends to bias most model evaluations towards high-\nresource languages (Blasi et al., 2022; Ranasinghe\net al., 2024).\nInterestingly, there are several ongoing efforts\nthat aim to encourage research on low-resource lan-\nguages and mitigate the bias in NLP approaches to-\nwards high-resource languages (Chakravarthi et al.,\n2022; Ojha et al., 2023; Melero et al., 2024). We\norganised the first Workshop on Language Mod-\nels for Low-Resource Languages (LoResLM 2025)\nto further strengthen this trend. LoResLM 20251\nspecifically focused on LM-based approaches for\nlow-resource languages, inviting submissions on a\nbroad range of topics, including creating corpora,\ndeveloping benchmarks, building or adapting LMs,\nand exploring LM applications for low-resource\nlanguages. Section 2 provides a summary of the\nworkshop contributions, highlighting language and\ntask/research area coverage. We invite you to refer\nto the full papers available in the proceedings for\nmore detailed information.\n2\nWorkshop Contributions\nLoResLM 2025 received 52 submissions, including\n40 long papers and 12 short papers. Among these,\nwe accepted 35 papers, including 28 long papers\nand seven short papers, to appear in the workshop\nproceedings, following the review process. We\nprovide a detailed summary of the distribution of\naccepted papers across various languages and re-\nsearch areas below.\n2.1\nLanguages\nAs illustrated in Figure 1, the papers accepted to\nLoResLM 2025 mainly span eight language fam-\nilies. The majority representation is from Indo-\nEuropean family, while Koreanic, Sino-Tibetan and\nIsolate language families have equal minority rep-\nresentation. Languages with no relationships with\n1Available at https://loreslm.github.io/\nothers were considered under the Isolate family.\nFigure 1: Distribution of workshop contributions across\nlanguage families\nWe present a detailed language-level analysis in\nTable 1. We further divided the Indo-European fam-\nily into its first branch level for a comprehensive\nexploration, given its wide contributions. Over-\nall, there were contributions from four distinct\nbranches of the Indo-European language family.\nDuring this analysis, we focused exclusively on\nlow-resource languages, excluding high-resource\nlanguages involved in comparison studies. How-\never, some languages that would typically classify\nas high-resource considering the general resource\ndistribution across popular research areas (e.g. Ara-\nbic, German, etc.) were considered low-resource in\nspecific contexts where resources are limited, such\nas particular domains, research areas, or dialects.\nIn total, contributions covered 28 low-resource lan-\nguages. Additionally, a few papers experimented\nwith multiple languages (more than five) from var-\nious language families. These were categorised\nunder ‘Multiple’ but excluded from the language\ncount given above, as their focus was more on the\ntask level rather than the language level.\n2.2\nResearch Areas\nTable 2 shows the distribution of the accepted pa-\npers across various NLP research areas. These\nareas were adopted based on the topics of call for\npapers from leading NLP conferences in 2024.\nOverall, the accepted papers contributed to 13\nNLP research areas. As expected, the most popular\ntopic among the accepted papers was ‘Language\nModelling’ with eleven papers. ‘Machine Trans-\nLanguage Family\nLanguage\nPapers\nAfro-Asiatic\nArabic\nNacar et al. (2025); Shang et al. (2025); Zeinalipour et al. (2025b)\nHausa\nSani et al. (2025)\nAustronesian\nFilipino\nGamboa and Lee (2025)\nTagalog\nCruz (2025)\nIndo-European\n(Germanic)\nGerman\nZhukova et al. (2025)\nOld English\nHarju and van der Goot (2025)\nIndo-European\n(Hellenic)\nAncient Greek\nRapacz and Smywi´nski-Pohl (2025)\nIndo-European\n(Indo-Iranian)\nBengali\nAlam et al. (2025); Sadhu et al. (2025)\nMarathi\nMutsaddi and Choudhary (2025); Dmonte et al. (2025)\nPersian\nHabibzadeh and Asadpour (2025); Mokhtarabadi et al. (2025);\nZeinalipour et al. (2025a)\nSinhala\nDmonte et al. (2025)\nUrdu\nAmin et al. (2025); Donthi et al. (2025)\nIndo-European\n(Italic)\nItalian\nAmin et al. (2025)\nMedieval Latin\nLiu et al. (2025)\nMonégasque\nMerad et al. (2025)\nPortuguese\nLasheras and Pinheiro (2025)\nIsolate\nBasque\nKryvosheieva and Levy (2025)\nKoreanic\nKorean\nTran et al. (2025)\nNiger-Congo\nisiXhosa\nMatzopoulos et al. (2025)\nIsiZulu\nMahlaza et al. (2025)\nMooré\nOuattara et al. (2025)\nSwahili\nKryvosheieva and Levy (2025)\nSino-Tibetan\nCantonese\nDai et al. (2025)\nTurkic\nKazakh\nVeitsman and Hartmann (2025)\nKyrgyz\nVeitsman and Hartmann (2025)\nTurkish\nVeitsman and Hartmann (2025)\nTurkmen\nVeitsman and Hartmann (2025)\nUzbek\nVeitsman and Hartmann (2025); Bobojonova et al. (2025)\nMultiple\nBagheri Nezhad et al. (2025); Zhu et al. (2025); Tashu and Tudor (2025);\nSindhujan et al. (2025); Dewangan et al. (2025)\nTable 1: Coverage of workshop papers across different languages. The final row (‘Multiple’) represents the scenario\nwhere more than five languages from multiple language families are experimented with.\nlation and Translation Aids’ was the second most\npopular topic with six papers. The other topics ap-\nproximately had a similar number of papers. Apart\nfrom the papers mentioned in Table 2, Veitsman\nand Hartmann (2025) provided a survey on Central\nAsian Turkic languages spanning across several\nresearch areas.\n3\nConclusions\nThe first Workshop on Language Models for Low-\nResource Languages (LoResLM 2025) attracted\na lot of interest from the NLP community, hav-\ning 35 accepted papers from 52 submissions. The\naccepted papers mainly span eight language fam-\nilies, with the majority representation being from\nIndo-European families. Furthermore, the accepted\npapers contributed to 13 NLP research areas, with\nmajor contributions to ‘Language Modelling’ and\n‘Machine Translation and Translation Aids’. We\nbelieve the findings and resources from LoResLM\nwill open exciting new avenues to empower linguis-\ntic diversity for millions of low-resource languages.\nFor the future iterations of LoResLM, we expect\nbetter representation from more diverse linguistic\ngroups, particularly those from underrepresented\nfamilies such as Uralic, Dravidian and Indigenous\nlanguages of the Americas. Furthermore, we aim\nto diversify research topics, encouraging work in\nareas such as speech processing, information ex-\ntraction, and dialogue systems, which are critical\nfor many practical applications.\nPaper\nDialogue and\nInteractive Systems\nEthics, Bias,\nand Fairness\nInformation Retrieval\nand Text Mining\nLanguage Modelling\nLinguistic Insights Derived\nusing Computational Techniques\nMachine Translation and\nTranslation Aids\nNLP and LLM Applications\nOffensive Speech Detection\nand Analysis\nPhonology, Morphology and\nWord Segmentation\nQuestion Answering\nLexical Semantics\nSentiment Analysis, Stylistic Analysis,\nOpinion and Argument Mining\nSyntactic analysis\n(Tagging, Chunking, Parsing)\nLiu et al. (2025)\n✓\nGamboa and Lee (2025)\n✓\nAlam et al. (2025)\n✓\nCruz (2025)\n✓\nDai et al. (2025)\n✓\nTurumtaev (2025)\n✓\nSani et al. (2025)\n✓\nMutsaddi and Choudhary (2025)\n✓\nAmin et al. (2025)\n✓\nBagheri Nezhad et al. (2025)\n✓\nOuattara et al. (2025)\n✓\nZhu et al. (2025)\n✓\nMatzopoulos et al. (2025)\n✓\nRapacz and Smywi´nski-Pohl (2025)\n✓\nHabibzadeh and Asadpour (2025)\n✓\nDmonte et al. (2025)\n✓\n✓\nTashu and Tudor (2025)\n✓\nMokhtarabadi et al. (2025)\n✓\nTran et al. (2025)\n✓\nMerad et al. (2025)\n✓\nMahlaza et al. (2025)\n✓\nNacar et al. (2025)\n✓\nKryvosheieva and Levy (2025)\n✓\nHarju and van der Goot (2025)\n✓\nShang et al. (2025)\n✓\nDonthi et al. (2025)\n✓\nSadhu et al. (2025)\n✓\nSindhujan et al. (2025)\n✓\nBobojonova et al. (2025)\n✓\nDewangan et al. (2025)\n✓\nZeinalipour et al. (2025a)\n✓\nLasheras and Pinheiro (2025)\n✓\nZeinalipour et al. (2025b)\n✓\nZhukova et al. (2025)\n✓\nTable 2: Coverage of workshop papers across different NLP areas.\nReferences\nKabir Ahuja, Shanu Kumar, Sandipan Dandapat, and\nMonojit Choudhury. 2022. Multi Task Learning For\nZero Shot Performance Prediction of Multilingual\nModels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5454–5467, Dublin,\nIreland. Association for Computational Linguistics.\nSadia Alam, Md Farhan Ishmam, Navid Hasin Alvee,\nMd Shahnewaz Siddique, Md Azam Hossain, and\nAbu Raihan Mostofa Kamal. 2025. BnSentMix: A\nDiverse Bengali-English Code-Mixed Dataset for\nSentiment Analysis. In Proceedings of the 1st Work-\nshop on Language Models for Low-Resource Lan-\nguages (LoResLM2025), Abu Dhabi, UAE. Interna-\ntional Committee on Computational Linguistics.\nMuhammad Saad Amin, Luca Anselma, and Alessan-\ndro Mazzei. 2025.\nExploiting Task Reversibility\nof DRS Parsing and Generation: Challenges and\nInsights from a Multi-lingual Perspective. In Pro-\nceedings of the 1st Workshop on Language Models\nfor Low-Resource Languages (LoResLM2025), Abu\nDhabi, UAE. International Committee on Computa-\ntional Linguistics.\nSina Bagheri Nezhad, Ameeta Agrawal, and Rhitabrat\nPokharel. 2025. Beyond Data Quantity: Key Fac-\ntors Driving Performance in Multilingual Language\nModels.\nIn Proceedings of the 1st Workshop on\nLanguage Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\nEmily M. Bender. 2011. On Achieving and Evaluating\nLanguage-Independence in NLP. Linguistic Issues\nin Language Technology, 6.\nDamian Blasi, Antonios Anastasopoulos, and Graham\nNeubig. 2022. Systematic Inequalities in Language\nTechnology Performance across the World’s Lan-\nguages. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5486–5505, Dublin,\nIreland. Association for Computational Linguistics.\nLatofat Bobojonova, Arofat Akhundjanova, Phil Sid-\nney Ostheimer, and Sophie Fellenz. 2025. BBPOS:\nBERT-based Part-of-Speech Tagging for Uzbek. In\nProceedings of the 1st Workshop on Language Mod-\nels for Low-Resource Languages (LoResLM2025),\nAbu Dhabi, UAE. International Committee on Com-\nputational Linguistics.\nBharathi Raja Chakravarthi, B Bharathi, John P McCrae,\nManel Zarrouk, Kalika Bali, and Paul Buitelaar, ed-\nitors. 2022. Proceedings of the Second Workshop\non Language Technology for Equality, Diversity and\nInclusion. Association for Computational Linguistics,\nDublin, Ireland.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJan Christian Blaise Cruz. 2025. Extracting General-\nuse Transformers for Low-resource Languages via\nKnowledge Distillation. In Proceedings of the 1st\nWorkshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nYuqian Dai, Chun Fai Chan, Ying Ki Wong, and Tsz Ho\nPun. 2025.\nNext-Level Cantonese-to-Mandarin\nTranslation: Fine-Tuning and Post-Processing with\nLLMs.\nIn Proceedings of the 1st Workshop on\nLanguage Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nVikrant Dewangan, Bharath Raj S, Garvit Suri, and\nRaghav Sonavane. 2025. When Every Token Counts:\nOptimal Segmentation for Low-Resource Language\nModels.\nIn Proceedings of the 1st Workshop on\nLanguage Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\nVinura Dhananjaya, Piyumal Demotte, Surangika\nRanathunga, and Sanath Jayasena. 2022. BERTi-\nfying Sinhala - A Comprehensive Analysis of Pre-\ntrained Language Models for Sinhala Text Classifi-\ncation. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 7377–\n7385, Marseille, France. European Language Re-\nsources Association.\nAlphaeus Dmonte, Shrey Satapara, Rehab Alsudais,\nTharindu Ranasinghe, and Marcos Zampieri. 2025.\nDoes Machine Translation Impact Offensive Lan-\nguage Identification?\nThe Case of Indo-Aryan\nLanguages.\nIn Proceedings of the 1st Workshop\non Language Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\nSundesh Donthi, Maximilian Spencer, Om B. Patel,\nJoon Young Doh, Eid Rodan, Kevin Zhu, and Sean\nO’Brien. 2025.\nImproving LLM Abilities in Id-\niomatic Translation. In Proceedings of the 1st Work-\nshop on Language Models for Low-Resource Lan-\nguages (LoResLM2025), Abu Dhabi, UAE. Interna-\ntional Committee on Computational Linguistics.\nLance Calvin Lim Gamboa and Mark Lee. 2025. Fil-\nipino Benchmarks for Measuring Sexist and Homo-\nphobic Bias in Multilingual Language Models from\nSoutheast Asia.\nIn Proceedings of the 1st Work-\nshop on Language Models for Low-Resource Lan-\nguages (LoResLM2025), Abu Dhabi, UAE. Interna-\ntional Committee on Computational Linguistics.\nZahra Habibzadeh and Masoud Asadpour. 2025. Using\nLanguage Models for Assessment of Users’ Satisfac-\ntion with Their Partner in Persian. In Proceedings\nof the 1st Workshop on Language Models for Low-\nResource Languages (LoResLM2025), Abu Dhabi,\nUAE. International Committee on Computational\nLinguistics.\nAnika Harju and Rob van der Goot. 2025. How to\nage BERT Well: Continuous Training for Histori-\ncal Language Adaptation. In Proceedings of the 1st\nWorkshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nHansi Hettiarachchi, Mariam Adedoyin-Olowe, Jagdev\nBhogal, and Mohamed Medhat Gaber. 2023. TTL:\ntransformer-based two-phase transfer learning for\ncross-lingual news event detection. International\nJournal of Machine Learning and Cybernetics,\n14(8):2739–2760.\nHansi Hettiarachchi, Damith Premasiri, Lasitha Ran-\ndunu Chandrakantha Uyangodage, and Tharindu\nRanasinghe. 2024. NSina: A news corpus for Sin-\nhala. In Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024),\npages 12307–12312, Torino, Italia. ELRA and ICCL.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7B. arXiv preprint arXiv:2310.06825.\nDaria Kryvosheieva and Roger Levy. 2025. Controlled\nEvaluation of Syntactic Knowledge in Multilingual\nLanguage Models. In Proceedings of the 1st Work-\nshop on Language Models for Low-Resource Lan-\nguages (LoResLM2025), Abu Dhabi, UAE. Interna-\ntional Committee on Computational Linguistics.\nUriel Anderson Lasheras and Vladia Pinheiro. 2025.\nCaLQuest.PT: Towards the Collection and Evalu-\nation of Natural Causal Ladder Questions in Por-\ntuguese for AI Agents. In Proceedings of the 1st\nWorkshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nConstantine Lignos, Nolan Holley, Chester Palen-\nMichel, and Jonne Sälevä. 2022. Toward More Mean-\ningful Resources for Lower-resourced Languages. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, pages 523–532, Dublin, Ireland.\nAssociation for Computational Linguistics.\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy,\nM Lewis, L Zettlemoyer, and V Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692, 364.\nYifan Liu, Gelila Tilahun, Xinxiang Gao, Qianfeng Wen,\nand Michael Gervers. 2025. A Comparative Study\nof Static and Contextual Embeddings for Analyzing\nSemantic Changes in Medieval Latin Charters. In\nProceedings of the 1st Workshop on Language Mod-\nels for Low-Resource Languages (LoResLM2025),\nAbu Dhabi, UAE. International Committee on Com-\nputational Linguistics.\nAlexandre Magueresse, Vincent Carles, and Evan Heet-\nderks. 2020.\nLow-resource languages: A review\nof past work and future challenges. arXiv preprint\narXiv:2006.07264.\nZola Mahlaza, C. Maria Keet, Imaan Sayed, and Alexan-\nder Van Der Leek. 2025. IsiZulu Noun Classifica-\ntion Based on Replicating the Ensemble Approach\nfor Runyankore. In Proceedings of the 1st Work-\nshop on Language Models for Low-Resource Lan-\nguages (LoResLM2025), Abu Dhabi, UAE. Interna-\ntional Committee on Computational Linguistics.\nAlexis Matzopoulos, Charl Hendriks, Hishaam Ma-\nhomed, and Francois Meyer. 2025. BabyLMs for\nisiXhosa: Data-Efficient Language Modelling in a\nLow-Resource Context. In Proceedings of the 1st\nWorkshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nMaite Melero, Sakriani Sakti, and Claudia Soria, edi-\ntors. 2024. Proceedings of the 3rd Annual Meeting of\nthe Special Interest Group on Under-resourced Lan-\nguages @ LREC-COLING 2024. ELRA and ICCL,\nTorino, Italia.\nIbrahim Merad, Amos Wolf, Ziad Mazzawi, and Yan-\nnick Léo. 2025. Language verY Rare for All. In\nProceedings of the 1st Workshop on Language Mod-\nels for Low-Resource Languages (LoResLM2025),\nAbu Dhabi, UAE. International Committee on Com-\nputational Linguistics.\nShervin Minaee, Tomas Mikolov, Narjes Nikzad,\nMeysam Chenaghlu, Richard Socher, Xavier Am-\natriain, and Jianfeng Gao. 2024. Large language\nmodels: A survey. arXiv preprint arXiv:2402.06196.\nHojjat Mokhtarabadi, Ziba Zamani, Abbas Maazallahi,\nand Mohammad Hossein Manshaei. 2025. Empower-\ning Persian LLMs for Instruction Following: A Novel\nDataset and Training Approach.\nIn Proceedings\nof the 1st Workshop on Language Models for Low-\nResource Languages (LoResLM2025), Abu Dhabi,\nUAE. International Committee on Computational\nLinguistics.\nAtharva Mutsaddi and Aditya Prashant Choudhary.\n2025. Enhancing Plagiarism Detection in Marathi\nwith a Weighted Ensemble of TF-IDF and BERT Em-\nbeddings for Low-Resource Language Processing. In\nProceedings of the 1st Workshop on Language Mod-\nels for Low-Resource Languages (LoResLM2025),\nAbu Dhabi, UAE. International Committee on Com-\nputational Linguistics.\nOmer Nacar, Serry Taiseer Sibaee, Samar Ahmed, Safa\nBen Atitallah, Adel Ammar, Yasser Alhabashi, Ab-\ndulrahman S. Al-Batati, Arwa Alsehibani, Nour Qan-\ndos, Omar Elshehy, Mohamed Abdelkader, and Anis\nKoubaa. 2025. Towards Inclusive Arabic LLMs: A\nCulturally Aligned Benchmark in Arabic Large Lan-\nguage Model Evaluation. In Proceedings of the 1st\nWorkshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhoBERT: Pre-trained language models for Viet-\nnamese. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1037–1042,\nOnline. Association for Computational Linguistics.\nAtul Kr. Ojha, Chao-hong Liu, Ekaterina Vylomova,\nFlammie Pirinen, Jade Abbott, Jonathan Washington,\nNathaniel Oco, Valentin Malykh, Varvara Logacheva,\nand Xiaobing Zhao, editors. 2023. Proceedings of\nthe Sixth Workshop on Technologies for Machine\nTranslation of Low-Resource Languages (LoResMT\n2023). Association for Computational Linguistics,\nDubrovnik, Croatia.\nMaimouna Ouattara, Abdoul Kader Kaboré, Jacques\nKlein, and Tegawendé F. Bissyandé. 2025. Bridging\nLiteracy Gaps in African Informal Business Manage-\nment with Low-Resource Conversational Agents. In\nProceedings of the 1st Workshop on Language Mod-\nels for Low-Resource Languages (LoResLM2025),\nAbu Dhabi, UAE. International Committee on Com-\nputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nTharindu Ranasinghe, Isuri Anuradha, Damith Pre-\nmasiri, Kanishka Silva, Hansi Hettiarachchi, Lasitha\nUyangodage, and Marcos Zampieri. 2024. Sold: Sin-\nhala offensive language dataset. Language Resources\nand Evaluation, pages 1–41.\nSurangika Ranathunga, En-Shiun Annie Lee, Marjana\nPrifti Skenduli, Ravi Shekhar, Mehreen Alam, and\nRishemjit Kaur. 2023. Neural Machine Translation\nfor Low-resource Languages: A Survey. ACM Com-\nput. Surv., 55(11).\nMaciej Rapacz and Aleksander Smywi´nski-Pohl. 2025.\nLow-Resource Interlinear Translation: Morphology-\nEnhanced Neural Models for Ancient Greek. In Pro-\nceedings of the 1st Workshop on Language Models\nfor Low-Resource Languages (LoResLM2025), Abu\nDhabi, UAE. International Committee on Computa-\ntional Linguistics.\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard.\n2022. Square One Bias in NLP: Towards a Multi-\nDimensional Exploration of the Research Manifold.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 2340–2354, Dublin,\nIreland. Association for Computational Linguistics.\nJayanta Sadhu, Maneesha Rani Saha, and Rifat Shahri-\nyar. 2025. Social Bias in Large Language Models\nfor Bangla: An Empirical Study on Gender and Re-\nligious Bias. In Proceedings of the 1st Workshop\non Language Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\nSani Abdullahi Sani, Shamsuddeen Hassan Muhammad,\nand Devon Jarvis. 2025. Investigating the Impact of\nLanguage-Adaptive Fine-Tuning on Sentiment Anal-\nysis in Hausa Language Using AfriBERTa. In Pro-\nceedings of the 1st Workshop on Language Models\nfor Low-Resource Languages (LoResLM2025), Abu\nDhabi, UAE. International Committee on Computa-\ntional Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\net al. 2022.\nBloom:\nA 176b-parameter open-\naccess multilingual language model. arXiv preprint\narXiv:2211.05100.\nGuokan Shang, Hadi Abdine, Yousef Khoubrane, Amr\nMohamed, Yassine ABBAHADDOU, Sofiane En-\nnadir, Imane Momayiz, Xuguang Ren, Eric Moulines,\nPreslav Nakov, Michalis Vazirgiannis, and Eric Xing.\n2025. Atlas-Chat: Adapting Large Language Models\nfor Low-Resource Moroccan Arabic Dialect. In Pro-\nceedings of the 1st Workshop on Language Models\nfor Low-Resource Languages (LoResLM2025), Abu\nDhabi, UAE. International Committee on Computa-\ntional Linguistics.\nClaude E Shannon. 1951.\nPrediction and entropy\nof printed English. Bell system technical journal,\n30(1):50–64.\nArchchana Sindhujan, Diptesh Kanojia, Constantin\nOrasan, and Shenbin Qian. 2025. When LLMs Strug-\ngle: Reference-less Translation Evaluation for Low-\nresource Languages. In Proceedings of the 1st Work-\nshop on Language Models for Low-Resource Lan-\nguages (LoResLM2025), Abu Dhabi, UAE. Interna-\ntional Committee on Computational Linguistics.\nTsegaye Misikir Tashu and Andreea Ioana Tudor. 2025.\nMapping Cross-Lingual Sentence Representations\nfor Low-Resource Language Pairs Using Pre-trained\nLanguage Models. In Proceedings of the 1st Work-\nshop on Language Models for Low-Resource Lan-\nguages (LoResLM2025), Abu Dhabi, UAE. Interna-\ntional Committee on Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nVan-Hien Tran, Raj Dabre, Hour Kaing, Haiyue Song,\nHideki Tanaka, and Masao Utiyama. 2025. Exploit-\ning Word Sense Disambiguation in Large Language\nModels for Machine Translation. In Proceedings\nof the 1st Workshop on Language Models for Low-\nResource Languages (LoResLM2025), Abu Dhabi,\nUAE. International Committee on Computational\nLinguistics.\nGalim Turumtaev. 2025.\nStop Jostling: Adaptive\nNegative Sampling Reduces the Marginalization of\nLow-Resource Language Tokens by Cross-Entropy\nLoss.\nIn Proceedings of the 1st Workshop on\nLanguage Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\nDaan van Esch, Tamar Lucassen, Sebastian Ruder, Isaac\nCaswell, and Clara Rivera. 2022. Writing System\nand Speaker Metadata for 2,800+ Language Varieties.\nIn Proceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 5035–5046, Mar-\nseille, France. European Language Resources Asso-\nciation.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nYana Veitsman and Mareike Hartmann. 2025.\nRe-\ncent Advancements and Challenges of Turkic Central\nAsian Language Processing. In Proceedings of the\n1st Workshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nKamyar Zeinalipour, Neda Jamshidi, Fahimeh Akbari,\nMarco Maggini, Monica Bianchini, and Marco Gori.\n2025a. PersianMCQ-Instruct: A Comprehensive Re-\nsource for Generating Multiple-Choice Questions\nin Persian.\nIn Proceedings of the 1st Workshop\non Language Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\nKamyar Zeinalipour, Moahmmad Saad, Marco Mag-\ngini, and Marco Gori. 2025b. From Arabic Text to\nPuzzles: LLM-Driven Development of Arabic Ed-\nucational Crosswords.\nIn Proceedings of the 1st\nWorkshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models.\narXiv preprint\narXiv:2303.18223.\nHongpu Zhu, Yuqi Liang, Wenjing Xu, and Hongzhi\nXu. 2025. Evaluating Large Language Models for\nIn-Context Learning of Linguistic Patterns In Unseen\nLow Resource Languages. In Proceedings of the 1st\nWorkshop on Language Models for Low-Resource\nLanguages (LoResLM2025), Abu Dhabi, UAE. Inter-\nnational Committee on Computational Linguistics.\nAnastasia Zhukova, Christian E. Matt, and Bela Gipp.\n2025. Automated Collection of Evaluation Dataset\nfor Semantic Search in Low-Resource Domain Lan-\nguage.\nIn Proceedings of the 1st Workshop on\nLanguage Models for Low-Resource Languages\n(LoResLM2025), Abu Dhabi, UAE. International\nCommittee on Computational Linguistics.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-12-20",
  "updated": "2024-12-20"
}