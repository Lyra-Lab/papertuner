{
  "id": "http://arxiv.org/abs/2412.17349v1",
  "title": "Deep Learning in Proteomics Informatics: Applications, Challenges, and Future Directions",
  "authors": [
    "Yindan Luo",
    "Jiaxin Cai"
  ],
  "abstract": "Deep learning is an advanced technology that relies on large-scale data and\ncomplex models for feature extraction and pattern recognition. It has been\nwidely applied across various fields, including computer vision, natural\nlanguage processing, and speech recognition. In recent years, deep learning has\ndemonstrated significant potential in the realm of proteomics informatics,\nparticularly in deciphering complex biological information. The introduction of\nthis technology not only accelerates the processing speed of protein data but\nalso enhances the accuracy of predictions regarding protein structure and\nfunction. This provides robust support for both fundamental biology research\nand applied biotechnological studies. Currently, deep learning is primarily\nfocused on applications such as protein sequence analysis, three-dimensional\nstructure prediction, functional annotation, and the construction of protein\ninteraction networks. These applications offer numerous advantages to proteomic\nresearch. Despite its growing prevalence in this field, deep learning faces\nseveral challenges including data scarcity, insufficient model\ninterpretability, and computational complexity; these factors hinder its\nfurther advancement within proteomics. This paper comprehensively reviews the\napplications of deep learning in proteomics along with the challenges it\nencounters. The aim is to provide a systematic theoretical discussion and\npractical basis for research in this domain to facilitate ongoing development\nand innovation of deep learning technologies within proteomics.",
  "text": "Deep Learning in Proteomics Informatics: \nApplications, Challenges, and Future Directions \n \nYindan Luo+                                                         Jiaxin Cai+* \nSchool of Computer and Imformation Engineering            School of Mathematics and Statistics \nXiamen University of Technology                    Xiamen University of Technology \nXiamen, China                                   Xiamen, China \n1015959737@qq.com                             caijiaxin@xmut.edu.cn \n* Corresponding author: Jiaxin Cai.  \n+ Yindan Luo and Jiaxin Cai are co-first authors. \n \nAbstract: Deep learning is an advanced technology that relies on large-scale data and complex \nmodels for feature extraction and pattern recognition. It has been widely applied across various fields, \nincluding computer vision, natural language processing, and speech recognition. In recent years, deep \nlearning has demonstrated significant potential in the realm of proteomics informatics, particularly in \ndeciphering complex biological information. The introduction of this technology not only accelerates \nthe processing speed of protein data but also enhances the accuracy of predictions regarding protein \nstructure and function. This provides robust support for both fundamental biology research and applied \nbiotechnological studies. Currently, deep learning is primarily focused on applications such as protein \nsequence analysis, three-dimensional structure prediction, functional annotation, and the construction \nof protein interaction networks. These applications offer numerous advantages to proteomic research. \nDespite its growing prevalence in this field, deep learning faces several challenges including data \nscarcity, insufficient model interpretability, and computational complexity; these factors hinder its \nfurther advancement within proteomics. This paper comprehensively reviews the applications of deep \nlearning in proteomics along with the challenges it encounters. The aim is to provide a systematic \ntheoretical discussion and practical basis for research in this domain to facilitate ongoing development \nand innovation of deep learning technologies within proteomics. \nKeywords: Deep Learning; Proteomics Informatics; Sequence Analysis; Structure Prediction; \nFunctional Annotation; Interaction Networks \nI. INTRODUCTION \nDeep learning, a prominent subset of machine learning, processes and analyzes data by emulating \nthe neural network structures found in the human brain, utilizing multi-layered network architectures [1]. \nThe basic architecture of deep learning models consists of an input layer, one or more hidden layers, \nand an output layer [2]. Although deep learning has garnered substantial attention in recent years, its \nconceptual origins can be traced back to 1943, when neuroscientist Warren McCulloch and \nmathematician Walter Pitts introduced a model for artificial neural networks [3]. Deep learning did not \nexperience significant advancements until the early 21st century, primarily due to enhancements in \ncomputational power and the proliferation of large-scale data resources. Since then, this technology has \nbeen widely applied across diverse domains, including image recognition, natural language processing, \nautonomous driving, and bioinformatics [4]. These advancements have established a robust foundation \nfor accelerated progress in the application of deep learning within the field of proteomics.Deep learning \nmodels predominantly comprise several types, such as deep neural networks [5-6], convolutional neural \nnetworks [7], and recurrent neural networks [8-9]. These models are designed to extract and learn features \nfrom data via a multi-layered network architecture. Owing to the remarkable flexibility and adaptability \nof deep learning, it has been extensively applied across various domains, especially in tasks related to \nthe automatic recognition, classification, and prediction of complex data patterns, where it exhibits \nconsiderable advantages. \nIn the domain of proteomics [10], the burgeoning volume of proteomic data presents a substantial \nchallenge for scientific research in terms of efficient extraction, analysis, and interpretation. \nConventional data analysis methodologies are not only labor-intensive but also susceptible to \nsubjective biases. Moreover, they frequently fall short in elucidating the intricate patterns and \nassociations inherent within the data [11]. The advent of deep learning technologies, characterized by \ntheir robust data processing capabilities and automated feature extraction methods, has opened new \navenues for overcoming these bottlenecks, thereby substantially improving the efficiency of data \nprocessing and analysis. Deep learning is predominantly utilized in diverse areas, including protein \nsequence analysis, three-dimensional structure prediction, functional annotation, and interaction studies, \nand its applications further extend to genomics [12] and personalized medicine. For example, in the \nrealm of protein structure prediction, deep learning methodologies such as AlphaFold [13] have \nexhibited significant superiority over conventional techniques like homology modeling and \nconformational search methods by accurately forecasting protein folding structures. This advancement \noffers vital insights for drug design and pathological research. \nDespite the promising potential of deep learning in the analysis of biomedical data, it concurrently \nencounters a range of challenges and limitations. Specifically, in the context of multimodal data \nprocessing [14], various types of biological information—such as genomics, transcriptomics, and \nproteomics—are often characterized by diverse forms and structures. Analyzing a single data type may \nbe insufficient to fully capture the complexity of biological systems. Consequently, there is an urgent \nneed to develop more effective strategies for data integration. This article aims to offer a \ncomprehensive review of recent advancements in proteomics informatics through the lens of deep \nlearning, with a particular emphasis on its applications in this domain. Such advancements not only \nintroduce novel perspectives and tools for biomedical research but also propel the progress of \nmultimodal deep learning technologies, thereby creating new avenues for future medical research and \nclinical applications. While contemporary deep learning algorithms have yet to achieve an optimal state, \nthey are instrumental in the progression of bioinformatics and medical research. The structure of this \npaper is organized as follows: Section II provides an overview of the developmental background of \ndeep learning; Section III examines the current status and potential applications of deep learning in \nproteomics informatics; Section IV analyzes the challenges, limitations, and risks associated with deep \nlearning; and Section V explores future directions for deep learning in proteomics informatics; and \nSection VI provides a summary of the entire paper. This paper focuses on recent deep learning \nadvancements in proteomics amid rapid AI progress, serving as a reference and inspiration for \nresearchers, rather than an exhaustive survey. \n \nII. BACKGROUND OF DEEP LEARNING \nIn recent years, the exponential increase in data volume has underscored the advantages of deep \nneural networks, thereby significantly accelerating the advancement of deep learning technologies. \nThese networks are endowed with the ability to automatically extract features, a trait that is of \nconsiderable value in biological applications [15]. Numerous protein prediction methods using deep \nlearning have been created. This section analyzes six common deep neural network architectures used \nin deep learning, with Figure I comparing and summarizing various models. \nA. Deep Neural Network \nIn 2006, Hinton and colleagues [16] published a seminal paper in the journal Science that \nintroduced deep neural networks, heralding the advent of deep learning. Deep neural networks (DNNs) \nare advanced feedforward neural networks based on multilayer perceptrons, inspired by the human \nbrain's neural architecture. Unlike shallow networks, DNNs consist of an input layer, multiple hidden \nlayers, and an output layer, allowing them to process data through nonlinear transformations to learn \ncomplex features and extract valuable information. This method reduces reliance on manual feature \nengineering by adjusting weights to produce outputs. During DNN training, the backpropagation \nalgorithm is used to update network weights layer by layer, optimizing the model by minimizing the \ndifference between the loss function and actual outputs. DNNs excel in fields like computer vision, \nnatural language processing, and medical image analysis. In proteomics, they effectively extract \ncomplex features from amino acid sequences using multi-layer networks, enabling accurate predictions \nof protein interactions. \nB. Convolutional Neural Network \nThe introduction of the LeNet-5 convolutional neural network [17] represents the formal beginning \nof CNN architectures. Since the groundbreaking development of AlexNet [18] in 2012, CNNs are a key \npart of deep learning, designed to handle grid-like data, especially images. They are composed of \nconvolutional, pooling, and fully connected layers. The convolutional layer is tasked with extracting \nlocal features from the data [19] and conducting feature extraction, while the pooling layer downsamples \nfeature maps, preserving essential information. Unlike DNNs, CNNs reduce fully connected layers to \nlower parameter count and computational complexity. They offer three key benefits: local connectivity, \nweight sharing, and pooling. Local connectivity helps identify feature similarities, and weight sharing \ncuts down model parameters, further reducing complexity. Concurrently, pooling operations reduce \nfeature space dimensionality. CNNs are popular in computer vision for their robust representation \nlearning and resistance to transformations such as scaling, translation, and rotation. In biomedicine, \nthey excel in image registration and recognition. Early deep learning methods for protein function \nprediction also utilized CNN architectures. \nC. Recurrent Neural Network \nRecurrent Neural Networks (RNNs) are deep learning models designed for sequential data \nanalysis. They consist of an input layer, one or more hidden layers, and an output layer. Unlike DNNs \nand CNNs, RNNs have interconnected inputs and outputs, enabling them to use previous context \neffectively when processing current data. RNNs can efficiently capture dynamic features and \ndependencies in time series data, showcasing short-term memory retention. Typically, RNNs are trained \nusing backpropagation algorithms, particularly the Backpropagation Through Time (BPTT) technique \n[20], to update weight matrices. Nonetheless, conventional RNNs frequently face challenges such as \nvanishing or exploding gradients when learning from extended sequences, which can result in reduced \nefficacy in capturing long-term dependencies. To overcome these limitations, researchers have \nintroduced several variants of RNNs, notably Long Short-Term Memory (LSTM) networks [21] and \nGated Recurrent Units (GRUs) [22]. These variants use gating mechanisms to better manage information \nflow and improve handling of long-range dependencies. As a result, RNNs and their advanced versions \nare widely used in fields like natural language processing, computer vision, and computational biology, \nparticularly for identifying cases and predicting protein subcellular localization. \nD. Graph Neural Network \nThe inception of Graph Neural Networks (GNNs) can be traced to 2005, when Gori et al. [23] \ninitially introduced the concept. GNNs are neural networks designed for graph data, effectively \ncapturing node relationships and attributes. They are used in fields like natural language processing, \nimage processing, and drug development. Presently, prominent algorithms within the GNN domain \ninclude Graph Convolutional Networks (GCNs) [24] and Graph Attention Networks (GATs) [25]. \nTraditional neural networks struggle with irregular non-Euclidean data, while GCNs, a type of \nconvolutional neural network, excel in processing graph-structured data by aggregating features from \nneighboring nodes. This capability makes GCNs popular in fields like biological data analysis and \nknowledge graph construction. A key limitation of GCNs is their uniform weighting of neighboring \nnodes, which hinders capturing complex node relationships. To overcome this, GATs were developed, \nincorporating an attention mechanism to dynamically adjust node influence. This spatial approach \nenables neural networks to focus on important nodes and edges, improving training efficiency and \nmodel interpretability. Analyzing graph data helps us better understand complex relationships in \nunstructured data, which is crucial for studying residue interactions in protein structures. \nE. Generative Adversarial Network \nDeep learning research has traditionally been closely linked with discriminative models. \nNonetheless, in 2014, Ian Goodfellow and his colleagues [26] made a seminal contribution by proposing \nan unsupervised learning approach termed Generative Adversarial Networks (GANs). This framework \naims to generate data resembling real data through competitive and collaborative training. The GAN \narchitecture consists of two main parts: the generator, which creates realistic data from random noise, \nand the discriminator, which determines if the data is real or generated. These networks undergo \niterative training, constantly improving their parameters. This adversarial process helps the generator \nlearn progressively, producing high-quality outputs that test the discriminator's ability to assess \nauthenticity. Unlike traditional deep learning models, GANs excel at capturing real-world data patterns, \nallowing them to generate highly realistic synthetic data. However, the imbalance between the \ngenerator and discriminator in GANs can lead to training instability and gradient vanishing. \nNonetheless, GANs excel in image segmentation, style transfer, and data augmentation, and hold \npotential for predicting protein functions, particularly in analyzing sequences of proteins with unknown \nfunctions. \nF. Transformer \nThe attention mechanism, introduced by Ashish Vaswani and colleagues in 2017 [27], underpins the \nTransformer deep learning model. Inspired by human vision research, this mechanism efficiently \nallocates resources by assigning different weights to important information, improving data exchange \nand transmission. Self-attention, a specialized variant of this attention mechanism [28], is an essential \ncomponent of the Transformer architecture. Transformers can effectively use contextual information by \nadaptively focusing on different positions in sequences, unlike traditional RNNs. They excel at \ncapturing long-range dependencies, reducing gradient vanishing issues. A Transformer's core structure \nincludes an encoder and a decoder. The encoder is responsible for extracting features from input \nsequences and transforming them into continuous context vectors, whereas the decoder utilizes these \nvectors to generate output sequences. Additionally, Transformers use positional encoding to maintain \nand utilize positional information, improving computational efficiency. Currently, Transformer models \nare widely used in fields like natural language processing, computer vision, and protein-protein \ninteraction analysis. The previous text provides a comprehensive overview of six distinct types of deep \nneural networks, highlighting their historical development and practical applications. Each network is \ncharacterized by a unique architecture, making it suitable for specific use cases. Collectively, these \nnetworks have significantly contributed to the continuous advancement of deep learning technologies \nand have played a pivotal role in advancing the field of proteomics.\n \n \nFig. I: The various types of neural networks, encompassing Deep Neural Networks (DNN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), \nGraph Neural Networks (GNN), Generative Adversarial Networks (GAN), and Transformers, are extensively applied across multiple domains, including computer vision, \nnatural language processing, and biomedicine. \n \nIII. APPLICATIONS OF DEEP LEARNING IN PROTEOMICS INFORMATICS \n3.1 Protein Sequence Analysis \nProtein sequence analysis constitutes a central research domain within proteomics, encompassing \ndiverse facets such as the prediction, design, and classification of protein sequences (as depicted in \nFigure II). This field provides an essential foundation for comprehending biological systems. Such \nanalyses facilitate a more profound exploration of the functions and mechanisms of proteins within \nliving organisms. Protein sequence prediction entails the inference of the amino acid sequence from \ngiven genetic data. This process is essential for understanding protein structure and function, and it \nserves as a theoretical basis for drug design and the investigation of disease mechanisms. Recent \nresearch has achieved substantial progress in this domain. For example, the work of Repecka et al. [29] \nintroduced ProteinGAN, an innovative generative adversarial network that integrates self-attention \nmechanisms. This model is capable of discerning evolutionary relationships within protein sequences \nand generating a wide variety of novel variants exhibiting natural properties. Additionally, the ProGen \n[30] model represents a significant development, employing approximately 280 million annotated \nprotein sequences for unsupervised learning, which enables the generation of evolutionarily diverse \nsequences. While these models improve protein sequence prediction, they have limitations. The \ngenerated sequences, though diverse, may lack the complex nuances of real biological systems, \npotentially reducing their accuracy in predicting functionality due to missing context-specific \ninformation. Furthermore, the high computational demands for training large-scale models can restrict \ntheir accessibility and practical use in the research community. \nAs AI technology progresses, self-supervised deep language models have excelled in natural \nlanguage processing. Yet, applying these models to protein sequences fails to fully utilize proteins' \nunique properties. Recent efforts aim to bridge this gap. For instance, ProteinBERT, as developed by \nNadav Brandes et al. [31], represents a deep learning model specifically tailored for protein analysis. It \nexhibits the ability to adapt to diverse sequence lengths and efficiently handle exceedingly long protein \nsequences. This capability underscores its potential to address the inherent complexities of protein data, \nwhile also demonstrating promising performance across various benchmarks. Another notable \ncontribution is ProteinMPNN, developed by J. Dauparas et al. \n[32]. This model employs \nmessage-passing neural networks [33] to generate protein sequences with innovative structures and \nfunctions. The method achieves a sequence recovery rate of 52.4%, significantly surpassing Rosetta's \n[34] rate of 32.9%, thereby underscoring its superior efficacy in sequence design. Furthermore, \nGeoSeqBuilder, as developed by Jiale Liu et al. [35], constitutes an innovative deep learning framework \nthat synergistically combines protein sequence generation with side-chain conformation prediction. \nThis integration allows for complex all-atom structural designs, achieving a 51.6% native residue \nrecovery rate for side-chain prediction, with average pLDDT and TM-score values of 78.42 and 0.75 \nacross ten new protein structure datasets. These results match ProteinMPNN's performance and surpass \nother methods, though further research is necessary. These models show potential for generating \nprotein sequences with desired traits, but their biological relevance and functionality require further \nvalidation. There may be discrepancies between predicted and actual protein behaviors, and \nunderstanding these deep learning models is challenging. To enhance model performance and scientific \ninsight, it's essential to understand how these models make decisions and what influences specific \nsequences. \nWith the advancement of research on protein sequences, the constraints of conventional biological \nexperimental techniques in handling large-scale protein sequence classification have become more \napparent. Gu Xingyue [36] proposed a novel neural network methodology utilizing 188-dimensional \nfeature vectors to predict vesicular transport protein sequences. This approach demonstrated substantial \nprogress in the field of protein sequence prediction, achieving precision, accuracy, and recall rates of \n0.29, 0.71, and 0.86, respectively, on an independent dataset. Farzana Tasnim et al. [37] employed \nnatural language processing and encoding strategies for the classification of protein sequences. Their \nresults demonstrate that support vector machines [38], when implemented with count vectorization \ntechniques, can attain accuracy, precision, recall, and F1-Score values of 0.92. Furthermore, the robust \nperformance of convolutional neural networks utilizing various encoding methods underscores the \npotential of these techniques in the analysis of protein sequences. Umesh Kumar Lilhore et al. [39] \nintroduced the ProtICNN-BiLSTM model, which combines an advanced convolutional neural network \n[40] with attention mechanismsand bidirectional long short-term memory networks [41]. Utilizing \nBayesian optimization, this model demonstrated remarkable performance metrics: a specificity of \n94.65%, an accuracy of 96.57%, a sensitivity of 95.67%, and a Matthews correlation coefficient of \n96.85%. These outcomes are beneficial for the inference of novel protein functions. Despite \nadvancements, limitations remain. Gu Xingyue's method has a low precision rate of 0.29, limiting its \nuse where high precision is crucial. Other methods, though accurate, rely on complex encoding and \noptimization, complicating implementation and requiring significant computational resources and data \npreprocessing. The models' ability to generalize across diverse protein sequences and datasets needs \nmore study, as there's a risk of overfitting with varied protein families.  \n \n \nFig. II: The analysis of protein sequences can be divided into three principal components: protein \nsequence prediction, protein sequence design, and protein sequence classification. Each component \nemploys specific methodologies and tools, each with its own set of performance metrics. \n \n3.2 Protein Structure Prediction \nProtein structure prediction constitutes a pivotal domain of investigation within proteomics, with \nits efficacy commonly assessed through the Critical Assessment of Protein Structure Prediction (CASP) \n[42] and the Critical Assessment of Methods of Protein Structure Prediction (CAMPE) [43]. Detailed \ninformation is provided in Table I. Most contemporary deep learning models employ evolutionary data \nderived from multiple sequence alignments (MSA) [44] to enhance the precision of protein structure \nprediction. For instance, AlphaFold, created by Andrew W. Senior et al. [45], uses neural networks to \npredict inter-residue distances and angles, then optimizes a potential energy function with the L-BFGS \nalgorithm. It excels in accuracy even with limited homologous data, achieving a notable TM-score of \n0.7 in the CASP13 free modeling evaluation, marking a major advancement in protein structure \nprediction. Another significant example is trRosetta, a deep residual network introduced by Jianyi Yang \net al. [46]. trRosetta employs the Rosetta energy function to predict residue orientations and distances, \nfacilitating rapid and accurate structural modeling. It achieved TM-scores of 0.625 and 0.621 in \nCASP13 and CAMEO, surpassing many other methods. Additionally, trRosetta excels in de novo \ndesign and accurately predicts protein structures without co-evolutionary signals. Recent advancements \nin deep learning have significantly revolutionized protein structure prediction by improving both the \nspeed and accuracy of the process, thereby facilitating a deeper understanding of protein functions and \nthe design of novel proteins. However, these models face limitations, such as reduced accuracy for \ncomplex proteins and reliance on evolutionary data, which can be problematic when such data is \nunavailable or unreliable. \nCo-evolution of amino acids is crucial for estimating inter-residue distances, key for accurate \nprotein structure prediction. While traditional methods use indirect approaches, recent deep learning \nadvancements enable direct distance estimation from multiple sequence alignments. A significant \nadvancement is CopulaNet, a deep neural network by Fusong Ju et.al. [47], designed for accurate residue \ndistance predictions from sequence alignments, aiding protein structure prediction. ProFOLD, \nincorporating CopulaNet, outperformed AlphaFold in accuracy and TM-score on CASP13 targets, \nhighlighting the potential of new neural network models. The MSA Transformer, created by Roshan \nRao et.al. [48], integrates multiple sequence alignment with deep learning attention mechanisms to \nimprove protein structure prediction. It outperformed trRosetta in CASP13 and CAMEO benchmarks, \nshowcasing the power of advanced machine learning in this field. Recent research demonstrates that \nnovel neural network architectures and techniques are significantly improving predictive accuracy. \nSpecifically, CopulaNet advances residue distance predictions, while the MSA Transformer effectively \nintegrates multiple sequence alignment with deep learning methodologies, offering valuable insights \nfor future scholarly investigations. \nThrough continuous technological innovations and performance optimizations, the field of \npredictive modeling has witnessed substantial advancements, leading to enhanced accuracy and \nefficiency. On July 16, 2021, the unveiling of two pioneering models, AlphaFold2 and RoseTTAFold, \nrepresented a significant breakthrough in protein structure prediction. AlphaFold2, developed by \nDeepMind, successfully integrated principles from physics and bioinformatics to attain \nexperimental-level accuracy, outperforming other methodologies in the CASP14 competition. This \nnotable accomplishment was acknowledged through the conferment of the 2024 Nobel Prize in \nChemistry and is expected to catalyze further progress in drug design. The RoseTTAFold model, a \nthree-track neural network developed by Minkyung Baek et.al. [49], effectively tackled key challenges \nrelated to crystallography and cryo-electron microscopy by swiftly producing models of protein-protein \ncomplexes from sequence data. Despite ranking slightly below AlphaFold2 in the CASP14 competition, \nit nonetheless constituted a substantial advancement in the field of protein structure prediction. Recent \nadvancements in deep learning models, such as DMPfold2 [50], Fold [51], OpenFold [52], and \nPaddlePaddle [53], have significantly improved the efficiency and accessibility of protein structure \nprediction in comparison to AlphaFold2 and RoseTTAFold. These contemporary models demonstrate \nenhanced prediction speeds while sustaining high accuracy levels. The progress within deep learning \nfor protein structure prediction has been remarkable, with each successive model surpassing its \npredecessor. Nonetheless, substantial challenges remain, particularly in achieving accurate predictions \nfor complex proteins. \nThe protein structure prediction techniques previously examined predominantly rely on multiple \nsequence alignments. Nevertheless, these methodologies encounter several challenges, including their \ndependence on homologous sequences, substantial computational demands, and difficulties in \naddressing complex structural configurations. To tackle these challenges, Wenzhi Mao et al. [54] created \na rapid protein structure prediction pipeline by combining the AmoebaContact residue contact predictor \nwith the GDFold folding algorithm. GDFold performs exceptionally well on the PSICOV150 dataset \nand, although slightly less accurate than RaptorX-Contact on the CASP dataset, it is notably faster. \nRatul Chowdhury et al. [55] developed RGN2, a differentiable recurrent geometric network utilizing \nAminoBERT, for predicting protein structures from sequences. RGN2 surpassed AlphaFold2 and \nRoseTTAFold in GDT_TS and dRMSD metrics for orphan proteins and was also more computationally \nefficient and faster. Moreover, Ruidong Wu et al. [56] introduced OmegaFold, a method that accurately \npredicts high-resolution protein structures from a single sequence. It excels with orphan proteins and \nantibodies, matching MSA-based methods in CASP and CAMEO evaluations, and surpassing \nAlphaFold2 and RoseTTAFold with single-sequence inputs. Finally, Zeming Lin et al. [57] developed \nESMFold, a large language model that predicts atomic-level protein structures from primary sequences. \nIt operates six times faster than AlphaFold2 on an NVIDIA V100 GPU, facilitating extensive protein \nstructure analysis in metagenomes. These advancements reflect a major shift in protein structure \nprediction. Methods like GDFold, RGN2, OmegaFold, and ESMFold demonstrate the field's dynamism, \neach providing unique advantages such as faster processing, enhanced accuracy for specific proteins, or \nimproved computational efficiency. Combining different methods or creating hybrid models could \nfurther enhance accuracy and efficiency. To tackle the challenges posed by inadequate sample sizes and \nlimited diversity in existing open-source datasets, Sirui Liu et al. [58] developed the inaugural \nmillion-scale protein structure prediction dataset, known as PSP. This dataset secured the first position \nin the CAMEO competition, thereby offering robust validation of its efficacy. Proteins, being \nfundamental constituents of life, rely on their three-dimensional conformations for functional and \nmechanistic analyses. Consequently, sophisticated protein structure prediction methodologies should be \nemployed in drug design and biopharmaceuticals to illustrate their broader potential applications. \n3.3 Protein Function Prediction \nProtein function prediction represents a critical and intricate area of research. The incorporation of \ndeep learning methodologies has introduced novel insights into this domain, substantially improving \nthe precision of predictive outcomes. Contemporary predictive strategies predominantly encompass \nsequence-based approaches, structural analysis, protein-protein interaction networks, and the \nintegration of multi-source information, as demonstrated in Table II. \n1) Sequence-based protein function prediction: Through the analysis of protein amino acid \nsequences, researchers can infer potential biological functions. This is accomplished by employing \nmethodologies such as homology alignment, functional domain identification, and deep learning \ntechniques. PfmulDL [59] is a protein function annotation method using recurrent and multi-kernel \nconvolutional neural networks with transfer learning to enhance Gene Ontology predictions, especially \nfor \"rare class\" proteins. Another example is PANDA2 [60], which integrates graph neural networks with \nevolutionary language models to enhance predictions of protein functions in cellular components and \nbiological processes. Additionally, GOProFormer [61] is a cutting-edge multimodal Transformer that \ncombines protein sequences with the Gene Ontology (GO) hierarchy to enhance protein function \nprediction accuracy, outperforming previous methods and using new dataset protocols for better model \nevaluation. Similarly, TEMPROT and TEMPROT+ [62] are Transformer-based frameworks for protein \nfunction annotation that improve performance by incorporating BLASTp, particularly for long \nsequences and rare term predictions. Additionally, the SPROF-GO [63] method uses pre-trained \nlanguage models for efficient sequence embedding, enhanced by self-attention pooling, hierarchical le- \nTABLE I: Comparison of Deep Learning Models for Protein Structure Prediction \nModel Name \nNetwork Architecture \nTraining Dataset \nTesting Dataset \nEvaluation Indicators \nReferences \nAlphaFold \nCNN \nPDB、CATH \nCASP13 \nTM-score = 0.7 \n[45] \ntrRosetta \nDeep Residual \nConvolutional Networks \nPDB \nCASP13 \nTM-score = 0.625 \n[46] \nCAMEO \nTM-score = 0.621 \nProFOLD \nCopulaNet \nPDB、CATH \nCASP13-FM \nTM-score = 0.662 \n[47] \nCASP13-TBM \nTM-score = 0.784 \nMSA Transformer \nAttention mechanism \nMSA \nCASP13、CAMEO、CB513 \nAccuracy = 0.729 \n[48] \nAlphaFold2 \nEvoformer \nPDB、MSA \nCASP14 \nTM-score = 0.87 \n[13] \nCAMEO \nTM-score = 0.88 \nRoseTTAFold \nThree-track neural \nnetwork \nPDB、MSA \nCASP14 \nTM-score = 0.85 \n[49] \nCAMEO \nTM-score = 0.82 \nDMPfold2 \nGRU \nCATH \nCASP13 \nTM-score = 0.590 \n[50] \nColabFold \nMMseqs2 \nBFD、MGnify \nCASP14 \nTM-score = 0.887 \n[51] \npLDDT = 88.78 \nOpenFold \nAlphaFold2 \nCATH、OpenProteinSet \nCASP15 \nGDT-TS = 73.8 \n[52] \nCAMEO \nIDDT-Cα = 0.911 \nHelixFold \nFused Gated \nSelf-Attention \nPDB \nCASP14 \nTM-score = 0.877 \n[53] \nCAMEO \nTM-score = 0.888 \nGDFold \nAmoebaNet \nCATH \nPSICOV150、CASP11-13 \nTM-score = 0.789 \n[54] \nr.m.s.d. = 3.66 Å \nAminoBERT \nRGN2 \nUniParc、ProteinNet12、\nASTRAL SCOPe \nUniRef30、PDB70、MGnify \nGDT_TS = 86.5 \n[55] \nOmegaFold \nGeoformer \nUniRef50 、PDB、SCOP \nCASP \nTM-score = 0.79 \n[56] \nCAMEO \nLDDT = 0.82 \nESMFold \nESM-2 \nUniRef \nCASP14 \nTM-score = 0.68 \n[57] \nCAMEO \nTM-score = 0.83 \nMEGA-Fold \nPSP \nPDB、 UniRef50 \nCASP14、CAMEO \nTM-score = 0.8 \n[58] \n \narning, and label diffusion. It outperforms existing methods in accuracy, robustness, and generalization, \nespecially for non-homologous proteins and new species. The HiFun model by Jun Wu et al. [64] \ntranslates protein sequences into a \"protein language\" to predict functions for non-homologous proteins. \nIt surpasses existing methods on the CAFA3 benchmark, particularly for low-homology proteins, and \nhas successfully annotated many previously uncharacterized proteins in the UHGP database, \nunderscoring its practical importance. These advancements suggest a growing trend towards the \nadoption of sophisticated machine learning techniques for protein function prediction. This shift not \nonly enhances predictive accuracy but also broadens the scope of discovery in the fields of genomics \nand proteomics. \n2) Structure-based prediction of protein function: Understanding protein structures is crucial for \ndetermining their functions. Techniques like X-ray crystallography, NMR, and cryo-electron \nmicroscopy, along with computational methods like molecular docking, sequence alignment, and deep \nlearning, enhance the accuracy of functional predictions. For example, DeepFRI [65] is an advanced tool \nfor predicting protein functions, combining a pre-trained LSTM-LM with graph convolutional \nnetworks to accurately capture protein structures. It allows high-resolution functional localization via \ngrad-CAM and outperforms on benchmark datasets. Conversely, GAT-GO approach, developed by \nBoqiao Lai et al. [66], integrates residue contact maps with protein sequence embeddings, thereby \nenhancing the functional annotation of proteins with low sequence identity and facilitating large-scale \nannotation efforts. Similarly, EnsembleFam [67] utilizes sequence homology to enhance functional \npredictions for proteins with low similarity, demonstrating robustness in identifying functions within \nunknown protein families. Meanwhile, the TransFun [68] model employs pre-trained protein language \nmodels and 3D graph neural networks, surpassing current methods on the CAFA3 dataset, with \npotential for further improvement through sequence similarity integration. Peishun Jiao et al. [69] \npresented Struct2GO, a graph-based deep learning model demonstrating robust predictive capabilities \nwithin the MFO branch, achieving an Fmax of 0.701, AUC of 0.969, and AUPR of 0.796. The HEAL \n[70] model uses hierarchical graph transformers and contrastive learning to surpass current methods on \nthe PDB test set and AlphaFold2 structures, while improving interpretability through gradient-weighted \nclass activation mapping to pinpoint key functional residues. These methods highlight the evolving \napproach to predicting protein functions, each with distinct advantages and drawbacks. Some use \nadvanced neural networks for complex features, others combine various data sources or apply \nensemble techniques for better accuracy. A major challenge is managing the diverse and complex \nprotein structures and functions. \n3) Protein Function Prediction Based on PPI Networks: The structure of PPI networks and traits \nof central nodes provide key insights into biological processes. Analyzing these networks with deep \nlearning improves the identification and prediction of protein interactions, deepening our understanding \nof protein functions. The method known as NetQuilt, developed by Barot et al. [71], constructs \nmulti-species protein interaction networks utilizing homologous data to improve the prediction of \nprotein functions. This approach has demonstrated remarkable efficacy in datasets pertaining to \nhumans and mice. Additionally, NetGO 2.0 [72] enhances large-scale protein function prediction by \nintegrating textual annotations with deep sequence data, outperforming its predecessor in predicting \nbiological processes and cellular components, as shown in the CAFA4 challenge. The S2F [73] method \nenhances protein function prediction in newly sequenced organisms by transferring functional data and \nemploying a label propagation algorithm. It combines homology and protein features, considers \noverlapping communities in functional networks, and surpasses existing methods, particularly for \norganisms with experimental data. Moreover, the GLIDER [74] algorithm effectively performs across \ndifferent PPI networks, emphasizing the need for suitable local similarity measures and optimal \nk-values for better function prediction. Comparative analyses show variations in network prediction \nperformance. Sai Hu et al. [75] presented RWRT, a tensor-based double random walk model that \nimproves traditional techniques by using functional similarity tensors with multi-omics data. This \nmethod identifies more functionally similar partners, minimizes false negatives, and achieves higher \naccuracy than existing methods on the DIP and BioGRID datasets. Furthermore, the MELISSA [76] \nmethod improves GO label prediction by integrating functional labels into embeddings. In large-scale \nhuman and yeast multi-network tests, MELISSA outperformed the original Mashup and deepNF \nmethods in predicting protein functions. Overall, these studies have improved protein function \nprediction, but each method has its pros and cons. Combining data sources can introduce noise and \ncomplexity, and more research is needed to generalize these methods across various organisms and \nsystems. Additionally, enhancing the interpretability of deep learning models remains a priority. \n4) Multi-Source Information Fusion for Protein Function Prediction: Recently, the field of \nprotein function prediction has evolved from a dependence on single-source data to a more advanced \nmethodology that incorporates information from multiple sources. This paradigm shift encompasses a \ndiverse array of elements, including PPIs, protein domains, amino acid sequences, protein structures, \nand genomic data. Significant contributions to this field are exemplified by the PFP-GO method \ndeveloped by Kaustav Sengupta et al. [77], this method combines protein sequences, domain data, and \nPPI networks to prioritize GO terms, enhancing accuracy and precision metrics while filtering \nnon-essential proteins based on physicochemical properties. Another notable advancement is the \nProTranslator [78] framework, which facilitates the prediction of protein functions through the analysis \nof textual descriptions. This framework efficiently annotates new functions and features in zero-shot \nand few-shot scenarios, linking genes to biological pathways. Additionally, the DeepGOZero [79] model \ndemonstrates that integrating ontology embedding with neural networks improves protein function \nprediction, especially for minimally annotated proteins. By leveraging GO axioms to improve \nembeddings, it attains a 0.903 AUC on the Molecular Function Ontology, outperforming existing \nmethods and supporting zero-shot predictions, demonstrating its practical value. Moreover, Tong Pan et \nal. [80] effectively utilize attention mechanisms in deep learning for functional annotation with the \nPfresGO model. By combining protein sequences with gene ontology structures and removing multiple \nsequence alignments, PfresGO achieves high specificity for GO terms despite low-sequence identity. \nSimilarly, HnetGO [81] combines protein sequences and interaction data to link proteins with GO terms, \nusing pre-trained models for feature extraction. It performs well in cellular component and molecular \nfunction categories, highlighting the value of integrating sequence and interaction data for accurate \nprotein function predictions. Lastly, the DeepGATGO [82] model employs a hierarchical pre-training \ngraph attention mechanism, excelling in protein function prediction from input sequences. Its \nimpressive results on CAFA3 and TALE benchmarks demonstrate its scalability and effectiveness. In \nsummary, these advances mark a paradigm shift in protein function prediction, emphasizing the \nintegration of diverse data and innovative computational methods. As research progresses, these \nmultifaceted approaches improve accuracy and deepen insights into protein functionality, paving the \nway for breakthroughs in understanding biological processes. \n3.4 Prediction of Protein-Protein Interactions \nWith the progression of biomedical big data, PPIs have assumed a pivotal role in biological \nresearch. Historically, these interactions have been predicted through sequence alignment, structural \nprediction, and experimental techniques such as mass spectrometry. However, these conventional \nmethods are frequently characterized by their time-intensive nature, high costs, limited generalizability, \nTABLE II: Comparison Table of Deep Learning Models for Protein Function Prediction \nModel Name \nClassify \nMethod \nEvaluation Indicators \nRef \nSequence \nStructure \nPPI \nOther \nFmax \nSmin \nF1-score \nAUC \nAUPR \nAUROC \nAUPRC \nOther \nPfmulDL \n√ \n \n \n \nCNN、RNN \n√ \n \n \n√ \n \n \n√ \n \n[59] \nPANDA2 \n√ \n \n \n \nGNN、ESM \n√ \n√ \n \n \n√ \n \n \n \n[60] \nGOProFormer \n√ \n \n \n \nTransformer \n√ \n√ \n \n \n \n \n√ \n \n[61] \nTEMPROT \n√ \n \n \n \nTransformer \n√ \n√ \n \n \n \n \n√ \nIAuPR \n[62] \nSPROF-GO \n√ \n \n \n \nAttention \n√ \n \n \n \n√ \n \n \n \n[63] \nHiFun \n√ \n \n \n \nCNN \nAttention \nBiLSTM \n√ \n√ \n \n \n√ \n \n \n \n[64] \nDeepFRI \n \n√ \n \n \nGCN、LSTM \n√ \n \n \n \n√ \n \n \n \n[65] \nGAT-GO \n \n√ \n \n \nGAT \nCNN \nGNN \nAttention \n√ \n \n \n \n \n \n√ \n \n[66] \nEnsembleFam \n \n√ \n \n \nSVM \n \n \n \n√ \n \n \n \nROC \n[67] \nTransFun \n \n√ \n \n \nEGNN \n√ \n \n \n \n√ \n \n \n \n[68] \nStruct2GO \n \n√ \n \n \nGNN \nAttention \n√ \n \n \n√ \n√ \n \n \n \n[69] \nHEAL \n \n√ \n \n \nMPNN \nHGT \nAttention \n√ \n√ \n \n \n√ \n \n \n \n[70] \nNetQuilt \n \n \n√ \n \nMLP \nMaxout \n \n \n√ \n \n \n \n \nMAUPR \n[71] \nNetGO 2.0 \n \n \n√ \n \nLR-Text \nSeq-RNN \n√ \n \n \n \n√ \n \n \n \n[72] \nS2F \n \n \n√ \n \nInterPro \nHMMER \n√ \n√ \n \n \n \n \n \nAUC-ROC \nAUC-PR \n[73] \nGLIDER \n \n \n√ \n \nKNN \n \n \n√ \n \n \n \n \nACC \n[74] \nRWRT \n \n \n√ \n \nDouble \nRandom \nWalk \nAlgorithm \n \n \n√ \n \n \n√ \n \n \n[75] \nMELISSA \n \n \n√ \n \nML、CL \nSVD 、KNN \n \n \n√ \n \n \n \n√ \nACC \n[76] \nPFP-GO \n \n \n \n√ \nSequence \nComparison \nAlgorithms \nn-Star \nConsensus \nMethod \n \n \n√ \n \n \n \n \nPrecision \nRecall \n[77] \nProTranslator \n \n \n \n√ \nTransformer \n \n \n \n \n \n√ \n \nBLEU \n[78] \nDeepGOZero \n \n \n \n√ \nMLP \n√ \n√ \n \n√ \n√ \n \n \n \n[79] \nPfresGO \n \n \n \n√ \nAttention \n√ \n√ \n \n \n \n√ \n√ \n \n[80] \nHnetGO \n \n \n \n√ \nAttention \n√ \n \n \n√ \n√ \n \n \n \n[81] \nDeepGATGO \n \n \n \n√ \nGAT \n√ \n \n \n \n \n \n√ \n \n[82] \nand susceptibility to high false positive rates. Recent advancements in deep learning have introduced \nnovel methodologies for the prediction of PPIs, as illustrated in Table III. \n1) Based on Deep Neural Networks: The development of neural network models enhances the \nefficient acquisition of structural features and functional information of proteins from extensive \nbiological datasets. These models possess the ability to identify potential interacting protein pairs and \nelucidate intricate molecular mechanisms. Satyajit Mahapatra et al. [83] created a hybrid model using \ndeep neural networks and extreme gradient boosting to enhance PPI prediction accuracy, excelling in \nboth intra- and inter-species predictions. Additionally, the DWPPI [84] model integrates multi-source \ndata with deep neural networks to precisely predict plant PPIs, achieving high accuracy and AUC in \nthree plant datasets, providing valuable tools for plant molecular biology research. CT-DNN [85] is a PPI \nprediction method that uses joint trimer encoding and deep neural networks. It effectively processes \nlarge protein sequences, automatically extracts features, and enhances prediction accuracy and \ngeneralization. Collectively, these models have markedly advanced the prediction of PPIs. The \nincorporation of deep neural networks, in conjunction with other methodologies, has substantially \nenriched our comprehension of protein interactions. Nonetheless, despite these advancements, certain \nareas warrant further investigation. Although the integration of multi-source data proves effective, \nchallenges related to data quality and consistency may arise. \n2) Based on Convolutional Neural Networks: CNNs exhibit substantial effectiveness in pattern \nrecognition, facilitating the extraction of feature information from protein sequences and structures. \nThrough the utilization of multi-layered convolutional and pooling processes, these models adeptly \nidentify both local and global features of diverse proteins, thereby improving the accuracy of \ninteraction predictions. Firstly, Xiaotian Hu et al. [86] introduced DeepTrio, a PPI prediction model \nemploying parallel convolutional neural networks. It uses single-protein training with masking to \nachieve accurate predictions and highlight protein residue importance across various datasets. Secondly, \nHongli Gao et al. [87] developed EResCNN, an ensemble residual CNN for PPI prediction, integrating \ndiverse feature extraction methods. It demonstrates high accuracy on datasets from S. cerevisiae, H. \npylori, and Human-Yersinia pestis, and performs well in cross-species predictions and PPI network \nanalyses. Finally, Jun Hu et al. [88] created D-PPIsite, a deep learning model for predicting PPI sites, \ncombining convolutional, squeeze-and-excitation, and fully connected layers with four sequence-driven \nfeatures. It outperforms existing methods on five independent datasets. In summary, CNN-based \nmodels have markedly enhanced our comprehension and prediction of protein-protein interactions. The \ndistinctive architectures and methodologies employed by each model contribute unique strengths to the \nfield. As the intricacies of these interaction networks continue to be elucidated, it will be imperative to \naddress the challenges of scalability and interpretability associated with these models. \n3) Based on Recurrent Neural Networks and their variant, Long Short-Term Memory: RNNs \nplay a pivotal role in the analysis of protein data due to their capacity to handle sequential information, \nthereby enhancing our comprehension of protein behavior and interactions. The LSTM variant, which \nincorporates gating mechanisms to address the vanishing gradient problem, represents a significant \nadvancement in this field. This resulted in models like LSTMPHV [89], which combines LSTM with \nword2vec embeddings to effectively learn from imbalanced datasets using only amino acid sequences. \nIts high-precision predictions without biochemical data demonstrate the approach's effectiveness in \nextracting valuable information from raw sequences. Another notable model, RAPPPID [90] uses a \nmodified AWD-LSTM and various regularization methods for PPI prediction, demonstrating strong \nperformance on the C3 dataset. Its ability to function independently of specific proteins during training \nand testing underscores its versatility and adaptability for diverse biological applications. Lastly, \nSENSDeep [91], a sequence-based method for predicting PPI sites, effectively combines deep learning \nmodels with innovative features to improve predictive performance. While it may not consistently \noutperform structure-based methods across all datasets, its efficiency in providing reliable results \nmakes it ideal for quick and dependable predictions. In general, RNN-based models have greatly \nimproved protein research by efficiently processing sequential data and identifying key features, \nenhancing our understanding of protein interactions and PPI sites. Despite their success with amino \nacid sequences, incorporating data such as protein structures or post-translational modifications could \nprovide a more complete view. \n4) Based on Graph Neural Networks: GNNs represent proteins as nodes and their interactions as \nedges, allowing them to capture complex interaction features via information propagation. This \napproach effectively combines structural, functional, and sequential protein data, efficiently handling \nlarge biological datasets to enhance predictive accuracy and efficiency. Many research groups have \nmade significant progress in this area. Manon Reau et al. [92] created DeepRank-GNN, a graph neural \nnetwork framework designed to learn protein-protein interaction patterns. It offers a customizable \ninterface for feature selection, target values, and GNN architectures. The model performed \nexceptionally well in BM5 and CAPRI benchmarks for scoring docking interactions and distinguishing \nbiological from crystal interfaces. Albu et al. [93] introduced the MM-StackEns method for predicting \nPPIs, integrating sequence and graph data using Wasserstein distance for feature fusion, significantly \nenhancing prediction accuracy and generalization with novel protein pairs. Furthermore, Yuting Zhou \net al. [94] developed AGAT-PPIS, a PPI site prediction model utilizing an enhanced graph attention \nnetwork. It incorporates two node features, two edge features, and merges initial residuals with identity \nmapping, demonstrating strong robustness and generalization across all independent test sets. GNN \nmodels have significantly improved our ability to understand and predict protein-protein interactions, \nopening new avenues for exploring complex protein relationships. Despite their effectiveness in \nintegrating diverse protein data, optimizing graph construction could enhance results. Further insights \ninto information propagation within graphs are essential for advancing and validating these models. \n5) Based on Attention and Transformer: The self-attention mechanism is utilized to capture \npositional relationships within sequences. By leveraging parallel processing and hierarchical structures, \nthe Transformer architecture efficiently handles large-scale protein sequence data, resulting in the \ngeneration of high-quality feature representations. For instance, the HANPPIS [95] framework employs \na hierarchical attention mechanism with bidirectional gated recurrent units to predict amino acid-level \nprotein interaction sites, offering superior accuracy and interpretability. Similarly, the SDNN-PPI [96] \nmethod combines self-attention with deep neural networks, achieving high accuracy across various \ndatasets and PPI network predictions. Furthermore, EnsemPPIS [97] is an ensemble framework that uses \ntransformers and gated convolutional networks for predicting PPI sites. It excels in capturing global \nand local patterns and residue interactions, demonstrating strong performance across tasks. Its \ninterpretability analysis reveals its ability to learn residue interactions from protein sequences, \nenhancing predictive accuracy. Generally, self-attention methods have greatly improved protein \nresearch by enhancing our ability to predict interaction sites and understand amino acid relationships. \nDespite their accuracy, these models are computationally intensive with large protein datasets. \nIncorporating biological knowledge like post-translational modifications and protein folding could \nfurther boost predictions. \n6) Based on Autoencoders: Given the expanding scale of protein datasets, autoencoders employ \nunsupervised learning methodologies to distill critical features. This process facilitates the efficient \ncompression of protein sequence or structural data into low-dimensional representations, which \nencapsulate potential interaction patterns. The AutoPPI method, as developed by Gabriela Czibula et al. \n[98], exhibits remarkable efficacy in predicting protein-protein interactions by employing a deep \nautoencoder, thereby attaining high accuracy and AUC metrics across diverse datasets. In parallel, The \nDHL-PPI model by Yue Jiang et al. [99] uses deep learning to transform protein sequences into binary \nhash codes, enabling efficient protein-protein interaction prediction via Hamming distance. It achieves \nhigh precision, recall, and F1-score across datasets from four species, while reducing computational \ncomplexity. Meanwhile, the ProtInteract framework by Farzan Soleymani et al. [100] uses autoencoders \nfor protein simplification and deep CNNs for PPI prediction. Experiments reveal that TCNs surpass \nLSTMs in both accuracy and speed, making them preferable for large-scale analyses due to reduced \ncomputational requirements. Autoencoder-based methods have significantly advanced protein research \nby enabling efficient analysis of large datasets and improving protein-protein interaction predictions. \nHowever, challenges remain, such as potential loss of detailed information when compressing data into \nlow-dimensional representations. Future studies should aim to balance data compression with the \npreservation of essential details. \n \nIV. CHALLENGES, LIMITATIONS, AND RISKS \nDespite significant advancements in the application of deep learning technologies in the field of \nproteomics, numerous challenges remain. \n1) Data: Deep learning models require large, high-quality datasets for effective training, but \nproteomics research often struggles with missing values and noise due to experimental methods and \nbiological sample variability, hindering model performance. Furthermore, the low expression levels of \nkey proteins within samples contribute to data imbalance issues [101]. These factors underscore the need \nfor data cleaning, preprocessing, augmentation, and regularization to accurately reflect biological \nphenomena. \n2) Calculation: In the domain of proteomics, the deployment of deep learning models generally \nnecessitates significant computational resources, especially when processing large-scale and high- \ndimensional biological datasets [102]. Traditional hardware often struggles to efficiently train models, \nlimiting their use. Integrating cloud and high-performance computing, along with improved model \ncompression and acceleration algorithms, is expected to boost training efficiency and reduce time. \n3) Interpretability: The intricate hidden layers and \"black box\" characteristics of deep learning \nTABLE III: Comparison Table of Deep Learning Models for Protein-Protein Interaction Prediction \nClassify \nModel \nName \nDataset \nEvaluation Indicators (%) \nRef \nACC \nSens \nSpec \nPrec \nMCC \nAUC \nF1-score \nAUPR \nAUROC \nRecall \nAUPRC \nOther \nDNN \nDNN-XGB \nSaccharomyces \ncerevisiae \n98.35 \n97.78 \n98.93 \n98.91 \n96.72 \n99.70 \n/ \n/ \n/ \n/ \n/ \n/ \n[83] \nHelicobacter pylori \n96.19 \n96.71 \n95.68 \n95.76 \n92.41 \n98.60 \nHuman-Bacillus \nAnthracis \n98.50 \n98.67 \n98.31 \n98.33 \n97.00 \n99.87 \nHuman-Yersinia pestis \n97.25 \n97.68 \n97.68 \n96.86 \n94.51 \n99.29 \nDWPPI \nArabidopsis thaliana \n89.47 \n91.47 \n87.48 \n87.97 \n79.02 \n95.48 \n/ \n/ \n/ \n/ \n/ \n/ \n[84] \nZea mays \n95.00 \n96.30 \n93.69 \n93.85 \n90.02 \n98.67 \nOryza sativa \n85.63 \n86.38 \n84.89 \n85.11 \n71.28 \n92.13 \nCT-DNN \nHPRD \n/ \n/ \n/ \n/ \n/ \n98.3 \n/ \n98.2 \n/ \n/ \n/ \n/ \n[85] \nSwiss-Prot \nCNN \nDeepTrio \nBioGRID S.cerevisiae \n97.55 \n96.12 \n98.98 \n98.95 \n95.15 \n/ \n97.52 \n/ \n/ \n/ \n/ \n/ \n[86] \nBioGRID H.sapiens \n98.12 \n97.23 \n99.01 \n99.00 \n96.26 \n98.11 \nDeepFE-PPI \nS.cerevisiae \n92.57 \n88.53 \n96.62 \n96.33 \n92.26 \n85.43 \nPIPR S.cerevisiae \n94.78 \n92.20 \n97.33 \n97.18 \n94.63 \n89.67 \nEResCNN \nS. cerevisiae \n95.34 \n/ \n/ \n98.32 \n90.86 \n/ \n/ \n/ \n/ \n92.26 \n/ \n/ \n[87] \nH. pylori \n87.89 \n87.84 \n75.81 \n87.96 \nHuman-Y. pestis \n98.61 \n97.56 \n97.23 \n98.65 \nD-PPIsite \nDset_186 \n80.9 \n37.3 \n88.7 \n37.3 \n26.0 \n73.2 \n37.3 \n35.7 \n/ \n/ \n/ \n/ \n[88] \nDset_72 \n85.1 \n29.9 \n91.7 \n29.9 \n21.6 \n74.0 \n29.9 \n25.4 \nDset_164 \n77.8 \n38.6 \n86.4 \n38.6 \n25.0 \n71.0 \n38.6 \n36.4 \nDset_448 \n85.9 \n48.1 \n91.9 \n48.0 \n39.9 \n82.4 \n48.0 \n47.9 \nDset_355 \n87.1 \n46.0 \n92.7 \n46.0 \n38.7 \n82.2 \n46.0 \n44.8 \nRNN \n& \nLSTM \nLSTM-PHV \nTraining Dataset \n98.4 \n/ \n/ \n/ \n/ \n97.6 \n/ \n/ \n/ \n/ \n/ \n/ \n[89] \nIndependent Dataset \n98.5 \n97.3 \nSARS-CoV-2 PPIs \n/ \n95.6 \nNon-viral pathogens \nPPIs \n92.2 \nRAPPPID \nSTRING \nC1 \n/ \n/ \n/ \n/ \n/ \n97.8 \n/ \n97.4 \n97.8 \n/ \n/ \n/ \n[90] \nC2 \n85.9 \n86.8 \n85.9 \nNegatome \nC3 \n80.3 \n81.0 \n80.3 \nSENSDeep \nDset_186 \n80.7 \n38.8 \n88.3 \n37.6 \n26.8 \n/ \n38.2 \n/ \n72.5 \n/ \n35.0 \n/ \n[91] \nDset_72 \n80.8 \n40.4 \n86.1 \n27.4 \n22.6 \n32.7 \n71.5 \n26.5 \nDset_164 \n78.9 \n30.9 \n89.2 \n38.0 \n21.8 \n34.1 \n68.6 \n33.9 \nDset_448 \n83.2 \n29.8 \n91.7 \n36.6 \n23.5 \n32.8 \n68.1 \n31.0 \nDset_355 \n84.8 \n30.7 \n92.2 \n34.9 \n24.2 \n32.6 \n69.0 \n29.7 \n \n \n \n \n \n \n \n \n \n \nDeepRank- \nGNN \nBM5 \n/ \n/ \n/ \n/ \n/ \n85 \n/ \n/ \n/ \n/ \n/ \nSuccess \nrates = \n93.3 \n[92] \nCAPRI \n71 \nSuccess \nrates = \n76.9 \nDC \n82 \n83 \n81 \n82 \n/ \n/ \n \n \n \nYeast \nC1 \n81 \n \n \n \n \n \n \n78 \n \n \n \n \n \n \n \n \n \n92 \n91 \n89 \n \n \n \n \n \n \n \n \n \nC2 \n69 \n69 \n77 \n76 \n71 \nC3 \n64 \n71 \n71 \n69 \n47 \n \n \n \n \nGNN \n \n \n \n \nMM- \nStackEns \nHuman \nC1 \n78 \n \n \n \n \n \n/ \n \n \n \n \n \n/ \n72 \n \n \n \n \n \n/ \n \n \n \n \n \n/ \n \n \n \n \n \n/ \n88 \n88 \n89 \n \n \n \n \n \n/ \n \n \n \n \n \n/ \n \n \n \n \n \n[93] \nC2 \n64 \n61 \n70 \n71 \n75 \nC3 \n62 \n66 \n68 \n68 \n50 \nHuman- \n2021 \n50% \nRandom \n85 \n80 \n92 \n92 \n92 \n10% \nRandom \n89 \n46 \n70.5 \n91 \n80 \n0.3% \nRandom \n90 \n30 \n13.6 \n91 \n81 \nYeast-2017 \n94.0 \n94.5 \n98.3 \n98.1 \n93.3 \nMulti- \nspecies \n<40% \n57.5 \n57.2 \n61.3 \n59.0 \n85.3 \n<25% \n58.5 \n58.5 \n62.8 \n59.6 \n85.0 \n<10% \n59.2 \n59.7 \n65.1 \n60.5 \n84.6 \n<1% \n59.3 \n60.2 \n65.6 \n60.2 \n85.0 \nAGAT-PPIS \nTest_60 \n85.6 \n/ \n/ \n53.9 \n48.4 \n/ \n56.9 \n/ \n86.7 \n60.3 \n57.4 \n/ \n[94] \nTest_315-28 \n/ \n/ \n48.1 \n/ \n/ \n/ \n57.2 \nBtest_31-6 \n48.5 \n58.3 \nUBtest_31-6 \n32.7 \n36.5 \nAttention  \n& \nTransformer \nHANPPIS \nDset_186 \n63.1 \n/ \n/ \n29.1 \n/ \n/ \n39.3 \n/ \n/ \n60.5 \n/ \n/ \n[95] \nDset_72 \nDset_164 \nSDNN-PPI \nS.cerevisiae \n95.48 \n93.80 \n97.23 \n97.13 \n91.02 \n98.63 \n/ \n/ \n/ \n/ \n/ \n/ \n[96] \nHuman \n98.94 \n98.77 \n99.10 \n99.02 \n97.57 \n99.60 \nHuman- \nB.Anthracis \n93.15 \n96.61 \n89.69 \n90.44 \n86.57 \n98.23 \nHuman-Y.pestis \n88.33 \n93.92 \n82.74 \n84.63 \n77.26 \n95.74 \nEnsemPPIS \nDeepPPISP task \n73.2 \n/ \n/ \n37.5 \n27.7 \n/ \n44.0 \n/ \n71.9 \n53.2 \n40.5 \n/ \n[97] \nDELPHI task \n82.1 \n/ \n29.1 \n38.5 \n77.0 \n/ \n35.4 \nAE \nAutoPPI \nHPRD \nJoint–\nJoint \n97.7 \n/ \n98.6 \n98.6 \n/ \n97.7 \n97.7 \n/ \n/ \n96.8 \n/ \n/ \n[98] \nSiamese–\nJoint \n97.9 \n97.3 \n97.3 \n97.9 \n97.9 \n98.5 \nSiamese–\nSiamese \n96 \n99.2 \n99.2 \n96.0 \n95.9 \n92.8 \nMulti- \nspecies \nJoint–\nJoint \n97 \n99.5 \n99.5 \n97 \n96.9 \n94.4 \nSiamese–\nJoint \n96.9 \n96.4 \n96.5 \n97 \n97 \n97.4 \nSiamese–\nSiamese \n98.2 \n100 \n100 \n98.2 \n98.2 \n96.4 \nMulti- \nspecies \n < 0.25 \nJoint–\nJoint \n97.3 \n99.5 \n99.5 \n97.5 \n97.5 \n95.6 \nSiamese–\nJoint \n97.6 \n96.8 \n97.4 \n97.5 \n97.8 \n98.3 \nSiamese–\nSiamese \n98.3 \n100 \n100 \n98.5 \n98.4 \n96.9 \nMulti- \nspecies  \n< 0.01 \nJoint–\nJoint \n97.2 \n99.1 \n99.3 \n97.5 \n97.5 \n95.8 \nSiamese–\nJoint \n97.8 \n96.6 \n97.5 \n97.6 \n98.1 \n98.7 \nSiamese–\n98.1 \n100 \n100 \n98.3 \n98.3 \n96.6 \nSiamese \nDHL-PPI \nC. elegans \n98.8 \n/ \n100 \n100 \n97.5 \n/ \n99.0 \n/ \n/ \n98.1 \n/ \n/ \n[99] \nDrosophila \n98.8 \n99.7 \n99.8 \n97.6 \n99.0 \n98.1 \nE. coli \n97.1 \n98.2 \n98.7 \n94.0 \n97.5 \n96.2 \nHuman \n97.1 \n98.0 \n98.4 \n94.1 \n97.3 \n96.3 \nProtInteract \nH.  \nsapiens \n2- classes \n95.68 \n/ \n/ \n95.50 \n/ \n96.00 \n/ \n/ \n/ \n/ \n/ \n/ \n[100] \n3- classes \n92.44 \n89.50 \n93.40 \n5- classes \n91.32 \n79.20 \n90.60 \nM. \nmusculus \n2- classes \n91.45 \n84.50 \n86.00 \n3- classes \n89.83 \n78.70 \n86.00 \n5- classes \n87.49 \n70.00 \n84.80 \nmodels significantly complicate the interpretability of their decision-making processes and outcomes \n[103]. In medicine and biological research, interpretability is crucial for understanding a model's \nconclusions, ensuring safe and effective use. Researchers are exploring strategies to enhance \ninterpretability, such as developing explanatory tools, visualization techniques, and knowledge-based \nmethods. These efforts aim to improve deep learning model transparency, fostering better \nunderstanding and use. \n4) Transparency: The transparency of deep learning models is crucial, especially in biomedical \napplications, due to the sensitivity of medical data [104] which often prevents open-sourcing. Enhancing \nmodel transparency can help clinicians understand decision-making processes and build trust in the \nresults. \n5) Ethics: Proteomics research uses data from patient samples, making privacy protection and \nethics crucial due to risks of genetic information exposure and discrimination. Future research should \nfocus on creating secure data processing and storage technologies to protect privacy and ensure security, \nwhile adhering to ethical guidelines for legal and moral integrity. \n \nV. FUTURE DIRECTIONS \nIn the domain of proteomics informatics leveraging deep learning, prospective research \ntrajectories of significant interest encompass multimodal data fusion, self-supervised learning [105], and \ntransfer learning [106]. Additionally, fostering interdisciplinary collaboration and the integration of \ndiverse knowledge bases are crucial areas for future exploration.  \nIntegrating multimodal data is expected to become crucial in proteomics research, enhancing \ntraditional proteomic data from techniques like mass spectrometry and NMR with genomics, \ntranscriptomics, and metabolomics. Deep learning technologies effectively integrate diverse data \nsources, uncovering key insights into protein expression, function, and interactions. This progress \nenhances biomarker identification and deepens our understanding of disease mechanisms. \nImplementing self-supervised and transfer learning is expected to significantly boost model \neffectiveness and applicability. Self-supervised learning leverages unlabeled data for feature \nrepresentation, performing well in data-scarce situations. Meanwhile, transfer learning applies existing \nknowledge by using model weights from other biological tasks in proteomics, speeding up training and \noptimization. \nInterdisciplinary collaboration and diverse knowledge integration are key to advancing proteomic \ninformatics. By uniting experts in biology, computer science, physics, and statistics, a more \ncomprehensive approach can be developed to understand proteins' complex roles in biological \nprocesses. Significant progress in the speed of proteomics research and its future applications can be \nmade by encouraging interdisciplinary networks that support data sharing and collaborative knowledge \ncreation, along with the integration of advanced technologies and theoretical models. \n \nVI. CONCLUSIONS \nThis research reviews recent advancements in proteomics informatics using deep learning, \nhighlighting the effectiveness of various algorithms in predicting protein sequences, structures, \nfunctions, and interactions. These technologies have significantly enhanced the predictive accuracy of \nproteomic research and laid a strong foundation for future studies. Despite these advancements, \nchallenges remain. Future research should focus on improving the transparency and interpretability of \ndeep learning models to further our understanding of proteomics informatics. \n \nACKNOWLEDGEMENTS \nThis work is supported by the Natural Science Foundation of Fujian Province (2023J05083, \n2022J011396, 2023J011434). \n \nREFERENCES \n[1] Yanzhong Wen, Yuexia Han, Jianfei Sun. AI-empowered Biomedical Research. Science. 2024. \n[2] Sarker I H. Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications \nand Research Directions. SN Computer Science. 2021. \n[3] Talaei Khoei Tala, Ould Slimane Hadjar, Kaabouch Naima. Deep learning: systematic review, \nmodels, challenges, and research directions. Neural Computing and Applications. 2023. \n[4] Xinyu Guo. A Brief Discussion on the Development Context and Current Status of Deep Learning. \nJournal of Hubei University of Economics. 2024. \n[5] Sze V, Chen Y H, Yang T J, et al. Efficient Processing of Deep Neural Networks: A Tutorial and \nSurvey. Proceedings of the IEEE. 2017. \n[6] Mahmud S, Soltanikazemi E, Boadu F, et al. Deep Learning Prediction of Severe Health Risks for \nPediatric COVID-19 Patients with a Large Feature Set in 2021 BARDA Data Challenge. ArXiv. 2022. \n[7] O'Shea K, Nash R. An Introduction to Convolutional Neural Networks. Computer Science. 2015. \n[8] Medsker L R, Jain L C. Recurrent Neural Networks: Design and Applications. 1999.  \n[9] Schuster M, Paliwal K K. Bidirectional recurrent neural networks. IEEE Transactions on Signal \nProcessing. 1997. \n[10] Wen B, Zeng W F, Liao Y, et al. Deep Learning in Proteomics. Proteomics. 2020. \n[11] Yue T, Wang H. Deep Learning for Genomics: A Concise Overview. 2018. \n[12] Qiong WU, Xintong SUI, Ruijun TIAN. Advances in High-Throughput Proteomics Analysis. \nChromatography. 2022. \n[13] Jumper J, Evans R, Pritzel A, et al. Highly accurate protein structure prediction with AlphaFold. \nNature. 2021. \n[14] Huang J, Zhang J. A Survey on Evaluation of Multimodal Large Language Models. 2024. \n[15] Tang B, Pan Z, Yin K, et al. Recent Advances of Deep Learning in Bioinformatics and \nComputational Biology. Frontiers in Genetics. 2019. \n[16] Hinton GE, Salakhutdinov RR. Reducing the Dimensionality of Data with Neural Networks. \nScience. 2006. \n[17] Lecun Y, Bottou L. Gradient-based learning applied to document recognition. Proceedings of the \nIEEE. 1998. \n[18] Krizhevsky A, Sutskever I, Hinton G. ImageNet Classification with Deep Convolutional Neural \nNetworks. Advances in neural information processing systems. 2012. \n[19] Talukdar S, Singha P, Mahato S, et al. Land-Use Land-Cover Classification by Machine Learning \nClassifiers for Satellite Observations—A Review. Remote Sensing. 2020. \n[20] Bird G, Polivoda M E. Backpropagation through time for networks with long-term dependencies. \n2021. \n[21] Hochreiter S, Schmidhuber J. Long Short-Term Memory. Neural Computation. 1997. \n[22] Cho K, Van Merrienboer B, Gulcehre C, et al. Learning Phrase Representations using RNN \nEncoder-Decoder for Statistical Machine Translation. Computer Science. 2014. \n[23] Gori M, Monfardini G, Scarselli F. A new model for learning in graph domains. IEEE International \nJoint Conference on Neural Networks. 2005. \n[24] Kipf T N, Welling M. Semi-Supervised Classification with Graph Convolutional Networks. 2016. \n[25] Veličković P, Cucurull G, Casanova A, et al. Graph Attention Networks. 2017. \n[26] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative Adversarial Nets. MIT Press. 2014. \n[27] Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need. ArXiv. 2017. \n[28] Liu G, Guo J. Bidirectional LSTM with attention mechanism and convolutional layer for text \nclassification. Neurocomputing. 2019. \n[29] Repecka D, Jauniskis V, Karpus L, et al. Expanding functional protein sequence space using \ngenerative adversarial networks. Nature Machine Intelligence. 2019. \n[30] Madani A, Mccann B, Naik N, et al. ProGen: Language Modeling for Protein Generation. Cold \nSpring Harbor Laboratory. 2020. \n[31] Brandes N, Ofer D, Peleg Y, et al. ProteinBERT: a universal deep-learning model of protein \nsequence and function. Bioinformatics. 2022. \n[32] Dauparas J, Anishchenko I, Bennett N, et al. Robust deep learning–based protein sequence design \nusing ProteinMPNN. Science. 2022. \n[33] Gilmer J, Schoenholz SS, Riley PF, et al. Neural Message Passing for Quantum Chemistry. 2017. \n[34] Bennett NR, Coventry B, Goreshnik I, et al. Improving de novo protein binder design with deep \nlearning. Nature communications vol. 2023. \n[35] Liu J, Guo Z, You H, et al. All-Atom Protein Sequence Design Based on Geometric Deep \nLearning. Angewandte Chemie International Edition. 2024. \n[36] Xingyue Gu. Research on Protein Sequence Classification Based on Deep Learning. Guangzhou \nUniversity. 2021. \n[37] Farzana Tasnim, Sultana Umme Habiba, Tanjim Mahmud, et al. Protein Sequence Classification \nThrough Deep Learning and Encoding Strategies. Procedia Computer Science. 2024. \n[38] Hearst M A, Dumais S T, Osman E, et al. Support vector machines. IEEE Intelligent Systems & \nTheir Applications. 1998. \n[39] Lilhore U K, Simiaya S, Alhussein M, et al. Optimizing protein sequence classification: \nintegrating deep learning models with Bayesian optimization for enhanced biological analysis. BMC \nMedical Informatics and Decision Making. 2024. \n[40] Amos B, Xu L, Kolter J Z. Input Convex Neural Networks. 2016. \n[41] Li L, Nie Y, Han W, et al. A Multi-Attention-Based Bidirectional Long Short-Term Memory \nNetwork for Relation Extraction. Interational Conference on Neural information Processing. 2017. \n[42] Yawu Zhao, Yihui Liu. Prediction of Protein Secondary Structure Based on Optimized \nConvolutional Neural Networks. Computer Applications and Software. 2021. \n[43] Jürgen Haas, Barbato A, Behringer D, et al. Continuous Automated Model Evaluation (CAMEO) \nComplementing the Critical Assessment of Structure Prediction in CASP12. Proteins: Structure, \nFunction, and Bioinformatics. 2018. \n[44] Edgar R C, Batzoglou S. Multiple sequence alignments. Current Opinion in Structural Biology. \n2006. \n[45] Senior A W, Evans R, Jumper J, et al. Improved protein structure prediction using potentials from \ndeep learning. Nature. 2020. \n[46] Yang J, Anishchenko I, Park H, et al. Improved protein structure prediction using predicted \ninterresidue orientations. Proceedings of the National Academy of Sciences. 2020. \n[47] Ju F, Zhu J, Shao B, et al. CopulaNet: Learning residue co-evolution directly from multiple \nsequence alignment for protein structure prediction. Nature Communications. 2021. \n[48] Rao R M, Liu J, Verkuil R, et al. MSA transformer. 2021. \n[49] Baek M, Dimaio F, Anishchenko I, et al. Accurate prediction of protein structures and interactions \nusing a three-track neural network. Science. 2021. \n[50] Kandathil S M, Greener J G, Lau A M, et al. Ultrafast end-to-end protein structure prediction \nenables high-throughput exploration of uncharacterised proteins. Cold Spring Harbor Laboratory. 2021. \n[51] Mirdita M, Schütze K, Moriwaki Y, et al. ColabFold: making protein folding accessible to all. \nNature Methods. 2022. \n[52] Ahdritz G, Bouatta N, Floristean C, et al. OpenFold: retraining AlphaFold2 yields new insights \ninto its learning mechanisms and capacity for generalization. Nature Methods. 2024. \n[53] Guoxia Wang, Xiaomin Fang, Zhihua Wu, et al. HelixFold: An Efficient Implementation of \nAlphaFold2 using PaddlePaddle. ArXiv. 2022. \n[54] Mao W, Ding W, Xing Y, et al. AmoebaContact and GDFold as a pipeline for rapid de novo \nprotein structure prediction. Nature Machine Intelligence. 2020. \n[55] Chowdhury R, Bouatta N, Biswas S, et al. Single-sequence protein structure prediction using a \nlanguage model and deep learning. Nature biotechnology. 2022. \n[56] Ruidong Wu, Fan Ding, Rui Wang, et al. High-resolution de novo structure prediction from \nprimary sequence. bioRxiv - Bioinformatics. 2022. \n[57] Lin Z, Akin H, Rives A, et al. Evolutionary-scale prediction of atomic-level protein structure with \na language model. Science. 2023. \n[58] Sirui Liu, Jun Zhang, Haotian Chu, et al. PSP: Million-level Protein Sequence Dataset for Protein \nStructure Prediction. arXiv - QuanBio - Biomolecules. 2022. \n[59] Weiqi Xia, Lingyan Zheng, Jiebin Fang, et al. PFmulDL: a novel strategy enabling multi-class and \nmulti-label protein function annotation by integrating diverse deep learning methods. Computers in \nBiology and Medicine. 2022. \n[60] Chenguang Zhao, Tong Liu, Zheng Wang. PANDA2: protein function prediction using graph \nneural networks. NAR Genomics and Bioinformatics. 2022. \n[61] Kabir A, Shehu A. GOProFormer: A Multi-Modal Transformer Method for Gene Ontology Protein \nFunction Prediction. Biomolecules. 2022. \n[62] Oliveira G B, Pedrini H, Dias Z. TEMPROT: protein function annotation using transformers \nembeddings and homology search. BMC Bioinformatics. 2023. \n[63] Qianmu Yuan, Junjie Xie, Jiancong Xie, et al. Fast and accurate protein function prediction from \nsequence through pretrained language model and homology-based label diffusion. Brief Bioinform. \n2023. \n[64] Jun Wu, Haipeng Qing, Jian Ouyang, et al. HiFun: homology independent protein function \nprediction by a novel protein-language self-attention model. Brief Bioinform. 2023. \n[65] Gligorijević V, Renfrew PD, Kosciolek T, et al. Structure-based protein function prediction using \ngraph convolutional networks. Nature communications. 2021. \n[66] Lai B, Xu J. Accurate protein function prediction via graph attention networks with predicted \nstructure information. Briefings in Bioinformatics. 2021. \n[67] Kabir MN, Wong L. EnsembleFam: towards more accurate protein family prediction in the \ntwilight zone. BMC Bioinformatics. 2022. \n[68] Boadu F, Cao H, Cheng J. Combining protein sequences and structures with transformers and \nequivariant graph neural networks to predict protein function. Bioinformatics. 2023. \n[69] Peishun Jiao, Beibei Wang, Xuan Wang, et al. Struct2GO: protein function prediction based on \ngraph pooling algorithm and AlphaFold2 structure information. Bioinformatics. 2023. \n[70] Zhonghui Gu, Xiao Luo, Jiaxiao Chen, et al. Hierarchical graph transformer with contrastive \nlearning for protein function prediction. Bioinformatics. 2023. \n[71] Barot M, Gligorijevic V, Cho K, et al. NetQuilt: Deep Multispecies Network-based Protein \nFunction Prediction using Homology-informed Network Similarity. Bioinformatics. 2021. \n[72] Yao S, You R, Wang S, et al. NetGO 2.0: improving large-scale protein function prediction with \nmassive sequence, text, domain, family and network information. Nucleic Acids Research. 2021. \n[73] Torres M, Yang H, Romero A E, et al. Protein function prediction for newly sequenced organisms. \nNature Machine Intelligence. 2021. \n[74] Kapil D, Henri S, Matt W, et al. GLIDER: function prediction from GLIDE-based neighborhoods. \nBioinformatics. 2022. \n[75] Sai Hu, Zhihong Zhang, Huijun Xiong, et al. A tensor-based bi-random walks model for protein \nfunction prediction. BMC Bioinformatics. 2022. \n[76] Kaiyi Wu, Di Zhou, Slonim Donna, et al. MELISSA: Semi-Supervised Embedding for Protein \nFunction Prediction Across Multiple Networks. bioRxiv - Bioinformatics. 2024. \n[77] Sengupta K, Saha S, Halder AK, et al. PFP-GO: Integrating protein sequence, domain and \nprotein-protein interaction information for protein function prediction using ranked GO terms. \nFrontiers in Genetics. 2022. \n[78] Hanwen Xu, Sheng Wang. ProTranslator: zero-shot protein function prediction using textual \ndescription. arXiv - QuanBio - Quantitative Methods. 2022. \n[79] Kulmanov Maxat, Hoehndorf Robert. DeepGOZero: improving protein function prediction from \nsequence and zero-shot learning based on ontology axioms. Bioinformatics. 2022. \n[80] Tong Pan, Chen Li, Yue Bi, et al. PFresGO: an attention mechanism-based deep-learning approach \nfor protein annotation by integrating gene ontology inter-relationships. Bioinformatics. 2023. \n[81] Xiaoshuai Zhang, Huannan Guo, Fan Zhang, et al. HNetGO: protein function prediction via \nheterogeneous network transformer. Briefings in Bioinformatics. 2023. \n[82] Zihao Li, Changkun Jiang, Jianqiang Li. DeepGATGO: A Hierarchical Pretraining-Based \nGraph-Attention Model for Automatic Protein Function Prediction. arXiv - CS - Machine Learning. \n2023. \n[83] Mahapatra S, Gupta V, Sahu S, et al. Deep Neural Network and extreme gradient boosting based \nHybrid classifier for improved prediction of Protein-Protein interaction. IEEE/ACM transactions on \ncomputational biology and bioinformatics. 2021. \n[84] Jie Pan, Zhu-Hong You, Li-Ping Li, et al. DWPPI: A Deep Learning Approach for Predicting \nProtein-Protein Interactions in Plants Based on Multi-Source Information with a Large-Scale \nBiological Network. Frontiers in Bioengineering and Biotechnology. 2022. \n[85] Wang J, Wang X, Chen W. Prediction of protein interactions based on CT-DNN. Proceedings of \nthe 2022 9th International Conference on Biomedical and Bioinformatics Engineering. 2023.  \n[86] Xiaotian Hu, Cong Feng, Yincong Zhou, et al. DeepTrio: a ternary prediction system for protein–\nprotein interaction using mask multiple parallel convolutional neural networks. Bioinformatics. 2021. \n[87] Hongli Gao, Cheng Chen, Shuangyi Li, et al. Prediction of protein-protein interactions based \non ensemble residual conventional neural network. Computers in Biology and Medicine. 2023. \n[88] Jun Hu, Ming Dong, Yu-Xuan Tang, et al. Improving protein-protein interaction site prediction \nusing deep residual neural network. Analytical Biochemistry. 2023.  \n[89] Sho T, Mehedi H M, Satoshi F, et al. LSTM-PHV: prediction of human-virus protein–protein \ninteractions by LSTM with word2vec. Briefings in Bioinformatics. 2021. \n[90] Joseph S, Amin E. RAPPPID: towards generalizable protein interaction prediction with \nAWD-LSTM twin networks. Bioinformatics. 2022. \n[91] Aybey E, Gümü, zgür. SENSDeep: An Ensemble Deep Learning Method for Protein–Protein \nInteraction Sites Prediction. Interdisciplinary Sciences: Computational Life Sciences. 2022. \n[92] Réau Manon, Nicolas R, Xue L C, et al. DeepRank-GNN: a graph neural network framework to \nlearn patterns in protein–protein interfaces. Bioinformatics. 2022. \n[93] Albu AI, Bocicor MI, Czibula G. MM-StackEns: A new deep multimodal stacked generalization \napproach for protein-protein interaction prediction. Computers in Biology and Medicine. 2023. \n[94] Yuting Zhou, Yongquan Jiang, Yan Yang. AGAT-PPIS: a novel protein–protein interaction site \npredictor based on augmented graph attention network with initial residual and identity mapping. \nBriefings in Bioinformatics. 2023. \n[95] Tang M, Wu L, Yu X, et al. Prediction of Protein–Protein Interaction Sites Based on Stratified \nAttentional Mechanisms. Frontiers in Genetics. 2021. \n[96] Xue Li, Peifu Han, Gan Wang, et al. SDNN-PPI: self-attention with deep neural network effect on \nprotein-protein interaction prediction. BMC Genomics. 2022. \n[97] Minjie Mou, Ziqi Pan, Zhimeng Zhou, et al. A Transformer-Based Ensemble Framework for the \nPrediction of Protein-Protein Interaction Sites. Research (Wash D C). 2023.  \n[98] Czibula G, Albu AI, Bocicor MI, et al. AutoPPI: An Ensemble of Deep Autoencoders for \nProtein-Protein Interaction Prediction. Entropy (Basel). 2021. \n[99] Yue Jiang, Yuxuan Wang, Lin Shen, et al. Identification of all-against-all protein–protein \ninteractions based on deep hash learning. BMC Bioinformatics. 2022. \n[100] Soleymani F, Paquet E, Viktor H, et al. ProtInteract: A deep learning framework for predicting \nprotein–protein interactions. Computational and Structural Biotechnology Journal. 2023. \n[101] Lee Minhyeok. Recent Advances in Deep Learning for Protein-Protein Interaction Analysis: A \nComprehensive Review. Molecules. 2023. \n[102] Xinhui Li, Yurong Qian, Haitao Yue, et al. A Review of Protein Function Prediction Research \nBased on Bioinformatics. Computer Engineering and Applications. 2023. \n[103] Jidong Zhang, Zhihan Wang, Bo Liu. Advances in the Application of Deep Learning in \nBiological Sequence Analysis. Journal of Beijing University of Technology. 2022. \n[104] Jianing Qiu, Lin Li, Jiankai Sun, et al. Large AI Models in Health Informatics: Applications, \nChallenges, and the Future. IEEE J Biomed Health Inform. 2023. \n[105] Bucci S, D'Innocente A, Liao Y, et al. Self-Supervised Learning Across Domains. IEEE \nTransactions on Pattern Analysis and Machine Intelligence. 2021. \n[106] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, et al. A Comprehensive Survey on Transfer Learning. \nProceedings of the IEEE. 2021. \n",
  "categories": [
    "q-bio.GN"
  ],
  "published": "2024-12-23",
  "updated": "2024-12-23"
}