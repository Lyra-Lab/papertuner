{
  "id": "http://arxiv.org/abs/2412.17921v1",
  "title": "VITRO: Vocabulary Inversion for Time-series Representation Optimization",
  "authors": [
    "Filippos Bellos",
    "Nam H. Nguyen",
    "Jason J. Corso"
  ],
  "abstract": "Although LLMs have demonstrated remarkable capabilities in processing and\ngenerating textual data, their pre-trained vocabularies are ill-suited for\ncapturing the nuanced temporal dynamics and patterns inherent in time series.\nThe discrete, symbolic nature of natural language tokens, which these\nvocabularies are designed to represent, does not align well with the\ncontinuous, numerical nature of time series data. To address this fundamental\nlimitation, we propose VITRO. Our method adapts textual inversion optimization\nfrom the vision-language domain in order to learn a new time series per-dataset\nvocabulary that bridges the gap between the discrete, semantic nature of\nnatural language and the continuous, numerical nature of time series data. We\nshow that learnable time series-specific pseudo-word embeddings represent time\nseries data better than existing general language model vocabularies, with\nVITRO-enhanced methods achieving state-of-the-art performance in long-term\nforecasting across most datasets.",
  "text": "VITRO: Vocabulary Inversion for Time-series\nRepresentation Optimization\nFilippos Bellos\nUniversity of Michigan\nAnn Arbor, MI, USA\nEmail: fbellos@umich.edu\nNam H. Nguyen\nCapital One\nMcLean, VA, USA\nEmail: nam.nguyen@capitalone.com\nJason J. Corso\nUniversity of Michigan\nAnn Arbor, MI, USA\nEmail: jjcorso@umich.edu\nAbstract‚ÄîAlthough LLMs have demonstrated remarkable ca-\npabilities in processing and generating textual data, their pre-\ntrained vocabularies are ill-suited for capturing the nuanced\ntemporal dynamics and patterns inherent in time series. The\ndiscrete, symbolic nature of natural language tokens, which these\nvocabularies are designed to represent, does not align well with\nthe continuous, numerical nature of time series data. To address\nthis fundamental limitation, we propose VITRO. Our method\nadapts textual inversion optimization from the vision-language\ndomain in order to learn a new time series per-dataset vocabulary\nthat bridges the gap between the discrete, semantic nature of\nnatural language and the continuous, numerical nature of time\nseries data. We show that learnable time series-specific pseudo-\nword embeddings represent time series data better than existing\ngeneral language model vocabularies, with VITRO-enhanced\nmethods achieving state-of-the-art performance in long-term\nforecasting across most datasets.\nIndex Terms‚ÄîMultivariate Time Series, Large Language Mod-\nels, Forecasting, Optimization, Textual Inversion.\nI. INTRODUCTION\nLarge Language Models (LLMs) have transformed natural\nlanguage processing (NLP), excelling in traditional NLP tasks\nlike text generation but also showing promise in tasks that\nrequire complex and structured reasoning [1, 2]. Their impact\nhas extended beyond NLP, contributing to rapid advancements\nin computer vision and other signal processing applications\nthrough the development of multimodal models that can pro-\ncess and integrate information from various modalities such as\ntext, images, and audio. This versatility has naturally led the\ncommunity to explore their potential in time series forecasting,\na fundamental capability in numerous real-world dynamic\nsystems [3] including energy load management [4], climate\nmodelling [5], traffic forecasting [6], etc. Traditionally, these\nforecasting tasks have required extensive domain expertise\nand task-specific model designs, an approach that stands in\ncontrast to LLMs, which demonstrate strong performance\nacross diverse tasks with minimal examples, often in few-shot\nor zero-shot scenarios\n[7, 8]. This contrast underscores the\nneed to consider if and how the pre-trained knowledge and\ngeneralization capabilities of LLMs can be fully harnessed to\nperform accurate time series forecasting without fine-tuning\nthe underlying model.\nTime-LLM [9] and TEST [10] attempt to address this\nchallenge by reprogramming the input time series into text\nprototype representations and using textual prompts to provide\nPatch Embedder\nXi¬†\nPi*\nS*\nStats\nNumerical\nLanguage\nFrozen\nTraining\nùë£·µ¢\ns\nestats\nEi\nv\nv\nùë£·µ¢\ns\nt0\nt53\nt54\nt907\nv\nv\nv\nv\nLLM\nVocab.\nVITRO\nVocab.\nProjection¬†\n≈∂i\nTokenizer\nEmbedding Lookup\nLarge Language Model¬†\nFig. 1: VITRO optimizes learnable pseudo-word embeddings\nvi for each time series instance Xi and a shared dataset embed-\nding s to construct a new data-centric time series vocabulary\ntailored for forecasting. Time series are normalized, patched,\nand embedded. These patch embeddings Ei serve as prompts\nto guide the optimization of pseudo-words. The composite\nrepresentation, including statistical features estats, is fed into\na frozen LLM, whose output is projected to generate forecasts\nÀÜYi.\nadditional context. Crucially, these methods enable the LLM to\nperform time series forecasting while keeping the pre-trained\nmodel completely frozen, thus fully leveraging the model‚Äôs\npre-trained capabilities. Other methods, such as OFA [11] and\nS2IP-LLM [12], also investigate the use of pre-trained LLMs\nfor time-series forecasting. However, they require partial fine-\ntuning of the underlying language model to achieve good\nperformance, potentially limiting their ability to fully exploit\nthe LLM‚Äôs pre-trained knowledge.\nDespite the promise in these methods, they are still limited\nby the existing LLM vocabulary they rely on, which fails to\ncapture the nuanced patterns and characteristics specific to\ntime series data. This limitation naturally raises the question:\nIs there a better way to represent time series data than using\nthe general-purpose vocabulary of LLMs, in order to leverage\nthe inherent capabilities of LLMs for effective time series\nforecasting?\nTo address this question, we propose VITRO, a new method\nthat, as depicted in Fig.1, constructs a time series specific\nvocabulary by learning unique pseudo-words for each time\nseries instance in a dataset, inspired by the concept of textual\ninversion [13]. In addition, VITRO optimizes a shared embed-\nding, able to capture the domain specific dataset information,\narXiv:2412.17921v1  [cs.LG]  23 Dec 2024\nwhich in many real-world applications may not always be\navailable or informative.\nIntuitively, this method bridges the gap between LLMs and\ntime series data by creating a vocabulary that encodes time\nseries information in a format interpretable by the language\nmodel, while at the same time capturing the subtle variations\nin time series.\nWe demonstrate that VITRO can be leveraged across dif-\nferent forecasting approaches and LLM architectures, show-\ncasing its potential for broad application in the field of time\nseries forecasting. Quantitative experiments show state-of-the-\nart performance for the methods that leverage VITRO, while\nqualitative analysis reveals that our learned vocabulary exhibits\ndistinct patterns in attention weights and embedding distribu-\ntions, indicating successful specialization for time series tasks.\nII. METHOD\nA. Problem Formulation and Overview\nWe formulate our problem of Vocabulary Inversion for\nTime Series Representation Optimization as follows. Let X ‚àà\nRN√óT denote the time series data consisting of N different\n1-dimensional variables across a lookback window of T time\nsteps, where the i-th series is denoted as Xi ‚ààR1√óT . We\naim to learn a new time series data-centric vocabulary that\nwill allow a large language model f(¬∑) to better understand\nthe input time series in order to more accurately predict\nthe next œÑ time steps based on the input window T. Let\nY ‚ààRN√óœÑ denote the ground truth values for the next œÑ time\nsteps, and ÀÜY ‚ààRN√óœÑ represent the corresponding predictions.\nThe overall objective is to minimize the mean square errors\nbetween Y and ÀÜY , defined as:\n1\nœÑ\nœÑ\nX\nt=1\n|Yt ‚àíÀÜYt|2\nF .\n(1)\nTo solve this problem, we are inspired by the approach of\ntextual inversion [13] from the text-to-image diffusion model\nliterature‚Äîa simple yet powerful technique in low-shot image\ngeneration that learns a common concept in given images\nas a single token in text embedding space. Motivated by its\nsuccess, we aim to learn different concepts-representations of\ntime series data as text embeddings and use them to develop a\nnew time series forecasting vocabulary, which we hypothesize\nwill represent time series data better than the existing general\nnatural language model vocabulary.\nOur method consists of two stages. The first stage optimizes\na specialized vocabulary tailored for time series forecasting.\nThis stage captures patterns across an entire dataset, creating\na rich vocabulary that reflects the temporal dynamics and\nvariations inherent in the time series. The primary goal here\nis to establish a comprehensive representation rather than\nimmediate forecasting accuracy. In the second stage, we use\nthis specialized vocabulary for the actual forecasting tasks.\nThis stage benefits from the broad context learned in the first\nstage, applying it to individual time series instances to enhance\nforecasting performance. The two-stage approach ensures that\nour vocabulary is informed by the full dataset. It allows us\nto first establish a strong foundational representation before\nfocusing on specific forecasting tasks.\nB. Stage 1: Vocabulary Inversion for Time Series\nLLMs begin with a text processing step where each word or\nsub-word in an input string is converted to a token, which is an\nindex in some pre-defined dictionary. Each token is then linked\nto a unique embedding vector that can be retrieved through an\nindex-based embedding lookup.\nWe choose this embedding space as the target for vocab-\nulary inversion. Specifically, let a time series dataset D =\n{X1, X2, . . . , Xn}, where n represents the number of time\nseries instances in a dataset and each time series instance\nXi ‚ààR1√óT represents a time series segment of length T\n(lookback window). For this dataset, we designate a set of\nplaceholder strings, P ‚àó= {P ‚àó\n1 , P ‚àó\n2 , . . . , P ‚àó\nn}, where each P ‚àó\ni\nrepresents a unique time series instance Xi. Additionally, we\nintroduce a placeholder S‚àóshared per dataset which represents\nthe entire dataset-domain information.\nConcretely, our approach involves an iterative optimization\nprocess for:\n‚Ä¢ A corresponding embedding vi for each P ‚àó\ni , effectively\nexpanding the LLM‚Äôs vocabulary with n new ‚Äúwords‚Äù\nthat encode time series information.\n‚Ä¢ A corresponding embedding s for S‚àó, that encodes the\ndomain information for D as a whole.\nWe start by randomly initializing each pseudo-word embed-\nding vi and associating the placeholder P ‚àó\ni to it. Similarly, we\ninitialize the shared embedding s and associate it with S‚àó. The\noptimization process then runs for a fixed number of iterations,\noptimizing these embeddings to better represent the time series\nforecasting data. To condition the generation process, we\nutilize a small set of text prompt templates containing these\nplaceholders, such as ‚ÄúThe time series is P ‚àó\ni ‚Äù or ‚ÄúForecast the\nnext steps of P ‚àó\ni ‚Äù.\nAs shown in Fig. 1, we essentially intervene in the LLM‚Äôs\nembedding process. This allows us to ‚Äúinject‚Äù a rich set\nof time series concepts into the LLM‚Äôs vocabulary, each\nreflecting a specific instance in our dataset. The same happens\nwith the shared embedding, which ‚Äùinjects‚Äù general domain\ninformation.\n1) Model Pipeline: Following convention, for each input\ntime series Xi ‚ààRT , we first apply reversible instance\nnormalization (RevIN) [14] to mitigate distribution shift: ÀúXi =\nRevIN(Xi). We then divide ÀúXi into P overlapping or non-\noverlapping patches of length Lp: XP,i ‚ààRP √óLp [15], where\nP =\nj\nT ‚àíLp\nS\nk\n+ 2, and S is the horizontal sliding stride.\nTo obtain the final embeddings, we apply a linear transfor-\nmation to each patch: Ei = WeXP,i + be, where Ei ‚ààRP √ód,\nWe ‚ààRd√óLp is a learnable weight matrix, be ‚ààRd is a\nlearnable bias vector, and d is the embedding dimension of\nthe target LLM.\nPatch Embeddings as Prompts. We leverage the patch\nembeddings Ei, as a composite prompt to guide the LLM‚Äôs\nprocessing of time series data and the optimization of pseudo-\nwords vi, and shared embedding s. This approach is inspired\nby recent advancements showing that non-textual data modal-\nities can be effectively integrated as prefixes in prompts to\nfacilitate reasoning [16]. In our case, the patch embeddings Ei\nserve as a numerical representation of the time series, while the\npseudo-words vi and shared embedding s provide learnable,\ntext-like anchors for the model.\nFor each pseudo-word P ‚àó\ni , we learn a corresponding embed-\nding vector vi ‚ààRd. We pass P ‚àó\ni through the LLM‚Äôs tokenizer\nto obtain a token representation which we associate in the\nLLM‚Äôs embedding lookup table with our learnable embedding\nvi. Accordingly, we associate the shared word S‚àówith the\nlearned shared embedding s in the embedding lookup process.\nWe then concatenate Ei, vi and s along with certain\nstatistics we calculate for Xi, and we feed them through the\nfrozen LLM to obtain the last hidden layer output hi ‚ààRh:\nhi = f([Ei; ui; s; estats]), where f(¬∑) denotes the frozen LLM\nand ; denotes concatenation.\nThe last hidden layer output hi is passed through a learnable\nlinear layer g(¬∑) to generate the forecasted values ÀÜYi: ÀÜYi =\ng(hi) = W √ó hi + b, where W ‚ààRœÑ√óh and b ‚ààRœÑ are the\nlearnable weights and bias.\n2) Optimization Objective: Our optimization objective is to\nminimize the loss L between the forecasted values ÀÜYi and the\nground truth future values Yi for each time series instance Xi:\nLMSE = 1\nœÑ ‚à•Yi ‚àí(g (f (Ei; ‚ÄúThe time series\nis [P ‚àó\ni ], The dataset is [S‚àó]‚Äù; Statistics(Xi)))‚à•2\nF\n(2)\nWe optimize the shared embedding s, pseudo-word embed-\ndings V = v1, v2, ..., vn and all other learnable parameters Œ∏\n(including We, be, W and b) to minimize the total loss:\ns‚àó, V ‚àó, Œ∏‚àó= arg min\ns,V,Œ∏\nn\nX\ni=1\nLMSE\n(3)\nC. Stage 2: Time Series Forecasting with Learned Vocabulary\nStage 2 of our method focuses on leveraging the learned\nvocabulary V for time series forecasting. We present two\napproaches using different LLM architectures (i.e. GPT2 [17]\nand LLaMa [18]) demostrating that the effectiveness of\nVITRO is not restricted to one type of LLM or LLM-\nbased method: a similarity-based selection method that directly\nutilizes the vocabulary, and TimeLLM‚Äôs [9] attention-based ap-\nproach that allowing us to assess VITRO‚Äôs benefits compared\nto the standard LLM vocabulary within the TimeLLM method,\nwhich is the current state of the art method that utilizes a\nfrozen pretrained LLM.\nSimilarity-based Selection (Sim) For computational ef-\nficiency, instead of using the word embeddings from the\nfull vocabulary V , we first derive a reduced set of core\nlexicon embeddings C using a linear mapping function h(¬∑):\nC = h(V ) = WvV + bv, where C ‚ààRn‚Ä≤√ód, n‚Ä≤ is the number\nof core lexicon embeddings with n‚Ä≤ ‚â™n, Wv ‚ààRn‚Ä≤√ón and\nbv ‚ààRn‚Ä≤ are learnable parameters. For each patch embedding\nEi and each core lexicon embedding cm ‚ààC, we compute\nthe cosine similarity: sim(Ei, cm) = (Ei ¬∑ cm)/(‚à•Ei‚à•‚à•cm‚à•).\nFrom these n‚Ä≤ core lexicon embeddings, we then select the\ntop k embeddings with the highest similarity scores, where\nk < n‚Ä≤. This similarity-based ranking and selection ensures\nthat we identify the most relevant core lexicon embeddings\nfor each patch, while maintaining computational efficiency by\noperating in a reduced similarity space (n‚Ä≤ instead of n).\nWe then form an augmented embedding for each patch:\nÀÜei = [Ei; c1; c2; ...; ck; s; estats], that will serve as input to\nthe frozen pre-trained LLM, which in this case is GPT2.\nAttention-based approach As mentioned, we also employ\nTimeLLM‚Äôs [9] attention-based approach , demonstrating the\nimproved results (in Section III) when using our optimized\nvocabulary over the existing one. This method involves a\nmulti-head cross-attention mechanism between patch embed-\ndings and our optimized vocabulary, allowing the model to\ndynamically select relevant information.\nConcretely, we employ a multi-head cross-attention layer.\nFor each head h = {1, ..., H}, we define the Query matrices\nas Q(i)\nh\n= EiW Q\nh , the Key matrices as K(i)\nh\n= CW K\nh\nand\nthe Value matrices as V (i)\nh\n= CW V\nh , where W Q\nh , W K\nh , W V\nh ‚àà\nRd√ódh, and dh = d/H.\nThe attention operation for each head is:\nZ(i)\nh\n= Softmax\n\u0012\nQ(i)\nh K(i)‚ä§\nh\n‚àödh\n\u0013\nV (i)\nh . Aggregating Z(i)\nh\n‚àà\nRP √ódh across all heads yields Z(i) ‚ààRP √ód, which is then\npassed through the frozen LLM, which in this case is Llama-\n7B, along with the optimized shared embedding that encapsu-\nlates the domain information, and the calculated statistics.\nIII. EXPERIMENTS\nIn our experimental evaluation, we compare the effec-\ntiveness of VITRO vocabularies against the general natural\nlanguage LLM vocabularies within our Stage 2 framework,\ndemonstrating the advantages of our optimized time series rep-\nresentation. We benchmark VITRO-enhanced methods against\nother LLM-based methods and traditional time series fore-\ncasting baselines on the task of long-term forecasting. We\nalso provide a qualitative analysis of VITRO and existing\nLLM vocabularies. For all baselines, we adhere to the exper-\nimental configurations outlined by [19], utilizing their unified\npipeline1.\nBaselines. The baselines include the best performing ap-\nproaches based on LLMs, i.e., Time-LLM [9] whose method\nis used as variant of our stage 2, and S2IP-LLM [12] even\nthough this method partly finetunes the backbone model. We\nalso include the best performing Transformer-based and non-\nTransformer methods, i.e. PatchTST [15] and DLinear [20].\nA. Long-term Forecasting\nSetup. We evaluate the effectiveness of VITRO across 7\npublic datasets: Weather, Electricity, Traffic, and four ETT\n1https://github.com/thuml/Time-Series-Library\nMethods\nVITRO-Sim\nSim\nVITRO-TimeLLM\nTimeLLM\nS2IP-LLM\nPatchTST\nDlinear\nMetric\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nETTh1\n0.412‚Üì\n0.430‚Üì\n0.442\n0.449\n0.416‚Üì\n0.437‚Üì\n0.437\n0.450\n0.425\n0.440\n0.444\n0.453\n0.418\n0.439\nETTh2\n0.351‚Üì\n0.393‚Üì\n0.370\n0.402\n0.349‚Üì\n0.395‚Üì\n0.360\n0.400\n0.358\n0.403\n0.381\n0.411\n0.502\n0.481\nETTm1\n0.353‚Üì\n0.380‚Üì\n0.365\n0.388\n0.352‚Üì\n0.387‚Üì\n0.367\n0.396\n0.347\n0.382\n0.363\n0.391\n0.357\n0.389\nETTm2\n0.260‚Üì\n0.323‚Üì\n0.284\n0.332\n0.263‚Üì\n0.321‚Üì\n0.264\n0.325\n0.261\n0.326\n0.267\n0.325\n0.275\n0.340\nWeather\n0.230‚Üì\n0.268‚Üì\n0.233\n0.273\n0.225‚Üì\n0.263‚Üì\n0.227\n0.265\n0.229\n0.267\n0.225\n0.264\n0.248\n0.300\nElectricity\n0.161‚Üì\n0.258‚Üì\n0.165\n0.261\n0.166‚Üì\n0.267‚Üì\n0.168\n0.270\n0.167\n0.263\n0.161\n0.252\n0.166\n0.263\nTraffic\n0.399‚Üì\n0.276‚Üì\n0.402\n0.279\n0.408‚Üì\n0.306‚Üì\n0.410\n0.310\n0.418\n0.303\n0.390\n0.263\n0.433\n0.295\nTABLE I: Long-term forecasting results for {96, 192, 336, 720} horizons. A lower value indicates a better performance. All\nresults are averaged from four forecasting horizons{96, 192, 336, 720}. Arrows‚Üìindicate positive impact of VITRO compared\nto existing LLM vocabulary. Bold: best results. Underlined: second best. We reproduced TimeLLM, S2IP-LLM, PatchTST,\nDlinear results using their official open-source implementations.\ndatasets (i.e., ETTh1, ETTh2, ETTm1, and ETTm2), which\nhave been widely adopted as benchmarking datasets for long-\nterm forecasting models. The input time series length is 512\nand we evaluate the performance on four different horizons\n{96, 192, 336, 720}. The evaluation metrics include the mean\nsquare error (MSE) and the mean absolute error (MAE).\nResults. Our results are shown in TABLE I. When we\nreplace the existing general-purpose vocabulary with VITRO‚Äôs\nlearned vocabulary in both the Time-LLM approach and our\nsimilarity-based method, we observe consistent improvements\nacross all 7 datasets tested for both MSE and MAE met-\nrics. The impact is particularly pronounced for the ETTh1,\nETTh2 and ETTm1 datasets. When compared to state-of-the-\nart methods, our VITRO-enhanced approaches consistently\noutperform across most datasets. Specifically, for the MAE\nmetric our methods outperform the LLM-based method (S2IP-\nLLM) in all 7 datasets while for MSE in 6 out of 7 datasets.\nComparing against the transformer based method (PatchTST)\nVITRO performs better in 5 out of 7 datasets for the MAE\nmetric, 4 out of 7 for MSE while achieving the same result\nfor the same metric in 2 datasets (i.e. Electricity, Weather).\nFinally, we outperform the non-transformer method (Dlinear)\nin all datasets for both metrics.\nIV. QUALITATIVE ANALYSIS\nFigures 2 and 3 reveal the impact of the specialized nature\nof the new vocabulary for time series tasks, contrasting with\nthe general-purpose characteristics of existing vocabularies.\nIn Fig. 3, the heatmaps, generated by the attention-based\napproach of stage 2 (TimeLLM approach), for the VITRO\nvocabulary show distinct horizontal striping patterns. This\nsuggests that certain vocabulary elements are consistently\nmore important across different parts of the input sequence,\nindicating that our vocabulary has captured some general fea-\ntures and underlying structures in time series data applicable\nacross various time steps. In contrast, the existing vocabulary‚Äôs\nweights show a more uniform distribution, suited for more\ngeneral language tasks. Figure 2‚Äôs PCA and TSNE visual-\nizations further support this distinction: the new vocabulary\nforms a U-shaped manifold, suggesting a robust and structured\nembedding space, which indicates a specialized representation\nof time series concepts, while the existing vocabulary reveals\nVITRO\nVocab.\nLLM\nVocab.\nPCA\nt-SNE\nFig. 2: PCA and t-SNE visualizations of VITRO and existing\ngeneral-purpose vocabulary embedding space.\nFig. 3: VITRO and LLM existing vocabularies heatmaps.\nEach row corresponds to a word in the vocabulary, the y-axis\nrepresents the index of the word, and the x-axis denotes the\nembedding dimensions. Brighter colors indicate higher values.\na diffuse, circular distribution typical of general-purpose lan-\nguage embeddings.\nV. CONCLUSION AND FUTURE WORK\nVITRO demonstrates significant potential in enhancing\nLLMs for time series forecasting by learning a time series\ndata-centric vocabulary through vocabulary inversion. Our\nresults consistently show that time series forecasting accuracy\ncan be improved by replacing the LLM‚Äôs general-purpose\nvocabulary with our VITRO-optimized one. However, as an\niterative optimization-based method, VITRO‚Äôs computational\ncost may limit its application in larger datasets. Future research\ndirections will explore further optimization of the vocabulary\nlearning process, extending VITRO to other time series tasks\nbeyond forecasting, and integrating VITRO with other LLM-\nbased methods (e.g. S2IP-LLM).\nREFERENCES\n[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou,\n‚ÄúChain-of-thought prompting elicits\nreasoning in large language models,‚Äù in Proceedings of\nthe 36th International Conference on Neural Information\nProcessing Systems, Red Hook, NY, USA, 2024, NIPS\n‚Äô22, Curran Associates Inc.\n[2] Filippos Bellos, Yayuan Li, Wuao Liu, and Jason Corso,\n‚ÄúCan large language models reason about goal-oriented\ntasks?,‚Äù in Proceedings of the First edition of the Work-\nshop on the Scaling Behavior of Large Language Models\n(SCALE-LLM 2024), St. Julian‚Äôs, Malta, Mar. 2024, pp.\n24‚Äì34, Association for Computational Linguistics.\n[3] Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zam-\nbon, Cesare Alippi, Geoffrey I. Webb, Irwin King, and\nShirui Pan,\n‚ÄúA survey on graph neural networks for\ntime series: Forecasting, classification, imputation, and\nanomaly detection,‚Äù IEEE transactions on pattern anal-\nysis and machine intelligence, vol. PP, 2023.\n[4] Hengbo Liu, Ziqing Ma, Linxiao Yang, Tian Zhou, Rui\nXia, Yi Wang, Qingsong Wen, and Liang Sun, ‚ÄúSadi:\nA self-adaptive decomposed interpretable framework for\nelectric load forecasting under extreme events,‚Äù\nin\nIEEE International Conference on Acoustics, Speech and\nSignal Processing, 2023.\n[5] Stephen H Schneider and Robert E Dickinson, ‚ÄúClimate\nmodeling,‚Äù Reviews of Geophysics, vol. 12, no. 3, pp.\n447‚Äì493, 1974.\n[6] Yihong Tang, Ao Qu, Andy H. F. Chow, William H. K.\nLam, Sze Chun Wong, and Wei Ma, ‚ÄúDomain adversarial\nspatial-temporal network: A transferable framework for\nshort-term traffic forecasting across cities,‚Äù CoRR, vol.\nabs/2202.03630, 2022.\n[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,\nJared\nKaplan,\nPrafulla\nDhariwal,\nArvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei,\n‚ÄúLanguage models are few-shot\nlearners,‚Äù in Advances in Neural Information Processing\nSystems 33, NeurIPS 2020, December 6-12, 2020, Hugo\nLarochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-\nFlorina Balcan, and Hsuan-Tien Lin, Eds., 2020.\n[8] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa,\n‚ÄúLarge language\nmodels are zero-shot reasoners,‚Äù\nAdvances in neural\ninformation processing systems, vol. 35, pp. 22199‚Äì\n22213, 2022.\n[9] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu,\nJames Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan\nLiang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen,\n‚ÄúTime-LLM: Time series forecasting by reprogramming\nlarge language models,‚Äù\nin The Twelfth International\nConference on Learning Representations, 2024.\n[10] Chenxi Sun, Hongyan Li, Yaliang Li, and Shenda Hong,\n‚ÄúTEST: Text prototype aligned embedding to activate\nLLM‚Äôs ability for time series,‚Äù\nin The Twelfth Inter-\nnational Conference on Learning Representations, 2024.\n[11] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and\nRong Jin,\n‚ÄúOne fits all: Power general time series\nanalysis by pretrained LM,‚Äù in Thirty-seventh Conference\non Neural Information Processing Systems, 2023.\n[12] Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider,\nYuriy Nevmyvaka, and Dongjin Song, ‚Äú$sÀÜ2$IP-LLM:\nSemantic space informed prompt learning with LLM\nfor time series forecasting,‚Äù in Forty-first International\nConference on Machine Learning, 2024.\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-\nor, ‚ÄúAn image is worth one word: Personalizing text-\nto-image generation using textual inversion,‚Äù\nin The\nEleventh International Conference on Learning Repre-\nsentations, 2023.\n[14] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park,\nJang-Ho Choi, and Jaegul Choo,\n‚ÄúReversible instance\nnormalization for accurate time-series forecasting against\ndistribution shift,‚Äù in International Conference on Learn-\ning Representations, 2022.\n[15] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and\nJayant Kalagnanam, ‚ÄúA time series is worth 64 words:\nLong-term forecasting with transformers,‚Äù\nin Interna-\ntional Conference on Learning Representations, 2023.\n[16] Maria\nTsimpoukelli,\nJacob\nMenick,\nSerkan\nCabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill, ‚ÄúMulti-\nmodal few-shot learning with frozen language models,‚Äù\nin Advances in Neural Information Processing Systems,\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman\nVaughan, Eds., 2021.\n[17] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al., ‚ÄúLanguage models\nare unsupervised multitask learners,‚Äù OpenAI blog, vol.\n1, no. 8, pp. 9, 2019.\n[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth¬¥ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al., ‚ÄúLlama: Open and efficient foundation language\nmodels,‚Äù arXiv preprint arXiv:2302.13971, 2023.\n[19] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin\nWang, and Mingsheng Long, ‚ÄúTimesnet: Temporal 2d-\nvariation modeling for general time series analysis,‚Äù in\nInternational Conference on Learning Representations,\n2023.\n[20] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu,\n‚ÄúAre transformers effective for time series forecasting?,‚Äù\nin Proceedings of the AAAI Conference on Artificial\nIntelligence, 2023.\n",
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "published": "2024-12-23",
  "updated": "2024-12-23"
}