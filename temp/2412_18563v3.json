{
  "id": "http://arxiv.org/abs/2412.18563v3",
  "title": "A Deep Reinforcement Learning Framework for Dynamic Portfolio Optimization: Evidence from China's Stock Market",
  "authors": [
    "Gang Huang",
    "Xiaohua Zhou",
    "Qingyang Song"
  ],
  "abstract": "Artificial intelligence is transforming financial investment decision-making\nframeworks, with deep reinforcement learning demonstrating substantial\npotential in robo-advisory applications. This paper addresses the limitations\nof traditional portfolio optimization methods in dynamic asset weight\nadjustment through the development of a deep reinforcement learning-based\ndynamic optimization model grounded in practical trading processes. The\nresearch advances two key innovations: first, the introduction of a novel\nSharpe ratio reward function engineered for Actor-Critic deep reinforcement\nlearning algorithms, which ensures stable convergence during training while\nconsistently achieving positive average Sharpe ratios; second, the development\nof an innovative comprehensive approach to portfolio optimization utilizing\ndeep reinforcement learning, which significantly enhances model optimization\ncapability through the integration of random sampling strategies during\ntraining with image-based deep neural network architectures for\nmulti-dimensional financial time series data processing, average Sharpe ratio\nreward functions, and deep reinforcement learning algorithms. The empirical\nanalysis validates the model using randomly selected constituent stocks from\nthe CSI 300 Index, benchmarking against established financial econometric\noptimization models. Backtesting results demonstrate the model's efficacy in\noptimizing portfolio allocation and mitigating investment risk, yielding\nsuperior comprehensive performance metrics.",
  "text": "A Deep Reinforcement Learning Framework for \nDynamic Portfolio Optimization: Evidence from \nChina's Stock Market \n \nGang Huang1*, Xiaohua Zhou1† and Qingyang Song2† \n1*Department of Applied Economics, Chongqing University, 174 Shazheng \nStreet, Chongqing, 400044, China. \n2Department of Technology Economics and Management, Chongqing \nUniversity, 174 Shazheng Street, Chongqing, 400044, China. \n \n*Corresponding author(s). E-mail(s): huanggangvoyager@163.com; \nContributing authors: zhxiaoh@aliyun.com; song211@126.com; \n†These authors contributed equally to this work. \n \n \n \nAbstract \nArtificial intelligence is transforming financial investment decision-making \nframeworks, with deep reinforcement learning demonstrating substantial potential in \nrobo-advisory applications. This paper addresses the limitations of traditional portfolio \noptimization methods in dynamic asset weight adjustment through the development of \na deep reinforcement learning-based dynamic optimization model grounded in practical \ntrading processes. The research advances two key innovations: first, the introduction of \na novel Sharpe ratio reward function engineered for Actor-Critic deep reinforcement \nlearning algorithms, which ensures stable convergence during training while \nconsistently achieving positive average Sharpe ratios; second, the development of an \ninnovative comprehensive approach to portfolio optimization utilizing deep \nreinforcement learning, which significantly enhances model optimization capability \nthrough the integration of random sampling strategies during training with image-based \ndeep neural network architectures for multi-dimensional financial time series data \nprocessing, average Sharpe ratio reward functions, and deep reinforcement learning \nalgorithms. The empirical analysis validates the model using randomly selected \nconstituent stocks from the CSI 300 Index, benchmarking against established financial \neconometric optimization models. Backtesting results demonstrate the model's efficacy \nin optimizing portfolio allocation and mitigating investment risk, yielding superior \ncomprehensive performance metrics. \n \nKeywords：Portfolio Management; Decision Optimization; Dynamic Optimization; \nDeep Reinforcement Learning \n \n \n1 INTRODUCTION \nIn recent years, Artificial Intelligence (AI) has achieved significant technological \nadvances, notably in natural language processing. ChatGPT, developed by OpenAI, has \ncatalyzed extensive discourse on AI's potential through its exceptional language \ncomprehension and generation capabilities. The system's success is predominantly \nattributed to \"Reinforcement Learning from Human Feedback\" (RLHF), an innovative \nmethodology that substantially enhances AI system performance and alignment through \nthe integration of human feedback into the reinforcement learning process. The \ntechnological foundation of RLHF is Deep Reinforcement Learning (DRL), an \nadvanced machine learning paradigm that synthesizes deep learning and reinforcement \nlearning methodologies. While DRL has demonstrated remarkable efficacy in natural \nlanguage processing and exhibited substantial potential across domains including game \nAI and robotic control, its applications in finance remain predominantly exploratory, \nparticularly in the complex domain of portfolio optimization. \nPortfolio optimization constitutes a fundamental challenge in finance, focusing on \nthe systematic allocation of funds across multiple assets based on investment decisions, \nmanifesting through dynamic adjustments in portfolio asset weights. Traditional \nportfolio optimization methodologies, originating from Modern Portfolio Theory[1] \nand evolving through subsequent enhancements, have contributed substantially to the \nfield's development. However, these approaches present inherent limitations, including \nrestrictive assumptions regarding asset return distributions, subjective utility function \nspecifications, and insufficient adaptability to dynamic market conditions. \nThis research examines the implementation potential of deep reinforcement \nlearning in portfolio optimization through the development of novel reward functions \nand deep neural network architectures, aimed at constructing an intelligent model for \neffective dynamic asset allocation. The study advances both theoretical and practical \ncontributions by introducing innovative approaches to portfolio optimization while \nestablishing new trajectories for artificial intelligence applications in financial domains. \n \n2 LITERATURE REVIEW \nMarkowitz[1] established modern portfolio theory, pioneering the application of \nquantitative analysis methods in portfolio optimization. Samuelson [2] contended that \nMarkowitz's model addressed single-period investment problems but was inadequate \nfor multi-period asset allocation, consequently proposing a utility function for \nanalyzing wealth planning problems. Subsequent researchers, including Kelly [3], \nMerton [4], and numerous scholars in behavioral finance, extended the application of \nutility functions in asset allocation optimization. However, utility function-based \noptimization approaches exhibit inherent limitations, particularly in the inherent \nsubjectivity of function selection and the unverified universal applicability of chosen \nfunctions. The Black-Litterman (BL) model[5,6] represents another approach \nincorporating subjective elements, proposing that markets possess an implicit \nequilibrium return, where asset returns under equilibrium allocation serve as prior \nreturns. In this model, expected returns represent a weighted average of prior returns \nand investors' subjective expectations. However, the significant subjectivity in \nestablishing confidence levels for investors' subjective expectations has resulted in the \nabsence of a unified standard for measuring the equilibrium return rate of portfolio \nassets. \nBeyond traditional financial econometric analysis methods, operations researchers \nCharnes et al.[7] introduced Data Envelopment Analysis (DEA), a non-parametric \nanalytical framework for asset allocation optimization. Subsequently, Kirkpatrick[8] \nintegrated the simulated annealing algorithm into portfolio optimization, based on \nprinciples derived from natural sciences. In parallel, Arnone et al.[9] implemented \ngenetic algorithms for portfolio selection to minimize investment risk. However, these \nmodels universally treat the portfolio weight adjustment process as static, neglecting \nthe temporal dimension and failing to incorporate how asset allocations evolve in \nresponse to the sequential nature of trading activities. \nFurthermore, classical asset allocation models, including Markowitz's framework, \ncompute portfolio returns by multiplying contemporaneous asset weights with \nExpected Returns to derive the period's portfolio return. This is expressed as 𝑅𝑅=\n∑\n𝜔𝜔𝑖𝑖𝛾𝛾𝑖𝑖\n𝑛𝑛\n𝑖𝑖=1\n, where R represents the portfolio return, i denotes the number of assets, 𝜔𝜔𝑖𝑖 \nsignifies the asset weight, and 𝛾𝛾𝑖𝑖  represents the corresponding Expected Return. \nHowever, in a realistic dynamic trading environment, the portfolio's terminal return \nshould be computed by multiplying the previous period's asset weights with the \nsubsequent period's asset returns, expressed as 𝑅𝑅𝑡𝑡= ∑\n𝜔𝜔𝑖𝑖,𝑡𝑡−1𝛾𝛾𝑖𝑖,𝑡𝑡\n𝑛𝑛\n𝑖𝑖=1\n , where 𝛾𝛾𝑖𝑖,𝑡𝑡 \nrepresents the realized return of asset i in period t (not the Expected Return), and 𝜔𝜔𝑖𝑖,𝑡𝑡−1 \nrepresents the portfolio's asset weight allocation in period t-1. This fundamental \ndistinction can lead to significant discrepancies - modeling errors in the trading process \ninevitably compromise the practical efficacy of these models. Notably, numerous \nwidely-implemented optimization models in finance, including the Conditional Value \nat Risk model[10], Risk Parity model[11], and Hierarchical Risk Parity model[12], \nentirely disregard the temporal evolution of asset weights. Consequently, neither \nconventional financial econometric analysis methods nor sophisticated approaches such \nas DEA, simulated annealing algorithms, and genetic algorithms can adequately capture \nthe dynamic nature of portfolio weight adjustments during the trading process, thereby \nfailing to achieve optimal asset allocation strategies. \nDeep Reinforcement Learning (DRL) represents a dynamic modeling paradigm. \nThe \"Deep\" denomination in DRL derives from its incorporation of deep neural \nnetworks, which supersede the conventional artificial neural networks employed in \nearly Reinforcement Learning (RL), including fully connected and recurrent neural \nnetworks. This architectural enhancement has substantially improved RL's capacity for \nobjective function approximation. Early applications of RL in asset management \nprimarily employed Policy Gradient (PG)[13,14] and Q-learning algorithms. Moody[15] \nintroduced a single-asset management model utilizing the PG algorithm, with \nsubsequent derivative models predominantly focusing on single-risk asset management \nor fixed investment decision frameworks, as exemplified by Dempster et al.'s[16] \nautomated forex trading model and Zhang et al.'s[17] asset management framework. In \nparallel, Ralph Neuneier[18], Gao et al.[19], and Lee et al.[20] implemented Q-learning \nalgorithms for asset management, though these models remained confined to single-\nasset management. Furthermore, notable contributions to the research field of DRL \napplications in single-asset trading have been made by Wu et al.[21], Liu et al.[22], \nPourahmadi et al.[23], and Kochliaridis et al.[24], among others. However, some \nscholars have neglected the design of deep neural networks when applying DRL to \noptimize asset allocation, such as Wang et al.[25], while others have overlooked the \nallocation of asset weights and missed the basic constraint condition that asset weights \nsum to 1 (∑𝜔𝜔𝑖𝑖,𝑡𝑡= 1), such as Jiang et al.[26]. \nRecent advancements in computational capabilities and dynamic optimization \ntheory have led to the widespread adoption of DRL in portfolio asset management \nresearch. Jiang et al.[27] proposed a DRL portfolio management model for asset \noptimization in the cryptocurrency market. The model incorporated the definitions of \nrelative price vectors and transaction costs from Ormos et al.[28]. However, Ormos et \nal. misinterpreted the dynamic changes of assets in their paper, resulting in incorrect \ntransaction cost derivations. Due to the adoption of the same derivation methodology, \nJiang et al.[27]'s derivation of transaction cost rates exhibited comparable mathematical \ninconsistencies. While Jiang et al.[27] subsequently provided correct implementation \nformulas through approximation methods, the model's effectiveness in alternative \ncapital markets requires further validation[29]. \nUnder short-selling restrictions (long-only positions), the reward function in \ncurrent DRL portfolio weight optimization models primarily consists of portfolio \nreturns[30]. However, DRL models using this reward function have not performed well \nin Chinese stock market[29], leading Qi Yue et al.[31] to artificially set fixed investment \nweights to achieve satisfactory backtesting results. This approach, however, contradicts \nthe original intention of using DRL models for automatic asset weight optimization.  \nIn the field of DRL portfolio applications, researchers have demonstrated that \nimplementing reward functions to enhance DRL's asset optimization performance \nrepresents an effective approach. Multiple scholars have developed new reward \nfunctions to improve DRL's portfolio optimization performance: Wu et al.[32] \ninvestigated Taiwan stock market portfolios using a customized Sharpe ratio reward \nfunction (Annual Return/Annualized Standard Deviation of Return). However, their \nresearch did not specify the underlying RL algorithm implemented. Almahdi et al.[33] \nincorporated the Calmar ratio as the optimization objective in the reward function, \nintegrating it with Recurrent Reinforcement Learning (RRL), a derivative algorithm of \nPG, to optimize US stocks and emerging market assets. Aboussalah et al.[34] developed \na Sharpe ratio reward function compatible with RRL derivative algorithms for asset \nallocation, though this reward function is fundamentally equivalent to the Sharpe ratio \nreward function of the PG algorithm. Furthermore, Lim et al[35] employ a reward \nfunction based on the Net Asset Value of the portfolio to develop an RL-based strategy \nfor dynamic portfolio rebalancing that optimizes investment performance under \nvarying market conditions. A comprehensive review of existing literature reveals that \nno research has established appropriate Sharpe ratio reward functions specifically \ndesigned for the algorithmic characteristics of Actor-Critic. \nA critical review of existing DRL portfolio optimization literature reveals a \nsignificant limitation: most studies fail to demonstrate the convergence of their reward \nfunctions during experimentation[36, 37, 38, 39, 40]. Even in cases where reward \nfunction convergence is presented, the convergence exhibits substantial deficiencies. \nFor example, Yang[41]'s TC-MAC algorithm, which employs GNN to capture dynamic \nrelationships between assets, demonstrates concerning convergence behavior: despite \nan initial capital of $10,000, the reward function converges to merely $3,000, indicating \npotentially significant losses. Similarly, Sun et al.[42], in their integration of DRL with \nthe Black-Litterman model, artificially constrain portfolio weights through \npredetermined long-short position limits, fundamentally contradicting DRL's \nautonomous optimization capability. These cases exemplify fundamental limitations in \ncurrent approaches, raising concerns about the models' stability and reliability in \npractical applications. This methodological gap undermines the robustness of existing \nDRL-based portfolio optimization frameworks and calls into question their practical \napplicability in real-world investment scenarios. \nThis paper implements DRL methodology based on artificial intelligence \nprinciples to optimize portfolio asset weights, effectively eliminating subjective bias in \nmodel implementation while comprehensively addressing the dynamic characteristics \nof asset weight variations in real-world trading environments. The research presents \ntwo primary innovations: \nFirst, we introduce a novel Sharpe ratio reward function specifically engineered \nfor Actor-Critic DRL algorithm characteristics. While seminal research in RL asset \nmanagement applications by Moody[15] and Gao[19] employed the Sharpe ratio as a \nreward function, their designs were constrained to simple structures of PG algorithm \nand Q-learning algorithm. Similarly, the Sharpe ratio reward function developed by \nAboussalah et al.[34] did not adequately address the specific requirements of Actor-\nCritic algorithms. Our proposed reward function incorporates the architectural \ncharacteristics of Actor-Critic systems, implementing step-size normalization to \nenhance model stability and optimize the guidance of portfolio dynamic optimization \nprocesses. Notably, our reward function design ensures stable convergence during \nmodel training and consistently achieves positive average Sharpe ratios, addressing a \ncritical gap in current DRL portfolio optimization applications where reward function \nconvergence remains a significant challenge. \nSecond, this research introduces an innovative comprehensive methodology for \nportfolio optimization utilizing deep reinforcement learning. This approach \nsignificantly enhances model optimization capability through the integration of random \nsampling strategies during training with image-based deep neural network architectures \nfor multi-dimensional financial time series data processing, average Sharpe ratio \nreward functions, and deep reinforcement learning algorithms. Specifically, the \nnetwork architecture incorporates VGG network design principles from computer \nvision to construct a deep neural network framework for processing three-dimensional \ntime series data. Additionally, the model's generalization capability is enhanced and \noverfitting risk is minimized through the random selection of continuous trading data \nfrom the dataset during training, establishing a robust technical framework for effective \ndynamic portfolio optimization. \nThis research implements long-only position constraints and applies the proposed \nDRL model to optimize portfolios comprising CSI300 constituent stocks. The \noptimization outcomes are systematically benchmarked against multiple econometric \noptimization models to evaluate the DRL model's efficacy in asset allocation \noptimization. The research methodology adheres rigorously to the DRL model's \nmodeling framework, establishing comprehensive guidelines for future research \nendeavors. The significance of this study extends across both theoretical and practical \ndomains: it introduces a novel portfolio optimization methodology to the academic \nliterature while providing an effective solution for portfolio management practitioners. \nThe model systematically incorporates the dynamic characteristics of asset weight \nvariations in real trading environments, demonstrating significant potential for \nenhanced performance in practical applications. \n \n3 DRL MODEL CONFIGURATION \nDeep Reinforcement Learning (DRL) represents a dynamic optimization method \nconforming to the Markov Decision Process (MDP) framework. The portfolio trading \nprocess can be conceptualized as an MDP, where the trajectory from account initiation \nto trading completion is represented by 𝜏𝜏= (𝑆𝑆0, 𝐴𝐴0, 𝑅𝑅1, 𝑆𝑆1, 𝐴𝐴1,𝑅𝑅2, 𝑆𝑆2, 𝐴𝐴2,𝑅𝑅3, ⋯) , \nconstituting an episode. This framework enables the application of DRL theory for \nmodeling the trading process. Following the DRL modeling framework, this study \ndefines a portfolio trader as an agent, establishes the state (environment), action, and \nreward specifications, and implements a DRL algorithm with deep neural networks for \nportfolio optimization. \n \n3.1 State Space Configuration \nThe state space in DRL constitutes the environment for agent interaction. \nFollowing the Efficient Market Hypothesis, all information affecting asset values is \nembedded in asset prices; consequently, the state space is constructed exclusively using \ndaily asset price data. This study adopts the three-dimensional state space configuration \nproposed by Jiang et al.[27] in modeling the DRL environment, based on two \nfundamental considerations. First, DRL achieved its breakthrough in artificial \nintelligence through video game applications[43]. Video game displays comprise three-\ndimensional data structures, which are inherently suitable for deep neural network \nprocessing. Deep neural networks had previously demonstrated exceptional progress in \nimage recognition, achieving human-comparable performance in this domain. Second, \ntraditional financial econometric models typically employ dimensionality reduction \ntechniques, such as Principal Component Analysis (PCA), to reduce analytical \ncomplexity. However, these methods frequently result in significant loss of valuable \ninformation, with the loss magnitude increasing proportionally with the number of data \nfeatures. In contrast, deep neural networks possess superior nonlinear function \napproximation capabilities, enabling effective analysis of complex feature \ninterrelationships and addressing the limitations of traditional financial econometric \nmodels. Consequently, this study implements a three-dimensional temporal data \nstructure for the state space, effectively leveraging the advanced data processing \ncapabilities of deep neural networks. \n \n \nFig1 Data structure of state Xt \nThe state is defined as 𝑆𝑆𝑡𝑡= 𝑋𝑋𝑡𝑡, where the price tensor 𝑋𝑋𝑡𝑡 comprises four data \nfeatures: daily opening price 𝑉𝑉𝑡𝑡\n(𝑜𝑜𝑜𝑜) , lowest price 𝑉𝑉𝑡𝑡\n(𝑙𝑙𝑙𝑙) , highest price 𝑉𝑉𝑡𝑡\n(ℎ𝑖𝑖) , and \nclosing price 𝑉𝑉𝑡𝑡\n(𝑐𝑐𝑐𝑐) . The data structure is illustrated in Figure 1, with the tensor 𝑋𝑋𝑡𝑡 \ncalculation formula given by: \n \n \n𝑽𝑽𝒕𝒕\n(𝒐𝒐𝒐𝒐) = [𝝂𝝂𝒕𝒕−𝒏𝒏+𝟏𝟏\n(𝒐𝒐𝒐𝒐)\n⊘𝝂𝝂𝒕𝒕|𝝂𝝂𝒕𝒕−𝒏𝒏+𝟐𝟐\n(𝒐𝒐𝒐𝒐)\n⊘𝝂𝝂𝒕𝒕| ⋯|𝝂𝝂𝒕𝒕−𝟏𝟏\n(𝒐𝒐𝒐𝒐) ⊘𝝂𝝂𝒕𝒕|𝝂𝝂𝒕𝒕\n(𝒐𝒐𝒐𝒐) ⊘𝝂𝝂𝒕𝒕] \n𝑽𝑽𝒕𝒕\n(𝒍𝒍𝒍𝒍) = [𝝂𝝂𝒕𝒕−𝒏𝒏+𝟏𝟏\n(𝒍𝒍𝒍𝒍)\n⊘𝝂𝝂𝒕𝒕|𝝂𝝂𝒕𝒕−𝒏𝒏+𝟐𝟐\n(𝒍𝒍𝒍𝒍)\n⊘𝝂𝝂𝒕𝒕| ⋯|𝝂𝝂𝒕𝒕−𝟏𝟏\n(𝒍𝒍𝒍𝒍) ⊘𝝂𝝂𝒕𝒕|𝝂𝝂𝒕𝒕\n(𝒍𝒍𝒍𝒍) ⊘𝝂𝝂𝒕𝒕] \n𝑽𝑽𝒕𝒕\n(𝒉𝒉𝒉𝒉) = [𝝂𝝂𝒕𝒕−𝒏𝒏+𝟏𝟏\n(𝒉𝒉𝒉𝒉)\n⊘𝝂𝝂𝒕𝒕|𝝂𝝂𝒕𝒕−𝒏𝒏+𝟐𝟐\n(𝒉𝒉𝒉𝒉)\n⊘𝝂𝝂𝒕𝒕| ⋯|𝝂𝝂𝒕𝒕−𝟏𝟏\n(𝒉𝒉𝒉𝒉) ⊘𝝂𝝂𝒕𝒕|𝝂𝝂𝒕𝒕\n(𝒉𝒉𝒉𝒉) ⊘𝝂𝝂𝒕𝒕] \n𝑽𝑽𝒕𝒕\n(𝒄𝒄𝒄𝒄) = [𝝂𝝂𝒕𝒕−𝒏𝒏+𝟏𝟏⊘𝝂𝝂𝒕𝒕|𝝂𝝂𝒕𝒕−𝒏𝒏+𝟐𝟐⊘𝝂𝝂𝒕𝒕| ⋯|𝝂𝝂𝒕𝒕−𝟏𝟏⊘𝝂𝝂𝒕𝒕|𝟏𝟏] \n \n⑴ \nHere, 𝒗𝒗𝒕𝒕denotes the closing price vector of assets on trading day t, and the symbol \n⊘ represents element-wise division, where each vector element is divided by its \ncounterpart at the corresponding position. Each element in the price tensor 𝑿𝑿𝒕𝒕  is \nnormalized through division by the closing price vector 𝝂𝝂𝒕𝒕 . The window length \n(windows) specifies the temporal span of observable data for the agent's trading \ndecisions, with each feature layer containing the corresponding features of all risky \nassets in the portfolio (i.e., assets_num in Figure 1). \n \n3.2 Action Space Configuration \nThe model only considers long positions without short selling. The portfolio \nweights (i.e., the ratio of asset value to total assets) represent the model's action vector: \n \n𝑾𝑾𝒕𝒕= ൫𝝎𝝎𝟎𝟎,𝒕𝒕, 𝝎𝝎𝟏𝟏,𝒕𝒕, 𝝎𝝎𝟐𝟐,𝒕𝒕, ⋯, 𝝎𝝎𝒎𝒎,𝒕𝒕൯ \n⑵ \nwhere 𝝎𝝎𝟎𝟎,𝒕𝒕 represents the weight of the risk-free asset, specifically defined as the \ncash asset weight in this study. At time t, the portfolio weights satisfy the following \nconstraint: \n \n෍𝜔𝜔𝑖𝑖,𝑡𝑡\n𝑚𝑚\n𝑖𝑖=0\n= 1 \n⑶ \nUnder the long-only constraint, 𝜔𝜔𝑖𝑖,𝑡𝑡≥0 . The portfolio is initialized with \nexclusively cash assets, characterized by the initial weight vector 𝑾𝑾𝟎𝟎= (1,0, ⋯,0)T. \n \n3.3 Other Elements Derivation and Reward Function Setting \nLet vector 𝐏𝐏𝐭𝐭 denote the closing prices of assets in the portfolio at period t, and \n𝐘𝐘𝐭𝐭 denote the relative price vector: \n \n𝒀𝒀𝒕𝒕≜𝑷𝑷𝒕𝒕⊘𝑷𝑷𝒕𝒕−𝟏𝟏= ൫1, 𝑝𝑝1,𝑡𝑡∕𝑝𝑝1,𝑡𝑡−1, ⋯, 𝑝𝑝𝑖𝑖,𝑡𝑡∕𝑝𝑝𝑖𝑖,𝑡𝑡−1൯\nT \n⑷ \nLet 𝐶𝐶𝑡𝑡  denote the transaction cost rate of the entire portfolio in period t. The \nportfolio price 𝜌𝜌𝑡𝑡 is expressed as: \n \n𝜌𝜌𝑡𝑡= 𝜌𝜌𝑡𝑡−1(1 −𝐶𝐶𝑡𝑡)exp[(𝐥𝐥𝐥𝐥𝒀𝒀𝒕𝒕) ⋅𝑾𝑾𝒕𝒕−𝟏𝟏] \n⑸ \nThe daily logarithmic return rate 𝛾𝛾𝑡𝑡 of the portfolio is defined as: \n \n𝛾𝛾𝑡𝑡= ln(𝜌𝜌𝑡𝑡∕𝜌𝜌𝑡𝑡−1) \n⑹ \nThe mean 𝑅𝑅ത and standard deviation 𝑆𝑆𝑆𝑆𝑆𝑆(𝛾𝛾𝑡𝑡) of daily logarithmic return rates are \ncalculated as: \n \n𝑅𝑅ത= 1\n𝑡𝑡𝑛𝑛\n෍𝛾𝛾𝑡𝑡\n𝑡𝑡𝑛𝑛\n𝑡𝑡=1\n \n⑺ \n \n𝑠𝑠𝑠𝑠𝑠𝑠(𝛾𝛾𝑡𝑡) = ඨ∑\n(𝛾𝛾𝑡𝑡−𝑅𝑅ത)2\n𝑡𝑡𝑛𝑛\n𝑡𝑡=1\n𝑡𝑡𝑛𝑛\n൘ \n⑻ \nIn formulas ⑺ and ⑻, 𝑡𝑡𝑛𝑛  denotes the nth trading period, and 𝛾𝛾𝑡𝑡  is derived \nfrom the closing price at the end of period t. At market entry, investors hold exclusively \ncash assets. With the initial trading time point defined as 𝑡𝑡= 0  and 𝛾𝛾0 = 0 , and \nconsidering no allocation to risky assets at this point, formulas ⑸ and ⑹ indicate \nthat 𝛾𝛾1 = 0. Consequently, the return-generating period initiates at t=2. \nThe reward function employs the average annualized Sharpe ratio: \n \nreward：𝐴𝐴𝐴𝐴_𝐴𝐴𝐴𝐴𝐴𝐴𝑆𝑆ℎ𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑡𝑡= ඥ𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹⋅(𝑅𝑅ത−𝑟𝑟𝑓𝑓)\n𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆⋅𝑠𝑠𝑠𝑠𝑠𝑠(𝛾𝛾𝑡𝑡−𝑟𝑟𝑓𝑓)\n൘\n \n⑼ \nFreq denotes the annual trading days, set at 252 in this paper. 𝑟𝑟𝑓𝑓 represents the \nrisk-free rate, set to 0 for cash assets. Steps denotes the step length in a training episode, \nwith one trading decision per step. The model's training objective consists of \nmaximizing this reward function. \n𝐶𝐶𝑡𝑡 is determined by: \n \n𝐶𝐶𝑡𝑡= 𝜇𝜇𝑡𝑡൭෍ฬ𝝎𝝎𝑖𝑖,𝑡𝑡\n，−𝜔𝜔𝑖𝑖,𝑡𝑡ฬ\n𝑚𝑚\n𝑖𝑖=1\n൱ \n⑽ \n𝜇𝜇𝑡𝑡 represents the transaction cost rate per asset in period t, set at 𝜇𝜇𝑡𝑡= 0.0025 in \nthis study, constituting a substantially high rate. 𝝎𝝎𝑖𝑖,𝑡𝑡\n， denotes a component of weight \nvector 𝑾𝑾𝒕𝒕\n′, given by: \n \n𝑾𝑾𝒕𝒕\n′ = (𝒀𝒀𝒕𝒕⊙𝑾𝑾𝒕𝒕−𝟏𝟏) ∕(𝒀𝒀𝒕𝒕⋅𝑾𝑾𝒕𝒕−𝟏𝟏) \n⑾ \nwhere ⊙ denotes the Hadamard product and ⋅ denotes the inner product. 𝑾𝑾𝒕𝒕\n′ \nrepresents the weight values resulting from autonomous price movements between \npost-trading at t-1 and pre-trading at t, as illustrated in Figure 2 below: \n \n \nFigure 2 Changes in asset weights \n \n4 DRL ALGORITHM SELECTION AND NETWORK STRUCTURE \n4.1 Design of Average Sharpe Ratio Reward Function for Actor-Critic Architecture \nIn Deep Reinforcement Learning (DRL), algorithms serve as strategic frameworks \nenabling agents to explore environments and maximize returns through optimal action \nselection in the action space and reward acquisition in the state space. DRL algorithms \ncomprise two primary categories: on-policy and off-policy approaches, each exhibiting \ndistinct performance characteristics across various tasks. Through comprehensive \nexperimental evaluation of multiple algorithmic structures, we determined that off-\npolicy algorithms demand greater computational resources and demonstrate slower \nconvergence rates. Given hardware constraints, we selected Proximal Policy \nOptimization (PPO), an on-policy algorithm. \nPPO incorporates multiple performance optimization techniques, including \nGeneralized Advantage Estimation (GAE) and value function clipping, fundamentally \nextending Trust Region Policy Optimization (TRPO)[44], which itself enhances PG \nalgorithms[13,14]. PG algorithms implement episode-based update mechanisms, \nparallel to policy iteration, optimizing through complete episode sampling. PPO's \nActor-Critic architecture uniquely combines both episode-level updates and step-wise \nupdates within episodes. The original PPO paper[45] elegantly presents the algorithm \nthrough concise pseudocode utilizing two nested for-loops: an outer loop managing \nepisode updates and an inner loop executing step-wise updates. \nCapitalizing on the distinct update mechanism of Actor-Critic algorithms, we \ndeveloped an innovative average Sharpe ratio reward function calculation method \noptimized for Actor-Critic frameworks, illustrated through PPO implementation. The \nmethodology initializes an empty list R for storing returns from each trading step. \nDuring episode execution, the Actor network generates portfolio weight w1, prompting \nthe environment to return price change information y1 (the relative price vector). These \nparameters, combined with transaction cost c1, determine the portfolio value change p1 \nat each time step. The value change converts to logarithmic return r1 and appends to \nreturns list R, enabling Sharpe ratio computation at each trading step using the \ncumulative returns. The detailed implementation methodology is outlined in Definition \n1. \nPPO extends the episode update mechanism from PG algorithms while \nimplementing step-wise updates within episodes. For optimizing agent performance \nduring training, we normalize the annualized Sharpe ratio by the number of steps in \nepisode updates (formula 9), computing the agent's average Sharpe ratio at each \ntemporal point per episode. This methodology ensures reward comparability across \nvaried episode lengths and trading sequences, substantially enhancing model training \nstability. \n \nDefinition 1. Average Sharpe Ratio Reward Function for PPO(Actor-Critic) Algorithm  \nEnvironment Parameters:  \n    steps: total number of steps T in an episode \n    w0: portfolio weight vector at previous timestep \n    p0: portfolio value at previous timestep \n \nEnvironment Variables: \n    R = []  # Initialize cumulative returns list for Sharpe ratio calculation \n \nReward Definition: \n    for each episode do: \n        for t = 1 to T do: \n            1. State-Action Interaction: \n                w1 = πθ(st) \n                y1 = price relative vector from environment \n         \n            2. Portfolio Value Update: \n                compute transaction cost c1 \n                p1 = p0 * (1 - c1) * exp(dot(log(y1), w0)) \n         \n            3. Return Calculation: \n                r1 = log(p1 / p0) \n                append r1 to R \n         \n            4. Reward Function: \n                reward = sharpe(R) / T \n         \n            5. State Update: \n                store (st, w1, r1, st+1, reward) \n                w0 ← w1 \n                p0 ← p1 \n        end for \n    end for \n \nOutput: Timestep reward signals for DRL training \n \nThe implementation leverages the Stable-Baselines3 (SB3) framework for PPO \ndeployment, incorporating the innovative average Sharpe ratio reward function within \nthe environment state for portfolio optimization. Crucially, the environment state \nmaintains independence from the DRL algorithm, with the reward function \nimplementation residing in the environment without modifying the core PPO algorithm \nlogic in SB3. Empirical results demonstrate that this average Sharpe ratio reward \nfunction effectively harnesses PPO (Actor-Critic) algorithm characteristics, yielding \nsignificant improvements in out-of-sample performance. \n \n4.2 Neural Network Design \nEarly artificial neural networks encountered limitations in developing data-driven \ntheoretical models due to challenges in balancing function approximation accuracy and \ngradient stability while increasing network depth. Advances in deep neural networks \nenabled reinforcement learning (RL) algorithm progression[43], facilitating deep \nreinforcement learning (DRL) development. Deep neural network architecture \nconstitutes a critical component in DRL, with empirical evidence indicating that an \nefficient network design enhances DRL performance. \nGiven the three-dimensional state space (i.e., price tensor), we conducted \ncomprehensive experiments comparing several mainstream deep neural architectures, \nincluding ResNet[46], VGG[47], and Vision Transformer (ViT)[48]. While ResNet \ndemonstrated superior performance in both training and backtesting phases, its \ncomputational overhead and slower training speed posed practical limitations. After \ncareful consideration of the trade-off between model performance and computational \nefficiency, we implemented the VGG architecture as our baseline network structure, as \nillustrated in Figure 3: \nInput\nData\nConv2D \n3×3\nPadding\n=2\nMax\nPool\n2×2\nConv2D \n3×3\nPadding\n=1\nConv2D \n3×3\nPadding\n=1\nMax\nPool\n2×2\nConv2D \n3×3\nPadding\n=1\nConv2D \n3×3\nPadding\n=1\nMax\nPool\n2×2\nrelu\nrelu\nrelu\nrelu\nrelu\nfc\nfc\nFlatten\nSoftmax\noutput_num =11\nrelu\nDrop\nout\n=0.1\nin=4,out=64\nin=64,out=64\nin=64,out=128\nin=128,out=128\nin=128,out=256\n128\n128\n \nFigure 3 Neural Network Structure \n \nIn Figure 3, 'in' specifies the input channel count, while 'out' defines the output \nchannel count. The network architecture incorporates 5 convolutional layers, each \nemploying 3×3 convolution kernels for feature extraction, complemented by Max Pool \nlayers for feature pooling. Post max pooling at the final convolutional layer, a Flatten \noperation transforms the feature data into a one-dimensional vector, followed by two \nfully connected (fc) layers executing linear processing, each containing 128 neurons. \nThe Actor network concludes with a softmax activation function, generating the action \nvector for asset weights, while the Critic network produces the value function output \nwithout activation functions. The Actor network's softmax output structure \naccommodates 11 assets (10 risky assets + 1 risk-free asset). \n \n5 EMPIRICAL TESTS \n5.1 Data Selection, Preprocessing and Assumptions \nThis study constructs an investment portfolio using randomly selected constituent \nstocks from the CSI300 index for empirical analysis. The portfolio consists of 1 risk-\nfree asset (cash) and 10 risky assets. The data is obtained from the Wind database's daily \ntrading records, with all prices forward-adjusted. The study restricts trading to one \ntransaction per stock per day. \nThis research implements random portfolio selection, departing from traditional \ninvestment theory's selective strategies based on liquidity, diversification, and other \nfactors. The methodology stems from the premise that an effective DRL model should \ndemonstrate adaptability across diverse portfolios, beyond carefully selected asset \ncombinations. Superior backtesting performance of randomly selected portfolios \nprovides empirical validation of the DRL model's decision-making capabilities and \ngeneralization effectiveness. This approach exemplifies DRL's fundamental advantage \nas a data-driven model: autonomous adaptation to market environments without manual \nasset screening procedures. \nFor asset selection, the study applies a single temporal criterion: assets must have \nbeen listed before December 31, 2012. This requirement reflects the data-driven nature \nof DRL models, which require substantial historical data for training. Extended listing \nhistories provide comprehensive trading data, enabling enhanced market feature \nlearning. In the random asset selection process, preference is given to stocks exhibiting \noverall upward price trends historically, as these patterns provide more meaningful \ntraining signals for long-only DRL models. The study assumes sufficient liquidity for \nrisky assets, immediate transaction execution, and negligible market impact from \ntrading activities. \n \n5.2 Performance Metrics, Backtesting Period and Comparative Optimization Models \nFollowing Zhang et al.[17, 40], this study incorporates performance metrics \nencompassing annualized average return E(R), annualized volatility Std(R), annualized \nSharpe ratio (Sharpe), annualized Sortino ratio (Sortino), maximum drawdown (MDD), \nCalmar ratio (Calmar), percentage of positive returns (%of+Ret), and average profit-\nloss ratio (Ave P/Ave L). \nThe research implements a six-month backtesting period to evaluate model \noptimization effectiveness. With 252 trading days per year, the selected 128 trading \ndays represent approximately six months of trading time. To address overfitting \nconcerns, the methodology adapts Wassname's [50] open-source github implementation, \nsampling 128 consecutive trading days randomly from the complete dataset for each \ntraining episode. The six-month backtesting period selection aligns with this sampling \nframework. \nFigure 4 depicts the training and testing sets. Price data undergoes standardization, \nwith each asset's price normalized by its final trading day opening price, enabling clear \ntrend visualization across portfolio assets with diverse price levels. \n \nFig4 Normalized Price Trends with Train-Test Split \n \nThe backtesting period encompasses exclusively out-of-sample data, independent \nfrom the training dataset. The agent (i.e., the investor) processes these data exclusively \nduring backtesting, without prior exposure to future price movements. Table 1 \ndelineates the specific time periods for training and backtesting data: \n \nTable 1 Data ranges for training and backtesting \nAsset \nTraining Period  \nTesting Period \nStock Portfolio \n03/17/2010 - 08/02/2023 \n 08/07/2023 - 02/20/2024 \n \nThe comparative analysis framework incorporates multiple established \noptimization models benchmarked against the DRL model. These models are \nimplemented through the Riskfolio-lib asset optimization package, maintaining default \nconfigurations across all comparative models with asset returns derived from closing \nprices. The optimization framework encompasses: Classic Mean Variance (MV), \nConditional Value at Risk (CVaR), Entropic Value at Risk (EVaR), Risk Parity (RP), \nHierarchical Risk Parity (HRP), Hierarchical Equal Risk Contribution (HERC), and \nNested Clustered Optimization (NCO). While these models support various objective \nfunctions including risk minimization (MinRisk), Sharpe ratio maximization (Sharpe), \nutility function maximization (Utility), and net asset value maximization (MaxRet), the \ncomparative analysis focuses exclusively on risk minimization and Sharpe ratio \nmaximization, given the subjective nature of utility functions and the empirical \nunderperformance of utility and net asset value maximization strategies. \nThe historical data window selection for comparative models adheres to the \nmethodological framework established by the original authors of EVaR[51] and \nHRP[12], employing 4-year and 1-year periods respectively. With an annual trading \ncalendar of 252 days, the 4-year period encompasses 1,008 trading days (252 * 4). \nGiven that these quantitative optimization models conceptualize asset weight \nmodifications as static processes, disregarding weight dynamics in continuous trading, \nthe study implements a rolling window methodology for weight prediction. Specifically, \nweight predictions for September 1, 2021, utilize historical data from the preceding 4 \nor 1 years through August 31, 2021, with this process continuing throughout the \nbacktesting period. Transaction costs emanating from weight adjustments are computed \nusing formula 10, maintaining consistency with the transaction cost parameters \nestablished in the DRL model. \n \n5.3 Training Results and Robustness Testing \nDeep Reinforcement Learning (DRL) constitutes a novel sequential statistical \ndecision-making methodology that leverages neural networks to model and estimate \nconditional probability distributions and expected returns across the state-action space. \nAt each timestep, the agent executes online statistical inference based on current \nobservations while optimizing its decision strategy through systematic exploration and \nexperience accumulation, implementing an iterative statistical learning process \ndesigned to maximize expected cumulative rewards. This methodology integrates the \nfunction approximation capabilities of deep learning with the sequential decision-\nmaking framework of reinforcement learning, establishing an end-to-end statistical \nmodeling and optimization approach. \nWhile traditional econometric testing methods based on linear assumptions \nstruggle to effectively evaluate the statistical significance of DRL models, the \nconvergence of training rewards provides a more appropriate evaluation criterion. The \nconvergence behavior indicates the agent's ability to consistently generate profits in \nhistorical market environments, serving as a necessary condition for model stability and \nrobustness. This necessity stems from DRL's non-linear nature and its adaptive learning \nmechanisms - a converged reward function demonstrates the model has learned stable \npatterns rather than overfitting to market noise. Furthermore, reward convergence \nimplies the agent has developed a generalizable strategy that maintains consistent \nperformance across various market conditions within the training distribution. \nFigure 5 Training rewards \n \nAs demonstrated in Figure 5, the model underwent training for 9 million steps. \nThe agent's acquired rewards exhibit a positive correlation with the progression of \ntraining steps, indicating systematic improvement in the training process and agent \nperformance. Throughout the training period, the reward values demonstrate \nconvergence, with the annualized Sharpe ratio stabilizing within the range of -0.3 to 0.8, \nand the predominant portion of training reward values maintaining convergence above \nzero. These results indicate that the agent demonstrates consistent return generation \ncapability within the known environment, supporting the robustness of the trained \nmodel. \n \n \n5.4 Backtesting results \n5.4.1 Portfolio Value, Asset Allocation and Trading Costs \n \n \n \nFigure 6 DRL Portfolio Value, Asset Allocation and Trading Costs  \n(2023.08-2024.02) \n \nThe upper panel of Figure 6 presents the relative prices of assets (calculated \naccording to formula 4). The relative price can be viewed as a standardization method \nto adjust the portfolio assets' prices to the same scale. The lower panels display the \nportfolio's asset weights and transaction costs respectively. \nThe DRL portfolio value demonstrated a consistent upward trajectory throughout \nthe backtesting period, appreciating from an initial value of 1.0 to 1.1256, generating a \ntotal return of 12.56%. Two significant upward movements materialized during mid-\nNovember to December 2023 and late January 2024. Despite experiencing a drawdown \nin mid-September 2023, where the portfolio value temporarily declined to \napproximately 0.98, the drawdown magnitude remained modest, followed by robust \nrecovery momentum. \nRegarding weight allocation, DRL implemented a robust asset allocation strategy. \nThe portfolio comprises cash and 10 stocks, with initial allocations approximating \nuniform distribution at 0.0909 per asset. Throughout the trading period, DRL \nmaintained consistent weight adjustment patterns, with weight standard deviation \nstabilizing between 0.031 and 0.033, demonstrating effective diversification \ncharacteristics. The cash position fluctuated within a narrow range of 0.11 to 0.12, \nensuring adequate portfolio liquidity. In terms of stock weights, CN002027.SZ and \nCN002371.SZ exhibited relatively active weight adjustments, while CN600029.SH and \nCN600183.SH maintained consistently lower allocation ratios. \nThroughout the backtesting period, the strategy exhibited efficient transaction cost \nmanagement. Substantial transaction costs (approximately 0.22%) were incurred solely \nduring initial capital allocation, with subsequent transaction cost rates maintaining \nminimal levels between 0.002% and 0.01% on most trading days. This performance \nindicates the implementation of a measured trading approach in asset allocation \nadjustments, effectively mitigating the impact of transaction costs on portfolio returns. \nThe PPO algorithm-based portfolio demonstrated favorable risk-return \ncharacteristics, \ngenerating \npositive \ninvestment \nreturns \nthrough \nmaintained \ndiversification and dynamic weight adjustments, while effectively managing downside \nrisk. These results suggest significant potential for deep reinforcement learning \napplications in portfolio management. \n \n5.4.2 Performance Comparison of Stock Optimization Models \nTable 2 presents the performance comparison of various stock optimization \nmodels. Models are designated according to the convention \"model type-optimization \nobjective-data window\", illustrated as follows: \n \nMV-MinRisk: Mean-variance model, with risk minimization as the \noptimization objective, employing a 1-year historical data rolling window. \n \nCVaR-Sharpe-4yr: Conditional Value-at-Risk model, with Sharpe ratio \nmaximization as the optimization objective, employing a 4-year historical \ndata rolling window. \nAdditional models follow this naming convention. \n \nTable 2 Results of various optimization models for stocks \n \nE(R) \nStd(R) \nSharpe \nSortino \nMDD \nCalmar \n%of+Ret \n𝐀𝐀𝐀𝐀𝐀𝐀. 𝐏𝐏\n𝐀𝐀𝐀𝐀𝐀𝐀. 𝐋𝐋 \nDRL \n0.1956 \n0.1258 \n1.5550 \n2.9567 \n5.85% \n3.3395 \n0.4728 \n1.4204 \nMV-MinRisk \n0.0892 \n0.1158 \n0.7707 \n1.2357 \n5.88% \n1.5175 \n0.5156 \n1.0641 \nMV-MinRisk-4yr \n0.1016 \n0.1195 \n0.8501 \n1.4115 \n6.37% \n1.5940 \n0.4843 \n1.2285 \nMV-Sharpe \n0.0738 \n0.1584 \n0.4662 \n0.7835 \n8.20% \n0.8998 \n0.4609 \n1.2726 \nMV-Sharpe-4yr \n0.0283 \n0.1288 \n0.2199 \n0.3920 \n9.41% \n0.3011 \n0.4687 \n1.1738 \nCVaR-MinRisk \n0.0817 \n0.1155 \n0.7073 \n1.1312 \n6.26% \n1.3039 \n0.5156 \n1.0566 \nCVaR-MinRisk-4yr \n0.1508 \n0.1227 \n1.2290 \n2.0967 \n5.38% \n2.8041 \n0.4843 \n1.3101 \nCVaR-Sharpe \n0.0326 \n0.1772 \n0.1840 \n0.2844 \n9.01% \n0.3618 \n0.4531 \n1.2492 \nCVaR-Sharpe-4yr \n0.0049 \n0.1349 \n0.0367 \n0.0638 \n10.80% \n0.0459 \n0.4609 \n1.1763 \nEVaR-MinRisk \n0.0350 \n0.1187 \n0.2953 \n0.4870 \n7.92% \n0.4423 \n0.4843 \n1.1175 \nEVaR-MinRisk-4yr \n0.1450 \n0.1297 \n1.1172 \n1.9260 \n6.13% \n2.3635 \n0.5312 \n1.0660 \nEVaR-Sharpe \n-0.0007 \n0.1742 \n-0.0042 \n-0.0059 \n9.16% \n-0.0080 \n0.4218 \n1.3692 \nEVaR-Sharpe-4yr \n0.0339 \n0.1392 \n0.2435 \n0.4341 \n9.40% \n0.3607 \n0.4609 \n1.2154 \nRP-MinRisk \n-0.0231 \n0.1342 \n-0.1721 \n-0.2736 \n9.38% \n-0.2461 \n0.4453 \n1.2106 \nRP-MinRisk-4yr \n-0.0154 \n0.1318 \n-0.1169 \n-0.1833 \n9.20% \n-0.1675 \n0.4375 \n1.2606 \nRP-Sharpe \n-0.0231 \n0.1342 \n-0.1721 \n-0.2736 \n9.38% \n-0.2461 \n0.4453 \n1.2106 \nRP-Sharpe-4yr \n-0.0154 \n0.1318 \n-0.1169 \n-0.1833 \n9.20% \n-0.1675 \n0.4375 \n1.2606 \nHRP \n0.0359 \n0.1241 \n0.2893 \n0.4553 \n7.19% \n0.4988 \n0.4609 \n1.2269 \n \nAdopting Ernest P. Chan[52]'s approach of utilizing the Sharpe ratio as the primary \nmetric, Table 2 demonstrates that the Deep Reinforcement Learning (DRL) model \nexhibits significant advantages in portfolio optimization. The model achieved the \nhighest annualized average return of 19.56%, maintained moderate volatility (12.58%), \nand notably outperformed other models with a Sharpe ratio of 1.5550, demonstrating \nsuperior risk-adjusted returns. The model attained the highest Sortino ratio (2.9567), \nvalidating its exceptional performance when considering downside risk. In terms of risk \nmanagement, the DRL model exhibited a maximum drawdown of merely 5.85%, \napproaching optimal levels, while achieving the highest Calmar ratio (3.3395), \nindicating superior downside risk management capabilities. \nAmong traditional optimization models, CVaR-MinRisk-4yr demonstrated \noptimal performance, generating a 15.08% annualized return and the lowest maximum \ndrawdown (5.38%). This model achieved a Sharpe ratio of 1.2290, which, although \nlower than the DRL model, represented the highest among traditional models. The \nsuperior performance may be attributed to its 4-year lookback period design, facilitating \nmore stable historical data analysis. Notably, models implementing the MinRisk \nstrategy consistently outperformed their Sharpe strategy counterparts, suggesting \nsuperior effectiveness of risk minimization compared to risk-adjusted return \nmaximization in the current market environment. \nSignificantly, Risk Parity (RP) series models and Hierarchical Equal Risk \nContribution (HERC) strategy exhibited suboptimal performance. RP models \nconsistently generated negative returns and Sharpe ratios, while HERC models \nrecorded a maximum negative return of -10.07% and maximum drawdown of 11.86%, \nindicating that excessive dependence on historical correlations or simplified risk \nallocation methodologies may insufficiently address market dynamics. \nFrom a trading efficiency perspective, the DRL model achieved superior \nperformance with an investment win rate of 47.28% and optimal average gain-loss ratio \nof 1.4204, demonstrating proficiency in both market opportunity capture and loss \nmitigation. Comprehensively, whether assessed through the primary Sharpe ratio metric \nor alternative risk-adjusted return indicators, the DRL model effectively integrated \nreturn generation capabilities with risk management efficiency, surpassing traditional \nmethodologies across all dimensions while exhibiting adaptability and robustness in \nHRP-4yr \n0.03100 \n0.1243 \n0.2492 \n0.3946 \n7.98% \n0.3880 \n0.4843 \n1.1106 \nHERC \n-0.1007 \n0.1372 \n-0.7339 \n-1.1000 \n11.86% \n-0.8488 \n0.4375 \n1.1361 \nHERC-4yr \n0.0288 \n0.1249 \n0.2310 \n0.3630 \n7.99% \n0.3610 \n0.4687 \n1.1787 \nNCO-MinRisk \n0.0768 \n0.1155 \n0.6652 \n1.0334 \n6.00% \n1.2806 \n0.5 \n1.1162 \nNCO-MinRisk-4yr \n0.1064 \n0.1188 \n0.8961 \n1.4863 \n5.99% \n1.7750 \n0.4843 \n1.2404 \nNCO-Sharpe \n0.0529 \n0.1600 \n0.3310 \n0.5574 \n8.07% \n0.6557 \n0.4531 \n1.2816 \nNCO-Sharpe -4yr \n0.0155 \n0.1265 \n0.1228 \n0.2186 \n9.89% \n0.1571 \n0.4609 \n1.1926 \ncomplex market environments. These findings validate the application potential of deep \nreinforcement learning in finance, particularly its efficacy in portfolio management \nrequiring dynamic decision-making and multi-objective optimization. \n \nFigure 7 Performance Comparison between DRL and Other Optimization Models \n \nTo facilitate quantitative comparison, this study examines the top 11 traditional \noptimization models ranked by annualized Sharpe ratio in comparison with the DRL \nmodel (as shown in Figure 7). The empirical results indicate that throughout the \nbacktesting period, the DRL model exhibits superior performance relative to traditional \noptimization models, as evidenced by the following findings: \ni. \nReturn metrics: The DRL model consistently outperforms traditional \noptimization models across the backtesting period, generating substantial \npositive returns. Within the traditional optimization framework, CVaR-\nMinRisk-4yr, EVaR-MinRisk-4yr, NCO-MinRisk-4yr, and MV-MinRisk-4yr \nyield the highest performance metrics, yet remain below those of the DRL \nmodel. \nii. \nStrategic characteristics: Traditional optimization models demonstrate strong \nhomogeneity, with portfolio value trajectories following virtually identical \npatterns post-optimization. In contrast, the DRL-optimized portfolio value \ntrajectories display distinct patterns from those of traditional optimization \napproaches. This suggests that the data-driven DRL framework demonstrates \nenhanced capability in capturing asset price dynamics, leading to more \nefficient asset weight allocation. \nEmpirical evidence from extensive experimental studies indicates that the DRL \nmodel exhibits substantial efficacy in portfolio optimization of CSI300 constituent \nstocks. This performance can be attributed to two key factors: firstly, the backtesting \nperiod coincided with an upward trajectory of CSI300 constituent stocks, creating \nfavorable conditions for long-strategy validation; secondly, CSI300 constituent stocks \nmaintain a stable investor composition characterized by a higher concentration of \ninstitutional investors, resulting in more systematic investment patterns. Relative to \nsmall and medium-cap segments, the price and trading data of CSI300 constituent \nstocks demonstrate enhanced reliability in reflecting market fundamentals and investor \nsentiment, thereby establishing a more robust learning environment for the DRL model. \nThese structural characteristics facilitate improved learning and market adaptation \ncapabilities of the DRL model, leading to enhanced performance in CSI300 constituent \nstock portfolio optimization. \n \n6 CONCLUSIONS \nTraditional financial econometric optimization models demonstrate inherent \nlimitations in portfolio asset allocation due to their static frameworks and homogeneous \ncharacteristics. While these approaches facilitate theoretical analysis, they struggle to \ncapture the dynamic evolution of asset weights in real trading environments and show \ninsufficient adaptability to market volatility. \nThis research advances the application of Deep Reinforcement Learning (DRL) in \nportfolio optimization through two key innovations. First, we introduce a novel Sharpe \nratio reward function specifically designed for Actor-Critic DRL algorithms, achieving \nstable convergence during training and consistently positive Sharpe ratios in practice. \nSecond, we develop a comprehensive DRL methodology integrating VGG-based neural \nnetworks for multi-dimensional financial data processing with random sampling \nstrategies, effectively enhancing model generalization while minimizing overfitting \nrisk. Empirical results demonstrate the superior performance of our DRL framework \ncompared to traditional optimization models across multiple metrics. The DRL model \nconsistently achieves higher returns than leading traditional approaches including \nCVaR-MinRisk-4yr and EVaR-MinRisk-4yr, while exhibiting distinct portfolio value \ntrajectories that indicate enhanced capability in capturing asset price dynamics. \nWith the continuous advancement of deep reinforcement learning (DRL) theory, \nits applications in finance are demonstrating significant potential. DRL optimization \nmethods integrate knowledge across multiple disciplines including machine learning, \nfinance, and statistics. This interdisciplinary convergence provides novel research \nperspectives and methodological foundations for portfolio optimization. We observe \nthat researchers frequently underemphasize the importance of DRL environment \nmodeling when applying DRL models to portfolio optimization. The modeling of DRL \nenvironments encompasses the operational logic of the entire model. The innovative \naverage Sharpe ratio reward function introduced in this paper is implemented within \nthe environment modeling framework, and the quality of environment modeling \ndirectly impacts model performance. Furthermore, financial data inherently contains \nsubstantial noise. The effective utilization of this data for environment modeling in \nDRL and extraction of valuable trading signals is crucial for enhancing model \nperformance. \nLooking forward, research directions should focus not only on optimizing DRL \nalgorithms but also on deeply exploring innovative approaches to environment \nmodeling. As a core component of the DRL framework, environment modeling directly \ndetermines the model's capacity to perceive and adapt to market dynamics, which is of \ndecisive importance for increasing the practical application value of DRL in portfolio \noptimization. \n \nREFERENCES \n[1] H. M. Markowitz, “Portfolio selection,” J. Financ., vol. 7, no. 1, p. 77, 1952. \n[2] P. A. Samuelson, “Lifetime portfolio selection by dynamic stochastic programming,” Rev. Econ. \nStat., vol. 51, no. 3, p. 239, 1969. \n[3] J. L. Kelly, “A new interpretation of information rate,” Bell Syst. Tech. J., vol. 35, no. 4, pp. 917–\n926, 1956. \n[4] R. C. Merton, “Optimum consumption and portfolio rules in a continuous-time model,” in \nStochastic optimization models in finance, Elsevier, 1975, pp. 621–661. \n[5] F. Black and R. Litterman, “Global asset allocation with equities, bonds, and currencies,” Fixed \nIncome Res., vol. 2, no. 15–28, pp. 1–44, 1991. \n[6] F. Black and R. Litterman, “Global portfolio optimization,” Financ. Anal. J., vol. 48, no. 5, pp. \n28–43, 1992. \n[7] A. Charnes, W. W. Cooper, and E. Rhodes, “Measuring the efficiency of decision making units,” \nEur. J. Oper. Res., vol. 2, no. 6, pp. 429–444, 1978. \n[8] S. Kirkpatrick, C. D. Gelatt Jr, and M. P. Vecchi, “Optimization by simulated annealing,” Science, \nvol. 220, no. 4598, pp. 671–680, 1983. \n[9] S. Arnone, A. Loraschi, A. Tettamanzi, and Others, “A genetic approach to portfolio selection,” \nNeural Netw. World, vol. 3, no. 6, pp. 597–604, 1993. \n[10] R. T. Rockafellar, S. Uryasev, and Others, “Optimization of conditional value-at-risk,” J. Risk, vol. \n2, pp. 21–42, 2000. \n[11] E. Qian, “Risk parity and diversification,” J. Invest., vol. 20, no. 1, pp. 119–127, 2011. \n[12] M. L. De Prado, “Building diversified portfolios that outperform out of sample,” J. Portfolio \nManage., vol. 42, no. 4, pp. 59–69, 2016. \n[13] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient methods for \nreinforcement learning with function approximation,” in Advances in Neural Information \nProcessing Systems, S. Solla, T. Leen, and K. Müller, Eds., MIT Press, 1999. \n[14] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement \nlearning,” Mach. Learn., vol. 8, pp. 229–256, 1992. \n[15] J. Moody, “Reinforcement learning for trading,” Adv. Neur. In., vol. 11, pp. 917–923, 1999. \n[16] M. A. Dempster and V. Leemans, “An automated FX trading system using adaptive \nreinforcement learning,” Expert Syst. Appl., vol. 30, no. 3, pp. 543–552, 2006. \n[17] Z. Zhang, S. Zohren, and R. Stephen, “Deep reinforcement learning for trading,” J. Financ. Data \nSci., 2020. \n[18] R. Neuneier, “Enhancing Q-learning for optimal asset allocation,” Adv. Neur. In., vol. 10, 1997. \n[19] X. Gao and L. Chan, “An algorithm for trading and portfolio management using q-learning and \nsharpe ratio maximization,” in Proceedings of the international conference on neural \ninformation processing, Citeseer, 2000, pp. 832–837. \n[20] J. W. Lee, J. Park, O. Jangmin, J. Lee, and E. Hong, “A multiagent approach to q -learning for \ndaily stock trading,” IEEE Trans. Syst. Man Cybern. Part A Syst. Humans, vol. 37, no. 6, pp. 864–\n877, 2007. \n[21] X. Wu, H. Chen, J. Wang, L. Troiano, V. Loia, and H. Fujita, “Adaptive stock trading strategies \nwith deep reinforcement learning methods,” Inf. Sci., vol. 538, pp. 142–158, Oct. 2020, doi: \n10.1016/j.ins.2020.05.066. \n[22] P. Liu, Y. Zhang, F. Bao, X. Yao, and C. Zhang, “Multi-type data fusion framework based on deep \nreinforcement learning for algorithmic trading,” Appl. Intell., vol. 53, no. 2, pp. 1683–1706, Jan. \n2023, doi: 10.1007/s10489-022-03321-w. \n[23] Z. Pourahmadi, D. Fareed, and H. R. Mirzaei, “A novel stock trading model based on \nreinforcement learning and technical analysis,” Ann. Data Sci., vol. 11, no. 5, pp. 1653–1674, \nOct. 2024, doi: 10.1007/s40745-023-00469-1. \n[24] V. Kochliaridis, E. Kouloumpris, and I. Vlahavas, “Combining deep reinforcement learning with \ntechnical analysis and trend monitoring on cryptocurrency markets,” Neural Comput. Appl., \nvol. 35, no. 29, pp. 21445–21462, Oct. 2023, doi: 10.1007/s00521-023-08516-x. \n[25] H. Wang and X. Y. Zhou, “Continuous-time mean–variance portfolio selection: a reinforcement \nlearning framework,” Math. Finance, vol. 30, no. 4, pp. 1273–1308, 2020, doi: \n10.1111/mafi.12281. \n[26] Y. Jiang, J. Olmo, and M. Atwi, “Deep reinforcement learning for portfolio selection,” Glob. \nFinance J., vol. 62, p. 101016, Sep. 2024, doi: 10.1016/j.gfj.2024.101016. \n[27] Z. Jiang, D. Xu, and J. Liang, “A deep reinforcement learning framework for the financial \nportfolio \nmanagement \nproblem,” \nJul. \n16, \n2017, \narXiv: \narXiv:1706.10059. \ndoi: \n10.48550/arXiv.1706.10059. \n[28] M. Ormos and A. Urbán, “Performance analysis of log-optimal portfolio strategies with \ntransaction costs,” Quant. Finance, Oct. 2013. \n[29] Z. Liang, H. Chen, J. Zhu, K. Jiang, and Y. Li, “Adversarial deep reinforcement learning in \nportfolio management,” arXiv prepr. arXiv:1808,09940, 2018. \n[30] J. Jang and N. Seong, “Deep reinforcement learning for stock portfolio optimization by \nconnecting with modern portfolio theory,” Expert Syst. Appl., vol. 218, p. 119556, May 2023, \ndoi: 10.1016/j.eswa.2023.119556. \n[31] H. S. QI Yue1，2，3, “Portfolio management based on DDPG algorithm of deep reinforcement \nlearning,” Comput. Mod., vol. 0, no. 5, p. 93, 27AD. \n[32] M.-E. Wu, J.-H. Syu, J. C.-W. Lin, and J.-M. Ho, “Portfolio management system in equity market \nneutral using reinforcement learning,” Appl. Intell., vol. 51, no. 11, pp. 8119–8131, Nov. 2021, \ndoi: 10.1007/s10489-021-02262-0. \n[33] S. Almahdi and S. Y. Yang, “An adaptive portfolio trading system: a risk-return portfolio \noptimization using recurrent reinforcement learning with expected maximum drawdown,” \nExpert Syst. Appl., vol. 87, pp. 267–279, Nov. 2017, doi: 10.1016/j.eswa.2017.06.023. \n[34] A. M. Aboussalah and C.-G. Lee, “Continuous control with stacked deep dynamic recurrent \nreinforcement learning for portfolio optimization,” Expert Syst. Appl., vol. 140, p. 112891, Feb. \n2020, doi: 10.1016/j.eswa.2019.112891. \n[35] Q. Y. E. Lim, Q. Cao, and C. Quek, “Dynamic portfolio rebalancing through reinforcement \nlearning,” Neural Comput & Applic, vol. 34, no. 9, pp. 7125–7139, May 2022, doi: \n10.1007/s00521-021-06853-3. \n[36] Z. Wei, D. Chen, Y. Zhang, D. Wen, X. Nie, and L. Xie, “Deep reinforcement learning portfolio \nmodel based on mixture of experts,” Appl. Intell., vol. 55, no. 5, pp. 1–16, Apr. 2025, doi: \n10.1007/s10489-025-06242-6. \n[37] H. Li and M. Hai, “Deep reinforcement learning model for stock portfolio management based \non data fusion,” Neural Process. Lett., vol. 56, no. 2, pp. 1–24, Apr. 2024, doi: 10.1007/s11063-\n024-11582-4. \n[38] M.-Y. Day, C.-Y. Yang, and Y. Ni, “Portfolio dynamic trading strategies using deep reinforcement \nlearning,” Soft Comput., vol. 28, no. 15, pp. 8715–8730, Aug. 2024, doi: 10.1007/s00500-023-\n08973-5. \n[39] D. Ramya and Suresha, “Reinforcement learning driven trading algorithm with optimized stock \nportfolio management scheme to control financial risk,” SN Comput. Sci., vol. 6, no. 1, pp. 1–\n16, Jan. 2025, doi: 10.1007/s42979-024-03555-0. \n[40] S. Liu, B. Wang, H. Li, C. Chen, and Z. Wang, “Continual portfolio selection in dynamic \nenvironments via incremental reinforcement learning,” Int. J. Mach. Learn. Cybern., vol. 14, no. \n1, pp. 269–279, Jan. 2023, doi: 10.1007/s13042-022-01639-y. \n[41] S. Yang, “Deep reinforcement learning for portfolio management,” Knowledge-Based Syst., vol. \n278, p. 110905, Oct. 2023, doi: 10.1016/j.knosys.2023.110905. \n[42] R. Sun, A. Stefanidis, Z. Jiang, and J. Su, “Combining transformer based deep reinforcement \nlearning with black-litterman model for portfolio optimization,” Neural Comput. Appl., vol. 36, \nno. 32, pp. 20111–20146, Nov. 2024, doi: 10.1007/s00521-024-09805-9. \n[43] V. Mnih et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, \nno. 7540, pp. 529–533, 2015. \n[44] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust region policy optimization,” \nApr. 20, 2017, arXiv: arXiv:1502.05477. doi: 10.48550/arXiv.1502.05477. \n[45] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization \nalgorithms,” Aug. 28, 2017, arXiv: arXiv:1707.06347. doi: 10.48550/arXiv.1707.06347. \n[46] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in \nProceedings of the IEEE conference on computer vision and pattern recognition (CVPR), Jun. \n2016. \n[47] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image \nrecognition,” Apr. 10, 2015, arXiv: arXiv:1409.1556. doi: 10.48550/arXiv.1409.1556. \n[48] A. Dosovitskiy et al., “An image is worth 16x16 words: transformers for image recognition at \nscale,” Jun. 03, 2021, arXiv: arXiv:2010.11929. doi: 10.48550/arXiv.2010.11929. \n[49] Z. Zhang, S. Zohren, and S. Roberts, “Deep learning for portfolio optimization,” J. Financ. Data \nSci., vol. 2, no. 4, pp. 8–20, Oct. 2020, doi: 10.3905/jfds.2020.1.042. \n[50] Wassname, “Reinforcement learning for portfolio management.” Year of Access. [Online]. \nAvailable: https://github.com/wassname/rl-portfolio-management \n[51] D. Cajas, “Entropic portfolio optimization: a disciplined convex programming framework,” Feb. \n24, \n2021, \nSocial \nScience \nResearch \nNetwork, \nRochester, \nNY: \n3792520. \ndoi: \n10.2139/ssrn.3792520. \n[52] E. P. Chan, Quantitative trading: how to build your own algorithmic trading business. John \nWiley & Sons, 2021. \n \n \n \nAPPENDIX \nResults of other optimization models for Stocks \nColumn 1: Portfolio value; Column 2: Asset weights; Column 3: Transaction cost rate \n \n \n \n",
  "categories": [
    "q-fin.PM"
  ],
  "published": "2024-12-24",
  "updated": "2025-02-20"
}