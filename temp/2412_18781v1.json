{
  "id": "http://arxiv.org/abs/2412.18781v1",
  "title": "Robustness Evaluation of Offline Reinforcement Learning for Robot Control Against Action Perturbations",
  "authors": [
    "Shingo Ayabe",
    "Takuto Otomo",
    "Hiroshi Kera",
    "Kazuhiko Kawamoto"
  ],
  "abstract": "Offline reinforcement learning, which learns solely from datasets without\nenvironmental interaction, has gained attention. This approach, similar to\ntraditional online deep reinforcement learning, is particularly promising for\nrobot control applications. Nevertheless, its robustness against real-world\nchallenges, such as joint actuator faults in robots, remains a critical\nconcern. This study evaluates the robustness of existing offline reinforcement\nlearning methods using legged robots from OpenAI Gym based on average episodic\nrewards. For robustness evaluation, we simulate failures by incorporating both\nrandom and adversarial perturbations, representing worst-case scenarios, into\nthe joint torque signals. Our experiments show that existing offline\nreinforcement learning methods exhibit significant vulnerabilities to these\naction perturbations and are more vulnerable than online reinforcement learning\nmethods, highlighting the need for more robust approaches in this field.",
  "text": "ROBUSTNESS EVALUATION OF OFFLINE REINFORCEMENT\nLEARNING FOR ROBOT CONTROL AGAINST ACTION\nPERTURBATIONS\nShingo Ayabe, Takuto Otomo\nGraduate School of Science and Engineering\nChiba University\nChiba, Japan\n{ayabe.shingo, takutootomo}@chiba-u.jp\nHiroshi Kera, Kazuhiko Kawamoto\nGraduate School of Informatics\nChiba University\nChiba, Japan\nkera@chiba-u.jp, kawa@faculty.chiba-u.jp\nABSTRACT\nOffline reinforcement learning, which learns solely from datasets without environmental interaction,\nhas gained attention. This approach, similar to traditional online deep reinforcement learning, is\nparticularly promising for robot control applications. Nevertheless, its robustness against real-world\nchallenges, such as joint actuator faults in robots, remains a critical concern. This study evaluates the\nrobustness of existing offline reinforcement learning methods using legged robots from OpenAI Gym\nbased on average episodic rewards. For robustness evaluation, we simulate failures by incorporating\nboth random and adversarial perturbations, representing worst-case scenarios, into the joint torque\nsignals. Our experiments show that existing offline reinforcement learning methods exhibit significant\nvulnerabilities to these action perturbations and are more vulnerable than online reinforcement\nlearning methods, highlighting the need for more robust approaches in this field.\nKeywords Offline Reinforcement Learning · Action-Space Perturbations · Adversarial Attack · Testing-Time\nRobustness · Robot Control\n1\nIntroduction\nOffline reinforcement learning (offline RL) [1] has emerged as a novel approach across various domains like health-\ncare [2], energy management [3], and robot control [4]. Unlike online RL, which requires direct interaction with\nenvironments, offline RL learns solely from pre-collected datasets without environmental interaction. Hence, offline RL\ncan potentially reduce costs and risks associated with frequent environmental interactions.\nFor offline RL to be more widely applicable, robustness is an essential requirement. The robustness of offline RL can\nbe broadly categorized into two types: training-time robustness [5–7] and testing-time robustness [8–14]. Training-time\nrobustness focuses on effectively training policies with corrupted datasets, which may include perturbations caused by\ndata logging errors and intentional data augmentation [5,15]. Testing-time robustness, on the other hand, refers to the\nability to handle discrepancies between training and deployment environments. In robot control, these discrepancies\ninclude state perturbations (e.g., sensor noise) [16,17] and action perturbations (e.g., mismatched friction coefficients or\nactuator failures) [18–20]. Online RL addresses similar challenges, often referred to as the reality gap, through methods\nlike domain randomization [21–23] and adversarial training [18, 24–27]. However, whether these solutions can be\neffectively applied to offline RL remains unclear.\nOnline RL benefits from extensive exploration across diverse state-action pairs via direct environmental interactions,\nenabling broad coverage of the state-action space. In contrast, offline RL is restricted to exploring only the state-action\npairs available in the pre-collected training dataset, rendering its performance highly dependent on the dataset’s quality\nand coverage. Moreover, offline RL relies on conservative training to mitigate the overestimation of action values\ncaused by out-of-distribution actions [28].\narXiv:2412.18781v1  [cs.RO]  25 Dec 2024\nAyabe et al.\nExisting studies [12–14] on testing-time robustness in offline RL have primarily focused on state-space perturbations.\nHowever, in real-world scenarios, especially in robotics, action perturbations, such as actuator failures, are equally\ncritical but remain unexplored. These perturbations result in deviations from the intended behavior dictated by the\npolicy network, posing significant challenges to the reliability of offline RL systems.\nThis study investigates the testing-time robustness of existing offline RL methods [29–31] against random and adversarial\naction perturbations [19]. Specifically, to evaluate the robustness of these methods, we simulate actuator failures by\nperturbing the actions generated by the policy network. These offline RL methods limit exploration to remain within the\ntraining data distribution, and we hypothesize this limitation renders them vulnerable to unencountered perturbations.\nThis vulnerability is particularly critical in the case of adversarial perturbations, which are intentionally crafted to\nexploit the system under worst-case failure scenarios. Adversarial perturbations underscore the need to understand\ntheir effects to develop reliable control systems. However, the impact of action-space perturbations on the robustness in\noffline RL remains yet to be fully explored.\nOur approach involves simulating operational challenges within the MuJoCo physical simulation environment [32],\nfocusing on forward walking tasks. We evaluate the testing-time robustness of offline RL methods using the average\nepisodic reward. Experimental results reveal that the existing methods are highly vulnerable to action perturbations.\nFurthermore, inspired by typical defensive strategies in online RL—training in environments with added perturbations—–\nwe train policies on datasets augmented with action perturbations and evaluate their robustness. The results, however,\ndemonstrate no significant improvement, highlighting the need for more robust offline RL methods to address failures\neffectively. The contributions of this paper are as follows:\n• We evaluate the testing-time robustness of the offline RL methods against random and adversarial action\nperturbations and reveal that the offline RL models are more vulnerable to action perturbations than online RL.\n• We find that the testing-time robustness of the offline RL models depends on the state-action coverage of the\ntraining dataset.\n• We show that training with action-perturbed datasets alone does not improve the testing-time robustness of\noffline RL against action perturbations.\n2\nRelated work\n2.1\nOffline reinforcement learning\nIn offline training, Q-values tend to be overestimated for unseen state-action pairs that are not represented in the\ndataset [1]. Since these overestimation cannot be corrected through direct interaction with the environment, traditional\nonline RL methods face significant challenges when training on static datasets [29]. To address these overestimation\nissues, various offline RL techniques have been proposed [29–31,33]. This study specifically evaluates the robustness\nof two common strategies for offline RL: policy constraints [29,30] and regularization [31].\nBatch-constrained Q-learning (BCQ) [29] utilizes policy constraints to ensure the learned policy remains close to\nthe behavior policy that generates the training data. Similarly, the Twin-Delayed DDPG with behavior cloning\n(TD3+BC) [30], an offline variant of TD3 [34], implicitly constrains the learned policy by adding a regularization term\nto the objective function. These policy constraints effectively limit the action space, preventing the agent from exploring\nunrealistic or unencountered actions in the dataset. Implicit Q-learning (IQL) [31] regularizes the value function update\nprocess, relying on the state value distribution from the dataset rather than the learned policy. This regularization\nmitigates the risk of overestimating Q-values, ensuring that the resulting policy remains within the empirical data\ndistribution. However, both policy constraints and regularization strategies may lose robustness against random and\nadversarial perturbations because these perturbations are not included in the dataset.\n2.2\nRobust reinforcement learning\nThe robustness of RL, especially in robot control, mainly stems from addressing the reality gap. This gap arises from\ndiscrepancies in physical parameters such as friction coefficients, terrain geometry, and mechanical failures. One\napproach to address the reality gap involves leveraging the Robust Markov Decision Process (RMDP) framework [35–\n39], which relies on a set of state transition models known as uncertainty set. RMDP seeks to derive an optimal\nstrategy by considering the worst-case scenarios within this uncertainty set. However, the high computational cost\nof RMDP makes it impractical for large state-action spaces. A more practical approach to bridging this gap involves\nintroducing perturbations during the training process. Domain randomization [21–23] generates diverse environments by\nrandomizing parameters such as friction coefficients, thereby training the model to adapt to these changes. Alternatively,\nadversarial training [18,24–27] improves robustness by introducing adversarial attacks that intentionally reduce rewards.\n2\nAyabe et al.\nFigure 1: Overview of the robustness evaluation for offline RL. Offline RL models are trained on varying-quality offline\ndatasets and evaluated on forward walking tasks in diverse robot environments. Testing-time robustness is evaluated\nby average episodic rewards under three conditions: normal (no perturbations), random, or adversarial perturbations\napplied to actions. Adversarial perturbations, generated using the differential evolution algorithm, minimize the average\nepisodic reward over multiple generations in the same robot environment. These adversarial perturbations are also\nused to create action-perturbed datasets, which are tested to determine whether training with such datasets improves\ntesting-time robustness.\nDifferential evolution has also been employed to generate adversarial perturbations [19], leading to significant reward\nreductions in walking robot scenarios. Curriculum-based adversarial learning [24] enhances robustness by applying\nadversarial attacks to the action space.\nIn offline RL, robustness can be categorized into training and testing time robustness. Training-time robustness\naddresses corrupted training data [5–7]. For example, IQL is more robust than other offline RL methods for training-\ntime robustness [7]. Furthermore, a variant, Robust-IQL, incorporates normalization, Huber loss, and quantile Q\nestimator to enhance resistance against corrupted transitions. Several studies have focused on the testing-time robustness\nin an environment with perturbations that are not represented in training datasets [8–14]. In relation to our work, Yan et\nal., [12] enhances robustness against adversarial attacks in the state space by smoothing the action value distribution\nand employing conservative value estimates. Liu et al., [13] proposes a model-based approach to address adversarial\nstate perturbations using ensemble learning with a set of state transition functions. Nguyen et al., [14] demonstrates the\nvulnerability of offline RL models to adversarial examples, where the outputs of critic and actor networks deviate from\nclean examples based on KL divergence. The study introduces a defense regularization term in the loss function to\nminimize the mean squared error between adversarial and clean example outputs, showing improved robustness against\nadversarial attacks.\nNevertheless, research on adversarial attacks specifically targeting action spaces and corresponding defense strategies\nremains limited. This study explores testing-time robustness against both adversarial and random perturbations in the\ncontext of model-free control for legged robots.\n3\nMethod\nWe evaluate the testing-time robustness of offline RL methods by conducting forward walking tasks with legged robots\nin three distinct environments, as shown in Figure 1. These environments include three perturbation scenarios: normal\n(no perturbations), random perturbations, and adversarial perturbations, both applied to the robot’s joint torque signals.\nFor the robustness evaluation, we employ two common approaches for offline RL: policy constraints and regularization.\n3.1\nOffline reinforcement learning\nReinforcement learning is modeled as a Markov decision process (MDP). An MDP is defined by a tuple\n(S, A, P, P0, r, γ) where S is a set of states, A is a set of actions, P : S × A × S →[0, 1] defines the transi-\n3\nAyabe et al.\ntion probability P(s′|s, a) from state s to state s′ given action a, P0 : S →[0, 1] denotes the initial state distribution,\nr : S × A →R is the reward function that assigns the immediate reward r(s, a) received after transitioning, and\nγ ∈[0, 1] is the discount factor that determines the importance of future rewards. The goal of an MDP is to find an\noptimal policy π∗that maximizes the expected sum of rewards.\nIn the offline RL setting, we are provided with dataset D = {(st, at, st+1, rt)i}, which may be sourced from previous\nlearning episodes, expert demonstrations, or even random actions. Here, the subscript t denotes a timestep within an\nepisode, and the subscript i indexes individual transitions in the dataset. The dataset D is assumed to be generated\naccording to a behavior policy πβ. The objective of offline RL is to find an optimal policy π∗using only the dataset D\nwithout further environmental interactions. A principal challenge in offline RL is the distributional shift problem. This\nproblem arises when the dataset D does not accurately reflect real-world situations or different scenarios. Such static\ndatasets may result in misguided learning, leading to policies that perform poorly in new or unseen environments. To\nmitigate the effects of the distribution shift, several methods have been proposed [29–31,33].\nPolicy constraint methods impose constraints to minimize the discrepancy between the learned policy πoff and the\nbehavior policy πβ. Similarly, policy regularization methods penalize deviations from behavior policy πβ by incorporat-\ning regularization terms into the objective function. These constraints and regularization terms help to keep the learned\npolicy within the support of the behavior policy, and ensure that the learned policy does not make decisions that deviate\nsignificantly from dataset D. However, this conservative strategy may backfire, potentially leading to vulnerabilities in\noffline RL models during testing phases.\n3.2\nAction perturbation\nFor action attack, a perturbation δ is applied to the action at as follows:\na′\nt = at + δ ⊙at,\n(1)\nwhere t denotes the time step in an episode, and ⊙denotes the Hadamard product, which represents the element-wise\nproduct of two vectors.\nIn this study, we consider three conditions: normal (no perturbations), uniform random perturbations, and adversarial\nperturbations. These perturbations are generated as follows:\nδ =\n\n\n\n0\nnormal,\nδr ∼U([−ϵ, ϵ]Na)\nrandom perturbations,\nδa ∼DE(ϵ)\nadversarial perturbations.\n(2)\nIn the normal condition, the perturbation δ is set to the zero vector 0, meaning no perturbations are applied to the actions\na. The random perturbation δr is drawn from the uniform distribution U([−ϵ, ϵ]Na), where Na denotes the dimension\nof a, corresponding to the number of actuators. Each element of a is independently perturbed by a random number\ndrawn from the uniform distribution over the interval [−ϵ, ϵ]. As illustrated in the bottom-right corner of Figure 1, The\nadversarial perturbation δa is obtained using the differential evolution (DE) method [19], denoted by DE(ϵ), where ϵ\nspecifies the strength of the adversarial attacks.\nThe DE method is represented as Algorithm 1, with perturbations for the g-th generation denoted by\n{δ(1)\ng , δ(2)\ng , . . . , δ(NP )\ng\n}, where NP denotes the number of individuals for each generation. Each perturbation δ(i)\ng\nis\nevaluated based on the average episodic reward ¯Repisode computed over M episodes as:\n¯Repisode = 1\nM\nM\nX\nm=1\nT −1\nX\nt=0\nr(m)\nt\n,\n(3)\nwhere r(m)\nt\n= r(st, a′\nt) denotes the immediate reward at time t for episode m. We conduct over 100 trials for each\ngeneration under the trained policy πoff. The DE algorithm uses mutation and crossover processes to generate new\nperturbations. For mutation, the i-th mutant individual v(i)\ng+1 is calculated as:\nv(i)\ng+1 = δbest + F(δ(r1)\ng\n−δ(r2)\ng\n),\n(4)\nwhere, F ∈(0.5, 1] is a random scaling factor, r1 and r2 are mutually exclusive indices randomly chosen from\n{1, 2, . . . , NP}. For crossover, the binominal crossover is used to compute the j-th element of the i-th trial individual\nas follows:\nui\nj,g+1 =\n\u001avi\nj,g+1\nif r ≤CR or j = jr,\nδi\nj,g\notherwise,\n(5)\n4\nAyabe et al.\nAlgorithm 1 Differential Evolution for Adversarial Attack\nRequire: NP: Population size\nRequire: M: Maximum number of episodes\n1: Generate initial population {δ(1)\n0 , δ(2)\n0 , . . . , δ(NP )\n0\n} with δ(i)\n0\n∼U\n\u0000[−ϵ, ϵ]Na\u0001\n2: Rmin ←∞\n3: for each generation g = 1, 2, . . . do\n4:\nfor each individual i = 1, 2, . . . , NP do\n5:\nGenerate trial individual u(i)\ng\nby mutation and crossover using Equation (4) and Equation (5)\n6:\nu(i)\ng\n←clip[−ϵ,ϵ](u(i)\ng )\n7:\nfor each episode m = 1, 2, . . . , M do\n8:\nCompute the average episodic reward of the g-th generation ¯Rg and target ¯Rg−1 using Equation (3)\n9:\nend for\n10:\nif ¯Rg ≤¯Rg−1 then\n11:\nAccept trial individual δ(i)\ng\n←u(i)\ng\n12:\nif ¯Rg ≤Rmin then\n13:\nRmin ←¯Rg\n14:\nUpdate best individual δbest ←δ(i)\ng\n15:\nend if\n16:\nelse\n17:\nRetain target individual δ(i)\ng\n←δ(i)\ng−1\n18:\nend if\n19:\nend for\n20: end for\nAlgorithm 2 Testing-Time Robustness Evaluation\nRequire: πoff: Trained policy with offline RL method\nRequire: M: Maximum number of episodes\n1: for each episode m = 1, 2, . . . , M do\n2:\nGenerate perturbation δ using Equation (2)\n3:\nSample initial state s0 ∼P0(·)\n4:\nSet t ←0 and R(m) ←0\n5:\nwhile not terminated do\n6:\nSample action at ∼πoff(·|st)\n7:\nObtain reward rt ←r(st, at + δ ⊙at)\n8:\nAccumulate reward R(m) ←R(m) + rt\n9:\nSample state st+1 ∼P(·|st, at)\n10:\nt ←t + 1\n11:\nend while\n12: end for\n13: return ¯Repisode =\n1\nM\nPM\nm=1 R(m)\nwhere r ∈[0, 1] is a uniform random number, CR = 0.7 is the crossover constant, and jr is an index randomly chosen\nfrom {1, 2, . . . , Na}. Each element of u(i)\ng+1 is constrained to the range of [−ϵ, ϵ] using the clip function, defined by\nclip[−ϵ,ϵ](x) = max (min (x, ϵ), −ϵ). Finally, the individual from the final generation is utilized as the adversarial\nperturbation δa.\n3.3\nTesting-time robustness evaluation\nThe procedure for evaluating the testing-time robustness of offline RL methods is outlined in Algorithm 2 and top-right\ncorner of Figure 1. The robustness is evaluated using the average episodic reward in Equation (3). We assume that\nperturbations δ remains constant throughout each episode; δ is initialized at the beginning of the episode and remains\nunchanged until the end. Each episode terminates when either T = 1000 time steps are reached or the robot falls over.\n5\nAyabe et al.\n4\nExperiments\nThis section evaluates the testing-time robustness of offline RL methods through three main experiments. First, we\ncompare the testing-time robustness of offline RL methods to online RL methods by measuring the average episodic\nrewards over 1,000 trials under three distinct conditions: the normal condition (no perturbations), the random condition\n(uniform random perturbations), and the adversarial condition (adversarial perturbations). Second, we examine how\nstate-action coverage in training datasets impacts the testing-time robustness of offline RL methods. Finally, we\ninvestigate whether training policies on action-perturbed datasets enhance their robustness against the perturbations.\n4.1\nExperimental setup\nRL methods\n: We employ existing offline RL methods (BCQ, TD3+BC, IQL) implemented in the Data-Driven Deep\nReinforcement Learning library for Python (d3rlpy) [40], a framework designed for offline RL. Additionally, we employ\nthree widely-used online RL methods: Proximal Policy Optimization (PPO) [41], Soft Actor-Critic (SAC) [42], and\nTD3 [34]. The network architecture for PPO followed the original paper [41], while the hyperparameter configurations\nare adapted from prior studies [19, 43] For SAC and TD3, we leverage the implementation provided by the d3rlpy\nlibrary.\nTraining datasets\n: Training is conducted using datasets from Datasets for Deep Data-Driven Reinforcement Learning\n(D4RL) [44], which include three quality levels: expert, medium-expert, and medium. The expert and medium datasets\nconsist of 1 million trajectories collected using well-trained and less-trained policies, respectively. The medium-expert\ndataset combines the expert and medium datasets, resulting in a total of 2 million trajectories. We create two types of\naction-perturbed datasets for all legged robot environments: randomly-perturbed datasets and adversarially-perturbed\ndatasets. Random and adversarial perturbations, as defined in Equation (2), are applied to all action data within the\nD4RL expert dataset, following Equation (1).\nTraining steps\n: For the expert dataset, RL models are trained for 3 million steps using BCQ and IQL methods,\nrespectively, whereas models using TD3+BC require extended training of up to 5 million steps to ensure convergence.\nWith the medium-expert dataset, all RL models are trained for 3 million steps. For the medium dataset, RL models are\ntrained for 3 million steps using BCQ and IQL, whereas models using TD3+BC require only up to 2 million steps of\ntraining. For the action-perturbed datasets, RL models using each offline RL method were trained for the same number\nof steps as those trained on the expert dataset. Regarding the online RL methods, RL models using PPO are trained as\ndescribed in [19] and [43]. RL models are trained for over 3 million steps using SAC and TD3 methods.\nLegged robot environment\n: Forward walking tasks are performed in Hopper-v2, HalfCheetah-v2, and Ant-v2\nenvironments from OpenAI Gym [45], as illustrated in Figure 2, within the MuJoCo physics simulation environment.\nThe reward functions for each robot are defined as follows:\nr(s, a) =\n\n\n\n\n\nvfwd −0.001 ∥a∥2 + 1, Hopper\nvfwd −0.1 ∥a∥2 + 1, HalfCheetah\nvfwd −0.5∥a∥2 −0.5 · 10−3 ∥f∥2 + 1, Ant\n(6)\nwhere vfwd is the forward walking speed and f denotes the contact force.\nPerturbation setting\n: Perturbations under each condition are defined according to Equation (2). In the Hopper-v2\nand HalfCheetah-v2 environments, the strengths of δr and δa are set to ϵ = 0.3 for all experiments. In the Ant-v2\nenvironment, the perturbation strength is set to ϵ = 0.5 for most experiments. However, for the final experiment\ninvolving the action-perturbed dataset, the perturbation strength is reduced to ϵ = 0.3 due to insufficient training\nwith the strength ϵ = 0.5. The adversarial perturbation δa in Equation (2) is generated using the DE method over\n30 generations. The population sizes for DE are set to 45, 90, and 120 for Hopper-v2, HalfCheetah-v2, and Ant-v2,\nrespectively.\n4.2\nComparing testing-time robustness: offline RL vs. online RL\nWe compare the testing-time robustness of offline RL to online RL. Offline RL models are generally expected to be\nmore vulnerable than online RL models because online RL models are typically trained on diverse state-action pairs\nthrough direct interaction with the environment. As shown below, the experimental results support this expectation.\nTable 1 summarizes the experiment results for the Hopper-v2, HalfCheetah-v2, and Ant-v2 environments. The table\npresents the average episodic rewards over 1,000 trials, along with the corresponding standard deviations.\n6\nAyabe et al.\nFigure 2: Legged robot environments: Hopper-v2 (left), HalfCheetah-v2 (center), Ant-v2 (right).\nFrom Table 1, both offline RL (BCQ, TD3+BC, IQL) and online RL (PPO, SAC, TD3) exhibit lower average episodic\nrewards under the random perturbation condition compared to the normal condition, with further deterioration observed\nunder the adversarial perturbation condition. This result demonstrates that the applied perturbations, particularly the\nadversarial ones, affect the legged robot control using both online and offline RL as expected. In the Hopper-v2\nenvironment, offline RL performs on par with online RL under both the normal and random conditions, but shows\nslightly lower performance under the adversarial condition. In contrast, in the HalfCheetah-v2 and Ant-v2 environments,\noffline RL yields results that are either comparable to or better than those of online RL under normal and random\nconditions. However, under adversarial conditions, the performance of offline RL significantly decreases, particularly in\nthe Ant-v2 environment, where the average episodic reward falls below zero.\nThis finding suggests that offline RL brings more vulnerability due to insufficient experiential learning. The significant\nreduction in average episodic rewards occurs consistently across different legged robot scenarios and the offline RL\nmethods, emphasizing the challenges for robust offline RL. The results further suggest that the vulnerability of offline\nRL stems primarily from the offline setting itself rather than from policy characteristics, as the offline RL methods\nexhibit greater vulnerability to adversarial perturbations than the online RL methods, regardless of whether the policies\nare stochastic and deterministic policies.\n4.3\nImpact of state-action coverage on offline RL robustness\nBased on the robustness differences between offline RL and online RL identified in the previous section, we hypothesize\nthat these differences stem from the dependency of offline RL training on the training dataset. Specifically, we\nhypothesize that variations in state-action coverage across different training datasets contribute to these robustness\ndisparities. To investigate this hypothesis, we evaluate the robustness of offline RL methods trained on the D4RL\nmedium-expert and medium datasets. Table 2 summarizes the results.\nFor the medium-expert dataset, average episodic rewards under the normal and random conditions are consistent with\nthose of the expert dataset. However, under the adversarial condition, the medium-expert dataset yields higher average\nrewards and small standard deviations than the expert dataset. This result indicates a shift toward a more positive reward\ndistribution.\nFor the medium dataset, average episodic rewards in the HalfCheetah-v2 environment under the normal condition are\napproximately half of those for the expert dataset. In contrast, the hopper-v2 and Ant-v2 environments show only slight\ndecreases compared to the expert dataset results. This finding aligns with previous studies [30,33,44,46], which report\nreduced rewards for policies trained on the medium dataset in HalfCheetah-v2 environment. This suggests that these\ndifferences reflect the specific characteristics of each legged robot environment. Furthermore, under the adversarial\ncondition, both the medium and medium-expert datasets yield significant increases in average rewards compared to the\nexpert dataset. Moreover, in many cases, the medium dataset even outperforms the medium-expert dataset in terms of\naverage rewards.\nThe medium-expert and medium datasets provide broader state-action coverage than the expert datasets, contributing to\nimproved robustness against perturbations. This observation suggests that increasing the diversity of training datasets\ncould enhance the robustness of offline RL methods, emphasizing the importance of strategically designing datasets for\nrobust performance.\n7\nAyabe et al.\nTable 1: Testing-time robustness evaluation results of offline RL (BCQ, IQL, TD3+BC) and online RL (PPO, SAC,\nTD3). The evaluation is conducted in three legged robot environment (Hopper-v2, HalfCheetah-v2, Ant-v2) under\nnormal, random, and adversarial conditions. Offline RL methods are trained with the expert dataset. Results are\npresented as average episodic rewards ± standard deviations. The values reported in the average row represents average\nresults of three offline RL methods under each experiment setting.\nEnvironment\nTraining datasets\nRL methods\nPerturbation conditions\nNormal\nRandom\nAdversarial\nHopper-v2\nexpert\nBCQ\n2371 ± 449\n1556 ± 1296\n282 ± 4\nTD3+BC\n3597 ± 93\n2027 ± 1394\n424 ± 78\nIQL\n1631 ± 597\n1290 ± 939\n421 ± 24\nAverage\n2533\n1624\n376\nonline\nPPO\n2529 ± 109\n1456 ± 935\n346 ± 6\nSAC\n3148 ± 746\n1945 ± 1294\n445 ± 89\nTD3\n3439 ± 2\n2706 ± 1130\n487 ± 18\nAverage\n3039\n2036\n426\nHalfCheetah-v2\nexpert\nBCQ\n11414 ± 1406\n8036 ± 3179\n244 ± 262\nTD3+BC\n11713 ± 421\n9034 ± 2337\n1227 ± 999\nIQL\n9316 ± 2214\n6198 ± 3057\n313 ± 366\nAverage\n10814\n7756\n595\nonline\nPPO\n6822 ± 32\n6579 ± 161\n5985 ± 107\nSAC\n7664 ± 58\n5610 ± 1069\n2841 ± 71\nTD3\n7076 ± 289\n5801 ± 646\n3713 ± 106\nAverage\n7187\n5997\n4180\nAnt-v2\nexpert\nBCQ\n4511 ± 1568\n3102 ± 1781\n−1129 ± 1083\nTD3+BC\n4715 ± 1676\n3317 ± 1822\n−1209 ± 1089\nIQL\n4175 ± 1546\n2851 ± 1731\n−1503 ± 1026\nAverage\n4467\n3090\n−1280\nonline\nPPO\n5698 ± 989\n4975 ± 1191\n2654 ± 1748\nSAC\n6070 ± 890\n4226 ± 1611\n942 ± 888\nTD3\n5154 ± 1100\n3349 ± 1464\n685 ± 1116\nAverage\n5641\n4183\n1427\n4.4\nImpact of action-perturbed datasets on offline RL robustness\nTo investigate the potential for improving robustness against action-space perturbations, we train policies using the\nrandomly and adversarially action-perturbed datasets and evaluate their robustness. In this experiment, unlike the\nprevious two experiments, the perturbation strength is set to ϵ = 0.3 for all legged robot environments, including\nAnt-v2. The results are presented in Table 3.\nCompared to the expert dataset results, some results for the randomly-perturbed dataset show a slight increase in\naverage episodic rewards under both the random and adversarial conditions. However, these increases are modest and\ncannot be conclusively interpreted as improvements in robustness. The results for the adversarially-perturbed dataset\nreveal that the average episodic rewards remain low, even under the normal condition. These findings cast doubt on the\neffectiveness of adding perturbations to the training dataset for improving the testing-time robustness.\nWhile perturbation-adding techniques have demonstrated efficacy in improving the testing-time robustness of online\nRL [18, 21, 22, 25], offline RL presents unique challenge. In offline RL, such techniques alone are insufficient,\nhighlighting the need for further methodological advancements to address these limitations.\n5\nConclusion\nThis study evaluates the testing-time robustness of offline RL methods in scenarios simulating joint torque failures in\nlegged robots. We focus on three offline RL methods—–BCQ, TD3+BC, and IQL—–across normal, random pertur-\nbation, and adversarial perturbation conditions. The results demonstrate that the offline RL methods are significantly\nmore vulnerable to both random and adversarial perturbations compared to online RL methods in identical conditions.\nThis vulnerability is attributed to the limited diversity of state-action pairs in training datasets, which restricts the\n8\nAyabe et al.\nTable 2: Testing-time robustness evaluation results of offline RL trained on the medium-expert and the medium datasets.\nThe evaluation is conducted in three legged robot environments (Hopper-v2, HalfCheetah-v2, Ant-v2) under normal,\nrandom, and adversarial conditions. Results are presented as the average episodic rewards ± standard deviations. The\nvalues reported in the average row represents average results of three offline RL methods under each experiment setting.\nEnvironment\nTraining datasets\nRL methods\nPerturbation conditions\nNormal\nRandom\nAdversarial\nHopper-v2\nexpert\nBCQ\n2371 ± 449\n1556 ± 1296\n282 ± 4\nTD3+BC\n3597 ± 93\n2027 ± 1394\n424 ± 78\nIQL\n1631 ± 597\n1290 ± 939\n421 ± 24\nAverage\n2533\n1624\n376\nmedium-expert\nBCQ\n2928 ± 973\n1581 ± 1063\n453 ± 57\nTD3+BC\n3059 ± 867\n1706 ± 1151\n500 ± 57\nIQL\n879 ± 277\n790 ± 454\n432 ± 44\nAverage\n2289\n1262\n700\nmedium\nBCQ\n2030 ± 502\n1483 ± 580\n609 ± 12\nTD3+BC\n1969 ± 398\n1569 ± 560\n741 ± 14\nIQL\n1200 ± 205\n1116 ± 439\n598 ± 28\nAverage\n1733\n1389\n649\nHalfCheetah-v2\nexpert\nBCQ\n11414 ± 1406\n8036 ± 3179\n244 ± 262\nTD3+BC\n11713 ± 421\n9034 ± 2337\n1227 ± 999\nIQL\n9316 ± 2214\n6198 ± 3057\n313 ± 366\nAverage\n10814\n7756\n595\nmedium-expert\nBCQ\n10566 ± 1563\n7139 ± 2448\n1135 ± 948\nTD3+BC\n11729 ± 724\n9494 ± 1994\n2273 ± 1470\nIQL\n6974 ± 1780\n5683 ± 1583\n2044 ± 382\nAverage\n9756\n7439\n1817\nmedium\nBCQ\n5504 ± 324\n5077 ± 338\n3462 ± 405\nTD3+BC\n5739 ± 81\n5329 ± 328\n3568 ± 135\nIQL\n5364 ± 267\n4903 ± 464\n2814 ± 240\nAverage\n5536\n5103\n3281\nAnt-v2\nexpert\nBCQ\n4511 ± 1568\n3102 ± 1781\n−1129 ± 1083\nTD3+BC\n4715 ± 1676\n3317 ± 1822\n−1209 ± 1089\nIQL\n4175 ± 1546\n2851 ± 1731\n−1503 ± 1026\nAverage\n4467\n3090\n−1280\nmedium-expert\nBCQ\n4250 ± 1568\n3397 ± 1343\n1467 ± 1571\nTD3+BC\n4588 ± 1718\n3818 ± 1431\n1837 ± 1505\nIQL\n4131 ± 1457\n3235 ± 1352\n1074 ± 605\nAverage\n4323\n3483\n1459\nmedium\nBCQ\n4219 ± 709\n3361 ± 1045\n2597 ± 376\nTD3+BC\n4194 ± 1294\n3433 ± 1340\n1206 ± 998\nIQL\n3527 ± 997\n2946 ± 979\n1329 ± 992\nAverage\n3980\n3247\n1711\nadaptability of offline RL policies. Our experiments further reveal that state-action coverage in training datasets\naffects the robustness of offline RL methods. Policies trained on medium-expert and medium datasets exhibit notable\ndifferences in performance under adversarial conditions when compared to those trained on expert datasets. However,\ntraining policies on action-perturbed datasets does not yield substantial improvements in robustness, highlighting\nthe challenges of addressing adversarial perturbations in offline RL. These findings emphasize the need for novel\nmethodologies to improve the robustness of offline RL in real-world scenarios, such as actuator failures in robotics.\nFuture work should explore strategies to address these challenges, such as incorporating adversarial training techniques\nor creating datasets with strategically enhanced diversity to better represent complex and dynamic environments. These\nadvancements could help mitigate vulnerabilities of offline RL, enabling their application in robotic control and other\ndomains.\n9\nAyabe et al.\nTable 3: Testing-time robustness evaluation results of offline RL trained on action-perturbed datasets. The evaluation\nis conducted in three legged robot environments (Hopper-v2, HalfCheetah-v2, Ant-v2) under normal, random, and\nadversarial conditions. Results are presented as average episodic rewards ± standard deviations. The values reported\nin the average row represents average results of three offline RL methods under each experiment setting. randomly-\nperturbed refers to the expert dataset with randomly-perturbed actions, and adversarially-perturbed refers to the expert\ndataset with adversarially-perturbed actions.\nEnvironment\nTraining datasets\nRL methods\nPerturbation conditions\nNormal\nRandom\nAdversarial\nHopper-v2\nexpert\nBCQ\n2371 ± 449\n1556 ± 1296\n282 ± 4\nTD3+BC\n3597 ± 93\n2027 ± 1394\n424 ± 78\nIQL\n1631 ± 597\n1290 ± 939\n421 ± 24\nAverage\n2533\n1624\n376\nrandomly-perturbed\nBCQ\n2858 ± 717\n1774 ± 1232\n506 ± 82\nTD3+BC\n3594 ± 84\n2077 ± 1350\n413 ± 13\nIQL\n1124 ± 335\n1006 ± 642\n413 ± 34\nAverage\n2525\n1619\n444\nadversarially-perturbed\nBCQ\n534 ± 25\n659 ± 355\n329 ± 18\nTD3+BC\n353 ± 5\n411 ± 195\n336 ± 3\nIQL\n474 ± 54\n498 ± 182\n333 ± 38\nAverage\n454\n523\n333\nHalfCheetah-v2\nexpert\nBCQ\n11414 ± 1406\n8036 ± 3179\n244 ± 262\nTD3+BC\n11713 ± 421\n9034 ± 2337\n1227 ± 999\nIQL\n9316 ± 2214\n6198 ± 3057\n313 ± 366\nAverage\n10814\n7756\n595\nrandomly-perturbed\nBCQ\n11110 ± 1411\n7660 ± 3167\n182 ± 315\nTD3+BC\n11334 ± 111\n8753 ± 2202\n828 ± 515\nIQL\n3618 ± 2611\n2367 ± 2102\n302 ± 377\nAverage\n8687\n6260\n437\nadversarially-perturbed\nBCQ\n427 ± 648\n1713 ± 2839\n43 ± 54\nTD3+BC\n1737 ± 1804\n2840 ± 3445\n−26 ± 68\nIQL\n407 ± 445\n863 ± 1242\n45 ± 193\nAverage\n857\n1805\n21\nAnt-v2\nexpert\nBCQ\n4511 ± 1568\n4127 ± 1536\n1670 ± 1769\nTD3+BC\n4715 ± 1676\n4189 ± 1745\n1432 ± 1484\nIQL\n4175 ± 1546\n3707 ± 1535\n1966 ± 1823\nAverage\n4467\n4008\n1689\nrandomly-perturbed\nBCQ\n4662 ± 1486\n4181 ± 1467\n2208 ± 1863\nTD3+BC\n5266 ± 1176\n4534 ± 1442\n2292 ± 1898\nIQL\n3385 ± 1759\n3231 ± 1634\n1609 ± 1652\nAverage\n4438\n3982\n2036\nadversarially-perturbed\nBCQ\n1609 ± 1685\n2351 ± 1956\n621 ± 1530\nTD3+BC\n−159 ± 534\n−209 ± 609\n−428 ± 815\nIQL\n1758 ± 1628\n2236 ± 1831\n505 ± 1414\nAverage\n1069\n1459\n233\nAcknowledgments\nThis work was supported by JSPS KAKENHI Grant Numbers JP23K24914 and the Telecommunications Advancement\nFoundation.\nReferences\n[1] Levine S, Kumar A, Tucker G et al. Offline reinforcement learning: Tutorial, review, and perspectives on open\nproblems, 2020. arXiv:2005.01643.\n10\nAyabe et al.\n[2] Emerson H, Guy M and McConville R. Offline reinforcement learning for safer blood glucose control in people\nwith type 1 diabetes. Journal of Biomedical Informatics 2023; 142: 104376.\n[3] Zhan X, Xu H, Zhang Y et al. Deepthermal: Combustion optimization for thermal power generating units using\noffline reinforcement learning. In the 36th AAAI Conference on Artificial Intelligence (AAAI 2022), volume 36. pp.\n4680–4688.\n[4] Gürtler N et al. Benchmarking offline reinforcement learning on real-robot hardware. In the 11th International\nConference on Learning Representations (ICLR 2023).\n[5] Zhang X, Chen Y, Zhu X et al. Corruption-robust offline reinforcement learning. In the 25th International\nConference on Artificial Intelligence and Statistics (AISTATS 2022), volume 151. pp. 5757–5773.\n[6] Ye C, Yang R, Gu Q et al. Corruption-robust offline reinforcement learning with general function approximation.\nIn the 37th Conference on Neural Information Processing Systems (NeurIPS 2023), volume 36. pp. 36208–36221.\n[7] Yang R, Zhong H, Xu J et al. Towards robust offline reinforcement learning under diverse data corruption. In the\n12th International Conference on Learning Representations (ICLR 2024).\n[8] Zhou Z, Zhou Z, Bai Q et al. Finite-sample regret bound for distributionally robust offline tabular reinforcement\nlearning. In the 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021), volume\n130. pp. 3331–3339.\n[9] Panaganti K, Xu Z, Kalathil D et al. Robust reinforcement learning using offline data. In the 36th Conference on\nNeural Information Processing Systems (NeurIPS 2022), volume 35. pp. 32211–32224.\n[10] Shi L and Chi Y. Distributionally robust model-based offline reinforcement learning with near-optimal sample\ncomplexity. Journal of Machine Learning Research 2024; 25(200): 1–91.\n[11] Blanchet J, Lu M, Zhang T et al. Double pessimism is provably efficient for distributionally robust offline\nreinforcement learning: Generic algorithm and robust partial coverage. In the 37th Conference on Neural\nInformation Processing Systems (NeurIPS 2023), volume 36. pp. 66845–66859.\n[12] Yang R, Bai C, Ma X et al. RORL: Robust offline reinforcement learning via conservative smoothing. In the 36th\nConference on Neural Information Processing System (NeurIPS 2022), volume 35. pp. 23851–23866.\n[13] Liu XY, Zhou XH, Li G et al. MICRO: Model-based offline reinforcement learning with a conservative bellman\noperator. In the 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024). pp. 4587–4595.\n[14] Nguyen T, Luu TM, Ton T et al. Towards robust policy: Enhancing offline reinforcement learning with adversarial\nattacks and defenses, 2024. arXiv:2405.11206.\n[15] Gong C, Yang Z, Bai Y et al. Baffle: Hiding Backdoors in Offline Reinforcement Learning Datasets . In 2024\nIEEE Symposium on Security and Privacy (SP 2024). pp. 2086–2104.\n[16] Zhang H, Chen H, Xiao C et al. Robust deep reinforcement learning against adversarial perturbations on state\nobservations. In the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), volume 33. pp.\n21024–21037.\n[17] Sun Y, Zheng R, Liang Y et al. Who is the strongest enemy? towards optimal and efficient evasion attacks in deep\nRL. In International Conference on Learning Representations (ICLR 2022).\n[18] Pinto L, Davidson J, Sukthankar R et al. Robust adversarial reinforcement learning. In the 34th International\nConference on Machine Learning (ICML 2017), volume 70. pp. 2817–2826.\n[19] Otomo T, Kera H and Kawamoto K. Adversarial joint attacks on legged robots. In 2022 IEEE International\nConference on Systems, Man, and Cybernetics. pp. 676–681.\n[20] Liu Q, Kuang Y and Wang J. Robust deep reinforcement learning with adaptive adversarial perturbations in action\nspace, 2024. arXiv:2405.11982.\n[21] Tobin J et al. Domain randomization for transferring deep neural networks from simulation to the real world. In\n2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017). pp. 23–30.\n[22] Chebotar Y et al. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In\n2019 IEEE International Conference on Robotics and Automation (ICRA 2019). pp. 8973–8979.\n[23] Kadokawa Y, Zhu L, Tsurumine Y et al. Cyclic policy distillation: Sample-efficient sim-to-real reinforcement\nlearning with domain randomization. Robotics and Autonomous Systems 2023; 165: 104425.\n[24] Tessler C, Efroni Y and Mannor S. Action robust reinforcement learning and applications in continuous control.\nIn the 36th International Conference on Machine Learning (ICML 2019), volume 97. pp. 6215–6224.\n11\nAyabe et al.\n[25] Zhang X, Chen Y, Zhu X et al. Robust policy gradient against strong data corruption. In the 38th International\nConference on Machine Learning (ICML 2021), volume 139. pp. 12391–12401.\n[26] Sheng J, Zhai P, Dong Z et al. Curriculum adversarial training for robust reinforcement learning. In 2022\nInternational Joint Conference on Neural Networks (IJCNN 2022). pp. 1–8.\n[27] Zheng X, Ma X, Wang S et al. Toward evaluating robustness of reinforcement learning with adversarial policy.\nIn the 54th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2024). pp.\n288–301.\n[28] Figueiredo Prudencio R, Maximo ROA Mardos and Colombini EL. A survey on offline reinforcement learning:\nTaxonomy, review, and open problems. IEEE Transactions on Neural Networks and Learning Systems 2024;\n35(8): 10237–10257.\n[29] Fujimoto S, Meger D and Precup D. Off-policy deep reinforcement learning without exploration. In the 36th\nInternational Conference on Machine Learning (ICML 2019), volume 97. pp. 2052–2062.\n[30] Fujimoto S and Gu SS. A minimalist approach to offline reinforcement learning. In the 35th Conference on\nNeural Information Processing Systems (NeurIPS 2021), volume 34. pp. 20132–20145.\n[31] Kostrikov I, Nair A and Levine S. Offline reinforcement learning with implicit q-learning. In the 10th International\nConference on Learning Representations (ICLR 2022).\n[32] Todorov E, Erez T and Tassa Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS 2012). pp. 5026–5033.\n[33] Kumar A, Zhou A, Tucker G et al. Conservative q-learning for offline reinforcement learning. In the 34th\nConference on Neural Information Processing Systems (NeurIPS 2020), volume 33. pp. 1179–1191.\n[34] Fujimoto S, Hoof H and Meger D. Addressing function approximation error in actor-critic methods. In the 35 th\nInternational Conference on Machine Learning (ICML 2018). pp. 1582–1591.\n[35] Iyengar G. Robust dynamic programming. Mathematics of Operation Research 2005; 30: 257–280.\n[36] Ho CP, Petrik M and Wiesemann W. Fast Bellman updates for robust MDPs. In the 35th International Conference\non Machine Learning (ICML 2018), volume 80. pp. 1979–1988.\n[37] Hu J, Zhong H, Jin C et al. Provable sim-to-real transfer in continuous domain with partial observations. In the\n11th International Conference on Learning Representations (ICLR 2023).\n[38] Sundhar Ramesh S, Giuseppe Sessa P, Hu Y et al. Distributionally robust model-based reinforcement learning\nwith large state spaces. In the 27th International Conference on Artificial Intelligence and Statistics (AISTATS\n2024), volume 238. pp. 100–108.\n[39] Liu J, Wu F and Zhang X. Model-free robust reinforcement learning via polynomial chaos. Knowledge-Based\nSystems 2025; 309: 112783.\n[40] Seno T and Imai M. d3rlpy: An offline deep reinforcement learning library. Journal of Machine Learning\nResearch 2022; 23: 1–20.\n[41] Schulman J, Wolski F, Dhariwal P et al. Proximal policy optimization algorithms, 2017. arXiv:1707.06347.\n[42] Haarnoja T, Zhou A, Abbeel P et al. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning\nwith a stochastic actor. In he 35th International Conference on Machine Learning (ICML 2018), volume 80. pp.\n1861–1870.\n[43] Babadi A, van de Panne M, Liu CK et al. Learning task-agnostic action spaces for movement optimization. IEEE\nTransactions on Visualization and Computer Graphics 2022; 28(12): 4700–4712.\n[44] Fu J, Kumar A, Nachum O et al. D4RL: Datasets for deep data-driven reinforcement learning, 2020. arXiv:\n2004.07219.\n[45] Brockman G, Cheung V, Pettersson L et al. Openai gym, 2016. arXiv:1606.01540.\n[46] Kostrikov I, Tompson J, Fergus R et al. Offline reinforcement learning with fisher divergence critic regularization.\nIn the 38 th International Conference on Machine Learning (ICML 2021), volume 139. pp. 5774–5783.\n12\n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2024-12-25",
  "updated": "2024-12-25"
}