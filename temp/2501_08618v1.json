{
  "id": "http://arxiv.org/abs/2501.08618v1",
  "title": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in Large Language Models",
  "authors": [
    "Aruna Sankaranarayanan",
    "Dylan Hadfield-Menell",
    "Aaron Mueller"
  ],
  "abstract": "All natural languages are structured hierarchically. In humans, this\nstructural restriction is neurologically coded: when two grammars are presented\nwith identical vocabularies, brain areas responsible for language processing\nare only sensitive to hierarchical grammars. Using large language models\n(LLMs), we investigate whether such functionally distinct hierarchical\nprocessing regions can arise solely from exposure to large-scale language\ndistributions. We generate inputs using English, Italian, Japanese, or nonce\nwords, varying the underlying grammars to conform to either hierarchical or\nlinear/positional rules. Using these grammars, we first observe that language\nmodels show distinct behaviors on hierarchical versus linearly structured\ninputs. Then, we find that the components responsible for processing\nhierarchical grammars are distinct from those that process linear grammars; we\ncausally verify this in ablation experiments. Finally, we observe that\nhierarchy-selective components are also active on nonce grammars; this suggests\nthat hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.",
  "text": "Disjoint Processing Mechanisms of Hierarchical and Linear\nGrammars in Large Language Models\nAruna Sankaranarayanan\nCSAIL, MIT\narunas@mit.edu\nDylan Hadfield-Menell\nCSAIL, MIT\ndhm@mit.edu\nAaron Mueller\nNortheastern University\naa.mueller@northeastern.edu\nAbstract\nAll natural languages are structured hierarchi-\ncally. In humans, this structural restriction is\nneurologically coded: when two grammars are\npresented with identical vocabularies, brain ar-\neas responsible for language processing are\nonly sensitive to hierarchical grammars. Using\nlarge language models (LLMs), we investigate\nwhether such functionally distinct hierarchical\nprocessing regions can arise solely from expo-\nsure to large-scale language distributions. We\ngenerate inputs using English, Italian, Japanese,\nor nonce words, varying the underlying gram-\nmars to conform to either hierarchical or lin-\near/positional rules. Using these grammars, we\nfirst observe that language models show dis-\ntinct behaviors on hierarchical versus linearly\nstructured inputs. Then, we find that the com-\nponents responsible for processing hierarchical\ngrammars are distinct from those that process\nlinear grammars; we causally verify this in ab-\nlation experiments. Finally, we observe that\nhierarchy-selective components are also active\non nonce grammars; this suggests that hierar-\nchy sensitivity is not tied to meaning, nor in-\ndistribution inputs.\n1\nIntroduction\nIn 1861, Broca found evidence that language pro-\ncessing functions are localized in specific brain\nregions. Since then, our mapping of the brain has\nadvanced tremendously; we now know that func-\ntional specialization can arise not only from bio-\nlogically coded mechanisms, but also from experi-\nence (Baker et al., 2007). More recently, there has\nbeen significant interest in understanding the mech-\nanisms of language processing in large language\nmodels (Olsson et al., 2022; Hanna et al., 2023; Yu\net al., 2023; Todd et al., 2024), whose inductive\nbiases are more general than those of humans.\nSensitivity and functional selectivity toward the\nhierarchical structure of language is a hallmark\nof human language processing (Chomsky, 1957,\n1965): brain regions selective towards hierarchi-\ncal grammars are disjoint from regions selective\ntowards linear structures, as well as hierarchical\nbut non-linguistic structures such as those found in\nmusic or programming languages, or sentences con-\nstructed from nonce words (Malik-Moraleda et al.,\n2023; Fedorenko et al., 2016; Ivanova et al., 2020;\nLiu et al., 2020; Varley and Siegal, 2000; Varley\net al., 2005; Apperly et al., 2006; Fedorenko and\nVarley, 2016; Monti et al., 2009; Fedorenko et al.,\n2011; Amalric and Dehaene, 2019; Ivanova et al.,\n2021; Chen et al., 2023). Despite large quantities of\nevidence from humans for language selectivity, it is\nnot clear whether language models would acquire\nsimilar selectivity from exposure to natural lan-\nguage data in the absence of human-like learning\nbiases. Kallini et al. (2024) recently find that au-\ntoregressive Transformer-based models (Vaswani\net al., 2017) can more easily learn grammars that ac-\ncord with the structures found in human language.\nWhile that study provides evidence from language\nacquisition, we are primarily interested in language\nprocessing in models trained on large text corpora.\nDo large language models (LLMs) demonstrate\ndistinct mechanisms for processing hierarchically\nstructured vs. non-hierarchically structured sen-\ntences that are otherwise superficially identical?\nWe derive inspiration from Musso et al.’s (2003)\nexperiment testing hierarchical and linear selectiv-\nity in human language processing. We replicate\nthis experiment, to the extent possible,1 on a series\nof large pretrained language models (§2). We de-\nsign a series of superficially similar but structurally\ndistinct grammaticality judgment tasks. We gen-\n1Musso et al.’s (2003) experiment required that subjects\nbe fluent in their native language (German, in their case) but\nnot have prior exposure to the foreign languages (Italian and\nJapanese) that they were tested on. We cannot fully satisfy this\ncondition for LLMs, whose training distributions consist of\nsignificant amounts of documents in non-English languages—\nthough orders-of-magnitude fewer documents than for En-\nglish.\n1\narXiv:2501.08618v1  [cs.CL]  15 Jan 2025\nerate hierarchical grammars that accord to natural\nlanguage structure, as well as linear grammars that\nare explained by positional insertion or transfor-\nmation rules. Using these models and stimuli, we\ninvestigate the following research questions:\nRQ1.\nDo models process hierarchically-\nstructured inputs in a distinct manner from linearly-\nstructured inputs? We find that LLMs demonstrate\ndistinct behaviors and mechanisms for grammars\ndefined by hierarchical versus linear structure.\nRQ2. Which model components are causally\nresponsible for judging the grammaticality of hier-\narchical versus linear inputs? To what extent are\nthese components shared? We find high overlap\nbetween hierarchical grammars, but low overlaps\nacross hierarchical and linear grammars. Ablat-\ning hierarchy-sensitive components significantly\nreduces accuracy on hierarchical grammars, but\naffects accuracy on linear grammars to a signifi-\ncantly lesser extent.\nRQ3. Are findings from RQ1 and RQ2 de-\npendent on grounding in the lexicon of the lan-\nguage(s) of the training corpus? Or do these dis-\ntinctions also hold when given grammars generated\nusing nonce words? We observe that the natural-\nlanguage hierarchy-sensitive components also have\nsignificant influence on nonce grammars.\nThese results provide evidence that model re-\ngions responsible for processing hierarchical lin-\nguistic structure are localizable and distinct. Fur-\nther, these regions are selective for hierarchically\nstructured language more broadly, and are not de-\npendent on meaning nor in-distribution language\ninputs. This suggests that functional specialization\ntoward hierarchical linguistic structure can arise\nsolely from exposure to language data. Thus, even\nin the absence of strong human-like inductive bi-\nases, language-selective regions can emerge.\n2\nMethods\n2.1\nModels\nWe use Mistral-v0.3 (7B; Jiang et al., 2023), QWen\n2 (0.5B and 1.5B; Yang et al., 2024), Llama 2 (7B;\nTouvron et al., 2023), and Llama 3.1 (8B and 70B;\nGrattafiori et al., 2024). We select these models be-\ncause they are open-weights, relatively commonly\nused, and are currently among the best-performing\nopen models. In all experiments, we use nucleus\nsampling (temperature= 0.1, p = 0.9) to reduce\nvariance. We run all experiments on a GPU cluster\ncontaining 4 A100s (80G each). We used approxi-\nmately 1000 GPU hours during this study.\n2.2\nData\nWe define 3 classes of hierarchical and linear gram-\nmars respectively in English, Italian, and Japanese,\nyielding 18 grammars total. Each sentence is gen-\nerated using templates inspired by the constructs\ndefined in Musso et al. (2003). For each struc-\nture, we generate positive and negative examples.\nEach grammar, its underlying rule, and examples\nof corresponding positive and negative examples\nare available in Tables 1 and 2. Our dataset consists\nof 7 verbs with at least 5 subjects and objects each;\nfully enumerated, we have 1106 positive-negative\nexample pairs for each grammar. We use a 50/50\ntrain/test split (for 553 pairs for each grammar).2\nThe difference between hierarchical and linear\ngrammars lies in whether their latent structure is\nexplained via hierarchical or positional rules. Hier-\narchical grammars contain rules that conform to the\nhierarchical structure of natural language (Chom-\nsky, 1957; Everaert et al., 2015). Linear grammars\ncontain rules that are defined by word positions\nor relative word orderings—e.g., insert a word at\nposition 4. Such rules are argued to be impossible\nin human language (Chomsky, 1957, 1965).\nFor each grammar, a positive example is one\nthat conforms to the rule, and a negative example\nis one that does not. For all hierarchical gram-\nmars, we form the negative example by swapping\nthe final two words of a positive example. For all\nlinear grammars that entail inserting a word at a\nspecific position, we form the negative example by\ninserting the word at the final position. For linear\ngrammars that entail reversing the word order, we\nform the negative example by swapping the final\ntwo words after reversing the input. For Italian\nlast-noun agreement, the positive example is cre-\nated by agreeing the gender of the determiner of the\nfirst noun with the gender of the last noun; we form\nnegative examples by using a gender determiner\nthat agrees with the first noun.\n3\nExperiments\nWe run 4 experiments to evaluate the behaviors and\nmechanisms of 6 LLMs in processing hierarchi-\ncal and linear grammars. We use an in-context\nlearning setup.\nAll models are pre-trained on\ndatasets primarily consisting of English sentences,\n2Data and code to replicate our experiments can be found\non github\n2\nGrammar\nPositive Example\nNegative Example\nHierarchical\nDeclarative. Subject, verb, object.\na woman reads a chapter\na woman reads chapter a\nSubordinate. Subject, verb taking a\nrelative clause complement.\nSheela thinks that the woman reads the\nchapter\nSheela thinks that the woman reads chapter the\nPassive. Like Declarative, but in\nthe passive voice.\na chapter is read by a woman\na chapter is read by woman a\nLinear\nNegation. Insert “doesn’t” or “don’t”\nat position 5.\na\n1\nwoman\n2\nreads\n3\na\n4\ndoesn’t\n5\nchapter\n6\na\n1\nwoman\n2\nreads\n3\na\n4\nchapter\n5\ndoesn’t\n6\nInversion. Invert the word order of\nDeclarative.\nchapter\n5\na\n4\nreads\n3\nwoman\n2\na\n1\nchapter\n5\na\n4\nreads\n3\na\n1\nwoman\n2\nWh-word. Insert wh-word at posi-\ntion 5.\ndid\n1\na\n2\nwoman\n3\nreads\n4\na\n5\nwhen\n6\nchapter?\n7\ndid\n1\na\n2\nwoman\n3\nreads\n4\na\n5\nchapter\n6\nwhen?\n7\nTable 1: Dataset. List of grammars, descriptions of the rules defining each grammar, and positive (grammatical)\nand negative (ungrammatical) examples for each. We provide only English examples here for space; see App. A.1\nfor descriptions and examples for all grammars.\nbut also containing significant amounts of other\nhigh-resource languages. The exact training com-\nposition of most of these models is unknown; given\nthat LLMs are typically trained on data extracted\nfrom the open web,3 one can conjecture that the\ndata would be composed largely of English text\n(W3Techs, 2024), with large amounts of text from\nother common languages.\nExperiment 1 compares the performance of all\nthe pre-trained LLMs on grammaticality judgment\ntasks given hierarchical and linear grammars (§3.1).\nExperiment 2 locates model components that are\nimportant for processing hierarchical and linear\nstructures by treating hierarchical and linear inputs\nas counterfactuals (§3.2). Experiment 3 investi-\ngates the causal role of these components by ablat-\ning them and then measuring changes in grammat-\nicality judgment performance (§3.3). Experiment\n4 investigates whether the components identified\nin Experiment 2 merely distinguish grammars that\nare in-distribution with the training data, or show a\nmore abstract universal sensitivity to hierarchical\nand linear structure on nonce sentences (§3.4).\nThe input prompt in our in-context learning\nsetup comprises ten demonstrations, followed by a\ntest example (details in §3.1). In all experiments,\nwe conduct four trials, presenting mean results\nacross four random seeds; demonstrations in the\ninput prompt are randomized between trials, while\ntest examples remain consistent. The format of the\nprompts remain consistent across experiments.\n3Relevant discussion on Huggingface.\n3.1\nExperiment 1: Are language models\nsignificantly more accurate at classifying\nthe grammaticality of sentences from\nhierarchical grammars?\nWe first evaluate the accuracy of LLMs on gram-\nmaticality judgments given examples from each\ngrammar. Musso et al. (2003) found that humans\nwere more accurate at classifying examples of hi-\nerarchical grammars, even when they had no prior\nfluency in the test languages; we therefore hypoth-\nesize that a similar phenomenon would arise in\nLLMs if they contain functionally specialized re-\ngions for processing hierarchical structure.\nAs described in §2, for each of the 18 gram-\nmars, we generate 1106 examples. We perform\na uniform 50/50 split to obtain our train/test split.\nGiven a grammar, we first prompt an LLM with an\ninstruction describing the nature of the in-context\ntask (see Appendix B.1.1). This is followed by 10\ndemonstrations that are uniformly sampled from\nthe training split.\nFor each demonstration, we\nuse the format “Q: {sentence}\\nA: {answer}”,\nwhere sentence is the generated sentence, and\nanswer is Yes if the sentence is a positive example,\nor No if it is a negative example. We ensure that\neach prompt contains exactly 5 positive and 5 neg-\native examples; these can appear in any order. The\nmodel is then given the metalinguistic judgment\ntask of generating “ Yes” or “ No” when given an\nexample from the test split.4 We extract the proba-\nbilities of the “ Yes” and “ No” tokens to determine\nif the model made the correct prediction. We report\nthe accuracy of all the models in Figure 1.\n4The inclusion of the leading space in the answer tokens is\nintentional, and conforms to the token that would have been\nincluded in the prompt if the answer were given to the model.\n3\nFigure 1: Few-shot accuracy on the grammaticality judgment task on hierarchical and linear inputs. On average,\nall models are better at the grammaticality judgment task on hierarchical inputs as compared to linear inputs. On\nhierarchical grammars, models are best at processing English inputs followed by Italian and Japanese. Model-wise\naccuracy on this task is shown in Figure 5 in App. B.1. Grammar-wise accuracy is shown in Table 5 in App. B.1.\nHypothesis.\nNatural language is largely ambigu-\nous with respect to linear versus hierarchical struc-\nture (Chomsky, 1957); human brains have biologi-\ncal preferences for hierarchical structures (Musso\net al., 2003), but LLMs do not have this prefer-\nence built into their architecture (Min et al., 2020;\nMcCoy et al., 2018; Mueller et al., 2022), so it is\nnot clear a priori whether they would treat these\nstructures in the same way. Given results from\nKallini et al. (2024), we hypothesize that models\nwill be significantly more accurate when labeling\nsentences from hierarchical than linear grammars.\nWe also expect larger models to be more accurate.\nResults.\nWe find (Figure 1) that for English and\nItalian grammars, models are better at distinguish-\ning positive and negative examples in hierarchical\ngrammars than linear grammars (p < .001; see Ta-\nble 4). This difference is greater for larger models\nthan smaller ones, perhaps indicating greater func-\ntional specialization with scale. This provides ini-\ntial support for our hypothesis that hierarchical and\nlinear grammars are processed in distinct manners.\n3.2\nExperiment 2: Are the model components\nimplicated in processing hierarchical\nstructures disjoint from those implicated\nin processing linear structures?\nOur behavioral evaluations suggest that LLMs are\nmore accurate on grammaticality judgment tasks\nwith hierarchical inputs, but this does not dis-\nambiguate whether models have separate mecha-\nnisms5 for processing hierarchical and linear gram-\n5We use “mechanism” to refer to a causal chain proceeding\nfrom an initial cause to a final effect. In language models, this\nrefers to a set of causally implicated model components that\nmars.\nIf a model has specialized mechanisms\nfor processing hierarchical and linear grammars,\nwe hypothesize that the set of model components\ncausally responsible for correct grammaticality pre-\ndictions on hierarchical inputs should be different\nfrom those responsible for correct predictions on\nlinear inputs.\nTo test this, we locate neurons in the model that\nare most sensitive towards processing hierarchical\nsyntax. Specifically, we investigate dimensions of\nthe output vector of the MLP and attention sub-\nmodules in each layer.6 We test whether there is\nsignificant overlap between the neurons responsible\nfor processing hierarchical and linear structures.\nRecall that we prompt the model with a task in-\nstruction followed by 10 uniformly sampled demon-\nstrations of positive and negative examples. Given\nthis prompt, we quantify the importance of each\nneuron z in increasing the logit difference m be-\ntween the correct and incorrect answer tokens y\nand y′ for a test sentence t. In other words, given\na language model M, m = M(t)y′ −M(t)y;\nM(t)y and M(t)y′ are the logits corresponding to\nthe correct and incorrect answer tokens. We com-\npute the component z’s indirect effect (IE; Pearl,\n2001; Robins and Greenland, 1992) on m given the\ntest sentence t, and a minimally different sentence\nt′ that flips the correct answer from y to y′.7 Acti-\nexplain how inputs are transformed into the observed output\nbehavior m, which we define below.\n6For MLPs, we use the output of the down-projection after\nthe non-linear transformation. For attention, we use the output\nof the out projection.\n7If t is a positive example, then t′ is the corresponding\nnegative example formed by swapping the appropriate word(s)\nor modifying the sentence. If t is a negative example, then t′\nis the corresponding positive example.\n4\nvation patching (Vig et al., 2020; Finlayson et al.,\n2021; Geiger et al., 2020; Meng et al., 2022), a\ncommon procedure for computing the IE of model\ncomponents, entails computing the IE as follows:\nIE(m; z; t, t′) = m(t|do(zt = zt′)) −m(t)\n(1)\nActivation patching is computationally expen-\nsive, as the number of required forward passes\nscales linearly with the number of neurons. There-\nfore, we instead use attribution patching (Kramár\net al., 2024; Syed et al., 2024), a first-order Taylor\napproximation of the IE:\nˆIE(m; z; t, t′) = ∇zm|t (zt′ −zt)\n(2)\nˆIE can be computed for all z using only 2 forward\npasses and 1 backward pass; i.e., the number of\npasses is constant with respect to the number of\nneurons. While not a perfect approximation, ˆIE\ncorrelates almost perfectly with IE in typical cases\n(Kramár et al., 2024; Marks et al., 2024).8\nWe select the top 1% of both attention and MLP\nneurons in the model by ˆIE. We compute the pair-\nwise overlap of this top 1% neuron subset for each\npair of grammars to measure mechanistic overlap.\nHypothesis.\nIf there are distinct mechanisms for\nprocessing hierarchical and linear grammars, there\nshould be significant overlap between pairs of hi-\nerarchical structures, and significant overlap be-\ntween pairs of linear structures. However, overlaps\nacross linear and hierarchical structures should be\nsignificantly lower than overlaps between pairs of\nhierarchical grammars or pairs of linear grammars.\nResults.\nWe first observe that all mean pairwise\ncomponent overlaps are significantly different from\n0 (Figure 3). However, this overlap is significantly\nhigher (p < 0.001) between pairs of hierarchical\ngrammars than across pairs of hierarchical and lin-\near grammars (See Table 8 in App. B.2 and Fig-\nure 2). This holds across English, Italian, and\nJapanese. This supports the hypothesis that LLMs\nuse specialized components for processing hierar-\nchical syntax that are distinct from those responsi-\nble for processing linear syntax.\nWe also observe that linear structures that share\na rule across languages, such as inversions, show\nstronger overlaps than arbitrary pairs of linear struc-\ntures (Figures 8 and 11 in Appendix B.2). This\nserves as a sanity check that the component over-\nlaps correlate with structural similarities.\n8Except at the first and last layer, where the correlation is\nstill strong but significantly lower.\nFigure 2: Mean pairwise overlap percentage of the top\n1% of neurons from hierarchical (H) or linear (L) gram-\nmars. We show means across models (error bars are\nstandard errors); see Figure 7a in App. B.2 for model-\nwise results. Overlaps are significantly (p < 0.001,\nTable 8) different between hierarchical-hierarchical\npairs and linear-linear pairs, and between hierarchical-\nhierarchical pairs and hierarchical-linear pairs.\n3.3\nExperiment 3: Does ablating\nhierarchy-sensitive components affect\nperformance on linear grammars, and\nvice versa?\nWe have located neurons responsible for processing\nhierarchical and linear grammars. If these neurons\nare selective for only hierarchical or linear struc-\nture, then ablating them should selectively impact\nthe model’s performance on the grammaticality\njudgment tasks from §3.1. We now perform an ab-\nlation experiment to causally verify this prediction.\nLet ¯ai be the mean activation of neuron a at to-\nken position i across training examples. We first\ncache ¯ai for each MLP and attention output dimen-\nsion. We then run three additional iterations of\nthe grammaticality judgment task from §3.1, each\nwhile ablating a different set of components. (i)\nWe ablate the union of the top 1% of neurons by\nˆIE across hierarchical grammars. (ii) We take the\nunion of the top 1% of neurons across linear gram-\nmars, subsample to the same number of neurons as\nin the hierarchical union (subsampling procedure\ndescribed below), and ablate this set. Finally, (iii)\nwe ablate a random uniform subsample of neurons,\nwhere the number of ablated neurons is the same\nas in (i) and (ii). Sets (i) and (ii) are derived from\n§3.3. We call the hierarchy-sensitive neuron set H\nand the linearity-sensitive neuron set L.\nDue to the strong overlaps between components\nresponsible for processing hierarchical syntax and\nonly minimal overlaps between components re-\nsponsible for processing linear syntax, we observe\n5\nthat |L| ≈2|H|. We therefore subsample L to be\nthe same size as H by (1) sorting components in L\nby their effect size (as found in §3.3) and (2) keep-\ning the top |Hℓ| components from layer ℓ, where\n|Hℓ| is the size of H at layer ℓ. When ablating\nthe uniform subsample, we uniformly sample and\nablate |Hℓ| components in each layer ℓ.\nHypothesis.\nIf H and L are functionally distinct,\nthen ablating H should reduce performance on\nhierarchical grammars more than ablating L and\nmore than ablating random components. Ablating\nL should reduce performance on linear grammars\nmore than ablating H and more than ablating ran-\ndom components.\nFigure 3: Mean relative change in accuracy across mod-\nels (error bars are standard errors) after ablating the top\n1% of neurons from hierarchical (H) or linear (L) gram-\nmars. We compare to a random ablation baseline. For\nmodel-wise ablations, see Figure 12a in App. B.3.\nResults.\nAblating components from H decreases\nthe models’ accuracy on hierarchical structures sig-\nnificantly more than ablating components in L (see\nTable 9 in App. 3.3 for significance tests). Ab-\nlating components from L decreases the model’s\naccuracy on linear structures more than ablating H.\nAblating uniformly sampled components causes a\nlower decrease in performance (if any) compared\nto ablations from the H or L sets.\nThese results are further mediated by model\nand language. The Llama-3.1 models as well as\nMistral-v0.3 and QWen-2 (1.5B) show larger de-\ncreases in relative accuracy on hierarchical and\nlinear inputs when ablating the H and L sets, re-\nspectively.\nLlama-2 and QWen-2 (0.5B) show\nsimilar changes in performance under ablations,\nthough not in the selective manner we observe for\nother models. At the language level, these trends\nare consistent across English and Italian, but only\nsometimes generalize to Japanese. Overall, our\nresults suggest that for Llama-3.1, Mistral-v0.3,\nand QWen (1.5B), the components discovered in\n§3.3 selectively reduce model performance in an\nexpected manner in English and Italian. For other\nmodels, there is more mechanistic overlap in how\ngrammatically judgments are performed for hierar-\nchical and linear inputs. Thus, we largely find sup-\nport for the hypothesis of hierarchical functional\nselectivity. Exceptions primarily include smaller\nmodels, and results in Japanese (a less frequent lan-\nguage in the training corpora of these models); this\nprovides preliminary evidence that greater func-\ntional specialization may emerge with scale, both\nwith respect to dataset size and number of parame-\nters.\n3.4\nExperiment 4: Are these neurons sensitive\nto hierarchical structure or in-distribution\nlanguage?\nThus far, our results have been confounded by the\nfact that hierarchical sentences are commonly at-\ntested in the natural language corpora that LLMs\nare trained on, whereas linear sentences would be\nunlikely to appear. Thus, it is unclear if we have\nobserved functional selectivity toward hierarchi-\ncal language, or merely toward in-distribution lan-\nguage. To address this confound, we propose ad-\nditional experiments using sentences constructed\nfrom nonce words—what Fedorenko et al. (2016)\ncall Jabberwocky sentences (abbreviated ZZ).\nWe define a bijective mapping from all words\nin the English grammars to nonce words; see Ta-\nble 3 for examples. Then, we replicate our previ-\nous experiments on this set of out-of-distribution\nJabberwocky sentences. By preserving the distinc-\ntion between hierarchical and linear grammars and\nusing a meaningless lexicon, we can disentangle\nhierarchy-sensitive mechanisms from mechanisms\nthat are merely sensitive to natural language distri-\nbutions resembling those in the training corpus.\nHypotheses.\nIn humans, Jabberwocky sentences\ncause a smaller increase in neural activity as com-\npared to natural sentences (Fedorenko et al., 2016),\nimplying that the language processing regions of\nthe brain are not sensitive to Jabberwocky sen-\ntences. If language models are similarly selec-\ntive for meaningful inputs—and therefore, if the\nH neurons from previous experiments are actu-\nally in-distribution-language neurons, and if the L\nneurons are actually out-of-distribution language\nneurons—then we expect the following three trends.\n6\n(a)\n(b)\n(c)\n(d)\nFigure 4: Results on Jabberwocky grammars. We show grammaticality judgment task performance (a), mean neuron\noverlap percentages between Jabberwocky hierarchical and linear grammars (b), neuron overlaps between English\nand Jabberwocky grammars (c), and the mean relative changes in accuracy as measured on Jabberwocky grammars\nafter ablating top 1% of neurons corresponding to English grammars (d). See App. B.4 for model-wise results.\n(1) There should not be significant differences in\nmodel performance on grammaticality judgments\nfor hierarchical and linear Jabberwocky grammars.\n(2) There should be little overlap between the hier-\narchical English and Jabberwocky neurons; by im-\nplication, ablating neurons discovered from natural\nlanguage inputs should not affect performance on\nJabberwocky sentences. (3) It is not clear whether\nwe should expect distinct mechanisms for process-\ning H and L Jabberwocky grammars; if there is an\nabstract acceptability judgment circuit that is not\ntied to natural language, then it should be present\nin the linear natural-language neurons. Thus, we\nhypothesize that the L neurons from previous ex-\nperiments will affect performance on Jabberwocky\nsentences more than the H neurons.\nResults.\nIn behavioral experiments using Jabber-\nwocky sentences, we find (Figure 4a) that the gap\nin performance from hierarchical to linear gram-\nmars is significantly lower than that for English\ngrammars—but still consistently present across\nmodels. The lower gap could be because perfor-\nmance is closer to chance than for natural gram-\nmars. The small gap in performance partially con-\ntradicts Hypothesis 1, but does not provide strong\nenough evidence to confidently reject.\nAttribution patching results (Figure 4b) suggest\nthat the components used to correctly judge hier-\narchical and linear Jabberwocky inputs are largely\ndisjoint: overlaps between pairs of hierarchical\nstructures are significantly higher than overlaps\nacross pairs of hierarchical and linear grammars.\nMoreover, the components used to process hierar-\nchical English grammars are strongly shared with\nthe components that are used to process hierarchi-\ncal Jabberwocky grammars (Figure 4c), while over-\nlaps between linear English grammars and hierar-\nchical Jabberwocky grammars is low. This con-\ntradicts Hypotheses 2 and 3, suggesting that the\nhierarchy-sensitive mechanisms we have observed\nin LLMs may be more abstract and generalized\nthan those in humans.\nLastly, we observe (Figure 4d) that ablating the\ntop 1% of neurons from the English hierarchical\ngrammars causes a significant decrease in accuracy\nwhen processing hierarchical Jabberwocky inputs;\nsimilar decreases in linear Jabberwocky accuracy\nresult from ablating English linear components.\nThis suggests that the causally relevant natural-\nlanguage and Jabberwocky neuron sets are shared\nto a significant extent (see Table 11 in App. B.4).\nFurther, ablations to English hierarchical compo-\nnents causes selective decreases in Jabberwocky\nhierarchical accuracy; selectivity is lower when\nablating English linear components.\nTaken together, these results provide evidence\nthat LLMs’ hierarchy-sensitive and linearity-\nsensitive component sets are sensitive primarily\nto the structure of the grammar, and only depend\non grounding in meaning or in-distribution lan-\nguage to a minor extent. This provides significant\n(p < .05) and causal evidence against Hypothesis\n2, which we reject. Results from Figure 4d and\nTable 11 also give sufficient evidence to reject Hy-\npothesis 3. Thus, there exist grammaticality judg-\nment mechanisms that are selective for hierarchical\nstructure in a highly abstract manner, and that do\nnot merely select for in-distribution language. That\nsaid, there are components are selective to both\nhierarchical and in-distribution language, but these\ndo not make up the majority of the components\nfound in previous experiments.\n7\n4\nDiscussion and Related Work\nAcquiring syntax-selective subnetworks.\nWe\nfind behavioral and causal evidence supporting the\nhypothesis that hierarchical and linear grammars\nare processed using largely disjoint mechanisms in\nlarge language models. Thus, as in humans (Baker\net al., 2007), general-purpose learners such as lan-\nguage models can acquire functionally specific re-\ngions. To some extent, linguistic functional selec-\ntivity in LLMs is surprising: humans process many\nmore modalities and signal types than language\nalone, so functional specialization toward linguis-\ntic signals may be sensible as one among many\nmodal specializations (Kanwisher, 2010). How-\never, unimodal language models like those we test\nare exposed only to text. While not all of this text\nis natural language, one might expect a larger por-\ntion of the model to be responsible for processing\nhierarchical structure. These as well as our behav-\nioral results extend prior evidence that pretraining\ninduces preferential reliance on syntactic features\nover positional features (Mueller et al., 2022; Murty\net al., 2023; Ahuja et al., 2024),9 and supports\nprior findings that there exist syntax-selective—and\nmore broadly, language-selective—subnetworks in\nLLMs (AlKhamissi et al., 2024; Sun et al., 2024).\nOn human-likeness and learnability.\nNote that\nhierarchical functional specialization is not evi-\ndence that humans and LLMs process language\nin the same manner. Fedorenko et al. (2016) find\nthat language processing circuits in the brain acti-\nvate significantly less on Jabberwocky sentences,\nwhereas we observe significant overlaps (albeit\nnot complete) in these circuits in LLMs.\nThis\nsuggests some degree of selectivity for natural\nin-distribution language, as in humans, but the\nhierarchy-sensitive mechanisms are also more ab-\nstract and not tied to meaning as in humans.\nThere is evidence that hierarchical grammars are\neasier to learn than grammars that do not occur\nin human languages (Kallini et al., 2024; Ahuja\net al., 2024). This could provide an explanation for\nwhy language models are so attuned to this struc-\nture and learn to explicitly represent it: it is easier\n9Note, however, that these behavioral results may be ex-\nplainable using teleological approaches such as those in Mc-\nCoy et al. (2024): linear grammaticality judgment is a low-\nprobability task and contains low-probability inputs (assum-\ning a pretraining distribution based on Internet text), and will\ntherefore be more difficult for a language model to perform,\neven if the model used a shared mechanism to perform each\ngrammaticality judgment task in this study.\nto learn a hierarchical organization than flat orga-\nnization of a vocabulary, and it may simply be a\nmore efficient explanation of the distribution. That\nsaid, randomly shuffling input data does not seem\nto destroy downstream performance (Sinha et al.,\n2021), despite destroying performance on struc-\ntural probing tasks (Hewitt and Manning, 2019).\nFuture work should investigate the relationship be-\ntween the syntax-sensitive components we discover\nand performance on downstream NLP tasks.\nMechanistic interpretability.\nUsing causal lo-\ncalizations to investigate the mechanisms under-\nlying model behaviors has recently become more\npopular (e.g., Wang et al., 2023; Hanna et al., 2023;\nPrakash et al., 2024; Merullo et al., 2024; Bayazit\net al., 2024). While localization is not equivalent\nto explanation, it can reveal distinctions in where\nand how certain phenomena are encoded in activa-\ntion space. Future work could employ techniques\nfrom the training dynamics and mechanistic inter-\npretability literature to better understand how and\nwhen these components arise during pretraining,\nas well as the (presumably numerous) functional\nsub-roles of these distinct component sets.\nMore broadly, this work suggests a less-explored\ndirection in interpretability based on high-level\ncoarse-grained abstractions. Much recent work\nhas aimed to discover more fine-grained and single-\npurpose units of causal analysis (e.g., sparse au-\ntoencoder features; Bricken et al., 2023; Cunning-\nham et al., 2024; Marks et al., 2024); we believe\nthat a parallel direction based in functionally coher-\nent sets (or subgraphs) of components would yield\nequally interesting insights. For example, effective\nrepresentations of syntax are a necessary condition\nfor robust language understanding and generation;\nthus, we would expect the hierarchy-sensitive com-\nponents we discover to be implicated in any NLP\ntask if the model were robustly understanding the\ninputs. Therefore, not relying on these components\ncould be a signal that models have learned to rely\non some mixture of heuristics.\n5\nConclusion\nWe have investigated whether there exist localiz-\nable and functionally distinct sets of components\nfor processing hierarchically versus linearly struc-\ntured language inputs. We find behavioral and\ncausal evidence that these component sets are dis-\ntinct, both in location and in their functional role\nin the network.\n8\nLimitations\nWe acknowledge that our work could be improved\nin several respects. First, neurons and attention\noutputs are problematic units of analysis due to pol-\nysemanticity (Elhage et al., 2022); i.e., observing\nthe activations of a component is often not informa-\ntive, as they are sensitive to many features simul-\ntaneously. Further, the component sets we analyze\nare unordered sets, which means that we do not\nyet understand how many distinct mechanisms are\nresponsible for the behaviors we observe, nor what\nthese mechanisms qualitatively represent. We have\nalso not evaluated the effect of these components\non tasks outside of grammaticality judgments; thus,\nwe do not yet understand how selective nor how\nrobust these behaviors or localizations are under\ndifferent settings.\nSecond, the grammaticality judgment task may\nprime the model to be sensitive to valid linguistic\nstructures more generally, rather than the structures\nthat we present to the models. Therefore, we cannot\nconfidently conclude that the significant accuracy\ndifferences we observe will generalize to other task\nsettings or prompt formats given the same gram-\nmars.\nReferences\nKabir Ahuja, Vidhisha Balachandran, Madhur Panwar,\nTianxing He, Noah A. Smith, Navin Goyal, and Yulia\nTsvetkov. 2024. Learning syntax without planting\ntrees: Understanding when and why transformers\ngeneralize hierarchically. In ICML 2024 Workshop\non Mechanistic Interpretability.\nBadr AlKhamissi, Greta Tuckute, Antoine Bosselut,\nand Martin Schrimpf. 2024.\nThe LLM language\nnetwork: A neuroscientific approach for identify-\ning causally task-relevant units.\narXiv preprint\narXiv:2411.02280.\nMarie Amalric and Stanislas Dehaene. 2019. A distinct\ncortical network for mathematical knowledge in the\nhuman brain. NeuroImage, 189:19–31.\nIan A Apperly, Dana Samson, Naomi Carroll, Shazia\nHussain, and Glyn Humphreys. 2006. Intact first-and\nsecond-order false belief reasoning in a patient with\nseverely impaired grammar. Social neuroscience,\n1(3-4):334–348.\nChris I. Baker, Jia Liu, Lawrence L. Wald, Kenneth K.\nKwong, Thomas Benner, and Nancy Kanwisher.\n2007. Visual word processing and experiential ori-\ngins of functional selectivity in human extrastriate\ncortex. Proceedings of the National Academy of Sci-\nences, 104(21):9087–9092.\nDeniz Bayazit, Negar Foroutan, Zeming Chen, Gail\nWeiss, and Antoine Bosselut. 2024.\nDiscovering\nknowledge-critical subnetworks in pretrained lan-\nguage models. In Proceedings of the 2024 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 6549–6583, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nTrenton Bricken, Adly Templeton, Joshua Batson,\nBrian Chen, Adam Jermyn, Tom Conerly, Nick\nTurner, Cem Anil, Carson Denison, Amanda Askell,\nRobert Lasenby, Yifan Wu, Shauna Kravec, Nicholas\nSchiefer, Tim Maxwell, Nicholas Joseph, Zac\nHatfield-Dodds, Alex Tamkin, Karina Nguyen, Bray-\nden McLean, Josiah E Burke, Tristan Hume, Shan\nCarter, Tom Henighan, and Christopher Olah. 2023.\nTowards monosemanticity: Decomposing language\nmodels with dictionary learning. Transformer Cir-\ncuits Thread.\nPaul Broca. 1861. Remarques sur le siége de la faculté\nlangage articulé; suives d’une observation d’aphémie.\nBulletins et mémoires de la Société Anatomique de\nParis, 6:330–357.\nXuanyi Chen, Josef Affourtit, Rachel Ryskin, Tamar I\nRegev, Samuel Norman-Haignere, Olessia Jouravlev,\nSaima Malik-Moraleda, Hope Kean, Rosemary Var-\nley, and Evelina Fedorenko. 2023. The human lan-\nguage system, including its inferior frontal compo-\nnent in “broca’s area,” does not support music per-\nception. Cerebral Cortex, 33(12):7904–7929.\nNoam Chomsky. 1957.\nSyntactic structures.\nDe\nGruyter Mouton.\nNoam Chomsky. 1965. Aspects of the theory of syntax.\nThe MIT Press.\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert\nHuben, and Lee Sharkey. 2024. Sparse autoencoders\nfind highly interpretable features in language models.\nIn The Twelfth International Conference on Learning\nRepresentations.\nNelson Elhage, Tristan Hume, Catherine Olsson,\nNicholas Schiefer, Tom Henighan, Shauna Kravec,\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain,\nCarol Chen, Roger Grosse, Sam McCandlish, Jared\nKaplan, Dario Amodei, Martin Wattenberg, and\nChristopher Olah. 2022. Toy models of superpo-\nsition. Transformer Circuits Thread.\nMartin BH Everaert, Marinus AC Huybregts, Noam\nChomsky, Robert C Berwick, and Johan J Bolhuis.\n2015. Structures, not strings: Linguistics as part of\nthe cognitive sciences. Trends in cognitive sciences,\n19(12):729–743.\nEvelina Fedorenko, Michael K Behr, and Nancy Kan-\nwisher. 2011. Functional specificity for high-level lin-\nguistic processing in the human brain. Proceedings\nof the National Academy of Sciences, 108(39):16428–\n16433.\n9\nEvelina Fedorenko, Terri L. Scott, Peter Brunner,\nWilliam G. Coon, Brianna Pritchett, Gerwin Schalk,\nand Nancy Kanwisher. 2016. Neural correlate of the\nconstruction of sentence meaning. Proceedings of\nthe National Academy of Sciences, 113(41):E6256–\nE6262.\nEvelina Fedorenko and Rosemary Varley. 2016. Lan-\nguage and thought are not the same thing: evidence\nfrom neuroimaging and neurological patients. Annals\nof the New York Academy of Sciences, 1369(1):132–\n153.\nMatthew\nFinlayson,\nAaron\nMueller,\nSebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021.\nCausal analysis of syntactic\nagreement mechanisms in neural language models.\nIn Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1:\nLong\nPapers), pages 1828–1843, Online. Association for\nComputational Linguistics.\nAtticus Geiger, Kyle Richardson, and Christopher Potts.\n2020.\nNeural natural language inference models\npartially embed theories of lexical entailment and\nnegation. In Proceedings of the Third BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Net-\nworks for NLP, pages 163–173, Online. Association\nfor Computational Linguistics.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, Arun Rao, Aston Zhang, Aurelien Ro-\ndriguez, Austen Gregerson, Ava Spataru, Baptiste\nRoziere, Bethany Biron, Binh Tang, Bobbie Chern,\nCharlotte Caucheteux, Chaya Nayak, Chloe Bi,\nChris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDanny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,\nElina Lobanova, Emily Dinan, Eric Michael Smith,\nFilip Radenovic, Francisco Guzmán, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis An-\nderson, Govind Thattai, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Is-\nhan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,\nJaewon Lee, Jan Geffert, Jana Vranes, Jason Park,\nJay Mahadeokar, Jeet Shah, Jelmer van der Linde,\nJennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,\nJiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Jun-\nteng Jia, Kalyan Vasuden Alwala, Karthik Prasad,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth\nHeafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal\nLakhotia, Lauren Rantala-Yeary, Laurens van der\nMaaten, Lawrence Chen, Liang Tan, Liz Jenkins,\nLouis Martin, Lovish Madaan, Lubo Malo, Lukas\nBlecher, Lukas Landzaat, Luke de Oliveira, Madeline\nMuzzi, Mahesh Pasupuleti, Mannat Singh, Manohar\nPaluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kam-\nbadur, Mike Lewis, Min Si, Mitesh Kumar Singh,\nMona Hassan, Naman Goyal, Narjes Torabi, Niko-\nlay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nNing Zhang, Olivier Duchenne, Onur Çelebi, Patrick\nAlrassy, Pengchuan Zhang, Pengwei Li, Petar Va-\nsic, Peter Weng, Prajjwal Bhargava, Pratik Dubal,\nPraveen Krishnan, Punit Singh Koura, Puxin Xu,\nQing He, Qingxiao Dong, Ragavan Srinivasan, Raj\nGanapathy, Ramon Calderer, Ricardo Silveira Cabral,\nRobert Stojnic, Roberta Raileanu, Rohan Maheswari,\nRohit Girdhar, Rohit Patel, Romain Sauvestre, Ron-\nnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sa-\nhana Chennabasappa, Sanjay Singh, Sean Bell, Seo-\nhyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sha-\nran Narang, Sharath Raparthy, Sheng Shen, Shengye\nWan, Shruti Bhosale, Shun Zhang, Simon Van-\ndenhende, Soumya Batra, Spencer Whitman, Sten\nSootla, Stephane Collot, Suchin Gururangan, Syd-\nney Borodinsky, Tamar Herman, Tara Fowler, Tarek\nSheasha, Thomas Georgiou, Thomas Scialom, Tobias\nSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal\nKarn, Vedanuj Goswami, Vibhor Gupta, Vignesh\nRamanathan, Viktor Kerkez, Vincent Gonguet, Vir-\nginie Do, Vish Vogeti, Vítor Albiero, Vladan Petro-\nvic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-\nney Meers, Xavier Martinet, Xiaodong Wang, Xi-\naofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xin-\nfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen,\nYiwen Song, Yuchen Zhang, Yue Li, Yuning Mao,\nZacharie Delpierre Coudert, Zheng Yan, Zhengxing\nChen, Zoe Papakipos, Aaditya Singh, Aayushi Sri-\nvastava, Abha Jain, Adam Kelsey, Adam Shajnfeld,\nAdithya Gangidi, Adolfo Victoria, Ahuva Goldstand,\nAjay Menon, Ajay Sharma, Alex Boesenberg, Alexei\nBaevski, Allie Feinstein, Amanda Kallet, Amit San-\ngani, Amos Teo, Anam Yunus, Andrei Lupu, An-\ndres Alvarado, Andrew Caples, Andrew Gu, Andrew\nHo, Andrew Poulton, Andrew Ryan, Ankit Ramchan-\ndani, Annie Dong, Annie Franco, Anuj Goyal, Apara-\njita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yaz-\ndan, Beau James, Ben Maurer, Benjamin Leonhardi,\nBernie Huang, Beth Loyd, Beto De Paola, Bhargavi\nParanjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han-\ncock, Bram Wasti, Brandon Spence, Brani Stojkovic,\nBrian Gamido, Britt Montalvo, Carl Parker, Carly\nBurton, Catalina Mejia, Ce Liu, Changhan Wang,\nChangkyu Kim, Chao Zhou, Chester Hu, Ching-\nHsiang Chu, Chris Cai, Chris Tindal, Christoph Fe-\nichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty,\nDaniel Kreymer, Daniel Li, David Adkins, David\nXu, Davide Testuggine, Delia David, Devi Parikh,\nDiana Liskovich, Didem Foss, Dingkang Wang, Duc\n10\nLe, Dustin Holland, Edward Dowling, Eissa Jamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn,\nEmily Wood, Eric-Tuan Le, Erik Brinkman, Este-\nban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,\nFelix Kreuk, Feng Tian, Filippos Kokkinos, Firat\nOzgenel, Francesco Caggioni, Frank Kanayet, Frank\nSeide, Gabriela Medina Florez, Gabriella Schwarz,\nGada Badeer, Georgia Swee, Gil Halpern, Grant\nHerman, Grigory Sizov, Guangyi, Zhang, Guna\nLakshminarayanan, Hakan Inan, Hamid Shojanaz-\neri, Han Zou, Hannah Wang, Hanwen Zha, Haroun\nHabeeb, Harrison Rudolph, Helen Suk, Henry As-\npegren, Hunter Goldman, Hongyuan Zhan, Ibrahim\nDamlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis,\nIrina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher,\nJean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-\nnifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy\nTeboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan Mc-\nPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang,\nKai Wu, Kam Hou U, Karan Saxena, Kartikay Khan-\ndelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Ki-\nran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A,\nLeandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrst-\nedt, Madian Khabsa, Manav Avalani, Manish Bhatt,\nMartynas Mankus, Matan Hasson, Matthew Lennie,\nMatthias Reso, Maxim Groshev, Maxim Naumov,\nMaya Lathi, Meghan Keneally, Miao Liu, Michael L.\nSeltzer, Michal Valko, Michelle Restrepo, Mihir Pa-\ntel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark,\nMike Macey, Mike Wang, Miquel Jubert Hermoso,\nMo Metanat, Mohammad Rastegari, Munish Bansal,\nNandhini Santhanam, Natascha Parks, Natasha\nWhite, Navyata Bawa, Nayan Singhal, Nick Egebo,\nNicolas Usunier, Nikhil Mehta, Nikolay Pavlovich\nLaptev, Ning Dong, Norman Cheng, Oleg Chernoguz,\nOlivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin\nKent, Parth Parekh, Paul Saab, Pavan Balaji, Pe-\ndro Rittner, Philip Bontrager, Pierre Roux, Piotr\nDollar, Polina Zvyagina, Prashant Ratanchandani,\nPritish Yuvraj, Qian Liang, Rachad Alao, Rachel\nRodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky\nWang, Russ Howes, Ruty Rinott, Sachin Mehta,\nSachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara\nChugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov,\nSatadru Pan, Saurabh Mahajan, Saurabh Verma,\nSeiji Yamamoto, Sharadh Ramaswamy, Shaun Lind-\nsay, Shaun Lindsay, Sheng Feng, Shenghao Lin,\nShengxin Cindy Zha, Shishir Patil, Shiva Shankar,\nShuqiang Zhang, Shuqiang Zhang, Sinong Wang,\nSneha Agarwal, Soji Sajuyigbe, Soumith Chintala,\nStephanie Max, Stephen Chen, Steve Kehoe, Steve\nSatterfield, Sudarshan Govindaprasad, Sumit Gupta,\nSummer Deng, Sungmin Cho, Sunny Virk, Suraj\nSubramanian, Sy Choudhury, Sydney Goldman, Tal\nRemez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim\nMatthews, Timothy Chou, Tzook Shaked, Varun\nVontimitta, Victoria Ajayi, Victoria Montanez, Vijai\nMohan, Vinay Satish Kumar, Vishal Mangla, Vlad\nIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,\nVladimir Ivanov, Wei Li, Wenchen Wang, Wen-\nwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng\nTang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo\nGao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia,\nYe Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao,\nYundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary\nDeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang,\nZhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3\nherd of models. arXiv preprint arXiv:2407.21783.\nMichael Hanna, Ollie Liu, and Alexandre Variengien.\n2023. How does GPT-2 compute greater-than?: In-\nterpreting mathematical abilities in a pre-trained lan-\nguage model. In Thirty-seventh Conference on Neu-\nral Information Processing Systems.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAnna A Ivanova, Zachary Mineroff, Vitor Zimmerer,\nNancy Kanwisher, Rosemary Varley, and Evelina Fe-\ndorenko. 2021. The language network is recruited\nbut not required for nonverbal event semantics. Neu-\nrobiology of Language, 2(2):176–201.\nAnna A Ivanova, Shashank Srikant, Yotaro Sueoka,\nHope H Kean, Riva Dhamala, Una-May O’reilly, Ma-\nrina U Bers, and Evelina Fedorenko. 2020. Compre-\nhension of computer code relies primarily on domain-\ngeneral executive brain regions. elife, 9:e58906.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nJulie Kallini, Isabel Papadimitriou, Richard Futrell,\nKyle Mahowald, and Christopher Potts. 2024. Mis-\nsion: Impossible language models. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 14691–14714, Bangkok, Thailand. Association\nfor Computational Linguistics.\nNancy Kanwisher. 2010. Functional specificity in the\nhuman brain: a window into the functional archi-\ntecture of the mind. Proceedings of the national\nacademy of sciences, 107(25):11163–11170.\nJános Kramár, Tom Lieberum, Rohin Shah, and Neel\nNanda. 2024. AtP*: An efficient and scalable method\nfor localizing LLM behaviour to components. arXiv\npreprint arXiv:2403.00745.\n11\nY Liu, J Kim, C Wilson, and M Bedny. 2020. Computer\ncode comprehension shares neural resources with for-\nmal logical inference in the fronto-parietal network.\nbiorxiv, 2020.05. 24.096180.\nSaima Malik-Moraleda, Maya Taliaferro, Steve Shan-\nnon, Niharika Jhingan, Sara Swords, David J Peter-\nson, Paul Frommer, Marc Okrand, Jessie Sams, Ram-\nsey Cardwell, et al. 2023. Constructed languages are\nprocessed by the same brain mechanisms as natural\nlanguages. bioRxiv.\nSamuel Marks, Can Rager, Eric J. Michaud, Yonatan Be-\nlinkov, David Bau, and Aaron Mueller. 2024. Sparse\nfeature circuits: Discovering and editing interpretable\ncausal graphs in language models. arXiv preprint\narXiv:2403.19647.\nR. Thomas McCoy, Robert Frank, and Tal Linzen. 2018.\nRevisiting the poverty of the stimulus: hierarchical\ngeneralization without a hierarchical bias in recurrent\nneural networks. In Proceedings of the 40th Annual\nMeeting of the Cognitive Science Society, CogSci\n2018, Proceedings of the 40th Annual Meeting of\nthe Cognitive Science Society, CogSci 2018, pages\n2096–2101. The Cognitive Science Society.\nR. Thomas McCoy, Shunyu Yao, Dan Friedman,\nMathew D. Hardy, and Thomas L. Griffiths. 2024.\nEmbers of autoregression show how large language\nmodels are shaped by the problem they are trained\nto solve. Proceedings of the National Academy of\nSciences, 121(41):e2322420121.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in gpt. Advances in Neural Information Pro-\ncessing Systems, 35:17359–17372.\nJack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2024.\nCircuit component reuse across tasks in transformer\nlanguage models. In The Twelfth International Con-\nference on Learning Representations.\nJunghyun Min, R. Thomas McCoy, Dipanjan Das,\nEmily Pitler, and Tal Linzen. 2020.\nSyntactic\ndata augmentation increases robustness to inference\nheuristics. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2339–2352, Online. Association for Computa-\ntional Linguistics.\nMartin M Monti, Lawrence M Parsons, and Daniel N\nOsherson. 2009. The boundaries of language and\nthought in deductive inference.\nProceedings of\nthe National Academy of Sciences, 106(30):12554–\n12559.\nAaron Mueller, Robert Frank, Tal Linzen, Luheng Wang,\nand Sebastian Schuster. 2022. Coloring the blank\nslate: Pre-training imparts a hierarchical inductive\nbias to sequence-to-sequence models. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1352–1368, Dublin, Ireland. Association\nfor Computational Linguistics.\nShikhar Murty, Pratyusha Sharma, Jacob Andreas, and\nChristopher Manning. 2023. Grokking of hierarchi-\ncal structure in vanilla transformers. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 439–448, Toronto, Canada. Association for\nComputational Linguistics.\nMariacristina Musso, Andrea Moro, Volkmar Glauche,\nMichel Rijntjes, Jürgen Reichenbach, Christian\nBüchel, and Cornelius Weiller. 2003. Broca’s area\nand the language instinct.\nNature neuroscience,\n6(7):774–781.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. 2022. In-context\nlearning and induction heads. Transformer Circuits\nThread.\nJudea Pearl. 2001. Direct and indirect effects. In Pro-\nceedings of the Seventeenth Conference on Uncer-\ntainty in Artificial Intelligence, pages 411–420. Mor-\ngan Kaufmann.\nNikhil Prakash, Tamar Rott Shaham, Tal Haklay,\nYonatan Belinkov, and David Bau. 2024. Fine-tuning\nenhances existing mechanisms: A case study on en-\ntity tracking. In The Twelfth International Confer-\nence on Learning Representations.\nJames M. Robins and Sander Greenland. 1992. Identi-\nfiability and exchangeability for direct and indirect\neffects. Epidemiology, 3(2):143–155.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2888–2913, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nHaiyang Sun, Lin Zhao, Zihao Wu, Xiaohui Gao, Yutao\nHu, Mengfei Zuo, Wei Zhang, Junwei Han, Tianming\nLiu, and Xintao Hu. 2024.\nBrain-like functional\norganization within large language models. arXiv\npreprint arXiv:2410.19542.\nAaquib Syed, Can Rager, and Arthur Conmy. 2024.\nAttribution patching outperforms automated circuit\ndiscovery. In The 7th BlackboxNLP Workshop.\nEric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron\nMueller, Byron C. Wallace, and David Bau. 2024.\nFunction vectors in large language models. In Pro-\nceedings of the 2024 International Conference on\nLearning Representations.\n12\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nRosemary Varley and Michael Siegal. 2000. Evidence\nfor cognition without grammar from causal reasoning\nand ‘theory of mind’in an agrammatic aphasic patient.\nCurrent Biology, 10(12):723–726.\nRosemary A Varley, Nicolai JC Klessinger, Charles AJ\nRomanowski, and Michael Siegal. 2005. Agram-\nmatic but numerate. Proceedings of the National\nAcademy of Sciences, 102(9):3519–3524.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 30.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating gender bias in lan-\nguage models using causal mediation analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12388–12401. Curran Associates,\nInc.\nW3Techs. 2024. Usage statistics and market share of\ncontent languages for websites, may 2024. Accessed:\n2024-05-18.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2023. Inter-\npretability in the wild: A circuit for indirect object\nidentification in GPT-2 small. In The Eleventh Inter-\nnational Conference on Learning Representations.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nMa, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai,\nJinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-\nqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni,\nPei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize\nGao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,\nTianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge,\nXiaodong Deng, Xiaohuan Zhou, Xingzhang Ren,\nXinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing\nLiu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan,\nYunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nZhifang Guo, and Zhihao Fan. 2024. Qwen2 techni-\ncal report. arXiv preprint arXiv:2407.10671.\nQinan Yu, Jack Merullo, and Ellie Pavlick. 2023. Char-\nacterizing mechanisms for factual recall in language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9924–9959, Singapore. Association for Com-\nputational Linguistics.\nA\nMethods\nA.1\nGrammar rule descriptions\nWe define a series of hierarchical sentences in En-\nglish, Japanese, and Italian.\n• Declarative sentence: For English sentences,\nsubjects and objects can be singular or plural\nnouns. Verbs agree with their subjects. IT\nsentences are Italian translations of the En-\nglish sentences. Unlike Italian and English\nwhich have SVO word order, Japanese trans-\nlations (JP sentences) have SOV word order.\n• Subordinate sentence: In each language, ma-\ntrix subjects, subordinate subjects, matrix ob-\njects, and subordinate objects can be singular\nor plural nouns. In English and Italian, verbs\nof the subordinate subject and the subject\nagree with their respective subjects in num-\nber. We generate subordinate clauses by using\nverbs which take complementizer phrases as\nobjects (e.g., “Tom sees that the dog carries\nthe fish”). English and Italian both place the\nmain clause’s verb before the start of the sub-\nordinate clause, whereas Japanese places the\nmain verb after the end of the clause.\n• Passive sentence: Subjects and objects can\nbe singular or plural nouns. Verbs are always\nin the passive form. Like in (Musso et al.,\n2003), in the passive construction, we include\nthe agent of a transitive verb in a prepositional\nphrase.\n• Null subject sentence: This structure is re-\nstricted to Italian. We use the verb and object\nwithout the subject, since the use of the sub-\nject is not a strict requirement in Italian.10\nLinear Grammars\nSimilar to Musso et al.\n(2003), the linear sentences we test are constructed\nby breaking the hierarchical order between the sub-\nject and the nominal words. While our linear sen-\ntences use English, Italian, and Japanese lexicons,\nthey break the hierarchical relationship between\n10Italian verbal morphology provides all person and num-\nber information needed to understand the subject of a sen-\ntence, whereas English morphology does not provide this in-\nformation. That said, there exist languages without the verbal\nperson/number inflection that optionally allow dropping the\nsubject of the sentence if it is the topic of that sentence, such\nas Mandarin and Japanese; thus, this structure is still attested\nand therefore still qualifies as a hierarchical (UG-compliant)\nstructure.\n13\nthe subject, verb, and object, using the strategies\ndescribed below.\n• Negation: We break the hierarchical order\nby inserting a negation word “doesn´t” after\nthe fifth word in English sentences. In Italian,\nwe insert ‘non’ (IT) after the third word. In\nJapanese, we insert ない(JP) after the third\nword.\n• Inversion: We invert the order of the words\nin a sentence (before tokenization) to form the\nsecond construction.\nThe third construction varies between languages.\n• Wh-word (English): We include a question\nin the subordinate clause of the sentence by\ninserting a ‘wh-’ word (who, why, what etc.)\nat the penultimate token position.\n• Last noun agreement (Italian): We change\nthe subject term’s gender to always match that\nof the final noun in the sentence.\n• Past Tense (Japanese): The Japanese past\ntense construction was built by adding the\nsuffix -ta, not on the verb element as in the hi-\nerarchical grammatical rule for Japanese, but\non the third word, counting from right to left.\nA.2\nDataset Description and Examples\nExamples of all the grammars we construct in En-\nglish, Italian, and Japanese may be found in Table 2.\nExamples of Jabberwocky sentences may be found\nin Table 3.\nB\nExperiments\nB.1\nExperiment 1: Few-shot learning\naccuracy\nExperiment 1 (§3.1) assesses the model’s perfor-\nmance on grammaticality judgments of hierarchical\nand linear structures. Here we share statistical com-\nparisons of the accuracy distributions (Table 4),\naccuracy values by language (Figure 5 and Table 5)\nand grammar-wise accuracy values (Table 5).\nB.1.1\nExample Prompts\nWe present example prompts from one of the hier-\narchical structures for each language. The prompt\nskeleton is in English, regardless of the language\nused for the examples. We intentionally strip the\nfinal whitespace after A:, as the model expects a\nleading space within the answer token (and thus, it\nshould not be present in the prompt).\nEnglish\nexample.\n\"Here\nare\nEnglish\nsentences that either follow or break a\ngrammar rule.\nEach sentence is labeled\n’Yes’ if it follows the rule and ’No’ if\nit doesn’t. Label the final sentence as\n’Yes’ or ’No’ based on whether it follows\nthe same rule.\nQ: Is this sentence grammatical? Yes or\nNo: a woman drinks espresso the\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: the architects touch a mouse\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: the women eat cucumber the\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: the writers drink a lemonade\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: a teacher touches a lightbulb\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: the actress touches toy a\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: a boy kicks bottle a\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: the woman pushes toy a\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: a professor reads a poem\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: the orators read a story\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: the doctor drinks milkshake the\nA:\"\n14\nLanguage\nGrammar\nPositive Example\nNegative Example\nHierarchical\nEnglish (EN)\nDeclarative\na woman reads a chapter\na woman reads chapter a\nSubordinate\nSheela thinks that the woman reads the\nchapter\nSheela thinks that the woman reads chapter the\nPassive\na chapter is read by a woman\na chapter is read by woman a\nItalian (IT)\nDeclarative\nuna donna legge un capitolo\nuna donna legge capitolo un\nSubordinate\nSheela pensa che una donna legge un\ncapitolo\nSheela pensa che la donna legge capitolo un\nPassive\nun capitolo è letto da una donna\nun capitolo è letto da donna una\nJapanese (JP)\nDeclarative\n女性は章を読む\n女性は章読むを\nSubordinate\nシーラは女性が章を読むと考\nえる\nシーラは女性が章を読む考えると\nPassive\n章は女性に読まれる\n章は女性読まれるに\nLinear\nEnglish (EN)\nNegation. Insert “doesn’t” or “don’t”\nat position 5.\na\n1\nwoman\n2\nreads\n3\na\n4\ndoesn’t\n5\nchapter\n6\na\n1\nwoman\n2\nreads\n3\na\n4\nchapter\n5\ndoesn’t\n6\nInversion.\nInvert the declarative\nword order.\nchapter\n5\na\n4\nreads\n3\nwoman\n2\na\n1\nchapter\n5\na\n4\nreads\n3\na\n1\nwoman\n2\nWh-word. Insert wh-word at posi-\ntion 5.\ndid\n1\na\n2\nwoman\n3\nreads\n4\na\n5\nwhen\n6\nchapter?\n7\ndid\n1\na\n2\nwoman\n3\nreads\n4\na\n5\nchapter\n6\nwhen?\n7\nItalian (IT)\nNegation. Insert “no” at position 5.\nuna\n1\ndonna\n2\nlegge\n3\nun\n4\nno\n5\ncapitolo\n6\nuna\n1\ndonna\n2\nlegge\n3\nun\n4\ncapitolo\n5\nno\n6\nInversion.\nInvert the declarative\nword order.\ncapitolo\n5\nun\n4\nlegge\n3\ndonna\n2\nuna\n1\ncapitolo\n5\nun\n4\nlegge\n3\nuna\n1\ndonna\n2\nLast-noun agreement. Make all de-\nterminers agree with the gender of\nthe final noun.\nuna un\n1\ndonna\n2\nlegge\n3\nun\n4\ncapitolo\n5\nuna\n1\ndonna\n2\nlegge\n3\nun\n4\ncapitolo\n5\nJapanese (JP)\nNegation. Insert a negation word at\nposition 4.\n女性\n1\nは\n2\n章\n3\nない\n4\nを\n5\n読む\n6\n女性\n1\nは\n2\n章\n3\nを\n4\n読む\n5\nない\n6\nInversion.\nInvert the declarative\nword order.\n読む\n5\nを\n4\n章\n3\nは\n2\n女性\n1\n読む\n5\nを\n4\n章\n3\n女性\n1\nは\n2\nPast tense.\nInsert the past tense\nmarker at position 4.\n女性\n1\nは\n2\n章\n3\nをた\n4\n読む\n5\n女性\n1\nは\n2\n章\n3\n読む\n4\nをた\n5\nTable 2: Dataset. List of grammars, descriptions of the rule defining each grammar, and corresponding positive and\nnegative examples.\nItalian\nexample.\n\"Here\nare\nItalian\nsentences that either follow or break a\ngrammar rule.\nEach sentence is labeled\n’Yes’ if it follows the rule and ’No’ if\nit doesn’t. Label the final sentence as\n’Yes’ or ’No’ based on whether it follows\nthe same rule.\nQ: Is this sentence grammatical? Yes or\nNo: una donna beve espresso il\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: l’ architette toccano il topo\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: le donne mangiano cetriolo il\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: le scrittrici bevono la limonata\nA: Yes\n15\nGrammar\nPositive Example\nNegative Example\nHierarchical\nDeclarative. Subject, verb, object.\na wug ungos the snorfle\na wug ungos snorfle the\nSubordinate. Subject, verb taking a\nrelative clause complement.\nGomu herdles that the blincos stoffle\nthe pelunko\nGomu herdles that the blincos stoffle reads\npelunko the\nPassive. Like Declarative, but in\nthe passive voice.\nthe gunzle is snugoed by the wugen\nthe gunzle is snugoed by wugen the\nLinear\nNegation. Insert “doesn’t” or “don’t”\nat position 5.\nthe\n1\narcuplos\n2\nungo\n3\na\n4\ndoesn’t\n5\nblorft\n6\nthe\n1\narcuplos\n2\nungo\n3\na\n4\nblorft\n5\ndoesn’t\n6\nInversion. Invert the word order of\nDeclarative.\nsnorfle\n5\nthe\n4\nungos\n3\nwug\n2\na\n1\nsnorfle\n5\nthe\n4\nungos\n3\na\n1\nwug\n2\nWh-word. Insert wh-word at posi-\ntion 5.\nDid\n1\na\n2\nknurkle\n3\ngurdles\n4\na\n5\nwhen\n6\nskerpo?\n7\nDid\n1\na\n2\nknurkle\n3\ngurdles\n4\na\n5\nskerpo\n6\nwhen?\n7\nTable 3: Jabberwocky dataset. List of grammars, descriptions of the rules defining each grammar, and positive\n(grammatical) and negative (ungrammatical) examples for each. We use similar prompt constructions as in the\nEnglish examples (Also see §B.1.1).\nQ: Is this sentence grammatical? Yes or\nNo: un’ insegnante tocca una lampadina\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: l attrice tocca giocattolo un\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: un ragazzo calcia bottiglia una\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: la donna spinge giocattolo un\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: una professoressa legge un poema\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: gli oratori leggono la storia\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: la dottoressa beve frappè il\nA:\"\nJapanese example.\n\"Here\nare\nJapanese\nsentences that either follow or break a\ngrammar rule.\nEach sentence is labeled\n’Yes’ if it follows the rule and ’No’ if\nit doesn’t. Label the final sentence as\n’Yes’ or ’No’ based on whether it follows\nthe same rule.\nQ: Is this sentence grammatical? Yes or\nNo: 女性はエスプレッソ飲むを\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: 建築家たちはマウスを触る\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: 女性たちは胡瓜食べるを\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: 作家たちはレモネードを飲む\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: 教師は電球を触る\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: 女優は玩具触るを\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: 少年はボトル蹴るを\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: 女性は玩具押すを\nA: No\nQ: Is this sentence grammatical? Yes or\nNo: 教授は詩を読む\nA: Yes\nQ: Is this sentence grammatical? Yes or\n16\nNo: 演説家たちは小説を読む\nA: Yes\nQ: Is this sentence grammatical? Yes or\nNo: 医者はミルクセーキ飲むを\nA:\"\nB.2\nExperiment 2: Identify MLP and\nAttention Components with the highest ˆIE\nExperiment 2 locates MLP and Attention neurons\nthat are implicated in processing hierarchical and\nlinear structures, and investigates if these compo-\nnents are disjoint. The language-level pairwise\noverlaps for all models across English, Italian and\nJapanese hierarchical and linear inputs, as well as\nJabberwocky hierarchical and linear inputs is given\nin Figures 7. Grammar wise overlaps for MLP\nand attention components for English, Italian, and\nJapanese grammars are shown in Figures 8 and 11,\nrespectively. Grammar wise overlaps for MLP and\nattention components for Jabberwocky grammars\nare shown in Figures 8 and 11, respectively. We\nalso share results testing whether the pairwise mean\noverlaps of H, L, and HxL structures are signifi-\ncantly different among grammars using English,\nItalian, and Japanese versus Jabberwocky tokens in\nTables 8 and 7.\nB.3\nExperiment 3: Ablations of top 1% of\nAttention and MLP Components\nExperiment 3 considers selective ablations of hi-\nerarchy and linearity sensitive components, and\nevaluates how these ablations impact the accuracy\nof the model on the in-context learning task. We\nshare ablation results by model in Figure 12a for\nEnglish, Italian, and Japanese grammars. Through\nmodel-wise comparisons, we find that relative accu-\nracy decreases on hierarchical grammars are signif-\nicantly different for English, Italian and Japanese\ngrammars depending on whether hierarchical, lin-\near, or uniformly sampled components are ablated\n(see Table 9). However, the same is not true for\nlinear grammars—relative accuracy decreases are\nnot significantly different between ablations of hi-\nerarchical/linear components. In the case of Italian\nstructures, ablating hierarchy-sensitive components\nappears akin to ablating uniformly sampled com-\nponents.\nAdditionally, we also run ablation experiments\non jabberwocky grammars using components that\nare sensitive to hierarchical or linear jabberwocky\ngrammars. We share ablation results by model for\njabberwocky grammars in Figure 12b. Here also\nwe find that ablating hierarchy versus linearity sen-\nsitive components can cause a significant difference\nin the decrease in accuracy on jabberwocky hier-\narchical grammars relative to the no ablation case.\nThis is not true for jabberwocky linear grammars\n(See Table 10).\nB.4\nExperiment 4: Are neurons identified in\nexperiment 3 sensitive to hierarchical\nstructure or in-distribution lexical\ntokens?\nExperiment 4 considers selective ablations of the\ntop 1% of hierarchy and linearity sensitive compo-\nnents, and evaluates how these ablations impact the\naccuracy of the model on the in-context learning\ntask, when processing Jabberwocky grammars. If\nthe neurons discovered in Experiment 3 are not sen-\nsitive to hierarchical structure and instead sensitive\nto in-distribution tokens, these ablations should not\ncause a decrease in model performance on Jabber-\nwocky grammars which are composed of meaning-\nless words. Alternatively, any decreases in model\nperformance on Jabberwocky grammars should be\ncaused by neurons in the L set which are, say, sen-\nsitive to out of distribution inputs. Ablation re-\nsults by model are in Figure 13. We also present\ngrammar-wise overlaps of the top 1% of attention\nand MLP neurons for hierarchical and linear En-\nglish and Jabberwocky grammars in Figures 15\nand 14 respectively, and show that the difference\nin overlaps between these grammars is statistically\nsignificant in Table 11. Then, we test the relative\nchange in accuracy in Jabberwocky grammars after\nablating components sensitive to hierarchical and\nlinear English structures as well as uniformly sam-\npled components. Ablating hierarchy vs. linearity\nsensitive components that are sensitive to the En-\nglish task, causes a significantly different decrease\nin model performance on Jabberwocky hierarchi-\ncal grammars. However, the same is not true for\nlinear Jabberwocky grammars where ablating hi-\nerarchy sensitive components is no different from\nablating linearity-sensitive or uniformly sampled\ncomponents (see Table 12. This suggests that the\ncomponents identified in Experiment 3 are at least\npartially sensitive to the structure of the inputs.\n17\nFigure 5: Experiment 1. Model-wise accuracy on the grammaticality judgments task given hierarchical and linear\ninputs from English, Italian and Japanese(See § 3.1 and Tables 5 and 2)\nFigure 6: Experiment 1. Model-wise accuracy on the grammaticality judgments task given hierarchical and linear\nJabberwocky inputs (See § 3.1 and Tables 6 and 3)\n(a) English, Italian, and Japanese grammars\n(b) Jabberwocky grammars.\nFigure 7: Experiment 2. Mean pairwise neuron overlaps, by model, for the top 1% of MLP and attention neurons\nby ˆ\nIE between hierarchical and linear inputs. (See § 3.2)\n18\nFigure 8: Experiment 2. MLP neuron overlaps by model for English, Italian and Japanese grammars.\n19\nFigure 9: Experiment 2. Attention neuron overlaps by model for English, Italian and Japanese grammars.\n20\nFigure 10: Experiment 2. MLP neuron overlaps by model for Jabberwocky grammars.\n21\nFigure 11: Experiment 2. Attention Overlaps by model for Jabberwocky grammars.\n22\nLanguage\nTest-Statistic\nP-value\nEN\n5.92\np < 0.001\nIT\n271.5\np < 0.001\nJP\n203\n0.2\nTable 4: Experiment 1. Results from a Mann-Whitney U-Test testing if the model accuracy on the grammaticality\njudgment task of hierarchical inputs is significantly different from that on linear inputs, when including English,\nItalian, and Japanese hierarchical and linear inputs (p < 0.05).\nGrammar\nLlama-2-7B\nLlama-3.1-8B\nLlama-3.1-70B\nQwen-2-0.5B\nQwen-2-1.5B\nMistral-v0.3\nEN Declarative (H)\n0.80\n0.94\n0.96\n0.68\n0.85\n0.91\nEN Subordinate (H)\n0.68\n0.92\n0.98\n0.67\n0.76\n0.83\nEN Passive (H)\n0.83\n0.93\n0.96\n0.70\n0.78\n0.87\nEN Negation (L)\n0.76\n0.83\n0.81\n0.65\n0.39\n0.60\nEN Inversion (L)\n0.65\n0.55\n0.61\n0.69\n0.65\n0.62\nEN Wh-word (L)\n0.61\n0.61\n0.62\n0.54\n0.55\n0.63\nIT Declarative (H)\n0.74\n0.81\n0.89\n0.63\n0.90\n0.76\nIT Subordinate (H)\n0.64\n0.71\n0.78\n0.52\n0.87\n0.69\nIT Passive (H)\n0.73\n0.84\n0.82\n0.66\n0.87\n0.73\nIT Negation (L)\n0.73\n0.87\n0.80\n0.60\n0.60\n0.85\nIT Inversion (L)\n0.61\n0.53\n0.52\n0.59\n0.46\n0.50\nIT Gender Agreement (L)\n0.48\n0.48\n0.50\n0.51\n0.38\n0.44\nJP Declarative (H)\n0.72\n0.95\n0.99\n0.54\n0.67\n0.78\nJP Subordinate (H)\n0.68\n0.72\n0.80\n0.57\n0.65\n0.58\nJP Passive (H)\n0.83\n0.99\n0.98\n0.59\n0.71\n0.90\nJP Negation (L)\n0.63\n0.94\n0.99\n0.61\n0.64\n0.75\nJP Inversion (L)\n0.62\n0.65\n0.63\n0.55\n0.61\n0.63\nJP Past-tense (L)\n0.73\n0.84\n0.99\n0.50\n0.64\n0.58\nTable 5: Experiment 1. Model accuracies on the grammaticality judgment task for English, Italian, and Japanese\nhierarchical and linear inputs.\nGrammar\nLlama-2-7B\nLlama-3.1-8B\nLlama-3.1-70B\nQwen-2-0.5B\nQwen-2-1.5B\nMistral-v0.3\nDeclarative (H)\n0.68\n0.64\n0.70\n0.57\n0.69\n0.61\nSubordinate (H)\n0.64\n0.62\n0.61\n0.53\n0.63\n0.58\nPassive (H)\n0.71\n0.77\n0.89\n0.60\n0.71\n0.62\nNegation (L)\n0.70\n0.73\n0.70\n0.54\n0.45\n0.54\nInversion (L)\n0.66\n0.57\n0.59\n0.59\n0.60\n0.52\nWh-word (L)\n0.55\n0.50\n0.55\n0.46\n0.45\n0.48\nTable 6: Experiment 1. Model accuracies on the grammaticality judgment task for Jabberwocky grammars.\nComponents\n(Test-Statistic, P-value)\nH-H vs L-L\n(8424, p < 0.001)\nH-H vs H-L\n(11594, p < 0.001)\nL-L vs H-L\n(7792, p < 0.001)\nTable 7: Experiment 2. Results from a Mann-Whitney U-Test investigating whether the overlap percentages for\ndifferent components across 7 models is significantly different for Jabberwocky grammars. We compare distributions\nof the mean overlap percentages for the top 1% of MLP and attention components (p < 0.05, N = 108).\n23\nLanguage\nComponents\nTest Statistic\nP-value\nEnglish\nH-H vs L-L\n74794.0\np < 0.001\nH-H vs H-L\n101160.0\np < 0.001\nL-L vs H-L\n65649.0\np < 0.001\nItalian\nH-H vs L-L\n74794.0\np < 0.001\nH-H vs H-L\n101160.0\np < 0.001\nL-L vs H-L\n65649.0\np < 0.001\nJapanese\nH-H vs L-L\n74794.0\np < 0.001\nH-H vs H-L\n101160.0\np < 0.001\nL-L vs H-L\n65649.0\np < 0.001\nTable 8: Experiment 2. Results from a Mann-Whitney U-Test investigating whether the overlap percentages for\ndifferent components across 7 models are significantly different for English, Italian, and Japanese grammars. We\ncompare distributions of the mean overlap percentages for the top 1% of MLP and attention components (p < 0.05,\nN = 108).\nAblation on\nAblation comparisons (Top 1% of components)\nTest-statistic\nP-Value\nEN (H)\nH-components vs L-components\n31.5\n< 0.001\nEN (H)\nH-components vs R-components\n0.0\n< 0.001\nEN (H)\nL-components vs R-components\n76.5\n0.01\nEN (L)\nH-components vs L-components\n173.5\n0.73\nEN (L)\nH-components vs R-components\n52.5\n< 0.001\nEN (L)\nL-components vs R-components\n40.0\n< 0.001\nIT (H)\nH-components vs L-components\n65.0\n< 0.001\nIT (H)\nH-components vs R-components\n15.5\n< 0.001\nIT (H)\nL-components vs R-components\n76.5\n0.01\nIT (L)\nH-components vs L-components\n193.0\n0.33\nIT (L)\nH-components vs R-components\n155.5\n0.85\nIT (L)\nL-components vs R-components\n91.5\n0.03\nJP (H)\nH-components vs L-components\n77.0\n0.01\nJP (H)\nH-components vs R-components\n17.0\n< 0.001\nJP (H)\nL-components vs R-components\n41.0\n< 0.001\nJP (L)\nH-components vs L-components\n102.0\n0.06\nJP (L)\nH-components vs R-components\n25.0\n< 0.001\nJP (L)\nL-components vs R-components\n43.5\n< 0.001\nTable 9: Experiment 3. Results from a Mann-Whitney U-test investigating whether ablating uniformly sampled\nmodel components, as well as components used in the grammaticality judgment tasks of hierarchical and linear\ngrammars containing natural language lexicons significantly differ in how they suppress performance on English,\nItalian, and Japanese hierarchical and linear grammars. Mean ablations are applied from the structure being judged\nin the task.\n24\nLanguage\nAblation\nTest-statistic\nP-Value\nZZ (H)\nZZ(H) vs ZZ(L)\n33.0\n< 0.001\nZZ (H)\nZZ(H) vs ZZ(R)\n14.5\n< 0.001\nZZ (H)\nZZ(L) vs ZZ(R)\n131.5\n0.34\nZZ (L)\nZZ(H) vs ZZ(L)\n197.0\n0.27\nZZ (L)\nZZ(H) vs ZZ(R)\n84.5\n0.01\nZZ (L)\nZZ(L) vs ZZ(R)\n44.5\n< 0.001\nTable 10: Experiment 3. Results from a Mann-Whitney U-test investigating whether ablating uniformly sampled\nmodel components, as well as components used in the grammaticality judgment tasks of jabberwocky hierarchical\nand linear grammars significantly differ in how they suppress performance on jabberwocky hierarchical and linear\ngrammars. Mean ablations are applied from the structure being judged in the task.\n(a) English, Italian, and Japanese grammars\n(b) Jabberwocky grammars.\nFigure 12: Experiment 3. Mean relative change in accuracy by model, when ablating the top 1% of attention and\nMLP neurons by ˆ\nIE between hierarchical and linear inputs. (See § 3.3)\nAblation\nStructure Type\nTest-statistic\nP-Value\nEN (H) vs EN (L)\nZZ (H)\n69.5\n0.004\nEN (H) vs EN (Random)\nZZ (H)\n11.0\n< 0.001\nEN (L) vs EN (Random)\nZZ (H)\n68.5\n0.003\nEN (H) vs EN (L)\nZZ (L)\n188.5\n0.41\nEN (H) vs EN (Random)\nZZ (L)\n104.0\n0.07\nEN (L) vs EN (Random)\nZZ (L)\n87.5\n0.02\nTable 11: Experiment 4. Results from a Mann-Whitney U-test investigating whether ablating uniformly sampled\nmodel components, as well as components used in the grammaticality judgment tasks of hierarchical and linear\nEnglish grammars significantly differ in how they suppress performance on jabberwocky hierarchical and linear\ngrammars.\nComponents\n(Test-Statistic, P-value)\nH(ZZ x EN) vs L(ZZ x EN)\n(9678, p < 0.001)\nH(ZZ x EN) vs H(ZZ) x L(EN)\n(11344, p < 0.001)\nL(ZZ x EN) vs H(ZZ) x L(EN)\n(7096, p < 0.01)\nTable 12: Experiment 4. Results from a Mann-Whitney U-Test investigating whether the overlap percentages\nfor different components across 7 models is significantly different for Jabberwocky and English hierarchical and\nlinear grammars. We compare distributions of the mean overlap percentages for the top 1% of MLP and attention\ncomponents (p < 0.05, N = 108)\n25\nFigure 13: Experiment 4. Mean relative change in accuracy by model, when ablating the top 1% of attention and\nMLP neurons pertaining to English hierarchical and linear grammars. Model is tested on Jabberwocky grammars\npost ablation, and performance decrease is measured on hierarchical and linear inputs. (See § 3.4)\nFigure 14: Experiment 4. MLP Overlaps by model between English and Jabberwocky grammars\n26\nFigure 15: Experiment 4. Attention Overlaps by model between English and Jabberwocky grammars\n27\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2025-01-15",
  "updated": "2025-01-15"
}