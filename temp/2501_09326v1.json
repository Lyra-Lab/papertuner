{
  "id": "http://arxiv.org/abs/2501.09326v1",
  "title": "Algorithm for Semantic Network Generation from Texts of Low Resource Languages Such as Kiswahili",
  "authors": [
    "Barack Wamkaya Wanjawa",
    "Lawrence Muchemi",
    "Evans Miriti"
  ],
  "abstract": "Processing low-resource languages, such as Kiswahili, using machine learning\nis difficult due to lack of adequate training data. However, such low-resource\nlanguages are still important for human communication and are already in daily\nuse and users need practical machine processing tasks such as summarization,\ndisambiguation and even question answering (QA). One method of processing such\nlanguages, while bypassing the need for training data, is the use semantic\nnetworks. Some low resource languages, such as Kiswahili, are of the\nsubject-verb-object (SVO) structure, and similarly semantic networks are a\ntriple of subject-predicate-object, hence SVO parts of speech tags can map into\na semantic network triple. An algorithm to process raw natural language text\nand map it into a semantic network is therefore necessary and desirable in\nstructuring low resource languages texts. This algorithm tested on the\nKiswahili QA task with upto 78.6% exact match.",
  "text": "1 \n \nAlgorithm for Semantic Network Generation from Texts of Low Resource Languages Such as \nKiswahili \n \nBarack Wamkaya Wanjawa* \nUniversity of Nairobi \nDepartment of Computer Science \nP.O. Box 30197 \nNairobi 00100, Kenya \nwanjawawb@gmail.com \n*Corresponding author \n \nLawrence Muchemi \nUniversity of Nairobi \nDepartment of Computer Science \nP.O. Box 30197 \nNairobi 00100, Kenya \nlmuchemi@uonbi.ac.ke \n \nEvans Miriti \nUniversity of Nairobi \nDepartment of Computer Science \nP.O. Box 30197 \nNairobi 00100, Kenya \neamiriti@uonbi.ac.ke \n \n \nAbstract \nProcessing low-resource languages, such as Kiswahili, using machine learning is difficult due to \nlack of adequate training data.  However, such low-resource languages are still important for \nhuman communication and are already in daily use and users need practical machine processing \ntasks such as summarization, disambiguation and even question answering (QA).  One method \nof processing such languages, while bypassing the need for training data, is the use semantic \nnetworks.  Some low resource languages, such as Kiswahili, are of the subject-verb-object (SVO) \nstructure, and similarly semantic networks are a triple of subject-predicate-object, hence SVO \nparts of speech tags can map into a semantic network triple.  An algorithm to process raw \nnatural language text and map it into a semantic network is therefore necessary and desirable \nin structuring low resource languages texts.  This algorithm tested on the Kiswahili QA task with \nupto 78.6% exact match. \n \nHighlights \n Languages, both low and high-resource are important for communication. \n Low resource languages lack vast data repositories necessary for machine learning. \n Use of language part of speech tags can create meaning from the language. \n An algorithm can create semantic networks out of the language parts of speech. \n The semantic network of the language can do practical tasks such as QA. \n \nKeywords \n2 \n \nAlgorithm, Low resource language, Question answering, Semantic networks, Kiswahili \n \n \n1.0 \nIntroduction \nLow-resource resource languages, which includes many African languages, have not been widely \nused on the internet for practical user needs such as information retrieval, summarization, \nmachine translation, disambiguation, question answering, or internet search.  This is attributed \nto the low research focus on these languages due to lack of readily available tools to facilitate \ntheir processing.  Any natural language, spoken by people, would usually need to be processed \nin some way for it to be useful for practical user applications on the computer systems or the \nweb (King, 2015). \n \nStructuring of natural language (NL) is a prerequisite step in the processing of the NL text for \nmachines processing tasks.  Structuring of high-resource languages has been possible and has \nlargely been done using the many available processing tools and training datasets.  However, \nlow resource languages have not spurred much research interest, hence the available tools and \ndatasets are comparatively few.  This could be due to lack of initial tools to even process the \nlanguage data where it is available, or lack of training data when the need for training of models \nis necessary (Hirschberg & Manning, 2015). \n \nThe processing of languages for use on the web or on other computer applications involves steps \nsuch as tokenizing, canonizing, normalizing to Unicode, stop work removal, synonym processing, \nstemming and even named entity recognition.  Thereafter, a knowledge representation is \ncreated using either a probabilistic or embedding approach to represent the language.  This is \nthe representation that the computer processes to realize the practical user tasks such as \nsearching, querying or information retrieval (Pennington et al., 2014).  These representations \nrequire training data before they are useful in practical systems (Y. Li et al., 2016; Yan & Jin, \n2017).   \n \nKnowledge representation (KR) refers to the use of symbols to represent propositions \n(Brachman & Levesque, 2004).  NL, which is what humans use for communication, is not directly \nrepresentable in KR used by machines.  This is because NL suffers from ambiguity, inconsistency, \nand expressiveness, while machine learning agents need explicitly defined assertions to always \nrepresent correct meaning.  NL is therefore difficult to directly process in computing systems \nwithout some KR modelling methods.  Some of these modelling methods are frames, description \nlogic, fuzzy logic or graphically (semantic networks, conceptual maps, and conceptual graphs).  \n \nHowever, not all KR requires data for training.  One such KR being the graphical methods e.g. \nsemantic networks (SN), concept maps and conceptual graphs.  The concept of \ninterconnectedness of data, as represented in semantic networks, is already exploited in linked \ndata systems.  Linking data enables data from diverse sources to be accessed through one entry \npoint that in turns links to other data sources.  Linked data, which is just a series of interrelated \ntriples, each linking another, is the request of the semantic web, which enables the querying of \ninformation from the data network (Berners-Lee, 2006).  \n \nIt is therefore possible to exploit the possibility and advantages of linking data to process \nknowledge and to represent it for other downstream applications.  Therefore, NL text structured \n3 \n \nas a SN is already good enough to be used in applications that require structured text, such as \nQuestion Answering tasks.  This is therefore done while bypassing the major bottleneck of \nprocessing low-resource languages, which is the lack of training data.  The NL text is therefore \nready, as is, for structuring into a machine understandable format, hence a method of resolving \nthe problem of low interest in low-resource languages (Besacier et al., 2014).  A step-by-step \nmethod of formatting this text, just based on the language structure itself is therefore desirable \nand can be represented as an algorithm. \n \nThe study of the structure of NL as used by humans, confirms that they have a particular format \nin the construction of sentences and derivation of meaning.  The popular structures are of three \nformats, namely, subject-verb-object (SVO), subject-object-verb (SOV) and verb-subject-object \n(VSO) (Gell-Mann & Ruhlen, 2011; Marno et al., 2015).  One such low-resource languages is \nKiswahili, and it is an SVO language (Sánchez-Martínez et al., 2020).  Kiswahili, also known as \nSwahili, is used by over 140million users worldwide and is predominantly spoken in the East \nAfrican countries of Kenya and Tanzania as the national language.  The language is also used in \nmany different countries in the world such as Australia, Canada, Saudi Arabia, UK and USA \n(omniglot, 2021).  It is therefore a language of international importance worthy of resourcing \n(Hirschberg & Manning, 2015). \n \nInterestingly, the knowledge representation of a semantic network (SN) is a triple of subject-\npredicate-object (SPO), while some languages, such as Swahili, are structured as SVO.  Careful \nstudy of the language structure (SVO) and SN structure (SPO) shows that the language can be \nmapped into a semantic network to give it meaning, through an SVO to SPO mapping.  This is \ndone at the language structure level (part of speech), without the need for training data, which \nis usually not available for low-resource languages.  This means that a rule-based system is a \ncandidate solution for structuring the language.  Such rules can easily be structured into an \nalgorithm to guide any computer processing system on how to do the SVO to SPO mapping. \n \nThere are many NL tasks that users derive from NL texts.  One of the tasks, Question answering \n(QA), remains a difficult NL problem with ongoing active research (De Cao et al., 2019; Welbl et \nal., 2018; Yao et al., 2019).  There have been different approaches employed in question \nanswering tasks.  The deep learning method of Graph Recurrent Network (GCN) using GloVe \nword embeddings method trained on Wikihop data can process natural English language text \nand then do QA task (Song et al., 2018).  Other deep learning systems such as Embeddings from \nLanguage Model (ElMo) (Peters et al., 2018) and deep learning models such as BERT already \nprocess high-resource texts to a high degree of accuracy, provided they are trained with large \namounts of data (The Stanford Question Answering Dataset, 2021).  QA is therefore a candidate \ntest case for confirming effectiveness of the SPO-to-SVO mapped knowledgebase. \n \nThe objective of this research is therefore to develop a rule-based algorithm that maps the SVO \nstructure of a low-resource language, into the SPO structure of a SN to then give the language a \nstructure.  This structure is then exploited directly by a computer to understand, process, and \ndo machine processing tasks on the language.  As proof of concept, the developed SN from the \nlow-resource language is tested on the NL task of question answering to gauge applicability.  \nThis research therefore benefits the many low-resource languages that are not being exploited \nnow, mainly due to lack of training data.   \n \n4 \n \nThe rest of the article is structured as follows: section 2 highlights the methodology of \ndeveloping the algorithm proposed in this research, while section 3 shows the results of using \nthe algorithm in typical applications.  Sections 4 and 5 giving the discussions and conclusion of \nthe research findings. \n \n \n2.0 \nMethodology  \nThis research aims at developing an algorithm to guide in the processing of raw text of a low-\nresource language, with the focus on the Kiswahili language.  The output of the algorithm is a \nsemantic network triple formulated from NL text, just by a review of the parts of speech tags of \nthe text itself. \n \nThe algorithm developed in this research is based on a model of Swahili language processing \nfrom previous works (B. Wanjawa & Muchemi, 2021).  The model provided the pipeline stages \nand highlighted the SVO identification stage as a critical processing step, as now expounded in \nthis research.  Fig. 1 below shows the flowchart that describes the key stages in typical SVO \nidentification from NL text. \n \nThe flowchart shows that the initial processing stage after reading the input sentence is part of \nspeech (POS) tagging of the text.  This tagging is important since the modeling of the language \nrelies on the POS tags and not the words themselves.  It is the POS e.g. noun or verb that leads \nto the realization of a triple of subject-verb-object (SVO), which is then mapped into the \nsemantic structure of SPO.  \n \n5 \n \n \n \nFig. 1. Flowchart for semantic network generation from natural language text (Source: author) \n \n \nThe algorithm formulated in this research is shown in Fig. 2 below.  It gives the step-by-step \nprocess of generating candidate SVO triples, and how to finally decide the most suitable SVOs \nfor extraction as the final SVOs for the semantic network’s SPOs. \n \n 1. Start \n 2.  Read the NL text \n 3.  Count no. of sentences \n 4.  Set max_sentences \n 5.  Set counter = 0 \n 6.  For sents from 1 to max_sent \n 7.  \nRead the sentence \n 8.  \nPOST the sentence \n 9.  \nCount the COMMAs \n10.  \nIf COMMA found DO Procedure COMMA_found \n11.  \nIf NO COMMA found DO Procedure no_COMMA \n6 \n \n \n12.  \n//Procedure no_COMMA \n13.  \nIf no COMMA in phrase:  \n14.  \n \nCount VERBs \n15.  \n \nSet max_verbs \n16.  \n \nIf max_verbs > 0 DO Procedure VERBS_found \n17.  \n \n//Procedure VERBs found \n18.  \n \nverb_count = 0 \n19.  \n \nRepeat \n20.  \n \nSet verb_count = verb_count + 1 \n21.  \n \nRead POST of LHS of V \n22.  \n \nIf N exists on LHS of V \n23.  \n \nDetermine all Ns on LHS of V and create array of the Ns \n24.  \n \nCheck RHS of V \n25.  \n \nIf N exist on the RHS of V \n26.  \n \nDetermine all Ns on RHS of V and create array of the Ns \n27.  \n \n// stitch each LHS token to every RHS token with V as connector \n28.  \n \nSet LHS_N_max \n29.  \n \nLHS_N_count = 0 \n30.  \n \nRepeat \n31.  \n \n \nSet LHS_N_count = LHS_N_count + 1 \n32.  \n \n \nNew_LHS_phrase = LHS_N(count) + append the V \n33.  \n \n \nSet RHS_N_max \n34.  \n \n \nRHS_N_count = 0 \n35.  \n \n \nRepeat \n36.  \n \n \nSet RHS_N_count = RHS_N_count + 1 \n37.  \n \n \nNew_triple = New_LHS_phrase + append the RHS_N(count) \n38.  \n \n \nUntil RHS_N_count > RHS_N_max \n39.  \n \nUntil LHS_N_count > LHS_N_max \n40.  \n \nelse exit // the triple not possible, no N on RHS, despite having S-V \n41.  \n \nelse exit // the triple not possible, no N on LHS of V \n42.  \n \nelse DO procedure OTHER_Rules // triple not possible, no V, try other rules \n43.  \n \n//Procedure OTHER_RULES – set any other rules e.g. ‘is-a’ rules \n44.  \n \nIf phrase has N-N – New_triple = N- <is_a> -N \n45.  \n \nIf phrase has N-NUM – New_triple = N- <is_a> -NUM \n46.  \n \netc. \n47.  \n \nelse exit // triples not possible with current POS structure \n \n48.  \n//Procedure COMMA_found \n49.  \nIf COMMA in phrase: \n50.  \n \nNEW_PHRASE = from start of sentence to COMMA position, excluding COMMA \n51.  \n \nDO Procedure No_COMMA \n52.  \n \nNEW_PHRASE = from start of sentence to V, ADD rest of sentence after COMMA \n53.  \n \nDO Procedure COMMA_found \n \n54.  \nEnd For  \n55. End \n \nFig. 2. Algorithm for Semantic Network Generation from natural language text (Source: author) \n \n \nThis research determined, through analysis of semantic network (SN) structures and language \nstructure, that there was possibility of direct mapping of SPO to SVO, with unit of consideration \nbeing a sentence of phrase.  Preliminary preprocessing of any text therefore determines the \nsentence or phrase lengths using full stops and commas, then processing that sentence or \nphrase (algorithm line 1 to 13).  Further study on the sentence structure of SVO languages, such \nas Kiswahili, led to the realization that the key anchor to the SVO format of the language is the \nverb ‘V’.  The ‘V’ could be just one, but then it can have several subjects (S) and objects (O) lying \non both of its sides. \n \n7 \n \nThe algorithm therefore determines the position of the verb (V) (line 14), then does several \niterations to mine the POS on the left of the ‘V’, to get all the subjects.  It then keeps them in a \nleft hand side (LHS) array (line 22-23).  The algorithm then mines the right side of the ‘V’ to \ndetermine all possible objects and stores them on the right-hand side (RHS) array (line 25-26).  \nA final array of dimension LHS (L) x RHS (R) is now created.  Any combination of L(1..n) + V + \nR(1..n) are candidate S-V-O triples, where ‘n’ is the total number of identified POS tagged \nsubjects or objects (usually NOUNS).  This generation of triples is what is achieved in line 38 of \nthe algorithm.   \n \nFor example, a sentence deconstructed into 2L subjects and 3R objects creates a 2x3 array with \n6 possible SPO combinations.  All these 6 combinations are based on only one single connector \npredicate being the ‘V’ (VERB).  These 6 possibilities as SPO candidate triples are: \n1. L1 + V + R1 \n2. L1 + V + R2 \n3. L1 + V + R3 \n4. L2 + V + R1 \n5. L2 + V + R2 \n6. L2 + V + R3 \n \nWhere:  \nSubject (S): L1, L2 are any subjects (usually a NOUN) – on the left hand side of the ‘V’ \nObject (O): R1, R2, R3 are also any objects (still usually NOUN) – on the right hand side of the ‘V’ \nPredicate (P): The static ‘V’ (VERB) is the predicate – in middle of the subject and object   \n \n \nThere are therefore 6 potential SPOs triples of SVO format (NOUN subject + V + NOUN object), \nwhich is the expected structure of an SVO format language.  That means that the algorithm is \nreformatting the SVO language into its basic SVO basic form. \n \nHowever, not all the language constructs of the low-resource language shall necessarily be \nexactly in the SVO order.  For example, some triples describe attributes such as ‘is-a’ relationship \nor even representation of attributes such as numbers or dates.  The algorithm therefore creates \na processing routine to look for such language constructs and then creates triples of ‘is-a’ type.  \nThis is done when a ‘V’ is not found in the sentence, while ‘N’ exists.  Such additional rules can \nbe quite many and are developed upon study of the POS tags of the NL text.  These are \nimplemented on lines 43 to 46 of the algorithm. \n \nThis algorithm therefore describes a rule-based system that leverages the POS tags of the \nlanguage, then uses these basic rules to formulate candidate SVO triples and then augments the \nbasic rules by any other inclusion or exclusion criteria to further increase or filter out any triples \nthat should be added or excluded from the final SVOs for creating the SN. \n \nAll the final accepted SVOs are extracted and moved to a datastore in Resource Description \nFramework (RDF) format ready for further processing, including querying of the datastore using \nlanguages such as SPARQL.  The triples stored as RDF can also be visualized on RDF graphs, which \ngives a visualization that the language has been decomposed into an interconnected structure.  \nThe proposed algorithm is then implemented using a typical programming language, such as \n8 \n \nPython, which is the chosen language to implement the algorithm in the tests done in this \nresearch as provided in the results section. \n \n \n3.0 \nResults \nTyDiQA dataset (Clark et al., 2020) is used as the data source for the following illustrations on \nhow the algorithm processes NL text.  The Swahili language portion of TyDiQA has a gold \nstandard set in JavaScript Object Notation (JSON) file format.  This file has 498 examples, with a \ncontext, a question, and an answer.  Though TyDiQA is structured for machine learning tasks, \nwe use it in the illustrations due to its ease of access and testing.  It can also be used as a \ncomparator with other machine learning methods since it is already formatted for machine \nlearning.  The full TyDiQA set is a collection of data in 11 languages i.e. Arabic, Bengali, English, \nFinnish, Indonesian, Kiswahili, Russian. Japanese, Korean, Thai and Telugu.  And even in this set \nof eleven languages, Kiswahili is among the bottom three in terms of number of examples \navailable (Wu & Wu, n.d.).   \n \nWe show how the algorithm processes raw text from TyDiQA item ref. swahili--\n3141018404948436558-0 as an example.  The JSON file content is reproduced below. \n \n{\"title\": \"Chelsea F.C.\", \"paragraphs\": [{\"qas\": [{\"question\": \"Klabu ya Soka ya Chelsea \nilianzishwa mwaka upi?\", \"answers\": [{\"text\": \"1905\", \"answer_start\": 135}], \"id\": \"swahili--\n3141018404948436558-0\"}], \"context\": \"Chelsea Football Club ni klabu ya mpira wa miguu ya \nnchini Uingereza iliyo na maskani yake Fulham, London. Klabu hii ilianzishwa mwaka 1905, na \nkwa miaka mingi sana imekuwa ikishiriki ligi kuu ya Uingereza. Uwanja wao wa nyumbani ni \nStamford Bridge ambao una uwezo wa kuingiza watazamaji 41,837, wameutumia uwanja huu \ntangu klabu ilivyoanzishwa.\"}]} \n \nLike all items on TyDiQA, this is a Wikipedia article in the Swahili language.  Wikipedia has an \nEnglish language equivalent of this article that would provide the English meaning of the \nKiswahili text (Contributors to Wikimedia projects, 2021).  Though not the direct translation, the \nEnglish version of Wikipedia is shown below: \n \nChelsea Football Club is an English professional football club based in Fulham, West London. \nFounded in 1905, the club competes in the Premier League, the top division of English football. \nChelsea is among England's most successful clubs, having won over thirty competitive honours, \nincluding six league titles and eight European trophies. Their home ground is Stamford Bridge. \n(*Note that the number of spectators, 41,837 is not mentioned on the introductory paragraph \nof the English version of Wikipedia)  \n \nThe algorithm creates a semantic network from the above NL text as per the flowchart in Fig. 1 \nand the description provided in section 2 above.  This is done through the following stages: \n \n3.1 \nStage 1 \nCount the number of sentences to determine the number of iterations in the processing, in this \ncase 3 sentences, which means 3 iterations through the SVO processing algorithm. \n \n9 \n \n3.2 \n Stage 2 \nUndertake iterations as per sentence count from stage 1 as per the following process: \n3.2.1 Iteration 1 – sentence 1: \nCheck for any COMMAs in the sentence and count them, in this case one comma.  The rules of \nthe algorithm assume that the comma is the enumeration type.  Using the comma procedure of \nthe algorithm, the sentence is decomposed into 2 parts as below: \n \nPhrase 1 (everything upto the comma) - Chelsea Football Club ni klabu ya mpira wa miguu ya \nnchini Uingereza iliyo na maskani yake Fulham \n \nPhrase 2 (everything upto the V, then everything after the comma) - Chelsea Football Club ni \nklabu ya mpira wa miguu ya nchini Uingereza iliyo na maskani yake London. \n \nThese two atomic phrases are processed in the full algorithm, which starts by undertaking the \npart of speech tagging (POST) of the first sentence ready for processing.  An online tool is \navailable for simple POST tasks for Kiswahili (aflat, 2020).  It accepts any Kiswahili text and \ngenerates the POS tag. \n \n3.2.1.1 Phrase 1 processing (using the no comma) procedure: \nCheck for any VERBs (algorithm line 17), and since found (DEF-V:ni) - verb with no inflection, \nthen check both the left and the right of the VERB and create an array of all nouns on the LHS \nand those on the RHS.  Thereafter, create all possible combinations of triples of S-V-O based on \nthe anchor verb.  This process, as per algorithm line 37, generates triples ref. T1, T2 and T3 as \nshown on Table 1 below.  Since there are no other connector verbs in phrase 1, the rest of the \ntriples from phrase 1 (ref. T4 to T9) are generated by considering other rules on line 43 of the \nalgorithm such as ‘is-a’ rules. \n \nThe first column of the table (Ref.) points to the line number of the algorithm of Fig. 2 that is \nbeing processed, while the triple in column 2 refers to the formulated candidate triples possible \nfrom the two or three POS tags considered.  Note that the POS is not lemmatized, though it is \npossible to generate the lemmas using other tools such as Treetagger (treetagger, 2020). \n \nTable 1 – Parts of Speech extracted by Algorithm as suitable SVO triples \nRef. \nTriple\nPOS considered \nGenerated Triples (Turtle \nformat) \n17\nPOS\nPROPNAME\nDEF-V:ni\nN\n@prefix : <http://testing.123>\nT1\nChelsea\nni\nklabu\n:chelsea :ni :klabu .\n \nT2 \nFootball \nni \nklabu \n:football :ni :klabu . \nT2\nClub\nni\nklabu\n:club :ni :klabu .\n43 \nPOS \nN \nGEN-CON \nN \n \nT4\nklabu\nya\nmpira\n:klabu :ya :mpira .\nT5\nmpira\nwa\nmiguu\n:mpira :wa :miguu .\nT6\nmiguu\nya\nnchini\n:miguu :ya :nchini .\n43\nPOS\nN\nPROPNAME\n \nT7 \nnchini \nUingereza \n  \n:nchi :ni :uingereza . \n43\nPOS\nPROPNAME\nCC\nN\n \nT8 \nUingereza \nna \nmaskani \n:uingereza :ni :maskani . \n10 \n \n43\nPOS\nN\nPROPNAME\n \nT9 \nmaskani \nFulham \n  \n:maskani :ni :fulham . \n43\nPOS\nN\nPROPNAME\n \nT10 \nmaskani \nLondon \n  \n:maskani :ni :london . \n \n \n3.2.1.2 Phrase 2 processing (using the no comma) procedure: \nThis is treated just like phrase 1, hence we repeat all the considerations done in processing \nphrase 1.  Since phrase 2 is exactly same as phrase 1 apart from the last word, it generates all \nthose triples already generated in phrase 1, apart from the last one which is different (T10 is \ngenerated instead of T9).  The list of all POS considered for SVO suitability and other POS \nconsidered by other rules in generating semantic network triples is shown on columns 3 to 5 of \nTable 1 above.   \n \n3.2.2 Iteration 2 – sentence 2: \nFollows the same process as above for sentence 1, by checking commas (one in this case) and \nthe comma intention (conjunction to phrase 1).  It then processes each phrase through the full \nalgorithm.  \n \n3.2.3 Iteration 3 – sentence 3: \nFollow the same process as above for sentence 1, by checking commas (one in this case) and the \nintention of the comma (conjunction to phrase 1).  It then processes each phrase through the \nfull algorithm.  Note that the number 41,837 also has a comma, but the algorithm rules of \nnumber processing shall check for such to remove the comma for the number and only leave \nthe one comma that combines the 2 phrases. \n \n \n3.3 \nGenerating the Semantic Network \nAll triples collected from the 3 sentences (3 iterations) are the triples that are then stored in a \ntriple store, such as in RDF format.  The triples are now ready for practical NLP tasks such as \nquestion answering.  The diagrammatic visualization of the semantic network is shown in Fig. 3 \nbelow.  The figure is generated from an online virtualization tool that accepts Turtle formatted \nRDF entries and then generate the graphical visualization (RDF Grapher, 2021).  However, \nprogramming languages such as Python can also generate such graphs using existing libraries. \n \n11 \n \n \n \nFig. 3. Visualization of RDF triples created using the algorithm (Source: author) \n \n \n3.4 \nProof of Concept Question \nWe test the SN on a typical NLP QA task.  The network in Fig. 3 is generated from TyDiQA text ID \nswahili--3141018404948436558-0, which was annotated with a question as below: \nQ-Klabu ya Soka ya Chelsea ilianzishwa mwaka upi? \n(Q-When was Chelsea Football Club founded?) \n \nThis answer is easily seen as the relationship between ‘Chelsea’ and ‘founded’ as per the \nvisualization on Fig. 3, though we query the datastore using SPARQL shown below (using Python \nRDFLIB module), since the triples are stored as RDF.   \n \nSELECT ?s ?p ?o \n       WHERE { \n        {:chelsea ?p ?o .} \n        UNION \n        {?s ?p :mwaka .} \n        UNION \n        {:mwaka ?p ?o .} \n       } \n \nNote that in the Kiswahili language text, the concepts (club name and year founded) are in \ndifferent sentences, however, the nodes and edges of the SN interconnect the whole story to \nshow the inter-relatedness of the domain.  Additionally, words in both the texts of the context \nand the text of the question would usually need to be lemmatized so that the lemmas of the \nquestion text is aligned to the lemmas of the story text e.g. anza/anzishwa (found/founded) \nneed to point to the same concept in the SN. \n \nThe response obtained is shown below, with predicates and objects indicated in bold: \n12 \n \nhttp://testing.123/ni http://testing.123/klabu  \nhttp://testing.123/klabu http://testing.123/anza  \nhttp://testing.123/ni http://testing.123/1905 \n \nWith the final triple giving the figure of ‘1905’, which is the correct answer for this question.  \nNote that our query did not exploit the concept of date extraction, but rather an object \nextraction, with the object being the year/date.  It could have been possible to create RDF triples \nthat indicate that ‘1905’ is a date, then create a query to look for a date object on the graph. \n \n \n3.5 \nTesting the Algorithm on different QA datasets \nThis algorithm was tested on 3 different question-answer datasets.  The first was the Tusome \ncorpus (Kenya Ministry of Education, n.d.; Piper et al., 2018), which is a collection of texts for \nearly childhood education in Kenya.  The exact match score for the sampled 33 QAs was 63.7% \ni.e. 21 correctly answered.  The second dataset was the TyDiQA set (Clark et al., 2020).  In \nTyDiQA, the low-resource language of Swahili has 17,613 training examples, 2,288 development \nset examples and 2,278 in the test set.  The gold standard set that supports SQuAD version 1.1 \nis a JavaScript Object Notation (JSON) file of 498 examples, with context, question, and answer.  \nThe research tested the data on this gold standard set.   \n \nWe used a purposive sample set of 54 questions to test the different question types.  We \ndynamically created individual SNs of each story texts, then subjected these SNs to QA task on \nthe fly.  The results were 35/54 correctly answered questions (64.8% exact match accuracy).  The \ndistribution of questions tested in this set is shown in Table 2 below. \n \nTable 2 – Analysis of Questions subjected to the SN generated from texts in the TyDiQA corpus \nQuestion \nType \nDate/Yr\nNum\nWhat\nDefine\nWhere\nWhich\nWho\nTotal\nTotal\n18\n5\n5\n3\n12\n8\n3\n54\nCorrect (EM) \n12 \n3 \n4 \n0 \n8 \n6 \n2 \n35 \n \n \nThe final QA dataset used to test the algorithm was the KenSwQuAD dataset (B. W. Wanjawa et \nal., 2023).  This is a QA dataset of 7526 QA pairs based on 2585 texts.  The KenSwQuAD is a gold \nstandard set Swahili QA dataset for testing QA systems.  We sampled 365 questions using \npurposive sampling to ensure that each question type is selected when picking the texts to \ncreate the SNs in readiness for the QA task.  Table 3 below shows the distribution of question \ntypes and the QA results.  We observe that 287/365 QAs were correctly answered, hence 78.6% \nexact match performance. \n \nTable 3 – Analysis of Questions subjected to the SN generated from texts in the KenSwQuAD \ncorpus \nQuestion \nType \nDate/Yr\nNum\nWhat\nDefine, \nHow, \nWhy \nWhere\nWhich\nWho\nTotal\nTotal \n15 \n15 \n125 \n36 \n25 \n76 \n73 \n365 \nCorrect (EM)\n8\n9\n118\n0\n21\n66\n65\n287\n13 \n \n \n \n4.0 \nDiscussions \nThe research formulated an algorithm that is useful in creating structure out of a natural \nlanguage (NL) text of a low-resource language such as Kiswahili.  Structuring language, or data, \nis usually a prerequisite step before further processing or mining of information.  For NL, \nstructuring has been done using methods such as term-frequency-inverse-document frequency \n(TF-IDF), word embeddings and deep learning that include transformer models, depending on \nthe ultimate use of the structured language.   \n \nMany of the existing machine learning models which are used to structure NL need training data, \nwhich is readily available for high resource languages e.g. English, French, Chinese, Arabic etc.  \nHowever, low-resource languages, such as Kiswahili and many other African languages how few \ndatasets for use in machine learning models.  Despite this drawback, languages, including low-\nresource, are important for human communication and are useful in disseminating information \nthat can be lifesaving, such as during disasters, natural calamities, medical emergencies e.g. \nduring the Corona Virus Disease (COVID) pandemic and even terrorism incidences. \n \nHowever, some knowledge representation systems such as semantic networks (SNs) do not \nnecessarily need training data.  These methods have traditionally been applied in other domains \nthat deal with unstructured data processing to get inter-relatedness of entities or objects e.g. \nFacebook (X. Li & Boucher, 2013), LinkedIn Graph (Markovic & Nelamangala, 2017) and Google \nKnowledge graph (Singhal, 2012). \n \nThe same reasoning of inter-relatedness of objects has been used in this research to structure \nnatural language text.  Some low-resource languages, such as Kiswahili, have a generic structure \ndescribed as subject-verb-object (SVO) – a general inter-relatedness of a triple of subject-verb-\nobject.  On the other hand, semantic networks have a triple of subject-predicate-object (SPO) \nrelationship, hence an SVO-to-SPO mapping is possible.   \n \nMapping SVO pattern of a language to SPO structure of a semantic network is however not \ntrivial, since language sentences are hardly a three-word triple of S then V then O.  The research \nalgorithm provided the method of identifying and extracting suitable SVO from NL text to map \ninto the SPO of SNs.  The method is based on rules that pick out these SVO triples.  Only the part \nof speech (POS) is required, not the words of the language itself.  The main consideration is to \nstart by identifying the key verb (V) as the predicate, then work both backwards and forward to \ndetermine the subject(S) and object(O), usually nouns.  Finally, increase coverage by considering \nother language constructs e.g. ‘is-a’ relationships.  Ultimately, there cannot be a ruleset that \ncovers all aspects of the very expressive nature of NL.  Some sentence constructs shall still be \nparsed incorrectly or not at all.  \n \nSince only the POS tags are necessary in creating the SN, any language whose POS is known can \ntherefore be structured into a SN.  This research only tested the direct link between SVO \nlanguage structure to the SPO structure of the SN.  However, by similar reasoning, an algorithm \ncan still transform even an SOV language to a SPO structure of a SN.  \n \n14 \n \nThe results got for the proof of concept task of QA from the SNs created from NL test realized \nan accuracy of 64.8% on exact match for the sampled TyDiQA set (Clark et al., 2020) and 78.6% \nexact match on the KenSwQuAD dataset (B. W. Wanjawa et al., 2023).  These results show that \nthe NL texts have been structured into SNs that now represent these domains, hence are \ncapable of practical tasks on the NL text, with no training data.  The only prerequisite is the POS \ntagging of the NL text.   \n \nHowever, we observe that this SN based method performs quite well at object-enquiry type of \nquestions (who, what, where, when/date) and not on the explanatory types of questions \n(define, why).  This is expected, since SN by its very nature is an object linking method, where \nobjects such as language POS tags are interlinked using connectors, which are also POS tags.  \nQuerying is therefore an enquiry into the objects within the SN.  Reasoning questions require \nmuch more than simple object linking within a context.  As reported on that KenSwQuAD \nresearch, even deep learning methods could not perform well on QA task based on the same \ndataset due to the limited training data, achieving only an F1 score of 59.4% and exact match of \n48% (B. W. Wanjawa et al., 2023). \n \nSome shortcomings and challenges with the use of the proposed rule-based algorithm are noted \nwhen processing typical language POS constructs.  One problem noted is the determination of \nintention of the COMMA part of speech.  In some cases, the comma signifies the enumeration \nof related items in a list, while in other cases it may signify the start of a new phrase that is \nrelated, or not, to the first phrase.  Commas can also be used after initial startup words or to \nsplit phrases, or mark pauses, and even within numeric values.  The same problem of \ninterpretation is observed with semicolons, colons, slashes, hyphens, and apostrophes.  The \nalgorithm has just modelled a few commonly used constructs and usually defaults to skipping \nany other possible extractions that are not on the ruleset.  Processing direct speech or \ndrama/plays is still a problem with rule-based or other methods of NL processing. \n \nOther language processing challenges in general include named entity recognition (NER) that \nshould be done at POS tagging.  Currently, without NER tool, the named entity is just \ndecomposed into separate nouns, and the connecting verb (V) is just likely to link each left-hand \nside noun with some right-hand side object.  This is manifested in the example where the named \nentity ‘Chelsea Football Club’ is decomposed into three separate proper nouns, instead of one \nsingle named entity.  Each of the proper nouns is then equated to be a club, instead of one single \nnamed entity being the club. \n \nCoreference resolution is another challenge that we face when processing low resource \nlanguage text.  In our three-sentence example of Fig. 3, we see a graph that is not fully \nconnected, despite this being one connected domain.  The lack of connectedness is caused by \nhow sentence 3 is processed, since it no longer mentions the subject by name, but uses ‘it’ \ninstead.  An NLP tool to resolve this article ‘it’ into the proper name of the subject is not available \nfor now.  However, these shortcomings also present an opportunity for further research into \nresolving these identified challenges.  \n \n \n5.0 \nConclusion \n15 \n \nThis research formulated an algorithm for use in transforming a natural language text of a low-\nresource language into a semantic network.  The low-resource language tested in this research \nwas an SVO-type language of Kiswahili.  Tests done on the Kiswahili language texts structured \ninto semantic networks using the algorithm have confirmed that the language is given a \nstructure.  The task of question answering has further been used as proof of concept confirming \nthat the structured language is capable of practical use including querying using existing \nquerying languages. \n \nStructuring the natural language text into triples that fit the semantic network structure is \nhowever not trivial and hence a guiding algorithm is necessary.  This is because the natural \nlanguage text of Kiswahili, though described as SVO-type, does not necessarily mean that the \nsentences are a series of S then V then O.  It is much more complex, and an algorithm with its \nruleset decomposes the complexity to guide on the exact processing steps in generating the final \nSVO triples.  The research recognized that anchoring on the V(verb) as the starting point, it is \nthen possible to process the suitable subjects and objects, and then combine the suitable SVO \ntriples for extraction into a datastore. \n \nThe algorithm has been tested in practical datasets of Tusome (Kenya Ministry of Education, \nn.d.), TyDiQA (Clark et al., 2020) and KenSwQuAD (B. W. Wanjawa et al., 2023), where we \nconfirm that it is possible to create semantic networks by just leveraging on the POS tags of a \nlanguage, achieving 78.6% exact match performance on the KenSwQuAD dataset, with no \ntraining data.  Only the parts of speech tagging were the prerequisite.  This structured language \nis then of practical computer information processing tasks such as QA.  Alternative methods \nsuch as machine learning, deep learning and even transformer models, that are highly successful \nin other high-resource languages, are unfortunately not applicable in the case of many of the \nlow-resource languages that do not have training data. \n \nChallenges still abound, such as dealing with direct speech within the texts and the inability to \ncreate rules that cover all language constructs e.g. intentions of punctuation such as commas, \nsemicolons, and hyphens.  Additional challenges include dealing with named entities which need \nto be resolved into noun objects.  Co-reference resolution also stands in the way of linking \nlanguage concepts into a jointed network during processing.  Other non-SVO relationships are \nalso difficult to catch and model e.g. ‘is-a’ relationships.  An increased rule set may assist in \nresolving some shortcomings, though natural language is so expressive hence difficult to fully \nmodel.  As was done with the high-source language of English, deliberate research and \ndevelopment is needed for low resource languages to come up with all these tools that assist in \nresolving these challenges. \n \nThere is also the challenge of being able to perform the initial part of speech (POS) tagging itself, \nwhich is a prerequisite for the algorithm.  This however is also an opportunity to focus research \ninterests on POS tagging as a priority, to in turn start exposing these languages to machine \nprocessing.  Language dictionaries, which are usually available in some form, can be a starting \npoint in generating POS tags.  Only the POS tags and the application of an algorithm such as \nwhat we propose in this research, are needed to then create a knowledgebase of the language \nready for practical use.   \n \n16 \n \nResearchers still need to work towards collection of the datasets of low-resource languages so \nthat existing methods that perform well in high-resource languages, such as deep learning \nmodels, can also be tried for low-resource languages to see whether there is improved \nperformance compared to the rule-based systems.  An immediate research opportunity is using \nthe algorithm to process SOV-type low-resource languages.  A tweak on the algorithm should be \nable to structure such SOV language into SPO structure of a semantic network.  This can be done \nafter the study of the SOV sentences and an analysis of how meaning is formed from the texts \nof these languages by SOV linkages.  This should further extend the coverage of the potential \nlanguages for structuring based on this research algorithm. \n \nFunding \nThis research did not receive any specific grant from funding agencies in the public, commercial, \nor not-for-profit sectors. \n \nConflicts of interest: none \n \n \nReferences \naflat. (2020). Kiswahili Part-of-Speech Tagger - Demo  AfLaT.org. Retrieved 14 December 2020, \nfrom https://www.aflat.org/swatag \nBerners-Lee, T. (2006). Linked Data. Retrieved 06 July 2022, from \nhttps://www.w3.org/DesignIssues/LinkedData.html \nBesacier, L., Barnard, E., Karpov, A., & Schultz, T. (2014). Automatic speech recognition for \nunder-resourced languages: A survey. Speech Communication, 56(1). Elsevier B.V. \nhttps://doi.org/10.1016/j.specom.2013.07.008 \nBrachman, R. J., & Levesque, H. J. (2004). Knowledge Representation and Reasoning. \nKnowledge Representation and Reasoning. Morgan Kaufmann. \nhttps://doi.org/10.1016/B978-1-55860-932-7.X5083-3 \nClark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., & Palomaki, J. \n(2020). TyDi QA: A benchmark for information-seeking question answering in typologically \ndiverse languages. ArXiv Preprint ArXiv:2003.05002. \nContributors to Wikimedia projects. (2021). Chelsea F.C. - Wikipedia. Retrieved 08 November \n2021, from https://en.wikipedia.org/w/index.php?title=Chelsea_F.C.&oldid=1054654568 \nDe Cao, N., Aziz, W., & Titov, I. (2019). Question Answering by Reasoning Across Documents \nwith Graph Convolutional Networks. Proceedings of the 2019 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics: Human Language \nTechnologies, 1 (Long and Short Papers), 2306–2317. \nGell-Mann, M., & Ruhlen, M. (2011). The origin and evolution of word order. Proceedings of \nthe National Academy of Sciences, 108(42), 17290–17295. \nhttps://doi.org/10.1073/PNAS.1113716108 \nHirschberg, J., & Manning, C. D. (2015). Advances in natural language processing. Science, \n349(6245), 261–266. \nKenya Ministry of Education. (n.d.). Brief on Tusome Early Literary Programme. Retrieved 12 \nDecember 2020, from https://www.education.go.ke/images/Project-\nKPED/Brief%20on%20TUSOME%20.pdf \nKing, B. P. (2015). Practical Natural Language Processing for Low-Resource Languages. \nRetrieved 05 June 2020, from https://deepblue.lib.umich.edu/handle/2027.42/113373 \n17 \n \nLi, X., & Boucher, M. (2013). Under the Hood: The natural language interface of Graph Search. \nRetrieved 16 October 2020, from http://www.facebook.com/notes/facebook-\nengineering/under-the-hood-the-natural-language-interface-of-graph-\nsearch/10151432733048920 \nLi, Y., Tan, S., Sun, H., Han, J., Roth, D., & Yan, X. (2016). Entity disambiguation with linkless \nknowledge bases. 25th International World Wide Web Conference, WWW 2016, 1261–\n1270. https://doi.org/10.1145/2872427.2883068 \nMarkovic, V., & Nelamangala, V. (2017). Building the Activity Graph, Part I. Retrieved 05 July \n2020, from https://engineering.linkedin.com/blog/2017/06/building-the-activity-graph--\npart-i \nMarno, H., Langus, A., Omidbeigi, M., Asaadi, S., Seyed-Allaei, S., & Nespor, M. (2015). A new \nperspective on word order preferences: the availability of a lexicon triggers the use of \nSVO word order. Frontiers in Psychology, 6, 1183. \nhttps://doi.org/10.3389/fpsyg.2015.01183 \nomniglot. (2021). Swahili alphabet, pronunciation and language. Retrieved 08 September \n2022, from https://omniglot.com/writing/swahili.htm \nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global vectors for word \nrepresentation. EMNLP 2014 - 2014 Conference on Empirical Methods in Natural \nLanguage Processing, Proceedings of the Conference, 1532–1543. \nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). \nDeep contextualized word representations. ArXiv Preprint ArXiv:1802.05365. \nPiper, B., Destefano, J., Kinyanjui, E. M., & Ong’ele, S. (2018). Scaling up successfully: Lessons \nfrom Kenya’s Tusome national literacy program. Journal of Educational Change, 19(3), \n293–321. \nRDF Grapher. (2021). https://www.ldf.fi/service/rdf-grapher \nSánchez-Martínez, F., Sánchez-Cartagena, V. M., Antonio Pérez-Ortiz, J., Forcada, M. L., Espì A-\nGomis, M., Secker, A., Coleman, S., & Wall, J. (2020). An English-Swahili parallel corpus \nand its use for neural machine translation in the news domain. November, 299–308. \nhttps://github.com/bitextor/bicleaner/ \nSinghal, A. (2012). Introducing the Knowledge Graph: things, not strings - Inside Search, \n2013:7/22/2013. http://insidesearch.blogspot.com/2012/05/introducing-knowledge-\ngraph-things-not.html \nSong, L., Wang, Z., Yu, M., Zhang, Y., Florian, R., & Gildea, D. (2018). Exploring graph-structured \npassage representation for multi-hop reading comprehension with graph neural \nnetworks. ArXiv Preprint ArXiv:1809.02040. \nThe Stanford Question Answering Dataset. (2021). Retrieved 16 March 2021, from \nhttps://rajpurkar.github.io/SQuAD-explorer \ntreetagger. (2020). TreeTagger. Retrieved 14 December 2020, from https://www.cis.uni-\nmuenchen.de/~schmid/tools/TreeTagger \nWanjawa, B., & Muchemi, L. (2021). Model for Semantic Network Generation from Low \nResource Languages as Applied to Question Answering–Case of Swahili. 2021 IST-Africa \nConference (IST-Africa), 1–8. \nWanjawa, B. W., Wanzare, L. D. A., Indede, F., McOnyango, O., Muchemi, L., & Ombui, E. \n(2023). KenSwQuAD—A Question Answering Dataset for Swahili Low-resource Language. \nACM Transactions on Asian and Low-Resource Language Information Processing, 22(4), 1–\n20. \n18 \n \nWelbl, J., Stenetorp, P., & Riedel, S. (2018). Constructing datasets for multi-hop reading \ncomprehension across documents. Transactions of the Association for Computational \nLinguistics, 6, 287–302. \nWu, C., & Wu, T. (n.d.). Typologically Diverse QA: How many training examples do you need for \na new language anyway? \nYan, P., & Jin, W. (2017). Building semantic kernels for cross-document knowledge discovery \nusing Wikipedia. Knowledge and Information Systems, 51(1), 287–310. \nhttps://doi.org/10.1007/s10115-016-0973-5 \nYao, L., Mao, C., & Luo, Y. (2019). Graph convolutional networks for text classification. \nProceedings of the AAAI Conference on Artificial Intelligence, 33, 7370–7377. \n  \n \n",
  "categories": [
    "cs.CL",
    "I.2.7"
  ],
  "published": "2025-01-16",
  "updated": "2025-01-16"
}