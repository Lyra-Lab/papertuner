{
  "id": "http://arxiv.org/abs/2501.15747v2",
  "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
  "authors": [
    "Sankalp KJ",
    "Ashutosh Kumar",
    "Laxmaan Balaji",
    "Nikunj Kotecha",
    "Vinija Jain",
    "Aman Chadha",
    "Sreyoshi Bhaduri"
  ],
  "abstract": "Known by more than 1.5 billion people in the Indian subcontinent, Indic\nlanguages present unique challenges and opportunities for natural language\nprocessing (NLP) research due to their rich cultural heritage, linguistic\ndiversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark\ndesigned to evaluate Large Language Models (LLMs) across Indic languages,\nbuilding upon the MMLU Pro (Massive Multitask Language Understanding)\nframework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi,\nKannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique\nchallenges and opportunities presented by the linguistic diversity of the\nIndian subcontinent. This benchmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and generation, meticulously crafted to\ncapture the intricacies of Indian languages. IndicMMLU-Pro provides a\nstandardized evaluation framework to push the research boundaries in Indic\nlanguage AI, facilitating the development of more accurate, efficient, and\nculturally sensitive models. This paper outlines the benchmarks' design\nprinciples, task taxonomy, and data collection methodology, and presents\nbaseline results from state-of-the-art multilingual models.",
  "text": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on\nMulti-Task Language Understanding\nSankalp KJ1, Ashutosh Kumar2, Laxmaan Balaji3,\nNikunj Kotecha3, Vinija Jain4*, Aman Chadha5†, Sreyoshi Bhaduri6‡\n1Artificial Intelligence Institute, University of South Carolina\n2Rochester Institute of Technology 3Independent Researcher\n4Meta AI 5Amazon Gen AI 6Amazon\nsjajee@email.sc.edu, ak1825@rit.edu, laxmaanb@gmail.com,\nkotecha.nikunj95@gmail.com, hi@vinija.ai, hi@aman.ai, sreyoshibhaduri@gmail.com\nAbstract\nKnown by more than 1.5 billion people in the\nIndian subcontinent, Indic languages present\nunique challenges and opportunities for natu-\nral language processing (NLP) research due to\ntheir rich cultural heritage, linguistic diversity,\nand complex structures. IndicMMLU-Pro is a\ncomprehensive benchmark designed to evalu-\nate Large Language Models (LLMs) across In-\ndic languages, building upon the MMLU Pro\n(Massive Multitask Language Understanding)\nframework. Covering major languages such\nas Hindi, Bengali, Gujarati, Marathi, Kannada,\nPunjabi, Tamil, Telugu, and Urdu, our bench-\nmark addresses the unique challenges and op-\nportunities presented by the linguistic diver-\nsity of the Indian subcontinent. This bench-\nmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and gen-\neration, meticulously crafted to capture the in-\ntricacies of Indian languages. IndicMMLU-\nPro provides a standardized evaluation frame-\nwork to push the research boundaries in In-\ndic language AI, facilitating the development\nof more accurate, efficient, and culturally sen-\nsitive models. This paper outlines the bench-\nmarks’ design principles, task taxonomy, and\ndata collection methodology, and presents\nbaseline results from state-of-the-art multilin-\ngual models. As a publicly available resource,\nIndicMMLU-Pro1 is set to contribute signif-\nicantly to advancements in Indic language-\nbased technologies and serve as a valuable tool\nfor the NLP community.\n1\nIntroduction\nWith over 1.5 billion speakers, Indic languages\nconstitute a substantial component of the world’s\n*Work done outside position at Meta\n†Work done outside position at Amazon Gen AI.\n‡Work done outside position at Amazon.\n1https://huggingface.co/datasets/LinguaLift/IndicMMLU-\nPro\nlinguistic tapestry, showcasing the incredible di-\nversity of the Indian subcontinent, where lan-\nguages from the Indo-Aryan and Dravidian fam-\nilies have evolved over centuries, shaped by a\nshared cultural and historical context. As an in-\ntegral part of daily life, these languages facili-\ntate communication and accessibility in various\naspects of society, including education, govern-\nment, media, healthcare, and social services. Fur-\nther, by developing language-specific benchmarks\nand conducting research on Indic languages, we\ncan increase the downstream impact on these lan-\nguages on inclusion, ultimately leading to better\nlanguage support and enhanced accessibility for\nthe deaf and hard-of-hearing communities across\nthe region who use Indic sign languages (Anan-\nthanarayana et al., 2021b). This, in turn, can en-\nable greater social inclusion, improved education\nand employment opportunities, and more effective\nparticipation in civic life for sign language users.\nThe disparity in Natural Language Processing\n(NLP) research and resources across languages\nis striking with a survey of existing literature\nshowing how resources and research on Indic\nlanguages lagging (KJ et al., 2024a).\nIn their\nwork, (Joshi et al., 2020) revealed a stark imbal-\nance, where a mere 28 percent of languages are\nconsidered \"winners,\" while a staggering 88 per-\ncent are \"left behind.\" This disparity is exempli-\nfied by the contrast between English and Ben-\ngali, languages with comparable speaker popula-\ntions (Lane, 2019). Despite this, English domi-\nnates Bengali regarding available resources, with\nhundreds of times more visibility on platforms like\nthe Linguistic Data Consortium, Wikipedia, and\nacademic publication venues of significance.\nThus, Indic languages have historically received\nless attention in the field of NLP compared to\nmore globally dominant languages (Das et al.,\n2024).\nThe disparity in NLP resources for In-\ndic languages has been attributed to their remark-\narXiv:2501.15747v2  [cs.CL]  28 Jan 2025\nMMLU-Pro Dataset\n(English)\nIndicTrans2\nIndicMMLU-Pro Dataset\n(9 Indic languages)\nFinal IndicMMLU-Pro Dataset\nTranslation Quality Check\nBack-translated dataset \n(English)\nQuality Assurance\nMetrics’ Thresholds\nchrF++ (>0.5) \nBLEU (>25-30)\nMETEOR (0.5-0.6) \nTER (<0.4-0.5) \nsacreBLEU (>25-30)\nHINDI\nBENGALI\nTELUGU\nMARATHI\nTAMIL\nGUJARATI\nURDU\nKANNADA\nPUNJABI\nLinguistic \nStyle\nFluency &\nReadability\nSemantic\nAccuracy\n9000 \nSentence Pairs\n13\nExpert Reviewers\nFigure 1: IndicMMLU-Pro Dataset Construction and Evaluation Pipeline. The diagram illustrates the end-to-\nend process of creating and validating the IndicMMLU-Pro dataset across nine Indic languages. Starting with\nthe English MMLU-Pro dataset, content is translated using IndicTrans2 (1B parameters) and undergoes rigorous\nquality assurance through back-translation and multiple metric evaluations (chrF++, BLEU, METEOR, TER, and\nSacreBLEU). Only translations meeting quality thresholds proceed to the final dataset. The workflow also shows\nthe comprehensive evaluation process including expert proofreading involving 13 reviewers who assess semantic\naccuracy, fluency, and linguistic style. This systematic approach ensures the creation of a high-quality, multilingual\nbenchmark dataset that maintains the integrity of the original MMLU-Pro while adapting to the linguistic nuances\nof Indic languages.\nable linguistic diversity, intricate morphology, and\nthe scarcity of annotated datasets (Kakwani et al.,\n2020a; Marreddy et al., 2022).\nNevertheless,\ndriven by the increasing demand for NLP appli-\ncations in Indic languages and the prospect of\ngroundbreaking technological innovations (Anan-\nthanarayana et al., 2021a), there is an urgent need\nfor rigorous evaluation benchmarks to assess the\nperformance of AI models in this domain accu-\nrately.\nTo address this gap, we introduce IndicMMLU-\nPro, a benchmark built upon the principles of the\nrecently released MMLU-Pro (Wang et al., 2024)\nby Tiger Labs. IndicMMLU-Pro adapts the robust\nmulti-task principles to the unique context of Indic\nlanguages, providing a comprehensive evaluation\nframework that assesses the linguistic understand-\ning, reasoning abilities, and generative capabilities\nof AI models.\nThis paper makes three key contributions. Pri-\nmarily, we introduce IndicMMLU-Pro, a novel\nbenchmark for evaluating AI models across a\nwide range of tasks and multiple Indic languages.\nNext, the design principles, task taxonomy, and\ndata collection methodology of IndicMMLU-Pro\nare presented in detail, to ensure that the bench-\nmark accurately captures the complex linguistic\nand cultural characteristics of Indic languages.\nFurthermore, we establish baseline results on\nIndicMMLU-Pro using state-of-the-art multilin-\ngual models, laying the groundwork for future re-\nsearch and development of Indic languages-based\nAI models.\n2\nMethodology\nTo create IndicMMLU-Pro, a benchmark for Indic\nlanguages equivalent to MMLU Pro, we adopted a\nprocess similar to the prior work on IndicMMLU,\norganizing our approach into two main steps:\ndataset creation and baseline benchmarking.\n2.1\nDataset Creation\nOur goal was to provide MMLU-Pro in nine In-\ndic languages: Hindi, Bengali, Telugu, Marathi,\nTamil, Gujarati, Urdu, Kannada and Punjabi.\nTo achieve this,\nwe used IndicTrans2 (Gala\net al., 2023), a state-of-the-art machine translation\nmodel specifically designed for Indic languages.\nAs shown in Figure1, we use IndicTrans2 to\nconvert the questions and corresponding options\nfrom the original English MMLU Pro dataset into\neach of the target languages. This approach al-\nlowed us to maintain the structure and content of\nthe original benchmark while adapting it to the lin-\nguistic characteristics of Indic languages.\nThe experimental settings for IndicTrans2 were\nas follows:\n• Model Size: 1B parameters\n• Quantization: None\n• Batch Size: 8\nWe applied these settings consistently across all\nnine language translations to ensure uniformity in\nthe translation process.\n2.2\nQuality Assurance\nTo maintain the integrity and accuracy of the trans-\nlated content, we implemented a rigorous quality\nassurance process:\n• Back-translation: For a subset of the data,\nwe performed back-translation to English and\ncompared it with the original text to identify\nany significant discrepancies.\n• Validation: After back-translation, we veri-\nfied the back-translated dataset with the orig-\ninal MMLU-Pro on numerous metrics, such\nas chrF++ BLEU, METEOR, TER & Sacre-\nBLEU to ensure the dataset’s quality and con-\nsistency, also described in Section 3. This\nmulti-metric evaluation provides a compre-\nhensive assessment of the translation’s accu-\nracy and fluency.\n2.3\nDataset Structure\nThe resulting IndicMMLU-Pro dataset maintains\nthe same structure as the original MMLU-Pro\ndataset, with separate subsets for each of the nine\nIndic languages. The dataset retains the original\ncategories and task types from MMLU Pro, thus\nensuring identical usage and facilitating compre-\nhensive evaluation in various domains and cogni-\ntive skills.\n2.4\nDataset Availability\nThe IndicMMLU-Pro dataset is publicly available\non the Hugging Face Hub (KJ et al., 2024b). This\nallows for easy access and reproducibility of our\nresults. Researchers and practitioners can directly\nuse or adapt this dataset for their studies and ap-\nplications in the processing of Indic languages.\n2.5\nBaseline Benchmarking\nTo establish baseline performance metrics for the\nIndicMMLU-Pro benchmark, we evaluated state-\nof-the-art multilingual language models: GPT-4o,\nGPT-4o-mini, Llama-3.1-8B-Instruct, IndicBERT\n(Kakwani et al., 2020a), IndicBART (Dabre et al.,\n2021), RemBERT (Chung et al., 2020), MuRIL\n(Khanuja et al., 2021), and XLM-RoBERTa (Con-\nneau et al., 2020), Navarasa, Airavata (Gala et al.,\n2024), OpenHathi, TamilLlama (Balachandran,\n2023), and MahaMarathi. These models were cho-\nsen due to their demonstrated capabilities in han-\ndling multiple languages and their specific design\nfor the inclusion of Indic languages in their train-\ning data.\nThe benchmarking process was con-\nducted as follows:\n• Model Selection: To provide a benchmark of\nperformance over the various languages con-\ntained in IndicMMLU-Pro, we leverage the\nfollowing language models with Indic lan-\nguage understanding capabilities.\n– IndicBERT and IndicBART: Specifi-\ncally designed for Indic languages, of-\nfering robust performance in this do-\nmain.\n– RemBERT: A multilingual model with\nstrong performance across diverse lan-\nguages, suitable for cross-lingual tasks.\n– MuRIL: A multilingual model with a\nfocus on Indian languages, providing\ncomprehensive coverage of Indic lan-\nguages.\n– XLM-RoBERTa: A large-scale multi-\nlingual model is known for its cross-\nlingual performance and ability to han-\ndle multiple languages efficiently.\n– GPT-4o and GPT-4o-mini: State-of-\nthe-art models with advanced capabili-\nties in handling multilingual tasks.\n– Llama-3.1-8B-Instruct: A model de-\nsigned for multilingual (Hindi) instruc-\ntion tasks.\n– Navarasa,\nAiravata,\nOpenHathi,\nTamilLlama,\nand\nMahaMarathi:\nModels specifically designed or fine-\ntuned for Indic languages, enhancing\ntheir performance in this domain.\n• Data Preparation: We used the test split of\nthe IndicMMLU-Pro dataset for each of the\nnine Indic languages. The data was prepro-\ncessed to match the input format required by\neach model.\n• Evaluation Process: We used accuracy as\nthe primary metric, calculated as the percent-\nage of correct predictions across all tasks in\nthe benchmark.\nWe acknowledge the im-\nportance of using multiple evaluation met-\nrics to garner a comprehensive understand-\ning of text data (Bhaduri et al., 2024a), as\ndifferent metrics can capture distinct aspects\nof model performance (Bedemariam et al.,\n2025).\nNevertheless, for this study, accu-\nracy served as a suitable baseline metric. No-\ntably, our evaluation was conducted sepa-\nrately for each language, enabling language-\nspecific performance analysis and facilitat-\ning a more nuanced understanding of model\nstrengths and weaknesses across diverse lin-\nguistic contexts.\n• Computational Resources:\nThe bench-\nmarking was performed on a cluster of\nNVIDIA A100 GPUs, with each model eval-\nuation taking approximately 24 hours per lan-\nguage.\nThe results of this baseline benchmarking are pre-\nsented in Table 1 of the Results section, showing\nthe accuracy scores for each model across all nine\nlanguages. This benchmarking process provides\na solid foundation for understanding the current\ncapabilities of multilingual models in Indic lan-\nguages and sets a baseline for future research and\nimprovements in this area.\n2.6\nProofreading and Setup\nWe conducted a proofreading exercise on 9,000\nsentence pairs (English and Indic), where 1,000\nof these pairs were from the test split of each\nof the nine languages in the IndicMMLU-Pro\ndataset.\nThese sentences pairs were stratified\nequally across the 14 categories and their respec-\ntive 4 sources in each of the nine languages. These\nsentence pairs were shuffled randomly across all\nIndic languages to eliminate any bias and ensure\nthere was no preservation for each source and cat-\negory after the selection process. The three criteria\nthat were defined for proofreading scores were:\n• Semantic Accuracy and Correctness: en-\nsuring the translation conveys the exact\nmeaning of the original text without errors,\nomissions, or additions.\n• Fluency and Readability: check if the trans-\nlation reads naturally, smoothly, and is easy\nto understand in the target language.\n• Linguistic and Stylistic Appropriateness:\nensuring the tone, style, and language fit the\npurpose, audience, and cultural context of the\ntext.\nA total of 13 experts, representing all 9 Indic\nlanguages, participated in the proofreading exer-\ncise. The expert distribution ensured that each lan-\nguage had at least one expert, with four languages\nhaving an additional expert to facilitate workload\nsharing. Using a standardized evaluation frame-\nwork, each expert assessed sentence pairs based on\nthree criteria, assigning scores on a 5-point scale\n(1 = lowest, 5 = highest). Comprehensive guide-\nlines, accompanied by illustrative samples, were\nprovided to all experts to ensure consistency in\nscoring.\nThe participating experts were native speak-\ners of their respective languages, with fluency\nin English and formal education in their na-\ntive languages.\nTo ensure consistency, experts’\nscores were systematically recorded in spread-\nsheets, which included sentence pairs and evalu-\nations based on the three criteria. We extend our\nsincere gratitude to the experts listed below under\nAcknowledgements, for generously volunteering\ntheir time and expertise to support this research\nendeavor.\n3\nResults\nTable 3 presents the evaluation metrics for the\nIndicMMLU-Pro dataset using back-translation\ntechniques.\nThe chrF++ (Popovi´c, 2017) and\nBLEU (Papineni et al., 2002) scores are provided\nfor each of the nine Indic languages (Bengali, Gu-\njarati, Hindi, Kannada, Marathi, Punjabi, Tamil,\nTelugu, and Urdu). These metrics assess the qual-\nity and accuracy of the translated content in the\nIndicMMLU-Pro benchmark, providing insights\ninto the dataset’s linguistic fidelity across different\nIndic languages.\nTable 1 and 2 present the accuracy scores\n(in percentages) of various pre-trained language\nmodels evaluated on the IndicMMLU-Pro bench-\nmark.\nThe benchmark covers nine major Indic\nlanguages:\nBengali, Gujarati, Hindi, Kannada,\nMarathi, Punjabi, Tamil, Telugu, and Urdu. The\nscores reflect the models’ ability to handle diverse\nlinguistic challenges specific to Indic languages.\n3.1\nOverall Performance\nPerformance Range: The accuracy scores across\nall models and languages now fall within a broader\nrange of 9.90% to 44.80%. This wider range re-\nflects the significant performance differences be-\ntween the newer, more advanced models (like\nGPT-4o) and the previously evaluated models.\nModel Comparison: GPT-4o consistently out-\nperforms all other models across all languages,\nachieving the highest scores ranging from 38.46%\nto 44.80%. GPT-4o mini follows as the second-\nbest performer with scores ranging from 25.75%\nto 35.08%. Among the previously evaluated mod-\nels, XLM-RoBERTa remains the top performer,\nconsistently outperforming other models across\nmost languages with scores ranging from 11.92%\nto 13.16%.\nLanguage-Model Variability: There is now a\nmuch more pronounced variability in performance\nacross different languages and models. This sug-\ngests that language-specific characteristics and\nmodel architectures play significant roles in per-\nformance outcomes.\n3.2\nLanguage-wise Performance\nBreaking down the performance for each language\nreveals interesting patterns:\n• Hindi:\nGPT-4o leads with 44.80%, fol-\nlowed by GPT-4o mini (32.33%).\nAmong\nother models, Navarasa (12.43%) and XLM-\nRoBERTa (12.33%) perform best.\n• Bengali:\nGPT-4o achieves 44.38%, with\nGPT-4o mini at 31.11%.\nXLM-RoBERTa\n(12.68%) and Navarasa (12.08%) lead among\nother models.\n• Telugu:\nGPT-4o scores 41.34%, GPT-4o\nmini 26.78%. XLM-RoBERTa (12.62%) out-\nperforms Navarasa (11.77%) and TamilL-\nlama (11.53%).\n• Marathi: GPT-4o reaches 42.20%, GPT-4o\nmini 27.13%. RemBERT (12.93%) performs\nbest among earlier models, with Navarasa at\n11.88% and MahaMarathi at 11.60%.\n• Tamil:\nGPT-4o mini shows strong per-\nformance at 35.08%, close to GPT-4o’s\n38.46%.\nNavarasa (12.38%) slightly out-\nperforms XLM-RoBERTa (12.34%), with\nTamilLlama at 11.54%.\n• Gujarati: GPT-4o scores 41.77%, GPT-4o\nmini 28.29%.\nIndicBART (12.14%) and\nRemBERT (12.13%) perform well among\nearlier models.\n• Urdu:\nGPT-4o achieves 44.18%, GPT-4o\nmini 31.13%.\nXLM-RoBERTa (12.53%)\nleads among other models.\n• Kannada: GPT-4o scores 38.97%, GPT-4o\nmini 25.75%. XLM-RoBERTa significantly\noutperforms other models (13.16%) among\nearlier models.\n• Punjabi: GPT-4o reaches 40.60%, GPT-4o\nmini 26.25%. XLM-RoBERTa (12.59%) per-\nforms best among other models.\nLanguage\nGPT-4o\nGPT-4o mini\nLlama-3.1-8B\nIndicBART\nIndicBERT\nRemBERT\nMuRIL\nHindi\n44.80\n32.33\n18.61\n11.21\n10.78\n11.41\n10.87\nBengali\n44.38\n31.11\nN/A\n12.52\n10.39\n12.00\n9.90\nPunjabi\n40.60\n26.25\nN/A\n11.78\n10.36\n11.06\n10.36\nMarathi\n42.20\n27.13\nN/A\n11.65\n10.59\n12.93\n11.79\nUrdu\n44.18\n31.13\nN/A\n12.11\n11.63\n11.32\n11.20\nGujarati\n41.77\n28.29\nN/A\n12.14\n11.06\n12.13\n10.79\nTelugu\n41.34\n26.78\nN/A\n12.05\n11.36\n10.20\n9.96\nTamil\n38.46\n35.08\nN/A\n11.70\n10.96\n10.98\n11.00\nKannada\n38.97\n25.75\nN/A\n11.51\n11.71\n10.87\n10.62\nTable 1: Performance comparison of language models on the IndicMMLU-Pro benchmark across nine Indic lan-\nguages, including Indo-Aryan (Hindi, Bengali, Punjabi, Marathi, Urdu, and Gujarati) and Dravidian (Telugu,\nTamil, and Kannada) languages. Accuracy scores are shown as percentages. Models compared include GPT-4o,\nGPT-4o mini, IndicBART, IndicBERT, RemBERT, MuRIL, and Llama-3.1-8B-Instruct.\nLanguage\nXLM-RoBERTa\nNavarasa\nAiravata\nOpenHathi\nTamilLlama\nMahaMarathi\nHindi\n12.33\n12.43\n11.60\n11.65\n-\n-\nBengali\n12.68\n12.08\n-\n-\n-\n-\nPunjabi\n12.59\n11.95\n-\n-\n-\n-\nMarathi\n12.57\n11.88\n-\n-\n-\n11.60\nUrdu\n12.53\n10.73\n-\n-\n-\n-\nGujarati\n11.92\n11.53\n-\n-\n-\n-\nTelugu\n12.62\n11.77\n-\n-\n11.53\n-\nTamil\n12.34\n12.38\n-\n-\n11.66\n-\nKannada\n13.16\n11.88\n-\n-\n-\n-\nTable 2: Comparison of language model performance across Indian languages, both Indo-Aryan (i.e., Hindi, Ben-\ngali, Punjabi, Marathi, Urdu, and Gujarati) and Dravidan (i.e., Telegu, Tamil, and Kannada). Scores are shown for\nLlama 3.1, Navarasa, Airavata, OpenHathi, TamilLlama, and MahaMarathi models where available.\n3.2.1\nCross-linguistic Analysis\nIndo-Aryan languages (Hindi, Bengali, Punjabi,\nGujarati, Urdu, Marathi) and Dravidian languages\n(Kannada, Tamil, Telugu) exhibit distinct perfor-\nmance patterns.\n• Performance Patterns: GPT-4o and GPT-4o\nmini consistently outperform all other models\nacross both language families.Among other\nmodels, XLM-RoBERTa generally performs\nbest across both language families. Navarasa\nshows competitive performance, particularly\nfor the Dravidian language\n• Language-specific Models:\nTamilLlama\nand MahaMarathi show promise but fail to\nsurpass the performance of top multilingual\nmodels like XLM-RoBERTa and Navarasa.\n• Consistency:\nGPT-4o and GPT-4o mini\ndemonstrate\nrelatively\nconsistent\nperfor-\nmance across different languages, suggest-\ning robust multilingual capabilities. Navarasa\nalso shows consistency, particularly among\nDravidian languages.\n• Script-based Patterns: Languages using the\nDevanagari script (Hindi, Marathi) exhibit\nsimilar performance patterns, while Urdu\n(using the Perso-Arabic script) shows more\nvaried performance.\n• Performance Gap:\nA significant perfor-\nmance gap exists between GPT-4o/GPT-4o\nmini and other models, highlighting the ad-\nvantages of larger, more advanced models in\nhandling the complexities of Indic languages.\n3.3\nDataset Quality Assessment\nIn order to heuristically assert the quality of Indic\nlanguage datasets, we convert the dataset from the\nrespective Indic language back to English again by\nleveraging IndicTrans2 as in Section 2.1. The En-\nglish translations are then scored by the following\nmetrics:\n• chrF++ (Popovi´c, 2017) (threshold > 50%):\nA language and token agnostic, character n-\ngram F1 score. We use n=6, as it is the stan-\ndard set in the chrF++ paper.\nLanguage\nchrF++\nBLEU\nMETEOR\nTER\nSacreBLEU\nHindi\n78.06\n0.59\n0.56\n42.27\n59.07\nGujarati\n77.67\n0.58\n0.55\n43.09\n58.28\nTamil\n74.32\n0.54\n0.52\n46.41\n53.64\nTable 3: Back-translation evaluation metrics for the IndicMMLU-Pro dataset for 3 Indic languages.\n• BLEU (Papineni et al., 2002) (threshold >\n25-30%): A score obtained by measuring the\nprecision of n-grams of the candidate trans-\nlation compared to the reference translation,\nwith a brevity penalty to penalize short trans-\nlations.\n• METEOR\n(Banerjee\nand\nLavie,\n2005)\n(threshold = 50-60%): A metric that evalu-\nates translation quality by considering preci-\nsion and recall, along with a harmonic mean\nof unigram matches, with additional features\nsuch as stemming and synonymy matching.\n• TER (Snover et al., 2006) (threshold < 40-\n50%): A score that measures the number of\nedits required to change a system-generated\ntranslation to exactly match a reference trans-\nlation.\n• SacreBLEU (Post, 2018) (threshold > 25-\n30%): A variant of BLEU that ensures stan-\ndard reference text processing and tokeniza-\ntion.\nTable 3 provides insights into the quality of the\nIndicMMLU-Pro dataset through back-translation\nevaluation metrics for three languages: Hindi, Gu-\njarati, and Tamil.\nThe metrics include chrF++,\nBLEU, METEOR, TER, and SacreBLEU scores.\nHindi shows the highest overall quality with\na chrF++ score of 78.06, BLEU of 0.59, ME-\nTEOR of 0.56, TER of 42.27, and SacreBLEU\nof 59.07. Gujarati follows closely with very sim-\nilar scores: chrF++ of 77.67, BLEU of 0.58, ME-\nTEOR of 0.55, TER of 43.09, and SacreBLEU\nof 58.28. Tamil, while still demonstrating good\nquality, shows slightly lower scores across all met-\nrics: chrF++ of 74.32, BLEU of 0.54, METEOR\nof 0.52, TER of 46.41, and SacreBLEU of 53.64.\nThe high chrF++ scores (above 74) for all three\nlanguages indicate good overall translation qual-\nity and semantic preservation.\nThe BLEU and\nSacreBLEU scores, ranging from 0.54 to 0.59 and\n53.64 to 59.07 respectively, suggest reasonably\ngood translation quality, though there’s room for\nimprovement. The METEOR scores (0.52-0.56)\nalso indicate good semantic similarity between the\noriginal and back-translated texts.\nThe Translation Error Rate (TER) scores, rang-\ning from 42.27 to 46.41, suggest that a moderate\namount of editing is required to match the ref-\nerence translation, with Tamil requiring slightly\nmore editing than Hindi or Gujarati.\nIt is important to note that data for the other\nsix languages (Bengali, Punjabi, Kannada, Telugu,\nUrdu, and Marathi) is missing from Table 2. This\nlimitation in the dataset quality assessment makes\nit challenging to draw comprehensive conclusions\nabout the overall quality of the IndicMMLU-Pro\ndataset across all nine languages.\nIn summary, the available metrics indicate good\ntranslation quality for Hindi, Gujarati, and Tamil,\nwith Hindi showing the highest quality across all\nmetrics. However, the lack of data for the remain-\ning languages highlights the need for a more com-\nprehensive evaluation of the entire dataset.\nTo better illustrate the meaning of chrF++\nscores in practice, let’s examine sample transla-\ntions for Hindi (highest score), Gujarati (middle\nscore), and Tamil (lowest score) as shown in\nFigure 2, 3, 4 respectively.\nHindi (chrF++: 78.06): The chrF++ score for\nHindi is the best among the three languages in-\ndicating a high structural and semantic similarity\nbetween the original text and the back-translated\ntext.\nGujarati (chrF++:\n77.67):\nWhile slightly\nlower, this score still represents a high-quality\ntranslation with minor variations.\nTamil (chrF++: 74.32): This lower score might\nreflect slight changes in word choice or structure,\nbut the core meaning is preserved.\nThese examples demonstrate that even with the\nlowest chrF++ score in our dataset (74.32 for\nTamil), the translations maintain good semantic\nfidelity.\nThe differences in scores often reflect\nOriginal\nHindi\nBack-translated\nA total of 30 players will play basketball at \na park. There will be exactly 5 players on \neach team. Which statement correctly \nexplains how to find the number of teams \nneeded?\nएक पाMर्क में कुल 30 Sखिलाड़ी बास्केटबॉल खेलेंगे। \nप्रत्येक टीम में ठीक 5 Sखिलाड़ी होंगे। कौन सा कथन \nसही ढंग से बताता है :कि आवश्यक टीमों की संख्या \nकैसे ज्ञात की जाए?\nA total of 30 players will play basketball in \na park. Each team will have exactly 5 \nplayers. Which statement correctly states \nhow to find the number of teams required?\nFigure 2: The original text sample, its Hindi translation, and the corresponding back-translated text\nOriginal\nGujarati\nBack-translated\nWhich of the following is the body cavity \nthat contains the pituitary gland?\nનીચેનામાંથી કઈ શરીરની પોલાણ છે જેમાં કફોત્પાદક \nગ્રંCથિ હોય છે?\nWhich of the following is the body cavity \nthat contains the pituitary gland?\nFigure 3: The original text sample, its Gujarati translation, and the corresponding back-translated text\nOriginal\nTamil\nBack-translated\nWhat is the embryological origin of the \nhyoid bone?\nCஹைய்டு எலும்பின் கருப்3பொருள் \n1தோற்றம் என்ன?\nWhat is the thematic appearance of the \nhyoid bone?\nFigure 4: The original text sample, its Tamil translation, and the corresponding back-translated text\nLanguage\nQuestions\nChoices\nHindi\n0.9109\n0.9250\nBengali\n0.9172\n0.9251\nTelugu\n0.9193\n0.9287\nMarathi\n0.9126\n0.9242\nTamil\n0.9194\n0.9255\nGujarati\n0.9164\n0.9320\nUrdu\n0.9121\n0.9302\nKannada\n0.9149\n0.9238\nPunjabi\n0.9177\n0.9254\nTable 4: Cosine similarity scores between LaBSE em-\nbeddings of IndicMMLU-Pro languages and English\nMMLU-Pro for questions and multiple-choice options.\nThese scores are used as a measure of semantic sim-\nilarity, with higher values suggesting closer meaning\nalignment across languages.\nnuances in word choice, sentence structure, or\nslight variations in conveying the same meaning,\nrather than significant errors in translation. This\nanalysis supports our earlier observation that the\nIndicMMLU-Pro dataset maintains good overall\ntranslation quality across the evaluated languages.\nPreliminary assessments of the other languages\n(Bengali, Punjabi, Kannada, Telugu, Urdu, and\nMarathi) suggest similar trends in translation qual-\nity, with chrF++ scores ranging from 73 to 79.\nComplete metrics for these languages are being\ncompiled and will be included in future publica-\ntions.\n3.4\nCosine Similarity Scores\nWe evaluated the semantic similarity between In-\ndic languages from the IndicMMLU-Pro dataset\nand the English MMLU-Pro dataset using the\nLaBSE (Language-agnostic BERT Sentence Em-\nbedding) model. It loads both datasets and pro-\ncesses all the samples from each. For each pair of\ncorresponding questions, it generates embeddings\nfor the questions and their multiple-choice options\nusing the LaBSE model.\nThe script then calculates cosine similarity be-\ntween these embeddings to measure how seman-\ntically similar Indic languages and their english\nversions are. It computes two main metrics: av-\nerage question similarity, and average choice sim-\nilarity. The question and choice similarities indi-\ncate how close the meanings are between the two\nlanguages. This analysis helps assess how accu-\nrately Indic languages in the IndicMMLU-Pro rep-\nresent the original English MMLU-Pro in terms\nof meaning and structure, which is crucial for en-\nsuring the quality and consistency of multilingual\ndatasets in machine learning and natural language\nprocessing tasks.\n4\nRelated Work\nThe study of multilingual language models has\nbeen significantly advanced by efforts to create\nand utilize datasets that encompass a wide range\nof languages. Indic languages, with their diverse\nscripts and rich morphological structures, present\nunique challenges and opportunities for multilin-\ngual NLP research.\nMultilingual datasets have been crucial in de-\nveloping language models capable of understand-\ning and generating text in multiple languages. The\nWikiAnn dataset, for instance, provides a valuable\nresource for named entity recognition across many\nlanguages, including several Indic languages (Pan\net al., 2017). The IndicNLP Corpus (Kunchukut-\ntan, 2020), developed as part of the IndicNLP Li-\nbrary, provides extensive monolingual corpora for\nvarious Indic languages.\nThis dataset has been\ninstrumental in training and evaluating models\nspecifically designed for these languages. Indic-\nCorp (Joshi et al., 2022), a large-scale dataset of\nIndic languages, and the IndicNLPSuite(Kakwani\net al., 2020b), which includes pre-trained models\nand linguistic resources, have further enhanced the\ndevelopment of multilingual NLP models. These\nresources have provided a solid foundation for var-\nious NLP tasks such as machine translation, text\nclassification, etc.\nIndicGLUE(Kakwani et al., 2020c) is a compre-\nhensive benchmark designed to evaluate the per-\nformance of NLP models on a variety of tasks\nacross multiple Indic languages. It includes tasks\nsuch as sentiment analysis, natural language infer-\nence, and question answering, making it a valuable\nresource for assessing the capabilities of multilin-\ngual models.\n(Ahuja et al., 2023) introduce Megaverse, a\ncomprehensive benchmark designed to evaluate\nlarge language models across a variety of lan-\nguages, modalities, models, and tasks.\nThis\nbenchmark aims to provide a holistic assessment\nof language models’ performance, emphasizing\ntheir capabilities and limitations in handling di-\nverse linguistic and contextual scenarios. Mega-\nverse is instrumental in identifying the strengths\nand weaknesses of current models and guiding fu-\nture improvements in multilingual and multimodal\nlanguage processing.\n(Holtermann et al., 2024) developed MultiQ,\na benchmark aimed at evaluating the elementary\nmultilingual capabilities of large language mod-\nels. MultiQ focuses on assessing basic language\nunderstanding and generation tasks across multi-\nple languages, providing insights into the foun-\ndational multilingual capabilities of these models.\nThis benchmark is critical for identifying gaps in\nlanguage model training and performance, partic-\nularly for less-represented languages.\n(Aggarwal et al., 2022) introduce IndicXNLI,\na benchmark specifically designed to evaluate the\nnatural language inference capabilities of multi-\nlingual models for Indian languages. This bench-\nmark includes a diverse set of inference tasks, re-\nflecting the linguistic richness and complexity of\nIndic languages. IndicXNLI is crucial for advanc-\ning the understanding and processing of Indian\nlanguages in NLP, providing a standard for eval-\nuating inference models in this context.\n(Kumar et al., 2022) develop the IndicNLG\nbenchmark, which provides multilingual datasets\nfor a variety of natural language generation (NLG)\ntasks in Indic languages.\nThis benchmark ad-\ndresses the unique challenges posed by the linguis-\ntic diversity of Indic languages, offering datasets\nfor tasks such as machine translation, summariza-\ntion, and text generation. IndicNLG is instrumen-\ntal in improving the performance and capabilities\nof NLG models for Indic languages, fostering ad-\nvancements in multilingual NLP.\nMMLU (Massively Multilingual Language Un-\nderstanding) (Liang et al., 2020; Hu et al., 2020) is\na comprehensive benchmark designed to evaluate\nthe performance of language models across mul-\ntiple languages and a wide range of tasks. This\nbenchmark has been instrumental in assessing the\ncapabilities of multilingual models and has pro-\nvided insights into their strengths and limitations.\n5\nKey Findings and Implications of\nIndicMMLU-Pro\nIndicMMLU-Pro represents a significant step for-\nward in the development and evaluation of AI\nmodels for Indic languages. Through this com-\nprehensive benchmark, we have shed light on the\ncurrent capabilities and limitations of state-of-the-\nart multilingual models in handling the linguistic\ndiversity and complexity of the Indian subconti-\nnent. This section details implications based on\nsome of our key findings.\n5.1\nPerformance Across Models\n5.1.1\nGPT-4o Dominance\nGPT-4o consistently outperformed all other mod-\nels across all languages, with accuracy scores\nranging from 38.46% to 44.80%. This suggests\nthat large, advanced language models have signif-\nicant potential for handling Indic languages.\n5.1.2\nPerformance Tiers\nA clear hierarchy emerged among the models:\n• Elite Tier: GPT-4o\n• Advanced Tier: GPT-4o mini\n• Specialized\nTier:\nXLM-RoBERTa,\nNavarasa, and other specialized models\n• Foundation\nTier:\nEarlier\nmodels\nlike\nIndicBERT,\nIndicBART,\nRemBERT,\nand\nMuRIL\n5.1.3\nSpecialized vs. General Models\nWhile GPT-4o and GPT-4o mini showed supe-\nrior performance, specialized models like XLM-\nRoBERTa\nand\nNavarasa\nconsistently\noutper-\nformed other models in the foundation tier. This\nhighlights the importance of both scale and spe-\ncialized training for Indic languages.\n5.2\nLanguage-Specific Insights\n5.2.1\nPerformance Variability\nSignificant variability in model performance was\nobserved across different Indic languages, empha-\nsizing the need for language-specific approaches\nin NLP.\n5.2.2\nScript and Language Family Impact\nLanguages using similar scripts (e.g., Devanagari\nfor Hindi and Marathi) showed similar perfor-\nmance patterns. Urdu, using a different script, ex-\nhibited more varied performance across models.\n5.2.3\nIndo-Aryan vs. Dravidian Languages\nWhile performance varied across both language\nfamilies, some models showed more consistency\nin Dravidian languages, suggesting potential dif-\nferences in how models handle these distinct lan-\nguage groups.\n5.3\nDataset and Evaluation Quality\n5.3.1\nTranslation Quality\nBack-translation evaluation metrics for Hindi, Gu-\njarati, and Tamil demonstrated good overall trans-\nlation quality and semantic preservation, with\nchrF++ scores above 74 for all three languages.\n5.3.2\nMetric Discrepancies\nDifferences between chrF++ and BLEU scores\nsuggest that while meaning is generally preserved,\nthere may be variations in phrasing and structure\ncompared to the original English text.\n5.3.3\nComprehensive Evaluation Need\nThe lack of evaluation metrics for six out of nine\nlanguages highlights the need for more thorough\nassessment across all included Indic languages.\n5.4\nMethodological Insights\n5.4.1\nBenchmark Design\nThe\nadaptation\nof\nMMLU-Pro\nto\ncreate\nIndicMMLU-Pro\ndemonstrates\na\nsuccessful\napproach to developing comprehensive, multi-\ntask benchmarks for specific language groups.\n5.4.2\nTranslation Approach\nThe use of IndicTrans2 for dataset creation, com-\nbined with rigorous quality assurance processes,\nprovides a template for developing high-quality\nmultilingual datasets.\n5.4.3\nEvaluation Metrics\nThe use of multiple metrics (chrF++, BLEU, ME-\nTEOR, TER, SacreBLEU) for assessing transla-\ntion quality offers a more nuanced understanding\nof dataset quality.\nThese findings and implications highlight the\nsignificant advancements made by IndicMMLU-\nPro in evaluating and understanding the perfor-\nmance of language models across Indic languages,\nwhile also pointing to crucial areas for future re-\nsearch and development in multilingual NLP.\n5.5\nFuture Directions\nBased on our findings, we propose several key ar-\neas for future research and development:\n• Data Collection: There is a pressing need\nfor more high-quality, diverse datasets across\nall Indic languages, particularly for low-\nresource languages. This will in turn enable\nmore robust model training and evaluation.\n• Model\nDevelopment:\nFuture\nresearch\nshould focus on developing models that can\nbetter handle the unique linguistic features\nof Indic languages, including their complex\nmorphology and script diversity. This may\ninvolve innovative architecture designs or\nnovel pre-training techniques.\n• Cross-lingual Transfer:\nExploring tech-\nniques to improve knowledge transfer be-\ntween related Indic languages will help boost\nperformance, especially for low-resource lan-\nguages.\nThis could involve leveraging lin-\nguistic similarities or developing more effec-\ntive multilingual training strategies.\n• Task-specific\nFine-tuning:\nDeveloping\nstrategies for effective fine-tuning of large\nmultilingual models on specific language\ntasks lead to significant performance im-\nprovements (Balne et al., 2024). Thus future\nwork in this area may include investigating\noptimal fine-tuning techniques or developing\nIndic-specific pre-training tasks.\n• Evaluation Metrics: Further refinement of\nevaluation metrics to account for the linguis-\ntic and cultural nuances of Indic languages is\nnecessary for more accurate performance as-\nsessment. Existing metrics often overlook the\nlinguistic and cultural nuances inherent to In-\ndic languages.\nIndicMMLU-Pro sets a new standard for eval-\nuating AI models in the context of Indic lan-\nguages.\nBy providing a comprehensive, multi-\ntask, and multilingual benchmark, it enables re-\nsearchers and developers to assess and improve the\ncapabilities of their models across a wide range of\nIndic languages and domains. As the field of NLP\ncontinues to advance, we hope that IndicMMLU-\nPro will catalyze increased research and develop-\nment of technologies based on Indic languages.\nThis benchmark not only highlights the current\nstate of the art but also points the way forward\nfor creating more linguistically diverse, culturally\nsensitive, and capable AI systems that can serve\nthe vast and diverse population of the Indian sub-\ncontinent.\nTo foster a more comprehensive understand-\ning of Indic language models, it is essential to\nintentionally integrate multi-disciplinary perspec-\ntives into engineering and science workstreams\n(Bhaduri et al., 2024b). Collaborations between\nengineers, scientists, and experts from social sci-\nences can provide nuanced insights into the com-\nplex interplay between language, culture, and\ntechnology (Mackenzie et al., 2024). By embrac-\ning interdisciplinary approaches and refining eval-\nuation metrics, we can work towards a more in-\nclusive and accurate assessment of Indic language\nmodels, ultimately driving progress in natural lan-\nguage processing for diverse languages and com-\nmunities.\nIn conclusion,\nwhile significant challenges\nremain in the field of Indic language NLP,\nIndicMMLU-Pro provides a robust framework for\nmeasuring progress and guiding future research\nefforts. As we continue to push the boundaries\nof what’s possible in multilingual AI, benchmarks\nlike IndicMMLU-Pro will play a crucial role in en-\nsuring that technological advancements benefit all\nlanguage communities, fostering greater inclusiv-\nity and accessibility in the digital age.\n6\nAcknowledgements\nWe extend our deepest gratitude to the experts who\nvolunteered their time and effort to evaluate our\ndataset.\nUnfortunately, Indic languages are of-\nten neglected in the development and evaluation\nof Large Language Models, making initiatives like\nours reliant on the generosity and expertise of indi-\nviduals like those listed below. The list of experts\nand the language they reviewed are:\n• Telugu Experts: Dushyanth Reddy and Sai\nKrishna Bolla\n• Tamil Expert: Veeramanohar Avudaiappan\n• Hindi Expert: Ojasmitha Pedirappagari\n• Gujarati Expert: Ashwini Thosar\n• Punjabi Expert: Shivan Bhatia\n• Marathi\nExperts:\nSameer\nJadhav\nand\nNityanand Bhadra\n• Kannada Expert: Nikhil Ragunath\n• Bengali Experts: Sounak Datta and Nirjhar\nBarua\n• Urdu Experts: Md Mahtab and Risha Fatema\nExpert feedback has significantly enhanced the\nquality of our dataset, and we are grateful for their\ndedication to promoting Indic languages in the\nglobal LLM community.\nReferences\nDivyanshu Aggarwal,\nVivek Gupta,\nand Anoop\nKunchukuttan. 2022.\nIndicxnli: Evaluating mul-\ntilingual inference for indian languages.\narXiv\npreprint arXiv:2204.08776.\nSanchit Ahuja, Divyanshu Aggarwal, Varun Gumma,\nIshaan Watts, Ashutosh Sathe, Millicent Ochieng,\nRishav Hada, Prachi Jain, Maxamed Axmed, Ka-\nlika Bali, et al. 2023. Megaverse: Benchmarking\nlarge language models across languages, modalities,\nmodels and tasks. arXiv preprint arXiv:2311.07463.\nTejaswini Ananthanarayana, Nikunj Kotecha, Priyan-\nshu\nSrivastava,\nLipisha\nChaudhary,\nNicholas\nWilkins, and Ifeoma Nwogu. 2021a.\nDynamic\ncross-feature fusion for american sign language\ntranslation. In 2021 16th IEEE International Con-\nference on Automatic Face and Gesture Recognition\n(FG 2021), pages 1–8. IEEE.\nTejaswini Ananthanarayana,\nPriyanshu Srivastava,\nAkash Chintha, Akhil Santha, Brian Landy, Joseph\nPanaro, Andre Webster, Nikunj Kotecha, Shagan\nSah, Thomastine Sarchet, et al. 2021b. Deep learn-\ning methods for sign language translation.\nACM\nTransactions on Accessible Computing (TACCESS),\n14(4):1–30.\nAbhinand Balachandran. 2023. Tamil-llama: A new\ntamil language model based on llama 2.\narXiv\npreprint arXiv:2311.05845.\nCharith Chandra Sai Balne, Sreyoshi Bhaduri, Ta-\nmoghna Roy, Vinija Jain, and Aman Chadha. 2024.\nParameter efficient fine tuning:\nA comprehen-\nsive analysis across applications.\narXiv preprint\narXiv:2404.13506.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65–72.\nRewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri,\nSatya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei\nItoku, David Theil, Aman Chadha, and Naumaan\nNayyar. 2025. Potential and perils of large language\nmodels as judges of unstructured textual data. arXiv\npreprint arXiv:2501.08167.\nSreyoshi Bhaduri, Satya Kapoor, Alex Gil, Anshul\nMittal, and Rutu Mulkar. 2024a.\nReconciling\nmethodological paradigms: Employing large lan-\nguage models as novice qualitative research assis-\ntants in talent management research. arXiv preprint\narXiv:2408.11043.\nSreyoshi Bhaduri, Kenneth Ohnemus, Jess Black-\nburn, Anshul Mittal, Yan Dong, Savannah Lafer-\nriere, Robert Pulvermacher, Marina Dias, Alex Gil,\nShahriar Sadighi, et al. 2024b. (multi-disciplinary)\nteamwork makes the (real) dream work:\nPrag-\nmatic recommendations from industry for engineer-\ning classrooms.\nHyung Won Chung, Thibault Févry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2020.\nRe-\nthinking embedding coupling in pre-trained lan-\nguage models. Preprint, arXiv:2010.12821.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020.\nUnsuper-\nvised cross-lingual representation learning at scale.\nPreprint, arXiv:1911.02116.\nRaj Dabre, Himani Shrotriya, Anoop Kunchukuttan,\nRatish Puduppully, Mitesh M. Khapra, and Pratyush\nKumar. 2021.\nIndicbart:\nA pre-trained model\nfor natural language generation of indic languages.\nPreprint, arXiv:2109.02903.\nDipto Das, Shion Guha, Jed R Brubaker, and Bryan\nSemaan. 2024.\nThe“colonial impulse\" of natural\nlanguage processing: An audit of bengali sentiment\nanalysis tools and their identity-based biases.\nIn\nProceedings of the CHI Conference on Human Fac-\ntors in Computing Systems, pages 1–18.\nJay Gala, Pranjal A. Chitale, Raghavan AK, Varun\nGumma, Sumanth Doddapaneni, Aswanth Kumar,\nJanki Nawale, Anupama Sujatha, Ratish Pudup-\npully, Vivek Raghavan, Pratyush Kumar, Mitesh M.\nKhapra, Raj Dabre, and Anoop Kunchukuttan. 2023.\nIndictrans2: Towards high-quality and accessible\nmachine translation models for all 22 scheduled in-\ndian languages. Preprint, arXiv:2305.16307.\nJay Gala, Thanmay Jayakumar, Jaavid Aktar Hu-\nsain, Mohammed Safi Ur Rahman Khan, Diptesh\nKanojia, Ratish Puduppully, Mitesh M Khapra, Raj\nDabre, Rudra Murthy, Anoop Kunchukuttan, et al.\n2024. Airavata: Introducing hindi instruction-tuned\nllm. arXiv preprint arXiv:2401.15006.\nCarolin Holtermann, Paul Röttger, Timm Dill, and\nAnne Lauscher. 2024.\nEvaluating the elementary\nmultilingual capabilities of large language models\nwith multiq. arXiv preprint arXiv:2403.03814.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. arXiv preprint arXiv:2003.11080.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the nlp\nworld. arXiv preprint arXiv:2004.09095.\nSachin Joshi et al. 2022.\nIndiccorp: A large-scale\ndataset for indic languages. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020a.\nIndicNLP-\nSuite:\nMonolingual Corpora, Evaluation Bench-\nmarks and Pre-trained Multilingual Language Mod-\nels for Indian Languages. In Findings of EMNLP.\nDivyanshu Kakwani, Anoop Kunchukuttan, Raj D\nShah, et al. 2020b. Indicnlpsuite: Monolingual cor-\npora, evaluation benchmarks and pre-trained multi-\nlingual language models for indian languages. arXiv\npreprint arXiv:2004.09095.\nDivyanshu Kakwani et al. 2020c. Indicglue: A bench-\nmark for evaluating indic language understanding.\narXiv preprint arXiv:2004.09095.\nSimran Khanuja, Diksha Bansal, Sarvesh Mehtani,\nSavya\nKhosla,\nAtreyee\nDey,\nBalaji\nGopalan,\nDilip Kumar Margam, Pooja Aggarwal, Rajiv Teja\nNagipogu, Shachi Dave, Shruti Gupta, Subhash\nChandra Bose Gali, Vish Subramanian, and Partha\nTalukdar. 2021. Muril: Multilingual representations\nfor indian languages. Preprint, arXiv:2103.10730.\nSankalp KJ, Vinija Jain, Sreyoshi Bhaduri, Tamoghna\nRoy, and Aman Chadha. 2024a. Decoding the di-\nversity: A review of the indic ai research landscape.\narXiv preprint arXiv:2406.09559.\nSankalp\nKJ,\nAshutosh\nKumar,\nLaxmaan\nBalaji,\nNikunj Kotecha, Vinija Jain, Aman Chadha, and\nSreyoshi Bhaduri. 2024b.\nIndicMMLU-Pro:\nA\nComprehensive Benchmark for Evaluating Mul-\ntilingual\nAI\nModels\nacross\nIndic\nLanguages.\nhttps://huggingface.co/datasets/\nLinguaLift/IndicMMLU-Pro.\nAccessed:\n2024-07-12.\nAman Kumar, Himani Shrotriya, Prachi Sahu, Raj\nDabre, Ratish Puduppully, Anoop Kunchukuttan,\nAmogh Mishra, Mitesh M Khapra, and Pratyush\nKumar. 2022.\nIndicnlg benchmark: Multilingual\ndatasets for diverse nlg tasks in indic languages.\narXiv preprint arXiv:2203.05437.\nAnoop Kunchukuttan. 2020. The indicnlp library.\nJames Lane. 2019. The 10 most spoken languages in\nthe world. Babbel Magazine, 6(09).\nYaobo Liang et al. 2020. Xglue: A new benchmark\ndataset for cross-lingual pre-training, understanding\nand generation. arXiv preprint arXiv:2004.01401.\nTammy Mackenzie, Leslie Salgado, Sreyoshi Bhaduri,\nVictoria Kuketz, Solenne Savoia, and Lilianny Vir-\nguez. 2024.\nBeyond the algorithm: Empowering\nai practitioners through liberal education. In 2024\nASEE Annual Conference & Exposition.\nMounika\nMarreddy,\nSubba\nReddy\nOota,\nLak-\nshmi Sireesha Vakada, Venkata Charan Chinni, and\nRadhika Mamidi. 2022. Am i a resource-poor lan-\nguage? data sets, embeddings, models and analysis\nfor four different nlp tasks in telugu language. ACM\nTransactions on Asian and Low-Resource Language\nInformation Processing, 22(1):1–34.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1946–1958.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei\njing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation.\nMaja Popovi´c. 2017. chrF++: words helping charac-\nter n-grams.\nIn Proceedings of the Second Con-\nference on Machine Translation, pages 612–618,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv preprint arXiv:1804.08771.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A study\nof translation edit rate with targeted human annota-\ntion. In Proceedings of the 7th Conference of the\nAssociation for Machine Translation in the Ameri-\ncas: Technical Papers, pages 223–231.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng\nNi, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle\nLi, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-\npro:\nA more robust and challenging multi-task\nlanguage understanding benchmark.\nPreprint,\narXiv:2406.01574.\n7\nAppendix\n7.1\nChoice of Models\nThe selection of models for our benchmark eval-\nuation was based on their relevance to Indic lan-\nguages, their prominence in multilingual NLP re-\nsearch, and their potential to handle the diverse\nlinguistic features of Indic languages. Here’s a de-\ntailed explanation of our choices:\n• GPT-4o:\nA highly advanced multimodal\nmodel that offers improved speed and cost-\neffectiveness compared to GPT-4 Turbo,\nwhile also featuring enhanced vision capa-\nbilities. With its 128K context window and\nknowledge cutoff in October 2023, GPT-\n4o represents the cutting edge in large lan-\nguage models, making it an excellent candi-\ndate for benchmarking across diverse Indic\nlanguages.\n• GPT-4o-mini:\nDesigned as a more cost-\nefficient alternative to larger models, GPT-\n4o-mini offers a balance between perfor-\nmance and resource utilization. Despite its\nsmaller size, it outperforms GPT-3.5 Turbo\nin intelligence while maintaining lower costs.\nIts inclusion in our benchmark allows us to\nassess the trade-offs between model size, per-\nformance, and efficiency in processing Indic\nlanguages.\n• IndicBERT: A transformer-based multilin-\ngual model specifically pre-trained on 12\nmajor Indian languages.\nDeveloped by\nAI4Bharat, IndicBERT is designed to cap-\nture the nuances of Indic languages through\nits training on a large corpus of Indian lan-\nguage text. Its inclusion in our benchmark is\ncrucial for evaluating performance on tasks\nthat require a deep understanding of Indian\nlanguage structures and semantics.\n• IndicBART: A sequence-to-sequence model\ntailored for Indian languages, IndicBART ex-\ntends the capabilities of mBART by incor-\nporating additional pre-training on Indic lan-\nguage corpora. It excels in generation tasks\nsuch as summarization, translation, and text\ncompletion across multiple Indian languages.\nIts inclusion allows us to assess performance\non more complex, generative NLP tasks spe-\ncific to Indic languages.\n• RemBERT: Developed by Google Research,\nRemBERT (Rebalanced Multilingual BERT)\nis an advanced multilingual model that ad-\ndresses the limitations of previous multilin-\ngual models. It uses a novel rebalancing tech-\nnique during pre-training to improve perfor-\nmance across a wide range of languages, in-\ncluding low-resource ones. RemBERT’s in-\nclusion in our benchmark provides a strong\nbaseline for assessing how well general mul-\ntilingual models perform on Indic language\ntasks compared to Indic-specific models.\n• MuRIL: (Multilingual Representations for\nIndian Languages): Developed by Google\nResearch India, MuRIL is specifically de-\nsigned to address the unique challenges of In-\ndian languages. It’s trained on significantly\nlarger amounts of Indian language data com-\npared to general multilingual models, cov-\nering 17 Indian languages including low-\nresource ones. MuRIL’s architecture incor-\nporates techniques to handle the linguistic di-\nversity and script complexity of Indian lan-\nguages, making it a crucial benchmark for\nevaluating specialized Indic language perfor-\nmance.\n• XLM-RoBERTa:\nA large-scale multilin-\ngual model trained on 100 languages, XLM-\nRoBERTa represents a significant advance-\nment in cross-lingual language understand-\ning.\nDeveloped by Facebook AI, it uses a\nself-supervised learning approach on a vast\namount of web-crawled data. Its inclusion\nin our benchmark allows us to assess how\nwell a general multilingual model, trained\non a diverse set of languages including In-\ndic ones, performs on specific Indic language\ntasks compared to models with a more fo-\ncused Indic language training.\n• OpenHathi: An open-source large language\nmodel focused primarily on Hindi and other\nIndian languages. Developed as part of an\ninitiative to create freely accessible AI re-\nsources for Indic languages, OpenHathi is\ntrained on a diverse corpus of Indian lan-\nguage texts, including literature, news, and\nweb content.\nIts inclusion in our bench-\nmark allows us to evaluate the performance\nof community-driven, open-source efforts in\nIndic language modeling, particularly for\nHindi, and compare it with proprietary and\ngeneral multilingual models.\n• Airavata: Developed by AI4Bharat, Aira-\nvata is a specialized large language model fo-\ncused on Hindi and other Indian languages.\nIt represents a significant effort in creating\ninstruction-tuned models specifically for In-\ndic languages. Trained on a diverse corpus\nof Hindi text and fine-tuned on carefully cu-\nrated instruction datasets, Airavata aims to\nunderstand and generate contextually appro-\npriate responses in Hindi. Its inclusion in our\nbenchmark allows us to evaluate the effec-\ntiveness of language-specific instruction tun-\ning for Indic languages, particularly in tasks\nrequiring a nuanced understanding of cultural\nand linguistic contexts unique to Hindi.\n• TamilLlama: A pioneering language model\nspecifically designed for Tamil, one of the\nclassical languages of India. Developed by\na team of researchers and language enthusi-\nasts, TamilLlama is built upon the LLaMA\narchitecture but fine-tuned extensively on a\nlarge corpus of Tamil text.\nIt incorporates\nunique features to handle the agglutinative\nnature of Tamil and its complex morphology.\nBy including TamilLlama in our benchmark,\nwe aim to assess the advantages of highly\nspecialized, language-specific models in cap-\nturing the intricacies of individual Indic lan-\nguages, particularly for tasks that require a\ndeep understanding of Tamil linguistic struc-\ntures and cultural nuances.\n7.2\nModel Output & Evaluation\n• IndicBERT: The IndicBERT model, when\nused\nwith\nAutoModelForMultipleChoice,\noutputs logits for each option in a multiple-\nchoice question. These logits represent the\nmodel’s confidence scores for each option.\nThe model produces logits for each option.\ntorch.argmax(logits, dim=1) selects the index\nof the option with the highest logit (i.e., the\nmodel’s predicted answer).\nThis index is\nconverted to a letter (A, B, C, D) using the\nindex_to_letter dictionary.\nThe predicted\nanswer (letter) is directly compared with the\ncorrect answer.\n• IndicBART: The IndicBART model is be-\ning loaded with AutoModelForSeq2SeqLM\nframework to construct input texts by com-\nbining the question with each of its options in\nthe format: ’Question: <question> Answer:\n<option>’. These input texts are encoded into\na format that the model can process.\nThe\nmodel generates responses for each encoded\ninput text, effectively simulating an answer\nfor each option. A scoring heuristic, such as\nthe length of the generated response, is then\napplied to determine the relevance or quality\nof each simulated answer. The option with\nthe highest score is selected as the model’s\npredicted answer. This method leverages the\nmodel’s ability to generate coherent and con-\ntextually appropriate responses, allowing it to\nadapt to different questions and options ef-\nfectively. It iterates through all questions, ap-\nplies this process, and calculates the accuracy\nby comparing predicted answers with the cor-\nrect ones, thus evaluating the model’s perfor-\nmance in handling multiple-choice questions\nacross various languages.\n• MURIL: The MURIL (Multilingual Repre-\nsentations for Indian Languages) model is\nbeing used for sequence classification. The\nmodel’s output is a single logit value for\neach question-option pair, representing the\nmodel’s confidence or relevance score for\nthat pair. The evaluation process works as\nfollows: For each question, the script passes\nthe question text paired with each option to\nthe model separately. The model produces a\nscore for each pair. These scores are collected\nfor all options of a question. The option with\nthe highest score is selected as the predicted\nanswer. This approach assumes that the op-\ntion most relevant or coherent with the ques-\ntion (as determined by the model’s score) is\nlikely to be the correct answer. While this\nmethod can work for some types of ques-\ntions, it’s not the standard way to use MURIL\nfor multiple-choice questions. MURIL is pri-\nmarily designed for tasks like text classifi-\ncation, not specifically for multiple-choice\nquestion answering. A more appropriate ap-\nproach would be to fine-tune MURIL on a\nmultiple-choice QA task, or to use a model\nspecifically designed for this purpose. The\ncurrent method might work to some extent\ndue to MURIL’s pretraining on diverse tasks,\nbut it may not be as accurate or reliable\nas a properly fine-tuned model for multiple-\nchoice questions.\n• RemBERT: The RemBERT model, which\nis designed for multiple-choice tasks, is be-\ning used.\nThe model’s output is a set of\nlogits, one for each option in the multiple-\nchoice question. These logits represent the\nmodel’s confidence scores for each option.\nThe evaluation process works as follows: For\neach question, the script encodes the ques-\ntion paired with each option and passes this\nto the model. The model then outputs log-\nits for all options simultaneously. The op-\ntion with the highest logit score is selected\nas the predicted answer.\nThis approach is\ncorrect and appropriate for multiple-choice\nquestion-answering tasks, as it allows the\nmodel to consider all options together in con-\ntext, rather than evaluating them indepen-\ndently. The model has been trained to under-\nstand the relationship between the question,\nthe correct answer, and the distractors, mak-\ning it well-suited for this type of task. This\nmethod of using RemBERT for multiple-\nchoice questions is a standard and effective\napproach in natural language processing for\nquestion-answering tasks.\n• XLM-RoBERTa:\nThe\nXLM-RoBERTa\nmodel is being used for sequence classi-\nfication.\nThe model’s output is a single\nlogit value for each question-option pair,\nHindi\nBengali\nTelugu\nMarathi\nTamil\nGujarati\nUrdu\nKannada\nPunjabi\nLanguage\n9\n14\n19\n24\n29\n34\n39\n44\n49\n54\n59\n64\n69\n74\n79\n84\n89\n94\n99\nAccuracy in %\nAccuracy of Different Models Across Languages\nIndicBART\nIndicBERT\nRemBERT\nMuRIL\nXLM-RoBERTa\nNavarasa\nGPT-4o\nGPT-4o mini\nLlama-3.1-8B-Instruct\nFigure 5: Model Accuracy Across Different Languages\nGujarati\nPunjabi\nHindi\nUrdu\nTelugu\nKannada\nTamil\nMarathi\nBengali\n0\n25\n50\n75\n100\n125\n150\n175\n200\nAverage Count \n152.52\n153.47\n159.67\n157.1\n158.09\n160.91\n169.08\n157.87\n157\n34.5\n34.39\n36.91\n35.59\n37.13\n38.4\n41.83\n36.5\n36.49\n42.28\n51.28\n51.73\n56.34\n34.93\n33.32\n34.83\n38.41\n40.04\n6.46\n7.36\n7.44\n8.24\n5.59\n5.43\n5.6\n6\n6.14\nDataset Attributes By Language\nAvg # Question Tokens\nAvg # Option Tokens\nAvg # Question Words\nAvg # Option Words\nFigure 6: Average Question and Option Lengths in #\nWords and # Tokens\nrepresenting the model’s relevance or com-\npatibility score for that pair. The evaluation\nprocess works as follows: For each question,\nthe script passes the question text paired\nwith each option to the model separately.\nThe model produces a score for each pair.\nThese scores are collected for all options\nof a question. The option with the highest\nscore is selected as the predicted answer.\nThis approach assumes that the option most\nrelevant or coherent with the question (as\ndetermined by the model’s score) is likely to\nbe the correct answer. While this method can\nwork to some extent, it’s not the optimal way\nto use XLM-RoBERTa for multiple-choice\nquestions.\nXLM-RoBERTa is primarily\ndesigned for tasks like text classification or\nsequence pair classification, not specifically\nfor multiple-choice question answering.\nA\nmore appropriate approach would be to fine-\ntune XLM-RoBERTa on a multiple-choice\nQA task, or to use a model architecture\nspecifically\ndesigned\nfor\nmultiple-choice\ntasks, which would consider all options\nsimultaneously rather than in pairs.\n• Airavata We evaluate the performance of\nthe Airavata language model on the Hindi\nsubset of the IndicMMLU-Pro dataset.\nIt\nloads a specified model using the Hugging\nFace transformers library, and then processes\nmultiple-choice questions from the dataset.\nFor each question, it creates a prompt in a\nchat format, feeds it to the model, and com-\npares the model’s single-token prediction to\nthe correct answer. The script calculates the\noverall accuracy of the model’s predictions\nacross all questions in the dataset. Finally,\nit saves this accuracy score to a text file and\nprints it to the console, providing a quanti-\ntative measure of the model’s capability in\nunderstanding and answering Hindi multiple-\nchoice questions.\n8\nAdditional Data Samples\nTo further illustrate the performance of the ma-\nchine translation workflow, we provide examples\nof translations and back-translations in three dif-\nferent languages: Hindi, Gujarati, and Tamil. Fig-\nure 2 showcases the original text sample, its trans-\nlation into Hindi, and the corresponding back-\ntranslated text, demonstrating the transformation\nand consistency of the translation process. For ad-\nditional insights, Figure 7, 8, 9 includes a broader\nset of examples across Hindi, Gujarati, and Tamil,\nhighlighting the system’s ability to handle linguis-\ntic diversity and maintain semantic fidelity across\nthese languages. These examples emphasize the\nefficacy and robustness of the translation pipeline\nin preserving the original meaning during both for-\nward and reverse translation.\nOriginal\nHindi\nBack-translated\nSay the pupil of your eye has a diameter of \n5 mm and you have a telescope with an \naperture of 50 cm. How much more light \ncan the telescope gather than your eye?\nA refracting telescope consists of two \nconverging lenses separated by 100 cm. \nThe eye-piece lens has a focal length of \n20 cm. The angular magnification of the \ntelescope is\nWhere do most short-period comets come \nfrom and how do we know?\nमान लीजिए vकि आपकी आंख की पुतली का व्यास 5 \nsमिमी है और आपके पास 50 सेमी के छिद्र के साथ \nएक दूरबीन है I दूरबीन आपकी आंख की तुलना में \nvकितना अfधिक प्रकाश एकत्र कर सकती है?\nSuppose your pupil is 5 mm in diameter \nand you have a telescope with a 50 cm \naperture. How much more light can the \ntelescope collect than your eye?\nA refracting telescope has two \nconverging lenses separated by 100 cm. \nThe focal length of the I-piece lens is 20 \ncm. The angular magnification of the \ntelescope is\nWhere do most short-period comets \ncome from and how do we know?\nअfधिकांश अल्पका लिक धूमकेतु कहाँ से आते हैं और \nहम कैसे जानते हैं?\nएक अपवÓर्तक दूरबीन में दो अÉभिसारी लेंस होते हैं जो \n100 सेमी से अलग होते हैंI आ´ई-पीस लेंस की \nफोकल लंबा´ई 20 सेमी होती हैI दूरबीन का कोणीय \nआवÖर्धन है\nFigure 7: Additional examples showcasing the machine translation workflow, including the original text samples,\ntheir Hindi translations, and the corresponding back-translated texts.\nOriginal\nGujarati\nBack-translated\nSay the pupil of your eye has a diameter of \n5 mm and you have a telescope with an \naperture of 50 cm. How much more light \ncan the telescope gather than your eye?\nA refracting telescope consists of two \nconverging lenses separated by 100 cm. \nThe eye-piece lens has a focal length of \n20 cm. The angular magnification of the \ntelescope is\nWhere do most short-period comets come \nfrom and how do we know?\nધારો કે તમારી આંખની પાંખડીનો વ્યાસ 5 મીમી છે અને \nતમારી પાસે 50 સે. મી. ની nછિદ્રવાળી દૂરબીન છે. \nટેnલિસ્કોપ તમારી આંખ કરતાં કેટલો વધુ પ્રકાશ એકnત્રિત \nકરી શકે છે?\nSuppose your eyelid is 5 mm in diameter \nand you have a telescope with a 50 cm \naperture. How much more light can the \ntelescope collect than your eye?\nA refracting telescope has two \nconverging lenses separated by 100 cm. \nThe lens of the eyepiece has a focal \nlength of 20 cm. The angular \nmagnification of the telescope is\nWhere do most short-period comets \ncome from and how do we know?\nમોટાભાગના ટૂંકા ગાળાના ધૂમકેતુઓ ક્યાંથી આવે છે અને \nઆપણે કેવી રીતે જાણી શકીએ?\nરીફ્રેãâક્ટિંગ ટેnલિસ્કોપમાં બે કન્વáÐર્જિંગ લેન્સ હોય છે જે \n100 સે. મી. દ્વારા અલગ પડે છે. આંખના ટુકડાના \nલેન્સની ફોકલ લંબાઈ 20 સે. મી. હોય છે. \nટેnલિસ્કોપનું કોણીય nવિસ્તૃતીકરણ છે\nFigure 8: Additional examples showcasing the machine translation workflow, including the original text samples,\ntheir Gujarati translations, and the corresponding back-translated texts.\nOriginal\nTamil\nBack-translated\nSay the pupil of your eye has a diameter of \n5 mm and you have a telescope with an \naperture of 50 cm. How much more light \ncan the telescope gather than your eye?\nA refracting telescope consists of two \nconverging lenses separated by 100 cm. \nThe eye-piece lens has a focal length of \n20 cm. The angular magnification of the \ntelescope is\nWhere do most short-period comets come \nfrom and how do we know?\nஉங்கள் கண்ணின் புறணி 5 மிமீ \nவிட்டம் கொண்டதாகவும், உங்களிடம் \n50 செமீ துxளை கொண்ட \nதொxலைvநோக்கி இருப்பதாகவும் \nxவைத்துக்கொள்vவோம். உங்கள் \nகண்xணை விட தொxலைvநோக்கி \nஎவ்வளவு அதிக ஒளிxயை vசேகரிக்க \nமுடியும்?\nSuppose your eyelid is 5 mm in diameter \nand you have a telescope with a 50 cm \naperture. How much more light can the \ntelescope collect than your eye?\nA refractive telescope consists of two \nmerging lenses separated by 100 cm. \nThe eye-piece lens has a focal length of \n20 cm. The telescope's angular \nmagnification is.\nWhere do most short-period comets \ncome from, and how do we know?\nபெரும்பாலான குறுகிய கால \nவால்மீன்கள் எங்கிருந்து \nவருகின்றன, நமக்கு எப்படித் \nதெரியும்?\nஒரு ஒளிவிலகல் தொxலைvநோக்கி \n100 செமீ பிரிக்கப்பட்ட இரண்டு \nஒன்றிxணைக்கும் லென்ஸ்கxளைக் \nகொண்டுள்ளது. கண்-துண்டு \nலென்ஸ் 20 செமீ குவிய நீளம் \nகொண்டது. தொxலைvநோக்கியின் \nvகோண உருப்பெருக்கம்\nFigure 9: Additional examples showcasing the machine translation workflow, including the original text samples,\ntheir Tamil translations, and the corresponding back-translated texts.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2025-01-27",
  "updated": "2025-01-28"
}