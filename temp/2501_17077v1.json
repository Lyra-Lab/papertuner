{
  "id": "http://arxiv.org/abs/2501.17077v1",
  "title": "Induced Modularity and Community Detection for Functionally Interpretable Reinforcement Learning",
  "authors": [
    "Anna Soligo",
    "Pietro Ferraro",
    "David Boyle"
  ],
  "abstract": "Interpretability in reinforcement learning is crucial for ensuring AI systems\nalign with human values and fulfill the diverse related requirements including\nsafety, robustness and fairness. Building on recent approaches to encouraging\nsparsity and locality in neural networks, we demonstrate how the penalisation\nof non-local weights leads to the emergence of functionally independent modules\nin the policy network of a reinforcement learning agent. To illustrate this, we\ndemonstrate the emergence of two parallel modules for assessment of movement\nalong the X and Y axes in a stochastic Minigrid environment. Through the novel\napplication of community detection algorithms, we show how these modules can be\nautomatically identified and their functional roles verified through direct\nintervention on the network weights prior to inference. This establishes a\nscalable framework for reinforcement learning interpretability through\nfunctional modularity, addressing challenges regarding the trade-off between\ncompleteness and cognitive tractability of reinforcement learning explanations.",
  "text": "Induced Modularity and Community Detection for Functionally\nInterpretable Reinforcement Learning\nAnna Soligo\nPietro Ferraro\nDavid Boyle\nImperial College London\nAbstract\nInterpretability in reinforcement learning is\ncrucial for ensuring AI systems align with hu-\nman values and fulfill the diverse related re-\nquirements including safety, robustness and\nfairness.\nBuilding on recent approaches to\nencouraging sparsity and locality in neural\nnetworks, we demonstrate how the penalisa-\ntion of non-local weights leads to the emer-\ngence of functionally independent modules in\nthe policy network of a reinforcement learn-\ning agent. To illustrate this, we demonstrate\nthe emergence of two parallel modules for as-\nsessment of movement along the X and Y\naxes in a stochastic Minigrid environment.\nThrough the novel application of commu-\nnity detection algorithms, we show how these\nmodules can be automatically identified and\ntheir functional roles verified through direct\nintervention on the network weights prior to\ninference. This establishes a scalable frame-\nwork for reinforcement learning interpretabil-\nity through functional modularity, addressing\nchallenges regarding the trade-off between\ncompleteness and cognitive tractability of re-\ninforcement learning explanations.\n1\nINTRODUCTION\nDeep reinforcement learning (RL) has emerged as a\npowerful approach for improving performance in com-\nplex decision making domains. Learning policies di-\nrectly from interactions can offer increased flexibility\nand improved performance, whilst avoiding the chal-\nlenges of designing effective cost functions and plan-\nning strategies faced by classical model-based control\napproaches (Song et al., 2023). A growing body of re-\nsearch is demonstrating the potential for RL to have\npositive impacts in diverse real-world domains, from\nbattery manufacturing (Lu et al., 2020) to the design\nof medical treatment regimes (Coronato et al., 2020):\napplications which directly implicate on critical issues\nsuch as climate-change and human-health. However,\nsuch broad implications also introduce societal risks,\nand the application of RL raises wide-ranging concerns\nrelated to topics of safety, reliability, privacy and bias,\namong others. For this reason, it becomes crucial that\nthe behaviour of RL agents can be properly charac-\nterised, to the extent that it can be reasonably veri-\nfied that their impacts will positively align with human\nvalues. As reflected in the EU’s AI ethics guidelines:\nsystems must allow for human oversight, accountabil-\nity and transparency (European Comission and High-\nLevel Expert Group on AI, 2019).\nHowever, there are fundamental challenges to achiev-\ning this, and current RL systems rarely afford suffi-\ncient levels of interpretability to fulfill these three re-\nquirements. Central to this is the ambiguity regard-\ning what constitutes an acceptable ‘explanation’ of a\nmodel. A balance must be struck between the scope\nand detail of an explanation and its suitability for a\nhuman audience. Human understanding of an expla-\nnation must be considered to ensure its utility, whilst\nalso avoiding incompleteness that risks leaving room\nfor subjective interpretations. Lipton (2016) consid-\ners this concept of a tractable explanation under the\nterm of ‘simulatability’: the idea that a human can, in\nreasonable time, take a model input and explanation\nand predict its output.\nHe pairs this with the par-\nallel notion of ‘decomposability’, which suggests that\nthe constituent components of a model should be in-\ndividually interpretable. These notions are echoed by\nDoshi-Velez and Kim (2017) through the concept of\n‘cognitive chunks’, emphasising their relevance to hu-\nman processing.\nQuestions arise regarding how the\nsize, number, organisation and interactions of these\nrelate to whether a model or explanation can be con-\nsidered understandable.\nIn this work we address these challenges by taking a\nmodular perspective to interpretability, drawing inspi-\narXiv:2501.17077v1  [cs.LG]  28 Jan 2025\nInduced Modularity and Community Detection for Functionally Interpretable RL\nration from cognitive science. Cognition is frequently\nviewed as consisting of semi-encapsulated modules\nwith distinct functions (Fodor, 1985; Sternberg, 2011)\nand modular processing is prevalent in both conscious\ndecision making and the physical structures of the\nbrain (Eppe et al., 2022; Gazzaniga et al., 2018). Given\nthe pervasive need to consider human comprehension\nof model explanations, this suggests interpretability\nthrough a modular lens is a promising approach to\nunderstanding RL decision making at a tractable level\nof abstraction.\n1.1\nContributions\nLeveraging this concept of interpretability at the level\nof functional modules, our work makes the following\ncontributions:\n• We build on recent bio-inspired algorithms for en-\ncouraging locality in neural networks (Achterberg\net al., 2023; Liu et al., 2023; Margalit et al., 2024)\nand demonstrate that the penalisation of non-\nlocal weights encourages the emergence of semi-\nencapsulated functional modules in the policy net-\nwork of an RL agent: a level of decomposition\nwhich we suggest strongly aligns with human de-\ncision making frameworks.\n• We show how the application of community de-\ntection methods, based on the Louvain algorithm\nand spectral analysis of the adjacency matrix,\nallows for the automatic detection of modules\nwithin neural networks.\nThis demonstrates the\npotential to automate and scale modular inter-\npretability.\n• Finally, drawing on techniques from mechanistic\ninterpretability, we demonstrate how direct inter-\nvention on network weights prior to inference en-\nables the characterisation of detected modules, of-\nfering a verifiable interpretation of their function-\nality.\nWe show interactive graphs of modular RL networks\nand animations of their functional behavior on the\nproject page:\nhttps://sites.google.com/view/\nfunctionally-interpretable-rl\n2\nBACKGROUND AND RELATED\nWORKS\nFollowing Glanois et al. (2024), and to avoid confu-\nsion arising from the inconsistent use of terms in the\nliterature, we define interpretability as a passive model\nquality denoting the extent to which a model’s inner\nworkings can be examined and understood. We distin-\nguish this from explainability, which we define as an\nexternal understanding of model behaviour that arises\nfrom active, generally post-hoc, attempts at explain-\ning the decision making process. Interpretability and\nexplainability thus present two different paths to ob-\ntaining information that can be used to generate ex-\nplanations for model behaviour.\nAlthough post-hoc\nexplainability methods can offer improved flexibility\nand scalability compared to intrinsic interpretability\napproaches, their lack of grounding in model inter-\nnals raises concerns regarding the potentially mislead-\ning and subjective nature of the resulting explanations\n(Adebayo et al., 2018; Atrey et al., 2019).\nIn this work we take a direct interpretability approach\nby aiming to learn an intrinsically more interpretable\nmodel architecture. Decision trees are a prevalent ex-\nisting approach to achieving this and can be used to\nlearn Q-values or policies. Although their classical im-\nplementation using boolean decision variables is not\ndifferentiable, recent work on ‘soft decision trees’ en-\nables efficient reinforcement learning through gradient\ndescent (Silva et al., 2020).\nSymbolic equations of-\nfer an alternative intrinsically interpretable architec-\nture, and diverse methods exist to generate effective\nRL policies in this form. For instance, using genetic\nprogramming to efficiently search a space of function\ntrees (Hein et al., 2017) or learning a recurrent neural\nnetwork (RNN) to directly generate policy equations\n(Landajuela et al., 2021). Beyond mathematical oper-\nators, policies have been learnt as weighted combina-\ntions of first order logic rules, from which natural lan-\nguage explanations can be extracted (Jiang and Luo,\n2019).\nThese approaches demonstrate a reliance on making\nfundamental model architecture changes to improve\ninterpretability, which leads to issues regarding scala-\nbility and performance. Although the components of\nthe described methods, such as formulaic polices, can\ntheoretically scale to complex scenarios, this rapidly\nbecomes computationally prohibitive, even when clas-\nsically discrete approaches are relaxed to be continuous\nor an indirect policy distillation approach is adopted\n(Glanois et al., 2024). Moreover, scaling these archi-\ntectures to improve performance compromises their\ninterpretability: a decision tree with an intractable\nnumber of nodes, for example, may be no more inter-\npretable than a network with an intractable number\nof neurons.\nTangentially, the field of mechanistic interpretability\ntakes a bottom up approach to reverse-engineering net-\nworks, particularly large language models.\nNotably\nthis can involve the identification of ‘circuits’: compu-\ntational sub graphs within a network which perform\nAnna Soligo, Pietro Ferraro, David Boyle\nspecific tasks (Wang et al., 2022). We share this task-\nlevel focus, but rather than attempting interpretability\nat the level of computations, as per the aforementioned\napproaches, we instead characterise higher-level collec-\ntions of neurons.\nThese collections of neurons, or ‘modules’, can be con-\nsidered in terms of both their topological and func-\ntional modularity.\nTopological (or structural) mod-\nules are communities of neurons that are densely in-\nterconnected by network weights, while being sparsely\nconnected to external neurons. Conversely, functional\nmodules are sets of neurons which together perform\nspecific tasks with a level of independence from the\nremainder of the network. A number of neural net-\nwork architectures exhibit principles of modularity. At\nthe data level, manual or learnt decomposition of the\ndomain can parallelise processing, for example by par-\ntitioning regions of an image to be independently pro-\ncessed by CNNs (Zhang et al., 2014).\nWithin net-\nworks, sequential structural modularity can be seen\nin the repeated-block structure of transformers, which\nis additionally parallelised in a functional manner in\nmulti-modal architectures (He et al., 2021).\nFor the purpose of interpretability, we are interested in\nfunctional modularity, for which topological modular-\nity is a necessary but not sufficient condition (Amer\nand Maul, 2019). This is approached by policy tree\napproaches to hierarchical RL, which decompose deci-\nsion making into sub policies (Pateria et al., 2021).\nHowever, these are limited to a predefined level of\ndecomposition and only afford interpretability when\ndiscernible sub-behaviours, such as motor primitives\n(Merel et al., 2018), are explicitly learnt based on prior\nknowledge.\nFunctional modularity in the brain arises alongside its\n‘small-world’ architecture: a combination of high clus-\ntering and short average path length hypothesised to\nhave evolved to satisfy spatial and energy constraints\n(Margalit et al., 2024). Recently, several works have\ninvestigated the impacts of applying analogous con-\nstraints to neural networks. Achterberg et al. (2023)\nspatially embed an RNN and penalise its connections\nrelative to their length, demonstrating high energy effi-\nciency and a level of functional clustering in a one-step\ninference task.\nConcurrently, Margalit et al. (2024)\nintroduced a training loss to encourage correlation be-\ntween local activations and applied this to convolu-\ntional layers projected onto simulated cortical sheets.\nWhile these studies apply bio-inspired training for\nthe purpose of advancing neuro-scientific understand-\ning and modelling, Liu et al. (2023) employ similar\ntechniques to improve the interpretability of neural\nnetwork visualisations. They demonstrate that their\n‘brain inspired modular training’ approach, which cou-\nples length-relative weight penalisation with neuron\nrelocation during training, reveals structures within\ntasks such as regression to symbolic mathematical for-\nmulae.\nBuilding on these findings, we extend this\n‘small-world’ approach to an RL context to encourage\nfunctional modularity.\nWe further propose methods\nfor automatic detection and characterisation of the re-\nsulting modules, enabling scalable interpretability in a\ndecision making context.\n3\nMETHODS\n3.1\nProximal Policy Optimisation (PPO)\nWe use the standard RL formalism for an agent in-\nteracting with a fully observable environment.\nThe\nenvironment is described by the set of states S, set of\ndiscrete actions A, reward function r : S × A →R,\ndiscount factor γ ∈[0, 1], and transition probabilities\np(st+1|st, at).\nThe policy πθ : S →A generates a\nprobability distribution over actions, from which an\naction at is sampled at each time step t. Each action\nresults in a reward rt = r(st, at) and an updated envi-\nronment state st+1. The return of a state-action pair\nis defined as the discounted sum of future rewards,\nRt = P∞\ni=t γi−tri, and the expected return is given by\nthe action-value function Qπ(st, at) = E[Rt|st, at].\nWe apply PPO (Schulman et al., 2017) due to its sta-\nbility and simplicity.\nAs a policy-gradient method,\nPPO directly learns the parameterised policy function,\nthe actor, by estimating the gradient of the expected\nreturn with the respect to the policy parameters. The\ngeneralised advantage estimator (GAE) is used to de-\ntermine the advantage of taking an action at over alter-\nnative actions in state st. This advantage is defined as\nA(st, at) = Q(st, at)−V (st), where V π(st) = E[Rt|st]:\nthe state-value function approximated by a critic net-\nwork. The GAE value is clipped to constrain the mag-\nnitude of the policy change at each learning step and\nprevent large changes which may lead to performance\ncollapse.\n3.2\nSpatially Aware Regularisation\nL1 regularisation encourages sparsity by penalising the\nabsolute values of the model parameters.\nFollowing\nAchterberg et al. (2023); Liu et al. (2023), we extend\nthis to encourage local connectivity by projecting the\nneural network into Euclidian space and scaling L1\nweight penalties by the distance between the neurons\nthey connect.\nFor a network with L weight layers and L + 1 neu-\nron layers, we denote neuron layers as Nl where\nl\n∈{0, 1, . . . , L} and weight layers as Wl where\nInduced Modularity and Community Detection for Functionally Interpretable RL\nl ∈{1, . . . , L}.\nFor our fully connected networks,\nWl ∈RNl−1×Nl and each weight wij\nl\nconnects the ith\nneuron in Nl−1 with the jth neuron in Nl. The di-\nmension of the neuron layer Nl is nl, such that n0 and\nnL represent the dimensions of the input features and\ndiscrete action space respectively.\nAll neurons ni\nl Nl share a y coordinate: yi\nl := yl, i ∈\n{1, . . . nl}.\nInitial x coordinates are defined as uni-\nformly spaced along this axis such that xi\nl =\ni\nnl . A\nweighting factor, λcc, scales the connection cost loss in\neach layer to give the total network connection cost,\nLcc defined in Equation 1.\nLcc = λcc\nL\nX\nl=1\nnl−1\nX\ni=1\nnl\nX\nj=1\n||pi\nl−1 −pj\nl |||wij\nl |\n(1)\nwhere\npi\nl = [xi\nl, yl]\nNote that if all neuron pairs are equidistant (i.e.,\n||pi\nl−1 −pj\nl || is a constant) Lcc reduces to L1 sparsity.\nTo further encourage locality, neurons positions are\nsystematically relocated during training in a manner\nthat minimises the total connection cost Lcc of the net-\nwork, as proposed by Liu et al. (2023). At a fixed up-\ndate interval and for every hidden layer, the weighted\ndegree of each neuron is calculated as the sum of its\nincoming and outgoing weights:\nw(n) =\nX\n|w(n)in| +\nX\n|w(n)out|\n(2)\nThe k = 10 neurons with highest w(n) are selected for\nposition optimization. For each candidate neuron nc,\nthe relocation algorithm:\n1. Computes the baseline connection cost CC0 for\nthe current configuration, based on Equation 1.\n2. For every other neuron ni in the same layer as\nnc, calculates an updated connection cost CCi ob-\ntained by swapping the positions of nc and ni\n3. Identifies\nthe\nswap\npartner\nns\nthat\nyields\nmin(CCi)\n4. If min(CCi) < CC0, executes the position swap\nbetween nc and ns\n3.3\nMinigrid\nWe conduct experiments using the dynamic obstacles\nMinigrid environment shown in Figure 1 (Chevalier-\nBoisvert et al., 2023). This consists of an agent, goal\nand three ‘ball’ obstacle entities randomly initialised in\na 4x4 grid, which is encoded into a symbolic observa-\ntion of entity coordinates relative to the agent. The ac-\ntion space consists of left, right, up and down steps and\nFigure 1: Example Environment Frames Showing the\nAgent (Red), Dynamic Obstacles (Blue) and Goal\n(Green).\neach ball takes one random step per agent step. Fol-\nlowing the NAVIX implementation (Pignatelli et al.,\n2024), a Markov reward function offers a sparse reward\nof 1 when the goal is reached and 0 for all other steps.\nThe episode terminates when the maximum number\nof steps (100) is reached, or the agent collides with an\nobstacle.\n4\nEXPERIMENTS\nWe implement the PPO actor and critic as MLPs\nwith the architecture described in Table 1.\nHyper-\nparameters were selected based on a grid search in the\nregion of the baseline presented in the NAVIX Mini-\ngrid implementation (Pignatelli et al., 2024).\nTable 1: PPO Hyperparameters\nArchitecture\nHidden Size\n64\nNumber of Layers\n2\nTraining\nParallel Environments\n16\nSteps per Environment\n128\nMinibatches\n8\nEpochs\n16\nLearning Rate\n1e-3\nGAE λ\n0.99\nClip ϵ\n0.2\nEntropy Coefficient\n0.02\nValue Function Coefficient\n0.5\nMax Gradient Norm\n0.5\nThe PPO agent and environment are implemented in\nJAX (Bradbury et al., 2018). All agents were trained\nfor 5M environment frames on a 24GB Nvidia 4090\nGPU, taking between 135 and 170 seconds per agent.\nWe observe that implementing the CC loss and neuron\nrelocation results in a 23% increase in total train time,\nwhich, as shown in Figure 2, is primarily due to the\nAnna Soligo, Pietro Ferraro, David Boyle\nFigure 2: The Impact of our Bio-inspired Training Pro-\ntocol on Total Training Time, Relative to Vanilla PPO\nand PPO with L1 Sparsity Baselines.\nimplementation of the CC loss.\n4.1\nEmergent Modularity\nFigure 3 shows the emergence of two distinct mod-\nules within the actor network of a dynamic-obstacles\nMinigrid agent. As the connection cost weighting fac-\ntor (λCC) is increased, independence emerges in the\nsecond and third weight layers, with feature-sharing\npersisting in the first. The neuron relocation results\nin the input features and output actions reordering\nin a manner that reflects their relevance, with feature\nx and y coordinates positioned on the same sides as\nmovements on the x and y axes respectively. Figure\n4 shows the necessity of both the connection cost loss\nand neuron relocation to achieve this result. When L1\nsparsity is applied in isolation or with only one of these\nmethods, the same visual clarity is not achieved.\n4.2\nModule Detection\nModularity within a network can be quantified as\nthe ratio of intra-community links to inter-community\nlinks.\nQ =\n1\n2m\nX\nij\n\u0012\nAij −γ kikj\n2m\n\u0013\nδ(ci, cj)\n(3)\nwhere m is the sum of all edge weights (m = P\nlij wij\nl ),\nA is the adjacency matrix of the network, ki is the\nweighted degree of node i in a null model of the net-\nwork, and δ(ci, cj) is a binary variable with a value of\n1 when i and j belong to the same community. This\nequation forms the basis of the heuristic Louvain ap-\nproach to community detection, which iteratively opti-\nmises Q through hierarchical local node reassignments\n(Blondel et al., 2008).\nThe resolution parameter γ\ninfluences the size of the detected communities by ad-\njusting the level of connection considered to constitute\nnodes within the same community.\nWe apply Louvain clustering to the adjacency matrices\nof the actor networks in order to automate the identi-\nfication of modules and quantify the networks overall\nmodularity. Figure 5a shows the increase in modular-\nity (Q) as the connection cost increases. Empirically,\nwe observe that Q values in the region of 0.4 and above\ncorrespond to a visual separation of two modules in the\nnetwork graph. This begins to occur from λcc values\nof 0.0004, and becomes consistent across all random\nseeds for λcc values greater than 0.0014. With a λcc\nof 0.0014 the mean modularity represents a 340% in-\ncrease on the baseline value of 0.12 observed with a\nλcc of 0. The induced sparsity does, however, impact\non performance, as shown in Figure 5b. The decrease\nin return ranges from 13.7% when modules initially\nemerge in some seeds to a mean of 20.6% at the stage\nwhere modularity occurs consistently. The reduction\nin performance plateaus between λcc values of 0.0016\nand 0.0026, corresponding to plateaus in both the con-\nnection cost loss and the modularity value. This sug-\ngests that there is a maximum level of sparsity that\ncan be achieved without a collapse in performance, as\nbegins to occur at λcc = 0.0028.\nWhile there is an impact on performance when con-\nsidering the complete model, the sparsity reduces the\nproportion of significant weights in the model, conse-\nquently allowing for significant pruning. We find that\nwe can remove 85% of the sparse models’ parameters\nwithout reducing mean performance, compared to 10%\nin the vanilla PPO-Clip implementation. This may of-\nfer further interpretability benefits in addition to sig-\nnificantly reduced computational overhead during in-\nference, as discussed in Appendix B.\nClustering results of the Louvain method are shown\nin Figure 6 for 4 randomly selected networks with λcc\nvalues between 0.0004 and 0.0008.\nThe Louvain al-\ngorithm (Equation 3) does not rely on a predefined\nnumber of communities, which offers advantages for\nscalability to module detection within large networks\nwith an unknown number of modules. However, we ob-\nserve that the resulting modules do not strongly align\nwith the visually apparent modularity of the networks.\nConsequently, we also apply a spectral clustering ap-\nproach to community detection (von Luxburg, 2007).\nWhile spectral clustering techniques are commonly ap-\nplied to forms of the graph Laplacian, E. Crisostomi\nand Shorten (2011) demonstrate how clustering of the\nvalues of the second eigenvector can be used to iden-\ntify communities based on the transition matrix of a\nroad network. Following this, we calculate the second\neigenvector of the adjacency matrix of the policy net-\nwork and demonstrate that this shows clustering corre-\nsponding to the network modules. Figure 7 shows the\nresult of classifying nodes by partitioning the eigen-\nvector at its largest value separation and shows clear\nalignment with the visually apparent modularity.\nInduced Modularity and Community Detection for Functionally Interpretable RL\n0\n0.0002\n0.0006\n0.001\n0.002\n154\n1756\nCC λ\nSeed\n6163\nFigure 3: Policy Network Plots Showing the Emergence of Parallel X and Y Processing Modules as the Connection\nCost Weighting is Increased for 3 Random Seeds (see Appendix A for Further Examples). Positive and Negative\nWeights are Shown in Blue and Red Respectively.\nB1X  GX  GY  B1Y  B2X  B3Y  B3X  B2Y\nB1X  GX  GY  B1Y  B2X  B3Y  B3X  B2Y\nB1X  B3X  GY  B2Y  B1Y  B2X  B3Y  GX\nGX  GY  B2X  B1X  B1Y  B3X  B3Y  B2X\nUP ​ RIGHT  ​DOWN  ​LEFT\nDOWN ​RIGHT ​LEFT ​UP\nLEFT ​ RIGHT ​ DOWN  ​UP\n(a) L1 Sparsity only\n(b) L1 Sparsity with \nRelocation\n(c) Distance-​weighted \nSparsity\n(d) Distance-​weighted \nSparsity with Relocation\nUP ​ RIGHT  ​DOWN  ​LEFT\nFigure 4: The Impact of Sparsity, Distance Weighting and Neuron Relocation on Visual Modularity.\n(a)\n(b)\nFigure 5: The Impact of Increasing Connection Cost on the Mean and Standard Deviation of (a) Modularity\n(Q) and (b) Return over 10 Random Seeds.\nAnna Soligo, Pietro Ferraro, David Boyle\nCC λ = 0.0004\nCC λ = 0.0006\nCC λ = 0.0008\nFigure 6: Policy Networks with Louvain Community Detection (γ = 1) .\nCC λ = 0.0004\nCC λ = 0.0006\nCC λ = 0.0008\nFigure 7: Policy Networks with Community Detection Based on the Second Eigenvector.\nIn this context, the ability to enforce a desired num-\nber of modules afforded by the eigenvector approach\noffers improves performance over the Louvain method,\nwhich subdivides the two visually apparent modules.\nWe hypothesize that this subdivision arises from the\nlack of intra-layer connections in a neural network,\nwhich disrupts the density of the modules and results\nin a mismatch with the null-model of the network (the\nrandom graph model used as a baseline for quanti-\nfying the significance of community structures). The\ndevelopment of neural network specific module detec-\ntion approaches is thus a valuable direction for future\nwork, particularly when considering complex applica-\ntions where the ability to detect large, unknown num-\nbers of modules is desirable.\n4.3\nModule Intervention\nThe automatic classification of neurons into commu-\nnities enables direct intervention on their behaviour.\nWe achieve this through modification of network pa-\nrameters prior to inference, which we demonstrate on\nthe eigenvector detected modules of the policy network\nshown in Figure 8.\nTable 2 gives statistics on the actions taken and their\noutcomes (Failure, Success or Continuation of the\nepisode) for three scenarios: the original network and\ntwo modified versions where all weights and biases in\nthe targeted community are masked with a value of\nRIGHT ​ ​ ​ ​ ​ ​ ​ LEFT ​ ​ ​ ​ ​ ​ ​ DOWN  ​ ​ ​ ​ ​ ​ ​UP\nGX  ​  ​ B3X ​   ​ B2X ​   ​ B1X ​   ​B1Y ​   ​ B2Y ​  ​ B3Y ​  ​ GY\nFigure 8: Policy Network (λcc = 0.0007) with Eigen-\nvector Based Detection of Community 1 (Red) and\nCommunity 2 (Green).\n-5. This effectively disables the module and the re-\nsults show that intervening on community 2 eliminates\nup and down actions, while the same intervention on\ncommunity 1 eliminates left and right. Notably, while\nthe success rate drops significantly when a module is\nmasked, the proportion of actions resulting in failures\ndoes not increase. This demonstrates that we are able\nto disable a module while retaining the ability of the\nother to select the optimal action from its correspond-\ning outputs.\nIn this example, the module intervention results intu-\nInduced Modularity and Community Detection for Functionally Interpretable RL\nTable 2:\nAction and Action-outcome Distributions\nover 1000 Episodes for the Network in Figure 8.\nOriginal network\nReturns: 0.688\nProp.\nFail\nSucc.\nCont\nup\n0.24\n0.09\n0.21\n0.7\nright\n0.25\n0.09\n0.24\n0.66\ndown\n0.24\n0.09\n0.19\n0.72\nleft\n0.26\n0.11\n0.21\n0.68\nCommunity 1 masked\nReturns: 0.151\nProp.\nFail\nSucc.\nCont.\nup\n0.49\n0.08\n0.01\n0.91\nright\n0\n-\n-\n-\ndown\n0.51\n0.08\n0.01\n0.91\nleft\n0\n-\n-\n-\nCommunity 2 masked\nReturns: 0.176\nProp.\nFail\nSucc.\nCont.\nup\n0\n-\n-\n-\nright\n0.53\n0.08\n0.02\n0.91\ndown\n0\n-\n-\n-\nleft\n0.47\n0.08\n0.02\n0.9\nitively align with the expected policy behaviour given\nthe task and observed structure of the network visual-\nisations. Moving beyond potentially subjective visual\ninterpretations, it provides an objective validation of\nmodule functionality, which becomes increasingly crit-\nical when considering scalability. The ability to auto-\nmatically detect and characterise modules enables the\ninterpretation functional modules in scenarios where\nvisual identification is not feasible due to network size\nor complexity.\n5\nCONCLUSIONS\nOur results demonstrate that functional modularity\ncan be explicitly encouraged in reinforcement learn-\ning through the use training modifications to promote\nsparsity and locality of connections. To enable scal-\nability to complex decision making applications and\nmodel architectures, we propose methods for the auto-\nmatic detection and characterisation of modules. This\nfunctional decomposition enhances interpretability by\ndecomposing the network into distinct, meaningful\nunits which provide insights into the agent’s decision-\nmaking process while maintaining a level of abstrac-\ntion that aligns with human-level understanding.\nSeveral further areas of investigation offer potential to\nimprove the performance and utility of this approach.\nOur approaches to module detection present oppor-\ntunities for refinement, particularly considering scal-\nability to more complex applications.\nThe Louvain\napproach could be modified to account for the sequen-\ntially constrained architecture of neural networks or\nthe spectral clustering approach could be extended\nto multiple eigenvectors to better capture higher di-\nmensional clusters (von Luxburg, 2007). Alternative\nmethods also merit being explored, such as grouping\nneurons based on the concurrency of their activations,\nin order to directly capture functional as opposed to\nstructural modularity. In this work, we characterise\nmodules using parameter modification prior to infer-\nence due to its ease of implementation. However, mod-\nification of activation values may offer greater insights,\nparticularly for models exhibiting more complex struc-\ntural modularity, as it offers the potential to intervene\non a module while preserving a natural distribution of\nactivation values. The current trade-off between in-\nterpretability and performance, while common among\n‘white-box’ approaches, is undesirable and a barrier\nto the adoption of interpretable systems. We expect\nthe specific trade-off between modularity, sparsity, and\nperformance observed here to vary across tasks, but it\nmay be possible to mitigate performance losses, for ex-\nample by encouraging modularity only in fine-tuning.\nCurrently, the lack of formal metrics for interpretabil-\nity make it challenging to comparatively evaluate the\nutility of differing interpretability approaches, particu-\nlarly those which approach interpretability at varying\nlevels of abstraction. In the absence of these, we dis-\ncuss performance, human understanding and scope of\nour approach, and anticipate evaluating against firm\nbenchmarks as these emerge.\nReferences\nAchterberg, J., Akarca, D., Strouse, D. J., Duncan,\nJ., and Astle, D. E. (2023).\nSpatially embedded\nrecurrent neural networks reveal widespread links\nbetween structural and functional neuroscience find-\nings. Nature Machine Intelligence, 5(12):1369–1381.\nAdebayo, J., Gilmer, J., Muelly, M., Goodfellow, I. J.,\nHardt, M., and Kim, B. (2018). Sanity checks for\nsaliency maps. CoRR, abs/1810.03292.\nAmer, M. and Maul, T. (2019).\nA review of mod-\nularization techniques in artificial neural networks.\nArtificial Intelligence Review, 52(1):527–561.\nAtrey, A., Clary, K., and Jensen, D. D. (2019). Ex-\nploratory not explanatory:\nCounterfactual anal-\nysis of saliency maps for deep RL.\nCoRR,\nabs/1912.05743.\nBlondel, V. D., Guillaume, J.-L., Lambiotte, R., and\nLefebvre, E. (2008). Fast unfolding of communities\nAnna Soligo, Pietro Ferraro, David Boyle\nin large networks. Journal of Statistical Mechanics:\nTheory and Experiment, 2008(10):P10008.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J.,\nLeary, C., Maclaurin, D., Necula, G., Paszke, A.,\nVanderPlas, J., Wanderman-Milne, S., and Zhang,\nQ. (2018).\nJAX: composable transformations of\nPython+NumPy programs.\nChevalier-Boisvert, M., Dai, B., Towers, M., de Laz-\ncano, R., Willems, L., Lahlou, S., Pal, S., Cas-\ntro, P. S., and Terry, J. (2023).\nMinigrid &\nminiworld:\nModular & customizable reinforce-\nment learning environments for goal-oriented tasks.\nCoRR, abs/2306.13831.\nCoronato, A., Naeem, M., De Pietro, G., and Paragli-\nola, G. (2020). Reinforcement learning for intelligent\nhealthcare applications: A survey. Artificial Intelli-\ngence in Medicine, 109:101964.\nDoshi-Velez, F. and Kim, B. (2017).\nTowards a\nrigorous science of interpretable machine learning.\narXiv:1702.08608.\nE. Crisostomi, S. K. and Shorten, R. (2011). A google-\nlike model of road network dynamics and its applica-\ntion to regulation and control. International Journal\nof Control, 84(3):633–651.\nEppe, M., Gumbsch, C., Kerzel, M., Nguyen, P. D. H.,\nButz, M. V., and Wermter, S. (2022). Hierarchical\nprinciples of embodied reinforcement learning: A re-\nview. arXiv: 2012.10147.\nEuropean Comission and High-Level Expert Group on\nAI (2019). Ethics guidelines for trustworthy artifi-\ncial intelligence.\nFodor, J. A. (1985). Pr´ecis of the modularity of mind.\nBehavioral and Brain Sciences, 8(1):1–5.\nGazzaniga, M., Ivry, R., and Mangun, G. (2018).\nCognitive Neuroscience: Fifth International Student\nEdition. International student edition. W.W. Nor-\nton.\nGlanois, C., Weng, P., Zimmer, M., Li, D., Yang, T.,\nHao, J., and Liu, W. (2024).\nA survey on inter-\npretable reinforcement learning. Machine Learning,\n113(8):5847–5890.\nHe, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Gir-\nshick, R. (2021). Masked autoencoders are scalable\nvision learners. arXiv:2111.06377.\nHein, D., Udluft, S., and Runkler, T. A. (2017). In-\nterpretable policies for reinforcement learning by ge-\nnetic programming. CoRR, abs/1712.04170.\nJiang, Z. and Luo, S. (2019). Neural logic reinforce-\nment learning. CoRR, abs/1904.10729.\nLandajuela, M., Petersen, B. K., Kim, S., Santiago,\nC. P., Glatt, R., Mundhenk, N., Pettit, J. F., and\nFaissol, D. (2021).\nDiscovering symbolic policies\nwith deep reinforcement learning. In Meila, M. and\nZhang, T., editors, Proceedings of the 38th Interna-\ntional Conference on Machine Learning, volume 139\nof Proceedings of Machine Learning Research, pages\n5979–5989. PMLR.\nLipton, Z. C. (2016).\nThe mythos of model inter-\npretability. CoRR, abs/1606.03490.\nLiu, Z., Gan, E., and Tegmark, M. (2023). Seeing is be-\nlieving: Brain-inspired modular training for mecha-\nnistic interpretability. arXiv: 2305.08746.\nLu, R., Li, Y.-C., Li, Y., Jiang, J., and Ding, Y. (2020).\nMulti-agent deep reinforcement learning based de-\nmand response for discrete manufacturing systems\nenergy management. Applied Energy, 276:115473.\nMargalit, E., Lee, H., Finzi, D., DiCarlo, J. J., Grill-\nSpector, K., and Yamins, D. L. (2024). A unifying\nframework for functional organization in early and\nhigher ventral visual cortex. Neuron, 112(14):2435–\n2451.e7.\nMerel, J., Hasenclever, L., Galashov, A., Ahuja, A.,\nPham, V., Wayne, G., Teh, Y. W., and Heess, N.\n(2018). Neural probabilistic motor primitives for hu-\nmanoid control. CoRR, abs/1811.11711.\nPateria, S., Subagdja, B., Tan, A.-h., and Quek, C.\n(2021). Hierarchical reinforcement learning: A com-\nprehensive survey. ACM Comput. Surv., 54(5).\nPignatelli, E., Liesen, J., Lange, R. T., Lu, C., Castro,\nP. S., and Toni, L. (2024). Navix: Scaling minigrid\nenvironments with jax. arXiv:2407.19396.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O. (2017). Proximal policy optimiza-\ntion algorithms. arXiv: 1707.06347.\nSilva, A., Killian, T., Rodriguez, I. D. J., Son, S.-H.,\nand Gombolay, M. (2020). Optimization methods\nfor interpretable differentiable decision trees in rein-\nforcement learning. arXiv: 1903.09338.\nSong, Y., Romero, A., M¨uller, M., Koltun, V., and\nScaramuzza, D. (2023). Reaching the limit in au-\ntonomous racing: Optimal control versus reinforce-\nment learning. Science Robotics, 8(82).\nSternberg, S. (2011). Modular processes in mind and\nbrain. Cognitive Neuropsychology, 28(3-4):156–208.\nPMID: 22185235.\nvon Luxburg, U. (2007). A tutorial on spectral clus-\ntering. Statistics and Computing, 17(4):395–416.\nWang, K., Variengien, A., Conmy, A., Shlegeris, B.,\nand Steinhardt, J. (2022).\nInterpretability in the\nwild: a circuit for indirect object identification in\ngpt-2 small. arXiv: 2211.00593.\nInduced Modularity and Community Detection for Functionally Interpretable RL\nZhang, N., Donahue, J., Girshick, R., and Darrell, T.\n(2014). Part-based r-cnns for fine-grained category\ndetection. In Fleet, D., Pajdla, T., Schiele, B., and\nTuytelaars, T., editors, Computer Vision – ECCV\n2014, pages 834–849, Cham. Springer International\nPublishing.\nAnna Soligo, Pietro Ferraro, David Boyle\nAPPENDIX\nA\nFurther Modularity Examples\nWe show further examples of increasing the connection cost over 5 random seeds to demonstrate the consistency\nwith which modularity emerges, and to show the impact on the model architecture when a performance collapse\noccurs. This can be seen to occur with a connection cost weighting factor, CC λ, of 0.004.\n154\n1756\n6163\n12345\n24680\n0\n0.0001\n0.0004\n0.0007\nCC λ\nSeed\n0.001\n0.002\n0.003\n0.004\n154\n1756\n6163\n12345\n24680\nFigure A.1: Further Examples Showing the Impact of Increasing Connection Cost for 5 Random Seeds.\nInduced Modularity and Community Detection for Functionally Interpretable RL\nB\nPruning Results\nWe implement parameter pruning based on magnitude by zeroing out a specified fraction of weights and biases\nwith the lowest absolute values in each layer prior to inference. This is done in a layer-wise manner due to the\ndiffering distributions of weights and biases across the layers. As illustrated in Figure A.2, models trained with\nL1 sparsity or our connection cost protocol are highly resilient to pruning and up to 90% of model parameters\ncan be zeroed without a reduction in return. In contrast, models trained with the vanilla PPO implementation\nexhibit performance degradation when pruning is increased beyond 10%, and by 90% pruning, the performance\nobserved is equivalent to that when actions are selected randomly.\nThis high degree of achievable sparsity\npresents opportunities for reducing the model memory requirements and increasing the computational efficiency\nduring inference.\nWe also note that the reduction in performance observed as we increase the weighting of the connection cost\nloss closely resembles that seen when L1 sparsity is increased, as shown in Figure A.3. Given that L1 sparsity is\nan established regularisation technique to prevent overfitting, we theorise that our modularity inducing training\nprotocol may offer similar benefits in certain applications.\nFigure A.2: The Impact of Pruning on Performance of Models with Different Training Protocols\nFigure A.3: The Return of Models Trained with L1 Sparsity Compared to our Connection Cost Protocol\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2025-01-28",
  "updated": "2025-01-28"
}