{
  "id": "http://arxiv.org/abs/2502.00833v1",
  "title": "Cross multiscale vision transformer for deep fake detection",
  "authors": [
    "Akhshan P",
    "Taneti Sanjay",
    "Chandrakala S"
  ],
  "abstract": "The proliferation of deep fake technology poses significant challenges to\ndigital media authenticity, necessitating robust detection mechanisms. This\nproject evaluates deep fake detection using the SP Cup's 2025 deep fake\ndetection challenge dataset. We focused on exploring various deep learning\nmodels for detecting deep fake content, utilizing traditional deep learning\ntechniques alongside newer architectures. Our approach involved training a\nseries of models and rigorously assessing their performance using metrics such\nas accuracy.",
  "text": "Deep Fake Detection\nAkhshan P1, Taneti Sanjay2, Chandrakala S3\n1,2UG Students, Dept of CSE, Shiv Nadar University Chennai, India\nEmail: {akhshan21110171, taneti22110220}@snuchennai.edu.in\n3Professor, Dept of CSE, Shiv Nadar University Chennai, India\nEmail: chandrakalas@snuchennai.edu.in\nAbstract—The proliferation of deep fake technology poses\nsignificant challenges to digital media authenticity, necessitating\nrobust detection mechanisms. This project evaluates deep fake\ndetection using the SP Cup’s 2025 deep fake detection challenge\ndataset. We focused on exploring various deep learning models\nfor detecting deep fake content, utilizing traditional deep learning\ntechniques alongside newer architectures. Our approach involved\ntraining a series of models and rigorously assessing their perfor-\nmance using metrics such as accuracy.\nI. INTRODUCTION\nA. Background\nThe rapid advancement of artificial intelligence has led to\nthe development of sophisticated techniques for generating\nrealistic synthetic media, commonly known as deep fakes.\nDeep fakes leverage deep learning algorithms, particularly\ngenerative adversarial networks (GANs), to create highly\nconvincing fake images, audio, and videos. While these tech-\nnologies have promising applications in entertainment and\neducation, they also pose serious threats to privacy, security,\nand the integrity of information. The potential misuse of deep\nfakes for malicious purposes, such as misinformation, fraud,\nand political manipulation, underscores the critical need for\neffective detection methods.\nB. Problem Statement\nThe detection of deep fakes is a complex and pressing\nchallenge due to the high degree of realism that these synthetic\nmedia can achieve. Traditional detection techniques often fall\nshort in identifying subtle manipulations. Therefore, there is an\nurgent need for advanced machine learning models capable of\ndiscerning genuine media from deep fakes with high accuracy.\nC. Significance\nDetecting deep fakes is crucial for maintaining the integrity\nof digital media and ensuring the trustworthiness of informa-\ntion disseminated through various platforms. Effective deep\nfake detection can help mitigate the risks associated with\nthe malicious use of synthetic media, protecting individuals\nand organizations from potential harm. By exploring and\ncomparing different model architectures, this project aims\nto contribute to the growing body of research dedicated to\ncombating the deep fake threat and enhancing media security.\nThe findings from this study are expected to provide\nvaluable insights into the capabilities and limitations of\nFunded by Shiv Nadar University Chennai\ncurrent\ndeep\nfake\ndetection\ntechniques,\nguiding\nfuture\nresearch and development efforts in this vital field.\nII. DATASET DESCRIPTION\nA. Dataset\nIn this study, we utilized the DeepfakeBench benchmark\ndataset, consisting of genuine and manipulated images, pro-\nvided as part of the SP Cup 2025 Deep Fake Detection\nChallenge [1]. The dataset, as provided, exhibited a significant\nimbalance between the number of real and fake images, which\ncould have introduced bias in the model’s performance. To\naddress this issue, we applied an undersampling strategy,\nreducing the dataset to 44,000 samples for each class to ensure\nan equal representation of real and fake images. By balancing\nthe dataset, we aimed to provide a fair and unbiased evalu-\nation of our models. This approach allowed us to accurately\nassess the models’ ability to differentiate between real and\nfake images under balanced conditions, thereby enhancing the\nrobustness and reliability of our results.\nB. Preprocessing\nThe following preprocessing steps were applied to the\nimages before training the models:\n• Normalization: Pixel values were normalized by dividing\nthem by 255. This scales the pixel values to the range\n[0, 1], which helps improve the efficiency and stability\nof the training process.\nIII. ARCHITECTURE OF THE CMVIT REPEAT MODEL\n(MODEL 1)\nThe Cross multi scale vision transformer model is a\nnovel deep learning architecture designed for image classi-\nfication tasks. It combines the strengths of multiscale Vision\nTransformers (MViT) and Cross Model Fusion (CMF) blocks,\nallowing the model to process both spatial and frequency-\ndomain features. This hybrid design enhances the model’s\nability to learn complex representations from input images.\n1) Multi scale Vision Transformer (MViT) Block: The\nVision Transformer (MViT) block is the backbone of the\nCMVit repeat model. It is composed of several key com-\nponents that allow the model to learn spatial relationships\nbetween image patches.\narXiv:2502.00833v1  [cs.CV]  2 Feb 2025\n2) Patch Embedding: The first step in the MViT block\nis the patch embedding, where the input image is divided\ninto smaller patches. These patches are flattened and then\nlinearly embedded into a fixed-size vector (referred to as\nthe embedding dimension). This embedding is achieved using\na convolutional layer with a patch size equal to the kernel\nsize. This transformation results in a set of patch embeddings\nrepresenting the image.\n3) Multi-Head Self-Attention: The next step in the MViT\nblock is the multi-head self-attention mechanism. This mech-\nanism allows the model to focus on different regions of\nthe image by attending to multiple ”heads” in parallel. The\nattention mechanism learns relationships between different\nparts of the image, enabling the model to capture long-range\ndependencies across the entire image.\n4) Feed-Forward Network (FFN):\nAfter the attention\nblock, the output is passed through a feed-forward network\n(FFN). The FFN consists of two fully connected layers with\na ReLU activation function in between. The FFN helps the\nmodel learn more complex representations of the data, further\nrefining the feature map generated by the attention block.\n5) Positional Encoding: Since transformers do not have\na built-in mechanism for capturing spatial information, posi-\ntional encodings are added to the patch embeddings. This pro-\nvides the model with information about the relative positions\nof the patches within the image, enabling it to understand the\nspatial arrangement of image content.\n6) Residual Connections and Layer Normalization: Each\nof the attention and feed-forward layers in the MViT block is\nfollowed by residual connections and layer normalization. The\nresidual connections help mitigate issues like the vanishing\ngradient problem by allowing gradients to flow more easily\nthrough the network. Layer normalization ensures that the\nmodel’s training is stable by normalizing the outputs at each\nlayer.\nA. Cross Model Fusion (CMF) Block\nThe CMF block is designed to process both spatial and\nfrequency-domain features of the input image. It is composed\nof several stages, including the Fourier transform, magnitude\nspectrum computation, convolutional processing, and feature\nfusion.\n1) Fast Fourier Transform (FFT): The first operation\nin the CMF block is the computation of the Fast Fourier\nTransform (FFT). The FFT converts the image from the\nspatial domain to the frequency domain, allowing the model to\ncapture frequency-based features that may not be obvious in\nthe spatial domain. The FFT operation transforms the image\ninto a frequency spectrum.\n2) Magnitude Spectrum Calculation: Once the FFT is\ncomputed, the magnitude spectrum is calculated. The mag-\nnitude spectrum represents the strength of various frequency\ncomponents in the image. By focusing on the magnitude, the\nmodel ignores phase information, which is often less important\nfor image classification tasks.\n3) Convolutional Layers:\nThe magnitude spectrum is\npassed through a series of convolutional layers. These lay-\ners extract high-level frequency features from the spectrum,\nsimilar to how traditional convolutional layers extract spatial\nfeatures from images.\n4) Concatenation with RGB Features: In the next step, the\nfeatures from the magnitude spectrum are concatenated with\nthe original RGB features from the input image. This fusion\nof spatial and frequency-domain features enhances the model’s\nability to learn complex patterns from both domains.\n5) Residual Connections and Layer Normalization: The\nCMF block also employs residual connections and layer nor-\nmalization. The residual connections help preserve important\ninformation throughout the layers, while layer normalization\nstabilizes the model’s training and learning process.\nB. Combined Architecture: Stacking MViT and CMF Blocks\nThe overall architecture of the CMVit repeat model con-\nsists of multiple stacked MViT combined cmf blocks. These\nblocks are repeated several times to allow the model to learn\nprogressively more complex features. Each block combines\nthe strengths of the Vision Transformer and CMF block,\nenabling the model to capture both spatial and frequency-\ndomain information.\nC. Output Layer\nAt the final stage of the architecture, the output of the\nstacked MViT and CMF blocks is passed through an adaptive\naverage pooling layer to reduce the spatial dimensions. The\nresulting feature map is then flattened and passed through\na series of fully connected layers. These layers process the\nfeatures further, enabling the model to classify the input\nimage. The final output is produced using a softmax activation,\ngenerating probabilities for each class.\nThe model is illustrated in fig1.\nIV. ARCHITECTURE OF THE CMVIT REPEAT + LOCAL\nBINARY PATTERN MODEL\n(MODEL 2)\nThe CMVit+LBP model extends the previously discussed\narchitecture by integrating the strengths of the CMVit model\nwith extracted Local Binary Patterns (LBPs). This combination\nenhances the model’s ability to capture texture information\nwithin the image, improving overall performance.\nA. Multi scale Vision Transformer (MViT) Block\nThe Multi scale Vision Transformer (MViT) block used\nin the CMVit Repeat model is the same as the MViT block in\nModel 1. It serves as the backbone for learning spatial relation-\nships between image patches, with the following components:\n1) Patch Embedding: Divides the image into smaller\npatches and embeds them into fixed-size vectors.\n2) Multi-Head Self-Attention: Focuses on different re-\ngions of the image to capture long-range dependencies.\n3) Feed-Forward Network (FFN): Refines the feature\nmap with two fully connected layers and ReLU acti-\nvation.\nFig. 1. Cmvit-repeat\n4) Positional Encoding: Adds spatial information to the\npatch embeddings.\n5) Residual Connections and Layer Normalization: En-\nsures stable training and mitigates vanishing gradient\nissues.\nB. Cross model fusion (CMF) Block\nThe CMF block in the CMVit Repeat model is the same as\nin the previous model. It processes both spatial and frequency-\ndomain features of the input image and includes:\n1) Fast Fourier Transform (FFT): Converts the image\nfrom the spatial domain to the frequency domain.\n2) Magnitude\nSpectrum\nCalculation: Represents the\nstrength of various frequency components.\n3) Convolutional Layers: Extracts high-level frequency\nfeatures from the magnitude spectrum.\n4) Concatenation with RGB Features: Fuses spatial and\nfrequency-domain features for enhanced pattern learn-\ning.\n5) Residual Connections and Layer Normalization: Sta-\nbilizes training and preserves important information\nthroughout the layers.\nC. Local Binary Pattern (LBP) Feature Extraction\nIn addition to the MViT and CMF blocks, the model incor-\nporates Local Binary Pattern (LBP) feature extraction as part\nof the feature preprocessing. LBP is a texture descriptor that\ncaptures local texture patterns by comparing pixel intensities in\na local neighborhood. These patterns are encoded into binary\nvalues and can provide additional discriminative features for\nclassification tasks. The LBP features are extracted and in-\ncorporated into the feature fusion process, further enhancing\nthe model’s ability to capture subtle texture variations in the\nimage.\nD. Combined Architecture: Stacking MViT, CMF, and LBP\nFeatures\nThe overall architecture of the CMVit Repeat model con-\nsists of multiple stacked MViT and CMF blocks, along with\nthe integration of LBP features. These blocks are repeated\nseveral times to allow the model to learn progressively more\ncomplex features. The MViT block captures spatial features,\nthe CMF block processes frequency-domain information, and\nthe LBP features contribute additional texture-based features\nfor a richer representation.\nE. Output Layer\nAt the final stage of the architecture, the output of the\nstacked MViT, CMF blocks, and LBP features is passed\nthrough an adaptive average pooling layer to reduce the\nspatial dimensions. The resulting feature map is then flattened\nand passed through a series of fully connected layers. These\nlayers process the features further, enabling the model to\nclassify the input image. The final output is produced using\na softmax activation, generating probabilities for each class.\nThe model is illustrated in fig2.\nV. ARCHITECTURE OF THE XCEPTIONNET-BASED\nMODEL\n(MODEL 3)\nThe XceptionNet-based model is a deep learning archi-\ntecture designed for deepfake detection tasks. It utilizes the\nXception architecture, which is built upon depthwise separable\nconvolutions, to efficiently capture both low-level and high-\nlevel features from input images. The model is fine-tuned for\nbinary classification, distinguishing between real and deepfake\nimages.\nA. XceptionNet Backbone\nThe XceptionNet backbone used in this model is the same\nas the original Xception model. The architecture is composed\nof a series of depthwise separable convolutions, where the\nconvolution operation is split into two stages: a depthwise\nconvolution and a pointwise convolution. This reduces the\nnumber of parameters and computations while maintaining\nhigh performance. The backbone consists of the following\ncomponents:\n1) Initial Convolutional Layer: The input image is pro-\ncessed by an initial convolutional layer, extracting basic\nvisual features.\nFig. 2. Cmvit-repeat-lbp\n2) Depthwise Separable Convolutions: The core of the\nXception architecture, consisting of depthwise separa-\nble convolution blocks that capture spatial patterns by\napplying convolutions to each channel independently.\n3) Residual Connections: In each block, residual connec-\ntions are used to ensure that important features are pre-\nserved and allow for smoother gradient flow, improving\nthe training process.\n4) Batch Normalization: Applied after each convolution\nto normalize the activations, improving convergence and\nmodel stability.\n5) ReLU Activation: The ReLU activation function is ap-\nplied to introduce non-linearity after each convolutional\noperation.\nB. Fully Connected Layer\nAfter the feature extraction by the Xception backbone, the\noutput is passed through a global average pooling layer to\nreduce the spatial dimensions. The pooled features are then\npassed through a fully connected (FC) layer to perform the\nfinal classification task:\n1) Global Average Pooling: This layer aggregates the\nspatial information by computing the average across all\nspatial locations in the feature map, effectively reducing\nthe feature map’s dimensions to a single vector per\nfeature map.\n2) Fully Connected Layer: The output of the global av-\nerage pooling is passed through a fully connected layer,\nwhich is modified to produce outputs corresponding to\nthe number of target classes.\n3) Output Layer Modification: The final fully connected\nlayer is adjusted to output num_classes logits, with\na softmax activation function applied to convert these\nlogits into class probabilities.\nThe model is illustrated in fig3.\nFig. 3. XCEPTION-NET\nVI. TRAINING AND ENVIRONMENT SETUP\nThe following details outline the training procedures and\nenvironment configurations utilized in this study:\n1) Framework:\n• PyTorch was the primary framework used for model\ntraining.\n• This framework was selected due to its flexibility\nand strong support for deep learning research.\n2) Optimizer:\n• Adam optimizer used with default parameters for its\nefficiency and adaptive learning rate.\n3) Loss Function:\n• Cross Entropy Loss for measuring the performance\nof classification models.\n4) Hardware:\n• Training on a high-performance cluster with One\nNVIDIA A5000 GPUs (24gb VRAM) for faster\ntraining and large batch handling.\n5) Batch Size:\n• Uniform batch size of 64 for consistency and bal-\nance between memory usage and speed.\n6) Stopping Criteria:\n• Training stopped when validation loss plateaued to\nprevent overfitting and ensure good generalization.\nVII. RESULTS\nIn this section, we present the evaluation results of the three deep fake detection models: CMVit, CMVit+LBP, and\nXceptionNet. Each model was assessed using validation accuracy only for the SP CUP 25 challenge dataset.\nType of result\nModel 1\nModel 2\nModel 3\nTrainable Parameters\n71,605,646\n125,631,088\n20,811,050\nNon Trainable Parameters\n0\n0\n0\nOptimizer\nAdam\nAdam\nAdam\nLoss Function\nCross Entropy Loss\nCross Entropy Loss\nCross Entropy Loss\nNo. of Epochs\n70\n23\n100\nBatch Size\n64\n64\n64\nTraining Loss\n0.0984\n0.0165\n0.0012\nStopping Criteria\nValidation loss plateau\nValidation loss plateau\nValidation loss plateau\nTraining Accuracy\n95.65%\n98.35%\n99.98%\nValidation Accuracy\n91.99%\n87.15%\n89%\nValidation F1 Score (Class 0)\n0.93\n0.86\n0.88\nValidation F1 Score (Class 1)\n0.91\n0.88\n0.90\nTime per test file\n0.0692 sec\n0.0564 sec\n0.0082 sec\nREFERENCES\n[1] Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, and Baoyuan Wu. Deepfakebench: A comprehensive benchmark of deepfake detection. In Advances\nin Neural Information Processing Systems, volume 36, pages 4534–4565. Curran Associates, Inc., 2023.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2025-02-02",
  "updated": "2025-02-02"
}