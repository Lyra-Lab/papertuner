{
  "id": "http://arxiv.org/abs/2502.06869v1",
  "title": "A Survey on Explainable Deep Reinforcement Learning",
  "authors": [
    "Zelei Cheng",
    "Jiahao Yu",
    "Xinyu Xing"
  ],
  "abstract": "Deep Reinforcement Learning (DRL) has achieved remarkable success in\nsequential decision-making tasks across diverse domains, yet its reliance on\nblack-box neural architectures hinders interpretability, trust, and deployment\nin high-stakes applications. Explainable Deep Reinforcement Learning (XRL)\naddresses these challenges by enhancing transparency through feature-level,\nstate-level, dataset-level, and model-level explanation techniques. This survey\nprovides a comprehensive review of XRL methods, evaluates their qualitative and\nquantitative assessment frameworks, and explores their role in policy\nrefinement, adversarial robustness, and security. Additionally, we examine the\nintegration of reinforcement learning with Large Language Models (LLMs),\nparticularly through Reinforcement Learning from Human Feedback (RLHF), which\noptimizes AI alignment with human preferences. We conclude by highlighting open\nresearch challenges and future directions to advance the development of\ninterpretable, reliable, and accountable DRL systems.",
  "text": "arXiv:2502.06869v1  [cs.LG]  8 Feb 2025\nA Survey on Explainable Deep Reinforcement Learning\nZelei Cheng1∗, Jiahao Yu1∗, Xinyu Xing1\n1Northwestern University\n{zelei.cheng, jiahao.yu, xinyu.xing}@northwestern.edu\nAbstract\nDeep Reinforcement Learning (DRL) has achieved\nremarkable success in sequential decision-making\ntasks across diverse domains, yet its reliance\non black-box neural architectures hinders inter-\npretability, trust, and deployment in high-stakes\napplications.\nExplainable Deep Reinforcement\nLearning (XRL) addresses these challenges by en-\nhancing transparency through feature-level, state-\nlevel, dataset-level, and model-level explanation\ntechniques. This survey provides a comprehensive\nreview of XRL methods, evaluates their qualita-\ntive and quantitative assessment frameworks, and\nexplores their role in policy reﬁnement, adver-\nsarial robustness, and security. Additionally, we\nexamine the integration of reinforcement learn-\ning with Large Language Models (LLMs), particu-\nlarly through Reinforcement Learning from Human\nFeedback (RLHF), which optimizes AI alignment\nwith human preferences.\nWe conclude by high-\nlighting open research challenges and future direc-\ntions to advance the development of interpretable,\nreliable, and accountable DRL systems.\n1\nIntroduction\nDeep Reinforcement Learning (DRL) has emerged as a trans-\nformative paradigm for solving complex sequential decision-\nmaking problems. By enabling autonomous agents to interact\nwith an environment, receive feedback in the form of rewards,\nand iteratively reﬁne their policies, DRL has demonstrated\nremarkable success across a diverse range of domains in-\ncluding games (e.g., Atari [Mnih, 2013; Kaiser et al., 2020],\nGo [Silver et al., 2018, 2017], and StarCraft II [Vinyals et al.,\n2019, 2017]), robotics [Kalashnikov et al., 2018], com-\nmunication networks [Feriani and Hossain, 2021], and ﬁ-\nnance [Liu et al., 2024]. These successes underscore DRL’s\ncapability to surpass traditional rule-based systems, particu-\nlarly in high-dimensional and dynamically evolving environ-\nments.\nDespite these advances, a fundamental challenge remains:\nDRL agents typically rely on deep neural networks, which\n∗These authors contributed equally to this work.\noperate as black-box models, obscuring the rationale behind\ntheir decision-making processes. This opacity poses signif-\nicant barriers to adoption in safety-critical and high-stakes\napplications, where interpretability is crucial for trust, com-\npliance, and debugging. The lack of transparency in DRL\ncan lead to unreliable decision-making, rendering it unsuit-\nable for domains where explainability is a prerequisite, such\nas healthcare, autonomous driving, and ﬁnancial risk assess-\nment.\nTo address these concerns, the ﬁeld of Explainable Deep\nReinforcement Learning (XRL) has emerged, aiming to de-\nvelop techniques that enhance the interpretability of DRL\npolicies.\nXRL seeks to provide insights into an agent’s\ndecision-making process, enabling researchers, practitioners,\nand end-users to understand, validate, and reﬁne learned poli-\ncies. By facilitating greater transparency, XRL contributes to\nthe development of safer, more robust, and ethically aligned\nAI systems.\nFurthermore, the increasing integration of Reinforcement\nLearning (RL) with Large Language Models (LLMs) has\nplaced RL at the forefront of natural language process-\ning (NLP) advancements. Methods such as Reinforcement\nLearning from Human Feedback (RLHF) [Bai et al., 2022;\nOuyang et al., 2022] have become essential for aligning LLM\noutputs with human preferences and ethical guidelines. By\ntreating language generation as a sequential decision-making\nprocess, RL-based ﬁne-tuning enables LLMs to optimize for\nattributes such as factual accuracy, coherence, and user sat-\nisfaction, surpassing conventional supervised learning tech-\nniques. However, the application of RL in LLM alignment\nfurther ampliﬁes the explainability challenge, as the complex\ninteractions between RL updates and neural representations\nremain poorly understood.\nThis survey provides a systematic review of explainabil-\nity methods in DRL, with a particular focus on their integra-\ntion with LLMs and human-in-the-loop systems.\nWe ﬁrst\nintroduce fundamental RL concepts and highlight key ad-\nvances in DRL. We then categorize and analyze existing ex-\nplanation techniques, encompassing feature-level, state-level,\ndataset-level, and model-level approaches. Additionally, we\ndiscuss methods for evaluating XRL techniques, considering\nboth qualitative and quantitative assessment criteria. Finally,\nwe explore real-world applications of XRL, including pol-\nicy reﬁnement, adversarial attack mitigation, and emerging\nchallenges in ensuring interpretability in modern AI systems.\nThrough this survey, we aim to provide a comprehensive per-\nspective on the current state of XRL and outline future re-\nsearch directions to advance the development of interpretable\nand trustworthy DRL models.\n2\nPreliminaries\n2.1\nReinforcement Learning Foundations\nReinforcement Learning (RL) is a subﬁeld of machine\nlearning that focuses on training agents to make sequen-\ntial decisions by interacting with an environment.\nThe\nenvironment is framed as a Markov Decision Process\n(MDP) [Sutton and Barto, 2018], speciﬁed by the tuple\n(S, A, P, ρ, R, γ):\n• S: A set of states representing possible conﬁgurations of\nthe environment.\n• A: A set of actions available to the agent.\n• P(s′ | s, a): The transition probability function describ-\ning how actions lead from one state s to another state\ns′.\n• ρ: the distribution of the initial state s0.\n• R(s, a): The immediate reward obtained after executing\naction a in state s.\n• γ ∈(0, 1): A discount factor that balances immediate\nand future rewards.\nThe goal of RL is to ﬁnd an optimal policy π(a|s): (S →\nA) which maximizes the agent’s long-term reward. Formally,\nthe long-term reward is deﬁned as the state-value function\nV π(s) =\nX\na∈A\nπ(a|s)\n\"\nR(s, a) + γ\nX\ns′∈S\nP(s′|s, a)V π(s′)\n#\n. (1)\nAccordingly, the action-value function Qπ(s, a) is deﬁned as\nQπ(s, a) = R(s, a) + γ\nX\ns′∈S\nP(s′|s, a)\nX\na′∈A\nπ(a′|s′)Qπ(s′, a′) .\n(2)\nThe advantage function Aπ(s, a) is deﬁned as\nAπ(s, a) = Qπ(s, a) −V π(s) .\n(3)\nIn reinforcement learning, the state-value function V π(s)\nrepresents the expected total reward for an agent starting from\nstate s. Slightly different from V π(s), the action-value func-\ntion Qπ(s, a) is the expected total reward for an agent to\nchoose action a while in state s.\nThe advantage function\nmeasures the expected additional reward for choosing action\na over the expected reward of the policy. The expected total\nreward of a policy π is deﬁned as\nη(π) = Es0,a0,...\n\" ∞\nX\nt=0\nγtR (st, at)\n#\n.\n(4)\nBy maximizing the expected total reward, an optimal policy\nπ∗can be derived, enabling the agent to receive the maximum\nrewards in the environment.\nReinforcement learning can be categorized into two pri-\nmary settings based on the agent’s ability to interact with the\nenvironment: online RL and ofﬂine RL. In online RL, the\nagent has direct, interactive access to the environment and\ncan continuously collect new experiences by executing and\nupdating its policy in real-time. This setting allows for active\nexploration and immediate policy adaptation. In contrast, of-\nﬂine RL restricts the agent to learn solely from a ﬁxed dataset\nof previously collected experiences, without any further envi-\nronment interaction. This dataset typically consists of state-\naction-reward trajectories collected by one or multiple behav-\nior policies. The ofﬂine setting is particularly relevant in sce-\nnarios where environment interaction is expensive, risky, or\nimpractical, such as in healthcare, autonomous driving, or in-\ndustrial control systems.\nThere are two types of main-stream algorithms, i.e., value-\nbased methods and policy-based methods. For value-based\nmethods such as Q-learning algorithm [Watkins and Dayan,\n1992], the agent estimates Q(s, a) and greedily chooses the\noptimal action. Regarding policy-based methods, the agent\ndirectly optimizes its policy based on the reward feedback\n(e.g., Policy Gradient methods [Sutton et al., 1999]). Classic\nalgorithms have been effective in relatively small or struc-\ntured environments. However, their performance may de-\ngrade in high-dimensional or unstructured domains due to\nchallenges in representation and exploration.\n2.2\nDeep Reinforcement Learning Advancements\nTo address the limitations of standard RL in complex or high-\ndimensional settings, Deep Reinforcement Learning (DRL)\nintegrates neural networks as function approximators for poli-\ncies or value functions. Two prominent approaches for learn-\ning deep reinforcement learning policies are Deep Q-Network\n(DQN) [Mnih et al., 2015] and Proximal Policy Optimization\n(PPO) [Schulman et al., 2017]. We provide a brief overview\nof the foundational principles underlying each of these algo-\nrithms.\nDeep Q-Network (DQN).\nDQN utilizes a deep neural\nnetwork to approximate the optimal action-value function (Q\nfunction). The network architecture typically processes state\ninputs s through several layers and outputs Q-values for all\npossible actions simultaneously. The network is trained by\nminimizing the temporal difference error between predicted\nand target Q-values using experience replay and a target net-\nwork to stabilize training. During execution, the optimal pol-\nicy is derived by selecting the action with the highest pre-\ndicted Q-value.\nProximal Policy Optimization (PPO). Different from\nDQN, policy gradient methods directly learn a parameterized\npolicy πθ(a|s) = P(a|s, θ) to maximize the expected total\nreward. While these methods offer more direct policy opti-\nmization, they often suffer from high variance and sensitivity\nto learning rates, leading to unstable training. PPO addresses\nthese challenges by introducing a clipped surrogate objec-\ntive function. It constrains policy updates to prevent exces-\nsive changes while optimizing performance. By maintaining\nproximity between consecutive policies and using advantage\nestimation, PPO achieves more stable training and better sam-\nple efﬁciency compared to traditional policy gradient meth-\nods, making it one of the most widely adopted algorithms in\npractice.\n2.3\nReinforcement Learning for LLMs\nThe integration of RL with Large Language Models (LLMs)\nhas emerged as a promising direction for improving the\nalignment and performance of AI systems.\nMultiple RL\napproaches such as PPO, Directed Preference Optimization\n(DPO) [Rafailov et al., 2023], Reward rAnked FineTuning\n(RAFT) [Dong et al., 2023] have been used to ﬁne-tune\nLLMs for speciﬁc tasks, such as dialogue generation, sum-\nmarization, and instruction following.\nBy leveraging re-\nward feedback, RL-based approaches enable LLMs to gener-\nate more coherent, contextually appropriate, and user-aligned\noutputs.\nDespite these advancements, the explainability of RL for\nLLMs remains an open challenge. The complexity of LLMs,\ncombined with the sequential decision-making nature of RL,\nmakes it difﬁcult to interpret how the input data impacts these\nmodels to generate outputs.\nRecent efforts have explored\ntechniques such as data inﬂuence functions to enhance the\ntransparency of RL for LLMs. However, there is still a need\nfor more systematic explanation approaches in this domain,\nparticularly for applications involving ethical considerations,\nbias mitigation, and user trust.\nIn the subsequent sections, we survey existing methods for\nproviding interpretability in DRL systems as well as LLMs,\nand discuss how these techniques can be evaluated and ap-\nplied in practice.\n3\nExplanation Techniques for DRL\nExisting approaches to explaining deep reinforcement learn-\ning can be broadly categorized into four categories:\n1\nFeature-level Explanation Methods, which focuses on pin-\npointing the most important feature in the DRL agent’s ob-\nservation; 2 State-level Explanation Methods, which iden-\ntiﬁes the most critical steps in the RL trajectory; 3 Dataset-\nlevel Explanation Methods, which selects the most inﬂuen-\ntial data in RL; 4 Model-level Explanation Methods, which\nfocuses on the self-explainability of RL policy models. A\nsummary of selected methods is provided in Figure 1.\n3.1\nFeature-level Explanation Methods\nFeature-level explanation methods aim to identify the most\nimportant features in the agent’s observation space that in-\nﬂuence its decision-making. These methods are particularly\nuseful for understanding how an agent processes visual in-\nputs.\nZahavy et al. [2016] approximated the behavior of DRL\nagents via Semi-Aggregated Markov Decision Processes\n(SAMDPs) and analyzed the high-level temporal structure of\nthe policy with the more interpretable SAMDPs. However,\nthe explanation from SAMDPs is drawn from t-SNE clus-\nters which could be uninformative for users without machine\nlearning backgrounds. To make the explanation more acces-\nsible, Greydanus et al. [2018] proposed a feature-level expla-\nnation method to visualize the importance of pixels in Atari\ngame frames by perturbing the input and observing changes\nin the agent’s policy.\nIn addition to perturbation-based saliency methods, some\nresearchers also proposed gradient-based saliency methods\nthat use gradients of the agent’s policy or value function to\npinpoint the most important feature in DRL agent’s obser-\nvation.\nWang et al. [2016] extend gradient-based saliency\nmaps to deep RL by computing the Jacobian of the output\nlogits with respect to a stack of input images. Joo and Kim\n[2019] leveraged Grad-Cam [Selvaraju et al., 2017] to visual-\nize the important features towards the DRL agent’s behavior.\nCheng et al. [2024] mentioned that we can also use integrated\ngradients [Sundararajan et al., 2017] to identify the most im-\nportant features.\nRecent advancements in deep reinforcement learning\n(DRL) also introduce attention-based mechanisms to enhance\nfeature-level explanations of agent behavior. These methods\naim to improve interpretability by enabling agents to focus on\ntask-relevant information within their observation space. For\ninstance, Mott et al. [2019] proposed an attention-augmented\nagent that employs a soft attention mechanism, allowing the\nagent to sequentially query its environment and focus on per-\ntinent features during decision-making. This approach not\nonly enhances performance but also provides interpretable at-\ntention maps that highlight the areas of the input contributing\nto the agent’s actions. Similarly, Nikulin et al. [2019] intro-\nduced a method that integrates an attention module into the\nagent’s architecture, producing saliency maps that visualize\nthe importance of different input regions in the agent’s deci-\nsion process.\nThese methods provide insights into the agent’s perception\nof the environment but are often limited to explaining low-\nlevel features rather than high-level decision-making pro-\ncesses.\n3.2\nState-level Explanation Methods\nState-level explanation methods focus on identifying critical\nstates in the agent’s trajectory that signiﬁcantly impact its per-\nformance. These methods are useful for understanding the\nagent’s behavior over time and diagnosing failures. We cat-\negorize state-level explanation methods into two categories:\n(1) Explain through ofﬂine trajectories; (2) Explain through\nonline interactions.\nFor the ﬁrst category, Guo et al. [2021] ﬁrst proposed\nEDGE that establishes state-reward relationship by collect-\ning a set of trajectories and then approximating an expla-\nnation model ofﬂine with the Gaussian Process. Note that,\nEDGE provides a global explanation for the policy network.\nAIRS [Yu et al., 2023] further introduces a local explanation\nmethod to identify critical time steps for a given trajectory of\ninterest. AIRS pre-collects a set of trajectories and utilizes\na deep neural network to estimate the contribution of each\nstate to the ﬁnal rewards for each trajectory. Liu et al. [2023]\nproposed a Deep State Identiﬁer that learns to predict returns\nfrom episodes and uses mask-based sensitivity analysis to ex-\ntract important states. However, the ﬁdelity of these methods\nis highly related to the quality of the pre-collected trajecto-\nries, which limits their ability to measure the importance of\n“unseen states”.\nFor the second category, Jacq et al. [2022] presented\nLazyMDP, which extends the action space with a lazy action\nand learns to switch between the default action and the lazy\naction. The states where the policy diverges from the default\nDRL Explanation Methods\nFeature-level\nPerturbation-based\nZahavy et al. [2016]; Greydanus et al. [2018]; Atrey et al. [2020]\nGradient-based\nWang et al. [2016]; Selvaraju et al. [2017]; Sundararajan et al. [2017]\nAttention-based\nMott et al. [2019]; Nikulin et al. [2019]\nState-level\nOfﬂine\nTrajectories\nGuo et al. [2021]; Yu et al. [2023]; Liu et al. [2023]\nOnline\nInteractions\nJacq et al. [2022]; Cheng et al. [2023, 2024]\nDataset-level\nInﬂuence\nFunctions\nKoh and Liang [2017]; Li et al. [2024]; Matelsky et al. [2024]; Ruis et al. [2024]\nData Shapley\nGhorbani and Zou [2019]; Wang et al. [2024a]; Schoch et al. [2023]\nData Masking\nDong et al. [2024]; Lin et al. [2024]\nModel-level\nTransparent\nArchitectures\nTopin et al. [2021]; Ding et al. [2020]; Demircan et al. [2024]\nRule\nExtraction\nSoares et al. [2020]; Likmeta et al. [2020]\nFigure 1: Taxonomy of DRL Explanation Methods\nare further interpreted as non-important states. Cheng et al.\n[2023] proposed StateMask, which online trains a mask net-\nwork in parallel with the agent’s policy network. The mask\nnetwork learns to “blind” the agent’s observations at certain\ntime steps (by taking random actions) while minimizing the\nimpact of blinding to the ﬁnal reward. The time steps when\nthe agent could be blinded are identiﬁed as non-critical steps.\nState-level explanations are particularly valuable for de-\nbugging and improving RL agents, as they highlight the most\ninﬂuential moments in the agent’s decision-making process.\n3.3\nDataset-level Explanation Methods\nDataset-level explanation methods focus on understanding\nhow speciﬁc training examples inﬂuence the learned policy of\nan RL agent. By identifying which data points have the most\nimpact on the policy updates, researchers and practitioners\ncan better diagnose training inefﬁciencies, detect harmful ex-\nperiences, and reﬁne data collection strategies. Recent work\nhas highlighted multiple approaches for quantifying this in-\nﬂuence:\nInﬂuence\nFunctions.\nOriginally\nintroduced\nby\nKoh and Liang [2017], inﬂuence functions estimate how an\nupweighting or removal of a single training example impacts\nmodel parameters. In RL contexts, these techniques can be\nadapted to analyze individual experiences in a replay buffer,\nthereby revealing which transitions most critically shape\nthe agent’s behavior. When incorporating RL with LLMs,\nLi et al. [2024]; Matelsky et al. [2024]; Ruis et al. [2024]\nalso investigated the feasibility of leveraging inﬂuence func-\ntions to identify inﬂuential data. However, they found that\ninﬂuence functions show poor performance and the reasons\nmight be (1) inevitable approximation errors when estimating\nthe inverse-Hessian vector products (iHVP) component due\nto the scale of LLMs, (2) uncertain convergence during\nﬁne-tuning, (3) the deﬁnition of inﬂuential data as changes in\nmodel parameters do not necessarily correlate with changes\nin LLM behavior.\nData\nShapley\nValues.\nShapley\nvalues,\nproposed\nby\nGhorbani and Zou [2019], offer a game-theoretic metric for\nattributing credit to each data point. By considering all possi-\nble subsets of the training set, Data Shapley Values can rank\nexperiences according to their overall contribution to policy\nperformance.\nHowever, the original Data Shapley Values\nare computationally intensive, Wang et al. [2024a] proposed\nan approximation method FreeShap for instance attribution\nbased on the neural tangent kernel, which makes this method\nfeasible for explaining LLM predictions.\nData Masking.\nRecent advances have introduced masking\nas a way to ﬁgure out how speciﬁc elements of a training\ndataset shape an agent’s learning process [Dong et al., 2024;\nLin et al., 2024]. Rather than simply omitting entire experi-\nences, data masking strategically hides or perturbs certain to-\nkens and observes how these modiﬁcations affect the LLM’s\nperformance. Therefore, researchers can pinpoint the data\ncomponents most critical to LLM training and can construct\na pruned dataset based on the critical data to efﬁciently train\na LLM.\nDataset-level explanations help researchers and practition-\ners understand the role of training data in shaping the RL\nagent’s behavior and can guide the design of more efﬁcient\nand effective training schemes.\n3.4\nModel-level Explanation Methods\nModel-level\nexplanation\nmethods\nfocus\non\nthe\nself-\nexplainability of RL policy models,\naiming to make\nthe\nagent’s\ndecision-making\nprocess\ninherently\ninter-\npretable.\nThese methods often involve designing trans-\nparent architectures (e.g., decision tree [Topin et al., 2021;\nDing et al.,\n2020])\nor\nextracting\nhuman-understandable\nrules [Soares et al., 2020; Likmeta et al., 2020] from the\nagent’s policy.\nDemircan et al. [2024] utilize sparse auto-\nencoders within the policy network to provide detailed ex-\nplanations of LLM’s behavior, speciﬁcally focusing on how\nthe network approximates Q-learning by revealing the under-\nlying structure and decision-making process of the model.\nModel-level explanations are particularly valuable for ap-\nplications requiring high transparency, such as healthcare and\nautonomous driving, where understanding the agent’s reason-\ning is critical for trust and safety.\n4\nMeasuring XRL\nEvaluating the quality of explanations in RL requires a multi-\nfaceted approach that captures both user-centered dimensions\nand objective metrics. This section outlines two broad cate-\ngories of assessment, i.e., qualitative and quantitative.\n4.1\nQualitative Evaluation\nInterpretability and Clarity. At the heart of XRL is the need\nfor explanations that humans ﬁnd meaningful and intuitive.\nQualitative evaluation often begins with user studies, such as\nsurveys, to gauge how well participants understand the ex-\nplanation and whether the information provided is perceived\nas coherent and sufﬁcient for understanding policy decisions.\nMost researchers provide a visualization of the proposed ex-\nplanation technique to demonstrate to the participants that\nthe explanation can help them understand the DRL agent’s\nbehavior.\nFor feature-level explanations, Greydanus et al.\n[2018] generated saliency videos to show the feature-level\nexplanations for Atari games and conducted a survey over\n31 students at Oregon State University to measure how their\nvisualization helps non-experts with these Atari games. For\nstate-level explanations, Cheng et al. [2023] generated game\ntrajectories with a color bar behind each frame to indicate the\nimportance of each state and invited participants to answer a\nquestionnaire to demonstrate their method StateMask could\nhelp humans gain a better understanding of a DRL agent’s\nbehavior.\nUser-Centered Design Considerations.\nThe qualitative\nevaluation also informs iterative reﬁnement of explanation in-\nterfaces. By examining user reactions, researchers and de-\nsigners can identify which presentation formats (e.g., visual\noverlays, textual rationales, or example-based justiﬁcations)\nare most effective. This feedback loop, encompassing pilot\ntesting and usability reviews, ensures that explanations re-\nmain aligned with the domain’s practical needs and the target\naudience’s expertise.\n4.2\nQuantitative Evaluation\nFidelity and Faithfulness.\nA key quantitative metric is\nhow closely an explanation reﬂects the true policy or be-\nhavior of the RL agent. To evaluate the ﬁdelity of the ex-\nplanation in RL, researchers commonly use a perturbation-\nbased approach [Guo et al., 2021; Cheng et al., 2023]. The\nresearchers remove features/states/data points identiﬁed as\ncritical in the explanation and check if such a removal sub-\nstantially degrades the agent’s performance. A dual form of\nthis approach is to remove the non-critical features/states/data\npoints and the agent’s performance is expected to have limited\ndifference. The ﬁdelity score is further measured as the per-\nformance difference of the DRL agent before and after per-\nturbing a ﬁxed number of pixels/states/data points. When per-\nturbing the same number of (critical) pixels/states/data points,\na higher performance difference indicates a higher ﬁdelity of\nthe explanation method.\nDownstream Performance Impact.\nXRL systems can\nalso be evaluated on whether their explanations enhance\nagent performance. For instance, Cheng et al. [2024] tested\ntheir proposed reﬁning method based on the critical steps\nidentiﬁed by different explanation methods and compared the\nagent’s performance after reﬁning to evaluate the quality of\nthese explanation methods.\n5\nApplications of Explanations\nWith the explanation of RL, there can be different applica-\ntions of it - they can be leveraged both constructively (for\npolicy reﬁnement and debugging) and potentially destruc-\ntively (for launching adversarial attacks). These applications\ndemonstrate how ﬁdelity and interpretability impact the ef-\nfectiveness of explanation-based interventions in real-world\nscenarios.\n5.1\nLaunching Adversarial Attacks\nRecent work demonstrates that explanations of a DRL agent’s\npolicy can be repurposed to compromise the agent’s perfor-\nmance.\nRecent studies have revealed that explanations of\na Deep Reinforcement Learning (DRL) agent’s policy can\nbe exploited to compromise the agent’s performance. For\ninstance, Lin et al. [2020] demonstrated the vulnerability of\ncooperative Multi-Agent Reinforcement Learning systems\nto adversarial attacks by introducing perturbations based on\nfeature-level explanations (i.e., saliency) to the state space.\nThey proposed a mechanism where an adversary adds pertur-\nbations to the observations of a single agent within a team,\nleading to a signiﬁcant decrease in overall team performance.\nBesides leveraging feature-level explanations to launch ad-\nversarial attacks, researchers also demonstrate that state-level\nexplanations can be utilized to attack DRL agents. EDGE\n[Guo et al., 2021] proposes a more targeted approach by\nleveraging explanations to identify critical time steps during\nan episode. The attacker ﬁrst collects winning episodes from\nthe victim agent and uses post-hoc explanations to highlight\nmoments where actions strongly contribute to victory. By\nforcing the agent to take sub-optimal actions at these identi-\nﬁed crucial steps, the attack achieves signiﬁcant performance\ndegradation with minimal intervention.\nSubsequent research by Cheng et al. [2023] conﬁrms this\nexplanation-driven attack generalizes across different DRL\nenvironments, showing that targeting just 10% of time steps\nCategory\nSubcategory\nCitation\nLaunching Adversarial Attacks\nTargeted Attack\n[Lin et al., 2020; Guo et al., 2021; Cheng et al., 2023; Wang et al., 2024b]\nMitigating Adversarial Attacks\nBlinding Observations\n[Guo et al., 2021]\nShielding Backdoor Triggers\n[Yuan et al., 2024]\nPolicy Reﬁnement\nHuman-in-the-Loop Correction\n[Van Waveren et al., 2022; Jiang et al., 2024]\nAutomated Policy Reﬁnement\n[Guo et al., 2021; Cheng et al., 2023; Yu et al., 2023; Cheng et al., 2024; Liu and Zhu, 2025]\nTable 1: Taxonomy of Explanation-based Interventions in DRL.\ncan substantially reduce agent reward.\nNotably, attacks\nguided by high-ﬁdelity explanation methods prove more ef-\nfective than those using lower-ﬁdelity alternatives, highlight-\ning how better interpretability tools can paradoxically in-\ncrease vulnerability.\nIn addition to exploiting feature-level and state-level expla-\nnations, recent research has explored the use of dataset-level\nexplanations to launch adversarial attacks on LLMs. A study\nby Wang et al. [2024b] investigates the vulnerabilities of re-\ninforcement learning with human feedback. The researchers\nemploy a gradient-based dataset-level explanation method to\nidentify inﬂuential data points within the training set. By poi-\nsoning a small percentage of critical data, an adversary can\nsigniﬁcantly manipulate the LLM’s behavior, leading to the\nelicitation of harmful responses.\nThis line of work highlights the dual-edged nature of inter-\npretability in RL. While explanations are invaluable for de-\nbugging and understanding agent behavior, they can also ex-\npose vulnerabilities. By identifying speciﬁc moments when\nan agent’s correct actions matter most, adversaries can focus\non minimal but high-impact interventions. Consequently, re-\nsearchers must carefully consider the security implications of\nproviding public or easily accessible explanation systems, es-\npecially in safety-critical or competitive domains.\n5.2\nMitigating Adversarial Attacks\nXRL methods not only reveal how adversaries can manipu-\nlate agents but can also guide the design of robust policies.\nBy pinpointing which states or actions are most vulnerable,\ndevelopers can selectively limit or modify the agent’s obser-\nvations and decision pathways at crucial moments, ultimately\nreducing susceptibility to adversarial inputs.\nBlinding Observations at Critical Time Steps. Guo et al.\n[2021] illustrated how explanations of the victim agent’s los-\ning episodes in the You-Shall-Not-Pass game [Todorov et al.,\n2012] uncover the speciﬁc times when adversarial actions\n(e.g., , pretending to fall) most effectively mislead the agent.\nBy analyzing contrastive explanations—comparing losing\nand winning trajectories—it becomes clear that the agent’s\nfocus on adversarial cues at certain time steps can trigger sub-\noptimal responses. The authors proposed “blinding” the vic-\ntim agent to these cues precisely at those critical moments.\nExperimental results show that this explanation-driven de-\nfense signiﬁcantly boosts the victim’s win rate, highlighting\nhow identifying the root cause of agent failures can lead to\ntargeted and effective countermeasures.\nDetecting and Shielding Backdoor Triggers. Another\nline of work focuses on a subtler attack vector: maliciously\ninjected backdoors. Yuan et al. [2024] introduced SHINE, a\nmethod to shield a pre-trained agent from both perturbation-\nbased and adversarial-agent attacks in a poisoned environ-\nment. SHINE ﬁrst gathers trajectories and employs a two-\nstage explanation process to (1) locate states where a back-\ndoor trigger is likely active and (2) isolate the common sub-\nset of features critical to the agent’s decisions in those states.\nThese features are then treated as the backdoor signature. In\nthe second stage, SHINE retrains the policy to neutralize the\ntrigger’s inﬂuence while preserving performance in a clean\nenvironment. This careful mixture of explanation and pol-\nicy adjustment provides theoretical guarantees of improved\nrobustness.\nThese defense mechanisms highlight how explanations\nserve defensive purposes in adversarial contexts. By precisely\nidentifying where and how an agent’s decision-making is\ncompromised, explanation-guided strategies enable targeted\nﬁxes that enhance robustness. This demonstrates that trans-\nparency, when properly leveraged, can be a powerful tool for\nsecuring DRL agents rather than just exposing their vulnera-\nbilities.\n5.3\nPolicy Reﬁnement Through Explanations\nTo reﬁne the policy of the agents, conventional methods such\nas continual training [Fickinger et al., 2021] often fall short\ndue to a lack of knowledge of the root causes of errors. There\nare two categories of methods for policy reﬁnement through\nexplanations:\n• Human-in-the-Loop Correction: Domain experts or\nnon-experts identify suboptimal actions or critical states,\nproviding corrective demonstrations or reward adjust-\nments.\n• Automated Policy Reﬁnement with Explanation:\nExplanation techniques automatically identify pivotal\nstates and reﬁne the target agent’s policy based on the\nexplanation.\nFor the ﬁrst category, Van Waveren et al. [2022] proposed\nto utilize human feedback to correct the agent’s failures.\nMore speciﬁcally, when the agent fails, humans (can be non-\nexperts) are involved to point out how to avoid such a fail-\nure (i.e., what action should be done instead, and what action\nshould be forbidden). Based on human feedback, the DRL\nagent gets retrained by taking the human-reﬁned action in\nthose important time steps and ﬁnally obtains the corrected\npolicy. The downside is that it relies on humans to identify\ncritical steps and craft rules for alternative actions. This can\nbe challenging for a large action space, and the retraining pro-\ncess is ad-hoc and time-consuming. To address the challenges\nof imperfect corrective actions and extensive human labor,\nJiang et al. [2024] introduced the Iterative learning from Cor-\nrective actions and Proxy rewards (ICoPro) framework. In\nthis approach, human labelers provide corrective actions on\nthe agent’s trajectories, which are then incorporated into the\nQ-function using a margin loss to enforce adherence to the\nlabeler’s preferences. The agent undergoes iterative train-\ning, balancing learning from both proxy rewards and human\nfeedback. Notably, ICoPro integrates pseudo-labels from the\ntarget Q-network to reduce human labor and stabilize train-\ning. Experimental results in various tasks, including Atari\ngames and autonomous driving scenarios, demonstrate that\nICoPro effectively aligns agent behavior with human prefer-\nences, even when both proxy rewards and corrective actions\nare imperfect.\nFor the second category, Guo et al. [2021] proposed an\nexplanation-guided policy reﬁnement approach to automati-\ncally correct policy errors without relying on explicit human\nfeedback.\nTheir method ﬁrst identiﬁes losing episodes of\nthe target agent and pinpoints crucial time steps within those\nepisodes using its proposed explanation technique. The au-\nthors employ a ﬁxed number of random explorations at the\nidentiﬁed critical time steps. Any random actions that trans-\nform a losing episode into a win get stored in a look-up ta-\nble as a remediation policy. When deployed, the agent con-\nsults this table at run-time: if the current state matches one\nof the stored entries, the agent applies the corresponding re-\nmediation action; otherwise, it defaults to its original policy.\nThe success of this policy reﬁnement approach depends heav-\nily on the budget of random exploration and the size of the\nlook-up table. Cheng et al. [2023]; Yu et al. [2023] further\nproposed to use DRL explanation methods to identify criti-\ncal time steps and reﬁne the agent by resetting the environ-\nment to the critical states and subsequently resuming training\nthe DRL agents from these critical states. However, this re-\nﬁning strategy can easily lead to overﬁtting as evidenced in\nCheng et al. [2024] and cannot help the agent escape the local\noptimal. Cheng et al. [2024] further proposed a novel reﬁn-\ning strategy to construct a mixed initial state distribution with\nboth the identiﬁed critical states and the default initial states\nto avoid overﬁtting and encourage the agent to perform ex-\nploration during the reﬁning process. Recently, Liu and Zhu\n[2025] proposed a novel framework that leverages explain-\nable reinforcement learning (XRL) to enhance policy reﬁne-\nment. This approach addresses the challenges of DRL agents’\nlack of transparency and suboptimal performance by provid-\ning a two-level explanation of the agents’ decision-making\nprocesses. The framework identiﬁes mistakes made by the\nDRL agent and formulates a constrained bi-level optimiza-\ntion problem to learn how to best utilize these explanations\nfor policy improvement. The upper level of the optimiza-\ntion learns how to use high-level explanations to shape the\nreward function, while the lower level solves a constrained\nRL problem using low-level explanations. The proposed al-\ngorithm theoretically guarantees global optimality and has\ndemonstrated superior performance in MuJoCo experiments\ncompared to state-of-the-art baselines.\n6\nConclusion and Future Directions\nThis survey has reviewed recent advances in the ﬁeld of XRL,\nemphasizing a range of techniques - from feature-level, and\nstate-level to dataset-level approaches, and illustrating their\nroles in adversarial attacks and mitigation, and policy reﬁne-\nment. Evidence across these methods indicates that effec-\ntive explanations can signiﬁcantly enhance trust and debug-\nging efﬁciency in real-world deployments of deep reinforce-\nment learning. Nonetheless, substantial gaps remain to be\naddressed, which are summarized as follows.\nUser-Oriented\nExplanations.\nAlthough\nexisting\ntech-\nniques could highlight critical features/states to illustrate an\nagent’s decision-making process, these granular depictions\ncan be difﬁcult for non-expert users to interpret.\nIn the\ncase of critical features, users who lack domain knowledge\n(e.g., speciﬁc familiarity with a particular game environment)\nmay struggle to grasp the signiﬁcance of highlighted features\nand how they inﬂuence the agent’s actions. Meanwhile, un-\nderstanding critical states often demands that users examine\nmultiple visual frames and then manually summarize what\nthese states imply about the agent’s strategy. This process\ncan be cognitively taxing, as it requires piecing together dis-\npersed information and inferring the agent’s underlying ratio-\nnale without clear contextual guidance.\nTo address these challenges, future research should there-\nfore prioritize strategy-level or narrative-based explanations,\nwhich can provide higher-level rationales that are more ac-\ncessible to general audiences.\nIn particular, leveraging\nvision–language models or other multimodal architectures\ncould facilitate the presentation of natural language narratives\nthat encapsulate an agent’s overarching goals, strategies, and\nreasoning. These narrative formats have the potential to re-\nduce cognitive load, enabling end users to more intuitively\ncomprehend and trust the agent’s behavior.\nDeveloper-Oriented Explanations.\nIn contrast, develop-\ners and researchers frequently require detailed insights into\nan agent’s decision-making process. Mechanistic interpreta-\ntion methods, such as sparse autoencoders or network dis-\nsection, could illuminate hidden representations and policy\nstructures. These more granular approaches enable targeted\npolicy debugging by pinpointing design ﬂaws or overﬁtting at\nthe architectural level. Crucially, explanations for developers\nshould be actionable, which could be compatible with policy\nreﬁnement workﬂows to accelerate iterative improvements.\nIn addition to improving interpretability, explainability\ntools offer considerable potential for enhancing policy per-\nformance. For instance, in game-theoretic contexts, explana-\ntions can help identify equilibrium strategies or support ro-\nbust multi-agent interactions. In hierarchical reinforcement\nlearning, clarifying subtask transitions can streamline learn-\ning in sparse-reward or long-horizon tasks. Similarly, in cur-\nriculum learning, highlighting critical states through explana-\ntion techniques can aid developers in selecting more effective\ninitial conditions. Moving forward, future research should fo-\ncus on aligning these interpretability and performance objec-\ntives by examining how transparent representations of policy\ndecisions can foster robust learning or facilitate agent learn-\ning.\nReferences\nAkanksha Atrey, Kaleigh Clary, and David Jensen.\nEx-\nploratory not explanatory:\nCounterfactual analysis of\nsaliency maps for deep reinforcement learning. In Proc.\nof ICLR, 2020.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,\nAnna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,\nDeep Ganguli, Tom Henighan, et al. Training a helpful\nand harmless assistant with reinforcement learning from\nhuman feedback. arXiv preprint arXiv:2204.05862, 2022.\nZelei Cheng, Xian Wu, Jiahao Yu, Wenhai Sun, Wenbo Guo,\nand Xinyu Xing. Statemask: Explaining deep reinforce-\nment learning through state mask. In Proc. of NeurIPS,\n2023.\nZelei Cheng, Xian Wu, Jiahao Yu, Sabrina Yang, Gang Wang,\nand Xinyu Xing. Rice: Breaking through the training bot-\ntlenecks of reinforcement learning with explanation.\nIn\nProc. of ICML, 2024.\nCan Demircan, Tankred Saanum, Akshay K Jagadish, Marcel\nBinz, and Eric Schulz. Sparse autoencoders reveal tem-\nporal difference learning in large language models. arXiv\npreprint arXiv:2410.01280, 2024.\nZihan Ding, Pablo Hernandez-Leal, Gavin Weiguang Ding,\nChangjian Li, and Ruitong Huang. Cdt: Cascading deci-\nsion trees for explainable reinforcement learning. arXiv\npreprint arXiv:2011.07553, 2020.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang,\nWinnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang,\nKaShun SHUM, and Tong Zhang. RAFT: Reward ranked\nﬁnetuning for generative foundation model alignment.\nTransactions on Machine Learning Research, 2023.\nXiming Dong, Shaowei Wang, Dayi Lin, Gopi Krishnan Ra-\njbahadur, Boquan Zhou, Shichao Liu, and Ahmed E Has-\nsan. Promptexp: Multi-granularity prompt explanation of\nlarge language models. arXiv preprint arXiv:2410.13073,\n2024.\nAmal Feriani and Ekram Hossain. Single and multi-agent\ndeep reinforcement learning for ai-enabled wireless net-\nworks: A tutorial. IEEE Communications Surveys & Tuto-\nrials, 23(2):1226–1252, 2021.\nArnaud Fickinger, Hengyuan Hu, Brandon Amos, Stuart Rus-\nsell, and Noam Brown. Scalable online planning via rein-\nforcement learning ﬁne-tuning. In Proc. of NeurIPS, 2021.\nAmirata Ghorbani and James Zou. Data shapley: Equitable\nvaluation of data for machine learning. In Proc. of ICML,\n2019.\nSamuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan\nFern. Visualizing and understanding atari agents. In Proc.\nof ICML, 2018.\nWenbo Guo, Xian Wu, Usmann Khan, and Xinyu Xing.\nEdge: Explaining deep reinforcement learning policies,\n2021.\nAlexis Jacq, Johan Ferret, Olivier Pietquin, and Matthieu\nGeist. Lazy-mdps: Towards interpretable rl by learning\nwhen to act. In Proc. of AAMAS, 2022.\nZhaohui Jiang, Xuening Feng, Paul Weng, Yifei Zhu, Yan\nSong, Tianze Zhou, Yujing Hu, Tangjie Lv, and Changjie\nFan. Reinforcement learning from imperfect corrective ac-\ntions and proxy rewards. arXiv preprint arXiv:2410.05782,\n2024.\nHo-Taek Joo and Kyung-Joong Kim. Visualization of deep\nreinforcement learning using grad-cam: how ai plays atari\ngames? In Proc. of CoG, 2019.\nŁukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Bła˙zej\nOsi´nski, Roy H Campbell, Konrad Czechowski, Dumitru\nErhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine,\net al.\nModel-based reinforcement learning for atari. In\nProc. of ICLR, 2020.\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz,\nAlexander Herzog, Eric Jang, Deirdre Quillen, Ethan\nHolly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al.\nScalable deep reinforcement learning for vision-based\nrobotic manipulation. In Proc. of CoRL, 2018.\nPang Wei Koh and Percy Liang. Understanding black-box\npredictions via inﬂuence functions.\nIn Proc. of ICML,\n2017.\nZhe Li, Wei Zhao, Yige Li, and Jun Sun. Do inﬂuence func-\ntions work on large language models?\narXiv preprint\narXiv:2409.19998, 2024.\nAmarildo Likmeta, Alberto Maria Metelli, Andrea Tirin-\nzoni, Riccardo Giol, Marcello Restelli, and Danilo Ro-\nmano. Combining reinforcement learning with rule-based\ncontrollers for transparent and general decision-making in\nautonomous driving. Robotics and Autonomous Systems,\n131:103568, 2020.\nJieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto\nLeon-Garcia, and Nicolas Papernot. On the robustness of\ncooperative multi-agent reinforcement learning. In Proc.\nof IEEE S&P Workshop, 2020.\nXinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng,\nYinwei Wei, and Tat-Seng Chua. Data-efﬁcient ﬁne-tuning\nfor llm-based recommendation. In Proc. of SIGIR, 2024.\nShicheng Liu and Minghui Zhu. Utilizing explainable rein-\nforcement learning to improve reinforcement learning: A\ntheoretical and systematic framework. In Proc. of ICLR,\n2025.\nHaozhe Liu, Mingchen Zhuge, Bing Li, Yuhui Wang,\nFrancesco Faccio, Bernard Ghanem, and J¨urgen Schmid-\nhuber. Learning to identify critical states for reinforcement\nlearning from videos. In Proc. of ICCV, 2023.\nXiao-Yang Liu, Ziyi Xia, Hongyang Yang, Jiechao Gao,\nDaochen Zha, Ming Zhu, Christina Dan Wang, Zhaoran\nWang, and Jian Guo. Dynamic datasets and market en-\nvironments for ﬁnancial reinforcement learning. Machine\nLearning, 113(5):2795–2839, 2024.\nJordan K Matelsky, Lyle Ungar, and Konrad P Kording. Em-\npirical inﬂuence functions to understand the logic of ﬁne-\ntuning. arXiv preprint arXiv:2406.00509, 2024.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, An-\ndrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,\net al.\nHuman-level control through deep reinforcement\nlearning. Nature, 518(7540):529–533, 2015.\nVolodymyr Mnih.\nPlaying atari with deep reinforcement\nlearning. arXiv preprint arXiv:1312.5602, 2013.\nAlexander Mott, Daniel Zoran, Mike Chrzanowski, Daan\nWierstra, and Danilo Jimenez Rezende.\nTowards inter-\npretable reinforcement learning using attention augmented\nagents. In Proc. of NeurIPS, 2019.\nDmitry Nikulin, Anastasia Ianina, Vladimir Aliev, and Sergey\nNikolenko.\nFree-lunch saliency via attention in atari\nagents. In Proc. of ICCV Workshop, 2019.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training lan-\nguage models to follow instructions with human feedback.\nIn Proc. of NeurIPS, 2022.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D\nManning, Stefano Ermon, and Chelsea Finn. Direct pref-\nerence optimization: Your language model is secretly a re-\nward model. In Proc. of NeurIPS, 2023.\nLaura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao\nKamalakara, Dwarak Talupuru, Acyr Locatelli, Robert\nKirk, Tim Rockt¨aschel, Edward Grefenstette, and Max\nBartolo.\nProcedural knowledge in pretraining drives\nreasoning in large language models.\narXiv preprint\narXiv:2411.12580, 2024.\nStephanie Schoch, Ritwick Mishra, and Yangfeng Ji. Data se-\nlection for ﬁne-tuning large language models using trans-\nferred shapley values. In Proc. of ACL, 2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization al-\ngorithms. arXiv preprint arXiv:1707.06347, 2017.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. In Proc. of ICCV, 2017.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioan-\nnis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert,\nLucas Baker, Matthew Lai, Adrian Bolton, et al. Master-\ning the game of go without human knowledge. Nature,\n550(7676):354–359, 2017.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioan-\nnis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanc-\ntot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\net al.\nA general reinforcement learning algorithm that\nmasters chess, shogi, and go through self-play. Science,\n362(6419):1140–1144, 2018.\nEduardo Soares, Plamen P Angelov, Bruno Costa, Marcos\nP Gerardo Castro, Subramanya Nageshrao, and Dimitar\nFilev. Explaining deep learning models through rule-based\napproximation and visualization.\nIEEE Transactions on\nFuzzy Systems, 29(8):2399–2407, 2020.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. In Proc. of ICML, 2017.\nRichard S Sutton and Andrew G Barto. Reinforcement learn-\ning: An introduction. MIT press, 2018.\nRichard S Sutton, David McAllester, Satinder Singh, and\nYishay Mansour. Policy gradient methods for reinforce-\nment learning with function approximation. In Proc. of\nNeurIPS, 1999.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A\nphysics engine for model-based control. In Proc. of IROS,\n2012.\nNicholay Topin, Stephanie Milani, Fei Fang, and Manuela\nVeloso. Iterative bounding mdps: Learning interpretable\npolicies via non-interpretable methods. In Proc. of AAAI,\n2021.\nSanne Van Waveren, Christian Pek, Jana Tumova, and\nIolanda Leite. Correct me if i’m wrong: Using non-experts\nto repair reinforcement learning policies. In Proc. of HRI,\n2022.\nOriol Vinyals,\nTimo Ewalds,\nSergey Bartunov,\nPetko\nGeorgiev, Alexander Sasha Vezhnevets, Michelle Yeo,\nAlireza Makhzani, Heinrich K¨uttler, John Agapiou, Julian\nSchrittwieser, et al. Starcraft ii: A new challenge for re-\ninforcement learning. arXiv preprint arXiv:1708.04782,\n2017.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,\nMicha¨el Mathieu, Andrew Dudzik, Junyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, et al.\nGrandmaster level in starcraft ii\nusing\nmulti-agent reinforcement learning.\nnature,\n575(7782):350–354, 2019.\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc\nLanctot, and Nando Freitas. Dueling network architectures\nfor deep reinforcement learning. In Proc. of ICML, 2016.\nJingtan Wang, Xiaoqiang Lin, Rui Qiao, Chuan-Sheng Foo,\nand Bryan Kian Hsiang Low. Helpful or harmful data?\nﬁne-tuning-free shapley attribution for explaining lan-\nguage model predictions. In Proc. of ICML, 2024.\nJiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorob-\neychik, and Chaowei Xiao. Rlhfpoison: Reward poisoning\nattack for reinforcement learning with human feedback in\nlarge language models. In Proc. of ACL, 2024.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Ma-\nchine learning, 8:279–292, 1992.\nJiahao Yu, Wenbo Guo, Qi Qin, Gang Wang, Ting Wang,\nand Xinyu Xing. Airs: Explanation for deep reinforcement\nlearning based security applications. In Proc. of USENIX\nSecurity, 2023.\nZhuowen Yuan, Wenbo Guo, Jinyuan Jia, Bo Li, and Dawn\nSong. SHINE: Shielding backdoors in deep reinforcement\nlearning. In Proc. of ICML, 2024.\nTom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the\nblack box: Understanding dqns. In Proc. of ICML, 2016.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2025-02-08",
  "updated": "2025-02-08"
}