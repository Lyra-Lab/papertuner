{
  "id": "http://arxiv.org/abs/2502.06963v1",
  "title": "Task Offloading in Vehicular Edge Computing using Deep Reinforcement Learning: A Survey",
  "authors": [
    "Ashab Uddin",
    "Ahmed Hamdi Sakr",
    "Ning Zhang"
  ],
  "abstract": "The increasing demand for Intelligent Transportation Systems (ITS) has\nintroduced significant challenges in managing the complex,\ncomputation-intensive tasks generated by modern vehicles while offloading tasks\nto external computing infrastructures such as edge computing (EC), nearby\nvehicular , and UAVs has become influential solution to these challenges.\nHowever, traditional computational offloading strategies often struggle to\nadapt to the dynamic and heterogeneous nature of vehicular environments. In\nthis study, we explored the potential of Reinforcement Learning (RL) and Deep\nReinforcement Learning (DRL) frameworks to optimize computational offloading\nthrough adaptive, real-time decision-making, and we have thoroughly\ninvestigated the Markov Decision Process (MDP) approaches on the existing\nliterature. The paper focuses on key aspects such as standardized learning\nmodels, optimized reward structures, and collaborative multi-agent systems,\naiming to advance the understanding and application of DRL in vehicular\nnetworks. Our findings offer insights into enhancing the efficiency,\nscalability, and robustness of ITS, setting the stage for future innovations in\nthis rapidly evolving field.",
  "text": "1\nTask Offloading in Vehicular Edge Computing using\nDeep Reinforcement Learning: A Survey\nAshab Uddin, Ning Zhang and Ahmed Hamdi Sakr\nAbstract\nThe increasing demand for Intelligent Transportation Systems (ITS) has introduced significant challenges in managing the\ncomplex, computation-intensive tasks generated by modern vehicles while offloading tasks to external computing infrastructures\nsuch as edge computing (EC), nearby vehicular , and UAVs has become influential solution to these challenges. However, traditional\ncomputational offloading strategies often struggle to adapt to the dynamic and heterogeneous nature of vehicular environments.\nIn this study, we explored the potential of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) frameworks to\noptimize computational offloading through adaptive, real-time decision-making, and we have thoroughly investigated the Markov\nDecision Process (MDP) approaches on the existing literature. The paper focuses on key aspects such as standardized learning\nmodels, optimized reward structures, and collaborative multi-agent systems, aiming to advance the understanding and application\nof DRL in vehicular networks. Our findings offer insights into enhancing the efficiency, scalability, and robustness of ITS, setting\nthe stage for future innovations in this rapidly evolving field.\nIndex Terms\nedge computing, reinforcement learning, vehicular networks, multi-agent learning.\nI. INTRODUCTION\nIn recent years, the automotive industry has undergone a significant transformation with the integration of various embedded\nsensors, processing units, and wireless communication modules to enhance the overall driving experience by making it safer,\nmore efficient, and more comfortable [1]. Consequently, the degree of data dependency, the workload associated with data\npreprocessing, as well as the computational intensity and delay sensitivity, have increased across various areas of vehicular\ntechnology, including autonomous driving, real-time traffic management, and advanced driver-assistance systems (ADAS).\nMoreover, due to the dynamic nature of vehicular environments, significant challenges to the effective offloading and processing\nof tasks are exacerbated by factors such as high mobility, variable network conditions, and stringent latency requirements [2].\nTo address the challenges of onboard vehicle computational limitations , latency sensitivity and energy scarcity , computation\noffloading enables vehicles to delegate resource-intensive tasks to more capable external computing infrastructures, such as\ncentralized cloud computing (CC) , Mobile Edge Computing (MEC) and FOG [3]–[7]. MEC brings computation closer to the\nuser by deploying resources at the network edge, including Roadside Units, onboard vehicle systems, and other infrastructure\nlike parking and gas stations [9]. The integration of MEC into the vehicular ecosystem is becoming increasingly critical, as\nit aligns with the low-latency, high-reliability requirements essential for the emerging 5G and future 6G networks [10], [11]\nwhile Fog computing, on the other hand, enables cooperation among edge devices creating an intermediate bridge between\nedge device and cloud [12].\nIn order to facilitate communication between these computing resources, vehicles rely on Vehicle-to-Vehicle (V2V) and\nVehicle-to-Infrastructure (V2I) communication protocols. These protocols leverage advanced radio access technologies (RATs)\nsuch as dedicated short-range communications (DSRC) and cellular networks, including LTE and the emerging new radio\n(NR) standards, to ensure reliable and efficient data exchange in vehicular networks [13]. Moreover, As vehicles and mobile\nusers move across different geographic areas, the ongoing services hosted on nearby edge servers can experience significant\nperformance degradation, a drop in Quality of Service (QoS), and even interruptions if they remain tied to a server that\nthe user has moved away from. Service migration [14] addresses these challenges by dynamically relocating services to the\nmost appropriate edge server, ensuring that the services remain close to the user and continue to function smoothly. This\nprocess is crucial to maintain seamless service continuity, minimizing latency, optimizing network performance,and balancing\nthe migration costs.\nDue to the massive interconnected and heterogeneous nature of vehicular networks, efficient resource management [15], [16]\nis essential to ensure service quality and reliability and optimal usage of computational resources, meeting the stringent latency\nand reliability requirements. Advanced techniques, such as predictive analytics and machine learning, can be employed to\nanticipate resource demands and optimize task distribution across available computing infrastructures, including edge servers,\nfog servers, cloud and nearby users. By dynamically balancing the load and prioritizing critical tasks, these approaches help\nprevent resource bottlenecks, reduce processing delays, and enhance overall system performance.\nA. Uddin, N. Zhang, and A. H. Sakr are with the Department of Electrical and Computer Engineering, University of Windsor, Canada. Email: {uddin81,\nning.zhang, ahmed.sakr}@uwindsor.ca\narXiv:2502.06963v1  [cs.LG]  10 Feb 2025\n2\nIn recent years, edge computing and computational offloading have emerged as essential solutions to address challenges in\nmodern vehicular, mobile, and IoT networks. Researchers have explored various architectures such as MEC, ad hoc systems,\nUAV networks, and cloudlets to improve performance, reduce latency, and optimize resource management. Surveys such as\nthose by Abbas et al. [17] and Shi et al. [18] emphasized MEC’s role in latency reduction, resource allocation, and privacy\nenhancement, with applications in smart homes, augmented reality, and healthcare. Comparative studies like Nouhas et al [19]\nhighlighted the strengths of hybrid Cloud-Edge models, while others, like Deepak et al. [20], Liu et al. [21], and Wang et\nal. [14], stressed the need for advancements in resource allocation, Edge Intelligence, and blockchain integration for security.\nStudies on vehicular networks, including the work of Hou et al. [7], examined AI and RL for dynamic task optimization and the\npotential of Digital Twin Edge Networks (DITEN), and Mao et al. [3] studied the feasibility of server deployment. Furthermore,\nVehicular fog computing (VFC) and heterogeneous Vehicular Networks (HetVNETs) have been explored to leverage vehicular\nresources and improve connectivity while the integration of MEC with SDN and NFV, has been discussed by Haibeh et\nal. [8] followed by Filali et al. [9], further highlighted advances in network performance and scalability, particularly in 5G\nenvironments.\nTraditional optimization techniques such as convex optimization [22], evolutionary algorithms [23], and game-theory [24]\napproaches face significant challenges in solving the problem of mixed integer nonlinearty, along with with the increasing size\nand complexity of vehicular networks. These methods often struggle with scalability, adaptability to dynamic environments,\nand computational efficiency, especially in systems with large state-action spaces and non-convex dynamics. In contrast, deep\nreinforcement learning (DRL) offers a promising solution by simultaneously learning state transitions and optimizing decision-\nmaking policies in real time. DRL’s ability to handle both centralized and multi-agent frameworks enables it to adapt to diverse\nvehicular scenarios, making it well-suited for dynamic and uncertain environments.\nConsidering the solution perspective through DRL, Nguyen et al. [25] highlighted DRL’s potential for MEC-enabled AANs\nbut emphasized challenges with RIS and NOMA integration while Chen et al. [26] noted MDP limitations in IoT, advocating\nrobust frameworks for decentralized and multi-agent systems. Meanwhile, Liu et al. [4] identified scalability and coordination\ngaps in vehicular networks, suggesting hierarchical DRL models for efficient offloading. Focusing on One of the key challenges\nthat arise in vehicular networks, such as high mobility and the non-stationary nature of the environment, Althamary et al. in\n[27] detailed how MARL frameworks can provide robust solutions, offering examples of MARL’s application to V2V, V2I, and\nV2X communications. Another key challenge i.e edge caching, has been examined by Zhu et al. in [28] illustrating how DRL\ncan improve caching strategies in mobile networks by making intelligent, data-driven decisions based on user behavior and\nnetwork conditions. Finally, Li et al. [29] presented a comprehensive survey of Multi-Agent Reinforcement Learning (MARL)\nin future Internet technologies, focusing on challenges like dynamic network access, transmit power control, and computation\noffloading.\nStudies on DRL highlights not only potential solutions for mixed integer nonlinear problems, but also demonstrates a\ngreater ability to adapt and cope with varying conditions and large networks. However, A key challenge that remains under-\nexplored in current DRL research is the examination of MDP formulations with learning methods that fully account for the\ncomplexities of dynamic environments while meeting system-wide goals. Effective coordination of MDP formulations with\nstate-action transitions and reward structures tailored to specific DRL approaches is essential for optimizing performance.\nThis includes designing reward functions that not only guide agents toward local objectives but also ensure overall system\nefficiency. In multi-agent systems, uncoordinated individual decisions often lead to suboptimal outcomes, such as resource\ncontention and inefficiency. Furthermore, aggregating individual POMDPs into a coherent global MDP is complex, requiring\nan understanding of how these MDPs interact, synchronize, or merge to form a valid and accurate representation of the whole\nenvironment. Studying this aggregation process is crucial for optimizing multi-agent systems’ performance. Mechanisms that\nfoster collaboration among agents are vital to prevent locally optimal decisions from leading to globally suboptimal results.\nDRL studies show promise in solving mixed integer nonlinear problems and adapting to dynamic, large-scale networks.\nHowever, challenges remain in formulating MDPs that address environmental complexities and system-wide goals., and at the\nsame time effective reward structures are crucial for balancing local objectives with overall efficiency. Morover, in multi-agent\nsystems, uncoordinated decisions often cause resource contention and inefficiency while aggregations of individual POMDPs\ninto a coherent global MDP is particularly complex, requiring synchronization and interaction analysis to optimize performance.\nFurthermore, uniform learning among agents is a potential bottleneck that need to be addressed and analyzed with performance\ndynamics.\nOur research objectives are to investigate how MDP formulations can be optimized to enhance task offloading and decision-\nmaking under uncertainty. Critical challenges such as value function or quality function approximation, policy convergence,\nexploration and exploitation trade off, central and decentral learning approach for MARL, uniform learning accross agents\nand reward function design are analyzed within a multi-agent context. By addressing these issues, we aim to improve the\noverall performance of MDP-based DRL methods, offering new solutions for both local and global optimization in dynamic,\nmulti-agent environments.\nThe key contributions of this work are summarized as follows:\n• We have investigated Markov Decision Process (MDP) formulations across different studies, underlining the gap and\naddressing simplified assumptions.\n3\n• Our study reveals that the variability in reward functions remains an unresolved research question in many works, i.e. the\ntrade-offs between latency and energy that fail to optimize both metrics simultaneously and fairness among multiagents.\n• We analyze value function approximation techniques for value iteration and policy gradient DRL models in vehicular\nnetworks, examining the impact of discrete vs. continuous action spaces and stochastic vs. deterministic policies. Our\nfindings highlight their influence on scalability and stability in single and multi-agent offloading, addressing real-time\nchallenges in autonomous driving and intelligent transportation systems.\n• Our study provides a comprehensive context for collaborative decision-making among multiple agents in vehicular\nnetworks, addressing learning architectures such as centralized and decentralized models. We investigate the impact of\ndynamic environmental changes on learning and coordination, enabling agents to adapt effectively to fluctuating conditions\nsuch as network topology, user mobility, and resource availability.\n• Our work offers valuable insights for future research on MDP objectives, collaborative and uniform learning in vehicular\ntask offloading scenarios for both single and multi agent scenerios.\nThe rest of the paper is organized as follows: Section III discusses the Computing Paradigms for Vehicular Networks,Section\nIV delves into the type connectivity of computational resources , and section V introduces the Basics of Deep Reinforcement\nLearning (DRL), offering a foundational understanding of RL approaches. Consequently Section VI presents a Comprehensive\nReview of the Literature, and finally, Section VIII provides the Conclusion, summarizing the paper’s contributions and the\npotential impact of DRL on enhancing vehicular network performance.\nII. COMPUTING PARADIGM FOR VEHICULAR NETWORK\nThe evolution of vehicular networks has introduced smart vehicles supported by advanced information and communication\ntechnologies, enabling significant innovations in communication, computing, and caching. Despite these advancements, vehicles\noften face constraints such as limited computational resources and finite onboard battery life. To address these challenges,\npowerful computing paradigms like Centralized Cloud (CC), Edge Cloud (MEC), and Vehicular Cloudlet (VC),UAV, Sattelite\nhave been developed, significantly enhancing the performance of vehicular applications through computation offloading.\nFig. 1. Different Computing Paradigm\nA. Centralized Cloud (CC)\nCloud computing is a transformative model that enables ubiquitous, convenient, and on-demand network access to a shared\npool of configurable computing resources. These resources include networks, servers, storage, applications, and services, which\ncan be rapidly provisioned and released with minimal management effort or interaction with service providers. One of the\n4\nkey advantages of cloud computing is its ability to enhance computing performance by leveraging abundant , innovative, cost-\neffective, and scalable storage and computeing solutions, facilitating a wide range of applications [16]. Despite its numerous\nbenefits, the physical distance between cloud servers and users can introduce delays, which may degrade the performance\nof latency-sensitive applications. Additionally, the transmission of large amounts of data can overburden limited network\nbandwidth, leading to potential congestion and higher transmission costs [30].\nB.\nEdge Cloud (EC)\nEdge cloud computing is an extension of cloud computing that brings computational resources closer to the data source\nor end-users. Edge cloud computing is particularly beneficial for applications requiring real-time data processing, such as\nautonomous driving, augmented reality, and smart grids. Edge cloud can be referred as:\n1) Mobile Edge Computing (MEC):: Mobile Edge computing [31] refers to the processing of data near the source of data\ngeneration, such as IoT devices, sensors, and smart gadgets. This proximity ensures low latency, real-time data processing,\nand reduced bandwidth usage and enables cloud computing capabilities and an IT service environment at the edge of the\ncellular network i.e placed in road side unit, a nearest coputational source from vehicles 1. MEC allows network operators to\nopen their radio access networks (RAN) to authorized third parties, such as application developers and content providers. This\nis particularly beneficial for latency-sensitive applications and services that require rapid data processing close to the mobile\nusers.\n2) Fog Computing(FC):: Fog computing [32], on the other hand, acts as an intermediary layer between the edge devices\nand the central cloud. It extends cloud computing capabilities to the edge of the network, providing additional computational\npower, storage, and networking services closer to the data sources but little i.e placed in parking station far from MEC .It\ninvolves the use of decentralized computing infrastructure in which data, compute, storage, and applications are distributed in\nthe most logical, efficient place between the data source and the cloud. Fog computing is particularly useful for applications\nthat require low latency and are bandwidth-sensitive, providing computation, storage, and networking services between end\ndevices and traditional cloud data centers.\n3) On Board Vehicle Server:: A vehicular cloud or On Board Vehicle Server [33] is a mobility-enhanced, small-scale cloud\ndatacenter mounted onboard vehicles, designed to support resource-intensive and interactive mobile applications. Vehicles with\nunderutilized resources create a distributed network by leveraging their computational, storage, and communication capacities\nas mobile nodes. This setup enables data processing to occur closer to the point of need, reducing reliance on distant data\ncenters. The decentralized structure of VCC enhances real-time applications in transportation, such as traffic management and\nautonomous driving, by improving network efficiency and reducing latency.\n4) Unmanned Aerial Vehicles (UAV): Unmanned Aerial Vehicles (UAVs) or drones [34], have become a critical asset\nin enhancing vehicular networks and Intelligent Transportation Systems (ITS) by providing dynamic, aerial platforms for\ndata collection, communication, and environmental monitoring. UAVs can be rapidly deployed in areas lacking ground-based\ninfrastructure, such as rural or disaster-stricken regions, to monitor traffic conditions, detect accidents, and relay real-time\ninformation to traffic management centers or directly to vehicles. Additionally, UAVs serve as mobile communication relays,\naugmenting traditional networks in high-density urban areas or large-scale events, thereby reducing latency and improving data\nthroughput. Despite their potential, challenges such as regulatory constraints, safety concerns, and limited flight endurance\nmust be addressed to ensure their effective integration into vehicular networks.\nIII. NETWORK TOPOLOGY FOR VEHICULAR NETWORK\nVehicular networks are rapidly evolving, driven by the increasing demands for low-latency communication, real-time data\nprocessing, and efficient resource management. With the rise of connected and autonomous vehicles along with multiple types\nof computing paradigm, different connectivity topologies, such as central, distributed, hierarchical, ad-hoc, and heterogeneous,\nenable efficient data handling across various layers of the network. Each of these topologies comes with its own strengths and\nweaknesses, depending on factors like latency requirements, the density of vehicles, and the availability of infrastructure. This\nsection delves into these diverse topologies, exploring how they can be optimized for vehicular networks to enhance overall\nsystem performance .\nA. Central MEC\nA single central MEC [35] system offers strong centralized control, efficiently coordinating resources like radio bandwidth\nand computational power. Deploying the MEC server at a base station (centre of the network) allows for task processing close\nto the user, reducing latency compared to traditional cloud computing. Centralized management simplifies resource allocation,\nenabling the central MEC server to use channel state information (CSI) and computation requests to optimize. However, this\nsetup faces challenges in scalability and resource limitations, as multiple users share the MEC server’s resources, potentially\nleading to bottlenecks, especially in high-traffic situations. Distributed MEC systems, with multiple edge servers, address these\nissues by distributing the load, enhancing scalability, and reducing latency.\n5\nB. Distributed MEC\nDistributed MEC [36] offers a solution to challenges in vehicular networks by dispersing computational resources across\nmultiple edge nodes, such as vehicles and roadside units (RSUs), instead of relying on a single centralized server. This setup\nsignificantly reduces latency by enabling data processing closer to its source, which is essential for real-time applications like\nautonomous driving and traffic management. By distributing tasks across nodes, the system better adapts to the mobility and\nresource variability in vehicular networks, improving scalability and resilience. Security and privacy are also enhanced, as data\ncan be processed locally, minimizing transmissions to distant servers.\nC. Hierarchical MEC Network\nDistributed edge computing systems face challenges [18] such as resource fragmentation, lack of centralized control, and\nissues with latency and data consistency, especially in environments with numerous edge nodes. Hierarchical edge computing\n[21] addresses these issues by organizing resources across layered levels, from local edge devices to centralized cloud systems.\nThis structure optimizes resource management by processing simpler tasks locally and offloading complex ones to higher\ntiers, reducing latency, network congestion, and energy use. Technologies like Software-Defined Networking (SDN), Network\nFunction Virtualization (NFV), and network slicing enhance control and flexibility, improving traffic flow, task scheduling, and\nefficiency. The hierarchical model also supports load balancing and fault tolerance, making it particularly suited for large-scale\nsettings like smart cities. Hierarchical network may include:\n1) Ad-hoc Networks in MEC: In MEC, ad-hoc networks formed by VANETs (Vehicular Ad-hoc Networks) [37], [38] and\nUAVs (Unmanned Aerial Vehicles) [39], [40] are essential for enabling decentralized, real-time data processing closer to the\nsource. These networks operate independently of centralized infrastructure, offering flexibility and adaptability in dynamic\nenvironments. Edge servers on vehicles or UAVs forming ad-hoc nature allows rapid, direct communication between vehicles\n(V2V), vehicles and UAVs (V2U), or UAVs (U2U), forming a dynamic and resilient network. This setup enhances robustness\nand scalability, enabling task offloading to nearby nodes even in areas with limited fixed infrastructure like disaster zones,\nrural areas, and dense urban spaces.\n2) Heterogeneous MEC: Heterogeneous MEC refers to a topology that integrates different types of network elements,\nincluding vehicles, RSUs, satellites, and cloud servers. This approach ensures that vehicular networks can support a variety\nof communication protocols and device capabilities, allowing for more robust and flexible architectures. Heterogeneous MEC\nis particularly useful for long-range communication and applications that require global coverage, such as integrating satellite\nlinks for vehicular communication in remote areas [41]. By combining diverse technologies, this topology provides seamless\nconnectivity, even in highly variable and dynamic network conditions.\nIV. DRL BASICS\nA. Markob Decision Process\nIn reinforcement learning Fig. 3, the agent interacts with the environment by observing states, taking actions, and receiving\nrewards. The goal is to maximize long-term expected rewards. This process is modeled as a Markov Decision Process (MDP),\ndefined by the tuple (S, A, P, R, γ) [42]:\nS: State space A: Action space P: Transition probabilities R: Rewards γ: Discount factor The transition probability p(s′, r |\ns, a) depends only on the current state and action, not past states a fundamental property of Markov Process. Policies can be\ndeterministic, a direct measure (a = π(s)) or stochastic, a probability distribution (π(a | s)). The state value function, Vπ(s),\nrepresents how good a state is when following a policy π. It is defined as:\nVπ(s) =\nX\na\nπ(a | s)\nX\ns′,r\np(s′ | s, a)(r + γVπ(s′))\n(1)\nThis can be simplified to:\nVπ(s) =\nX\na\nπ(a | s)Qπ(s, a)\n(2)\nThe action value function, Qπ(s, a), represents how good a state-action pair is when following a policy π. It is defined as:\nQπ(s, a) =\nX\ns′,r\np(s′ | s, a)(r + γVπ(s′))\nThis can be simplified to:\nQπ(s, a) =\nX\ns′,r\np(s′ | s, a)\n\"\nr + γ\nX\na′\nπ(a′ | s′)Qπ(s′, a′)\n#\nThe Bellman optimality equation [43] for the value function is given by:\n6\nFig. 2. Hierarchical Network with adhoc\nVπ∗(s) = max\na\n\"\nr + γ\nX\ns′\np(s′ | s, a)Vπ∗(s′)\n#\n(3)\nSimilarly, the Bellman optimality equation for the action-value function is:\nQπ∗(s, a) =\nX\ns′,r\np(s′ | s, a)\nh\nr + γ max\na′ Qπ∗(s′, a′)\ni\n(4)\nB. Value Function Approximation:Value Iteration\nValue Function Approximation (VFA) [44] uses supervised learning to map states (or state-action pairs) to value functions.\nThe parameters θ are updated based on observed data.\nThe loss function for VFA is:\nL(θ) = E(s,r,s′)\n\u0014\u0010\nr + γ ˆV (s′; θ) −ˆV (s; θ)\n\u00112\u0015\n(5)\nFor action-value function approximation:\nL(θ) = E(s,a,r,s′)\n\u0014\u0010\nr + γ max\na′\nˆQ(s′, a′; θ) −ˆQ(s, a; θ)\n\u00112\u0015\n(6)\n1) Deep Q-Network (DQN) with Variant: The Deep Q-Network (DQN) [45] is an off-policy reinforcement learning algorithm,\nmeaning that it learns the optimal policy from the experience buffer, collected following old policy. In DQN, a neural network\nis used to estimate current state action-values (Q-values). To stabilize training, a target network is updated less frequently\nand used to calculate target Q- values, reducing the instability caused by correlations in the training data. Double DQN [46]\naddresses the overestimation bias by using the main network to select actions and the target network to evaluate the selected\nactions, further refining the Q-value estimates while Dueling DQN improves the model by separately estimating the state value\nand advantage functions, which are then combined to produce more robust action-value predictions, enhancing performance\nand generalization.\n7\nFig. 3. Reinforcement Learning -MDP\nC. Value Function Approximation: Policy Gradient\nThe gradient of the value function is the expectation of the trajectory return times the accumulated score function [42]:\n∇θV π(s0) ∝\nX\ns∈S\ndπ(s)\nX\na∈A\nQπ(s, a)∇θπ(a | s)\n=\nX\ns∈S\n\"\ndπ(s)\nX\na∈A\nπ(a | s)Qπ(s, a)∇θ log π(a | s)\n#\n= Eπ [Qπ(St, At)∇θ log πθ(At | St)]\n(7)\n1) Actor-Critic Method: The Actor-Critic [47] method method involves two networks: the Actor, which generates ac-\ntions(policy) based on the current state, and the Critic, which estimates the expected return for that state following maximum\nquality function Q(s, a). The Critic updates its value estimates by minimizing the Temporal Difference error for bellman\nequation, which accounts for the immediate reward and the next state’s value. The Actor’s policy is refined using the gradient\nof the objective function, guided by the Critic’s feedback. This interaction helps the agent learn an optimal policy to maximize\nrewards.\n2) Trust Region Policy Optimization (TRPO): A variant of the Actor-Critic method , Trust Region Policy Optimization\n(TRPO) [48], is often called on policy method because of policy updates with the experienece collected by current policy,\nand utilize restricted policy updates during each iteration to ensure stability. It does so by applying a constraint on the change\nin policy, measured by the KL divergence between the new and old policies. The objective is to optimize the policy while\nkeeping the policy changes within a specified limit to prevent large, unstable updates. This approach helps in maintaining more\nreliable and consistent improvements during training.\n8\n3) Proximal Policy Optimization (PPO): Another variant of the Actor-Critic method (on policy), Proximal Policy Opti-\nmization (PPO) [49], restricts updates to ensure stability. The objective function balances policy improvement by taking the\nminimum of the advantage function and a clipped version to prevent excessive changes. Additionally, to encourage exploration,\nthe objective is refined by incorporating an entropy term, which promotes a diverse set of actions. This approach helps maintain\na balance between exploration and exploitation, leading to more stable and efficient learning.\n4) Soft Actor-Critic (SAC): The Soft Actor-Critic (SAC) [50], an off-policy algorithm uses three key components: a policy\nnetwork, Q-networks, and a value network. The policy network generates actions based on the current state, optimized by\nmaximizing expected returns and incorporating an entropy term to encourage exploration. The Q-networks estimate expected\nreturns for state-action pairs and are updated using the soft Bellman residual, which considers immediate rewards and future\nvalues. The value network predicts the overall value of a state, stabilizing training by minimizing the Temporal Difference (TD)\nerror. These components work together to balance exploration and exploitation, enabling the learning of effective policies.\n5) DDPG: The Deep Deterministic Policy Gradient (DDPG) [51] is an off-policy algorithm designed for environments with\ncontinuous action spaces. It involves an Actor network that generates deterministic actions based on the current state, meaning\nthe actions are not probabilistic but directly selected according to the policy. The Critic network evaluates these actions by\nestimating the expected returns, and it updates its estimates by minimizing the Temporal Difference error. The Actor’s policy\nis refined using gradients that drive the actions toward those that maximize the Q-value. To ensure stable learning, both the\nActor and Critic networks have corresponding target networks that are updated gradually. This framework allows DDPG to\nlearn effective and stable policies in environments requiring deterministic decisions and continuous actions.\n6) TD3: The Twin Delayed Deep Deterministic Policy Gradient (TD3) [52] algorithm improves upon the Deep Deterministic\nPolicy Gradient (DDPG) by addressing overestimation bias and instability in continuous action spaces. TD3 uses two Critic\nnetworks, selecting the smaller Q-value to reduce overestimation. It delays policy updates, updating the Actor less frequently\nthan the Critics to avoid unstable updates. Additionally, TD3 adds noise to target actions (Target Policy Smoothing) to prevent\noverfitting. These enhancements make TD3 more stable and reliable, leading to better performance in continuous action\nenvironments.\nD. MultiAgent DRL\nIn Vehicular Edge Computing (VEC), objectives such as latency reduction, energy efficiency, load balancing, scalability,\nreliability, and data security can be achieved through a single-agent approach with a fully observable MDP or via a multi-agent\nsystem using a partially observable Markov decision process (POMDP). Single Agent RL centrally optimizes latency, energy,\nand task allocation for multiple vehicles or servers while multiagent Reinforcement Learning (MARL) [53], [54] involves\nmultiple agents interacting within a shared environment, each optimizing the set objectives:\n• Collaborative MARL: Agents cooperate to achieve a shared/common goal, e.g., optimizing system latency and energy\nwithout regard for individual interests [55].\n• Cooperative MARL: Agents act independently to achieve own objective, but also consider cooperation to each other e.g.,\noptimizing overall system utility with cooperative channel sensing mechanism [56].\n• Competitive MARL: Agents compete with each other solely to maximize their own objectives, e.g., competing for channel\nor resource access [57].\n1) Key Challenges of MARL: In multi-agent environments, agents often operate under Partial Observability, making decisions\nbased on local information, modeled by Partially Observable Markov Decision Processes (POMDPs). This leads to the non-\nstationarity [58] issue, where agents learn and update their policies concurrently. As one agent adapts, others are doing the\nsame, causing the environment to change continuously. This violates the assumption that state transitions and rewards are going\nto be stationary, making it difficult for any agent to achieve stable learning. In addition to non-stationarity, scalability [59] is\na critical challenge in multi-agent systems. As the number of agents increases, the joint action space expands exponentially,\nmaking it computationally expensive to calculate optimal policies. While Deep Neural Networks (DNNs) are used in Multi-\nAgent Reinforcement Learning (MARL) to approximate large action spaces and enhance scalability, they introduce challenges\nin terms of convergence due to the complexity of deep learning theory. As the system size grows, ensuring efficient and\nstable learning remains a major concern for MARL algorithms, requiring advanced methods to handle the trade-offs between\ncomplexity and performance. The multi agent approach can be referred as the followings:\n2) Centralized Training Decentralized Execution (CTDE): In CTDE , [60] a centralized critic uses global information to\noptimize the policies of all agents during training, ensuring that the system can handle large-scale environments efficiently.\nHowever, during execution, agents act independently, which reduces communication overhead and enables decentralized\ndecision-making. Moroever, by using global information during centralized training, CTDE helps mitigate the non-stationarity\nproblem, as agents learn while considering the actions and policies of others. This results in more stable learning because\nthe centralized critic can account for the evolving environment and ensure policy convergence, even when agents interact\ndynamically. Following are two CTDE method:\n• MultiAgent DDPG: MADDPG ( [61])is a multi-agent reinforcement learning algorithm based on the Deep Deterministic\nPolicy Gradient (DDPG) framework. It follows the Centralized Training and Decentralized Execution (CTDE) paradigm,\n9\nwhere each agent has a critic that has access to global state and action information from all agents during training.\nThis access helps improve the evaluation of actions in complex multi-agent environments, where both cooperation and\ncompetition may occur. During execution, however, each agent acts independently using only its actor, which makes\ndecisions based solely on local observations, enabling decentralized and autonomous behavior in real-time environments.\nWhile MADDPG effectively handles complex multi-agent dynamics, its reliance on global state information during training\nintroduces scalability issues, especially as the number of agents increases. Additionally, it lacks explicit mechanisms for\nvalue decomposition or reward sharing among agents, which limits its ability to compute individual agent contributions\nto the overall team performance.\n• Counterfactual Multi-Agent Policy Gradient (COMA): As the central critic in MADDPG evaluates Q based on the\noverall state and joint actions, it can be difficult for an agent to determine how much its individual action contributed\nto the overall team reward. COMA (Counterfactual Multi-Agent Policy Gradients) ( [62]) is an actor-critic algorithm\ndesigned to address the credit assignment problem in cooperative multi-agent environments. Unike MADDPG, it uses a\ncentralized critic to evaluate joint actions, but with the added feature of calculating a counterfactual baseline to determine\neach agent’s contribution to the total reward. This method compares an agent’s action to what would have happened if the\nagent had taken a different action, thus improving credit assignment. COMA is efficient but is best suited for cooperative\nscenarios as it relies on knowing the global state, which can be impractical in large-scale systems.\n• Value Decomposition Network (VDN): VDN [63] is designed specifically for cooperative multi-agent systems by\nsimplifying the training process through value decomposition. In VDN, the global Q-value is estimated as the sum\n(decomposed) of individual agents’ Q-values, which allows each agent to focus on optimizing its individual task while\nstill contributing to a collective goal. This decomposition facilitates decentralized decision-making while ensuring that\nagents remain aligned towards maximizing the overall team reward. VDN is particularly useful in settings where agents\nmust work together, as it simplifies coordination by breaking down the global objective into smaller, independent tasks.\nHowever, this method assumes that the global value function can be accurately represented as a simple sum of individual\nQ-values, which may not capture the full complexity of certain interactions between agents in highly interdependent\nenvironments.\n3)\nDecentralized Learning with Networked Agents: Decentralized learning [64] excels at handling scalability since agents\nonly rely on their own local observations and, in some cases, information from nearby agents. This significantly reduces the need\nfor global information exchange, making it suitable for large-scale systems like IoT or vehicular networks. It also minimizes the\ncomputational and communication burden, enabling the system to scale efficiently. However, the reduced information sharing\nin decentralized learning can compromise stationarity. Agents make decisions based on limited information, and as other agents\nupdate their policies independently, this can lead to instability in the environment. The lack of global oversight may cause\nagents to adapt to a constantly changing environment, making it difficult for any single agent to converge to an optimal policy.\n• Multi-Agent Deep Q-Network(MADQN): MADQN ( [65]) is a decentralized reinforcement learning algorithm designed\nfor multi-agent environments. Each agent independently learns its Q-function based on local observations, allowing for\nautonomous decision-making. While agents do not communicate directly, they share a global reward, promoting cooperation\nin optimizing shared resources. MADQN is particularly effective in dynamic, resource-constrained environments like V2X\ncommunications, where agents need to manage heterogeneous traffic. By reducing the action space through techniques like\nusing virtual agents, MADQN enhances learning efficiency and improves performance in scenarios with varying resource\ndemands.\n• Multi-Agent Actor Critic (MAAC): The MAAC [66] algorithm is a multi-agent reinforcement learning approach where\neach agent uses a local actor and critic to make decisions based on local observations. The critic provides feedback\non action quality, while the actor updates the agent’s policy accordingly. A key feature of MAAC is its decentralized\napproach, where agents communicate with neighbors to share estimates of the global action-value function, enabling them\nto refine policies with limited global information. This method promotes collaboration and scalability, making it suitable\nfor complex environments with multiple agents working toward common goals.\n• Multi-Agent Soft Actor Critic (MASAC): MASAC [130] framework, each agent is responsible for optimizing its own\nactions to achieve a balance between exploration and exploitation using soft actor-critic (SAC) techniques. The SAC\nalgorithm is a stochastic policy method that incorporates entropy maximization, which encourages the agent to explore\na diverse range of strategies while learning an optimal policy. By adopting a decentralized approach, MASAC allows\nfor distributed decision-making, making it particularly useful in scenarios where direct communication between agents is\nlimited or infeasible.\n4) Game Theory integrated MARL: Game theory and Multi-Agent Reinforcement Learning (MARL) [67], [68] converge\nto address complex decision-making in dynamic multi-agent environments, where multiple agents must adapt their strategies\nbased on the actions of others. Game theory offers strategic frameworks like Nash equilibrium and Stackelberg equilibrium\nto model the interactions between rational agents, helping define stable solutions where no agent benefits from deviating\nunilaterally. In contrast, MARL focuses on how agents learn optimal strategies through trial and error, aiming to maximize\ncumulative rewards in an evolving environment. Integrating game-theoretic concepts into MARL allows for more structured\n10\nDQN\nAC\nA2C\nA3C\nSAC\nDDPG\nTRPO\nPPO\nPolicy Type\nOff Policy\nOn Policy\nOn Policy\nOn Policy\nOff Policy\nOff Policy\nOn Policy\nOn Policy\nMethod\nValue Iteration\nPolicy Iteration\nPolicy Iteration\nPolicy Iteration\nHybrid (Value + Pol-\nicy)\nHybrid (Value + Pol-\nicy)\nPolicy Iteration\nPolicy Iteration\nPolicy Evalu-\nation\nOptimize:\nVπ(s)/Qπ(s, a)\nVπ(s)/Qπ(s, a)\nAπ(s, a)\nAπ(s, a)\nOptimize: Entropy +\nVπ(s)/Qπ(s, a)\nOptimize\nVπ(s)/Qπ(s, a)\nπnew(a|s)\nπold(a|s) Aπ(s, a)\nClip( πnew(a|s)\nπold(a|s) Aπ(s, a))\nFeatures\nLearn from buffer, in-\ncrease sample efficiency\nReduce Variance in\nPolicy Gradient\nSynchronous\nLearning\nfor\nMultiagent\nenvironment\nPrevent\nCorrelation\nand\nvariance\nin\npolicy\nfor\nMultiagent\nenvironment\nSoft Objective with\nmore\nrandomness\nand exploration\nEfficient in Continu-\nous Control\nPrevent\nunexpected\nparameter updates\nPrevent unexpected pa-\nrameter updates\nTABLE I\nCOMPARISON OF REINFORCEMENT LEARNING METHODS\ncoordination, enabling agents to converge toward equilibrium solutions, manage cooperation and competition, and enhance the\nlearning process in decentralized, multi-agent systems.\nV. DRL LITERATURE REVIEW IN VEHICULAR COMPUTATIONAL OFFLOADING\nIn Vehicular Edge Computing (VEC), the efficiency of task handling, latency reduction, and energy optimization is signifi-\ncantly influenced by the underlying topology, which can be categorized into centralized, hierarchical, and distributed models,\neach with distinct advantages and challenges. Centralized models rely on a single computing server for resource allocation,\nsimplifying coordination but facing issues with scalability and bottlenecks. In contrast, distributed topologies offer greater\nflexibility by enabling the selection of optimal resource parameters, such as computation frequencies, server allocation, and\nmigration paths, to address these challenges. Meanwhile, hierarchical architectures, which utilize hybrid computing resources,\nfacilitate multi-tier control by combining protocol-based and ad-hoc advantages. This approach enhances scalability and supports\na more robust and adaptable system.\nA. Centralized Offloading: A central Server for Computation\n.\nCentralized offloading refers to a system architecture where a single central server or base station handles all the computational\noffloading requests from multiple user devices. In this model, the central server is responsible for task execution, and resource\nallocation. User devices, such as smartphones, IoT devices, or vehicles in a network, offload their tasks to the central server\nfor processing, as they may have limited computational capabilities or power constraints. This architecture, as implemented\nin [69], [70], [71], [72], [73], [74], [75], and [76], represents a simplified structure by centralizing offloading decisions and\nresource management, eliminating the need for complex interactions between user devices.\n1) Key Techniques in Offloading : A wide range of techniques such as Zhao et al. [69] introduced contract theory as\nan innovative approach to establishing optimal resource allocation agreements between vehicles and edge servers, striking a\nbalance between energy efficiency and computational offloading. Complementing this, Ju et al. [70] proposed a spectrum-\nsharing mechanism that facilitates more efficient bandwidth utilization by allowing Vehicle-to-Vehicle (V2V) and Vehicle-to-\nInfrastructure (V2I) communications to coexist seamlessly. In a different approach, Khan et al. [71] developed an adaptive\nmode selection strategy that dynamically shifts offloading tasks between edge and cloud servers based on real-time network\nconditions and available resources, ensuring flexibility in offloading decisions.\nFollowing strategic development on learning, Zhao et al. [73] refined offloading strategies further by employing a hierarchical\nlearning framework that integrates convex optimization and DRL, breaking down complex offloading problems into manageable\nsubproblems. Zhang et al. [74] addressed spectrum efficiency through a cooperative spectrum sensing mechanism, improving the\naccuracy of spectrum detection and enabling more effective resource sharing. Meanwhile, Guo et al. [75] focused on optimizing\nresource allocation by utilizing Quality of Service (QoS) flow estimation, tailoring the allocation based on the specific QoS\nrequirements of different applications. In a more interactive approach, Gao et al. [76] combined competitive game theory with\ncooperative multi-agent reinforcement learning (RL), enabling users to engage in both competitive and cooperative interactions\nto achieve the optimal task offloading decisions. Finally, Zheng et al. [78] explored Directed Acyclic Graph (DAG)-based task\noffloading determining the task dependency for terminal devices, employing a single UAV as a computational resource, thus\nbroadening the application of offloading strategies in dynamic environments.\n2) Reinforcement Learning Approaches: Different reinforcement learning (RL) paradigms have been employed across\nvarious studies to optimize task offloading decisions, each tailored to the unique requirements of their environments. Studies\nsuch as [69], [71], [72], and [75] utilized value iteration techniques, leveraging a central agent and Deep Q-Networks (DQN)\nto facilitate individual decision-making by agents operating in isolated environments. These approaches are motivated by the\neffectiveness of DQN in discrete action spaces, allowing agents to learn optimal policies through iterative updates. In contrast,\nactor-critic methods like Deep Deterministic Policy Gradient (DDPG) were applied in [73] to manage continuous action spaces\nand dynamic decision-making, particularly in complex environments where actions require fine-tuned adjustments.\nMulti-agent RL approaches have also gained attention, offering solutions for more interactive and cooperative decision-\nmaking. For instance, Ju et al. [70] and Zhang et al. [74] explored stable value function approximations using Double DQN to\n11\nenhance performance in multi-agent scenarios. Meanwhile folllowing centralized learning approach, Gao et al. [76] implemented\nmulti-agent DDPG to enable cooperative decision-making among multiple agents, particularly in vehicular networks, where\ncoordination is key to optimizing task offloading. Furthermore, Zheng et al. [78] introduced a multi-agent hierarchical system,\nemploying three agents to address multiple objectives for multi-user terminal devices with a single UAV serving as the\ncomputational resource.\n3)\nOptimization Objectives: Several optimization objectives were pursued across these studies, depending on the specific\nsystem requirements:\na) Energy Optimization: Reducing energy consumption was a common goal addressed by Zhao et al. [69], Khan et al.\n[71], Feng et al. [72], Zhao et al. [73], and Zhang et al. [74]. These studies employed techniques such as contract theory,\nadaptive mode selection, and probabilistic actions to optimize energy usage. Significant improvements in energy efficiency\nwere demonstrated, especially in scenarios with limited power budgets and high computational demands.\nb) Latency Optimization: Another key objective was minimizing latency, prioritized by Ju et al. [70], Zheng et al. [78],\nGuo et al. [75], and Gao et al. [76]. Techniques such as spectrum sharing, QoS-based resource allocation, and cooperative\nmulti-agent RL were implemented to reduce delays in offloading tasks, which proved particularly effective in time-sensitive\napplications. Additionally, Zhang et al. [74] focused on optimizing task scheduling to reduce processing queue congestion,\nwhich further enhanced system responsiveness by minimizing task delays.\nc) Resource Utilization: Improving resource utilization was emphasized by Guo et al. [75] through QoS flow estimation,\nensuring that resources were allocated according to the specific needs of each application. This approach was effective in\nmanaging diverse workloads with varying resource demands. Similarly, Feng et al. [72] addressed data rate optimization\nalongside energy consumption, balancing transmission rates with resource allocation to ensure efficient use of bandwidth\nwithout compromising energy efficiency.\n4) Challenges and Limitations: Despite the significant contributions made by these studies, several challenges and limita-\ntions remain unresolved.\na) MDP Presentation: Khan et al. [71] overlooked the energy cost associated with the transmission cost i.e terminal\ndevice to MEC transmission during cloud offloading in their reward function, which results in an incomplete assessment of\nenergy consumption. Similarly, Feng et al. [72] did not incorporate task and resource dynamics into their state representation,\nlimiting the model’s ability to adapt to real-time changes in system conditions. Zhao et al. [69] also oversimplified user-\nspecific and location-specific conditions, neglecting the influence of dynamic environmental factors on offloading decisions.\nThis abstraction could reduce the model’s accuracy in real-world scenarios, where users and edge servers experience constantly\nchanging conditions. Likewise, Gao et al. [76] failed to model channel and server dynamics, which restricted the model’s ability\nto accurately reflect network conditions. Proper representation of these dynamics is essential for optimizing task offloading\ndecisions in highly dynamic vehicular networks.\nb) Baseline Comparison: Several studies faced challenges related to baseline comparisons. Zhang et al. [74] demonstrated\nuniform convergence during the learning process, raising concerns about how the model behaves deterministically during initial\nlearning which is expected to be in random exploration through actions. Similarly, Zheng et al. [78] observed variations in\nconvergence times across their three-agent model, suggesting a need to consider different time scales and exploration trade-offs\nfor each agent when updating and learning their individual objectives. Ju et al. [70] incorporated fixed network parameters, such\nas a predefined exploration rate, which led to a more deterministic transition, failing to capture the variability of real-world\nnetworks. This limited the adaptability of their solution in dynamic environments. Zhao et al. [69] did not include baseline\ncomparisons with standard DRL algorithms, which could have provided further insights into the effectiveness of their approach.\nAdditionally, Zhao et al. [69] omitted detailed evaluations of QoS metrics, only visualizing utility or reward. This lack of focus\non latency, throughput, and other QoS factors makes it challenging to fully understand the practical performance trade-offs.\n5) Conclusion: Centralized offloading is an effective approach for managing task execution and resource allocation, partic-\nularly in systems with limited edge computing power. Various techniques, such as contract theory, spectrum sharing, and QoS\nflow estimation, have improved energy efficiency, reduced latency, and enhanced overall performance. However, challenges\nlike incomplete MDP representations, lack of baseline comparisons, and limited exploration might weaken the adaptability of\nthe applications to dynamic real-world environments, where task, resource, and channel dynamics are crucial.\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComp.\nSource\nRemark\nSingle\nAgentl\n[69]\nDQN\nEnergy\nand\nLatency\nContract Theory\nVehicle\nand\nFog\nmore\ndetailed\nperformance\nmetrics\nand a solid baseline could\nbe emphasized.\n[71]\nEnergy\nAdaptive\nOffloading\nMode\nselection\nfor\nMTC IOT device\nEdge\nand\nCloud\nEnergy consumed during\nthe MTCD-to-MEC trans-\nmission is overlooked.\n12\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComp.\nSource\nRemark\n[72]\nEnergy\nand\ndata rate\nProbabilistic\nAction\ngeneration\nLocal\nand\nedge\nArticulate MDP presenta-\ntion could be highlighted.\n[75]\nDelay\nand\nResource\nUtilization\nApplication /QoS Based\nResource allocation\nEdge\nFluctuation\nin\nconvergence\nsuggests\nroom\nfor\nMDP\nand\nlearning improvement.\n[73]\nDDPG\nEnergy\nHierarchical\nLearning\n(Problem\ndivided\nin\nsubproblem:\noptimization\nand\nDDPG)\nLocal\nand\nedge\nThe\ntrade-off\nbetween\nconvex optimization and\nDRL could be elaborated.\n[77]\nTD3\nComput.\nrate\nJoint\noptimization\nof\nRIS Phase Shifts and\nMobile\ndevice\nEnergy\nPartitioning\nLocal\nand\nedge(BS)\nOptimal mode of offload-\ning (direct or via IRS)\ncould be investigated.\n[82]\nSAC\nLatency and\nCost\nCooperative Vehicle Se-\nlection with Link Dura-\ntion Estimation\nNearby Vehi-\ncles\nAnalysis for resource star-\nvation\nof\nlow\npriority\ncould be conducted.\nMulti-\nAgent\n[76]\nGame\nMAD-\nDPG\n(CTDE)\nDelay\nCompetitive Game and\nCooperative MARL\nLocal\nand\nEdge\nMDP\npresentation\nand\nfairness could be focused.\n[79]\nGame\nMAAC\n(DTDE)\nLatency\nand\nCom-\nputational\nresource\nDifferential\nNeural\nComputer\n(DNC)\nfor\nmemorizing\npast\nlearning\nEdge\nWithout\nsharing\nany\nlearning knowledge and\ninformation, how NE was\nachieved could be well\njustified\nwith\ndetailed\nanalytical explanation.\n[70]\nMADDQN\n(DTDE)\nLatency\nSpectrum Sharing from\nV2V, for V2I\nEdge (BS)\nTraining parameter (Ex-\nploration rate) in state im-\nplies improper state tran-\nsition.\n[78]\nMATD3\n(DTDE)\nLatency\nDAG for UAV\nUAV and Lo-\ncal\nThe\nwork\nresembles\nhierarchical\ndistributed\ndeep\nlearning\nwith\nmultiple\nagents\nwith\ndifferent objectives.\n[74]\nMADQN\n(DTDE)\nEnergy\nand\nQueue\nLength\nCooperative\nSpectrum\nSensing Reward\nLocal\nand\nEdge\nUniform convergence of\nDQN\ninfers\ninsufficient\nexploration.\nB. Distributed Offloading: Collaborative Servers for Computation\nUnlike the centralized offloading approach, where a single server controls all task management , in distributed system\ncomputational offloading relies on a network of interconnected servers or edge nodes that share the computational burden\nand coordinate their actions to optimize the system’s performance as a whole. This collaborative nature means that servers\nexchange information about network conditions, available resources, and task requirements to collectively decide how to best\nhandle offloading requests. Each server in a distributed system contributes to a global or local decision-making process, often\nthrough centralized single-agent or cooperative learning mechanisms such as distributed optimization, federated learning, or\nmulti-agent reinforcement learning (MARL) i.e the works described in [85], [80], [81], [82], [86], [83], [87], [88], [89], [90],\n[91], [92], [93], [94], [95], [96] and [97].\n1) Key Techniques in Offloading: A variety of advanced techniques have been explored to optimize task offloading\nand resource allocation in distributed environments, with a focus on effective task scheduling and resource server selection.\nCooperative vehicle selection, one of key edge resources has been added in resource services by Lv et al. [80] using historical\ntrajectory data and LightGBM, while Liu et al. [81] extended this utilizing XGBoost for more robust decision-making and\naccurate vehicle selections. Consequently, Shi et al. [82] incorporated link duration estimation to ensure stable communication\n13\nlinks during offloading, further improving the process. Beyond improving resource reliability, Offloading strategies based on\ntask characteristics have been explored in many studies i.e, Tang et al. [83] proposed dynamic framing for subtask offloading\nbased on real-time conditions, improving allocation efficiency while Uddin et al. [84] augmented the adaptation of task priority\nfollowing prioritized task selection strategy to offload the task successfully according to environment state dynamics.\nTask migration has been emphasized in various works, including Li et al. [88] and Liu et al. [99], where the authors considered\nthe dynamicity of task allocation and rescheduling between vehicles through optimization. Following the same path, Yuan et\nal. [92] focused on seamless handover and migration within a distributed edge environment to maintain offloading efficiency\nwhile Li et al. [88] combined task partitioning and migration with DRL-based optimization, and Huang et al. [95] developed\na joint model that accounts for task types and vehicle speeds to meet delay constraints.\nIn addition to these efforts, learning network structures have been extensively studied for instance Zhang et al. [100] presented\nan attention-based bidirectional LSTM network to manage temporal dependencies in low earth orbit (LEO) networks. Gao et\nal. [101] applied game-based hierarchical DRL for service assignment and trajectory planning of UAVs, while Yu et al. [102]\nintroduced action branching mechanisms with separate networks to reduce the dimensionality of the action space. Apart from\nlearning collaborations, user coordination has also been addressed in numerous works. Zhang et al. [89] applied social-aware\ncontent caching to leverage social interactions for more efficient task offloading. Zhao et al. [90] focused on mobility-aware\noffloading by dynamically adjusting strategies based on vehicle movement patterns.\nAdditionally, communication-assisted decentralized trajectory planning for UAVs was discussed in [103]. Furthermore, Hou\net al. [104] explored service vehicle clustering in fog zones to utilize vehicle computational resources for V2V and V2R\ncommunication in distributed offloading. Various network topologies have also been proposed to enhance system efficiency.\nFor example, Cheng et al. [105] utilized ultra-dense networks for the joint optimization of task partitioning and resource\nallocation. Lu et al. [106] integrated HAP-supported networks for power harnessing and task offloading, whereas Li et al.\n[107] explored knowledge-based methods for multi-agent systems in UAV-assisted networks\n2) Reinforcement Learning Agent Approaches: In single-agent systems, DQN was widely applied for decision-making,\nwhere agents learned optimal policies by updating Q-values based on their interactions with the environment [85], [80]. In [84],\na variation of DQN with adaptive priority was adopted to emphasize strict latency. However, for better training stability, DDQN\nwas often used to mitigate overestimation issues, leading to more accurate decision-making [86], [83], and [87]. Building on\nthis foundation, Dueling Double DQN (D3QN) was implemented to further enhance stability and address reward fluctuations\nby separating the value and advantage streams within the network architecture [81]. Moving beyond value-based methods,\npolicy gradient approaches such as SAC (Soft Actor-Critic) were utilized to handle more complex environments that required\ncontinuous action spaces, such as cooperative vehicle selection with link duration estimation [82].\nIn multi-agent systems, decentralized training and decentralized execution-oriented MADQN had been implemented in [92],\nincorporating a migration and handover mechanism. Beyond decentralized and isolated learning, a centralized approach, for\nexample, MADDPG (CTDE), had been implemented in many works. For instance, [94] focused on federated learning and\ninterference coordination, whereas [95] applied a joint task type and vehicle speed-aware delay constraint model. Similarly,\n[105] and [101] implemented the same centralized approach with a global state-action sharing strategy for ultra-dense networks.\nFollowing the same objective, [93] employed a coordinated graph-based MADDPG system for distributed agent collaboration,\noptimizing communication and coordination in decentralized environments.\nMoreover, [96] used MAAC (CTDE), incorporating a cost-revenue model for task completion with agent-specific contracts\nand user significance as key decision factors in the centralized training phase. The Counterfactual Multi-Agent (COMA)\napproach, under the centralized training, decentralized execution (CTDE) framework, had been adopted in [104] for vehicular\nnetworks, in [100] for satellite networks, and in [99] for edge user networks. Additionally, the value decomposition network\nstrategy (CTDE), such as AC mix and MA2DDPG, had been implemented in [108] for UAV bandwidth and trajectory control,\nwhile actor-critic based on value-decomposition networks (VDN) had been applied, following an encoding and intention module\nin [103]. Similarly, multi-agent QMix, a centralized learning approach of value decomposition, had been used in [102]. Finally,\nsome decentralized approaches (DTDE) had been explored, i.e., [109], leveraging the concept of observing the POMDP as\nan MDP, the integration of DDPG and SAC for multi-agent DRL in [106], and MASAC in knowledge-guided exploration in\n[107].\n3) Optimization Objectives:\na) Latency: Digital Twin technology minimized delays in local and edge environments by creating real-time virtual\nmodels, which enhanced system responsiveness [85]. Subtask offloading, implemented with dynamic framing, addressed delays\nby flexibly redistributing tasks across local, edge, and base station (BS) environments, improving task execution efficiency [83].\nAdditionally, handover-enabled migration techniques enhanced task migration efficiency and reduced delays while optimizing\ncost management in distributed edge systems [92], [99]. Furthermore, service vehicle operations were optimized in [104] by\nemploying regional and cross-regional vehicles to minimize delays in offloading scenarios.\nSimilarly, [109] utilized non-cooperative multi-agent DRL to optimize latency and aggregated costs. Meanwhile, inverse\nlatency optimization [101] and latency difference optimization [102] were applied in UAV-based trajectory control and service\noffloading, respectively. Cooperative vehicle selection using XGBoost represented as an effective technique in optimizing\nresource allocation for nearby vehicles, resulting in smoother communication and task execution [81]. Additionally, link duration\n14\nestimation ensured both latency and cost efficiency, particularly in vehicle-to-vehicle communication scenarios, where timely\ndecision-making was crucial [82]. Mobility-aware dependent task offloading further optimized latency and energy consumption,\nespecially in local and Roadside Unit (RSU) environments, by dynamically adapting to changing mobility conditions [90].\nb) Duel Objective: Energy and Latency Optimization: Shared results with an ID pool balance latency, energy, and cost\nacross local, edge, and base station (BS) systems, providing a more efficient distribution of tasks and resources [86]. Similarly,\nhandover-enabled strategies enhance both energy and latency optimization, particularly in vehicle and RSU systems, ensuring\nsmooth task migration while managing these metrics effectively [87]. Federated Multi-Agent Deep Deterministic Policy Gradient\n(MADDPG), coupled with interference coordination, also plays a critical role in optimizing energy and latency, especially in\nRSU setups, where agent coordination is vital [94]. Additionally, DDPG and SAC are integrated to jointly optimize energy and\nlatency in [106] while [84] integrated prioritized reward with DQN to optmize latency and energy focusing on task completion\nratio.\nMoreover, a joint task type and vehicle speed-aware delay constraint model enhances energy and latency management\nacross local and nearby vehicles, improving system performance in dynamic environments [95]. Similarly, [100] addresses\ndelay-constrained energy optimization, while [105] focuses on joint latency and energy optimization with an emphasis on\ncomputation maximization.Link duration estimation serves a dual purpose in vehicle-to-vehicle communications by optimizing\nboth cost and latency, ensuring that tasks are processed efficiently with minimal expense [82]. Additionally, delay-constrained\nmigration techniques further enhance cost optimization by minimizing transformation efforts and streamlining resource usage\nin local and RSU systems, making them highly effective for real-time edge computing scenarios [92].\nc) QoS and QoE Improvement: To improve Quality of Service (QoS), user significance, cost, and revenue models are\nimplemented in edge environments, ensuring that user demands are met while maintaining system profitability [96]. For Quality\nof Experience (QoE), heterogeneous task offloading strategies cater to users by optimizing offloading decisions based on task\ntypes, providing more tailored and responsive system performance [97]. Additionally, data rate optimization has been addressed\nin [108] as a QoS metric, while fairness concerns have been tackled in [103]. Similarly, [107] approaches latency-constrained\nfairness optimization objectives for multi-agent UAV offloading and trajectory control, enhancing system performance under\nconstrained conditions.\n4) Challenges and Limitations:\na) MDP representation: Xu et al. [85] focused on data size, latitude, and longitude but overlooked critical communication\ndynamics like transmission speed, channel gain, and path loss, limiting the robustness of their state representation. This is\nsimilar to [100], where the state considered the visibility matrix of the satellite network but neglected channel dynamics.\nLikewise, Hou et al. [104] illustrated the selection of service vehicles in the action space, but state details about the service\nvehicles were not presented, creating ambiguity in learning, particularly regarding the zone the service vehicle belongs to and\nthe required communication service (e.g., V2V, V2R) based on coverage.\nSimilarly, Xu et al. [85] and Liu et al. [99] did not account for essential factors such as transmission speed and channel\nconditions, which are critical for capturing the full scope of user communication state transitions. Moreover, Peng et al. [86]\nexperienced instability due to fluctuating rewards, suggesting the need for a more observable Markov Decision Process (MDP).\nZhao et al. [90] also faced challenges where task priorities were determined only after offloading decisions, leading to inefficient\nresource allocation due to the lack of task urgency in the initial decision-making process. Wang et al. [108] highlighted that\nfixed and very strict penalties, such as those for SNR, boundary, and collision violations, can disproportionately dominate the\nreward. Given the logarithmic growth of communication capacity, this led to unbalanced decision-making, as these penalties\nbecame overly influential in the learning process.\nb) Algorithmic fairness and result comparison: Lv et al. [80] compared DQN with SARSA in a way that appeared\nbiased, as SARSA was implemented using a tabular method rather than a deep reinforcement learning (DRL) framework,\nskewing the results and rendering the comparison inadequate. Similarly, Tang et al. [83] noted a performance gap between\nDDQN and Dueling DQN but failed to provide a sufficient explanation for why Dueling DQN did not experience the same\nlevel of performance improvement, leaving this disparity unexplained. Liu et al. [81] did not offer a detailed comparison of\nthe performance differences between PATO and other schemes, such as D3DQN, making it difficult to identify the specific\nfactors contributing to performance gains, particularly why PATO outperforms D3QN. Shi et al. [82] omitted key metrics such\nas energy consumption and the number of tasks handled, limiting the evaluation of system efficiency. Similarly, Peng et al.\n[86] encountered fluctuations in reward curves, indicating unstable learning, while Huang et al. [94], [95] faced inconsistencies\nin their results due to varying priority levels, which led to unclear performance analysis.\nShi et al. [82] also found that their dynamic pricing model performed similarly to baseline greedy approaches when local\ncomputational resources were sufficient, casting doubt on the value of their offloading scheme. Zhao et al. [90] struggled with\nagents making short-sighted decisions based on immediate rewards, which could result in suboptimal long-term outcomes.\nThese challenges highlight the need for more robust methods in state representation, reward design, fairness, communication\nmanagement, and energy efficiency in task offloading frameworks for vehicular networks. Li et al. [107] illustrated a promising\nknowledge-guided exploration technique for the MASAC system, but where expert knowledge is unavailable, the solution could\nbenefit from further investigation with more advanced approaches. Gao et al. [101] proposed game-based service allocation\nand DRL-based trajectory planning, but both approaches utilized the same communication model, overlooking the impact of\n15\nDRL actions on UAV positioning. Additionally, Yu et al. [102] presented action branching in QMix, but convergence curves\nrevealed insufficient exploration of certain baselines, indicating the need for more in-depth analysis.\nc) Deterministic and Stochastic Policy Tradeoff: Many works, such as [88], [94] and [95], based on deterministic policy\napproaches, fail to address the challenges associated with discrete action spaces. One critical issue is the proper discretization\nof actions in a deterministic manner, which remains a significant challenge. Moreover, the added noise in deterministic policies,\naimed at obtaining discrete actions and ensuring exploration, requires greater emphasis. A more focused exploration mechanism\ncould strengthen the applicability and reliability of these works in discrete action spaces\nd) Synchronization and Coordination : Maleki et al. [87] encountered difficulties with task synchronization during partial\noffloading, raising concerns about the coordination and reliability of offloading decisions. Similarly, Zhou et al. [97] faced\nchallenges managing multiple users selecting the same server, which is a critical issue in distributed environments and can lead\nto resource contention. Deng et al. [91] investigated LSTM feature selection by including all state data but overlooked targeted\n(relevant) features, which could lead to inefficiencies in state representation. Tan et al. [103] illustrated that while information\nsharing can improve decision-making by providing additional context, it does not guarantee that agents will make perfectly\ncooperative decisions. In a different approach, Heydari et al. [109] investigated non-cooperative mechanisms by transforming\nPOMDP into MDP. However, the assumption of a finite state space for channels and waiting time might be problematic in\ncapturing the actual transition dynamics and the stationary distribution of the environment.\nFor central coordination, Cheng et al. [105] used DDPG to share global state and actions between agents, effectively\ncoordinating decisions. However, the study’s baseline was also DDPG, which may require a clearer presentation of how single-\nagent MDPs are compared with multi-agent MDPs. While Lu et al. [106] integrated DDPG and SAC for energy harnessing\nand latency optimization in task offloading, the final convergence performance exhibited inefficiencies, suggesting the need for\nfurther investigation into the learning strategies and MDP formulation.\n5) Conclusion: In summary, distributed offloading strategies in vehicular networks and edge computing environments offer\nsignificant potential for enhancing system efficiency, reducing latency, and optimizing resource allocation. By leveraging\ncollaborative frameworks, such as multi-agent systems and federated learning, these approaches distribute the computational\nload and decision-making processes across interconnected servers, addressing bottlenecks and central node failures. Despite\nthese advancements, several challenges remain such as incomplete state representations limits the robustness of decision-\nmaking processes, particularly when critical factors like communication dynamics, transmission speed, and channel gain are\noverlooked. Reward function design often presents issues, particularly when scaling metrics like task priority and latency,\nleading to unstable learning and skewed outcomes. Additionally, algorithmic fairness and adequate comparisons between\ndifferent reinforcement learning methods are frequently lacking, leaving gaps in performance evaluation. Moreover, resource\ncontention and synchronization difficulties persist, especially in distributed multi-agent environments, where multiple users\ncompete for the same resources.\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComputational\nSource\nRemark\nSingle\nAgent\n[85]\nDQN\nDelay\nDigital Twin\nLocal\nand\nedge\nCommunication\nstate\ncould\nbe\nenhanced\nin\nMDP\n[80]\nData\nRate\nand resource\nCooperative Vehicle se-\nlection with historic tra-\njectory and LightGBM\nNearby Vehi-\ncles\nStandard Baseline com-\nparison could be empha-\nsized,\ni.e.,\nDQN\ncom-\npared with SARSA.\n[84]\nLatency,\nEnergy, and\nCompletion\nRatio\nPrioritized Task Selec-\ntion for Offloading\nEdge\nUser\nFairness\nand\nQuality\nof\nSatisfaction/Experience\ncould\nbe\ninvestigated\nthrough MARL.\n[86]\nDDQN\nLatency,\nEnergy, and\nCost\nShared Result with ID\npool\nLocal,\nEdge,\nand BS\nFluctuation\nin\nfinal\nepisode\nimplies\ninsufficient\nlearning\nand improper stationary\ndistribution of state.\n[83]\nDelay\nSubtask Offloading with\nDynamic Framing\nLocal,\nEdge,\nand BS\nElaborated\nexplanation\nmight help to understand\nthe performance between\ntwo closely related RL\nmethods, i.e., D3QN and\nDDQN.\n16\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComputational\nSource\nRemark\n[87]\nEnergy\nand\nLatency\nHandover enabled\nLocal,\nNearby\nVehicles, and\nEdge\nTask\ndependencies\nand\ntask segmentation could\nbe investigated.\n[81]\nD3QN\nLatency and\nResource\nCooperative Vehicle Se-\nlection with XGBoost\nNearby Vehi-\ncles\nDeeper\nanalytical\ncomparison\ncould\nenhance\nperformance\nadvantage.\n[97]\nQoE\nHeterogeneous Task Of-\nfloading (task type)\nLocal\nand\nedge\nPerformance\nevaluation\ncould be observed in an\nepisodic manner, as time-\nslot-based evaluation does\nnot show convergence.\n[88]\nDDPG\nLatency\nTask\nPartitioning\nand\nMigration through Opti-\nmization, Decision DRL\nEdge\nExplanation of DDPG ac-\ntion, i.e., discrete to con-\ntinuous action conversion,\ncould be emphasized.\n[89]\nLatency\nSocial-Aware\nContent\nCaching\nVehicle\nand\nEdge\nMDP\ncould\nbe\nunder-\nscored, reflecting actual\ncommunication and com-\nputation dynamics.\n[90]\nEnergy\nand\nLatency\nMobility-Aware Depen-\ndent Task Offloading\nLocal\nand\nRSU\nRelative improvements in\nreward and priority after\ntaking action may direct\ntoward a suboptimal solu-\ntion.\n[91]\nEnergy\nand\nLatency\nCNN,\nGNN,\nLSTM\nused\nto\ncalculate\nVehicle Trust value to\navoid DDoS attack\nLocal\nand\nEdge\nCould\nbe\ninvestigated\nwith\ntargeted\n(relevant)\nfeatures\nusing\nLSTM,\nignoring all features of\nstates.\n[98]\nTD3\nEnergy\nand\nLatency\nCooperative\nRSU\nMi-\ngration and Load Trans-\nfer\nLocal\nand\nedge\nEdge load information has\nbeen overlooked in MDP\nstate.\nMulti\nAgent\n[92]\nMADQN\n(DTDE)\nDelay-\nConstrained\nMigration\nCost\nand\nTransfor-\nmation\nCost\nHandover/Migration en-\nabled\nLocal\nand\nEdge\nRoute efficiency could be\nfocused along with suc-\ncessful trip time.\n[94]\nMADDPG\n(CTDE)\nEnergy\nand\nLatency\nFederated\nMADDPG\nand\nInterference\nCoordination\nLocal\nand\nEdge\nCould\nfocus\non\nSBS-\nbased performance met-\nrics to validate uniform\nperformance.\n[95]\nEnergy\nand\nLatency\nJoint Task Type and Ve-\nhicle Speed-Aware De-\nlay Constraint Model\nLocal,\nNearby\nVehicles,\nEdge\nPerformance\nevaluation\nmight require justification\nwith consistent notation\nto\nensure\nclarity\nand\naccuracy in the results.\n[105]\nComputation\nBit, Energy,\nand Latency\nJoint\noptimization\nof\ntask\npartitioning\nand\nresource allocation\nLocal\nand\nEdge\nPerformance\nbaseline\nDDPG needs explanation\nof the MDP as a single\nagent\ncompared\nto\nMADDPG.\n17\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComputational\nSource\nRemark\n[93]\nLatency-\nConstrained\nCost\nCoordinated Graph from\nMulti-Agent\nCoordina-\ntion\nNearby Vehi-\ncles\nCollaboration among ag-\ngregated groups could be\nillustrated.\n[101]\nGame\nMAD-\nDPG\n(CTDE)\nLatency and\nFairness\n(bits)\nGame approach hierar-\nchical RL\nLocal\nand\nUAV\nUse of the same commu-\nnication model for game\nand DRL model might\nnot reflect the position\nchanges of UAV by DRL.\n[96]\nMAAC\n(CTDE)\nQoS\nUser Significance, Cost,\nand Revenue Agent\nLocal\nand\nEdge\nElaboration of conflicting\nactions might be focused.\n[106]\nMA-\nDDPG\n+\nSAC\n(CTDE)\nLatency and\nEnergy\nIntegrated\nMADDPG\nand SAC\nLocal\nand\nEdge (HAP)\nSignificant fluctuation in\nthe final episode suggests\nthe requirement of MDP\nand learning method im-\nprovement.\n[107]\nMASAC\n(CTDE)\nLatency and\nFairness\nKnowledge-driven\nand\nExploration Guidance\nLocal\nand\nUAV\nKnowledge-driven\nexploration-based\n(KMASAC)\nhas\nmore\ndirectional\nsearch\nof\nsolutions\nthan\nmore\ndynamic MADDPG.\n[109]\nMAA2C\n(CTDE)\nLatency\nNon-cooperative\nenvironment\ntransformed\nfrom\nPOMDP to MDP\nLocal\nand\nEdge\nThe assumption of a fi-\nnite state for channel and\nwaiting time may not ac-\ncurately capture the true\ntransition dynamics of the\nenvironment.\n[104]\nCOMA\n(CTDE)\nLatency\nService Vehicle Cluster-\ning and Cross-Regional\nOffloading\nService Vehi-\ncle\nImproper MDP such as\naction ambiguity and lack\nof clustering details.\n[99]\nLatency\nTask Migration\nEdge\nOnly task data in the state\nsuggests the model cannot\nfully capture the broader\nsystem dynamics, such as\nnetwork\nconditions\nand\nresource contention.\n[100]\nEnergy\nAttention-based bidirec-\ntional\nlong\nshort-term\nmemory network\nSatellite\nImproper MDP such as\noverlooking of link chan-\nnel state.\nC. Hiererchial Offloading: Multier, and Heterogeneous Servers\nDistributed edge computing, though flexible due to decentralized decision-making, faces challenges as networks scale.\nIncreased device numbers and task complexity lead to coordination issues, higher latency, and inefficiencies, especially in\ndynamic environments. Each node’s independent operation results in communication overhead and redundancy. Hierarchical\nedge computing offers a more efficient and flexible solution by leveraging heterogeneous servers at different levels. Heterogenous\nservers like intercommunicating nearby vehicles, and fog nodes collaborate, allowing tasks to be offloaded locally for low-\nlatency needs or to remote or nearby servers for resource-heavy operations. This layered approach optimizes resource use,\nimproves system performance, and reduces the burden on individual devices i.e [128], [113], [126], [120], [114], [127], [116],\n[129], [112], [117], [118], [110], [111], [121] and [122].\n1) Key techniques in Offloading : Various strategies have been developed to enhance task offloading and resource opti-\nmization in hierarchical, heterogeneous environments, with a strong focus on improving task scheduling and server selection.\nLearning coordination plays a critical role in these strategies. For example, DDPG combined with FCNN and Convex\nOptimization in [111] had been used to optimize energy efficiency and task offloading, while centralized training with distributed\naction approaches in [110] and [112] aimed to improve task distribution across systems.\nIn IoT environments, strategies like the Independent IoT Device Agent in [115] optimize task allocation based on energy\nrequirements and task volume, ensuring that IoT devices operate efficiently without system overload. To improve security,\n18\nTrust Value for Vehicles was calculated in [113], incorporating social awareness to enable safe and reliable offloading\ndecisions. Similarly, the Improved Clustering Algorithm based on social relations in [114] enhanced resource allocation\nby grouping tasks according to social interactions, leading to more effective computational resource utilization. In dynamic\nenvironments, predicting communication states is critical for effective offloading. Techniques like LSTM and BRNN in [117]\npredict future communication states, improving the accuracy of offloading decisions, while Seq2Seq-based BiGRU in [118]\nmanaged sequential sub-tasks and extracts environmental and task-specific features for better decision-making. Cooperative\nEdge Offloading in [116] leveraged cooperation among nearby edge nodes to optimize the offloading process, while joint\naction learning was implemented in [119] for offloading tasks from cloud centers to the edge.\nFollowing the fairness perspective, Comparison Model in [122] promoted equity by minimizing disparities in task completion\ntimes across vehicles or users. Meanwhile, the use of Directed Acyclic Graphs (DAG) and Experience Trajectory Sharing in\n[121] improved task scheduling efficiency by sharing task trajectories among agents, enhancing task execution in multi-agent\nsystems. Knowledge-driven approaches like model sharing with new users and historical knowledge-driven learning in [123]\nprovided a foundation for improving offloading efficiency, while [124] scales rewards based on task priority, ensuring higher\nrewards for higher-priority tasks. To reduce unnecessary overhead, Adaptive Video Quality Enhancement in [120] minimized\nvideo quality adjustments, conserving bandwidth and computational resources. Moreover, a multi-objective formulation had\nbeen adopted for UAV networks in [125], balancing energy, latency, and other objectives effectively.\n2) Reinforcement Learning Approaches: In single-agent systems, DQN and its variants are widely employed to enhance\noffloading and resource allocation. For instance, Distributed DQN [126], [112] focused on learning from distributed edge\nand UAV resources, while Double DQN [113] integrated trust values into offloading decisions, emphasizing security. DDPG,\nwhen combined with enhancements like FCNN and Convex Optimization [111], achieved a balance in energy efficiency.\nAdditionally, adaptive video quality transmission [120] and improved clustering algorithms based on social interactions [114]\nleveraged environmental factors to improve offloading performance, with cooperative edge processing [116] further optimizing\nthe process. Moreover, the application of the Ornstein-Uhlenbeck Noise Vector to DDPG [127] enhanced action exploration,\nwhile Prime Dual DDPG [129] efficiently allocated tasks between local and remote servers. A3C [122] ensured fairness in\ntask scheduling by minimizing delays and balancing offloading across tasks, while Distributed PPO [121] shared experience\ntrajectories among agents to enhance overall performance, and similarly PPO [118] was applied to manage sequential tasks,\nensuring smooth coordination between agents.\nIn multi-agent systems, federated DQN (DTDE) [128] allows distributed agents to learn local models while sharing insights\nfor global optimization. Although this approach explores federated learning in decentralized networks, a more uniform MDP\ncould enhance consistency in learning. Independent IoT device agents [115] manage task and energy distribution, with agents\nmaking independent decisions but coordinating to resolve task conflicts. Double DQN (DTDE) [113] incorporates trust value\nfor security, combining latency and energy optimization with security protocols, adding a trust-based decision layer to the\ntraditional DRL approach. Following the motivation for distributed learning, joint action Q-learning is implemented in [119],\nwhile SAC (DTDE) [130] incorporates fairness mechanisms into decision-making, ensuring equitable resource distribution and\nbalancing user success rates, with further exploration of user fairness potential.\nOn the other hand, DDPG (CTDE) [117] combined advanced techniques like LSTM and BRNN for communication state\nprediction, leveraging sequential models to enhance state prediction and incorporating priority-based decision-making. Similarly,\n[110] and [124] applied centralized training and distributed execution (CTDE) MADDPG with a focus on latency reduction,\nusing a coefficient-based latency and priority reward model to dynamically adjust actions during task execution. Lastly,\ndecentralized learning through A3C [123] had been implemented for multi-agent learning, where a central critic and individual\nagent critics share network weights without sharing MDP information, similar decentralized Multi-agent actor-critic methods\nhad also been adopted in [125], further exploring decentralized learning strategies.\n3) Optimization Objectives::\na) Latency: Strategies such as cooperative edge offloading [116] and Prime Dual DDPG [129] reduced processing delays\nby streamlining task distribution between edge and cloud resources. Additionally, various approaches, including [114], [118],\n[110], [121], [119], [123], and [122], focused on minimizing latency through optimized task scheduling and resource allocation\nwithin hierarchical architectures, ensuring that tasks are executed efficiently and promptly.\nb) Duel Objective: Latency, and Energy : A number of methods focused on optimizing both energy efficiency and\nlatency to improve system performance. For instance, works such as [128], [113], [117], and [126] implemented energy-\nconscious offloading strategies that showcased lower energy consumption while simultaneously reducing task completion times.\nAdditionally, [115] addressed energy and task management in IoT devices by minimizing energy usage and ensuring timely\ntask execution. Similarly, [111] prioritized energy optimization as a primary objective, while [125] optimized both energy and\nlatency with a focus on completing tasks efficiently.\nc)\nQoS and QoE Improvement : Moreover, the joint optimization of latency, data rate, and bandwidth were a key\ngoal in several for assuring QoS and QoE. For example, [127] addressed the trade-offs between these factors, particularly in\ndynamic edge environments. Similarly, [112] focuses on minimizing latency and optimizing data rates to ensure efficient task\nprocessing and smooth communication in high-traffic environments. Techniques like adaptive video quality transmission also\n19\nhad been presented in [120] to balance latency and bandwidth usage, facilitating seamless video streaming in edge computing\nsetups. On the other hand, channel access was utilized for latency optimization in [124].\n4) Challenges and Limitations::\na) MDP Presentation: Several studies have struggled with incomplete or insufficient state representations, which impacted\ndecision-making and overall performance. For example, Khayyat et al. [126] did not account for transmission and computation\ndynamics in their state data, leading to ambiguity in state transitions. Similarly, Xue et al. [116] omitted temporal features in\ntheir edge caching decisions, which limited the long-term efficiency of task processing. Zhang et al. [129] overlooked important\ntransmission dynamics, such as fading channel gain, reducing the precision of their task offloading decisions. Additionally,\nLi et al. [111] failed to incorporate task dynamics, resource dynamics, and channel dynamics, all of which are essential for\nensuring adaptability and optimization in real-world environments.\nb) Synchronization and Coordination: Synchronization among agents or networks posed significant challenges in several\nstudies. Khayyat et al. [126] did not provide a clear mechanism for synchronizing parallel policy networks, which led to potential\ninconsistencies in policy learning across agents. Chouikhi et al. [115] lacked a coordination mechanism among IIoT devices,\nraising concerns about system efficiency and scalability as the number of devices and server conflicts increased. Goudarzi et al.\n[121] did not propose a resource contention management strategy, such as TDMA or FDMA, to handle simultaneous offloading\nrequests, leading to bottlenecks when multiple users selected the same server. Similarly, Zhang et al. [119] approached joint\naction Q-learning but raised concerns about fair learning among agents, as simulation results depicted selfish behavior in\nindividual agents rather than coordination. Likewise, Qi et al. [123] presented A3C for multi-agent learning, but their approach\nfocused on weight sharing rather than sharing actual information, which is a significant bottleneck in achieving effective\ncooperative multi-agent learning.\nc) Reward Function and Optimization Trade-offs:: Reward function design presented another common issue. Ke et al.\n[127] raised concerns about squaring energy and latency values in their reward function, which diminished their impact when\nthe values were less than one. Gao et al. [117] struggled with appropriately scaling task priority and latency metrics in their\nreward function, which risked skewing the learning process. Similarly, the priority weight in the reward function by Cao et\nal. [124] led to reward scaling, but it did not show a significant impact on performance, as the results for latency, energy, and\nagent-specific outcomes were not adequately presented.\nd) Lack of Clarity: Some studies failed to address scalability and real-world constraints. For instance, Chouikhi et al.\n[115] did not account for bandwidth limitations or communication delays between IIoT devices and edge servers, which are\ncritical considerations in large-scale deployments. Similarly, Li et al. [111] neglected task arrival rates and waiting times at the\nserver, both of which are essential factors in real-world task offloading systems. Zheng et al. [113] lacked clarity in their Double\nDQN implementation regarding the incorporation of transmission dynamics, which could have compromised the accuracy of\noffloading decisions. Similarly, Gao et al. [117] encountered a lack of clarity in scaling task priority and latency in their reward\nfunction, which affected the robustness of the state representation. Xu et al. [120] faced difficulties adapting DDPG for discrete\nactions in mobile VR services, and their lack of explanation for state transitions further impacted decision-making accuracy.\nZhao et al. [114] left unresolved questions about the validity period of clustering aggregation and whether DDPG updated its\nstate after each aggregation.\nMoreover, Dai et al. [118] did not decouple dependent and independent subtasks, potentially limiting the flexibility and\nscalability of their task offloading model. Mishra et al. [128] failed to explain the necessity of uniform state representation\nin each fog zone, despite the varying number of fog servers and RSUs. For Federated Learning, achieving aggregation from\nall zones required a uniform network structure, but this was not clearly discussed. Additionally, [125] raised the need for\na clearer explanation and analytical review regarding the UAV height increase, which depicted higher task completion. This\naspect should be explored in greater depth, as increasing UAV height typically leads to higher path loss, potentially impacting\ntask completion efficiency.\n5) Conclusion: Hierarchical offloading presents a scalable solution to the limitations of distributed systems by coordinating\ntasks across multiple layers of devices,cooperated vehicles , fog nodes, and cloud servers. It balances low-latency offloading with\nthe computational demands of resource-intensive tasks by efficiently distributing workloads across heterogeneous environments.\nWhile various DRL optimization strategies models and cooperative learning techniques, have enhanced system performance,\nchallenges remain. Issues such as incomplete MDP representations, synchronization, reward function design, and real-world\nscalability constraints must be addressed to fully realize the potential of hierarchical offloading.\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComp.\nSource\nRemark\nSingle\nAgent\n[126]\nDQN\nEnergy\nand\nLatency\nDistributed RL\nLocal,\nEdge,\nand Cloud\nThe MDP and parallel\npolicy learning could be\nelaborated to fully capture\ntransmission and compu-\ntation dynamics.\n20\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComp.\nSource\nRemark\n[112]\nLatency and\nData Rate\nCentralized\nand\nDistributed Frameworks\nof DRL\nLocal,\nUAV,\nCloud\nModeling data rate with-\nout real-time channel state\ninformation might raise\nconcerns.\n[111]\nDDPG\nEnergy\nHybrid\nDRL\n(DDPG,\nFCNN, and Convex Op-\ntimization)\nLocal,\nEdge,\nand Cloud\nIncomplete\nMDP,\ni.e.,\ntask dynamics, resource\ndynamics,\nand\nchannel\ndynamics are not included\nin MDP.\n[120]\nLatency and\nBandwidth\nAdaptive Video Quality\nTransmission\nLocal,\nEdge,\nCloud\nExplanation\nof\nDDPG\naction,\ni.e.,\ndiscrete-\nto-continuous\naction\nconversion,\ncould\nbe\nemphasized.\n[114]\nLatency\nImproved Clustering Al-\ngorithm Based on Social\nRelation\nLocal,\nEdge,\nCloud,\nand\nVehicles\nClarification could be in-\ncluded about the valid-\nity period of clustering\naggregation and whether\nDDPG updated its state\nafter each aggregation.\n[127]\nEnergy,\nBandwidth,\nand Latency\nOrnstein-Uhlenbeck\nNoise Vector in Action\nfor DDPG\nLocal,\nEdge,\nand Cloud\nSquaring elements in the\nreward function may lead\nto disproportionate penal-\nization.\n[116]\nLatency\nCooperative Edge\nLocal,\nEdge,\nand Cloud\nLack of temporal features\nin the MDP framework\nfor effective caching may\nlead to suboptimal long-\nterm performance.\n[129]\nLatency\nPrime Dual DDPG\nLocal,\nEdge,\nand Cloud\nResults and MDP could\nbe improved with more\ndetails.\n[131]\nTD3\nLatency Op-\ntimization\nEdge-Assisted Learning\nApproach\nLocal,\nEdge,\nand Cloud\nFixed set of tasks suggests\nlimited load and state dy-\nnamics.\n[132]\nPeak Age of\nInformation\n(PAoI)\nUAV Cloud and Opti-\nmization of Age of In-\nformation\nLocal,\nEdge,\nUAV\nContinuous action conver-\nsion demonstrates ambi-\nguity, i.e., overlapping of\ntwo spaces.\n[133]\nResource\nUtilization\nEfficiency\nHierarchical\nDRL\nfor\nService Function Chain\nOffloading\nLocal,\nEdge,\nand Cloud\nThe\ntrade-off\nof\ncon-\nvergence speed between\nheuristics and TD3 could\nbe investigated.\n[134]\nJoint\nOpti-\nmization of\nDelay\nand\nEnergy\nMobility-Aware\nwith\nDynamic Location\nLocal\nand\nEdge\nMDP state representation\ndemonstrates lack of link-\nage between tasks and\ntask vehicles.\n[122]\nA3C\nLatency\nFairness with Relative\nDelay Comparison\nLocal,\nEdge,\nCloud\nExclusion of channel pa-\nrameters in state suggests\nimproper MDP.\n[121]\nPPO\nLatency\nDistributed\nDRL\nand\nExperience Trajectory\nLocal,\nEdge,\nCloud\nMulti-user conflict resolu-\ntion mechanism could be\naddressed.\n21\nAgent\nType\nCite.\nDRL\nMethod\nOptimization\nObj.\nKey\nTechniques\nComp.\nSource\nRemark\n[118]\nLatency\nSeq2Seq-based\nBiGRU\nfor Sequential Features\nLocal,\nEdge,\nCloud\nDecoupling of dependent\nand independent tasks in\nlearning\ncould\nbe\nas-\nsessed.\n[136]\nSAC\nLatency and\nEnergy\nEntropy\nNormalization\nSAC\nLocal\nand\nUAV\nExclusion of channel pa-\nrameters in state suggests\nimproper MDP.\n[135]\nService La-\ntency\nCentralized and Decen-\ntralized Agent\nLocal,\nEdge,\nCloud\nImbalanced\nperformance\nin some distributed agents\nsuggests unfair learning.\nMulti\nAgent\n[128]\nMADQN\n(DTDE)\nEnergy\nand\nLatency\nFederated DQN\nLocal,\nEdge,\nCloud\nUniform MDP presenta-\ntion could be investigated\nfor federated learning.\n[115]\nEnergy\nand\nNumber\nof\nTasks\nIndependent IoT Device\nAgent\nLocal,\nEdge,\nCloud\nModel\nmight\nimprove\nmulti-agent\ncoordination\nand conflict resolution.\n[113]\nMADDQN\n(DTDE)\nEnergy\nand\nLatency\nTrust Value for Security\nLocal,\nEdge,\nCloud\nClarification\nof\nDouble\nDQN could be enhanced\nfor better illustration of\nperformance.\n[119]\nMA Joint\nAction Q-\nLearning\n(DTDE)\nLatency\nJoint Action Q-Learning\nLocal,\nEdge,\nAccess Point\nSimulation results of ex-\ncessive latency of non-\nagent users (cloud center)\nraise concerns about fair\nlearning across agents.\n[117]\nMADDPG\n(CTDE)\nLatency and\nEnergy\nLSTM and BRNN for\nCommunication\nState\nPrediction\nLocal,\nEdge,\nCloud\nThe need for scaling the\nreward function could be\nexplored.\n[110]\nLatency\nCentralized Training and\nDistributed Action\nLocal,\nEdge,\nCloud\nPerformance metrics, in-\ncluding latency, could be\nexamined.\n[124]\nLatency and\nChannel Ac-\ncess\nPrioritized Reward\nLocal,\nEdge,\nCloud\nScaling\nof\nreward\nsuggests more resources\nfor high priority, which\ncould cause unbalanced\nresource allocation.\n[130]\nMASAC\n(DTDE)\nLatency\nand Success\nRate\nFairness\nLocal, Edge\nInter-fairness\namong\nusers could be studied.\n[123]\nMAA3C\n(DTDE)\nDelay\nKnowledge-Driven\nModel\nLocal,\nEdge,\nand BS\nThe MARL section de-\nscribed\nonly\ndecentral-\nized A3C, suggesting an\nexplanation about multi-\nagent coordination.\n[125]\nMAAC\n(CTDE)\nLatency and\nEnergy\nMulti-Objective Formu-\nlation\nUAV\nand\nEdge\nUAV height increase de-\npicts higher task comple-\ntion; this needs to be ex-\nplored as an increase in\nheight suggests more path\nloss.\nVI. LEARNED LESSON, OPEN ISSUES AND FUTURE RESEARCH ASPECTS\nA. Fully Informed MDP and Multiagent POMDP:\nA comprehensive state representation is crucial for effective decision-making in dynamic environments like edge computing\nand vehicular networks. Incomplete MDP formulations that neglect key dynamics such as transmission rates, task processing\n22\ntimes, resource availability, and temporal dependencies, particularly in caching mechanisms, lead to ambiguous state transitions\nand suboptimal decisions. Multi-agent systems face additional challenges due to partial observability, where agents lack full\nstate-space awareness, leading to suboptimal decisions and conflicting actions . The exponential growth of the state space (curse\nof dimensionality) complicates state representation, making optimal decision-making computationally expensive . Overlapping\nobservations between agents can lead to redundant information, further complicating coordination. To mitigate those challenges\nin POMDP and accessibility dificulties to states , proper estimation of belief states with Bayesian inference could enhance the\nactual state presentation even with reduced communication overhead.\nB.\nGame theory Approach challenges:\nGame theory has been widely applied to offloading problems, often leveraging Nash Equilibrium (NE) and Stackelberg\nEquilibrium (SE) for optimal decision-making. However, a key question remains underexplored: how to effectively integrate\nMDP-based DRL within game-theoretic frameworks, particularly in scenarios where agents must determine the optimal schedule\nfor updating their policies while accounting for the fixed strategies of opponents. Additionally, the impact of agent deviations\nfrom equilibrium in result demonstartion is often overlooked, raising concerns about system stability and efficiency. Some studies\nhave employed hierarchical approaches that combine game theory and DRL to address subproblems, but a major challenge\nlies in synchronizing their learning processes, as game-theoretic methods typically converge faster than DRL. Addressing these\nchallenges could serve as a strong foundation for future research in game-theoretic DRL approaches.\nC. Uniform State Action Representation in Federated and Multi-Agent Learning:\nEstablishing a uniform state size representation and action space in both federated and multi-agent learning environments\nis significantly important [128]. This uniformity is essential for effective aggregation and coordination, ensuring scalability\nand generalization across different zones or agents. The absence of such uniformity can lead to asymmetrical learning, where\ndiscrepancies in state and action representations result in inconsistent model updates and inefficiencies in learning processes.\nThis misalignment can prevent models from different zones or agents from properly integrating their learning experiences,\nthereby degrading overall system performance. Future research should investigate methods to standardize inputs and outputs\nor employ meta-learning approaches to address heterogeneity, thereby enhancing robustness and practical applicability.\nD. Handling DAG, Task and Data Dependencies:\nIn the context of vehicular networks and task offloading, the application of the Directed Acyclic Graph (DAG) approach to\nmap task dependencies presents an important area of study, particularly in differentiating dependent and independent subtasks .\nDependent tasks, relying on others’ outputs, could be addressed through sequential learning methods like Seq2Seq models, while\nindependent subtasks may benefit from parallel processing. Exploring how these distinctions impact computational efficiency\nand resource allocation could optimize task offloading strategies by balancing sequential and parallel processing for adaptability\nand scalability in dynamic environments. Additionally, recognizing the distinction between temporal (e.g., location changes,\nspeed) and non-temporal features (e.g., task size, wireless channel state) can improve learning efficiency. Temporal features,\nbest captured using methods like LSTMs, provide insights over time, while non-temporal features contribute differently to\ndecision-making. Decoupling these types of features allows for more focused learning, enhancing both the capture of temporal\ndynamics and the overall system’s adaptability. This approach ensures a comprehensive understanding of system behavior,\nimproving decision-making in dynamic environments.\nE. Reliable Baseline Comparisons and Results\nProviding comprehensive baseline comparisons is essential for conducting fair and reliable evaluations in reinforcement\nlearning research. Benchmarking new algorithms against established methods allows for a clear understanding of their effec-\ntiveness across diverse scenarios, ensuring practical applicability and robustness. However, numerous studies face challenges\nwith inconsistent or incomplete baseline comparisons, which can skew results and obscure the true impact of new methods. In\nmulti-agent reinforcement learning (MARL), the absence of dedicated multi-agent baselines further complicates evaluations,\nas the complexities and dynamics inherent to multi-agent systems are not fully captured, leading to biased assessments .\nAdditionally, the omission of essential performance metrics like energy consumption, task efficiency, and system throughput\nlimits the ability to assess models holistically, reducing the depth of their evaluations . Incomplete data prevents a thorough\nanalysis of how models perform across varying conditions, impacting the ability to generalize findings.\nFurthermore, unstable learning processes, characterized by reward fluctuations and inconsistent prioritization, reduce the\nreliability of models . Inadequate exploration strategies, such as fixed exploration rates or uniform convergence that fail to adapt\nto dynamic environments, weaken the adaptability and robustness of the models in real-world applications . These challenges\nhighlight the importance of incorporating thorough baseline comparisons, flexible exploration strategies, and comprehensive\nperformance metrics to ensure more accurate, scalable, and reliable reinforcement learning evaluations in dynamic environments.\n23\nF. Reward Function\nReward function scaling concerning different performances, such as latency and energy, is crucial for effective learning .\nFor instance, squaring the energy or latency in the reward function can lead to disproportionate reward changes, which can\nundermine learning effectiveness. Proper scaling ensures balanced reward representation, supporting better learning outcomes.\nMoreover, the inclusion of energy and latency terms without considering any coefficient leads to an imbalance in reward\ncontribution, which undermines the joint objective of optimizing latency and energy.\nMoreover, a subject of investigation is the impact of relative reward improvements in consecutive time steps that could\nrisk agents focusing on short-term gains at the expense of long-term outcomes, particularly in environments with fluctuating\nconditions, such as varying traffic and intermittent communication. To mitigate this, enhancing reward signals with more\nfrequent and informative feedback could help agents distinguish between transient conditions and stable trends. The reliance\non relative gains may reinforce short-term improvements, while absolute rewards based on overarching system goals, such as\nminimizing latency or maximizing throughput, would better prevent agents from becoming stuck in local optima. Investigating\nthe incorporation more measurable techniques i.e moving averages into reward functions is necessary to balance short-\nterm fluctuations and long-term optimization. This adjustment could lead to more stable, efficient performance in dynamic\nenvironments, ensuring both immediate and sustained improvements.\nG. Synchronization and Coordination among Multi Agent\nOne of the key lessons learned from the reviewed studies is the critical importance of synchronization and coordination in\nmulti-agent systems and distributed networks. When synchronization mechanisms are absent or poorly defined, inefficiencies and\nscalability challenges often arise, particularly as system complexity increases. Agents that fail to align their actions effectively\ncan create bottlenecks, especially in environments where multiple agents or devices attempt to offload tasks or access shared\nresources simultaneously, leading to suboptimal system performance. Another crucial takeaway is the need for effective resource\ncontention management. In scenarios where multiple users offload tasks to the same server or share communication channels,\nthe absence of strategies like TDMA or FDMA can result in conflicts, causing delays and reducing task execution efficiency.\nProper mechanisms for managing shared resources are essential to prevent such issues and ensure smooth system operation.\nFinally, central coordination strategies that rely on global state information can enhance decision-making by aligning actions\nacross the system. However, these methods must be carefully balanced to address scalability challenges as the number of\nagents might increase in multi-agent systems. To mitigate these issues, adaptive coordination frameworks, dynamic resource\nallocation techniques, and decentralized learning strategies can be explored to enhance efficiency and scalability.\nH. Handling Continuous and Discrete Action Spaces:\nDeterministic policy grdaient method like DDPG, PPO , and TRPO provide a deterministic policy, producing a single action\nrather than a probability distribution, making it well-suited for continuous action spaces. However, applying DDPG to discrete\naction spaces presents challenges, as it inherently generates continuous outputs and relies on adding noise (e.g., Ornstein-\nUhlenbeck or Gaussian noise) for exploration. This noise can lead to frequent switching between discrete actions, potentially\nunbalalnce exportation , destabilizing learning and reducing convergence efficiency. A key concern in such implementations is\nensuring balanced exploration across all possible actions, as improper exploration can result in biased action selection, limiting\nthe model’s ability to learn an optimal policy. Moreover, the misalignment between discrete and continuous action spaces can\nlead to suboptimal performance, requiring careful action space design. Focusing on this specific concern could be a potential\nstrt to work in Deterministic policy gradient implementation for discrete action space.\nI. Multi Objective Multi agent Learning:\nIn a multi-objective multi-agent system, decision dependencies between agents require careful coordination within a unified\nglobal time step to ensure efficient task execution. Actions taken by one agent, such as a vehicle offloading a task to a base\nstation (BS), directly impact subsequent decisions by other agents, such as the BS determining whether to process the task\nlocally or offload it further. Without synchronized decision-making, agents may operate asynchronously, leading to inefficiencies,\nincreased latency, or resource misallocation. For instance, if a BS offloads a task to a UAV, but the UAV’s learning process is\nunsynchronized, the task may remain idle, causing delays and disrupting the overall workflow.\nTo mitigate these challenges, implementing a global time step can help synchronize decision-making, ensuring agents interact\nefficiently and maintain system stability. Additionally, alternative solutions such as event-triggered coordination, where agents\nmake decisions based on state changes rather than fixed time steps, can enhance responsiveness. Asynchronous learning\nwith adaptive delays can also be explored, allowing agents to adjust their update frequency based on network conditions or\ncomputational load. These approaches help balance synchronization while maintaining flexibility, ultimately improving multi-\nagent coordination in dynamic environments.\n24\nVII. CONCLUSION\nIn conclusion, this research underscores the importance of several key factors in enhancing learning and decision-making in\ndynamic, multi-agent, and federated environments. Uniform state representation and action spaces are essential for scalability\nand coordination, while comprehensive baseline comparisons and well-informed state representations ensure robust evaluation\nand decision-making. The precise handling of reward functions, especially in balancing short-term and long-term objectives, is\ncrucial. Additionally, integrating real-time data, promoting reward sharing in multi-agent systems, and ensuring fair comparisons\nwith appropriate baselines are vital for improving system performance. Addressing these challenges paves the way for more\neffective and scalable learning frameworks in complex, dynamic environments.\nREFERENCES\n[1] I. W. Damaj, J. K. Yousafzai and H. T. Mouftah, ”Future Trends in Connected and Autonomous Vehicles: Enabling Communications and Processing\nTechnologies,” in IEEE Access, vol. 10, pp. 42334-42345, 2022, doi: 10.1109/ACCESS.2022.3168320.\n[2] K. Zheng, Q. Zheng, P. Chatzimisios, W. Xiang and Y. Zhou, ”Heterogeneous Vehicular Networking: A Survey on Architecture, Challenges, and\nSolutions,” in IEEE Communications Surveys & Tutorials, vol. 17, no. 4, pp. 2377-2396, Fourthquarter 2015, doi: 10.1109/COMST.2015.2440103.\n[3] Y. Mao, C. You, J. Zhang, K. Huang and K. B. Letaief, ”A Survey on Mobile Edge Computing: The Communication Perspective,” in IEEE Communications\nSurveys & Tutorials, vol. 19, no. 4, pp. 2322-2358, Fourthquarter 2017, doi: 10.1109/COMST.2017.2745201.\n[4] J. Liu et al., ”RL/DRL Meets Vehicular Task Offloading Using Edge and Vehicular Cloudlet: A Survey,” in IEEE Internet of Things Journal, vol. 9, no.\n11, pp. 8315-8338, 1 June1, 2022, doi: 10.1109/JIOT.2022.3155667.\n[5] P. Yang, N. Zhang, S. Zhang, L. Yu, J. Zhang, X. Shen, ”Content popularity prediction towards location-aware mobile edge caching”, IEEE Transactions\non Multimedia, vol.21, no.4, pp.915-929, 2018.\n[6] L. Ale, N. Zhang, H. Wu, D. Chen, T. Han, ”Online proactive caching in mobile edge computing using bidirectional deep recurrent neural network”,\nIEEE Internet of Things Journal, vol.6, no.3, pp.5520–5530, 2019.\n[7] L. Hou, M. A. Gregory and S. Li, ”A Survey of Multi-Access Edge Computing and Vehicular Networking,” in IEEE Access, vol. 10, pp. 123436-123451,\n2022, doi: 10.1109/ACCESS.2022.3224032.\n[8] L. A. Haibeh, M. C. E. Yagoub and A. Jarray, ”A Survey on Mobile Edge Computing Infrastructure: Design, Resource Management, and Optimization\nApproaches,” in IEEE Access, vol. 10, pp. 27591-27610, 2022, doi: 10.1109/ACCESS.2022.3152787.\n[9] A. Filali, A. Abouaomar, S. Cherkaoui, A. Kobbane and M. Guizani, ”Multi-Access Edge Computing: A Survey,” in IEEE Access, vol. 8, pp. 197017-\n197046, 2020, doi: 10.1109/ACCESS.2020.3034136.\n[10] Y. Yu, ”Mobile edge computing towards 5G: Vision, recent progress, and open challenges,” in China Communications, vol. 13, no. Supplement2, pp.\n89-99, 2016, doi: 10.1109/CC.2016.7833463.\n[11] A. Alawadhi, A. Almogahed and E. Azrag, ”Towards Edge Computing for 6G Internet of Everything: Challenges and Opportunities,” 2023 1st International\nConference on Advanced Innovations in Smart Cities (ICAISC), Jeddah, Saudi Arabia, 2023, pp. 1-6, doi: 10.1109/ICAISC56366.2023.10085007.\n[12] K. Jurczenia and J. Rak, ”A Survey of Vehicular Network Systems for Road Traffic Management,” in IEEE Access, vol. 10, pp. 42365-42385, 2022,\ndoi: 10.1109/ACCESS.2022.3168354.\n[13] E. Moradi-Pari, D. Tian, M. Bahramgiri, S. Rajab and S. Bai, ”DSRC Versus LTE-V2X: Empirical Performance Analysis of Direct Vehicular\nCommunication Technologies,” in IEEE Transactions on Intelligent Transportation Systems, vol. 24, no. 5, pp. 4889-4903, May 2023, doi:\n10.1109/TITS.2023.3247339.\n[14] S. Wang, J. Xu, N. Zhang and Y. Liu, ”A Survey on Service Migration in Mobile Edge Computing,” in IEEE Access, vol. 6, pp. 23511-23528, 2018,\ndoi: 10.1109/ACCESS.2018.2828102.\n[15] Y. Xu, W. Liang, and L. Wang, ”Deep Reinforcement Learning for Resource Management in Vehicular Networks,” IEEE Wireless Communications, vol.\n28, no. 2, pp. 120-126, April 2021.\n[16] W. M. Danquah and D. T. Altilar, ”Vehicular Cloud Resource Management, Issues and Challenges: A Survey,” in IEEE Access, vol. 8, pp. 180587-180607,\n2020, doi: 10.1109/ACCESS.2020.3027637.\n[17] N. Abbas, Y. Zhang, A. Taherkordi and T. Skeie, ”Mobile Edge Computing: A Survey,” in IEEE Internet of Things Journal, vol. 5, no. 1, pp. 450-465,\nFeb. 2018, doi: 10.1109/JIOT.2017.2750180.\n[18] W. Shi, J. Cao, Q. Zhang, Y. Li and L. Xu, ”Edge Computing: Vision and Challenges,” in IEEE Internet of Things Journal, vol. 3, no. 5, pp. 637-646,\nOct. 2016, doi: 10.1109/JIOT.2016.2579198.\n[19] H. Nouhas, A. Belangour and M. Nassar, ”Cloud and Edge Computing Architectures: A Survey,” 2023 IEEE 11th Conference on Systems, Process &\nControl (ICSPC), Malacca, Malaysia, 2023, pp. 210-215, doi: 10.1109/ICSPC59664.2023.10420123.\n[20] 1. Deepak, M. K. Upadhyay and M. Alam, ”Edge Computing: Architecture, Application, Opportunities, and Challenges,” 2023 3rd International\nConference on Technological Advancements in Computational Sciences (ICTACS), Tashkent, Uzbekistan, 2023, pp. 695-702, doi: 10.1109/IC-\nTACS59847.2023.10390171.\n[21] B. Liu, Z. Luo, H. Chen and C. Li, ”A Survey of State-of-the-art on Edge Computing: Theoretical Models, Technologies, Directions, and Development\nPaths,” in IEEE Access, vol. 10, pp. 54038-54063, 2022, doi: 10.1109/ACCESS.2022.3176106.\n[22] S. Singh and D. Ho Kim, “Profit optimization for mobile edge computing using genetic algorithm,” IEEE Region 10 Symposium (TENSYMP), 2021.\n[23] T. Zhou, Y. Yue, D. Qin, X. Nie, X. Li, and C. Li, “Mobile device association and resource allocation in HCNs with mobile edge computing and\ncaching,” IEEE Systems Journal, vol. 17, no. 1, pp. 976-987, March 2023.\n[24] M. Zakarya et al., “epcAware: A game-based, energy, performance and cost-efficient resource management technique for multi-access edge computing,”\nIEEE Transactions on Services Computing, vol. 15, no. 3, pp. 1634-1648, 1 May-June 2022.\n[25] T. -H. Nguyen and L. Park, ”A Survey on Deep Reinforcement Learning-driven Task Offloading in Aerial Access Networks,” 2022 13th Interna-\ntional Conference on Information and Communication Technology Convergence (ICTC), Jeju Island, Korea, Republic of, 2022, pp. 822-827, doi:\n10.1109/ICTC55196.2022.9952687.\n[26] W. Chen, X. Qiu, T. Cai, H. -N. Dai, Z. Zheng and Y. Zhang, ”Deep Reinforcement Learning for Internet of Things: A Comprehensive Survey,” in\nIEEE Communications Surveys & Tutorials, vol. 23, no. 3, pp. 1659-1692, thirdquarter 2021, doi: 10.1109/COMST.2021.3073036.\n[27] I. Althamary, C. -W. Huang and P. Lin, ”A Survey on Multi-Agent Reinforcement Learning Methods for Vehicular Networks,” 2019 15th International\nWireless Communications & Mobile Computing Conference (IWCMC), Tangier, Morocco, 2019, pp. 1154-1159, doi: 10.1109/IWCMC.2019.8766739.\n[28] H. Zhu, Y. Cao, W. Wang, T. Jiang and S. Jin, ”Deep Reinforcement Learning for Mobile Edge Caching: Review, New Features, and Open Issues,” in\nIEEE Network, vol. 32, no. 6, pp. 50-57, November/December 2018, doi: 10.1109/MNET.2018.1800109.\n[29] T. Li et al., ”Applications of Multi-Agent Reinforcement Learning in Future Internet: A Comprehensive Survey,” in IEEE Communications Surveys &\nTutorials, vol. 24, no. 2, pp. 1240-1279, Secondquarter 2022, doi: 10.1109/COMST.2022.3160697.\n[30] L. Liu, C. Chen, Q. Pei, S. Maharjan, and Y. L. Zhang, “Vehicular edge computing and networking: A survey,” Mobile Netw. Appl., vol. 26, no. 3, pp.\n1145–1168, 2021.\n25\n[31] P. Mach and Z. Becvar, ”Mobile Edge Computing: A Survey on Architecture and Computation Offloading,” in IEEE Communications Surveys &\nTutorials, vol. 19, no. 3, pp. 1628-1656, thirdquarter 2017, doi: 10.1109/COMST.2017.2682318.\n[32] M. Muniswamaiah, T. Agerwala and C. C. Tappert, ”Fog Computing and the Internet of Things (IoT): A Review,” 2021 8th IEEE International\nConference on Cyber Security and Cloud Computing (CSCloud)/2021 7th IEEE International Conference on Edge Computing and Scalable Cloud\n(EdgeCom), Washington, DC, USA, 2021, pp. 10-12, doi: 10.1109/CSCloud-EdgeCom52276.2021.00012.\n[33] X. Huang, R. Yu, J. Liu and L. Shu, ”Parked Vehicle Edge Computing: Exploiting Opportunistic Resources for Distributed Mobile Applications,” in\nIEEE Access, vol. 6, pp. 66649-66663, 2018, doi: 10.1109/ACCESS.2018.2879578.\n[34] Y. Bai, H. Zhao, X. Zhang, Z. Chang, R. J¨antti and K. Yang, ”Toward Autonomous Multi-UAV Wireless Network: A Survey of Reinforce-\nment Learning-Based Approaches,” in IEEE Communications Surveys & Tutorials, vol. 25, no. 4, pp. 3038-3067, Fourthquarter 2023, doi:\n10.1109/COMST.2023.3323344.\n[35] Y. Mao, C. You, J. Zhang, K. Huang and K. B. Letaief, ”A Survey on Mobile Edge Computing: The Communication Perspective,” in IEEE Communications\nSurveys & Tutorials, vol. 19, no. 4, pp. 2322-2358, Fourthquarter 2017, doi: 10.1109/COMST.2017.2745201.\n[36] X. Hou, Y. Li, M. Chen, D. Wu, D. Jin and S. Chen, ”Vehicular Fog Computing: A Viewpoint of Vehicles as the Infrastructures,” in IEEE Transactions\non Vehicular Technology, vol. 65, no. 6, pp. 3860-3873, June 2016, doi: 10.1109/TVT.2016.2532863.\n[37] J. Cui, L. Wei, H. Zhong, J. Zhang, Y. Xu and L. Liu, ”Edge Computing in VANETs-An Efficient and Privacy-Preserving Cooperative Downloading\nScheme,” in IEEE Journal on Selected Areas in Communications, vol. 38, no. 6, pp. 1191-1204, June 2020, doi: 10.1109/JSAC.2020.2986617.\n[38] M. A. Al-shareeda, M. A. Alazzawi, M. Anbar, S. Manickam and A. K. Al-Ani, ”A Comprehensive Survey on Vehicular Ad Hoc Networks (VANETs),”\n2021 International Conference on Advanced Computer Applications (ACA), Maysan, Iraq, 2021, pp. 156-160, doi: 10.1109/ACA52198.2021.9626779.\n[39] L. N. T. Huynh and E. -N. Huh, ”UAV-Enhanced Edge Intelligence: A Survey,” 2022 6th International Conference on Computing Methodologies and\nCommunication (ICCMC), Erode, India, 2022, pp. 42-47, doi: 10.1109/ICCMC53470.2022.9753892.\n[40] C. Lin, G. Han, S. B. H. Shah, Y. Zou and L. Gou, ”Integrating Mobile Edge Computing Into Unmanned Aerial Vehicle Networks: An Sdn-Enabled\nArchitecture,” in IEEE Internet of Things Magazine, vol. 4, no. 4, pp. 18-23, December 2021, doi: 10.1109/IOTM.001.2100070.\n[41] Y. Xu, G. Gui, H. Gacanin and F. Adachi, ”A Survey on Resource Allocation for 5G Heterogeneous Networks: Current Research, Future Trends, and\nChallenges,” in IEEE Communications Surveys & Tutorials, vol. 23, no. 2, pp. 668-695, Secondquarter 2021, doi: 10.1109/COMST.2021.3059896.\n[42] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018.\n[43] R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957.\n[44] M. Riedmiller, ”Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method,” in Proc. 16th European\nConf. Machine Learning (ECML 2005), pp. 317-328, 2005.\n[45] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, ”Playing Atari with Deep Reinforcement Learning,”\narXiv preprint arXiv:1312.5602, 2013.\n[46] H. van Hasselt, A. Guez, and D. Silver, ”Deep Reinforcement Learning with Double Q-learning,” arXiv preprint arXiv:1509.06461, 2015.\n[47] V. R. Konda and J. N. Tsitsiklis, ”Actor-Critic Algorithms,” in Advances in Neural Information Processing Systems (NIPS), 1999.\n[48] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel, ”Trust Region Policy Optimization,” in Proceedings of the 32nd International Conference\non Machine Learning (ICML 2015), Lille, France, Jul. 2015.\n[49] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, ”Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\n[50] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, ”Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic\nActor,” in Proceedings of the 35th International Conference on Machine Learning (ICML 2018), Stockholm, Sweden, Jul. 2018.\n[51] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, ”Continuous control with deep reinforcement learning,”\narXiv preprint arXiv:1509.02971, 2015.\n[52] S. Fujimoto, H. van Hoof, and D. Meger, ”Addressing function approximation error in actor-critic methods,” in Proc. 35th Int. Conf. Mach. Learn.\n(ICML), Stockholm, Sweden, Jul. 2018, pp. 1392–1401. [Online]. Available: arXiv:1802.09477.\n[53] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, ... and D. Silver, ”Value-Decomposition Networks For Cooperative\nMulti-Agent Learning,” in Proc. 17th Int. Conf. Autonomous Agents and MultiAgent Systems, 2018, pp. 2085-2087.\n[54] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, ”Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,” in\nAdvances in Neural Information Processing Systems, 2017, pp. 6379-6390.\n[55] A. M. Seid, G. O. Boateng, S. Anokye, T. Kwantwi, G. Sun and G. Liu, ”Collaborative Computation Offloading and Resource Allocation in Multi-UAV-\nAssisted IoT Networks: A Deep Reinforcement Learning Approach,” in IEEE Internet of Things Journal, vol. 8, no. 15, pp. 12203-12218, 1 Aug.1,\n2021, doi: 10.1109/JIOT.2021.3063188.\n[56] T. Cai et al., ”Cooperative Data Sensing and Computation Offloading in UAV-Assisted Crowdsensing With Multi-Agent Deep Reinforcement Learning,”\nin IEEE Transactions on Network Science and Engineering, vol. 9, no. 5, pp. 3197-3211, 1 Sept.-Oct. 2022, doi: 10.1109/TNSE.2021.3121690.\n[57] J. Cui, Y. Liu, and A. Nallanathan, “Multi-agent reinforcement learning-based resource allocation for UAV networks,” IEEE Trans. Wireless Commun.,\nvol. 19, no. 2, pp. 729–743, Feb. 2020.\n[58] P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. Munoz de Cote, ”A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity,”\narXiv preprint arXiv:1707.09183, Jul. 2017. [Online]. Available: https://arxiv.org/abs/1707.09183\n[59] K. Gogineni, P. Wei, T. Lan, and G. Venkataramani, “Scalability Bottlenecks in Multi-Agent Reinforcement Learning Systems,” arXiv preprint\narXiv:2302.05007, 2023..\n[60] S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed, “Distributed policy evaluation under multiple behavior strategies,” IEEE Trans. Autom. Control, vol.\n60, no. 5, pp. 1260–1274, May 2015.\n[61] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed cooperative-competitive environments,” 2017.\n[62] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy gradients,” in Proc. AAAI Conf. Artif. Intell., vol.\n32, 2018, pp. 1–10.\n[63] P. Sunehag et al., “Value-decomposition networks for cooperative multi-agent learning,” 2017, arXiv:1706.05296.\n[64] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked agents,” in Proc. Int. Conf.\nMach. Learn., 2018, pp. 5872–5881.\n[65] I. Lee and D. K. Kim, ”Decentralized Multi-Agent DQN-Based Resource Allocation for Heterogeneous Traffic in V2X Communications,” in IEEE\nAccess, vol. 12, pp. 3070-3084, 2024, doi: 10.1109/ACCESS.2023.3349350.\n[66] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked agents,” in Proc. Int. Conf.\nMach. Learn., 2018, pp. 5872–5881.\n[67] G. Jain, A. Kumar and S. A. Bhat, ”Recent Developments of Game Theory and Reinforcement Learning Approaches: A Systematic Review,” in IEEE\nAccess, vol. 12, pp. 9999-10011, 2024, doi: 10.1109/ACCESS.2024.3352749.\n[68] Z. Yan, H. Zhao, L. Li and S. Galland, ”Evolutionary game theory combined with reinforcement learning synthesis - A comprehensive survey,” 2024\n36th Chinese Control and Decision Conference (CCDC), Xi’an, China, 2024, pp. 1077-1082, doi: 10.1109/CCDC62350.2024.10587977.\n[69] J. Zhao, M. Kong, Q. Li and X. Sun, ”Contract-Based Computing Resource Management via Deep Reinforcement Learning in Vehicular Fog Computing,”\nin IEEE Access, vol. 8, pp. 3319-3329, 2020, doi: 10.1109/ACCESS.2019.2963051.\n[70] Y. Ju et al., ”Joint Secure Offloading and Resource Allocation for Vehicular Edge Computing Network: A Multi-Agent Deep Reinforcement Learning\nApproach,” in IEEE Transactions on Intelligent Transportation Systems, vol. 24, no. 5, pp. 5555-5569, May 2023, doi: 10.1109/TITS.2023.3242997.\n26\n[71] I. Khan, X. Tao, G. M. S. Rahman, W. U. Rehman and T. Salam, ”Advanced Energy-Efficient Computation Offloading Using Deep Reinforcement\nLearning in MTC Edge Computing,” in IEEE Access, vol. 8, pp. 82867-82875, 2020, doi: 10.1109/ACCESS.2020.2991057.\n[72] C. Wang, W. Lu, S. Peng, Y. Qu, G. Wang and S. Yu, ”Modeling on Energy-Efficiency Computation Offloading Using Probabilistic Action Generating,”\nin IEEE Internet of Things Journal, vol. 9, no. 20, pp. 20681-20692, 15 Oct.15, 2022, doi: 10.1109/JIOT.2022.3175760.\n[73] S. Zhao, Y. Liu, S. Gong, B. Gu, R. Fan and B. Lyu, ”Computation Offloading and Beamforming Optimization for Energy Minimization in Wireless-\nPowered IRS-Assisted MEC,” in IEEE Internet of Things Journal, vol. 10, no. 22, pp. 19466-19478, 15 Nov.15, 2023, doi: 10.1109/JIOT.2023.3265011.\n[74] X. Zhang, A. Pal and S. Debroy, ”Deep Reinforcement Learning Based Energy-efficient Task Offloading for Secondary Mobile Edge Sys-\ntems,” 2020 IEEE 45th LCN Symposium on Emerging Topics in Networking (LCN Symposium), Sydney, Australia, 2020, pp. 48-59,\ndoi:10.1109/LCNSymposium50271.2020.9363256.\n[75] B. Guo, X. Zhang, Y. Wang and H. Yang, ”Deep-Q-Network-Based Multimedia Multi-Service QoS Optimization for Mobile Edge Computing Systems,”\nin IEEE Access, vol. 7, pp. 160961-160972, 2019, doi: 10.1109/ACCESS.2019.2951219.\n[76] A. Gao, S. Zhang, Y. Hu, W. Liang and S. X. Ng, ”Game-Combined Multi-Agent DRL for Tasks Offloading in Wireless Powered MEC Networks,” in\nIEEE Transactions on Vehicular Technology, vol. 72, no. 7, pp. 9131-9144, July 2023, doi: 10.1109/TVT.2023.3250274.\n[77] J. Xu, B. Ai, L. Wu, Y. Zhang, W. Wang and H. Li, ”Deep Reinforcement Learning for Computation Rate Maximization in RIS-Enabled Mobile Edge\nComputing,” in IEEE Transactions on Vehicular Technology, vol. 73, no. 7, pp. 10862-10866, July 2024, doi: 10.1109/TVT.2024.3387759.\n[78] C. Zheng et al., ”Multi-Agent Collaborative Optimization of UAV Trajectory and Latency-Aware DAG Task Offloading in UAV-Assisted MEC,” in IEEE\nAccess, vol. 12, pp. 42521-42534, 2024, doi: 10.1109/ACCESS.2024.3378512.\n[79] Y. Zhan, S. Guo, P. Li and J. Zhang, ”A Deep Reinforcement Learning Based Offloading Game in Edge Computing,” in IEEE Transactions on Computers,\nvol. 69, no. 6, pp. 883-893, 1 June 2020, doi: 10.1109/TC.2020.2969148.\n[80] B. Lv, C. Yang, X. Chen, Z. Yao and J. Yang, ”Task Offloading and Serving Handover of Vehicular Edge Computing Networks Based on Trajectory\nPrediction,” in IEEE Access, vol. 9, pp. 130793-130804, 2021, doi: 10.1109/ACCESS.2021.3112077.\n[81] J. Liu, N. Liu, L. Liu, S. Li, H. Zhu and P. Zhang, ”A Proactive Stable Scheme for Vehicular Collaborative Edge Computing,” in IEEE Transactions on\nVehicular Technology, vol. 72, no. 8, pp. 10724-10736, Aug. 2023, doi: 10.1109/TVT.2023.3255213.\n[82] J. Shi, J. Du, J. Wang, J. Wang, and J. Yuan, “Priority-aware task offloading in vehicular fog computing based on deep reinforcement learning,” IEEE\nTrans. Veh. Technol., vol. 69, no. 12, pp. 16067–16081, Dec. 2020.\n[83] H. Tang, H. Wu, G. Qu and R. Li, ”Double Deep Q-Network Based Dynamic Framing Offloading in Vehicular Edge Computing,” in IEEE Transactions\non Network Science and Engineering, vol. 10, no. 3, pp. 1297-1310, 1 May-June 2023, doi: 10.1109/TNSE.2022.3172794.\n[84] A. Uddin, A. H. Sakr and N. Zhang, ”Prioritized Task Offloading in Vehicular Edge Computing Using Deep Reinforcement Learning,” 2024 IEEE 99th\nVehicular Technology Conference (VTC2024-Spring), Singapore, Singapore, 2024, pp. 1-6, doi: 10.1109/VTC2024-Spring62846.2024.10683050.\n[85] X. Xu et al., ”Service Offloading With Deep Q-Network for Digital Twinning-Empowered Internet of Vehicles in Edge Computing,” in IEEE Transactions\non Industrial Informatics, vol. 18, no. 2, pp. 1414-1423, Feb. 2022, doi: 10.1109/TII.2020.3040180.\n[86] X. Peng et al., ”Deep Reinforcement Learning for Shared Offloading Strategy in Vehicle Edge Computing,” in IEEE Systems Journal, vol. 17, no. 2,\npp. 2089-2100, June 2023, doi: 10.1109/JSYST.2022.3190926.\n[87] H. Maleki, M. Bas¸aran and L. Durak-Ata, ”Handover-Enabled Dynamic Computation Offloading for Vehicular Edge Computing Networks,” in IEEE\nTransactions on Vehicular Technology, vol. 72, no. 7, pp. 9394-9405, July 2023, doi: 10.1109/TVT.2023.3247889.\n[88] M. Li, J. Gao, L. Zhao and X. Shen, ”Deep Reinforcement Learning for Collaborative Edge Computing in Vehicular Networks,” in IEEE Transactions\non Cognitive Communications and Networking, vol. 6, no. 4, pp. 1122-1135, Dec. 2020, doi: 10.1109/TCCN.2020.3003036.\n[89] K. Zhang, J. Cao, H. Liu, S. Maharjan and Y. Zhang, ”Deep Reinforcement Learning for Social-Aware Edge Computing and Caching in Urban\nInformatics,” in IEEE Transactions on Industrial Informatics, vol. 16, no. 8, pp. 5467-5477, Aug. 2020, doi: 10.1109/TII.2019.2953189.\n[90] L. Zhao et al., ”MESON: A Mobility-Aware Dependent Task Offloading Scheme for Urban Vehicular Edge Computing,” in IEEE Transactions on Mobile\nComputing, vol. 23, no. 5, pp. 4259-4272, May 2024, doi: 10.1109/TMC.2023.3289611.\n[91] Y. Deng et al., ”Resource Provisioning for Mitigating Edge DDoS Attacks in MEC-Enabled SDVN,” in IEEE Internet of Things Journal, vol. 9, no. 23,\npp. 24264-24280, 1 Dec.1, 2022, doi: 10.1109/JIOT.2022.3189975.\n[92] Q. Yuan, J. Li, H. Zhou, T. Lin, G. Luo and X. Shen, ”A Joint Service Migration and Mobility Optimization Approach for Vehicular Edge Computing,”\nin IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9041-9052, Aug. 2020, doi: 10.1109/TVT.2020.2999617.\n[93] K. Zhang, J. Cao and Y. Zhang, ”Adaptive Digital Twin and Multiagent Deep Reinforcement Learning for Vehicular Edge Computing and Networks,”\nin IEEE Transactions on Industrial Informatics, vol. 18, no. 2, pp. 1405-1413, Feb. 2022, doi: 10.1109/TII.2021.3088407.\n[94] X. Huang, S. Leng, S. Maharjan and Y. Zhang, ”Multi-Agent Deep Reinforcement Learning for Computation Offloading and Interference Coordination\nin Small Cell Networks,” in IEEE Transactions on Vehicular Technology, vol. 70, no. 9, pp. 9282-9293, Sept. 2021, doi: 10.1109/TVT.2021.3096928.\n[95] X. Huang, L. He, X. Chen, L. Wang and F. Li, ”Revenue and Energy Efficiency-Driven Delay-Constrained Computing Task Offloading and Resource\nAllocation in a Vehicular Edge Computing Network: A Deep Reinforcement Learning Approach,” in IEEE Internet of Things Journal, vol. 9, no. 11,\npp. 8852-8868, 1 June1, 2022, doi: 10.1109/JIOT.2021.3116108.\n[96] K. Li, X. Wang, Q. He, B. Yi, A. Morichetta and M. Huang, ”Cooperative Multiagent Deep Reinforcement Learning for Computation Offloading: A Mobile\nNetwork Operator Perspective,” in IEEE Internet of Things Journal, vol. 9, no. 23, pp. 24161-24173, 1 Dec.1, 2022, doi: 10.1109/JIOT.2022.3189445.\n[97] Z. Zhou, Y. Wu and J. Hou, ”QoE-Guaranteed Heterogeneous Task Offloading with Deep Reinforcement Learning in Edge Computing,”\n2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS), Chengdu, China, 2022, pp. 558-564, doi:\n10.1109/CCIS57298.2022.10016367.\n[98] W. Fan, Y. Zhang, G. Zhou and Y. Liu, ”Deep Reinforcement Learning-Based Task Offloading for Vehicular Edge Computing With Flexible RSU-RSU\nCooperation,” in IEEE Transactions on Intelligent Transportation Systems, vol. 25, no. 7, pp. 7712-7725, July 2024, doi: 10.1109/TITS.2024.3349546.\n[99] C. Liu, F. Tang, Y. Hu, K. Li, Z. Tang and K. Li, ”Distributed Task Migration Optimization in MEC by Extending Multi-Agent Deep Reinforcement Learn-\ning Approach,” in IEEE Transactions on Parallel and Distributed Systems, vol. 32, no. 7, pp. 1603-1614, 1 July 2021, doi: 10.1109/TPDS.2020.3046737.\n[100] H. Zhang, H. Zhao, R. Liu, A. Kaushik, X. Gao and S. Xu, ”Collaborative Task Offloading Optimization for Satellite Mobile Edge Computing Using\nMulti-Agent Deep Reinforcement Learning,” in IEEE Transactions on Vehicular Technology, doi: 10.1109/TVT.2024.3405642.\n[101] A. Gao, Q. Wang, W. Liang and Z. Ding, ”Game Combined Multi-Agent Reinforcement Learning Approach for UAV Assisted Offloading,” in IEEE\nTransactions on Vehicular Technology, vol. 70, no. 12, pp. 12888-12901, Dec. 2021, doi: 10.1109/TVT.2021.3121281.\n[102] H. Yu, S. Leng and F. Wu, ”Joint Cooperative Computation Offloading and Trajectory Optimization in Heterogeneous UAV-Swarm-Enabled Aerial\nEdge Computing Networks,” in IEEE Internet of Things Journal, vol. 11, no. 10, pp. 17700-17711, 15 May15, 2024, doi: 10.1109/JIOT.2024.3362321.\n[103] S. Tan, B. Chen, D. Liu, J. Zhang and L. Hanzo, ”Communication-Assisted Multi-Agent Reinforcement Learning Improves Task-Offloading\nin UAV-Aided Edge-Computing Networks,” in IEEE Wireless Communications Letters, vol. 12, no. 12, pp. 2233-2237, Dec. 2023, doi:\n10.1109/LWC.2023.3316794.\n[104] Y. Hou, Z. Wei, R. Zhang, X. Cheng and L. Yang, ”Hierarchical Task Offloading for Vehicular Fog Computing Based on Multi-Agent Deep Reinforcement\nLearning,” in IEEE Transactions on Wireless Communications, vol. 23, no. 4, pp. 3074-3085, April 2024, doi: 10.1109/TWC.2023.3305321.\n[105] Z. Cheng, M. Min, Z. Gao and L. Huang, ”Joint Task Offloading and Resource Allocation for Mobile Edge Computing in Ultra-Dense Network,”\nGLOBECOM 2020 - 2020 IEEE Global Communications Conference, Taipei, Taiwan, 2020, pp. 1-6, doi: 10.1109/GLOBECOM42002.2020.9322099.\n[106] H. Lu, C. Gu, F. Luo, W. Ding, S. Zheng and Y. Shen, ”Optimization of Task Offloading Strategy for Mobile Edge Computing Based on Multi-Agent\nDeep Reinforcement Learning,” in IEEE Access, vol. 8, pp. 202573-202584, 2020, doi: 10.1109/ACCESS.2020.3036416.\n27\n[107] X. Li, Y. Qin, J. Huo and W. Huangfu, ”Computation Offloading and Trajectory Planning of Multi-UAV-Enabled MEC: A Knowledge-Assisted\nMultiagent Reinforcement Learning Approach,” in IEEE Transactions on Vehicular Technology, vol. 73, no. 5, pp. 7077-7088, May 2024, doi:\n10.1109/TVT.2023.3338612.\n[108] J. Wang, X. Zhang, X. He and Y. Sun, ”Bandwidth Allocation and Trajectory Control in UAV-Assisted IoV Edge Computing Using Multiagent\nReinforcement Learning,” in IEEE Transactions on Reliability, vol. 72, no. 2, pp. 599-608, June 2023, doi: 10.1109/TR.2022.3192020.\n[109] J. Heydari, V. Ganapathy and M. Shah, ”Dynamic Task Offloading in Multi-Agent Mobile Edge Computing Networks,” 2019 IEEE Global\nCommunications Conference (GLOBECOM), Waikoloa, HI, USA, 2019, pp. 1-6, doi: 10.1109/GLOBECOM38437.2019.9013115.\n[110] X. Zhu, Y. Luo, A. Liu, M. Z. A. Bhuiyan and S. Zhang, ”Multiagent Deep Reinforcement Learning for Vehicular Computation Offloading in IoT,” in\nIEEE Internet of Things Journal, vol. 8, no. 12, pp. 9763-9773, 15 June15, 2021, doi: 10.1109/JIOT.2020.3040768.\n[111] Y. Li et al., ”Two-Tier Multi-Access Partial Computation Offloading via NOMA: A Hybrid Deep Learning Approach for Energy Minimization,” 2022\n31st Wireless and Optical Communications Conference (WOCC), Shenzhen, China, 2022, pp. 138-143, doi: 10.1109/WOCC55104.2022.9880599.\n[112] Y. Liu, S. Xie and Y. Zhang, ”Cooperative Offloading and Resource Management for UAV-Enabled Mobile Edge Computing in Power IoT System,”\nin IEEE Transactions on Vehicular Technology, vol. 69, no. 10, pp. 12229-12239, Oct. 2020, doi: 10.1109/TVT.2020.3016840.\n[113] X. Zheng, M. Li, Y. Chen, J. Guo, M. Alam and W. Hu, ”Blockchain-Based Secure Computation Offloading in Vehicular Networks,” in IEEE Transactions\non Intelligent Transportation Systems, vol. 22, no. 7, pp. 4073-4087, July 2021, doi: 10.1109/TITS.2020.3014229.\n[114] L. Zhao et al., ”A Digital Twin-Assisted Intelligent Partial Offloading Approach for Vehicular Edge Computing,” in IEEE Journal on Selected Areas\nin Communications, vol. 41, no. 11, pp. 3386-3400, Nov. 2023, doi: 10.1109/JSAC.2023.3310062.\n[115] S. Chouikhi, M. Esseghir and L. Merghem-Boulahia, ”Energy-Efficient Computation Offloading Based on Multiagent Deep Reinforcement Learn-\ning for Industrial Internet of Things Systems,” in IEEE Internet of Things Journal, vol. 11, no. 7, pp. 12228-12239, 1 April1, 2024, doi:\n10.1109/JIOT.2023.3333044.\n[116] Z. Xue, C. Liu, C. Liao, G. Han and Z. Sheng, ”Joint Service Caching and Computation Offloading Scheme Based on Deep Reinforcement\nLearning in Vehicular Edge Computing Systems,” in IEEE Transactions on Vehicular Technology, vol. 72, no. 5, pp. 6709-6722, May 2023, doi:\n10.1109/TVT.2023.3234336.\n[117] H. Gao, X. Wang, W. Wei, A. Al-Dulaimi and Y. Xu, ”Com-DDPG: Task Offloading Based on Multiagent Reinforcement Learning for Information-\nCommunication-Enhanced Mobile Edge Computing in the Internet of Vehicles,” in IEEE Transactions on Vehicular Technology, vol. 73, no. 1, pp.\n348-361, Jan. 2024, doi: 10.1109/TVT.2023.3309321.\n[118] P. Dai, Y. Huang, K. Hu, X. Wu, H. Xing and Z. Yu, ”Meta Reinforcement Learning for Multi-Task Offloading in Vehicular Edge Computing,” in\nIEEE Transactions on Mobile Computing, vol. 23, no. 3, pp. 2123-2138, March 2024, doi: 10.1109/TMC.2023.3247579.\n[119] Y. Zhang, B. Di, Z. Zheng, J. Lin and L. Song, ”Joint Data Offloading and Resource Allocation for Multi-Cloud Heterogeneous Mobile Edge Computing\nUsing Multi-Agent Reinforcement Learning,” 2019 IEEE Global Communications Conference (GLOBECOM), Waikoloa, HI, USA, 2019, pp. 1-6, doi:\n10.1109/GLOBECOM38437.2019.9013596.\n[120] X. Xu and Y. Song, ”A Deep Reinforcement Learning-Based Optimal Computation Offloading Scheme for VR Video Transmission in Mobile Edge\nNetworks,” in IEEE Access, vol. 11, pp. 122772-122781, 2023, doi: 10.1109/ACCESS.2023.3327921.\n[121] M. Goudarzi, M. A. Rodriguez, M. Sarvi and R. Buyya, ”µ DDRL: A QoS-Aware Distributed Deep Reinforcement Learning Technique for\nService Offloading in Fog Computing Environments,” in IEEE Transactions on Services Computing, vol. 17, no. 1, pp. 47-59, Jan.-Feb. 2024, doi:\n10.1109/TSC.2023.3332308.\n[122] C. Chen, H. Li, H. Li, R. Fu, Y. Liu and S. Wan, ”Efficiency and Fairness Oriented Dynamic Task Offloading in Internet of Vehicles,” in IEEE\nTransactions on Green Communications and Networking, vol. 6, no. 3, pp. 1481-1493, Sept. 2022, doi: 10.1109/TGCN.2022.3167643.\n[123] Q. Qi et al., ”Knowledge-Driven Service Offloading Decision for Vehicular Edge Computing: A Deep Reinforcement Learning Approach,” in IEEE\nTransactions on Vehicular Technology, vol. 68, no. 5, pp. 4192-4203, May 2019, doi: 10.1109/TVT.2019.2894437.\n[124] Z. Cao, P. Zhou, R. Li, S. Huang and D. Wu, ”Multiagent Deep Reinforcement Learning for Joint Multichannel Access and Task Offloading of\nMobile-Edge Computing in Industry 4.0,” in IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6201-6213, July 2020, doi: 10.1109/JIOT.2020.2968951.\n[125] Z. Gao, J. Fu, Z. Jing, Y. Dai and L. Yang, ”MOIPC-MAAC: Communication-Assisted Multiobjective MARL for Trajectory Planning and Task Offloading\nin Multi-UAV-Assisted MEC,” in IEEE Internet of Things Journal, vol. 11, no. 10, pp. 18483-18502, 15 May15, 2024, doi: 10.1109/JIOT.2024.3362988.\n[126] M. Khayyat, I. A. Elgendy, A. Muthanna, A. S. Alshahrani, S. Alharbi and A. Koucheryavy, ”Advanced Deep Learning-Based Computational Offloading\nfor Multilevel Vehicular Edge-Cloud Computing Networks,” in IEEE Access, vol. 8, pp. 137052-137062, 2020, doi: 10.1109/ACCESS.2020.3011705.\n[127] H. Ke, J. Wang, L. Deng, Y. Ge and H. Wang, ”Deep Reinforcement Learning-Based Adaptive Computation Offloading for MEC in Heterogeneous\nVehicular Networks,” in IEEE Transactions on Vehicular Technology, vol. 69, no. 7, pp. 7916-7929, July 2020, doi: 10.1109/TVT.2020.2993849.\n[128] K. Mishra, G. N. V. Rajareddy, U. Ghugar, G. S. Chhabra and A. H. Gandomi, ”A Collaborative Computation and Offloading for Compute-Intensive and\nLatency-Sensitive Dependency-Aware Tasks in Dew-Enabled Vehicular Fog Computing: A Federated Deep Q-Learning Approach,” in IEEE Transactions\non Network and Service Management, vol. 20, no. 4, pp. 4600-4614, Dec. 2023, doi: 10.1109/TNSM.2023.3282795.\n[129] H. Zhang, L. Feng, X. Liu, K. Long and G. K. Karagiannidis, ”User Scheduling and Task Offloading in Multi-Tier Computing 6G Vehicular Network,”\nin IEEE Journal on Selected Areas in Communications, vol. 41, no. 2, pp. 446-456, Feb. 2023, doi: 10.1109/JSAC.2022.3227097.\n[130] X. Li, Y. Qin, J. Huo and W. Huangfu, ”Heuristically Assisted Multiagent RL-Based Framework for Computation Offloading and Resource Allocation\nof Mobile-Edge Computing,” in IEEE Internet of Things Journal, vol. 10, no. 17, pp. 15477-15487, 1 Sept.1, 2023, doi: 10.1109/JIOT.2023.3264253.\n[131] H. Huang, Q. Ye and Y. Zhou, ”6G-Empowered Offloading for Realtime Applications in Multi-Access Edge Computing,” in IEEE Transactions on\nNetwork Science and Engineering, vol. 10, no. 3, pp. 1311-1325, 1 May-June 2023, doi: 10.1109/TNSE.2022.3188921.\n[132] J. Yan, X. Zhao and Z. Li, ”Deep-Reinforcement-Learning-Based Computation Offloading in UAV-Assisted Vehicular Edge Computing Networks,” in\nIEEE Internet of Things Journal, vol. 11, no. 11, pp. 19882-19897, 1 June1, 2024, doi: 10.1109/JIOT.2024.3370553.\n[133] W. Fan, F. Yang, P. Wang, M. Miao, P. Zhao and T. Huang, ”DRL-Based Service Function Chain Edge-to-Edge and Edge-to-Cloud Joint\nOffloading in Edge-Cloud Network,” in IEEE Transactions on Network and Service Management, vol. 20, no. 4, pp. 4478-4493, Dec. 2023, doi:\n10.1109/TNSM.2023.3271769.\n[134] L. Yao, X. Xu, M. Bilal and H. Wang, ”Dynamic Edge Computation Offloading for Internet of Vehicles With Deep Reinforcement Learning,” in IEEE\nTransactions on Intelligent Transportation Systems, vol. 24, no. 11, pp. 12991-12999, Nov. 2023, doi: 10.1109/TITS.2022.3178759.\n[135] C. Sun, X. Wu, X. Li, Q. Fan, J. Wen and V. C. M. Leung, ”Cooperative Computation Offloading for Multi-Access Edge Computing in 6G Mobile\nNetworks via Soft Actor Critic,” in IEEE Transactions on Network Science and Engineering, doi: 10.1109/TNSE.2021.3076795.\n[136] T. Deng et al., ”Entropy Normalization SAC-Based Task Offloading for UAV-Assisted Mobile-Edge Computing,” in IEEE Internet of Things Journal,\nvol. 11, no. 15, pp. 26220-26233, 1 Aug.1, 2024, doi: 10.1109/JIOT.2024.3395276.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.DC",
    "cs.MA",
    "Machine Learning, Reinforcement Learning, Multi Agent Reinforcement\n  Learning, Computational Offloading and Edge Computing"
  ],
  "published": "2025-02-10",
  "updated": "2025-02-10"
}