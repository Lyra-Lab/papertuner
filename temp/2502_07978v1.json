{
  "id": "http://arxiv.org/abs/2502.07978v1",
  "title": "A Survey of In-Context Reinforcement Learning",
  "authors": [
    "Amir Moeini",
    "Jiuqi Wang",
    "Jacob Beck",
    "Ethan Blaser",
    "Shimon Whiteson",
    "Rohan Chandra",
    "Shangtong Zhang"
  ],
  "abstract": "Reinforcement learning (RL) agents typically optimize their policies by\nperforming expensive backward passes to update their network parameters.\nHowever, some agents can solve new tasks without updating any parameters by\nsimply conditioning on additional context such as their action-observation\nhistories. This paper surveys work on such behavior, known as in-context\nreinforcement learning.",
  "text": "arXiv:2502.07978v1  [cs.LG]  11 Feb 2025\nA Survey of In-Context Reinforcement Learning\nAmir Moeini1 , Jiuqi Wang1 , Jacob Beck2 , Ethan Blaser1 ,\nShimon Whiteson2 , Rohan Chandra1 , Shangtong Zhang1\n1University of Virginia\n2University of Oxford\n{amoeini, shangtong}@virginia.edu\nAbstract\nReinforcement learning (RL) agents typically opti-\nmize their policies by performing expensive back-\nward passes to update their network parameters.\nHowever, some agents can solve new tasks without\nupdating any parameters by simply conditioning on\nadditional context such as their action-observation\nhistories. This paper surveys work on such behav-\nior, known as in-context reinforcement learning.\n1\nIntroduction\nReinforcement learning (RL) [Sutton and Barto, 2018] is a\nparadigm for solving sequential decision-making tasks via\ntrial and error. In RL, an agent incrementally optimizes its\npolicy as it interacts with its environment so as to maximize\na reward signal in the long run. Here, the policy is a func-\ntion that maps the agent’s observation to the distribution from\nwhich its actions are sampled. Policies are typically repre-\nsented with neural networks whose parameters are contin-\nually updated during learning. Selecting actions requires a\nforward pass through the network. Updating the parameters\nusually requires a backward pass, which can be expensive for\nlarge neural networks in terms of both memory and computa-\ntion [Kingma and Ba, 2017].\nSome pretrained RL agents, however, can solve new tasks\nwithout updating any network parameters. When evaluating\nsuch an agent on a new task, the input to the agent includes\nboth the current observation and additional context that helps\nthe agent adapt to the new task. For example, the context\nmay include the agent’s history of observations and actions\nin this new task up to the current time step. The remark-\nable ability of such agents to generalize to new tasks using\ncontext but without ﬁne-tuning is hypothesized [Duan et al.,\n2016; Laskin et al., 2023] to arise from the pretrained neu-\nral network implementing some (known or unknown) RL\nalgorithm in its forward pass to process the context. We\nrefer to this behavior in the forward pass as in-context\nreinforcement learning (ICRL). An immediate implication\nof ICRL is that the agent’s performance improves as the\ntask-related information in the context accumulates, a phe-\nnomenon called in-context improvement. Figure 1 illustrates\nICRL’s pipeline. We follow Sutton and Barto [2018] and de-\nﬁne RL algorithms broadly as learning algorithms for solving\nsequential decision-making problems. This includes, for ex-\nample, imitation learning [Abbeel and Ng, 2005] and tempo-\nral difference learning (TD, Sutton [1988]).\nWe argue that ICRL is important because it enables agents\nto generalize to new tasks efﬁciently, requiring only a forward\npass without expensive parameter updates. Eliminating the\nneed for parameter updates creates new opportunities to op-\ntimize computation and memory requirements for inference\n[Zhu et al., 2024]. Furthermore, there is also evidence that\nthe RL algorithms implemented in the forward pass can po-\ntentially be more sample efﬁcient than manually engineered\nones [Laskin et al., 2023].\nScope.\nICRL falls in the category of black box meth-\nods for meta RL [Beck et al., 2023] and dates back to\nDuan et al. [2016]; Wang et al. [2016]. Early works of ICRL\ndemonstrate only limited out-of-distribution generalization\n(see Section 4 for more discussion) and are well surveyed\nby Beck et al. [2023]. We view Laskin et al. [2023] as an\nimportant milestone of ICRL since it both coins the term\nand provides the ﬁrst demonstration of remarkable out-of-\ndistribution generalization of the pretrained agent. This sur-\nvey, therefore, focuses on ICRL work after Laskin et al.\n[2023].\nTaxonomy.\nIn this paper, we survey work on ICRL along\ndifferent axes. We start with different pretraining methods,\nsuch as supervised pretraining and reinforcement pretrain-\ning. We then examine different methods for constructing con-\ntext at test time and the demonstrated test time performance.\nWe then survey recent theoretical advances in understanding\nICRL and, ﬁnally, neural network architecture design choices\nin both empirical and theoretical work on ICRL.\nRelated Surveys.\nThe closest work to this paper\nis Beck et al. [2023], which surveys meta-RL. Beck et al.\n[2023] do not include recent ICRL advances showing strong\nout-of-distribution generalization since Laskin et al. [2023].\nThis survey aims to close this gap and is thus complementary\nto Beck et al. [2023]. See Beck et al. [2023] for a full treat-\nment of other aspects of meta RL. The development of ICRL\nparallels the development of in-context learning (ICL), where\nsupervised (instead of reinforcement) learning occurs in the\nforward pass. See Dong et al. [2023] for a full treatment of\nICL. Furthermore, some ICRL pretraining methods resemble\nRL via supervised learning (RvS) in the ofﬂine RL commu-\nnity and goal-conditioned RL. However, many methods for\nPretraining (outer loop)\nIn-context RL (inner loop)\nMDP1\nEpisode 1\nEpisode 2\nEpisode 3\nIn-context RL (inner loop)\nMDP2\nEpisode 1\nEpisode 2\nEpisode 3\nIn-context RL (inner loop)\nMDP3\nEpisode 1\nEpisode 2\nEpisode 3\nFigure 1: Overview of ICRL. After pretraining, the forward pass of the network implements some RL algorithm. The implemented RL\nalgorithm is tested on multiple MDPs. The context in each MDP can span multiple episodes.\nRvS and goal-conditioned RL, e.g., Chen et al. [2021], do not\ndemonstrate key properties of ICRL, such as in-context im-\nprovement, and are therefore not included in this survey. That\nbeing said, we include RvS works that are able to demon-\nstrate ICRL, e.g., Laskin et al. [2023].\nNevertheless, see\nEmmons et al. [2022]; Wen et al. [2023] for a full treatment\nof RvS, and Liu et al. [2022] for goal-conditioned RL.\n2\nBackground\nReinforcement Learning.\nRL typically models environ-\nments or tasks as Markov Decision Processes (MDPs). An\nMDP consists of a state space S, an action space A, a re-\nward function r : S × A →R, a transition function p :\nS × S × A →[0, 1], an initial distribution p0 : S →[0, 1],\nand a discount factor γ ∈[0, 1). At time step 0, an initial\nstate S0 is sampled from p0. At time t, an agent at a state\nSt takes an action At according to its policy π : A × S →\n[0, 1], i.e., At ∼π(·|St). The agent then receives a reward\nRt+1 .= r(St, At) and proceeds to a successor state St+1 ∼\np(·|St, At). We use τt to denote the trajectory up to time t,\ndeﬁned as, τt\n.= (S0, A0, R1, S1, A1, . . . , St−1, At−1, Rt).\nThe value function of the policy π is deﬁned as vπ(s)\n.=\nE\n\u0002P∞\ni=1 γi−1Rt+i|St = s\n\u0003\n.\nThere are two fundamental\ntasks in RL (given an environment). The ﬁrst is policy eval-\nuation, where the goal is to estimate vπ for a given π. The\nsecond is control, where the goal is to ﬁnd a policy π to maxi-\nmize the expected total rewards J(π) .= P\ns p0(s)vπ(s). Pol-\nicy evaluation is the foundation of control. Given a policy π\nand its value function, a better-performing policy π′ can be\ncomputed. This is called a policy improvement step. Iterat-\ning between policy evaluation and policy improvement can\ngenerate the optimal policy to maximize J(π).\nRL methods usually adopt a parameterized policy πθ (and\na parameterized value estimate vθ). For example, θ could\nbe the parameters of a neural network.\nMany RL algo-\nrithms have been proposed to update θ during the agent-\nenvironment interaction. This is referred to as online pre-\ntraining. By contrast, ofﬂine pretraining relies on a dataset\nD consisting of previously collected transition tuples D .=\n{(si, ai, ri, s′\ni)}i=1,...,K, where ri is the reward recevied af-\nter executing the action ai in the state si, and s′\ni is the cor-\nresponding successor state. This ofﬂine dataset may come\nfrom one or more (known or unknown) behavior policies. The\ntransition tuples may or may not form a complete trajectory.\nMany RL algorithms are proposed to update θ using D as\nwell. Notably, both online and ofﬂine pretraining can involve\nlearning a single policy πθ from one or multiple MDPs. When\na single policy is used for multiple MDPs, it needs additional\ninput besides the current state to differentiate between and\nadapt to different MDPs. After pretraining, whether ofﬂine\nor online, the policy πθ is evaluated on the same MDP or one\nor more new MDPs.\nIn-Context Reinforcement Learning.\nThe key idea of\nICRL is to condition the policy both on St and some con-\ntext Ct, such that the action At is sampled according to\nπθ(·|St, Ct), rather than solely being conditioned on St. This\nsurvey considers different ways to construct Ct, but a sim-\nple example is to use τt as Ct. Some pretraining methods\ncan generate θ such that the policy πθ(·|St, Ct) can obtain\nhigh rewards in test MDPs that differ from the MDPs seen\nduring pretraining, despite θ remaining ﬁxed. Such general-\nization is hypothesized to arise because the forward pass of\nthe neural network θ implements an RL algorithm that learns\nfrom the current context Ct [Duan et al., 2016; Laskin et al.,\n2023]. This is called ICRL (for control). Furthermore, the\nperformance of πθ improves with the length of the context Ct\n(provided that Ct always consists of information related to\nthe task), which we call in-context improvement. Similarly,\nRL algorithms for policy evaluation can also be implemented\nin the forward pass if the neural network for value estima-\ntion takes as input both the state and the context [Wang et al.,\n2024a]. This is called ICRL for policy evaluation.\n3\nSupervised Pretraining\nThis section surveys the ﬁrst class of methods for pretraining\nthe network parameter θ – supervised pretraining, which is\ncommonly done through behavior cloning. The pretraining\nobjective is typically the log-likelihood log πθ(a∗|s, c) or its\nvariants, where (s, c) is the input state-context pair and a∗is\nthe desired output action. There are multiple ways to con-\nstruct the input and output.\nThe most common approach is to concatenate the trajecto-\nries from multiple episodes as input (called cross-episode in-\nput) and use the corresponding action at each step as output.\nVarious ways are proposed to obtain the episode trajectories\nfor constructing the input. Laskin et al. [2023] use trajecto-\nries generated by some existing RL algorithms across their\nentire lifecycle. As a result, the input includes trajectories\nproduced by existing RL algorithms during both early pre-\ntraining stages (when agents perform poorly) and later stages\n(when agents achieve strong performance).\nBuilding on\nLaskin et al. [2023], Shi et al. [2023] propose to ﬁll the con-\ntext with a curriculum of trajectories. Speciﬁcally, they order\nthe trajectories by task difﬁculty, demonstrator proﬁciency,\nor episode returns [Huang et al., 2024a,b; Liu and Abbeel,\n2023]. Alternative curriculum construction methods include\nadding decaying noise to expert demonstration trajectories\n[Zisman et al., 2023] and using an explicit feature in the tra-\njectory to indicate whether the current episode is better than\nthe ones before [Dai et al., 2024].\nThis indicator, called\ncross-episode return-to-go in Dai et al. [2024], alleviates the\nneed to rank episodes in the context.\nAll of these meth-\nods aim to demonstrate performance improvement in the\ninput-out pairs, encouraging the neural network to implement\nsome policy improvement algorithm in the forward pass.\nKirsch et al. [2023] show that the performance improvement\nrate in the input trajectories directly inﬂuences how quickly\nthe pretrained agent improves at test time.\nSome methods instead emphasize ﬁnding trajectories sim-\nilar to the current task from the ofﬂine data and prepending\nthem to the context for better generalization [Xu et al., 2022;\nWang et al., 2024b].\nThese initial trajectories are referred\nto as a prompt. This encourages the network to implement\nsome imitation learning algorithm in the forward pass to\nimitate the behavior demonstrated in the context. This ap-\nproach is most effective when the prompt is collected by an\nexpert agent [Raparthy et al., 2023; Xu et al., 2022]. Con-\nsequently, these methods require expert prompts at test time\nand cannot use their own suboptimal interactions as prompts.\nTo mitigate this issue, Lee et al. [2023] construct the prompts\nusing the suboptimal trajectories but replace the actions in the\ntrajectories with actions predicted by a performant policy.\nSome works incorporate hindsight information into context\nto facilitate policy optimization [Furuta et al., 2022]. Hind-\nsight information is data that can only be inferred from out-\ncomes of future time steps. A common example is the Return-\nTo-Go (RTG), which represents the sum of rewards from a\nstep until the end of the episode, [Xu et al., 2022; Wang et al.,\n2024b; Dai et al., 2024]. However, inspired by how RTG is\nestimated during testing (Section 5), some methods replace\nthis with a target RTG, deﬁned as either the maximum RTG in\nthe ofﬂine dataset [Schmied et al., 2024] or among episodes\nin the current context [Huang et al., 2024a,b].\nTo summarize, while some works do not rely on a curricu-\nlum in the context [Lee et al., 2023], they all rely on multiple\ntrajectories in the context. Using a single trajectory in the\ncontext [Chen et al., 2021] provably prevents an agent from\nimproving on the ofﬂine-data-generating policies or “stitch-\ning” suboptimal trajectories to get an optimal one without\nstrong assumptions [Brandfonbrener et al., 2022].\nSample efﬁciency during supervised pretraining is also an\nimportant research area. Here we deﬁne sample efﬁciency\nbroadly, including the amount of data, the amount of ex-\npert / optimal policy demonstration, and the amount of dif-\nferent tasks. Zisman et al. [2024] greatly improve the pre-\ntraining sample efﬁciency of Laskin et al. [2023] by hard-\ncoding n-gram induction heads into the transformer and bi-\nasing it toward using in-context information [Aky¨urek et al.,\n2024]. Dai et al. [2024] use importance sampling to remove\nthe pessimistic bias that keeps the pretrained policy close to\nthe data collection policy, thereby enabling the viable use\nof suboptimal data collection policies. Similarly, Dong et al.\n[2024] employ a weighted loss, where the weights, based on\nthe observed rewards, act as importance sampling ratios to\nguide the suboptimal policy toward the optimal policy. With\na different approach, Zisman et al. [2023] use a suboptimal\npolicy and add annealing noise to its trajectories to gener-\nate learning histories similar to those of Laskin et al. [2023].\nKirsch et al. [2023] construct augmented tasks to improve\nsample efﬁciency by randomly projecting the task’s observa-\ntion and action spaces. While these methods enhance overall\nsample efﬁciency, robotic tasks introduce unique challenges.\nDue to long sequence lengths, full rollouts are inefﬁcient.\nElawady et al. [2024] mitigate this issue by employing par-\ntial rollouts that reduce environment interactions.\nFinally, there are multiple ways to encode the con-\ntext before it is presented to the neural network.\nFor\nexample,\nin Transformer-based methods,\nthe token at\neach time step can be the stacked embeddings of ei-\nther (st, at, rt+1, st+1) [Kirsch et al., 2023; Lee et al., 2023]\nor (at−1, rt, st) [Laskin et al., 2023; Raparthy et al., 2023;\nZisman et al., 2023]. Similarly, action spaces can be embed-\nded in distinct ways. For instance, to enable test-time adap-\ntation to varying action spaces, Sinii et al. [2023] project dis-\ncrete actions into random vector embeddings and train the\nnetwork to output an embedding vector directly. Then, the\naction whose embedding is most similar to the network out-\nput is executed.\n4\nReinforcement Pretraining\nThis section surveys the second class of methods for pretrain-\ning the network parameter θ – reinforcement pretraining. In-\nstead of using the log-likelihood loss, reinforcement pretrain-\ning uses other established RL algorithms to train the policy\nπθ(a|s, c). In contrast to supervised pretraining which uses\nofﬂine datasets, reinforcement pretraining usually involves\nonline environment interactions.\nEarly ICRL works with reinforcement pretraining include\nDuan et al. [2016]; Wang et al. [2016]; Mishra et al. [2018];\nRitter et al.\n[2018];\nStadie et al.\n[2019];\nZintgraf et al.\n[2020]; Melo [2022], where a sequence model (e.g., an\nRNN) parameterizes the policy.\nAt test time, the policy\ntakes the agent’s online interaction history (usually across\nmultiple episodes) as input and outputs the action, without\nany parameter updates.\nThis history-dependent policy\neffectively functions as an RL algorithm, as both take the\ncomplete history as input and output an action. However,\nearly works in this line of research demonstrate only limited\nout-of-distribution generalization.\nThey only demonstrate\nthat the learned history-dependent policy performs well in\ntasks similar to pretraining tasks. One hypothesis for the lack\nof out-of-distribution generalization in those works is that\nthe pretrained network implements some task identiﬁcation\nalgorithm together with certain nearest neighbor match-\ning. In other words, at test time, the pretrained network tries\nto identify pretraining tasks that are similar to the test task\n(based on the entire online interaction history in the test task)\nand acts as if the test task was one of those similar pretraining\ntasks. Those task identiﬁcation works are well surveyed by\nBeck et al. [2023].\nIn this paper, we instead focus on more recent ad-\nvances in ICRL, where pretrained networks demonstrate\nstronger out-of-distribution generalization by implement-\ning more advanced RL algorithms in the forward pass.\nThese include Grigsby et al. [2024, 2023]; Lu et al. [2023];\nBauer et al. [2023]; Park et al. [2024]; Wang et al. [2024a];\nElawady et al. [2024]; Xu et al. [2024]; Cook et al. [2024].\nAlthough their pretraining is still performed by (modiﬁ-\ncation of) standard RL algorithms (Table 1), using long-\ncontext neural networks such as Transformers to parameter-\nize the policy introduces substantial learning stability chal-\nlenges [Grigsby et al., 2023].\nTo improve stability, sev-\neral modiﬁcations to the pretraining process have been pro-\nposed. Grigsby et al. [2023] employ multiple discount rates\nsimultaneously to stabilize long-horizon credit assignment.\nBauer et al. [2023] introduce a task selection strategy that\nprioritizes tasks slightly beyond the agent’s current exper-\ntise, signiﬁcantly enhancing sample efﬁciency. For scenar-\nios involving learning across tasks with highly varied return\nscales, Grigsby et al. [2024] utilize actor-critic objectives de-\ncoupled from return magnitudes, thereby improving conver-\ngence. That being said, why the recent works are able to\ndemonstrate stronger generalization remains an open prob-\nlem (Section 9).\n5\nTest Time Context\nHaving surveyed the two main pretraining paradigms, we\nnow turn to test time design choices, beginning with the con-\ntext construction. While context construction is often similar\nduring both pretraining and testing, some information pro-\nvided in the context construction during pretraining is not\navailable at test time. This section starts with examining how\nto address these differences.\nOne example of such information is expert demon-\nstrations.\nMethods using such demonstrations in pre-\ntraining [Rakelly et al., 2019; Lee et al., 2023; Wang et al.,\n2024b] often experience signiﬁcant performance drops when\nMethod\nPretraining Algorithm\nGrigsby et al. [2023]\nModiﬁed DDPG\n[Lillicrap et al., 2019]\nGrigsby et al. [2024]\nModiﬁed DDPG\nLu et al. [2023]\nMuesli [Hessel et al., 2021]\nBauer et al. [2023]\nMuesli\nElawady et al. [2024]\nModiﬁed PPO\n[Schulman et al., 2017]\nWang et al. [2024a]\nTD\nPark et al. [2024]\nRegret minimization\nCook et al. [2024]\nPPO\nXu et al. [2024]\nModiﬁed DQN\n[Mnih et al., 2015]\nTable 1: Algorithms used for reinforcement pretraining.\nprompted with suboptimal interactions in test time. This de-\ncline occurs because the model has limited exposure to the\nnear-optimal trajectory space during testing, leading to a mis-\nmatch between the context distribution in pretraining and test-\ning.\nIf at test time the agent outputs actions using only ofﬂine\ntrajectories as context, the performance will heavily depend\non the quality of the ofﬂine demonstrations [Lee et al., 2023].\nThis is also the case if the agent is expected to output good\nactions after obtaining only one or a few online trajectories\n[Raparthy et al., 2023; Wang et al., 2024b].\nBut when the\nagent is allowed to interact more extensively with the envi-\nronment at test time before it is expected to output good ac-\ntions, the need for expert demonstrations in the context is re-\nduced [Laskin et al., 2023; Huang et al., 2024a,b; Sinii et al.,\n2023; Kirsch et al., 2023; Lee et al., 2023]. Lee et al. [2023]\ndemonstrate the trade-off between the allowed online interac-\ntion budget and the need for expert demonstrations.\nAnother example of context information that is not avail-\nable at test time is RTG from Section 3. Various methods are\nused to estimate RTG at test time. Huang et al. [2024b,a];\nXu et al. [2022]; Schmied et al. [2024]; Wang et al. [2024b]\napproximate RTG once per task based on the ofﬂine trajecto-\nries. At the beginning of the episode, an initial RTG is given.\nThis RTG is iteratively updated based on the observed reward.\nAlternatively, Dai et al. [2024] train a secondary network to\npredict RTG dynamically based on the interaction history.\nDuring testing, methods that do not rely on demonstrations\nmust learn the task solely from their own previous interac-\ntions. However, we can selectively choose what to include\nin the context. For instance, Cook et al. [2024] divide the\ntotal interaction horizon into generations, with each genera-\ntion comprising several agents. These agents systematically\nuse the best-performing agent from the previous generation to\ngenerate interactions that are then incorporated into the cur-\nrent context. This approach allows the current agent to build\nupon prior experience. The framework, referred to as cultural\naccumulation, achieves superior test-time performance scal-\ning compared to the base single-generation method.\n6\nTest Time Performance\nIn this section, we survey the test-time performance of pre-\ntrained agents from two aspects, generalization and sam-\nple efﬁciency.\nSince comparing generalization across dif-\nferent benchmarks is challenging, we consider generalization\nbenchmark by benchmark. Notably, unlike goal-conditioned\nmethods that explicitly condition the pretrained agent on the\ntest task type, the ICRL agent must infer it implicitly by itself.\nThe ﬁrst remarkable out-of-distribution generalization in\nICRL is demonstrated by Laskin et al. [2023] in multi-armed\nbandit problems. Their pretrained agents learn new bandit\nproblems with adversarial rewards (i.e., engineered so that\npretraining-optimal policies perform poorly) and achieve re-\ngret nearly equivalent to standard bandit algorithms that in-\nvolve parameter updates.\nICRL’s out-of-distribution generalization improves as\nmodels, pretraining duration, and experience diversity scale\n[Bauer et al., 2023; Kirsch et al., 2023].\nFor instance,\nBauer et al. [2023] propose XLand 2.0, a procedurally gener-\nated 3D environment featuring diverse goals, rules, and con-\nﬁgurations. They demonstrate generalization on this chal-\nlenging benchmark using ICRL, enabled by large-scale super-\nvised pretraining with a curriculum and other improvements.\nOther commonly used benchmarks in the ICRL literature\ninclude Dark Room [Laskin et al., 2023], DMLab Watermaze\n[Laskin et al., 2023], Procgen [Cobbe et al., 2020], Meta-\nWorld [Yu et al., 2021], and Mujoco Control [Todorov et al.,\n2012].\nEach benchmark requires a speciﬁc form of gen-\neralization which is demonstrated by different works. Test\ntasks can differ from pretraining tasks across various task-\ndependent factors (such as goal location or object types). Ac-\ncordingly, the difﬁculty of generalization can be better under-\nstood by considering the types of factors and the extent of\nvariations [Kirk et al., 2021].\nTo succeed in the Dark Room benchmark and its vari-\nants, the agent should learn to efﬁciently ﬁnd held-out\n(i.e., not used during pretraining) invisible goal loca-\ntions.\nEach unique goal location (or combination of\nthem in the key-to-door variant) represents a new task.\nThis generalization is demonstrated by Laskin et al. [2023];\nLee et al. [2023]; Huang et al. [2024a,b]; Sinii et al. [2023];\nZisman et al. [2023, 2024]; Kirsch et al. [2023]; Dai et al.\n[2024]; Grigsby et al. [2023]; Elawady et al. [2024] to differ-\nent degrees. Notably, Grigsby et al. [2023] adapts to a new\nkey-to-door environment in only 300 interactions.\nSimilarly, to succeed in DMLab Watermaze, where the\ninput to the agent is raw pixels, the agent needs to ﬁnd a\ntrapdoor in new locations of a maze. This generalization is\ndemonstrated by Laskin et al. [2023]; Zisman et al. [2023];\nShi et al. [2023]; Ritter et al. [2018] to different degrees.\nTo succeed in the Mujoco Control benchmark, the agent\nmust control simulated robots to achieve given tasks (e.g.,\nmake a HalfCheetah run or an Ant navigate) with varia-\ntions in dynamics (e.g., altered friction or mass) or target\nparameters (e.g., desired speed or direction).\nThis gener-\nalization is demonstrated by Xu et al. [2022]; Wang et al.\n[2024b]; Grigsby et al. [2023]; Mishra et al. [2018]; Melo\n[2022]; Kirsch et al. [2023] to different degrees. In particular,\nKirsch et al. [2023] show that after pretraining on Ant tasks,\nthe agent can solve the Cartpole task in DeepMind Control\nSuite.\nProcgen is a benchmark consisting of 16 procedurally gen-\nerated 2D games (e.g., platformers, puzzles) with pixel ob-\nservations. To demonstrate generalization, the agent should\nlearn held-out games [Raparthy et al., 2023].\nThe games\ndiffer across many factors (such as objects, objectives, and\ntypes of movement), which have proved difﬁcult to generalize\nto, especially when expert demonstrations are not available\nduring the test. Grigsby et al. [2024]; Schmied et al. [2024]\nshow initial progress in an easier setting, where they test on\nthe same pretraining games with limited modiﬁcations (e.g.,\nchanges to the starting location or textures)\nMeta-World consists of robotic manipulation challenges.\nIn Meta-Learning 1 (ML1), the variations are continuous\n(e.g., different object or goal positions) within a single ma-\nnipulation category. By contrast, ML45 uses 45 manipula-\ntion categories (e.g., opening drawers or turning faucets) for\npretraining and 5 new categories for testing. Several studies\nhave shown models generalizing on ML1 [Xu et al., 2022;\nWang et al., 2024b; Grigsby et al., 2023; Mishra et al., 2018;\nMelo, 2022], and Grigsby et al. [2024] show generalization\non the 45 pretraining manipulation categories of ML45 in a\nsetting similar to the limited one described earlier for Proc-\ngen.\nWe now turn to the sample efﬁciency of the pretrained\nagents in the test time.\nLaskin et al. [2023] demonstrate\nthat a pretrained network with ﬁxed parameters needs fewer\nsamples at test time to achieve similar performance to that\nof baseline RL algorithms that require gradient updates.\nKirsch et al. [2023] successfully control test-time sample ef-\nﬁciency by manipulating how much an episode improves\nupon the previous one when constructing the cross-episode\npretraining contexts.\nLee et al. [2023] show that the for-\nward pass of their pretrained network is an efﬁcient imple-\nmentation of posterior sampling, a sample-efﬁcient RL al-\ngorithm, under speciﬁc conditions during the test.\nLike-\nwise, Xu et al. [2024] propose an end-to-end framework for\nlearning an agent that performs Bayesian inference in con-\ntext, thereby improving test-time sample efﬁciency on out-\nof-distribution tasks. Sample efﬁciency remains a challenge\nin sparse-reward tasks when the agent is not sufﬁciently\nbiased toward thorough exploration.\nStadie et al. [2019];\nNorman and Clune [2023] propose addressing this issue by\nmodifying the objective to maximize only the cumulative re-\nward of later exploitive episodes, thereby allowing the initial\nexplorative episodes to focus on better exploration for subse-\nquent exploitive episodes.\n7\nTheory\nWe now consider recent advances in the theoretical under-\nstanding of ICRL.\nSupervised Pretraining.\nSupervised pretraining can be\nunderstood through the lens of behavior cloning. In canon-\nical behavior cloning, the goal is to learn a policy.\nThe\npolicy usually depends on only the current state or the his-\ntory within the current episode. In supervised pretraining,\nthe goal is to learn an algorithm similar to the source algo-\nrithm used to generate the ofﬂine dataset. In other words,\nto learn a policy that depends on the entire history of pre-\nvious episodes. Lin et al. [2023] derive a general bound on\nthe behavioral similarity and performance gap between the\nlearned algorithm (in the forward pass of the neural net-\nwork) and the source algorithm. Behavioral similarity is the\nsimilarity between the action distributions generated by the\nlearned and source algorithms given the same input.\nThe\nperformance gap is their difference in episode return. They\nfurther demonstrate how Transformers can approximate Lin-\nUCB [Chu et al., 2011], Thompson sampling [Russo et al.,\n2018], and UCB-VI [Azar et al., 2017] in the forward pass\nand provide the respective regret bounds. A follow-up work\nby Shi et al. [2024] presents an analogous behavioral similar-\nity guarantee of supervised pretraining for decentralized and\ncentralized learning in two-player zero-sum Markov games.\nShi et al. [2024] further prove by construction that there exist\nTransformers that can realize V-learning [Jin et al., 2024] for\ndecentralized learning and VI-ULCB [Bai and Jin, 2020] for\ncentralized learning in the forward passes, accompanied with\nupper bounds of the approximation error of Nash equilibria\nfor both settings.\nReinforcement Pretraining.\nPark et al. [2024] propose\nto pretrain language models directly by minimizing regret\nwithout requiring action labels.\nThey show theoretically\nthat by minimizing regret with sufﬁciently many pretrain-\ning trajectories, the pretrained language models can demon-\nstrate no-regret learning at test time.\nLastly, they prove\nthat the global minimizer of the (surrogate) regret loss with\na single-layer linear attention transformer implements the\nknown no-regret algorithm Follow-The-Regularized-Leader\n(FTRL) [Shalev-Shwartz and Singer, 2007] in the forward\npass. Wang et al. [2024a] consider ICRL for policy evalua-\ntion. They prove by construction that Transformers can pre-\ncisely implement temporal difference methods in the forward\npass for policy evaluation, including TD(λ) [Sutton, 1988]\nand average reward TD [Tsitsiklis and Roy, 1999]. They also\nshow that those parameters naturally emerge when they train\na value estimation transformer with TD on multiple policy\nevaluation tasks.\nTheoretical understanding of this emer-\ngence of TD is provided from an invariant set perspective.\n8\nArchitectures\nA central design choice in ICRL is the architecture of the neu-\nral network used to process context. The neural network must\nbe able to handle long context lengths, often containing mul-\ntiple episodes of interaction, and effectively use information\nfrom many past interactions.\nAlthough earlier meta RL works [Duan et al., 2016;\nWang et al., 2016; Ritter et al., 2018] use RNN and its\nvariants to parameterize history-dependent policies, most\nsurveyed ICRL works employ a causal transformer back-\nbone [Laskin et al., 2023; Lee et al., 2023; Raparthy et al.,\n2023; Sinii et al., 2023; Zisman et al., 2023; Shi et al., 2023;\nKirsch et al., 2023; Xu et al., 2022; Grigsby et al., 2024,\n2023; Bauer et al., 2023; Melo, 2022; Elawady et al., 2024;\nZisman et al., 2024], given transformer’s demonstrated ef-\nﬁcacy in handling long sequences [Vaswani et al., 2017].\nHowever, the inference time of Transformers is quadratic\nw.r.t. the input length.\nTo speed it up, state space mod-\nels [Gu and Dao, 2023], whose inference time is linear, are\nused [Cook et al., 2024]. Huang et al. [2024b] employ a state\nspace model for their high-level decision maker, which pro-\ncesses long histories, and a transformer for their low-level\ndecision maker, which processes shorter sequences. Lu et al.\n[2023] modify an existing state space model, S5 [Smith et al.,\n2023], such that it becomes compatible with cross-episode\ncontext. Schmied et al. [2024] use an xLSTM [Beck et al.,\n2024] for similar purposes.\nHierarchical structures are also designed with different ob-\njectives. For instance, Wang et al. [2024b] improve Xu et al.\n[2022] by incorporating additional modules that extract both\ntask-level and step-speciﬁc prompts relevant to the current\ntask and step, which are then used to augment the context\nprovided to a Transformer. Dai et al. [2024] use a secondary\nnetwork to predict the RTG required for the context during\ninference. To improve computational efﬁciency by process-\ning fewer tokens in a transformer without sacriﬁcing overall\nhistorical information, Huang et al. [2024a,b] split decision-\nmaking into two levels. Speciﬁcally, the high-level module\nprocesses tokens sampled at ﬁxed intervals, while the low-\nlevel module predicts the intervening tokens corresponding\nto each high-level token.\nCompared to supervised pretraining, reinforcement pre-\ntraining introduces engineering challenges regarding stability\nduring pretraining [Grigsby et al., 2023]. To address these\nchallenges, Grigsby et al. [2023] share a single sequence\nmodel across both actor and critic networks and demonstrate\nthat preventing the critic’s objective from minimizing the ac-\ntor’s objective can ensure pretraining stability.\nThey also\nmodify the transformer architecture to preserve plasticity over\nlong pretraining durations and avoid performance collapse,\nalso adopted by Xu et al. [2024]. Similarly, Elawady et al.\n[2024] append learnable key and value vectors as “sinks” to\nthe transformer’s attention mechanism to provide the ﬂexi-\nbility of not attending to any input token. This modiﬁcation\nmakes learning faster and more stable in scenarios involv-\ning long but low-information observation sequences, such as\nthose encountered in robotics [Elawady et al., 2024].\nRegarding theoretical analysis, having a full-sized multi-\nlayer transformer with arbitrary nonlinear activations is pro-\nhibitively challenging due to the complexity of the network\nstructure. Lin et al. [2023] and Shi et al. [2024] use masked\nattentions with ReLU activations, and Park et al. [2024];\nWang et al. [2024a] use linear attentions.\n9\nOpen Problems and Opportunities\nICRL is an emerging area with many open problems.\nFirst, we draw attention to multi-agent RL. Generaliza-\ntion to unseen agents (teammates or opponents) during\nthe deployment time is a fundamental challenge in multi-\nagent RL and meta RL has been applied to address this\nchallenge [Charakorn et al., 2021; Gerstgrasser and Parkes,\n2022].\nHowever, the demonstrated generalization is only\nlimited in small-scale problems and only limited out-of-\ndistribution generalization is demonstrated. Recent advances\nin multi-agent RL with large sequence models [Meng et al.,\n2023] provide a new opportunity to address this challenge\nwith ICRL.\nSecond, we draw attention to robotics. ICRL is now only\ndemonstrated in simulated environments. The sim-to-real gap\nis a well-known generalization challenge in robotics. It is a\npromising direction to investigate whether ICRL will emerge\nin recent internet-scale robot pretraining [Brohan et al., 2023]\nand whether ICRL can help close the sim-to-real gap.\nLastly, we draw attention to reinforcement pretraining.\nKrishnamurthy et al. [2024] criticize ICRL saying “they are\nexplicitly trained to produce this behavior using data from re-\ninforcement learning agents or expert demonstrations on re-\nlated tasks.” This criticism might be true for supervised pre-\ntraining but clearly does not hold for reinforcement pretrain-\ning, where the network is only trained to output good actions\nor to output good value estimates without constraints on how\nthe network achieves this. The network itself discovers that\nimplementing certain RL algorithms in the forward pass is a\ngood solution. In this sense, ICRL truly emerges during rein-\nforcement pretraining. Existing theoretical analyses on rein-\nforcement pretraining [Park et al., 2024; Wang et al., 2024a]\nuse simpliﬁed models and simpliﬁed pretraining algorithms.\nFully white-boxing the emergence of ICRL during reinforce-\nment pretraining in more realistic settings remains an open\nproblem, both theoretically and empirically.\n10\nConclusion\nThis paper presented the ﬁrst comprehensive survey of ICRL,\nan emerging and ﬂourishing area. We surveyed ICRL from\ndifferent aspects, including both pretraining and testing, both\nempirical and theoretical analyses. We hope this survey will\nstimulate the growth of the ICRL community.\nAcknowledgements\nThis work is supported in part by the US National Sci-\nence Foundation (NSF) under grants III-2128019 and SLES-\n2331904. EB acknowledges support from the NSF Gradu-\nate Research Fellowship (NSF-GRFP) under award 1842490.\nThis work is also supported in part by the Coastal Virginia\nCenter for Cyber Innovation (COVA CCI) and the Common-\nwealth Cyber Initiative (CCI), an investment in the advance-\nment of cyber research and development, innovation, and\nworkforce development. For more information about CCI,\nvisit www.covacci.org and www.cyberinitiative.org.\nReferences\nPieter Abbeel and Andrew Y. Ng. Exploration and appren-\nticeship learning in reinforcement learning. In Proceed-\nings of the International Conference on Machine Learning,\n2005.\nEkin Aky¨urek, Bailin Wang, Yoon Kim, and Jacob Andreas.\nIn-Context Language Learning: Architectures and Algo-\nrithms. ArXiv preprint, 2024.\nMohammad Gheshlaghi Azar, Ian Osband, and R´emi Munos.\nMinimax regret bounds for reinforcement learning.\nIn\nProceedings of the International Conference on Machine\nLearning, 2017.\nYu Bai and Chi Jin. Provable self-play algorithms for com-\npetitive reinforcement learning. In Proceedings of the In-\nternational Conference on Machine Learning, 2020.\nJakob Bauer, Kate Baumli, Feryal M. P. Behbahani, Avishkar\nBhoopchand, Nathalie Bradley-Schmieg, Michael Chang,\nNatalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy\nGonzalez,\nKarol Gregor,\nEdward Hughes,\nSheleem\nKashem, Maria Loks-Thompson, Hannah Openshaw, Jack\nParker-Holder, Shreya Pathak, Nicolas Perez Nieves, Ne-\nmanja Rakicevic, Tim Rockt¨aschel, Yannick Schroecker,\nSatinder Singh, Jakub Sygnowski, Karl Tuyls, Sarah York,\nAlexander Zacherl, and Lei M. Zhang. Human-timescale\nadaptation in an open-ended task space. In Proceedings of\nthe International Conference on Machine Learning, 2023.\nJacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong,\nLuisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A\nSurvey of Meta-Reinforcement Learning. ArXiv preprint,\n2023.\nMaximilian Beck, Korbinian P¨oppel, Markus Spanring, An-\ndreas Auer, Oleksandra Prudnikova, Michael Kopp, G¨unter\nKlambauer, Johannes Brandstetter, and Sepp Hochreiter.\nxLSTM: Extended Long Short-Term Memory.\nArXiv\npreprint, 2024.\nDavid Brandfonbrener, Alberto Bietti, Jacob Buckman, Ro-\nmain Laroche, and Joan Bruna.\nWhen does return-\nconditioned supervised learning work for ofﬂine reinforce-\nment learning?\nIn Advances in Neural Information Pro-\ncessing Systems, 2022.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:\nVision-language-action models transfer web knowledge to\nrobotic control. arXiv preprint arXiv:2307.15818, 2023.\nRujikorn Charakorn,\nPoramate Manoonpong,\nand Nat\nDilokthanakul.\nLearning to cooperate with unseen\nagent via meta-reinforcement learning.\narXiv preprint\narXiv:2111.03431, 2021.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya\nGrover, Michael Laskin, Pieter Abbeel, Aravind Srinivas,\nand Igor Mordatch. Decision transformer: Reinforcement\nlearning via sequence modeling. In Advances in Neural\nInformation Processing Systems, 2021.\nWei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Con-\ntextual bandits with linear payoff functions. In Proceedings\nof the International Conference on Artiﬁcial Intelligence\nand Statistics, 2011.\nKarl Cobbe, Christopher Hesse, Jacob Hilton, and John\nSchulman.\nLeveraging procedural generation to bench-\nmark reinforcement learning. In Proceedings of the Inter-\nnational Conference on Machine Learning, 2020.\nJonathan Cook, Chris Lu, Edward Hughes, Joel Z. Leibo, and\nJakob Nicolaus Foerster.\nArtiﬁcial Generational Intelli-\ngence: Cultural Accumulation in Reinforcement Learning.\nIn The Thirty-eighth Annual Conference on Neural Infor-\nmation Processing Systems, 2024.\nZhenwen Dai,\nFederico Tomasi,\nand Sina Ghiassian.\nIn-context Exploration-Exploitation for Reinforcement\nLearning. ArXiv preprint, 2024.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma,\nRui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu\nLiu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. A\nSurvey on In-context Learning. ArXiv preprint, 2023.\nJuncheng Dong, Moyang Guo, Ethan X. Fang, Zhuoran Yang,\nand Vahid Tarokh.\nIn-Context Reinforcement Learning\nWithout Optimal Action Labels. In ICML 2024 Workshop\non In-Context Learning, 2024.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya\nSutskever, and Pieter Abbeel.\nRL$ˆ2$: Fast Reinforce-\nment Learning via Slow Reinforcement Learning. ArXiv\npreprint, 2016.\nAhmad Elawady, Gunjan Chhablani, Ram Ramrakhya,\nKarmesh Yadav, Dhruv Batra, Zsolt Kira, and Andrew\nSzot. ReLIC: A Recipe for 64k Steps of In-Context Re-\ninforcement Learning for Embodied AI. ArXiv preprint,\n2024.\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and\nSergey Levine. Rvs: What is essential for ofﬂine RL via\nsupervised learning? In Proceedings of the International\nConference on Learning Representations, 2022.\nHiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Gen-\neralized decision transformer for ofﬂine hindsight informa-\ntion matching. In Proceedings of the International Confer-\nence on Learning Representations, 2022.\nMatthias Gerstgrasser and David C Parkes. Meta-rl for multi-\nagent rl: Learning to adapt to evolving agents. In Advances\nin Neural Information Processing Systems, 2022.\nJake Grigsby, Linxi Fan, and Yuke Zhu. AMAGO: Scalable\nIn-Context Reinforcement Learning for Adaptive Agents.\nArXiv preprint, 2023.\nJake Grigsby, Justin Sasek, Samyak Parajuli, Daniel Adebi,\nAmy Zhang, and Yuke Zhu.\nAMAGO-2: Breaking the\nMulti-Task Barrier in Meta-Reinforcement Learning with\nTransformers. ArXiv preprint, 2024.\nAlbert Gu and Tri Dao. Mamba: Linear-Time Sequence Mod-\neling with Selective State Spaces. ArXiv preprint, 2023.\nMatteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Si-\nmon Schmitt, Laurent Sifre, Theophane Weber, David Sil-\nver, and Hado Van Hasselt. Muesli: Combining improve-\nments in policy optimization. In International conference\non machine learning, 2021.\nSili Huang, Jifeng Hu, Hechang Chen, Lichao Sun, and\nBo Yang. In-Context Decision Transformer: Reinforce-\nment Learning via Hierarchical Chain-of-Thought. ArXiv\npreprint, 2024.\nSili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo,\nHechang Chen, Lichao Sun, and Bo Yang.\nDecision\nMamba: Reinforcement Learning via Hybrid Selective Se-\nquence Modeling. ArXiv preprint, 2024.\nChi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-\nlearning—a simple, efﬁcient, decentralized algorithm for\nmultiagent reinforcement learning. Mathematics of Oper-\nations Research, 2024.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization. ArXiv preprint, 2017.\nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim\nRockt¨aschel.\nA Survey of Zero-shot Generalisation in\nDeep Reinforcement Learning. ArXiv preprint, 2021.\nLouis Kirsch, James Harrison, C. Daniel Freeman, Jascha\nSohl-Dickstein,\nand J¨urgen Schmidhuber.\nTowards\nGeneral-Purpose In-Context Learning Agents. In NeurIPS\n2023 Foundation Models for Decision Making Workshop,\n2023.\nAkshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril\nZhang, and Aleksandrs Slivkins. Can large language mod-\nels explore in-context? ArXiv preprint, 2024.\nMichael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto,\nStephen\nSpencer,\nRichie\nSteigerwald,\nDJ\nStrouse,\nSteven Stenberg Hansen, Angelos Filos, Ethan A. Brooks,\nMaxime Gazeau, Himanshu Sahni, Satinder Singh, and\nVolodymyr Mnih. In-context reinforcement learning with\nalgorithm distillation. In Proceedings of the International\nConference on Learning Representations, 2023.\nJonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak,\nChelsea Finn, Oﬁr Nachum, and Emma Brunskill. Super-\nvised pretraining can learn in-context reinforcement learn-\ning. In Advances in Neural Information Processing Sys-\ntems, 2023.\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,\nNicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. Continuous control with deep reinforce-\nment learning. ArXiv preprint, 2019.\nLicong Lin, Yu Bai, and Song Mei. Transformers as Decision\nMakers: Provable In-Context Reinforcement Learning via\nSupervised Pretraining. ArXiv preprint, 2023.\nHao Liu and Pieter Abbeel. Emergent agentic transformer\nfrom chain of hindsight experience. In Proceedings of the\nInternational Conference on Machine Learning, 2023.\nMinghuan Liu, Menghui Zhu, and Weinan Zhang.\nGoal-\nconditioned reinforcement learning: Problems and solu-\ntions. In Proceedings of the International Joint Conference\non Artiﬁcial Intelligence, 2022.\nChris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto,\nJakob N. Foerster, Satinder Singh, and Feryal M. P. Be-\nhbahani. Structured state space models for in-context re-\ninforcement learning. In Advances in Neural Information\nProcessing Systems, 2023.\nLuckeciano C. Melo. Transformers are meta-reinforcement\nlearners. In Proceedings of the International Conference\non Machine Learning, 2022.\nLinghui Meng, Muning Wen, Chenyang Le, Xiyun Li, Deng-\npeng Xing, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun\nWang, Yaodong Yang, et al.\nOfﬂine pre-trained multi-\nagent decision transformer.\nMachine Intelligence Re-\nsearch, 2023.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter\nAbbeel. A simple neural attentive meta-learner. In Pro-\nceedings of the International Conference on Learning Rep-\nresentations, 2018.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, An-\ndrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,\net al.\nHuman-level control through deep reinforcement\nlearning. nature, 2015.\nBen Norman and Jeff Clune.\nFirst-Explore, then Exploit:\nMeta-Learning to Solve Hard Exploration-Exploitation\nTrade-Offs. ArXiv preprint, 2023.\nChanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing\nZhang. Do LLM Agents Have Regret? A Case Study in\nOnline Learning and Games. ArXiv preprint, 2024.\nKate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and\nDeirdre Quillen. Efﬁcient off-policy meta-reinforcement\nlearning via probabilistic context variables. In Proceed-\nings of the International Conference on Machine Learning,\n2019.\nSharath Chandra Raparthy, Eric Hambro, Robert Kirk,\nMikael Henaff, and Roberta Raileanu. Generalization to\nNew Sequential Decision Making Tasks with In-Context\nLearning. ArXiv preprint, 2023.\nSamuel Ritter, Jane X. Wang, Zeb Kurth-Nelson, Sid-\ndhant M. Jayakumar, Charles Blundell, Razvan Pascanu,\nand Matthew Botvinick.\nBeen there, done that: Meta-\nlearning with episodic recall.\nIn Proceedings of the In-\nternational Conference on Machine Learning, 2018.\nDaniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian\nOsband, Zheng Wen, et al. A tutorial on thompson sam-\npling.\nFoundations and Trends® in Machine Learning,\n2018.\nThomas Schmied, Thomas Adler, Vihang Patil, Maximilian\nBeck, Korbinian P¨oppel, Johannes Brandstetter, G¨unter\nKlambauer, Razvan Pascanu, and Sepp Hochreiter.\nA\nLarge Recurrent Action Model: xLSTM enables Fast In-\nference for Robotics Tasks. ArXiv preprint, 2024.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization al-\ngorithms. ArXiv preprint, 2017.\nShai Shalev-Shwartz and Yoram Singer. A primal-dual per-\nspective of online learning algorithms. Machine Learning,\n2007.\nLucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby, Linxi Fan,\nand Yuke Zhu. Cross-episodic curriculum for transformer\nagents. In Advances in Neural Information Processing Sys-\ntems, 2023.\nChengshuai Shi, Kun Yang, Jing Yang, and Cong Shen.\nTransformers as Game Players: Provable In-context Game-\nplaying Capabilities of Pre-trained Models. ArXiv preprint,\n2024.\nViacheslav Sinii, Alexander Nikulin, Vladislav Kurenkov,\nIlya Zisman, and Sergey Kolesnikov.\nIn-Context Rein-\nforcement Learning for Variable Action Spaces.\nArXiv\npreprint, 2023.\nJimmy T. H. Smith, Andrew Warrington, and Scott W. Lin-\nderman. Simpliﬁed state space layers for sequence mod-\neling. In Proceedings of the International Conference on\nLearning Representations, 2023.\nBradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan\nDuan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever.\nSome Considerations on Learning to Explore via Meta-\nReinforcement Learning. ArXiv preprint, 2019.\nRichard S Sutton and Andrew G Barto. Reinforcement Learn-\ning: An Introduction (2nd Edition). MIT press, 2018.\nRichard S. Sutton. Learning to predict by the methods of\ntemporal differences. Machine Learning, 1988.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A\nphysics engine for model-based control. In Proceedings\nof the International Conference on Intelligent Robots and\nSystems, 2012.\nJohn N. Tsitsiklis and Benjamin Van Roy.\nAverage cost\ntemporal-difference learning. Automatica, 1999.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neu-\nral Information Processing Systems, 2017.\nJane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hu-\nbert Soyer, Joel Z. Leibo, Remi Munos, Charles Blundell,\nDharshan Kumaran, and Matt Botvinick. Learning to rein-\nforcement learn. ArXiv preprint, 2016.\nJiuqi Wang, Ethan Blaser, Hadi Daneshmand, and Shangtong\nZhang. Transformers Learn Temporal Difference Methods\nfor In-Context Reinforcement Learning. ArXiv preprint,\n2024.\nZhe Wang, Haozhu Wang, and Yanjun Qi.\nHierarchical\nPrompt Decision Transformer: Improving Few-Shot Pol-\nicy Generalization with Global and Adaptive Guidance.\nArXiv preprint, 2024.\nMuning Wen, Runji Lin, Hanjing Wang, Yaodong Yang, Ying\nWen, Luo Mai, Jun Wang, Haifeng Zhang, and Weinan\nZhang. Large Sequence Models for Sequential Decision-\nMaking: A Survey. ArXiv preprint, 2023.\nMengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding\nZhao, Joshua B. Tenenbaum, and Chuang Gan. Prompting\ndecision transformer for few-shot policy generalization. In\nProceedings of the International Conference on Machine\nLearning, 2022.\nTengye Xu, Zihao Li, and Qinyuan Ren. Meta-Reinforcement\nLearning Robust to Distributional Shift Via Performing\nLifelong In-Context Learning. In Proceedings of the In-\nternational Conference on Machine Learning, 2024.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nAvnish Narayan, Hayden Shively, Adithya Bellathur, Karol\nHausman, Chelsea Finn, and Sergey Levine. Meta-World:\nA Benchmark and Evaluation for Multi-Task and Meta Re-\ninforcement Learning. ArXiv preprint, 2021.\nXunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang.\nA survey on model compression for large language mod-\nels.\nTransactions of the Association for Computational\nLinguistics, 2024.\nLuisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Se-\nbastian Schulze, Yarin Gal, Katja Hofmann, and Shimon\nWhiteson.\nVaribad:\nA very good method for bayes-\nadaptive deep RL via meta-learning.\nIn Proceedings of\nthe International Conference on Learning Representations,\n2020.\nIlya Zisman, Vladislav Kurenkov, Alexander Nikulin, Viach-\neslav Sinii, and Sergey Kolesnikov.\nEmergence of In-\nContext Reinforcement Learning from Noise Distillation.\nArXiv preprint, 2023.\nIlya Zisman, Alexander Nikulin, Andrei Polubarov, Nikita\nLyubaykin, and Vladislav Kurenkov. N-Gram Induction\nHeads for In-Context RL: Improving Stability and Reduc-\ning Data Needs. ArXiv preprint, 2024.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2025-02-11",
  "updated": "2025-02-11"
}