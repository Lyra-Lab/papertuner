{
  "id": "http://arxiv.org/abs/2502.10303v1",
  "title": "Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations",
  "authors": [
    "Abdelrhman Shaheen",
    "Anas Badr",
    "Ali Abohendy",
    "Hatem Alsaadawy",
    "Nadine Alsayad"
  ],
  "abstract": "Reinforcement Learning (RL) has been widely used in many applications,\nparticularly in gaming, which serves as an excellent training ground for AI\nmodels. Google DeepMind has pioneered innovations in this field, employing\nreinforcement learning algorithms, including model-based, model-free, and deep\nQ-network approaches, to create advanced AI models such as AlphaGo, AlphaGo\nZero, and MuZero. AlphaGo, the initial model, integrates supervised learning\nand reinforcement learning to master the game of Go, surpassing professional\nhuman players. AlphaGo Zero refines this approach by eliminating reliance on\nhuman gameplay data, instead utilizing self-play for enhanced learning\nefficiency. MuZero further extends these advancements by learning the\nunderlying dynamics of game environments without explicit knowledge of the\nrules, achieving adaptability across various games, including complex Atari\ngames. This paper reviews the significance of reinforcement learning\napplications in Atari and strategy-based games, analyzing these three models,\ntheir key innovations, training processes, challenges encountered, and\nimprovements made. Additionally, we discuss advancements in the field of\ngaming, including MiniZero and multi-agent models, highlighting future\ndirections and emerging AI models from Google DeepMind.",
  "text": "Reinforcement Learning in Strategy-Based and\nAtari Games: A Review of Google DeepMind’s\nInnovations\nAbdelrhman Shaheen\nComputer Science Engineering Undergraduate\nStudent\nEgypt Japan University of Science and Technology\nAlexandria, Egypt\nabdelrhman.shaheen@ejust.edu.eg\nAnas Badr\nComputer Science Engineering Undergraduate\nStudent\nEgypt Japan University of Science and Technology\nAlexandria, Egypt\nanas.badr@ejust.edu.eg\nAli Abohendy\nComputer Science Engineering Undergraduate\nStudent\nEgypt Japan University of Science and Technology\nAlexandria, Egypt\nali.abohendy@ejust.edu.eg\nHatem Alsaadawy\nComputer Science Engineering Undergraduate\nStudent\nEgypt Japan University of Science and Technology\nAlexandria, Egypt\nhatem.alsaadawy@ejust.edu.eg\nNadine Alsayad\nComputer Science Engineering Undergraduate\nStudent\nEgypt Japan University of Science and Technology\nAlexandria, Egypt\nnadine.alsayad@ejust.edu.eg\nAbstract—Reinforcement Learning (Rl) has been widely used\nin many applications, one of these applications is the field\nof gaming, which is considered a very good training ground\nfor AI models. From the innovations of Google DeepMind in\nthis field using the reinforcement learning algorithms, includ-\ning model-based, model-free, and deep Q-network approaches,\nAlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial\nmodel, integrates supervised learning, reinforcement learning to\nachieve master in the game of Go, surpassing the performance of\nprofessional human players. AlphaGo Zero refines this approach\nby eliminating the dependency on human gameplay data, instead\nemploying self-play to enhance learning efficiency and model\nperformance. MuZero further extends these advancements by\nlearning the underlying dynamics of game environments without\nexplicit knowledge of the rules, achieving adaptability across\nmany games, including complex Atari games. In this paper,\nwe reviewed the importance of studying the applications of\nreinforcement Learning in Atari and strategy-based games, by\ndiscussing these three models, the key innovations of each\nmodel,and how the training process was done; then showing\nthe challenges that every model faced, how they encounterd\nthem, and how they improved the performance of the model.\nWe also highlighted the advancements in the field of gaming,\nincluding the advancment in the three models, like the MiniZero\nand multi-agent models, showing the future direction for these\nadvancements, and new models from Google DeepMind.\nIndex Terms—Deep Reinforcement Learning, Google Deep-\nMind, AlphaGo, AlphaGo Zero, MuZero, Atari Games, Go,\nChess, Shogi,\nI. INTRODUCTION\nArtificial Intelligence (AI) has revolutionized the gaming\nindustry, both as a tool for creating intelligent in-game oppo-\nnents and as a testing environment for advancing AI research.\nGames serve as an ideal environment for training and eval-\nuating AI systems because they provide well-defined rules,\nmeasurable objectives, and diverse challenges. From simple\npuzzles to complex strategy games, AI research in gaming has\npushed the boundaries of machine learning and reinforcement\nlearning. Also The benfits from such employment helped game\ndevelopers to realize the power of AI methods to analyze large\nvolumes of player data and optimize game designs. [1]\nAtari games, in particular, with their retro visuals and straight-\nforward mechanics, offer a challenging yet accessible bench-\nmark for testing AI algorithms. The simplicity of Atari games\nhides complexity that they require strategies that involve\nplanning, adaptability, and fast decision-making, making them\nalso a good testing environment for evaluating AI’s ability to\n1\narXiv:2502.10303v1  [cs.AI]  14 Feb 2025\nlearn and generalize. The development of AI in games has\nbeen a long journey, starting with rule-based systems and\nevolving into more sophisticated machine learning models.\nHowever, the machine learning models had a few challenges,\nfrom these challenges is that the games employing AI involves\ndecision making in the game evironment. Machine learning\nmodels are unable to interact with the decisions that the user\nmake because it depends on learning from datasets and have no\ninteraction with the environment. To overcome such problem,\ngame developers started to employ reinforcement learning\n(RL) in developing games. Years later, deep learning (DL)\nwas developed and shows remarkable results in video games\n[2]. The combination between both reinforcement learning\nand deep learning resulted in Deep Reinforcement Learning\n(DRL). The first employment of DRL was by game devel-\nopers in atari game [3]. One of the famous companies that\nemployed DRL in developing AI models is Google DeepMind.\nThis company is known for developing AI models, including\ngames. Google DeepMind passed through a long journey in\ndeveloping AI models for games. Prior to the first DRL game\nmodel they develop, which is AlphaGo, Google DeepMind\ngave a lot of contributions in developing DRL, by which these\ncontributions were first employed in Atari games.\nFor the employment of DRL in games to be efficient, solving\ntasks in games need to be sequential, so Google DeepMind\ncombined RL-like techniques with neural networks to create\nmodels capable of learning algorithms and solving tasks that\nrequire memory and reasoning, which is the Neural Turing\nMachines (NTMs) [4]. They then introduced the Deep Q-\nnetwork (DQN) algorithm, which is combine deep learning\nwith Q-learning and RL algorithm. Q-learning is a model in\nreinforcement learning which use the Q-network, which is is a\ntype of neural network to approximate the Q-function, which\npredicts the value of taking a particular action in a given state\n[5]. The DQN algorithm was the first algorithm that was able\nto learn directly from high-dimensional sensory input, the data\nthat have a large number of features or dimensions [6].\nTo enhance the speed of learning in reinforcement learning\nagents, Google DeepMind introduced the concept of expe-\nrience replay, which is a technique that randomly samples\nprevious experiences from the agent’s memory to break the\ncorrelation between experiences and stabilize the learning\nprocess [7]. They then developed asynchronus methods for\nDRL, which is the Actor-Critic (A3C) model. This model\nshowed faster and more stable training and showed a remark-\nable performance in Atari games [8]. By the usage of these\nalgorithms, Google DeepMind was able to develop the first AI\nmodel that was able to beat the world champion in the game\nof Go, which is AlphaGo in 2016.\nThe paper is organized as follows: Section II presents the\nrelated work that surveys the development of DRL in games\nand the contribution that we added to the previous surveys.\nSection III presents the background information about the\ndevelopment of DRL in games. Section IV presents the\nfirst AI model that Google DeepMind developed, which is\nAlphaGo. Section V presents AlphaGo Zero. Section VI\npresents MuZero. Section VII presents the advancements that\nwere made in developing AI models for games. Section VIII\npresents the future directions that AI models for games will\ntake, and their applications in real life.\nII. RELATED WORK\nThere are a lot of related work that reviewed the reinforce-\nment learning in strategy-based and atari games. Arulkumaran\net al [9] this paper serves as a foundational reference that out-\nlines the evolution and state-of-the-art developments in DRL\nup to its publication. It also offers insights into how combining\ndeep learning with reinforcement learning has led to significant\nadvancements in areas such as game playing, robotics, and\nautonomous decision-making systems. Zhao et al. [10] surveys\nhow DRL combines capabilities of deep learning with the\ndecision-making processes of reinforcement learning, enabling\nsystems to make control decisions directly from input images.\nIt also analysis the development of AlphaGo, and examines\nthe algorithms and techniques that contributed to AlphaGo’s\nsuccess, providing insights into the integration of DRL in com-\nplex decision-making tasks. Tang et al. [11] also surveys how\nAlphaGo marked a significant milestone by defeating human\nchampions in the game of Go, and its architecture and training\nprocess; then delves into AlphaGo Zero. Shao et al. [12]\ncategorize DRL methods into three primary approaches: value-\nbased, policy gradient, and model-based algorithms, offering\na comparative analysis of their techniques and properties.\nThe survey delves into the implementation of DRL across\nvarious video game types, ranging from classic arcade games\nto complex real-time strategy games. It highlights how DRL\nagents, equipped with deep neural network-based policies,\nprocess high-dimensional inputs to make decisions that maxi-\nmize returns in an end-to-end learning framework. this review\nalso discusses the achievement of superhuman performance\nby DRL agents in several games, underscoring the significant\nprogress in this field. However, it also addresses ongoing\nchallenges such as exploration-exploitation balance, sample\nefficiency, generalization and transfer learning, multi-agent\ncoordination, handling imperfect information, and managing\ndelayed sparse rewards.\nOur paper is similar to Shao et al. [12], as we discussed\nthe developments that Google DeepMind made in developing\nAI models for games and the advancments that they made\nover the last years to develop the models and the future\ndirections of implementating the DRL in games; how this\nimplementation helps in developing real life applications. The\nmain contribution in our paper is the comprehensive details\nof the three models AlphaGo, AlphaGo Zero, and MuZero,\nfocusing on the key Innovations for each model, how the\ntraining process was done, challenges that each model faced\nand the improvements that were made, and the preformance\nbenchmarks. Studying each on of these models in details helps\nin understanding how RL was developed in games reaching\nto the current state, by which it is now used in real life\napplications. Also we discussed the advancments in these three\nAI models, reaching to the future directions.\n2\nIII. BACKGROUND\nReinforcement Learning (RL) is a key area of machine\nlearning that focuses on learning through interaction with the\nenvironment. In RL, an agent takes actions (A) in specific\nstates (S) with the goal of maximizing the rewards (R)\nreceived from the environment. The foundations of RL can\nbe traced back to 1911, when Thorndike introduced the\nLaw of Effect, suggesting that actions leading to favorable\noutcomes are more likely to be repeated, while those causing\ndiscomfort are less likely to recur [13].\nRL emulates the human learning process of trial and error.\nThe agent receives positive rewards for beneficial actions and\nnegative rewards for detrimental ones, enabling it to refine\nits policy function—a strategy that dictates the best action to\ntake in each state. That’s said, for a give agent in state u, if it\ntakes action u, then the immediate reward r can be modeled\nas r(x, u) = E[rt | x = xt−1, u = ut−1].\nSo for a full episode of T steps, the cumulative reward R\ncan be modeled as R = PT\nt=1 rt.\nA. Markov Decision Process (MDP)\nIn reinforcement learning, the environment is often modeled\nas a Markov Decision Process (MDP), which is defined as\na tuple (S, A, P, R, γ), where:\n• S is the set of states,\n• A is the set of actions,\n• P is the transition probability function,\n• R is the reward function, and\n• γ is the discount factor.\nThe MDP framework is grounded in sequential decision-\nmaking, where the agent makes decisions at each time step\nbased on its current state. This process adheres to the Markov\nproperty, which asserts that the future state and reward\ndepend only on the present state and action, not on the history\nof past states and actions.\nFormally, the Markov property is represented by:\nP(s′ | s, a) = P[st+1 = s′ | st = s, at = a]\n(1)\nwhich denotes the probability of transitioning from state s\nto state s′ when action a is taken.\nThe reward function R is similarly defined as:\nR(s, a) = E[rt | st−1 = s, at−1 = a]\n(2)\nwhich represents the expected reward received after taking\naction a in state S.\nB. Policy and Value Functions\nIn reinforcement learning, an agent’s goal is to find the\noptimal policy that the agent should follow to maximize\ncumulative rewards over time. To facilitate this process, we\nneed to quantify the desirability of a given state, which is done\nthrough the value function V (s). Value function estimates the\nexpected cumulative reward an agent will receive starting from\nstate s and continuing thereafter. In essence, the value function\nreflects how beneficial it is to be in a particular state, guiding\nthe agent’s decision-making process. The state-value function\nis then defined as:\nVπ(s) = Eπ[Gt | st = s]\n= Eπ[rt + γrt+1 + . . . | st = s]\n(3)\nwhere Gt is the cumulative reward from time step t on-\nwards. From here we can define another value function the\naction-value function under policy π, which is Qπ(s, a), that\nestimates the expected cumulative reward from the state s and\ntaking action a and then following policy π:\nQπ(s, a) = Eπ[Gt | st = s, at = a]\n= Eπ[rt + γrt+1 + . . . | st = s, at = a]\n(4)\nwhere γ is the discount factor, which is a decimal value\nbetween 0 and 1 that detemines how much we care about\nimmediate rewards versus future reward rewards [14].\nWe say that a policy π is better than another policy π′\nif the expected return of every state under π is greater than\nor equal to the expected return of every state under π′, i.e.,\nVπ(s) ≥Vπ′(s) for all s ∈S. Eventually, there will be a policy\n(or policies) that are better than or equal to all other policies,\nthis is called the optimal policy π∗. All optimal policies will\nthen share the same optimal state-value function V ∗(s) and\nthe same optimal action-value function Q∗(s, a), which are\ndefined as:\nV ∗(s) = max\nπ\nVπ(s)\nQ∗(s, a) = max\nπ\nQπ(s, a)\n(5)\nIf we can estimate the optimal state-value (or action-value)\nfunction, then the optimal policy π∗can be obtained by\nselecting the action that maximizes the state-value (or action-\nvalue) function at each state, i.e., π∗(s) = arg maxa Q∗(s, a)\nand that’s the goal of reinforcement learning [14].\nC. Reinforcement Learning Algorithms\nThere are multiple reinforcement learning algorithms that\nhave been developed that falls under a lot of categories. But,\nfor the sake of this review, we will focus on the following\nalgorithms that have been used by the Google DeepMind\nteam in their reinforcement learning models.\n1) Model-Based Algorithms: Dynamic Programming\nDynamic programming (DP) algorithms can be applied\nwhen we have a perfect model of the environment, represented\nby the transition probability function P(s′, r | s, a). These\nalgorithms rely on solving the Bellman equations (recursive\nform of equations 3 and 4) iteratively to compute optimal\npolicies. The process alternates between two key steps: policy\nevaluation and policy improvement.\n3\n1.1\nPolicy Evaluation\nPolicy evaluation involves computing the value function V π(s)\nunder a given policy π. This is achieved iteratively by updating\nthe value of each state based on the Bellman equation:\nV π(s) =\nX\na∈A\nπ(a | s)\nX\ns′,r\nP(s′, r | s, a) [r + γV π(s′)] . (6)\nStarting with an arbitrary initial value V π(s), the updates\nare repeated for all states until the value function converges\nto a stable estimate.\n1.2\nPolicy Improvement\nOnce the value function V π(s) has been computed, the policy\nis improved by choosing the action a that maximizes the\nexpected return for each state:\nπ′(s) = arg max\na\nX\ns′,r\nP(s′, r | s, a) [r + γV π(s′)] .\n(7)\nThis step ensures that the updated policy π′ is better than or\nequal to the previous policy π. The process alternates between\npolicy evaluation and improvement until the policy converges\nto the optimal policy π∗, where no further improvement is\npossible. It can be visualized as:\nπ0\nEval\n−−→V π0\nImprove\n−−−−→π1\nEval\n−−→V π1\nImprove\n−−−−→π2\nEval\n−−→. . .\nImprove\n−−−−→π∗.\n1.3\nValue Iteration\nValue iteration simplifies the DP process by combining policy\nevaluation and policy improvement into a single step. Instead\nof evaluating a policy completely, it directly updates the value\nfunction using:\nV ∗(s) = max\na\nX\ns′,r\nP(s′, r | s, a) [r + γV ∗(s′)] .\n(8)\nThis method iteratively updates the value of each state\nuntil convergence and implicitly determines the optimal policy.\nThen the optimal policy can be obtained by selecting the action\nthat maximizes the value function at each state, as\nπ∗(s) = arg max\na\nX\ns′,r\nP(s′, r | s, a) [r + γV ∗(s′)] .\n(9)\nDynamic Programming’s systematic approach to policy\nevaluation and improvement forms the foundation for the\ntechniques that have been cruical in training systems like\nAlphaGo Zero and MuZero.\n2) Model-Free Algorithms\n2.1\nMonte Carlo Algorithm (MC)\nThe Monte Carlo (MC) algorithm is a model-free reinforce-\nment learning method that estimates the value of states or\nstate-action pairs under a given policy by averaging the returns\nof multiple episodes. Unlike DP, MC does not require a perfect\nmodel of the environment and instead learns from sampled\nexperiences.\nThe key steps in MC include:\n• Policy Evaluation: Estimate the value of a state or state-\naction pair Q(s, a) by averaging the returns observed in\nmultiple episodes.\n• Policy Improvement: Update the policy π to choose\nactions that maximize the estimated value Q(s, a).\nMC algorithms operate on complete episodes, requiring the\nagent to explore all state-action pairs sufficiently to ensure\naccurate value estimates. The updated policy is given by:\nπ(s) = arg max\na\nQ(s, a).\n(10)\nWhile both MC and DP alternate between policy evalua-\ntion and policy improvement, MC works with sampled data,\nmaking it suitable for environments where the dynamics are\nunknown or difficult to model.\nThis algorithm is particularly well-suited for environments\nthat are episodic, where each episode ends in a terminal state\nafter a finite number of steps.\nMonte Carlo’s reliance on episodic sampling and policy re-\nfinement has directly influenced the development of search-\nbased methods like Monte Carlo Tree Search (MCTS), which\nwas crucial in AlphaGo for evaluating potential moves during\ngameplay. The algorithm’s adaptability to model-free settings\nhas made it a cornerstone of modern reinforcement learning\nstrategies.\n2.2\nTemporal Diffrence (TD)\nTemporal Diffrence is another model free algorithm that’s very\nsimilar To Monte Carlo, but instead of waiting for termination\nof the episode to give the return, it estimates the return based\non the next state. The key idea behind TD is to update the\nvalue function based on the difference between the current\nestimate and the estimate of the next state. The TD return is\nthen given by:\nGt = rt+1 + γV (st+1)\n(11)\nthat’s the target (return value estimation) of state s at time t is\nthe immediate reward r plus the discounted value of the next\nstate st+1.\nThis here is called the TD(0) algorithm, which is the simplest\nform of TD that take only one future step into account. The\nupdate rule for TD(0) is:\nV (st) = V (st) + α[rt+1 + γV (st+1) −V (st)]\n(12)\nThere are other temporal difference algorithms that works\nexactly like TD(0), but with more future steps, like TD(λ).\nAnother important variant of TD is the Q-learning algo-\nrithm, which is an off-policy TD algorithm that estimates\nthe optimal action-value function Q∗by updating the current\naction value based on the optimal action value of the next\nstate. The update rule for Q-learning is:\nQ(st, at) = Q(st, at)+α[rt+1+γ max\na\nQ(st+1, a)−Q(st, at)].\n(13)\nand after the algorithm converges, the optimal policy can\nbe obtained by selecting the action that maximizes the action-\nvalue function at each state, as π∗(s) = arg maxa Q∗(s, a).\nTemporal Difference methods, including Q-learning, play\na crucial role in modern reinforcement learning by enabling\n4\nmodel-free value function estimation and action selection\nwithout the need to terminate the episode. In systems like\nAlphaGo and MuZero, TD methods are used to update value\nfunctions efficiently and support complex decision-making\nprocesses without requiring a model of the environment.\n3) Deep RL: Deep Q-Network (DQN)\nDeep Q-Networks (DQN) represent a significant leap for-\nward in the integration of deep learning with reinforcement\nlearning. DQN extends the traditional Q-learning algorithm\nby using a deep neural network to approximate the Q-value\nfunction, which is essential in environments with large state\nspaces where traditional tabular methods like Q-learning be-\ncome infeasible.\nIn standard Q-learning, the action-value function Q(s, a)\nis learned iteratively based on the Bellman equation, which\nupdates the Q-values according to the reward received and the\nvalue of the next state. However, when dealing with complex,\nhigh-dimensional inputs such as images or unstructured data,\na direct tabular representation of the Q-values is not practical.\nThis is where DQN comes in: it uses a neural network, typ-\nically a convolutional neural network (CNN), to approximate\nQ(s, a; θ), where θ represents the parameters of the network.\nThe core ideas behind DQN are similar to those of tradi-\ntional Q-learning, but with a few key innovations that address\nissues such as instability and high variance during training.\nThe DQN algorithm introduces the following components:\n• Experience Replay: To improve the stability of training\nand to break the correlation between consecutive expe-\nriences, DQN stores the agent’s experiences in a replay\nbuffer. Mini-batches of experiences are randomly sampled\nfrom this buffer to update the network, which helps in\nbetter generalization.\n• Target Network: DQN uses two networks: the primary\nQ-network and a target Q-network. The target network is\nupdated less frequently than the primary network and is\nused to calculate the target value in the Bellman update.\nThis reduces the risk of oscillations and divergence during\ntraining.\nThe update rule for DQN is based on the Bellman equation\nfor Q-learning, but with the neural network approximation:\nQ(st, at; θ) = Q(st, at; θ)+\nα\nh\nrt+1 + γ max\na′ Q(st+1, a′; θ−) −Q(st, at; θ)\ni\n(14)\nwhere θ−represents the parameters of the target network.\nBy training the network to minimize the difference between\nthe predicted Q-values and the target Q-values, the agent learns\nan optimal policy over time. [16]\nThe DQN algorithm revolutionized reinforcement learning,\nespecially in applications requiring decision-making in high-\ndimensional spaces. One of the most notable achievements\nof DQN was its success in mastering a variety of Atari\n2600 games directly from raw pixel input, achieving human-\nlevel performance across multiple games. This breakthrough\ndemonstrated the power of combining deep learning with\nreinforcement learning to solve complex, high-dimensional\nproblems.\nIn subsequent improvements, such as Double DQN, Dueling\nDQN, and Prioritized Experience Replay, enhancements were\nmade to further stabilize training and improve performance.\nHowever, the foundational concepts of using deep neural\nnetworks to approximate Q-values and leveraging experience\nreplay and target networks remain core to the DQN frame-\nwork.\nIV. ALPHAGO\nA. Introduction\nAlphaGo is a groundbreaking AI model that utilizes neural\nnetworks and tree search to play the game of Go, which\nis thought to be one of the most challenging classic games\nfor artificial intelligence owing to its enormous search space\nand the difficulty of evaluating board positions and moves [18].\nAlphaGo\nuses\nvalue\nnetworks\nfor\nposition\nevaluation\nand policy networks for taking actions, that combined with\nMonte Carlo simulation achieved a 99.8% winning rate, and\nbeating the European human Go champion in 5 out 5 games.\nB. Key Innovations\nIntegration of Policy and Value Networks with MCTS\nAlphaGo combines policy and value networks in an MCTS\nframework to efficiently explore and evaluate the game tree.\nEach edge (s, a) in the search tree stores:\n• Action value Q(s, a): The average reward for taking\naction a from state s.\n• Visit count N(s, a): The number of times this action has\nbeen explored.\n• Prior probability P(s, a): The probability of selecting\naction a, provided by the policy network.\nDuring the selection phase, actions are chosen to maximize:\nat = arg max\na\n(Q(s, a) + u(s, a))\n(15)\nwhere the exploration bonus u(s, a) is defined as:\nu(s, a) ∝\nP(s, a)\n1 + N(s, a)\n(16)\nWhen a simulation reaches a leaf node, its value is evaluated\nin two ways: 1. Value Network Evaluation: A forward pass\nthrough the value network predicts vθ(s), the likelihood of\nwinning. 2. Rollout Evaluation: A lightweight policy simulates\nthe game to its conclusion, and the terminal result z is\nrecorded.\nThese evaluations are combined using a mixing parameter\nλ:\nV (sL) = λvθ(sL) + (1 −λ)zL\n(17)\nThe back propagation step updates the statistics of all\nnodes along the path from the root to the leaf.\nIt’s also worth noting that the SL policy network performed\nbetter than the RL policy network and that’s probably because\n5\nhumans select a diverse beam of promising moves, whereas\nRL optimizes for the single best move.\nConversely though, the value network that was derived from\nthe RL policy performed better than the one derived from the\nSL policy.\nFig. 1. Performance of AlphaGo, on a single machine, for different combi-\nnations of components.\nC. Training Process\n1) Supervised Learning for Policy Networks\nThe policy network was initially trained using supervised\nlearning on human expert games. The training data consisted\nof 30 million board positions sampled from professional games\non the KGS Go Server. The goal was to maximize the\nlikelihood of selecting the human move for each position:\n∆σ ∝∇σ log pσ(a|s)\n(18)\nwhere pσ(a|s) is the probability of selecting action a given\nstate s.\nThis supervised learning approach achieved a move pre-\ndiction accuracy of 57.0% on the test set, significantly outper-\nforming prior methods. This stage provided a solid foundation\nfor replicating human expertise.\n2) Reinforcement Learning for Policy Networks\nThe supervised learning network was further refined through\nreinforcement learning (RL). The weights of the RL policy\nnetwork were initialized from the SL network. AlphaGo then\nengaged in self-play, where the RL policy network played\nagainst earlier versions of itself to iteratively improve.\nThe reward function used for RL was defined as:\nr(s) =\n\n\n\n\n\n+1\nif win\n−1\nif loss\n0\notherwise (non-terminal states).\n(19)\nAt each time step t, the network updated its weights\nto maximize the expected reward using the policy gradient\nmethod:\n∆ρ ∝zt∇ρ log pρ(at|st)\n(20)\nwhere zt is the final game outcome from the perspective of\nthe current player.\nThis self-play strategy allowed AlphaGo to discover novel\nstrategies beyond human knowledge. The RL policy network\noutperformed the SL network with an 80% win rate and\nachieved an 85% win rate against Pachi, a strong open-source\nGo program, without using MCTS.\n3) Value Network Training\nThe value network was designed to evaluate board positions\nby predicting the likelihood of winning from a given state.\nUnlike the policy network, it outputs a single scalar value\nvθ(s) between −1 (loss) and +1 (win).\nTraining the value network on full games led to overfitting\ndue to the strong correlation between successive positions\nin the same game. To mitigate this, a new dataset of 30\nmillion distinct board positions was generated through self-\nplay, ensuring that positions came from diverse contexts.\nThe value network was trained by minimizing the mean\nsquared error (MSE) between its predictions vθ(s) and the\nactual game outcomes z:\nL(θ) = E(s,z)∼D\n\u0002\n(vθ(s) −z)2\u0003\n(21)\nFig. 2. Comparison of evaluation accuracy between the value network and\nrollouts with different policies.\nD. Challenges and Solutions\nAlphaGo overcame several challenges:\n• Overfitting: Training the value network on full games\nled to memorization. This was mitigated by generating\na diverse self-play dataset.\n• Scalability: Combining neural networks with MCTS\nrequired significant computational resources, addressed\nthrough parallel processing on GPUs and CPUs.\n6\n• Exploration vs. Exploitation: Balancing these in MCTS\nwas achieved using the exploration bonus u(s, a) and the\npolicy network priors.\nE. Performance Benchmarks\nAlphaGo achieved the following milestones:\n• 85% win rate against Pachi without using MCTS.\n• 99.8% win rate against other Go programs in a tourna-\nment held to evaluate the performance of AlphaGo.\n• Won 77%, 86%, and 99% of handicap games against\nCrazy Stone, Zen and Pachi, respectively.\n• Victory against professional human players such as Fan\nHui (5-0) and Lee Sedol (4-1), marking a significant\nbreakthrough in AI.\nFig. 3. Elo rating comparison between AlphaGo and other Go programs.\nV. ALPHAGO ZERO\nA. Introduction\nAlphaGo Zero represents a groundbreaking advancement\nin artificial intelligence and reinforcement learning. Unlike its\npredecessor, AlphaGo, which relied on human gameplay data\nfor training, AlphaGo Zero learns entirely from self-play,\nemploying deep neural networks and Monte Carlo Tree\nSearch (MCTS). [19]\nBy starting with only the rules of the game and leveraging\nreinforcement learning, AlphaGo Zero achieved superhuman\nperformance in Go, defeating the previous version of AlphaGo\nin a 100-0 match.\nB. Key Innovations\nAlphaGo Zero introduced several groundbreaking advance-\nments over its predecessor, AlphaGo, streamlining and enhanc-\ning its architecture and training process:\n1) Unified Neural Network fθ: AlphaGo Zero replaced\nAlphaGo’s dual-network setup—separate networks for\npolicy and value—with a single neural network fθ. This\nnetwork outputs both the policy vector p and the value\nscalar v for a given game state, reprsented as\nfθ(s) = (p, v)\n(22)\nThis unified architecture simplifies the model and im-\nproves training efficiency.\n2) Self-Play Training: Unlike AlphaGo, which relied on hu-\nman games as training data, AlphaGo Zero was trained\nentirely through self-play. Starting from random moves,\nit learned by iteratively playing against itself, generating\ndata and refining fθ without any prior knowledge of hu-\nman strategies. This removed biases inherent in human\ngameplay and allowed AlphaGo Zero to discover novel\nand highly effective strategies.\n3) Removal of Rollouts: AlphaGo Zero eliminated the\nneed for rollouts, which were computationally expensive\nsimulations to the end of the game used by AlphaGo’s\nMCTS. Instead, fθ directly predicts the value v of a\nstate, providing a more efficient and accurate estimation.\n4) Superior Performance: By integrating these advance-\nments, AlphaGo Zero defeated AlphaGo 100-0 in direct\nmatches, demonstrating the superiority of its self-play\ntraining, unified architecture, and reliance on raw rules\nover pre-trained human data.\nC. Training Process\n1) Monte Carlo Tree Search (MCTS) as policy evaluation\noperator\nIntially the neural network fθ is not very accurate in\npredicting the best move, as it is intiallised with random\nweights at first. To overcome this, AlphaGo Zero uses MCTS\nto explore the game tree and improve the policy.\nAt a given state S, MCTS expands simualtions of the\nbest moved that are most likely to generate a win based on\nthe initial policy P(s, a) and the value V . MCTS iteratively\nselects moves that maximize the upper confidence bound\n(UCB) of the action value. UCB is designed to balanced\nexploration and exploitation. and it is defined as\nUCB = Q(s, a) + U(s, a)\n(23)\nwhere\nU(s, a) ∝\np(s, a)\n1 + N(s, a)\nMCTS at the end of the search returns the policy vector π\nwhich is used to update the neural network fθ by minimizing\nthe cross-entropy loss between the predicted policy by fθ and\nthe MCTS policy.\n7\n2) Policy Iteration and self play\nThe agent plays games against itself using the predicted\npolicy P(s, a). The agent uses the MCTS to select the best\nmove at each state and the game is played till the end in a\nprocess called self play. The agent then uses the outcome of\nthe game, z game winner and π to update the neural network.\nThis process is repeated for a large number of iterations.\n3) Network Training Process\nThe neural network is updated after each self-play game by\nusing the data collected during the game. This process involves\nseveral key steps:\n1) Intilisation of the network: The neural network starts\nwith random weights θ0, as there is no prior knowledge\nabout the game.\n2) Generating Self-play Games: For each iteration i ≥1\nself-play games are generated. During the game, the\nneural network uses its current parameters θi−1 to run\nMCTS and generate search probabilities πt for each\nmove at time step t.\n3) Game Termination and scoring: A game ends when\neither both players pass, a resignation threshold is met,\nor the game exceeds a maximum length. The winner\nof the game is determined, and the final result zt is\nrecorded, providing feedback to the model.\n4) Data Colletion: for each time step t, the training data\n(st, πt, zt) is stored, where st is the game state, πt is\nthe search probabilities, and zt is the game outcome.\n5) Network training process: after collecing data from self-\nplay, The neural network fθ is adjusted to minimize the\nerror between the predicted value v and the self-play\nwinnder z, and to maximize the similarity between the\nsearch probabilities P and the MCTS probabilities. This\nis done by using a loss function that combines the mean-\nsquared error and the cross entropy losses repsectibly.\nThe loss function is defined as\nL = (z −v)2 −πT log p + c||θ||2\n(24)\nwhere c is the L2 regularization term.\nD. Challenges and Solutions\nAlpha Go Zero overcame several challanges:\n1) Human knowledge Dependency: AlphaGo Zero elimi-\nnated the need for human gameplay data, relying solely\non self-play to learn the game of Go. This allowed it to\ndiscover novel strategies that surpassed human expertise.\n2) Compelxity of the dual network approach in alpha go:\nAlphaGo utilized separate neural networks for policy\nprediction p and value estimation V , increasing the com-\nputational complexity. AlphaGo Zero unified these into\na single network that outputs both p and V , simplifying\nthe architecture and improving training efficiency.\n3) The need of handcrafted features: AlphaGo relied on\nhandcrafted features, such as board symmetry and pre-\ndefined game heuristics, for feature extraction. AlphaGo\nZero eliminated the need for feature engineering by us-\ning raw board states as input, learning representations\ndirectly from the data.\nE. Performance Benchmarks\nAlphaGo Zero introduced a significant improvement in\nneural network architecture by employing a unified residual\nnetwork (ResNet) design for its fθ model. This replaced\nthe separate CNN-based architectures previously used in\nAlphaGo, which consisted of distinct networks for policy\nprediction and value estimation.\nThe superiority of this approach is evident in the Elo\nrating comparison shown in fig.4. The ”dual-res” architecture,\nutilized in AlphaGo Zero, achieved the highest Elo rating,\nsignificantly outperforming other architectures like ”dual-\nconv” and ”sep-conv” used in earlier versions of AlphaGo.\nFig. 4. Elo rating comparison of different neural network architectures.\nVI. MUZERO\nA. Introduction\nThrough the development of AlphaZero, a general model\nfor board games with superhuman ability has been achieved\nin three games: Go, chess, and Shogi. It could achieve these\nresults without the need for human gameplay data or history,\ninstead using self-play in an enclosed environment. However,\nthe model still relied on a simulator that could perfectly\nreplicate the behavior, which might not translate well to real-\nworld applications, where modeling the system might not be\nfeasible. MuZero was developed to address this challenge\nby developing a model-based RL approach that could learn\nwithout explicitly modeling the real environment. This allowed\n8\nfor the same general approach used in AlphaZero to be used\nin Atari environments where reconstructing the environment\nis costly. Essentially, MuZero was deployed to all the games\nwith no prior knowledge of them or specific optimization and\nmanaged to show state-of-the-art results in almost all of them.\nB. MuZero Algorithm\nThe model takes in an input of observations o1, . . . , ot that\nare then fed to a representation network h, which reduces\nthe dimensions of the input and produces a root hidden state\ns0. Internally, the model mirrors an MDP, with each state\nrepresenting a node with edges connecting it to the future\nstates depending on available actions. Unlike traditional RL\napproaches, this hidden state is not constrained to contain\nthe information necessary to reproduce the entire future ob-\nservations. Instead, the hidden states are only optimized for\npredicting information that is related to planning. At every\ntime step, the model predicts the policy, the immediate reward,\nand the value function. The output of the state-action pair is\nthen used by the dynamics function to produce future states.\nSimilar to AlphaZero, a Monte Carlo tree search is used to find\nthe best action policy given an input space. This is used to train\nthe model by comparing the MCTS policy with the predictor\nfunction policy. Also, after a few training runs, the model\nceases to use illegal moves, and the predicted actions map to\nthe real action space. This eliminates the need for a simulator,\nas the model internalizes the environment characteristics it\ndeems necessary for planning and acting, which generally\nconverges to reality through training. The value function at\nthe final step is compared against the game result in board\ngames, i.e., win, loss, or a draw.\nC. Loss function and learning equations\ns0 = hθ(o1, . . . , ot)\n(25)\nrk, sk = gθ(sk−1, ak)\n(26)\npk, vk = fθ(sk)\n(27)\n\n\npk\nvk\nrk\n\n= µθ(o1, . . . , ot, a1, . . . , ak)\n(28)\nνt, πt = MCTS(st\n0, µθ)\n(29)\nat ∼πt\n(30)\npt\nk, vt\nk, rt\nk = µθ(o1, . . . , ot, at+1, . . . , at+k)\n(31)\nzt =\n\n\n\n\n\nuT ,\nfor games\nut+1 + γut+2 + . . .\n+γn−1ut+n + γnνt+n,\nfor general MDPs\nlt(θ) =\nK\nX\nk=0\n\u0002\nlr(ut+k, rt\nk) + lv(zt+k, vt\nk)\n+ lp(πt+k, pt\nk)\n\u0003\n+ c∥θ∥2\n(32)\nlr(u, r) =\n(\n0,\nfor games\nϕ(u)T log r,\nfor general MDPs\n(33)\nlv(z, q) =\n(\n(z −q)2,\nfor games\nϕ(z)T log q,\nfor general MDPs\n(34)\nlp(π, p) = πT log p\n(35)\nD. MCTS\nMuZero uses the same approach developed in AlphaZero\nto find the optimum action given an internal state. MCTS is\nused where states are the nodes, and the edges store visit count,\nmean value, policy, and reward. The search is done in a three-\nphase setup: selection, expansion, and backup. The simulation\nstarts with a root state, and an action is chosen based on the\nstate-transition reward table. Then, after the end of the tree, a\nnew node is created using the output of the dynamics function\nas a value, and the data from the prediction function is stored\nin the edge connecting it to the previous state. Finally, the\nsimulation ends, and the updated trajectory is added to the\nstate-transition reward table. In two-player zero-sum games,\nboard games, for example, the value function is bounded\nbetween 0 and 1, which is helpful to use value estimation\nand probability using the pUCT rule. However, many other\nenvironments have unbounded values, so MuZero rescales the\nvalue to the maximum value observed by the model up to this\ntraining step, ensuring no environment-specific data is needed.\n[20]\nE. Results\nThe MuZero model demonstrated significant improvements\nacross various test cases, achieving state-of-the-art perfor-\nmance in several scenarios. Key findings include:\n1) Board Games\n• When tested on the three board games AlphaZero was\ntrained for (Go, chess, and shogi):\n– MuZero matched AlphaZero’s performance without\nany prior knowledge of the games’ rules.\n– It achieved this with reduced computational cost\ndue to fewer residual blocks in the representation\nfunction.\n2) Atari Games\n• MuZero was tested on 60 Atari games, competing against\nboth human players and state-of-the-art models (model-\nbased and model-free). Results showed:\n– Starting from regular positions: MuZero outper-\nformed competitors in 46 out of 60 games.\n– Starting from random positions: MuZero main-\ntained its lead in 37 out of 60 games, though its\nperformance was reduced.\n9\nFig. 5. (A) Represents the progression of the model through its MDP, while (B) Represents MuZero acting as an environment with MCTS as feedback, and\n(C) Represents a diagram of training MuZero’s model.\n• The\ncomputational\nefficiency\nand\ngeneralization\nof\nMuZero highlight its effectiveness in complex, unstruc-\ntured environments.\n3) Limitations\n• Despite its strengths, MuZero struggled in certain games,\nsuch as:\n– Montezuma’s Revenge and Pitfall, which require\nlong-term planning and strategy.\n• General challenges:\n– Long-term\ndependencies\nremain\ndifficult\nfor\nMuZero, as is the case for RL models in general.\n– Limited input space and lack of combinatorial inputs\nin Atari games could introduce scalability issues for\nbroader applications. [20]\nVII. ADVANCEMENTS\nThe evolution of AI in gaming, particularly through the\ndevelopment of AlphaGo, AlphaGo Zero, and MuZero, high-\nlights remarkable advancements in reinforcement learning and\nartificial intelligence. AlphaGo, the pioneering model, com-\nbined supervised learning and reinforcement learning to master\nthe complex game of Go, setting the stage for AI to exceed\nhuman capabilities in well-defined strategic games. Building\non, AlphaGo Zero eliminated the reliance on human data,\nintroducing a fully self-supervised approach that demonstrated\ngreater efficiency and performance by learning solely through\nself-play. MuZero took this innovation further by generalizing\nbeyond specific games like Go, Chess, and Shogi, employing\nmodel-based reinforcement learning to predict dynamics with-\nout explicitly knowing the rules of the environment. Complet-\ning on these three models, here are some of the advancements\nthat developed from them: AlphaZero and MiniZero; and one\nof the most used in generating AI models, Multi-agent models.\nAlphaZero\nWhile AlphaGo Zero was an impressive feat, designed\nspecifically to master the ancient game of Go through self-play,\nAlphaZero developes it by generalizing its learning framework\nto include multiple complex games: chess, shogi (Japanese\nchess), and Go. The key advancement is in its ability to apply\nthe same algorithm across different games without requiring\ngame-specific adjustments. AlphaZero’s neural network is\ntrained through self-play, predicting the move probabilities\nand game outcomes for various positions. This prediction\nis then used to guide the MCTS, which explores potential\nfuture moves and outcomes to determine the best action.\nThrough iterative self-play and continuous refinement of the\nneural network, AlphaZero efficiently learns and improves\nits strategies across different games [21]. Another significant\nimprovement is in AlphaZero’s generalized algorithm, is that\nit does not need to be fine-tuned for each specific game. This\nwas a departure from AlphaGo Zero’s Go-specific architecture,\nmaking AlphaZero a more versatile AI system.\nAlphaZero’s architecture integrates a single neural network\nthat evaluates both the best moves and the likelihood of win-\nning from any given position, streamlining the learning process\nby eliminating the need for separate policy and value networks\nused in earlier systems. This innovation not only enhances\ncomputational efficiency but also enables AlphaZero to adopt\nunconventional and creative playing styles that diverge from\nestablished human strategies.\nMiniZero\nMiniZero is a a zero-knowledge learning framework that\nsupports four state-of-the-art algorithms, including AlphaZero,\nMuZero, Gumbel AlphaZero, and Gumbel MuZero [22].\nGumbel AlphaZero and Gumbel MuZero are variants of the\nAlphaZero and MuZero algorithms that incorporate Gumbel\nnoise into their decision-making process to improve explo-\nration and planning efficiency in reinforcement learning tasks.\n10\nGumbel noise is a type of stochastic noise sampled from the\nGumbel distribution, commonly used in decision-making and\noptimization problems.\nMiniZero is a simplified version of the original MuZero\nalgorithm, which is designed to be have a more simplified\narchitecture reducing the complexity of the neural network\nused to model environment dynamics, making it easier to\nimplement and experiment with. This simplification allows\nMiniZero to perform well in smaller environments with fewer\nstates and actions, offering faster training times and requiring\nfewer computational power compared to MuZero.\nMulti-agent models\nMulti-agent models in reinforcement learning (MARL) rep-\nresent an extension of traditional single-agent reinforcement\nlearning. In these models, multiple agents are simultaneously\ninteracting, either competitively or cooperatively, making de-\ncisions that impact both their own outcomes and those of other\nagents. The complexity in multi-agent systems arises from\nthe dynamic nature of the environment, where the actions of\neach agent can alter the environment and the states of other\nagents. Unlike in single-agent environments, where the agent\nlearns by interacting with a static world, multi-agent systems\nrequire agents to learn not only from their direct experiences\nbut also from the behaviors of other agents, leading to a more\ncomplex learning process. Agents must adapt their strategies\nbased on what they perceive other agents are doing, and this\nleads to problems such as strategic coordination, deception,\nnegotiation, and competitive dynamics. In competitive sce-\nnarios, agents might attempt to outwit one another, while in\ncooperative scenarios, they must synchronize their actions to\nachieve a common goal [23].\nAlphaGo and AlphaGo Zero are not designed to handle multi-\nagent environments. The core reason lies in their foundational\ndesign, which assumes a single agent interacting with a static\nenvironment. AlphaGo and AlphaGo Zero both rely on model-\nbased reinforcement learning and self-play, where a single\nagent learns by interacting with itself or a fixed opponent,\nrefining its strategy over time. However, these models are not\nbuilt to adapt to the dynamic nature of multi-agent environ-\nments, where the state of the world constantly changes due to\nthe actions of other agents. In AlphaGo and AlphaGo Zero,\nthe environment is well-defined, and the agent’s objective is to\noptimize its moves based on a fixed set of rules. The agents in\nthese models do not need to account for the actions of other\nagents in real-time or consider competing strategies, which are\nessential in multi-agent systems. Additionally, AlphaGo and\nAlphaGo Zero are not designed to handle cooperation or ne-\ngotiation, which are key aspects of multi-agent environments.\nOn the other hand, MuZero offers a more flexible framework\nthat can be adapted to multi-agent environments. Unlike Al-\nphaGo and AlphaGo Zero, MuZero operates by learning the\ndynamics of the environment through its interactions, rather\nthan relying on a fixed model of the world. This approach\nallows MuZero to adapt to various types of environments,\nwhether single-agent or multi-agent, by learning to predict the\nconsequences of actions without needing explicit knowledge\nof the environment’s rules. The key advantage of MuZero in\nmulti-agent settings is its ability to plan and make decisions\nwithout needing to model the entire system upfront. In multi-\nagent environments, this ability becomes essential, as MuZero\ncan dynamically adjust its strategy based on the observed\nbehavior of other agents. By learning not just the immediate\noutcomes but also the strategic implications of others’ actions,\nMuZero can navigate both competitive and cooperative set-\ntings.\nVIII. FUTURE DIRECTIONS\nAs mentioned earlier in the paper, The development of the\nAI models and systems in the field of gaming represent a\ngood training set for the models to study the environment,\naddress the challenges, modify the models, and achieve good\nresults in this field, to judge whether this model is able to be\nimplemented in real world, and how it can be implemented.\nThe main purpose from such models, Google DeepMind,\nthrough the previous years, had been training the models\nto play games, and the main goal was to implement the\nmodels of reinforcement learning in real life, and benefit\nfrom them. DeepMind already started in this implementation\nwith MuZero, and developing other models to be able to be\nimplemented in real life directly.\nMuZero’s first step from research into the real world\nOne of the notable implementations of MuZero has been\nin collaboration with YouTube, where it was used to optimize\nvideo compression within the open-source VP9 codec. This\ninvolved adapting MuZero’s ability to predict and plan, which\nit had previously demonstrated in games, to a complex and\npractical task of video streaming. By optimizing the encoding\nprocess, as shown in fig. 3, MuZero achieved an average bitrate\nreduction of 4% without degrading video quality [24]. This im-\nprovement directly impacts the efficiency of video streaming\nservices such as YouTube, Twitch, and Google Meet, leading\nto faster loading times and reduced data usage for users. This\nimplementation is called MuZero Rate-Controller (MuZero-\nRC). Beyond video compression, this initial application of\nMuZero outside of game research settings exemplifies how\nreinforcement learning agents can address practical real-world\nchallenges. By designing agents with new capabilities to\nenhance products across different sectors, computer systems\ncan be more efficient, less resource-intensive, and increasingly\nautomated [24].\nAlphaFold\nAlphaFold is a model developed by DeepMind that\naddresses one of the challenging problems in biology, which\nis predicting the three-dimensional structures of proteins from\ntheir amino acid sequences. AlphaFold employs advanced\ndeep learning techniques, prominently featuring reinforcement\nlearning, to enhance its predictive capabilities.The model\noperates on a feedback loop where it generates predictions\nabout protein structures and receives rewards based on the\n11\nFig. 6. MuZero Rate-Controller (MuZero-RC) optimizing the encoding process in video streaming.\naccuracy of these predictions compared to experimentally\ndetermined structures. This process allows AlphaFold to\niteratively refine its models, optimizing them to better reflect\nthe complexities of protein folding dynamics. The architecture\nof AlphaFold includes deep neural networks that analyze\nboth the sequential and spatial relationships between amino\nacids, enabling it to capture intricate patterns for protein\nconformation. By training on extensive datasets of known\nprotein structures, AlphaFold has achieved unprecedented\naccuracy, often rivaling experimental methods such as X-ray\ncrystallography and cryo-electron microscopy [25].\nAs shown form the previous models, how the employment\nof reinforcementl learning changed starting from making AI\nsystems which play atari and strategy-based games, reaching\nto help in human biology and create protein structures, the\nenployment of reinforcement learning in games still has a\nlong journey to be developed which helps in both real life and\ngaming. Google DeepMind is still working on other models\nwhich are able to be implemented in real life applications.\nThey also developed models which use the multi-agent models\nin games, like AlphaStar, which is a model to play StarCraft\nII; but still didn’t apply them in real life applications, which\nis a good future direction to be developed.\nCONCLUSION\nGames as an environment for Reinforcement learning, have\nproven to be very helpful as a sandbox. Their modular nature\nenables experimentation for different scenarios from the de-\nterministic board games to visually complex and endless atari\ngames. Google’s DeepMind utilized this in developing and\nenhancing their models starting with AlphaGo that required\nhuman gameplay as well as knowledge of the game rules.\nIncrementally, they started stripping down game specific data\nand generalizing the models. AlphaGoZero removed the need\nfor human gameplay and AlphaZero generalized the approach\nto multiple board games. Subsequently, MuZero removed any\nknowledge requirements of games and was able to achieve\nbreak-through results in tens of games surpassing all previ-\nous models. These advancements were translated to real-life\napplications seen in MuZero’s optimization of the YouTube\ncompression algorithm, which was already highly optimized\nusing traditional techniques. The well defined nature of the\nproblem helped in achieving this result. Also, AlphaFold used\nreinforcement learning in combination with supervised learn-\ning and biology insights to simulate protein structures. While\nthese uses are impressive, especially coming from models\nprimarily trained to play simple games, they are still limited in\nscope. There are many possible holdbacks mainly the training\ncost, scalability, and stochastic environments. These models\nare very expensive to train despite the limited action and\nstate spaces. This cost would only increase at more complex\nenvironments, taking us to the second issue: scalability. In\nmany real applications, the actions aren’t mutually exclusive.\nThis would make the MCTS exponentially more expensive\nand would further increase the training cost. Finally, while\nthese models have been tested in deterministic environments,\nstochastic scenarios might cause trouble for their training and\ninference.\nREFERENCES\n[1] N. Y. Georgios and T. Julian, Artificial Intelligence and Games. New\nYork: Springer, 2018.\n[2] N. Justesen, P. Bontrager, J. Togelius, S. Risi, (2019). Deep learning for\nvideo game playing. arXiv.\n[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D.\nWierstra, M. Riedmiller, (2013). Playing Atari with deep reinforcement\nlearning. arXiv.\n[4] A. Graves, G. Wayne, I. Danihelka, (2014). Neural Turing Machines.\narXiv.\n[5] C.J.C.H.Watkins, P. Dayan, Q-learning. Mach Learn 8, 279–292 (1992).\n[6] DeepMind, (2015, February 12), Deep reinforcement learning.\n[7] T. Schaul, J. Quan, I. Antonoglou, D. Silver, (2015). Prioritized Expe-\nrience Replay. arXiv.\n12\n[8] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, K. Kavukcuoglu, (2016). Asynchronous Methods for Deep\nReinforcement Learning. arXiv.\n[9] A. Kailash, P. D. Marc, B. Miles, and A. B. Anil, (2017). Deep Rein-\nforcement Learning: A Brief Survey. IEEE Signal Processing Magazine,\nvol. 34, pp. 26–38, 2017. arXiv.\n[10] D. Zhao, K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou,\nand C. Wang, “Review of deep reinforcement learning and discussions\non the development of computer Go,” Control Theory and Applications,\nvol. 33, no. 6, pp. 701–717, 2016 arXiv.\n[11] Z. Tang, K. Shao, D. Zhao, and Y. Zhu, “Recent progress of deep\nreinforcement learning: from AlphaGo to AlphaGo Zero,” Control\nTheory and Applications, vol. 34, no. 12, pp. 1529–1546, 2017.\n[12] K. Shao, Z. Tang, Y. Zhu, N. Li, D. Zhao, (2019). A survey of deep\nreinforcement learning in video games. arXiv.\n[13] L. Thorndike and D. Bruce, Animal Intelligence. Routledge, 2017.\n[14] R. S. Sutton and A. Barto, Reinforcement learning : an introduction.\nCambridge, Ma ; London: The Mit Press, 2018.\n[15] A. Kumar Shakya, G. Pillai, and S. Chakrabarty, “Reinforcement Learn-\ning Algorithms: A brief survey,” Expert Systems with Applications, vol.\n231, p. 120495, May 2023\n[16] Mnih, Volodymyr, et al. “Human-Level Control through Deep Reinforce-\nment Learning.” Nature, vol. 518, no. 7540, Feb. 2015, pp. 529–533.\n[17] D. Silver et al., “Mastering the game of Go with deep neural networks\nand tree search,” Nature, vol. 529, no. 7587, pp. 484–489, Jan. 2016,\ndoi: https://doi.org/10.1038/nature16961.\n[18] D. Silver et al., “Mastering the game of Go with deep neural networks\nand tree search,” Nature, vol. 529, no. 7587, pp. 484–489, Jan. 2016,\ndoi: https://doi.org/10.1038/nature16961.\n[19] Silver,\nDavid,\nJulian\nSchrittwieser,\nKaren\nSimonyan,\nIoannis\nAntonoglou,\nAja\nHuang,\nArthur\nGuez,\nThomas\nHubert,\net\nal.\n2017. “Mastering the Game of Go without Human Knowledge.” Nature\n550 (7676): 354–59. https://doi.org/10.1038/nature24270.\n[20] J. Schrittwieser et al., “Mastering Atari, go, chess and shogi by planning\nwith a learned model,” Nature, vol. 588, no. 7839, pp. 604–609, Dec.\n2020. doi:10.1038/s41586-020-03051-4\n[21] DeepMind, ”AlphaZero: Shedding New Light on Chess, Shogi, and Go,”\nDeepMind, 06-Dec-2018.\n[22] T.-R. Wu, H. Guei, P.-C. Peng, P.-W. Huang, T. H. Wei, C.-C. Shih,\nY.-J. Tsai, (2023). MiniZero: Comparative analysis of AlphaZero and\nMuZero on Go, Othello, and Atari games. arXiv.\n[23] K. Zhang, Z. Yang, T. Bas¸ar, (2021). Multi-agent reinforcement learn-\ning: A selective overview of theories and algorithms. arXiv preprint\narXiv:2103.04994\n[24] ”MuZero’s first step from research into the real world,” DeepMind, Feb.\n11, 2022.\n[25] Jumper, J et al. Highly accurate protein structure prediction with\nAlphaFold. Nature (2021)\n13\n",
  "categories": [
    "cs.AI",
    "cs.GT"
  ],
  "published": "2025-02-14",
  "updated": "2025-02-14"
}