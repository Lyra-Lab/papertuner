{
  "id": "http://arxiv.org/abs/2502.12886v1",
  "title": "Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?",
  "authors": [
    "Georg Rehm",
    "Annika Grützner-Zahn",
    "Fabio Barth"
  ],
  "abstract": "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way.",
  "text": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models\nAre Multilingual Language Models an Off-ramp for Under-resourced\nLanguages? Will we arrive at Digital Language Equality in Europe in 2030?\nGeorg Rehm\nDFKI GmbH, Germany\nHumboldt-Universität zu Berlin, Germany\ngeorg.rehm@dfki.de (corresponding)\nAnnika Grützner-Zahn\nDFKI GmbH, Germany\nFabio Barth\nDFKI GmbH, Germany\nAbstract\nLarge language models (LLMs) demonstrate\nunprecedented capabilities and define the state\nof the art for almost all natural language pro-\ncessing (NLP) tasks and also for essentially\nall Language Technology (LT) applications.\nLLMs can only be trained for languages for\nwhich a sufficient amount of pre-training data\nis available, effectively excluding many lan-\nguages that are typically characterised as under-\nresourced. However, there is both circumstan-\ntial and empirical evidence that multilingual\nLLMs, which have been trained using data sets\nthat cover multiple languages (including under-\nresourced ones), do exhibit strong capabilities\nfor some of these under-resourced languages.\nEventually, this approach may have the poten-\ntial to be a technological off-ramp for those\nunder-resourced languages for which “native”\nLLMs – and LLM-based technologies – can-\nnot be developed due to a lack of training data.\nThis paper, which concentrates on European\nlanguages, examines this idea, analyses the cur-\nrent situation in terms of technology support\nand summarises related work. The article con-\ncludes by focusing on the key open questions\nthat need to be answered for the approach to be\nput into practice in a systematic way.\n1\nIntroduction\nEspecially in today’s data-driven, machine learning\nand language model-based era of Language Tech-\nnologies (LTs), it is intuitively evident that some\nlanguages have better, more advanced, more so-\nphisticated technology support than others. This\nintuitive notion concerns not only the scope and\ndeployment of technologies in applications or de-\nvices but also the coverage and robustness of these\ntechnologies. The natural language processing liter-\nature typically distinguishes between high-resource\nand low-resource, i. e., under-resourced, languages.\nConcentrating on the languages of Europe and\nthe European Union (EU), we are faced with a\nmultilingual society with 24 official EU Mem-\nber State languages and more than 60 additional\nlanguages (regional and minority languages, co-\nofficial languages etc.) plus languages of immi-\ngrants, trade partners and also tourists. For decades,\nmost European languages have been considered\nunder-resourced, which is peculiar since all EU\nMember State languages are politically on equal\nfooting according to the EU Treaty. However, tech-\nnically they are clearly not – this fact has been\nofficially recognised by the European Union (Eu-\nropean Parliament, 2018) in a resolution that calls\nfor the development of technologies and resources\nfor all European languages, supported through a\nlarge-scale and long-term funding programme.\nConcurrently with the publication of the EP res-\nolution (European Parliament, 2018) and the sub-\nsequent EU project European Language Equality\n(ELE, see Section 2), the full potential and ground-\nbreaking capabilities of large language models\n(LLMs) have become clear to the natural language\nprocessing (NLP) and Artificial Intelligence (AI)\nresearch community and also to the public at large\nwith a number of products such as, most notably,\nChatGPT, released in November 2022. Through\nthe development of multilingual LLMs, which have\nbeen trained on data sets covering not only one but\nmultiple languages, there is growing evidence that\nsuch multilingual LLMs exhibit strong capabilities\nfor some or many of the under-resourced languages\nthey have been trained on even though only very\nlittle training data for these languages was actually\nincluded in the pre-training data set. If this does\nin fact hold for many or all under-resourced lan-\nguages, this technical approach could become a\ntechnical off-ramp for these under-resourced lan-\nguages for which our field is unable to develop\n“native” stand-alone technologies simply because\nthere is too little pre-training data available. This\noff-ramp would not only avoid digital language ex-\ntinction and digital language death, it would also\n1\narXiv:2502.12886v1  [cs.CL]  18 Feb 2025\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nbring us closer to digital language equality in Eu-\nrope, maybe even in time by 2030, as initially de-\nmanded by the ELE project.\nThis position paper attempts to examine the ques-\ntion if multilingual LLMs can be considered an off-\nramp for under-resourced languages with a special\nemphasis on European languages. First, Section 2\nintroduces the concept of digital language equality\nincluding the current state of technology support\nof Europe’s languages. Section 3 concentrates on\nthe available data including a look at the current\nsituation and recent developments how to improve\nit. Section 4 introduces the idea of making use of\nmultilingual LLMs, trained on both high as well\nas low-resource languages, as an off-ramp for low-\nresource languages. Section 5 concludes the article.\n2\nDigital Language Inequality in Europe\nIn Europe’s multilingual setup, all 24 official EU\nlanguages are granted equal status by the EU Char-\nter and the Treaty on EU. The EU is also home\nto over 60 regional and minority languages which\nhave been protected and promoted under the Euro-\npean Charter for Regional or Minority Languages\n(ECRML) treaty since 1992, in addition to various\nsign languages and the languages of immigrants as\nwell as trade partners. Artificial Intelligence, Nat-\nural Language Processing, Natural Language Un-\nderstanding, Language Technologies, and Speech\nTechnologies have the potential to enable multilin-\ngualism – and the multilingual European informa-\ntion society – technologically but, as the META-\nNET White Paper Series Europe’s Languages in\nthe Digital Age (Rehm and Uszkoreit, 2012) found\nalready in 2012, our languages suffer from an ex-\ntreme imbalance in terms of technological support:\nEnglish is very well supported through technolo-\ngies, tools, data sets and corpora, but languages\nsuch as, among others, Maltese, Estonian and Ice-\nlandic have hardly any support at all. In fact, the\n2012 study assessed “at least 21 European lan-\nguages to be in danger of digital extinction”. If,\nas mentioned above, all European languages are\nsupposed to be on an equal footing in general, tech-\nnologically, they clearly are not (Kornai, 2013).\nAfter the META-NET findings and several\nfollow-up projects, studies and recommendations\n(Rehm and Uszkoreit, 2013; STOA, 2017, e. g.,),\nthe joint CULT/ITRE report Language Equality\nin the Digital Age (European Parliament, 2018)\nwas eventually passed by the European Parliament\nwith an overwhelming majority in September 2018.\nIt concerns the improvement of the institutional\nframework for LT policies at the EU level, EU re-\nsearch and education policies to improve the future\nof LTs in Europe, and the extension of the benefits\nof LTs for both private companies and public bod-\nies. This EP resolution also recognises that there\nis an imbalance in terms of technology support of\nEurope’s languages, that there has been a substan-\ntial amount of progress in research and technology\ndevelopment and that a large-scale, long-term fund-\ning programme should be established to ensure full\ntechnology support for all of Europe’s languages.\nThe goal is to enable multilingualism technologi-\ncally since “the EU and its institutions have a duty\nto enhance, promote and uphold linguistic diversity\nin Europe” (European Parliament, 2018).\nWhile the resolution was an important milestone,\nthere has been no concrete follow-up action along\nthe lines laid out in the resolution, i. e., to set\nup “a large-scale, long-term coordinated funding\nprogramme for research, development and inno-\nvation in the field of language technologies, at\nEuropean, national and regional levels, tailored\nspecifically to Europe’s needs and demands” (Euro-\npean Parliament, 2018). In the meantime, however,\nmany highly influential breakthroughs in the area\nof language-centric AI have been achieved, mostly\nby large enterprises in the US and Asia, especially\napproaches and technologies concerning large lan-\nguage models (LLMs).\nThe EU project European Language Equality\n(ELE), which ran from 2021 until 2023, set out to\nanalyse the current technology support of Europe’s\nlanguages ten years after the META-NET study\n(see Rehm and Way, 2023a, for a comprehensive\noverview of this project’s results). It defined digital\nlanguage equality (DLE) as the “state of affairs in\nwhich all languages have the technological support\nand situational context necessary for them to con-\ntinue to exist and to prosper as living languages\nin the digital age.” (Gaspari et al., 2023, p. 43).\nThe results of the ELE project demonstrate that we\nare still very far away from this ideal state: except\nfor English and, to a certain extent, French, Span-\nish and German, all European languages must be\nconsidered significantly or even massively under-\nresourced when comparing their individual scores\non the Digital Language Equality Metric (Gaspari\net al., 2022; Grützner-Zahn and Rehm, 2022). In\nother words, the findings from the META-NET\nstudy and the EP resolution have still been valid\n2\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nin 2023 and they continue to be valid today. As a\ndirect result of these observations, the ELE project\nprepared strategic recommendations (Rehm and\nWay, 2023b), presented to the European Union,\nthe full implementation of which would improve\nthe situation so that many of the currently under-\nresourced languages would eventually benefit from\nthe development of novel and more language tech-\nnologies as well as from making available novel\nand more data sets.1\nThe DLE Metric is “a measure that reflects the\ndigital readiness of a language and its contribu-\ntion to the state of technology-enabled multilin-\ngualism, tracking its progress towards the goal of\nDLE.” (Gaspari et al., 2023, p. 43), see Gaspari\net al. (2022) and Grützner-Zahn and Rehm (2022)\nfor more details. One implementation of the DLE\nMetric is available in the European Language Grid\n(ELG) platform (Rehm, 2023) in the form of a dash-\nboard.2 Figure 1 shows the current state (as of mid\nFebruary 2025) of technology support of Europe’s\nlanguages according to the DLE dashboard; the em-\npirical basis is the set of approx. 18,000 language\nresources and language technologies available in\nELG. The figure shows that English is the best\nsupported language by far, followed by German,\nSpanish and French. The long tail essentially starts\nwith Italian, Finnish and Portuguese, followed by\nPolish, Dutch and Swedish.\nWhile there are several very promising devel-\nopments in Europe that will all contribute to im-\nproved digital language equality (see the following\nsections), it is also very much evident that a lack of\navailable data for Europe’s languages is one of the\nmain current bottlenecks that we are facing.\n3\nAvailability of European Language\nData: Current State and Future\nDevelopments\nRegarding this situation, the most important, rel-\nevant and promising development both in the Eu-\nropean Union and also across various European\ncountries relates to data spaces. The EU has been\nimplementing its Data Strategy since 2019.3 A\n1Despite promising initial discussions with the European\nCommission and the European Parliament in 2023, at the\ncurrent point it does not seem particularly likely that the ELE\nProgramme will be financed by the European Union.\n2https://live.european-language-grid.eu/\ncatalogue/dashboard\n3https://commission.europa.eu/\nstrategy-and-policy/priorities-2019-2024/\neurope-fit-digital-age/european-data-strategy_en\nkey component of the industrially-oriented single\nmarket for data, in which “data can flow within\nthe EU and across sectors, for the benefit of all”,\n“European rules [...] are fully respected” and in\nwhich “the rules for access and use of data are fair,\npractical and clear”, is the vision of establishing\n“interoperable data spaces” for “pooling European\ndata in key sectors”. The EC (2022) describes how\nthese data spaces are supposed to be set up and\noperated including relevant legislation (also see\nNagel and Lycklama, 2021). This document also\nlists a number of ‘official’ EU data spaces target-\ning sectors such as manufacturing, mobility, health,\nfinance, energy, agriculture and skills. The Com-\nmon European Language Data Space (LDS, Rehm\net al., 2024), funded through the Digital Europe\nProgramme (DEP),4 is one of these official EU\ndata spaces.5\nThe LDS project develops a decentralised dig-\nital infrastructure for the sharing of any kind of\nlanguage data (written text, spoken language, mul-\ntimodal data etc.).\nDue to the importance of\nLLMs for industrial and academic applications\nand their dependence on vast amounts of language\ndata for training models, especially when consider-\ning the increasing push of the EU towards digital\nsovereignty, the development of the LDS has re-\ncently become increasingly relevant. The LDS is\none crucial part of a bigger initiative towards more\nEuropean independence and increased European\nparticipation in the global LLM landscape (includ-\ning research, development, innovation, application,\ndeployment and monetisation).\nThe main principle in the concept of data spaces\nis that of data sovereignty, i. e., the ability of a per-\nson, natural or legal, to exclusively decide, in a\nsovereign way, on the usage of their own data as\nan economic asset. As a result of this principle, the\nLDS gives owners/providers full control regarding\naccess to their data, also enabling data transactions\nincluding monetary transactions. The LDS will\nsupport tracking of data provenance and lineage\nand it will enforce access and usage policies, for-\nmally codified as contracts established in a trust-\nful environment. The typical operation technically\nsupported in LDS is that of exchange of metadata\nabout data assets (i. e., data, data products and data\nservices) and exchange or transfer of such assets\nbetween trusted participants. An asset can be any\n4https://digital-strategy.ec.europa.eu/en/\nactivities/digital-programme\n5https://language-data-space.ec.europa.eu\n3\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nFigure 1: The technological scores of the Digital Language Equality Metric (as of February 2025): the numbers\nshown in the bar chart strongly correlate with the data available for each language\ntype of language-related data set (corpus, lexicon\netc.) or a language processing service. The LDS\nwill eventually be a pan-European marketplace al-\nlowing users, providers and consumers to perform,\nmanage and monitor their commercial transactions\nand exchange data in full respect of the contractual\nagreements made between the parties.\nAll participants, data providers and data con-\nsumers, have to install the LDS connector software\nthrough which they can participate in this market-\nplace. It enables the documentation of language\ndata, cataloging of assets by the connector’s owner,\ncontract-based agreement for the exchange of data,\ntransfer or processing of data and logging of all\ntransactions. The overall marketplace can be con-\nceptualised as the aggregation of all its connectors,\nconnected in a peer-to-peer fashion. Each connec-\ntor is equipped with a local catalogue in which this\nprovider’s offerings are published and available for\nquerying or crawling by other connectors. In addi-\ntion, a central catalogue will give an overview of\nall offerings publicly available.\nThe LDS project has recently concluded its sec-\nond year and it has also recently made available\nVersion 1.0.0 beta of the LDS connector software,\nwhich is now being tested. The LDS user group,\nwhich was started in 2024, already has representa-\ntives of more than 100 organisations, all of which\nare interested in sharing or using language data.\nWith the LDS infrastructure, Europe will have a\nrobust and sustainable mechanism for sharing any\nkind of language data in a systematic and also trust-\nworthy way. The LDS will have a very strong\npositive impact on the overall language data situa-\ntion in Europe including the availability of data for\nunder-resourced languages and making available\nlanguage data from various industries and sectors.\nHowever, it will take a few years for this initia-\ntive to become so active, vibrant and successful\nthat thousands of participants from all over Europe,\nrepresenting all European languages (including mi-\nnority, co-official, regional and other languages),\nare comfortably represented in this marketplace,\nwhich is why we need to examine short-term alter-\nnatives for developing technologies that all Euro-\npean languages can benefit from so that they do not\nexperience digital language death (Kornai, 2013).\n4\nMultilingual LLMs as an Off-ramp for\nUnder-resourced Languages\nMultilingual LLMs have demonstrated remarkable\ncross-linguistic generalisation and zero-shot trans-\nfer learning capabilities, significantly advancing\nNLP and LT applications.\nIt must be stressed,\nthough, that most multilingual LLMs are trained\non relatively small data sets for most languages, ex-\ncept for English. Still, a number of these multilin-\ngual LLMs perform well on benchmarks assessing\nthe LLM’s capabilities on these under-resourced\nlanguages and they also outperform monolingual\nLLMs trained on these under-resourced models\nonly due to cross-lingual transfer (Chang et al.,\n2024).\nLeaderboards, like the European LLM\nLeaderboard,6 show for instance that LLama (Tou-\n6https://huggingface.co/spaces/openGPT-X/\neuropean-llm-leaderboard\n4\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nvron et al., 2023; Grattafiori et al., 2024), while\nonly having a small amount of training data on\nlow-resource languages like Romanian, Portuguese,\nSlovenian or Slovakian, reaches high accuracy\nscores for evaluation tasks on language understand-\ning. It primarily focuses on eight languages but has\ndemonstrated strong zero-shot and few-shot capa-\nbilities across a broader range (Thellmann et al.,\n2024). Another relevant case is the model Gemma\n2 (Team Gemma et al., 2024) which is stated to be\ntrained mainly on English data, yet it also achieves\nsurprisingly high accuracy scores for a number of\nEuropean low-resource languages, like Slovenian,\nSlovakian, Latvian and Romanian.\nSeveral European multilingual LLMs have been\ndeveloped especially with the idea to support Eu-\nropean languages, including under-resourced ones,\nin a more systematic way. The models that exhibit\nstrong transfer capabilities to low-resource lan-\nguages are BLOOM (BigScience Workshop et al.,\n2023), Mistral Nemo7, EuroLLM (Martins et al.,\n2024), Teuken-7B (Ali et al., 2024) and Salaman-\ndra (Gonzalez-Agirre et al., 2025). BLOOM was\ntrained on data covering 46 natural languages, Mis-\ntral Nemo on nine languages, Salamandra and Eu-\nroLLM on the official EU-24 plus eleven additional\nlanguages, and Teuken-7B (Ali et al., 2024) on\nthe EU-24 languages. All of these models demon-\nstrate consistent performance across all included\nlanguages (Thellmann et al., 2024). The models\nalso show that LLMs trained on more equally dis-\ntributed multilingual training data sets are able to\ncompete with the models trained on predominantly\nEnglish data and in some cases even outperform\nthem (Thellmann et al., 2024), i. e., multilingual\npre-training can yield strong results even for lan-\nguages with limited direct training data.\nThese promising and astonishing results have led\nto our idea of using pre-trained multilingual LLMs\nas off-ramps for under-resourced languages. They\nshow that it may be possible to develop LLMs with\nenough linguistic capabilities on low-resource lan-\nguages (see Figure 1), so that these languages can\nbe actually used in the digital sphere by their na-\ntive speakers and so that the corresponding LLMs\ncan be used to develop and provide technologies\nfor these language communities. For this to be-\ncome a realistic vision, several factors and current\nlimitations need to be examined more closely.\n7https://huggingface.co/mistralai/\nMistral-Nemo-Instruct-2407,\nhttps://mistral.\nai/en/news/mistral-nemo\nFirst, the availability and quality of data impacts\nsignificantly which languages can be covered by a\npre-trained model. Currently, there are a handful\nof sufficiently large multilingual data sets for pre-\ntraining. Data sets like the ROOTS corpus (Lau-\nrençon et al., 2023), MADLAD-400 (Kudugunta\net al., 2023), CulturaX (Nguyen et al., 2024), mC4\n(Xue et al., 2021), HPLT (de Gibert et al., 2024)\nand various Wikipedia dumps are commonly used.\nMADLAD-400 covers the most languages (419)\nwhile pointing to aspects regarding data quality for\nunder-resourced languages. In addition, except for\nROOTS, all data sets are based on CommonCrawl\ndumps. Even ROOTS, although trying to gather\ndata from other sources, had to complement their\ndata with a subset of OSCAR, which is also based\non CommonCrawl. These limitations in usable\ndata sources and the assumption of “the more, the\nbetter” lead to using as much data as possible for\nunder-resourced languages without considering the\nquestion how much under-resourced language data\nis actually needed.\nThere have been initial approaches to limit the\ntraining of LLMs by restricting the size of vocabu-\nlary and data following the idea of language learn-\ning in human development with promising results\n(Warstadt et al., 2023; Georges Gabriel Charpentier\nand Samuel, 2023). Moreover, more systematic\ntesting found the relevance of the size of the train-\ning data set for under-resourced languages for mul-\ntilingual LLMs up to a certain point depending on\nthe capacity of the model. Other factors influencing\ncross-linguistic transfer are linguistic similarity and\nscript type (Chang et al., 2024; Ahuja et al., 2023;\nBagheri Nezhad and Agrawal, 2024). As these\nfindings are only indicative and taking into account\nthe complexity of the topic, a careful, systematic\ntesting and analysis of the influence of LLM train-\ning data composition with a special emphasis on\nanalysing the resulting capabilities regarding under-\nresourced languages is still missing. This includes\naspects like the number of languages per model or\nproportionally per size, the impact of typological\nsimilarity or diversity of the languages, which lan-\nguage serves as the high-resource language from\nwhich the model transfers the linguistic capabilities,\nto name just a few.\nIn addition to the training data, the training pro-\ncess itself can be adapted to increase the efficiency\nof the learning process. Bootstrapping word transla-\ntion pairs from monolingual corpora (Hangya et al.,\n2022) can improve intrinsic cross-lingual word rep-\n5\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nresentation quality and downstream task perfor-\nmance in under-resourced languages of multilin-\ngual LLMs. In-context learning methods (Cahyaw-\nijaya et al., 2024), layer-wise data sampling (Pan\net al., 2024) or increased architecture flexibility\n(Georges Gabriel Charpentier and Samuel, 2023)\nare additional ways for improving the performance\nwithout the need for more training data. Combin-\ning these training-efficient methods with the ques-\ntions about necessary training data quantity and\ndata set composition for under-resourced languages\ncan yield effective answers for using multilingual\nLLMs as off-ramps for under-resourced languages.\nFinally, benchmarking is crucial in evaluating\nmultilingual language models, but existing eval-\nuation frameworks are often very much English-\ncentric and culturally biased toward western or US\nculture (Tao et al., 2024). The widely used Glob-\nalMMLU benchmark (Singh et al., 2024) was cre-\nated by automatically translating the established\nMMLU (Hendrycks et al., 2021) benchmark into\n42 languages and manually annotating the data set\nto verify and culturally de-bias the benchmark. It\nwas annotated through a community effort to iden-\ntify culturally-sensitive samples. However, Singh\net al. (2024) showed that the languages were not\nannotated in a balanced way. Also, determining\nif a data set is truly culturally inclusive is critical,\nas simple translations can create a false sense of\ninclusivity without addressing deeper cultural bi-\nases. Especially in terms of following up on the\noff-ramp idea, the goal should be to incorporate di-\nverse cultural knowledge to ensure fairness in mul-\ntilingual NLP/AI evaluation (Singh et al., 2024). A\ncounter-example is the INCLUDE (Romanou et al.,\n2024) benchmark, developed to assess multilingual\nlanguage understanding with a focus on regional\nknowledge, ensuring a more nuanced evaluation of\nmultilingual LLM performance and incorporating\ncultural knowledge.\n5\nSummary and Conclusions\nThe Strategic Agenda for Digital Language Equal-\nity (Rehm and Way, 2023b), developed by the Eu-\nropean project ELE, outlines a number of steps,\ninstruments and mechanisms in terms of how to\nachieve the overall goal of arriving at digital lan-\nguage equality in Europe. As the full implemen-\ntation of the complex and expensive ELE pro-\ngramme currently seems unlikely, we developed\nthe rather pragmatic and short-term term idea to\n“exploit” multilingual LLMs as off-ramps for under-\nresourced European languages. This way, speakers\nof these languages would be able to benefit from so-\nphisticated, state of the art language technologies,\nenabling them to participate in the digital sphere in\ntheir mother tongues.\nWhile there are still several open gaps in research\n(see Section 4 and also below), the overall situa-\ntion in Europe can be perceived as promising. The\nEU and also the Member States provide bigger\nbudgets for AI and LLM research and deployment\nprojects and also for overarching, pan-European ini-\ntiatives such as the AI Factories and the Alliance for\nLanguage Technologies EDIC (ALT-EDIC), they\nalso concentrate on the development of data spaces\nincluding, crucially, the Common European Lan-\nguage Data Space. In the next five to seven years\nwe can expect more and more participants in the\nLDS who will actively share, sell and purchase\nlarger amounts of written or spoken language data,\nespecially for developing LLMs. In this regard,\nEurope needs to make an effort to make available\nespecially those language data sets that do exist\nbut that are currently simply not available for LLM\ndevelopment purposes. Obivous candidates are\nnews data, as produced on a daily basis by newspa-\npers and online portals, and also audio-visual data,\nas produced, also on a daily basis, by public and\nprivate broadcasters throughout Europe, covering\nessentially all relevant European languages. It will\nbe a crucial challenge to “unlock” these classes of\npotential LLM training data for developing mul-\ntilingual LLMs that are tailor-made for Europe’s\nlanguages (see Rehm et al., 2023, for a suggestion\nhow to put this idea into practice).\nBefore the technological shortcut can be system-\natically used, a number of open research questions\nneed to be addressed: How much under-resourced\nlanguage data do we need for the multilingual LLM\nto exhibit strong capabilities for this language?\nAre there specific constraints regarding the high-\nresource languages that also need to be include in\nthe multilingual LLM? Should these be, for exam-\nple, typologically similar to the under-resourced\nlanguage(s) or is using English sufficient? Are\nthere alternatives to using English as the predomi-\nnant base language of a multilingual pre-training\ndata set? How many languages should be included\nin the pre-training data in total? Does a diverse set\nof languages improve multilingual performance?\n6\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nAcknowledgments\nThe work presented in this paper has re-\nceived funding from the German Federal Min-\nistry for Economic Affairs and Climate Action\n(BMWK) through the project OpenGPT-X (project\nno. 68GX21007D). The Common European Lan-\nguage Data Space is funded by the European Union\nthrough the contract LC-01936389.\nReferences\nKabir Ahuja, Harshita Diddee, Rishav Hada, Milli-\ncent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\nshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed\nAhmed, Kalika Bali, and Sunayana Sitaram. 2023.\nMEGA: Multilingual evaluation of generative AI.\nIn Proceedings of the 2023 Conference on Empir-\nical Methods in Natural Language Processing, pages\n4232–4267, Singapore. Association for Computa-\ntional Linguistics.\nMehdi Ali, Michael Fromm, Klaudia Thellmann, Jan\nEbert, Alexander Arno Weber, Richard Rutmann,\nCharvi Jain, Max Lübbering, Daniel Steinigen,\nJohannes Leveling, Katrin Klug, Jasper Schulze\nBuschhoff, Lena Jurkschat, Hammam Abdelwa-\nhab, Benny Jörg Stein, Karl-Heinz Sylla, Pavel\nDenisov, Nicolo’ Brandizzi, Qasid Saleem, Anirban\nBhowmick, Lennard Helmer, Chelsea John, Pedro Or-\ntiz Suarez, Malte Ostendorff, Alex Jude, Lalith Man-\njunath, Samuel Weinbach, Carolin Penke, Oleg Fila-\ntov, Shima Asaadi, Fabio Barth, Rafet Sifa, Fabian\nKüch, Andreas Herten, René Jäkel, Georg Rehm, Ste-\nfan Kesselheim, Joachim Köhler, and Nicolas Flores-\nHerr. 2024. Teuken-7b-base & teuken-7b-instruct:\nTowards european llms. Preprint, arXiv:2410.03730.\nSina Bagheri Nezhad and Ameeta Agrawal. 2024. What\ndrives performance in multilingual language mod-\nels? In Proceedings of the Eleventh Workshop on\nNLP for Similar Languages, Varieties, and Dialects\n(VarDial 2024), pages 16–27, Mexico City, Mexico.\nAssociation for Computational Linguistics.\nBigScience Workshop, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luc-\ncioni, François Yvon, Matthias Gallé, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Benoît Sagot, Niklas Muennighoff, Albert Vil-\nlanova del Moral, Olatunji Ruwase, Rachel Bawden,\nStas Bekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-\ndro Ortiz Suarez, Victor Sanh, Hugo Laurençon,\nYacine Jernite, Julien Launay, Margaret Mitchell,\nColin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, Dragomir\nRadev, Eduardo González Ponferrada, Efrat Lev-\nkovizh, Ethan Kim, Eyal Bar Natan, Francesco De\nToni, Gérard Dupont, Germán Kruszewski, Giada\nPistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,\nIan Yu, Idris Abdulmumin, Isaac Johnson, Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse\nDodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,\nJoseph Tobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra,\nLeon Weber, Long Phan, Loubna Ben allal, Lu-\ndovic Tanguy, Manan Dey, Manuel Romero Muñoz,\nMaraim Masoud, María Grandury, Mario Šaško,\nMax Huang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Moham-\nmad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,\nNora Kassner, Nurulaqilla Khamis, Olivier Nguyen,\nOmar Espejel, Ona de Gibert, Paulo Villegas, Pe-\nter Henderson, Pierre Colombo, Priscilla Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani,\nRoberto Luis López, Rui Ribeiro, Salomey Osei,\nSampo Pyysalo, Sebastian Nagel, Shamik Bose,\nShamsuddeen Hassan Muhammad, Shanya Sharma,\nShayne Longpre, Somaieh Nikpoor, Stanislav Silber-\nberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-\nrent, Timo Schick, Tristan Thrush, Valentin Danchev,\nVassilina Nikoulina, Veronika Laippala, Violette\nLepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-\nlat, Arun Raja, Benjamin Heinzerling, Chenglei Si,\nDavut Emre Ta¸sar, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y. Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chaffin, Arnaud Stiegler, Debajy-\noti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-\nXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,\nHadar Tojarieh, Adam Roberts, Hyung Won Chung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre François\nLavallée, Rémi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, Stéphane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aurélie Névéol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina Voloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\n7\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zdenˇek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Ammar\nKhan, Amy Faranak, Ana Santos, Anthony Hevia,\nAntigona Unldreaj, Arash Aghagol, Arezoo Abdol-\nlahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Ajibade, Bharat Saxena, Car-\nlos Muñoz Ferrandis, Daniel McDuff, Danish Con-\ntractor, David Lansky, Davis David, Douwe Kiela,\nDuong A. Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel León\nPeriñán, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, Maria A Castillo, Mari-\nanna Nezhurina, Mario Sänger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nThéo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-\nnis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2023.\nBloom: A 176b-parameter\nopen-access multilingual language model. Preprint,\narXiv:2211.05100.\nSamuel Cahyawijaya, Holy Lovenia, and Pascale Fung.\n2024. LLMs are few-shot in-context low-resource\nlanguage learners. In Proceedings of the 2024 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n405–433, Mexico City, Mexico. Association for Com-\nputational Linguistics.\nTyler A. Chang, Catherine Arnett, Zhuowen Tu, and\nBen Bergen. 2024. When is multilinguality a curse?\nlanguage modeling for 250 high- and low-resource\nlanguages. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4074–4096, Miami, Florida, USA. Association\nfor Computational Linguistics.\nOna de Gibert, Graeme Nail, Nikolay Arefyev, Marta\nBañón, Jelmer van der Linde, Shaoxiong Ji, Jaume\nZaragoza-Bernabeu, Mikko Aulamo, Gema Ramírez-\nSánchez, Andrey Kutuzov, Sampo Pyysalo, Stephan\nOepen, and Jörg Tiedemann. 2024. A new massive\nmultilingual dataset for high-performance language\ntechnologies. In Proceedings of the 2024 Joint In-\nternational Conference on Computational Linguis-\ntics, Language Resources and Evaluation (LREC-\nCOLING 2024), pages 1116–1128, Torino, Italia.\nELRA and ICCL.\nEC. 2022. Commission staff working document on\ncommon european data spaces.\nEuropean Parliament. 2018. Language Equality in the\nDigital Age. European Parliament resolution of 11\nSeptember 2018 on Language Equality in the Digital\nAge (2018/2028(INI).\nFederico Gaspari, Owen Gallagher, Georg Rehm, Maria\nGiagkou, Stelios Piperidis, Jane Dunne, and Andy\nWay. 2022. Introducing the Digital Language Equal-\nity Metric: Technological Factors. In Proceedings\nof the Workshop Towards Digital Language Equality\n(TDLE 2022; co-located with LREC 2022), pages\n1–12, Marseille, France. 20 June 2022.\nFederico Gaspari, Annika Grützner-Zahn, Georg Rehm,\nOwen Gallagher, Maria Giagkou, Stelios Piperidis,\nand Andy Way. 2023. Digital Language Equality:\nDefinition, Metric, Dashboard. In Georg Rehm and\nAndy Way, editors, European Language Equality: A\nStrategic Agenda for Digital Language Equality, Cog-\nnitive Technologies, pages 39–73. Springer, Cham,\nSwitzerland.\nLucas Georges Gabriel Charpentier and David Samuel.\n2023. Not all layers are equally as important: Every\nLayer Counts BERT. In Proceedings of the BabyLM\nChallenge at the 27th Conference on Computational\nNatural Language Learning, pages 210–224, Singa-\npore. Association for Computational Linguistics.\nAitor Gonzalez-Agirre, Marc Pàmies, Joan Llop,\nIrene Baucells, Severino Da Dalt, Daniel Tamayo,\nJosé Javier Saiz, Ferran Espuña, Jaume Prats, Javier\nAula-Blasco, Mario Mina, Iñigo Pikabea, Adrián\nRubio, Alexander Shvets, Anna Sallés, Iñaki La-\ncunza, Jorge Palomar, Júlia Falcão, Lucía Tormo,\nLuis Vasquez-Reina, Montserrat Marimon, Oriol\nPareras, Valle Ruiz-Fernández, and Marta Ville-\ngas. 2025. Salamandra technical report. Preprint,\narXiv:2502.08489.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\n8\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, Arun Rao, Aston Zhang, Aurelien Ro-\ndriguez, Austen Gregerson, Ava Spataru, Baptiste\nRoziere, Bethany Biron, Binh Tang, Bobbie Chern,\nCharlotte Caucheteux, Chaya Nayak, Chloe Bi,\nChris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDanny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,\nElina Lobanova, Emily Dinan, Eric Michael Smith,\nFilip Radenovic, Francisco Guzmán, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis An-\nderson, Govind Thattai, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Is-\nhan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,\nJaewon Lee, Jan Geffert, Jana Vranes, Jason Park,\nJay Mahadeokar, Jeet Shah, Jelmer van der Linde,\nJennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,\nJiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Jun-\nteng Jia, Kalyan Vasuden Alwala, Karthik Prasad,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth\nHeafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal\nLakhotia, Lauren Rantala-Yeary, Laurens van der\nMaaten, Lawrence Chen, Liang Tan, Liz Jenkins,\nLouis Martin, Lovish Madaan, Lubo Malo, Lukas\nBlecher, Lukas Landzaat, Luke de Oliveira, Madeline\nMuzzi, Mahesh Pasupuleti, Mannat Singh, Manohar\nPaluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kam-\nbadur, Mike Lewis, Min Si, Mitesh Kumar Singh,\nMona Hassan, Naman Goyal, Narjes Torabi, Niko-\nlay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nNing Zhang, Olivier Duchenne, Onur Çelebi, Patrick\nAlrassy, Pengchuan Zhang, Pengwei Li, Petar Va-\nsic, Peter Weng, Prajjwal Bhargava, Pratik Dubal,\nPraveen Krishnan, Punit Singh Koura, Puxin Xu,\nQing He, Qingxiao Dong, Ragavan Srinivasan, Raj\nGanapathy, Ramon Calderer, Ricardo Silveira Cabral,\nRobert Stojnic, Roberta Raileanu, Rohan Maheswari,\nRohit Girdhar, Rohit Patel, Romain Sauvestre, Ron-\nnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sa-\nhana Chennabasappa, Sanjay Singh, Sean Bell, Seo-\nhyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sha-\nran Narang, Sharath Raparthy, Sheng Shen, Shengye\nWan, Shruti Bhosale, Shun Zhang, Simon Van-\ndenhende, Soumya Batra, Spencer Whitman, Sten\nSootla, Stephane Collot, Suchin Gururangan, Syd-\nney Borodinsky, Tamar Herman, Tara Fowler, Tarek\nSheasha, Thomas Georgiou, Thomas Scialom, Tobias\nSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal\nKarn, Vedanuj Goswami, Vibhor Gupta, Vignesh\nRamanathan, Viktor Kerkez, Vincent Gonguet, Vir-\nginie Do, Vish Vogeti, Vítor Albiero, Vladan Petro-\nvic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-\nney Meers, Xavier Martinet, Xiaodong Wang, Xi-\naofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xin-\nfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen,\nYiwen Song, Yuchen Zhang, Yue Li, Yuning Mao,\nZacharie Delpierre Coudert, Zheng Yan, Zhengxing\nChen, Zoe Papakipos, Aaditya Singh, Aayushi Sri-\nvastava, Abha Jain, Adam Kelsey, Adam Shajnfeld,\nAdithya Gangidi, Adolfo Victoria, Ahuva Goldstand,\nAjay Menon, Ajay Sharma, Alex Boesenberg, Alexei\nBaevski, Allie Feinstein, Amanda Kallet, Amit San-\ngani, Amos Teo, Anam Yunus, Andrei Lupu, An-\ndres Alvarado, Andrew Caples, Andrew Gu, Andrew\nHo, Andrew Poulton, Andrew Ryan, Ankit Ramchan-\ndani, Annie Dong, Annie Franco, Anuj Goyal, Apara-\njita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yaz-\ndan, Beau James, Ben Maurer, Benjamin Leonhardi,\nBernie Huang, Beth Loyd, Beto De Paola, Bhargavi\nParanjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han-\ncock, Bram Wasti, Brandon Spence, Brani Stojkovic,\nBrian Gamido, Britt Montalvo, Carl Parker, Carly\nBurton, Catalina Mejia, Ce Liu, Changhan Wang,\nChangkyu Kim, Chao Zhou, Chester Hu, Ching-\nHsiang Chu, Chris Cai, Chris Tindal, Christoph Fe-\nichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty,\nDaniel Kreymer, Daniel Li, David Adkins, David\nXu, Davide Testuggine, Delia David, Devi Parikh,\nDiana Liskovich, Didem Foss, Dingkang Wang, Duc\nLe, Dustin Holland, Edward Dowling, Eissa Jamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn,\nEmily Wood, Eric-Tuan Le, Erik Brinkman, Este-\nban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,\nFelix Kreuk, Feng Tian, Filippos Kokkinos, Firat\nOzgenel, Francesco Caggioni, Frank Kanayet, Frank\nSeide, Gabriela Medina Florez, Gabriella Schwarz,\nGada Badeer, Georgia Swee, Gil Halpern, Grant\nHerman, Grigory Sizov, Guangyi, Zhang, Guna\nLakshminarayanan, Hakan Inan, Hamid Shojanaz-\neri, Han Zou, Hannah Wang, Hanwen Zha, Haroun\nHabeeb, Harrison Rudolph, Helen Suk, Henry As-\npegren, Hunter Goldman, Hongyuan Zhan, Ibrahim\nDamlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis,\nIrina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher,\nJean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-\nnifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy\nTeboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan Mc-\nPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang,\nKai Wu, Kam Hou U, Karan Saxena, Kartikay Khan-\ndelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Ki-\nran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A,\nLeandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrst-\nedt, Madian Khabsa, Manav Avalani, Manish Bhatt,\nMartynas Mankus, Matan Hasson, Matthew Lennie,\nMatthias Reso, Maxim Groshev, Maxim Naumov,\nMaya Lathi, Meghan Keneally, Miao Liu, Michael L.\nSeltzer, Michal Valko, Michelle Restrepo, Mihir Pa-\ntel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark,\nMike Macey, Mike Wang, Miquel Jubert Hermoso,\n9\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nMo Metanat, Mohammad Rastegari, Munish Bansal,\nNandhini Santhanam, Natascha Parks, Natasha\nWhite, Navyata Bawa, Nayan Singhal, Nick Egebo,\nNicolas Usunier, Nikhil Mehta, Nikolay Pavlovich\nLaptev, Ning Dong, Norman Cheng, Oleg Chernoguz,\nOlivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin\nKent, Parth Parekh, Paul Saab, Pavan Balaji, Pe-\ndro Rittner, Philip Bontrager, Pierre Roux, Piotr\nDollar, Polina Zvyagina, Prashant Ratanchandani,\nPritish Yuvraj, Qian Liang, Rachad Alao, Rachel\nRodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky\nWang, Russ Howes, Ruty Rinott, Sachin Mehta,\nSachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara\nChugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov,\nSatadru Pan, Saurabh Mahajan, Saurabh Verma,\nSeiji Yamamoto, Sharadh Ramaswamy, Shaun Lind-\nsay, Shaun Lindsay, Sheng Feng, Shenghao Lin,\nShengxin Cindy Zha, Shishir Patil, Shiva Shankar,\nShuqiang Zhang, Shuqiang Zhang, Sinong Wang,\nSneha Agarwal, Soji Sajuyigbe, Soumith Chintala,\nStephanie Max, Stephen Chen, Steve Kehoe, Steve\nSatterfield, Sudarshan Govindaprasad, Sumit Gupta,\nSummer Deng, Sungmin Cho, Sunny Virk, Suraj\nSubramanian, Sy Choudhury, Sydney Goldman, Tal\nRemez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim\nMatthews, Timothy Chou, Tzook Shaked, Varun\nVontimitta, Victoria Ajayi, Victoria Montanez, Vijai\nMohan, Vinay Satish Kumar, Vishal Mangla, Vlad\nIonescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,\nVladimir Ivanov, Wei Li, Wenchen Wang, Wen-\nwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng\nTang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo\nGao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia,\nYe Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao,\nYundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary\nDeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang,\nZhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd\nof models. Preprint, arXiv:2407.21783.\nAnnika Grützner-Zahn and Georg Rehm. 2022. Intro-\nducing the Digital Language Equality Metric: Con-\ntextual Factors.\nIn Proceedings of the Workshop\nTowards Digital Language Equality (TDLE 2022; co-\nlocated with LREC 2022), pages 13–26, Marseille,\nFrance. 20 June 2022.\nViktor Hangya, Hossain Shaikh Saadi, and Alexander\nFraser. 2022. Improving low-resource languages in\npre-trained multilingual language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 11993–\n12006, Abu Dhabi, United Arab Emirates. Associa-\ntion for Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. Preprint, arXiv:2009.03300.\nAndras Kornai. 2013. Digital Language Death. PLoS\nONE, 8(10).\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier\nGarcia, Derrick Xin, Aditya Kusupati, Romi Stella,\nAnkur Bapna, and Orhan Firat. 2023.\nMadlad-\n400: a multilingual and document-level large audited\ndataset. In Proceedings of the 37th International\nConference on Neural Information Processing Sys-\ntems, NIPS ’23, Red Hook, NY, USA. Curran Asso-\nciates Inc.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro Von Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, Jörg\nFrohberg, Mario Šaško, Quentin Lhoest, Angelina\nMcMillan-Major, Gerard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Muñoz, Jian Zhu,\nDaniel Van Strien, Zaid Alyafeai, Khalid Almubarak,\nMinh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa,\nKyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron\nGokaslan, Shamik Bose, David Adelani, Long\nPhan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim,\nViolette Lepercq, Suzana Ilic, Margaret Mitchell,\nSasha Alexandra Luccioni, and Yacine Jernite. 2023.\nThe bigscience roots corpus: A 1.6tb composite mul-\ntilingual dataset. Preprint, arXiv:2303.03915.\nPedro Henrique Martins, Patrick Fernandes, João Alves,\nNuno M. Guerreiro, Ricardo Rei, Duarte M. Alves,\nJosé Pombal, Amin Farajian, Manuel Faysse, Ma-\nteusz Klimaszewski, Pierre Colombo, Barry Haddow,\nJosé G. C. de Souza, Alexandra Birch, and André\nF. T. Martins. 2024. Eurollm: Multilingual language\nmodels for europe. Preprint, arXiv:2409.16235.\nLars Nagel and Douwe Lycklama. 2021. Design Prin-\nciples for Data Spaces. Position Paper. Technical\nreport, Berlin.\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai,\nHieu Man, Nghia Trung Ngo, Franck Dernoncourt,\nRyan A. Rossi, and Thien Huu Nguyen. 2024. Cul-\nturaX: A cleaned, enormous, and multilingual dataset\nfor large language models in 167 languages. In Pro-\nceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages 4226–\n4237, Torino, Italia. ELRA and ICCL.\nRui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng\nZhang, Chi Han, and Tong Zhang. 2024. LISA: Lay-\nerwise Importance Sampling for Memory-Efficient\nLarge Language Model Fine-Tuning. arXiv preprint.\nArXiv:2403.17919 [cs].\nGeorg Rehm, editor. 2023. European Language Grid:\nA Language Technology Platform for Multilingual\nEurope. Cognitive Technologies. Springer, Cham,\nSwitzerland.\nGeorg Rehm, Muhammad Zeshan Afzal, Aliki Anag-\nnostopoulou, Mahta Bakshizadeh, Stephan Baumann,\n10\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nCristina España-Bonet, Carlos Franzreb, Josef van\nGenabith, Annika Grützner-Zahn, Stefanie Hegele,\nTim Herzig, Jakob Karolus, Siting Liang, Paul\nLukowicz, Katrin Marheinecke, Julián Moreno-\nSchneider, Malte Ostendorff, Simon Ostermann,\nAlain Pagani, Tim Polzehl, and Sven Schmeier. 2023.\nEuropean streaming platform for national news ac-\ncessible in all EU languages: Technical feasibility\nstudy. European Parliament, European Parliamentary\nResearch Service. STUDY Panel for the Future of\nScience and Technology. Scientific Foresight Unit\n(STOA) PE 740.249 – June 2023.\nGeorg Rehm, Stelios Piperidis, Khalid Choukri, An-\ndrejs Vasil,jevs, Katrin Marheinecke, Victoria Ar-\nranz, Aivars B¯erzin, š, Miltos Deligiannis, Dimitrios\nGalanis, Maria Gavriilidou, Maria Giagkou, Kate-\nrina Gkirtzou, Dimitris Gkoumas, Annika Grützner-\nZahn, Athanasia Kolovou, Penny Labropoulou, An-\ndis Lagzdin, š, Elena Leitner, Valérie Mapelli, Hélène\nMazo, Simon Ostermann, Stefania Racioppa, Mick-\naël Rigault, and Leon Voukoutis. 2024. Common\nEuropean Language Data Space. In Proceedings\nof the Joint International Conference on Computa-\ntional Linguistics, Language Resources and Eval-\nuation (LREC-COLING 2024), pages 3579–3586,\nTurino, Italy. European Language Resources Associ-\nation (ELRA) and International Committee on Com-\nputational Linguistics (ICCL). May 20-25, 2024.\nGeorg Rehm and Hans Uszkoreit, editors. 2012. META-\nNET White Paper Series: Europe’s Languages in the\nDigital Age, 32 volumes on 31 European languages.\nSpringer, Heidelberg etc.\nGeorg Rehm and Hans Uszkoreit, editors. 2013. The\nMETA-NET Strategic Research Agenda for Multilin-\ngual Europe 2020. Springer, Heidelberg, New York,\nDordrecht, London. More than 200 contributors from\nresearch and industry. This book at amazon.de.\nGeorg Rehm and Andy Way, editors. 2023a.\nEuro-\npean Language Equality: A Strategic Agenda for\nDigital Language Equality. Cognitive Technologies.\nSpringer.\nGeorg Rehm and Andy Way. 2023b. Strategic Research,\nInnovation and Implementation Agenda for Digital\nLanguage Equality in Europe by 2030. In Georg\nRehm and Andy Way, editors, European Language\nEquality: A Strategic Agenda for Digital Language\nEquality, Cognitive Technologies, pages 387–412.\nSpringer, Cham, Switzerland.\nAngelika Romanou, Negar Foroutan, Anna Sotnikova,\nZeming Chen, Sree Harsha Nelaturu, Shivalika\nSingh, Rishabh Maheshwary, Micol Altomare, Mo-\nhamed A. Haggag, Snegha A, Alfonso Amayuelas,\nAzril Hafizi Amirudin, Viraat Aryabumi, Danylo\nBoiko, Michael Chang, Jenny Chim, Gal Cohen,\nAditya Kumar Dalmia, Abraham Diress, Sharad\nDuwal, Daniil Dzenhaliou, Daniel Fernando Erazo\nFlorez, Fabian Farestam, Joseph Marvin Imperial,\nShayekh Bin Islam, Perttu Isotalo, Maral Jabbar-\nishiviari, Börje F. Karlsson, Eldar Khalilov, Christo-\npher Klamm, Fajri Koto, Dominik Krzemi´nski,\nGabriel Adriano de Melo, Syrielle Montariol, Yiyang\nNan, Joel Niklaus, Jekaterina Novikova, Johan\nSamir Obando Ceron, Debjit Paul, Esther Ploeger,\nJebish Purbey, Swati Rajwal, Selvan Sunitha Ravi,\nSara Rydell, Roshan Santhosh, Drishti Sharma, Mar-\njana Prifti Skenduli, Arshia Soltani Moakhar, Bar-\ndia Soltani Moakhar, Ran Tamir, Ayush Kumar\nTarun, Azmine Toushik Wasi, Thenuka Ovin Weeras-\ninghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag,\nMarzieh Fadaee, Sara Hooker, and Antoine Bosse-\nlut. 2024. Include: Evaluating multilingual language\nunderstanding with regional knowledge. Preprint,\narXiv:2411.19799.\nShivalika Singh, Angelika Romanou, Clémentine Four-\nrier, David I. Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchi-\nsio, Wei Qi Leong, Yosephine Susanto, Raymond\nNg, Shayne Longpre, Wei-Yin Ko, Madeline Smith,\nAntoine Bosselut, Alice Oh, Andre F. T. Martins,\nLeshem Choshen, Daphne Ippolito, Enzo Ferrante,\nMarzieh Fadaee, Beyza Ermis, and Sara Hooker.\n2024. Global mmlu: Understanding and addressing\ncultural and linguistic biases in multilingual evalua-\ntion. Preprint, arXiv:2412.03304.\nSTOA. 2017. Language equality in the digital age –\nTowards a Human Language Project. STOA study\n(PE 598.621), IP/G/STOA/FWC/2013-001/Lot4/C2,\nMarch 2017. Carried out by Iclaves SL (Spain) at\nthe request of the Science and Technology Options\nAssessment (STOA) Panel, managed by the Scien-\ntific Foresight Unit (STOA), within the Directorate-\nGeneral for Parliamentary Research Services (DG\nEPRS) of the European Parliament. http://www.\neuroparl.europa.eu/stoa/.\nYan Tao, Olga Viberg, Ryan S Baker, and René F Kizil-\ncec. 2024. Cultural bias and cultural alignment of\nlarge language models. PNAS Nexus, 3(9).\nTeam Gemma, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, Johan Ferret, Peter\nLiu, Pouya Tafti, Abe Friesen, Michelle Casbon,\nSabela Ramos, Ravin Kumar, Charline Le Lan,\nSammy Jerome, Anton Tsitsulin, Nino Vieillard,\nPiotr Stanczyk, Sertan Girgin, Nikola Momchev,\nMatt Hoffman, Shantanu Thakoor, Jean-Bastien Grill,\nBehnam Neyshabur, Olivier Bachem, Alanna Wal-\nton, Aliaksei Severyn, Alicia Parrish, Aliya Ah-\nmad, Allen Hutchison, Alvin Abdagic, Amanda\nCarl, Amy Shen, Andy Brock, Andy Coenen, An-\nthony Laforge, Antonia Paterson, Ben Bastian, Bilal\nPiot, Bo Wu, Brandon Royal, Charlie Chen, Chintu\nKumar, Chris Perry, Chris Welty, Christopher A.\nChoquette-Choo, Danila Sinopalnikov, David Wein-\nberger, Dimple Vijaykumar, Dominika Rogozi´nska,\nDustin Herbison, Elisa Bandy, Emma Wang, Eric\nNoland, Erica Moreira, Evan Senter, Evgenii Elty-\nshev, Francesco Visin, Gabriel Rasskin, Gary Wei,\nGlenn Cameron, Gus Martins, Hadi Hashemi, Hanna\nKlimczak-Pluci´nska, Harleen Batra, Harsh Dhand,\n11\nISCA/ITG Workshop on Diversity in Large Speech and Language Models\nIvan Nardini, Jacinda Mein, Jack Zhou, James Svens-\nson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana\nCarrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fer-\nnandez, Joost van Amersfoort, Josh Gordon, Josh\nLipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mo-\nhamed, Kartikeya Badola, Kat Black, Katie Mil-\nlican, Keelin McDonell, Kelvin Nguyen, Kiranbir\nSodhia, Kish Greene, Lars Lowe Sjoesund, Lau-\nren Usui, Laurent Sifre, Lena Heuermann, Leti-\ncia Lago, Lilly McNealus, Livio Baldini Soares,\nLogan Kilpatrick, Lucas Dixon, Luciano Martins,\nMachel Reid, Manvinder Singh, Mark Iverson, Mar-\ntin Görner, Mat Velloso, Mateo Wirth, Matt Davi-\ndow, Matt Miller, Matthew Rahtz, Matthew Watson,\nMeg Risdal, Mehran Kazemi, Michael Moynihan,\nMing Zhang, Minsuk Kahng, Minwoo Park, Mofi\nRahman, Mohit Khatwani, Natalie Dao, Nenshad\nBardoliwalla, Nesh Devanathan, Neta Dumai, Nilay\nChauhan, Oscar Wahltinez, Pankil Botarda, Parker\nBarnes, Paul Barham, Paul Michel, Pengchong\nJin, Petko Georgiev, Phil Culliton, Pradeep Kup-\npala, Ramona Comanescu, Ramona Merhej, Reena\nJana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan\nMullins, Samaneh Saadat, Sara Mc Carthy, Sarah\nCogan, Sarah Perrin, Sébastien M. R. Arnold, Se-\nbastian Krause, Shengyang Dai, Shruti Garg, Shruti\nSheth, Sue Ronstrom, Susan Chan, Timothy Jor-\ndan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas\nKocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav,\nVilobh Meshram, Vishal Dharmadhikari, Warren\nBarkley, Wei Wei, Wenming Ye, Woohyun Han,\nWoosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong,\nZichuan Wei, Victor Cotruta, Phoebe Kirk, Anand\nRao, Minh Giang, Ludovic Peran, Tris Warkentin,\nEli Collins, Joelle Barral, Zoubin Ghahramani, Raia\nHadsell, D. Sculley, Jeanine Banks, Anca Dragan,\nSlav Petrov, Oriol Vinyals, Jeff Dean, Demis Hass-\nabis, Koray Kavukcuoglu, Clement Farabet, Elena\nBuchatskaya, Sebastian Borgeaud, Noah Fiedel, Ar-\nmand Joulin, Kathleen Kenealy, Robert Dadashi, and\nAlek Andreev. 2024. Gemma 2: Improving Open\nLanguage Models at a Practical Size. arXiv preprint.\nArXiv:2408.00118 [cs].\nKlaudia Thellmann, Bernhard Stadler, Michael Fromm,\nJasper Schulze Buschhoff, Alex Jude, Fabio Barth,\nJohannes Leveling, Nicolas Flores-Herr, Joachim\nKöhler, René Jäkel, and Mehdi Ali. 2024. Towards\nmultilingual llm evaluation for european languages.\nPreprint, arXiv:2410.08928.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. Preprint, arXiv:2307.09288.\nAlex Warstadt, Aaron Mueller, Leshem Choshen, Ethan\nWilcox, Chengxu Zhuang, Juan Ciro, Rafael Mos-\nquera, Bhargavi Paranjabe, Adina Williams, Tal\nLinzen, and Ryan Cotterell. 2023. Findings of the\nBabyLM Challenge: Sample-Efficient Pretraining on\nDevelopmentally Plausible Corpora. In Proceedings\nof the BabyLM Challenge at the 27th Conference on\nComputational Natural Language Learning, pages\n1–6, Singapore. Association for Computational Lin-\nguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\n12\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2025-02-18",
  "updated": "2025-02-18"
}