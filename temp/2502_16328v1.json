{
  "id": "http://arxiv.org/abs/2502.16328v1",
  "title": "Risk-Averse Reinforcement Learning: An Optimal Transport Perspective on Temporal Difference Learning",
  "authors": [
    "Zahra Shahrooei",
    "Ali Baheri"
  ],
  "abstract": "The primary goal of reinforcement learning is to develop decision-making\npolicies that prioritize optimal performance, frequently without considering\nrisk or safety. In contrast, safe reinforcement learning seeks to reduce or\navoid unsafe states. This letter introduces a risk-averse temporal difference\nalgorithm that uses optimal transport theory to direct the agent toward\npredictable behavior. By incorporating a risk indicator, the agent learns to\nfavor actions with predictable consequences. We evaluate the proposed algorithm\nin several case studies and show its effectiveness in the presence of\nuncertainty. The results demonstrate that our method reduces the frequency of\nvisits to risky states while preserving performance. A Python implementation of\nthe algorithm is available at https://\ngithub.com/SAILRIT/Risk-averse-TD-Learning.",
  "text": "Risk-Averse Reinforcement Learning: An Optimal Transport\nPerspective on Temporal Difference Learning\nZahra Shahrooei and Ali Baheri\nAbstract— The primary goal of reinforcement learning is\nto develop decision-making policies that prioritize optimal\nperformance, frequently without considering risk or safety. In\ncontrast, safe reinforcement learning seeks to reduce or avoid\nunsafe states. This letter introduces a risk-averse temporal\ndifference algorithm that uses optimal transport theory to direct\nthe agent toward predictable behavior. By incorporating a risk\nindicator, the agent learns to favor actions with predictable con-\nsequences. We evaluate the proposed algorithm in several case\nstudies and show its effectiveness in the presence of uncertainty.\nThe results demonstrate that our method reduces the frequency\nof visits to risky states while preserving performance. A Python\nimplementation of the algorithm is available at https://\ngithub.com/SAILRIT/Risk-averse-TD-Learning.\nI. INTRODUCTION\nReinforcement learning (RL) algorithms focus on max-\nimizing performance, primarily through long-term reward\noptimization. However, this objective alone does not always\nprevent negative or high-risk outcomes. Ensuring safety is\ncrucial for RL applications in robotics, autonomous sys-\ntems, and safety-critical tasks [1]. To address this concern,\nresearchers have explored various approaches to integrate\nsafety into RL. A comprehensive review of safe RL methods\ncan be found in [2].\nSeveral early safe RL studies incorporate safety into the\noptimization criterion [3]–[9]. For example, in the worst-\ncase criterion, a policy is considered optimal if it maximizes\nthe worst-case return, reducing variability due to inherent or\nparametric uncertainty that may lead to undesirable outcomes\n[3], [4]. The optimization criterion can also be adjusted to\nbalance return and risk using a subjective measure, such as a\nlinear combination of return and risk which can be defined as\nthe variance of return [5] or as the probability of entering an\nerror state [6]. The other way is to optimize the return sub-\nject to constraints resulting in the constrained optimization\ncriterion [7], [8]. Other approaches aim to avoid heuristic\nexploration strategies which are blind to the risk of actions.\nInstead, they propose modifications the exploration process\nto guide the agent toward safer regions. Safe exploration\ntechniques include prior knowledge of the task for search\ninitialization [10], learn from human demonstrations [11],\nand incorporate a risk metric to the algorithm [12], [13].\nBuilding on these strategies, we explore the use of optimal\ntransport (OT) theory to enhance safety in RL by guiding the\nagent to prioritize visiting safer states during training. OT is\nhighly valued for its ability to measure and optimize the\nZahra Shahrooei and Ali Baheri are with the Department of Mechanical\nEngineering, Rochester Institute of Technology, Rochester, NY 14623 USA.\n(e-mail: zs9580@rit.edu; akbeme@rit.edu).\nSource distribution\nTarget distribution\nT\nFig. 1: Conceptual illustration of optimal transport theory.\nHere, µ(x) is a probability distribution over the source space\nX, and ν(y) is a probability distribution over the target space\nY. The arrows represent the optimal transport plan T, which\nreallocates mass from µ to ν to minimize the total transport\ncost.\nalignment between probability distributions by minimizing\nthe cost of transforming one distribution into another as\nshown in Fig. 1. It takes into account the geometry of the\ndistributions and provides a more interpretable comparison,\nparticularly when the distributions have non-overlapping sup-\nports or complex structures [14]. There are a few applications\nof OT in safe RL [15]–[20]. For example, Queeney et al.\n[16] apply OT theory to develop a safe RL framework that\nincorporates robustness through an OT cost uncertainty set.\nThis approach constructs worst-case virtual state transitions\nusing OT perturbations which improves safety in continuous\ncontrol tasks compared to standard safe RL methods. Metelli\net al. [17] propose a novel approach called Wasserstein Q-\nlearning (WQL), which uses Bayesian posterior distribu-\ntions and Wasserstein barycenters to model and propagate\nuncertainty in RL. Their method demonstrates improved\nexploration and faster learning in tabular domains compared\nto classic RL algorithms. They also show preliminary success\nin adapting WQL to deep architectures for Atari games.\nShahrooei et al. [18] use OT for reward shaping in Q-\nlearning. Through minimization of the Wasserstein distance\nbetween the policy’s stationary distribution and a predefined\nrisk distribution, the agent is encouraged the agent to visit\nsafe states more frequently.\nIn this letter, we incorporate OT theory into temporal\ndifference (TD) learning to enhance agent safety during\nlearning. We propose a risk-averse TD framework that con-\nsiders both the reward and the uncertainty associated with\nactions without relying on expert knowledge or predefined\nsafety constraints. We use Wasserstein distance to quantify\nthe total risk level at each state and prioritize the actions\nthat contribute less to this risk. This encourages the agent\nto take safer actions more frequently and avoid higher-risk\narXiv:2502.16328v1  [cs.LG]  22 Feb 2025\nones. In other words, the agent tries to take actions with more\npredictable outcomes and avoid those with highly variable or\nuncertain consequences.\nThe contributions of this letter are: (i) introduction of a\nrisk-averse TD learning algorithm to enhance agent safety,\n(ii) safety bounds of the algorithm which demonstrates less\nvisitation to risky states, and (iii) applications on case stud-\nies with different forms of uncertainty in reward function,\ntransition function, and states to show our algorithm reduces\nvisiting unsafe states while preserving performance.\nII. PRELIMINARIES\nThis section reviews Markov decision processes, partially\nobservable Markov decision processes, temporal difference\nlearning algorithms, and the basic principles of OT theory.\nA. Markov Decision Processes and Temporal Difference\nLearning\nMarkov Decision Processes (MDPs). MDPs represent a\nfully observable reinforcement learning environment. A finite\nMDP is defined by the tuple M = (S, A, T , R, γ), where\nS is a set of states, A is a set of actions, T : S × A × S →\n[0, 1] is the transition probability function, where T (s′|s, a)\nrepresents the probability of transitioning to state s′ when\naction a is taken in state s, R : S × A →R is the reward\nfunction, and γ ∈[0, 1) is the discount factor. The agent\nfollows a policy π : S ×A →[0, 1] that maps states to action\nprobabilities, with the objective to maximize the expected\ndiscounted return, Gt = P∞\nk=0 γkrt+k+1.\nTemporal Difference Learning. The Q-value of action a in\nstate s under policy π is given by Qπ(s, a) = Eπ[Gt|st =\ns, at = a], which can be incrementally learned. In one-step\nTD, the Q-value update rule is Q(st, at) ←Q(st, at) + αδt\nwhere δt is the TD error at time t and α is the learning rate.\nIn the Q-learning algorithm [21], the TD error is defined as:\nδt = rt+1 + γ max\na\nQ(st+1, a) −Q(st, at)\n(1)\nSimilarly, in the SARSA algorithm [22], the TD error is\ngiven by:\nδt = rt+1 + γQ(st+1, at+1) −Q(st, at)\n(2)\nSARSA(λ) extends one-step SARSA using eligibility traces\nto incorporate multi-step updates and improve learning effi-\nciency. An eligibility trace tracks the degree to which each\nstate-action pair has been recently visited, which enables\nupdates to consider a weighted history of past experiences.\nThe Q-value update rule in SARSA(λ) is:\nQt+1(s, a) = Qt(s, a) + αδtet(s, a),\nfor all s, a\n(3)\nwhere et(s, a) is the eligibility trace. The eligibility trace\net(s, a) decays over time and is updated as follows:\net(s, a) =\n(\nγλet−1(s, a) + 1\nif s = st and a = at\nγλet−1(s, a)\notherwise\n(4)\nwhere λ ∈[0, 1] controls the trace decay rate. Higher\nλ values give greater weight to longer-term past states.\nTD algorithms often use an ϵ-greedy strategy for action\ngeneration. The parameter ϵ can either be fixed or decay over\ntime to balance exploration and exploitation. The behavioral\npolicy is expressed as:\nπ(st, at) =\n(\n1 −ϵ\nif at ∈arg maxa Q(st, a)\nϵ\n|A|\notherwise\n(5)\nPartially Observable MDPs (POMDPs). POMDPs gener-\nalize MDPs to account for environments where the agent\ncannot directly observe the underlying state. A POMDP is\ndefined by the tuple PM = (S, A, O, T , R, Z, γ), where\nS, A, T , R, and γ maintain their definitions from MDPs,\nand O is a set of observations the agent can perceive. The\nobservation function Z : S×A →O maps states and actions\nto observation probabilities.\nB. Optimal transport theory\nThe OT theory aims to find minimal-cost transport plans\nto move one probability distribution to another within a\nmetric space. This involves a cost function c(x, y) and two\nprobability distributions, µ(x) and ν(y). The goal is to find\na transport plan that minimizes the cost of moving µ to ν\nunder c(x, y), often using the Euclidean distance for explicit\nsolutions [23].\nWe focus on discrete OT theory, assuming µ and ν as\nsource and target distributions, respectively, both belonging\nto Pp(Rn) with finite supports {xi}m1\ni=1 and {yj}m2\nj=1, and\ncorresponding probability masses {ai}m1\ni=1 and {bj}m2\nj=1. The\ncost between support points is represented by an m1 × m2\nmatrix C, where Cij = |xi −yj|p\np denotes the transport cost\nfrom xi to yj. The OT problem seeks the transport plan P ∗\nthat minimizes the cost while ensuring that the marginals of\nP ∗match µ and ν:\nmin\nP ∈Rm1×m2\nm1\nX\ni=1\nm2\nX\nj=1\nPijCij\n(6)\nHere, the coupling matrix Pij indicates the probability mass\ntransported from xi to yj and Pm2\nj=1 Pij = ai for all i, and\nPm1\ni=1 Pij = bj for all j. Additionally, Pij ≥0 for all i, j.\nDefinition 1 [23]: The Wasserstein distance between µ and\nν is computed using the OT plan P ∗obtained from solving\nthe above linear programming problem:\nWp(µ, ν) = (⟨P ∗, C⟩)\n1\np\n(7)\nwhere p ≥1 and ⟨·, ·⟩denotes the inner product.\nTo enhance numerical stability and computational effi-\nciency, an entropy regularization term can be added to the\nobjective, leading to the regularized Wasserstein distance,\nwhich can be solved iteratively using the Sinkhorn iterations\n[24].\nDefinition 2 [24]: The Entropy-regularized OT problem is\nformulated as:\nmin\nP ∈Rm1×m2\nm1\nX\ni=1\nm2\nX\nj=1\nPijCij+ε\nm1\nX\ni=1\nm2\nX\nj=1\nPij(log Pij−1) (8)\nwhere ε > 0 is a regularization parameter that balances\ntransport cost and the entropy of the transport plan P.\nIII. METHODOLOGY\nConsider an RL agent interacting with an environment\ndefined by an MDP. For each state s, we define the Q-\ndistribution Qs as the normalized distribution over the\nagent’s estimated Q-values for the available actions ai, i =\n1, . . . , N. Formally, we have Qs = PN\ni=1 qs\ni δai, where\nqs\ni is the probability assigned to action ai based on the\ncurrent Q-value estimations, and δas\ni is the Dirac measure\ncentered at ai. Intuitively, Qs captures how the agent’s Q-\nvalues are distributed across actions at state s. We also\nintroduce a corresponding T-distribution Tt, which represents\na normalized distribution over the target values for the same\nset of actions. Specifically, we have\nTt = PN\ni=1 pt\niδai,\nwhere pt\ni is the probability associated with action ai based\non target values. For each action a in state s, we define a\nrisk indicator U(s, a). The goal is to quantify how much an\naction contributes to the overall risk of the agent’s policy in\nthat state. First, we compute the OT map P ∗between the\nQ-distribution Qs and the T-distribution Tt by solving the\nentropy-regularized Wasserstein distance formulation. The\ntotal risk in state s is measured by the Wasserstein distance\nW(Qs, Tt). For an action ai, the flow ∆(s, ai) is defined as\nthe absolute difference between the outgoing flow (transport\nfrom ai to other actions b ̸= ai) and the incoming flow\n(transport from other actions b ̸= ai to ai):\n∆(s, ai) =\n\f\f\f\f\f\f\nX\nb̸=ai\nP ∗\nai,b −\nX\nb̸=ai\nP ∗\nb,ai\n\f\f\f\f\f\f\n(9)\nHere, the first summation P\nb̸=ai P ∗\nai,b represents how much\nprobability mass is transported away from ai to other ac-\ntions, while the second summation P\nb̸=ai P ∗\nb,ai denotes how\nmuch mass flows into ai from other actions. The absolute\ndifference between these two flows, ∆(s, ai), captures how\nmuch the probability of an action in Q-distribution must\nbe “redistributed” to match T-distribution. We subsequently\nnormalize this value by the total Wasserstein distance:\nU(s, ai) =\n∆(s, ai)\nW(Qs, Tt)\n(10)\nThe risk indicator U(s, ai) thus reflects how much action\nai is responsible for the mismatch between Qs and Tt.\nEquivalently, it reveals to what extent the action ai needs to\nTABLE I: Example: Incorporating uncertainty to the behav-\nioral policy considering β = 0.5. Standard SARSA chooses\na4, while our algorithm prefers a2. The safest available\naction is a1.\nAction\nQ(s, a)\nU(s, a)\nQ(s, a) −βU(s, a)\na1\n0.2\n0.21\n0.09\na2\n0.3\n0.37\n0.112\na3\n0.1\n0.78\n−0.29\na4\n0.4\n0.62\n0.087\nbe adjusted (positively or negatively) for Qs to align with Tt\nin a cost-efficient manner. Higher values of ∆(s, ai) indicate\nlarger corrections to the probability of ai in Qs, which\nsuggest more uncertainty or risk. While standard SARSA\ndoes not account for safety, we incorporate the above risk\nindicator into the behavioral policy. Let β be a risk sensitivity\ncoefficient that specifies how strongly the agent prioritizes\nsafety relative to reward. We then modify the behavioral\npolicy as follows:\nQ(s, a) −βU(s, a)\n(11)\nImportantly, the agent’s action-selection process is biased to\nfavor actions with both high Q-values and low-risk indica-\ntors. Indeed, we encourage the agent to choose the action for\nwhich it has the highest confidence in the outcome among\nthe actions experienced in that state. This approach enables\na directed exploration strategy that prioritizes safer actions\nwith higher rewards. Notably, the uncertainty we address\nhere is aleatoric, which is inherent to the environment and\nirreducible. Example 1 further illustrates the influence of\nthis risk indicator on both Q-values and policy decisions.\nIn particular, it demonstrates the trade-off between exploiting\nhigh-return actions and mitigating high-risk actions to ensure\nsafer exploration and more stable learning.\nIn the context of POMDPs, we use the SARSA(λ) variant.\nSpecifically, the Q and target distributions associated with\nthe agent’s current observation o and the available actions\na1, · · · , aN are integrated into the safety indicator term\nU(o, ai). Moreover, the flexibility of our approach enables its\nextension to scenarios that involve using multiple consecutive\nobservations ot−n, · · · , ot, which can help the algorithm\nbetter capture non-Markovian properties.\nExample: Consider a fixed state s with four available ac-\ntions. For this state, we have access to Q- and T-distributions\nover actions as depicted in Fig. 2. Table. I shows that how the\nrisk indicator term affects the Q-values and action selection.\nAs observed, a1 is the safest action and a4 offers the highest\nreward. In decision-making, standard SARSA prefers action\na4, whereas risk-averse SARSA chooses a2 to balance the\nreward and safety.\nTheorem 1: Let Srisk\n⊂S be a set of hazardous or\na1\na2\na3\na4\n0.2\n0.4\n0.3\n0.1\n0.2\n0.18\n0.35\n0.27\nSafer actions\nRiskier actions\na1\na 2\na3\na4\nQ s\nTt\nFig. 2: Example: For a fixed state and four available actions,\nwe compute the total uncertainty of the state W(Qs, Tt) and\nthe contribution of each action to this uncertainty.\n“risky” states to avoid. For each t, define policy πt to be\nthe ϵ-greedy policy derived from the risk-augmented Q -\nvalues Qt(s, a) −βUt(s, a). Assume that every state-action\npair is explored infinitely often in the limit (i.e., persistent\nexploration) and the step-size {αt} satisfies usual stochastic-\napproximation conditions. Then for sufficiently large β, there\nexists a constant 0 < c < 1 such that\nlim\nt→∞Prπt [s ∈Srisk ] ≤c · Prπ0 [s ∈Srisk ] ,\n(12)\nwhere π0 is a baseline policy (e.g., standard SARSA’s ϵ-\ngreedy policy w.r.t. Q alone).\nProof: Let M = (S, A, T , R, γ) be a finite MDP with\nstate space S and action space A. Any stationary policy π :\nS →P(A) (where P(A) is the space of distributions over\nactions) induces a transition probability\nPπ (s′ | s) =\nX\na∈A\nπ(a | s)T (s′ | s, a)\n(13)\nBecause the state and action sets are finite, each π yields\na finite-state Markov chain with transition matrix Pπ. If\nPπ is irreducible and aperiodic, there is a unique stationary\ndistribution µπ. By the Ergodic Theorem for Markov chains,\nfor any initial state s0, the fraction of time the chain spends\nin state s converges to µπ(s) almost surely. Suppose π0\nis derived from (say) standard SARSA or Q-learning with\nϵ-greedy action selection based purely on Q(s, a). Let µ0\nbe the unique stationary distribution of the Markov chain\ninduced by π0. Then\nPr\nπ0 {s ∈Srisk } =\nX\ns∈Srikk\nµ0(s)\n(14)\nIn contrast, the “safe” policy πt at episode (or time) t follows\nan ϵ-greedy strategy with respect to the risk-adjusted Q-\nvalues, Qt(s, a)−βUt(s, a)\nπt(a | s) =\n\n\n\n1 −ϵ,\nif a ∈arg max\na′\n\u0000Qt(s, a′) −βUt(s, a′)\n\u0001\nϵ\n|A|,\notherwise.\n(15)\nFor large β, actions with large Ut(s, a) become less likely\nto be chosen. Over time t →∞, if the algorithm converges,\nthen Qt →Q∗\nβ and Ut →U ∗\nβ in some stable sense, and\nhence πt →πβ. Let πβ denote the limiting “risk-averse”\npolicy and µβ its stationary distribution. Intuitively, if an\naction a leads frequently to or transitions inside Srisk , it will\naccumulate a larger risk indicator U(s, a), because repeatedly\nvisiting or transitioning into hazardous states forces signifi-\ncant “corrections” in the Q-distribution (cf. the definition of\nU via OT). Under the update Q(s, a) −βU(s, a), if U(s, a)\nis large, then Q(s, a) −βU(s, a) might be substantially less\nthan competing actions. Thus, for large β, the probability\nπβ(a | s) of choosing such a risky action decreases, unless its\nQ-value is significantly higher than the alternatives. We now\ncompare µβ (Srisk ) (the stationary measure of risky states\nunder πβ) with µ0 (Srisk ) (the stationary measure under\nπ0). Let Arisk (s) ⊆A denote the actions in state s whose\ntransitions have high probability of landing in Srisk or staying\nthere. Formally, for a given threshold δ > 0, define\nArisk (s) = {a ∈A : Pr [st+1 ∈Srisk | st = s, at = a] ≥δ}\n(16)\nBecause βU(s, a) increases if an action is repeatedly leading\nto these hazardous states, for β sufficiently large, actions\nin Arisk (s) are given low preference in πβ. One classical\nmethod is to compare two Markov chains Pπ0 and Pπβ\nvia a coupling argument. Intuitively, whenever π0 chooses\na “risky” action in state s, πβ might choose a safer action\nin the same state with strictly higher probability if β is\nlarge. Over many transitions, the chain under πβ accumulates\nstrictly fewer visits to Srisk . In finite-state Markov chains, the\nstationary distribution µ is the normalized left-eigenvector of\nthe transition matrix P. As we tune β →∞, the transition\nprobabilities in Pπβ (s′ | s) that lead to Srisk shrink, while\ntransitions within the safe region S\\Srisk\nbecome more\nlikely. Consequently, the fraction of time spent in Srisk must\ndecrease compared to the baseline. Formally,\nµβ(s) = 1\nZβ\nexp(Φ(s, β))µ0(s)\n(17)\nfor some function Φ that accounts for changes in transition\nprobabilities. Because transitions to Srisk are heavily penal-\nized for large β, Φ is negative for s ∈Srisk ; thus µβ (Srisk )\nmust shrink relative to µ0 (Srisk ). Since the long-run fraction\nof time spent in Srisk converges to the respective stationary\nmeasure for each chain, we obtain\nlim\nt→∞Pr {s ∈Srisk } = µβ (Srisk )\n≤cµ0 (Srisk ) = cPr\nπ0 {s ∈Srisk } . (18)\nIV. EXPERIMENTAL RESULTS\nA. Case Studies\nWe evaluate risk-averse SARSA in three case studies\nwith uncertainties in rewards, transitions, and states (Fig.\n3). For each case, we conduct experiments under low and\nhigh uncertainty levels and further examine the effect of\nincreasing the environment size on performance.\nCase study 1: Grid-world with Reward Uncertainty. We\nconsider a 10×10 grid-world environment with normal, goal,\n G\n  S\n  S\n \n  S\n  S\n  S\n  S\n  S\n  \n  S\n  S\n  S\n  S\n  S\n  \n  S\n  S\n  S\n  S\n  S\n  S\n  S\n  S\n  S\n  \n  S\n  S\n  S\n  S\n \n  \n  S\n  S\n  S\n  S\n  \n  S\n  S\n  \n  S\n  S\n  S\n  G\nCliff\n0.05\n0.05\n0.05\n G\n0.05\n0.6\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.6\n0.05\n0.05\n0.6\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.05\n0.9\n0.05\n0.05\nS\nFig. 3: Case studies with low uncertainty level: (Left) grid-\nworld with slippery states, (Center) cliff walking with traps,\nwhere the blue cells represent the trap region. (Right) rover\nnavigation task with partial observability of obstacle loca-\ntions.\nTABLE II: Average R ± Std over different algorithms on\ngrid-world environment.\nScenario\nSARSA\nQ-learning\nOurs\n10 × 10 LU\n−15.70 ± 17.32\n−13.38 ± 14.53\n−12.04 ± 12.83\n10 × 10 HU\n−16.26 ± 20.38\n−14.14 ± 18.52\n−12.92 ± 16.79\n30 × 30 HU\n−119.16 ± 126.65\n−108.21 ± 118.76\n−98.77 ± 112.15\nand slippery states [12]. The agent can move up, down, left,\nand right. For any movement to a normal state, the agent\nreceives the reward of −1, while transitions to slippery states\nresult in a random reward in the range [−12, 10]. Collisions\nwith walls incur a reward of −10. The episode terminates\nwhen the agent either reaches the goal state in the top-right\ncorner or completes a maximum of 100 steps.\nCase study 2: Cliff Walking with Transition Uncertainty.\nThe cliff walking environment [25] consists of three zones:\nthe cliff region, the trap region, and the feasible region.\nThe agent starts at the bottom-left corner to reach the goal\nat the bottom-right corner while avoiding the cliff zone,\nwhich represents unsafe states. Entering the cliff region\nresults in task failure. The agent can move freely within\nthe feasible region in four directions: up, down, left, and\nright. Entering the trap region forces the agent to move\ndownward, regardless of its chosen action, eventually ending\nup in the cliff region. Each movement yields a reward of\n−1. If the agent collides with the environment borders, its\nposition remains unchanged, but it still earns the movement\nreward. Reaching the target earns the agent a reward of 101,\nwhile entering the cliff region results in a −49 penalty.\nCase study 3: Rover Navigation with Partial Observ-\nability. In this case study, a rover must navigate a two-\ndimensional terrain map represented as a 10×10 grid, where\n3 of the grid cells are obstacles. Each grid cell represents a\nstate, and the rover can move in eight geographic directions.\nHowever, the environment is stochastic; for example, as\nshown in Fig. 3, when the rover takes the action east, it\nmoves to the intended grid cell with a probability of 0.9 but\nmay move to one of the adjacent cells with a probability of\n0.05. Partial observability exists because the rover cannot\ndirectly detect the locations of obstacle cells through its\nmeasurements. When the rover moves to a cell adjacent to\nan obstacle, it can identify the exact location of the obstacle\n(marked in magenta) with a probability of 0.6 and observe a\nprobability distribution over nearby cells (marked in pink).\nColliding with an obstacle results in an immediate penalty\nof 10, while reaching the goal region provides no immediate\nreward. All other grid cells impose a penalty of 2. We\nconsider γ = 0.99 and λ = 0.9.\nB. Discussion\nFig. 4 shows the average return of different algorithms\nover 50 random seeds for different case studies with a\nlow level of uncertainty. The results demonstrate that the\nrisk-averse SARSA algorithm converges to a higher return\nvalue and exhibits higher stability throughout the learning\nprocess, mainly because of the risk indicator term, which\nguides exploration toward safer and more consistent actions.\nFor the cliff walking case study, risk-averse SARSA not\nonly achieves rapid convergence but also demonstrates a\nhigher confidence in its return estimates. In contrast, both\nQ-learning and SARSA display greater variability, which\nreflects higher uncertainty in their returns.\nTable. II, III, and IV present a quantitative comparison of\nrisk-averse SARSA performance to other baselines, provid-\ning the average return (R) and standard deviation (std) under\nlow uncertainty (LU), high uncertainty (HU), and increasing\nenvironment size scenarios across last 20 episodes for all\ncase studies. The results in Table. II confirm that risk-averse\nSARSA achieves the highest return and the lowest std in\ndifferent scenarios. We present the state visitation map for\nLU and HU shown in Fig. 5 and Fig. 6. As expected, the\nSARSA algorithm, which lacks any safety considerations,\ndemonstrates a high frequency of visits to slippery regions\n(darker red). In contrast, Q-learning performs better by\nexploring more efficient paths, but it still exhibits notable\nvisits to unsafe states in comparison to risk-averse SARSA.\nFor the cliff walking case study, the observations in Table.\nIII demonstrate that risk-averse SARSA outperforms SARSA\nand Q-learning algorithms by converging to higher return\nvalues with lower std values. For this case study, the state\nvisitation graph in Fig. 7 and Fig. 8 highlights the limitations\nof SARSA, where the agent struggles to identify the optimal\npath to the goal state in both LU and HU scenarios. Con-\nsequently, most episodes end without successfully reaching\nthe goal. Q-learning performance is closer to our algorithm,\nhowever the rate of reaching the goal and escaping from the\ncliff at the beginning of episodes is lower than risk-averse\nSARSA. Moreover, in this case study number of failures (F)\nfor risk-averse agent is significantly lower than both SARSA\n0\n200\n400\n600\n800\n1000\n1200\n1400\nEpisode\n−250\n−200\n−150\n−100\n−50\n0\nReturn\nQ-learning\nSARSA\nOurs\n(a)\n0\n200\n400\n600\n800\n1000\n1200\n1400\nEpisode\n−100\n−75\n−50\n−25\n0\n25\n50\n75\n100\nReturn\nQ-learning\nSARSA\nOurs\n(b)\n0\n500\n1000\n1500\n2000\n2500\n3000\nEpisode\n−300\n−280\n−260\n−240\n−220\n−200\n−180\n−160\n−140\n−120\nReturn\nSARSA(λ)\nSafe SARSA(λ)\n(c)\nFig. 4: Comparison between average cumulative reward over\n50 random seeds for (a) grid-world and (b) cliff walking\n(c) rover navigation case studies with low uncertainty level.\nIn all case studies, risk-averse SARSA outperforms other\nbaselines.\nScenario\nR/F\nSARSA\nQ-Learning\nOurs\n10 × 7 LU\nR ± Std\n72.48 ± 36.1\n74.59 ± 28.52\n87.99 ± 10.65\nF\n72.74\n61.46\n21.74\n10 × 7 HU\nR ± Std\n69.82 ± 38.69\n84.27 ± 23.99\n89.53 ± 8.65\nF\n196.94\n77.56\n30.14\n30 × 21 HU\nR ± Std\n−177.07 ± 54.61\n42.11 ± 37.44\n55.59 ± 22.36\nF\n523.64\n362.66\n196.48\nTABLE III: Average performance metrics for different sce-\nnarios of cliff walking environment.\nG\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nQ-learning\nG\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nSARSA\nG\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nOurs\nFig. 5: Comparison between state visitation density for\ndifferent algorithms on grid-world with low uncertainty.\nG\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nQ-learning\nG\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nSARSA\nG\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nS\nOurs\nFig. 6: Comparison between state visitation density for\ndifferent algorithms on grid-world with high uncertainty.\nG\nS\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nT\nT\nQ-learning\nG\nS\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nT\nT\nSARSA\nG\nS\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nT\nT\nOurs\nFig. 7: Comparison between the density of state visitation for\ndifferent algorithms in case of cliff walking low uncertainty.\nG\nS\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nT\nT\nT\nT\nT\nT\nQ-learning\nG\nS\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nT\nT\nT\nT\nT\nT\nSARSA\nG\nS\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nT\nT\nT\nT\nT\nT\nOurs\nFig. 8: Comparison between the density of state visitation for\ndifferent algorithms in case of cliff walking high uncertainty.\nand Q-learning. For instance, in the LU scenario, risk-averse\nSARSA reduces failures by 35% compared to SARSA and\n30% compared to Q-learning. This reduction is even greater\nin the other two scenarios. Overall, in both MDP case studies,\nour algorithm obtained a higher cumulative reward than\nQ-learning and SARSA, while improving the stability and\nthe safety of the agent by avoiding unpredictable actions.\nFurthermore, for both MDP case studies, although increasing\nthe size of the environment causes lower return values, risk-\naverse SARSA still maintains the best performance.\nFor the POMDP case study, as can be seen in Table.\nIV, risk-averse SARSA(λ) achieves superior performance\ncompared to standard SARSA(λ) in the case of low-partial\nobservability. By increasing the partial observability degree\n(HU) and the size of the environment, the agent performance\nfalls back. This shows that the performance of our algorithm\ncan be influenced by the degree of partial observability\nin the environment. Specifically, when the agent receives\nindistinguishable or highly similar observations for different\nScenario\nR/F\nSARSA(λ)\nOurs\n10 × 10 LU\nR ± Std\n−155.04 ± 109.09\n−149.96 ± 106.41\nF\n2805.73\n2663.48\n10 × 10 HU\nR ± Std\n−247.70 ± 102.58\n−264.00 ± 89.72\nF\n9039.29\n8948.18\n30 × 30 HU\nR ± Std\n−1020.70 ± 39.26\n−1024.74 ± 57.09\nF\n30026.71\n30026.71\nTABLE IV: Average number of obstacle collisions for\nSARSA(λ),\nand\nrisk-averse\nSARSA(λ)\nalgorithms\nfor\nPOMDP case study.\nunderlying states, the accuracy of the estimated Q-value\nand target distributions and, consequently, the reliability of\nthe risk indicator becomes questionable. The number of\nfailures for risk-averse SARSA(λ) across the three scenarios\nis slightly lower than SARSA(λ).\nV. CONCLUSIONS\nWe presented a risk-averse temporal difference algorithm\nbased on optimal transport theory. We demonstrated the\neffectiveness of this approach in encouraging agents to pri-\noritize less uncertain actions, leading to a reduction in visits\nto risky states and an improvement in cumulative rewards.\nCompared to standard temporal difference algorithms, our\nalgorithm demonstrated robust performance in environments\nwith reward, transition, and state uncertainties. Although\nour algorithm outperforms standard TD learning methods,\nit has its limitations. Determining the optimal transport\nmap for candidate actions at each state is computationally\nexpensive. While using the entropy-regularized extension of\nOT reduces this computational cost, further improvements in\ncomputational efficiency will be a focus of future work.\nREFERENCES\n[1]\nS. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, and A. Knoll,\n“A review of safe reinforcement learning: Methods, theories and\napplications,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2024.\n[2]\nJ. Garcıa and F. Fern´andez, “A comprehensive survey on safe\nreinforcement learning,” Journal of Machine Learning Research,\nvol. 16, no. 1, pp. 1437–1480, 2015.\n[3]\nM. Heger, “Consideration of risk in reinforcement learning,” in\nMachine Learning Proceedings, Elsevier, 1994, pp. 105–111.\n[4]\nC. Gaskett, “Reinforcement learning under circumstances beyond\nits control,” International Conference on Computational Intelligence\nfor Modelling Control and Automation, 2003.\n[5]\nM. Sato, H. Kimura, and S. Kobayashi, “TD algorithm for the\nvariance of return and mean-variance reinforcement learning,”\nTransactions of the Japanese Society for Artificial Intelligence,\nvol. 16, no. 3, pp. 353–362, 2001.\n[6]\nP. Geibel and F. Wysotzki, “Risk-sensitive reinforcement learning\napplied to control under constraints,” Journal of Artificial Intelli-\ngence Research, vol. 24, pp. 81–108, 2005.\n[7]\nJ. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy\noptimization,” in International Conference on Machine Learning,\nPMLR, 2017, pp. 22–31.\n[8]\nY. Chow, O. Nachum, A. Faust, E. Duenez-Guzman, and M.\nGhavamzadeh, “Lyapunov-based safe policy optimization for con-\ntinuous control,” arXiv preprint arXiv:1901.10031, 2019.\n[9]\nA. Baheri, S. Nageshrao, H. E. Tseng, I. Kolmanovsky, A. Girard,\nand D. Filev, “Deep reinforcement learning with enhanced safety\nfor autonomous highway driving,” in IEEE Intelligent Vehicles\nSymposium (IV), 2020, pp. 1550–1555.\n[10]\nY. Okawa, T. Sasaki, H. Yanami, and T. Namerikawa, “Safe\nexploration method for reinforcement learning under existence of\ndisturbance,” in Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases, Springer, 2022, pp. 132–\n147.\n[11]\nJ. Ramirez and W. Yu, “Safe reinforcement learning for learning\nfrom human demonstrations,” 2023.\n[12]\nC. Gehring and D. Precup, “Smart exploration in reinforcement\nlearning using absolute temporal difference errors,” in International\nConference on Autonomous Agents and Multi-agent Systems, 2013,\npp. 1037–1044.\n[13]\nE. L. Law, “Risk-directed exploration in reinforcement learning,”\nPhD thesis, 2005.\n[14]\nF. Santambrogio, “Optimal transport for applied mathematicians,”\nBirk¨auser, NY, vol. 55, no. 58-63, p. 94, 2015.\n[15]\nA. Baheri, “Risk-aware reinforcement learning through optimal\ntransport theory,” arXiv preprint arXiv:2309.06239, 2023.\n[16]\nJ. Queeney, E. C. Ozcan, I. C. Paschalidis, and C. G. Cassan-\ndras, “Optimal transport perturbations for safe reinforcement learn-\ning with robustness guarantees,” arXiv preprint arXiv:2301.13375,\n2023.\n[17]\nA. M. Metelli, A. Likmeta, and M. Restelli, “Propagating un-\ncertainty in reinforcement learning via Wasserstein barycenters,”\nAdvances in Neural Information Processing Systems, vol. 32, 2019.\n[18]\nZ. Shahrooei and A. Baheri, “Optimal transport-assisted risk-\nsensitive q-learning,” arXiv preprint arXiv:2406.11774, 2024.\n[19]\nA.\nBaheri\net\nal.,\n“The\nsynergy\nbetween\noptimal\ntransport\ntheory and multi-agent reinforcement learning,” arXiv preprint\narXiv:2401.10949, 2024.\n[20]\nA. Baheri, “Understanding reward ambiguity through optimal trans-\nport theory in inverse reinforcement learning,” arXiv preprint\narXiv:2310.12055, 2023.\n[21]\nC. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,\npp. 279–292, 1992.\n[22]\nG. A. Rummery and M. Niranjan, On-line Q-learning using con-\nnectionist systems. University of Cambridge, Department of Engi-\nneering Cambridge, UK, 1994, vol. 37.\n[23]\nC. Villani, Optimal transport: old and new. Springer, 2009, vol. 338.\n[24]\nM. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal\ntransport,” Advances in Neural Information Processing Systems,\nvol. 26, 2013.\n[25]\nC. Xuan, F. Zhang, and H.-K. Lam, “SEM: Safe exploration mask\nfor Q-learning,” Engineering Applications of Artificial Intelligence,\nvol. 111, p. 104 765, 2022.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2025-02-22",
  "updated": "2025-02-22"
}