{
  "id": "http://arxiv.org/abs/2502.16523v1",
  "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension",
  "authors": [
    "Yulong Wu",
    "Viktor Schlegel",
    "Riza Batista-Navarro"
  ],
  "abstract": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data.",
  "text": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation\nin Machine Reading Comprehension\nYulong Wu1, Viktor Schlegel1, 2 and Riza Batista-Navarro1\n1 Department of Computer Science, University of Manchester, United Kingdom\n2 Imperial College London, Imperial Global Singapore\n{yulong.wu, riza.batista}@manchester.ac.uk\nv.schlegel@imperial.ac.uk\nAbstract\nAs neural language models achieve human-\ncomparable performance on Machine Reading\nComprehension (MRC) and see widespread\nadoption, ensuring their robustness in real-\nworld scenarios has become increasingly im-\nportant. Current robustness evaluation research,\nthough, primarily develops synthetic perturba-\ntion methods, leaving unclear how well they\nreflect real life scenarios. Considering this, we\npresent a framework to automatically examine\nMRC models on naturally occurring textual\nperturbations, by replacing paragraph in MRC\nbenchmarks with their counterparts based on\navailable Wikipedia edit history. Such pertur-\nbation type is natural as its design does not\nstem from an arteficial generative process, in-\nherently distinct from the previously investi-\ngated synthetic approaches. In a large-scale\nstudy encompassing SQUAD datasets and vari-\nous model architectures we observe that natural\nperturbations result in performance degradation\nin pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and\nLarge Language Models (LLMs) inherit these\nerrors. Further experiments demonstrate that\nour findings generalise to natural perturbations\nfound in other more challenging MRC bench-\nmarks. In an effort to mitigate these errors,\nwe show that it is possible to improve the ro-\nbustness to natural perturbations by training on\nnaturally or synthetically perturbed examples,\nthough a noticeable gap still remains compared\nto performance on unperturbed data.\n1\nIntroduction\nTransformer-based pre-trained language models\ndemonstrate remarkable efficacy in addressing\nquestions based on a given passage of text, a task\ncommonly referred to as Machine Reading Com-\nprehension (MRC) (Devlin et al., 2019; Brown\net al., 2020; He et al., 2021; Wei et al., 2022; Tou-\nvron et al., 2023; OpenAI et al., 2024b). Despite\nthese advancements, high-performing MRC sys-\ntems are also known to succeed by relying on short-\ncuts in benchmark datasets rather than truly demon-\nstrating understanding of the passage, thereby lack-\ning robustness to various types of test-time pertur-\nbations (Ho et al., 2023; Schlegel et al., 2023; Levy\net al., 2023).\nEvaluating models’ resilience to textual perturba-\ntions during inference aids in identifying adversar-\nial instances that highlight their shortcut behavior\nand provides insights into mitigating these short-\ncuts (Ho et al., 2023). While numerous synthetic\nperturbation approaches have been explored and re-\nveal the vulnerabilities of MRC models to various\nlinguistic challenges (Ribeiro et al., 2018; Jiang\nand Bansal, 2019; Welbl et al., 2020; Tan et al.,\n2020; Tan and Joty, 2021; Schlegel et al., 2021;\nCao et al., 2022; Tran et al., 2023), a serious con-\ncern is that these carefully designed perturbations\nmight not necessarily appear in real-world settings.\nConsequently, this poses a risk of neglecting the\nweaknesses of reading comprehension systems to\nreal challenges when deployed in practical scenar-\nios, thus potentially hindering the improvement of\ntheir reliability in practical applications.\nTo counteract this issue, in this paper, we de-\nvelop a framework to inject textual changes that\narise in real-world conditions into MRC datasets\nand audit how well contemporary language mod-\nels perform under such perturbations. We deem\nthem as natural because the perturbation process\ndoes not involve any artificial manipulation, in line\nwith the definitions by Belinkov and Bisk (2018);\nHendrycks et al. (2021); Pedraza et al. (2022);\nAgarwal et al. (2022); Le et al. (2022) (Figure 1).\nResults of robustness evaluation are therefore more\nrepresentative of real-world applications. Similar\nto Belinkov and Bisk (2018), our approach utilises\nWikipedia revision histories as the source of natural\nperturbations, given that the differences between\nrevisions authentically capture the textual modifica-\n1\narXiv:2502.16523v1  [cs.CL]  23 Feb 2025\nOriginal Reading Paragraph\nRevision \nHistory\nResearchers\nInsert characters? \nSplit words?\nInject new characters randomly\nSplit words to two tokens randomly\nNaturally Perturbed Reading Paragraph\nDevelopment of the fertilized eggs is direct, in other words there is no distinctive larval form. \nJuveniles of all groups are generally planktonic, and in most species resemble miniature adult \ncydippids, gradually developing their adult body forms as they grow. In the genus Beroe, \nhowever, the juveniles have large mouths and, like the adults, lack both tentacles and \ntentacle sheaths, and in some groups, such as the flat, bottom-dwelling platyctenids, the \njuveniles behave more like true larvae, as they live among the plankton and thus occupy a \ndifferent ecological niche from their parents, and only attain the adult form by a more radical \nmetamorphosis after dropping to the sea-floor.\nDevelopment of the fertilized eggs is direct, in other words there is no distinctive larval form, and \njuveniles of all groups generally resemble miniature cydippid adults. In the genus Beroe the juveniles, like \nthe adults, lack tentacles and tentacle sheaths. In most species the juveniles gradually develop the body \nforms of their parents. In some groups, such as the flat, bottom-dwelling platyctenids, the juveniles \nbehave more like true larvae, as they live among the plankton and thus occupy a different ecological \nniche from their parents and attain the adult form by a more radical metamorphosis, after dropping to \nthe sea-floor.\nDevelopment of the fertilized eggs is direct, in other words theorJe is no distinctive larval form, and \njuveniles of all groups generally resemble miniature cydippid adults. In the genus Beroe the \njuveniles, like the adults, lack tentacles and 4tent!aLcle sheaths. In most species the juveniles \ngQradua4llsy develop the body forms of their parents. In some grkouups, suacSh as the Gfmlat, \nbottom - dwelling platyctenids, the juveniles behave more like tr0uVe larvae, as gthtey live among \nthe mplacnk1ton and thus +o7ccupy a different ecological niche from their parents and attain the \nadult form by a more radical metamorphosis, after dropping to the sea - floor.\nDevelopment of the fer tilized eg gs is dire ct, in other wor ds there is no distinctive larv al f orm, and \nj uveniles of all groups general ly re semble miniature cydippid adults. In the g enus Beroe the juve \nniles, li ke the a dults, lack tentacl es and tentacle she aths. In m ost spe cies the ju veniles gradually \ndevelop the body f orms of the ir parents. In some groups, such as the flat, bot tom - dwel ling \nplatycte nids, the juveniles behave m ore l ike true larvae, as t hey live among the plankton and thus \noccupy a different ecological niche fr om their pa rents and attain the adu lt form by a more radical \nmetamorphosis, after dropping to the sea - fl oor.\nFigure 1: Given a reading context, we extract and use\nWikipedia revision history to construct its naturally per-\nturbed version for a more realistic robustness evaluation\n(Bottom), rather than relying on a set of synthetic meth-\nods (Top).\ntions made by human editors in the real world. De-\nspite this, significant differences exist in the pertur-\nbation construction methodology between us. Per-\nturbation in (Belinkov and Bisk, 2018) is restricted\nto single word replacements and applied on non-\nEnglish source-side sentences in machine transla-\ntion. In detail, they build a look-up table of pos-\nsible lexical replacements by harvesting naturally\noccurring errors (typos, misspellings, etc.) from\navailable corpora of French/German Wikipedia ed-\nits (Max and Wisniewski, 2010; Zesch, 2012). Af-\nterwards, they replace every word in the source-\nside sentences with an error if one exists in the\nlook-up table. Different from (Belinkov and Bisk,\n2018), our approach does not restrict the pertur-\nbation level and utilise English Wikipedia. By\ncomparing the variances between each adjacent\nrevision, we identify perturbed versions for each\nWikipedia reading passage in the original MRC\nbenchmarks (if it exists). This enables us to cap-\nture more comprehensive and critical natural per-\nturbation patterns (see Section 5.2) that can not be\npossible to capture in (Belinkov and Bisk, 2018).\nOur perturbation method only alter the reading con-\ntext, while the questions and ground truth answers\nremain unchanged.\nWith the established framework, we conduct\nextensive experiments on nine datasets, evaluat-\ning forty-two models, including recently proposed\nLLMs such as DeepSeek. Experimental results on\nStanford Question Answering Dataset (SQUAD)\n(Rajpurkar et al., 2016, 2018) indicate that natural\nperturbations encompass rich linguistic variations\nand can lead to failures in the encoder-only mod-\nels, while humans are almost undeterred by their\npresence. Crucially, these errors also transfer to\nlarger and more powerful models, such as Flan-T5\nand state-of-the-art (SOTA) LLMs. These findings\nalso generalise to other and more challenging MRC\nbenchmarks (e.g., DROP (Dua et al., 2019) and\nHOTPOTQA (Yang et al., 2018)) resulting in a de-\ncrease of SOTA LLMs’ performance, emphasising\nthe harmful effects of natural perturbations. Adver-\nsarial re-training with either naturally or synthet-\nically perturbed MRC instances can enhance the\nrobustness of encoder-only models against natural\nperturbations, with the latter sometimes providing\ngreater benefits. However, there is still ample room\nfor improvement, calling for better defense strate-\ngies.\nThe contributions of this paper are as follows:\n• A Wikipedia revision history-based frame-\nwork to generate natural perturbed MRC\nbenchmarks for realistic robustness evalua-\ntion.\n• Perturbed datasets for nine diverse MRC tasks.\nTwo SQUAD challenge sets derived from er-\nror analysis of encoder-only models, on which\nSOTA LLMs struggle, even without being in-\nvolved in the creation in any capacity.\n• Empirical demonstration of the validity of\nnatural perturbations, their characterisation\nby different linguistic phenomena and their\nharmful effects on diverse model architectures\nacross benchmarks generated with the pro-\nposed framework.\n• Showcasing adversarial re-training with natu-\nral or, especially, synthetic perturbations as a\nway to enhance the robustness of encoder-only\nMRC models against natural perturbations.\n2\nRelated Work\nRobustness Evaluation in MRC\nA typical ap-\nproach to evaluate the robustness of MRC models\nis via test-time perturbation. This line of research\ndevelops different perturbation methods as attacks,\nsuch as adversarial distracting sentence addition\n(Jia and Liang, 2017; Tran et al., 2023), low-level\nattacks (Eger and Benz, 2020), word substitution\n(Wu et al., 2021), character swap (Si et al., 2021),\nentity renaming (Yan et al., 2022) and paraphras-\ning (Gan and Ng, 2019; Lai et al., 2021; Wu et al.,\n2023). Our work also fits within the category of\ntest-time perturbation, but differs from previous\nworks in that we introduce perturbations that natu-\nrally occur in real-world scenarios, therefore con-\ntributing to a more practical robustness test.\n2\nCandidate Passage Pairs Curation\nWikipedia Article Titles\nMRC Test Set\n \nOriginal \nPassages\nPerturbed \nPassages\nO1\nP1\nO2\nP2\n…\n…\nOn\nPn\nOriginal Reading\nPassages\nverbatim original passage matching\ncorresponding perturbed passage identification\nPerturbed Test Set\nanswers preserving checking\nOriginal Test Set\nPerturbed Test Set Construction\nFigure 2: Process of generating naturally perturbed MRC test sets.\nNatural Perturbation for Robustness Assess-\nment\nCompared with deliberately crafting the\nperturbed instances, the study of natural pertur-\nbation is quite under-explored. In the computer\nvision domain, researchers find that real-world\nclean images without intentional modifications can\nconfuse deep learning models as well, terming\nthem as natural adversarial examples (Hendrycks\net al., 2021; Pedraza et al., 2022). Similarly, in\nthe field of Natural Language Processing (NLP),\nnaturally occurring perturbations extracted from\nhuman-written texts can also degrade model per-\nformance in tasks such as machine translation (Be-\nlinkov and Bisk, 2018) and toxic comments de-\ntection (Le et al., 2022). Motivated by these, we\nattempt to harvest natural perturbations from avail-\nable Wikipedia revision histories and utilise them to\nmodify the original MRC instances. To the best of\nour knowledge, we are the first to investigate MRC\nmodel robustness under real natural perturbations.\n3\nNatural Perturbation Pipeline\nWe design a pipeline to automatically construct\nlabel-preserving stress MRC test sets with noises\nthat occur in real-world settings by leveraging\nWikipedia revision histories (Figure 2). Our ap-\nproach comprises two modules: candidate passage\npairs curation and perturbed test set construction.\nCandidate passage pairs curation.\nFor each En-\nglish Wikipedia article within the development set1\nof MRC datasets, we systematically extract its en-\ntire revision histories and preprocess them, includ-\ning the removal of markups and the segmentation\nof content. Subsequently, we obtain the content\n1Since not all test sets are public, we apply natural pertur-\nbations to the development sets. For simplicity, we use the\nterm “test set” throughout.\ndifferences between each current revision and the\nprevious adjacent one, identifying three distinct\nediting patterns: addition, deletion, and modifi-\ncation. In the case of an edit falling within the\nmodification pattern, we retain the paragraph from\nthe prior version as the original and the correspond-\ning one from the current version as the perturbed,\nprovided both paragraphs exceed 500 characters2.\nPerturbed test set construction.\nTo generate the\nnaturally perturbed test set, we begin by acquir-\ning all reading passages from the development set\nof each MRC dataset and identifying their entries\nin the collection of previously extracted candidate\noriginal passages, along with the corresponding per-\nturbed counterparts. Subsequently, for the matched\noriginal passages with a single occurrence, we keep\nthem and the corresponding perturbed passages;\nwhereas for those with multiple occurrences, we\nrandomly select one instance for each and extract\nits perturbed version. After obtaining the perturbed\nreading passages, we retain only those with at least\none question where all annotated ground truth an-\nswers (or all plausible answers for the unanswer-\nable question) can still be located within the per-\nturbed context, resulting in the Perturbed test set.\nFor the sake of comparison, we also construct an\nOriginal version of the test set keeping only the\noriginal passages and questions corresponding to\nthose that were included in the Perturbed version.\n4\nExperiment Setup\n4.1\nDatasets\nWe use nine English MRC datasets: SQUAD\n1.1 (Rajpurkar et al., 2016), SQUAD 2.0 (Ra-\n2This threshold setting adheres to the methodology em-\nployed in the collection of SQuAD 1.1 (Rajpurkar et al., 2016).\n3\njpurkar et al., 2018), BOOLQ (Clark et al., 2019),\nDROP (Dua et al., 2019), HOTPOTQA (distrac-\ntor) (Yang et al., 2018), TYDI QA (gold passage\ntask in English) (Clark et al., 2020) and three\ndatasets constructed in ADVERSARIAL QA (Bar-\ntolo et al., 2020): D(BIDAF), D(BERT) and\nD(ROBERTA). These are chosen as their read-\ning passages are sourced from Wikipedia, thereby\nenabling the utilisation of Wikipedia editing histo-\nries to generate the naturally perturbed test set.\n4.2\nModels\nOur evaluation study involves MRC models across\nthree different types:\nencoder-only, encoder-\ndecoder, and decoder-only. Under the encoder-\ndecoder and decoder-only model evaluation set-\ntings, we reframe MRC as the text generation task\nbased on the given context and question. Access to\nand experimentation with all models are possible\nvia the use of the HuggingFace’s Transformers li-\nbrary (Wolf et al., 2020), the vLLM library (Kwon\net al., 2023), two 80GB Nvidia A100 GPUs and\nthe OpenAI ChatGPT API.\nEncoder-only:\nWe select BERT (Devlin et al.,\n2019) and its various variants for evaluation, in-\ncluding DistilBERT (Sanh et al., 2019), SpanBERT\n(Joshi et al., 2020), RoBERTa (Liu et al., 2019),\nALBERT (Lan et al., 2020) and DeBERTa (He et al.,\n2021). Some of these model types also come with\ndifferent variations, such as size (e.g., base and\nlarge for RoBERTa), versions (e.g., v1 and v2 for\nALBERT) and whether the input text is cased or not\n(e.g., cased and uncased for BERT), all of which\nare included in the evaluation. We fine-tune these\nencoder-only pre-trained language models on the\ntraining set of the two SQUAD datasets (Rajpurkar\net al., 2016, 2018) and evaluate them on the con-\nstructed original and perturbed test sets. Model\ndetails and the hyperparameters used in model fine-\ntuning are shown in Appendix A.\nEncoder–Decoder:\nInstruction finetuning has\nbeen demonstrated to be effective in enhancing\nzero-shot performance of pretrained language mod-\nels, resulting in the development of Finetuned Lan-\nguage Net (FLAN) (Wei et al., 2022).\nIn this\nwork, we use the instruction-finetuned version of\nT5 model class, specifically the Flan-T5 (Chung\net al., 2022), available in sizes ranging from small\n(80M), base (250M), large (780M) to xl (3B). Dur-\ning evaluation, we utilise the instruction templates\nfrom MRC task collection in open-sourced FLAN\nrepository and report the model performance as the\naverage of those obtained across the employed tem-\nplates. Refer to Appendix B for various instruction\ntemplates used for the evaluation on the test sets\nwith the format as the two SQUAD datasets.\nDecoder-only:\nThere is an exponential increase\nof pre-trained generative LLMs and their fine-tuned\nchat versions, inspired by the remarkable success\nof ChatGPT (Bang et al., 2023). Therefore, our\nexperiments incorporate a broad range of recently\nproposed language model families, including GPT\n3.5\nTurbo, GPT-4o (OpenAI et al., 2024a),\nGemma (Mesnard et al., 2024), Gemma 2 (Riviere\net al., 2024), Llama 2 (Touvron et al., 2023), Llama\n3 and Llama 3.1 (Dubey et al., 2024), Llama 3.2,\nMistral (Jiang et al., 2023), OLMo (Groeneveld\net al., 2024), Qwen2.5 (Qwen et al., 2025), Falcon\n(Almazrouei et al., 2023), Falcon3 (Team, 2024),\nand DeepSeek LLM (DeepSeek-AI et al., 2024).\nThe zero-shot prompts designed for soliciting their\nresponses are presented in Appendix C.\n4.3\nEvaluation Metrics\nIn line with existing literature, we choose the\n(instance-averaged) Token-F1 score to assess the\nperformance of both encoder-only and encoder-\ndecoder models (Rajpurkar et al., 2016), as on\nSQUAD-style test sets, they are optimised to out-\nput the shortest continuous span from the context\nas the answer (or predict the question as unanswer-\nable) during inference. However, the outputs of the\ndecoder-only models do not consistently adhere to\nthe instruction due to their conversational style, ren-\ndering F1 unsuitable for evaluation. Consequently,\nwe employ a more lenient metric, namely Inclusion\nMatch (IM), which measures whether the response\nof the model contains any of the ground truth an-\nswers (Bhuiya et al., 2024). Furthermore, if the\nmodel’s output includes phrases such as “I can-\nnot answer this/the question” or “unanswerable”3,\nwe deem that the model believes the question is\nnot answerable. Model robustness is quantified by\nmeasuring the relative variation in performance (as\nreflected in the F1 or IM) under perturbations.\n5\nMRC under Natural Perturbation\nIn this section, we present and discuss the results\nof our experiments.\nWe first evaluate encoder-\n3We collate a collection of such phrases by manually ex-\namining the decoder-only models’ outputs (Check Appendix\nD for the full set).\n4\nonly models on SQUAD to establish a baseline\nevaluation of model behaviour under natural per-\nturbations. While neither represents the current\nSOTA, SQUAD’s simplicity and the stable, super-\nhuman performance of encoder-only models enable\na focused and controlled examination of perturba-\ntion effects (Section 5.1), error sources (Section\n5.2), and adversarial instance validity (Section 5.3).\nThen, we investigate the transferability of errors\nfrom encoder-only models to other architectures,\nshowing both FLAN-T5 and LLMs carry these er-\nrors significantly (Section 5.4) 4. We finally gen-\neralise the findings from the baseline evaluation\nto SOTA LLMs and other more complex datasets\n(Section 5.5).\n5.1\nAre Encoder-only MRC Models Resilient\nto Natural Perturbation?\nTable 1 presents the relative F1 change for all\nencoder-only MRC models on the naturally per-\nturbed test set generated based on the SQUAD 1.1\nand SQUAD 2.0 development set, respectively. It\ncan be clearly seen from Table 1 that overall, the\nperformance of all the examined models decreases,\nindicating that encoder-only MRC models suffer\nfrom natural perturbation. However, we notice\nthat the performance drop of all models is negli-\ngible (the biggest drop is only 3.06%), which sug-\ngests that those models also exhibit considerable\nrobustness to natural perturbations.\n5.2\nError Analysis\nAlthough encoder-only MRC models exhibit a rel-\natively small performance gap, it remains worth-\nwhile to investigate the sources of natural pertur-\nbation and reveal the perturbation phenomena con-\ntributing to models’ error. To this end, we manually\nlabel linguistic features between passages where\nmodels succeed and fail, to identify how they differ.\nWithin the original and the naturally perturbed\ntest set pair generated based on SQUAD 2.0 devel-\nopment set, we first identify 384 instances where at\nleast one encoder-only model succeeds on the orig-\ninal but fails5 on the perturbed (i.e., being adver-\n4To supplement, we further evaluate the full test set on\nFLAN-T5 and several LLMs, and measure the transferability\nof adversarial examples across all model architectures (Ap-\npendix E).\n5For answerable questions, a model’s prediction is consid-\nered correct if Exact Match (EM) score equals 1, and incorrect\nif F1 score is 0 or it determines the question is unanswerable.\nFor unanswerable questions, a model’s prediction is correct\nif it predicts the question is unanswerable, and wrong if it\nprovides an answer span.\nVictim\nSQUAD 1.1\nSQUAD 2.0\nOverall\n(Ans./Unans.)\ndistilbert-base\n−0.6\n−0.71\n(−2.76/1.71)\nbert-base-cased\n−0.21\n−0.63\n(−1.84/0.6)\nbert-base-uncased\n−0.87\n−0.49\n(−1.88/0.94)\nbert-large-cased\n−0.63\n−0.53\n(−1.61/0.55)\nbert-large-uncased\n−0.35\n−1.38\n(−2.51/−0.24)\nspanbert-base-cased\n−0.26\n−1.24\n(−2.66/0.15)\nspanbert-large-cased\n−0.51\n−1.20\n(−1.9/−0.56)\nroberta-base\n−0.61\n−0.60\n(−2.09/0.81)\nroberta-large\n−0.29\n−1.52\n(−2.6/−0.54)\nalbert-base-v1\n−1.0\n−1.07\n(−2.02/−0.22)\nalbert-base-v2\n−0.34\n−1.08\n(−2.03/−0.22)\nalbert-large-v1\n−0.42\n−0.41\n(−1.42/0.52)\nalbert-large-v2\n−0.8\n−0.69\n(−1.66/0.22)\nalbert-xxlarge-v1\n−0.75\n−1.23\n(−3.06/0.49)\nalbert-xxlarge-v2\n−0.46\n−1.28\n(−3.02/0.36)\ndeberta-large\n−0.52\n−1.05\n(−2.2/0.0)\nTable 1: Relative F1 change (%) for encoder-only\nMRC systems subjecting to natural perturbations. For\nSQUAD 2.0, the overall values are broken down to an-\nswerable and unanswerable questions, respectively.\nsarial), and then randomly select the same number\nof instances on which all encoder-only models suc-\nceed on both the original and perturbed versions\n(Naik et al., 2018). We refer to these two types\nof instances as C2W (correct to wrong) and C2C\n(correct to correct) instances, respectively. Among\nthe identified C2W and C2C instances, we further\nremove duplicates, resulting in 210 and 244 unique\noriginal and perturbed paragraph pairs, respectively.\nFurthermore, as natural perturbation can occasion-\nally help the model to get the answer correct, we\nalso filter 85 unique W2C (wrong to correct) in-\nstances on which at least two encoder-only models\nfail on the original but succeed on the perturbed.\nFinally, utilising an 8-category taxonomy of the\nsemantic edit intentions in Wikipedia revisions de-\nrived from Yang et al. (2017), the chosen 210 sam-\nples of C2W and C2C, as well as the 85 W2C were\nannotated, with 20% of the annotated C2W and\nC2C examples presented to a second annotator for\nadditional validation. See Appendix F for the in-\nstruction provided to the annotators, along with\ndetailed explanations of each edit intention. We\ncalculate the (micro-averaged) F1 score to evalu-\nate the inter-annotator agreement, which is 0.82.\nThis suggests that the annotators’ annotations align\nclosely. Figure 3 reports the annotation results.\nDistribution of perturbation types shown in Fig-\nure 3 generally aligns with the edit intentions distri-\nbution annotated in (Yang et al., 2017), with Copy\nEditing and Elaboration appearing more frequently\nthan others, such as Clarification, Fact Update,\n5\nFigure 3: The percentage (%) of samples annotated\nwith each edit intention in the C2W, C2C and W2C cate-\ngories. The percentages do not add up to 100% because\na single revision may fall into multiple intentions.\nand Refactoring. This reflects the inherent char-\nacteristics of Wikipedia revisions. From Figure 3,\nwe observe that there is no significant difference\nin the distribution of annotated edit intentions be-\ntween C2W and C2C examples, suggesting that\nthough these types of natural perturbations con-\nfuse the encoder-only MRC models, there seems\nno correlation with human-perceivable features.\nA roughly similar distribution is also observed in\nthe W2C examples, which indicates that these nat-\nural perturbation types can also facilitate correct\nanswers by the models, i.e., being beneficial. These\ndemonstrate that on SQUAD 2.0, there might be no\ncorrelation between the quality of the naturally per-\nturbed passage and its potential for being adversar-\nial6. Certain text edits aimed at improving the pas-\nsage quality, such as Copy Editing and Elaboration,\ndo render the perturbation adversarial, whereas ed-\nits intended to damage the article may not consis-\ntently result in adversarial instances; in fact, vandal-\nism can even assist models in providing correct an-\nswers. Instead, we infer that whether an edit to the\npassage can render the MRC instance adversarial or\nnot depends on the location of the edits in relation\nto the question. Among the 384 C2W and C2C ex-\namples, we measure the proportion of answerable\nquestions with the answer sentence(s) in the origi-\nnal passage remaining unmodified in the naturally\nperturbed version, which is 34.5% and 71.5%, re-\nspectively. This confirms our hypothesis that if the\n6We also find little or no significant correlation between the\nperturbation magnitude (measured as byte-level changes be-\ntween the original and perturbed passages) and model failure,\nwith point biserial correlation coefficient close to 0.\nedits affect the answer sentence(s), there is a higher\nlikelihood of the perturbed example becoming ad-\nversarial; otherwise, it might not. Copy Editing\nappears to alter the answer sentences in the read-\ning passage more frequently, making it the most\nimpactful category that confuses models (contribut-\ning to more than 40% of error cases), while other\ntypes have a lesser effect. Appendix G presents\none perturbed example for each of the C2W, C2C,\nand W2C categories, respectively, along with the\nannotated natural perturbation type(s).\n5.3\nValidity of Nature Adversarial Examples\nTo accurately assess a model’s robustness under\nperturbation, it is vital to examine the validity of\nadversarial example, i.e., whether humans can still\nfind the correct answer under the perturbation (Dyr-\nmishi et al., 2023). Two human annotators are\nrecruited to verify the validity of the 210 C2W\nexamples in Section 5.2 and the inter-annotator\nagreement is measured by computing the Cohen’s\nκ coefficient (Cohen, 1960). We then involve a\nthird human annotator to annotate the adversarial\nexamples on which the first two annotators disagree\nand take the majority label as ground truth. This\nvalidity verification process is detailed in Appendix\nH. Out of 210 C2W examples, we find that 86% of\nthem are valid (0.77 Cohen’s κ), indicating that a\nsubstantial proportion of natural adversarial ex-\namples for encoder-only MRC model(s) are valid.\n5.4\nCan Errors from Encoder-only Models\nAffect Other Architectures?\nWe further investigate whether the errors identified\nin encoder-only models carry over to other more\nrecent models and architectures, as SOTA advance-\nments in NLP would suggest otherwise. Therefore,\nwe propose an exhaustive search algorithm (Ap-\npendix I) to zoom in on the errors of encoder-only\nmodels as much as possible, curate the challenging\nnatural perturbed test set, and finally examine the\nperformance of Flan-T5 and LLMs. With the devel-\nopment set of SQUAD 1.1 and SQUAD 2.0 as the\nsource, the algorithm results in two challenge per-\nturbed test sets: NAT_V1_CHALLENGE (184 con-\ntexts, 234 questions) and NAT_V2_CHALLENGE\n(214 contexts, 442 questions (226 unanswerable)).\nTable 2 shows the evaluation results on the\nnewly generated challenge test sets.\nFrom the\ntable, we observe that the errors caused by\nnatural perturbation in encoder-only MRC\nmodels transfer to both Flan-T5 and LLMs.\n6\nOn the NAT_V1_CHALLENGE, flan-t5-small\ndemonstrates\nthe\ngreatest\nsusceptibility\nto\nnatural perturbations, experiencing a 14.27%\ndecrease in F1; while among LLMs, Gemma-7B-IT\nemerges as the least robust, with a 16.66% IM\ndrop,\nfollowed by Gemma-2B-IT (−15.83%)\nand\nLlama-3.1-8B-Instruct\n(−15.61%).\nTransitioning\nto\nthe\nNAT_V2_CHALLENGE,\nthe\nbase\nversion\nof\nflan-t5\nexhibits\nthe\nlargest performance decline at 13.83% and\nFalcon-7B-Instruct stands out as the LLM\nwith the lowest robustness (−28.28%).\nOther\nLLMs\nsuch\nas\nQwen2.5-7B-Instruct\nand\ndeepseek-llm-7b-chat also show severe robust-\nness loss, with drops of 12.21% and 11.29%,\nrespectively.\nFurther, we observe that the ro-\nbustness of models under natural perturbations\ndoes not necessarily size-dependent.\nWhile\nlarger models tend to exhibit greater robustness\nin some cases (e.g., Qwen2.5-14B-Instruct\nvs.\nQwen2.5-3B-Instruct), exceptions within\nthe Falcon and Llama model series suggest that\nfactors beyond model size–such as pretraining\ndata, training and fine-tuning methodology, and\narchitectural differences may also significantly\ninfluence susceptibility to natural perturbations.\nIn Appendix J, we showcase two adversarial\nexamples targeting LLMs sourced from our\ngenerated challenge sets.\nModel\nPerformance\noriginal vs. perturbed\nNAT_V1_CHALLENGE\nNAT_V2_CHALLENGE\nflan-t5-small\n58.76/64.76\n48.58/55.52−14.27\n42.57/44.57\n39.71/41.81−6.19\nflan-t5-base\n79.49/85.01\n66.1/73.42−13.63\n70.66/72.85\n61.16/62.78−13.83\nflan-t5-large\n88.1/92.53\n76.57/82.31−11.05\n79.11/81.01\n70.14/72.13−10.96\nflan-t5-xl\n86.25/91.57\n75.0/81.45−11.05\n83.71/85.84\n73.19/74.86−12.79\nGPT-3.5-turbo-0125\n91.03\n83.33−8.46\n51.58\n47.06−8.76\ngpt-4o-2024-11-20\n93.16\n85.9−7.79\n80.09\n75.11−6.22\nGemma-2B-IT\n51.28\n43.16−15.83\n55.66\n50.23−9.76\nGemma-7B-IT\n82.05\n68.38−16.66\n59.95\n57.01−4.9\nGemma 2-2b-IT\n85.47\n78.21−8.49\n48.87\n43.44−11.11\nGemma 2-9b-IT\n89.32\n81.62−8.62\n64.93\n59.95−7.67\nLlama 2-chat-7B\n82.91\n73.93−10.83\n41.63\n38.69−7.06\nLlama 2-chat-13B\n80.77\n73.93−8.47\n46.83\n41.18−12.06\nLlama-3-8B-Instruct\n88.89\n77.35−12.98\n51.81\n46.61−10.04\nLlama-3.1-8B-Instruct\n87.61\n73.93−15.61\n61.31\n55.43−9.59\nLlama-3.2-1B-Instruct\n54.27\n47.86−11.81\n35.29\n32.13−8.95\nLlama-3.2-3B-Instruct\n81.2\n71.37−12.11\n48.42\n43.44−10.29\nMistral-7B-Instruct-v0.2\n84.19\n73.08−13.2\n54.98\n51.36−6.58\nOLMo-7B-0724-Instruct\n90.17\n82.91−8.05\n51.36\n49.1−4.4\nQwen2.5-3B-Instruct\n78.63\n68.38−13.04\n61.31\n54.07−11.81\nQwen2.5-7B-Instruct\n88.03\n81.2−7.76\n76.02\n66.74−12.21\nQwen2.5-14B-Instruct\n92.31\n81.62−11.58\n80.54\n74.21−7.86\nFalcon-7B-Instruct\n53.42\n50.00−6.4\n32.81\n23.53−28.28\nFalcon-40B-Instruct\n69.66\n62.82−9.82\n38.69\n36.88−4.68\nFalcon3-7B-Instruct\n88.03\n79.49−9.7\n59.28\n55.43−6.49\nFalcon3-10B-Instruct\n90.6\n82.91−8.49\n64.48\n59.73−7.37\ndeepseek-llm-7b-chat\n70.51\n64.1−9.09\n42.08\n37.33−11.29\nTable 2: The performance (%) of Flan-T5 and LLMs\non the newly generated original and naturally perturbed\nchallenge test sets. Values in smaller font are changes\n(%) relative to the original performance of the model.\n5.5\nDo Our Findings Generalise to Other\nMRC Datasets?\nThe two SQUAD datasets investigated previously\nare relatively simple, as they lack challenging\nfeatures (Schlegel et al., 2020), leading to super-\nhuman performance of MRC models (Lan et al.,\n2020). To generalise our findings to more chal-\nlenging MRC benchmarks, we apply the natural\nperturbation methodology (Section 3) to the devel-\nopment set of seven more datasets and assess the\nperformance changes of multiple LLMs, as shown\nin Table 3. For DROP (Dua et al., 2019), we first\nuse the GPT-4o mini to infer the likely Wikipedia\narticle title from which each passage is retrieved 7\nand extract the revision histories for those articles.\nFor HOTPOTQA (Yang et al., 2018), we only per-\nturb the paragraphs containing the supporting facts.\nLLM\nIM Relative Change (%)\nBOOLQ\nDROP\nHOTPOTQA\nTYDI QA\nD(BIDAF)\nD(BERT)\nD(ROBERTA)\nGemma 2-2b-IT\n−2.53\n−22.22\n−3.19\n−10.3\n−13.67\n−16.12\n−8.86\nGemma 2-9b-IT\n−3.6\n−21.61\n−4.68\n−7.15\n−13.58\n−11.2\n−10.78\nLlama-3.1-8B-Instruct\n−2.93\n−21.55\n−4.85\n−7.47\n−11.11\n−14.36\n−8.46\nLlama-3.2-1B-Instruct\n1.24\n−43.23\n−2.64\n−3.84\n−8.46\n−16.47\n−13.09\nLlama-3.2-3B-Instruct\n−3.88\n−7.83\n−4.92\n−14.49\n−5.8\n−10.69\n−8.46\nMistral-7B-Instruct-v0.2\n−6.64\n−20.0\n−3.76\n−8.69\n−12.17\n−15.51\n−13.07\nOLMo-7B-0724-Instruct\n−4.82\n−33.32\n−4.99\n−6.94\n−13.67\n−14.56\n−8.79\nQwen2.5-3B-Instruct\n−5.03\n−18.18\n−5.42\n−6.16\n−9.91\n−15.22\n−9.87\nQwen2.5-7B-Instruct\n−4.24\n−16.4\n−5.55\n−10.53\n−14.49\n−12.76\n−9.69\nQwen2.5-14B-Instruct\n−4.64\n−14.7\n−5.51\n−10.39\n−9.05\n−10.37\n−12.31\nFalcon3-7B-Instruct\n−5.79\n−31.74\n−4.29\n−11.26\n−10.31\n−11.72\n−13.47\nFalcon3-10B-Instruct\n−4.42\n−12.86\n−5.86\n−13.16\n−10.98\n−5.88\n−14.34\ndeepseek-llm-7b-chat\n−3.24\n−25.49\n−5.59\n−1.71\n−10.09\n−9.88\n−12.64\naverage\n−3.89\n−22.24\n−4.71\n−8.62\n−11.02\n−12.67\n−11.06\nTable 3: IM changes (%) of SOTA LLMs on natu-\nrally perturbed test set of other more challenging MRC\ndatasets.\nOverall, when natural perturbations are applied\nto more challenging benchmarks, SOTA LLMs\nalso exhibit a lack of robustness. On average,\nthe largest performance decline occurs on DROP\n(−22.24%), suggesting that natural perturbations\nsignificantly impair models’ discrete reasoning ca-\npabilities. Besides the DROP, the average per-\nformance degradation remains substantial across\nADVERSARIAL QA and TYDI QA. HOTPOTQA,\nwhich requires multi-hop reasoning, also shows\nnon-negligible degradation (−4.71%). These fur-\nther demonstrate the broad and severe impact of\nnatural perturbations on diverse MRC tasks.\n6\nDealing With Natural Perturbations\nIn this section, we provide an initial exploration of\nmethods to defend against natural perturbations,\nfocusing on encoder-only models and SQUAD\ndatasets. Expanding to other datasets and archi-\ntectures could be explored in future work.\nTo\n7Prompt: “Given a reading paragraph, return the Wikipedia\npage title from which it is likely retrieved.”\n7\nenhance model robustness, we conduct adversar-\nial training by identifying six encoder-only model\narchitectures that already exhibit the highest ro-\nbustness to natural perturbations in their respec-\ntive categories (except albert-xxlarge-v2 on\nNAT_V2_CHALLENGE), and presenting them with\nboth original training data and the generated natu-\nrally perturbed training examples. We extract the\nentire Wikipedia revision histories for the 392 arti-\ncles in the original SQUAD training set, and then\nobtain 5, 262 (with 22, 033 questions) and 5, 311\n(with 32, 993 questions) perturbed contexts to aug-\nment the original SQUAD 1.1 and SQUAD 2.0\ntraining set, respectively, using the methodology\ndescribed in Section 3. Table 4 compares the perfor-\nmance of these models on NAT_V1_CHALLENGE\nand NAT_V2_CHALLENGE, before and after re-\ntraining.\nModel\nPerformance\n(EM/F1)\noriginal vs. perturbed\nNAT_V1_CHALLENGE\nNAT_V2_CHALLENGE\ndistilbert-base\n64.53/70.45\n41.03/47.6−32.43\n56.56/59.08\n41.18/43.3−26.71\n57.26/63.44\n43.59/51.87−18.24\n53.17/55.4\n43.89/45.51−17.85\nbert-large-cased\n79.06/83.66\n63.68/70.23−16.05\n66.29/68.35\n53.17/55.04−19.47\n74.79/80.14\n59.83/67.5−15.77\n67.87/69.31\n58.37/59.53−14.11\nspanbert-large-cased\n84.19/88.2\n67.95/74.77−15.23\n78.73/80.68\n62.44/64.99−19.45\n82.48/86.6\n69.66/76.05−12.18\n78.28/80.0\n65.61/67.12−16.1\nroberta-large\n86.75/90.21\n73.93/79.47−11.91\n82.13/84.27\n66.29/68.52−18.69\n83.33/87.15\n70.94/76.53−12.19\n81.22/82.67\n70.59/71.84−13.1\nalbert-xxlarge-v2\n84.62/89.64\n73.93/78.77−12.13\n84.62/86.07\n68.1/69.61−19.12\n86.32/90.93\n75.64/81.07−10.84\n82.58/84.08\n70.59/72.78−13.44\ndeberta-large\n88.46/92.5\n73.5/78.48−15.16\n85.07/86.65\n71.49/73.0−15.75\n88.03/91.84\n76.92/81.53−11.23\n83.03/85.1\n72.62/74.48−12.48\nTable 4: Comparison of the performance of several\nencoder-only MRC systems on NAT_V1_CHALLENGE\nand NAT_V2_CHALLENGE, before and after re-\ntraining. The results shown in the shaded areas repre-\nsent the performance of the model retrained on the aug-\nmented training set with naturally perturbed instances.\nApart from re-training with the same type of\nnoise, we also ask whether exposing models to\nsynthetic perturbations can help them confront\nnatural ones. Therefore, we incorporate thirteen\nsynthetic perturbation techniques spanning char-\nacter and word levels (see Appendix K). After-\nwards, we first retrain deberta-large with per-\nturbed training samples generated by each syn-\nthetic perturbation method, respectively, and as-\nsess the performance changes compared to the\nvanilla version on both NAT_V1_CHALLENGE and\nNAT_V2_CHALLENGE (Figure 8 in Appendix L).\nAs we observe that synthetic adversarial training\ncan assist deberta-large in handling natural per-\nturbations, we further retrain five other models in\nthe same manner and quantify the performance dif-\nference on NAT_V1_CHALLENGE compared to\nthe vanilla version, as shown in Figure 4.\nFigure 4: Absolute changes in original and perturbed\nperformance (F1), as well as the robustness of five\nencoder-only models under natural perturbations (on\nNAT_V1_CHALLENGE), following retraining with each\nsynthetic perturbation.\nIn general, for encoder-only MRC models, re-\ntraining with natural perturbations enhances the\nperformance on naturally perturbed test sets and\nimproves the robustness to such perturbations as\nwell, though this can lead to varying reductions\nin performance on the clean test set. Encourag-\ningly, adversarial training with synthetically per-\nturbed examples benefits the model’s capability\nto handle natural perturbations as well, a phe-\nnomenon differs from what is reported in machine\ntranslation task (Belinkov and Bisk, 2018).\nIn\nsome cases, the improvement even exceeds what\nachieved by retraining the model on natural pertur-\nbations alone. We also observe that the effective-\nness of adversarial training varies with model size\nand architecture. Generally, adversarial training\nbrings the most significant benefits for the weakest\ndistilbert-base, with the benefits diminishing\nin larger and more complex model architectures.\n7\nConclusion\nIn this paper, we first study the robustness of MRC\nmodels to natural perturbations, which occur under\nreal-world conditions without intentional human\nintervention. Using the proposed evaluation frame-\nwork, we show that certain naturally perturbed\nexamples can indeed be adversarial, i.e., lead to\nmodel failure, even when the modifications aim to\nimprove the overall passage quality. Natural per-\nturbations also appear to differ significantly from\nsynthetic ones, exhibiting a wide range of rich lin-\nguistic phenomena and may be more effective in\ngenerating valid adversarial instances. Adversarial\ntraining via augmentation with either naturally or\nsynthetically perturbed samples is generally benefi-\ncial for enhancing the model’s robustness to natural\nperturbations; yet, it can decrease performance on\nclean test set. Future work includes the exploration\nof alternative natural perturbation approaches and\nthe design of more effective defensive strategies.\n8\nLimitations\nWe acknowledge several limitations in our work:\n(1) Our perturbation framework constructs natu-\nral perturbations from Wikipedia edit history and\ntherefore only works with Wikipedia-based bench-\nmarks. Since the phenomenon of natural perturba-\ntions is by no means limited to Wikipedia and can\noccur in any kind of text that evolves over time,\nfuture work should explore alternative methods to\ngenerate natural perturbations for non-Wikipedia\nMRC datasets. (2) As training data augmentation\nhas a relatively limited impact, further research is\nneeded to develop better techniques for improving\nthe robustness of encoder-only models to natural\nperturbations and to investigate the relationship\nbetween robustness to natural and synthetic per-\nturbations. Enhancing the robustness of LLMs is\nalso an important direction for future work. (3) Po-\ntential data contamination may affect our findings\non LLM evaluation. Investigating its extent and\nimpact on robustness evaluation will be a focus of\nour future research efforts.\nEthical Considerations\nAll datasets, extracted natural perturbations, and\nmodels used in this work are publicly available,\nused consistently with their intended purpose and\nunder the permitted license. A very small propor-\ntion of natural perturbations may contain offensive\ncontent, as they come from reverted Wikipedia re-\nvisions intended to damage the articles. We in-\nclude these to raise awareness within the commu-\nnity about their potential impact on MRC models\nand to call for methods to improve the safety of\nMRC models–especially those LLMs operating un-\nder such adversarial conditions. While our ultimate\ngoal is to enhance model robustness, the findings\nfrom this work may carry the risk of being misused\nby malicious attackers to refine adversarial attack\nstrategies and craft attacks against similar systems.\nBefore starting the annotation task, we provide all\nannotators with clear instructions and inform the\nintended use of their annotations, obtaining their\nexplicit consent. No private or sensitive informa-\ntion was collected, other than their annotations.\nReferences\nAkshay Agarwal, Nalini Ratha, Mayank Vatsa, and\nRicha Singh. 2022. Exploring robustness connec-\ntion between artificial and natural adversarial exam-\nples. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR)\nWorkshops, pages 179–186.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMérouane Debbah, Étienne Goffinet, Daniel Hesslow,\nJulien Launay, Quentin Malartic, Daniele Mazzotta,\nBadreddine Noune, Baptiste Pannier, and Guilherme\nPenedo. 2023. The falcon series of open language\nmodels. Preprint, arXiv:2311.16867.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nMax Bartolo, Alastair Roberts, Johannes Welbl, Sebas-\ntian Riedel, and Pontus Stenetorp. 2020. Beat the AI:\nInvestigating adversarial human annotation for read-\ning comprehension. Transactions of the Association\nfor Computational Linguistics, 8:662–678.\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In International Conference on Learning Rep-\nresentations.\nNeeladri Bhuiya, Viktor Schlegel, and Stefan Winkler.\n2024. Seemingly plausible distractors in multi-hop\nreasoning: Are large language models attentive read-\ners?\nIn Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2514–2528, Miami, Florida, USA. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nYu Cao, Dianqi Li, Meng Fang, Tianyi Zhou, Jun Gao,\nYibing Zhan, and Dacheng Tao. 2022. TASA: De-\nceiving question answering models by twin answer\nsentences attack. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11975–11992, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\n9\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models. Preprint, arXiv:2210.11416.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924–2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and Psychological Mea-\nsurement, 20(1):37–46.\nDeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting\nChen, Shanhuang Chen, Damai Dai, Chengqi Deng,\nHonghui Ding, Kai Dong, Qiushi Du, Zhe Fu,\nHuazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge,\nKang Guan, Daya Guo, Jianzhong Guo, Guangbo\nHao, Zhewen Hao, Ying He, Wenjie Hu, Panpan\nHuang, Erhang Li, Guowei Li, Jiashi Li, Yao Li,\nY. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu,\nBo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan\nLiu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma,\nXiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu,\nTongzheng Ren, Zehui Ren, Chong Ruan, Zhangli\nSha, Zhihong Shao, Junxiao Song, Xuecheng Su,\nJingxiang Sun, Yaofeng Sun, Minghui Tang, Bingx-\nuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang,\nYongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie,\nZiwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu,\nYanhong Xu, Dejian Yang, Yuxiang You, Shuiping\nYu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong\nZhang, Liyue Zhang, Mingchuan Zhang, Minghua\nZhang, Wentao Zhang, Yichao Zhang, Chenggang\nZhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou,\nQihao Zhu, and Yuheng Zou. 2024. Deepseek llm:\nScaling open-source language models with longter-\nmism. Preprint, arXiv:2401.02954.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurelien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Roziere, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone,\nKhalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen-\nley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Lau-\nrens van der Maaten, Lawrence Chen, Liang Tan, Liz\nJenkins, Louis Martin, Lovish Madaan, Lubo Malo,\nLukas Blecher, Lukas Landzaat, Luke de Oliveira,\nMadeline Muzzi, Mahesh Pasupuleti, Mannat Singh,\nManohar Paluri, Marcin Kardas, Mathew Oldham,\nMathieu Rita, Maya Pavlova, Melanie Kambadur,\nMike Lewis, Min Si, Mitesh Kumar Singh, Mona\nHassan, Naman Goyal, Narjes Torabi, Nikolay Bash-\nlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier\nDuchenne, Onur Çelebi, Patrick Alrassy, Pengchuan\nZhang, Pengwei Li, Petar Vasic, Peter Weng, Pra-\njjwal Bhargava, Pratik Dubal, Praveen Krishnan,\nPunit Singh Koura, Puxin Xu, Qing He, Qingxiao\nDong, Ragavan Srinivasan, Raj Ganapathy, Ramon\nCalderer, Ricardo Silveira Cabral, Robert Stojnic,\n10\nRoberta Raileanu, Rohit Girdhar, Rohit Patel, Ro-\nmain Sauvestre, Ronnie Polidoro, Roshan Sumbaly,\nRoss Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar\nHosseini, Sahana Chennabasappa, Sanjay Singh,\nSean Bell, Seohyun Sonia Kim, Sergey Edunov,\nShaoliang Nie, Sharan Narang, Sharath Raparthy,\nSheng Shen, Shengye Wan, Shruti Bhosale, Shun\nZhang, Simon Vandenhende, Soumya Batra, Spencer\nWhitman, Sten Sootla, Stephane Collot, Suchin Gu-\nrurangan, Sydney Borodinsky, Tamar Herman, Tara\nFowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong\nXiao, Ujjwal Karn, Vedanuj Goswami, Vibhor\nGupta, Vignesh Ramanathan, Viktor Kerkez, Vincent\nGonguet, Virginie Do, Vish Vogeti, Vladan Petro-\nvic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-\nney Meers, Xavier Martinet, Xiaodong Wang, Xiao-\nqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei\nWang, Yaelle Goldschlag, Yashesh Gaur, Yasmine\nBabaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue\nLi, Yuning Mao, Zacharie Delpierre Coudert, Zheng\nYan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh,\nAaron Grattafiori, Abha Jain, Adam Kelsey, Adam\nShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva\nGoldstand, Ajay Menon, Ajay Sharma, Alex Boesen-\nberg, Alex Vaughan, Alexei Baevski, Allie Feinstein,\nAmanda Kallet, Amit Sangani, Anam Yunus, An-\ndrei Lupu, Andres Alvarado, Andrew Caples, An-\ndrew Gu, Andrew Ho, Andrew Poulton, Andrew\nRyan, Ankit Ramchandani, Annie Franco, Apara-\njita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yaz-\ndan, Beau James, Ben Maurer, Benjamin Leonhardi,\nBernie Huang, Beth Loyd, Beto De Paola, Bhargavi\nParanjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han-\ncock, Bram Wasti, Brandon Spence, Brani Stojkovic,\nBrian Gamido, Britt Montalvo, Carl Parker, Carly\nBurton, Catalina Mejia, Changhan Wang, Changkyu\nKim, Chao Zhou, Chester Hu, Ching-Hsiang Chu,\nChris Cai, Chris Tindal, Christoph Feichtenhofer, Da-\nmon Civin, Dana Beaty, Daniel Kreymer, Daniel Li,\nDanny Wyatt, David Adkins, David Xu, Davide Tes-\ntuggine, Delia David, Devi Parikh, Diana Liskovich,\nDidem Foss, Dingkang Wang, Duc Le, Dustin Hol-\nland, Edward Dowling, Eissa Jamil, Elaine Mont-\ngomery, Eleonora Presani, Emily Hahn, Emily Wood,\nErik Brinkman, Esteban Arcaute, Evan Dunbar, Evan\nSmothers, Fei Sun, Felix Kreuk, Feng Tian, Firat\nOzgenel, Francesco Caggioni, Francisco Guzmán,\nFrank Kanayet, Frank Seide, Gabriela Medina Flo-\nrez, Gabriella Schwarz, Gada Badeer, Georgia Swee,\nGil Halpern, Govind Thattai, Grant Herman, Grigory\nSizov, Guangyi, Zhang, Guna Lakshminarayanan,\nHamid Shojanazeri, Han Zou, Hannah Wang, Han-\nwen Zha, Haroun Habeeb, Harrison Rudolph, He-\nlen Suk, Henry Aspegren, Hunter Goldman, Ibrahim\nDamlaj, Igor Molybog, Igor Tufanov, Irina-Elena\nVeliche, Itai Gat, Jake Weissman, James Geboski,\nJames Kohli, Japhet Asher, Jean-Baptiste Gaya,\nJeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen,\nJeremy Reizenstein, Jeremy Teboul, Jessica Zhong,\nJian Jin, Jingyi Yang, Joe Cummings, Jon Carvill,\nJon Shepard, Jonathan McPhie, Jonathan Torres,\nJosh Ginsburg, Junjie Wang, Kai Wu, Kam Hou\nU, Karan Saxena, Karthik Prasad, Kartikay Khan-\ndelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Kun\nHuang, Kunal Chawla, Kushal Lakhotia, Kyle Huang,\nLailin Chen, Lakshya Garg, Lavender A, Leandro\nSilva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng\nYu, Liron Moshkovich, Luca Wehrstedt, Madian\nKhabsa, Manav Avalani, Manish Bhatt, Maria Tsim-\npoukelli, Martynas Mankus, Matan Hasson, Matthew\nLennie, Matthias Reso, Maxim Groshev, Maxim\nNaumov, Maya Lathi, Meghan Keneally, Michael L.\nSeltzer, Michal Valko, Michelle Restrepo, Mihir\nPatel, Mik Vyatskov, Mikayel Samvelyan, Mike\nClark, Mike Macey, Mike Wang, Miquel Jubert Her-\nmoso, Mo Metanat, Mohammad Rastegari, Mun-\nish Bansal, Nandhini Santhanam, Natascha Parks,\nNatasha White, Navyata Bawa, Nayan Singhal, Nick\nEgebo, Nicolas Usunier, Nikolay Pavlovich Laptev,\nNing Dong, Ning Zhang, Norman Cheng, Oleg\nChernoguz, Olivia Hart, Omkar Salpekar, Ozlem\nKalinli, Parkin Kent, Parth Parekh, Paul Saab, Pa-\nvan Balaji, Pedro Rittner, Philip Bontrager, Pierre\nRoux, Piotr Dollar, Polina Zvyagina, Prashant Ratan-\nchandani, Pritish Yuvraj, Qian Liang, Rachad Alao,\nRachel Rodriguez, Rafi Ayub, Raghotham Murthy,\nRaghu Nayani, Rahul Mitra, Raymond Li, Rebekkah\nHogan, Robin Battey, Rocky Wang, Rohan Mah-\neswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu,\nSamyak Datta, Sara Chugh, Sara Hunt, Sargun\nDhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma,\nSeiji Yamamoto, Sharadh Ramaswamy, Shaun Lind-\nsay, Shaun Lindsay, Sheng Feng, Shenghao Lin,\nShengxin Cindy Zha, Shiva Shankar, Shuqiang\nZhang, Shuqiang Zhang, Sinong Wang, Sneha Agar-\nwal, Soji Sajuyigbe, Soumith Chintala, Stephanie\nMax, Stephen Chen, Steve Kehoe, Steve Satterfield,\nSudarshan Govindaprasad, Sumit Gupta, Sungmin\nCho, Sunny Virk, Suraj Subramanian, Sy Choudhury,\nSydney Goldman, Tal Remez, Tamar Glaser, Tamara\nBest, Thilo Kohler, Thomas Robinson, Tianhe Li,\nTianjun Zhang, Tim Matthews, Timothy Chou, Tzook\nShaked, Varun Vontimitta, Victoria Ajayi, Victoria\nMontanez, Vijai Mohan, Vinay Satish Kumar, Vishal\nMangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru,\nVlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li,\nWenchen Wang, Wenwen Jiang, Wes Bouaziz, Will\nConstable, Xiaocheng Tang, Xiaofang Wang, Xiao-\njian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo\nGao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li,\nYilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam,\nYu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach\nRait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen,\nZhenyu Yang, and Zhiwei Zhao. 2024. The llama 3\nherd of models. Preprint, arXiv:2407.21783.\nSalijona Dyrmishi, Salah Ghamizi, and Maxime Cordy.\n2023. How do humans perceive adversarial text?\na reality check on the validity and naturalness of\nword-based adversarial attacks. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8822–8836, Toronto, Canada. Association for Com-\nputational Linguistics.\n11\nSteffen Eger and Yannik Benz. 2020. From hero to\nzéroe: A benchmark of low-level adversarial attacks.\nIn Proceedings of the 1st Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics and the 10th International Joint Confer-\nence on Natural Language Processing, pages 786–\n803, Suzhou, China. Association for Computational\nLinguistics.\nWee Chung Gan and Hwee Tou Ng. 2019. Improv-\ning the robustness of question answering systems to\nquestion paraphrasing. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 6065–6075, Florence, Italy. Asso-\nciation for Computational Linguistics.\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita\nBhagia, Rodney Kinney, Oyvind Tafjord, Ananya\nJha, Hamish Ivison, Ian Magnusson, Yizhong Wang,\nShane Arora, David Atkinson, Russell Authur,\nKhyathi Chandu, Arman Cohan, Jennifer Dumas,\nYanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot,\nWilliam Merrill, Jacob Morrison, Niklas Muen-\nnighoff, Aakanksha Naik, Crystal Nam, Matthew\nPeters, Valentina Pyatkin, Abhilasha Ravichander,\nDustin Schwenk, Saurabh Shah, William Smith,\nEmma Strubell, Nishant Subramani, Mitchell Worts-\nman, Pradeep Dasigi, Nathan Lambert, Kyle Richard-\nson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca\nSoldaini, Noah Smith, and Hannaneh Hajishirzi.\n2024. OLMo: Accelerating the science of language\nmodels. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15789–15809, Bangkok,\nThailand. Association for Computational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. {DEBERTA}: {DECODING}-\n{enhanced} {bert} {with} {disentangled} {attention}.\nIn International Conference on Learning Representa-\ntions.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. 2021. Natural adversarial\nexamples. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 15262–15271.\nXanh Ho, Johannes Mario Meissner, Saku Sugawara,\nand Akiko Aizawa. 2023. A survey on measuring and\nmitigating reasoning shortcuts in machine reading\ncomprehension. Preprint, arXiv:2209.01824.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2021–2031, Copenhagen, Denmark. Association for\nComputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023. Mistral 7b. Preprint,\narXiv:2310.06825.\nYichen Jiang and Mohit Bansal. 2019. Avoiding reason-\ning shortcuts: Adversarial evaluation, training, and\nmodel development for multi-hop QA. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 2726–2736, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020.\nSpan-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th\nSymposium on Operating Systems Principles, SOSP\n’23, page 611–626, New York, NY, USA. Association\nfor Computing Machinery.\nYuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang,\nand Dongyan Zhao. 2021. Why machine reading\ncomprehension models learn shortcuts?\nIn Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 989–1002, Online.\nAssociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nThai Le, Jooyoung Lee, Kevin Yen, Yifan Hu, and Dong-\nwon Lee. 2022. Perturbations in the wild: Leveraging\nhuman-written text perturbations for realistic adver-\nsarial attack and defense. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2022,\npages 2953–2965, Dublin, Ireland. Association for\nComputational Linguistics.\nMosh Levy, Shauli Ravfogel, and Yoav Goldberg. 2023.\nGuiding LLM to fool itself: Automatically manipulat-\ning machine reading comprehension shortcut triggers.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 8495–8505, Singapore.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nEdward\nMa.\n2019.\nNlp\naugmentation.\nhttps://github.com/makcedward/nlpaug.\nAurélien Max and Guillaume Wisniewski. 2010. Min-\ning naturally-occurring corrections and paraphrases\n12\nfrom Wikipedia‘s revision history. In Proceedings\nof the Seventh International Conference on Lan-\nguage Resources and Evaluation (LREC‘10), Val-\nletta, Malta. European Language Resources Associa-\ntion (ELRA).\nGemma Team Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nL. Sifre, Morgane Riviere, Mihir Kale, J Christo-\npher Love, Pouya Dehghani Tafti, L’eonard Hussenot,\nAakanksha Chowdhery, Adam Roberts, Aditya\nBarua, Alex Botev, Alex Castro-Ros, Ambrose\nSlone, Am’elie H’eliou, Andrea Tacchetti, Anna Bu-\nlanova, Antonia Paterson, Beth Tsai, Bobak Shahri-\nari, Charline Le Lan, Christopher A. Choquette-Choo,\nCl’ement Crepy, Daniel Cer, Daphne Ippolito, David\nReid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng\nYan, George Tucker, George-Christian Muraru, Grig-\nory Rozhdestvenskiy, Henryk Michalewski, Ian Ten-\nney, Ivan Grishchenko, Jacob Austin, James Keel-\ning, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\nStanway, Jenny Brennan, Jeremy Chen, Johan Fer-\nret, Justin Chiu, Justin Mao-Jones, Katherine Lee,\nKathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa\nLee, Lucas Dixon, Machel Reid, Maciej Mikula,\nMateo Wirth, Michael Sharman, Nikolai Chinaev,\nNithum Thain, Olivier Bachem, Oscar Chang, Oscar\nWahltinez, Paige Bailey, Paul Michel, Petko Yotov,\nPier Giuseppe Sessa, Rahma Chaabouni, Ramona\nComanescu, Reena Jana, Rohan Anil, Ross McIl-\nroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Se-\nbastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Kli-\nmenko, Tom Hennigan, Vladimir Feinberg, Woj-\nciech Stokowiec, Yu hui Chen, Zafarali Ahmed,\nZhitao Gong, Tris Brian Warkentin, Ludovic Peran,\nMinh Giang, Cl’ement Farabet, Oriol Vinyals, Jeffrey\nDean, Koray Kavukcuoglu, Demis Hassabis, Zoubin\nGhahramani, Douglas Eck, Joelle Barral, Fernando\nPereira, Eli Collins, Armand Joulin, Noah Fiedel,\nEvan Senter, Alek Andreev, and Kathleen Kenealy.\n2024. Gemma: Open models based on gemini re-\nsearch and technology. ArXiv, abs/2403.08295.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress test evaluation for natural language inference.\nIn Proceedings of the 27th International Conference\non Computational Linguistics, pages 2340–2353,\nSanta Fe, New Mexico, USA. Association for Com-\nputational Linguistics.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher,\nAdam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec\nRadford, Aleksander M ˛adry, Alex Baker-Whitcomb,\nAlex Beutel, Alex Borzunov, Alex Carney, Alex\nChow, Alex Kirillov, Alex Nichol, Alex Paino, Alex\nRenzin, Alex Tachard Passos, Alexander Kirillov,\nAlexi Christakis, Alexis Conneau, Ali Kamali, Allan\nJabri, Allison Moyer, Allison Tam, Amadou Crookes,\nAmin Tootoochian, Amin Tootoonchian, Ananya\nKumar, Andrea Vallone, Andrej Karpathy, Andrew\nBraunstein, Andrew Cann, Andrew Codispoti, An-\ndrew Galu, Andrew Kondrich, Andrew Tulloch, An-\ndrey Mishchenko, Angela Baek, Angela Jiang, An-\ntoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka\nDhar, Ashley Pantuliano, Avi Nayak, Avital Oliver,\nBarret Zoph, Behrooz Ghorbani, Ben Leimberger,\nBen Rossen, Ben Sokolowsky, Ben Wang, Benjamin\nZweig, Beth Hoover, Blake Samic, Bob McGrew,\nBobby Spero, Bogo Giertler, Bowen Cheng, Brad\nLightcap, Brandon Walkin, Brendan Quinn, Brian\nGuarraci, Brian Hsu, Bright Kellogg, Brydon East-\nman, Camillo Lugaresi, Carroll Wainwright, Cary\nBassin, Cary Hudson, Casey Chu, Chad Nelson,\nChak Li, Chan Jun Shern, Channing Conger, Char-\nlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu,\nChong Zhang, Chris Beaumont, Chris Hallacy, Chris\nKoch, Christian Gibson, Christina Kim, Christine\nChoi, Christine McLeavey, Christopher Hesse, Clau-\ndia Fischer, Clemens Winter, Coley Czarnecki, Colin\nJarvis, Colin Wei, Constantin Koumouzelis, Dane\nSherburn, Daniel Kappler, Daniel Levin, Daniel Levy,\nDavid Carr, David Farhi, David Mely, David Robin-\nson, David Sasaki, Denny Jin, Dev Valladares, Dim-\nitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan\nFindlay, Edede Oiwoh, Edmund Wong, Ehsan As-\ndar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow,\nEric Kramer, Eric Peterson, Eric Sigler, Eric Wal-\nlace, Eugene Brevdo, Evan Mays, Farzad Khorasani,\nFelipe Petroski Such, Filippo Raso, Francis Zhang,\nFred von Lohmann, Freddie Sulit, Gabriel Goh,\nGene Oden, Geoff Salmon, Giulio Starace, Greg\nBrockman, Hadi Salman, Haiming Bao, Haitang\nHu, Hannah Wong, Haoyu Wang, Heather Schmidt,\nHeather Whitney, Heewoo Jun, Hendrik Kirchner,\nHenrique Ponde de Oliveira Pinto, Hongyu Ren,\nHuiwen Chang, Hyung Won Chung, Ian Kivlichan,\nIan O’Connell, Ian O’Connell, Ian Osband, Ian Sil-\nber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya\nKostrikov, Ilya Sutskever, Ingmar Kanitscheider,\nIshaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub\nPachocki, James Aung, James Betker, James Crooks,\nJames Lennon, Jamie Kiros, Jan Leike, Jane Park,\nJason Kwon, Jason Phang, Jason Teplitz, Jason\nWei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Var-\navva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui\nYu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang,\nJoaquin Quinonero Candela, Joe Beutler, Joe Lan-\nders, Joel Parish, Johannes Heidecke, John Schul-\nman, Jonathan Lachman, Jonathan McKay, Jonathan\nUesato, Jonathan Ward, Jong Wook Kim, Joost\nHuizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross,\nJosh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao,\nJoyce Lee, Juntang Zhuang, Justyn Harriman, Kai\nFricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin\nKarthik, Kayla Wood, Kendra Rimbach, Kenny Hsu,\nKenny Nguyen, Keren Gu-Lemberg, Kevin Button,\nKevin Liu, Kiel Howe, Krithika Muthukumar, Kyle\nLuther, Lama Ahmad, Larry Kai, Lauren Itow, Lau-\nren Workman, Leher Pathak, Leo Chen, Li Jing, Lia\nGuy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lil-\nian Weng, Lindsay McCallum, Lindsey Held, Long\nOuyang, Louis Feuvrier, Lu Zhang, Lukas Kon-\ndraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz,\nLyric Doshi, Mada Aflak, Maddie Simens, Madelaine\nBoyd, Madeleine Thompson, Marat Dukhan, Mark\n13\nChen, Mark Gray, Mark Hudnall, Marvin Zhang,\nMarwan Aljubeh, Mateusz Litwin, Matthew Zeng,\nMax Johnson, Maya Shetty, Mayank Gupta, Meghan\nShah, Mehmet Yatbaz, Meng Jia Yang, Mengchao\nZhong, Mia Glaese, Mianna Chen, Michael Jan-\nner, Michael Lampe, Michael Petrov, Michael Wu,\nMichele Wang, Michelle Fradin, Michelle Pokrass,\nMiguel Castro, Miguel Oom Temudo de Castro,\nMikhail Pavlov, Miles Brundage, Miles Wang, Mi-\nnal Khan, Mira Murati, Mo Bavarian, Molly Lin,\nMurat Yesildal, Nacho Soto, Natalia Gimelshein, Na-\ntalie Cone, Natalie Staudacher, Natalie Summers,\nNatan LaFontaine, Neil Chowdhury, Nick Ryder,\nNick Stathas, Nick Turley, Nik Tezak, Niko Felix,\nNithanth Kudige, Nitish Keskar, Noah Deutsch, Noel\nBundick, Nora Puckett, Ofir Nachum, Ola Okelola,\nOleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins,\nOlivier Godement, Owen Campbell-Moore, Patrick\nChao, Paul McMillan, Pavel Belov, Peng Su, Pe-\nter Bak, Peter Bakkum, Peter Deng, Peter Dolan,\nPeter Hoeschele, Peter Welinder, Phil Tillet, Philip\nPronin, Philippe Tillet, Prafulla Dhariwal, Qiming\nYuan, Rachel Dias, Rachel Lim, Rahul Arora, Ra-\njan Troll, Randall Lin, Rapha Gontijo Lopes, Raul\nPuri, Reah Miyara, Reimar Leike, Renaud Gaubert,\nReza Zamani, Ricky Wang, Rob Donnelly, Rob\nHonsby, Rocky Smith, Rohan Sahai, Rohit Ramchan-\ndani, Romain Huet, Rory Carmichael, Rowan Zellers,\nRoy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan\nCheu, Saachi Jain, Sam Altman, Sam Schoenholz,\nSam Toizer, Samuel Miserendino, Sandhini Agar-\nwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean\nGrove, Sean Metzger, Shamez Hermani, Shantanu\nJain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shi-\nrong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay,\nSrinivas Narayanan, Steve Coffey, Steve Lee, Stew-\nart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao\nXu, Tarun Gogineni, Taya Christianson, Ted Sanders,\nTejal Patwardhan, Thomas Cunninghman, Thomas\nDegry, Thomas Dimson, Thomas Raoux, Thomas\nShadwell, Tianhao Zheng, Todd Underwood, Todor\nMarkov, Toki Sherbakov, Tom Rubin, Tom Stasi,\nTomer Kaftan, Tristan Heywood, Troy Peterson, Tyce\nWalters, Tyna Eloundou, Valerie Qi, Veit Moeller,\nVinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne\nChang, Weiyi Zheng, Wenda Zhou, Wesam Manassra,\nWill Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian,\nYongjik Kim, Youlong Cheng, Yu Zhang, Yuchen\nHe, Yuchen Zhang, Yujia Jin, Yunxing Dai, and\nYury Malkov. 2024a. Gpt-4o system card. Preprint,\narXiv:2410.21276.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, Jeff Belgum, Ir-\nwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko,\nMadelaine Boyd, Anna-Luisa Brakman, Greg Brock-\nman, Tim Brooks, Miles Brundage, Kevin Button,\nTrevor Cai, Rosie Campbell, Andrew Cann, Brittany\nCarey, Chelsea Carlson, Rory Carmichael, Brooke\nChan, Che Chang, Fotis Chantzis, Derek Chen, Sully\nChen, Ruby Chen, Jason Chen, Mark Chen, Ben\nChess, Chester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing Dai,\nCory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve\nDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSimón Posada Fishman, Juston Forte, Isabella Ful-\nford, Leo Gao, Elie Georges, Christian Gibson, Vik\nGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-\nLopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin\nHu, Joost Huizinga, Shantanu Jain, Shawn Jain,\nJoanne Jang, Angela Jiang, Roger Jiang, Haozhun\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-\nwoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-\nmali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim,\nChristina Kim, Yongjik Kim, Jan Hendrik Kirch-\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\nMarkov, Yaniv Markovski, Bianca Martin, Katie\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-\nell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine B. Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng,\nPreston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\n14\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu,\nQiming Yuan, Wojciech Zaremba, Rowan Zellers,\nChong Zhang, Marvin Zhang, Shengjia Zhao, Tian-\nhao Zheng, Juntang Zhuang, William Zhuk, and Bar-\nret Zoph. 2024b. Gpt-4 technical report. Preprint,\narXiv:2303.08774.\nEllie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,\nBenjamin Van Durme, and Chris Callison-Burch.\n2015. PPDB 2.0: Better paraphrase ranking, fine-\ngrained entailment relations, word embeddings, and\nstyle classification. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 425–430, Beijing, China. Asso-\nciation for Computational Linguistics.\nAnibal Pedraza, Oscar Deniz, and Gloria Bueno.\n2022. Really natural adversarial examples. Interna-\ntional Journal of Machine Learning and Cybernetics,\n13(4):1065–1077.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,\nJiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang,\nKeming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\nLin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang\nRen, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru\nZhang, and Zihan Qiu. 2025. Qwen2.5 technical\nreport. Preprint, arXiv:2412.15115.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don‘t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversar-\nial rules for debugging NLP models. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 856–865, Melbourne, Australia. Association\nfor Computational Linguistics.\nGemma Team Morgane Riviere,\nShreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, L’eonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ram’e, Johan Ferret, Peter\nLiu, Pouya Dehghani Tafti, Abe Friesen, Michelle\nCasbon, Sabela Ramos, Ravin Kumar, Charline Le\nLan, Sammy Jerome, Anton Tsitsulin, Nino Vieil-\nlard, Piotr Sta´nczyk, Sertan Girgin, Nikola Momchev,\nMatt Hoffman, Shantanu Thakoor, Jean-Bastien Grill,\nBehnam Neyshabur, Alanna Walton, Aliaksei Sev-\neryn, Alicia Parrish, Aliya Ahmad, Allen Hutchi-\nson, Alvin Abdagic, Amanda Carl, Amy Shen, Andy\nBrock, Andy Coenen, Anthony Laforge, Antonia Pa-\nterson, Ben Bastian, Bilal Piot, Boxi Wu, Brandon\nRoyal, Charlie Chen, Chintu Kumar, Chris Perry,\nChristoper A. Welty, Christopher A. Choquette-\nChoo, Danila Sinopalnikov, David Weinberger, Dim-\nple Vijaykumar, Dominika Rogozi’nska, D. Herbi-\nson, Elisa Bandy, Emma Wang, Eric Noland, Erica\nMoreira, Evan Senter, Evgenii Eltyshev, Francesco\nVisin, Gabriel Rasskin, Gary Wei, Glenn Cameron,\nGus Martins, Hadi Hashemi, Hanna Klimczak-\nPluci’nska, Harleen Batra, Harsh Dhand, Ivan Nar-\ndini, Jacinda Mein, Jack Zhou, James Svensson,\nJeff Stanway, Jetha Chan, Jin Zhou, Joana Car-\nrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernan-\ndez, Joost R. van Amersfoort, Josh Gordon, Josh\nLipschultz, Joshua Newlan, Junsong Ji, Kareem Mo-\nhamed, Kartikeya Badola, Kat Black, Katie Mil-\nlican, Keelin McDonell, Kelvin Nguyen, Kiranbir\nSodhia, Kish Greene, Lars Lowe Sjoesund, Lau-\nren Usui, L. Sifre, Lena Heuermann, Leti cia Lago,\nLilly McNealus, Livio Baldini Soares, Logan Kil-\npatrick, Lucas Dixon, Luciano Martins, Machel Reid,\nManvinder Singh, Mark Iverson, Martin Gorner,\nMat Velloso, Mateo Wirth, Matt Davidow, Matt\nMiller, Matthew Rahtz, Matthew Watson, Meg Ris-\ndal, Mehran Kazemi, Michael Moynihan, Ming\nZhang, Minsuk Kahng, Minwoo Park, Mofi Rah-\nman, Mohit Khatwani, Natalie Dao, Nenshad Bar-\ndoliwalla, Nesh Devanathan, Neta Dumai, Nilay\nChauhan, Oscar Wahltinez, Pankil Botarda, Parker\nBarnes, Paul Barham, Paul Michel, Peng chong\nJin, Petko Georgiev, Phil Culliton, Pradeep Kup-\npala, Ramona Comanescu, Ramona Merhej, Reena\nJana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan\nMullins, Samaneh Saadat, Sara Mc Carthy, Sarah\nPerrin, S’ebastien M. R. Arnold, Se bastian Krause,\nShengyang Dai, Shruti Garg, Shruti Sheth, Sue\nRonstrom, Susan Chan, Timothy Jordan, Ting Yu,\nTom Eccles, Tom Hennigan, Tomás Kociský, Tulsee\nDoshi, Vihan Jain, Vikas Yadav, Vilobh Meshram,\nVishal Dharmadhikari, Warren Barkley, Wei Wei,\nWenming Ye, Woohyun Han, Woosuk Kwon, Xi-\nang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Vic-\ntor Cotruta, Phoebe Kirk, Anand Rao, Minh Gi-\n15\nang, Ludovic Peran, Tris Warkentin, Eli Collins,\nJoelle Barral, Zoubin Ghahramani, Raia Hadsell,\nD. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov,\nOriol Vinyals, Jeffrey Dean, Demis Hassabis, Koray\nKavukcuoglu, Clément Farabet, Elena Buchatskaya,\nSebastian Borgeaud, Noah Fiedel, Armand Joulin,\nKathleen Kenealy, Robert Dadashi, and Alek An-\ndreev. 2024. Gemma 2: Improving open language\nmodels at a practical size. ArXiv, abs/2408.00118.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. In 5th\nWorkshop on Energy Efficient Machine Learning and\nCognitive Computing @ NeurIPS 2019.\nViktor Schlegel, Goran Nenadic, and Riza Batista-\nNavarro. 2021.\nSemantics altering modifications\nfor evaluating comprehension in machine reading.\nProceedings of the AAAI Conference on Artificial\nIntelligence, 35(15):13762–13770.\nViktor Schlegel, Goran Nenadic, and Riza Batista-\nNavarro. 2023. A survey of methods for revealing\nand overcoming weaknesses of data-driven natural\nlanguage understanding. Natural Language Engi-\nneering, 29(1):1–31.\nViktor Schlegel, Marco Valentino, Andre Freitas, Goran\nNenadic, and Riza Batista-Navarro. 2020. A frame-\nwork for evaluation of machine reading comprehen-\nsion gold standards. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference,\npages 5359–5369, Marseille, France. European Lan-\nguage Resources Association.\nChenglei Si, Ziqing Yang, Yiming Cui, Wentao Ma,\nTing Liu, and Shijin Wang. 2021. Benchmarking ro-\nbustness of machine reading comprehension models.\nIn Findings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 634–644, Online.\nAssociation for Computational Linguistics.\nSamson Tan and Shafiq Joty. 2021. Code-mixing on\nsesame street: Dawn of the adversarial polyglots. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3596–3616, Online. Association for Computa-\ntional Linguistics.\nSamson Tan, Shafiq Joty, Min-Yen Kan, and Richard\nSocher. 2020. It‘s morphin’ time! Combating lin-\nguistic discrimination with inflectional perturbations.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 2920–\n2935, Online. Association for Computational Lin-\nguistics.\nFalcon-LLM Team. 2024. The falcon 3 family of open\nmodels.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. Preprint, arXiv:2307.09288.\nSon Quoc Tran, Phong Nguyen-Thuan Do, Uyen Le,\nand Matt Kretchmar. 2023. The impacts of unanswer-\nable questions on the robustness of machine reading\ncomprehension models. In Proceedings of the 17th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 1543–\n1557, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJohannes Welbl, Pasquale Minervini, Max Bartolo, Pon-\ntus Stenetorp, and Sebastian Riedel. 2020. Under-\nsensitivity in neural reading comprehension. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1152–1165, Online. Association\nfor Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nWinston Wu, Dustin Arendt, and Svitlana Volkova.\n2021. Evaluating neural model robustness for ma-\nchine comprehension. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 2470–2481, Online. Association for Computa-\ntional Linguistics.\nYulong Wu, Viktor Schlegel, and Riza Batista-Navarro.\n2023. Are machine reading comprehension systems\n16\nrobust to context paraphrasing? In Proceedings of\nthe 13th International Joint Conference on Natural\nLanguage Processing and the 3rd Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n184–196, Nusa Dua, Bali. Association for Computa-\ntional Linguistics.\nJun Yan, Yang Xiao, Sagnik Mukherjee, Bill Yuchen\nLin, Robin Jia, and Xiang Ren. 2022. On the ro-\nbustness of reading comprehension models to entity\nrenaming. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 508–520, Seattle, United States.\nAssociation for Computational Linguistics.\nDiyi Yang, Aaron Halfaker, Robert Kraut, and Eduard\nHovy. 2017. Identifying semantic edit intentions\nfrom revisions in Wikipedia. In Proceedings of the\n2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2000–2010, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nTorsten Zesch. 2012. Measuring contextual fitness us-\ning error contexts extracted from the Wikipedia re-\nvision history. In Proceedings of the 13th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics, pages 529–538, Avignon,\nFrance. Association for Computational Linguistics.\nA\nEncoder-only Model Parameters and\nHyperparameters for Fine-tuning\nTable 5 shows the hyperparameters used to fine-\ntune the pre-trained encoder-only MRC models\nin this work and their number of parameters con-\ntained.\nB\nInstruction Templates for Flan-T5\nEvaluation\nIn Table 6, we present the instruction templates\nemployed in constructing the inputs to the Flan-T5\nmodel for the SQUAD 1.1 format and SQUAD\n2.0 format test sets, respectively.\nC\nMRC Prompts\nWe use the following zero-shot prompts to instruct\nthe decoder-only models to generate responses in\nthe task of MRC.\nModelParameters(M)\nd\nb\nlr\nep\nDistilBERT(66)\n384\n8\n3e −5\n3\nBERT(110/340)\n384\n8\n3e −5\n2\nSpanBERT(110/340)\n512\n4\n2e −5\n4\nRoBERTa(125/355)\n384\n8\n3e −5\n2\nALBERT(11/17/223)\n384\n4\n3e −5\n2\nDeBERTa(350)\n384\n4\n3e −6\n3\nTable 5: Number of parameters in each type of pre-\ntrained encoder-only MRC model and the hyperpa-\nrameters used to fine-tune them. For BERT, SpanBERT,\nRoBERTa and ALBERT, we show the number of model\nparameters in the order of base, large and xxlarge (if\napplicable) version. d is the size of the token sequence\nfed into the model, b is the training batch size, lr is the\nlearning rate, and ep is the number of training epochs.\nWe used stride = 128 for documents longer than d to-\nkens.\nSQUAD 1.1 & TYDI QA & D(BIDAF) &\nD(BERT) & D(ROBERTA): Use the provided\narticle delimited by triple quotes to answer ques-\ntion. Provide only the shortest continuous span\nfrom the context without any additional expla-\nnation.\\n\\n“““{context}\"\"\"\\n\\nQuestion: {ques-\ntion}\nSQUAD 2.0: Use the provided article delim-\nited by triple quotes to answer question.\nPro-\nvide only the shortest continuous span from the\ncontext without any additional explanation.\nIf\nthe question is unanswerable, return “unanswer-\nable\".\\n\\n“““{context}\"\"\"\\n\\nQuestion: {ques-\ntion}\nDROP & HOTPOTQA: Use the provided arti-\ncle delimited by triple quotes to answer question.\nProvide only the answer without any additional\nexplanation.\\n\\n“““{context}\"\"\"\\n\\nQuestion:\n{question}\nBOOLQ:\nUse\nthe\nprovided\narti-\ncle\ndelimited\nby\ntriple\nquotes\nto\nan-\nswer\nquestion.\nReturn\nonly\nTRUE\nor\nFALSE.\\n\\n“““{context}\"\"\"\\n\\nQuestion:\n{question}\nD\nIndicators of Unanswerable\nWe manually identify a set of phrases contained\nin the output of LLMs that indicate the unanswer-\nability of the question, including “I cannot answer\nthis/the question”, “unanswerable”, “There is no\nindication in the provided article”, “The context\nprovided does not provide enough information”,\n17\nSQUAD 1.1\n1 “Read\nthis\nand\nanswer\nthe\nques-\ntion\\n\\n{context}\\n\\n{question}”\n2 “{context}\\n{question}”\n3 “Answer\na\nquestion\nabout\nthis\narti-\ncle:\\n{context}\\n{question}”\n4 “Here is a question about this article: {con-\ntext}\\nWhat is the answer to this question:\n{question}”\n5 “Article:\n{context}\\n\\nQuestion:\n{ques-\ntion}”\n6 “Article: {context}\\n\\nNow answer this ques-\ntion: {question}”\nSQUAD 2.0\n1 “Read this and answer the question. If the\nquestion is unanswerable, say \\“unanswer-\nable\\\".\\n\\n{context}\\n\\n{question}”\n2 “{context}\\n{question} (If the question is\nunanswerable, say \\“unanswerable\\\")”\n3 “{context}\\nTry to answer this question if\npossible (otherwise reply \\“unanswerable\\\"):\n{question}”\n4 “{context}\\nIf it is possible to answer this\nquestion, answer it for me (else, reply \\“unan-\nswerable\\\"): {question}”\n5 “{context}\\n\\nAnswer this question, if pos-\nsible (if impossible, reply \\“unanswerable\\\"):\n{question}”\n6 “Read this: {context}\\nNow answer this ques-\ntion, if there is an answer (If it cannot be an-\nswered, return \\“unanswerable\\\"): {question}”\nTable 6: Various instruction templates for Flan-T5\nmodel evaluation.\n“There is no reference in the given article”, “The an-\nswer to the question is not provided in the given ar-\nticle”, “it is not possible”, “question cannot be an-\nswered” and “context/question/article/text/article\nprovided/passage does not”.\nE\nImpact of the Complete Set of\nPerturbed Instances on\nEncoder-Decoder and Decoder-Only\nArchitectures\nWe supplement Table 1 in Section 5.1 with addi-\ntional experiments on Flan-T5 and some SOTA\nLLMs such as Gemma 2 (Riviere et al., 2024) and\nLlama 3.2, to study the effect of all perturbed in-\nstances on these two architecture types (in addition\nto the encoder-only one). The results are presented\nin Table 7. From Table 7, we can see that simi-\nlar to encoder-only models, Flan-T5 and LLMs\ngenerally exhibit varying degrees of performance\ndegradation under natural perturbations, but also\nexhibit considerable robustness.\nVictim\nSQUAD 1.1\nSQUAD 2.0\nflan-t5-small\n−0.69\n−0.64\nflan-t5-base\n−0.91\n−1.32\nflan-t5-large\n−0.77\n−1.13\nflan-t5-xl\n−0.98\n−1.37\nGemma 2-2b-IT\n−\n−0.76\nGemma 2-9b-IT\n−0.89\n−0.92\nLlama-3.1-8B-Instruct\n−0.38\n0.39\nLlama-3.2-3B-Instruct\n−0.96\n−0.37\nMistral-7B-Instruct-v0.2\n0.39\n−1.28\nFalcon-7B-Instruct\n−0.88\n−5.38\nFalcon-40B-Instruct\n−0.80\n−\nTable 7: Performance change (%) for Flan-T5 and\nLLMs subjecting to natural perturbations.\nWe then measure the transferability of adversar-\nial examples across all the evaluated model archi-\ntectures and observe that these models share simi-\nlar error patterns, with LLMs (especially Falcon)\nshowing moderate differences. However, the low-\nest transferability metric is still as high as 0.86.\nF\nHuman Annotation Instructions\nIn Figure 5, we show the instructions given to hu-\nman annotators for error analysis (Section 5.2) and\nadversarial validity checking (Section 5.3), respec-\ntively. All our human annotators are university\nstudents in the United Kingdom and China. Be-\nfore commencing each task, we ask the annotators\nto annotate some examples and report the average\ntime spent on each. As compensation, annotators\nreceive 40 pence for each annotated example.\nG\nDemonstration of Perturbed MRC\nExamples for Encoder-only Models\nFigure 6 illustrates a naturally perturbed MRC in-\nstance each for categories C2W, C2C, and W2C,\nwith the annotated perturbation type(s).\nH\nProcess of Adversarial Validity\nVerification\nWe first present two human annotators with the\nsame collection of adversarial instances, which\nincludes only perturbed contexts and their corre-\nsponding questions, and then ask them to answer\nthe question based on the perturbed context. The\n18\nError Analysis\nYou will be presented with pairs of reading\ncontexts and their modified versions. The task\nis to compare each context and its modified\nversion, observe the changes made and classify\nthem into one or more of the semantic edit\nintention categories detailed below:\n• Copy Editing: Rephrase; improve\ngrammar, spelling, tone, or punctuation\n• Clarification: Specify or explain an\nexisting fact or meaning by example or\ndiscussion without adding new\ninformation\n• Elaboration: Extend/add new content;\ninsert a fact or new meaningful assertion\n• Fact Update: Update numbers, dates,\nscores, episodes, status, etc. based on\nnewly available information\n• Refactoring: Restructure the article; move\nand rewrite content, without changing the\nmeaning of it\n• Simplification: Reduce the complexity or\nbreadth of discussion; may remove\ninformation\n• Vandalism: Deliberately attempt to\ndamage the article\n• Other: None of the above\nWe will use your annotation to calculate the\npercentage of each edit category.\nAdversarial Validity Checking\nPlease read each provided context carefully and\nanswer a corresponding question. Select the\nshortest continuous span from the context as\nyour answer. If you believe a question cannot\nbe answered, leave the answer blank. Your\nanswer will be compared with the ground truth\nanswers, and the result will only be used to\ndecide the human answerability of the question.\nFigure 5: Instructions for the two distinct human annota-\ntion tasks. In the error analysis task, the eight semantic\nedit intentions are adopted from (Yang et al., 2017).\nannotators are required to select the shortest con-\ntinuous span in the perturbed context that answers\nthe question and are allowed to leave the answer\nblank if they are confident that the question is not\nanswerable. Full instructions given to the anno-\ntators can be seen in Appendix F. Subsequently,\nfor both annotators, we measure the correctness\n(1 or 0) of their provided answers by comparing\neach of them with the corresponding ground truth\nanswers8. The inter-annotator agreement is then\nmeasured by computing the Cohen’s κ coefficient\n(Cohen, 1960). We then involve a third human\nannotator to annotate the adversarial examples on\nwhich the first two annotators disagree and then\ntake the majority label as ground truth.\nI\nExhaustive Search Algorithm for\nChallenging Test Set Construction\nWe propose an exhaustive search algorithm that\nleverages the predictions of all encoder-only mod-\nels to create the challenging natural perturbed test\nset. In detailed terms, for each matched reading pas-\nsage from the prior version and its counterpart from\nthe current version, we determine which should be\ndesignated as the original and which as the per-\nturbed based on which scenario can yield the ques-\ntions on which the maximum sum of the number\nof encoder-only models demonstrates the lack of\nrobustness phenomenon9. To be specific:\nGiven a matched reading passage ( P ) from the\nprior version, its counterpart ( P’ ) from the current\nversion, and the associated questions:\nFirst Scenario: We treat ( P ) as the original\npassage and ( P’ ) as the perturbed one. We then\nevaluate, for each associated question, how many\nencoder-only models demonstrate the lack of ro-\nbustness phenomenon, i.e., succeed on ( P ) but\nfail on ( P’ ). We finally obtain the total number\nof models that demonstrate the lack of robustness\nphenomenon across all questions, denoted as ( N\n). Questions on which none of the models demon-\nstrate the lack of robustness phenomenon are re-\nmoved, leaving ( Q ) questions.\nSecond Scenario: We treat ( P’ ) as the original\npassage and ( P ) as the perturbed one. We then re-\npeat the same evaluation process as described in the\nfirst scenario and obtain the total number of models\ndemonstrating the lack of robustness phenomenon\n8Here, as long as one of the ground truth answers is in-\ncluded in the human-provided answer span, we consider the\nprediction to be correct.\n9We define A model as lacking robustness to the perturba-\ntion if it achieves 1 EM on the original question but attains less\nthan 0.4 F1 on the perturbed one (for answerable questions).\n19\nCategory: C2W\nOriginal Paragraph: Jacksonville, like most large cities in the United States, suffered from negative\neffects of rapid urban sprawl after World War II. The construction of highways led residents to move to\nnewer housing in the suburbs. After World War II, the government of the city of Jacksonville began to\nincrease spending to fund new public building projects in the boom that occurred after the war. [... ]\nPerturbed Paragraph: Jacksonville, like most large cities in the United States, suffered from negative\neffects of rapid urban sprawl after World War V. The construction of highways led residents to move to\nnewer housing in the suburbs. After World War II, the government of the city of Jacksonville began to\nincrease spending to fund new public building projects in the boom that occurred after the war. [... ]\nQuestion: What did Jacksonville suffer from following World War I?\nPrediction of distilbert-base and spanbert-large-cased: unanswerable→rapid urban sprawl\nAnnotated Natural Perturbation Type: Vandalism\nCategory: C2C\nOriginal Paragraph: Construction projects can suffer from preventable financial problems.\nUnderbids happen when builders ask for too little money to complete the project. Cash flow problems\nexist when the present amount of funding cannot cover the current costs for labour and materials, and\nbecause they are a matter of having sufficient funds at a specific time, can arise even when the overall\ntotal is enough. Fraud is a problem in many fields, but is notoriously prevalent in the construction field.\nFinancial planning for the project is intended to ensure that a solid plan with adequate safeguards and\ncontingency plans are in place before the project is started and is required to ensure that the plan is\nproperly executed over the life of the project.\nPerturbed Paragraph: Financial planning ensures adequate safeguards and contingency plans are in\nplace before the project is started, and ensures that the plan is properly executed over the life of the\nproject. Construction projects can suffer from preventable financial problems. Underbids happen when\nbuilders ask for too little money to complete the project. Cash flow problems exist when the present\namount of funding cannot cover the current costs for labour and materials; such problems may arise\neven when the overall budget is adequate, presenting a temporary issue. Fraud is also an occasional\nconstruction issue.\nQuestion: What can construction projects suffer from?\nPrediction of all encoder-only models: preventable financial problems→preventable financial\nproblems\nAnnotated Natural Perturbation Type: Copy Editing; Refactoring; Simplification\nCategory: W2C\nOriginal Paragraph: [. . . ] The antigens expressed by tumors have several sources; some are derived\nfrom oncogenic viruses like human papillomavirus, which causes cervical cancer, while others are the\norganism’s own proteins that occur at low levels in normal cells but reach high levels in tumor cells.\n[. . . ] A third possible source of tumor antigens are proteins normally important for regulating cell\ngrowth and survival, that commonly mutate into cancer inducing molecules called oncogenes.\nPerturbed Paragraph: [. . . ] The antigens expressed by tumors have several sources; some are\nderived from oncogenic viruses like human papillomavirus, which causes cancer of the cervix, vulva,\nvagina, penis, anus, mouth, and throat,while others are the organism’s own proteins that occur at low\nlevels in normal cells but reach high levels in tumor cells. [...] A third possible source of tumor\nantigens are proteins normally important for regulating cell growth and survival, that commonly\nmutate into cancer inducing molecules called oncogenes.\nQuestion: What is a fourth possible source for tumor antigens?\nPrediction of bert-base-uncased: proteins normally important for regulating cell growth and\nsurvival→unanswerable\nAnnotated Natural Perturbation Type: Elaboration\nFigure 6: Natural perturbed MRC example in C2W, C2C and W2C categories.\n20\nacross all questions, denoted as ( N’ ). Questions\non which none of the models demonstrate the lack\nof robustness phenomenon are removed as well,\nleaving ( Q’ ) questions.\nIf ( N > N′ ), we consider ( P ) as the original\npassage and ( P’ ) as the perturbed version.\nIf ( N < N′ ), we consider ( P’ ) as the original\nand ( P ) as the perturbed.\nIf ( N = N′ ), we compare ( Q ) and ( Q’ ):\n• If ( Q > Q′ ), we consider ( P ) as the original\npassage and ( P’ ) as the perturbed version.\n• If ( Q < Q′ ), we consider ( P’ ) as the original\nand ( P ) as the perturbed.\n• If ( Q = Q′ ), the order does not matter, and\nwe randomly decide which one should be the\noriginal and which should be the perturbed.\nWe finally process the identified original and\nperturbed passage pairs to ensure that the original\npassages are within the original SQUAD 1.1 de-\nvelopment set. For those original passages with\nmultiple occurrences, we select the one with the\nmaximum number of questions reserved.\nJ\nNatural Adversarial Samples for LLMs\nWe demonstrate two naturally perturbed reading\ncomprehension examples that pose challenges for\nLLMs in Figure 7.\nK\nSynthetic Perturbation Methods\nTable 8 presents the synthetic perturbation methods\nused in this study.\nWe employ methods including WSplit, WSyn-\nSub and WInsert (WE) to each sentence in the\noriginal reading passage, and then recombine the\nmodified sentences to generate the perturbed ver-\nsion. Conversely, other perturbation approaches\nare directly executed on the entire paragraph, as im-\nplementing them at the sentence-level might result\nin perturbed text that is even difficult for humans\nto read and comprehend (Si et al., 2021). The im-\nplementation of all character-level and word-level\nmethods is carried out using the NLPAug library\n(Ma, 2019). Moreover, we set the perturbation rate\nto 30%, in line with the default settings within the\nNLPAug library.\nL\nImpact of Synthetic Adversarial\nTraining\nFigure 8 describes the impact of synthetic adver-\nsarial training (for deberta-large) on handling\nnatural and synthetic perturbations.\n21\nNAT_V1_CHALLENGE\nOriginal Paragraph: In business, notable alumni include Microsoft CEO Satya Nadella, Oracle\nCorporation founder and the third richest man in America Larry Ellison, Goldman Sachs and MF\nGlobal CEO as well as former Governor of New Jersey Jon Corzine, McKinsey & Company founder\nand author of the first management accounting textbook James O. McKinsey, Arley D. Cathey,\nBloomberg L.P. CEO Daniel Doctoroff, Credit Suisse CEO Brady Dougan, Morningstar, Inc. founder\nand CEO Joe Mansueto, Chicago Cubs owner and chairman Thomas S. Ricketts, and NBA\ncommissioner Adam Silver.\nPerturbed Paragraph: In business, notable alumni include Microsoft CEO Satya Nadella, Oracle\nCorporation founder and the third richest man in America Larry Ellison, Goldman Sachs and MF\nGlobal CEO as well as former Governor of New Jersey Jon Corzine, McKinsey & Company founder\nand author of the first management accounting textbook James O. McKinsey, co-founder of the\nBlackstone Group Peter G. Peterson, co-founder of AQR Capital Management Cliff Asness, founder of\nDimensional Fund Advisors David Booth, founder of The Carlyle Group David Rubenstein, Lazard\nCEO Ken Jacobs, entrepreneur David O. Sacks, CEO of TPG Group and former COO of Goldman\nSachs Jon Winkelreid, former COO of Goldman Sachs Andrew Alper, billionaire investor and founder\nof Oaktree Capital Management Howard Marks, Bloomberg L.P. CEO Daniel Doctoroff, Credit Suisse\nCEO Brady Dougan, Morningstar, Inc. founder and CEO Joe Mansueto, Chicago Cubs owner and\nchairman Thomas S. Ricketts, and NBA commissioner Adam Silver.\nQuestion: What Goldman Sachs CEO is also an alumni of the University of Chicago?\nPrediction of GPT-3.5-turbo-0125 and Llama-3-8B-Instruct: Jon Corzine→Jon Winkelreid\nPrediction of Falcon-40B-Instruct: Jon Corzine→David Rubenstein, co-founder of The Carlyle\nGroup, is also an alumnus of the University of Chicago.\nNAT_V2_CHALLENGE\nOriginal Paragraph: Each chapter has a number of authors who are responsible for writing and\nediting the material. A chapter typically has two \"coordinating lead authors\", ten to fifteen \"lead\nauthors\", and a somewhat larger number of \"contributing authors\". The coordinating lead authors are\nresponsible for assembling the contributions of the other authors, ensuring that they meet stylistic and\nformatting requirements, and reporting to the Working Group chairs. Lead authors are responsible for\nwriting sections of chapters. Contributing authors prepare text, graphs or data for inclusion by the\nlead authors.\nPerturbed Paragraph: Each chapter has a number of authors to write and edit the material. A typical\nchapter has two coordinating lead authors, ten to fifteen lead authors and a larger number of\ncontributing authors. The coordinating lead authors assemble the contributions of the other authors.\nThey ensure that contributions meet stylistic and formatting requirements. They report to the Working\nGroup co-chairs. Lead authors write sections of chapters. They invite contributing authors to prepare\ntext, graphs or data for inclusion.\nQuestion: Who has the responsibility for publishing materials?\nPrediction of Mistral-7B-Instruct-v0.2: Unanswerable. The text does not mention any\nresponsibility related to publishing materials.→The coordinating lead authors are responsible for\npublishing materials in the given context.\nFigure 7: Natural perturbed MRC examples that confuse LLMs.\n22\nMethod\nDescription\ncharacter-level\nCharOCR\nReplace characters with Optical Character Recognition (OCR) errors.\nCharInsert\nInject new characters randomly.\nCharSubstitute\nSubstitute original characters randomly.\nCharSwapMid\nSwap adjacent characters within words randomly, excluding the first and\nlast character.\nCharSwapRand\nSwap characters randomly without constraint.\nword-level\nWInsert (CWE)\nInsert new words to random position according to contextual word embed-\ndings calculation from RoBERTa-base (Liu et al., 2019).\nWSubstitute (CWE)\nSubstitute words according to contextual word embeddings calculation\nfrom RoBERTa-base (Liu et al., 2019).\nWSplit\nSplit words to two tokens randomly.\nWSwap\nSwap adjacent words randomly.\nWDelete\nDelete words randomly.\nWCrop\nRemove a set of continuous word randomly.\nWord Synonym Sub-\nstitution (WSynSub)\nSubstitute words with synonyms from large size English PPDB (Pavlick\net al., 2015).\nWInsert (WE)\nInsert new words to random position according to GloVe (Pennington et al.,\n2014) word embeddings calculation (we use glove.6B.300d.txt).\nTable 8: Various synthetic perturbation approaches.\nFigure 8: Absolute changes in original and perturbed performance (F1), as well as the robustness of deberta-large\nunder natural and various synthetic noises, following retraining with each synthetic perturbation. The upper row and\nthe bottom row illustrate the results on the SQUAD 1.1 and SQUAD 2.0 format test sets, respectively.\n23\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2025-02-23",
  "updated": "2025-02-23"
}