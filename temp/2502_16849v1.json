{
  "id": "http://arxiv.org/abs/2502.16849v1",
  "title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models",
  "authors": [
    "Taj Jones-McCormick",
    "Aukosh Jagannath",
    "Subhabrata Sen"
  ],
  "abstract": "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.",
  "text": "Provable Benefits of Unsupervised Pre-training and Transfer\nLearning via Single-Index Models\nTaj Jones-McCormick∗, Aukosh Jagannath†, & Subhabrata Sen‡\nFebruary 25, 2025\nAbstract\nUnsupervised pre-training and transfer learning are commonly used techniques to initialize training\nalgorithms for neural networks, particularly in settings with limited labeled data. In this paper, we study\nthe effects of unsupervised pre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a single-layer neural network via\nonline stochastic gradient descent. We establish that pre-training and transfer learning (under concept\nshift) reduce sample complexity by polynomial factors (in the dimension) under very general assumptions.\nWe also uncover some surprising settings where pre-training grants exponential improvement over random\ninitialization in terms of sample complexity.\n1\nIntroduction\nThe canonical pipeline of modern supervised learning is as follows: given supervised data, (i) choose an\nappropriate model/estimator (usually specified by a deep neural network), (ii) choose a loss function and set\nup a suitable empirical risk minimization problem, and (iii) minimize this (possibly non-convex) empirical\nrisk using stochastic gradient descent (SGD). Extensions of this “basic” approach have been successfully\ndeployed to train state-of-the-art models in diverse domains. As deep learning models become larger and\nmore complex, one has to wrestle with the issue of model weight initialization during training. Without\nadditional information, one usually resorts to random initialization. However, access to additional data opens\nthe door to other avenues for initializing model weights.\nA prominent setting with additional data is semi-supervised learning, where one might have an abundance\nof unlabeled data. Unsupervised pre-training has emerged as a popular strategy in this context (Devlin\n[2018], Brown [2020]). The core idea behind pre-training is to train a model (on the unlabeled data) on a\ntask that is related or, even better, a necessary precursor to the supervised task of interest. Initializing from\na pre-trained model, the hope is that the model will have already learned useful features from the unlabeled\ndata and hence solve the supervised task with reduced sample complexity. The model trained in this way can\nthen be used to initialize a model for the supervised task in various ways e.g., by removing the final layer of\nthe network and replacing it with an output head relevant to the labeled task, such as a classification head.\nAnother prominent setup with additional data is transfer learning Pan and Yang [2009], where one has\naccess to samples from related supervised tasks. In this case, a natural idea is to initialize the model weights\non the target dataset using the model trained on the upstream task.\nThe precise scheme for unsupervised pre-training or transfer learning can vary significantly across\napplications. For example, BERT models employ self-supervised representation learning by predicting\n“masked” tokens based on the observed ones Devlin [2018]. Similar self-supervised pre-training algorithms\nform core components in the training of modern language models such as GPT (Radford et al. [2019], Brown\n∗Department of Statistics and Actuarial Science, University of Waterloo, Canada. Equal contribution.\n†Department of Statistics and Actuarial Science, University of Waterloo, Canada; Cheriton School of Computer Science,\nUniversity of Waterloo, Canada. Equal contribution.\n‡Department of Statistics, Harvard University, United States of America. Equal contribution.\n1\narXiv:2502.16849v1  [stat.ML]  24 Feb 2025\n[2020], Achiam et al. [2023]) and have attracted widespread attention recently. Many other forms of both\npre-training and transfer leaning are applied in countless works spanning many different fields within machine\nlearning (Wang et al. [2016] , He et al. [2017], Devlin [2018], Hagos and Kant [2019], Schneider et al. [2019]).\nTheir popularity underscores the importance of understanding the effects of these different methods of weight\ninitialization on supervised tasks.\nThe main goal of this work is to build towards a theoretical understanding of the benefits of pre-training,\nparticularly in terms of the effect on sample complexity for solving supervised learning tasks in high dimension\nthat involve optimizing non-convex losses. In general, characterizing the performance of neural networks on\nsupervised learning tasks is a challenging problem. Recent works focus on specific classes of problems such\nas single-index models learned with single-layer networks Ben Arous et al. [2021] or two-layer networks Lee\net al. [2024], and characterize the number of samples required for recovery of the latent signal. We study the\neffects of distinct initializations on the sample complexity for a closely related class of problems. Specifically,\nwe show provable benefits of pre-training and transfer learning in terms of reducing sample complexity for\nsingle-layer networks. We also highlight surprising complexities and powerful benefits of pre-training—we\ndiscover simple scenarios under which one cannot hope to succeed with random initialization, but the problem\ncan be solved easily with suitable pre-training. Finally, we demonstrate our findings empirically in finite\ndimensional settings with simulations.\n2\nRelated Work\nWe summarize some related works in this section, and compare these prior works with the contributions in\nthis article.\n2.1\nPre-training and Transfer Learning Theory\nThere has been significant recent progress in understanding the benefits of distinct unsupervised pre-training\nmethods. In Lee et al. [2021], the authors provide rigorous evidence of the benefits of self-supervised pre-\ntraining (SSL). They explain the benefits of SSL via specific conditional independence relations between\ntwo sets of observed features, given the response. In a related direction, Arora et al. [2019], Tosh et al.\n[2021a,b] examine the benefits of contrastive pretraining, while Zhai et al. [2023] examines the effects of\naugmentation-based self-supervised representation learning. In Wei et al. [2021], the authors explore the\nbenefits pre-trained language models, while Zhang and Hashimoto [2021], explores the inductive bias of\nmasked language modeling by connecting to the statistical literature on learning graphical models. Finally, we\nhighlight the work Azar and Nadler [2024], which exhibits provable computational benefits of semi-supervised\nlearning under the low-degree likelihood hardness conjecture.\nThe paucity of high-quality labeled data has directly motivated inquiries into the properties of transfer\nlearning across diverse application domains. The recent literature focuses on several distinct notions of transfer\nlearning (e.g., covariate shift Heckman [1979], Huang et al. [2006], Shimodaira [2000], model shift Wang and\nSchneider [2015], Wang et al. [2014], target shift Maity et al. [2022], conditional shift Qui˜nonero-Candela et al.\n[2022], Storkey [2008] etc) and develops distinct rigorous methods to ensure successful knowledge transfer in\nthese settings (see Shimodaira [2000], Wu et al. [2019], Sagawa et al. [2019], Ganin et al. [2016], Long et al.\n[2017] and the references therein for an incomplete list). From a learning theoretic perspective, recent works\nstudy the generalization performance as a function of the discrepancy between the source and the target\ndomains Albuquerque et al. [2019], Ben-David et al. [2010], David et al. [2010], Hanneke and Kpotufe [2019],\nTachet des Combes et al. [2020], Zhao et al. [2019].\nIn Damian et al. [2022], the authors study the benefits of transfer learning in the setting of single/multi-\nindex models. They keep the representation fixed across the source and target, and vary the link function\nacross the two tasks. In contrast, we keep the link function constant (and assume that the link is known),\nand study settings with distinct (but correlated) representations in the source and target tasks.\n2.2\nUnderstanding Sample Complexity for single-index models\nSingle-index models have emerged as popular toy-models for understanding the sample complexity of training\nof neural networks. This is due to the fact that they are both high-dimensional and non-convex. From a\n2\nstatistical perspective there has been work on the fundamental thresholds of inference in these problems\nBarbier et al. [2019], Maillard et al. [2020] and its landscape geometry Sun et al. [2018], Maillard et al. [2019],\nDudeja and Hsu [2018]. From the perspective of sample complexity, a substantial amount of deep work in\nthis direction focused on the sample complexity of spectral methods or related algorithms, particularly in\nrelation to the Phase Retrieval problem, Candes et al. [2015], Barbier et al. [2019], Lu and Li [2020].\nMore recently there have been tight analyses of the sample complexity for online stochastic gradient\ndescent from random initialization. In particular, it was shown in Ben Arous et al. [2021] that the sample\ncomplexity in the online setting is characterized by the Information Exponent. Since then there has been a\ntremendous body of work around complexity exponents, such as the Information Exponent, Leap Exponent\nAbbe et al. [2023], or Generative Exponent Damian et al. [2024b]. In particular, these exponents have enabled\nstudies which contrast the performance of various learning paradigms such as Correlational Statistical Query\n(CSQ) versus Statistical Query (SQ) bounds Damian et al. [2024b], feature learning versus kernel methods\nBa et al. [2024], better choices of loss function Damian et al. [2024a], and the importance of data reuse Dandi\net al. [2024], Lee et al. [2024]. We note here that there has been quite a lot of recent important work on the\ncase of multi-index models which we do not explore here, see, e.g., Abbe et al. [2023], Bietti et al. [2023], Ren\nand Lee [2024] for a small selection of this rich literature.\nTo our knowledge, most of this work has focused largely on the setting of isotropic Gaussian features\n(though note Zweig et al. [2024] for work on universality). However, given that pre-training only has access to\nthe features, one requires that the features have some correlation with the underlying spike. Inspired by the\nrecent works of Mousavi-Hosseini et al. [2023], Ba et al. [2024], we model this via a spiked covariance model.\n3\nPre-Training\n3.1\nProblem Set Up and Notation\nWe consider a single layer supervised network with specified activation function f. We consider Gaussian\nfeatures with spiked covariance, where the spike is correlated with the parameter vector of interest.\nLet the labeled data be (yi, ai)N\ni=1, with each (yi, ai) independent and identically distributed. We have the\nfollowing relationship between a and y: yi = f(ai · v0) + ϵi, for some ϵi independent of ai, with mean 0 and\nfinite fifth moment. The parameter vector we wish to estimate is v0 and f is a known activation function.\nThroughout we assume that f is twice differentiable almost everywhere with f, f ′, f ′′ of at most polynomial\ngrowth. We would like the model we consider to capture the essence of pre-training. To perform pre-training,\none should have access to additional unlabeled data. We thus assume access to some unlabeled (a′\ni)N′\ni=1 with\na′\ni\nD= ai. In order for pre-training to be useful, there is an implicit assumption that the unlabeled data contains\nsome information in its structure that is related to the supervised task. We thus let ai ∼N(0, Id + λvvT ),\nwith v · v0 = η1 ∈[0, 1] and v, v0 ∈Sd−1 (the unit sphere in Rd). Thus, the features are Gaussian with spiked\ncovariance, where the spike vector has some correlation with the unknown parameter vector of interest v0. In\nthis way, our model captures the significance of pre-training by allowing the unlabeled feature data to contain\ninformation hidden in its covariance structure that is directly correlated with the solution of the supervised\nlearning problem. The value of η1 measures the strength of this correlation. We define η2 =\np\n1 −η2\n1.\nOur goal is to estimate the unknown vector v0 with parameter vector X ∈Sd−1, by using SGD on the\nfollowing loss function: L(X, y) = [f(X · a) −y]2. We use spherical gradient descent with step-size δ/d to\noptimize parameters X, given by the following stochastic updates:\nXt+1 =\nXt −δ\nd∇L(Xt, y)\n∥Xt −δ\nd∇L(Xt, y)∥2\nwith initialization X0, where ∇denotes the spherical gradient with respect to the parameters X.\nWe want to understand the benefit of pre-training and so we consider two methods of initializations,\nrandom and with pre-training. For random initializations we let X0 ∼Uniform(Sd−1). To model pre-training,\nwe use Principal Component Analysis (PCA) on the unlabeled data (a′\ni)N ′\ni=1, to obtain an estimate ˆv of the\nspike direction v. We then use this to initialize SGD for our supervised task, that is we let X0 = ˆv. PCA is\narguably the ideal starting point for a rigorous investigation into unsupervised pre-training. From a statistical\nperspective, PCA is the simplest dimension-reduction algorithm; further, it’s properties are well-understood\n3\nin high-dimensions Bai and Silverstein [2010]. More importantly, it has been shown that more advanced\nrepresentation learning algorithms, such as reconstruction autoencoders, also implement PCA in certain\nregimes Bourlard and Kamp [1988], Baldi and Hornik [1989], Nguyen [2021]. In this light, we will restrict\nourselves to PCA-based pre-training in this paper. We note that this approach is distinct from the spectral\nmethods introduced for single-index models and, in particular, phase retrieval. There, the methods are\nsupervised in that they use both knowledge of the label and features, whereas in unsupervised pre-training\none only has access to the features.\nWith these two methods of initialization, our goal is to contrast their respective sample complexity\nrequirements for solving the supervised learning task (recovering the unknown vector v0). We are in particular\ninterested in the high dimensional regime. We consider spherical SGD (hence forth referred to as simply SGD)\nwith the total number of steps (and samples of (yi, ai)) given by N = αdd. Thus the number of samples we\nobserve is a function of the dimension, and we are then interested in analyzing the high dimensional limit\nd →∞.\n3.2\nMain Results\nIn this section we state our main results. Firstly we state a few definitions and assumptions. We defer the\nproofs of all results to Appendix A. Throughout we will often refer to the ‘population loss’:\nΦ(X) = E[f(X · a) −y]2\n= E[f(X · a) −f(v0 · a)]2 + Eϵ2\nWe note that there are two important directions of interest in this problem, namely v0 and the residual\ndirection of the spike vector, after subtracting off the projection onto v0, that is\n1\nη2 (v −η1v0). Without\nloss of generality, we let the first two basis vectors be written as e1 = v0 and e2 =\n1\nη2 (v −η1v0), so that\nv = η1e1 + η2e2. We can rewrite the population loss which is a function of X, solely through the correlation\nof X with each of these directions. Let m1(X) = X · e1 = x1 and m2(X) = X · e2 = x2, then\nΦ(X) = E[f(a · X) −f(a · e1)]2 + Eϵ2\n= E[f(a1x1 + a2x2 +\nq\n1 −x2\n1 −x2\n2g) −f(a1)]2 + Eϵ2\n= ϕ(x1, x2)\nwhere [a1, a2, g] is jointly Gaussian with mean 0 and g ⊥a1, a2 and Eg2 = 1. The covariance of a1, a2 is\ngiven by:\n\u00121 + λη2\n1\nλη1η2\nλη1η2\n1 + λη2\n2\n\u0013\nThroughout we will use the term ‘population flow’ which is simply the discretized gradient flow on the\npopulation loss Φ.\nDefinition 3.1. A sequence of initializations (X(d)\n0 )d≥1 is Effective for SGD with N = αdd steps of stepsize\nδ/d if m1(X(d)\nN ) →1 in probability as d →∞.\nA sequence of initializations is considered Effective if SGD with some number of steps and stepsize,\ninitialized from the given sequence, recovers the true solution in the high dimensional limit. To that end, we say\nthat a sequence of initializations is Ineffective if it is not effective. We have defined Effective initializations for\nSGD based on the convergence of the SGD process, and we can also consider these definitions for initializations\nof population flow, defined by the convergence of the population flow process in place of SGD.\nAssumption 3.2. We say that Assumption 3.2 holds with point m∗= (m∗\n1, m∗\n2) ∈B2(0, 1) if there exists\na point m∗, such that:\n∇Φ(X) · e1 > 0\nsgn(m2(X))∇Φ(X) · e2 < 0\nfor all X such that (m1(X), m2(X)) ∈B2(0, 1) : m1(X) ≥m∗\n1, |m2(X)| < m∗\n2.\n4\nWe note that the ability of Φ to meet Assumption 3.2 depends entirely on the choice of activation function\nf. It is clear that population flow, initialized within the rectangle defined by the points m∗and (1, 0) will\nrecover the correct solution (see Figure 1 for an example), and hence this assumption provides a simple way\nto verify that a sequence of initializations is Effective for population flow. Now we state our main result\nregarding initializing with pre-training.\nTheorem 3.3. Suppose that Assumption 3.2 holds with point m∗. Further η1 ≥m∗\n1 and |η2| ≤m∗\n2. Then for\nspherical SGD on the given loss with N = αd steps where α = ω(1), αδ2 = o(1), we have that the sequence\nof initializations X(d)\n0\n= ˆvd, the PCA estimators of vd obtained with N ′ = α′d unlabeled samples where\nα′ = ω(1), are Effective.\nThe theorem above states sufficient conditions on Φ and the correlation between v and v0 such that\nwith pre-training, we are able to recover the true parameter vector with high probability in large enough\ndimensions. Further, we see that α = ω(1), and hence our recovery with N = αd steps is just beyond linearly\nmany steps in the dimension.\nFor our next two results we will work with activations f which satisfy\nEf ′′(g) = Ef ′(g) = 0, E ∂2\n∂g2 f(g)2 > 0\n(1)\nfor g ∼N(0, 1). We now consider our second main result which considers recovery from random initializations.\nTheorem 3.4. Suppose that f satisfies (1). Then for spherical SGD with N = αd steps where α ≪d,\nαδ2 = O(1), for the sequence of initializations X(d)\n0\n∼Uniform(Sd−1) we have that m1(XN) →0 in probability,\nas d →∞.\nOur second main result states that under appropriate moment conditions on f, we have that in order to\nrecover the unknown parameter vector v0, we require at least N = Ω(d2) samples. We emphasize that this\nresult does not inform us of when we can recover the true parameter vector, only sufficient conditions for\nshowing that we cannot recover with less than quadratic samples in the dimension.\nWe now state our third main theorem which is the most surprising result and brings to light the\ncomplexity of the single layer supervised network with Gaussian features and spiked covariance.\nLet\nHd\nr = {X ∈Sd−1 : |m1(X)| < r}.\nTheorem 3.5. Suppose that f satisfies (1) for g ∼N(0, 1). When η1 = 1, for spherical SGD with N = αd\nsteps where α = ω(1), αδ2 ≪d1/3 , we have that there exists some r > 0 such that for all sequences of\ninitializations X(d)\n0\n∈H(d)\nr\n, then we have that: m1(XN) →0 in probability as d →∞. Further we have that\nfor all ϵ > 0:\nP(sup\nt≤N\n|m1(Xt)| > r + ϵ) →0\nin probability as d →∞.\nThis result demonstrates the surprising fact that in the simple scenario where the spike is perfectly aligned\nwith the unknown parameter vector, that with any amount of data, given appropriate stepsize, recovery is not\npossible from random initializations. Even more surprising, not only is recovery not possible from random\ninitialization, but even initializing with some fixed correlation, can result in not only a failure to recover but\nfurther a loss of the initial correlation. We also have that the maximum correlation attained over the course\nof SGD is contained in a ball around 0 with radius slightly larger than the ball containing the initializations.\nTaking into account Theorem 3.3, we see that there exists problems such that pre-training can allow us to\nsolve the problem in linear time, whereas the problem is unsolvable from random initialization in the given\nscaling regime, regardless of the amount of labeled data.\nIt is important here to note that a similar negative result was observed by Mousavi-Hosseini et al. [2023]\nfor population gradient flow when fitting a two-layer network with ReLU activation. There the authors\npropose to correct for this via preconditioning the gradient. By contrast, here we use this to illustrate the\npower of pre-training.\n5\n3.3\nDiscussion\nTogether, the first two theorems above tell us that for certain activation functions f such that both the\nassumptions of Theorem 3.3 and the assumptions of Theorem 3.4 are met, we establish a significant separation\nof the required samples for recovering the unknown parameter vector. With pre-training, we can achieve\nconvergence with N = αd whenever α = ω(1). That is, we can recover with less than log-linear samples in\nthe dimension. For random initializations, we require N = Ω(d2) at minimum. This gives us a separation of\ndζ for all ζ < 1. Theorem 3.5 not only highlights the complex scenarios that can arise by introduction of a\nspike vector but also serves as a demonstration of the powerful effects of pre-training. In the scenario where\nthe spike vector is equal to the parameter vector, pre-training alone is sufficient for solving the problem (of\ncourse this information would not be available to any practitioner) and without pre-training, no amount of\ndata is enough to solve the problem from random initialization under the given regime. However, provided\nenough correlation between the spike vector and unknown parameter vector, the problem is solvable via\npre-training with just over linearly many samples.\nWe point out that Theorems 3.3 and 3.4 have established a lower bound on the benefit of pre-training. As\nwe see from Theorem 3.5, there are scenarios which deviate significantly from the lower bound provided here.\nPast works Ben Arous et al. [2021] have shown that when the features are isotropic Gaussian, the sample\ncomplexity is governed by a quantity called the information exponent, which is essentially the order of the\nfirst non-zero term in the Taylor expansion of the population loss. In the case of single-index models the\ninformation exponent can be written in terms of the Hermite coefficients of the activation function f, which\ncan be similarly expressed as moment conditions on f. In light of the results in the isotropic Gaussian case,\nTheorems 3.3 and 3.4 may not seem that surprising. We emphasize here that the introduction of the spike to\nthe covariance, makes the problem much more complicated. This is made clear by Theorem 3.5, where in\ncontrast to the isotropic feature case where initializing with some fixed correlation puts us immediately into\nthe descent phase allowing for recovery with near linear sample complexity Ben Arous et al. [2021], with the\nintroduction of the spike with any positive magnitude, a local minima appears around m1(X) = 0 and hence\nwith random initialization or initializing with some fixed correlation that is within the attractor region of the\nlocal optima, SGD tends to the local optima, in effect learning nothing and perhaps destroying the initial\ninformation.\nWe quickly point out that Theorem 3.5 holds even with exponential data, stating that with the given\nstepsize, SGD does not recover the unknown parameter vector. We also note that the step-size specified in\nTheorem 3.5 is in fact more general than in 3.3 and 3.4. Hence, this is a reasonable range of stepsizes for\nwhich one would expect to solve the problem with sufficiently many samples.\n3.4\nMeeting Assumptions\nWe now take a moment to consider the assumptions in our theorems. Assumption 3.2 requires the existence of\nsome point m∗such that within the rectangle defined by this point and the global optima (m1(X), m2(X)) =\n(1, 0), the population dynamics are well behaved, tending to the global optima at (m1(X), m2(X)) = (1, 0) in\nlinear time. When it comes to the moment conditions on f required to apply Theorems 3.4 and 3.5, one can\neasily check (see Lemma A.10 in the supplementary material) that the Hermite polynomials with degree ≥3\nsatisfy them. While this claim does not extend to all linear combinations of Hermite Polynomials, it can be\nextended to linear combinations of Hermite Polynomials of degrees 3 or greater, with the added constraint\nthat any two coefficients in the Hermite expansion that are exactly 2 degrees apart, must have the same sign,\ni.e. Ef(g)hk(g)Ef(g)hk−2(g) ≥0. This provides a class of functions which demonstrate our Theorems and\nthus the effects of pre-training.\n4\nTransfer learning\nWe also consider a related problem which we find lends itself better to the notion of transfer learning. We\nconsider a related scenario under which we once again have labeled data (yi, ai)N\ni=1 according to a single\nlayer network with known activation function f and unknown parameter vector v0. We assume that f is\ndifferentiable almost everywhere with f, f ′ of at most polynomial growth. However, we now consider the\ncase of isotropic Gaussian features: ai ∼N(0, Id). Without the spike in the covariance, there is only one\n6\ncorrelation variable of interest, namely m1(X) as previously defined. It can be shown that for f differentiable\nalmost everywhere and f ′ of at most polynomial growth, the population loss Φ(X) can be expressed as\nϕ(m1(X)) ∈C1, with ϕ′(x) < 0, ∀x ∈(0, 1) and further, the sample complexity for solving this problem with\nSGD is well understood Ben Arous et al. [2021].\nTo introduce the notion of transfer learning, we consider the scenario where we have access to some\nsequence of vectors v(d) with v(d) · v(d)\n0\n= ηd. We may consider these correlated vectors v(d) to be a sequence\nof estimates of some vector correlated to v0 which were obtained via SGD on some related task. For the sake\nof analysis we are not concerned with how these correlated vectors are obtained, only the benefit provided by\nhaving access to them for initializing SGD. We are again interested in the sample complexity as a function of\ndimension and how this complexity is affected by initializing with transfer learning in contrast to uniform\nrandom initializations.\nRecall the information exponent from Ben Arous et al. [2021].\nDefinition 4.1. We say that a population loss ϕ has information exponent k if ϕ ∈Ck+1([−1, 1]) and\nthere exist C, c > 0 such that:\n\n\n\n\n\n\n\ndℓϕ\ndmℓ(0) = 0\nfor 1 ≤ℓ< k,\ndkϕ\ndmk (0) ≤−c < 0,\n\r\r\r dk+1ϕ\ndmk+1 (m)\n\r\r\r\n∞≤C.\nWe now state a result concerning transfer learning with isotropic Gaussian features as described above.\nThe details on sample complexity for this model are well understood from the work of Ben Arous et al. [2021],\nwhich allows us to easily analyze the effects of transfer learning.\nTheorem 4.2. Let k ≥2 be the information exponent of ϕ. Let v(d) · v(d)\n0\n= ηd = θ(d−ζ), with ζ ∈[0, 1/2).\nThen for spherical SGD with N = αd steps with α ≫d2ζ(k−2)(log d)21[ζ>0] and α−1 ≪δ ≪α−1/2, X0 = v,\nwe have that: m1(X) →1 in probability as d →∞. Here 1[ζ > 0] is the indicator function, taking value 0 if\nζ = 0 and 1 otherwise.\nThe proof of this theorem follows almost exactly from Ben Arous et al. [2021], noting that their arguments\nstill hold under slightly different initializations. Contrasting this theorem with the results of Ben Arous et al.\n[2021], we notice that when v(d) · v(d)\n0\n= ηd = O(d−ζ) for some ζ ∈(0, 1/2) and the information exponent is 3\nor greater, we benefit from a polynomial sample reduction from α ≫dk−2(log d)2 to α ≫d2ζ(k−2)(log d)2.\nFurther in the case that ζ = 0, i.e., we initialize with a fixed correlation independent of the dimension, we see\nthat only α = ω(1) is required, and hence we can recover in nearly linear sample complexity regardless of\nthe information exponent. This offers a substantial polynomial reduction in the event that the information\nexponent is large. We do note however that even in the case the of the information exponent 2, we still\nbenefit from a complexity reduction of a factor of (log d)2. In the case of information exponent 1, we do not\nbenefit from transfer learning, however, in this case we already recover with nearly linear sample complexity,\nhence we have no need to perform transfer learning.\n5\nExamples\nIn this section we show some examples of our theorems and provide simulations to empirically verify our\nclaims in large finite dimensions.\n5.1\nThe Third Hermite Polynomial\nWe will now show how one can apply our theorems from the past section. As noted in the past section, for\nall Hermite polynomials with degree 3 or greater, all assumptions are met for applying Theorems 3.4 and 3.3,\nprovided a large enough correlation between v and v0, ie for large enough η1. Hence in order to apply our\ntheorems to a specific problem set up with some Hermite polynomial in place of f and some values of λ, η1\nwe must simply verify whether or not η1 is large enough. To do this, we must identify the region of Effective\ninitializations for population flow and determine whether (η1, η2) falls inside.\n7\nNote that taking f to be any polynomial function, we solve for Φ(X) explicitly, by expanding and\ncomputing the moments of Gaussian random variables. After computing the explicit loss, one can compute\nthe spherical gradients with respect to x1 and x2 and analyze their signs in order to identify a value of m∗to\napply Assumption 3.2. Below we plot the phase diagram for population flow when f(x) = x3 −3x, the third\nHermite polynomial. We identify a point m∗to Apply assumption 3.2.\n5.2\nSimulations\nWe conduct a few simulations to empirically demonstrate our claims in finite dimensions. In the first\nsimulation, we consider letting f(x) = x3 −3x and setting λ = 1, η1 = 0.45. We then conduct SGD from both\nrandom initializations and from estimates of v obtained via PCA. We use dimension d = 1000 and let SGD\nrun for 3\n2d2 = 1, 500, 000 steps of size\n1\n10d2 = (10, 000, 000)−1. We select the parameters such that we would\nexpect to be able to recover the true parameter vector from a random initialization had we been in the case\nλ = 0. We determine this scaling based on the results of Ben Arous et al. [2021] and some experimenting.\nSee Figure 1.\nWe also perform simulations under the setting λ = 0.5 and η1 = 1 as in Theorem 3.5. We then perform\nSGD with the dimension, step size and number as steps as given above, only we consider initializing uniform\nrandomly, conditional on fixing the correlation m1(X0) = 0.1. See Figure 1.\n6\nProof Ideas\nWe make use of the ‘bounding flows’ approach from Ben Arous et al. [2020b], Ben Arous et al. [2020a], and\nBen Arous et al. [2021]. This approach was applied to Single-Index Models in Ben Arous et al. [2021]. The\nkey difference here is that the populations dynamics here cannot be reduced to a 1-dimensional correlation\nvariable but a 2-dimensional vector of correlations. As such more delicate analysis of the phase portrait and\nthe martingale fluctuations are involved.\n6.1\nTheorem 3.3\nOur first main theorem, provides a sufficient condition to check when pre-training for initializing SGD\ncan recover the unknown parameter vector in almost linear time. The proof of has two main components.\nFirst note that under Assumption 3.2, there exists a rectangle that, when initialized within, the population\ndynamics will find the global optima. Next Lemma A.3 shows that when initializing with some fixed initial\ncorrelations, SGD behaves like the population dynamics, which under Assumption 3.2 converge to the global\noptima when initialized in the rectangle. The second part of the proof of Theorem 3.3 is simply applying\na few well-known facts regarding PCA in high dimensions (see Appendix B). These facts along with the\nassumption on the strength of the correlation of the spike to the unknown parameter vector, tell us that when\ninitializing with pre-training, we find ourselves in the region of effective initializations as given by Assumption\n3.2 and hence we can recover the unknown parameter vector in approximately linear time.\n6.2\nTheorem 3.4\nWe consider the Taylor expansion of the population loss in the two correlation variables of interest around 0.\nRecall that random uniform initializations yield initial correlations on the order of m1(X0), m2(X0) = O( 1\n√\nd)\nVershynin [2018]. Under the Assumptions on f of Theorems 3.4 and 3.5 we have the following system for the\npopulation dynamics:\n∂ϕ(x1, x2)\n∂x1\n= 2λη2\n1cx1 + 2λη1η2cx2 + O(∥(x1, x2)∥2\n2)\n∂ϕ(x1, x2)\n∂x2\n= 2λη2\n2cx2 + 2λη1η2cx1 + O(∥(x1, x2)∥2\n2)\nfor some positive constant c. We remind the reader here of the definition (x1, x2) = (m1(X), m2(X)).\nAnalyzing the linearized system, we see that the first-order terms are orthogonal to (and point towards)\n8\n(a)\n(b)\n(c)\n(d)\nFigure 1: The figures on the left correspond to η1 = 0.45, λ = 1 and the right correspond η1 = 1, λ = 0.5.\nEach figure displays 4 SGD runs. One random initialization, one initialization via pre-training with PCA and\n2 fixed initializations. The left figures feature fixed initializations of (m1(X0), m2(X0)) = (0.25,\n√\n1 −0.252)\nand (m1(X0), m2(X0)) = (0.25, −0.75). We observe that pre-training results in finding the global optima at\n(m1(X0), m2(X0)) = (1, 0) and random initialization makes little to no progress. Fixing the value of m1(X0),\nwe also observe very different behaviors simply by varying the value of m2(X0) highlighting the complexity\nthat arises when introducing a second dimension to the population dynamics. In the case of η1 = 1 the\nfixed initializations are m1(X0) = 0.1 and m1(X0) = 0.25. In addition to noticing the behavior of random\ninitialization versus pre-training, we observe very different behavior for fixed initializations. For small enough\nm1(X0) SGD tends towards the local optima at m1(X) = 0 as suggested by Theorem 3.5. For larger enough\nm1(X0), SGD tends towards the global optima.\nthe line L = {(x1, x2) : x1 = −η2\nη1 x2}, and are equal to 0 on L. Letting T ⊥\nL (x1, x2) measure the distance of\n(x1, x2) to L, we have that the first terms exceed higher order terms when outside the set C = {(x1, x2) :\nT ⊥\nL (x1, x2) > c1∥(x1, x2)∥2\n2} for some constant c1. This set provides a cusp surrounding the line L. To prove\nour result, we carefully construct stopping times in order to observe the process over specific regions of the\nspace, such as the Cartesian quadrants and positioning relative to the set C. We show that in quadrants 1\nand 3, the process tends to quadrant 2 or quadrant 4. Once in these quadrants we bound the distance of\nthe process to the line L, ultimately ensuring the process gets close to L. Once this happens we show that\nthe given sample complexity is not sufficient to leave some fixed ball around the origin. The proof of this\ntheorem is the most involved. While this idea can be understood at the population level, the extension to\nSGD requires controlling the variance of the martingale along various important directions with martingale\ninequalities.\n9\n6.3\nTheorem 3.5\nWhen η1 = 1 (and hence η2 = 0) as in Theorem 3.5, the system no longer depends on x2 (Recall the definition\nof ϕ in the start of section 3.2). We then have the following 1-d system:\n∂ϕ(x1, x2)\n∂x1\n= 2λη2\n1cx1 + O(∥(x1, x2)∥2\n2)\nFrom this system we see there is a local optima at m1(X) = 0, whose attractor region is fixed and not\ndependent on the dimension. Hence the remainder of the proof of Theorem 3.5 is showing that in the high\ndimensional limit, the randomness of SGD is insufficient to escape the attractor region in the given scaling\nregime.\n7\nConclusion\nIn this paper we consider natural statistical models for which one can analyze the effects of pre-training and\ntransfer learning. Namely single-index models with Gaussian features with spiked covariance and isotropic\ncovariance. We analyze the ability to recover the unknown parameter vector in these models using stochastic\ngradient descent and the required sample complexity in the high dimensional regime, from both random\ninitializations and with pre-training / transfer learning. In both scenarios we prove polynomial separation in\nthe sample complexity as a function of the dimension required to solve, for a class of functions which contains\nthe Hermite polynomials of degree 3 or greater. We also highlight the complexity of analyzing recovery under\na single-index model with Gaussian features and spiked covariance, by highlighting a simple case (the case\nwhere the spike vector is equal to the parameter vector) in which a local optima arises and traps random\ninitializations. This paper contributes to the growing body of work attempting to add theoretical justification\nfor common practices of pre-training and transfer learning.\nAcknowledgements\nA.J. acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC),\nthe Canada Research Chairs programme, and the Ontario Research Fund. Cette recherche a ´et´e enterprise\ngrˆace, en partie, au soutien financier du Conseil de Recherches en Sciences Naturelles et en G´enie du Canada\n(CRSNG), [RGPIN-2020-04597, DGECR-2020-00199], et du Programme des chaires de recherche du Canada.\nS.S. gratefully acknowledges support from NSF (DMS CAREER 2239234), ONR (N00014-23-1-2489) and\nAFOSR (FA9950-23-1-0429).\nReferences\nEmmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap\ncomplexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory,\npages 2552–2623. PMLR, 2023.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nIsabela Albuquerque, Jo˜ao Monteiro, Mohammad Darvishi, Tiago H Falk, and Ioannis Mitliagkas. Generalizing\nto unseen domains via distribution matching. arXiv preprint arXiv:1911.00804, 2019.\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.\nA\ntheoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229,\n2019.\nEyar Azar and Boaz Nadler. Semi-supervised sparse gaussian classification: Provable benefits of unlabeled\ndata. arXiv preprint arXiv:2409.03335, 2024.\n10\nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning in the presence of\nlow-dimensional structure: a spiked random matrix perspective. Advances in Neural Information Processing\nSystems, 36, 2024.\nZhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices, volume 20.\nSpringer, 2010.\nJinho Baik, G´erard Ben Arous, and Sandrine P´ech´e. Phase transition of the largest eigenvalue for nonnull\ncomplex sample covariance matrices. 2005.\nPierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples\nwithout local minima. Neural networks, 2(1):53–58, 1989.\nAlfonso S. Bandeira, Amit Singer, and Thomas Strohmer. Mathematics of Data Science. 2020. URL\nhttps://people.math.ethz.ch/~abandeira/BandeiraSingerStrohmer-MDS-draft.pdf.\nJean Barbier, Florent Krzakala, Nicolas Macris, L´eo Miolane, and Lenka Zdeborov´a. Optimal errors and\nphase transitions in high-dimensional generalized linear models. Proceedings of the National Academy of\nSciences, 116(12):5451–5460, 2019.\nGerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Algorithmic thresholds for tensor pca. The\nAnnals of Probability, 48(4):2052–2087, 2020a.\nG´erard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Bounding flows for spherical spin glass dynamics.\nCommunications in Mathematical Physics, 373:1011–1048, 2020b.\nGerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex\nlosses from high-dimensional inference. Journal of Machine Learning Research, 22(106):1–51, 2021.\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010.\nAlberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index models with\ngradient flow. arXiv preprint arXiv:2310.19793, 2023.\nHerv´e Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value decomposition.\nBiological cybernetics, 59(4):291–294, 1988.\nTom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nEmmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and\nalgorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015.\nAlex Damian, Eshaan Nichani, Rong Ge, and Jason D Lee. Smoothing the landscape boosts the signal for sgd:\nOptimal sample complexity for learning single index models. Advances in Neural Information Processing\nSystems, 36, 2024a.\nAlex Damian, Loucas Pillaud-Vivien, Jason Lee, and Joan Bruna. Computational-statistical gaps in gaussian\nsingle-index models. In The Thirty Seventh Annual Conference on Learning Theory, pages 1262–1262.\nPMLR, 2024b.\nAlexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with\ngradient descent. In Conference on Learning Theory, pages 5413–5452. PMLR, 2022.\nYatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborov´a, and Florent Krzakala. The\nbenefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information\nand leap exponents. arXiv preprint arXiv:2402.03220, 2024.\nShai Ben David, Tyler Lu, Teresa Luu, and D´avid P´al. Impossibility theorems for domain adaptation.\nIn Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages\n129–136. JMLR Workshop and Conference Proceedings, 2010.\n11\nJacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\nRishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In Conference On Learning\nTheory, pages 1887–1930. PMLR, 2018.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran¸cois Laviolette,\nMario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of machine\nlearning research, 17(59):1–35, 2016.\nLeonard Gross. Gronwall’s inequality and its applications. The Journal of Mathematical Analysis and\nApplications, 20(3):359–370, 1967.\nMisgina Tsighe Hagos and Shri Kant. Transfer learning based detection of diabetic retinopathy from small\ndataset. arXiv preprint arXiv:1905.07203, 2019.\nSteve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. Advances in Neural\nInformation Processing Systems, 32, 2019.\nKaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2961–2969, 2017.\nJames J Heckman. Sample selection bias as a specification error. Econometrica: Journal of the econometric\nsociety, pages 153–161, 1979.\nJiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch¨olkopf, and Alex Smola. Correcting\nsample selection bias by unlabeled data. Advances in neural information processing systems, 19, 2006.\nJason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps: Provable\nself-supervised learning. Advances in Neural Information Processing Systems, 34:309–323, 2021.\nJason D Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-dimensional polynomials\nwith sgd near the information-theoretic limit. arXiv preprint arXiv:2406.01581, 2024.\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint adaptation\nnetworks. In International conference on machine learning, pages 2208–2217. PMLR, 2017.\nYue M Lu and Gen Li. Phase transitions of spectral initialization for high-dimensional non-convex estimation.\nInformation and Inference: A Journal of the IMA, 9(3):507–541, 2020.\nAntoine Maillard, G´erard Ben Arous, and Giulio Biroli. Landscape complexity for the empirical risk of\ngeneralized linear models. arXiv preprint arXiv:1912.02143, 2019.\nAntoine Maillard, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov´a. Phase retrieval in high dimensions:\nStatistical and computational phase transitions. Advances in Neural Information Processing Systems, 33:\n11071–11082, 2020.\nSubha Maity, Yuekai Sun, and Moulinath Banerjee. Minimax optimal approaches to the label shift problem\nin non-parametric settings. Journal of Machine Learning Research, 23(346):1–45, 2022.\nAlireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A Erdogdu. Gradient-based feature learning\nunder structured data. Advances in Neural Information Processing Systems, 36:71449–71485, 2023.\nPhan-Minh Nguyen. Analysis of feature learning in weight-tied autoencoders via the mean field lens. arXiv\npreprint arXiv:2102.08373, 2021.\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data\nengineering, 22(10):1345–1359, 2009.\nJoaquin Qui˜nonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in\nmachine learning. Mit Press, 2022.\n12\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nYunwei Ren and Jason D Lee. Learning orthogonal multi-index models: A fine-grained information exponent\nanalysis. arXiv preprint arXiv:2410.09678, 2024.\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural\nnetworks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint\narXiv:1911.08731, 2019.\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training\nfor speech recognition. arXiv preprint arXiv:1904.05862, 2019.\nHidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood\nfunction. Journal of statistical planning and inference, 90(2):227–244, 2000.\nAmos Storkey. When training and test sets are different: characterizing learning transfer. 2008.\nJu Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of Computational\nMathematics, 18:1131–1198, 2018.\nRemi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J Gordon. Domain adaptation with\nconditional distribution matching and generalized label shift. Advances in Neural Information Processing\nSystems, 33:19276–19289, 2020.\nChristopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic posterior\ninformation to linear models. Journal of Machine Learning Research, 22(281):1–31, 2021a.\nChristopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy,\nand linear models. In Algorithmic Learning Theory, pages 1179–1206. PMLR, 2021b.\nRoman Vershynin.\nHigh-Dimensional Probability: An Introduction with Applications in Data Science.\nCambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763, 2016.\nXuezhi Wang and Jeff G Schneider. Generalization bounds for transfer learning under model shift. In UAI,\npages 922–931, 2015.\nXuezhi Wang, Tzu-Kuo Huang, and Jeff Schneider. Active transfer learning under model shift. In International\nConference on Machine Learning, pages 1305–1313. PMLR, 2014.\nColin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream\ntasks? an analysis of head and prompt tuning. Advances in Neural Information Processing Systems, 34:\n16158–16170, 2021.\nDavid Williams. Probability with Martingales. Cambridge University Press, 1991.\nYifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with asymmetrically-\nrelaxed distribution alignment. In International conference on machine learning, pages 6872–6881. PMLR,\n2019.\nRuntian Zhai, Bingbin Liu, Andrej Risteski, Zico Kolter, and Pradeep Ravikumar.\nUnderstanding\naugmentation-based self-supervised representation learning via rkhs approximation and regression. arXiv\npreprint arXiv:2306.00788, 2023.\nTianyi Zhang and Tatsunori Hashimoto. On the inductive bias of masked language modeling: From statistical\nto syntactic dependencies. arXiv preprint arXiv:2104.05694, 2021.\n13\nHan Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations\nfor domain adaptation. In International conference on machine learning, pages 7523–7532. PMLR, 2019.\nAaron Zweig, Loucas Pillaud-Vivien, and Joan Bruna. On single-index models beyond gaussian data. Advances\nin Neural Information Processing Systems, 36, 2024.\n14\nA\nProofs of Main Results\nFor convenience we define the sample wise error as H(X, y) = L(X, y) −Φ(X). We first prove a lemma which\nis used throughout our proofs.\nLemma A.1. There exist constants C1, C2 > 0 such that the following moment bounds hold uniformly in d:\nsup\nx,θd∈Sd−1 E[(∇Hd(x, y) · θd)2] ≤C1\nsup\nx∈Sd−1 E[∥∇Hd(x, y)∥4+C2\n2\n] ≤C1d(4+C2)/2\nProof. See the proof of proposition B.1 in Ben Arous et al. [2021] and not that similar arguments apply when\nthe features have spiked covariance.\nA.1\nProof of Theorem 3.3.\nWe first prove Theorem 3.3.\nTheorem 3.3. Suppose that Assumption 3.2 holds with point m∗. Further η1 ≥m∗\n1 and |η2| ≤m∗\n2. Then\nfor spherical SGD on the given loss with N = α ∗d steps where α = ω(1), αδ2 = o(1), we have that the\nsequence of initializations X(d)\n0\n= ˆvd, the PCA estimators of vd obtained with N ′ = α′d unlabeled samples\nwhere α′ = ω(1), are Effective.\nProof. The proof of Theorem 3.3 is an immediate consequence of the sequence of PCA initializations being\nEffective for population flow as given by Proposition A.4 and the convergence between SGD and population\nflow as given by Lemma A.3 (below).\nLemma A.3. Suppose that Assumption 3.2 holds with point m∗. Fix any point (minit\n1\n, minit\n2\n) with minit\n1\n>\nm∗\n1, minit\n2\n< m∗\n2. For a sequence of initializations (m1(Xd\n0), m2(Xd\n0))d≥1 converging to (minit\n1\n, minit\n2\n), spherical\nSGD on the given loss with the given initializations and N = αd steps where α = ω(1), αδ2 = o(1) yields the\nfollowing:\nsup\nt≤N\n∥(m1(Xt), m2(Xt)) −(m1( ¯\nXt), m2( ¯\nXt))∥2 →0\nin probability as d →∞. Here ¯\nXt is population flow with the same initialization.\nWe will show that the spherical projections are negligible (arbitrarily small with probability 1 - o(1)). We\ncan then consider the linearized paths of (m1(Xt), m2(Xt)) and (m1( ¯\nXt), m2( ¯\nXt)). Bounding their difference\nwith Gronwall’s inequality Gross [1967] and Doob’s Inequality Williams [1991] to control the martingale term.\nProof. Let ∇E denote the usual Euclidean gradient. Then consider the spherical gradient:\n∇Φ(X) = ∇EΦ(X) −(∇EΦ(X) · X)X\nThat is the euclidean gradient projected onto the orthogonal space of X. We know that the population\nloss can be written as a function of (m1(X), m2(X)) = (x1, x2) (recall that (m1(X), m2(X)) = (x1, x2) due\nto the without loss of generality assumption that v0 = e1 and (η2)−1(v −η1v0) = e2). Hence:\nsup\nX∈Sd−1 ∥∇Φ(X)∥2 =\nsup\nX∈Sd−1 ∥∇EΦ(X) −(∂ϕ(x1, x2)\n∂x1\nx1 + ∂ϕ(x1, x2)\n∂x2\nx2)X∥2 ≤A\n(2)\nwhere A is some constant independent of d, using that Φ ∈C1 and X ∈Sd−1 (compact). Additionally\nwith the above, one can show that there exists K (independent of d) such that for X, Y ∈Sd−1\n∥∇Φ(X) −∇Φ(Y )∥2 ≤K∥X −Y ∥2\n(3)\n15\nWe will use this fact later. Now let\nrt+1 = ∥Xt −δ\nd(∇Φ(Xt) −∇H(Xt, yt))∥2\n≤\nr\n1 + δ\nd(∥∇Φ(Xt)∥2\n2 + ∥∇H(Xt, yt)∥2\n2)\n(4)\n≤1 + δ2( A\nd2 + Lt\nd )\n(5)\nIn (4) we use that Xt and δ\nd(∇Φ(Xt) −∇H(Xt, yt)) are orthogonal as the gradient is spherical. In (5)\nwe use that for u > 0, √1 + u < 1 + u and the spherical gradient has bounded norm shown in (2). Lt =\n∥∇H(Xt, yt)/\n√\nd∥2\n2, noting that while Lt is random, it’s expectation is bounded by a constant independent\nof d by Lemma A.1. Note that the quantity we have defined rt+1 is simply the radius of Xt after the gradient\nupdate, but before projecting back onto the sphere. We have that |rt+1 −1| ≤δ2( A\nd2 + Lt\nd ). This bounds the\ndistance between Xt with itself, had it not been projected onto the sphere after the last gradient update:\n∥Xt −(Xt−1 −δ\nd∇Φ(Xt−1) + δ\nd∇H(Xt−1, yt−1))∥2 = |rt −1|\nrt\n∥Xt∥2 ≤δ2( A\nd2 + Lt\nd )\nBy iterating this bound, we have the following:\nsup\nt≤N\n∥Xt −(X0 −δ\nd\nt−1\nX\ni=0\n∇Φ(Xi) + δ\nd\nt−1\nX\ni=0\n∇H(Xi, yi))∥2 ≤\nN−1\nX\ni=0\nδ2( A\nd2 + Li\nd )\nBy Markov’s inequality, the probability that the right hand side is greater than some ϵ > 0 is:\nP(\nN−1\nX\ni=0\nδ2( A\nd2 + Li\nd ) > ϵ) ≤ϵ−1αδ2(A\nd + sup\nt≤N\nELt)\nWhich is o(1) given αδ2 = o(1) and supt≤N ELt < ∞by Lemma A.1. It thus suffices to consider the\nlinearization of (Xt)N\nt=0, which for the two correlation variables of interest, we denote:\nYt = ⃗m(X0 −δ\nd\nt−1\nX\ni=0\n(∇Φ(Xi) −∇H(Xi, yi)))\nwhere ⃗m(x) = (m1(x), m2(x)). We note that this linearization (Yt)N\nt=0 is not the same as linear SGD.\nThis process is equivalent to performing spherical SGD, but adding back all of the projection vectors at each\nstage, that were used to map Xt to the sphere after each gradient update. Which is also not equivalent\nto doing regular gradient descent with spherical gradients. Redoing the above computations with respect\nto ¯\nXt one would see that deterministically, for large enough d we have that ¯Xt is also within ϵ of ¯Yt, the\nlinearization of gradient flow, given by:\n¯Yt = ⃗m( ¯X0 −δ\nd\nt−1\nX\ni=0\n∇Φ( ¯Xi))\nHence to prove our result it is enough to show the convergence in probability between Yt and ¯Yt. To do\nso, let us consider the martingale term given by δ\nd\nPt\ni=0 ∇H(Xt, yt). Applying Doob’s inequality with p = 2\nwe see that\nP(sup\nt≤N\nδ\nd∥\nt\nX\ni=0\n⃗m(∇H(Xt, yt))∥2 > ϵ) ≤2Cαδ2\nϵ2d\n= o(1/d)\nfor some constant C, independent of the dimension. To show that supt≤N ∥Yt −¯Yt∥2 →0, we first consider\nfor some fixed T, the quantity: supt≤T δ−1d ∥Yt −¯Yt∥2. Then on the set\n\b\n∥Xt −Yt∥2 ∨∥¯Xt −¯Yt∥2 < ϵ\n\t\n, for\n16\nall t ≤Tδ−1d:\n∥Yt −¯Yt∥2 ≤δ\nd\nt−1\nX\ni=0\n∥⃗m(∇Φ(Xi) −∇Φ( ¯Xi))∥2 + δ\nd\nt−1\nX\ni=0\n∥⃗m(∇H(Xi, yi))∥2\n≤δ\nd\nt−1\nX\ni=0\nK∥⃗m(Xi −¯Xi)∥2 + δ\nd\nt−1\nX\ni=0\n∥⃗m(∇H(Xi, yi))∥2\n≤2TKϵ + δ\nd\nt−1\nX\ni=0\nK∥Yi −¯Yi∥2 + δ\nd\nt−1\nX\ni=0\n∥⃗m(∇H(Xi, yi))∥2\n≤(2TK + 1)ϵ + δ\nd\nt−1\nX\ni=0\nK∥Yi −¯Yi∥2\nwith probability 1 −o(1), by applying Doob’s inequality and noting the use of (3). Thus applying the discrete\nGronwall inequality Gross [1967], we obtain:\nsup\nt≤T δ−1d\n∥Yt −¯Yt∥2 ≤(2TK + 1)ϵeKT\n(6)\nFor any γ > 0 the above can be made less than γ/5 by choice of ϵ(γ, T). We now let T be such that\nsup\nT δ−1d≤t≤N\n∥(m1( ¯Yt), m2( ¯Yt)) −(1, 0)∥2 < γ/5\n(7)\nThis T exists as a constant (which is independent of d) as a result of Assumption 3.2 and the fact that\n(m1( ¯Y d\n0 ), m2( ¯Y d\n0 ))d≥1 →(minit\n1\n, minit\n2\n). To better understand this, recall that with constant initialization, the\n2-d population dynamics of m1 and m2 are otherwise unaffected (other than through stepsize and number of\nsteps) by the dimension. By Assumption 3.2, we have that the population dynamics converge to the intended\nsolution at (m1, m2) = (1, 0) as d →∞. Now consider:\nsup\nT δ−1d≤t≤N\n∥Yt −(1, 0)∥2 ≤∥YT δ−1d −δ\nd\nt−1\nX\ni=T δ−1d\n⃗m(∇Φ(Xi)) −(1, 0)∥2 + ∥δ\nd\nt−1\nX\ni=T δ−1d\n⃗m(∇H(Xi, yi))∥2\n≤2γ/5 + ∥δ\nd\nt−1\nX\ni=T δ−1d\n⃗m(∇H(Xi, yi))∥2\nThe above bound comes from applying triangle inequality to separate out the martingale term and then\nnoting that at time Tδ−1d we have Yt is within γ/5 of ¯Yt which is within γ/5 of (1, 0) and the gradient with\nrespect to the population loss only moves Yt closer to (1, 0).\nsup\nT δ−1d≤t≤N\n∥Yt −¯Yt∥2\n≤\nsup\nT δ−1d≤t≤N\n(∥Yt −(1, 0)∥2 + ∥¯Yt −(1, 0)∥2)\n≤2γ/5 + ∥δ\nd\nt−1\nX\ni=T δ−1d\n⃗m(∇H(Xi, yi))∥2 + γ/5 ≤γ\nwith probability 1 −o(1). This follows from another application of Doob’s inequality onto the projection\nof the high dimensional martingale onto the two fixed directions of interest e1, e2 keeping in mind Lemma\nA.1. We have thus shown that for any γ > 0, with probability 1 −o(1), that supt≤N ∥(m1(Xt), m2(Xt)) −\n(m1( ¯\nXt), m2( ¯\nXt))∥2 < γ which concludes the proof.\nProposition A.4. Suppose Assumption 3.2 holds with point m∗. Further η1 ≥m∗\n1 and η2 ≤m∗\n2. The the\nsequence of PCA estimators (ˆvd)d≥1 of vd each obtained with Nd = α ∗d samples,α = ω(1), is Effective for\npopulation flow.\n17\nProof. We will use well-known facts about PCA in high dimensions, which we recall for the reader’s convenance\nin section B of the appendix. If we consider using a fixed linear portion of our samples for conducting PCA,\nwe can choose the fraction γ = d/N to be any constant we desire. Choosing γ sufficiently small, we can ensure\nboth that γ < λ2 and m1(ˆv) > m∗\n1 and m2(ˆv) < m∗\n2. To see this consider the following: Let △1 = |m∗\n1 −η1|,\n△2 = |m∗\n2 −η2| , ϵ = 1\n8 min(△2\n1, △2)2. We have:\n∥ˆv −v∥2\n2 = ∥v∥2\n2 + ∥ˆv∥2\n2 −2v · ˆv ≤2(1 −1 −γ/λ2\n1 + γ/λ2 ) + ϵ < 1\n4min(△2\n1, △2\n2)\nfor small enough γ. The second equality follows for large d using a well known result regarding the limiting\ncorrelation of v and ˆv as can be seen in Bandeira et al. [2020].\nThus by triangle inequality, we have that m1(ˆv) ≥m∗\n1 and |m2(ˆv)| ≤m∗\n2 for sufficiently large d.\nLetting the choice of γ tend to 0, it is clear the limit of (m1(X(d)\n0 ), m2(X(d)\n0 )) exists and is equal to\n(m1(v), m2(v)) = (η1, η2).\nA.2\nProof of Theorem 3.4\nTheorem 3.4. Suppose that f satisfies the following:\nEf ′′(g) = Ef ′(g) = 0, E ∂2\n∂g2 f(g)2 > 0\nfor g ∼N(0, 1). Then for spherical SGD with N = αd steps where α ≪d, αδ2 = O(1), for the sequence of\ninitializations X(d)\n0\n∼Uniform(Sd−1) we have that: |m1(XN)| →0 in probability, as d →∞.\nThe overall strategy of the proof is to show that for for any γ > 0, if ˆXt = (m1(Xt), m2(Xt)) is in the\nd−1/6-ball, i.e. {x ∈R2 : ∥x∥2 < d−1/6}, we have that ˆXt enters the 1\n2d−1/6-ball or ’times out’ at t = N,\nbefore it leaves the γ-ball with probability 1 −o(1).\nWe want to analyze the population dynamics and we will consider doing so via a second order Taylor\nexpansion of the population loss\nϕ(x1, x2) = E[f(a1x1 + a2x2 +\nq\n1 −x2\n1 −x2\n2g) −f(a1)]2 + Eϵ2\nin (m1(X), m2(X)) = (x1, x2), around the origin.\nNoting that, under our assumptions on f, we can\ndifferentiate under the expectation, we perform the following computations:\n∂ϕ(0, 0)\n∂x1\n= E2[f(g)f ′(g)a1 −f(a1)f ′(g)a1]\n∂ϕ(0, 0)\n∂x2\n= E2[f(g)f ′(g)a2 −f(a1)f ′(g)a2]\n∂2ϕ(0, 0)\n∂x2\n1\n= E2[(f ′(g)2 + f ′′(g)f(g))a2\n1 −f(g)f ′(g)g −f(a1)f ′′(g)a2\n1 + f(a1)f ′(g)g]\n∂2ϕ(0, 0)\n∂x2\n2\n= E2[(f ′(g)2 + f ′′(g)f(g))a2\n2 −f(g)f ′(g)g −f(a1)f ′′(g)a2\n2 + f(a1)f ′(g)g]\n∂2ϕ(0, 0)\n∂x1∂x2\n= E2[(f ′(g)2 + f ′′(g)f(g))a1a2 −f(a1)f ′′(g)a1a2]\nEvaluating the expectations we have:\n∂\n∂x1\nϕ(0, 0) = E2[f(g)f ′(g)a1 −f(a1)f ′(g)a1] = −2(1 + λη2\n1)Ef ′(a1)Ef ′(g)\nUsing that a1 ⊥g, Ea1 = 0, and Gaussian integration by parts: Ef(a1)a1 = var(a1)Ef ′(a1) for a1\nGaussian\n18\n∂\n∂x2\nϕ(0, 0) = E2[f(g)f ′(g)a2 −f(a1)f ′(g)a2] = −2λη1η2Ef ′(a1)Ef ′(g)\nUsing that Ef(a1)a2 = E[f(a1)Ea2|a1] =\nλη1η2\n1+λη2\n1 Ef(a1)a1.\nWhich follows by another application of\nGaussian integration by parts and given the following decomposition of a2:\na2 =\nλη1η2\n1 + λη2\n1\na1 +\ns\n1 + λη2\n2 −(λη1η2)2\n1 + λη2\n1\na⊥\n1\nwhere a⊥\n1 ⊥a1 and var(a⊥\n1 ) = 1.\n∂2\n∂x2\n1\nϕ(0, 0) = E2[(f ′(g)2 + f ′′(g)f(g))a2\n1 −f(g)f ′(g)g −f(a1)f ′′(g)a2\n1 + f(a1)f ′(g)g]\n(8)\n= 2[(1 + λη2\n1)(Ef ′(g)2 + f ′′(g)f(g)) −Egf(g)f ′(g) −Ef ′′(g)E(f(a1)(a2\n1 −1)]\n(9)\n= 2[(1 + λη2\n1)Egf(g)f ′(g) −Egf(g)f ′(g) −Ef ′′(g)E(f(a1)(a2\n1 −1)]\n(10)\n= 2[λη2\n1Egf(g)f ′(g) −Ef(g)(g2 −1)E(f(a1)(a2\n1 −1)]\nFrom (8) to (9) above, we used the fact that Ef ′(g)2 + f ′′(g)f(g) = Egf(g)f ′(g) which simply follows\nfrom Gaussian integration by parts. From (9) to (10) we used the fact that Ef ′′(g) = Ef(g)(g2 −1) which\nfollows from two applications of Gaussian integration by parts.\n∂2\n∂x2\n2\nϕ(0, 0) = E2[(f ′(g)2 + f ′′(g)f(g))a2\n2 −f(g)f ′(g)g −f(a1)f ′′(g)a2\n2 + f(a1)f ′(g)g]\n= 2[λη2\n2Egf(g)f ′(g) −Ef ′′(g)Ef(a1)(a2\n2 −1)]\n∂2\n∂x1∂x2\nϕ(0, 0) = E2[(f ′(g)2 + f ′′(g)f(g))a1a2 −f(a1)f ′′(g)a1a2]\n= 2λη1η2[Egf(g)f ′(g) −Ef ′′(g)Ef(a1)a2\n1(1 + λη2\n1)−1]\nUsing that Ef(a1)a1a2 = λη1η2\n1+λη2\n1 Ef(a1)a2\n1 by tower property and decomposition of a2. We use the above\ncalculations to compute the second order Taylor expansion of ϕ around (0, 0):\nϕ(x1, x2) = ϕ(0, 0) +\n∂\n∂x1\nϕ(0, 0)x1 +\n∂\n∂x2\nϕ(0, 0)x2\n+ 1\n2( ∂2\n∂x2\n1\nϕ(0, 0)x2\n1 + ∂2\n∂x2\n2\nϕ(0, 0)x2\n2 + 2\n∂2\n∂x1∂x2\nϕ(0, 0)x1x2) + R(x1, x2)\nwhere R(x1, x2) = O(∥(x1, x2)∥3\n2). Which allows us to compute the derivatives of ϕ(x1, x2) as follows:\n∂ϕ(x1, x2)\n∂x1\n=\n∂\n∂x1\nϕ(0, 0) + ∂2\n∂x2\n1\nϕ(0, 0)x1 +\n∂2\n∂x1∂x2\nϕ(0, 0)x2 + O(∥(x1, x2)∥2\n2)\n= −2(1 + λη2\n1)Ef ′(a1)Ef ′(g)\n+ 2[λη2\n1Egf(g)f ′(g) −Ef(g)(g2 −1)E(f(a1)(a2\n1 −1)]x1\n+ 2λη1η2[Egf(g)f ′(g) −Ef ′′(g)Ef(a1)a2\n1(1 + λη2\n1)−1]x2 + O(∥(x1, x2)∥2\n2)\n∂ϕ(x1, x2)\n∂x2\n=\n∂\n∂x2\nϕ(0, 0) + ∂2\n∂x2\n2\nϕ(0, 0)x2 +\n∂2\n∂x1∂x2\nϕ(0, 0)x1 + O(∥(x1, x2)∥2\n2)\n= −2λη1η2Ef ′(a1)Ef ′(g)\n+ 2[λη2\n2Egf(g)f ′(g) −Ef ′′(g)Ef(a1)(a2\n2 −1)]x2\n+ 2λη1η2[Egf(g)f ′(g) −Ef ′′(g)Ef(a1)a2\n1(1 + λη2\n1)−1]x1 + O(∥(x1, x2)∥2\n2)\n19\nUsing our assumptions on Ef ′′(g), Ef ′(g), we have the simplified system:\n∂ϕ(x1, x2)\n∂x1\n= 2λη2\n1Egf(g)f ′(g)x1 + 2λη1η2Egf(g)f ′(g)x2 + O(∥(x1, x2)∥2\n2)\n(11)\n∂ϕ(x1, x2)\n∂x2\n= 2λη2\n2Egf(g)f ′(g)x2 + 2λη1η2Egf(g)f ′(g)x1 + O(∥(x1, x2)∥2\n2)\n(12)\nWe recall that the spherical gradient correction term is higher order:\n(∂ϕ(x1, x2)\n∂x1\nx1 + ∂ϕ(x1, x2)\n∂x2\nx2)X\nAnalyzing the linearized system, given by (11) and (12), we see that the first order terms are orthogonal\nto the line L = {tv⊥\n2 : t ∈R} = {(x1, x2) : x2 = −η1\nη2 x1}, where v⊥\n2 = (η2, −η1). On the line L, the first order\nterms are both 0, and hence the magnitude of the first order effects tends to 0 as (x1, x2) tends to the line L.\nThe magnitude of the first order terms, exceeds the magnitude of all higher order terms when the distance of\n(x1, x1) to the line L, exceeds c∥(x1, x2)∥2\n2 for some constant c.\nBefore proceeding we define some notation. Let us introduce the four quadrants Q1 = {(x, y) : x > 0, y >\n0}, Q2 = {(x, y) : x < 0, y > 0}, Q3 = {(x, y) : x < 0, y < 0}, Q4 = {(x, y) : x > 0, y < 0}. We will consider\nthe variable ˆXt = ⃗m(Xt) = (m1(Xt), m2(Xt)) = (x1, x2)t.\nWe define two operators TL1 and TL2 as follows: TL1( ˆX) = v⊥\n2 · PL where PL is the orthogonal projection\noperator onto L. TL1 is defined such that for ˆX in Q4, TL1( ˆX) > 0. We define TL2 = −TL1 so that\nTL2( ˆX) > 0 for ˆX ∈Q2.\nWe also define the ’left and right half-spaces’ with respect to L as follows:\nHl = {(x1, x2) : x2 < −η1\nη2 x1} and Hr = {(x1, x2) : x2 > −η1\nη2 x1}.\nWe now define two additional operators T ⊥\nr and T ⊥\nl\nas follows. Let v2 = (η1, η2) and then let T ⊥\nr ( ˆX) =\nv2 · (I −PL)( ˆX). T ⊥\nr measures the signed distance of ˆX to the line L, with T ⊥\nr ( ˆX) > 0 when ˆX ∈Q1. Note\nthat TL1, TL2, T ⊥\nr , T ⊥\nl\nare all linear operators.\nWith our notation above we now define the following set: C = {(x1, x2) : |T ⊥\nl (x1, x2)| < c∥(x1, x2)∥2\n2}.\nFor ˆX /∈C, we have that the first order terms in the gradient of the population loss, exceed the higher order\nterms.\nLet L∗\n1 be the line {(x1, x2) : x2 = −η1/2η2x1} if η1 > η2 or {(x1, x2) : x2 = −2η1/η2x1} if η1 < η2. In\nessence this line is the line half way between L and whichever quadrant boundary is closer to L. We define it’s\ncounter part L∗\n2 as the line with the reciprocal slope, i.e. the slope of L∗\n2 is 1 over the slope of L∗\n1. Without\nloss of generality we assume η1 < η2. Similar to the ’left and right half-spaces’ with respect to L defined\nabove, we can define the left and right half-spaces of L∗\n1 and L∗\n2, Hl1, Hr1, Hl2, Hr2. Then we consider the set\nQ∗= (Hl2 ∩Hr1) ∪(Hr2 ∩Hl1). We now notice that if we consider the set C intersect the γ-ball, that for\nsufficiently small γ, the intersection is a subset of Q∗. For a better understanding of the definitions given\nabove see Figure 2. We will make use of the set C in Lemma A.7 below.\nWe now state three lemmas which we will use to complete our proof of Theorem 3.4. We defer the proofs\nof these lemmas until after the proof of Theorem 3.4. Lemma A.8 will tell us that when ˆXt is in Q1 or Q3, it\nmust leave, entering Q2 or Q4 before it’s norm grows by d−1/3. Lemma A.6 will tell us that ˆX cannot leave\nthe γ-ball through Q2 or Q3 without first exiting the quadrant, provided it arrived small enough. Lemma\n3.4 provides a bound on how far ˆX can move away from the line L before entering the set C defined above.\nTogether these lemmas allow us to control the magnitude of ˆXt before it enters the set C, and then show that\nonce ˆXt is in C it cannot escape the γ-ball before re-entering the 1/2d−1/6-ball.\n20\nFigure 2: A visual to help guide in understanding the definitions of L1, L2 and the set C. It is clear that\nregardless of η1, by decreasing the vale of γ, we find that C intersect the γ-ball is a subset of Q∗\n21\nFigure 3: A visual to help guide in the understanding of the proof of Theorem 3.4 and the use of the three\nlemmas. When initialized in Q3 in the d−1/6-ball, ˆX must leave Q3 before leaving the 3/2d−1/6-ball. Suppose\nit enters Q4, the red line provides a boundary ˆ\nXt cannot cross before entering the set C (see Figure 2).\nLemma A.6. There exists some sequence Kd growing to infinity such that under the assumptions of Theorem\n3.4 and restricting to the set E1 = {supt≤N | δ\nd\nPt\ni=1 ∇TL1 ⃗m((H(Xi)))| < 1\n2Kd/\n√\nd}, we have that if ˆXt∗∈Q4,\nand ∥ˆXt∗∥2 ≤d−1/10, then\n∥ˆ\nXt∥2 < γ, ∀t ∈[t∗, min(τQ−\n4 , N)]\n(13)\nwhere τQ−\n4 is the stopping time for the next time ˆ\nXt leaves Q4. On that event, the same statement is true\nwith Q2 in place of Q4 as well. Further, the P(E1) = 1 −o(1).\nThis lemma says that under the set E1, for ˆ\nXt with norm less than d−1/10 and in Q4, ˆ\nXt must leave the\n22\nquadrant Q4 before it leaves the γ-ball. Similarly, for ˆX in Q2 with norm less than d−1/10, ˆX exits Q2 before\nit leaves the γ-ball.\nLemma A.7. Letting ˆX⊥= T ⊥\nl ( ˆX), under the assumptions of Theorem 3.4 under the event: E2 =\n{supt≤N ∥δ\nd\nPt\ni=1 ∇T ⊥\nl (⃗m(H(Xi)))∥2 < 1\n2d−1/3}, when ˆXt∗∈Hl, ∥ˆXt∗∥2 ≤2d−1/6 we have that:\n∥ˆX⊥\nt ∥2 ≤∥ˆX⊥\nt∗∥2 + d−1/3, ∀t ∈[t∗, min(τC, τ1/2d−1/6, N)]\nwhere τC is the next time ˆXt enters the set C and τ1/2d−1/6 is the next time ˆXt enters the 1/2d−1/6-ball.\nFurther, the same statement holds when replacing T ⊥\nl\nwith T ⊥\nr\nand Hl with Hr.\nThis lemma says that if ˆXt is in the 2d−1/6 ball, then it’s distance from the line L can only increase by\nup to d−1/3 before it enters the set C or re-enters the 1/2d−1/6-ball.\nLemma A.8. Under the assumptions of Theorem 3.4 and on the sets E∗\nj = {supt≤N | δ\nd\nPt\ni=1 ∇H(Xi) · ej| <\n1\n10d−2/5}, j = 1, 2, when ˆXt∗∈Q3, we have that:\n∥ˆXt∥2 ≤∥ˆXt∗∥2 + d−1/5, ∀t ∈[t∗, τQ−\n3 ]\nWhere τQ−\n3 is the stopping time for the next time ˆXt leaves the quadrant Q3. Further the above statement\nholds replacing Q3 with Q1.\nThis lemma says that the maximum amount the norm of ˆXt can increase while in Q3 before leaving is\nd−1/5.\nProof of Theorem 3.4. We intend to show that for any γ > 0, P(supt≤N ∥(m1(Xt), m2(Xt)))∥2 > γ) →0 as\nd →∞.\nThe proof of this result will make use of the three lemmas above to show that if ˆXt is in the d−1/6-ball, it\nwill re-enter the d1/2−1/6 ball before it leaves the γ ball. To make use of the three lemmas above, we note\nthat the sets considered in these lemmas: E1, E2, E∗\n1, E∗\n2 are all probability 1 −o(1) by Doob’s inequality and\nhence so is their intersection E = E1 ∩E2 ∩E∗\n1 ∩E∗\n2. We now remind the reader that random initializations\nyield correlations on the order of m1(X0), m2(X0) = O( 1\n√\nd) ≪d−1/6. We now argue that under the set E\nfor ˆXt in the d−1/6 −ball, we have deterministically that ˆXt re-enters the 1/2d−1/6 ball or reaches t = N\nbefore it leaves the γ-ball, hence by the Markov property, this will conclude the proof.\nWe will prove this in a case by case manner, firstly considering the case that ˆXt is in the d−1/6-ball and\nin Q3.\nBy Lemma A.8, ˆXt must leave Q3 before it can leave the 3/2d−1/6-ball. Hence for ˆXt to leave the γ-ball,\nit must first exit the quadrant. So suppose ˆXt enters Q4 (the case where ˆXt enters Q2 follows by a similar\nargument). Since it entered small enough (by small enough we mean ∥ˆXt∥2 ≤d−1/10, allowing us to invoke\nlemma A.6), by Lemma A.6, it cannot leave the γ-ball before exiting the quadrant. Additionally, by Lemma\nA.7, it’s distance from L cannot increase by more than d−1/3 before entering the set C. Now also notice\nthat for ˆXt to enter Q3 \\ 2d−1/6-ball, it would require it’s distance from L to increase by a quantity that is\norder d−1/6 ≫d−1/3 which cannot happen before ˆXt enters C. Thus ˆXt cannot exit the γ-ball through Q3\nuntil entering C. ˆXt may re-enter Q3 or even Q2, but any entry to Q4 or Q2 requires ∥ˆXt∥2 ≤d−1/10 and\nhence by by lemma A.6, ˆXt cannot exit the γ-ball through quadrants Q2 or Q4. Hence ˆXt cannot exit the\nγ-ball through any quadrant without re-entering the 1/2d−1/6-ball, timing out or first entering the set C. So\nsuppose ˆXt enters C in Q4. ˆXt still cannot leave the γ −ball until first leaving the quadrant. But now recall\nthat C is contained in Q∗and hence to leave the quadrant without first re-entering the 1/2d−1/6-ball, requires\nˆXt to exit Q∗, and further move a distance on the order of d−1/6 away from the line, which by lemma A.7\ncannot happen. This completes the proof for the case where ˆXt is in Q3. Note that the case where ˆXt is in\nQ1 follows by symmetric arguments. Now finally notice that the cases where ˆXt is in either Q2 or Q4 follow\nby the same arguments as above, noting that the Q1 and Q3 cases reduce to the Q2 and Q4 cases.\n23\nProof of Lemma A.6. We will prove the case where ˆX ∈Q4 using TL1 and simply note that the case of\nˆX ∈Q2 with TL2 follows by the same arguments.\nNote that for any η1 and any value γ > 0, ∃ϵγ such that for x ∈Q4, ∥x∥2 > γ ⇒TL1(x) > ϵγ. We will\nnow show that the event E1 contains the following event:\n{TL1 ˆ\nXt < ϵγ, ∀t ∈[t∗, min(τQ−\n4 , N)]}\nObserve that for such t ∈[t∗, min(τQ−\n4 , N)]:\nTL1( ˆXt) ≤TL1( ˆXt−1 −δ\nd ⃗m(∇Φ(Xt−1)) −δ\nd ⃗m(∇H(Xt−1)))\n≤TL1( ˆXt∗) −TL1(δ\nd\nt−1\nX\ni=t∗\n⃗m(∇Φ(Xi)) + δ\nd\nt−1\nX\ni=t∗\n⃗m(∇H(Xi)))\nNote that this follows from the fact that TL1 ˆXt > 0 for as long as ˆXt ∈Q4, and that the radius of Xt plus\na gradient step, is deterministically greater than one as it is a spherical gradient. Now given ˆXt ∈Q4 and\nrecalling that the first order terms in the gradient are orthogonal to the line L, we have that TL1(⃗m(∇Φ(Xt)))\nis at most second order in TL1( ˆXt). Hence:\nTL1( ˆXt) ≤TL1( ˆXt∗) + cδ\nd\nt−1\nX\ni=t∗\n|TL1 ˆXi|2 −TL1(δ\nd\nt−1\nX\ni=t∗\n⃗m(∇H(Xi))))\nFor some constant c. Note that the event{supt≤N | δ\nd\nPt\ni=1 ∇TL1(⃗m(H(Xi)))| < 1\n2Kd/\n√\nd} contains the event\n{supj,t:1≤i≤t≤N | δ\nd\nPt\ni=j ∇TL1(⃗m(H(Xi)))| < Kd/\n√\nd}. Thus we have that:\nTL1( ˆXt) ≤TL1( ˆXt∗) + cδ\nd\nt−1\nX\ni=t∗\n|TL1 ˆXi|2 + Kd/\n√\nd, ∀t ∈[t∗, min(τQ−\n4 , N)]\nIt follows from this inequality that TL1 ˆXt < ϵγ, ∀t ∈[t∗, min(τQ−\n4 , N)]. To see this, observe that by the\ndiscrete Bihari-LaSalle inequality C that for some constant c1:\nTL1( ˆXt) ≤2Kd\n√\nd\n(1 −c1δKdd−3/2t)−1\nso long as TL1( ˆXt∗) ≤Kd\n√\nd (which is true so long as d−1\n10 ≤Kd\n√\nd). For sufficiently large d, the right hand\nside above is smaller than ϵγ for all t ≤ˆt where:\nˆt = K−ϵ\nd δ−1d3/2\nfor some ϵ > 0 sufficiently small, provided Kd is growing slower than d1/2−ζ for some ζ > 0.\nThus if we choose Kd to be diverging appropriately slowly, we have that ˆt > N (recalling that\n√α = o(\n√\nd) and √α = O(δ−1)).\nWe thus choose Kd such that TL1( ˆXt∗) ≤d−1\n10 ≤\nKd\n√\nd and Kd ≤\nd1/2−ζ (for some ζ > 0), for example Kd = d1/2−1/11, then Kd/\n√\nd = d−1/11 ≫d−1/10 and further\nP(supt≤N | δ\nd\nPt\ni=1 ∇TL1 ⃗m((H(Xi)))| < 1\n2Kd/\n√\nd) ≥1 −O(αδ2/K2\nd) = 1 −o(1).\nProof of Lemma A.7. We prove the result in the case that ˆXt∗∈Hl where ˆX⊥\nt∗= T ⊥\nl ( ˆXt∗) noting that this\nimplies ˆXt ∈Hl ∀t ∈[t∗, min(τC, τ1/2d−1/6, N)]. The case where ˆXt∗∈Hr and replacing T ⊥\nl\nwith T ⊥\nr , follows\nby an identical argument. Observe that\nˆX⊥\nt ≤ˆX⊥\nt−1 −T ⊥\nl (⃗m(δ\nd∇Φ(Xt−1))) −T ⊥\nl (⃗m(δ\nd∇H(Xt−1))) ≤ˆX⊥\nt−2 −T ⊥\nl (⃗m(δ\nd∇H(Xt−1) + δ\nd∇H(Xt−2)))\nOnce again the first inequality comes from the fact that the spherical gradient is always greater than 1 and\nˆX⊥\nt > 0 for ˆXt in Hl. The second inequality follows from two observations. The first observation is that\n24\nremoving the spherical projections provides an upper bound given ˆX⊥\nt > 0 in the time interval considered\nhere. The second observation is that removing the gradient of the population loss term, also provides an\nupper bound. This follows from our analysis of the population dynamics (provided in the discussion before\nthe proof), which told us that the first order terms in the population dynamics, point orthogonally towards\nthe line L. Hence when the first order terms exceed the higher order terms (i.e. when ˆ\nXt /∈C), the gradient\nof the population loss term, is negative under the operator T ⊥\nl\nwhen ˆX ∈Hl.\nExpanding the above we have:\n∥ˆX⊥\nt ∥2 ≤∥ˆX⊥\nt∗∥2 + ∥T ⊥\nl (δ\nd\nt\nX\ni=t∗\n∇H(Xi))∥2 ≤∥ˆX⊥\nt∗∥2 + d−1/3\nunder E2.\nProof of Lemma A.8. We prove the case of Q1 and make note that the case of Q3 follows by a symmetric\nargument. We note that in Q1 we have that m1(Xt), m2(Xt) > 0 and ∇Φ(Xt) · e1 > 0, ∇Φ(Xt) · e2 > 0 (this\nfollows from the discussion on the population dynamics, given prior to the proof of 3.4). We now consider\nm1(Xt) = Xt · e1:\nXt · e1 ≤Xt−1 · e1 −δ\nd∇Φ(Xt−1) · e1 −δ\nd∇H(Xt−1) · e1\n≤Xt∗· e1 −δ\nd\nt\nX\ni=t∗\n∇Φ(Xi) · e1 −δ\nd\nt\nX\ni=t∗\n∇H(Xi) · e1\n≤Xt∗· e1 −δ\nd\nt\nX\ni=t∗\n∇H(Xi) · e1\n≤Xt∗· e1 + 1\n5d−2/5\nThe inequalities above follow by similar arguments to the previous two lemmas noting that the sign of Xt · e1\nand the gradients are always the same while in Q1. The bound on the martingale term follows under the set\nE∗\n1. A similar argument follows for Xt · e2. We conclude the proof noting that:\n∥ˆXt∥2 ≤\nr\n(Xt∗· e1 + 1\n5d−2/5)2 + (Xt∗· e2 + 1\n5d−2/5)2\n≤\nq\n(Xt∗· e1)2 + (Xt∗· e2)2 + d−2/5\n≤\nq\n∥ˆXt∗∥2\n2 + d−2/5 + 2d−1/5∥ˆXt∗∥2\n=\nq\n(∥ˆXt∗∥2 + d−1/5)2\n= ∥ˆXt∗∥2 + d−1/5\nA.3\nProof of Theorem 3.5\nTheorem 3.5. Suppose that f satisfies the following:\nEf ′′(g) = Ef ′(g) = 0, E ∂2\n∂g2 f(g)2 > 0\nfor g ∼N(0, 1). When η1 = 1, for spherical SGD with N = αd steps where α = ω(1), αδ2 ≪d1/3 , we have\nthat there exists some r > 0 such that for all sequences of initializations X(d)\n0\n:\n\f\f\fm1(X(d)\n0 )\n\f\f\f < r ∀d, then we\nhave that:\n|m1(XN)| →0\n25\nin probability as d →∞. Further we have that for all ϵ > 0:\nP(sup\nt≤N\n|m1(Xt)| > r + ϵ) →0\nin probability as d →∞.\nProof. From the Taylor expansion used in the proof of Theorem 3.4, specifically equations (11) and (12),\nwe see that for η1 = 1 ⇒η2 = 0, the first order term in the population dynamics sends x1 to 0. This\nis to say that ∃r > 0 : ∀X s.t. m1(X) ∈B(0, 2r) (the 1-dimensional open ball of radius 2r) we have that\n∇Φ(X) · v0sgn(X · v0) > 0. This tells us that the population gradient step sends x1 towards 0, for all x1 in\nthe specified ball. We note that the value of r is independent of the dimension.\nWe now start by restricting to the set E = {supt≤N | δ\nd\nPt\ni=1 ∇H(Xi) · v0| < d−1/3} . We note that by\nDoob’s inequality we have that:\nP(sup\nt≤N\n|δ\nd\nt\nX\ni=1\n∇H(Xi) · v0| > d−1/3) ≤C αδ2\nd1/3 = o(1)\n(recalling the assumption that αδ2 ≪d1/3. Then note that the set E implies the set {sup0≤j≤t≤N | δ\nd\nPt\ni=j ∇H(Xi)·\nv0| < 2d−1/3}.\nWe now claim that under the set E, we have that deterministically m1(XN) →0 as d →∞. Consider\nthe sequence of stopping times, (τk)k≥1, corresponding to where m1(Xt) crosses zero, i.e., let τk = inf{t >\nτk−1 : sgn(m1(Xt)) ̸= sgn(m1(Xt−1))} and τ0 = 0. Note the maximum single step that m1(Xt) can take is\nbounded and tending to 0 in the dimension (recall the stepsize is δ\nd and the gradient of the loss is bounded,\nas seen in Lemma A.1 and the proof of Theorem 3.3 . Thus we have that m1(Xτk) →0 as d →∞whenever\nτk < N. This is because the distance of m1(Xτk) to 0 is bounded by the maximum single step.\nWe will proceed by breaking up the interval [0, N] into the union of ∪k≥0[τk, min(τk+1, N + 1)). We\nseparately consider the first interval [0, min(τ1, N + 1)) and each other interval [τk, min(τk+1, N + 1)), k ≥0,\nstarting with the latter.\nFix k > 0 and suppose that t ∈[τk, min(τk + 1, N + 1)). Suppose, without loss of generality, that\nm1(Xτk) ≥0. Now for t ∈[τk, min(τk+1, N + 1)), this process remains positive and we have that:\nm1(Xt+1) = Xt+1 · v0\n=\n\u0000Xt −δ\nd (∇Φ(Xt) + ∇H(Xt))\n\u0001\n· v0\n∥Xt −δ\nd (∇Φ(Xt) + ∇H(Xt)) ∥2\n≤\n\u0012\nXt −δ\nd (∇Φ(Xt) + ∇H(Xt))\n\u0013\n· v0\n≤(Xτk −δ\nd\nt\nX\ni=τk\n(∇Φ(Xi) + ∇H(Xi))) · v0\n≤(Xτk −δ\nd\nt\nX\ni=τk\n∇Φ(Xi)) · v0 + 2d−1/3\n≤m1(Xτk) + 2d−1/3\n≤4d−1/3\nThe first inequality follows from the fact that the spherical gradient step always results in a point with\nnorm greater than or equal to 1. The second inequality follows from the fact that removing the spherical\nprojections for each m1(Xt) provides an upper bound as the process is positive over the time interval\nconsidered. The third inequality simply applies the restricting set E. The fourth inequality comes from the\nfact that the ∇Φ(Xt)) · v0 > 0 whenever 0 < m1(Xt) < 2r. The final inequality comes from the maximum\none step change of m1(Xt) (which is O( δ\nd) ≪d−1/3) which provides an upper bound on m1(Xτk) for k > 0.\n26\nNow to see that m1(Xt) < 2r, ∀t ∈[τk, min(τk+1, N + 1)), we suppose for the sake of a contradiction,\nthat m1(Xt) > 2r for some time t∗∈[τk, min(τk+1, N + 1)) and further t∗is the first time m1(Xt) exceeds\n2r. Repeating the inequalities above (noting that m1(Xt) < 2r, ∀t ∈[τk, t∗)) we have that\nm1(Xt∗) ≤4d−1/3 ≤2r\na contradiction. We thus have that for any interval [τk, min(τk+1, N + 1)), the value of m1(Xt) is upper\nbounded by 4d−1/3. Note that we only showed this for the case m1(Xτk) > 0, but the case m1(Xτk) < 0\nfollows by a similar argument.\nThe first statement of the proof, i.e. that m1(XN) →0 in probability as d →∞would then follow so long\nas τ1 < N + 1. Which is to say that if the process m1(Xt) crosses 0 at least once, it will remain within d−1/3\nof 0 and hence be there at time N, proving the first statement of the Theorem. The second statement of the\nproof would also follow so long as supt≤min(τ1,N+1) |m1(Xt)| < r + ϵ for any fixed ϵ > 0, deterministically\nunder the restricted set E, which has probability 1 −o(1).\nWe now proceed to finish the proof by considering the time interval [0, min(τ1, N + 1)). We once again\nassume without loss of generality that m1(X0) > 0, i.e. that the process is positive over the time interval\nconsidered. Then we again have for t in this interval:\nm1(Xt+1) = Xt+1 · v0\n=\n\u0000Xt −δ\nd (∇Φ(Xt) + ∇H(Xt))\n\u0001\n· v0\n\r\r\u0000Xt −δ\nd (∇Φ(Xt) + ∇H(Xt))\n\u0001\n· v0\n\r\r\n2\n≤\n\u0012\nXt −δ\nd (∇Φ(Xt) + ∇H(Xt))\n\u0013\n· v0\n≤(X0 −δ\nd\nt\nX\ni=0\n∇Φ(Xi)) · v0 + δ\nd\nt\nX\ni=0\n∇H(Xi) · v0\n≤(X0 −δ\nd\nt\nX\ni=0\n∇Φ(Xi)) · v0 + 2d−1/3\nOnce, again, so long as m1(Xt) ≤2r, we have that ∇Φ(Xi)) · v0 > 0. By the same contradiction based\nargument as before, we can show that m1(Xt) ≤2r for all t ∈[0, min(τ1, N + 1)) (recalling that m1(X0) < r).\nHence we have the bound:\nm1(Xt+1) ≤r + 2d−1/3\nWhich completes the proof of the second statement of the Theorem, i.e. that for all ϵ > 0:\nP(sup\nt≤N\n|m1(Xt)| > r + ϵ) →0\nin probability as d →∞. We now complete the proof of the first statement of the theorem by showing that\nm1(XN) →0 in probability as d →∞in the case that τ1 > N. Suppose for the sake of contradiction that\nthere exists some c > 0 such that:\nlim sup\nd→∞\nP( inf\nt≤N |m1(Xt)| > c) > 0\n(14)\nBefore proceeding, consider the converse of this assumption which is that for every c > 0:\nlim sup\nd→∞\nP( inf\nt≤N |m1(Xt)| > c) = 0\n⇒P( inf\nt≤N |m1(Xt)| ≤c) = 1 −o(1)\nThen fixing any c > 0 and restricting to the 1 −o(1) probability set {inft≤N |m1(Xt)| ≤c} ∩E, let\nτc = inf{t : |m1(Xt)| ≤c}. Assume without loss of generality that m1(Xt) > 0, t ∈[0, τc]. Once again\n27\nrepeating the same sequence of inequalities used previously and the same contradiction argument that allows\nus to invoke those inequalities, we have that:\nm1(XN) ≤m1(Xτc) + 2d−1/3 ≤c + 2d−1/3 ≤2c\nWhich is to say that for any c > 0, m1(XN) ≤2c with probability 1 −o(1), which would complete the\nproof. Hence, we return to the assumption given by (14). Now recall that we have already shown that\nrestricting to E ∩{τ1 < N + 1} we have m1(Xt) ≤4d−1/3 deterministically, we thus restrict to the set\nE ∩{τ1 > N}. Now we may assume that 0 ≤m1(Xt) ≤2r for all t ∈[0, N). The lower bound follows under\n{τ1 < N + 1}, the upper bound m1(Xt) < 2r holds generally with probability 1 −o(1) due to the second\nstatement of the theorem with ϵ = r, which has already been proven at this point. Now by the previous\nanalysis of the population loss via Taylor expansion, we have that there exists some constant c1 such that\n∇Φ(Xt) · v0 ≥c1m1(Xt) for all t ∈[0, N) and further on the subsequence dk, k ≥1 such that:\nlim\ndk→∞P( inf\nt≤N |m1(Xt)| > c) > 0\nif we restrict to the positive probability set that {inft≤N |m1(Xt)| > c} again assuming without loss of\ngenerality m1(Xt) > 0 ∀t ∈[0, N], we have Φ(Xt) · v0 ≥c1c for all t ∈[0, N). We then see:\nm1(Xt) ≤(X0 −δ\nd\nt\nX\ni=0\n∇Φ(Xi)) · v0 + 2d−1/3\nk\n≤(X0 −δ\nd\nN\nX\ni=0\nc1c) · v0 + 2d−1/3\nk\n≤m1(X0) −cc1αδ + 2d−1/3\nk\nWhich is diverging to −∞as dk →∞. This is a contradiction as this places a negative upper bound on\nm1(Xt) which is strictly positive on the positive probability set considered. This completes the proof.\nA.4\nProof of Lemma A.10 and Theorem 4.2\nLemma A.10. For all Hermite Polynomials with degree 3 or greater, hk(x), k ≥3:\nEh′′\nk(g) = Eh′\nk(g) = 0, E ∂2\n∂g2 f(g)2 > 0\nfor g ∼N(0, 1)\nProof. For all Hermite polynomials with degree 1 or greater, Ehk = 0. Further it is well known the Hermite\npolynomials satisfy the following:\nh(k)\nn (x) =\nn!\n(n −k)!hn−k(x)\n⇒Eh(k)\nn (g) =\nk!\n(n −k)!Ehn−k(g) = 0, ∀k < n\nThis gives us that Eh′′\nk(g) = Eh′\nk(g) = 0. Now consider E ∂2\n∂g2 f(g)2 = Eh′\nk(g)2 + Eh′′\nk(g)hk(g). Now we\nhave that:\nEh′′\nk(g)hk(g) = k(k −1)Ehk(g)hk−1(g) = 0\nby orthogonality. This completes the claim.\n28\nProof of Theorem 4.2. The proof of this theorem follows from Ben Arous et al. [2021]. Specifically in section\n2.1 of their paper they show that the single-index model covered here meets the assumptions required for\nthere main results which apply more generally. Then noting that by replacing a random uniform initialization\nof order d−1/2 with a fixed initialization of order d−ζ, all their arguments for Theorem 1.3 of their paper\nstill hold, with the new sample complexity provided above (depending on ζ). In the case of ζ = 0, the result\nsimply follows by applying Theorem 3.2 of their paper, noting the arguments used to prove this theorem\napply just as well when considering a sequence of initializations which itself is not constant but is bounded\nabove and below by constants.\nB\nPCA in High Dimensions\nWe consider applying PCA in high dimensions in Theorem 3.3. There are a number of well known results\nabout applying PCA in the high dimensional limit including the BBP transitionBaik et al. [2005]. We\ninformally state a few of these results here for completeness, however, we refer the reader to Bandeira et al.\n[2020] for more details.\nConsider a d-dimensional isotropic Gaussian vector Z ∼N(0, Id). Letting X ∈Rd×n denote the data\nmatrix (n rows of observations of Z). If we let both n and d grow to infinity, keeping their ratio fixed d/n = γ,\nthe distribution of the eigenvalues of the matrix 1\nnXT X (the sample covariance) will in the limit, follow the\nMarcenko-Pastur distribution given by:\ndFγ(x) =\n1\n2πγx\np\n(γ+ −x)(x −γ−)(γx)1[γ−, γ+](x)dx\nwith γ+ = (1 + γ)2 and γ−= (1 −γ)2. Thus in high dimensions, we can expect to observe top eigenvalues\nof 1\nnXT X up to size (1 + d\nn)2, even when there is no covariance structure on X at all. In order to detect\nthe spike we require λ ≥\np\nd/n. The limiting squared correlation between the top eigenvector of the sample\ncovariance matrix and the true spike can be shown to be |v · ˆv|2 = 1−γ/λ2\n1+γ/λ2 when λ ≥\np\nd/n and 0 otherwise.\nC\nDiscrete Bihari-LaSalle Inequality\nThe discrete Bihari-LaSalle Inequality claims that for a sequence mt satisfying the following for some\nk ≥2, a, b > 0:\nmt ≤a +\nt−1\nX\ni=0\nbmk−1\ni\nthen we have that mt ≥\na\n(1−bak−2t)\n1\nk−2\nFor a proof see Appendix C of Ben Arous et al. [2021].\n29\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2025-02-24",
  "updated": "2025-02-24"
}