{
  "id": "http://arxiv.org/abs/2502.18353v1",
  "title": "DBR: Divergence-Based Regularization for Debiasing Natural Language Understanding Models",
  "authors": [
    "Zihao Li",
    "Ruixiang Tang",
    "Lu Cheng",
    "Shuaiqiang Wang",
    "Dawei Yin",
    "Mengnan Du"
  ],
  "abstract": "Pre-trained language models (PLMs) have achieved impressive results on\nvarious natural language processing tasks. However, recent research has\nrevealed that these models often rely on superficial features and shortcuts\ninstead of developing a genuine understanding of language, especially for\nnatural language understanding (NLU) tasks. Consequently, the models struggle\nto generalize to out-of-domain data. In this work, we propose Divergence Based\nRegularization (DBR) to mitigate this shortcut learning behavior. Our method\nmeasures the divergence between the output distributions for original examples\nand examples where shortcut tokens have been masked. This process prevents the\nmodel's predictions from being overly influenced by shortcut features or\nbiases. We evaluate our model on three NLU tasks and find that it improves\nout-of-domain performance with little loss of in-domain accuracy. Our results\ndemonstrate that reducing the reliance on shortcuts and superficial features\ncan enhance the generalization ability of large pre-trained language models.",
  "text": "DBR: Divergence-Based Regularization for Debiasing\nNatural Language Understanding Models\nZihao Li1∗, Ruixiang Tang2, Lu Cheng3, Shuaiqiang Wang4, Dawei Yin4, Mengnan Du1\n1New Jersey Institute of Technology, 2Rutgers University, 3University of Illinois Chicago, 4Baidu\nlizihao9885@gmail.com, mengnan.du@njit.edu\nABSTRACT\nPre-trained language models (PLMs) have achieved impres-\nsive results on various natural language processing tasks.\nHowever, recent research has revealed that these models\noften rely on superficial features and shortcuts instead of\ndeveloping a genuine understanding of language, especially\nfor natural language understanding (NLU) tasks.\nConse-\nquently, the models struggle to generalize to out-of-domain\ndata.\nIn this work, we propose Divergence Based Regu-\nlarization (DBR) to mitigate this shortcut learning behav-\nior. Our method measures the divergence between the out-\nput distributions for original examples and examples where\nshortcut tokens have been masked. This process prevents\nthe model’s predictions from being overly influenced by short-\ncut features or biases. We evaluate our model on three NLU\ntasks and find that it improves out-of-domain performance\nwith little loss of in-domain accuracy. Our results demon-\nstrate that reducing the reliance on shortcuts and superficial\nfeatures can enhance the generalization ability of large pre-\ntrained language models.\n1.\nINTRODUCTION\nPre-trained language models (PLMs), such as BERT [3],\nRoBERTa [16], and Electra [2], have achieved impressive\nresults on various natural language understanding (NLU)\ntasks.\nHowever, recent studies suggest that these PLMs\nheavily rely on a phenomenon called “shortcut learning” [4;\n34], where they capture shallow correlations between labels\nand shortcut features of examples instead of developing a\ndeeper semantic understanding of language [1; 5]. In nat-\nural language inference, for example, which involves deter-\nmining the logical relationship between two sentences, re-\ncent research indicates that models often associate negative\nor contradiction labels with specific negation words such as\n“no,” “none,” or “not.” Due to this shortcut learning, these\nbiased models demonstrate impressive performance for in-\ndomain data by exploiting spurious patterns but struggle to\ngeneralize to out-of-domain data.\nCorrecting these biases and training more robust models has\nrecently attracted significant interest [5; 18; 17]. Most exist-\ning debiasing methods relied on some prior human knowl-\nedge to identify bias types like partial-input bias [10; 22]\nand lexical overlap [20]. To address this issue automatically\n∗Work done during Zihao Li’s remote internship at NJIT.\nwithout specifying bias types, efforts have been made to pro-\npose debiasing methods that eliminate spurious correlations\nand improve OOD performance. These approaches include\ninstance reweighting [24; 31], confidence regularization [29],\nand product of experts [1; 23].\nDespite the recent advancements, effectively addressing bias\nin NLU models remains a challenging task. There are two\nprimary challenges associated with existing debiasing meth-\nods. Firstly, most existing methods rely on training a “bias-\nonly model” to assist in the debiasing process, which allows\nthe debiased model to focus on specific examples. However,\nthe generalization performance of debiased models heavily\ndepends on humans’ prior knowledge about biases in the\ntraining data. Unfortunately, this prior knowledge can only\nidentify a limited number of biases in the data. Although\nit is possible to reduce the use of some known shortcuts,\nmodels may still exploit other shortcuts for prediction. This\ncould explain why existing mitigation methods only provide\nlimited gains in generalization [5].\nTherefore, we need to\nreduce the reliance on bias-only models. Secondly, current\ndebiasing methods are often treated as black boxes, since\nit is unclear how these models actually improve generaliza-\ntion, and whether they genuinely reduce their dependence\non superficial features.\nThis lack of transparency hinders\nthe ability to understand the underlying mechanisms of im-\nprovement and limits further advancements of pre-trained\nlanguage models in model generalization.\nTo address these research challenges, we propose Divergence\nBased Regularization (DBR), a transparent approach to ex-\nplicitly enforce the model to reduce reliance on shortcut fea-\ntures (Figure 1), thereby improving the robustness of NLU\ntasks. Specifically, we first mask shortcut tokens to prevent\nthe prediction of the model from being affected by them. In\nthis way, we can construct unbiased versions of original ex-\namples, then add a regularization loss to make the original\nand unbiased examples’ representations as close as possi-\nble. However, not all examples exhibit shortcut behavior.\nApplying this process to all examples would damage seman-\ntic meaning. We use a bias-only model to determine which\nexamples actually rely on shortcuts, calculating each exam-\nple’s probability of being a shortcut example. We then use\nthe soft masking strategy of our proposed method to softly\nmask salient tokens based on these probabilities. This soft\nmasking strategy generates different masked examples for\neach epoch, improving the model’s robustness.\nWe eval-\nuate DBR on three common NLU tasks, and the results\nindicate that our approach improves out-of-domain perfor-\nmance. The major contributions of this work can be sum-\narXiv:2502.18353v1  [cs.CL]  25 Feb 2025\nmarized as follows:\n• We propose DBR, a debiasing framework to discourage\nthe NLU models from relying on the shortcut tokens\nfor prediction.\n• Our proposed DBR method reveals deeper factors that\naffect model robustness, including the impact of token-\nlevel factors.\n• Experimental results over three NLU tasks show im-\nproved OOD performance, demonstrating that our DBR\nmethod reduces shortcut learning and improves gener-\nalization.\n2.\nRELATED WORK\nIn this section, we summarize two lines of research that are\nmost relevant to ours.\nData Bias and Shortcut Learning.\nTextual data con-\ntain various types of biases, such as word co-occurrence [10],\nlexical overlap [20], partial inputs [10; 22], and negation\nwords [31]. Models trained on such biased data will capture\nspurious correlations in the data without achieving true se-\nmantic understanding. This phenomenon is known as short-\ncut learning. One study models the distribution of shortcut\nwords as a long-tail distribution and uses its characteristics\nto debias models [6]. Most shortcut phenomena stem from\nthe co-occurrence of specific words and labels. For example,\nnegation words like “no” and “none” often correlate with\ncontradiction labels in natural language inference tasks [10].\nRecent studies have shown that shortcut learning can nega-\ntively impact model performance on OOD datasets [8; 10].\nShortcut Mitigation.\nClark et al.\nproposed a Prod-\nuct of Experts method that combines a bias-only model’s\nknowledge with a base model [1].\nIt first trains a bias-\nonly model and then uses its predictions to train a robust\nmodel [24]. Similar to focal loss [15], example reweighting\n[1] improves models by down-weighting overconfident exam-\nples, i.e., shortcut examples. Confidence regularization [29]\nencourages models to reduce confidence in predictions for\nbiased samples.\nSoft label encoding proposed to train a\nteacher model to determine the shortcut degree, then the\ndegree is used to generate soft labels for robust model train-\ning [12]. DCT employs a positive sampling strategy to mit-\nigate features in the sample [18].\nIn contrast to these previous methods, our proposed frame-\nwork takes a more direct approach by explicitly suppress-\ning the NLU model’s ability to capture undesirable corre-\nlations between shortcut tokens and certain labels. This is\nachieved through a combination of strategic token masking\nand distribution alignment, providing a more transparent\nway to reduce shortcut reliance while maintaining model\nperformance.\n3.\nPROPOSED METHOD\nIn this section, we give a detailed introduction to the pro-\nposed Divergence Based Regularization (DBR) debiasing\nframework (Figure 1). It should be noted that, the proposed\nmethod is aimed at debiasing PLMs belonging to the tradi-\ntional pre-training and finetuning paradigm (such as BERT)\nthat are prone to suffer from shortcut learning issue.\n3.1\nProposed Debiasing Scheme\nThe goal of NLU is to classify the semantic relationship be-\ntween two sentences as one of multiple classes, and we for-\nmulate it into a multi-class classification task. Given a pair\nof a sentence xi ∈X and its label yi ∈Y, we aim to learn\na robust mapping function of F : xi →yi. We follow the\nstandard pre-training and fine-tuning paradigm. The model\nshould rely on semantic understanding for prediction rather\nthan relying on shortcuts, so that it can generalize well to\nout-of-domain datasets.\nThe key motivation of our approach is to discourage exces-\nsive reliance of NLU models on shortcuts. We propose to\nachieve this by masking shortcut tokens and aligning the\nprediction distributions between the original and masked\nsamples. Specifically, our framework consists of two stages.\nWe first develop a shortcut identification model using the\ntraining data to detect linguistic shortcuts in the text (Sec-\ntion 3.2).\nSubsequently, in the second stage, we train a\ndebiased model by introducing a regularization loss that fo-\ncuses on aligning distributions (Section 3.3). More specifi-\ncally, during the second stage, we mask the shortcut tokens\nand encourage the NLU model to generate similar prediction\ndistributions for both the original samples and the samples\nwith masked shortcut tokens. To ensure that the seman-\ntic meaning of the text remains unaffected, we employ the\nsoft masking strategy to further refine the masking process\n(Section 3.4).\n3.2\nShortcut Tokens Identification\nTo effectively capture shortcut features in the sample and\nanalyze the factors influencing model robustness in detail,\nwe utilize a gradient-based interpretation technique known\nas Integrated Gradients (IG) [26]. This method enables us\nto determine the impact of each token on the model’s predic-\ntion, aligning perfectly with our requirements. By attribut-\ning the ground-truth label to each input token, IG generates\ninterpretations for every token in the text. The outcome is\npresented as a feature importance vector, indicating the sig-\nnificance of each token. The main steps of IG are described\nas follows. We first construct a baseline input xbase with the\nsame dimensions as the original input xi, and then integrate\nthe gradients of prediction probability w.r.t. m intermediate\nsamples from the baseline input xbase to the original input\nxi. It can be formulated as follows:\ngxi = (xi−xbase)· 1\nm\nm\nX\nk=1\n∇xify\n\u0000xbase+ k\nm(xi−xbase)\n\u0001\n. (1)\nThe shape of the original input xi is (L, d) with L tokens,\nand each token represents its word embedding with d di-\nmensions. We employ all-zero word embeddings to represent\nthe baseline input xbase. As such, we obtain gxi, i.e., the\nattribute vector for each token, with the same shape as xi.\nTo compute the attribution of each token, we compute the\nℓ2 norm of each attribution vector to measure the attribu-\ntion of each token. Shortcut words mean that the prediction\nhighly relies on these words, thus the shortcut words can be\nregarded as tokens with high attribution to prediction. So\nwe select top-N tokens by their attribution values as our\nshortcut tokens of the input text xi.\n3.3\nDebiasing by Hard Masking\nIn this section, we introduce the details of the divergence\nOrign Input\nIdentification \nModel\nIntergrated\n  Gradient\n𝑤𝑜𝑟𝑑!\ninteresting\n𝑤𝑜𝑟𝑑\"\nintriguing\n𝑤𝑜𝑟𝑑#\nvery\nMLP\n0.2\n0.7\n0.1\nShortcut\n Degree: q\nSoft Masking\nBy Ber(q)\nUnbiased Input\nOrign Input\nUnbiased Input\n𝐶𝐿𝑆$%!&'\n𝐶𝐿𝑆(')!*+,-\nMinimize\nJS-Divergence 𝐿()*\nMinimze\nCross-Entropy 𝐿+,\n[CLS]well it‘s been very interesting[SEP]\nIt has been very intriguing.\n[CLS]well it‘s been very [MASK][SEP]\nIt has been [MASK] [MASK].\n[CLS]well it‘s been very interesting[SEP]\nIt has been very intriguing.\n[CLS]well it‘s been very [MASK][SEP]\nIt has been [MASK] [MASK].\nDebiased \nModel\nDebiased \nModel\nquantify the shortcut degree\n \nof the  orign input\nFigure 1: The proposed DBR framework. We first train a shortcut identification model to compute the shortcut degree of\neach sample, then use the regularization loss based on the JSD divergence to train the debiased model.\nbased regularization for debiasing NLU models.\nWe first\n(hard) mask of the shortcut tokens identified within the orig-\ninal sentence, to acquire an unbiased representation of the\noriginal sample, denoted as xunbiased. By masking these to-\nkens, which significantly influence the model’s predictions,\nwe ensure that the model is not influenced by these short-\ncuts when making predictions.\nConsequently, the sample\nwith the masked shortcut words can be considered an ap-\nproximately unbiased representation of the original sample.\nInspired by [9], after obtaining the unbiased representation\nof the sample, we align the distribution space of the unbiased\nsample xunbiased with that of the original sample xoriginal.\nThis helps mitigate the influence of shortcut tokens on the\nmodel. We use the Jensen-Shannon divergence (JSD) [7],\na function for measuring the distance between probability\ndistributions as our regularization loss function to minimize\nthe disagreement between the distributions of the unbiased\nsample and the original sample. Compared with the Kull-\nback–Leibler divergence (KLD) loss, the JSD loss is a sym-\nmetric representation of the latter. It can be described as:\nJSD(p1, p2) = 1\n2\n2\nX\ni=1\n(KLD(pi||p1 + p2\n2\n),\nKLD(p1||p2) =\nX\nk∈Y\np1(k)log(p1(k)\np2(k)).\n(2)\nWe compute the JSD score between the distribution of unbi-\nased sample punbias = p([CLS] = yi|F, xunbias) and that of\nthe original sample porign = p([CLS] = yi|F, xorign). Our\ngoal is to minimize the JSD score between punbias and porign\nto discourage the model from relying on shortcut tokens for\nprediction.\n3.4\nDebiasing by Soft Masking\nThe aforementioned hard masking scheme has two limita-\ntions.\nFirstly, the top-N shortcut tokens selected in Sec-\ntion 3.3 may not accurately represent the actual shortcut\ntokens.\nThere is a possibility that tokens that positively\ncontribute to the model’s prediction are genuine important\ntokens. Secondly, the hard masking scheme, which masks\nthe text input, can potentially impact the semantic meaning\nof the text. For instance, in the sentence “The movie I saw\nlast night is so excellent,” the hard masking scheme might\nmask the word “excellent,” which significantly contributes\nto the predicted label and also conveys important semantic\ninformation. Recent research [32; 25; 33] has shown that\n“genuine” tokens typically play a vital role in conveying se-\nmantic meaning, and their correlation with labels is what\nthe model aims to capture. On the other hand, the corre-\nlation between “spurious” tokens (i.e., non-genuine short-\ncut tokens) and labels fails to generalize to OOD datasets.\nTherefore, while masking shortcut tokens can enhance the\nmodel’s generalization, masking genuine tokens can com-\npromise the semantic meaning of the text and hinder the\nmodel from capturing the relationship between the text and\nthe label. These two limitations of hard masking motivate\nthe design of the soft masking strategy.\nQuantifying Shortcut Degree.\nThe first step is to quan-\ntify the shortcut degree of each training sample. After ob-\ntaining the shortcut tokens for each sample, we design a\nmetric to measure the amount of shortcut information con-\ntained in each sample. Based on a simple intuition, if a sam-\nple contains a significant amount of shortcut information,\nthe model can easily predict the label solely based on these\nshortcut words. Motivated by this idea, we train a bias-only\nmodel that takes shortcut words as input and predicts the\nlabel. Consequently, we obtain the prediction probabilities\nfor each label.\nNow, the question is how to quantify the\nshortcut degree based on these probabilities.\nIt is widely\naccepted that the more biased the model is, the more con-\nfident it becomes in its predictions, resulting in higher vari-\nance in the predicted outcomes [13]. Therefore, we utilize\nthe variance (Var) metric to represent the shortcut degree\nof a sample i, which is described as follows:\nˆsi\n2 =\ns2\ni −min{s2\nj}m\nj=1\nmax{s2\nj}m\nj=1 −min{s2\nj}m\nj=1\n, s2\ni =\nK\nP\nj=1\n(pj −p)2\nK −1\n,\n(3)\nwhere K denotes the number of labels, pj denotes the prob-\nability of predicting label j, and m denotes the batch size.\nThe example with high variance can be considered as an\noverconfident or shortcut example [13]. We normalize the\nsample variance s2\ni to ˆsi\n2 within the same batch to make it\nrange from 0 to 1. ˆsi\n2 can be considered as shortcut degree\nof sample i.\nSoft Masking Framework.\nOnce we have determined\nthe shortcut degree of each training sample, we proceed to\nimplement a soft masking strategy. In this strategy, the de-\ncision of whether to mask the top-N shortcut tokens in a\nsample is made using the Bernoulli distribution. This dis-\ntribution determines whether each token should be masked\nor left unchanged based on a probability threshold. The soft\nmasking can be described as follows:\nxunbias = M(xi), M ∼Ber( ˆsi\n2),\n(4)\nwhere M denotes the operation of masking shortcut tokens\nof xi, ˆsi\n2 denotes the normalized variance, representing the\nprobability that the sample belongs to the shortcut sample.\nTherefore, the final loss can be defined as follows:\nL = LCE(F(xi), yi) + λLJSD(punbias, porign),\n(5)\nwhere LCE denotes the cross-entropy loss and λ is the weight\nof JSD loss function. The overall steps of DBR are given\nin Algorithm 1.\n4.\nEXPERIMENTS\nWe conduct experiments to evaluate the debiasing perfor-\nmance of the proposed DBR debiasing framework, to answer\nthe following three research questions:\n1) In comparison to established baselines, does the proposed\nDBR debiasing method effectively optimize the trade-off be-\ntween in-domain and OOD performance? (Section 4.2)\n2) Does the proposed soft masking technique prove to be\neffective in debiasing shortcut learning? (Section 4.3)\n3) What are the key factors that contribute to the effective-\nness of the proposed method? (Section 5)\n4.1\nExperiment Settings\nIn this section, we present the comprehensive experimen-\ntal setup used to evaluate our proposed DBR framework.\nWe describe the datasets used for three NLU tasks, baseline\nmethods for comparison, and implementation details includ-\ning model architectures and training configurations.\n4.1.1\nDatasets\nWe evaluate the generalization performance of DBR in three\ncommon NLU tasks.\nAlgorithm 1 Pseudo-code for DBR framework\n1: Input: Training data D = {(xi, yi)}N\ni=1, Identification\nmodel Fi, Bias-only model Fbias.\n2: Output: Debiased model Fdebias.\n3: //obtain top-N tokens list Si using identification model\nFi for each sample xi ∈D by Equation 1;\n4: for (xi, yi) ∈D do\n5:\nSi = {};\n6:\nSi ←top-Nxi tokens by gxi;\n7:\n//where gxi obtained from Equation 1\n8: end for\n9: // Get the shortcut degree ˆsi\n2 for each sample xi ∈D\n10: for (xi, yi) ∈D do\n11:\np = Fbias(Si)\n12:\n// obtain ˆsi\n2 by Equation 3\n13: end for\n14: // Train the debiased model.\n15: for (xi, yi) ∈D do\n16:\n// Using shortcut degree ˆsi\n2 obtained from above to\ngenerate unbias sample by Equation 4\n17:\nTraining the model using the loss function L =\nLCE(Fdebias(xi), yi) + λLJSD(punbias, porign)\n18: end for\nNature Language Inference.\nThe training dataset of\nthis task is MNLI [35] which contains 392,702 samples. Each\ntraining sample consists of two sentences representing the\npremise and the hypothesis, the goal of the task is to predict\nwhether the relationship between the premise and the hy-\npothesis is entailed, contradicted, or neutral. There are two\ndevelopment sets of MNLI: dev-matched containing 9,815\nsamples and dev-mismatched 9,832 samples.\nThe differ-\nence between them is that dev-matched is consistent with\nthe source of the training datasets and dev-mismatched is\nnot. For OOD test sets, we employ HANS [20] and MNLI-\nhard [10] for evaluation.\nFact Verification. FEVER [27], which comprises 242,911\nsamples, is the training set. Each training sample consists of\ntwo sentences, representing the claim and the evidence. The\nobjective of the task is to predict the relationship between\nthe claim and the evidence, categorizing it as “refute,” “sup-\nport,” or “not enough information.” Additionally, we have\na development set containing 16,664 samples, which will be\nused for evaluating and fine-tuning our model. Symmetric\nv1 and v2 (Sym 1 and Sym 2) [24] are the OOD test sets.\nBoth test sets are synthesized and specifically created by\nintroducing perturbations to the sentence pairs. These per-\nturbations are designed to challenge the model and result\nin poor performance if no debiasing strategies are applied.\nEach synthesized test set include 712 samples.\nParaphrase Identification.\nFor QQP dataset, the task\naims at predicting whether the relationship between the sen-\ntence pair is duplicate or not.\nThe training set contains\n363,846 samples and the development set contains 404,30\nsamples. We use Quora Question Pairs (QQP)1 as our train-\ning dataset, and use PAWS as our challenging OOD test set\n[36].\nThe adversarial samples are generated with lexical-\noverlap bias.\n4.1.2\nComparing Baselines\n1https://www.kaggle.com/c/quora-question-pairs\nMethod\nMNLI\nFEVER\nQQP\ndev\nHANS\ndev hard\ndev\nSym1\nSym2\ndev\nPAWS\nBERT-base\n84.5\n62.4\n77.0\n85.6\n55.1\n63.1\n91.0\n33.5\nER[24]\n81.4\n68.6\n-\n87.2\n-\n65.6\n85.2\n57.4\nPoE[11; 1]\n80.7\n68.5\n-\n85.4\n-\n65.3\n-\n-\nConRe[30]\n83.9\n67.7\n-\n87.9\n-\n66.1\n89.0\n43.0\nLearned-Mixin[1]\n84.3\n64.0\n-\n83.3\n60.4\n64.9\n86.6\n56.8\nModeling Bias[19]\n84.2\n64.7\n76.8\n86.5\n-\n66.3\n-\n-\nSoft Label[12]\n81.2\n68.1\n-\n87.5\n60.3\n66.9\n-\n-\nDebias Mask[21]\n81.8\n68.7\n-\n84.6\n-\n64.9\n-\n-\nDCT[18]\n84.2\n68.3\n-\n87.1\n63.3\n68.4\n-\n-\nDBR-soft mask\n84.5\n68.6\n78.8\n86.4\n59.2\n66.4\n90.7\n41.8\nDBR-hard mask\n83.9\n67.4\n78.0\n85.4\n55.1\n64.9\n90.3\n41.2\nTable 1: Performance between DBR and other baselines on three NLU tasks. For MNLI task, we choose dev-mismatch as\nour dev set. The results for the baselines of ER, POE, ConRe are taken from [31]. Bold results indicate the best results of\nthe above baseline, excluding BERT-base.\nFigure 2: Attribution result visualization, the first and second row denote the attribution of each word before mitigation and\nafter mitigation respectively. Words marked in green represent that the word contributes to the model prediction results, and\nthe darker the color, the greater the contribution.\nWe compare the proposed DBR method, including soft mask\nand hard mask versions, with several representative debias-\ning baselines, detailed as follows.\nExample Reweight (ER) [24] ER first trains a basic\nmodel to obtain predictions with bias, then trains a debiased\nmodel using the following loss: L = −(1−pi\nb)yi·pi\nd, where pb\nand pd denote the softmax output of the basic model and de-\nbiased model, respectively. Examples with high confidence\nare allocated with less attention.\nProduct of Experts (PoE) [11; 1] PoE first trains a\nbasic model and combines the softmax output of it and\nthe debiased model.\nThe ensemble loss is described as:\nL = −yi · log softmax(pi\nb + pi\nd).\nConfidence Regularization (ConRe) [30] ConRe en-\ncourages the student model to assign less attention to sam-\nples that the teacher model considers biased: L = −S(pt, pi\nb)·\nlog pd, where S(pt, pi\nb) denotes the soft predictions with tem-\nperature pi\nb.\nIn addition to above basic debiasing methods, we also com-\npare our method with some complex baselines such as Learned-\nMixin[1], Modeling Bias [19], Soft Label Encoding [12], De-\nbias Mask [21], and DCT [18].\n4.1.3\nImplementation Details\nIn our experiments, we utilize the BERT-base-uncased model2\nas the backbone for both the identification model and the de-\nbiased model. This model consists of 12 Transformer blocks,\neach with a hidden layer dimension of 768. For the bias-\n2https://huggingface.co/bert-base-uncased\nonly model, we adopt a simple structure to effectively iden-\ntify shortcut samples.\nIt consists of a single Multi-Layer\nPerceptron (MLP) with the Rectified Linear Unit (ReLU)\nactivation function.\nMore detailed information about the\nbias-only model can be found in Section 5.2.\nRegarding the training settings, we train the identification\nmodel and the debiased model using the entire training\ndataset for 12 epochs. In contrast, the bias-only model is\ntrained using a smaller subset of the training dataset for\nonly 1 epoch to mitigate the risk of overfitting. The batch\nsize for training is set to 32 for the identification model and\n18 for both the bias-only model and the debiased model.\nThe learning rate is set to 2e −5. During the “top-N” se-\nlection process, the value of N is set to 3. The maximum\nlength of the input sequence is limited to 512 tokens. The\nhidden layer dimension of the MLP in the bias-only model\nis set to 100. In Equation 5, the value of λ is set to 1.5 for\nMNLI/QQP, and 3 for FEVER, respectively.\n4.2\nTrade-off between In-domain and OOD\nThe results of DBR and the baselines are presented in Ta-\nble 1. We can observe that DBR consistently outperforms\nBERT-base models on all OOD test sets.\nCompared to\nthe performance of other methods on most OOD test sets,\nDBR achieves comparable results without a significant drop\nin performance on the in-domain test set.\nThis suggests\nthat our method successfully achieves a trade-off between\nthe OOD test sets and the in-domain test set. Specifically,\nDBR outperforms all the debiasing techniques for HANS\nand MNLI dev hard.\nNotably, DBR achieves similar in-domain performance to\nBERT-base models on MNLI and QQP and even improves\nthe in-domain performance on FEVER. These results show\npositive evidence that the proposed soft masking strategy\nenhances the semantic expression of sentences and reduces\nthe reliance of NLU models on shortcut tokens.\nIn the\ncase of the QQP dataset, some baseline approaches, such\nas Learned-Mixin [1], achieve promising performance on the\nPAWS test set. However, these methods suffer from a sig-\nnificant drop in performance on the in-domain dataset. In\ncontrast, our proposed DBR strikes a balance between the\nin-domain test set and the OOD test set, achieving compet-\nitive performance in both scenarios.\n4.3\nAblation Studies\nWe also present the comparison results between using the\nsoft masking strategy and not using it in Table 1. From the\nresults, when considering the strategy of masking all sen-\ntences (hard mask), we observe a decline in performance on\nthe in-domain test set compared to BERT-base. However,\nwhen employing the soft mask strategy, we observe improve-\nments in performance on the in-domain test set. Addition-\nally, on the FEVER dataset, there are further improvements\ncompared to BERT-base. These results show that the soft\nmasking not only helps achieve a better understanding of\nthe sentence’s semantics compared to the hard masking but\nalso enhances the overall applicability of our method.\n5.\nA CLOSER LOOK\nIn this section, we provide further analysis and discussion of\nthe proposed debiasing algorithm.\n5.1\nGeneralization Visualization\nWe further conduct a visualization analysis through case\nstudies, as depicted in Figure 2. Prior to mitigation, it is ev-\nident that the model predominantly focuses on the shortcut\nword “sweeney,” as indicated by the strong attention weight\nassigned to it. However, after applying the mitigation strat-\negy, we observe a notable change in the visualization. In\nthe post-mitigation scenario, we can observe that a greater\nnumber of words are highlighted in green compared to the\npre-mitigation stage. Furthermore, the color distribution is\nmore uniform, indicating a more balanced contribution from\nmultiple words in the text input. This observation suggests\nthat the model now pays attention to a wider range of words\nin the input text. Consequently, the model’s reliance on the\nshortcut word is reduced, enabling it to better grasp the se-\nmantic meaning of the text. This visualization analysis pro-\nvides evidence of how DBR debiases the model, by affecting\nthe internal factors of the sentence, thereby enhancing the\ntransparency of the debiasing process.\n5.2\nLearning of Bias-only Model\nIn this section, we analyze the performance of the bias-only\nmodel, i.e., the MLP model shown in Figure 1.\nThe structure of Bias-only Model. The bias-only model\nin our design serves the purpose of quantifying the degree\nof shortcuts in the training samples. Therefore, it should\nachieve high training accuracy w.r.t. in-distribution train-\ning data when using a sufficiently simple model structure, a\nrelatively small training dataset, and a training input that\nis intentionally biased towards shortcuts. To fulfill these re-\nDatasets\nAccuracy\nSamples\nMNLI\n97.25\n2000\nFEVER\n95.68\n3000\nTable 2: Accuracy and training samples of the bias-only\nmodel in in-domain test set sample from training set.\nEntailment\nContradiction\nWords\nLMI ×10−3\nWords\nLMI×10−3\nthe\n8.88\nnot\n22.7\nand\n2.65\nno\n22.3\ncan\n1.25\nnever\n11.7\nmany\n0.93\ndon\n3.87\ngood\n0.89\ndidn\n2.63\ngreat\n0.58\ncannot\n1.23\nTable 3: The LMI of top-N shortcut words in the training\nset of MNLI with respect to the label of “entailment” and\n“contradiction”.\nquirements, we construct the model input vector by concate-\nnating the encoded text representation of the top-N shortcut\nwords from each sample. The resulting input shape is (m,\nN × dim), where m denotes the batch size and dim repre-\nsents the hidden size of the BERT encoder. The model ar-\nchitecture consists of only one layer of MLP with the ReLU\nactivation function.\nFor training, we randomly select 1000, 2000, and 3000 sam-\nples from the original training set, respectively, and train the\nbias-only model for one epoch. The remaining 10000 sam-\nples are used as the test set (both the training and test sets\nare derived from the MNLI training set). The accuracy re-\nsults are presented in Table 2, indicating that the bias-only\nmodel achieves high accuracy on the in-distribution dataset\nby relying solely on a small number of words in the text. In\ncomparison to previous approaches, our bias-only model ex-\nhibits a higher degree of bias, aligning with our objectives.\nThese findings validate that our bias-only model effectively\nquantifies the degree of shortcuts in the training samples,\nas it achieves remarkable accuracy on the in-distribution\ndataset by leveraging only a limited set of words.\nWhat Bias-only Model Learns. We also investigate the\ndistribution of top-N shortcut words. We use local mutual\ninformation (LMI) [24] to measure the correlation between\ntop-N words ω and labels l:\nLMI(ω, l) = p(ω, l) · log(p(l|ω)\np(l) ),\n(6)\nwhere p(ω, l) = count(ω,l)\n|D|\n,p(l|ω) = count(ω,l)\ncount(ω) , p(l) = count(l)\n|D|\nand |D| is the number of top-N shortcut words of the train-\ning set.\nIn Table 3, we present the selected shortcut words with\nhigh LMI that are correlated with the “entailment” and\n“contradiction” labels in the MNLI dataset.\nAn observa-\ntion from the table is that the majority of words associated\nwith the “contradiction” label exhibit negative emotions,\nsuch as “no” and “never,” which are highly consistent with\nthe nature of the “contradiction” label. The same pattern\nholds true for the words related to the “entailment” label.\nFurthermore, we notice that the LMI values for the “con-\ntradiction” label are significantly higher than those for the\n(a) HANS\n(b) FEVER Sym 2\n(c) PAWS\nFigure 3: Confidence distribution of the identification model and the debiased model. The orange denotes the identification\nmodel and the green denotes the debiased model.\nMNLI\nFEVER\nMethod\ndev\nHANS\ndev hard\ndev\nSym1\nSym2\nDBR-soft mask(filtered)\n84.5\n68.9\n78.8\n86.4\n59.4\n66.2\nDBR-soft mask\n84.5\n68.6\n78.8\n86.4\n59.2\n66.4\nDBR-hard mask\n83.9\n67.4\n78.0\n85.4\n55.1\n64.9\nTable 4: Performance of DBR-soft mask, DBR-hard mask\nand DBR-soft mask(filtered).\nFigure 4: Loss function curves for three training approaches\nduring the training stage:\nstandard training, DBR-hard\nmask and DBR-soft mask.\n“entailment” label. Consequently, to enhance the model’s\nrobustness, it becomes crucial to focus on the input associ-\nated with negative labels.\nBesides, we can find that some words associated with ”en-\ntailment” can’t carry meaningful information regarding the\nlabel. Therefore, we filter out the intersection of the top 10\nwords of the two labels which are considered as words with-\nout useful information. We filtered out these high-frequency\nwords and conducted another experiment based on the orig-\ninal settings.\nThe results show that the model performs\nbetter in OOD datasets like HANS and FEVER Sym1, and\nremains the same performance in the original dataset.\nIt\nshows that masking these high-frequency words that carry\nlittle information about the sentence will hinder the model’s\nunderstanding of the overall semantics.\n5.3\nThe Convergence of Loss\nIn Figure 4, we present the convergence of the loss function\nduring the first epoch for the original training approach and\nour proposed method. We observe that the loss function of\nthe original training approach converges faster compared to\nour proposed method. This phenomenon can be explained\nby the model’s tendency to prioritize learning the features\nof shortcut samples [6].\nThe slower convergence of DBR indicates two key points.\nFirstly, our method focuses more on hard samples rather\nthan shortcut samples, which requires additional training\niterations to achieve convergence. Secondly, the slower con-\nvergence suggests that DBR effectively guides the model to\npay less attention to shortcut features.\n5.4\nConfidence Distribution\nWe conduct a comparative analysis of confidence distribu-\ntions between the identification model and the debiased model,\nwith results presented in Figure 3. A notable observation\nis that the confidence density curve of the debiased model\n(green color) shows a leftward shift compared to the identi-\nfication model (orange color), indicating that DBR success-\nfully reduces overall prediction confidence levels.\nThis pattern is particularly pronounced in the FEVER and\nPAWS datasets, where the identification model’s curve (or-\nange color) exhibits a steeper profile than the debiased model’s\ncurve (green color). This steeper distribution suggests that\nthe identification model produces more concentrated confi-\ndence scores. This observation aligns with established find-\nings that models tend to display overconfidence when en-\ncountering biased or shortcut examples.\nSuch overconfi-\ndence typically manifests in shortcut learning, where models\nexploit superficial patterns rather than developing deeper\nunderstanding.\n6.\nCONCLUSIONS AND FUTURE WORK\nIn this work, we have introduced DBR, a novel debiasing\napproach for natural language understanding models. Our\nmethod operates by masking salient words to construct un-\nbiased example representations, then employing a regular-\nization loss to align the distributions between original and\nunbiased examples. The results show that DBR not only\nachieves significant improvements in out-of-domain perfor-\nmance but also maintains strong in-domain accuracy.\nMoving forward, we plan to explore alternative masking\nstrategies, such as substituting the masked shortcut tokens\nwith alternative tokens. and extend the debiasing for large\nlanguage models (LLMs) belonging to the prompting paradigm\nsuch as Llama-2 [28], Mistral [14].\n7.\nREFERENCES\n[1] Christopher Clark, Mark Yatskar, and Luke Zettle-\nmoyer. Don’t take the easy way out: Ensemble based\nmethods for avoiding known dataset biases. Empiri-\ncal Methods in Natural Language Processing (EMNLP),\n2019.\n[2] Kevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. Electra: Pre-training text en-\ncoders as discriminators rather than generators. In-\nternational Conference on Learning Representations\n(ICLR), 2020.\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. North\nAmerican Chapter of the Association for Computa-\ntional Linguistics (NAACL), 2019.\n[4] Varun Dogra, Sahil Verma, Kavita, Marcin Wo´zniak,\nJana Shafi, and Muhammad Fazal Ijaz. Shortcut learn-\ning explanations for deep natural language processing:\nA survey on dataset biases. IEEE Access, 12:26183–\n26195, 2024.\n[5] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao,\nand Xia Hu. Shortcut learning of large language models\nin natural language understanding. Communications of\nthe ACM (CACM), 2023.\n[6] Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi\nDeshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating\nshortcut learning behavior of nlu models. North Ameri-\ncan Chapter of the Association for Computational Lin-\nguistics (NAACL), 2021.\n[7] Bent Fuglede and Flemming Topsoe. Jensen-shannon\ndivergence and hilbert space embedding. In Interna-\ntional symposium onInformation theory, 2004. ISIT\n2004. Proceedings., page 31. IEEE, 2004.\n[8] Robert\nGeirhos,\nJ¨orn-Henrik\nJacobsen,\nClaudio\nMichaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in\ndeep neural networks. Nature Machine Intelligence,\n2(11):665–673, 2020.\n[9] Yue Guo, Yi Yang, and Ahmed Abbasi. Auto-debias:\nDebiasing masked language models with automated bi-\nased prompts. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1012–1023, 2022.\n[10] Suchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel R Bowman, and Noah A Smith.\nAnnotation artifacts in natural language inference data.\narXiv preprint arXiv:1803.02324, 2018.\n[11] He He, Sheng Zha, and Haohan Wang. Unlearn dataset\nbias in natural language inference by fitting the resid-\nual. 2019 EMNLP workshop, 2019.\n[12] Zirui He, Huiqi Deng, Haiyan Zhao, Ninghao Liu, and\nMengnan Du. Mitigating shortcuts in language models\nwith soft label encoding. LREC-COLING 2024, 2024.\n[13] Hao Huang, Tianyi Zhou, Guodong Long, Tao Shen,\nJing Jiang, and Chengqi Zhang. Biaspad:\nA bias-\nprogressive auto-debiasing framework.\n[14] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux,\nArthur Mensch, Blanche Savary, Chris Bamford, De-\nvendra Singh Chaplot, Diego de las Casas, Emma Bou\nHanna, Florian Bressand, et al. Mixtral of experts.\narXiv preprint arXiv:2401.04088, 2024.\n[15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\nand Piotr Doll´ar. Focal loss for dense object detection.\nIn Proceedings of the IEEE international conference on\ncomputer vision (ICCV), 2017.\n[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[17] Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun\nRen, Pengjie Ren, and Zhumin Chen. Self-supervised\nposition debiasing for large language models. In Find-\nings of the Association for Computational Linguistics\nACL 2024, pages 2897–2917, 2024.\n[18] Yougang Lyu, Piji Li, Yechang Yang, Maarten de Rijke,\nPengjie Ren, Yukun Zhao, Dawei Yin, and Zhaochun\nRen. Feature-level debiased natural language under-\nstanding. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, pages 13353–13361,\n2023.\n[19] Rabeeh\nKarimi\nMahabadi,\nYonatan\nBelinkov,\nand James Henderson. End-to-end bias mitigation\nby\nmodelling\nbiases\nin\ncorpora.\narXiv\npreprint\narXiv:1909.06321, 2019.\n[20] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right\nfor the wrong reasons: Diagnosing syntactic heuristics\nin natural language inference. 57th Annual Meeting of\nthe Association for Computational Linguistics (ACL),\n2019.\n[21] Johannes Mario Meissner, Saku Sugawara, and Akiko\nAizawa. Debiasing masks: A new framework for short-\ncut mitigation in nlu. arXiv preprint arXiv:2210.16079,\n2022.\n[22] Adam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. Hypoth-\nesis only baselines in natural language inference. arXiv\npreprint arXiv:1805.01042, 2018.\n[23] Victor Sanh, Thomas Wolf, Yonatan Belinkov, and\nAlexander M Rush. Learning from others’ mistakes:\nAvoiding dataset biases without modeling them. In-\nternational Conference on Learning Representations\n(ICLR), 2021.\n[24] Tal Schuster, Darsh J Shah, Yun Jie Serene Yeo, Daniel\nFilizzola, Enrico Santus, and Regina Barzilay. Towards\ndebiasing fact verification models. Empirical Methods\nin Natural Language Processing (EMNLP), 2019.\n[25] Megha Srivastava, Tatsunori Hashimoto, and Percy\nLiang. Robustness to spurious correlations via human\nannotations. In International Conference on Machine\nLearning, pages 9109–9119. PMLR, 2020.\n[26] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Ax-\niomatic attribution for deep networks. International\nConference on Machine Learning (ICML), 2017.\n[27] James\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\nFever:\na\nlarge-scale dataset for fact extraction and verifica-\ntion. North American Chapter of the Association for\nComputational Linguistics (NAACL), 2018.\n[28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023.\n[29] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna\nGurevych. Mind the trade-off:\nDebiasing nlu mod-\nels without degrading the in-distribution performance.\n58th Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), 2020.\n[30] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna\nGurevych. Mind the trade-off: Debiasing NLU mod-\nels without degrading the in-distribution performance.\nIn Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel\nTetreault, editors, Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 8717–8729, Online, July 2020. Association for\nComputational Linguistics.\n[31] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna\nGurevych. Towards debiasing nlu models from un-\nknown biases. Empirical Methods in Natural Language\nProcessing (EMNLP), 2020.\n[32] Tianlu Wang, Rohit Sridhar, Diyi Yang, and Xuezhi\nWang. Identifying and mitigating spurious correlations\nfor improving robustness in nlp models. NAACL 2022\nFindings, 2022.\n[33] Zhao Wang and Aron Culotta. Identifying spurious cor-\nrelations for robust text classification. arXiv preprint\narXiv:2010.02458, 2020.\n[34] Zhibo Wang, Peng Kuang, Zhixuan Chu, Jingyi Wang,\nand Kui Ren. Towards real world debiasing: A fine-\ngrained analysis on spurious correlation. arXiv preprint\narXiv:2405.15240, 2024.\n[35] Adina Williams, Nikita Nangia, and Samuel R Bow-\nman.\nA\nbroad-coverage\nchallenge\ncorpus\nfor\nsen-\ntence understanding through inference. North Ameri-\ncan Chapter of the Association for Computational Lin-\nguistics (NAACL), 2018.\n[36] Yuan Zhang, Jason Baldridge, and Luheng He. Paws:\nParaphrase adversaries from word scrambling. arXiv\npreprint arXiv:1904.01130, 2019.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2025-02-25",
  "updated": "2025-02-25"
}