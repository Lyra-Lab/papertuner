{
  "id": "http://arxiv.org/abs/2502.20089v1",
  "title": "RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning",
  "authors": [
    "Adib Karimi",
    "Mohammad Mehdi Ebadzadeh"
  ],
  "abstract": "We introduce a novel Inverse Reinforcement Learning (IRL) approach that\novercomes limitations of fixed reward assignments and constrained flexibility\nin implicit reward regularization. By extending the Maximum Entropy IRL\nframework with a squared temporal-difference (TD) regularizer and adaptive\ntargets, dynamically adjusted during training, our method indirectly optimizes\na reward function while incorporating reinforcement learning principles.\nFurthermore, we integrate distributional RL to capture richer return\ninformation. Our approach achieves state-of-the-art performance on challenging\nMuJoCo tasks, demonstrating expert-level results on the Humanoid task with only\n3 demonstrations. Extensive experiments and ablation studies validate the\neffectiveness of our method, providing insights into adaptive targets and\nreward dynamics in imitation learning.",
  "text": "RIZE: Regularized Imitation Learning via Distributional\nReinforcement Learning\nAdib Karimi\nAmirkabir University of Technology\nadibkarimi23@aut.ac.ir\nMohammad Mehdi Ebadzadeh\nAmirkabir University of Technology\nebadzadeh@aut.ac.ir\nAbstract\nWe introduce a novel Inverse Reinforcement Learning (IRL) approach that overcomes limita-\ntions of fixed reward assignments and constrained flexibility in implicit reward regularization.\nBy extending the Maximum Entropy IRL framework with a squared temporal-difference (TD)\nregularizer and adaptive targets, dynamically adjusted during training, our method indirectly\noptimizes a reward function while incorporating reinforcement learning principles. Furthermore,\nwe integrate distributional RL to capture richer return information. Our approach achieves state-\nof-the-art performance on challenging MuJoCo tasks, demonstrating expert-level results on the\nHumanoid task with only 3 demonstrations. Extensive experiments and ablation studies validate\nthe effectiveness of our method, providing insights into adaptive targets and reward dynamics in\nimitation learning.1\n1\nIntroduction\nInverse Reinforcement Learning (IRL) [Abbeel and Ng, 2004] is a foundational paradigm in artificial\nintelligence, enabling agents to acquire complex behaviors by observing expert demonstrations. This\napproach has catalyzed progress in diverse domains such as robotics [Osa et al., 2018], autonomous\ndriving [Knox et al., 2023], and drug discovery [Ai et al., 2024]. By eliminating the need for explicitly\ndefined reward functions, IRL provides a practical framework for training agents in environments\nwhere designing such functions is infeasible.\nA prominent framework in IRL is Maximum Entropy (MaxEnt) IRL [Ziebart, 2010], which underpins\nmany state-of-the-art (SOTA) IL methods. Prior works have combined MaxEnt IRL with adversarial\ntraining [Ho and Ermon, 2016, Fu et al., 2018] to minimize divergences between agent and expert\ndistributions. However, these adversarial methods often suffer from instability during training. To\naddress this, recent research has introduced implicit reward regularization, which indirectly represents\nrewards via Q-value functions by inverting the Bellman equation. For instance, IQ-Learn [Garg et al.,\n2021] unifies reward and policy representations using Q-functions with an L2-norm regularization on\nrewards, while LSIQ [Al-Hafez et al., 2023] minimizes the chi-squared divergence between expert and\nmixture distributions, resulting in a squared temporal difference (TD) error objective analogous to\nSQIL [Reddy et al., 2020]. Despite its effectiveness, this method has limitations: LSIQ assigns fixed\n1The code is available at https://github.com/adibka/RIZE.\n1\narXiv:2502.20089v1  [cs.LG]  27 Feb 2025\nrewards (e.g., +1 for expert samples and -1 for agent samples), which constrains flexibility by treating\nall tasks and state-action pairs uniformly, limiting performance and requiring additional gradient\nsteps for convergence.\nWe present an extension of implicit reward regularization under the MaxEnt IRL framework through\ntwo methodological innovations: (1) adaptive target rewards (λπE, λπ) that dynamically adjust during\ntraining to replace static targets, enabling context-sensitive alignment of expert and policy data, and\n(2) distributional RL integration, where value distributions Zπ(s, a) [Bellemare et al., 2017] are\ntrained to capture richer return information while preserving theoretical consistency with Q-values,\nas the expectation of the value distribution is used for policy and value function optimization. By\nunifying adaptive target regularization with distributional insights [Zhou et al., 2023], our framework\naddresses prior rigidity in reward learning and empirically outperforms online IL baselines on\nMuJoCo benchmarks [Todorov et al., 2012], marking a novel integration of distributional RL into\nnon-adversarial IRL.\nOur contributions are as follow:\n• We introduce adaptive target rewards for implicit reward regularization to enable dynamic\nadjustments during training.\n• We leverage value distributions instead of classical Q-functions to capture richer insights from\nreturn distributions.\n• We demonstrate the effectiveness of our approach through extensive experiments on MuJoCo\ntasks, providing detailed analyses of implicit rewards and adaptive target mechanisms.\n2\nRelated Works\nImitation learning (IL) and inverse reinforcement learning (IRL) [Watson et al., 2023] are foundational\nparadigms for training agents to mimic expert behavior from demonstrations. Behavioral Cloning\n(BC) [Pomerleau, 1991], the simplest IL approach, treats imitation as a supervised learning problem by\ndirectly mapping states to expert actions. While computationally efficient, BC is prone to compound-\ning errors [Ross and Bagnell, 2011] due to covariate shift during deployment. The Maximum Entropy\nIRL framework [Ziebart, 2010] addresses this limitation by probabilistically modeling expert behav-\nior as reward maximization under an entropy regularization constraint, establishing a theoretical\nfoundation for modern IRL methods.\nThe advent of adversarial training marked a pivotal shift in IL methodologies. Ho and Ermon\n[2016] introduced Generative Adversarial Imitation Learning (GAIL), which formulates imitation\nlearning as a generative adversarial game [Goodfellow et al., 2014] where an agent learns a policy\nindistinguishable from the expert’s by minimizing the Jensen-Shannon divergence between their\nstate-action distributions. This framework was generalized by Ghasemipour et al. [2019] in f-GAIL,\nwhich replaces the Jensen-Shannon divergence with arbitrary f-divergences to broaden applicability.\nConcurrently, Kostrikov et al. [2019] proposed Discriminator Actor-Critic (DAC), improving sample\nefficiency via off-policy updates and terminal-state reward modeling while mitigating reward bias.\nRecent advances have shifted toward methods that bypass explicit reward function estimation.\n2\nKostrikov et al. [2020] introduced ValueDICE, an offline IL method that leverages an inverse Bellman\noperator to avoid adversarial optimization. Similarly, Garg et al. [2021] developed IQ-Learn, which\ncircumvents the challenges of MaxEnt IRL by optimizing implicit rewards derived directly from expert\nQ-values. A parallel research direction simplifies reward engineering by assigning fixed rewards to\nexpert and agent samples. Reddy et al. [2020] pioneered this approach with Soft Q Imitation Learning\n(SQIL), which assigns binary rewards to transitions from expert and agent trajectories. Most recently,\nAl-Hafez et al. [2023] proposed Least Squares Inverse Q-Learning (LSIQ), enhancing regularization\nby minimizing chi-squared divergence between expert and mixture distributions while explicitly\nmanaging absorbing states through critic regularization. Our work builds on these foundations by\nintegrating distributional RL [Dabney et al., 2018a] and introducing a squared TD regularizer with\nadaptive targets.\n3\nBackground\n3.1\nPreliminary\nWe consider a Markov Decision Process (MDP) [Puterman, 2014] to model policy learning in Rein-\nforcement Learning (RL). The MDP framework is defined by the tuple ⟨S, A, p0, P, R, γ⟩, where S\ndenotes the state space, A the action space, p0 the initial state distribution, P : S × A × S →[0, 1] the\ntransition kernel with P(· | s, a) specifying the likelihood of transitioning from state s given action a,\nR : S × A →R the reward function, and γ ∈[0, 1] the discount factor which tempers future rewards.\nA stationary policy π ∈Π is characterized as a mapping from states s ∈S to distributions over\nactions a ∈A. The primary objective in RL [Sutton and Barto, 2018] is to maximize the expected\nsum of discounted rewards, expressed as Eπ\n\u0002\n∑∞\nt=0 γtR(st, at)\n\u0003\n. Furthermore, the occupancy measure\nρπ(s, a) for a policy π ∈Π is given by (1 −γ)π(a | s) ∑∞\nt=0 γtP(st = s | π). The corresponding\nmeasure for an expert policy, πE, is similarly denoted by ρE. In Imitation Learning (IL), it is posited\nthat the expert policy πE remains unknown, and access is restricted to a finite collection of expert\ndemonstrations, represented as (s, a, s′).\n3.2\nDistributional Reinforcement Learning\nMaximum Entropy (MaxEnt) RL [Haarnoja et al., 2018] focuses on addressing the stochastic nature of\naction selection by maximizing the entropy of the policy, while Distributional RL [Bellemare et al.,\n2017] emphasizes capturing the inherent randomness in returns. Combining these perspectives, the\ndistributional soft value function Zπ : S × A →Z [Ma et al., 2020] for a policy π ∈Π encapsulates\nuncertainty in both rewards and actions. It is formally defined as:\nZπ(s, a) =\n∞\n∑\nt=0\nγt[R(st, at) + αH(π(· | st))],\n(1)\nwhere Z represents the space of action-value distributions, H(π) = Eπ[−log π(a | s)] denotes the\nentropy of the policy, and α > 0 balances entropy with reward.\nThe distributional soft Bellman operator Bπ\nD : Z →Z for a given policy π is introduced as\n3\n(Bπ\nDZ)(s, a) D= R(s, a) + γ[Z(s′, a′) −α log π(a′ | s′)], where s′ ∼P(· | s, a), a′ ∼π(· | s′), and D=\nsignifies equality in distribution. Notably, this operator exhibits contraction properties under the\np-Wasserstein metric, ensuring convergence to the optimal distributional value function.\nA practical approach to approximating the value distribution Z involves modeling its quantile function\nF−1\nZ (τ), evaluated at specific quantile levels τ ∈[0, 1] [Dabney et al., 2018b]. The quantile function is\ndefined as F−1\nZ (τ) = inf{z ∈R : τ ≤FZ(z)}, where FZ(z) = P(Z ≤z) is the cumulative distribution\nfunction of Z. For simplicity, we denote the quantile-based representation as Zπ\nτ (s, a) := F−1\nZ (τ).\nTo discretize this representation, we define a sequence of quantile levels, denoted as {τi}i=0,...,N−1,\nwhere 0 = τ0 < ... < τN−1 = 1. These quantiles partition the unit interval into N fractions. Sampling\nuniformly from this interval τ ∼U(0, 1), we obtain an approximation of the value distribution\nZπ\nτ (s, a), which provides quantile values at these specified levels.\n3.3\nInverse Reinforcement Learning\nGiven expert trajectory data, Maximum Entropy (MaxEnt) Inverse RL [Ziebart, 2010] aims to infer\na reward function R(s, a) from the family R = RS×A. Instead of assuming a deterministic expert\npolicy, this method optimizes for stochastic policies π ∈Π that maximize R while matching expert\nbehavior. Ho and Ermon [2016] extend this framework by introducing a convex reward regularizer\nψ : RS×A →¯R, leading to the adversarial objective:\nmax\nR∈R min\nπ∈Π L(π, R) = EρE[R(s, a)] −Eρπ[R(s, a)] −H(π) −ψ(R)\n(2)\nIQ-Learn [Garg et al., 2021] departs from adversarial training by implicitly representing rewards\nthrough Q-functions Q ∈Ω[Piot et al., 2014]. It leverages the inverse soft Bellman operator T π,\ndefined as:\n(T πQ)(s, a) = Q(s, a) −γEs′∼P(·|s,a),a′∼π(·|s′)[Q(s′, a′) −α log π(a′|s′)]\n(3)\nFor a fixed policy π, T π is bijective, ensuring a one-to-one correspondence between Q-values and\nrewards: T πQ = R and Q = (T π)−1R. This allows reframing the MaxEnt IRL objective in Q-policy\nspace as maxQ∈Ωminπ∈Π J (π, Q). IQ-Learn simplifies the problem by defining the implicit reward\nRQ(s, a) = T πQ(s, a) and applying an L2 regularizer ψ(RQ). The final objective becomes:\nJ (π, Q) = EρE[RQ(s, a)] −Eρπ[RQ(s, a)] −H(π) −c\n\u0000EρE[R2\nQ] + Eρπ[R2\nQ]\n\u0001\n.\n(4)\n4\nMethodology\nIn this section, we present a systematic framework for developing our proposed approach. We\nbegin by integrating Distributional RL with Imitation Learning (IL). Next, we introduce a novel\nregularization technique that enhances existing Implicit Reward methods through an adaptive reward\nmechanism. Finally, we propose our algorithm, which leverages the strengths of both Distributional\nRL and IL to achieve superior performance in complex environments.\n4\n4.1\nDistributional Value Function\nOur method diverges from traditional imitation learning by modeling value distributions as critics\nin an actor-critic framework [Zhou et al., 2023]. We propose that learning the full return distribu-\ntion Zπ(s, a)—rather than point estimates like Qπ(s, a)—enables better decision-making in complex\nenvironments. By optimizing policies using the expectation of Zπ(s, a), we derive a more robust\nlearning signal. This robustness stems from explicitly capturing environmental stochasticity through\ndistributional modeling, avoiding the limitations of Q-networks that collapse variability into deter-\nministic estimates. In Distributional RL, the action-value function is defined as Qπ(s, a) = E[Zπ(s, a)],\npreserving the mathematical properties required by the IQ-Learn framework, ensuring theoretical\nconsistency (see Lemma A.1 for more details).\n4.2\nImplicit Reward Regularization\nIn this section, we propose a novel regularizer for Inverse Reinforcement Learning (IRL) that refines\nexisting implicit reward formulations [Garg et al., 2021]. We define the implicit reward as RQ(s, a) =\nQπ(s, a) −γVπ(s′), where Vπ(s′) = Qπ(s′, a′) −α log π(a′|s′). Prior work often regularizes implicit\nrewards using L2-norms, treating them as squared Temporal Difference (TD) errors between rewards\nand fixed targets [Reddy et al., 2020, Al-Hafez et al., 2023]. While adopting a similar squared TD\nframework [Mnih et al., 2015], we introduce adaptive targets λπE (for expert policy πE) and λπ (for\nimitation policy π) to derive our convex regularizer Γ : RS×A →¯R:\nΓ(RQ, λ) = EρE\n\u0002(RQ −λπE)2\u0003 + Eρπ\n\u0002(RQ −λπ)2\u0003\n.\n(5)\nLemma 4.1. The implicit reward structure RQ and regularizer Γ (Equation 5) guarantee:\n• Bounded Rewards: RQ converges to a convex combination of adaptive targets λπE and λπ.\n• Temporal Consistency: Ensures |Q(s, a) −γQ(s′, a′)| ≤|λ| + |ϵ| + γα| log π(a′|s′)|,\nwhere ϵ is the optimization tolerance.\n(Proof: Appendix A.)\nThis framework stabilizes learning by tethering rewards RQ to adaptive targets that evolve with the\npolicy, unlike static methods (e.g., LSIQ/SQIL) that use fixed rewards (±1). By co-optimizing RQ\nand λ, we establish dynamic equilibrium—targets adapt to the current policy to avoid brittle regular-\nization while maintaining bounded Q-values. The temporal consistency property ensures smooth\nvalue function updates, which directly translates to robust policy updates—reducing variance and\nimproving the likelihood of convergence to the optimal policy. This dual adaptability distinguishes\nour approach from prior rigid regularization schemes.\n4.3\nAlgorithm\nWe introduce RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning,\nan algorithm enhancing implicit reward IRL through a convex regularizer inspired by TD error\nminimization. Unlike standard RL that uses environmental rewards, RIZE employs self-updating\n5\nadaptable targets, creating a somewhat reinforcement paradigm where rewards automatically align\nwith their moving targets through regularization (see Algorithm 1).\nCritic Updates\nLet Zπ\nϕ,τ(s, a) denote the return distribution and πθ(a|s) the policy. We compute Q-values as:\nQπ(s, a) =\nN−1\n∑\ni=0\n(τi+1 −τi)Zπ\nϕ,τi(s, a)\n(6)\nwhere τi are quantile fractions and Zπ\nϕ,τi(s, a) represents quantile-specific return distributions.\nThe core objective integrates a squared-error regularizer Γ(RQ, λ) (Equation 5) to stabilize target\nalignment:\nL(π, Q) =EρE[RQ] −Eρπ[RQ] −H(π)\n−c\n\u0002\nEρE[(RQ −λπE)2] + Eρπ[(RQ −λπ)2]\n\u0003\n(7)\nwhere implicit rewards RQ derive from: RQ(s, a) = Qπ(s, a) −γ [Qπ(s′, a′) −α log π(a′|s′)]\nAdaptive Target Updates\nTargets λπE and λπ self-update via the following objectives, creating a feedback loop where reward\nestimates continuously adapt to match their moving targets.\nmin\nλπE EρE [RQ −λπE]2 ,\nmin\nλπ Eρπ [RQ −λπ]2\n(8)\nAlgorithm 1 Regularized Imitation Learning via Distributional Reinforcement Learning\n1: Initialize value distribution Zϕ, policy πθ, and target rewards λπE, λπ\n2: for step i in {1, . . . , N} do\n3:\nCalculate Q(s, a) = E[Zϕ(s, a)] using Equation 6\n4:\nUpdate value Zϕ using objective from Equation 7\n5:\nϕt+1 ←ϕt −βQ∇ϕ[−J(ϕ)]\n6:\nUpdate policy πθ (like SAC)\n7:\nθt+1 ←θt + βπ∇θEs∼D,a∼πθ(·|s)[min\nk=1,2 Qk(s, a) −α log πθ(a|s)]\n8:\nUpdate target rewards λπ and λπE using objectives in 8\n9:\nλπ\nt+1 ←λπ\nt −βλ∇λπΓ(RQ, λ)\n10:\nλπE\nt+1 ←λπE\nt\n−βλ∇λπE Γ(RQ, λ)\n11: end for\n6\n5\nExperiments\nWe evaluate our algorithm on four MuJoCo [Todorov et al., 2012] benchmarks (HalfCheetah-v2,\nWalker2d-v2, Ant-v2, Humanoid-v2) against state-of-the-art imitation learning methods: IQ-Learn\n[Garg et al., 2021], LSIQ [Al-Hafez et al., 2023], and SQIL [Reddy et al., 2020]. All experiments\nare conducted with five seeds for statistical significance [Henderson et al., 2018]. To assess sample\nefficiency, we test each method using three and ten expert trajectories. For ten trajectories, we retain\nbaseline hyperparameters; for three trajectories (not reported in prior works), we adapt configurations\nfrom single-demonstration settings. IQ-Learn and LSIQ use their official implementations, while\nSQIL is evaluated using the LSIQ codebase. Results report mean performance across five seeds, with\nhalf a standard deviation to indicate uncertainty, and lines are smoothed for better readability. We\nnormalize episode returns based on expert performance.\nFigure 1: Normalized return of RIZE vs. online imitation learning baselines on Gym MuJoCo tasks.\nWe depict the sorted top 25% episodic returns across five seeds to evaluate convergence to expert-level\nbehavior. We evaluate with three and ten expert trajectories.\n5.1\nMain Results\nOur method outperforms LSIQ and SQIL across tasks, with IQ-Learn as the only competitive baseline.\nNotably, in the most complex environment, Humanoid-v2, our approach is the sole method achieving\nexpert-level performance, while all baselines fail (see Figure 1). This demonstrates our algorithm’s\nscalability to high-dimensional control problems. Additionally, our method shows superior sample\nefficiency, requiring fewer gradient steps to match expert performance compared to SOTA algorithms.\nThese results highlight the robustness and efficiency of our approach in tackling complex tasks (see\nFigure 2).\n5.2\nReward Dynamics Analysis\nOur implicit reward regularizer Γ(RQ, λ) (Equation 5), ensures proximity between the learned RQ\nand target rewards λ. As illustrated in Figure 3, when provided with sufficient expert trajectories (ten\ndemonstrations), the rewards for both the expert and the policy stabilize around consistent values,\nas observed in environments like HalfCheetah and Ant. While our method is not Adversarial IL, it\nextends MaxEnt IRL principles. This alignment is evident as the policy increasingly mirrors expert\nbehavior, causing the discriminator to converge and provide similar reward signals for both the\nexpert and policy samples. This convergence reflects an equilibrium between policy optimization and\nreward learning, demonstrating the stability and effectiveness of our approach.\n7\nFigure 2: Normalized return of RIZE vs. online imitation learning baselines on Gym MuJoCo tasks.\nWe use 10 expert trajectories for all tasks.\nFigure 3: Implicit reward curves for expert and policy samples on Gym MuJoCo tasks. We use 10\nexpert trajectories for all tasks.\n5.3\nAblation: Regularization Strategies\nWe investigate the effect of squared TD error regularization on the Ant-v2 task using three distinct\nformulations: Expert-focused TD: EρE[RQ −λπE]2 + Eρπ[RQ]2, Policy-focused TD: EρE[RQ]2 + Eρπ[RQ −\nλπ]2, Baseline L2: EρE[RQ]2 + Eρπ[RQ]2.\n8\nOur results in Figure 4 demonstrate that apply-\ning squared TD regularization to both expert\nand policy samples improves robustness. While\nthe expert-focused TD formulation outperforms\nthe policy-focused variant, neither consistently\nachieves expert-level behavior. Over time, both\nexhibit gradual performance degradation, under-\nscoring the challenges of maintaining stable imi-\ntation. This highlights the need for more refined\nregularization strategies to ensure consistent and\nreliable learning in imitation learning.\nFigure 4: Ablation study on the effect of various\nregularization strategies on the Ant-v2 task, evalu-\nated using three demonstrations. Turquoise repre-\nsents RIZE using the convex regularizer Γ.\n6\nConclusion\nWe present a novel IRL framework that overcomes the rigidity of fixed reward mechanisms through\ndynamic reward adaptation via context-sensitive regularization and probabilistic return estimation\nwith distributional RL. By introducing adaptive target rewards that evolve during training—replacing\nstatic assignments—our method enables nuanced alignment of expert and agent trajectories, while\nvalue distribution integration captures richer return dynamics without sacrificing theoretical con-\nsistency. Empirical evaluations on MuJoCo benchmarks demonstrate state-of-the-art performance,\nachieving expert-level proficiency on the Humanoid task with only three demonstrations, alongside\nablation studies confirming the necessity of applying our regularization mechanism. This work\nadvances imitation learning by unifying flexible reward learning with probabilistic return representa-\ntions, offering a scalable, sample-efficient paradigm for complex decision making. Future directions\ninclude extending these principles to offline IL and risk-sensitive robotics, where adaptive rewards\nand uncertainty-aware distributions could further enhance robustness and generalization.\n9\nReferences\nP. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In International\nConference on Machine Learning, 2004.\nCheng Ai, Hong Yang, Xinyu Liu, Ruijie Dong, Ying Ding, and Fei Guo. Mtmol-gpt: De novo\nmulti-target molecular generation with transformer-based generative adversarial imitation learning.\nPLoS Computational Biology, 20(6):e1012229, 2024. doi: 10.1371/journal.pcbi.1012229. URL https:\n//doi.org/10.1371/journal.pcbi.1012229.\nFiras Al-Hafez, Davide Tateo, Oleg Arenz, Guoping Zhao, and Jan Peters. LS-IQ: Implicit reward\nregularization for inverse reinforcement learning. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=o3Q4m8jg4BR.\nMarc G. Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement learn-\ning. In International Conference on Machine Learning, 2017. URL https://api.semanticscholar.\norg/CorpusID:966543.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\nWill Dabney, Georg Ostrovski, David Silver, and Remi Munos.\nImplicit quantile networks for\ndistributional reinforcement learning. In Proceedings of the 35th International Conference on Machine\nLearning, Proceedings of Machine Learning Research, pages 1096–1105. PMLR, 2018a.\nWill Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional reinforcement\nlearning with quantile regression. In Proceedings of the Thirty-Second AAAI Conference on Artificial\nIntelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI\nSymposium on Educational Advances in Artificial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press,\n2018b. ISBN 978-1-57735-800-8.\nJustin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-\nment learning. In International Conference on Learning Representations, 2018.\nDivyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn:\nInverse soft-q learning for imitation. In Advances in Neural Information Processing Systems, volume 34,\n2021.\nSeyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization\nperspective on imitation learning methods. In Conference on Robot Learning, 2019.\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. In Advances in Neural\nInformation Processing Systems, volume 27, 2014.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International Conference\non Machine Learning, 2018.\n10\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.\nDeep reinforcement learning that matters. In Proceedings of the Thirty-Second AAAI Conference on\nArtificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth\nAAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI\nPress, 2018. ISBN 978-1-57735-800-8.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural\nInformation Processing Systems, volume 29, 2016.\nW. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward\n(mis)design for autonomous driving. Artif. Intell., 316(C), March 2023. ISSN 0004-3702. doi:\n10.1016/j.artint.2022.103829. URL https://doi.org/10.1016/j.artint.2022.103829.\nIlya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson.\nDiscriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation\nlearning. In International Conference on Learning Representations, 2019.\nIlya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution\nmatching. In International Conference on Learning Representations, 2020.\nXiaoteng Ma, Qiyuan Zhang, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. Dsac:\nDistributional soft actor critic for risk-sensitive reinforcement learning. arXiv, 2020.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,\nand Martin Riedmiller. Human-level control through deep reinforcement learning. Nature, 518\n(7540):529–533, 2015.\nTakayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, and Jan Peters.\nAn algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 2018.\nB. Piot, M. Geist, and O. Pietquin. Boosted and Reward-regularized Classification for Apprenticeship\nLearning. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS), 2014.\nD. A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural\nComputation, 3(1):88–97, 1991.\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &\nSons, 2014.\nSiddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement\nlearning with sparse rewards. In International Conference on Learning Representations, 2020.\nSt´ephane Ross and J. Andrew Bagnell. A reduction of imitation learning and structured prediction to\nno-regret online learning. In International Conference on Artificial Intelligence and Statistics, 2011.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,\nCambridge, Massachusetts; London, England, second edition, 2018.\n11\nE. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In 2012 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems, pages 5026–5033, 2012.\nJoe Watson, Sandy Huang, and Nicolas Heess. Coherent soft imitation learning. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.net/\nforum?id=kCCD8d2aEu.\nY. Zhou, M. Lu, X. Liu, Z. Che, Z. Xu, J. Tang, and Y Zhang. Distributional generative adversarial\nimitation learning with reproducing kernel generalization. Neural Networks, 165, 2023.\nBrian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD\nthesis, University of Washington, 2010.\n12\nA\nProofs\nLemma A.1. Consider an MDP with discount factor γ ∈[0, 1) and rewards bounded by some constant C > 0\nsuch that |R(s, a)| ≤C. Define the return distribution as\nZ(s, a) =\n∞\n∑\nt=0\nγtR(st, at),\nwhere the trajectory {(st, at)} is generated by following policy π under dynamics P. Then the classical\naction-value function is given by\nQπ(s, a) = Eπ,P[Z(s, a)].\nMoreover, if we approximate Z(s, a) by a learned distribution Zθ(s, a) with expectation ˜Q(s, a) = E[Zθ(s, a)],\nthen under the conditions of the Dominated Convergence Theorem (DCT), ˜Q(s, a) converges to Qπ(s, a).\nProof of Lemma A.1.\nExistence of the Expectation: Since the rewards are bounded, we have |R(st, at)| ≤C, and the\ndiscounted series satisfies\nE\n\"\n∞\n∑\nt=0\nγt|R(st, at)|\n#\n≤\n∞\n∑\nt=0\nγtC =\nC\n1 −γ < ∞.\nThis absolute convergence allows us to apply DCT:\nE[Z(s, a)] = E\n\"\n∞\n∑\nt=0\nγtR(st, at)\n#\n=\n∞\n∑\nt=0\nγtE[R(st, at)].\nEquivalence to Q-value: By definition,\nQπ(s, a) = Eπ,P\n\"\n∞\n∑\nt=0\nγtR(st, at)\n#\n= E[Z(s, a)].\nConvergence of the Learned Distribution: In distributional reinforcement learning, if we represent\nthe return distribution using Zθ(s, a), then its mean is given by\n˜Q(s, a) = E[Zθ(s, a)].\nUnder DCT conditions, we have\n˜Q(s, a) →Qπ(s, a).\n13\nProof of Lemma 4.1.\n1. Bounded Rewards\nThe expert and policy reward targets λπE, λπ are learned via:\nλπE ←EρE[RQ],\nλπ ←Eρπ[RQ].\nThis ensures λπE and λπ track the empirical mean of RQ.\nFixing λπE and λπ, solving ∇RQΓ = 0 yields:\nRQ = ρEλπE + ρπλπ\nρE + ρπ\n,\nThis is a convex combination of λπE and λπ. Since λπE, λπ are empirical means, |RQ| ≤max(|λπE|, |λπ|),\nensuring boundedness.\n2. Temporal Consistency\nThe reward RQ (for either expert or policy samples) is defined via the soft Q-function:\nRQ = Q(s, a) −γEs′ \u0002\nQ(s′, a′) −α log π(a′|s′)\n\u0003\n.\nRearranging gives:\nQ(s, a) = RQ + γEs′ \u0002\nQ(s′, a′) −α log π(a′|s′)\n\u0003 + ϵ,\nwhere ϵ is the Bellman optimization error (due to approximation or finite samples). Substituting\nRQ = λ:\nQ(s, a) −γQ(s′, a′) = λ + ϵ −γα log π(a′|s′).\nTaking absolute values:\n|Q(s, a) −γQ(s′, a′)| ≤|λ| + |ϵ| + γα| log π(a′|s′)|.\nThis bounds successive Q-values, ensuring temporal consistency.\nConclusion.\nThe adaptive regularizer Γ (Equation 5) enforces bounded rewards via convex combinations of\nempirical targets and ensures temporal consistency through entropy-regularized Q-value recursion.\nBoth properties stabilize learning by preventing divergence.\n14\nB\nExperiments\nB.1\nMuJoCo Control Suite\nWe evaluate our imitation learning approach, RIZE, on four benchmark Gym [Brockman et al., 2016]\nMuJoCo [Todorov et al., 2012] environments: HalfCheetah-v2, Walker2d-v2, Ant-v2, and Humanoid-v2.\nExpert trajectories are generated using a pretrained Soft Actor-Critic (SAC) [Haarnoja et al., 2018]\nagent, with each trajectory consisting of 1,000 state-action transitions. To facilitate performance\ncomparisons, episode returns are normalized relative to expert performance: HalfCheetah (5100),\nWalker2d (5200), Ant (4700), and Humanoid (5300).\nB.2\nImplementation Details\nOur architecture integrates components from Distributional SAC (DSAC)2 [Ma et al., 2020] and\nIQ-Learn3 [Garg et al., 2021], with hyperparameters tuned through systematic search and ablation\nstudies. Key configurations for experiments involving three demonstrations are summarized in\nTable 1, while Table 2 provides settings for scenarios with ten demonstrations. Our implementation\ncan be found at https://github.com/adibka/RIZE.\nDistributional SAC Components: The critic network is implemented as a three-layer multilayer\nperceptron (MLP) with 256 units per layer, trained using a learning rate of 3 × 10−4. The policy\nnetwork is a four-layer MLP, also with 256 units per layer. Environment-specific learning rates are\napplied: 1 × 10−5 for the Humanoid environment and 5 × 10−5 for all others. To enhance training\nstability, we employ a target policy—a delayed version of the online policy—and sample next-state\nactions from this module. For value distribution training Zπ\nϕ,τ, we adopt the Implicit Quantile\nNetworks (IQN) [Dabney et al., 2018a] approach by sampling quantile fractions τ uniformly from\nU(0, 1). Additionally, dual critic networks with delayed updates are used, which empirically improve\ntraining stability.\nIQ-Learn Adaptations: Key adaptations from IQ-Learn include adjustments to the regularizer coef-\nficient c and entropy coefficient α. Specifically, for the regularizer coefficient c, we find that c = 0.5\nyields robust performance on the Humanoid task, while c = 0.1 works better for other tasks. For the\nentropy coefficient α, smaller values lead to more stable training. Unlike RL, where exploration is\ncrucial, imitation learning relies less on entropy due to the availability of expert data. Across all tasks,\nwe set initial target reward parameters as λπE = 10 and λπ = 5. Furthermore, we observe that lower\nlearning rates for target rewards improve overall learning performance.\nPrevious implicit reward methods such as IQLearn, ValueDICE, and LSIQ4 have employed distinct\nmodifications to the loss function. In our setup, two main loss variants are defined:\n2https://github.com/xtma/dsac\n3https://github.com/Div99/IQ-Learn\n4https://github.com/robfiras/ls-iq/tree/main\n15\n1. Value loss:\nL(π, Q) = EρE[Qπ(s, a) −γVπ(s′)] −Eρ[Vπ(s) −γVπ(s′)] −cΓ(RQ, λ)\n(9)\n2. v0 loss:\nL(π, Q) = EρE[Qπ(s, a) −γVπ(s′)] −(1 −γ)Ep0[Vπ(s0)] −cΓ(RQ, λ)\n(10)\nHere, ρ is a mixture distribution, p0 denotes the initial distribution, RQ(s, a) is the implicit reward\ndefined as RQ(s, a) = Qπ(s, a) −γVπ(s′), the state-value function is given by Vπ(s′) = Qπ(s′, a′) −\nα log π(a′|s′), and lastly, our convex regularizer is expressed as Γ(RQ, λ) = EρE[(RQ −λπE)2] +\nEρπ[(RQ −λπ)2].\nThe choice between v0 or value loss variants depends on environment complexity: we find that for\na complex task like Humanoid-v2, the v0 variant demonstrates greater robustness. And, for simpler\ntasks such as HalfCheetah-v2, Walker2d-v2, and Ant-v2, the value variant performs better.\nTable 1: Three Demonstrations Hyperparameters\nEnvironment\nα\nc\nlr π\nλπE\nlr λπE\nλπ\nlr λπE\nAnt-v2\n0.05\n0.1\n5 × 10−5\n10\n1 × 10−4\n5\n1 × 10−5\nHalfCheetah-v2\n0.05\n0.1\n5 × 10−5\n10\n1 × 10−4\n5\n1 × 10−5\nWalker2d-v2\n0.05\n0.1\n5 × 10−5\n10\n1 × 10−4\n5\n1 × 10−5\nHumanoid-v2\n0.05\n0.5\n1 × 10−5\n10\n1 × 10−4\n5\n5 × 10−5\nTable 2: 10 Demonstrations Hyperparameters\nEnvironment\nα\nc\nlr π\nλπE\nlr λπE\nλπ\nlr λπE\nAnt-v2\n0.1\n0.1\n5 × 10−5\n10\n1 × 10−4\n5\n1 × 10−4\nHalfCheetah-v2\n0.1\n0.1\n5 × 10−5\n10\n1 × 10−4\n5\n1 × 10−4\nWalker2d-v2\n0.1\n0.1\n5 × 10−5\n10\n1 × 10−4\n5\n1 × 10−4\nHumanoid-v2\n0.1\n0.5\n1 × 10−5\n10\n1 × 10−4\n5\n1 × 10−5\nB.3\nExtended Main Results\nIn this section, we showcase the performance of our method, RIZE, using both ten and three expert\ndemonstrations. RIZE outshines LSIQ and SQIL in various tasks, with IQ-Learn as its only competitor.\nRemarkably, it reaches expert-level performance in the challenging Humanoid-v2 environment with\njust three trajectories, proving its scalability. Plus, it requires fewer training steps than other algorithms,\nemphasizing its efficiency and robustness in complex situations. Our findings demonstrate that using\nfewer expert samples does not negatively impact our imitation learning approach (see Figure 5).\nB.4\nExtended Recovered Rewards\nHere, we investigate the implicit reward dynamics of our method using both three and ten expert\ndemonstrations. As illustrated in Figure 6, with ample expert trajectories (n=10), the rewards for both\n16\nFigure 5: Normalized return of RIZE vs. online imitation learning baselines on Gym MuJoCo tasks. n\ndenotes the number of expert trajectories.\nthe expert and the policy stabilize to consistent values across most environments. Notably, even with\na three demonstrations, which presents a significant learning challenge, rewards still converge to\nsimilar values in tasks such as HalfCheetah and Humanoid.\nOur implicit reward regularization term, Γ(RQ, λ), ensures the learned rewards stay close to the\ntarget rewards. Looking at Figure X, you can see that optimizing λπE and λπ using Equations X\neffectively bounds both the implicit and target rewards, boosting overall stability. Interestingly, for\ncertain tasks like HalfCheetah-v2, Walker2d-v2, and Ant-v2, λπ doesn’t change much. This is because\nwe deliberately used smaller learning rates, which proved crucial for maintaining training stability.\nB.5\nHyperparameter Fine-Tuning\nWe present our analysis and comparison of important hyperparameters utilized in our algorithm. As\nbefore, all experiments use five seeds, and we show mean and half a standard deviation of all the\nseeds. As a challenging task among MuJoCo environments, we only experiment with Ant-v2.\nTarget Reward λ\nSelecting appropriate initial values and learning rates for the automatic fine-tuning of λπE and λπ\nis critical in our approach. First, we observe that a suitable learning rate is essential for the stable\ntraining of our imitation learning agent, as illustrated in Figure 8a. Our findings indicate that λπ must\nbe optimized very slowly; using larger learning rates can destabilize training and hinder progress. In\ncontrast, λπE demonstrates greater resilience when optimized with higher learning rates. Additionally,\nλπE remains robust even with varying initial values. However, as shown in Figure 8b, failing to\nselect an appropriate initial value for λπ can negatively impact learning. Overall, Figures 8a and 8b\nhighlight the need for careful selection of both the learning rate and initial value when optimizing λπ,\n17\nFigure 6: Implicit reward curves for expert and policy samples on Gym MuJoCo tasks. n represents\nthe number of expert demonstrations.\nwhile λπE exhibits considerable robustness in this regard.\nRegularization Coefficient C\nOur experiments on the regularizer coefficient, C, reveal that smaller values of C encourage expert-like\nperformance, while larger values overly constrain rewards and targets, limiting learning. This finding\nhighlights the critical role of selecting an appropriate C, as it directly impacts the balance between\nlearning from expert data and regularization: higher values prioritize regularization at the cost of\nlearning, whereas smaller values favor learning but reduce regularization (see Figure 9).\nEntropy Coefficient α\nWe observe that the entropy coefficient is a crucial hyperparameter in inverse reinforcement learning\n(IRL) problems. As shown in Figure 10, IRL methods typically require small values for α, a point\npreviously noted by Reference X. With expert demonstrations available, an imitation learning (IL)\npolicy does not need to explore for optimal actions, as these are provided by the demonstrations.\nConsequently, higher values of α can lead to training instability, ultimately resulting in policy collapse.\n18\nFigure 7: Target reward values for expert (λπE) and policy (λπ) on Gym MuJoCo tasks. n represents\nthe number of expert demonstrations.\n(a)\n(b)\nFigure 8: Fine-tuning of target parameters. (a) Turquoise represents our method’s primary result with\nlearning rates of 1e−4 for λπE and 1e−5 for λπ. Orange and blue lines indicate higher learning rates\n(e.g., 0.001) for λπE and λπ, respectively. (b) Turquoise shows the main result with initial values of 10\nfor λπE and 5 for λπ, while other lines explore different starting values. Only one parameter is varied\nat a time, and three trajectories are used throughout the analysis.\n19\nFigure 9: Fine-tuning of the regularizer coefficient (C). Turquoise shows our method’s primary result\nwith C = 0.1 for Ant-v2, while gray indicates a larger value (C = 0.6). Only one parameter is varied\nat a time, and three trajectories are used throughout the analysis.\nFigure 10: Fine-tuning of the temperature parameter (α). Turquoise shows our method’s primary\nresult with α = 0.05 for Ant-v2, while gray indicates a larger value (α = 0.5). Only one parameter is\nvaried at a time, and three trajectories are used throughout the analysis.\n20\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2025-02-27",
  "updated": "2025-02-27"
}