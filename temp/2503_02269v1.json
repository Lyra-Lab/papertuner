{
  "id": "http://arxiv.org/abs/2503.02269v1",
  "title": "Experience Replay with Random Reshuffling",
  "authors": [
    "Yasuhiro Fujita"
  ],
  "abstract": "Experience replay is a key component in reinforcement learning for\nstabilizing learning and improving sample efficiency. Its typical\nimplementation samples transitions with replacement from a replay buffer. In\ncontrast, in supervised learning with a fixed dataset, it is a common practice\nto shuffle the dataset every epoch and consume data sequentially, which is\ncalled random reshuffling (RR). RR enjoys theoretically better convergence\nproperties and has been shown to outperform with-replacement sampling\nempirically. To leverage the benefits of RR in reinforcement learning, we\npropose sampling methods that extend RR to experience replay, both in uniform\nand prioritized settings. We evaluate our sampling methods on Atari benchmarks,\ndemonstrating their effectiveness in deep reinforcement learning.",
  "text": "Experience Replay with Random Reshuffling\nYasuhiro Fujita1\nfujita@preferred.jp\n1Preferred Networks, Inc.\nAbstract\nExperience replay is a key component in reinforcement learning for stabilizing learn-\ning and improving sample efficiency. Its typical implementation samples transitions\nwith replacement from a replay buffer. In contrast, in supervised learning with a fixed\ndataset, it is a common practice to shuffle the dataset every epoch and consume data\nsequentially, which is called random reshuffling (RR). RR enjoys theoretically better\nconvergence properties and has been shown to outperform with-replacement sampling\nempirically. To leverage the benefits of RR in reinforcement learning, we propose sam-\npling methods that extend RR to experience replay, both in uniform and prioritized\nsettings. We evaluate our sampling methods on Atari benchmarks, demonstrating their\neffectiveness in deep reinforcement learning.\n1\nIntroduction\nReinforcement learning (RL) agents often learn from sequentially collected data, which can lead to\nhighly correlated updates and unstable learning. Experience replay was introduced to mitigate this\nissue by storing past transitions and sampling them later for training updates (Lin, 1992; Mnih et al.,\n2015). By drawing samples from a replay memory, the agent breaks the temporal correlations in the\ntraining data, leading to more stable and sample-efficient learning (Mnih et al., 2015). Typically,\nthese samples are drawn with replacement at random from the replay buffer. This approach treats\npast experiences as a reusable dataset like in supervised learning (SL).\nHowever, there is a key difference between SL and RL in how data can be reused. In SL, one\ncan reshuffle the entire dataset each epoch, guaranteeing that each sample is seen exactly once\nper epoch. In contrast, standard experience replay in RL samples experiences with replacement,\nmeaning some experiences may be seen multiple times while others are completely missed. In\nthe stochastic optimization literature, random reshuffling (RR)—shuffling the data each epoch—\nhas been shown to yield faster convergence than sampling with replacement in many cases, both\nin theory and in practice (Bottou, 2009; 2012; HaoChen & Sra, 2019; Nagaraj et al., 2019; Rajput\net al., 2020; Mishchenko et al., 2020; Gürbüzbalaban et al., 2021; Beneventano, 2023). While the\nassumptions of the theoretical results may not hold in RL settings in general, this insight suggests\nthat a similar strategy in experience replay might outperform the conventional sampling method.\nTo explore this possibility, we propose two novel methods that integrate RR. For uniform experience\nreplay, our method RR-C (random reshuffling with a circular buffer) applies reshuffling to the indices\nof a circular buffer so that each index is sampled exactly once per epoch, thereby reducing variance\nof sample counts and ensuring a balanced use of past experiences. For prioritized experience replay,\nwe introduce RR-M (random reshuffling by masking), which adapts RR to settings with dynamic\ntransition priorities by dynamically masking transitions that have been sampled more often than\nexpected. This strategy effectively approximates an epoch-level without-replacement sampling even\nas priorities continuously change.\nCode: https://github.com/pfnet-research/errr.\n1\narXiv:2503.02269v1  [cs.LG]  4 Mar 2025\nOur contributions are summarized as follows:\n• We bridge the gap between SL and RL sampling strategies by adapting RR to experience replay,\naddressing the unique challenges of dynamic buffer contents and changing priorities.\n• For uniform experience replay, we propose RR-C, which applies RR to circular buffer indices\nrather than transitions themselves, ensuring balanced utilization while maintaining computational\nefficiency and transition freshness.\n• For prioritized experience replay, we develop RR-M, which tracks actual versus expected sam-\nple counts and masks out priorities of oversampled transitions, providing the variance reduction\nbenefits of RR while respecting priority-based sampling.\n• We empirically demonstrate through experiments on Atari benchmarks that both RR-C and RR-M\nprovide modest but consistent performance improvements across different deep RL algorithms:\nDQN, C51, and DDQN+LAP.\n2\nPreliminaries\n2.1\nRandom reshuffling\nIn machine learning, many optimization problems involve minimizing an empirical loss F(w) =\n1\nN\nPN\ni=1 fi(w), where each fi(w) represents the loss on a data sample. Stochastic gradient descent\n(SGD) is a prevalent method for such tasks, especially when N is large. At each iteration t, SGD ran-\ndomly selects an index it ∈{1, . . . , N} and updates the parameter w as wt+1 = wt −ηt∇fit(wt),\nwith ηt being the learning rate. Sampling it with replacement from {1, . . . , N} provides an unbi-\nased estimate of the full gradient ∇F(wt), supporting convergence guarantees under appropriate\nconditions (Robbins & Monro, 1951).\nRandom reshuffling (RR) is an alternative way of sampling it that shuffles the order of the dataset\n{1, . . . , N} every epoch and sequentially processes each sample exactly once per epoch. This ap-\nproach ensures uniform data coverage within each cycle, potentially reducing variance in updates.\nNot only is it widely used in practice, but RR has also been shown to outperform with-replacement\nsampling theoretically under certain conditions; for example, achieving a faster convergence rate of\nO(1/K2) compared to the O(1/K) rate of with-replacement sampling SGD for smooth and strongly\nconvex loss functions, given a sufficiently large number of epochs K (Nagaraj et al., 2019).\n2.2\nExperience replay\nExperience replay is a technique for RL that stores past transitions (st, at, rt, st+1) in a replay\nbuffer and samples a minibatch from the buffer to update the parameters of a neural network (Lin,\n1992; Mnih et al., 2015). Typically, the replay buffer is a FIFO (first-in first-out) queue with a fixed\ncapacity and implemented as a circular buffer, where new transitions overwrite old transitions when\nthe buffer is full. If the probability of sampling a transition is uniform across the buffer, we call it\nuniform experience replay.\nPrioritized experience replay (Schaul et al., 2016) is a popular variant of experience replay that\nassigns a priority to each transition and samples transitions with a probability proportional to their\npriority. How to compute the priority of a transition is a key design choice in prioritized experience\nreplay, and several methods have been proposed in the literature (Fujimoto et al., 2020; Oh et al.,\n2022; Saglam et al., 2023; Sujit et al., 2023). The probability of sampling a transition is defined as\nP(i) = pi/ P\nk pk, where pi is the priority of transition i. The priority is updated after a transition\nis sampled, which makes it non-stationary. We can consider uniform experience replay as a special\ncase of prioritized experience replay where the priority of each transition is fixed to a global constant.\nTypical implementations use a sum tree data structure to efficiently sample transitions according to\npriorities (Schaul et al., 2016).\n2\n3\nExperience replay with random reshuffling\n3.1\nSampling methods for experience replay\nGiven the probabilities of transitions, flexibility remains in how minibatches are sampled accord-\ning to these probabilities. We summarize the sampling methods of experience replay in popular\nRL code bases in Table 3 in the supplementary materials. The most common method for both uni-\nform and prioritized experience replay is to sample transitions with replacement, where the same\ntransition can be sampled multiple times in a minibatch. The sample code for uniform experience\nreplay is shown as sample_with_replacement in Figure 1. Other approaches include within-\nminibatch without-replacement sampling (shown as sample_without_replacement in Fig-\nure 1) and stratified sampling that segments the range of query values into strata and samples from\neach stratum, both of which prevent sampling the same transition multiple times in a minibatch.\nThese sampling methods still exhibit variance in the number of times each transition is sampled\nthroughout the training process. It is possible that some transitions are never sampled while other\ntransitions are sampled multiple times even in the case of uniform experience replay, where each\ntransition is considered equally important. Intuitively, this variance is unnecessary and can be miti-\ngated by a better sampling method like RR. The variance reduction could lead to more stable learning\nand better sample efficiency. Why is RR not used in experience replay?\nWhile RR has been shown to be superior in SL, it is not directly applicable to experience replay\nin RL1. Both the size and the content of the replay buffer are dynamically changing, which makes\nit difficult to apply RR directly. Prioritized experience replay introduces additional complexity,\nwhere the priority of a transition is not fixed and changes every time it is sampled. We tackle these\nchallenges by proposing two sampling methods that extend RR to experience replay, both in uniform\nand prioritized settings, which we describe in the following sections.\n3.2\nUniform experience replay with random reshuffling\nOur aim is to extend RR to experience replay in a way that satisfies the following properties:\n• Equivalence to RR: If the transitions in the buffer are fixed, it should work just like RR in SL.\n• Freshness: Once a new transition is added to the buffer, it should be ready to be sampled in the\nnext minibatch.\nWe propose a simple extension of RR that satisfies these properties: applying RR to the indices\nof a circular buffer, rather than to the transitions themselves, assuming that the replay buffer is\nimplemented as a circular buffer. We call this method random reshuffling with a circular buffer\n(RR-C).\nThe sample code of RR-C is shown as sample_rrc of Figure 1. Specifically, we create a shuffled\nlist of indices [0, 1, . . . , C −1] (rr_buffer in the figure), where C is the capacity of the circular\nbuffer, and sequentially consume the list when we sample a minibatch during an epoch. When we\nreach the end of the list, we create a new shuffled list of indices and start a new epoch. It is easy to\nsee that each index is sampled exactly once in an epoch, which is the key property of RR. Although\nold transitions are overwritten by new transitions in a circular buffer as RL training proceeds, if a\ntransition stays in the buffer for at least n whole epochs, we can guarantee that it is sampled at least\nn times. If a sampled index has not been assigned to a transition yet, we simply skip it and continue\nwith the next.\nThis method is easy to implement and has no hyperparameters to be tuned. It is also computationally\nefficient because we only need to make a shuffled list of indices at the beginning of each epoch.\n1Offline RL algorithms with a fixed dataset and online RL algorithms that repeat minibatch updates over the latest batch\nof transitions like PPO (Schulman et al., 2017) can be exceptions if we define experience replay so that it includes these\ncases.\n3\n1 import numpy\n2\n3 def sample_with_replacement(transitions: list, batch_size: int) -> list:\n4\n\"\"\"Sample with replacement.\"\"\"\n5\nindices = numpy.random.randint(0, len(transitions), batch_size)\n6\nreturn [transitions[i] for i in indices]\n7\n8 def sample_without_replacement(transitions: list, batch_size: int) -> list:\n9\n\"\"\"Sample without replacement.\"\"\"\n10\nindices = numpy.random.choice(len(transitions), batch_size, replace=False)\n11\nreturn [transitions[i] for i in indices]\n12\n13 def sample_rrc(transitions: list, batch_size: int, max_size: int, rr_buffer: list)\n-> list:\n14\n\"\"\"Sample with RR-C.\n15\n16\nmax_size: the capacity of the replay buffer\n17\nrr_buffer: a list of shuffled indices\n18\n\"\"\"\n19\nindices = []\n20\nwhile len(indices) < batch_size:\n21\nif len(rr_buffer) == 0:\n22\nrr_indices = numpy.arange(0, max_size)\n23\nnumpy.random.shuffle(rr_indices)\n24\nrr_buffer[:] = list(rr_indices)\n25\ni = rr_buffer.pop()\n26\nif i < len(transitions):\n27\nindices.append(i)\n28\nreturn [transitions[i] for i in indices]\nFigure 1: Python code examples of sampling methods for uniform experience replay. This simplified\ncode is for illustrative purposes and is not identical to actual implementations.\n3.3\nPrioritized experience replay with random reshuffling\nSimilarly to the uniform case, we aim to extend RR to prioritized experience replay in a way that\nsatisfies the following properties:\n• Reducibility to RR: If the transitions in the buffer are fixed and their probabilities are all the\nsame, it should work just like RR in SL.\n• Equivalence to RR with repeated data: If the priorities of transitions are all rational numbers,\nwe can consider an epoch where each transition appears exactly how many times it should be\nsampled in expectation. For example, given three transitions whose priorities are fixed to [1, 0.5,\n2], for an epoch of length 7, the sampling method should sample the first transition twice, the\nsecond once, and the third four times.\n• Freshness: Same as in uniform experience replay.\nFor a simple case, it seems possible to make a list of shuffled indices and consume it sequen-\ntially as in RR-C to satisfy the equivalence to RR with repeated data, e.g., by shuffling the list\n[0, 0, 1, 2, 2, 2, 2] for the example above. However, this quickly becomes impractical for large buffer\nsizes. Also, it is not clear how to reflect the updated priorities of transitions in the shuffled list.\nWe propose a method that satisfies the above properties by masking the priorities of transitions\nthat have been oversampled relative to expectation, which we call RR by masking (RR-M). The\nsample code of this method is shown as sample_rrm in Figure 2. It keeps actual and expected\nsample counts of transitions and updates them every time a minibatch is sampled. When a transition\nis deemed oversampled, it is masked by setting its priority to a tiny value2 until its expected count\n2In our experiments, we multiply the original priority by 1e-8, not 0, so that the algorithm would work if all the transitions\nwere deemed oversampled due to numerical errors.\n4\ncatches up. When old transitions are overwritten by new transitions, we reset the actual and expected\ncounts of the overwritten transitions to zero and scale the expected counts so their sum matches the\nsum of actual counts. When we sample a minibatch, we do within-minibatch without-replacement\nsampling according to the masked priorities. This can be achieved by querying the sum tree in\na batch manner, removing the duplicated indices if any, temporarily masking the priorities of the\nsampled transitions, resample only the amount that is less than the desired minibatch size, and\nrepeating the process until the minibatch is filled. We include example trajectories of the behavior\nof RR-M in Table 4 in the supplementary materials.\nThis method is also easy to implement and has no hyperparameters to be tuned, but it is relatively\ncomputationally expensive because every time we sample a minibatch, we need to update the counts\nof transitions, which takes O(n) time, where n is the number of transitions in the replay buffer. In\npractice, it is easy to parallelize the computation of the counts and oversampled indices, e.g., with\nGPUs. Having two separate sum trees to keep both correct priorities and masked priorities also\nhelps minimize the computational overhead as it has to update a mask of a transition only when it\nchanges whether it is oversampled. We discuss the computational cost in more detail in Section C\nin the supplementary materials.\n1 import numpy\n2\n3 def sample_rrm(transitions: list, sumtree: Any, batch_size: int, actual_counts:\nnumpy.ndarray, expected_counts: numpy.ndarray) -> list:\n4\n\"\"\"Sample with RR-M.\n5\n6\nsumtree: a sum tree data structure priorities and masks\n7\nactual_counts: how many times each transition has been sampled\n8\nexpected_counts: how many times each transition should have been sampled in\nexpectation\n9\n\"\"\"\n10\n# Check if each transition is oversampled (by elementwise comparison).\n11\noversampled = numpy.greater(actual_counts, expected_counts)\n12\n# Mask oversampled transitions and unmask others.\n13\nsumtree.update_mask(oversampled)\n14\n# Sample without replacement according to masked priorities.\n15\nindices = sumtree.sample(batch_size)\n16\n# Update counts.\n17\nfor i in indices:\n18\nactual_counts[i] += 1\n19\nexpected_counts += sumtree.probabilities * batch_size\n20\nreturn [transitions[i] for i in indices]\nFigure 2: A Python code example of our sampling method for prioritized experience replay, RR-M.\nThis simplified code is for illustrative purposes and is not identical to actual implementations.\n4\nExperiments\n4.1\nExperience replay simulations\nWe first conduct simple numerical simulations of experience replay to illustrate the benefits of our\nRR-based sampling methods, RR-C and RR-M. In a 100-timestep simulation, an agent sequentially\nobserves transitions Tt indexed by timesteps t = 0, . . . , 99. At timestep t, the agent stores transition\nTt in a replay buffer of capacity 20. If the buffer contains at least 10 transitions, the agent samples\na minibatch of size 4 from the buffer. We record the indices of sampled transitions and visualize\nthe distribution of their sample counts for each transition across 1000 simulations using different\nrandom seeds.\nThe left side of Figure 3 shows the distributions of sample counts in uniform experience replay\nsimulations. When we compare with-replacement sampling and without-replacement sampling, we\n5\n0\n20\n40\n60\n80\n100\ntransition index\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nsample count\nuniform experience replay\nWR\nWOR\nRR-C\n0\n20\n40\n60\n80\n100\ntransition index\n0\n2\n4\n6\n8\n10\n12\nprioritized experience replay\nWR\nWOR\nRR-M\nFigure 3: Distributions of sample counts in experience replay simulations. We ran 100-timestep\nsimulations with different random seeds for each configuration. For each transition, we visualize\nhow many times it is sampled during a simulation. Solid lines represent mean sample counts, thick\nshaded areas represent mean±stdev, and light shaded areas represent minimum to maximum over\n1000 simulations. WR: with-replacement sampling, WOR: without-replacement sampling, RR-C:\nRR with a circular buffer, RR-M: RR by masking.\nobserve that without-replacement sampling exhibits slightly lower-variance sample counts. How-\never, the variance is still high; occasionally, some transitions are sampled more than 10 times while\nothers may not be sampled at all. RR-C further reduces the variance of sample counts without\nnoticeable bias; each transition is sampled at most 6 times.\nSimilarly, the right side of Figure 3 shows the distributions of sample counts in prioritized experience\nreplay simulations. We simulate non-uniform and dynamic priorities; the priority of each transition\nis set to pt = (t mod 25) + 5 when added to the buffer and decays by a factor of 0.8 every time it\nis sampled, where these values are chosen arbitrarily for illustration. Again, we observe that RR-M\nrealizes lower-variance sample counts without significant bias. We include additional simulation\nresults with different set of parameters in Section D.1 in the supplementary materials.\nFrom these simulations, we observe that both our RR-based sampling methods can reduce the vari-\nance of sample counts in experience replay, which is expected to enable lower-variance gradient\nestimation and improve learning stability. We verify the effectiveness in deep RL settings in the\nfollowing experiments.\n4.2\nDeep RL with uniform experience replay\nTo evaluate the effectiveness of experience replay with RR in deep RL settings, we conduct experi-\nments on the Arcade Learning Environments (Bellemare et al., 2013). To save computational costs,\nwe use the Atari-10 subset, which is chosen to be small but representative (Aitchison et al., 2023).\nWe run the DQN (Mnih et al., 2015) and C51 (Bellemare et al., 2017) algorithms and compare\nthe performance of different sampling methods. We base our implementation on the CleanRL li-\nbrary (Huang et al., 2022) and keep the hyperparameters the same as the default values. We run\neach experiment with 10 random seeds and report the final performance, measured by the mean\nscore over the last 100 episodes. Learning curves are shown in the supplementary materials.\nTable 1 shows the final performance of C51 and DQN with different sampling methods. Both C51\nand DQN with RR-C outperform their with-replacement sampling counterparts in the majority of\ngames, which demonstrates the effectiveness of RR in experience replay. The effect size of RR is\nmodest in most games, but it is consistent across different games and algorithms.\nIs within-minibatch without-replacement sampling sufficient?\nWe also evaluate the perfor-\nmance of C51 with within-minibatch without-replacement sampling and observe that it is no better\nthan with-replacement sampling, which suggests that the benefits of RR come from the buffer-level\n6\nTable 1: Final performance of C51 and DQN with with-replacement sampling (WR) and RR-C.\nFinal performance is measured by the mean episodic returns of the last 100 training episodes and\nreported as the mean over 10 random seeds with higher values bold. The p-values are computed by\nWelch’s t-test. *: p < 0.05, **: p < 0.01.\nC51\nDQN\nWR\nRR-C\np-value\nWR\nRR-C\np-value\nAmidar\n347.63\n420.61\n1.1e-02*\n319.04\n341.44\n3.2e-01\nBowling\n39.45\n33.90\n2.5e-01\n42.17\n43.97\n6.2e-01\nFrostbite\n3568.36\n4073.23\n3.9e-02*\n2351.80\n1897.50\n2.4e-01\nKungFuMaster\n21091.90\n21407.00\n7.7e-01\n3674.70\n4291.40\n8.3e-01\nRiverraid\n11984.34\n12776.65\n6.0e-02\n8346.39\n8513.25\n6.1e-01\nBattleZone\n22483.00\n23776.00\n1.9e-02*\n22651.00\n24546.00\n3.9e-03**\nDoubleDunk\n-15.04\n-16.42\n2.2e-02*\n-17.86\n-14.69\n2.1e-01\nNameThisGame\n8287.14\n8844.65\n1.9e-03**\n6064.81\n6919.35\n1.3e-04**\nPhoenix\n11920.93\n13749.39\n7.7e-03**\n9059.42\n10194.60\n6.8e-02\nQbert\n15685.83\n16352.80\n6.4e-02\n13191.77\n13859.60\n1.1e-01\nwithout-replacement sampling. This is not surprising because sampling with replacement in this\nsetting rarely samples duplicate transitions in the same minibatch; the capacity of the buffer is set\nto 1 million, which is much larger than the minibatch size of 32. The final performance is shown in\nFigure 7 in the supplementary materials.\n4.3\nDeep RL with prioritized experience replay\nTo compare sampling methods for prioritized experience replay, we run the double DQN algo-\nrithm (Van Hasselt et al., 2016) with loss-adjusted experience replay (DDQN+LAP) (Fujimoto et al.,\n2020) and compare the performance against with-replacement sampling. We use their official code\nbase and keep the hyperparameters3 the same as the default values except for the number of training\ntimesteps, which is set to 10 million steps across all experiments due to the limitation of computa-\ntional resources. As in the uniform experience replay experiments, we run each experiment with 10\nrandom seeds and report the final performance which is measured by the mean score over the last\n100 episodes.\nTable 2 shows the final performance of DDQN+LAP with different sampling methods. The left side\nof the table compares with-replacement sampling and RR-M. Although the differences are less pro-\nnounced compared to uniform experience replay experiments, RR-M outperforms with-replacement\nsampling 8 out of 10 games, which suggests that RR can also be beneficial in prioritized experience\nreplay.\nIs stratified sampling sufficient?\nStratified sampling is sometimes used in prioritized experience\nreplay, querying a different segment of the sum tree for each transition in a minibatch. Since pri-\norities are stored in a sum tree in chronological order, stratified sampling ensures diversity in age\nof transitions in a minibatch, which could have a different benefit for learning. The right side of\nTable 2 compares stratified sampling and RR-M+ST, where stratified sampling is used instead of\nwithin-minibatch without-replacement sampling. We observe that RR-M+ST outperforms strati-\nfied sampling in 8 out of 10 games, which suggests that RR can be beneficial even when stratified\nsampling is used.\n3Notably, we set action repeat to 0.25 for DDQN+LAP (following the original paper) and to 0 for C51 and DQN.\n7\nTable 2: Final performance of DDQN+LAP with with-replacement sampling (WR), RR-M, stratified\nsampling (ST), and RR-M+ST (ST applied to RR-M). Format follows Table 1.\nDDQN+LAP\nWR\nRR-M\np-value\nST\nRR-M+ST\np-value\nAmidar\n196.94\n195.35\n9.0e-01\n181.64\n206.07\n2.5e-02*\nBowling\n28.85\n29.87\n6.3e-01\n34.86\n27.57\n3.8e-02*\nFrostbite\n1602.00\n1761.34\n7.3e-02\n1672.56\n1684.80\n9.0e-01\nKungFuMaster\n16628.90\n17580.60\n1.4e-01\n16859.30\n17337.90\n4.7e-01\nRiverraid\n7609.97\n7756.11\n1.6e-01\n7474.01\n7810.06\n2.0e-02*\nBattleZone\n22897.00\n20813.00\n1.8e-01\n22099.00\n22584.00\n5.4e-01\nDoubleDunk\n-17.49\n-17.47\n9.5e-01\n-17.34\n-17.18\n8.2e-01\nNameThisGame\n2589.54\n2805.85\n4.9e-03**\n2635.89\n2759.89\n1.0e-01\nPhoenix\n4180.98\n4342.48\n1.3e-01\n4214.82\n4399.41\n1.3e-01\nQbert\n4128.27\n4266.98\n6.1e-01\n4109.27\n4017.55\n6.6e-01\n5\nRelated work\nExperience replay is a well-established technique in off-policy RL, originally proposed by Lin\n(1992) and later popularized in deep Q-networks by Mnih et al. (2015). This mechanism has be-\ncome a cornerstone of many deep RL algorithms, including DQN and its variants (Van Hasselt et al.,\n2016; Bellemare et al., 2017). Prioritized experience replay (Schaul et al., 2016) biases sampling\ntowards important transitions (e.g. those with high temporal-difference error), which can accelerate\nlearning by replaying valuable experiences more frequently, followed by extensions that differ in\nhow priorities are computed (Fujimoto et al., 2020; Oh et al., 2022; Saglam et al., 2023; Sujit et al.,\n2023; Yenicesu et al., 2024). Our proposed sampling methods can be used as a drop-in replace-\nment to with-replacement sampling for both uniform and prioritized experience replay. They can be\ncombined with extensions that are orthogonal to how transitions are sampled such as hindsight expe-\nrience replay (Andrychowicz et al., 2017), which relabels goals for multi-goal RL tasks to improve\nsample efficiency.\nClosely related to our work are combined experience replay (CER) (Zhang & Sutton, 2017) and\ncorrected uniform experience replay (CUER) (Yenicesu et al., 2024). CER ensures the most recent\ntransitions are always included in a minibatch, aiming to mitigate the negative effects of a large\nreplay buffer. CUER is a variant of prioritized experience replay that prioritizes transitions that have\nbeen sampled less often by reducing the priority every time a transition is sampled. Similarly to\nour work, both CER and CUER could reduce the variance in the number of times each transition\nis sampled, as CER ensures that each transition is sampled at least once and CUER reduces the\npriority of transitions that have been sampled more often than others. However, our RR-based sam-\npling methods aim to eliminate such unnecessary variance by either enforcing that each transition\nis sampled exactly once per epoch (RR-C) or detecting and excluding the oversampled transitions\n(RR-M). While CER and CUER favor recent transitions by design, our RR-based sampling methods\ndo not have such a bias and can be applied to both uniform and prioritized experience replay.\n6\nConclusion\nIn this work, we introduced two sampling methods that extend random reshuffling to uniform and\nprioritized experience replay, respectively. Our empirical results on Atari benchmarks demonstrate\nthat introducing RR to experience replay yields modest but consistent improvements across different\nRL algorithms. Importantly, our RR-based sampling methods are straightforward to implement and\ncan safely replace with-replacement sampling in existing RL code bases, making them practical for\nRL practitioners.\n8\nAcknowledgments\nWe would like to thank Pieter Abbeel for helpful discussions and feedback on earlier results of this\nwork, and Avinash Ummadisingu for useful suggestions on the manuscript.\nReferences\nMatthew Aitchison, Penny Sweetser, and Marcus Hutter. Atari-5: Distilling the arcade learning\nenvironment down to five games. In ICML, 2023.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-\nplay. NeurIPS, 2017.\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,\njun 2013.\nMarc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement\nlearning. In ICML, 2017.\nPierfrancesco Beneventano.\nOn the trajectories of SGD without replacement.\narXiv preprint\narXiv:2312.16143, 2023.\nLéon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. Unpub-\nlished open problem offered to the attendance of the SLDS 2009 conference, 2009.\nLéon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, Reloaded,\npp. 421–436. Springer, 2012.\nAlbert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang,\nGianni De Fabritiis, and Vincent Moens. TorchRL: A data-driven decision-making library for\nPyTorch. arXiv preprint arXiv:2306.00577, 2023.\nPablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare.\nDopamine: A Research Framework for Deep Reinforcement Learning. 2018.\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,\nJohn Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI baselines. https:\n//github.com/openai/baselines, 2017.\nScott Fujimoto, David Meger, and Doina Precup. An equivalence between loss functions and non-\nuniform sampling in experience replay. In NeurIPS, 2020.\nYasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. ChainerRL: A deep\nreinforcement learning library. Journal of Machine Learning Research, 22(77):1–14, 2021.\nMert Gürbüzbalaban, Asuman Ozdaglar, and Pablo A. Parrilo.\nWhy random reshuffling beats\nstochastic gradient descent. Mathematical Programming, 186(1-2):49–84, 2021.\nJeff Z. HaoChen and Suvrit Sra. Random shuffling beats SGD after finite epochs. In ICML, 2019.\nShengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Ki-\nnal Mehta, and João G.M. Araújo. CleanRL: High-quality single-file implementations of deep\nreinforcement learning algorithms. Journal of Machine Learning Research, 23(274):1–18, 2022.\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph E.\nGonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement\nlearning. In ICML, 2018.\n9\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine Learning, 8(3-4):293–321, 1992.\nKonstantin Mishchenko, Ahmed Khaled, and Peter Richtárik. Random reshuffling: Simple analysis\nwith vast improvements. NeurIPS, 2020.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, 2015.\nDheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement: Sharper rates\nfor general smooth convex functions. In ICML, 2019.\nYoungmin Oh, Jinwoo Shin, Eunho Yang, and Sung Ju Hwang. Model-augmented prioritized expe-\nrience replay. In ICLR, 2022.\nRyosuke Okuta, Yuya Unno, Daisuke Nishino, Shohei Hido, and Crissman Loomis.\nCuPy: A\nNumPy-compatible library for NVIDIA GPU calculations. In NeurIPS Workshop on Machine\nLearning Systems, 2017.\nJohn Quan and Georg Ostrovski. DQN Zoo: Reference implementations of DQN-based agents,\n2020. URL http://github.com/deepmind/dqn_zoo.\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dor-\nmann. Stable-Baselines3: Reliable reinforcement learning implementations. Journal of Machine\nLearning Research, 22(268):1–8, 2021.\nShashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD\nwithout replacement. In ICML, 2020.\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-\ncal statistics, pp. 400–407, 1951.\nBaturay Saglam, Furkan B Mutlu, Dogan C Cicek, and Suleyman S Kozat. Actor prioritized expe-\nrience replay. Journal of Artificial Intelligence Research, 78:639–672, 2023.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In\nICLR, 2016.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nShivakanth Sujit, Somjit Nath, Pedro Braga, and Samira Ebrahimi Kahou. Prioritizing samples in\nreinforcement learning with reducible loss. NeurIPS, 2023.\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-\nlearning. In AAAI, 2016.\nJiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang\nSu, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. Journal\nof Machine Learning Research, 23(267):1–6, 2022.\nArda Sarp Yenicesu, Furkan B Mutlu, Suleyman S Kozat, and Ozgur S Oguz. CUER: Corrected uni-\nform experience replay for off-policy continuous deep reinforcement learning algorithms. arXiv\npreprint arXiv:2406.09030, 2024.\nShangtong Zhang and Richard S. Sutton. A deeper look at experience replay: A study on parameters\nand batch updating. In NeurIPS Deep Reinforcement Learning Symposium, 2017.\n10\nSupplementary Materials\nA\nSampling methods of experience replay in popular RL code bases\nWe summarize the sampling methods of experience replay in popular RL code bases in Table 3.\nTable 3: Sampling methods of experience replay in popular RL code bases. WR: with-replacement\nsampling, WOR: (within-minibatch) without-replacement sampling, ST: stratified sampling. Code\nbases are sorted in alphabetical order. URLs were checked around February 23, 2025.\nCode base\nUniform ER\nPrioritized ER\nClearnRL (Huang et al., 2022)\nWRa\nN/A\nDopamine (Castro et al., 2018)\nWRb\nSTc\nDQN 3.0 (Mnih et al., 2015)\nWRd\nN/A\nDQN Zoo (Quan & Ostrovski, 2020)\nWRe\nWRf\nOpenAI Baselines (Dhariwal et al., 2017)\nWRg\nSTh\nPFRL (Fujita et al., 2021)\nWORi\nWORj\nRay RLlib (Liang et al., 2018)\nWRk\nWRl\nStable-Baselines3 (Raffin et al., 2021)\nWRm\nN/A\nTianshou (Weng et al., 2022)\nWRn\nWRo\nTorchRL (Bou et al., 2023)\nWRpq\nWRr\naCleanRL depends on Stable-Baselines3’s ReplayBuffer\nbhttps://github.com/google/dopamine/blob/bec5f4e108b0572e58fc1af73136e978237c84\n63/dopamine/tf/replay_memory/circular_replay_buffer.py#L552\nchttps://github.com/google/dopamine/blob/bec5f4e108b0572e58fc1af73136e978237c84\n63/dopamine/tf/replay_memory/prioritized_replay_buffer.py#L160\ndhttps://github.com/google-deepmind/dqn/blob/9d9b1d13a2b491d6ebd4d046740c511c662\nbbe0f/dqn/TransitionTable.lua#L124\nehttps://github.com/google-deepmind/dqn_zoo/blob/45061f4bbbcfa87d11bbba3cfc2305a\n650a41c26/dqn_zoo/replay.py#L158\nfhttps://github.com/google-deepmind/dqn_zoo/blob/45061f4bbbcfa87d11bbba3cfc2305a\n650a41c26/dqn_zoo/replay.py#L559\nghttps://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143\n998/baselines/deepq/replay_buffer.py#L67\nhhttps://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143\n998/baselines/deepq/replay_buffer.py#L112\nihttps://github.com/pfnet/pfrl/blob/c8cb3328899509be9ca68ca9a6cd2bcfce95725c/pf\nrl/collections/random_access_queue.py#L101\njhttps://github.com/pfnet/pfrl/blob/c8cb3328899509be9ca68ca9a6cd2bcfce95725c/pf\nrl/collections/prioritized.py#L303\nkhttps://github.com/ray-project/ray/blob/db419295a60657fc2be7e0e8053cea9ee8b82f\nbd/rllib/utils/replay_buffers/replay_buffer.py#L314\nlhttps://github.com/ray-project/ray/blob/db419295a60657fc2be7e0e8053cea9ee8b82f\nbd/rllib/utils/replay_buffers/prioritized_replay_buffer.py#L88\nmhttps://github.com/DLR-RM/stable-baselines3/blob/fa21bce04ee625c67f6ea2a7678bf\n46c39cd226c/stable_baselines3/common/buffers.py#L114\nnhttps://github.com/thu-ml/tianshou/blob/c4ae7cd924cc76b3c3f456008ce4df80d3d8c7\n46/tianshou/data/buffer/base.py#L502\nohttps://github.com/thu-ml/tianshou/blob/0a79016cf6f6c7f44aa5d6a1af36a96247bb1e\n78/tianshou/data/buffer/prio.py#L65\nphttps://github.com/pytorch/rl/blob/21c4d87c7ba5592a731757411468d080b7173b5f/to\nrchrl/data/replay_buffers/storages.py#L159\nqAdditionally, TorchRL also supports SamplerWithoutReplacement sampler, which resembles our RR-C. How-\never, it resets its list of shuffled indices every time the storage is expanded, thus making it unsuitable for a replay buffer of a\ngrowing size, which is typical in off-policy online RL training. https://github.com/pytorch/rl/blob/21c4d\n87c7ba5592a731757411468d080b7173b5f/torchrl/data/replay_buffers/samplers.py#L162\nrhttps://github.com/pytorch/rl/blob/21c4d87c7ba5592a731757411468d080b7173b5f/to\nrchrl/data/replay_buffers/samplers.py#L497\n11\nB\nExample trajectories of RR-M\nWe provide example trajectories of RR-M to illustrate its behavior. We use the simple example from\nSection 3.3, where three transitions with priorities [1, 0.5, 2] are in the buffer, and we sequentially\nsample 7 transitions. Two trajectories simulated using different random seeds are shown in Table 4.\nBoth trajectories sample transitions in different orders, but eventually they sample the first transition\ntwice, the second once, and the third four times, which is the expected behavior of RR-M.\nTable 4: Example trajectories of RR-M, where there are three transitions with priorities [1, 0.5, 2]\nin the buffer and we sequentially sample 7 transitions. Two trajectories simulated using different\nrandom seeds are shown.\niteration\n0\n1\n2\n3\n4\n5\n6\nactual counts\n0\n0\n0\n0\n0\n1\n1\n0\n1\n1\n1\n1\n1\n1\n2\n1\n1\n3\n2\n1\n3\nexpected counts\n0.00\n0.00\n0.00\n0.29\n0.14\n0.57\n0.57\n0.29\n1.14\n0.86\n0.43\n1.71\n1.14\n0.57\n2.29\n1.43\n0.71\n2.86\n1.71\n0.86\n3.43\nmask\nF\nF\nF\nF\nF\nT\nT\nF\nF\nT\nT\nF\nF\nT\nF\nF\nT\nT\nT\nT\nF\nmasked probabilities\n0.29\n0.14\n0.57\n0.67\n0.33\n0.00\n0.00\n0.20\n0.80\n0.00\n0.00\n1.00\n0.33\n0.00\n0.67\n1.00\n0.00\n0.00\n0.00\n0.00\n1.00\nsampled transition\n2\n0\n1\n2\n2\n0\n2\nupdated actual counts\n0\n0\n1\n1\n0\n1\n1\n1\n1\n1\n1\n2\n1\n1\n3\n2\n1\n3\n2\n1\n4\nupdated expected counts\n0.29\n0.14\n0.57\n0.57\n0.29\n1.14\n0.86\n0.43\n1.71\n1.14\n0.57\n2.29\n1.43\n0.71\n2.86\n1.71\n0.86\n3.43\n2.00\n1.00\n4.00\niteration\n0\n1\n2\n3\n4\n5\n6\nactual counts\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n2\n1\n3\nexpected counts\n0.00\n0.00\n0.00\n0.29\n0.14\n0.57\n0.57\n0.29\n1.14\n0.86\n0.43\n1.71\n1.14\n0.57\n2.29\n1.43\n0.71\n2.86\n1.71\n0.86\n3.43\nmask\nF\nF\nF\nF\nF\nT\nF\nT\nF\nT\nT\nF\nF\nT\nF\nT\nT\nF\nT\nT\nF\nmasked probabilities\n0.29\n0.14\n0.57\n0.67\n0.33\n0.00\n0.33\n0.00\n0.67\n0.00\n0.00\n1.00\n0.33\n0.00\n0.67\n0.00\n0.00\n1.00\n0.00\n0.00\n1.00\nsampled transition\n2\n1\n0\n2\n0\n2\n2\nupdated actual counts\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n2\n1\n3\n2\n1\n4\nupdated expected counts\n0.29\n0.14\n0.57\n0.57\n0.29\n1.14\n0.86\n0.43\n1.71\n1.14\n0.57\n2.29\n1.43\n0.71\n2.86\n1.71\n0.86\n3.43\n2.00\n1.00\n4.00\nC\nOn the computational efficiency of RR-M\nAs we discussed in the main text, RR-M requires O(n) time every time it samples a minibatch,\nwhere n is the number of transitions in the buffer. We provide a more detailed explanation of the\ncomputational cost of RR-M and how it can be implemented efficiently.\nC.1\nComputational cost of RR-M\nRR-M tracks actual and expected sample counts, updating them every time a minibatch is sampled\nActual counts are incremented only for sampled transitions, requiring O(b) time, where b is the\nminibatch size. In contrast, expected counts are updated for all transitions in the buffer, so it takes\nO(n) time. Identifying oversampled transitions involves comparing the actual and expected counts,\nwhich also requires O(n) time.\nThere could be at most n oversampled transitions, and each priority update takes O(log n) time in\na sum tree. Thus, naively masking their priorities with a single sum tree would take O(n log n)\ntime. This can be avoided by keeping two sum trees, one for the original priorities and the other\nfor the masked priorities, as well as a binary mask that indicates whether each transition is currently\noversampled. Masked priorities need updating only for the transitions whose oversampled status\nhas changed. There are at most b transitions that have switched to be oversampled, and there are on\naverage the same number of transitions that have switched in the opposite direction. Therefore, the\ntime complexity of updating masked priorities can be O(b log n).\nIn summary, provided the minibatch size is significantly smaller than the buffer size, the time com-\nplexity of RR-M is dominated by O(n) operations.\nC.2\nEfficient implementation of RR-M\nAlthough updating expected counts and the identification of oversampled transitions each require\nO(n) time, both tasks can be efficiently parallelized on GPUs. We implement two sum trees, actual\n12\nand expected sample counts, and a binary mask as GPU arrays using CuPy (Okuta et al., 2017), so\nwe can update them efficiently. Our implementation is included in the supplementary materials.\nAlthough we have not explored this in our experiments, further acceleration could be achieved by\nintroducing some approximation. For example, we could update the expected counts and identify\nthe oversampled transitions only after every k minibatches, where k is a hyperparameter, assuming\nthat the priorities of the transitions will not change significantly within k minibatches. This approach\nwould reduce computational costs by a factor of k.\nD\nExperimental details\nD.1\nAdditional simulation results\nWe provide additional results of experience replay simulations with different parameters in Figure 4,\nFigure 5, and Figure 6.\n0\n20\n40\n60\n80\n100\ntransition index\n0\n5\n10\n15\n20\nsample count\nuniform experience replay\nWR\nWOR\nRR-C\n0\n20\n40\n60\n80\n100\ntransition index\n0\n5\n10\n15\nprioritized experience replay\nWR\nWOR\nRR-M\nFigure 4: Distributions of sample counts in experience replay simulations with a different set of\nparameters from Figure 3: the minibatch size is 8. Format follows Figure 3.\n0\n200\n400\n600\n800\n1000\ntransition index\n0\n5\n10\n15\nsample count\nuniform experience replay\nWR\nWOR\nRR-C\n0\n200\n400\n600\n800\n1000\ntransition index\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nprioritized experience replay\nWR\nWOR\nRR-M\nFigure 5: Distributions of sample counts in experience replay simulations with a different set of\nparameters from Figure 3: the total timesteps is 1000, the capacity of the buffer size is 200, the size\nat which replay starts is 100, and pt = (t mod 250) + 50. Format follows Figure 3.\n13\n0\n200\n400\n600\n800\n1000\ntransition index\n0\n5\n10\n15\n20\n25\nsample count\nuniform experience replay\nWR\nWOR\nRR-C\n0\n200\n400\n600\n800\n1000\ntransition index\n0\n5\n10\n15\nprioritized experience replay\nWR\nWOR\nRR-M\nFigure 6: Distributions of sample counts in experience replay simulations with a different set of\nparameters from Figure 3: the total timesteps is 1000, the capacity of the buffer size is 200, the size\nat which replay starts is 100, the minibatch size is 8, and pt = (t mod 250) + 50. Format follows\nFigure 3.\nD.2\nExperimental setup\nFor deep RL experiments with uniform experience replay, we adapt CleanRL’s JAX-based imple-\nmentations of C51 and DQN (Huang et al., 2022):\n• C51: https://github.com/vwxyzjn/cleanrl/blob/1ed80620842b4cdeb1ed\nc07e12825dff18091da9/cleanrl/c51_atari_jax.py\n• DQN: https://github.com/vwxyzjn/cleanrl/blob/1ed80620842b4cdeb1ed\nc07e12825dff18091da9/cleanrl/dqn_atari_jax.py\nFor deep RL experiments with prioritized experience replay, we adapt the official implementation\nof DDQN+LAP (Fujimoto et al., 2020): https://github.com/sfujim/LAP-PAL/blob/\na649b1b977e979ed0e49d67335d3c79b555e9cfb/discrete/main.py.\nWe use the default hyperparameters of the original code unless otherwise noted. Our code for the\ndeep RL experiments is available at http://github.com/pfnet-research/errr.\nD.3\nFinal performance in details\nWe provide the final performance of C51, DQN, and DDQN+LAP with different sampling methods\nin the same way as Table 1 except that we report the mean ± standard deviations (ddof=1) in Table 5,\nTable 6, Table 7, Table 8, and Table 9.\nTable 5: Final performance of C51 with with-replacement sampling (WR) and RR-C. Format follows\nTable 1. Mean ± standard deviation (ddof=1) is reported.\nC51 WR\nC51 RR-C\np-value\nAmidar\n347.63±61.80\n420.61±52.97\n1.1e-02*\nBowling\n39.45±8.02\n33.90±12.23\n2.5e-01\nFrostbite\n3568.36±225.49\n4073.23±645.30\n3.9e-02*\nKungFuMaster\n21091.90±2181.97\n21407.00±2471.17\n7.7e-01\nRiverraid\n11984.34±910.74\n12776.65±852.50\n6.0e-02\nBattleZone\n22483.00±1021.12\n23776.00±1205.15\n1.9e-02*\nDoubleDunk\n-15.04±0.99\n-16.42±1.43\n2.2e-02*\nNameThisGame\n8287.14±358.54\n8844.65±325.02\n1.9e-03**\nPhoenix\n11920.93±1306.10\n13749.39±1416.65\n7.7e-03**\nQbert\n15685.83±742.83\n16352.80±771.19\n6.4e-02\n14\nTable 6: Final performance of DQN with with-replacement sampling (WR) and RR-C. Format fol-\nlows Table 1. Mean ± standard deviation (ddof=1) is reported.\nDQN WR\nDQN RR-C\np-value\nAmidar\n319.04±29.06\n341.44±61.32\n3.2e-01\nBowling\n42.17±6.48\n43.97±9.32\n6.2e-01\nFrostbite\n2351.80±794.45\n1897.50±882.38\n2.4e-01\nKungFuMaster\n3674.70±6043.96\n4291.40±6823.80\n8.3e-01\nRiverraid\n8346.39±655.53\n8513.25±783.58\n6.1e-01\nBattleZone\n22651.00±1178.15\n24546.00±1371.04\n3.9e-03**\nDoubleDunk\n-17.86±5.27\n-14.69±5.71\n2.1e-01\nNameThisGame\n6064.81±448.55\n6919.35±287.76\n1.3e-04**\nPhoenix\n9059.42±1617.43\n10194.60±805.68\n6.8e-02\nQbert\n13191.77±1076.35\n13859.60±628.38\n1.1e-01\nTable 7: Final performance of C51 with with-replacement sampling (WR) and within-minibatch\nwithout-replacement sampling (WOR). Format follows Table 1.\nMean ± standard deviation\n(ddof=1) is reported.\nC51 WR\nC51 WOR\np-value\nAmidar\n347.63±61.80\n342.91±27.90\n8.3e-01\nBowling\n39.45±8.02\n36.58±6.54\n3.9e-01\nFrostbite\n3568.36±225.49\n3487.60±198.76\n4.1e-01\nKungFuMaster\n21091.90±2181.97\n19852.80±2957.34\n3.0e-01\nRiverraid\n11984.34±910.74\n12325.34±468.77\n3.1e-01\nBattleZone\n22483.00±1021.12\n22984.00±1031.61\n2.9e-01\nDoubleDunk\n-15.04±0.99\n-16.52±0.91\n2.6e-03**\nNameThisGame\n8287.14±358.54\n8466.74±290.86\n2.4e-01\nPhoenix\n11920.93±1306.10\n11587.56±1311.67\n5.8e-01\nQbert\n15685.83±742.83\n15368.27±549.21\n2.9e-01\nTable 8: Final performance of DDQN+LAP with with-replacement sampling (WR) and RR-M.\nFormat follows Table 1. Mean ± standard deviation (ddof=1) is reported.\nDDQN+LAP WR\nDDQN+LAP RR-M\np-value\nAmidar\n196.94±24.30\n195.35±30.06\n9.0e-01\nBowling\n28.85±5.60\n29.87±3.39\n6.3e-01\nFrostbite\n1602.00±204.60\n1761.34±167.51\n7.3e-02\nKungFuMaster\n16628.90±1500.14\n17580.60±1274.08\n1.4e-01\nRiverraid\n7609.97±283.06\n7756.11±120.62\n1.6e-01\nBattleZone\n22897.00±1570.29\n20813.00±4352.74\n1.8e-01\nDoubleDunk\n-17.49±1.02\n-17.47±0.78\n9.5e-01\nNameThisGame\n2589.54±166.60\n2805.85±130.60\n4.9e-03**\nPhoenix\n4180.98±167.30\n4342.48±273.12\n1.3e-01\nQbert\n4128.27±476.63\n4266.98±685.46\n6.1e-01\n15\nTable 9: Final performance of DDQN+LAP with stratified sampling (ST) and RR-M+ST. Format\nfollows Table 1. Mean ± standard deviation (ddof=1) is reported.\nDDQN+LAP ST\nDDQN+LAP RR-M ST\np-value\nAmidar\n181.64±16.19\n206.07±26.32\n2.5e-02*\nBowling\n34.86±8.33\n27.57±5.84\n3.8e-02*\nFrostbite\n1672.56±214.35\n1684.80±211.25\n9.0e-01\nKungFuMaster\n16859.30±1379.02\n17337.90±1515.28\n4.7e-01\nRiverraid\n7474.01±260.02\n7810.06±321.56\n2.0e-02*\nBattleZone\n22099.00±1019.70\n22584.00±2232.24\n5.4e-01\nDoubleDunk\n-17.34±1.58\n-17.18±1.52\n8.2e-01\nNameThisGame\n2635.89±172.90\n2759.89±146.11\n1.0e-01\nPhoenix\n4214.82±307.08\n4399.41±206.47\n1.3e-01\nQbert\n4109.27±412.74\n4017.55±496.86\n6.6e-01\nD.4\nLearning curves\nWe provide the learning curves of C51, DQN, and DDQN+LAP with different sampling methods in\nFigure 7, Figure 8, Figure 9, Figure 10, and Figure 11.\n0\n1\n2\n3\n4\n5\n1e2\nAmidar\nC51 WR\nC51 RR-C\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n1e1\nBowling\n0\n1\n2\n3\n4\n5\n1e3\nFrostbite\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n1e4\nKungFuMaster\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1e4\nRiverraid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.5\n1.0\n1.5\n2.0\n2.5\n1e4\nBattleZone\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2.2\n2.0\n1.8\n1.6\n1.4\n1e1\nDoubleDunk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2\n3\n4\n5\n6\n7\n8\n9\n1e3\nNameThisGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1e4\nPhoenix\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n1e4\nQbert\ntimesteps\nepisodic return\nFigure 7: Learning curves of C51 with with-replacement sampling (WR) and RR-C. The 500-\nepisode rolling mean of episodic returns is plotted for each of 10 random seeds.\n0\n1\n2\n3\n4\n1e2\nAmidar\nDQN WR\nDQN RR-C\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n1e1\nBowling\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n1e3\nFrostbite\n0.0\n0.5\n1.0\n1.5\n2.0\n1e4\nKungFuMaster\n2\n4\n6\n8\n1e3\nRiverraid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.5\n1.0\n1.5\n2.0\n2.5\n1e4\nBattleZone\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2.4\n2.2\n2.0\n1.8\n1.6\n1.4\n1.2\n1.0\n1e1\nDoubleDunk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n4\n5\n6\n7\n1e3\nNameThisGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1e4\nPhoenix\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1e4\nQbert\ntimesteps\nepisodic return\nFigure 8: Learning curves of DQN with with-replacement sampling (WR) and RR-C. The 500-\nepisode rolling mean of episodic returns is plotted for each of 10 random seeds.\n16\n0\n1\n2\n3\n4\n5\n1e2\nAmidar\nC51 WR\nC51 WOR\n2.5\n3.0\n3.5\n4.0\n4.5\n1e1\nBowling\n0\n1\n2\n3\n1e3\nFrostbite\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n1e4\nKungFuMaster\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1e4\nRiverraid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.5\n1.0\n1.5\n2.0\n2.5\n1e4\nBattleZone\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2.2\n2.0\n1.8\n1.6\n1.4\n1e1\nDoubleDunk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2\n3\n4\n5\n6\n7\n8\n9\n1e3\nNameThisGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1e4\nPhoenix\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75 1e4\nQbert\ntimesteps\nepisodic return\nFigure 9: Learning curves of C51 with with-replacement sampling (WR) and within-minibatch\nwithout-replacement sampling (WOR). The 500-episode rolling mean of episodic returns is plotted\nfor each of 10 random seeds.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n1e2\nAmidar\nDDQN+LAP WR\nDDQN+LAP RR-M\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n1e1\nBowling\n0.0\n0.5\n1.0\n1.5\n2.0\n1e3\nFrostbite\n0.0\n0.5\n1.0\n1.5\n2.0\n1e4\nKungFuMaster\n2\n3\n4\n5\n6\n7\n8\n1e3\nRiverraid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.5\n1.0\n1.5\n2.0\n2.5\n1e4\nBattleZone\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2.0\n1.9\n1.8\n1.7\n1e1\nDoubleDunk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2.5\n3.0\n3.5\n4.0\n1e3\nNameThisGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n1\n2\n3\n4\n1e3\nPhoenix\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0\n1\n2\n3\n4\n5\n1e3\nQbert\ntimesteps\nepisodic return\nFigure 10: Learning curves of DDQN+LAP with with-replacement sampling (WR) and RR-M. The\n500-episode rolling mean of episodic returns is plotted for each of 10 random seeds.\n0.0\n0.5\n1.0\n1.5\n2.0\n1e2\nAmidar\nDDQN+LAP ST\nDDQN+LAP RR-M ST\n2.0\n2.5\n3.0\n3.5\n4.0\n1e1\nBowling\n0.0\n0.5\n1.0\n1.5\n2.0\n1e3\nFrostbite\n0.5\n1.0\n1.5\n2.0\n1e4\nKungFuMaster\n2\n3\n4\n5\n6\n7\n8\n1e3\nRiverraid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0.5\n1.0\n1.5\n2.0\n2.5\n1e4\nBattleZone\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1e1\nDoubleDunk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\n1e3\nNameThisGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n1\n2\n3\n4\n1e3\nPhoenix\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n0\n1\n2\n3\n4\n5\n1e3\nQbert\ntimesteps\nepisodic return\nFigure 11: Learning curves of DDQN+LAP with stratified sampling (ST) and RR-M+ST. The 500-\nepisode rolling mean of episodic returns is plotted for each of 10 random seeds.\n17\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2025-03-04",
  "updated": "2025-03-04"
}