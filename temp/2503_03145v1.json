{
  "id": "http://arxiv.org/abs/2503.03145v1",
  "title": "Causality-Based Reinforcement Learning Method for Multi-Stage Robotic Tasks",
  "authors": [
    "Jiechao Deng",
    "Ning Tan"
  ],
  "abstract": "Deep reinforcement learning has made significant strides in various robotic\ntasks. However, employing deep reinforcement learning methods to tackle\nmulti-stage tasks still a challenge. Reinforcement learning algorithms often\nencounter issues such as redundant exploration, getting stuck in dead ends, and\nprogress reversal in multi-stage tasks. To address this, we propose a method\nthat integrates causal relationships with reinforcement learning for\nmulti-stage tasks. Our approach enables robots to automatically discover the\ncausal relationships between their actions and the rewards of the tasks and\nconstructs the action space using only causal actions, thereby reducing\nredundant exploration and progress reversal. By integrating correct causal\nrelationships using the causal policy gradient method into the learning\nprocess, our approach can enhance the performance of reinforcement learning\nalgorithms in multi-stage robotic tasks.",
  "text": "1\nCausality-Based Reinforcement Learning Method\nfor Multi-Stage Robotic Tasks\nJiechao Deng, and Ning Tan*, Member, IEEE,\nAbstract—Deep reinforcement learning has made significant\nstrides in various robotic tasks. However, employing deep re-\ninforcement learning methods to tackle multi-stage tasks still\na challenge. Reinforcement learning algorithms often encounter\nissues such as redundant exploration, getting stuck in dead\nends, and progress reversal in multi-stage tasks. To address\nthis, we propose a method that integrates causal relationships\nwith reinforcement learning for multi-stage tasks. Our approach\nenables robots to automatically discover the causal relationships\nbetween their actions and the rewards of the tasks and constructs\nthe action space using only causal actions, thereby reducing\nredundant exploration and progress reversal. By integrating\ncorrect causal relationships using the causal policy gradient\nmethod into the learning process, our approach can enhance the\nperformance of reinforcement learning algorithms in multi-stage\nrobotic tasks.\nIndex Terms—reinforcement learning, action space, causality,\nmulti-stage tasks.\nI. INTRODUCTION\nD\nEEP reinforcement learning (RL) has achieved many\nresults in robot tasks in recent years, such as local\nobstacle avoidance [1], indoor navigation [2], door opening\n[3], object manipulation [4], and mobile manipulation [5].\nHowever, in robot related tasks, there are many tasks that need\nto be completed in stages. When faced with such multi-stage\ntasks, it is difficult to simply use a single agent for RL to solve\nproblems. In multi-stage tasks, agents need to learn both the\nimmediate results of their actions and how these results affect\nthe completion of the entire task, and these contents are often\nintertwined. When dealing with long-horizon tasks that require\nmultiple stages to complete, RL algorithms often explore into\ndead ends, and may even lead to a reversal of progress.[6].\nProgress reversal refers to the situation where a task is divided\ninto multiple stages, the actions of subsequent stage may\nalter the state achieved by the previous stage, leading to the\nincompleteness of the previous stage.\nAs shown in Fig. 1, the robot’s end effector has already\nmoved above the target point. The next step should be to\nrotate the end effector downward. However, if the end effector\nalso moves during the rotation, this will result in a reversal\nof progress, requiring the end effector to be moved back\nabove the target point. If the aforementioned robot has the\ncapability to both move and rotate its end effector, and if it\ndoes not perform the movement operation while rotating, it\nJ.C. Deng and N. Tan are with the School of Computer Science and\nEngineering, Sun Yat-sen University.\nThis work was supported in part by the National Natural Science Foundation\nof China under Grant 62173352, and in part by the Guangdong Basic and\nApplied Basic Research Foundation under Grant 2024B1515020104.\nFig. 1: During normal execution, the end effector should be\nadjusted to face downwards, but due to the movement of the\nrobotic arm, the position of the end effector deviated, causing\nthe entire task to regress from step 2 back to step 1, resulting\nin a progress reversal.\ncan significantly avoid the occurrence of progress reversal. In\nreal-life scenarios, humans, when executing tasks, instinctively\nselect actions that they know will influence the task’s comple-\ntion, based on their comprehension of how their actions affect\nthe surrounding environment. Upon observing the outcomes\nof these actions, they make fine-tuned adjustments to their\nsubsequent movements. That is, if one can obtain the causal\nrelationship between actions and the environment, it is possible\nto execute only the necessary actions to complete the task, and\nadjust oneself based on the feedback results corresponding to\nthe actions.\nIn RL, reward variables can be used as indicators of task\ncompletion. By identifying the causal relationship between\nactions and rewards, one can select the causal actions for\neach stage based on the rewards of that stage to construct the\naction space of the deep RL policy. This approach can reduce\nredundant exploration and the occurrence of progress reversal.\nFurthermore, causal policy gradient method [7] can utilize\nknown causal relationships to allow policy to learn based on\nthe rewards corresponding to actions, thereby reducing the\ngradient variance in the RL process. Therefore, if accurate\narXiv:2503.03145v1  [cs.RO]  5 Mar 2025\n2\ncausal relationships for each stage of multi-stage tasks can be\nobtained, the performance of RL in handling multi-stage tasks\ncan be enhanced by utilizing causal actions and causal policy\ngradients.\nBased on this, we propose a method that leverages causal\nrelationships and deep RL to handle multi-stage tasks. This\nmethod allows the robot to autonomously discover the causal\neffects of its actions on task completion and constructs the\naction space of the deep RL algorithm using only causal\nactions. By integrating causal policy gradients for learning,\nit can effectively handle multi-stage tasks. The contributions\nof this study can be summarized as follow:\n1) Approach multi-stage tasks from the perspective of\nbreaking down subtasks. By leveraging the robots’ abil-\nity to interact with the environment to establish causal\nrelationships for each subtask, thereby obtaining the\ncausal relationships between the robots’ actions and the\nenvironment at each stage.\n2) A RL method for multi-stage tasks was designed using\nthe obtained causal relationships.This method can reduce\nredundant exploration in RL by narrowing the action\nspace, decrease the instances of progress reversal in\nmulti-stage tasks, and enhance the performance of RL.\n3) Conduct experimental comparisons of the proposed\nmethod on a mobile manipulation task and a pure manip-\nulation task to verify the effectiveness of the proposed\napproach.\nII. RELATED WORK\nVarious approaches have been proposed for multi-stage\ntasks, some of these methods still employ a single agent for\nlearning multi-stage tasks. The direct physical consequences\nof multi-stage task behavior are entangled with the impact\nof these consequences on the overall goal, making robot task\nlearning difficult. In the process, RL wastes significant time\nexploring unproductive actions, such as spending a lot of\ntime grabbing air in tasks like stacking blocks. [6] proposed\nthe SPOT framework for more efficient RL of multi-stage\ntasks. [8] propose a novel Transformer-based model which\nenables a user to guide a robot arm through a 3D multi-step\nmanipulation task with natural language commands base on\nSPOT. To efficiently address sequential object manipulation\ntasks, [9] divide actions into different groups, assuming that\nsome actions are preconditions of others in multi-step tasks,\nand apply pixel-wise Q value-based critic networks to solve\nmulti-step sorting tasks.\nThe approach of decomposing tasks into multiple subtasks\nis inherent in hierarchical reinforcement learning (HRL), as\nits advantage lies in the ability to learn a series of simple\nand smaller subtasks, and then apply these subtasks to solve\nlarger and more complex problems. Using HRL, [10] enables\nthe agent to learn and perform subtasks based on given instruc-\ntions. If changing the state that the robot needs to achieve in\ncompleting the task is regarded as each subtask, [11], [12]\nutilize HRL, enabling the upper-level meta-controller to set\nsubtasks for the underlying policies to complete.\nThere are also many methods that split tasks into subtasks\nbut do not utilize HRL. With Hindsight Experience Replay\n(HER) [4], the agent can generate non-negative rewards by\ngoal relabeling strategy to alleviate the negative sparse reward\nproblem.\n[13] propose a novel self-guided continual RL framework,\nRelay-HER (RHER), utilizing HER and a Self-Guided Explo-\nration Strategy(SGES) to solve sequential object manipulation\ntasks efficiently. [14] also divides multi-stage tasks into mul-\ntiple subtasks for learning. However, it proposes a strategy of\nnot only training each subtask separately but also considering\nthe situation of the latter subtask during training. This allows\nagents of multiple tasks to learn to cooperate with each other.\n[15] introduce MRLM for non-prehensile manipulation of\nobjects. MRLM divides the task into multiple stages according\nto the switching of object poses and contact points. It then\nemploys an off-policy RL algorithm to train a point cloud\nmotion based manipulation network (P2ManNet) as the policy\nto complete each stage successively. [16] addresses multi-task\nRL guided by abstract sketches of high-level behavior.\nRL techniques based on causal modeling can leverage\nstructural causal knowledge to enhance the performance of\nRL algorithms. [17] employs the do-calcules [18] to formalise\nmodel-based RL as a causal inference problem, and present\na method that combining offline and online data in model-\nbased RL. [19] utilizes causal discovery algorithms to identify\ncausal relationships between actions and state variables. It\nthen constructs multi-layered policies based on these causal\nrelationships to facilitate HRL. [7] employs causality to ad-\ndress the mobile manipulation problem of robots. It use causal\ndiscovery algorithms to obtain the causal relationship between\nactions and rewards and applies the causal policy gradient\nmethod to update the policy.\nWe also approach the multi-stage robotic tasks by decom-\nposing them into multiple subtasks and leveraging causal\nrelationships.\nIII. PROBLEM STATEMENT\nA N-stages task T is modeled from the perspective of\nmulti-reward MDP (s, a, p, r), where s is the state space,\na = (a1, . . . , aK) is the action space, p is the markovian\ntransition model, r is a vector containing all reward items,\ni.e. r = (r1, ...., rM). We decompose each stage of the multi-\nstage task into a subtask, so T is decomposed into N subtasks\n{ST1, . . . , STN}. Each subtask corresponds to a part of the\nreward term ri = (ri\n1, . . . , ri\nJ) ⊆r, i ∈[1, N]. For subtask\nSTi, i ∈[1, N], there is an agent Ai consist of a policy πi.\nStarting from the step the task enters stage i, the sequence of\ninteractions between the agent and the environment through\na series of actions until the task transitions to another stage\nis considered an episode of subtask i. Let τi represent all the\ntime steps in this episode, ri\nt represents the reward obtained\nfor subtask i at time step t, t ∈τi. Each πi needs to maximize\nits corresponding return Ri = P\nt∈τi γri\nt, γ is the discount\nfactor. At step t, st represents the current environmental state,\nand U(st) ∈[1, N] is used to determine the current subtask.\nInvoke AU(st) based on the current subtask index U(st), input\nst into πU(st), output the action value to be executed, and\nobtain the reward rU(st).\n3\nMoving Forward\nTurning\nMove the End Effector\nAlong the x and y Axes\nFig. 2: Two Action Selection Schemes\nIn certain tasks, robots require only a subset of the available\nactions. For instance, as depicted in Fig.2, to move the end\neffector to the red dot, the robot has five actions at its disposal:\nmoving forward, turning, and moving along the x, y, and z\naxes. To achieve horizontal movement, one can either utilize\njust moving forward and turning, or solely the x and y axis\nmovements. For each πi, our goal is to select a set of actions\nthat are both complete enough to accomplish the task in\nstage i and non-redundant to construct the action space to\nreduce redundant exploration and progress reversal in RL.\nAdditionally, each action dimension should be updated based\non the reward items it affects. Both objectives require the use\nof the causal relationship between actions and rewards.\nA. Causal Relationship\nStructural causal modeling is a method that can formally\nexpress the causal assumptions behind the data and can\nbe used to describe the correlation characteristics and their\ninteractions in the real world[20]. The core idea of Structural\nCausal Models is to describe the causal relationships between\nvariables using Directed Acyclic Graphs. Each node represents\na variable, and the directed edges indicate causal relationships,\nmeaning that the arrow points from the cause to the effect.\nThe structural causal model contains two variable sets U\nand V , and a set of functions\nf = {fx : Wx →X|X ∈V }\n(1)\nwhere Wx ⊆(U ∪V ) −{X}. That is, function fx assigns\na value to variable X based on the values of other variables\nin the model. Therefore, the definition of cause and effect\nis: if Y is in the domain of fx, then Y is the cause of X.\nIf node X in the graph is a child node of another node Y ,\nthen Y is the cause of X. The adjacency matrix used to\nrepresent this causal relationship in a directed acyclic graph is\nreferred to as the causal matrix. Here, U represents exogenous\nvariables, which are determined by factors outside the model\nand are not influenced by other variables within the model. Our\nstudy focuses on the causal relationships among endogenous\nvariables V .\nB. Causal Action Space\nTo reduce redundant exploration in each subtask’s agent\nduring RL, one can seek out a complete yet non-redundant\nset of actions that are necessary to accomplish each subtask.\nThe number of reward terms for each subtask can typically\ncharacterize the completion status of the agent’s task. There-\nfore, finding actions that affect task completion is equivalent\nto finding actions that affect the task’s reward terms. To this\nend, it is necessary to study the causal relationships between\nactions and reward terms. This study focuses solely on the\ncausal relationships between action terms and reward terms,\nconsidering action variables as the causal variables and reward\nvariables as the outcome variables. For subtask STi, ri\nj =\nfr(ak) illustrating that ak is the causal of ri\nj, j ∈(1, . . . , J).\nCausal Action cai is:\ncai = {ak|(ak ∈a) ∧(∃ri\nj ∈ri, ri\nj = fr(ak))}\n(2)\nHere, our focus lies in establishing the causal graph model\nrather than specifying the mapping function fr. Therefore, our\nfocus is on obtaining the causal matrix mi for STi. Using\nactions as the rows of the causal matrix and reward items\nas the columns, the causal actions for the subtask STi are\nthe actions corresponding to the rows where the sum of the\ncolumn for ri is not zero. In the field of robot tasks, continuous\nactions are more common, so we discuss the scenario where\nthe action space consists of continuous values.\nC. Causal Policy Gradient\nFor each subtask STi, when obtaining causal actions, the\ncausal matrix mi is first derived. mi can be integrated into RL\nusing causal policy gradients to reduce the gradient variance\nduring the learning process. In robotic tasks, the reward func-\ntion is typically a composite form, and the reward value is the\nlinear summation of reward terms corresponding to each goal.\nHowever, by employing the causal policy gradient[7] method,\ndifferent reward combinations for each action can be used. It\nintegrating causality into the implementation of policy learning\nto reduce gradient variance. For STi, its corresponding agent\nAi’s policy πi has parameters θi, this approach redefines the\npolicy gradient to be:\n∇θiJ(θi) = ∇θilogπθi(cai|s) · mi · bAπi(s, cai)\n(3)\nbAπi(s, cai) is the advantage function factored across the\nreward terms.\nIV. METHOD\nGiven a multi-stage task T, the agent has available actions\na. Divide the reward terms r into {ri|i ∈{1, . . . , n}} ac-\ncording to different task stages. Each ri = {ri\n1, . . . , ri\nJ} is\ncomposed of one or more reward terms. For each subtask\nSTi, i ∈{1, . . . , n}, the first step is to obtain its causal matrix\nmi between actions a and reward terms ri. Then, we use mi\nto construct the action space of the RL policy and integrate\nit into the policy learning using the causal policy gradient\nmethod. Therefore, our approach mainly consists of two major\nsteps: automated causal matrix discovery and training. The\nframework of the method is shown in the Fig.3\n4\nSubtask1\nCausal\nDiscovery\n1\n⋯\n0\n⋮\n⋱\n⋮\n0\n⋯\n1\n𝒂1\n𝒓1\nPolicy1\nConstructing\nthe Action Space\nCausal Policy\nGradient\nSubtask2\nCausal\nDiscovery\n1\n⋯\n0\n⋮\n⋱\n⋮\n0\n⋯\n1\n𝒂2\n𝒓2\nPolicy2\nConstructing\nthe Action Space\nCausal Policy\nGradient\n...\nSubtaskn\nCausal\nDiscovery\n1\n⋯\n0\n⋮\n⋱\n⋮\n0\n⋯\n1\n𝒂n\n𝒓n\nPolicyn\nConstructing\nthe Action Space\nCausal Policy\nGradient\nMulti−stage Task\nFig. 3: Breaking down multi-stage tasks into multiple subtasks, each subtask discovers the causal relationships of the current\nstage, which are used to construct the action space of the current policy and are integrated into the learning process using the\ncausal policy gradient method.\nA. Causal Matrix Discovery\nThe causal models involved in this study only include two\ntypes of variables: action variables and reward item variables.\nAction variables serve only as cause variables, while reward\nitem variables serve only as effect variables. Leveraging the\ncharacteristic of robots to interact with the environment while\nexecuting actions, we propose a method for obtaining causal\nrelationships between actions and rewards, which can achieve\nmore accurate causal relationships than those specified by\nhumans.\nFor subtask STi, the objective is to determine whether\nthere is a causal relationship between ak, k ∈{1, . . . , K} and\nri\nj, j ∈{1, . . . , J}, that is, to determine whether there exists a\ndirected edge starting from vertex ak and ending at vertex ri\nj.\nThis can be determined by having the robot execute different\nvalues of action ak and comparing the changes in the reward\ndistribution under different action values. Therefore, we need\nto intervene on ak to observe whether the intervention affects\nthe distribution of ri\nj. Intervention refers to forcing a variable\nX to take a specific fixed value x [21], using do(X = x) to\ndenote setting the value of X to x. In our design, the inter-\nvention sets a single action variable ak to a fixed value. We\nuse do(ak = 0) to denote the robot’s inaction in dimension i,\nwhich means intervening on ak to set its value to 0. The main\nidea of our method is to compare the reward distributions when\nak takes random value and when it is not executed while all\nother actions except ak are set to random values. This ensures\nthat intervention only on ak, while the random sampling of\nother actions can encompass more complex situations that may\narise during task execution. Let P1 = P(ri\nj|s, do(ak = 0))\nrepresent the probability distribution of ri\nj after intervening\non robot action ak, P2 = P(ri\nj|s, random(ak)) denote the\nprobability distribution of ri\nj when robot action ak takes a\nrandom value. If the difference between P1 and P2 is greater\nthan a certain threshold, it is considered that ak is the causal\nvariable of ri\nj, i.e., ak →ri\nj.\nBelow, we introduce the causal identification process within\na single subtask STi.\n1) Collecting\nData:\nFor\neach\nri,\nplacing\nthe\nagent\nin\nthe\nenvironment\nof\nSTi\nfor\ninteraction\nto\nobtain\nrandom\ndata(Di\nrandom)\nand\nintervention\ndata(Di\ndo\n=\n{Di,a1\ndo , . . . , Di,aK\ndo\n}). Di\nrandom refers to the data when action\nvalues in all dimensions take random values, while Di\ndo\nincludes data when actions in each dimension are not executed\nindividually. The data Di,ak\ndo , k ∈(1, . . . , K) for intervening\non a single action ak must have the same number of data as\nDi\nrandom. At time t, both types of data include the environ-\nmental state st, the action taken at and the reward ri\nt. If during\nthe data collection process there is a situation where u(st) ̸= i,\nit indicates that the agent has entered a different stage from\ni due to the execution of at, and therefore the environment\nneeds to be reset to STi. The random data Di\nrandom and\nintervention data Di\ndo will then be combined for training the\nreward prediction model. In addition, it is necessary to collect\na portion of data where ak takes random values (Di\ninference)\nfor calculating the difference between the intervention and\nrandom probability distributions.\n2) Training Reward Prediction Models: A separate reward\nprediction model is constructed for each pair (ak, ri\nj) in STi.\nWe use a neural network to fit the reward situations under both\nrandom and intervention scenarios. For each action ak, Di,ak\ndo\nand Di\nrandom are combined to train the neural network to get\na reward prediction model. The reward prediction model takes\nthe state of the environment s and the action a performed by\nthe robot as input and output the mean and the logarithm of\nthe standard deviation of the expected reward ri. The mean\nand the standard deviation are then used to construct a normal\ndistribution. The loss function is the negative log probability\nof ri\nj under this normal distribution.\n3) Causal Discovery: After training the reward prediction\nmodel for each pair (ak, ri\nj) in STi, the distribution differences\nare calculated respectively. For each pair (ak, ri\nj), we first\ncreate a copy of Di\ninference and set all ak in the copy to\n5\n𝑐𝑜𝑝𝑦\n𝑟𝑎𝑛𝑑𝑜𝑚\n𝑎𝑐𝑡𝑖𝑜𝑛\n𝑐𝑜𝑝𝑦𝑑𝑎𝑡𝑎\n𝑠𝑒𝑡𝑎𝑘𝑡𝑜 0\n𝑅𝑒𝑤𝑎𝑟𝑑𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑀𝑜𝑑𝑒𝑙\n𝑜𝑓𝑎𝑘𝑤𝑖𝑡ℎ𝑟𝑗\n𝑖\n𝑃1 = 𝑃𝑟𝑗\n𝑖𝒔, 𝑟𝑎𝑛𝑑𝑜𝑚𝑎𝑘\n𝐶𝑎𝑙𝑐𝑢𝑙𝑎𝑡𝑖𝑛𝑔𝑡ℎ𝑒𝐷𝑖𝑓𝑓𝑒𝑟𝑒𝑛𝑐𝑒\n𝐾𝐿_𝑑𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒𝑃1, 𝑃2\n𝑃2 = 𝑃𝑟𝑗\n𝑖𝒔, 𝑑𝑜𝑎𝑘= 0\n𝐷𝑖𝑛𝑓𝑒𝑟𝑒𝑛𝑐𝑒\n𝑖\nFig. 4: Procedure for Calculating the Difference for Each Pair(ak,ri\nj)\n0. Setting the corresponding action in the copy to 0 simulates\nthe scenario where action ak is not executed, while all other\nenvironmental states remain the same. Therefore, the s and a\nin Di\ninference and copy correspond to the data when the robot\nperforms action ak or not in the same environment. Then, we\ninput the s and a from the copy and the Di\ninference into the\nreward prediction model to get P1 = P(ri\nj|s, do(ak = 0))\nand P2 = P(ri\nj|s, random(ak)) separately. P1 and P2 are\nthe reward distributions for taking or not taking ak under\nthe same s. After obtaining P1 and P2, we need a criterion\nto measure the difference between these two distributions.\nKL divergence is commonly used to measure the distance\nbetween two probability distributions [22]. Therefore, we\nchoose the KL divergence to calculate the difference between\nthe distributions P1 and P2. The entire process of obtaining\nthe KL divergence between the random distribution P2 and\nthe intervention distribution P1 of (ak, ri\nj) is shown in the\nFig. 4.\nFor ri\nj, a list of KL divergences that include all dimensions\nof actions will be obtained, represented as\nKLDlistri\nj = [klda1\nri\nj , klda2\nri\nj , ..., kldaK\nri\nj ]\n(4)\nThe process of selecting causal actions for ri\nj is illustrated\nin Algorithm 2. Assuming that there is always at least one\naction that affects ri\nj, we select the causal action based on\nthe degree of difference in KLDlistri\nj. The Coefficient of\nVariation is a normalized measure of the dispersion of a\nprobability distribution, defined as the ratio of the standard\ndeviation to the mean. Therefore, we use the coefficient of\nvariation to measure the degree of difference in KLDlistri\nj.\nIf the coefficient of variation of KLDlistri\nj is greater than\na threshold ϵcv, this indicates that different actions have a\nsignificant difference in their impact on ri\nj. In this case, to\navoid the influence of a particular action being too significant\nand overshadowing the effects of other causal actions, the\nKLDlistri\nj needs to be normalized first. Therefore, normalize\nKLDlistri\nj using min-max scaling to range between 0 and\n1, and then select actions that have values greater than the\nthreshold ϵnormalize as the reasonable actions of the ri\nj. If the\ncoefficient of variation of KLDlistri\nj is smaller than ϵcv, it\nsuggests that all actions have a similar impact on ri\nj and none\nstand out significantly. In this cases, instead of performing\nmin-max scaling on KLDlistri\nj, we directly select actions\nfrom it. Actions with KL divergence exceeding the threshold\nϵdirect are considered to be causal actions. After obtaining the\nset of causal actions for each ri\nj, it can be converted into the\nform of a causal matrix.\nAfter multiple sets of tests, we found that setting ϵcv to 1.00,\nϵnormalize to 0.10, and ϵdirect to 0.01 is more appropriate. The\nentire process of causal discovery is shown in Algorithm 1.\nAlgorithm 1 Causal Discovery\n1: Input:\n2:\nAn agent capable of performing action a\n3:\ntrainingCount, number of training samples\n4:\ninferenceCount, number of inference samples\n5:\nCausal matrix list causalMatrixs ←{}\n6: Output: Causal matrices for each stage\n7:\n8: for i from 1 to N do\n9:\nDi\nrandom ←{}, Di\ninference ←{}, Di\ndo ←{}\n10:\nwhile collectedCount < trainingCount do\n11:\ncollect random data add to Di\nrandom\n12:\nend while\n13:\nwhile collectedCount < inferenceCount do\n14:\ncollect predictive data add to Di\ninference\n15:\nend while\n16:\nfor a ∈a do\n17:\nwhile collectedCount < trainingCount do\n18:\ncollect intervention data on a and add it to Di\ndo\n19:\nend while\n20:\nend for\n21:\ntraining reward prediction model of ri using Di\nrandom\nand Di\ndo\n22:\nobtain the causal matrix mi through the reward predic-\ntion model of ri and Di\ninference\n23:\nadd mi to causalMatrixs\n24: end for\n25: return causalMatrixs\n6\nAlgorithm 2 Selection of Causal Actions\n1: Input:KLDlist, a list of KL divergences for a reward\nitem\n2: Output: SelectedActions, the causal actions for the\nreward item\n3:\n4: µ ←mean(KLDlist)\n5: σ ←Standard Deviation(KLDlist)\n6: cv ←σ/µ\n7: SelectedActions ←{}\n8: threshold ←ϵdirect\n9: if cv > ϵcv then\n10:\nKLDlist ←Min-Max Normalization(KLDlist)\n11:\nthreshold ←ϵnormalize\n12: end if\n13: for each KLD ∈KLDlist do\n14:\nif KLD > threshold then\n15:\nadd action corresponding to KLD\n16:\nto SelectedActions\n17:\nend if\n18: end for\n19: return SelectedActions\nB. Training\nThe causal matrix uses action items as rows and reward\nitems as columns. By following the causal discovery steps, we\nobtain causal matrixs {mi|i ∈1, . . . , n} corresponding to each\nstage’s subtask. For causal matrix mi, select the set of action\nterms whose row sums are not zero to construct the action\nspace for agent Ai. In addition to using the causal actions to\nconstruct the action space for the policy to reduce redundant\nexploration in RL, the causal matrix mi is also integrated into\nthe training process of RL using the causal policy gradient\nmethod. During the gradient ascent process, multiplying by\nthe causal matrix mi ensures that only the reward values of\nthe action outcomes are passed to the corresponding actions.\nThis can effectively reduce the gradient variance during the\ntraining of each subtask. The training process is shown in\nalgorithm 3.\nV. EXPERIMENTAL SETUP\nWe conducted comparative experiments on a mobile ma-\nnipulation task and a pure manipulation task. The first type\ninvolves scenarios that require both robot mobility and manip-\nulation by the robotic arm. The second type focuses solely on\nmanipulation. Both tasks were conducted using the Fetch robot\nfor experiments. The robot we use is equipped with a high-\nlevel controller, allowing the robot to directly perform macro\nactions such as moving forward and turning, and moving the\nend effector along the axes of its base coordinate system,\nwithout having to consider the control of each joint. Simulation\nexperiments were conducted in iGibson [23].\nA. Mobile Manipulation Task\nThe mobile manipulation task scenario is set in an indoor\nliving room, where each time randomly generates a target\nAlgorithm 3 Training\n1: Input:\n2:\n{An|n ∈(1, . . . , n)}, set of N agents\n3:\ncausalMatrixs, causal matrix list\n4:\ntotalSteps, total training steps\n5:\nn step, the number of steps sampled in an epoch\n6:\ncount, number of training samples\n7:\n8: for i from 1 to N do\n9:\nget Causal action cai from mi\n10:\nbuild Ai’s policy πi using cai\n11: end for\n12: if The current algorithm is on-policy. then\n13:\nwhile currentSteps < totalSteps do\n14:\nfor step from 1 to n step do\n15:\nSelect agent u(st) to interact with the environment,\ncollecting state, action, and reward data.\n16:\nend for\n17:\nfor i from 1 to N do\n18:\ntraining πi with causal policy gradient method by\nusing mi\n19:\nend for\n20:\nend while\n21: else\n22:\nwhile currentSteps < totalSteps do\n23:\nfor step from 1 to n step do\n24:\nSelect agent u(st) to interact with the environment,\ncollecting state, action, and reward data.\n25:\nif The cumulative number of new samples obtained\nfor agent u(st) >= count then\n26:\ntraining πi with causal policy gradient method\nby using mi\n27:\nend if\n28:\nend for\n29:\nend while\n30: end if\npoint. The robot needs to navigate autonomously to the vicinity\nof the target point and place its end effector inside the target\npoint. The mobile manipulation task consists of three stages:\n1) Navigate to a certain range around the target point.\nThis stage includes a reward item rρ that measures the\nhorizontal distance from the robot’s base to the target\npoint.\n2) The robot has reached near the target point and needs to\nturn to face the target point directly. This stage includes\na reward item rθ that measures the angle between the\nrobot and the target point.\n3) The robot, already near the target point and facing it,\nmoves the end effector of the manipulator to within\nthe target point. This stage includes 3 reward items\nreefx, reefy, reefz, each measuring the distance between\nthe end effector and the target point along the axes of\nthe world coordinate system.\nThe stages of the task are shown in the Fig.5. In addition\nto the aforementioned reward items, the mobile manipulation\n7\ntask also has three reward items: base collision, robotic arm\ncollision, and self collision. As collision-related reward items\ndo not signify task success, these three are not utilized in the\ncausal discovery phase, but are only used for training. During\ntraining, the corresponding positions in the causal matrix for\nthe reward items of these three types of collisions are all set\nto 1.\nIn this task, the Fetch robot is capable of moving forward\nforward, turning turn, and moving the end effector along\nits own base coordinate system armx, amry, armz, totaling\n5 executable actions. The observation space for this task is\ndetailed in the appendix A.\nWe conducted comparative experiments based on the on-\npolicy RL method PPO [24] for this task. Our method is named\ncmPPO (causal multiple PPO). In cmPPO, the action space of\neach agent is composed of causal actions obtained through\ncausal discovery, and causal policy gradient learning is imple-\nmented using the corresponding causal matrix. It is compared\nto the regular PPO algorithm (mPPO). In mPPO, each subtask\nutilizes all available actions as its action space, and each sub-\npolicy is trained using the standard PPO algorithm.\n(a) Stage 1\n(b) Stage 2\n(c) Stage 3\nFig. 5: Mobile Manipulation Task\nB. Pure Manipulation Task\nThis task is a four-stage mission that requires the Fetch\nrobot to use its end effector to grasp a pencil box placed on\na table:\n1) Move the end effector to a certain height directly\nabove the pencil box. This stage involves three reward\nitems reefx, reefy, reefz1, which measure the distance\nbetween the end effector and the target point above the\npencil box along the three axes of the world coordinate\nsystem.\n2) Maintain the position of the end effector reached in stage\n1, and rotate the end effector to face downward. This\nstage includes a reward item rori measuring the distance\nbetween the current pose of the end effector and its pose\nwhen facing downward.\n3) The end effector maintains the posture reached in stage\n2, and stays at the horizontal position achieved in stage\n1, only lowering the height of the end effector until it\ncan grasp the pencil box. This stage involves a reward\ncomponent reefz2 that measures the height difference\nbetween the end effector and the pencil box.\n4) Close the end effector to grasp the pencil box. This stage\ninvolves a reward item rgripper\nThe stages of the task are shown in the Fig.6. Stages 1 to 3 also\ninclude the reward rgripper in addition to the aforementioned\nrewards. This is because the end effector should not close\nbefore reaching stage 4. Similarly, the rewards for each stage\nalso take into account collisions of the robotic arm and self\ncollisions. These two reward items do not participate in the\ncausal discovery step, but are directly applied to the training\nstep.\nIn this task the Fetch robot has executable actions such\nas moving the end effector along its own base coordinate\nsystem armx, army, armz, rotating around the end effector\ncoordinate system armrx, armry, armrz, and grasping grip.\nThe observation space for this task is detailed in the appendix\nB.\nFor this task, we conducted comparative experiments based\non the off-policy RL algorithm Soft Actor-Critic (SAC)\n[25]. Our proposed method is called cmSAC (causal multi-\nple SAC). Similarly to mPPO, this experiment also sets up\nmSAC(multiple SAC) for comparison. Furthermore, to com-\npare the differences in RL training between manually specified\ncausal relationships and those obtained using automatic dis-\ncovery methods, the cmmSAC (causal manual multiple SAC)\nmethod was introduced for this task. This method uses a\nmanually specified causal graph to construct the action space\nand perform causal policy gradient learning. Finally, the cSAC\n(cooperative SAC) method [14], which involves cooperative\nlearning among agents, was also employed. In this method,\neach sub-agent similarly uses all actions to construct the action\nspace.\nC. Reward Setting\nThe reward setting logic for the two tasks is the same. Let\nxt denote the value of one of the environmental variables X\nat time t, and xtarget represent the value this environmental\nvariable should take when the subtask is completed. Dt =\n∥xt −xtarget∥represent the distance to the target point. For\nexample, in mobile manipulation task, Dt can represent the\nhorizontal distance between the robot base and the target point\nat time t. The reward at time t is defined as rt = λ(Dt−1 −\nDt), λ is a constant. This reward encourages agents to adjust\ntheir responsible state variables to appropriate values. For the\nreward item related to collisions, the value is usually 0, and\na fixed negative reward is given once a collision occurs. For\nthe reward rgripper, a fixed negative reward is given if the\nend effector closes before stage 4 of the pure manipulation\ntask. If it successfully grasps the pencil box by closing in\n8\nTABLE I: mobile manipulation task KL divergence\nstage\nreward forward\nturn\narmx\narmy\narmz\ncv\nforward n turn n armx n\narmy n armz n\n1\nrρ\n0.0532\n0.0122\n0.0002\n0.0003\n0.0001\n1.74\n1.0000\n0.2279\n0.0012\n0.0033\n0.0000\n2\nrθ\n0.1131\n0.1056\n0.0050\n0.0007\n0.0001\n1.17\n1.0000\n0.9343\n0.0438\n0.0054\n0.0000\n3\nreefx\n0.0076\n0.0163\n0.3679\n1.3100\n0.0236\n1.62\n0.0000\n0.0067\n0.2767\n1.0000\n0.0123\nreefy\n0.0084\n0.0042\n1.6487\n0.2820\n0.0167\n1.82\n0.0025\n0.0000\n1.0000\n0.1689\n0.0076\nreefz\n0.0017\n0.0007\n0.2389\n0.0128\n8.7254\n2.16\n0.0001\n0.0000\n0.0273\n0.0014\n1.0000\nTABLE II: pure manipulation task KL divergence\nstage\nreward\narmx\narmy\narmz\narmrx armry armrz\ngrip\ncv\narmx n army n armz n armrx n armry n armrz n grip n\n1\nreefx\n0.0001 0.3381 0.0003 0.0001 0.0000 0.0009 0.0001 2.44\n0.0002\n1.0000\n0.0008\n0.0003\n0.0000\n0.0026\n0.0002\nreefy\n0.1080 0.0000 0.0003 0.0000 0.0012 0.0011 0.0000 2.38\n1.0000\n0.0003\n0.0030\n0.0000\n0.0107\n0.0103\n0.0000\nreefz1\n0.0071 0.0000 0.0456 0.0009 0.0009 0.0005 0.0001 1.98\n0.1561\n0.0000\n1.0000\n0.0190\n0.0188\n0.0100\n0.0010\n2\nrori\n0.0045 0.0007 0.0011 0.2246 0.0599 0.0130 0.0006 1.76\n0.0173\n0.0002\n0.0022\n1.0000\n0.2648\n0.0550\n0.0000\n3\nreefz2\n0.0717 0.0018 3.4506 0.0658 0.0550 0.0149 0.0012 2.29\n0.0204\n0.0002\n1.0000\n0.0187\n0.0156\n0.0040\n0.0000\n4\nrgripper 0.0107 0.0025 0.0061 0.0001 0.0107 0.0001 4.6423 2.43\n0.0023\n0.0005\n0.0013\n0.0000\n0.0023\n0.0000\n1.0000\n(a) Stage 1\n(b) Stage 2\n(c) Stage 3\n(d) Stage 4\nFig. 6: Pure Manipulation Task\nstage 4, a fixed positive reward is given. In both tasks, a fixed\nreward value is directly given upon successful completion of\na subtask.\nThe neural network structures and hyperparameter settings\nused in the experiments are detailed in the appendix C, D.\n1\n1\n1\n1\n0\n1\n1\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n𝑓𝑜𝑟𝑤𝑎𝑟𝑑\n𝑡𝑢𝑟𝑛\n𝑎𝑟𝑚𝑥\n𝑎𝑟𝑚𝑦\n𝑎𝑟𝑚𝑧\n𝑟𝑒𝑒𝑓𝑥𝑟𝑒𝑒𝑓𝑦𝑟𝑒𝑒𝑓𝑧\n𝑟ρ\n𝑟θ\nFig. 7: Causal Matrix 1\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n𝑎𝑟𝑚𝑥\n𝑎𝑟𝑚𝑦\n𝑎𝑟𝑚𝑧\n𝑎𝑟𝑚𝑟𝑥\n𝑎𝑟𝑚𝑟𝑦\n𝑎𝑟𝑚𝑟𝑧\n𝑔𝑟𝑖𝑝\n𝑟𝑒𝑒𝑓𝑥𝑟𝑒𝑒𝑓𝑦𝑟𝑒𝑒𝑓𝑧1 𝑟𝑜𝑟𝑖𝑟𝑒𝑒𝑓𝑧2 𝑟𝑔𝑟𝑖𝑝𝑝𝑒𝑟\nFig. 8: Causal Matrix 2\n9\n𝑟ρ\n𝑓𝑜𝑟𝑤𝑎𝑟𝑑\n𝑡𝑢𝑟𝑛\n𝑎𝑟𝑚𝑥\n𝑎𝑟𝑚𝑦\n𝑎𝑟𝑚𝑧\n𝑟𝑒𝑒𝑓𝑥\n𝑟𝑒𝑒𝑓𝑦\n𝑟𝑒𝑒𝑓𝑧\n𝑟θ\nFig. 9: Causal Graph 1\n𝑎𝑟𝑚𝑥\n𝑎𝑟𝑚𝑦\n𝑎𝑟𝑚𝑧\n𝑟𝑒𝑒𝑓𝑥\n𝑟𝑒𝑒𝑓𝑦\n𝑟𝑒𝑒𝑓𝑧1\n𝑎𝑟𝑚𝑟𝑥\n𝑎𝑟𝑚𝑟𝑦\n𝑎𝑟𝑚𝑟𝑧\n𝑟𝑜𝑟𝑖\n𝑟𝑒𝑒𝑓𝑧2\n𝑟𝑔𝑟𝑖𝑝𝑝𝑒𝑟\n𝑔𝑟𝑖𝑝\nFig. 10: Causal Graph 2\nVI. RESULT\nThe causal matrices obtained from the two tasks are shown\nin Fig. 7 and Fig. 8. For the sake of concise representation in\nthis paper, we depict the causal matrices of multiple stages\nof a task as a single matrix, and use different colors to\nlabel the reward items to distinguish between task stages.\nThe KL divergence, coefficient of variation, and the results\nof the normalized KL divergence for the mobile manipulation\ntask and the pure operation task are shown in Table 1 and\nTable 2, respectively. Here, cv denotes the coefficient of\nvariation, and variables with the suffix ”n” correspond to\nthe normalized values. To more intuitively observe the causal\nrelationships between actions and reward items, the causal\nmatrices are drawn into the causal graphs shown in Fig.9 and\nFig.10. Similarly, for ease of presentation, the causal graphs\nfor different stages are drawn together, with reward items\nmarked in different colors. Except for rgripper, different colors\nrepresent rewards of different stages. rgripper is a reward item\nthat exists in every stage of the pure manipulation task.\nIt is worth noting that in stage 3 of the mobile manipulation\ntask, forward and turn are also causal actions. Because un-\nder the default settings of the iGibson controller, the magnitude\nof change for these two actions is relatively large compared to\nthe actions of moving the robotic arm. Although performing a\nsignificant degree of movement in stage 3 can cause the stage\nto regress to stage 2 or 1, making small forward movements\nwithin a limited range will not. Moreover, due to the noticeable\nmagnitude of movement of these two actions, their impact on\nthe horizontal position of the end effector in stage 3 is not less\nthan that of the actions of moving the robotic arm.\nIn the scenario setting of the grasping task, the robot\nbase coordinate system is exactly orthogonal to the world\ncoordinate system. The reward items are defined relative to\nthe world coordinate system, while the end effector movement\nactions of the robotic arm are defined relative to the robot\nbase coordinate system. Therefore, the results indicate that\narmx affects reefy, and army affects reefx. It is worth\n𝑎𝑟𝑚𝑥\n𝑎𝑟𝑚𝑦\n𝑎𝑟𝑚𝑧\n𝑟𝑒𝑒𝑓𝑥\n𝑟𝑒𝑒𝑓𝑦\n𝑟𝑒𝑒𝑓𝑧1\n𝑎𝑟𝑚𝑟𝑥\n𝑎𝑟𝑚𝑟𝑦\n𝑎𝑟𝑚𝑟𝑧\n𝑟𝑜𝑟𝑖\n𝑟𝑒𝑒𝑓𝑧2\n𝑟𝑔𝑟𝑖𝑝𝑝𝑒𝑟\n𝑔𝑟𝑖𝑝\nFig. 11: Manual Causal Graph\nnoting that in the pure manipulation task, moving the end\neffector along the robot’s own coordinate system x-axis will\nalso affect the reward of the end effector on the z-axis. This\nis because the length of the robotic arm is limited. In order\nto continue moving forward along the x-axis, it will force\nthe end effector to lower its height. Because the robot has\na macro controller that can directly move the end effector\nalong its own coordinate system, if humans manually provide\nthe causal relationship, it may mistakenly think that armx is\nnot the causal action of reefz1. As for reefz2, once the end\neffector moves horizontally beyond the specified range leading\nto u(st) ̸= u(st−1), it is no longer in the stage of reefz2.\nTherefore, armx is not a causal variable of reefz2. It was also\nfound that armrz does not affect the posture reward rori, as\nthe initial pose of the end effector only needs to be adjusted\naround the x and y axes to achieve the desired pose. If the\ncausal graphs for these two cases are manually provided, then\narmx would not be a cause of reefz1, while armrz would be\na cause of rori. The manually proposed causal relationships\nare shown in the Fig.11. Experiments with cmmSAC were\nconducted based on this manually provided causal graph.\nAccording to the obtained causal relationships, the causal\naction sets used for constructing the action spaces at each\nstage are shown in the table III. grip is the causal action of\nrgripper, and the reward rgripper exists in every stage of the\npure manipulation task. Therefore, the action space of every\nstage of the pure manipulation task includes grip.\nTABLE III: Action Spaces\ntask\nstage\nactions\nmobile manipulation\n1\nforward, turn\n2\nforward, turn\n3\nforward, turn, armx, army, armz\npure manipulation\n1\narmx, army, armz, grip\n2\narmrx, armry, grip\n3\narmz, grip\n4\ngrip\nFor the mobile manipulation task, comparative experiments\nare conducted using the causal relationships obtained above\nand based on the PPO algorithm for RL. The success rate curve\nof the mobile manipulation task is shown in the Fig.12. It can\n10\nbe seen that our cmPPO method has a significantly higher\nsuccess rate than the mPPO method under the same number\nof time steps. Here, it can be preliminarily demonstrated\nthat using causal actions and causal policy gradient methods\ncan effectively enhance the performance of RL. For the pure\nmanipulation task, experiments are conducted based on the\noff-policy SAC method. The success rate curves of the four\nmethods based on SAC are shown in the Fig.13. In the experi-\nmental results of this task, it can be seen that the two methods\nusing causal actions and causal policy gradients, cmSAC and\ncmmSAC, can complete the task effectively. However, the\ncSAC and mSAC methods, which use all available actions to\nconstruct the action space, show difficulty in completing this\nfour-stage task. The performance of the latter two methods\noften involves moving from stage 1 to stage 2 and then\nregressing back to stage 1, resulting in a reversal of progress.\nDue to frequent reversals of progress caused by actions in other\ndimensions during the learning process, the entire learning task\nstruggles to advance to later stages. The cSAC method allows\ntasks between different stages to learn from each other through\ncooperation. However, due to a large number of reversals in\nprogress, the policies of the subtasks in the later stages are\ndifficult to learn, leading to poor performance of the cSAC\nmethod on this task. By using causal actions, it is possible\nto eliminate to the greatest extent the unnecessary actions\nfor completing subtasks. This not only reduces redundant\nexploration but also helps to avoid some cases of progress\nreversal.\nFor cmSAC and cmmSAC, they both use causal actions and\ncausal policy gradient methods. The former uses the causal\nrelationships obtained by the automatic discovery method\nmentioned above, while the latter uses the causal relationships\nproposed by humans. From the success rate curve, it can be\nseen that at the same number of time steps, the cmSAC method\nhas a more stable and higher success rate than cmmSAC. This\nalso indicates that the causal relationships discovered through\nthe robot’s own interactions in the environment are more\naccurate than those specified by humans. Causal relationships\nobtained through interaction in specific environments can\nreveal points that humans might overlook. Utilizing this more\nprecise causal relationship can lead to better performance in\ndeep RL.\nVII. CONCLUSION\nIn this work, we propose a method that utilizes causal\nrelationships to handle robotic multi-stage tasks. Multi-stage\ntasks are broken down into multiple separate subtasks and\naddressed from the perspective of deep RL. By enabling the\nrobot to interact with the environment, the causal relationships\nbetween the robot’s macro actions and reward items are\nautomatically discovered. The discovered causal actions are\nused to construct the action space of the agent corresponding to\neach subtask. Moreover, the causal relationships are integrated\ninto the learning process of the subtasks using the causal policy\ngradient method. Experiments on a mobile manipulation task\nand a pure manipulation task demonstrate that the automatic\ndiscovery method, which leverages the robot’s interaction with\nFig. 12: Mobile Manipulation Task\nFig. 13: Pure Manipulation Task\nthe environment, can obtain more accurate causal relationships\nthan those specified by humans. Applying more accurate\ncausal relationships to the causal policy gradient method can\nachieve better performance in RL. Furthermore, constructing\nthe action space using causal actions can reduce redundant\nexploration in the RL process and also help to avoid some\ncases of progress reversal in multi-stage tasks.\nVIII. LIMITATIONS\nAlthough the method proposed in this paper significantly\nimproves the performance of RL in multi-stage robotic tasks\nthrough causal relationships, there are still some limitations\nin terms of the generalization ability of causal relationships,\nthe applicability to multi-robot collaborative tasks, and the\ndependence on task decomposition. Firstly, the discovery of\ncausal relationships in this paper relies on the interaction data\nbetween the robot and the environment, which means that\nthese causal relationships need to be relearned under differ-\nent environmental conditions or task variants. Secondly, the\nmethod in this paper mainly focuses on the causal relationships\nbetween individual robot actions and rewards. In scenarios\ninvolving multiple robots collaborating to complete a task,\nthe actions of individual robots may have complex causal\n11\ndependencies with the actions of other robots, and the method\nproposed in this paper may not be able to effectively handle\nsuch situations. Moreover, the effectiveness of this method de-\npends on whether the task can be reasonably decomposed into\nmultiple subtasks. If the task decomposition is not appropriate,\nit may lead to the identification of incorrect causal actions,\nmaking the task difficult to complete. Future work can explore\nhow to enhance the generalizability of causal relationships,\nextend the method to handle causal relationships in multi-robot\ncollaborative tasks, and investigate methods for reasonable task\ndecomposition, thereby further improving the applicability of\nthis method in diverse task scenarios.\nREFERENCES\n[1] M. Duguleana and G. Mogan, “Neural networks based reinforcement\nlearning for mobile robots obstacle avoidance,” Expert Systems with\nApplications, vol. 62, pp. 104–115, 2016.\n[2] J. Kulh´anek, E. Derner, T. De Bruin, and R. Babuˇska, “Vision-based\nnavigation using deep reinforcement learning,” in 2019 european con-\nference on mobile robots (ECMR), pp. 1–8, IEEE, 2019.\n[3] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement\nlearning for robotic manipulation with asynchronous off-policy updates,”\nin 2017 IEEE international conference on robotics and automation\n(ICRA), pp. 3389–3396, IEEE, 2017.\n[4] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, “Hindsight\nexperience replay,” Advances in neural information processing systems,\nvol. 30, 2017.\n[5] C. Wang, Q. Zhang, Q. Tian, S. Li, X. Wang, D. Lane, Y. Petillot, and\nS. Wang, “Learning mobile manipulation through deep reinforcement\nlearning,” Sensors, vol. 20, no. 3, p. 939, 2020.\n[6] A. Hundt, B. Killeen, N. Greene, H. Wu, H. Kwon, C. Paxton, and G. D.\nHager, ““good robot!”: Efficient reinforcement learning for multi-step\nvisual tasks with sim to real transfer,” IEEE Robotics and Automation\nLetters, vol. 5, no. 4, pp. 6724–6731, 2020.\n[7] J. Hu, P. Stone, and R. Mart´ın-Mart´ın, “Causal policy gradient for whole-\nbody mobile manipulation,” arXiv preprint arXiv:2305.04866, 2023.\n[8] E. Stengel-Eskin, A. Hundt, Z. He, A. Murali, N. Gopalan, M. Gombo-\nlay, and G. Hager, “Guiding multi-step rearrangement tasks with natural\nlanguage instructions,” in Conference on Robot Learning, pp. 1486–\n1501, PMLR, 2022.\n[9] J. Bao, G. Zhang, Y. Peng, Z. Shao, and A. Song, “Learn multi-step\nobject sorting tasks through deep reinforcement learning,” Robotica,\nvol. 40, no. 11, pp. 3878–3894, 2022.\n[10] J. Oh, S. Singh, H. Lee, and P. Kohli, “Zero-shot task generalization with\nmulti-task deep reinforcement learning,” in International Conference on\nMachine Learning, pp. 2661–2670, PMLR, 2017.\n[11] O. Nachum, S. S. Gu, H. Lee, and S. Levine, “Data-efficient hierarchical\nreinforcement learning,” Advances in neural information processing\nsystems, vol. 31, 2018.\n[12] C. Li, F. Xia, R. Martin-Martin, and S. Savarese, “Hrl4in: Hierarchical\nreinforcement learning for interactive navigation with mobile manipula-\ntors,” in Conference on Robot Learning, pp. 603–616, PMLR, 2020.\n[13] Y. Luo, Y. Wang, K. Dong, Q. Zhang, E. Cheng, Z. Sun, and B. Song,\n“Relay hindsight experience replay: Continual reinforcement learning\nfor robot manipulation tasks with sparse rewards,” arXiv preprint\narXiv:2208.00843, 2022.\n[14] J. Erskine and C. Lehnert, “Developing cooperative policies for multi-\nstage reinforcement learning tasks,” IEEE Robotics and Automation\nLetters, vol. 7, no. 3, pp. 6590–6597, 2022.\n[15] D. Wang, F. Chang, and C. Liu, “Multi-stage reinforcement learning for\nnon-prehensile manipulation,” arXiv preprint arXiv:2307.12074, 2023.\n[16] J. Andreas, D. Klein, and S. Levine, “Modular multitask reinforcement\nlearning with policy sketches,” in International conference on machine\nlearning, pp. 166–175, PMLR, 2017.\n[17] M. Gasse, D. Grasset, G. Gaudron, and P.-Y. Oudeyer, “Causal rein-\nforcement learning using observational and interventional data,” arXiv\npreprint arXiv:2106.14421, 2021.\n[18] J. Pearl, “The do-calculus revisited,” arXiv preprint arXiv:1210.4852,\n2012.\n[19] X. Hu, R. Zhang, K. Tang, J. Guo, Q. Yi, R. Chen, Z. Du, L. Li, Q. Guo,\nY. Chen, et al., “Causality-driven hierarchical structure discovery for\nreinforcement learning,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 20064–20076, 2022.\n[20] J. Pearl, M. Glymour, and N. P. Jewell, Causal inference in statistics:\nA primer. John Wiley & Sons, 2016.\n[21] J. Pearl, Causality. Cambridge university press, 2009.\n[22] T. M. Cover, Elements of information theory. John Wiley & Sons, 1999.\n[23] C. Li, F. Xia, R. Mart´ın-Mart´ın, M. Lingelbach, S. Srivastava, B. Shen,\nK. E. Vainio, C. Gokmen, G. Dharan, T. Jain, A. Kurenkov, K. Liu,\nH. Gweon, J. Wu, L. Fei-Fei, and S. Savarese, “igibson 2.0: Object-\ncentric simulation for robot learning of everyday household tasks,” in\nProceedings of the 5th Conference on Robot Learning (A. Faust, D. Hsu,\nand G. Neumann, eds.), vol. 164 of Proceedings of Machine Learning\nResearch, pp. 455–465, PMLR, 08–11 Nov 2022.\n[24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[25] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning, pp. 1861–1870,\nPMLR, 2018.\nAPPENDIX\nA. Mobile Manipulation Observation Space\nThe observable state variables of the robot in this task\ninclude:\n1) ρ : the polar radius of the target point relative to the\nrobot’s base coordinate system in the horizontal plane,\n2) θ : the polar angle of the target point relative to the\nrobot’s base coordinate system in the horizontal plane,\n3) height : the height of the target point relative to the\nrobot base coordinate system,\n4) v forward : the speed at which the robot moves\nforward,\n5) v turn : the speed at which the robot turns,\n6) eeflx, eefly, eeflz : the position of the robot end\neffector relative to the robot base coordinate system.\nIn addition to these environmental variables, the observation\nspace also includes a 220-dimensional LiDAR scan.\nB. Pure Manipulation Observation Space\nThe observation space includes an RGB image with dimen-\nsions (64x48x3). In this set of tasks, the observed environ-\nmental variables include:\n1) pencil box x,pencil box y,pencil box z : the posi-\ntion of the pencil box relative to the robot’s base\ncoordinate system,\n2) eefx, eefy, eefz : the position of the robot end effector\nrelative to the robot base coordinate system,\n3) target eeflz : the expected stopping height of the end\neffector in step 1,\n4) ori1, ori2, ori3, ori4 : the current posture of the end\neffector,\n5) target ori1, target ori2, target ori3, target ori4 :\nthe target posture of the end effector.\nC. Neural Network Architecture\nMobile manipulation task use 1D convolutional neural net-\nwork to process LiDAR scan data, while pure manipulation\ntask use 2D convolutional neural network to process RGB\n12\nimage data. Below are explanations for some related symbols:\nC1(n, k, s), 1D convolution layers, with n being the number\nof kernels, k being the kernel size, and s being the stride;\nC2(n, k, s), 2D convolution layers, with n being the number\nof kernels, k being the kernel size, and s being the stride;\nPool(k, s), pool layers, with k being the kernel size, and s\nbeing the stride; F(n), fully connected layer;L, Flattening.\nBoth the causal discovery phase and the reinforcement\nlearning training phase require processing of LiDAR data\nand\nRGB\nimage\ndata,\nand\nthe\nneural\nnetwork\nstruc-\ntures used in both phases are the same. The LiDAR\nscan data is passed through C1(32, 8, 4) −C1(64, 4, 2) −\nC1(64, 3, 1) −L, and the RGB data is passed through\nC2(32, (8, 6), 4) −C2(64, (4, 3), 2) −C2(32, (8, 6), 4) −\nPool((2, 2), 2)−C2(64, (2, 2), 2)−L. In the causal discovery\nphase, the output are concatenated with the vector of environ-\nmental variables. Then the concatenated vector passes through\nF(128) −F(128) −F(128) −(F(1), F(1)),outputs the mean\nand logarithm of the standard deviation. In the reinforcement\nlearning training phase, the results of the above feature extrac-\ntion are also concatenated with other environmental variables.\nSubsequently, for the mobile manipulation task experiments\nbased on PPO, the data is input into policy networks and\nvalue networks with the structure F(64) −F(64). For the\npure manipulation task experiments based on SAC, the data\nis input into policy networks and action-value networks with\nthe same F(512) −F(512).\nD. Hyperparameter Setting\nIn causal discovery phase, learning rate is set to 5e-4, batch\nsize is set to 32. The randomly sampled data is used with 80%\nfor training the reward prediction model and 20% for causal\ndiscovery. The amount of data collected through intervention\nsampling is the same as the amount of data collected through\nrandom sampling. For mobile manipulation task, epochs is\nset to 500. For each action-reward pair, 10,000 samples are\nrandomly collected. For pure manipulation task, epochs is set\nto 100, 5000 samples are randomly collected.\nThe hyperparameters for the reinforcement learning phase\nare shown in the table IV.\nTABLE IV: Hyperparameter Setting\ntask\nhyperparameter\nvalue\nmobile manipulation\ndiscount factor\n0.99\nlearning rate\n5e-5\nPPO clip range\n0.2\nThe number of steps sampled in an epoch\n10000\nbatch size\n64\nThe number of optimization epochs for each update\n10\npure manipulation\ndiscount factor\n0.99\nsubtask buffer size\n10000\nThe number of training samples for subtasks.\n1000\nbatch size\n512\nlearning rate\n5e-4\nsoft target update factor\n0.005\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2025-03-05",
  "updated": "2025-03-05"
}