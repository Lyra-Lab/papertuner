{
  "id": "http://arxiv.org/abs/cmp-lg/9404005v1",
  "title": "Memoization in Constraint Logic Programming",
  "authors": [
    "Mark Johnson"
  ],
  "abstract": "This paper shows how to apply memoization (caching of subgoals and associated\nanswer substitutions) in a constraint logic programming setting. The research\nis is motivated by the desire to apply constraint logic programming (CLP) to\nproblems in natural language processing that involve (constraint) interleaving\nor coroutining, such as GB and HPSG parsing.",
  "text": "arXiv:cmp-lg/9404005v1  11 Apr 1994\nMemoization in Constraint Logic Programming\nMark Johnson∗\nBrown University\n1\nIntroduction\nThis paper shows how to apply memoization (caching of subgoals and associated answer substi-\ntutions) in a constraint logic programming setting. The research is is motivated by the desire to\napply constraint logic programming (CLP) to problems in natural language processing.\nIn general, logic programming provides an excellent theoretical framework for computational\nlinguistics [13]. CLP extends “standard” logic programming by allowing program clauses to in-\nclude constraints from a specialized contraint language. For example, the CLP framework allows\nthe feature-structure constraints that have proven useful in computational linguistics [15] to be\nincorporated into logic programming in a natural way [1, 5, 16].\nBecause modern linguistic theories describe natural language syntax as a system of interacting\n“modules” which jointly determine the linguistic structures associated with an utterance [2], a\ngrammar can be regarded as a conjunction of constraints whose solutions are exactly the well-formed\nor grammatical analyses. Parsers for such grammars typically coroutine between a tree-building\ncomponent that generates nodes of the parse tree and the well-formedness constraints imposed by\nthe linguistic modules on these tree structures [4, 6, 8]. Both philosophically and practically, this\nﬁts in well with the CLP approach.\nBut the standard CLP framework inherits some of the weaknesses of the SLD resolution pro-\ncedure that it is based on. When used with the standard formalization of a context-free grammar\nthe SLD resolution procedure behaves as a recursive descent parser.\nWith left-recursive gram-\nmars such parsers typically fail to terminate because a goal corresponding to a prediction of a\nleft-recursive category can reduce to an identical subgoal (up to renaming), producing an “inﬁnite\nloop”. Standard techniques for left-recursion elimination [12, 13] in context-free grammars are not\nalways directly applicable to grammars formulated as the conjunction of several constraints [10].\nWith memoization, or the caching of intermediate goals (and their corresponding answer sub-\nstitutions), a goal is solved only once and its solutions are cached; the solutions to identical goals\nare obtained from this cache. Left recursion need not lead to non-termination because identical\nsubgoals are not evaluated, and the inﬁnite loop is avoided. Further, memoization can sometimes\nprovide the advantages of dynamic programming approaches to parsing: the Earley deduction proof\nprocedure (a memoized version of SLD resolution) simulates an Earley parse [3] when used with\nthe standard formalization of a context-free grammar [14].\n∗This paper was presented at the First Workshop on Principles and Practice of Constraint Programming, April\n28–30 1993, Newport, Rhode Island. The research descrined in it was initiated during a summer visit to the IMSV,\nUniversit¨at Stuttgart; which I would like to thank for their support. Thanks also to Pascal van Hentenryck and\nFernando Pereira for their important helpful suggestions.\n1\nparse(String, Tree) :- wf(Tree, s), y(Tree, String, []).\ny( -Word, [Word|Words], Words).\ny( /[Tree1], Words0, Words) :- y(Tree1, Words0, Words).\ny( /[Tree1,Tree2], Words0, Words) :-\ny(Tree1, Words0, Words1), y(Tree2, Words1, Words).\nwf(np-kim, np).\n% NP →Kim\nwf(n-friend, n).\n% N →friend\nwf(v-walks, v).\n% V →walks\nwf(s/[Tree1, Tree2], s) :- wf(Tree1, np), wf(Tree2, vp).\n% S →NP VP\nwf(np/[Tree1, Tree2], np) :- wf(Tree1, np), wf(Tree2, n).\n% NP →NP N\nwf(vp/[Tree1], vp) :- wf(Tree1, v).\n% VP →V\nFigure 1: A grammar fragment\nThus constraint logic programming and memoization are two recent developments in logic pro-\ngramming that are important for natural language processing. But it is not obvious how, or even if,\nthe two can be combined in a single proof procedure. For example, both Earley Deduction [14] and\nOLDT resolution [17, 20] resolve literals in a strict left-to-right order, so they are not capable of\nrudimentary constraint satisfaction techniques such as goal delaying. The strict left-to-right order\nrestriction is relaxed but not removed in [18, 19], where literals can be resolved in any local order.\nThis paper describes soundness and completeness proofs for a proof procedure that extends these\nmethods to allow for goal delaying. In fact, the lemma table proof procedure generalizes naturally\nto constraint logic programming over arbitrary domains, as described below.\nThe lemma table proof procedure generalizes Earley Deduction and OLDT resolution in three\nways.\n• Goals can be resolved in any order (including non-local orders), rather than a ﬁxed left-to-\nright or a local order.\n• The goals entered into the table consist of non-empty sets of literals rather than just single\nliterals. These sets can be viewed as a single program literal and zero or more constraints\nthat are being passed down into the subsidary proof.\n• The solutions recorded in the lemma table may contain unresolved goals. These unresolved\ngoals can be thought of as constraints that are being passed out of the subsidary proof.\n2\nA linguistic example\nConsider the grammar fragment in Figure 1 (cf. also [4, pages 142–177]). The parse relation holds\nbetween a string and a tree if the yield of the tree is the string to be parsed and tree satisﬁes a well-\nformedness condition. In this example, the well-formedness condition is that the tree is generated\nby a simple context-free grammar, but in more realistic fragments the constraints are considerably\nmore complicated.\n2\nTrees are represented by terms. A tree consisting of a single pre-terminal node labelled C whose\nsingle child is the word W is represented by the term C-W. A tree consisting of a root node labelled\nC dominating the sequence of trees T1 . . . Tn is represented by the term C/[T1, . . . , Tn].\nThe predicate wf(Tree, Cat) holds if Tree represents a well-formed parse tree with a root node\nlabelled Cat for the context free grammar shown in the comments. The predicate y(Tree, S0, S)\nholds if S0–S is a “diﬀerence list” representing the yield of Tree; it collects the terminal items in the\nfamiliar tree-walking fashion. From this program, the following instances of parse can be deduced\n(these are meant to approximate possessive constructions like Kim’s friend’s friend walks).\nparse([kim,walks], s/[np-kim,vp/[v-walks]]).\nparse([kim,friend,walks], s/[np/[np-kim,n-friend],vp/[v-walks]]).\nparse([kim,friend,friend,walks],\ns/[np/[np/[np-kim,n-friend],n-friend],vp/[v-walks]]).\n. . .\nThe parsing problem is encoded as a goal as follows. The goal consists of an atom with the\npredicate parse whose ﬁrst argument instantiated to the string to be parsed and whose second\nargument is uninstantiated. The answer substitution binds the second argument to the parse tree.\nFor example, an answer substitution for the goal parse([Kim,friend,walks], Tree) will have the parse\ntree for the string Kim friend walks as the binding for Tree.\nNow consider the problem of parsing using the program shown in Figure 1. Even with the vari-\nable String instantiated, both of the subgoals of parse taken independently have an inﬁnite number\nof subsumption-incomparable answer substitutions. Informally, this is because there are an inﬁnite\nnumber of trees generated by the context-free grammar, and there are an inﬁnite number of trees\nthat have any given non-empty string as their yield. Because the standard memoization techniques\nmentioned above all compute all of the answer substitutions to every subgoal independently, they\nnever terminate on such a program.\nHowever, the set of answer substitions that satisfy both constraints is ﬁnite, because the number\nof parse trees with the same yield with respect to this grammar is ﬁnite. The standard approach\nto parsing with such grammars takes advantage of this by using a selection rule that “coroutines”\nthe goals wf and y, delaying all wf goals until the ﬁrst argument is instantiated to a non-variable.\nIn such a system, the wf goals function as constraints that ﬁlter the trees generated by the goals y.\nIn this example, however, there is a second, related, problem. Coroutining is not suﬃcient to\nyield a ﬁnite SLD tree, even though the number of refutations is ﬁnite. Informally, this is because\nthe grammar in Figure 1 is left recursive, and the search space for a recursive descent parser (which\nan SLD refutation mimics with such a program) is inﬁnite.\nFigure 2 shows part of an inﬁnite SLD derivation from the goal parse(KW, T), where KW is\nassumed bound to [kim,walks] (although the binding is actually immaterial, as no step in this\nrefutation instantiates this variable). The selection rule expresses a “preference” for goals with\ncertain arguments instantiated. If there is a literal of the form wf(T, C) with T instantiated to a non-\nvariable then the left-most such literal is selected, otherwise if there is a literal of the form y(T, S0, S)\nwith S0 instantiated to a non-variable then the left-most such literal is selected, otherwise the left-\nmost literal is selected. The selected literal is underlined, and the new literals introduced by each\nreduction are inserted to the left of the old literals.\nNote that the y and wf literals resolved at steps (6) and (7) in Figure 2 are both children\n3\n(1) parse(KW,T).\n(2) wf(T,s), y(T,KW,[]).\n(3) wf( /[T1,T2],s), y(T1,KW,S1), y(T2,S1,[]).\n(4) wf(T1,np), wf(T2,vp), y(T1,KW,S1), . . .\n(5) y(T3,KW,S3), y(T4,S3,S1), wf( /[T3,T4],np), . . .\n(6) wf(T3,np), wf(T4,n), y(T3,KW,S3), . . .\n(7) y(T5,KW,S5), y(T6,S5,S3), wf( /[T5,T6],np), . . .\n. . .\nFigure 2: An inﬁnite SLD refutation, despite co-routining\nof and variants of the literals resolved at steps (5) and (6).\nThis sequence of resolution steps\ncan be iterated an arbitrary number of times. It is a manifestation of the left recursion in the\nwell-formedness constraint wf.\nOne standard technique for dealing with such left-recursion is memoization [14, 13]. But there\nare two related problems in applying the standard logic programming memoization techniques to\nthis problem.\nFirst, because the standard methods memoize and evaluate at the level of an individual literal,\nthe granularity at which they apply memoization is is to small. As noted above, in general individual\nwf or y literal can have an inﬁnite number of answer substitutions. The lemma table proof procedure\ncircumvents this problem by memoizing conjunctions of literals (in this example, a conjunction of\nwf and y literals which has only a ﬁnite number of subsumption-incomparable valid instances).\nThe second problem is that the standard memoization techniques restrict the order in which\nliterals can be resolved.\nIn general, these restrictions prevent the “goal delaying” required to\nco-routine among several constraints. The lemma table proof procedure lifts this restriction by\nallowing arbitrary selection rules.\n3\nThe Lemma Table proof procedure\nLike the Earley Deduction and the OLDT proof procedures, the Lemma Table proof procedure\nmaintains a lemma table that records goals and their corresponding solutions. After a goal has\nbeen entered into the lemma table, other occurences of instances of that goal can be reduced by\nthe solutions from the lemma table instead of the original program clauses.\nWe now turn to a formal presentation of the Lemma Table proof procedure. In what follows,\nlower-case letters are used for variables that range over atoms. Upper-case letters are used for\nvariables that range over goals, which are sets of atoms. Goals are interpreted conjunctively; a goal\nis satisﬁed iﬀall of its members are.\nA goal G subsumes a goal G′ iﬀthere is some substitution θ such that G′ = Gθ. (Note that\nm.g.u.’s for sets of goals are in general not unique even up to renaming).\nThe “informational units” manipulated by the lemma table proof procedure are called general-\nized clauses. A generalized clause is a pair of goals, and is written G1 ←G2. G1 is called the head\nof the clause and G2 is called the body. Both the head and body are interpreted conjunctively; i.e.,\nG1 ←G2 should be read as “if each of the G2 are true, then all of the G1 are true”. A generalized\n4\nclause has a natural interpretation as a goal subject to constraints: G1 is true in any interpretation\nwhich satisﬁes the constraints expressed by G2.\nA lemma table is a set of table entries of the form ⟨G, T, S⟩, where\n1. G is a goal (this entry is called a table entry for G),\n2. T is a lemma tree (see below), and\n3. S is a sequence of clauses, called the solution list for this entry.\nA lemma tree is a tree constructed by the algorithm described below. Its nodes have two labels.\nThese are\n1. a clause A ←B, called the clause labelling of the node, and\n2. an optional tag, which when present is one of solution, program(b) for some b ∈B, or\ntable(B′, p) where ∅⊂B′ ⊆B and p is either the null pointer nil or a pointer into a so-\nlution list of a table entry for some G that subsumes B′.\nUntagged nodes are nodes that have not yet been processed. All nodes are untagged when they\nare created, and they are assigned a tag as they are processed. The tags indicate which kind of\nresolution has been applied to this clause. A node tagged program(b) is resolved against the clauses\ndeﬁning b in the program. A node tagged table(B′, p) is resolved against the instances of a table\nentry E for some goal that subsumes B′; the pointer p keeps track of how many of the solutions\nfrom E have been inserted under this node (just as in OLDT resolution). Finally, a node tagged\nsolution is not resolved, rather its clause labelling is added to the solution list for this table entry.\nJust as SLD resolution is controlled by a selection rule that determines which literal will be\nreduced next, the Lemma Table proof procedure is controlled by a control rule R which determines\nthe next goal (if any) to be reduced and the manner of its reduction.\nMore precisely, R must tag a node with clause labelling A ←B with a tag that is either solution,\nprogram(b) for some b ∈B, or table(B′, nil) such that ∅⊂B′ ⊆B. Further, R must tag the root\nnode of every lemma tree with the tag program(b) for some b (this ensures that some program\nreductions are performed in every lemma tree, and hence that a lemma table entry cannot be used\nto reduce itself vacuously).\nFinally, as in OLDT resolution, the Lemma Table proof procedure allows a user-speciﬁed ab-\nstraction operation α that maps goals to goals such that α(G) subsumes G for all goals G. This\nis used to generalize the goals in the same way as the term-depth abstraction operation in OLDT\nresolution, which it generalizes.\nThe Lemma Table proof procedure can now be presented.\nInput: A non-empty goal G, a program P, an abstraction operation α, and a control rule R.\nOutput: A set γ of clauses of the form G′ ←C, where G′ is an instance of G.\nAlgorithm: Create a lemma table with one table entry ⟨G, T, []⟩, where T contains a single un-\ntagged node with the clause labelling G ←G. Then repeat the following operations until no\noperation applies. Finally, return the solution list from the table entry for G.\nThe operations are as follows.\n5\nPrediction Let v be an untagged node in a lemma tree T of table entry ⟨G, T, S⟩, and let\nv’s clause labelling be A ←B. Apply the rule R to v, and perform the action speciﬁed\nbelow depending on the form of the tag R assigned to v.\nsolution : Add A ←B to the end of the solution list S.\nprogram(b) : Let B′ = B −{b}. Then for each clause b′ ←C in P such that b and b′\nunify with a m.g.u. θ, create an untagged child node v′ of v labelled (A ←B′ ∪C)θ.\ntable(B′, nil) : The action in this case depends on whether there already is a table entry\n⟨G′, T ′, S′⟩for some G′ that subsumes B′. If there is, set the pointer in the tag to\nthe start of the sequence S′. If there is not, create a new table entry ⟨α(B′), T ′′, []⟩,\nwhere T ′′ contains a single untagged node with clause labelling α(B′) ←α(B′). Set\nthe pointer in v’s tag to point to the empty solution list of this new table entry.\nCompletion Let v be a node with clause labelling A ←B and tagged table(B′, p) such that\np points to a non-null portion S′ of a solution list of some table entry. Then advance\np over the ﬁrst element B′′ ←C of S′ to point to the remainder of S′.\nFurther, if\nB′ and B′′ unify with m.g.u. θ, then add a new untagged child node to v labelled\n(A ←(B −B′) ∪C)θ.\nIt may help to consider an example based on the program in Figure 1.\nThe computation\nrule R used is the following.\nLet v be a node in a lemma tree and let A ←B be its clause\nlabel.\nIf v is the root of a lemma tree and B contains a literal of the form y(T, S0, S) then\nR(v) = program(y(T, S0, S)).\nIf B is empty then R(v) = solution (no other tagging is pos-\nsible for such nodes).\nIf B contains a literal of the form wf(T, C) where T is a non-variable\nthen R(v) = program(wf(T, C)) (there is never more than one such literal).\nOtherwise, if B\ncontains two literals of the form wf(T, C), y(T, S0, S) where S0 is a non-variable, then R(v) =\ntable({wf(T, C), y(T, S0, S)}, nil). These four cases exhaust all of the node labelling encountered in\nthe example.1\nFigure 3 depicts the completed lemma table constructed using the rule R for the goal wf(Tree,s),\ny(Tree,[kim,walks],[]). To save space, kim and walks are abbreviated to k and w respectively. Only\nthe tree from each table entry is shown because the other components of the entry can be read\noﬀthe tree. The goal of each table entry is the head of the clause labelling its root clause, and is\nshown in bold face. Solution nodes are shown in italic face. Lookup nodes appear with a dashed\nline pointing to the table entry used to reduce them.\nThe ﬁrst few steps of the proof procedure are the following; each step corresponds to the circled\nnode of the same number.\n(1) The root node of the ﬁrst table entry’s tree restates the goal to be proven. Informally, this node\nsearches for an S located at the beginning of the utterance. The literal y(Tree,[kim,walks],[]) is\nselected for program reduction. This reduction produces two child nodes, one of which “dies”\nin the next step because there are no matching program nodes.\n(2) The reduction (1) partially instantiates the parse tree Tree.\nThe literal wf(C/[T1,T2]) is\nselected for program reduction.\n1 When used with a program encoding a context-free grammars in the manner of Figure 1, the Lemma Table\nproof procedure with the control rule R simulates Earley’s CFG parsing algorithm [3]. The operations in the Lemma\nTable proof procedure are named after the corresponding operations of Earley’s algorithm.\n6\nwf(Tree,s), y(Tree,[k,w],[]) ←wf(Tree,s), y(Tree,[k,w],[])\nwf(C/[T1],s), y(C/[T1],[k,w],[]) ←\n    wf(C/[T1],s), y(T1,[k,w],[])\nwf(C/[T1,T2],s), y(C/[T1,T2],[k,w],[]) ←\n     wf(C/[T1,T2],s), y(T1,[k,w],S1), y(T2,S1,[])\nwf(s/[T1,T2],s), y(s/[T1,T2],[k,w],[]) ←\n     wf(T1,np), wf(T2,vp), y(T1,[k,w],S1), y(T2,S1,[])\nwf(s/[np-kim,T2],s), y(s/[np-kim,T2],[k,w],[]) ←\n    wf(T2,vp), y(T2,[w])\nwf(s/[np-k,vp/[v-w]],s), y(s/[np-k,vp/[v-w]],[k,w],[]) ← ∅\nwf(T,np), y(T,[k,w],S) ← wf(T,np), y(T,[k,w],S)\nwf(C-k,np), y(C-k,[k,w],[w]) ←\n    wf(C-k,np)\nwf(np-k,np), y(np-k,[k,w],[w]) ← ∅\nwf(C/[T1],np), y(C/[T1],[k,w],S) ←\n    wf(C/[T1],np), y(T1,[k,w],S)\nwf(C/[T1,T2],np), y(C/[T1,T2],[k,w],S) ←\n    wf(C/[T1,T2],np), y(T1,[k,w],S1), y(T2,S1,S)\nwf(np/[T1,T2],np), y(np/[T1,T2],[k,w],S) ←\n    wf(T1,np), wf(T2,n), y(T1,[k,w],S1), y(T2,S1,S)\nwf(np/[np-k,T2],np), y(np/[np-k,T2],np) ←\n    wf(T2,n), y(T2,[w],S)\nwf(T,n), y(T,[w],S) ← wf(T,n), y(T,[w],S)\nwf(C-w,n), y(C-w,[w],[]) ←\n    wf(C-w,n)\nwf(C/[T1],n), y(C/[T1],[w],S) ←\n    wf(C/[T1],n), y(T1,[w],S)\nwf(C/[T1,T2],n), y(C/[T1,T2],[w],S) ←\n    wf(C/[T1,T2],n), y(T1,[w],S1), y(T2,S1,S)\nwf(T,vp), y(T,[w],S) ← wf(T,vp), y(T,[w],S)\nwf(C-w,vp), y(C-w,[w],[]) ←\n    wf(C-w,vp)\nwf(C/[T1],vp), y(C/[T1],[w],S) ←\n    wf(C/[T1],vp), y(T1,[w],S)\nwf(vp/[T1],vp), y(vp/[T1],[w],S) ←\n    wf(T1,v), y(T1,[w],S)\nwf(vp/[v-w],vp), y(vp/[v-w],[w],[]) ← ∅\nwf(C/[T1,T2],vp), y(C/[T1,T2],[w],S) ←\n    wf(C/[T1,T2],vp), y(T1,[w],S1), y(T2,S1,S)\nwf(T,v), y(T,[w],S) ← wf(T,v), y(T,[w],S)\nwf(C-w,v), y(C-w,[w],[]) ←\n    wf(C-w,v)\nwf(v-w,v), y(v-w,[w],[]) ← ∅\nwf(C/[T1],v), y(C/[T1],[w],S) ←\n    wf(C/[T1],v), y(T1,[w],S)\nwf(C/[T1,T2],v), y(C/[T1,T2],[w],S) ←\n    wf(C/[T1,T2],v), y(T1,[w],S1), y(T2,S1,S)\n1\n2\n3\n4\n5\n6\n8\n7\n9\nFigure 3: A lemma table for wf(Tree,s), y(Tree,[kim,walks],[])\n7\n(3) This produces a clause body that contains literals that refer to the subtree T1 and literals\nthat refer to the subtree T2. The computation rule in eﬀect partitions the literals and selects\nthose that refer to the subtree T1 (because they are associated with an instantiated left string\nargument).\n(4) A new table entry is created for the literals selected in (3). Informally, this entry searches for\nan NP located at the beginning of the utterance. Because this node is a root node, the literal\ny(T,[kim,walks],S) is selected for program expansion.\n(5–6) The literals with predicate wf are selected for program expansion.\n(7) The body of this node’s clause label is empty, so it’s label is added to the solutions list of the\ntable entry. Informally, this node corresponds to the string [kim] having been recognized as\nan NP.\n(8) The solution found in (7) is incorporated into the tree beneath (3). A new table entry is\ngenerated to search for a VP spanning the string [walks].\n(9) Just as in (3), the literals in the body of this clause’s label refer to two distinct subtrees, and\nas before the computation rule selects the literals that refer to T1. However there is already a\ntable entry (4) for the selected goal, so a new table entry is not created. The solutions already\nfound for (4) generate a child node to search for an N beginning at walks. No solutions are\nfound for this search.\n4\nSoundness and Completeness\nThis section demonstrates the soundness and completness of the lemma table proof procedure.\nSoundness is straight-forward, but completeness is more complex to prove. The completeness proof\nrelies on the notion of an unfolding of a lemma tree, in which the table nodes of a lemma tree are\nsystematically replaced with the tree that they point to. In the limit, the resulting tree can be\nviewed a kind of SLD proof tree, and completeness follows from the completeness of SLD resolution.\nTheorem 1 (Soundness) If the output of lemma table proof procedure contains a clause G ←C,\nthen P |= C →G.\nProof: Each of the clause labels on lemma tree nodes is either a tautology or derived by resolving\nother clause labels and program clauses. Soundness follows by induction on the number of steps\ntaken by the proof procedure.\nAs might be expected, the completeness proof is much longer than the soundness proof. For\nspace reasons it is only sketched here.\nFor the completeness proof we assume that the control rule R is such that the output γ of\nthe lemma table proof procedure contains only clauses with empty bodies.\nThis is reasonable\nin the current context, because non-empty clause bodies correspond to goals that have not been\ncompletely reduced.\nThen completeness follows if for all P, G and σ, if P |= Gσ then there is an instance G′ on the\nsolution list that subsumes Gσ.\n8\nFurther, without loss of generality the abstraction operation α is assumed to be the identity\nfunction on goals, since if α(G(⃗t)) = G(⃗t′), the goal G(⃗t) can be replaced with the equivalent\nG(⃗t′) ∪{⃗t′ = ⃗t} and α taken to be the identity function.\nNow, it is a corollary of the Switching Lemma [11, pages 45–47] that if P |= Gσ then there is an\nn such that for any computation rule there is an SLD refutation of G of length n whose computed\nanswer substitution θ subsumes σ. We show that if θ is a computed answer substitution for an\nSLD derivation of length n then there is a node tagged solution and labelled Gθ ←∅in lemma tree\nT for the top-level goal.\nFirst, a well-formedness condition on lemma trees is introduced. Every lemma tree in a lemma\ntable at the termination of the lemma table proof procedure is well-formed. Well-formedness and\nthe set of nodes tagged solution are preserved under an abstract operation on lemma trees called\nexpansion. The expansion of a lemma tree is the tree obtained by replacing each node tagged\ntable(B′, p) with the lemma tree in the table entry pointed to by p.\nBecause expansions preserves well-formedness, the lemma tree T ′ resulting from n iterated\nexpansions of the lemma tree T for the top-level goal is also well-formed. Moreover, since the root\nnode of every lemma tree is required to be tagged program, all nodes in T ′ within distance n arcs of\nthe root will be tagged program or solution. This top part of T ′ is isomorphic to the top part of an\nSLD tree Ts for G, so if θ is a computed answer substitution for an SLD derivation in Ts of length\nn or less then there is a node tagged solution and labelled Gθ ←∅in T ′, and hence T. Since n was\narbitrary, every SLD refutation in Ts has a corresponding node tagged solution in T ′ and hence in\nT.\n5\nConclusion\nThis paper generalizes standard memoization techniques for logic programming to allow them to\nbe used for constraint logic programming. The basic informational unit used in the Lemma Table\nproof procedure is the generalized clause G ←C. Generalized clauses can be given a constraint\ninterpretation as “any interpretation which satisﬁes the constraints C also satisﬁes G”. The lemmas\nrecorded in the lemma table state how sets of literals are reduced to other sets of literals. Because\nthe heads of the lemmas consist of sets of literals rather than just individual literals, the lemmas\nexpress properties of systems of constraints rather than just individual constraints. Because the\nsolutions recorded in the lemma table can contain unresolved constraints, it is possible to pass\nconstraints out of a lemma into the superordinate computation.\nIn this paper G and C were taken to be sets of literals and the constraints C were deﬁned by\nHorn clauses. In a more general setting, both G and C would be permitted to contain constraints\ndrawn from a specialized constraint language not deﬁned by a Horn clause program.\nH¨ohfeld\nand Smolka [5] show how to extend SLD resolution to allow general constraints over arbitrary\ndomains. Their elegant relational approach seems to be straight-forwardly applicable to the Lemma\nTable proof procedure, and would actually simplify its theoretical description because equality (and\nuniﬁcation) would be treated in the constraint system. Uniﬁcation failure would then be a special\ncase of constraint unsatisﬁability, and would be handled by the “optimization” described by H¨ofeld\nand Smolka that permits nodes labelled with clauses G ←C to be deleted if C is unsatisﬁable.\nReferences\n9\n[1] Chen, W. and D.S. Warren. 1989. C-Logic of Complex Objects. ms. Department of Computer\nScience, State University of New York at Stony Brook.\n[2] Chomsky, N. 1986. Knowledge of Language: Its nature, origins and use. Praeger. New York.\n[3] Earley, J. 1970. “An eﬃcient context-free parsing algorithm”, in Comm. ACM 13:2, pages 94–\n102.\n[4] Giannesini, F., H. Kanoiui, R. Pasero and M. van Canegham. 1986. Prolog. Addison-Wesley.\nReading, Massachusetts.\n[5] H¨ohfeld, M. and G. Smolka. 1988. Deﬁnite Relations over Constraint Languages Lilog re-\nport 53, IBM Deutschland.\n[6] Johnson, M. 1989. “The Use of Knowledge of Language”, in Journal of Psycholinguistic Re-\nsearch, 18.1.\n[7] Johnson, M. 1990. “Features, frames and quantiﬁer-free formulae”, in P. Saint-Dizier and S.\nSzpakowicz, eds., Logic and Logic Grammars for Language Processing, Ellis Horwood, New\nYork, pages 94–107.\n[8] Johnson, M. 1991. “Deductive Parsing: The Use of Knowledge of Language”, in R.C. Berwick,\nS.P. Abney and C. Tenny, eds., Principle-based Parsing: Computational and Psycholinguistics,\nKluwer Academic Publishers, Dordrecht, pages 39–65.\n[9] Johnson, M. 1991. “Techniques for deductive parsing”, in C.G. Brown and G. Koch, eds.,\nNatural Language and Logic Programming III, North Holland, Amsterdam, pages 27–42.\n[10] Johnson, M. 1992. “The left-corner program transformation”, ms., Brown University.\n[11] Lloyd, J. 1984. Foundations of Logic Programming. Springer-Verlag, Berlin.\n[12] Matsumoto, M., H. Tanaka, H. Hirakawa, H. Miyoshi and H. Yasukawa. 1983. “BUP: a bottom-\nup parser embedded in Prolog”, in New Generation Computing 1:2, pages 145–158.\n[13] Pereira, F. and S. Shieber. 1987 Prolog and Natural Language Analysis. CSLI Lecture Notes\nSeries, Chicago University Press.\n[14] Pereira, F. and D.H. Warren. 1983. “Parsing as Deduction”, in Proceedings of the 21st Annual\nMeeting of the Association for Computational Linguistics. MIT, Cambridge, Mass.\n[15] Shieber, S. 1985. Introduction to Uniﬁcation-based theories of Grammar. CLSI Lecture Notes\nSeries, Chicago University Press.\n[16] Smolka, G. 1992. “Feature Constraint Logics for Uniﬁcation Grammars”, in The Journal of\nLogic Programming 12:1-2, pages 51–87.\n[17] Tamaki, H. and T. Sato. 1986. “OLDT resolution with tabulation”, in Proceedings of Third\nInternational Conference on Logic Programming, Springer-Verlag, Berlin, pages 84–98.\n10\n[18] Vieille, L. 1987. “Database-complete proof procedures based on SLD resolution”, in Logic\nProgramming: Proceedings of the fourth international conference, The MIT Press. Cambridge,\nMassachusetts.\n[19] Vieille, L. 1989. “Recursive query processing: the power of logic”, Theoretical Computer\nScience 69, pages 1–53.\n[20] Warren, D. S. 1992. “Memoing for logic programs”, in Communications of the ACM 35:3,\npages 94–111.\n11\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1994-04-11",
  "updated": "1994-04-11"
}